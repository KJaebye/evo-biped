[32m[20230203 20:50:35 @logger.py:106][0m Log file set to ./tmp/coevo_bipedalwalker/easy/20230203_205035/log/coevo_bipedalwalker_easy-20230203_205035.log
[32m[20230203 20:50:35 @agent_ppo2.py:129][0m #------------------------ Iteration 0 --------------------------#
[32m[20230203 20:50:36 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:50:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:36 @agent_ppo2.py:193][0m |           0.0164 |         268.9002 |           1.8252 |
[32m[20230203 20:50:36 @agent_ppo2.py:193][0m |           0.0014 |         255.1845 |           1.8251 |
[32m[20230203 20:50:36 @agent_ppo2.py:193][0m |           0.0028 |         250.0331 |           1.8249 |
[32m[20230203 20:50:36 @agent_ppo2.py:193][0m |          -0.0051 |         235.2430 |           1.8248 |
[32m[20230203 20:50:36 @agent_ppo2.py:193][0m |          -0.0052 |         227.2636 |           1.8244 |
[32m[20230203 20:50:36 @agent_ppo2.py:193][0m |          -0.0044 |         221.2131 |           1.8245 |
[32m[20230203 20:50:36 @agent_ppo2.py:193][0m |          -0.0070 |         213.3648 |           1.8242 |
[32m[20230203 20:50:36 @agent_ppo2.py:193][0m |          -0.0012 |         211.5541 |           1.8239 |
[32m[20230203 20:50:36 @agent_ppo2.py:193][0m |          -0.0049 |         203.1790 |           1.8237 |
[32m[20230203 20:50:36 @agent_ppo2.py:193][0m |          -0.0044 |         202.5494 |           1.8236 |
[32m[20230203 20:50:36 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:50:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: -118.42
[32m[20230203 20:50:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -107.54
[32m[20230203 20:50:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -92.11
[32m[20230203 20:50:37 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: -92.11
[32m[20230203 20:50:37 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -92.11
[32m[20230203 20:50:37 @agent_ppo2.py:151][0m Total time:       0.02 min
[32m[20230203 20:50:37 @agent_ppo2.py:153][0m 2048 total steps have happened
[32m[20230203 20:50:37 @agent_ppo2.py:129][0m #------------------------ Iteration 1 --------------------------#
[32m[20230203 20:50:37 @agent_ppo2.py:135][0m Sampling time: 0.65 s by 1 slaves
[32m[20230203 20:50:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:37 @agent_ppo2.py:193][0m |          -0.0008 |          98.9666 |           1.8767 |
[32m[20230203 20:50:37 @agent_ppo2.py:193][0m |          -0.0034 |          94.8780 |           1.8768 |
[32m[20230203 20:50:37 @agent_ppo2.py:193][0m |          -0.0031 |          92.9385 |           1.8765 |
[32m[20230203 20:50:37 @agent_ppo2.py:193][0m |          -0.0039 |          91.7475 |           1.8766 |
[32m[20230203 20:50:38 @agent_ppo2.py:193][0m |          -0.0030 |          90.8520 |           1.8765 |
[32m[20230203 20:50:38 @agent_ppo2.py:193][0m |          -0.0007 |          90.9351 |           1.8766 |
[32m[20230203 20:50:38 @agent_ppo2.py:193][0m |           0.0048 |          93.0336 |           1.8763 |
[32m[20230203 20:50:38 @agent_ppo2.py:193][0m |          -0.0038 |          88.3271 |           1.8758 |
[32m[20230203 20:50:38 @agent_ppo2.py:193][0m |          -0.0007 |          88.9082 |           1.8757 |
[32m[20230203 20:50:38 @agent_ppo2.py:193][0m |          -0.0059 |          86.0333 |           1.8756 |
[32m[20230203 20:50:38 @agent_ppo2.py:138][0m Policy update time: 0.69 s
[32m[20230203 20:50:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: -106.66
[32m[20230203 20:50:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -100.28
[32m[20230203 20:50:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -92.25
[32m[20230203 20:50:38 @agent_ppo2.py:151][0m Total time:       0.05 min
[32m[20230203 20:50:38 @agent_ppo2.py:153][0m 4096 total steps have happened
[32m[20230203 20:50:38 @agent_ppo2.py:129][0m #------------------------ Iteration 2 --------------------------#
[32m[20230203 20:50:39 @agent_ppo2.py:135][0m Sampling time: 0.60 s by 1 slaves
[32m[20230203 20:50:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:39 @agent_ppo2.py:193][0m |          -0.0002 |           2.5225 |           1.8822 |
[32m[20230203 20:50:39 @agent_ppo2.py:193][0m |          -0.0012 |           1.8326 |           1.8829 |
[32m[20230203 20:50:39 @agent_ppo2.py:193][0m |          -0.0021 |           1.5546 |           1.8841 |
[32m[20230203 20:50:39 @agent_ppo2.py:193][0m |          -0.0031 |           1.3799 |           1.8850 |
[32m[20230203 20:50:39 @agent_ppo2.py:193][0m |          -0.0042 |           1.2666 |           1.8855 |
[32m[20230203 20:50:39 @agent_ppo2.py:193][0m |          -0.0053 |           1.1897 |           1.8859 |
[32m[20230203 20:50:39 @agent_ppo2.py:193][0m |          -0.0063 |           1.1369 |           1.8865 |
[32m[20230203 20:50:39 @agent_ppo2.py:193][0m |          -0.0073 |           1.0992 |           1.8868 |
[32m[20230203 20:50:39 @agent_ppo2.py:193][0m |          -0.0080 |           1.0692 |           1.8873 |
[32m[20230203 20:50:40 @agent_ppo2.py:193][0m |          -0.0087 |           1.0485 |           1.8876 |
[32m[20230203 20:50:40 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:50:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: -118.78
[32m[20230203 20:50:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -118.15
[32m[20230203 20:50:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -4.86
[32m[20230203 20:50:40 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: -4.86
[32m[20230203 20:50:40 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -4.86
[32m[20230203 20:50:40 @agent_ppo2.py:151][0m Total time:       0.08 min
[32m[20230203 20:50:40 @agent_ppo2.py:153][0m 6144 total steps have happened
[32m[20230203 20:50:40 @agent_ppo2.py:129][0m #------------------------ Iteration 3 --------------------------#
[32m[20230203 20:50:40 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:50:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:41 @agent_ppo2.py:193][0m |          -0.0081 |         252.3098 |           1.8672 |
[32m[20230203 20:50:41 @agent_ppo2.py:193][0m |          -0.0078 |         230.3096 |           1.8672 |
[32m[20230203 20:50:41 @agent_ppo2.py:193][0m |           0.0026 |         228.7146 |           1.8673 |
[32m[20230203 20:50:41 @agent_ppo2.py:193][0m |          -0.0139 |         206.8247 |           1.8675 |
[32m[20230203 20:50:41 @agent_ppo2.py:193][0m |          -0.0088 |         199.6476 |           1.8673 |
[32m[20230203 20:50:41 @agent_ppo2.py:193][0m |          -0.0058 |         199.8215 |           1.8672 |
[32m[20230203 20:50:41 @agent_ppo2.py:193][0m |          -0.0044 |         194.5702 |           1.8672 |
[32m[20230203 20:50:41 @agent_ppo2.py:193][0m |          -0.0111 |         180.3312 |           1.8672 |
[32m[20230203 20:50:41 @agent_ppo2.py:193][0m |          -0.0145 |         175.4779 |           1.8668 |
[32m[20230203 20:50:41 @agent_ppo2.py:193][0m |          -0.0114 |         171.8052 |           1.8668 |
[32m[20230203 20:50:41 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 20:50:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: -108.70
[32m[20230203 20:50:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -100.58
[32m[20230203 20:50:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -104.01
[32m[20230203 20:50:41 @agent_ppo2.py:151][0m Total time:       0.10 min
[32m[20230203 20:50:41 @agent_ppo2.py:153][0m 8192 total steps have happened
[32m[20230203 20:50:41 @agent_ppo2.py:129][0m #------------------------ Iteration 4 --------------------------#
[32m[20230203 20:50:42 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:50:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:42 @agent_ppo2.py:193][0m |          -0.0025 |         195.4802 |           1.8890 |
[32m[20230203 20:50:42 @agent_ppo2.py:193][0m |          -0.0001 |         187.9075 |           1.8896 |
[32m[20230203 20:50:42 @agent_ppo2.py:193][0m |          -0.0048 |         178.2162 |           1.8903 |
[32m[20230203 20:50:42 @agent_ppo2.py:193][0m |          -0.0002 |         174.4975 |           1.8902 |
[32m[20230203 20:50:42 @agent_ppo2.py:193][0m |          -0.0045 |         168.3095 |           1.8912 |
[32m[20230203 20:50:42 @agent_ppo2.py:193][0m |           0.0054 |         168.7183 |           1.8915 |
[32m[20230203 20:50:42 @agent_ppo2.py:193][0m |          -0.0041 |         158.7035 |           1.8921 |
[32m[20230203 20:50:42 @agent_ppo2.py:193][0m |           0.0010 |         156.4976 |           1.8926 |
[32m[20230203 20:50:42 @agent_ppo2.py:193][0m |           0.0003 |         154.5593 |           1.8927 |
[32m[20230203 20:50:42 @agent_ppo2.py:193][0m |          -0.0029 |         148.2779 |           1.8934 |
[32m[20230203 20:50:42 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 20:50:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: -105.63
[32m[20230203 20:50:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -99.10
[32m[20230203 20:50:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -117.78
[32m[20230203 20:50:43 @agent_ppo2.py:151][0m Total time:       0.12 min
[32m[20230203 20:50:43 @agent_ppo2.py:153][0m 10240 total steps have happened
[32m[20230203 20:50:43 @agent_ppo2.py:129][0m #------------------------ Iteration 5 --------------------------#
[32m[20230203 20:50:43 @agent_ppo2.py:135][0m Sampling time: 0.64 s by 1 slaves
[32m[20230203 20:50:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:43 @agent_ppo2.py:193][0m |           0.0002 |          48.1754 |           1.8918 |
[32m[20230203 20:50:43 @agent_ppo2.py:193][0m |          -0.0005 |          43.4914 |           1.8921 |
[32m[20230203 20:50:43 @agent_ppo2.py:193][0m |          -0.0012 |          42.6452 |           1.8925 |
[32m[20230203 20:50:43 @agent_ppo2.py:193][0m |          -0.0017 |          41.8894 |           1.8928 |
[32m[20230203 20:50:44 @agent_ppo2.py:193][0m |          -0.0024 |          41.2136 |           1.8930 |
[32m[20230203 20:50:44 @agent_ppo2.py:193][0m |          -0.0034 |          40.4735 |           1.8926 |
[32m[20230203 20:50:44 @agent_ppo2.py:193][0m |          -0.0037 |          39.8185 |           1.8929 |
[32m[20230203 20:50:44 @agent_ppo2.py:193][0m |          -0.0042 |          39.1662 |           1.8928 |
[32m[20230203 20:50:44 @agent_ppo2.py:193][0m |          -0.0048 |          38.4351 |           1.8932 |
[32m[20230203 20:50:44 @agent_ppo2.py:193][0m |          -0.0048 |          37.8165 |           1.8936 |
[32m[20230203 20:50:44 @agent_ppo2.py:138][0m Policy update time: 0.66 s
[32m[20230203 20:50:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: -103.68
[32m[20230203 20:50:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -99.64
[32m[20230203 20:50:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -92.96
[32m[20230203 20:50:44 @agent_ppo2.py:151][0m Total time:       0.15 min
[32m[20230203 20:50:44 @agent_ppo2.py:153][0m 12288 total steps have happened
[32m[20230203 20:50:44 @agent_ppo2.py:129][0m #------------------------ Iteration 6 --------------------------#
[32m[20230203 20:50:45 @agent_ppo2.py:135][0m Sampling time: 0.63 s by 1 slaves
[32m[20230203 20:50:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:45 @agent_ppo2.py:193][0m |           0.0005 |          30.3334 |           1.9371 |
[32m[20230203 20:50:45 @agent_ppo2.py:193][0m |          -0.0008 |          29.1404 |           1.9371 |
[32m[20230203 20:50:45 @agent_ppo2.py:193][0m |          -0.0014 |          28.7872 |           1.9375 |
[32m[20230203 20:50:45 @agent_ppo2.py:193][0m |          -0.0032 |          28.3069 |           1.9376 |
[32m[20230203 20:50:45 @agent_ppo2.py:193][0m |          -0.0040 |          27.9252 |           1.9378 |
[32m[20230203 20:50:45 @agent_ppo2.py:193][0m |          -0.0034 |          27.7844 |           1.9378 |
[32m[20230203 20:50:45 @agent_ppo2.py:193][0m |          -0.0048 |          27.2791 |           1.9372 |
[32m[20230203 20:50:45 @agent_ppo2.py:193][0m |          -0.0054 |          27.2000 |           1.9373 |
[32m[20230203 20:50:45 @agent_ppo2.py:193][0m |          -0.0066 |          26.6351 |           1.9374 |
[32m[20230203 20:50:45 @agent_ppo2.py:193][0m |          -0.0072 |          26.2580 |           1.9374 |
[32m[20230203 20:50:45 @agent_ppo2.py:138][0m Policy update time: 0.65 s
[32m[20230203 20:50:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: -105.69
[32m[20230203 20:50:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -95.81
[32m[20230203 20:50:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -93.00
[32m[20230203 20:50:46 @agent_ppo2.py:151][0m Total time:       0.18 min
[32m[20230203 20:50:46 @agent_ppo2.py:153][0m 14336 total steps have happened
[32m[20230203 20:50:46 @agent_ppo2.py:129][0m #------------------------ Iteration 7 --------------------------#
[32m[20230203 20:50:47 @agent_ppo2.py:135][0m Sampling time: 0.63 s by 1 slaves
[32m[20230203 20:50:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:47 @agent_ppo2.py:193][0m |          -0.0016 |          27.8261 |           1.9207 |
[32m[20230203 20:50:47 @agent_ppo2.py:193][0m |          -0.0045 |          25.9571 |           1.9207 |
[32m[20230203 20:50:47 @agent_ppo2.py:193][0m |          -0.0068 |          25.0621 |           1.9205 |
[32m[20230203 20:50:47 @agent_ppo2.py:193][0m |          -0.0079 |          24.3742 |           1.9197 |
[32m[20230203 20:50:47 @agent_ppo2.py:193][0m |          -0.0102 |          23.7697 |           1.9188 |
[32m[20230203 20:50:47 @agent_ppo2.py:193][0m |          -0.0093 |          23.2559 |           1.9177 |
[32m[20230203 20:50:47 @agent_ppo2.py:193][0m |          -0.0066 |          23.9635 |           1.9178 |
[32m[20230203 20:50:47 @agent_ppo2.py:193][0m |          -0.0119 |          22.2778 |           1.9173 |
[32m[20230203 20:50:47 @agent_ppo2.py:193][0m |          -0.0132 |          21.9599 |           1.9175 |
[32m[20230203 20:50:47 @agent_ppo2.py:193][0m |          -0.0139 |          21.3961 |           1.9172 |
[32m[20230203 20:50:47 @agent_ppo2.py:138][0m Policy update time: 0.64 s
[32m[20230203 20:50:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: -115.67
[32m[20230203 20:50:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -106.16
[32m[20230203 20:50:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -93.45
[32m[20230203 20:50:48 @agent_ppo2.py:151][0m Total time:       0.21 min
[32m[20230203 20:50:48 @agent_ppo2.py:153][0m 16384 total steps have happened
[32m[20230203 20:50:48 @agent_ppo2.py:129][0m #------------------------ Iteration 8 --------------------------#
[32m[20230203 20:50:48 @agent_ppo2.py:135][0m Sampling time: 0.63 s by 1 slaves
[32m[20230203 20:50:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:48 @agent_ppo2.py:193][0m |          -0.0002 |          13.7871 |           1.9605 |
[32m[20230203 20:50:48 @agent_ppo2.py:193][0m |          -0.0011 |          13.1572 |           1.9603 |
[32m[20230203 20:50:48 @agent_ppo2.py:193][0m |          -0.0021 |          12.9629 |           1.9598 |
[32m[20230203 20:50:49 @agent_ppo2.py:193][0m |          -0.0035 |          12.8035 |           1.9589 |
[32m[20230203 20:50:49 @agent_ppo2.py:193][0m |          -0.0045 |          12.6413 |           1.9579 |
[32m[20230203 20:50:49 @agent_ppo2.py:193][0m |          -0.0050 |          12.5128 |           1.9568 |
[32m[20230203 20:50:49 @agent_ppo2.py:193][0m |          -0.0057 |          12.3484 |           1.9555 |
[32m[20230203 20:50:49 @agent_ppo2.py:193][0m |          -0.0065 |          12.1659 |           1.9542 |
[32m[20230203 20:50:49 @agent_ppo2.py:193][0m |          -0.0068 |          12.0638 |           1.9532 |
[32m[20230203 20:50:49 @agent_ppo2.py:193][0m |          -0.0072 |          11.9345 |           1.9525 |
[32m[20230203 20:50:49 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:50:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: -106.85
[32m[20230203 20:50:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -102.26
[32m[20230203 20:50:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -93.44
[32m[20230203 20:50:49 @agent_ppo2.py:151][0m Total time:       0.23 min
[32m[20230203 20:50:49 @agent_ppo2.py:153][0m 18432 total steps have happened
[32m[20230203 20:50:49 @agent_ppo2.py:129][0m #------------------------ Iteration 9 --------------------------#
[32m[20230203 20:50:50 @agent_ppo2.py:135][0m Sampling time: 0.69 s by 1 slaves
[32m[20230203 20:50:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:50 @agent_ppo2.py:193][0m |           0.0002 |          50.3336 |           1.9420 |
[32m[20230203 20:50:50 @agent_ppo2.py:193][0m |          -0.0016 |          45.8795 |           1.9410 |
[32m[20230203 20:50:50 @agent_ppo2.py:193][0m |          -0.0001 |          43.8188 |           1.9397 |
[32m[20230203 20:50:50 @agent_ppo2.py:193][0m |          -0.0055 |          40.5512 |           1.9382 |
[32m[20230203 20:50:50 @agent_ppo2.py:193][0m |          -0.0057 |          38.7012 |           1.9372 |
[32m[20230203 20:50:50 @agent_ppo2.py:193][0m |          -0.0050 |          37.1902 |           1.9356 |
[32m[20230203 20:50:51 @agent_ppo2.py:193][0m |          -0.0082 |          35.2042 |           1.9346 |
[32m[20230203 20:50:51 @agent_ppo2.py:193][0m |          -0.0066 |          33.7916 |           1.9335 |
[32m[20230203 20:50:51 @agent_ppo2.py:193][0m |          -0.0069 |          32.5406 |           1.9327 |
[32m[20230203 20:50:51 @agent_ppo2.py:193][0m |          -0.0084 |          31.2902 |           1.9316 |
[32m[20230203 20:50:51 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230203 20:50:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: -105.18
[32m[20230203 20:50:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -96.45
[32m[20230203 20:50:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -93.56
[32m[20230203 20:50:51 @agent_ppo2.py:151][0m Total time:       0.26 min
[32m[20230203 20:50:51 @agent_ppo2.py:153][0m 20480 total steps have happened
[32m[20230203 20:50:51 @agent_ppo2.py:129][0m #------------------------ Iteration 10 --------------------------#
[32m[20230203 20:50:52 @agent_ppo2.py:135][0m Sampling time: 0.60 s by 1 slaves
[32m[20230203 20:50:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:52 @agent_ppo2.py:193][0m |           0.0004 |           2.7831 |           1.9098 |
[32m[20230203 20:50:52 @agent_ppo2.py:193][0m |          -0.0009 |           2.0026 |           1.9094 |
[32m[20230203 20:50:52 @agent_ppo2.py:193][0m |          -0.0019 |           1.8362 |           1.9096 |
[32m[20230203 20:50:52 @agent_ppo2.py:193][0m |          -0.0029 |           1.6973 |           1.9096 |
[32m[20230203 20:50:52 @agent_ppo2.py:193][0m |          -0.0033 |           1.6185 |           1.9092 |
[32m[20230203 20:50:52 @agent_ppo2.py:193][0m |          -0.0041 |           1.5410 |           1.9094 |
[32m[20230203 20:50:52 @agent_ppo2.py:193][0m |          -0.0045 |           1.5009 |           1.9091 |
[32m[20230203 20:50:52 @agent_ppo2.py:193][0m |          -0.0047 |           1.4802 |           1.9094 |
[32m[20230203 20:50:52 @agent_ppo2.py:193][0m |          -0.0049 |           1.4356 |           1.9093 |
[32m[20230203 20:50:52 @agent_ppo2.py:193][0m |          -0.0052 |           1.4168 |           1.9095 |
[32m[20230203 20:50:52 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:50:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: -105.55
[32m[20230203 20:50:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -97.67
[32m[20230203 20:50:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -93.66
[32m[20230203 20:50:53 @agent_ppo2.py:151][0m Total time:       0.29 min
[32m[20230203 20:50:53 @agent_ppo2.py:153][0m 22528 total steps have happened
[32m[20230203 20:50:53 @agent_ppo2.py:129][0m #------------------------ Iteration 11 --------------------------#
[32m[20230203 20:50:53 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:50:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:53 @agent_ppo2.py:193][0m |          -0.0031 |         112.9611 |           1.8774 |
[32m[20230203 20:50:53 @agent_ppo2.py:193][0m |          -0.0004 |          92.2688 |           1.8769 |
[32m[20230203 20:50:53 @agent_ppo2.py:193][0m |          -0.0066 |          83.0498 |           1.8762 |
[32m[20230203 20:50:53 @agent_ppo2.py:193][0m |          -0.0089 |          73.9672 |           1.8752 |
[32m[20230203 20:50:53 @agent_ppo2.py:193][0m |          -0.0096 |          68.4318 |           1.8749 |
[32m[20230203 20:50:53 @agent_ppo2.py:193][0m |          -0.0130 |          61.4823 |           1.8745 |
[32m[20230203 20:50:53 @agent_ppo2.py:193][0m |          -0.0123 |          56.8123 |           1.8743 |
[32m[20230203 20:50:54 @agent_ppo2.py:193][0m |          -0.0123 |          53.6408 |           1.8742 |
[32m[20230203 20:50:54 @agent_ppo2.py:193][0m |          -0.0166 |          50.0051 |           1.8737 |
[32m[20230203 20:50:54 @agent_ppo2.py:193][0m |          -0.0136 |          46.8995 |           1.8735 |
[32m[20230203 20:50:54 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 20:50:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: -110.70
[32m[20230203 20:50:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -102.62
[32m[20230203 20:50:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -21.22
[32m[20230203 20:50:54 @agent_ppo2.py:151][0m Total time:       0.32 min
[32m[20230203 20:50:54 @agent_ppo2.py:153][0m 24576 total steps have happened
[32m[20230203 20:50:54 @agent_ppo2.py:129][0m #------------------------ Iteration 12 --------------------------#
[32m[20230203 20:50:55 @agent_ppo2.py:135][0m Sampling time: 0.63 s by 1 slaves
[32m[20230203 20:50:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:55 @agent_ppo2.py:193][0m |           0.0038 |          22.1778 |           1.9234 |
[32m[20230203 20:50:55 @agent_ppo2.py:193][0m |          -0.0059 |          15.9293 |           1.9227 |
[32m[20230203 20:50:55 @agent_ppo2.py:193][0m |          -0.0080 |          14.7463 |           1.9213 |
[32m[20230203 20:50:55 @agent_ppo2.py:193][0m |          -0.0091 |          14.0504 |           1.9196 |
[32m[20230203 20:50:55 @agent_ppo2.py:193][0m |          -0.0104 |          13.5556 |           1.9174 |
[32m[20230203 20:50:55 @agent_ppo2.py:193][0m |          -0.0135 |          13.2491 |           1.9157 |
[32m[20230203 20:50:55 @agent_ppo2.py:193][0m |          -0.0148 |          12.8669 |           1.9144 |
[32m[20230203 20:50:55 @agent_ppo2.py:193][0m |          -0.0149 |          12.5286 |           1.9129 |
[32m[20230203 20:50:55 @agent_ppo2.py:193][0m |          -0.0146 |          12.2832 |           1.9122 |
[32m[20230203 20:50:55 @agent_ppo2.py:193][0m |          -0.0119 |          12.7047 |           1.9116 |
[32m[20230203 20:50:55 @agent_ppo2.py:138][0m Policy update time: 0.64 s
[32m[20230203 20:50:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: -113.35
[32m[20230203 20:50:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -101.58
[32m[20230203 20:50:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -34.97
[32m[20230203 20:50:56 @agent_ppo2.py:151][0m Total time:       0.35 min
[32m[20230203 20:50:56 @agent_ppo2.py:153][0m 26624 total steps have happened
[32m[20230203 20:50:56 @agent_ppo2.py:129][0m #------------------------ Iteration 13 --------------------------#
[32m[20230203 20:50:57 @agent_ppo2.py:135][0m Sampling time: 0.61 s by 1 slaves
[32m[20230203 20:50:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:57 @agent_ppo2.py:193][0m |          -0.0001 |           7.7014 |           1.9726 |
[32m[20230203 20:50:57 @agent_ppo2.py:193][0m |          -0.0021 |           4.8503 |           1.9722 |
[32m[20230203 20:50:57 @agent_ppo2.py:193][0m |          -0.0040 |           4.1387 |           1.9716 |
[32m[20230203 20:50:57 @agent_ppo2.py:193][0m |          -0.0058 |           3.7429 |           1.9715 |
[32m[20230203 20:50:57 @agent_ppo2.py:193][0m |          -0.0070 |           3.5611 |           1.9702 |
[32m[20230203 20:50:57 @agent_ppo2.py:193][0m |          -0.0077 |           3.4193 |           1.9698 |
[32m[20230203 20:50:57 @agent_ppo2.py:193][0m |          -0.0084 |           3.3165 |           1.9696 |
[32m[20230203 20:50:57 @agent_ppo2.py:193][0m |          -0.0091 |           3.2674 |           1.9699 |
[32m[20230203 20:50:57 @agent_ppo2.py:193][0m |          -0.0094 |           3.2101 |           1.9695 |
[32m[20230203 20:50:57 @agent_ppo2.py:193][0m |          -0.0098 |           3.1938 |           1.9697 |
[32m[20230203 20:50:57 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:50:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: -107.93
[32m[20230203 20:50:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -106.67
[32m[20230203 20:50:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -45.63
[32m[20230203 20:50:58 @agent_ppo2.py:151][0m Total time:       0.38 min
[32m[20230203 20:50:58 @agent_ppo2.py:153][0m 28672 total steps have happened
[32m[20230203 20:50:58 @agent_ppo2.py:129][0m #------------------------ Iteration 14 --------------------------#
[32m[20230203 20:50:59 @agent_ppo2.py:135][0m Sampling time: 0.63 s by 1 slaves
[32m[20230203 20:50:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:50:59 @agent_ppo2.py:193][0m |           0.0011 |           7.2396 |           1.9297 |
[32m[20230203 20:50:59 @agent_ppo2.py:193][0m |          -0.0043 |           6.1811 |           1.9288 |
[32m[20230203 20:50:59 @agent_ppo2.py:193][0m |          -0.0042 |           5.7000 |           1.9283 |
[32m[20230203 20:50:59 @agent_ppo2.py:193][0m |          -0.0053 |           5.4206 |           1.9271 |
[32m[20230203 20:50:59 @agent_ppo2.py:193][0m |          -0.0072 |           5.3083 |           1.9253 |
[32m[20230203 20:50:59 @agent_ppo2.py:193][0m |          -0.0076 |           5.1196 |           1.9245 |
[32m[20230203 20:50:59 @agent_ppo2.py:193][0m |          -0.0079 |           5.0216 |           1.9242 |
[32m[20230203 20:50:59 @agent_ppo2.py:193][0m |          -0.0075 |           4.9335 |           1.9236 |
[32m[20230203 20:50:59 @agent_ppo2.py:193][0m |          -0.0033 |           4.7511 |           1.9235 |
[32m[20230203 20:50:59 @agent_ppo2.py:193][0m |          -0.0121 |           4.6303 |           1.9235 |
[32m[20230203 20:50:59 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:51:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: -101.31
[32m[20230203 20:51:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -98.94
[32m[20230203 20:51:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -45.33
[32m[20230203 20:51:00 @agent_ppo2.py:151][0m Total time:       0.41 min
[32m[20230203 20:51:00 @agent_ppo2.py:153][0m 30720 total steps have happened
[32m[20230203 20:51:00 @agent_ppo2.py:129][0m #------------------------ Iteration 15 --------------------------#
[32m[20230203 20:51:01 @agent_ppo2.py:135][0m Sampling time: 0.62 s by 1 slaves
[32m[20230203 20:51:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:01 @agent_ppo2.py:193][0m |           0.0097 |           8.7912 |           1.9179 |
[32m[20230203 20:51:01 @agent_ppo2.py:193][0m |           0.0039 |           6.8625 |           1.9185 |
[32m[20230203 20:51:01 @agent_ppo2.py:193][0m |          -0.0178 |           6.2245 |           1.9189 |
[32m[20230203 20:51:01 @agent_ppo2.py:193][0m |          -0.0053 |           5.9108 |           1.9182 |
[32m[20230203 20:51:01 @agent_ppo2.py:193][0m |          -0.0129 |           5.7658 |           1.9179 |
[32m[20230203 20:51:01 @agent_ppo2.py:193][0m |          -0.0044 |           5.5606 |           1.9177 |
[32m[20230203 20:51:01 @agent_ppo2.py:193][0m |          -0.0066 |           5.3931 |           1.9175 |
[32m[20230203 20:51:01 @agent_ppo2.py:193][0m |          -0.0086 |           5.2419 |           1.9174 |
[32m[20230203 20:51:01 @agent_ppo2.py:193][0m |          -0.0090 |           5.1198 |           1.9174 |
[32m[20230203 20:51:01 @agent_ppo2.py:193][0m |           0.0062 |           5.1609 |           1.9175 |
[32m[20230203 20:51:01 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:51:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: -98.37
[32m[20230203 20:51:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -92.85
[32m[20230203 20:51:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -57.12
[32m[20230203 20:51:02 @agent_ppo2.py:151][0m Total time:       0.44 min
[32m[20230203 20:51:02 @agent_ppo2.py:153][0m 32768 total steps have happened
[32m[20230203 20:51:02 @agent_ppo2.py:129][0m #------------------------ Iteration 16 --------------------------#
[32m[20230203 20:51:02 @agent_ppo2.py:135][0m Sampling time: 0.61 s by 1 slaves
[32m[20230203 20:51:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:03 @agent_ppo2.py:193][0m |           0.0006 |           2.6452 |           2.0050 |
[32m[20230203 20:51:03 @agent_ppo2.py:193][0m |          -0.0005 |           1.9383 |           2.0043 |
[32m[20230203 20:51:03 @agent_ppo2.py:193][0m |          -0.0015 |           1.8084 |           2.0034 |
[32m[20230203 20:51:03 @agent_ppo2.py:193][0m |          -0.0027 |           1.7325 |           2.0021 |
[32m[20230203 20:51:03 @agent_ppo2.py:193][0m |          -0.0032 |           1.6813 |           2.0012 |
[32m[20230203 20:51:03 @agent_ppo2.py:193][0m |          -0.0034 |           1.6667 |           2.0007 |
[32m[20230203 20:51:03 @agent_ppo2.py:193][0m |          -0.0039 |           1.6733 |           2.0013 |
[32m[20230203 20:51:03 @agent_ppo2.py:193][0m |          -0.0041 |           1.6291 |           2.0022 |
[32m[20230203 20:51:03 @agent_ppo2.py:193][0m |          -0.0041 |           1.5930 |           2.0028 |
[32m[20230203 20:51:03 @agent_ppo2.py:193][0m |          -0.0045 |           1.5909 |           2.0040 |
[32m[20230203 20:51:03 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:51:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: -97.77
[32m[20230203 20:51:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -95.27
[32m[20230203 20:51:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -50.14
[32m[20230203 20:51:04 @agent_ppo2.py:151][0m Total time:       0.47 min
[32m[20230203 20:51:04 @agent_ppo2.py:153][0m 34816 total steps have happened
[32m[20230203 20:51:04 @agent_ppo2.py:129][0m #------------------------ Iteration 17 --------------------------#
[32m[20230203 20:51:04 @agent_ppo2.py:135][0m Sampling time: 0.65 s by 1 slaves
[32m[20230203 20:51:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:04 @agent_ppo2.py:193][0m |          -0.0004 |          16.7524 |           1.9915 |
[32m[20230203 20:51:05 @agent_ppo2.py:193][0m |          -0.0020 |          13.3636 |           1.9898 |
[32m[20230203 20:51:05 @agent_ppo2.py:193][0m |          -0.0050 |          12.1871 |           1.9880 |
[32m[20230203 20:51:05 @agent_ppo2.py:193][0m |          -0.0060 |          11.1967 |           1.9864 |
[32m[20230203 20:51:05 @agent_ppo2.py:193][0m |          -0.0085 |          10.8903 |           1.9847 |
[32m[20230203 20:51:05 @agent_ppo2.py:193][0m |          -0.0091 |          10.0029 |           1.9829 |
[32m[20230203 20:51:05 @agent_ppo2.py:193][0m |          -0.0105 |           9.6185 |           1.9820 |
[32m[20230203 20:51:05 @agent_ppo2.py:193][0m |          -0.0105 |           9.2287 |           1.9813 |
[32m[20230203 20:51:05 @agent_ppo2.py:193][0m |          -0.0113 |           8.8100 |           1.9795 |
[32m[20230203 20:51:05 @agent_ppo2.py:193][0m |          -0.0120 |           8.5852 |           1.9789 |
[32m[20230203 20:51:05 @agent_ppo2.py:138][0m Policy update time: 0.65 s
[32m[20230203 20:51:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: -108.76
[32m[20230203 20:51:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -104.59
[32m[20230203 20:51:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -55.31
[32m[20230203 20:51:06 @agent_ppo2.py:151][0m Total time:       0.51 min
[32m[20230203 20:51:06 @agent_ppo2.py:153][0m 36864 total steps have happened
[32m[20230203 20:51:06 @agent_ppo2.py:129][0m #------------------------ Iteration 18 --------------------------#
[32m[20230203 20:51:06 @agent_ppo2.py:135][0m Sampling time: 0.64 s by 1 slaves
[32m[20230203 20:51:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:06 @agent_ppo2.py:193][0m |          -0.0025 |          12.0981 |           1.9410 |
[32m[20230203 20:51:06 @agent_ppo2.py:193][0m |          -0.0051 |           8.2526 |           1.9396 |
[32m[20230203 20:51:07 @agent_ppo2.py:193][0m |          -0.0057 |           7.2331 |           1.9382 |
[32m[20230203 20:51:07 @agent_ppo2.py:193][0m |           0.0021 |           6.8595 |           1.9368 |
[32m[20230203 20:51:07 @agent_ppo2.py:193][0m |          -0.0086 |           6.5106 |           1.9351 |
[32m[20230203 20:51:07 @agent_ppo2.py:193][0m |          -0.0130 |           6.0035 |           1.9344 |
[32m[20230203 20:51:07 @agent_ppo2.py:193][0m |          -0.0095 |           5.7810 |           1.9333 |
[32m[20230203 20:51:07 @agent_ppo2.py:193][0m |          -0.0108 |           5.4999 |           1.9325 |
[32m[20230203 20:51:07 @agent_ppo2.py:193][0m |          -0.0109 |           5.3391 |           1.9318 |
[32m[20230203 20:51:07 @agent_ppo2.py:193][0m |          -0.0237 |           5.3243 |           1.9311 |
[32m[20230203 20:51:07 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230203 20:51:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: -102.12
[32m[20230203 20:51:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -96.39
[32m[20230203 20:51:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -59.20
[32m[20230203 20:51:07 @agent_ppo2.py:151][0m Total time:       0.54 min
[32m[20230203 20:51:07 @agent_ppo2.py:153][0m 38912 total steps have happened
[32m[20230203 20:51:07 @agent_ppo2.py:129][0m #------------------------ Iteration 19 --------------------------#
[32m[20230203 20:51:08 @agent_ppo2.py:135][0m Sampling time: 0.63 s by 1 slaves
[32m[20230203 20:51:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:08 @agent_ppo2.py:193][0m |          -0.0001 |           2.5191 |           1.9010 |
[32m[20230203 20:51:08 @agent_ppo2.py:193][0m |          -0.0022 |           1.6446 |           1.8990 |
[32m[20230203 20:51:08 @agent_ppo2.py:193][0m |          -0.0043 |           1.5032 |           1.8970 |
[32m[20230203 20:51:08 @agent_ppo2.py:193][0m |          -0.0056 |           1.4306 |           1.8947 |
[32m[20230203 20:51:08 @agent_ppo2.py:193][0m |          -0.0064 |           1.3791 |           1.8927 |
[32m[20230203 20:51:09 @agent_ppo2.py:193][0m |          -0.0070 |           1.3478 |           1.8922 |
[32m[20230203 20:51:09 @agent_ppo2.py:193][0m |          -0.0072 |           1.3212 |           1.8916 |
[32m[20230203 20:51:09 @agent_ppo2.py:193][0m |          -0.0074 |           1.2968 |           1.8910 |
[32m[20230203 20:51:09 @agent_ppo2.py:193][0m |          -0.0079 |           1.2837 |           1.8906 |
[32m[20230203 20:51:09 @agent_ppo2.py:193][0m |          -0.0083 |           1.2645 |           1.8900 |
[32m[20230203 20:51:09 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:51:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: -97.22
[32m[20230203 20:51:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -94.23
[32m[20230203 20:51:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -63.10
[32m[20230203 20:51:09 @agent_ppo2.py:151][0m Total time:       0.57 min
[32m[20230203 20:51:09 @agent_ppo2.py:153][0m 40960 total steps have happened
[32m[20230203 20:51:09 @agent_ppo2.py:129][0m #------------------------ Iteration 20 --------------------------#
[32m[20230203 20:51:10 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:51:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:10 @agent_ppo2.py:193][0m |           0.0225 |          49.5262 |           1.8522 |
[32m[20230203 20:51:10 @agent_ppo2.py:193][0m |          -0.0037 |          27.9919 |           1.8498 |
[32m[20230203 20:51:10 @agent_ppo2.py:193][0m |          -0.0033 |          23.7376 |           1.8465 |
[32m[20230203 20:51:10 @agent_ppo2.py:193][0m |          -0.0174 |          20.4325 |           1.8454 |
[32m[20230203 20:51:10 @agent_ppo2.py:193][0m |          -0.0099 |          18.7684 |           1.8444 |
[32m[20230203 20:51:10 @agent_ppo2.py:193][0m |          -0.0116 |          17.3308 |           1.8431 |
[32m[20230203 20:51:10 @agent_ppo2.py:193][0m |          -0.0106 |          15.4037 |           1.8419 |
[32m[20230203 20:51:10 @agent_ppo2.py:193][0m |          -0.0075 |          15.0332 |           1.8409 |
[32m[20230203 20:51:10 @agent_ppo2.py:193][0m |          -0.0217 |          13.4816 |           1.8401 |
[32m[20230203 20:51:10 @agent_ppo2.py:193][0m |          -0.0095 |          13.3851 |           1.8393 |
[32m[20230203 20:51:10 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 20:51:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: -105.01
[32m[20230203 20:51:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -89.55
[32m[20230203 20:51:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -61.98
[32m[20230203 20:51:11 @agent_ppo2.py:151][0m Total time:       0.59 min
[32m[20230203 20:51:11 @agent_ppo2.py:153][0m 43008 total steps have happened
[32m[20230203 20:51:11 @agent_ppo2.py:129][0m #------------------------ Iteration 21 --------------------------#
[32m[20230203 20:51:11 @agent_ppo2.py:135][0m Sampling time: 0.61 s by 1 slaves
[32m[20230203 20:51:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:12 @agent_ppo2.py:193][0m |          -0.0004 |          10.4687 |           1.9001 |
[32m[20230203 20:51:12 @agent_ppo2.py:193][0m |          -0.0025 |           4.9666 |           1.8991 |
[32m[20230203 20:51:12 @agent_ppo2.py:193][0m |          -0.0033 |           4.0777 |           1.8984 |
[32m[20230203 20:51:12 @agent_ppo2.py:193][0m |          -0.0041 |           3.5500 |           1.8983 |
[32m[20230203 20:51:12 @agent_ppo2.py:193][0m |          -0.0044 |           3.3148 |           1.8983 |
[32m[20230203 20:51:12 @agent_ppo2.py:193][0m |          -0.0051 |           3.0903 |           1.8982 |
[32m[20230203 20:51:12 @agent_ppo2.py:193][0m |          -0.0055 |           2.9021 |           1.8982 |
[32m[20230203 20:51:12 @agent_ppo2.py:193][0m |          -0.0059 |           2.7262 |           1.8982 |
[32m[20230203 20:51:12 @agent_ppo2.py:193][0m |          -0.0064 |           2.5892 |           1.8984 |
[32m[20230203 20:51:12 @agent_ppo2.py:193][0m |          -0.0071 |           2.5343 |           1.8981 |
[32m[20230203 20:51:12 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:51:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: -90.87
[32m[20230203 20:51:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -84.91
[32m[20230203 20:51:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -68.00
[32m[20230203 20:51:13 @agent_ppo2.py:151][0m Total time:       0.62 min
[32m[20230203 20:51:13 @agent_ppo2.py:153][0m 45056 total steps have happened
[32m[20230203 20:51:13 @agent_ppo2.py:129][0m #------------------------ Iteration 22 --------------------------#
[32m[20230203 20:51:13 @agent_ppo2.py:135][0m Sampling time: 0.62 s by 1 slaves
[32m[20230203 20:51:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:13 @agent_ppo2.py:193][0m |          -0.0002 |           1.9686 |           1.9427 |
[32m[20230203 20:51:13 @agent_ppo2.py:193][0m |          -0.0031 |           1.6505 |           1.9435 |
[32m[20230203 20:51:13 @agent_ppo2.py:193][0m |          -0.0050 |           1.5143 |           1.9429 |
[32m[20230203 20:51:13 @agent_ppo2.py:193][0m |          -0.0070 |           1.4320 |           1.9435 |
[32m[20230203 20:51:14 @agent_ppo2.py:193][0m |          -0.0079 |           1.3856 |           1.9436 |
[32m[20230203 20:51:14 @agent_ppo2.py:193][0m |          -0.0087 |           1.3591 |           1.9439 |
[32m[20230203 20:51:14 @agent_ppo2.py:193][0m |          -0.0093 |           1.3295 |           1.9444 |
[32m[20230203 20:51:14 @agent_ppo2.py:193][0m |          -0.0103 |           1.3106 |           1.9463 |
[32m[20230203 20:51:14 @agent_ppo2.py:193][0m |          -0.0110 |           1.2968 |           1.9470 |
[32m[20230203 20:51:14 @agent_ppo2.py:193][0m |          -0.0114 |           1.2839 |           1.9481 |
[32m[20230203 20:51:14 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:51:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: -96.14
[32m[20230203 20:51:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -91.43
[32m[20230203 20:51:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -77.87
[32m[20230203 20:51:14 @agent_ppo2.py:151][0m Total time:       0.65 min
[32m[20230203 20:51:14 @agent_ppo2.py:153][0m 47104 total steps have happened
[32m[20230203 20:51:14 @agent_ppo2.py:129][0m #------------------------ Iteration 23 --------------------------#
[32m[20230203 20:51:15 @agent_ppo2.py:135][0m Sampling time: 0.62 s by 1 slaves
[32m[20230203 20:51:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:15 @agent_ppo2.py:193][0m |           0.0007 |          19.8887 |           1.9922 |
[32m[20230203 20:51:15 @agent_ppo2.py:193][0m |          -0.0009 |           7.9849 |           1.9917 |
[32m[20230203 20:51:15 @agent_ppo2.py:193][0m |          -0.0018 |           3.4081 |           1.9912 |
[32m[20230203 20:51:15 @agent_ppo2.py:193][0m |          -0.0030 |           2.4932 |           1.9902 |
[32m[20230203 20:51:15 @agent_ppo2.py:193][0m |          -0.0039 |           2.1886 |           1.9896 |
[32m[20230203 20:51:15 @agent_ppo2.py:193][0m |          -0.0043 |           2.0137 |           1.9894 |
[32m[20230203 20:51:15 @agent_ppo2.py:193][0m |          -0.0050 |           1.9505 |           1.9887 |
[32m[20230203 20:51:16 @agent_ppo2.py:193][0m |          -0.0055 |           1.8479 |           1.9873 |
[32m[20230203 20:51:16 @agent_ppo2.py:193][0m |          -0.0060 |           1.7864 |           1.9874 |
[32m[20230203 20:51:16 @agent_ppo2.py:193][0m |          -0.0053 |           1.7546 |           1.9868 |
[32m[20230203 20:51:16 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:51:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: -96.24
[32m[20230203 20:51:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -86.64
[32m[20230203 20:51:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -83.48
[32m[20230203 20:51:16 @agent_ppo2.py:151][0m Total time:       0.68 min
[32m[20230203 20:51:16 @agent_ppo2.py:153][0m 49152 total steps have happened
[32m[20230203 20:51:16 @agent_ppo2.py:129][0m #------------------------ Iteration 24 --------------------------#
[32m[20230203 20:51:17 @agent_ppo2.py:135][0m Sampling time: 0.62 s by 1 slaves
[32m[20230203 20:51:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:17 @agent_ppo2.py:193][0m |          -0.0036 |          12.7447 |           1.9210 |
[32m[20230203 20:51:17 @agent_ppo2.py:193][0m |          -0.0050 |           4.5404 |           1.9206 |
[32m[20230203 20:51:17 @agent_ppo2.py:193][0m |          -0.0068 |           2.7983 |           1.9206 |
[32m[20230203 20:51:17 @agent_ppo2.py:193][0m |          -0.0075 |           2.4594 |           1.9210 |
[32m[20230203 20:51:17 @agent_ppo2.py:193][0m |          -0.0086 |           2.2168 |           1.9212 |
[32m[20230203 20:51:17 @agent_ppo2.py:193][0m |          -0.0116 |           2.0900 |           1.9209 |
[32m[20230203 20:51:17 @agent_ppo2.py:193][0m |          -0.0101 |           1.9898 |           1.9210 |
[32m[20230203 20:51:17 @agent_ppo2.py:193][0m |          -0.0099 |           1.9384 |           1.9199 |
[32m[20230203 20:51:17 @agent_ppo2.py:193][0m |          -0.0106 |           1.8647 |           1.9208 |
[32m[20230203 20:51:17 @agent_ppo2.py:193][0m |          -0.0125 |           1.8190 |           1.9205 |
[32m[20230203 20:51:17 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:51:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: -96.63
[32m[20230203 20:51:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -86.14
[32m[20230203 20:51:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -84.26
[32m[20230203 20:51:18 @agent_ppo2.py:151][0m Total time:       0.71 min
[32m[20230203 20:51:18 @agent_ppo2.py:153][0m 51200 total steps have happened
[32m[20230203 20:51:18 @agent_ppo2.py:129][0m #------------------------ Iteration 25 --------------------------#
[32m[20230203 20:51:19 @agent_ppo2.py:135][0m Sampling time: 0.62 s by 1 slaves
[32m[20230203 20:51:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:19 @agent_ppo2.py:193][0m |          -0.0000 |          11.3602 |           1.9814 |
[32m[20230203 20:51:19 @agent_ppo2.py:193][0m |          -0.0020 |           5.1593 |           1.9809 |
[32m[20230203 20:51:19 @agent_ppo2.py:193][0m |          -0.0060 |           4.1934 |           1.9806 |
[32m[20230203 20:51:19 @agent_ppo2.py:193][0m |          -0.0059 |           3.7278 |           1.9790 |
[32m[20230203 20:51:19 @agent_ppo2.py:193][0m |          -0.0078 |           3.4447 |           1.9782 |
[32m[20230203 20:51:19 @agent_ppo2.py:193][0m |          -0.0085 |           3.2361 |           1.9770 |
[32m[20230203 20:51:19 @agent_ppo2.py:193][0m |          -0.0100 |           3.1066 |           1.9758 |
[32m[20230203 20:51:19 @agent_ppo2.py:193][0m |          -0.0073 |           2.9353 |           1.9753 |
[32m[20230203 20:51:19 @agent_ppo2.py:193][0m |          -0.0065 |           2.8656 |           1.9750 |
[32m[20230203 20:51:19 @agent_ppo2.py:193][0m |          -0.0099 |           2.7119 |           1.9744 |
[32m[20230203 20:51:19 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:51:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: -100.92
[32m[20230203 20:51:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -89.11
[32m[20230203 20:51:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -86.43
[32m[20230203 20:51:20 @agent_ppo2.py:151][0m Total time:       0.74 min
[32m[20230203 20:51:20 @agent_ppo2.py:153][0m 53248 total steps have happened
[32m[20230203 20:51:20 @agent_ppo2.py:129][0m #------------------------ Iteration 26 --------------------------#
[32m[20230203 20:51:20 @agent_ppo2.py:135][0m Sampling time: 0.62 s by 1 slaves
[32m[20230203 20:51:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:21 @agent_ppo2.py:193][0m |           0.0038 |           8.1924 |           1.9566 |
[32m[20230203 20:51:21 @agent_ppo2.py:193][0m |          -0.0038 |           4.5332 |           1.9572 |
[32m[20230203 20:51:21 @agent_ppo2.py:193][0m |          -0.0062 |           3.6994 |           1.9579 |
[32m[20230203 20:51:21 @agent_ppo2.py:193][0m |          -0.0063 |           3.2146 |           1.9587 |
[32m[20230203 20:51:21 @agent_ppo2.py:193][0m |          -0.0046 |           2.8790 |           1.9591 |
[32m[20230203 20:51:21 @agent_ppo2.py:193][0m |          -0.0062 |           2.7318 |           1.9601 |
[32m[20230203 20:51:21 @agent_ppo2.py:193][0m |          -0.0065 |           2.5105 |           1.9594 |
[32m[20230203 20:51:21 @agent_ppo2.py:193][0m |          -0.0084 |           2.4385 |           1.9605 |
[32m[20230203 20:51:21 @agent_ppo2.py:193][0m |          -0.0090 |           2.2969 |           1.9604 |
[32m[20230203 20:51:21 @agent_ppo2.py:193][0m |          -0.0084 |           2.2170 |           1.9609 |
[32m[20230203 20:51:21 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:51:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: -98.95
[32m[20230203 20:51:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -90.15
[32m[20230203 20:51:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -100.11
[32m[20230203 20:51:22 @agent_ppo2.py:151][0m Total time:       0.77 min
[32m[20230203 20:51:22 @agent_ppo2.py:153][0m 55296 total steps have happened
[32m[20230203 20:51:22 @agent_ppo2.py:129][0m #------------------------ Iteration 27 --------------------------#
[32m[20230203 20:51:22 @agent_ppo2.py:135][0m Sampling time: 0.60 s by 1 slaves
[32m[20230203 20:51:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:22 @agent_ppo2.py:193][0m |          -0.0002 |           3.3822 |           1.9541 |
[32m[20230203 20:51:22 @agent_ppo2.py:193][0m |          -0.0028 |           1.7066 |           1.9546 |
[32m[20230203 20:51:22 @agent_ppo2.py:193][0m |          -0.0038 |           1.4633 |           1.9550 |
[32m[20230203 20:51:22 @agent_ppo2.py:193][0m |          -0.0046 |           1.3641 |           1.9550 |
[32m[20230203 20:51:23 @agent_ppo2.py:193][0m |          -0.0054 |           1.2844 |           1.9547 |
[32m[20230203 20:51:23 @agent_ppo2.py:193][0m |          -0.0053 |           1.2516 |           1.9551 |
[32m[20230203 20:51:23 @agent_ppo2.py:193][0m |          -0.0057 |           1.2136 |           1.9549 |
[32m[20230203 20:51:23 @agent_ppo2.py:193][0m |          -0.0061 |           1.1874 |           1.9554 |
[32m[20230203 20:51:23 @agent_ppo2.py:193][0m |          -0.0065 |           1.1696 |           1.9561 |
[32m[20230203 20:51:23 @agent_ppo2.py:193][0m |          -0.0066 |           1.1562 |           1.9566 |
[32m[20230203 20:51:23 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:51:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: -86.28
[32m[20230203 20:51:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -78.24
[32m[20230203 20:51:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -88.88
[32m[20230203 20:51:23 @agent_ppo2.py:151][0m Total time:       0.80 min
[32m[20230203 20:51:23 @agent_ppo2.py:153][0m 57344 total steps have happened
[32m[20230203 20:51:23 @agent_ppo2.py:129][0m #------------------------ Iteration 28 --------------------------#
[32m[20230203 20:51:24 @agent_ppo2.py:135][0m Sampling time: 0.60 s by 1 slaves
[32m[20230203 20:51:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:24 @agent_ppo2.py:193][0m |           0.0007 |           1.7436 |           1.9962 |
[32m[20230203 20:51:24 @agent_ppo2.py:193][0m |          -0.0021 |           1.4086 |           1.9962 |
[32m[20230203 20:51:24 @agent_ppo2.py:193][0m |          -0.0034 |           1.2632 |           1.9946 |
[32m[20230203 20:51:24 @agent_ppo2.py:193][0m |          -0.0045 |           1.2235 |           1.9936 |
[32m[20230203 20:51:24 @agent_ppo2.py:193][0m |          -0.0053 |           1.2101 |           1.9949 |
[32m[20230203 20:51:24 @agent_ppo2.py:193][0m |          -0.0057 |           1.1660 |           1.9953 |
[32m[20230203 20:51:24 @agent_ppo2.py:193][0m |          -0.0063 |           1.1458 |           1.9951 |
[32m[20230203 20:51:24 @agent_ppo2.py:193][0m |          -0.0068 |           1.1349 |           1.9964 |
[32m[20230203 20:51:24 @agent_ppo2.py:193][0m |          -0.0071 |           1.1195 |           1.9964 |
[32m[20230203 20:51:25 @agent_ppo2.py:193][0m |          -0.0076 |           1.1120 |           1.9978 |
[32m[20230203 20:51:25 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:51:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: -84.26
[32m[20230203 20:51:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -81.65
[32m[20230203 20:51:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -111.53
[32m[20230203 20:51:25 @agent_ppo2.py:151][0m Total time:       0.83 min
[32m[20230203 20:51:25 @agent_ppo2.py:153][0m 59392 total steps have happened
[32m[20230203 20:51:25 @agent_ppo2.py:129][0m #------------------------ Iteration 29 --------------------------#
[32m[20230203 20:51:26 @agent_ppo2.py:135][0m Sampling time: 0.61 s by 1 slaves
[32m[20230203 20:51:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:26 @agent_ppo2.py:193][0m |           0.0014 |           5.0089 |           2.0460 |
[32m[20230203 20:51:26 @agent_ppo2.py:193][0m |           0.0003 |           2.7582 |           2.0466 |
[32m[20230203 20:51:26 @agent_ppo2.py:193][0m |          -0.0022 |           2.5030 |           2.0446 |
[32m[20230203 20:51:26 @agent_ppo2.py:193][0m |          -0.0039 |           2.3567 |           2.0433 |
[32m[20230203 20:51:26 @agent_ppo2.py:193][0m |          -0.0047 |           2.2015 |           2.0419 |
[32m[20230203 20:51:26 @agent_ppo2.py:193][0m |          -0.0058 |           2.0797 |           2.0407 |
[32m[20230203 20:51:26 @agent_ppo2.py:193][0m |          -0.0050 |           2.0288 |           2.0399 |
[32m[20230203 20:51:26 @agent_ppo2.py:193][0m |          -0.0059 |           1.9528 |           2.0390 |
[32m[20230203 20:51:26 @agent_ppo2.py:193][0m |          -0.0058 |           1.8740 |           2.0380 |
[32m[20230203 20:51:26 @agent_ppo2.py:193][0m |          -0.0068 |           1.8127 |           2.0375 |
[32m[20230203 20:51:26 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:51:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: -100.65
[32m[20230203 20:51:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -83.56
[32m[20230203 20:51:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -93.43
[32m[20230203 20:51:27 @agent_ppo2.py:151][0m Total time:       0.86 min
[32m[20230203 20:51:27 @agent_ppo2.py:153][0m 61440 total steps have happened
[32m[20230203 20:51:27 @agent_ppo2.py:129][0m #------------------------ Iteration 30 --------------------------#
[32m[20230203 20:51:27 @agent_ppo2.py:135][0m Sampling time: 0.64 s by 1 slaves
[32m[20230203 20:51:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:28 @agent_ppo2.py:193][0m |          -0.0008 |          16.1148 |           2.0077 |
[32m[20230203 20:51:28 @agent_ppo2.py:193][0m |          -0.0032 |           6.7952 |           2.0064 |
[32m[20230203 20:51:28 @agent_ppo2.py:193][0m |          -0.0055 |           5.7091 |           2.0046 |
[32m[20230203 20:51:28 @agent_ppo2.py:193][0m |          -0.0079 |           5.1462 |           2.0030 |
[32m[20230203 20:51:28 @agent_ppo2.py:193][0m |          -0.0058 |           4.9152 |           2.0012 |
[32m[20230203 20:51:28 @agent_ppo2.py:193][0m |          -0.0084 |           4.4878 |           2.0002 |
[32m[20230203 20:51:28 @agent_ppo2.py:193][0m |          -0.0082 |           3.9514 |           1.9987 |
[32m[20230203 20:51:28 @agent_ppo2.py:193][0m |          -0.0085 |           3.9855 |           1.9979 |
[32m[20230203 20:51:28 @agent_ppo2.py:193][0m |          -0.0100 |           4.0711 |           1.9970 |
[32m[20230203 20:51:28 @agent_ppo2.py:193][0m |          -0.0104 |           3.5574 |           1.9956 |
[32m[20230203 20:51:28 @agent_ppo2.py:138][0m Policy update time: 0.66 s
[32m[20230203 20:51:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: -103.38
[32m[20230203 20:51:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -88.18
[32m[20230203 20:51:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -105.98
[32m[20230203 20:51:29 @agent_ppo2.py:151][0m Total time:       0.89 min
[32m[20230203 20:51:29 @agent_ppo2.py:153][0m 63488 total steps have happened
[32m[20230203 20:51:29 @agent_ppo2.py:129][0m #------------------------ Iteration 31 --------------------------#
[32m[20230203 20:51:29 @agent_ppo2.py:135][0m Sampling time: 0.62 s by 1 slaves
[32m[20230203 20:51:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:29 @agent_ppo2.py:193][0m |          -0.0008 |          13.2583 |           1.9865 |
[32m[20230203 20:51:29 @agent_ppo2.py:193][0m |          -0.0059 |           8.4716 |           1.9829 |
[32m[20230203 20:51:29 @agent_ppo2.py:193][0m |          -0.0071 |           7.4000 |           1.9798 |
[32m[20230203 20:51:30 @agent_ppo2.py:193][0m |          -0.0090 |           6.9412 |           1.9787 |
[32m[20230203 20:51:30 @agent_ppo2.py:193][0m |          -0.0081 |           6.5597 |           1.9773 |
[32m[20230203 20:51:30 @agent_ppo2.py:193][0m |          -0.0096 |           6.2201 |           1.9767 |
[32m[20230203 20:51:30 @agent_ppo2.py:193][0m |          -0.0104 |           6.0190 |           1.9751 |
[32m[20230203 20:51:30 @agent_ppo2.py:193][0m |          -0.0104 |           5.8865 |           1.9744 |
[32m[20230203 20:51:30 @agent_ppo2.py:193][0m |          -0.0107 |           5.7179 |           1.9732 |
[32m[20230203 20:51:30 @agent_ppo2.py:193][0m |          -0.0117 |           5.6299 |           1.9729 |
[32m[20230203 20:51:30 @agent_ppo2.py:138][0m Policy update time: 0.64 s
[32m[20230203 20:51:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: -99.37
[32m[20230203 20:51:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -74.72
[32m[20230203 20:51:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -101.64
[32m[20230203 20:51:30 @agent_ppo2.py:151][0m Total time:       0.92 min
[32m[20230203 20:51:30 @agent_ppo2.py:153][0m 65536 total steps have happened
[32m[20230203 20:51:30 @agent_ppo2.py:129][0m #------------------------ Iteration 32 --------------------------#
[32m[20230203 20:51:31 @agent_ppo2.py:135][0m Sampling time: 0.58 s by 1 slaves
[32m[20230203 20:51:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:31 @agent_ppo2.py:193][0m |          -0.0007 |           3.4875 |           1.9107 |
[32m[20230203 20:51:31 @agent_ppo2.py:193][0m |          -0.0042 |           2.2372 |           1.9090 |
[32m[20230203 20:51:31 @agent_ppo2.py:193][0m |          -0.0063 |           1.9823 |           1.9066 |
[32m[20230203 20:51:31 @agent_ppo2.py:193][0m |          -0.0074 |           1.9005 |           1.9052 |
[32m[20230203 20:51:31 @agent_ppo2.py:193][0m |          -0.0083 |           1.8350 |           1.9027 |
[32m[20230203 20:51:31 @agent_ppo2.py:193][0m |          -0.0086 |           1.8093 |           1.9025 |
[32m[20230203 20:51:31 @agent_ppo2.py:193][0m |          -0.0094 |           1.7767 |           1.9023 |
[32m[20230203 20:51:31 @agent_ppo2.py:193][0m |          -0.0096 |           1.7609 |           1.9017 |
[32m[20230203 20:51:32 @agent_ppo2.py:193][0m |          -0.0104 |           1.7264 |           1.9013 |
[32m[20230203 20:51:32 @agent_ppo2.py:193][0m |          -0.0109 |           1.7115 |           1.9012 |
[32m[20230203 20:51:32 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:51:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: -92.89
[32m[20230203 20:51:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -87.09
[32m[20230203 20:51:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -84.67
[32m[20230203 20:51:32 @agent_ppo2.py:151][0m Total time:       0.95 min
[32m[20230203 20:51:32 @agent_ppo2.py:153][0m 67584 total steps have happened
[32m[20230203 20:51:32 @agent_ppo2.py:129][0m #------------------------ Iteration 33 --------------------------#
[32m[20230203 20:51:33 @agent_ppo2.py:135][0m Sampling time: 0.58 s by 1 slaves
[32m[20230203 20:51:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:33 @agent_ppo2.py:193][0m |           0.0007 |           2.1236 |           2.0018 |
[32m[20230203 20:51:33 @agent_ppo2.py:193][0m |          -0.0012 |           1.3914 |           2.0008 |
[32m[20230203 20:51:33 @agent_ppo2.py:193][0m |          -0.0028 |           1.2955 |           1.9999 |
[32m[20230203 20:51:33 @agent_ppo2.py:193][0m |          -0.0040 |           1.2430 |           1.9998 |
[32m[20230203 20:51:33 @agent_ppo2.py:193][0m |          -0.0050 |           1.2182 |           1.9984 |
[32m[20230203 20:51:33 @agent_ppo2.py:193][0m |          -0.0063 |           1.1967 |           1.9988 |
[32m[20230203 20:51:33 @agent_ppo2.py:193][0m |          -0.0059 |           1.1852 |           1.9987 |
[32m[20230203 20:51:33 @agent_ppo2.py:193][0m |          -0.0069 |           1.1656 |           1.9973 |
[32m[20230203 20:51:33 @agent_ppo2.py:193][0m |          -0.0074 |           1.1538 |           1.9974 |
[32m[20230203 20:51:33 @agent_ppo2.py:193][0m |          -0.0074 |           1.1495 |           1.9982 |
[32m[20230203 20:51:33 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:51:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: -80.51
[32m[20230203 20:51:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -79.18
[32m[20230203 20:51:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -76.47
[32m[20230203 20:51:34 @agent_ppo2.py:151][0m Total time:       0.98 min
[32m[20230203 20:51:34 @agent_ppo2.py:153][0m 69632 total steps have happened
[32m[20230203 20:51:34 @agent_ppo2.py:129][0m #------------------------ Iteration 34 --------------------------#
[32m[20230203 20:51:34 @agent_ppo2.py:135][0m Sampling time: 0.59 s by 1 slaves
[32m[20230203 20:51:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:35 @agent_ppo2.py:193][0m |          -0.0003 |           1.2264 |           1.9994 |
[32m[20230203 20:51:35 @agent_ppo2.py:193][0m |          -0.0032 |           1.0038 |           1.9991 |
[32m[20230203 20:51:35 @agent_ppo2.py:193][0m |          -0.0048 |           0.9561 |           1.9985 |
[32m[20230203 20:51:35 @agent_ppo2.py:193][0m |          -0.0055 |           0.9428 |           1.9985 |
[32m[20230203 20:51:35 @agent_ppo2.py:193][0m |          -0.0062 |           0.9311 |           1.9981 |
[32m[20230203 20:51:35 @agent_ppo2.py:193][0m |          -0.0068 |           0.9261 |           1.9985 |
[32m[20230203 20:51:35 @agent_ppo2.py:193][0m |          -0.0072 |           0.9207 |           1.9983 |
[32m[20230203 20:51:35 @agent_ppo2.py:193][0m |          -0.0074 |           0.9159 |           1.9989 |
[32m[20230203 20:51:35 @agent_ppo2.py:193][0m |          -0.0081 |           0.9113 |           1.9992 |
[32m[20230203 20:51:35 @agent_ppo2.py:193][0m |          -0.0085 |           0.9086 |           1.9994 |
[32m[20230203 20:51:35 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:51:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: -88.17
[32m[20230203 20:51:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -83.94
[32m[20230203 20:51:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -85.47
[32m[20230203 20:51:36 @agent_ppo2.py:151][0m Total time:       1.01 min
[32m[20230203 20:51:36 @agent_ppo2.py:153][0m 71680 total steps have happened
[32m[20230203 20:51:36 @agent_ppo2.py:129][0m #------------------------ Iteration 35 --------------------------#
[32m[20230203 20:51:36 @agent_ppo2.py:135][0m Sampling time: 0.59 s by 1 slaves
[32m[20230203 20:51:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:36 @agent_ppo2.py:193][0m |           0.0003 |           1.1510 |           1.9747 |
[32m[20230203 20:51:36 @agent_ppo2.py:193][0m |          -0.0011 |           0.9945 |           1.9740 |
[32m[20230203 20:51:36 @agent_ppo2.py:193][0m |          -0.0025 |           0.9559 |           1.9724 |
[32m[20230203 20:51:36 @agent_ppo2.py:193][0m |          -0.0033 |           0.9371 |           1.9717 |
[32m[20230203 20:51:37 @agent_ppo2.py:193][0m |          -0.0039 |           0.9195 |           1.9711 |
[32m[20230203 20:51:37 @agent_ppo2.py:193][0m |          -0.0046 |           0.9132 |           1.9721 |
[32m[20230203 20:51:37 @agent_ppo2.py:193][0m |          -0.0049 |           0.9074 |           1.9723 |
[32m[20230203 20:51:37 @agent_ppo2.py:193][0m |          -0.0054 |           0.9025 |           1.9721 |
[32m[20230203 20:51:37 @agent_ppo2.py:193][0m |          -0.0059 |           0.9007 |           1.9735 |
[32m[20230203 20:51:37 @agent_ppo2.py:193][0m |          -0.0061 |           0.8975 |           1.9732 |
[32m[20230203 20:51:37 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:51:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: -81.36
[32m[20230203 20:51:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -80.94
[32m[20230203 20:51:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -56.11
[32m[20230203 20:51:37 @agent_ppo2.py:151][0m Total time:       1.03 min
[32m[20230203 20:51:37 @agent_ppo2.py:153][0m 73728 total steps have happened
[32m[20230203 20:51:37 @agent_ppo2.py:129][0m #------------------------ Iteration 36 --------------------------#
[32m[20230203 20:51:38 @agent_ppo2.py:135][0m Sampling time: 0.59 s by 1 slaves
[32m[20230203 20:51:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:38 @agent_ppo2.py:193][0m |           0.0001 |           1.5349 |           1.9835 |
[32m[20230203 20:51:38 @agent_ppo2.py:193][0m |          -0.0028 |           1.3265 |           1.9833 |
[32m[20230203 20:51:38 @agent_ppo2.py:193][0m |          -0.0049 |           1.2888 |           1.9835 |
[32m[20230203 20:51:38 @agent_ppo2.py:193][0m |          -0.0067 |           1.2405 |           1.9838 |
[32m[20230203 20:51:38 @agent_ppo2.py:193][0m |          -0.0077 |           1.2087 |           1.9849 |
[32m[20230203 20:51:38 @agent_ppo2.py:193][0m |          -0.0080 |           1.1914 |           1.9856 |
[32m[20230203 20:51:38 @agent_ppo2.py:193][0m |          -0.0092 |           1.1757 |           1.9857 |
[32m[20230203 20:51:38 @agent_ppo2.py:193][0m |          -0.0095 |           1.1605 |           1.9865 |
[32m[20230203 20:51:38 @agent_ppo2.py:193][0m |          -0.0099 |           1.1489 |           1.9864 |
[32m[20230203 20:51:38 @agent_ppo2.py:193][0m |          -0.0101 |           1.1430 |           1.9879 |
[32m[20230203 20:51:38 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:51:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: -82.79
[32m[20230203 20:51:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -81.06
[32m[20230203 20:51:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -93.95
[32m[20230203 20:51:39 @agent_ppo2.py:151][0m Total time:       1.06 min
[32m[20230203 20:51:39 @agent_ppo2.py:153][0m 75776 total steps have happened
[32m[20230203 20:51:39 @agent_ppo2.py:129][0m #------------------------ Iteration 37 --------------------------#
[32m[20230203 20:51:39 @agent_ppo2.py:135][0m Sampling time: 0.61 s by 1 slaves
[32m[20230203 20:51:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:40 @agent_ppo2.py:193][0m |           0.0002 |           5.7325 |           1.9760 |
[32m[20230203 20:51:40 @agent_ppo2.py:193][0m |          -0.0026 |           2.3411 |           1.9754 |
[32m[20230203 20:51:40 @agent_ppo2.py:193][0m |          -0.0037 |           1.8902 |           1.9744 |
[32m[20230203 20:51:40 @agent_ppo2.py:193][0m |          -0.0051 |           1.7331 |           1.9732 |
[32m[20230203 20:51:40 @agent_ppo2.py:193][0m |          -0.0050 |           1.6504 |           1.9734 |
[32m[20230203 20:51:40 @agent_ppo2.py:193][0m |          -0.0060 |           1.6201 |           1.9729 |
[32m[20230203 20:51:40 @agent_ppo2.py:193][0m |          -0.0059 |           1.5909 |           1.9728 |
[32m[20230203 20:51:40 @agent_ppo2.py:193][0m |          -0.0079 |           1.5348 |           1.9722 |
[32m[20230203 20:51:40 @agent_ppo2.py:193][0m |          -0.0076 |           1.5194 |           1.9719 |
[32m[20230203 20:51:40 @agent_ppo2.py:193][0m |          -0.0083 |           1.5126 |           1.9723 |
[32m[20230203 20:51:40 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:51:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: -93.38
[32m[20230203 20:51:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -81.04
[32m[20230203 20:51:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -96.48
[32m[20230203 20:51:40 @agent_ppo2.py:151][0m Total time:       1.09 min
[32m[20230203 20:51:40 @agent_ppo2.py:153][0m 77824 total steps have happened
[32m[20230203 20:51:40 @agent_ppo2.py:129][0m #------------------------ Iteration 38 --------------------------#
[32m[20230203 20:51:41 @agent_ppo2.py:135][0m Sampling time: 0.66 s by 1 slaves
[32m[20230203 20:51:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:41 @agent_ppo2.py:193][0m |           0.0008 |          15.2467 |           2.0317 |
[32m[20230203 20:51:41 @agent_ppo2.py:193][0m |          -0.0028 |           6.7081 |           2.0276 |
[32m[20230203 20:51:41 @agent_ppo2.py:193][0m |          -0.0044 |           4.9314 |           2.0268 |
[32m[20230203 20:51:41 @agent_ppo2.py:193][0m |          -0.0041 |           4.2233 |           2.0263 |
[32m[20230203 20:51:41 @agent_ppo2.py:193][0m |          -0.0062 |           3.9301 |           2.0243 |
[32m[20230203 20:51:42 @agent_ppo2.py:193][0m |          -0.0052 |           3.6962 |           2.0244 |
[32m[20230203 20:51:42 @agent_ppo2.py:193][0m |          -0.0053 |           3.5723 |           2.0232 |
[32m[20230203 20:51:42 @agent_ppo2.py:193][0m |          -0.0070 |           3.3517 |           2.0225 |
[32m[20230203 20:51:42 @agent_ppo2.py:193][0m |          -0.0073 |           3.3152 |           2.0224 |
[32m[20230203 20:51:42 @agent_ppo2.py:193][0m |          -0.0079 |           3.1030 |           2.0222 |
[32m[20230203 20:51:42 @agent_ppo2.py:138][0m Policy update time: 0.66 s
[32m[20230203 20:51:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: -100.30
[32m[20230203 20:51:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -75.64
[32m[20230203 20:51:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -107.70
[32m[20230203 20:51:42 @agent_ppo2.py:151][0m Total time:       1.12 min
[32m[20230203 20:51:42 @agent_ppo2.py:153][0m 79872 total steps have happened
[32m[20230203 20:51:42 @agent_ppo2.py:129][0m #------------------------ Iteration 39 --------------------------#
[32m[20230203 20:51:43 @agent_ppo2.py:135][0m Sampling time: 0.66 s by 1 slaves
[32m[20230203 20:51:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:43 @agent_ppo2.py:193][0m |          -0.0028 |          11.5635 |           1.9978 |
[32m[20230203 20:51:43 @agent_ppo2.py:193][0m |          -0.0090 |           7.9777 |           1.9975 |
[32m[20230203 20:51:43 @agent_ppo2.py:193][0m |          -0.0069 |           6.6615 |           1.9966 |
[32m[20230203 20:51:43 @agent_ppo2.py:193][0m |          -0.0091 |           5.4969 |           1.9957 |
[32m[20230203 20:51:43 @agent_ppo2.py:193][0m |          -0.0069 |           5.1987 |           1.9945 |
[32m[20230203 20:51:43 @agent_ppo2.py:193][0m |          -0.0070 |           4.9248 |           1.9945 |
[32m[20230203 20:51:44 @agent_ppo2.py:193][0m |          -0.0113 |           4.8107 |           1.9932 |
[32m[20230203 20:51:44 @agent_ppo2.py:193][0m |          -0.0036 |           4.6017 |           1.9926 |
[32m[20230203 20:51:44 @agent_ppo2.py:193][0m |          -0.0117 |           4.5014 |           1.9920 |
[32m[20230203 20:51:44 @agent_ppo2.py:193][0m |          -0.0030 |           4.4989 |           1.9914 |
[32m[20230203 20:51:44 @agent_ppo2.py:138][0m Policy update time: 0.68 s
[32m[20230203 20:51:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: -97.94
[32m[20230203 20:51:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -75.35
[32m[20230203 20:51:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -101.14
[32m[20230203 20:51:44 @agent_ppo2.py:151][0m Total time:       1.15 min
[32m[20230203 20:51:44 @agent_ppo2.py:153][0m 81920 total steps have happened
[32m[20230203 20:51:44 @agent_ppo2.py:129][0m #------------------------ Iteration 40 --------------------------#
[32m[20230203 20:51:45 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:51:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:45 @agent_ppo2.py:193][0m |          -0.0004 |          52.3660 |           1.9794 |
[32m[20230203 20:51:45 @agent_ppo2.py:193][0m |          -0.0003 |          25.1961 |           1.9787 |
[32m[20230203 20:51:45 @agent_ppo2.py:193][0m |          -0.0037 |          21.4212 |           1.9772 |
[32m[20230203 20:51:45 @agent_ppo2.py:193][0m |          -0.0075 |          20.1710 |           1.9761 |
[32m[20230203 20:51:45 @agent_ppo2.py:193][0m |          -0.0080 |          18.5373 |           1.9744 |
[32m[20230203 20:51:45 @agent_ppo2.py:193][0m |          -0.0105 |          17.7890 |           1.9734 |
[32m[20230203 20:51:45 @agent_ppo2.py:193][0m |          -0.0106 |          16.9296 |           1.9726 |
[32m[20230203 20:51:45 @agent_ppo2.py:193][0m |          -0.0105 |          16.5464 |           1.9719 |
[32m[20230203 20:51:45 @agent_ppo2.py:193][0m |          -0.0122 |          16.5711 |           1.9715 |
[32m[20230203 20:51:45 @agent_ppo2.py:193][0m |          -0.0124 |          15.9279 |           1.9707 |
[32m[20230203 20:51:45 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 20:51:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: -107.06
[32m[20230203 20:51:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -87.20
[32m[20230203 20:51:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -97.18
[32m[20230203 20:51:45 @agent_ppo2.py:151][0m Total time:       1.17 min
[32m[20230203 20:51:45 @agent_ppo2.py:153][0m 83968 total steps have happened
[32m[20230203 20:51:45 @agent_ppo2.py:129][0m #------------------------ Iteration 41 --------------------------#
[32m[20230203 20:51:46 @agent_ppo2.py:135][0m Sampling time: 0.60 s by 1 slaves
[32m[20230203 20:51:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:46 @agent_ppo2.py:193][0m |           0.0006 |           4.5563 |           1.9857 |
[32m[20230203 20:51:46 @agent_ppo2.py:193][0m |          -0.0016 |           3.5761 |           1.9835 |
[32m[20230203 20:51:46 @agent_ppo2.py:193][0m |          -0.0038 |           3.1966 |           1.9829 |
[32m[20230203 20:51:46 @agent_ppo2.py:193][0m |          -0.0047 |           3.0627 |           1.9816 |
[32m[20230203 20:51:46 @agent_ppo2.py:193][0m |          -0.0052 |           2.9641 |           1.9809 |
[32m[20230203 20:51:46 @agent_ppo2.py:193][0m |          -0.0056 |           2.8944 |           1.9807 |
[32m[20230203 20:51:47 @agent_ppo2.py:193][0m |          -0.0063 |           2.8340 |           1.9800 |
[32m[20230203 20:51:47 @agent_ppo2.py:193][0m |          -0.0064 |           2.7884 |           1.9797 |
[32m[20230203 20:51:47 @agent_ppo2.py:193][0m |          -0.0073 |           2.7470 |           1.9798 |
[32m[20230203 20:51:47 @agent_ppo2.py:193][0m |          -0.0070 |           2.7160 |           1.9795 |
[32m[20230203 20:51:47 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:51:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: -87.06
[32m[20230203 20:51:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -85.45
[32m[20230203 20:51:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -59.66
[32m[20230203 20:51:47 @agent_ppo2.py:151][0m Total time:       1.20 min
[32m[20230203 20:51:47 @agent_ppo2.py:153][0m 86016 total steps have happened
[32m[20230203 20:51:47 @agent_ppo2.py:129][0m #------------------------ Iteration 42 --------------------------#
[32m[20230203 20:51:48 @agent_ppo2.py:135][0m Sampling time: 0.64 s by 1 slaves
[32m[20230203 20:51:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:48 @agent_ppo2.py:193][0m |          -0.0012 |          17.1119 |           1.9828 |
[32m[20230203 20:51:48 @agent_ppo2.py:193][0m |          -0.0040 |          10.4261 |           1.9818 |
[32m[20230203 20:51:48 @agent_ppo2.py:193][0m |          -0.0056 |           8.7465 |           1.9814 |
[32m[20230203 20:51:48 @agent_ppo2.py:193][0m |          -0.0058 |           7.9751 |           1.9804 |
[32m[20230203 20:51:48 @agent_ppo2.py:193][0m |          -0.0057 |           7.5718 |           1.9800 |
[32m[20230203 20:51:48 @agent_ppo2.py:193][0m |          -0.0057 |           7.0096 |           1.9792 |
[32m[20230203 20:51:48 @agent_ppo2.py:193][0m |          -0.0095 |           6.9990 |           1.9797 |
[32m[20230203 20:51:48 @agent_ppo2.py:193][0m |          -0.0086 |           6.3060 |           1.9803 |
[32m[20230203 20:51:49 @agent_ppo2.py:193][0m |          -0.0061 |           6.1004 |           1.9803 |
[32m[20230203 20:51:49 @agent_ppo2.py:193][0m |          -0.0074 |           5.7921 |           1.9801 |
[32m[20230203 20:51:49 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230203 20:51:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: -104.59
[32m[20230203 20:51:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -82.38
[32m[20230203 20:51:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -79.16
[32m[20230203 20:51:49 @agent_ppo2.py:151][0m Total time:       1.23 min
[32m[20230203 20:51:49 @agent_ppo2.py:153][0m 88064 total steps have happened
[32m[20230203 20:51:49 @agent_ppo2.py:129][0m #------------------------ Iteration 43 --------------------------#
[32m[20230203 20:51:50 @agent_ppo2.py:135][0m Sampling time: 0.60 s by 1 slaves
[32m[20230203 20:51:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:50 @agent_ppo2.py:193][0m |          -0.0004 |          11.8605 |           1.9783 |
[32m[20230203 20:51:50 @agent_ppo2.py:193][0m |           0.0003 |           7.6771 |           1.9773 |
[32m[20230203 20:51:50 @agent_ppo2.py:193][0m |          -0.0048 |           6.2514 |           1.9774 |
[32m[20230203 20:51:50 @agent_ppo2.py:193][0m |          -0.0045 |           5.5951 |           1.9777 |
[32m[20230203 20:51:50 @agent_ppo2.py:193][0m |          -0.0064 |           5.1128 |           1.9790 |
[32m[20230203 20:51:50 @agent_ppo2.py:193][0m |          -0.0123 |           4.8600 |           1.9785 |
[32m[20230203 20:51:50 @agent_ppo2.py:193][0m |          -0.0106 |           4.6817 |           1.9798 |
[32m[20230203 20:51:50 @agent_ppo2.py:193][0m |          -0.0087 |           4.4800 |           1.9802 |
[32m[20230203 20:51:50 @agent_ppo2.py:193][0m |          -0.0063 |           4.2946 |           1.9804 |
[32m[20230203 20:51:50 @agent_ppo2.py:193][0m |          -0.0092 |           4.1514 |           1.9818 |
[32m[20230203 20:51:50 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:51:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: -89.12
[32m[20230203 20:51:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -78.99
[32m[20230203 20:51:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -78.53
[32m[20230203 20:51:51 @agent_ppo2.py:151][0m Total time:       1.26 min
[32m[20230203 20:51:51 @agent_ppo2.py:153][0m 90112 total steps have happened
[32m[20230203 20:51:51 @agent_ppo2.py:129][0m #------------------------ Iteration 44 --------------------------#
[32m[20230203 20:51:51 @agent_ppo2.py:135][0m Sampling time: 0.60 s by 1 slaves
[32m[20230203 20:51:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:52 @agent_ppo2.py:193][0m |          -0.0000 |           8.3743 |           1.9423 |
[32m[20230203 20:51:52 @agent_ppo2.py:193][0m |          -0.0004 |           4.1263 |           1.9419 |
[32m[20230203 20:51:52 @agent_ppo2.py:193][0m |          -0.0067 |           3.6824 |           1.9403 |
[32m[20230203 20:51:52 @agent_ppo2.py:193][0m |          -0.0025 |           3.4845 |           1.9399 |
[32m[20230203 20:51:52 @agent_ppo2.py:193][0m |          -0.0091 |           3.1128 |           1.9396 |
[32m[20230203 20:51:52 @agent_ppo2.py:193][0m |          -0.0118 |           2.9295 |           1.9392 |
[32m[20230203 20:51:52 @agent_ppo2.py:193][0m |          -0.0106 |           2.8714 |           1.9392 |
[32m[20230203 20:51:52 @agent_ppo2.py:193][0m |          -0.0173 |           2.5981 |           1.9375 |
[32m[20230203 20:51:52 @agent_ppo2.py:193][0m |          -0.0054 |           2.5087 |           1.9377 |
[32m[20230203 20:51:52 @agent_ppo2.py:193][0m |          -0.0113 |           2.4092 |           1.9361 |
[32m[20230203 20:51:52 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:51:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: -93.76
[32m[20230203 20:51:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -80.85
[32m[20230203 20:51:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -103.69
[32m[20230203 20:51:53 @agent_ppo2.py:151][0m Total time:       1.29 min
[32m[20230203 20:51:53 @agent_ppo2.py:153][0m 92160 total steps have happened
[32m[20230203 20:51:53 @agent_ppo2.py:129][0m #------------------------ Iteration 45 --------------------------#
[32m[20230203 20:51:53 @agent_ppo2.py:135][0m Sampling time: 0.58 s by 1 slaves
[32m[20230203 20:51:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:53 @agent_ppo2.py:193][0m |          -0.0004 |           1.5537 |           2.0400 |
[32m[20230203 20:51:53 @agent_ppo2.py:193][0m |          -0.0043 |           1.2320 |           2.0401 |
[32m[20230203 20:51:53 @agent_ppo2.py:193][0m |          -0.0059 |           1.1636 |           2.0393 |
[32m[20230203 20:51:53 @agent_ppo2.py:193][0m |          -0.0076 |           1.1196 |           2.0394 |
[32m[20230203 20:51:54 @agent_ppo2.py:193][0m |          -0.0065 |           1.0872 |           2.0395 |
[32m[20230203 20:51:54 @agent_ppo2.py:193][0m |          -0.0076 |           1.0648 |           2.0398 |
[32m[20230203 20:51:54 @agent_ppo2.py:193][0m |          -0.0081 |           1.0523 |           2.0389 |
[32m[20230203 20:51:54 @agent_ppo2.py:193][0m |          -0.0086 |           1.0344 |           2.0397 |
[32m[20230203 20:51:54 @agent_ppo2.py:193][0m |          -0.0088 |           1.0258 |           2.0400 |
[32m[20230203 20:51:54 @agent_ppo2.py:193][0m |          -0.0078 |           1.0150 |           2.0406 |
[32m[20230203 20:51:54 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:51:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: -80.20
[32m[20230203 20:51:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -78.31
[32m[20230203 20:51:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -66.39
[32m[20230203 20:51:54 @agent_ppo2.py:151][0m Total time:       1.32 min
[32m[20230203 20:51:54 @agent_ppo2.py:153][0m 94208 total steps have happened
[32m[20230203 20:51:54 @agent_ppo2.py:129][0m #------------------------ Iteration 46 --------------------------#
[32m[20230203 20:51:55 @agent_ppo2.py:135][0m Sampling time: 0.60 s by 1 slaves
[32m[20230203 20:51:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:55 @agent_ppo2.py:193][0m |           0.0012 |           3.5126 |           1.9978 |
[32m[20230203 20:51:55 @agent_ppo2.py:193][0m |          -0.0025 |           2.1416 |           1.9951 |
[32m[20230203 20:51:55 @agent_ppo2.py:193][0m |          -0.0038 |           1.8809 |           1.9936 |
[32m[20230203 20:51:55 @agent_ppo2.py:193][0m |          -0.0038 |           1.7386 |           1.9926 |
[32m[20230203 20:51:55 @agent_ppo2.py:193][0m |          -0.0051 |           1.7151 |           1.9915 |
[32m[20230203 20:51:55 @agent_ppo2.py:193][0m |          -0.0079 |           1.6079 |           1.9902 |
[32m[20230203 20:51:55 @agent_ppo2.py:193][0m |          -0.0064 |           1.5623 |           1.9890 |
[32m[20230203 20:51:55 @agent_ppo2.py:193][0m |          -0.0058 |           1.5176 |           1.9883 |
[32m[20230203 20:51:56 @agent_ppo2.py:193][0m |          -0.0076 |           1.4748 |           1.9880 |
[32m[20230203 20:51:56 @agent_ppo2.py:193][0m |          -0.0060 |           1.4777 |           1.9878 |
[32m[20230203 20:51:56 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:51:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: -89.91
[32m[20230203 20:51:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -80.68
[32m[20230203 20:51:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -58.23
[32m[20230203 20:51:56 @agent_ppo2.py:151][0m Total time:       1.35 min
[32m[20230203 20:51:56 @agent_ppo2.py:153][0m 96256 total steps have happened
[32m[20230203 20:51:56 @agent_ppo2.py:129][0m #------------------------ Iteration 47 --------------------------#
[32m[20230203 20:51:57 @agent_ppo2.py:135][0m Sampling time: 0.61 s by 1 slaves
[32m[20230203 20:51:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:57 @agent_ppo2.py:193][0m |          -0.0012 |          10.9865 |           2.0017 |
[32m[20230203 20:51:57 @agent_ppo2.py:193][0m |          -0.0020 |           7.8759 |           2.0011 |
[32m[20230203 20:51:57 @agent_ppo2.py:193][0m |          -0.0051 |           7.0162 |           1.9988 |
[32m[20230203 20:51:57 @agent_ppo2.py:193][0m |          -0.0022 |           6.5784 |           1.9982 |
[32m[20230203 20:51:57 @agent_ppo2.py:193][0m |          -0.0010 |           6.2386 |           1.9971 |
[32m[20230203 20:51:57 @agent_ppo2.py:193][0m |          -0.0002 |           6.0170 |           1.9958 |
[32m[20230203 20:51:57 @agent_ppo2.py:193][0m |          -0.0077 |           5.7555 |           1.9954 |
[32m[20230203 20:51:57 @agent_ppo2.py:193][0m |          -0.0057 |           5.5613 |           1.9951 |
[32m[20230203 20:51:57 @agent_ppo2.py:193][0m |          -0.0068 |           5.4621 |           1.9949 |
[32m[20230203 20:51:57 @agent_ppo2.py:193][0m |          -0.0095 |           5.3004 |           1.9938 |
[32m[20230203 20:51:57 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:51:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: -90.44
[32m[20230203 20:51:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -72.89
[32m[20230203 20:51:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -96.13
[32m[20230203 20:51:58 @agent_ppo2.py:151][0m Total time:       1.38 min
[32m[20230203 20:51:58 @agent_ppo2.py:153][0m 98304 total steps have happened
[32m[20230203 20:51:58 @agent_ppo2.py:129][0m #------------------------ Iteration 48 --------------------------#
[32m[20230203 20:51:59 @agent_ppo2.py:135][0m Sampling time: 0.60 s by 1 slaves
[32m[20230203 20:51:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:51:59 @agent_ppo2.py:193][0m |          -0.0001 |          13.2960 |           2.0543 |
[32m[20230203 20:51:59 @agent_ppo2.py:193][0m |          -0.0018 |           9.5515 |           2.0547 |
[32m[20230203 20:51:59 @agent_ppo2.py:193][0m |          -0.0027 |           8.4113 |           2.0555 |
[32m[20230203 20:51:59 @agent_ppo2.py:193][0m |          -0.0031 |           7.8453 |           2.0556 |
[32m[20230203 20:51:59 @agent_ppo2.py:193][0m |          -0.0041 |           7.4802 |           2.0559 |
[32m[20230203 20:51:59 @agent_ppo2.py:193][0m |          -0.0049 |           6.7273 |           2.0566 |
[32m[20230203 20:51:59 @agent_ppo2.py:193][0m |          -0.0055 |           6.7665 |           2.0577 |
[32m[20230203 20:51:59 @agent_ppo2.py:193][0m |          -0.0059 |           6.4358 |           2.0579 |
[32m[20230203 20:51:59 @agent_ppo2.py:193][0m |          -0.0063 |           6.1996 |           2.0589 |
[32m[20230203 20:51:59 @agent_ppo2.py:193][0m |          -0.0068 |           6.1763 |           2.0590 |
[32m[20230203 20:51:59 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: -77.17
[32m[20230203 20:52:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -76.66
[32m[20230203 20:52:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -62.71
[32m[20230203 20:52:00 @agent_ppo2.py:151][0m Total time:       1.41 min
[32m[20230203 20:52:00 @agent_ppo2.py:153][0m 100352 total steps have happened
[32m[20230203 20:52:00 @agent_ppo2.py:129][0m #------------------------ Iteration 49 --------------------------#
[32m[20230203 20:52:00 @agent_ppo2.py:135][0m Sampling time: 0.58 s by 1 slaves
[32m[20230203 20:52:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:00 @agent_ppo2.py:193][0m |           0.0004 |           2.1890 |           2.0277 |
[32m[20230203 20:52:00 @agent_ppo2.py:193][0m |          -0.0014 |           1.7791 |           2.0283 |
[32m[20230203 20:52:00 @agent_ppo2.py:193][0m |          -0.0037 |           1.6821 |           2.0285 |
[32m[20230203 20:52:01 @agent_ppo2.py:193][0m |          -0.0053 |           1.6176 |           2.0283 |
[32m[20230203 20:52:01 @agent_ppo2.py:193][0m |          -0.0066 |           1.5832 |           2.0286 |
[32m[20230203 20:52:01 @agent_ppo2.py:193][0m |          -0.0074 |           1.5643 |           2.0285 |
[32m[20230203 20:52:01 @agent_ppo2.py:193][0m |          -0.0077 |           1.5260 |           2.0292 |
[32m[20230203 20:52:01 @agent_ppo2.py:193][0m |          -0.0090 |           1.5297 |           2.0294 |
[32m[20230203 20:52:01 @agent_ppo2.py:193][0m |          -0.0092 |           1.5102 |           2.0298 |
[32m[20230203 20:52:01 @agent_ppo2.py:193][0m |          -0.0097 |           1.4689 |           2.0298 |
[32m[20230203 20:52:01 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: -76.02
[32m[20230203 20:52:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -72.67
[32m[20230203 20:52:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -66.45
[32m[20230203 20:52:01 @agent_ppo2.py:151][0m Total time:       1.44 min
[32m[20230203 20:52:01 @agent_ppo2.py:153][0m 102400 total steps have happened
[32m[20230203 20:52:01 @agent_ppo2.py:129][0m #------------------------ Iteration 50 --------------------------#
[32m[20230203 20:52:02 @agent_ppo2.py:135][0m Sampling time: 0.58 s by 1 slaves
[32m[20230203 20:52:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:02 @agent_ppo2.py:193][0m |           0.0001 |           1.7406 |           2.0337 |
[32m[20230203 20:52:02 @agent_ppo2.py:193][0m |          -0.0027 |           1.4681 |           2.0323 |
[32m[20230203 20:52:02 @agent_ppo2.py:193][0m |          -0.0047 |           1.3470 |           2.0326 |
[32m[20230203 20:52:02 @agent_ppo2.py:193][0m |          -0.0065 |           1.2461 |           2.0320 |
[32m[20230203 20:52:02 @agent_ppo2.py:193][0m |          -0.0074 |           1.1595 |           2.0300 |
[32m[20230203 20:52:02 @agent_ppo2.py:193][0m |          -0.0082 |           1.1302 |           2.0312 |
[32m[20230203 20:52:02 @agent_ppo2.py:193][0m |          -0.0091 |           1.0827 |           2.0315 |
[32m[20230203 20:52:02 @agent_ppo2.py:193][0m |          -0.0094 |           1.0733 |           2.0321 |
[32m[20230203 20:52:02 @agent_ppo2.py:193][0m |          -0.0098 |           1.0407 |           2.0326 |
[32m[20230203 20:52:03 @agent_ppo2.py:193][0m |          -0.0103 |           1.0470 |           2.0337 |
[32m[20230203 20:52:03 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:52:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: -73.61
[32m[20230203 20:52:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -70.58
[32m[20230203 20:52:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -61.58
[32m[20230203 20:52:03 @agent_ppo2.py:151][0m Total time:       1.46 min
[32m[20230203 20:52:03 @agent_ppo2.py:153][0m 104448 total steps have happened
[32m[20230203 20:52:03 @agent_ppo2.py:129][0m #------------------------ Iteration 51 --------------------------#
[32m[20230203 20:52:04 @agent_ppo2.py:135][0m Sampling time: 0.57 s by 1 slaves
[32m[20230203 20:52:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:04 @agent_ppo2.py:193][0m |           0.0001 |           1.3770 |           2.0721 |
[32m[20230203 20:52:04 @agent_ppo2.py:193][0m |          -0.0031 |           1.0728 |           2.0714 |
[32m[20230203 20:52:04 @agent_ppo2.py:193][0m |          -0.0062 |           0.9479 |           2.0711 |
[32m[20230203 20:52:04 @agent_ppo2.py:193][0m |          -0.0074 |           0.9478 |           2.0700 |
[32m[20230203 20:52:04 @agent_ppo2.py:193][0m |          -0.0087 |           0.8585 |           2.0693 |
[32m[20230203 20:52:04 @agent_ppo2.py:193][0m |          -0.0090 |           0.8396 |           2.0689 |
[32m[20230203 20:52:04 @agent_ppo2.py:193][0m |          -0.0097 |           0.8040 |           2.0698 |
[32m[20230203 20:52:04 @agent_ppo2.py:193][0m |          -0.0096 |           0.7981 |           2.0697 |
[32m[20230203 20:52:04 @agent_ppo2.py:193][0m |          -0.0105 |           0.7846 |           2.0706 |
[32m[20230203 20:52:04 @agent_ppo2.py:193][0m |          -0.0107 |           0.7734 |           2.0715 |
[32m[20230203 20:52:04 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: -73.44
[32m[20230203 20:52:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -72.67
[32m[20230203 20:52:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -52.99
[32m[20230203 20:52:05 @agent_ppo2.py:151][0m Total time:       1.49 min
[32m[20230203 20:52:05 @agent_ppo2.py:153][0m 106496 total steps have happened
[32m[20230203 20:52:05 @agent_ppo2.py:129][0m #------------------------ Iteration 52 --------------------------#
[32m[20230203 20:52:05 @agent_ppo2.py:135][0m Sampling time: 0.57 s by 1 slaves
[32m[20230203 20:52:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:06 @agent_ppo2.py:193][0m |           0.0002 |           1.3226 |           2.0807 |
[32m[20230203 20:52:06 @agent_ppo2.py:193][0m |          -0.0025 |           1.0780 |           2.0818 |
[32m[20230203 20:52:06 @agent_ppo2.py:193][0m |          -0.0045 |           0.9679 |           2.0811 |
[32m[20230203 20:52:06 @agent_ppo2.py:193][0m |          -0.0055 |           0.9328 |           2.0811 |
[32m[20230203 20:52:06 @agent_ppo2.py:193][0m |          -0.0063 |           0.9192 |           2.0810 |
[32m[20230203 20:52:06 @agent_ppo2.py:193][0m |          -0.0068 |           0.9038 |           2.0800 |
[32m[20230203 20:52:06 @agent_ppo2.py:193][0m |          -0.0073 |           0.9009 |           2.0801 |
[32m[20230203 20:52:06 @agent_ppo2.py:193][0m |          -0.0078 |           0.8872 |           2.0811 |
[32m[20230203 20:52:06 @agent_ppo2.py:193][0m |          -0.0083 |           0.8755 |           2.0809 |
[32m[20230203 20:52:06 @agent_ppo2.py:193][0m |          -0.0090 |           0.8743 |           2.0803 |
[32m[20230203 20:52:06 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:52:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: -69.22
[32m[20230203 20:52:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -67.93
[32m[20230203 20:52:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -55.44
[32m[20230203 20:52:07 @agent_ppo2.py:151][0m Total time:       1.52 min
[32m[20230203 20:52:07 @agent_ppo2.py:153][0m 108544 total steps have happened
[32m[20230203 20:52:07 @agent_ppo2.py:129][0m #------------------------ Iteration 53 --------------------------#
[32m[20230203 20:52:07 @agent_ppo2.py:135][0m Sampling time: 0.58 s by 1 slaves
[32m[20230203 20:52:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:07 @agent_ppo2.py:193][0m |          -0.0001 |          17.8019 |           2.0474 |
[32m[20230203 20:52:07 @agent_ppo2.py:193][0m |          -0.0050 |          10.0911 |           2.0472 |
[32m[20230203 20:52:07 @agent_ppo2.py:193][0m |          -0.0067 |           8.3909 |           2.0468 |
[32m[20230203 20:52:07 @agent_ppo2.py:193][0m |          -0.0087 |           7.5013 |           2.0453 |
[32m[20230203 20:52:07 @agent_ppo2.py:193][0m |          -0.0097 |           7.0521 |           2.0456 |
[32m[20230203 20:52:08 @agent_ppo2.py:193][0m |          -0.0105 |           6.7566 |           2.0468 |
[32m[20230203 20:52:08 @agent_ppo2.py:193][0m |          -0.0106 |           6.3242 |           2.0460 |
[32m[20230203 20:52:08 @agent_ppo2.py:193][0m |          -0.0113 |           6.1221 |           2.0463 |
[32m[20230203 20:52:08 @agent_ppo2.py:193][0m |          -0.0108 |           5.9996 |           2.0468 |
[32m[20230203 20:52:08 @agent_ppo2.py:193][0m |          -0.0131 |           5.8800 |           2.0474 |
[32m[20230203 20:52:08 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:52:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: -85.78
[32m[20230203 20:52:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -71.25
[32m[20230203 20:52:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -50.00
[32m[20230203 20:52:08 @agent_ppo2.py:151][0m Total time:       1.55 min
[32m[20230203 20:52:08 @agent_ppo2.py:153][0m 110592 total steps have happened
[32m[20230203 20:52:08 @agent_ppo2.py:129][0m #------------------------ Iteration 54 --------------------------#
[32m[20230203 20:52:09 @agent_ppo2.py:135][0m Sampling time: 0.60 s by 1 slaves
[32m[20230203 20:52:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:09 @agent_ppo2.py:193][0m |          -0.0049 |          10.9740 |           2.0723 |
[32m[20230203 20:52:09 @agent_ppo2.py:193][0m |          -0.0086 |           6.5049 |           2.0719 |
[32m[20230203 20:52:09 @agent_ppo2.py:193][0m |          -0.0101 |           5.5926 |           2.0713 |
[32m[20230203 20:52:09 @agent_ppo2.py:193][0m |          -0.0086 |           5.2159 |           2.0709 |
[32m[20230203 20:52:09 @agent_ppo2.py:193][0m |          -0.0128 |           5.3009 |           2.0693 |
[32m[20230203 20:52:09 @agent_ppo2.py:193][0m |          -0.0182 |           4.7483 |           2.0683 |
[32m[20230203 20:52:09 @agent_ppo2.py:193][0m |          -0.0158 |           4.6094 |           2.0686 |
[32m[20230203 20:52:09 @agent_ppo2.py:193][0m |          -0.0152 |           4.3434 |           2.0677 |
[32m[20230203 20:52:10 @agent_ppo2.py:193][0m |          -0.0159 |           4.3054 |           2.0673 |
[32m[20230203 20:52:10 @agent_ppo2.py:193][0m |          -0.0198 |           4.1732 |           2.0664 |
[32m[20230203 20:52:10 @agent_ppo2.py:138][0m Policy update time: 0.64 s
[32m[20230203 20:52:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: -89.37
[32m[20230203 20:52:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -68.36
[32m[20230203 20:52:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -109.64
[32m[20230203 20:52:10 @agent_ppo2.py:151][0m Total time:       1.58 min
[32m[20230203 20:52:10 @agent_ppo2.py:153][0m 112640 total steps have happened
[32m[20230203 20:52:10 @agent_ppo2.py:129][0m #------------------------ Iteration 55 --------------------------#
[32m[20230203 20:52:11 @agent_ppo2.py:135][0m Sampling time: 0.58 s by 1 slaves
[32m[20230203 20:52:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:11 @agent_ppo2.py:193][0m |          -0.0015 |           9.6252 |           2.1107 |
[32m[20230203 20:52:11 @agent_ppo2.py:193][0m |          -0.0049 |           7.2776 |           2.1087 |
[32m[20230203 20:52:11 @agent_ppo2.py:193][0m |          -0.0063 |           6.8920 |           2.1079 |
[32m[20230203 20:52:11 @agent_ppo2.py:193][0m |          -0.0071 |           6.7211 |           2.1071 |
[32m[20230203 20:52:11 @agent_ppo2.py:193][0m |          -0.0080 |           6.6206 |           2.1073 |
[32m[20230203 20:52:11 @agent_ppo2.py:193][0m |          -0.0087 |           6.5825 |           2.1068 |
[32m[20230203 20:52:11 @agent_ppo2.py:193][0m |          -0.0084 |           6.5751 |           2.1069 |
[32m[20230203 20:52:11 @agent_ppo2.py:193][0m |          -0.0101 |           6.4490 |           2.1063 |
[32m[20230203 20:52:11 @agent_ppo2.py:193][0m |          -0.0096 |           6.3887 |           2.1070 |
[32m[20230203 20:52:11 @agent_ppo2.py:193][0m |          -0.0100 |           6.4136 |           2.1062 |
[32m[20230203 20:52:11 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:52:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: -81.19
[32m[20230203 20:52:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -65.81
[32m[20230203 20:52:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -57.62
[32m[20230203 20:52:12 @agent_ppo2.py:151][0m Total time:       1.61 min
[32m[20230203 20:52:12 @agent_ppo2.py:153][0m 114688 total steps have happened
[32m[20230203 20:52:12 @agent_ppo2.py:129][0m #------------------------ Iteration 56 --------------------------#
[32m[20230203 20:52:12 @agent_ppo2.py:135][0m Sampling time: 0.57 s by 1 slaves
[32m[20230203 20:52:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:12 @agent_ppo2.py:193][0m |          -0.0003 |           1.8875 |           2.0814 |
[32m[20230203 20:52:12 @agent_ppo2.py:193][0m |          -0.0041 |           1.1301 |           2.0773 |
[32m[20230203 20:52:12 @agent_ppo2.py:193][0m |          -0.0051 |           1.0007 |           2.0761 |
[32m[20230203 20:52:13 @agent_ppo2.py:193][0m |          -0.0063 |           0.9604 |           2.0759 |
[32m[20230203 20:52:13 @agent_ppo2.py:193][0m |          -0.0066 |           0.9400 |           2.0758 |
[32m[20230203 20:52:13 @agent_ppo2.py:193][0m |          -0.0072 |           0.9291 |           2.0756 |
[32m[20230203 20:52:13 @agent_ppo2.py:193][0m |          -0.0078 |           0.9208 |           2.0745 |
[32m[20230203 20:52:13 @agent_ppo2.py:193][0m |          -0.0079 |           0.9149 |           2.0755 |
[32m[20230203 20:52:13 @agent_ppo2.py:193][0m |          -0.0081 |           0.9119 |           2.0767 |
[32m[20230203 20:52:13 @agent_ppo2.py:193][0m |          -0.0086 |           0.9071 |           2.0753 |
[32m[20230203 20:52:13 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:52:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: -73.98
[32m[20230203 20:52:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -72.96
[32m[20230203 20:52:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -59.06
[32m[20230203 20:52:13 @agent_ppo2.py:151][0m Total time:       1.64 min
[32m[20230203 20:52:13 @agent_ppo2.py:153][0m 116736 total steps have happened
[32m[20230203 20:52:13 @agent_ppo2.py:129][0m #------------------------ Iteration 57 --------------------------#
[32m[20230203 20:52:14 @agent_ppo2.py:135][0m Sampling time: 0.57 s by 1 slaves
[32m[20230203 20:52:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:14 @agent_ppo2.py:193][0m |           0.0005 |          22.8888 |           2.1126 |
[32m[20230203 20:52:14 @agent_ppo2.py:193][0m |          -0.0007 |          15.9929 |           2.1118 |
[32m[20230203 20:52:14 @agent_ppo2.py:193][0m |          -0.0016 |          14.5957 |           2.1108 |
[32m[20230203 20:52:14 @agent_ppo2.py:193][0m |          -0.0031 |          13.2891 |           2.1094 |
[32m[20230203 20:52:14 @agent_ppo2.py:193][0m |          -0.0039 |          12.4327 |           2.1088 |
[32m[20230203 20:52:14 @agent_ppo2.py:193][0m |          -0.0048 |          11.8118 |           2.1069 |
[32m[20230203 20:52:14 @agent_ppo2.py:193][0m |          -0.0054 |          11.2078 |           2.1057 |
[32m[20230203 20:52:14 @agent_ppo2.py:193][0m |          -0.0060 |          10.8702 |           2.1055 |
[32m[20230203 20:52:15 @agent_ppo2.py:193][0m |          -0.0068 |          10.4054 |           2.1046 |
[32m[20230203 20:52:15 @agent_ppo2.py:193][0m |          -0.0072 |          10.1768 |           2.1036 |
[32m[20230203 20:52:15 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:52:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: -67.83
[32m[20230203 20:52:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -67.82
[32m[20230203 20:52:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -61.42
[32m[20230203 20:52:15 @agent_ppo2.py:151][0m Total time:       1.66 min
[32m[20230203 20:52:15 @agent_ppo2.py:153][0m 118784 total steps have happened
[32m[20230203 20:52:15 @agent_ppo2.py:129][0m #------------------------ Iteration 58 --------------------------#
[32m[20230203 20:52:16 @agent_ppo2.py:135][0m Sampling time: 0.58 s by 1 slaves
[32m[20230203 20:52:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:16 @agent_ppo2.py:193][0m |          -0.0001 |          16.6806 |           2.0624 |
[32m[20230203 20:52:16 @agent_ppo2.py:193][0m |           0.0211 |           7.7531 |           2.0623 |
[32m[20230203 20:52:16 @agent_ppo2.py:193][0m |          -0.0054 |           5.6370 |           2.0625 |
[32m[20230203 20:52:16 @agent_ppo2.py:193][0m |          -0.0014 |           5.4112 |           2.0619 |
[32m[20230203 20:52:16 @agent_ppo2.py:193][0m |          -0.0064 |           4.9051 |           2.0616 |
[32m[20230203 20:52:16 @agent_ppo2.py:193][0m |          -0.0093 |           4.6850 |           2.0620 |
[32m[20230203 20:52:16 @agent_ppo2.py:193][0m |          -0.0084 |           4.4611 |           2.0603 |
[32m[20230203 20:52:16 @agent_ppo2.py:193][0m |          -0.0087 |           4.3484 |           2.0603 |
[32m[20230203 20:52:16 @agent_ppo2.py:193][0m |          -0.0068 |           4.2935 |           2.0599 |
[32m[20230203 20:52:16 @agent_ppo2.py:193][0m |          -0.0091 |           4.1454 |           2.0600 |
[32m[20230203 20:52:16 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:52:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: -83.60
[32m[20230203 20:52:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -64.88
[32m[20230203 20:52:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -44.86
[32m[20230203 20:52:17 @agent_ppo2.py:151][0m Total time:       1.69 min
[32m[20230203 20:52:17 @agent_ppo2.py:153][0m 120832 total steps have happened
[32m[20230203 20:52:17 @agent_ppo2.py:129][0m #------------------------ Iteration 59 --------------------------#
[32m[20230203 20:52:17 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:52:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:17 @agent_ppo2.py:193][0m |          -0.0003 |           2.3370 |           2.0957 |
[32m[20230203 20:52:18 @agent_ppo2.py:193][0m |          -0.0030 |           1.6132 |           2.0943 |
[32m[20230203 20:52:18 @agent_ppo2.py:193][0m |          -0.0047 |           1.4742 |           2.0921 |
[32m[20230203 20:52:18 @agent_ppo2.py:193][0m |          -0.0061 |           1.3871 |           2.0914 |
[32m[20230203 20:52:18 @agent_ppo2.py:193][0m |          -0.0071 |           1.3243 |           2.0902 |
[32m[20230203 20:52:18 @agent_ppo2.py:193][0m |          -0.0080 |           1.2970 |           2.0894 |
[32m[20230203 20:52:18 @agent_ppo2.py:193][0m |          -0.0087 |           1.2658 |           2.0881 |
[32m[20230203 20:52:18 @agent_ppo2.py:193][0m |          -0.0095 |           1.2495 |           2.0881 |
[32m[20230203 20:52:18 @agent_ppo2.py:193][0m |          -0.0099 |           1.2287 |           2.0876 |
[32m[20230203 20:52:18 @agent_ppo2.py:193][0m |          -0.0100 |           1.2158 |           2.0873 |
[32m[20230203 20:52:18 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:52:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: -67.17
[32m[20230203 20:52:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -66.97
[32m[20230203 20:52:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -117.69
[32m[20230203 20:52:18 @agent_ppo2.py:151][0m Total time:       1.72 min
[32m[20230203 20:52:18 @agent_ppo2.py:153][0m 122880 total steps have happened
[32m[20230203 20:52:18 @agent_ppo2.py:129][0m #------------------------ Iteration 60 --------------------------#
[32m[20230203 20:52:19 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:52:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:19 @agent_ppo2.py:193][0m |           0.0002 |           5.4808 |           2.0700 |
[32m[20230203 20:52:19 @agent_ppo2.py:193][0m |          -0.0025 |           3.7995 |           2.0686 |
[32m[20230203 20:52:19 @agent_ppo2.py:193][0m |          -0.0046 |           3.1296 |           2.0680 |
[32m[20230203 20:52:19 @agent_ppo2.py:193][0m |          -0.0055 |           2.8518 |           2.0685 |
[32m[20230203 20:52:19 @agent_ppo2.py:193][0m |          -0.0062 |           2.6330 |           2.0681 |
[32m[20230203 20:52:19 @agent_ppo2.py:193][0m |          -0.0071 |           2.4488 |           2.0682 |
[32m[20230203 20:52:19 @agent_ppo2.py:193][0m |          -0.0078 |           2.3319 |           2.0686 |
[32m[20230203 20:52:20 @agent_ppo2.py:193][0m |          -0.0083 |           2.2319 |           2.0691 |
[32m[20230203 20:52:20 @agent_ppo2.py:193][0m |          -0.0085 |           2.1434 |           2.0690 |
[32m[20230203 20:52:20 @agent_ppo2.py:193][0m |          -0.0091 |           2.0727 |           2.0693 |
[32m[20230203 20:52:20 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:52:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: -59.58
[32m[20230203 20:52:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -55.86
[32m[20230203 20:52:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -47.82
[32m[20230203 20:52:20 @agent_ppo2.py:151][0m Total time:       1.75 min
[32m[20230203 20:52:20 @agent_ppo2.py:153][0m 124928 total steps have happened
[32m[20230203 20:52:20 @agent_ppo2.py:129][0m #------------------------ Iteration 61 --------------------------#
[32m[20230203 20:52:21 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:52:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:21 @agent_ppo2.py:193][0m |          -0.0004 |           0.9432 |           2.1434 |
[32m[20230203 20:52:21 @agent_ppo2.py:193][0m |          -0.0038 |           0.8893 |           2.1413 |
[32m[20230203 20:52:21 @agent_ppo2.py:193][0m |          -0.0055 |           0.8591 |           2.1416 |
[32m[20230203 20:52:21 @agent_ppo2.py:193][0m |          -0.0063 |           0.8380 |           2.1410 |
[32m[20230203 20:52:21 @agent_ppo2.py:193][0m |          -0.0070 |           0.8238 |           2.1419 |
[32m[20230203 20:52:21 @agent_ppo2.py:193][0m |          -0.0077 |           0.8113 |           2.1426 |
[32m[20230203 20:52:21 @agent_ppo2.py:193][0m |          -0.0081 |           0.8010 |           2.1426 |
[32m[20230203 20:52:21 @agent_ppo2.py:193][0m |          -0.0084 |           0.7944 |           2.1426 |
[32m[20230203 20:52:21 @agent_ppo2.py:193][0m |          -0.0089 |           0.7847 |           2.1421 |
[32m[20230203 20:52:21 @agent_ppo2.py:193][0m |          -0.0092 |           0.7780 |           2.1435 |
[32m[20230203 20:52:21 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:52:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: -65.67
[32m[20230203 20:52:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -61.08
[32m[20230203 20:52:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -45.61
[32m[20230203 20:52:22 @agent_ppo2.py:151][0m Total time:       1.78 min
[32m[20230203 20:52:22 @agent_ppo2.py:153][0m 126976 total steps have happened
[32m[20230203 20:52:22 @agent_ppo2.py:129][0m #------------------------ Iteration 62 --------------------------#
[32m[20230203 20:52:22 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:52:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:23 @agent_ppo2.py:193][0m |          -0.0016 |           1.9203 |           2.1720 |
[32m[20230203 20:52:23 @agent_ppo2.py:193][0m |          -0.0055 |           1.4578 |           2.1737 |
[32m[20230203 20:52:23 @agent_ppo2.py:193][0m |          -0.0082 |           1.3324 |           2.1738 |
[32m[20230203 20:52:23 @agent_ppo2.py:193][0m |          -0.0097 |           1.2680 |           2.1731 |
[32m[20230203 20:52:23 @agent_ppo2.py:193][0m |          -0.0107 |           1.2310 |           2.1723 |
[32m[20230203 20:52:23 @agent_ppo2.py:193][0m |          -0.0115 |           1.2118 |           2.1726 |
[32m[20230203 20:52:23 @agent_ppo2.py:193][0m |          -0.0121 |           1.1774 |           2.1727 |
[32m[20230203 20:52:23 @agent_ppo2.py:193][0m |          -0.0126 |           1.1566 |           2.1720 |
[32m[20230203 20:52:23 @agent_ppo2.py:193][0m |          -0.0129 |           1.1457 |           2.1727 |
[32m[20230203 20:52:23 @agent_ppo2.py:193][0m |          -0.0133 |           1.1253 |           2.1728 |
[32m[20230203 20:52:23 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:52:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: -57.77
[32m[20230203 20:52:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -57.72
[32m[20230203 20:52:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -90.74
[32m[20230203 20:52:24 @agent_ppo2.py:151][0m Total time:       1.81 min
[32m[20230203 20:52:24 @agent_ppo2.py:153][0m 129024 total steps have happened
[32m[20230203 20:52:24 @agent_ppo2.py:129][0m #------------------------ Iteration 63 --------------------------#
[32m[20230203 20:52:24 @agent_ppo2.py:135][0m Sampling time: 0.59 s by 1 slaves
[32m[20230203 20:52:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:24 @agent_ppo2.py:193][0m |          -0.0018 |           0.8565 |           2.1374 |
[32m[20230203 20:52:24 @agent_ppo2.py:193][0m |          -0.0059 |           0.8040 |           2.1382 |
[32m[20230203 20:52:24 @agent_ppo2.py:193][0m |          -0.0072 |           0.7812 |           2.1398 |
[32m[20230203 20:52:24 @agent_ppo2.py:193][0m |          -0.0080 |           0.7712 |           2.1392 |
[32m[20230203 20:52:25 @agent_ppo2.py:193][0m |          -0.0089 |           0.7649 |           2.1391 |
[32m[20230203 20:52:25 @agent_ppo2.py:193][0m |          -0.0098 |           0.7567 |           2.1387 |
[32m[20230203 20:52:25 @agent_ppo2.py:193][0m |          -0.0105 |           0.7515 |           2.1390 |
[32m[20230203 20:52:25 @agent_ppo2.py:193][0m |          -0.0109 |           0.7480 |           2.1405 |
[32m[20230203 20:52:25 @agent_ppo2.py:193][0m |          -0.0112 |           0.7448 |           2.1399 |
[32m[20230203 20:52:25 @agent_ppo2.py:193][0m |          -0.0119 |           0.7404 |           2.1403 |
[32m[20230203 20:52:25 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:52:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: -55.39
[32m[20230203 20:52:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -54.91
[32m[20230203 20:52:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -49.89
[32m[20230203 20:52:25 @agent_ppo2.py:151][0m Total time:       1.84 min
[32m[20230203 20:52:25 @agent_ppo2.py:153][0m 131072 total steps have happened
[32m[20230203 20:52:25 @agent_ppo2.py:129][0m #------------------------ Iteration 64 --------------------------#
[32m[20230203 20:52:26 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:52:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:26 @agent_ppo2.py:193][0m |          -0.0004 |           1.2164 |           2.1042 |
[32m[20230203 20:52:26 @agent_ppo2.py:193][0m |          -0.0049 |           1.0691 |           2.1046 |
[32m[20230203 20:52:26 @agent_ppo2.py:193][0m |          -0.0072 |           1.0287 |           2.1039 |
[32m[20230203 20:52:26 @agent_ppo2.py:193][0m |          -0.0086 |           1.0049 |           2.1037 |
[32m[20230203 20:52:26 @agent_ppo2.py:193][0m |          -0.0094 |           0.9760 |           2.1042 |
[32m[20230203 20:52:26 @agent_ppo2.py:193][0m |          -0.0104 |           0.9598 |           2.1039 |
[32m[20230203 20:52:26 @agent_ppo2.py:193][0m |          -0.0109 |           0.9430 |           2.1043 |
[32m[20230203 20:52:26 @agent_ppo2.py:193][0m |          -0.0114 |           0.9295 |           2.1047 |
[32m[20230203 20:52:26 @agent_ppo2.py:193][0m |          -0.0116 |           0.9187 |           2.1057 |
[32m[20230203 20:52:27 @agent_ppo2.py:193][0m |          -0.0121 |           0.9154 |           2.1061 |
[32m[20230203 20:52:27 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:52:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: -47.92
[32m[20230203 20:52:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -44.47
[32m[20230203 20:52:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -34.29
[32m[20230203 20:52:27 @agent_ppo2.py:151][0m Total time:       1.86 min
[32m[20230203 20:52:27 @agent_ppo2.py:153][0m 133120 total steps have happened
[32m[20230203 20:52:27 @agent_ppo2.py:129][0m #------------------------ Iteration 65 --------------------------#
[32m[20230203 20:52:28 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:52:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:28 @agent_ppo2.py:193][0m |          -0.0018 |           0.9376 |           2.1533 |
[32m[20230203 20:52:28 @agent_ppo2.py:193][0m |          -0.0079 |           0.8486 |           2.1523 |
[32m[20230203 20:52:28 @agent_ppo2.py:193][0m |          -0.0101 |           0.8180 |           2.1505 |
[32m[20230203 20:52:28 @agent_ppo2.py:193][0m |          -0.0122 |           0.8028 |           2.1491 |
[32m[20230203 20:52:28 @agent_ppo2.py:193][0m |          -0.0130 |           0.7913 |           2.1489 |
[32m[20230203 20:52:28 @agent_ppo2.py:193][0m |          -0.0138 |           0.7857 |           2.1493 |
[32m[20230203 20:52:28 @agent_ppo2.py:193][0m |          -0.0142 |           0.7815 |           2.1487 |
[32m[20230203 20:52:28 @agent_ppo2.py:193][0m |          -0.0143 |           0.7763 |           2.1494 |
[32m[20230203 20:52:28 @agent_ppo2.py:193][0m |          -0.0148 |           0.7711 |           2.1499 |
[32m[20230203 20:52:28 @agent_ppo2.py:193][0m |          -0.0153 |           0.7663 |           2.1491 |
[32m[20230203 20:52:28 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: -43.43
[32m[20230203 20:52:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -39.41
[32m[20230203 20:52:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 26.62
[32m[20230203 20:52:29 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 26.62
[32m[20230203 20:52:29 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 26.62
[32m[20230203 20:52:29 @agent_ppo2.py:151][0m Total time:       1.89 min
[32m[20230203 20:52:29 @agent_ppo2.py:153][0m 135168 total steps have happened
[32m[20230203 20:52:29 @agent_ppo2.py:129][0m #------------------------ Iteration 66 --------------------------#
[32m[20230203 20:52:29 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:52:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:29 @agent_ppo2.py:193][0m |           0.0008 |           0.8333 |           2.1978 |
[32m[20230203 20:52:29 @agent_ppo2.py:193][0m |          -0.0021 |           0.7462 |           2.1977 |
[32m[20230203 20:52:30 @agent_ppo2.py:193][0m |          -0.0041 |           0.7218 |           2.1977 |
[32m[20230203 20:52:30 @agent_ppo2.py:193][0m |          -0.0054 |           0.7137 |           2.1968 |
[32m[20230203 20:52:30 @agent_ppo2.py:193][0m |          -0.0062 |           0.7030 |           2.1967 |
[32m[20230203 20:52:30 @agent_ppo2.py:193][0m |          -0.0078 |           0.6932 |           2.1971 |
[32m[20230203 20:52:30 @agent_ppo2.py:193][0m |          -0.0086 |           0.6894 |           2.1979 |
[32m[20230203 20:52:30 @agent_ppo2.py:193][0m |          -0.0091 |           0.6855 |           2.1975 |
[32m[20230203 20:52:30 @agent_ppo2.py:193][0m |          -0.0096 |           0.6832 |           2.1963 |
[32m[20230203 20:52:30 @agent_ppo2.py:193][0m |          -0.0104 |           0.6796 |           2.1972 |
[32m[20230203 20:52:30 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:52:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: -35.87
[32m[20230203 20:52:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -34.05
[32m[20230203 20:52:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 25.00
[32m[20230203 20:52:30 @agent_ppo2.py:151][0m Total time:       1.92 min
[32m[20230203 20:52:30 @agent_ppo2.py:153][0m 137216 total steps have happened
[32m[20230203 20:52:30 @agent_ppo2.py:129][0m #------------------------ Iteration 67 --------------------------#
[32m[20230203 20:52:31 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:52:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:31 @agent_ppo2.py:193][0m |          -0.0015 |           0.8169 |           2.2228 |
[32m[20230203 20:52:31 @agent_ppo2.py:193][0m |          -0.0067 |           0.7621 |           2.2235 |
[32m[20230203 20:52:31 @agent_ppo2.py:193][0m |          -0.0084 |           0.7486 |           2.2243 |
[32m[20230203 20:52:31 @agent_ppo2.py:193][0m |          -0.0094 |           0.7387 |           2.2247 |
[32m[20230203 20:52:31 @agent_ppo2.py:193][0m |          -0.0104 |           0.7320 |           2.2257 |
[32m[20230203 20:52:31 @agent_ppo2.py:193][0m |          -0.0112 |           0.7291 |           2.2276 |
[32m[20230203 20:52:31 @agent_ppo2.py:193][0m |          -0.0119 |           0.7245 |           2.2281 |
[32m[20230203 20:52:31 @agent_ppo2.py:193][0m |          -0.0121 |           0.7220 |           2.2292 |
[32m[20230203 20:52:32 @agent_ppo2.py:193][0m |          -0.0126 |           0.7167 |           2.2299 |
[32m[20230203 20:52:32 @agent_ppo2.py:193][0m |          -0.0127 |           0.7148 |           2.2307 |
[32m[20230203 20:52:32 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: -29.81
[32m[20230203 20:52:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -26.88
[32m[20230203 20:52:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -12.09
[32m[20230203 20:52:32 @agent_ppo2.py:151][0m Total time:       1.95 min
[32m[20230203 20:52:32 @agent_ppo2.py:153][0m 139264 total steps have happened
[32m[20230203 20:52:32 @agent_ppo2.py:129][0m #------------------------ Iteration 68 --------------------------#
[32m[20230203 20:52:33 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:52:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:33 @agent_ppo2.py:193][0m |          -0.0003 |           1.2253 |           2.2479 |
[32m[20230203 20:52:33 @agent_ppo2.py:193][0m |          -0.0055 |           1.1374 |           2.2472 |
[32m[20230203 20:52:33 @agent_ppo2.py:193][0m |          -0.0079 |           1.0964 |           2.2452 |
[32m[20230203 20:52:33 @agent_ppo2.py:193][0m |          -0.0092 |           1.0609 |           2.2442 |
[32m[20230203 20:52:33 @agent_ppo2.py:193][0m |          -0.0098 |           1.0513 |           2.2437 |
[32m[20230203 20:52:33 @agent_ppo2.py:193][0m |          -0.0113 |           1.0335 |           2.2439 |
[32m[20230203 20:52:33 @agent_ppo2.py:193][0m |          -0.0122 |           1.0372 |           2.2455 |
[32m[20230203 20:52:33 @agent_ppo2.py:193][0m |          -0.0130 |           1.0239 |           2.2465 |
[32m[20230203 20:52:33 @agent_ppo2.py:193][0m |          -0.0135 |           1.0104 |           2.2468 |
[32m[20230203 20:52:33 @agent_ppo2.py:193][0m |          -0.0138 |           1.0022 |           2.2474 |
[32m[20230203 20:52:33 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: -24.13
[32m[20230203 20:52:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -19.63
[32m[20230203 20:52:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -39.05
[32m[20230203 20:52:34 @agent_ppo2.py:151][0m Total time:       1.98 min
[32m[20230203 20:52:34 @agent_ppo2.py:153][0m 141312 total steps have happened
[32m[20230203 20:52:34 @agent_ppo2.py:129][0m #------------------------ Iteration 69 --------------------------#
[32m[20230203 20:52:34 @agent_ppo2.py:135][0m Sampling time: 0.58 s by 1 slaves
[32m[20230203 20:52:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:35 @agent_ppo2.py:193][0m |          -0.0012 |           8.6643 |           2.2525 |
[32m[20230203 20:52:35 @agent_ppo2.py:193][0m |          -0.0080 |           4.5190 |           2.2494 |
[32m[20230203 20:52:35 @agent_ppo2.py:193][0m |          -0.0094 |           3.5422 |           2.2492 |
[32m[20230203 20:52:35 @agent_ppo2.py:193][0m |          -0.0081 |           3.0857 |           2.2465 |
[32m[20230203 20:52:35 @agent_ppo2.py:193][0m |          -0.0041 |           2.9520 |           2.2452 |
[32m[20230203 20:52:35 @agent_ppo2.py:193][0m |          -0.0070 |           2.7309 |           2.2442 |
[32m[20230203 20:52:35 @agent_ppo2.py:193][0m |          -0.0126 |           2.6515 |           2.2424 |
[32m[20230203 20:52:35 @agent_ppo2.py:193][0m |          -0.0129 |           2.4952 |           2.2427 |
[32m[20230203 20:52:35 @agent_ppo2.py:193][0m |          -0.0137 |           2.3900 |           2.2417 |
[32m[20230203 20:52:35 @agent_ppo2.py:193][0m |          -0.0145 |           2.3219 |           2.2406 |
[32m[20230203 20:52:35 @agent_ppo2.py:138][0m Policy update time: 0.64 s
[32m[20230203 20:52:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: -59.89
[32m[20230203 20:52:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -8.95
[32m[20230203 20:52:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 55.00
[32m[20230203 20:52:36 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 55.00
[32m[20230203 20:52:36 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 55.00
[32m[20230203 20:52:36 @agent_ppo2.py:151][0m Total time:       2.01 min
[32m[20230203 20:52:36 @agent_ppo2.py:153][0m 143360 total steps have happened
[32m[20230203 20:52:36 @agent_ppo2.py:129][0m #------------------------ Iteration 70 --------------------------#
[32m[20230203 20:52:36 @agent_ppo2.py:135][0m Sampling time: 0.57 s by 1 slaves
[32m[20230203 20:52:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:36 @agent_ppo2.py:193][0m |          -0.0012 |           1.0582 |           2.2482 |
[32m[20230203 20:52:36 @agent_ppo2.py:193][0m |          -0.0056 |           0.9130 |           2.2472 |
[32m[20230203 20:52:36 @agent_ppo2.py:193][0m |          -0.0077 |           0.8467 |           2.2471 |
[32m[20230203 20:52:36 @agent_ppo2.py:193][0m |          -0.0082 |           0.8215 |           2.2470 |
[32m[20230203 20:52:36 @agent_ppo2.py:193][0m |          -0.0099 |           0.8096 |           2.2470 |
[32m[20230203 20:52:37 @agent_ppo2.py:193][0m |          -0.0106 |           0.7989 |           2.2488 |
[32m[20230203 20:52:37 @agent_ppo2.py:193][0m |          -0.0107 |           0.7864 |           2.2490 |
[32m[20230203 20:52:37 @agent_ppo2.py:193][0m |          -0.0118 |           0.7779 |           2.2490 |
[32m[20230203 20:52:37 @agent_ppo2.py:193][0m |          -0.0121 |           0.7746 |           2.2500 |
[32m[20230203 20:52:37 @agent_ppo2.py:193][0m |          -0.0126 |           0.7626 |           2.2509 |
[32m[20230203 20:52:37 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: -0.25
[32m[20230203 20:52:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 4.83
[32m[20230203 20:52:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 57.38
[32m[20230203 20:52:37 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 57.38
[32m[20230203 20:52:37 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 57.38
[32m[20230203 20:52:37 @agent_ppo2.py:151][0m Total time:       2.03 min
[32m[20230203 20:52:37 @agent_ppo2.py:153][0m 145408 total steps have happened
[32m[20230203 20:52:37 @agent_ppo2.py:129][0m #------------------------ Iteration 71 --------------------------#
[32m[20230203 20:52:38 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:52:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:38 @agent_ppo2.py:193][0m |          -0.0006 |           1.1336 |           2.2203 |
[32m[20230203 20:52:38 @agent_ppo2.py:193][0m |          -0.0044 |           1.0045 |           2.2193 |
[32m[20230203 20:52:38 @agent_ppo2.py:193][0m |          -0.0070 |           0.9493 |           2.2194 |
[32m[20230203 20:52:38 @agent_ppo2.py:193][0m |          -0.0079 |           0.8998 |           2.2191 |
[32m[20230203 20:52:38 @agent_ppo2.py:193][0m |          -0.0092 |           0.8793 |           2.2201 |
[32m[20230203 20:52:38 @agent_ppo2.py:193][0m |          -0.0099 |           0.8557 |           2.2217 |
[32m[20230203 20:52:38 @agent_ppo2.py:193][0m |          -0.0109 |           0.8483 |           2.2217 |
[32m[20230203 20:52:38 @agent_ppo2.py:193][0m |          -0.0116 |           0.8300 |           2.2227 |
[32m[20230203 20:52:38 @agent_ppo2.py:193][0m |          -0.0122 |           0.8197 |           2.2230 |
[32m[20230203 20:52:38 @agent_ppo2.py:193][0m |          -0.0125 |           0.8093 |           2.2244 |
[32m[20230203 20:52:38 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 1.30
[32m[20230203 20:52:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 1.37
[32m[20230203 20:52:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 75.26
[32m[20230203 20:52:39 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 75.26
[32m[20230203 20:52:39 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 75.26
[32m[20230203 20:52:39 @agent_ppo2.py:151][0m Total time:       2.06 min
[32m[20230203 20:52:39 @agent_ppo2.py:153][0m 147456 total steps have happened
[32m[20230203 20:52:39 @agent_ppo2.py:129][0m #------------------------ Iteration 72 --------------------------#
[32m[20230203 20:52:39 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:52:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:40 @agent_ppo2.py:193][0m |          -0.0032 |           0.9409 |           2.2761 |
[32m[20230203 20:52:40 @agent_ppo2.py:193][0m |          -0.0078 |           0.7965 |           2.2728 |
[32m[20230203 20:52:40 @agent_ppo2.py:193][0m |          -0.0096 |           0.7722 |           2.2720 |
[32m[20230203 20:52:40 @agent_ppo2.py:193][0m |          -0.0107 |           0.7507 |           2.2704 |
[32m[20230203 20:52:40 @agent_ppo2.py:193][0m |          -0.0114 |           0.7398 |           2.2723 |
[32m[20230203 20:52:40 @agent_ppo2.py:193][0m |          -0.0125 |           0.7252 |           2.2711 |
[32m[20230203 20:52:40 @agent_ppo2.py:193][0m |          -0.0127 |           0.7165 |           2.2709 |
[32m[20230203 20:52:40 @agent_ppo2.py:193][0m |          -0.0136 |           0.7085 |           2.2713 |
[32m[20230203 20:52:40 @agent_ppo2.py:193][0m |          -0.0134 |           0.7037 |           2.2725 |
[32m[20230203 20:52:40 @agent_ppo2.py:193][0m |          -0.0137 |           0.6960 |           2.2712 |
[32m[20230203 20:52:40 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 10.97
[32m[20230203 20:52:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 20.62
[32m[20230203 20:52:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 23.53
[32m[20230203 20:52:41 @agent_ppo2.py:151][0m Total time:       2.09 min
[32m[20230203 20:52:41 @agent_ppo2.py:153][0m 149504 total steps have happened
[32m[20230203 20:52:41 @agent_ppo2.py:129][0m #------------------------ Iteration 73 --------------------------#
[32m[20230203 20:52:41 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:52:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:41 @agent_ppo2.py:193][0m |          -0.0018 |           1.0953 |           2.2303 |
[32m[20230203 20:52:41 @agent_ppo2.py:193][0m |          -0.0081 |           0.9218 |           2.2276 |
[32m[20230203 20:52:41 @agent_ppo2.py:193][0m |          -0.0101 |           0.8667 |           2.2274 |
[32m[20230203 20:52:41 @agent_ppo2.py:193][0m |          -0.0120 |           0.8509 |           2.2261 |
[32m[20230203 20:52:41 @agent_ppo2.py:193][0m |          -0.0126 |           0.8326 |           2.2250 |
[32m[20230203 20:52:42 @agent_ppo2.py:193][0m |          -0.0131 |           0.8155 |           2.2248 |
[32m[20230203 20:52:42 @agent_ppo2.py:193][0m |          -0.0140 |           0.8118 |           2.2260 |
[32m[20230203 20:52:42 @agent_ppo2.py:193][0m |          -0.0145 |           0.7951 |           2.2246 |
[32m[20230203 20:52:42 @agent_ppo2.py:193][0m |          -0.0147 |           0.7887 |           2.2250 |
[32m[20230203 20:52:42 @agent_ppo2.py:193][0m |          -0.0149 |           0.7834 |           2.2255 |
[32m[20230203 20:52:42 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 24.63
[32m[20230203 20:52:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 34.85
[32m[20230203 20:52:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 22.93
[32m[20230203 20:52:42 @agent_ppo2.py:151][0m Total time:       2.12 min
[32m[20230203 20:52:42 @agent_ppo2.py:153][0m 151552 total steps have happened
[32m[20230203 20:52:42 @agent_ppo2.py:129][0m #------------------------ Iteration 74 --------------------------#
[32m[20230203 20:52:43 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:52:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:43 @agent_ppo2.py:193][0m |          -0.0015 |           1.0056 |           2.2801 |
[32m[20230203 20:52:43 @agent_ppo2.py:193][0m |          -0.0078 |           0.8464 |           2.2794 |
[32m[20230203 20:52:43 @agent_ppo2.py:193][0m |          -0.0108 |           0.8242 |           2.2781 |
[32m[20230203 20:52:43 @agent_ppo2.py:193][0m |          -0.0123 |           0.8088 |           2.2785 |
[32m[20230203 20:52:43 @agent_ppo2.py:193][0m |          -0.0134 |           0.8009 |           2.2784 |
[32m[20230203 20:52:43 @agent_ppo2.py:193][0m |          -0.0138 |           0.7916 |           2.2790 |
[32m[20230203 20:52:43 @agent_ppo2.py:193][0m |          -0.0147 |           0.7875 |           2.2779 |
[32m[20230203 20:52:43 @agent_ppo2.py:193][0m |          -0.0149 |           0.7800 |           2.2798 |
[32m[20230203 20:52:43 @agent_ppo2.py:193][0m |          -0.0159 |           0.7745 |           2.2793 |
[32m[20230203 20:52:43 @agent_ppo2.py:193][0m |          -0.0164 |           0.7675 |           2.2798 |
[32m[20230203 20:52:43 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 40.56
[32m[20230203 20:52:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 45.81
[32m[20230203 20:52:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 101.55
[32m[20230203 20:52:44 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 101.55
[32m[20230203 20:52:44 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 101.55
[32m[20230203 20:52:44 @agent_ppo2.py:151][0m Total time:       2.15 min
[32m[20230203 20:52:44 @agent_ppo2.py:153][0m 153600 total steps have happened
[32m[20230203 20:52:44 @agent_ppo2.py:129][0m #------------------------ Iteration 75 --------------------------#
[32m[20230203 20:52:45 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:52:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:45 @agent_ppo2.py:193][0m |          -0.0019 |           1.2227 |           2.2798 |
[32m[20230203 20:52:45 @agent_ppo2.py:193][0m |          -0.0085 |           0.9968 |           2.2756 |
[32m[20230203 20:52:45 @agent_ppo2.py:193][0m |          -0.0100 |           0.9531 |           2.2730 |
[32m[20230203 20:52:45 @agent_ppo2.py:193][0m |          -0.0109 |           0.9353 |           2.2731 |
[32m[20230203 20:52:45 @agent_ppo2.py:193][0m |          -0.0114 |           0.9156 |           2.2721 |
[32m[20230203 20:52:45 @agent_ppo2.py:193][0m |          -0.0123 |           0.9020 |           2.2711 |
[32m[20230203 20:52:45 @agent_ppo2.py:193][0m |          -0.0133 |           0.8891 |           2.2716 |
[32m[20230203 20:52:45 @agent_ppo2.py:193][0m |          -0.0136 |           0.8799 |           2.2731 |
[32m[20230203 20:52:45 @agent_ppo2.py:193][0m |          -0.0144 |           0.8728 |           2.2705 |
[32m[20230203 20:52:45 @agent_ppo2.py:193][0m |          -0.0143 |           0.8667 |           2.2713 |
[32m[20230203 20:52:45 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:52:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 42.43
[32m[20230203 20:52:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 47.66
[32m[20230203 20:52:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 130.63
[32m[20230203 20:52:46 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 130.63
[32m[20230203 20:52:46 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 130.63
[32m[20230203 20:52:46 @agent_ppo2.py:151][0m Total time:       2.17 min
[32m[20230203 20:52:46 @agent_ppo2.py:153][0m 155648 total steps have happened
[32m[20230203 20:52:46 @agent_ppo2.py:129][0m #------------------------ Iteration 76 --------------------------#
[32m[20230203 20:52:46 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:52:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:46 @agent_ppo2.py:193][0m |          -0.0044 |           1.0770 |           2.3682 |
[32m[20230203 20:52:46 @agent_ppo2.py:193][0m |          -0.0092 |           0.8843 |           2.3659 |
[32m[20230203 20:52:46 @agent_ppo2.py:193][0m |          -0.0105 |           0.8495 |           2.3671 |
[32m[20230203 20:52:46 @agent_ppo2.py:193][0m |          -0.0121 |           0.8206 |           2.3680 |
[32m[20230203 20:52:47 @agent_ppo2.py:193][0m |          -0.0130 |           0.8031 |           2.3693 |
[32m[20230203 20:52:47 @agent_ppo2.py:193][0m |          -0.0134 |           0.7898 |           2.3706 |
[32m[20230203 20:52:47 @agent_ppo2.py:193][0m |          -0.0143 |           0.7807 |           2.3705 |
[32m[20230203 20:52:47 @agent_ppo2.py:193][0m |          -0.0149 |           0.7693 |           2.3719 |
[32m[20230203 20:52:47 @agent_ppo2.py:193][0m |          -0.0153 |           0.7593 |           2.3714 |
[32m[20230203 20:52:47 @agent_ppo2.py:193][0m |          -0.0153 |           0.7564 |           2.3741 |
[32m[20230203 20:52:47 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:52:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 71.59
[32m[20230203 20:52:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 72.44
[32m[20230203 20:52:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 54.23
[32m[20230203 20:52:47 @agent_ppo2.py:151][0m Total time:       2.20 min
[32m[20230203 20:52:47 @agent_ppo2.py:153][0m 157696 total steps have happened
[32m[20230203 20:52:47 @agent_ppo2.py:129][0m #------------------------ Iteration 77 --------------------------#
[32m[20230203 20:52:48 @agent_ppo2.py:135][0m Sampling time: 0.57 s by 1 slaves
[32m[20230203 20:52:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:48 @agent_ppo2.py:193][0m |           0.0174 |           4.1413 |           2.2847 |
[32m[20230203 20:52:48 @agent_ppo2.py:193][0m |          -0.0083 |           1.5253 |           2.2817 |
[32m[20230203 20:52:48 @agent_ppo2.py:193][0m |          -0.0115 |           1.3033 |           2.2801 |
[32m[20230203 20:52:48 @agent_ppo2.py:193][0m |          -0.0119 |           1.2600 |           2.2796 |
[32m[20230203 20:52:48 @agent_ppo2.py:193][0m |          -0.0122 |           1.1522 |           2.2786 |
[32m[20230203 20:52:48 @agent_ppo2.py:193][0m |          -0.0145 |           1.1044 |           2.2798 |
[32m[20230203 20:52:48 @agent_ppo2.py:193][0m |          -0.0144 |           1.1142 |           2.2798 |
[32m[20230203 20:52:48 @agent_ppo2.py:193][0m |          -0.0142 |           1.0605 |           2.2801 |
[32m[20230203 20:52:48 @agent_ppo2.py:193][0m |          -0.0080 |           1.0642 |           2.2794 |
[32m[20230203 20:52:49 @agent_ppo2.py:193][0m |          -0.0172 |           1.0493 |           2.2810 |
[32m[20230203 20:52:49 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:52:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 5.99
[32m[20230203 20:52:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 75.83
[32m[20230203 20:52:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 177.31
[32m[20230203 20:52:49 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 177.31
[32m[20230203 20:52:49 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 177.31
[32m[20230203 20:52:49 @agent_ppo2.py:151][0m Total time:       2.23 min
[32m[20230203 20:52:49 @agent_ppo2.py:153][0m 159744 total steps have happened
[32m[20230203 20:52:49 @agent_ppo2.py:129][0m #------------------------ Iteration 78 --------------------------#
[32m[20230203 20:52:50 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:52:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:50 @agent_ppo2.py:193][0m |          -0.0006 |           1.5504 |           2.3250 |
[32m[20230203 20:52:50 @agent_ppo2.py:193][0m |          -0.0063 |           1.1927 |           2.3239 |
[32m[20230203 20:52:50 @agent_ppo2.py:193][0m |          -0.0087 |           1.1211 |           2.3233 |
[32m[20230203 20:52:50 @agent_ppo2.py:193][0m |          -0.0104 |           1.0506 |           2.3226 |
[32m[20230203 20:52:50 @agent_ppo2.py:193][0m |          -0.0122 |           1.0048 |           2.3234 |
[32m[20230203 20:52:50 @agent_ppo2.py:193][0m |          -0.0132 |           0.9604 |           2.3245 |
[32m[20230203 20:52:50 @agent_ppo2.py:193][0m |          -0.0144 |           0.9302 |           2.3249 |
[32m[20230203 20:52:50 @agent_ppo2.py:193][0m |          -0.0151 |           0.9063 |           2.3271 |
[32m[20230203 20:52:50 @agent_ppo2.py:193][0m |          -0.0163 |           0.8883 |           2.3273 |
[32m[20230203 20:52:50 @agent_ppo2.py:193][0m |          -0.0168 |           0.8714 |           2.3276 |
[32m[20230203 20:52:50 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 81.95
[32m[20230203 20:52:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 81.98
[32m[20230203 20:52:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -21.72
[32m[20230203 20:52:51 @agent_ppo2.py:151][0m Total time:       2.26 min
[32m[20230203 20:52:51 @agent_ppo2.py:153][0m 161792 total steps have happened
[32m[20230203 20:52:51 @agent_ppo2.py:129][0m #------------------------ Iteration 79 --------------------------#
[32m[20230203 20:52:51 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:52:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:51 @agent_ppo2.py:193][0m |          -0.0029 |           1.1900 |           2.4023 |
[32m[20230203 20:52:51 @agent_ppo2.py:193][0m |          -0.0071 |           0.9007 |           2.3991 |
[32m[20230203 20:52:51 @agent_ppo2.py:193][0m |          -0.0091 |           0.8656 |           2.3988 |
[32m[20230203 20:52:51 @agent_ppo2.py:193][0m |          -0.0107 |           0.8473 |           2.4010 |
[32m[20230203 20:52:52 @agent_ppo2.py:193][0m |          -0.0116 |           0.8317 |           2.4020 |
[32m[20230203 20:52:52 @agent_ppo2.py:193][0m |          -0.0125 |           0.8219 |           2.4029 |
[32m[20230203 20:52:52 @agent_ppo2.py:193][0m |          -0.0131 |           0.8166 |           2.4038 |
[32m[20230203 20:52:52 @agent_ppo2.py:193][0m |          -0.0138 |           0.8066 |           2.4051 |
[32m[20230203 20:52:52 @agent_ppo2.py:193][0m |          -0.0141 |           0.7975 |           2.4060 |
[32m[20230203 20:52:52 @agent_ppo2.py:193][0m |          -0.0145 |           0.7932 |           2.4062 |
[32m[20230203 20:52:52 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 102.02
[32m[20230203 20:52:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 104.79
[32m[20230203 20:52:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -51.98
[32m[20230203 20:52:52 @agent_ppo2.py:151][0m Total time:       2.28 min
[32m[20230203 20:52:52 @agent_ppo2.py:153][0m 163840 total steps have happened
[32m[20230203 20:52:52 @agent_ppo2.py:129][0m #------------------------ Iteration 80 --------------------------#
[32m[20230203 20:52:53 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:52:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:53 @agent_ppo2.py:193][0m |           0.0001 |           1.8559 |           2.4335 |
[32m[20230203 20:52:53 @agent_ppo2.py:193][0m |          -0.0052 |           1.4922 |           2.4315 |
[32m[20230203 20:52:53 @agent_ppo2.py:193][0m |          -0.0079 |           1.3901 |           2.4295 |
[32m[20230203 20:52:53 @agent_ppo2.py:193][0m |          -0.0091 |           1.3420 |           2.4282 |
[32m[20230203 20:52:53 @agent_ppo2.py:193][0m |          -0.0102 |           1.3085 |           2.4288 |
[32m[20230203 20:52:53 @agent_ppo2.py:193][0m |          -0.0107 |           1.3128 |           2.4307 |
[32m[20230203 20:52:53 @agent_ppo2.py:193][0m |          -0.0117 |           1.2500 |           2.4288 |
[32m[20230203 20:52:53 @agent_ppo2.py:193][0m |          -0.0121 |           1.2304 |           2.4304 |
[32m[20230203 20:52:53 @agent_ppo2.py:193][0m |          -0.0127 |           1.2189 |           2.4334 |
[32m[20230203 20:52:53 @agent_ppo2.py:193][0m |          -0.0133 |           1.1929 |           2.4318 |
[32m[20230203 20:52:53 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 88.24
[32m[20230203 20:52:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 101.14
[32m[20230203 20:52:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -65.22
[32m[20230203 20:52:54 @agent_ppo2.py:151][0m Total time:       2.31 min
[32m[20230203 20:52:54 @agent_ppo2.py:153][0m 165888 total steps have happened
[32m[20230203 20:52:54 @agent_ppo2.py:129][0m #------------------------ Iteration 81 --------------------------#
[32m[20230203 20:52:54 @agent_ppo2.py:135][0m Sampling time: 0.53 s by 1 slaves
[32m[20230203 20:52:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:54 @agent_ppo2.py:193][0m |          -0.0032 |           8.3338 |           2.3612 |
[32m[20230203 20:52:54 @agent_ppo2.py:193][0m |          -0.0059 |           3.6142 |           2.3568 |
[32m[20230203 20:52:54 @agent_ppo2.py:193][0m |          -0.0024 |           3.0093 |           2.3579 |
[32m[20230203 20:52:54 @agent_ppo2.py:193][0m |          -0.0133 |           2.6904 |           2.3554 |
[32m[20230203 20:52:55 @agent_ppo2.py:193][0m |          -0.0156 |           2.6003 |           2.3545 |
[32m[20230203 20:52:55 @agent_ppo2.py:193][0m |          -0.0188 |           2.3462 |           2.3557 |
[32m[20230203 20:52:55 @agent_ppo2.py:193][0m |          -0.0093 |           2.2827 |           2.3562 |
[32m[20230203 20:52:55 @agent_ppo2.py:193][0m |          -0.0123 |           2.1453 |           2.3566 |
[32m[20230203 20:52:55 @agent_ppo2.py:193][0m |          -0.0112 |           2.1154 |           2.3546 |
[32m[20230203 20:52:55 @agent_ppo2.py:193][0m |          -0.0141 |           2.0317 |           2.3580 |
[32m[20230203 20:52:55 @agent_ppo2.py:138][0m Policy update time: 0.59 s
[32m[20230203 20:52:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 4.03
[32m[20230203 20:52:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 129.84
[32m[20230203 20:52:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 13.64
[32m[20230203 20:52:55 @agent_ppo2.py:151][0m Total time:       2.34 min
[32m[20230203 20:52:55 @agent_ppo2.py:153][0m 167936 total steps have happened
[32m[20230203 20:52:55 @agent_ppo2.py:129][0m #------------------------ Iteration 82 --------------------------#
[32m[20230203 20:52:56 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:52:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:56 @agent_ppo2.py:193][0m |          -0.0011 |           1.6448 |           2.4544 |
[32m[20230203 20:52:56 @agent_ppo2.py:193][0m |          -0.0049 |           1.3483 |           2.4532 |
[32m[20230203 20:52:56 @agent_ppo2.py:193][0m |          -0.0063 |           1.2771 |           2.4541 |
[32m[20230203 20:52:56 @agent_ppo2.py:193][0m |          -0.0074 |           1.2355 |           2.4567 |
[32m[20230203 20:52:56 @agent_ppo2.py:193][0m |          -0.0085 |           1.2125 |           2.4580 |
[32m[20230203 20:52:56 @agent_ppo2.py:193][0m |          -0.0090 |           1.1845 |           2.4577 |
[32m[20230203 20:52:56 @agent_ppo2.py:193][0m |          -0.0098 |           1.1692 |           2.4613 |
[32m[20230203 20:52:56 @agent_ppo2.py:193][0m |          -0.0104 |           1.1533 |           2.4612 |
[32m[20230203 20:52:56 @agent_ppo2.py:193][0m |          -0.0108 |           1.1384 |           2.4641 |
[32m[20230203 20:52:57 @agent_ppo2.py:193][0m |          -0.0106 |           1.1274 |           2.4654 |
[32m[20230203 20:52:57 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:52:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 129.80
[32m[20230203 20:52:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 143.20
[32m[20230203 20:52:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 59.68
[32m[20230203 20:52:57 @agent_ppo2.py:151][0m Total time:       2.36 min
[32m[20230203 20:52:57 @agent_ppo2.py:153][0m 169984 total steps have happened
[32m[20230203 20:52:57 @agent_ppo2.py:129][0m #------------------------ Iteration 83 --------------------------#
[32m[20230203 20:52:58 @agent_ppo2.py:135][0m Sampling time: 0.48 s by 1 slaves
[32m[20230203 20:52:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:58 @agent_ppo2.py:193][0m |          -0.0032 |          28.8960 |           2.4103 |
[32m[20230203 20:52:58 @agent_ppo2.py:193][0m |          -0.0075 |          16.4177 |           2.4089 |
[32m[20230203 20:52:58 @agent_ppo2.py:193][0m |          -0.0017 |          13.4557 |           2.4074 |
[32m[20230203 20:52:58 @agent_ppo2.py:193][0m |          -0.0092 |          10.9030 |           2.4067 |
[32m[20230203 20:52:58 @agent_ppo2.py:193][0m |          -0.0101 |           9.0194 |           2.4067 |
[32m[20230203 20:52:58 @agent_ppo2.py:193][0m |          -0.0102 |           7.3467 |           2.4066 |
[32m[20230203 20:52:58 @agent_ppo2.py:193][0m |          -0.0102 |           5.8645 |           2.4069 |
[32m[20230203 20:52:58 @agent_ppo2.py:193][0m |          -0.0107 |           4.7564 |           2.4068 |
[32m[20230203 20:52:58 @agent_ppo2.py:193][0m |          -0.0090 |           4.2640 |           2.4053 |
[32m[20230203 20:52:58 @agent_ppo2.py:193][0m |          -0.0118 |           3.6614 |           2.4061 |
[32m[20230203 20:52:58 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:52:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 36.61
[32m[20230203 20:52:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 116.16
[32m[20230203 20:52:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 84.35
[32m[20230203 20:52:59 @agent_ppo2.py:151][0m Total time:       2.39 min
[32m[20230203 20:52:59 @agent_ppo2.py:153][0m 172032 total steps have happened
[32m[20230203 20:52:59 @agent_ppo2.py:129][0m #------------------------ Iteration 84 --------------------------#
[32m[20230203 20:52:59 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:52:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:52:59 @agent_ppo2.py:193][0m |          -0.0025 |           1.8511 |           2.4413 |
[32m[20230203 20:52:59 @agent_ppo2.py:193][0m |          -0.0077 |           1.5651 |           2.4390 |
[32m[20230203 20:52:59 @agent_ppo2.py:193][0m |          -0.0092 |           1.4826 |           2.4399 |
[32m[20230203 20:52:59 @agent_ppo2.py:193][0m |          -0.0102 |           1.4299 |           2.4377 |
[32m[20230203 20:53:00 @agent_ppo2.py:193][0m |          -0.0117 |           1.3997 |           2.4394 |
[32m[20230203 20:53:00 @agent_ppo2.py:193][0m |          -0.0128 |           1.3691 |           2.4396 |
[32m[20230203 20:53:00 @agent_ppo2.py:193][0m |          -0.0128 |           1.3512 |           2.4400 |
[32m[20230203 20:53:00 @agent_ppo2.py:193][0m |          -0.0140 |           1.3330 |           2.4414 |
[32m[20230203 20:53:00 @agent_ppo2.py:193][0m |          -0.0146 |           1.3187 |           2.4415 |
[32m[20230203 20:53:00 @agent_ppo2.py:193][0m |          -0.0147 |           1.3018 |           2.4403 |
[32m[20230203 20:53:00 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 124.06
[32m[20230203 20:53:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 127.06
[32m[20230203 20:53:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 41.61
[32m[20230203 20:53:00 @agent_ppo2.py:151][0m Total time:       2.42 min
[32m[20230203 20:53:00 @agent_ppo2.py:153][0m 174080 total steps have happened
[32m[20230203 20:53:00 @agent_ppo2.py:129][0m #------------------------ Iteration 85 --------------------------#
[32m[20230203 20:53:01 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:53:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:01 @agent_ppo2.py:193][0m |          -0.0000 |          14.6650 |           2.4694 |
[32m[20230203 20:53:01 @agent_ppo2.py:193][0m |          -0.0019 |           5.1609 |           2.4669 |
[32m[20230203 20:53:01 @agent_ppo2.py:193][0m |          -0.0034 |           4.0067 |           2.4668 |
[32m[20230203 20:53:01 @agent_ppo2.py:193][0m |          -0.0072 |           3.5122 |           2.4639 |
[32m[20230203 20:53:01 @agent_ppo2.py:193][0m |          -0.0041 |           3.1635 |           2.4632 |
[32m[20230203 20:53:01 @agent_ppo2.py:193][0m |          -0.0074 |           2.9358 |           2.4636 |
[32m[20230203 20:53:01 @agent_ppo2.py:193][0m |          -0.0073 |           2.7714 |           2.4624 |
[32m[20230203 20:53:01 @agent_ppo2.py:193][0m |          -0.0074 |           2.6006 |           2.4616 |
[32m[20230203 20:53:01 @agent_ppo2.py:193][0m |          -0.0093 |           2.4818 |           2.4602 |
[32m[20230203 20:53:01 @agent_ppo2.py:193][0m |          -0.0097 |           2.4040 |           2.4601 |
[32m[20230203 20:53:01 @agent_ppo2.py:138][0m Policy update time: 0.40 s
[32m[20230203 20:53:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 9.71
[32m[20230203 20:53:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 103.36
[32m[20230203 20:53:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 1.51
[32m[20230203 20:53:01 @agent_ppo2.py:151][0m Total time:       2.44 min
[32m[20230203 20:53:01 @agent_ppo2.py:153][0m 176128 total steps have happened
[32m[20230203 20:53:01 @agent_ppo2.py:129][0m #------------------------ Iteration 86 --------------------------#
[32m[20230203 20:53:02 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:53:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:02 @agent_ppo2.py:193][0m |          -0.0001 |           2.3536 |           2.4897 |
[32m[20230203 20:53:02 @agent_ppo2.py:193][0m |          -0.0054 |           1.8784 |           2.4875 |
[32m[20230203 20:53:02 @agent_ppo2.py:193][0m |          -0.0068 |           1.7832 |           2.4883 |
[32m[20230203 20:53:02 @agent_ppo2.py:193][0m |          -0.0083 |           1.7324 |           2.4892 |
[32m[20230203 20:53:02 @agent_ppo2.py:193][0m |          -0.0098 |           1.6863 |           2.4900 |
[32m[20230203 20:53:02 @agent_ppo2.py:193][0m |          -0.0102 |           1.6605 |           2.4908 |
[32m[20230203 20:53:02 @agent_ppo2.py:193][0m |          -0.0108 |           1.6301 |           2.4909 |
[32m[20230203 20:53:03 @agent_ppo2.py:193][0m |          -0.0116 |           1.6128 |           2.4913 |
[32m[20230203 20:53:03 @agent_ppo2.py:193][0m |          -0.0121 |           1.5942 |           2.4915 |
[32m[20230203 20:53:03 @agent_ppo2.py:193][0m |          -0.0127 |           1.5742 |           2.4926 |
[32m[20230203 20:53:03 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 122.32
[32m[20230203 20:53:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 124.99
[32m[20230203 20:53:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 42.76
[32m[20230203 20:53:03 @agent_ppo2.py:151][0m Total time:       2.46 min
[32m[20230203 20:53:03 @agent_ppo2.py:153][0m 178176 total steps have happened
[32m[20230203 20:53:03 @agent_ppo2.py:129][0m #------------------------ Iteration 87 --------------------------#
[32m[20230203 20:53:04 @agent_ppo2.py:135][0m Sampling time: 0.61 s by 1 slaves
[32m[20230203 20:53:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:04 @agent_ppo2.py:193][0m |          -0.0031 |           7.2326 |           2.5235 |
[32m[20230203 20:53:04 @agent_ppo2.py:193][0m |          -0.0034 |           2.9658 |           2.5198 |
[32m[20230203 20:53:04 @agent_ppo2.py:193][0m |          -0.0073 |           2.5833 |           2.5185 |
[32m[20230203 20:53:04 @agent_ppo2.py:193][0m |          -0.0151 |           2.3924 |           2.5200 |
[32m[20230203 20:53:04 @agent_ppo2.py:193][0m |          -0.0089 |           2.2201 |           2.5155 |
[32m[20230203 20:53:04 @agent_ppo2.py:193][0m |          -0.0067 |           2.0854 |           2.5157 |
[32m[20230203 20:53:04 @agent_ppo2.py:193][0m |          -0.0108 |           2.0043 |           2.5163 |
[32m[20230203 20:53:04 @agent_ppo2.py:193][0m |          -0.0121 |           1.9461 |           2.5158 |
[32m[20230203 20:53:04 @agent_ppo2.py:193][0m |          -0.0079 |           1.8831 |           2.5164 |
[32m[20230203 20:53:04 @agent_ppo2.py:193][0m |          -0.0097 |           1.8687 |           2.5156 |
[32m[20230203 20:53:04 @agent_ppo2.py:138][0m Policy update time: 0.68 s
[32m[20230203 20:53:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 64.58
[32m[20230203 20:53:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 154.38
[32m[20230203 20:53:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 101.60
[32m[20230203 20:53:05 @agent_ppo2.py:151][0m Total time:       2.49 min
[32m[20230203 20:53:05 @agent_ppo2.py:153][0m 180224 total steps have happened
[32m[20230203 20:53:05 @agent_ppo2.py:129][0m #------------------------ Iteration 88 --------------------------#
[32m[20230203 20:53:05 @agent_ppo2.py:135][0m Sampling time: 0.59 s by 1 slaves
[32m[20230203 20:53:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:06 @agent_ppo2.py:193][0m |          -0.0002 |          13.8875 |           2.4428 |
[32m[20230203 20:53:06 @agent_ppo2.py:193][0m |          -0.0078 |           3.8843 |           2.4369 |
[32m[20230203 20:53:06 @agent_ppo2.py:193][0m |          -0.0110 |           3.0957 |           2.4326 |
[32m[20230203 20:53:06 @agent_ppo2.py:193][0m |          -0.0124 |           2.6870 |           2.4300 |
[32m[20230203 20:53:06 @agent_ppo2.py:193][0m |          -0.0131 |           2.4226 |           2.4276 |
[32m[20230203 20:53:06 @agent_ppo2.py:193][0m |          -0.0145 |           2.2192 |           2.4265 |
[32m[20230203 20:53:06 @agent_ppo2.py:193][0m |          -0.0141 |           2.0976 |           2.4245 |
[32m[20230203 20:53:06 @agent_ppo2.py:193][0m |          -0.0154 |           1.9224 |           2.4240 |
[32m[20230203 20:53:06 @agent_ppo2.py:193][0m |          -0.0159 |           1.8363 |           2.4217 |
[32m[20230203 20:53:06 @agent_ppo2.py:193][0m |          -0.0170 |           1.7666 |           2.4214 |
[32m[20230203 20:53:06 @agent_ppo2.py:138][0m Policy update time: 0.65 s
[32m[20230203 20:53:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 15.31
[32m[20230203 20:53:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 145.96
[32m[20230203 20:53:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 101.69
[32m[20230203 20:53:07 @agent_ppo2.py:151][0m Total time:       2.52 min
[32m[20230203 20:53:07 @agent_ppo2.py:153][0m 182272 total steps have happened
[32m[20230203 20:53:07 @agent_ppo2.py:129][0m #------------------------ Iteration 89 --------------------------#
[32m[20230203 20:53:07 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:53:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:07 @agent_ppo2.py:193][0m |           0.0053 |          34.0254 |           2.4698 |
[32m[20230203 20:53:07 @agent_ppo2.py:193][0m |          -0.0048 |          12.5181 |           2.4634 |
[32m[20230203 20:53:07 @agent_ppo2.py:193][0m |          -0.0032 |          10.5942 |           2.4610 |
[32m[20230203 20:53:07 @agent_ppo2.py:193][0m |          -0.0114 |           9.5998 |           2.4615 |
[32m[20230203 20:53:07 @agent_ppo2.py:193][0m |          -0.0044 |           8.7806 |           2.4614 |
[32m[20230203 20:53:07 @agent_ppo2.py:193][0m |          -0.0166 |           8.2669 |           2.4598 |
[32m[20230203 20:53:07 @agent_ppo2.py:193][0m |           0.0040 |           7.9971 |           2.4603 |
[32m[20230203 20:53:08 @agent_ppo2.py:193][0m |          -0.0176 |           7.5380 |           2.4602 |
[32m[20230203 20:53:08 @agent_ppo2.py:193][0m |          -0.0085 |           7.1753 |           2.4586 |
[32m[20230203 20:53:08 @agent_ppo2.py:193][0m |          -0.0123 |           7.0923 |           2.4569 |
[32m[20230203 20:53:08 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:53:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: -69.00
[32m[20230203 20:53:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -21.92
[32m[20230203 20:53:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 167.17
[32m[20230203 20:53:08 @agent_ppo2.py:151][0m Total time:       2.55 min
[32m[20230203 20:53:08 @agent_ppo2.py:153][0m 184320 total steps have happened
[32m[20230203 20:53:08 @agent_ppo2.py:129][0m #------------------------ Iteration 90 --------------------------#
[32m[20230203 20:53:09 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 20:53:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:09 @agent_ppo2.py:193][0m |           0.0014 |          20.0814 |           2.4453 |
[32m[20230203 20:53:09 @agent_ppo2.py:193][0m |          -0.0031 |          11.7997 |           2.4428 |
[32m[20230203 20:53:09 @agent_ppo2.py:193][0m |          -0.0056 |           9.9622 |           2.4402 |
[32m[20230203 20:53:09 @agent_ppo2.py:193][0m |          -0.0066 |           8.8396 |           2.4380 |
[32m[20230203 20:53:09 @agent_ppo2.py:193][0m |          -0.0071 |           8.2810 |           2.4359 |
[32m[20230203 20:53:09 @agent_ppo2.py:193][0m |          -0.0085 |           7.8882 |           2.4358 |
[32m[20230203 20:53:09 @agent_ppo2.py:193][0m |          -0.0089 |           7.6538 |           2.4354 |
[32m[20230203 20:53:09 @agent_ppo2.py:193][0m |          -0.0100 |           7.4065 |           2.4344 |
[32m[20230203 20:53:09 @agent_ppo2.py:193][0m |          -0.0108 |           7.1574 |           2.4341 |
[32m[20230203 20:53:09 @agent_ppo2.py:193][0m |          -0.0108 |           6.8784 |           2.4323 |
[32m[20230203 20:53:09 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:53:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: -8.63
[32m[20230203 20:53:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 128.97
[32m[20230203 20:53:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 180.52
[32m[20230203 20:53:10 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 180.52
[32m[20230203 20:53:10 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 180.52
[32m[20230203 20:53:10 @agent_ppo2.py:151][0m Total time:       2.57 min
[32m[20230203 20:53:10 @agent_ppo2.py:153][0m 186368 total steps have happened
[32m[20230203 20:53:10 @agent_ppo2.py:129][0m #------------------------ Iteration 91 --------------------------#
[32m[20230203 20:53:10 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:53:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:10 @agent_ppo2.py:193][0m |          -0.0007 |          32.2707 |           2.4270 |
[32m[20230203 20:53:10 @agent_ppo2.py:193][0m |          -0.0055 |          22.1164 |           2.4266 |
[32m[20230203 20:53:10 @agent_ppo2.py:193][0m |          -0.0062 |          19.8689 |           2.4250 |
[32m[20230203 20:53:10 @agent_ppo2.py:193][0m |          -0.0080 |          17.9145 |           2.4248 |
[32m[20230203 20:53:10 @agent_ppo2.py:193][0m |          -0.0036 |          17.3092 |           2.4238 |
[32m[20230203 20:53:10 @agent_ppo2.py:193][0m |          -0.0096 |          15.7116 |           2.4224 |
[32m[20230203 20:53:10 @agent_ppo2.py:193][0m |          -0.0106 |          14.8190 |           2.4219 |
[32m[20230203 20:53:10 @agent_ppo2.py:193][0m |          -0.0091 |          14.5646 |           2.4222 |
[32m[20230203 20:53:10 @agent_ppo2.py:193][0m |          -0.0152 |          13.7417 |           2.4222 |
[32m[20230203 20:53:10 @agent_ppo2.py:193][0m |          -0.0085 |          13.6560 |           2.4213 |
[32m[20230203 20:53:10 @agent_ppo2.py:138][0m Policy update time: 0.44 s
[32m[20230203 20:53:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 11.70
[32m[20230203 20:53:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 100.90
[32m[20230203 20:53:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 171.36
[32m[20230203 20:53:11 @agent_ppo2.py:151][0m Total time:       2.60 min
[32m[20230203 20:53:11 @agent_ppo2.py:153][0m 188416 total steps have happened
[32m[20230203 20:53:11 @agent_ppo2.py:129][0m #------------------------ Iteration 92 --------------------------#
[32m[20230203 20:53:11 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:53:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:12 @agent_ppo2.py:193][0m |          -0.0009 |          16.6388 |           2.3582 |
[32m[20230203 20:53:12 @agent_ppo2.py:193][0m |          -0.0119 |           9.0637 |           2.3583 |
[32m[20230203 20:53:12 @agent_ppo2.py:193][0m |          -0.0098 |           8.0494 |           2.3575 |
[32m[20230203 20:53:12 @agent_ppo2.py:193][0m |           0.0010 |           7.6651 |           2.3588 |
[32m[20230203 20:53:12 @agent_ppo2.py:193][0m |          -0.0037 |           7.0945 |           2.3594 |
[32m[20230203 20:53:12 @agent_ppo2.py:193][0m |          -0.0216 |           6.5128 |           2.3582 |
[32m[20230203 20:53:12 @agent_ppo2.py:193][0m |          -0.0039 |           6.3108 |           2.3591 |
[32m[20230203 20:53:12 @agent_ppo2.py:193][0m |          -0.0092 |           5.8993 |           2.3601 |
[32m[20230203 20:53:12 @agent_ppo2.py:193][0m |          -0.0015 |           5.5993 |           2.3617 |
[32m[20230203 20:53:12 @agent_ppo2.py:193][0m |          -0.0103 |           5.5169 |           2.3609 |
[32m[20230203 20:53:12 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:53:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 35.72
[32m[20230203 20:53:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 112.35
[32m[20230203 20:53:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 179.79
[32m[20230203 20:53:12 @agent_ppo2.py:151][0m Total time:       2.62 min
[32m[20230203 20:53:12 @agent_ppo2.py:153][0m 190464 total steps have happened
[32m[20230203 20:53:12 @agent_ppo2.py:129][0m #------------------------ Iteration 93 --------------------------#
[32m[20230203 20:53:13 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:53:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:13 @agent_ppo2.py:193][0m |           0.0045 |          11.0574 |           2.3691 |
[32m[20230203 20:53:13 @agent_ppo2.py:193][0m |          -0.0033 |           6.2984 |           2.3691 |
[32m[20230203 20:53:13 @agent_ppo2.py:193][0m |          -0.0136 |           4.6139 |           2.3695 |
[32m[20230203 20:53:13 @agent_ppo2.py:193][0m |          -0.0165 |           3.8927 |           2.3701 |
[32m[20230203 20:53:13 @agent_ppo2.py:193][0m |          -0.0099 |           3.4967 |           2.3701 |
[32m[20230203 20:53:13 @agent_ppo2.py:193][0m |          -0.0123 |           3.1268 |           2.3690 |
[32m[20230203 20:53:13 @agent_ppo2.py:193][0m |          -0.0093 |           2.9580 |           2.3682 |
[32m[20230203 20:53:13 @agent_ppo2.py:193][0m |          -0.0161 |           2.7808 |           2.3668 |
[32m[20230203 20:53:13 @agent_ppo2.py:193][0m |          -0.0260 |           2.8321 |           2.3663 |
[32m[20230203 20:53:13 @agent_ppo2.py:193][0m |          -0.0218 |           2.6882 |           2.3650 |
[32m[20230203 20:53:13 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:53:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 57.86
[32m[20230203 20:53:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 129.20
[32m[20230203 20:53:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 148.30
[32m[20230203 20:53:14 @agent_ppo2.py:151][0m Total time:       2.65 min
[32m[20230203 20:53:14 @agent_ppo2.py:153][0m 192512 total steps have happened
[32m[20230203 20:53:14 @agent_ppo2.py:129][0m #------------------------ Iteration 94 --------------------------#
[32m[20230203 20:53:15 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:53:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:15 @agent_ppo2.py:193][0m |           0.0002 |           1.9156 |           2.4567 |
[32m[20230203 20:53:15 @agent_ppo2.py:193][0m |          -0.0028 |           1.7551 |           2.4530 |
[32m[20230203 20:53:15 @agent_ppo2.py:193][0m |          -0.0045 |           1.6814 |           2.4507 |
[32m[20230203 20:53:15 @agent_ppo2.py:193][0m |          -0.0056 |           1.6301 |           2.4506 |
[32m[20230203 20:53:15 @agent_ppo2.py:193][0m |          -0.0056 |           1.5942 |           2.4479 |
[32m[20230203 20:53:15 @agent_ppo2.py:193][0m |          -0.0074 |           1.5653 |           2.4494 |
[32m[20230203 20:53:15 @agent_ppo2.py:193][0m |          -0.0078 |           1.5393 |           2.4490 |
[32m[20230203 20:53:15 @agent_ppo2.py:193][0m |          -0.0085 |           1.5234 |           2.4477 |
[32m[20230203 20:53:15 @agent_ppo2.py:193][0m |          -0.0091 |           1.5025 |           2.4487 |
[32m[20230203 20:53:15 @agent_ppo2.py:193][0m |          -0.0093 |           1.4865 |           2.4496 |
[32m[20230203 20:53:15 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 113.89
[32m[20230203 20:53:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 120.04
[32m[20230203 20:53:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 174.70
[32m[20230203 20:53:16 @agent_ppo2.py:151][0m Total time:       2.67 min
[32m[20230203 20:53:16 @agent_ppo2.py:153][0m 194560 total steps have happened
[32m[20230203 20:53:16 @agent_ppo2.py:129][0m #------------------------ Iteration 95 --------------------------#
[32m[20230203 20:53:16 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:53:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:16 @agent_ppo2.py:193][0m |           0.0003 |           1.1745 |           2.4788 |
[32m[20230203 20:53:16 @agent_ppo2.py:193][0m |          -0.0028 |           1.0831 |           2.4799 |
[32m[20230203 20:53:16 @agent_ppo2.py:193][0m |          -0.0052 |           1.0511 |           2.4769 |
[32m[20230203 20:53:16 @agent_ppo2.py:193][0m |          -0.0066 |           1.0280 |           2.4820 |
[32m[20230203 20:53:17 @agent_ppo2.py:193][0m |          -0.0080 |           1.0103 |           2.4820 |
[32m[20230203 20:53:17 @agent_ppo2.py:193][0m |          -0.0092 |           0.9972 |           2.4821 |
[32m[20230203 20:53:17 @agent_ppo2.py:193][0m |          -0.0096 |           0.9838 |           2.4839 |
[32m[20230203 20:53:17 @agent_ppo2.py:193][0m |          -0.0103 |           0.9764 |           2.4859 |
[32m[20230203 20:53:17 @agent_ppo2.py:193][0m |          -0.0107 |           0.9652 |           2.4875 |
[32m[20230203 20:53:17 @agent_ppo2.py:193][0m |          -0.0117 |           0.9565 |           2.4896 |
[32m[20230203 20:53:17 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 129.40
[32m[20230203 20:53:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 138.75
[32m[20230203 20:53:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 181.27
[32m[20230203 20:53:17 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 181.27
[32m[20230203 20:53:17 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 181.27
[32m[20230203 20:53:17 @agent_ppo2.py:151][0m Total time:       2.70 min
[32m[20230203 20:53:17 @agent_ppo2.py:153][0m 196608 total steps have happened
[32m[20230203 20:53:17 @agent_ppo2.py:129][0m #------------------------ Iteration 96 --------------------------#
[32m[20230203 20:53:18 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:53:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:18 @agent_ppo2.py:193][0m |          -0.0015 |           1.7356 |           2.4960 |
[32m[20230203 20:53:18 @agent_ppo2.py:193][0m |          -0.0070 |           1.6303 |           2.4922 |
[32m[20230203 20:53:18 @agent_ppo2.py:193][0m |          -0.0083 |           1.5803 |           2.4906 |
[32m[20230203 20:53:18 @agent_ppo2.py:193][0m |          -0.0096 |           1.5524 |           2.4914 |
[32m[20230203 20:53:18 @agent_ppo2.py:193][0m |          -0.0104 |           1.5330 |           2.4915 |
[32m[20230203 20:53:18 @agent_ppo2.py:193][0m |          -0.0111 |           1.5121 |           2.4924 |
[32m[20230203 20:53:18 @agent_ppo2.py:193][0m |          -0.0115 |           1.5011 |           2.4930 |
[32m[20230203 20:53:18 @agent_ppo2.py:193][0m |          -0.0119 |           1.4894 |           2.4943 |
[32m[20230203 20:53:18 @agent_ppo2.py:193][0m |          -0.0127 |           1.4818 |           2.4938 |
[32m[20230203 20:53:18 @agent_ppo2.py:193][0m |          -0.0123 |           1.4754 |           2.4950 |
[32m[20230203 20:53:18 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 122.70
[32m[20230203 20:53:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 123.79
[32m[20230203 20:53:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 56.09
[32m[20230203 20:53:19 @agent_ppo2.py:151][0m Total time:       2.73 min
[32m[20230203 20:53:19 @agent_ppo2.py:153][0m 198656 total steps have happened
[32m[20230203 20:53:19 @agent_ppo2.py:129][0m #------------------------ Iteration 97 --------------------------#
[32m[20230203 20:53:19 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:53:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:19 @agent_ppo2.py:193][0m |          -0.0001 |           1.2893 |           2.5566 |
[32m[20230203 20:53:20 @agent_ppo2.py:193][0m |          -0.0049 |           1.2298 |           2.5546 |
[32m[20230203 20:53:20 @agent_ppo2.py:193][0m |          -0.0065 |           1.2165 |           2.5521 |
[32m[20230203 20:53:20 @agent_ppo2.py:193][0m |          -0.0075 |           1.2057 |           2.5522 |
[32m[20230203 20:53:20 @agent_ppo2.py:193][0m |          -0.0076 |           1.1962 |           2.5514 |
[32m[20230203 20:53:20 @agent_ppo2.py:193][0m |          -0.0091 |           1.1853 |           2.5522 |
[32m[20230203 20:53:20 @agent_ppo2.py:193][0m |          -0.0095 |           1.1812 |           2.5536 |
[32m[20230203 20:53:20 @agent_ppo2.py:193][0m |          -0.0099 |           1.1735 |           2.5547 |
[32m[20230203 20:53:20 @agent_ppo2.py:193][0m |          -0.0106 |           1.1662 |           2.5531 |
[32m[20230203 20:53:20 @agent_ppo2.py:193][0m |          -0.0112 |           1.1614 |           2.5548 |
[32m[20230203 20:53:20 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 128.00
[32m[20230203 20:53:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 132.39
[32m[20230203 20:53:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 182.50
[32m[20230203 20:53:20 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 182.50
[32m[20230203 20:53:20 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 182.50
[32m[20230203 20:53:20 @agent_ppo2.py:151][0m Total time:       2.75 min
[32m[20230203 20:53:20 @agent_ppo2.py:153][0m 200704 total steps have happened
[32m[20230203 20:53:20 @agent_ppo2.py:129][0m #------------------------ Iteration 98 --------------------------#
[32m[20230203 20:53:21 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 20:53:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:21 @agent_ppo2.py:193][0m |           0.0076 |           8.9120 |           2.4813 |
[32m[20230203 20:53:21 @agent_ppo2.py:193][0m |          -0.0042 |           4.4592 |           2.4789 |
[32m[20230203 20:53:21 @agent_ppo2.py:193][0m |          -0.0084 |           3.2132 |           2.4778 |
[32m[20230203 20:53:21 @agent_ppo2.py:193][0m |          -0.0086 |           2.9827 |           2.4757 |
[32m[20230203 20:53:21 @agent_ppo2.py:193][0m |          -0.0081 |           2.7353 |           2.4742 |
[32m[20230203 20:53:21 @agent_ppo2.py:193][0m |          -0.0089 |           2.6178 |           2.4733 |
[32m[20230203 20:53:21 @agent_ppo2.py:193][0m |          -0.0121 |           2.4886 |           2.4723 |
[32m[20230203 20:53:21 @agent_ppo2.py:193][0m |          -0.0121 |           2.4476 |           2.4706 |
[32m[20230203 20:53:21 @agent_ppo2.py:193][0m |          -0.0128 |           2.3911 |           2.4697 |
[32m[20230203 20:53:21 @agent_ppo2.py:193][0m |          -0.0120 |           2.2984 |           2.4688 |
[32m[20230203 20:53:21 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:53:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 62.55
[32m[20230203 20:53:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 140.33
[32m[20230203 20:53:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 189.26
[32m[20230203 20:53:22 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 189.26
[32m[20230203 20:53:22 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 189.26
[32m[20230203 20:53:22 @agent_ppo2.py:151][0m Total time:       2.78 min
[32m[20230203 20:53:22 @agent_ppo2.py:153][0m 202752 total steps have happened
[32m[20230203 20:53:22 @agent_ppo2.py:129][0m #------------------------ Iteration 99 --------------------------#
[32m[20230203 20:53:23 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:53:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:23 @agent_ppo2.py:193][0m |          -0.0030 |           8.5967 |           2.5014 |
[32m[20230203 20:53:23 @agent_ppo2.py:193][0m |           0.0002 |           3.7486 |           2.4997 |
[32m[20230203 20:53:23 @agent_ppo2.py:193][0m |          -0.0065 |           3.2915 |           2.4989 |
[32m[20230203 20:53:23 @agent_ppo2.py:193][0m |          -0.0069 |           3.0950 |           2.4972 |
[32m[20230203 20:53:23 @agent_ppo2.py:193][0m |          -0.0013 |           2.8098 |           2.4975 |
[32m[20230203 20:53:23 @agent_ppo2.py:193][0m |          -0.0090 |           2.7097 |           2.4976 |
[32m[20230203 20:53:23 @agent_ppo2.py:193][0m |          -0.0104 |           2.5883 |           2.4974 |
[32m[20230203 20:53:23 @agent_ppo2.py:193][0m |          -0.0053 |           2.6159 |           2.4984 |
[32m[20230203 20:53:23 @agent_ppo2.py:193][0m |          -0.0097 |           2.5418 |           2.4960 |
[32m[20230203 20:53:23 @agent_ppo2.py:193][0m |          -0.0107 |           2.4447 |           2.4953 |
[32m[20230203 20:53:23 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:53:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 58.49
[32m[20230203 20:53:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 146.55
[32m[20230203 20:53:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 170.91
[32m[20230203 20:53:24 @agent_ppo2.py:151][0m Total time:       2.81 min
[32m[20230203 20:53:24 @agent_ppo2.py:153][0m 204800 total steps have happened
[32m[20230203 20:53:24 @agent_ppo2.py:129][0m #------------------------ Iteration 100 --------------------------#
[32m[20230203 20:53:24 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:53:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:24 @agent_ppo2.py:193][0m |          -0.0012 |           2.1515 |           2.5140 |
[32m[20230203 20:53:24 @agent_ppo2.py:193][0m |          -0.0050 |           1.9965 |           2.5121 |
[32m[20230203 20:53:24 @agent_ppo2.py:193][0m |          -0.0068 |           1.9318 |           2.5128 |
[32m[20230203 20:53:24 @agent_ppo2.py:193][0m |          -0.0079 |           1.8928 |           2.5129 |
[32m[20230203 20:53:25 @agent_ppo2.py:193][0m |          -0.0079 |           1.8658 |           2.5129 |
[32m[20230203 20:53:25 @agent_ppo2.py:193][0m |          -0.0088 |           1.8401 |           2.5151 |
[32m[20230203 20:53:25 @agent_ppo2.py:193][0m |          -0.0087 |           1.8172 |           2.5141 |
[32m[20230203 20:53:25 @agent_ppo2.py:193][0m |          -0.0093 |           1.8023 |           2.5169 |
[32m[20230203 20:53:25 @agent_ppo2.py:193][0m |          -0.0096 |           1.7871 |           2.5172 |
[32m[20230203 20:53:25 @agent_ppo2.py:193][0m |          -0.0102 |           1.7784 |           2.5193 |
[32m[20230203 20:53:25 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 133.60
[32m[20230203 20:53:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 145.53
[32m[20230203 20:53:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 170.32
[32m[20230203 20:53:25 @agent_ppo2.py:151][0m Total time:       2.84 min
[32m[20230203 20:53:25 @agent_ppo2.py:153][0m 206848 total steps have happened
[32m[20230203 20:53:25 @agent_ppo2.py:129][0m #------------------------ Iteration 101 --------------------------#
[32m[20230203 20:53:26 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:53:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:26 @agent_ppo2.py:193][0m |           0.0002 |           2.0270 |           2.5544 |
[32m[20230203 20:53:26 @agent_ppo2.py:193][0m |          -0.0040 |           1.8846 |           2.5474 |
[32m[20230203 20:53:26 @agent_ppo2.py:193][0m |          -0.0056 |           1.8319 |           2.5420 |
[32m[20230203 20:53:26 @agent_ppo2.py:193][0m |          -0.0070 |           1.8082 |           2.5417 |
[32m[20230203 20:53:26 @agent_ppo2.py:193][0m |          -0.0072 |           1.7807 |           2.5401 |
[32m[20230203 20:53:26 @agent_ppo2.py:193][0m |          -0.0086 |           1.7583 |           2.5404 |
[32m[20230203 20:53:26 @agent_ppo2.py:193][0m |          -0.0090 |           1.7480 |           2.5396 |
[32m[20230203 20:53:26 @agent_ppo2.py:193][0m |          -0.0091 |           1.7280 |           2.5414 |
[32m[20230203 20:53:26 @agent_ppo2.py:193][0m |          -0.0099 |           1.7190 |           2.5402 |
[32m[20230203 20:53:27 @agent_ppo2.py:193][0m |          -0.0101 |           1.7083 |           2.5416 |
[32m[20230203 20:53:27 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 151.88
[32m[20230203 20:53:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 161.22
[32m[20230203 20:53:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 180.90
[32m[20230203 20:53:27 @agent_ppo2.py:151][0m Total time:       2.86 min
[32m[20230203 20:53:27 @agent_ppo2.py:153][0m 208896 total steps have happened
[32m[20230203 20:53:27 @agent_ppo2.py:129][0m #------------------------ Iteration 102 --------------------------#
[32m[20230203 20:53:28 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:53:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:28 @agent_ppo2.py:193][0m |          -0.0015 |           1.6731 |           2.5539 |
[32m[20230203 20:53:28 @agent_ppo2.py:193][0m |          -0.0074 |           1.5632 |           2.5511 |
[32m[20230203 20:53:28 @agent_ppo2.py:193][0m |          -0.0091 |           1.5273 |           2.5488 |
[32m[20230203 20:53:28 @agent_ppo2.py:193][0m |          -0.0103 |           1.5038 |           2.5502 |
[32m[20230203 20:53:28 @agent_ppo2.py:193][0m |          -0.0112 |           1.4894 |           2.5499 |
[32m[20230203 20:53:28 @agent_ppo2.py:193][0m |          -0.0116 |           1.4735 |           2.5502 |
[32m[20230203 20:53:28 @agent_ppo2.py:193][0m |          -0.0125 |           1.4668 |           2.5499 |
[32m[20230203 20:53:28 @agent_ppo2.py:193][0m |          -0.0127 |           1.4602 |           2.5511 |
[32m[20230203 20:53:28 @agent_ppo2.py:193][0m |          -0.0132 |           1.4456 |           2.5524 |
[32m[20230203 20:53:28 @agent_ppo2.py:193][0m |          -0.0134 |           1.4393 |           2.5501 |
[32m[20230203 20:53:28 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 155.45
[32m[20230203 20:53:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 172.09
[32m[20230203 20:53:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 32.64
[32m[20230203 20:53:29 @agent_ppo2.py:151][0m Total time:       2.89 min
[32m[20230203 20:53:29 @agent_ppo2.py:153][0m 210944 total steps have happened
[32m[20230203 20:53:29 @agent_ppo2.py:129][0m #------------------------ Iteration 103 --------------------------#
[32m[20230203 20:53:29 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:53:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:29 @agent_ppo2.py:193][0m |           0.0016 |          14.9066 |           2.6013 |
[32m[20230203 20:53:29 @agent_ppo2.py:193][0m |          -0.0025 |           9.0623 |           2.5966 |
[32m[20230203 20:53:29 @agent_ppo2.py:193][0m |          -0.0047 |           7.4825 |           2.5959 |
[32m[20230203 20:53:29 @agent_ppo2.py:193][0m |          -0.0038 |           6.5811 |           2.5940 |
[32m[20230203 20:53:29 @agent_ppo2.py:193][0m |          -0.0084 |           6.3080 |           2.5908 |
[32m[20230203 20:53:29 @agent_ppo2.py:193][0m |          -0.0088 |           5.8706 |           2.5893 |
[32m[20230203 20:53:29 @agent_ppo2.py:193][0m |          -0.0070 |           5.6663 |           2.5869 |
[32m[20230203 20:53:30 @agent_ppo2.py:193][0m |          -0.0099 |           5.4628 |           2.5857 |
[32m[20230203 20:53:30 @agent_ppo2.py:193][0m |          -0.0106 |           5.2919 |           2.5846 |
[32m[20230203 20:53:30 @agent_ppo2.py:193][0m |          -0.0097 |           5.2308 |           2.5822 |
[32m[20230203 20:53:30 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:53:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 58.70
[32m[20230203 20:53:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 147.95
[32m[20230203 20:53:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -38.98
[32m[20230203 20:53:30 @agent_ppo2.py:151][0m Total time:       2.91 min
[32m[20230203 20:53:30 @agent_ppo2.py:153][0m 212992 total steps have happened
[32m[20230203 20:53:30 @agent_ppo2.py:129][0m #------------------------ Iteration 104 --------------------------#
[32m[20230203 20:53:30 @agent_ppo2.py:135][0m Sampling time: 0.48 s by 1 slaves
[32m[20230203 20:53:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:31 @agent_ppo2.py:193][0m |           0.0008 |          19.2338 |           2.5440 |
[32m[20230203 20:53:31 @agent_ppo2.py:193][0m |          -0.0039 |           8.0045 |           2.5373 |
[32m[20230203 20:53:31 @agent_ppo2.py:193][0m |          -0.0079 |           6.6972 |           2.5367 |
[32m[20230203 20:53:31 @agent_ppo2.py:193][0m |          -0.0085 |           6.1066 |           2.5376 |
[32m[20230203 20:53:31 @agent_ppo2.py:193][0m |          -0.0110 |           5.7013 |           2.5385 |
[32m[20230203 20:53:31 @agent_ppo2.py:193][0m |          -0.0119 |           5.4176 |           2.5373 |
[32m[20230203 20:53:31 @agent_ppo2.py:193][0m |          -0.0090 |           5.2165 |           2.5349 |
[32m[20230203 20:53:31 @agent_ppo2.py:193][0m |          -0.0123 |           5.0829 |           2.5389 |
[32m[20230203 20:53:31 @agent_ppo2.py:193][0m |          -0.0081 |           4.9987 |           2.5384 |
[32m[20230203 20:53:31 @agent_ppo2.py:193][0m |          -0.0127 |           4.8224 |           2.5382 |
[32m[20230203 20:53:31 @agent_ppo2.py:138][0m Policy update time: 0.54 s
[32m[20230203 20:53:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 86.74
[32m[20230203 20:53:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 158.13
[32m[20230203 20:53:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 91.10
[32m[20230203 20:53:31 @agent_ppo2.py:151][0m Total time:       2.94 min
[32m[20230203 20:53:31 @agent_ppo2.py:153][0m 215040 total steps have happened
[32m[20230203 20:53:31 @agent_ppo2.py:129][0m #------------------------ Iteration 105 --------------------------#
[32m[20230203 20:53:32 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:53:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:32 @agent_ppo2.py:193][0m |           0.0007 |          23.0558 |           2.5189 |
[32m[20230203 20:53:32 @agent_ppo2.py:193][0m |          -0.0060 |          15.4093 |           2.5174 |
[32m[20230203 20:53:32 @agent_ppo2.py:193][0m |          -0.0075 |          12.1307 |           2.5128 |
[32m[20230203 20:53:32 @agent_ppo2.py:193][0m |          -0.0088 |          10.7364 |           2.5156 |
[32m[20230203 20:53:32 @agent_ppo2.py:193][0m |          -0.0062 |          10.2356 |           2.5143 |
[32m[20230203 20:53:32 @agent_ppo2.py:193][0m |          -0.0077 |           9.6004 |           2.5128 |
[32m[20230203 20:53:32 @agent_ppo2.py:193][0m |          -0.0111 |           8.9440 |           2.5139 |
[32m[20230203 20:53:32 @agent_ppo2.py:193][0m |          -0.0104 |           8.4325 |           2.5138 |
[32m[20230203 20:53:32 @agent_ppo2.py:193][0m |          -0.0136 |           8.4933 |           2.5110 |
[32m[20230203 20:53:32 @agent_ppo2.py:193][0m |          -0.0098 |           8.1321 |           2.5112 |
[32m[20230203 20:53:32 @agent_ppo2.py:138][0m Policy update time: 0.41 s
[32m[20230203 20:53:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 54.44
[32m[20230203 20:53:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 162.74
[32m[20230203 20:53:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 180.06
[32m[20230203 20:53:33 @agent_ppo2.py:151][0m Total time:       2.96 min
[32m[20230203 20:53:33 @agent_ppo2.py:153][0m 217088 total steps have happened
[32m[20230203 20:53:33 @agent_ppo2.py:129][0m #------------------------ Iteration 106 --------------------------#
[32m[20230203 20:53:33 @agent_ppo2.py:135][0m Sampling time: 0.51 s by 1 slaves
[32m[20230203 20:53:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:33 @agent_ppo2.py:193][0m |           0.0134 |          17.9841 |           2.5006 |
[32m[20230203 20:53:33 @agent_ppo2.py:193][0m |          -0.0031 |          13.4477 |           2.4984 |
[32m[20230203 20:53:33 @agent_ppo2.py:193][0m |          -0.0064 |          11.2702 |           2.4965 |
[32m[20230203 20:53:33 @agent_ppo2.py:193][0m |          -0.0074 |          10.3837 |           2.4982 |
[32m[20230203 20:53:33 @agent_ppo2.py:193][0m |          -0.0063 |           9.7354 |           2.4939 |
[32m[20230203 20:53:34 @agent_ppo2.py:193][0m |          -0.0100 |           9.3633 |           2.4959 |
[32m[20230203 20:53:34 @agent_ppo2.py:193][0m |          -0.0093 |           8.9731 |           2.4956 |
[32m[20230203 20:53:34 @agent_ppo2.py:193][0m |          -0.0102 |           8.6559 |           2.4913 |
[32m[20230203 20:53:34 @agent_ppo2.py:193][0m |          -0.0098 |           8.4233 |           2.4922 |
[32m[20230203 20:53:34 @agent_ppo2.py:193][0m |          -0.0119 |           8.2453 |           2.4915 |
[32m[20230203 20:53:34 @agent_ppo2.py:138][0m Policy update time: 0.56 s
[32m[20230203 20:53:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 87.22
[32m[20230203 20:53:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 145.95
[32m[20230203 20:53:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 190.80
[32m[20230203 20:53:34 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 190.80
[32m[20230203 20:53:34 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 190.80
[32m[20230203 20:53:34 @agent_ppo2.py:151][0m Total time:       2.98 min
[32m[20230203 20:53:34 @agent_ppo2.py:153][0m 219136 total steps have happened
[32m[20230203 20:53:34 @agent_ppo2.py:129][0m #------------------------ Iteration 107 --------------------------#
[32m[20230203 20:53:35 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:53:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:35 @agent_ppo2.py:193][0m |           0.0016 |          11.2839 |           2.4957 |
[32m[20230203 20:53:35 @agent_ppo2.py:193][0m |          -0.0064 |           4.0930 |           2.4931 |
[32m[20230203 20:53:35 @agent_ppo2.py:193][0m |          -0.0043 |           3.5399 |           2.4931 |
[32m[20230203 20:53:35 @agent_ppo2.py:193][0m |          -0.0089 |           3.3362 |           2.4924 |
[32m[20230203 20:53:35 @agent_ppo2.py:193][0m |          -0.0043 |           3.1600 |           2.4925 |
[32m[20230203 20:53:35 @agent_ppo2.py:193][0m |          -0.0112 |           3.0401 |           2.4911 |
[32m[20230203 20:53:35 @agent_ppo2.py:193][0m |          -0.0092 |           2.9104 |           2.4918 |
[32m[20230203 20:53:35 @agent_ppo2.py:193][0m |          -0.0067 |           2.8092 |           2.4929 |
[32m[20230203 20:53:35 @agent_ppo2.py:193][0m |          -0.0106 |           2.6683 |           2.4914 |
[32m[20230203 20:53:35 @agent_ppo2.py:193][0m |          -0.0134 |           2.5501 |           2.4918 |
[32m[20230203 20:53:35 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:53:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 46.61
[32m[20230203 20:53:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 141.85
[32m[20230203 20:53:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 188.85
[32m[20230203 20:53:36 @agent_ppo2.py:151][0m Total time:       3.01 min
[32m[20230203 20:53:36 @agent_ppo2.py:153][0m 221184 total steps have happened
[32m[20230203 20:53:36 @agent_ppo2.py:129][0m #------------------------ Iteration 108 --------------------------#
[32m[20230203 20:53:36 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:53:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:36 @agent_ppo2.py:193][0m |           0.0010 |          21.0545 |           2.5306 |
[32m[20230203 20:53:36 @agent_ppo2.py:193][0m |          -0.0029 |          11.3463 |           2.5327 |
[32m[20230203 20:53:36 @agent_ppo2.py:193][0m |          -0.0086 |           8.0617 |           2.5305 |
[32m[20230203 20:53:36 @agent_ppo2.py:193][0m |          -0.0134 |           6.7353 |           2.5308 |
[32m[20230203 20:53:36 @agent_ppo2.py:193][0m |          -0.0100 |           6.3544 |           2.5309 |
[32m[20230203 20:53:36 @agent_ppo2.py:193][0m |          -0.0163 |           5.8534 |           2.5307 |
[32m[20230203 20:53:36 @agent_ppo2.py:193][0m |          -0.0173 |           5.5772 |           2.5318 |
[32m[20230203 20:53:36 @agent_ppo2.py:193][0m |          -0.0197 |           5.3858 |           2.5307 |
[32m[20230203 20:53:36 @agent_ppo2.py:193][0m |          -0.0203 |           5.0908 |           2.5295 |
[32m[20230203 20:53:36 @agent_ppo2.py:193][0m |          -0.0209 |           5.1004 |           2.5303 |
[32m[20230203 20:53:36 @agent_ppo2.py:138][0m Policy update time: 0.41 s
[32m[20230203 20:53:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 43.88
[32m[20230203 20:53:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 151.59
[32m[20230203 20:53:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 180.11
[32m[20230203 20:53:37 @agent_ppo2.py:151][0m Total time:       3.03 min
[32m[20230203 20:53:37 @agent_ppo2.py:153][0m 223232 total steps have happened
[32m[20230203 20:53:37 @agent_ppo2.py:129][0m #------------------------ Iteration 109 --------------------------#
[32m[20230203 20:53:38 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:53:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:38 @agent_ppo2.py:193][0m |          -0.0004 |           3.3756 |           2.6070 |
[32m[20230203 20:53:38 @agent_ppo2.py:193][0m |          -0.0052 |           2.7388 |           2.6066 |
[32m[20230203 20:53:38 @agent_ppo2.py:193][0m |          -0.0070 |           2.6119 |           2.6059 |
[32m[20230203 20:53:38 @agent_ppo2.py:193][0m |          -0.0083 |           2.5415 |           2.6091 |
[32m[20230203 20:53:38 @agent_ppo2.py:193][0m |          -0.0085 |           2.4902 |           2.6103 |
[32m[20230203 20:53:38 @agent_ppo2.py:193][0m |          -0.0083 |           2.4486 |           2.6116 |
[32m[20230203 20:53:38 @agent_ppo2.py:193][0m |          -0.0094 |           2.4216 |           2.6120 |
[32m[20230203 20:53:38 @agent_ppo2.py:193][0m |          -0.0102 |           2.3961 |           2.6142 |
[32m[20230203 20:53:38 @agent_ppo2.py:193][0m |          -0.0099 |           2.3745 |           2.6148 |
[32m[20230203 20:53:38 @agent_ppo2.py:193][0m |          -0.0112 |           2.3553 |           2.6169 |
[32m[20230203 20:53:38 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 146.02
[32m[20230203 20:53:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 153.79
[32m[20230203 20:53:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 60.11
[32m[20230203 20:53:38 @agent_ppo2.py:151][0m Total time:       3.05 min
[32m[20230203 20:53:38 @agent_ppo2.py:153][0m 225280 total steps have happened
[32m[20230203 20:53:38 @agent_ppo2.py:129][0m #------------------------ Iteration 110 --------------------------#
[32m[20230203 20:53:39 @agent_ppo2.py:135][0m Sampling time: 0.50 s by 1 slaves
[32m[20230203 20:53:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:39 @agent_ppo2.py:193][0m |           0.0002 |          21.2325 |           2.5622 |
[32m[20230203 20:53:39 @agent_ppo2.py:193][0m |           0.0012 |          14.2721 |           2.5575 |
[32m[20230203 20:53:39 @agent_ppo2.py:193][0m |          -0.0095 |          12.1598 |           2.5559 |
[32m[20230203 20:53:39 @agent_ppo2.py:193][0m |          -0.0106 |          11.0817 |           2.5572 |
[32m[20230203 20:53:39 @agent_ppo2.py:193][0m |          -0.0099 |          10.3779 |           2.5568 |
[32m[20230203 20:53:39 @agent_ppo2.py:193][0m |          -0.0086 |          10.1009 |           2.5550 |
[32m[20230203 20:53:39 @agent_ppo2.py:193][0m |          -0.0155 |           9.3590 |           2.5544 |
[32m[20230203 20:53:39 @agent_ppo2.py:193][0m |          -0.0153 |           8.9951 |           2.5524 |
[32m[20230203 20:53:39 @agent_ppo2.py:193][0m |          -0.0109 |           9.0221 |           2.5556 |
[32m[20230203 20:53:39 @agent_ppo2.py:193][0m |          -0.0160 |           8.6695 |           2.5552 |
[32m[20230203 20:53:39 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:53:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 14.53
[32m[20230203 20:53:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 168.81
[32m[20230203 20:53:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 204.63
[32m[20230203 20:53:40 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 204.63
[32m[20230203 20:53:40 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 204.63
[32m[20230203 20:53:40 @agent_ppo2.py:151][0m Total time:       3.08 min
[32m[20230203 20:53:40 @agent_ppo2.py:153][0m 227328 total steps have happened
[32m[20230203 20:53:40 @agent_ppo2.py:129][0m #------------------------ Iteration 111 --------------------------#
[32m[20230203 20:53:41 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:53:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:41 @agent_ppo2.py:193][0m |           0.0013 |          18.9317 |           2.6286 |
[32m[20230203 20:53:41 @agent_ppo2.py:193][0m |          -0.0052 |          12.1249 |           2.6223 |
[32m[20230203 20:53:41 @agent_ppo2.py:193][0m |          -0.0075 |          11.6987 |           2.6181 |
[32m[20230203 20:53:41 @agent_ppo2.py:193][0m |          -0.0093 |           9.1367 |           2.6164 |
[32m[20230203 20:53:41 @agent_ppo2.py:193][0m |          -0.0098 |           9.2664 |           2.6145 |
[32m[20230203 20:53:41 @agent_ppo2.py:193][0m |          -0.0109 |           8.2075 |           2.6152 |
[32m[20230203 20:53:41 @agent_ppo2.py:193][0m |          -0.0111 |           8.1771 |           2.6153 |
[32m[20230203 20:53:41 @agent_ppo2.py:193][0m |          -0.0126 |           7.6661 |           2.6149 |
[32m[20230203 20:53:41 @agent_ppo2.py:193][0m |          -0.0133 |           7.6386 |           2.6135 |
[32m[20230203 20:53:41 @agent_ppo2.py:193][0m |          -0.0136 |           7.3086 |           2.6141 |
[32m[20230203 20:53:41 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:53:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 71.71
[32m[20230203 20:53:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 169.54
[32m[20230203 20:53:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 172.41
[32m[20230203 20:53:42 @agent_ppo2.py:151][0m Total time:       3.11 min
[32m[20230203 20:53:42 @agent_ppo2.py:153][0m 229376 total steps have happened
[32m[20230203 20:53:42 @agent_ppo2.py:129][0m #------------------------ Iteration 112 --------------------------#
[32m[20230203 20:53:42 @agent_ppo2.py:135][0m Sampling time: 0.36 s by 1 slaves
[32m[20230203 20:53:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:42 @agent_ppo2.py:193][0m |          -0.0031 |          61.6773 |           2.5586 |
[32m[20230203 20:53:42 @agent_ppo2.py:193][0m |          -0.0084 |          40.6496 |           2.5566 |
[32m[20230203 20:53:42 @agent_ppo2.py:193][0m |          -0.0117 |          32.6644 |           2.5547 |
[32m[20230203 20:53:42 @agent_ppo2.py:193][0m |          -0.0050 |          30.4920 |           2.5559 |
[32m[20230203 20:53:42 @agent_ppo2.py:193][0m |          -0.0157 |          29.9476 |           2.5543 |
[32m[20230203 20:53:42 @agent_ppo2.py:193][0m |          -0.0060 |          28.0750 |           2.5529 |
[32m[20230203 20:53:42 @agent_ppo2.py:193][0m |          -0.0070 |          28.7293 |           2.5512 |
[32m[20230203 20:53:42 @agent_ppo2.py:193][0m |          -0.0126 |          25.5016 |           2.5491 |
[32m[20230203 20:53:42 @agent_ppo2.py:193][0m |          -0.0137 |          24.8509 |           2.5493 |
[32m[20230203 20:53:42 @agent_ppo2.py:193][0m |          -0.0087 |          24.4210 |           2.5478 |
[32m[20230203 20:53:42 @agent_ppo2.py:138][0m Policy update time: 0.41 s
[32m[20230203 20:53:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 38.29
[32m[20230203 20:53:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 152.04
[32m[20230203 20:53:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 191.08
[32m[20230203 20:53:43 @agent_ppo2.py:151][0m Total time:       3.13 min
[32m[20230203 20:53:43 @agent_ppo2.py:153][0m 231424 total steps have happened
[32m[20230203 20:53:43 @agent_ppo2.py:129][0m #------------------------ Iteration 113 --------------------------#
[32m[20230203 20:53:44 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:53:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:44 @agent_ppo2.py:193][0m |          -0.0007 |           9.9587 |           2.6295 |
[32m[20230203 20:53:44 @agent_ppo2.py:193][0m |          -0.0068 |           4.8877 |           2.6282 |
[32m[20230203 20:53:44 @agent_ppo2.py:193][0m |          -0.0092 |           4.0030 |           2.6269 |
[32m[20230203 20:53:44 @agent_ppo2.py:193][0m |          -0.0103 |           3.7692 |           2.6268 |
[32m[20230203 20:53:44 @agent_ppo2.py:193][0m |          -0.0120 |           3.4840 |           2.6277 |
[32m[20230203 20:53:44 @agent_ppo2.py:193][0m |          -0.0128 |           3.3027 |           2.6289 |
[32m[20230203 20:53:44 @agent_ppo2.py:193][0m |          -0.0137 |           3.2211 |           2.6300 |
[32m[20230203 20:53:44 @agent_ppo2.py:193][0m |          -0.0139 |           3.1207 |           2.6298 |
[32m[20230203 20:53:44 @agent_ppo2.py:193][0m |          -0.0140 |           3.0738 |           2.6298 |
[32m[20230203 20:53:44 @agent_ppo2.py:193][0m |          -0.0147 |           3.0099 |           2.6321 |
[32m[20230203 20:53:44 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:53:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 160.51
[32m[20230203 20:53:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 164.31
[32m[20230203 20:53:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 189.27
[32m[20230203 20:53:45 @agent_ppo2.py:151][0m Total time:       3.16 min
[32m[20230203 20:53:45 @agent_ppo2.py:153][0m 233472 total steps have happened
[32m[20230203 20:53:45 @agent_ppo2.py:129][0m #------------------------ Iteration 114 --------------------------#
[32m[20230203 20:53:45 @agent_ppo2.py:135][0m Sampling time: 0.57 s by 1 slaves
[32m[20230203 20:53:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:45 @agent_ppo2.py:193][0m |          -0.0008 |           2.3522 |           2.6244 |
[32m[20230203 20:53:45 @agent_ppo2.py:193][0m |          -0.0045 |           2.1205 |           2.6200 |
[32m[20230203 20:53:46 @agent_ppo2.py:193][0m |          -0.0071 |           2.0154 |           2.6154 |
[32m[20230203 20:53:46 @agent_ppo2.py:193][0m |          -0.0074 |           1.9623 |           2.6148 |
[32m[20230203 20:53:46 @agent_ppo2.py:193][0m |          -0.0082 |           1.9287 |           2.6177 |
[32m[20230203 20:53:46 @agent_ppo2.py:193][0m |          -0.0090 |           1.9033 |           2.6172 |
[32m[20230203 20:53:46 @agent_ppo2.py:193][0m |          -0.0090 |           1.8807 |           2.6181 |
[32m[20230203 20:53:46 @agent_ppo2.py:193][0m |          -0.0090 |           1.8603 |           2.6175 |
[32m[20230203 20:53:46 @agent_ppo2.py:193][0m |          -0.0100 |           1.8424 |           2.6189 |
[32m[20230203 20:53:46 @agent_ppo2.py:193][0m |          -0.0103 |           1.8288 |           2.6176 |
[32m[20230203 20:53:46 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230203 20:53:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 160.06
[32m[20230203 20:53:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 168.72
[32m[20230203 20:53:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 195.74
[32m[20230203 20:53:46 @agent_ppo2.py:151][0m Total time:       3.19 min
[32m[20230203 20:53:46 @agent_ppo2.py:153][0m 235520 total steps have happened
[32m[20230203 20:53:46 @agent_ppo2.py:129][0m #------------------------ Iteration 115 --------------------------#
[32m[20230203 20:53:47 @agent_ppo2.py:135][0m Sampling time: 0.59 s by 1 slaves
[32m[20230203 20:53:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:47 @agent_ppo2.py:193][0m |          -0.0005 |           2.1058 |           2.6089 |
[32m[20230203 20:53:47 @agent_ppo2.py:193][0m |          -0.0047 |           2.0257 |           2.6035 |
[32m[20230203 20:53:47 @agent_ppo2.py:193][0m |          -0.0060 |           1.9886 |           2.6065 |
[32m[20230203 20:53:47 @agent_ppo2.py:193][0m |          -0.0066 |           1.9673 |           2.6057 |
[32m[20230203 20:53:47 @agent_ppo2.py:193][0m |          -0.0081 |           1.9434 |           2.6058 |
[32m[20230203 20:53:47 @agent_ppo2.py:193][0m |          -0.0087 |           1.9318 |           2.6073 |
[32m[20230203 20:53:48 @agent_ppo2.py:193][0m |          -0.0085 |           1.9198 |           2.6087 |
[32m[20230203 20:53:48 @agent_ppo2.py:193][0m |          -0.0105 |           1.8992 |           2.6080 |
[32m[20230203 20:53:48 @agent_ppo2.py:193][0m |          -0.0105 |           1.8910 |           2.6104 |
[32m[20230203 20:53:48 @agent_ppo2.py:193][0m |          -0.0109 |           1.8771 |           2.6125 |
[32m[20230203 20:53:48 @agent_ppo2.py:138][0m Policy update time: 0.66 s
[32m[20230203 20:53:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 149.61
[32m[20230203 20:53:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 153.43
[32m[20230203 20:53:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 197.70
[32m[20230203 20:53:48 @agent_ppo2.py:151][0m Total time:       3.22 min
[32m[20230203 20:53:48 @agent_ppo2.py:153][0m 237568 total steps have happened
[32m[20230203 20:53:48 @agent_ppo2.py:129][0m #------------------------ Iteration 116 --------------------------#
[32m[20230203 20:53:49 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:53:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:49 @agent_ppo2.py:193][0m |          -0.0011 |          12.3173 |           2.6985 |
[32m[20230203 20:53:49 @agent_ppo2.py:193][0m |          -0.0076 |           6.2399 |           2.6962 |
[32m[20230203 20:53:49 @agent_ppo2.py:193][0m |          -0.0105 |           5.4192 |           2.6922 |
[32m[20230203 20:53:49 @agent_ppo2.py:193][0m |          -0.0120 |           5.0300 |           2.6927 |
[32m[20230203 20:53:49 @agent_ppo2.py:193][0m |          -0.0131 |           4.8211 |           2.6925 |
[32m[20230203 20:53:49 @agent_ppo2.py:193][0m |          -0.0139 |           4.7104 |           2.6919 |
[32m[20230203 20:53:49 @agent_ppo2.py:193][0m |          -0.0143 |           4.5858 |           2.6921 |
[32m[20230203 20:53:49 @agent_ppo2.py:193][0m |          -0.0150 |           4.4534 |           2.6925 |
[32m[20230203 20:53:49 @agent_ppo2.py:193][0m |          -0.0155 |           4.3707 |           2.6927 |
[32m[20230203 20:53:49 @agent_ppo2.py:193][0m |          -0.0160 |           4.3069 |           2.6932 |
[32m[20230203 20:53:49 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:53:50 @agent_ppo2.py:146][0m Average TRAINING episode reward: 88.13
[32m[20230203 20:53:50 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 174.77
[32m[20230203 20:53:50 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 189.11
[32m[20230203 20:53:50 @agent_ppo2.py:151][0m Total time:       3.24 min
[32m[20230203 20:53:50 @agent_ppo2.py:153][0m 239616 total steps have happened
[32m[20230203 20:53:50 @agent_ppo2.py:129][0m #------------------------ Iteration 117 --------------------------#
[32m[20230203 20:53:50 @agent_ppo2.py:135][0m Sampling time: 0.59 s by 1 slaves
[32m[20230203 20:53:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:51 @agent_ppo2.py:193][0m |           0.0067 |          14.2734 |           2.6621 |
[32m[20230203 20:53:51 @agent_ppo2.py:193][0m |          -0.0023 |           7.3320 |           2.6620 |
[32m[20230203 20:53:51 @agent_ppo2.py:193][0m |          -0.0063 |           6.2890 |           2.6608 |
[32m[20230203 20:53:51 @agent_ppo2.py:193][0m |          -0.0075 |           5.9775 |           2.6619 |
[32m[20230203 20:53:51 @agent_ppo2.py:193][0m |          -0.0116 |           5.8096 |           2.6625 |
[32m[20230203 20:53:51 @agent_ppo2.py:193][0m |          -0.0061 |           5.6852 |           2.6625 |
[32m[20230203 20:53:51 @agent_ppo2.py:193][0m |          -0.0083 |           5.4676 |           2.6633 |
[32m[20230203 20:53:51 @agent_ppo2.py:193][0m |          -0.0078 |           5.3626 |           2.6645 |
[32m[20230203 20:53:51 @agent_ppo2.py:193][0m |          -0.0129 |           5.2797 |           2.6634 |
[32m[20230203 20:53:51 @agent_ppo2.py:193][0m |          -0.0053 |           5.2784 |           2.6662 |
[32m[20230203 20:53:51 @agent_ppo2.py:138][0m Policy update time: 0.66 s
[32m[20230203 20:53:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 70.29
[32m[20230203 20:53:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 171.81
[32m[20230203 20:53:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 194.31
[32m[20230203 20:53:52 @agent_ppo2.py:151][0m Total time:       3.27 min
[32m[20230203 20:53:52 @agent_ppo2.py:153][0m 241664 total steps have happened
[32m[20230203 20:53:52 @agent_ppo2.py:129][0m #------------------------ Iteration 118 --------------------------#
[32m[20230203 20:53:52 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:53:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:52 @agent_ppo2.py:193][0m |          -0.0105 |          18.6080 |           2.7002 |
[32m[20230203 20:53:52 @agent_ppo2.py:193][0m |          -0.0218 |          10.8731 |           2.6964 |
[32m[20230203 20:53:52 @agent_ppo2.py:193][0m |           0.0004 |           8.5644 |           2.6900 |
[32m[20230203 20:53:52 @agent_ppo2.py:193][0m |          -0.0114 |           7.3215 |           2.6935 |
[32m[20230203 20:53:52 @agent_ppo2.py:193][0m |          -0.0148 |           6.4621 |           2.6959 |
[32m[20230203 20:53:52 @agent_ppo2.py:193][0m |          -0.0318 |           5.9177 |           2.6979 |
[32m[20230203 20:53:52 @agent_ppo2.py:193][0m |          -0.0160 |           5.5731 |           2.6864 |
[32m[20230203 20:53:52 @agent_ppo2.py:193][0m |          -0.0186 |           5.1109 |           2.6894 |
[32m[20230203 20:53:52 @agent_ppo2.py:193][0m |          -0.0127 |           4.8354 |           2.6964 |
[32m[20230203 20:53:53 @agent_ppo2.py:193][0m |          -0.0129 |           4.8683 |           2.6959 |
[32m[20230203 20:53:53 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:53:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 63.97
[32m[20230203 20:53:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 160.58
[32m[20230203 20:53:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 192.85
[32m[20230203 20:53:53 @agent_ppo2.py:151][0m Total time:       3.30 min
[32m[20230203 20:53:53 @agent_ppo2.py:153][0m 243712 total steps have happened
[32m[20230203 20:53:53 @agent_ppo2.py:129][0m #------------------------ Iteration 119 --------------------------#
[32m[20230203 20:53:54 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:53:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:54 @agent_ppo2.py:193][0m |          -0.0008 |           2.1889 |           2.7282 |
[32m[20230203 20:53:54 @agent_ppo2.py:193][0m |          -0.0062 |           1.8494 |           2.7250 |
[32m[20230203 20:53:54 @agent_ppo2.py:193][0m |          -0.0081 |           1.7266 |           2.7240 |
[32m[20230203 20:53:54 @agent_ppo2.py:193][0m |          -0.0093 |           1.6590 |           2.7219 |
[32m[20230203 20:53:54 @agent_ppo2.py:193][0m |          -0.0104 |           1.6109 |           2.7229 |
[32m[20230203 20:53:54 @agent_ppo2.py:193][0m |          -0.0112 |           1.5777 |           2.7241 |
[32m[20230203 20:53:54 @agent_ppo2.py:193][0m |          -0.0118 |           1.5521 |           2.7243 |
[32m[20230203 20:53:54 @agent_ppo2.py:193][0m |          -0.0124 |           1.5317 |           2.7238 |
[32m[20230203 20:53:54 @agent_ppo2.py:193][0m |          -0.0129 |           1.5174 |           2.7239 |
[32m[20230203 20:53:54 @agent_ppo2.py:193][0m |          -0.0133 |           1.4969 |           2.7234 |
[32m[20230203 20:53:54 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 171.25
[32m[20230203 20:53:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 178.71
[32m[20230203 20:53:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 191.98
[32m[20230203 20:53:55 @agent_ppo2.py:151][0m Total time:       3.32 min
[32m[20230203 20:53:55 @agent_ppo2.py:153][0m 245760 total steps have happened
[32m[20230203 20:53:55 @agent_ppo2.py:129][0m #------------------------ Iteration 120 --------------------------#
[32m[20230203 20:53:55 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:53:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:55 @agent_ppo2.py:193][0m |           0.0000 |           2.4910 |           2.7546 |
[32m[20230203 20:53:55 @agent_ppo2.py:193][0m |          -0.0038 |           2.2977 |           2.7486 |
[32m[20230203 20:53:55 @agent_ppo2.py:193][0m |          -0.0055 |           2.1991 |           2.7449 |
[32m[20230203 20:53:56 @agent_ppo2.py:193][0m |          -0.0068 |           2.1358 |           2.7467 |
[32m[20230203 20:53:56 @agent_ppo2.py:193][0m |          -0.0076 |           2.0840 |           2.7466 |
[32m[20230203 20:53:56 @agent_ppo2.py:193][0m |          -0.0079 |           2.0344 |           2.7474 |
[32m[20230203 20:53:56 @agent_ppo2.py:193][0m |          -0.0085 |           2.0014 |           2.7449 |
[32m[20230203 20:53:56 @agent_ppo2.py:193][0m |          -0.0084 |           1.9628 |           2.7457 |
[32m[20230203 20:53:56 @agent_ppo2.py:193][0m |          -0.0094 |           1.9334 |           2.7479 |
[32m[20230203 20:53:56 @agent_ppo2.py:193][0m |          -0.0100 |           1.9073 |           2.7465 |
[32m[20230203 20:53:56 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 161.51
[32m[20230203 20:53:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 175.20
[32m[20230203 20:53:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 186.94
[32m[20230203 20:53:56 @agent_ppo2.py:151][0m Total time:       3.35 min
[32m[20230203 20:53:56 @agent_ppo2.py:153][0m 247808 total steps have happened
[32m[20230203 20:53:56 @agent_ppo2.py:129][0m #------------------------ Iteration 121 --------------------------#
[32m[20230203 20:53:57 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:53:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:57 @agent_ppo2.py:193][0m |           0.0009 |           2.0964 |           2.7308 |
[32m[20230203 20:53:57 @agent_ppo2.py:193][0m |          -0.0035 |           1.9998 |           2.7314 |
[32m[20230203 20:53:57 @agent_ppo2.py:193][0m |          -0.0059 |           1.9724 |           2.7299 |
[32m[20230203 20:53:57 @agent_ppo2.py:193][0m |          -0.0075 |           1.9515 |           2.7292 |
[32m[20230203 20:53:57 @agent_ppo2.py:193][0m |          -0.0084 |           1.9353 |           2.7283 |
[32m[20230203 20:53:57 @agent_ppo2.py:193][0m |          -0.0094 |           1.9212 |           2.7291 |
[32m[20230203 20:53:57 @agent_ppo2.py:193][0m |          -0.0097 |           1.9117 |           2.7280 |
[32m[20230203 20:53:57 @agent_ppo2.py:193][0m |          -0.0108 |           1.9010 |           2.7275 |
[32m[20230203 20:53:57 @agent_ppo2.py:193][0m |          -0.0109 |           1.8995 |           2.7288 |
[32m[20230203 20:53:58 @agent_ppo2.py:193][0m |          -0.0113 |           1.8861 |           2.7299 |
[32m[20230203 20:53:58 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:53:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 178.82
[32m[20230203 20:53:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 193.18
[32m[20230203 20:53:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 207.32
[32m[20230203 20:53:58 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 207.32
[32m[20230203 20:53:58 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 207.32
[32m[20230203 20:53:58 @agent_ppo2.py:151][0m Total time:       3.38 min
[32m[20230203 20:53:58 @agent_ppo2.py:153][0m 249856 total steps have happened
[32m[20230203 20:53:58 @agent_ppo2.py:129][0m #------------------------ Iteration 122 --------------------------#
[32m[20230203 20:53:58 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:53:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:53:59 @agent_ppo2.py:193][0m |          -0.0033 |          14.0241 |           2.6952 |
[32m[20230203 20:53:59 @agent_ppo2.py:193][0m |          -0.0066 |           5.8783 |           2.6938 |
[32m[20230203 20:53:59 @agent_ppo2.py:193][0m |          -0.0086 |           5.1572 |           2.6930 |
[32m[20230203 20:53:59 @agent_ppo2.py:193][0m |           0.0272 |           4.2394 |           2.6917 |
[32m[20230203 20:53:59 @agent_ppo2.py:193][0m |          -0.0098 |           4.3590 |           2.6882 |
[32m[20230203 20:53:59 @agent_ppo2.py:193][0m |           0.0072 |           3.8222 |           2.6868 |
[32m[20230203 20:53:59 @agent_ppo2.py:193][0m |          -0.0135 |           3.6946 |           2.6851 |
[32m[20230203 20:53:59 @agent_ppo2.py:193][0m |           0.0029 |           3.5200 |           2.6832 |
[32m[20230203 20:53:59 @agent_ppo2.py:193][0m |           0.0042 |           3.4431 |           2.6805 |
[32m[20230203 20:53:59 @agent_ppo2.py:193][0m |          -0.0141 |           3.4536 |           2.6763 |
[32m[20230203 20:53:59 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 20:53:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 44.84
[32m[20230203 20:53:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 152.43
[32m[20230203 20:53:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 208.54
[32m[20230203 20:53:59 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 208.54
[32m[20230203 20:53:59 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 208.54
[32m[20230203 20:53:59 @agent_ppo2.py:151][0m Total time:       3.40 min
[32m[20230203 20:53:59 @agent_ppo2.py:153][0m 251904 total steps have happened
[32m[20230203 20:53:59 @agent_ppo2.py:129][0m #------------------------ Iteration 123 --------------------------#
[32m[20230203 20:54:00 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:54:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:00 @agent_ppo2.py:193][0m |          -0.0049 |          12.3401 |           2.6961 |
[32m[20230203 20:54:00 @agent_ppo2.py:193][0m |          -0.0071 |           7.6808 |           2.6915 |
[32m[20230203 20:54:00 @agent_ppo2.py:193][0m |          -0.0137 |           6.8384 |           2.6894 |
[32m[20230203 20:54:00 @agent_ppo2.py:193][0m |          -0.0134 |           6.4676 |           2.6890 |
[32m[20230203 20:54:00 @agent_ppo2.py:193][0m |          -0.0135 |           6.2734 |           2.6883 |
[32m[20230203 20:54:00 @agent_ppo2.py:193][0m |          -0.0155 |           6.1661 |           2.6880 |
[32m[20230203 20:54:00 @agent_ppo2.py:193][0m |          -0.0137 |           6.0861 |           2.6869 |
[32m[20230203 20:54:00 @agent_ppo2.py:193][0m |          -0.0146 |           6.0244 |           2.6853 |
[32m[20230203 20:54:00 @agent_ppo2.py:193][0m |          -0.0116 |           6.1631 |           2.6877 |
[32m[20230203 20:54:01 @agent_ppo2.py:193][0m |          -0.0167 |           5.8572 |           2.6864 |
[32m[20230203 20:54:01 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:54:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 73.64
[32m[20230203 20:54:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 173.40
[32m[20230203 20:54:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 217.76
[32m[20230203 20:54:01 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 217.76
[32m[20230203 20:54:01 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 217.76
[32m[20230203 20:54:01 @agent_ppo2.py:151][0m Total time:       3.43 min
[32m[20230203 20:54:01 @agent_ppo2.py:153][0m 253952 total steps have happened
[32m[20230203 20:54:01 @agent_ppo2.py:129][0m #------------------------ Iteration 124 --------------------------#
[32m[20230203 20:54:02 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:54:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:02 @agent_ppo2.py:193][0m |          -0.0002 |           2.2473 |           2.7629 |
[32m[20230203 20:54:02 @agent_ppo2.py:193][0m |          -0.0062 |           2.1285 |           2.7582 |
[32m[20230203 20:54:02 @agent_ppo2.py:193][0m |          -0.0081 |           2.0859 |           2.7583 |
[32m[20230203 20:54:02 @agent_ppo2.py:193][0m |          -0.0097 |           2.0548 |           2.7576 |
[32m[20230203 20:54:02 @agent_ppo2.py:193][0m |          -0.0101 |           2.0409 |           2.7593 |
[32m[20230203 20:54:02 @agent_ppo2.py:193][0m |          -0.0108 |           2.0256 |           2.7604 |
[32m[20230203 20:54:02 @agent_ppo2.py:193][0m |          -0.0115 |           2.0067 |           2.7602 |
[32m[20230203 20:54:02 @agent_ppo2.py:193][0m |          -0.0118 |           1.9932 |           2.7611 |
[32m[20230203 20:54:02 @agent_ppo2.py:193][0m |          -0.0124 |           1.9851 |           2.7630 |
[32m[20230203 20:54:02 @agent_ppo2.py:193][0m |          -0.0128 |           1.9753 |           2.7633 |
[32m[20230203 20:54:02 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 178.84
[32m[20230203 20:54:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 182.31
[32m[20230203 20:54:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 223.54
[32m[20230203 20:54:03 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 223.54
[32m[20230203 20:54:03 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 223.54
[32m[20230203 20:54:03 @agent_ppo2.py:151][0m Total time:       3.46 min
[32m[20230203 20:54:03 @agent_ppo2.py:153][0m 256000 total steps have happened
[32m[20230203 20:54:03 @agent_ppo2.py:129][0m #------------------------ Iteration 125 --------------------------#
[32m[20230203 20:54:03 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:54:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:03 @agent_ppo2.py:193][0m |           0.0014 |           2.6622 |           2.7464 |
[32m[20230203 20:54:03 @agent_ppo2.py:193][0m |          -0.0019 |           2.4922 |           2.7437 |
[32m[20230203 20:54:04 @agent_ppo2.py:193][0m |          -0.0054 |           2.3902 |           2.7423 |
[32m[20230203 20:54:04 @agent_ppo2.py:193][0m |          -0.0068 |           2.3207 |           2.7392 |
[32m[20230203 20:54:04 @agent_ppo2.py:193][0m |          -0.0075 |           2.2769 |           2.7376 |
[32m[20230203 20:54:04 @agent_ppo2.py:193][0m |          -0.0075 |           2.2455 |           2.7425 |
[32m[20230203 20:54:04 @agent_ppo2.py:193][0m |          -0.0091 |           2.2223 |           2.7377 |
[32m[20230203 20:54:04 @agent_ppo2.py:193][0m |          -0.0099 |           2.1943 |           2.7380 |
[32m[20230203 20:54:04 @agent_ppo2.py:193][0m |          -0.0106 |           2.1815 |           2.7386 |
[32m[20230203 20:54:04 @agent_ppo2.py:193][0m |          -0.0109 |           2.1642 |           2.7395 |
[32m[20230203 20:54:04 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 179.60
[32m[20230203 20:54:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 179.62
[32m[20230203 20:54:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 216.83
[32m[20230203 20:54:04 @agent_ppo2.py:151][0m Total time:       3.49 min
[32m[20230203 20:54:04 @agent_ppo2.py:153][0m 258048 total steps have happened
[32m[20230203 20:54:04 @agent_ppo2.py:129][0m #------------------------ Iteration 126 --------------------------#
[32m[20230203 20:54:05 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:54:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:05 @agent_ppo2.py:193][0m |          -0.0001 |           2.1282 |           2.7722 |
[32m[20230203 20:54:05 @agent_ppo2.py:193][0m |          -0.0051 |           2.0581 |           2.7710 |
[32m[20230203 20:54:05 @agent_ppo2.py:193][0m |          -0.0063 |           2.0178 |           2.7677 |
[32m[20230203 20:54:05 @agent_ppo2.py:193][0m |          -0.0080 |           1.9962 |           2.7680 |
[32m[20230203 20:54:05 @agent_ppo2.py:193][0m |          -0.0075 |           1.9741 |           2.7692 |
[32m[20230203 20:54:05 @agent_ppo2.py:193][0m |          -0.0090 |           1.9577 |           2.7709 |
[32m[20230203 20:54:05 @agent_ppo2.py:193][0m |          -0.0099 |           1.9420 |           2.7713 |
[32m[20230203 20:54:05 @agent_ppo2.py:193][0m |          -0.0102 |           1.9281 |           2.7731 |
[32m[20230203 20:54:06 @agent_ppo2.py:193][0m |          -0.0102 |           1.9145 |           2.7730 |
[32m[20230203 20:54:06 @agent_ppo2.py:193][0m |          -0.0112 |           1.9021 |           2.7739 |
[32m[20230203 20:54:06 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 181.70
[32m[20230203 20:54:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 183.98
[32m[20230203 20:54:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 210.85
[32m[20230203 20:54:06 @agent_ppo2.py:151][0m Total time:       3.51 min
[32m[20230203 20:54:06 @agent_ppo2.py:153][0m 260096 total steps have happened
[32m[20230203 20:54:06 @agent_ppo2.py:129][0m #------------------------ Iteration 127 --------------------------#
[32m[20230203 20:54:07 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:54:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:07 @agent_ppo2.py:193][0m |          -0.0007 |          20.2127 |           2.7745 |
[32m[20230203 20:54:07 @agent_ppo2.py:193][0m |          -0.0042 |           7.4265 |           2.7704 |
[32m[20230203 20:54:07 @agent_ppo2.py:193][0m |          -0.0055 |           5.9069 |           2.7695 |
[32m[20230203 20:54:07 @agent_ppo2.py:193][0m |          -0.0063 |           5.1882 |           2.7683 |
[32m[20230203 20:54:07 @agent_ppo2.py:193][0m |          -0.0076 |           4.7304 |           2.7667 |
[32m[20230203 20:54:07 @agent_ppo2.py:193][0m |          -0.0080 |           4.3922 |           2.7670 |
[32m[20230203 20:54:07 @agent_ppo2.py:193][0m |          -0.0088 |           4.1649 |           2.7655 |
[32m[20230203 20:54:07 @agent_ppo2.py:193][0m |          -0.0090 |           4.0510 |           2.7656 |
[32m[20230203 20:54:07 @agent_ppo2.py:193][0m |          -0.0094 |           3.8291 |           2.7637 |
[32m[20230203 20:54:07 @agent_ppo2.py:193][0m |          -0.0099 |           3.7516 |           2.7633 |
[32m[20230203 20:54:07 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:54:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 85.57
[32m[20230203 20:54:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 184.26
[32m[20230203 20:54:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 228.37
[32m[20230203 20:54:08 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 228.37
[32m[20230203 20:54:08 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 228.37
[32m[20230203 20:54:08 @agent_ppo2.py:151][0m Total time:       3.54 min
[32m[20230203 20:54:08 @agent_ppo2.py:153][0m 262144 total steps have happened
[32m[20230203 20:54:08 @agent_ppo2.py:129][0m #------------------------ Iteration 128 --------------------------#
[32m[20230203 20:54:08 @agent_ppo2.py:135][0m Sampling time: 0.60 s by 1 slaves
[32m[20230203 20:54:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:09 @agent_ppo2.py:193][0m |          -0.0010 |          34.4641 |           2.8157 |
[32m[20230203 20:54:09 @agent_ppo2.py:193][0m |          -0.0076 |          21.0727 |           2.8126 |
[32m[20230203 20:54:09 @agent_ppo2.py:193][0m |          -0.0087 |          18.3069 |           2.8105 |
[32m[20230203 20:54:09 @agent_ppo2.py:193][0m |          -0.0068 |          16.1739 |           2.8099 |
[32m[20230203 20:54:09 @agent_ppo2.py:193][0m |          -0.0006 |          15.2081 |           2.8087 |
[32m[20230203 20:54:09 @agent_ppo2.py:193][0m |          -0.0118 |          14.0750 |           2.8078 |
[32m[20230203 20:54:09 @agent_ppo2.py:193][0m |          -0.0109 |          13.5820 |           2.8076 |
[32m[20230203 20:54:09 @agent_ppo2.py:193][0m |          -0.0114 |          12.8650 |           2.8078 |
[32m[20230203 20:54:09 @agent_ppo2.py:193][0m |          -0.0129 |          12.6543 |           2.8075 |
[32m[20230203 20:54:09 @agent_ppo2.py:193][0m |          -0.0138 |          12.0741 |           2.8079 |
[32m[20230203 20:54:09 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230203 20:54:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 66.34
[32m[20230203 20:54:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 182.96
[32m[20230203 20:54:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 240.83
[32m[20230203 20:54:10 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 240.83
[32m[20230203 20:54:10 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 240.83
[32m[20230203 20:54:10 @agent_ppo2.py:151][0m Total time:       3.57 min
[32m[20230203 20:54:10 @agent_ppo2.py:153][0m 264192 total steps have happened
[32m[20230203 20:54:10 @agent_ppo2.py:129][0m #------------------------ Iteration 129 --------------------------#
[32m[20230203 20:54:10 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:54:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:10 @agent_ppo2.py:193][0m |           0.0000 |           8.6182 |           2.7768 |
[32m[20230203 20:54:10 @agent_ppo2.py:193][0m |          -0.0022 |           6.4215 |           2.7768 |
[32m[20230203 20:54:10 @agent_ppo2.py:193][0m |          -0.0027 |           5.9328 |           2.7774 |
[32m[20230203 20:54:10 @agent_ppo2.py:193][0m |          -0.0049 |           5.6444 |           2.7768 |
[32m[20230203 20:54:10 @agent_ppo2.py:193][0m |          -0.0064 |           5.3863 |           2.7780 |
[32m[20230203 20:54:10 @agent_ppo2.py:193][0m |          -0.0072 |           5.2531 |           2.7792 |
[32m[20230203 20:54:11 @agent_ppo2.py:193][0m |          -0.0075 |           5.0730 |           2.7802 |
[32m[20230203 20:54:11 @agent_ppo2.py:193][0m |          -0.0077 |           4.9847 |           2.7820 |
[32m[20230203 20:54:11 @agent_ppo2.py:193][0m |          -0.0084 |           4.8173 |           2.7814 |
[32m[20230203 20:54:11 @agent_ppo2.py:193][0m |          -0.0091 |           4.7529 |           2.7821 |
[32m[20230203 20:54:11 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 197.81
[32m[20230203 20:54:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 211.70
[32m[20230203 20:54:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 245.79
[32m[20230203 20:54:11 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 245.79
[32m[20230203 20:54:11 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 245.79
[32m[20230203 20:54:11 @agent_ppo2.py:151][0m Total time:       3.60 min
[32m[20230203 20:54:11 @agent_ppo2.py:153][0m 266240 total steps have happened
[32m[20230203 20:54:11 @agent_ppo2.py:129][0m #------------------------ Iteration 130 --------------------------#
[32m[20230203 20:54:12 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:54:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:12 @agent_ppo2.py:193][0m |          -0.0008 |           4.4487 |           2.8522 |
[32m[20230203 20:54:12 @agent_ppo2.py:193][0m |          -0.0074 |           3.6716 |           2.8516 |
[32m[20230203 20:54:12 @agent_ppo2.py:193][0m |          -0.0097 |           3.4956 |           2.8494 |
[32m[20230203 20:54:12 @agent_ppo2.py:193][0m |          -0.0110 |           3.4016 |           2.8493 |
[32m[20230203 20:54:12 @agent_ppo2.py:193][0m |          -0.0119 |           3.3587 |           2.8506 |
[32m[20230203 20:54:12 @agent_ppo2.py:193][0m |          -0.0121 |           3.3056 |           2.8517 |
[32m[20230203 20:54:12 @agent_ppo2.py:193][0m |          -0.0128 |           3.2694 |           2.8499 |
[32m[20230203 20:54:12 @agent_ppo2.py:193][0m |          -0.0137 |           3.2328 |           2.8505 |
[32m[20230203 20:54:12 @agent_ppo2.py:193][0m |          -0.0140 |           3.2151 |           2.8514 |
[32m[20230203 20:54:12 @agent_ppo2.py:193][0m |          -0.0142 |           3.1842 |           2.8546 |
[32m[20230203 20:54:12 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 198.02
[32m[20230203 20:54:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 199.13
[32m[20230203 20:54:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.29
[32m[20230203 20:54:13 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 254.29
[32m[20230203 20:54:13 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 254.29
[32m[20230203 20:54:13 @agent_ppo2.py:151][0m Total time:       3.63 min
[32m[20230203 20:54:13 @agent_ppo2.py:153][0m 268288 total steps have happened
[32m[20230203 20:54:13 @agent_ppo2.py:129][0m #------------------------ Iteration 131 --------------------------#
[32m[20230203 20:54:13 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:54:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:13 @agent_ppo2.py:193][0m |          -0.0003 |           2.5768 |           2.8669 |
[32m[20230203 20:54:13 @agent_ppo2.py:193][0m |          -0.0058 |           2.4317 |           2.8634 |
[32m[20230203 20:54:14 @agent_ppo2.py:193][0m |          -0.0073 |           2.3939 |           2.8607 |
[32m[20230203 20:54:14 @agent_ppo2.py:193][0m |          -0.0086 |           2.3670 |           2.8607 |
[32m[20230203 20:54:14 @agent_ppo2.py:193][0m |          -0.0092 |           2.3498 |           2.8580 |
[32m[20230203 20:54:14 @agent_ppo2.py:193][0m |          -0.0106 |           2.3264 |           2.8587 |
[32m[20230203 20:54:14 @agent_ppo2.py:193][0m |          -0.0110 |           2.3163 |           2.8590 |
[32m[20230203 20:54:14 @agent_ppo2.py:193][0m |          -0.0117 |           2.3029 |           2.8558 |
[32m[20230203 20:54:14 @agent_ppo2.py:193][0m |          -0.0122 |           2.2840 |           2.8583 |
[32m[20230203 20:54:14 @agent_ppo2.py:193][0m |          -0.0128 |           2.2695 |           2.8571 |
[32m[20230203 20:54:14 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 200.80
[32m[20230203 20:54:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 213.90
[32m[20230203 20:54:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 246.73
[32m[20230203 20:54:14 @agent_ppo2.py:151][0m Total time:       3.65 min
[32m[20230203 20:54:14 @agent_ppo2.py:153][0m 270336 total steps have happened
[32m[20230203 20:54:14 @agent_ppo2.py:129][0m #------------------------ Iteration 132 --------------------------#
[32m[20230203 20:54:15 @agent_ppo2.py:135][0m Sampling time: 0.52 s by 1 slaves
[32m[20230203 20:54:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:15 @agent_ppo2.py:193][0m |          -0.0007 |           8.3830 |           2.7938 |
[32m[20230203 20:54:15 @agent_ppo2.py:193][0m |          -0.0066 |           4.7163 |           2.7936 |
[32m[20230203 20:54:15 @agent_ppo2.py:193][0m |          -0.0095 |           4.3790 |           2.7883 |
[32m[20230203 20:54:15 @agent_ppo2.py:193][0m |          -0.0100 |           4.1518 |           2.7874 |
[32m[20230203 20:54:15 @agent_ppo2.py:193][0m |          -0.0142 |           3.9801 |           2.7863 |
[32m[20230203 20:54:15 @agent_ppo2.py:193][0m |          -0.0115 |           3.9429 |           2.7880 |
[32m[20230203 20:54:15 @agent_ppo2.py:193][0m |          -0.0156 |           3.7895 |           2.7851 |
[32m[20230203 20:54:15 @agent_ppo2.py:193][0m |          -0.0174 |           3.7693 |           2.7833 |
[32m[20230203 20:54:15 @agent_ppo2.py:193][0m |          -0.0115 |           3.7586 |           2.7844 |
[32m[20230203 20:54:16 @agent_ppo2.py:193][0m |          -0.0140 |           3.5966 |           2.7824 |
[32m[20230203 20:54:16 @agent_ppo2.py:138][0m Policy update time: 0.59 s
[32m[20230203 20:54:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 148.23
[32m[20230203 20:54:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 191.53
[32m[20230203 20:54:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 71.97
[32m[20230203 20:54:16 @agent_ppo2.py:151][0m Total time:       3.68 min
[32m[20230203 20:54:16 @agent_ppo2.py:153][0m 272384 total steps have happened
[32m[20230203 20:54:16 @agent_ppo2.py:129][0m #------------------------ Iteration 133 --------------------------#
[32m[20230203 20:54:16 @agent_ppo2.py:135][0m Sampling time: 0.48 s by 1 slaves
[32m[20230203 20:54:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:16 @agent_ppo2.py:193][0m |          -0.0007 |          11.9226 |           2.7842 |
[32m[20230203 20:54:17 @agent_ppo2.py:193][0m |          -0.0043 |           7.7195 |           2.7800 |
[32m[20230203 20:54:17 @agent_ppo2.py:193][0m |          -0.0054 |           6.7285 |           2.7791 |
[32m[20230203 20:54:17 @agent_ppo2.py:193][0m |          -0.0065 |           6.1475 |           2.7776 |
[32m[20230203 20:54:17 @agent_ppo2.py:193][0m |          -0.0079 |           5.5103 |           2.7770 |
[32m[20230203 20:54:17 @agent_ppo2.py:193][0m |          -0.0085 |           5.5923 |           2.7761 |
[32m[20230203 20:54:17 @agent_ppo2.py:193][0m |          -0.0069 |           5.1653 |           2.7784 |
[32m[20230203 20:54:17 @agent_ppo2.py:193][0m |          -0.0109 |           4.8638 |           2.7764 |
[32m[20230203 20:54:17 @agent_ppo2.py:193][0m |          -0.0109 |           4.7832 |           2.7765 |
[32m[20230203 20:54:17 @agent_ppo2.py:193][0m |          -0.0101 |           4.6099 |           2.7777 |
[32m[20230203 20:54:17 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:54:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 126.78
[32m[20230203 20:54:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 222.74
[32m[20230203 20:54:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 165.12
[32m[20230203 20:54:17 @agent_ppo2.py:151][0m Total time:       3.70 min
[32m[20230203 20:54:17 @agent_ppo2.py:153][0m 274432 total steps have happened
[32m[20230203 20:54:17 @agent_ppo2.py:129][0m #------------------------ Iteration 134 --------------------------#
[32m[20230203 20:54:18 @agent_ppo2.py:135][0m Sampling time: 0.60 s by 1 slaves
[32m[20230203 20:54:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:18 @agent_ppo2.py:193][0m |          -0.0022 |          13.7843 |           2.7345 |
[32m[20230203 20:54:18 @agent_ppo2.py:193][0m |          -0.0036 |          11.3831 |           2.7344 |
[32m[20230203 20:54:18 @agent_ppo2.py:193][0m |          -0.0124 |          10.4177 |           2.7337 |
[32m[20230203 20:54:18 @agent_ppo2.py:193][0m |          -0.0044 |           9.6391 |           2.7331 |
[32m[20230203 20:54:18 @agent_ppo2.py:193][0m |          -0.0078 |           9.1990 |           2.7349 |
[32m[20230203 20:54:18 @agent_ppo2.py:193][0m |          -0.0099 |           8.7582 |           2.7349 |
[32m[20230203 20:54:19 @agent_ppo2.py:193][0m |          -0.0210 |           8.2259 |           2.7349 |
[32m[20230203 20:54:19 @agent_ppo2.py:193][0m |          -0.0124 |           7.8740 |           2.7350 |
[32m[20230203 20:54:19 @agent_ppo2.py:193][0m |          -0.0077 |           7.8273 |           2.7350 |
[32m[20230203 20:54:19 @agent_ppo2.py:193][0m |          -0.0165 |           7.3370 |           2.7364 |
[32m[20230203 20:54:19 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230203 20:54:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 113.70
[32m[20230203 20:54:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 209.00
[32m[20230203 20:54:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 248.96
[32m[20230203 20:54:19 @agent_ppo2.py:151][0m Total time:       3.73 min
[32m[20230203 20:54:19 @agent_ppo2.py:153][0m 276480 total steps have happened
[32m[20230203 20:54:19 @agent_ppo2.py:129][0m #------------------------ Iteration 135 --------------------------#
[32m[20230203 20:54:20 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:54:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:20 @agent_ppo2.py:193][0m |           0.0014 |           3.9645 |           2.8595 |
[32m[20230203 20:54:20 @agent_ppo2.py:193][0m |          -0.0022 |           3.4035 |           2.8539 |
[32m[20230203 20:54:20 @agent_ppo2.py:193][0m |          -0.0049 |           3.3282 |           2.8564 |
[32m[20230203 20:54:20 @agent_ppo2.py:193][0m |          -0.0063 |           3.2703 |           2.8529 |
[32m[20230203 20:54:20 @agent_ppo2.py:193][0m |          -0.0078 |           3.2363 |           2.8531 |
[32m[20230203 20:54:20 @agent_ppo2.py:193][0m |          -0.0092 |           3.1955 |           2.8506 |
[32m[20230203 20:54:20 @agent_ppo2.py:193][0m |          -0.0097 |           3.1919 |           2.8504 |
[32m[20230203 20:54:20 @agent_ppo2.py:193][0m |          -0.0104 |           3.1583 |           2.8499 |
[32m[20230203 20:54:20 @agent_ppo2.py:193][0m |          -0.0104 |           3.1306 |           2.8485 |
[32m[20230203 20:54:20 @agent_ppo2.py:193][0m |          -0.0110 |           3.1142 |           2.8483 |
[32m[20230203 20:54:20 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 208.79
[32m[20230203 20:54:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 221.92
[32m[20230203 20:54:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 250.00
[32m[20230203 20:54:21 @agent_ppo2.py:151][0m Total time:       3.76 min
[32m[20230203 20:54:21 @agent_ppo2.py:153][0m 278528 total steps have happened
[32m[20230203 20:54:21 @agent_ppo2.py:129][0m #------------------------ Iteration 136 --------------------------#
[32m[20230203 20:54:21 @agent_ppo2.py:135][0m Sampling time: 0.36 s by 1 slaves
[32m[20230203 20:54:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:21 @agent_ppo2.py:193][0m |           0.0000 |          14.6716 |           2.8388 |
[32m[20230203 20:54:21 @agent_ppo2.py:193][0m |          -0.0028 |          11.6112 |           2.8357 |
[32m[20230203 20:54:21 @agent_ppo2.py:193][0m |          -0.0076 |          11.1970 |           2.8327 |
[32m[20230203 20:54:21 @agent_ppo2.py:193][0m |          -0.0088 |          10.9096 |           2.8288 |
[32m[20230203 20:54:21 @agent_ppo2.py:193][0m |          -0.0106 |          10.4787 |           2.8294 |
[32m[20230203 20:54:21 @agent_ppo2.py:193][0m |          -0.0115 |          10.3286 |           2.8263 |
[32m[20230203 20:54:21 @agent_ppo2.py:193][0m |          -0.0101 |          10.3466 |           2.8264 |
[32m[20230203 20:54:22 @agent_ppo2.py:193][0m |          -0.0122 |           9.9655 |           2.8241 |
[32m[20230203 20:54:22 @agent_ppo2.py:193][0m |          -0.0125 |           9.7535 |           2.8240 |
[32m[20230203 20:54:22 @agent_ppo2.py:193][0m |          -0.0137 |           9.6211 |           2.8221 |
[32m[20230203 20:54:22 @agent_ppo2.py:138][0m Policy update time: 0.40 s
[32m[20230203 20:54:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 79.84
[32m[20230203 20:54:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 195.44
[32m[20230203 20:54:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 122.88
[32m[20230203 20:54:22 @agent_ppo2.py:151][0m Total time:       3.78 min
[32m[20230203 20:54:22 @agent_ppo2.py:153][0m 280576 total steps have happened
[32m[20230203 20:54:22 @agent_ppo2.py:129][0m #------------------------ Iteration 137 --------------------------#
[32m[20230203 20:54:23 @agent_ppo2.py:135][0m Sampling time: 0.53 s by 1 slaves
[32m[20230203 20:54:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:23 @agent_ppo2.py:193][0m |          -0.0027 |           3.6261 |           2.8111 |
[32m[20230203 20:54:23 @agent_ppo2.py:193][0m |          -0.0044 |           2.8613 |           2.8092 |
[32m[20230203 20:54:23 @agent_ppo2.py:193][0m |          -0.0063 |           2.7866 |           2.8082 |
[32m[20230203 20:54:23 @agent_ppo2.py:193][0m |          -0.0087 |           2.7442 |           2.8072 |
[32m[20230203 20:54:23 @agent_ppo2.py:193][0m |          -0.0093 |           2.7085 |           2.8064 |
[32m[20230203 20:54:23 @agent_ppo2.py:193][0m |          -0.0099 |           2.6917 |           2.8056 |
[32m[20230203 20:54:23 @agent_ppo2.py:193][0m |          -0.0129 |           2.6740 |           2.8077 |
[32m[20230203 20:54:23 @agent_ppo2.py:193][0m |          -0.0110 |           2.6584 |           2.8071 |
[32m[20230203 20:54:23 @agent_ppo2.py:193][0m |          -0.0124 |           2.7121 |           2.8061 |
[32m[20230203 20:54:23 @agent_ppo2.py:193][0m |          -0.0131 |           2.6305 |           2.8066 |
[32m[20230203 20:54:23 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 222.85
[32m[20230203 20:54:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 231.85
[32m[20230203 20:54:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.37
[32m[20230203 20:54:24 @agent_ppo2.py:151][0m Total time:       3.81 min
[32m[20230203 20:54:24 @agent_ppo2.py:153][0m 282624 total steps have happened
[32m[20230203 20:54:24 @agent_ppo2.py:129][0m #------------------------ Iteration 138 --------------------------#
[32m[20230203 20:54:24 @agent_ppo2.py:135][0m Sampling time: 0.59 s by 1 slaves
[32m[20230203 20:54:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:24 @agent_ppo2.py:193][0m |          -0.0015 |          12.3454 |           2.7764 |
[32m[20230203 20:54:24 @agent_ppo2.py:193][0m |          -0.0040 |           6.3061 |           2.7763 |
[32m[20230203 20:54:24 @agent_ppo2.py:193][0m |          -0.0065 |           5.5058 |           2.7729 |
[32m[20230203 20:54:25 @agent_ppo2.py:193][0m |          -0.0075 |           5.1356 |           2.7722 |
[32m[20230203 20:54:25 @agent_ppo2.py:193][0m |          -0.0086 |           4.7292 |           2.7718 |
[32m[20230203 20:54:25 @agent_ppo2.py:193][0m |          -0.0071 |           4.8770 |           2.7735 |
[32m[20230203 20:54:25 @agent_ppo2.py:193][0m |           0.0025 |           4.4717 |           2.7726 |
[32m[20230203 20:54:25 @agent_ppo2.py:193][0m |          -0.0098 |           4.2591 |           2.7698 |
[32m[20230203 20:54:25 @agent_ppo2.py:193][0m |          -0.0115 |           4.1862 |           2.7714 |
[32m[20230203 20:54:25 @agent_ppo2.py:193][0m |          -0.0090 |           4.1038 |           2.7722 |
[32m[20230203 20:54:25 @agent_ppo2.py:138][0m Policy update time: 0.66 s
[32m[20230203 20:54:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 116.12
[32m[20230203 20:54:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 213.80
[32m[20230203 20:54:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.47
[32m[20230203 20:54:25 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 254.47
[32m[20230203 20:54:25 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 254.47
[32m[20230203 20:54:25 @agent_ppo2.py:151][0m Total time:       3.84 min
[32m[20230203 20:54:25 @agent_ppo2.py:153][0m 284672 total steps have happened
[32m[20230203 20:54:25 @agent_ppo2.py:129][0m #------------------------ Iteration 139 --------------------------#
[32m[20230203 20:54:26 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:54:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:26 @agent_ppo2.py:193][0m |           0.0029 |          27.9194 |           2.7635 |
[32m[20230203 20:54:26 @agent_ppo2.py:193][0m |          -0.0052 |          16.9217 |           2.7641 |
[32m[20230203 20:54:26 @agent_ppo2.py:193][0m |          -0.0046 |          15.5362 |           2.7598 |
[32m[20230203 20:54:26 @agent_ppo2.py:193][0m |          -0.0029 |          14.7318 |           2.7611 |
[32m[20230203 20:54:26 @agent_ppo2.py:193][0m |          -0.0077 |          13.0395 |           2.7616 |
[32m[20230203 20:54:26 @agent_ppo2.py:193][0m |          -0.0005 |          12.9420 |           2.7605 |
[32m[20230203 20:54:26 @agent_ppo2.py:193][0m |           0.0029 |          12.8135 |           2.7592 |
[32m[20230203 20:54:26 @agent_ppo2.py:193][0m |          -0.0063 |          11.7521 |           2.7589 |
[32m[20230203 20:54:26 @agent_ppo2.py:193][0m |          -0.0040 |          11.9204 |           2.7625 |
[32m[20230203 20:54:27 @agent_ppo2.py:193][0m |          -0.0103 |          11.2979 |           2.7616 |
[32m[20230203 20:54:27 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:54:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 103.00
[32m[20230203 20:54:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 227.63
[32m[20230203 20:54:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.29
[32m[20230203 20:54:27 @agent_ppo2.py:151][0m Total time:       3.86 min
[32m[20230203 20:54:27 @agent_ppo2.py:153][0m 286720 total steps have happened
[32m[20230203 20:54:27 @agent_ppo2.py:129][0m #------------------------ Iteration 140 --------------------------#
[32m[20230203 20:54:27 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:54:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:28 @agent_ppo2.py:193][0m |          -0.0040 |          26.4013 |           2.7782 |
[32m[20230203 20:54:28 @agent_ppo2.py:193][0m |          -0.0047 |          12.7731 |           2.7674 |
[32m[20230203 20:54:28 @agent_ppo2.py:193][0m |          -0.0037 |          10.1110 |           2.7710 |
[32m[20230203 20:54:28 @agent_ppo2.py:193][0m |          -0.0114 |           8.5150 |           2.7686 |
[32m[20230203 20:54:28 @agent_ppo2.py:193][0m |          -0.0102 |           7.7028 |           2.7734 |
[32m[20230203 20:54:28 @agent_ppo2.py:193][0m |          -0.0103 |           7.2420 |           2.7721 |
[32m[20230203 20:54:28 @agent_ppo2.py:193][0m |          -0.0123 |           6.7102 |           2.7700 |
[32m[20230203 20:54:28 @agent_ppo2.py:193][0m |          -0.0071 |           6.5820 |           2.7661 |
[32m[20230203 20:54:28 @agent_ppo2.py:193][0m |          -0.0130 |           6.2148 |           2.7681 |
[32m[20230203 20:54:28 @agent_ppo2.py:193][0m |          -0.0115 |           5.9754 |           2.7727 |
[32m[20230203 20:54:28 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 20:54:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 120.17
[32m[20230203 20:54:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 214.42
[32m[20230203 20:54:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 250.95
[32m[20230203 20:54:28 @agent_ppo2.py:151][0m Total time:       3.89 min
[32m[20230203 20:54:28 @agent_ppo2.py:153][0m 288768 total steps have happened
[32m[20230203 20:54:28 @agent_ppo2.py:129][0m #------------------------ Iteration 141 --------------------------#
[32m[20230203 20:54:29 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:54:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:29 @agent_ppo2.py:193][0m |          -0.0002 |           3.2591 |           2.8433 |
[32m[20230203 20:54:29 @agent_ppo2.py:193][0m |          -0.0061 |           2.9990 |           2.8440 |
[32m[20230203 20:54:29 @agent_ppo2.py:193][0m |          -0.0082 |           2.9195 |           2.8426 |
[32m[20230203 20:54:29 @agent_ppo2.py:193][0m |          -0.0099 |           2.8734 |           2.8433 |
[32m[20230203 20:54:29 @agent_ppo2.py:193][0m |          -0.0107 |           2.8369 |           2.8439 |
[32m[20230203 20:54:29 @agent_ppo2.py:193][0m |          -0.0116 |           2.8125 |           2.8445 |
[32m[20230203 20:54:29 @agent_ppo2.py:193][0m |          -0.0125 |           2.7889 |           2.8447 |
[32m[20230203 20:54:29 @agent_ppo2.py:193][0m |          -0.0126 |           2.7668 |           2.8476 |
[32m[20230203 20:54:29 @agent_ppo2.py:193][0m |          -0.0131 |           2.7494 |           2.8482 |
[32m[20230203 20:54:30 @agent_ppo2.py:193][0m |          -0.0133 |           2.7319 |           2.8489 |
[32m[20230203 20:54:30 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 205.33
[32m[20230203 20:54:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 209.07
[32m[20230203 20:54:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.84
[32m[20230203 20:54:30 @agent_ppo2.py:151][0m Total time:       3.91 min
[32m[20230203 20:54:30 @agent_ppo2.py:153][0m 290816 total steps have happened
[32m[20230203 20:54:30 @agent_ppo2.py:129][0m #------------------------ Iteration 142 --------------------------#
[32m[20230203 20:54:31 @agent_ppo2.py:135][0m Sampling time: 0.53 s by 1 slaves
[32m[20230203 20:54:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:31 @agent_ppo2.py:193][0m |          -0.0008 |           2.4826 |           2.8872 |
[32m[20230203 20:54:31 @agent_ppo2.py:193][0m |          -0.0049 |           2.3745 |           2.8805 |
[32m[20230203 20:54:31 @agent_ppo2.py:193][0m |          -0.0063 |           2.3304 |           2.8786 |
[32m[20230203 20:54:31 @agent_ppo2.py:193][0m |          -0.0067 |           2.2953 |           2.8776 |
[32m[20230203 20:54:31 @agent_ppo2.py:193][0m |          -0.0082 |           2.2697 |           2.8781 |
[32m[20230203 20:54:31 @agent_ppo2.py:193][0m |          -0.0085 |           2.2436 |           2.8807 |
[32m[20230203 20:54:31 @agent_ppo2.py:193][0m |          -0.0096 |           2.2255 |           2.8779 |
[32m[20230203 20:54:31 @agent_ppo2.py:193][0m |          -0.0095 |           2.2113 |           2.8794 |
[32m[20230203 20:54:31 @agent_ppo2.py:193][0m |          -0.0098 |           2.1882 |           2.8796 |
[32m[20230203 20:54:31 @agent_ppo2.py:193][0m |          -0.0105 |           2.1795 |           2.8794 |
[32m[20230203 20:54:31 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 219.83
[32m[20230203 20:54:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 222.22
[32m[20230203 20:54:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.42
[32m[20230203 20:54:32 @agent_ppo2.py:151][0m Total time:       3.94 min
[32m[20230203 20:54:32 @agent_ppo2.py:153][0m 292864 total steps have happened
[32m[20230203 20:54:32 @agent_ppo2.py:129][0m #------------------------ Iteration 143 --------------------------#
[32m[20230203 20:54:32 @agent_ppo2.py:135][0m Sampling time: 0.53 s by 1 slaves
[32m[20230203 20:54:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:32 @agent_ppo2.py:193][0m |           0.0002 |           2.8598 |           2.9872 |
[32m[20230203 20:54:32 @agent_ppo2.py:193][0m |          -0.0036 |           2.7198 |           2.9834 |
[32m[20230203 20:54:32 @agent_ppo2.py:193][0m |          -0.0046 |           2.6722 |           2.9829 |
[32m[20230203 20:54:32 @agent_ppo2.py:193][0m |          -0.0060 |           2.6440 |           2.9842 |
[32m[20230203 20:54:32 @agent_ppo2.py:193][0m |          -0.0068 |           2.6169 |           2.9852 |
[32m[20230203 20:54:32 @agent_ppo2.py:193][0m |          -0.0072 |           2.5983 |           2.9857 |
[32m[20230203 20:54:33 @agent_ppo2.py:193][0m |          -0.0082 |           2.6048 |           2.9851 |
[32m[20230203 20:54:33 @agent_ppo2.py:193][0m |          -0.0082 |           2.5815 |           2.9846 |
[32m[20230203 20:54:33 @agent_ppo2.py:193][0m |          -0.0090 |           2.5649 |           2.9860 |
[32m[20230203 20:54:33 @agent_ppo2.py:193][0m |          -0.0097 |           2.5534 |           2.9861 |
[32m[20230203 20:54:33 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 223.38
[32m[20230203 20:54:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 224.32
[32m[20230203 20:54:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.41
[32m[20230203 20:54:33 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 256.41
[32m[20230203 20:54:33 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 256.41
[32m[20230203 20:54:33 @agent_ppo2.py:151][0m Total time:       3.96 min
[32m[20230203 20:54:33 @agent_ppo2.py:153][0m 294912 total steps have happened
[32m[20230203 20:54:33 @agent_ppo2.py:129][0m #------------------------ Iteration 144 --------------------------#
[32m[20230203 20:54:34 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:54:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:34 @agent_ppo2.py:193][0m |           0.0020 |          12.0513 |           2.9998 |
[32m[20230203 20:54:34 @agent_ppo2.py:193][0m |          -0.0040 |           7.3449 |           2.9976 |
[32m[20230203 20:54:34 @agent_ppo2.py:193][0m |          -0.0084 |           5.6272 |           2.9958 |
[32m[20230203 20:54:34 @agent_ppo2.py:193][0m |          -0.0098 |           5.1750 |           2.9939 |
[32m[20230203 20:54:34 @agent_ppo2.py:193][0m |          -0.0093 |           4.8496 |           2.9925 |
[32m[20230203 20:54:34 @agent_ppo2.py:193][0m |          -0.0119 |           4.5743 |           2.9903 |
[32m[20230203 20:54:34 @agent_ppo2.py:193][0m |          -0.0122 |           4.4534 |           2.9906 |
[32m[20230203 20:54:34 @agent_ppo2.py:193][0m |          -0.0132 |           4.2508 |           2.9903 |
[32m[20230203 20:54:34 @agent_ppo2.py:193][0m |          -0.0129 |           4.2038 |           2.9885 |
[32m[20230203 20:54:34 @agent_ppo2.py:193][0m |          -0.0142 |           4.1016 |           2.9881 |
[32m[20230203 20:54:34 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:54:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 116.43
[32m[20230203 20:54:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 237.48
[32m[20230203 20:54:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.53
[32m[20230203 20:54:35 @agent_ppo2.py:151][0m Total time:       3.99 min
[32m[20230203 20:54:35 @agent_ppo2.py:153][0m 296960 total steps have happened
[32m[20230203 20:54:35 @agent_ppo2.py:129][0m #------------------------ Iteration 145 --------------------------#
[32m[20230203 20:54:35 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:54:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:35 @agent_ppo2.py:193][0m |          -0.0018 |          27.8195 |           2.8414 |
[32m[20230203 20:54:35 @agent_ppo2.py:193][0m |          -0.0063 |          13.8756 |           2.8394 |
[32m[20230203 20:54:35 @agent_ppo2.py:193][0m |          -0.0098 |          11.5677 |           2.8384 |
[32m[20230203 20:54:35 @agent_ppo2.py:193][0m |          -0.0075 |          10.6766 |           2.8348 |
[32m[20230203 20:54:35 @agent_ppo2.py:193][0m |          -0.0127 |          10.2176 |           2.8360 |
[32m[20230203 20:54:35 @agent_ppo2.py:193][0m |          -0.0110 |           9.4709 |           2.8321 |
[32m[20230203 20:54:36 @agent_ppo2.py:193][0m |          -0.0121 |           9.2205 |           2.8316 |
[32m[20230203 20:54:36 @agent_ppo2.py:193][0m |          -0.0178 |           8.6553 |           2.8284 |
[32m[20230203 20:54:36 @agent_ppo2.py:193][0m |          -0.0170 |           8.3890 |           2.8280 |
[32m[20230203 20:54:36 @agent_ppo2.py:193][0m |          -0.0157 |           8.0286 |           2.8232 |
[32m[20230203 20:54:36 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:54:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 7.00
[32m[20230203 20:54:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 68.39
[32m[20230203 20:54:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.91
[32m[20230203 20:54:36 @agent_ppo2.py:151][0m Total time:       4.01 min
[32m[20230203 20:54:36 @agent_ppo2.py:153][0m 299008 total steps have happened
[32m[20230203 20:54:36 @agent_ppo2.py:129][0m #------------------------ Iteration 146 --------------------------#
[32m[20230203 20:54:37 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:54:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:37 @agent_ppo2.py:193][0m |          -0.0119 |          12.5110 |           2.7884 |
[32m[20230203 20:54:37 @agent_ppo2.py:193][0m |          -0.0113 |          10.4097 |           2.7884 |
[32m[20230203 20:54:37 @agent_ppo2.py:193][0m |          -0.0123 |           9.8819 |           2.7875 |
[32m[20230203 20:54:37 @agent_ppo2.py:193][0m |           0.0053 |           9.8386 |           2.7872 |
[32m[20230203 20:54:37 @agent_ppo2.py:193][0m |          -0.0126 |           9.2392 |           2.7832 |
[32m[20230203 20:54:37 @agent_ppo2.py:193][0m |          -0.0488 |           8.8815 |           2.7873 |
[32m[20230203 20:54:37 @agent_ppo2.py:193][0m |          -0.0352 |           9.5320 |           2.7881 |
[32m[20230203 20:54:37 @agent_ppo2.py:193][0m |          -0.0204 |           9.8485 |           2.7905 |
[32m[20230203 20:54:37 @agent_ppo2.py:193][0m |          -0.0123 |           8.5730 |           2.7917 |
[32m[20230203 20:54:37 @agent_ppo2.py:193][0m |          -0.0114 |           8.4121 |           2.7930 |
[32m[20230203 20:54:37 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:54:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 135.23
[32m[20230203 20:54:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 227.60
[32m[20230203 20:54:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.24
[32m[20230203 20:54:37 @agent_ppo2.py:151][0m Total time:       4.04 min
[32m[20230203 20:54:37 @agent_ppo2.py:153][0m 301056 total steps have happened
[32m[20230203 20:54:37 @agent_ppo2.py:129][0m #------------------------ Iteration 147 --------------------------#
[32m[20230203 20:54:38 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:54:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:38 @agent_ppo2.py:193][0m |          -0.0019 |          14.9614 |           2.8756 |
[32m[20230203 20:54:38 @agent_ppo2.py:193][0m |          -0.0082 |          11.3264 |           2.8743 |
[32m[20230203 20:54:38 @agent_ppo2.py:193][0m |          -0.0102 |          10.2407 |           2.8736 |
[32m[20230203 20:54:38 @agent_ppo2.py:193][0m |          -0.0118 |           9.3724 |           2.8749 |
[32m[20230203 20:54:38 @agent_ppo2.py:193][0m |          -0.0343 |           8.7885 |           2.8732 |
[32m[20230203 20:54:38 @agent_ppo2.py:193][0m |           0.0085 |           8.8338 |           2.8789 |
[32m[20230203 20:54:38 @agent_ppo2.py:193][0m |          -0.0142 |           7.9299 |           2.8743 |
[32m[20230203 20:54:38 @agent_ppo2.py:193][0m |          -0.0059 |           7.6424 |           2.8757 |
[32m[20230203 20:54:38 @agent_ppo2.py:193][0m |          -0.0079 |           7.5183 |           2.8776 |
[32m[20230203 20:54:38 @agent_ppo2.py:193][0m |          -0.0167 |           7.2509 |           2.8761 |
[32m[20230203 20:54:38 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:54:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 115.98
[32m[20230203 20:54:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 218.23
[32m[20230203 20:54:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 243.99
[32m[20230203 20:54:39 @agent_ppo2.py:151][0m Total time:       4.06 min
[32m[20230203 20:54:39 @agent_ppo2.py:153][0m 303104 total steps have happened
[32m[20230203 20:54:39 @agent_ppo2.py:129][0m #------------------------ Iteration 148 --------------------------#
[32m[20230203 20:54:39 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:54:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:40 @agent_ppo2.py:193][0m |          -0.0001 |           4.8292 |           2.9147 |
[32m[20230203 20:54:40 @agent_ppo2.py:193][0m |          -0.0043 |           4.4361 |           2.9100 |
[32m[20230203 20:54:40 @agent_ppo2.py:193][0m |          -0.0052 |           4.3446 |           2.9086 |
[32m[20230203 20:54:40 @agent_ppo2.py:193][0m |          -0.0064 |           4.2868 |           2.9119 |
[32m[20230203 20:54:40 @agent_ppo2.py:193][0m |          -0.0071 |           4.2606 |           2.9108 |
[32m[20230203 20:54:40 @agent_ppo2.py:193][0m |          -0.0075 |           4.2249 |           2.9089 |
[32m[20230203 20:54:40 @agent_ppo2.py:193][0m |          -0.0082 |           4.1982 |           2.9089 |
[32m[20230203 20:54:40 @agent_ppo2.py:193][0m |          -0.0082 |           4.1823 |           2.9068 |
[32m[20230203 20:54:40 @agent_ppo2.py:193][0m |          -0.0082 |           4.1780 |           2.9087 |
[32m[20230203 20:54:40 @agent_ppo2.py:193][0m |          -0.0087 |           4.1529 |           2.9082 |
[32m[20230203 20:54:40 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 223.08
[32m[20230203 20:54:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 229.62
[32m[20230203 20:54:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 242.98
[32m[20230203 20:54:41 @agent_ppo2.py:151][0m Total time:       4.09 min
[32m[20230203 20:54:41 @agent_ppo2.py:153][0m 305152 total steps have happened
[32m[20230203 20:54:41 @agent_ppo2.py:129][0m #------------------------ Iteration 149 --------------------------#
[32m[20230203 20:54:41 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:54:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:41 @agent_ppo2.py:193][0m |           0.0011 |           2.7124 |           2.9682 |
[32m[20230203 20:54:41 @agent_ppo2.py:193][0m |          -0.0026 |           2.6326 |           2.9695 |
[32m[20230203 20:54:41 @agent_ppo2.py:193][0m |          -0.0037 |           2.5958 |           2.9682 |
[32m[20230203 20:54:41 @agent_ppo2.py:193][0m |          -0.0046 |           2.5668 |           2.9670 |
[32m[20230203 20:54:41 @agent_ppo2.py:193][0m |          -0.0056 |           2.5458 |           2.9678 |
[32m[20230203 20:54:41 @agent_ppo2.py:193][0m |          -0.0065 |           2.5269 |           2.9670 |
[32m[20230203 20:54:41 @agent_ppo2.py:193][0m |          -0.0074 |           2.5156 |           2.9672 |
[32m[20230203 20:54:42 @agent_ppo2.py:193][0m |          -0.0074 |           2.5041 |           2.9656 |
[32m[20230203 20:54:42 @agent_ppo2.py:193][0m |          -0.0085 |           2.4992 |           2.9662 |
[32m[20230203 20:54:42 @agent_ppo2.py:193][0m |          -0.0085 |           2.4881 |           2.9668 |
[32m[20230203 20:54:42 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 224.84
[32m[20230203 20:54:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 230.38
[32m[20230203 20:54:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 247.19
[32m[20230203 20:54:42 @agent_ppo2.py:151][0m Total time:       4.11 min
[32m[20230203 20:54:42 @agent_ppo2.py:153][0m 307200 total steps have happened
[32m[20230203 20:54:42 @agent_ppo2.py:129][0m #------------------------ Iteration 150 --------------------------#
[32m[20230203 20:54:43 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:54:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:43 @agent_ppo2.py:193][0m |          -0.0018 |           4.4603 |           2.9593 |
[32m[20230203 20:54:43 @agent_ppo2.py:193][0m |          -0.0059 |           3.7287 |           2.9541 |
[32m[20230203 20:54:43 @agent_ppo2.py:193][0m |          -0.0076 |           3.5320 |           2.9538 |
[32m[20230203 20:54:43 @agent_ppo2.py:193][0m |          -0.0083 |           3.4542 |           2.9502 |
[32m[20230203 20:54:43 @agent_ppo2.py:193][0m |          -0.0092 |           3.3647 |           2.9507 |
[32m[20230203 20:54:43 @agent_ppo2.py:193][0m |          -0.0104 |           3.3149 |           2.9511 |
[32m[20230203 20:54:43 @agent_ppo2.py:193][0m |          -0.0094 |           3.2707 |           2.9478 |
[32m[20230203 20:54:43 @agent_ppo2.py:193][0m |          -0.0110 |           3.2226 |           2.9489 |
[32m[20230203 20:54:43 @agent_ppo2.py:193][0m |          -0.0111 |           3.1775 |           2.9469 |
[32m[20230203 20:54:43 @agent_ppo2.py:193][0m |          -0.0119 |           3.1461 |           2.9488 |
[32m[20230203 20:54:43 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 222.38
[32m[20230203 20:54:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 225.02
[32m[20230203 20:54:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 241.80
[32m[20230203 20:54:44 @agent_ppo2.py:151][0m Total time:       4.14 min
[32m[20230203 20:54:44 @agent_ppo2.py:153][0m 309248 total steps have happened
[32m[20230203 20:54:44 @agent_ppo2.py:129][0m #------------------------ Iteration 151 --------------------------#
[32m[20230203 20:54:44 @agent_ppo2.py:135][0m Sampling time: 0.50 s by 1 slaves
[32m[20230203 20:54:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:44 @agent_ppo2.py:193][0m |          -0.0011 |          26.3574 |           2.8280 |
[32m[20230203 20:54:44 @agent_ppo2.py:193][0m |          -0.0101 |          12.1297 |           2.8253 |
[32m[20230203 20:54:44 @agent_ppo2.py:193][0m |          -0.0122 |           8.9230 |           2.8268 |
[32m[20230203 20:54:44 @agent_ppo2.py:193][0m |          -0.0144 |           7.8409 |           2.8272 |
[32m[20230203 20:54:45 @agent_ppo2.py:193][0m |          -0.0154 |           6.8286 |           2.8290 |
[32m[20230203 20:54:45 @agent_ppo2.py:193][0m |          -0.0163 |           6.4098 |           2.8264 |
[32m[20230203 20:54:45 @agent_ppo2.py:193][0m |          -0.0172 |           5.5431 |           2.8302 |
[32m[20230203 20:54:45 @agent_ppo2.py:193][0m |          -0.0157 |           5.0433 |           2.8299 |
[32m[20230203 20:54:45 @agent_ppo2.py:193][0m |          -0.0172 |           4.6102 |           2.8303 |
[32m[20230203 20:54:45 @agent_ppo2.py:193][0m |          -0.0180 |           4.3852 |           2.8282 |
[32m[20230203 20:54:45 @agent_ppo2.py:138][0m Policy update time: 0.57 s
[32m[20230203 20:54:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 156.37
[32m[20230203 20:54:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 208.36
[32m[20230203 20:54:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 242.15
[32m[20230203 20:54:45 @agent_ppo2.py:151][0m Total time:       4.17 min
[32m[20230203 20:54:45 @agent_ppo2.py:153][0m 311296 total steps have happened
[32m[20230203 20:54:45 @agent_ppo2.py:129][0m #------------------------ Iteration 152 --------------------------#
[32m[20230203 20:54:46 @agent_ppo2.py:135][0m Sampling time: 0.53 s by 1 slaves
[32m[20230203 20:54:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:46 @agent_ppo2.py:193][0m |           0.0012 |           3.5476 |           2.8891 |
[32m[20230203 20:54:46 @agent_ppo2.py:193][0m |          -0.0059 |           2.8064 |           2.8928 |
[32m[20230203 20:54:46 @agent_ppo2.py:193][0m |          -0.0097 |           2.5398 |           2.8912 |
[32m[20230203 20:54:46 @agent_ppo2.py:193][0m |          -0.0072 |           2.3515 |           2.8910 |
[32m[20230203 20:54:46 @agent_ppo2.py:193][0m |          -0.0077 |           2.1970 |           2.8907 |
[32m[20230203 20:54:46 @agent_ppo2.py:193][0m |          -0.0088 |           2.0553 |           2.8926 |
[32m[20230203 20:54:46 @agent_ppo2.py:193][0m |          -0.0156 |           1.9919 |           2.8889 |
[32m[20230203 20:54:46 @agent_ppo2.py:193][0m |          -0.0144 |           1.9169 |           2.8899 |
[32m[20230203 20:54:46 @agent_ppo2.py:193][0m |          -0.0053 |           1.9508 |           2.8926 |
[32m[20230203 20:54:46 @agent_ppo2.py:193][0m |          -0.0134 |           1.8177 |           2.8911 |
[32m[20230203 20:54:46 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 224.34
[32m[20230203 20:54:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 233.79
[32m[20230203 20:54:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 244.09
[32m[20230203 20:54:47 @agent_ppo2.py:151][0m Total time:       4.19 min
[32m[20230203 20:54:47 @agent_ppo2.py:153][0m 313344 total steps have happened
[32m[20230203 20:54:47 @agent_ppo2.py:129][0m #------------------------ Iteration 153 --------------------------#
[32m[20230203 20:54:47 @agent_ppo2.py:135][0m Sampling time: 0.51 s by 1 slaves
[32m[20230203 20:54:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:47 @agent_ppo2.py:193][0m |          -0.0013 |           4.9039 |           2.9852 |
[32m[20230203 20:54:48 @agent_ppo2.py:193][0m |          -0.0057 |           4.5542 |           2.9818 |
[32m[20230203 20:54:48 @agent_ppo2.py:193][0m |          -0.0077 |           4.4614 |           2.9817 |
[32m[20230203 20:54:48 @agent_ppo2.py:193][0m |          -0.0085 |           4.4208 |           2.9797 |
[32m[20230203 20:54:48 @agent_ppo2.py:193][0m |          -0.0101 |           4.3789 |           2.9801 |
[32m[20230203 20:54:48 @agent_ppo2.py:193][0m |          -0.0110 |           4.3680 |           2.9805 |
[32m[20230203 20:54:48 @agent_ppo2.py:193][0m |          -0.0118 |           4.3241 |           2.9796 |
[32m[20230203 20:54:48 @agent_ppo2.py:193][0m |          -0.0118 |           4.3063 |           2.9785 |
[32m[20230203 20:54:48 @agent_ppo2.py:193][0m |          -0.0130 |           4.2866 |           2.9793 |
[32m[20230203 20:54:48 @agent_ppo2.py:193][0m |          -0.0130 |           4.2741 |           2.9825 |
[32m[20230203 20:54:48 @agent_ppo2.py:138][0m Policy update time: 0.58 s
[32m[20230203 20:54:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 235.79
[32m[20230203 20:54:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 236.27
[32m[20230203 20:54:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 235.86
[32m[20230203 20:54:48 @agent_ppo2.py:151][0m Total time:       4.22 min
[32m[20230203 20:54:48 @agent_ppo2.py:153][0m 315392 total steps have happened
[32m[20230203 20:54:48 @agent_ppo2.py:129][0m #------------------------ Iteration 154 --------------------------#
[32m[20230203 20:54:49 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:54:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:49 @agent_ppo2.py:193][0m |          -0.0012 |           3.0846 |           2.9776 |
[32m[20230203 20:54:49 @agent_ppo2.py:193][0m |          -0.0067 |           2.9921 |           2.9750 |
[32m[20230203 20:54:49 @agent_ppo2.py:193][0m |          -0.0082 |           2.9455 |           2.9740 |
[32m[20230203 20:54:49 @agent_ppo2.py:193][0m |          -0.0095 |           2.9208 |           2.9754 |
[32m[20230203 20:54:49 @agent_ppo2.py:193][0m |          -0.0101 |           2.9003 |           2.9733 |
[32m[20230203 20:54:49 @agent_ppo2.py:193][0m |          -0.0109 |           2.8814 |           2.9751 |
[32m[20230203 20:54:49 @agent_ppo2.py:193][0m |          -0.0111 |           2.8633 |           2.9733 |
[32m[20230203 20:54:49 @agent_ppo2.py:193][0m |          -0.0116 |           2.8578 |           2.9725 |
[32m[20230203 20:54:50 @agent_ppo2.py:193][0m |          -0.0122 |           2.8418 |           2.9738 |
[32m[20230203 20:54:50 @agent_ppo2.py:193][0m |          -0.0124 |           2.8313 |           2.9733 |
[32m[20230203 20:54:50 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:54:50 @agent_ppo2.py:146][0m Average TRAINING episode reward: 222.50
[32m[20230203 20:54:50 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 227.02
[32m[20230203 20:54:50 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 245.82
[32m[20230203 20:54:50 @agent_ppo2.py:151][0m Total time:       4.25 min
[32m[20230203 20:54:50 @agent_ppo2.py:153][0m 317440 total steps have happened
[32m[20230203 20:54:50 @agent_ppo2.py:129][0m #------------------------ Iteration 155 --------------------------#
[32m[20230203 20:54:51 @agent_ppo2.py:135][0m Sampling time: 0.51 s by 1 slaves
[32m[20230203 20:54:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:51 @agent_ppo2.py:193][0m |          -0.0106 |           3.9296 |           2.9358 |
[32m[20230203 20:54:51 @agent_ppo2.py:193][0m |           0.0009 |           3.4284 |           2.9315 |
[32m[20230203 20:54:51 @agent_ppo2.py:193][0m |          -0.0058 |           3.2941 |           2.9322 |
[32m[20230203 20:54:51 @agent_ppo2.py:193][0m |          -0.0235 |           3.2737 |           2.9302 |
[32m[20230203 20:54:51 @agent_ppo2.py:193][0m |          -0.0204 |           3.2694 |           2.9329 |
[32m[20230203 20:54:51 @agent_ppo2.py:193][0m |          -0.0127 |           3.2081 |           2.9365 |
[32m[20230203 20:54:51 @agent_ppo2.py:193][0m |          -0.0199 |           3.1814 |           2.9317 |
[32m[20230203 20:54:51 @agent_ppo2.py:193][0m |          -0.0126 |           3.1576 |           2.9302 |
[32m[20230203 20:54:51 @agent_ppo2.py:193][0m |          -0.0114 |           3.1150 |           2.9334 |
[32m[20230203 20:54:51 @agent_ppo2.py:193][0m |          -0.0105 |           3.1141 |           2.9318 |
[32m[20230203 20:54:51 @agent_ppo2.py:138][0m Policy update time: 0.59 s
[32m[20230203 20:54:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 229.09
[32m[20230203 20:54:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 237.91
[32m[20230203 20:54:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 243.11
[32m[20230203 20:54:52 @agent_ppo2.py:151][0m Total time:       4.27 min
[32m[20230203 20:54:52 @agent_ppo2.py:153][0m 319488 total steps have happened
[32m[20230203 20:54:52 @agent_ppo2.py:129][0m #------------------------ Iteration 156 --------------------------#
[32m[20230203 20:54:52 @agent_ppo2.py:135][0m Sampling time: 0.53 s by 1 slaves
[32m[20230203 20:54:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:52 @agent_ppo2.py:193][0m |           0.0009 |           4.2973 |           2.9184 |
[32m[20230203 20:54:52 @agent_ppo2.py:193][0m |          -0.0013 |           4.2445 |           2.9221 |
[32m[20230203 20:54:52 @agent_ppo2.py:193][0m |          -0.0067 |           4.1231 |           2.9175 |
[32m[20230203 20:54:52 @agent_ppo2.py:193][0m |          -0.0098 |           4.0096 |           2.9154 |
[32m[20230203 20:54:52 @agent_ppo2.py:193][0m |          -0.0088 |           3.9722 |           2.9142 |
[32m[20230203 20:54:53 @agent_ppo2.py:193][0m |          -0.0111 |           3.9136 |           2.9135 |
[32m[20230203 20:54:53 @agent_ppo2.py:193][0m |          -0.0116 |           3.8847 |           2.9116 |
[32m[20230203 20:54:53 @agent_ppo2.py:193][0m |          -0.0113 |           3.8227 |           2.9101 |
[32m[20230203 20:54:53 @agent_ppo2.py:193][0m |          -0.0120 |           3.7988 |           2.9121 |
[32m[20230203 20:54:53 @agent_ppo2.py:193][0m |          -0.0133 |           3.7793 |           2.9111 |
[32m[20230203 20:54:53 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:54:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 227.01
[32m[20230203 20:54:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 230.63
[32m[20230203 20:54:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 246.27
[32m[20230203 20:54:53 @agent_ppo2.py:151][0m Total time:       4.30 min
[32m[20230203 20:54:53 @agent_ppo2.py:153][0m 321536 total steps have happened
[32m[20230203 20:54:53 @agent_ppo2.py:129][0m #------------------------ Iteration 157 --------------------------#
[32m[20230203 20:54:54 @agent_ppo2.py:135][0m Sampling time: 0.53 s by 1 slaves
[32m[20230203 20:54:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:54 @agent_ppo2.py:193][0m |          -0.0021 |           5.3982 |           2.9272 |
[32m[20230203 20:54:54 @agent_ppo2.py:193][0m |          -0.0065 |           4.9492 |           2.9284 |
[32m[20230203 20:54:54 @agent_ppo2.py:193][0m |          -0.0060 |           4.8138 |           2.9253 |
[32m[20230203 20:54:54 @agent_ppo2.py:193][0m |          -0.0097 |           4.7287 |           2.9234 |
[32m[20230203 20:54:54 @agent_ppo2.py:193][0m |          -0.0093 |           4.6596 |           2.9259 |
[32m[20230203 20:54:54 @agent_ppo2.py:193][0m |          -0.0120 |           4.5998 |           2.9235 |
[32m[20230203 20:54:54 @agent_ppo2.py:193][0m |          -0.0100 |           4.6410 |           2.9253 |
[32m[20230203 20:54:54 @agent_ppo2.py:193][0m |          -0.0120 |           4.5253 |           2.9215 |
[32m[20230203 20:54:54 @agent_ppo2.py:193][0m |          -0.0118 |           4.5063 |           2.9224 |
[32m[20230203 20:54:54 @agent_ppo2.py:193][0m |          -0.0130 |           4.4472 |           2.9249 |
[32m[20230203 20:54:54 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:54:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 219.82
[32m[20230203 20:54:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 233.29
[32m[20230203 20:54:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 249.10
[32m[20230203 20:54:55 @agent_ppo2.py:151][0m Total time:       4.33 min
[32m[20230203 20:54:55 @agent_ppo2.py:153][0m 323584 total steps have happened
[32m[20230203 20:54:55 @agent_ppo2.py:129][0m #------------------------ Iteration 158 --------------------------#
[32m[20230203 20:54:55 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:54:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:55 @agent_ppo2.py:193][0m |          -0.0014 |          18.1540 |           2.9205 |
[32m[20230203 20:54:56 @agent_ppo2.py:193][0m |          -0.0039 |          12.5576 |           2.9169 |
[32m[20230203 20:54:56 @agent_ppo2.py:193][0m |          -0.0048 |          11.3500 |           2.9140 |
[32m[20230203 20:54:56 @agent_ppo2.py:193][0m |          -0.0048 |          10.8046 |           2.9183 |
[32m[20230203 20:54:56 @agent_ppo2.py:193][0m |          -0.0055 |          10.2756 |           2.9157 |
[32m[20230203 20:54:56 @agent_ppo2.py:193][0m |          -0.0109 |           9.8516 |           2.9162 |
[32m[20230203 20:54:56 @agent_ppo2.py:193][0m |          -0.0116 |           9.3795 |           2.9160 |
[32m[20230203 20:54:56 @agent_ppo2.py:193][0m |          -0.0118 |           9.1085 |           2.9127 |
[32m[20230203 20:54:56 @agent_ppo2.py:193][0m |          -0.0086 |           8.8377 |           2.9147 |
[32m[20230203 20:54:56 @agent_ppo2.py:193][0m |          -0.0103 |           8.3115 |           2.9157 |
[32m[20230203 20:54:56 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:54:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 103.17
[32m[20230203 20:54:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 233.18
[32m[20230203 20:54:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 246.88
[32m[20230203 20:54:56 @agent_ppo2.py:151][0m Total time:       4.35 min
[32m[20230203 20:54:56 @agent_ppo2.py:153][0m 325632 total steps have happened
[32m[20230203 20:54:56 @agent_ppo2.py:129][0m #------------------------ Iteration 159 --------------------------#
[32m[20230203 20:54:57 @agent_ppo2.py:135][0m Sampling time: 0.52 s by 1 slaves
[32m[20230203 20:54:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:57 @agent_ppo2.py:193][0m |          -0.0020 |           6.0651 |           2.9600 |
[32m[20230203 20:54:57 @agent_ppo2.py:193][0m |          -0.0087 |           5.3918 |           2.9573 |
[32m[20230203 20:54:57 @agent_ppo2.py:193][0m |          -0.0066 |           5.1987 |           2.9564 |
[32m[20230203 20:54:57 @agent_ppo2.py:193][0m |          -0.0071 |           5.1124 |           2.9552 |
[32m[20230203 20:54:57 @agent_ppo2.py:193][0m |          -0.0131 |           5.0405 |           2.9567 |
[32m[20230203 20:54:57 @agent_ppo2.py:193][0m |          -0.0104 |           4.9860 |           2.9534 |
[32m[20230203 20:54:57 @agent_ppo2.py:193][0m |          -0.0078 |           4.9420 |           2.9523 |
[32m[20230203 20:54:57 @agent_ppo2.py:193][0m |          -0.0104 |           4.9014 |           2.9528 |
[32m[20230203 20:54:58 @agent_ppo2.py:193][0m |          -0.0123 |           4.8739 |           2.9511 |
[32m[20230203 20:54:58 @agent_ppo2.py:193][0m |          -0.0142 |           4.8521 |           2.9519 |
[32m[20230203 20:54:58 @agent_ppo2.py:138][0m Policy update time: 0.59 s
[32m[20230203 20:54:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 234.42
[32m[20230203 20:54:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 235.59
[32m[20230203 20:54:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 246.39
[32m[20230203 20:54:58 @agent_ppo2.py:151][0m Total time:       4.38 min
[32m[20230203 20:54:58 @agent_ppo2.py:153][0m 327680 total steps have happened
[32m[20230203 20:54:58 @agent_ppo2.py:129][0m #------------------------ Iteration 160 --------------------------#
[32m[20230203 20:54:59 @agent_ppo2.py:135][0m Sampling time: 0.52 s by 1 slaves
[32m[20230203 20:54:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:54:59 @agent_ppo2.py:193][0m |           0.0006 |           5.6118 |           2.9993 |
[32m[20230203 20:54:59 @agent_ppo2.py:193][0m |          -0.0037 |           5.3674 |           2.9958 |
[32m[20230203 20:54:59 @agent_ppo2.py:193][0m |          -0.0068 |           5.2520 |           2.9938 |
[32m[20230203 20:54:59 @agent_ppo2.py:193][0m |          -0.0076 |           5.1703 |           2.9914 |
[32m[20230203 20:54:59 @agent_ppo2.py:193][0m |          -0.0090 |           5.1182 |           2.9903 |
[32m[20230203 20:54:59 @agent_ppo2.py:193][0m |          -0.0099 |           5.0585 |           2.9902 |
[32m[20230203 20:54:59 @agent_ppo2.py:193][0m |          -0.0102 |           5.0469 |           2.9917 |
[32m[20230203 20:54:59 @agent_ppo2.py:193][0m |          -0.0109 |           5.0215 |           2.9879 |
[32m[20230203 20:54:59 @agent_ppo2.py:193][0m |          -0.0103 |           4.9996 |           2.9876 |
[32m[20230203 20:54:59 @agent_ppo2.py:193][0m |          -0.0115 |           4.9636 |           2.9892 |
[32m[20230203 20:54:59 @agent_ppo2.py:138][0m Policy update time: 0.59 s
[32m[20230203 20:55:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 231.20
[32m[20230203 20:55:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 234.57
[32m[20230203 20:55:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 240.94
[32m[20230203 20:55:00 @agent_ppo2.py:151][0m Total time:       4.41 min
[32m[20230203 20:55:00 @agent_ppo2.py:153][0m 329728 total steps have happened
[32m[20230203 20:55:00 @agent_ppo2.py:129][0m #------------------------ Iteration 161 --------------------------#
[32m[20230203 20:55:00 @agent_ppo2.py:135][0m Sampling time: 0.51 s by 1 slaves
[32m[20230203 20:55:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:00 @agent_ppo2.py:193][0m |          -0.0004 |          17.5040 |           2.9478 |
[32m[20230203 20:55:00 @agent_ppo2.py:193][0m |          -0.0042 |           7.7444 |           2.9478 |
[32m[20230203 20:55:00 @agent_ppo2.py:193][0m |          -0.0058 |           6.0690 |           2.9488 |
[32m[20230203 20:55:00 @agent_ppo2.py:193][0m |          -0.0087 |           5.7763 |           2.9458 |
[32m[20230203 20:55:00 @agent_ppo2.py:193][0m |          -0.0091 |           5.4732 |           2.9445 |
[32m[20230203 20:55:00 @agent_ppo2.py:193][0m |          -0.0119 |           5.3751 |           2.9439 |
[32m[20230203 20:55:01 @agent_ppo2.py:193][0m |          -0.0092 |           5.3563 |           2.9452 |
[32m[20230203 20:55:01 @agent_ppo2.py:193][0m |          -0.0109 |           5.0785 |           2.9444 |
[32m[20230203 20:55:01 @agent_ppo2.py:193][0m |          -0.0113 |           5.1959 |           2.9438 |
[32m[20230203 20:55:01 @agent_ppo2.py:193][0m |          -0.0134 |           4.9259 |           2.9445 |
[32m[20230203 20:55:01 @agent_ppo2.py:138][0m Policy update time: 0.58 s
[32m[20230203 20:55:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 122.33
[32m[20230203 20:55:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 239.29
[32m[20230203 20:55:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 245.64
[32m[20230203 20:55:01 @agent_ppo2.py:151][0m Total time:       4.43 min
[32m[20230203 20:55:01 @agent_ppo2.py:153][0m 331776 total steps have happened
[32m[20230203 20:55:01 @agent_ppo2.py:129][0m #------------------------ Iteration 162 --------------------------#
[32m[20230203 20:55:02 @agent_ppo2.py:135][0m Sampling time: 0.52 s by 1 slaves
[32m[20230203 20:55:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:02 @agent_ppo2.py:193][0m |          -0.0005 |          25.8147 |           2.9716 |
[32m[20230203 20:55:02 @agent_ppo2.py:193][0m |          -0.0056 |          21.8913 |           2.9643 |
[32m[20230203 20:55:02 @agent_ppo2.py:193][0m |          -0.0093 |          19.0492 |           2.9609 |
[32m[20230203 20:55:02 @agent_ppo2.py:193][0m |          -0.0074 |          17.5007 |           2.9591 |
[32m[20230203 20:55:02 @agent_ppo2.py:193][0m |          -0.0101 |          16.3516 |           2.9560 |
[32m[20230203 20:55:02 @agent_ppo2.py:193][0m |          -0.0139 |          15.4103 |           2.9536 |
[32m[20230203 20:55:02 @agent_ppo2.py:193][0m |          -0.0132 |          14.5370 |           2.9565 |
[32m[20230203 20:55:02 @agent_ppo2.py:193][0m |          -0.0128 |          13.7246 |           2.9549 |
[32m[20230203 20:55:02 @agent_ppo2.py:193][0m |          -0.0140 |          12.8827 |           2.9551 |
[32m[20230203 20:55:02 @agent_ppo2.py:193][0m |          -0.0147 |          12.2802 |           2.9555 |
[32m[20230203 20:55:02 @agent_ppo2.py:138][0m Policy update time: 0.59 s
[32m[20230203 20:55:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 160.17
[32m[20230203 20:55:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 229.57
[32m[20230203 20:55:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 243.10
[32m[20230203 20:55:03 @agent_ppo2.py:151][0m Total time:       4.46 min
[32m[20230203 20:55:03 @agent_ppo2.py:153][0m 333824 total steps have happened
[32m[20230203 20:55:03 @agent_ppo2.py:129][0m #------------------------ Iteration 163 --------------------------#
[32m[20230203 20:55:03 @agent_ppo2.py:135][0m Sampling time: 0.52 s by 1 slaves
[32m[20230203 20:55:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:03 @agent_ppo2.py:193][0m |           0.0000 |           6.7293 |           2.9498 |
[32m[20230203 20:55:03 @agent_ppo2.py:193][0m |          -0.0059 |           6.0605 |           2.9496 |
[32m[20230203 20:55:03 @agent_ppo2.py:193][0m |          -0.0084 |           5.8667 |           2.9447 |
[32m[20230203 20:55:03 @agent_ppo2.py:193][0m |          -0.0084 |           5.7548 |           2.9445 |
[32m[20230203 20:55:04 @agent_ppo2.py:193][0m |          -0.0092 |           5.6335 |           2.9419 |
[32m[20230203 20:55:04 @agent_ppo2.py:193][0m |          -0.0123 |           5.5869 |           2.9452 |
[32m[20230203 20:55:04 @agent_ppo2.py:193][0m |          -0.0086 |           5.5966 |           2.9431 |
[32m[20230203 20:55:04 @agent_ppo2.py:193][0m |          -0.0092 |           5.8222 |           2.9425 |
[32m[20230203 20:55:04 @agent_ppo2.py:193][0m |          -0.0137 |           5.4250 |           2.9433 |
[32m[20230203 20:55:04 @agent_ppo2.py:193][0m |          -0.0158 |           5.4242 |           2.9430 |
[32m[20230203 20:55:04 @agent_ppo2.py:138][0m Policy update time: 0.59 s
[32m[20230203 20:55:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 232.03
[32m[20230203 20:55:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 237.06
[32m[20230203 20:55:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 246.19
[32m[20230203 20:55:04 @agent_ppo2.py:151][0m Total time:       4.48 min
[32m[20230203 20:55:04 @agent_ppo2.py:153][0m 335872 total steps have happened
[32m[20230203 20:55:04 @agent_ppo2.py:129][0m #------------------------ Iteration 164 --------------------------#
[32m[20230203 20:55:05 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:55:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:05 @agent_ppo2.py:193][0m |          -0.0035 |          17.6052 |           2.9422 |
[32m[20230203 20:55:05 @agent_ppo2.py:193][0m |           0.0166 |          10.9924 |           2.9438 |
[32m[20230203 20:55:05 @agent_ppo2.py:193][0m |           0.0171 |           7.9825 |           2.9346 |
[32m[20230203 20:55:05 @agent_ppo2.py:193][0m |          -0.0137 |           8.1598 |           2.9349 |
[32m[20230203 20:55:05 @agent_ppo2.py:193][0m |           0.0106 |           6.9310 |           2.9310 |
[32m[20230203 20:55:05 @agent_ppo2.py:193][0m |           0.0107 |           6.6711 |           2.9260 |
[32m[20230203 20:55:05 @agent_ppo2.py:193][0m |          -0.0188 |           6.3376 |           2.9286 |
[32m[20230203 20:55:05 @agent_ppo2.py:193][0m |          -0.0214 |           6.1192 |           2.9267 |
[32m[20230203 20:55:05 @agent_ppo2.py:193][0m |          -0.0254 |           6.0183 |           2.9287 |
[32m[20230203 20:55:05 @agent_ppo2.py:193][0m |          -0.0184 |           5.8544 |           2.9211 |
[32m[20230203 20:55:05 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 20:55:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 149.11
[32m[20230203 20:55:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 239.52
[32m[20230203 20:55:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 223.50
[32m[20230203 20:55:06 @agent_ppo2.py:151][0m Total time:       4.51 min
[32m[20230203 20:55:06 @agent_ppo2.py:153][0m 337920 total steps have happened
[32m[20230203 20:55:06 @agent_ppo2.py:129][0m #------------------------ Iteration 165 --------------------------#
[32m[20230203 20:55:06 @agent_ppo2.py:135][0m Sampling time: 0.52 s by 1 slaves
[32m[20230203 20:55:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:06 @agent_ppo2.py:193][0m |          -0.0121 |           6.4216 |           2.9664 |
[32m[20230203 20:55:06 @agent_ppo2.py:193][0m |          -0.0106 |           5.9761 |           2.9620 |
[32m[20230203 20:55:06 @agent_ppo2.py:193][0m |          -0.0111 |           5.8007 |           2.9611 |
[32m[20230203 20:55:07 @agent_ppo2.py:193][0m |          -0.0053 |           5.6816 |           2.9591 |
[32m[20230203 20:55:07 @agent_ppo2.py:193][0m |          -0.0122 |           5.5081 |           2.9600 |
[32m[20230203 20:55:07 @agent_ppo2.py:193][0m |          -0.0173 |           5.4258 |           2.9576 |
[32m[20230203 20:55:07 @agent_ppo2.py:193][0m |          -0.0089 |           5.3296 |           2.9576 |
[32m[20230203 20:55:07 @agent_ppo2.py:193][0m |          -0.0102 |           5.2314 |           2.9549 |
[32m[20230203 20:55:07 @agent_ppo2.py:193][0m |          -0.0122 |           5.1424 |           2.9558 |
[32m[20230203 20:55:07 @agent_ppo2.py:193][0m |          -0.0143 |           5.0312 |           2.9516 |
[32m[20230203 20:55:07 @agent_ppo2.py:138][0m Policy update time: 0.59 s
[32m[20230203 20:55:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 233.52
[32m[20230203 20:55:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 238.19
[32m[20230203 20:55:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 241.48
[32m[20230203 20:55:07 @agent_ppo2.py:151][0m Total time:       4.54 min
[32m[20230203 20:55:07 @agent_ppo2.py:153][0m 339968 total steps have happened
[32m[20230203 20:55:07 @agent_ppo2.py:129][0m #------------------------ Iteration 166 --------------------------#
[32m[20230203 20:55:08 @agent_ppo2.py:135][0m Sampling time: 0.53 s by 1 slaves
[32m[20230203 20:55:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:08 @agent_ppo2.py:193][0m |          -0.0008 |          15.7722 |           2.9487 |
[32m[20230203 20:55:08 @agent_ppo2.py:193][0m |          -0.0048 |           9.3965 |           2.9512 |
[32m[20230203 20:55:08 @agent_ppo2.py:193][0m |          -0.0072 |           8.2999 |           2.9436 |
[32m[20230203 20:55:08 @agent_ppo2.py:193][0m |          -0.0087 |           8.0491 |           2.9394 |
[32m[20230203 20:55:08 @agent_ppo2.py:193][0m |          -0.0099 |           7.8646 |           2.9401 |
[32m[20230203 20:55:08 @agent_ppo2.py:193][0m |          -0.0105 |           7.9623 |           2.9383 |
[32m[20230203 20:55:08 @agent_ppo2.py:193][0m |          -0.0101 |           7.7402 |           2.9365 |
[32m[20230203 20:55:08 @agent_ppo2.py:193][0m |          -0.0110 |           7.3726 |           2.9404 |
[32m[20230203 20:55:08 @agent_ppo2.py:193][0m |          -0.0117 |           7.4430 |           2.9371 |
[32m[20230203 20:55:08 @agent_ppo2.py:193][0m |          -0.0108 |           7.3223 |           2.9329 |
[32m[20230203 20:55:08 @agent_ppo2.py:138][0m Policy update time: 0.59 s
[32m[20230203 20:55:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 119.74
[32m[20230203 20:55:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 235.40
[32m[20230203 20:55:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 245.01
[32m[20230203 20:55:09 @agent_ppo2.py:151][0m Total time:       4.56 min
[32m[20230203 20:55:09 @agent_ppo2.py:153][0m 342016 total steps have happened
[32m[20230203 20:55:09 @agent_ppo2.py:129][0m #------------------------ Iteration 167 --------------------------#
[32m[20230203 20:55:09 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:55:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:10 @agent_ppo2.py:193][0m |          -0.0030 |           6.0668 |           2.9638 |
[32m[20230203 20:55:10 @agent_ppo2.py:193][0m |          -0.0008 |           5.5339 |           2.9622 |
[32m[20230203 20:55:10 @agent_ppo2.py:193][0m |          -0.0137 |           5.3835 |           2.9606 |
[32m[20230203 20:55:10 @agent_ppo2.py:193][0m |           0.0281 |           6.6172 |           2.9620 |
[32m[20230203 20:55:10 @agent_ppo2.py:193][0m |          -0.0131 |           5.2927 |           2.9662 |
[32m[20230203 20:55:10 @agent_ppo2.py:193][0m |          -0.0111 |           5.2344 |           2.9656 |
[32m[20230203 20:55:10 @agent_ppo2.py:193][0m |          -0.0132 |           5.1817 |           2.9635 |
[32m[20230203 20:55:10 @agent_ppo2.py:193][0m |          -0.0052 |           5.3864 |           2.9646 |
[32m[20230203 20:55:10 @agent_ppo2.py:193][0m |          -0.0184 |           5.1458 |           2.9678 |
[32m[20230203 20:55:10 @agent_ppo2.py:193][0m |          -0.0136 |           5.0938 |           2.9645 |
[32m[20230203 20:55:10 @agent_ppo2.py:138][0m Policy update time: 0.57 s
[32m[20230203 20:55:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 239.48
[32m[20230203 20:55:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 242.93
[32m[20230203 20:55:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.34
[32m[20230203 20:55:10 @agent_ppo2.py:151][0m Total time:       4.59 min
[32m[20230203 20:55:10 @agent_ppo2.py:153][0m 344064 total steps have happened
[32m[20230203 20:55:10 @agent_ppo2.py:129][0m #------------------------ Iteration 168 --------------------------#
[32m[20230203 20:55:11 @agent_ppo2.py:135][0m Sampling time: 0.51 s by 1 slaves
[32m[20230203 20:55:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:11 @agent_ppo2.py:193][0m |          -0.0004 |           9.1163 |           3.0575 |
[32m[20230203 20:55:11 @agent_ppo2.py:193][0m |          -0.0065 |           7.6464 |           3.0568 |
[32m[20230203 20:55:11 @agent_ppo2.py:193][0m |          -0.0080 |           7.1286 |           3.0581 |
[32m[20230203 20:55:11 @agent_ppo2.py:193][0m |          -0.0096 |           6.8586 |           3.0584 |
[32m[20230203 20:55:11 @agent_ppo2.py:193][0m |          -0.0112 |           6.6610 |           3.0595 |
[32m[20230203 20:55:11 @agent_ppo2.py:193][0m |          -0.0112 |           6.4674 |           3.0592 |
[32m[20230203 20:55:11 @agent_ppo2.py:193][0m |          -0.0118 |           6.3363 |           3.0553 |
[32m[20230203 20:55:11 @agent_ppo2.py:193][0m |          -0.0118 |           6.2639 |           3.0575 |
[32m[20230203 20:55:11 @agent_ppo2.py:193][0m |          -0.0133 |           6.1811 |           3.0569 |
[32m[20230203 20:55:12 @agent_ppo2.py:193][0m |          -0.0134 |           6.1675 |           3.0566 |
[32m[20230203 20:55:12 @agent_ppo2.py:138][0m Policy update time: 0.58 s
[32m[20230203 20:55:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 219.81
[32m[20230203 20:55:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 240.77
[32m[20230203 20:55:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 242.52
[32m[20230203 20:55:12 @agent_ppo2.py:151][0m Total time:       4.61 min
[32m[20230203 20:55:12 @agent_ppo2.py:153][0m 346112 total steps have happened
[32m[20230203 20:55:12 @agent_ppo2.py:129][0m #------------------------ Iteration 169 --------------------------#
[32m[20230203 20:55:12 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:55:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:13 @agent_ppo2.py:193][0m |          -0.0003 |           6.6976 |           3.0166 |
[32m[20230203 20:55:13 @agent_ppo2.py:193][0m |          -0.0053 |           6.3246 |           3.0153 |
[32m[20230203 20:55:13 @agent_ppo2.py:193][0m |          -0.0075 |           6.1580 |           3.0143 |
[32m[20230203 20:55:13 @agent_ppo2.py:193][0m |          -0.0070 |           6.0805 |           3.0099 |
[32m[20230203 20:55:13 @agent_ppo2.py:193][0m |          -0.0085 |           6.0377 |           3.0085 |
[32m[20230203 20:55:13 @agent_ppo2.py:193][0m |          -0.0093 |           5.9837 |           3.0106 |
[32m[20230203 20:55:13 @agent_ppo2.py:193][0m |          -0.0109 |           5.9412 |           3.0074 |
[32m[20230203 20:55:13 @agent_ppo2.py:193][0m |          -0.0111 |           5.8993 |           3.0068 |
[32m[20230203 20:55:13 @agent_ppo2.py:193][0m |          -0.0108 |           5.8564 |           3.0068 |
[32m[20230203 20:55:13 @agent_ppo2.py:193][0m |          -0.0111 |           5.8586 |           3.0037 |
[32m[20230203 20:55:13 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:55:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 239.92
[32m[20230203 20:55:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 242.00
[32m[20230203 20:55:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.57
[32m[20230203 20:55:13 @agent_ppo2.py:151][0m Total time:       4.64 min
[32m[20230203 20:55:13 @agent_ppo2.py:153][0m 348160 total steps have happened
[32m[20230203 20:55:13 @agent_ppo2.py:129][0m #------------------------ Iteration 170 --------------------------#
[32m[20230203 20:55:14 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:55:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:14 @agent_ppo2.py:193][0m |          -0.0009 |           6.1028 |           2.9823 |
[32m[20230203 20:55:14 @agent_ppo2.py:193][0m |          -0.0036 |           5.9458 |           2.9769 |
[32m[20230203 20:55:14 @agent_ppo2.py:193][0m |          -0.0076 |           5.7413 |           2.9745 |
[32m[20230203 20:55:14 @agent_ppo2.py:193][0m |          -0.0073 |           5.5994 |           2.9737 |
[32m[20230203 20:55:14 @agent_ppo2.py:193][0m |          -0.0091 |           5.4502 |           2.9731 |
[32m[20230203 20:55:14 @agent_ppo2.py:193][0m |          -0.0092 |           5.3106 |           2.9727 |
[32m[20230203 20:55:14 @agent_ppo2.py:193][0m |          -0.0090 |           5.1652 |           2.9713 |
[32m[20230203 20:55:14 @agent_ppo2.py:193][0m |          -0.0099 |           5.0673 |           2.9709 |
[32m[20230203 20:55:14 @agent_ppo2.py:193][0m |          -0.0094 |           4.9732 |           2.9722 |
[32m[20230203 20:55:14 @agent_ppo2.py:193][0m |          -0.0116 |           4.8785 |           2.9735 |
[32m[20230203 20:55:14 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:55:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 240.07
[32m[20230203 20:55:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 242.85
[32m[20230203 20:55:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 250.13
[32m[20230203 20:55:15 @agent_ppo2.py:151][0m Total time:       4.66 min
[32m[20230203 20:55:15 @agent_ppo2.py:153][0m 350208 total steps have happened
[32m[20230203 20:55:15 @agent_ppo2.py:129][0m #------------------------ Iteration 171 --------------------------#
[32m[20230203 20:55:15 @agent_ppo2.py:135][0m Sampling time: 0.48 s by 1 slaves
[32m[20230203 20:55:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:16 @agent_ppo2.py:193][0m |           0.0108 |           6.3659 |           2.9432 |
[32m[20230203 20:55:16 @agent_ppo2.py:193][0m |          -0.0028 |           5.3653 |           2.9382 |
[32m[20230203 20:55:16 @agent_ppo2.py:193][0m |          -0.0077 |           5.2871 |           2.9359 |
[32m[20230203 20:55:16 @agent_ppo2.py:193][0m |          -0.0042 |           5.3023 |           2.9339 |
[32m[20230203 20:55:16 @agent_ppo2.py:193][0m |          -0.0088 |           5.2000 |           2.9334 |
[32m[20230203 20:55:16 @agent_ppo2.py:193][0m |          -0.0084 |           5.1791 |           2.9318 |
[32m[20230203 20:55:16 @agent_ppo2.py:193][0m |          -0.0109 |           5.1423 |           2.9329 |
[32m[20230203 20:55:16 @agent_ppo2.py:193][0m |           0.0022 |           5.6297 |           2.9305 |
[32m[20230203 20:55:16 @agent_ppo2.py:193][0m |          -0.0020 |           5.5676 |           2.9307 |
[32m[20230203 20:55:16 @agent_ppo2.py:193][0m |          -0.0066 |           5.2597 |           2.9312 |
[32m[20230203 20:55:16 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:55:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 241.82
[32m[20230203 20:55:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.50
[32m[20230203 20:55:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.15
[32m[20230203 20:55:16 @agent_ppo2.py:151][0m Total time:       4.69 min
[32m[20230203 20:55:16 @agent_ppo2.py:153][0m 352256 total steps have happened
[32m[20230203 20:55:16 @agent_ppo2.py:129][0m #------------------------ Iteration 172 --------------------------#
[32m[20230203 20:55:17 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:55:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:17 @agent_ppo2.py:193][0m |          -0.0001 |          34.4343 |           2.9530 |
[32m[20230203 20:55:17 @agent_ppo2.py:193][0m |          -0.0052 |          15.7694 |           2.9543 |
[32m[20230203 20:55:17 @agent_ppo2.py:193][0m |          -0.0048 |          12.7718 |           2.9524 |
[32m[20230203 20:55:17 @agent_ppo2.py:193][0m |          -0.0102 |          11.8213 |           2.9516 |
[32m[20230203 20:55:17 @agent_ppo2.py:193][0m |          -0.0104 |          10.9191 |           2.9483 |
[32m[20230203 20:55:17 @agent_ppo2.py:193][0m |          -0.0111 |          10.3775 |           2.9493 |
[32m[20230203 20:55:17 @agent_ppo2.py:193][0m |          -0.0119 |           9.8262 |           2.9510 |
[32m[20230203 20:55:17 @agent_ppo2.py:193][0m |          -0.0120 |           9.8193 |           2.9478 |
[32m[20230203 20:55:17 @agent_ppo2.py:193][0m |          -0.0137 |           9.4045 |           2.9485 |
[32m[20230203 20:55:17 @agent_ppo2.py:193][0m |          -0.0151 |           8.8572 |           2.9484 |
[32m[20230203 20:55:17 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:55:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: 32.99
[32m[20230203 20:55:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 76.11
[32m[20230203 20:55:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.50
[32m[20230203 20:55:18 @agent_ppo2.py:151][0m Total time:       4.71 min
[32m[20230203 20:55:18 @agent_ppo2.py:153][0m 354304 total steps have happened
[32m[20230203 20:55:18 @agent_ppo2.py:129][0m #------------------------ Iteration 173 --------------------------#
[32m[20230203 20:55:18 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:55:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:18 @agent_ppo2.py:193][0m |           0.0012 |          36.6127 |           2.8963 |
[32m[20230203 20:55:18 @agent_ppo2.py:193][0m |          -0.0011 |          19.1707 |           2.8908 |
[32m[20230203 20:55:18 @agent_ppo2.py:193][0m |          -0.0063 |          14.9385 |           2.8862 |
[32m[20230203 20:55:18 @agent_ppo2.py:193][0m |          -0.0050 |          13.3391 |           2.8837 |
[32m[20230203 20:55:18 @agent_ppo2.py:193][0m |          -0.0078 |          12.2498 |           2.8821 |
[32m[20230203 20:55:18 @agent_ppo2.py:193][0m |          -0.0104 |          11.4921 |           2.8812 |
[32m[20230203 20:55:19 @agent_ppo2.py:193][0m |          -0.0097 |          10.4363 |           2.8782 |
[32m[20230203 20:55:19 @agent_ppo2.py:193][0m |          -0.0109 |           9.6813 |           2.8826 |
[32m[20230203 20:55:19 @agent_ppo2.py:193][0m |          -0.0132 |           9.1264 |           2.8824 |
[32m[20230203 20:55:19 @agent_ppo2.py:193][0m |          -0.0132 |           8.6989 |           2.8825 |
[32m[20230203 20:55:19 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:55:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 119.71
[32m[20230203 20:55:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 198.06
[32m[20230203 20:55:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 248.74
[32m[20230203 20:55:19 @agent_ppo2.py:151][0m Total time:       4.73 min
[32m[20230203 20:55:19 @agent_ppo2.py:153][0m 356352 total steps have happened
[32m[20230203 20:55:19 @agent_ppo2.py:129][0m #------------------------ Iteration 174 --------------------------#
[32m[20230203 20:55:19 @agent_ppo2.py:135][0m Sampling time: 0.36 s by 1 slaves
[32m[20230203 20:55:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:20 @agent_ppo2.py:193][0m |           0.0004 |          31.2612 |           3.0851 |
[32m[20230203 20:55:20 @agent_ppo2.py:193][0m |          -0.0057 |          25.7509 |           3.0811 |
[32m[20230203 20:55:20 @agent_ppo2.py:193][0m |          -0.0104 |          23.7122 |           3.0776 |
[32m[20230203 20:55:20 @agent_ppo2.py:193][0m |          -0.0113 |          22.1710 |           3.0743 |
[32m[20230203 20:55:20 @agent_ppo2.py:193][0m |          -0.0091 |          21.4485 |           3.0718 |
[32m[20230203 20:55:20 @agent_ppo2.py:193][0m |          -0.0054 |          22.1380 |           3.0689 |
[32m[20230203 20:55:20 @agent_ppo2.py:193][0m |          -0.0136 |          19.3563 |           3.0694 |
[32m[20230203 20:55:20 @agent_ppo2.py:193][0m |          -0.0120 |          19.2151 |           3.0671 |
[32m[20230203 20:55:20 @agent_ppo2.py:193][0m |          -0.0188 |          18.4401 |           3.0640 |
[32m[20230203 20:55:20 @agent_ppo2.py:193][0m |          -0.0174 |          17.8163 |           3.0650 |
[32m[20230203 20:55:20 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 20:55:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 107.45
[32m[20230203 20:55:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 233.17
[32m[20230203 20:55:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.33
[32m[20230203 20:55:20 @agent_ppo2.py:151][0m Total time:       4.75 min
[32m[20230203 20:55:20 @agent_ppo2.py:153][0m 358400 total steps have happened
[32m[20230203 20:55:20 @agent_ppo2.py:129][0m #------------------------ Iteration 175 --------------------------#
[32m[20230203 20:55:21 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 20:55:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:21 @agent_ppo2.py:193][0m |          -0.0010 |          30.0797 |           3.1092 |
[32m[20230203 20:55:21 @agent_ppo2.py:193][0m |          -0.0067 |          18.3620 |           3.1042 |
[32m[20230203 20:55:21 @agent_ppo2.py:193][0m |          -0.0084 |          14.2779 |           3.1051 |
[32m[20230203 20:55:21 @agent_ppo2.py:193][0m |          -0.0105 |          12.2109 |           3.1022 |
[32m[20230203 20:55:21 @agent_ppo2.py:193][0m |          -0.0107 |          11.0221 |           3.0986 |
[32m[20230203 20:55:21 @agent_ppo2.py:193][0m |          -0.0111 |          10.1396 |           3.0970 |
[32m[20230203 20:55:21 @agent_ppo2.py:193][0m |          -0.0120 |           9.4968 |           3.0983 |
[32m[20230203 20:55:21 @agent_ppo2.py:193][0m |          -0.0123 |           8.8928 |           3.1000 |
[32m[20230203 20:55:21 @agent_ppo2.py:193][0m |          -0.0127 |           8.5074 |           3.0996 |
[32m[20230203 20:55:21 @agent_ppo2.py:193][0m |          -0.0132 |           8.2689 |           3.0958 |
[32m[20230203 20:55:21 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:55:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 179.81
[32m[20230203 20:55:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 243.31
[32m[20230203 20:55:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 249.73
[32m[20230203 20:55:22 @agent_ppo2.py:151][0m Total time:       4.77 min
[32m[20230203 20:55:22 @agent_ppo2.py:153][0m 360448 total steps have happened
[32m[20230203 20:55:22 @agent_ppo2.py:129][0m #------------------------ Iteration 176 --------------------------#
[32m[20230203 20:55:22 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:55:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:22 @agent_ppo2.py:193][0m |          -0.0010 |           6.9282 |           2.9028 |
[32m[20230203 20:55:22 @agent_ppo2.py:193][0m |          -0.0092 |           6.4539 |           2.9045 |
[32m[20230203 20:55:22 @agent_ppo2.py:193][0m |          -0.0207 |           6.3875 |           2.9019 |
[32m[20230203 20:55:22 @agent_ppo2.py:193][0m |          -0.0143 |           6.2983 |           2.9013 |
[32m[20230203 20:55:23 @agent_ppo2.py:193][0m |          -0.0185 |           6.2635 |           2.8990 |
[32m[20230203 20:55:23 @agent_ppo2.py:193][0m |          -0.0017 |           6.2151 |           2.9015 |
[32m[20230203 20:55:23 @agent_ppo2.py:193][0m |          -0.0157 |           6.1812 |           2.8980 |
[32m[20230203 20:55:23 @agent_ppo2.py:193][0m |          -0.0084 |           6.1573 |           2.9015 |
[32m[20230203 20:55:23 @agent_ppo2.py:193][0m |          -0.0136 |           6.1556 |           2.9035 |
[32m[20230203 20:55:23 @agent_ppo2.py:193][0m |          -0.0399 |           6.1401 |           2.9040 |
[32m[20230203 20:55:23 @agent_ppo2.py:138][0m Policy update time: 0.57 s
[32m[20230203 20:55:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: 237.13
[32m[20230203 20:55:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 241.37
[32m[20230203 20:55:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.23
[32m[20230203 20:55:23 @agent_ppo2.py:151][0m Total time:       4.80 min
[32m[20230203 20:55:23 @agent_ppo2.py:153][0m 362496 total steps have happened
[32m[20230203 20:55:23 @agent_ppo2.py:129][0m #------------------------ Iteration 177 --------------------------#
[32m[20230203 20:55:24 @agent_ppo2.py:135][0m Sampling time: 0.48 s by 1 slaves
[32m[20230203 20:55:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:24 @agent_ppo2.py:193][0m |          -0.0041 |          12.9860 |           2.9742 |
[32m[20230203 20:55:24 @agent_ppo2.py:193][0m |          -0.0066 |           9.9336 |           2.9785 |
[32m[20230203 20:55:24 @agent_ppo2.py:193][0m |          -0.0035 |           9.3383 |           2.9785 |
[32m[20230203 20:55:24 @agent_ppo2.py:193][0m |          -0.0063 |           8.6254 |           2.9783 |
[32m[20230203 20:55:24 @agent_ppo2.py:193][0m |          -0.0109 |           8.1240 |           2.9782 |
[32m[20230203 20:55:24 @agent_ppo2.py:193][0m |           0.0004 |           8.3773 |           2.9724 |
[32m[20230203 20:55:24 @agent_ppo2.py:193][0m |          -0.0179 |           7.5193 |           2.9775 |
[32m[20230203 20:55:24 @agent_ppo2.py:193][0m |          -0.0231 |           7.2924 |           2.9769 |
[32m[20230203 20:55:24 @agent_ppo2.py:193][0m |          -0.0068 |           7.0586 |           2.9738 |
[32m[20230203 20:55:24 @agent_ppo2.py:193][0m |          -0.0062 |           7.3809 |           2.9763 |
[32m[20230203 20:55:24 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:55:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 238.20
[32m[20230203 20:55:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 240.87
[32m[20230203 20:55:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.85
[32m[20230203 20:55:25 @agent_ppo2.py:151][0m Total time:       4.82 min
[32m[20230203 20:55:25 @agent_ppo2.py:153][0m 364544 total steps have happened
[32m[20230203 20:55:25 @agent_ppo2.py:129][0m #------------------------ Iteration 178 --------------------------#
[32m[20230203 20:55:25 @agent_ppo2.py:135][0m Sampling time: 0.51 s by 1 slaves
[32m[20230203 20:55:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:25 @agent_ppo2.py:193][0m |          -0.0015 |           7.3623 |           3.0549 |
[32m[20230203 20:55:25 @agent_ppo2.py:193][0m |          -0.0048 |           6.5186 |           3.0507 |
[32m[20230203 20:55:25 @agent_ppo2.py:193][0m |          -0.0062 |           6.2343 |           3.0508 |
[32m[20230203 20:55:25 @agent_ppo2.py:193][0m |          -0.0079 |           6.0638 |           3.0498 |
[32m[20230203 20:55:25 @agent_ppo2.py:193][0m |          -0.0089 |           5.9540 |           3.0483 |
[32m[20230203 20:55:26 @agent_ppo2.py:193][0m |          -0.0100 |           5.8816 |           3.0442 |
[32m[20230203 20:55:26 @agent_ppo2.py:193][0m |          -0.0091 |           5.8877 |           3.0434 |
[32m[20230203 20:55:26 @agent_ppo2.py:193][0m |          -0.0095 |           5.8802 |           3.0426 |
[32m[20230203 20:55:26 @agent_ppo2.py:193][0m |          -0.0111 |           5.7805 |           3.0456 |
[32m[20230203 20:55:26 @agent_ppo2.py:193][0m |          -0.0110 |           5.7208 |           3.0448 |
[32m[20230203 20:55:26 @agent_ppo2.py:138][0m Policy update time: 0.58 s
[32m[20230203 20:55:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 232.14
[32m[20230203 20:55:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 235.59
[32m[20230203 20:55:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 249.00
[32m[20230203 20:55:26 @agent_ppo2.py:151][0m Total time:       4.85 min
[32m[20230203 20:55:26 @agent_ppo2.py:153][0m 366592 total steps have happened
[32m[20230203 20:55:26 @agent_ppo2.py:129][0m #------------------------ Iteration 179 --------------------------#
[32m[20230203 20:55:27 @agent_ppo2.py:135][0m Sampling time: 0.48 s by 1 slaves
[32m[20230203 20:55:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:27 @agent_ppo2.py:193][0m |          -0.0020 |           6.5678 |           2.9709 |
[32m[20230203 20:55:27 @agent_ppo2.py:193][0m |          -0.0062 |           6.2113 |           2.9749 |
[32m[20230203 20:55:27 @agent_ppo2.py:193][0m |          -0.0100 |           6.0339 |           2.9720 |
[32m[20230203 20:55:27 @agent_ppo2.py:193][0m |          -0.0087 |           5.9113 |           2.9685 |
[32m[20230203 20:55:27 @agent_ppo2.py:193][0m |          -0.0136 |           5.7804 |           2.9668 |
[32m[20230203 20:55:27 @agent_ppo2.py:193][0m |          -0.0123 |           5.7083 |           2.9674 |
[32m[20230203 20:55:27 @agent_ppo2.py:193][0m |          -0.0140 |           5.5942 |           2.9669 |
[32m[20230203 20:55:27 @agent_ppo2.py:193][0m |          -0.0118 |           5.6181 |           2.9687 |
[32m[20230203 20:55:27 @agent_ppo2.py:193][0m |          -0.0119 |           5.4832 |           2.9677 |
[32m[20230203 20:55:27 @agent_ppo2.py:193][0m |          -0.0131 |           5.5247 |           2.9686 |
[32m[20230203 20:55:27 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:55:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 237.43
[32m[20230203 20:55:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.26
[32m[20230203 20:55:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.01
[32m[20230203 20:55:28 @agent_ppo2.py:151][0m Total time:       4.87 min
[32m[20230203 20:55:28 @agent_ppo2.py:153][0m 368640 total steps have happened
[32m[20230203 20:55:28 @agent_ppo2.py:129][0m #------------------------ Iteration 180 --------------------------#
[32m[20230203 20:55:28 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:55:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:28 @agent_ppo2.py:193][0m |          -0.0016 |          16.8083 |           3.0004 |
[32m[20230203 20:55:28 @agent_ppo2.py:193][0m |          -0.0053 |          10.7097 |           2.9983 |
[32m[20230203 20:55:28 @agent_ppo2.py:193][0m |          -0.0057 |          10.2152 |           2.9952 |
[32m[20230203 20:55:28 @agent_ppo2.py:193][0m |          -0.0137 |           9.3488 |           2.9956 |
[32m[20230203 20:55:28 @agent_ppo2.py:193][0m |          -0.0135 |           8.7566 |           2.9909 |
[32m[20230203 20:55:28 @agent_ppo2.py:193][0m |          -0.0177 |           8.3058 |           2.9911 |
[32m[20230203 20:55:28 @agent_ppo2.py:193][0m |          -0.0182 |           8.0976 |           2.9901 |
[32m[20230203 20:55:28 @agent_ppo2.py:193][0m |          -0.0188 |           7.7629 |           2.9861 |
[32m[20230203 20:55:28 @agent_ppo2.py:193][0m |          -0.0184 |           7.5268 |           2.9852 |
[32m[20230203 20:55:28 @agent_ppo2.py:193][0m |          -0.0173 |           7.5922 |           2.9885 |
[32m[20230203 20:55:28 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 20:55:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 148.75
[32m[20230203 20:55:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.41
[32m[20230203 20:55:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.64
[32m[20230203 20:55:29 @agent_ppo2.py:151][0m Total time:       4.89 min
[32m[20230203 20:55:29 @agent_ppo2.py:153][0m 370688 total steps have happened
[32m[20230203 20:55:29 @agent_ppo2.py:129][0m #------------------------ Iteration 181 --------------------------#
[32m[20230203 20:55:29 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:55:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:29 @agent_ppo2.py:193][0m |          -0.0037 |           7.4146 |           2.9867 |
[32m[20230203 20:55:29 @agent_ppo2.py:193][0m |          -0.0064 |           7.0508 |           2.9811 |
[32m[20230203 20:55:29 @agent_ppo2.py:193][0m |          -0.0098 |           6.8555 |           2.9800 |
[32m[20230203 20:55:29 @agent_ppo2.py:193][0m |          -0.0081 |           6.7037 |           2.9773 |
[32m[20230203 20:55:30 @agent_ppo2.py:193][0m |          -0.0085 |           6.5882 |           2.9762 |
[32m[20230203 20:55:30 @agent_ppo2.py:193][0m |          -0.0075 |           6.6549 |           2.9776 |
[32m[20230203 20:55:30 @agent_ppo2.py:193][0m |          -0.0123 |           6.4141 |           2.9768 |
[32m[20230203 20:55:30 @agent_ppo2.py:193][0m |          -0.0123 |           6.3490 |           2.9768 |
[32m[20230203 20:55:30 @agent_ppo2.py:193][0m |          -0.0122 |           6.3007 |           2.9752 |
[32m[20230203 20:55:30 @agent_ppo2.py:193][0m |          -0.0113 |           6.2215 |           2.9719 |
[32m[20230203 20:55:30 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:55:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.61
[32m[20230203 20:55:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.13
[32m[20230203 20:55:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.16
[32m[20230203 20:55:30 @agent_ppo2.py:151][0m Total time:       4.92 min
[32m[20230203 20:55:30 @agent_ppo2.py:153][0m 372736 total steps have happened
[32m[20230203 20:55:30 @agent_ppo2.py:129][0m #------------------------ Iteration 182 --------------------------#
[32m[20230203 20:55:31 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 1 slaves
[32m[20230203 20:55:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:31 @agent_ppo2.py:193][0m |          -0.0009 |          45.9183 |           2.8008 |
[32m[20230203 20:55:31 @agent_ppo2.py:193][0m |          -0.0044 |          16.2036 |           2.7960 |
[32m[20230203 20:55:31 @agent_ppo2.py:193][0m |          -0.0070 |          10.1810 |           2.7962 |
[32m[20230203 20:55:31 @agent_ppo2.py:193][0m |          -0.0077 |           7.9616 |           2.7952 |
[32m[20230203 20:55:31 @agent_ppo2.py:193][0m |          -0.0073 |           7.3246 |           2.7943 |
[32m[20230203 20:55:31 @agent_ppo2.py:193][0m |          -0.0097 |           6.4926 |           2.7918 |
[32m[20230203 20:55:31 @agent_ppo2.py:193][0m |          -0.0121 |           6.0841 |           2.7919 |
[32m[20230203 20:55:31 @agent_ppo2.py:193][0m |          -0.0053 |           6.0456 |           2.7908 |
[32m[20230203 20:55:31 @agent_ppo2.py:193][0m |          -0.0118 |           5.6257 |           2.7899 |
[32m[20230203 20:55:31 @agent_ppo2.py:193][0m |          -0.0134 |           5.4616 |           2.7896 |
[32m[20230203 20:55:31 @agent_ppo2.py:138][0m Policy update time: 0.40 s
[32m[20230203 20:55:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 125.57
[32m[20230203 20:55:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 243.54
[32m[20230203 20:55:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 183.04
[32m[20230203 20:55:31 @agent_ppo2.py:151][0m Total time:       4.94 min
[32m[20230203 20:55:31 @agent_ppo2.py:153][0m 374784 total steps have happened
[32m[20230203 20:55:31 @agent_ppo2.py:129][0m #------------------------ Iteration 183 --------------------------#
[32m[20230203 20:55:32 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 1 slaves
[32m[20230203 20:55:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:32 @agent_ppo2.py:193][0m |          -0.0032 |          20.4335 |           2.8801 |
[32m[20230203 20:55:32 @agent_ppo2.py:193][0m |          -0.0098 |          12.9325 |           2.8722 |
[32m[20230203 20:55:32 @agent_ppo2.py:193][0m |          -0.0128 |          11.8662 |           2.8795 |
[32m[20230203 20:55:32 @agent_ppo2.py:193][0m |          -0.0019 |          11.4229 |           2.8774 |
[32m[20230203 20:55:32 @agent_ppo2.py:193][0m |          -0.0122 |          10.7288 |           2.8762 |
[32m[20230203 20:55:32 @agent_ppo2.py:193][0m |           0.0076 |          11.4978 |           2.8770 |
[32m[20230203 20:55:32 @agent_ppo2.py:193][0m |          -0.0007 |          10.0158 |           2.8786 |
[32m[20230203 20:55:32 @agent_ppo2.py:193][0m |          -0.0023 |           9.8420 |           2.8799 |
[32m[20230203 20:55:32 @agent_ppo2.py:193][0m |          -0.0142 |           9.5367 |           2.8775 |
[32m[20230203 20:55:32 @agent_ppo2.py:193][0m |          -0.0212 |           9.2980 |           2.8776 |
[32m[20230203 20:55:32 @agent_ppo2.py:138][0m Policy update time: 0.41 s
[32m[20230203 20:55:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 122.41
[32m[20230203 20:55:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 239.45
[32m[20230203 20:55:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.75
[32m[20230203 20:55:33 @agent_ppo2.py:151][0m Total time:       4.96 min
[32m[20230203 20:55:33 @agent_ppo2.py:153][0m 376832 total steps have happened
[32m[20230203 20:55:33 @agent_ppo2.py:129][0m #------------------------ Iteration 184 --------------------------#
[32m[20230203 20:55:33 @agent_ppo2.py:135][0m Sampling time: 0.51 s by 1 slaves
[32m[20230203 20:55:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:33 @agent_ppo2.py:193][0m |          -0.0025 |          31.5211 |           2.9918 |
[32m[20230203 20:55:33 @agent_ppo2.py:193][0m |          -0.0034 |          26.2725 |           2.9912 |
[32m[20230203 20:55:33 @agent_ppo2.py:193][0m |          -0.0088 |          22.8186 |           2.9879 |
[32m[20230203 20:55:33 @agent_ppo2.py:193][0m |          -0.0201 |          23.0353 |           2.9898 |
[32m[20230203 20:55:33 @agent_ppo2.py:193][0m |          -0.0141 |          20.1787 |           2.9857 |
[32m[20230203 20:55:33 @agent_ppo2.py:193][0m |          -0.0141 |          19.4625 |           2.9848 |
[32m[20230203 20:55:34 @agent_ppo2.py:193][0m |          -0.0150 |          18.1585 |           2.9857 |
[32m[20230203 20:55:34 @agent_ppo2.py:193][0m |          -0.0133 |          17.8776 |           2.9860 |
[32m[20230203 20:55:34 @agent_ppo2.py:193][0m |          -0.0168 |          17.0450 |           2.9837 |
[32m[20230203 20:55:34 @agent_ppo2.py:193][0m |          -0.0183 |          16.8135 |           2.9854 |
[32m[20230203 20:55:34 @agent_ppo2.py:138][0m Policy update time: 0.58 s
[32m[20230203 20:55:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 134.18
[32m[20230203 20:55:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.74
[32m[20230203 20:55:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.47
[32m[20230203 20:55:34 @agent_ppo2.py:151][0m Total time:       4.98 min
[32m[20230203 20:55:34 @agent_ppo2.py:153][0m 378880 total steps have happened
[32m[20230203 20:55:34 @agent_ppo2.py:129][0m #------------------------ Iteration 185 --------------------------#
[32m[20230203 20:55:35 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:55:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:35 @agent_ppo2.py:193][0m |           0.0033 |          31.7979 |           2.9704 |
[32m[20230203 20:55:35 @agent_ppo2.py:193][0m |          -0.0062 |          24.9345 |           2.9715 |
[32m[20230203 20:55:35 @agent_ppo2.py:193][0m |          -0.0072 |          23.5206 |           2.9691 |
[32m[20230203 20:55:35 @agent_ppo2.py:193][0m |          -0.0096 |          22.5710 |           2.9701 |
[32m[20230203 20:55:35 @agent_ppo2.py:193][0m |          -0.0098 |          22.1987 |           2.9662 |
[32m[20230203 20:55:35 @agent_ppo2.py:193][0m |          -0.0115 |          21.5539 |           2.9684 |
[32m[20230203 20:55:35 @agent_ppo2.py:193][0m |          -0.0120 |          20.8784 |           2.9661 |
[32m[20230203 20:55:35 @agent_ppo2.py:193][0m |          -0.0120 |          20.5938 |           2.9660 |
[32m[20230203 20:55:35 @agent_ppo2.py:193][0m |          -0.0133 |          20.1667 |           2.9634 |
[32m[20230203 20:55:35 @agent_ppo2.py:193][0m |          -0.0122 |          19.9112 |           2.9633 |
[32m[20230203 20:55:35 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:55:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 66.94
[32m[20230203 20:55:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 241.21
[32m[20230203 20:55:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.04
[32m[20230203 20:55:35 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 258.04
[32m[20230203 20:55:35 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 258.04
[32m[20230203 20:55:35 @agent_ppo2.py:151][0m Total time:       5.00 min
[32m[20230203 20:55:35 @agent_ppo2.py:153][0m 380928 total steps have happened
[32m[20230203 20:55:35 @agent_ppo2.py:129][0m #------------------------ Iteration 186 --------------------------#
[32m[20230203 20:55:36 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:55:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:36 @agent_ppo2.py:193][0m |           0.0012 |          30.1964 |           3.0512 |
[32m[20230203 20:55:36 @agent_ppo2.py:193][0m |          -0.0050 |          23.1753 |           3.0427 |
[32m[20230203 20:55:36 @agent_ppo2.py:193][0m |          -0.0052 |          21.2964 |           3.0367 |
[32m[20230203 20:55:36 @agent_ppo2.py:193][0m |          -0.0092 |          19.9507 |           3.0377 |
[32m[20230203 20:55:36 @agent_ppo2.py:193][0m |          -0.0089 |          19.0133 |           3.0363 |
[32m[20230203 20:55:36 @agent_ppo2.py:193][0m |          -0.0084 |          17.8250 |           3.0382 |
[32m[20230203 20:55:36 @agent_ppo2.py:193][0m |          -0.0091 |          17.2022 |           3.0368 |
[32m[20230203 20:55:36 @agent_ppo2.py:193][0m |          -0.0089 |          16.6893 |           3.0341 |
[32m[20230203 20:55:36 @agent_ppo2.py:193][0m |          -0.0097 |          16.2354 |           3.0330 |
[32m[20230203 20:55:36 @agent_ppo2.py:193][0m |          -0.0101 |          15.7260 |           3.0348 |
[32m[20230203 20:55:36 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:55:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 177.76
[32m[20230203 20:55:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 238.36
[32m[20230203 20:55:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 250.48
[32m[20230203 20:55:37 @agent_ppo2.py:151][0m Total time:       5.03 min
[32m[20230203 20:55:37 @agent_ppo2.py:153][0m 382976 total steps have happened
[32m[20230203 20:55:37 @agent_ppo2.py:129][0m #------------------------ Iteration 187 --------------------------#
[32m[20230203 20:55:37 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:55:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:37 @agent_ppo2.py:193][0m |          -0.0005 |           7.1718 |           2.9750 |
[32m[20230203 20:55:37 @agent_ppo2.py:193][0m |          -0.0063 |           6.6197 |           2.9699 |
[32m[20230203 20:55:37 @agent_ppo2.py:193][0m |          -0.0084 |           6.3936 |           2.9681 |
[32m[20230203 20:55:38 @agent_ppo2.py:193][0m |          -0.0100 |           6.2197 |           2.9673 |
[32m[20230203 20:55:38 @agent_ppo2.py:193][0m |          -0.0057 |           6.3396 |           2.9685 |
[32m[20230203 20:55:38 @agent_ppo2.py:193][0m |          -0.0081 |           6.0043 |           2.9650 |
[32m[20230203 20:55:38 @agent_ppo2.py:193][0m |          -0.0090 |           5.9472 |           2.9660 |
[32m[20230203 20:55:38 @agent_ppo2.py:193][0m |          -0.0110 |           5.8822 |           2.9665 |
[32m[20230203 20:55:38 @agent_ppo2.py:193][0m |          -0.0116 |           5.8207 |           2.9634 |
[32m[20230203 20:55:38 @agent_ppo2.py:193][0m |          -0.0109 |           5.7832 |           2.9614 |
[32m[20230203 20:55:38 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:55:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 238.21
[32m[20230203 20:55:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 241.17
[32m[20230203 20:55:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 247.50
[32m[20230203 20:55:38 @agent_ppo2.py:151][0m Total time:       5.05 min
[32m[20230203 20:55:38 @agent_ppo2.py:153][0m 385024 total steps have happened
[32m[20230203 20:55:38 @agent_ppo2.py:129][0m #------------------------ Iteration 188 --------------------------#
[32m[20230203 20:55:39 @agent_ppo2.py:135][0m Sampling time: 0.56 s by 1 slaves
[32m[20230203 20:55:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:39 @agent_ppo2.py:193][0m |           0.0111 |          21.4707 |           2.9609 |
[32m[20230203 20:55:39 @agent_ppo2.py:193][0m |          -0.0083 |          14.0831 |           2.9609 |
[32m[20230203 20:55:39 @agent_ppo2.py:193][0m |          -0.0067 |          12.5228 |           2.9588 |
[32m[20230203 20:55:39 @agent_ppo2.py:193][0m |          -0.0085 |          10.7574 |           2.9573 |
[32m[20230203 20:55:39 @agent_ppo2.py:193][0m |          -0.0123 |           9.9317 |           2.9558 |
[32m[20230203 20:55:39 @agent_ppo2.py:193][0m |          -0.0118 |           9.4498 |           2.9568 |
[32m[20230203 20:55:39 @agent_ppo2.py:193][0m |          -0.0141 |           8.9044 |           2.9535 |
[32m[20230203 20:55:39 @agent_ppo2.py:193][0m |          -0.0155 |           8.6856 |           2.9537 |
[32m[20230203 20:55:39 @agent_ppo2.py:193][0m |          -0.0108 |           8.5305 |           2.9548 |
[32m[20230203 20:55:39 @agent_ppo2.py:193][0m |          -0.0112 |           8.5131 |           2.9536 |
[32m[20230203 20:55:39 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:55:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 135.31
[32m[20230203 20:55:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 231.68
[32m[20230203 20:55:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 246.62
[32m[20230203 20:55:40 @agent_ppo2.py:151][0m Total time:       5.08 min
[32m[20230203 20:55:40 @agent_ppo2.py:153][0m 387072 total steps have happened
[32m[20230203 20:55:40 @agent_ppo2.py:129][0m #------------------------ Iteration 189 --------------------------#
[32m[20230203 20:55:40 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:55:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:40 @agent_ppo2.py:193][0m |           0.0002 |           8.3445 |           3.0015 |
[32m[20230203 20:55:40 @agent_ppo2.py:193][0m |          -0.0049 |           7.5641 |           2.9970 |
[32m[20230203 20:55:41 @agent_ppo2.py:193][0m |          -0.0074 |           7.3918 |           2.9941 |
[32m[20230203 20:55:41 @agent_ppo2.py:193][0m |          -0.0099 |           7.2965 |           2.9920 |
[32m[20230203 20:55:41 @agent_ppo2.py:193][0m |          -0.0081 |           7.2809 |           2.9942 |
[32m[20230203 20:55:41 @agent_ppo2.py:193][0m |          -0.0096 |           7.2410 |           2.9940 |
[32m[20230203 20:55:41 @agent_ppo2.py:193][0m |          -0.0107 |           7.0940 |           2.9952 |
[32m[20230203 20:55:41 @agent_ppo2.py:193][0m |          -0.0113 |           7.0480 |           2.9938 |
[32m[20230203 20:55:41 @agent_ppo2.py:193][0m |          -0.0103 |           7.0671 |           2.9947 |
[32m[20230203 20:55:41 @agent_ppo2.py:193][0m |          -0.0135 |           6.9736 |           2.9925 |
[32m[20230203 20:55:41 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:55:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 233.33
[32m[20230203 20:55:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 234.79
[32m[20230203 20:55:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 247.55
[32m[20230203 20:55:41 @agent_ppo2.py:151][0m Total time:       5.10 min
[32m[20230203 20:55:41 @agent_ppo2.py:153][0m 389120 total steps have happened
[32m[20230203 20:55:41 @agent_ppo2.py:129][0m #------------------------ Iteration 190 --------------------------#
[32m[20230203 20:55:42 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:55:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:42 @agent_ppo2.py:193][0m |          -0.0010 |           6.5436 |           3.0037 |
[32m[20230203 20:55:42 @agent_ppo2.py:193][0m |          -0.0067 |           6.2559 |           2.9918 |
[32m[20230203 20:55:42 @agent_ppo2.py:193][0m |          -0.0074 |           6.1619 |           2.9864 |
[32m[20230203 20:55:42 @agent_ppo2.py:193][0m |          -0.0092 |           6.0953 |           2.9853 |
[32m[20230203 20:55:42 @agent_ppo2.py:193][0m |          -0.0102 |           6.0677 |           2.9843 |
[32m[20230203 20:55:42 @agent_ppo2.py:193][0m |          -0.0113 |           6.0137 |           2.9837 |
[32m[20230203 20:55:42 @agent_ppo2.py:193][0m |          -0.0118 |           6.0067 |           2.9865 |
[32m[20230203 20:55:42 @agent_ppo2.py:193][0m |          -0.0127 |           5.9555 |           2.9826 |
[32m[20230203 20:55:42 @agent_ppo2.py:193][0m |          -0.0131 |           5.9599 |           2.9812 |
[32m[20230203 20:55:42 @agent_ppo2.py:193][0m |          -0.0132 |           5.9192 |           2.9780 |
[32m[20230203 20:55:42 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:55:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 235.18
[32m[20230203 20:55:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 236.79
[32m[20230203 20:55:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.58
[32m[20230203 20:55:43 @agent_ppo2.py:151][0m Total time:       5.13 min
[32m[20230203 20:55:43 @agent_ppo2.py:153][0m 391168 total steps have happened
[32m[20230203 20:55:43 @agent_ppo2.py:129][0m #------------------------ Iteration 191 --------------------------#
[32m[20230203 20:55:43 @agent_ppo2.py:135][0m Sampling time: 0.48 s by 1 slaves
[32m[20230203 20:55:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:43 @agent_ppo2.py:193][0m |           0.0028 |           7.4406 |           2.9886 |
[32m[20230203 20:55:43 @agent_ppo2.py:193][0m |           0.0004 |           5.5272 |           2.9851 |
[32m[20230203 20:55:43 @agent_ppo2.py:193][0m |           0.0008 |           4.2694 |           2.9791 |
[32m[20230203 20:55:44 @agent_ppo2.py:193][0m |           0.0071 |           4.5887 |           2.9761 |
[32m[20230203 20:55:44 @agent_ppo2.py:193][0m |          -0.0024 |           4.0754 |           2.9809 |
[32m[20230203 20:55:44 @agent_ppo2.py:193][0m |           0.0067 |           4.5870 |           2.9837 |
[32m[20230203 20:55:44 @agent_ppo2.py:193][0m |          -0.0105 |           4.0113 |           2.9704 |
[32m[20230203 20:55:44 @agent_ppo2.py:193][0m |          -0.0065 |           3.9382 |           2.9782 |
[32m[20230203 20:55:44 @agent_ppo2.py:193][0m |          -0.0101 |           3.9276 |           2.9794 |
[32m[20230203 20:55:44 @agent_ppo2.py:193][0m |          -0.0085 |           3.8874 |           2.9799 |
[32m[20230203 20:55:44 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:55:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 236.52
[32m[20230203 20:55:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 242.83
[32m[20230203 20:55:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.48
[32m[20230203 20:55:44 @agent_ppo2.py:151][0m Total time:       5.15 min
[32m[20230203 20:55:44 @agent_ppo2.py:153][0m 393216 total steps have happened
[32m[20230203 20:55:44 @agent_ppo2.py:129][0m #------------------------ Iteration 192 --------------------------#
[32m[20230203 20:55:45 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 20:55:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:45 @agent_ppo2.py:193][0m |          -0.0042 |           6.9081 |           2.9950 |
[32m[20230203 20:55:45 @agent_ppo2.py:193][0m |          -0.0082 |           6.4069 |           2.9961 |
[32m[20230203 20:55:45 @agent_ppo2.py:193][0m |           0.0018 |           6.4807 |           2.9984 |
[32m[20230203 20:55:45 @agent_ppo2.py:193][0m |          -0.0089 |           5.8444 |           2.9963 |
[32m[20230203 20:55:45 @agent_ppo2.py:193][0m |          -0.0109 |           5.5972 |           2.9971 |
[32m[20230203 20:55:45 @agent_ppo2.py:193][0m |          -0.0094 |           5.4585 |           2.9979 |
[32m[20230203 20:55:45 @agent_ppo2.py:193][0m |          -0.0105 |           5.3761 |           2.9998 |
[32m[20230203 20:55:45 @agent_ppo2.py:193][0m |          -0.0077 |           5.3512 |           3.0000 |
[32m[20230203 20:55:45 @agent_ppo2.py:193][0m |          -0.0132 |           5.1823 |           2.9994 |
[32m[20230203 20:55:45 @agent_ppo2.py:193][0m |          -0.0088 |           5.0722 |           2.9973 |
[32m[20230203 20:55:45 @agent_ppo2.py:138][0m Policy update time: 0.54 s
[32m[20230203 20:55:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 240.05
[32m[20230203 20:55:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 241.90
[32m[20230203 20:55:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.93
[32m[20230203 20:55:46 @agent_ppo2.py:151][0m Total time:       5.17 min
[32m[20230203 20:55:46 @agent_ppo2.py:153][0m 395264 total steps have happened
[32m[20230203 20:55:46 @agent_ppo2.py:129][0m #------------------------ Iteration 193 --------------------------#
[32m[20230203 20:55:46 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:55:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:46 @agent_ppo2.py:193][0m |          -0.0052 |           9.4060 |           3.0028 |
[32m[20230203 20:55:46 @agent_ppo2.py:193][0m |           0.0074 |           9.1084 |           3.0045 |
[32m[20230203 20:55:46 @agent_ppo2.py:193][0m |          -0.0022 |           7.8030 |           3.0036 |
[32m[20230203 20:55:46 @agent_ppo2.py:193][0m |          -0.0043 |           7.2984 |           3.0003 |
[32m[20230203 20:55:46 @agent_ppo2.py:193][0m |          -0.0080 |           6.9725 |           3.0054 |
[32m[20230203 20:55:46 @agent_ppo2.py:193][0m |          -0.0100 |           6.6784 |           3.0050 |
[32m[20230203 20:55:47 @agent_ppo2.py:193][0m |          -0.0065 |           6.4627 |           3.0042 |
[32m[20230203 20:55:47 @agent_ppo2.py:193][0m |          -0.0142 |           6.1712 |           3.0028 |
[32m[20230203 20:55:47 @agent_ppo2.py:193][0m |          -0.0135 |           5.9891 |           3.0030 |
[32m[20230203 20:55:47 @agent_ppo2.py:193][0m |          -0.0172 |           5.8329 |           3.0030 |
[32m[20230203 20:55:47 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:55:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 234.91
[32m[20230203 20:55:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 242.06
[32m[20230203 20:55:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.56
[32m[20230203 20:55:47 @agent_ppo2.py:151][0m Total time:       5.20 min
[32m[20230203 20:55:47 @agent_ppo2.py:153][0m 397312 total steps have happened
[32m[20230203 20:55:47 @agent_ppo2.py:129][0m #------------------------ Iteration 194 --------------------------#
[32m[20230203 20:55:47 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:55:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:48 @agent_ppo2.py:193][0m |          -0.0023 |          22.2677 |           2.9750 |
[32m[20230203 20:55:48 @agent_ppo2.py:193][0m |          -0.0055 |          16.2065 |           2.9740 |
[32m[20230203 20:55:48 @agent_ppo2.py:193][0m |          -0.0094 |          15.2598 |           2.9740 |
[32m[20230203 20:55:48 @agent_ppo2.py:193][0m |          -0.0098 |          14.7279 |           2.9728 |
[32m[20230203 20:55:48 @agent_ppo2.py:193][0m |          -0.0081 |          14.9232 |           2.9715 |
[32m[20230203 20:55:48 @agent_ppo2.py:193][0m |          -0.0121 |          14.0402 |           2.9705 |
[32m[20230203 20:55:48 @agent_ppo2.py:193][0m |          -0.0129 |          13.9569 |           2.9689 |
[32m[20230203 20:55:48 @agent_ppo2.py:193][0m |          -0.0134 |          13.5580 |           2.9698 |
[32m[20230203 20:55:48 @agent_ppo2.py:193][0m |          -0.0145 |          13.3010 |           2.9683 |
[32m[20230203 20:55:48 @agent_ppo2.py:193][0m |          -0.0141 |          13.0570 |           2.9671 |
[32m[20230203 20:55:48 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:55:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 141.39
[32m[20230203 20:55:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 234.33
[32m[20230203 20:55:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 243.79
[32m[20230203 20:55:48 @agent_ppo2.py:151][0m Total time:       5.22 min
[32m[20230203 20:55:48 @agent_ppo2.py:153][0m 399360 total steps have happened
[32m[20230203 20:55:48 @agent_ppo2.py:129][0m #------------------------ Iteration 195 --------------------------#
[32m[20230203 20:55:49 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:55:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:49 @agent_ppo2.py:193][0m |           0.0002 |           9.0142 |           2.9501 |
[32m[20230203 20:55:49 @agent_ppo2.py:193][0m |          -0.0027 |           7.4657 |           2.9434 |
[32m[20230203 20:55:49 @agent_ppo2.py:193][0m |          -0.0047 |           7.3164 |           2.9440 |
[32m[20230203 20:55:49 @agent_ppo2.py:193][0m |          -0.0032 |           7.2884 |           2.9418 |
[32m[20230203 20:55:49 @agent_ppo2.py:193][0m |          -0.0062 |           7.1394 |           2.9372 |
[32m[20230203 20:55:49 @agent_ppo2.py:193][0m |          -0.0078 |           7.0764 |           2.9379 |
[32m[20230203 20:55:49 @agent_ppo2.py:193][0m |          -0.0082 |           7.0002 |           2.9352 |
[32m[20230203 20:55:49 @agent_ppo2.py:193][0m |          -0.0076 |           7.0396 |           2.9336 |
[32m[20230203 20:55:49 @agent_ppo2.py:193][0m |          -0.0093 |           6.9885 |           2.9341 |
[32m[20230203 20:55:49 @agent_ppo2.py:193][0m |          -0.0101 |           6.8802 |           2.9331 |
[32m[20230203 20:55:49 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:55:50 @agent_ppo2.py:146][0m Average TRAINING episode reward: 244.52
[32m[20230203 20:55:50 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.92
[32m[20230203 20:55:50 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 203.02
[32m[20230203 20:55:50 @agent_ppo2.py:151][0m Total time:       5.24 min
[32m[20230203 20:55:50 @agent_ppo2.py:153][0m 401408 total steps have happened
[32m[20230203 20:55:50 @agent_ppo2.py:129][0m #------------------------ Iteration 196 --------------------------#
[32m[20230203 20:55:50 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:55:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:50 @agent_ppo2.py:193][0m |           0.0015 |          10.5980 |           2.9666 |
[32m[20230203 20:55:50 @agent_ppo2.py:193][0m |          -0.0048 |           7.6386 |           2.9641 |
[32m[20230203 20:55:50 @agent_ppo2.py:193][0m |          -0.0068 |           6.5898 |           2.9634 |
[32m[20230203 20:55:50 @agent_ppo2.py:193][0m |          -0.0071 |           5.9664 |           2.9604 |
[32m[20230203 20:55:51 @agent_ppo2.py:193][0m |          -0.0081 |           5.6126 |           2.9601 |
[32m[20230203 20:55:51 @agent_ppo2.py:193][0m |          -0.0086 |           5.2764 |           2.9578 |
[32m[20230203 20:55:51 @agent_ppo2.py:193][0m |          -0.0111 |           5.0926 |           2.9592 |
[32m[20230203 20:55:51 @agent_ppo2.py:193][0m |          -0.0103 |           4.9505 |           2.9600 |
[32m[20230203 20:55:51 @agent_ppo2.py:193][0m |          -0.0111 |           4.8006 |           2.9577 |
[32m[20230203 20:55:51 @agent_ppo2.py:193][0m |          -0.0113 |           4.7032 |           2.9590 |
[32m[20230203 20:55:51 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:55:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 235.59
[32m[20230203 20:55:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 239.76
[32m[20230203 20:55:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.47
[32m[20230203 20:55:51 @agent_ppo2.py:151][0m Total time:       5.27 min
[32m[20230203 20:55:51 @agent_ppo2.py:153][0m 403456 total steps have happened
[32m[20230203 20:55:51 @agent_ppo2.py:129][0m #------------------------ Iteration 197 --------------------------#
[32m[20230203 20:55:52 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:55:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:52 @agent_ppo2.py:193][0m |           0.0014 |           6.5677 |           3.0084 |
[32m[20230203 20:55:52 @agent_ppo2.py:193][0m |          -0.0035 |           6.4276 |           3.0010 |
[32m[20230203 20:55:52 @agent_ppo2.py:193][0m |          -0.0073 |           6.3319 |           2.9977 |
[32m[20230203 20:55:52 @agent_ppo2.py:193][0m |          -0.0081 |           6.2976 |           2.9946 |
[32m[20230203 20:55:52 @agent_ppo2.py:193][0m |          -0.0099 |           6.2202 |           2.9920 |
[32m[20230203 20:55:52 @agent_ppo2.py:193][0m |          -0.0115 |           6.1709 |           2.9888 |
[32m[20230203 20:55:52 @agent_ppo2.py:193][0m |          -0.0113 |           6.1333 |           2.9864 |
[32m[20230203 20:55:52 @agent_ppo2.py:193][0m |          -0.0127 |           6.1043 |           2.9876 |
[32m[20230203 20:55:52 @agent_ppo2.py:193][0m |          -0.0127 |           6.0751 |           2.9874 |
[32m[20230203 20:55:52 @agent_ppo2.py:193][0m |          -0.0130 |           6.0800 |           2.9868 |
[32m[20230203 20:55:52 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:55:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 241.27
[32m[20230203 20:55:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 242.41
[32m[20230203 20:55:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.72
[32m[20230203 20:55:53 @agent_ppo2.py:151][0m Total time:       5.29 min
[32m[20230203 20:55:53 @agent_ppo2.py:153][0m 405504 total steps have happened
[32m[20230203 20:55:53 @agent_ppo2.py:129][0m #------------------------ Iteration 198 --------------------------#
[32m[20230203 20:55:53 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:55:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:53 @agent_ppo2.py:193][0m |          -0.0009 |           5.9422 |           2.9380 |
[32m[20230203 20:55:53 @agent_ppo2.py:193][0m |          -0.0046 |           5.6485 |           2.9349 |
[32m[20230203 20:55:53 @agent_ppo2.py:193][0m |          -0.0087 |           5.4628 |           2.9303 |
[32m[20230203 20:55:53 @agent_ppo2.py:193][0m |          -0.0100 |           5.2979 |           2.9322 |
[32m[20230203 20:55:53 @agent_ppo2.py:193][0m |          -0.0112 |           5.1911 |           2.9312 |
[32m[20230203 20:55:53 @agent_ppo2.py:193][0m |          -0.0113 |           5.1021 |           2.9319 |
[32m[20230203 20:55:53 @agent_ppo2.py:193][0m |          -0.0112 |           5.0495 |           2.9325 |
[32m[20230203 20:55:54 @agent_ppo2.py:193][0m |          -0.0114 |           4.9732 |           2.9358 |
[32m[20230203 20:55:54 @agent_ppo2.py:193][0m |          -0.0121 |           4.8865 |           2.9346 |
[32m[20230203 20:55:54 @agent_ppo2.py:193][0m |          -0.0126 |           4.8245 |           2.9378 |
[32m[20230203 20:55:54 @agent_ppo2.py:138][0m Policy update time: 0.56 s
[32m[20230203 20:55:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 237.67
[32m[20230203 20:55:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 242.63
[32m[20230203 20:55:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.36
[32m[20230203 20:55:54 @agent_ppo2.py:151][0m Total time:       5.31 min
[32m[20230203 20:55:54 @agent_ppo2.py:153][0m 407552 total steps have happened
[32m[20230203 20:55:54 @agent_ppo2.py:129][0m #------------------------ Iteration 199 --------------------------#
[32m[20230203 20:55:55 @agent_ppo2.py:135][0m Sampling time: 0.50 s by 1 slaves
[32m[20230203 20:55:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:55 @agent_ppo2.py:193][0m |           0.0010 |           6.9483 |           3.0267 |
[32m[20230203 20:55:55 @agent_ppo2.py:193][0m |          -0.0045 |           6.3215 |           3.0170 |
[32m[20230203 20:55:55 @agent_ppo2.py:193][0m |          -0.0050 |           6.1153 |           3.0183 |
[32m[20230203 20:55:55 @agent_ppo2.py:193][0m |          -0.0068 |           5.9955 |           3.0188 |
[32m[20230203 20:55:55 @agent_ppo2.py:193][0m |          -0.0070 |           5.9794 |           3.0169 |
[32m[20230203 20:55:55 @agent_ppo2.py:193][0m |          -0.0097 |           5.9022 |           3.0186 |
[32m[20230203 20:55:55 @agent_ppo2.py:193][0m |          -0.0090 |           5.8336 |           3.0178 |
[32m[20230203 20:55:55 @agent_ppo2.py:193][0m |          -0.0100 |           5.8203 |           3.0166 |
[32m[20230203 20:55:55 @agent_ppo2.py:193][0m |          -0.0122 |           5.7245 |           3.0174 |
[32m[20230203 20:55:55 @agent_ppo2.py:193][0m |          -0.0135 |           5.7011 |           3.0184 |
[32m[20230203 20:55:55 @agent_ppo2.py:138][0m Policy update time: 0.57 s
[32m[20230203 20:55:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 234.34
[32m[20230203 20:55:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 235.70
[32m[20230203 20:55:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.42
[32m[20230203 20:55:56 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 261.42
[32m[20230203 20:55:56 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 261.42
[32m[20230203 20:55:56 @agent_ppo2.py:151][0m Total time:       5.34 min
[32m[20230203 20:55:56 @agent_ppo2.py:153][0m 409600 total steps have happened
[32m[20230203 20:55:56 @agent_ppo2.py:129][0m #------------------------ Iteration 200 --------------------------#
[32m[20230203 20:55:56 @agent_ppo2.py:135][0m Sampling time: 0.51 s by 1 slaves
[32m[20230203 20:55:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:56 @agent_ppo2.py:193][0m |          -0.0001 |          13.2171 |           3.0067 |
[32m[20230203 20:55:56 @agent_ppo2.py:193][0m |          -0.0049 |          10.1260 |           3.0010 |
[32m[20230203 20:55:56 @agent_ppo2.py:193][0m |          -0.0066 |           9.3450 |           3.0024 |
[32m[20230203 20:55:56 @agent_ppo2.py:193][0m |          -0.0074 |           9.1486 |           2.9991 |
[32m[20230203 20:55:56 @agent_ppo2.py:193][0m |          -0.0094 |           8.9195 |           3.0030 |
[32m[20230203 20:55:56 @agent_ppo2.py:193][0m |          -0.0101 |           8.8226 |           3.0008 |
[32m[20230203 20:55:56 @agent_ppo2.py:193][0m |          -0.0106 |           8.7502 |           3.0025 |
[32m[20230203 20:55:56 @agent_ppo2.py:193][0m |          -0.0112 |           8.6256 |           3.0029 |
[32m[20230203 20:55:57 @agent_ppo2.py:193][0m |          -0.0124 |           8.5389 |           3.0060 |
[32m[20230203 20:55:57 @agent_ppo2.py:193][0m |          -0.0114 |           8.4920 |           3.0056 |
[32m[20230203 20:55:57 @agent_ppo2.py:138][0m Policy update time: 0.58 s
[32m[20230203 20:55:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 153.46
[32m[20230203 20:55:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.25
[32m[20230203 20:55:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.55
[32m[20230203 20:55:57 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 262.55
[32m[20230203 20:55:57 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 262.55
[32m[20230203 20:55:57 @agent_ppo2.py:151][0m Total time:       5.36 min
[32m[20230203 20:55:57 @agent_ppo2.py:153][0m 411648 total steps have happened
[32m[20230203 20:55:57 @agent_ppo2.py:129][0m #------------------------ Iteration 201 --------------------------#
[32m[20230203 20:55:57 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 20:55:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:58 @agent_ppo2.py:193][0m |           0.0001 |          16.9446 |           3.0277 |
[32m[20230203 20:55:58 @agent_ppo2.py:193][0m |          -0.0054 |           9.9922 |           3.0270 |
[32m[20230203 20:55:58 @agent_ppo2.py:193][0m |          -0.0086 |           9.1815 |           3.0234 |
[32m[20230203 20:55:58 @agent_ppo2.py:193][0m |          -0.0082 |           8.8414 |           3.0256 |
[32m[20230203 20:55:58 @agent_ppo2.py:193][0m |          -0.0119 |           8.6216 |           3.0240 |
[32m[20230203 20:55:58 @agent_ppo2.py:193][0m |          -0.0122 |           8.4937 |           3.0229 |
[32m[20230203 20:55:58 @agent_ppo2.py:193][0m |          -0.0123 |           8.3908 |           3.0226 |
[32m[20230203 20:55:58 @agent_ppo2.py:193][0m |          -0.0120 |           8.2293 |           3.0223 |
[32m[20230203 20:55:58 @agent_ppo2.py:193][0m |          -0.0139 |           8.2202 |           3.0219 |
[32m[20230203 20:55:58 @agent_ppo2.py:193][0m |          -0.0159 |           8.1799 |           3.0205 |
[32m[20230203 20:55:58 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:55:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 126.81
[32m[20230203 20:55:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.38
[32m[20230203 20:55:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.02
[32m[20230203 20:55:58 @agent_ppo2.py:151][0m Total time:       5.39 min
[32m[20230203 20:55:58 @agent_ppo2.py:153][0m 413696 total steps have happened
[32m[20230203 20:55:58 @agent_ppo2.py:129][0m #------------------------ Iteration 202 --------------------------#
[32m[20230203 20:55:59 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 20:55:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:55:59 @agent_ppo2.py:193][0m |          -0.0003 |          30.5714 |           3.0528 |
[32m[20230203 20:55:59 @agent_ppo2.py:193][0m |          -0.0048 |          16.9540 |           3.0493 |
[32m[20230203 20:55:59 @agent_ppo2.py:193][0m |          -0.0068 |          13.2282 |           3.0447 |
[32m[20230203 20:55:59 @agent_ppo2.py:193][0m |          -0.0084 |          12.2650 |           3.0410 |
[32m[20230203 20:55:59 @agent_ppo2.py:193][0m |          -0.0090 |          11.8278 |           3.0442 |
[32m[20230203 20:55:59 @agent_ppo2.py:193][0m |          -0.0118 |          10.9311 |           3.0447 |
[32m[20230203 20:55:59 @agent_ppo2.py:193][0m |          -0.0120 |          10.6226 |           3.0413 |
[32m[20230203 20:55:59 @agent_ppo2.py:193][0m |          -0.0131 |          10.1210 |           3.0406 |
[32m[20230203 20:55:59 @agent_ppo2.py:193][0m |          -0.0136 |           9.9316 |           3.0404 |
[32m[20230203 20:55:59 @agent_ppo2.py:193][0m |          -0.0134 |           9.5964 |           3.0392 |
[32m[20230203 20:55:59 @agent_ppo2.py:138][0m Policy update time: 0.44 s
[32m[20230203 20:56:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 127.61
[32m[20230203 20:56:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 237.53
[32m[20230203 20:56:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.41
[32m[20230203 20:56:00 @agent_ppo2.py:151][0m Total time:       5.41 min
[32m[20230203 20:56:00 @agent_ppo2.py:153][0m 415744 total steps have happened
[32m[20230203 20:56:00 @agent_ppo2.py:129][0m #------------------------ Iteration 203 --------------------------#
[32m[20230203 20:56:00 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:56:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:00 @agent_ppo2.py:193][0m |           0.0031 |          63.5926 |           3.0652 |
[32m[20230203 20:56:00 @agent_ppo2.py:193][0m |          -0.0012 |          53.8268 |           3.0712 |
[32m[20230203 20:56:00 @agent_ppo2.py:193][0m |          -0.0047 |          51.1718 |           3.0745 |
[32m[20230203 20:56:00 @agent_ppo2.py:193][0m |          -0.0067 |          49.4966 |           3.0701 |
[32m[20230203 20:56:00 @agent_ppo2.py:193][0m |          -0.0085 |          48.0536 |           3.0709 |
[32m[20230203 20:56:00 @agent_ppo2.py:193][0m |          -0.0089 |          48.0301 |           3.0671 |
[32m[20230203 20:56:00 @agent_ppo2.py:193][0m |          -0.0075 |          45.9157 |           3.0681 |
[32m[20230203 20:56:00 @agent_ppo2.py:193][0m |          -0.0103 |          45.1883 |           3.0701 |
[32m[20230203 20:56:00 @agent_ppo2.py:193][0m |          -0.0115 |          44.9643 |           3.0737 |
[32m[20230203 20:56:00 @agent_ppo2.py:193][0m |          -0.0128 |          44.2830 |           3.0715 |
[32m[20230203 20:56:00 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 20:56:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 67.36
[32m[20230203 20:56:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.93
[32m[20230203 20:56:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 263.86
[32m[20230203 20:56:01 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 263.86
[32m[20230203 20:56:01 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 263.86
[32m[20230203 20:56:01 @agent_ppo2.py:151][0m Total time:       5.42 min
[32m[20230203 20:56:01 @agent_ppo2.py:153][0m 417792 total steps have happened
[32m[20230203 20:56:01 @agent_ppo2.py:129][0m #------------------------ Iteration 204 --------------------------#
[32m[20230203 20:56:01 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:56:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:01 @agent_ppo2.py:193][0m |           0.0012 |           8.8063 |           3.0724 |
[32m[20230203 20:56:01 @agent_ppo2.py:193][0m |          -0.0070 |           7.6242 |           3.0651 |
[32m[20230203 20:56:01 @agent_ppo2.py:193][0m |          -0.0074 |           7.3173 |           3.0640 |
[32m[20230203 20:56:01 @agent_ppo2.py:193][0m |          -0.0089 |           7.1590 |           3.0604 |
[32m[20230203 20:56:01 @agent_ppo2.py:193][0m |          -0.0046 |           7.6010 |           3.0602 |
[32m[20230203 20:56:01 @agent_ppo2.py:193][0m |          -0.0109 |           7.0851 |           3.0628 |
[32m[20230203 20:56:02 @agent_ppo2.py:193][0m |          -0.0112 |           6.9660 |           3.0574 |
[32m[20230203 20:56:02 @agent_ppo2.py:193][0m |          -0.0119 |           6.9227 |           3.0602 |
[32m[20230203 20:56:02 @agent_ppo2.py:193][0m |          -0.0115 |           6.8962 |           3.0548 |
[32m[20230203 20:56:02 @agent_ppo2.py:193][0m |          -0.0137 |           6.8857 |           3.0578 |
[32m[20230203 20:56:02 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 20:56:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 248.78
[32m[20230203 20:56:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.69
[32m[20230203 20:56:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.26
[32m[20230203 20:56:02 @agent_ppo2.py:151][0m Total time:       5.45 min
[32m[20230203 20:56:02 @agent_ppo2.py:153][0m 419840 total steps have happened
[32m[20230203 20:56:02 @agent_ppo2.py:129][0m #------------------------ Iteration 205 --------------------------#
[32m[20230203 20:56:02 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:56:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:03 @agent_ppo2.py:193][0m |          -0.0009 |          25.8878 |           3.0431 |
[32m[20230203 20:56:03 @agent_ppo2.py:193][0m |          -0.0091 |           7.5351 |           3.0391 |
[32m[20230203 20:56:03 @agent_ppo2.py:193][0m |          -0.0124 |           6.7950 |           3.0374 |
[32m[20230203 20:56:03 @agent_ppo2.py:193][0m |          -0.0144 |           6.3170 |           3.0360 |
[32m[20230203 20:56:03 @agent_ppo2.py:193][0m |          -0.0155 |           6.0039 |           3.0335 |
[32m[20230203 20:56:03 @agent_ppo2.py:193][0m |          -0.0154 |           5.7653 |           3.0327 |
[32m[20230203 20:56:03 @agent_ppo2.py:193][0m |          -0.0170 |           5.5448 |           3.0314 |
[32m[20230203 20:56:03 @agent_ppo2.py:193][0m |          -0.0169 |           5.3792 |           3.0297 |
[32m[20230203 20:56:03 @agent_ppo2.py:193][0m |          -0.0177 |           5.2390 |           3.0317 |
[32m[20230203 20:56:03 @agent_ppo2.py:193][0m |          -0.0184 |           5.1257 |           3.0304 |
[32m[20230203 20:56:03 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:56:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 152.90
[32m[20230203 20:56:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 242.47
[32m[20230203 20:56:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.97
[32m[20230203 20:56:03 @agent_ppo2.py:151][0m Total time:       5.47 min
[32m[20230203 20:56:03 @agent_ppo2.py:153][0m 421888 total steps have happened
[32m[20230203 20:56:03 @agent_ppo2.py:129][0m #------------------------ Iteration 206 --------------------------#
[32m[20230203 20:56:04 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:56:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:04 @agent_ppo2.py:193][0m |          -0.0002 |          36.9439 |           3.0937 |
[32m[20230203 20:56:04 @agent_ppo2.py:193][0m |          -0.0094 |          21.7116 |           3.0918 |
[32m[20230203 20:56:04 @agent_ppo2.py:193][0m |          -0.0128 |          18.8415 |           3.0905 |
[32m[20230203 20:56:04 @agent_ppo2.py:193][0m |          -0.0151 |          17.5520 |           3.0877 |
[32m[20230203 20:56:04 @agent_ppo2.py:193][0m |          -0.0168 |          16.8791 |           3.0889 |
[32m[20230203 20:56:04 @agent_ppo2.py:193][0m |          -0.0177 |          15.8937 |           3.0918 |
[32m[20230203 20:56:04 @agent_ppo2.py:193][0m |          -0.0183 |          15.5135 |           3.0916 |
[32m[20230203 20:56:04 @agent_ppo2.py:193][0m |          -0.0191 |          15.1928 |           3.0926 |
[32m[20230203 20:56:04 @agent_ppo2.py:193][0m |          -0.0194 |          15.0876 |           3.0943 |
[32m[20230203 20:56:04 @agent_ppo2.py:193][0m |          -0.0204 |          14.7541 |           3.0936 |
[32m[20230203 20:56:04 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:56:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 59.65
[32m[20230203 20:56:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 239.22
[32m[20230203 20:56:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.39
[32m[20230203 20:56:05 @agent_ppo2.py:151][0m Total time:       5.49 min
[32m[20230203 20:56:05 @agent_ppo2.py:153][0m 423936 total steps have happened
[32m[20230203 20:56:05 @agent_ppo2.py:129][0m #------------------------ Iteration 207 --------------------------#
[32m[20230203 20:56:05 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:56:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:05 @agent_ppo2.py:193][0m |          -0.0026 |          45.8050 |           3.0583 |
[32m[20230203 20:56:05 @agent_ppo2.py:193][0m |          -0.0076 |          33.9882 |           3.0504 |
[32m[20230203 20:56:05 @agent_ppo2.py:193][0m |          -0.0098 |          29.1021 |           3.0474 |
[32m[20230203 20:56:05 @agent_ppo2.py:193][0m |          -0.0115 |          27.5173 |           3.0467 |
[32m[20230203 20:56:05 @agent_ppo2.py:193][0m |          -0.0137 |          25.2938 |           3.0489 |
[32m[20230203 20:56:05 @agent_ppo2.py:193][0m |          -0.0149 |          24.2184 |           3.0458 |
[32m[20230203 20:56:05 @agent_ppo2.py:193][0m |          -0.0135 |          23.0140 |           3.0473 |
[32m[20230203 20:56:05 @agent_ppo2.py:193][0m |          -0.0119 |          22.0180 |           3.0423 |
[32m[20230203 20:56:05 @agent_ppo2.py:193][0m |          -0.0147 |          21.2171 |           3.0455 |
[32m[20230203 20:56:06 @agent_ppo2.py:193][0m |          -0.0180 |          20.7650 |           3.0439 |
[32m[20230203 20:56:06 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:56:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 163.49
[32m[20230203 20:56:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 241.38
[32m[20230203 20:56:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 246.28
[32m[20230203 20:56:06 @agent_ppo2.py:151][0m Total time:       5.51 min
[32m[20230203 20:56:06 @agent_ppo2.py:153][0m 425984 total steps have happened
[32m[20230203 20:56:06 @agent_ppo2.py:129][0m #------------------------ Iteration 208 --------------------------#
[32m[20230203 20:56:06 @agent_ppo2.py:135][0m Sampling time: 0.48 s by 1 slaves
[32m[20230203 20:56:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:07 @agent_ppo2.py:193][0m |           0.0002 |           7.6814 |           3.0623 |
[32m[20230203 20:56:07 @agent_ppo2.py:193][0m |          -0.0066 |           7.0353 |           3.0582 |
[32m[20230203 20:56:07 @agent_ppo2.py:193][0m |          -0.0083 |           6.8367 |           3.0582 |
[32m[20230203 20:56:07 @agent_ppo2.py:193][0m |          -0.0113 |           6.6307 |           3.0570 |
[32m[20230203 20:56:07 @agent_ppo2.py:193][0m |          -0.0112 |           6.5744 |           3.0538 |
[32m[20230203 20:56:07 @agent_ppo2.py:193][0m |          -0.0131 |           6.4511 |           3.0521 |
[32m[20230203 20:56:07 @agent_ppo2.py:193][0m |          -0.0130 |           6.3807 |           3.0515 |
[32m[20230203 20:56:07 @agent_ppo2.py:193][0m |          -0.0125 |           6.3592 |           3.0524 |
[32m[20230203 20:56:07 @agent_ppo2.py:193][0m |          -0.0144 |           6.2673 |           3.0514 |
[32m[20230203 20:56:07 @agent_ppo2.py:193][0m |          -0.0146 |           6.1768 |           3.0474 |
[32m[20230203 20:56:07 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:56:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 239.66
[32m[20230203 20:56:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 241.06
[32m[20230203 20:56:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 249.75
[32m[20230203 20:56:07 @agent_ppo2.py:151][0m Total time:       5.54 min
[32m[20230203 20:56:07 @agent_ppo2.py:153][0m 428032 total steps have happened
[32m[20230203 20:56:07 @agent_ppo2.py:129][0m #------------------------ Iteration 209 --------------------------#
[32m[20230203 20:56:08 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:56:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:08 @agent_ppo2.py:193][0m |           0.0013 |          22.4669 |           3.0774 |
[32m[20230203 20:56:08 @agent_ppo2.py:193][0m |          -0.0004 |          11.4625 |           3.0752 |
[32m[20230203 20:56:08 @agent_ppo2.py:193][0m |          -0.0053 |          10.4377 |           3.0708 |
[32m[20230203 20:56:08 @agent_ppo2.py:193][0m |          -0.0075 |           9.3641 |           3.0716 |
[32m[20230203 20:56:08 @agent_ppo2.py:193][0m |          -0.0091 |           8.5398 |           3.0676 |
[32m[20230203 20:56:08 @agent_ppo2.py:193][0m |          -0.0097 |           7.8802 |           3.0644 |
[32m[20230203 20:56:08 @agent_ppo2.py:193][0m |          -0.0106 |           7.2667 |           3.0641 |
[32m[20230203 20:56:08 @agent_ppo2.py:193][0m |          -0.0114 |           6.8557 |           3.0606 |
[32m[20230203 20:56:08 @agent_ppo2.py:193][0m |          -0.0113 |           6.4319 |           3.0593 |
[32m[20230203 20:56:08 @agent_ppo2.py:193][0m |          -0.0119 |           6.1346 |           3.0583 |
[32m[20230203 20:56:08 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:56:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 158.05
[32m[20230203 20:56:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 238.81
[32m[20230203 20:56:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.62
[32m[20230203 20:56:09 @agent_ppo2.py:151][0m Total time:       5.56 min
[32m[20230203 20:56:09 @agent_ppo2.py:153][0m 430080 total steps have happened
[32m[20230203 20:56:09 @agent_ppo2.py:129][0m #------------------------ Iteration 210 --------------------------#
[32m[20230203 20:56:09 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:56:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:09 @agent_ppo2.py:193][0m |          -0.0010 |           8.9179 |           3.0702 |
[32m[20230203 20:56:09 @agent_ppo2.py:193][0m |          -0.0062 |           7.9770 |           3.0737 |
[32m[20230203 20:56:09 @agent_ppo2.py:193][0m |          -0.0089 |           7.7060 |           3.0720 |
[32m[20230203 20:56:09 @agent_ppo2.py:193][0m |          -0.0122 |           7.4387 |           3.0726 |
[32m[20230203 20:56:10 @agent_ppo2.py:193][0m |          -0.0147 |           7.3340 |           3.0717 |
[32m[20230203 20:56:10 @agent_ppo2.py:193][0m |          -0.0096 |           7.4900 |           3.0740 |
[32m[20230203 20:56:10 @agent_ppo2.py:193][0m |          -0.0146 |           7.2003 |           3.0718 |
[32m[20230203 20:56:10 @agent_ppo2.py:193][0m |          -0.0207 |           7.0557 |           3.0732 |
[32m[20230203 20:56:10 @agent_ppo2.py:193][0m |          -0.0155 |           6.9633 |           3.0769 |
[32m[20230203 20:56:10 @agent_ppo2.py:193][0m |          -0.0152 |           7.0869 |           3.0760 |
[32m[20230203 20:56:10 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:56:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 242.83
[32m[20230203 20:56:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.09
[32m[20230203 20:56:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.79
[32m[20230203 20:56:10 @agent_ppo2.py:151][0m Total time:       5.58 min
[32m[20230203 20:56:10 @agent_ppo2.py:153][0m 432128 total steps have happened
[32m[20230203 20:56:10 @agent_ppo2.py:129][0m #------------------------ Iteration 211 --------------------------#
[32m[20230203 20:56:11 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:56:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:11 @agent_ppo2.py:193][0m |          -0.0031 |           7.3540 |           3.0076 |
[32m[20230203 20:56:11 @agent_ppo2.py:193][0m |          -0.0087 |           6.8105 |           3.0005 |
[32m[20230203 20:56:11 @agent_ppo2.py:193][0m |          -0.0061 |           6.5949 |           3.0005 |
[32m[20230203 20:56:11 @agent_ppo2.py:193][0m |          -0.0113 |           6.4386 |           2.9994 |
[32m[20230203 20:56:11 @agent_ppo2.py:193][0m |          -0.0125 |           6.3304 |           2.9974 |
[32m[20230203 20:56:11 @agent_ppo2.py:193][0m |          -0.0144 |           6.2173 |           2.9975 |
[32m[20230203 20:56:11 @agent_ppo2.py:193][0m |          -0.0133 |           6.1954 |           2.9983 |
[32m[20230203 20:56:11 @agent_ppo2.py:193][0m |          -0.0162 |           6.0752 |           2.9997 |
[32m[20230203 20:56:11 @agent_ppo2.py:193][0m |          -0.0157 |           6.0085 |           3.0006 |
[32m[20230203 20:56:11 @agent_ppo2.py:193][0m |          -0.0119 |           6.2297 |           2.9997 |
[32m[20230203 20:56:11 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:56:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 242.86
[32m[20230203 20:56:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 243.76
[32m[20230203 20:56:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.71
[32m[20230203 20:56:12 @agent_ppo2.py:151][0m Total time:       5.61 min
[32m[20230203 20:56:12 @agent_ppo2.py:153][0m 434176 total steps have happened
[32m[20230203 20:56:12 @agent_ppo2.py:129][0m #------------------------ Iteration 212 --------------------------#
[32m[20230203 20:56:12 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:56:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:12 @agent_ppo2.py:193][0m |          -0.0031 |           7.9942 |           3.0994 |
[32m[20230203 20:56:12 @agent_ppo2.py:193][0m |          -0.0073 |           7.6655 |           3.0954 |
[32m[20230203 20:56:12 @agent_ppo2.py:193][0m |          -0.0083 |           7.5919 |           3.0839 |
[32m[20230203 20:56:12 @agent_ppo2.py:193][0m |          -0.0096 |           7.4743 |           3.0845 |
[32m[20230203 20:56:12 @agent_ppo2.py:193][0m |          -0.0111 |           7.4173 |           3.0840 |
[32m[20230203 20:56:12 @agent_ppo2.py:193][0m |          -0.0108 |           7.3960 |           3.0872 |
[32m[20230203 20:56:12 @agent_ppo2.py:193][0m |          -0.0122 |           7.3357 |           3.0831 |
[32m[20230203 20:56:12 @agent_ppo2.py:193][0m |          -0.0108 |           7.3336 |           3.0858 |
[32m[20230203 20:56:12 @agent_ppo2.py:193][0m |          -0.0121 |           7.2982 |           3.0841 |
[32m[20230203 20:56:13 @agent_ppo2.py:193][0m |          -0.0125 |           7.2798 |           3.0837 |
[32m[20230203 20:56:13 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 20:56:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 242.88
[32m[20230203 20:56:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.07
[32m[20230203 20:56:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.79
[32m[20230203 20:56:13 @agent_ppo2.py:151][0m Total time:       5.63 min
[32m[20230203 20:56:13 @agent_ppo2.py:153][0m 436224 total steps have happened
[32m[20230203 20:56:13 @agent_ppo2.py:129][0m #------------------------ Iteration 213 --------------------------#
[32m[20230203 20:56:13 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:56:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:13 @agent_ppo2.py:193][0m |          -0.0019 |           7.3233 |           3.1062 |
[32m[20230203 20:56:13 @agent_ppo2.py:193][0m |          -0.0038 |           7.1394 |           3.1037 |
[32m[20230203 20:56:14 @agent_ppo2.py:193][0m |          -0.0067 |           7.1017 |           3.0998 |
[32m[20230203 20:56:14 @agent_ppo2.py:193][0m |          -0.0091 |           6.9707 |           3.0974 |
[32m[20230203 20:56:14 @agent_ppo2.py:193][0m |          -0.0100 |           6.9254 |           3.0992 |
[32m[20230203 20:56:14 @agent_ppo2.py:193][0m |          -0.0108 |           6.9599 |           3.0925 |
[32m[20230203 20:56:14 @agent_ppo2.py:193][0m |          -0.0110 |           6.8388 |           3.0950 |
[32m[20230203 20:56:14 @agent_ppo2.py:193][0m |          -0.0140 |           6.8034 |           3.0915 |
[32m[20230203 20:56:14 @agent_ppo2.py:193][0m |          -0.0121 |           6.8274 |           3.0899 |
[32m[20230203 20:56:14 @agent_ppo2.py:193][0m |          -0.0130 |           6.7718 |           3.0907 |
[32m[20230203 20:56:14 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:56:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 244.90
[32m[20230203 20:56:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.72
[32m[20230203 20:56:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 161.51
[32m[20230203 20:56:14 @agent_ppo2.py:151][0m Total time:       5.65 min
[32m[20230203 20:56:14 @agent_ppo2.py:153][0m 438272 total steps have happened
[32m[20230203 20:56:14 @agent_ppo2.py:129][0m #------------------------ Iteration 214 --------------------------#
[32m[20230203 20:56:15 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:56:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:15 @agent_ppo2.py:193][0m |           0.0022 |           7.5416 |           3.0568 |
[32m[20230203 20:56:15 @agent_ppo2.py:193][0m |          -0.0046 |           6.9065 |           3.0554 |
[32m[20230203 20:56:15 @agent_ppo2.py:193][0m |           0.0025 |           7.2344 |           3.0474 |
[32m[20230203 20:56:15 @agent_ppo2.py:193][0m |          -0.0061 |           6.4524 |           3.0475 |
[32m[20230203 20:56:15 @agent_ppo2.py:193][0m |          -0.0103 |           6.2620 |           3.0464 |
[32m[20230203 20:56:15 @agent_ppo2.py:193][0m |          -0.0087 |           6.1437 |           3.0442 |
[32m[20230203 20:56:15 @agent_ppo2.py:193][0m |          -0.0095 |           5.8511 |           3.0424 |
[32m[20230203 20:56:15 @agent_ppo2.py:193][0m |          -0.0105 |           5.5511 |           3.0424 |
[32m[20230203 20:56:15 @agent_ppo2.py:193][0m |          -0.0116 |           5.2565 |           3.0395 |
[32m[20230203 20:56:15 @agent_ppo2.py:193][0m |          -0.0114 |           4.9821 |           3.0400 |
[32m[20230203 20:56:15 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:56:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 242.26
[32m[20230203 20:56:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 243.49
[32m[20230203 20:56:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.82
[32m[20230203 20:56:16 @agent_ppo2.py:151][0m Total time:       5.67 min
[32m[20230203 20:56:16 @agent_ppo2.py:153][0m 440320 total steps have happened
[32m[20230203 20:56:16 @agent_ppo2.py:129][0m #------------------------ Iteration 215 --------------------------#
[32m[20230203 20:56:16 @agent_ppo2.py:135][0m Sampling time: 0.51 s by 1 slaves
[32m[20230203 20:56:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:16 @agent_ppo2.py:193][0m |          -0.0006 |          19.2328 |           3.1579 |
[32m[20230203 20:56:16 @agent_ppo2.py:193][0m |          -0.0064 |           9.3455 |           3.1452 |
[32m[20230203 20:56:16 @agent_ppo2.py:193][0m |          -0.0080 |           8.5639 |           3.1455 |
[32m[20230203 20:56:16 @agent_ppo2.py:193][0m |          -0.0086 |           8.3962 |           3.1417 |
[32m[20230203 20:56:16 @agent_ppo2.py:193][0m |          -0.0087 |           8.1326 |           3.1438 |
[32m[20230203 20:56:16 @agent_ppo2.py:193][0m |          -0.0096 |           7.9792 |           3.1394 |
[32m[20230203 20:56:16 @agent_ppo2.py:193][0m |          -0.0107 |           7.8871 |           3.1373 |
[32m[20230203 20:56:17 @agent_ppo2.py:193][0m |          -0.0108 |           7.8329 |           3.1407 |
[32m[20230203 20:56:17 @agent_ppo2.py:193][0m |          -0.0112 |           7.6949 |           3.1359 |
[32m[20230203 20:56:17 @agent_ppo2.py:193][0m |          -0.0109 |           7.6490 |           3.1375 |
[32m[20230203 20:56:17 @agent_ppo2.py:138][0m Policy update time: 0.58 s
[32m[20230203 20:56:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 152.25
[32m[20230203 20:56:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.32
[32m[20230203 20:56:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.04
[32m[20230203 20:56:17 @agent_ppo2.py:151][0m Total time:       5.70 min
[32m[20230203 20:56:17 @agent_ppo2.py:153][0m 442368 total steps have happened
[32m[20230203 20:56:17 @agent_ppo2.py:129][0m #------------------------ Iteration 216 --------------------------#
[32m[20230203 20:56:17 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:56:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:18 @agent_ppo2.py:193][0m |           0.0062 |           9.4397 |           2.9802 |
[32m[20230203 20:56:18 @agent_ppo2.py:193][0m |          -0.0084 |           8.3632 |           2.9807 |
[32m[20230203 20:56:18 @agent_ppo2.py:193][0m |          -0.0069 |           8.1496 |           2.9815 |
[32m[20230203 20:56:18 @agent_ppo2.py:193][0m |          -0.0112 |           7.9680 |           2.9776 |
[32m[20230203 20:56:18 @agent_ppo2.py:193][0m |          -0.0147 |           7.8780 |           2.9794 |
[32m[20230203 20:56:18 @agent_ppo2.py:193][0m |          -0.0044 |           8.5161 |           2.9808 |
[32m[20230203 20:56:18 @agent_ppo2.py:193][0m |          -0.0125 |           7.7576 |           2.9787 |
[32m[20230203 20:56:18 @agent_ppo2.py:193][0m |          -0.0116 |           7.6161 |           2.9801 |
[32m[20230203 20:56:18 @agent_ppo2.py:193][0m |          -0.0099 |           7.5581 |           2.9816 |
[32m[20230203 20:56:18 @agent_ppo2.py:193][0m |          -0.0166 |           7.4936 |           2.9792 |
[32m[20230203 20:56:18 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:56:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.15
[32m[20230203 20:56:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.45
[32m[20230203 20:56:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.42
[32m[20230203 20:56:18 @agent_ppo2.py:151][0m Total time:       5.72 min
[32m[20230203 20:56:18 @agent_ppo2.py:153][0m 444416 total steps have happened
[32m[20230203 20:56:18 @agent_ppo2.py:129][0m #------------------------ Iteration 217 --------------------------#
[32m[20230203 20:56:19 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:56:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:19 @agent_ppo2.py:193][0m |          -0.0009 |          37.1025 |           3.1029 |
[32m[20230203 20:56:19 @agent_ppo2.py:193][0m |          -0.0179 |          17.9166 |           3.1013 |
[32m[20230203 20:56:19 @agent_ppo2.py:193][0m |          -0.0010 |          15.4248 |           3.0994 |
[32m[20230203 20:56:19 @agent_ppo2.py:193][0m |          -0.0129 |          14.4898 |           3.1013 |
[32m[20230203 20:56:19 @agent_ppo2.py:193][0m |          -0.0221 |          13.0338 |           3.0984 |
[32m[20230203 20:56:19 @agent_ppo2.py:193][0m |          -0.0248 |          12.5459 |           3.0991 |
[32m[20230203 20:56:19 @agent_ppo2.py:193][0m |          -0.0234 |          12.5594 |           3.0977 |
[32m[20230203 20:56:19 @agent_ppo2.py:193][0m |          -0.0100 |          13.4893 |           3.0966 |
[32m[20230203 20:56:19 @agent_ppo2.py:193][0m |          -0.0238 |          11.8704 |           3.0976 |
[32m[20230203 20:56:19 @agent_ppo2.py:193][0m |          -0.0180 |          11.3480 |           3.0958 |
[32m[20230203 20:56:19 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 20:56:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 56.52
[32m[20230203 20:56:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.37
[32m[20230203 20:56:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.15
[32m[20230203 20:56:19 @agent_ppo2.py:151][0m Total time:       5.74 min
[32m[20230203 20:56:19 @agent_ppo2.py:153][0m 446464 total steps have happened
[32m[20230203 20:56:19 @agent_ppo2.py:129][0m #------------------------ Iteration 218 --------------------------#
[32m[20230203 20:56:20 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 20:56:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:20 @agent_ppo2.py:193][0m |           0.0176 |          53.9783 |           3.0532 |
[32m[20230203 20:56:20 @agent_ppo2.py:193][0m |          -0.0127 |          45.7594 |           3.0511 |
[32m[20230203 20:56:20 @agent_ppo2.py:193][0m |           0.0015 |          38.8254 |           3.0492 |
[32m[20230203 20:56:20 @agent_ppo2.py:193][0m |          -0.0037 |          34.7835 |           3.0528 |
[32m[20230203 20:56:20 @agent_ppo2.py:193][0m |           0.0309 |          36.5415 |           3.0517 |
[32m[20230203 20:56:20 @agent_ppo2.py:193][0m |           0.0139 |          34.9176 |           3.0383 |
[32m[20230203 20:56:20 @agent_ppo2.py:193][0m |           0.0018 |          28.1989 |           3.0476 |
[32m[20230203 20:56:20 @agent_ppo2.py:193][0m |          -0.0186 |          26.7153 |           3.0450 |
[32m[20230203 20:56:20 @agent_ppo2.py:193][0m |           0.0027 |          28.7956 |           3.0497 |
[32m[20230203 20:56:20 @agent_ppo2.py:193][0m |          -0.0261 |          24.8713 |           3.0465 |
[32m[20230203 20:56:20 @agent_ppo2.py:138][0m Policy update time: 0.44 s
[32m[20230203 20:56:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 91.08
[32m[20230203 20:56:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 130.84
[32m[20230203 20:56:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.16
[32m[20230203 20:56:21 @agent_ppo2.py:151][0m Total time:       5.76 min
[32m[20230203 20:56:21 @agent_ppo2.py:153][0m 448512 total steps have happened
[32m[20230203 20:56:21 @agent_ppo2.py:129][0m #------------------------ Iteration 219 --------------------------#
[32m[20230203 20:56:21 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:56:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:21 @agent_ppo2.py:193][0m |           0.0055 |          14.5755 |           3.0802 |
[32m[20230203 20:56:21 @agent_ppo2.py:193][0m |          -0.0038 |          12.8438 |           3.0801 |
[32m[20230203 20:56:21 @agent_ppo2.py:193][0m |          -0.0073 |          12.4044 |           3.0825 |
[32m[20230203 20:56:21 @agent_ppo2.py:193][0m |          -0.0063 |          12.1670 |           3.0814 |
[32m[20230203 20:56:21 @agent_ppo2.py:193][0m |          -0.0035 |          12.1607 |           3.0794 |
[32m[20230203 20:56:21 @agent_ppo2.py:193][0m |          -0.0099 |          11.6478 |           3.0803 |
[32m[20230203 20:56:22 @agent_ppo2.py:193][0m |          -0.0096 |          11.4333 |           3.0818 |
[32m[20230203 20:56:22 @agent_ppo2.py:193][0m |          -0.0105 |          11.3454 |           3.0820 |
[32m[20230203 20:56:22 @agent_ppo2.py:193][0m |          -0.0150 |          11.2978 |           3.0793 |
[32m[20230203 20:56:22 @agent_ppo2.py:193][0m |          -0.0157 |          11.0870 |           3.0804 |
[32m[20230203 20:56:22 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:56:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.90
[32m[20230203 20:56:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.37
[32m[20230203 20:56:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.30
[32m[20230203 20:56:22 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 264.30
[32m[20230203 20:56:22 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 264.30
[32m[20230203 20:56:22 @agent_ppo2.py:151][0m Total time:       5.78 min
[32m[20230203 20:56:22 @agent_ppo2.py:153][0m 450560 total steps have happened
[32m[20230203 20:56:22 @agent_ppo2.py:129][0m #------------------------ Iteration 220 --------------------------#
[32m[20230203 20:56:22 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:56:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:23 @agent_ppo2.py:193][0m |          -0.0056 |           9.1988 |           3.0135 |
[32m[20230203 20:56:23 @agent_ppo2.py:193][0m |          -0.0100 |           8.6977 |           3.0112 |
[32m[20230203 20:56:23 @agent_ppo2.py:193][0m |           0.0013 |           9.2267 |           3.0164 |
[32m[20230203 20:56:23 @agent_ppo2.py:193][0m |          -0.0141 |           8.2026 |           3.0152 |
[32m[20230203 20:56:23 @agent_ppo2.py:193][0m |          -0.0128 |           8.1563 |           3.0139 |
[32m[20230203 20:56:23 @agent_ppo2.py:193][0m |          -0.0018 |           8.1605 |           3.0122 |
[32m[20230203 20:56:23 @agent_ppo2.py:193][0m |          -0.0127 |           7.7907 |           3.0151 |
[32m[20230203 20:56:23 @agent_ppo2.py:193][0m |          -0.0162 |           7.6576 |           3.0056 |
[32m[20230203 20:56:23 @agent_ppo2.py:193][0m |          -0.0143 |           7.5168 |           3.0113 |
[32m[20230203 20:56:23 @agent_ppo2.py:193][0m |          -0.0072 |           7.8002 |           3.0089 |
[32m[20230203 20:56:23 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:56:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: 240.98
[32m[20230203 20:56:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.44
[32m[20230203 20:56:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.03
[32m[20230203 20:56:23 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 265.03
[32m[20230203 20:56:23 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 265.03
[32m[20230203 20:56:23 @agent_ppo2.py:151][0m Total time:       5.80 min
[32m[20230203 20:56:23 @agent_ppo2.py:153][0m 452608 total steps have happened
[32m[20230203 20:56:23 @agent_ppo2.py:129][0m #------------------------ Iteration 221 --------------------------#
[32m[20230203 20:56:24 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:56:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:24 @agent_ppo2.py:193][0m |           0.0013 |           9.9596 |           3.0827 |
[32m[20230203 20:56:24 @agent_ppo2.py:193][0m |          -0.0041 |           9.2546 |           3.0810 |
[32m[20230203 20:56:24 @agent_ppo2.py:193][0m |          -0.0053 |           8.8586 |           3.0797 |
[32m[20230203 20:56:24 @agent_ppo2.py:193][0m |          -0.0125 |           8.6612 |           3.0760 |
[32m[20230203 20:56:24 @agent_ppo2.py:193][0m |          -0.0112 |           8.5315 |           3.0761 |
[32m[20230203 20:56:24 @agent_ppo2.py:193][0m |          -0.0115 |           8.4349 |           3.0733 |
[32m[20230203 20:56:24 @agent_ppo2.py:193][0m |          -0.0086 |           8.8314 |           3.0723 |
[32m[20230203 20:56:24 @agent_ppo2.py:193][0m |          -0.0116 |           8.3039 |           3.0717 |
[32m[20230203 20:56:24 @agent_ppo2.py:193][0m |          -0.0156 |           8.2431 |           3.0716 |
[32m[20230203 20:56:24 @agent_ppo2.py:193][0m |          -0.0151 |           8.1563 |           3.0725 |
[32m[20230203 20:56:24 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:56:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.23
[32m[20230203 20:56:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.97
[32m[20230203 20:56:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 263.41
[32m[20230203 20:56:25 @agent_ppo2.py:151][0m Total time:       5.82 min
[32m[20230203 20:56:25 @agent_ppo2.py:153][0m 454656 total steps have happened
[32m[20230203 20:56:25 @agent_ppo2.py:129][0m #------------------------ Iteration 222 --------------------------#
[32m[20230203 20:56:25 @agent_ppo2.py:135][0m Sampling time: 0.53 s by 1 slaves
[32m[20230203 20:56:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:25 @agent_ppo2.py:193][0m |          -0.0013 |          28.7830 |           3.0905 |
[32m[20230203 20:56:25 @agent_ppo2.py:193][0m |          -0.0098 |          24.6759 |           3.0849 |
[32m[20230203 20:56:25 @agent_ppo2.py:193][0m |          -0.0098 |          23.0163 |           3.0875 |
[32m[20230203 20:56:25 @agent_ppo2.py:193][0m |          -0.0123 |          22.1607 |           3.0884 |
[32m[20230203 20:56:26 @agent_ppo2.py:193][0m |          -0.0016 |          21.3782 |           3.0911 |
[32m[20230203 20:56:26 @agent_ppo2.py:193][0m |          -0.0139 |          20.6218 |           3.0905 |
[32m[20230203 20:56:26 @agent_ppo2.py:193][0m |          -0.0137 |          20.2421 |           3.0906 |
[32m[20230203 20:56:26 @agent_ppo2.py:193][0m |          -0.0162 |          19.8198 |           3.0934 |
[32m[20230203 20:56:26 @agent_ppo2.py:193][0m |          -0.0161 |          19.4632 |           3.0908 |
[32m[20230203 20:56:26 @agent_ppo2.py:193][0m |          -0.0141 |          19.2238 |           3.0919 |
[32m[20230203 20:56:26 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:56:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 169.92
[32m[20230203 20:56:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.93
[32m[20230203 20:56:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 158.32
[32m[20230203 20:56:26 @agent_ppo2.py:151][0m Total time:       5.85 min
[32m[20230203 20:56:26 @agent_ppo2.py:153][0m 456704 total steps have happened
[32m[20230203 20:56:26 @agent_ppo2.py:129][0m #------------------------ Iteration 223 --------------------------#
[32m[20230203 20:56:27 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:56:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:27 @agent_ppo2.py:193][0m |          -0.0020 |           9.9467 |           3.0818 |
[32m[20230203 20:56:27 @agent_ppo2.py:193][0m |          -0.0133 |           9.1445 |           3.0776 |
[32m[20230203 20:56:27 @agent_ppo2.py:193][0m |          -0.0004 |           9.3541 |           3.0740 |
[32m[20230203 20:56:27 @agent_ppo2.py:193][0m |          -0.0138 |           8.8391 |           3.0729 |
[32m[20230203 20:56:27 @agent_ppo2.py:193][0m |          -0.0114 |           8.7531 |           3.0710 |
[32m[20230203 20:56:27 @agent_ppo2.py:193][0m |          -0.0076 |           8.6936 |           3.0697 |
[32m[20230203 20:56:27 @agent_ppo2.py:193][0m |          -0.0145 |           8.5851 |           3.0675 |
[32m[20230203 20:56:27 @agent_ppo2.py:193][0m |          -0.0123 |           8.5446 |           3.0683 |
[32m[20230203 20:56:27 @agent_ppo2.py:193][0m |          -0.0166 |           8.5183 |           3.0688 |
[32m[20230203 20:56:27 @agent_ppo2.py:193][0m |          -0.0017 |           9.0689 |           3.0680 |
[32m[20230203 20:56:27 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:56:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.83
[32m[20230203 20:56:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.79
[32m[20230203 20:56:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 195.46
[32m[20230203 20:56:28 @agent_ppo2.py:151][0m Total time:       5.87 min
[32m[20230203 20:56:28 @agent_ppo2.py:153][0m 458752 total steps have happened
[32m[20230203 20:56:28 @agent_ppo2.py:129][0m #------------------------ Iteration 224 --------------------------#
[32m[20230203 20:56:28 @agent_ppo2.py:135][0m Sampling time: 0.50 s by 1 slaves
[32m[20230203 20:56:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:28 @agent_ppo2.py:193][0m |          -0.0006 |          37.9712 |           3.0185 |
[32m[20230203 20:56:28 @agent_ppo2.py:193][0m |          -0.0067 |          25.7832 |           3.0159 |
[32m[20230203 20:56:28 @agent_ppo2.py:193][0m |          -0.0085 |          23.8960 |           3.0146 |
[32m[20230203 20:56:28 @agent_ppo2.py:193][0m |          -0.0105 |          22.4227 |           3.0121 |
[32m[20230203 20:56:28 @agent_ppo2.py:193][0m |          -0.0121 |          21.5718 |           3.0125 |
[32m[20230203 20:56:28 @agent_ppo2.py:193][0m |          -0.0119 |          21.7122 |           3.0131 |
[32m[20230203 20:56:28 @agent_ppo2.py:193][0m |          -0.0137 |          20.6780 |           3.0126 |
[32m[20230203 20:56:29 @agent_ppo2.py:193][0m |          -0.0142 |          20.1588 |           3.0097 |
[32m[20230203 20:56:29 @agent_ppo2.py:193][0m |          -0.0137 |          19.9689 |           3.0106 |
[32m[20230203 20:56:29 @agent_ppo2.py:193][0m |          -0.0147 |          19.5845 |           3.0116 |
[32m[20230203 20:56:29 @agent_ppo2.py:138][0m Policy update time: 0.57 s
[32m[20230203 20:56:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 104.75
[32m[20230203 20:56:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 237.64
[32m[20230203 20:56:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.70
[32m[20230203 20:56:29 @agent_ppo2.py:151][0m Total time:       5.90 min
[32m[20230203 20:56:29 @agent_ppo2.py:153][0m 460800 total steps have happened
[32m[20230203 20:56:29 @agent_ppo2.py:129][0m #------------------------ Iteration 225 --------------------------#
[32m[20230203 20:56:29 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:56:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:30 @agent_ppo2.py:193][0m |           0.0011 |          11.1588 |           3.1296 |
[32m[20230203 20:56:30 @agent_ppo2.py:193][0m |          -0.0112 |           9.2851 |           3.1249 |
[32m[20230203 20:56:30 @agent_ppo2.py:193][0m |          -0.0040 |           9.2154 |           3.1229 |
[32m[20230203 20:56:30 @agent_ppo2.py:193][0m |          -0.0089 |           8.9545 |           3.1258 |
[32m[20230203 20:56:30 @agent_ppo2.py:193][0m |          -0.0059 |           9.0036 |           3.1252 |
[32m[20230203 20:56:30 @agent_ppo2.py:193][0m |          -0.0076 |           8.8256 |           3.1269 |
[32m[20230203 20:56:30 @agent_ppo2.py:193][0m |          -0.0107 |           8.6754 |           3.1261 |
[32m[20230203 20:56:30 @agent_ppo2.py:193][0m |          -0.0111 |           8.7373 |           3.1276 |
[32m[20230203 20:56:30 @agent_ppo2.py:193][0m |          -0.0136 |           8.5539 |           3.1259 |
[32m[20230203 20:56:30 @agent_ppo2.py:193][0m |          -0.0108 |           8.5014 |           3.1322 |
[32m[20230203 20:56:30 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:56:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.12
[32m[20230203 20:56:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.73
[32m[20230203 20:56:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.77
[32m[20230203 20:56:30 @agent_ppo2.py:151][0m Total time:       5.92 min
[32m[20230203 20:56:30 @agent_ppo2.py:153][0m 462848 total steps have happened
[32m[20230203 20:56:30 @agent_ppo2.py:129][0m #------------------------ Iteration 226 --------------------------#
[32m[20230203 20:56:31 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:56:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:31 @agent_ppo2.py:193][0m |           0.0010 |          22.7710 |           3.0724 |
[32m[20230203 20:56:31 @agent_ppo2.py:193][0m |          -0.0025 |          16.7742 |           3.0638 |
[32m[20230203 20:56:31 @agent_ppo2.py:193][0m |          -0.0060 |          15.4012 |           3.0698 |
[32m[20230203 20:56:31 @agent_ppo2.py:193][0m |          -0.0054 |          14.5052 |           3.0653 |
[32m[20230203 20:56:31 @agent_ppo2.py:193][0m |          -0.0034 |          13.7303 |           3.0628 |
[32m[20230203 20:56:31 @agent_ppo2.py:193][0m |          -0.0085 |          13.3091 |           3.0647 |
[32m[20230203 20:56:31 @agent_ppo2.py:193][0m |          -0.0125 |          12.4951 |           3.0637 |
[32m[20230203 20:56:31 @agent_ppo2.py:193][0m |          -0.0126 |          12.3734 |           3.0618 |
[32m[20230203 20:56:31 @agent_ppo2.py:193][0m |          -0.0038 |          12.0486 |           3.0632 |
[32m[20230203 20:56:31 @agent_ppo2.py:193][0m |          -0.0125 |          11.6043 |           3.0591 |
[32m[20230203 20:56:31 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:56:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 127.25
[32m[20230203 20:56:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.93
[32m[20230203 20:56:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.30
[32m[20230203 20:56:32 @agent_ppo2.py:151][0m Total time:       5.94 min
[32m[20230203 20:56:32 @agent_ppo2.py:153][0m 464896 total steps have happened
[32m[20230203 20:56:32 @agent_ppo2.py:129][0m #------------------------ Iteration 227 --------------------------#
[32m[20230203 20:56:32 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:56:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:32 @agent_ppo2.py:193][0m |           0.0023 |          10.4535 |           3.1653 |
[32m[20230203 20:56:32 @agent_ppo2.py:193][0m |          -0.0028 |           9.2802 |           3.1671 |
[32m[20230203 20:56:32 @agent_ppo2.py:193][0m |           0.0005 |           9.2967 |           3.1694 |
[32m[20230203 20:56:32 @agent_ppo2.py:193][0m |          -0.0001 |           9.1297 |           3.1691 |
[32m[20230203 20:56:32 @agent_ppo2.py:193][0m |          -0.0064 |           8.8122 |           3.1690 |
[32m[20230203 20:56:32 @agent_ppo2.py:193][0m |          -0.0097 |           8.4884 |           3.1671 |
[32m[20230203 20:56:32 @agent_ppo2.py:193][0m |          -0.0060 |           8.6613 |           3.1705 |
[32m[20230203 20:56:33 @agent_ppo2.py:193][0m |          -0.0075 |           8.3794 |           3.1683 |
[32m[20230203 20:56:33 @agent_ppo2.py:193][0m |          -0.0050 |           8.4916 |           3.1700 |
[32m[20230203 20:56:33 @agent_ppo2.py:193][0m |          -0.0085 |           8.2896 |           3.1725 |
[32m[20230203 20:56:33 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:56:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.82
[32m[20230203 20:56:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.68
[32m[20230203 20:56:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 260.75
[32m[20230203 20:56:33 @agent_ppo2.py:151][0m Total time:       5.96 min
[32m[20230203 20:56:33 @agent_ppo2.py:153][0m 466944 total steps have happened
[32m[20230203 20:56:33 @agent_ppo2.py:129][0m #------------------------ Iteration 228 --------------------------#
[32m[20230203 20:56:33 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:56:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:33 @agent_ppo2.py:193][0m |           0.0015 |          19.1379 |           3.1300 |
[32m[20230203 20:56:34 @agent_ppo2.py:193][0m |          -0.0023 |          10.8173 |           3.1329 |
[32m[20230203 20:56:34 @agent_ppo2.py:193][0m |          -0.0064 |           9.9656 |           3.1346 |
[32m[20230203 20:56:34 @agent_ppo2.py:193][0m |          -0.0075 |           9.5647 |           3.1336 |
[32m[20230203 20:56:34 @agent_ppo2.py:193][0m |          -0.0095 |           9.2920 |           3.1309 |
[32m[20230203 20:56:34 @agent_ppo2.py:193][0m |          -0.0105 |           9.0630 |           3.1318 |
[32m[20230203 20:56:34 @agent_ppo2.py:193][0m |          -0.0112 |           8.9472 |           3.1296 |
[32m[20230203 20:56:34 @agent_ppo2.py:193][0m |          -0.0120 |           8.6938 |           3.1310 |
[32m[20230203 20:56:34 @agent_ppo2.py:193][0m |          -0.0111 |           8.5459 |           3.1334 |
[32m[20230203 20:56:34 @agent_ppo2.py:193][0m |          -0.0129 |           8.4011 |           3.1308 |
[32m[20230203 20:56:34 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 20:56:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 158.60
[32m[20230203 20:56:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.32
[32m[20230203 20:56:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.05
[32m[20230203 20:56:34 @agent_ppo2.py:151][0m Total time:       5.98 min
[32m[20230203 20:56:34 @agent_ppo2.py:153][0m 468992 total steps have happened
[32m[20230203 20:56:34 @agent_ppo2.py:129][0m #------------------------ Iteration 229 --------------------------#
[32m[20230203 20:56:35 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:56:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:35 @agent_ppo2.py:193][0m |          -0.0012 |          26.7556 |           3.1128 |
[32m[20230203 20:56:35 @agent_ppo2.py:193][0m |          -0.0049 |          19.1227 |           3.1037 |
[32m[20230203 20:56:35 @agent_ppo2.py:193][0m |          -0.0019 |          17.9930 |           3.0999 |
[32m[20230203 20:56:35 @agent_ppo2.py:193][0m |          -0.0090 |          17.0841 |           3.0984 |
[32m[20230203 20:56:35 @agent_ppo2.py:193][0m |          -0.0027 |          17.2769 |           3.0923 |
[32m[20230203 20:56:35 @agent_ppo2.py:193][0m |          -0.0096 |          16.4695 |           3.0906 |
[32m[20230203 20:56:35 @agent_ppo2.py:193][0m |          -0.0110 |          16.1118 |           3.0912 |
[32m[20230203 20:56:35 @agent_ppo2.py:193][0m |          -0.0131 |          15.8489 |           3.0884 |
[32m[20230203 20:56:35 @agent_ppo2.py:193][0m |          -0.0137 |          15.7118 |           3.0900 |
[32m[20230203 20:56:35 @agent_ppo2.py:193][0m |          -0.0125 |          15.5498 |           3.0874 |
[32m[20230203 20:56:35 @agent_ppo2.py:138][0m Policy update time: 0.56 s
[32m[20230203 20:56:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 144.33
[32m[20230203 20:56:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.86
[32m[20230203 20:56:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 185.31
[32m[20230203 20:56:36 @agent_ppo2.py:151][0m Total time:       6.01 min
[32m[20230203 20:56:36 @agent_ppo2.py:153][0m 471040 total steps have happened
[32m[20230203 20:56:36 @agent_ppo2.py:129][0m #------------------------ Iteration 230 --------------------------#
[32m[20230203 20:56:36 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:56:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:36 @agent_ppo2.py:193][0m |          -0.0035 |          16.9290 |           3.1964 |
[32m[20230203 20:56:36 @agent_ppo2.py:193][0m |          -0.0084 |          14.0266 |           3.1906 |
[32m[20230203 20:56:36 @agent_ppo2.py:193][0m |          -0.0100 |          13.5397 |           3.1903 |
[32m[20230203 20:56:36 @agent_ppo2.py:193][0m |          -0.0118 |          13.0044 |           3.1859 |
[32m[20230203 20:56:36 @agent_ppo2.py:193][0m |          -0.0102 |          12.7062 |           3.1852 |
[32m[20230203 20:56:36 @agent_ppo2.py:193][0m |          -0.0119 |          12.4602 |           3.1864 |
[32m[20230203 20:56:37 @agent_ppo2.py:193][0m |          -0.0130 |          12.1229 |           3.1847 |
[32m[20230203 20:56:37 @agent_ppo2.py:193][0m |          -0.0156 |          11.8889 |           3.1831 |
[32m[20230203 20:56:37 @agent_ppo2.py:193][0m |          -0.0140 |          11.5714 |           3.1839 |
[32m[20230203 20:56:37 @agent_ppo2.py:193][0m |          -0.0155 |          11.3947 |           3.1865 |
[32m[20230203 20:56:37 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:56:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.49
[32m[20230203 20:56:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.93
[32m[20230203 20:56:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.36
[32m[20230203 20:56:37 @agent_ppo2.py:151][0m Total time:       6.03 min
[32m[20230203 20:56:37 @agent_ppo2.py:153][0m 473088 total steps have happened
[32m[20230203 20:56:37 @agent_ppo2.py:129][0m #------------------------ Iteration 231 --------------------------#
[32m[20230203 20:56:37 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 1 slaves
[32m[20230203 20:56:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:37 @agent_ppo2.py:193][0m |           0.0015 |          24.4451 |           3.1265 |
[32m[20230203 20:56:38 @agent_ppo2.py:193][0m |          -0.0002 |          12.8027 |           3.1279 |
[32m[20230203 20:56:38 @agent_ppo2.py:193][0m |          -0.0023 |          11.8810 |           3.1259 |
[32m[20230203 20:56:38 @agent_ppo2.py:193][0m |          -0.0058 |          10.8284 |           3.1250 |
[32m[20230203 20:56:38 @agent_ppo2.py:193][0m |          -0.0060 |          10.4973 |           3.1232 |
[32m[20230203 20:56:38 @agent_ppo2.py:193][0m |          -0.0064 |          10.2567 |           3.1218 |
[32m[20230203 20:56:38 @agent_ppo2.py:193][0m |          -0.0073 |           9.8609 |           3.1204 |
[32m[20230203 20:56:38 @agent_ppo2.py:193][0m |          -0.0126 |           9.5904 |           3.1210 |
[32m[20230203 20:56:38 @agent_ppo2.py:193][0m |          -0.0132 |           9.3296 |           3.1200 |
[32m[20230203 20:56:38 @agent_ppo2.py:193][0m |          -0.0123 |           9.1567 |           3.1186 |
[32m[20230203 20:56:38 @agent_ppo2.py:138][0m Policy update time: 0.41 s
[32m[20230203 20:56:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 133.74
[32m[20230203 20:56:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.36
[32m[20230203 20:56:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 187.24
[32m[20230203 20:56:38 @agent_ppo2.py:151][0m Total time:       6.05 min
[32m[20230203 20:56:38 @agent_ppo2.py:153][0m 475136 total steps have happened
[32m[20230203 20:56:38 @agent_ppo2.py:129][0m #------------------------ Iteration 232 --------------------------#
[32m[20230203 20:56:39 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:56:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:39 @agent_ppo2.py:193][0m |          -0.0005 |           8.5903 |           3.0764 |
[32m[20230203 20:56:39 @agent_ppo2.py:193][0m |          -0.0048 |           7.8781 |           3.0780 |
[32m[20230203 20:56:39 @agent_ppo2.py:193][0m |          -0.0058 |           7.5826 |           3.0762 |
[32m[20230203 20:56:39 @agent_ppo2.py:193][0m |          -0.0089 |           7.4009 |           3.0742 |
[32m[20230203 20:56:39 @agent_ppo2.py:193][0m |          -0.0075 |           7.3623 |           3.0728 |
[32m[20230203 20:56:39 @agent_ppo2.py:193][0m |          -0.0078 |           7.2944 |           3.0747 |
[32m[20230203 20:56:39 @agent_ppo2.py:193][0m |          -0.0097 |           7.1708 |           3.0756 |
[32m[20230203 20:56:39 @agent_ppo2.py:193][0m |          -0.0112 |           7.1328 |           3.0724 |
[32m[20230203 20:56:39 @agent_ppo2.py:193][0m |          -0.0117 |           7.0753 |           3.0732 |
[32m[20230203 20:56:39 @agent_ppo2.py:193][0m |          -0.0116 |           7.0336 |           3.0701 |
[32m[20230203 20:56:39 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:56:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 242.54
[32m[20230203 20:56:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 243.71
[32m[20230203 20:56:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 169.63
[32m[20230203 20:56:39 @agent_ppo2.py:151][0m Total time:       6.07 min
[32m[20230203 20:56:39 @agent_ppo2.py:153][0m 477184 total steps have happened
[32m[20230203 20:56:39 @agent_ppo2.py:129][0m #------------------------ Iteration 233 --------------------------#
[32m[20230203 20:56:40 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:56:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:40 @agent_ppo2.py:193][0m |           0.0089 |          29.4507 |           3.1101 |
[32m[20230203 20:56:40 @agent_ppo2.py:193][0m |          -0.0002 |          24.2671 |           3.1083 |
[32m[20230203 20:56:40 @agent_ppo2.py:193][0m |          -0.0069 |          21.4988 |           3.1050 |
[32m[20230203 20:56:40 @agent_ppo2.py:193][0m |           0.0052 |          21.5530 |           3.1017 |
[32m[20230203 20:56:40 @agent_ppo2.py:193][0m |          -0.0083 |          19.8599 |           3.0974 |
[32m[20230203 20:56:40 @agent_ppo2.py:193][0m |           0.0023 |          19.2322 |           3.0966 |
[32m[20230203 20:56:40 @agent_ppo2.py:193][0m |          -0.0080 |          18.8652 |           3.0909 |
[32m[20230203 20:56:40 @agent_ppo2.py:193][0m |          -0.0092 |          18.9136 |           3.0912 |
[32m[20230203 20:56:40 @agent_ppo2.py:193][0m |          -0.0096 |          18.2752 |           3.0906 |
[32m[20230203 20:56:41 @agent_ppo2.py:193][0m |          -0.0042 |          18.8167 |           3.0878 |
[32m[20230203 20:56:41 @agent_ppo2.py:138][0m Policy update time: 0.56 s
[32m[20230203 20:56:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 143.19
[32m[20230203 20:56:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.57
[32m[20230203 20:56:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.80
[32m[20230203 20:56:41 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 265.80
[32m[20230203 20:56:41 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 265.80
[32m[20230203 20:56:41 @agent_ppo2.py:151][0m Total time:       6.09 min
[32m[20230203 20:56:41 @agent_ppo2.py:153][0m 479232 total steps have happened
[32m[20230203 20:56:41 @agent_ppo2.py:129][0m #------------------------ Iteration 234 --------------------------#
[32m[20230203 20:56:41 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:56:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:41 @agent_ppo2.py:193][0m |          -0.0028 |          30.6559 |           3.1187 |
[32m[20230203 20:56:41 @agent_ppo2.py:193][0m |          -0.0047 |          20.2604 |           3.1158 |
[32m[20230203 20:56:42 @agent_ppo2.py:193][0m |          -0.0045 |          17.2211 |           3.1165 |
[32m[20230203 20:56:42 @agent_ppo2.py:193][0m |          -0.0056 |          15.2190 |           3.1125 |
[32m[20230203 20:56:42 @agent_ppo2.py:193][0m |          -0.0110 |          13.6960 |           3.1116 |
[32m[20230203 20:56:42 @agent_ppo2.py:193][0m |          -0.0079 |          13.1587 |           3.1106 |
[32m[20230203 20:56:42 @agent_ppo2.py:193][0m |          -0.0071 |          12.3885 |           3.1130 |
[32m[20230203 20:56:42 @agent_ppo2.py:193][0m |          -0.0111 |          12.1820 |           3.1100 |
[32m[20230203 20:56:42 @agent_ppo2.py:193][0m |          -0.0080 |          12.1018 |           3.1114 |
[32m[20230203 20:56:42 @agent_ppo2.py:193][0m |          -0.0147 |          11.5358 |           3.1105 |
[32m[20230203 20:56:42 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:56:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 185.58
[32m[20230203 20:56:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.73
[32m[20230203 20:56:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.35
[32m[20230203 20:56:42 @agent_ppo2.py:151][0m Total time:       6.12 min
[32m[20230203 20:56:42 @agent_ppo2.py:153][0m 481280 total steps have happened
[32m[20230203 20:56:42 @agent_ppo2.py:129][0m #------------------------ Iteration 235 --------------------------#
[32m[20230203 20:56:43 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:56:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:43 @agent_ppo2.py:193][0m |          -0.0003 |          37.5673 |           3.1568 |
[32m[20230203 20:56:43 @agent_ppo2.py:193][0m |          -0.0066 |          28.9027 |           3.1544 |
[32m[20230203 20:56:43 @agent_ppo2.py:193][0m |          -0.0074 |          26.4367 |           3.1543 |
[32m[20230203 20:56:43 @agent_ppo2.py:193][0m |          -0.0095 |          24.3408 |           3.1522 |
[32m[20230203 20:56:43 @agent_ppo2.py:193][0m |          -0.0094 |          23.2875 |           3.1520 |
[32m[20230203 20:56:43 @agent_ppo2.py:193][0m |          -0.0129 |          22.2304 |           3.1514 |
[32m[20230203 20:56:43 @agent_ppo2.py:193][0m |          -0.0122 |          21.8614 |           3.1530 |
[32m[20230203 20:56:43 @agent_ppo2.py:193][0m |          -0.0143 |          21.1295 |           3.1533 |
[32m[20230203 20:56:43 @agent_ppo2.py:193][0m |          -0.0146 |          20.7427 |           3.1552 |
[32m[20230203 20:56:43 @agent_ppo2.py:193][0m |          -0.0126 |          20.5159 |           3.1534 |
[32m[20230203 20:56:43 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:56:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 243.44
[32m[20230203 20:56:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.88
[32m[20230203 20:56:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.03
[32m[20230203 20:56:44 @agent_ppo2.py:151][0m Total time:       6.14 min
[32m[20230203 20:56:44 @agent_ppo2.py:153][0m 483328 total steps have happened
[32m[20230203 20:56:44 @agent_ppo2.py:129][0m #------------------------ Iteration 236 --------------------------#
[32m[20230203 20:56:44 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:56:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:44 @agent_ppo2.py:193][0m |          -0.0015 |          58.2385 |           3.0668 |
[32m[20230203 20:56:44 @agent_ppo2.py:193][0m |          -0.0065 |          41.4379 |           3.0633 |
[32m[20230203 20:56:44 @agent_ppo2.py:193][0m |          -0.0099 |          36.6669 |           3.0585 |
[32m[20230203 20:56:44 @agent_ppo2.py:193][0m |          -0.0083 |          32.7636 |           3.0542 |
[32m[20230203 20:56:44 @agent_ppo2.py:193][0m |          -0.0129 |          30.1561 |           3.0551 |
[32m[20230203 20:56:44 @agent_ppo2.py:193][0m |          -0.0127 |          28.3391 |           3.0507 |
[32m[20230203 20:56:44 @agent_ppo2.py:193][0m |          -0.0154 |          26.9472 |           3.0491 |
[32m[20230203 20:56:45 @agent_ppo2.py:193][0m |          -0.0138 |          25.7271 |           3.0452 |
[32m[20230203 20:56:45 @agent_ppo2.py:193][0m |          -0.0160 |          24.6461 |           3.0475 |
[32m[20230203 20:56:45 @agent_ppo2.py:193][0m |          -0.0161 |          23.6083 |           3.0447 |
[32m[20230203 20:56:45 @agent_ppo2.py:138][0m Policy update time: 0.56 s
[32m[20230203 20:56:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 110.40
[32m[20230203 20:56:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.65
[32m[20230203 20:56:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.57
[32m[20230203 20:56:45 @agent_ppo2.py:151][0m Total time:       6.16 min
[32m[20230203 20:56:45 @agent_ppo2.py:153][0m 485376 total steps have happened
[32m[20230203 20:56:45 @agent_ppo2.py:129][0m #------------------------ Iteration 237 --------------------------#
[32m[20230203 20:56:45 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 1 slaves
[32m[20230203 20:56:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:45 @agent_ppo2.py:193][0m |          -0.0006 |          36.4401 |           3.0758 |
[32m[20230203 20:56:46 @agent_ppo2.py:193][0m |          -0.0059 |          27.2463 |           3.0724 |
[32m[20230203 20:56:46 @agent_ppo2.py:193][0m |          -0.0100 |          24.3238 |           3.0707 |
[32m[20230203 20:56:46 @agent_ppo2.py:193][0m |          -0.0101 |          22.1080 |           3.0697 |
[32m[20230203 20:56:46 @agent_ppo2.py:193][0m |          -0.0104 |          20.6625 |           3.0703 |
[32m[20230203 20:56:46 @agent_ppo2.py:193][0m |          -0.0097 |          20.0766 |           3.0673 |
[32m[20230203 20:56:46 @agent_ppo2.py:193][0m |          -0.0148 |          18.6445 |           3.0682 |
[32m[20230203 20:56:46 @agent_ppo2.py:193][0m |          -0.0148 |          17.9288 |           3.0680 |
[32m[20230203 20:56:46 @agent_ppo2.py:193][0m |          -0.0166 |          16.9213 |           3.0648 |
[32m[20230203 20:56:46 @agent_ppo2.py:193][0m |          -0.0161 |          16.6067 |           3.0626 |
[32m[20230203 20:56:46 @agent_ppo2.py:138][0m Policy update time: 0.41 s
[32m[20230203 20:56:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 135.79
[32m[20230203 20:56:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.66
[32m[20230203 20:56:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.25
[32m[20230203 20:56:46 @agent_ppo2.py:151][0m Total time:       6.18 min
[32m[20230203 20:56:46 @agent_ppo2.py:153][0m 487424 total steps have happened
[32m[20230203 20:56:46 @agent_ppo2.py:129][0m #------------------------ Iteration 238 --------------------------#
[32m[20230203 20:56:47 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:56:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:47 @agent_ppo2.py:193][0m |           0.0001 |          39.1449 |           3.0667 |
[32m[20230203 20:56:47 @agent_ppo2.py:193][0m |          -0.0022 |          22.0275 |           3.0643 |
[32m[20230203 20:56:47 @agent_ppo2.py:193][0m |          -0.0063 |          18.3082 |           3.0622 |
[32m[20230203 20:56:47 @agent_ppo2.py:193][0m |          -0.0172 |          16.9867 |           3.0577 |
[32m[20230203 20:56:47 @agent_ppo2.py:193][0m |           0.0069 |          18.9322 |           3.0571 |
[32m[20230203 20:56:47 @agent_ppo2.py:193][0m |          -0.0138 |          15.3235 |           3.0572 |
[32m[20230203 20:56:47 @agent_ppo2.py:193][0m |          -0.0120 |          14.7387 |           3.0582 |
[32m[20230203 20:56:47 @agent_ppo2.py:193][0m |          -0.0173 |          14.1618 |           3.0556 |
[32m[20230203 20:56:47 @agent_ppo2.py:193][0m |          -0.0146 |          13.7221 |           3.0570 |
[32m[20230203 20:56:47 @agent_ppo2.py:193][0m |          -0.0147 |          13.3308 |           3.0584 |
[32m[20230203 20:56:47 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:56:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 182.00
[32m[20230203 20:56:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.87
[32m[20230203 20:56:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.58
[32m[20230203 20:56:48 @agent_ppo2.py:151][0m Total time:       6.20 min
[32m[20230203 20:56:48 @agent_ppo2.py:153][0m 489472 total steps have happened
[32m[20230203 20:56:48 @agent_ppo2.py:129][0m #------------------------ Iteration 239 --------------------------#
[32m[20230203 20:56:48 @agent_ppo2.py:135][0m Sampling time: 0.58 s by 1 slaves
[32m[20230203 20:56:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:48 @agent_ppo2.py:193][0m |          -0.0008 |          23.7190 |           3.1360 |
[32m[20230203 20:56:48 @agent_ppo2.py:193][0m |          -0.0085 |          19.2387 |           3.1308 |
[32m[20230203 20:56:48 @agent_ppo2.py:193][0m |          -0.0109 |          17.8370 |           3.1273 |
[32m[20230203 20:56:48 @agent_ppo2.py:193][0m |          -0.0110 |          16.7815 |           3.1238 |
[32m[20230203 20:56:48 @agent_ppo2.py:193][0m |          -0.0114 |          16.2048 |           3.1239 |
[32m[20230203 20:56:49 @agent_ppo2.py:193][0m |          -0.0139 |          15.4709 |           3.1212 |
[32m[20230203 20:56:49 @agent_ppo2.py:193][0m |          -0.0156 |          14.9799 |           3.1228 |
[32m[20230203 20:56:49 @agent_ppo2.py:193][0m |          -0.0174 |          14.4362 |           3.1189 |
[32m[20230203 20:56:49 @agent_ppo2.py:193][0m |          -0.0155 |          14.1488 |           3.1190 |
[32m[20230203 20:56:49 @agent_ppo2.py:193][0m |          -0.0173 |          13.5596 |           3.1177 |
[32m[20230203 20:56:49 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230203 20:56:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 160.81
[32m[20230203 20:56:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.73
[32m[20230203 20:56:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 260.04
[32m[20230203 20:56:49 @agent_ppo2.py:151][0m Total time:       6.23 min
[32m[20230203 20:56:49 @agent_ppo2.py:153][0m 491520 total steps have happened
[32m[20230203 20:56:49 @agent_ppo2.py:129][0m #------------------------ Iteration 240 --------------------------#
[32m[20230203 20:56:50 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:56:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:50 @agent_ppo2.py:193][0m |          -0.0006 |          10.0717 |           3.1645 |
[32m[20230203 20:56:50 @agent_ppo2.py:193][0m |          -0.0088 |           8.7968 |           3.1545 |
[32m[20230203 20:56:50 @agent_ppo2.py:193][0m |          -0.0103 |           8.5511 |           3.1529 |
[32m[20230203 20:56:50 @agent_ppo2.py:193][0m |          -0.0110 |           8.4499 |           3.1509 |
[32m[20230203 20:56:50 @agent_ppo2.py:193][0m |          -0.0123 |           8.2710 |           3.1508 |
[32m[20230203 20:56:50 @agent_ppo2.py:193][0m |          -0.0138 |           8.2033 |           3.1494 |
[32m[20230203 20:56:50 @agent_ppo2.py:193][0m |          -0.0140 |           8.1380 |           3.1488 |
[32m[20230203 20:56:50 @agent_ppo2.py:193][0m |          -0.0136 |           8.1007 |           3.1479 |
[32m[20230203 20:56:50 @agent_ppo2.py:193][0m |          -0.0143 |           8.1235 |           3.1457 |
[32m[20230203 20:56:50 @agent_ppo2.py:193][0m |          -0.0149 |           8.0828 |           3.1496 |
[32m[20230203 20:56:50 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:56:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 241.48
[32m[20230203 20:56:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 242.27
[32m[20230203 20:56:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.57
[32m[20230203 20:56:51 @agent_ppo2.py:151][0m Total time:       6.26 min
[32m[20230203 20:56:51 @agent_ppo2.py:153][0m 493568 total steps have happened
[32m[20230203 20:56:51 @agent_ppo2.py:129][0m #------------------------ Iteration 241 --------------------------#
[32m[20230203 20:56:51 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:56:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:51 @agent_ppo2.py:193][0m |          -0.0027 |          17.2554 |           3.0074 |
[32m[20230203 20:56:51 @agent_ppo2.py:193][0m |          -0.0114 |           9.4333 |           3.0021 |
[32m[20230203 20:56:51 @agent_ppo2.py:193][0m |           0.0083 |           8.2608 |           2.9978 |
[32m[20230203 20:56:51 @agent_ppo2.py:193][0m |           0.0101 |           7.6984 |           2.9955 |
[32m[20230203 20:56:51 @agent_ppo2.py:193][0m |          -0.0165 |           7.4446 |           2.9928 |
[32m[20230203 20:56:51 @agent_ppo2.py:193][0m |          -0.0155 |           7.1553 |           2.9902 |
[32m[20230203 20:56:51 @agent_ppo2.py:193][0m |          -0.0170 |           7.0067 |           2.9901 |
[32m[20230203 20:56:51 @agent_ppo2.py:193][0m |          -0.0148 |           6.8547 |           2.9891 |
[32m[20230203 20:56:51 @agent_ppo2.py:193][0m |          -0.0225 |           6.8972 |           2.9867 |
[32m[20230203 20:56:51 @agent_ppo2.py:193][0m |          -0.0194 |           6.7063 |           2.9869 |
[32m[20230203 20:56:51 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:56:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 170.53
[32m[20230203 20:56:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 240.55
[32m[20230203 20:56:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 266.33
[32m[20230203 20:56:52 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 266.33
[32m[20230203 20:56:52 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 266.33
[32m[20230203 20:56:52 @agent_ppo2.py:151][0m Total time:       6.28 min
[32m[20230203 20:56:52 @agent_ppo2.py:153][0m 495616 total steps have happened
[32m[20230203 20:56:52 @agent_ppo2.py:129][0m #------------------------ Iteration 242 --------------------------#
[32m[20230203 20:56:52 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:56:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:52 @agent_ppo2.py:193][0m |          -0.0012 |           9.7955 |           3.1132 |
[32m[20230203 20:56:52 @agent_ppo2.py:193][0m |          -0.0038 |           8.9717 |           3.1135 |
[32m[20230203 20:56:52 @agent_ppo2.py:193][0m |          -0.0054 |           8.8237 |           3.1095 |
[32m[20230203 20:56:52 @agent_ppo2.py:193][0m |          -0.0083 |           8.7204 |           3.1077 |
[32m[20230203 20:56:53 @agent_ppo2.py:193][0m |          -0.0070 |           8.6484 |           3.1037 |
[32m[20230203 20:56:53 @agent_ppo2.py:193][0m |          -0.0062 |           8.7077 |           3.1059 |
[32m[20230203 20:56:53 @agent_ppo2.py:193][0m |          -0.0068 |           8.6623 |           3.1033 |
[32m[20230203 20:56:53 @agent_ppo2.py:193][0m |          -0.0069 |           8.5127 |           3.0988 |
[32m[20230203 20:56:53 @agent_ppo2.py:193][0m |          -0.0088 |           8.4913 |           3.1026 |
[32m[20230203 20:56:53 @agent_ppo2.py:193][0m |          -0.0103 |           8.4968 |           3.0982 |
[32m[20230203 20:56:53 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 20:56:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 243.95
[32m[20230203 20:56:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.87
[32m[20230203 20:56:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.98
[32m[20230203 20:56:53 @agent_ppo2.py:151][0m Total time:       6.30 min
[32m[20230203 20:56:53 @agent_ppo2.py:153][0m 497664 total steps have happened
[32m[20230203 20:56:53 @agent_ppo2.py:129][0m #------------------------ Iteration 243 --------------------------#
[32m[20230203 20:56:54 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 20:56:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:54 @agent_ppo2.py:193][0m |          -0.0017 |          22.9894 |           3.0932 |
[32m[20230203 20:56:54 @agent_ppo2.py:193][0m |          -0.0019 |          14.3445 |           3.0876 |
[32m[20230203 20:56:54 @agent_ppo2.py:193][0m |          -0.0015 |          13.1191 |           3.0843 |
[32m[20230203 20:56:54 @agent_ppo2.py:193][0m |          -0.0045 |          12.1760 |           3.0812 |
[32m[20230203 20:56:54 @agent_ppo2.py:193][0m |          -0.0118 |          11.2377 |           3.0750 |
[32m[20230203 20:56:54 @agent_ppo2.py:193][0m |          -0.0091 |          10.9747 |           3.0771 |
[32m[20230203 20:56:54 @agent_ppo2.py:193][0m |          -0.0125 |          10.7140 |           3.0764 |
[32m[20230203 20:56:54 @agent_ppo2.py:193][0m |          -0.0100 |          10.6411 |           3.0786 |
[32m[20230203 20:56:54 @agent_ppo2.py:193][0m |          -0.0119 |          10.4335 |           3.0747 |
[32m[20230203 20:56:54 @agent_ppo2.py:193][0m |          -0.0118 |          10.1745 |           3.0757 |
[32m[20230203 20:56:54 @agent_ppo2.py:138][0m Policy update time: 0.54 s
[32m[20230203 20:56:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 239.92
[32m[20230203 20:56:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 241.75
[32m[20230203 20:56:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.39
[32m[20230203 20:56:55 @agent_ppo2.py:151][0m Total time:       6.32 min
[32m[20230203 20:56:55 @agent_ppo2.py:153][0m 499712 total steps have happened
[32m[20230203 20:56:55 @agent_ppo2.py:129][0m #------------------------ Iteration 244 --------------------------#
[32m[20230203 20:56:55 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:56:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:55 @agent_ppo2.py:193][0m |          -0.0043 |          29.1590 |           3.1005 |
[32m[20230203 20:56:55 @agent_ppo2.py:193][0m |          -0.0051 |          18.7765 |           3.0995 |
[32m[20230203 20:56:55 @agent_ppo2.py:193][0m |          -0.0050 |          16.5585 |           3.0992 |
[32m[20230203 20:56:55 @agent_ppo2.py:193][0m |          -0.0088 |          15.4039 |           3.1019 |
[32m[20230203 20:56:55 @agent_ppo2.py:193][0m |          -0.0158 |          14.1212 |           3.1027 |
[32m[20230203 20:56:55 @agent_ppo2.py:193][0m |          -0.0171 |          14.2438 |           3.1038 |
[32m[20230203 20:56:55 @agent_ppo2.py:193][0m |          -0.0165 |          13.3943 |           3.1010 |
[32m[20230203 20:56:55 @agent_ppo2.py:193][0m |          -0.0134 |          13.0709 |           3.1041 |
[32m[20230203 20:56:55 @agent_ppo2.py:193][0m |          -0.0145 |          12.8401 |           3.0998 |
[32m[20230203 20:56:55 @agent_ppo2.py:193][0m |          -0.0096 |          12.1523 |           3.1010 |
[32m[20230203 20:56:55 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:56:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 70.67
[32m[20230203 20:56:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.33
[32m[20230203 20:56:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 266.67
[32m[20230203 20:56:56 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 266.67
[32m[20230203 20:56:56 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 266.67
[32m[20230203 20:56:56 @agent_ppo2.py:151][0m Total time:       6.34 min
[32m[20230203 20:56:56 @agent_ppo2.py:153][0m 501760 total steps have happened
[32m[20230203 20:56:56 @agent_ppo2.py:129][0m #------------------------ Iteration 245 --------------------------#
[32m[20230203 20:56:56 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:56:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:56 @agent_ppo2.py:193][0m |          -0.0053 |           9.9876 |           3.0385 |
[32m[20230203 20:56:56 @agent_ppo2.py:193][0m |          -0.0070 |           8.8278 |           3.0296 |
[32m[20230203 20:56:56 @agent_ppo2.py:193][0m |          -0.0196 |           8.5718 |           3.0323 |
[32m[20230203 20:56:57 @agent_ppo2.py:193][0m |          -0.0166 |           8.4178 |           3.0307 |
[32m[20230203 20:56:57 @agent_ppo2.py:193][0m |          -0.0111 |           8.3028 |           3.0288 |
[32m[20230203 20:56:57 @agent_ppo2.py:193][0m |           0.0056 |           8.7850 |           3.0281 |
[32m[20230203 20:56:57 @agent_ppo2.py:193][0m |          -0.0130 |           8.1089 |           3.0292 |
[32m[20230203 20:56:57 @agent_ppo2.py:193][0m |          -0.0147 |           8.1890 |           3.0272 |
[32m[20230203 20:56:57 @agent_ppo2.py:193][0m |          -0.0209 |           7.9614 |           3.0282 |
[32m[20230203 20:56:57 @agent_ppo2.py:193][0m |          -0.0209 |           7.8957 |           3.0285 |
[32m[20230203 20:56:57 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:56:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 243.58
[32m[20230203 20:56:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.97
[32m[20230203 20:56:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 266.68
[32m[20230203 20:56:57 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 266.68
[32m[20230203 20:56:57 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 266.68
[32m[20230203 20:56:57 @agent_ppo2.py:151][0m Total time:       6.37 min
[32m[20230203 20:56:57 @agent_ppo2.py:153][0m 503808 total steps have happened
[32m[20230203 20:56:57 @agent_ppo2.py:129][0m #------------------------ Iteration 246 --------------------------#
[32m[20230203 20:56:58 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 20:56:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:58 @agent_ppo2.py:193][0m |          -0.0007 |           7.8308 |           3.0475 |
[32m[20230203 20:56:58 @agent_ppo2.py:193][0m |          -0.0012 |           7.6459 |           3.0483 |
[32m[20230203 20:56:58 @agent_ppo2.py:193][0m |          -0.0080 |           7.2952 |           3.0521 |
[32m[20230203 20:56:58 @agent_ppo2.py:193][0m |          -0.0027 |           7.2754 |           3.0529 |
[32m[20230203 20:56:58 @agent_ppo2.py:193][0m |          -0.0090 |           7.1784 |           3.0522 |
[32m[20230203 20:56:58 @agent_ppo2.py:193][0m |          -0.0086 |           7.1074 |           3.0505 |
[32m[20230203 20:56:58 @agent_ppo2.py:193][0m |          -0.0140 |           7.0148 |           3.0510 |
[32m[20230203 20:56:58 @agent_ppo2.py:193][0m |          -0.0029 |           7.7744 |           3.0508 |
[32m[20230203 20:56:58 @agent_ppo2.py:193][0m |          -0.0176 |           6.9558 |           3.0492 |
[32m[20230203 20:56:58 @agent_ppo2.py:193][0m |          -0.0158 |           7.1574 |           3.0516 |
[32m[20230203 20:56:58 @agent_ppo2.py:138][0m Policy update time: 0.54 s
[32m[20230203 20:56:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 240.76
[32m[20230203 20:56:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.52
[32m[20230203 20:56:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.84
[32m[20230203 20:56:59 @agent_ppo2.py:151][0m Total time:       6.39 min
[32m[20230203 20:56:59 @agent_ppo2.py:153][0m 505856 total steps have happened
[32m[20230203 20:56:59 @agent_ppo2.py:129][0m #------------------------ Iteration 247 --------------------------#
[32m[20230203 20:56:59 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:56:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:56:59 @agent_ppo2.py:193][0m |          -0.0004 |           7.9784 |           3.1121 |
[32m[20230203 20:56:59 @agent_ppo2.py:193][0m |          -0.0049 |           7.7338 |           3.1071 |
[32m[20230203 20:56:59 @agent_ppo2.py:193][0m |          -0.0071 |           7.5978 |           3.1060 |
[32m[20230203 20:56:59 @agent_ppo2.py:193][0m |          -0.0087 |           7.4935 |           3.1064 |
[32m[20230203 20:56:59 @agent_ppo2.py:193][0m |          -0.0094 |           7.4103 |           3.1063 |
[32m[20230203 20:56:59 @agent_ppo2.py:193][0m |          -0.0097 |           7.3574 |           3.1021 |
[32m[20230203 20:56:59 @agent_ppo2.py:193][0m |          -0.0108 |           7.3062 |           3.1047 |
[32m[20230203 20:56:59 @agent_ppo2.py:193][0m |          -0.0120 |           7.2662 |           3.1034 |
[32m[20230203 20:56:59 @agent_ppo2.py:193][0m |          -0.0116 |           7.2251 |           3.1038 |
[32m[20230203 20:56:59 @agent_ppo2.py:193][0m |          -0.0123 |           7.1854 |           3.1056 |
[32m[20230203 20:56:59 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:57:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.70
[32m[20230203 20:57:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.61
[32m[20230203 20:57:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.27
[32m[20230203 20:57:00 @agent_ppo2.py:151][0m Total time:       6.41 min
[32m[20230203 20:57:00 @agent_ppo2.py:153][0m 507904 total steps have happened
[32m[20230203 20:57:00 @agent_ppo2.py:129][0m #------------------------ Iteration 248 --------------------------#
[32m[20230203 20:57:00 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:57:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:00 @agent_ppo2.py:193][0m |          -0.0018 |           8.6203 |           3.1451 |
[32m[20230203 20:57:00 @agent_ppo2.py:193][0m |          -0.0082 |           8.2711 |           3.1428 |
[32m[20230203 20:57:00 @agent_ppo2.py:193][0m |          -0.0104 |           8.1201 |           3.1381 |
[32m[20230203 20:57:01 @agent_ppo2.py:193][0m |          -0.0032 |           8.2781 |           3.1368 |
[32m[20230203 20:57:01 @agent_ppo2.py:193][0m |          -0.0088 |           8.0568 |           3.1362 |
[32m[20230203 20:57:01 @agent_ppo2.py:193][0m |          -0.0116 |           7.8874 |           3.1327 |
[32m[20230203 20:57:01 @agent_ppo2.py:193][0m |          -0.0070 |           8.3066 |           3.1311 |
[32m[20230203 20:57:01 @agent_ppo2.py:193][0m |          -0.0124 |           7.7704 |           3.1290 |
[32m[20230203 20:57:01 @agent_ppo2.py:193][0m |          -0.0123 |           7.7548 |           3.1265 |
[32m[20230203 20:57:01 @agent_ppo2.py:193][0m |          -0.0139 |           7.6821 |           3.1259 |
[32m[20230203 20:57:01 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:57:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.50
[32m[20230203 20:57:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.30
[32m[20230203 20:57:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 263.81
[32m[20230203 20:57:01 @agent_ppo2.py:151][0m Total time:       6.43 min
[32m[20230203 20:57:01 @agent_ppo2.py:153][0m 509952 total steps have happened
[32m[20230203 20:57:01 @agent_ppo2.py:129][0m #------------------------ Iteration 249 --------------------------#
[32m[20230203 20:57:02 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:57:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:02 @agent_ppo2.py:193][0m |          -0.0046 |           7.8468 |           3.0623 |
[32m[20230203 20:57:02 @agent_ppo2.py:193][0m |          -0.0094 |           7.5807 |           3.0563 |
[32m[20230203 20:57:02 @agent_ppo2.py:193][0m |          -0.0071 |           7.4282 |           3.0570 |
[32m[20230203 20:57:02 @agent_ppo2.py:193][0m |          -0.0091 |           7.3174 |           3.0576 |
[32m[20230203 20:57:02 @agent_ppo2.py:193][0m |          -0.0103 |           7.2329 |           3.0586 |
[32m[20230203 20:57:02 @agent_ppo2.py:193][0m |          -0.0106 |           7.1626 |           3.0596 |
[32m[20230203 20:57:02 @agent_ppo2.py:193][0m |          -0.0107 |           7.1047 |           3.0560 |
[32m[20230203 20:57:02 @agent_ppo2.py:193][0m |          -0.0108 |           7.0974 |           3.0579 |
[32m[20230203 20:57:02 @agent_ppo2.py:193][0m |          -0.0103 |           7.0169 |           3.0567 |
[32m[20230203 20:57:02 @agent_ppo2.py:193][0m |          -0.0141 |           6.9526 |           3.0583 |
[32m[20230203 20:57:02 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:57:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.33
[32m[20230203 20:57:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.90
[32m[20230203 20:57:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.26
[32m[20230203 20:57:02 @agent_ppo2.py:151][0m Total time:       6.45 min
[32m[20230203 20:57:02 @agent_ppo2.py:153][0m 512000 total steps have happened
[32m[20230203 20:57:03 @agent_ppo2.py:129][0m #------------------------ Iteration 250 --------------------------#
[32m[20230203 20:57:03 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:57:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:03 @agent_ppo2.py:193][0m |           0.0017 |           7.8309 |           3.1716 |
[32m[20230203 20:57:03 @agent_ppo2.py:193][0m |          -0.0031 |           7.6274 |           3.1687 |
[32m[20230203 20:57:03 @agent_ppo2.py:193][0m |          -0.0073 |           7.4216 |           3.1696 |
[32m[20230203 20:57:03 @agent_ppo2.py:193][0m |          -0.0081 |           7.3480 |           3.1629 |
[32m[20230203 20:57:03 @agent_ppo2.py:193][0m |          -0.0083 |           7.4083 |           3.1640 |
[32m[20230203 20:57:03 @agent_ppo2.py:193][0m |          -0.0103 |           7.2481 |           3.1632 |
[32m[20230203 20:57:03 @agent_ppo2.py:193][0m |          -0.0100 |           7.3340 |           3.1623 |
[32m[20230203 20:57:03 @agent_ppo2.py:193][0m |          -0.0126 |           7.1844 |           3.1581 |
[32m[20230203 20:57:03 @agent_ppo2.py:193][0m |          -0.0127 |           7.1572 |           3.1604 |
[32m[20230203 20:57:03 @agent_ppo2.py:193][0m |          -0.0121 |           7.1196 |           3.1607 |
[32m[20230203 20:57:03 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:57:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 242.06
[32m[20230203 20:57:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.24
[32m[20230203 20:57:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.90
[32m[20230203 20:57:04 @agent_ppo2.py:151][0m Total time:       6.48 min
[32m[20230203 20:57:04 @agent_ppo2.py:153][0m 514048 total steps have happened
[32m[20230203 20:57:04 @agent_ppo2.py:129][0m #------------------------ Iteration 251 --------------------------#
[32m[20230203 20:57:04 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 20:57:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:04 @agent_ppo2.py:193][0m |           0.0053 |           8.9763 |           3.0345 |
[32m[20230203 20:57:04 @agent_ppo2.py:193][0m |          -0.0059 |           8.1950 |           3.0343 |
[32m[20230203 20:57:05 @agent_ppo2.py:193][0m |          -0.0056 |           7.9364 |           3.0313 |
[32m[20230203 20:57:05 @agent_ppo2.py:193][0m |          -0.0052 |           7.8992 |           3.0351 |
[32m[20230203 20:57:05 @agent_ppo2.py:193][0m |          -0.0008 |           8.2157 |           3.0348 |
[32m[20230203 20:57:05 @agent_ppo2.py:193][0m |          -0.0120 |           7.6358 |           3.0353 |
[32m[20230203 20:57:05 @agent_ppo2.py:193][0m |          -0.0077 |           7.7618 |           3.0355 |
[32m[20230203 20:57:05 @agent_ppo2.py:193][0m |          -0.0140 |           7.5006 |           3.0355 |
[32m[20230203 20:57:05 @agent_ppo2.py:193][0m |          -0.0097 |           7.4960 |           3.0354 |
[32m[20230203 20:57:05 @agent_ppo2.py:193][0m |          -0.0037 |           7.7451 |           3.0347 |
[32m[20230203 20:57:05 @agent_ppo2.py:138][0m Policy update time: 0.54 s
[32m[20230203 20:57:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 238.31
[32m[20230203 20:57:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 243.00
[32m[20230203 20:57:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.36
[32m[20230203 20:57:05 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 267.36
[32m[20230203 20:57:05 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 267.36
[32m[20230203 20:57:05 @agent_ppo2.py:151][0m Total time:       6.50 min
[32m[20230203 20:57:05 @agent_ppo2.py:153][0m 516096 total steps have happened
[32m[20230203 20:57:05 @agent_ppo2.py:129][0m #------------------------ Iteration 252 --------------------------#
[32m[20230203 20:57:06 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:57:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:06 @agent_ppo2.py:193][0m |           0.0050 |           7.6822 |           3.0470 |
[32m[20230203 20:57:06 @agent_ppo2.py:193][0m |          -0.0020 |           7.2192 |           3.0507 |
[32m[20230203 20:57:06 @agent_ppo2.py:193][0m |          -0.0067 |           7.0590 |           3.0434 |
[32m[20230203 20:57:06 @agent_ppo2.py:193][0m |          -0.0022 |           7.3295 |           3.0410 |
[32m[20230203 20:57:06 @agent_ppo2.py:193][0m |           0.0091 |           8.6723 |           3.0396 |
[32m[20230203 20:57:06 @agent_ppo2.py:193][0m |          -0.0123 |           6.9204 |           3.0323 |
[32m[20230203 20:57:06 @agent_ppo2.py:193][0m |          -0.0144 |           6.8752 |           3.0308 |
[32m[20230203 20:57:06 @agent_ppo2.py:193][0m |          -0.0129 |           6.8786 |           3.0309 |
[32m[20230203 20:57:06 @agent_ppo2.py:193][0m |          -0.0136 |           6.8162 |           3.0303 |
[32m[20230203 20:57:06 @agent_ppo2.py:193][0m |          -0.0138 |           6.7750 |           3.0298 |
[32m[20230203 20:57:06 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:57:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 243.54
[32m[20230203 20:57:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.14
[32m[20230203 20:57:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 263.48
[32m[20230203 20:57:07 @agent_ppo2.py:151][0m Total time:       6.52 min
[32m[20230203 20:57:07 @agent_ppo2.py:153][0m 518144 total steps have happened
[32m[20230203 20:57:07 @agent_ppo2.py:129][0m #------------------------ Iteration 253 --------------------------#
[32m[20230203 20:57:07 @agent_ppo2.py:135][0m Sampling time: 0.36 s by 1 slaves
[32m[20230203 20:57:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:07 @agent_ppo2.py:193][0m |          -0.0021 |          21.0771 |           3.1051 |
[32m[20230203 20:57:07 @agent_ppo2.py:193][0m |          -0.0095 |           9.3522 |           3.1022 |
[32m[20230203 20:57:07 @agent_ppo2.py:193][0m |          -0.0096 |           7.9905 |           3.1002 |
[32m[20230203 20:57:07 @agent_ppo2.py:193][0m |          -0.0136 |           7.8091 |           3.0956 |
[32m[20230203 20:57:07 @agent_ppo2.py:193][0m |          -0.0107 |           7.3676 |           3.0962 |
[32m[20230203 20:57:07 @agent_ppo2.py:193][0m |          -0.0142 |           7.0419 |           3.0987 |
[32m[20230203 20:57:07 @agent_ppo2.py:193][0m |          -0.0167 |           6.8635 |           3.0972 |
[32m[20230203 20:57:07 @agent_ppo2.py:193][0m |          -0.0165 |           6.7254 |           3.0985 |
[32m[20230203 20:57:07 @agent_ppo2.py:193][0m |          -0.0151 |           6.6179 |           3.0976 |
[32m[20230203 20:57:07 @agent_ppo2.py:193][0m |          -0.0172 |           6.5396 |           3.0974 |
[32m[20230203 20:57:07 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 20:57:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 129.00
[32m[20230203 20:57:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.46
[32m[20230203 20:57:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.40
[32m[20230203 20:57:08 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 267.40
[32m[20230203 20:57:08 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 267.40
[32m[20230203 20:57:08 @agent_ppo2.py:151][0m Total time:       6.54 min
[32m[20230203 20:57:08 @agent_ppo2.py:153][0m 520192 total steps have happened
[32m[20230203 20:57:08 @agent_ppo2.py:129][0m #------------------------ Iteration 254 --------------------------#
[32m[20230203 20:57:08 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:57:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:08 @agent_ppo2.py:193][0m |           0.0038 |          22.4365 |           3.0018 |
[32m[20230203 20:57:08 @agent_ppo2.py:193][0m |          -0.0039 |          10.7056 |           2.9937 |
[32m[20230203 20:57:08 @agent_ppo2.py:193][0m |          -0.0049 |           8.7189 |           2.9890 |
[32m[20230203 20:57:08 @agent_ppo2.py:193][0m |          -0.0054 |           8.0222 |           2.9935 |
[32m[20230203 20:57:08 @agent_ppo2.py:193][0m |          -0.0098 |           7.5396 |           2.9881 |
[32m[20230203 20:57:08 @agent_ppo2.py:193][0m |          -0.0102 |           7.2156 |           2.9873 |
[32m[20230203 20:57:09 @agent_ppo2.py:193][0m |          -0.0107 |           6.9625 |           2.9879 |
[32m[20230203 20:57:09 @agent_ppo2.py:193][0m |          -0.0111 |           6.8888 |           2.9859 |
[32m[20230203 20:57:09 @agent_ppo2.py:193][0m |          -0.0100 |           6.6386 |           2.9850 |
[32m[20230203 20:57:09 @agent_ppo2.py:193][0m |          -0.0121 |           6.5514 |           2.9847 |
[32m[20230203 20:57:09 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:57:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 179.73
[32m[20230203 20:57:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.92
[32m[20230203 20:57:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 266.71
[32m[20230203 20:57:09 @agent_ppo2.py:151][0m Total time:       6.56 min
[32m[20230203 20:57:09 @agent_ppo2.py:153][0m 522240 total steps have happened
[32m[20230203 20:57:09 @agent_ppo2.py:129][0m #------------------------ Iteration 255 --------------------------#
[32m[20230203 20:57:10 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:57:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:10 @agent_ppo2.py:193][0m |          -0.0001 |           8.7670 |           3.1697 |
[32m[20230203 20:57:10 @agent_ppo2.py:193][0m |          -0.0063 |           7.5704 |           3.1617 |
[32m[20230203 20:57:10 @agent_ppo2.py:193][0m |          -0.0089 |           7.0327 |           3.1602 |
[32m[20230203 20:57:10 @agent_ppo2.py:193][0m |          -0.0089 |           6.7144 |           3.1571 |
[32m[20230203 20:57:10 @agent_ppo2.py:193][0m |          -0.0116 |           6.5001 |           3.1537 |
[32m[20230203 20:57:10 @agent_ppo2.py:193][0m |          -0.0119 |           6.2442 |           3.1537 |
[32m[20230203 20:57:10 @agent_ppo2.py:193][0m |          -0.0132 |           6.0607 |           3.1523 |
[32m[20230203 20:57:10 @agent_ppo2.py:193][0m |          -0.0143 |           5.8786 |           3.1503 |
[32m[20230203 20:57:10 @agent_ppo2.py:193][0m |          -0.0146 |           5.7072 |           3.1524 |
[32m[20230203 20:57:10 @agent_ppo2.py:193][0m |          -0.0151 |           5.5863 |           3.1505 |
[32m[20230203 20:57:10 @agent_ppo2.py:138][0m Policy update time: 0.54 s
[32m[20230203 20:57:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 236.86
[32m[20230203 20:57:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 238.87
[32m[20230203 20:57:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 263.69
[32m[20230203 20:57:10 @agent_ppo2.py:151][0m Total time:       6.59 min
[32m[20230203 20:57:10 @agent_ppo2.py:153][0m 524288 total steps have happened
[32m[20230203 20:57:10 @agent_ppo2.py:129][0m #------------------------ Iteration 256 --------------------------#
[32m[20230203 20:57:11 @agent_ppo2.py:135][0m Sampling time: 0.52 s by 1 slaves
[32m[20230203 20:57:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:11 @agent_ppo2.py:193][0m |          -0.0008 |          51.4668 |           3.0420 |
[32m[20230203 20:57:11 @agent_ppo2.py:193][0m |          -0.0042 |          36.6882 |           3.0427 |
[32m[20230203 20:57:11 @agent_ppo2.py:193][0m |          -0.0086 |          31.9962 |           3.0402 |
[32m[20230203 20:57:11 @agent_ppo2.py:193][0m |          -0.0075 |          30.0500 |           3.0402 |
[32m[20230203 20:57:11 @agent_ppo2.py:193][0m |          -0.0121 |          28.8241 |           3.0432 |
[32m[20230203 20:57:11 @agent_ppo2.py:193][0m |          -0.0139 |          27.7114 |           3.0406 |
[32m[20230203 20:57:11 @agent_ppo2.py:193][0m |          -0.0129 |          26.9951 |           3.0354 |
[32m[20230203 20:57:11 @agent_ppo2.py:193][0m |          -0.0149 |          26.5557 |           3.0403 |
[32m[20230203 20:57:11 @agent_ppo2.py:193][0m |          -0.0127 |          26.1040 |           3.0360 |
[32m[20230203 20:57:12 @agent_ppo2.py:193][0m |          -0.0157 |          25.7500 |           3.0420 |
[32m[20230203 20:57:12 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:57:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 61.52
[32m[20230203 20:57:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.19
[32m[20230203 20:57:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.81
[32m[20230203 20:57:12 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 267.81
[32m[20230203 20:57:12 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 267.81
[32m[20230203 20:57:12 @agent_ppo2.py:151][0m Total time:       6.61 min
[32m[20230203 20:57:12 @agent_ppo2.py:153][0m 526336 total steps have happened
[32m[20230203 20:57:12 @agent_ppo2.py:129][0m #------------------------ Iteration 257 --------------------------#
[32m[20230203 20:57:12 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:57:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:12 @agent_ppo2.py:193][0m |           0.0008 |          14.3162 |           3.1594 |
[32m[20230203 20:57:12 @agent_ppo2.py:193][0m |          -0.0044 |          11.9410 |           3.1551 |
[32m[20230203 20:57:13 @agent_ppo2.py:193][0m |          -0.0073 |          11.3275 |           3.1517 |
[32m[20230203 20:57:13 @agent_ppo2.py:193][0m |          -0.0092 |          10.9939 |           3.1508 |
[32m[20230203 20:57:13 @agent_ppo2.py:193][0m |          -0.0102 |          10.7923 |           3.1485 |
[32m[20230203 20:57:13 @agent_ppo2.py:193][0m |          -0.0115 |          10.5658 |           3.1474 |
[32m[20230203 20:57:13 @agent_ppo2.py:193][0m |          -0.0110 |          10.4095 |           3.1454 |
[32m[20230203 20:57:13 @agent_ppo2.py:193][0m |          -0.0119 |          10.2865 |           3.1479 |
[32m[20230203 20:57:13 @agent_ppo2.py:193][0m |          -0.0132 |          10.1561 |           3.1450 |
[32m[20230203 20:57:13 @agent_ppo2.py:193][0m |          -0.0133 |          10.0666 |           3.1458 |
[32m[20230203 20:57:13 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 20:57:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.16
[32m[20230203 20:57:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.16
[32m[20230203 20:57:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.91
[32m[20230203 20:57:13 @agent_ppo2.py:151][0m Total time:       6.63 min
[32m[20230203 20:57:13 @agent_ppo2.py:153][0m 528384 total steps have happened
[32m[20230203 20:57:13 @agent_ppo2.py:129][0m #------------------------ Iteration 258 --------------------------#
[32m[20230203 20:57:14 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:57:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:14 @agent_ppo2.py:193][0m |          -0.0047 |           9.7976 |           3.1368 |
[32m[20230203 20:57:14 @agent_ppo2.py:193][0m |          -0.0042 |           8.0371 |           3.1305 |
[32m[20230203 20:57:14 @agent_ppo2.py:193][0m |          -0.0114 |           7.7054 |           3.1291 |
[32m[20230203 20:57:14 @agent_ppo2.py:193][0m |          -0.0102 |           7.6107 |           3.1253 |
[32m[20230203 20:57:14 @agent_ppo2.py:193][0m |          -0.0065 |           7.5245 |           3.1239 |
[32m[20230203 20:57:14 @agent_ppo2.py:193][0m |          -0.0117 |           7.4821 |           3.1234 |
[32m[20230203 20:57:14 @agent_ppo2.py:193][0m |          -0.0126 |           7.4396 |           3.1210 |
[32m[20230203 20:57:14 @agent_ppo2.py:193][0m |          -0.0103 |           7.4310 |           3.1232 |
[32m[20230203 20:57:14 @agent_ppo2.py:193][0m |          -0.0138 |           7.3923 |           3.1235 |
[32m[20230203 20:57:14 @agent_ppo2.py:193][0m |          -0.0098 |           7.3681 |           3.1236 |
[32m[20230203 20:57:14 @agent_ppo2.py:138][0m Policy update time: 0.54 s
[32m[20230203 20:57:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 239.41
[32m[20230203 20:57:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 240.41
[32m[20230203 20:57:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 260.19
[32m[20230203 20:57:15 @agent_ppo2.py:151][0m Total time:       6.66 min
[32m[20230203 20:57:15 @agent_ppo2.py:153][0m 530432 total steps have happened
[32m[20230203 20:57:15 @agent_ppo2.py:129][0m #------------------------ Iteration 259 --------------------------#
[32m[20230203 20:57:15 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:57:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:15 @agent_ppo2.py:193][0m |           0.0057 |           8.1069 |           3.1120 |
[32m[20230203 20:57:15 @agent_ppo2.py:193][0m |          -0.0018 |           7.7479 |           3.1112 |
[32m[20230203 20:57:15 @agent_ppo2.py:193][0m |          -0.0082 |           7.4195 |           3.1091 |
[32m[20230203 20:57:15 @agent_ppo2.py:193][0m |          -0.0091 |           7.3964 |           3.1065 |
[32m[20230203 20:57:15 @agent_ppo2.py:193][0m |          -0.0072 |           7.3689 |           3.1057 |
[32m[20230203 20:57:15 @agent_ppo2.py:193][0m |          -0.0135 |           7.2718 |           3.1041 |
[32m[20230203 20:57:15 @agent_ppo2.py:193][0m |          -0.0132 |           7.2456 |           3.1074 |
[32m[20230203 20:57:15 @agent_ppo2.py:193][0m |          -0.0107 |           7.2506 |           3.1025 |
[32m[20230203 20:57:16 @agent_ppo2.py:193][0m |          -0.0078 |           7.6379 |           3.1049 |
[32m[20230203 20:57:16 @agent_ppo2.py:193][0m |          -0.0099 |           7.4848 |           3.1042 |
[32m[20230203 20:57:16 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 20:57:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 244.31
[32m[20230203 20:57:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.81
[32m[20230203 20:57:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.47
[32m[20230203 20:57:16 @agent_ppo2.py:151][0m Total time:       6.68 min
[32m[20230203 20:57:16 @agent_ppo2.py:153][0m 532480 total steps have happened
[32m[20230203 20:57:16 @agent_ppo2.py:129][0m #------------------------ Iteration 260 --------------------------#
[32m[20230203 20:57:16 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:57:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:16 @agent_ppo2.py:193][0m |           0.0002 |           8.8654 |           3.1367 |
[32m[20230203 20:57:17 @agent_ppo2.py:193][0m |          -0.0136 |           8.3425 |           3.1356 |
[32m[20230203 20:57:17 @agent_ppo2.py:193][0m |          -0.0121 |           8.1099 |           3.1324 |
[32m[20230203 20:57:17 @agent_ppo2.py:193][0m |          -0.0074 |           8.0770 |           3.1315 |
[32m[20230203 20:57:17 @agent_ppo2.py:193][0m |          -0.0136 |           7.8401 |           3.1302 |
[32m[20230203 20:57:17 @agent_ppo2.py:193][0m |           0.0015 |           8.8221 |           3.1313 |
[32m[20230203 20:57:17 @agent_ppo2.py:193][0m |          -0.0122 |           7.6774 |           3.1310 |
[32m[20230203 20:57:17 @agent_ppo2.py:193][0m |          -0.0137 |           7.5680 |           3.1304 |
[32m[20230203 20:57:17 @agent_ppo2.py:193][0m |          -0.0139 |           7.4876 |           3.1314 |
[32m[20230203 20:57:17 @agent_ppo2.py:193][0m |          -0.0131 |           7.4365 |           3.1306 |
[32m[20230203 20:57:17 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:57:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 243.83
[32m[20230203 20:57:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.31
[32m[20230203 20:57:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.36
[32m[20230203 20:57:17 @agent_ppo2.py:151][0m Total time:       6.70 min
[32m[20230203 20:57:17 @agent_ppo2.py:153][0m 534528 total steps have happened
[32m[20230203 20:57:17 @agent_ppo2.py:129][0m #------------------------ Iteration 261 --------------------------#
[32m[20230203 20:57:18 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 20:57:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:18 @agent_ppo2.py:193][0m |          -0.0040 |          21.5679 |           3.1686 |
[32m[20230203 20:57:18 @agent_ppo2.py:193][0m |          -0.0008 |          12.1773 |           3.1684 |
[32m[20230203 20:57:18 @agent_ppo2.py:193][0m |          -0.0059 |          10.6025 |           3.1705 |
[32m[20230203 20:57:18 @agent_ppo2.py:193][0m |          -0.0081 |          10.0474 |           3.1707 |
[32m[20230203 20:57:18 @agent_ppo2.py:193][0m |          -0.0099 |           9.6899 |           3.1694 |
[32m[20230203 20:57:18 @agent_ppo2.py:193][0m |           0.0079 |           9.4168 |           3.1704 |
[32m[20230203 20:57:18 @agent_ppo2.py:193][0m |          -0.0102 |           9.0735 |           3.1707 |
[32m[20230203 20:57:18 @agent_ppo2.py:193][0m |          -0.0096 |           8.8230 |           3.1693 |
[32m[20230203 20:57:18 @agent_ppo2.py:193][0m |          -0.0051 |           8.8822 |           3.1686 |
[32m[20230203 20:57:18 @agent_ppo2.py:193][0m |          -0.0114 |           8.5319 |           3.1681 |
[32m[20230203 20:57:18 @agent_ppo2.py:138][0m Policy update time: 0.44 s
[32m[20230203 20:57:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: 163.09
[32m[20230203 20:57:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.36
[32m[20230203 20:57:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.61
[32m[20230203 20:57:18 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 269.61
[32m[20230203 20:57:18 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 269.61
[32m[20230203 20:57:18 @agent_ppo2.py:151][0m Total time:       6.72 min
[32m[20230203 20:57:18 @agent_ppo2.py:153][0m 536576 total steps have happened
[32m[20230203 20:57:18 @agent_ppo2.py:129][0m #------------------------ Iteration 262 --------------------------#
[32m[20230203 20:57:19 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:57:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:19 @agent_ppo2.py:193][0m |          -0.0039 |          24.3015 |           3.1413 |
[32m[20230203 20:57:19 @agent_ppo2.py:193][0m |          -0.0094 |          17.0176 |           3.1377 |
[32m[20230203 20:57:19 @agent_ppo2.py:193][0m |          -0.0130 |          15.0506 |           3.1324 |
[32m[20230203 20:57:19 @agent_ppo2.py:193][0m |          -0.0097 |          14.2483 |           3.1331 |
[32m[20230203 20:57:19 @agent_ppo2.py:193][0m |          -0.0143 |          14.0006 |           3.1310 |
[32m[20230203 20:57:19 @agent_ppo2.py:193][0m |          -0.0155 |          13.3514 |           3.1301 |
[32m[20230203 20:57:19 @agent_ppo2.py:193][0m |          -0.0169 |          13.0743 |           3.1249 |
[32m[20230203 20:57:19 @agent_ppo2.py:193][0m |          -0.0159 |          12.6622 |           3.1270 |
[32m[20230203 20:57:19 @agent_ppo2.py:193][0m |          -0.0173 |          12.4311 |           3.1213 |
[32m[20230203 20:57:19 @agent_ppo2.py:193][0m |          -0.0199 |          12.1690 |           3.1199 |
[32m[20230203 20:57:19 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 20:57:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 147.78
[32m[20230203 20:57:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.67
[32m[20230203 20:57:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 214.66
[32m[20230203 20:57:20 @agent_ppo2.py:151][0m Total time:       6.74 min
[32m[20230203 20:57:20 @agent_ppo2.py:153][0m 538624 total steps have happened
[32m[20230203 20:57:20 @agent_ppo2.py:129][0m #------------------------ Iteration 263 --------------------------#
[32m[20230203 20:57:20 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:57:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:20 @agent_ppo2.py:193][0m |          -0.0084 |          11.6255 |           3.0443 |
[32m[20230203 20:57:20 @agent_ppo2.py:193][0m |          -0.0086 |          10.5257 |           3.0397 |
[32m[20230203 20:57:20 @agent_ppo2.py:193][0m |          -0.0282 |          10.2416 |           3.0408 |
[32m[20230203 20:57:20 @agent_ppo2.py:193][0m |          -0.0145 |           9.9290 |           3.0380 |
[32m[20230203 20:57:20 @agent_ppo2.py:193][0m |          -0.0187 |           9.7012 |           3.0390 |
[32m[20230203 20:57:20 @agent_ppo2.py:193][0m |          -0.0198 |           9.6293 |           3.0380 |
[32m[20230203 20:57:20 @agent_ppo2.py:193][0m |          -0.0068 |           9.6159 |           3.0368 |
[32m[20230203 20:57:20 @agent_ppo2.py:193][0m |          -0.0317 |           9.3750 |           3.0338 |
[32m[20230203 20:57:20 @agent_ppo2.py:193][0m |          -0.0157 |           9.2850 |           3.0350 |
[32m[20230203 20:57:21 @agent_ppo2.py:193][0m |          -0.0241 |           9.1952 |           3.0343 |
[32m[20230203 20:57:21 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:57:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.61
[32m[20230203 20:57:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.82
[32m[20230203 20:57:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 185.37
[32m[20230203 20:57:21 @agent_ppo2.py:151][0m Total time:       6.76 min
[32m[20230203 20:57:21 @agent_ppo2.py:153][0m 540672 total steps have happened
[32m[20230203 20:57:21 @agent_ppo2.py:129][0m #------------------------ Iteration 264 --------------------------#
[32m[20230203 20:57:21 @agent_ppo2.py:135][0m Sampling time: 0.48 s by 1 slaves
[32m[20230203 20:57:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:21 @agent_ppo2.py:193][0m |           0.0027 |          15.1018 |           3.0721 |
[32m[20230203 20:57:21 @agent_ppo2.py:193][0m |          -0.0091 |          10.6266 |           3.0627 |
[32m[20230203 20:57:22 @agent_ppo2.py:193][0m |          -0.0144 |           9.9641 |           3.0610 |
[32m[20230203 20:57:22 @agent_ppo2.py:193][0m |          -0.0117 |           9.6132 |           3.0577 |
[32m[20230203 20:57:22 @agent_ppo2.py:193][0m |          -0.0168 |           9.3265 |           3.0571 |
[32m[20230203 20:57:22 @agent_ppo2.py:193][0m |          -0.0145 |           9.2367 |           3.0568 |
[32m[20230203 20:57:22 @agent_ppo2.py:193][0m |          -0.0171 |           8.9504 |           3.0555 |
[32m[20230203 20:57:22 @agent_ppo2.py:193][0m |          -0.0153 |           8.9378 |           3.0537 |
[32m[20230203 20:57:22 @agent_ppo2.py:193][0m |          -0.0194 |           8.7924 |           3.0523 |
[32m[20230203 20:57:22 @agent_ppo2.py:193][0m |          -0.0171 |           8.8235 |           3.0528 |
[32m[20230203 20:57:22 @agent_ppo2.py:138][0m Policy update time: 0.56 s
[32m[20230203 20:57:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 134.29
[32m[20230203 20:57:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.78
[32m[20230203 20:57:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 266.96
[32m[20230203 20:57:22 @agent_ppo2.py:151][0m Total time:       6.78 min
[32m[20230203 20:57:22 @agent_ppo2.py:153][0m 542720 total steps have happened
[32m[20230203 20:57:22 @agent_ppo2.py:129][0m #------------------------ Iteration 265 --------------------------#
[32m[20230203 20:57:23 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:57:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:23 @agent_ppo2.py:193][0m |          -0.0028 |          32.0894 |           3.0931 |
[32m[20230203 20:57:23 @agent_ppo2.py:193][0m |          -0.0070 |          27.5242 |           3.0877 |
[32m[20230203 20:57:23 @agent_ppo2.py:193][0m |          -0.0107 |          24.9955 |           3.0883 |
[32m[20230203 20:57:23 @agent_ppo2.py:193][0m |          -0.0106 |          23.1229 |           3.0901 |
[32m[20230203 20:57:23 @agent_ppo2.py:193][0m |          -0.0117 |          22.2539 |           3.0865 |
[32m[20230203 20:57:23 @agent_ppo2.py:193][0m |          -0.0127 |          21.2246 |           3.0897 |
[32m[20230203 20:57:23 @agent_ppo2.py:193][0m |          -0.0127 |          19.8371 |           3.0870 |
[32m[20230203 20:57:23 @agent_ppo2.py:193][0m |          -0.0154 |          19.6908 |           3.0882 |
[32m[20230203 20:57:23 @agent_ppo2.py:193][0m |          -0.0129 |          18.8362 |           3.0895 |
[32m[20230203 20:57:23 @agent_ppo2.py:193][0m |          -0.0150 |          18.4148 |           3.0886 |
[32m[20230203 20:57:23 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:57:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 178.00
[32m[20230203 20:57:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 243.86
[32m[20230203 20:57:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 263.10
[32m[20230203 20:57:24 @agent_ppo2.py:151][0m Total time:       6.80 min
[32m[20230203 20:57:24 @agent_ppo2.py:153][0m 544768 total steps have happened
[32m[20230203 20:57:24 @agent_ppo2.py:129][0m #------------------------ Iteration 266 --------------------------#
[32m[20230203 20:57:24 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:57:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:24 @agent_ppo2.py:193][0m |          -0.0062 |           9.6966 |           3.1079 |
[32m[20230203 20:57:24 @agent_ppo2.py:193][0m |          -0.0058 |           8.7639 |           3.1056 |
[32m[20230203 20:57:24 @agent_ppo2.py:193][0m |          -0.0083 |           8.3870 |           3.1024 |
[32m[20230203 20:57:24 @agent_ppo2.py:193][0m |          -0.0091 |           8.1788 |           3.0986 |
[32m[20230203 20:57:24 @agent_ppo2.py:193][0m |          -0.0078 |           8.1702 |           3.1015 |
[32m[20230203 20:57:24 @agent_ppo2.py:193][0m |          -0.0133 |           7.8072 |           3.1007 |
[32m[20230203 20:57:24 @agent_ppo2.py:193][0m |          -0.0121 |           7.6768 |           3.0979 |
[32m[20230203 20:57:24 @agent_ppo2.py:193][0m |          -0.0104 |           7.8302 |           3.1011 |
[32m[20230203 20:57:24 @agent_ppo2.py:193][0m |          -0.0123 |           7.4124 |           3.0993 |
[32m[20230203 20:57:24 @agent_ppo2.py:193][0m |          -0.0107 |           7.3836 |           3.1008 |
[32m[20230203 20:57:24 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:57:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 243.33
[32m[20230203 20:57:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.94
[32m[20230203 20:57:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.95
[32m[20230203 20:57:25 @agent_ppo2.py:151][0m Total time:       6.83 min
[32m[20230203 20:57:25 @agent_ppo2.py:153][0m 546816 total steps have happened
[32m[20230203 20:57:25 @agent_ppo2.py:129][0m #------------------------ Iteration 267 --------------------------#
[32m[20230203 20:57:25 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:57:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:25 @agent_ppo2.py:193][0m |           0.0001 |           7.9266 |           3.0156 |
[32m[20230203 20:57:25 @agent_ppo2.py:193][0m |          -0.0061 |           7.3887 |           3.0113 |
[32m[20230203 20:57:26 @agent_ppo2.py:193][0m |          -0.0082 |           7.1668 |           3.0132 |
[32m[20230203 20:57:26 @agent_ppo2.py:193][0m |          -0.0101 |           6.9781 |           3.0130 |
[32m[20230203 20:57:26 @agent_ppo2.py:193][0m |          -0.0112 |           6.8433 |           3.0162 |
[32m[20230203 20:57:26 @agent_ppo2.py:193][0m |          -0.0123 |           6.7456 |           3.0152 |
[32m[20230203 20:57:26 @agent_ppo2.py:193][0m |          -0.0122 |           6.6588 |           3.0173 |
[32m[20230203 20:57:26 @agent_ppo2.py:193][0m |          -0.0131 |           6.6226 |           3.0140 |
[32m[20230203 20:57:26 @agent_ppo2.py:193][0m |          -0.0139 |           6.5524 |           3.0166 |
[32m[20230203 20:57:26 @agent_ppo2.py:193][0m |          -0.0136 |           6.4937 |           3.0163 |
[32m[20230203 20:57:26 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:57:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 243.50
[32m[20230203 20:57:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.11
[32m[20230203 20:57:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.07
[32m[20230203 20:57:26 @agent_ppo2.py:151][0m Total time:       6.85 min
[32m[20230203 20:57:26 @agent_ppo2.py:153][0m 548864 total steps have happened
[32m[20230203 20:57:26 @agent_ppo2.py:129][0m #------------------------ Iteration 268 --------------------------#
[32m[20230203 20:57:27 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:57:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:27 @agent_ppo2.py:193][0m |           0.0057 |           8.6022 |           3.0770 |
[32m[20230203 20:57:27 @agent_ppo2.py:193][0m |          -0.0036 |           8.0829 |           3.0769 |
[32m[20230203 20:57:27 @agent_ppo2.py:193][0m |          -0.0054 |           7.9179 |           3.0749 |
[32m[20230203 20:57:27 @agent_ppo2.py:193][0m |          -0.0046 |           7.7928 |           3.0758 |
[32m[20230203 20:57:27 @agent_ppo2.py:193][0m |          -0.0135 |           7.7489 |           3.0746 |
[32m[20230203 20:57:27 @agent_ppo2.py:193][0m |          -0.0036 |           7.7771 |           3.0759 |
[32m[20230203 20:57:27 @agent_ppo2.py:193][0m |          -0.0150 |           7.6535 |           3.0746 |
[32m[20230203 20:57:27 @agent_ppo2.py:193][0m |          -0.0140 |           7.5931 |           3.0750 |
[32m[20230203 20:57:27 @agent_ppo2.py:193][0m |          -0.0160 |           7.5695 |           3.0748 |
[32m[20230203 20:57:27 @agent_ppo2.py:193][0m |          -0.0136 |           7.5170 |           3.0756 |
[32m[20230203 20:57:27 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:57:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 241.25
[32m[20230203 20:57:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 243.59
[32m[20230203 20:57:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.36
[32m[20230203 20:57:28 @agent_ppo2.py:151][0m Total time:       6.87 min
[32m[20230203 20:57:28 @agent_ppo2.py:153][0m 550912 total steps have happened
[32m[20230203 20:57:28 @agent_ppo2.py:129][0m #------------------------ Iteration 269 --------------------------#
[32m[20230203 20:57:28 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:57:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:28 @agent_ppo2.py:193][0m |          -0.0002 |           7.8882 |           3.0467 |
[32m[20230203 20:57:28 @agent_ppo2.py:193][0m |          -0.0045 |           7.5594 |           3.0406 |
[32m[20230203 20:57:28 @agent_ppo2.py:193][0m |          -0.0080 |           7.3989 |           3.0418 |
[32m[20230203 20:57:28 @agent_ppo2.py:193][0m |          -0.0086 |           7.3301 |           3.0384 |
[32m[20230203 20:57:28 @agent_ppo2.py:193][0m |          -0.0031 |           7.2945 |           3.0390 |
[32m[20230203 20:57:28 @agent_ppo2.py:193][0m |          -0.0097 |           7.1809 |           3.0393 |
[32m[20230203 20:57:28 @agent_ppo2.py:193][0m |          -0.0102 |           7.1032 |           3.0403 |
[32m[20230203 20:57:28 @agent_ppo2.py:193][0m |          -0.0094 |           7.1127 |           3.0387 |
[32m[20230203 20:57:28 @agent_ppo2.py:193][0m |          -0.0108 |           6.9976 |           3.0384 |
[32m[20230203 20:57:29 @agent_ppo2.py:193][0m |          -0.0123 |           6.9355 |           3.0392 |
[32m[20230203 20:57:29 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:57:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.42
[32m[20230203 20:57:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.68
[32m[20230203 20:57:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.82
[32m[20230203 20:57:29 @agent_ppo2.py:151][0m Total time:       6.89 min
[32m[20230203 20:57:29 @agent_ppo2.py:153][0m 552960 total steps have happened
[32m[20230203 20:57:29 @agent_ppo2.py:129][0m #------------------------ Iteration 270 --------------------------#
[32m[20230203 20:57:29 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:57:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:29 @agent_ppo2.py:193][0m |           0.0065 |          14.1431 |           3.1146 |
[32m[20230203 20:57:29 @agent_ppo2.py:193][0m |          -0.0023 |           7.0326 |           3.1108 |
[32m[20230203 20:57:29 @agent_ppo2.py:193][0m |          -0.0054 |           6.6577 |           3.1065 |
[32m[20230203 20:57:30 @agent_ppo2.py:193][0m |          -0.0073 |           6.4187 |           3.1033 |
[32m[20230203 20:57:30 @agent_ppo2.py:193][0m |          -0.0077 |           6.3039 |           3.0984 |
[32m[20230203 20:57:30 @agent_ppo2.py:193][0m |          -0.0059 |           6.7850 |           3.0975 |
[32m[20230203 20:57:30 @agent_ppo2.py:193][0m |          -0.0066 |           6.0637 |           3.0961 |
[32m[20230203 20:57:30 @agent_ppo2.py:193][0m |          -0.0119 |           5.9408 |           3.0905 |
[32m[20230203 20:57:30 @agent_ppo2.py:193][0m |          -0.0110 |           5.8968 |           3.0895 |
[32m[20230203 20:57:30 @agent_ppo2.py:193][0m |          -0.0085 |           5.8398 |           3.0884 |
[32m[20230203 20:57:30 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:57:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 172.24
[32m[20230203 20:57:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.62
[32m[20230203 20:57:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 199.89
[32m[20230203 20:57:30 @agent_ppo2.py:151][0m Total time:       6.92 min
[32m[20230203 20:57:30 @agent_ppo2.py:153][0m 555008 total steps have happened
[32m[20230203 20:57:30 @agent_ppo2.py:129][0m #------------------------ Iteration 271 --------------------------#
[32m[20230203 20:57:31 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:57:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:31 @agent_ppo2.py:193][0m |          -0.0020 |          12.8956 |           3.0639 |
[32m[20230203 20:57:31 @agent_ppo2.py:193][0m |          -0.0314 |           8.9809 |           3.0592 |
[32m[20230203 20:57:31 @agent_ppo2.py:193][0m |          -0.0015 |           8.2652 |           3.0470 |
[32m[20230203 20:57:31 @agent_ppo2.py:193][0m |          -0.0219 |           8.0374 |           3.0560 |
[32m[20230203 20:57:31 @agent_ppo2.py:193][0m |          -0.0099 |           7.8739 |           3.0587 |
[32m[20230203 20:57:31 @agent_ppo2.py:193][0m |          -0.0031 |           8.0460 |           3.0565 |
[32m[20230203 20:57:31 @agent_ppo2.py:193][0m |          -0.0127 |           7.8047 |           3.0603 |
[32m[20230203 20:57:31 @agent_ppo2.py:193][0m |          -0.0121 |           7.6517 |           3.0589 |
[32m[20230203 20:57:31 @agent_ppo2.py:193][0m |          -0.0001 |           8.7473 |           3.0581 |
[32m[20230203 20:57:31 @agent_ppo2.py:193][0m |          -0.0140 |           7.5935 |           3.0572 |
[32m[20230203 20:57:31 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 20:57:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 243.83
[32m[20230203 20:57:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.07
[32m[20230203 20:57:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.61
[32m[20230203 20:57:32 @agent_ppo2.py:151][0m Total time:       6.94 min
[32m[20230203 20:57:32 @agent_ppo2.py:153][0m 557056 total steps have happened
[32m[20230203 20:57:32 @agent_ppo2.py:129][0m #------------------------ Iteration 272 --------------------------#
[32m[20230203 20:57:32 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:57:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:32 @agent_ppo2.py:193][0m |          -0.0011 |           9.1179 |           3.1025 |
[32m[20230203 20:57:32 @agent_ppo2.py:193][0m |          -0.0073 |           8.7387 |           3.0919 |
[32m[20230203 20:57:32 @agent_ppo2.py:193][0m |          -0.0075 |           8.5935 |           3.0860 |
[32m[20230203 20:57:32 @agent_ppo2.py:193][0m |          -0.0082 |           8.5632 |           3.0852 |
[32m[20230203 20:57:32 @agent_ppo2.py:193][0m |          -0.0114 |           8.3884 |           3.0838 |
[32m[20230203 20:57:32 @agent_ppo2.py:193][0m |          -0.0091 |           8.3613 |           3.0795 |
[32m[20230203 20:57:32 @agent_ppo2.py:193][0m |          -0.0116 |           8.3054 |           3.0801 |
[32m[20230203 20:57:32 @agent_ppo2.py:193][0m |          -0.0124 |           8.1946 |           3.0798 |
[32m[20230203 20:57:32 @agent_ppo2.py:193][0m |          -0.0110 |           8.3699 |           3.0791 |
[32m[20230203 20:57:32 @agent_ppo2.py:193][0m |          -0.0113 |           8.2056 |           3.0774 |
[32m[20230203 20:57:32 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:57:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.56
[32m[20230203 20:57:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.05
[32m[20230203 20:57:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.46
[32m[20230203 20:57:33 @agent_ppo2.py:151][0m Total time:       6.96 min
[32m[20230203 20:57:33 @agent_ppo2.py:153][0m 559104 total steps have happened
[32m[20230203 20:57:33 @agent_ppo2.py:129][0m #------------------------ Iteration 273 --------------------------#
[32m[20230203 20:57:33 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 20:57:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:33 @agent_ppo2.py:193][0m |           0.0003 |          28.2171 |           3.0816 |
[32m[20230203 20:57:33 @agent_ppo2.py:193][0m |          -0.0061 |          20.7310 |           3.0606 |
[32m[20230203 20:57:33 @agent_ppo2.py:193][0m |          -0.0100 |          17.8385 |           3.0789 |
[32m[20230203 20:57:34 @agent_ppo2.py:193][0m |          -0.0112 |          16.4742 |           3.0763 |
[32m[20230203 20:57:34 @agent_ppo2.py:193][0m |          -0.0115 |          15.6763 |           3.0768 |
[32m[20230203 20:57:34 @agent_ppo2.py:193][0m |          -0.0126 |          14.8243 |           3.0787 |
[32m[20230203 20:57:34 @agent_ppo2.py:193][0m |          -0.0121 |          14.1660 |           3.0796 |
[32m[20230203 20:57:34 @agent_ppo2.py:193][0m |          -0.0141 |          13.7690 |           3.0770 |
[32m[20230203 20:57:34 @agent_ppo2.py:193][0m |          -0.0146 |          13.4049 |           3.0810 |
[32m[20230203 20:57:34 @agent_ppo2.py:193][0m |          -0.0149 |          13.0690 |           3.0801 |
[32m[20230203 20:57:34 @agent_ppo2.py:138][0m Policy update time: 0.54 s
[32m[20230203 20:57:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 95.86
[32m[20230203 20:57:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.79
[32m[20230203 20:57:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.83
[32m[20230203 20:57:34 @agent_ppo2.py:151][0m Total time:       6.98 min
[32m[20230203 20:57:34 @agent_ppo2.py:153][0m 561152 total steps have happened
[32m[20230203 20:57:34 @agent_ppo2.py:129][0m #------------------------ Iteration 274 --------------------------#
[32m[20230203 20:57:35 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:57:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:35 @agent_ppo2.py:193][0m |          -0.0042 |          12.5876 |           3.1152 |
[32m[20230203 20:57:35 @agent_ppo2.py:193][0m |          -0.0046 |          10.1939 |           3.1108 |
[32m[20230203 20:57:35 @agent_ppo2.py:193][0m |          -0.0096 |           9.8122 |           3.1040 |
[32m[20230203 20:57:35 @agent_ppo2.py:193][0m |          -0.0099 |           9.6231 |           3.1045 |
[32m[20230203 20:57:35 @agent_ppo2.py:193][0m |          -0.0149 |           9.5191 |           3.1006 |
[32m[20230203 20:57:35 @agent_ppo2.py:193][0m |          -0.0124 |           9.3636 |           3.1027 |
[32m[20230203 20:57:35 @agent_ppo2.py:193][0m |          -0.0036 |           9.6150 |           3.1022 |
[32m[20230203 20:57:35 @agent_ppo2.py:193][0m |          -0.0132 |           9.2573 |           3.1010 |
[32m[20230203 20:57:35 @agent_ppo2.py:193][0m |          -0.0105 |           9.2709 |           3.1030 |
[32m[20230203 20:57:35 @agent_ppo2.py:193][0m |          -0.0151 |           9.2054 |           3.1009 |
[32m[20230203 20:57:35 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:57:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.04
[32m[20230203 20:57:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.53
[32m[20230203 20:57:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.03
[32m[20230203 20:57:36 @agent_ppo2.py:151][0m Total time:       7.00 min
[32m[20230203 20:57:36 @agent_ppo2.py:153][0m 563200 total steps have happened
[32m[20230203 20:57:36 @agent_ppo2.py:129][0m #------------------------ Iteration 275 --------------------------#
[32m[20230203 20:57:36 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:57:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:36 @agent_ppo2.py:193][0m |           0.0020 |          20.5709 |           3.0585 |
[32m[20230203 20:57:36 @agent_ppo2.py:193][0m |           0.0002 |          12.5396 |           3.0619 |
[32m[20230203 20:57:36 @agent_ppo2.py:193][0m |          -0.0049 |          11.0990 |           3.0489 |
[32m[20230203 20:57:36 @agent_ppo2.py:193][0m |          -0.0064 |          10.3733 |           3.0530 |
[32m[20230203 20:57:36 @agent_ppo2.py:193][0m |          -0.0088 |           9.9654 |           3.0490 |
[32m[20230203 20:57:36 @agent_ppo2.py:193][0m |          -0.0106 |           9.7297 |           3.0444 |
[32m[20230203 20:57:36 @agent_ppo2.py:193][0m |          -0.0101 |           9.5355 |           3.0428 |
[32m[20230203 20:57:36 @agent_ppo2.py:193][0m |          -0.0121 |           9.3643 |           3.0447 |
[32m[20230203 20:57:36 @agent_ppo2.py:193][0m |          -0.0117 |           9.2077 |           3.0432 |
[32m[20230203 20:57:36 @agent_ppo2.py:193][0m |          -0.0126 |           9.1244 |           3.0409 |
[32m[20230203 20:57:36 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 20:57:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 161.61
[32m[20230203 20:57:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.09
[32m[20230203 20:57:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.33
[32m[20230203 20:57:37 @agent_ppo2.py:151][0m Total time:       7.02 min
[32m[20230203 20:57:37 @agent_ppo2.py:153][0m 565248 total steps have happened
[32m[20230203 20:57:37 @agent_ppo2.py:129][0m #------------------------ Iteration 276 --------------------------#
[32m[20230203 20:57:37 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:57:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:37 @agent_ppo2.py:193][0m |           0.0015 |           9.0446 |           3.0723 |
[32m[20230203 20:57:37 @agent_ppo2.py:193][0m |          -0.0072 |           8.5171 |           3.0659 |
[32m[20230203 20:57:37 @agent_ppo2.py:193][0m |          -0.0108 |           8.3482 |           3.0607 |
[32m[20230203 20:57:37 @agent_ppo2.py:193][0m |          -0.0092 |           8.2428 |           3.0622 |
[32m[20230203 20:57:37 @agent_ppo2.py:193][0m |          -0.0089 |           8.1345 |           3.0592 |
[32m[20230203 20:57:37 @agent_ppo2.py:193][0m |          -0.0141 |           8.0746 |           3.0575 |
[32m[20230203 20:57:38 @agent_ppo2.py:193][0m |          -0.0112 |           7.9957 |           3.0581 |
[32m[20230203 20:57:38 @agent_ppo2.py:193][0m |          -0.0163 |           7.9161 |           3.0577 |
[32m[20230203 20:57:38 @agent_ppo2.py:193][0m |           0.0120 |           9.2619 |           3.0562 |
[32m[20230203 20:57:38 @agent_ppo2.py:193][0m |          -0.0172 |           7.8667 |           3.0513 |
[32m[20230203 20:57:38 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:57:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.88
[32m[20230203 20:57:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.01
[32m[20230203 20:57:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 266.39
[32m[20230203 20:57:38 @agent_ppo2.py:151][0m Total time:       7.05 min
[32m[20230203 20:57:38 @agent_ppo2.py:153][0m 567296 total steps have happened
[32m[20230203 20:57:38 @agent_ppo2.py:129][0m #------------------------ Iteration 277 --------------------------#
[32m[20230203 20:57:38 @agent_ppo2.py:135][0m Sampling time: 0.51 s by 1 slaves
[32m[20230203 20:57:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:39 @agent_ppo2.py:193][0m |          -0.0078 |          35.8259 |           3.0934 |
[32m[20230203 20:57:39 @agent_ppo2.py:193][0m |          -0.0110 |          24.9924 |           3.0878 |
[32m[20230203 20:57:39 @agent_ppo2.py:193][0m |          -0.0141 |          19.0966 |           3.0870 |
[32m[20230203 20:57:39 @agent_ppo2.py:193][0m |          -0.0142 |          16.3018 |           3.0847 |
[32m[20230203 20:57:39 @agent_ppo2.py:193][0m |          -0.0138 |          14.5326 |           3.0811 |
[32m[20230203 20:57:39 @agent_ppo2.py:193][0m |          -0.0112 |          13.5461 |           3.0830 |
[32m[20230203 20:57:39 @agent_ppo2.py:193][0m |          -0.0138 |          12.3930 |           3.0824 |
[32m[20230203 20:57:39 @agent_ppo2.py:193][0m |          -0.0107 |          11.7532 |           3.0834 |
[32m[20230203 20:57:39 @agent_ppo2.py:193][0m |          -0.0161 |          11.3481 |           3.0786 |
[32m[20230203 20:57:39 @agent_ppo2.py:193][0m |          -0.0154 |          12.6463 |           3.0817 |
[32m[20230203 20:57:39 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:57:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 166.29
[32m[20230203 20:57:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.50
[32m[20230203 20:57:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 260.94
[32m[20230203 20:57:39 @agent_ppo2.py:151][0m Total time:       7.07 min
[32m[20230203 20:57:39 @agent_ppo2.py:153][0m 569344 total steps have happened
[32m[20230203 20:57:39 @agent_ppo2.py:129][0m #------------------------ Iteration 278 --------------------------#
[32m[20230203 20:57:40 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:57:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:40 @agent_ppo2.py:193][0m |          -0.0007 |          13.5799 |           3.0608 |
[32m[20230203 20:57:40 @agent_ppo2.py:193][0m |          -0.0075 |          11.2959 |           3.0545 |
[32m[20230203 20:57:40 @agent_ppo2.py:193][0m |          -0.0073 |          10.6599 |           3.0524 |
[32m[20230203 20:57:40 @agent_ppo2.py:193][0m |          -0.0070 |          10.0910 |           3.0493 |
[32m[20230203 20:57:40 @agent_ppo2.py:193][0m |          -0.0096 |          10.0068 |           3.0498 |
[32m[20230203 20:57:40 @agent_ppo2.py:193][0m |          -0.0109 |           9.5926 |           3.0516 |
[32m[20230203 20:57:40 @agent_ppo2.py:193][0m |          -0.0167 |           9.4610 |           3.0508 |
[32m[20230203 20:57:40 @agent_ppo2.py:193][0m |          -0.0090 |           9.4381 |           3.0475 |
[32m[20230203 20:57:40 @agent_ppo2.py:193][0m |          -0.0104 |           9.6183 |           3.0462 |
[32m[20230203 20:57:40 @agent_ppo2.py:193][0m |          -0.0219 |           9.3482 |           3.0437 |
[32m[20230203 20:57:40 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:57:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.46
[32m[20230203 20:57:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.53
[32m[20230203 20:57:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.19
[32m[20230203 20:57:41 @agent_ppo2.py:151][0m Total time:       7.09 min
[32m[20230203 20:57:41 @agent_ppo2.py:153][0m 571392 total steps have happened
[32m[20230203 20:57:41 @agent_ppo2.py:129][0m #------------------------ Iteration 279 --------------------------#
[32m[20230203 20:57:41 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:57:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:41 @agent_ppo2.py:193][0m |           0.0007 |          24.3863 |           3.1649 |
[32m[20230203 20:57:41 @agent_ppo2.py:193][0m |          -0.0037 |          14.7478 |           3.1701 |
[32m[20230203 20:57:41 @agent_ppo2.py:193][0m |          -0.0061 |          13.9618 |           3.1679 |
[32m[20230203 20:57:41 @agent_ppo2.py:193][0m |          -0.0090 |          13.6259 |           3.1661 |
[32m[20230203 20:57:41 @agent_ppo2.py:193][0m |          -0.0096 |          13.5179 |           3.1645 |
[32m[20230203 20:57:41 @agent_ppo2.py:193][0m |          -0.0111 |          13.1945 |           3.1661 |
[32m[20230203 20:57:42 @agent_ppo2.py:193][0m |          -0.0107 |          13.1308 |           3.1667 |
[32m[20230203 20:57:42 @agent_ppo2.py:193][0m |          -0.0114 |          12.9056 |           3.1661 |
[32m[20230203 20:57:42 @agent_ppo2.py:193][0m |          -0.0119 |          12.7583 |           3.1668 |
[32m[20230203 20:57:42 @agent_ppo2.py:193][0m |          -0.0128 |          12.5651 |           3.1649 |
[32m[20230203 20:57:42 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:57:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 178.44
[32m[20230203 20:57:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.49
[32m[20230203 20:57:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 263.69
[32m[20230203 20:57:42 @agent_ppo2.py:151][0m Total time:       7.11 min
[32m[20230203 20:57:42 @agent_ppo2.py:153][0m 573440 total steps have happened
[32m[20230203 20:57:42 @agent_ppo2.py:129][0m #------------------------ Iteration 280 --------------------------#
[32m[20230203 20:57:43 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:57:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:43 @agent_ppo2.py:193][0m |          -0.0021 |          33.4628 |           3.0810 |
[32m[20230203 20:57:43 @agent_ppo2.py:193][0m |          -0.0055 |          31.0380 |           3.0783 |
[32m[20230203 20:57:43 @agent_ppo2.py:193][0m |          -0.0066 |          29.4311 |           3.0786 |
[32m[20230203 20:57:43 @agent_ppo2.py:193][0m |          -0.0086 |          28.0775 |           3.0734 |
[32m[20230203 20:57:43 @agent_ppo2.py:193][0m |          -0.0077 |          27.4932 |           3.0770 |
[32m[20230203 20:57:43 @agent_ppo2.py:193][0m |          -0.0117 |          25.7427 |           3.0773 |
[32m[20230203 20:57:43 @agent_ppo2.py:193][0m |          -0.0102 |          25.1064 |           3.0774 |
[32m[20230203 20:57:43 @agent_ppo2.py:193][0m |          -0.0123 |          24.2815 |           3.0736 |
[32m[20230203 20:57:43 @agent_ppo2.py:193][0m |          -0.0112 |          23.8904 |           3.0738 |
[32m[20230203 20:57:43 @agent_ppo2.py:193][0m |          -0.0123 |          23.3335 |           3.0755 |
[32m[20230203 20:57:43 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:57:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 162.06
[32m[20230203 20:57:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.01
[32m[20230203 20:57:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.01
[32m[20230203 20:57:44 @agent_ppo2.py:151][0m Total time:       7.14 min
[32m[20230203 20:57:44 @agent_ppo2.py:153][0m 575488 total steps have happened
[32m[20230203 20:57:44 @agent_ppo2.py:129][0m #------------------------ Iteration 281 --------------------------#
[32m[20230203 20:57:44 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:57:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:44 @agent_ppo2.py:193][0m |           0.0067 |          19.7468 |           3.1041 |
[32m[20230203 20:57:44 @agent_ppo2.py:193][0m |          -0.0072 |          13.4530 |           3.1015 |
[32m[20230203 20:57:44 @agent_ppo2.py:193][0m |          -0.0118 |          12.4743 |           3.0945 |
[32m[20230203 20:57:44 @agent_ppo2.py:193][0m |           0.0012 |          12.4606 |           3.0944 |
[32m[20230203 20:57:44 @agent_ppo2.py:193][0m |          -0.0131 |          11.7434 |           3.0943 |
[32m[20230203 20:57:44 @agent_ppo2.py:193][0m |          -0.0149 |          11.5765 |           3.0941 |
[32m[20230203 20:57:44 @agent_ppo2.py:193][0m |          -0.0101 |          11.4788 |           3.0926 |
[32m[20230203 20:57:44 @agent_ppo2.py:193][0m |          -0.0123 |          11.8855 |           3.0949 |
[32m[20230203 20:57:44 @agent_ppo2.py:193][0m |          -0.0175 |          11.1442 |           3.0897 |
[32m[20230203 20:57:44 @agent_ppo2.py:193][0m |          -0.0163 |          11.1407 |           3.0940 |
[32m[20230203 20:57:44 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:57:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 135.59
[32m[20230203 20:57:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.77
[32m[20230203 20:57:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.03
[32m[20230203 20:57:45 @agent_ppo2.py:151][0m Total time:       7.16 min
[32m[20230203 20:57:45 @agent_ppo2.py:153][0m 577536 total steps have happened
[32m[20230203 20:57:45 @agent_ppo2.py:129][0m #------------------------ Iteration 282 --------------------------#
[32m[20230203 20:57:45 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:57:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:45 @agent_ppo2.py:193][0m |          -0.0045 |          12.9518 |           3.0766 |
[32m[20230203 20:57:45 @agent_ppo2.py:193][0m |          -0.0052 |          10.2076 |           3.0741 |
[32m[20230203 20:57:46 @agent_ppo2.py:193][0m |          -0.0126 |           9.6916 |           3.0733 |
[32m[20230203 20:57:46 @agent_ppo2.py:193][0m |          -0.0045 |           9.5955 |           3.0709 |
[32m[20230203 20:57:46 @agent_ppo2.py:193][0m |          -0.0131 |           9.1073 |           3.0715 |
[32m[20230203 20:57:46 @agent_ppo2.py:193][0m |          -0.0114 |           8.9579 |           3.0717 |
[32m[20230203 20:57:46 @agent_ppo2.py:193][0m |          -0.0068 |           8.7680 |           3.0730 |
[32m[20230203 20:57:46 @agent_ppo2.py:193][0m |          -0.0141 |           8.5311 |           3.0704 |
[32m[20230203 20:57:46 @agent_ppo2.py:193][0m |          -0.0183 |           8.4434 |           3.0722 |
[32m[20230203 20:57:46 @agent_ppo2.py:193][0m |          -0.0187 |           8.3094 |           3.0728 |
[32m[20230203 20:57:46 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 20:57:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 242.32
[32m[20230203 20:57:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.51
[32m[20230203 20:57:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.92
[32m[20230203 20:57:46 @agent_ppo2.py:151][0m Total time:       7.18 min
[32m[20230203 20:57:46 @agent_ppo2.py:153][0m 579584 total steps have happened
[32m[20230203 20:57:46 @agent_ppo2.py:129][0m #------------------------ Iteration 283 --------------------------#
[32m[20230203 20:57:47 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:57:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:47 @agent_ppo2.py:193][0m |          -0.0100 |           8.9396 |           3.0300 |
[32m[20230203 20:57:47 @agent_ppo2.py:193][0m |          -0.0087 |           8.5600 |           3.0251 |
[32m[20230203 20:57:47 @agent_ppo2.py:193][0m |          -0.0120 |           7.7853 |           3.0283 |
[32m[20230203 20:57:47 @agent_ppo2.py:193][0m |          -0.0096 |           7.5094 |           3.0293 |
[32m[20230203 20:57:47 @agent_ppo2.py:193][0m |          -0.0147 |           7.2150 |           3.0310 |
[32m[20230203 20:57:47 @agent_ppo2.py:193][0m |          -0.0149 |           6.9537 |           3.0324 |
[32m[20230203 20:57:47 @agent_ppo2.py:193][0m |          -0.0133 |           6.7476 |           3.0345 |
[32m[20230203 20:57:47 @agent_ppo2.py:193][0m |          -0.0088 |           6.6847 |           3.0324 |
[32m[20230203 20:57:47 @agent_ppo2.py:193][0m |          -0.0184 |           6.4216 |           3.0339 |
[32m[20230203 20:57:47 @agent_ppo2.py:193][0m |          -0.0088 |           6.2433 |           3.0336 |
[32m[20230203 20:57:47 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:57:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.16
[32m[20230203 20:57:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.86
[32m[20230203 20:57:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.56
[32m[20230203 20:57:48 @agent_ppo2.py:151][0m Total time:       7.20 min
[32m[20230203 20:57:48 @agent_ppo2.py:153][0m 581632 total steps have happened
[32m[20230203 20:57:48 @agent_ppo2.py:129][0m #------------------------ Iteration 284 --------------------------#
[32m[20230203 20:57:48 @agent_ppo2.py:135][0m Sampling time: 0.53 s by 1 slaves
[32m[20230203 20:57:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:48 @agent_ppo2.py:193][0m |           0.0021 |          21.2776 |           3.0896 |
[32m[20230203 20:57:48 @agent_ppo2.py:193][0m |          -0.0040 |          14.8650 |           3.0871 |
[32m[20230203 20:57:48 @agent_ppo2.py:193][0m |          -0.0073 |          13.9642 |           3.0832 |
[32m[20230203 20:57:48 @agent_ppo2.py:193][0m |          -0.0097 |          13.5168 |           3.0822 |
[32m[20230203 20:57:48 @agent_ppo2.py:193][0m |          -0.0084 |          13.4518 |           3.0826 |
[32m[20230203 20:57:48 @agent_ppo2.py:193][0m |          -0.0105 |          13.0982 |           3.0835 |
[32m[20230203 20:57:48 @agent_ppo2.py:193][0m |          -0.0111 |          12.7030 |           3.0810 |
[32m[20230203 20:57:49 @agent_ppo2.py:193][0m |          -0.0125 |          12.5190 |           3.0804 |
[32m[20230203 20:57:49 @agent_ppo2.py:193][0m |          -0.0124 |          12.4457 |           3.0816 |
[32m[20230203 20:57:49 @agent_ppo2.py:193][0m |          -0.0115 |          12.2854 |           3.0838 |
[32m[20230203 20:57:49 @agent_ppo2.py:138][0m Policy update time: 0.61 s
[32m[20230203 20:57:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 162.19
[32m[20230203 20:57:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.78
[32m[20230203 20:57:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.62
[32m[20230203 20:57:49 @agent_ppo2.py:151][0m Total time:       7.23 min
[32m[20230203 20:57:49 @agent_ppo2.py:153][0m 583680 total steps have happened
[32m[20230203 20:57:49 @agent_ppo2.py:129][0m #------------------------ Iteration 285 --------------------------#
[32m[20230203 20:57:49 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:57:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:50 @agent_ppo2.py:193][0m |          -0.0098 |           8.6929 |           3.1657 |
[32m[20230203 20:57:50 @agent_ppo2.py:193][0m |          -0.0099 |           6.9392 |           3.1567 |
[32m[20230203 20:57:50 @agent_ppo2.py:193][0m |          -0.0067 |           6.5196 |           3.1520 |
[32m[20230203 20:57:50 @agent_ppo2.py:193][0m |          -0.0109 |           6.3666 |           3.1514 |
[32m[20230203 20:57:50 @agent_ppo2.py:193][0m |           0.0018 |           6.1883 |           3.1466 |
[32m[20230203 20:57:50 @agent_ppo2.py:193][0m |          -0.0126 |           5.9740 |           3.1465 |
[32m[20230203 20:57:50 @agent_ppo2.py:193][0m |          -0.0063 |           5.9361 |           3.1461 |
[32m[20230203 20:57:50 @agent_ppo2.py:193][0m |          -0.0198 |           5.8896 |           3.1410 |
[32m[20230203 20:57:50 @agent_ppo2.py:193][0m |          -0.0136 |           5.7552 |           3.1458 |
[32m[20230203 20:57:50 @agent_ppo2.py:193][0m |           0.0119 |           6.4674 |           3.1435 |
[32m[20230203 20:57:50 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:57:50 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.04
[32m[20230203 20:57:50 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.71
[32m[20230203 20:57:50 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 171.35
[32m[20230203 20:57:50 @agent_ppo2.py:151][0m Total time:       7.25 min
[32m[20230203 20:57:50 @agent_ppo2.py:153][0m 585728 total steps have happened
[32m[20230203 20:57:50 @agent_ppo2.py:129][0m #------------------------ Iteration 286 --------------------------#
[32m[20230203 20:57:51 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:57:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:51 @agent_ppo2.py:193][0m |          -0.0028 |          11.3851 |           3.1528 |
[32m[20230203 20:57:51 @agent_ppo2.py:193][0m |          -0.0094 |          10.4537 |           3.1520 |
[32m[20230203 20:57:51 @agent_ppo2.py:193][0m |          -0.0117 |          10.1147 |           3.1531 |
[32m[20230203 20:57:51 @agent_ppo2.py:193][0m |          -0.0136 |           9.8476 |           3.1524 |
[32m[20230203 20:57:51 @agent_ppo2.py:193][0m |          -0.0150 |           9.7092 |           3.1524 |
[32m[20230203 20:57:51 @agent_ppo2.py:193][0m |          -0.0157 |           9.5869 |           3.1542 |
[32m[20230203 20:57:51 @agent_ppo2.py:193][0m |          -0.0163 |           9.4911 |           3.1509 |
[32m[20230203 20:57:51 @agent_ppo2.py:193][0m |          -0.0171 |           9.4256 |           3.1510 |
[32m[20230203 20:57:51 @agent_ppo2.py:193][0m |          -0.0179 |           9.3017 |           3.1539 |
[32m[20230203 20:57:51 @agent_ppo2.py:193][0m |          -0.0181 |           9.2336 |           3.1546 |
[32m[20230203 20:57:51 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:57:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 243.15
[32m[20230203 20:57:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.42
[32m[20230203 20:57:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 80.34
[32m[20230203 20:57:52 @agent_ppo2.py:151][0m Total time:       7.27 min
[32m[20230203 20:57:52 @agent_ppo2.py:153][0m 587776 total steps have happened
[32m[20230203 20:57:52 @agent_ppo2.py:129][0m #------------------------ Iteration 287 --------------------------#
[32m[20230203 20:57:52 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:57:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:52 @agent_ppo2.py:193][0m |          -0.0003 |          29.1426 |           3.1068 |
[32m[20230203 20:57:52 @agent_ppo2.py:193][0m |          -0.0066 |          20.0980 |           3.0912 |
[32m[20230203 20:57:52 @agent_ppo2.py:193][0m |          -0.0079 |          17.4324 |           3.0950 |
[32m[20230203 20:57:52 @agent_ppo2.py:193][0m |          -0.0089 |          15.5802 |           3.0988 |
[32m[20230203 20:57:52 @agent_ppo2.py:193][0m |          -0.0098 |          14.1993 |           3.0979 |
[32m[20230203 20:57:52 @agent_ppo2.py:193][0m |          -0.0105 |          13.2232 |           3.0940 |
[32m[20230203 20:57:52 @agent_ppo2.py:193][0m |          -0.0125 |          12.0787 |           3.0950 |
[32m[20230203 20:57:52 @agent_ppo2.py:193][0m |          -0.0122 |          11.3481 |           3.0903 |
[32m[20230203 20:57:52 @agent_ppo2.py:193][0m |          -0.0118 |          10.3520 |           3.0925 |
[32m[20230203 20:57:52 @agent_ppo2.py:193][0m |          -0.0133 |           9.6913 |           3.0918 |
[32m[20230203 20:57:52 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 20:57:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 195.24
[32m[20230203 20:57:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.20
[32m[20230203 20:57:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 260.61
[32m[20230203 20:57:53 @agent_ppo2.py:151][0m Total time:       7.29 min
[32m[20230203 20:57:53 @agent_ppo2.py:153][0m 589824 total steps have happened
[32m[20230203 20:57:53 @agent_ppo2.py:129][0m #------------------------ Iteration 288 --------------------------#
[32m[20230203 20:57:53 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:57:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:53 @agent_ppo2.py:193][0m |          -0.0001 |          15.1965 |           3.0882 |
[32m[20230203 20:57:53 @agent_ppo2.py:193][0m |          -0.0079 |          12.2830 |           3.0877 |
[32m[20230203 20:57:53 @agent_ppo2.py:193][0m |          -0.0105 |          11.7622 |           3.0839 |
[32m[20230203 20:57:53 @agent_ppo2.py:193][0m |          -0.0110 |          11.6205 |           3.0810 |
[32m[20230203 20:57:53 @agent_ppo2.py:193][0m |          -0.0072 |          11.1543 |           3.0810 |
[32m[20230203 20:57:53 @agent_ppo2.py:193][0m |          -0.0087 |          10.9269 |           3.0808 |
[32m[20230203 20:57:54 @agent_ppo2.py:193][0m |          -0.0137 |          10.4482 |           3.0868 |
[32m[20230203 20:57:54 @agent_ppo2.py:193][0m |          -0.0114 |          10.2932 |           3.0841 |
[32m[20230203 20:57:54 @agent_ppo2.py:193][0m |          -0.0133 |          10.0664 |           3.0809 |
[32m[20230203 20:57:54 @agent_ppo2.py:193][0m |          -0.0202 |           9.9285 |           3.0845 |
[32m[20230203 20:57:54 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:57:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 254.51
[32m[20230203 20:57:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.64
[32m[20230203 20:57:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.08
[32m[20230203 20:57:54 @agent_ppo2.py:151][0m Total time:       7.31 min
[32m[20230203 20:57:54 @agent_ppo2.py:153][0m 591872 total steps have happened
[32m[20230203 20:57:54 @agent_ppo2.py:129][0m #------------------------ Iteration 289 --------------------------#
[32m[20230203 20:57:54 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:57:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:55 @agent_ppo2.py:193][0m |          -0.0012 |          11.0748 |           3.0059 |
[32m[20230203 20:57:55 @agent_ppo2.py:193][0m |          -0.0040 |          10.3646 |           3.0031 |
[32m[20230203 20:57:55 @agent_ppo2.py:193][0m |          -0.0031 |          10.1272 |           2.9995 |
[32m[20230203 20:57:55 @agent_ppo2.py:193][0m |           0.0080 |          11.5260 |           3.0006 |
[32m[20230203 20:57:55 @agent_ppo2.py:193][0m |          -0.0088 |           9.8293 |           2.9965 |
[32m[20230203 20:57:55 @agent_ppo2.py:193][0m |          -0.0144 |           9.6906 |           2.9974 |
[32m[20230203 20:57:55 @agent_ppo2.py:193][0m |          -0.0028 |           9.7397 |           2.9970 |
[32m[20230203 20:57:55 @agent_ppo2.py:193][0m |          -0.0049 |           9.8108 |           2.9997 |
[32m[20230203 20:57:55 @agent_ppo2.py:193][0m |          -0.0160 |           9.4951 |           2.9978 |
[32m[20230203 20:57:55 @agent_ppo2.py:193][0m |          -0.0161 |           9.3306 |           2.9957 |
[32m[20230203 20:57:55 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:57:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.58
[32m[20230203 20:57:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.32
[32m[20230203 20:57:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.11
[32m[20230203 20:57:55 @agent_ppo2.py:151][0m Total time:       7.33 min
[32m[20230203 20:57:55 @agent_ppo2.py:153][0m 593920 total steps have happened
[32m[20230203 20:57:55 @agent_ppo2.py:129][0m #------------------------ Iteration 290 --------------------------#
[32m[20230203 20:57:56 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:57:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:56 @agent_ppo2.py:193][0m |           0.0009 |          10.6497 |           3.1754 |
[32m[20230203 20:57:56 @agent_ppo2.py:193][0m |          -0.0023 |          10.4859 |           3.1719 |
[32m[20230203 20:57:56 @agent_ppo2.py:193][0m |          -0.0042 |          10.1363 |           3.1677 |
[32m[20230203 20:57:56 @agent_ppo2.py:193][0m |          -0.0054 |          10.0319 |           3.1663 |
[32m[20230203 20:57:56 @agent_ppo2.py:193][0m |          -0.0095 |           9.7595 |           3.1632 |
[32m[20230203 20:57:56 @agent_ppo2.py:193][0m |          -0.0082 |           9.8454 |           3.1617 |
[32m[20230203 20:57:56 @agent_ppo2.py:193][0m |          -0.0099 |          10.0386 |           3.1622 |
[32m[20230203 20:57:56 @agent_ppo2.py:193][0m |          -0.0125 |           9.6168 |           3.1583 |
[32m[20230203 20:57:56 @agent_ppo2.py:193][0m |          -0.0093 |           9.8511 |           3.1583 |
[32m[20230203 20:57:56 @agent_ppo2.py:193][0m |          -0.0123 |           9.5160 |           3.1545 |
[32m[20230203 20:57:56 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:57:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 244.71
[32m[20230203 20:57:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.09
[32m[20230203 20:57:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.40
[32m[20230203 20:57:57 @agent_ppo2.py:151][0m Total time:       7.35 min
[32m[20230203 20:57:57 @agent_ppo2.py:153][0m 595968 total steps have happened
[32m[20230203 20:57:57 @agent_ppo2.py:129][0m #------------------------ Iteration 291 --------------------------#
[32m[20230203 20:57:57 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:57:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:57 @agent_ppo2.py:193][0m |          -0.0010 |          10.4808 |           3.1501 |
[32m[20230203 20:57:57 @agent_ppo2.py:193][0m |          -0.0068 |          10.0115 |           3.1497 |
[32m[20230203 20:57:57 @agent_ppo2.py:193][0m |          -0.0072 |           9.8192 |           3.1465 |
[32m[20230203 20:57:57 @agent_ppo2.py:193][0m |          -0.0103 |           9.6901 |           3.1434 |
[32m[20230203 20:57:57 @agent_ppo2.py:193][0m |          -0.0106 |           9.6026 |           3.1422 |
[32m[20230203 20:57:57 @agent_ppo2.py:193][0m |          -0.0118 |           9.5512 |           3.1440 |
[32m[20230203 20:57:57 @agent_ppo2.py:193][0m |          -0.0127 |           9.4323 |           3.1439 |
[32m[20230203 20:57:57 @agent_ppo2.py:193][0m |          -0.0122 |           9.4150 |           3.1410 |
[32m[20230203 20:57:57 @agent_ppo2.py:193][0m |          -0.0104 |           9.5013 |           3.1409 |
[32m[20230203 20:57:57 @agent_ppo2.py:193][0m |          -0.0106 |           9.4742 |           3.1398 |
[32m[20230203 20:57:57 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:57:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.92
[32m[20230203 20:57:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.84
[32m[20230203 20:57:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 43.83
[32m[20230203 20:57:58 @agent_ppo2.py:151][0m Total time:       7.37 min
[32m[20230203 20:57:58 @agent_ppo2.py:153][0m 598016 total steps have happened
[32m[20230203 20:57:58 @agent_ppo2.py:129][0m #------------------------ Iteration 292 --------------------------#
[32m[20230203 20:57:58 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:57:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:58 @agent_ppo2.py:193][0m |          -0.0005 |          11.2238 |           3.1668 |
[32m[20230203 20:57:58 @agent_ppo2.py:193][0m |          -0.0040 |          10.3013 |           3.1615 |
[32m[20230203 20:57:58 @agent_ppo2.py:193][0m |          -0.0068 |           9.8307 |           3.1578 |
[32m[20230203 20:57:58 @agent_ppo2.py:193][0m |          -0.0064 |           9.5980 |           3.1530 |
[32m[20230203 20:57:58 @agent_ppo2.py:193][0m |          -0.0091 |           9.2326 |           3.1546 |
[32m[20230203 20:57:58 @agent_ppo2.py:193][0m |          -0.0093 |           8.9744 |           3.1514 |
[32m[20230203 20:57:58 @agent_ppo2.py:193][0m |          -0.0098 |           8.7300 |           3.1518 |
[32m[20230203 20:57:59 @agent_ppo2.py:193][0m |          -0.0102 |           8.4622 |           3.1498 |
[32m[20230203 20:57:59 @agent_ppo2.py:193][0m |          -0.0116 |           8.0992 |           3.1488 |
[32m[20230203 20:57:59 @agent_ppo2.py:193][0m |          -0.0105 |           7.7886 |           3.1499 |
[32m[20230203 20:57:59 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:57:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.30
[32m[20230203 20:57:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.42
[32m[20230203 20:57:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.86
[32m[20230203 20:57:59 @agent_ppo2.py:151][0m Total time:       7.40 min
[32m[20230203 20:57:59 @agent_ppo2.py:153][0m 600064 total steps have happened
[32m[20230203 20:57:59 @agent_ppo2.py:129][0m #------------------------ Iteration 293 --------------------------#
[32m[20230203 20:57:59 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 1 slaves
[32m[20230203 20:57:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:57:59 @agent_ppo2.py:193][0m |          -0.0024 |          42.2886 |           3.0748 |
[32m[20230203 20:57:59 @agent_ppo2.py:193][0m |          -0.0071 |          25.1308 |           3.0693 |
[32m[20230203 20:57:59 @agent_ppo2.py:193][0m |          -0.0086 |          23.2560 |           3.0610 |
[32m[20230203 20:58:00 @agent_ppo2.py:193][0m |          -0.0130 |          20.3438 |           3.0680 |
[32m[20230203 20:58:00 @agent_ppo2.py:193][0m |          -0.0117 |          19.4641 |           3.0657 |
[32m[20230203 20:58:00 @agent_ppo2.py:193][0m |          -0.0106 |          17.8894 |           3.0687 |
[32m[20230203 20:58:00 @agent_ppo2.py:193][0m |          -0.0154 |          17.4946 |           3.0669 |
[32m[20230203 20:58:00 @agent_ppo2.py:193][0m |          -0.0139 |          17.0283 |           3.0736 |
[32m[20230203 20:58:00 @agent_ppo2.py:193][0m |          -0.0135 |          16.4287 |           3.0676 |
[32m[20230203 20:58:00 @agent_ppo2.py:193][0m |          -0.0142 |          16.0250 |           3.0726 |
[32m[20230203 20:58:00 @agent_ppo2.py:138][0m Policy update time: 0.40 s
[32m[20230203 20:58:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 58.45
[32m[20230203 20:58:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.14
[32m[20230203 20:58:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.10
[32m[20230203 20:58:00 @agent_ppo2.py:151][0m Total time:       7.41 min
[32m[20230203 20:58:00 @agent_ppo2.py:153][0m 602112 total steps have happened
[32m[20230203 20:58:00 @agent_ppo2.py:129][0m #------------------------ Iteration 294 --------------------------#
[32m[20230203 20:58:01 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:58:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:01 @agent_ppo2.py:193][0m |          -0.0015 |          29.3813 |           3.1374 |
[32m[20230203 20:58:01 @agent_ppo2.py:193][0m |          -0.0072 |          18.3275 |           3.1327 |
[32m[20230203 20:58:01 @agent_ppo2.py:193][0m |          -0.0084 |          16.5830 |           3.1296 |
[32m[20230203 20:58:01 @agent_ppo2.py:193][0m |          -0.0143 |          15.4454 |           3.1276 |
[32m[20230203 20:58:01 @agent_ppo2.py:193][0m |          -0.0141 |          14.8548 |           3.1256 |
[32m[20230203 20:58:01 @agent_ppo2.py:193][0m |          -0.0132 |          14.6059 |           3.1234 |
[32m[20230203 20:58:01 @agent_ppo2.py:193][0m |          -0.0163 |          13.7845 |           3.1216 |
[32m[20230203 20:58:01 @agent_ppo2.py:193][0m |          -0.0171 |          13.4483 |           3.1214 |
[32m[20230203 20:58:01 @agent_ppo2.py:193][0m |          -0.0158 |          13.3006 |           3.1195 |
[32m[20230203 20:58:01 @agent_ppo2.py:193][0m |          -0.0165 |          12.9237 |           3.1136 |
[32m[20230203 20:58:01 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:58:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 87.10
[32m[20230203 20:58:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.38
[32m[20230203 20:58:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 260.59
[32m[20230203 20:58:01 @agent_ppo2.py:151][0m Total time:       7.44 min
[32m[20230203 20:58:01 @agent_ppo2.py:153][0m 604160 total steps have happened
[32m[20230203 20:58:01 @agent_ppo2.py:129][0m #------------------------ Iteration 295 --------------------------#
[32m[20230203 20:58:02 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:58:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:02 @agent_ppo2.py:193][0m |           0.0011 |          11.1880 |           3.1918 |
[32m[20230203 20:58:02 @agent_ppo2.py:193][0m |          -0.0085 |           9.9605 |           3.1945 |
[32m[20230203 20:58:02 @agent_ppo2.py:193][0m |          -0.0107 |           9.5838 |           3.1918 |
[32m[20230203 20:58:02 @agent_ppo2.py:193][0m |          -0.0117 |           9.3121 |           3.1903 |
[32m[20230203 20:58:02 @agent_ppo2.py:193][0m |          -0.0130 |           9.0677 |           3.1887 |
[32m[20230203 20:58:02 @agent_ppo2.py:193][0m |          -0.0143 |           8.9012 |           3.1872 |
[32m[20230203 20:58:02 @agent_ppo2.py:193][0m |          -0.0143 |           8.7918 |           3.1808 |
[32m[20230203 20:58:02 @agent_ppo2.py:193][0m |          -0.0150 |           8.6492 |           3.1819 |
[32m[20230203 20:58:02 @agent_ppo2.py:193][0m |          -0.0151 |           8.5126 |           3.1831 |
[32m[20230203 20:58:02 @agent_ppo2.py:193][0m |          -0.0166 |           8.4475 |           3.1829 |
[32m[20230203 20:58:02 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:58:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.35
[32m[20230203 20:58:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.33
[32m[20230203 20:58:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.77
[32m[20230203 20:58:03 @agent_ppo2.py:151][0m Total time:       7.46 min
[32m[20230203 20:58:03 @agent_ppo2.py:153][0m 606208 total steps have happened
[32m[20230203 20:58:03 @agent_ppo2.py:129][0m #------------------------ Iteration 296 --------------------------#
[32m[20230203 20:58:03 @agent_ppo2.py:135][0m Sampling time: 0.55 s by 1 slaves
[32m[20230203 20:58:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:03 @agent_ppo2.py:193][0m |           0.0220 |          23.0147 |           3.0332 |
[32m[20230203 20:58:03 @agent_ppo2.py:193][0m |          -0.0080 |          15.4034 |           3.0206 |
[32m[20230203 20:58:03 @agent_ppo2.py:193][0m |          -0.0025 |          14.3251 |           3.0158 |
[32m[20230203 20:58:03 @agent_ppo2.py:193][0m |          -0.0118 |          13.7918 |           3.0159 |
[32m[20230203 20:58:03 @agent_ppo2.py:193][0m |          -0.0157 |          13.4448 |           3.0133 |
[32m[20230203 20:58:04 @agent_ppo2.py:193][0m |           0.0069 |          13.3779 |           3.0110 |
[32m[20230203 20:58:04 @agent_ppo2.py:193][0m |          -0.0108 |          13.0331 |           3.0090 |
[32m[20230203 20:58:04 @agent_ppo2.py:193][0m |          -0.0181 |          12.8990 |           3.0054 |
[32m[20230203 20:58:04 @agent_ppo2.py:193][0m |          -0.0170 |          12.8842 |           3.0072 |
[32m[20230203 20:58:04 @agent_ppo2.py:193][0m |          -0.0183 |          12.7457 |           3.0071 |
[32m[20230203 20:58:04 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:58:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 177.46
[32m[20230203 20:58:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.33
[32m[20230203 20:58:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 215.48
[32m[20230203 20:58:04 @agent_ppo2.py:151][0m Total time:       7.48 min
[32m[20230203 20:58:04 @agent_ppo2.py:153][0m 608256 total steps have happened
[32m[20230203 20:58:04 @agent_ppo2.py:129][0m #------------------------ Iteration 297 --------------------------#
[32m[20230203 20:58:05 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:58:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:05 @agent_ppo2.py:193][0m |           0.0000 |          24.0571 |           3.0144 |
[32m[20230203 20:58:05 @agent_ppo2.py:193][0m |          -0.0007 |          10.6757 |           3.0147 |
[32m[20230203 20:58:05 @agent_ppo2.py:193][0m |          -0.0075 |           9.3947 |           3.0109 |
[32m[20230203 20:58:05 @agent_ppo2.py:193][0m |          -0.0041 |           8.9841 |           3.0066 |
[32m[20230203 20:58:05 @agent_ppo2.py:193][0m |          -0.0083 |           8.8615 |           3.0051 |
[32m[20230203 20:58:05 @agent_ppo2.py:193][0m |          -0.0085 |           8.3094 |           3.0017 |
[32m[20230203 20:58:05 @agent_ppo2.py:193][0m |          -0.0078 |           8.1523 |           2.9979 |
[32m[20230203 20:58:05 @agent_ppo2.py:193][0m |          -0.0097 |           7.9326 |           2.9973 |
[32m[20230203 20:58:05 @agent_ppo2.py:193][0m |          -0.0090 |           7.7965 |           2.9966 |
[32m[20230203 20:58:05 @agent_ppo2.py:193][0m |          -0.0102 |           7.6452 |           2.9940 |
[32m[20230203 20:58:05 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:58:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 73.13
[32m[20230203 20:58:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.48
[32m[20230203 20:58:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 183.56
[32m[20230203 20:58:06 @agent_ppo2.py:151][0m Total time:       7.51 min
[32m[20230203 20:58:06 @agent_ppo2.py:153][0m 610304 total steps have happened
[32m[20230203 20:58:06 @agent_ppo2.py:129][0m #------------------------ Iteration 298 --------------------------#
[32m[20230203 20:58:06 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:58:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:06 @agent_ppo2.py:193][0m |          -0.0005 |          11.9254 |           3.0030 |
[32m[20230203 20:58:06 @agent_ppo2.py:193][0m |          -0.0086 |          10.7160 |           3.0009 |
[32m[20230203 20:58:06 @agent_ppo2.py:193][0m |          -0.0097 |          10.3089 |           2.9990 |
[32m[20230203 20:58:06 @agent_ppo2.py:193][0m |          -0.0110 |          10.0393 |           3.0020 |
[32m[20230203 20:58:06 @agent_ppo2.py:193][0m |          -0.0128 |           9.8427 |           3.0045 |
[32m[20230203 20:58:06 @agent_ppo2.py:193][0m |          -0.0143 |           9.6772 |           3.0079 |
[32m[20230203 20:58:06 @agent_ppo2.py:193][0m |          -0.0127 |           9.6610 |           3.0055 |
[32m[20230203 20:58:06 @agent_ppo2.py:193][0m |          -0.0153 |           9.4381 |           3.0091 |
[32m[20230203 20:58:06 @agent_ppo2.py:193][0m |          -0.0161 |           9.3124 |           3.0084 |
[32m[20230203 20:58:06 @agent_ppo2.py:193][0m |          -0.0130 |           9.2642 |           3.0094 |
[32m[20230203 20:58:06 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:58:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.99
[32m[20230203 20:58:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.75
[32m[20230203 20:58:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 200.49
[32m[20230203 20:58:07 @agent_ppo2.py:151][0m Total time:       7.53 min
[32m[20230203 20:58:07 @agent_ppo2.py:153][0m 612352 total steps have happened
[32m[20230203 20:58:07 @agent_ppo2.py:129][0m #------------------------ Iteration 299 --------------------------#
[32m[20230203 20:58:07 @agent_ppo2.py:135][0m Sampling time: 0.50 s by 1 slaves
[32m[20230203 20:58:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:08 @agent_ppo2.py:193][0m |          -0.0007 |          29.2281 |           3.0652 |
[32m[20230203 20:58:08 @agent_ppo2.py:193][0m |          -0.0066 |          21.0179 |           3.0674 |
[32m[20230203 20:58:08 @agent_ppo2.py:193][0m |          -0.0068 |          19.8295 |           3.0601 |
[32m[20230203 20:58:08 @agent_ppo2.py:193][0m |          -0.0048 |          19.4943 |           3.0581 |
[32m[20230203 20:58:08 @agent_ppo2.py:193][0m |          -0.0093 |          19.2234 |           3.0584 |
[32m[20230203 20:58:08 @agent_ppo2.py:193][0m |          -0.0115 |          18.7040 |           3.0540 |
[32m[20230203 20:58:08 @agent_ppo2.py:193][0m |          -0.0120 |          18.4423 |           3.0563 |
[32m[20230203 20:58:08 @agent_ppo2.py:193][0m |          -0.0131 |          18.2213 |           3.0520 |
[32m[20230203 20:58:08 @agent_ppo2.py:193][0m |          -0.0105 |          18.4295 |           3.0505 |
[32m[20230203 20:58:08 @agent_ppo2.py:193][0m |          -0.0104 |          18.5046 |           3.0513 |
[32m[20230203 20:58:08 @agent_ppo2.py:138][0m Policy update time: 0.57 s
[32m[20230203 20:58:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 165.42
[32m[20230203 20:58:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.05
[32m[20230203 20:58:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.16
[32m[20230203 20:58:08 @agent_ppo2.py:151][0m Total time:       7.55 min
[32m[20230203 20:58:08 @agent_ppo2.py:153][0m 614400 total steps have happened
[32m[20230203 20:58:08 @agent_ppo2.py:129][0m #------------------------ Iteration 300 --------------------------#
[32m[20230203 20:58:09 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:58:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:09 @agent_ppo2.py:193][0m |          -0.0013 |          10.9182 |           3.0248 |
[32m[20230203 20:58:09 @agent_ppo2.py:193][0m |          -0.0073 |          10.3402 |           3.0250 |
[32m[20230203 20:58:09 @agent_ppo2.py:193][0m |          -0.0056 |          10.2965 |           3.0224 |
[32m[20230203 20:58:09 @agent_ppo2.py:193][0m |          -0.0098 |          10.1224 |           3.0226 |
[32m[20230203 20:58:09 @agent_ppo2.py:193][0m |          -0.0103 |           9.9853 |           3.0224 |
[32m[20230203 20:58:09 @agent_ppo2.py:193][0m |          -0.0122 |           9.7480 |           3.0203 |
[32m[20230203 20:58:09 @agent_ppo2.py:193][0m |          -0.0116 |           9.8345 |           3.0221 |
[32m[20230203 20:58:09 @agent_ppo2.py:193][0m |          -0.0143 |           9.6044 |           3.0209 |
[32m[20230203 20:58:09 @agent_ppo2.py:193][0m |          -0.0160 |           9.5151 |           3.0224 |
[32m[20230203 20:58:09 @agent_ppo2.py:193][0m |          -0.0157 |           9.4497 |           3.0217 |
[32m[20230203 20:58:09 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:58:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.47
[32m[20230203 20:58:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.93
[32m[20230203 20:58:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.38
[32m[20230203 20:58:10 @agent_ppo2.py:151][0m Total time:       7.57 min
[32m[20230203 20:58:10 @agent_ppo2.py:153][0m 616448 total steps have happened
[32m[20230203 20:58:10 @agent_ppo2.py:129][0m #------------------------ Iteration 301 --------------------------#
[32m[20230203 20:58:10 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:58:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:10 @agent_ppo2.py:193][0m |          -0.0043 |          10.0757 |           2.9886 |
[32m[20230203 20:58:10 @agent_ppo2.py:193][0m |          -0.0086 |           9.7044 |           2.9727 |
[32m[20230203 20:58:10 @agent_ppo2.py:193][0m |          -0.0187 |           9.4662 |           2.9705 |
[32m[20230203 20:58:10 @agent_ppo2.py:193][0m |          -0.0135 |           9.3064 |           2.9722 |
[32m[20230203 20:58:10 @agent_ppo2.py:193][0m |          -0.0203 |           9.2031 |           2.9688 |
[32m[20230203 20:58:10 @agent_ppo2.py:193][0m |          -0.0176 |           9.0699 |           2.9710 |
[32m[20230203 20:58:10 @agent_ppo2.py:193][0m |          -0.0248 |           9.0235 |           2.9698 |
[32m[20230203 20:58:10 @agent_ppo2.py:193][0m |          -0.0213 |           8.8684 |           2.9676 |
[32m[20230203 20:58:10 @agent_ppo2.py:193][0m |          -0.0127 |           8.7588 |           2.9675 |
[32m[20230203 20:58:11 @agent_ppo2.py:193][0m |          -0.0235 |           8.5762 |           2.9690 |
[32m[20230203 20:58:11 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:58:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.67
[32m[20230203 20:58:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.96
[32m[20230203 20:58:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 89.67
[32m[20230203 20:58:11 @agent_ppo2.py:151][0m Total time:       7.60 min
[32m[20230203 20:58:11 @agent_ppo2.py:153][0m 618496 total steps have happened
[32m[20230203 20:58:11 @agent_ppo2.py:129][0m #------------------------ Iteration 302 --------------------------#
[32m[20230203 20:58:11 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:58:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:11 @agent_ppo2.py:193][0m |          -0.0012 |          10.9744 |           2.9484 |
[32m[20230203 20:58:12 @agent_ppo2.py:193][0m |          -0.0089 |          10.5870 |           2.9365 |
[32m[20230203 20:58:12 @agent_ppo2.py:193][0m |          -0.0046 |          10.3248 |           2.9361 |
[32m[20230203 20:58:12 @agent_ppo2.py:193][0m |           0.1205 |          24.6244 |           2.9357 |
[32m[20230203 20:58:12 @agent_ppo2.py:193][0m |          -0.0230 |          10.5725 |           2.9210 |
[32m[20230203 20:58:12 @agent_ppo2.py:193][0m |          -0.0205 |          10.2235 |           2.9239 |
[32m[20230203 20:58:12 @agent_ppo2.py:193][0m |          -0.0099 |           9.9041 |           2.9254 |
[32m[20230203 20:58:12 @agent_ppo2.py:193][0m |          -0.0046 |           9.7901 |           2.9269 |
[32m[20230203 20:58:12 @agent_ppo2.py:193][0m |          -0.0135 |           9.6707 |           2.9207 |
[32m[20230203 20:58:12 @agent_ppo2.py:193][0m |          -0.0037 |           9.6495 |           2.9217 |
[32m[20230203 20:58:12 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:58:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 255.84
[32m[20230203 20:58:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.52
[32m[20230203 20:58:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.62
[32m[20230203 20:58:12 @agent_ppo2.py:151][0m Total time:       7.62 min
[32m[20230203 20:58:12 @agent_ppo2.py:153][0m 620544 total steps have happened
[32m[20230203 20:58:12 @agent_ppo2.py:129][0m #------------------------ Iteration 303 --------------------------#
[32m[20230203 20:58:13 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:58:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:13 @agent_ppo2.py:193][0m |          -0.0001 |          10.3641 |           3.0379 |
[32m[20230203 20:58:13 @agent_ppo2.py:193][0m |          -0.0044 |           9.9531 |           3.0346 |
[32m[20230203 20:58:13 @agent_ppo2.py:193][0m |          -0.0089 |           9.7193 |           3.0338 |
[32m[20230203 20:58:13 @agent_ppo2.py:193][0m |          -0.0089 |           9.6038 |           3.0298 |
[32m[20230203 20:58:13 @agent_ppo2.py:193][0m |          -0.0104 |           9.4883 |           3.0292 |
[32m[20230203 20:58:13 @agent_ppo2.py:193][0m |          -0.0100 |           9.6158 |           3.0267 |
[32m[20230203 20:58:13 @agent_ppo2.py:193][0m |          -0.0136 |           9.2602 |           3.0276 |
[32m[20230203 20:58:13 @agent_ppo2.py:193][0m |          -0.0130 |           9.1845 |           3.0250 |
[32m[20230203 20:58:13 @agent_ppo2.py:193][0m |          -0.0122 |           9.2170 |           3.0273 |
[32m[20230203 20:58:13 @agent_ppo2.py:193][0m |          -0.0179 |           9.0124 |           3.0246 |
[32m[20230203 20:58:13 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:58:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.75
[32m[20230203 20:58:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.55
[32m[20230203 20:58:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 189.69
[32m[20230203 20:58:13 @agent_ppo2.py:151][0m Total time:       7.64 min
[32m[20230203 20:58:13 @agent_ppo2.py:153][0m 622592 total steps have happened
[32m[20230203 20:58:13 @agent_ppo2.py:129][0m #------------------------ Iteration 304 --------------------------#
[32m[20230203 20:58:14 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:58:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:14 @agent_ppo2.py:193][0m |           0.0001 |          11.7733 |           3.0527 |
[32m[20230203 20:58:14 @agent_ppo2.py:193][0m |          -0.0079 |          11.0821 |           3.0479 |
[32m[20230203 20:58:14 @agent_ppo2.py:193][0m |          -0.0105 |          10.7503 |           3.0426 |
[32m[20230203 20:58:14 @agent_ppo2.py:193][0m |          -0.0137 |          10.3111 |           3.0413 |
[32m[20230203 20:58:14 @agent_ppo2.py:193][0m |          -0.0123 |          10.0675 |           3.0401 |
[32m[20230203 20:58:14 @agent_ppo2.py:193][0m |          -0.0162 |           9.8296 |           3.0419 |
[32m[20230203 20:58:14 @agent_ppo2.py:193][0m |          -0.0155 |           9.6694 |           3.0411 |
[32m[20230203 20:58:14 @agent_ppo2.py:193][0m |          -0.0136 |           9.7596 |           3.0414 |
[32m[20230203 20:58:14 @agent_ppo2.py:193][0m |          -0.0163 |           9.3190 |           3.0392 |
[32m[20230203 20:58:14 @agent_ppo2.py:193][0m |          -0.0157 |           9.4434 |           3.0404 |
[32m[20230203 20:58:14 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 20:58:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.83
[32m[20230203 20:58:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.50
[32m[20230203 20:58:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -11.98
[32m[20230203 20:58:15 @agent_ppo2.py:151][0m Total time:       7.66 min
[32m[20230203 20:58:15 @agent_ppo2.py:153][0m 624640 total steps have happened
[32m[20230203 20:58:15 @agent_ppo2.py:129][0m #------------------------ Iteration 305 --------------------------#
[32m[20230203 20:58:15 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:58:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:15 @agent_ppo2.py:193][0m |          -0.0031 |          38.3885 |           3.0526 |
[32m[20230203 20:58:15 @agent_ppo2.py:193][0m |          -0.0072 |          29.0093 |           3.0454 |
[32m[20230203 20:58:15 @agent_ppo2.py:193][0m |          -0.0128 |          26.0615 |           3.0469 |
[32m[20230203 20:58:15 @agent_ppo2.py:193][0m |          -0.0121 |          23.7208 |           3.0441 |
[32m[20230203 20:58:15 @agent_ppo2.py:193][0m |          -0.0081 |          23.6144 |           3.0456 |
[32m[20230203 20:58:15 @agent_ppo2.py:193][0m |          -0.0097 |          21.7579 |           3.0421 |
[32m[20230203 20:58:15 @agent_ppo2.py:193][0m |          -0.0145 |          20.6483 |           3.0412 |
[32m[20230203 20:58:15 @agent_ppo2.py:193][0m |          -0.0156 |          20.4103 |           3.0440 |
[32m[20230203 20:58:15 @agent_ppo2.py:193][0m |          -0.0193 |          19.9497 |           3.0400 |
[32m[20230203 20:58:16 @agent_ppo2.py:193][0m |          -0.0079 |          19.6849 |           3.0394 |
[32m[20230203 20:58:16 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:58:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 121.98
[32m[20230203 20:58:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.39
[32m[20230203 20:58:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 118.10
[32m[20230203 20:58:16 @agent_ppo2.py:151][0m Total time:       7.68 min
[32m[20230203 20:58:16 @agent_ppo2.py:153][0m 626688 total steps have happened
[32m[20230203 20:58:16 @agent_ppo2.py:129][0m #------------------------ Iteration 306 --------------------------#
[32m[20230203 20:58:17 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 20:58:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:17 @agent_ppo2.py:193][0m |           0.0014 |          36.1129 |           3.0401 |
[32m[20230203 20:58:17 @agent_ppo2.py:193][0m |          -0.0074 |          26.5112 |           3.0392 |
[32m[20230203 20:58:17 @agent_ppo2.py:193][0m |          -0.0074 |          23.2644 |           3.0358 |
[32m[20230203 20:58:17 @agent_ppo2.py:193][0m |          -0.0082 |          20.1925 |           3.0367 |
[32m[20230203 20:58:17 @agent_ppo2.py:193][0m |          -0.0115 |          18.8944 |           3.0346 |
[32m[20230203 20:58:17 @agent_ppo2.py:193][0m |          -0.0074 |          18.6982 |           3.0333 |
[32m[20230203 20:58:17 @agent_ppo2.py:193][0m |          -0.0134 |          16.8441 |           3.0345 |
[32m[20230203 20:58:17 @agent_ppo2.py:193][0m |          -0.0119 |          16.5262 |           3.0362 |
[32m[20230203 20:58:17 @agent_ppo2.py:193][0m |          -0.0128 |          15.6899 |           3.0368 |
[32m[20230203 20:58:17 @agent_ppo2.py:193][0m |          -0.0139 |          15.2398 |           3.0344 |
[32m[20230203 20:58:17 @agent_ppo2.py:138][0m Policy update time: 0.56 s
[32m[20230203 20:58:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 166.72
[32m[20230203 20:58:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.28
[32m[20230203 20:58:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.25
[32m[20230203 20:58:17 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 271.25
[32m[20230203 20:58:17 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 271.25
[32m[20230203 20:58:17 @agent_ppo2.py:151][0m Total time:       7.70 min
[32m[20230203 20:58:17 @agent_ppo2.py:153][0m 628736 total steps have happened
[32m[20230203 20:58:17 @agent_ppo2.py:129][0m #------------------------ Iteration 307 --------------------------#
[32m[20230203 20:58:18 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:58:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:18 @agent_ppo2.py:193][0m |           0.0018 |          25.6109 |           3.0086 |
[32m[20230203 20:58:18 @agent_ppo2.py:193][0m |          -0.0030 |          14.6801 |           3.0059 |
[32m[20230203 20:58:18 @agent_ppo2.py:193][0m |          -0.0038 |          12.9202 |           3.0065 |
[32m[20230203 20:58:18 @agent_ppo2.py:193][0m |          -0.0050 |          12.4366 |           3.0047 |
[32m[20230203 20:58:18 @agent_ppo2.py:193][0m |          -0.0079 |          11.7730 |           3.0066 |
[32m[20230203 20:58:18 @agent_ppo2.py:193][0m |          -0.0084 |          11.4530 |           2.9997 |
[32m[20230203 20:58:18 @agent_ppo2.py:193][0m |          -0.0089 |          11.2227 |           3.0030 |
[32m[20230203 20:58:18 @agent_ppo2.py:193][0m |          -0.0103 |          10.9589 |           3.0023 |
[32m[20230203 20:58:18 @agent_ppo2.py:193][0m |          -0.0108 |          10.7657 |           2.9989 |
[32m[20230203 20:58:18 @agent_ppo2.py:193][0m |          -0.0111 |          10.4356 |           2.9964 |
[32m[20230203 20:58:18 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 20:58:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 170.18
[32m[20230203 20:58:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.33
[32m[20230203 20:58:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.31
[32m[20230203 20:58:19 @agent_ppo2.py:151][0m Total time:       7.72 min
[32m[20230203 20:58:19 @agent_ppo2.py:153][0m 630784 total steps have happened
[32m[20230203 20:58:19 @agent_ppo2.py:129][0m #------------------------ Iteration 308 --------------------------#
[32m[20230203 20:58:19 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:58:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:19 @agent_ppo2.py:193][0m |          -0.0008 |          15.5617 |           3.0000 |
[32m[20230203 20:58:19 @agent_ppo2.py:193][0m |          -0.0105 |          14.3490 |           2.9815 |
[32m[20230203 20:58:19 @agent_ppo2.py:193][0m |          -0.0100 |          13.9991 |           2.9837 |
[32m[20230203 20:58:19 @agent_ppo2.py:193][0m |          -0.0065 |          14.0611 |           2.9926 |
[32m[20230203 20:58:19 @agent_ppo2.py:193][0m |          -0.0113 |          13.5606 |           2.9808 |
[32m[20230203 20:58:19 @agent_ppo2.py:193][0m |          -0.0060 |          14.2361 |           2.9856 |
[32m[20230203 20:58:19 @agent_ppo2.py:193][0m |          -0.0091 |          13.4574 |           2.9832 |
[32m[20230203 20:58:19 @agent_ppo2.py:193][0m |          -0.0118 |          13.1350 |           2.9832 |
[32m[20230203 20:58:19 @agent_ppo2.py:193][0m |          -0.0103 |          13.6661 |           2.9841 |
[32m[20230203 20:58:19 @agent_ppo2.py:193][0m |          -0.0140 |          12.8915 |           2.9880 |
[32m[20230203 20:58:19 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:58:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.29
[32m[20230203 20:58:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.13
[32m[20230203 20:58:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.87
[32m[20230203 20:58:20 @agent_ppo2.py:151][0m Total time:       7.74 min
[32m[20230203 20:58:20 @agent_ppo2.py:153][0m 632832 total steps have happened
[32m[20230203 20:58:20 @agent_ppo2.py:129][0m #------------------------ Iteration 309 --------------------------#
[32m[20230203 20:58:20 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:58:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:20 @agent_ppo2.py:193][0m |           0.0013 |          55.6372 |           3.0576 |
[32m[20230203 20:58:20 @agent_ppo2.py:193][0m |          -0.0035 |          45.7491 |           3.0572 |
[32m[20230203 20:58:20 @agent_ppo2.py:193][0m |          -0.0064 |          39.9024 |           3.0570 |
[32m[20230203 20:58:20 @agent_ppo2.py:193][0m |          -0.0072 |          35.5059 |           3.0542 |
[32m[20230203 20:58:20 @agent_ppo2.py:193][0m |          -0.0093 |          31.9691 |           3.0557 |
[32m[20230203 20:58:20 @agent_ppo2.py:193][0m |          -0.0099 |          31.2809 |           3.0538 |
[32m[20230203 20:58:20 @agent_ppo2.py:193][0m |          -0.0107 |          28.9455 |           3.0532 |
[32m[20230203 20:58:21 @agent_ppo2.py:193][0m |          -0.0119 |          27.1636 |           3.0537 |
[32m[20230203 20:58:21 @agent_ppo2.py:193][0m |          -0.0133 |          26.5095 |           3.0503 |
[32m[20230203 20:58:21 @agent_ppo2.py:193][0m |          -0.0139 |          25.4606 |           3.0507 |
[32m[20230203 20:58:21 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:58:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 72.81
[32m[20230203 20:58:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.85
[32m[20230203 20:58:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.14
[32m[20230203 20:58:21 @agent_ppo2.py:151][0m Total time:       7.76 min
[32m[20230203 20:58:21 @agent_ppo2.py:153][0m 634880 total steps have happened
[32m[20230203 20:58:21 @agent_ppo2.py:129][0m #------------------------ Iteration 310 --------------------------#
[32m[20230203 20:58:21 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:58:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:21 @agent_ppo2.py:193][0m |           0.0006 |          15.6085 |           3.0795 |
[32m[20230203 20:58:21 @agent_ppo2.py:193][0m |          -0.0029 |          13.7398 |           3.0777 |
[32m[20230203 20:58:22 @agent_ppo2.py:193][0m |          -0.0041 |          13.2715 |           3.0734 |
[32m[20230203 20:58:22 @agent_ppo2.py:193][0m |          -0.0067 |          12.9085 |           3.0767 |
[32m[20230203 20:58:22 @agent_ppo2.py:193][0m |          -0.0074 |          12.6470 |           3.0737 |
[32m[20230203 20:58:22 @agent_ppo2.py:193][0m |          -0.0102 |          12.4420 |           3.0751 |
[32m[20230203 20:58:22 @agent_ppo2.py:193][0m |          -0.0118 |          12.2943 |           3.0720 |
[32m[20230203 20:58:22 @agent_ppo2.py:193][0m |          -0.0122 |          12.1305 |           3.0716 |
[32m[20230203 20:58:22 @agent_ppo2.py:193][0m |          -0.0126 |          12.0434 |           3.0709 |
[32m[20230203 20:58:22 @agent_ppo2.py:193][0m |          -0.0136 |          11.9152 |           3.0714 |
[32m[20230203 20:58:22 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:58:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.61
[32m[20230203 20:58:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.32
[32m[20230203 20:58:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 183.24
[32m[20230203 20:58:22 @agent_ppo2.py:151][0m Total time:       7.78 min
[32m[20230203 20:58:22 @agent_ppo2.py:153][0m 636928 total steps have happened
[32m[20230203 20:58:22 @agent_ppo2.py:129][0m #------------------------ Iteration 311 --------------------------#
[32m[20230203 20:58:23 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:58:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:23 @agent_ppo2.py:193][0m |          -0.0012 |          11.8671 |           3.0387 |
[32m[20230203 20:58:23 @agent_ppo2.py:193][0m |          -0.0073 |          11.4637 |           3.0370 |
[32m[20230203 20:58:23 @agent_ppo2.py:193][0m |          -0.0093 |          11.2587 |           3.0358 |
[32m[20230203 20:58:23 @agent_ppo2.py:193][0m |          -0.0127 |          11.0612 |           3.0358 |
[32m[20230203 20:58:23 @agent_ppo2.py:193][0m |          -0.0133 |          10.9609 |           3.0333 |
[32m[20230203 20:58:23 @agent_ppo2.py:193][0m |          -0.0149 |          10.9051 |           3.0344 |
[32m[20230203 20:58:23 @agent_ppo2.py:193][0m |          -0.0141 |          10.8644 |           3.0336 |
[32m[20230203 20:58:23 @agent_ppo2.py:193][0m |          -0.0163 |          10.6930 |           3.0314 |
[32m[20230203 20:58:23 @agent_ppo2.py:193][0m |          -0.0175 |          10.5805 |           3.0336 |
[32m[20230203 20:58:23 @agent_ppo2.py:193][0m |          -0.0159 |          10.5683 |           3.0350 |
[32m[20230203 20:58:23 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:58:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 241.77
[32m[20230203 20:58:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.56
[32m[20230203 20:58:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.88
[32m[20230203 20:58:24 @agent_ppo2.py:151][0m Total time:       7.81 min
[32m[20230203 20:58:24 @agent_ppo2.py:153][0m 638976 total steps have happened
[32m[20230203 20:58:24 @agent_ppo2.py:129][0m #------------------------ Iteration 312 --------------------------#
[32m[20230203 20:58:24 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:58:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:24 @agent_ppo2.py:193][0m |           0.0001 |          11.5596 |           3.0412 |
[32m[20230203 20:58:24 @agent_ppo2.py:193][0m |          -0.0060 |          10.4108 |           3.0403 |
[32m[20230203 20:58:24 @agent_ppo2.py:193][0m |          -0.0044 |          10.1256 |           3.0362 |
[32m[20230203 20:58:24 @agent_ppo2.py:193][0m |          -0.0131 |           9.9866 |           3.0376 |
[32m[20230203 20:58:24 @agent_ppo2.py:193][0m |          -0.0095 |           9.8822 |           3.0395 |
[32m[20230203 20:58:24 @agent_ppo2.py:193][0m |          -0.0087 |           9.8965 |           3.0373 |
[32m[20230203 20:58:24 @agent_ppo2.py:193][0m |          -0.0129 |           9.7191 |           3.0396 |
[32m[20230203 20:58:24 @agent_ppo2.py:193][0m |          -0.0059 |          10.3749 |           3.0352 |
[32m[20230203 20:58:24 @agent_ppo2.py:193][0m |          -0.0095 |          10.2824 |           3.0366 |
[32m[20230203 20:58:24 @agent_ppo2.py:193][0m |          -0.0066 |          10.1183 |           3.0352 |
[32m[20230203 20:58:24 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 20:58:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 244.59
[32m[20230203 20:58:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.32
[32m[20230203 20:58:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.50
[32m[20230203 20:58:25 @agent_ppo2.py:151][0m Total time:       7.83 min
[32m[20230203 20:58:25 @agent_ppo2.py:153][0m 641024 total steps have happened
[32m[20230203 20:58:25 @agent_ppo2.py:129][0m #------------------------ Iteration 313 --------------------------#
[32m[20230203 20:58:25 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:58:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:25 @agent_ppo2.py:193][0m |           0.0001 |          22.4985 |           3.0074 |
[32m[20230203 20:58:25 @agent_ppo2.py:193][0m |          -0.0015 |          16.9738 |           3.0045 |
[32m[20230203 20:58:25 @agent_ppo2.py:193][0m |          -0.0056 |          16.2619 |           3.0012 |
[32m[20230203 20:58:25 @agent_ppo2.py:193][0m |          -0.0083 |          15.8026 |           2.9998 |
[32m[20230203 20:58:26 @agent_ppo2.py:193][0m |          -0.0080 |          15.7500 |           2.9972 |
[32m[20230203 20:58:26 @agent_ppo2.py:193][0m |          -0.0079 |          15.4333 |           2.9977 |
[32m[20230203 20:58:26 @agent_ppo2.py:193][0m |          -0.0113 |          15.3118 |           2.9980 |
[32m[20230203 20:58:26 @agent_ppo2.py:193][0m |          -0.0119 |          15.0294 |           2.9954 |
[32m[20230203 20:58:26 @agent_ppo2.py:193][0m |          -0.0115 |          15.1250 |           2.9926 |
[32m[20230203 20:58:26 @agent_ppo2.py:193][0m |          -0.0141 |          14.7795 |           2.9903 |
[32m[20230203 20:58:26 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:58:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 134.09
[32m[20230203 20:58:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.96
[32m[20230203 20:58:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.92
[32m[20230203 20:58:26 @agent_ppo2.py:151][0m Total time:       7.85 min
[32m[20230203 20:58:26 @agent_ppo2.py:153][0m 643072 total steps have happened
[32m[20230203 20:58:26 @agent_ppo2.py:129][0m #------------------------ Iteration 314 --------------------------#
[32m[20230203 20:58:27 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:58:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:27 @agent_ppo2.py:193][0m |          -0.0054 |          10.6895 |           2.9836 |
[32m[20230203 20:58:27 @agent_ppo2.py:193][0m |          -0.0112 |          10.2572 |           2.9752 |
[32m[20230203 20:58:27 @agent_ppo2.py:193][0m |          -0.0154 |           9.9312 |           2.9666 |
[32m[20230203 20:58:27 @agent_ppo2.py:193][0m |          -0.0163 |           9.7487 |           2.9681 |
[32m[20230203 20:58:27 @agent_ppo2.py:193][0m |          -0.0121 |          10.0030 |           2.9680 |
[32m[20230203 20:58:27 @agent_ppo2.py:193][0m |           0.0021 |          10.9342 |           2.9681 |
[32m[20230203 20:58:27 @agent_ppo2.py:193][0m |          -0.0116 |           9.7368 |           2.9647 |
[32m[20230203 20:58:27 @agent_ppo2.py:193][0m |          -0.0229 |           9.3698 |           2.9679 |
[32m[20230203 20:58:27 @agent_ppo2.py:193][0m |          -0.0220 |           9.3134 |           2.9667 |
[32m[20230203 20:58:27 @agent_ppo2.py:193][0m |          -0.0037 |           9.8878 |           2.9681 |
[32m[20230203 20:58:27 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:58:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.95
[32m[20230203 20:58:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.86
[32m[20230203 20:58:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.98
[32m[20230203 20:58:27 @agent_ppo2.py:151][0m Total time:       7.87 min
[32m[20230203 20:58:27 @agent_ppo2.py:153][0m 645120 total steps have happened
[32m[20230203 20:58:27 @agent_ppo2.py:129][0m #------------------------ Iteration 315 --------------------------#
[32m[20230203 20:58:28 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:58:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:28 @agent_ppo2.py:193][0m |           0.0019 |          11.0644 |           3.0153 |
[32m[20230203 20:58:28 @agent_ppo2.py:193][0m |          -0.0036 |          10.3148 |           3.0173 |
[32m[20230203 20:58:28 @agent_ppo2.py:193][0m |          -0.0054 |          10.0263 |           3.0209 |
[32m[20230203 20:58:28 @agent_ppo2.py:193][0m |          -0.0067 |           9.8407 |           3.0244 |
[32m[20230203 20:58:28 @agent_ppo2.py:193][0m |          -0.0090 |           9.6885 |           3.0207 |
[32m[20230203 20:58:28 @agent_ppo2.py:193][0m |          -0.0103 |           9.6326 |           3.0212 |
[32m[20230203 20:58:28 @agent_ppo2.py:193][0m |          -0.0105 |           9.5360 |           3.0197 |
[32m[20230203 20:58:28 @agent_ppo2.py:193][0m |          -0.0092 |           9.4972 |           3.0228 |
[32m[20230203 20:58:28 @agent_ppo2.py:193][0m |          -0.0119 |           9.3821 |           3.0218 |
[32m[20230203 20:58:28 @agent_ppo2.py:193][0m |          -0.0107 |           9.3269 |           3.0186 |
[32m[20230203 20:58:28 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:58:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.17
[32m[20230203 20:58:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.26
[32m[20230203 20:58:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 260.17
[32m[20230203 20:58:29 @agent_ppo2.py:151][0m Total time:       7.89 min
[32m[20230203 20:58:29 @agent_ppo2.py:153][0m 647168 total steps have happened
[32m[20230203 20:58:29 @agent_ppo2.py:129][0m #------------------------ Iteration 316 --------------------------#
[32m[20230203 20:58:29 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:58:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:29 @agent_ppo2.py:193][0m |           0.0050 |          10.8393 |           2.9929 |
[32m[20230203 20:58:29 @agent_ppo2.py:193][0m |           0.0014 |          10.3402 |           2.9900 |
[32m[20230203 20:58:29 @agent_ppo2.py:193][0m |          -0.0025 |          10.0192 |           2.9887 |
[32m[20230203 20:58:29 @agent_ppo2.py:193][0m |          -0.0046 |           9.8746 |           2.9855 |
[32m[20230203 20:58:29 @agent_ppo2.py:193][0m |          -0.0109 |           9.0984 |           2.9860 |
[32m[20230203 20:58:29 @agent_ppo2.py:193][0m |          -0.0109 |           9.1426 |           2.9879 |
[32m[20230203 20:58:29 @agent_ppo2.py:193][0m |          -0.0095 |           8.9706 |           2.9864 |
[32m[20230203 20:58:29 @agent_ppo2.py:193][0m |          -0.0151 |           8.8460 |           2.9868 |
[32m[20230203 20:58:30 @agent_ppo2.py:193][0m |          -0.0138 |           8.7581 |           2.9872 |
[32m[20230203 20:58:30 @agent_ppo2.py:193][0m |          -0.0160 |           8.7444 |           2.9865 |
[32m[20230203 20:58:30 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:58:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 240.27
[32m[20230203 20:58:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 242.79
[32m[20230203 20:58:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.81
[32m[20230203 20:58:30 @agent_ppo2.py:151][0m Total time:       7.91 min
[32m[20230203 20:58:30 @agent_ppo2.py:153][0m 649216 total steps have happened
[32m[20230203 20:58:30 @agent_ppo2.py:129][0m #------------------------ Iteration 317 --------------------------#
[32m[20230203 20:58:30 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:58:30 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:30 @agent_ppo2.py:193][0m |           0.0089 |          12.8096 |           3.0011 |
[32m[20230203 20:58:30 @agent_ppo2.py:193][0m |          -0.0065 |          10.6358 |           2.9983 |
[32m[20230203 20:58:30 @agent_ppo2.py:193][0m |          -0.0127 |           9.9310 |           2.9974 |
[32m[20230203 20:58:30 @agent_ppo2.py:193][0m |          -0.0154 |           9.4607 |           2.9942 |
[32m[20230203 20:58:31 @agent_ppo2.py:193][0m |          -0.0167 |           8.9201 |           2.9941 |
[32m[20230203 20:58:31 @agent_ppo2.py:193][0m |          -0.0119 |           8.5429 |           2.9922 |
[32m[20230203 20:58:31 @agent_ppo2.py:193][0m |          -0.0173 |           8.3265 |           2.9951 |
[32m[20230203 20:58:31 @agent_ppo2.py:193][0m |           0.0028 |           9.8391 |           2.9941 |
[32m[20230203 20:58:31 @agent_ppo2.py:193][0m |          -0.0185 |           7.9683 |           2.9963 |
[32m[20230203 20:58:31 @agent_ppo2.py:193][0m |          -0.0159 |           7.8166 |           2.9915 |
[32m[20230203 20:58:31 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:58:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.60
[32m[20230203 20:58:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.91
[32m[20230203 20:58:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.79
[32m[20230203 20:58:31 @agent_ppo2.py:151][0m Total time:       7.93 min
[32m[20230203 20:58:31 @agent_ppo2.py:153][0m 651264 total steps have happened
[32m[20230203 20:58:31 @agent_ppo2.py:129][0m #------------------------ Iteration 318 --------------------------#
[32m[20230203 20:58:31 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:58:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:32 @agent_ppo2.py:193][0m |          -0.0017 |          12.3876 |           3.1170 |
[32m[20230203 20:58:32 @agent_ppo2.py:193][0m |          -0.0065 |          12.1184 |           3.1016 |
[32m[20230203 20:58:32 @agent_ppo2.py:193][0m |          -0.0087 |          11.9887 |           3.1024 |
[32m[20230203 20:58:32 @agent_ppo2.py:193][0m |          -0.0112 |          11.7190 |           3.0995 |
[32m[20230203 20:58:32 @agent_ppo2.py:193][0m |          -0.0129 |          11.6608 |           3.0968 |
[32m[20230203 20:58:32 @agent_ppo2.py:193][0m |          -0.0117 |          11.8939 |           3.0991 |
[32m[20230203 20:58:32 @agent_ppo2.py:193][0m |          -0.0144 |          11.5792 |           3.0977 |
[32m[20230203 20:58:32 @agent_ppo2.py:193][0m |          -0.0149 |          11.5025 |           3.0992 |
[32m[20230203 20:58:32 @agent_ppo2.py:193][0m |          -0.0156 |          11.3977 |           3.0979 |
[32m[20230203 20:58:32 @agent_ppo2.py:193][0m |          -0.0157 |          11.3580 |           3.0955 |
[32m[20230203 20:58:32 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 20:58:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 254.55
[32m[20230203 20:58:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.18
[32m[20230203 20:58:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.71
[32m[20230203 20:58:32 @agent_ppo2.py:151][0m Total time:       7.95 min
[32m[20230203 20:58:32 @agent_ppo2.py:153][0m 653312 total steps have happened
[32m[20230203 20:58:32 @agent_ppo2.py:129][0m #------------------------ Iteration 319 --------------------------#
[32m[20230203 20:58:33 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:58:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:33 @agent_ppo2.py:193][0m |          -0.0023 |          11.0263 |           3.0359 |
[32m[20230203 20:58:33 @agent_ppo2.py:193][0m |          -0.0078 |          10.6380 |           3.0310 |
[32m[20230203 20:58:33 @agent_ppo2.py:193][0m |          -0.0029 |          10.6307 |           3.0312 |
[32m[20230203 20:58:33 @agent_ppo2.py:193][0m |          -0.0026 |          11.5634 |           3.0285 |
[32m[20230203 20:58:33 @agent_ppo2.py:193][0m |          -0.0017 |          10.9849 |           3.0224 |
[32m[20230203 20:58:33 @agent_ppo2.py:193][0m |          -0.0023 |          10.6926 |           3.0229 |
[32m[20230203 20:58:33 @agent_ppo2.py:193][0m |          -0.0079 |           9.9546 |           3.0166 |
[32m[20230203 20:58:33 @agent_ppo2.py:193][0m |          -0.0070 |          10.3083 |           3.0200 |
[32m[20230203 20:58:33 @agent_ppo2.py:193][0m |          -0.0126 |           9.7733 |           3.0198 |
[32m[20230203 20:58:33 @agent_ppo2.py:193][0m |          -0.0131 |           9.7806 |           3.0165 |
[32m[20230203 20:58:33 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:58:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.45
[32m[20230203 20:58:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.04
[32m[20230203 20:58:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 119.97
[32m[20230203 20:58:34 @agent_ppo2.py:151][0m Total time:       7.97 min
[32m[20230203 20:58:34 @agent_ppo2.py:153][0m 655360 total steps have happened
[32m[20230203 20:58:34 @agent_ppo2.py:129][0m #------------------------ Iteration 320 --------------------------#
[32m[20230203 20:58:34 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:58:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:34 @agent_ppo2.py:193][0m |          -0.0024 |          33.9912 |           3.0015 |
[32m[20230203 20:58:34 @agent_ppo2.py:193][0m |          -0.0084 |          25.4396 |           2.9974 |
[32m[20230203 20:58:34 @agent_ppo2.py:193][0m |          -0.0089 |          23.3762 |           2.9958 |
[32m[20230203 20:58:34 @agent_ppo2.py:193][0m |          -0.0114 |          22.0442 |           2.9969 |
[32m[20230203 20:58:34 @agent_ppo2.py:193][0m |          -0.0123 |          21.5995 |           2.9972 |
[32m[20230203 20:58:34 @agent_ppo2.py:193][0m |          -0.0125 |          21.2298 |           2.9948 |
[32m[20230203 20:58:34 @agent_ppo2.py:193][0m |          -0.0133 |          21.5636 |           2.9932 |
[32m[20230203 20:58:34 @agent_ppo2.py:193][0m |          -0.0130 |          20.2585 |           2.9958 |
[32m[20230203 20:58:34 @agent_ppo2.py:193][0m |          -0.0150 |          20.1605 |           2.9956 |
[32m[20230203 20:58:34 @agent_ppo2.py:193][0m |          -0.0018 |          23.5904 |           2.9941 |
[32m[20230203 20:58:34 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:58:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 139.06
[32m[20230203 20:58:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.70
[32m[20230203 20:58:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.38
[32m[20230203 20:58:35 @agent_ppo2.py:151][0m Total time:       7.99 min
[32m[20230203 20:58:35 @agent_ppo2.py:153][0m 657408 total steps have happened
[32m[20230203 20:58:35 @agent_ppo2.py:129][0m #------------------------ Iteration 321 --------------------------#
[32m[20230203 20:58:35 @agent_ppo2.py:135][0m Sampling time: 0.50 s by 1 slaves
[32m[20230203 20:58:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:35 @agent_ppo2.py:193][0m |          -0.0035 |          48.2743 |           3.0178 |
[32m[20230203 20:58:35 @agent_ppo2.py:193][0m |           0.0312 |          30.3432 |           3.0185 |
[32m[20230203 20:58:35 @agent_ppo2.py:193][0m |          -0.0056 |          26.0136 |           3.0070 |
[32m[20230203 20:58:35 @agent_ppo2.py:193][0m |          -0.0103 |          24.2189 |           3.0085 |
[32m[20230203 20:58:36 @agent_ppo2.py:193][0m |          -0.0128 |          23.4640 |           3.0117 |
[32m[20230203 20:58:36 @agent_ppo2.py:193][0m |          -0.0182 |          22.9307 |           3.0098 |
[32m[20230203 20:58:36 @agent_ppo2.py:193][0m |          -0.0143 |          22.4437 |           3.0088 |
[32m[20230203 20:58:36 @agent_ppo2.py:193][0m |          -0.0073 |          22.0697 |           3.0097 |
[32m[20230203 20:58:36 @agent_ppo2.py:193][0m |          -0.0130 |          21.8600 |           3.0091 |
[32m[20230203 20:58:36 @agent_ppo2.py:193][0m |           0.0055 |          21.9702 |           3.0092 |
[32m[20230203 20:58:36 @agent_ppo2.py:138][0m Policy update time: 0.57 s
[32m[20230203 20:58:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 103.80
[32m[20230203 20:58:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 258.20
[32m[20230203 20:58:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.44
[32m[20230203 20:58:36 @agent_ppo2.py:151][0m Total time:       8.01 min
[32m[20230203 20:58:36 @agent_ppo2.py:153][0m 659456 total steps have happened
[32m[20230203 20:58:36 @agent_ppo2.py:129][0m #------------------------ Iteration 322 --------------------------#
[32m[20230203 20:58:37 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:58:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:37 @agent_ppo2.py:193][0m |           0.0029 |          76.3518 |           3.0530 |
[32m[20230203 20:58:37 @agent_ppo2.py:193][0m |          -0.0062 |          64.7918 |           3.0511 |
[32m[20230203 20:58:37 @agent_ppo2.py:193][0m |          -0.0075 |          59.0423 |           3.0485 |
[32m[20230203 20:58:37 @agent_ppo2.py:193][0m |          -0.0087 |          55.4048 |           3.0456 |
[32m[20230203 20:58:37 @agent_ppo2.py:193][0m |          -0.0127 |          53.0450 |           3.0435 |
[32m[20230203 20:58:37 @agent_ppo2.py:193][0m |          -0.0098 |          51.0459 |           3.0439 |
[32m[20230203 20:58:37 @agent_ppo2.py:193][0m |          -0.0122 |          47.5252 |           3.0426 |
[32m[20230203 20:58:37 @agent_ppo2.py:193][0m |          -0.0098 |          45.2684 |           3.0410 |
[32m[20230203 20:58:37 @agent_ppo2.py:193][0m |          -0.0126 |          43.5061 |           3.0378 |
[32m[20230203 20:58:37 @agent_ppo2.py:193][0m |          -0.0153 |          41.0616 |           3.0392 |
[32m[20230203 20:58:37 @agent_ppo2.py:138][0m Policy update time: 0.41 s
[32m[20230203 20:58:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 73.15
[32m[20230203 20:58:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.35
[32m[20230203 20:58:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.66
[32m[20230203 20:58:37 @agent_ppo2.py:151][0m Total time:       8.03 min
[32m[20230203 20:58:37 @agent_ppo2.py:153][0m 661504 total steps have happened
[32m[20230203 20:58:37 @agent_ppo2.py:129][0m #------------------------ Iteration 323 --------------------------#
[32m[20230203 20:58:38 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:58:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:38 @agent_ppo2.py:193][0m |           0.0004 |          17.5564 |           3.0221 |
[32m[20230203 20:58:38 @agent_ppo2.py:193][0m |          -0.0028 |          14.1939 |           3.0214 |
[32m[20230203 20:58:38 @agent_ppo2.py:193][0m |          -0.0019 |          13.5309 |           3.0192 |
[32m[20230203 20:58:38 @agent_ppo2.py:193][0m |          -0.0130 |          12.7415 |           3.0198 |
[32m[20230203 20:58:38 @agent_ppo2.py:193][0m |          -0.0117 |          12.3725 |           3.0156 |
[32m[20230203 20:58:38 @agent_ppo2.py:193][0m |          -0.0116 |          12.1503 |           3.0163 |
[32m[20230203 20:58:38 @agent_ppo2.py:193][0m |          -0.0128 |          11.9849 |           3.0145 |
[32m[20230203 20:58:38 @agent_ppo2.py:193][0m |          -0.0151 |          11.6570 |           3.0127 |
[32m[20230203 20:58:38 @agent_ppo2.py:193][0m |          -0.0145 |          11.6742 |           3.0128 |
[32m[20230203 20:58:38 @agent_ppo2.py:193][0m |          -0.0088 |          12.1156 |           3.0135 |
[32m[20230203 20:58:38 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:58:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.15
[32m[20230203 20:58:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.89
[32m[20230203 20:58:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 172.86
[32m[20230203 20:58:39 @agent_ppo2.py:151][0m Total time:       8.06 min
[32m[20230203 20:58:39 @agent_ppo2.py:153][0m 663552 total steps have happened
[32m[20230203 20:58:39 @agent_ppo2.py:129][0m #------------------------ Iteration 324 --------------------------#
[32m[20230203 20:58:39 @agent_ppo2.py:135][0m Sampling time: 0.50 s by 1 slaves
[32m[20230203 20:58:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:39 @agent_ppo2.py:193][0m |          -0.0007 |          25.0052 |           3.0979 |
[32m[20230203 20:58:39 @agent_ppo2.py:193][0m |          -0.0051 |          19.2632 |           3.0916 |
[32m[20230203 20:58:39 @agent_ppo2.py:193][0m |          -0.0068 |          17.7781 |           3.0922 |
[32m[20230203 20:58:39 @agent_ppo2.py:193][0m |          -0.0092 |          17.0068 |           3.0945 |
[32m[20230203 20:58:39 @agent_ppo2.py:193][0m |          -0.0094 |          16.2174 |           3.0920 |
[32m[20230203 20:58:39 @agent_ppo2.py:193][0m |          -0.0102 |          16.0172 |           3.0918 |
[32m[20230203 20:58:39 @agent_ppo2.py:193][0m |          -0.0103 |          15.5403 |           3.0901 |
[32m[20230203 20:58:40 @agent_ppo2.py:193][0m |          -0.0123 |          15.2131 |           3.0876 |
[32m[20230203 20:58:40 @agent_ppo2.py:193][0m |          -0.0116 |          14.9187 |           3.0944 |
[32m[20230203 20:58:40 @agent_ppo2.py:193][0m |          -0.0127 |          14.6004 |           3.0907 |
[32m[20230203 20:58:40 @agent_ppo2.py:138][0m Policy update time: 0.57 s
[32m[20230203 20:58:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 183.36
[32m[20230203 20:58:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.13
[32m[20230203 20:58:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 197.26
[32m[20230203 20:58:40 @agent_ppo2.py:151][0m Total time:       8.08 min
[32m[20230203 20:58:40 @agent_ppo2.py:153][0m 665600 total steps have happened
[32m[20230203 20:58:40 @agent_ppo2.py:129][0m #------------------------ Iteration 325 --------------------------#
[32m[20230203 20:58:41 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:58:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:41 @agent_ppo2.py:193][0m |          -0.0031 |          16.5119 |           3.0847 |
[32m[20230203 20:58:41 @agent_ppo2.py:193][0m |          -0.0077 |          13.2706 |           3.0822 |
[32m[20230203 20:58:41 @agent_ppo2.py:193][0m |          -0.0116 |          12.6972 |           3.0827 |
[32m[20230203 20:58:41 @agent_ppo2.py:193][0m |          -0.0117 |          12.4130 |           3.0825 |
[32m[20230203 20:58:41 @agent_ppo2.py:193][0m |          -0.0130 |          12.1793 |           3.0843 |
[32m[20230203 20:58:41 @agent_ppo2.py:193][0m |          -0.0133 |          12.0958 |           3.0875 |
[32m[20230203 20:58:41 @agent_ppo2.py:193][0m |          -0.0137 |          11.8532 |           3.0839 |
[32m[20230203 20:58:41 @agent_ppo2.py:193][0m |          -0.0123 |          11.8942 |           3.0876 |
[32m[20230203 20:58:41 @agent_ppo2.py:193][0m |          -0.0139 |          11.6042 |           3.0879 |
[32m[20230203 20:58:41 @agent_ppo2.py:193][0m |          -0.0158 |          11.4533 |           3.0872 |
[32m[20230203 20:58:41 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:58:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.05
[32m[20230203 20:58:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.44
[32m[20230203 20:58:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 260.06
[32m[20230203 20:58:41 @agent_ppo2.py:151][0m Total time:       8.10 min
[32m[20230203 20:58:41 @agent_ppo2.py:153][0m 667648 total steps have happened
[32m[20230203 20:58:41 @agent_ppo2.py:129][0m #------------------------ Iteration 326 --------------------------#
[32m[20230203 20:58:42 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 20:58:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:42 @agent_ppo2.py:193][0m |          -0.0041 |          61.6794 |           3.0204 |
[32m[20230203 20:58:42 @agent_ppo2.py:193][0m |          -0.0060 |          41.1888 |           3.0118 |
[32m[20230203 20:58:42 @agent_ppo2.py:193][0m |          -0.0135 |          35.0311 |           3.0089 |
[32m[20230203 20:58:42 @agent_ppo2.py:193][0m |          -0.0131 |          34.1477 |           3.0085 |
[32m[20230203 20:58:42 @agent_ppo2.py:193][0m |          -0.0142 |          32.5843 |           3.0084 |
[32m[20230203 20:58:42 @agent_ppo2.py:193][0m |          -0.0045 |          30.8711 |           3.0063 |
[32m[20230203 20:58:42 @agent_ppo2.py:193][0m |          -0.0107 |          30.5727 |           3.0029 |
[32m[20230203 20:58:42 @agent_ppo2.py:193][0m |          -0.0077 |          32.2308 |           3.0013 |
[32m[20230203 20:58:42 @agent_ppo2.py:193][0m |          -0.0152 |          30.1378 |           3.0016 |
[32m[20230203 20:58:42 @agent_ppo2.py:193][0m |          -0.0185 |          28.6844 |           3.0021 |
[32m[20230203 20:58:42 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 20:58:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 10.17
[32m[20230203 20:58:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.76
[32m[20230203 20:58:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.85
[32m[20230203 20:58:43 @agent_ppo2.py:151][0m Total time:       8.12 min
[32m[20230203 20:58:43 @agent_ppo2.py:153][0m 669696 total steps have happened
[32m[20230203 20:58:43 @agent_ppo2.py:129][0m #------------------------ Iteration 327 --------------------------#
[32m[20230203 20:58:43 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:58:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:43 @agent_ppo2.py:193][0m |           0.0006 |          20.2575 |           3.0580 |
[32m[20230203 20:58:43 @agent_ppo2.py:193][0m |          -0.0064 |          15.1439 |           3.0570 |
[32m[20230203 20:58:43 @agent_ppo2.py:193][0m |          -0.0072 |          14.4346 |           3.0551 |
[32m[20230203 20:58:43 @agent_ppo2.py:193][0m |          -0.0088 |          13.9763 |           3.0552 |
[32m[20230203 20:58:43 @agent_ppo2.py:193][0m |          -0.0090 |          13.5890 |           3.0546 |
[32m[20230203 20:58:43 @agent_ppo2.py:193][0m |          -0.0098 |          13.3304 |           3.0564 |
[32m[20230203 20:58:43 @agent_ppo2.py:193][0m |          -0.0105 |          13.0790 |           3.0555 |
[32m[20230203 20:58:43 @agent_ppo2.py:193][0m |          -0.0109 |          12.8224 |           3.0587 |
[32m[20230203 20:58:43 @agent_ppo2.py:193][0m |          -0.0113 |          12.6400 |           3.0563 |
[32m[20230203 20:58:43 @agent_ppo2.py:193][0m |          -0.0119 |          12.5236 |           3.0540 |
[32m[20230203 20:58:43 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:58:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.25
[32m[20230203 20:58:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.51
[32m[20230203 20:58:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.35
[32m[20230203 20:58:44 @agent_ppo2.py:151][0m Total time:       8.14 min
[32m[20230203 20:58:44 @agent_ppo2.py:153][0m 671744 total steps have happened
[32m[20230203 20:58:44 @agent_ppo2.py:129][0m #------------------------ Iteration 328 --------------------------#
[32m[20230203 20:58:44 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 1 slaves
[32m[20230203 20:58:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:44 @agent_ppo2.py:193][0m |          -0.0026 |          26.1695 |           3.1656 |
[32m[20230203 20:58:44 @agent_ppo2.py:193][0m |          -0.0100 |          17.7207 |           3.1571 |
[32m[20230203 20:58:44 @agent_ppo2.py:193][0m |          -0.0132 |          16.5324 |           3.1530 |
[32m[20230203 20:58:44 @agent_ppo2.py:193][0m |          -0.0151 |          15.6620 |           3.1551 |
[32m[20230203 20:58:44 @agent_ppo2.py:193][0m |          -0.0164 |          14.9404 |           3.1551 |
[32m[20230203 20:58:44 @agent_ppo2.py:193][0m |          -0.0173 |          14.2786 |           3.1552 |
[32m[20230203 20:58:44 @agent_ppo2.py:193][0m |          -0.0175 |          13.7312 |           3.1528 |
[32m[20230203 20:58:45 @agent_ppo2.py:193][0m |          -0.0176 |          13.2521 |           3.1481 |
[32m[20230203 20:58:45 @agent_ppo2.py:193][0m |          -0.0189 |          12.7234 |           3.1507 |
[32m[20230203 20:58:45 @agent_ppo2.py:193][0m |          -0.0196 |          12.1477 |           3.1504 |
[32m[20230203 20:58:45 @agent_ppo2.py:138][0m Policy update time: 0.40 s
[32m[20230203 20:58:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 148.44
[32m[20230203 20:58:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.63
[32m[20230203 20:58:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.03
[32m[20230203 20:58:45 @agent_ppo2.py:151][0m Total time:       8.16 min
[32m[20230203 20:58:45 @agent_ppo2.py:153][0m 673792 total steps have happened
[32m[20230203 20:58:45 @agent_ppo2.py:129][0m #------------------------ Iteration 329 --------------------------#
[32m[20230203 20:58:45 @agent_ppo2.py:135][0m Sampling time: 0.36 s by 1 slaves
[32m[20230203 20:58:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:45 @agent_ppo2.py:193][0m |          -0.0014 |         120.7443 |           3.0708 |
[32m[20230203 20:58:45 @agent_ppo2.py:193][0m |          -0.0090 |          63.6326 |           3.0631 |
[32m[20230203 20:58:45 @agent_ppo2.py:193][0m |          -0.0140 |          49.7114 |           3.0622 |
[32m[20230203 20:58:45 @agent_ppo2.py:193][0m |          -0.0159 |          42.8956 |           3.0575 |
[32m[20230203 20:58:46 @agent_ppo2.py:193][0m |          -0.0154 |          38.8946 |           3.0557 |
[32m[20230203 20:58:46 @agent_ppo2.py:193][0m |          -0.0195 |          36.2772 |           3.0538 |
[32m[20230203 20:58:46 @agent_ppo2.py:193][0m |          -0.0200 |          34.9343 |           3.0506 |
[32m[20230203 20:58:46 @agent_ppo2.py:193][0m |          -0.0215 |          33.9057 |           3.0508 |
[32m[20230203 20:58:46 @agent_ppo2.py:193][0m |          -0.0205 |          33.0398 |           3.0505 |
[32m[20230203 20:58:46 @agent_ppo2.py:193][0m |          -0.0206 |          32.0139 |           3.0482 |
[32m[20230203 20:58:46 @agent_ppo2.py:138][0m Policy update time: 0.41 s
[32m[20230203 20:58:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 50.54
[32m[20230203 20:58:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 237.27
[32m[20230203 20:58:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.88
[32m[20230203 20:58:46 @agent_ppo2.py:151][0m Total time:       8.18 min
[32m[20230203 20:58:46 @agent_ppo2.py:153][0m 675840 total steps have happened
[32m[20230203 20:58:46 @agent_ppo2.py:129][0m #------------------------ Iteration 330 --------------------------#
[32m[20230203 20:58:47 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:58:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:47 @agent_ppo2.py:193][0m |           0.0018 |          54.0787 |           3.0595 |
[32m[20230203 20:58:47 @agent_ppo2.py:193][0m |          -0.0016 |          41.6946 |           3.0570 |
[32m[20230203 20:58:47 @agent_ppo2.py:193][0m |          -0.0038 |          38.3297 |           3.0604 |
[32m[20230203 20:58:47 @agent_ppo2.py:193][0m |          -0.0074 |          36.5922 |           3.0564 |
[32m[20230203 20:58:47 @agent_ppo2.py:193][0m |          -0.0086 |          34.9464 |           3.0559 |
[32m[20230203 20:58:47 @agent_ppo2.py:193][0m |          -0.0092 |          33.8927 |           3.0578 |
[32m[20230203 20:58:47 @agent_ppo2.py:193][0m |          -0.0121 |          33.1919 |           3.0556 |
[32m[20230203 20:58:47 @agent_ppo2.py:193][0m |          -0.0120 |          32.2176 |           3.0556 |
[32m[20230203 20:58:47 @agent_ppo2.py:193][0m |          -0.0131 |          31.6075 |           3.0587 |
[32m[20230203 20:58:47 @agent_ppo2.py:193][0m |          -0.0129 |          31.3383 |           3.0571 |
[32m[20230203 20:58:47 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:58:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 97.26
[32m[20230203 20:58:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.86
[32m[20230203 20:58:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.64
[32m[20230203 20:58:47 @agent_ppo2.py:151][0m Total time:       8.20 min
[32m[20230203 20:58:47 @agent_ppo2.py:153][0m 677888 total steps have happened
[32m[20230203 20:58:47 @agent_ppo2.py:129][0m #------------------------ Iteration 331 --------------------------#
[32m[20230203 20:58:48 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 20:58:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:48 @agent_ppo2.py:193][0m |          -0.0024 |          65.4930 |           3.1049 |
[32m[20230203 20:58:48 @agent_ppo2.py:193][0m |          -0.0095 |          53.2142 |           3.0956 |
[32m[20230203 20:58:48 @agent_ppo2.py:193][0m |          -0.0125 |          49.7319 |           3.0931 |
[32m[20230203 20:58:48 @agent_ppo2.py:193][0m |          -0.0143 |          47.1199 |           3.0938 |
[32m[20230203 20:58:48 @agent_ppo2.py:193][0m |          -0.0155 |          45.1816 |           3.0957 |
[32m[20230203 20:58:48 @agent_ppo2.py:193][0m |          -0.0162 |          44.0130 |           3.0946 |
[32m[20230203 20:58:48 @agent_ppo2.py:193][0m |          -0.0169 |          42.6297 |           3.0944 |
[32m[20230203 20:58:48 @agent_ppo2.py:193][0m |          -0.0172 |          41.7238 |           3.0909 |
[32m[20230203 20:58:48 @agent_ppo2.py:193][0m |          -0.0176 |          40.8269 |           3.0940 |
[32m[20230203 20:58:48 @agent_ppo2.py:193][0m |          -0.0182 |          39.9431 |           3.0949 |
[32m[20230203 20:58:48 @agent_ppo2.py:138][0m Policy update time: 0.44 s
[32m[20230203 20:58:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 162.06
[32m[20230203 20:58:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.75
[32m[20230203 20:58:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 198.36
[32m[20230203 20:58:49 @agent_ppo2.py:151][0m Total time:       8.22 min
[32m[20230203 20:58:49 @agent_ppo2.py:153][0m 679936 total steps have happened
[32m[20230203 20:58:49 @agent_ppo2.py:129][0m #------------------------ Iteration 332 --------------------------#
[32m[20230203 20:58:49 @agent_ppo2.py:135][0m Sampling time: 0.52 s by 1 slaves
[32m[20230203 20:58:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:49 @agent_ppo2.py:193][0m |           0.0026 |          50.3534 |           3.0414 |
[32m[20230203 20:58:49 @agent_ppo2.py:193][0m |          -0.0050 |          43.5996 |           3.0312 |
[32m[20230203 20:58:49 @agent_ppo2.py:193][0m |          -0.0067 |          38.9666 |           3.0293 |
[32m[20230203 20:58:49 @agent_ppo2.py:193][0m |          -0.0070 |          37.6091 |           3.0294 |
[32m[20230203 20:58:49 @agent_ppo2.py:193][0m |          -0.0135 |          33.9401 |           3.0291 |
[32m[20230203 20:58:49 @agent_ppo2.py:193][0m |          -0.0144 |          32.8560 |           3.0329 |
[32m[20230203 20:58:49 @agent_ppo2.py:193][0m |          -0.0131 |          31.6009 |           3.0312 |
[32m[20230203 20:58:50 @agent_ppo2.py:193][0m |          -0.0115 |          31.9029 |           3.0336 |
[32m[20230203 20:58:50 @agent_ppo2.py:193][0m |          -0.0201 |          29.6179 |           3.0329 |
[32m[20230203 20:58:50 @agent_ppo2.py:193][0m |          -0.0123 |          29.8493 |           3.0340 |
[32m[20230203 20:58:50 @agent_ppo2.py:138][0m Policy update time: 0.58 s
[32m[20230203 20:58:50 @agent_ppo2.py:146][0m Average TRAINING episode reward: 157.07
[32m[20230203 20:58:50 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.83
[32m[20230203 20:58:50 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.82
[32m[20230203 20:58:50 @agent_ppo2.py:151][0m Total time:       8.25 min
[32m[20230203 20:58:50 @agent_ppo2.py:153][0m 681984 total steps have happened
[32m[20230203 20:58:50 @agent_ppo2.py:129][0m #------------------------ Iteration 333 --------------------------#
[32m[20230203 20:58:50 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:58:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:50 @agent_ppo2.py:193][0m |          -0.0020 |          15.0083 |           2.9768 |
[32m[20230203 20:58:51 @agent_ppo2.py:193][0m |          -0.0075 |          11.5078 |           2.9744 |
[32m[20230203 20:58:51 @agent_ppo2.py:193][0m |          -0.0072 |          10.6968 |           2.9734 |
[32m[20230203 20:58:51 @agent_ppo2.py:193][0m |          -0.0095 |          10.2678 |           2.9747 |
[32m[20230203 20:58:51 @agent_ppo2.py:193][0m |          -0.0075 |          10.0679 |           2.9747 |
[32m[20230203 20:58:51 @agent_ppo2.py:193][0m |          -0.0133 |           9.8460 |           2.9753 |
[32m[20230203 20:58:51 @agent_ppo2.py:193][0m |          -0.0139 |           9.7718 |           2.9754 |
[32m[20230203 20:58:51 @agent_ppo2.py:193][0m |          -0.0156 |           9.6025 |           2.9761 |
[32m[20230203 20:58:51 @agent_ppo2.py:193][0m |          -0.0085 |           9.6918 |           2.9744 |
[32m[20230203 20:58:51 @agent_ppo2.py:193][0m |          -0.0136 |           9.4586 |           2.9746 |
[32m[20230203 20:58:51 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:58:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.24
[32m[20230203 20:58:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.91
[32m[20230203 20:58:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 260.37
[32m[20230203 20:58:51 @agent_ppo2.py:151][0m Total time:       8.27 min
[32m[20230203 20:58:51 @agent_ppo2.py:153][0m 684032 total steps have happened
[32m[20230203 20:58:51 @agent_ppo2.py:129][0m #------------------------ Iteration 334 --------------------------#
[32m[20230203 20:58:52 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:58:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:52 @agent_ppo2.py:193][0m |           0.0003 |          12.5142 |           3.0361 |
[32m[20230203 20:58:52 @agent_ppo2.py:193][0m |           0.0015 |          11.8555 |           3.0367 |
[32m[20230203 20:58:52 @agent_ppo2.py:193][0m |          -0.0080 |          11.0028 |           3.0357 |
[32m[20230203 20:58:52 @agent_ppo2.py:193][0m |          -0.0066 |          10.7716 |           3.0356 |
[32m[20230203 20:58:52 @agent_ppo2.py:193][0m |          -0.0083 |          10.4468 |           3.0343 |
[32m[20230203 20:58:52 @agent_ppo2.py:193][0m |          -0.0137 |          10.2303 |           3.0349 |
[32m[20230203 20:58:52 @agent_ppo2.py:193][0m |          -0.0124 |          10.0544 |           3.0320 |
[32m[20230203 20:58:52 @agent_ppo2.py:193][0m |          -0.0114 |           9.8653 |           3.0314 |
[32m[20230203 20:58:52 @agent_ppo2.py:193][0m |          -0.0152 |           9.7074 |           3.0316 |
[32m[20230203 20:58:52 @agent_ppo2.py:193][0m |          -0.0134 |           9.5121 |           3.0325 |
[32m[20230203 20:58:52 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:58:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.11
[32m[20230203 20:58:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.77
[32m[20230203 20:58:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.32
[32m[20230203 20:58:52 @agent_ppo2.py:151][0m Total time:       8.29 min
[32m[20230203 20:58:52 @agent_ppo2.py:153][0m 686080 total steps have happened
[32m[20230203 20:58:52 @agent_ppo2.py:129][0m #------------------------ Iteration 335 --------------------------#
[32m[20230203 20:58:53 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:58:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:53 @agent_ppo2.py:193][0m |          -0.0003 |          34.2258 |           3.0764 |
[32m[20230203 20:58:53 @agent_ppo2.py:193][0m |          -0.0038 |          24.7704 |           3.0761 |
[32m[20230203 20:58:53 @agent_ppo2.py:193][0m |          -0.0075 |          22.7443 |           3.0744 |
[32m[20230203 20:58:53 @agent_ppo2.py:193][0m |          -0.0071 |          21.6128 |           3.0740 |
[32m[20230203 20:58:53 @agent_ppo2.py:193][0m |          -0.0065 |          21.5785 |           3.0716 |
[32m[20230203 20:58:53 @agent_ppo2.py:193][0m |          -0.0091 |          20.8164 |           3.0696 |
[32m[20230203 20:58:53 @agent_ppo2.py:193][0m |          -0.0112 |          20.0787 |           3.0661 |
[32m[20230203 20:58:53 @agent_ppo2.py:193][0m |          -0.0119 |          19.6071 |           3.0670 |
[32m[20230203 20:58:53 @agent_ppo2.py:193][0m |          -0.0148 |          19.2700 |           3.0680 |
[32m[20230203 20:58:53 @agent_ppo2.py:193][0m |          -0.0149 |          19.1379 |           3.0657 |
[32m[20230203 20:58:53 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:58:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 192.25
[32m[20230203 20:58:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.32
[32m[20230203 20:58:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.31
[32m[20230203 20:58:54 @agent_ppo2.py:151][0m Total time:       8.31 min
[32m[20230203 20:58:54 @agent_ppo2.py:153][0m 688128 total steps have happened
[32m[20230203 20:58:54 @agent_ppo2.py:129][0m #------------------------ Iteration 336 --------------------------#
[32m[20230203 20:58:54 @agent_ppo2.py:135][0m Sampling time: 0.52 s by 1 slaves
[32m[20230203 20:58:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:54 @agent_ppo2.py:193][0m |          -0.0019 |          53.6984 |           3.1022 |
[32m[20230203 20:58:54 @agent_ppo2.py:193][0m |          -0.0085 |          43.2789 |           3.0961 |
[32m[20230203 20:58:54 @agent_ppo2.py:193][0m |          -0.0076 |          40.5002 |           3.0956 |
[32m[20230203 20:58:54 @agent_ppo2.py:193][0m |          -0.0105 |          38.6743 |           3.0925 |
[32m[20230203 20:58:54 @agent_ppo2.py:193][0m |          -0.0084 |          37.9586 |           3.0938 |
[32m[20230203 20:58:55 @agent_ppo2.py:193][0m |          -0.0122 |          36.6485 |           3.0894 |
[32m[20230203 20:58:55 @agent_ppo2.py:193][0m |          -0.0095 |          36.3463 |           3.0907 |
[32m[20230203 20:58:55 @agent_ppo2.py:193][0m |          -0.0136 |          35.6128 |           3.0919 |
[32m[20230203 20:58:55 @agent_ppo2.py:193][0m |          -0.0085 |          36.6382 |           3.0898 |
[32m[20230203 20:58:55 @agent_ppo2.py:193][0m |          -0.0071 |          35.7692 |           3.0896 |
[32m[20230203 20:58:55 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:58:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 101.00
[32m[20230203 20:58:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.26
[32m[20230203 20:58:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.75
[32m[20230203 20:58:55 @agent_ppo2.py:151][0m Total time:       8.33 min
[32m[20230203 20:58:55 @agent_ppo2.py:153][0m 690176 total steps have happened
[32m[20230203 20:58:55 @agent_ppo2.py:129][0m #------------------------ Iteration 337 --------------------------#
[32m[20230203 20:58:55 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:58:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:56 @agent_ppo2.py:193][0m |           0.0045 |          12.8215 |           3.1175 |
[32m[20230203 20:58:56 @agent_ppo2.py:193][0m |          -0.0032 |          11.6286 |           3.1085 |
[32m[20230203 20:58:56 @agent_ppo2.py:193][0m |          -0.0076 |          10.9604 |           3.1047 |
[32m[20230203 20:58:56 @agent_ppo2.py:193][0m |          -0.0072 |          10.7687 |           3.1038 |
[32m[20230203 20:58:56 @agent_ppo2.py:193][0m |          -0.0088 |          10.6525 |           3.1002 |
[32m[20230203 20:58:56 @agent_ppo2.py:193][0m |          -0.0112 |          10.3927 |           3.0997 |
[32m[20230203 20:58:56 @agent_ppo2.py:193][0m |          -0.0112 |          10.2781 |           3.0999 |
[32m[20230203 20:58:56 @agent_ppo2.py:193][0m |          -0.0115 |          10.2224 |           3.0950 |
[32m[20230203 20:58:56 @agent_ppo2.py:193][0m |          -0.0118 |          10.1183 |           3.0968 |
[32m[20230203 20:58:56 @agent_ppo2.py:193][0m |          -0.0118 |          10.0740 |           3.0983 |
[32m[20230203 20:58:56 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:58:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 248.36
[32m[20230203 20:58:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.25
[32m[20230203 20:58:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.05
[32m[20230203 20:58:56 @agent_ppo2.py:151][0m Total time:       8.35 min
[32m[20230203 20:58:56 @agent_ppo2.py:153][0m 692224 total steps have happened
[32m[20230203 20:58:56 @agent_ppo2.py:129][0m #------------------------ Iteration 338 --------------------------#
[32m[20230203 20:58:57 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 20:58:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:57 @agent_ppo2.py:193][0m |          -0.0022 |          32.1445 |           3.0730 |
[32m[20230203 20:58:57 @agent_ppo2.py:193][0m |          -0.0085 |          27.3135 |           3.0699 |
[32m[20230203 20:58:57 @agent_ppo2.py:193][0m |          -0.0133 |          25.6267 |           3.0670 |
[32m[20230203 20:58:57 @agent_ppo2.py:193][0m |          -0.0145 |          24.5286 |           3.0666 |
[32m[20230203 20:58:57 @agent_ppo2.py:193][0m |          -0.0076 |          24.6997 |           3.0644 |
[32m[20230203 20:58:57 @agent_ppo2.py:193][0m |          -0.0162 |          22.9157 |           3.0636 |
[32m[20230203 20:58:57 @agent_ppo2.py:193][0m |          -0.0126 |          23.3262 |           3.0622 |
[32m[20230203 20:58:57 @agent_ppo2.py:193][0m |          -0.0162 |          22.1192 |           3.0623 |
[32m[20230203 20:58:57 @agent_ppo2.py:193][0m |          -0.0164 |          21.6342 |           3.0614 |
[32m[20230203 20:58:57 @agent_ppo2.py:193][0m |          -0.0166 |          21.1664 |           3.0618 |
[32m[20230203 20:58:57 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 20:58:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 176.90
[32m[20230203 20:58:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.28
[32m[20230203 20:58:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 166.29
[32m[20230203 20:58:58 @agent_ppo2.py:151][0m Total time:       8.37 min
[32m[20230203 20:58:58 @agent_ppo2.py:153][0m 694272 total steps have happened
[32m[20230203 20:58:58 @agent_ppo2.py:129][0m #------------------------ Iteration 339 --------------------------#
[32m[20230203 20:58:58 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:58:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:58 @agent_ppo2.py:193][0m |          -0.0036 |          14.0184 |           3.0381 |
[32m[20230203 20:58:58 @agent_ppo2.py:193][0m |          -0.0121 |          11.9034 |           3.0296 |
[32m[20230203 20:58:58 @agent_ppo2.py:193][0m |          -0.0130 |          11.1463 |           3.0299 |
[32m[20230203 20:58:58 @agent_ppo2.py:193][0m |          -0.0144 |          10.8649 |           3.0248 |
[32m[20230203 20:58:58 @agent_ppo2.py:193][0m |          -0.0008 |          12.0603 |           3.0311 |
[32m[20230203 20:58:58 @agent_ppo2.py:193][0m |          -0.0083 |          10.9738 |           3.0331 |
[32m[20230203 20:58:58 @agent_ppo2.py:193][0m |          -0.0117 |          10.4837 |           3.0278 |
[32m[20230203 20:58:58 @agent_ppo2.py:193][0m |          -0.0068 |          10.7951 |           3.0311 |
[32m[20230203 20:58:58 @agent_ppo2.py:193][0m |          -0.0123 |          10.3420 |           3.0300 |
[32m[20230203 20:58:58 @agent_ppo2.py:193][0m |          -0.0135 |          10.3235 |           3.0248 |
[32m[20230203 20:58:58 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:58:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.48
[32m[20230203 20:58:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.01
[32m[20230203 20:58:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.30
[32m[20230203 20:58:59 @agent_ppo2.py:151][0m Total time:       8.39 min
[32m[20230203 20:58:59 @agent_ppo2.py:153][0m 696320 total steps have happened
[32m[20230203 20:58:59 @agent_ppo2.py:129][0m #------------------------ Iteration 340 --------------------------#
[32m[20230203 20:58:59 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:58:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:58:59 @agent_ppo2.py:193][0m |           0.0025 |          20.3522 |           3.1322 |
[32m[20230203 20:58:59 @agent_ppo2.py:193][0m |          -0.0054 |          17.2779 |           3.1215 |
[32m[20230203 20:58:59 @agent_ppo2.py:193][0m |          -0.0102 |          16.6385 |           3.1222 |
[32m[20230203 20:58:59 @agent_ppo2.py:193][0m |          -0.0096 |          16.4558 |           3.1171 |
[32m[20230203 20:58:59 @agent_ppo2.py:193][0m |          -0.0117 |          15.6924 |           3.1172 |
[32m[20230203 20:58:59 @agent_ppo2.py:193][0m |          -0.0117 |          15.3299 |           3.1164 |
[32m[20230203 20:58:59 @agent_ppo2.py:193][0m |          -0.0143 |          14.9944 |           3.1125 |
[32m[20230203 20:59:00 @agent_ppo2.py:193][0m |          -0.0183 |          14.7014 |           3.1099 |
[32m[20230203 20:59:00 @agent_ppo2.py:193][0m |          -0.0161 |          14.2578 |           3.1108 |
[32m[20230203 20:59:00 @agent_ppo2.py:193][0m |          -0.0146 |          14.0679 |           3.1120 |
[32m[20230203 20:59:00 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 248.17
[32m[20230203 20:59:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.59
[32m[20230203 20:59:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.40
[32m[20230203 20:59:00 @agent_ppo2.py:151][0m Total time:       8.41 min
[32m[20230203 20:59:00 @agent_ppo2.py:153][0m 698368 total steps have happened
[32m[20230203 20:59:00 @agent_ppo2.py:129][0m #------------------------ Iteration 341 --------------------------#
[32m[20230203 20:59:00 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:59:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:00 @agent_ppo2.py:193][0m |           0.0021 |          14.0294 |           3.0443 |
[32m[20230203 20:59:00 @agent_ppo2.py:193][0m |          -0.0070 |          12.0267 |           3.0445 |
[32m[20230203 20:59:00 @agent_ppo2.py:193][0m |          -0.0110 |          11.2953 |           3.0367 |
[32m[20230203 20:59:01 @agent_ppo2.py:193][0m |          -0.0116 |          10.9193 |           3.0353 |
[32m[20230203 20:59:01 @agent_ppo2.py:193][0m |          -0.0124 |          10.8376 |           3.0381 |
[32m[20230203 20:59:01 @agent_ppo2.py:193][0m |          -0.0126 |          10.3466 |           3.0367 |
[32m[20230203 20:59:01 @agent_ppo2.py:193][0m |          -0.0135 |          10.2202 |           3.0396 |
[32m[20230203 20:59:01 @agent_ppo2.py:193][0m |          -0.0140 |           9.9293 |           3.0382 |
[32m[20230203 20:59:01 @agent_ppo2.py:193][0m |          -0.0156 |           9.7170 |           3.0357 |
[32m[20230203 20:59:01 @agent_ppo2.py:193][0m |          -0.0159 |           9.5200 |           3.0380 |
[32m[20230203 20:59:01 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 20:59:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.64
[32m[20230203 20:59:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.74
[32m[20230203 20:59:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.63
[32m[20230203 20:59:01 @agent_ppo2.py:151][0m Total time:       8.43 min
[32m[20230203 20:59:01 @agent_ppo2.py:153][0m 700416 total steps have happened
[32m[20230203 20:59:01 @agent_ppo2.py:129][0m #------------------------ Iteration 342 --------------------------#
[32m[20230203 20:59:02 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:59:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:02 @agent_ppo2.py:193][0m |           0.0001 |          49.0448 |           3.0512 |
[32m[20230203 20:59:02 @agent_ppo2.py:193][0m |          -0.0045 |          33.3026 |           3.0508 |
[32m[20230203 20:59:02 @agent_ppo2.py:193][0m |          -0.0076 |          29.4676 |           3.0517 |
[32m[20230203 20:59:02 @agent_ppo2.py:193][0m |          -0.0086 |          27.6873 |           3.0484 |
[32m[20230203 20:59:02 @agent_ppo2.py:193][0m |          -0.0083 |          26.4741 |           3.0518 |
[32m[20230203 20:59:02 @agent_ppo2.py:193][0m |          -0.0101 |          25.5939 |           3.0481 |
[32m[20230203 20:59:02 @agent_ppo2.py:193][0m |          -0.0106 |          25.0455 |           3.0510 |
[32m[20230203 20:59:02 @agent_ppo2.py:193][0m |          -0.0109 |          24.4594 |           3.0467 |
[32m[20230203 20:59:02 @agent_ppo2.py:193][0m |          -0.0108 |          23.8644 |           3.0484 |
[32m[20230203 20:59:02 @agent_ppo2.py:193][0m |          -0.0126 |          23.4603 |           3.0485 |
[32m[20230203 20:59:02 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:59:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 84.14
[32m[20230203 20:59:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.66
[32m[20230203 20:59:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.30
[32m[20230203 20:59:02 @agent_ppo2.py:151][0m Total time:       8.45 min
[32m[20230203 20:59:02 @agent_ppo2.py:153][0m 702464 total steps have happened
[32m[20230203 20:59:02 @agent_ppo2.py:129][0m #------------------------ Iteration 343 --------------------------#
[32m[20230203 20:59:03 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 1 slaves
[32m[20230203 20:59:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:03 @agent_ppo2.py:193][0m |          -0.0010 |          39.2955 |           3.1833 |
[32m[20230203 20:59:03 @agent_ppo2.py:193][0m |          -0.0073 |          26.8463 |           3.1794 |
[32m[20230203 20:59:03 @agent_ppo2.py:193][0m |          -0.0113 |          23.8768 |           3.1784 |
[32m[20230203 20:59:03 @agent_ppo2.py:193][0m |          -0.0115 |          22.6079 |           3.1721 |
[32m[20230203 20:59:03 @agent_ppo2.py:193][0m |          -0.0135 |          21.2410 |           3.1725 |
[32m[20230203 20:59:03 @agent_ppo2.py:193][0m |          -0.0152 |          20.8287 |           3.1675 |
[32m[20230203 20:59:03 @agent_ppo2.py:193][0m |          -0.0160 |          20.5966 |           3.1672 |
[32m[20230203 20:59:03 @agent_ppo2.py:193][0m |          -0.0155 |          20.2928 |           3.1685 |
[32m[20230203 20:59:03 @agent_ppo2.py:193][0m |          -0.0160 |          20.2356 |           3.1654 |
[32m[20230203 20:59:03 @agent_ppo2.py:193][0m |          -0.0164 |          20.1136 |           3.1656 |
[32m[20230203 20:59:03 @agent_ppo2.py:138][0m Policy update time: 0.40 s
[32m[20230203 20:59:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 62.66
[32m[20230203 20:59:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.00
[32m[20230203 20:59:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.41
[32m[20230203 20:59:04 @agent_ppo2.py:151][0m Total time:       8.47 min
[32m[20230203 20:59:04 @agent_ppo2.py:153][0m 704512 total steps have happened
[32m[20230203 20:59:04 @agent_ppo2.py:129][0m #------------------------ Iteration 344 --------------------------#
[32m[20230203 20:59:04 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:59:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:04 @agent_ppo2.py:193][0m |           0.0013 |          59.1087 |           3.0553 |
[32m[20230203 20:59:04 @agent_ppo2.py:193][0m |          -0.0047 |          46.2359 |           3.0553 |
[32m[20230203 20:59:04 @agent_ppo2.py:193][0m |          -0.0099 |          41.1515 |           3.0545 |
[32m[20230203 20:59:04 @agent_ppo2.py:193][0m |          -0.0118 |          38.6627 |           3.0550 |
[32m[20230203 20:59:04 @agent_ppo2.py:193][0m |          -0.0114 |          36.7505 |           3.0527 |
[32m[20230203 20:59:04 @agent_ppo2.py:193][0m |          -0.0158 |          35.3353 |           3.0562 |
[32m[20230203 20:59:04 @agent_ppo2.py:193][0m |          -0.0133 |          34.1857 |           3.0510 |
[32m[20230203 20:59:04 @agent_ppo2.py:193][0m |          -0.0136 |          33.2174 |           3.0542 |
[32m[20230203 20:59:04 @agent_ppo2.py:193][0m |          -0.0128 |          33.0850 |           3.0542 |
[32m[20230203 20:59:04 @agent_ppo2.py:193][0m |          -0.0159 |          31.8875 |           3.0536 |
[32m[20230203 20:59:04 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 20:59:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 73.31
[32m[20230203 20:59:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.21
[32m[20230203 20:59:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.55
[32m[20230203 20:59:05 @agent_ppo2.py:151][0m Total time:       8.49 min
[32m[20230203 20:59:05 @agent_ppo2.py:153][0m 706560 total steps have happened
[32m[20230203 20:59:05 @agent_ppo2.py:129][0m #------------------------ Iteration 345 --------------------------#
[32m[20230203 20:59:05 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 20:59:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:05 @agent_ppo2.py:193][0m |           0.0005 |          45.9899 |           3.0648 |
[32m[20230203 20:59:05 @agent_ppo2.py:193][0m |          -0.0060 |          35.4396 |           3.0619 |
[32m[20230203 20:59:05 @agent_ppo2.py:193][0m |          -0.0083 |          31.9584 |           3.0563 |
[32m[20230203 20:59:05 @agent_ppo2.py:193][0m |          -0.0095 |          29.5311 |           3.0518 |
[32m[20230203 20:59:05 @agent_ppo2.py:193][0m |          -0.0123 |          28.2996 |           3.0537 |
[32m[20230203 20:59:05 @agent_ppo2.py:193][0m |          -0.0132 |          26.1400 |           3.0540 |
[32m[20230203 20:59:06 @agent_ppo2.py:193][0m |          -0.0129 |          25.0898 |           3.0536 |
[32m[20230203 20:59:06 @agent_ppo2.py:193][0m |          -0.0128 |          24.7993 |           3.0563 |
[32m[20230203 20:59:06 @agent_ppo2.py:193][0m |          -0.0145 |          23.3149 |           3.0542 |
[32m[20230203 20:59:06 @agent_ppo2.py:193][0m |          -0.0144 |          22.6533 |           3.0553 |
[32m[20230203 20:59:06 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 20:59:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 147.06
[32m[20230203 20:59:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.39
[32m[20230203 20:59:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.46
[32m[20230203 20:59:06 @agent_ppo2.py:151][0m Total time:       8.51 min
[32m[20230203 20:59:06 @agent_ppo2.py:153][0m 708608 total steps have happened
[32m[20230203 20:59:06 @agent_ppo2.py:129][0m #------------------------ Iteration 346 --------------------------#
[32m[20230203 20:59:06 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:59:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:06 @agent_ppo2.py:193][0m |           0.0021 |          14.3158 |           3.0680 |
[32m[20230203 20:59:07 @agent_ppo2.py:193][0m |          -0.0025 |          11.8467 |           3.0585 |
[32m[20230203 20:59:07 @agent_ppo2.py:193][0m |          -0.0073 |          10.9666 |           3.0584 |
[32m[20230203 20:59:07 @agent_ppo2.py:193][0m |          -0.0117 |          10.5328 |           3.0583 |
[32m[20230203 20:59:07 @agent_ppo2.py:193][0m |          -0.0165 |          10.2802 |           3.0590 |
[32m[20230203 20:59:07 @agent_ppo2.py:193][0m |          -0.0080 |          10.1375 |           3.0559 |
[32m[20230203 20:59:07 @agent_ppo2.py:193][0m |          -0.0175 |           9.9579 |           3.0600 |
[32m[20230203 20:59:07 @agent_ppo2.py:193][0m |          -0.0147 |           9.8333 |           3.0586 |
[32m[20230203 20:59:07 @agent_ppo2.py:193][0m |          -0.0169 |           9.7582 |           3.0574 |
[32m[20230203 20:59:07 @agent_ppo2.py:193][0m |          -0.0204 |           9.7085 |           3.0599 |
[32m[20230203 20:59:07 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.12
[32m[20230203 20:59:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.59
[32m[20230203 20:59:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 263.76
[32m[20230203 20:59:07 @agent_ppo2.py:151][0m Total time:       8.53 min
[32m[20230203 20:59:07 @agent_ppo2.py:153][0m 710656 total steps have happened
[32m[20230203 20:59:07 @agent_ppo2.py:129][0m #------------------------ Iteration 347 --------------------------#
[32m[20230203 20:59:08 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:08 @agent_ppo2.py:193][0m |          -0.0023 |          10.7440 |           3.1309 |
[32m[20230203 20:59:08 @agent_ppo2.py:193][0m |          -0.0050 |          10.2988 |           3.1245 |
[32m[20230203 20:59:08 @agent_ppo2.py:193][0m |          -0.0071 |          10.0451 |           3.1207 |
[32m[20230203 20:59:08 @agent_ppo2.py:193][0m |          -0.0096 |           9.8549 |           3.1241 |
[32m[20230203 20:59:08 @agent_ppo2.py:193][0m |          -0.0100 |           9.8527 |           3.1221 |
[32m[20230203 20:59:08 @agent_ppo2.py:193][0m |          -0.0078 |           9.8487 |           3.1229 |
[32m[20230203 20:59:08 @agent_ppo2.py:193][0m |          -0.0109 |           9.6733 |           3.1231 |
[32m[20230203 20:59:08 @agent_ppo2.py:193][0m |          -0.0126 |           9.4197 |           3.1227 |
[32m[20230203 20:59:08 @agent_ppo2.py:193][0m |          -0.0120 |           9.5310 |           3.1226 |
[32m[20230203 20:59:08 @agent_ppo2.py:193][0m |          -0.0130 |           9.2702 |           3.1219 |
[32m[20230203 20:59:08 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.05
[32m[20230203 20:59:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.55
[32m[20230203 20:59:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 113.49
[32m[20230203 20:59:08 @agent_ppo2.py:151][0m Total time:       8.55 min
[32m[20230203 20:59:08 @agent_ppo2.py:153][0m 712704 total steps have happened
[32m[20230203 20:59:08 @agent_ppo2.py:129][0m #------------------------ Iteration 348 --------------------------#
[32m[20230203 20:59:09 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:59:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:09 @agent_ppo2.py:193][0m |          -0.0002 |          14.0554 |           3.0765 |
[32m[20230203 20:59:09 @agent_ppo2.py:193][0m |          -0.0014 |          11.6699 |           3.0739 |
[32m[20230203 20:59:09 @agent_ppo2.py:193][0m |          -0.0077 |          10.8141 |           3.0701 |
[32m[20230203 20:59:09 @agent_ppo2.py:193][0m |          -0.0101 |          10.4317 |           3.0686 |
[32m[20230203 20:59:09 @agent_ppo2.py:193][0m |          -0.0115 |          10.1809 |           3.0677 |
[32m[20230203 20:59:09 @agent_ppo2.py:193][0m |          -0.0130 |          10.0202 |           3.0687 |
[32m[20230203 20:59:09 @agent_ppo2.py:193][0m |          -0.0109 |           9.9094 |           3.0654 |
[32m[20230203 20:59:09 @agent_ppo2.py:193][0m |          -0.0123 |           9.7506 |           3.0680 |
[32m[20230203 20:59:09 @agent_ppo2.py:193][0m |          -0.0138 |           9.7405 |           3.0665 |
[32m[20230203 20:59:09 @agent_ppo2.py:193][0m |          -0.0143 |           9.5536 |           3.0639 |
[32m[20230203 20:59:09 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.44
[32m[20230203 20:59:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.56
[32m[20230203 20:59:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.46
[32m[20230203 20:59:10 @agent_ppo2.py:151][0m Total time:       8.57 min
[32m[20230203 20:59:10 @agent_ppo2.py:153][0m 714752 total steps have happened
[32m[20230203 20:59:10 @agent_ppo2.py:129][0m #------------------------ Iteration 349 --------------------------#
[32m[20230203 20:59:10 @agent_ppo2.py:135][0m Sampling time: 0.50 s by 1 slaves
[32m[20230203 20:59:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:10 @agent_ppo2.py:193][0m |           0.0065 |          36.2197 |           3.0679 |
[32m[20230203 20:59:10 @agent_ppo2.py:193][0m |          -0.0077 |          25.0887 |           3.0608 |
[32m[20230203 20:59:10 @agent_ppo2.py:193][0m |          -0.0059 |          24.5495 |           3.0583 |
[32m[20230203 20:59:10 @agent_ppo2.py:193][0m |          -0.0057 |          23.8459 |           3.0568 |
[32m[20230203 20:59:11 @agent_ppo2.py:193][0m |           0.0010 |          24.7035 |           3.0559 |
[32m[20230203 20:59:11 @agent_ppo2.py:193][0m |          -0.0103 |          23.4302 |           3.0529 |
[32m[20230203 20:59:11 @agent_ppo2.py:193][0m |          -0.0123 |          22.7026 |           3.0551 |
[32m[20230203 20:59:11 @agent_ppo2.py:193][0m |          -0.0133 |          22.4796 |           3.0568 |
[32m[20230203 20:59:11 @agent_ppo2.py:193][0m |          -0.0139 |          22.2761 |           3.0547 |
[32m[20230203 20:59:11 @agent_ppo2.py:193][0m |          -0.0148 |          22.0206 |           3.0536 |
[32m[20230203 20:59:11 @agent_ppo2.py:138][0m Policy update time: 0.58 s
[32m[20230203 20:59:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 173.39
[32m[20230203 20:59:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.76
[32m[20230203 20:59:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.86
[32m[20230203 20:59:11 @agent_ppo2.py:151][0m Total time:       8.60 min
[32m[20230203 20:59:11 @agent_ppo2.py:153][0m 716800 total steps have happened
[32m[20230203 20:59:11 @agent_ppo2.py:129][0m #------------------------ Iteration 350 --------------------------#
[32m[20230203 20:59:12 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:59:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:12 @agent_ppo2.py:193][0m |           0.0019 |          12.4962 |           3.0561 |
[32m[20230203 20:59:12 @agent_ppo2.py:193][0m |          -0.0107 |          10.8009 |           3.0528 |
[32m[20230203 20:59:12 @agent_ppo2.py:193][0m |          -0.0112 |          10.5332 |           3.0490 |
[32m[20230203 20:59:12 @agent_ppo2.py:193][0m |          -0.0118 |          10.2950 |           3.0491 |
[32m[20230203 20:59:12 @agent_ppo2.py:193][0m |          -0.0139 |          10.1298 |           3.0448 |
[32m[20230203 20:59:12 @agent_ppo2.py:193][0m |          -0.0119 |          10.2538 |           3.0464 |
[32m[20230203 20:59:12 @agent_ppo2.py:193][0m |          -0.0164 |           9.8446 |           3.0476 |
[32m[20230203 20:59:12 @agent_ppo2.py:193][0m |          -0.0096 |          10.2956 |           3.0491 |
[32m[20230203 20:59:12 @agent_ppo2.py:193][0m |          -0.0172 |           9.6545 |           3.0451 |
[32m[20230203 20:59:12 @agent_ppo2.py:193][0m |          -0.0158 |           9.5599 |           3.0469 |
[32m[20230203 20:59:12 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:59:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.16
[32m[20230203 20:59:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.42
[32m[20230203 20:59:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 166.10
[32m[20230203 20:59:12 @agent_ppo2.py:151][0m Total time:       8.62 min
[32m[20230203 20:59:12 @agent_ppo2.py:153][0m 718848 total steps have happened
[32m[20230203 20:59:12 @agent_ppo2.py:129][0m #------------------------ Iteration 351 --------------------------#
[32m[20230203 20:59:13 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:59:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:13 @agent_ppo2.py:193][0m |           0.0001 |          53.8869 |           3.0958 |
[32m[20230203 20:59:13 @agent_ppo2.py:193][0m |          -0.0086 |          41.3562 |           3.0869 |
[32m[20230203 20:59:13 @agent_ppo2.py:193][0m |          -0.0133 |          38.7507 |           3.0882 |
[32m[20230203 20:59:13 @agent_ppo2.py:193][0m |          -0.0131 |          37.4585 |           3.0873 |
[32m[20230203 20:59:13 @agent_ppo2.py:193][0m |          -0.0157 |          36.7455 |           3.0884 |
[32m[20230203 20:59:13 @agent_ppo2.py:193][0m |          -0.0135 |          35.3579 |           3.0859 |
[32m[20230203 20:59:13 @agent_ppo2.py:193][0m |          -0.0137 |          34.8269 |           3.0890 |
[32m[20230203 20:59:13 @agent_ppo2.py:193][0m |          -0.0144 |          33.6000 |           3.0863 |
[32m[20230203 20:59:13 @agent_ppo2.py:193][0m |          -0.0162 |          32.5741 |           3.0885 |
[32m[20230203 20:59:13 @agent_ppo2.py:193][0m |          -0.0164 |          31.8967 |           3.0857 |
[32m[20230203 20:59:13 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:59:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 83.94
[32m[20230203 20:59:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 241.41
[32m[20230203 20:59:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.56
[32m[20230203 20:59:14 @agent_ppo2.py:151][0m Total time:       8.64 min
[32m[20230203 20:59:14 @agent_ppo2.py:153][0m 720896 total steps have happened
[32m[20230203 20:59:14 @agent_ppo2.py:129][0m #------------------------ Iteration 352 --------------------------#
[32m[20230203 20:59:14 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:59:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:14 @agent_ppo2.py:193][0m |          -0.0033 |          20.6094 |           3.0295 |
[32m[20230203 20:59:14 @agent_ppo2.py:193][0m |          -0.0022 |          13.9072 |           3.0301 |
[32m[20230203 20:59:14 @agent_ppo2.py:193][0m |          -0.0064 |          13.2860 |           3.0271 |
[32m[20230203 20:59:14 @agent_ppo2.py:193][0m |          -0.0088 |          12.8968 |           3.0260 |
[32m[20230203 20:59:14 @agent_ppo2.py:193][0m |          -0.0082 |          12.6199 |           3.0246 |
[32m[20230203 20:59:14 @agent_ppo2.py:193][0m |          -0.0123 |          12.4201 |           3.0278 |
[32m[20230203 20:59:14 @agent_ppo2.py:193][0m |          -0.0212 |          12.5027 |           3.0249 |
[32m[20230203 20:59:14 @agent_ppo2.py:193][0m |          -0.0089 |          12.1916 |           3.0270 |
[32m[20230203 20:59:14 @agent_ppo2.py:193][0m |          -0.0170 |          12.0360 |           3.0239 |
[32m[20230203 20:59:15 @agent_ppo2.py:193][0m |          -0.0124 |          11.8019 |           3.0261 |
[32m[20230203 20:59:15 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.48
[32m[20230203 20:59:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.80
[32m[20230203 20:59:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.49
[32m[20230203 20:59:15 @agent_ppo2.py:151][0m Total time:       8.66 min
[32m[20230203 20:59:15 @agent_ppo2.py:153][0m 722944 total steps have happened
[32m[20230203 20:59:15 @agent_ppo2.py:129][0m #------------------------ Iteration 353 --------------------------#
[32m[20230203 20:59:15 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:15 @agent_ppo2.py:193][0m |           0.0018 |          10.5460 |           3.0191 |
[32m[20230203 20:59:15 @agent_ppo2.py:193][0m |          -0.0043 |          10.2061 |           3.0163 |
[32m[20230203 20:59:15 @agent_ppo2.py:193][0m |          -0.0025 |          10.3884 |           3.0077 |
[32m[20230203 20:59:16 @agent_ppo2.py:193][0m |          -0.0048 |           9.6988 |           3.0072 |
[32m[20230203 20:59:16 @agent_ppo2.py:193][0m |          -0.0120 |           9.4527 |           3.0061 |
[32m[20230203 20:59:16 @agent_ppo2.py:193][0m |          -0.0134 |           9.3142 |           3.0049 |
[32m[20230203 20:59:16 @agent_ppo2.py:193][0m |          -0.0104 |           9.2216 |           3.0024 |
[32m[20230203 20:59:16 @agent_ppo2.py:193][0m |          -0.0132 |           9.1252 |           3.0019 |
[32m[20230203 20:59:16 @agent_ppo2.py:193][0m |          -0.0115 |           9.1472 |           3.0023 |
[32m[20230203 20:59:16 @agent_ppo2.py:193][0m |          -0.0132 |           8.9578 |           3.0045 |
[32m[20230203 20:59:16 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:59:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.95
[32m[20230203 20:59:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.38
[32m[20230203 20:59:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 159.39
[32m[20230203 20:59:16 @agent_ppo2.py:151][0m Total time:       8.68 min
[32m[20230203 20:59:16 @agent_ppo2.py:153][0m 724992 total steps have happened
[32m[20230203 20:59:16 @agent_ppo2.py:129][0m #------------------------ Iteration 354 --------------------------#
[32m[20230203 20:59:17 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:17 @agent_ppo2.py:193][0m |          -0.0025 |          12.8764 |           3.1649 |
[32m[20230203 20:59:17 @agent_ppo2.py:193][0m |          -0.0097 |          12.0421 |           3.1588 |
[32m[20230203 20:59:17 @agent_ppo2.py:193][0m |          -0.0118 |          11.6465 |           3.1554 |
[32m[20230203 20:59:17 @agent_ppo2.py:193][0m |          -0.0121 |          11.3571 |           3.1567 |
[32m[20230203 20:59:17 @agent_ppo2.py:193][0m |          -0.0134 |          11.1847 |           3.1532 |
[32m[20230203 20:59:17 @agent_ppo2.py:193][0m |          -0.0134 |          10.9855 |           3.1554 |
[32m[20230203 20:59:17 @agent_ppo2.py:193][0m |          -0.0144 |          10.8222 |           3.1530 |
[32m[20230203 20:59:17 @agent_ppo2.py:193][0m |          -0.0144 |          10.7970 |           3.1535 |
[32m[20230203 20:59:17 @agent_ppo2.py:193][0m |          -0.0150 |          10.6154 |           3.1528 |
[32m[20230203 20:59:17 @agent_ppo2.py:193][0m |          -0.0149 |          10.5317 |           3.1528 |
[32m[20230203 20:59:17 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 244.71
[32m[20230203 20:59:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.49
[32m[20230203 20:59:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 260.64
[32m[20230203 20:59:17 @agent_ppo2.py:151][0m Total time:       8.70 min
[32m[20230203 20:59:17 @agent_ppo2.py:153][0m 727040 total steps have happened
[32m[20230203 20:59:17 @agent_ppo2.py:129][0m #------------------------ Iteration 355 --------------------------#
[32m[20230203 20:59:18 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 20:59:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:18 @agent_ppo2.py:193][0m |          -0.0061 |          43.7990 |           3.0225 |
[32m[20230203 20:59:18 @agent_ppo2.py:193][0m |          -0.0058 |          33.0346 |           3.0172 |
[32m[20230203 20:59:18 @agent_ppo2.py:193][0m |          -0.0076 |          29.8399 |           3.0106 |
[32m[20230203 20:59:18 @agent_ppo2.py:193][0m |           0.0566 |          57.0638 |           3.0137 |
[32m[20230203 20:59:18 @agent_ppo2.py:193][0m |          -0.0147 |          28.1777 |           3.0047 |
[32m[20230203 20:59:18 @agent_ppo2.py:193][0m |          -0.0115 |          26.7861 |           3.0073 |
[32m[20230203 20:59:18 @agent_ppo2.py:193][0m |          -0.0145 |          26.1621 |           3.0109 |
[32m[20230203 20:59:18 @agent_ppo2.py:193][0m |          -0.0190 |          25.7361 |           3.0124 |
[32m[20230203 20:59:18 @agent_ppo2.py:193][0m |          -0.0229 |          25.1548 |           3.0135 |
[32m[20230203 20:59:18 @agent_ppo2.py:193][0m |          -0.0227 |          24.6536 |           3.0089 |
[32m[20230203 20:59:18 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 20:59:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 135.76
[32m[20230203 20:59:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.46
[32m[20230203 20:59:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.30
[32m[20230203 20:59:19 @agent_ppo2.py:151][0m Total time:       8.73 min
[32m[20230203 20:59:19 @agent_ppo2.py:153][0m 729088 total steps have happened
[32m[20230203 20:59:19 @agent_ppo2.py:129][0m #------------------------ Iteration 356 --------------------------#
[32m[20230203 20:59:19 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:59:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:19 @agent_ppo2.py:193][0m |           0.0012 |          29.2970 |           3.1164 |
[32m[20230203 20:59:19 @agent_ppo2.py:193][0m |          -0.0057 |          24.0582 |           3.1192 |
[32m[20230203 20:59:20 @agent_ppo2.py:193][0m |          -0.0075 |          22.8341 |           3.1173 |
[32m[20230203 20:59:20 @agent_ppo2.py:193][0m |          -0.0095 |          22.2200 |           3.1176 |
[32m[20230203 20:59:20 @agent_ppo2.py:193][0m |          -0.0088 |          21.7800 |           3.1183 |
[32m[20230203 20:59:20 @agent_ppo2.py:193][0m |          -0.0100 |          21.4184 |           3.1155 |
[32m[20230203 20:59:20 @agent_ppo2.py:193][0m |          -0.0093 |          20.9095 |           3.1155 |
[32m[20230203 20:59:20 @agent_ppo2.py:193][0m |          -0.0106 |          20.8737 |           3.1165 |
[32m[20230203 20:59:20 @agent_ppo2.py:193][0m |          -0.0116 |          20.3580 |           3.1186 |
[32m[20230203 20:59:20 @agent_ppo2.py:193][0m |          -0.0114 |          19.9873 |           3.1175 |
[32m[20230203 20:59:20 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230203 20:59:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 181.39
[32m[20230203 20:59:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.72
[32m[20230203 20:59:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 46.97
[32m[20230203 20:59:20 @agent_ppo2.py:151][0m Total time:       8.75 min
[32m[20230203 20:59:20 @agent_ppo2.py:153][0m 731136 total steps have happened
[32m[20230203 20:59:20 @agent_ppo2.py:129][0m #------------------------ Iteration 357 --------------------------#
[32m[20230203 20:59:21 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:59:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:21 @agent_ppo2.py:193][0m |          -0.0013 |          13.7879 |           3.1476 |
[32m[20230203 20:59:21 @agent_ppo2.py:193][0m |          -0.0023 |          11.7112 |           3.1418 |
[32m[20230203 20:59:21 @agent_ppo2.py:193][0m |          -0.0105 |          11.1507 |           3.1404 |
[32m[20230203 20:59:21 @agent_ppo2.py:193][0m |          -0.0146 |          10.9438 |           3.1388 |
[32m[20230203 20:59:21 @agent_ppo2.py:193][0m |          -0.0061 |          11.1312 |           3.1379 |
[32m[20230203 20:59:21 @agent_ppo2.py:193][0m |          -0.0133 |          10.6333 |           3.1405 |
[32m[20230203 20:59:21 @agent_ppo2.py:193][0m |          -0.0143 |          10.5193 |           3.1353 |
[32m[20230203 20:59:21 @agent_ppo2.py:193][0m |          -0.0143 |          10.4368 |           3.1382 |
[32m[20230203 20:59:21 @agent_ppo2.py:193][0m |          -0.0143 |          10.3587 |           3.1372 |
[32m[20230203 20:59:21 @agent_ppo2.py:193][0m |          -0.0147 |          10.2461 |           3.1353 |
[32m[20230203 20:59:21 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.19
[32m[20230203 20:59:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.29
[32m[20230203 20:59:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.21
[32m[20230203 20:59:22 @agent_ppo2.py:151][0m Total time:       8.77 min
[32m[20230203 20:59:22 @agent_ppo2.py:153][0m 733184 total steps have happened
[32m[20230203 20:59:22 @agent_ppo2.py:129][0m #------------------------ Iteration 358 --------------------------#
[32m[20230203 20:59:22 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:22 @agent_ppo2.py:193][0m |           0.0010 |          30.4841 |           3.0847 |
[32m[20230203 20:59:22 @agent_ppo2.py:193][0m |          -0.0022 |          19.4462 |           3.0830 |
[32m[20230203 20:59:22 @agent_ppo2.py:193][0m |          -0.0056 |          18.1437 |           3.0853 |
[32m[20230203 20:59:22 @agent_ppo2.py:193][0m |          -0.0072 |          17.2030 |           3.0844 |
[32m[20230203 20:59:22 @agent_ppo2.py:193][0m |          -0.0092 |          16.6442 |           3.0836 |
[32m[20230203 20:59:22 @agent_ppo2.py:193][0m |          -0.0094 |          16.2331 |           3.0864 |
[32m[20230203 20:59:22 @agent_ppo2.py:193][0m |          -0.0119 |          15.8721 |           3.0824 |
[32m[20230203 20:59:22 @agent_ppo2.py:193][0m |          -0.0122 |          15.5517 |           3.0863 |
[32m[20230203 20:59:22 @agent_ppo2.py:193][0m |          -0.0138 |          15.2580 |           3.0862 |
[32m[20230203 20:59:22 @agent_ppo2.py:193][0m |          -0.0127 |          15.0956 |           3.0815 |
[32m[20230203 20:59:22 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.80
[32m[20230203 20:59:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.01
[32m[20230203 20:59:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.47
[32m[20230203 20:59:23 @agent_ppo2.py:151][0m Total time:       8.79 min
[32m[20230203 20:59:23 @agent_ppo2.py:153][0m 735232 total steps have happened
[32m[20230203 20:59:23 @agent_ppo2.py:129][0m #------------------------ Iteration 359 --------------------------#
[32m[20230203 20:59:23 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:23 @agent_ppo2.py:193][0m |           0.0007 |          10.7319 |           3.0531 |
[32m[20230203 20:59:23 @agent_ppo2.py:193][0m |          -0.0116 |          10.0145 |           3.0435 |
[32m[20230203 20:59:23 @agent_ppo2.py:193][0m |          -0.0108 |           9.8841 |           3.0393 |
[32m[20230203 20:59:23 @agent_ppo2.py:193][0m |          -0.0145 |           9.5528 |           3.0385 |
[32m[20230203 20:59:23 @agent_ppo2.py:193][0m |          -0.0157 |           9.4383 |           3.0389 |
[32m[20230203 20:59:23 @agent_ppo2.py:193][0m |          -0.0148 |           9.3779 |           3.0384 |
[32m[20230203 20:59:24 @agent_ppo2.py:193][0m |          -0.0176 |           9.2139 |           3.0363 |
[32m[20230203 20:59:24 @agent_ppo2.py:193][0m |          -0.0150 |           9.1510 |           3.0365 |
[32m[20230203 20:59:24 @agent_ppo2.py:193][0m |          -0.0197 |           9.0384 |           3.0376 |
[32m[20230203 20:59:24 @agent_ppo2.py:193][0m |          -0.0178 |           8.9620 |           3.0356 |
[32m[20230203 20:59:24 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.47
[32m[20230203 20:59:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.87
[32m[20230203 20:59:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 178.46
[32m[20230203 20:59:24 @agent_ppo2.py:151][0m Total time:       8.81 min
[32m[20230203 20:59:24 @agent_ppo2.py:153][0m 737280 total steps have happened
[32m[20230203 20:59:24 @agent_ppo2.py:129][0m #------------------------ Iteration 360 --------------------------#
[32m[20230203 20:59:25 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 20:59:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:25 @agent_ppo2.py:193][0m |          -0.0046 |          40.3724 |           2.9923 |
[32m[20230203 20:59:25 @agent_ppo2.py:193][0m |          -0.0120 |          31.7254 |           2.9860 |
[32m[20230203 20:59:25 @agent_ppo2.py:193][0m |          -0.0040 |          27.0180 |           2.9745 |
[32m[20230203 20:59:25 @agent_ppo2.py:193][0m |          -0.0141 |          24.8146 |           2.9683 |
[32m[20230203 20:59:25 @agent_ppo2.py:193][0m |          -0.0198 |          23.6889 |           2.9635 |
[32m[20230203 20:59:25 @agent_ppo2.py:193][0m |          -0.0110 |          23.0289 |           2.9621 |
[32m[20230203 20:59:25 @agent_ppo2.py:193][0m |          -0.0171 |          22.0831 |           2.9639 |
[32m[20230203 20:59:25 @agent_ppo2.py:193][0m |          -0.0150 |          21.8937 |           2.9600 |
[32m[20230203 20:59:25 @agent_ppo2.py:193][0m |          -0.0236 |          21.2775 |           2.9589 |
[32m[20230203 20:59:25 @agent_ppo2.py:193][0m |          -0.0203 |          21.0464 |           2.9600 |
[32m[20230203 20:59:25 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 20:59:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 139.27
[32m[20230203 20:59:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.95
[32m[20230203 20:59:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 49.57
[32m[20230203 20:59:25 @agent_ppo2.py:151][0m Total time:       8.84 min
[32m[20230203 20:59:25 @agent_ppo2.py:153][0m 739328 total steps have happened
[32m[20230203 20:59:25 @agent_ppo2.py:129][0m #------------------------ Iteration 361 --------------------------#
[32m[20230203 20:59:26 @agent_ppo2.py:135][0m Sampling time: 0.34 s by 1 slaves
[32m[20230203 20:59:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:26 @agent_ppo2.py:193][0m |           0.0016 |          57.8516 |           3.1216 |
[32m[20230203 20:59:26 @agent_ppo2.py:193][0m |          -0.0058 |          38.1557 |           3.1161 |
[32m[20230203 20:59:26 @agent_ppo2.py:193][0m |          -0.0090 |          32.0816 |           3.1118 |
[32m[20230203 20:59:26 @agent_ppo2.py:193][0m |          -0.0103 |          29.0951 |           3.1069 |
[32m[20230203 20:59:26 @agent_ppo2.py:193][0m |          -0.0120 |          27.2725 |           3.1068 |
[32m[20230203 20:59:26 @agent_ppo2.py:193][0m |          -0.0131 |          25.7358 |           3.1047 |
[32m[20230203 20:59:26 @agent_ppo2.py:193][0m |          -0.0137 |          24.1899 |           3.1029 |
[32m[20230203 20:59:26 @agent_ppo2.py:193][0m |          -0.0149 |          23.0114 |           3.1025 |
[32m[20230203 20:59:26 @agent_ppo2.py:193][0m |          -0.0152 |          22.4285 |           3.1001 |
[32m[20230203 20:59:26 @agent_ppo2.py:193][0m |          -0.0156 |          21.0657 |           3.1003 |
[32m[20230203 20:59:26 @agent_ppo2.py:138][0m Policy update time: 0.40 s
[32m[20230203 20:59:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 159.32
[32m[20230203 20:59:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.87
[32m[20230203 20:59:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.09
[32m[20230203 20:59:26 @agent_ppo2.py:151][0m Total time:       8.85 min
[32m[20230203 20:59:26 @agent_ppo2.py:153][0m 741376 total steps have happened
[32m[20230203 20:59:26 @agent_ppo2.py:129][0m #------------------------ Iteration 362 --------------------------#
[32m[20230203 20:59:27 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:59:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:27 @agent_ppo2.py:193][0m |           0.0011 |          12.3702 |           3.1429 |
[32m[20230203 20:59:27 @agent_ppo2.py:193][0m |          -0.0035 |          11.8844 |           3.1368 |
[32m[20230203 20:59:27 @agent_ppo2.py:193][0m |          -0.0066 |          11.3205 |           3.1338 |
[32m[20230203 20:59:27 @agent_ppo2.py:193][0m |          -0.0086 |          11.0724 |           3.1343 |
[32m[20230203 20:59:27 @agent_ppo2.py:193][0m |          -0.0113 |          10.8760 |           3.1307 |
[32m[20230203 20:59:27 @agent_ppo2.py:193][0m |          -0.0122 |          10.7184 |           3.1301 |
[32m[20230203 20:59:27 @agent_ppo2.py:193][0m |          -0.0113 |          10.6214 |           3.1279 |
[32m[20230203 20:59:27 @agent_ppo2.py:193][0m |          -0.0115 |          10.5112 |           3.1299 |
[32m[20230203 20:59:27 @agent_ppo2.py:193][0m |          -0.0145 |          10.3674 |           3.1273 |
[32m[20230203 20:59:27 @agent_ppo2.py:193][0m |          -0.0152 |          10.2897 |           3.1275 |
[32m[20230203 20:59:27 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:59:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.45
[32m[20230203 20:59:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.57
[32m[20230203 20:59:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.85
[32m[20230203 20:59:28 @agent_ppo2.py:151][0m Total time:       8.88 min
[32m[20230203 20:59:28 @agent_ppo2.py:153][0m 743424 total steps have happened
[32m[20230203 20:59:28 @agent_ppo2.py:129][0m #------------------------ Iteration 363 --------------------------#
[32m[20230203 20:59:28 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:59:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:28 @agent_ppo2.py:193][0m |          -0.0006 |          51.6534 |           3.1225 |
[32m[20230203 20:59:28 @agent_ppo2.py:193][0m |          -0.0082 |          28.9435 |           3.1132 |
[32m[20230203 20:59:28 @agent_ppo2.py:193][0m |          -0.0096 |          20.7616 |           3.1112 |
[32m[20230203 20:59:28 @agent_ppo2.py:193][0m |          -0.0087 |          18.8716 |           3.1097 |
[32m[20230203 20:59:28 @agent_ppo2.py:193][0m |          -0.0098 |          17.5090 |           3.1109 |
[32m[20230203 20:59:28 @agent_ppo2.py:193][0m |          -0.0120 |          16.2212 |           3.1097 |
[32m[20230203 20:59:28 @agent_ppo2.py:193][0m |          -0.0119 |          15.5435 |           3.1077 |
[32m[20230203 20:59:29 @agent_ppo2.py:193][0m |          -0.0129 |          15.2268 |           3.1114 |
[32m[20230203 20:59:29 @agent_ppo2.py:193][0m |          -0.0166 |          14.6802 |           3.1088 |
[32m[20230203 20:59:29 @agent_ppo2.py:193][0m |          -0.0164 |          13.9039 |           3.1104 |
[32m[20230203 20:59:29 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:59:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.12
[32m[20230203 20:59:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.01
[32m[20230203 20:59:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.79
[32m[20230203 20:59:29 @agent_ppo2.py:151][0m Total time:       8.90 min
[32m[20230203 20:59:29 @agent_ppo2.py:153][0m 745472 total steps have happened
[32m[20230203 20:59:29 @agent_ppo2.py:129][0m #------------------------ Iteration 364 --------------------------#
[32m[20230203 20:59:29 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:29 @agent_ppo2.py:193][0m |           0.0036 |          18.1113 |           3.1110 |
[32m[20230203 20:59:30 @agent_ppo2.py:193][0m |          -0.0055 |          13.2701 |           3.1091 |
[32m[20230203 20:59:30 @agent_ppo2.py:193][0m |          -0.0068 |          12.7355 |           3.1029 |
[32m[20230203 20:59:30 @agent_ppo2.py:193][0m |          -0.0078 |          12.3857 |           3.0977 |
[32m[20230203 20:59:30 @agent_ppo2.py:193][0m |          -0.0091 |          12.1976 |           3.0991 |
[32m[20230203 20:59:30 @agent_ppo2.py:193][0m |          -0.0101 |          12.0262 |           3.0970 |
[32m[20230203 20:59:30 @agent_ppo2.py:193][0m |          -0.0115 |          11.9491 |           3.0952 |
[32m[20230203 20:59:30 @agent_ppo2.py:193][0m |          -0.0116 |          11.8975 |           3.0932 |
[32m[20230203 20:59:30 @agent_ppo2.py:193][0m |          -0.0113 |          11.7939 |           3.0950 |
[32m[20230203 20:59:30 @agent_ppo2.py:193][0m |          -0.0118 |          11.7592 |           3.0928 |
[32m[20230203 20:59:30 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.91
[32m[20230203 20:59:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.79
[32m[20230203 20:59:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.58
[32m[20230203 20:59:30 @agent_ppo2.py:151][0m Total time:       8.92 min
[32m[20230203 20:59:30 @agent_ppo2.py:153][0m 747520 total steps have happened
[32m[20230203 20:59:30 @agent_ppo2.py:129][0m #------------------------ Iteration 365 --------------------------#
[32m[20230203 20:59:31 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:31 @agent_ppo2.py:193][0m |          -0.0017 |          12.4977 |           3.0546 |
[32m[20230203 20:59:31 @agent_ppo2.py:193][0m |          -0.0062 |          11.0946 |           3.0549 |
[32m[20230203 20:59:31 @agent_ppo2.py:193][0m |          -0.0143 |          10.7231 |           3.0518 |
[32m[20230203 20:59:31 @agent_ppo2.py:193][0m |           0.0117 |          12.9349 |           3.0488 |
[32m[20230203 20:59:31 @agent_ppo2.py:193][0m |          -0.0152 |          10.2889 |           3.0459 |
[32m[20230203 20:59:31 @agent_ppo2.py:193][0m |          -0.0171 |          10.1072 |           3.0481 |
[32m[20230203 20:59:31 @agent_ppo2.py:193][0m |          -0.0070 |          10.4205 |           3.0448 |
[32m[20230203 20:59:31 @agent_ppo2.py:193][0m |          -0.0158 |           9.9336 |           3.0430 |
[32m[20230203 20:59:31 @agent_ppo2.py:193][0m |           0.0027 |          11.2385 |           3.0422 |
[32m[20230203 20:59:31 @agent_ppo2.py:193][0m |          -0.0187 |           9.8318 |           3.0419 |
[32m[20230203 20:59:31 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:59:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.68
[32m[20230203 20:59:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.36
[32m[20230203 20:59:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.33
[32m[20230203 20:59:31 @agent_ppo2.py:151][0m Total time:       8.94 min
[32m[20230203 20:59:31 @agent_ppo2.py:153][0m 749568 total steps have happened
[32m[20230203 20:59:31 @agent_ppo2.py:129][0m #------------------------ Iteration 366 --------------------------#
[32m[20230203 20:59:32 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:59:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:32 @agent_ppo2.py:193][0m |          -0.0010 |          36.1405 |           3.1806 |
[32m[20230203 20:59:32 @agent_ppo2.py:193][0m |          -0.0070 |          22.5322 |           3.1745 |
[32m[20230203 20:59:32 @agent_ppo2.py:193][0m |          -0.0087 |          17.8205 |           3.1757 |
[32m[20230203 20:59:32 @agent_ppo2.py:193][0m |          -0.0104 |          16.0949 |           3.1737 |
[32m[20230203 20:59:32 @agent_ppo2.py:193][0m |          -0.0122 |          15.1408 |           3.1716 |
[32m[20230203 20:59:32 @agent_ppo2.py:193][0m |          -0.0131 |          14.7325 |           3.1714 |
[32m[20230203 20:59:32 @agent_ppo2.py:193][0m |          -0.0130 |          14.3736 |           3.1704 |
[32m[20230203 20:59:32 @agent_ppo2.py:193][0m |          -0.0138 |          13.9535 |           3.1705 |
[32m[20230203 20:59:32 @agent_ppo2.py:193][0m |          -0.0147 |          13.8807 |           3.1687 |
[32m[20230203 20:59:32 @agent_ppo2.py:193][0m |          -0.0130 |          13.7144 |           3.1660 |
[32m[20230203 20:59:32 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 20:59:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 149.96
[32m[20230203 20:59:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.87
[32m[20230203 20:59:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.83
[32m[20230203 20:59:33 @agent_ppo2.py:151][0m Total time:       8.96 min
[32m[20230203 20:59:33 @agent_ppo2.py:153][0m 751616 total steps have happened
[32m[20230203 20:59:33 @agent_ppo2.py:129][0m #------------------------ Iteration 367 --------------------------#
[32m[20230203 20:59:33 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:59:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:33 @agent_ppo2.py:193][0m |           0.0001 |          11.8168 |           3.1199 |
[32m[20230203 20:59:33 @agent_ppo2.py:193][0m |          -0.0055 |          11.2155 |           3.1212 |
[32m[20230203 20:59:33 @agent_ppo2.py:193][0m |          -0.0088 |          10.8611 |           3.1172 |
[32m[20230203 20:59:33 @agent_ppo2.py:193][0m |          -0.0107 |          10.6537 |           3.1149 |
[32m[20230203 20:59:33 @agent_ppo2.py:193][0m |          -0.0123 |          10.4477 |           3.1133 |
[32m[20230203 20:59:34 @agent_ppo2.py:193][0m |          -0.0120 |          10.4162 |           3.1118 |
[32m[20230203 20:59:34 @agent_ppo2.py:193][0m |          -0.0123 |          10.3346 |           3.1144 |
[32m[20230203 20:59:34 @agent_ppo2.py:193][0m |          -0.0109 |          10.2571 |           3.1105 |
[32m[20230203 20:59:34 @agent_ppo2.py:193][0m |          -0.0142 |          10.0111 |           3.1073 |
[32m[20230203 20:59:34 @agent_ppo2.py:193][0m |          -0.0123 |          10.0933 |           3.1059 |
[32m[20230203 20:59:34 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:59:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.16
[32m[20230203 20:59:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.81
[32m[20230203 20:59:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 250.77
[32m[20230203 20:59:34 @agent_ppo2.py:151][0m Total time:       8.98 min
[32m[20230203 20:59:34 @agent_ppo2.py:153][0m 753664 total steps have happened
[32m[20230203 20:59:34 @agent_ppo2.py:129][0m #------------------------ Iteration 368 --------------------------#
[32m[20230203 20:59:34 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:35 @agent_ppo2.py:193][0m |           0.0044 |          10.1999 |           3.1025 |
[32m[20230203 20:59:35 @agent_ppo2.py:193][0m |          -0.0150 |           9.3807 |           3.0991 |
[32m[20230203 20:59:35 @agent_ppo2.py:193][0m |          -0.0121 |           8.8452 |           3.0935 |
[32m[20230203 20:59:35 @agent_ppo2.py:193][0m |          -0.0082 |           8.5078 |           3.0903 |
[32m[20230203 20:59:35 @agent_ppo2.py:193][0m |          -0.0120 |           8.1964 |           3.0888 |
[32m[20230203 20:59:35 @agent_ppo2.py:193][0m |          -0.0104 |           7.9620 |           3.0883 |
[32m[20230203 20:59:35 @agent_ppo2.py:193][0m |          -0.0258 |           7.7556 |           3.0882 |
[32m[20230203 20:59:35 @agent_ppo2.py:193][0m |          -0.0144 |           7.5630 |           3.0858 |
[32m[20230203 20:59:35 @agent_ppo2.py:193][0m |          -0.0193 |           7.3587 |           3.0871 |
[32m[20230203 20:59:35 @agent_ppo2.py:193][0m |          -0.0178 |           7.2744 |           3.0839 |
[32m[20230203 20:59:35 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:59:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.23
[32m[20230203 20:59:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.63
[32m[20230203 20:59:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.80
[32m[20230203 20:59:35 @agent_ppo2.py:151][0m Total time:       9.00 min
[32m[20230203 20:59:35 @agent_ppo2.py:153][0m 755712 total steps have happened
[32m[20230203 20:59:35 @agent_ppo2.py:129][0m #------------------------ Iteration 369 --------------------------#
[32m[20230203 20:59:36 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 20:59:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:36 @agent_ppo2.py:193][0m |           0.0003 |          10.4928 |           3.1275 |
[32m[20230203 20:59:36 @agent_ppo2.py:193][0m |          -0.0055 |          10.0916 |           3.1205 |
[32m[20230203 20:59:36 @agent_ppo2.py:193][0m |          -0.0077 |           9.8808 |           3.1127 |
[32m[20230203 20:59:36 @agent_ppo2.py:193][0m |          -0.0100 |           9.6534 |           3.1137 |
[32m[20230203 20:59:36 @agent_ppo2.py:193][0m |          -0.0098 |           9.4835 |           3.1118 |
[32m[20230203 20:59:36 @agent_ppo2.py:193][0m |          -0.0106 |           9.3338 |           3.1097 |
[32m[20230203 20:59:36 @agent_ppo2.py:193][0m |          -0.0111 |           9.2000 |           3.1091 |
[32m[20230203 20:59:36 @agent_ppo2.py:193][0m |          -0.0114 |           9.1044 |           3.1066 |
[32m[20230203 20:59:36 @agent_ppo2.py:193][0m |          -0.0120 |           8.9877 |           3.1096 |
[32m[20230203 20:59:36 @agent_ppo2.py:193][0m |          -0.0124 |           8.8845 |           3.1088 |
[32m[20230203 20:59:36 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:59:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.30
[32m[20230203 20:59:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.97
[32m[20230203 20:59:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.32
[32m[20230203 20:59:37 @agent_ppo2.py:151][0m Total time:       9.02 min
[32m[20230203 20:59:37 @agent_ppo2.py:153][0m 757760 total steps have happened
[32m[20230203 20:59:37 @agent_ppo2.py:129][0m #------------------------ Iteration 370 --------------------------#
[32m[20230203 20:59:37 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:37 @agent_ppo2.py:193][0m |           0.0005 |          10.4543 |           3.0991 |
[32m[20230203 20:59:37 @agent_ppo2.py:193][0m |          -0.0060 |           9.9516 |           3.0955 |
[32m[20230203 20:59:37 @agent_ppo2.py:193][0m |          -0.0075 |           9.7173 |           3.0930 |
[32m[20230203 20:59:37 @agent_ppo2.py:193][0m |          -0.0080 |           9.5154 |           3.0914 |
[32m[20230203 20:59:37 @agent_ppo2.py:193][0m |          -0.0094 |           9.3996 |           3.0889 |
[32m[20230203 20:59:37 @agent_ppo2.py:193][0m |          -0.0101 |           9.2809 |           3.0888 |
[32m[20230203 20:59:37 @agent_ppo2.py:193][0m |          -0.0114 |           9.1438 |           3.0915 |
[32m[20230203 20:59:37 @agent_ppo2.py:193][0m |          -0.0120 |           9.0502 |           3.0917 |
[32m[20230203 20:59:37 @agent_ppo2.py:193][0m |          -0.0053 |           9.3752 |           3.0904 |
[32m[20230203 20:59:37 @agent_ppo2.py:193][0m |          -0.0110 |           8.9091 |           3.0886 |
[32m[20230203 20:59:37 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.67
[32m[20230203 20:59:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.64
[32m[20230203 20:59:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.07
[32m[20230203 20:59:38 @agent_ppo2.py:151][0m Total time:       9.04 min
[32m[20230203 20:59:38 @agent_ppo2.py:153][0m 759808 total steps have happened
[32m[20230203 20:59:38 @agent_ppo2.py:129][0m #------------------------ Iteration 371 --------------------------#
[32m[20230203 20:59:38 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:38 @agent_ppo2.py:193][0m |           0.0038 |          13.0520 |           3.0757 |
[32m[20230203 20:59:38 @agent_ppo2.py:193][0m |           0.0061 |          12.5271 |           3.0698 |
[32m[20230203 20:59:38 @agent_ppo2.py:193][0m |          -0.0079 |          11.4358 |           3.0675 |
[32m[20230203 20:59:38 @agent_ppo2.py:193][0m |          -0.0100 |          11.2308 |           3.0717 |
[32m[20230203 20:59:38 @agent_ppo2.py:193][0m |          -0.0111 |          11.0464 |           3.0685 |
[32m[20230203 20:59:39 @agent_ppo2.py:193][0m |          -0.0078 |          11.0858 |           3.0680 |
[32m[20230203 20:59:39 @agent_ppo2.py:193][0m |          -0.0101 |          10.8659 |           3.0687 |
[32m[20230203 20:59:39 @agent_ppo2.py:193][0m |          -0.0138 |          10.7968 |           3.0672 |
[32m[20230203 20:59:39 @agent_ppo2.py:193][0m |          -0.0117 |          10.7530 |           3.0704 |
[32m[20230203 20:59:39 @agent_ppo2.py:193][0m |          -0.0048 |          11.2773 |           3.0723 |
[32m[20230203 20:59:39 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:59:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.01
[32m[20230203 20:59:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.85
[32m[20230203 20:59:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.85
[32m[20230203 20:59:39 @agent_ppo2.py:151][0m Total time:       9.06 min
[32m[20230203 20:59:39 @agent_ppo2.py:153][0m 761856 total steps have happened
[32m[20230203 20:59:39 @agent_ppo2.py:129][0m #------------------------ Iteration 372 --------------------------#
[32m[20230203 20:59:40 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:40 @agent_ppo2.py:193][0m |           0.0035 |          10.8898 |           3.0557 |
[32m[20230203 20:59:40 @agent_ppo2.py:193][0m |           0.0004 |          10.7872 |           3.0562 |
[32m[20230203 20:59:40 @agent_ppo2.py:193][0m |          -0.0049 |          10.4614 |           3.0543 |
[32m[20230203 20:59:40 @agent_ppo2.py:193][0m |          -0.0096 |          10.3702 |           3.0545 |
[32m[20230203 20:59:40 @agent_ppo2.py:193][0m |          -0.0086 |          10.2617 |           3.0526 |
[32m[20230203 20:59:40 @agent_ppo2.py:193][0m |          -0.0141 |          10.1851 |           3.0488 |
[32m[20230203 20:59:40 @agent_ppo2.py:193][0m |          -0.0141 |          10.1164 |           3.0531 |
[32m[20230203 20:59:40 @agent_ppo2.py:193][0m |           0.0068 |          11.8511 |           3.0498 |
[32m[20230203 20:59:40 @agent_ppo2.py:193][0m |          -0.0187 |          10.0508 |           3.0434 |
[32m[20230203 20:59:40 @agent_ppo2.py:193][0m |          -0.0130 |           9.9557 |           3.0495 |
[32m[20230203 20:59:40 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:59:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.96
[32m[20230203 20:59:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.77
[32m[20230203 20:59:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.16
[32m[20230203 20:59:40 @agent_ppo2.py:151][0m Total time:       9.09 min
[32m[20230203 20:59:40 @agent_ppo2.py:153][0m 763904 total steps have happened
[32m[20230203 20:59:40 @agent_ppo2.py:129][0m #------------------------ Iteration 373 --------------------------#
[32m[20230203 20:59:41 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:59:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:41 @agent_ppo2.py:193][0m |           0.0005 |          22.4430 |           3.0938 |
[32m[20230203 20:59:41 @agent_ppo2.py:193][0m |          -0.0041 |          16.4980 |           3.0938 |
[32m[20230203 20:59:41 @agent_ppo2.py:193][0m |          -0.0090 |          15.1739 |           3.0941 |
[32m[20230203 20:59:41 @agent_ppo2.py:193][0m |          -0.0083 |          14.7228 |           3.0922 |
[32m[20230203 20:59:41 @agent_ppo2.py:193][0m |          -0.0090 |          14.8525 |           3.0907 |
[32m[20230203 20:59:41 @agent_ppo2.py:193][0m |          -0.0120 |          14.0519 |           3.0913 |
[32m[20230203 20:59:41 @agent_ppo2.py:193][0m |          -0.0106 |          13.9226 |           3.0897 |
[32m[20230203 20:59:41 @agent_ppo2.py:193][0m |          -0.0087 |          13.8361 |           3.0900 |
[32m[20230203 20:59:41 @agent_ppo2.py:193][0m |          -0.0120 |          13.7021 |           3.0899 |
[32m[20230203 20:59:42 @agent_ppo2.py:193][0m |          -0.0137 |          13.3328 |           3.0897 |
[32m[20230203 20:59:42 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230203 20:59:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 173.37
[32m[20230203 20:59:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.02
[32m[20230203 20:59:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.90
[32m[20230203 20:59:42 @agent_ppo2.py:151][0m Total time:       9.11 min
[32m[20230203 20:59:42 @agent_ppo2.py:153][0m 765952 total steps have happened
[32m[20230203 20:59:42 @agent_ppo2.py:129][0m #------------------------ Iteration 374 --------------------------#
[32m[20230203 20:59:42 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:59:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:42 @agent_ppo2.py:193][0m |           0.0025 |          11.9914 |           3.0936 |
[32m[20230203 20:59:42 @agent_ppo2.py:193][0m |          -0.0060 |          11.0055 |           3.0935 |
[32m[20230203 20:59:42 @agent_ppo2.py:193][0m |          -0.0068 |          10.8526 |           3.0909 |
[32m[20230203 20:59:42 @agent_ppo2.py:193][0m |          -0.0097 |          10.7349 |           3.0872 |
[32m[20230203 20:59:43 @agent_ppo2.py:193][0m |          -0.0099 |          10.6419 |           3.0822 |
[32m[20230203 20:59:43 @agent_ppo2.py:193][0m |          -0.0092 |          10.6394 |           3.0806 |
[32m[20230203 20:59:43 @agent_ppo2.py:193][0m |          -0.0111 |          10.4650 |           3.0801 |
[32m[20230203 20:59:43 @agent_ppo2.py:193][0m |          -0.0115 |          10.4306 |           3.0772 |
[32m[20230203 20:59:43 @agent_ppo2.py:193][0m |          -0.0114 |          10.3512 |           3.0740 |
[32m[20230203 20:59:43 @agent_ppo2.py:193][0m |          -0.0125 |          10.2549 |           3.0733 |
[32m[20230203 20:59:43 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:59:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.65
[32m[20230203 20:59:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.87
[32m[20230203 20:59:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 249.83
[32m[20230203 20:59:43 @agent_ppo2.py:151][0m Total time:       9.13 min
[32m[20230203 20:59:43 @agent_ppo2.py:153][0m 768000 total steps have happened
[32m[20230203 20:59:43 @agent_ppo2.py:129][0m #------------------------ Iteration 375 --------------------------#
[32m[20230203 20:59:43 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:59:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:44 @agent_ppo2.py:193][0m |           0.0007 |          20.1177 |           3.1022 |
[32m[20230203 20:59:44 @agent_ppo2.py:193][0m |          -0.0051 |          12.7753 |           3.0979 |
[32m[20230203 20:59:44 @agent_ppo2.py:193][0m |          -0.0112 |          11.2073 |           3.1006 |
[32m[20230203 20:59:44 @agent_ppo2.py:193][0m |          -0.0120 |          10.3819 |           3.0984 |
[32m[20230203 20:59:44 @agent_ppo2.py:193][0m |          -0.0115 |          10.1941 |           3.0976 |
[32m[20230203 20:59:44 @agent_ppo2.py:193][0m |          -0.0146 |           9.7341 |           3.0974 |
[32m[20230203 20:59:44 @agent_ppo2.py:193][0m |          -0.0157 |           9.5607 |           3.0968 |
[32m[20230203 20:59:44 @agent_ppo2.py:193][0m |          -0.0148 |           9.5669 |           3.0959 |
[32m[20230203 20:59:44 @agent_ppo2.py:193][0m |          -0.0139 |           9.3020 |           3.0941 |
[32m[20230203 20:59:44 @agent_ppo2.py:193][0m |          -0.0168 |           9.0892 |           3.0926 |
[32m[20230203 20:59:44 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 20:59:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 159.48
[32m[20230203 20:59:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.03
[32m[20230203 20:59:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.58
[32m[20230203 20:59:44 @agent_ppo2.py:151][0m Total time:       9.15 min
[32m[20230203 20:59:44 @agent_ppo2.py:153][0m 770048 total steps have happened
[32m[20230203 20:59:44 @agent_ppo2.py:129][0m #------------------------ Iteration 376 --------------------------#
[32m[20230203 20:59:45 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:45 @agent_ppo2.py:193][0m |          -0.0022 |          14.9235 |           3.0561 |
[32m[20230203 20:59:45 @agent_ppo2.py:193][0m |          -0.0085 |          12.2404 |           3.0473 |
[32m[20230203 20:59:45 @agent_ppo2.py:193][0m |          -0.0059 |          12.7093 |           3.0412 |
[32m[20230203 20:59:45 @agent_ppo2.py:193][0m |          -0.0116 |          11.7424 |           3.0402 |
[32m[20230203 20:59:45 @agent_ppo2.py:193][0m |          -0.0063 |          12.1426 |           3.0383 |
[32m[20230203 20:59:45 @agent_ppo2.py:193][0m |          -0.0162 |          11.4865 |           3.0378 |
[32m[20230203 20:59:45 @agent_ppo2.py:193][0m |          -0.0155 |          11.3603 |           3.0373 |
[32m[20230203 20:59:45 @agent_ppo2.py:193][0m |          -0.0110 |          12.1994 |           3.0365 |
[32m[20230203 20:59:45 @agent_ppo2.py:193][0m |          -0.0114 |          11.3138 |           3.0358 |
[32m[20230203 20:59:45 @agent_ppo2.py:193][0m |          -0.0153 |          11.4658 |           3.0326 |
[32m[20230203 20:59:45 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:59:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.71
[32m[20230203 20:59:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.05
[32m[20230203 20:59:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.81
[32m[20230203 20:59:46 @agent_ppo2.py:151][0m Total time:       9.17 min
[32m[20230203 20:59:46 @agent_ppo2.py:153][0m 772096 total steps have happened
[32m[20230203 20:59:46 @agent_ppo2.py:129][0m #------------------------ Iteration 377 --------------------------#
[32m[20230203 20:59:46 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:59:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:46 @agent_ppo2.py:193][0m |          -0.0004 |          14.4118 |           3.1107 |
[32m[20230203 20:59:46 @agent_ppo2.py:193][0m |          -0.0085 |          12.0817 |           3.1004 |
[32m[20230203 20:59:46 @agent_ppo2.py:193][0m |          -0.0121 |          11.7918 |           3.0972 |
[32m[20230203 20:59:46 @agent_ppo2.py:193][0m |          -0.0123 |          11.6613 |           3.0956 |
[32m[20230203 20:59:46 @agent_ppo2.py:193][0m |          -0.0140 |          11.3701 |           3.0981 |
[32m[20230203 20:59:46 @agent_ppo2.py:193][0m |          -0.0160 |          11.2724 |           3.0947 |
[32m[20230203 20:59:46 @agent_ppo2.py:193][0m |          -0.0152 |          11.1713 |           3.1000 |
[32m[20230203 20:59:46 @agent_ppo2.py:193][0m |          -0.0177 |          11.0845 |           3.0949 |
[32m[20230203 20:59:46 @agent_ppo2.py:193][0m |          -0.0176 |          10.9945 |           3.0985 |
[32m[20230203 20:59:46 @agent_ppo2.py:193][0m |          -0.0184 |          10.9340 |           3.0958 |
[32m[20230203 20:59:46 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 20:59:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.84
[32m[20230203 20:59:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.79
[32m[20230203 20:59:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.59
[32m[20230203 20:59:47 @agent_ppo2.py:151][0m Total time:       9.19 min
[32m[20230203 20:59:47 @agent_ppo2.py:153][0m 774144 total steps have happened
[32m[20230203 20:59:47 @agent_ppo2.py:129][0m #------------------------ Iteration 378 --------------------------#
[32m[20230203 20:59:47 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:47 @agent_ppo2.py:193][0m |           0.0030 |          10.5310 |           3.0836 |
[32m[20230203 20:59:47 @agent_ppo2.py:193][0m |          -0.0064 |           9.7243 |           3.0733 |
[32m[20230203 20:59:47 @agent_ppo2.py:193][0m |          -0.0061 |           9.4544 |           3.0725 |
[32m[20230203 20:59:47 @agent_ppo2.py:193][0m |          -0.0075 |           9.2094 |           3.0706 |
[32m[20230203 20:59:47 @agent_ppo2.py:193][0m |          -0.0108 |           8.9336 |           3.0724 |
[32m[20230203 20:59:47 @agent_ppo2.py:193][0m |          -0.0049 |           9.3482 |           3.0703 |
[32m[20230203 20:59:47 @agent_ppo2.py:193][0m |          -0.0114 |           8.6722 |           3.0719 |
[32m[20230203 20:59:48 @agent_ppo2.py:193][0m |          -0.0115 |           8.5209 |           3.0739 |
[32m[20230203 20:59:48 @agent_ppo2.py:193][0m |          -0.0083 |           8.6425 |           3.0724 |
[32m[20230203 20:59:48 @agent_ppo2.py:193][0m |          -0.0140 |           8.3429 |           3.0726 |
[32m[20230203 20:59:48 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.21
[32m[20230203 20:59:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.33
[32m[20230203 20:59:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.67
[32m[20230203 20:59:48 @agent_ppo2.py:151][0m Total time:       9.21 min
[32m[20230203 20:59:48 @agent_ppo2.py:153][0m 776192 total steps have happened
[32m[20230203 20:59:48 @agent_ppo2.py:129][0m #------------------------ Iteration 379 --------------------------#
[32m[20230203 20:59:48 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:59:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:48 @agent_ppo2.py:193][0m |          -0.0064 |          10.7984 |           3.0549 |
[32m[20230203 20:59:49 @agent_ppo2.py:193][0m |          -0.0075 |          10.4742 |           3.0426 |
[32m[20230203 20:59:49 @agent_ppo2.py:193][0m |          -0.0112 |          10.2715 |           3.0473 |
[32m[20230203 20:59:49 @agent_ppo2.py:193][0m |          -0.0092 |          10.1331 |           3.0483 |
[32m[20230203 20:59:49 @agent_ppo2.py:193][0m |          -0.0137 |          10.0673 |           3.0481 |
[32m[20230203 20:59:49 @agent_ppo2.py:193][0m |          -0.0140 |           9.9632 |           3.0465 |
[32m[20230203 20:59:49 @agent_ppo2.py:193][0m |          -0.0102 |          10.0118 |           3.0497 |
[32m[20230203 20:59:49 @agent_ppo2.py:193][0m |          -0.0160 |           9.8057 |           3.0492 |
[32m[20230203 20:59:49 @agent_ppo2.py:193][0m |          -0.0152 |           9.7680 |           3.0513 |
[32m[20230203 20:59:49 @agent_ppo2.py:193][0m |          -0.0074 |          10.5984 |           3.0508 |
[32m[20230203 20:59:49 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.28
[32m[20230203 20:59:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.45
[32m[20230203 20:59:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.94
[32m[20230203 20:59:49 @agent_ppo2.py:151][0m Total time:       9.23 min
[32m[20230203 20:59:49 @agent_ppo2.py:153][0m 778240 total steps have happened
[32m[20230203 20:59:49 @agent_ppo2.py:129][0m #------------------------ Iteration 380 --------------------------#
[32m[20230203 20:59:50 @agent_ppo2.py:135][0m Sampling time: 0.50 s by 1 slaves
[32m[20230203 20:59:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:50 @agent_ppo2.py:193][0m |          -0.0023 |          16.1639 |           3.0788 |
[32m[20230203 20:59:50 @agent_ppo2.py:193][0m |          -0.0099 |          11.8509 |           3.0713 |
[32m[20230203 20:59:50 @agent_ppo2.py:193][0m |          -0.0103 |          11.0582 |           3.0726 |
[32m[20230203 20:59:50 @agent_ppo2.py:193][0m |          -0.0129 |          10.6319 |           3.0719 |
[32m[20230203 20:59:50 @agent_ppo2.py:193][0m |          -0.0150 |          10.3591 |           3.0710 |
[32m[20230203 20:59:50 @agent_ppo2.py:193][0m |          -0.0160 |          10.1860 |           3.0710 |
[32m[20230203 20:59:50 @agent_ppo2.py:193][0m |          -0.0044 |          11.5997 |           3.0720 |
[32m[20230203 20:59:50 @agent_ppo2.py:193][0m |          -0.0170 |           9.9691 |           3.0687 |
[32m[20230203 20:59:50 @agent_ppo2.py:193][0m |          -0.0159 |          10.1245 |           3.0678 |
[32m[20230203 20:59:50 @agent_ppo2.py:193][0m |          -0.0186 |           9.7572 |           3.0704 |
[32m[20230203 20:59:50 @agent_ppo2.py:138][0m Policy update time: 0.58 s
[32m[20230203 20:59:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 161.65
[32m[20230203 20:59:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.05
[32m[20230203 20:59:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.19
[32m[20230203 20:59:51 @agent_ppo2.py:151][0m Total time:       9.26 min
[32m[20230203 20:59:51 @agent_ppo2.py:153][0m 780288 total steps have happened
[32m[20230203 20:59:51 @agent_ppo2.py:129][0m #------------------------ Iteration 381 --------------------------#
[32m[20230203 20:59:51 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:59:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:51 @agent_ppo2.py:193][0m |          -0.0001 |          11.2665 |           3.0589 |
[32m[20230203 20:59:51 @agent_ppo2.py:193][0m |           0.0063 |          11.8900 |           3.0513 |
[32m[20230203 20:59:51 @agent_ppo2.py:193][0m |           0.0204 |          13.3766 |           3.0515 |
[32m[20230203 20:59:51 @agent_ppo2.py:193][0m |          -0.0140 |          10.6347 |           3.0523 |
[32m[20230203 20:59:51 @agent_ppo2.py:193][0m |          -0.0139 |          10.4630 |           3.0513 |
[32m[20230203 20:59:51 @agent_ppo2.py:193][0m |           0.0008 |          11.1959 |           3.0503 |
[32m[20230203 20:59:51 @agent_ppo2.py:193][0m |          -0.0159 |          10.3309 |           3.0516 |
[32m[20230203 20:59:51 @agent_ppo2.py:193][0m |          -0.0092 |          10.3831 |           3.0517 |
[32m[20230203 20:59:52 @agent_ppo2.py:193][0m |          -0.0123 |          10.1850 |           3.0535 |
[32m[20230203 20:59:52 @agent_ppo2.py:193][0m |          -0.0140 |          10.3191 |           3.0544 |
[32m[20230203 20:59:52 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 20:59:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.69
[32m[20230203 20:59:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.68
[32m[20230203 20:59:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.81
[32m[20230203 20:59:52 @agent_ppo2.py:151][0m Total time:       9.28 min
[32m[20230203 20:59:52 @agent_ppo2.py:153][0m 782336 total steps have happened
[32m[20230203 20:59:52 @agent_ppo2.py:129][0m #------------------------ Iteration 382 --------------------------#
[32m[20230203 20:59:52 @agent_ppo2.py:135][0m Sampling time: 0.54 s by 1 slaves
[32m[20230203 20:59:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:53 @agent_ppo2.py:193][0m |           0.0017 |          33.2457 |           3.1204 |
[32m[20230203 20:59:53 @agent_ppo2.py:193][0m |          -0.0095 |          19.6680 |           3.1211 |
[32m[20230203 20:59:53 @agent_ppo2.py:193][0m |          -0.0120 |          16.4260 |           3.1203 |
[32m[20230203 20:59:53 @agent_ppo2.py:193][0m |          -0.0106 |          14.9284 |           3.1192 |
[32m[20230203 20:59:53 @agent_ppo2.py:193][0m |          -0.0156 |          12.6592 |           3.1184 |
[32m[20230203 20:59:53 @agent_ppo2.py:193][0m |          -0.0010 |          12.1858 |           3.1155 |
[32m[20230203 20:59:53 @agent_ppo2.py:193][0m |          -0.0162 |          11.9495 |           3.1113 |
[32m[20230203 20:59:53 @agent_ppo2.py:193][0m |          -0.0144 |          12.5582 |           3.1091 |
[32m[20230203 20:59:53 @agent_ppo2.py:193][0m |          -0.0178 |          11.0687 |           3.1095 |
[32m[20230203 20:59:53 @agent_ppo2.py:193][0m |          -0.0169 |          11.1154 |           3.1085 |
[32m[20230203 20:59:53 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 20:59:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 173.35
[32m[20230203 20:59:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.98
[32m[20230203 20:59:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.41
[32m[20230203 20:59:53 @agent_ppo2.py:151][0m Total time:       9.30 min
[32m[20230203 20:59:53 @agent_ppo2.py:153][0m 784384 total steps have happened
[32m[20230203 20:59:53 @agent_ppo2.py:129][0m #------------------------ Iteration 383 --------------------------#
[32m[20230203 20:59:54 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 20:59:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:54 @agent_ppo2.py:193][0m |          -0.0089 |          89.1881 |           3.0545 |
[32m[20230203 20:59:54 @agent_ppo2.py:193][0m |          -0.0123 |          63.5356 |           3.0467 |
[32m[20230203 20:59:54 @agent_ppo2.py:193][0m |           0.0145 |          57.9606 |           3.0496 |
[32m[20230203 20:59:54 @agent_ppo2.py:193][0m |           0.0161 |          64.4520 |           3.0232 |
[32m[20230203 20:59:54 @agent_ppo2.py:193][0m |          -0.0096 |          49.8241 |           3.0377 |
[32m[20230203 20:59:54 @agent_ppo2.py:193][0m |          -0.0155 |          47.7062 |           3.0442 |
[32m[20230203 20:59:54 @agent_ppo2.py:193][0m |          -0.0135 |          45.9454 |           3.0483 |
[32m[20230203 20:59:54 @agent_ppo2.py:193][0m |          -0.0168 |          45.1522 |           3.0523 |
[32m[20230203 20:59:54 @agent_ppo2.py:193][0m |          -0.0184 |          43.9618 |           3.0528 |
[32m[20230203 20:59:54 @agent_ppo2.py:193][0m |           0.0087 |          59.9483 |           3.0522 |
[32m[20230203 20:59:54 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 20:59:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 44.02
[32m[20230203 20:59:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 69.06
[32m[20230203 20:59:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.47
[32m[20230203 20:59:55 @agent_ppo2.py:151][0m Total time:       9.32 min
[32m[20230203 20:59:55 @agent_ppo2.py:153][0m 786432 total steps have happened
[32m[20230203 20:59:55 @agent_ppo2.py:129][0m #------------------------ Iteration 384 --------------------------#
[32m[20230203 20:59:55 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 20:59:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:55 @agent_ppo2.py:193][0m |           0.0002 |          39.4600 |           3.1480 |
[32m[20230203 20:59:55 @agent_ppo2.py:193][0m |          -0.0040 |          24.1213 |           3.1503 |
[32m[20230203 20:59:55 @agent_ppo2.py:193][0m |          -0.0043 |          23.1375 |           3.1511 |
[32m[20230203 20:59:55 @agent_ppo2.py:193][0m |          -0.0060 |          22.4257 |           3.1516 |
[32m[20230203 20:59:55 @agent_ppo2.py:193][0m |          -0.0069 |          21.9647 |           3.1511 |
[32m[20230203 20:59:55 @agent_ppo2.py:193][0m |          -0.0087 |          21.6544 |           3.1514 |
[32m[20230203 20:59:55 @agent_ppo2.py:193][0m |          -0.0085 |          21.3914 |           3.1494 |
[32m[20230203 20:59:55 @agent_ppo2.py:193][0m |          -0.0097 |          21.1788 |           3.1533 |
[32m[20230203 20:59:55 @agent_ppo2.py:193][0m |          -0.0090 |          20.9725 |           3.1501 |
[32m[20230203 20:59:55 @agent_ppo2.py:193][0m |          -0.0077 |          20.8274 |           3.1501 |
[32m[20230203 20:59:55 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:59:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.61
[32m[20230203 20:59:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.41
[32m[20230203 20:59:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.47
[32m[20230203 20:59:56 @agent_ppo2.py:151][0m Total time:       9.34 min
[32m[20230203 20:59:56 @agent_ppo2.py:153][0m 788480 total steps have happened
[32m[20230203 20:59:56 @agent_ppo2.py:129][0m #------------------------ Iteration 385 --------------------------#
[32m[20230203 20:59:56 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 20:59:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:56 @agent_ppo2.py:193][0m |          -0.0026 |          25.6368 |           3.0809 |
[32m[20230203 20:59:56 @agent_ppo2.py:193][0m |          -0.0092 |          16.0377 |           3.0808 |
[32m[20230203 20:59:56 @agent_ppo2.py:193][0m |          -0.0096 |          14.9871 |           3.0779 |
[32m[20230203 20:59:56 @agent_ppo2.py:193][0m |          -0.0129 |          14.2845 |           3.0761 |
[32m[20230203 20:59:57 @agent_ppo2.py:193][0m |          -0.0137 |          13.8836 |           3.0738 |
[32m[20230203 20:59:57 @agent_ppo2.py:193][0m |          -0.0168 |          13.5328 |           3.0762 |
[32m[20230203 20:59:57 @agent_ppo2.py:193][0m |          -0.0130 |          13.1306 |           3.0746 |
[32m[20230203 20:59:57 @agent_ppo2.py:193][0m |          -0.0148 |          13.0508 |           3.0745 |
[32m[20230203 20:59:57 @agent_ppo2.py:193][0m |          -0.0153 |          12.6224 |           3.0751 |
[32m[20230203 20:59:57 @agent_ppo2.py:193][0m |          -0.0171 |          12.3401 |           3.0747 |
[32m[20230203 20:59:57 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 20:59:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 126.79
[32m[20230203 20:59:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.53
[32m[20230203 20:59:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.58
[32m[20230203 20:59:57 @agent_ppo2.py:151][0m Total time:       9.36 min
[32m[20230203 20:59:57 @agent_ppo2.py:153][0m 790528 total steps have happened
[32m[20230203 20:59:57 @agent_ppo2.py:129][0m #------------------------ Iteration 386 --------------------------#
[32m[20230203 20:59:57 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 20:59:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:58 @agent_ppo2.py:193][0m |          -0.0039 |          10.8291 |           3.0640 |
[32m[20230203 20:59:58 @agent_ppo2.py:193][0m |          -0.0036 |          10.7762 |           3.0513 |
[32m[20230203 20:59:58 @agent_ppo2.py:193][0m |          -0.0110 |          10.2987 |           3.0485 |
[32m[20230203 20:59:58 @agent_ppo2.py:193][0m |          -0.0152 |          10.1022 |           3.0459 |
[32m[20230203 20:59:58 @agent_ppo2.py:193][0m |          -0.0091 |          10.5330 |           3.0449 |
[32m[20230203 20:59:58 @agent_ppo2.py:193][0m |          -0.0162 |           9.8352 |           3.0442 |
[32m[20230203 20:59:58 @agent_ppo2.py:193][0m |          -0.0143 |           9.8100 |           3.0442 |
[32m[20230203 20:59:58 @agent_ppo2.py:193][0m |          -0.0105 |          10.1693 |           3.0463 |
[32m[20230203 20:59:58 @agent_ppo2.py:193][0m |          -0.0188 |           9.5028 |           3.0405 |
[32m[20230203 20:59:58 @agent_ppo2.py:193][0m |          -0.0133 |           9.5279 |           3.0410 |
[32m[20230203 20:59:58 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 20:59:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.03
[32m[20230203 20:59:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.63
[32m[20230203 20:59:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.87
[32m[20230203 20:59:58 @agent_ppo2.py:151][0m Total time:       9.38 min
[32m[20230203 20:59:58 @agent_ppo2.py:153][0m 792576 total steps have happened
[32m[20230203 20:59:58 @agent_ppo2.py:129][0m #------------------------ Iteration 387 --------------------------#
[32m[20230203 20:59:59 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 20:59:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 20:59:59 @agent_ppo2.py:193][0m |           0.0031 |          27.5389 |           2.9988 |
[32m[20230203 20:59:59 @agent_ppo2.py:193][0m |          -0.0050 |          20.8686 |           2.9969 |
[32m[20230203 20:59:59 @agent_ppo2.py:193][0m |          -0.0018 |          19.1755 |           2.9974 |
[32m[20230203 20:59:59 @agent_ppo2.py:193][0m |          -0.0062 |          17.6241 |           2.9978 |
[32m[20230203 20:59:59 @agent_ppo2.py:193][0m |          -0.0068 |          16.9678 |           2.9975 |
[32m[20230203 20:59:59 @agent_ppo2.py:193][0m |          -0.0074 |          16.4255 |           2.9972 |
[32m[20230203 20:59:59 @agent_ppo2.py:193][0m |          -0.0089 |          16.0472 |           2.9971 |
[32m[20230203 20:59:59 @agent_ppo2.py:193][0m |          -0.0094 |          15.7987 |           2.9949 |
[32m[20230203 20:59:59 @agent_ppo2.py:193][0m |          -0.0111 |          15.3727 |           2.9947 |
[32m[20230203 20:59:59 @agent_ppo2.py:193][0m |          -0.0134 |          14.9441 |           2.9950 |
[32m[20230203 20:59:59 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 21:00:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 143.44
[32m[20230203 21:00:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.94
[32m[20230203 21:00:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.66
[32m[20230203 21:00:00 @agent_ppo2.py:151][0m Total time:       9.41 min
[32m[20230203 21:00:00 @agent_ppo2.py:153][0m 794624 total steps have happened
[32m[20230203 21:00:00 @agent_ppo2.py:129][0m #------------------------ Iteration 388 --------------------------#
[32m[20230203 21:00:00 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 21:00:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:00 @agent_ppo2.py:193][0m |          -0.0017 |          42.3781 |           3.1630 |
[32m[20230203 21:00:00 @agent_ppo2.py:193][0m |          -0.0078 |          34.8938 |           3.1622 |
[32m[20230203 21:00:00 @agent_ppo2.py:193][0m |          -0.0128 |          32.2298 |           3.1600 |
[32m[20230203 21:00:00 @agent_ppo2.py:193][0m |          -0.0108 |          31.1228 |           3.1581 |
[32m[20230203 21:00:00 @agent_ppo2.py:193][0m |          -0.0157 |          29.6131 |           3.1569 |
[32m[20230203 21:00:00 @agent_ppo2.py:193][0m |          -0.0136 |          29.1937 |           3.1536 |
[32m[20230203 21:00:00 @agent_ppo2.py:193][0m |          -0.0112 |          28.8686 |           3.1534 |
[32m[20230203 21:00:01 @agent_ppo2.py:193][0m |          -0.0096 |          27.6486 |           3.1519 |
[32m[20230203 21:00:01 @agent_ppo2.py:193][0m |          -0.0165 |          27.3854 |           3.1478 |
[32m[20230203 21:00:01 @agent_ppo2.py:193][0m |          -0.0143 |          27.5085 |           3.1508 |
[32m[20230203 21:00:01 @agent_ppo2.py:138][0m Policy update time: 0.54 s
[32m[20230203 21:00:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 135.08
[32m[20230203 21:00:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.49
[32m[20230203 21:00:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.84
[32m[20230203 21:00:01 @agent_ppo2.py:151][0m Total time:       9.43 min
[32m[20230203 21:00:01 @agent_ppo2.py:153][0m 796672 total steps have happened
[32m[20230203 21:00:01 @agent_ppo2.py:129][0m #------------------------ Iteration 389 --------------------------#
[32m[20230203 21:00:01 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:00:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:01 @agent_ppo2.py:193][0m |           0.0031 |          13.3927 |           2.9985 |
[32m[20230203 21:00:02 @agent_ppo2.py:193][0m |          -0.0098 |          11.8038 |           2.9948 |
[32m[20230203 21:00:02 @agent_ppo2.py:193][0m |          -0.0085 |          11.4405 |           2.9918 |
[32m[20230203 21:00:02 @agent_ppo2.py:193][0m |          -0.0092 |          11.2320 |           2.9891 |
[32m[20230203 21:00:02 @agent_ppo2.py:193][0m |          -0.0165 |          10.9593 |           2.9879 |
[32m[20230203 21:00:02 @agent_ppo2.py:193][0m |          -0.0088 |          11.4867 |           2.9883 |
[32m[20230203 21:00:02 @agent_ppo2.py:193][0m |          -0.0148 |          10.6748 |           2.9891 |
[32m[20230203 21:00:02 @agent_ppo2.py:193][0m |          -0.0157 |          10.5418 |           2.9869 |
[32m[20230203 21:00:02 @agent_ppo2.py:193][0m |          -0.0121 |          10.8866 |           2.9888 |
[32m[20230203 21:00:02 @agent_ppo2.py:193][0m |          -0.0143 |          10.3925 |           2.9887 |
[32m[20230203 21:00:02 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:00:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.80
[32m[20230203 21:00:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.78
[32m[20230203 21:00:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 173.95
[32m[20230203 21:00:02 @agent_ppo2.py:151][0m Total time:       9.45 min
[32m[20230203 21:00:02 @agent_ppo2.py:153][0m 798720 total steps have happened
[32m[20230203 21:00:02 @agent_ppo2.py:129][0m #------------------------ Iteration 390 --------------------------#
[32m[20230203 21:00:03 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 21:00:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:03 @agent_ppo2.py:193][0m |           0.0054 |          12.3229 |           3.1574 |
[32m[20230203 21:00:03 @agent_ppo2.py:193][0m |          -0.0040 |          11.4066 |           3.1503 |
[32m[20230203 21:00:03 @agent_ppo2.py:193][0m |          -0.0100 |          11.0410 |           3.1493 |
[32m[20230203 21:00:03 @agent_ppo2.py:193][0m |          -0.0064 |          11.0701 |           3.1513 |
[32m[20230203 21:00:03 @agent_ppo2.py:193][0m |          -0.0084 |          10.7095 |           3.1476 |
[32m[20230203 21:00:03 @agent_ppo2.py:193][0m |          -0.0106 |          10.5625 |           3.1482 |
[32m[20230203 21:00:03 @agent_ppo2.py:193][0m |          -0.0149 |          10.4390 |           3.1472 |
[32m[20230203 21:00:03 @agent_ppo2.py:193][0m |          -0.0152 |          10.3883 |           3.1468 |
[32m[20230203 21:00:03 @agent_ppo2.py:193][0m |          -0.0073 |          11.0979 |           3.1468 |
[32m[20230203 21:00:03 @agent_ppo2.py:193][0m |          -0.0098 |          11.0579 |           3.1378 |
[32m[20230203 21:00:03 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 21:00:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 256.91
[32m[20230203 21:00:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.28
[32m[20230203 21:00:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.08
[32m[20230203 21:00:04 @agent_ppo2.py:151][0m Total time:       9.47 min
[32m[20230203 21:00:04 @agent_ppo2.py:153][0m 800768 total steps have happened
[32m[20230203 21:00:04 @agent_ppo2.py:129][0m #------------------------ Iteration 391 --------------------------#
[32m[20230203 21:00:04 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:00:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:04 @agent_ppo2.py:193][0m |          -0.0013 |          10.9639 |           3.1468 |
[32m[20230203 21:00:04 @agent_ppo2.py:193][0m |          -0.0046 |          10.7680 |           3.1399 |
[32m[20230203 21:00:04 @agent_ppo2.py:193][0m |          -0.0072 |          10.6050 |           3.1424 |
[32m[20230203 21:00:04 @agent_ppo2.py:193][0m |          -0.0083 |          10.4634 |           3.1390 |
[32m[20230203 21:00:04 @agent_ppo2.py:193][0m |          -0.0093 |          10.3804 |           3.1370 |
[32m[20230203 21:00:04 @agent_ppo2.py:193][0m |          -0.0098 |          10.2873 |           3.1336 |
[32m[20230203 21:00:04 @agent_ppo2.py:193][0m |          -0.0107 |          10.2808 |           3.1281 |
[32m[20230203 21:00:04 @agent_ppo2.py:193][0m |          -0.0118 |          10.1477 |           3.1259 |
[32m[20230203 21:00:04 @agent_ppo2.py:193][0m |          -0.0115 |          10.1269 |           3.1277 |
[32m[20230203 21:00:04 @agent_ppo2.py:193][0m |          -0.0128 |          10.0234 |           3.1245 |
[32m[20230203 21:00:04 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:00:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.97
[32m[20230203 21:00:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.04
[32m[20230203 21:00:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.27
[32m[20230203 21:00:05 @agent_ppo2.py:151][0m Total time:       9.49 min
[32m[20230203 21:00:05 @agent_ppo2.py:153][0m 802816 total steps have happened
[32m[20230203 21:00:05 @agent_ppo2.py:129][0m #------------------------ Iteration 392 --------------------------#
[32m[20230203 21:00:05 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 21:00:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:05 @agent_ppo2.py:193][0m |          -0.0046 |          13.8870 |           3.1627 |
[32m[20230203 21:00:05 @agent_ppo2.py:193][0m |          -0.0128 |          11.6976 |           3.1572 |
[32m[20230203 21:00:05 @agent_ppo2.py:193][0m |          -0.0132 |          11.1501 |           3.1506 |
[32m[20230203 21:00:05 @agent_ppo2.py:193][0m |          -0.0151 |          10.8073 |           3.1505 |
[32m[20230203 21:00:05 @agent_ppo2.py:193][0m |          -0.0177 |          10.5961 |           3.1511 |
[32m[20230203 21:00:06 @agent_ppo2.py:193][0m |          -0.0182 |          10.4433 |           3.1471 |
[32m[20230203 21:00:06 @agent_ppo2.py:193][0m |          -0.0182 |          10.3035 |           3.1485 |
[32m[20230203 21:00:06 @agent_ppo2.py:193][0m |          -0.0159 |          10.3234 |           3.1470 |
[32m[20230203 21:00:06 @agent_ppo2.py:193][0m |          -0.0181 |          10.0417 |           3.1445 |
[32m[20230203 21:00:06 @agent_ppo2.py:193][0m |          -0.0175 |          10.0477 |           3.1459 |
[32m[20230203 21:00:06 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 21:00:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 244.99
[32m[20230203 21:00:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.24
[32m[20230203 21:00:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.32
[32m[20230203 21:00:06 @agent_ppo2.py:151][0m Total time:       9.51 min
[32m[20230203 21:00:06 @agent_ppo2.py:153][0m 804864 total steps have happened
[32m[20230203 21:00:06 @agent_ppo2.py:129][0m #------------------------ Iteration 393 --------------------------#
[32m[20230203 21:00:06 @agent_ppo2.py:135][0m Sampling time: 0.36 s by 1 slaves
[32m[20230203 21:00:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:06 @agent_ppo2.py:193][0m |           0.0019 |          88.2891 |           3.1298 |
[32m[20230203 21:00:07 @agent_ppo2.py:193][0m |          -0.0021 |          62.1055 |           3.1308 |
[32m[20230203 21:00:07 @agent_ppo2.py:193][0m |          -0.0054 |          57.5523 |           3.1295 |
[32m[20230203 21:00:07 @agent_ppo2.py:193][0m |          -0.0075 |          53.9381 |           3.1290 |
[32m[20230203 21:00:07 @agent_ppo2.py:193][0m |          -0.0097 |          51.7179 |           3.1312 |
[32m[20230203 21:00:07 @agent_ppo2.py:193][0m |          -0.0106 |          49.2429 |           3.1307 |
[32m[20230203 21:00:07 @agent_ppo2.py:193][0m |          -0.0110 |          47.9353 |           3.1312 |
[32m[20230203 21:00:07 @agent_ppo2.py:193][0m |          -0.0116 |          46.4087 |           3.1307 |
[32m[20230203 21:00:07 @agent_ppo2.py:193][0m |          -0.0121 |          45.4209 |           3.1304 |
[32m[20230203 21:00:07 @agent_ppo2.py:193][0m |          -0.0129 |          44.6302 |           3.1286 |
[32m[20230203 21:00:07 @agent_ppo2.py:138][0m Policy update time: 0.41 s
[32m[20230203 21:00:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 13.69
[32m[20230203 21:00:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 240.39
[32m[20230203 21:00:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.88
[32m[20230203 21:00:07 @agent_ppo2.py:151][0m Total time:       9.53 min
[32m[20230203 21:00:07 @agent_ppo2.py:153][0m 806912 total steps have happened
[32m[20230203 21:00:07 @agent_ppo2.py:129][0m #------------------------ Iteration 394 --------------------------#
[32m[20230203 21:00:08 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:00:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:08 @agent_ppo2.py:193][0m |           0.0009 |          12.4121 |           3.1371 |
[32m[20230203 21:00:08 @agent_ppo2.py:193][0m |          -0.0056 |          11.5686 |           3.1286 |
[32m[20230203 21:00:08 @agent_ppo2.py:193][0m |          -0.0093 |          11.1797 |           3.1360 |
[32m[20230203 21:00:08 @agent_ppo2.py:193][0m |          -0.0106 |          10.8827 |           3.1343 |
[32m[20230203 21:00:08 @agent_ppo2.py:193][0m |          -0.0126 |          10.6998 |           3.1364 |
[32m[20230203 21:00:08 @agent_ppo2.py:193][0m |          -0.0126 |          10.5371 |           3.1327 |
[32m[20230203 21:00:08 @agent_ppo2.py:193][0m |          -0.0147 |          10.3881 |           3.1317 |
[32m[20230203 21:00:08 @agent_ppo2.py:193][0m |          -0.0137 |          10.2584 |           3.1302 |
[32m[20230203 21:00:08 @agent_ppo2.py:193][0m |          -0.0145 |          10.1889 |           3.1313 |
[32m[20230203 21:00:08 @agent_ppo2.py:193][0m |          -0.0158 |          10.0187 |           3.1318 |
[32m[20230203 21:00:08 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 21:00:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 254.67
[32m[20230203 21:00:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.41
[32m[20230203 21:00:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.69
[32m[20230203 21:00:08 @agent_ppo2.py:151][0m Total time:       9.55 min
[32m[20230203 21:00:08 @agent_ppo2.py:153][0m 808960 total steps have happened
[32m[20230203 21:00:08 @agent_ppo2.py:129][0m #------------------------ Iteration 395 --------------------------#
[32m[20230203 21:00:09 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 21:00:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:09 @agent_ppo2.py:193][0m |          -0.0014 |          27.0056 |           3.0402 |
[32m[20230203 21:00:09 @agent_ppo2.py:193][0m |          -0.0099 |          16.4746 |           3.0387 |
[32m[20230203 21:00:09 @agent_ppo2.py:193][0m |          -0.0011 |          15.3353 |           3.0344 |
[32m[20230203 21:00:09 @agent_ppo2.py:193][0m |          -0.0132 |          14.3411 |           3.0299 |
[32m[20230203 21:00:09 @agent_ppo2.py:193][0m |          -0.0106 |          14.0835 |           3.0300 |
[32m[20230203 21:00:09 @agent_ppo2.py:193][0m |          -0.0163 |          13.3584 |           3.0310 |
[32m[20230203 21:00:09 @agent_ppo2.py:193][0m |          -0.0142 |          13.1407 |           3.0297 |
[32m[20230203 21:00:09 @agent_ppo2.py:193][0m |          -0.0164 |          12.6653 |           3.0303 |
[32m[20230203 21:00:09 @agent_ppo2.py:193][0m |          -0.0166 |          12.4474 |           3.0276 |
[32m[20230203 21:00:09 @agent_ppo2.py:193][0m |          -0.0166 |          12.1762 |           3.0299 |
[32m[20230203 21:00:09 @agent_ppo2.py:138][0m Policy update time: 0.54 s
[32m[20230203 21:00:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 156.31
[32m[20230203 21:00:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.08
[32m[20230203 21:00:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.31
[32m[20230203 21:00:10 @agent_ppo2.py:151][0m Total time:       9.58 min
[32m[20230203 21:00:10 @agent_ppo2.py:153][0m 811008 total steps have happened
[32m[20230203 21:00:10 @agent_ppo2.py:129][0m #------------------------ Iteration 396 --------------------------#
[32m[20230203 21:00:10 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 21:00:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:10 @agent_ppo2.py:193][0m |           0.0012 |          24.8914 |           3.0993 |
[32m[20230203 21:00:10 @agent_ppo2.py:193][0m |          -0.0021 |          19.2918 |           3.0972 |
[32m[20230203 21:00:10 @agent_ppo2.py:193][0m |          -0.0059 |          17.5975 |           3.0958 |
[32m[20230203 21:00:10 @agent_ppo2.py:193][0m |          -0.0080 |          16.9598 |           3.0929 |
[32m[20230203 21:00:10 @agent_ppo2.py:193][0m |          -0.0092 |          16.3761 |           3.0932 |
[32m[20230203 21:00:10 @agent_ppo2.py:193][0m |          -0.0084 |          15.9200 |           3.0899 |
[32m[20230203 21:00:10 @agent_ppo2.py:193][0m |          -0.0105 |          15.6583 |           3.0926 |
[32m[20230203 21:00:11 @agent_ppo2.py:193][0m |          -0.0113 |          15.4600 |           3.0888 |
[32m[20230203 21:00:11 @agent_ppo2.py:193][0m |          -0.0114 |          15.1414 |           3.0875 |
[32m[20230203 21:00:11 @agent_ppo2.py:193][0m |          -0.0132 |          14.8040 |           3.0864 |
[32m[20230203 21:00:11 @agent_ppo2.py:138][0m Policy update time: 0.44 s
[32m[20230203 21:00:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 174.64
[32m[20230203 21:00:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.94
[32m[20230203 21:00:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.05
[32m[20230203 21:00:11 @agent_ppo2.py:151][0m Total time:       9.60 min
[32m[20230203 21:00:11 @agent_ppo2.py:153][0m 813056 total steps have happened
[32m[20230203 21:00:11 @agent_ppo2.py:129][0m #------------------------ Iteration 397 --------------------------#
[32m[20230203 21:00:11 @agent_ppo2.py:135][0m Sampling time: 0.48 s by 1 slaves
[32m[20230203 21:00:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:12 @agent_ppo2.py:193][0m |           0.0029 |          38.9300 |           3.0389 |
[32m[20230203 21:00:12 @agent_ppo2.py:193][0m |          -0.0099 |          33.6791 |           3.0288 |
[32m[20230203 21:00:12 @agent_ppo2.py:193][0m |          -0.0102 |          30.8301 |           3.0347 |
[32m[20230203 21:00:12 @agent_ppo2.py:193][0m |          -0.0119 |          29.4947 |           3.0353 |
[32m[20230203 21:00:12 @agent_ppo2.py:193][0m |          -0.0145 |          27.7372 |           3.0317 |
[32m[20230203 21:00:12 @agent_ppo2.py:193][0m |          -0.0026 |          29.8467 |           3.0339 |
[32m[20230203 21:00:12 @agent_ppo2.py:193][0m |          -0.0063 |          27.6078 |           3.0229 |
[32m[20230203 21:00:12 @agent_ppo2.py:193][0m |          -0.0054 |          24.7951 |           3.0250 |
[32m[20230203 21:00:12 @agent_ppo2.py:193][0m |          -0.0102 |          23.7865 |           3.0325 |
[32m[20230203 21:00:12 @agent_ppo2.py:193][0m |          -0.0111 |          22.5716 |           3.0304 |
[32m[20230203 21:00:12 @agent_ppo2.py:138][0m Policy update time: 0.55 s
[32m[20230203 21:00:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 157.51
[32m[20230203 21:00:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.19
[32m[20230203 21:00:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.48
[32m[20230203 21:00:12 @agent_ppo2.py:151][0m Total time:       9.62 min
[32m[20230203 21:00:12 @agent_ppo2.py:153][0m 815104 total steps have happened
[32m[20230203 21:00:12 @agent_ppo2.py:129][0m #------------------------ Iteration 398 --------------------------#
[32m[20230203 21:00:13 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 21:00:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:13 @agent_ppo2.py:193][0m |           0.0019 |          17.8205 |           3.0469 |
[32m[20230203 21:00:13 @agent_ppo2.py:193][0m |          -0.0068 |          14.2897 |           3.0425 |
[32m[20230203 21:00:13 @agent_ppo2.py:193][0m |          -0.0114 |          13.4360 |           3.0437 |
[32m[20230203 21:00:13 @agent_ppo2.py:193][0m |          -0.0125 |          12.9631 |           3.0435 |
[32m[20230203 21:00:13 @agent_ppo2.py:193][0m |          -0.0153 |          12.5201 |           3.0470 |
[32m[20230203 21:00:13 @agent_ppo2.py:193][0m |          -0.0159 |          12.2651 |           3.0485 |
[32m[20230203 21:00:13 @agent_ppo2.py:193][0m |          -0.0160 |          11.8998 |           3.0496 |
[32m[20230203 21:00:13 @agent_ppo2.py:193][0m |          -0.0187 |          11.6216 |           3.0503 |
[32m[20230203 21:00:13 @agent_ppo2.py:193][0m |          -0.0185 |          11.5159 |           3.0520 |
[32m[20230203 21:00:13 @agent_ppo2.py:193][0m |          -0.0188 |          11.2258 |           3.0509 |
[32m[20230203 21:00:13 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 21:00:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 123.22
[32m[20230203 21:00:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.71
[32m[20230203 21:00:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.42
[32m[20230203 21:00:14 @agent_ppo2.py:151][0m Total time:       9.64 min
[32m[20230203 21:00:14 @agent_ppo2.py:153][0m 817152 total steps have happened
[32m[20230203 21:00:14 @agent_ppo2.py:129][0m #------------------------ Iteration 399 --------------------------#
[32m[20230203 21:00:14 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:00:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:14 @agent_ppo2.py:193][0m |           0.0023 |          12.3979 |           3.0715 |
[32m[20230203 21:00:14 @agent_ppo2.py:193][0m |          -0.0055 |          11.7886 |           3.0683 |
[32m[20230203 21:00:14 @agent_ppo2.py:193][0m |          -0.0071 |          11.5925 |           3.0681 |
[32m[20230203 21:00:14 @agent_ppo2.py:193][0m |          -0.0110 |          11.2321 |           3.0670 |
[32m[20230203 21:00:14 @agent_ppo2.py:193][0m |          -0.0118 |          11.1469 |           3.0672 |
[32m[20230203 21:00:14 @agent_ppo2.py:193][0m |          -0.0131 |          11.0820 |           3.0684 |
[32m[20230203 21:00:14 @agent_ppo2.py:193][0m |          -0.0123 |          11.0327 |           3.0591 |
[32m[20230203 21:00:14 @agent_ppo2.py:193][0m |          -0.0140 |          10.8938 |           3.0664 |
[32m[20230203 21:00:14 @agent_ppo2.py:193][0m |          -0.0122 |          10.9671 |           3.0637 |
[32m[20230203 21:00:14 @agent_ppo2.py:193][0m |          -0.0145 |          10.8481 |           3.0647 |
[32m[20230203 21:00:14 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:00:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.32
[32m[20230203 21:00:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.55
[32m[20230203 21:00:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.30
[32m[20230203 21:00:15 @agent_ppo2.py:151][0m Total time:       9.66 min
[32m[20230203 21:00:15 @agent_ppo2.py:153][0m 819200 total steps have happened
[32m[20230203 21:00:15 @agent_ppo2.py:129][0m #------------------------ Iteration 400 --------------------------#
[32m[20230203 21:00:15 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 1 slaves
[32m[20230203 21:00:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:15 @agent_ppo2.py:193][0m |           0.0023 |          80.2843 |           3.0740 |
[32m[20230203 21:00:15 @agent_ppo2.py:193][0m |          -0.0109 |          46.5750 |           3.0649 |
[32m[20230203 21:00:15 @agent_ppo2.py:193][0m |          -0.0096 |          37.3260 |           3.0639 |
[32m[20230203 21:00:15 @agent_ppo2.py:193][0m |          -0.0104 |          34.7442 |           3.0581 |
[32m[20230203 21:00:15 @agent_ppo2.py:193][0m |          -0.0174 |          31.0166 |           3.0574 |
[32m[20230203 21:00:15 @agent_ppo2.py:193][0m |          -0.0162 |          30.3735 |           3.0556 |
[32m[20230203 21:00:16 @agent_ppo2.py:193][0m |          -0.0171 |          29.0819 |           3.0555 |
[32m[20230203 21:00:16 @agent_ppo2.py:193][0m |          -0.0212 |          27.9442 |           3.0540 |
[32m[20230203 21:00:16 @agent_ppo2.py:193][0m |          -0.0169 |          27.4017 |           3.0521 |
[32m[20230203 21:00:16 @agent_ppo2.py:193][0m |          -0.0210 |          26.9350 |           3.0512 |
[32m[20230203 21:00:16 @agent_ppo2.py:138][0m Policy update time: 0.41 s
[32m[20230203 21:00:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 71.21
[32m[20230203 21:00:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.32
[32m[20230203 21:00:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.27
[32m[20230203 21:00:16 @agent_ppo2.py:151][0m Total time:       9.68 min
[32m[20230203 21:00:16 @agent_ppo2.py:153][0m 821248 total steps have happened
[32m[20230203 21:00:16 @agent_ppo2.py:129][0m #------------------------ Iteration 401 --------------------------#
[32m[20230203 21:00:16 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:00:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:16 @agent_ppo2.py:193][0m |          -0.0015 |          12.0808 |           2.9887 |
[32m[20230203 21:00:16 @agent_ppo2.py:193][0m |           0.0226 |          13.1633 |           2.9830 |
[32m[20230203 21:00:17 @agent_ppo2.py:193][0m |          -0.0018 |          10.9527 |           2.9771 |
[32m[20230203 21:00:17 @agent_ppo2.py:193][0m |          -0.0103 |          10.5104 |           2.9864 |
[32m[20230203 21:00:17 @agent_ppo2.py:193][0m |           0.0054 |          10.7975 |           2.9816 |
[32m[20230203 21:00:17 @agent_ppo2.py:193][0m |           0.0153 |          12.6447 |           2.9784 |
[32m[20230203 21:00:17 @agent_ppo2.py:193][0m |          -0.0186 |          10.0880 |           2.9765 |
[32m[20230203 21:00:17 @agent_ppo2.py:193][0m |          -0.0151 |           9.9496 |           2.9751 |
[32m[20230203 21:00:17 @agent_ppo2.py:193][0m |          -0.0275 |           9.8943 |           2.9751 |
[32m[20230203 21:00:17 @agent_ppo2.py:193][0m |          -0.0108 |           9.8583 |           2.9732 |
[32m[20230203 21:00:17 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:00:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.31
[32m[20230203 21:00:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.06
[32m[20230203 21:00:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.79
[32m[20230203 21:00:17 @agent_ppo2.py:151][0m Total time:       9.70 min
[32m[20230203 21:00:17 @agent_ppo2.py:153][0m 823296 total steps have happened
[32m[20230203 21:00:17 @agent_ppo2.py:129][0m #------------------------ Iteration 402 --------------------------#
[32m[20230203 21:00:18 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 21:00:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:18 @agent_ppo2.py:193][0m |           0.0004 |          10.1809 |           3.1233 |
[32m[20230203 21:00:18 @agent_ppo2.py:193][0m |          -0.0051 |           9.7969 |           3.1199 |
[32m[20230203 21:00:18 @agent_ppo2.py:193][0m |          -0.0080 |           9.5385 |           3.1129 |
[32m[20230203 21:00:18 @agent_ppo2.py:193][0m |          -0.0098 |           9.3475 |           3.1157 |
[32m[20230203 21:00:18 @agent_ppo2.py:193][0m |          -0.0107 |           9.2049 |           3.1142 |
[32m[20230203 21:00:18 @agent_ppo2.py:193][0m |          -0.0112 |           9.0862 |           3.1097 |
[32m[20230203 21:00:18 @agent_ppo2.py:193][0m |          -0.0122 |           8.9702 |           3.1100 |
[32m[20230203 21:00:18 @agent_ppo2.py:193][0m |          -0.0122 |           8.9563 |           3.1099 |
[32m[20230203 21:00:18 @agent_ppo2.py:193][0m |          -0.0132 |           8.7889 |           3.1067 |
[32m[20230203 21:00:18 @agent_ppo2.py:193][0m |          -0.0148 |           8.6125 |           3.1079 |
[32m[20230203 21:00:18 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:00:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 248.40
[32m[20230203 21:00:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.23
[32m[20230203 21:00:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 163.54
[32m[20230203 21:00:19 @agent_ppo2.py:151][0m Total time:       9.72 min
[32m[20230203 21:00:19 @agent_ppo2.py:153][0m 825344 total steps have happened
[32m[20230203 21:00:19 @agent_ppo2.py:129][0m #------------------------ Iteration 403 --------------------------#
[32m[20230203 21:00:19 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:00:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:19 @agent_ppo2.py:193][0m |          -0.0013 |          12.1063 |           3.1185 |
[32m[20230203 21:00:19 @agent_ppo2.py:193][0m |          -0.0064 |          11.4801 |           3.1115 |
[32m[20230203 21:00:19 @agent_ppo2.py:193][0m |          -0.0096 |          11.1535 |           3.1101 |
[32m[20230203 21:00:19 @agent_ppo2.py:193][0m |          -0.0110 |          10.9531 |           3.1065 |
[32m[20230203 21:00:19 @agent_ppo2.py:193][0m |          -0.0107 |          10.8302 |           3.1084 |
[32m[20230203 21:00:19 @agent_ppo2.py:193][0m |          -0.0123 |          10.6971 |           3.1044 |
[32m[20230203 21:00:19 @agent_ppo2.py:193][0m |          -0.0126 |          10.6171 |           3.1050 |
[32m[20230203 21:00:19 @agent_ppo2.py:193][0m |          -0.0125 |          10.5527 |           3.1053 |
[32m[20230203 21:00:19 @agent_ppo2.py:193][0m |          -0.0130 |          10.5130 |           3.1034 |
[32m[20230203 21:00:19 @agent_ppo2.py:193][0m |          -0.0140 |          10.4313 |           3.1014 |
[32m[20230203 21:00:19 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 21:00:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.22
[32m[20230203 21:00:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.81
[32m[20230203 21:00:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.46
[32m[20230203 21:00:20 @agent_ppo2.py:151][0m Total time:       9.74 min
[32m[20230203 21:00:20 @agent_ppo2.py:153][0m 827392 total steps have happened
[32m[20230203 21:00:20 @agent_ppo2.py:129][0m #------------------------ Iteration 404 --------------------------#
[32m[20230203 21:00:20 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:00:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:20 @agent_ppo2.py:193][0m |          -0.0113 |           9.7490 |           2.9909 |
[32m[20230203 21:00:20 @agent_ppo2.py:193][0m |          -0.0016 |           9.2387 |           2.9910 |
[32m[20230203 21:00:20 @agent_ppo2.py:193][0m |          -0.0088 |           8.8313 |           2.9859 |
[32m[20230203 21:00:20 @agent_ppo2.py:193][0m |          -0.0181 |           8.5550 |           2.9863 |
[32m[20230203 21:00:20 @agent_ppo2.py:193][0m |          -0.0182 |           8.3118 |           2.9854 |
[32m[20230203 21:00:20 @agent_ppo2.py:193][0m |          -0.0217 |           8.0992 |           2.9833 |
[32m[20230203 21:00:20 @agent_ppo2.py:193][0m |           0.0666 |          13.9957 |           2.9848 |
[32m[20230203 21:00:21 @agent_ppo2.py:193][0m |          -0.0067 |           7.9731 |           2.9805 |
[32m[20230203 21:00:21 @agent_ppo2.py:193][0m |          -0.0174 |           7.5202 |           2.9779 |
[32m[20230203 21:00:21 @agent_ppo2.py:193][0m |          -0.0255 |           7.3726 |           2.9770 |
[32m[20230203 21:00:21 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:00:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.77
[32m[20230203 21:00:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.81
[32m[20230203 21:00:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.75
[32m[20230203 21:00:21 @agent_ppo2.py:151][0m Total time:       9.76 min
[32m[20230203 21:00:21 @agent_ppo2.py:153][0m 829440 total steps have happened
[32m[20230203 21:00:21 @agent_ppo2.py:129][0m #------------------------ Iteration 405 --------------------------#
[32m[20230203 21:00:21 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:00:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:21 @agent_ppo2.py:193][0m |          -0.0029 |          11.9300 |           3.0653 |
[32m[20230203 21:00:22 @agent_ppo2.py:193][0m |          -0.0095 |          10.5637 |           3.0590 |
[32m[20230203 21:00:22 @agent_ppo2.py:193][0m |          -0.0129 |           9.9688 |           3.0630 |
[32m[20230203 21:00:22 @agent_ppo2.py:193][0m |          -0.0049 |          10.1950 |           3.0631 |
[32m[20230203 21:00:22 @agent_ppo2.py:193][0m |          -0.0184 |           9.4209 |           3.0593 |
[32m[20230203 21:00:22 @agent_ppo2.py:193][0m |          -0.0164 |           9.1344 |           3.0614 |
[32m[20230203 21:00:22 @agent_ppo2.py:193][0m |          -0.0142 |           8.8675 |           3.0603 |
[32m[20230203 21:00:22 @agent_ppo2.py:193][0m |          -0.0193 |           8.7463 |           3.0607 |
[32m[20230203 21:00:22 @agent_ppo2.py:193][0m |          -0.0202 |           8.4505 |           3.0638 |
[32m[20230203 21:00:22 @agent_ppo2.py:193][0m |          -0.0240 |           8.3256 |           3.0632 |
[32m[20230203 21:00:22 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:00:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.76
[32m[20230203 21:00:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.24
[32m[20230203 21:00:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.84
[32m[20230203 21:00:22 @agent_ppo2.py:151][0m Total time:       9.78 min
[32m[20230203 21:00:22 @agent_ppo2.py:153][0m 831488 total steps have happened
[32m[20230203 21:00:22 @agent_ppo2.py:129][0m #------------------------ Iteration 406 --------------------------#
[32m[20230203 21:00:23 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:00:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:23 @agent_ppo2.py:193][0m |           0.0003 |          14.4481 |           3.1280 |
[32m[20230203 21:00:23 @agent_ppo2.py:193][0m |          -0.0056 |          13.1740 |           3.1186 |
[32m[20230203 21:00:23 @agent_ppo2.py:193][0m |          -0.0075 |          12.9112 |           3.1189 |
[32m[20230203 21:00:23 @agent_ppo2.py:193][0m |          -0.0082 |          12.7541 |           3.1169 |
[32m[20230203 21:00:23 @agent_ppo2.py:193][0m |          -0.0105 |          12.4281 |           3.1177 |
[32m[20230203 21:00:23 @agent_ppo2.py:193][0m |          -0.0115 |          12.2749 |           3.1133 |
[32m[20230203 21:00:23 @agent_ppo2.py:193][0m |          -0.0125 |          12.1757 |           3.1139 |
[32m[20230203 21:00:23 @agent_ppo2.py:193][0m |          -0.0134 |          11.9884 |           3.1153 |
[32m[20230203 21:00:23 @agent_ppo2.py:193][0m |          -0.0132 |          11.9128 |           3.1134 |
[32m[20230203 21:00:23 @agent_ppo2.py:193][0m |          -0.0150 |          11.7652 |           3.1129 |
[32m[20230203 21:00:23 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:00:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.29
[32m[20230203 21:00:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.55
[32m[20230203 21:00:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.59
[32m[20230203 21:00:23 @agent_ppo2.py:151][0m Total time:       9.80 min
[32m[20230203 21:00:23 @agent_ppo2.py:153][0m 833536 total steps have happened
[32m[20230203 21:00:23 @agent_ppo2.py:129][0m #------------------------ Iteration 407 --------------------------#
[32m[20230203 21:00:24 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:00:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:24 @agent_ppo2.py:193][0m |          -0.0032 |          11.7223 |           3.0602 |
[32m[20230203 21:00:24 @agent_ppo2.py:193][0m |           0.0010 |          11.5114 |           3.0553 |
[32m[20230203 21:00:24 @agent_ppo2.py:193][0m |          -0.0084 |          10.5871 |           3.0528 |
[32m[20230203 21:00:24 @agent_ppo2.py:193][0m |          -0.0137 |          10.2592 |           3.0497 |
[32m[20230203 21:00:24 @agent_ppo2.py:193][0m |          -0.0130 |          10.1346 |           3.0503 |
[32m[20230203 21:00:24 @agent_ppo2.py:193][0m |          -0.0135 |          10.0474 |           3.0481 |
[32m[20230203 21:00:24 @agent_ppo2.py:193][0m |          -0.0124 |           9.9372 |           3.0436 |
[32m[20230203 21:00:24 @agent_ppo2.py:193][0m |          -0.0112 |          10.0076 |           3.0456 |
[32m[20230203 21:00:24 @agent_ppo2.py:193][0m |          -0.0185 |           9.7985 |           3.0507 |
[32m[20230203 21:00:24 @agent_ppo2.py:193][0m |          -0.0088 |          10.2942 |           3.0490 |
[32m[20230203 21:00:24 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:00:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.70
[32m[20230203 21:00:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.41
[32m[20230203 21:00:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.12
[32m[20230203 21:00:25 @agent_ppo2.py:151][0m Total time:       9.82 min
[32m[20230203 21:00:25 @agent_ppo2.py:153][0m 835584 total steps have happened
[32m[20230203 21:00:25 @agent_ppo2.py:129][0m #------------------------ Iteration 408 --------------------------#
[32m[20230203 21:00:25 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 21:00:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:25 @agent_ppo2.py:193][0m |           0.0005 |          13.4364 |           3.0583 |
[32m[20230203 21:00:25 @agent_ppo2.py:193][0m |          -0.0033 |          12.2717 |           3.0537 |
[32m[20230203 21:00:25 @agent_ppo2.py:193][0m |          -0.0063 |          11.7971 |           3.0502 |
[32m[20230203 21:00:25 @agent_ppo2.py:193][0m |          -0.0065 |          11.2322 |           3.0496 |
[32m[20230203 21:00:25 @agent_ppo2.py:193][0m |          -0.0110 |          10.5729 |           3.0474 |
[32m[20230203 21:00:25 @agent_ppo2.py:193][0m |          -0.0083 |          10.1979 |           3.0471 |
[32m[20230203 21:00:25 @agent_ppo2.py:193][0m |          -0.0074 |           9.8104 |           3.0466 |
[32m[20230203 21:00:25 @agent_ppo2.py:193][0m |          -0.0113 |           9.4696 |           3.0431 |
[32m[20230203 21:00:25 @agent_ppo2.py:193][0m |          -0.0097 |           9.2663 |           3.0423 |
[32m[20230203 21:00:26 @agent_ppo2.py:193][0m |          -0.0114 |           8.9820 |           3.0444 |
[32m[20230203 21:00:26 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 21:00:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.73
[32m[20230203 21:00:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.66
[32m[20230203 21:00:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.12
[32m[20230203 21:00:26 @agent_ppo2.py:151][0m Total time:       9.84 min
[32m[20230203 21:00:26 @agent_ppo2.py:153][0m 837632 total steps have happened
[32m[20230203 21:00:26 @agent_ppo2.py:129][0m #------------------------ Iteration 409 --------------------------#
[32m[20230203 21:00:26 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:00:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:26 @agent_ppo2.py:193][0m |           0.0011 |          12.1713 |           3.1274 |
[32m[20230203 21:00:26 @agent_ppo2.py:193][0m |          -0.0034 |          11.8479 |           3.1195 |
[32m[20230203 21:00:26 @agent_ppo2.py:193][0m |          -0.0138 |          11.2652 |           3.1159 |
[32m[20230203 21:00:26 @agent_ppo2.py:193][0m |          -0.0082 |          11.4758 |           3.1133 |
[32m[20230203 21:00:27 @agent_ppo2.py:193][0m |          -0.0040 |          11.7465 |           3.1133 |
[32m[20230203 21:00:27 @agent_ppo2.py:193][0m |          -0.0158 |          10.8032 |           3.1126 |
[32m[20230203 21:00:27 @agent_ppo2.py:193][0m |          -0.0145 |          10.6682 |           3.1087 |
[32m[20230203 21:00:27 @agent_ppo2.py:193][0m |          -0.0175 |          10.5621 |           3.1103 |
[32m[20230203 21:00:27 @agent_ppo2.py:193][0m |          -0.0066 |          11.3835 |           3.1098 |
[32m[20230203 21:00:27 @agent_ppo2.py:193][0m |          -0.0196 |          10.3670 |           3.1065 |
[32m[20230203 21:00:27 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:00:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.90
[32m[20230203 21:00:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.99
[32m[20230203 21:00:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 193.83
[32m[20230203 21:00:27 @agent_ppo2.py:151][0m Total time:       9.87 min
[32m[20230203 21:00:27 @agent_ppo2.py:153][0m 839680 total steps have happened
[32m[20230203 21:00:27 @agent_ppo2.py:129][0m #------------------------ Iteration 410 --------------------------#
[32m[20230203 21:00:28 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 21:00:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:28 @agent_ppo2.py:193][0m |           0.0012 |          20.2663 |           3.0805 |
[32m[20230203 21:00:28 @agent_ppo2.py:193][0m |          -0.0071 |          14.7756 |           3.0829 |
[32m[20230203 21:00:28 @agent_ppo2.py:193][0m |          -0.0082 |          13.2934 |           3.0792 |
[32m[20230203 21:00:28 @agent_ppo2.py:193][0m |          -0.0101 |          12.4547 |           3.0764 |
[32m[20230203 21:00:28 @agent_ppo2.py:193][0m |          -0.0145 |          11.6787 |           3.0733 |
[32m[20230203 21:00:28 @agent_ppo2.py:193][0m |          -0.0152 |          11.2590 |           3.0752 |
[32m[20230203 21:00:28 @agent_ppo2.py:193][0m |          -0.0139 |          11.0254 |           3.0739 |
[32m[20230203 21:00:28 @agent_ppo2.py:193][0m |          -0.0160 |          10.9617 |           3.0711 |
[32m[20230203 21:00:28 @agent_ppo2.py:193][0m |          -0.0143 |          10.6072 |           3.0709 |
[32m[20230203 21:00:28 @agent_ppo2.py:193][0m |          -0.0172 |          10.2476 |           3.0693 |
[32m[20230203 21:00:28 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 21:00:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 135.70
[32m[20230203 21:00:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.06
[32m[20230203 21:00:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 193.35
[32m[20230203 21:00:28 @agent_ppo2.py:151][0m Total time:       9.89 min
[32m[20230203 21:00:28 @agent_ppo2.py:153][0m 841728 total steps have happened
[32m[20230203 21:00:28 @agent_ppo2.py:129][0m #------------------------ Iteration 411 --------------------------#
[32m[20230203 21:00:29 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 21:00:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:29 @agent_ppo2.py:193][0m |           0.0247 |          14.8295 |           2.9938 |
[32m[20230203 21:00:29 @agent_ppo2.py:193][0m |           0.0003 |          12.0685 |           2.9696 |
[32m[20230203 21:00:29 @agent_ppo2.py:193][0m |          -0.0086 |          11.8787 |           2.9838 |
[32m[20230203 21:00:29 @agent_ppo2.py:193][0m |          -0.0081 |          11.7990 |           2.9826 |
[32m[20230203 21:00:29 @agent_ppo2.py:193][0m |           0.0092 |          13.4934 |           2.9784 |
[32m[20230203 21:00:29 @agent_ppo2.py:193][0m |          -0.0075 |          11.7118 |           2.9720 |
[32m[20230203 21:00:29 @agent_ppo2.py:193][0m |          -0.0163 |          11.4989 |           2.9763 |
[32m[20230203 21:00:29 @agent_ppo2.py:193][0m |          -0.0091 |          11.4727 |           2.9780 |
[32m[20230203 21:00:29 @agent_ppo2.py:193][0m |          -0.0171 |          11.3777 |           2.9760 |
[32m[20230203 21:00:29 @agent_ppo2.py:193][0m |          -0.0117 |          11.3702 |           2.9743 |
[32m[20230203 21:00:29 @agent_ppo2.py:138][0m Policy update time: 0.44 s
[32m[20230203 21:00:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 256.33
[32m[20230203 21:00:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.40
[32m[20230203 21:00:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.73
[32m[20230203 21:00:30 @agent_ppo2.py:151][0m Total time:       9.91 min
[32m[20230203 21:00:30 @agent_ppo2.py:153][0m 843776 total steps have happened
[32m[20230203 21:00:30 @agent_ppo2.py:129][0m #------------------------ Iteration 412 --------------------------#
[32m[20230203 21:00:30 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 21:00:30 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:30 @agent_ppo2.py:193][0m |          -0.0021 |          12.9541 |           3.0167 |
[32m[20230203 21:00:30 @agent_ppo2.py:193][0m |          -0.0055 |          12.0976 |           3.0151 |
[32m[20230203 21:00:30 @agent_ppo2.py:193][0m |          -0.0033 |          11.7045 |           3.0087 |
[32m[20230203 21:00:30 @agent_ppo2.py:193][0m |          -0.0118 |          11.3339 |           3.0092 |
[32m[20230203 21:00:30 @agent_ppo2.py:193][0m |          -0.0098 |          11.1820 |           3.0114 |
[32m[20230203 21:00:30 @agent_ppo2.py:193][0m |          -0.0096 |          11.0578 |           3.0102 |
[32m[20230203 21:00:30 @agent_ppo2.py:193][0m |          -0.0087 |          10.9298 |           3.0094 |
[32m[20230203 21:00:30 @agent_ppo2.py:193][0m |          -0.0053 |          10.8954 |           3.0090 |
[32m[20230203 21:00:30 @agent_ppo2.py:193][0m |          -0.0138 |          10.7692 |           3.0094 |
[32m[20230203 21:00:31 @agent_ppo2.py:193][0m |          -0.0141 |          10.6616 |           3.0082 |
[32m[20230203 21:00:31 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 21:00:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 254.02
[32m[20230203 21:00:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.85
[32m[20230203 21:00:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.08
[32m[20230203 21:00:31 @agent_ppo2.py:151][0m Total time:       9.93 min
[32m[20230203 21:00:31 @agent_ppo2.py:153][0m 845824 total steps have happened
[32m[20230203 21:00:31 @agent_ppo2.py:129][0m #------------------------ Iteration 413 --------------------------#
[32m[20230203 21:00:31 @agent_ppo2.py:135][0m Sampling time: 0.36 s by 1 slaves
[32m[20230203 21:00:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:31 @agent_ppo2.py:193][0m |          -0.0028 |          44.5032 |           3.0288 |
[32m[20230203 21:00:31 @agent_ppo2.py:193][0m |          -0.0120 |          36.6613 |           3.0147 |
[32m[20230203 21:00:31 @agent_ppo2.py:193][0m |          -0.0136 |          35.1185 |           3.0160 |
[32m[20230203 21:00:31 @agent_ppo2.py:193][0m |          -0.0165 |          33.7890 |           3.0168 |
[32m[20230203 21:00:31 @agent_ppo2.py:193][0m |          -0.0165 |          33.4635 |           3.0155 |
[32m[20230203 21:00:31 @agent_ppo2.py:193][0m |          -0.0162 |          32.4598 |           3.0176 |
[32m[20230203 21:00:32 @agent_ppo2.py:193][0m |          -0.0175 |          31.6981 |           3.0145 |
[32m[20230203 21:00:32 @agent_ppo2.py:193][0m |          -0.0180 |          31.6037 |           3.0163 |
[32m[20230203 21:00:32 @agent_ppo2.py:193][0m |          -0.0180 |          31.0462 |           3.0136 |
[32m[20230203 21:00:32 @agent_ppo2.py:193][0m |          -0.0189 |          30.2056 |           3.0158 |
[32m[20230203 21:00:32 @agent_ppo2.py:138][0m Policy update time: 0.41 s
[32m[20230203 21:00:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 188.25
[32m[20230203 21:00:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.10
[32m[20230203 21:00:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.39
[32m[20230203 21:00:32 @agent_ppo2.py:151][0m Total time:       9.95 min
[32m[20230203 21:00:32 @agent_ppo2.py:153][0m 847872 total steps have happened
[32m[20230203 21:00:32 @agent_ppo2.py:129][0m #------------------------ Iteration 414 --------------------------#
[32m[20230203 21:00:32 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 21:00:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:33 @agent_ppo2.py:193][0m |          -0.0028 |          32.6847 |           3.0978 |
[32m[20230203 21:00:33 @agent_ppo2.py:193][0m |          -0.0101 |          21.4684 |           3.0945 |
[32m[20230203 21:00:33 @agent_ppo2.py:193][0m |          -0.0114 |          18.5813 |           3.0938 |
[32m[20230203 21:00:33 @agent_ppo2.py:193][0m |          -0.0131 |          17.5727 |           3.0911 |
[32m[20230203 21:00:33 @agent_ppo2.py:193][0m |          -0.0161 |          16.8045 |           3.0901 |
[32m[20230203 21:00:33 @agent_ppo2.py:193][0m |          -0.0150 |          16.5347 |           3.0889 |
[32m[20230203 21:00:33 @agent_ppo2.py:193][0m |          -0.0162 |          16.2123 |           3.0896 |
[32m[20230203 21:00:33 @agent_ppo2.py:193][0m |          -0.0171 |          15.9346 |           3.0868 |
[32m[20230203 21:00:33 @agent_ppo2.py:193][0m |          -0.0162 |          15.6861 |           3.0873 |
[32m[20230203 21:00:33 @agent_ppo2.py:193][0m |          -0.0165 |          15.8248 |           3.0864 |
[32m[20230203 21:00:33 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:00:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 132.68
[32m[20230203 21:00:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.14
[32m[20230203 21:00:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.38
[32m[20230203 21:00:33 @agent_ppo2.py:151][0m Total time:       9.97 min
[32m[20230203 21:00:33 @agent_ppo2.py:153][0m 849920 total steps have happened
[32m[20230203 21:00:33 @agent_ppo2.py:129][0m #------------------------ Iteration 415 --------------------------#
[32m[20230203 21:00:34 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:00:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:34 @agent_ppo2.py:193][0m |          -0.0007 |          21.9890 |           3.1009 |
[32m[20230203 21:00:34 @agent_ppo2.py:193][0m |          -0.0106 |          14.4129 |           3.0988 |
[32m[20230203 21:00:34 @agent_ppo2.py:193][0m |          -0.0137 |          13.7281 |           3.0984 |
[32m[20230203 21:00:34 @agent_ppo2.py:193][0m |          -0.0135 |          13.4643 |           3.0973 |
[32m[20230203 21:00:34 @agent_ppo2.py:193][0m |          -0.0148 |          13.2318 |           3.0954 |
[32m[20230203 21:00:34 @agent_ppo2.py:193][0m |          -0.0147 |          13.1606 |           3.0925 |
[32m[20230203 21:00:34 @agent_ppo2.py:193][0m |          -0.0157 |          12.9131 |           3.0922 |
[32m[20230203 21:00:34 @agent_ppo2.py:193][0m |          -0.0166 |          12.8069 |           3.0913 |
[32m[20230203 21:00:34 @agent_ppo2.py:193][0m |          -0.0186 |          12.7155 |           3.0923 |
[32m[20230203 21:00:34 @agent_ppo2.py:193][0m |          -0.0175 |          12.6734 |           3.0913 |
[32m[20230203 21:00:34 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:00:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.64
[32m[20230203 21:00:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.08
[32m[20230203 21:00:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.78
[32m[20230203 21:00:35 @agent_ppo2.py:151][0m Total time:       9.99 min
[32m[20230203 21:00:35 @agent_ppo2.py:153][0m 851968 total steps have happened
[32m[20230203 21:00:35 @agent_ppo2.py:129][0m #------------------------ Iteration 416 --------------------------#
[32m[20230203 21:00:35 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 21:00:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:35 @agent_ppo2.py:193][0m |           0.0104 |          14.9604 |           2.9597 |
[32m[20230203 21:00:35 @agent_ppo2.py:193][0m |          -0.0045 |          13.1038 |           2.9521 |
[32m[20230203 21:00:35 @agent_ppo2.py:193][0m |          -0.0121 |          12.2751 |           2.9507 |
[32m[20230203 21:00:35 @agent_ppo2.py:193][0m |          -0.0046 |          13.1604 |           2.9505 |
[32m[20230203 21:00:35 @agent_ppo2.py:193][0m |          -0.0106 |          11.7896 |           2.9534 |
[32m[20230203 21:00:35 @agent_ppo2.py:193][0m |          -0.0172 |          11.6622 |           2.9523 |
[32m[20230203 21:00:35 @agent_ppo2.py:193][0m |          -0.0133 |          11.5170 |           2.9508 |
[32m[20230203 21:00:35 @agent_ppo2.py:193][0m |          -0.0031 |          12.4321 |           2.9503 |
[32m[20230203 21:00:35 @agent_ppo2.py:193][0m |          -0.0087 |          11.7175 |           2.9486 |
[32m[20230203 21:00:35 @agent_ppo2.py:193][0m |          -0.0124 |          11.2075 |           2.9489 |
[32m[20230203 21:00:35 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 21:00:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 254.12
[32m[20230203 21:00:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.30
[32m[20230203 21:00:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.58
[32m[20230203 21:00:36 @agent_ppo2.py:151][0m Total time:      10.01 min
[32m[20230203 21:00:36 @agent_ppo2.py:153][0m 854016 total steps have happened
[32m[20230203 21:00:36 @agent_ppo2.py:129][0m #------------------------ Iteration 417 --------------------------#
[32m[20230203 21:00:36 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:00:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:36 @agent_ppo2.py:193][0m |          -0.0043 |          57.7985 |           3.0077 |
[32m[20230203 21:00:36 @agent_ppo2.py:193][0m |          -0.0139 |          35.6514 |           3.0035 |
[32m[20230203 21:00:36 @agent_ppo2.py:193][0m |          -0.0154 |          30.9981 |           3.0050 |
[32m[20230203 21:00:36 @agent_ppo2.py:193][0m |          -0.0156 |          26.3300 |           3.0042 |
[32m[20230203 21:00:36 @agent_ppo2.py:193][0m |          -0.0120 |          25.2413 |           2.9977 |
[32m[20230203 21:00:36 @agent_ppo2.py:193][0m |          -0.0181 |          24.5602 |           3.0022 |
[32m[20230203 21:00:36 @agent_ppo2.py:193][0m |          -0.0207 |          22.7130 |           3.0026 |
[32m[20230203 21:00:36 @agent_ppo2.py:193][0m |          -0.0159 |          21.2168 |           3.0012 |
[32m[20230203 21:00:37 @agent_ppo2.py:193][0m |          -0.0221 |          20.4093 |           3.0010 |
[32m[20230203 21:00:37 @agent_ppo2.py:193][0m |          -0.0217 |          20.0997 |           3.0011 |
[32m[20230203 21:00:37 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 21:00:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 85.78
[32m[20230203 21:00:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.21
[32m[20230203 21:00:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.73
[32m[20230203 21:00:37 @agent_ppo2.py:151][0m Total time:      10.03 min
[32m[20230203 21:00:37 @agent_ppo2.py:153][0m 856064 total steps have happened
[32m[20230203 21:00:37 @agent_ppo2.py:129][0m #------------------------ Iteration 418 --------------------------#
[32m[20230203 21:00:37 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:00:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:37 @agent_ppo2.py:193][0m |          -0.0036 |          14.1557 |           2.9834 |
[32m[20230203 21:00:37 @agent_ppo2.py:193][0m |          -0.0041 |          12.5766 |           2.9733 |
[32m[20230203 21:00:38 @agent_ppo2.py:193][0m |          -0.0039 |          11.4468 |           2.9764 |
[32m[20230203 21:00:38 @agent_ppo2.py:193][0m |          -0.0119 |          11.0559 |           2.9745 |
[32m[20230203 21:00:38 @agent_ppo2.py:193][0m |          -0.0110 |          10.8476 |           2.9751 |
[32m[20230203 21:00:38 @agent_ppo2.py:193][0m |          -0.0134 |          10.9187 |           2.9714 |
[32m[20230203 21:00:38 @agent_ppo2.py:193][0m |          -0.0101 |          10.6434 |           2.9713 |
[32m[20230203 21:00:38 @agent_ppo2.py:193][0m |          -0.0137 |          10.5650 |           2.9713 |
[32m[20230203 21:00:38 @agent_ppo2.py:193][0m |          -0.0129 |          10.5138 |           2.9731 |
[32m[20230203 21:00:38 @agent_ppo2.py:193][0m |          -0.0141 |          10.4114 |           2.9737 |
[32m[20230203 21:00:38 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:00:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.07
[32m[20230203 21:00:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.62
[32m[20230203 21:00:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.11
[32m[20230203 21:00:38 @agent_ppo2.py:151][0m Total time:      10.05 min
[32m[20230203 21:00:38 @agent_ppo2.py:153][0m 858112 total steps have happened
[32m[20230203 21:00:38 @agent_ppo2.py:129][0m #------------------------ Iteration 419 --------------------------#
[32m[20230203 21:00:39 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 1 slaves
[32m[20230203 21:00:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:39 @agent_ppo2.py:193][0m |           0.0006 |          58.7710 |           3.0208 |
[32m[20230203 21:00:39 @agent_ppo2.py:193][0m |          -0.0062 |          40.0050 |           3.0126 |
[32m[20230203 21:00:39 @agent_ppo2.py:193][0m |          -0.0101 |          32.6163 |           3.0105 |
[32m[20230203 21:00:39 @agent_ppo2.py:193][0m |          -0.0103 |          29.4051 |           3.0120 |
[32m[20230203 21:00:39 @agent_ppo2.py:193][0m |          -0.0148 |          26.7549 |           3.0112 |
[32m[20230203 21:00:39 @agent_ppo2.py:193][0m |          -0.0163 |          24.7675 |           3.0086 |
[32m[20230203 21:00:39 @agent_ppo2.py:193][0m |          -0.0157 |          23.4846 |           3.0096 |
[32m[20230203 21:00:39 @agent_ppo2.py:193][0m |          -0.0183 |          22.2658 |           3.0071 |
[32m[20230203 21:00:39 @agent_ppo2.py:193][0m |          -0.0155 |          21.3202 |           3.0070 |
[32m[20230203 21:00:39 @agent_ppo2.py:193][0m |          -0.0185 |          20.1775 |           3.0045 |
[32m[20230203 21:00:39 @agent_ppo2.py:138][0m Policy update time: 0.41 s
[32m[20230203 21:00:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 169.51
[32m[20230203 21:00:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.87
[32m[20230203 21:00:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.00
[32m[20230203 21:00:39 @agent_ppo2.py:151][0m Total time:      10.07 min
[32m[20230203 21:00:39 @agent_ppo2.py:153][0m 860160 total steps have happened
[32m[20230203 21:00:39 @agent_ppo2.py:129][0m #------------------------ Iteration 420 --------------------------#
[32m[20230203 21:00:40 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 21:00:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:40 @agent_ppo2.py:193][0m |          -0.0001 |          13.0320 |           3.0508 |
[32m[20230203 21:00:40 @agent_ppo2.py:193][0m |          -0.0074 |          11.7329 |           3.0486 |
[32m[20230203 21:00:40 @agent_ppo2.py:193][0m |          -0.0108 |          11.2557 |           3.0479 |
[32m[20230203 21:00:40 @agent_ppo2.py:193][0m |          -0.0124 |          11.0248 |           3.0478 |
[32m[20230203 21:00:40 @agent_ppo2.py:193][0m |          -0.0110 |          10.8493 |           3.0480 |
[32m[20230203 21:00:40 @agent_ppo2.py:193][0m |          -0.0115 |          10.7352 |           3.0466 |
[32m[20230203 21:00:40 @agent_ppo2.py:193][0m |          -0.0146 |          10.6710 |           3.0484 |
[32m[20230203 21:00:40 @agent_ppo2.py:193][0m |          -0.0076 |          10.8455 |           3.0507 |
[32m[20230203 21:00:40 @agent_ppo2.py:193][0m |          -0.0139 |          10.5014 |           3.0494 |
[32m[20230203 21:00:40 @agent_ppo2.py:193][0m |          -0.0240 |          10.4335 |           3.0492 |
[32m[20230203 21:00:40 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 21:00:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.26
[32m[20230203 21:00:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.39
[32m[20230203 21:00:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.41
[32m[20230203 21:00:41 @agent_ppo2.py:151][0m Total time:      10.09 min
[32m[20230203 21:00:41 @agent_ppo2.py:153][0m 862208 total steps have happened
[32m[20230203 21:00:41 @agent_ppo2.py:129][0m #------------------------ Iteration 421 --------------------------#
[32m[20230203 21:00:41 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:00:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:41 @agent_ppo2.py:193][0m |           0.0078 |          13.7450 |           3.0376 |
[32m[20230203 21:00:41 @agent_ppo2.py:193][0m |          -0.0051 |          12.1562 |           3.0384 |
[32m[20230203 21:00:41 @agent_ppo2.py:193][0m |          -0.0071 |          11.7039 |           3.0335 |
[32m[20230203 21:00:41 @agent_ppo2.py:193][0m |          -0.0080 |          11.5585 |           3.0365 |
[32m[20230203 21:00:41 @agent_ppo2.py:193][0m |          -0.0036 |          11.7971 |           3.0373 |
[32m[20230203 21:00:41 @agent_ppo2.py:193][0m |          -0.0139 |          11.2226 |           3.0364 |
[32m[20230203 21:00:41 @agent_ppo2.py:193][0m |          -0.0078 |          11.4512 |           3.0384 |
[32m[20230203 21:00:41 @agent_ppo2.py:193][0m |          -0.0134 |          11.0456 |           3.0355 |
[32m[20230203 21:00:41 @agent_ppo2.py:193][0m |          -0.0093 |          11.4983 |           3.0391 |
[32m[20230203 21:00:41 @agent_ppo2.py:193][0m |          -0.0136 |          10.8880 |           3.0379 |
[32m[20230203 21:00:41 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:00:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.71
[32m[20230203 21:00:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.59
[32m[20230203 21:00:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.26
[32m[20230203 21:00:42 @agent_ppo2.py:151][0m Total time:      10.11 min
[32m[20230203 21:00:42 @agent_ppo2.py:153][0m 864256 total steps have happened
[32m[20230203 21:00:42 @agent_ppo2.py:129][0m #------------------------ Iteration 422 --------------------------#
[32m[20230203 21:00:42 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:00:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:42 @agent_ppo2.py:193][0m |           0.0046 |          12.4365 |           3.1289 |
[32m[20230203 21:00:42 @agent_ppo2.py:193][0m |          -0.0050 |          11.7959 |           3.1176 |
[32m[20230203 21:00:42 @agent_ppo2.py:193][0m |          -0.0058 |          11.5301 |           3.1089 |
[32m[20230203 21:00:42 @agent_ppo2.py:193][0m |          -0.0049 |          11.6180 |           3.1144 |
[32m[20230203 21:00:42 @agent_ppo2.py:193][0m |          -0.0041 |          11.7237 |           3.1097 |
[32m[20230203 21:00:43 @agent_ppo2.py:193][0m |          -0.0078 |          11.3385 |           3.1039 |
[32m[20230203 21:00:43 @agent_ppo2.py:193][0m |          -0.0104 |          11.1926 |           3.1046 |
[32m[20230203 21:00:43 @agent_ppo2.py:193][0m |          -0.0090 |          11.0977 |           3.1097 |
[32m[20230203 21:00:43 @agent_ppo2.py:193][0m |          -0.0118 |          11.0345 |           3.1014 |
[32m[20230203 21:00:43 @agent_ppo2.py:193][0m |          -0.0118 |          11.0203 |           3.1028 |
[32m[20230203 21:00:43 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:00:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.13
[32m[20230203 21:00:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.52
[32m[20230203 21:00:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.69
[32m[20230203 21:00:43 @agent_ppo2.py:151][0m Total time:      10.13 min
[32m[20230203 21:00:43 @agent_ppo2.py:153][0m 866304 total steps have happened
[32m[20230203 21:00:43 @agent_ppo2.py:129][0m #------------------------ Iteration 423 --------------------------#
[32m[20230203 21:00:43 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:00:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:44 @agent_ppo2.py:193][0m |           0.0020 |          11.9604 |           3.1021 |
[32m[20230203 21:00:44 @agent_ppo2.py:193][0m |          -0.0051 |          11.5457 |           3.1011 |
[32m[20230203 21:00:44 @agent_ppo2.py:193][0m |          -0.0077 |          11.4091 |           3.0972 |
[32m[20230203 21:00:44 @agent_ppo2.py:193][0m |          -0.0081 |          11.4094 |           3.0979 |
[32m[20230203 21:00:44 @agent_ppo2.py:193][0m |          -0.0114 |          11.2841 |           3.0957 |
[32m[20230203 21:00:44 @agent_ppo2.py:193][0m |          -0.0124 |          11.1856 |           3.0914 |
[32m[20230203 21:00:44 @agent_ppo2.py:193][0m |          -0.0118 |          11.2134 |           3.0945 |
[32m[20230203 21:00:44 @agent_ppo2.py:193][0m |          -0.0145 |          11.0234 |           3.0934 |
[32m[20230203 21:00:44 @agent_ppo2.py:193][0m |          -0.0147 |          10.9629 |           3.0931 |
[32m[20230203 21:00:44 @agent_ppo2.py:193][0m |          -0.0146 |          10.9571 |           3.0919 |
[32m[20230203 21:00:44 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:00:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.01
[32m[20230203 21:00:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.97
[32m[20230203 21:00:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.58
[32m[20230203 21:00:44 @agent_ppo2.py:151][0m Total time:      10.15 min
[32m[20230203 21:00:44 @agent_ppo2.py:153][0m 868352 total steps have happened
[32m[20230203 21:00:44 @agent_ppo2.py:129][0m #------------------------ Iteration 424 --------------------------#
[32m[20230203 21:00:45 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 21:00:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:45 @agent_ppo2.py:193][0m |          -0.0021 |          68.6895 |           3.0955 |
[32m[20230203 21:00:45 @agent_ppo2.py:193][0m |          -0.0074 |          30.9533 |           3.0927 |
[32m[20230203 21:00:45 @agent_ppo2.py:193][0m |          -0.0136 |          23.2161 |           3.0883 |
[32m[20230203 21:00:45 @agent_ppo2.py:193][0m |          -0.0148 |          19.8425 |           3.0871 |
[32m[20230203 21:00:45 @agent_ppo2.py:193][0m |          -0.0160 |          18.3918 |           3.0838 |
[32m[20230203 21:00:45 @agent_ppo2.py:193][0m |          -0.0165 |          17.5197 |           3.0814 |
[32m[20230203 21:00:45 @agent_ppo2.py:193][0m |          -0.0175 |          16.6435 |           3.0830 |
[32m[20230203 21:00:45 @agent_ppo2.py:193][0m |          -0.0170 |          15.9954 |           3.0804 |
[32m[20230203 21:00:45 @agent_ppo2.py:193][0m |          -0.0197 |          15.4098 |           3.0815 |
[32m[20230203 21:00:45 @agent_ppo2.py:193][0m |          -0.0196 |          14.8212 |           3.0806 |
[32m[20230203 21:00:45 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 21:00:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 112.74
[32m[20230203 21:00:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.58
[32m[20230203 21:00:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 256.44
[32m[20230203 21:00:46 @agent_ppo2.py:151][0m Total time:      10.17 min
[32m[20230203 21:00:46 @agent_ppo2.py:153][0m 870400 total steps have happened
[32m[20230203 21:00:46 @agent_ppo2.py:129][0m #------------------------ Iteration 425 --------------------------#
[32m[20230203 21:00:46 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 21:00:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:46 @agent_ppo2.py:193][0m |           0.0005 |          44.6164 |           3.1238 |
[32m[20230203 21:00:46 @agent_ppo2.py:193][0m |          -0.0060 |          37.8038 |           3.1214 |
[32m[20230203 21:00:46 @agent_ppo2.py:193][0m |          -0.0112 |          33.4075 |           3.1204 |
[32m[20230203 21:00:46 @agent_ppo2.py:193][0m |          -0.0094 |          31.4654 |           3.1208 |
[32m[20230203 21:00:46 @agent_ppo2.py:193][0m |          -0.0130 |          30.0758 |           3.1216 |
[32m[20230203 21:00:46 @agent_ppo2.py:193][0m |          -0.0136 |          28.6724 |           3.1176 |
[32m[20230203 21:00:46 @agent_ppo2.py:193][0m |          -0.0146 |          27.8436 |           3.1191 |
[32m[20230203 21:00:46 @agent_ppo2.py:193][0m |          -0.0157 |          26.9048 |           3.1154 |
[32m[20230203 21:00:47 @agent_ppo2.py:193][0m |          -0.0150 |          26.7280 |           3.1154 |
[32m[20230203 21:00:47 @agent_ppo2.py:193][0m |          -0.0149 |          25.9933 |           3.1169 |
[32m[20230203 21:00:47 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 21:00:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 150.78
[32m[20230203 21:00:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.52
[32m[20230203 21:00:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.00
[32m[20230203 21:00:47 @agent_ppo2.py:151][0m Total time:      10.19 min
[32m[20230203 21:00:47 @agent_ppo2.py:153][0m 872448 total steps have happened
[32m[20230203 21:00:47 @agent_ppo2.py:129][0m #------------------------ Iteration 426 --------------------------#
[32m[20230203 21:00:47 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:00:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:47 @agent_ppo2.py:193][0m |          -0.0032 |          13.6985 |           2.9916 |
[32m[20230203 21:00:47 @agent_ppo2.py:193][0m |          -0.0076 |          11.4190 |           2.9843 |
[32m[20230203 21:00:48 @agent_ppo2.py:193][0m |          -0.0124 |          11.1454 |           2.9798 |
[32m[20230203 21:00:48 @agent_ppo2.py:193][0m |          -0.0100 |          10.9948 |           2.9799 |
[32m[20230203 21:00:48 @agent_ppo2.py:193][0m |          -0.0114 |          10.7855 |           2.9806 |
[32m[20230203 21:00:48 @agent_ppo2.py:193][0m |          -0.0124 |          10.6762 |           2.9792 |
[32m[20230203 21:00:48 @agent_ppo2.py:193][0m |          -0.0170 |          10.5001 |           2.9787 |
[32m[20230203 21:00:48 @agent_ppo2.py:193][0m |          -0.0011 |          10.9732 |           2.9793 |
[32m[20230203 21:00:48 @agent_ppo2.py:193][0m |          -0.0137 |          10.3681 |           2.9787 |
[32m[20230203 21:00:48 @agent_ppo2.py:193][0m |          -0.0026 |          11.3818 |           2.9770 |
[32m[20230203 21:00:48 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:00:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.63
[32m[20230203 21:00:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.95
[32m[20230203 21:00:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.84
[32m[20230203 21:00:48 @agent_ppo2.py:151][0m Total time:      10.22 min
[32m[20230203 21:00:48 @agent_ppo2.py:153][0m 874496 total steps have happened
[32m[20230203 21:00:48 @agent_ppo2.py:129][0m #------------------------ Iteration 427 --------------------------#
[32m[20230203 21:00:49 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 21:00:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:49 @agent_ppo2.py:193][0m |          -0.0061 |          11.3962 |           3.0089 |
[32m[20230203 21:00:49 @agent_ppo2.py:193][0m |          -0.0131 |          11.0288 |           3.0101 |
[32m[20230203 21:00:49 @agent_ppo2.py:193][0m |          -0.0088 |          10.8003 |           3.0050 |
[32m[20230203 21:00:49 @agent_ppo2.py:193][0m |          -0.0128 |          10.5903 |           3.0020 |
[32m[20230203 21:00:49 @agent_ppo2.py:193][0m |          -0.0135 |          10.4268 |           3.0066 |
[32m[20230203 21:00:49 @agent_ppo2.py:193][0m |          -0.0170 |          10.3507 |           3.0062 |
[32m[20230203 21:00:49 @agent_ppo2.py:193][0m |          -0.0164 |          10.2796 |           3.0050 |
[32m[20230203 21:00:49 @agent_ppo2.py:193][0m |          -0.0190 |          10.1964 |           3.0037 |
[32m[20230203 21:00:49 @agent_ppo2.py:193][0m |           0.0087 |          11.8708 |           3.0038 |
[32m[20230203 21:00:49 @agent_ppo2.py:193][0m |          -0.0158 |          10.1254 |           3.0055 |
[32m[20230203 21:00:49 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 21:00:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.54
[32m[20230203 21:00:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.01
[32m[20230203 21:00:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.06
[32m[20230203 21:00:49 @agent_ppo2.py:151][0m Total time:      10.24 min
[32m[20230203 21:00:49 @agent_ppo2.py:153][0m 876544 total steps have happened
[32m[20230203 21:00:49 @agent_ppo2.py:129][0m #------------------------ Iteration 428 --------------------------#
[32m[20230203 21:00:50 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:00:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:50 @agent_ppo2.py:193][0m |          -0.0001 |          12.8619 |           3.1000 |
[32m[20230203 21:00:50 @agent_ppo2.py:193][0m |          -0.0029 |          12.1234 |           3.0922 |
[32m[20230203 21:00:50 @agent_ppo2.py:193][0m |          -0.0063 |          11.7502 |           3.0887 |
[32m[20230203 21:00:50 @agent_ppo2.py:193][0m |          -0.0089 |          11.4977 |           3.0843 |
[32m[20230203 21:00:50 @agent_ppo2.py:193][0m |          -0.0110 |          11.4144 |           3.0865 |
[32m[20230203 21:00:50 @agent_ppo2.py:193][0m |          -0.0099 |          11.2419 |           3.0819 |
[32m[20230203 21:00:50 @agent_ppo2.py:193][0m |          -0.0119 |          11.1178 |           3.0833 |
[32m[20230203 21:00:50 @agent_ppo2.py:193][0m |          -0.0130 |          11.0134 |           3.0811 |
[32m[20230203 21:00:50 @agent_ppo2.py:193][0m |          -0.0131 |          10.9149 |           3.0826 |
[32m[20230203 21:00:50 @agent_ppo2.py:193][0m |          -0.0123 |          10.8862 |           3.0850 |
[32m[20230203 21:00:50 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:00:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.57
[32m[20230203 21:00:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.56
[32m[20230203 21:00:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 250.09
[32m[20230203 21:00:51 @agent_ppo2.py:151][0m Total time:      10.26 min
[32m[20230203 21:00:51 @agent_ppo2.py:153][0m 878592 total steps have happened
[32m[20230203 21:00:51 @agent_ppo2.py:129][0m #------------------------ Iteration 429 --------------------------#
[32m[20230203 21:00:51 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 21:00:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:51 @agent_ppo2.py:193][0m |          -0.0001 |          45.8149 |           3.1670 |
[32m[20230203 21:00:51 @agent_ppo2.py:193][0m |          -0.0041 |          28.6459 |           3.1632 |
[32m[20230203 21:00:51 @agent_ppo2.py:193][0m |          -0.0060 |          25.0910 |           3.1586 |
[32m[20230203 21:00:51 @agent_ppo2.py:193][0m |          -0.0093 |          23.2913 |           3.1545 |
[32m[20230203 21:00:51 @agent_ppo2.py:193][0m |          -0.0084 |          22.3143 |           3.1564 |
[32m[20230203 21:00:51 @agent_ppo2.py:193][0m |          -0.0096 |          20.8817 |           3.1583 |
[32m[20230203 21:00:51 @agent_ppo2.py:193][0m |          -0.0087 |          20.4051 |           3.1559 |
[32m[20230203 21:00:51 @agent_ppo2.py:193][0m |          -0.0066 |          20.0249 |           3.1555 |
[32m[20230203 21:00:52 @agent_ppo2.py:193][0m |          -0.0105 |          20.0847 |           3.1549 |
[32m[20230203 21:00:52 @agent_ppo2.py:193][0m |          -0.0123 |          19.1270 |           3.1556 |
[32m[20230203 21:00:52 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:00:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 123.87
[32m[20230203 21:00:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.24
[32m[20230203 21:00:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 246.58
[32m[20230203 21:00:52 @agent_ppo2.py:151][0m Total time:      10.28 min
[32m[20230203 21:00:52 @agent_ppo2.py:153][0m 880640 total steps have happened
[32m[20230203 21:00:52 @agent_ppo2.py:129][0m #------------------------ Iteration 430 --------------------------#
[32m[20230203 21:00:52 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:00:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:52 @agent_ppo2.py:193][0m |           0.0010 |          13.5867 |           3.0533 |
[32m[20230203 21:00:52 @agent_ppo2.py:193][0m |          -0.0038 |          12.4776 |           3.0472 |
[32m[20230203 21:00:53 @agent_ppo2.py:193][0m |          -0.0113 |          11.6201 |           3.0483 |
[32m[20230203 21:00:53 @agent_ppo2.py:193][0m |          -0.0118 |          11.2459 |           3.0453 |
[32m[20230203 21:00:53 @agent_ppo2.py:193][0m |          -0.0113 |          11.2558 |           3.0445 |
[32m[20230203 21:00:53 @agent_ppo2.py:193][0m |          -0.0167 |          10.8836 |           3.0418 |
[32m[20230203 21:00:53 @agent_ppo2.py:193][0m |          -0.0127 |          11.2663 |           3.0451 |
[32m[20230203 21:00:53 @agent_ppo2.py:193][0m |          -0.0207 |          10.6420 |           3.0459 |
[32m[20230203 21:00:53 @agent_ppo2.py:193][0m |          -0.0167 |          10.6128 |           3.0454 |
[32m[20230203 21:00:53 @agent_ppo2.py:193][0m |          -0.0205 |          10.4694 |           3.0434 |
[32m[20230203 21:00:53 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:00:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.71
[32m[20230203 21:00:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.72
[32m[20230203 21:00:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.52
[32m[20230203 21:00:53 @agent_ppo2.py:151][0m Total time:      10.30 min
[32m[20230203 21:00:53 @agent_ppo2.py:153][0m 882688 total steps have happened
[32m[20230203 21:00:53 @agent_ppo2.py:129][0m #------------------------ Iteration 431 --------------------------#
[32m[20230203 21:00:54 @agent_ppo2.py:135][0m Sampling time: 0.36 s by 1 slaves
[32m[20230203 21:00:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:54 @agent_ppo2.py:193][0m |           0.0036 |          22.2633 |           3.0119 |
[32m[20230203 21:00:54 @agent_ppo2.py:193][0m |          -0.0028 |          14.5364 |           3.0090 |
[32m[20230203 21:00:54 @agent_ppo2.py:193][0m |          -0.0073 |          13.0720 |           3.0076 |
[32m[20230203 21:00:54 @agent_ppo2.py:193][0m |          -0.0081 |          12.3866 |           3.0071 |
[32m[20230203 21:00:54 @agent_ppo2.py:193][0m |          -0.0077 |          11.7934 |           3.0078 |
[32m[20230203 21:00:54 @agent_ppo2.py:193][0m |          -0.0098 |          11.2887 |           3.0028 |
[32m[20230203 21:00:54 @agent_ppo2.py:193][0m |          -0.0100 |          11.0734 |           3.0008 |
[32m[20230203 21:00:54 @agent_ppo2.py:193][0m |          -0.0126 |          10.5584 |           3.0019 |
[32m[20230203 21:00:54 @agent_ppo2.py:193][0m |          -0.0129 |          10.2525 |           2.9980 |
[32m[20230203 21:00:54 @agent_ppo2.py:193][0m |          -0.0142 |           9.9841 |           2.9962 |
[32m[20230203 21:00:54 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 21:00:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 156.66
[32m[20230203 21:00:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.20
[32m[20230203 21:00:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.23
[32m[20230203 21:00:54 @agent_ppo2.py:151][0m Total time:      10.32 min
[32m[20230203 21:00:54 @agent_ppo2.py:153][0m 884736 total steps have happened
[32m[20230203 21:00:54 @agent_ppo2.py:129][0m #------------------------ Iteration 432 --------------------------#
[32m[20230203 21:00:55 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 21:00:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:55 @agent_ppo2.py:193][0m |          -0.0016 |          40.7268 |           3.0954 |
[32m[20230203 21:00:55 @agent_ppo2.py:193][0m |          -0.0076 |          27.4681 |           3.0882 |
[32m[20230203 21:00:55 @agent_ppo2.py:193][0m |          -0.0119 |          22.3074 |           3.0919 |
[32m[20230203 21:00:55 @agent_ppo2.py:193][0m |          -0.0134 |          20.1497 |           3.0910 |
[32m[20230203 21:00:55 @agent_ppo2.py:193][0m |          -0.0142 |          18.5377 |           3.0937 |
[32m[20230203 21:00:55 @agent_ppo2.py:193][0m |          -0.0143 |          17.4326 |           3.0911 |
[32m[20230203 21:00:55 @agent_ppo2.py:193][0m |          -0.0143 |          16.5850 |           3.0924 |
[32m[20230203 21:00:55 @agent_ppo2.py:193][0m |          -0.0160 |          15.9786 |           3.0915 |
[32m[20230203 21:00:55 @agent_ppo2.py:193][0m |          -0.0174 |          15.2760 |           3.0918 |
[32m[20230203 21:00:55 @agent_ppo2.py:193][0m |          -0.0164 |          15.2060 |           3.0940 |
[32m[20230203 21:00:55 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 21:00:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 242.44
[32m[20230203 21:00:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.28
[32m[20230203 21:00:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 129.64
[32m[20230203 21:00:56 @agent_ppo2.py:151][0m Total time:      10.34 min
[32m[20230203 21:00:56 @agent_ppo2.py:153][0m 886784 total steps have happened
[32m[20230203 21:00:56 @agent_ppo2.py:129][0m #------------------------ Iteration 433 --------------------------#
[32m[20230203 21:00:56 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 21:00:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:56 @agent_ppo2.py:193][0m |          -0.0055 |          12.7600 |           3.1230 |
[32m[20230203 21:00:56 @agent_ppo2.py:193][0m |          -0.0011 |          12.5141 |           3.1151 |
[32m[20230203 21:00:56 @agent_ppo2.py:193][0m |          -0.0150 |          11.3182 |           3.1101 |
[32m[20230203 21:00:56 @agent_ppo2.py:193][0m |          -0.0124 |          11.1050 |           3.1096 |
[32m[20230203 21:00:56 @agent_ppo2.py:193][0m |          -0.0131 |          10.9206 |           3.1080 |
[32m[20230203 21:00:56 @agent_ppo2.py:193][0m |          -0.0123 |          10.8543 |           3.1057 |
[32m[20230203 21:00:56 @agent_ppo2.py:193][0m |          -0.0100 |          11.3228 |           3.1042 |
[32m[20230203 21:00:57 @agent_ppo2.py:193][0m |          -0.0093 |          10.9583 |           3.1012 |
[32m[20230203 21:00:57 @agent_ppo2.py:193][0m |          -0.0138 |          10.6294 |           3.1002 |
[32m[20230203 21:00:57 @agent_ppo2.py:193][0m |          -0.0141 |          10.5662 |           3.1014 |
[32m[20230203 21:00:57 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:00:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 248.22
[32m[20230203 21:00:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.10
[32m[20230203 21:00:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.34
[32m[20230203 21:00:57 @agent_ppo2.py:151][0m Total time:      10.36 min
[32m[20230203 21:00:57 @agent_ppo2.py:153][0m 888832 total steps have happened
[32m[20230203 21:00:57 @agent_ppo2.py:129][0m #------------------------ Iteration 434 --------------------------#
[32m[20230203 21:00:57 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:00:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:57 @agent_ppo2.py:193][0m |           0.0003 |          16.3977 |           3.0515 |
[32m[20230203 21:00:58 @agent_ppo2.py:193][0m |          -0.0043 |          14.7224 |           3.0480 |
[32m[20230203 21:00:58 @agent_ppo2.py:193][0m |          -0.0103 |          13.9012 |           3.0450 |
[32m[20230203 21:00:58 @agent_ppo2.py:193][0m |          -0.0099 |          13.4756 |           3.0449 |
[32m[20230203 21:00:58 @agent_ppo2.py:193][0m |          -0.0115 |          13.3681 |           3.0428 |
[32m[20230203 21:00:58 @agent_ppo2.py:193][0m |          -0.0144 |          12.9571 |           3.0437 |
[32m[20230203 21:00:58 @agent_ppo2.py:193][0m |          -0.0133 |          12.8111 |           3.0419 |
[32m[20230203 21:00:58 @agent_ppo2.py:193][0m |          -0.0128 |          12.7482 |           3.0406 |
[32m[20230203 21:00:58 @agent_ppo2.py:193][0m |          -0.0135 |          12.4384 |           3.0407 |
[32m[20230203 21:00:58 @agent_ppo2.py:193][0m |          -0.0144 |          12.3032 |           3.0385 |
[32m[20230203 21:00:58 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:00:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.93
[32m[20230203 21:00:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.03
[32m[20230203 21:00:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.11
[32m[20230203 21:00:58 @agent_ppo2.py:151][0m Total time:      10.38 min
[32m[20230203 21:00:58 @agent_ppo2.py:153][0m 890880 total steps have happened
[32m[20230203 21:00:58 @agent_ppo2.py:129][0m #------------------------ Iteration 435 --------------------------#
[32m[20230203 21:00:59 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 21:00:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:00:59 @agent_ppo2.py:193][0m |           0.0006 |          26.4720 |           3.1286 |
[32m[20230203 21:00:59 @agent_ppo2.py:193][0m |          -0.0085 |          20.3115 |           3.1190 |
[32m[20230203 21:00:59 @agent_ppo2.py:193][0m |          -0.0102 |          18.1339 |           3.1172 |
[32m[20230203 21:00:59 @agent_ppo2.py:193][0m |          -0.0124 |          17.6464 |           3.1147 |
[32m[20230203 21:00:59 @agent_ppo2.py:193][0m |          -0.0160 |          17.3769 |           3.1109 |
[32m[20230203 21:00:59 @agent_ppo2.py:193][0m |          -0.0176 |          17.1378 |           3.1098 |
[32m[20230203 21:00:59 @agent_ppo2.py:193][0m |          -0.0134 |          16.8412 |           3.1074 |
[32m[20230203 21:00:59 @agent_ppo2.py:193][0m |          -0.0077 |          17.4125 |           3.1069 |
[32m[20230203 21:00:59 @agent_ppo2.py:193][0m |          -0.0151 |          16.9775 |           3.1051 |
[32m[20230203 21:00:59 @agent_ppo2.py:193][0m |          -0.0182 |          16.4480 |           3.1048 |
[32m[20230203 21:00:59 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 21:01:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 163.05
[32m[20230203 21:01:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.94
[32m[20230203 21:01:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 250.96
[32m[20230203 21:01:00 @agent_ppo2.py:151][0m Total time:      10.41 min
[32m[20230203 21:01:00 @agent_ppo2.py:153][0m 892928 total steps have happened
[32m[20230203 21:01:00 @agent_ppo2.py:129][0m #------------------------ Iteration 436 --------------------------#
[32m[20230203 21:01:00 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:01:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:00 @agent_ppo2.py:193][0m |          -0.0006 |          12.7440 |           3.0940 |
[32m[20230203 21:01:00 @agent_ppo2.py:193][0m |          -0.0083 |          12.2339 |           3.0867 |
[32m[20230203 21:01:00 @agent_ppo2.py:193][0m |          -0.0109 |          11.8200 |           3.0857 |
[32m[20230203 21:01:00 @agent_ppo2.py:193][0m |          -0.0112 |          11.5219 |           3.0868 |
[32m[20230203 21:01:00 @agent_ppo2.py:193][0m |          -0.0125 |          11.2163 |           3.0846 |
[32m[20230203 21:01:00 @agent_ppo2.py:193][0m |          -0.0147 |          11.0128 |           3.0836 |
[32m[20230203 21:01:00 @agent_ppo2.py:193][0m |          -0.0148 |          10.8598 |           3.0844 |
[32m[20230203 21:01:00 @agent_ppo2.py:193][0m |          -0.0152 |          10.7328 |           3.0837 |
[32m[20230203 21:01:00 @agent_ppo2.py:193][0m |          -0.0159 |          10.5792 |           3.0859 |
[32m[20230203 21:01:00 @agent_ppo2.py:193][0m |          -0.0165 |          10.4037 |           3.0838 |
[32m[20230203 21:01:00 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 21:01:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.26
[32m[20230203 21:01:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.90
[32m[20230203 21:01:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 250.84
[32m[20230203 21:01:01 @agent_ppo2.py:151][0m Total time:      10.43 min
[32m[20230203 21:01:01 @agent_ppo2.py:153][0m 894976 total steps have happened
[32m[20230203 21:01:01 @agent_ppo2.py:129][0m #------------------------ Iteration 437 --------------------------#
[32m[20230203 21:01:01 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:01:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:01 @agent_ppo2.py:193][0m |           0.0001 |          11.7597 |           3.0732 |
[32m[20230203 21:01:01 @agent_ppo2.py:193][0m |          -0.0073 |          11.2590 |           3.0706 |
[32m[20230203 21:01:01 @agent_ppo2.py:193][0m |          -0.0078 |          11.1330 |           3.0652 |
[32m[20230203 21:01:01 @agent_ppo2.py:193][0m |          -0.0076 |          11.0997 |           3.0636 |
[32m[20230203 21:01:01 @agent_ppo2.py:193][0m |          -0.0123 |          10.8069 |           3.0641 |
[32m[20230203 21:01:01 @agent_ppo2.py:193][0m |          -0.0116 |          10.7949 |           3.0587 |
[32m[20230203 21:01:02 @agent_ppo2.py:193][0m |          -0.0136 |          10.7167 |           3.0628 |
[32m[20230203 21:01:02 @agent_ppo2.py:193][0m |          -0.0136 |          10.7441 |           3.0646 |
[32m[20230203 21:01:02 @agent_ppo2.py:193][0m |          -0.0144 |          10.5495 |           3.0623 |
[32m[20230203 21:01:02 @agent_ppo2.py:193][0m |          -0.0158 |          10.4872 |           3.0610 |
[32m[20230203 21:01:02 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:01:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.71
[32m[20230203 21:01:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.62
[32m[20230203 21:01:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.04
[32m[20230203 21:01:02 @agent_ppo2.py:151][0m Total time:      10.45 min
[32m[20230203 21:01:02 @agent_ppo2.py:153][0m 897024 total steps have happened
[32m[20230203 21:01:02 @agent_ppo2.py:129][0m #------------------------ Iteration 438 --------------------------#
[32m[20230203 21:01:02 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:01:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:03 @agent_ppo2.py:193][0m |           0.0006 |          12.1553 |           3.1430 |
[32m[20230203 21:01:03 @agent_ppo2.py:193][0m |          -0.0053 |          11.8446 |           3.1414 |
[32m[20230203 21:01:03 @agent_ppo2.py:193][0m |          -0.0081 |          11.6184 |           3.1430 |
[32m[20230203 21:01:03 @agent_ppo2.py:193][0m |          -0.0085 |          11.5384 |           3.1401 |
[32m[20230203 21:01:03 @agent_ppo2.py:193][0m |          -0.0094 |          11.5225 |           3.1391 |
[32m[20230203 21:01:03 @agent_ppo2.py:193][0m |          -0.0097 |          11.4275 |           3.1376 |
[32m[20230203 21:01:03 @agent_ppo2.py:193][0m |          -0.0120 |          11.2864 |           3.1376 |
[32m[20230203 21:01:03 @agent_ppo2.py:193][0m |          -0.0123 |          11.2025 |           3.1364 |
[32m[20230203 21:01:03 @agent_ppo2.py:193][0m |          -0.0132 |          11.1324 |           3.1359 |
[32m[20230203 21:01:03 @agent_ppo2.py:193][0m |          -0.0138 |          11.0693 |           3.1338 |
[32m[20230203 21:01:03 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:01:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.43
[32m[20230203 21:01:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.77
[32m[20230203 21:01:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.24
[32m[20230203 21:01:03 @agent_ppo2.py:151][0m Total time:      10.47 min
[32m[20230203 21:01:03 @agent_ppo2.py:153][0m 899072 total steps have happened
[32m[20230203 21:01:03 @agent_ppo2.py:129][0m #------------------------ Iteration 439 --------------------------#
[32m[20230203 21:01:04 @agent_ppo2.py:135][0m Sampling time: 0.51 s by 1 slaves
[32m[20230203 21:01:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:04 @agent_ppo2.py:193][0m |           0.0011 |          40.5016 |           3.0996 |
[32m[20230203 21:01:04 @agent_ppo2.py:193][0m |          -0.0076 |          28.4766 |           3.0999 |
[32m[20230203 21:01:04 @agent_ppo2.py:193][0m |          -0.0069 |          26.6573 |           3.0941 |
[32m[20230203 21:01:04 @agent_ppo2.py:193][0m |          -0.0100 |          25.6388 |           3.0918 |
[32m[20230203 21:01:04 @agent_ppo2.py:193][0m |          -0.0115 |          24.8461 |           3.0875 |
[32m[20230203 21:01:04 @agent_ppo2.py:193][0m |          -0.0112 |          24.0209 |           3.0878 |
[32m[20230203 21:01:04 @agent_ppo2.py:193][0m |          -0.0091 |          24.2040 |           3.0882 |
[32m[20230203 21:01:04 @agent_ppo2.py:193][0m |          -0.0134 |          23.3877 |           3.0864 |
[32m[20230203 21:01:04 @agent_ppo2.py:193][0m |          -0.0063 |          24.5780 |           3.0844 |
[32m[20230203 21:01:04 @agent_ppo2.py:193][0m |          -0.0101 |          23.2283 |           3.0858 |
[32m[20230203 21:01:04 @agent_ppo2.py:138][0m Policy update time: 0.57 s
[32m[20230203 21:01:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 92.30
[32m[20230203 21:01:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.87
[32m[20230203 21:01:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.26
[32m[20230203 21:01:05 @agent_ppo2.py:151][0m Total time:      10.49 min
[32m[20230203 21:01:05 @agent_ppo2.py:153][0m 901120 total steps have happened
[32m[20230203 21:01:05 @agent_ppo2.py:129][0m #------------------------ Iteration 440 --------------------------#
[32m[20230203 21:01:05 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 21:01:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:05 @agent_ppo2.py:193][0m |           0.0038 |          17.4225 |           3.0233 |
[32m[20230203 21:01:05 @agent_ppo2.py:193][0m |          -0.0069 |          13.8112 |           3.0185 |
[32m[20230203 21:01:05 @agent_ppo2.py:193][0m |          -0.0077 |          13.2604 |           3.0166 |
[32m[20230203 21:01:05 @agent_ppo2.py:193][0m |          -0.0134 |          12.2936 |           3.0129 |
[32m[20230203 21:01:05 @agent_ppo2.py:193][0m |          -0.0132 |          11.9462 |           3.0131 |
[32m[20230203 21:01:05 @agent_ppo2.py:193][0m |          -0.0144 |          11.7238 |           3.0105 |
[32m[20230203 21:01:05 @agent_ppo2.py:193][0m |          -0.0237 |          11.2933 |           3.0147 |
[32m[20230203 21:01:06 @agent_ppo2.py:193][0m |          -0.0195 |          11.1327 |           3.0126 |
[32m[20230203 21:01:06 @agent_ppo2.py:193][0m |          -0.0181 |          10.7827 |           3.0132 |
[32m[20230203 21:01:06 @agent_ppo2.py:193][0m |          -0.0180 |          10.5148 |           3.0133 |
[32m[20230203 21:01:06 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:01:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 248.04
[32m[20230203 21:01:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.89
[32m[20230203 21:01:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.89
[32m[20230203 21:01:06 @agent_ppo2.py:151][0m Total time:      10.51 min
[32m[20230203 21:01:06 @agent_ppo2.py:153][0m 903168 total steps have happened
[32m[20230203 21:01:06 @agent_ppo2.py:129][0m #------------------------ Iteration 441 --------------------------#
[32m[20230203 21:01:06 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:01:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:06 @agent_ppo2.py:193][0m |          -0.0181 |          11.4408 |           3.0340 |
[32m[20230203 21:01:06 @agent_ppo2.py:193][0m |          -0.0091 |          10.4761 |           3.0270 |
[32m[20230203 21:01:07 @agent_ppo2.py:193][0m |          -0.0184 |          10.1938 |           3.0237 |
[32m[20230203 21:01:07 @agent_ppo2.py:193][0m |          -0.0154 |          10.0616 |           3.0201 |
[32m[20230203 21:01:07 @agent_ppo2.py:193][0m |          -0.0141 |           9.8955 |           3.0184 |
[32m[20230203 21:01:07 @agent_ppo2.py:193][0m |          -0.0081 |           9.8037 |           3.0180 |
[32m[20230203 21:01:07 @agent_ppo2.py:193][0m |          -0.0166 |           9.6689 |           3.0198 |
[32m[20230203 21:01:07 @agent_ppo2.py:193][0m |          -0.0143 |           9.5549 |           3.0189 |
[32m[20230203 21:01:07 @agent_ppo2.py:193][0m |          -0.0175 |           9.4962 |           3.0207 |
[32m[20230203 21:01:07 @agent_ppo2.py:193][0m |          -0.0096 |           9.4511 |           3.0227 |
[32m[20230203 21:01:07 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:01:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.50
[32m[20230203 21:01:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.88
[32m[20230203 21:01:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.91
[32m[20230203 21:01:07 @agent_ppo2.py:151][0m Total time:      10.53 min
[32m[20230203 21:01:07 @agent_ppo2.py:153][0m 905216 total steps have happened
[32m[20230203 21:01:07 @agent_ppo2.py:129][0m #------------------------ Iteration 442 --------------------------#
[32m[20230203 21:01:08 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 21:01:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:08 @agent_ppo2.py:193][0m |          -0.0003 |          11.7725 |           3.0716 |
[32m[20230203 21:01:08 @agent_ppo2.py:193][0m |          -0.0100 |          11.0475 |           3.0632 |
[32m[20230203 21:01:08 @agent_ppo2.py:193][0m |          -0.0114 |          10.7282 |           3.0593 |
[32m[20230203 21:01:08 @agent_ppo2.py:193][0m |          -0.0122 |          10.4497 |           3.0577 |
[32m[20230203 21:01:08 @agent_ppo2.py:193][0m |          -0.0141 |          10.2259 |           3.0533 |
[32m[20230203 21:01:08 @agent_ppo2.py:193][0m |          -0.0147 |          10.1173 |           3.0538 |
[32m[20230203 21:01:08 @agent_ppo2.py:193][0m |          -0.0150 |           9.9727 |           3.0532 |
[32m[20230203 21:01:08 @agent_ppo2.py:193][0m |          -0.0160 |           9.8764 |           3.0521 |
[32m[20230203 21:01:08 @agent_ppo2.py:193][0m |          -0.0165 |           9.7968 |           3.0512 |
[32m[20230203 21:01:08 @agent_ppo2.py:193][0m |          -0.0168 |           9.6970 |           3.0493 |
[32m[20230203 21:01:08 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:01:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.51
[32m[20230203 21:01:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.11
[32m[20230203 21:01:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.39
[32m[20230203 21:01:08 @agent_ppo2.py:151][0m Total time:      10.55 min
[32m[20230203 21:01:08 @agent_ppo2.py:153][0m 907264 total steps have happened
[32m[20230203 21:01:08 @agent_ppo2.py:129][0m #------------------------ Iteration 443 --------------------------#
[32m[20230203 21:01:09 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 21:01:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:09 @agent_ppo2.py:193][0m |           0.0015 |           9.2339 |           2.9887 |
[32m[20230203 21:01:09 @agent_ppo2.py:193][0m |          -0.0076 |           8.4071 |           2.9805 |
[32m[20230203 21:01:09 @agent_ppo2.py:193][0m |           0.0065 |           8.9304 |           2.9760 |
[32m[20230203 21:01:09 @agent_ppo2.py:193][0m |          -0.0111 |           7.7452 |           2.9728 |
[32m[20230203 21:01:09 @agent_ppo2.py:193][0m |          -0.0100 |           7.5074 |           2.9732 |
[32m[20230203 21:01:09 @agent_ppo2.py:193][0m |          -0.0070 |           7.3480 |           2.9693 |
[32m[20230203 21:01:09 @agent_ppo2.py:193][0m |          -0.0128 |           7.2203 |           2.9694 |
[32m[20230203 21:01:09 @agent_ppo2.py:193][0m |          -0.0122 |           7.1033 |           2.9706 |
[32m[20230203 21:01:09 @agent_ppo2.py:193][0m |          -0.0027 |           7.3013 |           2.9696 |
[32m[20230203 21:01:09 @agent_ppo2.py:193][0m |          -0.0128 |           6.8942 |           2.9692 |
[32m[20230203 21:01:09 @agent_ppo2.py:138][0m Policy update time: 0.50 s
[32m[20230203 21:01:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 237.64
[32m[20230203 21:01:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 240.86
[32m[20230203 21:01:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.18
[32m[20230203 21:01:10 @agent_ppo2.py:151][0m Total time:      10.58 min
[32m[20230203 21:01:10 @agent_ppo2.py:153][0m 909312 total steps have happened
[32m[20230203 21:01:10 @agent_ppo2.py:129][0m #------------------------ Iteration 444 --------------------------#
[32m[20230203 21:01:10 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 21:01:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:10 @agent_ppo2.py:193][0m |          -0.0027 |          10.9779 |           3.0233 |
[32m[20230203 21:01:10 @agent_ppo2.py:193][0m |          -0.0093 |           9.8523 |           3.0247 |
[32m[20230203 21:01:10 @agent_ppo2.py:193][0m |          -0.0082 |           9.5757 |           3.0237 |
[32m[20230203 21:01:10 @agent_ppo2.py:193][0m |          -0.0083 |           9.3690 |           3.0207 |
[32m[20230203 21:01:10 @agent_ppo2.py:193][0m |          -0.0121 |           8.8499 |           3.0198 |
[32m[20230203 21:01:10 @agent_ppo2.py:193][0m |          -0.0119 |           8.8857 |           3.0194 |
[32m[20230203 21:01:11 @agent_ppo2.py:193][0m |          -0.0100 |           8.4301 |           3.0217 |
[32m[20230203 21:01:11 @agent_ppo2.py:193][0m |          -0.0145 |           8.1951 |           3.0226 |
[32m[20230203 21:01:11 @agent_ppo2.py:193][0m |          -0.0183 |           8.0813 |           3.0239 |
[32m[20230203 21:01:11 @agent_ppo2.py:193][0m |          -0.0181 |           7.9550 |           3.0242 |
[32m[20230203 21:01:11 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:01:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.43
[32m[20230203 21:01:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.67
[32m[20230203 21:01:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.77
[32m[20230203 21:01:11 @agent_ppo2.py:151][0m Total time:      10.60 min
[32m[20230203 21:01:11 @agent_ppo2.py:153][0m 911360 total steps have happened
[32m[20230203 21:01:11 @agent_ppo2.py:129][0m #------------------------ Iteration 445 --------------------------#
[32m[20230203 21:01:11 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 21:01:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:11 @agent_ppo2.py:193][0m |          -0.0013 |          11.1701 |           3.0578 |
[32m[20230203 21:01:12 @agent_ppo2.py:193][0m |          -0.0049 |          10.3133 |           3.0505 |
[32m[20230203 21:01:12 @agent_ppo2.py:193][0m |          -0.0094 |          10.0235 |           3.0465 |
[32m[20230203 21:01:12 @agent_ppo2.py:193][0m |          -0.0105 |           9.9222 |           3.0402 |
[32m[20230203 21:01:12 @agent_ppo2.py:193][0m |          -0.0101 |           9.7091 |           3.0391 |
[32m[20230203 21:01:12 @agent_ppo2.py:193][0m |          -0.0110 |           9.8841 |           3.0352 |
[32m[20230203 21:01:12 @agent_ppo2.py:193][0m |          -0.0115 |           9.6913 |           3.0345 |
[32m[20230203 21:01:12 @agent_ppo2.py:193][0m |          -0.0061 |           9.6769 |           3.0322 |
[32m[20230203 21:01:12 @agent_ppo2.py:193][0m |          -0.0125 |           9.5771 |           3.0319 |
[32m[20230203 21:01:12 @agent_ppo2.py:193][0m |          -0.0155 |           9.1677 |           3.0268 |
[32m[20230203 21:01:12 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:01:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.13
[32m[20230203 21:01:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.39
[32m[20230203 21:01:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 249.68
[32m[20230203 21:01:12 @agent_ppo2.py:151][0m Total time:      10.62 min
[32m[20230203 21:01:12 @agent_ppo2.py:153][0m 913408 total steps have happened
[32m[20230203 21:01:12 @agent_ppo2.py:129][0m #------------------------ Iteration 446 --------------------------#
[32m[20230203 21:01:13 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:01:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:13 @agent_ppo2.py:193][0m |          -0.0026 |          11.8041 |           3.0433 |
[32m[20230203 21:01:13 @agent_ppo2.py:193][0m |          -0.0033 |          11.2435 |           3.0458 |
[32m[20230203 21:01:13 @agent_ppo2.py:193][0m |          -0.0123 |          10.5097 |           3.0398 |
[32m[20230203 21:01:13 @agent_ppo2.py:193][0m |          -0.0044 |          10.4575 |           3.0393 |
[32m[20230203 21:01:13 @agent_ppo2.py:193][0m |          -0.0103 |          10.1858 |           3.0410 |
[32m[20230203 21:01:13 @agent_ppo2.py:193][0m |          -0.0159 |          10.0649 |           3.0402 |
[32m[20230203 21:01:13 @agent_ppo2.py:193][0m |          -0.0094 |          10.3606 |           3.0392 |
[32m[20230203 21:01:13 @agent_ppo2.py:193][0m |          -0.0094 |          10.2150 |           3.0409 |
[32m[20230203 21:01:13 @agent_ppo2.py:193][0m |          -0.0157 |           9.8004 |           3.0359 |
[32m[20230203 21:01:13 @agent_ppo2.py:193][0m |          -0.0160 |           9.7627 |           3.0392 |
[32m[20230203 21:01:13 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:01:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.77
[32m[20230203 21:01:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.13
[32m[20230203 21:01:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.04
[32m[20230203 21:01:14 @agent_ppo2.py:151][0m Total time:      10.64 min
[32m[20230203 21:01:14 @agent_ppo2.py:153][0m 915456 total steps have happened
[32m[20230203 21:01:14 @agent_ppo2.py:129][0m #------------------------ Iteration 447 --------------------------#
[32m[20230203 21:01:14 @agent_ppo2.py:135][0m Sampling time: 0.49 s by 1 slaves
[32m[20230203 21:01:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:14 @agent_ppo2.py:193][0m |          -0.0001 |          18.2720 |           3.0702 |
[32m[20230203 21:01:14 @agent_ppo2.py:193][0m |          -0.0068 |          13.1413 |           3.0688 |
[32m[20230203 21:01:14 @agent_ppo2.py:193][0m |          -0.0068 |          12.0695 |           3.0655 |
[32m[20230203 21:01:14 @agent_ppo2.py:193][0m |          -0.0119 |          11.5265 |           3.0609 |
[32m[20230203 21:01:14 @agent_ppo2.py:193][0m |          -0.0105 |          11.2753 |           3.0565 |
[32m[20230203 21:01:14 @agent_ppo2.py:193][0m |          -0.0101 |          11.3310 |           3.0588 |
[32m[20230203 21:01:14 @agent_ppo2.py:193][0m |          -0.0132 |          11.0541 |           3.0572 |
[32m[20230203 21:01:14 @agent_ppo2.py:193][0m |          -0.0137 |          11.4393 |           3.0555 |
[32m[20230203 21:01:15 @agent_ppo2.py:193][0m |          -0.0156 |          10.6951 |           3.0578 |
[32m[20230203 21:01:15 @agent_ppo2.py:193][0m |          -0.0141 |          10.6192 |           3.0563 |
[32m[20230203 21:01:15 @agent_ppo2.py:138][0m Policy update time: 0.56 s
[32m[20230203 21:01:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 148.09
[32m[20230203 21:01:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.69
[32m[20230203 21:01:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.14
[32m[20230203 21:01:15 @agent_ppo2.py:151][0m Total time:      10.66 min
[32m[20230203 21:01:15 @agent_ppo2.py:153][0m 917504 total steps have happened
[32m[20230203 21:01:15 @agent_ppo2.py:129][0m #------------------------ Iteration 448 --------------------------#
[32m[20230203 21:01:15 @agent_ppo2.py:135][0m Sampling time: 0.44 s by 1 slaves
[32m[20230203 21:01:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:15 @agent_ppo2.py:193][0m |           0.0040 |          40.9954 |           2.9220 |
[32m[20230203 21:01:16 @agent_ppo2.py:193][0m |          -0.0065 |          17.2645 |           2.9181 |
[32m[20230203 21:01:16 @agent_ppo2.py:193][0m |          -0.0077 |          15.7485 |           2.9157 |
[32m[20230203 21:01:16 @agent_ppo2.py:193][0m |          -0.0061 |          14.6555 |           2.9124 |
[32m[20230203 21:01:16 @agent_ppo2.py:193][0m |          -0.0083 |          14.4206 |           2.9130 |
[32m[20230203 21:01:16 @agent_ppo2.py:193][0m |          -0.0112 |          13.5995 |           2.9122 |
[32m[20230203 21:01:16 @agent_ppo2.py:193][0m |          -0.0115 |          13.3181 |           2.9129 |
[32m[20230203 21:01:16 @agent_ppo2.py:193][0m |          -0.0134 |          12.9978 |           2.9140 |
[32m[20230203 21:01:16 @agent_ppo2.py:193][0m |          -0.0124 |          12.8124 |           2.9149 |
[32m[20230203 21:01:16 @agent_ppo2.py:193][0m |          -0.0151 |          12.4928 |           2.9144 |
[32m[20230203 21:01:16 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 21:01:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 116.96
[32m[20230203 21:01:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.17
[32m[20230203 21:01:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.00
[32m[20230203 21:01:16 @agent_ppo2.py:151][0m Total time:      10.68 min
[32m[20230203 21:01:16 @agent_ppo2.py:153][0m 919552 total steps have happened
[32m[20230203 21:01:16 @agent_ppo2.py:129][0m #------------------------ Iteration 449 --------------------------#
[32m[20230203 21:01:17 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:01:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:17 @agent_ppo2.py:193][0m |           0.0015 |          11.1842 |           2.9922 |
[32m[20230203 21:01:17 @agent_ppo2.py:193][0m |           0.0055 |          10.6221 |           2.9936 |
[32m[20230203 21:01:17 @agent_ppo2.py:193][0m |          -0.0315 |          10.4515 |           2.9940 |
[32m[20230203 21:01:17 @agent_ppo2.py:193][0m |          -0.0049 |          10.0722 |           2.9914 |
[32m[20230203 21:01:17 @agent_ppo2.py:193][0m |          -0.0064 |           9.9320 |           2.9947 |
[32m[20230203 21:01:17 @agent_ppo2.py:193][0m |          -0.0256 |           9.8456 |           2.9893 |
[32m[20230203 21:01:17 @agent_ppo2.py:193][0m |          -0.0140 |           9.7059 |           2.9893 |
[32m[20230203 21:01:17 @agent_ppo2.py:193][0m |          -0.0226 |           9.5993 |           2.9903 |
[32m[20230203 21:01:17 @agent_ppo2.py:193][0m |          -0.0172 |           9.5119 |           2.9864 |
[32m[20230203 21:01:17 @agent_ppo2.py:193][0m |          -0.0196 |           9.4145 |           2.9875 |
[32m[20230203 21:01:17 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:01:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 248.33
[32m[20230203 21:01:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.09
[32m[20230203 21:01:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.13
[32m[20230203 21:01:17 @agent_ppo2.py:151][0m Total time:      10.70 min
[32m[20230203 21:01:17 @agent_ppo2.py:153][0m 921600 total steps have happened
[32m[20230203 21:01:17 @agent_ppo2.py:129][0m #------------------------ Iteration 450 --------------------------#
[32m[20230203 21:01:18 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:01:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:18 @agent_ppo2.py:193][0m |           0.0008 |          11.6850 |           2.9744 |
[32m[20230203 21:01:18 @agent_ppo2.py:193][0m |          -0.0037 |          11.3456 |           2.9736 |
[32m[20230203 21:01:18 @agent_ppo2.py:193][0m |          -0.0080 |          11.1401 |           2.9701 |
[32m[20230203 21:01:18 @agent_ppo2.py:193][0m |          -0.0077 |          11.0644 |           2.9671 |
[32m[20230203 21:01:18 @agent_ppo2.py:193][0m |          -0.0108 |          10.8131 |           2.9669 |
[32m[20230203 21:01:18 @agent_ppo2.py:193][0m |          -0.0107 |          10.7551 |           2.9657 |
[32m[20230203 21:01:18 @agent_ppo2.py:193][0m |          -0.0103 |          10.7604 |           2.9641 |
[32m[20230203 21:01:18 @agent_ppo2.py:193][0m |          -0.0131 |          10.4975 |           2.9653 |
[32m[20230203 21:01:18 @agent_ppo2.py:193][0m |          -0.0121 |          10.4142 |           2.9618 |
[32m[20230203 21:01:18 @agent_ppo2.py:193][0m |          -0.0118 |          10.4753 |           2.9609 |
[32m[20230203 21:01:18 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:01:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 248.75
[32m[20230203 21:01:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.02
[32m[20230203 21:01:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 258.35
[32m[20230203 21:01:19 @agent_ppo2.py:151][0m Total time:      10.72 min
[32m[20230203 21:01:19 @agent_ppo2.py:153][0m 923648 total steps have happened
[32m[20230203 21:01:19 @agent_ppo2.py:129][0m #------------------------ Iteration 451 --------------------------#
[32m[20230203 21:01:19 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:01:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:19 @agent_ppo2.py:193][0m |           0.0011 |          11.2872 |           3.0093 |
[32m[20230203 21:01:19 @agent_ppo2.py:193][0m |          -0.0112 |          10.8255 |           3.0022 |
[32m[20230203 21:01:19 @agent_ppo2.py:193][0m |          -0.0136 |          10.7483 |           3.0036 |
[32m[20230203 21:01:19 @agent_ppo2.py:193][0m |          -0.0155 |          10.6277 |           3.0026 |
[32m[20230203 21:01:19 @agent_ppo2.py:193][0m |          -0.0155 |          10.5547 |           2.9992 |
[32m[20230203 21:01:19 @agent_ppo2.py:193][0m |          -0.0128 |          10.5822 |           2.9999 |
[32m[20230203 21:01:19 @agent_ppo2.py:193][0m |          -0.0173 |          10.4186 |           3.0006 |
[32m[20230203 21:01:19 @agent_ppo2.py:193][0m |          -0.0170 |          10.3647 |           3.0022 |
[32m[20230203 21:01:19 @agent_ppo2.py:193][0m |          -0.0180 |          10.3221 |           2.9998 |
[32m[20230203 21:01:20 @agent_ppo2.py:193][0m |          -0.0126 |          10.7523 |           2.9996 |
[32m[20230203 21:01:20 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:01:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.36
[32m[20230203 21:01:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.23
[32m[20230203 21:01:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.08
[32m[20230203 21:01:20 @agent_ppo2.py:151][0m Total time:      10.74 min
[32m[20230203 21:01:20 @agent_ppo2.py:153][0m 925696 total steps have happened
[32m[20230203 21:01:20 @agent_ppo2.py:129][0m #------------------------ Iteration 452 --------------------------#
[32m[20230203 21:01:20 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 21:01:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:20 @agent_ppo2.py:193][0m |          -0.0007 |          12.4417 |           3.0488 |
[32m[20230203 21:01:20 @agent_ppo2.py:193][0m |          -0.0046 |          11.4060 |           3.0386 |
[32m[20230203 21:01:20 @agent_ppo2.py:193][0m |          -0.0072 |          11.0702 |           3.0366 |
[32m[20230203 21:01:21 @agent_ppo2.py:193][0m |          -0.0087 |          10.7708 |           3.0338 |
[32m[20230203 21:01:21 @agent_ppo2.py:193][0m |          -0.0095 |          10.5936 |           3.0351 |
[32m[20230203 21:01:21 @agent_ppo2.py:193][0m |          -0.0110 |          10.3829 |           3.0284 |
[32m[20230203 21:01:21 @agent_ppo2.py:193][0m |          -0.0132 |          10.2344 |           3.0279 |
[32m[20230203 21:01:21 @agent_ppo2.py:193][0m |          -0.0141 |          10.0789 |           3.0253 |
[32m[20230203 21:01:21 @agent_ppo2.py:193][0m |          -0.0113 |          10.1340 |           3.0226 |
[32m[20230203 21:01:21 @agent_ppo2.py:193][0m |          -0.0132 |           9.9776 |           3.0229 |
[32m[20230203 21:01:21 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:01:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 244.93
[32m[20230203 21:01:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.88
[32m[20230203 21:01:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 257.33
[32m[20230203 21:01:21 @agent_ppo2.py:151][0m Total time:      10.76 min
[32m[20230203 21:01:21 @agent_ppo2.py:153][0m 927744 total steps have happened
[32m[20230203 21:01:21 @agent_ppo2.py:129][0m #------------------------ Iteration 453 --------------------------#
[32m[20230203 21:01:22 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:01:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:22 @agent_ppo2.py:193][0m |          -0.0049 |          11.0544 |           2.9262 |
[32m[20230203 21:01:22 @agent_ppo2.py:193][0m |          -0.0091 |          10.6013 |           2.9145 |
[32m[20230203 21:01:22 @agent_ppo2.py:193][0m |          -0.0127 |          10.0757 |           2.9128 |
[32m[20230203 21:01:22 @agent_ppo2.py:193][0m |          -0.0123 |           9.8341 |           2.9150 |
[32m[20230203 21:01:22 @agent_ppo2.py:193][0m |          -0.0097 |          10.0327 |           2.9139 |
[32m[20230203 21:01:22 @agent_ppo2.py:193][0m |          -0.0158 |           9.4701 |           2.9152 |
[32m[20230203 21:01:22 @agent_ppo2.py:193][0m |          -0.0156 |           9.3269 |           2.9151 |
[32m[20230203 21:01:22 @agent_ppo2.py:193][0m |          -0.0157 |           9.1444 |           2.9113 |
[32m[20230203 21:01:22 @agent_ppo2.py:193][0m |          -0.0175 |           8.9300 |           2.9109 |
[32m[20230203 21:01:22 @agent_ppo2.py:193][0m |          -0.0178 |           8.6669 |           2.9108 |
[32m[20230203 21:01:22 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:01:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.04
[32m[20230203 21:01:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.88
[32m[20230203 21:01:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 255.78
[32m[20230203 21:01:22 @agent_ppo2.py:151][0m Total time:      10.79 min
[32m[20230203 21:01:22 @agent_ppo2.py:153][0m 929792 total steps have happened
[32m[20230203 21:01:22 @agent_ppo2.py:129][0m #------------------------ Iteration 454 --------------------------#
[32m[20230203 21:01:23 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 21:01:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:23 @agent_ppo2.py:193][0m |          -0.0015 |          11.1516 |           2.9448 |
[32m[20230203 21:01:23 @agent_ppo2.py:193][0m |          -0.0106 |          10.7496 |           2.9439 |
[32m[20230203 21:01:23 @agent_ppo2.py:193][0m |          -0.0118 |          10.4997 |           2.9421 |
[32m[20230203 21:01:23 @agent_ppo2.py:193][0m |          -0.0139 |          10.4178 |           2.9406 |
[32m[20230203 21:01:23 @agent_ppo2.py:193][0m |          -0.0134 |          10.1861 |           2.9400 |
[32m[20230203 21:01:23 @agent_ppo2.py:193][0m |          -0.0104 |          10.5295 |           2.9386 |
[32m[20230203 21:01:23 @agent_ppo2.py:193][0m |          -0.0128 |          10.3677 |           2.9394 |
[32m[20230203 21:01:23 @agent_ppo2.py:193][0m |          -0.0167 |           9.8092 |           2.9372 |
[32m[20230203 21:01:23 @agent_ppo2.py:193][0m |          -0.0172 |           9.7031 |           2.9394 |
[32m[20230203 21:01:23 @agent_ppo2.py:193][0m |          -0.0124 |           9.9981 |           2.9396 |
[32m[20230203 21:01:23 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 21:01:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 243.02
[32m[20230203 21:01:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 243.52
[32m[20230203 21:01:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.18
[32m[20230203 21:01:24 @agent_ppo2.py:151][0m Total time:      10.81 min
[32m[20230203 21:01:24 @agent_ppo2.py:153][0m 931840 total steps have happened
[32m[20230203 21:01:24 @agent_ppo2.py:129][0m #------------------------ Iteration 455 --------------------------#
[32m[20230203 21:01:24 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:01:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:24 @agent_ppo2.py:193][0m |           0.0007 |          12.5928 |           2.9607 |
[32m[20230203 21:01:24 @agent_ppo2.py:193][0m |           0.0067 |          12.5930 |           2.9576 |
[32m[20230203 21:01:24 @agent_ppo2.py:193][0m |          -0.0128 |          11.5195 |           2.9564 |
[32m[20230203 21:01:24 @agent_ppo2.py:193][0m |          -0.0198 |          11.2726 |           2.9591 |
[32m[20230203 21:01:24 @agent_ppo2.py:193][0m |          -0.0109 |          11.0277 |           2.9572 |
[32m[20230203 21:01:24 @agent_ppo2.py:193][0m |          -0.0218 |          10.8651 |           2.9607 |
[32m[20230203 21:01:24 @agent_ppo2.py:193][0m |          -0.0188 |          10.6688 |           2.9574 |
[32m[20230203 21:01:24 @agent_ppo2.py:193][0m |          -0.0007 |          11.9558 |           2.9600 |
[32m[20230203 21:01:24 @agent_ppo2.py:193][0m |          -0.0165 |          10.3004 |           2.9573 |
[32m[20230203 21:01:24 @agent_ppo2.py:193][0m |          -0.0018 |          11.4365 |           2.9544 |
[32m[20230203 21:01:24 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:01:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.36
[32m[20230203 21:01:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.83
[32m[20230203 21:01:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 254.93
[32m[20230203 21:01:25 @agent_ppo2.py:151][0m Total time:      10.83 min
[32m[20230203 21:01:25 @agent_ppo2.py:153][0m 933888 total steps have happened
[32m[20230203 21:01:25 @agent_ppo2.py:129][0m #------------------------ Iteration 456 --------------------------#
[32m[20230203 21:01:25 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:01:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:25 @agent_ppo2.py:193][0m |          -0.0008 |          11.3703 |           2.9480 |
[32m[20230203 21:01:25 @agent_ppo2.py:193][0m |          -0.0071 |          10.8871 |           2.9467 |
[32m[20230203 21:01:25 @agent_ppo2.py:193][0m |          -0.0112 |          10.5550 |           2.9422 |
[32m[20230203 21:01:25 @agent_ppo2.py:193][0m |          -0.0046 |          10.7455 |           2.9418 |
[32m[20230203 21:01:25 @agent_ppo2.py:193][0m |          -0.0117 |          10.6433 |           2.9409 |
[32m[20230203 21:01:26 @agent_ppo2.py:193][0m |          -0.0157 |          10.0387 |           2.9381 |
[32m[20230203 21:01:26 @agent_ppo2.py:193][0m |          -0.0165 |           9.9239 |           2.9401 |
[32m[20230203 21:01:26 @agent_ppo2.py:193][0m |          -0.0155 |           9.8189 |           2.9381 |
[32m[20230203 21:01:26 @agent_ppo2.py:193][0m |          -0.0125 |           9.9218 |           2.9362 |
[32m[20230203 21:01:26 @agent_ppo2.py:193][0m |          -0.0157 |           9.6630 |           2.9339 |
[32m[20230203 21:01:26 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:01:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.92
[32m[20230203 21:01:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.20
[32m[20230203 21:01:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 259.89
[32m[20230203 21:01:26 @agent_ppo2.py:151][0m Total time:      10.85 min
[32m[20230203 21:01:26 @agent_ppo2.py:153][0m 935936 total steps have happened
[32m[20230203 21:01:26 @agent_ppo2.py:129][0m #------------------------ Iteration 457 --------------------------#
[32m[20230203 21:01:26 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 21:01:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:27 @agent_ppo2.py:193][0m |           0.0023 |          31.2440 |           2.9616 |
[32m[20230203 21:01:27 @agent_ppo2.py:193][0m |           0.0020 |          19.6247 |           2.9602 |
[32m[20230203 21:01:27 @agent_ppo2.py:193][0m |          -0.0036 |          17.1970 |           2.9587 |
[32m[20230203 21:01:27 @agent_ppo2.py:193][0m |          -0.0070 |          16.1091 |           2.9581 |
[32m[20230203 21:01:27 @agent_ppo2.py:193][0m |          -0.0087 |          15.3840 |           2.9562 |
[32m[20230203 21:01:27 @agent_ppo2.py:193][0m |          -0.0102 |          14.5331 |           2.9545 |
[32m[20230203 21:01:27 @agent_ppo2.py:193][0m |          -0.0107 |          14.0270 |           2.9519 |
[32m[20230203 21:01:27 @agent_ppo2.py:193][0m |          -0.0106 |          13.6784 |           2.9526 |
[32m[20230203 21:01:27 @agent_ppo2.py:193][0m |          -0.0111 |          13.3357 |           2.9508 |
[32m[20230203 21:01:27 @agent_ppo2.py:193][0m |          -0.0125 |          13.0332 |           2.9479 |
[32m[20230203 21:01:27 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 21:01:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 109.36
[32m[20230203 21:01:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.90
[32m[20230203 21:01:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 155.63
[32m[20230203 21:01:27 @agent_ppo2.py:151][0m Total time:      10.87 min
[32m[20230203 21:01:27 @agent_ppo2.py:153][0m 937984 total steps have happened
[32m[20230203 21:01:27 @agent_ppo2.py:129][0m #------------------------ Iteration 458 --------------------------#
[32m[20230203 21:01:28 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:01:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:28 @agent_ppo2.py:193][0m |           0.0046 |          14.7804 |           2.9476 |
[32m[20230203 21:01:28 @agent_ppo2.py:193][0m |          -0.0085 |          11.9017 |           2.9370 |
[32m[20230203 21:01:28 @agent_ppo2.py:193][0m |          -0.0021 |          12.6156 |           2.9442 |
[32m[20230203 21:01:28 @agent_ppo2.py:193][0m |          -0.0184 |          11.0714 |           2.9439 |
[32m[20230203 21:01:28 @agent_ppo2.py:193][0m |          -0.0188 |          10.8507 |           2.9449 |
[32m[20230203 21:01:28 @agent_ppo2.py:193][0m |          -0.0199 |          10.5908 |           2.9471 |
[32m[20230203 21:01:28 @agent_ppo2.py:193][0m |          -0.0105 |          10.4670 |           2.9484 |
[32m[20230203 21:01:28 @agent_ppo2.py:193][0m |          -0.0189 |          10.3308 |           2.9456 |
[32m[20230203 21:01:28 @agent_ppo2.py:193][0m |          -0.0167 |          10.1326 |           2.9484 |
[32m[20230203 21:01:28 @agent_ppo2.py:193][0m |          -0.0233 |           9.9623 |           2.9485 |
[32m[20230203 21:01:28 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:01:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.08
[32m[20230203 21:01:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.90
[32m[20230203 21:01:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 154.40
[32m[20230203 21:01:29 @agent_ppo2.py:151][0m Total time:      10.89 min
[32m[20230203 21:01:29 @agent_ppo2.py:153][0m 940032 total steps have happened
[32m[20230203 21:01:29 @agent_ppo2.py:129][0m #------------------------ Iteration 459 --------------------------#
[32m[20230203 21:01:29 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 21:01:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:29 @agent_ppo2.py:193][0m |           0.0003 |          11.6884 |           2.8842 |
[32m[20230203 21:01:29 @agent_ppo2.py:193][0m |          -0.0075 |          11.0216 |           2.8805 |
[32m[20230203 21:01:29 @agent_ppo2.py:193][0m |          -0.0109 |          10.6978 |           2.8762 |
[32m[20230203 21:01:29 @agent_ppo2.py:193][0m |          -0.0117 |          10.5286 |           2.8766 |
[32m[20230203 21:01:29 @agent_ppo2.py:193][0m |          -0.0137 |          10.3960 |           2.8758 |
[32m[20230203 21:01:29 @agent_ppo2.py:193][0m |          -0.0153 |          10.2450 |           2.8745 |
[32m[20230203 21:01:29 @agent_ppo2.py:193][0m |          -0.0151 |          10.2942 |           2.8763 |
[32m[20230203 21:01:29 @agent_ppo2.py:193][0m |          -0.0163 |          10.0960 |           2.8748 |
[32m[20230203 21:01:29 @agent_ppo2.py:193][0m |          -0.0162 |          10.0898 |           2.8747 |
[32m[20230203 21:01:30 @agent_ppo2.py:193][0m |          -0.0173 |          10.1038 |           2.8742 |
[32m[20230203 21:01:30 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:01:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 246.07
[32m[20230203 21:01:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.74
[32m[20230203 21:01:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.06
[32m[20230203 21:01:30 @agent_ppo2.py:151][0m Total time:      10.91 min
[32m[20230203 21:01:30 @agent_ppo2.py:153][0m 942080 total steps have happened
[32m[20230203 21:01:30 @agent_ppo2.py:129][0m #------------------------ Iteration 460 --------------------------#
[32m[20230203 21:01:30 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 21:01:30 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:30 @agent_ppo2.py:193][0m |          -0.0011 |          12.8775 |           2.9528 |
[32m[20230203 21:01:30 @agent_ppo2.py:193][0m |          -0.0067 |          12.4479 |           2.9474 |
[32m[20230203 21:01:30 @agent_ppo2.py:193][0m |          -0.0097 |          12.2077 |           2.9497 |
[32m[20230203 21:01:30 @agent_ppo2.py:193][0m |          -0.0112 |          12.0380 |           2.9499 |
[32m[20230203 21:01:30 @agent_ppo2.py:193][0m |          -0.0131 |          11.9171 |           2.9471 |
[32m[20230203 21:01:31 @agent_ppo2.py:193][0m |          -0.0139 |          11.7958 |           2.9450 |
[32m[20230203 21:01:31 @agent_ppo2.py:193][0m |          -0.0146 |          11.7191 |           2.9415 |
[32m[20230203 21:01:31 @agent_ppo2.py:193][0m |          -0.0157 |          11.6114 |           2.9408 |
[32m[20230203 21:01:31 @agent_ppo2.py:193][0m |          -0.0161 |          11.5515 |           2.9406 |
[32m[20230203 21:01:31 @agent_ppo2.py:193][0m |          -0.0165 |          11.4484 |           2.9387 |
[32m[20230203 21:01:31 @agent_ppo2.py:138][0m Policy update time: 0.44 s
[32m[20230203 21:01:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 254.59
[32m[20230203 21:01:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.22
[32m[20230203 21:01:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 221.61
[32m[20230203 21:01:31 @agent_ppo2.py:151][0m Total time:      10.93 min
[32m[20230203 21:01:31 @agent_ppo2.py:153][0m 944128 total steps have happened
[32m[20230203 21:01:31 @agent_ppo2.py:129][0m #------------------------ Iteration 461 --------------------------#
[32m[20230203 21:01:32 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 21:01:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:32 @agent_ppo2.py:193][0m |          -0.0031 |          12.1122 |           2.9128 |
[32m[20230203 21:01:32 @agent_ppo2.py:193][0m |          -0.0093 |          11.2537 |           2.9047 |
[32m[20230203 21:01:32 @agent_ppo2.py:193][0m |          -0.0042 |          11.2642 |           2.8985 |
[32m[20230203 21:01:32 @agent_ppo2.py:193][0m |          -0.0110 |           9.9114 |           2.8948 |
[32m[20230203 21:01:32 @agent_ppo2.py:193][0m |          -0.0118 |           9.1229 |           2.8951 |
[32m[20230203 21:01:32 @agent_ppo2.py:193][0m |          -0.0145 |           8.9813 |           2.8934 |
[32m[20230203 21:01:32 @agent_ppo2.py:193][0m |          -0.0170 |           8.5673 |           2.8892 |
[32m[20230203 21:01:32 @agent_ppo2.py:193][0m |          -0.0190 |           8.3803 |           2.8889 |
[32m[20230203 21:01:32 @agent_ppo2.py:193][0m |          -0.0173 |           8.3079 |           2.8868 |
[32m[20230203 21:01:32 @agent_ppo2.py:193][0m |          -0.0146 |           8.3194 |           2.8873 |
[32m[20230203 21:01:32 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 21:01:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 256.00
[32m[20230203 21:01:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.93
[32m[20230203 21:01:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.37
[32m[20230203 21:01:32 @agent_ppo2.py:151][0m Total time:      10.95 min
[32m[20230203 21:01:32 @agent_ppo2.py:153][0m 946176 total steps have happened
[32m[20230203 21:01:32 @agent_ppo2.py:129][0m #------------------------ Iteration 462 --------------------------#
[32m[20230203 21:01:33 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 21:01:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:33 @agent_ppo2.py:193][0m |           0.0010 |          19.9012 |           2.9273 |
[32m[20230203 21:01:33 @agent_ppo2.py:193][0m |          -0.0036 |          16.0269 |           2.9153 |
[32m[20230203 21:01:33 @agent_ppo2.py:193][0m |          -0.0076 |          14.9559 |           2.9121 |
[32m[20230203 21:01:33 @agent_ppo2.py:193][0m |          -0.0123 |          14.2487 |           2.9114 |
[32m[20230203 21:01:33 @agent_ppo2.py:193][0m |          -0.0128 |          14.0943 |           2.9129 |
[32m[20230203 21:01:33 @agent_ppo2.py:193][0m |          -0.0193 |          13.5355 |           2.9116 |
[32m[20230203 21:01:33 @agent_ppo2.py:193][0m |          -0.0119 |          13.5536 |           2.9116 |
[32m[20230203 21:01:33 @agent_ppo2.py:193][0m |          -0.0127 |          13.0968 |           2.9106 |
[32m[20230203 21:01:33 @agent_ppo2.py:193][0m |          -0.0186 |          12.9252 |           2.9086 |
[32m[20230203 21:01:33 @agent_ppo2.py:193][0m |          -0.0171 |          12.7213 |           2.9081 |
[32m[20230203 21:01:33 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 21:01:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 244.62
[32m[20230203 21:01:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.92
[32m[20230203 21:01:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.25
[32m[20230203 21:01:34 @agent_ppo2.py:151][0m Total time:      10.97 min
[32m[20230203 21:01:34 @agent_ppo2.py:153][0m 948224 total steps have happened
[32m[20230203 21:01:34 @agent_ppo2.py:129][0m #------------------------ Iteration 463 --------------------------#
[32m[20230203 21:01:34 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 21:01:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:34 @agent_ppo2.py:193][0m |           0.0004 |          13.7991 |           2.9457 |
[32m[20230203 21:01:34 @agent_ppo2.py:193][0m |          -0.0085 |          12.3839 |           2.9436 |
[32m[20230203 21:01:34 @agent_ppo2.py:193][0m |          -0.0113 |          11.9425 |           2.9394 |
[32m[20230203 21:01:34 @agent_ppo2.py:193][0m |          -0.0133 |          11.5474 |           2.9390 |
[32m[20230203 21:01:34 @agent_ppo2.py:193][0m |          -0.0150 |          11.2657 |           2.9378 |
[32m[20230203 21:01:34 @agent_ppo2.py:193][0m |          -0.0160 |          11.0225 |           2.9360 |
[32m[20230203 21:01:34 @agent_ppo2.py:193][0m |          -0.0171 |          10.7747 |           2.9337 |
[32m[20230203 21:01:34 @agent_ppo2.py:193][0m |          -0.0177 |          10.6187 |           2.9361 |
[32m[20230203 21:01:34 @agent_ppo2.py:193][0m |          -0.0184 |          10.4859 |           2.9324 |
[32m[20230203 21:01:34 @agent_ppo2.py:193][0m |          -0.0188 |          10.3697 |           2.9331 |
[32m[20230203 21:01:34 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 21:01:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 259.77
[32m[20230203 21:01:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.92
[32m[20230203 21:01:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.51
[32m[20230203 21:01:35 @agent_ppo2.py:151][0m Total time:      10.99 min
[32m[20230203 21:01:35 @agent_ppo2.py:153][0m 950272 total steps have happened
[32m[20230203 21:01:35 @agent_ppo2.py:129][0m #------------------------ Iteration 464 --------------------------#
[32m[20230203 21:01:35 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:01:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:35 @agent_ppo2.py:193][0m |          -0.0082 |          14.4360 |           2.8952 |
[32m[20230203 21:01:35 @agent_ppo2.py:193][0m |          -0.0003 |          14.2668 |           2.8935 |
[32m[20230203 21:01:35 @agent_ppo2.py:193][0m |           0.0023 |          14.3796 |           2.8913 |
[32m[20230203 21:01:35 @agent_ppo2.py:193][0m |          -0.0024 |          13.5264 |           2.8899 |
[32m[20230203 21:01:35 @agent_ppo2.py:193][0m |          -0.0125 |          12.9718 |           2.8879 |
[32m[20230203 21:01:35 @agent_ppo2.py:193][0m |          -0.0091 |          12.7777 |           2.8869 |
[32m[20230203 21:01:35 @agent_ppo2.py:193][0m |          -0.0136 |          12.6830 |           2.8859 |
[32m[20230203 21:01:35 @agent_ppo2.py:193][0m |          -0.0146 |          12.5829 |           2.8849 |
[32m[20230203 21:01:35 @agent_ppo2.py:193][0m |          -0.0033 |          13.0867 |           2.8825 |
[32m[20230203 21:01:35 @agent_ppo2.py:193][0m |          -0.0139 |          12.4038 |           2.8839 |
[32m[20230203 21:01:35 @agent_ppo2.py:138][0m Policy update time: 0.44 s
[32m[20230203 21:01:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.51
[32m[20230203 21:01:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 258.56
[32m[20230203 21:01:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 75.20
[32m[20230203 21:01:36 @agent_ppo2.py:151][0m Total time:      11.01 min
[32m[20230203 21:01:36 @agent_ppo2.py:153][0m 952320 total steps have happened
[32m[20230203 21:01:36 @agent_ppo2.py:129][0m #------------------------ Iteration 465 --------------------------#
[32m[20230203 21:01:36 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 21:01:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:36 @agent_ppo2.py:193][0m |          -0.0018 |          15.4150 |           2.8431 |
[32m[20230203 21:01:36 @agent_ppo2.py:193][0m |          -0.0086 |          14.6193 |           2.8401 |
[32m[20230203 21:01:36 @agent_ppo2.py:193][0m |          -0.0100 |          14.5168 |           2.8380 |
[32m[20230203 21:01:36 @agent_ppo2.py:193][0m |          -0.0135 |          14.0776 |           2.8366 |
[32m[20230203 21:01:36 @agent_ppo2.py:193][0m |          -0.0120 |          14.0435 |           2.8347 |
[32m[20230203 21:01:37 @agent_ppo2.py:193][0m |          -0.0149 |          13.7457 |           2.8342 |
[32m[20230203 21:01:37 @agent_ppo2.py:193][0m |          -0.0156 |          13.6472 |           2.8374 |
[32m[20230203 21:01:37 @agent_ppo2.py:193][0m |          -0.0164 |          13.5610 |           2.8365 |
[32m[20230203 21:01:37 @agent_ppo2.py:193][0m |          -0.0153 |          13.5565 |           2.8361 |
[32m[20230203 21:01:37 @agent_ppo2.py:193][0m |          -0.0172 |          13.3763 |           2.8355 |
[32m[20230203 21:01:37 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 21:01:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 259.03
[32m[20230203 21:01:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.15
[32m[20230203 21:01:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 176.56
[32m[20230203 21:01:37 @agent_ppo2.py:151][0m Total time:      11.03 min
[32m[20230203 21:01:37 @agent_ppo2.py:153][0m 954368 total steps have happened
[32m[20230203 21:01:37 @agent_ppo2.py:129][0m #------------------------ Iteration 466 --------------------------#
[32m[20230203 21:01:38 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 21:01:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:38 @agent_ppo2.py:193][0m |           0.0014 |          27.9707 |           2.9795 |
[32m[20230203 21:01:38 @agent_ppo2.py:193][0m |          -0.0018 |          16.5156 |           2.9774 |
[32m[20230203 21:01:38 @agent_ppo2.py:193][0m |          -0.0041 |          14.3279 |           2.9697 |
[32m[20230203 21:01:38 @agent_ppo2.py:193][0m |          -0.0060 |          13.7337 |           2.9672 |
[32m[20230203 21:01:38 @agent_ppo2.py:193][0m |          -0.0068 |          13.3187 |           2.9653 |
[32m[20230203 21:01:38 @agent_ppo2.py:193][0m |          -0.0080 |          13.1356 |           2.9577 |
[32m[20230203 21:01:38 @agent_ppo2.py:193][0m |          -0.0094 |          12.7764 |           2.9570 |
[32m[20230203 21:01:38 @agent_ppo2.py:193][0m |          -0.0106 |          12.5926 |           2.9556 |
[32m[20230203 21:01:38 @agent_ppo2.py:193][0m |          -0.0102 |          12.4166 |           2.9535 |
[32m[20230203 21:01:38 @agent_ppo2.py:193][0m |          -0.0108 |          12.2931 |           2.9527 |
[32m[20230203 21:01:38 @agent_ppo2.py:138][0m Policy update time: 0.53 s
[32m[20230203 21:01:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 176.92
[32m[20230203 21:01:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.06
[32m[20230203 21:01:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 156.91
[32m[20230203 21:01:39 @agent_ppo2.py:151][0m Total time:      11.05 min
[32m[20230203 21:01:39 @agent_ppo2.py:153][0m 956416 total steps have happened
[32m[20230203 21:01:39 @agent_ppo2.py:129][0m #------------------------ Iteration 467 --------------------------#
[32m[20230203 21:01:39 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 21:01:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:39 @agent_ppo2.py:193][0m |          -0.0022 |          33.9366 |           2.8753 |
[32m[20230203 21:01:39 @agent_ppo2.py:193][0m |          -0.0044 |          24.8843 |           2.8648 |
[32m[20230203 21:01:39 @agent_ppo2.py:193][0m |          -0.0066 |          22.1828 |           2.8671 |
[32m[20230203 21:01:39 @agent_ppo2.py:193][0m |          -0.0105 |          21.2701 |           2.8662 |
[32m[20230203 21:01:39 @agent_ppo2.py:193][0m |          -0.0124 |          20.3687 |           2.8624 |
[32m[20230203 21:01:39 @agent_ppo2.py:193][0m |          -0.0135 |          20.0761 |           2.8607 |
[32m[20230203 21:01:39 @agent_ppo2.py:193][0m |          -0.0121 |          19.3793 |           2.8625 |
[32m[20230203 21:01:39 @agent_ppo2.py:193][0m |          -0.0148 |          18.9565 |           2.8601 |
[32m[20230203 21:01:39 @agent_ppo2.py:193][0m |          -0.0128 |          18.8626 |           2.8570 |
[32m[20230203 21:01:39 @agent_ppo2.py:193][0m |          -0.0134 |          18.3356 |           2.8576 |
[32m[20230203 21:01:39 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 21:01:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 156.04
[32m[20230203 21:01:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.25
[32m[20230203 21:01:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.04
[32m[20230203 21:01:40 @agent_ppo2.py:151][0m Total time:      11.08 min
[32m[20230203 21:01:40 @agent_ppo2.py:153][0m 958464 total steps have happened
[32m[20230203 21:01:40 @agent_ppo2.py:129][0m #------------------------ Iteration 468 --------------------------#
[32m[20230203 21:01:40 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:01:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:40 @agent_ppo2.py:193][0m |           0.0006 |          48.9636 |           2.9073 |
[32m[20230203 21:01:40 @agent_ppo2.py:193][0m |          -0.0054 |          39.1042 |           2.8988 |
[32m[20230203 21:01:40 @agent_ppo2.py:193][0m |          -0.0077 |          34.1981 |           2.8953 |
[32m[20230203 21:01:40 @agent_ppo2.py:193][0m |          -0.0100 |          32.2906 |           2.8965 |
[32m[20230203 21:01:40 @agent_ppo2.py:193][0m |          -0.0118 |          31.0359 |           2.8946 |
[32m[20230203 21:01:40 @agent_ppo2.py:193][0m |          -0.0123 |          30.2294 |           2.8938 |
[32m[20230203 21:01:41 @agent_ppo2.py:193][0m |          -0.0130 |          29.6870 |           2.8944 |
[32m[20230203 21:01:41 @agent_ppo2.py:193][0m |          -0.0133 |          29.1193 |           2.8921 |
[32m[20230203 21:01:41 @agent_ppo2.py:193][0m |          -0.0128 |          28.5722 |           2.8949 |
[32m[20230203 21:01:41 @agent_ppo2.py:193][0m |          -0.0137 |          28.3883 |           2.8945 |
[32m[20230203 21:01:41 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 21:01:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 193.97
[32m[20230203 21:01:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.07
[32m[20230203 21:01:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.29
[32m[20230203 21:01:41 @agent_ppo2.py:151][0m Total time:      11.10 min
[32m[20230203 21:01:41 @agent_ppo2.py:153][0m 960512 total steps have happened
[32m[20230203 21:01:41 @agent_ppo2.py:129][0m #------------------------ Iteration 469 --------------------------#
[32m[20230203 21:01:41 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:01:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:41 @agent_ppo2.py:193][0m |           0.0042 |          13.9076 |           2.8692 |
[32m[20230203 21:01:42 @agent_ppo2.py:193][0m |          -0.0094 |          12.1881 |           2.8625 |
[32m[20230203 21:01:42 @agent_ppo2.py:193][0m |          -0.0101 |          12.2097 |           2.8596 |
[32m[20230203 21:01:42 @agent_ppo2.py:193][0m |          -0.0154 |          11.6604 |           2.8623 |
[32m[20230203 21:01:42 @agent_ppo2.py:193][0m |          -0.0139 |          11.5254 |           2.8637 |
[32m[20230203 21:01:42 @agent_ppo2.py:193][0m |          -0.0153 |          11.4152 |           2.8599 |
[32m[20230203 21:01:42 @agent_ppo2.py:193][0m |          -0.0156 |          11.2835 |           2.8632 |
[32m[20230203 21:01:42 @agent_ppo2.py:193][0m |          -0.0154 |          11.3023 |           2.8622 |
[32m[20230203 21:01:42 @agent_ppo2.py:193][0m |          -0.0156 |          11.2522 |           2.8634 |
[32m[20230203 21:01:42 @agent_ppo2.py:193][0m |          -0.0169 |          11.1756 |           2.8631 |
[32m[20230203 21:01:42 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 21:01:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.65
[32m[20230203 21:01:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.21
[32m[20230203 21:01:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 266.85
[32m[20230203 21:01:42 @agent_ppo2.py:151][0m Total time:      11.12 min
[32m[20230203 21:01:42 @agent_ppo2.py:153][0m 962560 total steps have happened
[32m[20230203 21:01:42 @agent_ppo2.py:129][0m #------------------------ Iteration 470 --------------------------#
[32m[20230203 21:01:43 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 21:01:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:43 @agent_ppo2.py:193][0m |          -0.0043 |          58.1636 |           2.8257 |
[32m[20230203 21:01:43 @agent_ppo2.py:193][0m |          -0.0115 |          43.0720 |           2.8195 |
[32m[20230203 21:01:43 @agent_ppo2.py:193][0m |          -0.0094 |          39.8562 |           2.8186 |
[32m[20230203 21:01:43 @agent_ppo2.py:193][0m |          -0.0141 |          38.7787 |           2.8186 |
[32m[20230203 21:01:43 @agent_ppo2.py:193][0m |          -0.0127 |          37.5426 |           2.8176 |
[32m[20230203 21:01:43 @agent_ppo2.py:193][0m |          -0.0092 |          36.6480 |           2.8183 |
[32m[20230203 21:01:43 @agent_ppo2.py:193][0m |          -0.0090 |          35.6782 |           2.8188 |
[32m[20230203 21:01:43 @agent_ppo2.py:193][0m |          -0.0180 |          35.0614 |           2.8165 |
[32m[20230203 21:01:43 @agent_ppo2.py:193][0m |          -0.0063 |          34.6449 |           2.8178 |
[32m[20230203 21:01:43 @agent_ppo2.py:193][0m |          -0.0161 |          33.9726 |           2.8165 |
[32m[20230203 21:01:43 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 21:01:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 129.28
[32m[20230203 21:01:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.75
[32m[20230203 21:01:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 126.59
[32m[20230203 21:01:44 @agent_ppo2.py:151][0m Total time:      11.14 min
[32m[20230203 21:01:44 @agent_ppo2.py:153][0m 964608 total steps have happened
[32m[20230203 21:01:44 @agent_ppo2.py:129][0m #------------------------ Iteration 471 --------------------------#
[32m[20230203 21:01:44 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 21:01:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:44 @agent_ppo2.py:193][0m |          -0.0009 |          63.8501 |           2.8853 |
[32m[20230203 21:01:44 @agent_ppo2.py:193][0m |          -0.0090 |          54.6832 |           2.8782 |
[32m[20230203 21:01:44 @agent_ppo2.py:193][0m |          -0.0144 |          49.3280 |           2.8786 |
[32m[20230203 21:01:44 @agent_ppo2.py:193][0m |          -0.0087 |          47.4587 |           2.8817 |
[32m[20230203 21:01:44 @agent_ppo2.py:193][0m |          -0.0179 |          43.8293 |           2.8787 |
[32m[20230203 21:01:44 @agent_ppo2.py:193][0m |          -0.0161 |          42.5147 |           2.8807 |
[32m[20230203 21:01:44 @agent_ppo2.py:193][0m |          -0.0147 |          41.3025 |           2.8843 |
[32m[20230203 21:01:44 @agent_ppo2.py:193][0m |          -0.0149 |          40.1446 |           2.8846 |
[32m[20230203 21:01:44 @agent_ppo2.py:193][0m |           0.0034 |          46.6537 |           2.8850 |
[32m[20230203 21:01:44 @agent_ppo2.py:193][0m |          -0.0160 |          39.0204 |           2.8806 |
[32m[20230203 21:01:44 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 21:01:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 198.46
[32m[20230203 21:01:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.92
[32m[20230203 21:01:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.10
[32m[20230203 21:01:45 @agent_ppo2.py:151][0m Total time:      11.16 min
[32m[20230203 21:01:45 @agent_ppo2.py:153][0m 966656 total steps have happened
[32m[20230203 21:01:45 @agent_ppo2.py:129][0m #------------------------ Iteration 472 --------------------------#
[32m[20230203 21:01:45 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 21:01:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:45 @agent_ppo2.py:193][0m |          -0.0003 |          13.9264 |           2.8633 |
[32m[20230203 21:01:45 @agent_ppo2.py:193][0m |          -0.0090 |          12.7991 |           2.8580 |
[32m[20230203 21:01:45 @agent_ppo2.py:193][0m |          -0.0114 |          12.1258 |           2.8585 |
[32m[20230203 21:01:45 @agent_ppo2.py:193][0m |          -0.0108 |          11.6214 |           2.8638 |
[32m[20230203 21:01:45 @agent_ppo2.py:193][0m |          -0.0152 |          11.1475 |           2.8609 |
[32m[20230203 21:01:45 @agent_ppo2.py:193][0m |          -0.0125 |          10.9886 |           2.8612 |
[32m[20230203 21:01:45 @agent_ppo2.py:193][0m |          -0.0151 |          10.5688 |           2.8610 |
[32m[20230203 21:01:45 @agent_ppo2.py:193][0m |          -0.0172 |          10.3199 |           2.8604 |
[32m[20230203 21:01:45 @agent_ppo2.py:193][0m |          -0.0169 |          10.1343 |           2.8628 |
[32m[20230203 21:01:46 @agent_ppo2.py:193][0m |          -0.0168 |           9.9603 |           2.8604 |
[32m[20230203 21:01:46 @agent_ppo2.py:138][0m Policy update time: 0.44 s
[32m[20230203 21:01:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 256.20
[32m[20230203 21:01:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.62
[32m[20230203 21:01:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.99
[32m[20230203 21:01:46 @agent_ppo2.py:151][0m Total time:      11.18 min
[32m[20230203 21:01:46 @agent_ppo2.py:153][0m 968704 total steps have happened
[32m[20230203 21:01:46 @agent_ppo2.py:129][0m #------------------------ Iteration 473 --------------------------#
[32m[20230203 21:01:46 @agent_ppo2.py:135][0m Sampling time: 0.43 s by 1 slaves
[32m[20230203 21:01:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:46 @agent_ppo2.py:193][0m |          -0.0003 |         125.3977 |           2.9278 |
[32m[20230203 21:01:46 @agent_ppo2.py:193][0m |          -0.0075 |          70.8829 |           2.9182 |
[32m[20230203 21:01:46 @agent_ppo2.py:193][0m |          -0.0112 |          60.8645 |           2.9143 |
[32m[20230203 21:01:46 @agent_ppo2.py:193][0m |          -0.0147 |          56.5198 |           2.9135 |
[32m[20230203 21:01:47 @agent_ppo2.py:193][0m |          -0.0158 |          53.9860 |           2.9134 |
[32m[20230203 21:01:47 @agent_ppo2.py:193][0m |          -0.0150 |          52.9519 |           2.9129 |
[32m[20230203 21:01:47 @agent_ppo2.py:193][0m |          -0.0154 |          52.1217 |           2.9130 |
[32m[20230203 21:01:47 @agent_ppo2.py:193][0m |          -0.0186 |          50.5051 |           2.9113 |
[32m[20230203 21:01:47 @agent_ppo2.py:193][0m |          -0.0187 |          48.9593 |           2.9104 |
[32m[20230203 21:01:47 @agent_ppo2.py:193][0m |          -0.0188 |          48.0930 |           2.9132 |
[32m[20230203 21:01:47 @agent_ppo2.py:138][0m Policy update time: 0.48 s
[32m[20230203 21:01:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: -7.35
[32m[20230203 21:01:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 258.91
[32m[20230203 21:01:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.23
[32m[20230203 21:01:47 @agent_ppo2.py:151][0m Total time:      11.20 min
[32m[20230203 21:01:47 @agent_ppo2.py:153][0m 970752 total steps have happened
[32m[20230203 21:01:47 @agent_ppo2.py:129][0m #------------------------ Iteration 474 --------------------------#
[32m[20230203 21:01:47 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 21:01:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:48 @agent_ppo2.py:193][0m |           0.0098 |          30.7231 |           2.8883 |
[32m[20230203 21:01:48 @agent_ppo2.py:193][0m |           0.0025 |          19.2417 |           2.8813 |
[32m[20230203 21:01:48 @agent_ppo2.py:193][0m |          -0.0088 |          17.8390 |           2.8843 |
[32m[20230203 21:01:48 @agent_ppo2.py:193][0m |          -0.0110 |          17.2743 |           2.8856 |
[32m[20230203 21:01:48 @agent_ppo2.py:193][0m |          -0.0151 |          16.9392 |           2.8873 |
[32m[20230203 21:01:48 @agent_ppo2.py:193][0m |          -0.0021 |          17.0860 |           2.8876 |
[32m[20230203 21:01:48 @agent_ppo2.py:193][0m |          -0.0271 |          16.4744 |           2.8853 |
[32m[20230203 21:01:48 @agent_ppo2.py:193][0m |          -0.0192 |          16.1847 |           2.8851 |
[32m[20230203 21:01:48 @agent_ppo2.py:193][0m |          -0.0172 |          16.0030 |           2.8882 |
[32m[20230203 21:01:48 @agent_ppo2.py:193][0m |          -0.0365 |          16.3571 |           2.8868 |
[32m[20230203 21:01:48 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 21:01:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 257.54
[32m[20230203 21:01:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.71
[32m[20230203 21:01:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 266.29
[32m[20230203 21:01:48 @agent_ppo2.py:151][0m Total time:      11.22 min
[32m[20230203 21:01:48 @agent_ppo2.py:153][0m 972800 total steps have happened
[32m[20230203 21:01:48 @agent_ppo2.py:129][0m #------------------------ Iteration 475 --------------------------#
[32m[20230203 21:01:49 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 21:01:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:49 @agent_ppo2.py:193][0m |           0.0035 |          13.4649 |           2.9109 |
[32m[20230203 21:01:49 @agent_ppo2.py:193][0m |          -0.0039 |          12.5734 |           2.9074 |
[32m[20230203 21:01:49 @agent_ppo2.py:193][0m |          -0.0083 |          12.1145 |           2.9014 |
[32m[20230203 21:01:49 @agent_ppo2.py:193][0m |          -0.0095 |          11.8239 |           2.8992 |
[32m[20230203 21:01:49 @agent_ppo2.py:193][0m |          -0.0123 |          11.5306 |           2.8966 |
[32m[20230203 21:01:49 @agent_ppo2.py:193][0m |          -0.0123 |          11.3280 |           2.8945 |
[32m[20230203 21:01:49 @agent_ppo2.py:193][0m |          -0.0137 |          11.1388 |           2.8946 |
[32m[20230203 21:01:49 @agent_ppo2.py:193][0m |          -0.0138 |          10.9931 |           2.8922 |
[32m[20230203 21:01:49 @agent_ppo2.py:193][0m |          -0.0153 |          10.8615 |           2.8921 |
[32m[20230203 21:01:49 @agent_ppo2.py:193][0m |          -0.0149 |          10.8306 |           2.8903 |
[32m[20230203 21:01:49 @agent_ppo2.py:138][0m Policy update time: 0.44 s
[32m[20230203 21:01:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 255.79
[32m[20230203 21:01:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.32
[32m[20230203 21:01:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 171.33
[32m[20230203 21:01:49 @agent_ppo2.py:151][0m Total time:      11.24 min
[32m[20230203 21:01:49 @agent_ppo2.py:153][0m 974848 total steps have happened
[32m[20230203 21:01:49 @agent_ppo2.py:129][0m #------------------------ Iteration 476 --------------------------#
[32m[20230203 21:01:50 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 21:01:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:50 @agent_ppo2.py:193][0m |           0.0062 |          39.2489 |           3.0170 |
[32m[20230203 21:01:50 @agent_ppo2.py:193][0m |          -0.0046 |          23.1656 |           3.0073 |
[32m[20230203 21:01:50 @agent_ppo2.py:193][0m |          -0.0055 |          20.7378 |           3.0080 |
[32m[20230203 21:01:50 @agent_ppo2.py:193][0m |          -0.0046 |          19.6988 |           3.0065 |
[32m[20230203 21:01:50 @agent_ppo2.py:193][0m |          -0.0091 |          18.8110 |           3.0021 |
[32m[20230203 21:01:50 @agent_ppo2.py:193][0m |          -0.0159 |          17.5565 |           3.0059 |
[32m[20230203 21:01:50 @agent_ppo2.py:193][0m |          -0.0093 |          17.0131 |           3.0043 |
[32m[20230203 21:01:50 @agent_ppo2.py:193][0m |          -0.0112 |          16.5515 |           3.0028 |
[32m[20230203 21:01:50 @agent_ppo2.py:193][0m |          -0.0125 |          16.0070 |           3.0013 |
[32m[20230203 21:01:50 @agent_ppo2.py:193][0m |          -0.0161 |          15.5990 |           3.0016 |
[32m[20230203 21:01:50 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 21:01:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 82.88
[32m[20230203 21:01:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 258.43
[32m[20230203 21:01:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.76
[32m[20230203 21:01:51 @agent_ppo2.py:151][0m Total time:      11.26 min
[32m[20230203 21:01:51 @agent_ppo2.py:153][0m 976896 total steps have happened
[32m[20230203 21:01:51 @agent_ppo2.py:129][0m #------------------------ Iteration 477 --------------------------#
[32m[20230203 21:01:51 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:01:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:51 @agent_ppo2.py:193][0m |          -0.0003 |          13.6885 |           2.9557 |
[32m[20230203 21:01:51 @agent_ppo2.py:193][0m |          -0.0069 |          13.0200 |           2.9527 |
[32m[20230203 21:01:51 @agent_ppo2.py:193][0m |          -0.0108 |          12.6183 |           2.9568 |
[32m[20230203 21:01:51 @agent_ppo2.py:193][0m |          -0.0115 |          12.4242 |           2.9576 |
[32m[20230203 21:01:51 @agent_ppo2.py:193][0m |          -0.0128 |          12.2638 |           2.9568 |
[32m[20230203 21:01:51 @agent_ppo2.py:193][0m |          -0.0108 |          12.2869 |           2.9570 |
[32m[20230203 21:01:51 @agent_ppo2.py:193][0m |          -0.0106 |          12.1939 |           2.9597 |
[32m[20230203 21:01:52 @agent_ppo2.py:193][0m |          -0.0125 |          11.9500 |           2.9577 |
[32m[20230203 21:01:52 @agent_ppo2.py:193][0m |          -0.0152 |          11.8442 |           2.9580 |
[32m[20230203 21:01:52 @agent_ppo2.py:193][0m |          -0.0159 |          11.7265 |           2.9584 |
[32m[20230203 21:01:52 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 21:01:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.44
[32m[20230203 21:01:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.06
[32m[20230203 21:01:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 188.45
[32m[20230203 21:01:52 @agent_ppo2.py:151][0m Total time:      11.28 min
[32m[20230203 21:01:52 @agent_ppo2.py:153][0m 978944 total steps have happened
[32m[20230203 21:01:52 @agent_ppo2.py:129][0m #------------------------ Iteration 478 --------------------------#
[32m[20230203 21:01:52 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 21:01:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:52 @agent_ppo2.py:193][0m |          -0.0039 |          13.4622 |           2.8276 |
[32m[20230203 21:01:52 @agent_ppo2.py:193][0m |          -0.0090 |          12.9968 |           2.8228 |
[32m[20230203 21:01:53 @agent_ppo2.py:193][0m |          -0.0009 |          13.1346 |           2.8170 |
[32m[20230203 21:01:53 @agent_ppo2.py:193][0m |          -0.0146 |          12.5980 |           2.8133 |
[32m[20230203 21:01:53 @agent_ppo2.py:193][0m |          -0.0162 |          12.4581 |           2.8163 |
[32m[20230203 21:01:53 @agent_ppo2.py:193][0m |          -0.0139 |          12.3378 |           2.8140 |
[32m[20230203 21:01:53 @agent_ppo2.py:193][0m |          -0.0158 |          12.2433 |           2.8087 |
[32m[20230203 21:01:53 @agent_ppo2.py:193][0m |          -0.0175 |          12.1732 |           2.8112 |
[32m[20230203 21:01:53 @agent_ppo2.py:193][0m |          -0.0098 |          12.3000 |           2.8097 |
[32m[20230203 21:01:53 @agent_ppo2.py:193][0m |          -0.0202 |          12.0007 |           2.8097 |
[32m[20230203 21:01:53 @agent_ppo2.py:138][0m Policy update time: 0.44 s
[32m[20230203 21:01:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 257.59
[32m[20230203 21:01:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.19
[32m[20230203 21:01:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.40
[32m[20230203 21:01:53 @agent_ppo2.py:151][0m Total time:      11.30 min
[32m[20230203 21:01:53 @agent_ppo2.py:153][0m 980992 total steps have happened
[32m[20230203 21:01:53 @agent_ppo2.py:129][0m #------------------------ Iteration 479 --------------------------#
[32m[20230203 21:01:54 @agent_ppo2.py:135][0m Sampling time: 0.52 s by 1 slaves
[32m[20230203 21:01:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:54 @agent_ppo2.py:193][0m |          -0.0017 |          51.1123 |           2.9240 |
[32m[20230203 21:01:54 @agent_ppo2.py:193][0m |          -0.0115 |          32.0338 |           2.9219 |
[32m[20230203 21:01:54 @agent_ppo2.py:193][0m |          -0.0127 |          29.0106 |           2.9202 |
[32m[20230203 21:01:54 @agent_ppo2.py:193][0m |          -0.0094 |          27.1399 |           2.9223 |
[32m[20230203 21:01:54 @agent_ppo2.py:193][0m |          -0.0114 |          25.4049 |           2.9222 |
[32m[20230203 21:01:54 @agent_ppo2.py:193][0m |          -0.0159 |          24.5646 |           2.9243 |
[32m[20230203 21:01:54 @agent_ppo2.py:193][0m |          -0.0130 |          23.9984 |           2.9243 |
[32m[20230203 21:01:54 @agent_ppo2.py:193][0m |          -0.0165 |          23.5257 |           2.9271 |
[32m[20230203 21:01:54 @agent_ppo2.py:193][0m |          -0.0030 |          23.4507 |           2.9238 |
[32m[20230203 21:01:54 @agent_ppo2.py:193][0m |          -0.0175 |          22.8516 |           2.9268 |
[32m[20230203 21:01:54 @agent_ppo2.py:138][0m Policy update time: 0.60 s
[32m[20230203 21:01:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 69.66
[32m[20230203 21:01:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.47
[32m[20230203 21:01:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 106.13
[32m[20230203 21:01:55 @agent_ppo2.py:151][0m Total time:      11.32 min
[32m[20230203 21:01:55 @agent_ppo2.py:153][0m 983040 total steps have happened
[32m[20230203 21:01:55 @agent_ppo2.py:129][0m #------------------------ Iteration 480 --------------------------#
[32m[20230203 21:01:55 @agent_ppo2.py:135][0m Sampling time: 0.46 s by 1 slaves
[32m[20230203 21:01:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:55 @agent_ppo2.py:193][0m |           0.0002 |          53.7757 |           3.0595 |
[32m[20230203 21:01:55 @agent_ppo2.py:193][0m |          -0.0053 |          32.9269 |           3.0537 |
[32m[20230203 21:01:55 @agent_ppo2.py:193][0m |          -0.0099 |          30.2957 |           3.0565 |
[32m[20230203 21:01:55 @agent_ppo2.py:193][0m |          -0.0119 |          28.2846 |           3.0526 |
[32m[20230203 21:01:55 @agent_ppo2.py:193][0m |          -0.0144 |          28.0521 |           3.0521 |
[32m[20230203 21:01:55 @agent_ppo2.py:193][0m |          -0.0131 |          25.9666 |           3.0543 |
[32m[20230203 21:01:55 @agent_ppo2.py:193][0m |          -0.0162 |          25.1286 |           3.0539 |
[32m[20230203 21:01:55 @agent_ppo2.py:193][0m |          -0.0165 |          24.9349 |           3.0525 |
[32m[20230203 21:01:56 @agent_ppo2.py:193][0m |          -0.0173 |          23.8751 |           3.0542 |
[32m[20230203 21:01:56 @agent_ppo2.py:193][0m |          -0.0177 |          23.4048 |           3.0530 |
[32m[20230203 21:01:56 @agent_ppo2.py:138][0m Policy update time: 0.52 s
[32m[20230203 21:01:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 37.65
[32m[20230203 21:01:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.05
[32m[20230203 21:01:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.47
[32m[20230203 21:01:56 @agent_ppo2.py:151][0m Total time:      11.34 min
[32m[20230203 21:01:56 @agent_ppo2.py:153][0m 985088 total steps have happened
[32m[20230203 21:01:56 @agent_ppo2.py:129][0m #------------------------ Iteration 481 --------------------------#
[32m[20230203 21:01:56 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 1 slaves
[32m[20230203 21:01:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:56 @agent_ppo2.py:193][0m |          -0.0051 |          61.3810 |           2.8981 |
[32m[20230203 21:01:57 @agent_ppo2.py:193][0m |          -0.0123 |          47.3531 |           2.8950 |
[32m[20230203 21:01:57 @agent_ppo2.py:193][0m |          -0.0090 |          43.5884 |           2.8942 |
[32m[20230203 21:01:57 @agent_ppo2.py:193][0m |          -0.0105 |          40.5910 |           2.8935 |
[32m[20230203 21:01:57 @agent_ppo2.py:193][0m |          -0.0179 |          39.4680 |           2.8922 |
[32m[20230203 21:01:57 @agent_ppo2.py:193][0m |          -0.0201 |          37.9117 |           2.8903 |
[32m[20230203 21:01:57 @agent_ppo2.py:193][0m |          -0.0265 |          37.3817 |           2.8886 |
[32m[20230203 21:01:57 @agent_ppo2.py:193][0m |          -0.0142 |          36.1506 |           2.8886 |
[32m[20230203 21:01:57 @agent_ppo2.py:193][0m |          -0.0241 |          35.9950 |           2.8875 |
[32m[20230203 21:01:57 @agent_ppo2.py:193][0m |          -0.0233 |          34.8034 |           2.8891 |
[32m[20230203 21:01:57 @agent_ppo2.py:138][0m Policy update time: 0.54 s
[32m[20230203 21:01:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 128.85
[32m[20230203 21:01:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 258.97
[32m[20230203 21:01:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.56
[32m[20230203 21:01:57 @agent_ppo2.py:151][0m Total time:      11.37 min
[32m[20230203 21:01:57 @agent_ppo2.py:153][0m 987136 total steps have happened
[32m[20230203 21:01:57 @agent_ppo2.py:129][0m #------------------------ Iteration 482 --------------------------#
[32m[20230203 21:01:58 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 21:01:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:58 @agent_ppo2.py:193][0m |          -0.0003 |          18.5680 |           2.9558 |
[32m[20230203 21:01:58 @agent_ppo2.py:193][0m |          -0.0071 |          14.2518 |           2.9522 |
[32m[20230203 21:01:58 @agent_ppo2.py:193][0m |          -0.0107 |          13.5082 |           2.9509 |
[32m[20230203 21:01:58 @agent_ppo2.py:193][0m |          -0.0133 |          12.9959 |           2.9518 |
[32m[20230203 21:01:58 @agent_ppo2.py:193][0m |          -0.0149 |          12.7482 |           2.9520 |
[32m[20230203 21:01:58 @agent_ppo2.py:193][0m |          -0.0154 |          12.6115 |           2.9498 |
[32m[20230203 21:01:58 @agent_ppo2.py:193][0m |          -0.0204 |          12.4689 |           2.9507 |
[32m[20230203 21:01:58 @agent_ppo2.py:193][0m |          -0.0171 |          12.3087 |           2.9518 |
[32m[20230203 21:01:58 @agent_ppo2.py:193][0m |          -0.0141 |          12.6355 |           2.9515 |
[32m[20230203 21:01:58 @agent_ppo2.py:193][0m |          -0.0185 |          12.0335 |           2.9506 |
[32m[20230203 21:01:58 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 21:01:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 257.35
[32m[20230203 21:01:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.66
[32m[20230203 21:01:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.23
[32m[20230203 21:01:59 @agent_ppo2.py:151][0m Total time:      11.39 min
[32m[20230203 21:01:59 @agent_ppo2.py:153][0m 989184 total steps have happened
[32m[20230203 21:01:59 @agent_ppo2.py:129][0m #------------------------ Iteration 483 --------------------------#
[32m[20230203 21:01:59 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 21:01:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:01:59 @agent_ppo2.py:193][0m |          -0.0005 |          37.4859 |           2.9471 |
[32m[20230203 21:01:59 @agent_ppo2.py:193][0m |          -0.0032 |          23.6349 |           2.9396 |
[32m[20230203 21:01:59 @agent_ppo2.py:193][0m |          -0.0073 |          19.5565 |           2.9384 |
[32m[20230203 21:01:59 @agent_ppo2.py:193][0m |          -0.0086 |          18.2269 |           2.9363 |
[32m[20230203 21:01:59 @agent_ppo2.py:193][0m |          -0.0101 |          17.6222 |           2.9374 |
[32m[20230203 21:01:59 @agent_ppo2.py:193][0m |          -0.0083 |          17.3993 |           2.9342 |
[32m[20230203 21:01:59 @agent_ppo2.py:193][0m |          -0.0100 |          16.7755 |           2.9341 |
[32m[20230203 21:01:59 @agent_ppo2.py:193][0m |          -0.0123 |          16.2414 |           2.9322 |
[32m[20230203 21:01:59 @agent_ppo2.py:193][0m |          -0.0110 |          16.1484 |           2.9316 |
[32m[20230203 21:01:59 @agent_ppo2.py:193][0m |          -0.0146 |          15.5897 |           2.9302 |
[32m[20230203 21:01:59 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 21:02:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 137.62
[32m[20230203 21:02:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.89
[32m[20230203 21:02:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.54
[32m[20230203 21:02:00 @agent_ppo2.py:151][0m Total time:      11.41 min
[32m[20230203 21:02:00 @agent_ppo2.py:153][0m 991232 total steps have happened
[32m[20230203 21:02:00 @agent_ppo2.py:129][0m #------------------------ Iteration 484 --------------------------#
[32m[20230203 21:02:00 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 1 slaves
[32m[20230203 21:02:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:00 @agent_ppo2.py:193][0m |           0.0001 |          47.9844 |           2.9096 |
[32m[20230203 21:02:00 @agent_ppo2.py:193][0m |          -0.0099 |          31.3041 |           2.9050 |
[32m[20230203 21:02:00 @agent_ppo2.py:193][0m |          -0.0117 |          29.8195 |           2.8967 |
[32m[20230203 21:02:00 @agent_ppo2.py:193][0m |          -0.0141 |          28.2741 |           2.8932 |
[32m[20230203 21:02:00 @agent_ppo2.py:193][0m |          -0.0149 |          27.2148 |           2.8929 |
[32m[20230203 21:02:00 @agent_ppo2.py:193][0m |          -0.0160 |          26.4503 |           2.8901 |
[32m[20230203 21:02:00 @agent_ppo2.py:193][0m |          -0.0181 |          25.8374 |           2.8900 |
[32m[20230203 21:02:00 @agent_ppo2.py:193][0m |          -0.0173 |          25.6260 |           2.8897 |
[32m[20230203 21:02:01 @agent_ppo2.py:193][0m |          -0.0179 |          25.1055 |           2.8879 |
[32m[20230203 21:02:01 @agent_ppo2.py:193][0m |          -0.0188 |          25.1703 |           2.8865 |
[32m[20230203 21:02:01 @agent_ppo2.py:138][0m Policy update time: 0.49 s
[32m[20230203 21:02:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 78.72
[32m[20230203 21:02:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.57
[32m[20230203 21:02:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.75
[32m[20230203 21:02:01 @agent_ppo2.py:151][0m Total time:      11.43 min
[32m[20230203 21:02:01 @agent_ppo2.py:153][0m 993280 total steps have happened
[32m[20230203 21:02:01 @agent_ppo2.py:129][0m #------------------------ Iteration 485 --------------------------#
[32m[20230203 21:02:01 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 1 slaves
[32m[20230203 21:02:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:01 @agent_ppo2.py:193][0m |           0.0014 |          14.5835 |           3.0054 |
[32m[20230203 21:02:01 @agent_ppo2.py:193][0m |          -0.0051 |          14.0478 |           2.9949 |
[32m[20230203 21:02:01 @agent_ppo2.py:193][0m |          -0.0088 |          13.6732 |           2.9902 |
[32m[20230203 21:02:01 @agent_ppo2.py:193][0m |          -0.0079 |          13.6957 |           2.9886 |
[32m[20230203 21:02:02 @agent_ppo2.py:193][0m |          -0.0109 |          13.6497 |           2.9901 |
[32m[20230203 21:02:02 @agent_ppo2.py:193][0m |          -0.0115 |          13.3827 |           2.9922 |
[32m[20230203 21:02:02 @agent_ppo2.py:193][0m |          -0.0116 |          13.2354 |           2.9930 |
[32m[20230203 21:02:02 @agent_ppo2.py:193][0m |          -0.0100 |          13.5810 |           2.9927 |
[32m[20230203 21:02:02 @agent_ppo2.py:193][0m |          -0.0132 |          13.1175 |           2.9958 |
[32m[20230203 21:02:02 @agent_ppo2.py:193][0m |          -0.0138 |          13.1111 |           2.9938 |
[32m[20230203 21:02:02 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 21:02:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 257.39
[32m[20230203 21:02:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.63
[32m[20230203 21:02:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 216.04
[32m[20230203 21:02:02 @agent_ppo2.py:151][0m Total time:      11.45 min
[32m[20230203 21:02:02 @agent_ppo2.py:153][0m 995328 total steps have happened
[32m[20230203 21:02:02 @agent_ppo2.py:129][0m #------------------------ Iteration 486 --------------------------#
[32m[20230203 21:02:03 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 21:02:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:03 @agent_ppo2.py:193][0m |           0.0006 |          13.9998 |           2.9756 |
[32m[20230203 21:02:03 @agent_ppo2.py:193][0m |          -0.0079 |          13.5467 |           2.9700 |
[32m[20230203 21:02:03 @agent_ppo2.py:193][0m |          -0.0082 |          13.5190 |           2.9701 |
[32m[20230203 21:02:03 @agent_ppo2.py:193][0m |          -0.0109 |          13.1227 |           2.9662 |
[32m[20230203 21:02:03 @agent_ppo2.py:193][0m |          -0.0121 |          12.8724 |           2.9641 |
[32m[20230203 21:02:03 @agent_ppo2.py:193][0m |          -0.0124 |          12.6701 |           2.9640 |
[32m[20230203 21:02:03 @agent_ppo2.py:193][0m |          -0.0135 |          12.4331 |           2.9686 |
[32m[20230203 21:02:03 @agent_ppo2.py:193][0m |          -0.0122 |          12.2302 |           2.9661 |
[32m[20230203 21:02:03 @agent_ppo2.py:193][0m |          -0.0143 |          11.7778 |           2.9675 |
[32m[20230203 21:02:03 @agent_ppo2.py:193][0m |          -0.0152 |          11.4436 |           2.9665 |
[32m[20230203 21:02:03 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 21:02:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.89
[32m[20230203 21:02:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.03
[32m[20230203 21:02:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 212.99
[32m[20230203 21:02:03 @agent_ppo2.py:151][0m Total time:      11.47 min
[32m[20230203 21:02:03 @agent_ppo2.py:153][0m 997376 total steps have happened
[32m[20230203 21:02:03 @agent_ppo2.py:129][0m #------------------------ Iteration 487 --------------------------#
[32m[20230203 21:02:04 @agent_ppo2.py:135][0m Sampling time: 0.45 s by 1 slaves
[32m[20230203 21:02:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:04 @agent_ppo2.py:193][0m |          -0.0037 |          24.1455 |           2.9212 |
[32m[20230203 21:02:04 @agent_ppo2.py:193][0m |          -0.0069 |          19.1162 |           2.9194 |
[32m[20230203 21:02:04 @agent_ppo2.py:193][0m |          -0.0007 |          17.7661 |           2.9170 |
[32m[20230203 21:02:04 @agent_ppo2.py:193][0m |          -0.0123 |          16.7852 |           2.9163 |
[32m[20230203 21:02:04 @agent_ppo2.py:193][0m |          -0.0057 |          16.2136 |           2.9180 |
[32m[20230203 21:02:04 @agent_ppo2.py:193][0m |          -0.0138 |          15.8441 |           2.9128 |
[32m[20230203 21:02:04 @agent_ppo2.py:193][0m |          -0.0086 |          15.3873 |           2.9139 |
[32m[20230203 21:02:04 @agent_ppo2.py:193][0m |          -0.0099 |          15.0152 |           2.9124 |
[32m[20230203 21:02:04 @agent_ppo2.py:193][0m |          -0.0162 |          14.6062 |           2.9107 |
[32m[20230203 21:02:04 @agent_ppo2.py:193][0m |          -0.0120 |          15.3352 |           2.9099 |
[32m[20230203 21:02:04 @agent_ppo2.py:138][0m Policy update time: 0.51 s
[32m[20230203 21:02:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 158.55
[32m[20230203 21:02:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.63
[32m[20230203 21:02:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.37
[32m[20230203 21:02:05 @agent_ppo2.py:151][0m Total time:      11.49 min
[32m[20230203 21:02:05 @agent_ppo2.py:153][0m 999424 total steps have happened
[32m[20230203 21:02:05 @agent_ppo2.py:129][0m #------------------------ Iteration 488 --------------------------#
[32m[20230203 21:02:05 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 21:02:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:05 @agent_ppo2.py:193][0m |          -0.0091 |          72.7569 |           2.8196 |
[32m[20230203 21:02:05 @agent_ppo2.py:193][0m |          -0.0133 |          58.1835 |           2.8147 |
[32m[20230203 21:02:05 @agent_ppo2.py:193][0m |          -0.0160 |          53.0664 |           2.8081 |
[32m[20230203 21:02:06 @agent_ppo2.py:193][0m |          -0.0049 |          53.3096 |           2.8078 |
[32m[20230203 21:02:06 @agent_ppo2.py:193][0m |          -0.0177 |          49.5931 |           2.8041 |
[32m[20230203 21:02:06 @agent_ppo2.py:193][0m |          -0.0052 |          48.9897 |           2.8046 |
[32m[20230203 21:02:06 @agent_ppo2.py:193][0m |          -0.0172 |          47.3419 |           2.8071 |
[32m[20230203 21:02:06 @agent_ppo2.py:193][0m |          -0.0220 |          45.8219 |           2.8063 |
[32m[20230203 21:02:06 @agent_ppo2.py:193][0m |          -0.0106 |          51.2654 |           2.8021 |
[32m[20230203 21:02:06 @agent_ppo2.py:193][0m |          -0.0098 |          45.2714 |           2.8041 |
[32m[20230203 21:02:06 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:02:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 53.82
[32m[20230203 21:02:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.16
[32m[20230203 21:02:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.66
[32m[20230203 21:02:06 @agent_ppo2.py:151][0m Total time:      11.52 min
[32m[20230203 21:02:06 @agent_ppo2.py:153][0m 1001472 total steps have happened
[32m[20230203 21:02:06 @agent_ppo2.py:129][0m #------------------------ Iteration 489 --------------------------#
[32m[20230203 21:02:07 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 1 slaves
[32m[20230203 21:02:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:07 @agent_ppo2.py:193][0m |          -0.0032 |          46.9183 |           2.9635 |
[32m[20230203 21:02:07 @agent_ppo2.py:193][0m |          -0.0078 |          31.6689 |           2.9624 |
[32m[20230203 21:02:07 @agent_ppo2.py:193][0m |          -0.0127 |          28.6803 |           2.9611 |
[32m[20230203 21:02:07 @agent_ppo2.py:193][0m |          -0.0143 |          26.6583 |           2.9605 |
[32m[20230203 21:02:07 @agent_ppo2.py:193][0m |          -0.0161 |          25.7807 |           2.9618 |
[32m[20230203 21:02:07 @agent_ppo2.py:193][0m |          -0.0163 |          25.2091 |           2.9612 |
[32m[20230203 21:02:07 @agent_ppo2.py:193][0m |          -0.0150 |          24.7257 |           2.9610 |
[32m[20230203 21:02:07 @agent_ppo2.py:193][0m |          -0.0165 |          24.2825 |           2.9634 |
[32m[20230203 21:02:07 @agent_ppo2.py:193][0m |          -0.0193 |          23.9335 |           2.9613 |
[32m[20230203 21:02:07 @agent_ppo2.py:193][0m |          -0.0194 |          23.7884 |           2.9605 |
[32m[20230203 21:02:07 @agent_ppo2.py:138][0m Policy update time: 0.46 s
[32m[20230203 21:02:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 137.74
[32m[20230203 21:02:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.30
[32m[20230203 21:02:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 186.37
[32m[20230203 21:02:08 @agent_ppo2.py:151][0m Total time:      11.54 min
[32m[20230203 21:02:08 @agent_ppo2.py:153][0m 1003520 total steps have happened
[32m[20230203 21:02:08 @agent_ppo2.py:129][0m #------------------------ Iteration 490 --------------------------#
[32m[20230203 21:02:08 @agent_ppo2.py:135][0m Sampling time: 0.41 s by 1 slaves
[32m[20230203 21:02:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:08 @agent_ppo2.py:193][0m |          -0.0008 |          41.1833 |           2.8970 |
[32m[20230203 21:02:08 @agent_ppo2.py:193][0m |          -0.0039 |          24.0963 |           2.8943 |
[32m[20230203 21:02:08 @agent_ppo2.py:193][0m |          -0.0104 |          20.6394 |           2.8909 |
[32m[20230203 21:02:08 @agent_ppo2.py:193][0m |          -0.0113 |          19.6719 |           2.8918 |
[32m[20230203 21:02:08 @agent_ppo2.py:193][0m |          -0.0145 |          18.7203 |           2.8892 |
[32m[20230203 21:02:08 @agent_ppo2.py:193][0m |          -0.0110 |          18.0632 |           2.8877 |
[32m[20230203 21:02:08 @agent_ppo2.py:193][0m |          -0.0148 |          17.5123 |           2.8858 |
[32m[20230203 21:02:08 @agent_ppo2.py:193][0m |          -0.0120 |          17.5629 |           2.8872 |
[32m[20230203 21:02:08 @agent_ppo2.py:193][0m |          -0.0149 |          16.6067 |           2.8871 |
[32m[20230203 21:02:08 @agent_ppo2.py:193][0m |          -0.0171 |          16.3056 |           2.8855 |
[32m[20230203 21:02:08 @agent_ppo2.py:138][0m Policy update time: 0.47 s
[32m[20230203 21:02:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 114.17
[32m[20230203 21:02:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.10
[32m[20230203 21:02:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 180.39
[32m[20230203 21:02:09 @agent_ppo2.py:151][0m Total time:      11.56 min
[32m[20230203 21:02:09 @agent_ppo2.py:153][0m 1005568 total steps have happened
[32m[20230203 21:02:09 @agent_ppo2.py:129][0m #------------------------ Iteration 491 --------------------------#
[32m[20230203 21:02:09 @agent_ppo2.py:135][0m Sampling time: 0.36 s by 1 slaves
[32m[20230203 21:02:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:09 @agent_ppo2.py:193][0m |           0.0010 |          53.2958 |           2.9926 |
[32m[20230203 21:02:09 @agent_ppo2.py:193][0m |          -0.0050 |          30.4348 |           2.9937 |
[32m[20230203 21:02:09 @agent_ppo2.py:193][0m |          -0.0071 |          21.7623 |           2.9889 |
[32m[20230203 21:02:09 @agent_ppo2.py:193][0m |          -0.0061 |          19.4171 |           2.9924 |
[32m[20230203 21:02:09 @agent_ppo2.py:193][0m |          -0.0090 |          18.3012 |           2.9891 |
[32m[20230203 21:02:09 @agent_ppo2.py:193][0m |          -0.0120 |          17.4912 |           2.9922 |
[32m[20230203 21:02:10 @agent_ppo2.py:193][0m |          -0.0119 |          16.6955 |           2.9915 |
[32m[20230203 21:02:10 @agent_ppo2.py:193][0m |          -0.0108 |          16.0221 |           2.9911 |
[32m[20230203 21:02:10 @agent_ppo2.py:193][0m |          -0.0163 |          15.7284 |           2.9915 |
[32m[20230203 21:02:10 @agent_ppo2.py:193][0m |          -0.0129 |          15.3205 |           2.9920 |
[32m[20230203 21:02:10 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 21:02:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 260.39
[32m[20230203 21:02:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.56
[32m[20230203 21:02:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.56
[32m[20230203 21:02:10 @agent_ppo2.py:151][0m Total time:      11.58 min
[32m[20230203 21:02:10 @agent_ppo2.py:153][0m 1007616 total steps have happened
[32m[20230203 21:02:10 @agent_ppo2.py:129][0m #------------------------ Iteration 492 --------------------------#
[32m[20230203 21:02:10 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 21:02:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:11 @agent_ppo2.py:193][0m |          -0.0106 |          17.3098 |           2.9115 |
[32m[20230203 21:02:11 @agent_ppo2.py:193][0m |          -0.0065 |          13.9778 |           2.9081 |
[32m[20230203 21:02:11 @agent_ppo2.py:193][0m |          -0.0007 |          14.2118 |           2.9065 |
[32m[20230203 21:02:11 @agent_ppo2.py:193][0m |          -0.0108 |          12.9176 |           2.8999 |
[32m[20230203 21:02:11 @agent_ppo2.py:193][0m |          -0.0143 |          12.9040 |           2.9009 |
[32m[20230203 21:02:11 @agent_ppo2.py:193][0m |          -0.0070 |          13.2806 |           2.9013 |
[32m[20230203 21:02:11 @agent_ppo2.py:193][0m |          -0.0070 |          12.6682 |           2.9000 |
[32m[20230203 21:02:11 @agent_ppo2.py:193][0m |          -0.0150 |          12.0699 |           2.9016 |
[32m[20230203 21:02:11 @agent_ppo2.py:193][0m |          -0.0144 |          11.9489 |           2.9028 |
[32m[20230203 21:02:11 @agent_ppo2.py:193][0m |          -0.0115 |          12.0629 |           2.9006 |
[32m[20230203 21:02:11 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 21:02:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.40
[32m[20230203 21:02:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.95
[32m[20230203 21:02:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.10
[32m[20230203 21:02:11 @agent_ppo2.py:151][0m Total time:      11.60 min
[32m[20230203 21:02:11 @agent_ppo2.py:153][0m 1009664 total steps have happened
[32m[20230203 21:02:11 @agent_ppo2.py:129][0m #------------------------ Iteration 493 --------------------------#
[32m[20230203 21:02:12 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 1 slaves
[32m[20230203 21:02:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:12 @agent_ppo2.py:193][0m |           0.0015 |          31.9111 |           3.0125 |
[32m[20230203 21:02:12 @agent_ppo2.py:193][0m |          -0.0049 |          23.5230 |           3.0121 |
[32m[20230203 21:02:12 @agent_ppo2.py:193][0m |          -0.0071 |          22.0918 |           3.0082 |
[32m[20230203 21:02:12 @agent_ppo2.py:193][0m |          -0.0087 |          21.5606 |           3.0120 |
[32m[20230203 21:02:12 @agent_ppo2.py:193][0m |          -0.0100 |          21.2174 |           3.0126 |
[32m[20230203 21:02:12 @agent_ppo2.py:193][0m |          -0.0086 |          21.2848 |           3.0109 |
[32m[20230203 21:02:12 @agent_ppo2.py:193][0m |          -0.0093 |          20.7417 |           3.0138 |
[32m[20230203 21:02:12 @agent_ppo2.py:193][0m |          -0.0108 |          20.6090 |           3.0123 |
[32m[20230203 21:02:12 @agent_ppo2.py:193][0m |          -0.0125 |          20.0914 |           3.0112 |
[32m[20230203 21:02:12 @agent_ppo2.py:193][0m |          -0.0107 |          19.5670 |           3.0149 |
[32m[20230203 21:02:12 @agent_ppo2.py:138][0m Policy update time: 0.45 s
[32m[20230203 21:02:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 136.81
[32m[20230203 21:02:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.00
[32m[20230203 21:02:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.30
[32m[20230203 21:02:13 @agent_ppo2.py:151][0m Total time:      11.62 min
[32m[20230203 21:02:13 @agent_ppo2.py:153][0m 1011712 total steps have happened
[32m[20230203 21:02:13 @agent_ppo2.py:129][0m #------------------------ Iteration 494 --------------------------#
[32m[20230203 21:02:13 @agent_ppo2.py:135][0m Sampling time: 0.53 s by 1 slaves
[32m[20230203 21:02:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:13 @agent_ppo2.py:193][0m |          -0.0041 |          27.3937 |           3.0468 |
[32m[20230203 21:02:13 @agent_ppo2.py:193][0m |          -0.0102 |          16.2736 |           3.0445 |
[32m[20230203 21:02:13 @agent_ppo2.py:193][0m |          -0.0131 |          14.2579 |           3.0431 |
[32m[20230203 21:02:14 @agent_ppo2.py:193][0m |          -0.0147 |          13.3357 |           3.0438 |
[32m[20230203 21:02:14 @agent_ppo2.py:193][0m |          -0.0149 |          12.4088 |           3.0427 |
[32m[20230203 21:02:14 @agent_ppo2.py:193][0m |          -0.0176 |          11.7142 |           3.0404 |
[32m[20230203 21:02:14 @agent_ppo2.py:193][0m |          -0.0187 |          11.2354 |           3.0382 |
[32m[20230203 21:02:14 @agent_ppo2.py:193][0m |          -0.0183 |          10.5841 |           3.0387 |
[32m[20230203 21:02:14 @agent_ppo2.py:193][0m |          -0.0172 |          10.2241 |           3.0366 |
[32m[20230203 21:02:14 @agent_ppo2.py:193][0m |          -0.0171 |           9.9875 |           3.0380 |
[32m[20230203 21:02:14 @agent_ppo2.py:138][0m Policy update time: 0.59 s
[32m[20230203 21:02:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 204.89
[32m[20230203 21:02:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.05
[32m[20230203 21:02:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 139.57
[32m[20230203 21:02:14 @agent_ppo2.py:151][0m Total time:      11.65 min
[32m[20230203 21:02:14 @agent_ppo2.py:153][0m 1013760 total steps have happened
[32m[20230203 21:02:14 @agent_ppo2.py:129][0m #------------------------ Iteration 495 --------------------------#
[32m[20230203 21:02:15 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 21:02:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:15 @agent_ppo2.py:193][0m |           0.0149 |          17.3671 |           3.0199 |
[32m[20230203 21:02:15 @agent_ppo2.py:193][0m |          -0.0011 |          14.8524 |           3.0141 |
[32m[20230203 21:02:15 @agent_ppo2.py:193][0m |          -0.0128 |          14.3267 |           3.0057 |
[32m[20230203 21:02:15 @agent_ppo2.py:193][0m |          -0.0135 |          14.1213 |           3.0021 |
[32m[20230203 21:02:15 @agent_ppo2.py:193][0m |          -0.0136 |          13.8882 |           3.0007 |
[32m[20230203 21:02:15 @agent_ppo2.py:193][0m |          -0.0107 |          13.8257 |           3.0012 |
[32m[20230203 21:02:15 @agent_ppo2.py:193][0m |          -0.0156 |          13.5918 |           2.9993 |
[32m[20230203 21:02:15 @agent_ppo2.py:193][0m |          -0.0192 |          13.5333 |           2.9967 |
[32m[20230203 21:02:15 @agent_ppo2.py:193][0m |          -0.0204 |          13.4518 |           2.9957 |
[32m[20230203 21:02:15 @agent_ppo2.py:193][0m |          -0.0036 |          16.1431 |           2.9975 |
[32m[20230203 21:02:15 @agent_ppo2.py:138][0m Policy update time: 0.43 s
[32m[20230203 21:02:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.10
[32m[20230203 21:02:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.08
[32m[20230203 21:02:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.72
[32m[20230203 21:02:15 @agent_ppo2.py:151][0m Total time:      11.67 min
[32m[20230203 21:02:15 @agent_ppo2.py:153][0m 1015808 total steps have happened
[32m[20230203 21:02:15 @agent_ppo2.py:129][0m #------------------------ Iteration 496 --------------------------#
[32m[20230203 21:02:16 @agent_ppo2.py:135][0m Sampling time: 0.36 s by 1 slaves
[32m[20230203 21:02:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:16 @agent_ppo2.py:193][0m |           0.0020 |          15.7391 |           2.9429 |
[32m[20230203 21:02:16 @agent_ppo2.py:193][0m |          -0.0080 |          14.9489 |           2.9367 |
[32m[20230203 21:02:16 @agent_ppo2.py:193][0m |          -0.0129 |          14.6328 |           2.9386 |
[32m[20230203 21:02:16 @agent_ppo2.py:193][0m |          -0.0137 |          14.4896 |           2.9374 |
[32m[20230203 21:02:16 @agent_ppo2.py:193][0m |          -0.0060 |          15.2115 |           2.9323 |
[32m[20230203 21:02:16 @agent_ppo2.py:193][0m |          -0.0171 |          14.2408 |           2.9323 |
[32m[20230203 21:02:16 @agent_ppo2.py:193][0m |          -0.0149 |          14.1289 |           2.9345 |
[32m[20230203 21:02:16 @agent_ppo2.py:193][0m |          -0.0108 |          14.4356 |           2.9338 |
[32m[20230203 21:02:16 @agent_ppo2.py:193][0m |          -0.0171 |          13.9840 |           2.9358 |
[32m[20230203 21:02:16 @agent_ppo2.py:193][0m |          -0.0202 |          13.9384 |           2.9379 |
[32m[20230203 21:02:16 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 21:02:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.16
[32m[20230203 21:02:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.95
[32m[20230203 21:02:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.00
[32m[20230203 21:02:17 @agent_ppo2.py:151][0m Total time:      11.69 min
[32m[20230203 21:02:17 @agent_ppo2.py:153][0m 1017856 total steps have happened
[32m[20230203 21:02:17 @agent_ppo2.py:129][0m #------------------------ Iteration 497 --------------------------#
[32m[20230203 21:02:17 @agent_ppo2.py:135][0m Sampling time: 0.34 s by 1 slaves
[32m[20230203 21:02:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:17 @agent_ppo2.py:193][0m |          -0.0082 |          39.9441 |           3.0072 |
[32m[20230203 21:02:17 @agent_ppo2.py:193][0m |          -0.0096 |          31.1737 |           3.0101 |
[32m[20230203 21:02:17 @agent_ppo2.py:193][0m |          -0.0082 |          30.4004 |           3.0081 |
[32m[20230203 21:02:17 @agent_ppo2.py:193][0m |          -0.0147 |          26.8099 |           3.0083 |
[32m[20230203 21:02:17 @agent_ppo2.py:193][0m |          -0.0081 |          26.5183 |           3.0085 |
[32m[20230203 21:02:17 @agent_ppo2.py:193][0m |          -0.0131 |          25.5716 |           3.0064 |
[32m[20230203 21:02:17 @agent_ppo2.py:193][0m |          -0.0178 |          23.9656 |           3.0068 |
[32m[20230203 21:02:17 @agent_ppo2.py:193][0m |          -0.0190 |          25.7270 |           3.0080 |
[32m[20230203 21:02:17 @agent_ppo2.py:193][0m |          -0.0109 |          23.8099 |           3.0079 |
[32m[20230203 21:02:17 @agent_ppo2.py:193][0m |           0.0022 |          23.3720 |           3.0050 |
[32m[20230203 21:02:17 @agent_ppo2.py:138][0m Policy update time: 0.40 s
[32m[20230203 21:02:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: 80.28
[32m[20230203 21:02:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 258.97
[32m[20230203 21:02:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 212.44
[32m[20230203 21:02:18 @agent_ppo2.py:151][0m Total time:      11.71 min
[32m[20230203 21:02:18 @agent_ppo2.py:153][0m 1019904 total steps have happened
[32m[20230203 21:02:18 @agent_ppo2.py:129][0m #------------------------ Iteration 498 --------------------------#
[32m[20230203 21:02:18 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 21:02:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:18 @agent_ppo2.py:193][0m |           0.0020 |          20.5010 |           3.0163 |
[32m[20230203 21:02:18 @agent_ppo2.py:193][0m |          -0.0034 |          18.8228 |           3.0133 |
[32m[20230203 21:02:18 @agent_ppo2.py:193][0m |          -0.0056 |          18.1835 |           3.0080 |
[32m[20230203 21:02:18 @agent_ppo2.py:193][0m |          -0.0049 |          17.8042 |           3.0054 |
[32m[20230203 21:02:18 @agent_ppo2.py:193][0m |          -0.0066 |          17.5241 |           3.0027 |
[32m[20230203 21:02:19 @agent_ppo2.py:193][0m |          -0.0077 |          17.3629 |           3.0009 |
[32m[20230203 21:02:19 @agent_ppo2.py:193][0m |          -0.0085 |          17.0799 |           2.9997 |
[32m[20230203 21:02:19 @agent_ppo2.py:193][0m |          -0.0087 |          16.9799 |           2.9958 |
[32m[20230203 21:02:19 @agent_ppo2.py:193][0m |          -0.0107 |          16.7915 |           2.9952 |
[32m[20230203 21:02:19 @agent_ppo2.py:193][0m |          -0.0092 |          16.7906 |           2.9929 |
[32m[20230203 21:02:19 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 21:02:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 259.16
[32m[20230203 21:02:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.66
[32m[20230203 21:02:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 205.54
[32m[20230203 21:02:19 @agent_ppo2.py:151][0m Total time:      11.73 min
[32m[20230203 21:02:19 @agent_ppo2.py:153][0m 1021952 total steps have happened
[32m[20230203 21:02:19 @agent_ppo2.py:129][0m #------------------------ Iteration 499 --------------------------#
[32m[20230203 21:02:19 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 1 slaves
[32m[20230203 21:02:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230203 21:02:20 @agent_ppo2.py:193][0m |          -0.0003 |          14.7579 |           3.0405 |
[32m[20230203 21:02:20 @agent_ppo2.py:193][0m |          -0.0036 |          14.3705 |           3.0363 |
[32m[20230203 21:02:20 @agent_ppo2.py:193][0m |          -0.0080 |          13.9424 |           3.0296 |
[32m[20230203 21:02:20 @agent_ppo2.py:193][0m |          -0.0079 |          13.8386 |           3.0298 |
[32m[20230203 21:02:20 @agent_ppo2.py:193][0m |          -0.0094 |          13.6740 |           3.0291 |
[32m[20230203 21:02:20 @agent_ppo2.py:193][0m |          -0.0110 |          13.5643 |           3.0276 |
[32m[20230203 21:02:20 @agent_ppo2.py:193][0m |          -0.0103 |          13.5311 |           3.0265 |
[32m[20230203 21:02:20 @agent_ppo2.py:193][0m |          -0.0119 |          13.3666 |           3.0260 |
[32m[20230203 21:02:20 @agent_ppo2.py:193][0m |          -0.0121 |          13.4011 |           3.0271 |
[32m[20230203 21:02:20 @agent_ppo2.py:193][0m |          -0.0130 |          13.2053 |           3.0248 |
[32m[20230203 21:02:20 @agent_ppo2.py:138][0m Policy update time: 0.42 s
[32m[20230203 21:02:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.70
[32m[20230203 21:02:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 258.81
[32m[20230203 21:02:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.47
[32m[20230203 21:02:20 @agent_ppo2.py:108][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 271.25
[32m[20230203 21:02:20 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 271.47
[32m[20230203 21:02:20 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 271.47
[32m[20230203 21:02:20 @agent_ppo2.py:151][0m Total time:      11.75 min
[32m[20230203 21:02:20 @agent_ppo2.py:153][0m 1024000 total steps have happened
[32m[20230203 21:02:20 @train.py:59][0m [4m[34mCRITICAL[0m Training completed!
