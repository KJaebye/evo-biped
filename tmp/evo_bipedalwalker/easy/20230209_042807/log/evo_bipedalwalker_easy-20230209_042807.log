[32m[20230209 04:28:07 @logger.py:106][0m Log file set to ./tmp/evo_bipedalwalker/easy/20230209_042807/log/evo_bipedalwalker_easy-20230209_042807.log
[32m[20230209 04:28:07 @agent_ppo2.py:128][0m #------------------------ Iteration 0 --------------------------#
[32m[20230209 04:28:08 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230209 04:28:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:08 @agent_ppo2.py:193][0m |          -0.0106 |         532.1775 |           0.0640 |
[32m[20230209 04:28:08 @agent_ppo2.py:193][0m |          -0.0141 |         526.3191 |           0.0639 |
[32m[20230209 04:28:08 @agent_ppo2.py:193][0m |          -0.0139 |         509.4056 |           0.0639 |
[32m[20230209 04:28:08 @agent_ppo2.py:193][0m |          -0.0140 |         485.2385 |           0.0639 |
[32m[20230209 04:28:08 @agent_ppo2.py:193][0m |          -0.0138 |         463.4084 |           0.0639 |
[32m[20230209 04:28:08 @agent_ppo2.py:193][0m |          -0.0059 |         447.8267 |           0.0639 |
[32m[20230209 04:28:08 @agent_ppo2.py:193][0m |          -0.0148 |         433.0021 |           0.0639 |
[32m[20230209 04:28:08 @agent_ppo2.py:193][0m |          -0.0157 |         422.0639 |           0.0639 |
[32m[20230209 04:28:08 @agent_ppo2.py:193][0m |           0.0293 |         463.6733 |           0.0639 |
[32m[20230209 04:28:09 @agent_ppo2.py:193][0m |           0.0485 |         454.1964 |           0.0639 |
[32m[20230209 04:28:09 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230209 04:28:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: -122.69
[32m[20230209 04:28:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.02
[32m[20230209 04:28:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -93.08
[32m[20230209 04:28:09 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -93.08
[32m[20230209 04:28:09 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -93.08
[32m[20230209 04:28:09 @agent_ppo2.py:150][0m Total time:       0.03 min
[32m[20230209 04:28:09 @agent_ppo2.py:152][0m 2048 total steps have happened
[32m[20230209 04:28:09 @agent_ppo2.py:128][0m #------------------------ Iteration 1 --------------------------#
[32m[20230209 04:28:10 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230209 04:28:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:10 @agent_ppo2.py:193][0m |           0.0021 |         321.8024 |           0.0643 |
[32m[20230209 04:28:10 @agent_ppo2.py:193][0m |           0.0013 |         313.5805 |           0.0643 |
[32m[20230209 04:28:10 @agent_ppo2.py:193][0m |          -0.0015 |         304.7222 |           0.0643 |
[32m[20230209 04:28:10 @agent_ppo2.py:193][0m |          -0.0012 |         300.8019 |           0.0643 |
[32m[20230209 04:28:10 @agent_ppo2.py:193][0m |          -0.0034 |         294.9265 |           0.0643 |
[32m[20230209 04:28:10 @agent_ppo2.py:193][0m |          -0.0013 |         292.6462 |           0.0643 |
[32m[20230209 04:28:10 @agent_ppo2.py:193][0m |           0.0013 |         293.4717 |           0.0643 |
[32m[20230209 04:28:10 @agent_ppo2.py:193][0m |          -0.0011 |         289.3432 |           0.0642 |
[32m[20230209 04:28:10 @agent_ppo2.py:193][0m |          -0.0027 |         285.3209 |           0.0642 |
[32m[20230209 04:28:10 @agent_ppo2.py:193][0m |          -0.0032 |         283.1429 |           0.0642 |
[32m[20230209 04:28:10 @agent_ppo2.py:137][0m Policy update time: 0.58 s
[32m[20230209 04:28:11 @agent_ppo2.py:145][0m Average TRAINING episode reward: -123.00
[32m[20230209 04:28:11 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.67
[32m[20230209 04:28:11 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -91.76
[32m[20230209 04:28:11 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -91.76
[32m[20230209 04:28:11 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -91.76
[32m[20230209 04:28:11 @agent_ppo2.py:150][0m Total time:       0.06 min
[32m[20230209 04:28:11 @agent_ppo2.py:152][0m 4096 total steps have happened
[32m[20230209 04:28:11 @agent_ppo2.py:128][0m #------------------------ Iteration 2 --------------------------#
[32m[20230209 04:28:11 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230209 04:28:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:12 @agent_ppo2.py:193][0m |          -0.0007 |         197.4610 |           0.0637 |
[32m[20230209 04:28:12 @agent_ppo2.py:193][0m |          -0.0004 |         189.2228 |           0.0637 |
[32m[20230209 04:28:12 @agent_ppo2.py:193][0m |          -0.0023 |         185.5753 |           0.0637 |
[32m[20230209 04:28:12 @agent_ppo2.py:193][0m |           0.0003 |         185.8361 |           0.0637 |
[32m[20230209 04:28:12 @agent_ppo2.py:193][0m |          -0.0036 |         182.1984 |           0.0637 |
[32m[20230209 04:28:12 @agent_ppo2.py:193][0m |          -0.0037 |         180.8922 |           0.0637 |
[32m[20230209 04:28:12 @agent_ppo2.py:193][0m |          -0.0021 |         181.8670 |           0.0637 |
[32m[20230209 04:28:12 @agent_ppo2.py:193][0m |          -0.0039 |         180.1707 |           0.0637 |
[32m[20230209 04:28:12 @agent_ppo2.py:193][0m |          -0.0027 |         179.6483 |           0.0636 |
[32m[20230209 04:28:12 @agent_ppo2.py:193][0m |          -0.0032 |         180.3185 |           0.0636 |
[32m[20230209 04:28:12 @agent_ppo2.py:137][0m Policy update time: 0.58 s
[32m[20230209 04:28:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -117.62
[32m[20230209 04:28:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -106.31
[32m[20230209 04:28:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -91.18
[32m[20230209 04:28:13 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -91.18
[32m[20230209 04:28:13 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -91.18
[32m[20230209 04:28:13 @agent_ppo2.py:150][0m Total time:       0.09 min
[32m[20230209 04:28:13 @agent_ppo2.py:152][0m 6144 total steps have happened
[32m[20230209 04:28:13 @agent_ppo2.py:128][0m #------------------------ Iteration 3 --------------------------#
[32m[20230209 04:28:13 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230209 04:28:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:13 @agent_ppo2.py:193][0m |          -0.0039 |         162.3936 |           0.0629 |
[32m[20230209 04:28:13 @agent_ppo2.py:193][0m |           0.0094 |         161.8020 |           0.0629 |
[32m[20230209 04:28:13 @agent_ppo2.py:193][0m |          -0.0016 |         149.8939 |           0.0629 |
[32m[20230209 04:28:13 @agent_ppo2.py:193][0m |          -0.0054 |         143.8214 |           0.0629 |
[32m[20230209 04:28:14 @agent_ppo2.py:193][0m |          -0.0047 |         141.1234 |           0.0629 |
[32m[20230209 04:28:14 @agent_ppo2.py:193][0m |          -0.0043 |         140.4135 |           0.0629 |
[32m[20230209 04:28:14 @agent_ppo2.py:193][0m |           0.0102 |         145.7398 |           0.0629 |
[32m[20230209 04:28:14 @agent_ppo2.py:193][0m |           0.0050 |         141.0937 |           0.0628 |
[32m[20230209 04:28:14 @agent_ppo2.py:193][0m |          -0.0031 |         133.1487 |           0.0628 |
[32m[20230209 04:28:14 @agent_ppo2.py:193][0m |          -0.0056 |         130.5013 |           0.0628 |
[32m[20230209 04:28:14 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230209 04:28:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: -113.67
[32m[20230209 04:28:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.46
[32m[20230209 04:28:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -91.52
[32m[20230209 04:28:14 @agent_ppo2.py:150][0m Total time:       0.12 min
[32m[20230209 04:28:14 @agent_ppo2.py:152][0m 8192 total steps have happened
[32m[20230209 04:28:14 @agent_ppo2.py:128][0m #------------------------ Iteration 4 --------------------------#
[32m[20230209 04:28:15 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230209 04:28:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:15 @agent_ppo2.py:193][0m |          -0.0117 |         402.9119 |           0.0615 |
[32m[20230209 04:28:15 @agent_ppo2.py:193][0m |          -0.0132 |         381.5280 |           0.0615 |
[32m[20230209 04:28:15 @agent_ppo2.py:193][0m |          -0.0118 |         365.2125 |           0.0615 |
[32m[20230209 04:28:15 @agent_ppo2.py:193][0m |          -0.0047 |         356.9408 |           0.0615 |
[32m[20230209 04:28:15 @agent_ppo2.py:193][0m |          -0.0011 |         350.2182 |           0.0615 |
[32m[20230209 04:28:15 @agent_ppo2.py:193][0m |          -0.0098 |         331.2636 |           0.0615 |
[32m[20230209 04:28:15 @agent_ppo2.py:193][0m |           0.0163 |         354.1398 |           0.0615 |
[32m[20230209 04:28:16 @agent_ppo2.py:193][0m |          -0.0123 |         313.5400 |           0.0615 |
[32m[20230209 04:28:16 @agent_ppo2.py:193][0m |          -0.0141 |         305.2201 |           0.0615 |
[32m[20230209 04:28:16 @agent_ppo2.py:193][0m |           0.0146 |         310.7720 |           0.0615 |
[32m[20230209 04:28:16 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230209 04:28:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -113.55
[32m[20230209 04:28:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.56
[32m[20230209 04:28:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -91.87
[32m[20230209 04:28:16 @agent_ppo2.py:150][0m Total time:       0.15 min
[32m[20230209 04:28:16 @agent_ppo2.py:152][0m 10240 total steps have happened
[32m[20230209 04:28:16 @agent_ppo2.py:128][0m #------------------------ Iteration 5 --------------------------#
[32m[20230209 04:28:17 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230209 04:28:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:17 @agent_ppo2.py:193][0m |          -0.0027 |         143.7403 |           0.0629 |
[32m[20230209 04:28:17 @agent_ppo2.py:193][0m |          -0.0002 |         138.8149 |           0.0630 |
[32m[20230209 04:28:17 @agent_ppo2.py:193][0m |           0.0018 |         137.6281 |           0.0630 |
[32m[20230209 04:28:17 @agent_ppo2.py:193][0m |           0.0011 |         136.3252 |           0.0630 |
[32m[20230209 04:28:17 @agent_ppo2.py:193][0m |          -0.0027 |         131.3268 |           0.0630 |
[32m[20230209 04:28:17 @agent_ppo2.py:193][0m |          -0.0016 |         130.7021 |           0.0630 |
[32m[20230209 04:28:17 @agent_ppo2.py:193][0m |          -0.0005 |         128.5116 |           0.0630 |
[32m[20230209 04:28:17 @agent_ppo2.py:193][0m |          -0.0021 |         127.0357 |           0.0630 |
[32m[20230209 04:28:17 @agent_ppo2.py:193][0m |          -0.0029 |         124.8868 |           0.0630 |
[32m[20230209 04:28:18 @agent_ppo2.py:193][0m |          -0.0037 |         123.1692 |           0.0630 |
[32m[20230209 04:28:18 @agent_ppo2.py:137][0m Policy update time: 0.67 s
[32m[20230209 04:28:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.66
[32m[20230209 04:28:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -99.78
[32m[20230209 04:28:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -92.00
[32m[20230209 04:28:18 @agent_ppo2.py:150][0m Total time:       0.18 min
[32m[20230209 04:28:18 @agent_ppo2.py:152][0m 12288 total steps have happened
[32m[20230209 04:28:18 @agent_ppo2.py:128][0m #------------------------ Iteration 6 --------------------------#
[32m[20230209 04:28:19 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230209 04:28:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:19 @agent_ppo2.py:193][0m |           0.0930 |         395.8626 |           0.0622 |
[32m[20230209 04:28:19 @agent_ppo2.py:193][0m |          -0.0189 |         283.8704 |           0.0622 |
[32m[20230209 04:28:19 @agent_ppo2.py:193][0m |           0.1049 |         400.7555 |           0.0622 |
[32m[20230209 04:28:19 @agent_ppo2.py:193][0m |          -0.0194 |         271.7120 |           0.0621 |
[32m[20230209 04:28:19 @agent_ppo2.py:193][0m |          -0.0214 |         267.4928 |           0.0622 |
[32m[20230209 04:28:19 @agent_ppo2.py:193][0m |          -0.0212 |         262.4762 |           0.0622 |
[32m[20230209 04:28:19 @agent_ppo2.py:193][0m |          -0.0075 |         255.0857 |           0.0622 |
[32m[20230209 04:28:19 @agent_ppo2.py:193][0m |          -0.0208 |         248.8146 |           0.0622 |
[32m[20230209 04:28:19 @agent_ppo2.py:193][0m |          -0.0092 |         242.8510 |           0.0622 |
[32m[20230209 04:28:19 @agent_ppo2.py:193][0m |          -0.0234 |         236.9729 |           0.0622 |
[32m[20230209 04:28:19 @agent_ppo2.py:137][0m Policy update time: 0.58 s
[32m[20230209 04:28:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.64
[32m[20230209 04:28:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.55
[32m[20230209 04:28:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -92.13
[32m[20230209 04:28:20 @agent_ppo2.py:150][0m Total time:       0.21 min
[32m[20230209 04:28:20 @agent_ppo2.py:152][0m 14336 total steps have happened
[32m[20230209 04:28:20 @agent_ppo2.py:128][0m #------------------------ Iteration 7 --------------------------#
[32m[20230209 04:28:21 @agent_ppo2.py:134][0m Sampling time: 0.64 s by 1 slaves
[32m[20230209 04:28:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:21 @agent_ppo2.py:193][0m |           0.0008 |         254.7006 |           0.0645 |
[32m[20230209 04:28:21 @agent_ppo2.py:193][0m |          -0.0011 |         242.7461 |           0.0645 |
[32m[20230209 04:28:21 @agent_ppo2.py:193][0m |          -0.0011 |         235.9666 |           0.0645 |
[32m[20230209 04:28:21 @agent_ppo2.py:193][0m |          -0.0014 |         227.8995 |           0.0645 |
[32m[20230209 04:28:21 @agent_ppo2.py:193][0m |          -0.0009 |         220.9770 |           0.0645 |
[32m[20230209 04:28:21 @agent_ppo2.py:193][0m |          -0.0014 |         214.2037 |           0.0645 |
[32m[20230209 04:28:21 @agent_ppo2.py:193][0m |          -0.0022 |         206.7017 |           0.0644 |
[32m[20230209 04:28:21 @agent_ppo2.py:193][0m |          -0.0029 |         199.8707 |           0.0644 |
[32m[20230209 04:28:21 @agent_ppo2.py:193][0m |          -0.0033 |         192.7508 |           0.0644 |
[32m[20230209 04:28:21 @agent_ppo2.py:193][0m |          -0.0042 |         186.1726 |           0.0644 |
[32m[20230209 04:28:21 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230209 04:28:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: -115.92
[32m[20230209 04:28:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.65
[32m[20230209 04:28:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -93.04
[32m[20230209 04:28:22 @agent_ppo2.py:150][0m Total time:       0.24 min
[32m[20230209 04:28:22 @agent_ppo2.py:152][0m 16384 total steps have happened
[32m[20230209 04:28:22 @agent_ppo2.py:128][0m #------------------------ Iteration 8 --------------------------#
[32m[20230209 04:28:22 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230209 04:28:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:23 @agent_ppo2.py:193][0m |          -0.0015 |          61.6207 |           0.0638 |
[32m[20230209 04:28:23 @agent_ppo2.py:193][0m |          -0.0005 |          55.0617 |           0.0638 |
[32m[20230209 04:28:23 @agent_ppo2.py:193][0m |          -0.0039 |          52.2898 |           0.0638 |
[32m[20230209 04:28:23 @agent_ppo2.py:193][0m |          -0.0032 |          51.6473 |           0.0637 |
[32m[20230209 04:28:23 @agent_ppo2.py:193][0m |          -0.0053 |          50.6720 |           0.0637 |
[32m[20230209 04:28:23 @agent_ppo2.py:193][0m |          -0.0044 |          49.7557 |           0.0637 |
[32m[20230209 04:28:23 @agent_ppo2.py:193][0m |          -0.0078 |          48.3676 |           0.0636 |
[32m[20230209 04:28:23 @agent_ppo2.py:193][0m |          -0.0080 |          47.9268 |           0.0636 |
[32m[20230209 04:28:23 @agent_ppo2.py:193][0m |          -0.0069 |          49.0168 |           0.0636 |
[32m[20230209 04:28:23 @agent_ppo2.py:193][0m |          -0.0106 |          46.4380 |           0.0635 |
[32m[20230209 04:28:23 @agent_ppo2.py:137][0m Policy update time: 0.64 s
[32m[20230209 04:28:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: -120.36
[32m[20230209 04:28:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.22
[32m[20230209 04:28:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -96.97
[32m[20230209 04:28:24 @agent_ppo2.py:150][0m Total time:       0.27 min
[32m[20230209 04:28:24 @agent_ppo2.py:152][0m 18432 total steps have happened
[32m[20230209 04:28:24 @agent_ppo2.py:128][0m #------------------------ Iteration 9 --------------------------#
[32m[20230209 04:28:24 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230209 04:28:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:25 @agent_ppo2.py:193][0m |          -0.0005 |          37.1022 |           0.0639 |
[32m[20230209 04:28:25 @agent_ppo2.py:193][0m |          -0.0001 |          33.1065 |           0.0639 |
[32m[20230209 04:28:25 @agent_ppo2.py:193][0m |          -0.0018 |          31.5780 |           0.0639 |
[32m[20230209 04:28:25 @agent_ppo2.py:193][0m |          -0.0025 |          30.5575 |           0.0639 |
[32m[20230209 04:28:25 @agent_ppo2.py:193][0m |          -0.0035 |          29.8104 |           0.0639 |
[32m[20230209 04:28:25 @agent_ppo2.py:193][0m |          -0.0039 |          29.0070 |           0.0638 |
[32m[20230209 04:28:25 @agent_ppo2.py:193][0m |          -0.0035 |          28.6381 |           0.0638 |
[32m[20230209 04:28:25 @agent_ppo2.py:193][0m |          -0.0043 |          28.1987 |           0.0638 |
[32m[20230209 04:28:25 @agent_ppo2.py:193][0m |          -0.0051 |          27.6323 |           0.0638 |
[32m[20230209 04:28:25 @agent_ppo2.py:193][0m |          -0.0052 |          27.5054 |           0.0638 |
[32m[20230209 04:28:25 @agent_ppo2.py:137][0m Policy update time: 0.79 s
[32m[20230209 04:28:26 @agent_ppo2.py:145][0m Average TRAINING episode reward: -171.39
[32m[20230209 04:28:26 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.58
[32m[20230209 04:28:26 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -99.02
[32m[20230209 04:28:26 @agent_ppo2.py:150][0m Total time:       0.31 min
[32m[20230209 04:28:26 @agent_ppo2.py:152][0m 20480 total steps have happened
[32m[20230209 04:28:26 @agent_ppo2.py:128][0m #------------------------ Iteration 10 --------------------------#
[32m[20230209 04:28:27 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230209 04:28:27 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:27 @agent_ppo2.py:193][0m |          -0.0084 |          48.6183 |           0.0619 |
[32m[20230209 04:28:27 @agent_ppo2.py:193][0m |          -0.0029 |          45.0048 |           0.0618 |
[32m[20230209 04:28:27 @agent_ppo2.py:193][0m |          -0.0010 |          43.7923 |           0.0618 |
[32m[20230209 04:28:27 @agent_ppo2.py:193][0m |          -0.0068 |          41.6672 |           0.0617 |
[32m[20230209 04:28:27 @agent_ppo2.py:193][0m |           0.0040 |          42.1990 |           0.0617 |
[32m[20230209 04:28:27 @agent_ppo2.py:193][0m |          -0.0118 |          37.7322 |           0.0617 |
[32m[20230209 04:28:27 @agent_ppo2.py:193][0m |          -0.0083 |          35.7707 |           0.0618 |
[32m[20230209 04:28:27 @agent_ppo2.py:193][0m |          -0.0082 |          34.8584 |           0.0618 |
[32m[20230209 04:28:27 @agent_ppo2.py:193][0m |          -0.0161 |          34.1814 |           0.0618 |
[32m[20230209 04:28:27 @agent_ppo2.py:193][0m |           0.0007 |          34.8657 |           0.0618 |
[32m[20230209 04:28:27 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230209 04:28:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: -149.00
[32m[20230209 04:28:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.01
[32m[20230209 04:28:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -99.27
[32m[20230209 04:28:28 @agent_ppo2.py:150][0m Total time:       0.34 min
[32m[20230209 04:28:28 @agent_ppo2.py:152][0m 22528 total steps have happened
[32m[20230209 04:28:28 @agent_ppo2.py:128][0m #------------------------ Iteration 11 --------------------------#
[32m[20230209 04:28:29 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230209 04:28:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:29 @agent_ppo2.py:193][0m |           0.0000 |         309.5739 |           0.0651 |
[32m[20230209 04:28:29 @agent_ppo2.py:193][0m |          -0.0007 |         280.0733 |           0.0651 |
[32m[20230209 04:28:29 @agent_ppo2.py:193][0m |          -0.0014 |         258.8236 |           0.0651 |
[32m[20230209 04:28:29 @agent_ppo2.py:193][0m |          -0.0022 |         240.9557 |           0.0651 |
[32m[20230209 04:28:29 @agent_ppo2.py:193][0m |          -0.0029 |         225.7887 |           0.0651 |
[32m[20230209 04:28:29 @agent_ppo2.py:193][0m |          -0.0037 |         211.4967 |           0.0651 |
[32m[20230209 04:28:29 @agent_ppo2.py:193][0m |          -0.0043 |         199.1057 |           0.0651 |
[32m[20230209 04:28:29 @agent_ppo2.py:193][0m |          -0.0049 |         187.7396 |           0.0651 |
[32m[20230209 04:28:29 @agent_ppo2.py:193][0m |          -0.0053 |         177.5394 |           0.0651 |
[32m[20230209 04:28:29 @agent_ppo2.py:193][0m |          -0.0060 |         167.9932 |           0.0651 |
[32m[20230209 04:28:29 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230209 04:28:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -106.26
[32m[20230209 04:28:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.18
[32m[20230209 04:28:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -91.25
[32m[20230209 04:28:30 @agent_ppo2.py:150][0m Total time:       0.38 min
[32m[20230209 04:28:30 @agent_ppo2.py:152][0m 24576 total steps have happened
[32m[20230209 04:28:30 @agent_ppo2.py:128][0m #------------------------ Iteration 12 --------------------------#
[32m[20230209 04:28:31 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230209 04:28:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:31 @agent_ppo2.py:193][0m |          -0.0087 |         166.6872 |           0.0620 |
[32m[20230209 04:28:31 @agent_ppo2.py:193][0m |           0.0025 |         160.3342 |           0.0620 |
[32m[20230209 04:28:31 @agent_ppo2.py:193][0m |          -0.0023 |         146.2659 |           0.0620 |
[32m[20230209 04:28:31 @agent_ppo2.py:193][0m |          -0.0123 |         132.6524 |           0.0620 |
[32m[20230209 04:28:31 @agent_ppo2.py:193][0m |           0.0008 |         129.3064 |           0.0619 |
[32m[20230209 04:28:31 @agent_ppo2.py:193][0m |          -0.0108 |         119.8605 |           0.0619 |
[32m[20230209 04:28:31 @agent_ppo2.py:193][0m |          -0.0107 |         115.4127 |           0.0618 |
[32m[20230209 04:28:31 @agent_ppo2.py:193][0m |          -0.0093 |         108.1662 |           0.0618 |
[32m[20230209 04:28:31 @agent_ppo2.py:193][0m |          -0.0024 |         106.7508 |           0.0618 |
[32m[20230209 04:28:31 @agent_ppo2.py:193][0m |          -0.0012 |         100.6804 |           0.0618 |
[32m[20230209 04:28:31 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230209 04:28:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -111.02
[32m[20230209 04:28:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.95
[32m[20230209 04:28:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -54.89
[32m[20230209 04:28:32 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -54.89
[32m[20230209 04:28:32 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -54.89
[32m[20230209 04:28:32 @agent_ppo2.py:150][0m Total time:       0.41 min
[32m[20230209 04:28:32 @agent_ppo2.py:152][0m 26624 total steps have happened
[32m[20230209 04:28:32 @agent_ppo2.py:128][0m #------------------------ Iteration 13 --------------------------#
[32m[20230209 04:28:33 @agent_ppo2.py:134][0m Sampling time: 0.94 s by 1 slaves
[32m[20230209 04:28:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:33 @agent_ppo2.py:193][0m |           0.0008 |          88.2015 |           0.0635 |
[32m[20230209 04:28:33 @agent_ppo2.py:193][0m |          -0.0027 |          75.3179 |           0.0635 |
[32m[20230209 04:28:33 @agent_ppo2.py:193][0m |          -0.0046 |          72.6201 |           0.0634 |
[32m[20230209 04:28:33 @agent_ppo2.py:193][0m |          -0.0049 |          68.7315 |           0.0634 |
[32m[20230209 04:28:33 @agent_ppo2.py:193][0m |          -0.0058 |          66.0914 |           0.0634 |
[32m[20230209 04:28:33 @agent_ppo2.py:193][0m |          -0.0071 |          64.3636 |           0.0634 |
[32m[20230209 04:28:34 @agent_ppo2.py:193][0m |          -0.0072 |          61.3053 |           0.0634 |
[32m[20230209 04:28:34 @agent_ppo2.py:193][0m |          -0.0081 |          58.5090 |           0.0634 |
[32m[20230209 04:28:34 @agent_ppo2.py:193][0m |          -0.0066 |          56.6388 |           0.0634 |
[32m[20230209 04:28:34 @agent_ppo2.py:193][0m |          -0.0078 |          54.6625 |           0.0634 |
[32m[20230209 04:28:34 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230209 04:28:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.73
[32m[20230209 04:28:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.78
[32m[20230209 04:28:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -97.95
[32m[20230209 04:28:34 @agent_ppo2.py:150][0m Total time:       0.45 min
[32m[20230209 04:28:34 @agent_ppo2.py:152][0m 28672 total steps have happened
[32m[20230209 04:28:34 @agent_ppo2.py:128][0m #------------------------ Iteration 14 --------------------------#
[32m[20230209 04:28:35 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230209 04:28:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:35 @agent_ppo2.py:193][0m |          -0.0141 |          51.7323 |           0.0627 |
[32m[20230209 04:28:35 @agent_ppo2.py:193][0m |           0.0353 |          42.3632 |           0.0627 |
[32m[20230209 04:28:35 @agent_ppo2.py:193][0m |           0.0002 |          41.2864 |           0.0626 |
[32m[20230209 04:28:35 @agent_ppo2.py:193][0m |          -0.0200 |          39.6899 |           0.0626 |
[32m[20230209 04:28:35 @agent_ppo2.py:193][0m |          -0.0104 |          38.8823 |           0.0625 |
[32m[20230209 04:28:35 @agent_ppo2.py:193][0m |          -0.0138 |          37.9315 |           0.0625 |
[32m[20230209 04:28:35 @agent_ppo2.py:193][0m |          -0.0115 |          37.2166 |           0.0625 |
[32m[20230209 04:28:35 @agent_ppo2.py:193][0m |           0.0171 |          37.4068 |           0.0624 |
[32m[20230209 04:28:36 @agent_ppo2.py:193][0m |          -0.0111 |          36.4936 |           0.0624 |
[32m[20230209 04:28:36 @agent_ppo2.py:193][0m |          -0.0315 |          35.5964 |           0.0624 |
[32m[20230209 04:28:36 @agent_ppo2.py:137][0m Policy update time: 0.58 s
[32m[20230209 04:28:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -126.81
[32m[20230209 04:28:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.58
[32m[20230209 04:28:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -99.73
[32m[20230209 04:28:36 @agent_ppo2.py:150][0m Total time:       0.48 min
[32m[20230209 04:28:36 @agent_ppo2.py:152][0m 30720 total steps have happened
[32m[20230209 04:28:36 @agent_ppo2.py:128][0m #------------------------ Iteration 15 --------------------------#
[32m[20230209 04:28:37 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230209 04:28:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:37 @agent_ppo2.py:193][0m |          -0.0054 |          68.2287 |           0.0643 |
[32m[20230209 04:28:37 @agent_ppo2.py:193][0m |           0.0003 |          57.7474 |           0.0643 |
[32m[20230209 04:28:37 @agent_ppo2.py:193][0m |          -0.0056 |          52.7448 |           0.0643 |
[32m[20230209 04:28:37 @agent_ppo2.py:193][0m |          -0.0087 |          50.2494 |           0.0643 |
[32m[20230209 04:28:38 @agent_ppo2.py:193][0m |          -0.0049 |          46.9843 |           0.0643 |
[32m[20230209 04:28:38 @agent_ppo2.py:193][0m |          -0.0032 |          44.6974 |           0.0643 |
[32m[20230209 04:28:38 @agent_ppo2.py:193][0m |          -0.0038 |          43.1582 |           0.0643 |
[32m[20230209 04:28:38 @agent_ppo2.py:193][0m |          -0.0022 |          42.4942 |           0.0643 |
[32m[20230209 04:28:38 @agent_ppo2.py:193][0m |          -0.0060 |          39.5590 |           0.0643 |
[32m[20230209 04:28:38 @agent_ppo2.py:193][0m |           0.0009 |          38.3320 |           0.0643 |
[32m[20230209 04:28:38 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230209 04:28:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -112.98
[32m[20230209 04:28:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -99.62
[32m[20230209 04:28:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -100.25
[32m[20230209 04:28:39 @agent_ppo2.py:150][0m Total time:       0.52 min
[32m[20230209 04:28:39 @agent_ppo2.py:152][0m 32768 total steps have happened
[32m[20230209 04:28:39 @agent_ppo2.py:128][0m #------------------------ Iteration 16 --------------------------#
[32m[20230209 04:28:39 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230209 04:28:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:39 @agent_ppo2.py:193][0m |          -0.0255 |          40.9694 |           0.0622 |
[32m[20230209 04:28:39 @agent_ppo2.py:193][0m |           0.0172 |          31.4926 |           0.0622 |
[32m[20230209 04:28:39 @agent_ppo2.py:193][0m |          -0.0025 |          28.7946 |           0.0622 |
[32m[20230209 04:28:39 @agent_ppo2.py:193][0m |          -0.0102 |          27.4550 |           0.0622 |
[32m[20230209 04:28:39 @agent_ppo2.py:193][0m |           0.0066 |          27.0445 |           0.0621 |
[32m[20230209 04:28:40 @agent_ppo2.py:193][0m |           0.0843 |          39.6109 |           0.0621 |
[32m[20230209 04:28:40 @agent_ppo2.py:193][0m |          -0.0005 |          25.8089 |           0.0621 |
[32m[20230209 04:28:40 @agent_ppo2.py:193][0m |          -0.0064 |          25.5922 |           0.0620 |
[32m[20230209 04:28:40 @agent_ppo2.py:193][0m |          -0.0179 |          24.4794 |           0.0620 |
[32m[20230209 04:28:40 @agent_ppo2.py:193][0m |           0.0083 |          23.9357 |           0.0620 |
[32m[20230209 04:28:40 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230209 04:28:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: -108.90
[32m[20230209 04:28:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.55
[32m[20230209 04:28:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -98.41
[32m[20230209 04:28:40 @agent_ppo2.py:150][0m Total time:       0.55 min
[32m[20230209 04:28:40 @agent_ppo2.py:152][0m 34816 total steps have happened
[32m[20230209 04:28:40 @agent_ppo2.py:128][0m #------------------------ Iteration 17 --------------------------#
[32m[20230209 04:28:41 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230209 04:28:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:41 @agent_ppo2.py:193][0m |           0.0028 |          37.4530 |           0.0633 |
[32m[20230209 04:28:41 @agent_ppo2.py:193][0m |          -0.0066 |          20.5551 |           0.0632 |
[32m[20230209 04:28:41 @agent_ppo2.py:193][0m |          -0.0092 |          19.7986 |           0.0632 |
[32m[20230209 04:28:41 @agent_ppo2.py:193][0m |          -0.0111 |          17.6122 |           0.0632 |
[32m[20230209 04:28:41 @agent_ppo2.py:193][0m |          -0.0050 |          16.9411 |           0.0632 |
[32m[20230209 04:28:41 @agent_ppo2.py:193][0m |          -0.0166 |          16.2366 |           0.0632 |
[32m[20230209 04:28:41 @agent_ppo2.py:193][0m |          -0.0145 |          15.8227 |           0.0632 |
[32m[20230209 04:28:41 @agent_ppo2.py:193][0m |          -0.0033 |          15.4540 |           0.0632 |
[32m[20230209 04:28:41 @agent_ppo2.py:193][0m |          -0.0131 |          15.1386 |           0.0632 |
[32m[20230209 04:28:42 @agent_ppo2.py:193][0m |          -0.0185 |          14.6024 |           0.0632 |
[32m[20230209 04:28:42 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230209 04:28:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: -112.45
[32m[20230209 04:28:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.81
[32m[20230209 04:28:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -96.97
[32m[20230209 04:28:42 @agent_ppo2.py:150][0m Total time:       0.58 min
[32m[20230209 04:28:42 @agent_ppo2.py:152][0m 36864 total steps have happened
[32m[20230209 04:28:42 @agent_ppo2.py:128][0m #------------------------ Iteration 18 --------------------------#
[32m[20230209 04:28:43 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230209 04:28:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:43 @agent_ppo2.py:193][0m |          -0.0040 |          25.4400 |           0.0634 |
[32m[20230209 04:28:43 @agent_ppo2.py:193][0m |          -0.0047 |          15.4428 |           0.0634 |
[32m[20230209 04:28:43 @agent_ppo2.py:193][0m |          -0.0064 |          14.5223 |           0.0635 |
[32m[20230209 04:28:43 @agent_ppo2.py:193][0m |          -0.0062 |          13.8278 |           0.0634 |
[32m[20230209 04:28:43 @agent_ppo2.py:193][0m |          -0.0049 |          13.7077 |           0.0635 |
[32m[20230209 04:28:43 @agent_ppo2.py:193][0m |          -0.0032 |          13.1881 |           0.0634 |
[32m[20230209 04:28:43 @agent_ppo2.py:193][0m |          -0.0055 |          12.6361 |           0.0635 |
[32m[20230209 04:28:43 @agent_ppo2.py:193][0m |          -0.0061 |          12.2914 |           0.0635 |
[32m[20230209 04:28:43 @agent_ppo2.py:193][0m |          -0.0094 |          12.0332 |           0.0635 |
[32m[20230209 04:28:43 @agent_ppo2.py:193][0m |          -0.0078 |          11.6141 |           0.0635 |
[32m[20230209 04:28:43 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230209 04:28:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: -110.26
[32m[20230209 04:28:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.49
[32m[20230209 04:28:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -97.51
[32m[20230209 04:28:44 @agent_ppo2.py:150][0m Total time:       0.61 min
[32m[20230209 04:28:44 @agent_ppo2.py:152][0m 38912 total steps have happened
[32m[20230209 04:28:44 @agent_ppo2.py:128][0m #------------------------ Iteration 19 --------------------------#
[32m[20230209 04:28:45 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230209 04:28:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:45 @agent_ppo2.py:193][0m |           0.0060 |          22.0986 |           0.0647 |
[32m[20230209 04:28:45 @agent_ppo2.py:193][0m |          -0.0011 |          18.2344 |           0.0647 |
[32m[20230209 04:28:45 @agent_ppo2.py:193][0m |          -0.0061 |          17.4266 |           0.0647 |
[32m[20230209 04:28:45 @agent_ppo2.py:193][0m |          -0.0019 |          17.3029 |           0.0646 |
[32m[20230209 04:28:45 @agent_ppo2.py:193][0m |          -0.0065 |          16.6367 |           0.0646 |
[32m[20230209 04:28:45 @agent_ppo2.py:193][0m |          -0.0018 |          16.6985 |           0.0646 |
[32m[20230209 04:28:45 @agent_ppo2.py:193][0m |          -0.0013 |          16.4813 |           0.0645 |
[32m[20230209 04:28:45 @agent_ppo2.py:193][0m |          -0.0038 |          15.8076 |           0.0646 |
[32m[20230209 04:28:45 @agent_ppo2.py:193][0m |          -0.0052 |          15.3389 |           0.0645 |
[32m[20230209 04:28:45 @agent_ppo2.py:193][0m |          -0.0010 |          15.4626 |           0.0645 |
[32m[20230209 04:28:45 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230209 04:28:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -110.93
[32m[20230209 04:28:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.63
[32m[20230209 04:28:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -96.67
[32m[20230209 04:28:46 @agent_ppo2.py:150][0m Total time:       0.64 min
[32m[20230209 04:28:46 @agent_ppo2.py:152][0m 40960 total steps have happened
[32m[20230209 04:28:46 @agent_ppo2.py:128][0m #------------------------ Iteration 20 --------------------------#
[32m[20230209 04:28:46 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230209 04:28:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:28:47 @agent_ppo2.py:193][0m |          -0.0053 |          31.9846 |           0.0640 |
[32m[20230209 04:28:47 @agent_ppo2.py:193][0m |          -0.0069 |          21.3266 |           0.0640 |
[32m[20230209 04:28:47 @agent_ppo2.py:193][0m |           0.0089 |          19.8339 |           0.0640 |
[32m[20230209 04:28:47 @agent_ppo2.py:193][0m |           0.0029 |          19.5014 |           0.0640 |
[32m[20230209 04:28:47 @agent_ppo2.py:193][0m |          -0.0110 |          18.2036 |           0.0639 |
[32m[20230209 04:28:47 @agent_ppo2.py:193][0m |          -0.0057 |          17.7264 |           0.0639 |
[32m[20230209 04:28:47 @agent_ppo2.py:193][0m |          -0.0056 |          17.3619 |           0.0638 |
[32m[20230209 04:28:47 @agent_ppo2.py:193][0m |          -0.0138 |          16.8973 |           0.0638 |
[32m[20230209 04:28:47 @agent_ppo2.py:193][0m |          -0.0024 |          17.1067 |           0.0637 |
