[32m[20230209 04:34:47 @logger.py:106][0m Log file set to ./tmp/evo_bipedalwalker/easy/20230209_043447/log/evo_bipedalwalker_easy-20230209_043447.log
[32m[20230209 04:34:47 @agent_ppo2.py:128][0m #------------------------ Iteration 0 --------------------------#
[32m[20230209 04:34:48 @agent_ppo2.py:134][0m Sampling time: 0.74 s by 1 slaves
[32m[20230209 04:34:48 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:48 @agent_ppo2.py:193][0m |          -0.0022 |         446.3368 |           0.0651 |
[32m[20230209 04:34:48 @agent_ppo2.py:193][0m |          -0.0007 |         437.6363 |           0.0651 |
[32m[20230209 04:34:48 @agent_ppo2.py:193][0m |           0.0006 |         421.0956 |           0.0650 |
[32m[20230209 04:34:48 @agent_ppo2.py:193][0m |          -0.0044 |         396.7618 |           0.0650 |
[32m[20230209 04:34:48 @agent_ppo2.py:193][0m |          -0.0081 |         378.0497 |           0.0649 |
[32m[20230209 04:34:48 @agent_ppo2.py:193][0m |          -0.0067 |         371.0575 |           0.0649 |
[32m[20230209 04:34:49 @agent_ppo2.py:193][0m |          -0.0077 |         358.1287 |           0.0648 |
[32m[20230209 04:34:49 @agent_ppo2.py:193][0m |          -0.0074 |         350.3031 |           0.0647 |
[32m[20230209 04:34:49 @agent_ppo2.py:193][0m |          -0.0104 |         338.0336 |           0.0647 |
[32m[20230209 04:34:49 @agent_ppo2.py:193][0m |          -0.0129 |         328.9096 |           0.0646 |
[32m[20230209 04:34:49 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230209 04:34:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: -108.92
[32m[20230209 04:34:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.00
[32m[20230209 04:34:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -117.79
[32m[20230209 04:34:49 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -117.79
[32m[20230209 04:34:49 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -117.79
[32m[20230209 04:34:49 @agent_ppo2.py:150][0m Total time:       0.04 min
[32m[20230209 04:34:49 @agent_ppo2.py:152][0m 2048 total steps have happened
[32m[20230209 04:34:49 @agent_ppo2.py:128][0m #------------------------ Iteration 1 --------------------------#
[32m[20230209 04:34:50 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230209 04:34:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:50 @agent_ppo2.py:193][0m |           0.0018 |         149.5441 |           0.0624 |
[32m[20230209 04:34:50 @agent_ppo2.py:193][0m |           0.0027 |         148.4130 |           0.0624 |
[32m[20230209 04:34:50 @agent_ppo2.py:193][0m |          -0.0083 |         143.1166 |           0.0624 |
[32m[20230209 04:34:50 @agent_ppo2.py:193][0m |           0.0021 |         148.7655 |           0.0624 |
[32m[20230209 04:34:50 @agent_ppo2.py:193][0m |           0.0141 |         155.7603 |           0.0623 |
[32m[20230209 04:34:50 @agent_ppo2.py:193][0m |           0.0222 |         171.1510 |           0.0623 |
[32m[20230209 04:34:50 @agent_ppo2.py:193][0m |          -0.0137 |         139.3676 |           0.0623 |
[32m[20230209 04:34:51 @agent_ppo2.py:193][0m |          -0.0044 |         146.3453 |           0.0623 |
[32m[20230209 04:34:51 @agent_ppo2.py:193][0m |          -0.0119 |         137.8105 |           0.0623 |
[32m[20230209 04:34:51 @agent_ppo2.py:193][0m |          -0.0163 |         135.1452 |           0.0623 |
[32m[20230209 04:34:51 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230209 04:34:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: -125.02
[32m[20230209 04:34:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.06
[32m[20230209 04:34:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -111.87
[32m[20230209 04:34:51 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -111.87
[32m[20230209 04:34:51 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -111.87
[32m[20230209 04:34:51 @agent_ppo2.py:150][0m Total time:       0.07 min
[32m[20230209 04:34:51 @agent_ppo2.py:152][0m 4096 total steps have happened
[32m[20230209 04:34:51 @agent_ppo2.py:128][0m #------------------------ Iteration 2 --------------------------#
[32m[20230209 04:34:52 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230209 04:34:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:52 @agent_ppo2.py:193][0m |           0.0020 |         255.7238 |           0.0620 |
[32m[20230209 04:34:52 @agent_ppo2.py:193][0m |          -0.0056 |         239.1056 |           0.0620 |
[32m[20230209 04:34:52 @agent_ppo2.py:193][0m |           0.0008 |         241.2366 |           0.0620 |
[32m[20230209 04:34:52 @agent_ppo2.py:193][0m |          -0.0024 |         229.5064 |           0.0620 |
[32m[20230209 04:34:52 @agent_ppo2.py:193][0m |          -0.0091 |         219.6969 |           0.0619 |
[32m[20230209 04:34:53 @agent_ppo2.py:193][0m |          -0.0047 |         217.3953 |           0.0619 |
[32m[20230209 04:34:53 @agent_ppo2.py:193][0m |          -0.0062 |         213.8349 |           0.0619 |
[32m[20230209 04:34:53 @agent_ppo2.py:193][0m |          -0.0078 |         209.5760 |           0.0618 |
[32m[20230209 04:34:53 @agent_ppo2.py:193][0m |          -0.0106 |         200.5155 |           0.0618 |
[32m[20230209 04:34:53 @agent_ppo2.py:193][0m |          -0.0102 |         197.3698 |           0.0618 |
[32m[20230209 04:34:53 @agent_ppo2.py:137][0m Policy update time: 0.74 s
[32m[20230209 04:34:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.13
[32m[20230209 04:34:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -99.66
[32m[20230209 04:34:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -112.27
[32m[20230209 04:34:53 @agent_ppo2.py:150][0m Total time:       0.10 min
[32m[20230209 04:34:53 @agent_ppo2.py:152][0m 6144 total steps have happened
[32m[20230209 04:34:53 @agent_ppo2.py:128][0m #------------------------ Iteration 3 --------------------------#
[32m[20230209 04:34:54 @agent_ppo2.py:134][0m Sampling time: 0.99 s by 1 slaves
[32m[20230209 04:34:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:55 @agent_ppo2.py:193][0m |           0.0002 |         141.2261 |           0.0627 |
[32m[20230209 04:34:55 @agent_ppo2.py:193][0m |          -0.0014 |         139.7793 |           0.0627 |
[32m[20230209 04:34:55 @agent_ppo2.py:193][0m |          -0.0030 |         139.1842 |           0.0627 |
[32m[20230209 04:34:55 @agent_ppo2.py:193][0m |          -0.0047 |         138.3201 |           0.0627 |
[32m[20230209 04:34:55 @agent_ppo2.py:193][0m |          -0.0061 |         137.2374 |           0.0627 |
[32m[20230209 04:34:55 @agent_ppo2.py:193][0m |          -0.0076 |         133.5224 |           0.0627 |
[32m[20230209 04:34:55 @agent_ppo2.py:193][0m |          -0.0085 |         128.7034 |           0.0626 |
[32m[20230209 04:34:55 @agent_ppo2.py:193][0m |          -0.0092 |         125.5482 |           0.0626 |
[32m[20230209 04:34:55 @agent_ppo2.py:193][0m |          -0.0099 |         122.2083 |           0.0626 |
[32m[20230209 04:34:56 @agent_ppo2.py:193][0m |          -0.0099 |         118.8788 |           0.0626 |
[32m[20230209 04:34:56 @agent_ppo2.py:137][0m Policy update time: 1.06 s
[32m[20230209 04:34:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: -131.00
[32m[20230209 04:34:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -105.05
[32m[20230209 04:34:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -36.25
[32m[20230209 04:34:56 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -36.25
[32m[20230209 04:34:56 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -36.25
[32m[20230209 04:34:56 @agent_ppo2.py:150][0m Total time:       0.15 min
[32m[20230209 04:34:56 @agent_ppo2.py:152][0m 8192 total steps have happened
[32m[20230209 04:34:56 @agent_ppo2.py:128][0m #------------------------ Iteration 4 --------------------------#
[32m[20230209 04:34:57 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230209 04:34:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:57 @agent_ppo2.py:193][0m |           0.0004 |          18.5279 |           0.0622 |
[32m[20230209 04:34:57 @agent_ppo2.py:193][0m |          -0.0022 |          17.5212 |           0.0622 |
[32m[20230209 04:34:58 @agent_ppo2.py:193][0m |          -0.0032 |          17.3792 |           0.0622 |
[32m[20230209 04:34:58 @agent_ppo2.py:193][0m |          -0.0019 |          18.0023 |           0.0622 |
[32m[20230209 04:34:58 @agent_ppo2.py:193][0m |          -0.0052 |          17.1869 |           0.0622 |
[32m[20230209 04:34:58 @agent_ppo2.py:193][0m |          -0.0063 |          17.0925 |           0.0622 |
[32m[20230209 04:34:58 @agent_ppo2.py:193][0m |          -0.0056 |          17.1236 |           0.0622 |
[32m[20230209 04:34:58 @agent_ppo2.py:193][0m |          -0.0071 |          16.9640 |           0.0622 |
[32m[20230209 04:34:58 @agent_ppo2.py:193][0m |          -0.0089 |          16.8878 |           0.0621 |
[32m[20230209 04:34:58 @agent_ppo2.py:193][0m |          -0.0071 |          17.1807 |           0.0621 |
[32m[20230209 04:34:58 @agent_ppo2.py:137][0m Policy update time: 0.93 s
[32m[20230209 04:34:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: -129.82
[32m[20230209 04:34:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -106.74
[32m[20230209 04:34:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -107.98
[32m[20230209 04:34:59 @agent_ppo2.py:150][0m Total time:       0.19 min
[32m[20230209 04:34:59 @agent_ppo2.py:152][0m 10240 total steps have happened
[32m[20230209 04:34:59 @agent_ppo2.py:128][0m #------------------------ Iteration 5 --------------------------#
[32m[20230209 04:34:59 @agent_ppo2.py:134][0m Sampling time: 0.68 s by 1 slaves
[32m[20230209 04:35:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:00 @agent_ppo2.py:193][0m |           0.0126 |         161.9135 |           0.0618 |
[32m[20230209 04:35:00 @agent_ppo2.py:193][0m |           0.0021 |         140.8367 |           0.0618 |
[32m[20230209 04:35:00 @agent_ppo2.py:193][0m |          -0.0057 |         138.5291 |           0.0618 |
[32m[20230209 04:35:00 @agent_ppo2.py:193][0m |          -0.0020 |         140.0156 |           0.0619 |
[32m[20230209 04:35:00 @agent_ppo2.py:193][0m |          -0.0032 |         137.9421 |           0.0619 |
[32m[20230209 04:35:00 @agent_ppo2.py:193][0m |          -0.0081 |         137.3676 |           0.0619 |
[32m[20230209 04:35:00 @agent_ppo2.py:193][0m |          -0.0015 |         143.9088 |           0.0619 |
[32m[20230209 04:35:00 @agent_ppo2.py:193][0m |          -0.0146 |         136.2225 |           0.0619 |
[32m[20230209 04:35:00 @agent_ppo2.py:193][0m |          -0.0076 |         138.2587 |           0.0619 |
[32m[20230209 04:35:00 @agent_ppo2.py:193][0m |          -0.0083 |         136.0541 |           0.0619 |
[32m[20230209 04:35:00 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230209 04:35:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -146.43
[32m[20230209 04:35:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.92
[32m[20230209 04:35:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -98.39
[32m[20230209 04:35:01 @agent_ppo2.py:150][0m Total time:       0.23 min
[32m[20230209 04:35:01 @agent_ppo2.py:152][0m 12288 total steps have happened
[32m[20230209 04:35:01 @agent_ppo2.py:128][0m #------------------------ Iteration 6 --------------------------#
[32m[20230209 04:35:01 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230209 04:35:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:02 @agent_ppo2.py:193][0m |          -0.0013 |         179.4062 |           0.0632 |
[32m[20230209 04:35:02 @agent_ppo2.py:193][0m |          -0.0007 |         173.5895 |           0.0631 |
[32m[20230209 04:35:02 @agent_ppo2.py:193][0m |          -0.0025 |         167.7250 |           0.0631 |
[32m[20230209 04:35:02 @agent_ppo2.py:193][0m |          -0.0040 |         161.6738 |           0.0631 |
[32m[20230209 04:35:02 @agent_ppo2.py:193][0m |          -0.0086 |         153.3559 |           0.0630 |
[32m[20230209 04:35:02 @agent_ppo2.py:193][0m |          -0.0114 |         146.9781 |           0.0630 |
[32m[20230209 04:35:02 @agent_ppo2.py:193][0m |          -0.0143 |         141.2518 |           0.0629 |
[32m[20230209 04:35:02 @agent_ppo2.py:193][0m |          -0.0105 |         137.7369 |           0.0629 |
[32m[20230209 04:35:02 @agent_ppo2.py:193][0m |          -0.0114 |         133.8599 |           0.0629 |
[32m[20230209 04:35:02 @agent_ppo2.py:193][0m |          -0.0103 |         130.4237 |           0.0629 |
[32m[20230209 04:35:02 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230209 04:35:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -126.23
[32m[20230209 04:35:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -99.83
[32m[20230209 04:35:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -114.16
[32m[20230209 04:35:03 @agent_ppo2.py:150][0m Total time:       0.26 min
[32m[20230209 04:35:03 @agent_ppo2.py:152][0m 14336 total steps have happened
[32m[20230209 04:35:03 @agent_ppo2.py:128][0m #------------------------ Iteration 7 --------------------------#
[32m[20230209 04:35:04 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230209 04:35:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:04 @agent_ppo2.py:193][0m |          -0.0011 |           7.7639 |           0.0625 |
[32m[20230209 04:35:04 @agent_ppo2.py:193][0m |          -0.0018 |           3.2013 |           0.0625 |
[32m[20230209 04:35:04 @agent_ppo2.py:193][0m |          -0.0017 |           2.8713 |           0.0623 |
[32m[20230209 04:35:04 @agent_ppo2.py:193][0m |          -0.0132 |           2.8180 |           0.0623 |
[32m[20230209 04:35:04 @agent_ppo2.py:193][0m |           0.0292 |           2.6519 |           0.0623 |
[32m[20230209 04:35:04 @agent_ppo2.py:193][0m |           0.0016 |           2.6194 |           0.0622 |
[32m[20230209 04:35:04 @agent_ppo2.py:193][0m |           0.0000 |           2.6210 |           0.0623 |
[32m[20230209 04:35:04 @agent_ppo2.py:193][0m |          -0.0054 |           2.5376 |           0.0623 |
[32m[20230209 04:35:04 @agent_ppo2.py:193][0m |          -0.0061 |           2.3930 |           0.0622 |
[32m[20230209 04:35:04 @agent_ppo2.py:193][0m |          -0.0090 |           2.4020 |           0.0622 |
[32m[20230209 04:35:04 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230209 04:35:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -149.20
[32m[20230209 04:35:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -129.77
[32m[20230209 04:35:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -100.51
[32m[20230209 04:35:05 @agent_ppo2.py:150][0m Total time:       0.30 min
[32m[20230209 04:35:05 @agent_ppo2.py:152][0m 16384 total steps have happened
[32m[20230209 04:35:05 @agent_ppo2.py:128][0m #------------------------ Iteration 8 --------------------------#
[32m[20230209 04:35:06 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230209 04:35:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:06 @agent_ppo2.py:193][0m |          -0.0004 |         202.5088 |           0.0638 |
[32m[20230209 04:35:06 @agent_ppo2.py:193][0m |          -0.0044 |         180.1934 |           0.0638 |
[32m[20230209 04:35:06 @agent_ppo2.py:193][0m |          -0.0076 |         167.3059 |           0.0638 |
[32m[20230209 04:35:06 @agent_ppo2.py:193][0m |          -0.0097 |         156.9312 |           0.0637 |
[32m[20230209 04:35:06 @agent_ppo2.py:193][0m |          -0.0109 |         148.4038 |           0.0637 |
[32m[20230209 04:35:07 @agent_ppo2.py:193][0m |          -0.0120 |         140.6766 |           0.0636 |
[32m[20230209 04:35:07 @agent_ppo2.py:193][0m |          -0.0126 |         133.8194 |           0.0636 |
[32m[20230209 04:35:07 @agent_ppo2.py:193][0m |          -0.0134 |         127.5791 |           0.0636 |
[32m[20230209 04:35:07 @agent_ppo2.py:193][0m |          -0.0140 |         121.9532 |           0.0635 |
[32m[20230209 04:35:07 @agent_ppo2.py:193][0m |          -0.0145 |         116.9501 |           0.0635 |
[32m[20230209 04:35:07 @agent_ppo2.py:137][0m Policy update time: 1.02 s
[32m[20230209 04:35:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.17
[32m[20230209 04:35:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.79
[32m[20230209 04:35:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -97.65
[32m[20230209 04:35:08 @agent_ppo2.py:150][0m Total time:       0.34 min
[32m[20230209 04:35:08 @agent_ppo2.py:152][0m 18432 total steps have happened
[32m[20230209 04:35:08 @agent_ppo2.py:128][0m #------------------------ Iteration 9 --------------------------#
[32m[20230209 04:35:08 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230209 04:35:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:09 @agent_ppo2.py:193][0m |           0.0007 |         132.8494 |           0.0620 |
[32m[20230209 04:35:09 @agent_ppo2.py:193][0m |          -0.0012 |         122.6608 |           0.0620 |
[32m[20230209 04:35:09 @agent_ppo2.py:193][0m |          -0.0023 |         117.2342 |           0.0620 |
[32m[20230209 04:35:09 @agent_ppo2.py:193][0m |          -0.0044 |         111.3052 |           0.0619 |
[32m[20230209 04:35:09 @agent_ppo2.py:193][0m |          -0.0058 |         107.0349 |           0.0619 |
[32m[20230209 04:35:09 @agent_ppo2.py:193][0m |          -0.0070 |         103.4345 |           0.0618 |
[32m[20230209 04:35:09 @agent_ppo2.py:193][0m |          -0.0089 |          99.7054 |           0.0617 |
[32m[20230209 04:35:09 @agent_ppo2.py:193][0m |          -0.0102 |          96.8744 |           0.0617 |
[32m[20230209 04:35:09 @agent_ppo2.py:193][0m |          -0.0098 |          94.0541 |           0.0617 |
[32m[20230209 04:35:09 @agent_ppo2.py:193][0m |          -0.0107 |          91.2837 |           0.0616 |
[32m[20230209 04:35:09 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230209 04:35:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -122.42
[32m[20230209 04:35:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -97.81
[32m[20230209 04:35:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -96.31
[32m[20230209 04:35:10 @agent_ppo2.py:150][0m Total time:       0.38 min
[32m[20230209 04:35:10 @agent_ppo2.py:152][0m 20480 total steps have happened
[32m[20230209 04:35:10 @agent_ppo2.py:128][0m #------------------------ Iteration 10 --------------------------#
[32m[20230209 04:35:10 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230209 04:35:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:10 @agent_ppo2.py:193][0m |          -0.0023 |         132.9647 |           0.0612 |
[32m[20230209 04:35:11 @agent_ppo2.py:193][0m |          -0.0131 |         112.1025 |           0.0612 |
[32m[20230209 04:35:11 @agent_ppo2.py:193][0m |          -0.0117 |         104.0215 |           0.0612 |
[32m[20230209 04:35:11 @agent_ppo2.py:193][0m |          -0.0055 |          98.5194 |           0.0612 |
[32m[20230209 04:35:11 @agent_ppo2.py:193][0m |          -0.0069 |          93.0284 |           0.0611 |
[32m[20230209 04:35:11 @agent_ppo2.py:193][0m |          -0.0108 |          87.1399 |           0.0611 |
[32m[20230209 04:35:11 @agent_ppo2.py:193][0m |          -0.0205 |          83.1715 |           0.0611 |
[32m[20230209 04:35:11 @agent_ppo2.py:193][0m |          -0.0159 |          79.3228 |           0.0611 |
[32m[20230209 04:35:11 @agent_ppo2.py:193][0m |          -0.0096 |          78.3475 |           0.0611 |
[32m[20230209 04:35:11 @agent_ppo2.py:193][0m |          -0.0084 |          76.3514 |           0.0611 |
[32m[20230209 04:35:11 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230209 04:35:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: -120.44
[32m[20230209 04:35:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.53
[32m[20230209 04:35:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -99.68
[32m[20230209 04:35:12 @agent_ppo2.py:150][0m Total time:       0.41 min
[32m[20230209 04:35:12 @agent_ppo2.py:152][0m 22528 total steps have happened
[32m[20230209 04:35:12 @agent_ppo2.py:128][0m #------------------------ Iteration 11 --------------------------#
[32m[20230209 04:35:13 @agent_ppo2.py:134][0m Sampling time: 0.95 s by 1 slaves
[32m[20230209 04:35:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:13 @agent_ppo2.py:193][0m |          -0.0002 |          56.3176 |           0.0617 |
[32m[20230209 04:35:13 @agent_ppo2.py:193][0m |          -0.0024 |          40.0028 |           0.0617 |
[32m[20230209 04:35:13 @agent_ppo2.py:193][0m |          -0.0049 |          32.5969 |           0.0616 |
[32m[20230209 04:35:13 @agent_ppo2.py:193][0m |          -0.0063 |          30.4761 |           0.0616 |
[32m[20230209 04:35:13 @agent_ppo2.py:193][0m |          -0.0072 |          29.6558 |           0.0615 |
[32m[20230209 04:35:13 @agent_ppo2.py:193][0m |          -0.0079 |          29.0458 |           0.0615 |
[32m[20230209 04:35:14 @agent_ppo2.py:193][0m |          -0.0085 |          28.6936 |           0.0615 |
[32m[20230209 04:35:14 @agent_ppo2.py:193][0m |          -0.0088 |          28.2333 |           0.0615 |
[32m[20230209 04:35:14 @agent_ppo2.py:193][0m |          -0.0090 |          27.8779 |           0.0614 |
[32m[20230209 04:35:14 @agent_ppo2.py:193][0m |          -0.0096 |          27.6161 |           0.0614 |
[32m[20230209 04:35:14 @agent_ppo2.py:137][0m Policy update time: 1.15 s
[32m[20230209 04:35:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: -115.87
[32m[20230209 04:35:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.99
[32m[20230209 04:35:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -99.79
[32m[20230209 04:35:15 @agent_ppo2.py:150][0m Total time:       0.46 min
[32m[20230209 04:35:15 @agent_ppo2.py:152][0m 24576 total steps have happened
[32m[20230209 04:35:15 @agent_ppo2.py:128][0m #------------------------ Iteration 12 --------------------------#
[32m[20230209 04:35:16 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230209 04:35:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:16 @agent_ppo2.py:193][0m |           0.0019 |         119.5212 |           0.0632 |
[32m[20230209 04:35:16 @agent_ppo2.py:193][0m |          -0.0025 |         100.7285 |           0.0632 |
[32m[20230209 04:35:16 @agent_ppo2.py:193][0m |          -0.0044 |          92.1125 |           0.0631 |
[32m[20230209 04:35:16 @agent_ppo2.py:193][0m |          -0.0104 |          84.1981 |           0.0631 |
[32m[20230209 04:35:16 @agent_ppo2.py:193][0m |          -0.0118 |          77.9638 |           0.0631 |
[32m[20230209 04:35:16 @agent_ppo2.py:193][0m |          -0.0060 |          73.8464 |           0.0630 |
[32m[20230209 04:35:16 @agent_ppo2.py:193][0m |          -0.0095 |          68.5746 |           0.0630 |
[32m[20230209 04:35:16 @agent_ppo2.py:193][0m |          -0.0169 |          63.3456 |           0.0630 |
[32m[20230209 04:35:16 @agent_ppo2.py:193][0m |          -0.0138 |          60.2244 |           0.0630 |
[32m[20230209 04:35:17 @agent_ppo2.py:193][0m |          -0.0159 |          56.4836 |           0.0630 |
[32m[20230209 04:35:17 @agent_ppo2.py:137][0m Policy update time: 1.02 s
[32m[20230209 04:35:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: -107.67
[32m[20230209 04:35:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.68
[32m[20230209 04:35:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -22.86
[32m[20230209 04:35:17 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -22.86
[32m[20230209 04:35:17 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -22.86
[32m[20230209 04:35:17 @agent_ppo2.py:150][0m Total time:       0.50 min
[32m[20230209 04:35:17 @agent_ppo2.py:152][0m 26624 total steps have happened
[32m[20230209 04:35:17 @agent_ppo2.py:128][0m #------------------------ Iteration 13 --------------------------#
[32m[20230209 04:35:18 @agent_ppo2.py:134][0m Sampling time: 0.93 s by 1 slaves
[32m[20230209 04:35:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:18 @agent_ppo2.py:193][0m |          -0.0011 |          28.6963 |           0.0620 |
[32m[20230209 04:35:19 @agent_ppo2.py:193][0m |           0.0009 |          18.5732 |           0.0620 |
[32m[20230209 04:35:19 @agent_ppo2.py:193][0m |          -0.0020 |          16.6049 |           0.0619 |
[32m[20230209 04:35:19 @agent_ppo2.py:193][0m |          -0.0041 |          15.4510 |           0.0618 |
[32m[20230209 04:35:19 @agent_ppo2.py:193][0m |          -0.0039 |          14.8162 |           0.0618 |
[32m[20230209 04:35:19 @agent_ppo2.py:193][0m |          -0.0058 |          14.2647 |           0.0617 |
[32m[20230209 04:35:19 @agent_ppo2.py:193][0m |          -0.0091 |          13.8875 |           0.0617 |
[32m[20230209 04:35:19 @agent_ppo2.py:193][0m |          -0.0076 |          13.6145 |           0.0616 |
[32m[20230209 04:35:19 @agent_ppo2.py:193][0m |          -0.0078 |          13.2583 |           0.0616 |
[32m[20230209 04:35:19 @agent_ppo2.py:193][0m |          -0.0074 |          13.0183 |           0.0615 |
[32m[20230209 04:35:19 @agent_ppo2.py:137][0m Policy update time: 1.12 s
[32m[20230209 04:35:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -126.94
[32m[20230209 04:35:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.06
[32m[20230209 04:35:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -97.67
[32m[20230209 04:35:20 @agent_ppo2.py:150][0m Total time:       0.55 min
[32m[20230209 04:35:20 @agent_ppo2.py:152][0m 28672 total steps have happened
[32m[20230209 04:35:20 @agent_ppo2.py:128][0m #------------------------ Iteration 14 --------------------------#
[32m[20230209 04:35:21 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230209 04:35:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:21 @agent_ppo2.py:193][0m |          -0.0036 |          34.9300 |           0.0618 |
[32m[20230209 04:35:21 @agent_ppo2.py:193][0m |          -0.0056 |          26.9810 |           0.0617 |
[32m[20230209 04:35:21 @agent_ppo2.py:193][0m |          -0.0052 |          25.3672 |           0.0617 |
[32m[20230209 04:35:21 @agent_ppo2.py:193][0m |          -0.0144 |          24.2796 |           0.0616 |
[32m[20230209 04:35:21 @agent_ppo2.py:193][0m |          -0.0074 |          23.7110 |           0.0616 |
[32m[20230209 04:35:21 @agent_ppo2.py:193][0m |          -0.0125 |          22.6857 |           0.0616 |
[32m[20230209 04:35:21 @agent_ppo2.py:193][0m |          -0.0008 |          23.5259 |           0.0615 |
[32m[20230209 04:35:21 @agent_ppo2.py:193][0m |          -0.0046 |          21.5181 |           0.0615 |
[32m[20230209 04:35:21 @agent_ppo2.py:193][0m |          -0.0057 |          20.9654 |           0.0614 |
[32m[20230209 04:35:21 @agent_ppo2.py:193][0m |          -0.0202 |          20.1437 |           0.0614 |
[32m[20230209 04:35:21 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230209 04:35:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.79
[32m[20230209 04:35:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -99.06
[32m[20230209 04:35:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -99.86
[32m[20230209 04:35:22 @agent_ppo2.py:150][0m Total time:       0.58 min
[32m[20230209 04:35:22 @agent_ppo2.py:152][0m 30720 total steps have happened
[32m[20230209 04:35:22 @agent_ppo2.py:128][0m #------------------------ Iteration 15 --------------------------#
[32m[20230209 04:35:22 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230209 04:35:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:23 @agent_ppo2.py:193][0m |          -0.0003 |          54.1359 |           0.0603 |
[32m[20230209 04:35:23 @agent_ppo2.py:193][0m |          -0.0030 |          27.2577 |           0.0603 |
[32m[20230209 04:35:23 @agent_ppo2.py:193][0m |          -0.0053 |          25.0588 |           0.0602 |
[32m[20230209 04:35:23 @agent_ppo2.py:193][0m |          -0.0076 |          23.6297 |           0.0602 |
[32m[20230209 04:35:23 @agent_ppo2.py:193][0m |          -0.0086 |          22.5460 |           0.0602 |
[32m[20230209 04:35:23 @agent_ppo2.py:193][0m |          -0.0087 |          21.8093 |           0.0602 |
[32m[20230209 04:35:23 @agent_ppo2.py:193][0m |          -0.0090 |          21.1202 |           0.0601 |
[32m[20230209 04:35:23 @agent_ppo2.py:193][0m |          -0.0110 |          20.2319 |           0.0601 |
[32m[20230209 04:35:23 @agent_ppo2.py:193][0m |          -0.0120 |          19.5289 |           0.0601 |
[32m[20230209 04:35:23 @agent_ppo2.py:193][0m |          -0.0122 |          18.9740 |           0.0601 |
[32m[20230209 04:35:23 @agent_ppo2.py:137][0m Policy update time: 0.69 s
[32m[20230209 04:35:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: -110.99
[32m[20230209 04:35:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -99.86
[32m[20230209 04:35:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -98.93
[32m[20230209 04:35:24 @agent_ppo2.py:150][0m Total time:       0.61 min
[32m[20230209 04:35:24 @agent_ppo2.py:152][0m 32768 total steps have happened
[32m[20230209 04:35:24 @agent_ppo2.py:128][0m #------------------------ Iteration 16 --------------------------#
[32m[20230209 04:35:25 @agent_ppo2.py:134][0m Sampling time: 0.98 s by 1 slaves
[32m[20230209 04:35:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:25 @agent_ppo2.py:193][0m |          -0.0007 |          15.0121 |           0.0634 |
[32m[20230209 04:35:25 @agent_ppo2.py:193][0m |          -0.0054 |           8.3966 |           0.0633 |
[32m[20230209 04:35:25 @agent_ppo2.py:193][0m |          -0.0073 |           8.1070 |           0.0633 |
[32m[20230209 04:35:25 @agent_ppo2.py:193][0m |          -0.0090 |           7.9702 |           0.0632 |
[32m[20230209 04:35:25 @agent_ppo2.py:193][0m |          -0.0095 |           7.8621 |           0.0633 |
[32m[20230209 04:35:25 @agent_ppo2.py:193][0m |          -0.0100 |           7.7829 |           0.0632 |
[32m[20230209 04:35:26 @agent_ppo2.py:193][0m |          -0.0106 |           7.6898 |           0.0632 |
[32m[20230209 04:35:26 @agent_ppo2.py:193][0m |          -0.0112 |           7.6626 |           0.0632 |
[32m[20230209 04:35:26 @agent_ppo2.py:193][0m |          -0.0114 |           7.5838 |           0.0632 |
[32m[20230209 04:35:26 @agent_ppo2.py:193][0m |          -0.0119 |           7.5755 |           0.0632 |
[32m[20230209 04:35:26 @agent_ppo2.py:137][0m Policy update time: 1.03 s
[32m[20230209 04:35:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -139.43
[32m[20230209 04:35:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.88
[32m[20230209 04:35:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -98.85
[32m[20230209 04:35:27 @agent_ppo2.py:150][0m Total time:       0.66 min
[32m[20230209 04:35:27 @agent_ppo2.py:152][0m 34816 total steps have happened
[32m[20230209 04:35:27 @agent_ppo2.py:128][0m #------------------------ Iteration 17 --------------------------#
[32m[20230209 04:35:27 @agent_ppo2.py:134][0m Sampling time: 0.71 s by 1 slaves
[32m[20230209 04:35:27 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:27 @agent_ppo2.py:193][0m |          -0.0011 |          45.5928 |           0.0619 |
[32m[20230209 04:35:27 @agent_ppo2.py:193][0m |          -0.0038 |          34.2907 |           0.0618 |
[32m[20230209 04:35:27 @agent_ppo2.py:193][0m |          -0.0007 |          31.6391 |           0.0618 |
[32m[20230209 04:35:28 @agent_ppo2.py:193][0m |          -0.0051 |          29.5840 |           0.0618 |
[32m[20230209 04:35:28 @agent_ppo2.py:193][0m |          -0.0112 |          27.7724 |           0.0618 |
[32m[20230209 04:35:28 @agent_ppo2.py:193][0m |          -0.0134 |          26.1619 |           0.0618 |
[32m[20230209 04:35:28 @agent_ppo2.py:193][0m |          -0.0136 |          24.1921 |           0.0617 |
[32m[20230209 04:35:28 @agent_ppo2.py:193][0m |          -0.0147 |          21.4012 |           0.0617 |
[32m[20230209 04:35:28 @agent_ppo2.py:193][0m |          -0.0118 |          19.6828 |           0.0617 |
[32m[20230209 04:35:28 @agent_ppo2.py:193][0m |          -0.0118 |          18.6425 |           0.0616 |
[32m[20230209 04:35:28 @agent_ppo2.py:137][0m Policy update time: 0.77 s
[32m[20230209 04:35:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -110.97
[32m[20230209 04:35:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.05
[32m[20230209 04:35:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -100.23
[32m[20230209 04:35:29 @agent_ppo2.py:150][0m Total time:       0.69 min
[32m[20230209 04:35:29 @agent_ppo2.py:152][0m 36864 total steps have happened
[32m[20230209 04:35:29 @agent_ppo2.py:128][0m #------------------------ Iteration 18 --------------------------#
[32m[20230209 04:35:30 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230209 04:35:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:30 @agent_ppo2.py:193][0m |           0.0003 |          20.8108 |           0.0624 |
[32m[20230209 04:35:30 @agent_ppo2.py:193][0m |          -0.0017 |           8.6116 |           0.0624 |
[32m[20230209 04:35:30 @agent_ppo2.py:193][0m |          -0.0032 |           7.0854 |           0.0624 |
[32m[20230209 04:35:30 @agent_ppo2.py:193][0m |          -0.0041 |           6.5877 |           0.0624 |
[32m[20230209 04:35:30 @agent_ppo2.py:193][0m |          -0.0063 |           6.2225 |           0.0623 |
[32m[20230209 04:35:30 @agent_ppo2.py:193][0m |          -0.0069 |           5.9451 |           0.0623 |
[32m[20230209 04:35:30 @agent_ppo2.py:193][0m |          -0.0082 |           5.7397 |           0.0623 |
[32m[20230209 04:35:30 @agent_ppo2.py:193][0m |          -0.0079 |           5.5588 |           0.0623 |
[32m[20230209 04:35:30 @agent_ppo2.py:193][0m |          -0.0091 |           5.4099 |           0.0623 |
[32m[20230209 04:35:30 @agent_ppo2.py:193][0m |          -0.0099 |           5.2470 |           0.0623 |
[32m[20230209 04:35:30 @agent_ppo2.py:137][0m Policy update time: 0.97 s
[32m[20230209 04:35:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -149.38
[32m[20230209 04:35:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.56
[32m[20230209 04:35:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -99.92
[32m[20230209 04:35:31 @agent_ppo2.py:150][0m Total time:       0.73 min
[32m[20230209 04:35:31 @agent_ppo2.py:152][0m 38912 total steps have happened
[32m[20230209 04:35:31 @agent_ppo2.py:128][0m #------------------------ Iteration 19 --------------------------#
[32m[20230209 04:35:32 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230209 04:35:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:32 @agent_ppo2.py:193][0m |           0.0045 |          37.5391 |           0.0612 |
[32m[20230209 04:35:32 @agent_ppo2.py:193][0m |          -0.0006 |          21.3212 |           0.0611 |
[32m[20230209 04:35:32 @agent_ppo2.py:193][0m |          -0.0076 |          16.6864 |           0.0611 |
[32m[20230209 04:35:32 @agent_ppo2.py:193][0m |          -0.0058 |          14.6117 |           0.0611 |
[32m[20230209 04:35:32 @agent_ppo2.py:193][0m |          -0.0034 |          13.6519 |           0.0611 |
[32m[20230209 04:35:32 @agent_ppo2.py:193][0m |          -0.0131 |          12.5731 |           0.0611 |
[32m[20230209 04:35:32 @agent_ppo2.py:193][0m |          -0.0141 |          11.7555 |           0.0612 |
[32m[20230209 04:35:32 @agent_ppo2.py:193][0m |          -0.0135 |          11.2764 |           0.0612 |
[32m[20230209 04:35:32 @agent_ppo2.py:193][0m |          -0.0109 |          11.0166 |           0.0612 |
[32m[20230209 04:35:32 @agent_ppo2.py:193][0m |          -0.0113 |          10.9296 |           0.0612 |
[32m[20230209 04:35:32 @agent_ppo2.py:137][0m Policy update time: 0.69 s
[32m[20230209 04:35:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: -108.92
[32m[20230209 04:35:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.35
[32m[20230209 04:35:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -100.16
[32m[20230209 04:35:33 @agent_ppo2.py:150][0m Total time:       0.76 min
[32m[20230209 04:35:33 @agent_ppo2.py:152][0m 40960 total steps have happened
[32m[20230209 04:35:33 @agent_ppo2.py:128][0m #------------------------ Iteration 20 --------------------------#
[32m[20230209 04:35:34 @agent_ppo2.py:134][0m Sampling time: 0.73 s by 1 slaves
[32m[20230209 04:35:34 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:34 @agent_ppo2.py:193][0m |           0.0012 |          39.7806 |           0.0612 |
[32m[20230209 04:35:34 @agent_ppo2.py:193][0m |          -0.0043 |          28.3406 |           0.0611 |
[32m[20230209 04:35:34 @agent_ppo2.py:193][0m |          -0.0054 |          24.4675 |           0.0612 |
[32m[20230209 04:35:34 @agent_ppo2.py:193][0m |          -0.0071 |          21.8730 |           0.0611 |
[32m[20230209 04:35:34 @agent_ppo2.py:193][0m |          -0.0111 |          20.1611 |           0.0611 |
[32m[20230209 04:35:34 @agent_ppo2.py:193][0m |          -0.0107 |          18.6952 |           0.0610 |
[32m[20230209 04:35:34 @agent_ppo2.py:193][0m |          -0.0118 |          17.6706 |           0.0610 |
[32m[20230209 04:35:34 @agent_ppo2.py:193][0m |          -0.0145 |          16.9113 |           0.0610 |
[32m[20230209 04:35:35 @agent_ppo2.py:193][0m |          -0.0131 |          15.9840 |           0.0610 |
[32m[20230209 04:35:35 @agent_ppo2.py:193][0m |          -0.0150 |          14.8326 |           0.0610 |
[32m[20230209 04:35:35 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230209 04:35:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: -108.94
[32m[20230209 04:35:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.01
[32m[20230209 04:35:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -100.32
[32m[20230209 04:35:35 @agent_ppo2.py:150][0m Total time:       0.80 min
[32m[20230209 04:35:35 @agent_ppo2.py:152][0m 43008 total steps have happened
[32m[20230209 04:35:35 @agent_ppo2.py:128][0m #------------------------ Iteration 21 --------------------------#
[32m[20230209 04:35:36 @agent_ppo2.py:134][0m Sampling time: 0.68 s by 1 slaves
[32m[20230209 04:35:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:36 @agent_ppo2.py:193][0m |          -0.0004 |          23.3874 |           0.0605 |
[32m[20230209 04:35:36 @agent_ppo2.py:193][0m |          -0.0039 |          11.1138 |           0.0605 |
[32m[20230209 04:35:36 @agent_ppo2.py:193][0m |          -0.0064 |          10.1713 |           0.0605 |
[32m[20230209 04:35:36 @agent_ppo2.py:193][0m |          -0.0081 |           9.8123 |           0.0605 |
[32m[20230209 04:35:37 @agent_ppo2.py:193][0m |          -0.0093 |           9.5848 |           0.0606 |
[32m[20230209 04:35:37 @agent_ppo2.py:193][0m |          -0.0103 |           9.4116 |           0.0606 |
[32m[20230209 04:35:37 @agent_ppo2.py:193][0m |          -0.0112 |           9.2537 |           0.0607 |
[32m[20230209 04:35:37 @agent_ppo2.py:193][0m |          -0.0118 |           9.1636 |           0.0607 |
[32m[20230209 04:35:37 @agent_ppo2.py:193][0m |          -0.0124 |           9.0608 |           0.0607 |
[32m[20230209 04:35:37 @agent_ppo2.py:193][0m |          -0.0130 |           9.0207 |           0.0608 |
[32m[20230209 04:35:37 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230209 04:35:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: -140.52
[32m[20230209 04:35:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.28
[32m[20230209 04:35:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -99.84
[32m[20230209 04:35:38 @agent_ppo2.py:150][0m Total time:       0.84 min
[32m[20230209 04:35:38 @agent_ppo2.py:152][0m 45056 total steps have happened
[32m[20230209 04:35:38 @agent_ppo2.py:128][0m #------------------------ Iteration 22 --------------------------#
[32m[20230209 04:35:38 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230209 04:35:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:38 @agent_ppo2.py:193][0m |          -0.0079 |          14.5374 |           0.0612 |
[32m[20230209 04:35:38 @agent_ppo2.py:193][0m |          -0.0001 |           8.3426 |           0.0612 |
[32m[20230209 04:35:38 @agent_ppo2.py:193][0m |          -0.0050 |           7.2564 |           0.0611 |
[32m[20230209 04:35:39 @agent_ppo2.py:193][0m |          -0.0039 |           6.7878 |           0.0610 |
[32m[20230209 04:35:39 @agent_ppo2.py:193][0m |          -0.0130 |           6.4546 |           0.0611 |
[32m[20230209 04:35:39 @agent_ppo2.py:193][0m |          -0.0056 |           6.1040 |           0.0611 |
[32m[20230209 04:35:39 @agent_ppo2.py:193][0m |          -0.0065 |           5.8819 |           0.0611 |
[32m[20230209 04:35:39 @agent_ppo2.py:193][0m |          -0.0114 |           5.6422 |           0.0611 |
[32m[20230209 04:35:39 @agent_ppo2.py:193][0m |          -0.0070 |           5.4952 |           0.0611 |
[32m[20230209 04:35:39 @agent_ppo2.py:193][0m |          -0.0189 |           5.2239 |           0.0611 |
[32m[20230209 04:35:39 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230209 04:35:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.79
[32m[20230209 04:35:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.71
[32m[20230209 04:35:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -96.32
[32m[20230209 04:35:39 @agent_ppo2.py:150][0m Total time:       0.87 min
[32m[20230209 04:35:39 @agent_ppo2.py:152][0m 47104 total steps have happened
[32m[20230209 04:35:39 @agent_ppo2.py:128][0m #------------------------ Iteration 23 --------------------------#
[32m[20230209 04:35:40 @agent_ppo2.py:134][0m Sampling time: 0.92 s by 1 slaves
[32m[20230209 04:35:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:40 @agent_ppo2.py:193][0m |          -0.0001 |           9.0458 |           0.0612 |
[32m[20230209 04:35:41 @agent_ppo2.py:193][0m |          -0.0044 |           4.8101 |           0.0611 |
[32m[20230209 04:35:41 @agent_ppo2.py:193][0m |          -0.0057 |           4.5917 |           0.0611 |
[32m[20230209 04:35:41 @agent_ppo2.py:193][0m |          -0.0087 |           4.4784 |           0.0610 |
[32m[20230209 04:35:41 @agent_ppo2.py:193][0m |          -0.0084 |           4.4110 |           0.0610 |
[32m[20230209 04:35:41 @agent_ppo2.py:193][0m |          -0.0164 |           4.6704 |           0.0610 |
[32m[20230209 04:35:41 @agent_ppo2.py:193][0m |          -0.0115 |           4.2384 |           0.0610 |
[32m[20230209 04:35:41 @agent_ppo2.py:193][0m |          -0.0135 |           4.1895 |           0.0610 |
[32m[20230209 04:35:41 @agent_ppo2.py:193][0m |          -0.0118 |           4.1995 |           0.0610 |
[32m[20230209 04:35:41 @agent_ppo2.py:193][0m |          -0.0116 |           4.0950 |           0.0610 |
[32m[20230209 04:35:41 @agent_ppo2.py:137][0m Policy update time: 1.06 s
[32m[20230209 04:35:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: -141.38
[32m[20230209 04:35:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -126.51
[32m[20230209 04:35:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -97.85
[32m[20230209 04:35:42 @agent_ppo2.py:150][0m Total time:       0.92 min
[32m[20230209 04:35:42 @agent_ppo2.py:152][0m 49152 total steps have happened
[32m[20230209 04:35:42 @agent_ppo2.py:128][0m #------------------------ Iteration 24 --------------------------#
[32m[20230209 04:35:43 @agent_ppo2.py:134][0m Sampling time: 0.98 s by 1 slaves
[32m[20230209 04:35:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:43 @agent_ppo2.py:193][0m |          -0.0004 |           8.4389 |           0.0642 |
[32m[20230209 04:35:44 @agent_ppo2.py:193][0m |          -0.0043 |           5.2040 |           0.0641 |
[32m[20230209 04:35:44 @agent_ppo2.py:193][0m |          -0.0087 |           4.7495 |           0.0641 |
[32m[20230209 04:35:44 @agent_ppo2.py:193][0m |          -0.0100 |           4.5347 |           0.0640 |
[32m[20230209 04:35:44 @agent_ppo2.py:193][0m |          -0.0111 |           4.3566 |           0.0639 |
[32m[20230209 04:35:44 @agent_ppo2.py:193][0m |          -0.0125 |           4.2331 |           0.0639 |
[32m[20230209 04:35:44 @agent_ppo2.py:193][0m |          -0.0137 |           4.1231 |           0.0638 |
[32m[20230209 04:35:44 @agent_ppo2.py:193][0m |          -0.0127 |           4.0632 |           0.0638 |
[32m[20230209 04:35:44 @agent_ppo2.py:193][0m |          -0.0137 |           4.0201 |           0.0637 |
[32m[20230209 04:35:44 @agent_ppo2.py:193][0m |          -0.0139 |           3.9602 |           0.0637 |
[32m[20230209 04:35:44 @agent_ppo2.py:137][0m Policy update time: 1.11 s
[32m[20230209 04:35:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: -117.02
[32m[20230209 04:35:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.88
[32m[20230209 04:35:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -85.65
[32m[20230209 04:35:45 @agent_ppo2.py:150][0m Total time:       0.97 min
[32m[20230209 04:35:45 @agent_ppo2.py:152][0m 51200 total steps have happened
[32m[20230209 04:35:45 @agent_ppo2.py:128][0m #------------------------ Iteration 25 --------------------------#
[32m[20230209 04:35:46 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230209 04:35:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:46 @agent_ppo2.py:193][0m |           0.0003 |          19.8827 |           0.0620 |
[32m[20230209 04:35:46 @agent_ppo2.py:193][0m |          -0.0003 |          11.2560 |           0.0620 |
[32m[20230209 04:35:46 @agent_ppo2.py:193][0m |          -0.0057 |          10.2417 |           0.0620 |
[32m[20230209 04:35:46 @agent_ppo2.py:193][0m |          -0.0085 |           9.2347 |           0.0619 |
[32m[20230209 04:35:46 @agent_ppo2.py:193][0m |          -0.0004 |           8.6908 |           0.0619 |
[32m[20230209 04:35:47 @agent_ppo2.py:193][0m |          -0.0069 |           8.4015 |           0.0618 |
[32m[20230209 04:35:47 @agent_ppo2.py:193][0m |          -0.0081 |           8.1909 |           0.0618 |
[32m[20230209 04:35:47 @agent_ppo2.py:193][0m |          -0.0072 |           7.9864 |           0.0618 |
[32m[20230209 04:35:47 @agent_ppo2.py:193][0m |          -0.0059 |           7.6504 |           0.0618 |
[32m[20230209 04:35:47 @agent_ppo2.py:193][0m |          -0.0056 |           7.7299 |           0.0617 |
[32m[20230209 04:35:47 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230209 04:35:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -159.53
[32m[20230209 04:35:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -139.29
[32m[20230209 04:35:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -101.33
[32m[20230209 04:35:48 @agent_ppo2.py:150][0m Total time:       1.01 min
[32m[20230209 04:35:48 @agent_ppo2.py:152][0m 53248 total steps have happened
[32m[20230209 04:35:48 @agent_ppo2.py:128][0m #------------------------ Iteration 26 --------------------------#
[32m[20230209 04:35:48 @agent_ppo2.py:134][0m Sampling time: 0.92 s by 1 slaves
[32m[20230209 04:35:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:49 @agent_ppo2.py:193][0m |          -0.0053 |          17.2599 |           0.0624 |
[32m[20230209 04:35:49 @agent_ppo2.py:193][0m |          -0.0040 |           7.3079 |           0.0624 |
[32m[20230209 04:35:49 @agent_ppo2.py:193][0m |          -0.0082 |           6.4557 |           0.0623 |
[32m[20230209 04:35:49 @agent_ppo2.py:193][0m |          -0.0136 |           6.3500 |           0.0624 |
[32m[20230209 04:35:49 @agent_ppo2.py:193][0m |          -0.0095 |           6.0903 |           0.0624 |
[32m[20230209 04:35:49 @agent_ppo2.py:193][0m |          -0.0098 |           5.9923 |           0.0624 |
[32m[20230209 04:35:49 @agent_ppo2.py:193][0m |          -0.0064 |           6.0473 |           0.0625 |
[32m[20230209 04:35:49 @agent_ppo2.py:193][0m |          -0.0112 |           5.8659 |           0.0625 |
[32m[20230209 04:35:49 @agent_ppo2.py:193][0m |          -0.0166 |           5.7600 |           0.0625 |
[32m[20230209 04:35:50 @agent_ppo2.py:193][0m |          -0.0148 |           5.6701 |           0.0625 |
[32m[20230209 04:35:50 @agent_ppo2.py:137][0m Policy update time: 1.05 s
[32m[20230209 04:35:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: -118.75
[32m[20230209 04:35:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.45
[32m[20230209 04:35:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -98.29
[32m[20230209 04:35:50 @agent_ppo2.py:150][0m Total time:       1.05 min
[32m[20230209 04:35:50 @agent_ppo2.py:152][0m 55296 total steps have happened
[32m[20230209 04:35:50 @agent_ppo2.py:128][0m #------------------------ Iteration 27 --------------------------#
[32m[20230209 04:35:51 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230209 04:35:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:51 @agent_ppo2.py:193][0m |           0.0001 |          15.5770 |           0.0632 |
[32m[20230209 04:35:51 @agent_ppo2.py:193][0m |          -0.0027 |           6.7154 |           0.0632 |
[32m[20230209 04:35:51 @agent_ppo2.py:193][0m |          -0.0057 |           5.4850 |           0.0631 |
[32m[20230209 04:35:52 @agent_ppo2.py:193][0m |          -0.0077 |           5.0528 |           0.0631 |
[32m[20230209 04:35:52 @agent_ppo2.py:193][0m |          -0.0089 |           4.9153 |           0.0630 |
[32m[20230209 04:35:52 @agent_ppo2.py:193][0m |          -0.0097 |           4.6526 |           0.0630 |
[32m[20230209 04:35:52 @agent_ppo2.py:193][0m |          -0.0103 |           4.5303 |           0.0630 |
[32m[20230209 04:35:52 @agent_ppo2.py:193][0m |          -0.0108 |           4.4202 |           0.0630 |
[32m[20230209 04:35:52 @agent_ppo2.py:193][0m |          -0.0113 |           4.3247 |           0.0630 |
[32m[20230209 04:35:52 @agent_ppo2.py:193][0m |          -0.0118 |           4.2456 |           0.0630 |
[32m[20230209 04:35:52 @agent_ppo2.py:137][0m Policy update time: 0.93 s
[32m[20230209 04:35:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -149.98
[32m[20230209 04:35:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.97
[32m[20230209 04:35:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -112.34
[32m[20230209 04:35:53 @agent_ppo2.py:150][0m Total time:       1.10 min
[32m[20230209 04:35:53 @agent_ppo2.py:152][0m 57344 total steps have happened
[32m[20230209 04:35:53 @agent_ppo2.py:128][0m #------------------------ Iteration 28 --------------------------#
[32m[20230209 04:35:54 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230209 04:35:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:54 @agent_ppo2.py:193][0m |           0.0045 |           9.2290 |           0.0621 |
[32m[20230209 04:35:54 @agent_ppo2.py:193][0m |          -0.0035 |           5.3987 |           0.0621 |
[32m[20230209 04:35:54 @agent_ppo2.py:193][0m |          -0.0070 |           4.8636 |           0.0621 |
[32m[20230209 04:35:54 @agent_ppo2.py:193][0m |          -0.0011 |           4.7272 |           0.0621 |
[32m[20230209 04:35:54 @agent_ppo2.py:193][0m |          -0.0054 |           4.2668 |           0.0621 |
[32m[20230209 04:35:54 @agent_ppo2.py:193][0m |          -0.0092 |           4.1350 |           0.0621 |
[32m[20230209 04:35:54 @agent_ppo2.py:193][0m |          -0.0065 |           3.9777 |           0.0621 |
[32m[20230209 04:35:54 @agent_ppo2.py:193][0m |          -0.0231 |           3.8485 |           0.0622 |
[32m[20230209 04:35:54 @agent_ppo2.py:193][0m |          -0.0017 |           3.9445 |           0.0622 |
[32m[20230209 04:35:54 @agent_ppo2.py:193][0m |          -0.0143 |           3.7013 |           0.0622 |
[32m[20230209 04:35:54 @agent_ppo2.py:137][0m Policy update time: 0.79 s
[32m[20230209 04:35:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: -147.34
[32m[20230209 04:35:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -110.22
[32m[20230209 04:35:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -111.08
[32m[20230209 04:35:55 @agent_ppo2.py:150][0m Total time:       1.13 min
[32m[20230209 04:35:55 @agent_ppo2.py:152][0m 59392 total steps have happened
[32m[20230209 04:35:55 @agent_ppo2.py:128][0m #------------------------ Iteration 29 --------------------------#
[32m[20230209 04:35:56 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230209 04:35:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:56 @agent_ppo2.py:193][0m |           0.0001 |           5.9178 |           0.0619 |
[32m[20230209 04:35:56 @agent_ppo2.py:193][0m |          -0.0048 |           3.2968 |           0.0619 |
[32m[20230209 04:35:56 @agent_ppo2.py:193][0m |          -0.0080 |           3.0923 |           0.0618 |
[32m[20230209 04:35:56 @agent_ppo2.py:193][0m |          -0.0083 |           2.9897 |           0.0618 |
[32m[20230209 04:35:57 @agent_ppo2.py:193][0m |          -0.0098 |           2.9469 |           0.0618 |
[32m[20230209 04:35:57 @agent_ppo2.py:193][0m |          -0.0114 |           2.8762 |           0.0617 |
[32m[20230209 04:35:57 @agent_ppo2.py:193][0m |          -0.0110 |           2.8667 |           0.0617 |
[32m[20230209 04:35:57 @agent_ppo2.py:193][0m |          -0.0107 |           2.8363 |           0.0617 |
[32m[20230209 04:35:57 @agent_ppo2.py:193][0m |          -0.0127 |           2.7819 |           0.0617 |
[32m[20230209 04:35:57 @agent_ppo2.py:193][0m |          -0.0134 |           2.7820 |           0.0617 |
[32m[20230209 04:35:57 @agent_ppo2.py:137][0m Policy update time: 0.95 s
[32m[20230209 04:35:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: -130.21
[32m[20230209 04:35:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.62
[32m[20230209 04:35:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -83.67
[32m[20230209 04:35:58 @agent_ppo2.py:150][0m Total time:       1.18 min
[32m[20230209 04:35:58 @agent_ppo2.py:152][0m 61440 total steps have happened
[32m[20230209 04:35:58 @agent_ppo2.py:128][0m #------------------------ Iteration 30 --------------------------#
[32m[20230209 04:35:58 @agent_ppo2.py:134][0m Sampling time: 0.61 s by 1 slaves
[32m[20230209 04:35:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:35:59 @agent_ppo2.py:193][0m |           0.0143 |          16.9781 |           0.0614 |
[32m[20230209 04:35:59 @agent_ppo2.py:193][0m |           0.0057 |           9.7118 |           0.0614 |
[32m[20230209 04:35:59 @agent_ppo2.py:193][0m |          -0.0094 |           8.1558 |           0.0614 |
[32m[20230209 04:35:59 @agent_ppo2.py:193][0m |          -0.0175 |           7.0554 |           0.0614 |
[32m[20230209 04:35:59 @agent_ppo2.py:193][0m |          -0.0138 |           6.8390 |           0.0614 |
[32m[20230209 04:35:59 @agent_ppo2.py:193][0m |          -0.0149 |           6.4125 |           0.0614 |
[32m[20230209 04:35:59 @agent_ppo2.py:193][0m |          -0.0137 |           6.2581 |           0.0614 |
[32m[20230209 04:35:59 @agent_ppo2.py:193][0m |          -0.0159 |           5.8582 |           0.0614 |
[32m[20230209 04:35:59 @agent_ppo2.py:193][0m |          -0.0127 |           5.7883 |           0.0614 |
[32m[20230209 04:35:59 @agent_ppo2.py:193][0m |          -0.0170 |           5.7459 |           0.0614 |
[32m[20230209 04:35:59 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230209 04:36:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.20
[32m[20230209 04:36:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -99.48
[32m[20230209 04:36:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -78.85
[32m[20230209 04:36:00 @agent_ppo2.py:150][0m Total time:       1.21 min
[32m[20230209 04:36:00 @agent_ppo2.py:152][0m 63488 total steps have happened
[32m[20230209 04:36:00 @agent_ppo2.py:128][0m #------------------------ Iteration 31 --------------------------#
[32m[20230209 04:36:01 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230209 04:36:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:01 @agent_ppo2.py:193][0m |           0.0008 |           9.8195 |           0.0641 |
[32m[20230209 04:36:01 @agent_ppo2.py:193][0m |          -0.0045 |           5.4162 |           0.0641 |
[32m[20230209 04:36:01 @agent_ppo2.py:193][0m |          -0.0065 |           4.5283 |           0.0641 |
[32m[20230209 04:36:01 @agent_ppo2.py:193][0m |          -0.0079 |           4.0874 |           0.0640 |
[32m[20230209 04:36:01 @agent_ppo2.py:193][0m |          -0.0069 |           3.8144 |           0.0640 |
[32m[20230209 04:36:01 @agent_ppo2.py:193][0m |          -0.0098 |           3.5079 |           0.0640 |
[32m[20230209 04:36:01 @agent_ppo2.py:193][0m |          -0.0088 |           3.3613 |           0.0640 |
[32m[20230209 04:36:02 @agent_ppo2.py:193][0m |          -0.0057 |           3.2751 |           0.0640 |
[32m[20230209 04:36:02 @agent_ppo2.py:193][0m |          -0.0102 |           3.2505 |           0.0641 |
[32m[20230209 04:36:02 @agent_ppo2.py:193][0m |          -0.0061 |           3.0655 |           0.0641 |
[32m[20230209 04:36:02 @agent_ppo2.py:137][0m Policy update time: 0.92 s
[32m[20230209 04:36:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -137.74
[32m[20230209 04:36:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.42
[32m[20230209 04:36:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -88.20
[32m[20230209 04:36:03 @agent_ppo2.py:150][0m Total time:       1.26 min
[32m[20230209 04:36:03 @agent_ppo2.py:152][0m 65536 total steps have happened
[32m[20230209 04:36:03 @agent_ppo2.py:128][0m #------------------------ Iteration 32 --------------------------#
[32m[20230209 04:36:04 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230209 04:36:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:04 @agent_ppo2.py:193][0m |          -0.0006 |          25.9360 |           0.0641 |
[32m[20230209 04:36:04 @agent_ppo2.py:193][0m |          -0.0024 |          13.3549 |           0.0641 |
[32m[20230209 04:36:04 @agent_ppo2.py:193][0m |          -0.0050 |          11.3424 |           0.0640 |
[32m[20230209 04:36:04 @agent_ppo2.py:193][0m |          -0.0071 |          10.3242 |           0.0640 |
[32m[20230209 04:36:04 @agent_ppo2.py:193][0m |          -0.0075 |           9.8110 |           0.0639 |
[32m[20230209 04:36:04 @agent_ppo2.py:193][0m |          -0.0086 |           9.4425 |           0.0639 |
[32m[20230209 04:36:04 @agent_ppo2.py:193][0m |          -0.0100 |           9.3064 |           0.0639 |
[32m[20230209 04:36:04 @agent_ppo2.py:193][0m |          -0.0110 |           9.0161 |           0.0639 |
[32m[20230209 04:36:04 @agent_ppo2.py:193][0m |          -0.0105 |           8.7236 |           0.0639 |
[32m[20230209 04:36:04 @agent_ppo2.py:193][0m |          -0.0111 |           8.3945 |           0.0638 |
[32m[20230209 04:36:04 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230209 04:36:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -119.64
[32m[20230209 04:36:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -99.34
[32m[20230209 04:36:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -116.91
[32m[20230209 04:36:05 @agent_ppo2.py:150][0m Total time:       1.30 min
[32m[20230209 04:36:05 @agent_ppo2.py:152][0m 67584 total steps have happened
[32m[20230209 04:36:05 @agent_ppo2.py:128][0m #------------------------ Iteration 33 --------------------------#
[32m[20230209 04:36:06 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230209 04:36:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:06 @agent_ppo2.py:193][0m |          -0.0038 |          46.0202 |           0.0636 |
[32m[20230209 04:36:06 @agent_ppo2.py:193][0m |          -0.0055 |          16.0179 |           0.0635 |
[32m[20230209 04:36:06 @agent_ppo2.py:193][0m |          -0.0090 |          11.0385 |           0.0635 |
[32m[20230209 04:36:06 @agent_ppo2.py:193][0m |          -0.0107 |           8.6827 |           0.0634 |
[32m[20230209 04:36:06 @agent_ppo2.py:193][0m |          -0.0113 |           7.4635 |           0.0633 |
[32m[20230209 04:36:06 @agent_ppo2.py:193][0m |          -0.0159 |           6.5565 |           0.0633 |
[32m[20230209 04:36:06 @agent_ppo2.py:193][0m |          -0.0128 |           6.2100 |           0.0632 |
[32m[20230209 04:36:06 @agent_ppo2.py:193][0m |          -0.0122 |           5.7950 |           0.0633 |
[32m[20230209 04:36:06 @agent_ppo2.py:193][0m |          -0.0159 |           5.4866 |           0.0632 |
[32m[20230209 04:36:06 @agent_ppo2.py:193][0m |          -0.0149 |           5.2490 |           0.0632 |
[32m[20230209 04:36:06 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230209 04:36:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -112.47
[32m[20230209 04:36:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.35
[32m[20230209 04:36:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -125.73
[32m[20230209 04:36:07 @agent_ppo2.py:150][0m Total time:       1.33 min
[32m[20230209 04:36:07 @agent_ppo2.py:152][0m 69632 total steps have happened
[32m[20230209 04:36:07 @agent_ppo2.py:128][0m #------------------------ Iteration 34 --------------------------#
[32m[20230209 04:36:08 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230209 04:36:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:08 @agent_ppo2.py:193][0m |          -0.0033 |          16.1770 |           0.0621 |
[32m[20230209 04:36:08 @agent_ppo2.py:193][0m |          -0.0103 |           8.2019 |           0.0620 |
[32m[20230209 04:36:08 @agent_ppo2.py:193][0m |          -0.0142 |           6.6334 |           0.0619 |
[32m[20230209 04:36:08 @agent_ppo2.py:193][0m |          -0.0309 |           6.1855 |           0.0619 |
[32m[20230209 04:36:08 @agent_ppo2.py:193][0m |          -0.0270 |           5.4800 |           0.0619 |
[32m[20230209 04:36:08 @agent_ppo2.py:193][0m |          -0.0097 |           5.2874 |           0.0618 |
[32m[20230209 04:36:08 @agent_ppo2.py:193][0m |          -0.0528 |           4.9430 |           0.0618 |
[32m[20230209 04:36:08 @agent_ppo2.py:193][0m |          -0.0298 |           5.1659 |           0.0618 |
[32m[20230209 04:36:08 @agent_ppo2.py:193][0m |          -0.0403 |           4.8102 |           0.0618 |
[32m[20230209 04:36:08 @agent_ppo2.py:193][0m |          -0.0026 |           5.6152 |           0.0618 |
[32m[20230209 04:36:08 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230209 04:36:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: -112.26
[32m[20230209 04:36:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.20
[32m[20230209 04:36:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -122.38
[32m[20230209 04:36:09 @agent_ppo2.py:150][0m Total time:       1.36 min
[32m[20230209 04:36:09 @agent_ppo2.py:152][0m 71680 total steps have happened
[32m[20230209 04:36:09 @agent_ppo2.py:128][0m #------------------------ Iteration 35 --------------------------#
[32m[20230209 04:36:10 @agent_ppo2.py:134][0m Sampling time: 0.97 s by 1 slaves
[32m[20230209 04:36:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:10 @agent_ppo2.py:193][0m |          -0.0004 |          24.3887 |           0.0623 |
[32m[20230209 04:36:10 @agent_ppo2.py:193][0m |          -0.0053 |          14.9748 |           0.0622 |
[32m[20230209 04:36:10 @agent_ppo2.py:193][0m |          -0.0085 |          11.8285 |           0.0621 |
[32m[20230209 04:36:10 @agent_ppo2.py:193][0m |          -0.0088 |          10.3009 |           0.0620 |
[32m[20230209 04:36:10 @agent_ppo2.py:193][0m |          -0.0107 |           9.2256 |           0.0620 |
[32m[20230209 04:36:10 @agent_ppo2.py:193][0m |          -0.0113 |           8.8673 |           0.0620 |
[32m[20230209 04:36:11 @agent_ppo2.py:193][0m |          -0.0124 |           8.2819 |           0.0620 |
[32m[20230209 04:36:11 @agent_ppo2.py:193][0m |          -0.0122 |           7.9721 |           0.0619 |
[32m[20230209 04:36:11 @agent_ppo2.py:193][0m |          -0.0136 |           7.7747 |           0.0619 |
[32m[20230209 04:36:11 @agent_ppo2.py:193][0m |          -0.0146 |           7.6257 |           0.0619 |
[32m[20230209 04:36:11 @agent_ppo2.py:137][0m Policy update time: 0.97 s
[32m[20230209 04:36:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: -125.13
[32m[20230209 04:36:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.25
[32m[20230209 04:36:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -110.34
[32m[20230209 04:36:12 @agent_ppo2.py:150][0m Total time:       1.41 min
[32m[20230209 04:36:12 @agent_ppo2.py:152][0m 73728 total steps have happened
[32m[20230209 04:36:12 @agent_ppo2.py:128][0m #------------------------ Iteration 36 --------------------------#
[32m[20230209 04:36:13 @agent_ppo2.py:134][0m Sampling time: 0.93 s by 1 slaves
[32m[20230209 04:36:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:13 @agent_ppo2.py:193][0m |          -0.0021 |           9.2895 |           0.0634 |
[32m[20230209 04:36:13 @agent_ppo2.py:193][0m |           0.0255 |           5.9509 |           0.0634 |
[32m[20230209 04:36:13 @agent_ppo2.py:193][0m |          -0.0085 |           5.5460 |           0.0634 |
[32m[20230209 04:36:13 @agent_ppo2.py:193][0m |          -0.0161 |           5.2598 |           0.0634 |
[32m[20230209 04:36:13 @agent_ppo2.py:193][0m |          -0.0079 |           5.1027 |           0.0634 |
[32m[20230209 04:36:13 @agent_ppo2.py:193][0m |          -0.0122 |           5.0151 |           0.0634 |
[32m[20230209 04:36:13 @agent_ppo2.py:193][0m |          -0.0074 |           4.8505 |           0.0634 |
[32m[20230209 04:36:13 @agent_ppo2.py:193][0m |          -0.0231 |           4.7799 |           0.0634 |
[32m[20230209 04:36:13 @agent_ppo2.py:193][0m |          -0.0116 |           4.8201 |           0.0635 |
[32m[20230209 04:36:13 @agent_ppo2.py:193][0m |          -0.0106 |           4.7765 |           0.0635 |
[32m[20230209 04:36:13 @agent_ppo2.py:137][0m Policy update time: 0.94 s
[32m[20230209 04:36:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: -116.99
[32m[20230209 04:36:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -106.01
[32m[20230209 04:36:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -125.61
[32m[20230209 04:36:14 @agent_ppo2.py:150][0m Total time:       1.45 min
[32m[20230209 04:36:14 @agent_ppo2.py:152][0m 75776 total steps have happened
[32m[20230209 04:36:14 @agent_ppo2.py:128][0m #------------------------ Iteration 37 --------------------------#
[32m[20230209 04:36:15 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230209 04:36:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:15 @agent_ppo2.py:193][0m |           0.0023 |          21.5252 |           0.0628 |
[32m[20230209 04:36:15 @agent_ppo2.py:193][0m |           0.0180 |           8.3175 |           0.0628 |
[32m[20230209 04:36:15 @agent_ppo2.py:193][0m |          -0.0053 |           5.4827 |           0.0628 |
[32m[20230209 04:36:15 @agent_ppo2.py:193][0m |           0.0210 |           4.7597 |           0.0628 |
[32m[20230209 04:36:15 @agent_ppo2.py:193][0m |           0.0231 |           4.3649 |           0.0628 |
[32m[20230209 04:36:15 @agent_ppo2.py:193][0m |          -0.0114 |           3.7784 |           0.0628 |
[32m[20230209 04:36:15 @agent_ppo2.py:193][0m |          -0.0144 |           3.0867 |           0.0628 |
[32m[20230209 04:36:15 @agent_ppo2.py:193][0m |          -0.0180 |           2.9132 |           0.0627 |
[32m[20230209 04:36:15 @agent_ppo2.py:193][0m |          -0.0154 |           2.7189 |           0.0627 |
[32m[20230209 04:36:15 @agent_ppo2.py:193][0m |          -0.0204 |           2.4448 |           0.0627 |
[32m[20230209 04:36:15 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230209 04:36:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -119.44
[32m[20230209 04:36:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.91
[32m[20230209 04:36:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -124.26
[32m[20230209 04:36:16 @agent_ppo2.py:150][0m Total time:       1.48 min
[32m[20230209 04:36:16 @agent_ppo2.py:152][0m 77824 total steps have happened
[32m[20230209 04:36:16 @agent_ppo2.py:128][0m #------------------------ Iteration 38 --------------------------#
[32m[20230209 04:36:17 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230209 04:36:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:17 @agent_ppo2.py:193][0m |           0.0090 |          45.3558 |           0.0627 |
[32m[20230209 04:36:17 @agent_ppo2.py:193][0m |          -0.0021 |          17.6262 |           0.0626 |
[32m[20230209 04:36:17 @agent_ppo2.py:193][0m |          -0.0091 |          12.5219 |           0.0626 |
[32m[20230209 04:36:17 @agent_ppo2.py:193][0m |          -0.0063 |          10.2341 |           0.0625 |
[32m[20230209 04:36:17 @agent_ppo2.py:193][0m |          -0.0093 |           8.7591 |           0.0625 |
[32m[20230209 04:36:17 @agent_ppo2.py:193][0m |          -0.0032 |           7.9285 |           0.0624 |
[32m[20230209 04:36:17 @agent_ppo2.py:193][0m |          -0.0110 |           7.1127 |           0.0624 |
[32m[20230209 04:36:17 @agent_ppo2.py:193][0m |          -0.0113 |           6.8424 |           0.0624 |
[32m[20230209 04:36:17 @agent_ppo2.py:193][0m |          -0.0173 |           6.4847 |           0.0624 |
[32m[20230209 04:36:18 @agent_ppo2.py:193][0m |          -0.0089 |           6.4894 |           0.0624 |
[32m[20230209 04:36:18 @agent_ppo2.py:137][0m Policy update time: 0.70 s
[32m[20230209 04:36:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -122.90
[32m[20230209 04:36:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.74
[32m[20230209 04:36:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -114.62
[32m[20230209 04:36:18 @agent_ppo2.py:150][0m Total time:       1.52 min
[32m[20230209 04:36:18 @agent_ppo2.py:152][0m 79872 total steps have happened
[32m[20230209 04:36:18 @agent_ppo2.py:128][0m #------------------------ Iteration 39 --------------------------#
[32m[20230209 04:36:19 @agent_ppo2.py:134][0m Sampling time: 0.93 s by 1 slaves
[32m[20230209 04:36:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:19 @agent_ppo2.py:193][0m |           0.0107 |          12.3376 |           0.0625 |
[32m[20230209 04:36:20 @agent_ppo2.py:193][0m |          -0.0031 |           6.7091 |           0.0625 |
[32m[20230209 04:36:20 @agent_ppo2.py:193][0m |          -0.0108 |           6.2524 |           0.0625 |
[32m[20230209 04:36:20 @agent_ppo2.py:193][0m |          -0.0050 |           6.0454 |           0.0624 |
[32m[20230209 04:36:20 @agent_ppo2.py:193][0m |          -0.0028 |           5.9573 |           0.0624 |
[32m[20230209 04:36:20 @agent_ppo2.py:193][0m |          -0.0122 |           6.0186 |           0.0624 |
[32m[20230209 04:36:20 @agent_ppo2.py:193][0m |          -0.0064 |           5.8552 |           0.0624 |
[32m[20230209 04:36:20 @agent_ppo2.py:193][0m |          -0.0103 |           5.7710 |           0.0624 |
[32m[20230209 04:36:20 @agent_ppo2.py:193][0m |          -0.0148 |           5.7393 |           0.0625 |
[32m[20230209 04:36:20 @agent_ppo2.py:193][0m |          -0.0093 |           5.7122 |           0.0625 |
[32m[20230209 04:36:20 @agent_ppo2.py:137][0m Policy update time: 1.00 s
[32m[20230209 04:36:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -125.26
[32m[20230209 04:36:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -116.15
[32m[20230209 04:36:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -122.98
[32m[20230209 04:36:21 @agent_ppo2.py:150][0m Total time:       1.56 min
[32m[20230209 04:36:21 @agent_ppo2.py:152][0m 81920 total steps have happened
[32m[20230209 04:36:21 @agent_ppo2.py:128][0m #------------------------ Iteration 40 --------------------------#
[32m[20230209 04:36:22 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230209 04:36:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:22 @agent_ppo2.py:193][0m |          -0.0013 |          33.4241 |           0.0616 |
[32m[20230209 04:36:22 @agent_ppo2.py:193][0m |           0.0028 |          18.7439 |           0.0616 |
[32m[20230209 04:36:22 @agent_ppo2.py:193][0m |          -0.0140 |          14.2860 |           0.0616 |
[32m[20230209 04:36:22 @agent_ppo2.py:193][0m |           0.0000 |          11.9519 |           0.0616 |
[32m[20230209 04:36:22 @agent_ppo2.py:193][0m |          -0.0127 |          10.0392 |           0.0616 |
[32m[20230209 04:36:22 @agent_ppo2.py:193][0m |          -0.0270 |           9.2775 |           0.0616 |
[32m[20230209 04:36:22 @agent_ppo2.py:193][0m |          -0.0094 |           8.3553 |           0.0616 |
[32m[20230209 04:36:22 @agent_ppo2.py:193][0m |          -0.0215 |           7.7716 |           0.0616 |
[32m[20230209 04:36:22 @agent_ppo2.py:193][0m |          -0.0198 |           7.3974 |           0.0616 |
[32m[20230209 04:36:22 @agent_ppo2.py:193][0m |          -0.0317 |           6.8975 |           0.0616 |
[32m[20230209 04:36:22 @agent_ppo2.py:137][0m Policy update time: 0.67 s
[32m[20230209 04:36:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -139.69
[32m[20230209 04:36:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -110.71
[32m[20230209 04:36:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -113.89
[32m[20230209 04:36:23 @agent_ppo2.py:150][0m Total time:       1.59 min
[32m[20230209 04:36:23 @agent_ppo2.py:152][0m 83968 total steps have happened
[32m[20230209 04:36:23 @agent_ppo2.py:128][0m #------------------------ Iteration 41 --------------------------#
[32m[20230209 04:36:24 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230209 04:36:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:24 @agent_ppo2.py:193][0m |          -0.0103 |           4.0338 |           0.0637 |
[32m[20230209 04:36:24 @agent_ppo2.py:193][0m |           0.0082 |           2.6418 |           0.0635 |
[32m[20230209 04:36:24 @agent_ppo2.py:193][0m |          -0.0240 |           2.5447 |           0.0634 |
[32m[20230209 04:36:24 @agent_ppo2.py:193][0m |          -0.0050 |           2.4410 |           0.0634 |
[32m[20230209 04:36:24 @agent_ppo2.py:193][0m |          -0.0129 |           2.3176 |           0.0633 |
[32m[20230209 04:36:24 @agent_ppo2.py:193][0m |          -0.0076 |           2.3325 |           0.0634 |
[32m[20230209 04:36:24 @agent_ppo2.py:193][0m |           0.0108 |           2.3193 |           0.0634 |
[32m[20230209 04:36:24 @agent_ppo2.py:193][0m |           0.0008 |           2.2245 |           0.0633 |
[32m[20230209 04:36:24 @agent_ppo2.py:193][0m |          -0.0044 |           2.1768 |           0.0633 |
[32m[20230209 04:36:25 @agent_ppo2.py:193][0m |          -0.0039 |           2.1617 |           0.0633 |
[32m[20230209 04:36:25 @agent_ppo2.py:137][0m Policy update time: 0.94 s
[32m[20230209 04:36:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -145.03
[32m[20230209 04:36:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -141.92
[32m[20230209 04:36:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -119.23
[32m[20230209 04:36:25 @agent_ppo2.py:150][0m Total time:       1.63 min
[32m[20230209 04:36:25 @agent_ppo2.py:152][0m 86016 total steps have happened
[32m[20230209 04:36:25 @agent_ppo2.py:128][0m #------------------------ Iteration 42 --------------------------#
[32m[20230209 04:36:26 @agent_ppo2.py:134][0m Sampling time: 0.92 s by 1 slaves
[32m[20230209 04:36:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:26 @agent_ppo2.py:193][0m |          -0.0007 |          23.9843 |           0.0644 |
[32m[20230209 04:36:26 @agent_ppo2.py:193][0m |          -0.0038 |          16.4258 |           0.0644 |
[32m[20230209 04:36:26 @agent_ppo2.py:193][0m |          -0.0052 |          15.3948 |           0.0644 |
[32m[20230209 04:36:26 @agent_ppo2.py:193][0m |          -0.0064 |          14.5833 |           0.0644 |
[32m[20230209 04:36:27 @agent_ppo2.py:193][0m |          -0.0068 |          14.1368 |           0.0644 |
[32m[20230209 04:36:27 @agent_ppo2.py:193][0m |          -0.0078 |          13.7080 |           0.0644 |
[32m[20230209 04:36:27 @agent_ppo2.py:193][0m |          -0.0085 |          13.4001 |           0.0644 |
[32m[20230209 04:36:27 @agent_ppo2.py:193][0m |          -0.0082 |          13.3207 |           0.0644 |
[32m[20230209 04:36:27 @agent_ppo2.py:193][0m |          -0.0088 |          13.1510 |           0.0644 |
[32m[20230209 04:36:27 @agent_ppo2.py:193][0m |          -0.0087 |          12.7304 |           0.0644 |
[32m[20230209 04:36:27 @agent_ppo2.py:137][0m Policy update time: 1.05 s
[32m[20230209 04:36:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: -117.89
[32m[20230209 04:36:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.82
[32m[20230209 04:36:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -120.44
[32m[20230209 04:36:28 @agent_ppo2.py:150][0m Total time:       1.68 min
[32m[20230209 04:36:28 @agent_ppo2.py:152][0m 88064 total steps have happened
[32m[20230209 04:36:28 @agent_ppo2.py:128][0m #------------------------ Iteration 43 --------------------------#
[32m[20230209 04:36:29 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230209 04:36:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:29 @agent_ppo2.py:193][0m |          -0.0159 |           3.3981 |           0.0634 |
[32m[20230209 04:36:29 @agent_ppo2.py:193][0m |          -0.0101 |           1.9208 |           0.0632 |
[32m[20230209 04:36:29 @agent_ppo2.py:193][0m |          -0.0125 |           1.8353 |           0.0632 |
[32m[20230209 04:36:29 @agent_ppo2.py:193][0m |          -0.0268 |           1.8034 |           0.0632 |
[32m[20230209 04:36:29 @agent_ppo2.py:193][0m |           0.0013 |           1.7773 |           0.0632 |
[32m[20230209 04:36:29 @agent_ppo2.py:193][0m |          -0.0016 |           1.7906 |           0.0632 |
[32m[20230209 04:36:29 @agent_ppo2.py:193][0m |           0.0004 |           1.8113 |           0.0632 |
[32m[20230209 04:36:29 @agent_ppo2.py:193][0m |          -0.0292 |           1.7441 |           0.0632 |
[32m[20230209 04:36:30 @agent_ppo2.py:193][0m |          -0.0122 |           1.7084 |           0.0633 |
[32m[20230209 04:36:30 @agent_ppo2.py:193][0m |          -0.0037 |           1.7140 |           0.0633 |
[32m[20230209 04:36:30 @agent_ppo2.py:137][0m Policy update time: 0.98 s
[32m[20230209 04:36:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -153.53
[32m[20230209 04:36:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -150.42
[32m[20230209 04:36:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -93.46
[32m[20230209 04:36:30 @agent_ppo2.py:150][0m Total time:       1.72 min
[32m[20230209 04:36:30 @agent_ppo2.py:152][0m 90112 total steps have happened
[32m[20230209 04:36:30 @agent_ppo2.py:128][0m #------------------------ Iteration 44 --------------------------#
[32m[20230209 04:36:31 @agent_ppo2.py:134][0m Sampling time: 1.00 s by 1 slaves
[32m[20230209 04:36:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:32 @agent_ppo2.py:193][0m |          -0.0032 |          19.6656 |           0.0644 |
[32m[20230209 04:36:32 @agent_ppo2.py:193][0m |          -0.0079 |           9.4726 |           0.0643 |
[32m[20230209 04:36:32 @agent_ppo2.py:193][0m |          -0.0081 |           8.0532 |           0.0643 |
[32m[20230209 04:36:32 @agent_ppo2.py:193][0m |          -0.0127 |           7.5595 |           0.0642 |
[32m[20230209 04:36:32 @agent_ppo2.py:193][0m |          -0.0088 |           7.2806 |           0.0642 |
[32m[20230209 04:36:32 @agent_ppo2.py:193][0m |          -0.0191 |           7.1470 |           0.0641 |
[32m[20230209 04:36:32 @agent_ppo2.py:193][0m |          -0.0054 |           7.0908 |           0.0641 |
[32m[20230209 04:36:32 @agent_ppo2.py:193][0m |          -0.0112 |           7.0437 |           0.0641 |
[32m[20230209 04:36:32 @agent_ppo2.py:193][0m |          -0.0106 |           6.8295 |           0.0641 |
[32m[20230209 04:36:33 @agent_ppo2.py:193][0m |          -0.0075 |           6.7849 |           0.0641 |
[32m[20230209 04:36:33 @agent_ppo2.py:137][0m Policy update time: 1.09 s
[32m[20230209 04:36:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: -116.23
[32m[20230209 04:36:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.27
[32m[20230209 04:36:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -84.40
[32m[20230209 04:36:33 @agent_ppo2.py:150][0m Total time:       1.77 min
[32m[20230209 04:36:33 @agent_ppo2.py:152][0m 92160 total steps have happened
[32m[20230209 04:36:33 @agent_ppo2.py:128][0m #------------------------ Iteration 45 --------------------------#
[32m[20230209 04:36:34 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230209 04:36:34 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:34 @agent_ppo2.py:193][0m |          -0.0017 |           1.5426 |           0.0635 |
[32m[20230209 04:36:34 @agent_ppo2.py:193][0m |          -0.0075 |           1.1692 |           0.0635 |
[32m[20230209 04:36:35 @agent_ppo2.py:193][0m |          -0.0091 |           1.0893 |           0.0634 |
[32m[20230209 04:36:35 @agent_ppo2.py:193][0m |          -0.0012 |           1.0393 |           0.0634 |
[32m[20230209 04:36:35 @agent_ppo2.py:193][0m |          -0.0050 |           0.9915 |           0.0634 |
[32m[20230209 04:36:35 @agent_ppo2.py:193][0m |          -0.0058 |           0.9722 |           0.0633 |
[32m[20230209 04:36:35 @agent_ppo2.py:193][0m |          -0.0098 |           0.9503 |           0.0634 |
[32m[20230209 04:36:35 @agent_ppo2.py:193][0m |           0.0127 |           0.9420 |           0.0634 |
[32m[20230209 04:36:35 @agent_ppo2.py:193][0m |           0.0055 |           0.9321 |           0.0633 |
[32m[20230209 04:36:35 @agent_ppo2.py:193][0m |          -0.0158 |           0.9216 |           0.0633 |
[32m[20230209 04:36:35 @agent_ppo2.py:137][0m Policy update time: 0.95 s
[32m[20230209 04:36:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -143.41
[32m[20230209 04:36:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -140.81
[32m[20230209 04:36:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -79.15
[32m[20230209 04:36:36 @agent_ppo2.py:150][0m Total time:       1.81 min
[32m[20230209 04:36:36 @agent_ppo2.py:152][0m 94208 total steps have happened
[32m[20230209 04:36:36 @agent_ppo2.py:128][0m #------------------------ Iteration 46 --------------------------#
[32m[20230209 04:36:37 @agent_ppo2.py:134][0m Sampling time: 0.96 s by 1 slaves
[32m[20230209 04:36:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:37 @agent_ppo2.py:193][0m |          -0.0009 |          19.9524 |           0.0647 |
[32m[20230209 04:36:37 @agent_ppo2.py:193][0m |          -0.0071 |           7.6766 |           0.0646 |
[32m[20230209 04:36:37 @agent_ppo2.py:193][0m |          -0.0054 |           6.8840 |           0.0645 |
[32m[20230209 04:36:37 @agent_ppo2.py:193][0m |          -0.0103 |           6.2837 |           0.0645 |
[32m[20230209 04:36:37 @agent_ppo2.py:193][0m |          -0.0102 |           6.0965 |           0.0644 |
[32m[20230209 04:36:38 @agent_ppo2.py:193][0m |          -0.0117 |           5.8602 |           0.0644 |
[32m[20230209 04:36:38 @agent_ppo2.py:193][0m |          -0.0108 |           5.7106 |           0.0644 |
[32m[20230209 04:36:38 @agent_ppo2.py:193][0m |          -0.0099 |           5.5237 |           0.0643 |
[32m[20230209 04:36:38 @agent_ppo2.py:193][0m |          -0.0086 |           5.4130 |           0.0643 |
[32m[20230209 04:36:38 @agent_ppo2.py:193][0m |          -0.0128 |           5.2508 |           0.0643 |
[32m[20230209 04:36:38 @agent_ppo2.py:137][0m Policy update time: 1.04 s
[32m[20230209 04:36:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -121.83
[32m[20230209 04:36:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.63
[32m[20230209 04:36:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -76.70
[32m[20230209 04:36:39 @agent_ppo2.py:150][0m Total time:       1.86 min
[32m[20230209 04:36:39 @agent_ppo2.py:152][0m 96256 total steps have happened
[32m[20230209 04:36:39 @agent_ppo2.py:128][0m #------------------------ Iteration 47 --------------------------#
[32m[20230209 04:36:40 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230209 04:36:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:40 @agent_ppo2.py:193][0m |          -0.0007 |          24.1224 |           0.0642 |
[32m[20230209 04:36:40 @agent_ppo2.py:193][0m |          -0.0054 |          10.7093 |           0.0642 |
[32m[20230209 04:36:40 @agent_ppo2.py:193][0m |          -0.0108 |           9.3842 |           0.0642 |
[32m[20230209 04:36:40 @agent_ppo2.py:193][0m |          -0.0108 |           8.7485 |           0.0641 |
[32m[20230209 04:36:40 @agent_ppo2.py:193][0m |          -0.0129 |           8.1126 |           0.0642 |
[32m[20230209 04:36:40 @agent_ppo2.py:193][0m |          -0.0109 |           7.6730 |           0.0642 |
[32m[20230209 04:36:40 @agent_ppo2.py:193][0m |          -0.0142 |           7.1551 |           0.0641 |
[32m[20230209 04:36:40 @agent_ppo2.py:193][0m |          -0.0129 |           6.8744 |           0.0641 |
[32m[20230209 04:36:40 @agent_ppo2.py:193][0m |          -0.0148 |           6.6499 |           0.0641 |
[32m[20230209 04:36:40 @agent_ppo2.py:193][0m |          -0.0141 |           6.6775 |           0.0641 |
[32m[20230209 04:36:40 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230209 04:36:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -120.80
[32m[20230209 04:36:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.47
[32m[20230209 04:36:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -76.57
[32m[20230209 04:36:41 @agent_ppo2.py:150][0m Total time:       1.90 min
[32m[20230209 04:36:41 @agent_ppo2.py:152][0m 98304 total steps have happened
[32m[20230209 04:36:41 @agent_ppo2.py:128][0m #------------------------ Iteration 48 --------------------------#
[32m[20230209 04:36:42 @agent_ppo2.py:134][0m Sampling time: 0.94 s by 1 slaves
[32m[20230209 04:36:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:42 @agent_ppo2.py:193][0m |          -0.0010 |          18.6521 |           0.0642 |
[32m[20230209 04:36:42 @agent_ppo2.py:193][0m |          -0.0040 |          11.0051 |           0.0642 |
[32m[20230209 04:36:42 @agent_ppo2.py:193][0m |          -0.0061 |          10.2319 |           0.0641 |
[32m[20230209 04:36:43 @agent_ppo2.py:193][0m |          -0.0077 |           9.6364 |           0.0641 |
[32m[20230209 04:36:43 @agent_ppo2.py:193][0m |          -0.0082 |           9.4612 |           0.0641 |
[32m[20230209 04:36:43 @agent_ppo2.py:193][0m |          -0.0106 |           9.2990 |           0.0640 |
[32m[20230209 04:36:43 @agent_ppo2.py:193][0m |          -0.0111 |           9.0798 |           0.0640 |
[32m[20230209 04:36:43 @agent_ppo2.py:193][0m |          -0.0121 |           9.0011 |           0.0640 |
[32m[20230209 04:36:43 @agent_ppo2.py:193][0m |          -0.0124 |           8.9444 |           0.0639 |
[32m[20230209 04:36:43 @agent_ppo2.py:193][0m |          -0.0128 |           8.7578 |           0.0639 |
[32m[20230209 04:36:43 @agent_ppo2.py:137][0m Policy update time: 1.05 s
[32m[20230209 04:36:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: -118.50
[32m[20230209 04:36:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -96.50
[32m[20230209 04:36:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -123.29
[32m[20230209 04:36:44 @agent_ppo2.py:150][0m Total time:       1.95 min
[32m[20230209 04:36:44 @agent_ppo2.py:152][0m 100352 total steps have happened
[32m[20230209 04:36:44 @agent_ppo2.py:128][0m #------------------------ Iteration 49 --------------------------#
[32m[20230209 04:36:45 @agent_ppo2.py:134][0m Sampling time: 0.61 s by 1 slaves
[32m[20230209 04:36:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:45 @agent_ppo2.py:193][0m |          -0.0001 |          14.7013 |           0.0656 |
[32m[20230209 04:36:45 @agent_ppo2.py:193][0m |          -0.0023 |           6.8211 |           0.0656 |
[32m[20230209 04:36:45 @agent_ppo2.py:193][0m |          -0.0047 |           5.7431 |           0.0656 |
[32m[20230209 04:36:45 @agent_ppo2.py:193][0m |          -0.0056 |           5.3858 |           0.0656 |
[32m[20230209 04:36:45 @agent_ppo2.py:193][0m |          -0.0070 |           5.2225 |           0.0656 |
[32m[20230209 04:36:45 @agent_ppo2.py:193][0m |          -0.0076 |           4.9718 |           0.0656 |
[32m[20230209 04:36:45 @agent_ppo2.py:193][0m |          -0.0083 |           4.8836 |           0.0656 |
[32m[20230209 04:36:45 @agent_ppo2.py:193][0m |          -0.0097 |           4.7561 |           0.0656 |
[32m[20230209 04:36:45 @agent_ppo2.py:193][0m |          -0.0105 |           4.6991 |           0.0656 |
[32m[20230209 04:36:45 @agent_ppo2.py:193][0m |          -0.0095 |           4.6706 |           0.0655 |
[32m[20230209 04:36:45 @agent_ppo2.py:137][0m Policy update time: 0.70 s
[32m[20230209 04:36:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -129.41
[32m[20230209 04:36:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -111.21
[32m[20230209 04:36:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -124.12
[32m[20230209 04:36:46 @evo_bipedalwalker_agent.py:106][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards -22.86
[32m[20230209 04:36:46 @agent_ppo2.py:150][0m Total time:       1.98 min
[32m[20230209 04:36:46 @agent_ppo2.py:152][0m 102400 total steps have happened
[32m[20230209 04:36:46 @agent_ppo2.py:128][0m #------------------------ Iteration 50 --------------------------#
[32m[20230209 04:36:47 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230209 04:36:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:47 @agent_ppo2.py:193][0m |          -0.0017 |          37.1099 |           0.0661 |
[32m[20230209 04:36:47 @agent_ppo2.py:193][0m |          -0.0041 |          16.5900 |           0.0661 |
[32m[20230209 04:36:47 @agent_ppo2.py:193][0m |          -0.0055 |          14.3752 |           0.0661 |
[32m[20230209 04:36:47 @agent_ppo2.py:193][0m |          -0.0097 |          13.3204 |           0.0661 |
[32m[20230209 04:36:47 @agent_ppo2.py:193][0m |          -0.0099 |          12.7548 |           0.0661 |
[32m[20230209 04:36:47 @agent_ppo2.py:193][0m |          -0.0098 |          12.1945 |           0.0660 |
[32m[20230209 04:36:47 @agent_ppo2.py:193][0m |          -0.0145 |          11.8408 |           0.0660 |
[32m[20230209 04:36:47 @agent_ppo2.py:193][0m |          -0.0149 |          11.6183 |           0.0660 |
[32m[20230209 04:36:47 @agent_ppo2.py:193][0m |          -0.0149 |          11.2622 |           0.0660 |
[32m[20230209 04:36:47 @agent_ppo2.py:193][0m |          -0.0147 |          11.1719 |           0.0660 |
[32m[20230209 04:36:47 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230209 04:36:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -111.29
[32m[20230209 04:36:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -99.20
[32m[20230209 04:36:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -130.31
[32m[20230209 04:36:48 @agent_ppo2.py:150][0m Total time:       2.02 min
[32m[20230209 04:36:48 @agent_ppo2.py:152][0m 104448 total steps have happened
[32m[20230209 04:36:48 @agent_ppo2.py:128][0m #------------------------ Iteration 51 --------------------------#
[32m[20230209 04:36:49 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230209 04:36:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:49 @agent_ppo2.py:193][0m |           0.0024 |           2.4911 |           0.0634 |
[32m[20230209 04:36:49 @agent_ppo2.py:193][0m |          -0.0069 |           1.6653 |           0.0634 |
[32m[20230209 04:36:49 @agent_ppo2.py:193][0m |           0.0027 |           1.5002 |           0.0634 |
[32m[20230209 04:36:49 @agent_ppo2.py:193][0m |          -0.0087 |           1.4180 |           0.0634 |
[32m[20230209 04:36:50 @agent_ppo2.py:193][0m |          -0.0071 |           1.3665 |           0.0634 |
[32m[20230209 04:36:50 @agent_ppo2.py:193][0m |          -0.0078 |           1.3178 |           0.0634 |
[32m[20230209 04:36:50 @agent_ppo2.py:193][0m |          -0.0050 |           1.2909 |           0.0635 |
[32m[20230209 04:36:50 @agent_ppo2.py:193][0m |           0.0202 |           1.2586 |           0.0635 |
[32m[20230209 04:36:50 @agent_ppo2.py:193][0m |          -0.0151 |           1.2341 |           0.0635 |
[32m[20230209 04:36:50 @agent_ppo2.py:193][0m |          -0.0063 |           1.2173 |           0.0635 |
[32m[20230209 04:36:50 @agent_ppo2.py:137][0m Policy update time: 0.96 s
[32m[20230209 04:36:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: -142.41
[32m[20230209 04:36:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -141.83
[32m[20230209 04:36:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -137.36
[32m[20230209 04:36:51 @agent_ppo2.py:150][0m Total time:       2.06 min
[32m[20230209 04:36:51 @agent_ppo2.py:152][0m 106496 total steps have happened
[32m[20230209 04:36:51 @agent_ppo2.py:128][0m #------------------------ Iteration 52 --------------------------#
[32m[20230209 04:36:51 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230209 04:36:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:51 @agent_ppo2.py:193][0m |          -0.0015 |          14.3776 |           0.0657 |
[32m[20230209 04:36:51 @agent_ppo2.py:193][0m |          -0.0025 |           9.5575 |           0.0657 |
[32m[20230209 04:36:52 @agent_ppo2.py:193][0m |          -0.0024 |           8.6150 |           0.0657 |
[32m[20230209 04:36:52 @agent_ppo2.py:193][0m |          -0.0040 |           7.9714 |           0.0656 |
[32m[20230209 04:36:52 @agent_ppo2.py:193][0m |          -0.0009 |           7.7740 |           0.0656 |
[32m[20230209 04:36:52 @agent_ppo2.py:193][0m |          -0.0094 |           7.2164 |           0.0656 |
[32m[20230209 04:36:52 @agent_ppo2.py:193][0m |          -0.0015 |           7.1513 |           0.0656 |
[32m[20230209 04:36:52 @agent_ppo2.py:193][0m |          -0.0099 |           6.6911 |           0.0656 |
[32m[20230209 04:36:52 @agent_ppo2.py:193][0m |          -0.0096 |           6.4850 |           0.0656 |
[32m[20230209 04:36:52 @agent_ppo2.py:193][0m |          -0.0159 |           6.3862 |           0.0656 |
[32m[20230209 04:36:52 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230209 04:36:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -135.16
[32m[20230209 04:36:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -132.67
[32m[20230209 04:36:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -86.66
[32m[20230209 04:36:53 @agent_ppo2.py:150][0m Total time:       2.09 min
[32m[20230209 04:36:53 @agent_ppo2.py:152][0m 108544 total steps have happened
[32m[20230209 04:36:53 @agent_ppo2.py:128][0m #------------------------ Iteration 53 --------------------------#
[32m[20230209 04:36:54 @agent_ppo2.py:134][0m Sampling time: 0.92 s by 1 slaves
[32m[20230209 04:36:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:54 @agent_ppo2.py:193][0m |          -0.0037 |          12.3346 |           0.0677 |
[32m[20230209 04:36:54 @agent_ppo2.py:193][0m |          -0.0095 |           7.0444 |           0.0676 |
[32m[20230209 04:36:54 @agent_ppo2.py:193][0m |          -0.0137 |           6.7337 |           0.0676 |
[32m[20230209 04:36:54 @agent_ppo2.py:193][0m |          -0.0135 |           6.5610 |           0.0676 |
[32m[20230209 04:36:54 @agent_ppo2.py:193][0m |          -0.0131 |           6.4474 |           0.0676 |
[32m[20230209 04:36:54 @agent_ppo2.py:193][0m |          -0.0129 |           6.3599 |           0.0676 |
[32m[20230209 04:36:54 @agent_ppo2.py:193][0m |          -0.0151 |           6.3098 |           0.0675 |
[32m[20230209 04:36:54 @agent_ppo2.py:193][0m |          -0.0145 |           6.2871 |           0.0675 |
[32m[20230209 04:36:55 @agent_ppo2.py:193][0m |          -0.0160 |           6.1992 |           0.0675 |
[32m[20230209 04:36:55 @agent_ppo2.py:193][0m |          -0.0146 |           6.1503 |           0.0675 |
[32m[20230209 04:36:55 @agent_ppo2.py:137][0m Policy update time: 0.95 s
[32m[20230209 04:36:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: -130.23
[32m[20230209 04:36:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -106.10
[32m[20230209 04:36:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -101.40
[32m[20230209 04:36:55 @agent_ppo2.py:150][0m Total time:       2.14 min
[32m[20230209 04:36:55 @agent_ppo2.py:152][0m 110592 total steps have happened
[32m[20230209 04:36:55 @agent_ppo2.py:128][0m #------------------------ Iteration 54 --------------------------#
[32m[20230209 04:36:56 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230209 04:36:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:57 @agent_ppo2.py:193][0m |           0.0005 |          34.6577 |           0.0662 |
[32m[20230209 04:36:57 @agent_ppo2.py:193][0m |          -0.0018 |          19.5695 |           0.0662 |
[32m[20230209 04:36:57 @agent_ppo2.py:193][0m |          -0.0037 |          16.9526 |           0.0662 |
[32m[20230209 04:36:57 @agent_ppo2.py:193][0m |          -0.0043 |          15.8949 |           0.0662 |
[32m[20230209 04:36:57 @agent_ppo2.py:193][0m |          -0.0056 |          15.2797 |           0.0661 |
[32m[20230209 04:36:57 @agent_ppo2.py:193][0m |          -0.0067 |          14.9179 |           0.0661 |
[32m[20230209 04:36:57 @agent_ppo2.py:193][0m |          -0.0076 |          14.7315 |           0.0661 |
[32m[20230209 04:36:57 @agent_ppo2.py:193][0m |          -0.0078 |          14.6481 |           0.0661 |
[32m[20230209 04:36:57 @agent_ppo2.py:193][0m |          -0.0086 |          14.4066 |           0.0661 |
[32m[20230209 04:36:57 @agent_ppo2.py:193][0m |          -0.0088 |          14.0827 |           0.0661 |
[32m[20230209 04:36:57 @agent_ppo2.py:137][0m Policy update time: 1.05 s
[32m[20230209 04:36:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: -112.86
[32m[20230209 04:36:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.60
[32m[20230209 04:36:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -121.06
[32m[20230209 04:36:58 @agent_ppo2.py:150][0m Total time:       2.18 min
[32m[20230209 04:36:58 @agent_ppo2.py:152][0m 112640 total steps have happened
[32m[20230209 04:36:58 @agent_ppo2.py:128][0m #------------------------ Iteration 55 --------------------------#
[32m[20230209 04:36:59 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230209 04:36:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:36:59 @agent_ppo2.py:193][0m |          -0.0016 |          12.7169 |           0.0657 |
[32m[20230209 04:36:59 @agent_ppo2.py:193][0m |          -0.0016 |           6.7368 |           0.0655 |
[32m[20230209 04:36:59 @agent_ppo2.py:193][0m |          -0.0045 |           6.1801 |           0.0654 |
[32m[20230209 04:37:00 @agent_ppo2.py:193][0m |          -0.0083 |           5.8811 |           0.0653 |
[32m[20230209 04:37:00 @agent_ppo2.py:193][0m |          -0.0067 |           6.1773 |           0.0652 |
[32m[20230209 04:37:00 @agent_ppo2.py:193][0m |          -0.0187 |           6.0792 |           0.0652 |
[32m[20230209 04:37:00 @agent_ppo2.py:193][0m |          -0.0118 |           5.5203 |           0.0651 |
[32m[20230209 04:37:00 @agent_ppo2.py:193][0m |          -0.0126 |           5.9361 |           0.0651 |
[32m[20230209 04:37:00 @agent_ppo2.py:193][0m |          -0.0113 |           5.4296 |           0.0650 |
[32m[20230209 04:37:00 @agent_ppo2.py:193][0m |          -0.0105 |           5.4115 |           0.0650 |
[32m[20230209 04:37:00 @agent_ppo2.py:137][0m Policy update time: 0.94 s
[32m[20230209 04:37:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -121.26
[32m[20230209 04:37:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.70
[32m[20230209 04:37:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -129.02
[32m[20230209 04:37:01 @agent_ppo2.py:150][0m Total time:       2.23 min
[32m[20230209 04:37:01 @agent_ppo2.py:152][0m 114688 total steps have happened
[32m[20230209 04:37:01 @agent_ppo2.py:128][0m #------------------------ Iteration 56 --------------------------#
[32m[20230209 04:37:02 @agent_ppo2.py:134][0m Sampling time: 0.95 s by 1 slaves
[32m[20230209 04:37:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:37:02 @agent_ppo2.py:193][0m |           0.0059 |          12.0856 |           0.0654 |
[32m[20230209 04:37:02 @agent_ppo2.py:193][0m |           0.0104 |           5.9557 |           0.0653 |
[32m[20230209 04:37:02 @agent_ppo2.py:193][0m |          -0.0075 |           5.2335 |           0.0653 |
[32m[20230209 04:37:02 @agent_ppo2.py:193][0m |          -0.0269 |           4.9639 |           0.0652 |
[32m[20230209 04:37:02 @agent_ppo2.py:193][0m |          -0.0084 |           4.5485 |           0.0652 |
[32m[20230209 04:37:03 @agent_ppo2.py:193][0m |          -0.0111 |           4.3434 |           0.0652 |
[32m[20230209 04:37:03 @agent_ppo2.py:193][0m |          -0.0175 |           4.2450 |           0.0652 |
[32m[20230209 04:37:03 @agent_ppo2.py:193][0m |          -0.0177 |           4.0558 |           0.0652 |
[32m[20230209 04:37:03 @agent_ppo2.py:193][0m |          -0.0129 |           3.9493 |           0.0651 |
[32m[20230209 04:37:03 @agent_ppo2.py:193][0m |          -0.0108 |           3.7984 |           0.0651 |
[32m[20230209 04:37:03 @agent_ppo2.py:137][0m Policy update time: 1.03 s
[32m[20230209 04:37:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: -115.46
[32m[20230209 04:37:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.64
[32m[20230209 04:37:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -75.93
[32m[20230209 04:37:04 @agent_ppo2.py:150][0m Total time:       2.28 min
[32m[20230209 04:37:04 @agent_ppo2.py:152][0m 116736 total steps have happened
[32m[20230209 04:37:04 @agent_ppo2.py:128][0m #------------------------ Iteration 57 --------------------------#
[32m[20230209 04:37:05 @agent_ppo2.py:134][0m Sampling time: 1.00 s by 1 slaves
[32m[20230209 04:37:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:37:05 @agent_ppo2.py:193][0m |           0.0006 |          13.2214 |           0.0658 |
[32m[20230209 04:37:05 @agent_ppo2.py:193][0m |          -0.0026 |           6.0080 |           0.0657 |
[32m[20230209 04:37:05 @agent_ppo2.py:193][0m |          -0.0050 |           5.1808 |           0.0656 |
[32m[20230209 04:37:05 @agent_ppo2.py:193][0m |          -0.0062 |           4.6830 |           0.0656 |
[32m[20230209 04:37:05 @agent_ppo2.py:193][0m |          -0.0079 |           4.5726 |           0.0655 |
[32m[20230209 04:37:05 @agent_ppo2.py:193][0m |          -0.0081 |           4.3205 |           0.0655 |
[32m[20230209 04:37:06 @agent_ppo2.py:193][0m |          -0.0092 |           4.2238 |           0.0655 |
[32m[20230209 04:37:06 @agent_ppo2.py:193][0m |          -0.0104 |           4.1676 |           0.0654 |
[32m[20230209 04:37:06 @agent_ppo2.py:193][0m |          -0.0107 |           4.0734 |           0.0654 |
[32m[20230209 04:37:06 @agent_ppo2.py:193][0m |          -0.0117 |           4.0467 |           0.0654 |
[32m[20230209 04:37:06 @agent_ppo2.py:137][0m Policy update time: 1.09 s
[32m[20230209 04:37:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.27
[32m[20230209 04:37:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.71
[32m[20230209 04:37:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -134.48
[32m[20230209 04:37:07 @agent_ppo2.py:150][0m Total time:       2.32 min
[32m[20230209 04:37:07 @agent_ppo2.py:152][0m 118784 total steps have happened
[32m[20230209 04:37:07 @agent_ppo2.py:128][0m #------------------------ Iteration 58 --------------------------#
[32m[20230209 04:37:08 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230209 04:37:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:37:08 @agent_ppo2.py:193][0m |           0.0027 |           8.8200 |           0.0633 |
[32m[20230209 04:37:08 @agent_ppo2.py:193][0m |           0.0029 |           5.9077 |           0.0634 |
[32m[20230209 04:37:08 @agent_ppo2.py:193][0m |          -0.0033 |           5.7971 |           0.0633 |
[32m[20230209 04:37:08 @agent_ppo2.py:193][0m |           0.0086 |           5.7167 |           0.0633 |
[32m[20230209 04:37:08 @agent_ppo2.py:193][0m |           0.0006 |           5.6689 |           0.0633 |
[32m[20230209 04:37:08 @agent_ppo2.py:193][0m |           0.0180 |           5.6020 |           0.0634 |
[32m[20230209 04:37:08 @agent_ppo2.py:193][0m |          -0.0157 |           5.5649 |           0.0634 |
[32m[20230209 04:37:08 @agent_ppo2.py:193][0m |          -0.0171 |           5.5042 |           0.0634 |
[32m[20230209 04:37:08 @agent_ppo2.py:193][0m |          -0.0007 |           5.4715 |           0.0635 |
[32m[20230209 04:37:08 @agent_ppo2.py:193][0m |           0.0118 |           5.4955 |           0.0635 |
[32m[20230209 04:37:08 @agent_ppo2.py:137][0m Policy update time: 0.96 s
[32m[20230209 04:37:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: -137.10
[32m[20230209 04:37:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -119.61
[32m[20230209 04:37:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -129.60
[32m[20230209 04:37:09 @agent_ppo2.py:150][0m Total time:       2.36 min
[32m[20230209 04:37:09 @agent_ppo2.py:152][0m 120832 total steps have happened
[32m[20230209 04:37:09 @agent_ppo2.py:128][0m #------------------------ Iteration 59 --------------------------#
[32m[20230209 04:37:10 @agent_ppo2.py:134][0m Sampling time: 0.68 s by 1 slaves
[32m[20230209 04:37:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:37:10 @agent_ppo2.py:193][0m |          -0.0006 |           8.9907 |           0.0663 |
[32m[20230209 04:37:10 @agent_ppo2.py:193][0m |          -0.0014 |           2.9402 |           0.0662 |
[32m[20230209 04:37:10 @agent_ppo2.py:193][0m |          -0.0057 |           1.9492 |           0.0661 |
[32m[20230209 04:37:10 @agent_ppo2.py:193][0m |          -0.0062 |           1.7916 |           0.0661 |
[32m[20230209 04:37:10 @agent_ppo2.py:193][0m |          -0.0094 |           1.6086 |           0.0660 |
[32m[20230209 04:37:10 @agent_ppo2.py:193][0m |          -0.0091 |           1.5378 |           0.0659 |
[32m[20230209 04:37:10 @agent_ppo2.py:193][0m |          -0.0102 |           1.4727 |           0.0659 |
[32m[20230209 04:37:10 @agent_ppo2.py:193][0m |          -0.0097 |           1.3989 |           0.0659 |
[32m[20230209 04:37:10 @agent_ppo2.py:193][0m |          -0.0116 |           1.3636 |           0.0658 |
[32m[20230209 04:37:10 @agent_ppo2.py:193][0m |          -0.0118 |           1.3080 |           0.0658 |
[32m[20230209 04:37:10 @agent_ppo2.py:137][0m Policy update time: 0.72 s
[32m[20230209 04:37:11 @agent_ppo2.py:145][0m Average TRAINING episode reward: -129.53
[32m[20230209 04:37:11 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -106.30
[32m[20230209 04:37:11 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -148.95
[32m[20230209 04:37:11 @agent_ppo2.py:150][0m Total time:       2.40 min
[32m[20230209 04:37:11 @agent_ppo2.py:152][0m 122880 total steps have happened
[32m[20230209 04:37:11 @agent_ppo2.py:128][0m #------------------------ Iteration 60 --------------------------#
[32m[20230209 04:37:12 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230209 04:37:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:37:12 @agent_ppo2.py:193][0m |          -0.0116 |           5.0470 |           0.0649 |
[32m[20230209 04:37:12 @agent_ppo2.py:193][0m |          -0.0178 |           2.9035 |           0.0647 |
[32m[20230209 04:37:12 @agent_ppo2.py:193][0m |           0.0014 |           2.6872 |           0.0648 |
[32m[20230209 04:37:13 @agent_ppo2.py:193][0m |          -0.0169 |           2.6408 |           0.0648 |
[32m[20230209 04:37:13 @agent_ppo2.py:193][0m |          -0.0188 |           2.6230 |           0.0648 |
[32m[20230209 04:37:13 @agent_ppo2.py:193][0m |          -0.0008 |           2.5904 |           0.0649 |
[32m[20230209 04:37:13 @agent_ppo2.py:193][0m |          -0.0084 |           2.5648 |           0.0649 |
[32m[20230209 04:37:13 @agent_ppo2.py:193][0m |          -0.0071 |           2.5687 |           0.0649 |
[32m[20230209 04:37:13 @agent_ppo2.py:193][0m |           0.0126 |           2.5876 |           0.0648 |
[32m[20230209 04:37:13 @agent_ppo2.py:193][0m |          -0.0114 |           2.5440 |           0.0648 |
[32m[20230209 04:37:13 @agent_ppo2.py:137][0m Policy update time: 0.93 s
[32m[20230209 04:37:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: -156.92
[32m[20230209 04:37:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -146.06
[32m[20230209 04:37:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -104.62
[32m[20230209 04:37:14 @agent_ppo2.py:150][0m Total time:       2.44 min
[32m[20230209 04:37:14 @agent_ppo2.py:152][0m 124928 total steps have happened
[32m[20230209 04:37:14 @agent_ppo2.py:128][0m #------------------------ Iteration 61 --------------------------#
[32m[20230209 04:37:15 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230209 04:37:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:37:15 @agent_ppo2.py:193][0m |          -0.0005 |          11.5112 |           0.0650 |
[32m[20230209 04:37:15 @agent_ppo2.py:193][0m |          -0.0082 |           4.2129 |           0.0650 |
[32m[20230209 04:37:15 @agent_ppo2.py:193][0m |           0.0002 |           3.6178 |           0.0650 |
[32m[20230209 04:37:15 @agent_ppo2.py:193][0m |          -0.0004 |           3.3664 |           0.0650 |
[32m[20230209 04:37:15 @agent_ppo2.py:193][0m |          -0.0028 |           3.1126 |           0.0650 |
[32m[20230209 04:37:15 @agent_ppo2.py:193][0m |          -0.0098 |           2.9567 |           0.0650 |
[32m[20230209 04:37:15 @agent_ppo2.py:193][0m |          -0.0157 |           2.8636 |           0.0651 |
[32m[20230209 04:37:16 @agent_ppo2.py:193][0m |          -0.0116 |           2.7462 |           0.0651 |
[32m[20230209 04:37:16 @agent_ppo2.py:193][0m |          -0.0128 |           2.6672 |           0.0651 |
[32m[20230209 04:37:16 @agent_ppo2.py:193][0m |          -0.0132 |           2.6550 |           0.0651 |
[32m[20230209 04:37:16 @agent_ppo2.py:137][0m Policy update time: 0.98 s
[32m[20230209 04:37:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: -125.65
[32m[20230209 04:37:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -114.81
[32m[20230209 04:37:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -80.20
[32m[20230209 04:37:17 @agent_ppo2.py:150][0m Total time:       2.49 min
[32m[20230209 04:37:17 @agent_ppo2.py:152][0m 126976 total steps have happened
[32m[20230209 04:37:17 @agent_ppo2.py:128][0m #------------------------ Iteration 62 --------------------------#
[32m[20230209 04:37:17 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230209 04:37:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:37:17 @agent_ppo2.py:193][0m |          -0.0023 |           8.1362 |           0.0659 |
[32m[20230209 04:37:17 @agent_ppo2.py:193][0m |          -0.0026 |           4.5949 |           0.0659 |
[32m[20230209 04:37:17 @agent_ppo2.py:193][0m |          -0.0045 |           4.3393 |           0.0658 |
[32m[20230209 04:37:17 @agent_ppo2.py:193][0m |          -0.0072 |           3.8152 |           0.0658 |
[32m[20230209 04:37:17 @agent_ppo2.py:193][0m |          -0.0071 |           3.6574 |           0.0658 |
[32m[20230209 04:37:18 @agent_ppo2.py:193][0m |          -0.0063 |           3.6219 |           0.0658 |
[32m[20230209 04:37:18 @agent_ppo2.py:193][0m |          -0.0072 |           3.4381 |           0.0657 |
[32m[20230209 04:37:18 @agent_ppo2.py:193][0m |          -0.0072 |           3.4871 |           0.0657 |
[32m[20230209 04:37:18 @agent_ppo2.py:193][0m |          -0.0086 |           3.3944 |           0.0657 |
[32m[20230209 04:37:18 @agent_ppo2.py:193][0m |          -0.0111 |           3.3195 |           0.0657 |
[32m[20230209 04:37:18 @agent_ppo2.py:137][0m Policy update time: 0.64 s
[32m[20230209 04:37:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: -154.23
[32m[20230209 04:37:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -133.10
[32m[20230209 04:37:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -102.28
[32m[20230209 04:37:19 @agent_ppo2.py:150][0m Total time:       2.52 min
[32m[20230209 04:37:19 @agent_ppo2.py:152][0m 129024 total steps have happened
[32m[20230209 04:37:19 @agent_ppo2.py:128][0m #------------------------ Iteration 63 --------------------------#
[32m[20230209 04:37:19 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230209 04:37:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:37:19 @agent_ppo2.py:193][0m |          -0.0006 |          22.4574 |           0.0670 |
[32m[20230209 04:37:19 @agent_ppo2.py:193][0m |          -0.0045 |           7.0228 |           0.0669 |
[32m[20230209 04:37:19 @agent_ppo2.py:193][0m |          -0.0065 |           4.7700 |           0.0668 |
[32m[20230209 04:37:19 @agent_ppo2.py:193][0m |          -0.0091 |           4.0526 |           0.0667 |
[32m[20230209 04:37:20 @agent_ppo2.py:193][0m |          -0.0111 |           3.6270 |           0.0667 |
[32m[20230209 04:37:20 @agent_ppo2.py:193][0m |          -0.0109 |           3.0991 |           0.0666 |
[32m[20230209 04:37:20 @agent_ppo2.py:193][0m |          -0.0129 |           2.8008 |           0.0666 |
[32m[20230209 04:37:20 @agent_ppo2.py:193][0m |          -0.0135 |           2.5604 |           0.0666 |
[32m[20230209 04:37:20 @agent_ppo2.py:193][0m |          -0.0137 |           2.3660 |           0.0666 |
[32m[20230209 04:37:20 @agent_ppo2.py:193][0m |          -0.0139 |           2.2073 |           0.0666 |
[32m[20230209 04:37:20 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230209 04:37:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -123.30
[32m[20230209 04:37:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.63
[32m[20230209 04:37:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -104.21
[32m[20230209 04:37:21 @agent_ppo2.py:150][0m Total time:       2.56 min
[32m[20230209 04:37:21 @agent_ppo2.py:152][0m 131072 total steps have happened
[32m[20230209 04:37:21 @agent_ppo2.py:128][0m #------------------------ Iteration 64 --------------------------#
[32m[20230209 04:37:22 @agent_ppo2.py:134][0m Sampling time: 1.00 s by 1 slaves
[32m[20230209 04:37:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:37:22 @agent_ppo2.py:193][0m |          -0.0001 |           7.4303 |           0.0673 |
[32m[20230209 04:37:22 @agent_ppo2.py:193][0m |          -0.0042 |           4.7284 |           0.0673 |
[32m[20230209 04:37:22 @agent_ppo2.py:193][0m |          -0.0058 |           4.3525 |           0.0673 |
[32m[20230209 04:37:22 @agent_ppo2.py:193][0m |          -0.0083 |           4.1090 |           0.0672 |
[32m[20230209 04:37:22 @agent_ppo2.py:193][0m |          -0.0096 |           3.9227 |           0.0672 |
[32m[20230209 04:37:22 @agent_ppo2.py:193][0m |          -0.0104 |           3.7680 |           0.0672 |
[32m[20230209 04:37:22 @agent_ppo2.py:193][0m |          -0.0110 |           3.6611 |           0.0672 |
[32m[20230209 04:37:23 @agent_ppo2.py:193][0m |          -0.0116 |           3.5719 |           0.0672 |
[32m[20230209 04:37:23 @agent_ppo2.py:193][0m |          -0.0122 |           3.5069 |           0.0671 |
[32m[20230209 04:37:23 @agent_ppo2.py:193][0m |          -0.0124 |           3.4723 |           0.0671 |
[32m[20230209 04:37:23 @agent_ppo2.py:137][0m Policy update time: 1.19 s
[32m[20230209 04:37:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: -119.38
[32m[20230209 04:37:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -107.20
[32m[20230209 04:37:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -154.70
[32m[20230209 04:37:24 @agent_ppo2.py:150][0m Total time:       2.61 min
[32m[20230209 04:37:24 @agent_ppo2.py:152][0m 133120 total steps have happened
[32m[20230209 04:37:24 @agent_ppo2.py:128][0m #------------------------ Iteration 65 --------------------------#
[32m[20230209 04:37:24 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230209 04:37:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:37:25 @agent_ppo2.py:193][0m |           0.0040 |           7.6365 |           0.0654 |
[32m[20230209 04:37:25 @agent_ppo2.py:193][0m |          -0.0022 |           5.0050 |           0.0654 |
[32m[20230209 04:37:25 @agent_ppo2.py:193][0m |          -0.0106 |           4.8454 |           0.0653 |
[32m[20230209 04:37:25 @agent_ppo2.py:193][0m |           0.0020 |           4.7843 |           0.0653 |
[32m[20230209 04:37:25 @agent_ppo2.py:193][0m |          -0.0017 |           4.7092 |           0.0652 |
[32m[20230209 04:37:25 @agent_ppo2.py:193][0m |          -0.0132 |           4.7097 |           0.0652 |
[32m[20230209 04:37:25 @agent_ppo2.py:193][0m |          -0.0165 |           4.7034 |           0.0651 |
[32m[20230209 04:37:25 @agent_ppo2.py:193][0m |          -0.0026 |           4.6796 |           0.0652 |
[32m[20230209 04:37:25 @agent_ppo2.py:193][0m |          -0.0137 |           4.6563 |           0.0651 |
[32m[20230209 04:37:25 @agent_ppo2.py:193][0m |          -0.0038 |           4.6242 |           0.0651 |
[32m[20230209 04:37:25 @agent_ppo2.py:137][0m Policy update time: 0.89 s
