[32m[20230207 15:00:41 @logger.py:106][0m Log file set to ./tmp/evo_bipedalwalker/easy/20230207_150041/log/evo_bipedalwalker_easy-20230207_150041.log
[32m[20230207 15:00:41 @agent_ppo2.py:128][0m #------------------------ Iteration 0 --------------------------#
[32m[20230207 15:00:42 @agent_ppo2.py:134][0m Sampling time: 0.54 s by 1 slaves
[32m[20230207 15:00:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:00:42 @agent_ppo2.py:192][0m |          -0.0023 |         146.5016 |           0.0653 |
[32m[20230207 15:00:42 @agent_ppo2.py:192][0m |          -0.0043 |         143.4101 |           0.0653 |
[32m[20230207 15:00:42 @agent_ppo2.py:192][0m |          -0.0039 |         138.3899 |           0.0653 |
[32m[20230207 15:00:42 @agent_ppo2.py:192][0m |           0.0000 |         134.3790 |           0.0653 |
[32m[20230207 15:00:42 @agent_ppo2.py:192][0m |          -0.0031 |         127.8157 |           0.0653 |
[32m[20230207 15:00:42 @agent_ppo2.py:192][0m |           0.0004 |         126.5860 |           0.0653 |
[32m[20230207 15:00:42 @agent_ppo2.py:192][0m |          -0.0036 |         121.6820 |           0.0653 |
[32m[20230207 15:00:42 @agent_ppo2.py:192][0m |           0.0011 |         122.0232 |           0.0653 |
[32m[20230207 15:00:42 @agent_ppo2.py:192][0m |           0.0032 |         125.9389 |           0.0653 |
[32m[20230207 15:00:42 @agent_ppo2.py:192][0m |          -0.0052 |         115.6602 |           0.0653 |
[32m[20230207 15:00:42 @agent_ppo2.py:137][0m Policy update time: 0.61 s
[32m[20230207 15:00:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -120.21
[32m[20230207 15:00:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.96
[32m[20230207 15:00:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -114.56
[32m[20230207 15:00:43 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -114.56
[32m[20230207 15:00:43 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -114.56
[32m[20230207 15:00:43 @agent_ppo2.py:150][0m Total time:       0.03 min
[32m[20230207 15:00:43 @agent_ppo2.py:152][0m 2048 total steps have happened
[32m[20230207 15:00:43 @agent_ppo2.py:128][0m #------------------------ Iteration 1 --------------------------#
[32m[20230207 15:00:44 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:00:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:00:44 @agent_ppo2.py:192][0m |          -0.0015 |         493.6545 |           0.0642 |
[32m[20230207 15:00:44 @agent_ppo2.py:192][0m |          -0.0018 |         457.2475 |           0.0642 |
[32m[20230207 15:00:44 @agent_ppo2.py:192][0m |           0.0030 |         444.6086 |           0.0642 |
[32m[20230207 15:00:44 @agent_ppo2.py:192][0m |          -0.0036 |         424.2139 |           0.0642 |
[32m[20230207 15:00:44 @agent_ppo2.py:192][0m |          -0.0018 |         417.7425 |           0.0642 |
[32m[20230207 15:00:44 @agent_ppo2.py:192][0m |           0.0035 |         415.1527 |           0.0642 |
[32m[20230207 15:00:44 @agent_ppo2.py:192][0m |          -0.0011 |         412.4303 |           0.0642 |
[32m[20230207 15:00:45 @agent_ppo2.py:192][0m |           0.0008 |         405.3302 |           0.0642 |
[32m[20230207 15:00:45 @agent_ppo2.py:192][0m |          -0.0046 |         384.5423 |           0.0642 |
[32m[20230207 15:00:45 @agent_ppo2.py:192][0m |          -0.0031 |         357.7560 |           0.0642 |
[32m[20230207 15:00:45 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230207 15:00:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: -110.73
[32m[20230207 15:00:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.01
[32m[20230207 15:00:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -159.02
[32m[20230207 15:00:45 @agent_ppo2.py:150][0m Total time:       0.07 min
[32m[20230207 15:00:45 @agent_ppo2.py:152][0m 4096 total steps have happened
[32m[20230207 15:00:45 @agent_ppo2.py:128][0m #------------------------ Iteration 2 --------------------------#
[32m[20230207 15:00:46 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:00:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:00:46 @agent_ppo2.py:192][0m |           0.0008 |          59.4695 |           0.0636 |
[32m[20230207 15:00:46 @agent_ppo2.py:192][0m |          -0.0001 |          56.2149 |           0.0637 |
[32m[20230207 15:00:46 @agent_ppo2.py:192][0m |          -0.0001 |          55.6772 |           0.0637 |
[32m[20230207 15:00:47 @agent_ppo2.py:192][0m |          -0.0016 |          54.6408 |           0.0637 |
[32m[20230207 15:00:47 @agent_ppo2.py:192][0m |           0.0008 |          55.7924 |           0.0638 |
[32m[20230207 15:00:47 @agent_ppo2.py:192][0m |          -0.0022 |          54.1110 |           0.0638 |
[32m[20230207 15:00:47 @agent_ppo2.py:192][0m |          -0.0019 |          54.4824 |           0.0638 |
[32m[20230207 15:00:47 @agent_ppo2.py:192][0m |          -0.0031 |          53.8498 |           0.0639 |
[32m[20230207 15:00:47 @agent_ppo2.py:192][0m |          -0.0034 |          53.6925 |           0.0639 |
[32m[20230207 15:00:47 @agent_ppo2.py:192][0m |          -0.0054 |          53.3293 |           0.0639 |
[32m[20230207 15:00:47 @agent_ppo2.py:137][0m Policy update time: 1.00 s
[32m[20230207 15:00:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -127.12
[32m[20230207 15:00:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.61
[32m[20230207 15:00:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -118.44
[32m[20230207 15:00:48 @agent_ppo2.py:150][0m Total time:       0.11 min
[32m[20230207 15:00:48 @agent_ppo2.py:152][0m 6144 total steps have happened
[32m[20230207 15:00:48 @agent_ppo2.py:128][0m #------------------------ Iteration 3 --------------------------#
[32m[20230207 15:00:49 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 15:00:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:00:49 @agent_ppo2.py:192][0m |          -0.0013 |          53.7635 |           0.0638 |
[32m[20230207 15:00:49 @agent_ppo2.py:192][0m |           0.0074 |          48.7653 |           0.0638 |
[32m[20230207 15:00:49 @agent_ppo2.py:192][0m |          -0.0056 |          47.9057 |           0.0638 |
[32m[20230207 15:00:49 @agent_ppo2.py:192][0m |           0.0066 |          47.1447 |           0.0638 |
[32m[20230207 15:00:49 @agent_ppo2.py:192][0m |          -0.0047 |          46.7903 |           0.0638 |
[32m[20230207 15:00:49 @agent_ppo2.py:192][0m |          -0.0065 |          46.5045 |           0.0637 |
[32m[20230207 15:00:49 @agent_ppo2.py:192][0m |          -0.0056 |          46.3054 |           0.0637 |
[32m[20230207 15:00:49 @agent_ppo2.py:192][0m |          -0.0067 |          46.0449 |           0.0637 |
[32m[20230207 15:00:49 @agent_ppo2.py:192][0m |          -0.0065 |          45.8909 |           0.0637 |
[32m[20230207 15:00:49 @agent_ppo2.py:192][0m |           0.0322 |          66.7873 |           0.0637 |
[32m[20230207 15:00:49 @agent_ppo2.py:137][0m Policy update time: 0.70 s
[32m[20230207 15:00:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: -165.87
[32m[20230207 15:00:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -120.78
[32m[20230207 15:00:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -162.79
[32m[20230207 15:00:50 @agent_ppo2.py:150][0m Total time:       0.15 min
[32m[20230207 15:00:50 @agent_ppo2.py:152][0m 8192 total steps have happened
[32m[20230207 15:00:50 @agent_ppo2.py:128][0m #------------------------ Iteration 4 --------------------------#
[32m[20230207 15:00:51 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:00:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:00:51 @agent_ppo2.py:192][0m |           0.0020 |         320.9115 |           0.0648 |
[32m[20230207 15:00:51 @agent_ppo2.py:192][0m |          -0.0022 |         298.5441 |           0.0648 |
[32m[20230207 15:00:51 @agent_ppo2.py:192][0m |          -0.0007 |         286.8275 |           0.0647 |
[32m[20230207 15:00:51 @agent_ppo2.py:192][0m |          -0.0031 |         279.0791 |           0.0647 |
[32m[20230207 15:00:51 @agent_ppo2.py:192][0m |          -0.0019 |         275.8733 |           0.0647 |
[32m[20230207 15:00:51 @agent_ppo2.py:192][0m |          -0.0021 |         272.6827 |           0.0647 |
[32m[20230207 15:00:51 @agent_ppo2.py:192][0m |          -0.0003 |         272.5618 |           0.0646 |
[32m[20230207 15:00:51 @agent_ppo2.py:192][0m |          -0.0027 |         268.6007 |           0.0646 |
[32m[20230207 15:00:51 @agent_ppo2.py:192][0m |          -0.0026 |         269.4100 |           0.0646 |
[32m[20230207 15:00:51 @agent_ppo2.py:192][0m |          -0.0018 |         268.5346 |           0.0645 |
[32m[20230207 15:00:51 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:00:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: -122.63
[32m[20230207 15:00:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.45
[32m[20230207 15:00:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -127.54
[32m[20230207 15:00:52 @agent_ppo2.py:150][0m Total time:       0.18 min
[32m[20230207 15:00:52 @agent_ppo2.py:152][0m 10240 total steps have happened
[32m[20230207 15:00:52 @agent_ppo2.py:128][0m #------------------------ Iteration 5 --------------------------#
[32m[20230207 15:00:53 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 15:00:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:00:53 @agent_ppo2.py:192][0m |           0.0003 |         143.1517 |           0.0630 |
[32m[20230207 15:00:53 @agent_ppo2.py:192][0m |           0.0065 |         140.9041 |           0.0630 |
[32m[20230207 15:00:53 @agent_ppo2.py:192][0m |          -0.0006 |         133.1355 |           0.0630 |
[32m[20230207 15:00:53 @agent_ppo2.py:192][0m |           0.0003 |         132.4254 |           0.0630 |
[32m[20230207 15:00:53 @agent_ppo2.py:192][0m |          -0.0037 |         127.5817 |           0.0630 |
[32m[20230207 15:00:53 @agent_ppo2.py:192][0m |          -0.0019 |         126.5802 |           0.0630 |
[32m[20230207 15:00:53 @agent_ppo2.py:192][0m |          -0.0046 |         123.9709 |           0.0630 |
[32m[20230207 15:00:53 @agent_ppo2.py:192][0m |           0.0023 |         124.2701 |           0.0630 |
[32m[20230207 15:00:53 @agent_ppo2.py:192][0m |          -0.0005 |         120.8062 |           0.0630 |
[32m[20230207 15:00:53 @agent_ppo2.py:192][0m |          -0.0039 |         115.5037 |           0.0630 |
[32m[20230207 15:00:53 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230207 15:00:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: -142.93
[32m[20230207 15:00:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -107.03
[32m[20230207 15:00:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -116.12
[32m[20230207 15:00:54 @agent_ppo2.py:150][0m Total time:       0.22 min
[32m[20230207 15:00:54 @agent_ppo2.py:152][0m 12288 total steps have happened
[32m[20230207 15:00:54 @agent_ppo2.py:128][0m #------------------------ Iteration 6 --------------------------#
[32m[20230207 15:00:55 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230207 15:00:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:00:55 @agent_ppo2.py:192][0m |           0.0006 |          81.9506 |           0.0641 |
[32m[20230207 15:00:55 @agent_ppo2.py:192][0m |          -0.0041 |          77.3512 |           0.0641 |
[32m[20230207 15:00:55 @agent_ppo2.py:192][0m |          -0.0012 |          76.5249 |           0.0641 |
[32m[20230207 15:00:55 @agent_ppo2.py:192][0m |          -0.0049 |          73.8150 |           0.0641 |
[32m[20230207 15:00:55 @agent_ppo2.py:192][0m |           0.0003 |          74.9090 |           0.0641 |
[32m[20230207 15:00:55 @agent_ppo2.py:192][0m |          -0.0050 |          71.3179 |           0.0641 |
[32m[20230207 15:00:55 @agent_ppo2.py:192][0m |          -0.0057 |          70.4280 |           0.0641 |
[32m[20230207 15:00:55 @agent_ppo2.py:192][0m |          -0.0019 |          71.0424 |           0.0641 |
[32m[20230207 15:00:55 @agent_ppo2.py:192][0m |          -0.0010 |          71.3821 |           0.0641 |
[32m[20230207 15:00:56 @agent_ppo2.py:192][0m |          -0.0047 |          69.3250 |           0.0641 |
[32m[20230207 15:00:56 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230207 15:00:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: -134.72
[32m[20230207 15:00:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.22
[32m[20230207 15:00:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -126.86
[32m[20230207 15:00:56 @agent_ppo2.py:150][0m Total time:       0.25 min
[32m[20230207 15:00:56 @agent_ppo2.py:152][0m 14336 total steps have happened
[32m[20230207 15:00:56 @agent_ppo2.py:128][0m #------------------------ Iteration 7 --------------------------#
[32m[20230207 15:00:57 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:00:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:00:57 @agent_ppo2.py:192][0m |          -0.0009 |          70.5520 |           0.0647 |
[32m[20230207 15:00:58 @agent_ppo2.py:192][0m |          -0.0014 |          67.3954 |           0.0647 |
[32m[20230207 15:00:58 @agent_ppo2.py:192][0m |          -0.0008 |          66.3458 |           0.0647 |
[32m[20230207 15:00:58 @agent_ppo2.py:192][0m |          -0.0014 |          65.2302 |           0.0647 |
[32m[20230207 15:00:58 @agent_ppo2.py:192][0m |          -0.0029 |          63.4646 |           0.0647 |
[32m[20230207 15:00:58 @agent_ppo2.py:192][0m |          -0.0032 |          62.5629 |           0.0647 |
[32m[20230207 15:00:58 @agent_ppo2.py:192][0m |          -0.0052 |          60.7999 |           0.0647 |
[32m[20230207 15:00:58 @agent_ppo2.py:192][0m |          -0.0054 |          59.6998 |           0.0647 |
[32m[20230207 15:00:58 @agent_ppo2.py:192][0m |          -0.0057 |          58.9685 |           0.0647 |
[32m[20230207 15:00:58 @agent_ppo2.py:192][0m |          -0.0070 |          57.6358 |           0.0647 |
[32m[20230207 15:00:58 @agent_ppo2.py:137][0m Policy update time: 0.79 s
[32m[20230207 15:00:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: -141.79
[32m[20230207 15:00:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -106.63
[32m[20230207 15:00:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -130.85
[32m[20230207 15:00:59 @agent_ppo2.py:150][0m Total time:       0.29 min
[32m[20230207 15:00:59 @agent_ppo2.py:152][0m 16384 total steps have happened
[32m[20230207 15:00:59 @agent_ppo2.py:128][0m #------------------------ Iteration 8 --------------------------#
[32m[20230207 15:00:59 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:00:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:00:59 @agent_ppo2.py:192][0m |          -0.0000 |         322.5273 |           0.0677 |
[32m[20230207 15:00:59 @agent_ppo2.py:192][0m |          -0.0006 |         294.0928 |           0.0677 |
[32m[20230207 15:00:59 @agent_ppo2.py:192][0m |          -0.0013 |         276.5236 |           0.0677 |
[32m[20230207 15:00:59 @agent_ppo2.py:192][0m |          -0.0020 |         262.3742 |           0.0677 |
[32m[20230207 15:01:00 @agent_ppo2.py:192][0m |          -0.0027 |         249.8980 |           0.0677 |
[32m[20230207 15:01:00 @agent_ppo2.py:192][0m |          -0.0035 |         239.7067 |           0.0676 |
[32m[20230207 15:01:00 @agent_ppo2.py:192][0m |          -0.0043 |         231.5067 |           0.0676 |
[32m[20230207 15:01:00 @agent_ppo2.py:192][0m |          -0.0051 |         224.3202 |           0.0676 |
[32m[20230207 15:01:00 @agent_ppo2.py:192][0m |          -0.0059 |         218.7873 |           0.0676 |
[32m[20230207 15:01:00 @agent_ppo2.py:192][0m |          -0.0067 |         213.0206 |           0.0675 |
[32m[20230207 15:01:00 @agent_ppo2.py:137][0m Policy update time: 0.51 s
[32m[20230207 15:01:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -111.67
[32m[20230207 15:01:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.50
[32m[20230207 15:01:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -137.23
[32m[20230207 15:01:01 @agent_ppo2.py:150][0m Total time:       0.32 min
[32m[20230207 15:01:01 @agent_ppo2.py:152][0m 18432 total steps have happened
[32m[20230207 15:01:01 @agent_ppo2.py:128][0m #------------------------ Iteration 9 --------------------------#
[32m[20230207 15:01:01 @agent_ppo2.py:134][0m Sampling time: 0.64 s by 1 slaves
[32m[20230207 15:01:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:01 @agent_ppo2.py:192][0m |          -0.0008 |          49.5215 |           0.0660 |
[32m[20230207 15:01:01 @agent_ppo2.py:192][0m |          -0.0009 |          45.4725 |           0.0660 |
[32m[20230207 15:01:02 @agent_ppo2.py:192][0m |          -0.0016 |          44.0399 |           0.0660 |
[32m[20230207 15:01:02 @agent_ppo2.py:192][0m |          -0.0025 |          43.3780 |           0.0660 |
[32m[20230207 15:01:02 @agent_ppo2.py:192][0m |          -0.0033 |          42.4205 |           0.0659 |
[32m[20230207 15:01:02 @agent_ppo2.py:192][0m |          -0.0043 |          41.5922 |           0.0659 |
[32m[20230207 15:01:02 @agent_ppo2.py:192][0m |          -0.0057 |          40.4870 |           0.0659 |
[32m[20230207 15:01:02 @agent_ppo2.py:192][0m |          -0.0057 |          39.8025 |           0.0659 |
[32m[20230207 15:01:02 @agent_ppo2.py:192][0m |          -0.0073 |          39.0519 |           0.0659 |
[32m[20230207 15:01:02 @agent_ppo2.py:192][0m |          -0.0054 |          38.9176 |           0.0659 |
[32m[20230207 15:01:02 @agent_ppo2.py:137][0m Policy update time: 0.75 s
[32m[20230207 15:01:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -128.73
[32m[20230207 15:01:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.74
[32m[20230207 15:01:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -111.45
[32m[20230207 15:01:03 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -111.45
[32m[20230207 15:01:03 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -111.45
[32m[20230207 15:01:03 @agent_ppo2.py:150][0m Total time:       0.36 min
[32m[20230207 15:01:03 @agent_ppo2.py:152][0m 20480 total steps have happened
[32m[20230207 15:01:03 @agent_ppo2.py:128][0m #------------------------ Iteration 10 --------------------------#
[32m[20230207 15:01:03 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230207 15:01:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:03 @agent_ppo2.py:192][0m |           0.0300 |          79.2584 |           0.0641 |
[32m[20230207 15:01:03 @agent_ppo2.py:192][0m |          -0.0103 |          70.5465 |           0.0641 |
[32m[20230207 15:01:03 @agent_ppo2.py:192][0m |          -0.0078 |          70.2198 |           0.0641 |
[32m[20230207 15:01:04 @agent_ppo2.py:192][0m |          -0.0081 |          69.9808 |           0.0641 |
[32m[20230207 15:01:04 @agent_ppo2.py:192][0m |           0.0141 |          72.2259 |           0.0641 |
[32m[20230207 15:01:04 @agent_ppo2.py:192][0m |          -0.0068 |          69.8142 |           0.0641 |
[32m[20230207 15:01:04 @agent_ppo2.py:192][0m |          -0.0122 |          69.1654 |           0.0641 |
[32m[20230207 15:01:04 @agent_ppo2.py:192][0m |          -0.0094 |          68.9787 |           0.0641 |
[32m[20230207 15:01:04 @agent_ppo2.py:192][0m |          -0.0073 |          69.5286 |           0.0640 |
[32m[20230207 15:01:04 @agent_ppo2.py:192][0m |           0.0100 |          72.1713 |           0.0640 |
[32m[20230207 15:01:04 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230207 15:01:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.07
[32m[20230207 15:01:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -105.40
[32m[20230207 15:01:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -118.31
[32m[20230207 15:01:05 @agent_ppo2.py:150][0m Total time:       0.39 min
[32m[20230207 15:01:05 @agent_ppo2.py:152][0m 22528 total steps have happened
[32m[20230207 15:01:05 @agent_ppo2.py:128][0m #------------------------ Iteration 11 --------------------------#
[32m[20230207 15:01:06 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 15:01:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:06 @agent_ppo2.py:192][0m |          -0.0044 |         115.0497 |           0.0630 |
[32m[20230207 15:01:06 @agent_ppo2.py:192][0m |          -0.0116 |         106.2557 |           0.0630 |
[32m[20230207 15:01:06 @agent_ppo2.py:192][0m |          -0.0245 |         101.7371 |           0.0630 |
[32m[20230207 15:01:06 @agent_ppo2.py:192][0m |          -0.0122 |          94.9667 |           0.0631 |
[32m[20230207 15:01:06 @agent_ppo2.py:192][0m |          -0.0133 |          90.7263 |           0.0631 |
[32m[20230207 15:01:06 @agent_ppo2.py:192][0m |           0.0060 |          87.3561 |           0.0631 |
[32m[20230207 15:01:06 @agent_ppo2.py:192][0m |          -0.0164 |          83.2396 |           0.0631 |
[32m[20230207 15:01:06 @agent_ppo2.py:192][0m |          -0.0146 |          79.6667 |           0.0631 |
[32m[20230207 15:01:06 @agent_ppo2.py:192][0m |          -0.0113 |          76.3627 |           0.0631 |
[32m[20230207 15:01:06 @agent_ppo2.py:192][0m |           0.0006 |          73.4918 |           0.0631 |
[32m[20230207 15:01:06 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230207 15:01:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.47
[32m[20230207 15:01:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.27
[32m[20230207 15:01:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -112.64
[32m[20230207 15:01:07 @agent_ppo2.py:150][0m Total time:       0.43 min
[32m[20230207 15:01:07 @agent_ppo2.py:152][0m 24576 total steps have happened
[32m[20230207 15:01:07 @agent_ppo2.py:128][0m #------------------------ Iteration 12 --------------------------#
[32m[20230207 15:01:08 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:01:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:08 @agent_ppo2.py:192][0m |           0.0000 |          31.0677 |           0.0650 |
[32m[20230207 15:01:08 @agent_ppo2.py:192][0m |          -0.0012 |          26.3313 |           0.0650 |
[32m[20230207 15:01:08 @agent_ppo2.py:192][0m |          -0.0010 |          25.5836 |           0.0650 |
[32m[20230207 15:01:08 @agent_ppo2.py:192][0m |          -0.0023 |          24.7168 |           0.0650 |
[32m[20230207 15:01:08 @agent_ppo2.py:192][0m |          -0.0022 |          24.1487 |           0.0650 |
[32m[20230207 15:01:08 @agent_ppo2.py:192][0m |          -0.0031 |          23.4512 |           0.0650 |
[32m[20230207 15:01:08 @agent_ppo2.py:192][0m |          -0.0036 |          22.8973 |           0.0650 |
[32m[20230207 15:01:08 @agent_ppo2.py:192][0m |          -0.0046 |          22.3452 |           0.0650 |
[32m[20230207 15:01:08 @agent_ppo2.py:192][0m |          -0.0045 |          21.8631 |           0.0650 |
[32m[20230207 15:01:09 @agent_ppo2.py:192][0m |          -0.0061 |          21.3204 |           0.0651 |
[32m[20230207 15:01:09 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:01:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: -142.32
[32m[20230207 15:01:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -108.82
[32m[20230207 15:01:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -136.78
[32m[20230207 15:01:09 @agent_ppo2.py:150][0m Total time:       0.47 min
[32m[20230207 15:01:09 @agent_ppo2.py:152][0m 26624 total steps have happened
[32m[20230207 15:01:09 @agent_ppo2.py:128][0m #------------------------ Iteration 13 --------------------------#
[32m[20230207 15:01:10 @agent_ppo2.py:134][0m Sampling time: 0.64 s by 1 slaves
[32m[20230207 15:01:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:10 @agent_ppo2.py:192][0m |           0.0037 |          91.4742 |           0.0655 |
[32m[20230207 15:01:10 @agent_ppo2.py:192][0m |           0.0010 |          75.6885 |           0.0655 |
[32m[20230207 15:01:10 @agent_ppo2.py:192][0m |          -0.0034 |          67.0393 |           0.0654 |
[32m[20230207 15:01:10 @agent_ppo2.py:192][0m |          -0.0073 |          63.6288 |           0.0654 |
[32m[20230207 15:01:10 @agent_ppo2.py:192][0m |          -0.0091 |          61.0789 |           0.0654 |
[32m[20230207 15:01:10 @agent_ppo2.py:192][0m |          -0.0025 |          58.5083 |           0.0653 |
[32m[20230207 15:01:10 @agent_ppo2.py:192][0m |          -0.0113 |          55.8659 |           0.0653 |
[32m[20230207 15:01:10 @agent_ppo2.py:192][0m |          -0.0098 |          53.9326 |           0.0653 |
[32m[20230207 15:01:10 @agent_ppo2.py:192][0m |          -0.0044 |          53.6783 |           0.0653 |
[32m[20230207 15:01:10 @agent_ppo2.py:192][0m |          -0.0075 |          48.3629 |           0.0653 |
[32m[20230207 15:01:10 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230207 15:01:11 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.77
[32m[20230207 15:01:11 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.75
[32m[20230207 15:01:11 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -126.31
[32m[20230207 15:01:11 @agent_ppo2.py:150][0m Total time:       0.50 min
[32m[20230207 15:01:11 @agent_ppo2.py:152][0m 28672 total steps have happened
[32m[20230207 15:01:11 @agent_ppo2.py:128][0m #------------------------ Iteration 14 --------------------------#
[32m[20230207 15:01:12 @agent_ppo2.py:134][0m Sampling time: 0.61 s by 1 slaves
[32m[20230207 15:01:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:12 @agent_ppo2.py:192][0m |           0.0001 |          49.6582 |           0.0664 |
[32m[20230207 15:01:12 @agent_ppo2.py:192][0m |          -0.0009 |          41.9184 |           0.0664 |
[32m[20230207 15:01:12 @agent_ppo2.py:192][0m |          -0.0010 |          39.5441 |           0.0664 |
[32m[20230207 15:01:12 @agent_ppo2.py:192][0m |          -0.0018 |          38.0812 |           0.0664 |
[32m[20230207 15:01:12 @agent_ppo2.py:192][0m |          -0.0024 |          36.6993 |           0.0663 |
[32m[20230207 15:01:12 @agent_ppo2.py:192][0m |          -0.0029 |          35.4803 |           0.0663 |
[32m[20230207 15:01:12 @agent_ppo2.py:192][0m |          -0.0035 |          34.3816 |           0.0663 |
[32m[20230207 15:01:12 @agent_ppo2.py:192][0m |          -0.0044 |          33.3846 |           0.0663 |
[32m[20230207 15:01:12 @agent_ppo2.py:192][0m |          -0.0050 |          32.3859 |           0.0663 |
[32m[20230207 15:01:13 @agent_ppo2.py:192][0m |          -0.0054 |          31.4201 |           0.0662 |
[32m[20230207 15:01:13 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:01:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.42
[32m[20230207 15:01:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.31
[32m[20230207 15:01:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -109.99
[32m[20230207 15:01:13 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -109.99
[32m[20230207 15:01:13 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -109.99
[32m[20230207 15:01:13 @agent_ppo2.py:150][0m Total time:       0.53 min
[32m[20230207 15:01:13 @agent_ppo2.py:152][0m 30720 total steps have happened
[32m[20230207 15:01:13 @agent_ppo2.py:128][0m #------------------------ Iteration 15 --------------------------#
[32m[20230207 15:01:14 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:01:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:14 @agent_ppo2.py:192][0m |           0.0039 |          22.9440 |           0.0628 |
[32m[20230207 15:01:14 @agent_ppo2.py:192][0m |           0.0033 |          17.6639 |           0.0628 |
[32m[20230207 15:01:14 @agent_ppo2.py:192][0m |          -0.0030 |          16.2565 |           0.0628 |
[32m[20230207 15:01:14 @agent_ppo2.py:192][0m |          -0.0044 |          15.9916 |           0.0628 |
[32m[20230207 15:01:14 @agent_ppo2.py:192][0m |          -0.0060 |          15.6829 |           0.0627 |
[32m[20230207 15:01:14 @agent_ppo2.py:192][0m |          -0.0072 |          15.3649 |           0.0627 |
[32m[20230207 15:01:15 @agent_ppo2.py:192][0m |          -0.0070 |          15.1754 |           0.0627 |
[32m[20230207 15:01:15 @agent_ppo2.py:192][0m |          -0.0045 |          15.0869 |           0.0627 |
[32m[20230207 15:01:15 @agent_ppo2.py:192][0m |          -0.0056 |          14.9189 |           0.0627 |
[32m[20230207 15:01:15 @agent_ppo2.py:192][0m |          -0.0078 |          14.8111 |           0.0627 |
[32m[20230207 15:01:15 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:01:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -181.68
[32m[20230207 15:01:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -115.21
[32m[20230207 15:01:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -115.62
[32m[20230207 15:01:16 @agent_ppo2.py:150][0m Total time:       0.57 min
[32m[20230207 15:01:16 @agent_ppo2.py:152][0m 32768 total steps have happened
[32m[20230207 15:01:16 @agent_ppo2.py:128][0m #------------------------ Iteration 16 --------------------------#
[32m[20230207 15:01:16 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 15:01:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:16 @agent_ppo2.py:192][0m |           0.0006 |          31.1273 |           0.0658 |
[32m[20230207 15:01:16 @agent_ppo2.py:192][0m |           0.0034 |          23.0094 |           0.0658 |
[32m[20230207 15:01:16 @agent_ppo2.py:192][0m |          -0.0039 |          20.6582 |           0.0658 |
[32m[20230207 15:01:16 @agent_ppo2.py:192][0m |          -0.0009 |          19.8869 |           0.0658 |
[32m[20230207 15:01:16 @agent_ppo2.py:192][0m |          -0.0008 |          19.2856 |           0.0658 |
[32m[20230207 15:01:17 @agent_ppo2.py:192][0m |           0.0099 |          19.9002 |           0.0658 |
[32m[20230207 15:01:17 @agent_ppo2.py:192][0m |          -0.0003 |          18.7387 |           0.0658 |
[32m[20230207 15:01:17 @agent_ppo2.py:192][0m |          -0.0031 |          17.4887 |           0.0658 |
[32m[20230207 15:01:17 @agent_ppo2.py:192][0m |          -0.0042 |          17.2960 |           0.0658 |
[32m[20230207 15:01:17 @agent_ppo2.py:192][0m |          -0.0018 |          16.6730 |           0.0658 |
[32m[20230207 15:01:17 @agent_ppo2.py:137][0m Policy update time: 0.53 s
[32m[20230207 15:01:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: -127.29
[32m[20230207 15:01:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -109.57
[32m[20230207 15:01:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -121.43
[32m[20230207 15:01:17 @agent_ppo2.py:150][0m Total time:       0.60 min
[32m[20230207 15:01:17 @agent_ppo2.py:152][0m 34816 total steps have happened
[32m[20230207 15:01:17 @agent_ppo2.py:128][0m #------------------------ Iteration 17 --------------------------#
[32m[20230207 15:01:18 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230207 15:01:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:18 @agent_ppo2.py:192][0m |          -0.0016 |          50.2686 |           0.0650 |
[32m[20230207 15:01:18 @agent_ppo2.py:192][0m |           0.0019 |          39.1562 |           0.0650 |
[32m[20230207 15:01:18 @agent_ppo2.py:192][0m |          -0.0005 |          36.3798 |           0.0650 |
[32m[20230207 15:01:18 @agent_ppo2.py:192][0m |          -0.0030 |          34.2081 |           0.0650 |
[32m[20230207 15:01:18 @agent_ppo2.py:192][0m |          -0.0015 |          32.5103 |           0.0650 |
[32m[20230207 15:01:18 @agent_ppo2.py:192][0m |          -0.0012 |          30.3148 |           0.0650 |
[32m[20230207 15:01:18 @agent_ppo2.py:192][0m |          -0.0019 |          28.9718 |           0.0650 |
[32m[20230207 15:01:18 @agent_ppo2.py:192][0m |          -0.0065 |          27.0120 |           0.0649 |
[32m[20230207 15:01:18 @agent_ppo2.py:192][0m |          -0.0043 |          25.8407 |           0.0649 |
[32m[20230207 15:01:18 @agent_ppo2.py:192][0m |          -0.0055 |          24.2867 |           0.0649 |
[32m[20230207 15:01:18 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:01:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: -107.77
[32m[20230207 15:01:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.99
[32m[20230207 15:01:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -124.65
[32m[20230207 15:01:19 @agent_ppo2.py:150][0m Total time:       0.63 min
[32m[20230207 15:01:19 @agent_ppo2.py:152][0m 36864 total steps have happened
[32m[20230207 15:01:19 @agent_ppo2.py:128][0m #------------------------ Iteration 18 --------------------------#
[32m[20230207 15:01:20 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:01:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:20 @agent_ppo2.py:192][0m |          -0.0020 |          33.5974 |           0.0633 |
[32m[20230207 15:01:20 @agent_ppo2.py:192][0m |           0.0008 |          27.5869 |           0.0633 |
[32m[20230207 15:01:20 @agent_ppo2.py:192][0m |           0.0005 |          27.4849 |           0.0633 |
[32m[20230207 15:01:20 @agent_ppo2.py:192][0m |           0.0007 |          26.1555 |           0.0633 |
[32m[20230207 15:01:21 @agent_ppo2.py:192][0m |          -0.0027 |          25.6029 |           0.0633 |
[32m[20230207 15:01:21 @agent_ppo2.py:192][0m |          -0.0016 |          24.9845 |           0.0634 |
[32m[20230207 15:01:21 @agent_ppo2.py:192][0m |          -0.0004 |          25.6712 |           0.0634 |
[32m[20230207 15:01:21 @agent_ppo2.py:192][0m |          -0.0031 |          24.5372 |           0.0634 |
[32m[20230207 15:01:21 @agent_ppo2.py:192][0m |          -0.0010 |          24.4103 |           0.0634 |
[32m[20230207 15:01:21 @agent_ppo2.py:192][0m |          -0.0027 |          24.3467 |           0.0634 |
[32m[20230207 15:01:21 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:01:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: -118.72
[32m[20230207 15:01:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.70
[32m[20230207 15:01:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -151.43
[32m[20230207 15:01:22 @agent_ppo2.py:150][0m Total time:       0.68 min
[32m[20230207 15:01:22 @agent_ppo2.py:152][0m 38912 total steps have happened
[32m[20230207 15:01:22 @agent_ppo2.py:128][0m #------------------------ Iteration 19 --------------------------#
[32m[20230207 15:01:23 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230207 15:01:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:23 @agent_ppo2.py:192][0m |           0.0003 |          56.1996 |           0.0668 |
[32m[20230207 15:01:23 @agent_ppo2.py:192][0m |          -0.0010 |          45.4227 |           0.0668 |
[32m[20230207 15:01:23 @agent_ppo2.py:192][0m |          -0.0018 |          40.5127 |           0.0668 |
[32m[20230207 15:01:23 @agent_ppo2.py:192][0m |          -0.0028 |          36.7954 |           0.0668 |
[32m[20230207 15:01:23 @agent_ppo2.py:192][0m |          -0.0034 |          34.1245 |           0.0667 |
[32m[20230207 15:01:23 @agent_ppo2.py:192][0m |          -0.0039 |          32.1545 |           0.0667 |
[32m[20230207 15:01:23 @agent_ppo2.py:192][0m |          -0.0046 |          30.8029 |           0.0667 |
[32m[20230207 15:01:23 @agent_ppo2.py:192][0m |          -0.0048 |          29.5144 |           0.0666 |
[32m[20230207 15:01:23 @agent_ppo2.py:192][0m |          -0.0053 |          27.5615 |           0.0666 |
[32m[20230207 15:01:23 @agent_ppo2.py:192][0m |          -0.0057 |          26.4086 |           0.0666 |
[32m[20230207 15:01:23 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:01:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: -115.43
[32m[20230207 15:01:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.54
[32m[20230207 15:01:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -168.34
[32m[20230207 15:01:24 @agent_ppo2.py:150][0m Total time:       0.71 min
[32m[20230207 15:01:24 @agent_ppo2.py:152][0m 40960 total steps have happened
[32m[20230207 15:01:24 @agent_ppo2.py:128][0m #------------------------ Iteration 20 --------------------------#
[32m[20230207 15:01:24 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:01:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:25 @agent_ppo2.py:192][0m |          -0.0013 |          19.2529 |           0.0658 |
[32m[20230207 15:01:25 @agent_ppo2.py:192][0m |          -0.0014 |          13.7266 |           0.0658 |
[32m[20230207 15:01:25 @agent_ppo2.py:192][0m |          -0.0042 |          12.7761 |           0.0657 |
[32m[20230207 15:01:25 @agent_ppo2.py:192][0m |          -0.0054 |          12.2104 |           0.0657 |
[32m[20230207 15:01:25 @agent_ppo2.py:192][0m |          -0.0066 |          11.6287 |           0.0657 |
[32m[20230207 15:01:25 @agent_ppo2.py:192][0m |          -0.0071 |          11.0742 |           0.0656 |
[32m[20230207 15:01:25 @agent_ppo2.py:192][0m |          -0.0083 |          10.5524 |           0.0656 |
[32m[20230207 15:01:25 @agent_ppo2.py:192][0m |          -0.0086 |          10.0925 |           0.0656 |
[32m[20230207 15:01:25 @agent_ppo2.py:192][0m |          -0.0091 |           9.7410 |           0.0655 |
[32m[20230207 15:01:25 @agent_ppo2.py:192][0m |          -0.0105 |           9.3549 |           0.0655 |
[32m[20230207 15:01:25 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:01:26 @agent_ppo2.py:145][0m Average TRAINING episode reward: -112.51
[32m[20230207 15:01:26 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.52
[32m[20230207 15:01:26 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -121.42
[32m[20230207 15:01:26 @agent_ppo2.py:150][0m Total time:       0.74 min
[32m[20230207 15:01:26 @agent_ppo2.py:152][0m 43008 total steps have happened
[32m[20230207 15:01:26 @agent_ppo2.py:128][0m #------------------------ Iteration 21 --------------------------#
[32m[20230207 15:01:26 @agent_ppo2.py:134][0m Sampling time: 0.66 s by 1 slaves
[32m[20230207 15:01:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:26 @agent_ppo2.py:192][0m |          -0.0001 |          28.4362 |           0.0660 |
[32m[20230207 15:01:26 @agent_ppo2.py:192][0m |          -0.0012 |          22.7043 |           0.0659 |
[32m[20230207 15:01:26 @agent_ppo2.py:192][0m |          -0.0022 |          20.1236 |           0.0659 |
[32m[20230207 15:01:27 @agent_ppo2.py:192][0m |          -0.0032 |          18.4419 |           0.0658 |
[32m[20230207 15:01:27 @agent_ppo2.py:192][0m |          -0.0040 |          17.3453 |           0.0658 |
[32m[20230207 15:01:27 @agent_ppo2.py:192][0m |          -0.0045 |          16.5078 |           0.0657 |
[32m[20230207 15:01:27 @agent_ppo2.py:192][0m |          -0.0050 |          15.9864 |           0.0657 |
[32m[20230207 15:01:27 @agent_ppo2.py:192][0m |          -0.0055 |          15.4617 |           0.0656 |
[32m[20230207 15:01:27 @agent_ppo2.py:192][0m |          -0.0059 |          15.1493 |           0.0656 |
[32m[20230207 15:01:27 @agent_ppo2.py:192][0m |          -0.0063 |          14.8385 |           0.0656 |
[32m[20230207 15:01:27 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230207 15:01:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -117.30
[32m[20230207 15:01:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.89
[32m[20230207 15:01:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -126.91
[32m[20230207 15:01:27 @agent_ppo2.py:150][0m Total time:       0.77 min
[32m[20230207 15:01:27 @agent_ppo2.py:152][0m 45056 total steps have happened
[32m[20230207 15:01:27 @agent_ppo2.py:128][0m #------------------------ Iteration 22 --------------------------#
[32m[20230207 15:01:28 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:01:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:28 @agent_ppo2.py:192][0m |          -0.0006 |          20.8010 |           0.0649 |
[32m[20230207 15:01:28 @agent_ppo2.py:192][0m |          -0.0023 |          17.6006 |           0.0648 |
[32m[20230207 15:01:29 @agent_ppo2.py:192][0m |          -0.0041 |          16.8420 |           0.0648 |
[32m[20230207 15:01:29 @agent_ppo2.py:192][0m |          -0.0051 |          16.3963 |           0.0647 |
[32m[20230207 15:01:29 @agent_ppo2.py:192][0m |          -0.0060 |          16.1022 |           0.0647 |
[32m[20230207 15:01:29 @agent_ppo2.py:192][0m |          -0.0056 |          15.7130 |           0.0647 |
[32m[20230207 15:01:29 @agent_ppo2.py:192][0m |          -0.0055 |          15.2965 |           0.0647 |
[32m[20230207 15:01:29 @agent_ppo2.py:192][0m |          -0.0067 |          15.1131 |           0.0647 |
[32m[20230207 15:01:29 @agent_ppo2.py:192][0m |          -0.0063 |          14.6409 |           0.0647 |
[32m[20230207 15:01:29 @agent_ppo2.py:192][0m |          -0.0069 |          14.4921 |           0.0647 |
[32m[20230207 15:01:29 @agent_ppo2.py:137][0m Policy update time: 0.78 s
[32m[20230207 15:01:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -151.15
[32m[20230207 15:01:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -105.32
[32m[20230207 15:01:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -112.25
[32m[20230207 15:01:30 @agent_ppo2.py:150][0m Total time:       0.81 min
[32m[20230207 15:01:30 @agent_ppo2.py:152][0m 47104 total steps have happened
[32m[20230207 15:01:30 @agent_ppo2.py:128][0m #------------------------ Iteration 23 --------------------------#
[32m[20230207 15:01:31 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:01:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:31 @agent_ppo2.py:192][0m |          -0.0034 |          15.8736 |           0.0640 |
[32m[20230207 15:01:31 @agent_ppo2.py:192][0m |           0.0138 |          10.5740 |           0.0640 |
[32m[20230207 15:01:31 @agent_ppo2.py:192][0m |          -0.0045 |           9.2952 |           0.0641 |
[32m[20230207 15:01:31 @agent_ppo2.py:192][0m |          -0.0147 |           9.5259 |           0.0641 |
[32m[20230207 15:01:31 @agent_ppo2.py:192][0m |          -0.0050 |           8.5933 |           0.0641 |
[32m[20230207 15:01:31 @agent_ppo2.py:192][0m |           0.0065 |           8.7053 |           0.0642 |
[32m[20230207 15:01:31 @agent_ppo2.py:192][0m |          -0.0031 |           8.1593 |           0.0642 |
[32m[20230207 15:01:31 @agent_ppo2.py:192][0m |          -0.0101 |           7.9476 |           0.0642 |
[32m[20230207 15:01:31 @agent_ppo2.py:192][0m |          -0.0030 |           7.8519 |           0.0642 |
[32m[20230207 15:01:31 @agent_ppo2.py:192][0m |          -0.0053 |           7.6898 |           0.0642 |
[32m[20230207 15:01:31 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:01:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -144.82
[32m[20230207 15:01:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -107.66
[32m[20230207 15:01:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -119.45
[32m[20230207 15:01:32 @agent_ppo2.py:150][0m Total time:       0.85 min
[32m[20230207 15:01:32 @agent_ppo2.py:152][0m 49152 total steps have happened
[32m[20230207 15:01:32 @agent_ppo2.py:128][0m #------------------------ Iteration 24 --------------------------#
[32m[20230207 15:01:33 @agent_ppo2.py:134][0m Sampling time: 0.64 s by 1 slaves
[32m[20230207 15:01:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:33 @agent_ppo2.py:192][0m |          -0.0000 |          18.7309 |           0.0659 |
[32m[20230207 15:01:33 @agent_ppo2.py:192][0m |          -0.0008 |           7.5022 |           0.0659 |
[32m[20230207 15:01:33 @agent_ppo2.py:192][0m |          -0.0018 |           6.6997 |           0.0658 |
[32m[20230207 15:01:33 @agent_ppo2.py:192][0m |          -0.0031 |           6.2755 |           0.0658 |
[32m[20230207 15:01:33 @agent_ppo2.py:192][0m |          -0.0019 |           6.0505 |           0.0658 |
[32m[20230207 15:01:33 @agent_ppo2.py:192][0m |          -0.0038 |           5.8722 |           0.0658 |
[32m[20230207 15:01:33 @agent_ppo2.py:192][0m |          -0.0036 |           5.6635 |           0.0658 |
[32m[20230207 15:01:33 @agent_ppo2.py:192][0m |          -0.0041 |           5.4965 |           0.0658 |
[32m[20230207 15:01:33 @agent_ppo2.py:192][0m |          -0.0036 |           5.3576 |           0.0658 |
[32m[20230207 15:01:33 @agent_ppo2.py:192][0m |          -0.0064 |           5.1904 |           0.0658 |
[32m[20230207 15:01:33 @agent_ppo2.py:137][0m Policy update time: 0.61 s
[32m[20230207 15:01:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: -139.06
[32m[20230207 15:01:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.24
[32m[20230207 15:01:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -122.58
[32m[20230207 15:01:34 @agent_ppo2.py:150][0m Total time:       0.88 min
[32m[20230207 15:01:34 @agent_ppo2.py:152][0m 51200 total steps have happened
[32m[20230207 15:01:34 @agent_ppo2.py:128][0m #------------------------ Iteration 25 --------------------------#
[32m[20230207 15:01:35 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:01:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:35 @agent_ppo2.py:192][0m |          -0.0247 |          12.2695 |           0.0639 |
[32m[20230207 15:01:35 @agent_ppo2.py:192][0m |           0.0010 |           7.8183 |           0.0639 |
[32m[20230207 15:01:35 @agent_ppo2.py:192][0m |          -0.0001 |           7.1418 |           0.0639 |
[32m[20230207 15:01:35 @agent_ppo2.py:192][0m |          -0.0051 |           7.0130 |           0.0639 |
[32m[20230207 15:01:35 @agent_ppo2.py:192][0m |          -0.0063 |           6.7955 |           0.0639 |
[32m[20230207 15:01:35 @agent_ppo2.py:192][0m |          -0.0095 |           6.7222 |           0.0639 |
[32m[20230207 15:01:35 @agent_ppo2.py:192][0m |          -0.0119 |           7.0152 |           0.0639 |
[32m[20230207 15:01:36 @agent_ppo2.py:192][0m |          -0.0055 |           6.6041 |           0.0639 |
[32m[20230207 15:01:36 @agent_ppo2.py:192][0m |          -0.0006 |           6.5362 |           0.0639 |
[32m[20230207 15:01:36 @agent_ppo2.py:192][0m |          -0.0065 |           6.5388 |           0.0639 |
[32m[20230207 15:01:36 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:01:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -125.22
[32m[20230207 15:01:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -106.48
[32m[20230207 15:01:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -120.88
[32m[20230207 15:01:36 @agent_ppo2.py:150][0m Total time:       0.92 min
[32m[20230207 15:01:36 @agent_ppo2.py:152][0m 53248 total steps have happened
[32m[20230207 15:01:36 @agent_ppo2.py:128][0m #------------------------ Iteration 26 --------------------------#
[32m[20230207 15:01:37 @agent_ppo2.py:134][0m Sampling time: 0.93 s by 1 slaves
[32m[20230207 15:01:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:37 @agent_ppo2.py:192][0m |           0.0009 |          18.3246 |           0.0658 |
[32m[20230207 15:01:38 @agent_ppo2.py:192][0m |          -0.0034 |          14.9307 |           0.0658 |
[32m[20230207 15:01:38 @agent_ppo2.py:192][0m |          -0.0051 |          15.3702 |           0.0658 |
[32m[20230207 15:01:38 @agent_ppo2.py:192][0m |          -0.0034 |          14.0351 |           0.0658 |
[32m[20230207 15:01:38 @agent_ppo2.py:192][0m |          -0.0064 |          14.1786 |           0.0658 |
[32m[20230207 15:01:38 @agent_ppo2.py:192][0m |          -0.0072 |          13.6736 |           0.0658 |
[32m[20230207 15:01:38 @agent_ppo2.py:192][0m |          -0.0044 |          13.6043 |           0.0658 |
[32m[20230207 15:01:38 @agent_ppo2.py:192][0m |          -0.0045 |          13.5777 |           0.0658 |
[32m[20230207 15:01:38 @agent_ppo2.py:192][0m |          -0.0089 |          14.3385 |           0.0658 |
[32m[20230207 15:01:38 @agent_ppo2.py:192][0m |          -0.0050 |          13.3425 |           0.0659 |
[32m[20230207 15:01:38 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:01:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.56
[32m[20230207 15:01:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.15
[32m[20230207 15:01:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -159.17
[32m[20230207 15:01:39 @agent_ppo2.py:150][0m Total time:       0.96 min
[32m[20230207 15:01:39 @agent_ppo2.py:152][0m 55296 total steps have happened
[32m[20230207 15:01:39 @agent_ppo2.py:128][0m #------------------------ Iteration 27 --------------------------#
[32m[20230207 15:01:40 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:01:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:40 @agent_ppo2.py:192][0m |           0.0067 |          33.2408 |           0.0645 |
[32m[20230207 15:01:40 @agent_ppo2.py:192][0m |           0.0001 |          25.2353 |           0.0645 |
[32m[20230207 15:01:40 @agent_ppo2.py:192][0m |          -0.0031 |          24.3732 |           0.0645 |
[32m[20230207 15:01:40 @agent_ppo2.py:192][0m |          -0.0142 |          23.6101 |           0.0644 |
[32m[20230207 15:01:40 @agent_ppo2.py:192][0m |          -0.0116 |          23.5105 |           0.0644 |
[32m[20230207 15:01:40 @agent_ppo2.py:192][0m |          -0.0081 |          23.1389 |           0.0644 |
[32m[20230207 15:01:40 @agent_ppo2.py:192][0m |           0.0027 |          23.1484 |           0.0644 |
[32m[20230207 15:01:40 @agent_ppo2.py:192][0m |          -0.0127 |          22.8359 |           0.0644 |
[32m[20230207 15:01:40 @agent_ppo2.py:192][0m |          -0.0104 |          22.8575 |           0.0644 |
[32m[20230207 15:01:40 @agent_ppo2.py:192][0m |          -0.0036 |          22.6287 |           0.0644 |
[32m[20230207 15:01:40 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:01:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -161.90
[32m[20230207 15:01:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -146.41
[32m[20230207 15:01:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -147.98
[32m[20230207 15:01:41 @agent_ppo2.py:150][0m Total time:       0.99 min
[32m[20230207 15:01:41 @agent_ppo2.py:152][0m 57344 total steps have happened
[32m[20230207 15:01:41 @agent_ppo2.py:128][0m #------------------------ Iteration 28 --------------------------#
[32m[20230207 15:01:42 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:01:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:42 @agent_ppo2.py:192][0m |           0.0012 |          12.5883 |           0.0657 |
[32m[20230207 15:01:42 @agent_ppo2.py:192][0m |          -0.0021 |           7.9536 |           0.0657 |
[32m[20230207 15:01:42 @agent_ppo2.py:192][0m |           0.0006 |           7.5796 |           0.0658 |
[32m[20230207 15:01:42 @agent_ppo2.py:192][0m |          -0.0045 |           7.1484 |           0.0657 |
[32m[20230207 15:01:42 @agent_ppo2.py:192][0m |          -0.0065 |           6.9124 |           0.0657 |
[32m[20230207 15:01:42 @agent_ppo2.py:192][0m |          -0.0066 |           6.7984 |           0.0658 |
[32m[20230207 15:01:42 @agent_ppo2.py:192][0m |          -0.0142 |           6.4999 |           0.0657 |
[32m[20230207 15:01:42 @agent_ppo2.py:192][0m |          -0.0032 |           6.1864 |           0.0658 |
[32m[20230207 15:01:42 @agent_ppo2.py:192][0m |          -0.0033 |           6.0672 |           0.0658 |
[32m[20230207 15:01:42 @agent_ppo2.py:192][0m |          -0.0063 |           5.8816 |           0.0658 |
[32m[20230207 15:01:42 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:01:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -160.10
[32m[20230207 15:01:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -105.96
[32m[20230207 15:01:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -120.30
[32m[20230207 15:01:43 @agent_ppo2.py:150][0m Total time:       1.04 min
[32m[20230207 15:01:43 @agent_ppo2.py:152][0m 59392 total steps have happened
[32m[20230207 15:01:43 @agent_ppo2.py:128][0m #------------------------ Iteration 29 --------------------------#
[32m[20230207 15:01:44 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 15:01:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:44 @agent_ppo2.py:192][0m |          -0.0023 |          23.5910 |           0.0656 |
[32m[20230207 15:01:44 @agent_ppo2.py:192][0m |          -0.0012 |          13.7455 |           0.0656 |
[32m[20230207 15:01:44 @agent_ppo2.py:192][0m |          -0.0024 |          11.4112 |           0.0656 |
[32m[20230207 15:01:44 @agent_ppo2.py:192][0m |          -0.0048 |           9.9759 |           0.0655 |
[32m[20230207 15:01:44 @agent_ppo2.py:192][0m |          -0.0059 |           9.4836 |           0.0655 |
[32m[20230207 15:01:44 @agent_ppo2.py:192][0m |          -0.0065 |           9.4127 |           0.0654 |
[32m[20230207 15:01:44 @agent_ppo2.py:192][0m |          -0.0074 |           8.9443 |           0.0654 |
[32m[20230207 15:01:44 @agent_ppo2.py:192][0m |          -0.0067 |           8.5148 |           0.0653 |
[32m[20230207 15:01:44 @agent_ppo2.py:192][0m |          -0.0096 |           8.3461 |           0.0653 |
[32m[20230207 15:01:44 @agent_ppo2.py:192][0m |          -0.0101 |           8.2117 |           0.0653 |
[32m[20230207 15:01:44 @agent_ppo2.py:137][0m Policy update time: 0.53 s
[32m[20230207 15:01:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: -179.47
[32m[20230207 15:01:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -116.69
[32m[20230207 15:01:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -136.64
[32m[20230207 15:01:45 @agent_ppo2.py:150][0m Total time:       1.07 min
[32m[20230207 15:01:45 @agent_ppo2.py:152][0m 61440 total steps have happened
[32m[20230207 15:01:45 @agent_ppo2.py:128][0m #------------------------ Iteration 30 --------------------------#
[32m[20230207 15:01:46 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 15:01:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:46 @agent_ppo2.py:192][0m |          -0.0002 |          20.0312 |           0.0665 |
[32m[20230207 15:01:46 @agent_ppo2.py:192][0m |          -0.0020 |          17.0551 |           0.0664 |
[32m[20230207 15:01:46 @agent_ppo2.py:192][0m |          -0.0040 |          16.3758 |           0.0664 |
[32m[20230207 15:01:46 @agent_ppo2.py:192][0m |          -0.0048 |          15.7419 |           0.0664 |
[32m[20230207 15:01:46 @agent_ppo2.py:192][0m |          -0.0056 |          15.2982 |           0.0664 |
[32m[20230207 15:01:46 @agent_ppo2.py:192][0m |          -0.0061 |          14.4111 |           0.0664 |
[32m[20230207 15:01:47 @agent_ppo2.py:192][0m |          -0.0063 |          13.7753 |           0.0664 |
[32m[20230207 15:01:47 @agent_ppo2.py:192][0m |          -0.0069 |          13.0374 |           0.0664 |
[32m[20230207 15:01:47 @agent_ppo2.py:192][0m |          -0.0073 |          12.8199 |           0.0664 |
[32m[20230207 15:01:47 @agent_ppo2.py:192][0m |          -0.0076 |          12.2771 |           0.0664 |
[32m[20230207 15:01:47 @agent_ppo2.py:137][0m Policy update time: 0.69 s
[32m[20230207 15:01:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -175.30
[32m[20230207 15:01:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -119.43
[32m[20230207 15:01:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -126.42
[32m[20230207 15:01:48 @agent_ppo2.py:150][0m Total time:       1.11 min
[32m[20230207 15:01:48 @agent_ppo2.py:152][0m 63488 total steps have happened
[32m[20230207 15:01:48 @agent_ppo2.py:128][0m #------------------------ Iteration 31 --------------------------#
[32m[20230207 15:01:48 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:01:48 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:48 @agent_ppo2.py:192][0m |           0.0007 |          19.8572 |           0.0660 |
[32m[20230207 15:01:48 @agent_ppo2.py:192][0m |          -0.0038 |          13.7350 |           0.0660 |
[32m[20230207 15:01:48 @agent_ppo2.py:192][0m |          -0.0025 |          12.8888 |           0.0660 |
[32m[20230207 15:01:48 @agent_ppo2.py:192][0m |          -0.0024 |          12.6514 |           0.0659 |
[32m[20230207 15:01:48 @agent_ppo2.py:192][0m |          -0.0116 |          13.0945 |           0.0659 |
[32m[20230207 15:01:49 @agent_ppo2.py:192][0m |          -0.0086 |          12.2921 |           0.0659 |
[32m[20230207 15:01:49 @agent_ppo2.py:192][0m |          -0.0074 |          12.0857 |           0.0658 |
[32m[20230207 15:01:49 @agent_ppo2.py:192][0m |          -0.0078 |          12.0841 |           0.0658 |
[32m[20230207 15:01:49 @agent_ppo2.py:192][0m |          -0.0103 |          12.0663 |           0.0658 |
[32m[20230207 15:01:49 @agent_ppo2.py:192][0m |          -0.0091 |          11.8377 |           0.0658 |
[32m[20230207 15:01:49 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:01:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: -166.76
[32m[20230207 15:01:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -153.20
[32m[20230207 15:01:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -115.24
[32m[20230207 15:01:49 @agent_ppo2.py:150][0m Total time:       1.14 min
[32m[20230207 15:01:49 @agent_ppo2.py:152][0m 65536 total steps have happened
[32m[20230207 15:01:49 @agent_ppo2.py:128][0m #------------------------ Iteration 32 --------------------------#
[32m[20230207 15:01:50 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:01:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:50 @agent_ppo2.py:192][0m |           0.0006 |           9.3056 |           0.0659 |
[32m[20230207 15:01:50 @agent_ppo2.py:192][0m |          -0.0006 |           6.6696 |           0.0659 |
[32m[20230207 15:01:51 @agent_ppo2.py:192][0m |          -0.0012 |           6.4086 |           0.0660 |
[32m[20230207 15:01:51 @agent_ppo2.py:192][0m |          -0.0023 |           6.2502 |           0.0660 |
[32m[20230207 15:01:51 @agent_ppo2.py:192][0m |          -0.0030 |           6.1575 |           0.0660 |
[32m[20230207 15:01:51 @agent_ppo2.py:192][0m |          -0.0033 |           6.1110 |           0.0660 |
[32m[20230207 15:01:51 @agent_ppo2.py:192][0m |          -0.0038 |           6.0723 |           0.0660 |
[32m[20230207 15:01:51 @agent_ppo2.py:192][0m |          -0.0042 |           6.0911 |           0.0661 |
[32m[20230207 15:01:51 @agent_ppo2.py:192][0m |          -0.0051 |           5.9983 |           0.0661 |
[32m[20230207 15:01:51 @agent_ppo2.py:192][0m |          -0.0051 |           5.9513 |           0.0661 |
[32m[20230207 15:01:51 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:01:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: -126.80
[32m[20230207 15:01:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.88
[32m[20230207 15:01:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -154.50
[32m[20230207 15:01:52 @agent_ppo2.py:150][0m Total time:       1.18 min
[32m[20230207 15:01:52 @agent_ppo2.py:152][0m 67584 total steps have happened
[32m[20230207 15:01:52 @agent_ppo2.py:128][0m #------------------------ Iteration 33 --------------------------#
[32m[20230207 15:01:53 @agent_ppo2.py:134][0m Sampling time: 0.64 s by 1 slaves
[32m[20230207 15:01:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:53 @agent_ppo2.py:192][0m |           0.0046 |          25.3372 |           0.0665 |
[32m[20230207 15:01:53 @agent_ppo2.py:192][0m |          -0.0017 |          15.0419 |           0.0664 |
[32m[20230207 15:01:53 @agent_ppo2.py:192][0m |          -0.0099 |          12.2557 |           0.0664 |
[32m[20230207 15:01:53 @agent_ppo2.py:192][0m |           0.0063 |          11.0818 |           0.0664 |
[32m[20230207 15:01:53 @agent_ppo2.py:192][0m |          -0.0043 |          10.6552 |           0.0664 |
[32m[20230207 15:01:53 @agent_ppo2.py:192][0m |          -0.0169 |          10.1924 |           0.0663 |
[32m[20230207 15:01:53 @agent_ppo2.py:192][0m |          -0.0052 |           9.4047 |           0.0663 |
[32m[20230207 15:01:53 @agent_ppo2.py:192][0m |          -0.0205 |           8.9684 |           0.0663 |
[32m[20230207 15:01:53 @agent_ppo2.py:192][0m |          -0.0085 |           8.7121 |           0.0663 |
[32m[20230207 15:01:53 @agent_ppo2.py:192][0m |          -0.0023 |           8.2882 |           0.0663 |
[32m[20230207 15:01:53 @agent_ppo2.py:137][0m Policy update time: 0.61 s
[32m[20230207 15:01:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: -113.18
[32m[20230207 15:01:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.40
[32m[20230207 15:01:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -143.83
[32m[20230207 15:01:54 @agent_ppo2.py:150][0m Total time:       1.21 min
[32m[20230207 15:01:54 @agent_ppo2.py:152][0m 69632 total steps have happened
[32m[20230207 15:01:54 @agent_ppo2.py:128][0m #------------------------ Iteration 34 --------------------------#
[32m[20230207 15:01:54 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:01:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:54 @agent_ppo2.py:192][0m |          -0.0009 |          15.7131 |           0.0669 |
[32m[20230207 15:01:55 @agent_ppo2.py:192][0m |          -0.0009 |           7.5815 |           0.0669 |
[32m[20230207 15:01:55 @agent_ppo2.py:192][0m |          -0.0014 |           6.1559 |           0.0669 |
[32m[20230207 15:01:55 @agent_ppo2.py:192][0m |          -0.0027 |           5.3952 |           0.0669 |
[32m[20230207 15:01:55 @agent_ppo2.py:192][0m |          -0.0029 |           5.0230 |           0.0669 |
[32m[20230207 15:01:55 @agent_ppo2.py:192][0m |          -0.0037 |           4.6549 |           0.0669 |
[32m[20230207 15:01:55 @agent_ppo2.py:192][0m |          -0.0032 |           4.5082 |           0.0669 |
[32m[20230207 15:01:55 @agent_ppo2.py:192][0m |          -0.0035 |           4.2323 |           0.0669 |
[32m[20230207 15:01:55 @agent_ppo2.py:192][0m |          -0.0054 |           4.0928 |           0.0669 |
[32m[20230207 15:01:55 @agent_ppo2.py:192][0m |          -0.0042 |           3.9282 |           0.0669 |
[32m[20230207 15:01:55 @agent_ppo2.py:137][0m Policy update time: 0.53 s
[32m[20230207 15:01:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: -118.99
[32m[20230207 15:01:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.28
[32m[20230207 15:01:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -149.59
[32m[20230207 15:01:56 @agent_ppo2.py:150][0m Total time:       1.24 min
[32m[20230207 15:01:56 @agent_ppo2.py:152][0m 71680 total steps have happened
[32m[20230207 15:01:56 @agent_ppo2.py:128][0m #------------------------ Iteration 35 --------------------------#
[32m[20230207 15:01:56 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:01:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:57 @agent_ppo2.py:192][0m |          -0.0074 |          11.0079 |           0.0663 |
[32m[20230207 15:01:57 @agent_ppo2.py:192][0m |          -0.0008 |           8.1298 |           0.0664 |
[32m[20230207 15:01:57 @agent_ppo2.py:192][0m |           0.0001 |           7.7799 |           0.0664 |
[32m[20230207 15:01:57 @agent_ppo2.py:192][0m |          -0.0017 |           7.4258 |           0.0664 |
[32m[20230207 15:01:57 @agent_ppo2.py:192][0m |          -0.0129 |           7.3851 |           0.0665 |
[32m[20230207 15:01:57 @agent_ppo2.py:192][0m |          -0.0032 |           7.2853 |           0.0665 |
[32m[20230207 15:01:57 @agent_ppo2.py:192][0m |          -0.0064 |           7.4583 |           0.0665 |
[32m[20230207 15:01:57 @agent_ppo2.py:192][0m |          -0.0002 |           7.0469 |           0.0665 |
[32m[20230207 15:01:57 @agent_ppo2.py:192][0m |          -0.0129 |           7.9694 |           0.0665 |
[32m[20230207 15:01:57 @agent_ppo2.py:192][0m |          -0.0121 |           7.0616 |           0.0665 |
[32m[20230207 15:01:57 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:01:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: -120.93
[32m[20230207 15:01:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.67
[32m[20230207 15:01:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -125.57
[32m[20230207 15:01:58 @agent_ppo2.py:150][0m Total time:       1.28 min
[32m[20230207 15:01:58 @agent_ppo2.py:152][0m 73728 total steps have happened
[32m[20230207 15:01:58 @agent_ppo2.py:128][0m #------------------------ Iteration 36 --------------------------#
[32m[20230207 15:01:59 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:01:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:01:59 @agent_ppo2.py:192][0m |           0.0004 |          11.9462 |           0.0687 |
[32m[20230207 15:01:59 @agent_ppo2.py:192][0m |          -0.0014 |           7.9846 |           0.0687 |
[32m[20230207 15:01:59 @agent_ppo2.py:192][0m |          -0.0026 |           7.1974 |           0.0687 |
[32m[20230207 15:01:59 @agent_ppo2.py:192][0m |          -0.0040 |           6.5931 |           0.0687 |
[32m[20230207 15:01:59 @agent_ppo2.py:192][0m |          -0.0047 |           6.1276 |           0.0687 |
[32m[20230207 15:01:59 @agent_ppo2.py:192][0m |          -0.0056 |           5.9746 |           0.0686 |
[32m[20230207 15:01:59 @agent_ppo2.py:192][0m |          -0.0060 |           5.8599 |           0.0686 |
[32m[20230207 15:01:59 @agent_ppo2.py:192][0m |          -0.0063 |           5.8015 |           0.0686 |
[32m[20230207 15:01:59 @agent_ppo2.py:192][0m |          -0.0078 |           5.5992 |           0.0686 |
[32m[20230207 15:01:59 @agent_ppo2.py:192][0m |          -0.0074 |           5.4088 |           0.0686 |
[32m[20230207 15:01:59 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:02:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.81
[32m[20230207 15:02:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.99
[32m[20230207 15:02:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -128.76
[32m[20230207 15:02:00 @agent_ppo2.py:150][0m Total time:       1.32 min
[32m[20230207 15:02:00 @agent_ppo2.py:152][0m 75776 total steps have happened
[32m[20230207 15:02:00 @agent_ppo2.py:128][0m #------------------------ Iteration 37 --------------------------#
[32m[20230207 15:02:01 @agent_ppo2.py:134][0m Sampling time: 0.94 s by 1 slaves
[32m[20230207 15:02:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:01 @agent_ppo2.py:192][0m |           0.0001 |          44.6145 |           0.0671 |
[32m[20230207 15:02:01 @agent_ppo2.py:192][0m |          -0.0013 |          29.5803 |           0.0672 |
[32m[20230207 15:02:01 @agent_ppo2.py:192][0m |          -0.0033 |          26.1387 |           0.0673 |
[32m[20230207 15:02:02 @agent_ppo2.py:192][0m |          -0.0044 |          24.4772 |           0.0673 |
[32m[20230207 15:02:02 @agent_ppo2.py:192][0m |          -0.0050 |          22.8642 |           0.0674 |
[32m[20230207 15:02:02 @agent_ppo2.py:192][0m |          -0.0046 |          21.3340 |           0.0675 |
[32m[20230207 15:02:02 @agent_ppo2.py:192][0m |          -0.0035 |          20.2044 |           0.0675 |
[32m[20230207 15:02:02 @agent_ppo2.py:192][0m |          -0.0058 |          18.6218 |           0.0676 |
[32m[20230207 15:02:02 @agent_ppo2.py:192][0m |          -0.0069 |          17.5767 |           0.0676 |
[32m[20230207 15:02:02 @agent_ppo2.py:192][0m |          -0.0060 |          17.3495 |           0.0677 |
[32m[20230207 15:02:02 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:02:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -125.89
[32m[20230207 15:02:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -109.63
[32m[20230207 15:02:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -140.58
[32m[20230207 15:02:03 @agent_ppo2.py:150][0m Total time:       1.36 min
[32m[20230207 15:02:03 @agent_ppo2.py:152][0m 77824 total steps have happened
[32m[20230207 15:02:03 @agent_ppo2.py:128][0m #------------------------ Iteration 38 --------------------------#
[32m[20230207 15:02:04 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:02:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:04 @agent_ppo2.py:192][0m |          -0.0007 |          16.1525 |           0.0698 |
[32m[20230207 15:02:04 @agent_ppo2.py:192][0m |          -0.0022 |           9.9277 |           0.0698 |
[32m[20230207 15:02:04 @agent_ppo2.py:192][0m |          -0.0017 |           9.3270 |           0.0697 |
[32m[20230207 15:02:04 @agent_ppo2.py:192][0m |          -0.0053 |           9.0908 |           0.0697 |
[32m[20230207 15:02:04 @agent_ppo2.py:192][0m |          -0.0049 |           8.9170 |           0.0697 |
[32m[20230207 15:02:04 @agent_ppo2.py:192][0m |          -0.0066 |           8.8161 |           0.0697 |
[32m[20230207 15:02:04 @agent_ppo2.py:192][0m |          -0.0039 |           8.6393 |           0.0697 |
[32m[20230207 15:02:04 @agent_ppo2.py:192][0m |          -0.0057 |           8.5332 |           0.0697 |
[32m[20230207 15:02:05 @agent_ppo2.py:192][0m |          -0.0079 |           8.5288 |           0.0697 |
[32m[20230207 15:02:05 @agent_ppo2.py:192][0m |          -0.0086 |           8.4227 |           0.0696 |
[32m[20230207 15:02:05 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:02:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -126.49
[32m[20230207 15:02:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -107.29
[32m[20230207 15:02:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -119.44
[32m[20230207 15:02:05 @agent_ppo2.py:150][0m Total time:       1.40 min
[32m[20230207 15:02:05 @agent_ppo2.py:152][0m 79872 total steps have happened
[32m[20230207 15:02:05 @agent_ppo2.py:128][0m #------------------------ Iteration 39 --------------------------#
[32m[20230207 15:02:06 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:02:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:06 @agent_ppo2.py:192][0m |           0.0011 |          19.0877 |           0.0694 |
[32m[20230207 15:02:06 @agent_ppo2.py:192][0m |           0.0035 |          12.9283 |           0.0695 |
[32m[20230207 15:02:06 @agent_ppo2.py:192][0m |           0.0049 |          11.6659 |           0.0694 |
[32m[20230207 15:02:06 @agent_ppo2.py:192][0m |          -0.0009 |          11.3111 |           0.0694 |
[32m[20230207 15:02:06 @agent_ppo2.py:192][0m |          -0.0023 |          10.8830 |           0.0694 |
[32m[20230207 15:02:06 @agent_ppo2.py:192][0m |          -0.0047 |          10.6068 |           0.0694 |
[32m[20230207 15:02:06 @agent_ppo2.py:192][0m |           0.0014 |          10.5730 |           0.0693 |
[32m[20230207 15:02:06 @agent_ppo2.py:192][0m |          -0.0085 |          10.3135 |           0.0693 |
[32m[20230207 15:02:06 @agent_ppo2.py:192][0m |          -0.0046 |          10.2071 |           0.0693 |
[32m[20230207 15:02:06 @agent_ppo2.py:192][0m |           0.0015 |          10.0961 |           0.0693 |
[32m[20230207 15:02:06 @agent_ppo2.py:137][0m Policy update time: 0.53 s
[32m[20230207 15:02:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.84
[32m[20230207 15:02:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.49
[32m[20230207 15:02:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -118.26
[32m[20230207 15:02:07 @agent_ppo2.py:150][0m Total time:       1.43 min
[32m[20230207 15:02:07 @agent_ppo2.py:152][0m 81920 total steps have happened
[32m[20230207 15:02:07 @agent_ppo2.py:128][0m #------------------------ Iteration 40 --------------------------#
[32m[20230207 15:02:08 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:02:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:08 @agent_ppo2.py:192][0m |          -0.0002 |          29.3627 |           0.0712 |
[32m[20230207 15:02:08 @agent_ppo2.py:192][0m |          -0.0014 |          19.1776 |           0.0712 |
[32m[20230207 15:02:08 @agent_ppo2.py:192][0m |          -0.0030 |          16.8159 |           0.0711 |
[32m[20230207 15:02:08 @agent_ppo2.py:192][0m |          -0.0044 |          15.5481 |           0.0711 |
[32m[20230207 15:02:08 @agent_ppo2.py:192][0m |          -0.0053 |          14.6185 |           0.0710 |
[32m[20230207 15:02:09 @agent_ppo2.py:192][0m |          -0.0051 |          14.1960 |           0.0710 |
[32m[20230207 15:02:09 @agent_ppo2.py:192][0m |          -0.0062 |          13.6007 |           0.0710 |
[32m[20230207 15:02:09 @agent_ppo2.py:192][0m |          -0.0062 |          13.2817 |           0.0710 |
[32m[20230207 15:02:09 @agent_ppo2.py:192][0m |          -0.0068 |          13.1633 |           0.0709 |
[32m[20230207 15:02:09 @agent_ppo2.py:192][0m |          -0.0080 |          12.8225 |           0.0709 |
[32m[20230207 15:02:09 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:02:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -143.34
[32m[20230207 15:02:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -118.76
[32m[20230207 15:02:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -136.17
[32m[20230207 15:02:10 @agent_ppo2.py:150][0m Total time:       1.48 min
[32m[20230207 15:02:10 @agent_ppo2.py:152][0m 83968 total steps have happened
[32m[20230207 15:02:10 @agent_ppo2.py:128][0m #------------------------ Iteration 41 --------------------------#
[32m[20230207 15:02:11 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:02:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:11 @agent_ppo2.py:192][0m |          -0.0027 |          18.4780 |           0.0701 |
[32m[20230207 15:02:11 @agent_ppo2.py:192][0m |          -0.0028 |          15.8552 |           0.0701 |
[32m[20230207 15:02:11 @agent_ppo2.py:192][0m |          -0.0019 |          15.6955 |           0.0701 |
[32m[20230207 15:02:11 @agent_ppo2.py:192][0m |          -0.0006 |          15.4194 |           0.0701 |
[32m[20230207 15:02:11 @agent_ppo2.py:192][0m |          -0.0031 |          15.1493 |           0.0701 |
[32m[20230207 15:02:11 @agent_ppo2.py:192][0m |          -0.0018 |          15.1400 |           0.0701 |
[32m[20230207 15:02:11 @agent_ppo2.py:192][0m |          -0.0072 |          15.5101 |           0.0701 |
[32m[20230207 15:02:11 @agent_ppo2.py:192][0m |          -0.0044 |          14.9263 |           0.0702 |
[32m[20230207 15:02:11 @agent_ppo2.py:192][0m |          -0.0041 |          14.8834 |           0.0702 |
[32m[20230207 15:02:11 @agent_ppo2.py:192][0m |          -0.0049 |          14.6384 |           0.0702 |
[32m[20230207 15:02:11 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:02:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: -121.84
[32m[20230207 15:02:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.02
[32m[20230207 15:02:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -158.56
[32m[20230207 15:02:12 @agent_ppo2.py:150][0m Total time:       1.52 min
[32m[20230207 15:02:12 @agent_ppo2.py:152][0m 86016 total steps have happened
[32m[20230207 15:02:12 @agent_ppo2.py:128][0m #------------------------ Iteration 42 --------------------------#
[32m[20230207 15:02:13 @agent_ppo2.py:134][0m Sampling time: 0.92 s by 1 slaves
[32m[20230207 15:02:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:13 @agent_ppo2.py:192][0m |          -0.0005 |          19.6328 |           0.0683 |
[32m[20230207 15:02:13 @agent_ppo2.py:192][0m |           0.0000 |          14.8474 |           0.0684 |
[32m[20230207 15:02:13 @agent_ppo2.py:192][0m |          -0.0045 |          13.6536 |           0.0684 |
[32m[20230207 15:02:13 @agent_ppo2.py:192][0m |          -0.0038 |          12.7524 |           0.0684 |
[32m[20230207 15:02:14 @agent_ppo2.py:192][0m |          -0.0047 |          11.9410 |           0.0684 |
[32m[20230207 15:02:14 @agent_ppo2.py:192][0m |          -0.0085 |          11.1502 |           0.0684 |
[32m[20230207 15:02:14 @agent_ppo2.py:192][0m |          -0.0090 |          10.7387 |           0.0683 |
[32m[20230207 15:02:14 @agent_ppo2.py:192][0m |          -0.0067 |          10.2941 |           0.0683 |
[32m[20230207 15:02:14 @agent_ppo2.py:192][0m |          -0.0085 |          10.0490 |           0.0683 |
[32m[20230207 15:02:14 @agent_ppo2.py:192][0m |          -0.0120 |           9.9148 |           0.0683 |
[32m[20230207 15:02:14 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:02:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: -120.46
[32m[20230207 15:02:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -108.50
[32m[20230207 15:02:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -124.34
[32m[20230207 15:02:15 @agent_ppo2.py:150][0m Total time:       1.56 min
[32m[20230207 15:02:15 @agent_ppo2.py:152][0m 88064 total steps have happened
[32m[20230207 15:02:15 @agent_ppo2.py:128][0m #------------------------ Iteration 43 --------------------------#
[32m[20230207 15:02:15 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230207 15:02:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:15 @agent_ppo2.py:192][0m |           0.0010 |          10.0422 |           0.0707 |
[32m[20230207 15:02:16 @agent_ppo2.py:192][0m |          -0.0018 |           6.1351 |           0.0706 |
[32m[20230207 15:02:16 @agent_ppo2.py:192][0m |          -0.0042 |           5.4579 |           0.0705 |
[32m[20230207 15:02:16 @agent_ppo2.py:192][0m |          -0.0086 |           5.2285 |           0.0704 |
[32m[20230207 15:02:16 @agent_ppo2.py:192][0m |          -0.0003 |           5.2340 |           0.0703 |
[32m[20230207 15:02:16 @agent_ppo2.py:192][0m |          -0.0091 |           4.9905 |           0.0703 |
[32m[20230207 15:02:16 @agent_ppo2.py:192][0m |          -0.0111 |           4.8740 |           0.0703 |
[32m[20230207 15:02:16 @agent_ppo2.py:192][0m |          -0.0093 |           4.7446 |           0.0703 |
[32m[20230207 15:02:16 @agent_ppo2.py:192][0m |          -0.0084 |           4.6871 |           0.0702 |
[32m[20230207 15:02:16 @agent_ppo2.py:192][0m |          -0.0089 |           4.6484 |           0.0702 |
[32m[20230207 15:02:16 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:02:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: -131.24
[32m[20230207 15:02:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -118.48
[32m[20230207 15:02:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -122.19
[32m[20230207 15:02:17 @agent_ppo2.py:150][0m Total time:       1.59 min
[32m[20230207 15:02:17 @agent_ppo2.py:152][0m 90112 total steps have happened
[32m[20230207 15:02:17 @agent_ppo2.py:128][0m #------------------------ Iteration 44 --------------------------#
[32m[20230207 15:02:17 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:02:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:18 @agent_ppo2.py:192][0m |           0.0010 |          11.4065 |           0.0703 |
[32m[20230207 15:02:18 @agent_ppo2.py:192][0m |          -0.0016 |           9.0096 |           0.0702 |
[32m[20230207 15:02:18 @agent_ppo2.py:192][0m |          -0.0068 |           8.7894 |           0.0702 |
[32m[20230207 15:02:18 @agent_ppo2.py:192][0m |          -0.0028 |           8.5148 |           0.0701 |
[32m[20230207 15:02:18 @agent_ppo2.py:192][0m |          -0.0063 |           8.4562 |           0.0701 |
[32m[20230207 15:02:18 @agent_ppo2.py:192][0m |          -0.0070 |           8.4284 |           0.0700 |
[32m[20230207 15:02:18 @agent_ppo2.py:192][0m |          -0.0058 |           8.3276 |           0.0700 |
[32m[20230207 15:02:18 @agent_ppo2.py:192][0m |          -0.0069 |           8.1647 |           0.0700 |
[32m[20230207 15:02:18 @agent_ppo2.py:192][0m |          -0.0089 |           8.0634 |           0.0700 |
[32m[20230207 15:02:18 @agent_ppo2.py:192][0m |          -0.0087 |           8.0402 |           0.0700 |
[32m[20230207 15:02:18 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:02:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: -115.86
[32m[20230207 15:02:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.62
[32m[20230207 15:02:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -127.32
[32m[20230207 15:02:19 @agent_ppo2.py:150][0m Total time:       1.63 min
[32m[20230207 15:02:19 @agent_ppo2.py:152][0m 92160 total steps have happened
[32m[20230207 15:02:19 @agent_ppo2.py:128][0m #------------------------ Iteration 45 --------------------------#
[32m[20230207 15:02:20 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:02:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:20 @agent_ppo2.py:192][0m |          -0.0010 |          10.4784 |           0.0695 |
[32m[20230207 15:02:20 @agent_ppo2.py:192][0m |           0.0192 |           8.8659 |           0.0694 |
[32m[20230207 15:02:20 @agent_ppo2.py:192][0m |          -0.0060 |           8.7109 |           0.0694 |
[32m[20230207 15:02:20 @agent_ppo2.py:192][0m |          -0.0630 |          11.0020 |           0.0693 |
[32m[20230207 15:02:20 @agent_ppo2.py:192][0m |          -0.0018 |          10.2249 |           0.0693 |
[32m[20230207 15:02:21 @agent_ppo2.py:192][0m |          -0.0015 |           8.8092 |           0.0694 |
[32m[20230207 15:02:21 @agent_ppo2.py:192][0m |           0.0027 |           8.5348 |           0.0694 |
[32m[20230207 15:02:21 @agent_ppo2.py:192][0m |           0.0123 |           8.5726 |           0.0694 |
[32m[20230207 15:02:21 @agent_ppo2.py:192][0m |          -0.0058 |           8.4041 |           0.0694 |
[32m[20230207 15:02:21 @agent_ppo2.py:192][0m |          -0.0092 |           8.2796 |           0.0694 |
[32m[20230207 15:02:21 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:02:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: -121.81
[32m[20230207 15:02:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -119.20
[32m[20230207 15:02:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -114.98
[32m[20230207 15:02:22 @agent_ppo2.py:150][0m Total time:       1.68 min
[32m[20230207 15:02:22 @agent_ppo2.py:152][0m 94208 total steps have happened
[32m[20230207 15:02:22 @agent_ppo2.py:128][0m #------------------------ Iteration 46 --------------------------#
[32m[20230207 15:02:23 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:02:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:23 @agent_ppo2.py:192][0m |           0.0018 |          11.2255 |           0.0681 |
[32m[20230207 15:02:23 @agent_ppo2.py:192][0m |          -0.0043 |           7.2038 |           0.0681 |
[32m[20230207 15:02:23 @agent_ppo2.py:192][0m |           0.0025 |           6.6790 |           0.0681 |
[32m[20230207 15:02:23 @agent_ppo2.py:192][0m |           0.0016 |           6.5555 |           0.0680 |
[32m[20230207 15:02:23 @agent_ppo2.py:192][0m |          -0.0053 |           6.4033 |           0.0680 |
[32m[20230207 15:02:23 @agent_ppo2.py:192][0m |          -0.0024 |           6.3558 |           0.0680 |
[32m[20230207 15:02:23 @agent_ppo2.py:192][0m |          -0.0072 |           6.2824 |           0.0680 |
[32m[20230207 15:02:23 @agent_ppo2.py:192][0m |          -0.0062 |           6.3415 |           0.0680 |
[32m[20230207 15:02:23 @agent_ppo2.py:192][0m |          -0.0066 |           6.2549 |           0.0680 |
[32m[20230207 15:02:23 @agent_ppo2.py:192][0m |          -0.0040 |           6.1927 |           0.0680 |
[32m[20230207 15:02:23 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:02:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: -115.19
[32m[20230207 15:02:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.29
[32m[20230207 15:02:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -120.49
[32m[20230207 15:02:24 @agent_ppo2.py:150][0m Total time:       1.72 min
[32m[20230207 15:02:24 @agent_ppo2.py:152][0m 96256 total steps have happened
[32m[20230207 15:02:24 @agent_ppo2.py:128][0m #------------------------ Iteration 47 --------------------------#
[32m[20230207 15:02:25 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:02:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:25 @agent_ppo2.py:192][0m |          -0.0027 |           9.9299 |           0.0680 |
[32m[20230207 15:02:26 @agent_ppo2.py:192][0m |           0.0007 |           7.8142 |           0.0680 |
[32m[20230207 15:02:26 @agent_ppo2.py:192][0m |          -0.0039 |           7.5300 |           0.0681 |
[32m[20230207 15:02:26 @agent_ppo2.py:192][0m |           0.0203 |           7.3799 |           0.0681 |
[32m[20230207 15:02:26 @agent_ppo2.py:192][0m |           0.0010 |           7.1939 |           0.0681 |
[32m[20230207 15:02:26 @agent_ppo2.py:192][0m |          -0.0002 |           7.0642 |           0.0682 |
[32m[20230207 15:02:26 @agent_ppo2.py:192][0m |           0.0009 |           6.9558 |           0.0682 |
[32m[20230207 15:02:26 @agent_ppo2.py:192][0m |           0.0004 |           6.7919 |           0.0683 |
[32m[20230207 15:02:26 @agent_ppo2.py:192][0m |          -0.0026 |           6.5513 |           0.0683 |
[32m[20230207 15:02:26 @agent_ppo2.py:192][0m |          -0.0057 |           6.2782 |           0.0683 |
[32m[20230207 15:02:26 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:02:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -111.02
[32m[20230207 15:02:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.13
[32m[20230207 15:02:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -135.38
[32m[20230207 15:02:27 @agent_ppo2.py:150][0m Total time:       1.76 min
[32m[20230207 15:02:27 @agent_ppo2.py:152][0m 98304 total steps have happened
[32m[20230207 15:02:27 @agent_ppo2.py:128][0m #------------------------ Iteration 48 --------------------------#
[32m[20230207 15:02:28 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:02:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:28 @agent_ppo2.py:192][0m |           0.0004 |          12.5727 |           0.0721 |
[32m[20230207 15:02:28 @agent_ppo2.py:192][0m |          -0.0007 |           9.3193 |           0.0721 |
[32m[20230207 15:02:28 @agent_ppo2.py:192][0m |          -0.0013 |           8.9336 |           0.0722 |
[32m[20230207 15:02:28 @agent_ppo2.py:192][0m |          -0.0024 |           8.7519 |           0.0722 |
[32m[20230207 15:02:28 @agent_ppo2.py:192][0m |          -0.0036 |           8.6904 |           0.0722 |
[32m[20230207 15:02:28 @agent_ppo2.py:192][0m |          -0.0041 |           8.5063 |           0.0722 |
[32m[20230207 15:02:28 @agent_ppo2.py:192][0m |          -0.0048 |           8.4406 |           0.0723 |
[32m[20230207 15:02:28 @agent_ppo2.py:192][0m |          -0.0053 |           8.3881 |           0.0723 |
[32m[20230207 15:02:28 @agent_ppo2.py:192][0m |          -0.0059 |           8.2758 |           0.0723 |
[32m[20230207 15:02:28 @agent_ppo2.py:192][0m |          -0.0067 |           8.2724 |           0.0723 |
[32m[20230207 15:02:28 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:02:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -120.88
[32m[20230207 15:02:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -106.25
[32m[20230207 15:02:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -111.39
[32m[20230207 15:02:29 @agent_ppo2.py:150][0m Total time:       1.80 min
[32m[20230207 15:02:29 @agent_ppo2.py:152][0m 100352 total steps have happened
[32m[20230207 15:02:29 @agent_ppo2.py:128][0m #------------------------ Iteration 49 --------------------------#
[32m[20230207 15:02:30 @agent_ppo2.py:134][0m Sampling time: 0.61 s by 1 slaves
[32m[20230207 15:02:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:30 @agent_ppo2.py:192][0m |          -0.0003 |          15.4979 |           0.0741 |
[32m[20230207 15:02:30 @agent_ppo2.py:192][0m |          -0.0019 |           8.8195 |           0.0741 |
[32m[20230207 15:02:30 @agent_ppo2.py:192][0m |          -0.0048 |           8.0830 |           0.0741 |
[32m[20230207 15:02:30 @agent_ppo2.py:192][0m |          -0.0063 |           7.8907 |           0.0740 |
[32m[20230207 15:02:30 @agent_ppo2.py:192][0m |          -0.0081 |           7.4477 |           0.0740 |
[32m[20230207 15:02:30 @agent_ppo2.py:192][0m |          -0.0092 |           7.2546 |           0.0740 |
[32m[20230207 15:02:30 @agent_ppo2.py:192][0m |          -0.0105 |           7.1648 |           0.0739 |
[32m[20230207 15:02:30 @agent_ppo2.py:192][0m |          -0.0109 |           7.1929 |           0.0739 |
[32m[20230207 15:02:30 @agent_ppo2.py:192][0m |          -0.0115 |           7.0641 |           0.0739 |
[32m[20230207 15:02:31 @agent_ppo2.py:192][0m |          -0.0117 |           7.1259 |           0.0739 |
[32m[20230207 15:02:31 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:02:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -131.77
[32m[20230207 15:02:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -113.73
[32m[20230207 15:02:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -125.81
[32m[20230207 15:02:31 @agent_ppo2.py:150][0m Total time:       1.84 min
[32m[20230207 15:02:31 @agent_ppo2.py:152][0m 102400 total steps have happened
[32m[20230207 15:02:31 @agent_ppo2.py:128][0m #------------------------ Iteration 50 --------------------------#
[32m[20230207 15:02:32 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:02:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:32 @agent_ppo2.py:192][0m |           0.0021 |          12.2371 |           0.0716 |
[32m[20230207 15:02:32 @agent_ppo2.py:192][0m |           0.0126 |           9.8306 |           0.0715 |
[32m[20230207 15:02:33 @agent_ppo2.py:192][0m |          -0.0077 |           9.8228 |           0.0715 |
[32m[20230207 15:02:33 @agent_ppo2.py:192][0m |           0.0057 |           9.6205 |           0.0715 |
[32m[20230207 15:02:33 @agent_ppo2.py:192][0m |          -0.0062 |           9.5824 |           0.0714 |
[32m[20230207 15:02:33 @agent_ppo2.py:192][0m |          -0.0010 |           9.4870 |           0.0714 |
[32m[20230207 15:02:33 @agent_ppo2.py:192][0m |           0.0003 |           9.4190 |           0.0714 |
[32m[20230207 15:02:33 @agent_ppo2.py:192][0m |          -0.0026 |           9.4295 |           0.0714 |
[32m[20230207 15:02:33 @agent_ppo2.py:192][0m |          -0.0045 |           9.3714 |           0.0714 |
[32m[20230207 15:02:33 @agent_ppo2.py:192][0m |          -0.0186 |           9.3112 |           0.0714 |
[32m[20230207 15:02:33 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:02:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: -140.43
[32m[20230207 15:02:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -135.71
[32m[20230207 15:02:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -106.25
[32m[20230207 15:02:34 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -106.25
[32m[20230207 15:02:34 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -106.25
[32m[20230207 15:02:34 @agent_ppo2.py:150][0m Total time:       1.88 min
[32m[20230207 15:02:34 @agent_ppo2.py:152][0m 104448 total steps have happened
[32m[20230207 15:02:34 @agent_ppo2.py:128][0m #------------------------ Iteration 51 --------------------------#
[32m[20230207 15:02:35 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:02:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:35 @agent_ppo2.py:192][0m |           0.0126 |           9.2595 |           0.0696 |
[32m[20230207 15:02:35 @agent_ppo2.py:192][0m |           0.0030 |           7.6852 |           0.0695 |
[32m[20230207 15:02:35 @agent_ppo2.py:192][0m |           0.0008 |           7.5418 |           0.0695 |
[32m[20230207 15:02:35 @agent_ppo2.py:192][0m |          -0.0058 |           7.4585 |           0.0695 |
[32m[20230207 15:02:35 @agent_ppo2.py:192][0m |          -0.0025 |           7.4157 |           0.0695 |
[32m[20230207 15:02:35 @agent_ppo2.py:192][0m |          -0.0150 |           7.8779 |           0.0695 |
[32m[20230207 15:02:35 @agent_ppo2.py:192][0m |          -0.0055 |           7.3884 |           0.0694 |
[32m[20230207 15:02:35 @agent_ppo2.py:192][0m |           0.0044 |           7.8492 |           0.0694 |
[32m[20230207 15:02:36 @agent_ppo2.py:192][0m |          -0.0058 |           7.2995 |           0.0694 |
[32m[20230207 15:02:36 @agent_ppo2.py:192][0m |          -0.0060 |           7.3195 |           0.0694 |
[32m[20230207 15:02:36 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:02:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -126.49
[32m[20230207 15:02:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.80
[32m[20230207 15:02:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -132.58
[32m[20230207 15:02:36 @agent_ppo2.py:150][0m Total time:       1.92 min
[32m[20230207 15:02:36 @agent_ppo2.py:152][0m 106496 total steps have happened
[32m[20230207 15:02:36 @agent_ppo2.py:128][0m #------------------------ Iteration 52 --------------------------#
[32m[20230207 15:02:37 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:02:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:37 @agent_ppo2.py:192][0m |          -0.0017 |           5.9422 |           0.0715 |
[32m[20230207 15:02:37 @agent_ppo2.py:192][0m |           0.0036 |           4.7766 |           0.0715 |
[32m[20230207 15:02:38 @agent_ppo2.py:192][0m |          -0.0748 |           7.3888 |           0.0715 |
[32m[20230207 15:02:38 @agent_ppo2.py:192][0m |           0.0012 |           5.3050 |           0.0716 |
[32m[20230207 15:02:38 @agent_ppo2.py:192][0m |           0.0042 |           4.4782 |           0.0716 |
[32m[20230207 15:02:38 @agent_ppo2.py:192][0m |          -0.0026 |           4.3766 |           0.0716 |
[32m[20230207 15:02:38 @agent_ppo2.py:192][0m |          -0.0069 |           4.3502 |           0.0716 |
[32m[20230207 15:02:38 @agent_ppo2.py:192][0m |           0.0015 |           4.2848 |           0.0716 |
[32m[20230207 15:02:38 @agent_ppo2.py:192][0m |          -0.0006 |           4.1951 |           0.0716 |
[32m[20230207 15:02:38 @agent_ppo2.py:192][0m |          -0.0121 |           4.1799 |           0.0716 |
[32m[20230207 15:02:38 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:02:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -126.01
[32m[20230207 15:02:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -125.75
[32m[20230207 15:02:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -134.42
[32m[20230207 15:02:39 @agent_ppo2.py:150][0m Total time:       1.96 min
[32m[20230207 15:02:39 @agent_ppo2.py:152][0m 108544 total steps have happened
[32m[20230207 15:02:39 @agent_ppo2.py:128][0m #------------------------ Iteration 53 --------------------------#
[32m[20230207 15:02:40 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:02:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:40 @agent_ppo2.py:192][0m |          -0.0016 |           7.6522 |           0.0718 |
[32m[20230207 15:02:40 @agent_ppo2.py:192][0m |          -0.0065 |           5.5679 |           0.0717 |
[32m[20230207 15:02:40 @agent_ppo2.py:192][0m |          -0.0045 |           5.3312 |           0.0716 |
[32m[20230207 15:02:40 @agent_ppo2.py:192][0m |          -0.0068 |           5.2503 |           0.0716 |
[32m[20230207 15:02:40 @agent_ppo2.py:192][0m |          -0.0064 |           5.1551 |           0.0716 |
[32m[20230207 15:02:40 @agent_ppo2.py:192][0m |          -0.0078 |           5.1260 |           0.0716 |
[32m[20230207 15:02:40 @agent_ppo2.py:192][0m |          -0.0077 |           5.1914 |           0.0716 |
[32m[20230207 15:02:40 @agent_ppo2.py:192][0m |          -0.0074 |           5.0061 |           0.0716 |
[32m[20230207 15:02:40 @agent_ppo2.py:192][0m |          -0.0087 |           5.0062 |           0.0715 |
[32m[20230207 15:02:40 @agent_ppo2.py:192][0m |          -0.0089 |           4.8967 |           0.0715 |
[32m[20230207 15:02:40 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:02:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -112.15
[32m[20230207 15:02:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -97.40
[32m[20230207 15:02:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -116.17
[32m[20230207 15:02:41 @agent_ppo2.py:150][0m Total time:       2.00 min
[32m[20230207 15:02:41 @agent_ppo2.py:152][0m 110592 total steps have happened
[32m[20230207 15:02:41 @agent_ppo2.py:128][0m #------------------------ Iteration 54 --------------------------#
[32m[20230207 15:02:42 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:02:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:42 @agent_ppo2.py:192][0m |          -0.0002 |          11.2744 |           0.0715 |
[32m[20230207 15:02:42 @agent_ppo2.py:192][0m |          -0.0016 |           9.8110 |           0.0715 |
[32m[20230207 15:02:42 @agent_ppo2.py:192][0m |          -0.0027 |           9.3293 |           0.0714 |
[32m[20230207 15:02:43 @agent_ppo2.py:192][0m |          -0.0035 |           9.2319 |           0.0714 |
[32m[20230207 15:02:43 @agent_ppo2.py:192][0m |          -0.0051 |           9.1514 |           0.0713 |
[32m[20230207 15:02:43 @agent_ppo2.py:192][0m |          -0.0035 |           9.3107 |           0.0713 |
[32m[20230207 15:02:43 @agent_ppo2.py:192][0m |          -0.0052 |           9.0504 |           0.0713 |
[32m[20230207 15:02:43 @agent_ppo2.py:192][0m |          -0.0065 |           9.3745 |           0.0713 |
[32m[20230207 15:02:43 @agent_ppo2.py:192][0m |          -0.0048 |           9.2158 |           0.0713 |
[32m[20230207 15:02:43 @agent_ppo2.py:192][0m |          -0.0056 |           9.0513 |           0.0713 |
[32m[20230207 15:02:43 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:02:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: -125.16
[32m[20230207 15:02:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -105.86
[32m[20230207 15:02:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -122.12
[32m[20230207 15:02:44 @agent_ppo2.py:150][0m Total time:       2.05 min
[32m[20230207 15:02:44 @agent_ppo2.py:152][0m 112640 total steps have happened
[32m[20230207 15:02:44 @agent_ppo2.py:128][0m #------------------------ Iteration 55 --------------------------#
[32m[20230207 15:02:45 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:02:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:45 @agent_ppo2.py:192][0m |           0.0005 |          14.5218 |           0.0714 |
[32m[20230207 15:02:45 @agent_ppo2.py:192][0m |          -0.0025 |          10.5344 |           0.0713 |
[32m[20230207 15:02:45 @agent_ppo2.py:192][0m |          -0.0047 |           7.7982 |           0.0713 |
[32m[20230207 15:02:45 @agent_ppo2.py:192][0m |          -0.0041 |           6.4501 |           0.0713 |
[32m[20230207 15:02:45 @agent_ppo2.py:192][0m |          -0.0075 |           6.0332 |           0.0713 |
[32m[20230207 15:02:45 @agent_ppo2.py:192][0m |          -0.0064 |           5.6720 |           0.0713 |
[32m[20230207 15:02:45 @agent_ppo2.py:192][0m |          -0.0073 |           5.4696 |           0.0713 |
[32m[20230207 15:02:45 @agent_ppo2.py:192][0m |          -0.0052 |           5.3866 |           0.0714 |
[32m[20230207 15:02:46 @agent_ppo2.py:192][0m |          -0.0073 |           5.2581 |           0.0714 |
[32m[20230207 15:02:46 @agent_ppo2.py:192][0m |          -0.0094 |           5.2176 |           0.0714 |
[32m[20230207 15:02:46 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:02:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -140.78
[32m[20230207 15:02:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -131.44
[32m[20230207 15:02:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -134.23
[32m[20230207 15:02:46 @agent_ppo2.py:150][0m Total time:       2.09 min
[32m[20230207 15:02:46 @agent_ppo2.py:152][0m 114688 total steps have happened
[32m[20230207 15:02:46 @agent_ppo2.py:128][0m #------------------------ Iteration 56 --------------------------#
[32m[20230207 15:02:47 @agent_ppo2.py:134][0m Sampling time: 0.74 s by 1 slaves
[32m[20230207 15:02:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:47 @agent_ppo2.py:192][0m |          -0.0001 |          10.8478 |           0.0741 |
[32m[20230207 15:02:47 @agent_ppo2.py:192][0m |          -0.0035 |           4.6995 |           0.0742 |
[32m[20230207 15:02:47 @agent_ppo2.py:192][0m |          -0.0054 |           3.9761 |           0.0742 |
[32m[20230207 15:02:48 @agent_ppo2.py:192][0m |          -0.0066 |           3.5769 |           0.0741 |
[32m[20230207 15:02:48 @agent_ppo2.py:192][0m |          -0.0079 |           3.3670 |           0.0741 |
[32m[20230207 15:02:48 @agent_ppo2.py:192][0m |          -0.0084 |           3.2449 |           0.0742 |
[32m[20230207 15:02:48 @agent_ppo2.py:192][0m |          -0.0086 |           3.1233 |           0.0742 |
[32m[20230207 15:02:48 @agent_ppo2.py:192][0m |          -0.0091 |           3.0750 |           0.0742 |
[32m[20230207 15:02:48 @agent_ppo2.py:192][0m |          -0.0096 |           2.9417 |           0.0742 |
[32m[20230207 15:02:48 @agent_ppo2.py:192][0m |          -0.0093 |           2.9674 |           0.0742 |
[32m[20230207 15:02:48 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230207 15:02:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: -143.32
[32m[20230207 15:02:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -107.95
[32m[20230207 15:02:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -133.26
[32m[20230207 15:02:49 @agent_ppo2.py:150][0m Total time:       2.13 min
[32m[20230207 15:02:49 @agent_ppo2.py:152][0m 116736 total steps have happened
[32m[20230207 15:02:49 @agent_ppo2.py:128][0m #------------------------ Iteration 57 --------------------------#
[32m[20230207 15:02:50 @agent_ppo2.py:134][0m Sampling time: 0.93 s by 1 slaves
[32m[20230207 15:02:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:50 @agent_ppo2.py:192][0m |           0.0001 |          20.1198 |           0.0727 |
[32m[20230207 15:02:50 @agent_ppo2.py:192][0m |          -0.0016 |          15.7932 |           0.0727 |
[32m[20230207 15:02:50 @agent_ppo2.py:192][0m |          -0.0034 |          14.8581 |           0.0727 |
[32m[20230207 15:02:50 @agent_ppo2.py:192][0m |          -0.0048 |          13.8642 |           0.0726 |
[32m[20230207 15:02:50 @agent_ppo2.py:192][0m |          -0.0044 |          13.1675 |           0.0726 |
[32m[20230207 15:02:50 @agent_ppo2.py:192][0m |          -0.0057 |          12.6206 |           0.0726 |
[32m[20230207 15:02:50 @agent_ppo2.py:192][0m |          -0.0063 |          12.2308 |           0.0726 |
[32m[20230207 15:02:50 @agent_ppo2.py:192][0m |          -0.0067 |          11.8101 |           0.0725 |
[32m[20230207 15:02:51 @agent_ppo2.py:192][0m |          -0.0069 |          11.4458 |           0.0726 |
[32m[20230207 15:02:51 @agent_ppo2.py:192][0m |          -0.0070 |          11.1370 |           0.0725 |
[32m[20230207 15:02:51 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:02:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: -120.66
[32m[20230207 15:02:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -93.40
[32m[20230207 15:02:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -139.52
[32m[20230207 15:02:51 @agent_ppo2.py:150][0m Total time:       2.17 min
[32m[20230207 15:02:51 @agent_ppo2.py:152][0m 118784 total steps have happened
[32m[20230207 15:02:51 @agent_ppo2.py:128][0m #------------------------ Iteration 58 --------------------------#
[32m[20230207 15:02:52 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:02:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:52 @agent_ppo2.py:192][0m |          -0.0050 |          12.4070 |           0.0729 |
[32m[20230207 15:02:52 @agent_ppo2.py:192][0m |          -0.0054 |           9.5792 |           0.0728 |
[32m[20230207 15:02:53 @agent_ppo2.py:192][0m |           0.0005 |           9.0605 |           0.0728 |
[32m[20230207 15:02:53 @agent_ppo2.py:192][0m |          -0.0107 |           8.8154 |           0.0727 |
[32m[20230207 15:02:53 @agent_ppo2.py:192][0m |          -0.0070 |           8.6878 |           0.0727 |
[32m[20230207 15:02:53 @agent_ppo2.py:192][0m |          -0.0128 |           9.0401 |           0.0727 |
[32m[20230207 15:02:53 @agent_ppo2.py:192][0m |          -0.0079 |           8.6309 |           0.0727 |
[32m[20230207 15:02:53 @agent_ppo2.py:192][0m |          -0.0156 |           9.7042 |           0.0727 |
[32m[20230207 15:02:53 @agent_ppo2.py:192][0m |          -0.0078 |           8.4217 |           0.0727 |
[32m[20230207 15:02:53 @agent_ppo2.py:192][0m |          -0.0082 |           8.3166 |           0.0727 |
[32m[20230207 15:02:53 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:02:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.78
[32m[20230207 15:02:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -96.01
[32m[20230207 15:02:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -126.31
[32m[20230207 15:02:54 @agent_ppo2.py:150][0m Total time:       2.21 min
[32m[20230207 15:02:54 @agent_ppo2.py:152][0m 120832 total steps have happened
[32m[20230207 15:02:54 @agent_ppo2.py:128][0m #------------------------ Iteration 59 --------------------------#
[32m[20230207 15:02:55 @agent_ppo2.py:134][0m Sampling time: 0.94 s by 1 slaves
[32m[20230207 15:02:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:55 @agent_ppo2.py:192][0m |          -0.0006 |          17.3851 |           0.0738 |
[32m[20230207 15:02:55 @agent_ppo2.py:192][0m |          -0.0033 |          12.2371 |           0.0738 |
[32m[20230207 15:02:55 @agent_ppo2.py:192][0m |          -0.0053 |          11.2869 |           0.0737 |
[32m[20230207 15:02:55 @agent_ppo2.py:192][0m |          -0.0060 |          10.5361 |           0.0737 |
[32m[20230207 15:02:55 @agent_ppo2.py:192][0m |          -0.0069 |           9.8879 |           0.0737 |
[32m[20230207 15:02:55 @agent_ppo2.py:192][0m |          -0.0075 |           9.4781 |           0.0737 |
[32m[20230207 15:02:55 @agent_ppo2.py:192][0m |          -0.0077 |           9.2374 |           0.0737 |
[32m[20230207 15:02:56 @agent_ppo2.py:192][0m |          -0.0080 |           8.8105 |           0.0737 |
[32m[20230207 15:02:56 @agent_ppo2.py:192][0m |          -0.0088 |           8.5732 |           0.0737 |
[32m[20230207 15:02:56 @agent_ppo2.py:192][0m |          -0.0085 |           8.3236 |           0.0737 |
[32m[20230207 15:02:56 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:02:57 @agent_ppo2.py:145][0m Average TRAINING episode reward: -140.97
[32m[20230207 15:02:57 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -109.65
[32m[20230207 15:02:57 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -131.88
[32m[20230207 15:02:57 @agent_ppo2.py:150][0m Total time:       2.26 min
[32m[20230207 15:02:57 @agent_ppo2.py:152][0m 122880 total steps have happened
[32m[20230207 15:02:57 @agent_ppo2.py:128][0m #------------------------ Iteration 60 --------------------------#
[32m[20230207 15:02:57 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:02:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:57 @agent_ppo2.py:192][0m |          -0.0017 |          12.3945 |           0.0745 |
[32m[20230207 15:02:58 @agent_ppo2.py:192][0m |          -0.0023 |           6.0320 |           0.0745 |
[32m[20230207 15:02:58 @agent_ppo2.py:192][0m |          -0.0037 |           4.0337 |           0.0745 |
[32m[20230207 15:02:58 @agent_ppo2.py:192][0m |          -0.0080 |           3.5439 |           0.0745 |
[32m[20230207 15:02:58 @agent_ppo2.py:192][0m |          -0.0103 |           3.3500 |           0.0745 |
[32m[20230207 15:02:58 @agent_ppo2.py:192][0m |          -0.0101 |           3.1243 |           0.0745 |
[32m[20230207 15:02:58 @agent_ppo2.py:192][0m |          -0.0125 |           3.0052 |           0.0745 |
[32m[20230207 15:02:58 @agent_ppo2.py:192][0m |          -0.0106 |           2.9137 |           0.0745 |
[32m[20230207 15:02:58 @agent_ppo2.py:192][0m |          -0.0112 |           2.8055 |           0.0745 |
[32m[20230207 15:02:58 @agent_ppo2.py:192][0m |          -0.0128 |           2.6306 |           0.0746 |
[32m[20230207 15:02:58 @agent_ppo2.py:137][0m Policy update time: 0.76 s
[32m[20230207 15:02:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: -126.85
[32m[20230207 15:02:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -107.21
[32m[20230207 15:02:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -113.30
[32m[20230207 15:02:59 @agent_ppo2.py:150][0m Total time:       2.29 min
[32m[20230207 15:02:59 @agent_ppo2.py:152][0m 124928 total steps have happened
[32m[20230207 15:02:59 @agent_ppo2.py:128][0m #------------------------ Iteration 61 --------------------------#
[32m[20230207 15:02:59 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:02:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:02:59 @agent_ppo2.py:192][0m |           0.0028 |           9.8006 |           0.0732 |
[32m[20230207 15:02:59 @agent_ppo2.py:192][0m |          -0.0024 |           7.1006 |           0.0732 |
[32m[20230207 15:02:59 @agent_ppo2.py:192][0m |          -0.0025 |           6.5770 |           0.0733 |
[32m[20230207 15:02:59 @agent_ppo2.py:192][0m |          -0.0030 |           6.1424 |           0.0733 |
[32m[20230207 15:03:00 @agent_ppo2.py:192][0m |          -0.0098 |           5.7763 |           0.0733 |
[32m[20230207 15:03:00 @agent_ppo2.py:192][0m |          -0.0063 |           5.5010 |           0.0733 |
[32m[20230207 15:03:00 @agent_ppo2.py:192][0m |          -0.0062 |           5.4063 |           0.0734 |
[32m[20230207 15:03:00 @agent_ppo2.py:192][0m |          -0.0097 |           5.1360 |           0.0734 |
[32m[20230207 15:03:00 @agent_ppo2.py:192][0m |          -0.0111 |           4.9313 |           0.0734 |
[32m[20230207 15:03:00 @agent_ppo2.py:192][0m |          -0.0107 |           4.9809 |           0.0735 |
[32m[20230207 15:03:00 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:03:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -147.41
[32m[20230207 15:03:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -114.57
[32m[20230207 15:03:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -116.72
[32m[20230207 15:03:01 @agent_ppo2.py:150][0m Total time:       2.32 min
[32m[20230207 15:03:01 @agent_ppo2.py:152][0m 126976 total steps have happened
[32m[20230207 15:03:01 @agent_ppo2.py:128][0m #------------------------ Iteration 62 --------------------------#
[32m[20230207 15:03:02 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:03:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:02 @agent_ppo2.py:192][0m |           0.0026 |           4.5864 |           0.0739 |
[32m[20230207 15:03:02 @agent_ppo2.py:192][0m |          -0.0038 |           3.8211 |           0.0739 |
[32m[20230207 15:03:02 @agent_ppo2.py:192][0m |           0.0039 |           3.7841 |           0.0738 |
[32m[20230207 15:03:02 @agent_ppo2.py:192][0m |          -0.0034 |           3.7041 |           0.0737 |
[32m[20230207 15:03:02 @agent_ppo2.py:192][0m |          -0.0028 |           3.6888 |           0.0737 |
[32m[20230207 15:03:02 @agent_ppo2.py:192][0m |          -0.0082 |           3.6596 |           0.0737 |
[32m[20230207 15:03:02 @agent_ppo2.py:192][0m |          -0.0059 |           3.6379 |           0.0736 |
[32m[20230207 15:03:02 @agent_ppo2.py:192][0m |          -0.0119 |           3.5929 |           0.0736 |
[32m[20230207 15:03:02 @agent_ppo2.py:192][0m |          -0.0049 |           3.6097 |           0.0736 |
[32m[20230207 15:03:02 @agent_ppo2.py:192][0m |          -0.0052 |           3.5978 |           0.0736 |
[32m[20230207 15:03:02 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -122.43
[32m[20230207 15:03:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -115.56
[32m[20230207 15:03:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -106.65
[32m[20230207 15:03:03 @agent_ppo2.py:150][0m Total time:       2.37 min
[32m[20230207 15:03:03 @agent_ppo2.py:152][0m 129024 total steps have happened
[32m[20230207 15:03:03 @agent_ppo2.py:128][0m #------------------------ Iteration 63 --------------------------#
[32m[20230207 15:03:04 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:03:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:04 @agent_ppo2.py:192][0m |           0.0083 |           4.6151 |           0.0761 |
[32m[20230207 15:03:04 @agent_ppo2.py:192][0m |           0.0061 |           3.8038 |           0.0760 |
[32m[20230207 15:03:04 @agent_ppo2.py:192][0m |          -0.0040 |           3.7736 |           0.0760 |
[32m[20230207 15:03:04 @agent_ppo2.py:192][0m |           0.0018 |           3.7129 |           0.0759 |
[32m[20230207 15:03:04 @agent_ppo2.py:192][0m |          -0.0066 |           3.6839 |           0.0759 |
[32m[20230207 15:03:05 @agent_ppo2.py:192][0m |           0.0005 |           3.6619 |           0.0759 |
[32m[20230207 15:03:05 @agent_ppo2.py:192][0m |           0.0002 |           3.6600 |           0.0759 |
[32m[20230207 15:03:05 @agent_ppo2.py:192][0m |          -0.0045 |           3.6239 |           0.0759 |
[32m[20230207 15:03:05 @agent_ppo2.py:192][0m |          -0.0019 |           3.6223 |           0.0759 |
[32m[20230207 15:03:05 @agent_ppo2.py:192][0m |          -0.0122 |           3.6035 |           0.0759 |
[32m[20230207 15:03:05 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: -112.10
[32m[20230207 15:03:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -88.14
[32m[20230207 15:03:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -144.72
[32m[20230207 15:03:06 @agent_ppo2.py:150][0m Total time:       2.41 min
[32m[20230207 15:03:06 @agent_ppo2.py:152][0m 131072 total steps have happened
[32m[20230207 15:03:06 @agent_ppo2.py:128][0m #------------------------ Iteration 64 --------------------------#
[32m[20230207 15:03:07 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:03:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:07 @agent_ppo2.py:192][0m |           0.0001 |           6.0894 |           0.0736 |
[32m[20230207 15:03:07 @agent_ppo2.py:192][0m |          -0.0035 |           4.2955 |           0.0736 |
[32m[20230207 15:03:07 @agent_ppo2.py:192][0m |          -0.0057 |           4.1349 |           0.0736 |
[32m[20230207 15:03:07 @agent_ppo2.py:192][0m |          -0.0068 |           3.9897 |           0.0736 |
[32m[20230207 15:03:07 @agent_ppo2.py:192][0m |          -0.0072 |           3.9188 |           0.0736 |
[32m[20230207 15:03:07 @agent_ppo2.py:192][0m |          -0.0095 |           3.8447 |           0.0736 |
[32m[20230207 15:03:07 @agent_ppo2.py:192][0m |          -0.0109 |           3.8571 |           0.0736 |
[32m[20230207 15:03:07 @agent_ppo2.py:192][0m |          -0.0100 |           3.7764 |           0.0736 |
[32m[20230207 15:03:07 @agent_ppo2.py:192][0m |          -0.0124 |           3.7951 |           0.0737 |
[32m[20230207 15:03:07 @agent_ppo2.py:192][0m |          -0.0123 |           3.7238 |           0.0736 |
[32m[20230207 15:03:07 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:03:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -98.66
[32m[20230207 15:03:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -94.30
[32m[20230207 15:03:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -119.13
[32m[20230207 15:03:08 @agent_ppo2.py:150][0m Total time:       2.45 min
[32m[20230207 15:03:08 @agent_ppo2.py:152][0m 133120 total steps have happened
[32m[20230207 15:03:08 @agent_ppo2.py:128][0m #------------------------ Iteration 65 --------------------------#
[32m[20230207 15:03:09 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:03:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:09 @agent_ppo2.py:192][0m |           0.0165 |           3.7628 |           0.0736 |
[32m[20230207 15:03:09 @agent_ppo2.py:192][0m |          -0.0070 |           3.0669 |           0.0737 |
[32m[20230207 15:03:09 @agent_ppo2.py:192][0m |           0.0069 |           2.9400 |           0.0737 |
[32m[20230207 15:03:09 @agent_ppo2.py:192][0m |          -0.0212 |           2.8762 |           0.0737 |
[32m[20230207 15:03:10 @agent_ppo2.py:192][0m |           0.0078 |           2.8158 |           0.0737 |
[32m[20230207 15:03:10 @agent_ppo2.py:192][0m |          -0.0090 |           2.8397 |           0.0737 |
[32m[20230207 15:03:10 @agent_ppo2.py:192][0m |          -0.0012 |           2.8080 |           0.0737 |
[32m[20230207 15:03:10 @agent_ppo2.py:192][0m |          -0.0096 |           2.7842 |           0.0738 |
[32m[20230207 15:03:10 @agent_ppo2.py:192][0m |          -0.0082 |           2.7591 |           0.0738 |
[32m[20230207 15:03:10 @agent_ppo2.py:192][0m |          -0.0100 |           2.7311 |           0.0738 |
[32m[20230207 15:03:10 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:11 @agent_ppo2.py:145][0m Average TRAINING episode reward: -120.53
[32m[20230207 15:03:11 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -108.76
[32m[20230207 15:03:11 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -122.32
[32m[20230207 15:03:11 @agent_ppo2.py:150][0m Total time:       2.49 min
[32m[20230207 15:03:11 @agent_ppo2.py:152][0m 135168 total steps have happened
[32m[20230207 15:03:11 @agent_ppo2.py:128][0m #------------------------ Iteration 66 --------------------------#
[32m[20230207 15:03:12 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:03:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:12 @agent_ppo2.py:192][0m |           0.0006 |           4.6569 |           0.0758 |
[32m[20230207 15:03:12 @agent_ppo2.py:192][0m |          -0.0090 |           3.5768 |           0.0757 |
[32m[20230207 15:03:12 @agent_ppo2.py:192][0m |          -0.0026 |           3.3369 |           0.0757 |
[32m[20230207 15:03:12 @agent_ppo2.py:192][0m |           0.0045 |           3.0539 |           0.0757 |
[32m[20230207 15:03:12 @agent_ppo2.py:192][0m |           0.0020 |           2.8816 |           0.0757 |
[32m[20230207 15:03:12 @agent_ppo2.py:192][0m |           0.0066 |           2.8292 |           0.0757 |
[32m[20230207 15:03:12 @agent_ppo2.py:192][0m |           0.0079 |           2.6494 |           0.0757 |
[32m[20230207 15:03:12 @agent_ppo2.py:192][0m |          -0.0053 |           2.5254 |           0.0757 |
[32m[20230207 15:03:12 @agent_ppo2.py:192][0m |          -0.0064 |           2.3999 |           0.0757 |
[32m[20230207 15:03:12 @agent_ppo2.py:192][0m |          -0.0102 |           2.3349 |           0.0757 |
[32m[20230207 15:03:12 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -101.79
[32m[20230207 15:03:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -91.69
[32m[20230207 15:03:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -88.88
[32m[20230207 15:03:13 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -88.88
[32m[20230207 15:03:13 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -88.88
[32m[20230207 15:03:13 @agent_ppo2.py:150][0m Total time:       2.53 min
[32m[20230207 15:03:13 @agent_ppo2.py:152][0m 137216 total steps have happened
[32m[20230207 15:03:13 @agent_ppo2.py:128][0m #------------------------ Iteration 67 --------------------------#
[32m[20230207 15:03:14 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:03:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:14 @agent_ppo2.py:192][0m |          -0.0008 |           6.0873 |           0.0759 |
[32m[20230207 15:03:14 @agent_ppo2.py:192][0m |          -0.0033 |           4.5517 |           0.0759 |
[32m[20230207 15:03:14 @agent_ppo2.py:192][0m |          -0.0048 |           4.4040 |           0.0758 |
[32m[20230207 15:03:14 @agent_ppo2.py:192][0m |          -0.0066 |           4.3319 |           0.0758 |
[32m[20230207 15:03:14 @agent_ppo2.py:192][0m |          -0.0077 |           4.2248 |           0.0757 |
[32m[20230207 15:03:15 @agent_ppo2.py:192][0m |          -0.0078 |           4.2097 |           0.0757 |
[32m[20230207 15:03:15 @agent_ppo2.py:192][0m |          -0.0095 |           4.1529 |           0.0757 |
[32m[20230207 15:03:15 @agent_ppo2.py:192][0m |          -0.0097 |           4.0820 |           0.0757 |
[32m[20230207 15:03:15 @agent_ppo2.py:192][0m |          -0.0095 |           4.0786 |           0.0757 |
[32m[20230207 15:03:15 @agent_ppo2.py:192][0m |          -0.0107 |           4.0534 |           0.0756 |
[32m[20230207 15:03:15 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -110.52
[32m[20230207 15:03:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.84
[32m[20230207 15:03:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -139.87
[32m[20230207 15:03:16 @agent_ppo2.py:150][0m Total time:       2.57 min
[32m[20230207 15:03:16 @agent_ppo2.py:152][0m 139264 total steps have happened
[32m[20230207 15:03:16 @agent_ppo2.py:128][0m #------------------------ Iteration 68 --------------------------#
[32m[20230207 15:03:16 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:03:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:17 @agent_ppo2.py:192][0m |           0.0025 |           3.2516 |           0.0763 |
[32m[20230207 15:03:17 @agent_ppo2.py:192][0m |          -0.0329 |           2.7873 |           0.0762 |
[32m[20230207 15:03:17 @agent_ppo2.py:192][0m |           0.0050 |           2.7033 |           0.0762 |
[32m[20230207 15:03:17 @agent_ppo2.py:192][0m |           0.0013 |           2.7371 |           0.0761 |
[32m[20230207 15:03:17 @agent_ppo2.py:192][0m |          -0.0262 |           2.6618 |           0.0761 |
[32m[20230207 15:03:17 @agent_ppo2.py:192][0m |           0.0019 |           2.6058 |           0.0761 |
[32m[20230207 15:03:17 @agent_ppo2.py:192][0m |           0.0019 |           2.5772 |           0.0763 |
[32m[20230207 15:03:17 @agent_ppo2.py:192][0m |          -0.0053 |           2.5492 |           0.0762 |
[32m[20230207 15:03:17 @agent_ppo2.py:192][0m |          -0.0041 |           2.5486 |           0.0762 |
[32m[20230207 15:03:17 @agent_ppo2.py:192][0m |           0.0021 |           2.5353 |           0.0762 |
[32m[20230207 15:03:17 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.77
[32m[20230207 15:03:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -66.74
[32m[20230207 15:03:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -155.21
[32m[20230207 15:03:18 @agent_ppo2.py:150][0m Total time:       2.61 min
[32m[20230207 15:03:18 @agent_ppo2.py:152][0m 141312 total steps have happened
[32m[20230207 15:03:18 @agent_ppo2.py:128][0m #------------------------ Iteration 69 --------------------------#
[32m[20230207 15:03:19 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:03:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:19 @agent_ppo2.py:192][0m |          -0.0000 |          13.8845 |           0.0786 |
[32m[20230207 15:03:19 @agent_ppo2.py:192][0m |          -0.0012 |           4.5714 |           0.0787 |
[32m[20230207 15:03:19 @agent_ppo2.py:192][0m |          -0.0036 |           3.7622 |           0.0787 |
[32m[20230207 15:03:19 @agent_ppo2.py:192][0m |          -0.0026 |           3.4776 |           0.0788 |
[32m[20230207 15:03:19 @agent_ppo2.py:192][0m |          -0.0044 |           3.3594 |           0.0788 |
[32m[20230207 15:03:19 @agent_ppo2.py:192][0m |          -0.0044 |           3.2190 |           0.0788 |
[32m[20230207 15:03:20 @agent_ppo2.py:192][0m |          -0.0055 |           3.1662 |           0.0788 |
[32m[20230207 15:03:20 @agent_ppo2.py:192][0m |          -0.0063 |           3.1365 |           0.0789 |
[32m[20230207 15:03:20 @agent_ppo2.py:192][0m |          -0.0068 |           3.0666 |           0.0789 |
[32m[20230207 15:03:20 @agent_ppo2.py:192][0m |          -0.0067 |           3.0144 |           0.0789 |
[32m[20230207 15:03:20 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:03:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -101.98
[32m[20230207 15:03:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -84.88
[32m[20230207 15:03:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -115.67
[32m[20230207 15:03:21 @agent_ppo2.py:150][0m Total time:       2.66 min
[32m[20230207 15:03:21 @agent_ppo2.py:152][0m 143360 total steps have happened
[32m[20230207 15:03:21 @agent_ppo2.py:128][0m #------------------------ Iteration 70 --------------------------#
[32m[20230207 15:03:21 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:03:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:22 @agent_ppo2.py:192][0m |           0.0010 |           7.6162 |           0.0764 |
[32m[20230207 15:03:22 @agent_ppo2.py:192][0m |          -0.0018 |           3.8799 |           0.0764 |
[32m[20230207 15:03:22 @agent_ppo2.py:192][0m |          -0.0023 |           3.6342 |           0.0763 |
[32m[20230207 15:03:22 @agent_ppo2.py:192][0m |          -0.0039 |           3.4621 |           0.0763 |
[32m[20230207 15:03:22 @agent_ppo2.py:192][0m |          -0.0050 |           3.3519 |           0.0763 |
[32m[20230207 15:03:22 @agent_ppo2.py:192][0m |          -0.0060 |           3.2829 |           0.0762 |
[32m[20230207 15:03:22 @agent_ppo2.py:192][0m |          -0.0074 |           3.2021 |           0.0762 |
[32m[20230207 15:03:22 @agent_ppo2.py:192][0m |          -0.0076 |           3.1215 |           0.0762 |
[32m[20230207 15:03:22 @agent_ppo2.py:192][0m |          -0.0082 |           3.0729 |           0.0761 |
[32m[20230207 15:03:22 @agent_ppo2.py:192][0m |          -0.0087 |           3.0678 |           0.0761 |
[32m[20230207 15:03:22 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:03:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -103.96
[32m[20230207 15:03:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.09
[32m[20230207 15:03:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -106.56
[32m[20230207 15:03:23 @agent_ppo2.py:150][0m Total time:       2.70 min
[32m[20230207 15:03:23 @agent_ppo2.py:152][0m 145408 total steps have happened
[32m[20230207 15:03:23 @agent_ppo2.py:128][0m #------------------------ Iteration 71 --------------------------#
[32m[20230207 15:03:24 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:03:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:24 @agent_ppo2.py:192][0m |           0.0028 |           2.6006 |           0.0765 |
[32m[20230207 15:03:24 @agent_ppo2.py:192][0m |           0.0060 |           2.2621 |           0.0765 |
[32m[20230207 15:03:24 @agent_ppo2.py:192][0m |          -0.0216 |           2.2546 |           0.0765 |
[32m[20230207 15:03:24 @agent_ppo2.py:192][0m |          -0.0125 |           2.2036 |           0.0765 |
[32m[20230207 15:03:24 @agent_ppo2.py:192][0m |           0.0023 |           2.1926 |           0.0765 |
[32m[20230207 15:03:24 @agent_ppo2.py:192][0m |          -0.0002 |           2.1949 |           0.0765 |
[32m[20230207 15:03:25 @agent_ppo2.py:192][0m |          -0.0092 |           2.1812 |           0.0765 |
[32m[20230207 15:03:25 @agent_ppo2.py:192][0m |          -0.0031 |           2.1844 |           0.0766 |
[32m[20230207 15:03:25 @agent_ppo2.py:192][0m |           0.0051 |           2.2061 |           0.0766 |
[32m[20230207 15:03:25 @agent_ppo2.py:192][0m |          -0.0087 |           2.1862 |           0.0766 |
[32m[20230207 15:03:25 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:26 @agent_ppo2.py:145][0m Average TRAINING episode reward: -117.21
[32m[20230207 15:03:26 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -81.59
[32m[20230207 15:03:26 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -110.43
[32m[20230207 15:03:26 @agent_ppo2.py:150][0m Total time:       2.74 min
[32m[20230207 15:03:26 @agent_ppo2.py:152][0m 147456 total steps have happened
[32m[20230207 15:03:26 @agent_ppo2.py:128][0m #------------------------ Iteration 72 --------------------------#
[32m[20230207 15:03:26 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:03:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:27 @agent_ppo2.py:192][0m |           0.0023 |           4.0930 |           0.0764 |
[32m[20230207 15:03:27 @agent_ppo2.py:192][0m |          -0.0119 |           3.3878 |           0.0763 |
[32m[20230207 15:03:27 @agent_ppo2.py:192][0m |           0.0003 |           3.3013 |           0.0762 |
[32m[20230207 15:03:27 @agent_ppo2.py:192][0m |          -0.0110 |           3.2269 |           0.0761 |
[32m[20230207 15:03:27 @agent_ppo2.py:192][0m |          -0.0065 |           3.1626 |           0.0761 |
[32m[20230207 15:03:27 @agent_ppo2.py:192][0m |          -0.0074 |           3.1322 |           0.0760 |
[32m[20230207 15:03:27 @agent_ppo2.py:192][0m |          -0.0014 |           3.0410 |           0.0760 |
[32m[20230207 15:03:27 @agent_ppo2.py:192][0m |          -0.0056 |           3.0066 |           0.0760 |
[32m[20230207 15:03:27 @agent_ppo2.py:192][0m |          -0.0062 |           2.9210 |           0.0760 |
[32m[20230207 15:03:27 @agent_ppo2.py:192][0m |          -0.0082 |           2.8540 |           0.0760 |
[32m[20230207 15:03:27 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: -91.45
[32m[20230207 15:03:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -89.24
[32m[20230207 15:03:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -91.27
[32m[20230207 15:03:28 @agent_ppo2.py:150][0m Total time:       2.78 min
[32m[20230207 15:03:28 @agent_ppo2.py:152][0m 149504 total steps have happened
[32m[20230207 15:03:28 @agent_ppo2.py:128][0m #------------------------ Iteration 73 --------------------------#
[32m[20230207 15:03:29 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:03:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:29 @agent_ppo2.py:192][0m |           0.0014 |           6.3702 |           0.0765 |
[32m[20230207 15:03:29 @agent_ppo2.py:192][0m |          -0.0038 |           2.7494 |           0.0764 |
[32m[20230207 15:03:29 @agent_ppo2.py:192][0m |          -0.0060 |           2.4904 |           0.0763 |
[32m[20230207 15:03:29 @agent_ppo2.py:192][0m |          -0.0073 |           2.1766 |           0.0763 |
[32m[20230207 15:03:29 @agent_ppo2.py:192][0m |          -0.0059 |           2.0801 |           0.0762 |
[32m[20230207 15:03:29 @agent_ppo2.py:192][0m |          -0.0091 |           1.9214 |           0.0762 |
[32m[20230207 15:03:29 @agent_ppo2.py:192][0m |          -0.0095 |           1.8735 |           0.0761 |
[32m[20230207 15:03:30 @agent_ppo2.py:192][0m |          -0.0115 |           1.7471 |           0.0761 |
[32m[20230207 15:03:30 @agent_ppo2.py:192][0m |          -0.0112 |           1.5969 |           0.0760 |
[32m[20230207 15:03:30 @agent_ppo2.py:192][0m |          -0.0117 |           1.5180 |           0.0760 |
[32m[20230207 15:03:30 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:03:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -107.23
[32m[20230207 15:03:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -65.82
[32m[20230207 15:03:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -102.85
[32m[20230207 15:03:31 @agent_ppo2.py:150][0m Total time:       2.82 min
[32m[20230207 15:03:31 @agent_ppo2.py:152][0m 151552 total steps have happened
[32m[20230207 15:03:31 @agent_ppo2.py:128][0m #------------------------ Iteration 74 --------------------------#
[32m[20230207 15:03:31 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:03:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:32 @agent_ppo2.py:192][0m |           0.0026 |           3.0777 |           0.0750 |
[32m[20230207 15:03:32 @agent_ppo2.py:192][0m |          -0.0047 |           2.3259 |           0.0747 |
[32m[20230207 15:03:32 @agent_ppo2.py:192][0m |          -0.0100 |           2.2527 |           0.0747 |
[32m[20230207 15:03:32 @agent_ppo2.py:192][0m |          -0.0029 |           2.2181 |           0.0747 |
[32m[20230207 15:03:32 @agent_ppo2.py:192][0m |           0.0029 |           2.1761 |           0.0746 |
[32m[20230207 15:03:32 @agent_ppo2.py:192][0m |          -0.0125 |           2.1614 |           0.0747 |
[32m[20230207 15:03:32 @agent_ppo2.py:192][0m |          -0.0127 |           2.1169 |           0.0747 |
[32m[20230207 15:03:32 @agent_ppo2.py:192][0m |          -0.0108 |           2.1388 |           0.0747 |
[32m[20230207 15:03:32 @agent_ppo2.py:192][0m |          -0.0139 |           2.0336 |           0.0748 |
[32m[20230207 15:03:32 @agent_ppo2.py:192][0m |          -0.0129 |           2.0158 |           0.0747 |
[32m[20230207 15:03:32 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: -131.97
[32m[20230207 15:03:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -112.96
[32m[20230207 15:03:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -83.07
[32m[20230207 15:03:33 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -83.07
[32m[20230207 15:03:33 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -83.07
[32m[20230207 15:03:33 @agent_ppo2.py:150][0m Total time:       2.86 min
[32m[20230207 15:03:33 @agent_ppo2.py:152][0m 153600 total steps have happened
[32m[20230207 15:03:33 @agent_ppo2.py:128][0m #------------------------ Iteration 75 --------------------------#
[32m[20230207 15:03:34 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:03:34 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:34 @agent_ppo2.py:192][0m |          -0.0067 |           2.4213 |           0.0792 |
[32m[20230207 15:03:34 @agent_ppo2.py:192][0m |          -0.0093 |           2.1153 |           0.0792 |
[32m[20230207 15:03:34 @agent_ppo2.py:192][0m |          -0.0016 |           2.0534 |           0.0792 |
[32m[20230207 15:03:34 @agent_ppo2.py:192][0m |          -0.0057 |           2.0101 |           0.0792 |
[32m[20230207 15:03:34 @agent_ppo2.py:192][0m |          -0.0125 |           1.9802 |           0.0792 |
[32m[20230207 15:03:34 @agent_ppo2.py:192][0m |          -0.0036 |           1.9698 |           0.0792 |
[32m[20230207 15:03:34 @agent_ppo2.py:192][0m |          -0.0097 |           1.9569 |           0.0792 |
[32m[20230207 15:03:34 @agent_ppo2.py:192][0m |          -0.0044 |           1.9435 |           0.0792 |
[32m[20230207 15:03:35 @agent_ppo2.py:192][0m |           0.0026 |           1.9306 |           0.0792 |
[32m[20230207 15:03:35 @agent_ppo2.py:192][0m |          -0.0021 |           1.9259 |           0.0792 |
[32m[20230207 15:03:35 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: -120.31
[32m[20230207 15:03:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -109.90
[32m[20230207 15:03:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -134.83
[32m[20230207 15:03:35 @agent_ppo2.py:150][0m Total time:       2.90 min
[32m[20230207 15:03:35 @agent_ppo2.py:152][0m 155648 total steps have happened
[32m[20230207 15:03:35 @agent_ppo2.py:128][0m #------------------------ Iteration 76 --------------------------#
[32m[20230207 15:03:36 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:03:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:36 @agent_ppo2.py:192][0m |           0.0036 |           1.7434 |           0.0759 |
[32m[20230207 15:03:37 @agent_ppo2.py:192][0m |          -0.0010 |           1.6292 |           0.0758 |
[32m[20230207 15:03:37 @agent_ppo2.py:192][0m |          -0.0235 |           1.6624 |           0.0757 |
[32m[20230207 15:03:37 @agent_ppo2.py:192][0m |          -0.0025 |           1.6038 |           0.0757 |
[32m[20230207 15:03:37 @agent_ppo2.py:192][0m |           0.0030 |           1.5989 |           0.0757 |
[32m[20230207 15:03:37 @agent_ppo2.py:192][0m |          -0.0072 |           1.5975 |           0.0757 |
[32m[20230207 15:03:37 @agent_ppo2.py:192][0m |           0.0001 |           1.5884 |           0.0757 |
[32m[20230207 15:03:37 @agent_ppo2.py:192][0m |          -0.0022 |           1.5781 |           0.0756 |
[32m[20230207 15:03:37 @agent_ppo2.py:192][0m |          -0.0040 |           1.5749 |           0.0756 |
[32m[20230207 15:03:37 @agent_ppo2.py:192][0m |          -0.0104 |           1.5629 |           0.0757 |
[32m[20230207 15:03:37 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: -136.92
[32m[20230207 15:03:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -126.04
[32m[20230207 15:03:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -128.88
[32m[20230207 15:03:38 @agent_ppo2.py:150][0m Total time:       2.95 min
[32m[20230207 15:03:38 @agent_ppo2.py:152][0m 157696 total steps have happened
[32m[20230207 15:03:38 @agent_ppo2.py:128][0m #------------------------ Iteration 77 --------------------------#
[32m[20230207 15:03:39 @agent_ppo2.py:134][0m Sampling time: 0.91 s by 1 slaves
[32m[20230207 15:03:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:39 @agent_ppo2.py:192][0m |          -0.0007 |           5.9902 |           0.0772 |
[32m[20230207 15:03:39 @agent_ppo2.py:192][0m |          -0.0035 |           3.0645 |           0.0771 |
[32m[20230207 15:03:39 @agent_ppo2.py:192][0m |          -0.0034 |           2.5184 |           0.0770 |
[32m[20230207 15:03:39 @agent_ppo2.py:192][0m |          -0.0047 |           2.3154 |           0.0770 |
[32m[20230207 15:03:39 @agent_ppo2.py:192][0m |          -0.0047 |           2.2475 |           0.0769 |
[32m[20230207 15:03:39 @agent_ppo2.py:192][0m |          -0.0057 |           2.1457 |           0.0769 |
[32m[20230207 15:03:40 @agent_ppo2.py:192][0m |          -0.0061 |           2.1405 |           0.0769 |
[32m[20230207 15:03:40 @agent_ppo2.py:192][0m |          -0.0061 |           2.0841 |           0.0768 |
[32m[20230207 15:03:40 @agent_ppo2.py:192][0m |          -0.0065 |           2.0620 |           0.0768 |
[32m[20230207 15:03:40 @agent_ppo2.py:192][0m |          -0.0066 |           1.9941 |           0.0767 |
[32m[20230207 15:03:40 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:03:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -124.51
[32m[20230207 15:03:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -110.60
[32m[20230207 15:03:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -125.29
[32m[20230207 15:03:41 @agent_ppo2.py:150][0m Total time:       2.99 min
[32m[20230207 15:03:41 @agent_ppo2.py:152][0m 159744 total steps have happened
[32m[20230207 15:03:41 @agent_ppo2.py:128][0m #------------------------ Iteration 78 --------------------------#
[32m[20230207 15:03:41 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:03:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:42 @agent_ppo2.py:192][0m |          -0.0072 |           2.3954 |           0.0766 |
[32m[20230207 15:03:42 @agent_ppo2.py:192][0m |          -0.0026 |           1.8176 |           0.0766 |
[32m[20230207 15:03:42 @agent_ppo2.py:192][0m |          -0.0113 |           1.7392 |           0.0766 |
[32m[20230207 15:03:42 @agent_ppo2.py:192][0m |          -0.0089 |           1.7415 |           0.0765 |
[32m[20230207 15:03:42 @agent_ppo2.py:192][0m |          -0.0014 |           1.7071 |           0.0766 |
[32m[20230207 15:03:42 @agent_ppo2.py:192][0m |           0.0004 |           1.6243 |           0.0766 |
[32m[20230207 15:03:42 @agent_ppo2.py:192][0m |          -0.0070 |           1.6177 |           0.0766 |
[32m[20230207 15:03:42 @agent_ppo2.py:192][0m |          -0.0156 |           1.6145 |           0.0766 |
[32m[20230207 15:03:42 @agent_ppo2.py:192][0m |           0.0064 |           1.5789 |           0.0766 |
[32m[20230207 15:03:42 @agent_ppo2.py:192][0m |          -0.0037 |           1.5581 |           0.0766 |
[32m[20230207 15:03:42 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -96.99
[32m[20230207 15:03:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -55.10
[32m[20230207 15:03:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -134.31
[32m[20230207 15:03:43 @agent_ppo2.py:150][0m Total time:       3.03 min
[32m[20230207 15:03:43 @agent_ppo2.py:152][0m 161792 total steps have happened
[32m[20230207 15:03:43 @agent_ppo2.py:128][0m #------------------------ Iteration 79 --------------------------#
[32m[20230207 15:03:44 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:03:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:44 @agent_ppo2.py:192][0m |          -0.0008 |           6.8114 |           0.0765 |
[32m[20230207 15:03:44 @agent_ppo2.py:192][0m |          -0.0035 |           5.5533 |           0.0764 |
[32m[20230207 15:03:44 @agent_ppo2.py:192][0m |          -0.0044 |           5.3646 |           0.0764 |
[32m[20230207 15:03:44 @agent_ppo2.py:192][0m |          -0.0056 |           5.2152 |           0.0764 |
[32m[20230207 15:03:44 @agent_ppo2.py:192][0m |          -0.0062 |           5.1773 |           0.0763 |
[32m[20230207 15:03:44 @agent_ppo2.py:192][0m |          -0.0073 |           5.1066 |           0.0763 |
[32m[20230207 15:03:44 @agent_ppo2.py:192][0m |          -0.0072 |           5.0450 |           0.0763 |
[32m[20230207 15:03:44 @agent_ppo2.py:192][0m |          -0.0081 |           5.0595 |           0.0762 |
[32m[20230207 15:03:44 @agent_ppo2.py:192][0m |          -0.0084 |           4.9772 |           0.0763 |
[32m[20230207 15:03:44 @agent_ppo2.py:192][0m |          -0.0090 |           4.9621 |           0.0762 |
[32m[20230207 15:03:44 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:03:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: -131.35
[32m[20230207 15:03:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -113.06
[32m[20230207 15:03:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -111.41
[32m[20230207 15:03:45 @agent_ppo2.py:150][0m Total time:       3.07 min
[32m[20230207 15:03:45 @agent_ppo2.py:152][0m 163840 total steps have happened
[32m[20230207 15:03:45 @agent_ppo2.py:128][0m #------------------------ Iteration 80 --------------------------#
[32m[20230207 15:03:46 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:03:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:46 @agent_ppo2.py:192][0m |           0.0022 |           1.9759 |           0.0758 |
[32m[20230207 15:03:46 @agent_ppo2.py:192][0m |           0.0009 |           1.8883 |           0.0758 |
[32m[20230207 15:03:46 @agent_ppo2.py:192][0m |           0.0014 |           1.6782 |           0.0758 |
[32m[20230207 15:03:46 @agent_ppo2.py:192][0m |          -0.0122 |           1.6671 |           0.0758 |
[32m[20230207 15:03:47 @agent_ppo2.py:192][0m |          -0.0244 |           1.6739 |           0.0757 |
[32m[20230207 15:03:47 @agent_ppo2.py:192][0m |          -0.0015 |           1.6445 |           0.0757 |
[32m[20230207 15:03:47 @agent_ppo2.py:192][0m |           0.0012 |           1.6345 |           0.0758 |
[32m[20230207 15:03:47 @agent_ppo2.py:192][0m |           0.0032 |           1.6361 |           0.0757 |
[32m[20230207 15:03:47 @agent_ppo2.py:192][0m |           0.0017 |           1.6338 |           0.0757 |
[32m[20230207 15:03:47 @agent_ppo2.py:192][0m |          -0.0302 |           1.6379 |           0.0758 |
[32m[20230207 15:03:47 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -150.82
[32m[20230207 15:03:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -142.93
[32m[20230207 15:03:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -109.53
[32m[20230207 15:03:48 @agent_ppo2.py:150][0m Total time:       3.11 min
[32m[20230207 15:03:48 @agent_ppo2.py:152][0m 165888 total steps have happened
[32m[20230207 15:03:48 @agent_ppo2.py:128][0m #------------------------ Iteration 81 --------------------------#
[32m[20230207 15:03:49 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:03:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:49 @agent_ppo2.py:192][0m |          -0.0111 |           2.1805 |           0.0744 |
[32m[20230207 15:03:49 @agent_ppo2.py:192][0m |           0.0025 |           1.9121 |           0.0744 |
[32m[20230207 15:03:49 @agent_ppo2.py:192][0m |          -0.0081 |           1.8390 |           0.0743 |
[32m[20230207 15:03:49 @agent_ppo2.py:192][0m |          -0.0114 |           1.8110 |           0.0743 |
[32m[20230207 15:03:49 @agent_ppo2.py:192][0m |          -0.0046 |           1.7898 |           0.0743 |
[32m[20230207 15:03:49 @agent_ppo2.py:192][0m |          -0.0021 |           1.7657 |           0.0743 |
[32m[20230207 15:03:49 @agent_ppo2.py:192][0m |          -0.0040 |           1.7472 |           0.0742 |
[32m[20230207 15:03:49 @agent_ppo2.py:192][0m |           0.0006 |           1.7369 |           0.0742 |
[32m[20230207 15:03:49 @agent_ppo2.py:192][0m |          -0.0065 |           1.7197 |           0.0742 |
[32m[20230207 15:03:49 @agent_ppo2.py:192][0m |          -0.0013 |           1.6995 |           0.0742 |
[32m[20230207 15:03:49 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: -90.34
[32m[20230207 15:03:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -89.05
[32m[20230207 15:03:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -130.16
[32m[20230207 15:03:50 @agent_ppo2.py:150][0m Total time:       3.15 min
[32m[20230207 15:03:50 @agent_ppo2.py:152][0m 167936 total steps have happened
[32m[20230207 15:03:50 @agent_ppo2.py:128][0m #------------------------ Iteration 82 --------------------------#
[32m[20230207 15:03:51 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:03:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:51 @agent_ppo2.py:192][0m |          -0.0003 |           4.0971 |           0.0757 |
[32m[20230207 15:03:51 @agent_ppo2.py:192][0m |          -0.0040 |           2.7965 |           0.0758 |
[32m[20230207 15:03:51 @agent_ppo2.py:192][0m |          -0.0033 |           2.7234 |           0.0757 |
[32m[20230207 15:03:51 @agent_ppo2.py:192][0m |          -0.0044 |           2.7789 |           0.0757 |
[32m[20230207 15:03:51 @agent_ppo2.py:192][0m |          -0.0049 |           2.7150 |           0.0757 |
[32m[20230207 15:03:52 @agent_ppo2.py:192][0m |          -0.0093 |           2.5538 |           0.0757 |
[32m[20230207 15:03:52 @agent_ppo2.py:192][0m |          -0.0093 |           2.5157 |           0.0758 |
[32m[20230207 15:03:52 @agent_ppo2.py:192][0m |          -0.0068 |           2.4556 |           0.0757 |
[32m[20230207 15:03:52 @agent_ppo2.py:192][0m |          -0.0060 |           2.4044 |           0.0758 |
[32m[20230207 15:03:52 @agent_ppo2.py:192][0m |          -0.0059 |           2.3470 |           0.0758 |
[32m[20230207 15:03:52 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:03:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.88
[32m[20230207 15:03:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -109.63
[32m[20230207 15:03:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -109.28
[32m[20230207 15:03:53 @agent_ppo2.py:150][0m Total time:       3.19 min
[32m[20230207 15:03:53 @agent_ppo2.py:152][0m 169984 total steps have happened
[32m[20230207 15:03:53 @agent_ppo2.py:128][0m #------------------------ Iteration 83 --------------------------#
[32m[20230207 15:03:53 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:03:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:54 @agent_ppo2.py:192][0m |          -0.0015 |           1.7736 |           0.0772 |
[32m[20230207 15:03:54 @agent_ppo2.py:192][0m |          -0.0026 |           1.5448 |           0.0772 |
[32m[20230207 15:03:54 @agent_ppo2.py:192][0m |           0.0030 |           1.4479 |           0.0772 |
[32m[20230207 15:03:54 @agent_ppo2.py:192][0m |          -0.0018 |           1.2536 |           0.0772 |
[32m[20230207 15:03:54 @agent_ppo2.py:192][0m |          -0.0061 |           1.0732 |           0.0772 |
[32m[20230207 15:03:54 @agent_ppo2.py:192][0m |          -0.0014 |           1.0235 |           0.0772 |
[32m[20230207 15:03:54 @agent_ppo2.py:192][0m |          -0.0056 |           0.9786 |           0.0772 |
[32m[20230207 15:03:54 @agent_ppo2.py:192][0m |          -0.0018 |           0.9595 |           0.0772 |
[32m[20230207 15:03:54 @agent_ppo2.py:192][0m |          -0.0091 |           0.9456 |           0.0773 |
[32m[20230207 15:03:54 @agent_ppo2.py:192][0m |          -0.0025 |           0.9116 |           0.0773 |
[32m[20230207 15:03:54 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:03:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: -118.40
[32m[20230207 15:03:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -111.28
[32m[20230207 15:03:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -116.96
[32m[20230207 15:03:55 @agent_ppo2.py:150][0m Total time:       3.23 min
[32m[20230207 15:03:55 @agent_ppo2.py:152][0m 172032 total steps have happened
[32m[20230207 15:03:55 @agent_ppo2.py:128][0m #------------------------ Iteration 84 --------------------------#
[32m[20230207 15:03:56 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:03:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:56 @agent_ppo2.py:192][0m |          -0.0053 |           2.5669 |           0.0779 |
[32m[20230207 15:03:56 @agent_ppo2.py:192][0m |          -0.0008 |           2.0348 |           0.0778 |
[32m[20230207 15:03:56 @agent_ppo2.py:192][0m |          -0.0045 |           1.9666 |           0.0778 |
[32m[20230207 15:03:56 @agent_ppo2.py:192][0m |          -0.0029 |           1.9367 |           0.0777 |
[32m[20230207 15:03:56 @agent_ppo2.py:192][0m |          -0.0011 |           1.8911 |           0.0777 |
[32m[20230207 15:03:56 @agent_ppo2.py:192][0m |          -0.0015 |           1.8486 |           0.0777 |
[32m[20230207 15:03:57 @agent_ppo2.py:192][0m |          -0.0043 |           1.8213 |           0.0777 |
[32m[20230207 15:03:57 @agent_ppo2.py:192][0m |          -0.0102 |           1.7802 |           0.0777 |
[32m[20230207 15:03:57 @agent_ppo2.py:192][0m |          -0.0047 |           1.7560 |           0.0777 |
[32m[20230207 15:03:57 @agent_ppo2.py:192][0m |          -0.0118 |           1.7122 |           0.0777 |
[32m[20230207 15:03:57 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:03:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: -96.95
[32m[20230207 15:03:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -93.59
[32m[20230207 15:03:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -111.60
[32m[20230207 15:03:58 @agent_ppo2.py:150][0m Total time:       3.27 min
[32m[20230207 15:03:58 @agent_ppo2.py:152][0m 174080 total steps have happened
[32m[20230207 15:03:58 @agent_ppo2.py:128][0m #------------------------ Iteration 85 --------------------------#
[32m[20230207 15:03:59 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:03:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:03:59 @agent_ppo2.py:192][0m |          -0.0020 |           3.5481 |           0.0766 |
[32m[20230207 15:03:59 @agent_ppo2.py:192][0m |          -0.0042 |           1.7658 |           0.0766 |
[32m[20230207 15:03:59 @agent_ppo2.py:192][0m |          -0.0063 |           1.7283 |           0.0765 |
[32m[20230207 15:03:59 @agent_ppo2.py:192][0m |          -0.0061 |           1.6349 |           0.0765 |
[32m[20230207 15:03:59 @agent_ppo2.py:192][0m |          -0.0077 |           1.5818 |           0.0765 |
[32m[20230207 15:03:59 @agent_ppo2.py:192][0m |          -0.0102 |           1.7921 |           0.0765 |
[32m[20230207 15:03:59 @agent_ppo2.py:192][0m |          -0.0097 |           1.7071 |           0.0765 |
[32m[20230207 15:03:59 @agent_ppo2.py:192][0m |          -0.0029 |           1.5512 |           0.0765 |
[32m[20230207 15:03:59 @agent_ppo2.py:192][0m |          -0.0071 |           1.5788 |           0.0765 |
[32m[20230207 15:03:59 @agent_ppo2.py:192][0m |          -0.0088 |           1.5176 |           0.0765 |
[32m[20230207 15:03:59 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:04:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: -130.31
[32m[20230207 15:04:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -114.11
[32m[20230207 15:04:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -120.01
[32m[20230207 15:04:00 @agent_ppo2.py:150][0m Total time:       3.32 min
[32m[20230207 15:04:00 @agent_ppo2.py:152][0m 176128 total steps have happened
[32m[20230207 15:04:00 @agent_ppo2.py:128][0m #------------------------ Iteration 86 --------------------------#
[32m[20230207 15:04:01 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:04:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:01 @agent_ppo2.py:192][0m |          -0.0010 |           4.2327 |           0.0776 |
[32m[20230207 15:04:01 @agent_ppo2.py:192][0m |          -0.0009 |           2.2559 |           0.0776 |
[32m[20230207 15:04:01 @agent_ppo2.py:192][0m |          -0.0025 |           2.0586 |           0.0776 |
[32m[20230207 15:04:01 @agent_ppo2.py:192][0m |          -0.0035 |           1.9143 |           0.0775 |
[32m[20230207 15:04:01 @agent_ppo2.py:192][0m |          -0.0044 |           1.9164 |           0.0775 |
[32m[20230207 15:04:02 @agent_ppo2.py:192][0m |          -0.0047 |           1.8083 |           0.0775 |
[32m[20230207 15:04:02 @agent_ppo2.py:192][0m |          -0.0062 |           1.7639 |           0.0775 |
[32m[20230207 15:04:02 @agent_ppo2.py:192][0m |          -0.0062 |           1.8492 |           0.0775 |
[32m[20230207 15:04:02 @agent_ppo2.py:192][0m |          -0.0063 |           1.7550 |           0.0775 |
[32m[20230207 15:04:02 @agent_ppo2.py:192][0m |          -0.0017 |           1.7014 |           0.0775 |
[32m[20230207 15:04:02 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:04:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -113.45
[32m[20230207 15:04:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.68
[32m[20230207 15:04:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -132.48
[32m[20230207 15:04:03 @agent_ppo2.py:150][0m Total time:       3.36 min
[32m[20230207 15:04:03 @agent_ppo2.py:152][0m 178176 total steps have happened
[32m[20230207 15:04:03 @agent_ppo2.py:128][0m #------------------------ Iteration 87 --------------------------#
[32m[20230207 15:04:03 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:04:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:04 @agent_ppo2.py:192][0m |          -0.0060 |           6.5061 |           0.0785 |
[32m[20230207 15:04:04 @agent_ppo2.py:192][0m |          -0.0011 |           3.6265 |           0.0785 |
[32m[20230207 15:04:04 @agent_ppo2.py:192][0m |          -0.0043 |           3.5265 |           0.0785 |
[32m[20230207 15:04:04 @agent_ppo2.py:192][0m |          -0.0054 |           3.3787 |           0.0785 |
[32m[20230207 15:04:04 @agent_ppo2.py:192][0m |          -0.0121 |           3.3785 |           0.0785 |
[32m[20230207 15:04:04 @agent_ppo2.py:192][0m |          -0.0096 |           3.2847 |           0.0785 |
[32m[20230207 15:04:04 @agent_ppo2.py:192][0m |          -0.0095 |           3.2559 |           0.0785 |
[32m[20230207 15:04:04 @agent_ppo2.py:192][0m |          -0.0122 |           3.2345 |           0.0785 |
[32m[20230207 15:04:04 @agent_ppo2.py:192][0m |          -0.0090 |           3.1963 |           0.0785 |
[32m[20230207 15:04:04 @agent_ppo2.py:192][0m |          -0.0085 |           3.1637 |           0.0785 |
[32m[20230207 15:04:04 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:04:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -84.27
[32m[20230207 15:04:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -33.52
[32m[20230207 15:04:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -94.64
[32m[20230207 15:04:05 @agent_ppo2.py:150][0m Total time:       3.40 min
[32m[20230207 15:04:05 @agent_ppo2.py:152][0m 180224 total steps have happened
[32m[20230207 15:04:05 @agent_ppo2.py:128][0m #------------------------ Iteration 88 --------------------------#
[32m[20230207 15:04:06 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:04:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:06 @agent_ppo2.py:192][0m |           0.0080 |           2.0418 |           0.0762 |
[32m[20230207 15:04:06 @agent_ppo2.py:192][0m |          -0.0142 |           1.7941 |           0.0761 |
[32m[20230207 15:04:06 @agent_ppo2.py:192][0m |           0.0028 |           1.7308 |           0.0761 |
[32m[20230207 15:04:06 @agent_ppo2.py:192][0m |          -0.0072 |           1.7118 |           0.0761 |
[32m[20230207 15:04:06 @agent_ppo2.py:192][0m |          -0.0030 |           1.7071 |           0.0760 |
[32m[20230207 15:04:07 @agent_ppo2.py:192][0m |          -0.0000 |           1.6729 |           0.0761 |
[32m[20230207 15:04:07 @agent_ppo2.py:192][0m |          -0.0055 |           1.6622 |           0.0760 |
[32m[20230207 15:04:07 @agent_ppo2.py:192][0m |          -0.0026 |           1.6522 |           0.0760 |
[32m[20230207 15:04:07 @agent_ppo2.py:192][0m |          -0.0014 |           1.6348 |           0.0760 |
[32m[20230207 15:04:07 @agent_ppo2.py:192][0m |           0.0080 |           1.6371 |           0.0760 |
[32m[20230207 15:04:07 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -99.59
[32m[20230207 15:04:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -89.76
[32m[20230207 15:04:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -103.25
[32m[20230207 15:04:08 @agent_ppo2.py:150][0m Total time:       3.44 min
[32m[20230207 15:04:08 @agent_ppo2.py:152][0m 182272 total steps have happened
[32m[20230207 15:04:08 @agent_ppo2.py:128][0m #------------------------ Iteration 89 --------------------------#
[32m[20230207 15:04:08 @agent_ppo2.py:134][0m Sampling time: 0.76 s by 1 slaves
[32m[20230207 15:04:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:09 @agent_ppo2.py:192][0m |          -0.0005 |          12.8810 |           0.0767 |
[32m[20230207 15:04:09 @agent_ppo2.py:192][0m |          -0.0021 |           5.9394 |           0.0768 |
[32m[20230207 15:04:09 @agent_ppo2.py:192][0m |          -0.0039 |           4.9071 |           0.0767 |
[32m[20230207 15:04:09 @agent_ppo2.py:192][0m |          -0.0038 |           5.3237 |           0.0767 |
[32m[20230207 15:04:09 @agent_ppo2.py:192][0m |          -0.0055 |           4.5526 |           0.0767 |
[32m[20230207 15:04:09 @agent_ppo2.py:192][0m |          -0.0056 |           4.3703 |           0.0767 |
[32m[20230207 15:04:09 @agent_ppo2.py:192][0m |          -0.0065 |           4.3512 |           0.0767 |
[32m[20230207 15:04:09 @agent_ppo2.py:192][0m |          -0.0058 |           4.0818 |           0.0767 |
[32m[20230207 15:04:09 @agent_ppo2.py:192][0m |          -0.0076 |           3.9180 |           0.0767 |
[32m[20230207 15:04:09 @agent_ppo2.py:192][0m |          -0.0082 |           3.8266 |           0.0767 |
[32m[20230207 15:04:09 @agent_ppo2.py:137][0m Policy update time: 0.75 s
[32m[20230207 15:04:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -154.02
[32m[20230207 15:04:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -150.60
[32m[20230207 15:04:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -128.47
[32m[20230207 15:04:10 @agent_ppo2.py:150][0m Total time:       3.48 min
[32m[20230207 15:04:10 @agent_ppo2.py:152][0m 184320 total steps have happened
[32m[20230207 15:04:10 @agent_ppo2.py:128][0m #------------------------ Iteration 90 --------------------------#
[32m[20230207 15:04:11 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:04:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:11 @agent_ppo2.py:192][0m |          -0.0020 |           1.6447 |           0.0787 |
[32m[20230207 15:04:11 @agent_ppo2.py:192][0m |          -0.0148 |           1.4790 |           0.0786 |
[32m[20230207 15:04:11 @agent_ppo2.py:192][0m |           0.0101 |           1.4410 |           0.0786 |
[32m[20230207 15:04:11 @agent_ppo2.py:192][0m |           0.0052 |           1.4198 |           0.0785 |
[32m[20230207 15:04:11 @agent_ppo2.py:192][0m |          -0.0068 |           1.4069 |           0.0784 |
[32m[20230207 15:04:11 @agent_ppo2.py:192][0m |          -0.0088 |           1.3902 |           0.0784 |
[32m[20230207 15:04:11 @agent_ppo2.py:192][0m |          -0.0108 |           1.3856 |           0.0784 |
[32m[20230207 15:04:12 @agent_ppo2.py:192][0m |          -0.0001 |           1.3663 |           0.0784 |
[32m[20230207 15:04:12 @agent_ppo2.py:192][0m |           0.0086 |           1.3619 |           0.0784 |
[32m[20230207 15:04:12 @agent_ppo2.py:192][0m |          -0.0074 |           1.3541 |           0.0784 |
[32m[20230207 15:04:12 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -101.59
[32m[20230207 15:04:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -74.22
[32m[20230207 15:04:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -119.19
[32m[20230207 15:04:13 @agent_ppo2.py:150][0m Total time:       3.52 min
[32m[20230207 15:04:13 @agent_ppo2.py:152][0m 186368 total steps have happened
[32m[20230207 15:04:13 @agent_ppo2.py:128][0m #------------------------ Iteration 91 --------------------------#
[32m[20230207 15:04:13 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:04:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:14 @agent_ppo2.py:192][0m |           0.0010 |           4.4785 |           0.0803 |
[32m[20230207 15:04:14 @agent_ppo2.py:192][0m |          -0.0012 |           2.1158 |           0.0803 |
[32m[20230207 15:04:14 @agent_ppo2.py:192][0m |          -0.0019 |           1.7121 |           0.0803 |
[32m[20230207 15:04:14 @agent_ppo2.py:192][0m |          -0.0026 |           1.4545 |           0.0802 |
[32m[20230207 15:04:14 @agent_ppo2.py:192][0m |          -0.0034 |           1.3844 |           0.0802 |
[32m[20230207 15:04:14 @agent_ppo2.py:192][0m |          -0.0038 |           1.3491 |           0.0801 |
[32m[20230207 15:04:14 @agent_ppo2.py:192][0m |          -0.0041 |           1.3287 |           0.0801 |
[32m[20230207 15:04:14 @agent_ppo2.py:192][0m |          -0.0047 |           1.2840 |           0.0801 |
[32m[20230207 15:04:14 @agent_ppo2.py:192][0m |          -0.0045 |           1.2828 |           0.0800 |
[32m[20230207 15:04:14 @agent_ppo2.py:192][0m |          -0.0050 |           1.2594 |           0.0800 |
[32m[20230207 15:04:14 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: -126.05
[32m[20230207 15:04:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -110.90
[32m[20230207 15:04:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -106.31
[32m[20230207 15:04:15 @agent_ppo2.py:150][0m Total time:       3.56 min
[32m[20230207 15:04:15 @agent_ppo2.py:152][0m 188416 total steps have happened
[32m[20230207 15:04:15 @agent_ppo2.py:128][0m #------------------------ Iteration 92 --------------------------#
[32m[20230207 15:04:16 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:04:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:16 @agent_ppo2.py:192][0m |           0.0096 |           1.6102 |           0.0778 |
[32m[20230207 15:04:16 @agent_ppo2.py:192][0m |          -0.0016 |           1.2860 |           0.0778 |
[32m[20230207 15:04:16 @agent_ppo2.py:192][0m |          -0.0036 |           1.2775 |           0.0778 |
[32m[20230207 15:04:16 @agent_ppo2.py:192][0m |          -0.0039 |           1.2159 |           0.0778 |
[32m[20230207 15:04:16 @agent_ppo2.py:192][0m |          -0.0039 |           1.2073 |           0.0778 |
[32m[20230207 15:04:16 @agent_ppo2.py:192][0m |          -0.0008 |           1.1766 |           0.0778 |
[32m[20230207 15:04:16 @agent_ppo2.py:192][0m |           0.0017 |           1.1704 |           0.0778 |
[32m[20230207 15:04:16 @agent_ppo2.py:192][0m |          -0.0019 |           1.1384 |           0.0779 |
[32m[20230207 15:04:17 @agent_ppo2.py:192][0m |           0.0059 |           1.1262 |           0.0779 |
[32m[20230207 15:04:17 @agent_ppo2.py:192][0m |          -0.0008 |           1.1194 |           0.0780 |
[32m[20230207 15:04:17 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: -87.23
[32m[20230207 15:04:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -84.80
[32m[20230207 15:04:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -104.57
[32m[20230207 15:04:17 @agent_ppo2.py:150][0m Total time:       3.60 min
[32m[20230207 15:04:17 @agent_ppo2.py:152][0m 190464 total steps have happened
[32m[20230207 15:04:17 @agent_ppo2.py:128][0m #------------------------ Iteration 93 --------------------------#
[32m[20230207 15:04:18 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:04:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:18 @agent_ppo2.py:192][0m |          -0.0024 |           6.4139 |           0.0783 |
[32m[20230207 15:04:19 @agent_ppo2.py:192][0m |          -0.0041 |           5.2556 |           0.0783 |
[32m[20230207 15:04:19 @agent_ppo2.py:192][0m |          -0.0057 |           4.5150 |           0.0782 |
[32m[20230207 15:04:19 @agent_ppo2.py:192][0m |          -0.0064 |           4.3915 |           0.0781 |
[32m[20230207 15:04:19 @agent_ppo2.py:192][0m |          -0.0071 |           4.2745 |           0.0781 |
[32m[20230207 15:04:19 @agent_ppo2.py:192][0m |          -0.0079 |           4.2119 |           0.0780 |
[32m[20230207 15:04:19 @agent_ppo2.py:192][0m |          -0.0068 |           4.2405 |           0.0780 |
[32m[20230207 15:04:19 @agent_ppo2.py:192][0m |          -0.0080 |           4.1157 |           0.0779 |
[32m[20230207 15:04:19 @agent_ppo2.py:192][0m |          -0.0073 |           4.1321 |           0.0779 |
[32m[20230207 15:04:19 @agent_ppo2.py:192][0m |          -0.0070 |           4.0249 |           0.0778 |
[32m[20230207 15:04:19 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -117.05
[32m[20230207 15:04:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -108.64
[32m[20230207 15:04:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -115.17
[32m[20230207 15:04:20 @agent_ppo2.py:150][0m Total time:       3.65 min
[32m[20230207 15:04:20 @agent_ppo2.py:152][0m 192512 total steps have happened
[32m[20230207 15:04:20 @agent_ppo2.py:128][0m #------------------------ Iteration 94 --------------------------#
[32m[20230207 15:04:21 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:04:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:21 @agent_ppo2.py:192][0m |           0.0002 |          10.1196 |           0.0778 |
[32m[20230207 15:04:21 @agent_ppo2.py:192][0m |           0.0036 |           6.0178 |           0.0777 |
[32m[20230207 15:04:21 @agent_ppo2.py:192][0m |           0.0116 |           5.2371 |           0.0777 |
[32m[20230207 15:04:21 @agent_ppo2.py:192][0m |          -0.0058 |           4.9613 |           0.0776 |
[32m[20230207 15:04:21 @agent_ppo2.py:192][0m |          -0.0051 |           4.9538 |           0.0775 |
[32m[20230207 15:04:21 @agent_ppo2.py:192][0m |          -0.0069 |           4.8044 |           0.0775 |
[32m[20230207 15:04:21 @agent_ppo2.py:192][0m |          -0.0065 |           4.7855 |           0.0775 |
[32m[20230207 15:04:22 @agent_ppo2.py:192][0m |          -0.0027 |           4.8607 |           0.0775 |
[32m[20230207 15:04:22 @agent_ppo2.py:192][0m |          -0.0075 |           4.7611 |           0.0775 |
[32m[20230207 15:04:22 @agent_ppo2.py:192][0m |           0.0027 |           5.3917 |           0.0775 |
[32m[20230207 15:04:22 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:04:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: -117.83
[32m[20230207 15:04:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -105.48
[32m[20230207 15:04:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -84.85
[32m[20230207 15:04:22 @agent_ppo2.py:150][0m Total time:       3.69 min
[32m[20230207 15:04:22 @agent_ppo2.py:152][0m 194560 total steps have happened
[32m[20230207 15:04:22 @agent_ppo2.py:128][0m #------------------------ Iteration 95 --------------------------#
[32m[20230207 15:04:23 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:04:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:23 @agent_ppo2.py:192][0m |           0.0024 |          17.2738 |           0.0794 |
[32m[20230207 15:04:24 @agent_ppo2.py:192][0m |           0.0014 |          12.7939 |           0.0795 |
[32m[20230207 15:04:24 @agent_ppo2.py:192][0m |          -0.0028 |          11.5872 |           0.0796 |
[32m[20230207 15:04:24 @agent_ppo2.py:192][0m |          -0.0052 |          11.1768 |           0.0796 |
[32m[20230207 15:04:24 @agent_ppo2.py:192][0m |          -0.0023 |          11.2911 |           0.0797 |
[32m[20230207 15:04:24 @agent_ppo2.py:192][0m |          -0.0044 |          11.0886 |           0.0797 |
[32m[20230207 15:04:24 @agent_ppo2.py:192][0m |          -0.0050 |          10.8842 |           0.0798 |
[32m[20230207 15:04:24 @agent_ppo2.py:192][0m |          -0.0081 |          10.6440 |           0.0799 |
[32m[20230207 15:04:24 @agent_ppo2.py:192][0m |          -0.0081 |          10.5119 |           0.0800 |
[32m[20230207 15:04:24 @agent_ppo2.py:192][0m |          -0.0081 |          10.4924 |           0.0800 |
[32m[20230207 15:04:24 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:04:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -118.12
[32m[20230207 15:04:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -78.61
[32m[20230207 15:04:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -100.35
[32m[20230207 15:04:25 @agent_ppo2.py:150][0m Total time:       3.73 min
[32m[20230207 15:04:25 @agent_ppo2.py:152][0m 196608 total steps have happened
[32m[20230207 15:04:25 @agent_ppo2.py:128][0m #------------------------ Iteration 96 --------------------------#
[32m[20230207 15:04:26 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:04:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:26 @agent_ppo2.py:192][0m |          -0.0093 |          12.5889 |           0.0788 |
[32m[20230207 15:04:26 @agent_ppo2.py:192][0m |          -0.0054 |           8.4939 |           0.0787 |
[32m[20230207 15:04:26 @agent_ppo2.py:192][0m |          -0.0181 |           8.1459 |           0.0787 |
[32m[20230207 15:04:26 @agent_ppo2.py:192][0m |          -0.0072 |           8.0578 |           0.0787 |
[32m[20230207 15:04:26 @agent_ppo2.py:192][0m |           0.0119 |           7.9809 |           0.0787 |
[32m[20230207 15:04:26 @agent_ppo2.py:192][0m |          -0.0021 |           7.8423 |           0.0787 |
[32m[20230207 15:04:26 @agent_ppo2.py:192][0m |          -0.0065 |           7.7494 |           0.0787 |
[32m[20230207 15:04:26 @agent_ppo2.py:192][0m |          -0.0113 |           7.7204 |           0.0787 |
[32m[20230207 15:04:26 @agent_ppo2.py:192][0m |          -0.0017 |           7.6451 |           0.0788 |
[32m[20230207 15:04:27 @agent_ppo2.py:192][0m |          -0.0094 |           7.6714 |           0.0788 |
[32m[20230207 15:04:27 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -84.05
[32m[20230207 15:04:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -64.15
[32m[20230207 15:04:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -106.79
[32m[20230207 15:04:27 @agent_ppo2.py:150][0m Total time:       3.77 min
[32m[20230207 15:04:27 @agent_ppo2.py:152][0m 198656 total steps have happened
[32m[20230207 15:04:27 @agent_ppo2.py:128][0m #------------------------ Iteration 97 --------------------------#
[32m[20230207 15:04:28 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:04:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:28 @agent_ppo2.py:192][0m |          -0.0366 |           5.9106 |           0.0812 |
[32m[20230207 15:04:28 @agent_ppo2.py:192][0m |           0.0044 |           5.0839 |           0.0811 |
[32m[20230207 15:04:29 @agent_ppo2.py:192][0m |           0.0034 |           4.9781 |           0.0811 |
[32m[20230207 15:04:29 @agent_ppo2.py:192][0m |          -0.0323 |           4.9318 |           0.0811 |
[32m[20230207 15:04:29 @agent_ppo2.py:192][0m |          -0.0044 |           4.8641 |           0.0811 |
[32m[20230207 15:04:29 @agent_ppo2.py:192][0m |          -0.0028 |           4.7876 |           0.0811 |
[32m[20230207 15:04:29 @agent_ppo2.py:192][0m |          -0.0051 |           4.7463 |           0.0811 |
[32m[20230207 15:04:29 @agent_ppo2.py:192][0m |          -0.0013 |           4.7179 |           0.0810 |
[32m[20230207 15:04:29 @agent_ppo2.py:192][0m |          -0.0095 |           4.6956 |           0.0811 |
[32m[20230207 15:04:29 @agent_ppo2.py:192][0m |          -0.0031 |           4.6685 |           0.0811 |
[32m[20230207 15:04:29 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -104.09
[32m[20230207 15:04:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -94.19
[32m[20230207 15:04:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -96.88
[32m[20230207 15:04:30 @agent_ppo2.py:150][0m Total time:       3.81 min
[32m[20230207 15:04:30 @agent_ppo2.py:152][0m 200704 total steps have happened
[32m[20230207 15:04:30 @agent_ppo2.py:128][0m #------------------------ Iteration 98 --------------------------#
[32m[20230207 15:04:31 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:04:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:31 @agent_ppo2.py:192][0m |           0.0005 |          12.3179 |           0.0793 |
[32m[20230207 15:04:31 @agent_ppo2.py:192][0m |          -0.0058 |           7.8904 |           0.0793 |
[32m[20230207 15:04:31 @agent_ppo2.py:192][0m |          -0.0046 |           7.3127 |           0.0793 |
[32m[20230207 15:04:31 @agent_ppo2.py:192][0m |          -0.0142 |           7.8627 |           0.0793 |
[32m[20230207 15:04:31 @agent_ppo2.py:192][0m |          -0.0083 |           7.1249 |           0.0793 |
[32m[20230207 15:04:31 @agent_ppo2.py:192][0m |          -0.0093 |           6.2059 |           0.0793 |
[32m[20230207 15:04:31 @agent_ppo2.py:192][0m |          -0.0060 |           6.1384 |           0.0793 |
[32m[20230207 15:04:31 @agent_ppo2.py:192][0m |          -0.0067 |           5.9622 |           0.0793 |
[32m[20230207 15:04:32 @agent_ppo2.py:192][0m |          -0.0117 |           6.1261 |           0.0793 |
[32m[20230207 15:04:32 @agent_ppo2.py:192][0m |          -0.0096 |           5.7119 |           0.0793 |
[32m[20230207 15:04:32 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:04:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: -123.53
[32m[20230207 15:04:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.49
[32m[20230207 15:04:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -122.10
[32m[20230207 15:04:33 @agent_ppo2.py:150][0m Total time:       3.86 min
[32m[20230207 15:04:33 @agent_ppo2.py:152][0m 202752 total steps have happened
[32m[20230207 15:04:33 @agent_ppo2.py:128][0m #------------------------ Iteration 99 --------------------------#
[32m[20230207 15:04:33 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:04:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:33 @agent_ppo2.py:192][0m |           0.0002 |           4.2336 |           0.0807 |
[32m[20230207 15:04:34 @agent_ppo2.py:192][0m |           0.0011 |           3.6758 |           0.0807 |
[32m[20230207 15:04:34 @agent_ppo2.py:192][0m |          -0.0043 |           3.6454 |           0.0806 |
[32m[20230207 15:04:34 @agent_ppo2.py:192][0m |          -0.0084 |           3.6233 |           0.0806 |
[32m[20230207 15:04:34 @agent_ppo2.py:192][0m |          -0.0004 |           3.5900 |           0.0806 |
[32m[20230207 15:04:34 @agent_ppo2.py:192][0m |          -0.0056 |           3.5516 |           0.0806 |
[32m[20230207 15:04:34 @agent_ppo2.py:192][0m |          -0.0056 |           3.5432 |           0.0806 |
[32m[20230207 15:04:34 @agent_ppo2.py:192][0m |          -0.0059 |           3.5304 |           0.0806 |
[32m[20230207 15:04:34 @agent_ppo2.py:192][0m |          -0.0783 |           3.5199 |           0.0806 |
[32m[20230207 15:04:34 @agent_ppo2.py:192][0m |          -0.0121 |           3.5175 |           0.0806 |
[32m[20230207 15:04:34 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: -140.75
[32m[20230207 15:04:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -139.69
[32m[20230207 15:04:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -120.81
[32m[20230207 15:04:35 @agent_ppo2.py:150][0m Total time:       3.90 min
[32m[20230207 15:04:35 @agent_ppo2.py:152][0m 204800 total steps have happened
[32m[20230207 15:04:35 @agent_ppo2.py:128][0m #------------------------ Iteration 100 --------------------------#
[32m[20230207 15:04:36 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:04:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:36 @agent_ppo2.py:192][0m |          -0.0156 |           9.5890 |           0.0813 |
[32m[20230207 15:04:36 @agent_ppo2.py:192][0m |          -0.0015 |           7.4609 |           0.0813 |
[32m[20230207 15:04:36 @agent_ppo2.py:192][0m |          -0.0012 |           7.4824 |           0.0813 |
[32m[20230207 15:04:36 @agent_ppo2.py:192][0m |          -0.0057 |           6.8423 |           0.0813 |
[32m[20230207 15:04:36 @agent_ppo2.py:192][0m |          -0.0063 |           6.7995 |           0.0812 |
[32m[20230207 15:04:36 @agent_ppo2.py:192][0m |          -0.0069 |           6.5532 |           0.0812 |
[32m[20230207 15:04:36 @agent_ppo2.py:192][0m |          -0.0012 |           6.5142 |           0.0812 |
[32m[20230207 15:04:37 @agent_ppo2.py:192][0m |          -0.0079 |           6.2785 |           0.0812 |
[32m[20230207 15:04:37 @agent_ppo2.py:192][0m |           0.0133 |           7.1086 |           0.0813 |
[32m[20230207 15:04:37 @agent_ppo2.py:192][0m |           0.0369 |           9.6746 |           0.0813 |
[32m[20230207 15:04:37 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:04:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: -106.16
[32m[20230207 15:04:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.70
[32m[20230207 15:04:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -123.26
[32m[20230207 15:04:37 @agent_ppo2.py:150][0m Total time:       3.94 min
[32m[20230207 15:04:37 @agent_ppo2.py:152][0m 206848 total steps have happened
[32m[20230207 15:04:37 @agent_ppo2.py:128][0m #------------------------ Iteration 101 --------------------------#
[32m[20230207 15:04:38 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:04:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:38 @agent_ppo2.py:192][0m |          -0.0027 |           7.1807 |           0.0818 |
[32m[20230207 15:04:38 @agent_ppo2.py:192][0m |          -0.0066 |           4.1835 |           0.0817 |
[32m[20230207 15:04:39 @agent_ppo2.py:192][0m |          -0.0078 |           3.0964 |           0.0817 |
[32m[20230207 15:04:39 @agent_ppo2.py:192][0m |          -0.0092 |           2.6761 |           0.0817 |
[32m[20230207 15:04:39 @agent_ppo2.py:192][0m |          -0.0089 |           2.4445 |           0.0817 |
[32m[20230207 15:04:39 @agent_ppo2.py:192][0m |          -0.0110 |           2.2971 |           0.0817 |
[32m[20230207 15:04:39 @agent_ppo2.py:192][0m |          -0.0080 |           2.1480 |           0.0817 |
[32m[20230207 15:04:39 @agent_ppo2.py:192][0m |          -0.0111 |           2.1212 |           0.0817 |
[32m[20230207 15:04:39 @agent_ppo2.py:192][0m |          -0.0109 |           2.0226 |           0.0817 |
[32m[20230207 15:04:39 @agent_ppo2.py:192][0m |          -0.0118 |           1.9709 |           0.0818 |
[32m[20230207 15:04:39 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: -93.76
[32m[20230207 15:04:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -59.16
[32m[20230207 15:04:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -108.28
[32m[20230207 15:04:40 @agent_ppo2.py:150][0m Total time:       3.98 min
[32m[20230207 15:04:40 @agent_ppo2.py:152][0m 208896 total steps have happened
[32m[20230207 15:04:40 @agent_ppo2.py:128][0m #------------------------ Iteration 102 --------------------------#
[32m[20230207 15:04:41 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:04:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:41 @agent_ppo2.py:192][0m |          -0.0010 |           2.4361 |           0.0821 |
[32m[20230207 15:04:41 @agent_ppo2.py:192][0m |          -0.0083 |           1.7891 |           0.0820 |
[32m[20230207 15:04:41 @agent_ppo2.py:192][0m |          -0.0034 |           1.7399 |           0.0820 |
[32m[20230207 15:04:41 @agent_ppo2.py:192][0m |           0.0016 |           1.7081 |           0.0820 |
[32m[20230207 15:04:41 @agent_ppo2.py:192][0m |          -0.0140 |           1.6973 |           0.0820 |
[32m[20230207 15:04:41 @agent_ppo2.py:192][0m |          -0.0066 |           1.6695 |           0.0820 |
[32m[20230207 15:04:41 @agent_ppo2.py:192][0m |          -0.0092 |           1.6420 |           0.0820 |
[32m[20230207 15:04:41 @agent_ppo2.py:192][0m |           0.0381 |           1.6222 |           0.0821 |
[32m[20230207 15:04:41 @agent_ppo2.py:192][0m |          -0.0089 |           1.6342 |           0.0821 |
[32m[20230207 15:04:41 @agent_ppo2.py:192][0m |          -0.0030 |           1.6084 |           0.0821 |
[32m[20230207 15:04:41 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: -101.04
[32m[20230207 15:04:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -90.39
[32m[20230207 15:04:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -95.28
[32m[20230207 15:04:42 @agent_ppo2.py:150][0m Total time:       4.02 min
[32m[20230207 15:04:42 @agent_ppo2.py:152][0m 210944 total steps have happened
[32m[20230207 15:04:42 @agent_ppo2.py:128][0m #------------------------ Iteration 103 --------------------------#
[32m[20230207 15:04:43 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:04:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:43 @agent_ppo2.py:192][0m |          -0.0024 |          22.6497 |           0.0826 |
[32m[20230207 15:04:43 @agent_ppo2.py:192][0m |           0.0007 |          19.8866 |           0.0825 |
[32m[20230207 15:04:43 @agent_ppo2.py:192][0m |          -0.0076 |          18.7623 |           0.0825 |
[32m[20230207 15:04:44 @agent_ppo2.py:192][0m |          -0.0085 |          16.6923 |           0.0824 |
[32m[20230207 15:04:44 @agent_ppo2.py:192][0m |          -0.0069 |          14.4956 |           0.0823 |
[32m[20230207 15:04:44 @agent_ppo2.py:192][0m |          -0.0096 |          11.6568 |           0.0823 |
[32m[20230207 15:04:44 @agent_ppo2.py:192][0m |          -0.0105 |           7.1632 |           0.0822 |
[32m[20230207 15:04:44 @agent_ppo2.py:192][0m |          -0.0013 |           4.1131 |           0.0821 |
[32m[20230207 15:04:44 @agent_ppo2.py:192][0m |          -0.0110 |           2.7223 |           0.0821 |
[32m[20230207 15:04:44 @agent_ppo2.py:192][0m |          -0.0106 |           2.2070 |           0.0820 |
[32m[20230207 15:04:44 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:04:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.74
[32m[20230207 15:04:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -78.53
[32m[20230207 15:04:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -88.96
[32m[20230207 15:04:45 @agent_ppo2.py:150][0m Total time:       4.06 min
[32m[20230207 15:04:45 @agent_ppo2.py:152][0m 212992 total steps have happened
[32m[20230207 15:04:45 @agent_ppo2.py:128][0m #------------------------ Iteration 104 --------------------------#
[32m[20230207 15:04:46 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:04:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:46 @agent_ppo2.py:192][0m |          -0.0037 |           2.6810 |           0.0789 |
[32m[20230207 15:04:46 @agent_ppo2.py:192][0m |           0.0080 |           1.4431 |           0.0789 |
[32m[20230207 15:04:46 @agent_ppo2.py:192][0m |          -0.0035 |           1.2657 |           0.0789 |
[32m[20230207 15:04:46 @agent_ppo2.py:192][0m |          -0.0061 |           1.1585 |           0.0789 |
[32m[20230207 15:04:46 @agent_ppo2.py:192][0m |          -0.0105 |           1.0740 |           0.0788 |
[32m[20230207 15:04:46 @agent_ppo2.py:192][0m |           0.0066 |           1.0088 |           0.0788 |
[32m[20230207 15:04:46 @agent_ppo2.py:192][0m |          -0.0019 |           0.9519 |           0.0788 |
[32m[20230207 15:04:46 @agent_ppo2.py:192][0m |          -0.0090 |           0.8999 |           0.0788 |
[32m[20230207 15:04:46 @agent_ppo2.py:192][0m |          -0.0053 |           0.8773 |           0.0788 |
[32m[20230207 15:04:46 @agent_ppo2.py:192][0m |          -0.0035 |           0.8424 |           0.0788 |
[32m[20230207 15:04:46 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:47 @agent_ppo2.py:145][0m Average TRAINING episode reward: -102.02
[32m[20230207 15:04:47 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -97.14
[32m[20230207 15:04:47 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -95.14
[32m[20230207 15:04:47 @agent_ppo2.py:150][0m Total time:       4.10 min
[32m[20230207 15:04:47 @agent_ppo2.py:152][0m 215040 total steps have happened
[32m[20230207 15:04:47 @agent_ppo2.py:128][0m #------------------------ Iteration 105 --------------------------#
[32m[20230207 15:04:48 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:04:48 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:48 @agent_ppo2.py:192][0m |           0.0059 |           1.6712 |           0.0791 |
[32m[20230207 15:04:48 @agent_ppo2.py:192][0m |           0.0041 |           1.3605 |           0.0791 |
[32m[20230207 15:04:48 @agent_ppo2.py:192][0m |          -0.0032 |           1.3269 |           0.0790 |
[32m[20230207 15:04:48 @agent_ppo2.py:192][0m |          -0.0044 |           1.2988 |           0.0789 |
[32m[20230207 15:04:49 @agent_ppo2.py:192][0m |          -0.0045 |           1.2617 |           0.0790 |
[32m[20230207 15:04:49 @agent_ppo2.py:192][0m |           0.0004 |           1.2428 |           0.0789 |
[32m[20230207 15:04:49 @agent_ppo2.py:192][0m |           0.0012 |           1.2304 |           0.0790 |
[32m[20230207 15:04:49 @agent_ppo2.py:192][0m |          -0.0125 |           1.2213 |           0.0791 |
[32m[20230207 15:04:49 @agent_ppo2.py:192][0m |          -0.0343 |           1.5635 |           0.0791 |
[32m[20230207 15:04:49 @agent_ppo2.py:192][0m |           0.0010 |           1.2300 |           0.0792 |
[32m[20230207 15:04:49 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: -107.22
[32m[20230207 15:04:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -96.57
[32m[20230207 15:04:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -148.08
[32m[20230207 15:04:50 @agent_ppo2.py:150][0m Total time:       4.14 min
[32m[20230207 15:04:50 @agent_ppo2.py:152][0m 217088 total steps have happened
[32m[20230207 15:04:50 @agent_ppo2.py:128][0m #------------------------ Iteration 106 --------------------------#
[32m[20230207 15:04:51 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:04:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:51 @agent_ppo2.py:192][0m |           0.0115 |           1.6029 |           0.0814 |
[32m[20230207 15:04:51 @agent_ppo2.py:192][0m |          -0.0178 |           1.4260 |           0.0814 |
[32m[20230207 15:04:51 @agent_ppo2.py:192][0m |          -0.0067 |           1.4001 |           0.0814 |
[32m[20230207 15:04:51 @agent_ppo2.py:192][0m |          -0.0093 |           1.3747 |           0.0814 |
[32m[20230207 15:04:51 @agent_ppo2.py:192][0m |          -0.0046 |           1.3568 |           0.0814 |
[32m[20230207 15:04:51 @agent_ppo2.py:192][0m |          -0.0002 |           1.3615 |           0.0814 |
[32m[20230207 15:04:51 @agent_ppo2.py:192][0m |           0.0002 |           1.3424 |           0.0814 |
[32m[20230207 15:04:51 @agent_ppo2.py:192][0m |           0.0090 |           1.3474 |           0.0814 |
[32m[20230207 15:04:51 @agent_ppo2.py:192][0m |          -0.0211 |           1.3357 |           0.0813 |
[32m[20230207 15:04:51 @agent_ppo2.py:192][0m |          -0.0075 |           1.3275 |           0.0814 |
[32m[20230207 15:04:51 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: -110.93
[32m[20230207 15:04:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -110.18
[32m[20230207 15:04:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -125.51
[32m[20230207 15:04:52 @agent_ppo2.py:150][0m Total time:       4.18 min
[32m[20230207 15:04:52 @agent_ppo2.py:152][0m 219136 total steps have happened
[32m[20230207 15:04:52 @agent_ppo2.py:128][0m #------------------------ Iteration 107 --------------------------#
[32m[20230207 15:04:53 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:04:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:53 @agent_ppo2.py:192][0m |           0.0023 |          24.8177 |           0.0827 |
[32m[20230207 15:04:53 @agent_ppo2.py:192][0m |          -0.0052 |          10.6210 |           0.0826 |
[32m[20230207 15:04:53 @agent_ppo2.py:192][0m |          -0.0005 |           5.3987 |           0.0826 |
[32m[20230207 15:04:53 @agent_ppo2.py:192][0m |          -0.0076 |           3.8817 |           0.0825 |
[32m[20230207 15:04:53 @agent_ppo2.py:192][0m |          -0.0067 |           2.7887 |           0.0825 |
[32m[20230207 15:04:53 @agent_ppo2.py:192][0m |          -0.0103 |           2.5033 |           0.0824 |
[32m[20230207 15:04:53 @agent_ppo2.py:192][0m |          -0.0096 |           2.2828 |           0.0824 |
[32m[20230207 15:04:53 @agent_ppo2.py:192][0m |          -0.0102 |           2.1716 |           0.0823 |
[32m[20230207 15:04:53 @agent_ppo2.py:192][0m |          -0.0134 |           2.1079 |           0.0823 |
[32m[20230207 15:04:53 @agent_ppo2.py:192][0m |          -0.0119 |           2.0128 |           0.0823 |
[32m[20230207 15:04:53 @agent_ppo2.py:137][0m Policy update time: 0.53 s
[32m[20230207 15:04:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: -134.90
[32m[20230207 15:04:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.75
[32m[20230207 15:04:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -111.60
[32m[20230207 15:04:54 @agent_ppo2.py:150][0m Total time:       4.22 min
[32m[20230207 15:04:54 @agent_ppo2.py:152][0m 221184 total steps have happened
[32m[20230207 15:04:54 @agent_ppo2.py:128][0m #------------------------ Iteration 108 --------------------------#
[32m[20230207 15:04:55 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:04:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:55 @agent_ppo2.py:192][0m |          -0.0012 |           6.6122 |           0.0842 |
[32m[20230207 15:04:55 @agent_ppo2.py:192][0m |          -0.0066 |           2.8819 |           0.0841 |
[32m[20230207 15:04:55 @agent_ppo2.py:192][0m |          -0.0074 |           2.5288 |           0.0840 |
[32m[20230207 15:04:55 @agent_ppo2.py:192][0m |          -0.0078 |           2.4968 |           0.0839 |
[32m[20230207 15:04:55 @agent_ppo2.py:192][0m |          -0.0085 |           2.3802 |           0.0839 |
[32m[20230207 15:04:56 @agent_ppo2.py:192][0m |          -0.0065 |           2.9150 |           0.0839 |
[32m[20230207 15:04:56 @agent_ppo2.py:192][0m |          -0.0096 |           2.6604 |           0.0839 |
[32m[20230207 15:04:56 @agent_ppo2.py:192][0m |          -0.0097 |           2.2868 |           0.0839 |
[32m[20230207 15:04:56 @agent_ppo2.py:192][0m |          -0.0102 |           2.2646 |           0.0839 |
[32m[20230207 15:04:56 @agent_ppo2.py:192][0m |          -0.0093 |           2.2699 |           0.0839 |
[32m[20230207 15:04:56 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:04:57 @agent_ppo2.py:145][0m Average TRAINING episode reward: -110.14
[32m[20230207 15:04:57 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -82.93
[32m[20230207 15:04:57 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -102.62
[32m[20230207 15:04:57 @agent_ppo2.py:150][0m Total time:       4.26 min
[32m[20230207 15:04:57 @agent_ppo2.py:152][0m 223232 total steps have happened
[32m[20230207 15:04:57 @agent_ppo2.py:128][0m #------------------------ Iteration 109 --------------------------#
[32m[20230207 15:04:57 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:04:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:04:58 @agent_ppo2.py:192][0m |           0.0016 |           1.9472 |           0.0825 |
[32m[20230207 15:04:58 @agent_ppo2.py:192][0m |           0.0215 |           1.7396 |           0.0825 |
[32m[20230207 15:04:58 @agent_ppo2.py:192][0m |          -0.0274 |           1.6482 |           0.0826 |
[32m[20230207 15:04:58 @agent_ppo2.py:192][0m |          -0.0068 |           1.5966 |           0.0826 |
[32m[20230207 15:04:58 @agent_ppo2.py:192][0m |           0.0009 |           1.5678 |           0.0827 |
[32m[20230207 15:04:58 @agent_ppo2.py:192][0m |          -0.0181 |           1.5480 |           0.0827 |
[32m[20230207 15:04:58 @agent_ppo2.py:192][0m |          -0.0089 |           1.5367 |           0.0827 |
[32m[20230207 15:04:58 @agent_ppo2.py:192][0m |          -0.0137 |           1.5165 |           0.0828 |
[32m[20230207 15:04:58 @agent_ppo2.py:192][0m |          -0.0070 |           1.4998 |           0.0827 |
[32m[20230207 15:04:58 @agent_ppo2.py:192][0m |           0.0018 |           1.4898 |           0.0827 |
[32m[20230207 15:04:58 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:04:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.93
[32m[20230207 15:04:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.35
[32m[20230207 15:04:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -95.68
[32m[20230207 15:04:59 @agent_ppo2.py:150][0m Total time:       4.30 min
[32m[20230207 15:04:59 @agent_ppo2.py:152][0m 225280 total steps have happened
[32m[20230207 15:04:59 @agent_ppo2.py:128][0m #------------------------ Iteration 110 --------------------------#
[32m[20230207 15:05:00 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:05:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:00 @agent_ppo2.py:192][0m |           0.0001 |           5.4008 |           0.0858 |
[32m[20230207 15:05:00 @agent_ppo2.py:192][0m |          -0.0013 |           3.5693 |           0.0859 |
[32m[20230207 15:05:00 @agent_ppo2.py:192][0m |          -0.0034 |           3.1529 |           0.0859 |
[32m[20230207 15:05:00 @agent_ppo2.py:192][0m |          -0.0040 |           2.9876 |           0.0859 |
[32m[20230207 15:05:00 @agent_ppo2.py:192][0m |          -0.0050 |           2.8916 |           0.0859 |
[32m[20230207 15:05:00 @agent_ppo2.py:192][0m |          -0.0053 |           2.8059 |           0.0859 |
[32m[20230207 15:05:00 @agent_ppo2.py:192][0m |          -0.0057 |           2.6999 |           0.0859 |
[32m[20230207 15:05:01 @agent_ppo2.py:192][0m |          -0.0063 |           2.6459 |           0.0860 |
[32m[20230207 15:05:01 @agent_ppo2.py:192][0m |          -0.0070 |           2.6033 |           0.0860 |
[32m[20230207 15:05:01 @agent_ppo2.py:192][0m |          -0.0074 |           2.5667 |           0.0860 |
[32m[20230207 15:05:01 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:05:02 @agent_ppo2.py:145][0m Average TRAINING episode reward: -113.81
[32m[20230207 15:05:02 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.30
[32m[20230207 15:05:02 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -117.04
[32m[20230207 15:05:02 @agent_ppo2.py:150][0m Total time:       4.34 min
[32m[20230207 15:05:02 @agent_ppo2.py:152][0m 227328 total steps have happened
[32m[20230207 15:05:02 @agent_ppo2.py:128][0m #------------------------ Iteration 111 --------------------------#
[32m[20230207 15:05:02 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:05:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:03 @agent_ppo2.py:192][0m |           0.0019 |           1.8385 |           0.0849 |
[32m[20230207 15:05:03 @agent_ppo2.py:192][0m |          -0.0000 |           1.3831 |           0.0849 |
[32m[20230207 15:05:03 @agent_ppo2.py:192][0m |           0.0037 |           1.2861 |           0.0849 |
[32m[20230207 15:05:03 @agent_ppo2.py:192][0m |          -0.0015 |           1.2486 |           0.0849 |
[32m[20230207 15:05:03 @agent_ppo2.py:192][0m |          -0.0093 |           1.2159 |           0.0849 |
[32m[20230207 15:05:03 @agent_ppo2.py:192][0m |          -0.0082 |           1.2053 |           0.0849 |
[32m[20230207 15:05:03 @agent_ppo2.py:192][0m |          -0.0008 |           1.1946 |           0.0850 |
[32m[20230207 15:05:03 @agent_ppo2.py:192][0m |          -0.0050 |           1.1751 |           0.0850 |
[32m[20230207 15:05:03 @agent_ppo2.py:192][0m |          -0.0059 |           1.1583 |           0.0850 |
[32m[20230207 15:05:03 @agent_ppo2.py:192][0m |           0.0055 |           1.1526 |           0.0851 |
[32m[20230207 15:05:03 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:05:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: -94.28
[32m[20230207 15:05:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -62.48
[32m[20230207 15:05:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -100.83
[32m[20230207 15:05:04 @agent_ppo2.py:150][0m Total time:       4.38 min
[32m[20230207 15:05:04 @agent_ppo2.py:152][0m 229376 total steps have happened
[32m[20230207 15:05:04 @agent_ppo2.py:128][0m #------------------------ Iteration 112 --------------------------#
[32m[20230207 15:05:05 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:05:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:05 @agent_ppo2.py:192][0m |          -0.0032 |           2.0237 |           0.0840 |
[32m[20230207 15:05:05 @agent_ppo2.py:192][0m |           0.0125 |           1.4297 |           0.0839 |
[32m[20230207 15:05:05 @agent_ppo2.py:192][0m |           0.0060 |           1.3726 |           0.0839 |
[32m[20230207 15:05:05 @agent_ppo2.py:192][0m |          -0.0014 |           1.3648 |           0.0840 |
[32m[20230207 15:05:05 @agent_ppo2.py:192][0m |          -0.0169 |           1.2852 |           0.0839 |
[32m[20230207 15:05:05 @agent_ppo2.py:192][0m |          -0.0168 |           1.2695 |           0.0840 |
[32m[20230207 15:05:05 @agent_ppo2.py:192][0m |          -0.0174 |           1.2499 |           0.0839 |
[32m[20230207 15:05:06 @agent_ppo2.py:192][0m |          -0.0006 |           1.2522 |           0.0839 |
[32m[20230207 15:05:06 @agent_ppo2.py:192][0m |          -0.0161 |           1.2342 |           0.0839 |
[32m[20230207 15:05:06 @agent_ppo2.py:192][0m |          -0.0901 |           1.2481 |           0.0839 |
[32m[20230207 15:05:06 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:05:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: -83.10
[32m[20230207 15:05:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -80.36
[32m[20230207 15:05:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -88.28
[32m[20230207 15:05:06 @agent_ppo2.py:150][0m Total time:       4.42 min
[32m[20230207 15:05:06 @agent_ppo2.py:152][0m 231424 total steps have happened
[32m[20230207 15:05:06 @agent_ppo2.py:128][0m #------------------------ Iteration 113 --------------------------#
[32m[20230207 15:05:07 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:05:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:07 @agent_ppo2.py:192][0m |          -0.0025 |           5.4108 |           0.0858 |
[32m[20230207 15:05:08 @agent_ppo2.py:192][0m |           0.0039 |           1.6978 |           0.0858 |
[32m[20230207 15:05:08 @agent_ppo2.py:192][0m |           0.0030 |           1.6380 |           0.0858 |
[32m[20230207 15:05:08 @agent_ppo2.py:192][0m |          -0.0053 |           1.5079 |           0.0857 |
[32m[20230207 15:05:08 @agent_ppo2.py:192][0m |           0.0027 |           1.6443 |           0.0857 |
[32m[20230207 15:05:08 @agent_ppo2.py:192][0m |          -0.0016 |           1.3890 |           0.0857 |
[32m[20230207 15:05:08 @agent_ppo2.py:192][0m |          -0.0061 |           1.3075 |           0.0856 |
[32m[20230207 15:05:08 @agent_ppo2.py:192][0m |          -0.0066 |           1.2636 |           0.0856 |
[32m[20230207 15:05:08 @agent_ppo2.py:192][0m |          -0.0040 |           1.2724 |           0.0856 |
[32m[20230207 15:05:08 @agent_ppo2.py:192][0m |          -0.0047 |           1.2756 |           0.0855 |
[32m[20230207 15:05:08 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:05:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: -123.03
[32m[20230207 15:05:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -111.20
[32m[20230207 15:05:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -105.92
[32m[20230207 15:05:09 @agent_ppo2.py:150][0m Total time:       4.46 min
[32m[20230207 15:05:09 @agent_ppo2.py:152][0m 233472 total steps have happened
[32m[20230207 15:05:09 @agent_ppo2.py:128][0m #------------------------ Iteration 114 --------------------------#
[32m[20230207 15:05:10 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:05:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:10 @agent_ppo2.py:192][0m |          -0.0006 |           7.3297 |           0.0833 |
[32m[20230207 15:05:10 @agent_ppo2.py:192][0m |          -0.0034 |           5.4534 |           0.0832 |
[32m[20230207 15:05:10 @agent_ppo2.py:192][0m |          -0.0061 |           4.4462 |           0.0831 |
[32m[20230207 15:05:10 @agent_ppo2.py:192][0m |          -0.0068 |           3.2674 |           0.0831 |
[32m[20230207 15:05:10 @agent_ppo2.py:192][0m |          -0.0047 |           2.7418 |           0.0830 |
[32m[20230207 15:05:10 @agent_ppo2.py:192][0m |           0.0030 |           2.5170 |           0.0829 |
[32m[20230207 15:05:10 @agent_ppo2.py:192][0m |          -0.0051 |           2.5636 |           0.0829 |
[32m[20230207 15:05:11 @agent_ppo2.py:192][0m |          -0.0096 |           2.1367 |           0.0829 |
[32m[20230207 15:05:11 @agent_ppo2.py:192][0m |          -0.0072 |           1.8854 |           0.0829 |
[32m[20230207 15:05:11 @agent_ppo2.py:192][0m |          -0.0157 |           1.8057 |           0.0829 |
[32m[20230207 15:05:11 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:05:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: -94.82
[32m[20230207 15:05:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -82.93
[32m[20230207 15:05:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -107.24
[32m[20230207 15:05:12 @agent_ppo2.py:150][0m Total time:       4.51 min
[32m[20230207 15:05:12 @agent_ppo2.py:152][0m 235520 total steps have happened
[32m[20230207 15:05:12 @agent_ppo2.py:128][0m #------------------------ Iteration 115 --------------------------#
[32m[20230207 15:05:12 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:05:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:12 @agent_ppo2.py:192][0m |           0.0019 |           1.1500 |           0.0819 |
[32m[20230207 15:05:13 @agent_ppo2.py:192][0m |          -0.0029 |           1.0838 |           0.0818 |
[32m[20230207 15:05:13 @agent_ppo2.py:192][0m |          -0.0048 |           1.0626 |           0.0817 |
[32m[20230207 15:05:13 @agent_ppo2.py:192][0m |          -0.0057 |           1.0520 |           0.0818 |
[32m[20230207 15:05:13 @agent_ppo2.py:192][0m |          -0.0043 |           1.0457 |           0.0818 |
[32m[20230207 15:05:13 @agent_ppo2.py:192][0m |           0.0006 |           1.0399 |           0.0818 |
[32m[20230207 15:05:13 @agent_ppo2.py:192][0m |          -0.0050 |           1.0372 |           0.0818 |
[32m[20230207 15:05:13 @agent_ppo2.py:192][0m |          -0.0090 |           1.0346 |           0.0819 |
[32m[20230207 15:05:13 @agent_ppo2.py:192][0m |          -0.0033 |           1.0297 |           0.0819 |
[32m[20230207 15:05:13 @agent_ppo2.py:192][0m |          -0.0035 |           1.0326 |           0.0819 |
[32m[20230207 15:05:13 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:05:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: -128.63
[32m[20230207 15:05:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -111.14
[32m[20230207 15:05:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -102.06
[32m[20230207 15:05:14 @agent_ppo2.py:150][0m Total time:       4.55 min
[32m[20230207 15:05:14 @agent_ppo2.py:152][0m 237568 total steps have happened
[32m[20230207 15:05:14 @agent_ppo2.py:128][0m #------------------------ Iteration 116 --------------------------#
[32m[20230207 15:05:15 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:05:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:15 @agent_ppo2.py:192][0m |           0.0122 |           1.3390 |           0.0821 |
[32m[20230207 15:05:15 @agent_ppo2.py:192][0m |          -0.0091 |           1.2159 |           0.0820 |
[32m[20230207 15:05:15 @agent_ppo2.py:192][0m |          -0.0035 |           1.1783 |           0.0820 |
[32m[20230207 15:05:15 @agent_ppo2.py:192][0m |          -0.0072 |           1.1445 |           0.0820 |
[32m[20230207 15:05:15 @agent_ppo2.py:192][0m |          -0.0128 |           1.1148 |           0.0820 |
[32m[20230207 15:05:15 @agent_ppo2.py:192][0m |          -0.0203 |           1.1086 |           0.0820 |
[32m[20230207 15:05:15 @agent_ppo2.py:192][0m |          -0.0001 |           1.0775 |           0.0819 |
[32m[20230207 15:05:15 @agent_ppo2.py:192][0m |          -0.0308 |           1.0686 |           0.0819 |
[32m[20230207 15:05:16 @agent_ppo2.py:192][0m |          -0.0113 |           1.0432 |           0.0821 |
[32m[20230207 15:05:16 @agent_ppo2.py:192][0m |          -0.0098 |           1.0246 |           0.0821 |
[32m[20230207 15:05:16 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:05:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -110.92
[32m[20230207 15:05:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -99.81
[32m[20230207 15:05:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -88.02
[32m[20230207 15:05:16 @agent_ppo2.py:150][0m Total time:       4.59 min
[32m[20230207 15:05:16 @agent_ppo2.py:152][0m 239616 total steps have happened
[32m[20230207 15:05:16 @agent_ppo2.py:128][0m #------------------------ Iteration 117 --------------------------#
[32m[20230207 15:05:17 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:05:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:17 @agent_ppo2.py:192][0m |          -0.0011 |           1.3891 |           0.0831 |
[32m[20230207 15:05:17 @agent_ppo2.py:192][0m |           0.0012 |           1.3183 |           0.0830 |
[32m[20230207 15:05:18 @agent_ppo2.py:192][0m |          -0.0020 |           1.2781 |           0.0828 |
[32m[20230207 15:05:18 @agent_ppo2.py:192][0m |          -0.0123 |           1.2548 |           0.0827 |
[32m[20230207 15:05:18 @agent_ppo2.py:192][0m |          -0.0253 |           1.2661 |           0.0826 |
[32m[20230207 15:05:18 @agent_ppo2.py:192][0m |          -0.0086 |           1.2161 |           0.0825 |
[32m[20230207 15:05:18 @agent_ppo2.py:192][0m |           0.0049 |           1.2052 |           0.0825 |
[32m[20230207 15:05:18 @agent_ppo2.py:192][0m |          -0.0054 |           1.1920 |           0.0824 |
[32m[20230207 15:05:18 @agent_ppo2.py:192][0m |          -0.0142 |           1.1663 |           0.0823 |
[32m[20230207 15:05:18 @agent_ppo2.py:192][0m |          -0.0055 |           1.1566 |           0.0824 |
[32m[20230207 15:05:18 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:05:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: -118.11
[32m[20230207 15:05:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -107.93
[32m[20230207 15:05:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -56.65
[32m[20230207 15:05:19 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -56.65
[32m[20230207 15:05:19 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -56.65
[32m[20230207 15:05:19 @agent_ppo2.py:150][0m Total time:       4.63 min
[32m[20230207 15:05:19 @agent_ppo2.py:152][0m 241664 total steps have happened
[32m[20230207 15:05:19 @agent_ppo2.py:128][0m #------------------------ Iteration 118 --------------------------#
[32m[20230207 15:05:20 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:05:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:20 @agent_ppo2.py:192][0m |          -0.0005 |           4.8587 |           0.0844 |
[32m[20230207 15:05:20 @agent_ppo2.py:192][0m |          -0.0032 |           2.0219 |           0.0843 |
[32m[20230207 15:05:20 @agent_ppo2.py:192][0m |          -0.0041 |           1.5900 |           0.0843 |
[32m[20230207 15:05:20 @agent_ppo2.py:192][0m |          -0.0054 |           1.4736 |           0.0843 |
[32m[20230207 15:05:20 @agent_ppo2.py:192][0m |          -0.0047 |           1.3498 |           0.0843 |
[32m[20230207 15:05:20 @agent_ppo2.py:192][0m |          -0.0058 |           1.2666 |           0.0843 |
[32m[20230207 15:05:20 @agent_ppo2.py:192][0m |          -0.0067 |           1.2172 |           0.0843 |
[32m[20230207 15:05:20 @agent_ppo2.py:192][0m |          -0.0063 |           1.1910 |           0.0842 |
[32m[20230207 15:05:21 @agent_ppo2.py:192][0m |          -0.0075 |           1.1447 |           0.0842 |
[32m[20230207 15:05:21 @agent_ppo2.py:192][0m |          -0.0073 |           1.1222 |           0.0842 |
[32m[20230207 15:05:21 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:05:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -99.64
[32m[20230207 15:05:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -30.81
[32m[20230207 15:05:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -109.02
[32m[20230207 15:05:21 @agent_ppo2.py:150][0m Total time:       4.67 min
[32m[20230207 15:05:21 @agent_ppo2.py:152][0m 243712 total steps have happened
[32m[20230207 15:05:21 @agent_ppo2.py:128][0m #------------------------ Iteration 119 --------------------------#
[32m[20230207 15:05:22 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:05:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:22 @agent_ppo2.py:192][0m |          -0.0001 |           9.2238 |           0.0834 |
[32m[20230207 15:05:23 @agent_ppo2.py:192][0m |          -0.0029 |           3.8059 |           0.0834 |
[32m[20230207 15:05:23 @agent_ppo2.py:192][0m |          -0.0046 |           3.6626 |           0.0833 |
[32m[20230207 15:05:23 @agent_ppo2.py:192][0m |          -0.0056 |           3.1428 |           0.0833 |
[32m[20230207 15:05:23 @agent_ppo2.py:192][0m |          -0.0064 |           3.0819 |           0.0832 |
[32m[20230207 15:05:23 @agent_ppo2.py:192][0m |          -0.0069 |           3.0333 |           0.0832 |
[32m[20230207 15:05:23 @agent_ppo2.py:192][0m |          -0.0072 |           3.0758 |           0.0831 |
[32m[20230207 15:05:23 @agent_ppo2.py:192][0m |          -0.0079 |           2.8339 |           0.0831 |
[32m[20230207 15:05:23 @agent_ppo2.py:192][0m |          -0.0085 |           2.8119 |           0.0831 |
[32m[20230207 15:05:23 @agent_ppo2.py:192][0m |          -0.0086 |           2.7883 |           0.0830 |
[32m[20230207 15:05:23 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:05:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: -106.13
[32m[20230207 15:05:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -92.08
[32m[20230207 15:05:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -100.69
[32m[20230207 15:05:24 @agent_ppo2.py:150][0m Total time:       4.71 min
[32m[20230207 15:05:24 @agent_ppo2.py:152][0m 245760 total steps have happened
[32m[20230207 15:05:24 @agent_ppo2.py:128][0m #------------------------ Iteration 120 --------------------------#
[32m[20230207 15:05:25 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:05:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:25 @agent_ppo2.py:192][0m |          -0.0039 |           3.7484 |           0.0803 |
[32m[20230207 15:05:25 @agent_ppo2.py:192][0m |          -0.0013 |           1.9949 |           0.0802 |
[32m[20230207 15:05:25 @agent_ppo2.py:192][0m |          -0.0055 |           1.7480 |           0.0802 |
[32m[20230207 15:05:25 @agent_ppo2.py:192][0m |          -0.0030 |           1.6664 |           0.0801 |
[32m[20230207 15:05:25 @agent_ppo2.py:192][0m |          -0.0019 |           1.5924 |           0.0801 |
[32m[20230207 15:05:25 @agent_ppo2.py:192][0m |           0.0009 |           1.6671 |           0.0801 |
[32m[20230207 15:05:25 @agent_ppo2.py:192][0m |          -0.0086 |           1.5162 |           0.0800 |
[32m[20230207 15:05:26 @agent_ppo2.py:192][0m |          -0.0062 |           1.4822 |           0.0800 |
[32m[20230207 15:05:26 @agent_ppo2.py:192][0m |          -0.0048 |           1.4858 |           0.0800 |
[32m[20230207 15:05:26 @agent_ppo2.py:192][0m |          -0.0054 |           1.4794 |           0.0800 |
[32m[20230207 15:05:26 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:05:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -132.05
[32m[20230207 15:05:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -116.66
[32m[20230207 15:05:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -90.68
[32m[20230207 15:05:27 @agent_ppo2.py:150][0m Total time:       4.76 min
[32m[20230207 15:05:27 @agent_ppo2.py:152][0m 247808 total steps have happened
[32m[20230207 15:05:27 @agent_ppo2.py:128][0m #------------------------ Iteration 121 --------------------------#
[32m[20230207 15:05:27 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:05:27 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:28 @agent_ppo2.py:192][0m |          -0.0005 |           3.5678 |           0.0836 |
[32m[20230207 15:05:28 @agent_ppo2.py:192][0m |          -0.0030 |           1.4783 |           0.0836 |
[32m[20230207 15:05:28 @agent_ppo2.py:192][0m |          -0.0044 |           1.3082 |           0.0835 |
[32m[20230207 15:05:28 @agent_ppo2.py:192][0m |          -0.0062 |           1.2646 |           0.0834 |
[32m[20230207 15:05:28 @agent_ppo2.py:192][0m |          -0.0070 |           1.2307 |           0.0833 |
[32m[20230207 15:05:28 @agent_ppo2.py:192][0m |          -0.0072 |           1.1657 |           0.0832 |
[32m[20230207 15:05:28 @agent_ppo2.py:192][0m |          -0.0078 |           1.1641 |           0.0832 |
[32m[20230207 15:05:28 @agent_ppo2.py:192][0m |          -0.0085 |           1.1207 |           0.0831 |
[32m[20230207 15:05:28 @agent_ppo2.py:192][0m |          -0.0086 |           1.1108 |           0.0831 |
[32m[20230207 15:05:28 @agent_ppo2.py:192][0m |          -0.0089 |           1.1054 |           0.0830 |
[32m[20230207 15:05:28 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:05:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -96.55
[32m[20230207 15:05:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -71.11
[32m[20230207 15:05:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -140.35
[32m[20230207 15:05:29 @agent_ppo2.py:150][0m Total time:       4.80 min
[32m[20230207 15:05:29 @agent_ppo2.py:152][0m 249856 total steps have happened
[32m[20230207 15:05:29 @agent_ppo2.py:128][0m #------------------------ Iteration 122 --------------------------#
[32m[20230207 15:05:30 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:05:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:30 @agent_ppo2.py:192][0m |          -0.0013 |           1.6995 |           0.0803 |
[32m[20230207 15:05:30 @agent_ppo2.py:192][0m |          -0.0177 |           1.2903 |           0.0803 |
[32m[20230207 15:05:30 @agent_ppo2.py:192][0m |          -0.0027 |           1.1270 |           0.0804 |
[32m[20230207 15:05:30 @agent_ppo2.py:192][0m |           0.0065 |           1.0406 |           0.0804 |
[32m[20230207 15:05:30 @agent_ppo2.py:192][0m |          -0.0100 |           0.9838 |           0.0804 |
[32m[20230207 15:05:30 @agent_ppo2.py:192][0m |          -0.0049 |           0.9566 |           0.0805 |
[32m[20230207 15:05:30 @agent_ppo2.py:192][0m |           0.0049 |           0.9688 |           0.0805 |
[32m[20230207 15:05:30 @agent_ppo2.py:192][0m |          -0.0077 |           0.9352 |           0.0805 |
[32m[20230207 15:05:31 @agent_ppo2.py:192][0m |          -0.0308 |           0.9334 |           0.0805 |
[32m[20230207 15:05:31 @agent_ppo2.py:192][0m |          -0.0040 |           0.9284 |           0.0806 |
[32m[20230207 15:05:31 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:05:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -76.18
[32m[20230207 15:05:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -29.95
[32m[20230207 15:05:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -136.07
[32m[20230207 15:05:31 @agent_ppo2.py:150][0m Total time:       4.83 min
[32m[20230207 15:05:31 @agent_ppo2.py:152][0m 251904 total steps have happened
[32m[20230207 15:05:31 @agent_ppo2.py:128][0m #------------------------ Iteration 123 --------------------------#
[32m[20230207 15:05:32 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:05:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:32 @agent_ppo2.py:192][0m |          -0.0064 |          23.3952 |           0.0806 |
[32m[20230207 15:05:32 @agent_ppo2.py:192][0m |           0.0006 |          12.0676 |           0.0806 |
[32m[20230207 15:05:32 @agent_ppo2.py:192][0m |           0.0035 |          10.0873 |           0.0806 |
[32m[20230207 15:05:32 @agent_ppo2.py:192][0m |          -0.0321 |           9.3406 |           0.0805 |
[32m[20230207 15:05:32 @agent_ppo2.py:192][0m |          -0.0072 |           8.6833 |           0.0805 |
[32m[20230207 15:05:33 @agent_ppo2.py:192][0m |          -0.0482 |           8.2953 |           0.0805 |
[32m[20230207 15:05:33 @agent_ppo2.py:192][0m |          -0.0114 |           7.8732 |           0.0806 |
[32m[20230207 15:05:33 @agent_ppo2.py:192][0m |          -0.0077 |           7.4504 |           0.0807 |
[32m[20230207 15:05:33 @agent_ppo2.py:192][0m |           0.0021 |           7.2656 |           0.0807 |
[32m[20230207 15:05:33 @agent_ppo2.py:192][0m |          -0.0047 |           6.8950 |           0.0807 |
[32m[20230207 15:05:33 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:05:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: -74.60
[32m[20230207 15:05:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -43.29
[32m[20230207 15:05:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -76.60
[32m[20230207 15:05:34 @agent_ppo2.py:150][0m Total time:       4.87 min
[32m[20230207 15:05:34 @agent_ppo2.py:152][0m 253952 total steps have happened
[32m[20230207 15:05:34 @agent_ppo2.py:128][0m #------------------------ Iteration 124 --------------------------#
[32m[20230207 15:05:35 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:05:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:35 @agent_ppo2.py:192][0m |           0.0011 |           6.6895 |           0.0827 |
[32m[20230207 15:05:35 @agent_ppo2.py:192][0m |          -0.0002 |           3.2750 |           0.0827 |
[32m[20230207 15:05:35 @agent_ppo2.py:192][0m |          -0.0006 |           2.8046 |           0.0827 |
[32m[20230207 15:05:35 @agent_ppo2.py:192][0m |          -0.0037 |           2.5155 |           0.0827 |
[32m[20230207 15:05:35 @agent_ppo2.py:192][0m |          -0.0034 |           2.3744 |           0.0827 |
[32m[20230207 15:05:35 @agent_ppo2.py:192][0m |          -0.0049 |           2.2724 |           0.0827 |
[32m[20230207 15:05:35 @agent_ppo2.py:192][0m |          -0.0046 |           2.1649 |           0.0827 |
[32m[20230207 15:05:35 @agent_ppo2.py:192][0m |          -0.0044 |           2.0886 |           0.0827 |
[32m[20230207 15:05:35 @agent_ppo2.py:192][0m |          -0.0058 |           2.0502 |           0.0827 |
[32m[20230207 15:05:35 @agent_ppo2.py:192][0m |          -0.0057 |           1.9867 |           0.0827 |
[32m[20230207 15:05:35 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:05:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -101.97
[32m[20230207 15:05:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -75.95
[32m[20230207 15:05:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -65.24
[32m[20230207 15:05:36 @agent_ppo2.py:150][0m Total time:       4.92 min
[32m[20230207 15:05:36 @agent_ppo2.py:152][0m 256000 total steps have happened
[32m[20230207 15:05:36 @agent_ppo2.py:128][0m #------------------------ Iteration 125 --------------------------#
[32m[20230207 15:05:37 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:05:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:37 @agent_ppo2.py:192][0m |          -0.0008 |          38.1339 |           0.0838 |
[32m[20230207 15:05:37 @agent_ppo2.py:192][0m |          -0.0042 |          28.4347 |           0.0839 |
[32m[20230207 15:05:37 @agent_ppo2.py:192][0m |          -0.0068 |          13.9064 |           0.0839 |
[32m[20230207 15:05:37 @agent_ppo2.py:192][0m |          -0.0047 |           8.5738 |           0.0839 |
[32m[20230207 15:05:37 @agent_ppo2.py:192][0m |          -0.0059 |           6.1777 |           0.0839 |
[32m[20230207 15:05:37 @agent_ppo2.py:192][0m |          -0.0020 |           5.4325 |           0.0839 |
[32m[20230207 15:05:37 @agent_ppo2.py:192][0m |          -0.0087 |           4.5674 |           0.0839 |
[32m[20230207 15:05:37 @agent_ppo2.py:192][0m |          -0.0080 |           4.1634 |           0.0839 |
[32m[20230207 15:05:37 @agent_ppo2.py:192][0m |          -0.0111 |           3.9125 |           0.0839 |
[32m[20230207 15:05:37 @agent_ppo2.py:192][0m |          -0.0060 |           3.8725 |           0.0839 |
[32m[20230207 15:05:37 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:05:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: -106.69
[32m[20230207 15:05:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -85.99
[32m[20230207 15:05:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -103.54
[32m[20230207 15:05:38 @agent_ppo2.py:150][0m Total time:       4.95 min
[32m[20230207 15:05:38 @agent_ppo2.py:152][0m 258048 total steps have happened
[32m[20230207 15:05:38 @agent_ppo2.py:128][0m #------------------------ Iteration 126 --------------------------#
[32m[20230207 15:05:39 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:05:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:39 @agent_ppo2.py:192][0m |          -0.0003 |          22.2724 |           0.0863 |
[32m[20230207 15:05:39 @agent_ppo2.py:192][0m |          -0.0021 |           8.1081 |           0.0863 |
[32m[20230207 15:05:39 @agent_ppo2.py:192][0m |          -0.0037 |           6.2115 |           0.0862 |
[32m[20230207 15:05:39 @agent_ppo2.py:192][0m |          -0.0045 |           5.6123 |           0.0862 |
[32m[20230207 15:05:39 @agent_ppo2.py:192][0m |          -0.0044 |           5.3246 |           0.0862 |
[32m[20230207 15:05:39 @agent_ppo2.py:192][0m |          -0.0054 |           4.9402 |           0.0861 |
[32m[20230207 15:05:40 @agent_ppo2.py:192][0m |          -0.0010 |           4.7045 |           0.0861 |
[32m[20230207 15:05:40 @agent_ppo2.py:192][0m |          -0.0075 |           4.5541 |           0.0860 |
[32m[20230207 15:05:40 @agent_ppo2.py:192][0m |          -0.0064 |           4.4339 |           0.0860 |
[32m[20230207 15:05:40 @agent_ppo2.py:192][0m |          -0.0065 |           4.2780 |           0.0860 |
[32m[20230207 15:05:40 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:05:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -79.26
[32m[20230207 15:05:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -40.79
[32m[20230207 15:05:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -124.29
[32m[20230207 15:05:41 @agent_ppo2.py:150][0m Total time:       4.99 min
[32m[20230207 15:05:41 @agent_ppo2.py:152][0m 260096 total steps have happened
[32m[20230207 15:05:41 @agent_ppo2.py:128][0m #------------------------ Iteration 127 --------------------------#
[32m[20230207 15:05:42 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:05:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:42 @agent_ppo2.py:192][0m |           0.0072 |           7.2176 |           0.0828 |
[32m[20230207 15:05:42 @agent_ppo2.py:192][0m |          -0.0037 |           3.9118 |           0.0828 |
[32m[20230207 15:05:42 @agent_ppo2.py:192][0m |          -0.0061 |           3.4970 |           0.0827 |
[32m[20230207 15:05:42 @agent_ppo2.py:192][0m |          -0.0317 |           3.3086 |           0.0827 |
[32m[20230207 15:05:42 @agent_ppo2.py:192][0m |          -0.0079 |           3.2202 |           0.0826 |
[32m[20230207 15:05:42 @agent_ppo2.py:192][0m |          -0.0049 |           3.0113 |           0.0826 |
[32m[20230207 15:05:42 @agent_ppo2.py:192][0m |          -0.0088 |           2.9599 |           0.0826 |
[32m[20230207 15:05:42 @agent_ppo2.py:192][0m |          -0.0058 |           2.8652 |           0.0826 |
[32m[20230207 15:05:42 @agent_ppo2.py:192][0m |          -0.0263 |           2.8466 |           0.0825 |
[32m[20230207 15:05:42 @agent_ppo2.py:192][0m |          -0.0002 |           2.7685 |           0.0826 |
[32m[20230207 15:05:42 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:05:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -98.57
[32m[20230207 15:05:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -74.85
[32m[20230207 15:05:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -102.82
[32m[20230207 15:05:43 @agent_ppo2.py:150][0m Total time:       5.03 min
[32m[20230207 15:05:43 @agent_ppo2.py:152][0m 262144 total steps have happened
[32m[20230207 15:05:43 @agent_ppo2.py:128][0m #------------------------ Iteration 128 --------------------------#
[32m[20230207 15:05:44 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:05:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:44 @agent_ppo2.py:192][0m |          -0.0067 |           4.1481 |           0.0835 |
[32m[20230207 15:05:44 @agent_ppo2.py:192][0m |          -0.0047 |           1.6956 |           0.0834 |
[32m[20230207 15:05:44 @agent_ppo2.py:192][0m |          -0.0035 |           1.5252 |           0.0833 |
[32m[20230207 15:05:44 @agent_ppo2.py:192][0m |          -0.0056 |           1.4046 |           0.0833 |
[32m[20230207 15:05:44 @agent_ppo2.py:192][0m |           0.0001 |           1.3651 |           0.0832 |
[32m[20230207 15:05:44 @agent_ppo2.py:192][0m |          -0.0160 |           1.5858 |           0.0831 |
[32m[20230207 15:05:45 @agent_ppo2.py:192][0m |          -0.0121 |           1.3462 |           0.0831 |
[32m[20230207 15:05:45 @agent_ppo2.py:192][0m |          -0.0045 |           1.3016 |           0.0830 |
[32m[20230207 15:05:45 @agent_ppo2.py:192][0m |          -0.0128 |           1.2627 |           0.0829 |
[32m[20230207 15:05:45 @agent_ppo2.py:192][0m |          -0.0078 |           1.2361 |           0.0828 |
[32m[20230207 15:05:45 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:05:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -89.85
[32m[20230207 15:05:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -73.00
[32m[20230207 15:05:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -81.48
[32m[20230207 15:05:46 @agent_ppo2.py:150][0m Total time:       5.07 min
[32m[20230207 15:05:46 @agent_ppo2.py:152][0m 264192 total steps have happened
[32m[20230207 15:05:46 @agent_ppo2.py:128][0m #------------------------ Iteration 129 --------------------------#
[32m[20230207 15:05:46 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:05:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:47 @agent_ppo2.py:192][0m |          -0.0008 |           3.8142 |           0.0816 |
[32m[20230207 15:05:47 @agent_ppo2.py:192][0m |          -0.0067 |           1.9777 |           0.0816 |
[32m[20230207 15:05:47 @agent_ppo2.py:192][0m |          -0.0059 |           1.7155 |           0.0816 |
[32m[20230207 15:05:47 @agent_ppo2.py:192][0m |          -0.0064 |           1.6163 |           0.0816 |
[32m[20230207 15:05:47 @agent_ppo2.py:192][0m |          -0.0074 |           1.5378 |           0.0815 |
[32m[20230207 15:05:47 @agent_ppo2.py:192][0m |          -0.0067 |           1.4573 |           0.0815 |
[32m[20230207 15:05:47 @agent_ppo2.py:192][0m |          -0.0098 |           1.4194 |           0.0815 |
[32m[20230207 15:05:47 @agent_ppo2.py:192][0m |          -0.0093 |           1.3956 |           0.0815 |
[32m[20230207 15:05:47 @agent_ppo2.py:192][0m |          -0.0098 |           1.3209 |           0.0814 |
[32m[20230207 15:05:47 @agent_ppo2.py:192][0m |          -0.0093 |           1.3169 |           0.0814 |
[32m[20230207 15:05:47 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:05:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -99.30
[32m[20230207 15:05:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -92.09
[32m[20230207 15:05:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -63.44
[32m[20230207 15:05:48 @agent_ppo2.py:150][0m Total time:       5.11 min
[32m[20230207 15:05:48 @agent_ppo2.py:152][0m 266240 total steps have happened
[32m[20230207 15:05:48 @agent_ppo2.py:128][0m #------------------------ Iteration 130 --------------------------#
[32m[20230207 15:05:49 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:05:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:49 @agent_ppo2.py:192][0m |          -0.0023 |           3.3515 |           0.0832 |
[32m[20230207 15:05:49 @agent_ppo2.py:192][0m |          -0.0042 |           1.9896 |           0.0832 |
[32m[20230207 15:05:49 @agent_ppo2.py:192][0m |          -0.0047 |           1.8439 |           0.0832 |
[32m[20230207 15:05:49 @agent_ppo2.py:192][0m |          -0.0072 |           1.6515 |           0.0832 |
[32m[20230207 15:05:49 @agent_ppo2.py:192][0m |          -0.0087 |           1.6115 |           0.0832 |
[32m[20230207 15:05:49 @agent_ppo2.py:192][0m |          -0.0089 |           1.5477 |           0.0832 |
[32m[20230207 15:05:49 @agent_ppo2.py:192][0m |          -0.0088 |           1.5006 |           0.0831 |
[32m[20230207 15:05:50 @agent_ppo2.py:192][0m |          -0.0090 |           1.3950 |           0.0831 |
[32m[20230207 15:05:50 @agent_ppo2.py:192][0m |          -0.0099 |           1.4642 |           0.0831 |
[32m[20230207 15:05:50 @agent_ppo2.py:192][0m |          -0.0106 |           1.3136 |           0.0831 |
[32m[20230207 15:05:50 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:05:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: -81.23
[32m[20230207 15:05:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -62.44
[32m[20230207 15:05:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -96.68
[32m[20230207 15:05:51 @agent_ppo2.py:150][0m Total time:       5.16 min
[32m[20230207 15:05:51 @agent_ppo2.py:152][0m 268288 total steps have happened
[32m[20230207 15:05:51 @agent_ppo2.py:128][0m #------------------------ Iteration 131 --------------------------#
[32m[20230207 15:05:51 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:05:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:52 @agent_ppo2.py:192][0m |          -0.0455 |          21.8682 |           0.0811 |
[32m[20230207 15:05:52 @agent_ppo2.py:192][0m |          -0.0072 |          13.6799 |           0.0810 |
[32m[20230207 15:05:52 @agent_ppo2.py:192][0m |           0.0135 |           9.8858 |           0.0810 |
[32m[20230207 15:05:52 @agent_ppo2.py:192][0m |          -0.0141 |           9.2455 |           0.0810 |
[32m[20230207 15:05:52 @agent_ppo2.py:192][0m |          -0.0103 |           8.9435 |           0.0810 |
[32m[20230207 15:05:52 @agent_ppo2.py:192][0m |          -0.0082 |           8.7073 |           0.0811 |
[32m[20230207 15:05:52 @agent_ppo2.py:192][0m |          -0.0028 |           8.5929 |           0.0811 |
[32m[20230207 15:05:52 @agent_ppo2.py:192][0m |          -0.0081 |           8.4412 |           0.0811 |
[32m[20230207 15:05:52 @agent_ppo2.py:192][0m |          -0.0071 |           8.3254 |           0.0812 |
[32m[20230207 15:05:52 @agent_ppo2.py:192][0m |          -0.0096 |           8.2746 |           0.0811 |
[32m[20230207 15:05:52 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:05:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -111.40
[32m[20230207 15:05:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -84.58
[32m[20230207 15:05:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -101.93
[32m[20230207 15:05:53 @agent_ppo2.py:150][0m Total time:       5.20 min
[32m[20230207 15:05:53 @agent_ppo2.py:152][0m 270336 total steps have happened
[32m[20230207 15:05:53 @agent_ppo2.py:128][0m #------------------------ Iteration 132 --------------------------#
[32m[20230207 15:05:54 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:05:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:54 @agent_ppo2.py:192][0m |          -0.0007 |           9.0282 |           0.0816 |
[32m[20230207 15:05:54 @agent_ppo2.py:192][0m |           0.0039 |           3.9077 |           0.0815 |
[32m[20230207 15:05:54 @agent_ppo2.py:192][0m |          -0.0082 |           3.2008 |           0.0814 |
[32m[20230207 15:05:54 @agent_ppo2.py:192][0m |          -0.0095 |           2.9467 |           0.0814 |
[32m[20230207 15:05:54 @agent_ppo2.py:192][0m |          -0.0065 |           2.8011 |           0.0813 |
[32m[20230207 15:05:55 @agent_ppo2.py:192][0m |          -0.0099 |           2.6250 |           0.0813 |
[32m[20230207 15:05:55 @agent_ppo2.py:192][0m |          -0.0036 |           2.5235 |           0.0812 |
[32m[20230207 15:05:55 @agent_ppo2.py:192][0m |          -0.0055 |           2.4622 |           0.0811 |
[32m[20230207 15:05:55 @agent_ppo2.py:192][0m |           0.0071 |           2.4310 |           0.0812 |
[32m[20230207 15:05:55 @agent_ppo2.py:192][0m |          -0.0140 |           2.4757 |           0.0811 |
[32m[20230207 15:05:55 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:05:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: -105.64
[32m[20230207 15:05:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -73.91
[32m[20230207 15:05:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -59.47
[32m[20230207 15:05:56 @agent_ppo2.py:150][0m Total time:       5.24 min
[32m[20230207 15:05:56 @agent_ppo2.py:152][0m 272384 total steps have happened
[32m[20230207 15:05:56 @agent_ppo2.py:128][0m #------------------------ Iteration 133 --------------------------#
[32m[20230207 15:05:56 @agent_ppo2.py:134][0m Sampling time: 0.54 s by 1 slaves
[32m[20230207 15:05:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:56 @agent_ppo2.py:192][0m |          -0.0014 |          24.0878 |           0.0816 |
[32m[20230207 15:05:56 @agent_ppo2.py:192][0m |           0.0059 |          15.5766 |           0.0816 |
[32m[20230207 15:05:56 @agent_ppo2.py:192][0m |          -0.0130 |          13.5739 |           0.0816 |
[32m[20230207 15:05:56 @agent_ppo2.py:192][0m |          -0.0069 |          12.5516 |           0.0816 |
[32m[20230207 15:05:57 @agent_ppo2.py:192][0m |          -0.0145 |          11.7296 |           0.0817 |
[32m[20230207 15:05:57 @agent_ppo2.py:192][0m |          -0.0107 |          10.9802 |           0.0816 |
[32m[20230207 15:05:57 @agent_ppo2.py:192][0m |          -0.0090 |          10.7512 |           0.0816 |
[32m[20230207 15:05:57 @agent_ppo2.py:192][0m |          -0.0152 |          10.4753 |           0.0817 |
[32m[20230207 15:05:57 @agent_ppo2.py:192][0m |          -0.0183 |           9.9650 |           0.0817 |
[32m[20230207 15:05:57 @agent_ppo2.py:192][0m |          -0.0187 |           9.8322 |           0.0817 |
[32m[20230207 15:05:57 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:05:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: -68.53
[32m[20230207 15:05:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -9.09
[32m[20230207 15:05:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -46.43
[32m[20230207 15:05:58 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -46.43
[32m[20230207 15:05:58 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -46.43
[32m[20230207 15:05:58 @agent_ppo2.py:150][0m Total time:       5.27 min
[32m[20230207 15:05:58 @agent_ppo2.py:152][0m 274432 total steps have happened
[32m[20230207 15:05:58 @agent_ppo2.py:128][0m #------------------------ Iteration 134 --------------------------#
[32m[20230207 15:05:58 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:05:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:05:59 @agent_ppo2.py:192][0m |           0.0112 |           1.7231 |           0.0830 |
[32m[20230207 15:05:59 @agent_ppo2.py:192][0m |           0.0005 |           1.3765 |           0.0829 |
[32m[20230207 15:05:59 @agent_ppo2.py:192][0m |          -0.0035 |           1.2724 |           0.0829 |
[32m[20230207 15:05:59 @agent_ppo2.py:192][0m |          -0.0057 |           1.2120 |           0.0829 |
[32m[20230207 15:05:59 @agent_ppo2.py:192][0m |          -0.0113 |           1.1727 |           0.0829 |
[32m[20230207 15:05:59 @agent_ppo2.py:192][0m |          -0.0076 |           1.1396 |           0.0829 |
[32m[20230207 15:05:59 @agent_ppo2.py:192][0m |          -0.0086 |           1.1139 |           0.0829 |
[32m[20230207 15:05:59 @agent_ppo2.py:192][0m |          -0.0031 |           1.0856 |           0.0829 |
[32m[20230207 15:05:59 @agent_ppo2.py:192][0m |          -0.0101 |           1.0766 |           0.0829 |
[32m[20230207 15:05:59 @agent_ppo2.py:192][0m |           0.0000 |           1.0596 |           0.0829 |
[32m[20230207 15:05:59 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:06:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: -94.26
[32m[20230207 15:06:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -72.33
[32m[20230207 15:06:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -106.70
[32m[20230207 15:06:00 @agent_ppo2.py:150][0m Total time:       5.31 min
[32m[20230207 15:06:00 @agent_ppo2.py:152][0m 276480 total steps have happened
[32m[20230207 15:06:00 @agent_ppo2.py:128][0m #------------------------ Iteration 135 --------------------------#
[32m[20230207 15:06:01 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:06:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:01 @agent_ppo2.py:192][0m |           0.0258 |           1.3048 |           0.0830 |
[32m[20230207 15:06:01 @agent_ppo2.py:192][0m |          -0.0062 |           1.1692 |           0.0830 |
[32m[20230207 15:06:01 @agent_ppo2.py:192][0m |          -0.0106 |           1.1345 |           0.0829 |
[32m[20230207 15:06:01 @agent_ppo2.py:192][0m |          -0.0107 |           1.1136 |           0.0829 |
[32m[20230207 15:06:01 @agent_ppo2.py:192][0m |           0.0074 |           1.1011 |           0.0829 |
[32m[20230207 15:06:01 @agent_ppo2.py:192][0m |          -0.0324 |           1.1089 |           0.0830 |
[32m[20230207 15:06:01 @agent_ppo2.py:192][0m |          -0.0068 |           1.0782 |           0.0830 |
[32m[20230207 15:06:02 @agent_ppo2.py:192][0m |          -0.0152 |           1.0650 |           0.0830 |
[32m[20230207 15:06:02 @agent_ppo2.py:192][0m |           0.0014 |           1.0498 |           0.0830 |
[32m[20230207 15:06:02 @agent_ppo2.py:192][0m |           0.0063 |           1.0433 |           0.0830 |
[32m[20230207 15:06:02 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:06:02 @agent_ppo2.py:145][0m Average TRAINING episode reward: -45.56
[32m[20230207 15:06:02 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -42.86
[32m[20230207 15:06:02 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -94.99
[32m[20230207 15:06:02 @agent_ppo2.py:150][0m Total time:       5.35 min
[32m[20230207 15:06:02 @agent_ppo2.py:152][0m 278528 total steps have happened
[32m[20230207 15:06:02 @agent_ppo2.py:128][0m #------------------------ Iteration 136 --------------------------#
[32m[20230207 15:06:03 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:06:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:03 @agent_ppo2.py:192][0m |          -0.0005 |          13.9100 |           0.0836 |
[32m[20230207 15:06:04 @agent_ppo2.py:192][0m |          -0.0056 |           5.4501 |           0.0835 |
[32m[20230207 15:06:04 @agent_ppo2.py:192][0m |          -0.0041 |           4.3033 |           0.0835 |
[32m[20230207 15:06:04 @agent_ppo2.py:192][0m |          -0.0048 |           4.3339 |           0.0834 |
[32m[20230207 15:06:04 @agent_ppo2.py:192][0m |          -0.0072 |           3.7088 |           0.0834 |
[32m[20230207 15:06:04 @agent_ppo2.py:192][0m |          -0.0037 |           3.4656 |           0.0833 |
[32m[20230207 15:06:04 @agent_ppo2.py:192][0m |          -0.0093 |           3.3657 |           0.0833 |
[32m[20230207 15:06:04 @agent_ppo2.py:192][0m |          -0.0079 |           3.0643 |           0.0832 |
[32m[20230207 15:06:04 @agent_ppo2.py:192][0m |          -0.0085 |           3.0011 |           0.0832 |
[32m[20230207 15:06:04 @agent_ppo2.py:192][0m |          -0.0086 |           2.8851 |           0.0832 |
[32m[20230207 15:06:04 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:06:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -77.19
[32m[20230207 15:06:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -37.05
[32m[20230207 15:06:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -107.61
[32m[20230207 15:06:05 @agent_ppo2.py:150][0m Total time:       5.40 min
[32m[20230207 15:06:05 @agent_ppo2.py:152][0m 280576 total steps have happened
[32m[20230207 15:06:05 @agent_ppo2.py:128][0m #------------------------ Iteration 137 --------------------------#
[32m[20230207 15:06:06 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:06:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:06 @agent_ppo2.py:192][0m |           0.0094 |           1.2499 |           0.0837 |
[32m[20230207 15:06:06 @agent_ppo2.py:192][0m |           0.0061 |           1.0066 |           0.0837 |
[32m[20230207 15:06:06 @agent_ppo2.py:192][0m |           0.0013 |           0.9853 |           0.0836 |
[32m[20230207 15:06:06 @agent_ppo2.py:192][0m |          -0.1031 |           2.1242 |           0.0836 |
[32m[20230207 15:06:06 @agent_ppo2.py:192][0m |          -0.0224 |           1.1809 |           0.0837 |
[32m[20230207 15:06:06 @agent_ppo2.py:192][0m |          -0.0049 |           0.9431 |           0.0836 |
[32m[20230207 15:06:06 @agent_ppo2.py:192][0m |          -0.0097 |           0.9337 |           0.0836 |
[32m[20230207 15:06:06 @agent_ppo2.py:192][0m |           0.0015 |           0.9258 |           0.0836 |
[32m[20230207 15:06:07 @agent_ppo2.py:192][0m |           0.0040 |           0.9316 |           0.0836 |
[32m[20230207 15:06:07 @agent_ppo2.py:192][0m |          -0.0071 |           0.9210 |           0.0836 |
[32m[20230207 15:06:07 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:06:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -73.05
[32m[20230207 15:06:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -6.70
[32m[20230207 15:06:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -88.51
[32m[20230207 15:06:07 @agent_ppo2.py:150][0m Total time:       5.44 min
[32m[20230207 15:06:07 @agent_ppo2.py:152][0m 282624 total steps have happened
[32m[20230207 15:06:07 @agent_ppo2.py:128][0m #------------------------ Iteration 138 --------------------------#
[32m[20230207 15:06:08 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:06:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:08 @agent_ppo2.py:192][0m |           0.0053 |           2.0775 |           0.0814 |
[32m[20230207 15:06:09 @agent_ppo2.py:192][0m |          -0.0044 |           1.9095 |           0.0814 |
[32m[20230207 15:06:09 @agent_ppo2.py:192][0m |          -0.0022 |           1.8821 |           0.0814 |
[32m[20230207 15:06:09 @agent_ppo2.py:192][0m |          -0.0006 |           1.8583 |           0.0814 |
[32m[20230207 15:06:09 @agent_ppo2.py:192][0m |           0.0004 |           1.8382 |           0.0815 |
[32m[20230207 15:06:09 @agent_ppo2.py:192][0m |          -0.0031 |           1.8246 |           0.0814 |
[32m[20230207 15:06:09 @agent_ppo2.py:192][0m |          -0.0087 |           1.8174 |           0.0814 |
[32m[20230207 15:06:09 @agent_ppo2.py:192][0m |          -0.0011 |           1.8073 |           0.0815 |
[32m[20230207 15:06:09 @agent_ppo2.py:192][0m |           0.0013 |           1.8177 |           0.0814 |
[32m[20230207 15:06:09 @agent_ppo2.py:192][0m |          -0.0066 |           1.7941 |           0.0814 |
[32m[20230207 15:06:09 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:06:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -131.49
[32m[20230207 15:06:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.94
[32m[20230207 15:06:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -91.18
[32m[20230207 15:06:10 @agent_ppo2.py:150][0m Total time:       5.48 min
[32m[20230207 15:06:10 @agent_ppo2.py:152][0m 284672 total steps have happened
[32m[20230207 15:06:10 @agent_ppo2.py:128][0m #------------------------ Iteration 139 --------------------------#
[32m[20230207 15:06:11 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:06:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:11 @agent_ppo2.py:192][0m |          -0.0006 |          18.0447 |           0.0847 |
[32m[20230207 15:06:11 @agent_ppo2.py:192][0m |          -0.0032 |           3.7878 |           0.0847 |
[32m[20230207 15:06:11 @agent_ppo2.py:192][0m |           0.0018 |           2.5903 |           0.0848 |
[32m[20230207 15:06:11 @agent_ppo2.py:192][0m |          -0.0042 |           2.3296 |           0.0848 |
[32m[20230207 15:06:11 @agent_ppo2.py:192][0m |          -0.0060 |           2.1839 |           0.0848 |
[32m[20230207 15:06:11 @agent_ppo2.py:192][0m |          -0.0064 |           2.0346 |           0.0848 |
[32m[20230207 15:06:11 @agent_ppo2.py:192][0m |          -0.0088 |           1.9616 |           0.0848 |
[32m[20230207 15:06:11 @agent_ppo2.py:192][0m |          -0.0082 |           1.8777 |           0.0848 |
[32m[20230207 15:06:11 @agent_ppo2.py:192][0m |          -0.0074 |           1.8341 |           0.0848 |
[32m[20230207 15:06:11 @agent_ppo2.py:192][0m |          -0.0087 |           1.7937 |           0.0848 |
[32m[20230207 15:06:11 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:06:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: -115.17
[32m[20230207 15:06:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -87.19
[32m[20230207 15:06:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -100.57
[32m[20230207 15:06:12 @agent_ppo2.py:150][0m Total time:       5.52 min
[32m[20230207 15:06:12 @agent_ppo2.py:152][0m 286720 total steps have happened
[32m[20230207 15:06:12 @agent_ppo2.py:128][0m #------------------------ Iteration 140 --------------------------#
[32m[20230207 15:06:13 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:06:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:13 @agent_ppo2.py:192][0m |          -0.0005 |           7.1316 |           0.0858 |
[32m[20230207 15:06:14 @agent_ppo2.py:192][0m |          -0.0042 |           2.9802 |           0.0857 |
[32m[20230207 15:06:14 @agent_ppo2.py:192][0m |          -0.0032 |           2.5459 |           0.0857 |
[32m[20230207 15:06:14 @agent_ppo2.py:192][0m |          -0.0046 |           2.3663 |           0.0856 |
[32m[20230207 15:06:14 @agent_ppo2.py:192][0m |          -0.0063 |           2.1742 |           0.0856 |
[32m[20230207 15:06:14 @agent_ppo2.py:192][0m |          -0.0057 |           2.1015 |           0.0855 |
[32m[20230207 15:06:14 @agent_ppo2.py:192][0m |          -0.0073 |           2.0337 |           0.0855 |
[32m[20230207 15:06:14 @agent_ppo2.py:192][0m |          -0.0062 |           1.9489 |           0.0855 |
[32m[20230207 15:06:14 @agent_ppo2.py:192][0m |          -0.0077 |           1.8712 |           0.0854 |
[32m[20230207 15:06:14 @agent_ppo2.py:192][0m |          -0.0070 |           1.8797 |           0.0854 |
[32m[20230207 15:06:14 @agent_ppo2.py:137][0m Policy update time: 1.07 s
[32m[20230207 15:06:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: -91.92
[32m[20230207 15:06:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -64.83
[32m[20230207 15:06:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -62.03
[32m[20230207 15:06:15 @agent_ppo2.py:150][0m Total time:       5.56 min
[32m[20230207 15:06:15 @agent_ppo2.py:152][0m 288768 total steps have happened
[32m[20230207 15:06:15 @agent_ppo2.py:128][0m #------------------------ Iteration 141 --------------------------#
[32m[20230207 15:06:16 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:06:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:16 @agent_ppo2.py:192][0m |          -0.0005 |           5.0975 |           0.0818 |
[32m[20230207 15:06:16 @agent_ppo2.py:192][0m |          -0.0016 |           2.0426 |           0.0818 |
[32m[20230207 15:06:16 @agent_ppo2.py:192][0m |           0.0096 |           1.8933 |           0.0818 |
[32m[20230207 15:06:16 @agent_ppo2.py:192][0m |          -0.0041 |           1.7599 |           0.0819 |
[32m[20230207 15:06:16 @agent_ppo2.py:192][0m |          -0.0046 |           1.6853 |           0.0818 |
[32m[20230207 15:06:17 @agent_ppo2.py:192][0m |          -0.0056 |           1.6237 |           0.0818 |
[32m[20230207 15:06:17 @agent_ppo2.py:192][0m |          -0.0087 |           1.5856 |           0.0817 |
[32m[20230207 15:06:17 @agent_ppo2.py:192][0m |          -0.0099 |           1.5683 |           0.0817 |
[32m[20230207 15:06:17 @agent_ppo2.py:192][0m |          -0.0096 |           1.5003 |           0.0817 |
[32m[20230207 15:06:17 @agent_ppo2.py:192][0m |          -0.0084 |           1.4717 |           0.0817 |
[32m[20230207 15:06:17 @agent_ppo2.py:137][0m Policy update time: 0.93 s
[32m[20230207 15:06:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -83.36
[32m[20230207 15:06:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -48.55
[32m[20230207 15:06:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -75.07
[32m[20230207 15:06:18 @agent_ppo2.py:150][0m Total time:       5.61 min
[32m[20230207 15:06:18 @agent_ppo2.py:152][0m 290816 total steps have happened
[32m[20230207 15:06:18 @agent_ppo2.py:128][0m #------------------------ Iteration 142 --------------------------#
[32m[20230207 15:06:19 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:06:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:19 @agent_ppo2.py:192][0m |           0.0031 |           1.7913 |           0.0830 |
[32m[20230207 15:06:19 @agent_ppo2.py:192][0m |          -0.0027 |           1.6876 |           0.0829 |
[32m[20230207 15:06:19 @agent_ppo2.py:192][0m |          -0.0011 |           1.6340 |           0.0829 |
[32m[20230207 15:06:19 @agent_ppo2.py:192][0m |          -0.0009 |           1.5844 |           0.0829 |
[32m[20230207 15:06:19 @agent_ppo2.py:192][0m |          -0.0049 |           1.5297 |           0.0830 |
[32m[20230207 15:06:19 @agent_ppo2.py:192][0m |          -0.0039 |           1.4868 |           0.0830 |
[32m[20230207 15:06:19 @agent_ppo2.py:192][0m |          -0.0172 |           1.4660 |           0.0830 |
[32m[20230207 15:06:19 @agent_ppo2.py:192][0m |           0.0033 |           1.4411 |           0.0830 |
[32m[20230207 15:06:19 @agent_ppo2.py:192][0m |          -0.0163 |           1.4125 |           0.0831 |
[32m[20230207 15:06:19 @agent_ppo2.py:192][0m |          -0.0167 |           1.3889 |           0.0831 |
[32m[20230207 15:06:19 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:06:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -87.06
[32m[20230207 15:06:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -73.95
[32m[20230207 15:06:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -85.39
[32m[20230207 15:06:20 @agent_ppo2.py:150][0m Total time:       5.65 min
[32m[20230207 15:06:20 @agent_ppo2.py:152][0m 292864 total steps have happened
[32m[20230207 15:06:20 @agent_ppo2.py:128][0m #------------------------ Iteration 143 --------------------------#
[32m[20230207 15:06:21 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:06:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:21 @agent_ppo2.py:192][0m |          -0.0034 |           4.6407 |           0.0842 |
[32m[20230207 15:06:21 @agent_ppo2.py:192][0m |           0.0057 |           3.4515 |           0.0842 |
[32m[20230207 15:06:21 @agent_ppo2.py:192][0m |           0.0060 |           3.2567 |           0.0842 |
[32m[20230207 15:06:22 @agent_ppo2.py:192][0m |          -0.0061 |           2.9544 |           0.0841 |
[32m[20230207 15:06:22 @agent_ppo2.py:192][0m |          -0.0036 |           2.8783 |           0.0842 |
[32m[20230207 15:06:22 @agent_ppo2.py:192][0m |           0.0019 |           2.8205 |           0.0841 |
[32m[20230207 15:06:22 @agent_ppo2.py:192][0m |           0.0073 |           2.7231 |           0.0841 |
[32m[20230207 15:06:22 @agent_ppo2.py:192][0m |          -0.0022 |           2.7090 |           0.0841 |
[32m[20230207 15:06:22 @agent_ppo2.py:192][0m |          -0.0275 |           2.6809 |           0.0841 |
[32m[20230207 15:06:22 @agent_ppo2.py:192][0m |          -0.0015 |           2.6192 |           0.0841 |
[32m[20230207 15:06:22 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:06:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -44.84
[32m[20230207 15:06:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -43.06
[32m[20230207 15:06:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -129.21
[32m[20230207 15:06:23 @agent_ppo2.py:150][0m Total time:       5.69 min
[32m[20230207 15:06:23 @agent_ppo2.py:152][0m 294912 total steps have happened
[32m[20230207 15:06:23 @agent_ppo2.py:128][0m #------------------------ Iteration 144 --------------------------#
[32m[20230207 15:06:24 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:06:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:24 @agent_ppo2.py:192][0m |           0.0162 |           1.7833 |           0.0841 |
[32m[20230207 15:06:24 @agent_ppo2.py:192][0m |           0.0032 |           1.5484 |           0.0841 |
[32m[20230207 15:06:24 @agent_ppo2.py:192][0m |           0.0095 |           1.4884 |           0.0841 |
[32m[20230207 15:06:24 @agent_ppo2.py:192][0m |          -0.0159 |           1.4658 |           0.0841 |
[32m[20230207 15:06:24 @agent_ppo2.py:192][0m |          -0.0074 |           1.4259 |           0.0842 |
[32m[20230207 15:06:24 @agent_ppo2.py:192][0m |          -0.0168 |           1.3920 |           0.0842 |
[32m[20230207 15:06:24 @agent_ppo2.py:192][0m |           0.0025 |           1.3645 |           0.0841 |
[32m[20230207 15:06:24 @agent_ppo2.py:192][0m |          -0.1002 |           2.8157 |           0.0842 |
[32m[20230207 15:06:24 @agent_ppo2.py:192][0m |          -0.0075 |           1.6349 |           0.0842 |
[32m[20230207 15:06:25 @agent_ppo2.py:192][0m |          -0.0109 |           1.3189 |           0.0842 |
[32m[20230207 15:06:25 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:06:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -90.33
[32m[20230207 15:06:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -54.87
[32m[20230207 15:06:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -33.80
[32m[20230207 15:06:25 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -33.80
[32m[20230207 15:06:25 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -33.80
[32m[20230207 15:06:25 @agent_ppo2.py:150][0m Total time:       5.74 min
[32m[20230207 15:06:25 @agent_ppo2.py:152][0m 296960 total steps have happened
[32m[20230207 15:06:25 @agent_ppo2.py:128][0m #------------------------ Iteration 145 --------------------------#
[32m[20230207 15:06:26 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:06:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:26 @agent_ppo2.py:192][0m |           0.0030 |           2.4317 |           0.0855 |
[32m[20230207 15:06:26 @agent_ppo2.py:192][0m |          -0.0004 |           2.0356 |           0.0855 |
[32m[20230207 15:06:26 @agent_ppo2.py:192][0m |          -0.0241 |           1.9587 |           0.0854 |
[32m[20230207 15:06:27 @agent_ppo2.py:192][0m |           0.0053 |           1.9070 |           0.0855 |
[32m[20230207 15:06:27 @agent_ppo2.py:192][0m |          -0.0299 |           1.8616 |           0.0855 |
[32m[20230207 15:06:27 @agent_ppo2.py:192][0m |          -0.0188 |           1.8338 |           0.0855 |
[32m[20230207 15:06:27 @agent_ppo2.py:192][0m |          -0.0059 |           1.8006 |           0.0856 |
[32m[20230207 15:06:27 @agent_ppo2.py:192][0m |          -0.0221 |           1.7854 |           0.0856 |
[32m[20230207 15:06:27 @agent_ppo2.py:192][0m |          -0.0067 |           1.7689 |           0.0856 |
[32m[20230207 15:06:27 @agent_ppo2.py:192][0m |          -0.0273 |           1.7302 |           0.0856 |
[32m[20230207 15:06:27 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:06:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: -90.10
[32m[20230207 15:06:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -55.29
[32m[20230207 15:06:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -44.86
[32m[20230207 15:06:28 @agent_ppo2.py:150][0m Total time:       5.78 min
[32m[20230207 15:06:28 @agent_ppo2.py:152][0m 299008 total steps have happened
[32m[20230207 15:06:28 @agent_ppo2.py:128][0m #------------------------ Iteration 146 --------------------------#
[32m[20230207 15:06:29 @agent_ppo2.py:134][0m Sampling time: 0.93 s by 1 slaves
[32m[20230207 15:06:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:29 @agent_ppo2.py:192][0m |          -0.0043 |           7.9177 |           0.0853 |
[32m[20230207 15:06:29 @agent_ppo2.py:192][0m |          -0.0088 |           3.0233 |           0.0852 |
[32m[20230207 15:06:29 @agent_ppo2.py:192][0m |          -0.0089 |           2.4935 |           0.0852 |
[32m[20230207 15:06:29 @agent_ppo2.py:192][0m |          -0.0011 |           2.3862 |           0.0852 |
[32m[20230207 15:06:29 @agent_ppo2.py:192][0m |          -0.0082 |           2.2466 |           0.0852 |
[32m[20230207 15:06:29 @agent_ppo2.py:192][0m |          -0.0112 |           1.9707 |           0.0852 |
[32m[20230207 15:06:29 @agent_ppo2.py:192][0m |          -0.0100 |           1.8944 |           0.0852 |
[32m[20230207 15:06:30 @agent_ppo2.py:192][0m |           0.0036 |           1.8219 |           0.0852 |
[32m[20230207 15:06:30 @agent_ppo2.py:192][0m |          -0.0131 |           1.7454 |           0.0852 |
[32m[20230207 15:06:30 @agent_ppo2.py:192][0m |          -0.0138 |           1.7132 |           0.0852 |
[32m[20230207 15:06:30 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:06:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -107.73
[32m[20230207 15:06:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -58.38
[32m[20230207 15:06:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -113.13
[32m[20230207 15:06:31 @agent_ppo2.py:150][0m Total time:       5.82 min
[32m[20230207 15:06:31 @agent_ppo2.py:152][0m 301056 total steps have happened
[32m[20230207 15:06:31 @agent_ppo2.py:128][0m #------------------------ Iteration 147 --------------------------#
[32m[20230207 15:06:31 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:06:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:32 @agent_ppo2.py:192][0m |          -0.0001 |          10.4851 |           0.0862 |
[32m[20230207 15:06:32 @agent_ppo2.py:192][0m |          -0.0038 |           4.5767 |           0.0862 |
[32m[20230207 15:06:32 @agent_ppo2.py:192][0m |          -0.0052 |           3.7430 |           0.0862 |
[32m[20230207 15:06:32 @agent_ppo2.py:192][0m |          -0.0067 |           3.4498 |           0.0862 |
[32m[20230207 15:06:32 @agent_ppo2.py:192][0m |          -0.0080 |           3.2477 |           0.0861 |
[32m[20230207 15:06:32 @agent_ppo2.py:192][0m |          -0.0083 |           3.0791 |           0.0861 |
[32m[20230207 15:06:32 @agent_ppo2.py:192][0m |          -0.0087 |           2.9770 |           0.0861 |
[32m[20230207 15:06:32 @agent_ppo2.py:192][0m |          -0.0090 |           2.9223 |           0.0861 |
[32m[20230207 15:06:32 @agent_ppo2.py:192][0m |          -0.0098 |           2.8609 |           0.0861 |
[32m[20230207 15:06:32 @agent_ppo2.py:192][0m |          -0.0098 |           2.7995 |           0.0861 |
[32m[20230207 15:06:32 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:06:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: -87.39
[32m[20230207 15:06:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -33.12
[32m[20230207 15:06:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -104.50
[32m[20230207 15:06:33 @agent_ppo2.py:150][0m Total time:       5.87 min
[32m[20230207 15:06:33 @agent_ppo2.py:152][0m 303104 total steps have happened
[32m[20230207 15:06:33 @agent_ppo2.py:128][0m #------------------------ Iteration 148 --------------------------#
[32m[20230207 15:06:34 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:06:34 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:34 @agent_ppo2.py:192][0m |           0.0006 |           4.0616 |           0.0862 |
[32m[20230207 15:06:34 @agent_ppo2.py:192][0m |          -0.0004 |           2.2346 |           0.0862 |
[32m[20230207 15:06:34 @agent_ppo2.py:192][0m |          -0.0032 |           2.0998 |           0.0861 |
[32m[20230207 15:06:34 @agent_ppo2.py:192][0m |          -0.0038 |           1.8309 |           0.0861 |
[32m[20230207 15:06:34 @agent_ppo2.py:192][0m |          -0.0045 |           1.7768 |           0.0861 |
[32m[20230207 15:06:34 @agent_ppo2.py:192][0m |          -0.0049 |           1.6229 |           0.0861 |
[32m[20230207 15:06:35 @agent_ppo2.py:192][0m |          -0.0057 |           1.6197 |           0.0861 |
[32m[20230207 15:06:35 @agent_ppo2.py:192][0m |          -0.0065 |           1.5828 |           0.0860 |
[32m[20230207 15:06:35 @agent_ppo2.py:192][0m |          -0.0059 |           1.5183 |           0.0860 |
[32m[20230207 15:06:35 @agent_ppo2.py:192][0m |          -0.0067 |           1.5191 |           0.0861 |
[32m[20230207 15:06:35 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:06:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -49.30
[32m[20230207 15:06:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 0.55
[32m[20230207 15:06:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -73.02
[32m[20230207 15:06:36 @agent_ppo2.py:150][0m Total time:       5.91 min
[32m[20230207 15:06:36 @agent_ppo2.py:152][0m 305152 total steps have happened
[32m[20230207 15:06:36 @agent_ppo2.py:128][0m #------------------------ Iteration 149 --------------------------#
[32m[20230207 15:06:36 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:06:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:37 @agent_ppo2.py:192][0m |          -0.0063 |           6.8003 |           0.0838 |
[32m[20230207 15:06:37 @agent_ppo2.py:192][0m |           0.0125 |           4.5857 |           0.0837 |
[32m[20230207 15:06:37 @agent_ppo2.py:192][0m |           0.0042 |           4.0921 |           0.0835 |
[32m[20230207 15:06:37 @agent_ppo2.py:192][0m |           0.0010 |           3.6981 |           0.0835 |
[32m[20230207 15:06:37 @agent_ppo2.py:192][0m |          -0.0126 |           3.5623 |           0.0835 |
[32m[20230207 15:06:37 @agent_ppo2.py:192][0m |          -0.0079 |           3.4402 |           0.0834 |
[32m[20230207 15:06:37 @agent_ppo2.py:192][0m |          -0.0245 |           3.4173 |           0.0834 |
[32m[20230207 15:06:37 @agent_ppo2.py:192][0m |          -0.0151 |           3.3729 |           0.0834 |
[32m[20230207 15:06:37 @agent_ppo2.py:192][0m |           0.0239 |           3.2793 |           0.0834 |
[32m[20230207 15:06:37 @agent_ppo2.py:192][0m |          -0.0005 |           3.2973 |           0.0833 |
[32m[20230207 15:06:37 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:06:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: -72.94
[32m[20230207 15:06:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -35.72
[32m[20230207 15:06:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -111.61
[32m[20230207 15:06:38 @agent_ppo2.py:150][0m Total time:       5.95 min
[32m[20230207 15:06:38 @agent_ppo2.py:152][0m 307200 total steps have happened
[32m[20230207 15:06:38 @agent_ppo2.py:128][0m #------------------------ Iteration 150 --------------------------#
[32m[20230207 15:06:39 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:06:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:39 @agent_ppo2.py:192][0m |           0.0049 |          22.6439 |           0.0851 |
[32m[20230207 15:06:39 @agent_ppo2.py:192][0m |          -0.0012 |          15.2829 |           0.0850 |
[32m[20230207 15:06:39 @agent_ppo2.py:192][0m |          -0.0078 |          11.5747 |           0.0849 |
[32m[20230207 15:06:39 @agent_ppo2.py:192][0m |          -0.0081 |           9.5662 |           0.0848 |
[32m[20230207 15:06:39 @agent_ppo2.py:192][0m |          -0.0102 |           7.5465 |           0.0848 |
[32m[20230207 15:06:39 @agent_ppo2.py:192][0m |          -0.0101 |           6.4388 |           0.0847 |
[32m[20230207 15:06:40 @agent_ppo2.py:192][0m |          -0.0102 |           5.8548 |           0.0847 |
[32m[20230207 15:06:40 @agent_ppo2.py:192][0m |          -0.0128 |           5.2282 |           0.0846 |
[32m[20230207 15:06:40 @agent_ppo2.py:192][0m |          -0.0109 |           4.6902 |           0.0846 |
[32m[20230207 15:06:40 @agent_ppo2.py:192][0m |          -0.0004 |           4.5141 |           0.0846 |
[32m[20230207 15:06:40 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:06:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -122.11
[32m[20230207 15:06:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -62.40
[32m[20230207 15:06:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -101.90
[32m[20230207 15:06:41 @agent_ppo2.py:150][0m Total time:       5.99 min
[32m[20230207 15:06:41 @agent_ppo2.py:152][0m 309248 total steps have happened
[32m[20230207 15:06:41 @agent_ppo2.py:128][0m #------------------------ Iteration 151 --------------------------#
[32m[20230207 15:06:41 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:06:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:42 @agent_ppo2.py:192][0m |          -0.1176 |           3.1913 |           0.0849 |
[32m[20230207 15:06:42 @agent_ppo2.py:192][0m |           0.0037 |           1.6337 |           0.0848 |
[32m[20230207 15:06:42 @agent_ppo2.py:192][0m |           0.0102 |           1.4280 |           0.0849 |
[32m[20230207 15:06:42 @agent_ppo2.py:192][0m |          -0.0026 |           1.3855 |           0.0849 |
[32m[20230207 15:06:42 @agent_ppo2.py:192][0m |          -0.0055 |           1.3495 |           0.0849 |
[32m[20230207 15:06:42 @agent_ppo2.py:192][0m |          -0.0966 |           2.2186 |           0.0849 |
[32m[20230207 15:06:42 @agent_ppo2.py:192][0m |           0.0130 |           1.3914 |           0.0846 |
[32m[20230207 15:06:42 @agent_ppo2.py:192][0m |          -0.0132 |           1.2914 |           0.0848 |
[32m[20230207 15:06:42 @agent_ppo2.py:192][0m |          -0.0029 |           1.2795 |           0.0849 |
[32m[20230207 15:06:42 @agent_ppo2.py:192][0m |           0.0046 |           1.2626 |           0.0849 |
[32m[20230207 15:06:42 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:06:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -82.35
[32m[20230207 15:06:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -71.66
[32m[20230207 15:06:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -77.82
[32m[20230207 15:06:43 @agent_ppo2.py:150][0m Total time:       6.03 min
[32m[20230207 15:06:43 @agent_ppo2.py:152][0m 311296 total steps have happened
[32m[20230207 15:06:43 @agent_ppo2.py:128][0m #------------------------ Iteration 152 --------------------------#
[32m[20230207 15:06:44 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:06:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:44 @agent_ppo2.py:192][0m |          -0.0007 |           3.1873 |           0.0872 |
[32m[20230207 15:06:44 @agent_ppo2.py:192][0m |          -0.0021 |           1.3258 |           0.0872 |
[32m[20230207 15:06:44 @agent_ppo2.py:192][0m |          -0.0033 |           1.1801 |           0.0872 |
[32m[20230207 15:06:44 @agent_ppo2.py:192][0m |          -0.0048 |           1.1343 |           0.0872 |
[32m[20230207 15:06:44 @agent_ppo2.py:192][0m |          -0.0047 |           1.0682 |           0.0871 |
[32m[20230207 15:06:44 @agent_ppo2.py:192][0m |          -0.0054 |           1.0480 |           0.0871 |
[32m[20230207 15:06:44 @agent_ppo2.py:192][0m |          -0.0059 |           1.0075 |           0.0871 |
[32m[20230207 15:06:45 @agent_ppo2.py:192][0m |          -0.0058 |           0.9756 |           0.0871 |
[32m[20230207 15:06:45 @agent_ppo2.py:192][0m |          -0.0064 |           0.9453 |           0.0871 |
[32m[20230207 15:06:45 @agent_ppo2.py:192][0m |          -0.0067 |           0.9341 |           0.0871 |
[32m[20230207 15:06:45 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:06:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: -97.29
[32m[20230207 15:06:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -79.87
[32m[20230207 15:06:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -124.88
[32m[20230207 15:06:45 @agent_ppo2.py:150][0m Total time:       6.07 min
[32m[20230207 15:06:45 @agent_ppo2.py:152][0m 313344 total steps have happened
[32m[20230207 15:06:45 @agent_ppo2.py:128][0m #------------------------ Iteration 153 --------------------------#
[32m[20230207 15:06:46 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:06:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:46 @agent_ppo2.py:192][0m |           0.0080 |           9.4097 |           0.0840 |
[32m[20230207 15:06:46 @agent_ppo2.py:192][0m |           0.0161 |           4.8667 |           0.0839 |
[32m[20230207 15:06:47 @agent_ppo2.py:192][0m |          -0.0003 |           4.6711 |           0.0839 |
[32m[20230207 15:06:47 @agent_ppo2.py:192][0m |          -0.0032 |           3.7262 |           0.0839 |
[32m[20230207 15:06:47 @agent_ppo2.py:192][0m |          -0.0003 |           3.4511 |           0.0838 |
[32m[20230207 15:06:47 @agent_ppo2.py:192][0m |          -0.0027 |           3.3906 |           0.0838 |
[32m[20230207 15:06:47 @agent_ppo2.py:192][0m |          -0.0007 |           3.1533 |           0.0838 |
[32m[20230207 15:06:47 @agent_ppo2.py:192][0m |          -0.0038 |           3.1179 |           0.0838 |
[32m[20230207 15:06:47 @agent_ppo2.py:192][0m |          -0.0083 |           3.0031 |           0.0838 |
[32m[20230207 15:06:47 @agent_ppo2.py:192][0m |          -0.0055 |           2.9421 |           0.0839 |
[32m[20230207 15:06:47 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:06:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -80.79
[32m[20230207 15:06:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -52.42
[32m[20230207 15:06:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -93.44
[32m[20230207 15:06:48 @agent_ppo2.py:150][0m Total time:       6.11 min
[32m[20230207 15:06:48 @agent_ppo2.py:152][0m 315392 total steps have happened
[32m[20230207 15:06:48 @agent_ppo2.py:128][0m #------------------------ Iteration 154 --------------------------#
[32m[20230207 15:06:49 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:06:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:49 @agent_ppo2.py:192][0m |          -0.0010 |           3.4665 |           0.0862 |
[32m[20230207 15:06:49 @agent_ppo2.py:192][0m |          -0.0029 |           1.5236 |           0.0861 |
[32m[20230207 15:06:49 @agent_ppo2.py:192][0m |          -0.0001 |           1.1802 |           0.0861 |
[32m[20230207 15:06:49 @agent_ppo2.py:192][0m |          -0.0062 |           1.0152 |           0.0860 |
[32m[20230207 15:06:49 @agent_ppo2.py:192][0m |          -0.0072 |           0.9689 |           0.0860 |
[32m[20230207 15:06:49 @agent_ppo2.py:192][0m |          -0.0065 |           0.8769 |           0.0860 |
[32m[20230207 15:06:49 @agent_ppo2.py:192][0m |          -0.0079 |           0.8446 |           0.0859 |
[32m[20230207 15:06:49 @agent_ppo2.py:192][0m |          -0.0086 |           0.8043 |           0.0859 |
[32m[20230207 15:06:49 @agent_ppo2.py:192][0m |          -0.0124 |           0.7856 |           0.0858 |
[32m[20230207 15:06:50 @agent_ppo2.py:192][0m |          -0.0124 |           0.7661 |           0.0858 |
[32m[20230207 15:06:50 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:06:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: -57.47
[32m[20230207 15:06:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -25.85
[32m[20230207 15:06:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -36.47
[32m[20230207 15:06:50 @agent_ppo2.py:150][0m Total time:       6.15 min
[32m[20230207 15:06:50 @agent_ppo2.py:152][0m 317440 total steps have happened
[32m[20230207 15:06:50 @agent_ppo2.py:128][0m #------------------------ Iteration 155 --------------------------#
[32m[20230207 15:06:51 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:06:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:51 @agent_ppo2.py:192][0m |           0.0120 |           3.2476 |           0.0853 |
[32m[20230207 15:06:51 @agent_ppo2.py:192][0m |          -0.0077 |           2.5566 |           0.0852 |
[32m[20230207 15:06:51 @agent_ppo2.py:192][0m |          -0.0005 |           2.4377 |           0.0851 |
[32m[20230207 15:06:52 @agent_ppo2.py:192][0m |          -0.0048 |           2.3767 |           0.0851 |
[32m[20230207 15:06:52 @agent_ppo2.py:192][0m |          -0.0105 |           2.3218 |           0.0851 |
[32m[20230207 15:06:52 @agent_ppo2.py:192][0m |          -0.0090 |           2.2740 |           0.0851 |
[32m[20230207 15:06:52 @agent_ppo2.py:192][0m |          -0.0605 |           2.3445 |           0.0850 |
[32m[20230207 15:06:52 @agent_ppo2.py:192][0m |          -0.0027 |           2.4043 |           0.0849 |
[32m[20230207 15:06:52 @agent_ppo2.py:192][0m |           0.0063 |           2.2805 |           0.0850 |
[32m[20230207 15:06:52 @agent_ppo2.py:192][0m |          -0.0080 |           2.2912 |           0.0850 |
[32m[20230207 15:06:52 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:06:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -68.13
[32m[20230207 15:06:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -59.46
[32m[20230207 15:06:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -110.87
[32m[20230207 15:06:53 @agent_ppo2.py:150][0m Total time:       6.19 min
[32m[20230207 15:06:53 @agent_ppo2.py:152][0m 319488 total steps have happened
[32m[20230207 15:06:53 @agent_ppo2.py:128][0m #------------------------ Iteration 156 --------------------------#
[32m[20230207 15:06:54 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:06:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:54 @agent_ppo2.py:192][0m |           0.0070 |           5.5848 |           0.0846 |
[32m[20230207 15:06:54 @agent_ppo2.py:192][0m |           0.0107 |           3.8079 |           0.0846 |
[32m[20230207 15:06:54 @agent_ppo2.py:192][0m |          -0.0006 |           3.5936 |           0.0846 |
[32m[20230207 15:06:54 @agent_ppo2.py:192][0m |          -0.0011 |           3.3982 |           0.0845 |
[32m[20230207 15:06:54 @agent_ppo2.py:192][0m |          -0.0267 |           3.2990 |           0.0845 |
[32m[20230207 15:06:54 @agent_ppo2.py:192][0m |          -0.0038 |           3.1908 |           0.0845 |
[32m[20230207 15:06:54 @agent_ppo2.py:192][0m |           0.0015 |           3.1737 |           0.0844 |
[32m[20230207 15:06:54 @agent_ppo2.py:192][0m |           0.0027 |           3.0567 |           0.0844 |
[32m[20230207 15:06:54 @agent_ppo2.py:192][0m |          -0.0028 |           2.9661 |           0.0843 |
[32m[20230207 15:06:54 @agent_ppo2.py:192][0m |          -0.0019 |           2.9415 |           0.0843 |
[32m[20230207 15:06:54 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:06:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: -77.04
[32m[20230207 15:06:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -50.48
[32m[20230207 15:06:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -82.52
[32m[20230207 15:06:55 @agent_ppo2.py:150][0m Total time:       6.23 min
[32m[20230207 15:06:55 @agent_ppo2.py:152][0m 321536 total steps have happened
[32m[20230207 15:06:55 @agent_ppo2.py:128][0m #------------------------ Iteration 157 --------------------------#
[32m[20230207 15:06:56 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:06:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:56 @agent_ppo2.py:192][0m |           0.0019 |           2.5376 |           0.0860 |
[32m[20230207 15:06:56 @agent_ppo2.py:192][0m |           0.0008 |           2.2110 |           0.0860 |
[32m[20230207 15:06:56 @agent_ppo2.py:192][0m |          -0.0008 |           2.1781 |           0.0861 |
[32m[20230207 15:06:56 @agent_ppo2.py:192][0m |          -0.0024 |           2.1670 |           0.0861 |
[32m[20230207 15:06:57 @agent_ppo2.py:192][0m |          -0.0085 |           2.1523 |           0.0861 |
[32m[20230207 15:06:57 @agent_ppo2.py:192][0m |          -0.0048 |           2.1281 |           0.0861 |
[32m[20230207 15:06:57 @agent_ppo2.py:192][0m |          -0.0102 |           2.1235 |           0.0861 |
[32m[20230207 15:06:57 @agent_ppo2.py:192][0m |          -0.0026 |           2.1212 |           0.0862 |
[32m[20230207 15:06:57 @agent_ppo2.py:192][0m |          -0.0024 |           2.1129 |           0.0862 |
[32m[20230207 15:06:57 @agent_ppo2.py:192][0m |          -0.0036 |           2.1056 |           0.0862 |
[32m[20230207 15:06:57 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:06:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: -142.00
[32m[20230207 15:06:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -121.11
[32m[20230207 15:06:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -73.08
[32m[20230207 15:06:58 @agent_ppo2.py:150][0m Total time:       6.28 min
[32m[20230207 15:06:58 @agent_ppo2.py:152][0m 323584 total steps have happened
[32m[20230207 15:06:58 @agent_ppo2.py:128][0m #------------------------ Iteration 158 --------------------------#
[32m[20230207 15:06:59 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:06:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:06:59 @agent_ppo2.py:192][0m |           0.0047 |           7.5353 |           0.0884 |
[32m[20230207 15:06:59 @agent_ppo2.py:192][0m |          -0.0037 |           4.3627 |           0.0884 |
[32m[20230207 15:06:59 @agent_ppo2.py:192][0m |           0.0006 |           3.8562 |           0.0883 |
[32m[20230207 15:06:59 @agent_ppo2.py:192][0m |          -0.0044 |           3.6741 |           0.0882 |
[32m[20230207 15:06:59 @agent_ppo2.py:192][0m |          -0.0063 |           3.5639 |           0.0882 |
[32m[20230207 15:06:59 @agent_ppo2.py:192][0m |          -0.0099 |           3.4713 |           0.0881 |
[32m[20230207 15:06:59 @agent_ppo2.py:192][0m |          -0.0087 |           3.4258 |           0.0881 |
[32m[20230207 15:06:59 @agent_ppo2.py:192][0m |          -0.0106 |           3.3834 |           0.0881 |
[32m[20230207 15:06:59 @agent_ppo2.py:192][0m |          -0.0057 |           3.3769 |           0.0880 |
[32m[20230207 15:06:59 @agent_ppo2.py:192][0m |          -0.0103 |           3.3124 |           0.0880 |
[32m[20230207 15:06:59 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:07:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: -96.90
[32m[20230207 15:07:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -77.80
[32m[20230207 15:07:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -92.22
[32m[20230207 15:07:00 @agent_ppo2.py:150][0m Total time:       6.32 min
[32m[20230207 15:07:00 @agent_ppo2.py:152][0m 325632 total steps have happened
[32m[20230207 15:07:00 @agent_ppo2.py:128][0m #------------------------ Iteration 159 --------------------------#
[32m[20230207 15:07:01 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:07:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:01 @agent_ppo2.py:192][0m |           0.0038 |           1.7538 |           0.0851 |
[32m[20230207 15:07:01 @agent_ppo2.py:192][0m |           0.0116 |           1.4680 |           0.0851 |
[32m[20230207 15:07:01 @agent_ppo2.py:192][0m |           0.0053 |           1.3803 |           0.0851 |
[32m[20230207 15:07:01 @agent_ppo2.py:192][0m |          -0.0026 |           1.3255 |           0.0851 |
[32m[20230207 15:07:02 @agent_ppo2.py:192][0m |          -0.0085 |           1.2984 |           0.0850 |
[32m[20230207 15:07:02 @agent_ppo2.py:192][0m |          -0.0085 |           1.2533 |           0.0850 |
[32m[20230207 15:07:02 @agent_ppo2.py:192][0m |          -0.0041 |           1.2322 |           0.0850 |
[32m[20230207 15:07:02 @agent_ppo2.py:192][0m |          -0.0190 |           1.2211 |           0.0850 |
[32m[20230207 15:07:02 @agent_ppo2.py:192][0m |          -0.0048 |           1.1917 |           0.0850 |
[32m[20230207 15:07:02 @agent_ppo2.py:192][0m |           0.0219 |           1.5646 |           0.0850 |
[32m[20230207 15:07:02 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -59.24
[32m[20230207 15:07:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -34.51
[32m[20230207 15:07:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -125.14
[32m[20230207 15:07:03 @agent_ppo2.py:150][0m Total time:       6.36 min
[32m[20230207 15:07:03 @agent_ppo2.py:152][0m 327680 total steps have happened
[32m[20230207 15:07:03 @agent_ppo2.py:128][0m #------------------------ Iteration 160 --------------------------#
[32m[20230207 15:07:03 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:07:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:04 @agent_ppo2.py:192][0m |           0.0002 |           7.6735 |           0.0854 |
[32m[20230207 15:07:04 @agent_ppo2.py:192][0m |          -0.0046 |           4.5100 |           0.0854 |
[32m[20230207 15:07:04 @agent_ppo2.py:192][0m |          -0.0046 |           4.1055 |           0.0853 |
[32m[20230207 15:07:04 @agent_ppo2.py:192][0m |          -0.0075 |           3.6764 |           0.0853 |
[32m[20230207 15:07:04 @agent_ppo2.py:192][0m |          -0.0122 |           3.4805 |           0.0853 |
[32m[20230207 15:07:04 @agent_ppo2.py:192][0m |          -0.0092 |           3.3329 |           0.0853 |
[32m[20230207 15:07:04 @agent_ppo2.py:192][0m |          -0.0099 |           3.2336 |           0.0852 |
[32m[20230207 15:07:04 @agent_ppo2.py:192][0m |          -0.0054 |           3.1720 |           0.0852 |
[32m[20230207 15:07:04 @agent_ppo2.py:192][0m |          -0.0088 |           3.1212 |           0.0852 |
[32m[20230207 15:07:04 @agent_ppo2.py:192][0m |          -0.0129 |           3.0283 |           0.0852 |
[32m[20230207 15:07:04 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:07:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -96.18
[32m[20230207 15:07:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -78.38
[32m[20230207 15:07:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -135.29
[32m[20230207 15:07:05 @agent_ppo2.py:150][0m Total time:       6.40 min
[32m[20230207 15:07:05 @agent_ppo2.py:152][0m 329728 total steps have happened
[32m[20230207 15:07:05 @agent_ppo2.py:128][0m #------------------------ Iteration 161 --------------------------#
[32m[20230207 15:07:06 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:07:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:06 @agent_ppo2.py:192][0m |          -0.0560 |           1.9138 |           0.0830 |
[32m[20230207 15:07:06 @agent_ppo2.py:192][0m |          -0.0032 |           1.5738 |           0.0830 |
[32m[20230207 15:07:06 @agent_ppo2.py:192][0m |          -0.0152 |           1.5427 |           0.0830 |
[32m[20230207 15:07:06 @agent_ppo2.py:192][0m |           0.0035 |           1.5329 |           0.0830 |
[32m[20230207 15:07:06 @agent_ppo2.py:192][0m |           0.0050 |           1.5239 |           0.0830 |
[32m[20230207 15:07:06 @agent_ppo2.py:192][0m |          -0.0036 |           1.5199 |           0.0829 |
[32m[20230207 15:07:07 @agent_ppo2.py:192][0m |          -0.0059 |           1.5038 |           0.0829 |
[32m[20230207 15:07:07 @agent_ppo2.py:192][0m |          -0.0036 |           1.5055 |           0.0829 |
[32m[20230207 15:07:07 @agent_ppo2.py:192][0m |          -0.0161 |           1.4934 |           0.0830 |
[32m[20230207 15:07:07 @agent_ppo2.py:192][0m |          -0.0126 |           1.4907 |           0.0830 |
[32m[20230207 15:07:07 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -80.93
[32m[20230207 15:07:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -48.64
[32m[20230207 15:07:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -140.43
[32m[20230207 15:07:08 @agent_ppo2.py:150][0m Total time:       6.44 min
[32m[20230207 15:07:08 @agent_ppo2.py:152][0m 331776 total steps have happened
[32m[20230207 15:07:08 @agent_ppo2.py:128][0m #------------------------ Iteration 162 --------------------------#
[32m[20230207 15:07:08 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:07:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:09 @agent_ppo2.py:192][0m |           0.0026 |           6.5337 |           0.0841 |
[32m[20230207 15:07:09 @agent_ppo2.py:192][0m |           0.0024 |           4.2409 |           0.0841 |
[32m[20230207 15:07:09 @agent_ppo2.py:192][0m |          -0.0034 |           3.6678 |           0.0842 |
[32m[20230207 15:07:09 @agent_ppo2.py:192][0m |          -0.0090 |           3.3541 |           0.0842 |
[32m[20230207 15:07:09 @agent_ppo2.py:192][0m |          -0.0084 |           3.1157 |           0.0841 |
[32m[20230207 15:07:09 @agent_ppo2.py:192][0m |          -0.0092 |           2.9534 |           0.0841 |
[32m[20230207 15:07:09 @agent_ppo2.py:192][0m |          -0.0055 |           2.8167 |           0.0842 |
[32m[20230207 15:07:09 @agent_ppo2.py:192][0m |          -0.0148 |           2.8883 |           0.0841 |
[32m[20230207 15:07:09 @agent_ppo2.py:192][0m |          -0.0091 |           2.7005 |           0.0842 |
[32m[20230207 15:07:09 @agent_ppo2.py:192][0m |          -0.0053 |           2.6988 |           0.0842 |
[32m[20230207 15:07:09 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:07:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -69.33
[32m[20230207 15:07:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -46.40
[32m[20230207 15:07:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -94.31
[32m[20230207 15:07:10 @agent_ppo2.py:150][0m Total time:       6.48 min
[32m[20230207 15:07:10 @agent_ppo2.py:152][0m 333824 total steps have happened
[32m[20230207 15:07:10 @agent_ppo2.py:128][0m #------------------------ Iteration 163 --------------------------#
[32m[20230207 15:07:11 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:07:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:11 @agent_ppo2.py:192][0m |          -0.0090 |           5.3050 |           0.0827 |
[32m[20230207 15:07:11 @agent_ppo2.py:192][0m |          -0.0072 |           4.6044 |           0.0826 |
[32m[20230207 15:07:11 @agent_ppo2.py:192][0m |          -0.0061 |           4.4646 |           0.0826 |
[32m[20230207 15:07:11 @agent_ppo2.py:192][0m |          -0.0022 |           4.4057 |           0.0825 |
[32m[20230207 15:07:11 @agent_ppo2.py:192][0m |          -0.0038 |           4.3184 |           0.0825 |
[32m[20230207 15:07:11 @agent_ppo2.py:192][0m |          -0.0088 |           4.2667 |           0.0825 |
[32m[20230207 15:07:11 @agent_ppo2.py:192][0m |          -0.0016 |           4.2374 |           0.0824 |
[32m[20230207 15:07:12 @agent_ppo2.py:192][0m |          -0.0089 |           4.2094 |           0.0825 |
[32m[20230207 15:07:12 @agent_ppo2.py:192][0m |          -0.0078 |           4.2088 |           0.0825 |
[32m[20230207 15:07:12 @agent_ppo2.py:192][0m |          -0.0102 |           4.1529 |           0.0824 |
[32m[20230207 15:07:12 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -103.19
[32m[20230207 15:07:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -49.03
[32m[20230207 15:07:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -74.27
[32m[20230207 15:07:13 @agent_ppo2.py:150][0m Total time:       6.52 min
[32m[20230207 15:07:13 @agent_ppo2.py:152][0m 335872 total steps have happened
[32m[20230207 15:07:13 @agent_ppo2.py:128][0m #------------------------ Iteration 164 --------------------------#
[32m[20230207 15:07:13 @agent_ppo2.py:134][0m Sampling time: 0.66 s by 1 slaves
[32m[20230207 15:07:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:13 @agent_ppo2.py:192][0m |          -0.0028 |          14.6920 |           0.0840 |
[32m[20230207 15:07:13 @agent_ppo2.py:192][0m |          -0.0046 |           6.9241 |           0.0840 |
[32m[20230207 15:07:13 @agent_ppo2.py:192][0m |          -0.0068 |           5.0335 |           0.0840 |
[32m[20230207 15:07:13 @agent_ppo2.py:192][0m |          -0.0012 |           4.0292 |           0.0839 |
[32m[20230207 15:07:14 @agent_ppo2.py:192][0m |          -0.0144 |           3.7620 |           0.0839 |
[32m[20230207 15:07:14 @agent_ppo2.py:192][0m |          -0.0118 |           3.4253 |           0.0838 |
[32m[20230207 15:07:14 @agent_ppo2.py:192][0m |          -0.0129 |           3.1504 |           0.0838 |
[32m[20230207 15:07:14 @agent_ppo2.py:192][0m |          -0.0117 |           2.9255 |           0.0838 |
[32m[20230207 15:07:14 @agent_ppo2.py:192][0m |          -0.0120 |           2.5754 |           0.0838 |
[32m[20230207 15:07:14 @agent_ppo2.py:192][0m |          -0.0071 |           2.1374 |           0.0837 |
[32m[20230207 15:07:14 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230207 15:07:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: -104.50
[32m[20230207 15:07:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -40.94
[32m[20230207 15:07:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -64.80
[32m[20230207 15:07:15 @agent_ppo2.py:150][0m Total time:       6.56 min
[32m[20230207 15:07:15 @agent_ppo2.py:152][0m 337920 total steps have happened
[32m[20230207 15:07:15 @agent_ppo2.py:128][0m #------------------------ Iteration 165 --------------------------#
[32m[20230207 15:07:16 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:07:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:16 @agent_ppo2.py:192][0m |          -0.0000 |           6.1099 |           0.0856 |
[32m[20230207 15:07:16 @agent_ppo2.py:192][0m |          -0.0026 |           4.8082 |           0.0857 |
[32m[20230207 15:07:16 @agent_ppo2.py:192][0m |          -0.0047 |           4.5807 |           0.0857 |
[32m[20230207 15:07:16 @agent_ppo2.py:192][0m |          -0.0076 |           4.4133 |           0.0857 |
[32m[20230207 15:07:16 @agent_ppo2.py:192][0m |          -0.0079 |           4.3219 |           0.0856 |
[32m[20230207 15:07:16 @agent_ppo2.py:192][0m |          -0.0071 |           4.1903 |           0.0856 |
[32m[20230207 15:07:16 @agent_ppo2.py:192][0m |          -0.0075 |           4.1131 |           0.0857 |
[32m[20230207 15:07:16 @agent_ppo2.py:192][0m |          -0.0075 |           4.0598 |           0.0857 |
[32m[20230207 15:07:16 @agent_ppo2.py:192][0m |          -0.0106 |           3.9995 |           0.0857 |
[32m[20230207 15:07:16 @agent_ppo2.py:192][0m |          -0.0086 |           3.9239 |           0.0857 |
[32m[20230207 15:07:16 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:07:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: -93.11
[32m[20230207 15:07:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -45.67
[32m[20230207 15:07:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -67.99
[32m[20230207 15:07:17 @agent_ppo2.py:150][0m Total time:       6.60 min
[32m[20230207 15:07:17 @agent_ppo2.py:152][0m 339968 total steps have happened
[32m[20230207 15:07:17 @agent_ppo2.py:128][0m #------------------------ Iteration 166 --------------------------#
[32m[20230207 15:07:18 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230207 15:07:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:18 @agent_ppo2.py:192][0m |          -0.0001 |          46.4198 |           0.0837 |
[32m[20230207 15:07:18 @agent_ppo2.py:192][0m |          -0.0037 |          33.8699 |           0.0838 |
[32m[20230207 15:07:18 @agent_ppo2.py:192][0m |          -0.0028 |          23.6400 |           0.0838 |
[32m[20230207 15:07:18 @agent_ppo2.py:192][0m |          -0.0012 |          12.8349 |           0.0838 |
[32m[20230207 15:07:18 @agent_ppo2.py:192][0m |          -0.0065 |           8.9083 |           0.0838 |
[32m[20230207 15:07:18 @agent_ppo2.py:192][0m |          -0.0074 |           7.3233 |           0.0838 |
[32m[20230207 15:07:18 @agent_ppo2.py:192][0m |          -0.0058 |           6.0411 |           0.0838 |
[32m[20230207 15:07:18 @agent_ppo2.py:192][0m |          -0.0081 |           4.9893 |           0.0838 |
[32m[20230207 15:07:18 @agent_ppo2.py:192][0m |          -0.0099 |           4.4080 |           0.0838 |
[32m[20230207 15:07:18 @agent_ppo2.py:192][0m |          -0.0099 |           3.9177 |           0.0838 |
[32m[20230207 15:07:18 @agent_ppo2.py:137][0m Policy update time: 0.61 s
[32m[20230207 15:07:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.20
[32m[20230207 15:07:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -83.55
[32m[20230207 15:07:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -71.68
[32m[20230207 15:07:19 @agent_ppo2.py:150][0m Total time:       6.63 min
[32m[20230207 15:07:19 @agent_ppo2.py:152][0m 342016 total steps have happened
[32m[20230207 15:07:19 @agent_ppo2.py:128][0m #------------------------ Iteration 167 --------------------------#
[32m[20230207 15:07:20 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:07:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:20 @agent_ppo2.py:192][0m |           0.0023 |           8.5787 |           0.0859 |
[32m[20230207 15:07:20 @agent_ppo2.py:192][0m |          -0.0025 |           4.5846 |           0.0859 |
[32m[20230207 15:07:20 @agent_ppo2.py:192][0m |          -0.0044 |           4.2410 |           0.0859 |
[32m[20230207 15:07:20 @agent_ppo2.py:192][0m |          -0.0043 |           3.7857 |           0.0859 |
[32m[20230207 15:07:21 @agent_ppo2.py:192][0m |          -0.0062 |           3.5979 |           0.0858 |
[32m[20230207 15:07:21 @agent_ppo2.py:192][0m |          -0.0063 |           3.4946 |           0.0859 |
[32m[20230207 15:07:21 @agent_ppo2.py:192][0m |          -0.0076 |           3.3036 |           0.0859 |
[32m[20230207 15:07:21 @agent_ppo2.py:192][0m |          -0.0091 |           3.2408 |           0.0858 |
[32m[20230207 15:07:21 @agent_ppo2.py:192][0m |          -0.0082 |           3.1900 |           0.0858 |
[32m[20230207 15:07:21 @agent_ppo2.py:192][0m |          -0.0083 |           3.1238 |           0.0859 |
[32m[20230207 15:07:21 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:07:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: -78.42
[32m[20230207 15:07:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -53.08
[32m[20230207 15:07:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -64.23
[32m[20230207 15:07:22 @agent_ppo2.py:150][0m Total time:       6.68 min
[32m[20230207 15:07:22 @agent_ppo2.py:152][0m 344064 total steps have happened
[32m[20230207 15:07:22 @agent_ppo2.py:128][0m #------------------------ Iteration 168 --------------------------#
[32m[20230207 15:07:23 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:07:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:23 @agent_ppo2.py:192][0m |          -0.0044 |           1.6935 |           0.0834 |
[32m[20230207 15:07:23 @agent_ppo2.py:192][0m |          -0.0135 |           1.3488 |           0.0834 |
[32m[20230207 15:07:23 @agent_ppo2.py:192][0m |           0.0097 |           1.2954 |           0.0832 |
[32m[20230207 15:07:23 @agent_ppo2.py:192][0m |          -0.0102 |           1.2569 |           0.0832 |
[32m[20230207 15:07:23 @agent_ppo2.py:192][0m |          -0.0123 |           1.2355 |           0.0833 |
[32m[20230207 15:07:23 @agent_ppo2.py:192][0m |          -0.0189 |           1.2148 |           0.0833 |
[32m[20230207 15:07:23 @agent_ppo2.py:192][0m |           0.0028 |           1.2113 |           0.0832 |
[32m[20230207 15:07:23 @agent_ppo2.py:192][0m |          -0.0108 |           1.1877 |           0.0834 |
[32m[20230207 15:07:23 @agent_ppo2.py:192][0m |          -0.0038 |           1.1773 |           0.0833 |
[32m[20230207 15:07:23 @agent_ppo2.py:192][0m |           0.0067 |           1.1668 |           0.0833 |
[32m[20230207 15:07:23 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: -79.20
[32m[20230207 15:07:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -65.04
[32m[20230207 15:07:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -101.52
[32m[20230207 15:07:24 @agent_ppo2.py:150][0m Total time:       6.72 min
[32m[20230207 15:07:24 @agent_ppo2.py:152][0m 346112 total steps have happened
[32m[20230207 15:07:24 @agent_ppo2.py:128][0m #------------------------ Iteration 169 --------------------------#
[32m[20230207 15:07:25 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:07:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:25 @agent_ppo2.py:192][0m |          -0.0134 |           2.3285 |           0.0850 |
[32m[20230207 15:07:25 @agent_ppo2.py:192][0m |           0.0092 |           1.6971 |           0.0848 |
[32m[20230207 15:07:25 @agent_ppo2.py:192][0m |          -0.0090 |           1.5362 |           0.0849 |
[32m[20230207 15:07:25 @agent_ppo2.py:192][0m |           0.0035 |           1.4350 |           0.0850 |
[32m[20230207 15:07:25 @agent_ppo2.py:192][0m |          -0.0015 |           1.3674 |           0.0850 |
[32m[20230207 15:07:26 @agent_ppo2.py:192][0m |           0.0063 |           1.3499 |           0.0850 |
[32m[20230207 15:07:26 @agent_ppo2.py:192][0m |          -0.0399 |           1.3083 |           0.0851 |
[32m[20230207 15:07:26 @agent_ppo2.py:192][0m |          -0.0017 |           1.2683 |           0.0851 |
[32m[20230207 15:07:26 @agent_ppo2.py:192][0m |          -0.0044 |           1.2476 |           0.0852 |
[32m[20230207 15:07:26 @agent_ppo2.py:192][0m |          -0.0032 |           1.2125 |           0.0853 |
[32m[20230207 15:07:26 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -47.84
[32m[20230207 15:07:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -26.82
[32m[20230207 15:07:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -108.99
[32m[20230207 15:07:27 @agent_ppo2.py:150][0m Total time:       6.76 min
[32m[20230207 15:07:27 @agent_ppo2.py:152][0m 348160 total steps have happened
[32m[20230207 15:07:27 @agent_ppo2.py:128][0m #------------------------ Iteration 170 --------------------------#
[32m[20230207 15:07:27 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:07:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:28 @agent_ppo2.py:192][0m |           0.0108 |           3.9494 |           0.0871 |
[32m[20230207 15:07:28 @agent_ppo2.py:192][0m |          -0.0016 |           1.9980 |           0.0871 |
[32m[20230207 15:07:28 @agent_ppo2.py:192][0m |          -0.0050 |           1.6502 |           0.0870 |
[32m[20230207 15:07:28 @agent_ppo2.py:192][0m |          -0.0066 |           1.5048 |           0.0870 |
[32m[20230207 15:07:28 @agent_ppo2.py:192][0m |          -0.0079 |           1.4468 |           0.0870 |
[32m[20230207 15:07:28 @agent_ppo2.py:192][0m |          -0.0010 |           1.4108 |           0.0870 |
[32m[20230207 15:07:28 @agent_ppo2.py:192][0m |          -0.0081 |           1.3643 |           0.0869 |
[32m[20230207 15:07:28 @agent_ppo2.py:192][0m |          -0.0071 |           1.3087 |           0.0869 |
[32m[20230207 15:07:28 @agent_ppo2.py:192][0m |           0.0011 |           1.2885 |           0.0869 |
[32m[20230207 15:07:28 @agent_ppo2.py:192][0m |          -0.0071 |           1.2593 |           0.0869 |
[32m[20230207 15:07:28 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:07:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -61.32
[32m[20230207 15:07:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -34.56
[32m[20230207 15:07:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -49.73
[32m[20230207 15:07:29 @agent_ppo2.py:150][0m Total time:       6.80 min
[32m[20230207 15:07:29 @agent_ppo2.py:152][0m 350208 total steps have happened
[32m[20230207 15:07:29 @agent_ppo2.py:128][0m #------------------------ Iteration 171 --------------------------#
[32m[20230207 15:07:30 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:07:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:30 @agent_ppo2.py:192][0m |           0.0020 |           1.0016 |           0.0855 |
[32m[20230207 15:07:30 @agent_ppo2.py:192][0m |          -0.0004 |           0.9524 |           0.0853 |
[32m[20230207 15:07:30 @agent_ppo2.py:192][0m |          -0.0145 |           0.9365 |           0.0852 |
[32m[20230207 15:07:30 @agent_ppo2.py:192][0m |          -0.0050 |           0.9135 |           0.0852 |
[32m[20230207 15:07:30 @agent_ppo2.py:192][0m |          -0.0104 |           0.9092 |           0.0852 |
[32m[20230207 15:07:30 @agent_ppo2.py:192][0m |          -0.0027 |           0.8925 |           0.0852 |
[32m[20230207 15:07:31 @agent_ppo2.py:192][0m |           0.0037 |           0.8935 |           0.0851 |
[32m[20230207 15:07:31 @agent_ppo2.py:192][0m |          -0.0266 |           0.8904 |           0.0851 |
[32m[20230207 15:07:31 @agent_ppo2.py:192][0m |          -0.0063 |           0.8753 |           0.0851 |
[32m[20230207 15:07:31 @agent_ppo2.py:192][0m |           0.0055 |           0.8727 |           0.0851 |
[32m[20230207 15:07:31 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -86.23
[32m[20230207 15:07:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -85.53
[32m[20230207 15:07:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -98.92
[32m[20230207 15:07:32 @agent_ppo2.py:150][0m Total time:       6.84 min
[32m[20230207 15:07:32 @agent_ppo2.py:152][0m 352256 total steps have happened
[32m[20230207 15:07:32 @agent_ppo2.py:128][0m #------------------------ Iteration 172 --------------------------#
[32m[20230207 15:07:32 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:07:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:33 @agent_ppo2.py:192][0m |           0.0152 |           0.9915 |           0.0884 |
[32m[20230207 15:07:33 @agent_ppo2.py:192][0m |           0.0001 |           0.9952 |           0.0884 |
[32m[20230207 15:07:33 @agent_ppo2.py:192][0m |           0.0016 |           0.8820 |           0.0884 |
[32m[20230207 15:07:33 @agent_ppo2.py:192][0m |          -0.0054 |           0.8671 |           0.0884 |
[32m[20230207 15:07:33 @agent_ppo2.py:192][0m |          -0.0002 |           0.8566 |           0.0885 |
[32m[20230207 15:07:33 @agent_ppo2.py:192][0m |           0.0010 |           0.8540 |           0.0885 |
[32m[20230207 15:07:33 @agent_ppo2.py:192][0m |          -0.0093 |           0.8514 |           0.0885 |
[32m[20230207 15:07:33 @agent_ppo2.py:192][0m |          -0.0014 |           0.8362 |           0.0886 |
[32m[20230207 15:07:33 @agent_ppo2.py:192][0m |          -0.0057 |           0.8294 |           0.0886 |
[32m[20230207 15:07:33 @agent_ppo2.py:192][0m |          -0.0028 |           0.8232 |           0.0886 |
[32m[20230207 15:07:33 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:07:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: -97.27
[32m[20230207 15:07:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -91.12
[32m[20230207 15:07:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -93.06
[32m[20230207 15:07:34 @agent_ppo2.py:150][0m Total time:       6.88 min
[32m[20230207 15:07:34 @agent_ppo2.py:152][0m 354304 total steps have happened
[32m[20230207 15:07:34 @agent_ppo2.py:128][0m #------------------------ Iteration 173 --------------------------#
[32m[20230207 15:07:35 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:07:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:35 @agent_ppo2.py:192][0m |          -0.0012 |          12.9015 |           0.0878 |
[32m[20230207 15:07:35 @agent_ppo2.py:192][0m |          -0.0035 |           7.1276 |           0.0879 |
[32m[20230207 15:07:35 @agent_ppo2.py:192][0m |          -0.0052 |           5.6408 |           0.0879 |
[32m[20230207 15:07:35 @agent_ppo2.py:192][0m |           0.0052 |           5.4828 |           0.0879 |
[32m[20230207 15:07:35 @agent_ppo2.py:192][0m |          -0.0150 |           5.6388 |           0.0880 |
[32m[20230207 15:07:35 @agent_ppo2.py:192][0m |          -0.0021 |           4.6146 |           0.0880 |
[32m[20230207 15:07:35 @agent_ppo2.py:192][0m |          -0.0121 |           4.2532 |           0.0880 |
[32m[20230207 15:07:35 @agent_ppo2.py:192][0m |          -0.0118 |           4.0481 |           0.0880 |
[32m[20230207 15:07:35 @agent_ppo2.py:192][0m |          -0.0131 |           3.8714 |           0.0881 |
[32m[20230207 15:07:35 @agent_ppo2.py:192][0m |          -0.0167 |           3.6588 |           0.0881 |
[32m[20230207 15:07:35 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:07:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -80.61
[32m[20230207 15:07:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -66.05
[32m[20230207 15:07:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -85.88
[32m[20230207 15:07:36 @agent_ppo2.py:150][0m Total time:       6.92 min
[32m[20230207 15:07:36 @agent_ppo2.py:152][0m 356352 total steps have happened
[32m[20230207 15:07:36 @agent_ppo2.py:128][0m #------------------------ Iteration 174 --------------------------#
[32m[20230207 15:07:37 @agent_ppo2.py:134][0m Sampling time: 0.91 s by 1 slaves
[32m[20230207 15:07:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:37 @agent_ppo2.py:192][0m |           0.0099 |          26.3290 |           0.0898 |
[32m[20230207 15:07:37 @agent_ppo2.py:192][0m |          -0.0061 |          19.6180 |           0.0898 |
[32m[20230207 15:07:37 @agent_ppo2.py:192][0m |          -0.0083 |          17.4253 |           0.0897 |
[32m[20230207 15:07:37 @agent_ppo2.py:192][0m |          -0.0075 |          14.9082 |           0.0897 |
[32m[20230207 15:07:38 @agent_ppo2.py:192][0m |           0.0052 |          14.7584 |           0.0897 |
[32m[20230207 15:07:38 @agent_ppo2.py:192][0m |          -0.0088 |          10.1020 |           0.0897 |
[32m[20230207 15:07:38 @agent_ppo2.py:192][0m |          -0.0076 |           8.0769 |           0.0897 |
[32m[20230207 15:07:38 @agent_ppo2.py:192][0m |           0.0042 |           6.9097 |           0.0896 |
[32m[20230207 15:07:38 @agent_ppo2.py:192][0m |          -0.0123 |           5.2484 |           0.0897 |
[32m[20230207 15:07:38 @agent_ppo2.py:192][0m |          -0.0125 |           4.0706 |           0.0897 |
[32m[20230207 15:07:38 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:07:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -110.59
[32m[20230207 15:07:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -88.40
[32m[20230207 15:07:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -111.72
[32m[20230207 15:07:39 @agent_ppo2.py:150][0m Total time:       6.96 min
[32m[20230207 15:07:39 @agent_ppo2.py:152][0m 358400 total steps have happened
[32m[20230207 15:07:39 @agent_ppo2.py:128][0m #------------------------ Iteration 175 --------------------------#
[32m[20230207 15:07:39 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230207 15:07:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:40 @agent_ppo2.py:192][0m |           0.0005 |          11.0155 |           0.0905 |
[32m[20230207 15:07:40 @agent_ppo2.py:192][0m |          -0.0010 |           5.4118 |           0.0905 |
[32m[20230207 15:07:40 @agent_ppo2.py:192][0m |          -0.0064 |           4.6528 |           0.0904 |
[32m[20230207 15:07:40 @agent_ppo2.py:192][0m |          -0.0063 |           4.4424 |           0.0904 |
[32m[20230207 15:07:40 @agent_ppo2.py:192][0m |          -0.0118 |           4.2060 |           0.0903 |
[32m[20230207 15:07:40 @agent_ppo2.py:192][0m |          -0.0055 |           4.0352 |           0.0903 |
[32m[20230207 15:07:40 @agent_ppo2.py:192][0m |          -0.0099 |           3.9191 |           0.0903 |
[32m[20230207 15:07:40 @agent_ppo2.py:192][0m |          -0.0107 |           3.9072 |           0.0903 |
[32m[20230207 15:07:40 @agent_ppo2.py:192][0m |          -0.0069 |           3.7186 |           0.0903 |
[32m[20230207 15:07:40 @agent_ppo2.py:192][0m |          -0.0122 |           3.6857 |           0.0903 |
[32m[20230207 15:07:40 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:07:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -115.42
[32m[20230207 15:07:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -64.42
[32m[20230207 15:07:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -45.16
[32m[20230207 15:07:41 @agent_ppo2.py:150][0m Total time:       6.99 min
[32m[20230207 15:07:41 @agent_ppo2.py:152][0m 360448 total steps have happened
[32m[20230207 15:07:41 @agent_ppo2.py:128][0m #------------------------ Iteration 176 --------------------------#
[32m[20230207 15:07:42 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:07:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:42 @agent_ppo2.py:192][0m |          -0.0068 |          14.5922 |           0.0880 |
[32m[20230207 15:07:42 @agent_ppo2.py:192][0m |           0.0176 |          10.8821 |           0.0879 |
[32m[20230207 15:07:42 @agent_ppo2.py:192][0m |           0.0025 |          10.1325 |           0.0879 |
[32m[20230207 15:07:42 @agent_ppo2.py:192][0m |          -0.0171 |           9.0524 |           0.0879 |
[32m[20230207 15:07:42 @agent_ppo2.py:192][0m |          -0.0119 |           8.7206 |           0.0878 |
[32m[20230207 15:07:42 @agent_ppo2.py:192][0m |          -0.0013 |           8.1828 |           0.0878 |
[32m[20230207 15:07:42 @agent_ppo2.py:192][0m |          -0.0026 |           7.9174 |           0.0878 |
[32m[20230207 15:07:42 @agent_ppo2.py:192][0m |          -0.0358 |           7.8111 |           0.0877 |
[32m[20230207 15:07:42 @agent_ppo2.py:192][0m |          -0.0061 |           7.7635 |           0.0877 |
[32m[20230207 15:07:42 @agent_ppo2.py:192][0m |          -0.0184 |           7.4782 |           0.0877 |
[32m[20230207 15:07:42 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -52.64
[32m[20230207 15:07:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -51.06
[32m[20230207 15:07:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -133.06
[32m[20230207 15:07:43 @agent_ppo2.py:150][0m Total time:       7.03 min
[32m[20230207 15:07:43 @agent_ppo2.py:152][0m 362496 total steps have happened
[32m[20230207 15:07:43 @agent_ppo2.py:128][0m #------------------------ Iteration 177 --------------------------#
[32m[20230207 15:07:44 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:07:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:44 @agent_ppo2.py:192][0m |          -0.0134 |           3.4553 |           0.0844 |
[32m[20230207 15:07:44 @agent_ppo2.py:192][0m |           0.0158 |           2.6610 |           0.0845 |
[32m[20230207 15:07:44 @agent_ppo2.py:192][0m |          -0.0139 |           2.4256 |           0.0844 |
[32m[20230207 15:07:44 @agent_ppo2.py:192][0m |           0.0001 |           2.3227 |           0.0845 |
[32m[20230207 15:07:44 @agent_ppo2.py:192][0m |          -0.0275 |           2.3333 |           0.0845 |
[32m[20230207 15:07:44 @agent_ppo2.py:192][0m |          -0.0029 |           2.2068 |           0.0845 |
[32m[20230207 15:07:45 @agent_ppo2.py:192][0m |          -0.0012 |           2.1437 |           0.0846 |
[32m[20230207 15:07:45 @agent_ppo2.py:192][0m |          -0.0017 |           2.1084 |           0.0846 |
[32m[20230207 15:07:45 @agent_ppo2.py:192][0m |          -0.0071 |           2.0846 |           0.0847 |
[32m[20230207 15:07:45 @agent_ppo2.py:192][0m |          -0.0250 |           2.0506 |           0.0847 |
[32m[20230207 15:07:45 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -68.91
[32m[20230207 15:07:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -39.76
[32m[20230207 15:07:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -110.18
[32m[20230207 15:07:46 @agent_ppo2.py:150][0m Total time:       7.07 min
[32m[20230207 15:07:46 @agent_ppo2.py:152][0m 364544 total steps have happened
[32m[20230207 15:07:46 @agent_ppo2.py:128][0m #------------------------ Iteration 178 --------------------------#
[32m[20230207 15:07:46 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:07:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:47 @agent_ppo2.py:192][0m |           0.0096 |           2.0899 |           0.0890 |
[32m[20230207 15:07:47 @agent_ppo2.py:192][0m |          -0.0116 |           1.5722 |           0.0890 |
[32m[20230207 15:07:47 @agent_ppo2.py:192][0m |          -0.0052 |           1.4218 |           0.0890 |
[32m[20230207 15:07:47 @agent_ppo2.py:192][0m |          -0.0142 |           1.3394 |           0.0890 |
[32m[20230207 15:07:47 @agent_ppo2.py:192][0m |          -0.0067 |           1.2910 |           0.0890 |
[32m[20230207 15:07:47 @agent_ppo2.py:192][0m |          -0.0192 |           1.2547 |           0.0891 |
[32m[20230207 15:07:47 @agent_ppo2.py:192][0m |          -0.0027 |           1.2225 |           0.0891 |
[32m[20230207 15:07:47 @agent_ppo2.py:192][0m |           0.0041 |           1.1982 |           0.0891 |
[32m[20230207 15:07:47 @agent_ppo2.py:192][0m |          -0.0035 |           1.1693 |           0.0892 |
[32m[20230207 15:07:47 @agent_ppo2.py:192][0m |          -0.0064 |           1.1944 |           0.0892 |
[32m[20230207 15:07:47 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -40.25
[32m[20230207 15:07:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -37.34
[32m[20230207 15:07:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -96.19
[32m[20230207 15:07:48 @agent_ppo2.py:150][0m Total time:       7.11 min
[32m[20230207 15:07:48 @agent_ppo2.py:152][0m 366592 total steps have happened
[32m[20230207 15:07:48 @agent_ppo2.py:128][0m #------------------------ Iteration 179 --------------------------#
[32m[20230207 15:07:49 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:07:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:49 @agent_ppo2.py:192][0m |           0.0050 |           4.3039 |           0.0916 |
[32m[20230207 15:07:49 @agent_ppo2.py:192][0m |          -0.0114 |           3.3271 |           0.0916 |
[32m[20230207 15:07:49 @agent_ppo2.py:192][0m |          -0.0118 |           3.0259 |           0.0916 |
[32m[20230207 15:07:49 @agent_ppo2.py:192][0m |           0.0011 |           2.8383 |           0.0916 |
[32m[20230207 15:07:49 @agent_ppo2.py:192][0m |           0.0055 |           2.7758 |           0.0916 |
[32m[20230207 15:07:49 @agent_ppo2.py:192][0m |           0.0173 |           2.7421 |           0.0916 |
[32m[20230207 15:07:49 @agent_ppo2.py:192][0m |          -0.0056 |           2.7348 |           0.0916 |
[32m[20230207 15:07:50 @agent_ppo2.py:192][0m |          -0.0001 |           2.5799 |           0.0916 |
[32m[20230207 15:07:50 @agent_ppo2.py:192][0m |           0.0030 |           2.5451 |           0.0916 |
[32m[20230207 15:07:50 @agent_ppo2.py:192][0m |          -0.0157 |           2.4865 |           0.0916 |
[32m[20230207 15:07:50 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: -90.56
[32m[20230207 15:07:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -69.73
[32m[20230207 15:07:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -122.04
[32m[20230207 15:07:50 @agent_ppo2.py:150][0m Total time:       7.15 min
[32m[20230207 15:07:50 @agent_ppo2.py:152][0m 368640 total steps have happened
[32m[20230207 15:07:50 @agent_ppo2.py:128][0m #------------------------ Iteration 180 --------------------------#
[32m[20230207 15:07:51 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:07:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:51 @agent_ppo2.py:192][0m |          -0.0118 |           2.6673 |           0.0903 |
[32m[20230207 15:07:51 @agent_ppo2.py:192][0m |          -0.0042 |           2.0481 |           0.0903 |
[32m[20230207 15:07:51 @agent_ppo2.py:192][0m |          -0.0015 |           1.9552 |           0.0903 |
[32m[20230207 15:07:51 @agent_ppo2.py:192][0m |          -0.0046 |           1.9025 |           0.0903 |
[32m[20230207 15:07:52 @agent_ppo2.py:192][0m |          -0.0134 |           1.8684 |           0.0903 |
[32m[20230207 15:07:52 @agent_ppo2.py:192][0m |          -0.0147 |           1.8519 |           0.0903 |
[32m[20230207 15:07:52 @agent_ppo2.py:192][0m |          -0.0052 |           1.8067 |           0.0903 |
[32m[20230207 15:07:52 @agent_ppo2.py:192][0m |           0.0007 |           1.7988 |           0.0902 |
[32m[20230207 15:07:52 @agent_ppo2.py:192][0m |          -0.0000 |           1.7676 |           0.0902 |
[32m[20230207 15:07:52 @agent_ppo2.py:192][0m |          -0.0200 |           1.7508 |           0.0903 |
[32m[20230207 15:07:52 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -69.23
[32m[20230207 15:07:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -30.45
[32m[20230207 15:07:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -32.78
[32m[20230207 15:07:53 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -32.78
[32m[20230207 15:07:53 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -32.78
[32m[20230207 15:07:53 @agent_ppo2.py:150][0m Total time:       7.19 min
[32m[20230207 15:07:53 @agent_ppo2.py:152][0m 370688 total steps have happened
[32m[20230207 15:07:53 @agent_ppo2.py:128][0m #------------------------ Iteration 181 --------------------------#
[32m[20230207 15:07:54 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:07:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:54 @agent_ppo2.py:192][0m |          -0.0010 |           3.8058 |           0.0909 |
[32m[20230207 15:07:54 @agent_ppo2.py:192][0m |          -0.0177 |           3.1003 |           0.0908 |
[32m[20230207 15:07:54 @agent_ppo2.py:192][0m |          -0.0058 |           2.9167 |           0.0908 |
[32m[20230207 15:07:54 @agent_ppo2.py:192][0m |          -0.0134 |           2.8657 |           0.0908 |
[32m[20230207 15:07:54 @agent_ppo2.py:192][0m |          -0.0026 |           2.8111 |           0.0908 |
[32m[20230207 15:07:54 @agent_ppo2.py:192][0m |           0.0034 |           2.7268 |           0.0907 |
[32m[20230207 15:07:54 @agent_ppo2.py:192][0m |          -0.0131 |           2.6967 |           0.0907 |
[32m[20230207 15:07:54 @agent_ppo2.py:192][0m |          -0.0161 |           2.6720 |           0.0906 |
[32m[20230207 15:07:54 @agent_ppo2.py:192][0m |          -0.0098 |           2.6227 |           0.0906 |
[32m[20230207 15:07:54 @agent_ppo2.py:192][0m |          -0.0181 |           2.6280 |           0.0906 |
[32m[20230207 15:07:54 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: -43.46
[32m[20230207 15:07:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -43.18
[32m[20230207 15:07:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -88.16
[32m[20230207 15:07:55 @agent_ppo2.py:150][0m Total time:       7.23 min
[32m[20230207 15:07:55 @agent_ppo2.py:152][0m 372736 total steps have happened
[32m[20230207 15:07:55 @agent_ppo2.py:128][0m #------------------------ Iteration 182 --------------------------#
[32m[20230207 15:07:56 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:07:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:56 @agent_ppo2.py:192][0m |           0.0079 |           1.3898 |           0.0901 |
[32m[20230207 15:07:56 @agent_ppo2.py:192][0m |          -0.0202 |           1.2569 |           0.0899 |
[32m[20230207 15:07:56 @agent_ppo2.py:192][0m |          -0.0020 |           1.2326 |           0.0898 |
[32m[20230207 15:07:56 @agent_ppo2.py:192][0m |           0.0204 |           1.2357 |           0.0897 |
[32m[20230207 15:07:57 @agent_ppo2.py:192][0m |          -0.0146 |           1.2110 |           0.0897 |
[32m[20230207 15:07:57 @agent_ppo2.py:192][0m |          -0.0034 |           1.1856 |           0.0896 |
[32m[20230207 15:07:57 @agent_ppo2.py:192][0m |          -0.0068 |           1.1825 |           0.0898 |
[32m[20230207 15:07:57 @agent_ppo2.py:192][0m |          -0.0087 |           1.1837 |           0.0896 |
[32m[20230207 15:07:57 @agent_ppo2.py:192][0m |          -0.0636 |           1.4913 |           0.0896 |
[32m[20230207 15:07:57 @agent_ppo2.py:192][0m |          -0.0006 |           1.2399 |           0.0896 |
[32m[20230207 15:07:57 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:07:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: -96.96
[32m[20230207 15:07:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -83.32
[32m[20230207 15:07:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -91.79
[32m[20230207 15:07:58 @agent_ppo2.py:150][0m Total time:       7.28 min
[32m[20230207 15:07:58 @agent_ppo2.py:152][0m 374784 total steps have happened
[32m[20230207 15:07:58 @agent_ppo2.py:128][0m #------------------------ Iteration 183 --------------------------#
[32m[20230207 15:07:59 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:07:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:07:59 @agent_ppo2.py:192][0m |          -0.0038 |           1.3607 |           0.0917 |
[32m[20230207 15:07:59 @agent_ppo2.py:192][0m |           0.0022 |           1.2533 |           0.0917 |
[32m[20230207 15:07:59 @agent_ppo2.py:192][0m |          -0.0121 |           1.2151 |           0.0917 |
[32m[20230207 15:07:59 @agent_ppo2.py:192][0m |          -0.0081 |           1.1906 |           0.0917 |
[32m[20230207 15:07:59 @agent_ppo2.py:192][0m |          -0.0053 |           1.1623 |           0.0917 |
[32m[20230207 15:07:59 @agent_ppo2.py:192][0m |           0.0037 |           1.1507 |           0.0917 |
[32m[20230207 15:07:59 @agent_ppo2.py:192][0m |           0.0171 |           1.2279 |           0.0917 |
[32m[20230207 15:07:59 @agent_ppo2.py:192][0m |          -0.0135 |           1.1543 |           0.0917 |
[32m[20230207 15:07:59 @agent_ppo2.py:192][0m |          -0.0224 |           1.1193 |           0.0917 |
[32m[20230207 15:07:59 @agent_ppo2.py:192][0m |          -0.0055 |           1.1127 |           0.0917 |
[32m[20230207 15:07:59 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:08:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: -107.54
[32m[20230207 15:08:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.01
[32m[20230207 15:08:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -67.33
[32m[20230207 15:08:00 @agent_ppo2.py:150][0m Total time:       7.32 min
[32m[20230207 15:08:00 @agent_ppo2.py:152][0m 376832 total steps have happened
[32m[20230207 15:08:00 @agent_ppo2.py:128][0m #------------------------ Iteration 184 --------------------------#
[32m[20230207 15:08:01 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:08:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:01 @agent_ppo2.py:192][0m |           0.0092 |           2.0619 |           0.0916 |
[32m[20230207 15:08:01 @agent_ppo2.py:192][0m |           0.0076 |           1.3206 |           0.0915 |
[32m[20230207 15:08:01 @agent_ppo2.py:192][0m |           0.0003 |           1.2537 |           0.0915 |
[32m[20230207 15:08:01 @agent_ppo2.py:192][0m |          -0.0022 |           1.1967 |           0.0914 |
[32m[20230207 15:08:02 @agent_ppo2.py:192][0m |          -0.0353 |           1.1966 |           0.0913 |
[32m[20230207 15:08:02 @agent_ppo2.py:192][0m |          -0.0039 |           1.3177 |           0.0914 |
[32m[20230207 15:08:02 @agent_ppo2.py:192][0m |          -0.0124 |           1.1338 |           0.0914 |
[32m[20230207 15:08:02 @agent_ppo2.py:192][0m |           0.0009 |           1.1318 |           0.0914 |
[32m[20230207 15:08:02 @agent_ppo2.py:192][0m |          -0.0036 |           1.1085 |           0.0914 |
[32m[20230207 15:08:02 @agent_ppo2.py:192][0m |          -0.0087 |           1.0833 |           0.0913 |
[32m[20230207 15:08:02 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:08:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -119.13
[32m[20230207 15:08:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -108.28
[32m[20230207 15:08:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -69.51
[32m[20230207 15:08:03 @agent_ppo2.py:150][0m Total time:       7.36 min
[32m[20230207 15:08:03 @agent_ppo2.py:152][0m 378880 total steps have happened
[32m[20230207 15:08:03 @agent_ppo2.py:128][0m #------------------------ Iteration 185 --------------------------#
[32m[20230207 15:08:03 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:08:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:04 @agent_ppo2.py:192][0m |          -0.0091 |           1.1758 |           0.0905 |
[32m[20230207 15:08:04 @agent_ppo2.py:192][0m |           0.0004 |           1.0599 |           0.0906 |
[32m[20230207 15:08:04 @agent_ppo2.py:192][0m |          -0.0061 |           1.0323 |           0.0906 |
[32m[20230207 15:08:04 @agent_ppo2.py:192][0m |           0.0022 |           1.0038 |           0.0906 |
[32m[20230207 15:08:04 @agent_ppo2.py:192][0m |           0.0070 |           0.9870 |           0.0906 |
[32m[20230207 15:08:04 @agent_ppo2.py:192][0m |           0.0240 |           0.9754 |           0.0906 |
[32m[20230207 15:08:04 @agent_ppo2.py:192][0m |          -0.0057 |           0.9606 |           0.0906 |
[32m[20230207 15:08:04 @agent_ppo2.py:192][0m |          -0.0053 |           0.9413 |           0.0906 |
[32m[20230207 15:08:04 @agent_ppo2.py:192][0m |          -0.0056 |           0.9339 |           0.0906 |
[32m[20230207 15:08:04 @agent_ppo2.py:192][0m |          -0.0053 |           0.9177 |           0.0907 |
[32m[20230207 15:08:04 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:08:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -85.88
[32m[20230207 15:08:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -68.97
[32m[20230207 15:08:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -115.02
[32m[20230207 15:08:05 @agent_ppo2.py:150][0m Total time:       7.40 min
[32m[20230207 15:08:05 @agent_ppo2.py:152][0m 380928 total steps have happened
[32m[20230207 15:08:05 @agent_ppo2.py:128][0m #------------------------ Iteration 186 --------------------------#
[32m[20230207 15:08:06 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:08:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:06 @agent_ppo2.py:192][0m |          -0.0026 |           1.4260 |           0.0921 |
[32m[20230207 15:08:06 @agent_ppo2.py:192][0m |          -0.0013 |           1.1768 |           0.0922 |
[32m[20230207 15:08:06 @agent_ppo2.py:192][0m |          -0.0048 |           1.1374 |           0.0922 |
[32m[20230207 15:08:06 @agent_ppo2.py:192][0m |          -0.0075 |           1.1189 |           0.0922 |
[32m[20230207 15:08:06 @agent_ppo2.py:192][0m |          -0.0079 |           1.1027 |           0.0923 |
[32m[20230207 15:08:07 @agent_ppo2.py:192][0m |          -0.0260 |           1.1171 |           0.0923 |
[32m[20230207 15:08:07 @agent_ppo2.py:192][0m |          -0.0144 |           1.0957 |           0.0923 |
[32m[20230207 15:08:07 @agent_ppo2.py:192][0m |          -0.0061 |           1.0693 |           0.0923 |
[32m[20230207 15:08:07 @agent_ppo2.py:192][0m |          -0.0086 |           1.0578 |           0.0924 |
[32m[20230207 15:08:07 @agent_ppo2.py:192][0m |          -0.0020 |           1.0556 |           0.0924 |
[32m[20230207 15:08:07 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:08:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -50.45
[32m[20230207 15:08:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -35.10
[32m[20230207 15:08:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -96.73
[32m[20230207 15:08:08 @agent_ppo2.py:150][0m Total time:       7.44 min
[32m[20230207 15:08:08 @agent_ppo2.py:152][0m 382976 total steps have happened
[32m[20230207 15:08:08 @agent_ppo2.py:128][0m #------------------------ Iteration 187 --------------------------#
[32m[20230207 15:08:09 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:08:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:09 @agent_ppo2.py:192][0m |           0.0004 |           9.4574 |           0.0945 |
[32m[20230207 15:08:09 @agent_ppo2.py:192][0m |          -0.0007 |           1.6389 |           0.0945 |
[32m[20230207 15:08:09 @agent_ppo2.py:192][0m |          -0.0018 |           1.2756 |           0.0944 |
[32m[20230207 15:08:09 @agent_ppo2.py:192][0m |          -0.0027 |           1.1250 |           0.0943 |
[32m[20230207 15:08:09 @agent_ppo2.py:192][0m |          -0.0031 |           1.0364 |           0.0943 |
[32m[20230207 15:08:09 @agent_ppo2.py:192][0m |          -0.0031 |           1.0006 |           0.0943 |
[32m[20230207 15:08:09 @agent_ppo2.py:192][0m |          -0.0036 |           0.9776 |           0.0942 |
[32m[20230207 15:08:09 @agent_ppo2.py:192][0m |          -0.0039 |           0.9661 |           0.0942 |
[32m[20230207 15:08:09 @agent_ppo2.py:192][0m |          -0.0041 |           0.9481 |           0.0941 |
[32m[20230207 15:08:09 @agent_ppo2.py:192][0m |          -0.0044 |           0.9432 |           0.0941 |
[32m[20230207 15:08:09 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:08:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -77.13
[32m[20230207 15:08:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -58.08
[32m[20230207 15:08:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -38.38
[32m[20230207 15:08:10 @agent_ppo2.py:150][0m Total time:       7.48 min
[32m[20230207 15:08:10 @agent_ppo2.py:152][0m 385024 total steps have happened
[32m[20230207 15:08:10 @agent_ppo2.py:128][0m #------------------------ Iteration 188 --------------------------#
[32m[20230207 15:08:11 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:08:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:11 @agent_ppo2.py:192][0m |           0.0071 |           2.6717 |           0.0916 |
[32m[20230207 15:08:11 @agent_ppo2.py:192][0m |          -0.0077 |           2.1208 |           0.0916 |
[32m[20230207 15:08:11 @agent_ppo2.py:192][0m |          -0.0089 |           2.0514 |           0.0915 |
[32m[20230207 15:08:11 @agent_ppo2.py:192][0m |          -0.0034 |           1.9789 |           0.0915 |
[32m[20230207 15:08:11 @agent_ppo2.py:192][0m |          -0.0214 |           1.9334 |           0.0915 |
[32m[20230207 15:08:12 @agent_ppo2.py:192][0m |           0.0062 |           1.9125 |           0.0914 |
[32m[20230207 15:08:12 @agent_ppo2.py:192][0m |          -0.0079 |           1.8803 |           0.0914 |
[32m[20230207 15:08:12 @agent_ppo2.py:192][0m |          -0.0116 |           1.8585 |           0.0914 |
[32m[20230207 15:08:12 @agent_ppo2.py:192][0m |          -0.0154 |           1.8579 |           0.0914 |
[32m[20230207 15:08:12 @agent_ppo2.py:192][0m |          -0.0029 |           1.8524 |           0.0914 |
[32m[20230207 15:08:12 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:08:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -87.53
[32m[20230207 15:08:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -82.83
[32m[20230207 15:08:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -78.16
[32m[20230207 15:08:13 @agent_ppo2.py:150][0m Total time:       7.52 min
[32m[20230207 15:08:13 @agent_ppo2.py:152][0m 387072 total steps have happened
[32m[20230207 15:08:13 @agent_ppo2.py:128][0m #------------------------ Iteration 189 --------------------------#
[32m[20230207 15:08:13 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:08:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:14 @agent_ppo2.py:192][0m |          -0.0008 |           1.5986 |           0.0908 |
[32m[20230207 15:08:14 @agent_ppo2.py:192][0m |           0.0030 |           1.4497 |           0.0908 |
[32m[20230207 15:08:14 @agent_ppo2.py:192][0m |          -0.0006 |           1.4116 |           0.0907 |
[32m[20230207 15:08:14 @agent_ppo2.py:192][0m |          -0.0026 |           1.3832 |           0.0907 |
[32m[20230207 15:08:14 @agent_ppo2.py:192][0m |          -0.0112 |           1.3497 |           0.0907 |
[32m[20230207 15:08:14 @agent_ppo2.py:192][0m |          -0.0094 |           1.3250 |           0.0907 |
[32m[20230207 15:08:14 @agent_ppo2.py:192][0m |           0.0017 |           1.3230 |           0.0907 |
[32m[20230207 15:08:14 @agent_ppo2.py:192][0m |           0.0030 |           1.3181 |           0.0907 |
[32m[20230207 15:08:14 @agent_ppo2.py:192][0m |           0.0216 |           1.4360 |           0.0906 |
[32m[20230207 15:08:14 @agent_ppo2.py:192][0m |           0.0003 |           1.2782 |           0.0906 |
[32m[20230207 15:08:14 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:08:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: -63.59
[32m[20230207 15:08:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -51.67
[32m[20230207 15:08:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -62.30
[32m[20230207 15:08:15 @agent_ppo2.py:150][0m Total time:       7.56 min
[32m[20230207 15:08:15 @agent_ppo2.py:152][0m 389120 total steps have happened
[32m[20230207 15:08:15 @agent_ppo2.py:128][0m #------------------------ Iteration 190 --------------------------#
[32m[20230207 15:08:16 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:08:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:16 @agent_ppo2.py:192][0m |           0.0017 |           2.5721 |           0.0928 |
[32m[20230207 15:08:16 @agent_ppo2.py:192][0m |           0.0009 |           1.3675 |           0.0928 |
[32m[20230207 15:08:16 @agent_ppo2.py:192][0m |          -0.0005 |           1.2692 |           0.0928 |
[32m[20230207 15:08:16 @agent_ppo2.py:192][0m |           0.0071 |           1.2576 |           0.0928 |
[32m[20230207 15:08:16 @agent_ppo2.py:192][0m |          -0.0049 |           1.1844 |           0.0927 |
[32m[20230207 15:08:16 @agent_ppo2.py:192][0m |          -0.0079 |           1.1117 |           0.0927 |
[32m[20230207 15:08:16 @agent_ppo2.py:192][0m |          -0.0059 |           1.1089 |           0.0927 |
[32m[20230207 15:08:17 @agent_ppo2.py:192][0m |          -0.0015 |           1.0821 |           0.0926 |
[32m[20230207 15:08:17 @agent_ppo2.py:192][0m |          -0.0103 |           1.1171 |           0.0926 |
[32m[20230207 15:08:17 @agent_ppo2.py:192][0m |          -0.0108 |           1.0266 |           0.0926 |
[32m[20230207 15:08:17 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:08:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -65.04
[32m[20230207 15:08:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -30.49
[32m[20230207 15:08:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -96.99
[32m[20230207 15:08:18 @agent_ppo2.py:150][0m Total time:       7.61 min
[32m[20230207 15:08:18 @agent_ppo2.py:152][0m 391168 total steps have happened
[32m[20230207 15:08:18 @agent_ppo2.py:128][0m #------------------------ Iteration 191 --------------------------#
[32m[20230207 15:08:18 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:08:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:18 @agent_ppo2.py:192][0m |           0.0077 |           3.2381 |           0.0946 |
[32m[20230207 15:08:19 @agent_ppo2.py:192][0m |          -0.0123 |           2.5246 |           0.0946 |
[32m[20230207 15:08:19 @agent_ppo2.py:192][0m |          -0.0073 |           2.3460 |           0.0947 |
[32m[20230207 15:08:19 @agent_ppo2.py:192][0m |          -0.0111 |           2.2332 |           0.0946 |
[32m[20230207 15:08:19 @agent_ppo2.py:192][0m |          -0.0014 |           2.1388 |           0.0947 |
[32m[20230207 15:08:19 @agent_ppo2.py:192][0m |          -0.0093 |           1.9781 |           0.0947 |
[32m[20230207 15:08:19 @agent_ppo2.py:192][0m |          -0.0161 |           1.9472 |           0.0947 |
[32m[20230207 15:08:19 @agent_ppo2.py:192][0m |          -0.0076 |           1.9408 |           0.0947 |
[32m[20230207 15:08:19 @agent_ppo2.py:192][0m |          -0.0178 |           1.9609 |           0.0948 |
[32m[20230207 15:08:19 @agent_ppo2.py:192][0m |          -0.0044 |           1.8799 |           0.0948 |
[32m[20230207 15:08:19 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:08:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -71.58
[32m[20230207 15:08:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -62.69
[32m[20230207 15:08:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -82.05
[32m[20230207 15:08:20 @agent_ppo2.py:150][0m Total time:       7.65 min
[32m[20230207 15:08:20 @agent_ppo2.py:152][0m 393216 total steps have happened
[32m[20230207 15:08:20 @agent_ppo2.py:128][0m #------------------------ Iteration 192 --------------------------#
[32m[20230207 15:08:21 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:08:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:21 @agent_ppo2.py:192][0m |           0.0016 |          10.9989 |           0.0933 |
[32m[20230207 15:08:21 @agent_ppo2.py:192][0m |          -0.0058 |           5.6666 |           0.0932 |
[32m[20230207 15:08:21 @agent_ppo2.py:192][0m |          -0.0111 |           5.1888 |           0.0931 |
[32m[20230207 15:08:21 @agent_ppo2.py:192][0m |          -0.0084 |           4.8512 |           0.0930 |
[32m[20230207 15:08:21 @agent_ppo2.py:192][0m |          -0.0113 |           4.6429 |           0.0930 |
[32m[20230207 15:08:21 @agent_ppo2.py:192][0m |          -0.0097 |           4.5544 |           0.0929 |
[32m[20230207 15:08:21 @agent_ppo2.py:192][0m |          -0.0155 |           4.5502 |           0.0929 |
[32m[20230207 15:08:22 @agent_ppo2.py:192][0m |          -0.0159 |           4.2866 |           0.0929 |
[32m[20230207 15:08:22 @agent_ppo2.py:192][0m |          -0.0094 |           4.1640 |           0.0928 |
[32m[20230207 15:08:22 @agent_ppo2.py:192][0m |          -0.0159 |           4.0623 |           0.0928 |
[32m[20230207 15:08:22 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:08:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -95.24
[32m[20230207 15:08:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -75.16
[32m[20230207 15:08:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -97.79
[32m[20230207 15:08:23 @agent_ppo2.py:150][0m Total time:       7.69 min
[32m[20230207 15:08:23 @agent_ppo2.py:152][0m 395264 total steps have happened
[32m[20230207 15:08:23 @agent_ppo2.py:128][0m #------------------------ Iteration 193 --------------------------#
[32m[20230207 15:08:23 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:08:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:24 @agent_ppo2.py:192][0m |           0.0013 |          12.1351 |           0.0946 |
[32m[20230207 15:08:24 @agent_ppo2.py:192][0m |          -0.0041 |           6.3948 |           0.0946 |
[32m[20230207 15:08:24 @agent_ppo2.py:192][0m |          -0.0067 |           5.1745 |           0.0945 |
[32m[20230207 15:08:24 @agent_ppo2.py:192][0m |          -0.0164 |           4.5430 |           0.0945 |
[32m[20230207 15:08:24 @agent_ppo2.py:192][0m |          -0.0067 |           4.1010 |           0.0944 |
[32m[20230207 15:08:24 @agent_ppo2.py:192][0m |          -0.0093 |           3.7954 |           0.0942 |
[32m[20230207 15:08:24 @agent_ppo2.py:192][0m |          -0.0172 |           3.6560 |           0.0942 |
[32m[20230207 15:08:24 @agent_ppo2.py:192][0m |          -0.0146 |           3.4556 |           0.0942 |
[32m[20230207 15:08:24 @agent_ppo2.py:192][0m |          -0.0230 |           3.2002 |           0.0941 |
[32m[20230207 15:08:24 @agent_ppo2.py:192][0m |          -0.0160 |           3.3016 |           0.0941 |
[32m[20230207 15:08:24 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:08:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -64.60
[32m[20230207 15:08:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -8.48
[32m[20230207 15:08:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -65.16
[32m[20230207 15:08:25 @agent_ppo2.py:150][0m Total time:       7.73 min
[32m[20230207 15:08:25 @agent_ppo2.py:152][0m 397312 total steps have happened
[32m[20230207 15:08:25 @agent_ppo2.py:128][0m #------------------------ Iteration 194 --------------------------#
[32m[20230207 15:08:26 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:08:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:26 @agent_ppo2.py:192][0m |          -0.0016 |           5.4355 |           0.0951 |
[32m[20230207 15:08:26 @agent_ppo2.py:192][0m |          -0.0046 |           3.7380 |           0.0951 |
[32m[20230207 15:08:26 @agent_ppo2.py:192][0m |          -0.0067 |           3.3728 |           0.0951 |
[32m[20230207 15:08:26 @agent_ppo2.py:192][0m |          -0.0082 |           3.2419 |           0.0950 |
[32m[20230207 15:08:26 @agent_ppo2.py:192][0m |          -0.0093 |           3.1807 |           0.0950 |
[32m[20230207 15:08:27 @agent_ppo2.py:192][0m |          -0.0099 |           3.0927 |           0.0950 |
[32m[20230207 15:08:27 @agent_ppo2.py:192][0m |          -0.0100 |           3.0743 |           0.0950 |
[32m[20230207 15:08:27 @agent_ppo2.py:192][0m |          -0.0108 |           2.9953 |           0.0950 |
[32m[20230207 15:08:27 @agent_ppo2.py:192][0m |          -0.0111 |           2.9606 |           0.0950 |
[32m[20230207 15:08:27 @agent_ppo2.py:192][0m |          -0.0111 |           2.9445 |           0.0951 |
[32m[20230207 15:08:27 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:08:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: -87.75
[32m[20230207 15:08:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -31.63
[32m[20230207 15:08:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -100.36
[32m[20230207 15:08:28 @agent_ppo2.py:150][0m Total time:       7.77 min
[32m[20230207 15:08:28 @agent_ppo2.py:152][0m 399360 total steps have happened
[32m[20230207 15:08:28 @agent_ppo2.py:128][0m #------------------------ Iteration 195 --------------------------#
[32m[20230207 15:08:28 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 15:08:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:28 @agent_ppo2.py:192][0m |          -0.0007 |          14.3676 |           0.0946 |
[32m[20230207 15:08:29 @agent_ppo2.py:192][0m |          -0.0063 |           9.6193 |           0.0947 |
[32m[20230207 15:08:29 @agent_ppo2.py:192][0m |          -0.0117 |           7.6246 |           0.0947 |
[32m[20230207 15:08:29 @agent_ppo2.py:192][0m |          -0.0120 |           6.4762 |           0.0947 |
[32m[20230207 15:08:29 @agent_ppo2.py:192][0m |          -0.0166 |           5.3832 |           0.0947 |
[32m[20230207 15:08:29 @agent_ppo2.py:192][0m |          -0.0182 |           4.6176 |           0.0947 |
[32m[20230207 15:08:29 @agent_ppo2.py:192][0m |          -0.0108 |           4.2062 |           0.0947 |
[32m[20230207 15:08:29 @agent_ppo2.py:192][0m |          -0.0132 |           3.8386 |           0.0948 |
[32m[20230207 15:08:29 @agent_ppo2.py:192][0m |          -0.0156 |           3.5374 |           0.0948 |
[32m[20230207 15:08:29 @agent_ppo2.py:192][0m |          -0.0165 |           3.3560 |           0.0948 |
[32m[20230207 15:08:29 @agent_ppo2.py:137][0m Policy update time: 0.72 s
[32m[20230207 15:08:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.40
[32m[20230207 15:08:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -50.63
[32m[20230207 15:08:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -42.78
[32m[20230207 15:08:30 @agent_ppo2.py:150][0m Total time:       7.81 min
[32m[20230207 15:08:30 @agent_ppo2.py:152][0m 401408 total steps have happened
[32m[20230207 15:08:30 @agent_ppo2.py:128][0m #------------------------ Iteration 196 --------------------------#
[32m[20230207 15:08:31 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:08:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:31 @agent_ppo2.py:192][0m |          -0.0000 |           6.8499 |           0.0946 |
[32m[20230207 15:08:31 @agent_ppo2.py:192][0m |          -0.0048 |           2.6237 |           0.0944 |
[32m[20230207 15:08:31 @agent_ppo2.py:192][0m |          -0.0068 |           2.3198 |           0.0944 |
[32m[20230207 15:08:31 @agent_ppo2.py:192][0m |          -0.0086 |           2.1669 |           0.0943 |
[32m[20230207 15:08:31 @agent_ppo2.py:192][0m |          -0.0084 |           2.0765 |           0.0943 |
[32m[20230207 15:08:31 @agent_ppo2.py:192][0m |          -0.0101 |           1.9688 |           0.0942 |
[32m[20230207 15:08:31 @agent_ppo2.py:192][0m |          -0.0115 |           1.8939 |           0.0942 |
[32m[20230207 15:08:31 @agent_ppo2.py:192][0m |          -0.0119 |           1.8280 |           0.0941 |
[32m[20230207 15:08:32 @agent_ppo2.py:192][0m |          -0.0118 |           1.7775 |           0.0941 |
[32m[20230207 15:08:32 @agent_ppo2.py:192][0m |          -0.0106 |           1.7572 |           0.0941 |
[32m[20230207 15:08:32 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:08:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -76.05
[32m[20230207 15:08:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 9.76
[32m[20230207 15:08:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -90.36
[32m[20230207 15:08:32 @agent_ppo2.py:150][0m Total time:       7.85 min
[32m[20230207 15:08:32 @agent_ppo2.py:152][0m 403456 total steps have happened
[32m[20230207 15:08:32 @agent_ppo2.py:128][0m #------------------------ Iteration 197 --------------------------#
[32m[20230207 15:08:33 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:08:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:33 @agent_ppo2.py:192][0m |           0.0119 |           2.7113 |           0.0933 |
[32m[20230207 15:08:33 @agent_ppo2.py:192][0m |          -0.0807 |           4.1415 |           0.0933 |
[32m[20230207 15:08:34 @agent_ppo2.py:192][0m |          -0.0204 |           2.1569 |           0.0932 |
[32m[20230207 15:08:34 @agent_ppo2.py:192][0m |           0.0057 |           1.9831 |           0.0933 |
[32m[20230207 15:08:34 @agent_ppo2.py:192][0m |          -0.0126 |           1.9160 |           0.0933 |
[32m[20230207 15:08:34 @agent_ppo2.py:192][0m |          -0.0006 |           1.8592 |           0.0934 |
[32m[20230207 15:08:34 @agent_ppo2.py:192][0m |          -0.0189 |           1.8330 |           0.0934 |
[32m[20230207 15:08:34 @agent_ppo2.py:192][0m |          -0.0801 |           3.2852 |           0.0934 |
[32m[20230207 15:08:34 @agent_ppo2.py:192][0m |          -0.0078 |           1.8539 |           0.0935 |
[32m[20230207 15:08:34 @agent_ppo2.py:192][0m |          -0.0105 |           1.7483 |           0.0935 |
[32m[20230207 15:08:34 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:08:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: -53.45
[32m[20230207 15:08:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -23.05
[32m[20230207 15:08:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -93.64
[32m[20230207 15:08:35 @agent_ppo2.py:150][0m Total time:       7.90 min
[32m[20230207 15:08:35 @agent_ppo2.py:152][0m 405504 total steps have happened
[32m[20230207 15:08:35 @agent_ppo2.py:128][0m #------------------------ Iteration 198 --------------------------#
[32m[20230207 15:08:36 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:08:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:36 @agent_ppo2.py:192][0m |           0.0005 |           1.2631 |           0.0918 |
[32m[20230207 15:08:36 @agent_ppo2.py:192][0m |          -0.0062 |           1.1953 |           0.0918 |
[32m[20230207 15:08:36 @agent_ppo2.py:192][0m |           0.0005 |           1.1747 |           0.0918 |
[32m[20230207 15:08:36 @agent_ppo2.py:192][0m |          -0.0098 |           1.1603 |           0.0918 |
[32m[20230207 15:08:36 @agent_ppo2.py:192][0m |          -0.0110 |           1.1495 |           0.0917 |
[32m[20230207 15:08:36 @agent_ppo2.py:192][0m |          -0.0017 |           1.1322 |           0.0917 |
[32m[20230207 15:08:36 @agent_ppo2.py:192][0m |          -0.0005 |           1.1236 |           0.0917 |
[32m[20230207 15:08:36 @agent_ppo2.py:192][0m |          -0.0094 |           1.1117 |           0.0917 |
[32m[20230207 15:08:36 @agent_ppo2.py:192][0m |          -0.0201 |           1.1108 |           0.0917 |
[32m[20230207 15:08:37 @agent_ppo2.py:192][0m |          -0.0183 |           1.1033 |           0.0918 |
[32m[20230207 15:08:37 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:08:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: -105.62
[32m[20230207 15:08:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -77.24
[32m[20230207 15:08:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -98.36
[32m[20230207 15:08:37 @agent_ppo2.py:150][0m Total time:       7.94 min
[32m[20230207 15:08:37 @agent_ppo2.py:152][0m 407552 total steps have happened
[32m[20230207 15:08:37 @agent_ppo2.py:128][0m #------------------------ Iteration 199 --------------------------#
[32m[20230207 15:08:38 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:08:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:38 @agent_ppo2.py:192][0m |           0.0062 |           1.4183 |           0.0954 |
[32m[20230207 15:08:38 @agent_ppo2.py:192][0m |          -0.0012 |           1.3258 |           0.0951 |
[32m[20230207 15:08:38 @agent_ppo2.py:192][0m |           0.0004 |           1.3193 |           0.0951 |
[32m[20230207 15:08:39 @agent_ppo2.py:192][0m |          -0.0016 |           1.2982 |           0.0951 |
[32m[20230207 15:08:39 @agent_ppo2.py:192][0m |          -0.0185 |           1.2914 |           0.0951 |
[32m[20230207 15:08:39 @agent_ppo2.py:192][0m |           0.0028 |           1.2802 |           0.0951 |
[32m[20230207 15:08:39 @agent_ppo2.py:192][0m |          -0.0012 |           1.2765 |           0.0952 |
[32m[20230207 15:08:39 @agent_ppo2.py:192][0m |          -0.0070 |           1.2636 |           0.0952 |
[32m[20230207 15:08:39 @agent_ppo2.py:192][0m |          -0.0108 |           1.2563 |           0.0952 |
[32m[20230207 15:08:39 @agent_ppo2.py:192][0m |          -0.0225 |           1.2527 |           0.0953 |
[32m[20230207 15:08:39 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:08:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: -91.39
[32m[20230207 15:08:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -76.16
[32m[20230207 15:08:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -111.54
[32m[20230207 15:08:40 @agent_ppo2.py:150][0m Total time:       7.98 min
[32m[20230207 15:08:40 @agent_ppo2.py:152][0m 409600 total steps have happened
[32m[20230207 15:08:40 @agent_ppo2.py:128][0m #------------------------ Iteration 200 --------------------------#
[32m[20230207 15:08:40 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:08:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:40 @agent_ppo2.py:192][0m |          -0.0000 |          40.2834 |           0.0993 |
[32m[20230207 15:08:41 @agent_ppo2.py:192][0m |          -0.0064 |          13.5213 |           0.0992 |
[32m[20230207 15:08:41 @agent_ppo2.py:192][0m |          -0.0087 |           7.2747 |           0.0991 |
[32m[20230207 15:08:41 @agent_ppo2.py:192][0m |          -0.0101 |           6.3322 |           0.0990 |
[32m[20230207 15:08:41 @agent_ppo2.py:192][0m |          -0.0106 |           5.2999 |           0.0991 |
[32m[20230207 15:08:41 @agent_ppo2.py:192][0m |          -0.0119 |           4.9837 |           0.0990 |
[32m[20230207 15:08:41 @agent_ppo2.py:192][0m |          -0.0122 |           4.7603 |           0.0990 |
[32m[20230207 15:08:41 @agent_ppo2.py:192][0m |          -0.0141 |           4.4666 |           0.0990 |
[32m[20230207 15:08:41 @agent_ppo2.py:192][0m |          -0.0148 |           4.0550 |           0.0989 |
[32m[20230207 15:08:41 @agent_ppo2.py:192][0m |          -0.0150 |           3.8491 |           0.0989 |
[32m[20230207 15:08:41 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:08:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: -103.11
[32m[20230207 15:08:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -44.69
[32m[20230207 15:08:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -63.09
[32m[20230207 15:08:42 @agent_ppo2.py:150][0m Total time:       8.01 min
[32m[20230207 15:08:42 @agent_ppo2.py:152][0m 411648 total steps have happened
[32m[20230207 15:08:42 @agent_ppo2.py:128][0m #------------------------ Iteration 201 --------------------------#
[32m[20230207 15:08:43 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:08:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:43 @agent_ppo2.py:192][0m |           0.0076 |           1.4434 |           0.0948 |
[32m[20230207 15:08:43 @agent_ppo2.py:192][0m |          -0.0043 |           1.1830 |           0.0947 |
[32m[20230207 15:08:43 @agent_ppo2.py:192][0m |          -0.0148 |           1.1444 |           0.0947 |
[32m[20230207 15:08:43 @agent_ppo2.py:192][0m |           0.0126 |           1.1291 |           0.0946 |
[32m[20230207 15:08:43 @agent_ppo2.py:192][0m |          -0.1007 |           2.1869 |           0.0945 |
[32m[20230207 15:08:43 @agent_ppo2.py:192][0m |          -0.0001 |           1.1994 |           0.0941 |
[32m[20230207 15:08:43 @agent_ppo2.py:192][0m |          -0.0041 |           1.0886 |           0.0942 |
[32m[20230207 15:08:43 @agent_ppo2.py:192][0m |          -0.0093 |           1.0740 |           0.0943 |
[32m[20230207 15:08:43 @agent_ppo2.py:192][0m |          -0.0068 |           1.0697 |           0.0943 |
[32m[20230207 15:08:43 @agent_ppo2.py:192][0m |          -0.0076 |           1.0612 |           0.0943 |
[32m[20230207 15:08:43 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:08:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: -67.32
[32m[20230207 15:08:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -61.28
[32m[20230207 15:08:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -80.07
[32m[20230207 15:08:44 @agent_ppo2.py:150][0m Total time:       8.05 min
[32m[20230207 15:08:44 @agent_ppo2.py:152][0m 413696 total steps have happened
[32m[20230207 15:08:44 @agent_ppo2.py:128][0m #------------------------ Iteration 202 --------------------------#
[32m[20230207 15:08:45 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:08:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:45 @agent_ppo2.py:192][0m |           0.0025 |           1.4046 |           0.0951 |
[32m[20230207 15:08:45 @agent_ppo2.py:192][0m |          -0.0248 |           1.1574 |           0.0951 |
[32m[20230207 15:08:45 @agent_ppo2.py:192][0m |           0.0218 |           1.1711 |           0.0952 |
[32m[20230207 15:08:45 @agent_ppo2.py:192][0m |          -0.0114 |           1.1142 |           0.0950 |
[32m[20230207 15:08:45 @agent_ppo2.py:192][0m |          -0.0091 |           1.0647 |           0.0951 |
[32m[20230207 15:08:45 @agent_ppo2.py:192][0m |           0.0029 |           1.0616 |           0.0951 |
[32m[20230207 15:08:45 @agent_ppo2.py:192][0m |           0.0045 |           1.0641 |           0.0951 |
[32m[20230207 15:08:45 @agent_ppo2.py:192][0m |          -0.0067 |           1.0318 |           0.0951 |
[32m[20230207 15:08:45 @agent_ppo2.py:192][0m |          -0.0062 |           1.0222 |           0.0951 |
[32m[20230207 15:08:46 @agent_ppo2.py:192][0m |          -0.0020 |           1.0211 |           0.0951 |
[32m[20230207 15:08:46 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:08:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -88.75
[32m[20230207 15:08:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -86.80
[32m[20230207 15:08:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -75.35
[32m[20230207 15:08:46 @agent_ppo2.py:150][0m Total time:       8.09 min
[32m[20230207 15:08:46 @agent_ppo2.py:152][0m 415744 total steps have happened
[32m[20230207 15:08:46 @agent_ppo2.py:128][0m #------------------------ Iteration 203 --------------------------#
[32m[20230207 15:08:47 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:08:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:47 @agent_ppo2.py:192][0m |          -0.0007 |           5.1275 |           0.0969 |
[32m[20230207 15:08:47 @agent_ppo2.py:192][0m |          -0.0054 |           1.7142 |           0.0968 |
[32m[20230207 15:08:48 @agent_ppo2.py:192][0m |          -0.0047 |           1.5125 |           0.0968 |
[32m[20230207 15:08:48 @agent_ppo2.py:192][0m |          -0.0058 |           1.4242 |           0.0967 |
[32m[20230207 15:08:48 @agent_ppo2.py:192][0m |          -0.0057 |           1.3207 |           0.0967 |
[32m[20230207 15:08:48 @agent_ppo2.py:192][0m |          -0.0071 |           1.2480 |           0.0967 |
[32m[20230207 15:08:48 @agent_ppo2.py:192][0m |          -0.0084 |           1.1926 |           0.0967 |
[32m[20230207 15:08:48 @agent_ppo2.py:192][0m |          -0.0079 |           1.1430 |           0.0967 |
[32m[20230207 15:08:48 @agent_ppo2.py:192][0m |          -0.0109 |           1.1112 |           0.0966 |
[32m[20230207 15:08:48 @agent_ppo2.py:192][0m |          -0.0088 |           1.0733 |           0.0967 |
[32m[20230207 15:08:48 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:08:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: -102.30
[32m[20230207 15:08:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -55.93
[32m[20230207 15:08:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -76.81
[32m[20230207 15:08:49 @agent_ppo2.py:150][0m Total time:       8.12 min
[32m[20230207 15:08:49 @agent_ppo2.py:152][0m 417792 total steps have happened
[32m[20230207 15:08:49 @agent_ppo2.py:128][0m #------------------------ Iteration 204 --------------------------#
[32m[20230207 15:08:49 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:08:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:50 @agent_ppo2.py:192][0m |           0.0019 |           6.6079 |           0.0941 |
[32m[20230207 15:08:50 @agent_ppo2.py:192][0m |           0.0113 |           5.3937 |           0.0941 |
[32m[20230207 15:08:50 @agent_ppo2.py:192][0m |          -0.0048 |           4.7878 |           0.0941 |
[32m[20230207 15:08:50 @agent_ppo2.py:192][0m |          -0.0067 |           4.7052 |           0.0941 |
[32m[20230207 15:08:50 @agent_ppo2.py:192][0m |          -0.0092 |           4.6065 |           0.0941 |
[32m[20230207 15:08:50 @agent_ppo2.py:192][0m |          -0.0051 |           4.5612 |           0.0941 |
[32m[20230207 15:08:50 @agent_ppo2.py:192][0m |          -0.0134 |           4.5248 |           0.0942 |
[32m[20230207 15:08:50 @agent_ppo2.py:192][0m |          -0.0102 |           4.5165 |           0.0942 |
[32m[20230207 15:08:50 @agent_ppo2.py:192][0m |          -0.0119 |           4.4633 |           0.0942 |
[32m[20230207 15:08:50 @agent_ppo2.py:192][0m |          -0.0131 |           4.4029 |           0.0942 |
[32m[20230207 15:08:50 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:08:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: -71.30
[32m[20230207 15:08:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -28.16
[32m[20230207 15:08:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -61.51
[32m[20230207 15:08:51 @agent_ppo2.py:150][0m Total time:       8.16 min
[32m[20230207 15:08:51 @agent_ppo2.py:152][0m 419840 total steps have happened
[32m[20230207 15:08:51 @agent_ppo2.py:128][0m #------------------------ Iteration 205 --------------------------#
[32m[20230207 15:08:52 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:08:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:52 @agent_ppo2.py:192][0m |           0.0038 |           5.2046 |           0.0942 |
[32m[20230207 15:08:52 @agent_ppo2.py:192][0m |          -0.0004 |           2.6651 |           0.0942 |
[32m[20230207 15:08:52 @agent_ppo2.py:192][0m |          -0.0035 |           2.2529 |           0.0941 |
[32m[20230207 15:08:52 @agent_ppo2.py:192][0m |          -0.0067 |           2.2093 |           0.0940 |
[32m[20230207 15:08:52 @agent_ppo2.py:192][0m |          -0.0071 |           2.0467 |           0.0939 |
[32m[20230207 15:08:52 @agent_ppo2.py:192][0m |          -0.0088 |           2.0042 |           0.0939 |
[32m[20230207 15:08:53 @agent_ppo2.py:192][0m |          -0.0099 |           1.9970 |           0.0938 |
[32m[20230207 15:08:53 @agent_ppo2.py:192][0m |          -0.0094 |           1.9491 |           0.0938 |
[32m[20230207 15:08:53 @agent_ppo2.py:192][0m |          -0.0079 |           1.9931 |           0.0937 |
[32m[20230207 15:08:53 @agent_ppo2.py:192][0m |          -0.0076 |           1.9486 |           0.0936 |
[32m[20230207 15:08:53 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:08:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: -112.78
[32m[20230207 15:08:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -91.64
[32m[20230207 15:08:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -65.83
[32m[20230207 15:08:54 @agent_ppo2.py:150][0m Total time:       8.21 min
[32m[20230207 15:08:54 @agent_ppo2.py:152][0m 421888 total steps have happened
[32m[20230207 15:08:54 @agent_ppo2.py:128][0m #------------------------ Iteration 206 --------------------------#
[32m[20230207 15:08:54 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:08:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:55 @agent_ppo2.py:192][0m |           0.0058 |           7.0452 |           0.0948 |
[32m[20230207 15:08:55 @agent_ppo2.py:192][0m |          -0.0009 |           4.6154 |           0.0947 |
[32m[20230207 15:08:55 @agent_ppo2.py:192][0m |          -0.0013 |           4.7259 |           0.0946 |
[32m[20230207 15:08:55 @agent_ppo2.py:192][0m |          -0.0009 |           4.2639 |           0.0946 |
[32m[20230207 15:08:55 @agent_ppo2.py:192][0m |          -0.0027 |           4.1771 |           0.0944 |
[32m[20230207 15:08:55 @agent_ppo2.py:192][0m |           0.0018 |           4.0855 |           0.0944 |
[32m[20230207 15:08:55 @agent_ppo2.py:192][0m |          -0.0058 |           4.1856 |           0.0944 |
[32m[20230207 15:08:55 @agent_ppo2.py:192][0m |           0.0015 |           3.9892 |           0.0943 |
[32m[20230207 15:08:55 @agent_ppo2.py:192][0m |          -0.0025 |           3.8357 |           0.0943 |
[32m[20230207 15:08:55 @agent_ppo2.py:192][0m |          -0.0059 |           3.8277 |           0.0942 |
[32m[20230207 15:08:55 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:08:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: -103.56
[32m[20230207 15:08:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -93.77
[32m[20230207 15:08:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -60.09
[32m[20230207 15:08:56 @agent_ppo2.py:150][0m Total time:       8.25 min
[32m[20230207 15:08:56 @agent_ppo2.py:152][0m 423936 total steps have happened
[32m[20230207 15:08:56 @agent_ppo2.py:128][0m #------------------------ Iteration 207 --------------------------#
[32m[20230207 15:08:57 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:08:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:08:57 @agent_ppo2.py:192][0m |           0.0004 |           9.6800 |           0.0941 |
[32m[20230207 15:08:57 @agent_ppo2.py:192][0m |           0.0002 |           3.9676 |           0.0940 |
[32m[20230207 15:08:57 @agent_ppo2.py:192][0m |          -0.0023 |           3.2663 |           0.0940 |
[32m[20230207 15:08:57 @agent_ppo2.py:192][0m |          -0.0045 |           2.7406 |           0.0939 |
[32m[20230207 15:08:57 @agent_ppo2.py:192][0m |          -0.0059 |           2.5058 |           0.0939 |
[32m[20230207 15:08:58 @agent_ppo2.py:192][0m |          -0.0058 |           2.3879 |           0.0939 |
[32m[20230207 15:08:58 @agent_ppo2.py:192][0m |          -0.0059 |           2.3461 |           0.0940 |
[32m[20230207 15:08:58 @agent_ppo2.py:192][0m |          -0.0069 |           2.2081 |           0.0939 |
[32m[20230207 15:08:58 @agent_ppo2.py:192][0m |          -0.0080 |           2.1182 |           0.0940 |
[32m[20230207 15:08:58 @agent_ppo2.py:192][0m |          -0.0083 |           2.1238 |           0.0939 |
[32m[20230207 15:08:58 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:08:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.22
[32m[20230207 15:08:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -94.11
[32m[20230207 15:08:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -28.76
[32m[20230207 15:08:59 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -28.76
[32m[20230207 15:08:59 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -28.76
[32m[20230207 15:08:59 @agent_ppo2.py:150][0m Total time:       8.29 min
[32m[20230207 15:08:59 @agent_ppo2.py:152][0m 425984 total steps have happened
[32m[20230207 15:08:59 @agent_ppo2.py:128][0m #------------------------ Iteration 208 --------------------------#
[32m[20230207 15:08:59 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:08:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:00 @agent_ppo2.py:192][0m |          -0.1899 |           3.2329 |           0.0946 |
[32m[20230207 15:09:00 @agent_ppo2.py:192][0m |           0.0040 |           1.8341 |           0.0947 |
[32m[20230207 15:09:00 @agent_ppo2.py:192][0m |          -0.0009 |           1.6950 |           0.0948 |
[32m[20230207 15:09:00 @agent_ppo2.py:192][0m |          -0.0155 |           1.6295 |           0.0948 |
[32m[20230207 15:09:00 @agent_ppo2.py:192][0m |          -0.0102 |           1.5953 |           0.0948 |
[32m[20230207 15:09:00 @agent_ppo2.py:192][0m |          -0.0223 |           1.5752 |           0.0947 |
[32m[20230207 15:09:00 @agent_ppo2.py:192][0m |          -0.0046 |           1.5409 |           0.0948 |
[32m[20230207 15:09:00 @agent_ppo2.py:192][0m |           0.0024 |           1.5111 |           0.0948 |
[32m[20230207 15:09:00 @agent_ppo2.py:192][0m |          -0.0077 |           1.5112 |           0.0948 |
[32m[20230207 15:09:00 @agent_ppo2.py:192][0m |          -0.0107 |           1.5360 |           0.0948 |
[32m[20230207 15:09:00 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -59.52
[32m[20230207 15:09:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -49.66
[32m[20230207 15:09:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -87.71
[32m[20230207 15:09:01 @agent_ppo2.py:150][0m Total time:       8.33 min
[32m[20230207 15:09:01 @agent_ppo2.py:152][0m 428032 total steps have happened
[32m[20230207 15:09:01 @agent_ppo2.py:128][0m #------------------------ Iteration 209 --------------------------#
[32m[20230207 15:09:02 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:09:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:02 @agent_ppo2.py:192][0m |           0.0136 |           4.3101 |           0.0948 |
[32m[20230207 15:09:02 @agent_ppo2.py:192][0m |           0.0086 |           2.8347 |           0.0948 |
[32m[20230207 15:09:02 @agent_ppo2.py:192][0m |          -0.0046 |           2.6331 |           0.0947 |
[32m[20230207 15:09:02 @agent_ppo2.py:192][0m |           0.0052 |           2.4730 |           0.0947 |
[32m[20230207 15:09:02 @agent_ppo2.py:192][0m |          -0.0154 |           2.4325 |           0.0946 |
[32m[20230207 15:09:02 @agent_ppo2.py:192][0m |          -0.0053 |           2.3885 |           0.0946 |
[32m[20230207 15:09:03 @agent_ppo2.py:192][0m |          -0.0343 |           2.2472 |           0.0946 |
[32m[20230207 15:09:03 @agent_ppo2.py:192][0m |          -0.0080 |           2.1994 |           0.0945 |
[32m[20230207 15:09:03 @agent_ppo2.py:192][0m |          -0.0076 |           2.1715 |           0.0945 |
[32m[20230207 15:09:03 @agent_ppo2.py:192][0m |           0.0106 |           2.1769 |           0.0944 |
[32m[20230207 15:09:03 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: -59.47
[32m[20230207 15:09:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -15.84
[32m[20230207 15:09:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -81.00
[32m[20230207 15:09:04 @agent_ppo2.py:150][0m Total time:       8.37 min
[32m[20230207 15:09:04 @agent_ppo2.py:152][0m 430080 total steps have happened
[32m[20230207 15:09:04 @agent_ppo2.py:128][0m #------------------------ Iteration 210 --------------------------#
[32m[20230207 15:09:04 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:09:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:05 @agent_ppo2.py:192][0m |          -0.0033 |           6.3084 |           0.0957 |
[32m[20230207 15:09:05 @agent_ppo2.py:192][0m |          -0.0060 |           2.6054 |           0.0957 |
[32m[20230207 15:09:05 @agent_ppo2.py:192][0m |          -0.0101 |           1.9435 |           0.0956 |
[32m[20230207 15:09:05 @agent_ppo2.py:192][0m |          -0.0089 |           1.6945 |           0.0956 |
[32m[20230207 15:09:05 @agent_ppo2.py:192][0m |          -0.0081 |           1.6749 |           0.0956 |
[32m[20230207 15:09:05 @agent_ppo2.py:192][0m |          -0.0103 |           1.5702 |           0.0956 |
[32m[20230207 15:09:05 @agent_ppo2.py:192][0m |          -0.0109 |           1.5472 |           0.0956 |
[32m[20230207 15:09:05 @agent_ppo2.py:192][0m |          -0.0113 |           1.5148 |           0.0956 |
[32m[20230207 15:09:05 @agent_ppo2.py:192][0m |          -0.0113 |           1.4956 |           0.0956 |
[32m[20230207 15:09:05 @agent_ppo2.py:192][0m |          -0.0083 |           1.4902 |           0.0956 |
[32m[20230207 15:09:05 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:09:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: -66.70
[32m[20230207 15:09:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -30.76
[32m[20230207 15:09:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -82.65
[32m[20230207 15:09:06 @agent_ppo2.py:150][0m Total time:       8.42 min
[32m[20230207 15:09:06 @agent_ppo2.py:152][0m 432128 total steps have happened
[32m[20230207 15:09:06 @agent_ppo2.py:128][0m #------------------------ Iteration 211 --------------------------#
[32m[20230207 15:09:07 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230207 15:09:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:07 @agent_ppo2.py:192][0m |           0.0024 |          17.3355 |           0.0977 |
[32m[20230207 15:09:07 @agent_ppo2.py:192][0m |          -0.0018 |           5.6561 |           0.0976 |
[32m[20230207 15:09:07 @agent_ppo2.py:192][0m |          -0.0068 |           4.4011 |           0.0974 |
[32m[20230207 15:09:07 @agent_ppo2.py:192][0m |          -0.0067 |           4.0060 |           0.0974 |
[32m[20230207 15:09:07 @agent_ppo2.py:192][0m |          -0.0066 |           3.6704 |           0.0974 |
[32m[20230207 15:09:07 @agent_ppo2.py:192][0m |          -0.0102 |           3.4461 |           0.0974 |
[32m[20230207 15:09:07 @agent_ppo2.py:192][0m |          -0.0091 |           3.2907 |           0.0973 |
[32m[20230207 15:09:07 @agent_ppo2.py:192][0m |          -0.0128 |           3.2033 |           0.0973 |
[32m[20230207 15:09:07 @agent_ppo2.py:192][0m |          -0.0132 |           3.3059 |           0.0973 |
[32m[20230207 15:09:07 @agent_ppo2.py:192][0m |          -0.0120 |           3.1040 |           0.0972 |
[32m[20230207 15:09:07 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230207 15:09:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -110.56
[32m[20230207 15:09:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -81.47
[32m[20230207 15:09:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -26.58
[32m[20230207 15:09:08 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -26.58
[32m[20230207 15:09:08 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -26.58
[32m[20230207 15:09:08 @agent_ppo2.py:150][0m Total time:       8.45 min
[32m[20230207 15:09:08 @agent_ppo2.py:152][0m 434176 total steps have happened
[32m[20230207 15:09:08 @agent_ppo2.py:128][0m #------------------------ Iteration 212 --------------------------#
[32m[20230207 15:09:09 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:09:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:09 @agent_ppo2.py:192][0m |           0.0125 |           1.7340 |           0.0936 |
[32m[20230207 15:09:09 @agent_ppo2.py:192][0m |          -0.0311 |           1.3917 |           0.0934 |
[32m[20230207 15:09:09 @agent_ppo2.py:192][0m |          -0.0074 |           1.3110 |           0.0933 |
[32m[20230207 15:09:09 @agent_ppo2.py:192][0m |          -0.0433 |           1.3662 |           0.0933 |
[32m[20230207 15:09:09 @agent_ppo2.py:192][0m |           0.0068 |           1.2617 |           0.0931 |
[32m[20230207 15:09:10 @agent_ppo2.py:192][0m |          -0.0065 |           1.2380 |           0.0932 |
[32m[20230207 15:09:10 @agent_ppo2.py:192][0m |          -0.0236 |           1.2213 |           0.0931 |
[32m[20230207 15:09:10 @agent_ppo2.py:192][0m |          -0.0096 |           1.2105 |           0.0931 |
[32m[20230207 15:09:10 @agent_ppo2.py:192][0m |          -0.0059 |           1.2069 |           0.0931 |
[32m[20230207 15:09:10 @agent_ppo2.py:192][0m |          -0.0111 |           1.2025 |           0.0931 |
[32m[20230207 15:09:10 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -69.90
[32m[20230207 15:09:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -40.83
[32m[20230207 15:09:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -110.79
[32m[20230207 15:09:10 @agent_ppo2.py:150][0m Total time:       8.49 min
[32m[20230207 15:09:10 @agent_ppo2.py:152][0m 436224 total steps have happened
[32m[20230207 15:09:10 @agent_ppo2.py:128][0m #------------------------ Iteration 213 --------------------------#
[32m[20230207 15:09:11 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:09:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:11 @agent_ppo2.py:192][0m |          -0.0023 |           1.4889 |           0.0978 |
[32m[20230207 15:09:11 @agent_ppo2.py:192][0m |           0.0021 |           1.1866 |           0.0978 |
[32m[20230207 15:09:12 @agent_ppo2.py:192][0m |          -0.0057 |           1.1360 |           0.0977 |
[32m[20230207 15:09:12 @agent_ppo2.py:192][0m |           0.0085 |           1.1026 |           0.0978 |
[32m[20230207 15:09:12 @agent_ppo2.py:192][0m |          -0.0055 |           1.0810 |           0.0977 |
[32m[20230207 15:09:12 @agent_ppo2.py:192][0m |          -0.0001 |           1.0636 |           0.0977 |
[32m[20230207 15:09:12 @agent_ppo2.py:192][0m |          -0.0000 |           1.0540 |           0.0978 |
[32m[20230207 15:09:12 @agent_ppo2.py:192][0m |           0.0197 |           1.0522 |           0.0979 |
[32m[20230207 15:09:12 @agent_ppo2.py:192][0m |          -0.0244 |           1.0355 |           0.0980 |
[32m[20230207 15:09:12 @agent_ppo2.py:192][0m |           0.0019 |           1.0154 |           0.0979 |
[32m[20230207 15:09:12 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -36.37
[32m[20230207 15:09:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -36.16
[32m[20230207 15:09:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -109.45
[32m[20230207 15:09:13 @agent_ppo2.py:150][0m Total time:       8.52 min
[32m[20230207 15:09:13 @agent_ppo2.py:152][0m 438272 total steps have happened
[32m[20230207 15:09:13 @agent_ppo2.py:128][0m #------------------------ Iteration 214 --------------------------#
[32m[20230207 15:09:13 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:09:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:14 @agent_ppo2.py:192][0m |           0.0060 |           1.0691 |           0.0959 |
[32m[20230207 15:09:14 @agent_ppo2.py:192][0m |          -0.0028 |           0.9484 |           0.0957 |
[32m[20230207 15:09:14 @agent_ppo2.py:192][0m |          -0.0014 |           0.9338 |           0.0956 |
[32m[20230207 15:09:14 @agent_ppo2.py:192][0m |          -0.0085 |           0.9130 |           0.0956 |
[32m[20230207 15:09:14 @agent_ppo2.py:192][0m |          -0.0021 |           0.9109 |           0.0956 |
[32m[20230207 15:09:14 @agent_ppo2.py:192][0m |          -0.0050 |           0.8974 |           0.0956 |
[32m[20230207 15:09:14 @agent_ppo2.py:192][0m |           0.0051 |           0.8898 |           0.0954 |
[32m[20230207 15:09:14 @agent_ppo2.py:192][0m |          -0.0119 |           0.8868 |           0.0954 |
[32m[20230207 15:09:14 @agent_ppo2.py:192][0m |          -0.0889 |           1.2924 |           0.0955 |
[32m[20230207 15:09:14 @agent_ppo2.py:192][0m |          -0.0074 |           0.9706 |           0.0955 |
[32m[20230207 15:09:14 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: -95.25
[32m[20230207 15:09:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -76.83
[32m[20230207 15:09:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -86.99
[32m[20230207 15:09:15 @agent_ppo2.py:150][0m Total time:       8.56 min
[32m[20230207 15:09:15 @agent_ppo2.py:152][0m 440320 total steps have happened
[32m[20230207 15:09:15 @agent_ppo2.py:128][0m #------------------------ Iteration 215 --------------------------#
[32m[20230207 15:09:16 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:09:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:16 @agent_ppo2.py:192][0m |          -0.0146 |           1.2097 |           0.0976 |
[32m[20230207 15:09:16 @agent_ppo2.py:192][0m |           0.0065 |           1.0798 |           0.0975 |
[32m[20230207 15:09:16 @agent_ppo2.py:192][0m |          -0.0017 |           1.0685 |           0.0974 |
[32m[20230207 15:09:16 @agent_ppo2.py:192][0m |           0.0051 |           1.0762 |           0.0974 |
[32m[20230207 15:09:16 @agent_ppo2.py:192][0m |          -0.0030 |           1.0592 |           0.0974 |
[32m[20230207 15:09:16 @agent_ppo2.py:192][0m |           0.0090 |           1.0513 |           0.0974 |
[32m[20230207 15:09:16 @agent_ppo2.py:192][0m |           0.0081 |           1.0345 |           0.0975 |
[32m[20230207 15:09:17 @agent_ppo2.py:192][0m |           0.0072 |           1.0335 |           0.0975 |
[32m[20230207 15:09:17 @agent_ppo2.py:192][0m |          -0.0039 |           1.0262 |           0.0975 |
[32m[20230207 15:09:17 @agent_ppo2.py:192][0m |          -0.0068 |           1.0211 |           0.0975 |
[32m[20230207 15:09:17 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -70.39
[32m[20230207 15:09:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -42.26
[32m[20230207 15:09:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -63.41
[32m[20230207 15:09:18 @agent_ppo2.py:150][0m Total time:       8.61 min
[32m[20230207 15:09:18 @agent_ppo2.py:152][0m 442368 total steps have happened
[32m[20230207 15:09:18 @agent_ppo2.py:128][0m #------------------------ Iteration 216 --------------------------#
[32m[20230207 15:09:18 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:09:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:18 @agent_ppo2.py:192][0m |           0.0124 |           1.5212 |           0.0955 |
[32m[20230207 15:09:19 @agent_ppo2.py:192][0m |          -0.0029 |           1.2481 |           0.0954 |
[32m[20230207 15:09:19 @agent_ppo2.py:192][0m |           0.0076 |           1.1832 |           0.0953 |
[32m[20230207 15:09:19 @agent_ppo2.py:192][0m |          -0.0039 |           1.1526 |           0.0954 |
[32m[20230207 15:09:19 @agent_ppo2.py:192][0m |          -0.0020 |           1.1266 |           0.0953 |
[32m[20230207 15:09:19 @agent_ppo2.py:192][0m |          -0.0055 |           1.0995 |           0.0953 |
[32m[20230207 15:09:19 @agent_ppo2.py:192][0m |          -0.0046 |           1.0762 |           0.0953 |
[32m[20230207 15:09:19 @agent_ppo2.py:192][0m |          -0.0227 |           1.0761 |           0.0953 |
[32m[20230207 15:09:19 @agent_ppo2.py:192][0m |          -0.0148 |           1.0295 |           0.0953 |
[32m[20230207 15:09:19 @agent_ppo2.py:192][0m |          -0.1018 |           1.5417 |           0.0953 |
[32m[20230207 15:09:19 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -80.72
[32m[20230207 15:09:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -70.69
[32m[20230207 15:09:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -110.43
[32m[20230207 15:09:20 @agent_ppo2.py:150][0m Total time:       8.64 min
[32m[20230207 15:09:20 @agent_ppo2.py:152][0m 444416 total steps have happened
[32m[20230207 15:09:20 @agent_ppo2.py:128][0m #------------------------ Iteration 217 --------------------------#
[32m[20230207 15:09:21 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:09:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:21 @agent_ppo2.py:192][0m |           0.0081 |           1.7321 |           0.0958 |
[32m[20230207 15:09:21 @agent_ppo2.py:192][0m |           0.0049 |           1.3031 |           0.0956 |
[32m[20230207 15:09:21 @agent_ppo2.py:192][0m |          -0.0081 |           1.2263 |           0.0954 |
[32m[20230207 15:09:21 @agent_ppo2.py:192][0m |          -0.0146 |           1.2000 |           0.0954 |
[32m[20230207 15:09:21 @agent_ppo2.py:192][0m |          -0.0225 |           1.1894 |           0.0955 |
[32m[20230207 15:09:21 @agent_ppo2.py:192][0m |           0.0058 |           1.1401 |           0.0956 |
[32m[20230207 15:09:21 @agent_ppo2.py:192][0m |          -0.0142 |           1.1234 |           0.0955 |
[32m[20230207 15:09:21 @agent_ppo2.py:192][0m |          -0.0056 |           1.1055 |           0.0955 |
[32m[20230207 15:09:21 @agent_ppo2.py:192][0m |          -0.0275 |           1.0815 |           0.0955 |
[32m[20230207 15:09:21 @agent_ppo2.py:192][0m |          -0.0091 |           1.0797 |           0.0955 |
[32m[20230207 15:09:21 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: -24.16
[32m[20230207 15:09:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 50.59
[32m[20230207 15:09:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -82.88
[32m[20230207 15:09:22 @agent_ppo2.py:150][0m Total time:       8.68 min
[32m[20230207 15:09:22 @agent_ppo2.py:152][0m 446464 total steps have happened
[32m[20230207 15:09:22 @agent_ppo2.py:128][0m #------------------------ Iteration 218 --------------------------#
[32m[20230207 15:09:23 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:09:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:23 @agent_ppo2.py:192][0m |           0.0033 |           1.1597 |           0.0971 |
[32m[20230207 15:09:23 @agent_ppo2.py:192][0m |           0.0021 |           1.0669 |           0.0970 |
[32m[20230207 15:09:23 @agent_ppo2.py:192][0m |          -0.0178 |           1.0367 |           0.0970 |
[32m[20230207 15:09:23 @agent_ppo2.py:192][0m |          -0.0056 |           1.0246 |           0.0970 |
[32m[20230207 15:09:23 @agent_ppo2.py:192][0m |          -0.0068 |           1.0126 |           0.0970 |
[32m[20230207 15:09:24 @agent_ppo2.py:192][0m |          -0.0071 |           0.9984 |           0.0970 |
[32m[20230207 15:09:24 @agent_ppo2.py:192][0m |          -0.0070 |           0.9966 |           0.0970 |
[32m[20230207 15:09:24 @agent_ppo2.py:192][0m |          -0.0264 |           1.0116 |           0.0969 |
[32m[20230207 15:09:24 @agent_ppo2.py:192][0m |          -0.0239 |           1.0045 |           0.0969 |
[32m[20230207 15:09:24 @agent_ppo2.py:192][0m |          -0.0130 |           0.9813 |           0.0969 |
[32m[20230207 15:09:24 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -76.16
[32m[20230207 15:09:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -70.28
[32m[20230207 15:09:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -26.88
[32m[20230207 15:09:25 @agent_ppo2.py:150][0m Total time:       8.72 min
[32m[20230207 15:09:25 @agent_ppo2.py:152][0m 448512 total steps have happened
[32m[20230207 15:09:25 @agent_ppo2.py:128][0m #------------------------ Iteration 219 --------------------------#
[32m[20230207 15:09:25 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:09:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:26 @agent_ppo2.py:192][0m |           0.0224 |           0.9719 |           0.0994 |
[32m[20230207 15:09:26 @agent_ppo2.py:192][0m |           0.0129 |           0.8911 |           0.0994 |
[32m[20230207 15:09:26 @agent_ppo2.py:192][0m |           0.0141 |           0.8652 |           0.0994 |
[32m[20230207 15:09:26 @agent_ppo2.py:192][0m |           0.0058 |           0.8482 |           0.0994 |
[32m[20230207 15:09:26 @agent_ppo2.py:192][0m |           0.0014 |           0.8334 |           0.0994 |
[32m[20230207 15:09:26 @agent_ppo2.py:192][0m |          -0.0175 |           0.8249 |           0.0993 |
[32m[20230207 15:09:26 @agent_ppo2.py:192][0m |          -0.0256 |           0.8157 |           0.0993 |
[32m[20230207 15:09:26 @agent_ppo2.py:192][0m |           0.0218 |           0.8390 |           0.0992 |
[32m[20230207 15:09:26 @agent_ppo2.py:192][0m |           0.0077 |           0.8161 |           0.0993 |
[32m[20230207 15:09:26 @agent_ppo2.py:192][0m |           0.0010 |           0.8012 |           0.0993 |
[32m[20230207 15:09:26 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -69.54
[32m[20230207 15:09:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -38.90
[32m[20230207 15:09:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -94.29
[32m[20230207 15:09:27 @agent_ppo2.py:150][0m Total time:       8.76 min
[32m[20230207 15:09:27 @agent_ppo2.py:152][0m 450560 total steps have happened
[32m[20230207 15:09:27 @agent_ppo2.py:128][0m #------------------------ Iteration 220 --------------------------#
[32m[20230207 15:09:28 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:09:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:28 @agent_ppo2.py:192][0m |           0.0001 |          21.3122 |           0.0986 |
[32m[20230207 15:09:28 @agent_ppo2.py:192][0m |          -0.0023 |          15.7211 |           0.0986 |
[32m[20230207 15:09:28 @agent_ppo2.py:192][0m |          -0.0045 |          10.2301 |           0.0986 |
[32m[20230207 15:09:28 @agent_ppo2.py:192][0m |          -0.0070 |           6.8553 |           0.0985 |
[32m[20230207 15:09:28 @agent_ppo2.py:192][0m |          -0.0078 |           4.1955 |           0.0985 |
[32m[20230207 15:09:29 @agent_ppo2.py:192][0m |          -0.0088 |           3.0238 |           0.0985 |
[32m[20230207 15:09:29 @agent_ppo2.py:192][0m |          -0.0095 |           2.5643 |           0.0984 |
[32m[20230207 15:09:29 @agent_ppo2.py:192][0m |          -0.0098 |           2.2048 |           0.0984 |
[32m[20230207 15:09:29 @agent_ppo2.py:192][0m |          -0.0101 |           1.9755 |           0.0984 |
[32m[20230207 15:09:29 @agent_ppo2.py:192][0m |          -0.0102 |           1.8391 |           0.0984 |
[32m[20230207 15:09:29 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:09:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -105.28
[32m[20230207 15:09:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.36
[32m[20230207 15:09:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -92.03
[32m[20230207 15:09:30 @agent_ppo2.py:150][0m Total time:       8.81 min
[32m[20230207 15:09:30 @agent_ppo2.py:152][0m 452608 total steps have happened
[32m[20230207 15:09:30 @agent_ppo2.py:128][0m #------------------------ Iteration 221 --------------------------#
[32m[20230207 15:09:31 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:09:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:31 @agent_ppo2.py:192][0m |          -0.0055 |           1.5659 |           0.0959 |
[32m[20230207 15:09:31 @agent_ppo2.py:192][0m |          -0.0016 |           1.2293 |           0.0957 |
[32m[20230207 15:09:31 @agent_ppo2.py:192][0m |          -0.0054 |           1.1508 |           0.0957 |
[32m[20230207 15:09:31 @agent_ppo2.py:192][0m |          -0.0027 |           1.1323 |           0.0957 |
[32m[20230207 15:09:31 @agent_ppo2.py:192][0m |          -0.0076 |           1.1441 |           0.0957 |
[32m[20230207 15:09:31 @agent_ppo2.py:192][0m |          -0.0114 |           1.0890 |           0.0956 |
[32m[20230207 15:09:31 @agent_ppo2.py:192][0m |          -0.0021 |           1.0725 |           0.0956 |
[32m[20230207 15:09:31 @agent_ppo2.py:192][0m |          -0.0109 |           1.0680 |           0.0956 |
[32m[20230207 15:09:31 @agent_ppo2.py:192][0m |          -0.0035 |           1.0427 |           0.0956 |
[32m[20230207 15:09:31 @agent_ppo2.py:192][0m |          -0.0044 |           1.0437 |           0.0957 |
[32m[20230207 15:09:31 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -47.15
[32m[20230207 15:09:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -22.15
[32m[20230207 15:09:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -99.78
[32m[20230207 15:09:32 @agent_ppo2.py:150][0m Total time:       8.85 min
[32m[20230207 15:09:32 @agent_ppo2.py:152][0m 454656 total steps have happened
[32m[20230207 15:09:32 @agent_ppo2.py:128][0m #------------------------ Iteration 222 --------------------------#
[32m[20230207 15:09:33 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:09:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:33 @agent_ppo2.py:192][0m |           0.0064 |           1.9489 |           0.0967 |
[32m[20230207 15:09:33 @agent_ppo2.py:192][0m |          -0.0027 |           1.4019 |           0.0967 |
[32m[20230207 15:09:33 @agent_ppo2.py:192][0m |          -0.0068 |           1.3414 |           0.0967 |
[32m[20230207 15:09:33 @agent_ppo2.py:192][0m |           0.0003 |           1.3007 |           0.0968 |
[32m[20230207 15:09:33 @agent_ppo2.py:192][0m |           0.0185 |           1.3846 |           0.0968 |
[32m[20230207 15:09:33 @agent_ppo2.py:192][0m |          -0.0060 |           1.3122 |           0.0969 |
[32m[20230207 15:09:34 @agent_ppo2.py:192][0m |          -0.0118 |           1.2335 |           0.0969 |
[32m[20230207 15:09:34 @agent_ppo2.py:192][0m |          -0.0170 |           1.2331 |           0.0970 |
[32m[20230207 15:09:34 @agent_ppo2.py:192][0m |          -0.0108 |           1.2070 |           0.0970 |
[32m[20230207 15:09:34 @agent_ppo2.py:192][0m |          -0.0050 |           1.1933 |           0.0970 |
[32m[20230207 15:09:34 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: -58.39
[32m[20230207 15:09:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -41.16
[32m[20230207 15:09:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -43.75
[32m[20230207 15:09:35 @agent_ppo2.py:150][0m Total time:       8.89 min
[32m[20230207 15:09:35 @agent_ppo2.py:152][0m 456704 total steps have happened
[32m[20230207 15:09:35 @agent_ppo2.py:128][0m #------------------------ Iteration 223 --------------------------#
[32m[20230207 15:09:35 @agent_ppo2.py:134][0m Sampling time: 0.66 s by 1 slaves
[32m[20230207 15:09:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:35 @agent_ppo2.py:192][0m |           0.0059 |          15.9649 |           0.0944 |
[32m[20230207 15:09:35 @agent_ppo2.py:192][0m |           0.0147 |           4.6548 |           0.0943 |
[32m[20230207 15:09:35 @agent_ppo2.py:192][0m |          -0.0031 |           3.0001 |           0.0942 |
[32m[20230207 15:09:36 @agent_ppo2.py:192][0m |          -0.0100 |           2.4649 |           0.0942 |
[32m[20230207 15:09:36 @agent_ppo2.py:192][0m |          -0.0120 |           2.1905 |           0.0942 |
[32m[20230207 15:09:36 @agent_ppo2.py:192][0m |          -0.0133 |           1.9951 |           0.0941 |
[32m[20230207 15:09:36 @agent_ppo2.py:192][0m |          -0.0159 |           1.8392 |           0.0941 |
[32m[20230207 15:09:36 @agent_ppo2.py:192][0m |           0.0103 |           1.7586 |           0.0940 |
[32m[20230207 15:09:36 @agent_ppo2.py:192][0m |          -0.0160 |           1.5991 |           0.0940 |
[32m[20230207 15:09:36 @agent_ppo2.py:192][0m |          -0.0177 |           1.4758 |           0.0940 |
[32m[20230207 15:09:36 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230207 15:09:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: -102.72
[32m[20230207 15:09:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -87.67
[32m[20230207 15:09:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -83.76
[32m[20230207 15:09:37 @agent_ppo2.py:150][0m Total time:       8.93 min
[32m[20230207 15:09:37 @agent_ppo2.py:152][0m 458752 total steps have happened
[32m[20230207 15:09:37 @agent_ppo2.py:128][0m #------------------------ Iteration 224 --------------------------#
[32m[20230207 15:09:38 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:09:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:38 @agent_ppo2.py:192][0m |           0.0137 |           1.3781 |           0.0974 |
[32m[20230207 15:09:38 @agent_ppo2.py:192][0m |          -0.0299 |           1.1649 |           0.0974 |
[32m[20230207 15:09:38 @agent_ppo2.py:192][0m |          -0.0129 |           1.0380 |           0.0974 |
[32m[20230207 15:09:38 @agent_ppo2.py:192][0m |           0.0022 |           1.0067 |           0.0974 |
[32m[20230207 15:09:38 @agent_ppo2.py:192][0m |          -0.0049 |           0.9856 |           0.0974 |
[32m[20230207 15:09:38 @agent_ppo2.py:192][0m |          -0.0083 |           0.9703 |           0.0974 |
[32m[20230207 15:09:38 @agent_ppo2.py:192][0m |          -0.0014 |           0.9568 |           0.0974 |
[32m[20230207 15:09:38 @agent_ppo2.py:192][0m |          -0.0102 |           0.9451 |           0.0975 |
[32m[20230207 15:09:38 @agent_ppo2.py:192][0m |          -0.0359 |           0.9517 |           0.0975 |
[32m[20230207 15:09:38 @agent_ppo2.py:192][0m |          -0.0170 |           0.9341 |           0.0975 |
[32m[20230207 15:09:38 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -57.37
[32m[20230207 15:09:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -36.16
[32m[20230207 15:09:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -42.06
[32m[20230207 15:09:39 @agent_ppo2.py:150][0m Total time:       8.97 min
[32m[20230207 15:09:39 @agent_ppo2.py:152][0m 460800 total steps have happened
[32m[20230207 15:09:39 @agent_ppo2.py:128][0m #------------------------ Iteration 225 --------------------------#
[32m[20230207 15:09:40 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:09:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:40 @agent_ppo2.py:192][0m |           0.0008 |           9.9032 |           0.0994 |
[32m[20230207 15:09:40 @agent_ppo2.py:192][0m |          -0.0018 |           5.8974 |           0.0993 |
[32m[20230207 15:09:40 @agent_ppo2.py:192][0m |          -0.0023 |           5.4812 |           0.0993 |
[32m[20230207 15:09:40 @agent_ppo2.py:192][0m |          -0.0036 |           5.8455 |           0.0993 |
[32m[20230207 15:09:41 @agent_ppo2.py:192][0m |          -0.0049 |           5.0330 |           0.0992 |
[32m[20230207 15:09:41 @agent_ppo2.py:192][0m |          -0.0060 |           4.7342 |           0.0992 |
[32m[20230207 15:09:41 @agent_ppo2.py:192][0m |          -0.0065 |           4.3800 |           0.0992 |
[32m[20230207 15:09:41 @agent_ppo2.py:192][0m |          -0.0067 |           4.1275 |           0.0991 |
[32m[20230207 15:09:41 @agent_ppo2.py:192][0m |          -0.0067 |           3.9385 |           0.0991 |
[32m[20230207 15:09:41 @agent_ppo2.py:192][0m |          -0.0083 |           3.7290 |           0.0990 |
[32m[20230207 15:09:41 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:09:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -75.48
[32m[20230207 15:09:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -5.87
[32m[20230207 15:09:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -60.68
[32m[20230207 15:09:41 @agent_ppo2.py:150][0m Total time:       9.00 min
[32m[20230207 15:09:41 @agent_ppo2.py:152][0m 462848 total steps have happened
[32m[20230207 15:09:41 @agent_ppo2.py:128][0m #------------------------ Iteration 226 --------------------------#
[32m[20230207 15:09:42 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:09:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:42 @agent_ppo2.py:192][0m |           0.0013 |          36.2767 |           0.1006 |
[32m[20230207 15:09:42 @agent_ppo2.py:192][0m |          -0.0017 |          26.4345 |           0.1005 |
[32m[20230207 15:09:42 @agent_ppo2.py:192][0m |          -0.0046 |           8.3621 |           0.1005 |
[32m[20230207 15:09:42 @agent_ppo2.py:192][0m |          -0.0052 |           3.5826 |           0.1004 |
[32m[20230207 15:09:42 @agent_ppo2.py:192][0m |          -0.0054 |           2.3793 |           0.1004 |
[32m[20230207 15:09:42 @agent_ppo2.py:192][0m |          -0.0072 |           1.8943 |           0.1004 |
[32m[20230207 15:09:42 @agent_ppo2.py:192][0m |          -0.0079 |           1.5919 |           0.1003 |
[32m[20230207 15:09:43 @agent_ppo2.py:192][0m |          -0.0087 |           1.4256 |           0.1003 |
[32m[20230207 15:09:43 @agent_ppo2.py:192][0m |          -0.0078 |           1.2908 |           0.1003 |
[32m[20230207 15:09:43 @agent_ppo2.py:192][0m |          -0.0096 |           1.2152 |           0.1002 |
[32m[20230207 15:09:43 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:09:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -73.94
[32m[20230207 15:09:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -29.21
[32m[20230207 15:09:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -72.43
[32m[20230207 15:09:43 @agent_ppo2.py:150][0m Total time:       9.04 min
[32m[20230207 15:09:43 @agent_ppo2.py:152][0m 464896 total steps have happened
[32m[20230207 15:09:43 @agent_ppo2.py:128][0m #------------------------ Iteration 227 --------------------------#
[32m[20230207 15:09:44 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:09:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:44 @agent_ppo2.py:192][0m |          -0.0054 |           1.0909 |           0.0959 |
[32m[20230207 15:09:45 @agent_ppo2.py:192][0m |           0.0027 |           0.9165 |           0.0959 |
[32m[20230207 15:09:45 @agent_ppo2.py:192][0m |          -0.0031 |           0.8644 |           0.0959 |
[32m[20230207 15:09:45 @agent_ppo2.py:192][0m |          -0.0175 |           0.8404 |           0.0958 |
[32m[20230207 15:09:45 @agent_ppo2.py:192][0m |          -0.0088 |           0.8168 |           0.0958 |
[32m[20230207 15:09:45 @agent_ppo2.py:192][0m |           0.0013 |           0.7984 |           0.0958 |
[32m[20230207 15:09:45 @agent_ppo2.py:192][0m |           0.0011 |           0.7883 |           0.0959 |
[32m[20230207 15:09:45 @agent_ppo2.py:192][0m |          -0.0000 |           0.7806 |           0.0958 |
[32m[20230207 15:09:45 @agent_ppo2.py:192][0m |          -0.0155 |           0.7726 |           0.0960 |
[32m[20230207 15:09:45 @agent_ppo2.py:192][0m |          -0.0082 |           0.7657 |           0.0959 |
[32m[20230207 15:09:45 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:09:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -113.53
[32m[20230207 15:09:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -113.37
[32m[20230207 15:09:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -93.99
[32m[20230207 15:09:46 @agent_ppo2.py:150][0m Total time:       9.07 min
[32m[20230207 15:09:46 @agent_ppo2.py:152][0m 466944 total steps have happened
[32m[20230207 15:09:46 @agent_ppo2.py:128][0m #------------------------ Iteration 228 --------------------------#
[32m[20230207 15:09:47 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:09:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:47 @agent_ppo2.py:192][0m |          -0.0123 |          16.8358 |           0.1007 |
[32m[20230207 15:09:47 @agent_ppo2.py:192][0m |           0.0064 |           9.0940 |           0.1002 |
[32m[20230207 15:09:47 @agent_ppo2.py:192][0m |          -0.0005 |           6.0041 |           0.1004 |
[32m[20230207 15:09:47 @agent_ppo2.py:192][0m |           0.0073 |           5.5355 |           0.1005 |
[32m[20230207 15:09:47 @agent_ppo2.py:192][0m |           0.0001 |           4.6006 |           0.1004 |
[32m[20230207 15:09:47 @agent_ppo2.py:192][0m |          -0.0014 |           3.8438 |           0.1006 |
[32m[20230207 15:09:47 @agent_ppo2.py:192][0m |          -0.0040 |           3.4582 |           0.1006 |
[32m[20230207 15:09:47 @agent_ppo2.py:192][0m |           0.0042 |           3.2561 |           0.1006 |
[32m[20230207 15:09:47 @agent_ppo2.py:192][0m |          -0.0037 |           3.0909 |           0.1006 |
[32m[20230207 15:09:47 @agent_ppo2.py:192][0m |          -0.0046 |           2.8598 |           0.1005 |
[32m[20230207 15:09:47 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -14.95
[32m[20230207 15:09:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 61.68
[32m[20230207 15:09:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -78.92
[32m[20230207 15:09:48 @agent_ppo2.py:150][0m Total time:       9.12 min
[32m[20230207 15:09:48 @agent_ppo2.py:152][0m 468992 total steps have happened
[32m[20230207 15:09:48 @agent_ppo2.py:128][0m #------------------------ Iteration 229 --------------------------#
[32m[20230207 15:09:49 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:09:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:49 @agent_ppo2.py:192][0m |          -0.0092 |           1.3967 |           0.0975 |
[32m[20230207 15:09:49 @agent_ppo2.py:192][0m |          -0.0062 |           1.1852 |           0.0974 |
[32m[20230207 15:09:49 @agent_ppo2.py:192][0m |           0.0015 |           1.1204 |           0.0974 |
[32m[20230207 15:09:49 @agent_ppo2.py:192][0m |          -0.0035 |           1.0852 |           0.0974 |
[32m[20230207 15:09:49 @agent_ppo2.py:192][0m |          -0.0162 |           1.0638 |           0.0973 |
[32m[20230207 15:09:49 @agent_ppo2.py:192][0m |           0.0004 |           1.0521 |           0.0973 |
[32m[20230207 15:09:50 @agent_ppo2.py:192][0m |          -0.0554 |           1.0349 |           0.0973 |
[32m[20230207 15:09:50 @agent_ppo2.py:192][0m |           0.0054 |           1.0240 |           0.0973 |
[32m[20230207 15:09:50 @agent_ppo2.py:192][0m |          -0.0058 |           1.0115 |           0.0973 |
[32m[20230207 15:09:50 @agent_ppo2.py:192][0m |          -0.0148 |           1.0245 |           0.0973 |
[32m[20230207 15:09:50 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: -64.48
[32m[20230207 15:09:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -62.39
[32m[20230207 15:09:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -75.90
[32m[20230207 15:09:51 @agent_ppo2.py:150][0m Total time:       9.16 min
[32m[20230207 15:09:51 @agent_ppo2.py:152][0m 471040 total steps have happened
[32m[20230207 15:09:51 @agent_ppo2.py:128][0m #------------------------ Iteration 230 --------------------------#
[32m[20230207 15:09:51 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:09:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:52 @agent_ppo2.py:192][0m |          -0.0025 |          29.4931 |           0.0964 |
[32m[20230207 15:09:52 @agent_ppo2.py:192][0m |          -0.0184 |          13.4168 |           0.0964 |
[32m[20230207 15:09:52 @agent_ppo2.py:192][0m |           0.0311 |           6.5117 |           0.0962 |
[32m[20230207 15:09:52 @agent_ppo2.py:192][0m |          -0.0057 |           5.3944 |           0.0963 |
[32m[20230207 15:09:52 @agent_ppo2.py:192][0m |          -0.0084 |           4.7101 |           0.0963 |
[32m[20230207 15:09:52 @agent_ppo2.py:192][0m |          -0.0144 |           4.6511 |           0.0963 |
[32m[20230207 15:09:52 @agent_ppo2.py:192][0m |          -0.0071 |           4.5232 |           0.0963 |
[32m[20230207 15:09:52 @agent_ppo2.py:192][0m |          -0.0118 |           4.1525 |           0.0963 |
[32m[20230207 15:09:52 @agent_ppo2.py:192][0m |          -0.0083 |           3.9849 |           0.0963 |
[32m[20230207 15:09:52 @agent_ppo2.py:192][0m |          -0.0114 |           3.8645 |           0.0962 |
[32m[20230207 15:09:52 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:09:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -89.23
[32m[20230207 15:09:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -31.87
[32m[20230207 15:09:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -30.87
[32m[20230207 15:09:53 @agent_ppo2.py:150][0m Total time:       9.20 min
[32m[20230207 15:09:53 @agent_ppo2.py:152][0m 473088 total steps have happened
[32m[20230207 15:09:53 @agent_ppo2.py:128][0m #------------------------ Iteration 231 --------------------------#
[32m[20230207 15:09:54 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:09:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:54 @agent_ppo2.py:192][0m |           0.0003 |           2.2481 |           0.0992 |
[32m[20230207 15:09:54 @agent_ppo2.py:192][0m |           0.0077 |           1.5982 |           0.0992 |
[32m[20230207 15:09:54 @agent_ppo2.py:192][0m |           0.0104 |           1.3914 |           0.0991 |
[32m[20230207 15:09:54 @agent_ppo2.py:192][0m |          -0.0178 |           1.2708 |           0.0991 |
[32m[20230207 15:09:54 @agent_ppo2.py:192][0m |           0.0029 |           1.2002 |           0.0990 |
[32m[20230207 15:09:54 @agent_ppo2.py:192][0m |          -0.0078 |           1.1344 |           0.0990 |
[32m[20230207 15:09:55 @agent_ppo2.py:192][0m |           0.0024 |           1.0791 |           0.0990 |
[32m[20230207 15:09:55 @agent_ppo2.py:192][0m |          -0.0007 |           1.0406 |           0.0990 |
[32m[20230207 15:09:55 @agent_ppo2.py:192][0m |          -0.0095 |           1.0126 |           0.0990 |
[32m[20230207 15:09:55 @agent_ppo2.py:192][0m |           0.0093 |           1.0030 |           0.0991 |
[32m[20230207 15:09:55 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:09:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: -55.31
[32m[20230207 15:09:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -36.37
[32m[20230207 15:09:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -23.25
[32m[20230207 15:09:56 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -23.25
[32m[20230207 15:09:56 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -23.25
[32m[20230207 15:09:56 @agent_ppo2.py:150][0m Total time:       9.24 min
[32m[20230207 15:09:56 @agent_ppo2.py:152][0m 475136 total steps have happened
[32m[20230207 15:09:56 @agent_ppo2.py:128][0m #------------------------ Iteration 232 --------------------------#
[32m[20230207 15:09:56 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:09:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:57 @agent_ppo2.py:192][0m |           0.0022 |           7.0354 |           0.1015 |
[32m[20230207 15:09:57 @agent_ppo2.py:192][0m |          -0.0051 |           4.3389 |           0.1013 |
[32m[20230207 15:09:57 @agent_ppo2.py:192][0m |          -0.0049 |           3.7790 |           0.1012 |
[32m[20230207 15:09:57 @agent_ppo2.py:192][0m |          -0.0060 |           3.5092 |           0.1013 |
[32m[20230207 15:09:57 @agent_ppo2.py:192][0m |          -0.0050 |           3.3614 |           0.1013 |
[32m[20230207 15:09:57 @agent_ppo2.py:192][0m |          -0.0069 |           2.9931 |           0.1012 |
[32m[20230207 15:09:57 @agent_ppo2.py:192][0m |          -0.0097 |           2.9864 |           0.1013 |
[32m[20230207 15:09:57 @agent_ppo2.py:192][0m |          -0.0090 |           2.8704 |           0.1013 |
[32m[20230207 15:09:57 @agent_ppo2.py:192][0m |          -0.0100 |           2.7237 |           0.1013 |
[32m[20230207 15:09:57 @agent_ppo2.py:192][0m |          -0.0129 |           2.8103 |           0.1013 |
[32m[20230207 15:09:57 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:09:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: -93.56
[32m[20230207 15:09:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -43.20
[32m[20230207 15:09:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -104.75
[32m[20230207 15:09:58 @agent_ppo2.py:150][0m Total time:       9.28 min
[32m[20230207 15:09:58 @agent_ppo2.py:152][0m 477184 total steps have happened
[32m[20230207 15:09:58 @agent_ppo2.py:128][0m #------------------------ Iteration 233 --------------------------#
[32m[20230207 15:09:59 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:09:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:09:59 @agent_ppo2.py:192][0m |          -0.0014 |          29.0920 |           0.0992 |
[32m[20230207 15:09:59 @agent_ppo2.py:192][0m |           0.0028 |          24.4353 |           0.0991 |
[32m[20230207 15:09:59 @agent_ppo2.py:192][0m |          -0.0048 |          23.7078 |           0.0989 |
[32m[20230207 15:09:59 @agent_ppo2.py:192][0m |          -0.0066 |          23.1337 |           0.0988 |
[32m[20230207 15:09:59 @agent_ppo2.py:192][0m |          -0.0140 |          24.6129 |           0.0987 |
[32m[20230207 15:09:59 @agent_ppo2.py:192][0m |          -0.0062 |          22.4565 |           0.0985 |
[32m[20230207 15:10:00 @agent_ppo2.py:192][0m |          -0.0091 |          21.9674 |           0.0985 |
[32m[20230207 15:10:00 @agent_ppo2.py:192][0m |          -0.0031 |          21.6165 |           0.0985 |
[32m[20230207 15:10:00 @agent_ppo2.py:192][0m |          -0.0079 |          21.2665 |           0.0983 |
[32m[20230207 15:10:00 @agent_ppo2.py:192][0m |          -0.0047 |          20.6853 |           0.0983 |
[32m[20230207 15:10:00 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:10:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -101.60
[32m[20230207 15:10:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -50.01
[32m[20230207 15:10:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -76.02
[32m[20230207 15:10:01 @agent_ppo2.py:150][0m Total time:       9.32 min
[32m[20230207 15:10:01 @agent_ppo2.py:152][0m 479232 total steps have happened
[32m[20230207 15:10:01 @agent_ppo2.py:128][0m #------------------------ Iteration 234 --------------------------#
[32m[20230207 15:10:01 @agent_ppo2.py:134][0m Sampling time: 0.69 s by 1 slaves
[32m[20230207 15:10:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:01 @agent_ppo2.py:192][0m |          -0.0003 |          28.8754 |           0.0982 |
[32m[20230207 15:10:01 @agent_ppo2.py:192][0m |          -0.0019 |          17.2734 |           0.0981 |
[32m[20230207 15:10:02 @agent_ppo2.py:192][0m |          -0.0062 |          15.6273 |           0.0981 |
[32m[20230207 15:10:02 @agent_ppo2.py:192][0m |          -0.0064 |          14.1386 |           0.0980 |
[32m[20230207 15:10:02 @agent_ppo2.py:192][0m |          -0.0072 |          13.9375 |           0.0979 |
[32m[20230207 15:10:02 @agent_ppo2.py:192][0m |          -0.0090 |          12.8297 |           0.0978 |
[32m[20230207 15:10:02 @agent_ppo2.py:192][0m |          -0.0078 |          12.4412 |           0.0978 |
[32m[20230207 15:10:02 @agent_ppo2.py:192][0m |          -0.0102 |          12.0976 |           0.0977 |
[32m[20230207 15:10:02 @agent_ppo2.py:192][0m |          -0.0099 |          11.9550 |           0.0977 |
[32m[20230207 15:10:02 @agent_ppo2.py:192][0m |          -0.0084 |          11.5234 |           0.0976 |
[32m[20230207 15:10:02 @agent_ppo2.py:137][0m Policy update time: 0.69 s
[32m[20230207 15:10:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -101.88
[32m[20230207 15:10:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -41.30
[32m[20230207 15:10:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -28.09
[32m[20230207 15:10:03 @agent_ppo2.py:150][0m Total time:       9.36 min
[32m[20230207 15:10:03 @agent_ppo2.py:152][0m 481280 total steps have happened
[32m[20230207 15:10:03 @agent_ppo2.py:128][0m #------------------------ Iteration 235 --------------------------#
[32m[20230207 15:10:04 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:10:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:04 @agent_ppo2.py:192][0m |           0.0004 |          38.3228 |           0.0977 |
[32m[20230207 15:10:04 @agent_ppo2.py:192][0m |          -0.0039 |          26.2181 |           0.0975 |
[32m[20230207 15:10:04 @agent_ppo2.py:192][0m |          -0.0061 |          24.5288 |           0.0975 |
[32m[20230207 15:10:04 @agent_ppo2.py:192][0m |          -0.0036 |          23.5260 |           0.0973 |
[32m[20230207 15:10:04 @agent_ppo2.py:192][0m |           0.0047 |          22.7020 |           0.0973 |
[32m[20230207 15:10:04 @agent_ppo2.py:192][0m |          -0.0065 |          22.8313 |           0.0973 |
[32m[20230207 15:10:04 @agent_ppo2.py:192][0m |          -0.0039 |          21.8787 |           0.0972 |
[32m[20230207 15:10:04 @agent_ppo2.py:192][0m |          -0.0082 |          21.5934 |           0.0972 |
[32m[20230207 15:10:04 @agent_ppo2.py:192][0m |          -0.0524 |          21.4555 |           0.0972 |
[32m[20230207 15:10:04 @agent_ppo2.py:192][0m |          -0.0176 |          21.1453 |           0.0971 |
[32m[20230207 15:10:04 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:10:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -63.19
[32m[20230207 15:10:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 36.55
[32m[20230207 15:10:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -5.22
[32m[20230207 15:10:05 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -5.22
[32m[20230207 15:10:05 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -5.22
[32m[20230207 15:10:05 @agent_ppo2.py:150][0m Total time:       9.40 min
[32m[20230207 15:10:05 @agent_ppo2.py:152][0m 483328 total steps have happened
[32m[20230207 15:10:05 @agent_ppo2.py:128][0m #------------------------ Iteration 236 --------------------------#
[32m[20230207 15:10:06 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:10:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:06 @agent_ppo2.py:192][0m |           0.0034 |          12.3898 |           0.0970 |
[32m[20230207 15:10:06 @agent_ppo2.py:192][0m |          -0.0011 |           4.5809 |           0.0970 |
[32m[20230207 15:10:06 @agent_ppo2.py:192][0m |          -0.0034 |           4.2848 |           0.0970 |
[32m[20230207 15:10:06 @agent_ppo2.py:192][0m |          -0.0049 |           4.0911 |           0.0969 |
[32m[20230207 15:10:07 @agent_ppo2.py:192][0m |          -0.0064 |           3.9764 |           0.0969 |
[32m[20230207 15:10:07 @agent_ppo2.py:192][0m |          -0.0073 |           3.8552 |           0.0969 |
[32m[20230207 15:10:07 @agent_ppo2.py:192][0m |          -0.0063 |           3.7535 |           0.0969 |
[32m[20230207 15:10:07 @agent_ppo2.py:192][0m |          -0.0066 |           3.6717 |           0.0969 |
[32m[20230207 15:10:07 @agent_ppo2.py:192][0m |          -0.0086 |           3.5852 |           0.0969 |
[32m[20230207 15:10:07 @agent_ppo2.py:192][0m |          -0.0095 |           3.5317 |           0.0970 |
[32m[20230207 15:10:07 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:10:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -78.08
[32m[20230207 15:10:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -59.99
[32m[20230207 15:10:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -55.91
[32m[20230207 15:10:08 @agent_ppo2.py:150][0m Total time:       9.44 min
[32m[20230207 15:10:08 @agent_ppo2.py:152][0m 485376 total steps have happened
[32m[20230207 15:10:08 @agent_ppo2.py:128][0m #------------------------ Iteration 237 --------------------------#
[32m[20230207 15:10:09 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:10:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:09 @agent_ppo2.py:192][0m |           0.0009 |           7.7137 |           0.0957 |
[32m[20230207 15:10:09 @agent_ppo2.py:192][0m |           0.0003 |           4.1249 |           0.0958 |
[32m[20230207 15:10:09 @agent_ppo2.py:192][0m |          -0.0058 |           3.5239 |           0.0958 |
[32m[20230207 15:10:09 @agent_ppo2.py:192][0m |          -0.0130 |           3.3238 |           0.0959 |
[32m[20230207 15:10:09 @agent_ppo2.py:192][0m |          -0.0023 |           3.1471 |           0.0959 |
[32m[20230207 15:10:09 @agent_ppo2.py:192][0m |          -0.0081 |           3.0282 |           0.0959 |
[32m[20230207 15:10:09 @agent_ppo2.py:192][0m |          -0.0082 |           3.2087 |           0.0959 |
[32m[20230207 15:10:09 @agent_ppo2.py:192][0m |          -0.0077 |           3.0392 |           0.0959 |
[32m[20230207 15:10:09 @agent_ppo2.py:192][0m |          -0.0025 |           2.7953 |           0.0958 |
[32m[20230207 15:10:09 @agent_ppo2.py:192][0m |          -0.0167 |           2.7846 |           0.0958 |
[32m[20230207 15:10:09 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:10:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -62.05
[32m[20230207 15:10:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -16.58
[32m[20230207 15:10:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -102.32
[32m[20230207 15:10:10 @agent_ppo2.py:150][0m Total time:       9.48 min
[32m[20230207 15:10:10 @agent_ppo2.py:152][0m 487424 total steps have happened
[32m[20230207 15:10:10 @agent_ppo2.py:128][0m #------------------------ Iteration 238 --------------------------#
[32m[20230207 15:10:11 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:10:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:11 @agent_ppo2.py:192][0m |          -0.0138 |           4.2152 |           0.0996 |
[32m[20230207 15:10:11 @agent_ppo2.py:192][0m |           0.0123 |           3.0557 |           0.0996 |
[32m[20230207 15:10:11 @agent_ppo2.py:192][0m |          -0.0279 |           2.7914 |           0.0996 |
[32m[20230207 15:10:11 @agent_ppo2.py:192][0m |          -0.0054 |           2.6507 |           0.0996 |
[32m[20230207 15:10:11 @agent_ppo2.py:192][0m |          -0.0063 |           2.5444 |           0.0996 |
[32m[20230207 15:10:11 @agent_ppo2.py:192][0m |          -0.0131 |           2.4816 |           0.0996 |
[32m[20230207 15:10:11 @agent_ppo2.py:192][0m |           0.0034 |           2.3715 |           0.0996 |
[32m[20230207 15:10:11 @agent_ppo2.py:192][0m |          -0.0158 |           2.3541 |           0.0996 |
[32m[20230207 15:10:11 @agent_ppo2.py:192][0m |           0.0074 |           2.2417 |           0.0997 |
[32m[20230207 15:10:12 @agent_ppo2.py:192][0m |          -0.0122 |           2.1386 |           0.0996 |
[32m[20230207 15:10:12 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:10:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: -64.47
[32m[20230207 15:10:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -48.69
[32m[20230207 15:10:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -53.94
[32m[20230207 15:10:12 @agent_ppo2.py:150][0m Total time:       9.52 min
[32m[20230207 15:10:12 @agent_ppo2.py:152][0m 489472 total steps have happened
[32m[20230207 15:10:12 @agent_ppo2.py:128][0m #------------------------ Iteration 239 --------------------------#
[32m[20230207 15:10:13 @agent_ppo2.py:134][0m Sampling time: 0.92 s by 1 slaves
[32m[20230207 15:10:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:13 @agent_ppo2.py:192][0m |           0.0008 |          14.7711 |           0.1007 |
[32m[20230207 15:10:14 @agent_ppo2.py:192][0m |          -0.0014 |           8.2280 |           0.1007 |
[32m[20230207 15:10:14 @agent_ppo2.py:192][0m |          -0.0036 |           6.9580 |           0.1007 |
[32m[20230207 15:10:14 @agent_ppo2.py:192][0m |          -0.0049 |           6.2324 |           0.1006 |
[32m[20230207 15:10:14 @agent_ppo2.py:192][0m |          -0.0056 |           5.6402 |           0.1007 |
[32m[20230207 15:10:14 @agent_ppo2.py:192][0m |          -0.0065 |           5.2008 |           0.1006 |
[32m[20230207 15:10:14 @agent_ppo2.py:192][0m |          -0.0062 |           4.8211 |           0.1007 |
[32m[20230207 15:10:14 @agent_ppo2.py:192][0m |          -0.0070 |           4.5140 |           0.1006 |
[32m[20230207 15:10:14 @agent_ppo2.py:192][0m |          -0.0081 |           4.1801 |           0.1006 |
[32m[20230207 15:10:14 @agent_ppo2.py:192][0m |          -0.0083 |           3.9785 |           0.1006 |
[32m[20230207 15:10:14 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:10:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: -69.95
[32m[20230207 15:10:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 0.35
[32m[20230207 15:10:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -54.55
[32m[20230207 15:10:15 @agent_ppo2.py:150][0m Total time:       9.56 min
[32m[20230207 15:10:15 @agent_ppo2.py:152][0m 491520 total steps have happened
[32m[20230207 15:10:15 @agent_ppo2.py:128][0m #------------------------ Iteration 240 --------------------------#
[32m[20230207 15:10:16 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:10:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:16 @agent_ppo2.py:192][0m |           0.0091 |          11.7718 |           0.0973 |
[32m[20230207 15:10:16 @agent_ppo2.py:192][0m |          -0.0025 |           5.2177 |           0.0972 |
[32m[20230207 15:10:16 @agent_ppo2.py:192][0m |          -0.0055 |           4.3986 |           0.0971 |
[32m[20230207 15:10:16 @agent_ppo2.py:192][0m |          -0.0047 |           4.0029 |           0.0971 |
[32m[20230207 15:10:16 @agent_ppo2.py:192][0m |          -0.0057 |           3.7472 |           0.0970 |
[32m[20230207 15:10:16 @agent_ppo2.py:192][0m |          -0.0129 |           3.6099 |           0.0970 |
[32m[20230207 15:10:16 @agent_ppo2.py:192][0m |          -0.0174 |           3.5722 |           0.0970 |
[32m[20230207 15:10:16 @agent_ppo2.py:192][0m |          -0.0049 |           3.3918 |           0.0969 |
[32m[20230207 15:10:17 @agent_ppo2.py:192][0m |          -0.0092 |           3.2983 |           0.0968 |
[32m[20230207 15:10:17 @agent_ppo2.py:192][0m |          -0.0114 |           3.1458 |           0.0968 |
[32m[20230207 15:10:17 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:10:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: -48.35
[32m[20230207 15:10:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 6.20
[32m[20230207 15:10:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -26.68
[32m[20230207 15:10:17 @agent_ppo2.py:150][0m Total time:       9.60 min
[32m[20230207 15:10:17 @agent_ppo2.py:152][0m 493568 total steps have happened
[32m[20230207 15:10:17 @agent_ppo2.py:128][0m #------------------------ Iteration 241 --------------------------#
[32m[20230207 15:10:18 @agent_ppo2.py:134][0m Sampling time: 0.91 s by 1 slaves
[32m[20230207 15:10:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:19 @agent_ppo2.py:192][0m |          -0.0001 |          13.1033 |           0.1006 |
[32m[20230207 15:10:19 @agent_ppo2.py:192][0m |          -0.0028 |           5.6089 |           0.1006 |
[32m[20230207 15:10:19 @agent_ppo2.py:192][0m |          -0.0044 |           4.8353 |           0.1006 |
[32m[20230207 15:10:19 @agent_ppo2.py:192][0m |          -0.0060 |           4.1959 |           0.1006 |
[32m[20230207 15:10:19 @agent_ppo2.py:192][0m |          -0.0071 |           3.9970 |           0.1006 |
[32m[20230207 15:10:19 @agent_ppo2.py:192][0m |          -0.0080 |           3.6885 |           0.1005 |
[32m[20230207 15:10:19 @agent_ppo2.py:192][0m |          -0.0091 |           3.4624 |           0.1006 |
[32m[20230207 15:10:19 @agent_ppo2.py:192][0m |          -0.0096 |           3.4024 |           0.1005 |
[32m[20230207 15:10:19 @agent_ppo2.py:192][0m |          -0.0103 |           3.2045 |           0.1005 |
[32m[20230207 15:10:19 @agent_ppo2.py:192][0m |          -0.0108 |           3.1392 |           0.1005 |
[32m[20230207 15:10:19 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:10:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -81.36
[32m[20230207 15:10:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -30.32
[32m[20230207 15:10:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -40.34
[32m[20230207 15:10:20 @agent_ppo2.py:150][0m Total time:       9.65 min
[32m[20230207 15:10:20 @agent_ppo2.py:152][0m 495616 total steps have happened
[32m[20230207 15:10:20 @agent_ppo2.py:128][0m #------------------------ Iteration 242 --------------------------#
[32m[20230207 15:10:21 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:10:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:21 @agent_ppo2.py:192][0m |           0.0001 |          12.6545 |           0.1026 |
[32m[20230207 15:10:21 @agent_ppo2.py:192][0m |          -0.0046 |           7.9011 |           0.1025 |
[32m[20230207 15:10:21 @agent_ppo2.py:192][0m |          -0.0072 |           7.2325 |           0.1024 |
[32m[20230207 15:10:21 @agent_ppo2.py:192][0m |          -0.0086 |           6.8006 |           0.1023 |
[32m[20230207 15:10:21 @agent_ppo2.py:192][0m |          -0.0101 |           6.5394 |           0.1023 |
[32m[20230207 15:10:21 @agent_ppo2.py:192][0m |          -0.0110 |           6.2711 |           0.1024 |
[32m[20230207 15:10:21 @agent_ppo2.py:192][0m |          -0.0119 |           6.0933 |           0.1023 |
[32m[20230207 15:10:22 @agent_ppo2.py:192][0m |          -0.0117 |           5.9600 |           0.1024 |
[32m[20230207 15:10:22 @agent_ppo2.py:192][0m |          -0.0125 |           5.8458 |           0.1024 |
[32m[20230207 15:10:22 @agent_ppo2.py:192][0m |          -0.0131 |           5.6841 |           0.1024 |
[32m[20230207 15:10:22 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:10:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -72.55
[32m[20230207 15:10:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -21.22
[32m[20230207 15:10:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -73.41
[32m[20230207 15:10:23 @agent_ppo2.py:150][0m Total time:       9.69 min
[32m[20230207 15:10:23 @agent_ppo2.py:152][0m 497664 total steps have happened
[32m[20230207 15:10:23 @agent_ppo2.py:128][0m #------------------------ Iteration 243 --------------------------#
[32m[20230207 15:10:23 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:10:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:23 @agent_ppo2.py:192][0m |           0.0004 |           5.7060 |           0.1001 |
[32m[20230207 15:10:24 @agent_ppo2.py:192][0m |          -0.0029 |           3.3929 |           0.1000 |
[32m[20230207 15:10:24 @agent_ppo2.py:192][0m |          -0.0058 |           3.1131 |           0.0999 |
[32m[20230207 15:10:24 @agent_ppo2.py:192][0m |          -0.0053 |           2.9548 |           0.0998 |
[32m[20230207 15:10:24 @agent_ppo2.py:192][0m |          -0.0064 |           2.7576 |           0.0998 |
[32m[20230207 15:10:24 @agent_ppo2.py:192][0m |          -0.0070 |           2.6439 |           0.0998 |
[32m[20230207 15:10:24 @agent_ppo2.py:192][0m |          -0.0085 |           2.5829 |           0.0998 |
[32m[20230207 15:10:24 @agent_ppo2.py:192][0m |          -0.0069 |           2.4471 |           0.0998 |
[32m[20230207 15:10:24 @agent_ppo2.py:192][0m |          -0.0092 |           2.3756 |           0.0998 |
[32m[20230207 15:10:24 @agent_ppo2.py:192][0m |          -0.0088 |           2.2665 |           0.0998 |
[32m[20230207 15:10:24 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:10:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -74.40
[32m[20230207 15:10:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -56.65
[32m[20230207 15:10:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -77.01
[32m[20230207 15:10:25 @agent_ppo2.py:150][0m Total time:       9.73 min
[32m[20230207 15:10:25 @agent_ppo2.py:152][0m 499712 total steps have happened
[32m[20230207 15:10:25 @agent_ppo2.py:128][0m #------------------------ Iteration 244 --------------------------#
[32m[20230207 15:10:26 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:10:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:26 @agent_ppo2.py:192][0m |          -0.0017 |           4.2285 |           0.0990 |
[32m[20230207 15:10:26 @agent_ppo2.py:192][0m |           0.0009 |           3.0017 |           0.0990 |
[32m[20230207 15:10:26 @agent_ppo2.py:192][0m |          -0.0089 |           2.8508 |           0.0989 |
[32m[20230207 15:10:26 @agent_ppo2.py:192][0m |           0.0018 |           2.7349 |           0.0988 |
[32m[20230207 15:10:26 @agent_ppo2.py:192][0m |          -0.0079 |           2.6187 |           0.0988 |
[32m[20230207 15:10:26 @agent_ppo2.py:192][0m |          -0.0014 |           2.5866 |           0.0988 |
[32m[20230207 15:10:26 @agent_ppo2.py:192][0m |          -0.0075 |           2.4972 |           0.0988 |
[32m[20230207 15:10:26 @agent_ppo2.py:192][0m |          -0.0107 |           2.3890 |           0.0988 |
[32m[20230207 15:10:27 @agent_ppo2.py:192][0m |          -0.0059 |           2.2793 |           0.0988 |
[32m[20230207 15:10:27 @agent_ppo2.py:192][0m |          -0.0074 |           2.1861 |           0.0988 |
[32m[20230207 15:10:27 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:10:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -57.93
[32m[20230207 15:10:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -53.32
[32m[20230207 15:10:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -47.81
[32m[20230207 15:10:27 @agent_ppo2.py:150][0m Total time:       9.77 min
[32m[20230207 15:10:27 @agent_ppo2.py:152][0m 501760 total steps have happened
[32m[20230207 15:10:27 @agent_ppo2.py:128][0m #------------------------ Iteration 245 --------------------------#
[32m[20230207 15:10:28 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:10:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:28 @agent_ppo2.py:192][0m |           0.0035 |           5.1008 |           0.0961 |
[32m[20230207 15:10:28 @agent_ppo2.py:192][0m |           0.0106 |           3.4119 |           0.0961 |
[32m[20230207 15:10:28 @agent_ppo2.py:192][0m |          -0.0147 |           2.9408 |           0.0960 |
[32m[20230207 15:10:29 @agent_ppo2.py:192][0m |          -0.0027 |           2.7234 |           0.0961 |
[32m[20230207 15:10:29 @agent_ppo2.py:192][0m |           0.0044 |           2.5669 |           0.0961 |
[32m[20230207 15:10:29 @agent_ppo2.py:192][0m |          -0.0083 |           2.4596 |           0.0960 |
[32m[20230207 15:10:29 @agent_ppo2.py:192][0m |           0.0040 |           2.3961 |           0.0960 |
[32m[20230207 15:10:29 @agent_ppo2.py:192][0m |          -0.0184 |           2.3185 |           0.0960 |
[32m[20230207 15:10:29 @agent_ppo2.py:192][0m |          -0.0093 |           2.2482 |           0.0959 |
[32m[20230207 15:10:29 @agent_ppo2.py:192][0m |          -0.0130 |           2.2239 |           0.0959 |
[32m[20230207 15:10:29 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:10:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: 30.88
[32m[20230207 15:10:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 68.48
[32m[20230207 15:10:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 25.14
[32m[20230207 15:10:30 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 25.14
[32m[20230207 15:10:30 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 25.14
[32m[20230207 15:10:30 @agent_ppo2.py:150][0m Total time:       9.81 min
[32m[20230207 15:10:30 @agent_ppo2.py:152][0m 503808 total steps have happened
[32m[20230207 15:10:30 @agent_ppo2.py:128][0m #------------------------ Iteration 246 --------------------------#
[32m[20230207 15:10:31 @agent_ppo2.py:134][0m Sampling time: 0.73 s by 1 slaves
[32m[20230207 15:10:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:31 @agent_ppo2.py:192][0m |          -0.0007 |           7.4791 |           0.1028 |
[32m[20230207 15:10:31 @agent_ppo2.py:192][0m |          -0.0117 |           4.3182 |           0.1027 |
[32m[20230207 15:10:31 @agent_ppo2.py:192][0m |          -0.0052 |           3.1446 |           0.1027 |
[32m[20230207 15:10:31 @agent_ppo2.py:192][0m |          -0.0086 |           2.6961 |           0.1027 |
[32m[20230207 15:10:31 @agent_ppo2.py:192][0m |          -0.0127 |           2.5358 |           0.1026 |
[32m[20230207 15:10:31 @agent_ppo2.py:192][0m |          -0.0085 |           2.3937 |           0.1026 |
[32m[20230207 15:10:31 @agent_ppo2.py:192][0m |          -0.0048 |           2.3473 |           0.1026 |
[32m[20230207 15:10:31 @agent_ppo2.py:192][0m |          -0.0116 |           2.2467 |           0.1026 |
[32m[20230207 15:10:31 @agent_ppo2.py:192][0m |          -0.0133 |           2.1426 |           0.1026 |
[32m[20230207 15:10:31 @agent_ppo2.py:192][0m |          -0.0257 |           2.0959 |           0.1026 |
[32m[20230207 15:10:31 @agent_ppo2.py:137][0m Policy update time: 0.75 s
[32m[20230207 15:10:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -24.58
[32m[20230207 15:10:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 8.85
[32m[20230207 15:10:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -34.71
[32m[20230207 15:10:32 @agent_ppo2.py:150][0m Total time:       9.85 min
[32m[20230207 15:10:32 @agent_ppo2.py:152][0m 505856 total steps have happened
[32m[20230207 15:10:32 @agent_ppo2.py:128][0m #------------------------ Iteration 247 --------------------------#
[32m[20230207 15:10:33 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:10:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:33 @agent_ppo2.py:192][0m |           0.0037 |           7.9711 |           0.1021 |
[32m[20230207 15:10:33 @agent_ppo2.py:192][0m |          -0.0003 |           5.0863 |           0.1020 |
[32m[20230207 15:10:33 @agent_ppo2.py:192][0m |          -0.0021 |           4.5169 |           0.1019 |
[32m[20230207 15:10:33 @agent_ppo2.py:192][0m |          -0.0034 |           4.2889 |           0.1017 |
[32m[20230207 15:10:33 @agent_ppo2.py:192][0m |          -0.0073 |           4.0228 |           0.1016 |
[32m[20230207 15:10:34 @agent_ppo2.py:192][0m |          -0.0039 |           3.8529 |           0.1014 |
[32m[20230207 15:10:34 @agent_ppo2.py:192][0m |          -0.0084 |           3.7893 |           0.1013 |
[32m[20230207 15:10:34 @agent_ppo2.py:192][0m |          -0.0090 |           3.5723 |           0.1012 |
[32m[20230207 15:10:34 @agent_ppo2.py:192][0m |          -0.0114 |           3.4409 |           0.1011 |
[32m[20230207 15:10:34 @agent_ppo2.py:192][0m |          -0.0118 |           3.3831 |           0.1010 |
[32m[20230207 15:10:34 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:10:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: -59.35
[32m[20230207 15:10:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -32.34
[32m[20230207 15:10:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -62.49
[32m[20230207 15:10:35 @agent_ppo2.py:150][0m Total time:       9.89 min
[32m[20230207 15:10:35 @agent_ppo2.py:152][0m 507904 total steps have happened
[32m[20230207 15:10:35 @agent_ppo2.py:128][0m #------------------------ Iteration 248 --------------------------#
[32m[20230207 15:10:35 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:10:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:36 @agent_ppo2.py:192][0m |          -0.0052 |          23.0466 |           0.0971 |
[32m[20230207 15:10:36 @agent_ppo2.py:192][0m |           0.0004 |          11.2747 |           0.0972 |
[32m[20230207 15:10:36 @agent_ppo2.py:192][0m |          -0.0151 |           7.6015 |           0.0972 |
[32m[20230207 15:10:36 @agent_ppo2.py:192][0m |          -0.0158 |           6.4214 |           0.0972 |
[32m[20230207 15:10:36 @agent_ppo2.py:192][0m |          -0.0143 |           6.0447 |           0.0972 |
[32m[20230207 15:10:36 @agent_ppo2.py:192][0m |          -0.0135 |           5.6002 |           0.0972 |
[32m[20230207 15:10:36 @agent_ppo2.py:192][0m |          -0.0170 |           5.4694 |           0.0972 |
[32m[20230207 15:10:36 @agent_ppo2.py:192][0m |           0.0215 |           5.4367 |           0.0972 |
[32m[20230207 15:10:36 @agent_ppo2.py:192][0m |          -0.0162 |           5.2738 |           0.0972 |
[32m[20230207 15:10:36 @agent_ppo2.py:192][0m |          -0.0152 |           5.0790 |           0.0971 |
[32m[20230207 15:10:36 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:10:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: -90.84
[32m[20230207 15:10:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -27.80
[32m[20230207 15:10:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -55.08
[32m[20230207 15:10:37 @agent_ppo2.py:150][0m Total time:       9.93 min
[32m[20230207 15:10:37 @agent_ppo2.py:152][0m 509952 total steps have happened
[32m[20230207 15:10:37 @agent_ppo2.py:128][0m #------------------------ Iteration 249 --------------------------#
[32m[20230207 15:10:38 @agent_ppo2.py:134][0m Sampling time: 0.54 s by 1 slaves
[32m[20230207 15:10:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:38 @agent_ppo2.py:192][0m |          -0.0027 |          50.2601 |           0.1014 |
[32m[20230207 15:10:38 @agent_ppo2.py:192][0m |          -0.0029 |          30.6391 |           0.1013 |
[32m[20230207 15:10:38 @agent_ppo2.py:192][0m |          -0.0085 |          26.4839 |           0.1012 |
[32m[20230207 15:10:38 @agent_ppo2.py:192][0m |          -0.0117 |          23.9764 |           0.1011 |
[32m[20230207 15:10:38 @agent_ppo2.py:192][0m |          -0.0141 |          22.5919 |           0.1011 |
[32m[20230207 15:10:38 @agent_ppo2.py:192][0m |          -0.0121 |          21.4506 |           0.1012 |
[32m[20230207 15:10:38 @agent_ppo2.py:192][0m |          -0.0102 |          21.0651 |           0.1011 |
[32m[20230207 15:10:38 @agent_ppo2.py:192][0m |          -0.0138 |          17.2861 |           0.1011 |
[32m[20230207 15:10:38 @agent_ppo2.py:192][0m |          -0.0137 |          16.3255 |           0.1011 |
[32m[20230207 15:10:38 @agent_ppo2.py:192][0m |          -0.0145 |          13.7124 |           0.1010 |
[32m[20230207 15:10:38 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:10:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -95.89
[32m[20230207 15:10:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -31.37
[32m[20230207 15:10:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -60.46
[32m[20230207 15:10:39 @agent_ppo2.py:150][0m Total time:       9.96 min
[32m[20230207 15:10:39 @agent_ppo2.py:152][0m 512000 total steps have happened
[32m[20230207 15:10:39 @agent_ppo2.py:128][0m #------------------------ Iteration 250 --------------------------#
[32m[20230207 15:10:40 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:10:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:40 @agent_ppo2.py:192][0m |           0.0054 |           4.6448 |           0.0983 |
[32m[20230207 15:10:40 @agent_ppo2.py:192][0m |          -0.0013 |           2.6380 |           0.0981 |
[32m[20230207 15:10:40 @agent_ppo2.py:192][0m |          -0.0022 |           2.3735 |           0.0982 |
[32m[20230207 15:10:40 @agent_ppo2.py:192][0m |          -0.0024 |           2.3734 |           0.0981 |
[32m[20230207 15:10:40 @agent_ppo2.py:192][0m |           0.0028 |           2.2391 |           0.0980 |
[32m[20230207 15:10:40 @agent_ppo2.py:192][0m |          -0.0355 |           2.1667 |           0.0981 |
[32m[20230207 15:10:40 @agent_ppo2.py:192][0m |          -0.0192 |           2.1542 |           0.0981 |
[32m[20230207 15:10:40 @agent_ppo2.py:192][0m |          -0.0450 |           2.1126 |           0.0981 |
[32m[20230207 15:10:40 @agent_ppo2.py:192][0m |          -0.0093 |           2.0543 |           0.0979 |
[32m[20230207 15:10:40 @agent_ppo2.py:192][0m |          -0.0030 |           1.9891 |           0.0981 |
[32m[20230207 15:10:40 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:10:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -47.12
[32m[20230207 15:10:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -41.11
[32m[20230207 15:10:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -34.02
[32m[20230207 15:10:41 @agent_ppo2.py:150][0m Total time:      10.00 min
[32m[20230207 15:10:41 @agent_ppo2.py:152][0m 514048 total steps have happened
[32m[20230207 15:10:41 @agent_ppo2.py:128][0m #------------------------ Iteration 251 --------------------------#
[32m[20230207 15:10:42 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:10:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:42 @agent_ppo2.py:192][0m |           0.0100 |           3.5013 |           0.0986 |
[32m[20230207 15:10:42 @agent_ppo2.py:192][0m |           0.0101 |           2.1657 |           0.0985 |
[32m[20230207 15:10:42 @agent_ppo2.py:192][0m |          -0.0157 |           2.1057 |           0.0985 |
[32m[20230207 15:10:42 @agent_ppo2.py:192][0m |          -0.0085 |           1.9467 |           0.0984 |
[32m[20230207 15:10:43 @agent_ppo2.py:192][0m |           0.0035 |           1.9237 |           0.0984 |
[32m[20230207 15:10:43 @agent_ppo2.py:192][0m |          -0.0325 |           1.8997 |           0.0984 |
[32m[20230207 15:10:43 @agent_ppo2.py:192][0m |          -0.0198 |           1.8242 |           0.0983 |
[32m[20230207 15:10:43 @agent_ppo2.py:192][0m |          -0.0230 |           1.8037 |           0.0983 |
[32m[20230207 15:10:43 @agent_ppo2.py:192][0m |          -0.0331 |           1.8298 |           0.0983 |
[32m[20230207 15:10:43 @agent_ppo2.py:192][0m |           0.0094 |           1.7501 |           0.0983 |
[32m[20230207 15:10:43 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:10:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -12.13
[32m[20230207 15:10:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -4.24
[32m[20230207 15:10:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -62.69
[32m[20230207 15:10:43 @agent_ppo2.py:150][0m Total time:      10.04 min
[32m[20230207 15:10:43 @agent_ppo2.py:152][0m 516096 total steps have happened
[32m[20230207 15:10:43 @agent_ppo2.py:128][0m #------------------------ Iteration 252 --------------------------#
[32m[20230207 15:10:44 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230207 15:10:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:44 @agent_ppo2.py:192][0m |          -0.0018 |           9.2943 |           0.1025 |
[32m[20230207 15:10:44 @agent_ppo2.py:192][0m |          -0.0050 |           5.1790 |           0.1024 |
[32m[20230207 15:10:44 @agent_ppo2.py:192][0m |          -0.0050 |           3.9100 |           0.1023 |
[32m[20230207 15:10:44 @agent_ppo2.py:192][0m |          -0.0084 |           3.2328 |           0.1022 |
[32m[20230207 15:10:45 @agent_ppo2.py:192][0m |          -0.0091 |           2.7625 |           0.1022 |
[32m[20230207 15:10:45 @agent_ppo2.py:192][0m |          -0.0101 |           2.3935 |           0.1021 |
[32m[20230207 15:10:45 @agent_ppo2.py:192][0m |          -0.0110 |           2.1757 |           0.1020 |
[32m[20230207 15:10:45 @agent_ppo2.py:192][0m |          -0.0110 |           1.9629 |           0.1020 |
[32m[20230207 15:10:45 @agent_ppo2.py:192][0m |          -0.0129 |           1.8607 |           0.1020 |
[32m[20230207 15:10:45 @agent_ppo2.py:192][0m |          -0.0114 |           1.8009 |           0.1020 |
[32m[20230207 15:10:45 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230207 15:10:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -43.36
[32m[20230207 15:10:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 21.71
[32m[20230207 15:10:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -59.94
[32m[20230207 15:10:46 @agent_ppo2.py:150][0m Total time:      10.08 min
[32m[20230207 15:10:46 @agent_ppo2.py:152][0m 518144 total steps have happened
[32m[20230207 15:10:46 @agent_ppo2.py:128][0m #------------------------ Iteration 253 --------------------------#
[32m[20230207 15:10:47 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:10:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:47 @agent_ppo2.py:192][0m |           0.0098 |           2.6514 |           0.1007 |
[32m[20230207 15:10:47 @agent_ppo2.py:192][0m |           0.0058 |           2.1066 |           0.1006 |
[32m[20230207 15:10:47 @agent_ppo2.py:192][0m |          -0.0211 |           1.9848 |           0.1006 |
[32m[20230207 15:10:47 @agent_ppo2.py:192][0m |          -0.0193 |           1.8665 |           0.1006 |
[32m[20230207 15:10:47 @agent_ppo2.py:192][0m |           0.0057 |           1.8027 |           0.1006 |
[32m[20230207 15:10:47 @agent_ppo2.py:192][0m |          -0.0050 |           1.7648 |           0.1006 |
[32m[20230207 15:10:47 @agent_ppo2.py:192][0m |           0.0111 |           1.7315 |           0.1006 |
[32m[20230207 15:10:47 @agent_ppo2.py:192][0m |          -0.0256 |           1.7457 |           0.1007 |
[32m[20230207 15:10:47 @agent_ppo2.py:192][0m |          -0.0049 |           1.6854 |           0.1007 |
[32m[20230207 15:10:47 @agent_ppo2.py:192][0m |          -0.0179 |           1.6287 |           0.1008 |
[32m[20230207 15:10:47 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:10:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -12.98
[32m[20230207 15:10:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -9.85
[32m[20230207 15:10:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -80.14
[32m[20230207 15:10:48 @agent_ppo2.py:150][0m Total time:      10.11 min
[32m[20230207 15:10:48 @agent_ppo2.py:152][0m 520192 total steps have happened
[32m[20230207 15:10:48 @agent_ppo2.py:128][0m #------------------------ Iteration 254 --------------------------#
[32m[20230207 15:10:49 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:10:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:49 @agent_ppo2.py:192][0m |           0.0001 |          11.6245 |           0.1035 |
[32m[20230207 15:10:49 @agent_ppo2.py:192][0m |          -0.0036 |           3.4197 |           0.1035 |
[32m[20230207 15:10:49 @agent_ppo2.py:192][0m |          -0.0050 |           2.3949 |           0.1035 |
[32m[20230207 15:10:49 @agent_ppo2.py:192][0m |          -0.0072 |           2.0975 |           0.1035 |
[32m[20230207 15:10:49 @agent_ppo2.py:192][0m |          -0.0083 |           1.9501 |           0.1034 |
[32m[20230207 15:10:49 @agent_ppo2.py:192][0m |          -0.0101 |           1.8220 |           0.1034 |
[32m[20230207 15:10:49 @agent_ppo2.py:192][0m |          -0.0104 |           1.7439 |           0.1034 |
[32m[20230207 15:10:50 @agent_ppo2.py:192][0m |          -0.0111 |           1.6814 |           0.1034 |
[32m[20230207 15:10:50 @agent_ppo2.py:192][0m |          -0.0116 |           1.6186 |           0.1034 |
[32m[20230207 15:10:50 @agent_ppo2.py:192][0m |          -0.0117 |           1.5849 |           0.1034 |
[32m[20230207 15:10:50 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:10:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: -77.67
[32m[20230207 15:10:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -24.82
[32m[20230207 15:10:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -38.83
[32m[20230207 15:10:51 @agent_ppo2.py:150][0m Total time:      10.16 min
[32m[20230207 15:10:51 @agent_ppo2.py:152][0m 522240 total steps have happened
[32m[20230207 15:10:51 @agent_ppo2.py:128][0m #------------------------ Iteration 255 --------------------------#
[32m[20230207 15:10:51 @agent_ppo2.py:134][0m Sampling time: 0.66 s by 1 slaves
[32m[20230207 15:10:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:51 @agent_ppo2.py:192][0m |          -0.0001 |          23.5920 |           0.1016 |
[32m[20230207 15:10:51 @agent_ppo2.py:192][0m |          -0.0038 |           6.5392 |           0.1016 |
[32m[20230207 15:10:51 @agent_ppo2.py:192][0m |          -0.0057 |           4.0907 |           0.1015 |
[32m[20230207 15:10:51 @agent_ppo2.py:192][0m |          -0.0074 |           3.3576 |           0.1013 |
[32m[20230207 15:10:52 @agent_ppo2.py:192][0m |          -0.0087 |           2.6981 |           0.1013 |
[32m[20230207 15:10:52 @agent_ppo2.py:192][0m |          -0.0095 |           2.3817 |           0.1013 |
[32m[20230207 15:10:52 @agent_ppo2.py:192][0m |          -0.0105 |           2.2392 |           0.1012 |
[32m[20230207 15:10:52 @agent_ppo2.py:192][0m |          -0.0111 |           2.0570 |           0.1012 |
[32m[20230207 15:10:52 @agent_ppo2.py:192][0m |          -0.0115 |           1.8754 |           0.1012 |
[32m[20230207 15:10:52 @agent_ppo2.py:192][0m |          -0.0122 |           1.7697 |           0.1012 |
[32m[20230207 15:10:52 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230207 15:10:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -73.29
[32m[20230207 15:10:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -10.56
[32m[20230207 15:10:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -67.16
[32m[20230207 15:10:53 @agent_ppo2.py:150][0m Total time:      10.19 min
[32m[20230207 15:10:53 @agent_ppo2.py:152][0m 524288 total steps have happened
[32m[20230207 15:10:53 @agent_ppo2.py:128][0m #------------------------ Iteration 256 --------------------------#
[32m[20230207 15:10:53 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:10:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:54 @agent_ppo2.py:192][0m |          -0.0015 |          29.4496 |           0.1015 |
[32m[20230207 15:10:54 @agent_ppo2.py:192][0m |           0.0686 |          17.2207 |           0.1013 |
[32m[20230207 15:10:54 @agent_ppo2.py:192][0m |          -0.0022 |          15.1418 |           0.1010 |
[32m[20230207 15:10:54 @agent_ppo2.py:192][0m |          -0.0114 |          12.4825 |           0.1012 |
[32m[20230207 15:10:54 @agent_ppo2.py:192][0m |           0.0043 |          11.7206 |           0.1013 |
[32m[20230207 15:10:54 @agent_ppo2.py:192][0m |          -0.0375 |          11.3974 |           0.1011 |
[32m[20230207 15:10:54 @agent_ppo2.py:192][0m |          -0.0163 |          10.8277 |           0.1011 |
[32m[20230207 15:10:54 @agent_ppo2.py:192][0m |           0.0049 |          10.3570 |           0.1011 |
[32m[20230207 15:10:54 @agent_ppo2.py:192][0m |          -0.0070 |           9.9088 |           0.1010 |
[32m[20230207 15:10:54 @agent_ppo2.py:192][0m |          -0.0129 |          10.6434 |           0.1009 |
[32m[20230207 15:10:54 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:10:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: -20.53
[32m[20230207 15:10:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 23.99
[32m[20230207 15:10:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -86.09
[32m[20230207 15:10:55 @agent_ppo2.py:150][0m Total time:      10.23 min
[32m[20230207 15:10:55 @agent_ppo2.py:152][0m 526336 total steps have happened
[32m[20230207 15:10:55 @agent_ppo2.py:128][0m #------------------------ Iteration 257 --------------------------#
[32m[20230207 15:10:56 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:10:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:56 @agent_ppo2.py:192][0m |          -0.0113 |          30.5042 |           0.0968 |
[32m[20230207 15:10:56 @agent_ppo2.py:192][0m |          -0.0007 |          14.1269 |           0.0966 |
[32m[20230207 15:10:56 @agent_ppo2.py:192][0m |           0.0076 |          11.9120 |           0.0965 |
[32m[20230207 15:10:56 @agent_ppo2.py:192][0m |          -0.0195 |          11.1631 |           0.0964 |
[32m[20230207 15:10:56 @agent_ppo2.py:192][0m |          -0.0153 |          10.7450 |           0.0963 |
[32m[20230207 15:10:56 @agent_ppo2.py:192][0m |           0.0054 |          10.2703 |           0.0962 |
[32m[20230207 15:10:56 @agent_ppo2.py:192][0m |          -0.0171 |           9.8458 |           0.0962 |
[32m[20230207 15:10:56 @agent_ppo2.py:192][0m |          -0.0030 |           9.5910 |           0.0961 |
[32m[20230207 15:10:56 @agent_ppo2.py:192][0m |          -0.0289 |           9.5071 |           0.0960 |
[32m[20230207 15:10:56 @agent_ppo2.py:192][0m |          -0.0232 |           9.1863 |           0.0961 |
[32m[20230207 15:10:56 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:10:57 @agent_ppo2.py:145][0m Average TRAINING episode reward: -87.72
[32m[20230207 15:10:57 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -71.46
[32m[20230207 15:10:57 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -96.35
[32m[20230207 15:10:57 @agent_ppo2.py:150][0m Total time:      10.26 min
[32m[20230207 15:10:57 @agent_ppo2.py:152][0m 528384 total steps have happened
[32m[20230207 15:10:57 @agent_ppo2.py:128][0m #------------------------ Iteration 258 --------------------------#
[32m[20230207 15:10:58 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:10:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:10:58 @agent_ppo2.py:192][0m |           0.0002 |          21.7038 |           0.0979 |
[32m[20230207 15:10:58 @agent_ppo2.py:192][0m |          -0.0011 |          12.7483 |           0.0978 |
[32m[20230207 15:10:58 @agent_ppo2.py:192][0m |          -0.0033 |           8.8223 |           0.0977 |
[32m[20230207 15:10:58 @agent_ppo2.py:192][0m |          -0.0024 |           7.3186 |           0.0977 |
[32m[20230207 15:10:58 @agent_ppo2.py:192][0m |          -0.0058 |           6.2588 |           0.0978 |
[32m[20230207 15:10:58 @agent_ppo2.py:192][0m |          -0.0045 |           4.7791 |           0.0978 |
[32m[20230207 15:10:58 @agent_ppo2.py:192][0m |          -0.0021 |           4.0133 |           0.0977 |
[32m[20230207 15:10:58 @agent_ppo2.py:192][0m |          -0.0070 |           3.2688 |           0.0978 |
[32m[20230207 15:10:58 @agent_ppo2.py:192][0m |          -0.0039 |           2.6869 |           0.0978 |
[32m[20230207 15:10:58 @agent_ppo2.py:192][0m |           0.0006 |           2.8378 |           0.0978 |
[32m[20230207 15:10:58 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230207 15:10:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: -92.16
[32m[20230207 15:10:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -65.74
[32m[20230207 15:10:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -77.60
[32m[20230207 15:10:59 @agent_ppo2.py:150][0m Total time:      10.29 min
[32m[20230207 15:10:59 @agent_ppo2.py:152][0m 530432 total steps have happened
[32m[20230207 15:10:59 @agent_ppo2.py:128][0m #------------------------ Iteration 259 --------------------------#
[32m[20230207 15:11:00 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:11:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:00 @agent_ppo2.py:192][0m |           0.0024 |          13.8258 |           0.0978 |
[32m[20230207 15:11:00 @agent_ppo2.py:192][0m |          -0.0098 |           8.2896 |           0.0976 |
[32m[20230207 15:11:00 @agent_ppo2.py:192][0m |          -0.0033 |           7.1904 |           0.0973 |
[32m[20230207 15:11:00 @agent_ppo2.py:192][0m |          -0.0014 |           6.5857 |           0.0973 |
[32m[20230207 15:11:00 @agent_ppo2.py:192][0m |          -0.0051 |           6.1670 |           0.0973 |
[32m[20230207 15:11:00 @agent_ppo2.py:192][0m |          -0.0079 |           5.9272 |           0.0972 |
[32m[20230207 15:11:00 @agent_ppo2.py:192][0m |          -0.0015 |           5.6280 |           0.0972 |
[32m[20230207 15:11:00 @agent_ppo2.py:192][0m |          -0.0082 |           5.3610 |           0.0971 |
[32m[20230207 15:11:00 @agent_ppo2.py:192][0m |          -0.0098 |           5.1894 |           0.0971 |
[32m[20230207 15:11:00 @agent_ppo2.py:192][0m |           0.0013 |           5.2173 |           0.0971 |
[32m[20230207 15:11:00 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:11:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -38.57
[32m[20230207 15:11:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 2.78
[32m[20230207 15:11:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -45.96
[32m[20230207 15:11:01 @agent_ppo2.py:150][0m Total time:      10.33 min
[32m[20230207 15:11:01 @agent_ppo2.py:152][0m 532480 total steps have happened
[32m[20230207 15:11:01 @agent_ppo2.py:128][0m #------------------------ Iteration 260 --------------------------#
[32m[20230207 15:11:02 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:11:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:02 @agent_ppo2.py:192][0m |           0.0000 |          25.5917 |           0.0999 |
[32m[20230207 15:11:02 @agent_ppo2.py:192][0m |          -0.0013 |          15.7741 |           0.0999 |
[32m[20230207 15:11:02 @agent_ppo2.py:192][0m |          -0.0031 |          11.3529 |           0.0999 |
[32m[20230207 15:11:02 @agent_ppo2.py:192][0m |          -0.0041 |           9.2302 |           0.0999 |
[32m[20230207 15:11:02 @agent_ppo2.py:192][0m |          -0.0053 |           8.0643 |           0.0999 |
[32m[20230207 15:11:03 @agent_ppo2.py:192][0m |          -0.0069 |           7.3735 |           0.0999 |
[32m[20230207 15:11:03 @agent_ppo2.py:192][0m |          -0.0074 |           6.5864 |           0.0999 |
[32m[20230207 15:11:03 @agent_ppo2.py:192][0m |          -0.0083 |           6.1398 |           0.0999 |
[32m[20230207 15:11:03 @agent_ppo2.py:192][0m |          -0.0089 |           5.6841 |           0.0999 |
[32m[20230207 15:11:03 @agent_ppo2.py:192][0m |          -0.0095 |           5.3812 |           0.0999 |
[32m[20230207 15:11:03 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:11:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: -72.92
[32m[20230207 15:11:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -13.03
[32m[20230207 15:11:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -15.01
[32m[20230207 15:11:04 @agent_ppo2.py:150][0m Total time:      10.37 min
[32m[20230207 15:11:04 @agent_ppo2.py:152][0m 534528 total steps have happened
[32m[20230207 15:11:04 @agent_ppo2.py:128][0m #------------------------ Iteration 261 --------------------------#
[32m[20230207 15:11:04 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:11:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:05 @agent_ppo2.py:192][0m |          -0.0130 |           7.0585 |           0.0986 |
[32m[20230207 15:11:05 @agent_ppo2.py:192][0m |          -0.0106 |           5.2512 |           0.0986 |
[32m[20230207 15:11:05 @agent_ppo2.py:192][0m |          -0.0029 |           5.0183 |           0.0986 |
[32m[20230207 15:11:05 @agent_ppo2.py:192][0m |          -0.0004 |           4.8542 |           0.0987 |
[32m[20230207 15:11:05 @agent_ppo2.py:192][0m |          -0.0067 |           4.7533 |           0.0986 |
[32m[20230207 15:11:05 @agent_ppo2.py:192][0m |          -0.0050 |           4.6808 |           0.0986 |
[32m[20230207 15:11:05 @agent_ppo2.py:192][0m |          -0.0079 |           4.5836 |           0.0986 |
[32m[20230207 15:11:05 @agent_ppo2.py:192][0m |          -0.0055 |           4.5548 |           0.0986 |
[32m[20230207 15:11:05 @agent_ppo2.py:192][0m |          -0.0212 |           4.5478 |           0.0987 |
[32m[20230207 15:11:05 @agent_ppo2.py:192][0m |          -0.0041 |           4.6342 |           0.0986 |
[32m[20230207 15:11:05 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: -57.10
[32m[20230207 15:11:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -19.82
[32m[20230207 15:11:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -48.06
[32m[20230207 15:11:06 @agent_ppo2.py:150][0m Total time:      10.42 min
[32m[20230207 15:11:06 @agent_ppo2.py:152][0m 536576 total steps have happened
[32m[20230207 15:11:06 @agent_ppo2.py:128][0m #------------------------ Iteration 262 --------------------------#
[32m[20230207 15:11:07 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230207 15:11:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:07 @agent_ppo2.py:192][0m |          -0.0001 |          46.5218 |           0.1032 |
[32m[20230207 15:11:07 @agent_ppo2.py:192][0m |          -0.0012 |          18.8834 |           0.1032 |
[32m[20230207 15:11:07 @agent_ppo2.py:192][0m |          -0.0031 |          13.5317 |           0.1032 |
[32m[20230207 15:11:07 @agent_ppo2.py:192][0m |          -0.0051 |          11.4486 |           0.1030 |
[32m[20230207 15:11:07 @agent_ppo2.py:192][0m |          -0.0067 |          10.1359 |           0.1030 |
[32m[20230207 15:11:07 @agent_ppo2.py:192][0m |          -0.0082 |           9.4136 |           0.1030 |
[32m[20230207 15:11:07 @agent_ppo2.py:192][0m |          -0.0094 |           8.6988 |           0.1029 |
[32m[20230207 15:11:07 @agent_ppo2.py:192][0m |          -0.0100 |           8.1641 |           0.1029 |
[32m[20230207 15:11:07 @agent_ppo2.py:192][0m |          -0.0119 |           7.8481 |           0.1029 |
[32m[20230207 15:11:07 @agent_ppo2.py:192][0m |          -0.0115 |           7.4603 |           0.1028 |
[32m[20230207 15:11:07 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:11:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -88.77
[32m[20230207 15:11:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -18.00
[32m[20230207 15:11:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 15.90
[32m[20230207 15:11:08 @agent_ppo2.py:150][0m Total time:      10.45 min
[32m[20230207 15:11:08 @agent_ppo2.py:152][0m 538624 total steps have happened
[32m[20230207 15:11:08 @agent_ppo2.py:128][0m #------------------------ Iteration 263 --------------------------#
[32m[20230207 15:11:09 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:11:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:09 @agent_ppo2.py:192][0m |           0.0144 |           8.2348 |           0.1023 |
[32m[20230207 15:11:09 @agent_ppo2.py:192][0m |           0.0023 |           5.4342 |           0.1022 |
[32m[20230207 15:11:09 @agent_ppo2.py:192][0m |          -0.0121 |           4.8063 |           0.1023 |
[32m[20230207 15:11:09 @agent_ppo2.py:192][0m |          -0.0186 |           4.4592 |           0.1022 |
[32m[20230207 15:11:09 @agent_ppo2.py:192][0m |          -0.0113 |           4.3235 |           0.1021 |
[32m[20230207 15:11:09 @agent_ppo2.py:192][0m |           0.0194 |           4.1510 |           0.1021 |
[32m[20230207 15:11:10 @agent_ppo2.py:192][0m |           0.0006 |           4.1091 |           0.1020 |
[32m[20230207 15:11:10 @agent_ppo2.py:192][0m |          -0.0241 |           3.9885 |           0.1020 |
[32m[20230207 15:11:10 @agent_ppo2.py:192][0m |          -0.0044 |           3.8483 |           0.1020 |
[32m[20230207 15:11:10 @agent_ppo2.py:192][0m |          -0.0073 |           3.7823 |           0.1019 |
[32m[20230207 15:11:10 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:11:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -62.86
[32m[20230207 15:11:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -18.13
[32m[20230207 15:11:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -71.52
[32m[20230207 15:11:10 @agent_ppo2.py:150][0m Total time:      10.49 min
[32m[20230207 15:11:10 @agent_ppo2.py:152][0m 540672 total steps have happened
[32m[20230207 15:11:10 @agent_ppo2.py:128][0m #------------------------ Iteration 264 --------------------------#
[32m[20230207 15:11:11 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:11:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:11 @agent_ppo2.py:192][0m |          -0.0112 |           4.2961 |           0.0997 |
[32m[20230207 15:11:11 @agent_ppo2.py:192][0m |          -0.0210 |           3.2205 |           0.0997 |
[32m[20230207 15:11:11 @agent_ppo2.py:192][0m |           0.0107 |           2.9806 |           0.0996 |
[32m[20230207 15:11:12 @agent_ppo2.py:192][0m |          -0.0050 |           2.8013 |           0.0995 |
[32m[20230207 15:11:12 @agent_ppo2.py:192][0m |          -0.0281 |           2.7066 |           0.0995 |
[32m[20230207 15:11:12 @agent_ppo2.py:192][0m |           0.0118 |           2.6026 |           0.0995 |
[32m[20230207 15:11:12 @agent_ppo2.py:192][0m |          -0.0022 |           2.4529 |           0.0995 |
[32m[20230207 15:11:12 @agent_ppo2.py:192][0m |          -0.0004 |           2.3846 |           0.0995 |
[32m[20230207 15:11:12 @agent_ppo2.py:192][0m |          -0.0187 |           2.3356 |           0.0995 |
[32m[20230207 15:11:12 @agent_ppo2.py:192][0m |          -0.0053 |           2.3116 |           0.0994 |
[32m[20230207 15:11:12 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -28.81
[32m[20230207 15:11:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -25.01
[32m[20230207 15:11:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -25.29
[32m[20230207 15:11:13 @agent_ppo2.py:150][0m Total time:      10.53 min
[32m[20230207 15:11:13 @agent_ppo2.py:152][0m 542720 total steps have happened
[32m[20230207 15:11:13 @agent_ppo2.py:128][0m #------------------------ Iteration 265 --------------------------#
[32m[20230207 15:11:14 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:11:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:14 @agent_ppo2.py:192][0m |          -0.0091 |           4.6718 |           0.1022 |
[32m[20230207 15:11:14 @agent_ppo2.py:192][0m |           0.0044 |           3.7495 |           0.1022 |
[32m[20230207 15:11:14 @agent_ppo2.py:192][0m |          -0.0005 |           3.6069 |           0.1021 |
[32m[20230207 15:11:14 @agent_ppo2.py:192][0m |           0.0068 |           3.6685 |           0.1021 |
[32m[20230207 15:11:14 @agent_ppo2.py:192][0m |          -0.0084 |           3.6125 |           0.1021 |
[32m[20230207 15:11:14 @agent_ppo2.py:192][0m |          -0.0033 |           3.3993 |           0.1020 |
[32m[20230207 15:11:14 @agent_ppo2.py:192][0m |          -0.0095 |           3.3455 |           0.1021 |
[32m[20230207 15:11:14 @agent_ppo2.py:192][0m |          -0.0743 |           4.1152 |           0.1021 |
[32m[20230207 15:11:14 @agent_ppo2.py:192][0m |          -0.0122 |           3.3832 |           0.1021 |
[32m[20230207 15:11:14 @agent_ppo2.py:192][0m |          -0.0298 |           3.2803 |           0.1021 |
[32m[20230207 15:11:14 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: -43.48
[32m[20230207 15:11:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -40.67
[32m[20230207 15:11:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -73.75
[32m[20230207 15:11:15 @agent_ppo2.py:150][0m Total time:      10.57 min
[32m[20230207 15:11:15 @agent_ppo2.py:152][0m 544768 total steps have happened
[32m[20230207 15:11:15 @agent_ppo2.py:128][0m #------------------------ Iteration 266 --------------------------#
[32m[20230207 15:11:16 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:11:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:16 @agent_ppo2.py:192][0m |          -0.0003 |           5.5771 |           0.0992 |
[32m[20230207 15:11:16 @agent_ppo2.py:192][0m |           0.0043 |           3.3379 |           0.0991 |
[32m[20230207 15:11:16 @agent_ppo2.py:192][0m |          -0.0129 |           2.9914 |           0.0990 |
[32m[20230207 15:11:16 @agent_ppo2.py:192][0m |           0.0023 |           2.8161 |           0.0990 |
[32m[20230207 15:11:16 @agent_ppo2.py:192][0m |           0.0028 |           2.6760 |           0.0990 |
[32m[20230207 15:11:17 @agent_ppo2.py:192][0m |          -0.0074 |           2.5385 |           0.0989 |
[32m[20230207 15:11:17 @agent_ppo2.py:192][0m |          -0.0066 |           2.4527 |           0.0989 |
[32m[20230207 15:11:17 @agent_ppo2.py:192][0m |          -0.0017 |           2.3615 |           0.0989 |
[32m[20230207 15:11:17 @agent_ppo2.py:192][0m |          -0.0111 |           2.3133 |           0.0989 |
[32m[20230207 15:11:17 @agent_ppo2.py:192][0m |          -0.0047 |           2.2137 |           0.0989 |
[32m[20230207 15:11:17 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -6.22
[32m[20230207 15:11:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 28.80
[32m[20230207 15:11:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -24.24
[32m[20230207 15:11:18 @agent_ppo2.py:150][0m Total time:      10.61 min
[32m[20230207 15:11:18 @agent_ppo2.py:152][0m 546816 total steps have happened
[32m[20230207 15:11:18 @agent_ppo2.py:128][0m #------------------------ Iteration 267 --------------------------#
[32m[20230207 15:11:19 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:11:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:19 @agent_ppo2.py:192][0m |           0.0019 |           3.5296 |           0.1002 |
[32m[20230207 15:11:19 @agent_ppo2.py:192][0m |           0.0063 |           2.1150 |           0.1001 |
[32m[20230207 15:11:19 @agent_ppo2.py:192][0m |          -0.0093 |           1.8904 |           0.1002 |
[32m[20230207 15:11:19 @agent_ppo2.py:192][0m |          -0.0130 |           1.7731 |           0.1002 |
[32m[20230207 15:11:19 @agent_ppo2.py:192][0m |           0.0168 |           1.6789 |           0.1003 |
[32m[20230207 15:11:19 @agent_ppo2.py:192][0m |           0.0002 |           1.6074 |           0.1003 |
[32m[20230207 15:11:19 @agent_ppo2.py:192][0m |          -0.0087 |           1.5577 |           0.1003 |
[32m[20230207 15:11:19 @agent_ppo2.py:192][0m |          -0.0051 |           1.5078 |           0.1003 |
[32m[20230207 15:11:19 @agent_ppo2.py:192][0m |          -0.0159 |           1.4517 |           0.1003 |
[32m[20230207 15:11:19 @agent_ppo2.py:192][0m |           0.0127 |           1.4866 |           0.1004 |
[32m[20230207 15:11:19 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -7.58
[32m[20230207 15:11:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 6.27
[32m[20230207 15:11:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -53.89
[32m[20230207 15:11:20 @agent_ppo2.py:150][0m Total time:      10.65 min
[32m[20230207 15:11:20 @agent_ppo2.py:152][0m 548864 total steps have happened
[32m[20230207 15:11:20 @agent_ppo2.py:128][0m #------------------------ Iteration 268 --------------------------#
[32m[20230207 15:11:21 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:11:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:21 @agent_ppo2.py:192][0m |          -0.0028 |           3.1923 |           0.1007 |
[32m[20230207 15:11:21 @agent_ppo2.py:192][0m |           0.0040 |           2.6497 |           0.1008 |
[32m[20230207 15:11:21 @agent_ppo2.py:192][0m |          -0.0020 |           2.5176 |           0.1008 |
[32m[20230207 15:11:21 @agent_ppo2.py:192][0m |          -0.0004 |           2.4217 |           0.1008 |
[32m[20230207 15:11:21 @agent_ppo2.py:192][0m |          -0.0191 |           2.3908 |           0.1007 |
[32m[20230207 15:11:21 @agent_ppo2.py:192][0m |          -0.0087 |           2.3228 |           0.1006 |
[32m[20230207 15:11:22 @agent_ppo2.py:192][0m |          -0.0111 |           2.2633 |           0.1006 |
[32m[20230207 15:11:22 @agent_ppo2.py:192][0m |          -0.0085 |           2.2181 |           0.1006 |
[32m[20230207 15:11:22 @agent_ppo2.py:192][0m |          -0.0069 |           2.1692 |           0.1006 |
[32m[20230207 15:11:22 @agent_ppo2.py:192][0m |          -0.0912 |           3.1908 |           0.1007 |
[32m[20230207 15:11:22 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -53.65
[32m[20230207 15:11:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -44.19
[32m[20230207 15:11:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -20.50
[32m[20230207 15:11:23 @agent_ppo2.py:150][0m Total time:      10.69 min
[32m[20230207 15:11:23 @agent_ppo2.py:152][0m 550912 total steps have happened
[32m[20230207 15:11:23 @agent_ppo2.py:128][0m #------------------------ Iteration 269 --------------------------#
[32m[20230207 15:11:23 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:11:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:24 @agent_ppo2.py:192][0m |           0.0027 |           2.1255 |           0.1019 |
[32m[20230207 15:11:24 @agent_ppo2.py:192][0m |          -0.0157 |           1.6659 |           0.1018 |
[32m[20230207 15:11:24 @agent_ppo2.py:192][0m |          -0.0109 |           1.5877 |           0.1017 |
[32m[20230207 15:11:24 @agent_ppo2.py:192][0m |          -0.0230 |           1.4991 |           0.1018 |
[32m[20230207 15:11:24 @agent_ppo2.py:192][0m |           0.0144 |           1.4989 |           0.1018 |
[32m[20230207 15:11:24 @agent_ppo2.py:192][0m |          -0.0214 |           1.4623 |           0.1018 |
[32m[20230207 15:11:24 @agent_ppo2.py:192][0m |          -0.0236 |           1.4309 |           0.1018 |
[32m[20230207 15:11:24 @agent_ppo2.py:192][0m |          -0.0004 |           1.4232 |           0.1019 |
[32m[20230207 15:11:24 @agent_ppo2.py:192][0m |          -0.0065 |           1.3827 |           0.1018 |
[32m[20230207 15:11:24 @agent_ppo2.py:192][0m |          -0.0070 |           1.3746 |           0.1019 |
[32m[20230207 15:11:24 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -37.38
[32m[20230207 15:11:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -30.84
[32m[20230207 15:11:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -35.79
[32m[20230207 15:11:25 @agent_ppo2.py:150][0m Total time:      10.73 min
[32m[20230207 15:11:25 @agent_ppo2.py:152][0m 552960 total steps have happened
[32m[20230207 15:11:25 @agent_ppo2.py:128][0m #------------------------ Iteration 270 --------------------------#
[32m[20230207 15:11:26 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:11:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:26 @agent_ppo2.py:192][0m |          -0.0009 |          24.6700 |           0.1030 |
[32m[20230207 15:11:26 @agent_ppo2.py:192][0m |          -0.0047 |          13.6440 |           0.1029 |
[32m[20230207 15:11:26 @agent_ppo2.py:192][0m |          -0.0025 |           9.3219 |           0.1029 |
[32m[20230207 15:11:26 @agent_ppo2.py:192][0m |          -0.0076 |           7.5154 |           0.1029 |
[32m[20230207 15:11:26 @agent_ppo2.py:192][0m |          -0.0079 |           6.7494 |           0.1029 |
[32m[20230207 15:11:26 @agent_ppo2.py:192][0m |          -0.0095 |           5.9790 |           0.1028 |
[32m[20230207 15:11:26 @agent_ppo2.py:192][0m |          -0.0090 |           5.0373 |           0.1027 |
[32m[20230207 15:11:26 @agent_ppo2.py:192][0m |          -0.0101 |           4.4305 |           0.1028 |
[32m[20230207 15:11:26 @agent_ppo2.py:192][0m |          -0.0105 |           3.9230 |           0.1027 |
[32m[20230207 15:11:26 @agent_ppo2.py:192][0m |          -0.0108 |           3.5194 |           0.1028 |
[32m[20230207 15:11:26 @agent_ppo2.py:137][0m Policy update time: 0.64 s
[32m[20230207 15:11:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -83.14
[32m[20230207 15:11:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -13.58
[32m[20230207 15:11:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -43.49
[32m[20230207 15:11:27 @agent_ppo2.py:150][0m Total time:      10.76 min
[32m[20230207 15:11:27 @agent_ppo2.py:152][0m 555008 total steps have happened
[32m[20230207 15:11:27 @agent_ppo2.py:128][0m #------------------------ Iteration 271 --------------------------#
[32m[20230207 15:11:28 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:11:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:28 @agent_ppo2.py:192][0m |           0.0080 |           4.0673 |           0.1019 |
[32m[20230207 15:11:28 @agent_ppo2.py:192][0m |          -0.0038 |           2.4766 |           0.1019 |
[32m[20230207 15:11:28 @agent_ppo2.py:192][0m |          -0.0267 |           2.2653 |           0.1019 |
[32m[20230207 15:11:28 @agent_ppo2.py:192][0m |          -0.0010 |           2.1278 |           0.1018 |
[32m[20230207 15:11:28 @agent_ppo2.py:192][0m |          -0.0165 |           2.0445 |           0.1018 |
[32m[20230207 15:11:28 @agent_ppo2.py:192][0m |          -0.0156 |           1.9768 |           0.1018 |
[32m[20230207 15:11:28 @agent_ppo2.py:192][0m |          -0.0105 |           1.9422 |           0.1018 |
[32m[20230207 15:11:29 @agent_ppo2.py:192][0m |          -0.0458 |           1.8645 |           0.1018 |
[32m[20230207 15:11:29 @agent_ppo2.py:192][0m |          -0.0043 |           1.8372 |           0.1019 |
[32m[20230207 15:11:29 @agent_ppo2.py:192][0m |          -0.0034 |           1.7952 |           0.1019 |
[32m[20230207 15:11:29 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -14.00
[32m[20230207 15:11:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 35.88
[32m[20230207 15:11:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -87.66
[32m[20230207 15:11:29 @agent_ppo2.py:150][0m Total time:      10.80 min
[32m[20230207 15:11:29 @agent_ppo2.py:152][0m 557056 total steps have happened
[32m[20230207 15:11:29 @agent_ppo2.py:128][0m #------------------------ Iteration 272 --------------------------#
[32m[20230207 15:11:30 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:11:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:30 @agent_ppo2.py:192][0m |           0.0057 |           2.3250 |           0.1027 |
[32m[20230207 15:11:30 @agent_ppo2.py:192][0m |          -0.0091 |           1.8490 |           0.1025 |
[32m[20230207 15:11:30 @agent_ppo2.py:192][0m |          -0.0059 |           1.7268 |           0.1025 |
[32m[20230207 15:11:30 @agent_ppo2.py:192][0m |          -0.0111 |           1.6641 |           0.1025 |
[32m[20230207 15:11:31 @agent_ppo2.py:192][0m |          -0.0203 |           1.6122 |           0.1024 |
[32m[20230207 15:11:31 @agent_ppo2.py:192][0m |          -0.0171 |           1.5651 |           0.1025 |
[32m[20230207 15:11:31 @agent_ppo2.py:192][0m |          -0.0047 |           1.5352 |           0.1025 |
[32m[20230207 15:11:31 @agent_ppo2.py:192][0m |          -0.0082 |           1.5232 |           0.1026 |
[32m[20230207 15:11:31 @agent_ppo2.py:192][0m |          -0.0146 |           1.4973 |           0.1024 |
[32m[20230207 15:11:31 @agent_ppo2.py:192][0m |          -0.0071 |           1.4891 |           0.1025 |
[32m[20230207 15:11:31 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -25.96
[32m[20230207 15:11:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 33.66
[32m[20230207 15:11:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 12.63
[32m[20230207 15:11:32 @agent_ppo2.py:150][0m Total time:      10.84 min
[32m[20230207 15:11:32 @agent_ppo2.py:152][0m 559104 total steps have happened
[32m[20230207 15:11:32 @agent_ppo2.py:128][0m #------------------------ Iteration 273 --------------------------#
[32m[20230207 15:11:33 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:11:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:33 @agent_ppo2.py:192][0m |           0.0017 |           5.1592 |           0.1059 |
[32m[20230207 15:11:33 @agent_ppo2.py:192][0m |          -0.0002 |           2.0370 |           0.1059 |
[32m[20230207 15:11:33 @agent_ppo2.py:192][0m |          -0.0012 |           1.7795 |           0.1058 |
[32m[20230207 15:11:33 @agent_ppo2.py:192][0m |          -0.0022 |           1.6051 |           0.1057 |
[32m[20230207 15:11:33 @agent_ppo2.py:192][0m |          -0.0040 |           1.5306 |           0.1057 |
[32m[20230207 15:11:33 @agent_ppo2.py:192][0m |          -0.0040 |           1.4612 |           0.1056 |
[32m[20230207 15:11:33 @agent_ppo2.py:192][0m |          -0.0047 |           1.4148 |           0.1056 |
[32m[20230207 15:11:33 @agent_ppo2.py:192][0m |          -0.0052 |           1.3658 |           0.1055 |
[32m[20230207 15:11:33 @agent_ppo2.py:192][0m |          -0.0042 |           1.3275 |           0.1055 |
[32m[20230207 15:11:33 @agent_ppo2.py:192][0m |          -0.0064 |           1.3105 |           0.1054 |
[32m[20230207 15:11:33 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:11:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: -32.15
[32m[20230207 15:11:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 24.22
[32m[20230207 15:11:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -6.62
[32m[20230207 15:11:34 @agent_ppo2.py:150][0m Total time:      10.88 min
[32m[20230207 15:11:34 @agent_ppo2.py:152][0m 561152 total steps have happened
[32m[20230207 15:11:34 @agent_ppo2.py:128][0m #------------------------ Iteration 274 --------------------------#
[32m[20230207 15:11:35 @agent_ppo2.py:134][0m Sampling time: 0.61 s by 1 slaves
[32m[20230207 15:11:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:35 @agent_ppo2.py:192][0m |           0.0008 |          18.7637 |           0.1023 |
[32m[20230207 15:11:35 @agent_ppo2.py:192][0m |          -0.0011 |           6.5468 |           0.1021 |
[32m[20230207 15:11:35 @agent_ppo2.py:192][0m |          -0.0061 |           3.3568 |           0.1020 |
[32m[20230207 15:11:35 @agent_ppo2.py:192][0m |          -0.0046 |           2.7161 |           0.1020 |
[32m[20230207 15:11:35 @agent_ppo2.py:192][0m |          -0.0072 |           2.4067 |           0.1019 |
[32m[20230207 15:11:35 @agent_ppo2.py:192][0m |          -0.0064 |           2.1757 |           0.1020 |
[32m[20230207 15:11:35 @agent_ppo2.py:192][0m |          -0.0078 |           1.9999 |           0.1019 |
[32m[20230207 15:11:35 @agent_ppo2.py:192][0m |          -0.0056 |           1.9284 |           0.1019 |
[32m[20230207 15:11:35 @agent_ppo2.py:192][0m |          -0.0098 |           1.7783 |           0.1020 |
[32m[20230207 15:11:36 @agent_ppo2.py:192][0m |          -0.0110 |           1.6857 |           0.1020 |
[32m[20230207 15:11:36 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230207 15:11:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -72.45
[32m[20230207 15:11:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 22.37
[32m[20230207 15:11:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -45.41
[32m[20230207 15:11:36 @agent_ppo2.py:150][0m Total time:      10.92 min
[32m[20230207 15:11:36 @agent_ppo2.py:152][0m 563200 total steps have happened
[32m[20230207 15:11:36 @agent_ppo2.py:128][0m #------------------------ Iteration 275 --------------------------#
[32m[20230207 15:11:37 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:11:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:37 @agent_ppo2.py:192][0m |           0.0071 |           3.4571 |           0.1033 |
[32m[20230207 15:11:37 @agent_ppo2.py:192][0m |          -0.0295 |           2.3378 |           0.1031 |
[32m[20230207 15:11:37 @agent_ppo2.py:192][0m |           0.0076 |           2.0667 |           0.1032 |
[32m[20230207 15:11:37 @agent_ppo2.py:192][0m |          -0.0486 |           2.8761 |           0.1032 |
[32m[20230207 15:11:38 @agent_ppo2.py:192][0m |           0.0024 |           1.9930 |           0.1031 |
[32m[20230207 15:11:38 @agent_ppo2.py:192][0m |          -0.0120 |           1.7291 |           0.1032 |
[32m[20230207 15:11:38 @agent_ppo2.py:192][0m |          -0.0347 |           1.6619 |           0.1034 |
[32m[20230207 15:11:38 @agent_ppo2.py:192][0m |          -0.0092 |           1.6027 |           0.1033 |
[32m[20230207 15:11:38 @agent_ppo2.py:192][0m |           0.0019 |           1.5577 |           0.1033 |
[32m[20230207 15:11:38 @agent_ppo2.py:192][0m |          -0.0259 |           1.5155 |           0.1032 |
[32m[20230207 15:11:38 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:11:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -29.74
[32m[20230207 15:11:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -21.76
[32m[20230207 15:11:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -51.37
[32m[20230207 15:11:39 @agent_ppo2.py:150][0m Total time:      10.96 min
[32m[20230207 15:11:39 @agent_ppo2.py:152][0m 565248 total steps have happened
[32m[20230207 15:11:39 @agent_ppo2.py:128][0m #------------------------ Iteration 276 --------------------------#
[32m[20230207 15:11:40 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:11:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:40 @agent_ppo2.py:192][0m |           0.0057 |           1.3117 |           0.1038 |
[32m[20230207 15:11:40 @agent_ppo2.py:192][0m |          -0.0007 |           1.1553 |           0.1037 |
[32m[20230207 15:11:40 @agent_ppo2.py:192][0m |           0.0100 |           1.1388 |           0.1036 |
[32m[20230207 15:11:40 @agent_ppo2.py:192][0m |           0.0001 |           1.1175 |           0.1036 |
[32m[20230207 15:11:40 @agent_ppo2.py:192][0m |          -0.0155 |           1.1119 |           0.1035 |
[32m[20230207 15:11:40 @agent_ppo2.py:192][0m |           0.0011 |           1.1076 |           0.1035 |
[32m[20230207 15:11:40 @agent_ppo2.py:192][0m |          -0.0082 |           1.0929 |           0.1034 |
[32m[20230207 15:11:40 @agent_ppo2.py:192][0m |          -0.0203 |           1.0923 |           0.1035 |
[32m[20230207 15:11:40 @agent_ppo2.py:192][0m |          -0.0130 |           1.0767 |           0.1035 |
[32m[20230207 15:11:40 @agent_ppo2.py:192][0m |          -0.0118 |           1.0676 |           0.1034 |
[32m[20230207 15:11:40 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -21.34
[32m[20230207 15:11:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 36.48
[32m[20230207 15:11:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -84.86
[32m[20230207 15:11:41 @agent_ppo2.py:150][0m Total time:      11.00 min
[32m[20230207 15:11:41 @agent_ppo2.py:152][0m 567296 total steps have happened
[32m[20230207 15:11:41 @agent_ppo2.py:128][0m #------------------------ Iteration 277 --------------------------#
[32m[20230207 15:11:42 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:11:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:42 @agent_ppo2.py:192][0m |           0.0000 |          25.9544 |           0.1078 |
[32m[20230207 15:11:42 @agent_ppo2.py:192][0m |          -0.0032 |          15.5603 |           0.1078 |
[32m[20230207 15:11:42 @agent_ppo2.py:192][0m |          -0.0042 |          11.7413 |           0.1077 |
[32m[20230207 15:11:42 @agent_ppo2.py:192][0m |          -0.0061 |           8.6970 |           0.1076 |
[32m[20230207 15:11:42 @agent_ppo2.py:192][0m |          -0.0075 |           7.0863 |           0.1076 |
[32m[20230207 15:11:42 @agent_ppo2.py:192][0m |          -0.0077 |           6.2160 |           0.1075 |
[32m[20230207 15:11:42 @agent_ppo2.py:192][0m |          -0.0085 |           5.8186 |           0.1076 |
[32m[20230207 15:11:42 @agent_ppo2.py:192][0m |          -0.0089 |           6.0080 |           0.1076 |
[32m[20230207 15:11:43 @agent_ppo2.py:192][0m |          -0.0098 |           5.0895 |           0.1075 |
[32m[20230207 15:11:43 @agent_ppo2.py:192][0m |          -0.0105 |           4.8915 |           0.1076 |
[32m[20230207 15:11:43 @agent_ppo2.py:137][0m Policy update time: 0.77 s
[32m[20230207 15:11:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -85.03
[32m[20230207 15:11:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -25.27
[32m[20230207 15:11:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -82.42
[32m[20230207 15:11:43 @agent_ppo2.py:150][0m Total time:      11.03 min
[32m[20230207 15:11:43 @agent_ppo2.py:152][0m 569344 total steps have happened
[32m[20230207 15:11:43 @agent_ppo2.py:128][0m #------------------------ Iteration 278 --------------------------#
[32m[20230207 15:11:44 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:11:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:44 @agent_ppo2.py:192][0m |           0.0096 |           7.2892 |           0.1037 |
[32m[20230207 15:11:44 @agent_ppo2.py:192][0m |          -0.0020 |           5.7554 |           0.1034 |
[32m[20230207 15:11:44 @agent_ppo2.py:192][0m |          -0.0065 |           5.4890 |           0.1034 |
[32m[20230207 15:11:44 @agent_ppo2.py:192][0m |          -0.0057 |           5.8004 |           0.1034 |
[32m[20230207 15:11:44 @agent_ppo2.py:192][0m |           0.0041 |           5.4406 |           0.1034 |
[32m[20230207 15:11:45 @agent_ppo2.py:192][0m |          -0.0102 |           5.2966 |           0.1034 |
[32m[20230207 15:11:45 @agent_ppo2.py:192][0m |           0.0063 |           5.1641 |           0.1034 |
[32m[20230207 15:11:45 @agent_ppo2.py:192][0m |           0.0015 |           5.1246 |           0.1033 |
[32m[20230207 15:11:45 @agent_ppo2.py:192][0m |          -0.0019 |           5.0633 |           0.1032 |
[32m[20230207 15:11:45 @agent_ppo2.py:192][0m |          -0.0052 |           5.0938 |           0.1034 |
[32m[20230207 15:11:45 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -124.97
[32m[20230207 15:11:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.60
[32m[20230207 15:11:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -90.47
[32m[20230207 15:11:46 @agent_ppo2.py:150][0m Total time:      11.07 min
[32m[20230207 15:11:46 @agent_ppo2.py:152][0m 571392 total steps have happened
[32m[20230207 15:11:46 @agent_ppo2.py:128][0m #------------------------ Iteration 279 --------------------------#
[32m[20230207 15:11:46 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:11:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:47 @agent_ppo2.py:192][0m |          -0.0023 |           7.8219 |           0.1042 |
[32m[20230207 15:11:47 @agent_ppo2.py:192][0m |          -0.0065 |           3.0893 |           0.1041 |
[32m[20230207 15:11:47 @agent_ppo2.py:192][0m |          -0.0085 |           1.8821 |           0.1040 |
[32m[20230207 15:11:47 @agent_ppo2.py:192][0m |          -0.0094 |           1.4984 |           0.1039 |
[32m[20230207 15:11:47 @agent_ppo2.py:192][0m |          -0.0113 |           1.3364 |           0.1039 |
[32m[20230207 15:11:47 @agent_ppo2.py:192][0m |          -0.0128 |           1.2741 |           0.1039 |
[32m[20230207 15:11:47 @agent_ppo2.py:192][0m |          -0.0131 |           1.2221 |           0.1038 |
[32m[20230207 15:11:47 @agent_ppo2.py:192][0m |          -0.0140 |           1.1884 |           0.1038 |
[32m[20230207 15:11:47 @agent_ppo2.py:192][0m |          -0.0135 |           1.1601 |           0.1038 |
[32m[20230207 15:11:47 @agent_ppo2.py:192][0m |          -0.0142 |           1.1425 |           0.1037 |
[32m[20230207 15:11:47 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:11:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -38.72
[32m[20230207 15:11:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -0.00
[32m[20230207 15:11:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 18.07
[32m[20230207 15:11:48 @agent_ppo2.py:150][0m Total time:      11.12 min
[32m[20230207 15:11:48 @agent_ppo2.py:152][0m 573440 total steps have happened
[32m[20230207 15:11:48 @agent_ppo2.py:128][0m #------------------------ Iteration 280 --------------------------#
[32m[20230207 15:11:49 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:11:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:49 @agent_ppo2.py:192][0m |          -0.0625 |           1.7888 |           0.1035 |
[32m[20230207 15:11:49 @agent_ppo2.py:192][0m |          -0.0234 |           1.2455 |           0.1031 |
[32m[20230207 15:11:49 @agent_ppo2.py:192][0m |          -0.0030 |           1.1139 |           0.1032 |
[32m[20230207 15:11:49 @agent_ppo2.py:192][0m |           0.0001 |           1.0760 |           0.1034 |
[32m[20230207 15:11:49 @agent_ppo2.py:192][0m |          -0.0057 |           1.0606 |           0.1034 |
[32m[20230207 15:11:49 @agent_ppo2.py:192][0m |          -0.0105 |           1.0439 |           0.1035 |
[32m[20230207 15:11:50 @agent_ppo2.py:192][0m |          -0.0082 |           1.0237 |           0.1035 |
[32m[20230207 15:11:50 @agent_ppo2.py:192][0m |          -0.0134 |           1.0301 |           0.1036 |
[32m[20230207 15:11:50 @agent_ppo2.py:192][0m |          -0.0174 |           1.0042 |           0.1036 |
[32m[20230207 15:11:50 @agent_ppo2.py:192][0m |          -0.0034 |           0.9945 |           0.1036 |
[32m[20230207 15:11:50 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: -21.90
[32m[20230207 15:11:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -20.00
[32m[20230207 15:11:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -21.75
[32m[20230207 15:11:51 @agent_ppo2.py:150][0m Total time:      11.16 min
[32m[20230207 15:11:51 @agent_ppo2.py:152][0m 575488 total steps have happened
[32m[20230207 15:11:51 @agent_ppo2.py:128][0m #------------------------ Iteration 281 --------------------------#
[32m[20230207 15:11:51 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:11:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:52 @agent_ppo2.py:192][0m |           0.0004 |           7.2092 |           0.1064 |
[32m[20230207 15:11:52 @agent_ppo2.py:192][0m |          -0.0035 |           2.9087 |           0.1063 |
[32m[20230207 15:11:52 @agent_ppo2.py:192][0m |          -0.0047 |           2.5414 |           0.1063 |
[32m[20230207 15:11:52 @agent_ppo2.py:192][0m |          -0.0050 |           2.3940 |           0.1063 |
[32m[20230207 15:11:52 @agent_ppo2.py:192][0m |          -0.0052 |           2.2726 |           0.1063 |
[32m[20230207 15:11:52 @agent_ppo2.py:192][0m |          -0.0069 |           2.2133 |           0.1063 |
[32m[20230207 15:11:52 @agent_ppo2.py:192][0m |          -0.0068 |           2.2105 |           0.1063 |
[32m[20230207 15:11:52 @agent_ppo2.py:192][0m |          -0.0076 |           2.0889 |           0.1064 |
[32m[20230207 15:11:52 @agent_ppo2.py:192][0m |          -0.0075 |           2.0771 |           0.1064 |
[32m[20230207 15:11:52 @agent_ppo2.py:192][0m |          -0.0086 |           2.0558 |           0.1064 |
[32m[20230207 15:11:52 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:11:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -58.25
[32m[20230207 15:11:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -24.04
[32m[20230207 15:11:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -26.34
[32m[20230207 15:11:53 @agent_ppo2.py:150][0m Total time:      11.20 min
[32m[20230207 15:11:53 @agent_ppo2.py:152][0m 577536 total steps have happened
[32m[20230207 15:11:53 @agent_ppo2.py:128][0m #------------------------ Iteration 282 --------------------------#
[32m[20230207 15:11:54 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 15:11:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:54 @agent_ppo2.py:192][0m |           0.0004 |          19.1664 |           0.1047 |
[32m[20230207 15:11:54 @agent_ppo2.py:192][0m |          -0.0027 |           4.6762 |           0.1045 |
[32m[20230207 15:11:54 @agent_ppo2.py:192][0m |          -0.0057 |           3.1681 |           0.1044 |
[32m[20230207 15:11:54 @agent_ppo2.py:192][0m |          -0.0058 |           2.5552 |           0.1043 |
[32m[20230207 15:11:54 @agent_ppo2.py:192][0m |          -0.0072 |           2.2019 |           0.1043 |
[32m[20230207 15:11:54 @agent_ppo2.py:192][0m |          -0.0078 |           1.9989 |           0.1042 |
[32m[20230207 15:11:54 @agent_ppo2.py:192][0m |          -0.0096 |           1.8742 |           0.1042 |
[32m[20230207 15:11:54 @agent_ppo2.py:192][0m |          -0.0106 |           1.7454 |           0.1042 |
[32m[20230207 15:11:54 @agent_ppo2.py:192][0m |          -0.0111 |           1.6601 |           0.1042 |
[32m[20230207 15:11:54 @agent_ppo2.py:192][0m |          -0.0106 |           1.5918 |           0.1041 |
[32m[20230207 15:11:54 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:11:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: -52.15
[32m[20230207 15:11:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 6.06
[32m[20230207 15:11:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -89.30
[32m[20230207 15:11:55 @agent_ppo2.py:150][0m Total time:      11.22 min
[32m[20230207 15:11:55 @agent_ppo2.py:152][0m 579584 total steps have happened
[32m[20230207 15:11:55 @agent_ppo2.py:128][0m #------------------------ Iteration 283 --------------------------#
[32m[20230207 15:11:55 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:11:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:56 @agent_ppo2.py:192][0m |          -0.0048 |           3.2357 |           0.1032 |
[32m[20230207 15:11:56 @agent_ppo2.py:192][0m |          -0.0095 |           1.9732 |           0.1033 |
[32m[20230207 15:11:56 @agent_ppo2.py:192][0m |          -0.0153 |           1.8052 |           0.1034 |
[32m[20230207 15:11:56 @agent_ppo2.py:192][0m |           0.0007 |           1.7097 |           0.1032 |
[32m[20230207 15:11:56 @agent_ppo2.py:192][0m |          -0.0230 |           1.6722 |           0.1034 |
[32m[20230207 15:11:56 @agent_ppo2.py:192][0m |          -0.0083 |           1.6075 |           0.1035 |
[32m[20230207 15:11:56 @agent_ppo2.py:192][0m |          -0.0008 |           1.5411 |           0.1035 |
[32m[20230207 15:11:56 @agent_ppo2.py:192][0m |          -0.0076 |           1.5336 |           0.1035 |
[32m[20230207 15:11:56 @agent_ppo2.py:192][0m |          -0.0115 |           1.4913 |           0.1036 |
[32m[20230207 15:11:56 @agent_ppo2.py:192][0m |           0.0176 |           1.5318 |           0.1036 |
[32m[20230207 15:11:56 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:11:57 @agent_ppo2.py:145][0m Average TRAINING episode reward: 5.60
[32m[20230207 15:11:57 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 25.33
[32m[20230207 15:11:57 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -0.06
[32m[20230207 15:11:57 @agent_ppo2.py:150][0m Total time:      11.26 min
[32m[20230207 15:11:57 @agent_ppo2.py:152][0m 581632 total steps have happened
[32m[20230207 15:11:57 @agent_ppo2.py:128][0m #------------------------ Iteration 284 --------------------------#
[32m[20230207 15:11:58 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:11:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:11:58 @agent_ppo2.py:192][0m |           0.0080 |           1.5791 |           0.1045 |
[32m[20230207 15:11:58 @agent_ppo2.py:192][0m |          -0.0044 |           1.4058 |           0.1045 |
[32m[20230207 15:11:58 @agent_ppo2.py:192][0m |          -0.0038 |           1.3666 |           0.1044 |
[32m[20230207 15:11:58 @agent_ppo2.py:192][0m |          -0.0020 |           1.3331 |           0.1044 |
[32m[20230207 15:11:58 @agent_ppo2.py:192][0m |          -0.0290 |           1.3344 |           0.1044 |
[32m[20230207 15:11:58 @agent_ppo2.py:192][0m |          -0.0400 |           1.3187 |           0.1044 |
[32m[20230207 15:11:58 @agent_ppo2.py:192][0m |           0.0036 |           1.3059 |           0.1044 |
[32m[20230207 15:11:59 @agent_ppo2.py:192][0m |          -0.0082 |           1.2726 |           0.1044 |
[32m[20230207 15:11:59 @agent_ppo2.py:192][0m |           0.0056 |           1.2659 |           0.1044 |
[32m[20230207 15:11:59 @agent_ppo2.py:192][0m |           0.0049 |           1.2510 |           0.1044 |
[32m[20230207 15:11:59 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: -31.82
[32m[20230207 15:12:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 6.01
[32m[20230207 15:12:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -42.95
[32m[20230207 15:12:00 @agent_ppo2.py:150][0m Total time:      11.30 min
[32m[20230207 15:12:00 @agent_ppo2.py:152][0m 583680 total steps have happened
[32m[20230207 15:12:00 @agent_ppo2.py:128][0m #------------------------ Iteration 285 --------------------------#
[32m[20230207 15:12:00 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:12:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:00 @agent_ppo2.py:192][0m |           0.0308 |           1.7683 |           0.1093 |
[32m[20230207 15:12:01 @agent_ppo2.py:192][0m |           0.0072 |           0.9898 |           0.1090 |
[32m[20230207 15:12:01 @agent_ppo2.py:192][0m |          -0.0107 |           0.9012 |           0.1090 |
[32m[20230207 15:12:01 @agent_ppo2.py:192][0m |          -0.0114 |           0.8777 |           0.1090 |
[32m[20230207 15:12:01 @agent_ppo2.py:192][0m |          -0.0144 |           0.8630 |           0.1090 |
[32m[20230207 15:12:01 @agent_ppo2.py:192][0m |           0.0072 |           0.8421 |           0.1090 |
[32m[20230207 15:12:01 @agent_ppo2.py:192][0m |          -0.0311 |           0.8203 |           0.1090 |
[32m[20230207 15:12:01 @agent_ppo2.py:192][0m |           0.0003 |           0.8042 |           0.1088 |
[32m[20230207 15:12:01 @agent_ppo2.py:192][0m |          -0.0398 |           0.8033 |           0.1089 |
[32m[20230207 15:12:01 @agent_ppo2.py:192][0m |           0.0024 |           0.7876 |           0.1089 |
[32m[20230207 15:12:01 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:02 @agent_ppo2.py:145][0m Average TRAINING episode reward: 57.44
[32m[20230207 15:12:02 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 59.03
[32m[20230207 15:12:02 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 16.29
[32m[20230207 15:12:02 @agent_ppo2.py:150][0m Total time:      11.35 min
[32m[20230207 15:12:02 @agent_ppo2.py:152][0m 585728 total steps have happened
[32m[20230207 15:12:02 @agent_ppo2.py:128][0m #------------------------ Iteration 286 --------------------------#
[32m[20230207 15:12:03 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:12:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:03 @agent_ppo2.py:192][0m |          -0.0091 |           1.5772 |           0.1060 |
[32m[20230207 15:12:03 @agent_ppo2.py:192][0m |           0.0062 |           1.2577 |           0.1057 |
[32m[20230207 15:12:03 @agent_ppo2.py:192][0m |          -0.0079 |           1.2134 |           0.1056 |
[32m[20230207 15:12:03 @agent_ppo2.py:192][0m |          -0.0091 |           1.1838 |           0.1056 |
[32m[20230207 15:12:03 @agent_ppo2.py:192][0m |          -0.0167 |           1.1617 |           0.1056 |
[32m[20230207 15:12:03 @agent_ppo2.py:192][0m |          -0.0081 |           1.1919 |           0.1055 |
[32m[20230207 15:12:03 @agent_ppo2.py:192][0m |          -0.0102 |           1.1511 |           0.1055 |
[32m[20230207 15:12:03 @agent_ppo2.py:192][0m |          -0.0175 |           1.1007 |           0.1055 |
[32m[20230207 15:12:03 @agent_ppo2.py:192][0m |          -0.0098 |           1.0832 |           0.1054 |
[32m[20230207 15:12:04 @agent_ppo2.py:192][0m |          -0.0158 |           1.0776 |           0.1054 |
[32m[20230207 15:12:04 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: -25.71
[32m[20230207 15:12:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 39.23
[32m[20230207 15:12:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -21.49
[32m[20230207 15:12:04 @agent_ppo2.py:150][0m Total time:      11.39 min
[32m[20230207 15:12:04 @agent_ppo2.py:152][0m 587776 total steps have happened
[32m[20230207 15:12:04 @agent_ppo2.py:128][0m #------------------------ Iteration 287 --------------------------#
[32m[20230207 15:12:05 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:12:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:05 @agent_ppo2.py:192][0m |          -0.0016 |           8.6660 |           0.1072 |
[32m[20230207 15:12:05 @agent_ppo2.py:192][0m |          -0.0039 |           3.8346 |           0.1071 |
[32m[20230207 15:12:05 @agent_ppo2.py:192][0m |          -0.0053 |           3.1848 |           0.1070 |
[32m[20230207 15:12:06 @agent_ppo2.py:192][0m |          -0.0034 |           2.8767 |           0.1070 |
[32m[20230207 15:12:06 @agent_ppo2.py:192][0m |          -0.0045 |           2.7575 |           0.1069 |
[32m[20230207 15:12:06 @agent_ppo2.py:192][0m |          -0.0053 |           2.6227 |           0.1070 |
[32m[20230207 15:12:06 @agent_ppo2.py:192][0m |          -0.0059 |           2.4327 |           0.1069 |
[32m[20230207 15:12:06 @agent_ppo2.py:192][0m |          -0.0071 |           2.3333 |           0.1069 |
[32m[20230207 15:12:06 @agent_ppo2.py:192][0m |          -0.0066 |           2.2779 |           0.1069 |
[32m[20230207 15:12:06 @agent_ppo2.py:192][0m |          -0.0070 |           2.1888 |           0.1068 |
[32m[20230207 15:12:06 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:12:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -37.54
[32m[20230207 15:12:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 33.88
[32m[20230207 15:12:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -8.08
[32m[20230207 15:12:07 @agent_ppo2.py:150][0m Total time:      11.43 min
[32m[20230207 15:12:07 @agent_ppo2.py:152][0m 589824 total steps have happened
[32m[20230207 15:12:07 @agent_ppo2.py:128][0m #------------------------ Iteration 288 --------------------------#
[32m[20230207 15:12:08 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:12:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:08 @agent_ppo2.py:192][0m |          -0.0004 |          16.5224 |           0.1047 |
[32m[20230207 15:12:08 @agent_ppo2.py:192][0m |          -0.0052 |           5.0976 |           0.1045 |
[32m[20230207 15:12:08 @agent_ppo2.py:192][0m |          -0.0093 |           4.0666 |           0.1046 |
[32m[20230207 15:12:08 @agent_ppo2.py:192][0m |          -0.0137 |           3.6290 |           0.1046 |
[32m[20230207 15:12:08 @agent_ppo2.py:192][0m |          -0.0142 |           3.2224 |           0.1047 |
[32m[20230207 15:12:08 @agent_ppo2.py:192][0m |          -0.0092 |           3.2885 |           0.1047 |
[32m[20230207 15:12:08 @agent_ppo2.py:192][0m |          -0.0151 |           3.0213 |           0.1047 |
[32m[20230207 15:12:08 @agent_ppo2.py:192][0m |          -0.0061 |           2.6669 |           0.1047 |
[32m[20230207 15:12:08 @agent_ppo2.py:192][0m |          -0.0170 |           2.5280 |           0.1048 |
[32m[20230207 15:12:08 @agent_ppo2.py:192][0m |          -0.0158 |           2.4099 |           0.1048 |
[32m[20230207 15:12:08 @agent_ppo2.py:137][0m Policy update time: 0.78 s
[32m[20230207 15:12:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: -72.10
[32m[20230207 15:12:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -5.25
[32m[20230207 15:12:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -33.47
[32m[20230207 15:12:09 @agent_ppo2.py:150][0m Total time:      11.47 min
[32m[20230207 15:12:09 @agent_ppo2.py:152][0m 591872 total steps have happened
[32m[20230207 15:12:09 @agent_ppo2.py:128][0m #------------------------ Iteration 289 --------------------------#
[32m[20230207 15:12:10 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:12:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:10 @agent_ppo2.py:192][0m |           0.0036 |           1.2141 |           0.1047 |
[32m[20230207 15:12:10 @agent_ppo2.py:192][0m |          -0.0181 |           1.0587 |           0.1047 |
[32m[20230207 15:12:10 @agent_ppo2.py:192][0m |          -0.0148 |           1.0150 |           0.1047 |
[32m[20230207 15:12:10 @agent_ppo2.py:192][0m |          -0.0013 |           0.9572 |           0.1046 |
[32m[20230207 15:12:10 @agent_ppo2.py:192][0m |          -0.0098 |           0.9373 |           0.1047 |
[32m[20230207 15:12:10 @agent_ppo2.py:192][0m |          -0.0052 |           0.9270 |           0.1046 |
[32m[20230207 15:12:11 @agent_ppo2.py:192][0m |          -0.0057 |           0.9154 |           0.1046 |
[32m[20230207 15:12:11 @agent_ppo2.py:192][0m |          -0.0147 |           0.9077 |           0.1047 |
[32m[20230207 15:12:11 @agent_ppo2.py:192][0m |          -0.0012 |           0.8989 |           0.1047 |
[32m[20230207 15:12:11 @agent_ppo2.py:192][0m |          -0.0028 |           0.8860 |           0.1047 |
[32m[20230207 15:12:11 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: -59.20
[32m[20230207 15:12:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -28.88
[32m[20230207 15:12:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -53.54
[32m[20230207 15:12:12 @agent_ppo2.py:150][0m Total time:      11.51 min
[32m[20230207 15:12:12 @agent_ppo2.py:152][0m 593920 total steps have happened
[32m[20230207 15:12:12 @agent_ppo2.py:128][0m #------------------------ Iteration 290 --------------------------#
[32m[20230207 15:12:12 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:12:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:12 @agent_ppo2.py:192][0m |           0.0080 |           1.1862 |           0.1087 |
[32m[20230207 15:12:13 @agent_ppo2.py:192][0m |          -0.0029 |           0.8888 |           0.1086 |
[32m[20230207 15:12:13 @agent_ppo2.py:192][0m |          -0.0039 |           0.8552 |           0.1086 |
[32m[20230207 15:12:13 @agent_ppo2.py:192][0m |          -0.0184 |           0.8452 |           0.1087 |
[32m[20230207 15:12:13 @agent_ppo2.py:192][0m |           0.0027 |           0.8292 |           0.1087 |
[32m[20230207 15:12:13 @agent_ppo2.py:192][0m |          -0.0110 |           0.8155 |           0.1087 |
[32m[20230207 15:12:13 @agent_ppo2.py:192][0m |          -0.0305 |           0.8101 |           0.1088 |
[32m[20230207 15:12:13 @agent_ppo2.py:192][0m |          -0.0118 |           0.8017 |           0.1088 |
[32m[20230207 15:12:13 @agent_ppo2.py:192][0m |          -0.0287 |           0.7950 |           0.1088 |
[32m[20230207 15:12:13 @agent_ppo2.py:192][0m |          -0.0259 |           0.7845 |           0.1089 |
[32m[20230207 15:12:13 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:12:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: 11.48
[32m[20230207 15:12:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 70.15
[32m[20230207 15:12:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -88.10
[32m[20230207 15:12:14 @agent_ppo2.py:150][0m Total time:      11.54 min
[32m[20230207 15:12:14 @agent_ppo2.py:152][0m 595968 total steps have happened
[32m[20230207 15:12:14 @agent_ppo2.py:128][0m #------------------------ Iteration 291 --------------------------#
[32m[20230207 15:12:15 @agent_ppo2.py:134][0m Sampling time: 0.71 s by 1 slaves
[32m[20230207 15:12:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:15 @agent_ppo2.py:192][0m |           0.0030 |          13.6236 |           0.1103 |
[32m[20230207 15:12:15 @agent_ppo2.py:192][0m |          -0.0033 |           5.7699 |           0.1102 |
[32m[20230207 15:12:15 @agent_ppo2.py:192][0m |          -0.0045 |           5.1429 |           0.1102 |
[32m[20230207 15:12:15 @agent_ppo2.py:192][0m |          -0.0071 |           4.5209 |           0.1102 |
[32m[20230207 15:12:15 @agent_ppo2.py:192][0m |          -0.0080 |           4.1177 |           0.1102 |
[32m[20230207 15:12:15 @agent_ppo2.py:192][0m |          -0.0076 |           3.6811 |           0.1102 |
[32m[20230207 15:12:15 @agent_ppo2.py:192][0m |          -0.0084 |           3.1535 |           0.1102 |
[32m[20230207 15:12:15 @agent_ppo2.py:192][0m |          -0.0102 |           2.6713 |           0.1102 |
[32m[20230207 15:12:15 @agent_ppo2.py:192][0m |          -0.0116 |           2.2401 |           0.1102 |
[32m[20230207 15:12:15 @agent_ppo2.py:192][0m |          -0.0131 |           1.9127 |           0.1102 |
[32m[20230207 15:12:15 @agent_ppo2.py:137][0m Policy update time: 0.72 s
[32m[20230207 15:12:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -70.33
[32m[20230207 15:12:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -24.54
[32m[20230207 15:12:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -100.57
[32m[20230207 15:12:16 @agent_ppo2.py:150][0m Total time:      11.58 min
[32m[20230207 15:12:16 @agent_ppo2.py:152][0m 598016 total steps have happened
[32m[20230207 15:12:16 @agent_ppo2.py:128][0m #------------------------ Iteration 292 --------------------------#
[32m[20230207 15:12:17 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:12:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:17 @agent_ppo2.py:192][0m |          -0.0015 |          29.8053 |           0.1087 |
[32m[20230207 15:12:17 @agent_ppo2.py:192][0m |          -0.0061 |          17.6625 |           0.1088 |
[32m[20230207 15:12:17 @agent_ppo2.py:192][0m |          -0.0073 |           7.8357 |           0.1087 |
[32m[20230207 15:12:17 @agent_ppo2.py:192][0m |          -0.0343 |           6.1917 |           0.1087 |
[32m[20230207 15:12:17 @agent_ppo2.py:192][0m |          -0.0140 |           3.5733 |           0.1086 |
[32m[20230207 15:12:17 @agent_ppo2.py:192][0m |          -0.0094 |           2.9684 |           0.1086 |
[32m[20230207 15:12:17 @agent_ppo2.py:192][0m |          -0.0138 |           2.6218 |           0.1085 |
[32m[20230207 15:12:17 @agent_ppo2.py:192][0m |          -0.0131 |           2.4119 |           0.1085 |
[32m[20230207 15:12:17 @agent_ppo2.py:192][0m |          -0.0151 |           2.2723 |           0.1085 |
[32m[20230207 15:12:18 @agent_ppo2.py:192][0m |          -0.0176 |           2.1896 |           0.1084 |
[32m[20230207 15:12:18 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:12:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -45.17
[32m[20230207 15:12:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 41.25
[32m[20230207 15:12:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -28.40
[32m[20230207 15:12:18 @agent_ppo2.py:150][0m Total time:      11.62 min
[32m[20230207 15:12:18 @agent_ppo2.py:152][0m 600064 total steps have happened
[32m[20230207 15:12:18 @agent_ppo2.py:128][0m #------------------------ Iteration 293 --------------------------#
[32m[20230207 15:12:19 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:12:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:19 @agent_ppo2.py:192][0m |           0.0026 |          18.3464 |           0.1071 |
[32m[20230207 15:12:19 @agent_ppo2.py:192][0m |           0.0078 |           9.4526 |           0.1070 |
[32m[20230207 15:12:19 @agent_ppo2.py:192][0m |          -0.0053 |           8.4283 |           0.1070 |
[32m[20230207 15:12:20 @agent_ppo2.py:192][0m |          -0.0055 |           7.5757 |           0.1070 |
[32m[20230207 15:12:20 @agent_ppo2.py:192][0m |          -0.0035 |           7.1358 |           0.1070 |
[32m[20230207 15:12:20 @agent_ppo2.py:192][0m |          -0.0066 |           6.9291 |           0.1070 |
[32m[20230207 15:12:20 @agent_ppo2.py:192][0m |           0.0125 |           6.7605 |           0.1069 |
[32m[20230207 15:12:20 @agent_ppo2.py:192][0m |          -0.0457 |           7.1519 |           0.1069 |
[32m[20230207 15:12:20 @agent_ppo2.py:192][0m |           0.0187 |           6.8703 |           0.1069 |
[32m[20230207 15:12:20 @agent_ppo2.py:192][0m |          -0.0029 |           6.3978 |           0.1068 |
[32m[20230207 15:12:20 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:12:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -0.68
[32m[20230207 15:12:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 17.42
[32m[20230207 15:12:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -25.16
[32m[20230207 15:12:21 @agent_ppo2.py:150][0m Total time:      11.66 min
[32m[20230207 15:12:21 @agent_ppo2.py:152][0m 602112 total steps have happened
[32m[20230207 15:12:21 @agent_ppo2.py:128][0m #------------------------ Iteration 294 --------------------------#
[32m[20230207 15:12:22 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:12:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:22 @agent_ppo2.py:192][0m |           0.0003 |           8.3047 |           0.1071 |
[32m[20230207 15:12:22 @agent_ppo2.py:192][0m |          -0.0087 |           3.1174 |           0.1070 |
[32m[20230207 15:12:22 @agent_ppo2.py:192][0m |          -0.0111 |           2.7537 |           0.1069 |
[32m[20230207 15:12:22 @agent_ppo2.py:192][0m |          -0.0088 |           2.3675 |           0.1069 |
[32m[20230207 15:12:22 @agent_ppo2.py:192][0m |          -0.0098 |           2.2273 |           0.1068 |
[32m[20230207 15:12:22 @agent_ppo2.py:192][0m |          -0.0161 |           2.0737 |           0.1067 |
[32m[20230207 15:12:22 @agent_ppo2.py:192][0m |           0.0060 |           2.1180 |           0.1066 |
[32m[20230207 15:12:22 @agent_ppo2.py:192][0m |          -0.0150 |           2.2280 |           0.1066 |
[32m[20230207 15:12:22 @agent_ppo2.py:192][0m |          -0.0119 |           1.7303 |           0.1065 |
[32m[20230207 15:12:23 @agent_ppo2.py:192][0m |          -0.0154 |           1.6255 |           0.1065 |
[32m[20230207 15:12:23 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:12:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -48.99
[32m[20230207 15:12:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 21.26
[32m[20230207 15:12:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -37.18
[32m[20230207 15:12:23 @agent_ppo2.py:150][0m Total time:      11.70 min
[32m[20230207 15:12:23 @agent_ppo2.py:152][0m 604160 total steps have happened
[32m[20230207 15:12:23 @agent_ppo2.py:128][0m #------------------------ Iteration 295 --------------------------#
[32m[20230207 15:12:24 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:12:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:24 @agent_ppo2.py:192][0m |          -0.0098 |           3.2395 |           0.1048 |
[32m[20230207 15:12:24 @agent_ppo2.py:192][0m |          -0.0038 |           1.9837 |           0.1048 |
[32m[20230207 15:12:24 @agent_ppo2.py:192][0m |           0.0067 |           1.8177 |           0.1048 |
[32m[20230207 15:12:24 @agent_ppo2.py:192][0m |          -0.0035 |           1.6941 |           0.1047 |
[32m[20230207 15:12:25 @agent_ppo2.py:192][0m |          -0.0038 |           1.6171 |           0.1047 |
[32m[20230207 15:12:25 @agent_ppo2.py:192][0m |          -0.0502 |           1.5718 |           0.1047 |
[32m[20230207 15:12:25 @agent_ppo2.py:192][0m |          -0.0146 |           1.5692 |           0.1046 |
[32m[20230207 15:12:25 @agent_ppo2.py:192][0m |           0.0128 |           1.5377 |           0.1046 |
[32m[20230207 15:12:25 @agent_ppo2.py:192][0m |          -0.0264 |           1.4827 |           0.1045 |
[32m[20230207 15:12:25 @agent_ppo2.py:192][0m |          -0.0149 |           1.4690 |           0.1045 |
[32m[20230207 15:12:25 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:26 @agent_ppo2.py:145][0m Average TRAINING episode reward: 7.70
[32m[20230207 15:12:26 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 15.84
[32m[20230207 15:12:26 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -99.64
[32m[20230207 15:12:26 @agent_ppo2.py:150][0m Total time:      11.74 min
[32m[20230207 15:12:26 @agent_ppo2.py:152][0m 606208 total steps have happened
[32m[20230207 15:12:26 @agent_ppo2.py:128][0m #------------------------ Iteration 296 --------------------------#
[32m[20230207 15:12:27 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:12:27 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:27 @agent_ppo2.py:192][0m |          -0.0006 |          16.9245 |           0.1104 |
[32m[20230207 15:12:27 @agent_ppo2.py:192][0m |          -0.0037 |          11.6160 |           0.1102 |
[32m[20230207 15:12:27 @agent_ppo2.py:192][0m |          -0.0063 |           9.7133 |           0.1103 |
[32m[20230207 15:12:27 @agent_ppo2.py:192][0m |          -0.0071 |           8.2234 |           0.1102 |
[32m[20230207 15:12:27 @agent_ppo2.py:192][0m |          -0.0079 |           6.8890 |           0.1102 |
[32m[20230207 15:12:27 @agent_ppo2.py:192][0m |          -0.0092 |           5.8358 |           0.1101 |
[32m[20230207 15:12:27 @agent_ppo2.py:192][0m |          -0.0099 |           4.7707 |           0.1101 |
[32m[20230207 15:12:27 @agent_ppo2.py:192][0m |          -0.0114 |           3.9193 |           0.1101 |
[32m[20230207 15:12:27 @agent_ppo2.py:192][0m |          -0.0118 |           3.7021 |           0.1101 |
[32m[20230207 15:12:27 @agent_ppo2.py:192][0m |          -0.0118 |           3.2841 |           0.1101 |
[32m[20230207 15:12:27 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:12:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: -117.51
[32m[20230207 15:12:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -20.18
[32m[20230207 15:12:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -32.28
[32m[20230207 15:12:28 @agent_ppo2.py:150][0m Total time:      11.78 min
[32m[20230207 15:12:28 @agent_ppo2.py:152][0m 608256 total steps have happened
[32m[20230207 15:12:28 @agent_ppo2.py:128][0m #------------------------ Iteration 297 --------------------------#
[32m[20230207 15:12:29 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:12:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:29 @agent_ppo2.py:192][0m |          -0.0004 |           9.9473 |           0.1066 |
[32m[20230207 15:12:29 @agent_ppo2.py:192][0m |          -0.0044 |           3.0713 |           0.1065 |
[32m[20230207 15:12:29 @agent_ppo2.py:192][0m |          -0.0066 |           2.5040 |           0.1065 |
[32m[20230207 15:12:29 @agent_ppo2.py:192][0m |          -0.0094 |           2.2497 |           0.1066 |
[32m[20230207 15:12:29 @agent_ppo2.py:192][0m |          -0.0103 |           2.0423 |           0.1066 |
[32m[20230207 15:12:30 @agent_ppo2.py:192][0m |          -0.0109 |           1.9113 |           0.1067 |
[32m[20230207 15:12:30 @agent_ppo2.py:192][0m |          -0.0093 |           1.8209 |           0.1066 |
[32m[20230207 15:12:30 @agent_ppo2.py:192][0m |          -0.0143 |           1.7287 |           0.1067 |
[32m[20230207 15:12:30 @agent_ppo2.py:192][0m |          -0.0134 |           1.6624 |           0.1067 |
[32m[20230207 15:12:30 @agent_ppo2.py:192][0m |          -0.0141 |           1.6054 |           0.1067 |
[32m[20230207 15:12:30 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:12:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -62.97
[32m[20230207 15:12:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -7.23
[32m[20230207 15:12:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -45.96
[32m[20230207 15:12:31 @agent_ppo2.py:150][0m Total time:      11.83 min
[32m[20230207 15:12:31 @agent_ppo2.py:152][0m 610304 total steps have happened
[32m[20230207 15:12:31 @agent_ppo2.py:128][0m #------------------------ Iteration 298 --------------------------#
[32m[20230207 15:12:32 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:12:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:32 @agent_ppo2.py:192][0m |          -0.0073 |           1.8753 |           0.1076 |
[32m[20230207 15:12:32 @agent_ppo2.py:192][0m |          -0.0029 |           1.4379 |           0.1076 |
[32m[20230207 15:12:32 @agent_ppo2.py:192][0m |           0.0007 |           1.3724 |           0.1075 |
[32m[20230207 15:12:32 @agent_ppo2.py:192][0m |          -0.0140 |           1.3284 |           0.1075 |
[32m[20230207 15:12:32 @agent_ppo2.py:192][0m |          -0.0111 |           1.2943 |           0.1074 |
[32m[20230207 15:12:32 @agent_ppo2.py:192][0m |          -0.0067 |           1.2664 |           0.1074 |
[32m[20230207 15:12:32 @agent_ppo2.py:192][0m |          -0.0090 |           1.2515 |           0.1074 |
[32m[20230207 15:12:32 @agent_ppo2.py:192][0m |           0.0128 |           1.2343 |           0.1074 |
[32m[20230207 15:12:32 @agent_ppo2.py:192][0m |          -0.0003 |           1.2150 |           0.1075 |
[32m[20230207 15:12:32 @agent_ppo2.py:192][0m |           0.0009 |           1.2045 |           0.1074 |
[32m[20230207 15:12:32 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: -21.86
[32m[20230207 15:12:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 8.88
[32m[20230207 15:12:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -32.50
[32m[20230207 15:12:33 @agent_ppo2.py:150][0m Total time:      11.87 min
[32m[20230207 15:12:33 @agent_ppo2.py:152][0m 612352 total steps have happened
[32m[20230207 15:12:33 @agent_ppo2.py:128][0m #------------------------ Iteration 299 --------------------------#
[32m[20230207 15:12:34 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:12:34 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:34 @agent_ppo2.py:192][0m |           0.0105 |           5.7893 |           0.1056 |
[32m[20230207 15:12:34 @agent_ppo2.py:192][0m |          -0.0079 |           3.7768 |           0.1055 |
[32m[20230207 15:12:34 @agent_ppo2.py:192][0m |          -0.0092 |           3.3391 |           0.1054 |
[32m[20230207 15:12:34 @agent_ppo2.py:192][0m |          -0.0006 |           3.1041 |           0.1055 |
[32m[20230207 15:12:34 @agent_ppo2.py:192][0m |          -0.0191 |           2.9512 |           0.1053 |
[32m[20230207 15:12:34 @agent_ppo2.py:192][0m |           0.0175 |           2.8824 |           0.1052 |
[32m[20230207 15:12:35 @agent_ppo2.py:192][0m |          -0.0122 |           2.7812 |           0.1051 |
[32m[20230207 15:12:35 @agent_ppo2.py:192][0m |           0.0010 |           2.6454 |           0.1051 |
[32m[20230207 15:12:35 @agent_ppo2.py:192][0m |          -0.0052 |           2.4919 |           0.1052 |
[32m[20230207 15:12:35 @agent_ppo2.py:192][0m |          -0.0020 |           2.4429 |           0.1052 |
[32m[20230207 15:12:35 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -16.50
[32m[20230207 15:12:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 49.05
[32m[20230207 15:12:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 5.49
[32m[20230207 15:12:36 @agent_ppo2.py:150][0m Total time:      11.91 min
[32m[20230207 15:12:36 @agent_ppo2.py:152][0m 614400 total steps have happened
[32m[20230207 15:12:36 @agent_ppo2.py:128][0m #------------------------ Iteration 300 --------------------------#
[32m[20230207 15:12:36 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:12:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:36 @agent_ppo2.py:192][0m |          -0.0179 |           1.4315 |           0.1061 |
[32m[20230207 15:12:37 @agent_ppo2.py:192][0m |           0.0169 |           1.1837 |           0.1062 |
[32m[20230207 15:12:37 @agent_ppo2.py:192][0m |          -0.0071 |           1.1394 |           0.1063 |
[32m[20230207 15:12:37 @agent_ppo2.py:192][0m |          -0.0105 |           1.1161 |           0.1063 |
[32m[20230207 15:12:37 @agent_ppo2.py:192][0m |          -0.0055 |           1.0977 |           0.1064 |
[32m[20230207 15:12:37 @agent_ppo2.py:192][0m |          -0.0007 |           1.0700 |           0.1064 |
[32m[20230207 15:12:37 @agent_ppo2.py:192][0m |          -0.0095 |           1.0575 |           0.1064 |
[32m[20230207 15:12:37 @agent_ppo2.py:192][0m |          -0.0050 |           1.0420 |           0.1064 |
[32m[20230207 15:12:37 @agent_ppo2.py:192][0m |           0.0078 |           1.0413 |           0.1065 |
[32m[20230207 15:12:37 @agent_ppo2.py:192][0m |           0.0014 |           1.0204 |           0.1065 |
[32m[20230207 15:12:37 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: -49.64
[32m[20230207 15:12:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -48.04
[32m[20230207 15:12:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -39.22
[32m[20230207 15:12:38 @agent_ppo2.py:150][0m Total time:      11.95 min
[32m[20230207 15:12:38 @agent_ppo2.py:152][0m 616448 total steps have happened
[32m[20230207 15:12:38 @agent_ppo2.py:128][0m #------------------------ Iteration 301 --------------------------#
[32m[20230207 15:12:39 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:12:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:39 @agent_ppo2.py:192][0m |           0.0022 |           1.4202 |           0.1095 |
[32m[20230207 15:12:39 @agent_ppo2.py:192][0m |          -0.0004 |           1.1764 |           0.1094 |
[32m[20230207 15:12:39 @agent_ppo2.py:192][0m |          -0.0200 |           1.1254 |           0.1095 |
[32m[20230207 15:12:39 @agent_ppo2.py:192][0m |           0.0133 |           1.1084 |           0.1095 |
[32m[20230207 15:12:39 @agent_ppo2.py:192][0m |          -0.0063 |           1.0785 |           0.1096 |
[32m[20230207 15:12:39 @agent_ppo2.py:192][0m |          -0.0207 |           1.0693 |           0.1095 |
[32m[20230207 15:12:39 @agent_ppo2.py:192][0m |          -0.0219 |           1.0524 |           0.1095 |
[32m[20230207 15:12:39 @agent_ppo2.py:192][0m |           0.0013 |           1.0255 |           0.1095 |
[32m[20230207 15:12:39 @agent_ppo2.py:192][0m |          -0.0055 |           1.0102 |           0.1095 |
[32m[20230207 15:12:40 @agent_ppo2.py:192][0m |           0.0088 |           1.0086 |           0.1095 |
[32m[20230207 15:12:40 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: -10.32
[32m[20230207 15:12:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -3.22
[32m[20230207 15:12:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -73.56
[32m[20230207 15:12:40 @agent_ppo2.py:150][0m Total time:      11.98 min
[32m[20230207 15:12:40 @agent_ppo2.py:152][0m 618496 total steps have happened
[32m[20230207 15:12:40 @agent_ppo2.py:128][0m #------------------------ Iteration 302 --------------------------#
[32m[20230207 15:12:41 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:12:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:41 @agent_ppo2.py:192][0m |           0.0025 |           1.9784 |           0.1085 |
[32m[20230207 15:12:41 @agent_ppo2.py:192][0m |          -0.0163 |           1.4751 |           0.1084 |
[32m[20230207 15:12:41 @agent_ppo2.py:192][0m |           0.0052 |           1.3993 |           0.1084 |
[32m[20230207 15:12:41 @agent_ppo2.py:192][0m |          -0.0056 |           1.3566 |           0.1084 |
[32m[20230207 15:12:41 @agent_ppo2.py:192][0m |          -0.0033 |           1.3157 |           0.1084 |
[32m[20230207 15:12:41 @agent_ppo2.py:192][0m |           0.0081 |           1.2747 |           0.1084 |
[32m[20230207 15:12:41 @agent_ppo2.py:192][0m |          -0.0040 |           1.2470 |           0.1083 |
[32m[20230207 15:12:42 @agent_ppo2.py:192][0m |          -0.0167 |           1.2255 |           0.1084 |
[32m[20230207 15:12:42 @agent_ppo2.py:192][0m |          -0.0242 |           1.2112 |           0.1084 |
[32m[20230207 15:12:42 @agent_ppo2.py:192][0m |           0.0058 |           1.2292 |           0.1083 |
[32m[20230207 15:12:42 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: -35.90
[32m[20230207 15:12:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -0.12
[32m[20230207 15:12:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -66.27
[32m[20230207 15:12:42 @agent_ppo2.py:150][0m Total time:      12.02 min
[32m[20230207 15:12:42 @agent_ppo2.py:152][0m 620544 total steps have happened
[32m[20230207 15:12:42 @agent_ppo2.py:128][0m #------------------------ Iteration 303 --------------------------#
[32m[20230207 15:12:43 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:12:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:43 @agent_ppo2.py:192][0m |           0.0138 |           1.6566 |           0.1081 |
[32m[20230207 15:12:43 @agent_ppo2.py:192][0m |          -0.0183 |           1.2290 |           0.1080 |
[32m[20230207 15:12:43 @agent_ppo2.py:192][0m |           0.0056 |           1.1525 |           0.1080 |
[32m[20230207 15:12:44 @agent_ppo2.py:192][0m |          -0.0088 |           1.1162 |           0.1079 |
[32m[20230207 15:12:44 @agent_ppo2.py:192][0m |          -0.0237 |           1.0886 |           0.1079 |
[32m[20230207 15:12:44 @agent_ppo2.py:192][0m |          -0.0393 |           1.1992 |           0.1079 |
[32m[20230207 15:12:44 @agent_ppo2.py:192][0m |          -0.0075 |           1.0795 |           0.1078 |
[32m[20230207 15:12:44 @agent_ppo2.py:192][0m |          -0.0010 |           1.0392 |           0.1079 |
[32m[20230207 15:12:44 @agent_ppo2.py:192][0m |          -0.0190 |           1.0132 |           0.1079 |
[32m[20230207 15:12:44 @agent_ppo2.py:192][0m |          -0.0089 |           1.0016 |           0.1078 |
[32m[20230207 15:12:44 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: -25.44
[32m[20230207 15:12:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -9.74
[32m[20230207 15:12:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 23.29
[32m[20230207 15:12:45 @agent_ppo2.py:150][0m Total time:      12.06 min
[32m[20230207 15:12:45 @agent_ppo2.py:152][0m 622592 total steps have happened
[32m[20230207 15:12:45 @agent_ppo2.py:128][0m #------------------------ Iteration 304 --------------------------#
[32m[20230207 15:12:45 @agent_ppo2.py:134][0m Sampling time: 0.51 s by 1 slaves
[32m[20230207 15:12:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:45 @agent_ppo2.py:192][0m |           0.0109 |           3.7520 |           0.1101 |
[32m[20230207 15:12:45 @agent_ppo2.py:192][0m |          -0.0060 |           2.0363 |           0.1100 |
[32m[20230207 15:12:45 @agent_ppo2.py:192][0m |          -0.0046 |           1.7616 |           0.1100 |
[32m[20230207 15:12:46 @agent_ppo2.py:192][0m |          -0.0079 |           1.6070 |           0.1099 |
[32m[20230207 15:12:46 @agent_ppo2.py:192][0m |          -0.0026 |           1.4999 |           0.1099 |
[32m[20230207 15:12:46 @agent_ppo2.py:192][0m |          -0.0091 |           1.4301 |           0.1099 |
[32m[20230207 15:12:46 @agent_ppo2.py:192][0m |          -0.0114 |           1.4183 |           0.1099 |
[32m[20230207 15:12:46 @agent_ppo2.py:192][0m |          -0.0118 |           1.3087 |           0.1098 |
[32m[20230207 15:12:46 @agent_ppo2.py:192][0m |          -0.0117 |           1.2777 |           0.1098 |
[32m[20230207 15:12:46 @agent_ppo2.py:192][0m |          -0.0085 |           1.2312 |           0.1098 |
[32m[20230207 15:12:46 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:12:47 @agent_ppo2.py:145][0m Average TRAINING episode reward: -65.89
[32m[20230207 15:12:47 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -30.05
[32m[20230207 15:12:47 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -26.54
[32m[20230207 15:12:47 @agent_ppo2.py:150][0m Total time:      12.09 min
[32m[20230207 15:12:47 @agent_ppo2.py:152][0m 624640 total steps have happened
[32m[20230207 15:12:47 @agent_ppo2.py:128][0m #------------------------ Iteration 305 --------------------------#
[32m[20230207 15:12:47 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:12:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:48 @agent_ppo2.py:192][0m |           0.0101 |           1.5960 |           0.1102 |
[32m[20230207 15:12:48 @agent_ppo2.py:192][0m |           0.0011 |           1.1712 |           0.1101 |
[32m[20230207 15:12:48 @agent_ppo2.py:192][0m |          -0.0234 |           1.1560 |           0.1100 |
[32m[20230207 15:12:48 @agent_ppo2.py:192][0m |          -0.0055 |           1.0967 |           0.1100 |
[32m[20230207 15:12:48 @agent_ppo2.py:192][0m |          -0.0259 |           1.0578 |           0.1100 |
[32m[20230207 15:12:48 @agent_ppo2.py:192][0m |          -0.0082 |           1.0409 |           0.1100 |
[32m[20230207 15:12:48 @agent_ppo2.py:192][0m |           0.0029 |           1.0245 |           0.1100 |
[32m[20230207 15:12:48 @agent_ppo2.py:192][0m |          -0.0075 |           1.0125 |           0.1101 |
[32m[20230207 15:12:48 @agent_ppo2.py:192][0m |          -0.0132 |           1.0018 |           0.1100 |
[32m[20230207 15:12:48 @agent_ppo2.py:192][0m |           0.0025 |           0.9938 |           0.1101 |
[32m[20230207 15:12:48 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: 29.97
[32m[20230207 15:12:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 62.30
[32m[20230207 15:12:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -4.79
[32m[20230207 15:12:49 @agent_ppo2.py:150][0m Total time:      12.13 min
[32m[20230207 15:12:49 @agent_ppo2.py:152][0m 626688 total steps have happened
[32m[20230207 15:12:49 @agent_ppo2.py:128][0m #------------------------ Iteration 306 --------------------------#
[32m[20230207 15:12:50 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:12:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:50 @agent_ppo2.py:192][0m |           0.0146 |           1.3613 |           0.1113 |
[32m[20230207 15:12:50 @agent_ppo2.py:192][0m |           0.0007 |           1.1382 |           0.1112 |
[32m[20230207 15:12:50 @agent_ppo2.py:192][0m |           0.0044 |           1.0810 |           0.1112 |
[32m[20230207 15:12:50 @agent_ppo2.py:192][0m |           0.0138 |           1.0655 |           0.1111 |
[32m[20230207 15:12:50 @agent_ppo2.py:192][0m |          -0.0109 |           1.0304 |           0.1109 |
[32m[20230207 15:12:50 @agent_ppo2.py:192][0m |           0.0177 |           1.0113 |           0.1109 |
[32m[20230207 15:12:50 @agent_ppo2.py:192][0m |          -0.0106 |           0.9871 |           0.1110 |
[32m[20230207 15:12:51 @agent_ppo2.py:192][0m |          -0.0104 |           0.9757 |           0.1110 |
[32m[20230207 15:12:51 @agent_ppo2.py:192][0m |          -0.0053 |           0.9706 |           0.1110 |
[32m[20230207 15:12:51 @agent_ppo2.py:192][0m |          -0.0001 |           0.9531 |           0.1110 |
[32m[20230207 15:12:51 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: 5.97
[32m[20230207 15:12:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 12.52
[32m[20230207 15:12:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 22.92
[32m[20230207 15:12:51 @agent_ppo2.py:150][0m Total time:      12.17 min
[32m[20230207 15:12:51 @agent_ppo2.py:152][0m 628736 total steps have happened
[32m[20230207 15:12:51 @agent_ppo2.py:128][0m #------------------------ Iteration 307 --------------------------#
[32m[20230207 15:12:52 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:12:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:52 @agent_ppo2.py:192][0m |          -0.0018 |          32.9699 |           0.1120 |
[32m[20230207 15:12:53 @agent_ppo2.py:192][0m |          -0.0011 |          23.0359 |           0.1120 |
[32m[20230207 15:12:53 @agent_ppo2.py:192][0m |          -0.0062 |          18.3457 |           0.1121 |
[32m[20230207 15:12:53 @agent_ppo2.py:192][0m |          -0.0082 |          14.2470 |           0.1121 |
[32m[20230207 15:12:53 @agent_ppo2.py:192][0m |          -0.0076 |          10.4145 |           0.1121 |
[32m[20230207 15:12:53 @agent_ppo2.py:192][0m |          -0.0088 |           8.0813 |           0.1121 |
[32m[20230207 15:12:53 @agent_ppo2.py:192][0m |          -0.0074 |           6.6692 |           0.1121 |
[32m[20230207 15:12:53 @agent_ppo2.py:192][0m |          -0.0122 |           5.3558 |           0.1121 |
[32m[20230207 15:12:53 @agent_ppo2.py:192][0m |          -0.0127 |           4.4660 |           0.1121 |
[32m[20230207 15:12:53 @agent_ppo2.py:192][0m |          -0.0129 |           3.9847 |           0.1121 |
[32m[20230207 15:12:53 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:12:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: -75.44
[32m[20230207 15:12:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 23.64
[32m[20230207 15:12:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -3.06
[32m[20230207 15:12:54 @agent_ppo2.py:150][0m Total time:      12.21 min
[32m[20230207 15:12:54 @agent_ppo2.py:152][0m 630784 total steps have happened
[32m[20230207 15:12:54 @agent_ppo2.py:128][0m #------------------------ Iteration 308 --------------------------#
[32m[20230207 15:12:55 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:12:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:55 @agent_ppo2.py:192][0m |          -0.0518 |           3.5853 |           0.1130 |
[32m[20230207 15:12:55 @agent_ppo2.py:192][0m |           0.0124 |           1.7311 |           0.1125 |
[32m[20230207 15:12:55 @agent_ppo2.py:192][0m |           0.0008 |           1.4650 |           0.1129 |
[32m[20230207 15:12:55 @agent_ppo2.py:192][0m |           0.0037 |           1.3582 |           0.1129 |
[32m[20230207 15:12:55 @agent_ppo2.py:192][0m |           0.0047 |           1.2992 |           0.1130 |
[32m[20230207 15:12:55 @agent_ppo2.py:192][0m |          -0.0007 |           1.2619 |           0.1131 |
[32m[20230207 15:12:55 @agent_ppo2.py:192][0m |          -0.0116 |           1.2233 |           0.1131 |
[32m[20230207 15:12:55 @agent_ppo2.py:192][0m |          -0.0013 |           1.1931 |           0.1131 |
[32m[20230207 15:12:56 @agent_ppo2.py:192][0m |          -0.0057 |           1.1701 |           0.1131 |
[32m[20230207 15:12:56 @agent_ppo2.py:192][0m |          -0.0060 |           1.2267 |           0.1131 |
[32m[20230207 15:12:56 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: -50.13
[32m[20230207 15:12:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -47.02
[32m[20230207 15:12:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -61.02
[32m[20230207 15:12:56 @agent_ppo2.py:150][0m Total time:      12.25 min
[32m[20230207 15:12:56 @agent_ppo2.py:152][0m 632832 total steps have happened
[32m[20230207 15:12:56 @agent_ppo2.py:128][0m #------------------------ Iteration 309 --------------------------#
[32m[20230207 15:12:57 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:12:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:57 @agent_ppo2.py:192][0m |           0.0044 |           1.5687 |           0.1115 |
[32m[20230207 15:12:57 @agent_ppo2.py:192][0m |          -0.0028 |           1.0674 |           0.1113 |
[32m[20230207 15:12:57 @agent_ppo2.py:192][0m |           0.0103 |           1.0745 |           0.1113 |
[32m[20230207 15:12:57 @agent_ppo2.py:192][0m |           0.0006 |           0.9455 |           0.1111 |
[32m[20230207 15:12:57 @agent_ppo2.py:192][0m |           0.0054 |           0.9096 |           0.1112 |
[32m[20230207 15:12:57 @agent_ppo2.py:192][0m |          -0.0205 |           0.8923 |           0.1113 |
[32m[20230207 15:12:58 @agent_ppo2.py:192][0m |           0.0034 |           0.8788 |           0.1112 |
[32m[20230207 15:12:58 @agent_ppo2.py:192][0m |          -0.0142 |           0.8616 |           0.1112 |
[32m[20230207 15:12:58 @agent_ppo2.py:192][0m |          -0.0398 |           0.8497 |           0.1112 |
[32m[20230207 15:12:58 @agent_ppo2.py:192][0m |          -0.0089 |           0.8419 |           0.1113 |
[32m[20230207 15:12:58 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:12:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: 14.19
[32m[20230207 15:12:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 61.13
[32m[20230207 15:12:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -11.78
[32m[20230207 15:12:59 @agent_ppo2.py:150][0m Total time:      12.29 min
[32m[20230207 15:12:59 @agent_ppo2.py:152][0m 634880 total steps have happened
[32m[20230207 15:12:59 @agent_ppo2.py:128][0m #------------------------ Iteration 310 --------------------------#
[32m[20230207 15:12:59 @agent_ppo2.py:134][0m Sampling time: 0.66 s by 1 slaves
[32m[20230207 15:12:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:12:59 @agent_ppo2.py:192][0m |           0.0038 |          13.7083 |           0.1116 |
[32m[20230207 15:12:59 @agent_ppo2.py:192][0m |          -0.0046 |           1.6904 |           0.1116 |
[32m[20230207 15:12:59 @agent_ppo2.py:192][0m |           0.0002 |           1.4541 |           0.1117 |
[32m[20230207 15:13:00 @agent_ppo2.py:192][0m |          -0.0064 |           1.3979 |           0.1117 |
[32m[20230207 15:13:00 @agent_ppo2.py:192][0m |          -0.0005 |           1.2825 |           0.1117 |
[32m[20230207 15:13:00 @agent_ppo2.py:192][0m |          -0.0077 |           1.3613 |           0.1117 |
[32m[20230207 15:13:00 @agent_ppo2.py:192][0m |          -0.0089 |           1.2189 |           0.1117 |
[32m[20230207 15:13:00 @agent_ppo2.py:192][0m |          -0.0097 |           1.1670 |           0.1117 |
[32m[20230207 15:13:00 @agent_ppo2.py:192][0m |          -0.0080 |           1.1232 |           0.1117 |
[32m[20230207 15:13:00 @agent_ppo2.py:192][0m |          -0.0096 |           1.1413 |           0.1117 |
[32m[20230207 15:13:00 @agent_ppo2.py:137][0m Policy update time: 0.67 s
[32m[20230207 15:13:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -48.27
[32m[20230207 15:13:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 40.46
[32m[20230207 15:13:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -22.60
[32m[20230207 15:13:01 @agent_ppo2.py:150][0m Total time:      12.33 min
[32m[20230207 15:13:01 @agent_ppo2.py:152][0m 636928 total steps have happened
[32m[20230207 15:13:01 @agent_ppo2.py:128][0m #------------------------ Iteration 311 --------------------------#
[32m[20230207 15:13:02 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:13:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:02 @agent_ppo2.py:192][0m |          -0.0113 |           1.4603 |           0.1120 |
[32m[20230207 15:13:02 @agent_ppo2.py:192][0m |          -0.0015 |           1.2196 |           0.1119 |
[32m[20230207 15:13:02 @agent_ppo2.py:192][0m |          -0.0221 |           1.1314 |           0.1118 |
[32m[20230207 15:13:02 @agent_ppo2.py:192][0m |          -0.0068 |           1.0831 |           0.1118 |
[32m[20230207 15:13:02 @agent_ppo2.py:192][0m |          -0.0026 |           1.0391 |           0.1118 |
[32m[20230207 15:13:02 @agent_ppo2.py:192][0m |          -0.0119 |           1.0221 |           0.1118 |
[32m[20230207 15:13:02 @agent_ppo2.py:192][0m |           0.0021 |           1.0025 |           0.1118 |
[32m[20230207 15:13:02 @agent_ppo2.py:192][0m |          -0.0283 |           0.9860 |           0.1118 |
[32m[20230207 15:13:02 @agent_ppo2.py:192][0m |          -0.0028 |           0.9654 |           0.1118 |
[32m[20230207 15:13:02 @agent_ppo2.py:192][0m |          -0.0063 |           0.9458 |           0.1119 |
[32m[20230207 15:13:02 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:13:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -52.31
[32m[20230207 15:13:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -44.70
[32m[20230207 15:13:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -105.37
[32m[20230207 15:13:03 @agent_ppo2.py:150][0m Total time:      12.36 min
[32m[20230207 15:13:03 @agent_ppo2.py:152][0m 638976 total steps have happened
[32m[20230207 15:13:03 @agent_ppo2.py:128][0m #------------------------ Iteration 312 --------------------------#
[32m[20230207 15:13:04 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:13:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:04 @agent_ppo2.py:192][0m |          -0.0007 |           1.2246 |           0.1167 |
[32m[20230207 15:13:04 @agent_ppo2.py:192][0m |          -0.0073 |           1.0270 |           0.1165 |
[32m[20230207 15:13:04 @agent_ppo2.py:192][0m |           0.0670 |           1.2625 |           0.1166 |
[32m[20230207 15:13:04 @agent_ppo2.py:192][0m |          -0.0047 |           1.0306 |           0.1167 |
[32m[20230207 15:13:04 @agent_ppo2.py:192][0m |          -0.0560 |           0.9663 |           0.1167 |
[32m[20230207 15:13:04 @agent_ppo2.py:192][0m |          -0.0033 |           0.9358 |           0.1168 |
[32m[20230207 15:13:04 @agent_ppo2.py:192][0m |          -0.0070 |           0.9217 |           0.1168 |
[32m[20230207 15:13:04 @agent_ppo2.py:192][0m |          -0.0151 |           0.9094 |           0.1168 |
[32m[20230207 15:13:04 @agent_ppo2.py:192][0m |           0.0038 |           0.9107 |           0.1168 |
[32m[20230207 15:13:04 @agent_ppo2.py:192][0m |          -0.0116 |           0.9084 |           0.1168 |
[32m[20230207 15:13:04 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:13:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -2.61
[32m[20230207 15:13:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 25.24
[32m[20230207 15:13:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -57.11
[32m[20230207 15:13:05 @agent_ppo2.py:150][0m Total time:      12.40 min
[32m[20230207 15:13:05 @agent_ppo2.py:152][0m 641024 total steps have happened
[32m[20230207 15:13:05 @agent_ppo2.py:128][0m #------------------------ Iteration 313 --------------------------#
[32m[20230207 15:13:06 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:13:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:06 @agent_ppo2.py:192][0m |           0.0076 |           3.8871 |           0.1140 |
[32m[20230207 15:13:06 @agent_ppo2.py:192][0m |           0.0080 |           3.1818 |           0.1138 |
[32m[20230207 15:13:06 @agent_ppo2.py:192][0m |          -0.0010 |           2.8064 |           0.1139 |
[32m[20230207 15:13:06 @agent_ppo2.py:192][0m |          -0.0028 |           2.6474 |           0.1138 |
[32m[20230207 15:13:07 @agent_ppo2.py:192][0m |          -0.0772 |           2.5685 |           0.1138 |
[32m[20230207 15:13:07 @agent_ppo2.py:192][0m |          -0.0017 |           2.4481 |           0.1137 |
[32m[20230207 15:13:07 @agent_ppo2.py:192][0m |          -0.0022 |           2.3623 |           0.1138 |
[32m[20230207 15:13:07 @agent_ppo2.py:192][0m |          -0.0049 |           2.2747 |           0.1139 |
[32m[20230207 15:13:07 @agent_ppo2.py:192][0m |          -0.0104 |           2.2792 |           0.1139 |
[32m[20230207 15:13:07 @agent_ppo2.py:192][0m |          -0.0013 |           2.1828 |           0.1139 |
[32m[20230207 15:13:07 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:13:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -0.48
[32m[20230207 15:13:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 18.29
[32m[20230207 15:13:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -40.99
[32m[20230207 15:13:08 @agent_ppo2.py:150][0m Total time:      12.44 min
[32m[20230207 15:13:08 @agent_ppo2.py:152][0m 643072 total steps have happened
[32m[20230207 15:13:08 @agent_ppo2.py:128][0m #------------------------ Iteration 314 --------------------------#
[32m[20230207 15:13:09 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:13:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:09 @agent_ppo2.py:192][0m |           0.0021 |           3.3740 |           0.1182 |
[32m[20230207 15:13:09 @agent_ppo2.py:192][0m |          -0.0026 |           1.4430 |           0.1181 |
[32m[20230207 15:13:09 @agent_ppo2.py:192][0m |          -0.0046 |           1.2291 |           0.1181 |
[32m[20230207 15:13:09 @agent_ppo2.py:192][0m |          -0.0064 |           1.1282 |           0.1180 |
[32m[20230207 15:13:09 @agent_ppo2.py:192][0m |          -0.0092 |           1.0728 |           0.1179 |
[32m[20230207 15:13:09 @agent_ppo2.py:192][0m |          -0.0100 |           1.0447 |           0.1179 |
[32m[20230207 15:13:09 @agent_ppo2.py:192][0m |          -0.0107 |           1.0115 |           0.1179 |
[32m[20230207 15:13:09 @agent_ppo2.py:192][0m |          -0.0116 |           0.9839 |           0.1178 |
[32m[20230207 15:13:09 @agent_ppo2.py:192][0m |          -0.0114 |           0.9662 |           0.1178 |
[32m[20230207 15:13:09 @agent_ppo2.py:192][0m |          -0.0116 |           0.9652 |           0.1178 |
[32m[20230207 15:13:09 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:13:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -54.71
[32m[20230207 15:13:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -15.97
[32m[20230207 15:13:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -40.43
[32m[20230207 15:13:10 @agent_ppo2.py:150][0m Total time:      12.48 min
[32m[20230207 15:13:10 @agent_ppo2.py:152][0m 645120 total steps have happened
[32m[20230207 15:13:10 @agent_ppo2.py:128][0m #------------------------ Iteration 315 --------------------------#
[32m[20230207 15:13:11 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:13:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:11 @agent_ppo2.py:192][0m |          -0.0002 |           2.0041 |           0.1146 |
[32m[20230207 15:13:11 @agent_ppo2.py:192][0m |          -0.0228 |           1.4583 |           0.1146 |
[32m[20230207 15:13:11 @agent_ppo2.py:192][0m |          -0.0151 |           1.3539 |           0.1144 |
[32m[20230207 15:13:11 @agent_ppo2.py:192][0m |           0.0010 |           1.2860 |           0.1144 |
[32m[20230207 15:13:11 @agent_ppo2.py:192][0m |          -0.0191 |           1.2461 |           0.1143 |
[32m[20230207 15:13:11 @agent_ppo2.py:192][0m |           0.0005 |           1.2209 |           0.1144 |
[32m[20230207 15:13:12 @agent_ppo2.py:192][0m |          -0.0527 |           1.1915 |           0.1143 |
[32m[20230207 15:13:12 @agent_ppo2.py:192][0m |          -0.0015 |           1.1583 |           0.1140 |
[32m[20230207 15:13:12 @agent_ppo2.py:192][0m |           0.0157 |           1.1518 |           0.1141 |
[32m[20230207 15:13:12 @agent_ppo2.py:192][0m |          -0.0081 |           1.1151 |           0.1142 |
[32m[20230207 15:13:12 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:13:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: 13.15
[32m[20230207 15:13:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 22.69
[32m[20230207 15:13:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -16.52
[32m[20230207 15:13:13 @agent_ppo2.py:150][0m Total time:      12.52 min
[32m[20230207 15:13:13 @agent_ppo2.py:152][0m 647168 total steps have happened
[32m[20230207 15:13:13 @agent_ppo2.py:128][0m #------------------------ Iteration 316 --------------------------#
[32m[20230207 15:13:13 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:13:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:14 @agent_ppo2.py:192][0m |           0.0060 |           1.0579 |           0.1154 |
[32m[20230207 15:13:14 @agent_ppo2.py:192][0m |          -0.0141 |           0.8103 |           0.1154 |
[32m[20230207 15:13:14 @agent_ppo2.py:192][0m |          -0.0368 |           0.7752 |           0.1154 |
[32m[20230207 15:13:14 @agent_ppo2.py:192][0m |          -0.0161 |           0.7706 |           0.1153 |
[32m[20230207 15:13:14 @agent_ppo2.py:192][0m |          -0.0044 |           0.7517 |           0.1153 |
[32m[20230207 15:13:14 @agent_ppo2.py:192][0m |           0.0086 |           0.7270 |           0.1153 |
[32m[20230207 15:13:14 @agent_ppo2.py:192][0m |          -0.0006 |           0.7235 |           0.1153 |
[32m[20230207 15:13:14 @agent_ppo2.py:192][0m |          -0.0268 |           0.7157 |           0.1154 |
[32m[20230207 15:13:14 @agent_ppo2.py:192][0m |           0.0013 |           0.7073 |           0.1154 |
[32m[20230207 15:13:14 @agent_ppo2.py:192][0m |          -0.0109 |           0.7078 |           0.1154 |
[32m[20230207 15:13:14 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:13:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: 2.79
[32m[20230207 15:13:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 28.31
[32m[20230207 15:13:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -4.87
[32m[20230207 15:13:15 @agent_ppo2.py:150][0m Total time:      12.56 min
[32m[20230207 15:13:15 @agent_ppo2.py:152][0m 649216 total steps have happened
[32m[20230207 15:13:15 @agent_ppo2.py:128][0m #------------------------ Iteration 317 --------------------------#
[32m[20230207 15:13:16 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:13:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:16 @agent_ppo2.py:192][0m |          -0.0137 |           0.8599 |           0.1159 |
[32m[20230207 15:13:16 @agent_ppo2.py:192][0m |          -0.0141 |           0.7781 |           0.1159 |
[32m[20230207 15:13:16 @agent_ppo2.py:192][0m |          -0.0180 |           0.7481 |           0.1159 |
[32m[20230207 15:13:16 @agent_ppo2.py:192][0m |          -0.0145 |           0.7378 |           0.1160 |
[32m[20230207 15:13:16 @agent_ppo2.py:192][0m |          -0.0304 |           0.7437 |           0.1160 |
[32m[20230207 15:13:16 @agent_ppo2.py:192][0m |          -0.0189 |           0.7206 |           0.1161 |
[32m[20230207 15:13:16 @agent_ppo2.py:192][0m |          -0.0121 |           0.7078 |           0.1162 |
[32m[20230207 15:13:16 @agent_ppo2.py:192][0m |           0.0007 |           0.7037 |           0.1161 |
[32m[20230207 15:13:17 @agent_ppo2.py:192][0m |          -0.0043 |           0.7047 |           0.1161 |
[32m[20230207 15:13:17 @agent_ppo2.py:192][0m |          -0.0110 |           0.6997 |           0.1162 |
[32m[20230207 15:13:17 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:13:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: -29.25
[32m[20230207 15:13:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -7.05
[32m[20230207 15:13:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -28.47
[32m[20230207 15:13:17 @agent_ppo2.py:150][0m Total time:      12.60 min
[32m[20230207 15:13:17 @agent_ppo2.py:152][0m 651264 total steps have happened
[32m[20230207 15:13:17 @agent_ppo2.py:128][0m #------------------------ Iteration 318 --------------------------#
[32m[20230207 15:13:18 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:13:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:18 @agent_ppo2.py:192][0m |           0.0088 |           0.9439 |           0.1157 |
[32m[20230207 15:13:18 @agent_ppo2.py:192][0m |          -0.0099 |           0.7784 |           0.1157 |
[32m[20230207 15:13:18 @agent_ppo2.py:192][0m |          -0.0209 |           0.7409 |           0.1156 |
[32m[20230207 15:13:19 @agent_ppo2.py:192][0m |          -0.0041 |           0.7214 |           0.1157 |
[32m[20230207 15:13:19 @agent_ppo2.py:192][0m |           0.0094 |           0.7002 |           0.1157 |
[32m[20230207 15:13:19 @agent_ppo2.py:192][0m |           0.0038 |           0.6877 |           0.1158 |
[32m[20230207 15:13:19 @agent_ppo2.py:192][0m |           0.0042 |           0.6765 |           0.1158 |
[32m[20230207 15:13:19 @agent_ppo2.py:192][0m |          -0.0103 |           0.6757 |           0.1158 |
[32m[20230207 15:13:19 @agent_ppo2.py:192][0m |           0.0208 |           0.6717 |           0.1158 |
[32m[20230207 15:13:19 @agent_ppo2.py:192][0m |          -0.0112 |           0.6612 |           0.1159 |
[32m[20230207 15:13:19 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:13:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: 24.90
[32m[20230207 15:13:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 56.52
[32m[20230207 15:13:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -19.40
[32m[20230207 15:13:20 @agent_ppo2.py:150][0m Total time:      12.64 min
[32m[20230207 15:13:20 @agent_ppo2.py:152][0m 653312 total steps have happened
[32m[20230207 15:13:20 @agent_ppo2.py:128][0m #------------------------ Iteration 319 --------------------------#
[32m[20230207 15:13:21 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:13:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:21 @agent_ppo2.py:192][0m |          -0.0055 |           1.1082 |           0.1184 |
[32m[20230207 15:13:21 @agent_ppo2.py:192][0m |           0.0164 |           0.9402 |           0.1184 |
[32m[20230207 15:13:21 @agent_ppo2.py:192][0m |          -0.0085 |           0.8993 |           0.1184 |
[32m[20230207 15:13:21 @agent_ppo2.py:192][0m |          -0.0067 |           0.8848 |           0.1184 |
[32m[20230207 15:13:21 @agent_ppo2.py:192][0m |          -0.0133 |           0.8741 |           0.1184 |
[32m[20230207 15:13:21 @agent_ppo2.py:192][0m |          -0.0299 |           0.8553 |           0.1183 |
[32m[20230207 15:13:21 @agent_ppo2.py:192][0m |          -0.0027 |           0.8478 |           0.1183 |
[32m[20230207 15:13:21 @agent_ppo2.py:192][0m |          -0.0067 |           0.8380 |           0.1184 |
[32m[20230207 15:13:21 @agent_ppo2.py:192][0m |           0.0052 |           0.8376 |           0.1184 |
[32m[20230207 15:13:21 @agent_ppo2.py:192][0m |          -0.0273 |           0.8386 |           0.1184 |
[32m[20230207 15:13:21 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:13:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: 12.91
[32m[20230207 15:13:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 32.57
[32m[20230207 15:13:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -85.27
[32m[20230207 15:13:22 @agent_ppo2.py:150][0m Total time:      12.68 min
[32m[20230207 15:13:22 @agent_ppo2.py:152][0m 655360 total steps have happened
[32m[20230207 15:13:22 @agent_ppo2.py:128][0m #------------------------ Iteration 320 --------------------------#
[32m[20230207 15:13:23 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:13:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:23 @agent_ppo2.py:192][0m |          -0.0009 |           4.1536 |           0.1190 |
[32m[20230207 15:13:23 @agent_ppo2.py:192][0m |          -0.0083 |           1.3423 |           0.1190 |
[32m[20230207 15:13:23 @agent_ppo2.py:192][0m |          -0.0097 |           1.0876 |           0.1189 |
[32m[20230207 15:13:23 @agent_ppo2.py:192][0m |          -0.0089 |           0.9426 |           0.1188 |
[32m[20230207 15:13:23 @agent_ppo2.py:192][0m |          -0.0098 |           0.8937 |           0.1188 |
[32m[20230207 15:13:23 @agent_ppo2.py:192][0m |          -0.0126 |           0.8630 |           0.1187 |
[32m[20230207 15:13:23 @agent_ppo2.py:192][0m |          -0.0134 |           0.8388 |           0.1187 |
[32m[20230207 15:13:24 @agent_ppo2.py:192][0m |          -0.0128 |           0.8320 |           0.1187 |
[32m[20230207 15:13:24 @agent_ppo2.py:192][0m |          -0.0143 |           0.8192 |           0.1187 |
[32m[20230207 15:13:24 @agent_ppo2.py:192][0m |          -0.0108 |           0.8028 |           0.1186 |
[32m[20230207 15:13:24 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:13:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: -47.23
[32m[20230207 15:13:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 19.47
[32m[20230207 15:13:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -36.68
[32m[20230207 15:13:24 @agent_ppo2.py:150][0m Total time:      12.72 min
[32m[20230207 15:13:24 @agent_ppo2.py:152][0m 657408 total steps have happened
[32m[20230207 15:13:24 @agent_ppo2.py:128][0m #------------------------ Iteration 321 --------------------------#
[32m[20230207 15:13:25 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:13:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:25 @agent_ppo2.py:192][0m |          -0.0014 |          18.8551 |           0.1206 |
[32m[20230207 15:13:26 @agent_ppo2.py:192][0m |          -0.0052 |           2.1116 |           0.1204 |
[32m[20230207 15:13:26 @agent_ppo2.py:192][0m |          -0.0074 |           1.3492 |           0.1204 |
[32m[20230207 15:13:26 @agent_ppo2.py:192][0m |          -0.0077 |           1.1942 |           0.1204 |
[32m[20230207 15:13:26 @agent_ppo2.py:192][0m |          -0.0096 |           1.0671 |           0.1203 |
[32m[20230207 15:13:26 @agent_ppo2.py:192][0m |          -0.0089 |           1.0280 |           0.1204 |
[32m[20230207 15:13:26 @agent_ppo2.py:192][0m |          -0.0115 |           0.9691 |           0.1204 |
[32m[20230207 15:13:26 @agent_ppo2.py:192][0m |          -0.0121 |           0.9490 |           0.1203 |
[32m[20230207 15:13:26 @agent_ppo2.py:192][0m |          -0.0117 |           0.9258 |           0.1203 |
[32m[20230207 15:13:26 @agent_ppo2.py:192][0m |          -0.0124 |           0.9079 |           0.1203 |
[32m[20230207 15:13:26 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:13:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -83.40
[32m[20230207 15:13:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -10.47
[32m[20230207 15:13:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -15.92
[32m[20230207 15:13:27 @agent_ppo2.py:150][0m Total time:      12.76 min
[32m[20230207 15:13:27 @agent_ppo2.py:152][0m 659456 total steps have happened
[32m[20230207 15:13:27 @agent_ppo2.py:128][0m #------------------------ Iteration 322 --------------------------#
[32m[20230207 15:13:28 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:13:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:28 @agent_ppo2.py:192][0m |           0.0016 |           1.0162 |           0.1164 |
[32m[20230207 15:13:28 @agent_ppo2.py:192][0m |          -0.0167 |           0.8986 |           0.1162 |
[32m[20230207 15:13:28 @agent_ppo2.py:192][0m |          -0.0143 |           0.8362 |           0.1165 |
[32m[20230207 15:13:28 @agent_ppo2.py:192][0m |          -0.0153 |           0.8059 |           0.1161 |
[32m[20230207 15:13:28 @agent_ppo2.py:192][0m |          -0.0154 |           0.7916 |           0.1164 |
[32m[20230207 15:13:28 @agent_ppo2.py:192][0m |          -0.0154 |           0.7785 |           0.1163 |
[32m[20230207 15:13:28 @agent_ppo2.py:192][0m |          -0.0114 |           0.7661 |           0.1162 |
[32m[20230207 15:13:28 @agent_ppo2.py:192][0m |          -0.0151 |           0.7583 |           0.1164 |
[32m[20230207 15:13:28 @agent_ppo2.py:192][0m |          -0.0008 |           0.7565 |           0.1163 |
[32m[20230207 15:13:28 @agent_ppo2.py:192][0m |          -0.0126 |           0.7492 |           0.1165 |
[32m[20230207 15:13:28 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:13:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -40.01
[32m[20230207 15:13:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -33.25
[32m[20230207 15:13:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -21.99
[32m[20230207 15:13:29 @agent_ppo2.py:150][0m Total time:      12.80 min
[32m[20230207 15:13:29 @agent_ppo2.py:152][0m 661504 total steps have happened
[32m[20230207 15:13:29 @agent_ppo2.py:128][0m #------------------------ Iteration 323 --------------------------#
[32m[20230207 15:13:30 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:13:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:30 @agent_ppo2.py:192][0m |          -0.0066 |           2.8704 |           0.1178 |
[32m[20230207 15:13:30 @agent_ppo2.py:192][0m |          -0.0067 |           1.4897 |           0.1178 |
[32m[20230207 15:13:30 @agent_ppo2.py:192][0m |          -0.0006 |           1.3320 |           0.1178 |
[32m[20230207 15:13:30 @agent_ppo2.py:192][0m |           0.0209 |           1.3092 |           0.1177 |
[32m[20230207 15:13:31 @agent_ppo2.py:192][0m |           0.0076 |           1.2223 |           0.1173 |
[32m[20230207 15:13:31 @agent_ppo2.py:192][0m |           0.0020 |           1.1125 |           0.1174 |
[32m[20230207 15:13:31 @agent_ppo2.py:192][0m |          -0.0032 |           1.0710 |           0.1176 |
[32m[20230207 15:13:31 @agent_ppo2.py:192][0m |          -0.0093 |           1.0404 |           0.1175 |
[32m[20230207 15:13:31 @agent_ppo2.py:192][0m |           0.0026 |           1.0385 |           0.1176 |
[32m[20230207 15:13:31 @agent_ppo2.py:192][0m |          -0.0152 |           0.9963 |           0.1175 |
[32m[20230207 15:13:31 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:13:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: 13.14
[32m[20230207 15:13:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 45.33
[32m[20230207 15:13:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -86.38
[32m[20230207 15:13:31 @agent_ppo2.py:150][0m Total time:      12.84 min
[32m[20230207 15:13:31 @agent_ppo2.py:152][0m 663552 total steps have happened
[32m[20230207 15:13:31 @agent_ppo2.py:128][0m #------------------------ Iteration 324 --------------------------#
[32m[20230207 15:13:32 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:13:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:32 @agent_ppo2.py:192][0m |          -0.0068 |           2.2638 |           0.1164 |
[32m[20230207 15:13:32 @agent_ppo2.py:192][0m |           0.0057 |           1.3189 |           0.1163 |
[32m[20230207 15:13:33 @agent_ppo2.py:192][0m |          -0.0095 |           1.1325 |           0.1163 |
[32m[20230207 15:13:33 @agent_ppo2.py:192][0m |          -0.0110 |           1.0544 |           0.1162 |
[32m[20230207 15:13:33 @agent_ppo2.py:192][0m |          -0.0100 |           1.0164 |           0.1161 |
[32m[20230207 15:13:33 @agent_ppo2.py:192][0m |          -0.0091 |           0.9785 |           0.1161 |
[32m[20230207 15:13:33 @agent_ppo2.py:192][0m |          -0.0027 |           0.9616 |           0.1161 |
[32m[20230207 15:13:33 @agent_ppo2.py:192][0m |          -0.0068 |           0.9380 |           0.1161 |
[32m[20230207 15:13:33 @agent_ppo2.py:192][0m |          -0.0024 |           0.9224 |           0.1160 |
[32m[20230207 15:13:33 @agent_ppo2.py:192][0m |          -0.0238 |           0.9120 |           0.1160 |
[32m[20230207 15:13:33 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:13:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: 55.28
[32m[20230207 15:13:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 96.63
[32m[20230207 15:13:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -50.29
[32m[20230207 15:13:34 @agent_ppo2.py:150][0m Total time:      12.88 min
[32m[20230207 15:13:34 @agent_ppo2.py:152][0m 665600 total steps have happened
[32m[20230207 15:13:34 @agent_ppo2.py:128][0m #------------------------ Iteration 325 --------------------------#
[32m[20230207 15:13:35 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:13:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:35 @agent_ppo2.py:192][0m |           0.0010 |           8.8944 |           0.1206 |
[32m[20230207 15:13:35 @agent_ppo2.py:192][0m |          -0.0020 |           2.7417 |           0.1206 |
[32m[20230207 15:13:35 @agent_ppo2.py:192][0m |          -0.0056 |           1.9807 |           0.1205 |
[32m[20230207 15:13:35 @agent_ppo2.py:192][0m |          -0.0051 |           1.8184 |           0.1204 |
[32m[20230207 15:13:35 @agent_ppo2.py:192][0m |          -0.0030 |           1.6327 |           0.1203 |
[32m[20230207 15:13:35 @agent_ppo2.py:192][0m |          -0.0068 |           1.5251 |           0.1203 |
[32m[20230207 15:13:35 @agent_ppo2.py:192][0m |          -0.0080 |           1.4416 |           0.1202 |
[32m[20230207 15:13:36 @agent_ppo2.py:192][0m |          -0.0106 |           1.3855 |           0.1201 |
[32m[20230207 15:13:36 @agent_ppo2.py:192][0m |          -0.0121 |           1.3149 |           0.1201 |
[32m[20230207 15:13:36 @agent_ppo2.py:192][0m |          -0.0082 |           1.3240 |           0.1201 |
[32m[20230207 15:13:36 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:13:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -27.83
[32m[20230207 15:13:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 99.97
[32m[20230207 15:13:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -59.78
[32m[20230207 15:13:36 @agent_ppo2.py:150][0m Total time:      12.92 min
[32m[20230207 15:13:36 @agent_ppo2.py:152][0m 667648 total steps have happened
[32m[20230207 15:13:36 @agent_ppo2.py:128][0m #------------------------ Iteration 326 --------------------------#
[32m[20230207 15:13:37 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:13:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:37 @agent_ppo2.py:192][0m |          -0.0120 |           1.3298 |           0.1186 |
[32m[20230207 15:13:37 @agent_ppo2.py:192][0m |           0.0139 |           1.0709 |           0.1187 |
[32m[20230207 15:13:37 @agent_ppo2.py:192][0m |          -0.0023 |           1.0207 |           0.1187 |
[32m[20230207 15:13:37 @agent_ppo2.py:192][0m |          -0.0019 |           0.9834 |           0.1187 |
[32m[20230207 15:13:37 @agent_ppo2.py:192][0m |          -0.0239 |           0.9646 |           0.1187 |
[32m[20230207 15:13:38 @agent_ppo2.py:192][0m |          -0.0213 |           0.9425 |           0.1188 |
[32m[20230207 15:13:38 @agent_ppo2.py:192][0m |          -0.0202 |           0.9289 |           0.1188 |
[32m[20230207 15:13:38 @agent_ppo2.py:192][0m |          -0.0268 |           0.9170 |           0.1188 |
[32m[20230207 15:13:38 @agent_ppo2.py:192][0m |          -0.0059 |           0.9257 |           0.1188 |
[32m[20230207 15:13:38 @agent_ppo2.py:192][0m |          -0.0201 |           0.9140 |           0.1188 |
[32m[20230207 15:13:38 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:13:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -13.84
[32m[20230207 15:13:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 35.83
[32m[20230207 15:13:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -45.15
[32m[20230207 15:13:39 @agent_ppo2.py:150][0m Total time:      12.96 min
[32m[20230207 15:13:39 @agent_ppo2.py:152][0m 669696 total steps have happened
[32m[20230207 15:13:39 @agent_ppo2.py:128][0m #------------------------ Iteration 327 --------------------------#
[32m[20230207 15:13:39 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:13:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:40 @agent_ppo2.py:192][0m |          -0.0072 |           3.5584 |           0.1208 |
[32m[20230207 15:13:40 @agent_ppo2.py:192][0m |          -0.0046 |           2.1018 |           0.1209 |
[32m[20230207 15:13:40 @agent_ppo2.py:192][0m |          -0.0209 |           1.9712 |           0.1208 |
[32m[20230207 15:13:40 @agent_ppo2.py:192][0m |          -0.0002 |           1.9024 |           0.1209 |
[32m[20230207 15:13:40 @agent_ppo2.py:192][0m |          -0.0077 |           1.8150 |           0.1209 |
[32m[20230207 15:13:40 @agent_ppo2.py:192][0m |          -0.0062 |           1.7706 |           0.1209 |
[32m[20230207 15:13:40 @agent_ppo2.py:192][0m |          -0.0015 |           1.7369 |           0.1210 |
[32m[20230207 15:13:40 @agent_ppo2.py:192][0m |           0.0011 |           1.7163 |           0.1211 |
[32m[20230207 15:13:40 @agent_ppo2.py:192][0m |           0.0006 |           1.6659 |           0.1211 |
[32m[20230207 15:13:40 @agent_ppo2.py:192][0m |          -0.0062 |           1.6616 |           0.1211 |
[32m[20230207 15:13:40 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:13:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -37.55
[32m[20230207 15:13:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -32.67
[32m[20230207 15:13:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -50.27
[32m[20230207 15:13:41 @agent_ppo2.py:150][0m Total time:      12.99 min
[32m[20230207 15:13:41 @agent_ppo2.py:152][0m 671744 total steps have happened
[32m[20230207 15:13:41 @agent_ppo2.py:128][0m #------------------------ Iteration 328 --------------------------#
[32m[20230207 15:13:42 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:13:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:42 @agent_ppo2.py:192][0m |           0.0015 |           1.7631 |           0.1189 |
[32m[20230207 15:13:42 @agent_ppo2.py:192][0m |           0.0109 |           1.1517 |           0.1189 |
[32m[20230207 15:13:42 @agent_ppo2.py:192][0m |           0.0091 |           1.0755 |           0.1189 |
[32m[20230207 15:13:42 @agent_ppo2.py:192][0m |          -0.0066 |           1.0346 |           0.1188 |
[32m[20230207 15:13:42 @agent_ppo2.py:192][0m |          -0.0122 |           1.0170 |           0.1190 |
[32m[20230207 15:13:42 @agent_ppo2.py:192][0m |          -0.0208 |           0.9808 |           0.1190 |
[32m[20230207 15:13:42 @agent_ppo2.py:192][0m |          -0.0038 |           0.9733 |           0.1190 |
[32m[20230207 15:13:42 @agent_ppo2.py:192][0m |          -0.0543 |           0.9760 |           0.1190 |
[32m[20230207 15:13:42 @agent_ppo2.py:192][0m |           0.0043 |           0.9373 |           0.1191 |
[32m[20230207 15:13:42 @agent_ppo2.py:192][0m |           0.0237 |           0.9888 |           0.1192 |
[32m[20230207 15:13:42 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:13:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: 74.20
[32m[20230207 15:13:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 87.75
[32m[20230207 15:13:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -7.23
[32m[20230207 15:13:43 @agent_ppo2.py:150][0m Total time:      13.03 min
[32m[20230207 15:13:43 @agent_ppo2.py:152][0m 673792 total steps have happened
[32m[20230207 15:13:43 @agent_ppo2.py:128][0m #------------------------ Iteration 329 --------------------------#
[32m[20230207 15:13:44 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:13:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:44 @agent_ppo2.py:192][0m |          -0.0023 |           1.0616 |           0.1236 |
[32m[20230207 15:13:44 @agent_ppo2.py:192][0m |          -0.0204 |           0.8330 |           0.1236 |
[32m[20230207 15:13:44 @agent_ppo2.py:192][0m |          -0.0058 |           0.7896 |           0.1236 |
[32m[20230207 15:13:44 @agent_ppo2.py:192][0m |           0.0020 |           0.7665 |           0.1235 |
[32m[20230207 15:13:44 @agent_ppo2.py:192][0m |          -0.0082 |           0.7448 |           0.1234 |
[32m[20230207 15:13:44 @agent_ppo2.py:192][0m |           0.0061 |           0.7365 |           0.1235 |
[32m[20230207 15:13:45 @agent_ppo2.py:192][0m |          -0.0140 |           0.7049 |           0.1236 |
[32m[20230207 15:13:45 @agent_ppo2.py:192][0m |          -0.0121 |           0.7003 |           0.1236 |
[32m[20230207 15:13:45 @agent_ppo2.py:192][0m |          -0.0364 |           0.6864 |           0.1236 |
[32m[20230207 15:13:45 @agent_ppo2.py:192][0m |          -0.0068 |           0.6820 |           0.1236 |
[32m[20230207 15:13:45 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:13:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -25.78
[32m[20230207 15:13:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 22.34
[32m[20230207 15:13:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -10.94
[32m[20230207 15:13:46 @agent_ppo2.py:150][0m Total time:      13.07 min
[32m[20230207 15:13:46 @agent_ppo2.py:152][0m 675840 total steps have happened
[32m[20230207 15:13:46 @agent_ppo2.py:128][0m #------------------------ Iteration 330 --------------------------#
[32m[20230207 15:13:46 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:13:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:47 @agent_ppo2.py:192][0m |           0.0061 |           1.8699 |           0.1204 |
[32m[20230207 15:13:47 @agent_ppo2.py:192][0m |          -0.0003 |           1.4865 |           0.1204 |
[32m[20230207 15:13:47 @agent_ppo2.py:192][0m |          -0.0070 |           1.3632 |           0.1202 |
[32m[20230207 15:13:47 @agent_ppo2.py:192][0m |           0.0065 |           1.2992 |           0.1203 |
[32m[20230207 15:13:47 @agent_ppo2.py:192][0m |          -0.0234 |           1.2521 |           0.1203 |
[32m[20230207 15:13:47 @agent_ppo2.py:192][0m |          -0.0069 |           1.2018 |           0.1203 |
[32m[20230207 15:13:47 @agent_ppo2.py:192][0m |          -0.0257 |           1.1596 |           0.1201 |
[32m[20230207 15:13:47 @agent_ppo2.py:192][0m |          -0.0010 |           1.1272 |           0.1202 |
[32m[20230207 15:13:47 @agent_ppo2.py:192][0m |          -0.0049 |           1.0958 |           0.1201 |
[32m[20230207 15:13:47 @agent_ppo2.py:192][0m |          -0.0100 |           1.0594 |           0.1199 |
[32m[20230207 15:13:47 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:13:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: 38.90
[32m[20230207 15:13:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 60.88
[32m[20230207 15:13:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 5.03
[32m[20230207 15:13:48 @agent_ppo2.py:150][0m Total time:      13.11 min
[32m[20230207 15:13:48 @agent_ppo2.py:152][0m 677888 total steps have happened
[32m[20230207 15:13:48 @agent_ppo2.py:128][0m #------------------------ Iteration 331 --------------------------#
[32m[20230207 15:13:49 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:13:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:49 @agent_ppo2.py:192][0m |          -0.0260 |           1.5775 |           0.1190 |
[32m[20230207 15:13:49 @agent_ppo2.py:192][0m |           0.0027 |           1.2060 |           0.1190 |
[32m[20230207 15:13:49 @agent_ppo2.py:192][0m |           0.0051 |           1.1255 |           0.1191 |
[32m[20230207 15:13:49 @agent_ppo2.py:192][0m |           0.0183 |           1.0636 |           0.1190 |
[32m[20230207 15:13:49 @agent_ppo2.py:192][0m |          -0.0130 |           1.0245 |           0.1191 |
[32m[20230207 15:13:49 @agent_ppo2.py:192][0m |          -0.0094 |           0.9894 |           0.1191 |
[32m[20230207 15:13:49 @agent_ppo2.py:192][0m |          -0.0178 |           0.9634 |           0.1191 |
[32m[20230207 15:13:49 @agent_ppo2.py:192][0m |          -0.0014 |           0.9405 |           0.1191 |
[32m[20230207 15:13:50 @agent_ppo2.py:192][0m |           0.0014 |           0.9136 |           0.1191 |
[32m[20230207 15:13:50 @agent_ppo2.py:192][0m |          -0.0172 |           0.9005 |           0.1191 |
[32m[20230207 15:13:50 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:13:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: -11.19
[32m[20230207 15:13:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -0.23
[32m[20230207 15:13:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -44.16
[32m[20230207 15:13:50 @agent_ppo2.py:150][0m Total time:      13.15 min
[32m[20230207 15:13:50 @agent_ppo2.py:152][0m 679936 total steps have happened
[32m[20230207 15:13:50 @agent_ppo2.py:128][0m #------------------------ Iteration 332 --------------------------#
[32m[20230207 15:13:51 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:13:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:51 @agent_ppo2.py:192][0m |           0.0016 |           3.7701 |           0.1194 |
[32m[20230207 15:13:51 @agent_ppo2.py:192][0m |          -0.0041 |           1.4943 |           0.1193 |
[32m[20230207 15:13:52 @agent_ppo2.py:192][0m |          -0.0073 |           1.1003 |           0.1192 |
[32m[20230207 15:13:52 @agent_ppo2.py:192][0m |          -0.0057 |           0.9818 |           0.1192 |
[32m[20230207 15:13:52 @agent_ppo2.py:192][0m |          -0.0099 |           0.9351 |           0.1191 |
[32m[20230207 15:13:52 @agent_ppo2.py:192][0m |          -0.0067 |           0.8755 |           0.1191 |
[32m[20230207 15:13:52 @agent_ppo2.py:192][0m |          -0.0120 |           0.8248 |           0.1190 |
[32m[20230207 15:13:52 @agent_ppo2.py:192][0m |          -0.0098 |           0.8046 |           0.1189 |
[32m[20230207 15:13:52 @agent_ppo2.py:192][0m |          -0.0131 |           0.7813 |           0.1189 |
[32m[20230207 15:13:52 @agent_ppo2.py:192][0m |          -0.0131 |           0.7734 |           0.1189 |
[32m[20230207 15:13:52 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:13:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -54.81
[32m[20230207 15:13:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 7.01
[32m[20230207 15:13:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -16.48
[32m[20230207 15:13:53 @agent_ppo2.py:150][0m Total time:      13.19 min
[32m[20230207 15:13:53 @agent_ppo2.py:152][0m 681984 total steps have happened
[32m[20230207 15:13:53 @agent_ppo2.py:128][0m #------------------------ Iteration 333 --------------------------#
[32m[20230207 15:13:54 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:13:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:54 @agent_ppo2.py:192][0m |          -0.0023 |          10.9031 |           0.1192 |
[32m[20230207 15:13:54 @agent_ppo2.py:192][0m |          -0.0072 |           3.5618 |           0.1192 |
[32m[20230207 15:13:54 @agent_ppo2.py:192][0m |          -0.0071 |           2.8269 |           0.1190 |
[32m[20230207 15:13:54 @agent_ppo2.py:192][0m |          -0.0062 |           2.3430 |           0.1191 |
[32m[20230207 15:13:54 @agent_ppo2.py:192][0m |          -0.0095 |           2.3750 |           0.1190 |
[32m[20230207 15:13:54 @agent_ppo2.py:192][0m |          -0.0114 |           2.1008 |           0.1191 |
[32m[20230207 15:13:54 @agent_ppo2.py:192][0m |          -0.0091 |           1.9470 |           0.1190 |
[32m[20230207 15:13:54 @agent_ppo2.py:192][0m |          -0.0100 |           1.9391 |           0.1191 |
[32m[20230207 15:13:54 @agent_ppo2.py:192][0m |          -0.0127 |           1.8415 |           0.1190 |
[32m[20230207 15:13:55 @agent_ppo2.py:192][0m |          -0.0095 |           1.7591 |           0.1190 |
[32m[20230207 15:13:55 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:13:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: -63.44
[32m[20230207 15:13:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -46.86
[32m[20230207 15:13:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 21.77
[32m[20230207 15:13:55 @agent_ppo2.py:150][0m Total time:      13.23 min
[32m[20230207 15:13:55 @agent_ppo2.py:152][0m 684032 total steps have happened
[32m[20230207 15:13:55 @agent_ppo2.py:128][0m #------------------------ Iteration 334 --------------------------#
[32m[20230207 15:13:56 @agent_ppo2.py:134][0m Sampling time: 0.96 s by 1 slaves
[32m[20230207 15:13:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:56 @agent_ppo2.py:192][0m |          -0.0007 |           5.7680 |           0.1225 |
[32m[20230207 15:13:56 @agent_ppo2.py:192][0m |          -0.0036 |           1.9396 |           0.1225 |
[32m[20230207 15:13:57 @agent_ppo2.py:192][0m |          -0.0073 |           1.3940 |           0.1224 |
[32m[20230207 15:13:57 @agent_ppo2.py:192][0m |          -0.0086 |           1.2593 |           0.1222 |
[32m[20230207 15:13:57 @agent_ppo2.py:192][0m |          -0.0092 |           1.1898 |           0.1222 |
[32m[20230207 15:13:57 @agent_ppo2.py:192][0m |          -0.0097 |           1.1364 |           0.1221 |
[32m[20230207 15:13:57 @agent_ppo2.py:192][0m |          -0.0087 |           1.0978 |           0.1221 |
[32m[20230207 15:13:57 @agent_ppo2.py:192][0m |          -0.0086 |           1.0667 |           0.1221 |
[32m[20230207 15:13:57 @agent_ppo2.py:192][0m |          -0.0085 |           1.0514 |           0.1220 |
[32m[20230207 15:13:57 @agent_ppo2.py:192][0m |          -0.0119 |           1.0225 |           0.1220 |
[32m[20230207 15:13:57 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:13:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: 12.01
[32m[20230207 15:13:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 80.83
[32m[20230207 15:13:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -27.48
[32m[20230207 15:13:58 @agent_ppo2.py:150][0m Total time:      13.28 min
[32m[20230207 15:13:58 @agent_ppo2.py:152][0m 686080 total steps have happened
[32m[20230207 15:13:58 @agent_ppo2.py:128][0m #------------------------ Iteration 335 --------------------------#
[32m[20230207 15:13:59 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:13:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:13:59 @agent_ppo2.py:192][0m |           0.0003 |           1.2200 |           0.1213 |
[32m[20230207 15:13:59 @agent_ppo2.py:192][0m |          -0.0119 |           0.9837 |           0.1211 |
[32m[20230207 15:13:59 @agent_ppo2.py:192][0m |          -0.0136 |           0.9401 |           0.1210 |
[32m[20230207 15:13:59 @agent_ppo2.py:192][0m |          -0.0146 |           0.9067 |           0.1210 |
[32m[20230207 15:13:59 @agent_ppo2.py:192][0m |          -0.0162 |           0.8895 |           0.1211 |
[32m[20230207 15:13:59 @agent_ppo2.py:192][0m |          -0.0084 |           0.8715 |           0.1213 |
[32m[20230207 15:13:59 @agent_ppo2.py:192][0m |          -0.0088 |           0.8553 |           0.1212 |
[32m[20230207 15:13:59 @agent_ppo2.py:192][0m |          -0.0318 |           0.8395 |           0.1213 |
[32m[20230207 15:13:59 @agent_ppo2.py:192][0m |          -0.0011 |           0.8257 |           0.1213 |
[32m[20230207 15:13:59 @agent_ppo2.py:192][0m |          -0.0153 |           0.8283 |           0.1215 |
[32m[20230207 15:13:59 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:14:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: 43.36
[32m[20230207 15:14:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 54.67
[32m[20230207 15:14:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -15.78
[32m[20230207 15:14:00 @agent_ppo2.py:150][0m Total time:      13.32 min
[32m[20230207 15:14:00 @agent_ppo2.py:152][0m 688128 total steps have happened
[32m[20230207 15:14:00 @agent_ppo2.py:128][0m #------------------------ Iteration 336 --------------------------#
[32m[20230207 15:14:01 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:14:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:01 @agent_ppo2.py:192][0m |          -0.0002 |          36.7879 |           0.1258 |
[32m[20230207 15:14:01 @agent_ppo2.py:192][0m |          -0.0047 |          17.7612 |           0.1257 |
[32m[20230207 15:14:01 @agent_ppo2.py:192][0m |          -0.0036 |          12.3253 |           0.1256 |
[32m[20230207 15:14:01 @agent_ppo2.py:192][0m |          -0.0089 |           9.6503 |           0.1255 |
[32m[20230207 15:14:01 @agent_ppo2.py:192][0m |          -0.0098 |           7.9273 |           0.1254 |
[32m[20230207 15:14:01 @agent_ppo2.py:192][0m |          -0.0099 |           5.9725 |           0.1254 |
[32m[20230207 15:14:01 @agent_ppo2.py:192][0m |          -0.0104 |           5.3025 |           0.1253 |
[32m[20230207 15:14:01 @agent_ppo2.py:192][0m |          -0.0100 |           5.1198 |           0.1252 |
[32m[20230207 15:14:01 @agent_ppo2.py:192][0m |          -0.0112 |           5.0419 |           0.1252 |
[32m[20230207 15:14:01 @agent_ppo2.py:192][0m |          -0.0091 |           3.6291 |           0.1251 |
[32m[20230207 15:14:01 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:14:02 @agent_ppo2.py:145][0m Average TRAINING episode reward: -81.76
[32m[20230207 15:14:02 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -3.76
[32m[20230207 15:14:02 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -24.41
[32m[20230207 15:14:02 @agent_ppo2.py:150][0m Total time:      13.35 min
[32m[20230207 15:14:02 @agent_ppo2.py:152][0m 690176 total steps have happened
[32m[20230207 15:14:02 @agent_ppo2.py:128][0m #------------------------ Iteration 337 --------------------------#
[32m[20230207 15:14:03 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:14:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:03 @agent_ppo2.py:192][0m |          -0.0198 |           1.5489 |           0.1202 |
[32m[20230207 15:14:03 @agent_ppo2.py:192][0m |          -0.0180 |           1.4307 |           0.1200 |
[32m[20230207 15:14:03 @agent_ppo2.py:192][0m |          -0.0046 |           1.3630 |           0.1202 |
[32m[20230207 15:14:03 @agent_ppo2.py:192][0m |           0.0063 |           1.3079 |           0.1202 |
[32m[20230207 15:14:03 @agent_ppo2.py:192][0m |          -0.0038 |           1.2673 |           0.1203 |
[32m[20230207 15:14:03 @agent_ppo2.py:192][0m |          -0.0153 |           1.2363 |           0.1203 |
[32m[20230207 15:14:04 @agent_ppo2.py:192][0m |          -0.0051 |           1.2275 |           0.1204 |
[32m[20230207 15:14:04 @agent_ppo2.py:192][0m |          -0.0170 |           1.1976 |           0.1205 |
[32m[20230207 15:14:04 @agent_ppo2.py:192][0m |          -0.0101 |           1.1846 |           0.1205 |
[32m[20230207 15:14:04 @agent_ppo2.py:192][0m |          -0.0162 |           1.1686 |           0.1205 |
[32m[20230207 15:14:04 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:14:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -14.59
[32m[20230207 15:14:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -4.21
[32m[20230207 15:14:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -60.18
[32m[20230207 15:14:05 @agent_ppo2.py:150][0m Total time:      13.39 min
[32m[20230207 15:14:05 @agent_ppo2.py:152][0m 692224 total steps have happened
[32m[20230207 15:14:05 @agent_ppo2.py:128][0m #------------------------ Iteration 338 --------------------------#
[32m[20230207 15:14:05 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:14:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:06 @agent_ppo2.py:192][0m |           0.0022 |           7.4169 |           0.1233 |
[32m[20230207 15:14:06 @agent_ppo2.py:192][0m |          -0.0034 |           1.7920 |           0.1231 |
[32m[20230207 15:14:06 @agent_ppo2.py:192][0m |          -0.0046 |           1.3577 |           0.1230 |
[32m[20230207 15:14:06 @agent_ppo2.py:192][0m |          -0.0059 |           1.2394 |           0.1229 |
[32m[20230207 15:14:06 @agent_ppo2.py:192][0m |          -0.0065 |           1.1850 |           0.1228 |
[32m[20230207 15:14:06 @agent_ppo2.py:192][0m |          -0.0076 |           1.1533 |           0.1227 |
[32m[20230207 15:14:06 @agent_ppo2.py:192][0m |          -0.0078 |           1.1200 |           0.1227 |
[32m[20230207 15:14:06 @agent_ppo2.py:192][0m |          -0.0067 |           1.1017 |           0.1227 |
[32m[20230207 15:14:06 @agent_ppo2.py:192][0m |          -0.0083 |           1.0827 |           0.1226 |
[32m[20230207 15:14:06 @agent_ppo2.py:192][0m |          -0.0090 |           1.0701 |           0.1225 |
[32m[20230207 15:14:06 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:14:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -19.37
[32m[20230207 15:14:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 73.62
[32m[20230207 15:14:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -4.90
[32m[20230207 15:14:07 @agent_ppo2.py:150][0m Total time:      13.43 min
[32m[20230207 15:14:07 @agent_ppo2.py:152][0m 694272 total steps have happened
[32m[20230207 15:14:07 @agent_ppo2.py:128][0m #------------------------ Iteration 339 --------------------------#
[32m[20230207 15:14:08 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:14:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:08 @agent_ppo2.py:192][0m |          -0.0109 |           1.3739 |           0.1221 |
[32m[20230207 15:14:08 @agent_ppo2.py:192][0m |           0.0033 |           1.2678 |           0.1219 |
[32m[20230207 15:14:08 @agent_ppo2.py:192][0m |           0.0207 |           1.2406 |           0.1221 |
[32m[20230207 15:14:08 @agent_ppo2.py:192][0m |          -0.0216 |           1.2107 |           0.1220 |
[32m[20230207 15:14:08 @agent_ppo2.py:192][0m |          -0.0090 |           1.1727 |           0.1220 |
[32m[20230207 15:14:08 @agent_ppo2.py:192][0m |          -0.0098 |           1.1574 |           0.1219 |
[32m[20230207 15:14:09 @agent_ppo2.py:192][0m |          -0.0113 |           1.1342 |           0.1220 |
[32m[20230207 15:14:09 @agent_ppo2.py:192][0m |          -0.0049 |           1.1313 |           0.1220 |
[32m[20230207 15:14:09 @agent_ppo2.py:192][0m |          -0.0048 |           1.1188 |           0.1220 |
[32m[20230207 15:14:09 @agent_ppo2.py:192][0m |          -0.0101 |           1.1050 |           0.1220 |
[32m[20230207 15:14:09 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:14:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: 11.73
[32m[20230207 15:14:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 15.10
[32m[20230207 15:14:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 32.18
[32m[20230207 15:14:10 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 32.18
[32m[20230207 15:14:10 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 32.18
[32m[20230207 15:14:10 @agent_ppo2.py:150][0m Total time:      13.47 min
[32m[20230207 15:14:10 @agent_ppo2.py:152][0m 696320 total steps have happened
[32m[20230207 15:14:10 @agent_ppo2.py:128][0m #------------------------ Iteration 340 --------------------------#
[32m[20230207 15:14:10 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:14:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:11 @agent_ppo2.py:192][0m |          -0.0007 |           3.8993 |           0.1212 |
[32m[20230207 15:14:11 @agent_ppo2.py:192][0m |          -0.0067 |           1.7981 |           0.1210 |
[32m[20230207 15:14:11 @agent_ppo2.py:192][0m |          -0.0078 |           1.4482 |           0.1209 |
[32m[20230207 15:14:11 @agent_ppo2.py:192][0m |          -0.0101 |           1.2918 |           0.1208 |
[32m[20230207 15:14:11 @agent_ppo2.py:192][0m |          -0.0107 |           1.1749 |           0.1207 |
[32m[20230207 15:14:11 @agent_ppo2.py:192][0m |          -0.0097 |           1.1192 |           0.1207 |
[32m[20230207 15:14:11 @agent_ppo2.py:192][0m |          -0.0140 |           1.0571 |           0.1207 |
[32m[20230207 15:14:11 @agent_ppo2.py:192][0m |          -0.0124 |           1.0329 |           0.1206 |
[32m[20230207 15:14:11 @agent_ppo2.py:192][0m |          -0.0117 |           1.0077 |           0.1206 |
[32m[20230207 15:14:11 @agent_ppo2.py:192][0m |          -0.0129 |           1.0079 |           0.1206 |
[32m[20230207 15:14:11 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:14:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: 5.13
[32m[20230207 15:14:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 71.97
[32m[20230207 15:14:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 52.92
[32m[20230207 15:14:12 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 52.92
[32m[20230207 15:14:12 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 52.92
[32m[20230207 15:14:12 @agent_ppo2.py:150][0m Total time:      13.51 min
[32m[20230207 15:14:12 @agent_ppo2.py:152][0m 698368 total steps have happened
[32m[20230207 15:14:12 @agent_ppo2.py:128][0m #------------------------ Iteration 341 --------------------------#
[32m[20230207 15:14:13 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:14:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:13 @agent_ppo2.py:192][0m |           0.0060 |           3.8932 |           0.1242 |
[32m[20230207 15:14:13 @agent_ppo2.py:192][0m |          -0.0035 |           1.6043 |           0.1240 |
[32m[20230207 15:14:13 @agent_ppo2.py:192][0m |          -0.0041 |           1.3265 |           0.1240 |
[32m[20230207 15:14:13 @agent_ppo2.py:192][0m |          -0.0043 |           1.2016 |           0.1239 |
[32m[20230207 15:14:13 @agent_ppo2.py:192][0m |          -0.0056 |           1.1169 |           0.1239 |
[32m[20230207 15:14:13 @agent_ppo2.py:192][0m |          -0.0058 |           1.0824 |           0.1239 |
[32m[20230207 15:14:14 @agent_ppo2.py:192][0m |          -0.0055 |           1.0273 |           0.1240 |
[32m[20230207 15:14:14 @agent_ppo2.py:192][0m |          -0.0057 |           1.0072 |           0.1239 |
[32m[20230207 15:14:14 @agent_ppo2.py:192][0m |          -0.0007 |           0.9855 |           0.1239 |
[32m[20230207 15:14:14 @agent_ppo2.py:192][0m |          -0.0055 |           0.9545 |           0.1238 |
[32m[20230207 15:14:14 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:14:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: -44.29
[32m[20230207 15:14:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 36.43
[32m[20230207 15:14:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -52.08
[32m[20230207 15:14:15 @agent_ppo2.py:150][0m Total time:      13.56 min
[32m[20230207 15:14:15 @agent_ppo2.py:152][0m 700416 total steps have happened
[32m[20230207 15:14:15 @agent_ppo2.py:128][0m #------------------------ Iteration 342 --------------------------#
[32m[20230207 15:14:16 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:14:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:16 @agent_ppo2.py:192][0m |          -0.0015 |          15.4176 |           0.1236 |
[32m[20230207 15:14:16 @agent_ppo2.py:192][0m |          -0.0046 |           7.7563 |           0.1235 |
[32m[20230207 15:14:16 @agent_ppo2.py:192][0m |          -0.0051 |           5.1805 |           0.1234 |
[32m[20230207 15:14:16 @agent_ppo2.py:192][0m |          -0.0087 |           3.8775 |           0.1234 |
[32m[20230207 15:14:16 @agent_ppo2.py:192][0m |          -0.0091 |           3.1409 |           0.1234 |
[32m[20230207 15:14:16 @agent_ppo2.py:192][0m |          -0.0101 |           2.7972 |           0.1233 |
[32m[20230207 15:14:16 @agent_ppo2.py:192][0m |          -0.0091 |           2.4201 |           0.1233 |
[32m[20230207 15:14:16 @agent_ppo2.py:192][0m |          -0.0119 |           2.1810 |           0.1232 |
[32m[20230207 15:14:16 @agent_ppo2.py:192][0m |          -0.0097 |           2.0442 |           0.1233 |
[32m[20230207 15:14:16 @agent_ppo2.py:192][0m |          -0.0130 |           1.9906 |           0.1232 |
[32m[20230207 15:14:16 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:14:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: 13.28
[32m[20230207 15:14:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 143.98
[32m[20230207 15:14:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -14.57
[32m[20230207 15:14:17 @agent_ppo2.py:150][0m Total time:      13.60 min
[32m[20230207 15:14:17 @agent_ppo2.py:152][0m 702464 total steps have happened
[32m[20230207 15:14:17 @agent_ppo2.py:128][0m #------------------------ Iteration 343 --------------------------#
[32m[20230207 15:14:18 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 15:14:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:18 @agent_ppo2.py:192][0m |          -0.0023 |          23.3218 |           0.1230 |
[32m[20230207 15:14:18 @agent_ppo2.py:192][0m |          -0.0046 |           7.8697 |           0.1229 |
[32m[20230207 15:14:18 @agent_ppo2.py:192][0m |          -0.0062 |           5.7575 |           0.1230 |
[32m[20230207 15:14:18 @agent_ppo2.py:192][0m |          -0.0081 |           4.7991 |           0.1230 |
[32m[20230207 15:14:18 @agent_ppo2.py:192][0m |          -0.0095 |           4.2518 |           0.1230 |
[32m[20230207 15:14:18 @agent_ppo2.py:192][0m |          -0.0060 |           3.5948 |           0.1231 |
[32m[20230207 15:14:18 @agent_ppo2.py:192][0m |          -0.0073 |           3.3088 |           0.1231 |
[32m[20230207 15:14:19 @agent_ppo2.py:192][0m |          -0.0107 |           3.0666 |           0.1232 |
[32m[20230207 15:14:19 @agent_ppo2.py:192][0m |          -0.0119 |           2.9586 |           0.1232 |
[32m[20230207 15:14:19 @agent_ppo2.py:192][0m |          -0.0146 |           2.7191 |           0.1232 |
[32m[20230207 15:14:19 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230207 15:14:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -65.82
[32m[20230207 15:14:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 11.72
[32m[20230207 15:14:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -43.47
[32m[20230207 15:14:20 @agent_ppo2.py:150][0m Total time:      13.64 min
[32m[20230207 15:14:20 @agent_ppo2.py:152][0m 704512 total steps have happened
[32m[20230207 15:14:20 @agent_ppo2.py:128][0m #------------------------ Iteration 344 --------------------------#
[32m[20230207 15:14:20 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:14:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:21 @agent_ppo2.py:192][0m |          -0.0032 |           2.7286 |           0.1249 |
[32m[20230207 15:14:21 @agent_ppo2.py:192][0m |          -0.0227 |           1.8434 |           0.1248 |
[32m[20230207 15:14:21 @agent_ppo2.py:192][0m |          -0.0264 |           1.6501 |           0.1248 |
[32m[20230207 15:14:21 @agent_ppo2.py:192][0m |           0.0029 |           1.5466 |           0.1246 |
[32m[20230207 15:14:21 @agent_ppo2.py:192][0m |          -0.0064 |           1.4196 |           0.1246 |
[32m[20230207 15:14:21 @agent_ppo2.py:192][0m |          -0.0116 |           1.3692 |           0.1246 |
[32m[20230207 15:14:21 @agent_ppo2.py:192][0m |          -0.0497 |           1.2983 |           0.1245 |
[32m[20230207 15:14:21 @agent_ppo2.py:192][0m |          -0.0189 |           1.2708 |           0.1244 |
[32m[20230207 15:14:21 @agent_ppo2.py:192][0m |          -0.0205 |           1.2101 |           0.1243 |
[32m[20230207 15:14:21 @agent_ppo2.py:192][0m |          -0.0039 |           1.1818 |           0.1244 |
[32m[20230207 15:14:21 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:14:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: 25.21
[32m[20230207 15:14:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 57.47
[32m[20230207 15:14:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 13.24
[32m[20230207 15:14:22 @agent_ppo2.py:150][0m Total time:      13.68 min
[32m[20230207 15:14:22 @agent_ppo2.py:152][0m 706560 total steps have happened
[32m[20230207 15:14:22 @agent_ppo2.py:128][0m #------------------------ Iteration 345 --------------------------#
[32m[20230207 15:14:23 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:14:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:23 @agent_ppo2.py:192][0m |          -0.0004 |          15.0981 |           0.1248 |
[32m[20230207 15:14:23 @agent_ppo2.py:192][0m |          -0.0034 |           2.6823 |           0.1245 |
[32m[20230207 15:14:23 @agent_ppo2.py:192][0m |          -0.0047 |           1.8317 |           0.1246 |
[32m[20230207 15:14:23 @agent_ppo2.py:192][0m |          -0.0055 |           1.5963 |           0.1246 |
[32m[20230207 15:14:23 @agent_ppo2.py:192][0m |          -0.0060 |           1.5220 |           0.1245 |
[32m[20230207 15:14:23 @agent_ppo2.py:192][0m |          -0.0073 |           1.4179 |           0.1245 |
[32m[20230207 15:14:23 @agent_ppo2.py:192][0m |          -0.0026 |           1.3836 |           0.1245 |
[32m[20230207 15:14:24 @agent_ppo2.py:192][0m |          -0.0073 |           1.3366 |           0.1244 |
[32m[20230207 15:14:24 @agent_ppo2.py:192][0m |          -0.0080 |           1.2989 |           0.1244 |
[32m[20230207 15:14:24 @agent_ppo2.py:192][0m |          -0.0087 |           1.2738 |           0.1246 |
[32m[20230207 15:14:24 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:14:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -19.38
[32m[20230207 15:14:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 96.46
[32m[20230207 15:14:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -49.56
[32m[20230207 15:14:25 @agent_ppo2.py:150][0m Total time:      13.72 min
[32m[20230207 15:14:25 @agent_ppo2.py:152][0m 708608 total steps have happened
[32m[20230207 15:14:25 @agent_ppo2.py:128][0m #------------------------ Iteration 346 --------------------------#
[32m[20230207 15:14:25 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:14:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:25 @agent_ppo2.py:192][0m |          -0.0017 |           1.3440 |           0.1216 |
[32m[20230207 15:14:26 @agent_ppo2.py:192][0m |          -0.0204 |           1.1483 |           0.1215 |
[32m[20230207 15:14:26 @agent_ppo2.py:192][0m |          -0.0108 |           1.0865 |           0.1216 |
[32m[20230207 15:14:26 @agent_ppo2.py:192][0m |           0.0069 |           1.0553 |           0.1216 |
[32m[20230207 15:14:26 @agent_ppo2.py:192][0m |          -0.0142 |           1.0350 |           0.1215 |
[32m[20230207 15:14:26 @agent_ppo2.py:192][0m |          -0.0091 |           0.9985 |           0.1215 |
[32m[20230207 15:14:26 @agent_ppo2.py:192][0m |          -0.0225 |           0.9804 |           0.1214 |
[32m[20230207 15:14:26 @agent_ppo2.py:192][0m |          -0.0183 |           0.9842 |           0.1214 |
[32m[20230207 15:14:26 @agent_ppo2.py:192][0m |          -0.0042 |           0.9507 |           0.1213 |
[32m[20230207 15:14:26 @agent_ppo2.py:192][0m |           0.0013 |           0.9380 |           0.1213 |
[32m[20230207 15:14:26 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:14:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -17.15
[32m[20230207 15:14:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 17.26
[32m[20230207 15:14:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 0.71
[32m[20230207 15:14:27 @agent_ppo2.py:150][0m Total time:      13.76 min
[32m[20230207 15:14:27 @agent_ppo2.py:152][0m 710656 total steps have happened
[32m[20230207 15:14:27 @agent_ppo2.py:128][0m #------------------------ Iteration 347 --------------------------#
[32m[20230207 15:14:28 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:14:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:28 @agent_ppo2.py:192][0m |          -0.0155 |           1.9311 |           0.1246 |
[32m[20230207 15:14:28 @agent_ppo2.py:192][0m |          -0.0058 |           1.2834 |           0.1244 |
[32m[20230207 15:14:28 @agent_ppo2.py:192][0m |          -0.0005 |           1.1314 |           0.1242 |
[32m[20230207 15:14:28 @agent_ppo2.py:192][0m |          -0.0009 |           1.0730 |           0.1244 |
[32m[20230207 15:14:28 @agent_ppo2.py:192][0m |          -0.0359 |           1.0150 |           0.1244 |
[32m[20230207 15:14:28 @agent_ppo2.py:192][0m |          -0.0044 |           0.9860 |           0.1244 |
[32m[20230207 15:14:28 @agent_ppo2.py:192][0m |          -0.0212 |           0.9597 |           0.1244 |
[32m[20230207 15:14:28 @agent_ppo2.py:192][0m |          -0.0024 |           0.9424 |           0.1245 |
[32m[20230207 15:14:28 @agent_ppo2.py:192][0m |          -0.0029 |           0.9261 |           0.1245 |
[32m[20230207 15:14:29 @agent_ppo2.py:192][0m |          -0.0187 |           0.9205 |           0.1245 |
[32m[20230207 15:14:29 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:14:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: 43.56
[32m[20230207 15:14:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 59.46
[32m[20230207 15:14:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 20.76
[32m[20230207 15:14:29 @agent_ppo2.py:150][0m Total time:      13.80 min
[32m[20230207 15:14:29 @agent_ppo2.py:152][0m 712704 total steps have happened
[32m[20230207 15:14:29 @agent_ppo2.py:128][0m #------------------------ Iteration 348 --------------------------#
[32m[20230207 15:14:30 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:14:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:30 @agent_ppo2.py:192][0m |           0.0205 |           0.8020 |           0.1248 |
[32m[20230207 15:14:30 @agent_ppo2.py:192][0m |           0.0203 |           0.7340 |           0.1247 |
[32m[20230207 15:14:30 @agent_ppo2.py:192][0m |           0.0277 |           0.7317 |           0.1245 |
[32m[20230207 15:14:31 @agent_ppo2.py:192][0m |           0.0063 |           0.6829 |           0.1247 |
[32m[20230207 15:14:31 @agent_ppo2.py:192][0m |          -0.0179 |           0.6674 |           0.1246 |
[32m[20230207 15:14:31 @agent_ppo2.py:192][0m |          -0.0021 |           0.6551 |           0.1247 |
[32m[20230207 15:14:31 @agent_ppo2.py:192][0m |           0.0065 |           0.6509 |           0.1245 |
[32m[20230207 15:14:31 @agent_ppo2.py:192][0m |          -0.0411 |           0.6616 |           0.1246 |
[32m[20230207 15:14:31 @agent_ppo2.py:192][0m |          -0.0231 |           0.6439 |           0.1244 |
[32m[20230207 15:14:31 @agent_ppo2.py:192][0m |           0.0011 |           0.6364 |           0.1245 |
[32m[20230207 15:14:31 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:14:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -57.68
[32m[20230207 15:14:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -49.80
[32m[20230207 15:14:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 37.17
[32m[20230207 15:14:32 @agent_ppo2.py:150][0m Total time:      13.84 min
[32m[20230207 15:14:32 @agent_ppo2.py:152][0m 714752 total steps have happened
[32m[20230207 15:14:32 @agent_ppo2.py:128][0m #------------------------ Iteration 349 --------------------------#
[32m[20230207 15:14:33 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:14:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:33 @agent_ppo2.py:192][0m |          -0.0005 |           5.1286 |           0.1250 |
[32m[20230207 15:14:33 @agent_ppo2.py:192][0m |          -0.0069 |           2.4768 |           0.1248 |
[32m[20230207 15:14:33 @agent_ppo2.py:192][0m |          -0.0086 |           2.2017 |           0.1248 |
[32m[20230207 15:14:33 @agent_ppo2.py:192][0m |          -0.0096 |           1.9701 |           0.1247 |
[32m[20230207 15:14:33 @agent_ppo2.py:192][0m |          -0.0106 |           1.8533 |           0.1247 |
[32m[20230207 15:14:33 @agent_ppo2.py:192][0m |          -0.0123 |           1.7713 |           0.1247 |
[32m[20230207 15:14:33 @agent_ppo2.py:192][0m |          -0.0120 |           1.6965 |           0.1247 |
[32m[20230207 15:14:33 @agent_ppo2.py:192][0m |          -0.0128 |           1.6437 |           0.1246 |
[32m[20230207 15:14:33 @agent_ppo2.py:192][0m |          -0.0118 |           1.5906 |           0.1246 |
[32m[20230207 15:14:33 @agent_ppo2.py:192][0m |          -0.0129 |           1.5334 |           0.1246 |
[32m[20230207 15:14:33 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:14:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: 9.75
[32m[20230207 15:14:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 67.79
[32m[20230207 15:14:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 36.71
[32m[20230207 15:14:34 @agent_ppo2.py:150][0m Total time:      13.88 min
[32m[20230207 15:14:34 @agent_ppo2.py:152][0m 716800 total steps have happened
[32m[20230207 15:14:34 @agent_ppo2.py:128][0m #------------------------ Iteration 350 --------------------------#
[32m[20230207 15:14:35 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:14:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:35 @agent_ppo2.py:192][0m |          -0.0061 |          15.1809 |           0.1231 |
[32m[20230207 15:14:35 @agent_ppo2.py:192][0m |           0.0140 |           5.6199 |           0.1230 |
[32m[20230207 15:14:35 @agent_ppo2.py:192][0m |          -0.0092 |           3.7937 |           0.1227 |
[32m[20230207 15:14:35 @agent_ppo2.py:192][0m |          -0.0006 |           2.9480 |           0.1227 |
[32m[20230207 15:14:35 @agent_ppo2.py:192][0m |          -0.0138 |           2.5507 |           0.1227 |
[32m[20230207 15:14:35 @agent_ppo2.py:192][0m |          -0.0123 |           2.4089 |           0.1225 |
[32m[20230207 15:14:35 @agent_ppo2.py:192][0m |          -0.0110 |           2.2468 |           0.1226 |
[32m[20230207 15:14:35 @agent_ppo2.py:192][0m |          -0.0080 |           2.2632 |           0.1225 |
[32m[20230207 15:14:36 @agent_ppo2.py:192][0m |          -0.0169 |           2.1071 |           0.1225 |
[32m[20230207 15:14:36 @agent_ppo2.py:192][0m |          -0.0079 |           2.0125 |           0.1224 |
[32m[20230207 15:14:36 @agent_ppo2.py:137][0m Policy update time: 0.67 s
[32m[20230207 15:14:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -56.65
[32m[20230207 15:14:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 15.36
[32m[20230207 15:14:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 50.10
[32m[20230207 15:14:36 @agent_ppo2.py:150][0m Total time:      13.92 min
[32m[20230207 15:14:36 @agent_ppo2.py:152][0m 718848 total steps have happened
[32m[20230207 15:14:36 @agent_ppo2.py:128][0m #------------------------ Iteration 351 --------------------------#
[32m[20230207 15:14:37 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:14:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:37 @agent_ppo2.py:192][0m |           0.0121 |           1.1542 |           0.1234 |
[32m[20230207 15:14:37 @agent_ppo2.py:192][0m |           0.0044 |           0.9834 |           0.1231 |
[32m[20230207 15:14:37 @agent_ppo2.py:192][0m |          -0.0116 |           0.9706 |           0.1231 |
[32m[20230207 15:14:38 @agent_ppo2.py:192][0m |          -0.0189 |           0.9295 |           0.1232 |
[32m[20230207 15:14:38 @agent_ppo2.py:192][0m |          -0.0137 |           0.9164 |           0.1232 |
[32m[20230207 15:14:38 @agent_ppo2.py:192][0m |          -0.0091 |           0.9024 |           0.1232 |
[32m[20230207 15:14:38 @agent_ppo2.py:192][0m |          -0.0135 |           0.9140 |           0.1232 |
[32m[20230207 15:14:38 @agent_ppo2.py:192][0m |          -0.0095 |           0.8897 |           0.1233 |
[32m[20230207 15:14:38 @agent_ppo2.py:192][0m |          -0.0095 |           0.8778 |           0.1233 |
[32m[20230207 15:14:38 @agent_ppo2.py:192][0m |          -0.0062 |           0.8799 |           0.1232 |
[32m[20230207 15:14:38 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:14:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -36.92
[32m[20230207 15:14:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -19.43
[32m[20230207 15:14:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -6.66
[32m[20230207 15:14:39 @agent_ppo2.py:150][0m Total time:      13.96 min
[32m[20230207 15:14:39 @agent_ppo2.py:152][0m 720896 total steps have happened
[32m[20230207 15:14:39 @agent_ppo2.py:128][0m #------------------------ Iteration 352 --------------------------#
[32m[20230207 15:14:40 @agent_ppo2.py:134][0m Sampling time: 0.76 s by 1 slaves
[32m[20230207 15:14:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:40 @agent_ppo2.py:192][0m |          -0.0039 |          47.5655 |           0.1293 |
[32m[20230207 15:14:40 @agent_ppo2.py:192][0m |          -0.0086 |          24.4261 |           0.1291 |
[32m[20230207 15:14:40 @agent_ppo2.py:192][0m |           0.0032 |          13.8420 |           0.1290 |
[32m[20230207 15:14:40 @agent_ppo2.py:192][0m |          -0.0095 |           9.9087 |           0.1289 |
[32m[20230207 15:14:40 @agent_ppo2.py:192][0m |          -0.0097 |           8.2379 |           0.1289 |
[32m[20230207 15:14:40 @agent_ppo2.py:192][0m |          -0.0128 |           6.9161 |           0.1289 |
[32m[20230207 15:14:40 @agent_ppo2.py:192][0m |           0.0369 |           6.3126 |           0.1289 |
[32m[20230207 15:14:40 @agent_ppo2.py:192][0m |           0.0276 |           6.5852 |           0.1283 |
[32m[20230207 15:14:40 @agent_ppo2.py:192][0m |          -0.0111 |           5.3786 |           0.1283 |
[32m[20230207 15:14:40 @agent_ppo2.py:192][0m |          -0.0104 |           5.1444 |           0.1284 |
[32m[20230207 15:14:40 @agent_ppo2.py:137][0m Policy update time: 0.77 s
[32m[20230207 15:14:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -63.72
[32m[20230207 15:14:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 17.18
[32m[20230207 15:14:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 8.16
[32m[20230207 15:14:41 @agent_ppo2.py:150][0m Total time:      14.00 min
[32m[20230207 15:14:41 @agent_ppo2.py:152][0m 722944 total steps have happened
[32m[20230207 15:14:41 @agent_ppo2.py:128][0m #------------------------ Iteration 353 --------------------------#
[32m[20230207 15:14:42 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:14:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:42 @agent_ppo2.py:192][0m |          -0.0007 |           4.9179 |           0.1243 |
[32m[20230207 15:14:42 @agent_ppo2.py:192][0m |          -0.0011 |           2.2593 |           0.1243 |
[32m[20230207 15:14:42 @agent_ppo2.py:192][0m |          -0.0017 |           2.1011 |           0.1243 |
[32m[20230207 15:14:42 @agent_ppo2.py:192][0m |          -0.0064 |           1.7531 |           0.1244 |
[32m[20230207 15:14:42 @agent_ppo2.py:192][0m |          -0.0046 |           1.6263 |           0.1244 |
[32m[20230207 15:14:42 @agent_ppo2.py:192][0m |          -0.0073 |           1.5276 |           0.1244 |
[32m[20230207 15:14:43 @agent_ppo2.py:192][0m |          -0.0062 |           1.4344 |           0.1244 |
[32m[20230207 15:14:43 @agent_ppo2.py:192][0m |          -0.0082 |           1.3574 |           0.1244 |
[32m[20230207 15:14:43 @agent_ppo2.py:192][0m |          -0.0081 |           1.2935 |           0.1245 |
[32m[20230207 15:14:43 @agent_ppo2.py:192][0m |          -0.0091 |           1.2663 |           0.1245 |
[32m[20230207 15:14:43 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:14:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: 13.46
[32m[20230207 15:14:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 91.04
[32m[20230207 15:14:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -70.15
[32m[20230207 15:14:43 @agent_ppo2.py:150][0m Total time:      14.04 min
[32m[20230207 15:14:43 @agent_ppo2.py:152][0m 724992 total steps have happened
[32m[20230207 15:14:43 @agent_ppo2.py:128][0m #------------------------ Iteration 354 --------------------------#
[32m[20230207 15:14:44 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:14:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:44 @agent_ppo2.py:192][0m |          -0.0001 |           6.1701 |           0.1280 |
[32m[20230207 15:14:44 @agent_ppo2.py:192][0m |          -0.0055 |           1.7640 |           0.1279 |
[32m[20230207 15:14:45 @agent_ppo2.py:192][0m |          -0.0061 |           1.5718 |           0.1279 |
[32m[20230207 15:14:45 @agent_ppo2.py:192][0m |          -0.0079 |           1.4766 |           0.1278 |
[32m[20230207 15:14:45 @agent_ppo2.py:192][0m |          -0.0084 |           1.4207 |           0.1278 |
[32m[20230207 15:14:45 @agent_ppo2.py:192][0m |          -0.0091 |           1.3715 |           0.1277 |
[32m[20230207 15:14:45 @agent_ppo2.py:192][0m |          -0.0102 |           1.3399 |           0.1276 |
[32m[20230207 15:14:45 @agent_ppo2.py:192][0m |          -0.0099 |           1.3183 |           0.1276 |
[32m[20230207 15:14:45 @agent_ppo2.py:192][0m |          -0.0110 |           1.2832 |           0.1276 |
[32m[20230207 15:14:45 @agent_ppo2.py:192][0m |          -0.0106 |           1.2599 |           0.1275 |
[32m[20230207 15:14:45 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:14:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -6.26
[32m[20230207 15:14:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 82.23
[32m[20230207 15:14:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -43.62
[32m[20230207 15:14:46 @agent_ppo2.py:150][0m Total time:      14.08 min
[32m[20230207 15:14:46 @agent_ppo2.py:152][0m 727040 total steps have happened
[32m[20230207 15:14:46 @agent_ppo2.py:128][0m #------------------------ Iteration 355 --------------------------#
[32m[20230207 15:14:46 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:14:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:47 @agent_ppo2.py:192][0m |           0.0004 |          49.3627 |           0.1259 |
[32m[20230207 15:14:47 @agent_ppo2.py:192][0m |          -0.0046 |          16.1210 |           0.1258 |
[32m[20230207 15:14:47 @agent_ppo2.py:192][0m |          -0.0072 |          10.6089 |           0.1257 |
[32m[20230207 15:14:47 @agent_ppo2.py:192][0m |          -0.0110 |           7.7834 |           0.1257 |
[32m[20230207 15:14:47 @agent_ppo2.py:192][0m |          -0.0126 |           6.4965 |           0.1257 |
[32m[20230207 15:14:47 @agent_ppo2.py:192][0m |          -0.0146 |           5.5664 |           0.1257 |
[32m[20230207 15:14:47 @agent_ppo2.py:192][0m |          -0.0080 |           5.1330 |           0.1257 |
[32m[20230207 15:14:47 @agent_ppo2.py:192][0m |          -0.0148 |           4.6506 |           0.1256 |
[32m[20230207 15:14:47 @agent_ppo2.py:192][0m |          -0.0121 |           4.2779 |           0.1256 |
[32m[20230207 15:14:47 @agent_ppo2.py:192][0m |          -0.0100 |           3.9967 |           0.1256 |
[32m[20230207 15:14:47 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:14:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -66.65
[32m[20230207 15:14:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 77.76
[32m[20230207 15:14:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 43.02
[32m[20230207 15:14:48 @agent_ppo2.py:150][0m Total time:      14.11 min
[32m[20230207 15:14:48 @agent_ppo2.py:152][0m 729088 total steps have happened
[32m[20230207 15:14:48 @agent_ppo2.py:128][0m #------------------------ Iteration 356 --------------------------#
[32m[20230207 15:14:49 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:14:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:49 @agent_ppo2.py:192][0m |          -0.0026 |           3.2198 |           0.1273 |
[32m[20230207 15:14:49 @agent_ppo2.py:192][0m |           0.0029 |           1.9777 |           0.1271 |
[32m[20230207 15:14:49 @agent_ppo2.py:192][0m |           0.0030 |           1.8334 |           0.1271 |
[32m[20230207 15:14:49 @agent_ppo2.py:192][0m |           0.0088 |           1.6269 |           0.1270 |
[32m[20230207 15:14:49 @agent_ppo2.py:192][0m |           0.0079 |           1.5765 |           0.1271 |
[32m[20230207 15:14:49 @agent_ppo2.py:192][0m |          -0.0163 |           1.4824 |           0.1271 |
[32m[20230207 15:14:49 @agent_ppo2.py:192][0m |          -0.0030 |           1.3986 |           0.1271 |
[32m[20230207 15:14:49 @agent_ppo2.py:192][0m |          -0.0067 |           1.3469 |           0.1272 |
[32m[20230207 15:14:49 @agent_ppo2.py:192][0m |           0.0008 |           1.3103 |           0.1272 |
[32m[20230207 15:14:49 @agent_ppo2.py:192][0m |          -0.0391 |           1.3504 |           0.1271 |
[32m[20230207 15:14:49 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:14:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: 2.40
[32m[20230207 15:14:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 74.59
[32m[20230207 15:14:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -50.04
[32m[20230207 15:14:50 @agent_ppo2.py:150][0m Total time:      14.15 min
[32m[20230207 15:14:50 @agent_ppo2.py:152][0m 731136 total steps have happened
[32m[20230207 15:14:50 @agent_ppo2.py:128][0m #------------------------ Iteration 357 --------------------------#
[32m[20230207 15:14:51 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:14:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:51 @agent_ppo2.py:192][0m |           0.0021 |          40.1784 |           0.1245 |
[32m[20230207 15:14:51 @agent_ppo2.py:192][0m |          -0.0041 |          21.5866 |           0.1244 |
[32m[20230207 15:14:51 @agent_ppo2.py:192][0m |          -0.0065 |          16.4225 |           0.1244 |
[32m[20230207 15:14:51 @agent_ppo2.py:192][0m |          -0.0080 |          15.3013 |           0.1244 |
[32m[20230207 15:14:51 @agent_ppo2.py:192][0m |          -0.0102 |          11.8369 |           0.1244 |
[32m[20230207 15:14:51 @agent_ppo2.py:192][0m |          -0.0072 |          10.7549 |           0.1245 |
[32m[20230207 15:14:51 @agent_ppo2.py:192][0m |          -0.0109 |          10.0821 |           0.1244 |
[32m[20230207 15:14:51 @agent_ppo2.py:192][0m |          -0.0091 |           9.3822 |           0.1243 |
[32m[20230207 15:14:52 @agent_ppo2.py:192][0m |          -0.0124 |           8.9966 |           0.1243 |
[32m[20230207 15:14:52 @agent_ppo2.py:192][0m |          -0.0113 |           8.3603 |           0.1243 |
[32m[20230207 15:14:52 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230207 15:14:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: -71.49
[32m[20230207 15:14:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 64.96
[32m[20230207 15:14:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -31.95
[32m[20230207 15:14:52 @agent_ppo2.py:150][0m Total time:      14.19 min
[32m[20230207 15:14:52 @agent_ppo2.py:152][0m 733184 total steps have happened
[32m[20230207 15:14:52 @agent_ppo2.py:128][0m #------------------------ Iteration 358 --------------------------#
[32m[20230207 15:14:53 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:14:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:53 @agent_ppo2.py:192][0m |           0.0015 |          41.7896 |           0.1216 |
[32m[20230207 15:14:53 @agent_ppo2.py:192][0m |          -0.0234 |          16.1650 |           0.1216 |
[32m[20230207 15:14:53 @agent_ppo2.py:192][0m |          -0.0153 |          11.1760 |           0.1215 |
[32m[20230207 15:14:53 @agent_ppo2.py:192][0m |          -0.0176 |           9.1884 |           0.1215 |
[32m[20230207 15:14:53 @agent_ppo2.py:192][0m |          -0.0196 |           7.7873 |           0.1214 |
[32m[20230207 15:14:53 @agent_ppo2.py:192][0m |          -0.0070 |           7.1509 |           0.1212 |
[32m[20230207 15:14:53 @agent_ppo2.py:192][0m |          -0.0088 |           6.3005 |           0.1213 |
[32m[20230207 15:14:53 @agent_ppo2.py:192][0m |          -0.0141 |           6.0142 |           0.1212 |
[32m[20230207 15:14:53 @agent_ppo2.py:192][0m |          -0.0094 |           5.3259 |           0.1212 |
[32m[20230207 15:14:53 @agent_ppo2.py:192][0m |          -0.0155 |           5.1134 |           0.1212 |
[32m[20230207 15:14:53 @agent_ppo2.py:137][0m Policy update time: 0.53 s
[32m[20230207 15:14:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: -93.75
[32m[20230207 15:14:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -63.24
[32m[20230207 15:14:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -21.65
[32m[20230207 15:14:54 @agent_ppo2.py:150][0m Total time:      14.22 min
[32m[20230207 15:14:54 @agent_ppo2.py:152][0m 735232 total steps have happened
[32m[20230207 15:14:54 @agent_ppo2.py:128][0m #------------------------ Iteration 359 --------------------------#
[32m[20230207 15:14:55 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:14:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:55 @agent_ppo2.py:192][0m |           0.0051 |           5.6925 |           0.1241 |
[32m[20230207 15:14:55 @agent_ppo2.py:192][0m |           0.0032 |           2.9558 |           0.1241 |
[32m[20230207 15:14:55 @agent_ppo2.py:192][0m |          -0.0056 |           2.3769 |           0.1241 |
[32m[20230207 15:14:55 @agent_ppo2.py:192][0m |          -0.0089 |           2.0852 |           0.1240 |
[32m[20230207 15:14:55 @agent_ppo2.py:192][0m |           0.0030 |           1.9264 |           0.1240 |
[32m[20230207 15:14:56 @agent_ppo2.py:192][0m |           0.0003 |           1.8277 |           0.1241 |
[32m[20230207 15:14:56 @agent_ppo2.py:192][0m |          -0.0106 |           1.6974 |           0.1240 |
[32m[20230207 15:14:56 @agent_ppo2.py:192][0m |          -0.0110 |           1.6460 |           0.1239 |
[32m[20230207 15:14:56 @agent_ppo2.py:192][0m |          -0.0156 |           1.5800 |           0.1240 |
[32m[20230207 15:14:56 @agent_ppo2.py:192][0m |          -0.0142 |           1.5253 |           0.1240 |
[32m[20230207 15:14:56 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:14:57 @agent_ppo2.py:145][0m Average TRAINING episode reward: -10.29
[32m[20230207 15:14:57 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 0.50
[32m[20230207 15:14:57 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 29.23
[32m[20230207 15:14:57 @agent_ppo2.py:150][0m Total time:      14.26 min
[32m[20230207 15:14:57 @agent_ppo2.py:152][0m 737280 total steps have happened
[32m[20230207 15:14:57 @agent_ppo2.py:128][0m #------------------------ Iteration 360 --------------------------#
[32m[20230207 15:14:57 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:14:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:14:58 @agent_ppo2.py:192][0m |           0.0078 |           8.5792 |           0.1253 |
[32m[20230207 15:14:58 @agent_ppo2.py:192][0m |           0.0074 |           4.4530 |           0.1251 |
[32m[20230207 15:14:58 @agent_ppo2.py:192][0m |          -0.0008 |           4.0181 |           0.1250 |
[32m[20230207 15:14:58 @agent_ppo2.py:192][0m |          -0.0188 |           3.7392 |           0.1251 |
[32m[20230207 15:14:58 @agent_ppo2.py:192][0m |          -0.0423 |           3.5968 |           0.1248 |
[32m[20230207 15:14:58 @agent_ppo2.py:192][0m |          -0.0044 |           3.4318 |           0.1248 |
[32m[20230207 15:14:58 @agent_ppo2.py:192][0m |           0.0031 |           3.1974 |           0.1249 |
[32m[20230207 15:14:58 @agent_ppo2.py:192][0m |          -0.0095 |           3.2799 |           0.1248 |
[32m[20230207 15:14:58 @agent_ppo2.py:192][0m |          -0.0286 |           3.0127 |           0.1249 |
[32m[20230207 15:14:58 @agent_ppo2.py:192][0m |          -0.0044 |           2.9106 |           0.1249 |
[32m[20230207 15:14:58 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:14:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: 5.07
[32m[20230207 15:14:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 5.77
[32m[20230207 15:14:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 7.67
[32m[20230207 15:14:59 @agent_ppo2.py:150][0m Total time:      14.30 min
[32m[20230207 15:14:59 @agent_ppo2.py:152][0m 739328 total steps have happened
[32m[20230207 15:14:59 @agent_ppo2.py:128][0m #------------------------ Iteration 361 --------------------------#
[32m[20230207 15:15:00 @agent_ppo2.py:134][0m Sampling time: 0.54 s by 1 slaves
[32m[20230207 15:15:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:00 @agent_ppo2.py:192][0m |           0.0004 |           8.0570 |           0.1283 |
[32m[20230207 15:15:00 @agent_ppo2.py:192][0m |          -0.0000 |           3.7872 |           0.1282 |
[32m[20230207 15:15:00 @agent_ppo2.py:192][0m |          -0.0029 |           2.9709 |           0.1282 |
[32m[20230207 15:15:00 @agent_ppo2.py:192][0m |          -0.0043 |           2.5606 |           0.1282 |
[32m[20230207 15:15:00 @agent_ppo2.py:192][0m |          -0.0058 |           2.3541 |           0.1281 |
[32m[20230207 15:15:00 @agent_ppo2.py:192][0m |          -0.0057 |           2.0669 |           0.1281 |
[32m[20230207 15:15:00 @agent_ppo2.py:192][0m |          -0.0072 |           1.9047 |           0.1280 |
[32m[20230207 15:15:00 @agent_ppo2.py:192][0m |          -0.0083 |           1.8058 |           0.1280 |
[32m[20230207 15:15:00 @agent_ppo2.py:192][0m |          -0.0092 |           1.6743 |           0.1280 |
[32m[20230207 15:15:00 @agent_ppo2.py:192][0m |          -0.0095 |           1.6414 |           0.1279 |
[32m[20230207 15:15:00 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:15:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -68.77
[32m[20230207 15:15:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -23.51
[32m[20230207 15:15:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 47.39
[32m[20230207 15:15:01 @agent_ppo2.py:150][0m Total time:      14.33 min
[32m[20230207 15:15:01 @agent_ppo2.py:152][0m 741376 total steps have happened
[32m[20230207 15:15:01 @agent_ppo2.py:128][0m #------------------------ Iteration 362 --------------------------#
[32m[20230207 15:15:02 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:15:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:02 @agent_ppo2.py:192][0m |          -0.0318 |           5.3981 |           0.1239 |
[32m[20230207 15:15:02 @agent_ppo2.py:192][0m |           0.0009 |           3.7064 |           0.1237 |
[32m[20230207 15:15:02 @agent_ppo2.py:192][0m |          -0.0104 |           3.3828 |           0.1236 |
[32m[20230207 15:15:02 @agent_ppo2.py:192][0m |          -0.0097 |           3.2580 |           0.1235 |
[32m[20230207 15:15:02 @agent_ppo2.py:192][0m |          -0.0004 |           3.1190 |           0.1234 |
[32m[20230207 15:15:02 @agent_ppo2.py:192][0m |           0.0017 |           3.0692 |           0.1234 |
[32m[20230207 15:15:02 @agent_ppo2.py:192][0m |          -0.0064 |           2.9885 |           0.1234 |
[32m[20230207 15:15:02 @agent_ppo2.py:192][0m |          -0.0264 |           2.9591 |           0.1233 |
[32m[20230207 15:15:02 @agent_ppo2.py:192][0m |          -0.0090 |           2.8991 |           0.1232 |
[32m[20230207 15:15:02 @agent_ppo2.py:192][0m |          -0.0085 |           2.8459 |           0.1233 |
[32m[20230207 15:15:02 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:15:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: 37.87
[32m[20230207 15:15:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 48.21
[32m[20230207 15:15:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -67.92
[32m[20230207 15:15:03 @agent_ppo2.py:150][0m Total time:      14.37 min
[32m[20230207 15:15:03 @agent_ppo2.py:152][0m 743424 total steps have happened
[32m[20230207 15:15:03 @agent_ppo2.py:128][0m #------------------------ Iteration 363 --------------------------#
[32m[20230207 15:15:04 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:15:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:04 @agent_ppo2.py:192][0m |           0.0078 |           1.2664 |           0.1246 |
[32m[20230207 15:15:04 @agent_ppo2.py:192][0m |          -0.0076 |           1.1087 |           0.1245 |
[32m[20230207 15:15:04 @agent_ppo2.py:192][0m |          -0.0000 |           1.0774 |           0.1244 |
[32m[20230207 15:15:04 @agent_ppo2.py:192][0m |          -0.0076 |           1.0543 |           0.1244 |
[32m[20230207 15:15:05 @agent_ppo2.py:192][0m |          -0.0066 |           1.0484 |           0.1243 |
[32m[20230207 15:15:05 @agent_ppo2.py:192][0m |           0.0001 |           1.0254 |           0.1243 |
[32m[20230207 15:15:05 @agent_ppo2.py:192][0m |          -0.0132 |           1.0152 |           0.1243 |
[32m[20230207 15:15:05 @agent_ppo2.py:192][0m |          -0.0134 |           1.0114 |           0.1241 |
[32m[20230207 15:15:05 @agent_ppo2.py:192][0m |          -0.0252 |           0.9957 |           0.1241 |
[32m[20230207 15:15:05 @agent_ppo2.py:192][0m |          -0.0143 |           0.9915 |           0.1240 |
[32m[20230207 15:15:05 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:15:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: -50.63
[32m[20230207 15:15:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -49.84
[32m[20230207 15:15:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 29.71
[32m[20230207 15:15:06 @agent_ppo2.py:150][0m Total time:      14.41 min
[32m[20230207 15:15:06 @agent_ppo2.py:152][0m 745472 total steps have happened
[32m[20230207 15:15:06 @agent_ppo2.py:128][0m #------------------------ Iteration 364 --------------------------#
[32m[20230207 15:15:07 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:15:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:07 @agent_ppo2.py:192][0m |          -0.0014 |          17.6591 |           0.1235 |
[32m[20230207 15:15:07 @agent_ppo2.py:192][0m |          -0.0041 |           6.5492 |           0.1234 |
[32m[20230207 15:15:07 @agent_ppo2.py:192][0m |          -0.0014 |           3.2436 |           0.1234 |
[32m[20230207 15:15:07 @agent_ppo2.py:192][0m |          -0.0065 |           1.9271 |           0.1233 |
[32m[20230207 15:15:07 @agent_ppo2.py:192][0m |          -0.0058 |           1.5570 |           0.1233 |
[32m[20230207 15:15:07 @agent_ppo2.py:192][0m |          -0.0070 |           1.3658 |           0.1233 |
[32m[20230207 15:15:07 @agent_ppo2.py:192][0m |          -0.0078 |           1.2747 |           0.1232 |
[32m[20230207 15:15:07 @agent_ppo2.py:192][0m |          -0.0091 |           1.2041 |           0.1232 |
[32m[20230207 15:15:07 @agent_ppo2.py:192][0m |          -0.0065 |           1.1568 |           0.1232 |
[32m[20230207 15:15:07 @agent_ppo2.py:192][0m |          -0.0089 |           1.1222 |           0.1231 |
[32m[20230207 15:15:07 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:15:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -18.11
[32m[20230207 15:15:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 40.91
[32m[20230207 15:15:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -37.77
[32m[20230207 15:15:08 @agent_ppo2.py:150][0m Total time:      14.45 min
[32m[20230207 15:15:08 @agent_ppo2.py:152][0m 747520 total steps have happened
[32m[20230207 15:15:08 @agent_ppo2.py:128][0m #------------------------ Iteration 365 --------------------------#
[32m[20230207 15:15:09 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:15:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:09 @agent_ppo2.py:192][0m |          -0.0001 |           3.3498 |           0.1287 |
[32m[20230207 15:15:09 @agent_ppo2.py:192][0m |          -0.0052 |           1.7176 |           0.1287 |
[32m[20230207 15:15:09 @agent_ppo2.py:192][0m |          -0.0073 |           1.5964 |           0.1287 |
[32m[20230207 15:15:09 @agent_ppo2.py:192][0m |          -0.0101 |           1.4757 |           0.1285 |
[32m[20230207 15:15:09 @agent_ppo2.py:192][0m |          -0.0100 |           1.4161 |           0.1285 |
[32m[20230207 15:15:09 @agent_ppo2.py:192][0m |          -0.0116 |           1.3785 |           0.1285 |
[32m[20230207 15:15:09 @agent_ppo2.py:192][0m |          -0.0122 |           1.3273 |           0.1285 |
[32m[20230207 15:15:10 @agent_ppo2.py:192][0m |          -0.0115 |           1.2710 |           0.1285 |
[32m[20230207 15:15:10 @agent_ppo2.py:192][0m |          -0.0113 |           1.2796 |           0.1284 |
[32m[20230207 15:15:10 @agent_ppo2.py:192][0m |          -0.0130 |           1.2282 |           0.1285 |
[32m[20230207 15:15:10 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:15:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: 2.82
[32m[20230207 15:15:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 122.61
[32m[20230207 15:15:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -5.78
[32m[20230207 15:15:10 @agent_ppo2.py:150][0m Total time:      14.49 min
[32m[20230207 15:15:10 @agent_ppo2.py:152][0m 749568 total steps have happened
[32m[20230207 15:15:10 @agent_ppo2.py:128][0m #------------------------ Iteration 366 --------------------------#
[32m[20230207 15:15:11 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:15:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:11 @agent_ppo2.py:192][0m |          -0.0002 |          17.8868 |           0.1263 |
[32m[20230207 15:15:12 @agent_ppo2.py:192][0m |          -0.0074 |           7.2909 |           0.1261 |
[32m[20230207 15:15:12 @agent_ppo2.py:192][0m |          -0.0092 |           6.5662 |           0.1261 |
[32m[20230207 15:15:12 @agent_ppo2.py:192][0m |          -0.0079 |           6.3633 |           0.1261 |
[32m[20230207 15:15:12 @agent_ppo2.py:192][0m |          -0.0109 |           6.0377 |           0.1261 |
[32m[20230207 15:15:12 @agent_ppo2.py:192][0m |          -0.0136 |           5.7755 |           0.1262 |
[32m[20230207 15:15:12 @agent_ppo2.py:192][0m |          -0.0136 |           5.6995 |           0.1261 |
[32m[20230207 15:15:12 @agent_ppo2.py:192][0m |          -0.0150 |           5.6574 |           0.1262 |
[32m[20230207 15:15:12 @agent_ppo2.py:192][0m |          -0.0150 |           5.3895 |           0.1263 |
[32m[20230207 15:15:12 @agent_ppo2.py:192][0m |          -0.0163 |           5.2114 |           0.1263 |
[32m[20230207 15:15:12 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:15:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -16.42
[32m[20230207 15:15:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 52.10
[32m[20230207 15:15:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -50.15
[32m[20230207 15:15:13 @agent_ppo2.py:150][0m Total time:      14.53 min
[32m[20230207 15:15:13 @agent_ppo2.py:152][0m 751616 total steps have happened
[32m[20230207 15:15:13 @agent_ppo2.py:128][0m #------------------------ Iteration 367 --------------------------#
[32m[20230207 15:15:14 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:15:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:14 @agent_ppo2.py:192][0m |          -0.0001 |          14.2083 |           0.1276 |
[32m[20230207 15:15:14 @agent_ppo2.py:192][0m |          -0.0023 |           6.1514 |           0.1275 |
[32m[20230207 15:15:14 @agent_ppo2.py:192][0m |          -0.0043 |           5.1538 |           0.1277 |
[32m[20230207 15:15:14 @agent_ppo2.py:192][0m |          -0.0058 |           4.8027 |           0.1277 |
[32m[20230207 15:15:14 @agent_ppo2.py:192][0m |          -0.0064 |           4.3708 |           0.1278 |
[32m[20230207 15:15:14 @agent_ppo2.py:192][0m |          -0.0081 |           4.1812 |           0.1278 |
[32m[20230207 15:15:14 @agent_ppo2.py:192][0m |          -0.0090 |           4.0944 |           0.1279 |
[32m[20230207 15:15:15 @agent_ppo2.py:192][0m |          -0.0099 |           3.9448 |           0.1279 |
[32m[20230207 15:15:15 @agent_ppo2.py:192][0m |          -0.0104 |           3.8281 |           0.1280 |
[32m[20230207 15:15:15 @agent_ppo2.py:192][0m |          -0.0112 |           3.7041 |           0.1280 |
[32m[20230207 15:15:15 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:15:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -46.60
[32m[20230207 15:15:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 60.60
[32m[20230207 15:15:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 39.84
[32m[20230207 15:15:16 @agent_ppo2.py:150][0m Total time:      14.57 min
[32m[20230207 15:15:16 @agent_ppo2.py:152][0m 753664 total steps have happened
[32m[20230207 15:15:16 @agent_ppo2.py:128][0m #------------------------ Iteration 368 --------------------------#
[32m[20230207 15:15:16 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:15:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:17 @agent_ppo2.py:192][0m |           0.0017 |          11.0288 |           0.1265 |
[32m[20230207 15:15:17 @agent_ppo2.py:192][0m |          -0.0033 |           5.6754 |           0.1264 |
[32m[20230207 15:15:17 @agent_ppo2.py:192][0m |          -0.0035 |           4.2113 |           0.1264 |
[32m[20230207 15:15:17 @agent_ppo2.py:192][0m |          -0.0072 |           3.7300 |           0.1263 |
[32m[20230207 15:15:17 @agent_ppo2.py:192][0m |          -0.0055 |           3.4663 |           0.1262 |
[32m[20230207 15:15:17 @agent_ppo2.py:192][0m |          -0.0053 |           3.2939 |           0.1262 |
[32m[20230207 15:15:17 @agent_ppo2.py:192][0m |          -0.0058 |           3.1981 |           0.1261 |
[32m[20230207 15:15:17 @agent_ppo2.py:192][0m |          -0.0076 |           3.0963 |           0.1261 |
[32m[20230207 15:15:17 @agent_ppo2.py:192][0m |          -0.0063 |           3.0393 |           0.1261 |
[32m[20230207 15:15:17 @agent_ppo2.py:192][0m |          -0.0084 |           2.9598 |           0.1260 |
[32m[20230207 15:15:17 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:15:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -29.19
[32m[20230207 15:15:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 90.79
[32m[20230207 15:15:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -55.10
[32m[20230207 15:15:18 @agent_ppo2.py:150][0m Total time:      14.61 min
[32m[20230207 15:15:18 @agent_ppo2.py:152][0m 755712 total steps have happened
[32m[20230207 15:15:18 @agent_ppo2.py:128][0m #------------------------ Iteration 369 --------------------------#
[32m[20230207 15:15:19 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:15:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:19 @agent_ppo2.py:192][0m |          -0.0034 |           2.1781 |           0.1291 |
[32m[20230207 15:15:19 @agent_ppo2.py:192][0m |           0.0130 |           1.4802 |           0.1291 |
[32m[20230207 15:15:19 @agent_ppo2.py:192][0m |          -0.0024 |           1.4106 |           0.1290 |
[32m[20230207 15:15:19 @agent_ppo2.py:192][0m |          -0.0206 |           1.3633 |           0.1291 |
[32m[20230207 15:15:19 @agent_ppo2.py:192][0m |          -0.0041 |           1.3202 |           0.1292 |
[32m[20230207 15:15:19 @agent_ppo2.py:192][0m |           0.0128 |           1.2902 |           0.1292 |
[32m[20230207 15:15:19 @agent_ppo2.py:192][0m |           0.0013 |           1.2772 |           0.1292 |
[32m[20230207 15:15:19 @agent_ppo2.py:192][0m |           0.0059 |           1.2653 |           0.1293 |
[32m[20230207 15:15:19 @agent_ppo2.py:192][0m |          -0.0108 |           1.2479 |           0.1294 |
[32m[20230207 15:15:19 @agent_ppo2.py:192][0m |           0.0002 |           1.2438 |           0.1295 |
[32m[20230207 15:15:19 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:15:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: 0.27
[32m[20230207 15:15:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 20.80
[32m[20230207 15:15:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -64.44
[32m[20230207 15:15:20 @agent_ppo2.py:150][0m Total time:      14.65 min
[32m[20230207 15:15:20 @agent_ppo2.py:152][0m 757760 total steps have happened
[32m[20230207 15:15:20 @agent_ppo2.py:128][0m #------------------------ Iteration 370 --------------------------#
[32m[20230207 15:15:21 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:15:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:21 @agent_ppo2.py:192][0m |          -0.0077 |           2.6485 |           0.1253 |
[32m[20230207 15:15:21 @agent_ppo2.py:192][0m |          -0.0108 |           2.3164 |           0.1253 |
[32m[20230207 15:15:21 @agent_ppo2.py:192][0m |          -0.0075 |           2.2150 |           0.1252 |
[32m[20230207 15:15:21 @agent_ppo2.py:192][0m |          -0.0102 |           2.1654 |           0.1251 |
[32m[20230207 15:15:21 @agent_ppo2.py:192][0m |          -0.0156 |           2.1264 |           0.1251 |
[32m[20230207 15:15:21 @agent_ppo2.py:192][0m |          -0.0058 |           2.0956 |           0.1250 |
[32m[20230207 15:15:21 @agent_ppo2.py:192][0m |          -0.0061 |           2.0688 |           0.1251 |
[32m[20230207 15:15:21 @agent_ppo2.py:192][0m |           0.0041 |           2.0609 |           0.1251 |
[32m[20230207 15:15:22 @agent_ppo2.py:192][0m |          -0.0014 |           2.0347 |           0.1249 |
[32m[20230207 15:15:22 @agent_ppo2.py:192][0m |          -0.0076 |           2.0050 |           0.1250 |
[32m[20230207 15:15:22 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:15:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: -17.36
[32m[20230207 15:15:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 27.15
[32m[20230207 15:15:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -9.18
[32m[20230207 15:15:22 @agent_ppo2.py:150][0m Total time:      14.69 min
[32m[20230207 15:15:22 @agent_ppo2.py:152][0m 759808 total steps have happened
[32m[20230207 15:15:22 @agent_ppo2.py:128][0m #------------------------ Iteration 371 --------------------------#
[32m[20230207 15:15:23 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:15:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:23 @agent_ppo2.py:192][0m |           0.0121 |           1.9853 |           0.1269 |
[32m[20230207 15:15:23 @agent_ppo2.py:192][0m |          -0.0215 |           1.4385 |           0.1268 |
[32m[20230207 15:15:24 @agent_ppo2.py:192][0m |           0.0116 |           1.3498 |           0.1268 |
[32m[20230207 15:15:24 @agent_ppo2.py:192][0m |          -0.0102 |           1.2872 |           0.1268 |
[32m[20230207 15:15:24 @agent_ppo2.py:192][0m |           0.0032 |           1.2373 |           0.1269 |
[32m[20230207 15:15:24 @agent_ppo2.py:192][0m |          -0.0204 |           1.2137 |           0.1270 |
[32m[20230207 15:15:24 @agent_ppo2.py:192][0m |          -0.0100 |           1.1967 |           0.1270 |
[32m[20230207 15:15:24 @agent_ppo2.py:192][0m |          -0.0269 |           1.1791 |           0.1272 |
[32m[20230207 15:15:24 @agent_ppo2.py:192][0m |          -0.0198 |           1.1579 |           0.1273 |
[32m[20230207 15:15:24 @agent_ppo2.py:192][0m |           0.0018 |           1.1488 |           0.1274 |
[32m[20230207 15:15:24 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:15:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -8.72
[32m[20230207 15:15:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 19.11
[32m[20230207 15:15:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -87.65
[32m[20230207 15:15:25 @agent_ppo2.py:150][0m Total time:      14.73 min
[32m[20230207 15:15:25 @agent_ppo2.py:152][0m 761856 total steps have happened
[32m[20230207 15:15:25 @agent_ppo2.py:128][0m #------------------------ Iteration 372 --------------------------#
[32m[20230207 15:15:26 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:15:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:26 @agent_ppo2.py:192][0m |           0.0002 |           9.0676 |           0.1296 |
[32m[20230207 15:15:26 @agent_ppo2.py:192][0m |          -0.0034 |           4.9919 |           0.1296 |
[32m[20230207 15:15:26 @agent_ppo2.py:192][0m |          -0.0073 |           3.9238 |           0.1297 |
[32m[20230207 15:15:26 @agent_ppo2.py:192][0m |          -0.0080 |           3.6642 |           0.1297 |
[32m[20230207 15:15:26 @agent_ppo2.py:192][0m |          -0.0092 |           3.5419 |           0.1296 |
[32m[20230207 15:15:26 @agent_ppo2.py:192][0m |          -0.0108 |           3.4279 |           0.1296 |
[32m[20230207 15:15:26 @agent_ppo2.py:192][0m |          -0.0108 |           3.2925 |           0.1296 |
[32m[20230207 15:15:26 @agent_ppo2.py:192][0m |          -0.0126 |           3.2404 |           0.1295 |
[32m[20230207 15:15:26 @agent_ppo2.py:192][0m |          -0.0123 |           3.1137 |           0.1295 |
[32m[20230207 15:15:26 @agent_ppo2.py:192][0m |          -0.0131 |           3.0761 |           0.1295 |
[32m[20230207 15:15:26 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:15:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -26.75
[32m[20230207 15:15:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 71.80
[32m[20230207 15:15:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -20.76
[32m[20230207 15:15:27 @agent_ppo2.py:150][0m Total time:      14.77 min
[32m[20230207 15:15:27 @agent_ppo2.py:152][0m 763904 total steps have happened
[32m[20230207 15:15:27 @agent_ppo2.py:128][0m #------------------------ Iteration 373 --------------------------#
[32m[20230207 15:15:28 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:15:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:28 @agent_ppo2.py:192][0m |           0.0113 |           6.5341 |           0.1281 |
[32m[20230207 15:15:28 @agent_ppo2.py:192][0m |           0.0027 |           2.5923 |           0.1282 |
[32m[20230207 15:15:29 @agent_ppo2.py:192][0m |          -0.0089 |           2.5626 |           0.1282 |
[32m[20230207 15:15:29 @agent_ppo2.py:192][0m |          -0.0112 |           1.5559 |           0.1281 |
[32m[20230207 15:15:29 @agent_ppo2.py:192][0m |          -0.0154 |           1.3769 |           0.1281 |
[32m[20230207 15:15:29 @agent_ppo2.py:192][0m |          -0.0128 |           1.2947 |           0.1281 |
[32m[20230207 15:15:29 @agent_ppo2.py:192][0m |          -0.0153 |           1.2335 |           0.1281 |
[32m[20230207 15:15:29 @agent_ppo2.py:192][0m |           0.0035 |           1.1662 |           0.1280 |
[32m[20230207 15:15:29 @agent_ppo2.py:192][0m |          -0.0118 |           1.1008 |           0.1279 |
[32m[20230207 15:15:29 @agent_ppo2.py:192][0m |          -0.0123 |           1.0840 |           0.1279 |
[32m[20230207 15:15:29 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:15:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -75.90
[32m[20230207 15:15:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -1.34
[32m[20230207 15:15:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 31.04
[32m[20230207 15:15:30 @agent_ppo2.py:150][0m Total time:      14.81 min
[32m[20230207 15:15:30 @agent_ppo2.py:152][0m 765952 total steps have happened
[32m[20230207 15:15:30 @agent_ppo2.py:128][0m #------------------------ Iteration 374 --------------------------#
[32m[20230207 15:15:31 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:15:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:31 @agent_ppo2.py:192][0m |          -0.0004 |           6.1831 |           0.1285 |
[32m[20230207 15:15:31 @agent_ppo2.py:192][0m |          -0.0004 |           4.9824 |           0.1284 |
[32m[20230207 15:15:31 @agent_ppo2.py:192][0m |           0.0007 |           4.7320 |           0.1285 |
[32m[20230207 15:15:31 @agent_ppo2.py:192][0m |          -0.0056 |           4.6376 |           0.1284 |
[32m[20230207 15:15:31 @agent_ppo2.py:192][0m |           0.0043 |           4.5500 |           0.1284 |
[32m[20230207 15:15:31 @agent_ppo2.py:192][0m |          -0.0112 |           4.4232 |           0.1284 |
[32m[20230207 15:15:31 @agent_ppo2.py:192][0m |          -0.0498 |           4.8825 |           0.1285 |
[32m[20230207 15:15:31 @agent_ppo2.py:192][0m |          -0.0254 |           4.5223 |           0.1283 |
[32m[20230207 15:15:31 @agent_ppo2.py:192][0m |          -0.0008 |           4.1466 |           0.1284 |
[32m[20230207 15:15:31 @agent_ppo2.py:192][0m |          -0.0188 |           4.1394 |           0.1283 |
[32m[20230207 15:15:31 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:15:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: 9.77
[32m[20230207 15:15:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 15.56
[32m[20230207 15:15:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -25.30
[32m[20230207 15:15:32 @agent_ppo2.py:150][0m Total time:      14.85 min
[32m[20230207 15:15:32 @agent_ppo2.py:152][0m 768000 total steps have happened
[32m[20230207 15:15:32 @agent_ppo2.py:128][0m #------------------------ Iteration 375 --------------------------#
[32m[20230207 15:15:33 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:15:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:33 @agent_ppo2.py:192][0m |           0.0031 |           1.9909 |           0.1298 |
[32m[20230207 15:15:33 @agent_ppo2.py:192][0m |           0.0116 |           1.2374 |           0.1297 |
[32m[20230207 15:15:33 @agent_ppo2.py:192][0m |          -0.0020 |           1.1479 |           0.1297 |
[32m[20230207 15:15:34 @agent_ppo2.py:192][0m |          -0.0180 |           1.1116 |           0.1296 |
[32m[20230207 15:15:34 @agent_ppo2.py:192][0m |          -0.0134 |           1.1083 |           0.1297 |
[32m[20230207 15:15:34 @agent_ppo2.py:192][0m |          -0.0068 |           1.0686 |           0.1297 |
[32m[20230207 15:15:34 @agent_ppo2.py:192][0m |           0.0010 |           1.0532 |           0.1299 |
[32m[20230207 15:15:34 @agent_ppo2.py:192][0m |           0.0014 |           1.0454 |           0.1298 |
[32m[20230207 15:15:34 @agent_ppo2.py:192][0m |           0.0009 |           1.0315 |           0.1298 |
[32m[20230207 15:15:34 @agent_ppo2.py:192][0m |          -0.0369 |           1.0249 |           0.1299 |
[32m[20230207 15:15:34 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:15:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: 38.26
[32m[20230207 15:15:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 41.61
[32m[20230207 15:15:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -32.09
[32m[20230207 15:15:35 @agent_ppo2.py:150][0m Total time:      14.89 min
[32m[20230207 15:15:35 @agent_ppo2.py:152][0m 770048 total steps have happened
[32m[20230207 15:15:35 @agent_ppo2.py:128][0m #------------------------ Iteration 376 --------------------------#
[32m[20230207 15:15:36 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:15:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:36 @agent_ppo2.py:192][0m |          -0.0015 |          15.7274 |           0.1315 |
[32m[20230207 15:15:36 @agent_ppo2.py:192][0m |          -0.0031 |           5.1375 |           0.1314 |
[32m[20230207 15:15:36 @agent_ppo2.py:192][0m |          -0.0063 |           3.7523 |           0.1314 |
[32m[20230207 15:15:36 @agent_ppo2.py:192][0m |          -0.0066 |           3.1520 |           0.1315 |
[32m[20230207 15:15:36 @agent_ppo2.py:192][0m |          -0.0075 |           2.7536 |           0.1315 |
[32m[20230207 15:15:36 @agent_ppo2.py:192][0m |          -0.0084 |           2.5228 |           0.1313 |
[32m[20230207 15:15:36 @agent_ppo2.py:192][0m |          -0.0092 |           2.3963 |           0.1314 |
[32m[20230207 15:15:36 @agent_ppo2.py:192][0m |          -0.0100 |           2.2805 |           0.1314 |
[32m[20230207 15:15:37 @agent_ppo2.py:192][0m |          -0.0094 |           2.1402 |           0.1314 |
[32m[20230207 15:15:37 @agent_ppo2.py:192][0m |          -0.0117 |           2.1233 |           0.1314 |
[32m[20230207 15:15:37 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:15:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: -25.01
[32m[20230207 15:15:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 93.32
[32m[20230207 15:15:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -48.56
[32m[20230207 15:15:37 @agent_ppo2.py:150][0m Total time:      14.93 min
[32m[20230207 15:15:37 @agent_ppo2.py:152][0m 772096 total steps have happened
[32m[20230207 15:15:37 @agent_ppo2.py:128][0m #------------------------ Iteration 377 --------------------------#
[32m[20230207 15:15:38 @agent_ppo2.py:134][0m Sampling time: 0.71 s by 1 slaves
[32m[20230207 15:15:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:38 @agent_ppo2.py:192][0m |           0.0005 |          18.6645 |           0.1313 |
[32m[20230207 15:15:38 @agent_ppo2.py:192][0m |          -0.0042 |           9.6157 |           0.1312 |
[32m[20230207 15:15:38 @agent_ppo2.py:192][0m |          -0.0077 |           7.5964 |           0.1311 |
[32m[20230207 15:15:38 @agent_ppo2.py:192][0m |          -0.0102 |           6.0457 |           0.1310 |
[32m[20230207 15:15:38 @agent_ppo2.py:192][0m |          -0.0101 |           5.0656 |           0.1309 |
[32m[20230207 15:15:38 @agent_ppo2.py:192][0m |          -0.0119 |           4.4121 |           0.1308 |
[32m[20230207 15:15:38 @agent_ppo2.py:192][0m |          -0.0134 |           3.7590 |           0.1307 |
[32m[20230207 15:15:38 @agent_ppo2.py:192][0m |          -0.0075 |           3.3144 |           0.1306 |
[32m[20230207 15:15:39 @agent_ppo2.py:192][0m |          -0.0154 |           3.0162 |           0.1305 |
[32m[20230207 15:15:39 @agent_ppo2.py:192][0m |          -0.0141 |           2.8552 |           0.1305 |
[32m[20230207 15:15:39 @agent_ppo2.py:137][0m Policy update time: 0.72 s
[32m[20230207 15:15:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -99.96
[32m[20230207 15:15:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -69.79
[32m[20230207 15:15:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 57.88
[32m[20230207 15:15:39 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 57.88
[32m[20230207 15:15:39 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 57.88
[32m[20230207 15:15:39 @agent_ppo2.py:150][0m Total time:      14.97 min
[32m[20230207 15:15:39 @agent_ppo2.py:152][0m 774144 total steps have happened
[32m[20230207 15:15:39 @agent_ppo2.py:128][0m #------------------------ Iteration 378 --------------------------#
[32m[20230207 15:15:40 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:15:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:40 @agent_ppo2.py:192][0m |          -0.0013 |           2.6745 |           0.1284 |
[32m[20230207 15:15:40 @agent_ppo2.py:192][0m |          -0.0075 |           2.1686 |           0.1284 |
[32m[20230207 15:15:40 @agent_ppo2.py:192][0m |          -0.0005 |           2.0571 |           0.1286 |
[32m[20230207 15:15:41 @agent_ppo2.py:192][0m |           0.0065 |           1.9980 |           0.1286 |
[32m[20230207 15:15:41 @agent_ppo2.py:192][0m |          -0.0024 |           1.9330 |           0.1286 |
[32m[20230207 15:15:41 @agent_ppo2.py:192][0m |           0.0012 |           1.8958 |           0.1286 |
[32m[20230207 15:15:41 @agent_ppo2.py:192][0m |          -0.0124 |           1.8713 |           0.1286 |
[32m[20230207 15:15:41 @agent_ppo2.py:192][0m |          -0.0170 |           1.8116 |           0.1287 |
[32m[20230207 15:15:41 @agent_ppo2.py:192][0m |          -0.0191 |           1.7855 |           0.1287 |
[32m[20230207 15:15:41 @agent_ppo2.py:192][0m |          -0.0066 |           1.7688 |           0.1288 |
[32m[20230207 15:15:41 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:15:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: 7.91
[32m[20230207 15:15:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 54.05
[32m[20230207 15:15:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -82.79
[32m[20230207 15:15:42 @agent_ppo2.py:150][0m Total time:      15.01 min
[32m[20230207 15:15:42 @agent_ppo2.py:152][0m 776192 total steps have happened
[32m[20230207 15:15:42 @agent_ppo2.py:128][0m #------------------------ Iteration 379 --------------------------#
[32m[20230207 15:15:42 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:15:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:42 @agent_ppo2.py:192][0m |          -0.0053 |           1.4594 |           0.1340 |
[32m[20230207 15:15:43 @agent_ppo2.py:192][0m |           0.0075 |           1.2551 |           0.1337 |
[32m[20230207 15:15:43 @agent_ppo2.py:192][0m |           0.0042 |           1.1796 |           0.1336 |
[32m[20230207 15:15:43 @agent_ppo2.py:192][0m |          -0.0002 |           1.1434 |           0.1337 |
[32m[20230207 15:15:43 @agent_ppo2.py:192][0m |          -0.0072 |           1.1216 |           0.1336 |
[32m[20230207 15:15:43 @agent_ppo2.py:192][0m |          -0.0125 |           1.0920 |           0.1337 |
[32m[20230207 15:15:43 @agent_ppo2.py:192][0m |          -0.0142 |           1.0787 |           0.1337 |
[32m[20230207 15:15:43 @agent_ppo2.py:192][0m |          -0.0103 |           1.0629 |           0.1337 |
[32m[20230207 15:15:43 @agent_ppo2.py:192][0m |          -0.0213 |           1.0540 |           0.1336 |
[32m[20230207 15:15:43 @agent_ppo2.py:192][0m |          -0.0140 |           1.0430 |           0.1337 |
[32m[20230207 15:15:43 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:15:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: -30.22
[32m[20230207 15:15:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -28.78
[32m[20230207 15:15:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 19.01
[32m[20230207 15:15:44 @agent_ppo2.py:150][0m Total time:      15.05 min
[32m[20230207 15:15:44 @agent_ppo2.py:152][0m 778240 total steps have happened
[32m[20230207 15:15:44 @agent_ppo2.py:128][0m #------------------------ Iteration 380 --------------------------#
[32m[20230207 15:15:45 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:15:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:45 @agent_ppo2.py:192][0m |          -0.0002 |           1.8344 |           0.1298 |
[32m[20230207 15:15:45 @agent_ppo2.py:192][0m |          -0.0223 |           1.1861 |           0.1300 |
[32m[20230207 15:15:45 @agent_ppo2.py:192][0m |          -0.0085 |           1.0806 |           0.1299 |
[32m[20230207 15:15:45 @agent_ppo2.py:192][0m |          -0.0143 |           1.0342 |           0.1300 |
[32m[20230207 15:15:45 @agent_ppo2.py:192][0m |           0.0017 |           0.9889 |           0.1299 |
[32m[20230207 15:15:45 @agent_ppo2.py:192][0m |          -0.0002 |           0.9634 |           0.1299 |
[32m[20230207 15:15:45 @agent_ppo2.py:192][0m |          -0.0227 |           0.9325 |           0.1300 |
[32m[20230207 15:15:45 @agent_ppo2.py:192][0m |          -0.0112 |           0.9596 |           0.1300 |
[32m[20230207 15:15:45 @agent_ppo2.py:192][0m |          -0.0299 |           0.9295 |           0.1298 |
[32m[20230207 15:15:46 @agent_ppo2.py:192][0m |          -0.0342 |           0.9021 |           0.1298 |
[32m[20230207 15:15:46 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:15:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: 49.08
[32m[20230207 15:15:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 67.36
[32m[20230207 15:15:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 12.60
[32m[20230207 15:15:46 @agent_ppo2.py:150][0m Total time:      15.09 min
[32m[20230207 15:15:46 @agent_ppo2.py:152][0m 780288 total steps have happened
[32m[20230207 15:15:46 @agent_ppo2.py:128][0m #------------------------ Iteration 381 --------------------------#
[32m[20230207 15:15:47 @agent_ppo2.py:134][0m Sampling time: 0.71 s by 1 slaves
[32m[20230207 15:15:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:47 @agent_ppo2.py:192][0m |           0.0023 |          22.6448 |           0.1339 |
[32m[20230207 15:15:47 @agent_ppo2.py:192][0m |          -0.0013 |           7.0753 |           0.1339 |
[32m[20230207 15:15:47 @agent_ppo2.py:192][0m |          -0.0038 |           5.8054 |           0.1339 |
[32m[20230207 15:15:47 @agent_ppo2.py:192][0m |          -0.0057 |           4.9064 |           0.1338 |
[32m[20230207 15:15:48 @agent_ppo2.py:192][0m |          -0.0070 |           4.6176 |           0.1338 |
[32m[20230207 15:15:48 @agent_ppo2.py:192][0m |          -0.0094 |           4.1251 |           0.1337 |
[32m[20230207 15:15:48 @agent_ppo2.py:192][0m |          -0.0102 |           3.9429 |           0.1337 |
[32m[20230207 15:15:48 @agent_ppo2.py:192][0m |          -0.0122 |           3.7225 |           0.1336 |
[32m[20230207 15:15:48 @agent_ppo2.py:192][0m |          -0.0125 |           3.5242 |           0.1336 |
[32m[20230207 15:15:48 @agent_ppo2.py:192][0m |          -0.0135 |           3.3262 |           0.1335 |
[32m[20230207 15:15:48 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230207 15:15:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: -79.76
[32m[20230207 15:15:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 64.21
[32m[20230207 15:15:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -23.01
[32m[20230207 15:15:49 @agent_ppo2.py:150][0m Total time:      15.12 min
[32m[20230207 15:15:49 @agent_ppo2.py:152][0m 782336 total steps have happened
[32m[20230207 15:15:49 @agent_ppo2.py:128][0m #------------------------ Iteration 382 --------------------------#
[32m[20230207 15:15:49 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:15:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:50 @agent_ppo2.py:192][0m |          -0.0182 |           3.4620 |           0.1314 |
[32m[20230207 15:15:50 @agent_ppo2.py:192][0m |          -0.0084 |           1.8808 |           0.1312 |
[32m[20230207 15:15:50 @agent_ppo2.py:192][0m |          -0.0190 |           1.6817 |           0.1312 |
[32m[20230207 15:15:50 @agent_ppo2.py:192][0m |          -0.0037 |           1.5759 |           0.1312 |
[32m[20230207 15:15:50 @agent_ppo2.py:192][0m |          -0.0050 |           1.5018 |           0.1311 |
[32m[20230207 15:15:50 @agent_ppo2.py:192][0m |           0.0080 |           1.4529 |           0.1311 |
[32m[20230207 15:15:50 @agent_ppo2.py:192][0m |          -0.0329 |           1.4161 |           0.1311 |
[32m[20230207 15:15:50 @agent_ppo2.py:192][0m |          -0.0370 |           1.4030 |           0.1311 |
[32m[20230207 15:15:50 @agent_ppo2.py:192][0m |          -0.0175 |           1.3649 |           0.1311 |
[32m[20230207 15:15:50 @agent_ppo2.py:192][0m |          -0.0099 |           1.3239 |           0.1311 |
[32m[20230207 15:15:50 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:15:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: 0.62
[32m[20230207 15:15:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 2.36
[32m[20230207 15:15:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -42.88
[32m[20230207 15:15:51 @agent_ppo2.py:150][0m Total time:      15.17 min
[32m[20230207 15:15:51 @agent_ppo2.py:152][0m 784384 total steps have happened
[32m[20230207 15:15:51 @agent_ppo2.py:128][0m #------------------------ Iteration 383 --------------------------#
[32m[20230207 15:15:52 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:15:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:52 @agent_ppo2.py:192][0m |           0.0093 |           1.1398 |           0.1305 |
[32m[20230207 15:15:52 @agent_ppo2.py:192][0m |          -0.0079 |           0.9343 |           0.1304 |
[32m[20230207 15:15:52 @agent_ppo2.py:192][0m |          -0.0049 |           0.8808 |           0.1302 |
[32m[20230207 15:15:52 @agent_ppo2.py:192][0m |          -0.0226 |           0.8546 |           0.1303 |
[32m[20230207 15:15:52 @agent_ppo2.py:192][0m |          -0.0157 |           0.8310 |           0.1303 |
[32m[20230207 15:15:52 @agent_ppo2.py:192][0m |          -0.0065 |           0.8238 |           0.1303 |
[32m[20230207 15:15:53 @agent_ppo2.py:192][0m |          -0.0103 |           0.8046 |           0.1303 |
[32m[20230207 15:15:53 @agent_ppo2.py:192][0m |          -0.0036 |           0.7973 |           0.1304 |
[32m[20230207 15:15:53 @agent_ppo2.py:192][0m |          -0.0014 |           0.7901 |           0.1303 |
[32m[20230207 15:15:53 @agent_ppo2.py:192][0m |          -0.0295 |           0.7819 |           0.1304 |
[32m[20230207 15:15:53 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:15:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: -5.83
[32m[20230207 15:15:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 4.56
[32m[20230207 15:15:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -22.40
[32m[20230207 15:15:54 @agent_ppo2.py:150][0m Total time:      15.21 min
[32m[20230207 15:15:54 @agent_ppo2.py:152][0m 786432 total steps have happened
[32m[20230207 15:15:54 @agent_ppo2.py:128][0m #------------------------ Iteration 384 --------------------------#
[32m[20230207 15:15:54 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:15:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:55 @agent_ppo2.py:192][0m |           0.0040 |           1.9823 |           0.1311 |
[32m[20230207 15:15:55 @agent_ppo2.py:192][0m |          -0.0038 |           1.3407 |           0.1308 |
[32m[20230207 15:15:55 @agent_ppo2.py:192][0m |           0.0019 |           1.2442 |           0.1309 |
[32m[20230207 15:15:55 @agent_ppo2.py:192][0m |          -0.0065 |           1.1969 |           0.1307 |
[32m[20230207 15:15:55 @agent_ppo2.py:192][0m |          -0.0038 |           1.1681 |           0.1307 |
[32m[20230207 15:15:55 @agent_ppo2.py:192][0m |          -0.0089 |           1.1487 |           0.1307 |
[32m[20230207 15:15:55 @agent_ppo2.py:192][0m |          -0.0070 |           1.1088 |           0.1308 |
[32m[20230207 15:15:55 @agent_ppo2.py:192][0m |          -0.0098 |           1.0997 |           0.1307 |
[32m[20230207 15:15:55 @agent_ppo2.py:192][0m |          -0.0128 |           1.0879 |           0.1306 |
[32m[20230207 15:15:55 @agent_ppo2.py:192][0m |          -0.0104 |           1.0706 |           0.1306 |
[32m[20230207 15:15:55 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:15:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: 39.89
[32m[20230207 15:15:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 47.78
[32m[20230207 15:15:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -8.47
[32m[20230207 15:15:56 @agent_ppo2.py:150][0m Total time:      15.25 min
[32m[20230207 15:15:56 @agent_ppo2.py:152][0m 788480 total steps have happened
[32m[20230207 15:15:56 @agent_ppo2.py:128][0m #------------------------ Iteration 385 --------------------------#
[32m[20230207 15:15:57 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:15:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:15:57 @agent_ppo2.py:192][0m |           0.0008 |           4.6384 |           0.1334 |
[32m[20230207 15:15:57 @agent_ppo2.py:192][0m |          -0.0049 |           1.6825 |           0.1333 |
[32m[20230207 15:15:57 @agent_ppo2.py:192][0m |          -0.0059 |           1.3735 |           0.1332 |
[32m[20230207 15:15:57 @agent_ppo2.py:192][0m |          -0.0089 |           1.2899 |           0.1331 |
[32m[20230207 15:15:57 @agent_ppo2.py:192][0m |          -0.0102 |           1.2599 |           0.1330 |
[32m[20230207 15:15:57 @agent_ppo2.py:192][0m |          -0.0117 |           1.2159 |           0.1329 |
[32m[20230207 15:15:58 @agent_ppo2.py:192][0m |          -0.0118 |           1.2033 |           0.1328 |
[32m[20230207 15:15:58 @agent_ppo2.py:192][0m |          -0.0129 |           1.1747 |           0.1328 |
[32m[20230207 15:15:58 @agent_ppo2.py:192][0m |          -0.0119 |           1.1586 |           0.1327 |
[32m[20230207 15:15:58 @agent_ppo2.py:192][0m |          -0.0137 |           1.1484 |           0.1326 |
[32m[20230207 15:15:58 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:15:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: -44.26
[32m[20230207 15:15:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 13.85
[32m[20230207 15:15:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 19.63
[32m[20230207 15:15:59 @agent_ppo2.py:150][0m Total time:      15.29 min
[32m[20230207 15:15:59 @agent_ppo2.py:152][0m 790528 total steps have happened
[32m[20230207 15:15:59 @agent_ppo2.py:128][0m #------------------------ Iteration 386 --------------------------#
[32m[20230207 15:15:59 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:15:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:00 @agent_ppo2.py:192][0m |          -0.0042 |           7.6110 |           0.1355 |
[32m[20230207 15:16:00 @agent_ppo2.py:192][0m |          -0.0051 |           2.1803 |           0.1353 |
[32m[20230207 15:16:00 @agent_ppo2.py:192][0m |          -0.0072 |           1.8332 |           0.1351 |
[32m[20230207 15:16:00 @agent_ppo2.py:192][0m |          -0.0096 |           1.6838 |           0.1351 |
[32m[20230207 15:16:00 @agent_ppo2.py:192][0m |          -0.0106 |           1.6095 |           0.1351 |
[32m[20230207 15:16:00 @agent_ppo2.py:192][0m |          -0.0116 |           1.5421 |           0.1349 |
[32m[20230207 15:16:00 @agent_ppo2.py:192][0m |          -0.0118 |           1.5033 |           0.1349 |
[32m[20230207 15:16:00 @agent_ppo2.py:192][0m |          -0.0036 |           1.4687 |           0.1348 |
[32m[20230207 15:16:00 @agent_ppo2.py:192][0m |          -0.0117 |           1.4372 |           0.1346 |
[32m[20230207 15:16:00 @agent_ppo2.py:192][0m |          -0.0115 |           1.4178 |           0.1345 |
[32m[20230207 15:16:00 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:16:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -1.23
[32m[20230207 15:16:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 96.27
[32m[20230207 15:16:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 11.75
[32m[20230207 15:16:01 @agent_ppo2.py:150][0m Total time:      15.33 min
[32m[20230207 15:16:01 @agent_ppo2.py:152][0m 792576 total steps have happened
[32m[20230207 15:16:01 @agent_ppo2.py:128][0m #------------------------ Iteration 387 --------------------------#
[32m[20230207 15:16:02 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:16:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:02 @agent_ppo2.py:192][0m |          -0.0012 |          27.8659 |           0.1311 |
[32m[20230207 15:16:02 @agent_ppo2.py:192][0m |          -0.0062 |          10.2782 |           0.1311 |
[32m[20230207 15:16:02 @agent_ppo2.py:192][0m |          -0.0055 |           7.5113 |           0.1311 |
[32m[20230207 15:16:02 @agent_ppo2.py:192][0m |          -0.0084 |           5.4638 |           0.1311 |
[32m[20230207 15:16:02 @agent_ppo2.py:192][0m |          -0.0105 |           4.5552 |           0.1310 |
[32m[20230207 15:16:03 @agent_ppo2.py:192][0m |          -0.0106 |           3.6730 |           0.1310 |
[32m[20230207 15:16:03 @agent_ppo2.py:192][0m |          -0.0124 |           3.8295 |           0.1310 |
[32m[20230207 15:16:03 @agent_ppo2.py:192][0m |          -0.0123 |           3.0238 |           0.1310 |
[32m[20230207 15:16:03 @agent_ppo2.py:192][0m |          -0.0128 |           2.5930 |           0.1310 |
[32m[20230207 15:16:03 @agent_ppo2.py:192][0m |          -0.0128 |           2.9671 |           0.1310 |
[32m[20230207 15:16:03 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:16:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: -76.66
[32m[20230207 15:16:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 15.62
[32m[20230207 15:16:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -27.12
[32m[20230207 15:16:04 @agent_ppo2.py:150][0m Total time:      15.37 min
[32m[20230207 15:16:04 @agent_ppo2.py:152][0m 794624 total steps have happened
[32m[20230207 15:16:04 @agent_ppo2.py:128][0m #------------------------ Iteration 388 --------------------------#
[32m[20230207 15:16:05 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:16:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:05 @agent_ppo2.py:192][0m |          -0.0001 |          16.3142 |           0.1286 |
[32m[20230207 15:16:05 @agent_ppo2.py:192][0m |          -0.0037 |           5.8707 |           0.1286 |
[32m[20230207 15:16:05 @agent_ppo2.py:192][0m |          -0.0047 |           4.7494 |           0.1286 |
[32m[20230207 15:16:05 @agent_ppo2.py:192][0m |          -0.0067 |           4.1788 |           0.1286 |
[32m[20230207 15:16:05 @agent_ppo2.py:192][0m |          -0.0087 |           3.8637 |           0.1285 |
[32m[20230207 15:16:05 @agent_ppo2.py:192][0m |          -0.0088 |           3.7116 |           0.1285 |
[32m[20230207 15:16:05 @agent_ppo2.py:192][0m |          -0.0090 |           3.3612 |           0.1283 |
[32m[20230207 15:16:05 @agent_ppo2.py:192][0m |          -0.0107 |           3.2301 |           0.1283 |
[32m[20230207 15:16:05 @agent_ppo2.py:192][0m |          -0.0110 |           3.1173 |           0.1284 |
[32m[20230207 15:16:05 @agent_ppo2.py:192][0m |          -0.0123 |           2.9801 |           0.1283 |
[32m[20230207 15:16:05 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:16:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: -58.10
[32m[20230207 15:16:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 24.98
[32m[20230207 15:16:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 6.57
[32m[20230207 15:16:06 @agent_ppo2.py:150][0m Total time:      15.42 min
[32m[20230207 15:16:06 @agent_ppo2.py:152][0m 796672 total steps have happened
[32m[20230207 15:16:06 @agent_ppo2.py:128][0m #------------------------ Iteration 389 --------------------------#
[32m[20230207 15:16:07 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:16:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:07 @agent_ppo2.py:192][0m |           0.0003 |          18.7850 |           0.1299 |
[32m[20230207 15:16:07 @agent_ppo2.py:192][0m |          -0.0046 |           8.5047 |           0.1297 |
[32m[20230207 15:16:07 @agent_ppo2.py:192][0m |          -0.0074 |           6.9085 |           0.1297 |
[32m[20230207 15:16:07 @agent_ppo2.py:192][0m |          -0.0081 |           5.9674 |           0.1296 |
[32m[20230207 15:16:08 @agent_ppo2.py:192][0m |          -0.0086 |           5.5853 |           0.1297 |
[32m[20230207 15:16:08 @agent_ppo2.py:192][0m |          -0.0097 |           5.3593 |           0.1296 |
[32m[20230207 15:16:08 @agent_ppo2.py:192][0m |          -0.0103 |           5.0738 |           0.1295 |
[32m[20230207 15:16:08 @agent_ppo2.py:192][0m |          -0.0113 |           4.8847 |           0.1295 |
[32m[20230207 15:16:08 @agent_ppo2.py:192][0m |          -0.0119 |           4.7845 |           0.1295 |
[32m[20230207 15:16:08 @agent_ppo2.py:192][0m |          -0.0124 |           4.5741 |           0.1294 |
[32m[20230207 15:16:08 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:16:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: -14.78
[32m[20230207 15:16:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 94.96
[32m[20230207 15:16:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -33.01
[32m[20230207 15:16:09 @agent_ppo2.py:150][0m Total time:      15.46 min
[32m[20230207 15:16:09 @agent_ppo2.py:152][0m 798720 total steps have happened
[32m[20230207 15:16:09 @agent_ppo2.py:128][0m #------------------------ Iteration 390 --------------------------#
[32m[20230207 15:16:09 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:16:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:10 @agent_ppo2.py:192][0m |           0.0015 |          11.5210 |           0.1300 |
[32m[20230207 15:16:10 @agent_ppo2.py:192][0m |          -0.0044 |           5.4439 |           0.1300 |
[32m[20230207 15:16:10 @agent_ppo2.py:192][0m |          -0.0036 |           4.7094 |           0.1300 |
[32m[20230207 15:16:10 @agent_ppo2.py:192][0m |          -0.0058 |           4.3485 |           0.1300 |
[32m[20230207 15:16:10 @agent_ppo2.py:192][0m |          -0.0082 |           4.1021 |           0.1301 |
[32m[20230207 15:16:10 @agent_ppo2.py:192][0m |          -0.0092 |           3.8690 |           0.1300 |
[32m[20230207 15:16:10 @agent_ppo2.py:192][0m |          -0.0081 |           3.6625 |           0.1300 |
[32m[20230207 15:16:10 @agent_ppo2.py:192][0m |          -0.0107 |           3.4803 |           0.1300 |
[32m[20230207 15:16:10 @agent_ppo2.py:192][0m |          -0.0117 |           3.3597 |           0.1300 |
[32m[20230207 15:16:10 @agent_ppo2.py:192][0m |          -0.0124 |           3.2297 |           0.1300 |
[32m[20230207 15:16:10 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:16:11 @agent_ppo2.py:145][0m Average TRAINING episode reward: 44.21
[32m[20230207 15:16:11 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 130.29
[32m[20230207 15:16:11 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 8.80
[32m[20230207 15:16:11 @agent_ppo2.py:150][0m Total time:      15.50 min
[32m[20230207 15:16:11 @agent_ppo2.py:152][0m 800768 total steps have happened
[32m[20230207 15:16:11 @agent_ppo2.py:128][0m #------------------------ Iteration 391 --------------------------#
[32m[20230207 15:16:12 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:16:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:12 @agent_ppo2.py:192][0m |          -0.0134 |           3.0278 |           0.1275 |
[32m[20230207 15:16:12 @agent_ppo2.py:192][0m |           0.0104 |           1.5240 |           0.1274 |
[32m[20230207 15:16:12 @agent_ppo2.py:192][0m |          -0.0084 |           1.3968 |           0.1274 |
[32m[20230207 15:16:12 @agent_ppo2.py:192][0m |           0.0122 |           1.3250 |           0.1273 |
[32m[20230207 15:16:12 @agent_ppo2.py:192][0m |           0.0213 |           1.3156 |           0.1273 |
[32m[20230207 15:16:12 @agent_ppo2.py:192][0m |          -0.0106 |           1.2678 |           0.1274 |
[32m[20230207 15:16:12 @agent_ppo2.py:192][0m |          -0.0150 |           1.2263 |           0.1274 |
[32m[20230207 15:16:13 @agent_ppo2.py:192][0m |           0.0119 |           1.2201 |           0.1273 |
[32m[20230207 15:16:13 @agent_ppo2.py:192][0m |          -0.0010 |           1.1916 |           0.1272 |
[32m[20230207 15:16:13 @agent_ppo2.py:192][0m |           0.0003 |           1.1696 |           0.1272 |
[32m[20230207 15:16:13 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:16:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: 74.14
[32m[20230207 15:16:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 95.62
[32m[20230207 15:16:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -64.99
[32m[20230207 15:16:14 @agent_ppo2.py:150][0m Total time:      15.54 min
[32m[20230207 15:16:14 @agent_ppo2.py:152][0m 802816 total steps have happened
[32m[20230207 15:16:14 @agent_ppo2.py:128][0m #------------------------ Iteration 392 --------------------------#
[32m[20230207 15:16:14 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:16:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:14 @agent_ppo2.py:192][0m |          -0.0063 |           7.0845 |           0.1317 |
[32m[20230207 15:16:15 @agent_ppo2.py:192][0m |          -0.0237 |           4.3121 |           0.1315 |
[32m[20230207 15:16:15 @agent_ppo2.py:192][0m |          -0.0032 |           3.9372 |           0.1311 |
[32m[20230207 15:16:15 @agent_ppo2.py:192][0m |          -0.0485 |           4.6503 |           0.1311 |
[32m[20230207 15:16:15 @agent_ppo2.py:192][0m |           0.0005 |           3.8182 |           0.1309 |
[32m[20230207 15:16:15 @agent_ppo2.py:192][0m |          -0.0130 |           3.5199 |           0.1311 |
[32m[20230207 15:16:15 @agent_ppo2.py:192][0m |           0.0023 |           3.3822 |           0.1310 |
[32m[20230207 15:16:15 @agent_ppo2.py:192][0m |          -0.0416 |           3.2497 |           0.1311 |
[32m[20230207 15:16:15 @agent_ppo2.py:192][0m |          -0.0062 |           3.1875 |           0.1310 |
[32m[20230207 15:16:15 @agent_ppo2.py:192][0m |          -0.0035 |           3.1329 |           0.1311 |
[32m[20230207 15:16:15 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:16:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: 92.91
[32m[20230207 15:16:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 108.25
[32m[20230207 15:16:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -42.67
[32m[20230207 15:16:16 @agent_ppo2.py:150][0m Total time:      15.58 min
[32m[20230207 15:16:16 @agent_ppo2.py:152][0m 804864 total steps have happened
[32m[20230207 15:16:16 @agent_ppo2.py:128][0m #------------------------ Iteration 393 --------------------------#
[32m[20230207 15:16:17 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:16:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:17 @agent_ppo2.py:192][0m |          -0.0007 |          11.8443 |           0.1325 |
[32m[20230207 15:16:17 @agent_ppo2.py:192][0m |          -0.0033 |           2.2475 |           0.1323 |
[32m[20230207 15:16:17 @agent_ppo2.py:192][0m |          -0.0052 |           1.7533 |           0.1323 |
[32m[20230207 15:16:17 @agent_ppo2.py:192][0m |          -0.0064 |           1.5920 |           0.1323 |
[32m[20230207 15:16:17 @agent_ppo2.py:192][0m |          -0.0071 |           1.4914 |           0.1323 |
[32m[20230207 15:16:17 @agent_ppo2.py:192][0m |          -0.0081 |           1.4209 |           0.1322 |
[32m[20230207 15:16:17 @agent_ppo2.py:192][0m |          -0.0082 |           1.3682 |           0.1322 |
[32m[20230207 15:16:17 @agent_ppo2.py:192][0m |          -0.0092 |           1.3404 |           0.1322 |
[32m[20230207 15:16:18 @agent_ppo2.py:192][0m |          -0.0095 |           1.3086 |           0.1322 |
[32m[20230207 15:16:18 @agent_ppo2.py:192][0m |          -0.0100 |           1.2917 |           0.1322 |
[32m[20230207 15:16:18 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:16:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -0.58
[32m[20230207 15:16:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 94.21
[32m[20230207 15:16:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -14.41
[32m[20230207 15:16:18 @agent_ppo2.py:150][0m Total time:      15.62 min
[32m[20230207 15:16:18 @agent_ppo2.py:152][0m 806912 total steps have happened
[32m[20230207 15:16:18 @agent_ppo2.py:128][0m #------------------------ Iteration 394 --------------------------#
[32m[20230207 15:16:19 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:16:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:19 @agent_ppo2.py:192][0m |          -0.0013 |           2.8977 |           0.1265 |
[32m[20230207 15:16:19 @agent_ppo2.py:192][0m |          -0.0034 |           2.3628 |           0.1263 |
[32m[20230207 15:16:19 @agent_ppo2.py:192][0m |           0.0017 |           2.2468 |           0.1262 |
[32m[20230207 15:16:19 @agent_ppo2.py:192][0m |           0.0016 |           2.2153 |           0.1260 |
[32m[20230207 15:16:19 @agent_ppo2.py:192][0m |          -0.0144 |           2.1474 |           0.1259 |
[32m[20230207 15:16:20 @agent_ppo2.py:192][0m |          -0.0090 |           2.0926 |           0.1257 |
[32m[20230207 15:16:20 @agent_ppo2.py:192][0m |          -0.0211 |           2.0666 |           0.1257 |
[32m[20230207 15:16:20 @agent_ppo2.py:192][0m |          -0.0113 |           2.0487 |           0.1255 |
[32m[20230207 15:16:20 @agent_ppo2.py:192][0m |           0.0112 |           2.0266 |           0.1256 |
[32m[20230207 15:16:20 @agent_ppo2.py:192][0m |          -0.0132 |           1.9797 |           0.1256 |
[32m[20230207 15:16:20 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:16:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -0.38
[32m[20230207 15:16:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 34.07
[32m[20230207 15:16:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 60.37
[32m[20230207 15:16:21 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 60.37
[32m[20230207 15:16:21 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 60.37
[32m[20230207 15:16:21 @agent_ppo2.py:150][0m Total time:      15.66 min
[32m[20230207 15:16:21 @agent_ppo2.py:152][0m 808960 total steps have happened
[32m[20230207 15:16:21 @agent_ppo2.py:128][0m #------------------------ Iteration 395 --------------------------#
[32m[20230207 15:16:21 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:16:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:22 @agent_ppo2.py:192][0m |           0.0014 |           2.2871 |           0.1287 |
[32m[20230207 15:16:22 @agent_ppo2.py:192][0m |          -0.0169 |           1.6136 |           0.1287 |
[32m[20230207 15:16:22 @agent_ppo2.py:192][0m |          -0.0176 |           1.5032 |           0.1287 |
[32m[20230207 15:16:22 @agent_ppo2.py:192][0m |          -0.0130 |           1.4383 |           0.1287 |
[32m[20230207 15:16:22 @agent_ppo2.py:192][0m |          -0.0127 |           1.4150 |           0.1289 |
[32m[20230207 15:16:22 @agent_ppo2.py:192][0m |          -0.0125 |           1.3415 |           0.1289 |
[32m[20230207 15:16:22 @agent_ppo2.py:192][0m |          -0.0260 |           1.3264 |           0.1290 |
[32m[20230207 15:16:22 @agent_ppo2.py:192][0m |          -0.0234 |           1.2916 |           0.1289 |
[32m[20230207 15:16:22 @agent_ppo2.py:192][0m |          -0.0076 |           1.2664 |           0.1290 |
[32m[20230207 15:16:22 @agent_ppo2.py:192][0m |          -0.0320 |           1.2650 |           0.1290 |
[32m[20230207 15:16:22 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:16:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: 80.71
[32m[20230207 15:16:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 110.22
[32m[20230207 15:16:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -35.72
[32m[20230207 15:16:23 @agent_ppo2.py:150][0m Total time:      15.70 min
[32m[20230207 15:16:23 @agent_ppo2.py:152][0m 811008 total steps have happened
[32m[20230207 15:16:23 @agent_ppo2.py:128][0m #------------------------ Iteration 396 --------------------------#
[32m[20230207 15:16:24 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:16:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:24 @agent_ppo2.py:192][0m |          -0.0002 |           6.0779 |           0.1315 |
[32m[20230207 15:16:24 @agent_ppo2.py:192][0m |          -0.0026 |           2.6606 |           0.1315 |
[32m[20230207 15:16:24 @agent_ppo2.py:192][0m |          -0.0024 |           2.4037 |           0.1316 |
[32m[20230207 15:16:24 @agent_ppo2.py:192][0m |          -0.0030 |           2.2137 |           0.1317 |
[32m[20230207 15:16:24 @agent_ppo2.py:192][0m |          -0.0010 |           2.1556 |           0.1317 |
[32m[20230207 15:16:24 @agent_ppo2.py:192][0m |          -0.0050 |           2.0228 |           0.1318 |
[32m[20230207 15:16:25 @agent_ppo2.py:192][0m |          -0.0050 |           1.9652 |           0.1318 |
[32m[20230207 15:16:25 @agent_ppo2.py:192][0m |          -0.0055 |           1.9025 |           0.1318 |
[32m[20230207 15:16:25 @agent_ppo2.py:192][0m |          -0.0066 |           1.8911 |           0.1318 |
[32m[20230207 15:16:25 @agent_ppo2.py:192][0m |          -0.0067 |           1.8124 |           0.1319 |
[32m[20230207 15:16:25 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:16:26 @agent_ppo2.py:145][0m Average TRAINING episode reward: -2.01
[32m[20230207 15:16:26 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 65.06
[32m[20230207 15:16:26 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -1.08
[32m[20230207 15:16:26 @agent_ppo2.py:150][0m Total time:      15.74 min
[32m[20230207 15:16:26 @agent_ppo2.py:152][0m 813056 total steps have happened
[32m[20230207 15:16:26 @agent_ppo2.py:128][0m #------------------------ Iteration 397 --------------------------#
[32m[20230207 15:16:26 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230207 15:16:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:26 @agent_ppo2.py:192][0m |           0.0002 |          12.3985 |           0.1307 |
[32m[20230207 15:16:26 @agent_ppo2.py:192][0m |           0.0050 |           4.4728 |           0.1306 |
[32m[20230207 15:16:26 @agent_ppo2.py:192][0m |          -0.0018 |           3.0764 |           0.1306 |
[32m[20230207 15:16:26 @agent_ppo2.py:192][0m |          -0.0042 |           2.8071 |           0.1305 |
[32m[20230207 15:16:27 @agent_ppo2.py:192][0m |          -0.0046 |           2.1914 |           0.1305 |
[32m[20230207 15:16:27 @agent_ppo2.py:192][0m |          -0.0103 |           2.0282 |           0.1303 |
[32m[20230207 15:16:27 @agent_ppo2.py:192][0m |          -0.0120 |           1.8623 |           0.1304 |
[32m[20230207 15:16:27 @agent_ppo2.py:192][0m |          -0.0118 |           1.7917 |           0.1303 |
[32m[20230207 15:16:27 @agent_ppo2.py:192][0m |          -0.0046 |           1.7239 |           0.1304 |
[32m[20230207 15:16:27 @agent_ppo2.py:192][0m |          -0.0113 |           1.6000 |           0.1303 |
[32m[20230207 15:16:27 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:16:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -68.85
[32m[20230207 15:16:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 5.65
[32m[20230207 15:16:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -62.58
[32m[20230207 15:16:27 @agent_ppo2.py:150][0m Total time:      15.77 min
[32m[20230207 15:16:27 @agent_ppo2.py:152][0m 815104 total steps have happened
[32m[20230207 15:16:27 @agent_ppo2.py:128][0m #------------------------ Iteration 398 --------------------------#
[32m[20230207 15:16:28 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:16:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:28 @agent_ppo2.py:192][0m |          -0.0035 |          22.2531 |           0.1286 |
[32m[20230207 15:16:28 @agent_ppo2.py:192][0m |          -0.0108 |          14.7166 |           0.1283 |
[32m[20230207 15:16:28 @agent_ppo2.py:192][0m |          -0.0142 |          13.5167 |           0.1283 |
[32m[20230207 15:16:28 @agent_ppo2.py:192][0m |          -0.0163 |          12.1755 |           0.1283 |
[32m[20230207 15:16:28 @agent_ppo2.py:192][0m |          -0.0162 |          11.5896 |           0.1283 |
[32m[20230207 15:16:28 @agent_ppo2.py:192][0m |           0.0047 |          11.1167 |           0.1282 |
[32m[20230207 15:16:28 @agent_ppo2.py:192][0m |          -0.0178 |          10.6899 |           0.1281 |
[32m[20230207 15:16:28 @agent_ppo2.py:192][0m |          -0.0189 |          10.5447 |           0.1281 |
[32m[20230207 15:16:28 @agent_ppo2.py:192][0m |          -0.0185 |          10.4245 |           0.1281 |
[32m[20230207 15:16:29 @agent_ppo2.py:192][0m |          -0.0313 |          10.5652 |           0.1280 |
[32m[20230207 15:16:29 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:16:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -56.13
[32m[20230207 15:16:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 38.50
[32m[20230207 15:16:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -5.62
[32m[20230207 15:16:29 @agent_ppo2.py:150][0m Total time:      15.80 min
[32m[20230207 15:16:29 @agent_ppo2.py:152][0m 817152 total steps have happened
[32m[20230207 15:16:29 @agent_ppo2.py:128][0m #------------------------ Iteration 399 --------------------------#
[32m[20230207 15:16:30 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:16:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:30 @agent_ppo2.py:192][0m |          -0.0265 |           3.5466 |           0.1284 |
[32m[20230207 15:16:30 @agent_ppo2.py:192][0m |          -0.0014 |           2.3755 |           0.1280 |
[32m[20230207 15:16:30 @agent_ppo2.py:192][0m |          -0.0080 |           2.1532 |           0.1280 |
[32m[20230207 15:16:30 @agent_ppo2.py:192][0m |          -0.0009 |           1.9868 |           0.1281 |
[32m[20230207 15:16:31 @agent_ppo2.py:192][0m |          -0.0053 |           1.8867 |           0.1280 |
[32m[20230207 15:16:31 @agent_ppo2.py:192][0m |           0.0040 |           1.8158 |           0.1280 |
[32m[20230207 15:16:31 @agent_ppo2.py:192][0m |          -0.0004 |           1.7401 |           0.1280 |
[32m[20230207 15:16:31 @agent_ppo2.py:192][0m |          -0.0055 |           1.6831 |           0.1280 |
[32m[20230207 15:16:31 @agent_ppo2.py:192][0m |          -0.0464 |           1.6486 |           0.1280 |
[32m[20230207 15:16:31 @agent_ppo2.py:192][0m |           0.0070 |           1.5865 |           0.1279 |
[32m[20230207 15:16:31 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:16:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -25.00
[32m[20230207 15:16:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 21.07
[32m[20230207 15:16:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -3.48
[32m[20230207 15:16:31 @agent_ppo2.py:150][0m Total time:      15.84 min
[32m[20230207 15:16:31 @agent_ppo2.py:152][0m 819200 total steps have happened
[32m[20230207 15:16:31 @agent_ppo2.py:128][0m #------------------------ Iteration 400 --------------------------#
[32m[20230207 15:16:32 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:16:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:32 @agent_ppo2.py:192][0m |           0.0137 |           2.5428 |           0.1299 |
[32m[20230207 15:16:32 @agent_ppo2.py:192][0m |           0.0072 |           1.7964 |           0.1298 |
[32m[20230207 15:16:33 @agent_ppo2.py:192][0m |          -0.0225 |           1.7312 |           0.1297 |
[32m[20230207 15:16:33 @agent_ppo2.py:192][0m |          -0.0146 |           1.6967 |           0.1297 |
[32m[20230207 15:16:33 @agent_ppo2.py:192][0m |           0.0058 |           1.6298 |           0.1297 |
[32m[20230207 15:16:33 @agent_ppo2.py:192][0m |          -0.0113 |           1.5885 |           0.1297 |
[32m[20230207 15:16:33 @agent_ppo2.py:192][0m |          -0.0420 |           1.5496 |           0.1297 |
[32m[20230207 15:16:33 @agent_ppo2.py:192][0m |          -0.0374 |           1.5569 |           0.1296 |
[32m[20230207 15:16:33 @agent_ppo2.py:192][0m |          -0.0259 |           1.5315 |           0.1296 |
[32m[20230207 15:16:33 @agent_ppo2.py:192][0m |          -0.0031 |           1.5041 |           0.1297 |
[32m[20230207 15:16:33 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:16:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: 47.99
[32m[20230207 15:16:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 86.77
[32m[20230207 15:16:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -20.32
[32m[20230207 15:16:34 @agent_ppo2.py:150][0m Total time:      15.88 min
[32m[20230207 15:16:34 @agent_ppo2.py:152][0m 821248 total steps have happened
[32m[20230207 15:16:34 @agent_ppo2.py:128][0m #------------------------ Iteration 401 --------------------------#
[32m[20230207 15:16:35 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:16:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:35 @agent_ppo2.py:192][0m |          -0.0008 |           3.9196 |           0.1333 |
[32m[20230207 15:16:35 @agent_ppo2.py:192][0m |          -0.0041 |           2.4058 |           0.1331 |
[32m[20230207 15:16:35 @agent_ppo2.py:192][0m |          -0.0073 |           2.1642 |           0.1330 |
[32m[20230207 15:16:35 @agent_ppo2.py:192][0m |          -0.0073 |           2.0410 |           0.1330 |
[32m[20230207 15:16:35 @agent_ppo2.py:192][0m |          -0.0070 |           1.9648 |           0.1330 |
[32m[20230207 15:16:35 @agent_ppo2.py:192][0m |          -0.0093 |           1.8920 |           0.1329 |
[32m[20230207 15:16:35 @agent_ppo2.py:192][0m |          -0.0042 |           1.8474 |           0.1328 |
[32m[20230207 15:16:35 @agent_ppo2.py:192][0m |          -0.0101 |           1.8217 |           0.1328 |
[32m[20230207 15:16:35 @agent_ppo2.py:192][0m |          -0.0152 |           1.8225 |           0.1327 |
[32m[20230207 15:16:36 @agent_ppo2.py:192][0m |          -0.0098 |           1.7721 |           0.1327 |
[32m[20230207 15:16:36 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:16:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -36.73
[32m[20230207 15:16:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 8.78
[32m[20230207 15:16:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 28.70
[32m[20230207 15:16:36 @agent_ppo2.py:150][0m Total time:      15.92 min
[32m[20230207 15:16:36 @agent_ppo2.py:152][0m 823296 total steps have happened
[32m[20230207 15:16:36 @agent_ppo2.py:128][0m #------------------------ Iteration 402 --------------------------#
[32m[20230207 15:16:37 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:16:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:37 @agent_ppo2.py:192][0m |          -0.0011 |           6.4790 |           0.1329 |
[32m[20230207 15:16:37 @agent_ppo2.py:192][0m |          -0.0034 |           4.4029 |           0.1327 |
[32m[20230207 15:16:37 @agent_ppo2.py:192][0m |          -0.0025 |           3.4942 |           0.1328 |
[32m[20230207 15:16:38 @agent_ppo2.py:192][0m |          -0.0063 |           3.4044 |           0.1327 |
[32m[20230207 15:16:38 @agent_ppo2.py:192][0m |          -0.0083 |           3.3783 |           0.1327 |
[32m[20230207 15:16:38 @agent_ppo2.py:192][0m |          -0.0101 |           3.4816 |           0.1326 |
[32m[20230207 15:16:38 @agent_ppo2.py:192][0m |          -0.0081 |           3.2465 |           0.1326 |
[32m[20230207 15:16:38 @agent_ppo2.py:192][0m |          -0.0074 |           3.1432 |           0.1326 |
[32m[20230207 15:16:38 @agent_ppo2.py:192][0m |          -0.0097 |           3.3357 |           0.1326 |
[32m[20230207 15:16:38 @agent_ppo2.py:192][0m |          -0.0110 |           3.3070 |           0.1325 |
[32m[20230207 15:16:38 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:16:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -6.16
[32m[20230207 15:16:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 71.34
[32m[20230207 15:16:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 13.51
[32m[20230207 15:16:39 @agent_ppo2.py:150][0m Total time:      15.96 min
[32m[20230207 15:16:39 @agent_ppo2.py:152][0m 825344 total steps have happened
[32m[20230207 15:16:39 @agent_ppo2.py:128][0m #------------------------ Iteration 403 --------------------------#
[32m[20230207 15:16:40 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:16:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:40 @agent_ppo2.py:192][0m |          -0.0622 |           1.6554 |           0.1329 |
[32m[20230207 15:16:40 @agent_ppo2.py:192][0m |           0.0056 |           1.2487 |           0.1327 |
[32m[20230207 15:16:40 @agent_ppo2.py:192][0m |          -0.0076 |           1.1806 |           0.1330 |
[32m[20230207 15:16:40 @agent_ppo2.py:192][0m |          -0.0146 |           1.1273 |           0.1330 |
[32m[20230207 15:16:40 @agent_ppo2.py:192][0m |          -0.0043 |           1.0978 |           0.1331 |
[32m[20230207 15:16:40 @agent_ppo2.py:192][0m |          -0.0114 |           1.0722 |           0.1332 |
[32m[20230207 15:16:40 @agent_ppo2.py:192][0m |           0.0101 |           1.0715 |           0.1332 |
[32m[20230207 15:16:40 @agent_ppo2.py:192][0m |          -0.0052 |           1.0438 |           0.1333 |
[32m[20230207 15:16:40 @agent_ppo2.py:192][0m |          -0.0036 |           1.0236 |           0.1333 |
[32m[20230207 15:16:40 @agent_ppo2.py:192][0m |          -0.0088 |           1.0035 |           0.1334 |
[32m[20230207 15:16:40 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:16:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: 63.40
[32m[20230207 15:16:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 78.58
[32m[20230207 15:16:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -48.26
[32m[20230207 15:16:41 @agent_ppo2.py:150][0m Total time:      16.00 min
[32m[20230207 15:16:41 @agent_ppo2.py:152][0m 827392 total steps have happened
[32m[20230207 15:16:41 @agent_ppo2.py:128][0m #------------------------ Iteration 404 --------------------------#
[32m[20230207 15:16:42 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:16:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:42 @agent_ppo2.py:192][0m |          -0.0028 |           1.0391 |           0.1301 |
[32m[20230207 15:16:42 @agent_ppo2.py:192][0m |          -0.0108 |           0.8696 |           0.1301 |
[32m[20230207 15:16:42 @agent_ppo2.py:192][0m |          -0.0057 |           0.8305 |           0.1300 |
[32m[20230207 15:16:42 @agent_ppo2.py:192][0m |          -0.0035 |           0.7984 |           0.1301 |
[32m[20230207 15:16:43 @agent_ppo2.py:192][0m |           0.0143 |           0.7919 |           0.1300 |
[32m[20230207 15:16:43 @agent_ppo2.py:192][0m |          -0.0048 |           0.7672 |           0.1299 |
[32m[20230207 15:16:43 @agent_ppo2.py:192][0m |           0.0045 |           0.7464 |           0.1300 |
[32m[20230207 15:16:43 @agent_ppo2.py:192][0m |          -0.0083 |           0.7301 |           0.1299 |
[32m[20230207 15:16:43 @agent_ppo2.py:192][0m |          -0.0029 |           0.7203 |           0.1299 |
[32m[20230207 15:16:43 @agent_ppo2.py:192][0m |          -0.0089 |           0.7121 |           0.1299 |
[32m[20230207 15:16:43 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:16:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -7.35
[32m[20230207 15:16:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 34.43
[32m[20230207 15:16:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -69.71
[32m[20230207 15:16:43 @agent_ppo2.py:150][0m Total time:      16.04 min
[32m[20230207 15:16:43 @agent_ppo2.py:152][0m 829440 total steps have happened
[32m[20230207 15:16:43 @agent_ppo2.py:128][0m #------------------------ Iteration 405 --------------------------#
[32m[20230207 15:16:44 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:16:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:44 @agent_ppo2.py:192][0m |           0.0018 |          42.9249 |           0.1362 |
[32m[20230207 15:16:44 @agent_ppo2.py:192][0m |          -0.0048 |          20.5839 |           0.1360 |
[32m[20230207 15:16:45 @agent_ppo2.py:192][0m |          -0.0056 |          15.2434 |           0.1358 |
[32m[20230207 15:16:45 @agent_ppo2.py:192][0m |          -0.0070 |          13.9081 |           0.1359 |
[32m[20230207 15:16:45 @agent_ppo2.py:192][0m |          -0.0090 |          12.0111 |           0.1356 |
[32m[20230207 15:16:45 @agent_ppo2.py:192][0m |          -0.0104 |          10.9566 |           0.1358 |
[32m[20230207 15:16:45 @agent_ppo2.py:192][0m |          -0.0107 |           9.9147 |           0.1355 |
[32m[20230207 15:16:45 @agent_ppo2.py:192][0m |          -0.0125 |           8.8899 |           0.1355 |
[32m[20230207 15:16:45 @agent_ppo2.py:192][0m |          -0.0140 |           7.9592 |           0.1355 |
[32m[20230207 15:16:45 @agent_ppo2.py:192][0m |          -0.0135 |           7.3999 |           0.1355 |
[32m[20230207 15:16:45 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:16:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -73.34
[32m[20230207 15:16:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 7.86
[32m[20230207 15:16:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -70.89
[32m[20230207 15:16:46 @agent_ppo2.py:150][0m Total time:      16.08 min
[32m[20230207 15:16:46 @agent_ppo2.py:152][0m 831488 total steps have happened
[32m[20230207 15:16:46 @agent_ppo2.py:128][0m #------------------------ Iteration 406 --------------------------#
[32m[20230207 15:16:47 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:16:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:47 @agent_ppo2.py:192][0m |           0.0134 |           1.8748 |           0.1309 |
[32m[20230207 15:16:47 @agent_ppo2.py:192][0m |          -0.0108 |           1.4853 |           0.1307 |
[32m[20230207 15:16:47 @agent_ppo2.py:192][0m |           0.0051 |           1.4250 |           0.1307 |
[32m[20230207 15:16:47 @agent_ppo2.py:192][0m |           0.0087 |           1.3727 |           0.1307 |
[32m[20230207 15:16:47 @agent_ppo2.py:192][0m |          -0.0006 |           1.3343 |           0.1307 |
[32m[20230207 15:16:47 @agent_ppo2.py:192][0m |          -0.0091 |           1.3043 |           0.1306 |
[32m[20230207 15:16:47 @agent_ppo2.py:192][0m |          -0.0021 |           1.3048 |           0.1307 |
[32m[20230207 15:16:47 @agent_ppo2.py:192][0m |           0.0126 |           1.2680 |           0.1307 |
[32m[20230207 15:16:47 @agent_ppo2.py:192][0m |          -0.0026 |           1.2309 |           0.1307 |
[32m[20230207 15:16:47 @agent_ppo2.py:192][0m |          -0.0157 |           1.2014 |           0.1308 |
[32m[20230207 15:16:47 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:16:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: 37.24
[32m[20230207 15:16:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 57.55
[32m[20230207 15:16:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 9.97
[32m[20230207 15:16:48 @agent_ppo2.py:150][0m Total time:      16.12 min
[32m[20230207 15:16:48 @agent_ppo2.py:152][0m 833536 total steps have happened
[32m[20230207 15:16:48 @agent_ppo2.py:128][0m #------------------------ Iteration 407 --------------------------#
[32m[20230207 15:16:49 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:16:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:49 @agent_ppo2.py:192][0m |           0.0006 |          16.8721 |           0.1342 |
[32m[20230207 15:16:49 @agent_ppo2.py:192][0m |          -0.0051 |           9.2572 |           0.1340 |
[32m[20230207 15:16:49 @agent_ppo2.py:192][0m |          -0.0079 |           7.9183 |           0.1341 |
[32m[20230207 15:16:49 @agent_ppo2.py:192][0m |          -0.0110 |           6.8969 |           0.1339 |
[32m[20230207 15:16:50 @agent_ppo2.py:192][0m |          -0.0101 |           6.3937 |           0.1340 |
[32m[20230207 15:16:50 @agent_ppo2.py:192][0m |          -0.0132 |           5.7802 |           0.1340 |
[32m[20230207 15:16:50 @agent_ppo2.py:192][0m |          -0.0132 |           5.4766 |           0.1339 |
[32m[20230207 15:16:50 @agent_ppo2.py:192][0m |          -0.0146 |           5.1681 |           0.1339 |
[32m[20230207 15:16:50 @agent_ppo2.py:192][0m |          -0.0133 |           4.9033 |           0.1339 |
[32m[20230207 15:16:50 @agent_ppo2.py:192][0m |          -0.0144 |           4.6844 |           0.1339 |
[32m[20230207 15:16:50 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:16:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: -25.03
[32m[20230207 15:16:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 68.00
[32m[20230207 15:16:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -43.24
[32m[20230207 15:16:51 @agent_ppo2.py:150][0m Total time:      16.16 min
[32m[20230207 15:16:51 @agent_ppo2.py:152][0m 835584 total steps have happened
[32m[20230207 15:16:51 @agent_ppo2.py:128][0m #------------------------ Iteration 408 --------------------------#
[32m[20230207 15:16:51 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:16:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:52 @agent_ppo2.py:192][0m |           0.0005 |          10.5120 |           0.1328 |
[32m[20230207 15:16:52 @agent_ppo2.py:192][0m |          -0.0024 |           4.5213 |           0.1326 |
[32m[20230207 15:16:52 @agent_ppo2.py:192][0m |          -0.0049 |           3.3441 |           0.1326 |
[32m[20230207 15:16:52 @agent_ppo2.py:192][0m |          -0.0064 |           2.8997 |           0.1326 |
[32m[20230207 15:16:52 @agent_ppo2.py:192][0m |          -0.0079 |           2.6232 |           0.1326 |
[32m[20230207 15:16:52 @agent_ppo2.py:192][0m |          -0.0085 |           2.4629 |           0.1327 |
[32m[20230207 15:16:52 @agent_ppo2.py:192][0m |          -0.0094 |           2.3316 |           0.1327 |
[32m[20230207 15:16:52 @agent_ppo2.py:192][0m |          -0.0100 |           2.2368 |           0.1327 |
[32m[20230207 15:16:52 @agent_ppo2.py:192][0m |          -0.0117 |           2.1822 |           0.1327 |
[32m[20230207 15:16:52 @agent_ppo2.py:192][0m |          -0.0118 |           2.1441 |           0.1328 |
[32m[20230207 15:16:52 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:16:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -42.60
[32m[20230207 15:16:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 39.16
[32m[20230207 15:16:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 54.53
[32m[20230207 15:16:53 @agent_ppo2.py:150][0m Total time:      16.20 min
[32m[20230207 15:16:53 @agent_ppo2.py:152][0m 837632 total steps have happened
[32m[20230207 15:16:53 @agent_ppo2.py:128][0m #------------------------ Iteration 409 --------------------------#
[32m[20230207 15:16:54 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:16:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:54 @agent_ppo2.py:192][0m |          -0.0003 |           5.5292 |           0.1368 |
[32m[20230207 15:16:54 @agent_ppo2.py:192][0m |          -0.0025 |           2.2331 |           0.1366 |
[32m[20230207 15:16:54 @agent_ppo2.py:192][0m |          -0.0064 |           2.0040 |           0.1366 |
[32m[20230207 15:16:54 @agent_ppo2.py:192][0m |          -0.0082 |           1.8723 |           0.1365 |
[32m[20230207 15:16:54 @agent_ppo2.py:192][0m |          -0.0113 |           1.7723 |           0.1366 |
[32m[20230207 15:16:55 @agent_ppo2.py:192][0m |          -0.0079 |           1.7048 |           0.1365 |
[32m[20230207 15:16:55 @agent_ppo2.py:192][0m |          -0.0097 |           1.6250 |           0.1365 |
[32m[20230207 15:16:55 @agent_ppo2.py:192][0m |          -0.0080 |           1.5671 |           0.1366 |
[32m[20230207 15:16:55 @agent_ppo2.py:192][0m |          -0.0119 |           1.5277 |           0.1366 |
[32m[20230207 15:16:55 @agent_ppo2.py:192][0m |          -0.0117 |           1.4883 |           0.1366 |
[32m[20230207 15:16:55 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:16:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: 2.69
[32m[20230207 15:16:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 117.58
[32m[20230207 15:16:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 43.99
[32m[20230207 15:16:56 @agent_ppo2.py:150][0m Total time:      16.24 min
[32m[20230207 15:16:56 @agent_ppo2.py:152][0m 839680 total steps have happened
[32m[20230207 15:16:56 @agent_ppo2.py:128][0m #------------------------ Iteration 410 --------------------------#
[32m[20230207 15:16:56 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:16:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:56 @agent_ppo2.py:192][0m |           0.0095 |          12.8453 |           0.1362 |
[32m[20230207 15:16:56 @agent_ppo2.py:192][0m |           0.0033 |           6.6209 |           0.1360 |
[32m[20230207 15:16:56 @agent_ppo2.py:192][0m |          -0.0077 |           5.4572 |           0.1359 |
[32m[20230207 15:16:56 @agent_ppo2.py:192][0m |          -0.0096 |           5.0780 |           0.1359 |
[32m[20230207 15:16:57 @agent_ppo2.py:192][0m |          -0.0026 |           4.9441 |           0.1359 |
[32m[20230207 15:16:57 @agent_ppo2.py:192][0m |          -0.0136 |           4.8763 |           0.1357 |
[32m[20230207 15:16:57 @agent_ppo2.py:192][0m |          -0.0117 |           4.3772 |           0.1358 |
[32m[20230207 15:16:57 @agent_ppo2.py:192][0m |          -0.0104 |           4.2708 |           0.1357 |
[32m[20230207 15:16:57 @agent_ppo2.py:192][0m |          -0.0098 |           4.1198 |           0.1356 |
[32m[20230207 15:16:57 @agent_ppo2.py:192][0m |          -0.0095 |           3.9942 |           0.1356 |
[32m[20230207 15:16:57 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:16:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: -69.48
[32m[20230207 15:16:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -1.32
[32m[20230207 15:16:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -25.87
[32m[20230207 15:16:58 @agent_ppo2.py:150][0m Total time:      16.27 min
[32m[20230207 15:16:58 @agent_ppo2.py:152][0m 841728 total steps have happened
[32m[20230207 15:16:58 @agent_ppo2.py:128][0m #------------------------ Iteration 411 --------------------------#
[32m[20230207 15:16:58 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:16:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:16:59 @agent_ppo2.py:192][0m |          -0.0213 |           1.8624 |           0.1348 |
[32m[20230207 15:16:59 @agent_ppo2.py:192][0m |           0.0056 |           1.3177 |           0.1347 |
[32m[20230207 15:16:59 @agent_ppo2.py:192][0m |          -0.0018 |           1.2085 |           0.1346 |
[32m[20230207 15:16:59 @agent_ppo2.py:192][0m |          -0.0070 |           1.1359 |           0.1345 |
[32m[20230207 15:16:59 @agent_ppo2.py:192][0m |          -0.0155 |           1.0995 |           0.1344 |
[32m[20230207 15:16:59 @agent_ppo2.py:192][0m |          -0.0075 |           1.0964 |           0.1343 |
[32m[20230207 15:16:59 @agent_ppo2.py:192][0m |          -0.0378 |           1.0624 |           0.1342 |
[32m[20230207 15:16:59 @agent_ppo2.py:192][0m |           0.0026 |           1.0384 |           0.1342 |
[32m[20230207 15:16:59 @agent_ppo2.py:192][0m |          -0.0154 |           1.0100 |           0.1340 |
[32m[20230207 15:16:59 @agent_ppo2.py:192][0m |          -0.0098 |           0.9977 |           0.1340 |
[32m[20230207 15:16:59 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:17:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: 32.40
[32m[20230207 15:17:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 46.54
[32m[20230207 15:17:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -33.68
[32m[20230207 15:17:00 @agent_ppo2.py:150][0m Total time:      16.31 min
[32m[20230207 15:17:00 @agent_ppo2.py:152][0m 843776 total steps have happened
[32m[20230207 15:17:00 @agent_ppo2.py:128][0m #------------------------ Iteration 412 --------------------------#
[32m[20230207 15:17:01 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:17:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:01 @agent_ppo2.py:192][0m |          -0.0012 |           6.2574 |           0.1341 |
[32m[20230207 15:17:01 @agent_ppo2.py:192][0m |          -0.0037 |           3.8316 |           0.1341 |
[32m[20230207 15:17:01 @agent_ppo2.py:192][0m |          -0.0076 |           3.3471 |           0.1341 |
[32m[20230207 15:17:01 @agent_ppo2.py:192][0m |          -0.0062 |           3.0897 |           0.1342 |
[32m[20230207 15:17:01 @agent_ppo2.py:192][0m |          -0.0098 |           2.9347 |           0.1341 |
[32m[20230207 15:17:01 @agent_ppo2.py:192][0m |          -0.0109 |           2.7514 |           0.1342 |
[32m[20230207 15:17:02 @agent_ppo2.py:192][0m |          -0.0127 |           2.7178 |           0.1342 |
[32m[20230207 15:17:02 @agent_ppo2.py:192][0m |          -0.0128 |           2.6127 |           0.1341 |
[32m[20230207 15:17:02 @agent_ppo2.py:192][0m |          -0.0142 |           2.5143 |           0.1342 |
[32m[20230207 15:17:02 @agent_ppo2.py:192][0m |          -0.0133 |           2.4669 |           0.1341 |
[32m[20230207 15:17:02 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:17:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -47.02
[32m[20230207 15:17:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 12.15
[32m[20230207 15:17:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 32.27
[32m[20230207 15:17:03 @agent_ppo2.py:150][0m Total time:      16.36 min
[32m[20230207 15:17:03 @agent_ppo2.py:152][0m 845824 total steps have happened
[32m[20230207 15:17:03 @agent_ppo2.py:128][0m #------------------------ Iteration 413 --------------------------#
[32m[20230207 15:17:03 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:17:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:03 @agent_ppo2.py:192][0m |          -0.0085 |           1.5311 |           0.1371 |
[32m[20230207 15:17:04 @agent_ppo2.py:192][0m |          -0.0147 |           1.0925 |           0.1370 |
[32m[20230207 15:17:04 @agent_ppo2.py:192][0m |          -0.0075 |           0.9865 |           0.1370 |
[32m[20230207 15:17:04 @agent_ppo2.py:192][0m |          -0.0089 |           0.9287 |           0.1369 |
[32m[20230207 15:17:04 @agent_ppo2.py:192][0m |          -0.0186 |           0.9017 |           0.1370 |
[32m[20230207 15:17:04 @agent_ppo2.py:192][0m |          -0.0139 |           0.8774 |           0.1370 |
[32m[20230207 15:17:04 @agent_ppo2.py:192][0m |          -0.0168 |           0.8621 |           0.1371 |
[32m[20230207 15:17:04 @agent_ppo2.py:192][0m |          -0.0044 |           0.8499 |           0.1371 |
[32m[20230207 15:17:04 @agent_ppo2.py:192][0m |          -0.0154 |           0.8320 |           0.1372 |
[32m[20230207 15:17:04 @agent_ppo2.py:192][0m |           0.0057 |           0.8279 |           0.1372 |
[32m[20230207 15:17:04 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:17:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: 42.73
[32m[20230207 15:17:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 76.36
[32m[20230207 15:17:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 0.64
[32m[20230207 15:17:05 @agent_ppo2.py:150][0m Total time:      16.39 min
[32m[20230207 15:17:05 @agent_ppo2.py:152][0m 847872 total steps have happened
[32m[20230207 15:17:05 @agent_ppo2.py:128][0m #------------------------ Iteration 414 --------------------------#
[32m[20230207 15:17:05 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230207 15:17:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:05 @agent_ppo2.py:192][0m |          -0.0016 |           9.0423 |           0.1373 |
[32m[20230207 15:17:06 @agent_ppo2.py:192][0m |          -0.0035 |           3.8663 |           0.1372 |
[32m[20230207 15:17:06 @agent_ppo2.py:192][0m |          -0.0049 |           3.1185 |           0.1373 |
[32m[20230207 15:17:06 @agent_ppo2.py:192][0m |          -0.0062 |           2.8149 |           0.1372 |
[32m[20230207 15:17:06 @agent_ppo2.py:192][0m |          -0.0091 |           2.5921 |           0.1371 |
[32m[20230207 15:17:06 @agent_ppo2.py:192][0m |          -0.0101 |           2.3713 |           0.1372 |
[32m[20230207 15:17:06 @agent_ppo2.py:192][0m |          -0.0095 |           2.1611 |           0.1372 |
[32m[20230207 15:17:06 @agent_ppo2.py:192][0m |          -0.0085 |           1.9832 |           0.1372 |
[32m[20230207 15:17:06 @agent_ppo2.py:192][0m |          -0.0083 |           2.0053 |           0.1371 |
[32m[20230207 15:17:06 @agent_ppo2.py:192][0m |          -0.0122 |           1.8508 |           0.1372 |
[32m[20230207 15:17:06 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:17:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -67.74
[32m[20230207 15:17:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 13.93
[32m[20230207 15:17:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -56.22
[32m[20230207 15:17:07 @agent_ppo2.py:150][0m Total time:      16.43 min
[32m[20230207 15:17:07 @agent_ppo2.py:152][0m 849920 total steps have happened
[32m[20230207 15:17:07 @agent_ppo2.py:128][0m #------------------------ Iteration 415 --------------------------#
[32m[20230207 15:17:08 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:17:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:08 @agent_ppo2.py:192][0m |           0.0032 |           6.4246 |           0.1409 |
[32m[20230207 15:17:08 @agent_ppo2.py:192][0m |          -0.0015 |           3.9080 |           0.1408 |
[32m[20230207 15:17:08 @agent_ppo2.py:192][0m |           0.0120 |           3.6030 |           0.1408 |
[32m[20230207 15:17:08 @agent_ppo2.py:192][0m |          -0.0161 |           3.4151 |           0.1407 |
[32m[20230207 15:17:08 @agent_ppo2.py:192][0m |          -0.0028 |           3.2771 |           0.1408 |
[32m[20230207 15:17:08 @agent_ppo2.py:192][0m |          -0.0188 |           3.1620 |           0.1408 |
[32m[20230207 15:17:08 @agent_ppo2.py:192][0m |          -0.0020 |           3.1109 |           0.1408 |
[32m[20230207 15:17:08 @agent_ppo2.py:192][0m |          -0.0128 |           3.0351 |           0.1408 |
[32m[20230207 15:17:08 @agent_ppo2.py:192][0m |          -0.0052 |           2.9826 |           0.1407 |
[32m[20230207 15:17:08 @agent_ppo2.py:192][0m |          -0.0203 |           2.9234 |           0.1407 |
[32m[20230207 15:17:08 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:17:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: 39.18
[32m[20230207 15:17:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 56.35
[32m[20230207 15:17:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 25.63
[32m[20230207 15:17:09 @agent_ppo2.py:150][0m Total time:      16.46 min
[32m[20230207 15:17:09 @agent_ppo2.py:152][0m 851968 total steps have happened
[32m[20230207 15:17:09 @agent_ppo2.py:128][0m #------------------------ Iteration 416 --------------------------#
[32m[20230207 15:17:10 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:17:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:10 @agent_ppo2.py:192][0m |          -0.0026 |           5.4404 |           0.1396 |
[32m[20230207 15:17:10 @agent_ppo2.py:192][0m |          -0.0044 |           1.3983 |           0.1394 |
[32m[20230207 15:17:10 @agent_ppo2.py:192][0m |          -0.0020 |           1.2876 |           0.1396 |
[32m[20230207 15:17:10 @agent_ppo2.py:192][0m |          -0.0043 |           1.2489 |           0.1396 |
[32m[20230207 15:17:10 @agent_ppo2.py:192][0m |          -0.0069 |           1.2368 |           0.1396 |
[32m[20230207 15:17:10 @agent_ppo2.py:192][0m |          -0.0063 |           1.2625 |           0.1397 |
[32m[20230207 15:17:10 @agent_ppo2.py:192][0m |          -0.0079 |           1.1940 |           0.1396 |
[32m[20230207 15:17:10 @agent_ppo2.py:192][0m |          -0.0098 |           1.1726 |           0.1396 |
[32m[20230207 15:17:11 @agent_ppo2.py:192][0m |          -0.0041 |           1.1672 |           0.1396 |
[32m[20230207 15:17:11 @agent_ppo2.py:192][0m |          -0.0108 |           1.1541 |           0.1396 |
[32m[20230207 15:17:11 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:17:11 @agent_ppo2.py:145][0m Average TRAINING episode reward: -28.41
[32m[20230207 15:17:11 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 25.54
[32m[20230207 15:17:11 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -49.73
[32m[20230207 15:17:11 @agent_ppo2.py:150][0m Total time:      16.50 min
[32m[20230207 15:17:11 @agent_ppo2.py:152][0m 854016 total steps have happened
[32m[20230207 15:17:11 @agent_ppo2.py:128][0m #------------------------ Iteration 417 --------------------------#
[32m[20230207 15:17:12 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:17:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:12 @agent_ppo2.py:192][0m |           0.0073 |           1.7884 |           0.1404 |
[32m[20230207 15:17:12 @agent_ppo2.py:192][0m |           0.0054 |           1.4167 |           0.1404 |
[32m[20230207 15:17:12 @agent_ppo2.py:192][0m |          -0.0054 |           1.3718 |           0.1402 |
[32m[20230207 15:17:12 @agent_ppo2.py:192][0m |          -0.0241 |           1.3515 |           0.1402 |
[32m[20230207 15:17:12 @agent_ppo2.py:192][0m |           0.0054 |           1.3260 |           0.1397 |
[32m[20230207 15:17:12 @agent_ppo2.py:192][0m |          -0.0101 |           1.3129 |           0.1399 |
[32m[20230207 15:17:13 @agent_ppo2.py:192][0m |           0.0424 |           1.7465 |           0.1398 |
[32m[20230207 15:17:13 @agent_ppo2.py:192][0m |          -0.0528 |           1.5277 |           0.1397 |
[32m[20230207 15:17:13 @agent_ppo2.py:192][0m |          -0.0061 |           1.2874 |           0.1397 |
[32m[20230207 15:17:13 @agent_ppo2.py:192][0m |          -0.0151 |           1.2617 |           0.1398 |
[32m[20230207 15:17:13 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:17:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: 20.13
[32m[20230207 15:17:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 39.73
[32m[20230207 15:17:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -47.95
[32m[20230207 15:17:14 @agent_ppo2.py:150][0m Total time:      16.54 min
[32m[20230207 15:17:14 @agent_ppo2.py:152][0m 856064 total steps have happened
[32m[20230207 15:17:14 @agent_ppo2.py:128][0m #------------------------ Iteration 418 --------------------------#
[32m[20230207 15:17:14 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:17:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:15 @agent_ppo2.py:192][0m |          -0.0187 |           1.2162 |           0.1374 |
[32m[20230207 15:17:15 @agent_ppo2.py:192][0m |          -0.0118 |           1.0503 |           0.1374 |
[32m[20230207 15:17:15 @agent_ppo2.py:192][0m |           0.0056 |           1.0121 |           0.1375 |
[32m[20230207 15:17:15 @agent_ppo2.py:192][0m |           0.0058 |           0.9896 |           0.1375 |
[32m[20230207 15:17:15 @agent_ppo2.py:192][0m |          -0.0262 |           0.9743 |           0.1374 |
[32m[20230207 15:17:15 @agent_ppo2.py:192][0m |          -0.0060 |           0.9695 |           0.1374 |
[32m[20230207 15:17:15 @agent_ppo2.py:192][0m |          -0.0022 |           0.9548 |           0.1374 |
[32m[20230207 15:17:15 @agent_ppo2.py:192][0m |          -0.0062 |           0.9436 |           0.1374 |
[32m[20230207 15:17:15 @agent_ppo2.py:192][0m |          -0.0115 |           0.9324 |           0.1375 |
[32m[20230207 15:17:15 @agent_ppo2.py:192][0m |          -0.0121 |           0.9411 |           0.1375 |
[32m[20230207 15:17:15 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:17:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: 25.43
[32m[20230207 15:17:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 51.09
[32m[20230207 15:17:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -45.19
[32m[20230207 15:17:16 @agent_ppo2.py:150][0m Total time:      16.58 min
[32m[20230207 15:17:16 @agent_ppo2.py:152][0m 858112 total steps have happened
[32m[20230207 15:17:16 @agent_ppo2.py:128][0m #------------------------ Iteration 419 --------------------------#
[32m[20230207 15:17:17 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:17:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:17 @agent_ppo2.py:192][0m |          -0.0066 |          22.7887 |           0.1346 |
[32m[20230207 15:17:17 @agent_ppo2.py:192][0m |          -0.0205 |          12.2206 |           0.1346 |
[32m[20230207 15:17:17 @agent_ppo2.py:192][0m |           0.0046 |          11.7472 |           0.1345 |
[32m[20230207 15:17:17 @agent_ppo2.py:192][0m |          -0.0032 |           8.8091 |           0.1345 |
[32m[20230207 15:17:17 @agent_ppo2.py:192][0m |          -0.0037 |           7.9679 |           0.1345 |
[32m[20230207 15:17:17 @agent_ppo2.py:192][0m |          -0.0062 |           8.0856 |           0.1344 |
[32m[20230207 15:17:17 @agent_ppo2.py:192][0m |          -0.0019 |           7.0830 |           0.1344 |
[32m[20230207 15:17:17 @agent_ppo2.py:192][0m |          -0.0046 |           6.6663 |           0.1344 |
[32m[20230207 15:17:17 @agent_ppo2.py:192][0m |          -0.0213 |           6.4932 |           0.1344 |
[32m[20230207 15:17:17 @agent_ppo2.py:192][0m |          -0.0156 |           6.3049 |           0.1344 |
[32m[20230207 15:17:17 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230207 15:17:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -47.29
[32m[20230207 15:17:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -0.23
[32m[20230207 15:17:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -70.46
[32m[20230207 15:17:18 @agent_ppo2.py:150][0m Total time:      16.61 min
[32m[20230207 15:17:18 @agent_ppo2.py:152][0m 860160 total steps have happened
[32m[20230207 15:17:18 @agent_ppo2.py:128][0m #------------------------ Iteration 420 --------------------------#
[32m[20230207 15:17:19 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:17:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:19 @agent_ppo2.py:192][0m |          -0.0008 |           3.0578 |           0.1390 |
[32m[20230207 15:17:19 @agent_ppo2.py:192][0m |          -0.0209 |           1.9764 |           0.1389 |
[32m[20230207 15:17:19 @agent_ppo2.py:192][0m |          -0.0014 |           1.7548 |           0.1387 |
[32m[20230207 15:17:19 @agent_ppo2.py:192][0m |          -0.0322 |           1.7769 |           0.1385 |
[32m[20230207 15:17:19 @agent_ppo2.py:192][0m |           0.0069 |           1.7512 |           0.1383 |
[32m[20230207 15:17:19 @agent_ppo2.py:192][0m |          -0.0056 |           1.5283 |           0.1384 |
[32m[20230207 15:17:19 @agent_ppo2.py:192][0m |           0.0006 |           1.4455 |           0.1383 |
[32m[20230207 15:17:20 @agent_ppo2.py:192][0m |          -0.0083 |           1.4253 |           0.1383 |
[32m[20230207 15:17:20 @agent_ppo2.py:192][0m |           0.0178 |           1.3770 |           0.1381 |
[32m[20230207 15:17:20 @agent_ppo2.py:192][0m |          -0.0089 |           1.3486 |           0.1382 |
[32m[20230207 15:17:20 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:17:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -5.39
[32m[20230207 15:17:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 5.56
[32m[20230207 15:17:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 80.70
[32m[20230207 15:17:20 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 80.70
[32m[20230207 15:17:20 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 80.70
[32m[20230207 15:17:20 @agent_ppo2.py:150][0m Total time:      16.65 min
[32m[20230207 15:17:20 @agent_ppo2.py:152][0m 862208 total steps have happened
[32m[20230207 15:17:20 @agent_ppo2.py:128][0m #------------------------ Iteration 421 --------------------------#
[32m[20230207 15:17:21 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:17:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:21 @agent_ppo2.py:192][0m |          -0.0025 |           4.0285 |           0.1396 |
[32m[20230207 15:17:22 @agent_ppo2.py:192][0m |          -0.0042 |           1.5628 |           0.1394 |
[32m[20230207 15:17:22 @agent_ppo2.py:192][0m |          -0.0050 |           1.2139 |           0.1394 |
[32m[20230207 15:17:22 @agent_ppo2.py:192][0m |          -0.0058 |           1.0970 |           0.1392 |
[32m[20230207 15:17:22 @agent_ppo2.py:192][0m |          -0.0075 |           1.0556 |           0.1392 |
[32m[20230207 15:17:22 @agent_ppo2.py:192][0m |          -0.0071 |           1.0001 |           0.1392 |
[32m[20230207 15:17:22 @agent_ppo2.py:192][0m |          -0.0088 |           0.9628 |           0.1392 |
[32m[20230207 15:17:22 @agent_ppo2.py:192][0m |          -0.0092 |           0.9165 |           0.1391 |
[32m[20230207 15:17:22 @agent_ppo2.py:192][0m |          -0.0091 |           0.9162 |           0.1390 |
[32m[20230207 15:17:22 @agent_ppo2.py:192][0m |          -0.0084 |           0.8968 |           0.1390 |
[32m[20230207 15:17:22 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:17:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: 3.64
[32m[20230207 15:17:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 66.39
[32m[20230207 15:17:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -7.88
[32m[20230207 15:17:23 @agent_ppo2.py:150][0m Total time:      16.70 min
[32m[20230207 15:17:23 @agent_ppo2.py:152][0m 864256 total steps have happened
[32m[20230207 15:17:23 @agent_ppo2.py:128][0m #------------------------ Iteration 422 --------------------------#
[32m[20230207 15:17:23 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:17:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:24 @agent_ppo2.py:192][0m |          -0.0031 |          10.9886 |           0.1397 |
[32m[20230207 15:17:24 @agent_ppo2.py:192][0m |          -0.0020 |           4.2003 |           0.1397 |
[32m[20230207 15:17:24 @agent_ppo2.py:192][0m |          -0.0018 |           3.0630 |           0.1395 |
[32m[20230207 15:17:24 @agent_ppo2.py:192][0m |          -0.0113 |           2.5386 |           0.1394 |
[32m[20230207 15:17:24 @agent_ppo2.py:192][0m |          -0.0046 |           2.1528 |           0.1393 |
[32m[20230207 15:17:24 @agent_ppo2.py:192][0m |          -0.0130 |           1.8791 |           0.1392 |
[32m[20230207 15:17:24 @agent_ppo2.py:192][0m |          -0.0103 |           1.8230 |           0.1391 |
[32m[20230207 15:17:24 @agent_ppo2.py:192][0m |          -0.0099 |           1.6201 |           0.1390 |
[32m[20230207 15:17:24 @agent_ppo2.py:192][0m |          -0.0181 |           1.6045 |           0.1390 |
[32m[20230207 15:17:24 @agent_ppo2.py:192][0m |          -0.0163 |           1.5315 |           0.1389 |
[32m[20230207 15:17:24 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:17:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: 16.42
[32m[20230207 15:17:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 113.59
[32m[20230207 15:17:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 64.63
[32m[20230207 15:17:25 @agent_ppo2.py:150][0m Total time:      16.73 min
[32m[20230207 15:17:25 @agent_ppo2.py:152][0m 866304 total steps have happened
[32m[20230207 15:17:25 @agent_ppo2.py:128][0m #------------------------ Iteration 423 --------------------------#
[32m[20230207 15:17:26 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:17:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:26 @agent_ppo2.py:192][0m |          -0.0201 |           5.4102 |           0.1363 |
[32m[20230207 15:17:26 @agent_ppo2.py:192][0m |           0.0044 |           3.4118 |           0.1358 |
[32m[20230207 15:17:26 @agent_ppo2.py:192][0m |          -0.0018 |           3.1054 |           0.1360 |
[32m[20230207 15:17:26 @agent_ppo2.py:192][0m |          -0.0243 |           2.9167 |           0.1360 |
[32m[20230207 15:17:26 @agent_ppo2.py:192][0m |          -0.0004 |           2.8536 |           0.1360 |
[32m[20230207 15:17:26 @agent_ppo2.py:192][0m |          -0.0199 |           2.7798 |           0.1359 |
[32m[20230207 15:17:26 @agent_ppo2.py:192][0m |          -0.0017 |           2.6899 |           0.1361 |
[32m[20230207 15:17:26 @agent_ppo2.py:192][0m |           0.0100 |           2.6239 |           0.1360 |
[32m[20230207 15:17:26 @agent_ppo2.py:192][0m |          -0.0028 |           2.5249 |           0.1360 |
[32m[20230207 15:17:26 @agent_ppo2.py:192][0m |          -0.0137 |           2.5086 |           0.1359 |
[32m[20230207 15:17:26 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:17:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: 56.77
[32m[20230207 15:17:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 77.48
[32m[20230207 15:17:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 37.15
[32m[20230207 15:17:27 @agent_ppo2.py:150][0m Total time:      16.77 min
[32m[20230207 15:17:27 @agent_ppo2.py:152][0m 868352 total steps have happened
[32m[20230207 15:17:27 @agent_ppo2.py:128][0m #------------------------ Iteration 424 --------------------------#
[32m[20230207 15:17:28 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:17:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:28 @agent_ppo2.py:192][0m |           0.0321 |           1.5912 |           0.1346 |
[32m[20230207 15:17:28 @agent_ppo2.py:192][0m |           0.0007 |           1.0283 |           0.1344 |
[32m[20230207 15:17:28 @agent_ppo2.py:192][0m |          -0.0134 |           1.0202 |           0.1345 |
[32m[20230207 15:17:28 @agent_ppo2.py:192][0m |          -0.0200 |           0.9329 |           0.1344 |
[32m[20230207 15:17:28 @agent_ppo2.py:192][0m |          -0.0019 |           0.8982 |           0.1343 |
[32m[20230207 15:17:29 @agent_ppo2.py:192][0m |           0.0138 |           0.8797 |           0.1343 |
[32m[20230207 15:17:29 @agent_ppo2.py:192][0m |          -0.0335 |           0.8644 |           0.1342 |
[32m[20230207 15:17:29 @agent_ppo2.py:192][0m |          -0.0011 |           0.8557 |           0.1342 |
[32m[20230207 15:17:29 @agent_ppo2.py:192][0m |          -0.0161 |           0.8442 |           0.1341 |
[32m[20230207 15:17:29 @agent_ppo2.py:192][0m |           0.0068 |           0.8391 |           0.1341 |
[32m[20230207 15:17:29 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:17:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: 86.11
[32m[20230207 15:17:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 119.24
[32m[20230207 15:17:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 10.31
[32m[20230207 15:17:30 @agent_ppo2.py:150][0m Total time:      16.81 min
[32m[20230207 15:17:30 @agent_ppo2.py:152][0m 870400 total steps have happened
[32m[20230207 15:17:30 @agent_ppo2.py:128][0m #------------------------ Iteration 425 --------------------------#
[32m[20230207 15:17:30 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:17:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:31 @agent_ppo2.py:192][0m |           0.0001 |           5.4292 |           0.1346 |
[32m[20230207 15:17:31 @agent_ppo2.py:192][0m |           0.0015 |           2.7172 |           0.1344 |
[32m[20230207 15:17:31 @agent_ppo2.py:192][0m |          -0.0093 |           2.2635 |           0.1344 |
[32m[20230207 15:17:31 @agent_ppo2.py:192][0m |          -0.0081 |           2.0326 |           0.1343 |
[32m[20230207 15:17:31 @agent_ppo2.py:192][0m |          -0.0088 |           1.9329 |           0.1342 |
[32m[20230207 15:17:31 @agent_ppo2.py:192][0m |          -0.0103 |           1.8131 |           0.1343 |
[32m[20230207 15:17:31 @agent_ppo2.py:192][0m |          -0.0163 |           1.7479 |           0.1342 |
[32m[20230207 15:17:31 @agent_ppo2.py:192][0m |          -0.0108 |           1.6938 |           0.1341 |
[32m[20230207 15:17:31 @agent_ppo2.py:192][0m |          -0.0106 |           1.6425 |           0.1341 |
[32m[20230207 15:17:31 @agent_ppo2.py:192][0m |          -0.0143 |           1.5723 |           0.1339 |
[32m[20230207 15:17:31 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:17:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -6.96
[32m[20230207 15:17:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 50.15
[32m[20230207 15:17:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -5.70
[32m[20230207 15:17:32 @agent_ppo2.py:150][0m Total time:      16.85 min
[32m[20230207 15:17:32 @agent_ppo2.py:152][0m 872448 total steps have happened
[32m[20230207 15:17:32 @agent_ppo2.py:128][0m #------------------------ Iteration 426 --------------------------#
[32m[20230207 15:17:33 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:17:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:33 @agent_ppo2.py:192][0m |          -0.0004 |           8.9378 |           0.1399 |
[32m[20230207 15:17:33 @agent_ppo2.py:192][0m |          -0.0036 |           3.9916 |           0.1399 |
[32m[20230207 15:17:33 @agent_ppo2.py:192][0m |          -0.0067 |           3.1798 |           0.1398 |
[32m[20230207 15:17:33 @agent_ppo2.py:192][0m |          -0.0078 |           2.9326 |           0.1397 |
[32m[20230207 15:17:33 @agent_ppo2.py:192][0m |          -0.0099 |           2.6648 |           0.1396 |
[32m[20230207 15:17:34 @agent_ppo2.py:192][0m |          -0.0112 |           2.5253 |           0.1395 |
[32m[20230207 15:17:34 @agent_ppo2.py:192][0m |          -0.0115 |           2.4091 |           0.1395 |
[32m[20230207 15:17:34 @agent_ppo2.py:192][0m |          -0.0120 |           2.3474 |           0.1394 |
[32m[20230207 15:17:34 @agent_ppo2.py:192][0m |          -0.0130 |           2.2326 |           0.1394 |
[32m[20230207 15:17:34 @agent_ppo2.py:192][0m |          -0.0138 |           2.1867 |           0.1394 |
[32m[20230207 15:17:34 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:17:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: 0.74
[32m[20230207 15:17:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 85.75
[32m[20230207 15:17:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -36.88
[32m[20230207 15:17:35 @agent_ppo2.py:150][0m Total time:      16.89 min
[32m[20230207 15:17:35 @agent_ppo2.py:152][0m 874496 total steps have happened
[32m[20230207 15:17:35 @agent_ppo2.py:128][0m #------------------------ Iteration 427 --------------------------#
[32m[20230207 15:17:35 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:17:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:36 @agent_ppo2.py:192][0m |           0.0057 |           4.9333 |           0.1362 |
[32m[20230207 15:17:36 @agent_ppo2.py:192][0m |          -0.0062 |           2.1770 |           0.1362 |
[32m[20230207 15:17:36 @agent_ppo2.py:192][0m |           0.0025 |           1.9725 |           0.1362 |
[32m[20230207 15:17:36 @agent_ppo2.py:192][0m |          -0.0089 |           1.9765 |           0.1362 |
[32m[20230207 15:17:36 @agent_ppo2.py:192][0m |          -0.0057 |           1.6457 |           0.1361 |
[32m[20230207 15:17:36 @agent_ppo2.py:192][0m |          -0.0076 |           1.5834 |           0.1361 |
[32m[20230207 15:17:36 @agent_ppo2.py:192][0m |          -0.0052 |           1.5419 |           0.1360 |
[32m[20230207 15:17:36 @agent_ppo2.py:192][0m |          -0.0119 |           1.5370 |           0.1360 |
[32m[20230207 15:17:36 @agent_ppo2.py:192][0m |          -0.0115 |           1.4793 |           0.1359 |
[32m[20230207 15:17:36 @agent_ppo2.py:192][0m |          -0.0099 |           1.4961 |           0.1359 |
[32m[20230207 15:17:36 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:17:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: -16.82
[32m[20230207 15:17:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 49.52
[32m[20230207 15:17:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -33.21
[32m[20230207 15:17:37 @agent_ppo2.py:150][0m Total time:      16.93 min
[32m[20230207 15:17:37 @agent_ppo2.py:152][0m 876544 total steps have happened
[32m[20230207 15:17:37 @agent_ppo2.py:128][0m #------------------------ Iteration 428 --------------------------#
[32m[20230207 15:17:38 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:17:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:38 @agent_ppo2.py:192][0m |          -0.0028 |          11.8684 |           0.1349 |
[32m[20230207 15:17:38 @agent_ppo2.py:192][0m |          -0.0053 |           5.5003 |           0.1348 |
[32m[20230207 15:17:38 @agent_ppo2.py:192][0m |          -0.0061 |           4.9087 |           0.1349 |
[32m[20230207 15:17:38 @agent_ppo2.py:192][0m |          -0.0060 |           4.6062 |           0.1350 |
[32m[20230207 15:17:38 @agent_ppo2.py:192][0m |          -0.0080 |           4.3235 |           0.1350 |
[32m[20230207 15:17:38 @agent_ppo2.py:192][0m |          -0.0072 |           4.0878 |           0.1350 |
[32m[20230207 15:17:38 @agent_ppo2.py:192][0m |          -0.0112 |           3.9211 |           0.1350 |
[32m[20230207 15:17:38 @agent_ppo2.py:192][0m |          -0.0135 |           3.7461 |           0.1351 |
[32m[20230207 15:17:39 @agent_ppo2.py:192][0m |          -0.0111 |           3.6246 |           0.1350 |
[32m[20230207 15:17:39 @agent_ppo2.py:192][0m |          -0.0163 |           3.5268 |           0.1350 |
[32m[20230207 15:17:39 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:17:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: 25.21
[32m[20230207 15:17:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 103.33
[32m[20230207 15:17:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 32.61
[32m[20230207 15:17:39 @agent_ppo2.py:150][0m Total time:      16.97 min
[32m[20230207 15:17:39 @agent_ppo2.py:152][0m 878592 total steps have happened
[32m[20230207 15:17:39 @agent_ppo2.py:128][0m #------------------------ Iteration 429 --------------------------#
[32m[20230207 15:17:40 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230207 15:17:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:40 @agent_ppo2.py:192][0m |          -0.0054 |          14.6784 |           0.1381 |
[32m[20230207 15:17:40 @agent_ppo2.py:192][0m |          -0.0147 |           5.9369 |           0.1380 |
[32m[20230207 15:17:40 @agent_ppo2.py:192][0m |          -0.0088 |           4.7147 |           0.1379 |
[32m[20230207 15:17:40 @agent_ppo2.py:192][0m |          -0.0080 |           4.0782 |           0.1378 |
[32m[20230207 15:17:40 @agent_ppo2.py:192][0m |          -0.0031 |           3.8139 |           0.1378 |
[32m[20230207 15:17:40 @agent_ppo2.py:192][0m |          -0.0303 |           3.6078 |           0.1377 |
[32m[20230207 15:17:40 @agent_ppo2.py:192][0m |          -0.0036 |           3.4593 |           0.1378 |
[32m[20230207 15:17:40 @agent_ppo2.py:192][0m |          -0.0145 |           3.3124 |           0.1377 |
[32m[20230207 15:17:41 @agent_ppo2.py:192][0m |          -0.0159 |           3.2295 |           0.1376 |
[32m[20230207 15:17:41 @agent_ppo2.py:192][0m |          -0.0098 |           3.1323 |           0.1376 |
[32m[20230207 15:17:41 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230207 15:17:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -2.33
[32m[20230207 15:17:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 63.65
[32m[20230207 15:17:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -74.18
[32m[20230207 15:17:41 @agent_ppo2.py:150][0m Total time:      17.00 min
[32m[20230207 15:17:41 @agent_ppo2.py:152][0m 880640 total steps have happened
[32m[20230207 15:17:41 @agent_ppo2.py:128][0m #------------------------ Iteration 430 --------------------------#
[32m[20230207 15:17:42 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:17:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:42 @agent_ppo2.py:192][0m |          -0.0020 |           8.3909 |           0.1384 |
[32m[20230207 15:17:42 @agent_ppo2.py:192][0m |          -0.0084 |           3.1216 |           0.1381 |
[32m[20230207 15:17:42 @agent_ppo2.py:192][0m |          -0.0074 |           2.6060 |           0.1380 |
[32m[20230207 15:17:42 @agent_ppo2.py:192][0m |          -0.0118 |           2.4496 |           0.1380 |
[32m[20230207 15:17:42 @agent_ppo2.py:192][0m |          -0.0102 |           2.3532 |           0.1380 |
[32m[20230207 15:17:43 @agent_ppo2.py:192][0m |          -0.0097 |           2.3175 |           0.1381 |
[32m[20230207 15:17:43 @agent_ppo2.py:192][0m |          -0.0040 |           2.2441 |           0.1380 |
[32m[20230207 15:17:43 @agent_ppo2.py:192][0m |          -0.0116 |           2.2241 |           0.1380 |
[32m[20230207 15:17:43 @agent_ppo2.py:192][0m |          -0.0124 |           2.1111 |           0.1382 |
[32m[20230207 15:17:43 @agent_ppo2.py:192][0m |          -0.0136 |           2.0891 |           0.1381 |
[32m[20230207 15:17:43 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:17:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: 2.57
[32m[20230207 15:17:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 90.83
[32m[20230207 15:17:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -27.12
[32m[20230207 15:17:44 @agent_ppo2.py:150][0m Total time:      17.04 min
[32m[20230207 15:17:44 @agent_ppo2.py:152][0m 882688 total steps have happened
[32m[20230207 15:17:44 @agent_ppo2.py:128][0m #------------------------ Iteration 431 --------------------------#
[32m[20230207 15:17:44 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:17:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:45 @agent_ppo2.py:192][0m |           0.0079 |           1.5984 |           0.1389 |
[32m[20230207 15:17:45 @agent_ppo2.py:192][0m |           0.0003 |           1.3139 |           0.1389 |
[32m[20230207 15:17:45 @agent_ppo2.py:192][0m |          -0.0038 |           1.2501 |           0.1388 |
[32m[20230207 15:17:45 @agent_ppo2.py:192][0m |          -0.0008 |           1.1939 |           0.1389 |
[32m[20230207 15:17:45 @agent_ppo2.py:192][0m |           0.0135 |           1.1790 |           0.1388 |
[32m[20230207 15:17:45 @agent_ppo2.py:192][0m |          -0.0205 |           1.1417 |           0.1387 |
[32m[20230207 15:17:45 @agent_ppo2.py:192][0m |          -0.0066 |           1.1141 |           0.1387 |
[32m[20230207 15:17:45 @agent_ppo2.py:192][0m |          -0.0078 |           1.0953 |           0.1386 |
[32m[20230207 15:17:45 @agent_ppo2.py:192][0m |          -0.0084 |           1.0792 |           0.1386 |
[32m[20230207 15:17:45 @agent_ppo2.py:192][0m |          -0.0104 |           1.0752 |           0.1386 |
[32m[20230207 15:17:45 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:17:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -8.76
[32m[20230207 15:17:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 64.28
[32m[20230207 15:17:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 34.53
[32m[20230207 15:17:46 @agent_ppo2.py:150][0m Total time:      17.08 min
[32m[20230207 15:17:46 @agent_ppo2.py:152][0m 884736 total steps have happened
[32m[20230207 15:17:46 @agent_ppo2.py:128][0m #------------------------ Iteration 432 --------------------------#
[32m[20230207 15:17:47 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:17:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:47 @agent_ppo2.py:192][0m |          -0.0057 |           5.5051 |           0.1361 |
[32m[20230207 15:17:47 @agent_ppo2.py:192][0m |          -0.0120 |           2.7085 |           0.1360 |
[32m[20230207 15:17:47 @agent_ppo2.py:192][0m |          -0.0129 |           2.3612 |           0.1358 |
[32m[20230207 15:17:47 @agent_ppo2.py:192][0m |          -0.0147 |           2.1917 |           0.1357 |
[32m[20230207 15:17:47 @agent_ppo2.py:192][0m |           0.0058 |           2.1193 |           0.1356 |
[32m[20230207 15:17:48 @agent_ppo2.py:192][0m |          -0.0060 |           2.0374 |           0.1356 |
[32m[20230207 15:17:48 @agent_ppo2.py:192][0m |          -0.0079 |           1.9527 |           0.1355 |
[32m[20230207 15:17:48 @agent_ppo2.py:192][0m |          -0.0159 |           1.9275 |           0.1354 |
[32m[20230207 15:17:48 @agent_ppo2.py:192][0m |          -0.0260 |           1.8804 |           0.1353 |
[32m[20230207 15:17:48 @agent_ppo2.py:192][0m |          -0.0140 |           1.8163 |           0.1353 |
[32m[20230207 15:17:48 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:17:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: -1.32
[32m[20230207 15:17:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 59.35
[32m[20230207 15:17:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -61.76
[32m[20230207 15:17:49 @agent_ppo2.py:150][0m Total time:      17.12 min
[32m[20230207 15:17:49 @agent_ppo2.py:152][0m 886784 total steps have happened
[32m[20230207 15:17:49 @agent_ppo2.py:128][0m #------------------------ Iteration 433 --------------------------#
[32m[20230207 15:17:49 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:17:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:49 @agent_ppo2.py:192][0m |          -0.0011 |          14.9376 |           0.1362 |
[32m[20230207 15:17:49 @agent_ppo2.py:192][0m |          -0.0026 |           8.1919 |           0.1362 |
[32m[20230207 15:17:49 @agent_ppo2.py:192][0m |          -0.0055 |           6.4266 |           0.1362 |
[32m[20230207 15:17:49 @agent_ppo2.py:192][0m |          -0.0095 |           5.6175 |           0.1361 |
[32m[20230207 15:17:49 @agent_ppo2.py:192][0m |          -0.0117 |           5.1831 |           0.1359 |
[32m[20230207 15:17:49 @agent_ppo2.py:192][0m |          -0.0112 |           5.0115 |           0.1358 |
[32m[20230207 15:17:49 @agent_ppo2.py:192][0m |          -0.0116 |           4.6765 |           0.1357 |
[32m[20230207 15:17:49 @agent_ppo2.py:192][0m |          -0.0142 |           4.4965 |           0.1357 |
[32m[20230207 15:17:50 @agent_ppo2.py:192][0m |          -0.0119 |           4.3910 |           0.1356 |
[32m[20230207 15:17:50 @agent_ppo2.py:192][0m |          -0.0165 |           4.2177 |           0.1355 |
[32m[20230207 15:17:50 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:17:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: -72.15
[32m[20230207 15:17:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -8.18
[32m[20230207 15:17:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 40.60
[32m[20230207 15:17:50 @agent_ppo2.py:150][0m Total time:      17.15 min
[32m[20230207 15:17:50 @agent_ppo2.py:152][0m 888832 total steps have happened
[32m[20230207 15:17:50 @agent_ppo2.py:128][0m #------------------------ Iteration 434 --------------------------#
[32m[20230207 15:17:51 @agent_ppo2.py:134][0m Sampling time: 0.75 s by 1 slaves
[32m[20230207 15:17:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:51 @agent_ppo2.py:192][0m |           0.0043 |           7.8395 |           0.1321 |
[32m[20230207 15:17:51 @agent_ppo2.py:192][0m |          -0.0038 |           3.2927 |           0.1319 |
[32m[20230207 15:17:51 @agent_ppo2.py:192][0m |          -0.0093 |           2.8477 |           0.1318 |
[32m[20230207 15:17:51 @agent_ppo2.py:192][0m |          -0.0178 |           2.5755 |           0.1317 |
[32m[20230207 15:17:52 @agent_ppo2.py:192][0m |          -0.0151 |           2.3363 |           0.1317 |
[32m[20230207 15:17:52 @agent_ppo2.py:192][0m |          -0.0074 |           2.1964 |           0.1316 |
[32m[20230207 15:17:52 @agent_ppo2.py:192][0m |          -0.0109 |           2.1586 |           0.1316 |
[32m[20230207 15:17:52 @agent_ppo2.py:192][0m |          -0.0022 |           2.0476 |           0.1316 |
[32m[20230207 15:17:52 @agent_ppo2.py:192][0m |           0.0043 |           1.9639 |           0.1316 |
[32m[20230207 15:17:52 @agent_ppo2.py:192][0m |          -0.0136 |           1.8936 |           0.1315 |
[32m[20230207 15:17:52 @agent_ppo2.py:137][0m Policy update time: 0.78 s
[32m[20230207 15:17:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: 9.10
[32m[20230207 15:17:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 16.46
[32m[20230207 15:17:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 94.55
[32m[20230207 15:17:53 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 94.55
[32m[20230207 15:17:53 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 94.55
[32m[20230207 15:17:53 @agent_ppo2.py:150][0m Total time:      17.19 min
[32m[20230207 15:17:53 @agent_ppo2.py:152][0m 890880 total steps have happened
[32m[20230207 15:17:53 @agent_ppo2.py:128][0m #------------------------ Iteration 435 --------------------------#
[32m[20230207 15:17:53 @agent_ppo2.py:134][0m Sampling time: 0.64 s by 1 slaves
[32m[20230207 15:17:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:53 @agent_ppo2.py:192][0m |          -0.0009 |           8.2137 |           0.1340 |
[32m[20230207 15:17:54 @agent_ppo2.py:192][0m |          -0.0017 |           3.1147 |           0.1339 |
[32m[20230207 15:17:54 @agent_ppo2.py:192][0m |          -0.0047 |           2.2558 |           0.1338 |
[32m[20230207 15:17:54 @agent_ppo2.py:192][0m |          -0.0089 |           1.8934 |           0.1337 |
[32m[20230207 15:17:54 @agent_ppo2.py:192][0m |          -0.0089 |           1.7029 |           0.1336 |
[32m[20230207 15:17:54 @agent_ppo2.py:192][0m |          -0.0148 |           1.5827 |           0.1336 |
[32m[20230207 15:17:54 @agent_ppo2.py:192][0m |          -0.0131 |           1.4113 |           0.1335 |
[32m[20230207 15:17:54 @agent_ppo2.py:192][0m |          -0.0095 |           1.3893 |           0.1335 |
[32m[20230207 15:17:54 @agent_ppo2.py:192][0m |          -0.0106 |           1.2788 |           0.1336 |
[32m[20230207 15:17:54 @agent_ppo2.py:192][0m |          -0.0155 |           1.2444 |           0.1336 |
[32m[20230207 15:17:54 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230207 15:17:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: -14.50
[32m[20230207 15:17:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 48.67
[32m[20230207 15:17:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -4.26
[32m[20230207 15:17:55 @agent_ppo2.py:150][0m Total time:      17.23 min
[32m[20230207 15:17:55 @agent_ppo2.py:152][0m 892928 total steps have happened
[32m[20230207 15:17:55 @agent_ppo2.py:128][0m #------------------------ Iteration 436 --------------------------#
[32m[20230207 15:17:56 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:17:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:56 @agent_ppo2.py:192][0m |          -0.0036 |           1.4864 |           0.1350 |
[32m[20230207 15:17:56 @agent_ppo2.py:192][0m |           0.0079 |           1.2563 |           0.1347 |
[32m[20230207 15:17:56 @agent_ppo2.py:192][0m |          -0.0030 |           1.2017 |           0.1348 |
[32m[20230207 15:17:56 @agent_ppo2.py:192][0m |          -0.0165 |           1.1570 |           0.1347 |
[32m[20230207 15:17:56 @agent_ppo2.py:192][0m |           0.0166 |           1.1372 |           0.1346 |
[32m[20230207 15:17:56 @agent_ppo2.py:192][0m |          -0.0178 |           1.1223 |           0.1347 |
[32m[20230207 15:17:56 @agent_ppo2.py:192][0m |          -0.0179 |           1.1121 |           0.1348 |
[32m[20230207 15:17:56 @agent_ppo2.py:192][0m |          -0.0161 |           1.1004 |           0.1349 |
[32m[20230207 15:17:56 @agent_ppo2.py:192][0m |          -0.0157 |           1.0874 |           0.1349 |
[32m[20230207 15:17:56 @agent_ppo2.py:192][0m |          -0.0106 |           1.0858 |           0.1349 |
[32m[20230207 15:17:56 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:17:57 @agent_ppo2.py:145][0m Average TRAINING episode reward: 1.78
[32m[20230207 15:17:57 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 40.42
[32m[20230207 15:17:57 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 44.87
[32m[20230207 15:17:57 @agent_ppo2.py:150][0m Total time:      17.27 min
[32m[20230207 15:17:57 @agent_ppo2.py:152][0m 894976 total steps have happened
[32m[20230207 15:17:57 @agent_ppo2.py:128][0m #------------------------ Iteration 437 --------------------------#
[32m[20230207 15:17:58 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:17:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:17:58 @agent_ppo2.py:192][0m |          -0.0116 |          24.6833 |           0.1328 |
[32m[20230207 15:17:58 @agent_ppo2.py:192][0m |          -0.0020 |          13.1809 |           0.1326 |
[32m[20230207 15:17:58 @agent_ppo2.py:192][0m |          -0.0037 |          11.1130 |           0.1325 |
[32m[20230207 15:17:58 @agent_ppo2.py:192][0m |          -0.0009 |          10.0623 |           0.1324 |
[32m[20230207 15:17:59 @agent_ppo2.py:192][0m |          -0.0011 |           9.3704 |           0.1321 |
[32m[20230207 15:17:59 @agent_ppo2.py:192][0m |           0.0007 |           8.7536 |           0.1322 |
[32m[20230207 15:17:59 @agent_ppo2.py:192][0m |          -0.0075 |           8.5033 |           0.1320 |
[32m[20230207 15:17:59 @agent_ppo2.py:192][0m |          -0.0093 |           7.9528 |           0.1321 |
[32m[20230207 15:17:59 @agent_ppo2.py:192][0m |          -0.0063 |           7.8772 |           0.1319 |
[32m[20230207 15:17:59 @agent_ppo2.py:192][0m |          -0.0082 |           7.7305 |           0.1318 |
[32m[20230207 15:17:59 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: 72.30
[32m[20230207 15:18:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 173.78
[32m[20230207 15:18:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 6.10
[32m[20230207 15:18:00 @agent_ppo2.py:150][0m Total time:      17.31 min
[32m[20230207 15:18:00 @agent_ppo2.py:152][0m 897024 total steps have happened
[32m[20230207 15:18:00 @agent_ppo2.py:128][0m #------------------------ Iteration 438 --------------------------#
[32m[20230207 15:18:01 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:18:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:01 @agent_ppo2.py:192][0m |          -0.0008 |           7.4212 |           0.1371 |
[32m[20230207 15:18:01 @agent_ppo2.py:192][0m |          -0.0008 |           2.1510 |           0.1369 |
[32m[20230207 15:18:01 @agent_ppo2.py:192][0m |          -0.0051 |           1.5378 |           0.1367 |
[32m[20230207 15:18:01 @agent_ppo2.py:192][0m |          -0.0062 |           1.3929 |           0.1366 |
[32m[20230207 15:18:01 @agent_ppo2.py:192][0m |          -0.0067 |           1.3187 |           0.1365 |
[32m[20230207 15:18:01 @agent_ppo2.py:192][0m |          -0.0074 |           1.2707 |           0.1365 |
[32m[20230207 15:18:01 @agent_ppo2.py:192][0m |          -0.0076 |           1.2201 |           0.1365 |
[32m[20230207 15:18:01 @agent_ppo2.py:192][0m |          -0.0076 |           1.1918 |           0.1364 |
[32m[20230207 15:18:01 @agent_ppo2.py:192][0m |          -0.0080 |           1.1630 |           0.1364 |
[32m[20230207 15:18:01 @agent_ppo2.py:192][0m |          -0.0090 |           1.1331 |           0.1364 |
[32m[20230207 15:18:01 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:18:02 @agent_ppo2.py:145][0m Average TRAINING episode reward: -31.36
[32m[20230207 15:18:02 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 21.88
[32m[20230207 15:18:02 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -63.04
[32m[20230207 15:18:02 @agent_ppo2.py:150][0m Total time:      17.35 min
[32m[20230207 15:18:02 @agent_ppo2.py:152][0m 899072 total steps have happened
[32m[20230207 15:18:02 @agent_ppo2.py:128][0m #------------------------ Iteration 439 --------------------------#
[32m[20230207 15:18:03 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:18:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:03 @agent_ppo2.py:192][0m |          -0.0034 |           0.9201 |           0.1335 |
[32m[20230207 15:18:03 @agent_ppo2.py:192][0m |          -0.0169 |           0.7931 |           0.1333 |
[32m[20230207 15:18:03 @agent_ppo2.py:192][0m |          -0.0129 |           0.7618 |           0.1332 |
[32m[20230207 15:18:03 @agent_ppo2.py:192][0m |           0.0183 |           0.7569 |           0.1333 |
[32m[20230207 15:18:03 @agent_ppo2.py:192][0m |          -0.0127 |           0.7294 |           0.1329 |
[32m[20230207 15:18:03 @agent_ppo2.py:192][0m |          -0.0058 |           0.7177 |           0.1329 |
[32m[20230207 15:18:03 @agent_ppo2.py:192][0m |          -0.0092 |           0.7006 |           0.1329 |
[32m[20230207 15:18:03 @agent_ppo2.py:192][0m |          -0.0289 |           0.7024 |           0.1330 |
[32m[20230207 15:18:03 @agent_ppo2.py:192][0m |           0.0031 |           0.6867 |           0.1330 |
[32m[20230207 15:18:04 @agent_ppo2.py:192][0m |          -0.0160 |           0.6751 |           0.1330 |
[32m[20230207 15:18:04 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: -4.51
[32m[20230207 15:18:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 25.99
[32m[20230207 15:18:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -88.75
[32m[20230207 15:18:04 @agent_ppo2.py:150][0m Total time:      17.38 min
[32m[20230207 15:18:04 @agent_ppo2.py:152][0m 901120 total steps have happened
[32m[20230207 15:18:04 @agent_ppo2.py:128][0m #------------------------ Iteration 440 --------------------------#
[32m[20230207 15:18:05 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:18:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:05 @agent_ppo2.py:192][0m |           0.0022 |           2.5215 |           0.1375 |
[32m[20230207 15:18:05 @agent_ppo2.py:192][0m |           0.0180 |           1.6475 |           0.1373 |
[32m[20230207 15:18:05 @agent_ppo2.py:192][0m |          -0.0119 |           1.4922 |           0.1375 |
[32m[20230207 15:18:05 @agent_ppo2.py:192][0m |          -0.0325 |           1.3755 |           0.1372 |
[32m[20230207 15:18:05 @agent_ppo2.py:192][0m |           0.0167 |           1.3238 |           0.1372 |
[32m[20230207 15:18:06 @agent_ppo2.py:192][0m |          -0.0062 |           1.2851 |           0.1372 |
[32m[20230207 15:18:06 @agent_ppo2.py:192][0m |          -0.0253 |           1.2341 |           0.1371 |
[32m[20230207 15:18:06 @agent_ppo2.py:192][0m |          -0.0064 |           1.1891 |           0.1372 |
[32m[20230207 15:18:06 @agent_ppo2.py:192][0m |           0.0045 |           1.1719 |           0.1373 |
[32m[20230207 15:18:06 @agent_ppo2.py:192][0m |          -0.0025 |           1.1677 |           0.1373 |
[32m[20230207 15:18:06 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: 0.44
[32m[20230207 15:18:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 14.74
[32m[20230207 15:18:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 13.24
[32m[20230207 15:18:07 @agent_ppo2.py:150][0m Total time:      17.42 min
[32m[20230207 15:18:07 @agent_ppo2.py:152][0m 903168 total steps have happened
[32m[20230207 15:18:07 @agent_ppo2.py:128][0m #------------------------ Iteration 441 --------------------------#
[32m[20230207 15:18:07 @agent_ppo2.py:134][0m Sampling time: 0.66 s by 1 slaves
[32m[20230207 15:18:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:07 @agent_ppo2.py:192][0m |          -0.0018 |          11.4121 |           0.1380 |
[32m[20230207 15:18:08 @agent_ppo2.py:192][0m |          -0.0082 |           4.0515 |           0.1381 |
[32m[20230207 15:18:08 @agent_ppo2.py:192][0m |          -0.0056 |           2.9435 |           0.1381 |
[32m[20230207 15:18:08 @agent_ppo2.py:192][0m |          -0.0067 |           2.5799 |           0.1381 |
[32m[20230207 15:18:08 @agent_ppo2.py:192][0m |          -0.0038 |           2.3484 |           0.1381 |
[32m[20230207 15:18:08 @agent_ppo2.py:192][0m |          -0.0086 |           2.1625 |           0.1381 |
[32m[20230207 15:18:08 @agent_ppo2.py:192][0m |          -0.0131 |           2.0306 |           0.1380 |
[32m[20230207 15:18:08 @agent_ppo2.py:192][0m |          -0.0082 |           1.9425 |           0.1381 |
[32m[20230207 15:18:08 @agent_ppo2.py:192][0m |          -0.0166 |           1.8500 |           0.1380 |
[32m[20230207 15:18:08 @agent_ppo2.py:192][0m |          -0.0134 |           1.6862 |           0.1380 |
[32m[20230207 15:18:08 @agent_ppo2.py:137][0m Policy update time: 0.69 s
[32m[20230207 15:18:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: -75.14
[32m[20230207 15:18:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 25.73
[32m[20230207 15:18:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -32.98
[32m[20230207 15:18:09 @agent_ppo2.py:150][0m Total time:      17.46 min
[32m[20230207 15:18:09 @agent_ppo2.py:152][0m 905216 total steps have happened
[32m[20230207 15:18:09 @agent_ppo2.py:128][0m #------------------------ Iteration 442 --------------------------#
[32m[20230207 15:18:10 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:18:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:10 @agent_ppo2.py:192][0m |           0.0004 |          12.0815 |           0.1366 |
[32m[20230207 15:18:10 @agent_ppo2.py:192][0m |          -0.0026 |           8.7147 |           0.1366 |
[32m[20230207 15:18:10 @agent_ppo2.py:192][0m |          -0.0051 |           6.8712 |           0.1366 |
[32m[20230207 15:18:10 @agent_ppo2.py:192][0m |          -0.0060 |           6.0133 |           0.1365 |
[32m[20230207 15:18:10 @agent_ppo2.py:192][0m |          -0.0063 |           5.5560 |           0.1365 |
[32m[20230207 15:18:10 @agent_ppo2.py:192][0m |          -0.0075 |           5.2308 |           0.1365 |
[32m[20230207 15:18:10 @agent_ppo2.py:192][0m |          -0.0070 |           4.8830 |           0.1364 |
[32m[20230207 15:18:10 @agent_ppo2.py:192][0m |          -0.0068 |           4.4490 |           0.1366 |
[32m[20230207 15:18:10 @agent_ppo2.py:192][0m |          -0.0096 |           4.2272 |           0.1365 |
[32m[20230207 15:18:11 @agent_ppo2.py:192][0m |          -0.0104 |           3.9338 |           0.1366 |
[32m[20230207 15:18:11 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:18:11 @agent_ppo2.py:145][0m Average TRAINING episode reward: -9.92
[32m[20230207 15:18:11 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 56.09
[32m[20230207 15:18:11 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 16.98
[32m[20230207 15:18:11 @agent_ppo2.py:150][0m Total time:      17.50 min
[32m[20230207 15:18:11 @agent_ppo2.py:152][0m 907264 total steps have happened
[32m[20230207 15:18:11 @agent_ppo2.py:128][0m #------------------------ Iteration 443 --------------------------#
[32m[20230207 15:18:12 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:18:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:12 @agent_ppo2.py:192][0m |           0.0011 |           8.9227 |           0.1362 |
[32m[20230207 15:18:12 @agent_ppo2.py:192][0m |          -0.0036 |           6.2441 |           0.1361 |
[32m[20230207 15:18:13 @agent_ppo2.py:192][0m |           0.0030 |           5.1993 |           0.1360 |
[32m[20230207 15:18:13 @agent_ppo2.py:192][0m |          -0.0120 |           4.8283 |           0.1360 |
[32m[20230207 15:18:13 @agent_ppo2.py:192][0m |          -0.0100 |           4.0808 |           0.1360 |
[32m[20230207 15:18:13 @agent_ppo2.py:192][0m |          -0.0146 |           3.6837 |           0.1359 |
[32m[20230207 15:18:13 @agent_ppo2.py:192][0m |          -0.0140 |           3.3865 |           0.1359 |
[32m[20230207 15:18:13 @agent_ppo2.py:192][0m |          -0.0102 |           3.0598 |           0.1359 |
[32m[20230207 15:18:13 @agent_ppo2.py:192][0m |          -0.0134 |           2.8631 |           0.1359 |
[32m[20230207 15:18:13 @agent_ppo2.py:192][0m |          -0.0136 |           2.5624 |           0.1358 |
[32m[20230207 15:18:13 @agent_ppo2.py:137][0m Policy update time: 0.92 s
[32m[20230207 15:18:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: -4.76
[32m[20230207 15:18:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 33.88
[32m[20230207 15:18:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -17.54
[32m[20230207 15:18:14 @agent_ppo2.py:150][0m Total time:      17.55 min
[32m[20230207 15:18:14 @agent_ppo2.py:152][0m 909312 total steps have happened
[32m[20230207 15:18:14 @agent_ppo2.py:128][0m #------------------------ Iteration 444 --------------------------#
[32m[20230207 15:18:15 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:18:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:15 @agent_ppo2.py:192][0m |           0.0040 |           7.8496 |           0.1327 |
[32m[20230207 15:18:15 @agent_ppo2.py:192][0m |          -0.0092 |           3.3770 |           0.1325 |
[32m[20230207 15:18:15 @agent_ppo2.py:192][0m |           0.0262 |           2.4399 |           0.1324 |
[32m[20230207 15:18:15 @agent_ppo2.py:192][0m |          -0.0133 |           2.1860 |           0.1321 |
[32m[20230207 15:18:15 @agent_ppo2.py:192][0m |          -0.0106 |           2.0031 |           0.1322 |
[32m[20230207 15:18:15 @agent_ppo2.py:192][0m |          -0.0132 |           1.8897 |           0.1322 |
[32m[20230207 15:18:16 @agent_ppo2.py:192][0m |          -0.0071 |           1.8497 |           0.1321 |
[32m[20230207 15:18:16 @agent_ppo2.py:192][0m |          -0.0143 |           1.7766 |           0.1321 |
[32m[20230207 15:18:16 @agent_ppo2.py:192][0m |          -0.0123 |           1.6756 |           0.1321 |
[32m[20230207 15:18:16 @agent_ppo2.py:192][0m |          -0.0147 |           1.6504 |           0.1321 |
[32m[20230207 15:18:16 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:18:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: 14.75
[32m[20230207 15:18:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 95.01
[32m[20230207 15:18:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -66.21
[32m[20230207 15:18:16 @agent_ppo2.py:150][0m Total time:      17.59 min
[32m[20230207 15:18:16 @agent_ppo2.py:152][0m 911360 total steps have happened
[32m[20230207 15:18:16 @agent_ppo2.py:128][0m #------------------------ Iteration 445 --------------------------#
[32m[20230207 15:18:17 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:18:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:17 @agent_ppo2.py:192][0m |           0.0013 |           8.3232 |           0.1394 |
[32m[20230207 15:18:17 @agent_ppo2.py:192][0m |          -0.0044 |           5.2382 |           0.1392 |
[32m[20230207 15:18:17 @agent_ppo2.py:192][0m |          -0.0068 |           4.8243 |           0.1392 |
[32m[20230207 15:18:17 @agent_ppo2.py:192][0m |          -0.0094 |           4.6891 |           0.1391 |
[32m[20230207 15:18:17 @agent_ppo2.py:192][0m |          -0.0103 |           4.5102 |           0.1390 |
[32m[20230207 15:18:17 @agent_ppo2.py:192][0m |          -0.0112 |           4.2975 |           0.1389 |
[32m[20230207 15:18:17 @agent_ppo2.py:192][0m |          -0.0118 |           4.1730 |           0.1391 |
[32m[20230207 15:18:18 @agent_ppo2.py:192][0m |          -0.0121 |           4.0521 |           0.1389 |
[32m[20230207 15:18:18 @agent_ppo2.py:192][0m |          -0.0130 |           3.8125 |           0.1388 |
[32m[20230207 15:18:18 @agent_ppo2.py:192][0m |          -0.0143 |           3.6574 |           0.1390 |
[32m[20230207 15:18:18 @agent_ppo2.py:137][0m Policy update time: 0.58 s
[32m[20230207 15:18:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -36.96
[32m[20230207 15:18:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -4.99
[32m[20230207 15:18:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -58.54
[32m[20230207 15:18:18 @agent_ppo2.py:150][0m Total time:      17.62 min
[32m[20230207 15:18:18 @agent_ppo2.py:152][0m 913408 total steps have happened
[32m[20230207 15:18:18 @agent_ppo2.py:128][0m #------------------------ Iteration 446 --------------------------#
[32m[20230207 15:18:19 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:18:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:19 @agent_ppo2.py:192][0m |          -0.0020 |          30.1109 |           0.1380 |
[32m[20230207 15:18:19 @agent_ppo2.py:192][0m |          -0.0092 |          13.6544 |           0.1379 |
[32m[20230207 15:18:19 @agent_ppo2.py:192][0m |          -0.0060 |          11.2491 |           0.1378 |
[32m[20230207 15:18:19 @agent_ppo2.py:192][0m |          -0.0119 |          10.0595 |           0.1378 |
[32m[20230207 15:18:19 @agent_ppo2.py:192][0m |          -0.0123 |           9.4707 |           0.1376 |
[32m[20230207 15:18:19 @agent_ppo2.py:192][0m |          -0.0117 |           8.5638 |           0.1376 |
[32m[20230207 15:18:19 @agent_ppo2.py:192][0m |          -0.0139 |           7.8848 |           0.1375 |
[32m[20230207 15:18:19 @agent_ppo2.py:192][0m |          -0.0170 |           7.5887 |           0.1374 |
[32m[20230207 15:18:19 @agent_ppo2.py:192][0m |          -0.0169 |           7.1002 |           0.1374 |
[32m[20230207 15:18:20 @agent_ppo2.py:192][0m |          -0.0167 |           6.8623 |           0.1373 |
[32m[20230207 15:18:20 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:18:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: 10.26
[32m[20230207 15:18:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 77.07
[32m[20230207 15:18:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 13.69
[32m[20230207 15:18:20 @agent_ppo2.py:150][0m Total time:      17.65 min
[32m[20230207 15:18:20 @agent_ppo2.py:152][0m 915456 total steps have happened
[32m[20230207 15:18:20 @agent_ppo2.py:128][0m #------------------------ Iteration 447 --------------------------#
[32m[20230207 15:18:21 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:18:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:21 @agent_ppo2.py:192][0m |           0.0036 |           1.9166 |           0.1370 |
[32m[20230207 15:18:21 @agent_ppo2.py:192][0m |          -0.0054 |           1.4187 |           0.1371 |
[32m[20230207 15:18:21 @agent_ppo2.py:192][0m |          -0.0114 |           1.2967 |           0.1372 |
[32m[20230207 15:18:22 @agent_ppo2.py:192][0m |           0.0097 |           1.2488 |           0.1372 |
[32m[20230207 15:18:22 @agent_ppo2.py:192][0m |          -0.0029 |           1.1844 |           0.1371 |
[32m[20230207 15:18:22 @agent_ppo2.py:192][0m |          -0.0204 |           1.1551 |           0.1371 |
[32m[20230207 15:18:22 @agent_ppo2.py:192][0m |          -0.0084 |           1.1258 |           0.1372 |
[32m[20230207 15:18:22 @agent_ppo2.py:192][0m |          -0.0323 |           1.1431 |           0.1371 |
[32m[20230207 15:18:22 @agent_ppo2.py:192][0m |          -0.0042 |           1.1132 |           0.1372 |
[32m[20230207 15:18:22 @agent_ppo2.py:192][0m |          -0.0280 |           1.0856 |           0.1372 |
[32m[20230207 15:18:22 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: 15.56
[32m[20230207 15:18:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 30.58
[32m[20230207 15:18:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 23.64
[32m[20230207 15:18:23 @agent_ppo2.py:150][0m Total time:      17.69 min
[32m[20230207 15:18:23 @agent_ppo2.py:152][0m 917504 total steps have happened
[32m[20230207 15:18:23 @agent_ppo2.py:128][0m #------------------------ Iteration 448 --------------------------#
[32m[20230207 15:18:24 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:18:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:24 @agent_ppo2.py:192][0m |          -0.0186 |           1.3764 |           0.1375 |
[32m[20230207 15:18:24 @agent_ppo2.py:192][0m |          -0.0009 |           1.0826 |           0.1375 |
[32m[20230207 15:18:24 @agent_ppo2.py:192][0m |          -0.0357 |           1.0126 |           0.1375 |
[32m[20230207 15:18:24 @agent_ppo2.py:192][0m |           0.0074 |           0.9625 |           0.1376 |
[32m[20230207 15:18:24 @agent_ppo2.py:192][0m |          -0.0245 |           0.9640 |           0.1375 |
[32m[20230207 15:18:24 @agent_ppo2.py:192][0m |           0.0036 |           0.9189 |           0.1375 |
[32m[20230207 15:18:24 @agent_ppo2.py:192][0m |           0.0061 |           0.8941 |           0.1374 |
[32m[20230207 15:18:24 @agent_ppo2.py:192][0m |           0.0020 |           0.8731 |           0.1372 |
[32m[20230207 15:18:24 @agent_ppo2.py:192][0m |           0.0033 |           0.8484 |           0.1372 |
[32m[20230207 15:18:24 @agent_ppo2.py:192][0m |          -0.0235 |           0.8399 |           0.1372 |
[32m[20230207 15:18:24 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: 12.43
[32m[20230207 15:18:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 29.29
[32m[20230207 15:18:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 24.97
[32m[20230207 15:18:25 @agent_ppo2.py:150][0m Total time:      17.73 min
[32m[20230207 15:18:25 @agent_ppo2.py:152][0m 919552 total steps have happened
[32m[20230207 15:18:25 @agent_ppo2.py:128][0m #------------------------ Iteration 449 --------------------------#
[32m[20230207 15:18:26 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230207 15:18:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:26 @agent_ppo2.py:192][0m |          -0.0018 |          17.1232 |           0.1351 |
[32m[20230207 15:18:26 @agent_ppo2.py:192][0m |          -0.0048 |           5.7738 |           0.1349 |
[32m[20230207 15:18:26 @agent_ppo2.py:192][0m |          -0.0017 |           4.7301 |           0.1348 |
[32m[20230207 15:18:26 @agent_ppo2.py:192][0m |          -0.0082 |           4.1551 |           0.1347 |
[32m[20230207 15:18:26 @agent_ppo2.py:192][0m |          -0.0017 |           3.8155 |           0.1346 |
[32m[20230207 15:18:26 @agent_ppo2.py:192][0m |          -0.0108 |           3.5569 |           0.1346 |
[32m[20230207 15:18:26 @agent_ppo2.py:192][0m |          -0.0098 |           3.3852 |           0.1345 |
[32m[20230207 15:18:26 @agent_ppo2.py:192][0m |          -0.0108 |           3.1637 |           0.1344 |
[32m[20230207 15:18:26 @agent_ppo2.py:192][0m |          -0.0116 |           3.0051 |           0.1344 |
[32m[20230207 15:18:26 @agent_ppo2.py:192][0m |          -0.0122 |           2.8866 |           0.1344 |
[32m[20230207 15:18:26 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:18:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -36.14
[32m[20230207 15:18:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 64.63
[32m[20230207 15:18:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -40.40
[32m[20230207 15:18:27 @agent_ppo2.py:150][0m Total time:      17.77 min
[32m[20230207 15:18:27 @agent_ppo2.py:152][0m 921600 total steps have happened
[32m[20230207 15:18:27 @agent_ppo2.py:128][0m #------------------------ Iteration 450 --------------------------#
[32m[20230207 15:18:28 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:18:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:28 @agent_ppo2.py:192][0m |          -0.0009 |           1.2535 |           0.1374 |
[32m[20230207 15:18:28 @agent_ppo2.py:192][0m |          -0.0087 |           1.0647 |           0.1373 |
[32m[20230207 15:18:28 @agent_ppo2.py:192][0m |          -0.0082 |           1.0226 |           0.1372 |
[32m[20230207 15:18:28 @agent_ppo2.py:192][0m |          -0.0173 |           1.0049 |           0.1372 |
[32m[20230207 15:18:28 @agent_ppo2.py:192][0m |          -0.0269 |           0.9901 |           0.1372 |
[32m[20230207 15:18:29 @agent_ppo2.py:192][0m |           0.0082 |           0.9643 |           0.1372 |
[32m[20230207 15:18:29 @agent_ppo2.py:192][0m |          -0.0041 |           0.9496 |           0.1373 |
[32m[20230207 15:18:29 @agent_ppo2.py:192][0m |          -0.0168 |           0.9415 |           0.1373 |
[32m[20230207 15:18:29 @agent_ppo2.py:192][0m |          -0.0092 |           0.9523 |           0.1373 |
[32m[20230207 15:18:29 @agent_ppo2.py:192][0m |          -0.0115 |           0.9240 |           0.1371 |
[32m[20230207 15:18:29 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:18:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: 8.65
[32m[20230207 15:18:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 42.64
[32m[20230207 15:18:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -19.44
[32m[20230207 15:18:29 @agent_ppo2.py:150][0m Total time:      17.80 min
[32m[20230207 15:18:29 @agent_ppo2.py:152][0m 923648 total steps have happened
[32m[20230207 15:18:29 @agent_ppo2.py:128][0m #------------------------ Iteration 451 --------------------------#
[32m[20230207 15:18:30 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:18:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:30 @agent_ppo2.py:192][0m |           0.0022 |           2.9672 |           0.1370 |
[32m[20230207 15:18:30 @agent_ppo2.py:192][0m |          -0.0056 |           2.3860 |           0.1368 |
[32m[20230207 15:18:30 @agent_ppo2.py:192][0m |          -0.0118 |           2.2573 |           0.1367 |
[32m[20230207 15:18:31 @agent_ppo2.py:192][0m |          -0.0023 |           2.1783 |           0.1366 |
[32m[20230207 15:18:31 @agent_ppo2.py:192][0m |          -0.0510 |           2.3749 |           0.1364 |
[32m[20230207 15:18:31 @agent_ppo2.py:192][0m |          -0.0108 |           2.1280 |           0.1364 |
[32m[20230207 15:18:31 @agent_ppo2.py:192][0m |          -0.0063 |           2.0356 |           0.1365 |
[32m[20230207 15:18:31 @agent_ppo2.py:192][0m |          -0.0187 |           1.9967 |           0.1365 |
[32m[20230207 15:18:31 @agent_ppo2.py:192][0m |          -0.0183 |           1.9952 |           0.1364 |
[32m[20230207 15:18:31 @agent_ppo2.py:192][0m |          -0.0246 |           1.9473 |           0.1364 |
[32m[20230207 15:18:31 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: 48.26
[32m[20230207 15:18:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 73.06
[32m[20230207 15:18:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 8.61
[32m[20230207 15:18:32 @agent_ppo2.py:150][0m Total time:      17.84 min
[32m[20230207 15:18:32 @agent_ppo2.py:152][0m 925696 total steps have happened
[32m[20230207 15:18:32 @agent_ppo2.py:128][0m #------------------------ Iteration 452 --------------------------#
[32m[20230207 15:18:33 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:18:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:33 @agent_ppo2.py:192][0m |           0.0064 |           1.7800 |           0.1356 |
[32m[20230207 15:18:33 @agent_ppo2.py:192][0m |          -0.0177 |           1.4219 |           0.1357 |
[32m[20230207 15:18:33 @agent_ppo2.py:192][0m |          -0.0130 |           1.2865 |           0.1356 |
[32m[20230207 15:18:33 @agent_ppo2.py:192][0m |          -0.0311 |           1.2209 |           0.1358 |
[32m[20230207 15:18:33 @agent_ppo2.py:192][0m |           0.0288 |           1.3454 |           0.1357 |
[32m[20230207 15:18:33 @agent_ppo2.py:192][0m |          -0.0041 |           1.1816 |           0.1353 |
[32m[20230207 15:18:33 @agent_ppo2.py:192][0m |          -0.0142 |           1.0926 |           0.1354 |
[32m[20230207 15:18:33 @agent_ppo2.py:192][0m |          -0.0115 |           1.0720 |           0.1356 |
[32m[20230207 15:18:33 @agent_ppo2.py:192][0m |          -0.0038 |           1.0588 |           0.1356 |
[32m[20230207 15:18:33 @agent_ppo2.py:192][0m |          -0.0169 |           1.0379 |           0.1356 |
[32m[20230207 15:18:33 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: 75.60
[32m[20230207 15:18:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 83.73
[32m[20230207 15:18:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -17.32
[32m[20230207 15:18:34 @agent_ppo2.py:150][0m Total time:      17.88 min
[32m[20230207 15:18:34 @agent_ppo2.py:152][0m 927744 total steps have happened
[32m[20230207 15:18:34 @agent_ppo2.py:128][0m #------------------------ Iteration 453 --------------------------#
[32m[20230207 15:18:35 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:18:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:35 @agent_ppo2.py:192][0m |          -0.0027 |          12.9264 |           0.1391 |
[32m[20230207 15:18:35 @agent_ppo2.py:192][0m |          -0.0033 |           4.6375 |           0.1391 |
[32m[20230207 15:18:35 @agent_ppo2.py:192][0m |          -0.0048 |           3.6821 |           0.1389 |
[32m[20230207 15:18:35 @agent_ppo2.py:192][0m |          -0.0069 |           3.1770 |           0.1389 |
[32m[20230207 15:18:36 @agent_ppo2.py:192][0m |          -0.0092 |           2.9504 |           0.1389 |
[32m[20230207 15:18:36 @agent_ppo2.py:192][0m |          -0.0120 |           2.8013 |           0.1388 |
[32m[20230207 15:18:36 @agent_ppo2.py:192][0m |          -0.0146 |           2.5262 |           0.1387 |
[32m[20230207 15:18:36 @agent_ppo2.py:192][0m |          -0.0119 |           2.3916 |           0.1386 |
[32m[20230207 15:18:36 @agent_ppo2.py:192][0m |          -0.0114 |           2.2684 |           0.1385 |
[32m[20230207 15:18:36 @agent_ppo2.py:192][0m |          -0.0102 |           2.2220 |           0.1385 |
[32m[20230207 15:18:36 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:18:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: 10.76
[32m[20230207 15:18:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 125.82
[32m[20230207 15:18:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -47.69
[32m[20230207 15:18:37 @agent_ppo2.py:150][0m Total time:      17.92 min
[32m[20230207 15:18:37 @agent_ppo2.py:152][0m 929792 total steps have happened
[32m[20230207 15:18:37 @agent_ppo2.py:128][0m #------------------------ Iteration 454 --------------------------#
[32m[20230207 15:18:37 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:18:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:37 @agent_ppo2.py:192][0m |          -0.0067 |           7.5567 |           0.1405 |
[32m[20230207 15:18:37 @agent_ppo2.py:192][0m |           0.0036 |           3.3562 |           0.1403 |
[32m[20230207 15:18:37 @agent_ppo2.py:192][0m |          -0.0011 |           2.9675 |           0.1401 |
[32m[20230207 15:18:37 @agent_ppo2.py:192][0m |          -0.0090 |           2.9217 |           0.1400 |
[32m[20230207 15:18:37 @agent_ppo2.py:192][0m |          -0.0134 |           2.6104 |           0.1398 |
[32m[20230207 15:18:37 @agent_ppo2.py:192][0m |          -0.0209 |           2.4552 |           0.1398 |
[32m[20230207 15:18:38 @agent_ppo2.py:192][0m |          -0.0117 |           2.4373 |           0.1397 |
[32m[20230207 15:18:38 @agent_ppo2.py:192][0m |          -0.0103 |           2.2378 |           0.1395 |
[32m[20230207 15:18:38 @agent_ppo2.py:192][0m |          -0.0150 |           2.1776 |           0.1394 |
[32m[20230207 15:18:38 @agent_ppo2.py:192][0m |          -0.0156 |           2.0984 |           0.1394 |
[32m[20230207 15:18:38 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:18:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -30.21
[32m[20230207 15:18:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 92.86
[32m[20230207 15:18:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 54.65
[32m[20230207 15:18:39 @agent_ppo2.py:150][0m Total time:      17.96 min
[32m[20230207 15:18:39 @agent_ppo2.py:152][0m 931840 total steps have happened
[32m[20230207 15:18:39 @agent_ppo2.py:128][0m #------------------------ Iteration 455 --------------------------#
[32m[20230207 15:18:39 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:18:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:39 @agent_ppo2.py:192][0m |          -0.0209 |           1.1562 |           0.1343 |
[32m[20230207 15:18:40 @agent_ppo2.py:192][0m |           0.0169 |           0.9089 |           0.1338 |
[32m[20230207 15:18:40 @agent_ppo2.py:192][0m |          -0.0201 |           0.8605 |           0.1345 |
[32m[20230207 15:18:40 @agent_ppo2.py:192][0m |          -0.0092 |           0.8303 |           0.1344 |
[32m[20230207 15:18:40 @agent_ppo2.py:192][0m |          -0.0115 |           0.8130 |           0.1345 |
[32m[20230207 15:18:40 @agent_ppo2.py:192][0m |          -0.0184 |           0.8064 |           0.1345 |
[32m[20230207 15:18:40 @agent_ppo2.py:192][0m |          -0.0236 |           0.7926 |           0.1345 |
[32m[20230207 15:18:40 @agent_ppo2.py:192][0m |           0.0036 |           0.7854 |           0.1346 |
[32m[20230207 15:18:40 @agent_ppo2.py:192][0m |          -0.0023 |           0.7712 |           0.1346 |
[32m[20230207 15:18:40 @agent_ppo2.py:192][0m |          -0.0103 |           0.7648 |           0.1348 |
[32m[20230207 15:18:40 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: 40.60
[32m[20230207 15:18:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 65.44
[32m[20230207 15:18:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 61.11
[32m[20230207 15:18:41 @agent_ppo2.py:150][0m Total time:      18.00 min
[32m[20230207 15:18:41 @agent_ppo2.py:152][0m 933888 total steps have happened
[32m[20230207 15:18:41 @agent_ppo2.py:128][0m #------------------------ Iteration 456 --------------------------#
[32m[20230207 15:18:42 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:18:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:42 @agent_ppo2.py:192][0m |          -0.0087 |           0.9484 |           0.1389 |
[32m[20230207 15:18:42 @agent_ppo2.py:192][0m |           0.0014 |           0.8200 |           0.1386 |
[32m[20230207 15:18:42 @agent_ppo2.py:192][0m |          -0.0078 |           0.7767 |           0.1383 |
[32m[20230207 15:18:42 @agent_ppo2.py:192][0m |          -0.0166 |           0.7593 |           0.1385 |
[32m[20230207 15:18:42 @agent_ppo2.py:192][0m |           0.0005 |           0.7284 |           0.1383 |
[32m[20230207 15:18:42 @agent_ppo2.py:192][0m |          -0.0191 |           0.7140 |           0.1383 |
[32m[20230207 15:18:42 @agent_ppo2.py:192][0m |          -0.0371 |           0.7127 |           0.1383 |
[32m[20230207 15:18:42 @agent_ppo2.py:192][0m |          -0.0277 |           0.6974 |           0.1379 |
[32m[20230207 15:18:42 @agent_ppo2.py:192][0m |          -0.0157 |           0.6796 |           0.1379 |
[32m[20230207 15:18:43 @agent_ppo2.py:192][0m |          -0.0103 |           0.6700 |           0.1378 |
[32m[20230207 15:18:43 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: 20.12
[32m[20230207 15:18:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 42.87
[32m[20230207 15:18:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -82.89
[32m[20230207 15:18:43 @agent_ppo2.py:150][0m Total time:      18.03 min
[32m[20230207 15:18:43 @agent_ppo2.py:152][0m 935936 total steps have happened
[32m[20230207 15:18:43 @agent_ppo2.py:128][0m #------------------------ Iteration 457 --------------------------#
[32m[20230207 15:18:44 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:18:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:44 @agent_ppo2.py:192][0m |           0.0027 |           0.9522 |           0.1350 |
[32m[20230207 15:18:44 @agent_ppo2.py:192][0m |           0.0151 |           0.8019 |           0.1349 |
[32m[20230207 15:18:44 @agent_ppo2.py:192][0m |           0.0191 |           0.7654 |           0.1348 |
[32m[20230207 15:18:44 @agent_ppo2.py:192][0m |           0.0149 |           0.7421 |           0.1349 |
[32m[20230207 15:18:45 @agent_ppo2.py:192][0m |           0.0086 |           0.7290 |           0.1348 |
[32m[20230207 15:18:45 @agent_ppo2.py:192][0m |          -0.0076 |           0.7095 |           0.1349 |
[32m[20230207 15:18:45 @agent_ppo2.py:192][0m |          -0.0213 |           0.7005 |           0.1348 |
[32m[20230207 15:18:45 @agent_ppo2.py:192][0m |          -0.0297 |           0.6968 |           0.1348 |
[32m[20230207 15:18:45 @agent_ppo2.py:192][0m |          -0.0335 |           0.6870 |           0.1348 |
[32m[20230207 15:18:45 @agent_ppo2.py:192][0m |           0.0106 |           0.6726 |           0.1347 |
[32m[20230207 15:18:45 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: 13.56
[32m[20230207 15:18:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 32.64
[32m[20230207 15:18:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -65.37
[32m[20230207 15:18:46 @agent_ppo2.py:150][0m Total time:      18.08 min
[32m[20230207 15:18:46 @agent_ppo2.py:152][0m 937984 total steps have happened
[32m[20230207 15:18:46 @agent_ppo2.py:128][0m #------------------------ Iteration 458 --------------------------#
[32m[20230207 15:18:47 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:18:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:47 @agent_ppo2.py:192][0m |           0.0015 |           7.7436 |           0.1389 |
[32m[20230207 15:18:47 @agent_ppo2.py:192][0m |          -0.0026 |           3.0225 |           0.1388 |
[32m[20230207 15:18:47 @agent_ppo2.py:192][0m |          -0.0061 |           2.2752 |           0.1388 |
[32m[20230207 15:18:47 @agent_ppo2.py:192][0m |          -0.0075 |           1.7944 |           0.1388 |
[32m[20230207 15:18:47 @agent_ppo2.py:192][0m |          -0.0073 |           1.7364 |           0.1387 |
[32m[20230207 15:18:47 @agent_ppo2.py:192][0m |          -0.0076 |           1.4815 |           0.1387 |
[32m[20230207 15:18:47 @agent_ppo2.py:192][0m |          -0.0074 |           1.3663 |           0.1386 |
[32m[20230207 15:18:47 @agent_ppo2.py:192][0m |          -0.0092 |           1.3127 |           0.1387 |
[32m[20230207 15:18:47 @agent_ppo2.py:192][0m |          -0.0102 |           1.2501 |           0.1386 |
[32m[20230207 15:18:48 @agent_ppo2.py:192][0m |          -0.0096 |           1.2178 |           0.1386 |
[32m[20230207 15:18:48 @agent_ppo2.py:137][0m Policy update time: 0.92 s
[32m[20230207 15:18:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: 14.31
[32m[20230207 15:18:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 86.67
[32m[20230207 15:18:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 51.46
[32m[20230207 15:18:48 @agent_ppo2.py:150][0m Total time:      18.12 min
[32m[20230207 15:18:48 @agent_ppo2.py:152][0m 940032 total steps have happened
[32m[20230207 15:18:48 @agent_ppo2.py:128][0m #------------------------ Iteration 459 --------------------------#
[32m[20230207 15:18:49 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:18:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:49 @agent_ppo2.py:192][0m |           0.0098 |           1.1203 |           0.1352 |
[32m[20230207 15:18:49 @agent_ppo2.py:192][0m |           0.0001 |           0.9308 |           0.1352 |
[32m[20230207 15:18:49 @agent_ppo2.py:192][0m |           0.0009 |           0.9043 |           0.1351 |
[32m[20230207 15:18:50 @agent_ppo2.py:192][0m |          -0.0189 |           0.8784 |           0.1351 |
[32m[20230207 15:18:50 @agent_ppo2.py:192][0m |          -0.0129 |           0.8695 |           0.1351 |
[32m[20230207 15:18:50 @agent_ppo2.py:192][0m |          -0.0128 |           0.8633 |           0.1350 |
[32m[20230207 15:18:50 @agent_ppo2.py:192][0m |          -0.0089 |           0.8473 |           0.1350 |
[32m[20230207 15:18:50 @agent_ppo2.py:192][0m |          -0.0437 |           0.8736 |           0.1350 |
[32m[20230207 15:18:50 @agent_ppo2.py:192][0m |           0.0073 |           0.8989 |           0.1349 |
[32m[20230207 15:18:50 @agent_ppo2.py:192][0m |          -0.0267 |           0.8638 |           0.1349 |
[32m[20230207 15:18:50 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: 39.90
[32m[20230207 15:18:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 59.47
[32m[20230207 15:18:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 6.49
[32m[20230207 15:18:51 @agent_ppo2.py:150][0m Total time:      18.16 min
[32m[20230207 15:18:51 @agent_ppo2.py:152][0m 942080 total steps have happened
[32m[20230207 15:18:51 @agent_ppo2.py:128][0m #------------------------ Iteration 460 --------------------------#
[32m[20230207 15:18:52 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:18:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:52 @agent_ppo2.py:192][0m |          -0.0201 |           1.8593 |           0.1400 |
[32m[20230207 15:18:52 @agent_ppo2.py:192][0m |          -0.0035 |           1.3211 |           0.1399 |
[32m[20230207 15:18:52 @agent_ppo2.py:192][0m |           0.0032 |           1.2266 |           0.1398 |
[32m[20230207 15:18:52 @agent_ppo2.py:192][0m |           0.0021 |           1.1803 |           0.1398 |
[32m[20230207 15:18:52 @agent_ppo2.py:192][0m |          -0.0173 |           1.1585 |           0.1398 |
[32m[20230207 15:18:52 @agent_ppo2.py:192][0m |          -0.0014 |           1.0919 |           0.1397 |
[32m[20230207 15:18:52 @agent_ppo2.py:192][0m |          -0.0010 |           1.0604 |           0.1397 |
[32m[20230207 15:18:52 @agent_ppo2.py:192][0m |          -0.0121 |           1.0196 |           0.1398 |
[32m[20230207 15:18:52 @agent_ppo2.py:192][0m |          -0.0006 |           1.0055 |           0.1398 |
[32m[20230207 15:18:52 @agent_ppo2.py:192][0m |           0.0008 |           0.9991 |           0.1399 |
[32m[20230207 15:18:52 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: 71.12
[32m[20230207 15:18:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 127.36
[32m[20230207 15:18:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 42.80
[32m[20230207 15:18:53 @agent_ppo2.py:150][0m Total time:      18.20 min
[32m[20230207 15:18:53 @agent_ppo2.py:152][0m 944128 total steps have happened
[32m[20230207 15:18:53 @agent_ppo2.py:128][0m #------------------------ Iteration 461 --------------------------#
[32m[20230207 15:18:54 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:18:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:54 @agent_ppo2.py:192][0m |          -0.0005 |           1.3026 |           0.1358 |
[32m[20230207 15:18:54 @agent_ppo2.py:192][0m |           0.0055 |           1.1077 |           0.1355 |
[32m[20230207 15:18:54 @agent_ppo2.py:192][0m |          -0.0103 |           1.0458 |           0.1352 |
[32m[20230207 15:18:54 @agent_ppo2.py:192][0m |          -0.0126 |           1.0244 |           0.1353 |
[32m[20230207 15:18:54 @agent_ppo2.py:192][0m |          -0.0187 |           0.9878 |           0.1352 |
[32m[20230207 15:18:54 @agent_ppo2.py:192][0m |          -0.0156 |           0.9662 |           0.1352 |
[32m[20230207 15:18:55 @agent_ppo2.py:192][0m |          -0.0074 |           0.9555 |           0.1351 |
[32m[20230207 15:18:55 @agent_ppo2.py:192][0m |          -0.0272 |           0.9462 |           0.1350 |
[32m[20230207 15:18:55 @agent_ppo2.py:192][0m |          -0.0167 |           0.9285 |           0.1352 |
[32m[20230207 15:18:55 @agent_ppo2.py:192][0m |          -0.0188 |           0.9215 |           0.1351 |
[32m[20230207 15:18:55 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: -11.23
[32m[20230207 15:18:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -5.91
[32m[20230207 15:18:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 14.92
[32m[20230207 15:18:56 @agent_ppo2.py:150][0m Total time:      18.24 min
[32m[20230207 15:18:56 @agent_ppo2.py:152][0m 946176 total steps have happened
[32m[20230207 15:18:56 @agent_ppo2.py:128][0m #------------------------ Iteration 462 --------------------------#
[32m[20230207 15:18:56 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:18:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:57 @agent_ppo2.py:192][0m |           0.0000 |           7.6140 |           0.1388 |
[32m[20230207 15:18:57 @agent_ppo2.py:192][0m |          -0.0017 |           2.2480 |           0.1388 |
[32m[20230207 15:18:57 @agent_ppo2.py:192][0m |          -0.0009 |           2.0705 |           0.1389 |
[32m[20230207 15:18:57 @agent_ppo2.py:192][0m |          -0.0042 |           1.9801 |           0.1388 |
[32m[20230207 15:18:57 @agent_ppo2.py:192][0m |          -0.0057 |           1.9386 |           0.1387 |
[32m[20230207 15:18:57 @agent_ppo2.py:192][0m |          -0.0057 |           1.9199 |           0.1389 |
[32m[20230207 15:18:57 @agent_ppo2.py:192][0m |          -0.0068 |           1.8604 |           0.1389 |
[32m[20230207 15:18:57 @agent_ppo2.py:192][0m |          -0.0075 |           1.8567 |           0.1389 |
[32m[20230207 15:18:57 @agent_ppo2.py:192][0m |          -0.0084 |           1.8464 |           0.1389 |
[32m[20230207 15:18:57 @agent_ppo2.py:192][0m |          -0.0094 |           1.8245 |           0.1389 |
[32m[20230207 15:18:57 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:18:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: 8.18
[32m[20230207 15:18:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 68.36
[32m[20230207 15:18:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 69.15
[32m[20230207 15:18:58 @agent_ppo2.py:150][0m Total time:      18.28 min
[32m[20230207 15:18:58 @agent_ppo2.py:152][0m 948224 total steps have happened
[32m[20230207 15:18:58 @agent_ppo2.py:128][0m #------------------------ Iteration 463 --------------------------#
[32m[20230207 15:18:59 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:18:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:18:59 @agent_ppo2.py:192][0m |          -0.0019 |          18.8065 |           0.1362 |
[32m[20230207 15:18:59 @agent_ppo2.py:192][0m |          -0.0027 |           9.6729 |           0.1361 |
[32m[20230207 15:18:59 @agent_ppo2.py:192][0m |          -0.0092 |           7.6321 |           0.1361 |
[32m[20230207 15:18:59 @agent_ppo2.py:192][0m |          -0.0041 |           6.9811 |           0.1360 |
[32m[20230207 15:18:59 @agent_ppo2.py:192][0m |          -0.0094 |           5.7618 |           0.1361 |
[32m[20230207 15:18:59 @agent_ppo2.py:192][0m |          -0.0105 |           4.8252 |           0.1360 |
[32m[20230207 15:18:59 @agent_ppo2.py:192][0m |          -0.0128 |           4.3411 |           0.1359 |
[32m[20230207 15:18:59 @agent_ppo2.py:192][0m |          -0.0105 |           4.0353 |           0.1360 |
[32m[20230207 15:19:00 @agent_ppo2.py:192][0m |          -0.0154 |           3.6455 |           0.1360 |
[32m[20230207 15:19:00 @agent_ppo2.py:192][0m |          -0.0173 |           3.4019 |           0.1359 |
[32m[20230207 15:19:00 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:19:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: -85.14
[32m[20230207 15:19:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -2.91
[32m[20230207 15:19:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -29.74
[32m[20230207 15:19:00 @agent_ppo2.py:150][0m Total time:      18.32 min
[32m[20230207 15:19:00 @agent_ppo2.py:152][0m 950272 total steps have happened
[32m[20230207 15:19:00 @agent_ppo2.py:128][0m #------------------------ Iteration 464 --------------------------#
[32m[20230207 15:19:01 @agent_ppo2.py:134][0m Sampling time: 0.73 s by 1 slaves
[32m[20230207 15:19:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:01 @agent_ppo2.py:192][0m |          -0.0007 |          21.0192 |           0.1427 |
[32m[20230207 15:19:01 @agent_ppo2.py:192][0m |          -0.0000 |           6.8845 |           0.1426 |
[32m[20230207 15:19:01 @agent_ppo2.py:192][0m |          -0.0045 |           5.1423 |           0.1425 |
[32m[20230207 15:19:01 @agent_ppo2.py:192][0m |          -0.0078 |           4.6593 |           0.1423 |
[32m[20230207 15:19:01 @agent_ppo2.py:192][0m |          -0.0092 |           4.1961 |           0.1422 |
[32m[20230207 15:19:01 @agent_ppo2.py:192][0m |          -0.0093 |           4.0731 |           0.1422 |
[32m[20230207 15:19:01 @agent_ppo2.py:192][0m |          -0.0114 |           3.7060 |           0.1422 |
[32m[20230207 15:19:02 @agent_ppo2.py:192][0m |          -0.0075 |           3.7768 |           0.1421 |
[32m[20230207 15:19:02 @agent_ppo2.py:192][0m |          -0.0128 |           3.2612 |           0.1421 |
[32m[20230207 15:19:02 @agent_ppo2.py:192][0m |          -0.0139 |           3.2887 |           0.1420 |
[32m[20230207 15:19:02 @agent_ppo2.py:137][0m Policy update time: 0.74 s
[32m[20230207 15:19:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -78.04
[32m[20230207 15:19:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 44.40
[32m[20230207 15:19:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -9.62
[32m[20230207 15:19:03 @agent_ppo2.py:150][0m Total time:      18.36 min
[32m[20230207 15:19:03 @agent_ppo2.py:152][0m 952320 total steps have happened
[32m[20230207 15:19:03 @agent_ppo2.py:128][0m #------------------------ Iteration 465 --------------------------#
[32m[20230207 15:19:03 @agent_ppo2.py:134][0m Sampling time: 0.50 s by 1 slaves
[32m[20230207 15:19:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:03 @agent_ppo2.py:192][0m |          -0.0061 |          29.2978 |           0.1382 |
[32m[20230207 15:19:03 @agent_ppo2.py:192][0m |           0.0284 |          18.5114 |           0.1378 |
[32m[20230207 15:19:03 @agent_ppo2.py:192][0m |          -0.0070 |           7.0603 |           0.1376 |
[32m[20230207 15:19:03 @agent_ppo2.py:192][0m |          -0.0114 |           4.4157 |           0.1374 |
[32m[20230207 15:19:03 @agent_ppo2.py:192][0m |          -0.0179 |           3.6975 |           0.1374 |
[32m[20230207 15:19:03 @agent_ppo2.py:192][0m |          -0.0117 |           3.2850 |           0.1375 |
[32m[20230207 15:19:03 @agent_ppo2.py:192][0m |          -0.0151 |           2.9980 |           0.1373 |
[32m[20230207 15:19:03 @agent_ppo2.py:192][0m |          -0.0096 |           2.7900 |           0.1373 |
[32m[20230207 15:19:04 @agent_ppo2.py:192][0m |          -0.0141 |           2.5732 |           0.1372 |
[32m[20230207 15:19:04 @agent_ppo2.py:192][0m |          -0.0119 |           2.3556 |           0.1371 |
[32m[20230207 15:19:04 @agent_ppo2.py:137][0m Policy update time: 0.53 s
[32m[20230207 15:19:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: -11.85
[32m[20230207 15:19:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 87.24
[32m[20230207 15:19:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -11.35
[32m[20230207 15:19:04 @agent_ppo2.py:150][0m Total time:      18.38 min
[32m[20230207 15:19:04 @agent_ppo2.py:152][0m 954368 total steps have happened
[32m[20230207 15:19:04 @agent_ppo2.py:128][0m #------------------------ Iteration 466 --------------------------#
[32m[20230207 15:19:05 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:19:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:05 @agent_ppo2.py:192][0m |           0.0045 |           8.4371 |           0.1378 |
[32m[20230207 15:19:05 @agent_ppo2.py:192][0m |          -0.0024 |           5.4723 |           0.1376 |
[32m[20230207 15:19:05 @agent_ppo2.py:192][0m |          -0.0816 |           7.1942 |           0.1376 |
[32m[20230207 15:19:05 @agent_ppo2.py:192][0m |          -0.0028 |           5.6022 |           0.1374 |
[32m[20230207 15:19:06 @agent_ppo2.py:192][0m |          -0.0252 |           4.1576 |           0.1374 |
[32m[20230207 15:19:06 @agent_ppo2.py:192][0m |           0.0029 |           3.9682 |           0.1371 |
[32m[20230207 15:19:06 @agent_ppo2.py:192][0m |          -0.0095 |           3.7966 |           0.1371 |
[32m[20230207 15:19:06 @agent_ppo2.py:192][0m |          -0.0129 |           3.6925 |           0.1370 |
[32m[20230207 15:19:06 @agent_ppo2.py:192][0m |          -0.0135 |           3.5591 |           0.1369 |
[32m[20230207 15:19:06 @agent_ppo2.py:192][0m |          -0.0002 |           3.6036 |           0.1370 |
[32m[20230207 15:19:06 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:19:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: 14.58
[32m[20230207 15:19:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 47.14
[32m[20230207 15:19:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 51.44
[32m[20230207 15:19:07 @agent_ppo2.py:150][0m Total time:      18.42 min
[32m[20230207 15:19:07 @agent_ppo2.py:152][0m 956416 total steps have happened
[32m[20230207 15:19:07 @agent_ppo2.py:128][0m #------------------------ Iteration 467 --------------------------#
[32m[20230207 15:19:07 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:19:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:08 @agent_ppo2.py:192][0m |           0.0036 |           8.2518 |           0.1376 |
[32m[20230207 15:19:08 @agent_ppo2.py:192][0m |          -0.0088 |           4.0917 |           0.1374 |
[32m[20230207 15:19:08 @agent_ppo2.py:192][0m |          -0.0036 |           3.3618 |           0.1373 |
[32m[20230207 15:19:08 @agent_ppo2.py:192][0m |          -0.0115 |           3.0124 |           0.1373 |
[32m[20230207 15:19:08 @agent_ppo2.py:192][0m |          -0.0097 |           2.6478 |           0.1373 |
[32m[20230207 15:19:08 @agent_ppo2.py:192][0m |          -0.0065 |           2.4772 |           0.1372 |
[32m[20230207 15:19:08 @agent_ppo2.py:192][0m |          -0.0051 |           2.3594 |           0.1373 |
[32m[20230207 15:19:08 @agent_ppo2.py:192][0m |          -0.0085 |           2.1229 |           0.1372 |
[32m[20230207 15:19:08 @agent_ppo2.py:192][0m |          -0.0113 |           2.0102 |           0.1371 |
[32m[20230207 15:19:08 @agent_ppo2.py:192][0m |          -0.0121 |           1.9152 |           0.1371 |
[32m[20230207 15:19:08 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:19:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: 27.64
[32m[20230207 15:19:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 94.25
[32m[20230207 15:19:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 36.16
[32m[20230207 15:19:09 @agent_ppo2.py:150][0m Total time:      18.46 min
[32m[20230207 15:19:09 @agent_ppo2.py:152][0m 958464 total steps have happened
[32m[20230207 15:19:09 @agent_ppo2.py:128][0m #------------------------ Iteration 468 --------------------------#
[32m[20230207 15:19:10 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:19:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:10 @agent_ppo2.py:192][0m |           0.0023 |          11.0056 |           0.1387 |
[32m[20230207 15:19:10 @agent_ppo2.py:192][0m |          -0.0020 |           3.2303 |           0.1387 |
[32m[20230207 15:19:10 @agent_ppo2.py:192][0m |          -0.0065 |           2.3301 |           0.1387 |
[32m[20230207 15:19:10 @agent_ppo2.py:192][0m |          -0.0071 |           1.8162 |           0.1386 |
[32m[20230207 15:19:10 @agent_ppo2.py:192][0m |          -0.0078 |           1.5350 |           0.1386 |
[32m[20230207 15:19:10 @agent_ppo2.py:192][0m |          -0.0092 |           1.3763 |           0.1385 |
[32m[20230207 15:19:11 @agent_ppo2.py:192][0m |          -0.0091 |           1.3168 |           0.1385 |
[32m[20230207 15:19:11 @agent_ppo2.py:192][0m |          -0.0105 |           1.2866 |           0.1385 |
[32m[20230207 15:19:11 @agent_ppo2.py:192][0m |          -0.0114 |           1.1427 |           0.1384 |
[32m[20230207 15:19:11 @agent_ppo2.py:192][0m |          -0.0111 |           1.1155 |           0.1384 |
[32m[20230207 15:19:11 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:19:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: 4.92
[32m[20230207 15:19:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 68.44
[32m[20230207 15:19:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 47.91
[32m[20230207 15:19:12 @agent_ppo2.py:150][0m Total time:      18.51 min
[32m[20230207 15:19:12 @agent_ppo2.py:152][0m 960512 total steps have happened
[32m[20230207 15:19:12 @agent_ppo2.py:128][0m #------------------------ Iteration 469 --------------------------#
[32m[20230207 15:19:12 @agent_ppo2.py:134][0m Sampling time: 0.74 s by 1 slaves
[32m[20230207 15:19:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:12 @agent_ppo2.py:192][0m |           0.0001 |          18.8702 |           0.1366 |
[32m[20230207 15:19:12 @agent_ppo2.py:192][0m |          -0.0065 |          10.3839 |           0.1363 |
[32m[20230207 15:19:13 @agent_ppo2.py:192][0m |          -0.0046 |           8.5119 |           0.1363 |
[32m[20230207 15:19:13 @agent_ppo2.py:192][0m |          -0.0097 |           7.7309 |           0.1363 |
[32m[20230207 15:19:13 @agent_ppo2.py:192][0m |          -0.0112 |           7.1000 |           0.1362 |
[32m[20230207 15:19:13 @agent_ppo2.py:192][0m |          -0.0096 |           6.6710 |           0.1362 |
[32m[20230207 15:19:13 @agent_ppo2.py:192][0m |          -0.0119 |           6.3354 |           0.1362 |
[32m[20230207 15:19:13 @agent_ppo2.py:192][0m |          -0.0114 |           6.0413 |           0.1361 |
[32m[20230207 15:19:13 @agent_ppo2.py:192][0m |          -0.0162 |           5.6557 |           0.1361 |
[32m[20230207 15:19:13 @agent_ppo2.py:192][0m |          -0.0132 |           5.1636 |           0.1361 |
[32m[20230207 15:19:13 @agent_ppo2.py:137][0m Policy update time: 0.78 s
[32m[20230207 15:19:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: -52.14
[32m[20230207 15:19:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 69.38
[32m[20230207 15:19:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -30.98
[32m[20230207 15:19:14 @agent_ppo2.py:150][0m Total time:      18.54 min
[32m[20230207 15:19:14 @agent_ppo2.py:152][0m 962560 total steps have happened
[32m[20230207 15:19:14 @agent_ppo2.py:128][0m #------------------------ Iteration 470 --------------------------#
[32m[20230207 15:19:15 @agent_ppo2.py:134][0m Sampling time: 0.69 s by 1 slaves
[32m[20230207 15:19:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:15 @agent_ppo2.py:192][0m |           0.0003 |          13.7269 |           0.1362 |
[32m[20230207 15:19:15 @agent_ppo2.py:192][0m |           0.0013 |           5.6229 |           0.1361 |
[32m[20230207 15:19:15 @agent_ppo2.py:192][0m |          -0.0098 |           4.6477 |           0.1360 |
[32m[20230207 15:19:15 @agent_ppo2.py:192][0m |          -0.0089 |           4.0630 |           0.1358 |
[32m[20230207 15:19:15 @agent_ppo2.py:192][0m |          -0.0133 |           3.5215 |           0.1359 |
[32m[20230207 15:19:15 @agent_ppo2.py:192][0m |          -0.0078 |           3.5065 |           0.1358 |
[32m[20230207 15:19:15 @agent_ppo2.py:192][0m |          -0.0057 |           3.1464 |           0.1359 |
[32m[20230207 15:19:15 @agent_ppo2.py:192][0m |          -0.0094 |           2.8589 |           0.1357 |
[32m[20230207 15:19:15 @agent_ppo2.py:192][0m |          -0.0026 |           2.8469 |           0.1357 |
[32m[20230207 15:19:15 @agent_ppo2.py:192][0m |          -0.0119 |           2.7112 |           0.1357 |
[32m[20230207 15:19:15 @agent_ppo2.py:137][0m Policy update time: 0.72 s
[32m[20230207 15:19:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -26.80
[32m[20230207 15:19:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 37.54
[32m[20230207 15:19:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 43.75
[32m[20230207 15:19:16 @agent_ppo2.py:150][0m Total time:      18.58 min
[32m[20230207 15:19:16 @agent_ppo2.py:152][0m 964608 total steps have happened
[32m[20230207 15:19:16 @agent_ppo2.py:128][0m #------------------------ Iteration 471 --------------------------#
[32m[20230207 15:19:17 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:19:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:17 @agent_ppo2.py:192][0m |           0.0147 |           1.2791 |           0.1351 |
[32m[20230207 15:19:17 @agent_ppo2.py:192][0m |           0.0139 |           1.0256 |           0.1349 |
[32m[20230207 15:19:17 @agent_ppo2.py:192][0m |          -0.0031 |           0.9610 |           0.1348 |
[32m[20230207 15:19:17 @agent_ppo2.py:192][0m |          -0.0057 |           0.9297 |           0.1347 |
[32m[20230207 15:19:17 @agent_ppo2.py:192][0m |          -0.0195 |           0.9059 |           0.1346 |
[32m[20230207 15:19:17 @agent_ppo2.py:192][0m |          -0.0096 |           0.8879 |           0.1346 |
[32m[20230207 15:19:17 @agent_ppo2.py:192][0m |          -0.0239 |           0.8697 |           0.1345 |
[32m[20230207 15:19:17 @agent_ppo2.py:192][0m |          -0.0158 |           0.8518 |           0.1344 |
[32m[20230207 15:19:18 @agent_ppo2.py:192][0m |          -0.0118 |           0.8427 |           0.1345 |
[32m[20230207 15:19:18 @agent_ppo2.py:192][0m |          -0.0084 |           0.8332 |           0.1344 |
[32m[20230207 15:19:18 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:19:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: 44.50
[32m[20230207 15:19:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 47.72
[32m[20230207 15:19:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -24.17
[32m[20230207 15:19:18 @agent_ppo2.py:150][0m Total time:      18.62 min
[32m[20230207 15:19:18 @agent_ppo2.py:152][0m 966656 total steps have happened
[32m[20230207 15:19:18 @agent_ppo2.py:128][0m #------------------------ Iteration 472 --------------------------#
[32m[20230207 15:19:19 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:19:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:19 @agent_ppo2.py:192][0m |          -0.0056 |          10.3367 |           0.1352 |
[32m[20230207 15:19:20 @agent_ppo2.py:192][0m |           0.0375 |           5.5667 |           0.1352 |
[32m[20230207 15:19:20 @agent_ppo2.py:192][0m |          -0.0044 |           3.9766 |           0.1347 |
[32m[20230207 15:19:20 @agent_ppo2.py:192][0m |          -0.0075 |           2.6480 |           0.1350 |
[32m[20230207 15:19:20 @agent_ppo2.py:192][0m |           0.0201 |           2.1287 |           0.1350 |
[32m[20230207 15:19:20 @agent_ppo2.py:192][0m |          -0.0075 |           1.8160 |           0.1349 |
[32m[20230207 15:19:20 @agent_ppo2.py:192][0m |          -0.0090 |           1.6584 |           0.1349 |
[32m[20230207 15:19:20 @agent_ppo2.py:192][0m |          -0.0122 |           1.5043 |           0.1349 |
[32m[20230207 15:19:20 @agent_ppo2.py:192][0m |          -0.0007 |           1.4228 |           0.1349 |
[32m[20230207 15:19:20 @agent_ppo2.py:192][0m |          -0.0101 |           1.3516 |           0.1349 |
[32m[20230207 15:19:20 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:19:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -29.96
[32m[20230207 15:19:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 87.77
[32m[20230207 15:19:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 9.56
[32m[20230207 15:19:21 @agent_ppo2.py:150][0m Total time:      18.66 min
[32m[20230207 15:19:21 @agent_ppo2.py:152][0m 968704 total steps have happened
[32m[20230207 15:19:21 @agent_ppo2.py:128][0m #------------------------ Iteration 473 --------------------------#
[32m[20230207 15:19:22 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:19:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:22 @agent_ppo2.py:192][0m |           0.0046 |           1.0998 |           0.1346 |
[32m[20230207 15:19:22 @agent_ppo2.py:192][0m |          -0.0058 |           0.8887 |           0.1345 |
[32m[20230207 15:19:22 @agent_ppo2.py:192][0m |          -0.0056 |           0.8485 |           0.1345 |
[32m[20230207 15:19:22 @agent_ppo2.py:192][0m |           0.0022 |           0.8085 |           0.1344 |
[32m[20230207 15:19:22 @agent_ppo2.py:192][0m |          -0.0034 |           0.7912 |           0.1345 |
[32m[20230207 15:19:22 @agent_ppo2.py:192][0m |          -0.0080 |           0.7745 |           0.1345 |
[32m[20230207 15:19:22 @agent_ppo2.py:192][0m |          -0.0049 |           0.7778 |           0.1345 |
[32m[20230207 15:19:22 @agent_ppo2.py:192][0m |          -0.0066 |           0.7551 |           0.1344 |
[32m[20230207 15:19:23 @agent_ppo2.py:192][0m |          -0.0336 |           0.7530 |           0.1345 |
[32m[20230207 15:19:23 @agent_ppo2.py:192][0m |          -0.0062 |           0.7424 |           0.1348 |
[32m[20230207 15:19:23 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:19:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: 20.44
[32m[20230207 15:19:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 47.60
[32m[20230207 15:19:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 25.04
[32m[20230207 15:19:23 @agent_ppo2.py:150][0m Total time:      18.70 min
[32m[20230207 15:19:23 @agent_ppo2.py:152][0m 970752 total steps have happened
[32m[20230207 15:19:23 @agent_ppo2.py:128][0m #------------------------ Iteration 474 --------------------------#
[32m[20230207 15:19:24 @agent_ppo2.py:134][0m Sampling time: 0.51 s by 1 slaves
[32m[20230207 15:19:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:24 @agent_ppo2.py:192][0m |          -0.0010 |           9.8139 |           0.1375 |
[32m[20230207 15:19:24 @agent_ppo2.py:192][0m |          -0.0063 |           3.6741 |           0.1376 |
[32m[20230207 15:19:24 @agent_ppo2.py:192][0m |          -0.0093 |           3.0806 |           0.1375 |
[32m[20230207 15:19:24 @agent_ppo2.py:192][0m |          -0.0096 |           2.8269 |           0.1376 |
[32m[20230207 15:19:24 @agent_ppo2.py:192][0m |          -0.0117 |           2.5757 |           0.1376 |
[32m[20230207 15:19:24 @agent_ppo2.py:192][0m |          -0.0089 |           2.3889 |           0.1375 |
[32m[20230207 15:19:24 @agent_ppo2.py:192][0m |          -0.0087 |           2.2755 |           0.1375 |
[32m[20230207 15:19:24 @agent_ppo2.py:192][0m |          -0.0127 |           2.1987 |           0.1375 |
[32m[20230207 15:19:24 @agent_ppo2.py:192][0m |          -0.0174 |           2.1120 |           0.1375 |
[32m[20230207 15:19:24 @agent_ppo2.py:192][0m |          -0.0162 |           2.0465 |           0.1375 |
[32m[20230207 15:19:24 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:19:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -47.40
[32m[20230207 15:19:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 2.16
[32m[20230207 15:19:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -43.04
[32m[20230207 15:19:25 @agent_ppo2.py:150][0m Total time:      18.73 min
[32m[20230207 15:19:25 @agent_ppo2.py:152][0m 972800 total steps have happened
[32m[20230207 15:19:25 @agent_ppo2.py:128][0m #------------------------ Iteration 475 --------------------------#
[32m[20230207 15:19:26 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:19:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:26 @agent_ppo2.py:192][0m |           0.0016 |           6.0425 |           0.1351 |
[32m[20230207 15:19:26 @agent_ppo2.py:192][0m |           0.0009 |           3.2746 |           0.1348 |
[32m[20230207 15:19:26 @agent_ppo2.py:192][0m |          -0.0048 |           2.8731 |           0.1348 |
[32m[20230207 15:19:26 @agent_ppo2.py:192][0m |          -0.0059 |           2.5546 |           0.1348 |
[32m[20230207 15:19:26 @agent_ppo2.py:192][0m |          -0.0120 |           2.3579 |           0.1347 |
[32m[20230207 15:19:26 @agent_ppo2.py:192][0m |          -0.0112 |           2.1907 |           0.1347 |
[32m[20230207 15:19:27 @agent_ppo2.py:192][0m |          -0.0114 |           2.0861 |           0.1347 |
[32m[20230207 15:19:27 @agent_ppo2.py:192][0m |          -0.0140 |           1.9842 |           0.1347 |
[32m[20230207 15:19:27 @agent_ppo2.py:192][0m |          -0.0130 |           1.9537 |           0.1347 |
[32m[20230207 15:19:27 @agent_ppo2.py:192][0m |          -0.0148 |           1.8634 |           0.1346 |
[32m[20230207 15:19:27 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:19:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: -2.18
[32m[20230207 15:19:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 89.75
[32m[20230207 15:19:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 7.19
[32m[20230207 15:19:28 @agent_ppo2.py:150][0m Total time:      18.77 min
[32m[20230207 15:19:28 @agent_ppo2.py:152][0m 974848 total steps have happened
[32m[20230207 15:19:28 @agent_ppo2.py:128][0m #------------------------ Iteration 476 --------------------------#
[32m[20230207 15:19:28 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:19:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:29 @agent_ppo2.py:192][0m |           0.0009 |           4.7639 |           0.1381 |
[32m[20230207 15:19:29 @agent_ppo2.py:192][0m |          -0.0023 |           2.5343 |           0.1378 |
[32m[20230207 15:19:29 @agent_ppo2.py:192][0m |          -0.0082 |           2.2868 |           0.1376 |
[32m[20230207 15:19:29 @agent_ppo2.py:192][0m |          -0.0080 |           2.1538 |           0.1376 |
[32m[20230207 15:19:29 @agent_ppo2.py:192][0m |          -0.0114 |           2.0926 |           0.1375 |
[32m[20230207 15:19:29 @agent_ppo2.py:192][0m |          -0.0084 |           2.0462 |           0.1375 |
[32m[20230207 15:19:29 @agent_ppo2.py:192][0m |          -0.0097 |           2.0047 |           0.1374 |
[32m[20230207 15:19:29 @agent_ppo2.py:192][0m |          -0.0117 |           1.9831 |           0.1374 |
[32m[20230207 15:19:29 @agent_ppo2.py:192][0m |          -0.0120 |           1.9566 |           0.1373 |
[32m[20230207 15:19:29 @agent_ppo2.py:192][0m |          -0.0138 |           1.9215 |           0.1373 |
[32m[20230207 15:19:29 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:19:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -15.97
[32m[20230207 15:19:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 47.43
[32m[20230207 15:19:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 30.79
[32m[20230207 15:19:30 @agent_ppo2.py:150][0m Total time:      18.82 min
[32m[20230207 15:19:30 @agent_ppo2.py:152][0m 976896 total steps have happened
[32m[20230207 15:19:30 @agent_ppo2.py:128][0m #------------------------ Iteration 477 --------------------------#
[32m[20230207 15:19:31 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:19:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:31 @agent_ppo2.py:192][0m |           0.0018 |           1.8668 |           0.1361 |
[32m[20230207 15:19:31 @agent_ppo2.py:192][0m |          -0.0165 |           1.3895 |           0.1361 |
[32m[20230207 15:19:31 @agent_ppo2.py:192][0m |          -0.0100 |           1.2740 |           0.1360 |
[32m[20230207 15:19:31 @agent_ppo2.py:192][0m |           0.0248 |           1.2686 |           0.1359 |
[32m[20230207 15:19:31 @agent_ppo2.py:192][0m |           0.0033 |           1.1604 |           0.1360 |
[32m[20230207 15:19:31 @agent_ppo2.py:192][0m |          -0.0054 |           1.0931 |           0.1360 |
[32m[20230207 15:19:31 @agent_ppo2.py:192][0m |           0.0034 |           1.0524 |           0.1360 |
[32m[20230207 15:19:32 @agent_ppo2.py:192][0m |          -0.0141 |           1.0380 |           0.1360 |
[32m[20230207 15:19:32 @agent_ppo2.py:192][0m |          -0.0244 |           1.0076 |           0.1361 |
[32m[20230207 15:19:32 @agent_ppo2.py:192][0m |          -0.0295 |           0.9815 |           0.1359 |
[32m[20230207 15:19:32 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:19:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: 43.85
[32m[20230207 15:19:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 97.32
[32m[20230207 15:19:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 116.82
[32m[20230207 15:19:32 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 116.82
[32m[20230207 15:19:32 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 116.82
[32m[20230207 15:19:32 @agent_ppo2.py:150][0m Total time:      18.85 min
[32m[20230207 15:19:32 @agent_ppo2.py:152][0m 978944 total steps have happened
[32m[20230207 15:19:32 @agent_ppo2.py:128][0m #------------------------ Iteration 478 --------------------------#
[32m[20230207 15:19:33 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:19:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:33 @agent_ppo2.py:192][0m |           0.0045 |           1.2265 |           0.1348 |
[32m[20230207 15:19:33 @agent_ppo2.py:192][0m |          -0.0013 |           1.0128 |           0.1347 |
[32m[20230207 15:19:34 @agent_ppo2.py:192][0m |          -0.0075 |           0.9679 |           0.1346 |
[32m[20230207 15:19:34 @agent_ppo2.py:192][0m |          -0.0242 |           0.9850 |           0.1345 |
[32m[20230207 15:19:34 @agent_ppo2.py:192][0m |          -0.0711 |           1.0135 |           0.1344 |
[32m[20230207 15:19:34 @agent_ppo2.py:192][0m |          -0.0173 |           0.9343 |           0.1341 |
[32m[20230207 15:19:34 @agent_ppo2.py:192][0m |          -0.0102 |           0.8921 |           0.1342 |
[32m[20230207 15:19:34 @agent_ppo2.py:192][0m |           0.0049 |           0.8809 |           0.1343 |
[32m[20230207 15:19:34 @agent_ppo2.py:192][0m |          -0.0064 |           0.8676 |           0.1342 |
[32m[20230207 15:19:34 @agent_ppo2.py:192][0m |          -0.0292 |           0.8636 |           0.1342 |
[32m[20230207 15:19:34 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:19:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: 59.79
[32m[20230207 15:19:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 75.62
[32m[20230207 15:19:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 25.72
[32m[20230207 15:19:35 @agent_ppo2.py:150][0m Total time:      18.89 min
[32m[20230207 15:19:35 @agent_ppo2.py:152][0m 980992 total steps have happened
[32m[20230207 15:19:35 @agent_ppo2.py:128][0m #------------------------ Iteration 479 --------------------------#
[32m[20230207 15:19:36 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:19:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:36 @agent_ppo2.py:192][0m |           0.0192 |           5.1876 |           0.1340 |
[32m[20230207 15:19:36 @agent_ppo2.py:192][0m |           0.0065 |           3.0711 |           0.1338 |
[32m[20230207 15:19:36 @agent_ppo2.py:192][0m |           0.0057 |           2.6708 |           0.1335 |
[32m[20230207 15:19:36 @agent_ppo2.py:192][0m |          -0.0083 |           2.4588 |           0.1338 |
[32m[20230207 15:19:36 @agent_ppo2.py:192][0m |          -0.0082 |           2.2764 |           0.1337 |
[32m[20230207 15:19:36 @agent_ppo2.py:192][0m |          -0.0339 |           2.1753 |           0.1337 |
[32m[20230207 15:19:36 @agent_ppo2.py:192][0m |          -0.0353 |           2.2340 |           0.1334 |
[32m[20230207 15:19:36 @agent_ppo2.py:192][0m |          -0.0084 |           2.0913 |           0.1335 |
[32m[20230207 15:19:36 @agent_ppo2.py:192][0m |          -0.0123 |           1.9456 |           0.1337 |
[32m[20230207 15:19:36 @agent_ppo2.py:192][0m |          -0.0060 |           1.9267 |           0.1337 |
[32m[20230207 15:19:36 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:19:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: 114.04
[32m[20230207 15:19:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 141.22
[32m[20230207 15:19:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 30.36
[32m[20230207 15:19:37 @agent_ppo2.py:150][0m Total time:      18.93 min
[32m[20230207 15:19:37 @agent_ppo2.py:152][0m 983040 total steps have happened
[32m[20230207 15:19:37 @agent_ppo2.py:128][0m #------------------------ Iteration 480 --------------------------#
[32m[20230207 15:19:38 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:19:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:38 @agent_ppo2.py:192][0m |          -0.0071 |           1.5110 |           0.1366 |
[32m[20230207 15:19:38 @agent_ppo2.py:192][0m |          -0.0180 |           1.1259 |           0.1365 |
[32m[20230207 15:19:38 @agent_ppo2.py:192][0m |          -0.0046 |           0.9897 |           0.1365 |
[32m[20230207 15:19:38 @agent_ppo2.py:192][0m |          -0.0651 |           0.9896 |           0.1365 |
[32m[20230207 15:19:38 @agent_ppo2.py:192][0m |          -0.0146 |           0.9846 |           0.1365 |
[32m[20230207 15:19:39 @agent_ppo2.py:192][0m |          -0.0064 |           0.8404 |           0.1364 |
[32m[20230207 15:19:39 @agent_ppo2.py:192][0m |          -0.0166 |           0.8214 |           0.1364 |
[32m[20230207 15:19:39 @agent_ppo2.py:192][0m |           0.0085 |           0.7913 |           0.1364 |
[32m[20230207 15:19:39 @agent_ppo2.py:192][0m |          -0.0117 |           0.7724 |           0.1364 |
[32m[20230207 15:19:39 @agent_ppo2.py:192][0m |          -0.0170 |           0.7670 |           0.1363 |
[32m[20230207 15:19:39 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:19:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: -49.52
[32m[20230207 15:19:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -27.09
[32m[20230207 15:19:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 46.04
[32m[20230207 15:19:40 @agent_ppo2.py:150][0m Total time:      18.97 min
[32m[20230207 15:19:40 @agent_ppo2.py:152][0m 985088 total steps have happened
[32m[20230207 15:19:40 @agent_ppo2.py:128][0m #------------------------ Iteration 481 --------------------------#
[32m[20230207 15:19:40 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:19:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:40 @agent_ppo2.py:192][0m |          -0.0021 |          12.1123 |           0.1290 |
[32m[20230207 15:19:40 @agent_ppo2.py:192][0m |          -0.0043 |           5.5482 |           0.1290 |
[32m[20230207 15:19:40 @agent_ppo2.py:192][0m |          -0.0101 |           4.2856 |           0.1290 |
[32m[20230207 15:19:40 @agent_ppo2.py:192][0m |          -0.0096 |           3.9118 |           0.1289 |
[32m[20230207 15:19:40 @agent_ppo2.py:192][0m |          -0.0125 |           3.3907 |           0.1290 |
[32m[20230207 15:19:41 @agent_ppo2.py:192][0m |          -0.0159 |           3.1929 |           0.1289 |
[32m[20230207 15:19:41 @agent_ppo2.py:192][0m |           0.0009 |           3.1289 |           0.1289 |
[32m[20230207 15:19:41 @agent_ppo2.py:192][0m |          -0.0149 |           2.7843 |           0.1289 |
[32m[20230207 15:19:41 @agent_ppo2.py:192][0m |           0.0084 |           2.5433 |           0.1289 |
[32m[20230207 15:19:41 @agent_ppo2.py:192][0m |          -0.0153 |           2.4904 |           0.1289 |
[32m[20230207 15:19:41 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:19:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -57.16
[32m[20230207 15:19:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 73.41
[32m[20230207 15:19:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -5.78
[32m[20230207 15:19:41 @agent_ppo2.py:150][0m Total time:      19.00 min
[32m[20230207 15:19:41 @agent_ppo2.py:152][0m 987136 total steps have happened
[32m[20230207 15:19:41 @agent_ppo2.py:128][0m #------------------------ Iteration 482 --------------------------#
[32m[20230207 15:19:42 @agent_ppo2.py:134][0m Sampling time: 0.64 s by 1 slaves
[32m[20230207 15:19:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:42 @agent_ppo2.py:192][0m |           0.0019 |          12.3725 |           0.1412 |
[32m[20230207 15:19:42 @agent_ppo2.py:192][0m |          -0.0024 |           5.3935 |           0.1410 |
[32m[20230207 15:19:42 @agent_ppo2.py:192][0m |          -0.0050 |           2.9890 |           0.1410 |
[32m[20230207 15:19:42 @agent_ppo2.py:192][0m |          -0.0071 |           2.7974 |           0.1409 |
[32m[20230207 15:19:42 @agent_ppo2.py:192][0m |          -0.0093 |           2.4484 |           0.1408 |
[32m[20230207 15:19:42 @agent_ppo2.py:192][0m |          -0.0096 |           2.3608 |           0.1408 |
[32m[20230207 15:19:42 @agent_ppo2.py:192][0m |          -0.0107 |           2.2940 |           0.1407 |
[32m[20230207 15:19:42 @agent_ppo2.py:192][0m |          -0.0107 |           2.1539 |           0.1406 |
[32m[20230207 15:19:43 @agent_ppo2.py:192][0m |          -0.0121 |           2.1624 |           0.1406 |
[32m[20230207 15:19:43 @agent_ppo2.py:192][0m |          -0.0126 |           2.0919 |           0.1405 |
[32m[20230207 15:19:43 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230207 15:19:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -77.43
[32m[20230207 15:19:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -3.15
[32m[20230207 15:19:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -12.06
[32m[20230207 15:19:43 @agent_ppo2.py:150][0m Total time:      19.04 min
[32m[20230207 15:19:43 @agent_ppo2.py:152][0m 989184 total steps have happened
[32m[20230207 15:19:43 @agent_ppo2.py:128][0m #------------------------ Iteration 483 --------------------------#
[32m[20230207 15:19:44 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:19:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:44 @agent_ppo2.py:192][0m |           0.0113 |           0.8767 |           0.1350 |
[32m[20230207 15:19:44 @agent_ppo2.py:192][0m |          -0.0346 |           0.8268 |           0.1346 |
[32m[20230207 15:19:44 @agent_ppo2.py:192][0m |          -0.0130 |           0.7860 |           0.1345 |
[32m[20230207 15:19:45 @agent_ppo2.py:192][0m |          -0.0002 |           0.7671 |           0.1347 |
[32m[20230207 15:19:45 @agent_ppo2.py:192][0m |          -0.0031 |           0.7531 |           0.1346 |
[32m[20230207 15:19:45 @agent_ppo2.py:192][0m |          -0.0105 |           0.7469 |           0.1346 |
[32m[20230207 15:19:45 @agent_ppo2.py:192][0m |           0.0037 |           0.7514 |           0.1346 |
[32m[20230207 15:19:45 @agent_ppo2.py:192][0m |           0.0002 |           0.7412 |           0.1344 |
[32m[20230207 15:19:45 @agent_ppo2.py:192][0m |          -0.0031 |           0.7352 |           0.1344 |
[32m[20230207 15:19:45 @agent_ppo2.py:192][0m |          -0.0064 |           0.7253 |           0.1344 |
[32m[20230207 15:19:45 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:19:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -16.41
[32m[20230207 15:19:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -7.74
[32m[20230207 15:19:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -13.53
[32m[20230207 15:19:46 @agent_ppo2.py:150][0m Total time:      19.08 min
[32m[20230207 15:19:46 @agent_ppo2.py:152][0m 991232 total steps have happened
[32m[20230207 15:19:46 @agent_ppo2.py:128][0m #------------------------ Iteration 484 --------------------------#
[32m[20230207 15:19:46 @agent_ppo2.py:134][0m Sampling time: 0.54 s by 1 slaves
[32m[20230207 15:19:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:46 @agent_ppo2.py:192][0m |          -0.0016 |           5.5409 |           0.1368 |
[32m[20230207 15:19:46 @agent_ppo2.py:192][0m |          -0.0011 |           2.1005 |           0.1364 |
[32m[20230207 15:19:47 @agent_ppo2.py:192][0m |          -0.0083 |           1.8066 |           0.1362 |
[32m[20230207 15:19:47 @agent_ppo2.py:192][0m |          -0.0103 |           1.6359 |           0.1361 |
[32m[20230207 15:19:47 @agent_ppo2.py:192][0m |          -0.0079 |           1.5189 |           0.1362 |
[32m[20230207 15:19:47 @agent_ppo2.py:192][0m |          -0.0142 |           1.4003 |           0.1361 |
[32m[20230207 15:19:47 @agent_ppo2.py:192][0m |          -0.0111 |           1.3702 |           0.1359 |
[32m[20230207 15:19:47 @agent_ppo2.py:192][0m |          -0.0118 |           1.3225 |           0.1359 |
[32m[20230207 15:19:47 @agent_ppo2.py:192][0m |          -0.0103 |           1.2675 |           0.1358 |
[32m[20230207 15:19:47 @agent_ppo2.py:192][0m |          -0.0147 |           1.2381 |           0.1359 |
[32m[20230207 15:19:47 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:19:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: -21.85
[32m[20230207 15:19:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 9.95
[32m[20230207 15:19:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -83.27
[32m[20230207 15:19:48 @agent_ppo2.py:150][0m Total time:      19.11 min
[32m[20230207 15:19:48 @agent_ppo2.py:152][0m 993280 total steps have happened
[32m[20230207 15:19:48 @agent_ppo2.py:128][0m #------------------------ Iteration 485 --------------------------#
[32m[20230207 15:19:48 @agent_ppo2.py:134][0m Sampling time: 0.54 s by 1 slaves
[32m[20230207 15:19:48 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:48 @agent_ppo2.py:192][0m |          -0.0047 |          15.5027 |           0.1312 |
[32m[20230207 15:19:48 @agent_ppo2.py:192][0m |          -0.0091 |           7.4494 |           0.1312 |
[32m[20230207 15:19:48 @agent_ppo2.py:192][0m |          -0.0035 |           6.1443 |           0.1312 |
[32m[20230207 15:19:48 @agent_ppo2.py:192][0m |          -0.0135 |           5.2231 |           0.1311 |
[32m[20230207 15:19:48 @agent_ppo2.py:192][0m |          -0.0125 |           4.8265 |           0.1310 |
[32m[20230207 15:19:49 @agent_ppo2.py:192][0m |           0.0119 |           5.2764 |           0.1310 |
[32m[20230207 15:19:49 @agent_ppo2.py:192][0m |          -0.0129 |           7.1142 |           0.1310 |
[32m[20230207 15:19:49 @agent_ppo2.py:192][0m |          -0.0107 |           4.4262 |           0.1309 |
[32m[20230207 15:19:49 @agent_ppo2.py:192][0m |          -0.0034 |           4.6337 |           0.1309 |
[32m[20230207 15:19:49 @agent_ppo2.py:192][0m |          -0.0162 |           4.1635 |           0.1309 |
[32m[20230207 15:19:49 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:19:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: 8.93
[32m[20230207 15:19:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 126.52
[32m[20230207 15:19:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 44.73
[32m[20230207 15:19:50 @agent_ppo2.py:150][0m Total time:      19.14 min
[32m[20230207 15:19:50 @agent_ppo2.py:152][0m 995328 total steps have happened
[32m[20230207 15:19:50 @agent_ppo2.py:128][0m #------------------------ Iteration 486 --------------------------#
[32m[20230207 15:19:50 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:19:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:50 @agent_ppo2.py:192][0m |           0.0168 |           1.6015 |           0.1332 |
[32m[20230207 15:19:51 @agent_ppo2.py:192][0m |          -0.0193 |           1.2081 |           0.1332 |
[32m[20230207 15:19:51 @agent_ppo2.py:192][0m |          -0.0006 |           1.1276 |           0.1333 |
[32m[20230207 15:19:51 @agent_ppo2.py:192][0m |          -0.0055 |           1.0802 |           0.1332 |
[32m[20230207 15:19:51 @agent_ppo2.py:192][0m |          -0.0251 |           1.0731 |           0.1333 |
[32m[20230207 15:19:51 @agent_ppo2.py:192][0m |          -0.0155 |           1.0378 |           0.1333 |
[32m[20230207 15:19:51 @agent_ppo2.py:192][0m |          -0.0164 |           1.0135 |           0.1333 |
[32m[20230207 15:19:51 @agent_ppo2.py:192][0m |          -0.0167 |           1.0258 |           0.1334 |
[32m[20230207 15:19:51 @agent_ppo2.py:192][0m |          -0.0131 |           1.0035 |           0.1334 |
[32m[20230207 15:19:51 @agent_ppo2.py:192][0m |          -0.0074 |           0.9844 |           0.1335 |
[32m[20230207 15:19:51 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:19:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: 45.78
[32m[20230207 15:19:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 87.14
[32m[20230207 15:19:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 76.28
[32m[20230207 15:19:52 @agent_ppo2.py:150][0m Total time:      19.18 min
[32m[20230207 15:19:52 @agent_ppo2.py:152][0m 997376 total steps have happened
[32m[20230207 15:19:52 @agent_ppo2.py:128][0m #------------------------ Iteration 487 --------------------------#
[32m[20230207 15:19:52 @agent_ppo2.py:134][0m Sampling time: 0.50 s by 1 slaves
[32m[20230207 15:19:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:53 @agent_ppo2.py:192][0m |          -0.0039 |           9.6911 |           0.1359 |
[32m[20230207 15:19:53 @agent_ppo2.py:192][0m |          -0.0136 |           4.4183 |           0.1359 |
[32m[20230207 15:19:53 @agent_ppo2.py:192][0m |          -0.0111 |           3.4400 |           0.1358 |
[32m[20230207 15:19:53 @agent_ppo2.py:192][0m |          -0.0073 |           3.1836 |           0.1358 |
[32m[20230207 15:19:53 @agent_ppo2.py:192][0m |          -0.0221 |           2.5943 |           0.1357 |
[32m[20230207 15:19:53 @agent_ppo2.py:192][0m |          -0.0111 |           2.4131 |           0.1357 |
[32m[20230207 15:19:53 @agent_ppo2.py:192][0m |          -0.0180 |           2.3544 |           0.1357 |
[32m[20230207 15:19:53 @agent_ppo2.py:192][0m |          -0.0242 |           2.0828 |           0.1357 |
[32m[20230207 15:19:53 @agent_ppo2.py:192][0m |          -0.0282 |           2.0653 |           0.1357 |
[32m[20230207 15:19:53 @agent_ppo2.py:192][0m |          -0.0201 |           1.7503 |           0.1357 |
[32m[20230207 15:19:53 @agent_ppo2.py:137][0m Policy update time: 0.53 s
[32m[20230207 15:19:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: -0.80
[32m[20230207 15:19:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 66.48
[32m[20230207 15:19:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -74.11
[32m[20230207 15:19:54 @agent_ppo2.py:150][0m Total time:      19.21 min
[32m[20230207 15:19:54 @agent_ppo2.py:152][0m 999424 total steps have happened
[32m[20230207 15:19:54 @agent_ppo2.py:128][0m #------------------------ Iteration 488 --------------------------#
[32m[20230207 15:19:54 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:19:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:54 @agent_ppo2.py:192][0m |           0.0029 |           7.9710 |           0.1349 |
[32m[20230207 15:19:54 @agent_ppo2.py:192][0m |          -0.0045 |           4.2652 |           0.1349 |
[32m[20230207 15:19:54 @agent_ppo2.py:192][0m |          -0.0033 |           3.3697 |           0.1349 |
[32m[20230207 15:19:54 @agent_ppo2.py:192][0m |          -0.0093 |           2.9512 |           0.1348 |
[32m[20230207 15:19:54 @agent_ppo2.py:192][0m |          -0.0117 |           2.6944 |           0.1348 |
[32m[20230207 15:19:54 @agent_ppo2.py:192][0m |          -0.0087 |           2.4402 |           0.1348 |
[32m[20230207 15:19:55 @agent_ppo2.py:192][0m |          -0.0124 |           2.2513 |           0.1347 |
[32m[20230207 15:19:55 @agent_ppo2.py:192][0m |          -0.0117 |           2.1078 |           0.1346 |
[32m[20230207 15:19:55 @agent_ppo2.py:192][0m |          -0.0168 |           2.0215 |           0.1346 |
[32m[20230207 15:19:55 @agent_ppo2.py:192][0m |          -0.0061 |           2.0290 |           0.1346 |
[32m[20230207 15:19:55 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:19:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: -3.57
[32m[20230207 15:19:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 49.06
[32m[20230207 15:19:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -20.77
[32m[20230207 15:19:56 @agent_ppo2.py:150][0m Total time:      19.24 min
[32m[20230207 15:19:56 @agent_ppo2.py:152][0m 1001472 total steps have happened
[32m[20230207 15:19:56 @agent_ppo2.py:128][0m #------------------------ Iteration 489 --------------------------#
[32m[20230207 15:19:56 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:19:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:56 @agent_ppo2.py:192][0m |           0.0007 |           5.3263 |           0.1352 |
[32m[20230207 15:19:57 @agent_ppo2.py:192][0m |          -0.0032 |           1.2009 |           0.1349 |
[32m[20230207 15:19:57 @agent_ppo2.py:192][0m |          -0.0056 |           0.9871 |           0.1350 |
[32m[20230207 15:19:57 @agent_ppo2.py:192][0m |          -0.0060 |           0.9265 |           0.1350 |
[32m[20230207 15:19:57 @agent_ppo2.py:192][0m |          -0.0083 |           0.8881 |           0.1349 |
[32m[20230207 15:19:57 @agent_ppo2.py:192][0m |          -0.0078 |           0.8620 |           0.1349 |
[32m[20230207 15:19:57 @agent_ppo2.py:192][0m |          -0.0090 |           0.8496 |           0.1349 |
[32m[20230207 15:19:57 @agent_ppo2.py:192][0m |          -0.0089 |           0.8325 |           0.1348 |
[32m[20230207 15:19:57 @agent_ppo2.py:192][0m |          -0.0080 |           0.8183 |           0.1347 |
[32m[20230207 15:19:57 @agent_ppo2.py:192][0m |          -0.0100 |           0.8060 |           0.1347 |
[32m[20230207 15:19:57 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:19:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: -45.20
[32m[20230207 15:19:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 1.43
[32m[20230207 15:19:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 67.32
[32m[20230207 15:19:58 @agent_ppo2.py:150][0m Total time:      19.28 min
[32m[20230207 15:19:58 @agent_ppo2.py:152][0m 1003520 total steps have happened
[32m[20230207 15:19:58 @agent_ppo2.py:128][0m #------------------------ Iteration 490 --------------------------#
[32m[20230207 15:19:59 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:19:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:19:59 @agent_ppo2.py:192][0m |           0.0072 |           4.4049 |           0.1390 |
[32m[20230207 15:19:59 @agent_ppo2.py:192][0m |          -0.0335 |           2.3879 |           0.1390 |
[32m[20230207 15:19:59 @agent_ppo2.py:192][0m |          -0.0073 |           1.9724 |           0.1390 |
[32m[20230207 15:19:59 @agent_ppo2.py:192][0m |           0.0053 |           1.9914 |           0.1390 |
[32m[20230207 15:19:59 @agent_ppo2.py:192][0m |           0.0012 |           1.7885 |           0.1389 |
[32m[20230207 15:19:59 @agent_ppo2.py:192][0m |          -0.0072 |           1.6773 |           0.1388 |
[32m[20230207 15:19:59 @agent_ppo2.py:192][0m |          -0.0043 |           1.5834 |           0.1389 |
[32m[20230207 15:19:59 @agent_ppo2.py:192][0m |           0.0252 |           1.9012 |           0.1389 |
[32m[20230207 15:19:59 @agent_ppo2.py:192][0m |           0.0018 |           1.9389 |           0.1388 |
[32m[20230207 15:20:00 @agent_ppo2.py:192][0m |          -0.0078 |           1.4846 |           0.1389 |
[32m[20230207 15:20:00 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:20:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: 51.83
[32m[20230207 15:20:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 62.45
[32m[20230207 15:20:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 15.47
[32m[20230207 15:20:00 @agent_ppo2.py:150][0m Total time:      19.32 min
[32m[20230207 15:20:00 @agent_ppo2.py:152][0m 1005568 total steps have happened
[32m[20230207 15:20:00 @agent_ppo2.py:128][0m #------------------------ Iteration 491 --------------------------#
[32m[20230207 15:20:01 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:20:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:01 @agent_ppo2.py:192][0m |           0.0019 |           3.6522 |           0.1352 |
[32m[20230207 15:20:01 @agent_ppo2.py:192][0m |          -0.0066 |           2.2340 |           0.1352 |
[32m[20230207 15:20:01 @agent_ppo2.py:192][0m |          -0.0231 |           2.0284 |           0.1349 |
[32m[20230207 15:20:02 @agent_ppo2.py:192][0m |          -0.0072 |           1.8832 |           0.1352 |
[32m[20230207 15:20:02 @agent_ppo2.py:192][0m |          -0.0055 |           1.8062 |           0.1351 |
[32m[20230207 15:20:02 @agent_ppo2.py:192][0m |          -0.0120 |           1.7331 |           0.1351 |
[32m[20230207 15:20:02 @agent_ppo2.py:192][0m |           0.0014 |           1.7125 |           0.1353 |
[32m[20230207 15:20:02 @agent_ppo2.py:192][0m |          -0.0125 |           1.6474 |           0.1353 |
[32m[20230207 15:20:02 @agent_ppo2.py:192][0m |          -0.0118 |           1.5977 |           0.1353 |
[32m[20230207 15:20:02 @agent_ppo2.py:192][0m |          -0.0281 |           1.5763 |           0.1356 |
[32m[20230207 15:20:02 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:20:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: 47.08
[32m[20230207 15:20:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 54.35
[32m[20230207 15:20:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 24.46
[32m[20230207 15:20:03 @agent_ppo2.py:150][0m Total time:      19.36 min
[32m[20230207 15:20:03 @agent_ppo2.py:152][0m 1007616 total steps have happened
[32m[20230207 15:20:03 @agent_ppo2.py:128][0m #------------------------ Iteration 492 --------------------------#
[32m[20230207 15:20:04 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:20:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:04 @agent_ppo2.py:192][0m |           0.0070 |           1.5961 |           0.1392 |
[32m[20230207 15:20:04 @agent_ppo2.py:192][0m |           0.0192 |           1.1960 |           0.1392 |
[32m[20230207 15:20:04 @agent_ppo2.py:192][0m |          -0.0079 |           1.0955 |           0.1391 |
[32m[20230207 15:20:04 @agent_ppo2.py:192][0m |          -0.0156 |           1.0275 |           0.1390 |
[32m[20230207 15:20:04 @agent_ppo2.py:192][0m |          -0.0198 |           0.9945 |           0.1389 |
[32m[20230207 15:20:04 @agent_ppo2.py:192][0m |          -0.0190 |           0.9808 |           0.1388 |
[32m[20230207 15:20:04 @agent_ppo2.py:192][0m |          -0.0038 |           0.9589 |           0.1389 |
[32m[20230207 15:20:04 @agent_ppo2.py:192][0m |          -0.0096 |           0.9402 |           0.1389 |
[32m[20230207 15:20:04 @agent_ppo2.py:192][0m |          -0.0132 |           0.9066 |           0.1388 |
[32m[20230207 15:20:04 @agent_ppo2.py:192][0m |          -0.0074 |           0.8874 |           0.1389 |
[32m[20230207 15:20:04 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:20:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: 82.41
[32m[20230207 15:20:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 126.35
[32m[20230207 15:20:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -12.93
[32m[20230207 15:20:05 @agent_ppo2.py:150][0m Total time:      19.40 min
[32m[20230207 15:20:05 @agent_ppo2.py:152][0m 1009664 total steps have happened
[32m[20230207 15:20:05 @agent_ppo2.py:128][0m #------------------------ Iteration 493 --------------------------#
[32m[20230207 15:20:06 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:20:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:06 @agent_ppo2.py:192][0m |           0.0003 |           9.6618 |           0.1407 |
[32m[20230207 15:20:06 @agent_ppo2.py:192][0m |          -0.0026 |           3.3976 |           0.1406 |
[32m[20230207 15:20:06 @agent_ppo2.py:192][0m |          -0.0045 |           3.1274 |           0.1407 |
[32m[20230207 15:20:06 @agent_ppo2.py:192][0m |          -0.0037 |           3.0469 |           0.1407 |
[32m[20230207 15:20:06 @agent_ppo2.py:192][0m |          -0.0058 |           2.8976 |           0.1406 |
[32m[20230207 15:20:06 @agent_ppo2.py:192][0m |          -0.0091 |           2.8469 |           0.1407 |
[32m[20230207 15:20:07 @agent_ppo2.py:192][0m |          -0.0087 |           2.8284 |           0.1406 |
[32m[20230207 15:20:07 @agent_ppo2.py:192][0m |          -0.0087 |           2.7640 |           0.1406 |
[32m[20230207 15:20:07 @agent_ppo2.py:192][0m |          -0.0111 |           2.6930 |           0.1407 |
[32m[20230207 15:20:07 @agent_ppo2.py:192][0m |          -0.0090 |           2.6625 |           0.1406 |
[32m[20230207 15:20:07 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:20:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: 9.59
[32m[20230207 15:20:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 108.22
[32m[20230207 15:20:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -30.02
[32m[20230207 15:20:08 @agent_ppo2.py:150][0m Total time:      19.44 min
[32m[20230207 15:20:08 @agent_ppo2.py:152][0m 1011712 total steps have happened
[32m[20230207 15:20:08 @agent_ppo2.py:128][0m #------------------------ Iteration 494 --------------------------#
[32m[20230207 15:20:08 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230207 15:20:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:08 @agent_ppo2.py:192][0m |          -0.0002 |          17.0254 |           0.1359 |
[32m[20230207 15:20:08 @agent_ppo2.py:192][0m |          -0.0032 |           5.2788 |           0.1356 |
[32m[20230207 15:20:08 @agent_ppo2.py:192][0m |          -0.0043 |           3.6133 |           0.1357 |
[32m[20230207 15:20:08 @agent_ppo2.py:192][0m |          -0.0042 |           3.1562 |           0.1357 |
[32m[20230207 15:20:08 @agent_ppo2.py:192][0m |          -0.0082 |           2.8344 |           0.1356 |
[32m[20230207 15:20:09 @agent_ppo2.py:192][0m |          -0.0090 |           2.6262 |           0.1355 |
[32m[20230207 15:20:09 @agent_ppo2.py:192][0m |          -0.0082 |           2.4628 |           0.1355 |
[32m[20230207 15:20:09 @agent_ppo2.py:192][0m |          -0.0101 |           2.3323 |           0.1355 |
[32m[20230207 15:20:09 @agent_ppo2.py:192][0m |          -0.0109 |           2.2179 |           0.1354 |
[32m[20230207 15:20:09 @agent_ppo2.py:192][0m |          -0.0118 |           2.1494 |           0.1354 |
[32m[20230207 15:20:09 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:20:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -26.72
[32m[20230207 15:20:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 55.31
[32m[20230207 15:20:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 33.03
[32m[20230207 15:20:10 @agent_ppo2.py:150][0m Total time:      19.47 min
[32m[20230207 15:20:10 @agent_ppo2.py:152][0m 1013760 total steps have happened
[32m[20230207 15:20:10 @agent_ppo2.py:128][0m #------------------------ Iteration 495 --------------------------#
[32m[20230207 15:20:10 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:20:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:11 @agent_ppo2.py:192][0m |          -0.0004 |           6.5649 |           0.1378 |
[32m[20230207 15:20:11 @agent_ppo2.py:192][0m |          -0.0030 |           2.7320 |           0.1378 |
[32m[20230207 15:20:11 @agent_ppo2.py:192][0m |          -0.0062 |           2.1790 |           0.1378 |
[32m[20230207 15:20:11 @agent_ppo2.py:192][0m |          -0.0068 |           1.9349 |           0.1377 |
[32m[20230207 15:20:11 @agent_ppo2.py:192][0m |          -0.0071 |           1.7442 |           0.1376 |
[32m[20230207 15:20:11 @agent_ppo2.py:192][0m |          -0.0071 |           1.6733 |           0.1375 |
[32m[20230207 15:20:11 @agent_ppo2.py:192][0m |          -0.0099 |           1.6403 |           0.1375 |
[32m[20230207 15:20:11 @agent_ppo2.py:192][0m |          -0.0099 |           1.5045 |           0.1375 |
[32m[20230207 15:20:11 @agent_ppo2.py:192][0m |          -0.0101 |           1.4960 |           0.1375 |
[32m[20230207 15:20:11 @agent_ppo2.py:192][0m |          -0.0114 |           1.4570 |           0.1375 |
[32m[20230207 15:20:11 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:20:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: -16.87
[32m[20230207 15:20:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 79.23
[32m[20230207 15:20:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -15.62
[32m[20230207 15:20:12 @agent_ppo2.py:150][0m Total time:      19.51 min
[32m[20230207 15:20:12 @agent_ppo2.py:152][0m 1015808 total steps have happened
[32m[20230207 15:20:12 @agent_ppo2.py:128][0m #------------------------ Iteration 496 --------------------------#
[32m[20230207 15:20:13 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:20:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:13 @agent_ppo2.py:192][0m |           0.0151 |           1.1647 |           0.1349 |
[32m[20230207 15:20:13 @agent_ppo2.py:192][0m |          -0.0067 |           0.9764 |           0.1345 |
[32m[20230207 15:20:13 @agent_ppo2.py:192][0m |          -0.0182 |           0.9308 |           0.1348 |
[32m[20230207 15:20:13 @agent_ppo2.py:192][0m |          -0.0172 |           0.9195 |           0.1348 |
[32m[20230207 15:20:13 @agent_ppo2.py:192][0m |          -0.0280 |           0.8964 |           0.1349 |
[32m[20230207 15:20:13 @agent_ppo2.py:192][0m |          -0.0120 |           0.8664 |           0.1348 |
[32m[20230207 15:20:13 @agent_ppo2.py:192][0m |           0.0154 |           0.8901 |           0.1349 |
[32m[20230207 15:20:13 @agent_ppo2.py:192][0m |          -0.0011 |           0.8577 |           0.1348 |
[32m[20230207 15:20:13 @agent_ppo2.py:192][0m |          -0.0244 |           0.8339 |           0.1347 |
[32m[20230207 15:20:13 @agent_ppo2.py:192][0m |          -0.0134 |           0.8213 |           0.1348 |
[32m[20230207 15:20:13 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:20:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: -8.26
[32m[20230207 15:20:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -4.04
[32m[20230207 15:20:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -19.85
[32m[20230207 15:20:14 @agent_ppo2.py:150][0m Total time:      19.55 min
[32m[20230207 15:20:14 @agent_ppo2.py:152][0m 1017856 total steps have happened
[32m[20230207 15:20:14 @agent_ppo2.py:128][0m #------------------------ Iteration 497 --------------------------#
[32m[20230207 15:20:15 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:20:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:15 @agent_ppo2.py:192][0m |          -0.0061 |           4.2000 |           0.1361 |
[32m[20230207 15:20:15 @agent_ppo2.py:192][0m |           0.0042 |           2.4055 |           0.1360 |
[32m[20230207 15:20:15 @agent_ppo2.py:192][0m |          -0.0011 |           2.0416 |           0.1361 |
[32m[20230207 15:20:15 @agent_ppo2.py:192][0m |          -0.0113 |           1.8283 |           0.1360 |
[32m[20230207 15:20:15 @agent_ppo2.py:192][0m |          -0.0094 |           1.7442 |           0.1360 |
[32m[20230207 15:20:15 @agent_ppo2.py:192][0m |          -0.0056 |           1.5239 |           0.1360 |
[32m[20230207 15:20:16 @agent_ppo2.py:192][0m |          -0.0025 |           1.3880 |           0.1361 |
[32m[20230207 15:20:16 @agent_ppo2.py:192][0m |          -0.0053 |           1.3191 |           0.1359 |
[32m[20230207 15:20:16 @agent_ppo2.py:192][0m |          -0.0094 |           1.2831 |           0.1361 |
[32m[20230207 15:20:16 @agent_ppo2.py:192][0m |          -0.0100 |           1.2425 |           0.1360 |
[32m[20230207 15:20:16 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:20:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: 26.05
[32m[20230207 15:20:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 32.96
[32m[20230207 15:20:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 14.10
[32m[20230207 15:20:17 @agent_ppo2.py:150][0m Total time:      19.59 min
[32m[20230207 15:20:17 @agent_ppo2.py:152][0m 1019904 total steps have happened
[32m[20230207 15:20:17 @agent_ppo2.py:128][0m #------------------------ Iteration 498 --------------------------#
[32m[20230207 15:20:17 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:20:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:17 @agent_ppo2.py:192][0m |          -0.0099 |           1.7544 |           0.1372 |
[32m[20230207 15:20:18 @agent_ppo2.py:192][0m |          -0.0057 |           1.3714 |           0.1372 |
[32m[20230207 15:20:18 @agent_ppo2.py:192][0m |          -0.0099 |           1.2876 |           0.1372 |
[32m[20230207 15:20:18 @agent_ppo2.py:192][0m |          -0.0001 |           1.2380 |           0.1372 |
[32m[20230207 15:20:18 @agent_ppo2.py:192][0m |          -0.0176 |           1.1894 |           0.1372 |
[32m[20230207 15:20:18 @agent_ppo2.py:192][0m |           0.0013 |           1.1571 |           0.1369 |
[32m[20230207 15:20:18 @agent_ppo2.py:192][0m |          -0.0157 |           1.1357 |           0.1371 |
[32m[20230207 15:20:18 @agent_ppo2.py:192][0m |          -0.0167 |           1.1081 |           0.1372 |
[32m[20230207 15:20:18 @agent_ppo2.py:192][0m |          -0.0152 |           1.0872 |           0.1373 |
[32m[20230207 15:20:18 @agent_ppo2.py:192][0m |          -0.0085 |           1.0763 |           0.1373 |
[32m[20230207 15:20:18 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:20:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: 38.50
[32m[20230207 15:20:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 53.30
[32m[20230207 15:20:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 6.55
[32m[20230207 15:20:19 @agent_ppo2.py:150][0m Total time:      19.63 min
[32m[20230207 15:20:19 @agent_ppo2.py:152][0m 1021952 total steps have happened
[32m[20230207 15:20:19 @agent_ppo2.py:128][0m #------------------------ Iteration 499 --------------------------#
[32m[20230207 15:20:20 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230207 15:20:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:20 @agent_ppo2.py:192][0m |          -0.0002 |          11.1090 |           0.1390 |
[32m[20230207 15:20:20 @agent_ppo2.py:192][0m |          -0.0091 |           4.9098 |           0.1389 |
[32m[20230207 15:20:20 @agent_ppo2.py:192][0m |          -0.0013 |           2.8197 |           0.1387 |
[32m[20230207 15:20:20 @agent_ppo2.py:192][0m |          -0.0077 |           2.6584 |           0.1383 |
[32m[20230207 15:20:20 @agent_ppo2.py:192][0m |          -0.0083 |           2.1872 |           0.1383 |
[32m[20230207 15:20:20 @agent_ppo2.py:192][0m |          -0.0148 |           2.0083 |           0.1384 |
[32m[20230207 15:20:20 @agent_ppo2.py:192][0m |          -0.0115 |           1.9466 |           0.1384 |
[32m[20230207 15:20:20 @agent_ppo2.py:192][0m |          -0.0104 |           1.8722 |           0.1385 |
[32m[20230207 15:20:20 @agent_ppo2.py:192][0m |           0.0013 |           1.7748 |           0.1386 |
[32m[20230207 15:20:20 @agent_ppo2.py:192][0m |           0.0080 |           1.7311 |           0.1384 |
[32m[20230207 15:20:20 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230207 15:20:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -32.38
[32m[20230207 15:20:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 39.15
[32m[20230207 15:20:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 44.49
[32m[20230207 15:20:21 @evo_bipedalwalker_agent.py:104][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 116.82
[32m[20230207 15:20:21 @agent_ppo2.py:150][0m Total time:      19.66 min
[32m[20230207 15:20:21 @agent_ppo2.py:152][0m 1024000 total steps have happened
[32m[20230207 15:20:21 @agent_ppo2.py:128][0m #------------------------ Iteration 500 --------------------------#
[32m[20230207 15:20:22 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:20:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:22 @agent_ppo2.py:192][0m |           0.0133 |           1.2584 |           0.1392 |
[32m[20230207 15:20:22 @agent_ppo2.py:192][0m |          -0.0100 |           0.9353 |           0.1389 |
[32m[20230207 15:20:22 @agent_ppo2.py:192][0m |          -0.0081 |           0.8421 |           0.1389 |
[32m[20230207 15:20:22 @agent_ppo2.py:192][0m |          -0.0213 |           0.8014 |           0.1390 |
[32m[20230207 15:20:22 @agent_ppo2.py:192][0m |           0.0013 |           0.7693 |           0.1388 |
[32m[20230207 15:20:22 @agent_ppo2.py:192][0m |          -0.0138 |           0.7471 |           0.1391 |
[32m[20230207 15:20:22 @agent_ppo2.py:192][0m |           0.0079 |           0.7326 |           0.1391 |
[32m[20230207 15:20:22 @agent_ppo2.py:192][0m |          -0.0120 |           0.7206 |           0.1389 |
[32m[20230207 15:20:23 @agent_ppo2.py:192][0m |           0.0012 |           0.7160 |           0.1391 |
[32m[20230207 15:20:23 @agent_ppo2.py:192][0m |           0.0028 |           0.7135 |           0.1389 |
[32m[20230207 15:20:23 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:20:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -13.48
[32m[20230207 15:20:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 56.71
[32m[20230207 15:20:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 57.34
[32m[20230207 15:20:23 @agent_ppo2.py:150][0m Total time:      19.70 min
[32m[20230207 15:20:23 @agent_ppo2.py:152][0m 1026048 total steps have happened
[32m[20230207 15:20:23 @agent_ppo2.py:128][0m #------------------------ Iteration 501 --------------------------#
[32m[20230207 15:20:24 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:20:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:24 @agent_ppo2.py:192][0m |           0.0000 |          13.6664 |           0.1394 |
[32m[20230207 15:20:24 @agent_ppo2.py:192][0m |          -0.0028 |           6.5963 |           0.1393 |
[32m[20230207 15:20:24 @agent_ppo2.py:192][0m |          -0.0054 |           4.5742 |           0.1392 |
[32m[20230207 15:20:24 @agent_ppo2.py:192][0m |          -0.0078 |           3.4446 |           0.1392 |
[32m[20230207 15:20:24 @agent_ppo2.py:192][0m |          -0.0093 |           2.9663 |           0.1391 |
[32m[20230207 15:20:24 @agent_ppo2.py:192][0m |          -0.0113 |           2.7195 |           0.1390 |
[32m[20230207 15:20:24 @agent_ppo2.py:192][0m |          -0.0120 |           2.6514 |           0.1390 |
[32m[20230207 15:20:24 @agent_ppo2.py:192][0m |          -0.0131 |           2.4427 |           0.1388 |
[32m[20230207 15:20:24 @agent_ppo2.py:192][0m |          -0.0145 |           2.3610 |           0.1387 |
[32m[20230207 15:20:24 @agent_ppo2.py:192][0m |          -0.0153 |           2.2138 |           0.1386 |
[32m[20230207 15:20:24 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:20:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -27.30
[32m[20230207 15:20:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 27.18
[32m[20230207 15:20:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 13.94
[32m[20230207 15:20:25 @agent_ppo2.py:150][0m Total time:      19.73 min
[32m[20230207 15:20:25 @agent_ppo2.py:152][0m 1028096 total steps have happened
[32m[20230207 15:20:25 @agent_ppo2.py:128][0m #------------------------ Iteration 502 --------------------------#
[32m[20230207 15:20:26 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:20:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:26 @agent_ppo2.py:192][0m |           0.0030 |          22.3269 |           0.1426 |
[32m[20230207 15:20:26 @agent_ppo2.py:192][0m |          -0.0015 |           8.5728 |           0.1425 |
[32m[20230207 15:20:26 @agent_ppo2.py:192][0m |          -0.0023 |           2.7390 |           0.1425 |
[32m[20230207 15:20:26 @agent_ppo2.py:192][0m |          -0.0021 |           1.7620 |           0.1426 |
[32m[20230207 15:20:26 @agent_ppo2.py:192][0m |          -0.0040 |           1.5471 |           0.1426 |
[32m[20230207 15:20:27 @agent_ppo2.py:192][0m |          -0.0048 |           1.4378 |           0.1426 |
[32m[20230207 15:20:27 @agent_ppo2.py:192][0m |          -0.0041 |           1.3679 |           0.1427 |
[32m[20230207 15:20:27 @agent_ppo2.py:192][0m |          -0.0052 |           1.3186 |           0.1425 |
[32m[20230207 15:20:27 @agent_ppo2.py:192][0m |          -0.0061 |           1.2760 |           0.1425 |
[32m[20230207 15:20:27 @agent_ppo2.py:192][0m |          -0.0067 |           1.2449 |           0.1426 |
[32m[20230207 15:20:27 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:20:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: -7.62
[32m[20230207 15:20:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 82.51
[32m[20230207 15:20:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -28.53
[32m[20230207 15:20:28 @agent_ppo2.py:150][0m Total time:      19.77 min
[32m[20230207 15:20:28 @agent_ppo2.py:152][0m 1030144 total steps have happened
[32m[20230207 15:20:28 @agent_ppo2.py:128][0m #------------------------ Iteration 503 --------------------------#
[32m[20230207 15:20:29 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:20:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:29 @agent_ppo2.py:192][0m |           0.0011 |           9.8360 |           0.1441 |
[32m[20230207 15:20:29 @agent_ppo2.py:192][0m |          -0.0021 |           4.6439 |           0.1441 |
[32m[20230207 15:20:29 @agent_ppo2.py:192][0m |          -0.0070 |           3.8976 |           0.1441 |
[32m[20230207 15:20:29 @agent_ppo2.py:192][0m |          -0.0073 |           3.6073 |           0.1441 |
[32m[20230207 15:20:29 @agent_ppo2.py:192][0m |          -0.0056 |           3.4886 |           0.1440 |
[32m[20230207 15:20:29 @agent_ppo2.py:192][0m |          -0.0093 |           3.1495 |           0.1440 |
[32m[20230207 15:20:29 @agent_ppo2.py:192][0m |          -0.0086 |           3.0925 |           0.1440 |
[32m[20230207 15:20:29 @agent_ppo2.py:192][0m |          -0.0080 |           2.9148 |           0.1439 |
[32m[20230207 15:20:29 @agent_ppo2.py:192][0m |          -0.0091 |           2.7424 |           0.1438 |
[32m[20230207 15:20:30 @agent_ppo2.py:192][0m |          -0.0061 |           2.7044 |           0.1438 |
[32m[20230207 15:20:30 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:20:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -43.06
[32m[20230207 15:20:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 31.10
[32m[20230207 15:20:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 22.76
[32m[20230207 15:20:30 @agent_ppo2.py:150][0m Total time:      19.82 min
[32m[20230207 15:20:30 @agent_ppo2.py:152][0m 1032192 total steps have happened
[32m[20230207 15:20:30 @agent_ppo2.py:128][0m #------------------------ Iteration 504 --------------------------#
[32m[20230207 15:20:31 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:20:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:31 @agent_ppo2.py:192][0m |           0.0000 |          17.7930 |           0.1412 |
[32m[20230207 15:20:31 @agent_ppo2.py:192][0m |          -0.0056 |           5.6069 |           0.1410 |
[32m[20230207 15:20:31 @agent_ppo2.py:192][0m |          -0.0091 |           4.2656 |           0.1410 |
[32m[20230207 15:20:31 @agent_ppo2.py:192][0m |          -0.0071 |           3.7009 |           0.1409 |
[32m[20230207 15:20:31 @agent_ppo2.py:192][0m |          -0.0126 |           3.2774 |           0.1408 |
[32m[20230207 15:20:31 @agent_ppo2.py:192][0m |          -0.0132 |           3.0561 |           0.1408 |
[32m[20230207 15:20:31 @agent_ppo2.py:192][0m |          -0.0146 |           2.8345 |           0.1408 |
[32m[20230207 15:20:31 @agent_ppo2.py:192][0m |          -0.0140 |           2.6562 |           0.1407 |
[32m[20230207 15:20:31 @agent_ppo2.py:192][0m |          -0.0181 |           2.4981 |           0.1407 |
[32m[20230207 15:20:31 @agent_ppo2.py:192][0m |          -0.0176 |           2.4577 |           0.1406 |
[32m[20230207 15:20:31 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:20:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -43.99
[32m[20230207 15:20:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 107.29
[32m[20230207 15:20:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 100.88
[32m[20230207 15:20:32 @agent_ppo2.py:150][0m Total time:      19.85 min
[32m[20230207 15:20:32 @agent_ppo2.py:152][0m 1034240 total steps have happened
[32m[20230207 15:20:32 @agent_ppo2.py:128][0m #------------------------ Iteration 505 --------------------------#
[32m[20230207 15:20:33 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:20:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:33 @agent_ppo2.py:192][0m |           0.0022 |          10.7846 |           0.1368 |
[32m[20230207 15:20:33 @agent_ppo2.py:192][0m |          -0.0071 |           6.6170 |           0.1367 |
[32m[20230207 15:20:33 @agent_ppo2.py:192][0m |           0.0015 |           5.9876 |           0.1366 |
[32m[20230207 15:20:33 @agent_ppo2.py:192][0m |          -0.0082 |           5.4775 |           0.1365 |
[32m[20230207 15:20:33 @agent_ppo2.py:192][0m |          -0.0080 |           5.3095 |           0.1366 |
[32m[20230207 15:20:34 @agent_ppo2.py:192][0m |          -0.0293 |           4.7313 |           0.1365 |
[32m[20230207 15:20:34 @agent_ppo2.py:192][0m |          -0.0004 |           4.4740 |           0.1365 |
[32m[20230207 15:20:34 @agent_ppo2.py:192][0m |          -0.0049 |           4.2613 |           0.1365 |
[32m[20230207 15:20:34 @agent_ppo2.py:192][0m |           0.0015 |           4.0807 |           0.1366 |
[32m[20230207 15:20:34 @agent_ppo2.py:192][0m |          -0.0048 |           3.8757 |           0.1366 |
[32m[20230207 15:20:34 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:20:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: 7.36
[32m[20230207 15:20:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 43.36
[32m[20230207 15:20:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 16.65
[32m[20230207 15:20:35 @agent_ppo2.py:150][0m Total time:      19.89 min
[32m[20230207 15:20:35 @agent_ppo2.py:152][0m 1036288 total steps have happened
[32m[20230207 15:20:35 @agent_ppo2.py:128][0m #------------------------ Iteration 506 --------------------------#
[32m[20230207 15:20:35 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:20:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:36 @agent_ppo2.py:192][0m |          -0.0032 |           6.8716 |           0.1380 |
[32m[20230207 15:20:36 @agent_ppo2.py:192][0m |           0.0047 |           4.3986 |           0.1379 |
[32m[20230207 15:20:36 @agent_ppo2.py:192][0m |          -0.0032 |           3.8287 |           0.1378 |
[32m[20230207 15:20:36 @agent_ppo2.py:192][0m |          -0.0011 |           3.5339 |           0.1379 |
[32m[20230207 15:20:36 @agent_ppo2.py:192][0m |          -0.0101 |           3.1574 |           0.1377 |
[32m[20230207 15:20:36 @agent_ppo2.py:192][0m |          -0.0104 |           2.9497 |           0.1378 |
[32m[20230207 15:20:36 @agent_ppo2.py:192][0m |          -0.0070 |           2.7221 |           0.1377 |
[32m[20230207 15:20:36 @agent_ppo2.py:192][0m |          -0.0453 |           2.7286 |           0.1377 |
[32m[20230207 15:20:36 @agent_ppo2.py:192][0m |          -0.0093 |           2.9079 |           0.1377 |
[32m[20230207 15:20:36 @agent_ppo2.py:192][0m |          -0.0343 |           3.4167 |           0.1377 |
[32m[20230207 15:20:36 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:20:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: 36.03
[32m[20230207 15:20:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 66.12
[32m[20230207 15:20:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -32.38
[32m[20230207 15:20:37 @agent_ppo2.py:150][0m Total time:      19.93 min
[32m[20230207 15:20:37 @agent_ppo2.py:152][0m 1038336 total steps have happened
[32m[20230207 15:20:37 @agent_ppo2.py:128][0m #------------------------ Iteration 507 --------------------------#
[32m[20230207 15:20:38 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:20:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:38 @agent_ppo2.py:192][0m |           0.0006 |          29.5706 |           0.1393 |
[32m[20230207 15:20:38 @agent_ppo2.py:192][0m |          -0.0026 |          11.3470 |           0.1392 |
[32m[20230207 15:20:38 @agent_ppo2.py:192][0m |          -0.0046 |           3.6296 |           0.1391 |
[32m[20230207 15:20:38 @agent_ppo2.py:192][0m |          -0.0067 |           2.1358 |           0.1392 |
[32m[20230207 15:20:38 @agent_ppo2.py:192][0m |          -0.0079 |           1.6933 |           0.1392 |
[32m[20230207 15:20:38 @agent_ppo2.py:192][0m |          -0.0086 |           1.5131 |           0.1393 |
[32m[20230207 15:20:38 @agent_ppo2.py:192][0m |          -0.0091 |           1.3974 |           0.1392 |
[32m[20230207 15:20:38 @agent_ppo2.py:192][0m |          -0.0100 |           1.3207 |           0.1392 |
[32m[20230207 15:20:38 @agent_ppo2.py:192][0m |          -0.0098 |           1.2488 |           0.1392 |
[32m[20230207 15:20:38 @agent_ppo2.py:192][0m |          -0.0108 |           1.2015 |           0.1391 |
[32m[20230207 15:20:38 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:20:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -40.40
[32m[20230207 15:20:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 71.34
[32m[20230207 15:20:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -28.90
[32m[20230207 15:20:39 @agent_ppo2.py:150][0m Total time:      19.97 min
[32m[20230207 15:20:39 @agent_ppo2.py:152][0m 1040384 total steps have happened
[32m[20230207 15:20:39 @agent_ppo2.py:128][0m #------------------------ Iteration 508 --------------------------#
[32m[20230207 15:20:40 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:20:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:40 @agent_ppo2.py:192][0m |           0.0094 |          10.5409 |           0.1374 |
[32m[20230207 15:20:40 @agent_ppo2.py:192][0m |          -0.0128 |           6.6285 |           0.1374 |
[32m[20230207 15:20:40 @agent_ppo2.py:192][0m |          -0.0016 |           6.0549 |           0.1373 |
[32m[20230207 15:20:40 @agent_ppo2.py:192][0m |          -0.0139 |           4.8296 |           0.1373 |
[32m[20230207 15:20:40 @agent_ppo2.py:192][0m |          -0.0071 |           4.4699 |           0.1371 |
[32m[20230207 15:20:41 @agent_ppo2.py:192][0m |           0.0003 |           4.1112 |           0.1371 |
[32m[20230207 15:20:41 @agent_ppo2.py:192][0m |          -0.0027 |           3.9128 |           0.1371 |
[32m[20230207 15:20:41 @agent_ppo2.py:192][0m |           0.0354 |           3.7829 |           0.1371 |
[32m[20230207 15:20:41 @agent_ppo2.py:192][0m |          -0.0313 |           4.0789 |           0.1367 |
[32m[20230207 15:20:41 @agent_ppo2.py:192][0m |          -0.0049 |           3.3289 |           0.1368 |
[32m[20230207 15:20:41 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:20:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: 15.49
[32m[20230207 15:20:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 36.49
[32m[20230207 15:20:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 66.87
[32m[20230207 15:20:42 @agent_ppo2.py:150][0m Total time:      20.01 min
[32m[20230207 15:20:42 @agent_ppo2.py:152][0m 1042432 total steps have happened
[32m[20230207 15:20:42 @agent_ppo2.py:128][0m #------------------------ Iteration 509 --------------------------#
[32m[20230207 15:20:42 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:20:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:43 @agent_ppo2.py:192][0m |          -0.0074 |           3.8945 |           0.1409 |
[32m[20230207 15:20:43 @agent_ppo2.py:192][0m |           0.0264 |           3.9881 |           0.1409 |
[32m[20230207 15:20:43 @agent_ppo2.py:192][0m |          -0.0028 |           3.3648 |           0.1399 |
[32m[20230207 15:20:43 @agent_ppo2.py:192][0m |          -0.0109 |           3.2452 |           0.1403 |
[32m[20230207 15:20:43 @agent_ppo2.py:192][0m |          -0.0141 |           3.1259 |           0.1404 |
[32m[20230207 15:20:43 @agent_ppo2.py:192][0m |          -0.0073 |           3.0580 |           0.1405 |
[32m[20230207 15:20:43 @agent_ppo2.py:192][0m |           0.0845 |           6.5430 |           0.1403 |
[32m[20230207 15:20:43 @agent_ppo2.py:192][0m |          -0.0090 |           3.2925 |           0.1398 |
[32m[20230207 15:20:43 @agent_ppo2.py:192][0m |          -0.0000 |           2.8470 |           0.1402 |
[32m[20230207 15:20:43 @agent_ppo2.py:192][0m |          -0.0070 |           2.7471 |           0.1402 |
[32m[20230207 15:20:43 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:20:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: 5.97
[32m[20230207 15:20:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 9.96
[32m[20230207 15:20:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 31.36
[32m[20230207 15:20:44 @agent_ppo2.py:150][0m Total time:      20.05 min
[32m[20230207 15:20:44 @agent_ppo2.py:152][0m 1044480 total steps have happened
[32m[20230207 15:20:44 @agent_ppo2.py:128][0m #------------------------ Iteration 510 --------------------------#
[32m[20230207 15:20:45 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:20:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:45 @agent_ppo2.py:192][0m |          -0.0014 |           9.1017 |           0.1411 |
[32m[20230207 15:20:45 @agent_ppo2.py:192][0m |          -0.0040 |           5.3490 |           0.1409 |
[32m[20230207 15:20:45 @agent_ppo2.py:192][0m |          -0.0043 |           4.5599 |           0.1409 |
[32m[20230207 15:20:45 @agent_ppo2.py:192][0m |          -0.0070 |           3.9544 |           0.1409 |
[32m[20230207 15:20:45 @agent_ppo2.py:192][0m |          -0.0065 |           3.5315 |           0.1408 |
[32m[20230207 15:20:45 @agent_ppo2.py:192][0m |          -0.0101 |           3.2680 |           0.1408 |
[32m[20230207 15:20:46 @agent_ppo2.py:192][0m |          -0.0086 |           3.0446 |           0.1409 |
[32m[20230207 15:20:46 @agent_ppo2.py:192][0m |          -0.0095 |           2.8618 |           0.1408 |
[32m[20230207 15:20:46 @agent_ppo2.py:192][0m |          -0.0113 |           2.7422 |           0.1407 |
[32m[20230207 15:20:46 @agent_ppo2.py:192][0m |          -0.0104 |           2.6216 |           0.1407 |
[32m[20230207 15:20:46 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:20:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -16.12
[32m[20230207 15:20:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 36.65
[32m[20230207 15:20:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -24.01
[32m[20230207 15:20:46 @agent_ppo2.py:150][0m Total time:      20.09 min
[32m[20230207 15:20:46 @agent_ppo2.py:152][0m 1046528 total steps have happened
[32m[20230207 15:20:46 @agent_ppo2.py:128][0m #------------------------ Iteration 511 --------------------------#
[32m[20230207 15:20:47 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:20:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:47 @agent_ppo2.py:192][0m |          -0.0023 |           1.7804 |           0.1385 |
[32m[20230207 15:20:47 @agent_ppo2.py:192][0m |           0.0103 |           1.4696 |           0.1383 |
[32m[20230207 15:20:47 @agent_ppo2.py:192][0m |          -0.0056 |           1.3404 |           0.1383 |
[32m[20230207 15:20:48 @agent_ppo2.py:192][0m |          -0.0157 |           1.2971 |           0.1381 |
[32m[20230207 15:20:48 @agent_ppo2.py:192][0m |          -0.0164 |           1.2369 |           0.1381 |
[32m[20230207 15:20:48 @agent_ppo2.py:192][0m |          -0.0102 |           1.1982 |           0.1380 |
[32m[20230207 15:20:48 @agent_ppo2.py:192][0m |          -0.0023 |           1.1725 |           0.1381 |
[32m[20230207 15:20:48 @agent_ppo2.py:192][0m |          -0.0156 |           1.1525 |           0.1379 |
[32m[20230207 15:20:48 @agent_ppo2.py:192][0m |          -0.0061 |           1.1342 |           0.1380 |
[32m[20230207 15:20:48 @agent_ppo2.py:192][0m |          -0.0226 |           1.1190 |           0.1379 |
[32m[20230207 15:20:48 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:20:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: -31.39
[32m[20230207 15:20:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -19.51
[32m[20230207 15:20:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 89.41
[32m[20230207 15:20:49 @agent_ppo2.py:150][0m Total time:      20.13 min
[32m[20230207 15:20:49 @agent_ppo2.py:152][0m 1048576 total steps have happened
[32m[20230207 15:20:49 @agent_ppo2.py:128][0m #------------------------ Iteration 512 --------------------------#
[32m[20230207 15:20:50 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:20:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:50 @agent_ppo2.py:192][0m |          -0.0526 |           6.4707 |           0.1367 |
[32m[20230207 15:20:50 @agent_ppo2.py:192][0m |          -0.0059 |           4.5206 |           0.1367 |
[32m[20230207 15:20:50 @agent_ppo2.py:192][0m |           0.0031 |           4.1225 |           0.1368 |
[32m[20230207 15:20:50 @agent_ppo2.py:192][0m |           0.0005 |           3.6921 |           0.1368 |
[32m[20230207 15:20:50 @agent_ppo2.py:192][0m |          -0.0051 |           3.6951 |           0.1367 |
[32m[20230207 15:20:50 @agent_ppo2.py:192][0m |           0.0035 |           3.4299 |           0.1367 |
[32m[20230207 15:20:50 @agent_ppo2.py:192][0m |          -0.0013 |           3.2602 |           0.1367 |
[32m[20230207 15:20:50 @agent_ppo2.py:192][0m |          -0.0031 |           3.1699 |           0.1367 |
[32m[20230207 15:20:50 @agent_ppo2.py:192][0m |          -0.0105 |           3.1616 |           0.1368 |
[32m[20230207 15:20:50 @agent_ppo2.py:192][0m |          -0.0068 |           3.1313 |           0.1368 |
[32m[20230207 15:20:50 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:20:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: 0.63
[32m[20230207 15:20:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 16.86
[32m[20230207 15:20:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 6.72
[32m[20230207 15:20:51 @agent_ppo2.py:150][0m Total time:      20.16 min
[32m[20230207 15:20:51 @agent_ppo2.py:152][0m 1050624 total steps have happened
[32m[20230207 15:20:51 @agent_ppo2.py:128][0m #------------------------ Iteration 513 --------------------------#
[32m[20230207 15:20:52 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:20:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:52 @agent_ppo2.py:192][0m |          -0.0021 |           9.2507 |           0.1451 |
[32m[20230207 15:20:52 @agent_ppo2.py:192][0m |          -0.0046 |           3.7802 |           0.1450 |
[32m[20230207 15:20:52 @agent_ppo2.py:192][0m |          -0.0056 |           2.9100 |           0.1450 |
[32m[20230207 15:20:52 @agent_ppo2.py:192][0m |          -0.0069 |           2.6680 |           0.1450 |
[32m[20230207 15:20:52 @agent_ppo2.py:192][0m |          -0.0074 |           2.5148 |           0.1452 |
[32m[20230207 15:20:52 @agent_ppo2.py:192][0m |          -0.0085 |           2.3792 |           0.1451 |
[32m[20230207 15:20:52 @agent_ppo2.py:192][0m |          -0.0092 |           2.2571 |           0.1453 |
[32m[20230207 15:20:53 @agent_ppo2.py:192][0m |          -0.0099 |           2.1605 |           0.1452 |
[32m[20230207 15:20:53 @agent_ppo2.py:192][0m |          -0.0111 |           2.0727 |           0.1453 |
[32m[20230207 15:20:53 @agent_ppo2.py:192][0m |          -0.0114 |           2.0414 |           0.1452 |
[32m[20230207 15:20:53 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:20:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -1.42
[32m[20230207 15:20:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 104.73
[32m[20230207 15:20:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 73.30
[32m[20230207 15:20:53 @agent_ppo2.py:150][0m Total time:      20.20 min
[32m[20230207 15:20:53 @agent_ppo2.py:152][0m 1052672 total steps have happened
[32m[20230207 15:20:53 @agent_ppo2.py:128][0m #------------------------ Iteration 514 --------------------------#
[32m[20230207 15:20:54 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:20:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:54 @agent_ppo2.py:192][0m |          -0.0033 |           2.8644 |           0.1401 |
[32m[20230207 15:20:54 @agent_ppo2.py:192][0m |           0.0173 |           1.6976 |           0.1400 |
[32m[20230207 15:20:55 @agent_ppo2.py:192][0m |           0.0131 |           1.5020 |           0.1398 |
[32m[20230207 15:20:55 @agent_ppo2.py:192][0m |          -0.0048 |           1.3763 |           0.1397 |
[32m[20230207 15:20:55 @agent_ppo2.py:192][0m |          -0.0171 |           1.3520 |           0.1396 |
[32m[20230207 15:20:55 @agent_ppo2.py:192][0m |          -0.0135 |           1.3006 |           0.1393 |
[32m[20230207 15:20:55 @agent_ppo2.py:192][0m |          -0.0083 |           1.2044 |           0.1393 |
[32m[20230207 15:20:55 @agent_ppo2.py:192][0m |           0.0018 |           1.2569 |           0.1392 |
[32m[20230207 15:20:55 @agent_ppo2.py:192][0m |          -0.0119 |           1.2012 |           0.1391 |
[32m[20230207 15:20:55 @agent_ppo2.py:192][0m |          -0.0351 |           1.1223 |           0.1391 |
[32m[20230207 15:20:55 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:20:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: 46.86
[32m[20230207 15:20:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 90.24
[32m[20230207 15:20:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -24.18
[32m[20230207 15:20:56 @agent_ppo2.py:150][0m Total time:      20.24 min
[32m[20230207 15:20:56 @agent_ppo2.py:152][0m 1054720 total steps have happened
[32m[20230207 15:20:56 @agent_ppo2.py:128][0m #------------------------ Iteration 515 --------------------------#
[32m[20230207 15:20:57 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:20:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:57 @agent_ppo2.py:192][0m |           0.0087 |           1.0295 |           0.1402 |
[32m[20230207 15:20:57 @agent_ppo2.py:192][0m |          -0.0111 |           0.9017 |           0.1401 |
[32m[20230207 15:20:57 @agent_ppo2.py:192][0m |          -0.0047 |           0.8655 |           0.1403 |
[32m[20230207 15:20:57 @agent_ppo2.py:192][0m |           0.0017 |           0.8380 |           0.1404 |
[32m[20230207 15:20:57 @agent_ppo2.py:192][0m |          -0.0146 |           0.8220 |           0.1404 |
[32m[20230207 15:20:57 @agent_ppo2.py:192][0m |          -0.0159 |           0.8146 |           0.1406 |
[32m[20230207 15:20:57 @agent_ppo2.py:192][0m |          -0.0071 |           0.8100 |           0.1405 |
[32m[20230207 15:20:57 @agent_ppo2.py:192][0m |          -0.0145 |           0.7872 |           0.1406 |
[32m[20230207 15:20:57 @agent_ppo2.py:192][0m |          -0.0092 |           0.7799 |           0.1406 |
[32m[20230207 15:20:57 @agent_ppo2.py:192][0m |          -0.0139 |           0.7806 |           0.1407 |
[32m[20230207 15:20:57 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:20:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: 19.55
[32m[20230207 15:20:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 33.63
[32m[20230207 15:20:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 21.98
[32m[20230207 15:20:58 @agent_ppo2.py:150][0m Total time:      20.28 min
[32m[20230207 15:20:58 @agent_ppo2.py:152][0m 1056768 total steps have happened
[32m[20230207 15:20:58 @agent_ppo2.py:128][0m #------------------------ Iteration 516 --------------------------#
[32m[20230207 15:20:59 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:20:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:20:59 @agent_ppo2.py:192][0m |           0.0016 |           6.4738 |           0.1380 |
[32m[20230207 15:20:59 @agent_ppo2.py:192][0m |          -0.0014 |           3.4669 |           0.1380 |
[32m[20230207 15:20:59 @agent_ppo2.py:192][0m |          -0.0043 |           3.1049 |           0.1380 |
[32m[20230207 15:20:59 @agent_ppo2.py:192][0m |          -0.0067 |           2.8582 |           0.1380 |
[32m[20230207 15:21:00 @agent_ppo2.py:192][0m |          -0.0052 |           2.6178 |           0.1379 |
[32m[20230207 15:21:00 @agent_ppo2.py:192][0m |          -0.0084 |           2.5477 |           0.1379 |
[32m[20230207 15:21:00 @agent_ppo2.py:192][0m |          -0.0112 |           2.4385 |           0.1379 |
[32m[20230207 15:21:00 @agent_ppo2.py:192][0m |          -0.0123 |           2.3512 |           0.1377 |
[32m[20230207 15:21:00 @agent_ppo2.py:192][0m |          -0.0099 |           2.2897 |           0.1379 |
[32m[20230207 15:21:00 @agent_ppo2.py:192][0m |          -0.0075 |           2.2632 |           0.1378 |
[32m[20230207 15:21:00 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:21:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: 1.68
[32m[20230207 15:21:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 81.87
[32m[20230207 15:21:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 96.94
[32m[20230207 15:21:01 @agent_ppo2.py:150][0m Total time:      20.33 min
[32m[20230207 15:21:01 @agent_ppo2.py:152][0m 1058816 total steps have happened
[32m[20230207 15:21:01 @agent_ppo2.py:128][0m #------------------------ Iteration 517 --------------------------#
[32m[20230207 15:21:01 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:21:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:02 @agent_ppo2.py:192][0m |           0.0102 |           4.9585 |           0.1434 |
[32m[20230207 15:21:02 @agent_ppo2.py:192][0m |          -0.0087 |           3.6781 |           0.1432 |
[32m[20230207 15:21:02 @agent_ppo2.py:192][0m |          -0.0099 |           3.4546 |           0.1435 |
[32m[20230207 15:21:02 @agent_ppo2.py:192][0m |          -0.0007 |           3.1323 |           0.1435 |
[32m[20230207 15:21:02 @agent_ppo2.py:192][0m |          -0.0130 |           3.0386 |           0.1434 |
[32m[20230207 15:21:02 @agent_ppo2.py:192][0m |           0.0014 |           2.9954 |           0.1435 |
[32m[20230207 15:21:02 @agent_ppo2.py:192][0m |          -0.0088 |           2.9053 |           0.1436 |
[32m[20230207 15:21:02 @agent_ppo2.py:192][0m |          -0.0106 |           2.7307 |           0.1435 |
[32m[20230207 15:21:02 @agent_ppo2.py:192][0m |          -0.0041 |           2.7076 |           0.1436 |
[32m[20230207 15:21:02 @agent_ppo2.py:192][0m |          -0.0240 |           2.6172 |           0.1436 |
[32m[20230207 15:21:02 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:21:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: 10.46
[32m[20230207 15:21:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 50.72
[32m[20230207 15:21:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 99.60
[32m[20230207 15:21:03 @agent_ppo2.py:150][0m Total time:      20.36 min
[32m[20230207 15:21:03 @agent_ppo2.py:152][0m 1060864 total steps have happened
[32m[20230207 15:21:03 @agent_ppo2.py:128][0m #------------------------ Iteration 518 --------------------------#
[32m[20230207 15:21:04 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:21:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:04 @agent_ppo2.py:192][0m |           0.0049 |           3.3818 |           0.1428 |
[32m[20230207 15:21:04 @agent_ppo2.py:192][0m |           0.0014 |           2.2054 |           0.1424 |
[32m[20230207 15:21:04 @agent_ppo2.py:192][0m |          -0.0022 |           2.0549 |           0.1424 |
[32m[20230207 15:21:04 @agent_ppo2.py:192][0m |           0.0162 |           1.9046 |           0.1425 |
[32m[20230207 15:21:04 @agent_ppo2.py:192][0m |          -0.0010 |           1.8050 |           0.1424 |
[32m[20230207 15:21:04 @agent_ppo2.py:192][0m |          -0.0042 |           1.7353 |           0.1424 |
[32m[20230207 15:21:04 @agent_ppo2.py:192][0m |          -0.0130 |           1.6934 |           0.1426 |
[32m[20230207 15:21:05 @agent_ppo2.py:192][0m |          -0.0048 |           1.6631 |           0.1425 |
[32m[20230207 15:21:05 @agent_ppo2.py:192][0m |          -0.0391 |           1.6337 |           0.1426 |
[32m[20230207 15:21:05 @agent_ppo2.py:192][0m |          -0.0054 |           1.6217 |           0.1427 |
[32m[20230207 15:21:05 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:21:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: 96.75
[32m[20230207 15:21:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 111.68
[32m[20230207 15:21:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 31.70
[32m[20230207 15:21:06 @agent_ppo2.py:150][0m Total time:      20.40 min
[32m[20230207 15:21:06 @agent_ppo2.py:152][0m 1062912 total steps have happened
[32m[20230207 15:21:06 @agent_ppo2.py:128][0m #------------------------ Iteration 519 --------------------------#
[32m[20230207 15:21:06 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:21:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:06 @agent_ppo2.py:192][0m |           0.0039 |           5.0622 |           0.1412 |
[32m[20230207 15:21:06 @agent_ppo2.py:192][0m |          -0.0034 |           2.2116 |           0.1413 |
[32m[20230207 15:21:06 @agent_ppo2.py:192][0m |          -0.0092 |           1.8376 |           0.1413 |
[32m[20230207 15:21:06 @agent_ppo2.py:192][0m |          -0.0095 |           1.6622 |           0.1413 |
[32m[20230207 15:21:06 @agent_ppo2.py:192][0m |          -0.0059 |           1.5768 |           0.1412 |
[32m[20230207 15:21:06 @agent_ppo2.py:192][0m |          -0.0057 |           1.5188 |           0.1410 |
[32m[20230207 15:21:06 @agent_ppo2.py:192][0m |          -0.0102 |           1.4798 |           0.1410 |
[32m[20230207 15:21:06 @agent_ppo2.py:192][0m |          -0.0112 |           1.4249 |           0.1410 |
[32m[20230207 15:21:07 @agent_ppo2.py:192][0m |          -0.0146 |           1.3933 |           0.1410 |
[32m[20230207 15:21:07 @agent_ppo2.py:192][0m |          -0.0115 |           1.3719 |           0.1410 |
[32m[20230207 15:21:07 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:21:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -40.81
[32m[20230207 15:21:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 23.30
[32m[20230207 15:21:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 58.89
[32m[20230207 15:21:07 @agent_ppo2.py:150][0m Total time:      20.44 min
[32m[20230207 15:21:07 @agent_ppo2.py:152][0m 1064960 total steps have happened
[32m[20230207 15:21:07 @agent_ppo2.py:128][0m #------------------------ Iteration 520 --------------------------#
[32m[20230207 15:21:08 @agent_ppo2.py:134][0m Sampling time: 0.71 s by 1 slaves
[32m[20230207 15:21:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:08 @agent_ppo2.py:192][0m |           0.0017 |          23.0000 |           0.1437 |
[32m[20230207 15:21:08 @agent_ppo2.py:192][0m |          -0.0013 |           9.4220 |           0.1437 |
[32m[20230207 15:21:08 @agent_ppo2.py:192][0m |          -0.0053 |           7.1650 |           0.1437 |
[32m[20230207 15:21:08 @agent_ppo2.py:192][0m |          -0.0070 |           6.4162 |           0.1437 |
[32m[20230207 15:21:09 @agent_ppo2.py:192][0m |          -0.0083 |           5.8839 |           0.1436 |
[32m[20230207 15:21:09 @agent_ppo2.py:192][0m |          -0.0099 |           5.4923 |           0.1436 |
[32m[20230207 15:21:09 @agent_ppo2.py:192][0m |          -0.0104 |           5.2957 |           0.1436 |
[32m[20230207 15:21:09 @agent_ppo2.py:192][0m |          -0.0100 |           5.0746 |           0.1435 |
[32m[20230207 15:21:09 @agent_ppo2.py:192][0m |          -0.0120 |           4.9057 |           0.1434 |
[32m[20230207 15:21:09 @agent_ppo2.py:192][0m |          -0.0126 |           4.7259 |           0.1434 |
[32m[20230207 15:21:09 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230207 15:21:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: -66.12
[32m[20230207 15:21:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -28.41
[32m[20230207 15:21:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -54.16
[32m[20230207 15:21:09 @agent_ppo2.py:150][0m Total time:      20.47 min
[32m[20230207 15:21:09 @agent_ppo2.py:152][0m 1067008 total steps have happened
[32m[20230207 15:21:09 @agent_ppo2.py:128][0m #------------------------ Iteration 521 --------------------------#
[32m[20230207 15:21:10 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:21:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:10 @agent_ppo2.py:192][0m |           0.0133 |           1.6135 |           0.1416 |
[32m[20230207 15:21:10 @agent_ppo2.py:192][0m |           0.0035 |           1.1614 |           0.1415 |
[32m[20230207 15:21:10 @agent_ppo2.py:192][0m |          -0.0179 |           1.1009 |           0.1415 |
[32m[20230207 15:21:11 @agent_ppo2.py:192][0m |           0.0119 |           1.0592 |           0.1412 |
[32m[20230207 15:21:11 @agent_ppo2.py:192][0m |          -0.0048 |           1.0422 |           0.1414 |
[32m[20230207 15:21:11 @agent_ppo2.py:192][0m |          -0.0389 |           1.0216 |           0.1413 |
[32m[20230207 15:21:11 @agent_ppo2.py:192][0m |          -0.0054 |           1.0084 |           0.1412 |
[32m[20230207 15:21:11 @agent_ppo2.py:192][0m |          -0.0068 |           1.0073 |           0.1414 |
[32m[20230207 15:21:11 @agent_ppo2.py:192][0m |          -0.0024 |           0.9849 |           0.1416 |
[32m[20230207 15:21:11 @agent_ppo2.py:192][0m |          -0.0012 |           0.9729 |           0.1414 |
[32m[20230207 15:21:11 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:21:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: 42.68
[32m[20230207 15:21:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 86.56
[32m[20230207 15:21:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 104.65
[32m[20230207 15:21:12 @agent_ppo2.py:150][0m Total time:      20.51 min
[32m[20230207 15:21:12 @agent_ppo2.py:152][0m 1069056 total steps have happened
[32m[20230207 15:21:12 @agent_ppo2.py:128][0m #------------------------ Iteration 522 --------------------------#
[32m[20230207 15:21:13 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:21:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:13 @agent_ppo2.py:192][0m |          -0.0012 |           9.2027 |           0.1444 |
[32m[20230207 15:21:13 @agent_ppo2.py:192][0m |          -0.0072 |           3.1420 |           0.1441 |
[32m[20230207 15:21:13 @agent_ppo2.py:192][0m |          -0.0073 |           2.3006 |           0.1440 |
[32m[20230207 15:21:13 @agent_ppo2.py:192][0m |          -0.0071 |           1.9026 |           0.1440 |
[32m[20230207 15:21:13 @agent_ppo2.py:192][0m |          -0.0106 |           1.7253 |           0.1439 |
[32m[20230207 15:21:13 @agent_ppo2.py:192][0m |          -0.0099 |           1.6111 |           0.1438 |
[32m[20230207 15:21:13 @agent_ppo2.py:192][0m |          -0.0070 |           1.5510 |           0.1439 |
[32m[20230207 15:21:13 @agent_ppo2.py:192][0m |          -0.0106 |           1.4781 |           0.1439 |
[32m[20230207 15:21:13 @agent_ppo2.py:192][0m |          -0.0126 |           1.4505 |           0.1437 |
[32m[20230207 15:21:14 @agent_ppo2.py:192][0m |          -0.0103 |           1.3884 |           0.1438 |
[32m[20230207 15:21:14 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:21:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: -15.57
[32m[20230207 15:21:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 35.95
[32m[20230207 15:21:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -15.83
[32m[20230207 15:21:14 @agent_ppo2.py:150][0m Total time:      20.55 min
[32m[20230207 15:21:14 @agent_ppo2.py:152][0m 1071104 total steps have happened
[32m[20230207 15:21:14 @agent_ppo2.py:128][0m #------------------------ Iteration 523 --------------------------#
[32m[20230207 15:21:15 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:21:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:15 @agent_ppo2.py:192][0m |           0.0183 |           1.4435 |           0.1421 |
[32m[20230207 15:21:15 @agent_ppo2.py:192][0m |          -0.0025 |           1.1501 |           0.1417 |
[32m[20230207 15:21:15 @agent_ppo2.py:192][0m |           0.0125 |           1.1057 |           0.1417 |
[32m[20230207 15:21:15 @agent_ppo2.py:192][0m |          -0.0064 |           1.0679 |           0.1413 |
[32m[20230207 15:21:15 @agent_ppo2.py:192][0m |           0.0045 |           1.0399 |           0.1413 |
[32m[20230207 15:21:15 @agent_ppo2.py:192][0m |          -0.0190 |           1.0276 |           0.1412 |
[32m[20230207 15:21:15 @agent_ppo2.py:192][0m |          -0.0026 |           1.0151 |           0.1412 |
[32m[20230207 15:21:16 @agent_ppo2.py:192][0m |          -0.0115 |           1.0496 |           0.1411 |
[32m[20230207 15:21:16 @agent_ppo2.py:192][0m |           0.0006 |           1.0028 |           0.1412 |
[32m[20230207 15:21:16 @agent_ppo2.py:192][0m |          -0.0029 |           0.9742 |           0.1411 |
[32m[20230207 15:21:16 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:21:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: 69.24
[32m[20230207 15:21:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 124.25
[32m[20230207 15:21:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 46.25
[32m[20230207 15:21:16 @agent_ppo2.py:150][0m Total time:      20.59 min
[32m[20230207 15:21:16 @agent_ppo2.py:152][0m 1073152 total steps have happened
[32m[20230207 15:21:16 @agent_ppo2.py:128][0m #------------------------ Iteration 524 --------------------------#
[32m[20230207 15:21:17 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:21:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:17 @agent_ppo2.py:192][0m |          -0.0110 |           1.8348 |           0.1416 |
[32m[20230207 15:21:17 @agent_ppo2.py:192][0m |          -0.0221 |           1.2437 |           0.1416 |
[32m[20230207 15:21:18 @agent_ppo2.py:192][0m |           0.0121 |           1.1061 |           0.1415 |
[32m[20230207 15:21:18 @agent_ppo2.py:192][0m |          -0.0071 |           1.0360 |           0.1415 |
[32m[20230207 15:21:18 @agent_ppo2.py:192][0m |          -0.0082 |           0.9895 |           0.1414 |
[32m[20230207 15:21:18 @agent_ppo2.py:192][0m |          -0.0100 |           0.9564 |           0.1414 |
[32m[20230207 15:21:18 @agent_ppo2.py:192][0m |          -0.0044 |           0.9302 |           0.1414 |
[32m[20230207 15:21:18 @agent_ppo2.py:192][0m |           0.0086 |           0.9149 |           0.1414 |
[32m[20230207 15:21:18 @agent_ppo2.py:192][0m |          -0.0034 |           0.8934 |           0.1414 |
[32m[20230207 15:21:18 @agent_ppo2.py:192][0m |          -0.0254 |           0.8768 |           0.1414 |
[32m[20230207 15:21:18 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:21:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: 69.41
[32m[20230207 15:21:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 73.94
[32m[20230207 15:21:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -8.25
[32m[20230207 15:21:19 @agent_ppo2.py:150][0m Total time:      20.63 min
[32m[20230207 15:21:19 @agent_ppo2.py:152][0m 1075200 total steps have happened
[32m[20230207 15:21:19 @agent_ppo2.py:128][0m #------------------------ Iteration 525 --------------------------#
[32m[20230207 15:21:20 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230207 15:21:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:20 @agent_ppo2.py:192][0m |          -0.0018 |          24.3579 |           0.1408 |
[32m[20230207 15:21:20 @agent_ppo2.py:192][0m |          -0.0047 |          14.8552 |           0.1407 |
[32m[20230207 15:21:20 @agent_ppo2.py:192][0m |          -0.0070 |           9.9744 |           0.1407 |
[32m[20230207 15:21:20 @agent_ppo2.py:192][0m |          -0.0083 |           6.9123 |           0.1406 |
[32m[20230207 15:21:20 @agent_ppo2.py:192][0m |          -0.0109 |           5.3925 |           0.1405 |
[32m[20230207 15:21:20 @agent_ppo2.py:192][0m |          -0.0106 |           4.6288 |           0.1405 |
[32m[20230207 15:21:20 @agent_ppo2.py:192][0m |          -0.0128 |           3.8434 |           0.1406 |
[32m[20230207 15:21:20 @agent_ppo2.py:192][0m |          -0.0133 |           3.5454 |           0.1404 |
[32m[20230207 15:21:20 @agent_ppo2.py:192][0m |          -0.0135 |           3.0886 |           0.1405 |
[32m[20230207 15:21:20 @agent_ppo2.py:192][0m |          -0.0157 |           2.8296 |           0.1405 |
[32m[20230207 15:21:20 @agent_ppo2.py:137][0m Policy update time: 0.70 s
[32m[20230207 15:21:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -30.14
[32m[20230207 15:21:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 30.08
[32m[20230207 15:21:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 28.45
[32m[20230207 15:21:21 @agent_ppo2.py:150][0m Total time:      20.66 min
[32m[20230207 15:21:21 @agent_ppo2.py:152][0m 1077248 total steps have happened
[32m[20230207 15:21:21 @agent_ppo2.py:128][0m #------------------------ Iteration 526 --------------------------#
[32m[20230207 15:21:22 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230207 15:21:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:22 @agent_ppo2.py:192][0m |           0.0019 |          25.1069 |           0.1441 |
[32m[20230207 15:21:22 @agent_ppo2.py:192][0m |          -0.0048 |          11.1947 |           0.1440 |
[32m[20230207 15:21:22 @agent_ppo2.py:192][0m |          -0.0067 |           7.9687 |           0.1438 |
[32m[20230207 15:21:22 @agent_ppo2.py:192][0m |          -0.0092 |           6.4220 |           0.1437 |
[32m[20230207 15:21:22 @agent_ppo2.py:192][0m |          -0.0107 |           5.3913 |           0.1436 |
[32m[20230207 15:21:22 @agent_ppo2.py:192][0m |          -0.0122 |           4.7829 |           0.1436 |
[32m[20230207 15:21:22 @agent_ppo2.py:192][0m |          -0.0128 |           4.3466 |           0.1435 |
[32m[20230207 15:21:22 @agent_ppo2.py:192][0m |          -0.0137 |           3.9033 |           0.1435 |
[32m[20230207 15:21:22 @agent_ppo2.py:192][0m |          -0.0135 |           3.7561 |           0.1434 |
[32m[20230207 15:21:22 @agent_ppo2.py:192][0m |          -0.0139 |           3.3551 |           0.1434 |
[32m[20230207 15:21:22 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230207 15:21:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -38.98
[32m[20230207 15:21:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -31.23
[32m[20230207 15:21:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -45.53
[32m[20230207 15:21:23 @agent_ppo2.py:150][0m Total time:      20.70 min
[32m[20230207 15:21:23 @agent_ppo2.py:152][0m 1079296 total steps have happened
[32m[20230207 15:21:23 @agent_ppo2.py:128][0m #------------------------ Iteration 527 --------------------------#
[32m[20230207 15:21:24 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:21:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:24 @agent_ppo2.py:192][0m |           0.0043 |           7.8770 |           0.1436 |
[32m[20230207 15:21:24 @agent_ppo2.py:192][0m |          -0.0048 |           3.4509 |           0.1436 |
[32m[20230207 15:21:24 @agent_ppo2.py:192][0m |          -0.0018 |           2.9975 |           0.1436 |
[32m[20230207 15:21:24 @agent_ppo2.py:192][0m |          -0.0093 |           2.7775 |           0.1436 |
[32m[20230207 15:21:24 @agent_ppo2.py:192][0m |          -0.0121 |           2.6625 |           0.1435 |
[32m[20230207 15:21:25 @agent_ppo2.py:192][0m |          -0.0090 |           2.5215 |           0.1435 |
[32m[20230207 15:21:25 @agent_ppo2.py:192][0m |          -0.0097 |           2.4560 |           0.1436 |
[32m[20230207 15:21:25 @agent_ppo2.py:192][0m |          -0.0015 |           2.4089 |           0.1436 |
[32m[20230207 15:21:25 @agent_ppo2.py:192][0m |          -0.0133 |           2.3533 |           0.1435 |
[32m[20230207 15:21:25 @agent_ppo2.py:192][0m |          -0.0100 |           2.2953 |           0.1436 |
[32m[20230207 15:21:25 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:21:26 @agent_ppo2.py:145][0m Average TRAINING episode reward: -19.65
[32m[20230207 15:21:26 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 37.47
[32m[20230207 15:21:26 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -86.59
[32m[20230207 15:21:26 @agent_ppo2.py:150][0m Total time:      20.74 min
[32m[20230207 15:21:26 @agent_ppo2.py:152][0m 1081344 total steps have happened
[32m[20230207 15:21:26 @agent_ppo2.py:128][0m #------------------------ Iteration 528 --------------------------#
[32m[20230207 15:21:26 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230207 15:21:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:26 @agent_ppo2.py:192][0m |           0.0019 |          12.2972 |           0.1461 |
[32m[20230207 15:21:26 @agent_ppo2.py:192][0m |           0.0007 |           4.7982 |           0.1458 |
[32m[20230207 15:21:26 @agent_ppo2.py:192][0m |          -0.0047 |           3.8605 |           0.1456 |
[32m[20230207 15:21:27 @agent_ppo2.py:192][0m |          -0.0084 |           3.5802 |           0.1456 |
[32m[20230207 15:21:27 @agent_ppo2.py:192][0m |          -0.0067 |           3.2903 |           0.1456 |
[32m[20230207 15:21:27 @agent_ppo2.py:192][0m |          -0.0122 |           3.1897 |           0.1456 |
[32m[20230207 15:21:27 @agent_ppo2.py:192][0m |          -0.0175 |           2.8791 |           0.1456 |
[32m[20230207 15:21:27 @agent_ppo2.py:192][0m |          -0.0157 |           2.7953 |           0.1456 |
[32m[20230207 15:21:27 @agent_ppo2.py:192][0m |          -0.0121 |           2.6770 |           0.1455 |
[32m[20230207 15:21:27 @agent_ppo2.py:192][0m |           0.0019 |           2.5652 |           0.1456 |
[32m[20230207 15:21:27 @agent_ppo2.py:137][0m Policy update time: 0.67 s
[32m[20230207 15:21:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: -35.18
[32m[20230207 15:21:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 19.24
[32m[20230207 15:21:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 5.77
[32m[20230207 15:21:28 @agent_ppo2.py:150][0m Total time:      20.77 min
[32m[20230207 15:21:28 @agent_ppo2.py:152][0m 1083392 total steps have happened
[32m[20230207 15:21:28 @agent_ppo2.py:128][0m #------------------------ Iteration 529 --------------------------#
[32m[20230207 15:21:28 @agent_ppo2.py:134][0m Sampling time: 0.51 s by 1 slaves
[32m[20230207 15:21:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:28 @agent_ppo2.py:192][0m |          -0.0030 |          17.6682 |           0.1402 |
[32m[20230207 15:21:28 @agent_ppo2.py:192][0m |          -0.0080 |           7.0716 |           0.1403 |
[32m[20230207 15:21:28 @agent_ppo2.py:192][0m |          -0.0031 |           4.2436 |           0.1401 |
[32m[20230207 15:21:28 @agent_ppo2.py:192][0m |           0.0057 |           3.3424 |           0.1401 |
[32m[20230207 15:21:28 @agent_ppo2.py:192][0m |          -0.0158 |           2.9547 |           0.1400 |
[32m[20230207 15:21:29 @agent_ppo2.py:192][0m |          -0.0165 |           2.5765 |           0.1399 |
[32m[20230207 15:21:29 @agent_ppo2.py:192][0m |          -0.0162 |           2.4454 |           0.1398 |
[32m[20230207 15:21:29 @agent_ppo2.py:192][0m |          -0.0146 |           2.2923 |           0.1397 |
[32m[20230207 15:21:29 @agent_ppo2.py:192][0m |          -0.0196 |           2.2223 |           0.1396 |
[32m[20230207 15:21:29 @agent_ppo2.py:192][0m |          -0.0135 |           2.1202 |           0.1395 |
[32m[20230207 15:21:29 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:21:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -61.28
[32m[20230207 15:21:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 7.40
[32m[20230207 15:21:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 7.34
[32m[20230207 15:21:30 @agent_ppo2.py:150][0m Total time:      20.80 min
[32m[20230207 15:21:30 @agent_ppo2.py:152][0m 1085440 total steps have happened
[32m[20230207 15:21:30 @agent_ppo2.py:128][0m #------------------------ Iteration 530 --------------------------#
[32m[20230207 15:21:30 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:21:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:30 @agent_ppo2.py:192][0m |           0.0012 |          17.2498 |           0.1440 |
[32m[20230207 15:21:31 @agent_ppo2.py:192][0m |          -0.0048 |           5.6180 |           0.1439 |
[32m[20230207 15:21:31 @agent_ppo2.py:192][0m |          -0.0083 |           4.7362 |           0.1440 |
[32m[20230207 15:21:31 @agent_ppo2.py:192][0m |          -0.0089 |           4.2652 |           0.1441 |
[32m[20230207 15:21:31 @agent_ppo2.py:192][0m |          -0.0074 |           4.0116 |           0.1441 |
[32m[20230207 15:21:31 @agent_ppo2.py:192][0m |          -0.0106 |           3.7083 |           0.1441 |
[32m[20230207 15:21:31 @agent_ppo2.py:192][0m |          -0.0114 |           3.5043 |           0.1442 |
[32m[20230207 15:21:31 @agent_ppo2.py:192][0m |          -0.0111 |           3.2610 |           0.1443 |
[32m[20230207 15:21:31 @agent_ppo2.py:192][0m |          -0.0153 |           3.1040 |           0.1442 |
[32m[20230207 15:21:31 @agent_ppo2.py:192][0m |          -0.0147 |           2.9320 |           0.1444 |
[32m[20230207 15:21:31 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:21:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -57.09
[32m[20230207 15:21:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 30.83
[32m[20230207 15:21:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -67.62
[32m[20230207 15:21:32 @agent_ppo2.py:150][0m Total time:      20.84 min
[32m[20230207 15:21:32 @agent_ppo2.py:152][0m 1087488 total steps have happened
[32m[20230207 15:21:32 @agent_ppo2.py:128][0m #------------------------ Iteration 531 --------------------------#
[32m[20230207 15:21:33 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:21:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:33 @agent_ppo2.py:192][0m |          -0.0024 |          14.9394 |           0.1421 |
[32m[20230207 15:21:33 @agent_ppo2.py:192][0m |          -0.0010 |           7.3209 |           0.1420 |
[32m[20230207 15:21:33 @agent_ppo2.py:192][0m |          -0.0082 |           5.7563 |           0.1420 |
[32m[20230207 15:21:33 @agent_ppo2.py:192][0m |          -0.0073 |           4.8676 |           0.1420 |
[32m[20230207 15:21:33 @agent_ppo2.py:192][0m |          -0.0084 |           4.2927 |           0.1420 |
[32m[20230207 15:21:33 @agent_ppo2.py:192][0m |          -0.0091 |           3.9250 |           0.1420 |
[32m[20230207 15:21:33 @agent_ppo2.py:192][0m |          -0.0083 |           3.5805 |           0.1421 |
[32m[20230207 15:21:33 @agent_ppo2.py:192][0m |          -0.0113 |           3.3251 |           0.1421 |
[32m[20230207 15:21:33 @agent_ppo2.py:192][0m |          -0.0113 |           3.1945 |           0.1421 |
[32m[20230207 15:21:33 @agent_ppo2.py:192][0m |          -0.0113 |           3.0098 |           0.1421 |
[32m[20230207 15:21:33 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:21:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: 2.73
[32m[20230207 15:21:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 59.66
[32m[20230207 15:21:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 43.63
[32m[20230207 15:21:34 @agent_ppo2.py:150][0m Total time:      20.88 min
[32m[20230207 15:21:34 @agent_ppo2.py:152][0m 1089536 total steps have happened
[32m[20230207 15:21:34 @agent_ppo2.py:128][0m #------------------------ Iteration 532 --------------------------#
[32m[20230207 15:21:35 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230207 15:21:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:35 @agent_ppo2.py:192][0m |          -0.0037 |          15.0180 |           0.1435 |
[32m[20230207 15:21:35 @agent_ppo2.py:192][0m |          -0.0087 |           7.1925 |           0.1433 |
[32m[20230207 15:21:35 @agent_ppo2.py:192][0m |          -0.0073 |           5.7781 |           0.1436 |
[32m[20230207 15:21:35 @agent_ppo2.py:192][0m |           0.0091 |           5.2841 |           0.1434 |
[32m[20230207 15:21:35 @agent_ppo2.py:192][0m |          -0.0113 |           4.8985 |           0.1432 |
[32m[20230207 15:21:35 @agent_ppo2.py:192][0m |          -0.0099 |           4.5973 |           0.1432 |
[32m[20230207 15:21:35 @agent_ppo2.py:192][0m |          -0.0178 |           4.3381 |           0.1432 |
[32m[20230207 15:21:35 @agent_ppo2.py:192][0m |          -0.0123 |           4.1964 |           0.1434 |
[32m[20230207 15:21:36 @agent_ppo2.py:192][0m |          -0.0087 |           4.4547 |           0.1432 |
[32m[20230207 15:21:36 @agent_ppo2.py:192][0m |          -0.0148 |           3.8827 |           0.1433 |
[32m[20230207 15:21:36 @agent_ppo2.py:137][0m Policy update time: 0.69 s
[32m[20230207 15:21:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -31.56
[32m[20230207 15:21:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 80.07
[32m[20230207 15:21:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 43.58
[32m[20230207 15:21:36 @agent_ppo2.py:150][0m Total time:      20.92 min
[32m[20230207 15:21:36 @agent_ppo2.py:152][0m 1091584 total steps have happened
[32m[20230207 15:21:36 @agent_ppo2.py:128][0m #------------------------ Iteration 533 --------------------------#
[32m[20230207 15:21:37 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:21:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:37 @agent_ppo2.py:192][0m |          -0.0047 |          34.6060 |           0.1420 |
[32m[20230207 15:21:37 @agent_ppo2.py:192][0m |          -0.0041 |          18.4794 |           0.1415 |
[32m[20230207 15:21:37 @agent_ppo2.py:192][0m |          -0.0152 |          12.9324 |           0.1416 |
[32m[20230207 15:21:37 @agent_ppo2.py:192][0m |          -0.0134 |          11.4337 |           0.1414 |
[32m[20230207 15:21:37 @agent_ppo2.py:192][0m |          -0.0151 |           9.0040 |           0.1413 |
[32m[20230207 15:21:37 @agent_ppo2.py:192][0m |          -0.0111 |           7.9945 |           0.1411 |
[32m[20230207 15:21:37 @agent_ppo2.py:192][0m |          -0.0174 |           7.5757 |           0.1411 |
[32m[20230207 15:21:37 @agent_ppo2.py:192][0m |          -0.0127 |           6.9675 |           0.1411 |
[32m[20230207 15:21:37 @agent_ppo2.py:192][0m |          -0.0193 |           6.6461 |           0.1411 |
[32m[20230207 15:21:37 @agent_ppo2.py:192][0m |          -0.0189 |           6.2838 |           0.1409 |
[32m[20230207 15:21:37 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:21:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: -83.16
[32m[20230207 15:21:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -41.41
[32m[20230207 15:21:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 38.22
[32m[20230207 15:21:38 @agent_ppo2.py:150][0m Total time:      20.95 min
[32m[20230207 15:21:38 @agent_ppo2.py:152][0m 1093632 total steps have happened
[32m[20230207 15:21:38 @agent_ppo2.py:128][0m #------------------------ Iteration 534 --------------------------#
[32m[20230207 15:21:39 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:21:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:39 @agent_ppo2.py:192][0m |           0.0064 |           2.8425 |           0.1431 |
[32m[20230207 15:21:39 @agent_ppo2.py:192][0m |          -0.0009 |           2.0644 |           0.1431 |
[32m[20230207 15:21:39 @agent_ppo2.py:192][0m |           0.0144 |           2.0148 |           0.1431 |
[32m[20230207 15:21:39 @agent_ppo2.py:192][0m |          -0.0033 |           1.9197 |           0.1430 |
[32m[20230207 15:21:39 @agent_ppo2.py:192][0m |          -0.0630 |           2.3461 |           0.1431 |
[32m[20230207 15:21:40 @agent_ppo2.py:192][0m |          -0.0050 |           1.9003 |           0.1431 |
[32m[20230207 15:21:40 @agent_ppo2.py:192][0m |          -0.0091 |           1.8442 |           0.1431 |
[32m[20230207 15:21:40 @agent_ppo2.py:192][0m |          -0.0027 |           1.8416 |           0.1432 |
[32m[20230207 15:21:40 @agent_ppo2.py:192][0m |          -0.0115 |           1.8139 |           0.1432 |
[32m[20230207 15:21:40 @agent_ppo2.py:192][0m |           0.0028 |           1.8026 |           0.1430 |
[32m[20230207 15:21:40 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:21:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: 31.93
[32m[20230207 15:21:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 38.09
[32m[20230207 15:21:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -10.84
[32m[20230207 15:21:41 @agent_ppo2.py:150][0m Total time:      20.99 min
[32m[20230207 15:21:41 @agent_ppo2.py:152][0m 1095680 total steps have happened
[32m[20230207 15:21:41 @agent_ppo2.py:128][0m #------------------------ Iteration 535 --------------------------#
[32m[20230207 15:21:41 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230207 15:21:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:41 @agent_ppo2.py:192][0m |          -0.0015 |          17.2062 |           0.1417 |
[32m[20230207 15:21:41 @agent_ppo2.py:192][0m |          -0.0061 |           5.7725 |           0.1416 |
[32m[20230207 15:21:42 @agent_ppo2.py:192][0m |          -0.0124 |           4.0130 |           0.1416 |
[32m[20230207 15:21:42 @agent_ppo2.py:192][0m |          -0.0155 |           3.2953 |           0.1415 |
[32m[20230207 15:21:42 @agent_ppo2.py:192][0m |          -0.0158 |           3.0174 |           0.1415 |
[32m[20230207 15:21:42 @agent_ppo2.py:192][0m |          -0.0166 |           2.6235 |           0.1414 |
[32m[20230207 15:21:42 @agent_ppo2.py:192][0m |          -0.0181 |           2.4719 |           0.1413 |
[32m[20230207 15:21:42 @agent_ppo2.py:192][0m |          -0.0171 |           2.3493 |           0.1413 |
[32m[20230207 15:21:42 @agent_ppo2.py:192][0m |          -0.0151 |           2.1907 |           0.1412 |
[32m[20230207 15:21:42 @agent_ppo2.py:192][0m |          -0.0207 |           2.0815 |           0.1412 |
[32m[20230207 15:21:42 @agent_ppo2.py:137][0m Policy update time: 0.64 s
[32m[20230207 15:21:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -56.41
[32m[20230207 15:21:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 45.68
[32m[20230207 15:21:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -21.59
[32m[20230207 15:21:43 @agent_ppo2.py:150][0m Total time:      21.03 min
[32m[20230207 15:21:43 @agent_ppo2.py:152][0m 1097728 total steps have happened
[32m[20230207 15:21:43 @agent_ppo2.py:128][0m #------------------------ Iteration 536 --------------------------#
[32m[20230207 15:21:44 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:21:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:44 @agent_ppo2.py:192][0m |           0.0006 |           9.7307 |           0.1468 |
[32m[20230207 15:21:44 @agent_ppo2.py:192][0m |          -0.0033 |           4.7518 |           0.1467 |
[32m[20230207 15:21:44 @agent_ppo2.py:192][0m |          -0.0066 |           4.0321 |           0.1467 |
[32m[20230207 15:21:44 @agent_ppo2.py:192][0m |          -0.0074 |           3.5960 |           0.1464 |
[32m[20230207 15:21:44 @agent_ppo2.py:192][0m |          -0.0093 |           3.3602 |           0.1465 |
[32m[20230207 15:21:44 @agent_ppo2.py:192][0m |          -0.0110 |           3.1440 |           0.1465 |
[32m[20230207 15:21:44 @agent_ppo2.py:192][0m |          -0.0119 |           2.9473 |           0.1463 |
[32m[20230207 15:21:44 @agent_ppo2.py:192][0m |          -0.0123 |           2.8001 |           0.1463 |
[32m[20230207 15:21:44 @agent_ppo2.py:192][0m |          -0.0146 |           2.6850 |           0.1462 |
[32m[20230207 15:21:44 @agent_ppo2.py:192][0m |          -0.0143 |           2.6146 |           0.1463 |
[32m[20230207 15:21:44 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:21:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: -26.23
[32m[20230207 15:21:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 70.40
[32m[20230207 15:21:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 61.88
[32m[20230207 15:21:45 @agent_ppo2.py:150][0m Total time:      21.07 min
[32m[20230207 15:21:45 @agent_ppo2.py:152][0m 1099776 total steps have happened
[32m[20230207 15:21:45 @agent_ppo2.py:128][0m #------------------------ Iteration 537 --------------------------#
[32m[20230207 15:21:46 @agent_ppo2.py:134][0m Sampling time: 0.69 s by 1 slaves
[32m[20230207 15:21:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:46 @agent_ppo2.py:192][0m |          -0.0007 |          12.4828 |           0.1479 |
[32m[20230207 15:21:46 @agent_ppo2.py:192][0m |          -0.0041 |           4.9877 |           0.1479 |
[32m[20230207 15:21:46 @agent_ppo2.py:192][0m |          -0.0072 |           3.8252 |           0.1479 |
[32m[20230207 15:21:46 @agent_ppo2.py:192][0m |          -0.0099 |           3.4343 |           0.1477 |
[32m[20230207 15:21:46 @agent_ppo2.py:192][0m |          -0.0105 |           3.1474 |           0.1479 |
[32m[20230207 15:21:46 @agent_ppo2.py:192][0m |          -0.0119 |           3.0677 |           0.1479 |
[32m[20230207 15:21:46 @agent_ppo2.py:192][0m |          -0.0122 |           2.7583 |           0.1478 |
[32m[20230207 15:21:47 @agent_ppo2.py:192][0m |          -0.0129 |           2.5860 |           0.1479 |
[32m[20230207 15:21:47 @agent_ppo2.py:192][0m |          -0.0128 |           2.4704 |           0.1479 |
[32m[20230207 15:21:47 @agent_ppo2.py:192][0m |          -0.0126 |           2.3358 |           0.1479 |
[32m[20230207 15:21:47 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230207 15:21:47 @agent_ppo2.py:145][0m Average TRAINING episode reward: -24.25
[32m[20230207 15:21:47 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 79.99
[32m[20230207 15:21:47 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -54.12
[32m[20230207 15:21:47 @agent_ppo2.py:150][0m Total time:      21.10 min
[32m[20230207 15:21:47 @agent_ppo2.py:152][0m 1101824 total steps have happened
[32m[20230207 15:21:47 @agent_ppo2.py:128][0m #------------------------ Iteration 538 --------------------------#
[32m[20230207 15:21:48 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:21:48 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:48 @agent_ppo2.py:192][0m |           0.0006 |          22.6397 |           0.1458 |
[32m[20230207 15:21:48 @agent_ppo2.py:192][0m |          -0.0207 |          10.9448 |           0.1457 |
[32m[20230207 15:21:48 @agent_ppo2.py:192][0m |          -0.0499 |           9.4177 |           0.1458 |
[32m[20230207 15:21:48 @agent_ppo2.py:192][0m |           0.0003 |           8.8349 |           0.1456 |
[32m[20230207 15:21:49 @agent_ppo2.py:192][0m |          -0.0025 |           7.7407 |           0.1457 |
[32m[20230207 15:21:49 @agent_ppo2.py:192][0m |          -0.0140 |           7.2256 |           0.1457 |
[32m[20230207 15:21:49 @agent_ppo2.py:192][0m |           0.0041 |           6.9700 |           0.1456 |
[32m[20230207 15:21:49 @agent_ppo2.py:192][0m |          -0.0304 |           6.8275 |           0.1456 |
[32m[20230207 15:21:49 @agent_ppo2.py:192][0m |          -0.0030 |           6.5397 |           0.1456 |
[32m[20230207 15:21:49 @agent_ppo2.py:192][0m |          -0.0037 |           6.4020 |           0.1458 |
[32m[20230207 15:21:49 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:21:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: 60.23
[32m[20230207 15:21:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 68.26
[32m[20230207 15:21:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -25.70
[32m[20230207 15:21:50 @agent_ppo2.py:150][0m Total time:      21.14 min
[32m[20230207 15:21:50 @agent_ppo2.py:152][0m 1103872 total steps have happened
[32m[20230207 15:21:50 @agent_ppo2.py:128][0m #------------------------ Iteration 539 --------------------------#
[32m[20230207 15:21:50 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:21:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:50 @agent_ppo2.py:192][0m |          -0.0001 |          24.4793 |           0.1538 |
[32m[20230207 15:21:51 @agent_ppo2.py:192][0m |          -0.0056 |          10.1422 |           0.1537 |
[32m[20230207 15:21:51 @agent_ppo2.py:192][0m |          -0.0082 |           7.6032 |           0.1536 |
[32m[20230207 15:21:51 @agent_ppo2.py:192][0m |          -0.0116 |           6.3570 |           0.1535 |
[32m[20230207 15:21:51 @agent_ppo2.py:192][0m |          -0.0132 |           5.7110 |           0.1534 |
[32m[20230207 15:21:51 @agent_ppo2.py:192][0m |          -0.0138 |           5.1504 |           0.1532 |
[32m[20230207 15:21:51 @agent_ppo2.py:192][0m |          -0.0143 |           4.6554 |           0.1533 |
[32m[20230207 15:21:51 @agent_ppo2.py:192][0m |          -0.0149 |           4.2783 |           0.1532 |
[32m[20230207 15:21:51 @agent_ppo2.py:192][0m |          -0.0167 |           4.0778 |           0.1533 |
[32m[20230207 15:21:51 @agent_ppo2.py:192][0m |          -0.0168 |           3.7608 |           0.1531 |
[32m[20230207 15:21:51 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:21:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: -69.59
[32m[20230207 15:21:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -53.35
[32m[20230207 15:21:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 9.13
[32m[20230207 15:21:52 @agent_ppo2.py:150][0m Total time:      21.18 min
[32m[20230207 15:21:52 @agent_ppo2.py:152][0m 1105920 total steps have happened
[32m[20230207 15:21:52 @agent_ppo2.py:128][0m #------------------------ Iteration 540 --------------------------#
[32m[20230207 15:21:53 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:21:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:53 @agent_ppo2.py:192][0m |          -0.0047 |           1.5945 |           0.1459 |
[32m[20230207 15:21:53 @agent_ppo2.py:192][0m |          -0.0214 |           1.1879 |           0.1456 |
[32m[20230207 15:21:53 @agent_ppo2.py:192][0m |          -0.0293 |           1.0622 |           0.1458 |
[32m[20230207 15:21:53 @agent_ppo2.py:192][0m |           0.0080 |           1.0046 |           0.1457 |
[32m[20230207 15:21:53 @agent_ppo2.py:192][0m |           0.0054 |           0.9592 |           0.1454 |
[32m[20230207 15:21:53 @agent_ppo2.py:192][0m |          -0.0197 |           0.9347 |           0.1454 |
[32m[20230207 15:21:53 @agent_ppo2.py:192][0m |          -0.0015 |           0.9176 |           0.1454 |
[32m[20230207 15:21:53 @agent_ppo2.py:192][0m |          -0.0033 |           0.9054 |           0.1454 |
[32m[20230207 15:21:53 @agent_ppo2.py:192][0m |          -0.0213 |           0.8856 |           0.1454 |
[32m[20230207 15:21:53 @agent_ppo2.py:192][0m |          -0.0209 |           0.8705 |           0.1455 |
[32m[20230207 15:21:53 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:21:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: 46.64
[32m[20230207 15:21:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 52.16
[32m[20230207 15:21:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -43.24
[32m[20230207 15:21:54 @agent_ppo2.py:150][0m Total time:      21.21 min
[32m[20230207 15:21:54 @agent_ppo2.py:152][0m 1107968 total steps have happened
[32m[20230207 15:21:54 @agent_ppo2.py:128][0m #------------------------ Iteration 541 --------------------------#
[32m[20230207 15:21:55 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:21:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:55 @agent_ppo2.py:192][0m |          -0.0106 |           2.3262 |           0.1466 |
[32m[20230207 15:21:55 @agent_ppo2.py:192][0m |          -0.0100 |           1.5445 |           0.1465 |
[32m[20230207 15:21:55 @agent_ppo2.py:192][0m |           0.0129 |           1.4392 |           0.1464 |
[32m[20230207 15:21:55 @agent_ppo2.py:192][0m |           0.0022 |           1.3555 |           0.1463 |
[32m[20230207 15:21:55 @agent_ppo2.py:192][0m |          -0.0239 |           1.3078 |           0.1464 |
[32m[20230207 15:21:55 @agent_ppo2.py:192][0m |          -0.0000 |           1.2633 |           0.1464 |
[32m[20230207 15:21:55 @agent_ppo2.py:192][0m |          -0.0032 |           1.2218 |           0.1464 |
[32m[20230207 15:21:56 @agent_ppo2.py:192][0m |          -0.0018 |           1.1915 |           0.1464 |
[32m[20230207 15:21:56 @agent_ppo2.py:192][0m |          -0.0239 |           1.1641 |           0.1466 |
[32m[20230207 15:21:56 @agent_ppo2.py:192][0m |          -0.0045 |           1.1453 |           0.1465 |
[32m[20230207 15:21:56 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:21:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: 89.17
[32m[20230207 15:21:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 136.29
[32m[20230207 15:21:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 119.68
[32m[20230207 15:21:56 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 119.68
[32m[20230207 15:21:56 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 119.68
[32m[20230207 15:21:56 @agent_ppo2.py:150][0m Total time:      21.25 min
[32m[20230207 15:21:56 @agent_ppo2.py:152][0m 1110016 total steps have happened
[32m[20230207 15:21:56 @agent_ppo2.py:128][0m #------------------------ Iteration 542 --------------------------#
[32m[20230207 15:21:57 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:21:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:21:57 @agent_ppo2.py:192][0m |           0.0009 |           5.2898 |           0.1487 |
[32m[20230207 15:21:58 @agent_ppo2.py:192][0m |          -0.0013 |           2.0684 |           0.1486 |
[32m[20230207 15:21:58 @agent_ppo2.py:192][0m |          -0.0034 |           1.8133 |           0.1485 |
[32m[20230207 15:21:58 @agent_ppo2.py:192][0m |          -0.0048 |           1.7102 |           0.1486 |
[32m[20230207 15:21:58 @agent_ppo2.py:192][0m |          -0.0058 |           1.6393 |           0.1486 |
[32m[20230207 15:21:58 @agent_ppo2.py:192][0m |          -0.0062 |           1.5948 |           0.1485 |
[32m[20230207 15:21:58 @agent_ppo2.py:192][0m |          -0.0076 |           1.5706 |           0.1485 |
[32m[20230207 15:21:58 @agent_ppo2.py:192][0m |          -0.0083 |           1.5255 |           0.1484 |
[32m[20230207 15:21:58 @agent_ppo2.py:192][0m |          -0.0089 |           1.4916 |           0.1484 |
[32m[20230207 15:21:58 @agent_ppo2.py:192][0m |          -0.0094 |           1.4823 |           0.1484 |
[32m[20230207 15:21:58 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:21:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: -19.99
[32m[20230207 15:21:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 35.09
[32m[20230207 15:21:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -12.23
[32m[20230207 15:21:59 @agent_ppo2.py:150][0m Total time:      21.30 min
[32m[20230207 15:21:59 @agent_ppo2.py:152][0m 1112064 total steps have happened
[32m[20230207 15:21:59 @agent_ppo2.py:128][0m #------------------------ Iteration 543 --------------------------#
[32m[20230207 15:22:00 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:22:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:00 @agent_ppo2.py:192][0m |          -0.0008 |           1.8192 |           0.1451 |
[32m[20230207 15:22:00 @agent_ppo2.py:192][0m |           0.0012 |           1.2660 |           0.1450 |
[32m[20230207 15:22:00 @agent_ppo2.py:192][0m |          -0.0033 |           1.1765 |           0.1451 |
[32m[20230207 15:22:00 @agent_ppo2.py:192][0m |          -0.0073 |           1.1423 |           0.1450 |
[32m[20230207 15:22:00 @agent_ppo2.py:192][0m |           0.0085 |           1.1171 |           0.1448 |
[32m[20230207 15:22:00 @agent_ppo2.py:192][0m |           0.0044 |           1.1547 |           0.1448 |
[32m[20230207 15:22:00 @agent_ppo2.py:192][0m |          -0.0066 |           1.0938 |           0.1447 |
[32m[20230207 15:22:00 @agent_ppo2.py:192][0m |          -0.0009 |           1.0610 |           0.1449 |
[32m[20230207 15:22:00 @agent_ppo2.py:192][0m |          -0.0130 |           1.0478 |           0.1449 |
[32m[20230207 15:22:01 @agent_ppo2.py:192][0m |          -0.0307 |           1.0417 |           0.1449 |
[32m[20230207 15:22:01 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:22:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: 43.53
[32m[20230207 15:22:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 77.86
[32m[20230207 15:22:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 64.70
[32m[20230207 15:22:01 @agent_ppo2.py:150][0m Total time:      21.34 min
[32m[20230207 15:22:01 @agent_ppo2.py:152][0m 1114112 total steps have happened
[32m[20230207 15:22:01 @agent_ppo2.py:128][0m #------------------------ Iteration 544 --------------------------#
[32m[20230207 15:22:02 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:22:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:02 @agent_ppo2.py:192][0m |           0.0007 |          12.3659 |           0.1498 |
[32m[20230207 15:22:02 @agent_ppo2.py:192][0m |          -0.0055 |           3.3351 |           0.1497 |
[32m[20230207 15:22:03 @agent_ppo2.py:192][0m |          -0.0058 |           2.5039 |           0.1497 |
[32m[20230207 15:22:03 @agent_ppo2.py:192][0m |          -0.0095 |           2.1742 |           0.1496 |
[32m[20230207 15:22:03 @agent_ppo2.py:192][0m |          -0.0074 |           1.9032 |           0.1494 |
[32m[20230207 15:22:03 @agent_ppo2.py:192][0m |          -0.0050 |           1.6433 |           0.1495 |
[32m[20230207 15:22:03 @agent_ppo2.py:192][0m |          -0.0127 |           1.6520 |           0.1494 |
[32m[20230207 15:22:03 @agent_ppo2.py:192][0m |          -0.0124 |           1.4714 |           0.1495 |
[32m[20230207 15:22:03 @agent_ppo2.py:192][0m |          -0.0147 |           1.3952 |           0.1494 |
[32m[20230207 15:22:03 @agent_ppo2.py:192][0m |          -0.0086 |           1.3526 |           0.1495 |
[32m[20230207 15:22:03 @agent_ppo2.py:137][0m Policy update time: 0.92 s
[32m[20230207 15:22:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: -41.14
[32m[20230207 15:22:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 41.95
[32m[20230207 15:22:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -3.84
[32m[20230207 15:22:04 @agent_ppo2.py:150][0m Total time:      21.38 min
[32m[20230207 15:22:04 @agent_ppo2.py:152][0m 1116160 total steps have happened
[32m[20230207 15:22:04 @agent_ppo2.py:128][0m #------------------------ Iteration 545 --------------------------#
[32m[20230207 15:22:05 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:22:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:05 @agent_ppo2.py:192][0m |           0.0024 |          13.5329 |           0.1468 |
[32m[20230207 15:22:05 @agent_ppo2.py:192][0m |          -0.0053 |           3.3560 |           0.1468 |
[32m[20230207 15:22:05 @agent_ppo2.py:192][0m |          -0.0018 |           2.3355 |           0.1466 |
[32m[20230207 15:22:05 @agent_ppo2.py:192][0m |          -0.0085 |           1.9872 |           0.1463 |
[32m[20230207 15:22:05 @agent_ppo2.py:192][0m |          -0.0090 |           1.7854 |           0.1464 |
[32m[20230207 15:22:05 @agent_ppo2.py:192][0m |          -0.0110 |           1.6268 |           0.1463 |
[32m[20230207 15:22:05 @agent_ppo2.py:192][0m |          -0.0114 |           1.5579 |           0.1461 |
[32m[20230207 15:22:06 @agent_ppo2.py:192][0m |          -0.0135 |           1.4644 |           0.1461 |
[32m[20230207 15:22:06 @agent_ppo2.py:192][0m |          -0.0142 |           1.3797 |           0.1459 |
[32m[20230207 15:22:06 @agent_ppo2.py:192][0m |          -0.0132 |           1.3346 |           0.1459 |
[32m[20230207 15:22:06 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:22:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -41.45
[32m[20230207 15:22:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 52.35
[32m[20230207 15:22:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -43.40
[32m[20230207 15:22:07 @agent_ppo2.py:150][0m Total time:      21.42 min
[32m[20230207 15:22:07 @agent_ppo2.py:152][0m 1118208 total steps have happened
[32m[20230207 15:22:07 @agent_ppo2.py:128][0m #------------------------ Iteration 546 --------------------------#
[32m[20230207 15:22:07 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230207 15:22:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:07 @agent_ppo2.py:192][0m |          -0.0025 |          15.5461 |           0.1411 |
[32m[20230207 15:22:07 @agent_ppo2.py:192][0m |           0.0170 |           7.7654 |           0.1407 |
[32m[20230207 15:22:08 @agent_ppo2.py:192][0m |           0.0076 |           6.6157 |           0.1410 |
[32m[20230207 15:22:08 @agent_ppo2.py:192][0m |           0.0000 |           5.1081 |           0.1409 |
[32m[20230207 15:22:08 @agent_ppo2.py:192][0m |          -0.0070 |           4.4398 |           0.1407 |
[32m[20230207 15:22:08 @agent_ppo2.py:192][0m |          -0.0114 |           4.1905 |           0.1406 |
[32m[20230207 15:22:08 @agent_ppo2.py:192][0m |          -0.0069 |           3.8923 |           0.1407 |
[32m[20230207 15:22:08 @agent_ppo2.py:192][0m |          -0.0056 |           3.7601 |           0.1406 |
[32m[20230207 15:22:08 @agent_ppo2.py:192][0m |          -0.0138 |           3.4223 |           0.1407 |
[32m[20230207 15:22:08 @agent_ppo2.py:192][0m |          -0.0173 |           3.2381 |           0.1406 |
[32m[20230207 15:22:08 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230207 15:22:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: 18.28
[32m[20230207 15:22:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 42.74
[32m[20230207 15:22:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -33.71
[32m[20230207 15:22:09 @agent_ppo2.py:150][0m Total time:      21.46 min
[32m[20230207 15:22:09 @agent_ppo2.py:152][0m 1120256 total steps have happened
[32m[20230207 15:22:09 @agent_ppo2.py:128][0m #------------------------ Iteration 547 --------------------------#
[32m[20230207 15:22:10 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:22:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:10 @agent_ppo2.py:192][0m |          -0.0038 |           6.3121 |           0.1494 |
[32m[20230207 15:22:10 @agent_ppo2.py:192][0m |          -0.0067 |           3.5290 |           0.1492 |
[32m[20230207 15:22:10 @agent_ppo2.py:192][0m |          -0.0044 |           2.9452 |           0.1492 |
[32m[20230207 15:22:10 @agent_ppo2.py:192][0m |          -0.0107 |           2.2765 |           0.1492 |
[32m[20230207 15:22:10 @agent_ppo2.py:192][0m |          -0.0093 |           1.9284 |           0.1493 |
[32m[20230207 15:22:10 @agent_ppo2.py:192][0m |          -0.0104 |           1.7908 |           0.1492 |
[32m[20230207 15:22:10 @agent_ppo2.py:192][0m |          -0.0021 |           1.6797 |           0.1492 |
[32m[20230207 15:22:10 @agent_ppo2.py:192][0m |          -0.0115 |           1.5697 |           0.1492 |
[32m[20230207 15:22:10 @agent_ppo2.py:192][0m |          -0.0036 |           1.4738 |           0.1492 |
[32m[20230207 15:22:11 @agent_ppo2.py:192][0m |          -0.0126 |           1.4647 |           0.1492 |
[32m[20230207 15:22:11 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:22:11 @agent_ppo2.py:145][0m Average TRAINING episode reward: 4.14
[32m[20230207 15:22:11 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 110.64
[32m[20230207 15:22:11 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 48.57
[32m[20230207 15:22:11 @agent_ppo2.py:150][0m Total time:      21.50 min
[32m[20230207 15:22:11 @agent_ppo2.py:152][0m 1122304 total steps have happened
[32m[20230207 15:22:11 @agent_ppo2.py:128][0m #------------------------ Iteration 548 --------------------------#
[32m[20230207 15:22:12 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:22:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:12 @agent_ppo2.py:192][0m |          -0.0175 |           1.6650 |           0.1464 |
[32m[20230207 15:22:12 @agent_ppo2.py:192][0m |          -0.0080 |           1.2380 |           0.1459 |
[32m[20230207 15:22:12 @agent_ppo2.py:192][0m |           0.0080 |           1.1534 |           0.1462 |
[32m[20230207 15:22:12 @agent_ppo2.py:192][0m |           0.0136 |           1.0944 |           0.1463 |
[32m[20230207 15:22:13 @agent_ppo2.py:192][0m |           0.0004 |           1.0694 |           0.1462 |
[32m[20230207 15:22:13 @agent_ppo2.py:192][0m |          -0.0087 |           1.0157 |           0.1462 |
[32m[20230207 15:22:13 @agent_ppo2.py:192][0m |          -0.0003 |           0.9872 |           0.1462 |
[32m[20230207 15:22:13 @agent_ppo2.py:192][0m |          -0.0208 |           0.9767 |           0.1462 |
[32m[20230207 15:22:13 @agent_ppo2.py:192][0m |          -0.0264 |           0.9417 |           0.1462 |
[32m[20230207 15:22:13 @agent_ppo2.py:192][0m |          -0.0179 |           0.9294 |           0.1464 |
[32m[20230207 15:22:13 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:22:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: 51.83
[32m[20230207 15:22:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 84.61
[32m[20230207 15:22:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -39.15
[32m[20230207 15:22:14 @agent_ppo2.py:150][0m Total time:      21.54 min
[32m[20230207 15:22:14 @agent_ppo2.py:152][0m 1124352 total steps have happened
[32m[20230207 15:22:14 @agent_ppo2.py:128][0m #------------------------ Iteration 549 --------------------------#
[32m[20230207 15:22:15 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:22:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:15 @agent_ppo2.py:192][0m |           0.0113 |           1.2112 |           0.1444 |
[32m[20230207 15:22:15 @agent_ppo2.py:192][0m |          -0.0636 |           1.0457 |           0.1443 |
[32m[20230207 15:22:15 @agent_ppo2.py:192][0m |          -0.0044 |           1.0184 |           0.1443 |
[32m[20230207 15:22:15 @agent_ppo2.py:192][0m |          -0.0192 |           0.9845 |           0.1443 |
[32m[20230207 15:22:15 @agent_ppo2.py:192][0m |          -0.0163 |           0.9649 |           0.1443 |
[32m[20230207 15:22:15 @agent_ppo2.py:192][0m |          -0.0064 |           0.9508 |           0.1443 |
[32m[20230207 15:22:15 @agent_ppo2.py:192][0m |          -0.0070 |           0.9403 |           0.1443 |
[32m[20230207 15:22:15 @agent_ppo2.py:192][0m |          -0.0085 |           0.9266 |           0.1441 |
[32m[20230207 15:22:15 @agent_ppo2.py:192][0m |          -0.0196 |           0.9227 |           0.1444 |
[32m[20230207 15:22:15 @agent_ppo2.py:192][0m |           0.0072 |           0.9167 |           0.1444 |
[32m[20230207 15:22:15 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:22:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -5.55
[32m[20230207 15:22:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 18.24
[32m[20230207 15:22:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -13.90
[32m[20230207 15:22:16 @agent_ppo2.py:150][0m Total time:      21.58 min
[32m[20230207 15:22:16 @agent_ppo2.py:152][0m 1126400 total steps have happened
[32m[20230207 15:22:16 @agent_ppo2.py:128][0m #------------------------ Iteration 550 --------------------------#
[32m[20230207 15:22:17 @agent_ppo2.py:134][0m Sampling time: 0.75 s by 1 slaves
[32m[20230207 15:22:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:17 @agent_ppo2.py:192][0m |           0.0016 |           8.1532 |           0.1475 |
[32m[20230207 15:22:17 @agent_ppo2.py:192][0m |          -0.0024 |           4.8721 |           0.1476 |
[32m[20230207 15:22:17 @agent_ppo2.py:192][0m |          -0.0049 |           4.4356 |           0.1476 |
[32m[20230207 15:22:17 @agent_ppo2.py:192][0m |          -0.0018 |           4.0877 |           0.1478 |
[32m[20230207 15:22:17 @agent_ppo2.py:192][0m |          -0.0084 |           3.8700 |           0.1477 |
[32m[20230207 15:22:17 @agent_ppo2.py:192][0m |          -0.0086 |           3.5202 |           0.1475 |
[32m[20230207 15:22:18 @agent_ppo2.py:192][0m |          -0.0088 |           3.1766 |           0.1477 |
[32m[20230207 15:22:18 @agent_ppo2.py:192][0m |          -0.0030 |           2.9684 |           0.1477 |
[32m[20230207 15:22:18 @agent_ppo2.py:192][0m |          -0.0097 |           2.7698 |           0.1476 |
[32m[20230207 15:22:18 @agent_ppo2.py:192][0m |          -0.0129 |           2.5818 |           0.1475 |
[32m[20230207 15:22:18 @agent_ppo2.py:137][0m Policy update time: 0.79 s
[32m[20230207 15:22:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: 24.23
[32m[20230207 15:22:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 66.86
[32m[20230207 15:22:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -73.74
[32m[20230207 15:22:18 @agent_ppo2.py:150][0m Total time:      21.62 min
[32m[20230207 15:22:18 @agent_ppo2.py:152][0m 1128448 total steps have happened
[32m[20230207 15:22:18 @agent_ppo2.py:128][0m #------------------------ Iteration 551 --------------------------#
[32m[20230207 15:22:19 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:22:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:19 @agent_ppo2.py:192][0m |           0.0035 |           5.3211 |           0.1486 |
[32m[20230207 15:22:19 @agent_ppo2.py:192][0m |          -0.0062 |           1.6484 |           0.1484 |
[32m[20230207 15:22:20 @agent_ppo2.py:192][0m |          -0.0066 |           1.3008 |           0.1482 |
[32m[20230207 15:22:20 @agent_ppo2.py:192][0m |          -0.0089 |           1.1998 |           0.1483 |
[32m[20230207 15:22:20 @agent_ppo2.py:192][0m |          -0.0099 |           1.0943 |           0.1482 |
[32m[20230207 15:22:20 @agent_ppo2.py:192][0m |          -0.0117 |           1.0683 |           0.1481 |
[32m[20230207 15:22:20 @agent_ppo2.py:192][0m |          -0.0127 |           1.0142 |           0.1481 |
[32m[20230207 15:22:20 @agent_ppo2.py:192][0m |          -0.0085 |           0.9854 |           0.1481 |
[32m[20230207 15:22:20 @agent_ppo2.py:192][0m |          -0.0126 |           0.9545 |           0.1480 |
[32m[20230207 15:22:20 @agent_ppo2.py:192][0m |          -0.0137 |           0.9338 |           0.1481 |
[32m[20230207 15:22:20 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:22:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -11.41
[32m[20230207 15:22:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 51.10
[32m[20230207 15:22:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -18.66
[32m[20230207 15:22:21 @agent_ppo2.py:150][0m Total time:      21.66 min
[32m[20230207 15:22:21 @agent_ppo2.py:152][0m 1130496 total steps have happened
[32m[20230207 15:22:21 @agent_ppo2.py:128][0m #------------------------ Iteration 552 --------------------------#
[32m[20230207 15:22:22 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:22:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:22 @agent_ppo2.py:192][0m |           0.0035 |           1.7533 |           0.1441 |
[32m[20230207 15:22:22 @agent_ppo2.py:192][0m |          -0.0071 |           1.4241 |           0.1441 |
[32m[20230207 15:22:22 @agent_ppo2.py:192][0m |          -0.0059 |           1.3760 |           0.1442 |
[32m[20230207 15:22:22 @agent_ppo2.py:192][0m |           0.0456 |           1.3390 |           0.1442 |
[32m[20230207 15:22:22 @agent_ppo2.py:192][0m |          -0.0195 |           1.3285 |           0.1439 |
[32m[20230207 15:22:22 @agent_ppo2.py:192][0m |           0.0008 |           1.3014 |           0.1442 |
[32m[20230207 15:22:22 @agent_ppo2.py:192][0m |          -0.0218 |           1.2997 |           0.1444 |
[32m[20230207 15:22:22 @agent_ppo2.py:192][0m |           0.0073 |           1.3040 |           0.1445 |
[32m[20230207 15:22:23 @agent_ppo2.py:192][0m |          -0.0033 |           1.2853 |           0.1447 |
[32m[20230207 15:22:23 @agent_ppo2.py:192][0m |          -0.0130 |           1.2581 |           0.1447 |
[32m[20230207 15:22:23 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:22:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: 59.78
[32m[20230207 15:22:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 71.78
[32m[20230207 15:22:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 18.10
[32m[20230207 15:22:23 @agent_ppo2.py:150][0m Total time:      21.70 min
[32m[20230207 15:22:23 @agent_ppo2.py:152][0m 1132544 total steps have happened
[32m[20230207 15:22:23 @agent_ppo2.py:128][0m #------------------------ Iteration 553 --------------------------#
[32m[20230207 15:22:24 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:22:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:24 @agent_ppo2.py:192][0m |           0.0017 |          10.3087 |           0.1500 |
[32m[20230207 15:22:24 @agent_ppo2.py:192][0m |          -0.0053 |           4.9110 |           0.1499 |
[32m[20230207 15:22:24 @agent_ppo2.py:192][0m |          -0.0073 |           3.8465 |           0.1498 |
[32m[20230207 15:22:24 @agent_ppo2.py:192][0m |          -0.0088 |           3.4476 |           0.1497 |
[32m[20230207 15:22:24 @agent_ppo2.py:192][0m |          -0.0082 |           3.1265 |           0.1496 |
[32m[20230207 15:22:24 @agent_ppo2.py:192][0m |          -0.0100 |           2.7762 |           0.1494 |
[32m[20230207 15:22:24 @agent_ppo2.py:192][0m |          -0.0071 |           2.5809 |           0.1495 |
[32m[20230207 15:22:24 @agent_ppo2.py:192][0m |          -0.0130 |           2.4041 |           0.1494 |
[32m[20230207 15:22:25 @agent_ppo2.py:192][0m |          -0.0073 |           2.3084 |           0.1493 |
[32m[20230207 15:22:25 @agent_ppo2.py:192][0m |          -0.0149 |           2.2557 |           0.1494 |
[32m[20230207 15:22:25 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:22:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -65.09
[32m[20230207 15:22:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -12.56
[32m[20230207 15:22:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -37.51
[32m[20230207 15:22:25 @agent_ppo2.py:150][0m Total time:      21.74 min
[32m[20230207 15:22:25 @agent_ppo2.py:152][0m 1134592 total steps have happened
[32m[20230207 15:22:25 @agent_ppo2.py:128][0m #------------------------ Iteration 554 --------------------------#
[32m[20230207 15:22:26 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:22:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:26 @agent_ppo2.py:192][0m |          -0.0030 |           5.2771 |           0.1494 |
[32m[20230207 15:22:26 @agent_ppo2.py:192][0m |          -0.0251 |           2.8065 |           0.1491 |
[32m[20230207 15:22:27 @agent_ppo2.py:192][0m |          -0.0045 |           2.5678 |           0.1489 |
[32m[20230207 15:22:27 @agent_ppo2.py:192][0m |          -0.0002 |           2.3765 |           0.1489 |
[32m[20230207 15:22:27 @agent_ppo2.py:192][0m |          -0.0059 |           2.2908 |           0.1488 |
[32m[20230207 15:22:27 @agent_ppo2.py:192][0m |          -0.0058 |           2.1871 |           0.1488 |
[32m[20230207 15:22:27 @agent_ppo2.py:192][0m |          -0.0074 |           2.1417 |           0.1487 |
[32m[20230207 15:22:27 @agent_ppo2.py:192][0m |           0.0151 |           2.2803 |           0.1488 |
[32m[20230207 15:22:27 @agent_ppo2.py:192][0m |          -0.0338 |           2.0821 |           0.1487 |
[32m[20230207 15:22:27 @agent_ppo2.py:192][0m |          -0.0059 |           1.9907 |           0.1485 |
[32m[20230207 15:22:27 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:22:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: 68.22
[32m[20230207 15:22:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 85.21
[32m[20230207 15:22:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 3.02
[32m[20230207 15:22:28 @agent_ppo2.py:150][0m Total time:      21.78 min
[32m[20230207 15:22:28 @agent_ppo2.py:152][0m 1136640 total steps have happened
[32m[20230207 15:22:28 @agent_ppo2.py:128][0m #------------------------ Iteration 555 --------------------------#
[32m[20230207 15:22:28 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 15:22:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:29 @agent_ppo2.py:192][0m |           0.0025 |          23.6138 |           0.1504 |
[32m[20230207 15:22:29 @agent_ppo2.py:192][0m |          -0.0021 |           6.5086 |           0.1499 |
[32m[20230207 15:22:29 @agent_ppo2.py:192][0m |          -0.0088 |           4.6085 |           0.1500 |
[32m[20230207 15:22:29 @agent_ppo2.py:192][0m |          -0.0078 |           3.9715 |           0.1499 |
[32m[20230207 15:22:29 @agent_ppo2.py:192][0m |          -0.0090 |           3.5781 |           0.1496 |
[32m[20230207 15:22:29 @agent_ppo2.py:192][0m |          -0.0108 |           3.2632 |           0.1498 |
[32m[20230207 15:22:29 @agent_ppo2.py:192][0m |          -0.0077 |           3.1340 |           0.1497 |
[32m[20230207 15:22:29 @agent_ppo2.py:192][0m |          -0.0129 |           2.8809 |           0.1496 |
[32m[20230207 15:22:29 @agent_ppo2.py:192][0m |          -0.0160 |           2.8406 |           0.1497 |
[32m[20230207 15:22:29 @agent_ppo2.py:192][0m |          -0.0138 |           2.6697 |           0.1495 |
[32m[20230207 15:22:29 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:22:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -72.08
[32m[20230207 15:22:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 5.35
[32m[20230207 15:22:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -3.87
[32m[20230207 15:22:30 @agent_ppo2.py:150][0m Total time:      21.81 min
[32m[20230207 15:22:30 @agent_ppo2.py:152][0m 1138688 total steps have happened
[32m[20230207 15:22:30 @agent_ppo2.py:128][0m #------------------------ Iteration 556 --------------------------#
[32m[20230207 15:22:30 @agent_ppo2.py:134][0m Sampling time: 0.73 s by 1 slaves
[32m[20230207 15:22:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:30 @agent_ppo2.py:192][0m |          -0.0007 |          28.7899 |           0.1494 |
[32m[20230207 15:22:30 @agent_ppo2.py:192][0m |          -0.0048 |          16.1383 |           0.1490 |
[32m[20230207 15:22:31 @agent_ppo2.py:192][0m |          -0.0064 |          13.2803 |           0.1489 |
[32m[20230207 15:22:31 @agent_ppo2.py:192][0m |          -0.0082 |          11.9945 |           0.1488 |
[32m[20230207 15:22:31 @agent_ppo2.py:192][0m |          -0.0091 |          11.0511 |           0.1486 |
[32m[20230207 15:22:31 @agent_ppo2.py:192][0m |          -0.0091 |          10.2388 |           0.1484 |
[32m[20230207 15:22:31 @agent_ppo2.py:192][0m |          -0.0108 |           9.6118 |           0.1483 |
[32m[20230207 15:22:31 @agent_ppo2.py:192][0m |          -0.0118 |           8.9909 |           0.1483 |
[32m[20230207 15:22:31 @agent_ppo2.py:192][0m |          -0.0121 |           8.4400 |           0.1482 |
[32m[20230207 15:22:31 @agent_ppo2.py:192][0m |          -0.0132 |           8.0355 |           0.1481 |
[32m[20230207 15:22:31 @agent_ppo2.py:137][0m Policy update time: 0.76 s
[32m[20230207 15:22:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: 48.08
[32m[20230207 15:22:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 122.16
[32m[20230207 15:22:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -7.08
[32m[20230207 15:22:32 @agent_ppo2.py:150][0m Total time:      21.84 min
[32m[20230207 15:22:32 @agent_ppo2.py:152][0m 1140736 total steps have happened
[32m[20230207 15:22:32 @agent_ppo2.py:128][0m #------------------------ Iteration 557 --------------------------#
[32m[20230207 15:22:33 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:22:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:33 @agent_ppo2.py:192][0m |           0.0003 |           8.7917 |           0.1545 |
[32m[20230207 15:22:33 @agent_ppo2.py:192][0m |          -0.0064 |           5.3229 |           0.1544 |
[32m[20230207 15:22:33 @agent_ppo2.py:192][0m |          -0.0082 |           4.3157 |           0.1545 |
[32m[20230207 15:22:33 @agent_ppo2.py:192][0m |          -0.0061 |           3.8016 |           0.1544 |
[32m[20230207 15:22:33 @agent_ppo2.py:192][0m |          -0.0102 |           3.3876 |           0.1544 |
[32m[20230207 15:22:33 @agent_ppo2.py:192][0m |          -0.0115 |           2.8943 |           0.1544 |
[32m[20230207 15:22:33 @agent_ppo2.py:192][0m |          -0.0115 |           2.6360 |           0.1544 |
[32m[20230207 15:22:33 @agent_ppo2.py:192][0m |          -0.0123 |           2.4371 |           0.1544 |
[32m[20230207 15:22:33 @agent_ppo2.py:192][0m |          -0.0137 |           2.3210 |           0.1544 |
[32m[20230207 15:22:33 @agent_ppo2.py:192][0m |          -0.0127 |           2.1248 |           0.1544 |
[32m[20230207 15:22:33 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:22:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: 30.05
[32m[20230207 15:22:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 135.12
[32m[20230207 15:22:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -13.56
[32m[20230207 15:22:34 @agent_ppo2.py:150][0m Total time:      21.88 min
[32m[20230207 15:22:34 @agent_ppo2.py:152][0m 1142784 total steps have happened
[32m[20230207 15:22:34 @agent_ppo2.py:128][0m #------------------------ Iteration 558 --------------------------#
[32m[20230207 15:22:35 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:22:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:35 @agent_ppo2.py:192][0m |           0.0008 |           7.5171 |           0.1507 |
[32m[20230207 15:22:35 @agent_ppo2.py:192][0m |          -0.0071 |           2.9187 |           0.1505 |
[32m[20230207 15:22:35 @agent_ppo2.py:192][0m |          -0.0019 |           2.2565 |           0.1503 |
[32m[20230207 15:22:36 @agent_ppo2.py:192][0m |          -0.0093 |           1.8249 |           0.1503 |
[32m[20230207 15:22:36 @agent_ppo2.py:192][0m |          -0.0105 |           1.6122 |           0.1502 |
[32m[20230207 15:22:36 @agent_ppo2.py:192][0m |          -0.0156 |           1.4950 |           0.1501 |
[32m[20230207 15:22:36 @agent_ppo2.py:192][0m |          -0.0130 |           1.4009 |           0.1500 |
[32m[20230207 15:22:36 @agent_ppo2.py:192][0m |           0.0321 |           1.3180 |           0.1500 |
[32m[20230207 15:22:36 @agent_ppo2.py:192][0m |          -0.0121 |           1.2658 |           0.1497 |
[32m[20230207 15:22:36 @agent_ppo2.py:192][0m |          -0.0175 |           1.2176 |           0.1496 |
[32m[20230207 15:22:36 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:22:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: -24.93
[32m[20230207 15:22:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 108.32
[32m[20230207 15:22:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 77.31
[32m[20230207 15:22:37 @agent_ppo2.py:150][0m Total time:      21.93 min
[32m[20230207 15:22:37 @agent_ppo2.py:152][0m 1144832 total steps have happened
[32m[20230207 15:22:37 @agent_ppo2.py:128][0m #------------------------ Iteration 559 --------------------------#
[32m[20230207 15:22:38 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:22:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:38 @agent_ppo2.py:192][0m |           0.0008 |           5.2683 |           0.1479 |
[32m[20230207 15:22:38 @agent_ppo2.py:192][0m |           0.0007 |           2.3043 |           0.1478 |
[32m[20230207 15:22:38 @agent_ppo2.py:192][0m |          -0.0000 |           1.9741 |           0.1477 |
[32m[20230207 15:22:38 @agent_ppo2.py:192][0m |          -0.0055 |           1.8064 |           0.1476 |
[32m[20230207 15:22:38 @agent_ppo2.py:192][0m |          -0.0079 |           1.7156 |           0.1476 |
[32m[20230207 15:22:38 @agent_ppo2.py:192][0m |          -0.0069 |           1.6247 |           0.1475 |
[32m[20230207 15:22:38 @agent_ppo2.py:192][0m |          -0.0086 |           1.5428 |           0.1474 |
[32m[20230207 15:22:38 @agent_ppo2.py:192][0m |          -0.0089 |           1.4978 |           0.1474 |
[32m[20230207 15:22:38 @agent_ppo2.py:192][0m |          -0.0132 |           1.4536 |           0.1474 |
[32m[20230207 15:22:38 @agent_ppo2.py:192][0m |          -0.0136 |           1.3995 |           0.1474 |
[32m[20230207 15:22:38 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:22:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: 12.84
[32m[20230207 15:22:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 93.06
[32m[20230207 15:22:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -5.81
[32m[20230207 15:22:39 @agent_ppo2.py:150][0m Total time:      21.97 min
[32m[20230207 15:22:39 @agent_ppo2.py:152][0m 1146880 total steps have happened
[32m[20230207 15:22:39 @agent_ppo2.py:128][0m #------------------------ Iteration 560 --------------------------#
[32m[20230207 15:22:40 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:22:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:40 @agent_ppo2.py:192][0m |           0.0006 |          26.6083 |           0.1531 |
[32m[20230207 15:22:40 @agent_ppo2.py:192][0m |          -0.0048 |          10.8568 |           0.1528 |
[32m[20230207 15:22:40 @agent_ppo2.py:192][0m |          -0.0066 |           9.1723 |           0.1530 |
[32m[20230207 15:22:40 @agent_ppo2.py:192][0m |          -0.0082 |           8.1907 |           0.1528 |
[32m[20230207 15:22:41 @agent_ppo2.py:192][0m |          -0.0097 |           7.6975 |           0.1526 |
[32m[20230207 15:22:41 @agent_ppo2.py:192][0m |          -0.0105 |           7.4295 |           0.1527 |
[32m[20230207 15:22:41 @agent_ppo2.py:192][0m |          -0.0114 |           6.7794 |           0.1528 |
[32m[20230207 15:22:41 @agent_ppo2.py:192][0m |          -0.0117 |           6.4550 |           0.1527 |
[32m[20230207 15:22:41 @agent_ppo2.py:192][0m |          -0.0118 |           6.1130 |           0.1527 |
[32m[20230207 15:22:41 @agent_ppo2.py:192][0m |          -0.0125 |           5.7270 |           0.1526 |
[32m[20230207 15:22:41 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:22:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: -21.87
[32m[20230207 15:22:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 23.26
[32m[20230207 15:22:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 93.17
[32m[20230207 15:22:42 @agent_ppo2.py:150][0m Total time:      22.01 min
[32m[20230207 15:22:42 @agent_ppo2.py:152][0m 1148928 total steps have happened
[32m[20230207 15:22:42 @agent_ppo2.py:128][0m #------------------------ Iteration 561 --------------------------#
[32m[20230207 15:22:43 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:22:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:43 @agent_ppo2.py:192][0m |          -0.0012 |          13.2695 |           0.1513 |
[32m[20230207 15:22:43 @agent_ppo2.py:192][0m |          -0.0016 |           7.8411 |           0.1511 |
[32m[20230207 15:22:43 @agent_ppo2.py:192][0m |          -0.0067 |           6.7097 |           0.1511 |
[32m[20230207 15:22:43 @agent_ppo2.py:192][0m |          -0.0094 |           5.9266 |           0.1511 |
[32m[20230207 15:22:43 @agent_ppo2.py:192][0m |          -0.0110 |           5.1919 |           0.1510 |
[32m[20230207 15:22:43 @agent_ppo2.py:192][0m |          -0.0091 |           4.8724 |           0.1507 |
[32m[20230207 15:22:43 @agent_ppo2.py:192][0m |          -0.0093 |           4.6631 |           0.1508 |
[32m[20230207 15:22:43 @agent_ppo2.py:192][0m |          -0.0100 |           4.4320 |           0.1508 |
[32m[20230207 15:22:43 @agent_ppo2.py:192][0m |          -0.0137 |           4.2900 |           0.1508 |
[32m[20230207 15:22:43 @agent_ppo2.py:192][0m |          -0.0114 |           4.0197 |           0.1507 |
[32m[20230207 15:22:43 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:22:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: -9.18
[32m[20230207 15:22:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 73.84
[32m[20230207 15:22:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -67.03
[32m[20230207 15:22:44 @agent_ppo2.py:150][0m Total time:      22.05 min
[32m[20230207 15:22:44 @agent_ppo2.py:152][0m 1150976 total steps have happened
[32m[20230207 15:22:44 @agent_ppo2.py:128][0m #------------------------ Iteration 562 --------------------------#
[32m[20230207 15:22:45 @agent_ppo2.py:134][0m Sampling time: 0.68 s by 1 slaves
[32m[20230207 15:22:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:45 @agent_ppo2.py:192][0m |           0.0076 |          17.4610 |           0.1464 |
[32m[20230207 15:22:45 @agent_ppo2.py:192][0m |          -0.0049 |           8.5891 |           0.1460 |
[32m[20230207 15:22:45 @agent_ppo2.py:192][0m |          -0.0094 |           7.2663 |           0.1461 |
[32m[20230207 15:22:45 @agent_ppo2.py:192][0m |          -0.0071 |           6.5901 |           0.1461 |
[32m[20230207 15:22:45 @agent_ppo2.py:192][0m |          -0.0092 |           6.1455 |           0.1461 |
[32m[20230207 15:22:45 @agent_ppo2.py:192][0m |          -0.0111 |           5.6566 |           0.1461 |
[32m[20230207 15:22:45 @agent_ppo2.py:192][0m |          -0.0037 |           5.5414 |           0.1460 |
[32m[20230207 15:22:45 @agent_ppo2.py:192][0m |          -0.0178 |           5.1799 |           0.1462 |
[32m[20230207 15:22:46 @agent_ppo2.py:192][0m |          -0.0157 |           5.0308 |           0.1460 |
[32m[20230207 15:22:46 @agent_ppo2.py:192][0m |          -0.0114 |           4.6780 |           0.1460 |
[32m[20230207 15:22:46 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230207 15:22:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -56.07
[32m[20230207 15:22:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -21.64
[32m[20230207 15:22:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 45.30
[32m[20230207 15:22:46 @agent_ppo2.py:150][0m Total time:      22.09 min
[32m[20230207 15:22:46 @agent_ppo2.py:152][0m 1153024 total steps have happened
[32m[20230207 15:22:46 @agent_ppo2.py:128][0m #------------------------ Iteration 563 --------------------------#
[32m[20230207 15:22:47 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:22:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:47 @agent_ppo2.py:192][0m |          -0.0268 |           1.6062 |           0.1472 |
[32m[20230207 15:22:47 @agent_ppo2.py:192][0m |          -0.0135 |           1.2624 |           0.1467 |
[32m[20230207 15:22:47 @agent_ppo2.py:192][0m |           0.0026 |           1.1641 |           0.1462 |
[32m[20230207 15:22:48 @agent_ppo2.py:192][0m |           0.0059 |           1.1207 |           0.1466 |
[32m[20230207 15:22:48 @agent_ppo2.py:192][0m |          -0.0008 |           1.1309 |           0.1465 |
[32m[20230207 15:22:48 @agent_ppo2.py:192][0m |          -0.0233 |           1.0819 |           0.1465 |
[32m[20230207 15:22:48 @agent_ppo2.py:192][0m |          -0.0151 |           1.0600 |           0.1466 |
[32m[20230207 15:22:48 @agent_ppo2.py:192][0m |          -0.0071 |           1.0432 |           0.1467 |
[32m[20230207 15:22:48 @agent_ppo2.py:192][0m |          -0.0175 |           1.0371 |           0.1465 |
[32m[20230207 15:22:48 @agent_ppo2.py:192][0m |          -0.0086 |           1.0223 |           0.1467 |
[32m[20230207 15:22:48 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:22:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: 0.02
[32m[20230207 15:22:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 62.80
[32m[20230207 15:22:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 63.95
[32m[20230207 15:22:49 @agent_ppo2.py:150][0m Total time:      22.13 min
[32m[20230207 15:22:49 @agent_ppo2.py:152][0m 1155072 total steps have happened
[32m[20230207 15:22:49 @agent_ppo2.py:128][0m #------------------------ Iteration 564 --------------------------#
[32m[20230207 15:22:50 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:22:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:50 @agent_ppo2.py:192][0m |           0.0106 |           1.9206 |           0.1519 |
[32m[20230207 15:22:50 @agent_ppo2.py:192][0m |           0.0024 |           1.4411 |           0.1518 |
[32m[20230207 15:22:50 @agent_ppo2.py:192][0m |          -0.0134 |           1.3591 |           0.1516 |
[32m[20230207 15:22:50 @agent_ppo2.py:192][0m |           0.0020 |           1.3181 |           0.1515 |
[32m[20230207 15:22:50 @agent_ppo2.py:192][0m |           0.0015 |           1.2833 |           0.1515 |
[32m[20230207 15:22:50 @agent_ppo2.py:192][0m |          -0.0055 |           1.2570 |           0.1513 |
[32m[20230207 15:22:50 @agent_ppo2.py:192][0m |          -0.0261 |           1.2292 |           0.1513 |
[32m[20230207 15:22:50 @agent_ppo2.py:192][0m |          -0.0086 |           1.2139 |           0.1512 |
[32m[20230207 15:22:50 @agent_ppo2.py:192][0m |          -0.0094 |           1.1891 |           0.1511 |
[32m[20230207 15:22:50 @agent_ppo2.py:192][0m |          -0.0162 |           1.1905 |           0.1511 |
[32m[20230207 15:22:50 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:22:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: 58.37
[32m[20230207 15:22:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 63.81
[32m[20230207 15:22:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 36.84
[32m[20230207 15:22:51 @agent_ppo2.py:150][0m Total time:      22.17 min
[32m[20230207 15:22:51 @agent_ppo2.py:152][0m 1157120 total steps have happened
[32m[20230207 15:22:51 @agent_ppo2.py:128][0m #------------------------ Iteration 565 --------------------------#
[32m[20230207 15:22:52 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:22:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:52 @agent_ppo2.py:192][0m |           0.0220 |           2.5603 |           0.1481 |
[32m[20230207 15:22:52 @agent_ppo2.py:192][0m |           0.0047 |           1.7983 |           0.1474 |
[32m[20230207 15:22:52 @agent_ppo2.py:192][0m |           0.0054 |           1.6382 |           0.1480 |
[32m[20230207 15:22:52 @agent_ppo2.py:192][0m |           0.0109 |           1.5457 |           0.1480 |
[32m[20230207 15:22:52 @agent_ppo2.py:192][0m |          -0.0161 |           1.4695 |           0.1479 |
[32m[20230207 15:22:52 @agent_ppo2.py:192][0m |          -0.0250 |           1.4150 |           0.1478 |
[32m[20230207 15:22:53 @agent_ppo2.py:192][0m |          -0.0102 |           1.3881 |           0.1478 |
[32m[20230207 15:22:53 @agent_ppo2.py:192][0m |          -0.0171 |           1.3575 |           0.1478 |
[32m[20230207 15:22:53 @agent_ppo2.py:192][0m |          -0.0133 |           1.3060 |           0.1476 |
[32m[20230207 15:22:53 @agent_ppo2.py:192][0m |          -0.0365 |           1.2846 |           0.1477 |
[32m[20230207 15:22:53 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:22:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: 60.68
[32m[20230207 15:22:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 117.58
[32m[20230207 15:22:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -69.74
[32m[20230207 15:22:53 @agent_ppo2.py:150][0m Total time:      22.20 min
[32m[20230207 15:22:53 @agent_ppo2.py:152][0m 1159168 total steps have happened
[32m[20230207 15:22:53 @agent_ppo2.py:128][0m #------------------------ Iteration 566 --------------------------#
[32m[20230207 15:22:54 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:22:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:54 @agent_ppo2.py:192][0m |           0.0052 |           9.6935 |           0.1563 |
[32m[20230207 15:22:54 @agent_ppo2.py:192][0m |          -0.0044 |           4.0361 |           0.1563 |
[32m[20230207 15:22:54 @agent_ppo2.py:192][0m |          -0.0071 |           2.9905 |           0.1560 |
[32m[20230207 15:22:54 @agent_ppo2.py:192][0m |          -0.0090 |           2.5804 |           0.1558 |
[32m[20230207 15:22:54 @agent_ppo2.py:192][0m |          -0.0073 |           2.3793 |           0.1559 |
[32m[20230207 15:22:54 @agent_ppo2.py:192][0m |          -0.0116 |           2.2261 |           0.1559 |
[32m[20230207 15:22:54 @agent_ppo2.py:192][0m |          -0.0121 |           2.2902 |           0.1558 |
[32m[20230207 15:22:54 @agent_ppo2.py:192][0m |          -0.0110 |           2.0303 |           0.1557 |
[32m[20230207 15:22:54 @agent_ppo2.py:192][0m |          -0.0131 |           1.9130 |           0.1556 |
[32m[20230207 15:22:55 @agent_ppo2.py:192][0m |          -0.0146 |           1.8188 |           0.1556 |
[32m[20230207 15:22:55 @agent_ppo2.py:137][0m Policy update time: 0.58 s
[32m[20230207 15:22:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: -17.72
[32m[20230207 15:22:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 44.59
[32m[20230207 15:22:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 50.73
[32m[20230207 15:22:55 @agent_ppo2.py:150][0m Total time:      22.24 min
[32m[20230207 15:22:55 @agent_ppo2.py:152][0m 1161216 total steps have happened
[32m[20230207 15:22:55 @agent_ppo2.py:128][0m #------------------------ Iteration 567 --------------------------#
[32m[20230207 15:22:56 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:22:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:56 @agent_ppo2.py:192][0m |          -0.0109 |           1.8759 |           0.1505 |
[32m[20230207 15:22:56 @agent_ppo2.py:192][0m |           0.0019 |           1.4452 |           0.1502 |
[32m[20230207 15:22:56 @agent_ppo2.py:192][0m |           0.0007 |           1.2592 |           0.1499 |
[32m[20230207 15:22:56 @agent_ppo2.py:192][0m |          -0.0295 |           1.2492 |           0.1500 |
[32m[20230207 15:22:57 @agent_ppo2.py:192][0m |          -0.0200 |           1.1513 |           0.1497 |
[32m[20230207 15:22:57 @agent_ppo2.py:192][0m |           0.0030 |           1.0668 |           0.1495 |
[32m[20230207 15:22:57 @agent_ppo2.py:192][0m |          -0.0083 |           1.0321 |           0.1498 |
[32m[20230207 15:22:57 @agent_ppo2.py:192][0m |          -0.0205 |           1.0028 |           0.1497 |
[32m[20230207 15:22:57 @agent_ppo2.py:192][0m |          -0.0065 |           0.9829 |           0.1498 |
[32m[20230207 15:22:57 @agent_ppo2.py:192][0m |          -0.0304 |           0.9609 |           0.1498 |
[32m[20230207 15:22:57 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:22:57 @agent_ppo2.py:145][0m Average TRAINING episode reward: -27.55
[32m[20230207 15:22:57 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -17.14
[32m[20230207 15:22:57 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -60.42
[32m[20230207 15:22:57 @agent_ppo2.py:150][0m Total time:      22.27 min
[32m[20230207 15:22:57 @agent_ppo2.py:152][0m 1163264 total steps have happened
[32m[20230207 15:22:57 @agent_ppo2.py:128][0m #------------------------ Iteration 568 --------------------------#
[32m[20230207 15:22:58 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:22:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:22:58 @agent_ppo2.py:192][0m |          -0.0254 |           1.2142 |           0.1478 |
[32m[20230207 15:22:58 @agent_ppo2.py:192][0m |           0.0024 |           0.9981 |           0.1474 |
[32m[20230207 15:22:59 @agent_ppo2.py:192][0m |          -0.0074 |           0.9441 |           0.1475 |
[32m[20230207 15:22:59 @agent_ppo2.py:192][0m |          -0.0176 |           0.9242 |           0.1473 |
[32m[20230207 15:22:59 @agent_ppo2.py:192][0m |          -0.0267 |           0.9100 |           0.1472 |
[32m[20230207 15:22:59 @agent_ppo2.py:192][0m |          -0.0261 |           0.9022 |           0.1471 |
[32m[20230207 15:22:59 @agent_ppo2.py:192][0m |          -0.0314 |           0.8992 |           0.1470 |
[32m[20230207 15:22:59 @agent_ppo2.py:192][0m |          -0.0117 |           0.8930 |           0.1468 |
[32m[20230207 15:22:59 @agent_ppo2.py:192][0m |           0.0002 |           0.8665 |           0.1469 |
[32m[20230207 15:22:59 @agent_ppo2.py:192][0m |          -0.0173 |           0.8563 |           0.1469 |
[32m[20230207 15:22:59 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:23:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: 41.45
[32m[20230207 15:23:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 74.95
[32m[20230207 15:23:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -17.05
[32m[20230207 15:23:00 @agent_ppo2.py:150][0m Total time:      22.31 min
[32m[20230207 15:23:00 @agent_ppo2.py:152][0m 1165312 total steps have happened
[32m[20230207 15:23:00 @agent_ppo2.py:128][0m #------------------------ Iteration 569 --------------------------#
[32m[20230207 15:23:01 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:23:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:01 @agent_ppo2.py:192][0m |           0.0009 |           9.6670 |           0.1479 |
[32m[20230207 15:23:01 @agent_ppo2.py:192][0m |          -0.0030 |           3.9764 |           0.1476 |
[32m[20230207 15:23:01 @agent_ppo2.py:192][0m |          -0.0079 |           2.6685 |           0.1475 |
[32m[20230207 15:23:01 @agent_ppo2.py:192][0m |          -0.0097 |           2.4407 |           0.1474 |
[32m[20230207 15:23:01 @agent_ppo2.py:192][0m |          -0.0098 |           2.1806 |           0.1475 |
[32m[20230207 15:23:01 @agent_ppo2.py:192][0m |          -0.0090 |           2.0803 |           0.1473 |
[32m[20230207 15:23:01 @agent_ppo2.py:192][0m |          -0.0146 |           1.9257 |           0.1473 |
[32m[20230207 15:23:01 @agent_ppo2.py:192][0m |          -0.0101 |           1.6627 |           0.1473 |
[32m[20230207 15:23:01 @agent_ppo2.py:192][0m |          -0.0129 |           1.6453 |           0.1474 |
[32m[20230207 15:23:01 @agent_ppo2.py:192][0m |          -0.0118 |           1.4226 |           0.1473 |
[32m[20230207 15:23:01 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:23:02 @agent_ppo2.py:145][0m Average TRAINING episode reward: 4.03
[32m[20230207 15:23:02 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 135.58
[32m[20230207 15:23:02 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 94.09
[32m[20230207 15:23:02 @agent_ppo2.py:150][0m Total time:      22.35 min
[32m[20230207 15:23:02 @agent_ppo2.py:152][0m 1167360 total steps have happened
[32m[20230207 15:23:02 @agent_ppo2.py:128][0m #------------------------ Iteration 570 --------------------------#
[32m[20230207 15:23:03 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:23:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:03 @agent_ppo2.py:192][0m |          -0.0001 |           4.4246 |           0.1493 |
[32m[20230207 15:23:03 @agent_ppo2.py:192][0m |          -0.0052 |           1.6720 |           0.1488 |
[32m[20230207 15:23:03 @agent_ppo2.py:192][0m |          -0.0089 |           1.3717 |           0.1492 |
[32m[20230207 15:23:03 @agent_ppo2.py:192][0m |          -0.0121 |           1.2639 |           0.1489 |
[32m[20230207 15:23:04 @agent_ppo2.py:192][0m |          -0.0116 |           1.2012 |           0.1488 |
[32m[20230207 15:23:04 @agent_ppo2.py:192][0m |          -0.0138 |           1.1644 |           0.1488 |
[32m[20230207 15:23:04 @agent_ppo2.py:192][0m |          -0.0141 |           1.1385 |           0.1487 |
[32m[20230207 15:23:04 @agent_ppo2.py:192][0m |          -0.0157 |           1.1038 |           0.1488 |
[32m[20230207 15:23:04 @agent_ppo2.py:192][0m |          -0.0154 |           1.0746 |           0.1488 |
[32m[20230207 15:23:04 @agent_ppo2.py:192][0m |          -0.0168 |           1.0504 |           0.1488 |
[32m[20230207 15:23:04 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:23:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -11.87
[32m[20230207 15:23:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 47.99
[32m[20230207 15:23:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 9.42
[32m[20230207 15:23:05 @agent_ppo2.py:150][0m Total time:      22.39 min
[32m[20230207 15:23:05 @agent_ppo2.py:152][0m 1169408 total steps have happened
[32m[20230207 15:23:05 @agent_ppo2.py:128][0m #------------------------ Iteration 571 --------------------------#
[32m[20230207 15:23:06 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:23:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:06 @agent_ppo2.py:192][0m |          -0.0008 |          25.5883 |           0.1492 |
[32m[20230207 15:23:06 @agent_ppo2.py:192][0m |          -0.0083 |           9.7522 |           0.1492 |
[32m[20230207 15:23:06 @agent_ppo2.py:192][0m |          -0.0115 |           7.7603 |           0.1491 |
[32m[20230207 15:23:06 @agent_ppo2.py:192][0m |          -0.0119 |           6.5493 |           0.1491 |
[32m[20230207 15:23:06 @agent_ppo2.py:192][0m |          -0.0099 |           5.8156 |           0.1490 |
[32m[20230207 15:23:06 @agent_ppo2.py:192][0m |          -0.0174 |           5.6249 |           0.1490 |
[32m[20230207 15:23:06 @agent_ppo2.py:192][0m |          -0.0118 |           5.0189 |           0.1490 |
[32m[20230207 15:23:06 @agent_ppo2.py:192][0m |          -0.0120 |           4.5894 |           0.1490 |
[32m[20230207 15:23:06 @agent_ppo2.py:192][0m |          -0.0165 |           4.5241 |           0.1489 |
[32m[20230207 15:23:07 @agent_ppo2.py:192][0m |          -0.0146 |           3.8346 |           0.1489 |
[32m[20230207 15:23:07 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:23:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -49.87
[32m[20230207 15:23:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 73.54
[32m[20230207 15:23:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 3.08
[32m[20230207 15:23:07 @agent_ppo2.py:150][0m Total time:      22.44 min
[32m[20230207 15:23:07 @agent_ppo2.py:152][0m 1171456 total steps have happened
[32m[20230207 15:23:07 @agent_ppo2.py:128][0m #------------------------ Iteration 572 --------------------------#
[32m[20230207 15:23:08 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:23:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:08 @agent_ppo2.py:192][0m |          -0.0024 |           1.3833 |           0.1493 |
[32m[20230207 15:23:08 @agent_ppo2.py:192][0m |          -0.0026 |           1.0169 |           0.1494 |
[32m[20230207 15:23:08 @agent_ppo2.py:192][0m |          -0.0070 |           1.0045 |           0.1493 |
[32m[20230207 15:23:09 @agent_ppo2.py:192][0m |          -0.0019 |           0.9818 |           0.1493 |
[32m[20230207 15:23:09 @agent_ppo2.py:192][0m |          -0.0098 |           0.9546 |           0.1495 |
[32m[20230207 15:23:09 @agent_ppo2.py:192][0m |          -0.0249 |           0.9347 |           0.1493 |
[32m[20230207 15:23:09 @agent_ppo2.py:192][0m |          -0.0177 |           0.9401 |           0.1494 |
[32m[20230207 15:23:09 @agent_ppo2.py:192][0m |          -0.0153 |           0.9120 |           0.1494 |
[32m[20230207 15:23:09 @agent_ppo2.py:192][0m |          -0.0087 |           0.9075 |           0.1494 |
[32m[20230207 15:23:09 @agent_ppo2.py:192][0m |           0.0021 |           0.8969 |           0.1494 |
[32m[20230207 15:23:09 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:23:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: 38.78
[32m[20230207 15:23:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 70.29
[32m[20230207 15:23:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -51.63
[32m[20230207 15:23:10 @agent_ppo2.py:150][0m Total time:      22.48 min
[32m[20230207 15:23:10 @agent_ppo2.py:152][0m 1173504 total steps have happened
[32m[20230207 15:23:10 @agent_ppo2.py:128][0m #------------------------ Iteration 573 --------------------------#
[32m[20230207 15:23:11 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:23:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:11 @agent_ppo2.py:192][0m |          -0.0059 |           0.9910 |           0.1434 |
[32m[20230207 15:23:11 @agent_ppo2.py:192][0m |          -0.0109 |           0.8905 |           0.1433 |
[32m[20230207 15:23:11 @agent_ppo2.py:192][0m |          -0.0055 |           0.8577 |           0.1434 |
[32m[20230207 15:23:11 @agent_ppo2.py:192][0m |           0.0089 |           0.8523 |           0.1436 |
[32m[20230207 15:23:11 @agent_ppo2.py:192][0m |          -0.0037 |           0.8177 |           0.1435 |
[32m[20230207 15:23:11 @agent_ppo2.py:192][0m |          -0.0104 |           0.8168 |           0.1435 |
[32m[20230207 15:23:11 @agent_ppo2.py:192][0m |          -0.0130 |           0.7964 |           0.1434 |
[32m[20230207 15:23:11 @agent_ppo2.py:192][0m |           0.0032 |           0.7893 |           0.1436 |
[32m[20230207 15:23:11 @agent_ppo2.py:192][0m |          -0.0317 |           0.7726 |           0.1437 |
[32m[20230207 15:23:11 @agent_ppo2.py:192][0m |          -0.0102 |           0.7637 |           0.1437 |
[32m[20230207 15:23:11 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:23:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: -10.42
[32m[20230207 15:23:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 4.09
[32m[20230207 15:23:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 30.61
[32m[20230207 15:23:12 @agent_ppo2.py:150][0m Total time:      22.52 min
[32m[20230207 15:23:12 @agent_ppo2.py:152][0m 1175552 total steps have happened
[32m[20230207 15:23:12 @agent_ppo2.py:128][0m #------------------------ Iteration 574 --------------------------#
[32m[20230207 15:23:13 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:23:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:13 @agent_ppo2.py:192][0m |          -0.0008 |           5.1823 |           0.1522 |
[32m[20230207 15:23:13 @agent_ppo2.py:192][0m |           0.0010 |           2.8773 |           0.1522 |
[32m[20230207 15:23:13 @agent_ppo2.py:192][0m |          -0.0013 |           2.5629 |           0.1523 |
[32m[20230207 15:23:13 @agent_ppo2.py:192][0m |          -0.0387 |           2.3133 |           0.1523 |
[32m[20230207 15:23:13 @agent_ppo2.py:192][0m |          -0.0090 |           2.2481 |           0.1522 |
[32m[20230207 15:23:14 @agent_ppo2.py:192][0m |          -0.0055 |           1.9946 |           0.1522 |
[32m[20230207 15:23:14 @agent_ppo2.py:192][0m |          -0.0070 |           1.9783 |           0.1523 |
[32m[20230207 15:23:14 @agent_ppo2.py:192][0m |          -0.0229 |           1.9734 |           0.1523 |
[32m[20230207 15:23:14 @agent_ppo2.py:192][0m |          -0.0141 |           2.1830 |           0.1524 |
[32m[20230207 15:23:14 @agent_ppo2.py:192][0m |          -0.0150 |           1.6974 |           0.1524 |
[32m[20230207 15:23:14 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:23:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: 69.16
[32m[20230207 15:23:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 89.15
[32m[20230207 15:23:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -18.58
[32m[20230207 15:23:15 @agent_ppo2.py:150][0m Total time:      22.56 min
[32m[20230207 15:23:15 @agent_ppo2.py:152][0m 1177600 total steps have happened
[32m[20230207 15:23:15 @agent_ppo2.py:128][0m #------------------------ Iteration 575 --------------------------#
[32m[20230207 15:23:16 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:23:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:16 @agent_ppo2.py:192][0m |          -0.0001 |           8.9584 |           0.1541 |
[32m[20230207 15:23:16 @agent_ppo2.py:192][0m |          -0.0040 |           3.4848 |           0.1539 |
[32m[20230207 15:23:16 @agent_ppo2.py:192][0m |          -0.0056 |           2.9776 |           0.1540 |
[32m[20230207 15:23:16 @agent_ppo2.py:192][0m |          -0.0058 |           2.6881 |           0.1539 |
[32m[20230207 15:23:16 @agent_ppo2.py:192][0m |          -0.0074 |           2.4048 |           0.1538 |
[32m[20230207 15:23:16 @agent_ppo2.py:192][0m |          -0.0083 |           2.2073 |           0.1538 |
[32m[20230207 15:23:16 @agent_ppo2.py:192][0m |          -0.0084 |           2.0614 |           0.1537 |
[32m[20230207 15:23:16 @agent_ppo2.py:192][0m |          -0.0089 |           1.9756 |           0.1536 |
[32m[20230207 15:23:16 @agent_ppo2.py:192][0m |          -0.0094 |           1.8667 |           0.1536 |
[32m[20230207 15:23:16 @agent_ppo2.py:192][0m |          -0.0097 |           1.7874 |           0.1535 |
[32m[20230207 15:23:16 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:23:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: 7.01
[32m[20230207 15:23:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 83.66
[32m[20230207 15:23:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 14.28
[32m[20230207 15:23:17 @agent_ppo2.py:150][0m Total time:      22.60 min
[32m[20230207 15:23:17 @agent_ppo2.py:152][0m 1179648 total steps have happened
[32m[20230207 15:23:17 @agent_ppo2.py:128][0m #------------------------ Iteration 576 --------------------------#
[32m[20230207 15:23:18 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:23:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:18 @agent_ppo2.py:192][0m |          -0.0001 |           3.5318 |           0.1540 |
[32m[20230207 15:23:18 @agent_ppo2.py:192][0m |          -0.0045 |           1.3013 |           0.1539 |
[32m[20230207 15:23:18 @agent_ppo2.py:192][0m |          -0.0067 |           1.1830 |           0.1539 |
[32m[20230207 15:23:18 @agent_ppo2.py:192][0m |          -0.0086 |           1.1354 |           0.1538 |
[32m[20230207 15:23:18 @agent_ppo2.py:192][0m |          -0.0104 |           1.1073 |           0.1538 |
[32m[20230207 15:23:18 @agent_ppo2.py:192][0m |          -0.0110 |           1.0757 |           0.1538 |
[32m[20230207 15:23:19 @agent_ppo2.py:192][0m |          -0.0120 |           1.0447 |           0.1538 |
[32m[20230207 15:23:19 @agent_ppo2.py:192][0m |          -0.0129 |           1.0253 |           0.1538 |
[32m[20230207 15:23:19 @agent_ppo2.py:192][0m |          -0.0135 |           0.9909 |           0.1538 |
[32m[20230207 15:23:19 @agent_ppo2.py:192][0m |          -0.0134 |           0.9678 |           0.1538 |
[32m[20230207 15:23:19 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:23:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -23.09
[32m[20230207 15:23:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 57.91
[32m[20230207 15:23:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -41.18
[32m[20230207 15:23:20 @agent_ppo2.py:150][0m Total time:      22.64 min
[32m[20230207 15:23:20 @agent_ppo2.py:152][0m 1181696 total steps have happened
[32m[20230207 15:23:20 @agent_ppo2.py:128][0m #------------------------ Iteration 577 --------------------------#
[32m[20230207 15:23:20 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:23:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:21 @agent_ppo2.py:192][0m |           0.0010 |          31.0436 |           0.1504 |
[32m[20230207 15:23:21 @agent_ppo2.py:192][0m |          -0.0031 |          13.6392 |           0.1502 |
[32m[20230207 15:23:21 @agent_ppo2.py:192][0m |          -0.0074 |          11.7448 |           0.1501 |
[32m[20230207 15:23:21 @agent_ppo2.py:192][0m |          -0.0095 |          10.2072 |           0.1499 |
[32m[20230207 15:23:21 @agent_ppo2.py:192][0m |          -0.0096 |           9.4968 |           0.1498 |
[32m[20230207 15:23:21 @agent_ppo2.py:192][0m |          -0.0124 |           8.6346 |           0.1498 |
[32m[20230207 15:23:21 @agent_ppo2.py:192][0m |          -0.0132 |           8.0689 |           0.1499 |
[32m[20230207 15:23:21 @agent_ppo2.py:192][0m |          -0.0152 |           7.5640 |           0.1499 |
[32m[20230207 15:23:21 @agent_ppo2.py:192][0m |          -0.0151 |           7.1852 |           0.1499 |
[32m[20230207 15:23:21 @agent_ppo2.py:192][0m |          -0.0144 |           6.9269 |           0.1498 |
[32m[20230207 15:23:21 @agent_ppo2.py:137][0m Policy update time: 0.79 s
[32m[20230207 15:23:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: 6.32
[32m[20230207 15:23:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 101.90
[32m[20230207 15:23:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 26.13
[32m[20230207 15:23:22 @agent_ppo2.py:150][0m Total time:      22.68 min
[32m[20230207 15:23:22 @agent_ppo2.py:152][0m 1183744 total steps have happened
[32m[20230207 15:23:22 @agent_ppo2.py:128][0m #------------------------ Iteration 578 --------------------------#
[32m[20230207 15:23:23 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:23:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:23 @agent_ppo2.py:192][0m |           0.0030 |           7.8072 |           0.1520 |
[32m[20230207 15:23:23 @agent_ppo2.py:192][0m |           0.0007 |           3.0371 |           0.1517 |
[32m[20230207 15:23:23 @agent_ppo2.py:192][0m |          -0.0047 |           2.6232 |           0.1520 |
[32m[20230207 15:23:23 @agent_ppo2.py:192][0m |          -0.0036 |           2.5169 |           0.1520 |
[32m[20230207 15:23:23 @agent_ppo2.py:192][0m |          -0.0078 |           2.3720 |           0.1519 |
[32m[20230207 15:23:23 @agent_ppo2.py:192][0m |          -0.0100 |           2.2884 |           0.1520 |
[32m[20230207 15:23:23 @agent_ppo2.py:192][0m |          -0.0087 |           2.2410 |           0.1520 |
[32m[20230207 15:23:23 @agent_ppo2.py:192][0m |          -0.0126 |           2.1690 |           0.1522 |
[32m[20230207 15:23:24 @agent_ppo2.py:192][0m |          -0.0119 |           2.1191 |           0.1520 |
[32m[20230207 15:23:24 @agent_ppo2.py:192][0m |          -0.0106 |           2.0699 |           0.1519 |
[32m[20230207 15:23:24 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:23:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: -24.02
[32m[20230207 15:23:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 44.00
[32m[20230207 15:23:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 40.74
[32m[20230207 15:23:24 @agent_ppo2.py:150][0m Total time:      22.72 min
[32m[20230207 15:23:24 @agent_ppo2.py:152][0m 1185792 total steps have happened
[32m[20230207 15:23:24 @agent_ppo2.py:128][0m #------------------------ Iteration 579 --------------------------#
[32m[20230207 15:23:25 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:23:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:25 @agent_ppo2.py:192][0m |          -0.0031 |          10.7383 |           0.1522 |
[32m[20230207 15:23:25 @agent_ppo2.py:192][0m |          -0.0117 |           5.0521 |           0.1520 |
[32m[20230207 15:23:26 @agent_ppo2.py:192][0m |          -0.0129 |           4.3383 |           0.1518 |
[32m[20230207 15:23:26 @agent_ppo2.py:192][0m |          -0.0113 |           3.8890 |           0.1518 |
[32m[20230207 15:23:26 @agent_ppo2.py:192][0m |          -0.0123 |           3.5716 |           0.1519 |
[32m[20230207 15:23:26 @agent_ppo2.py:192][0m |          -0.0152 |           3.3236 |           0.1516 |
[32m[20230207 15:23:26 @agent_ppo2.py:192][0m |          -0.0155 |           3.2571 |           0.1516 |
[32m[20230207 15:23:26 @agent_ppo2.py:192][0m |          -0.0169 |           3.1330 |           0.1516 |
[32m[20230207 15:23:26 @agent_ppo2.py:192][0m |          -0.0192 |           2.8998 |           0.1516 |
[32m[20230207 15:23:26 @agent_ppo2.py:192][0m |          -0.0215 |           2.7846 |           0.1515 |
[32m[20230207 15:23:26 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:23:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -16.73
[32m[20230207 15:23:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 55.10
[32m[20230207 15:23:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 17.22
[32m[20230207 15:23:27 @agent_ppo2.py:150][0m Total time:      22.76 min
[32m[20230207 15:23:27 @agent_ppo2.py:152][0m 1187840 total steps have happened
[32m[20230207 15:23:27 @agent_ppo2.py:128][0m #------------------------ Iteration 580 --------------------------#
[32m[20230207 15:23:28 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:23:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:28 @agent_ppo2.py:192][0m |           0.0052 |           4.9972 |           0.1502 |
[32m[20230207 15:23:28 @agent_ppo2.py:192][0m |           0.0025 |           2.2387 |           0.1498 |
[32m[20230207 15:23:28 @agent_ppo2.py:192][0m |          -0.0029 |           1.9267 |           0.1501 |
[32m[20230207 15:23:28 @agent_ppo2.py:192][0m |          -0.0074 |           1.7791 |           0.1500 |
[32m[20230207 15:23:28 @agent_ppo2.py:192][0m |          -0.0073 |           1.7040 |           0.1498 |
[32m[20230207 15:23:28 @agent_ppo2.py:192][0m |          -0.0101 |           1.6358 |           0.1498 |
[32m[20230207 15:23:28 @agent_ppo2.py:192][0m |          -0.0101 |           1.5831 |           0.1497 |
[32m[20230207 15:23:28 @agent_ppo2.py:192][0m |          -0.0075 |           1.5449 |           0.1496 |
[32m[20230207 15:23:28 @agent_ppo2.py:192][0m |          -0.0108 |           1.5217 |           0.1495 |
[32m[20230207 15:23:29 @agent_ppo2.py:192][0m |          -0.0120 |           1.5052 |           0.1494 |
[32m[20230207 15:23:29 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:23:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -8.30
[32m[20230207 15:23:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 53.44
[32m[20230207 15:23:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 51.26
[32m[20230207 15:23:29 @agent_ppo2.py:150][0m Total time:      22.80 min
[32m[20230207 15:23:29 @agent_ppo2.py:152][0m 1189888 total steps have happened
[32m[20230207 15:23:29 @agent_ppo2.py:128][0m #------------------------ Iteration 581 --------------------------#
[32m[20230207 15:23:30 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:23:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:30 @agent_ppo2.py:192][0m |           0.0149 |           1.6371 |           0.1519 |
[32m[20230207 15:23:30 @agent_ppo2.py:192][0m |          -0.0102 |           1.3023 |           0.1510 |
[32m[20230207 15:23:30 @agent_ppo2.py:192][0m |           0.0040 |           1.2383 |           0.1518 |
[32m[20230207 15:23:30 @agent_ppo2.py:192][0m |          -0.0040 |           1.1943 |           0.1518 |
[32m[20230207 15:23:31 @agent_ppo2.py:192][0m |          -0.0161 |           1.1572 |           0.1517 |
[32m[20230207 15:23:31 @agent_ppo2.py:192][0m |           0.0069 |           1.1424 |           0.1519 |
[32m[20230207 15:23:31 @agent_ppo2.py:192][0m |          -0.0119 |           1.1312 |           0.1518 |
[32m[20230207 15:23:31 @agent_ppo2.py:192][0m |          -0.0137 |           1.1138 |           0.1519 |
[32m[20230207 15:23:31 @agent_ppo2.py:192][0m |          -0.0202 |           1.0964 |           0.1518 |
[32m[20230207 15:23:31 @agent_ppo2.py:192][0m |          -0.0213 |           1.0914 |           0.1517 |
[32m[20230207 15:23:31 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:23:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: 54.16
[32m[20230207 15:23:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 90.52
[32m[20230207 15:23:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -29.86
[32m[20230207 15:23:32 @agent_ppo2.py:150][0m Total time:      22.84 min
[32m[20230207 15:23:32 @agent_ppo2.py:152][0m 1191936 total steps have happened
[32m[20230207 15:23:32 @agent_ppo2.py:128][0m #------------------------ Iteration 582 --------------------------#
[32m[20230207 15:23:33 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:23:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:33 @agent_ppo2.py:192][0m |           0.0060 |           1.6291 |           0.1519 |
[32m[20230207 15:23:33 @agent_ppo2.py:192][0m |          -0.0029 |           1.2642 |           0.1517 |
[32m[20230207 15:23:33 @agent_ppo2.py:192][0m |          -0.0073 |           1.1622 |           0.1517 |
[32m[20230207 15:23:33 @agent_ppo2.py:192][0m |          -0.0243 |           1.1249 |           0.1515 |
[32m[20230207 15:23:33 @agent_ppo2.py:192][0m |          -0.0236 |           1.1035 |           0.1515 |
[32m[20230207 15:23:33 @agent_ppo2.py:192][0m |          -0.0137 |           1.0503 |           0.1515 |
[32m[20230207 15:23:33 @agent_ppo2.py:192][0m |          -0.0101 |           1.0366 |           0.1517 |
[32m[20230207 15:23:33 @agent_ppo2.py:192][0m |          -0.0090 |           1.0175 |           0.1516 |
[32m[20230207 15:23:33 @agent_ppo2.py:192][0m |          -0.0305 |           1.0091 |           0.1516 |
[32m[20230207 15:23:33 @agent_ppo2.py:192][0m |          -0.0279 |           0.9919 |           0.1518 |
[32m[20230207 15:23:33 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:23:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: 94.72
[32m[20230207 15:23:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 108.92
[32m[20230207 15:23:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 84.86
[32m[20230207 15:23:34 @agent_ppo2.py:150][0m Total time:      22.88 min
[32m[20230207 15:23:34 @agent_ppo2.py:152][0m 1193984 total steps have happened
[32m[20230207 15:23:34 @agent_ppo2.py:128][0m #------------------------ Iteration 583 --------------------------#
[32m[20230207 15:23:35 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:23:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:35 @agent_ppo2.py:192][0m |          -0.0056 |           1.5121 |           0.1481 |
[32m[20230207 15:23:35 @agent_ppo2.py:192][0m |          -0.0121 |           1.3683 |           0.1479 |
[32m[20230207 15:23:35 @agent_ppo2.py:192][0m |          -0.0160 |           1.3156 |           0.1480 |
[32m[20230207 15:23:35 @agent_ppo2.py:192][0m |          -0.0288 |           1.2988 |           0.1480 |
[32m[20230207 15:23:35 @agent_ppo2.py:192][0m |          -0.0005 |           1.2741 |           0.1479 |
[32m[20230207 15:23:36 @agent_ppo2.py:192][0m |          -0.0233 |           1.2555 |           0.1482 |
[32m[20230207 15:23:36 @agent_ppo2.py:192][0m |          -0.0295 |           1.2177 |           0.1480 |
[32m[20230207 15:23:36 @agent_ppo2.py:192][0m |          -0.0231 |           1.2157 |           0.1480 |
[32m[20230207 15:23:36 @agent_ppo2.py:192][0m |           0.0035 |           1.1983 |           0.1479 |
[32m[20230207 15:23:36 @agent_ppo2.py:192][0m |          -0.0163 |           1.1811 |           0.1480 |
[32m[20230207 15:23:36 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:23:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: 10.15
[32m[20230207 15:23:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 44.04
[32m[20230207 15:23:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 84.63
[32m[20230207 15:23:37 @agent_ppo2.py:150][0m Total time:      22.92 min
[32m[20230207 15:23:37 @agent_ppo2.py:152][0m 1196032 total steps have happened
[32m[20230207 15:23:37 @agent_ppo2.py:128][0m #------------------------ Iteration 584 --------------------------#
[32m[20230207 15:23:37 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:23:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:38 @agent_ppo2.py:192][0m |          -0.0029 |           1.2265 |           0.1477 |
[32m[20230207 15:23:38 @agent_ppo2.py:192][0m |          -0.0023 |           1.1227 |           0.1474 |
[32m[20230207 15:23:38 @agent_ppo2.py:192][0m |           0.0103 |           1.0923 |           0.1475 |
[32m[20230207 15:23:38 @agent_ppo2.py:192][0m |           0.0476 |           1.2748 |           0.1474 |
[32m[20230207 15:23:38 @agent_ppo2.py:192][0m |          -0.0147 |           1.1210 |           0.1468 |
[32m[20230207 15:23:38 @agent_ppo2.py:192][0m |           0.0007 |           1.0571 |           0.1472 |
[32m[20230207 15:23:38 @agent_ppo2.py:192][0m |          -0.0010 |           1.0381 |           0.1473 |
[32m[20230207 15:23:38 @agent_ppo2.py:192][0m |          -0.0058 |           1.0288 |           0.1473 |
[32m[20230207 15:23:38 @agent_ppo2.py:192][0m |          -0.0121 |           1.0143 |           0.1474 |
[32m[20230207 15:23:38 @agent_ppo2.py:192][0m |           0.0078 |           1.0096 |           0.1476 |
[32m[20230207 15:23:38 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:23:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: 41.81
[32m[20230207 15:23:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 46.37
[32m[20230207 15:23:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 40.16
[32m[20230207 15:23:39 @agent_ppo2.py:150][0m Total time:      22.96 min
[32m[20230207 15:23:39 @agent_ppo2.py:152][0m 1198080 total steps have happened
[32m[20230207 15:23:39 @agent_ppo2.py:128][0m #------------------------ Iteration 585 --------------------------#
[32m[20230207 15:23:40 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:23:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:40 @agent_ppo2.py:192][0m |          -0.0101 |           1.1490 |           0.1479 |
[32m[20230207 15:23:40 @agent_ppo2.py:192][0m |          -0.0114 |           0.9902 |           0.1478 |
[32m[20230207 15:23:40 @agent_ppo2.py:192][0m |          -0.0125 |           0.9432 |           0.1478 |
[32m[20230207 15:23:40 @agent_ppo2.py:192][0m |          -0.0099 |           0.9156 |           0.1477 |
[32m[20230207 15:23:40 @agent_ppo2.py:192][0m |          -0.0074 |           0.9030 |           0.1478 |
[32m[20230207 15:23:40 @agent_ppo2.py:192][0m |           0.0083 |           0.8828 |           0.1478 |
[32m[20230207 15:23:40 @agent_ppo2.py:192][0m |          -0.0135 |           0.8730 |           0.1477 |
[32m[20230207 15:23:40 @agent_ppo2.py:192][0m |          -0.0097 |           0.8553 |           0.1477 |
[32m[20230207 15:23:41 @agent_ppo2.py:192][0m |          -0.0128 |           0.8451 |           0.1477 |
[32m[20230207 15:23:41 @agent_ppo2.py:192][0m |          -0.0215 |           0.8391 |           0.1477 |
[32m[20230207 15:23:41 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:23:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: 74.22
[32m[20230207 15:23:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 106.38
[32m[20230207 15:23:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 25.70
[32m[20230207 15:23:41 @agent_ppo2.py:150][0m Total time:      23.00 min
[32m[20230207 15:23:41 @agent_ppo2.py:152][0m 1200128 total steps have happened
[32m[20230207 15:23:41 @agent_ppo2.py:128][0m #------------------------ Iteration 586 --------------------------#
[32m[20230207 15:23:42 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:23:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:42 @agent_ppo2.py:192][0m |           0.0004 |           6.8589 |           0.1529 |
[32m[20230207 15:23:42 @agent_ppo2.py:192][0m |          -0.0042 |           2.4573 |           0.1529 |
[32m[20230207 15:23:43 @agent_ppo2.py:192][0m |          -0.0057 |           1.7476 |           0.1530 |
[32m[20230207 15:23:43 @agent_ppo2.py:192][0m |          -0.0090 |           1.5323 |           0.1527 |
[32m[20230207 15:23:43 @agent_ppo2.py:192][0m |          -0.0096 |           1.3849 |           0.1527 |
[32m[20230207 15:23:43 @agent_ppo2.py:192][0m |          -0.0116 |           1.3323 |           0.1527 |
[32m[20230207 15:23:43 @agent_ppo2.py:192][0m |          -0.0121 |           1.2718 |           0.1525 |
[32m[20230207 15:23:43 @agent_ppo2.py:192][0m |          -0.0133 |           1.2315 |           0.1525 |
[32m[20230207 15:23:43 @agent_ppo2.py:192][0m |          -0.0139 |           1.1998 |           0.1525 |
[32m[20230207 15:23:43 @agent_ppo2.py:192][0m |          -0.0147 |           1.1643 |           0.1525 |
[32m[20230207 15:23:43 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:23:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: 41.14
[32m[20230207 15:23:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 128.93
[32m[20230207 15:23:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -12.85
[32m[20230207 15:23:44 @agent_ppo2.py:150][0m Total time:      23.04 min
[32m[20230207 15:23:44 @agent_ppo2.py:152][0m 1202176 total steps have happened
[32m[20230207 15:23:44 @agent_ppo2.py:128][0m #------------------------ Iteration 587 --------------------------#
[32m[20230207 15:23:44 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:23:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:45 @agent_ppo2.py:192][0m |          -0.0047 |           1.6091 |           0.1516 |
[32m[20230207 15:23:45 @agent_ppo2.py:192][0m |           0.0043 |           1.3513 |           0.1512 |
[32m[20230207 15:23:45 @agent_ppo2.py:192][0m |          -0.0027 |           1.2938 |           0.1514 |
[32m[20230207 15:23:45 @agent_ppo2.py:192][0m |           0.0071 |           1.2583 |           0.1512 |
[32m[20230207 15:23:45 @agent_ppo2.py:192][0m |          -0.0089 |           1.2759 |           0.1512 |
[32m[20230207 15:23:45 @agent_ppo2.py:192][0m |          -0.0313 |           1.2233 |           0.1508 |
[32m[20230207 15:23:45 @agent_ppo2.py:192][0m |           0.0014 |           1.2027 |           0.1510 |
[32m[20230207 15:23:45 @agent_ppo2.py:192][0m |          -0.0125 |           1.1967 |           0.1510 |
[32m[20230207 15:23:45 @agent_ppo2.py:192][0m |          -0.0080 |           1.1872 |           0.1511 |
[32m[20230207 15:23:45 @agent_ppo2.py:192][0m |          -0.0368 |           1.1755 |           0.1512 |
[32m[20230207 15:23:45 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:23:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: 54.37
[32m[20230207 15:23:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 55.18
[32m[20230207 15:23:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -56.40
[32m[20230207 15:23:46 @agent_ppo2.py:150][0m Total time:      23.08 min
[32m[20230207 15:23:46 @agent_ppo2.py:152][0m 1204224 total steps have happened
[32m[20230207 15:23:46 @agent_ppo2.py:128][0m #------------------------ Iteration 588 --------------------------#
[32m[20230207 15:23:47 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:23:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:47 @agent_ppo2.py:192][0m |           0.0006 |           6.5078 |           0.1492 |
[32m[20230207 15:23:47 @agent_ppo2.py:192][0m |          -0.0039 |           2.2441 |           0.1492 |
[32m[20230207 15:23:47 @agent_ppo2.py:192][0m |          -0.0044 |           1.9895 |           0.1493 |
[32m[20230207 15:23:47 @agent_ppo2.py:192][0m |          -0.0074 |           1.7621 |           0.1491 |
[32m[20230207 15:23:47 @agent_ppo2.py:192][0m |          -0.0079 |           1.5910 |           0.1490 |
[32m[20230207 15:23:47 @agent_ppo2.py:192][0m |          -0.0092 |           1.4800 |           0.1489 |
[32m[20230207 15:23:48 @agent_ppo2.py:192][0m |          -0.0099 |           1.4823 |           0.1489 |
[32m[20230207 15:23:48 @agent_ppo2.py:192][0m |          -0.0081 |           1.3566 |           0.1488 |
[32m[20230207 15:23:48 @agent_ppo2.py:192][0m |          -0.0090 |           1.2971 |           0.1486 |
[32m[20230207 15:23:48 @agent_ppo2.py:192][0m |          -0.0109 |           1.3235 |           0.1485 |
[32m[20230207 15:23:48 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:23:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: -43.64
[32m[20230207 15:23:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 22.94
[32m[20230207 15:23:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -10.58
[32m[20230207 15:23:49 @agent_ppo2.py:150][0m Total time:      23.12 min
[32m[20230207 15:23:49 @agent_ppo2.py:152][0m 1206272 total steps have happened
[32m[20230207 15:23:49 @agent_ppo2.py:128][0m #------------------------ Iteration 589 --------------------------#
[32m[20230207 15:23:49 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:23:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:50 @agent_ppo2.py:192][0m |           0.0028 |           4.5719 |           0.1474 |
[32m[20230207 15:23:50 @agent_ppo2.py:192][0m |          -0.0038 |           1.6075 |           0.1473 |
[32m[20230207 15:23:50 @agent_ppo2.py:192][0m |          -0.0077 |           1.3558 |           0.1473 |
[32m[20230207 15:23:50 @agent_ppo2.py:192][0m |           0.0155 |           1.3198 |           0.1472 |
[32m[20230207 15:23:50 @agent_ppo2.py:192][0m |          -0.0048 |           1.2813 |           0.1471 |
[32m[20230207 15:23:50 @agent_ppo2.py:192][0m |          -0.0047 |           1.1916 |           0.1471 |
[32m[20230207 15:23:50 @agent_ppo2.py:192][0m |          -0.0142 |           1.1514 |           0.1470 |
[32m[20230207 15:23:50 @agent_ppo2.py:192][0m |          -0.0078 |           1.1196 |           0.1470 |
[32m[20230207 15:23:50 @agent_ppo2.py:192][0m |          -0.0077 |           1.1212 |           0.1470 |
[32m[20230207 15:23:50 @agent_ppo2.py:192][0m |           0.0091 |           1.0775 |           0.1469 |
[32m[20230207 15:23:50 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:23:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: 8.25
[32m[20230207 15:23:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 120.43
[32m[20230207 15:23:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -3.51
[32m[20230207 15:23:51 @agent_ppo2.py:150][0m Total time:      23.16 min
[32m[20230207 15:23:51 @agent_ppo2.py:152][0m 1208320 total steps have happened
[32m[20230207 15:23:51 @agent_ppo2.py:128][0m #------------------------ Iteration 590 --------------------------#
[32m[20230207 15:23:52 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:23:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:52 @agent_ppo2.py:192][0m |          -0.0006 |           7.7228 |           0.1512 |
[32m[20230207 15:23:52 @agent_ppo2.py:192][0m |          -0.0049 |           2.3812 |           0.1511 |
[32m[20230207 15:23:52 @agent_ppo2.py:192][0m |          -0.0069 |           1.9088 |           0.1511 |
[32m[20230207 15:23:52 @agent_ppo2.py:192][0m |          -0.0082 |           1.7402 |           0.1510 |
[32m[20230207 15:23:52 @agent_ppo2.py:192][0m |          -0.0096 |           1.6318 |           0.1509 |
[32m[20230207 15:23:52 @agent_ppo2.py:192][0m |          -0.0103 |           1.5447 |           0.1508 |
[32m[20230207 15:23:53 @agent_ppo2.py:192][0m |          -0.0111 |           1.4644 |           0.1509 |
[32m[20230207 15:23:53 @agent_ppo2.py:192][0m |          -0.0117 |           1.3931 |           0.1507 |
[32m[20230207 15:23:53 @agent_ppo2.py:192][0m |          -0.0120 |           1.3597 |           0.1506 |
[32m[20230207 15:23:53 @agent_ppo2.py:192][0m |          -0.0133 |           1.3262 |           0.1505 |
[32m[20230207 15:23:53 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:23:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -0.66
[32m[20230207 15:23:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 61.36
[32m[20230207 15:23:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -35.14
[32m[20230207 15:23:53 @agent_ppo2.py:150][0m Total time:      23.20 min
[32m[20230207 15:23:53 @agent_ppo2.py:152][0m 1210368 total steps have happened
[32m[20230207 15:23:53 @agent_ppo2.py:128][0m #------------------------ Iteration 591 --------------------------#
[32m[20230207 15:23:54 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:23:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:54 @agent_ppo2.py:192][0m |          -0.0133 |           1.3455 |           0.1463 |
[32m[20230207 15:23:54 @agent_ppo2.py:192][0m |           0.0017 |           1.1769 |           0.1463 |
[32m[20230207 15:23:55 @agent_ppo2.py:192][0m |           0.0053 |           1.1322 |           0.1463 |
[32m[20230207 15:23:55 @agent_ppo2.py:192][0m |          -0.0073 |           1.1045 |           0.1462 |
[32m[20230207 15:23:55 @agent_ppo2.py:192][0m |          -0.0056 |           1.0755 |           0.1463 |
[32m[20230207 15:23:55 @agent_ppo2.py:192][0m |           0.0014 |           1.0482 |           0.1464 |
[32m[20230207 15:23:55 @agent_ppo2.py:192][0m |          -0.0159 |           1.0363 |           0.1462 |
[32m[20230207 15:23:55 @agent_ppo2.py:192][0m |           0.0023 |           1.0126 |           0.1464 |
[32m[20230207 15:23:55 @agent_ppo2.py:192][0m |          -0.0160 |           1.0031 |           0.1465 |
[32m[20230207 15:23:55 @agent_ppo2.py:192][0m |          -0.0259 |           0.9884 |           0.1465 |
[32m[20230207 15:23:55 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:23:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: 70.54
[32m[20230207 15:23:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 81.05
[32m[20230207 15:23:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -46.08
[32m[20230207 15:23:56 @agent_ppo2.py:150][0m Total time:      23.24 min
[32m[20230207 15:23:56 @agent_ppo2.py:152][0m 1212416 total steps have happened
[32m[20230207 15:23:56 @agent_ppo2.py:128][0m #------------------------ Iteration 592 --------------------------#
[32m[20230207 15:23:56 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:23:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:57 @agent_ppo2.py:192][0m |           0.0001 |          21.3522 |           0.1482 |
[32m[20230207 15:23:57 @agent_ppo2.py:192][0m |          -0.0046 |          17.0445 |           0.1479 |
[32m[20230207 15:23:57 @agent_ppo2.py:192][0m |           0.0006 |          11.5938 |           0.1480 |
[32m[20230207 15:23:57 @agent_ppo2.py:192][0m |          -0.0064 |           9.0544 |           0.1479 |
[32m[20230207 15:23:57 @agent_ppo2.py:192][0m |          -0.0067 |           8.0069 |           0.1479 |
[32m[20230207 15:23:57 @agent_ppo2.py:192][0m |           0.0084 |           8.0332 |           0.1480 |
[32m[20230207 15:23:57 @agent_ppo2.py:192][0m |          -0.0042 |           7.1864 |           0.1479 |
[32m[20230207 15:23:57 @agent_ppo2.py:192][0m |          -0.0086 |           6.1189 |           0.1479 |
[32m[20230207 15:23:57 @agent_ppo2.py:192][0m |          -0.0078 |           5.4927 |           0.1480 |
[32m[20230207 15:23:57 @agent_ppo2.py:192][0m |          -0.0093 |           4.5696 |           0.1479 |
[32m[20230207 15:23:57 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230207 15:23:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: 13.04
[32m[20230207 15:23:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 81.74
[32m[20230207 15:23:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -56.01
[32m[20230207 15:23:58 @agent_ppo2.py:150][0m Total time:      23.28 min
[32m[20230207 15:23:58 @agent_ppo2.py:152][0m 1214464 total steps have happened
[32m[20230207 15:23:58 @agent_ppo2.py:128][0m #------------------------ Iteration 593 --------------------------#
[32m[20230207 15:23:58 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:23:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:23:58 @agent_ppo2.py:192][0m |          -0.0014 |          22.0450 |           0.1491 |
[32m[20230207 15:23:59 @agent_ppo2.py:192][0m |          -0.0034 |           6.6783 |           0.1490 |
[32m[20230207 15:23:59 @agent_ppo2.py:192][0m |          -0.0029 |           4.8351 |           0.1487 |
[32m[20230207 15:23:59 @agent_ppo2.py:192][0m |          -0.0099 |           4.0476 |           0.1488 |
[32m[20230207 15:23:59 @agent_ppo2.py:192][0m |          -0.0127 |           3.6323 |           0.1488 |
[32m[20230207 15:23:59 @agent_ppo2.py:192][0m |          -0.0135 |           3.3163 |           0.1487 |
[32m[20230207 15:23:59 @agent_ppo2.py:192][0m |          -0.0076 |           3.1870 |           0.1488 |
[32m[20230207 15:23:59 @agent_ppo2.py:192][0m |          -0.0077 |           2.9747 |           0.1487 |
[32m[20230207 15:23:59 @agent_ppo2.py:192][0m |          -0.0128 |           2.8666 |           0.1488 |
[32m[20230207 15:23:59 @agent_ppo2.py:192][0m |          -0.0154 |           2.6980 |           0.1487 |
[32m[20230207 15:23:59 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:24:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: -41.48
[32m[20230207 15:24:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 56.04
[32m[20230207 15:24:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 1.49
[32m[20230207 15:24:00 @agent_ppo2.py:150][0m Total time:      23.31 min
[32m[20230207 15:24:00 @agent_ppo2.py:152][0m 1216512 total steps have happened
[32m[20230207 15:24:00 @agent_ppo2.py:128][0m #------------------------ Iteration 594 --------------------------#
[32m[20230207 15:24:00 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:24:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:01 @agent_ppo2.py:192][0m |          -0.0127 |           1.1743 |           0.1478 |
[32m[20230207 15:24:01 @agent_ppo2.py:192][0m |          -0.0269 |           1.0752 |           0.1476 |
[32m[20230207 15:24:01 @agent_ppo2.py:192][0m |           0.0033 |           1.0375 |           0.1475 |
[32m[20230207 15:24:01 @agent_ppo2.py:192][0m |          -0.0104 |           1.0133 |           0.1477 |
[32m[20230207 15:24:01 @agent_ppo2.py:192][0m |           0.0055 |           0.9955 |           0.1476 |
[32m[20230207 15:24:01 @agent_ppo2.py:192][0m |          -0.0048 |           0.9833 |           0.1476 |
[32m[20230207 15:24:01 @agent_ppo2.py:192][0m |          -0.0184 |           0.9750 |           0.1474 |
[32m[20230207 15:24:01 @agent_ppo2.py:192][0m |           0.0004 |           0.9641 |           0.1477 |
[32m[20230207 15:24:01 @agent_ppo2.py:192][0m |           0.0243 |           1.0093 |           0.1477 |
[32m[20230207 15:24:01 @agent_ppo2.py:192][0m |          -0.0236 |           0.9734 |           0.1475 |
[32m[20230207 15:24:01 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:24:02 @agent_ppo2.py:145][0m Average TRAINING episode reward: -13.48
[32m[20230207 15:24:02 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -0.73
[32m[20230207 15:24:02 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 0.15
[32m[20230207 15:24:02 @agent_ppo2.py:150][0m Total time:      23.35 min
[32m[20230207 15:24:02 @agent_ppo2.py:152][0m 1218560 total steps have happened
[32m[20230207 15:24:02 @agent_ppo2.py:128][0m #------------------------ Iteration 595 --------------------------#
[32m[20230207 15:24:03 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:24:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:03 @agent_ppo2.py:192][0m |          -0.0024 |          13.1699 |           0.1484 |
[32m[20230207 15:24:03 @agent_ppo2.py:192][0m |          -0.0077 |           4.9110 |           0.1482 |
[32m[20230207 15:24:03 @agent_ppo2.py:192][0m |          -0.0068 |           3.8373 |           0.1482 |
[32m[20230207 15:24:03 @agent_ppo2.py:192][0m |          -0.0113 |           3.4053 |           0.1482 |
[32m[20230207 15:24:03 @agent_ppo2.py:192][0m |          -0.0104 |           3.0975 |           0.1483 |
[32m[20230207 15:24:04 @agent_ppo2.py:192][0m |          -0.0135 |           2.9782 |           0.1482 |
[32m[20230207 15:24:04 @agent_ppo2.py:192][0m |          -0.0089 |           2.7486 |           0.1484 |
[32m[20230207 15:24:04 @agent_ppo2.py:192][0m |          -0.0160 |           2.6685 |           0.1482 |
[32m[20230207 15:24:04 @agent_ppo2.py:192][0m |          -0.0137 |           2.5345 |           0.1483 |
[32m[20230207 15:24:04 @agent_ppo2.py:192][0m |          -0.0138 |           2.4529 |           0.1483 |
[32m[20230207 15:24:04 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:24:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -23.20
[32m[20230207 15:24:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 149.65
[32m[20230207 15:24:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 27.87
[32m[20230207 15:24:05 @agent_ppo2.py:150][0m Total time:      23.39 min
[32m[20230207 15:24:05 @agent_ppo2.py:152][0m 1220608 total steps have happened
[32m[20230207 15:24:05 @agent_ppo2.py:128][0m #------------------------ Iteration 596 --------------------------#
[32m[20230207 15:24:06 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:24:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:06 @agent_ppo2.py:192][0m |          -0.0010 |           8.8507 |           0.1494 |
[32m[20230207 15:24:06 @agent_ppo2.py:192][0m |          -0.0057 |           2.4368 |           0.1495 |
[32m[20230207 15:24:06 @agent_ppo2.py:192][0m |          -0.0030 |           1.7967 |           0.1495 |
[32m[20230207 15:24:06 @agent_ppo2.py:192][0m |          -0.0103 |           1.6697 |           0.1494 |
[32m[20230207 15:24:06 @agent_ppo2.py:192][0m |          -0.0085 |           1.5394 |           0.1493 |
[32m[20230207 15:24:06 @agent_ppo2.py:192][0m |          -0.0114 |           1.4979 |           0.1494 |
[32m[20230207 15:24:06 @agent_ppo2.py:192][0m |          -0.0118 |           1.4609 |           0.1494 |
[32m[20230207 15:24:06 @agent_ppo2.py:192][0m |          -0.0102 |           1.4020 |           0.1493 |
[32m[20230207 15:24:06 @agent_ppo2.py:192][0m |          -0.0112 |           1.3683 |           0.1493 |
[32m[20230207 15:24:06 @agent_ppo2.py:192][0m |          -0.0111 |           1.3555 |           0.1493 |
[32m[20230207 15:24:06 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:24:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -8.40
[32m[20230207 15:24:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 51.44
[32m[20230207 15:24:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -31.46
[32m[20230207 15:24:07 @agent_ppo2.py:150][0m Total time:      23.43 min
[32m[20230207 15:24:07 @agent_ppo2.py:152][0m 1222656 total steps have happened
[32m[20230207 15:24:07 @agent_ppo2.py:128][0m #------------------------ Iteration 597 --------------------------#
[32m[20230207 15:24:08 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:24:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:08 @agent_ppo2.py:192][0m |          -0.0008 |          18.6985 |           0.1544 |
[32m[20230207 15:24:08 @agent_ppo2.py:192][0m |          -0.0005 |          18.1510 |           0.1543 |
[32m[20230207 15:24:08 @agent_ppo2.py:192][0m |          -0.0084 |          16.4849 |           0.1542 |
[32m[20230207 15:24:08 @agent_ppo2.py:192][0m |          -0.0081 |          16.2789 |           0.1541 |
[32m[20230207 15:24:08 @agent_ppo2.py:192][0m |          -0.0086 |          15.9899 |           0.1539 |
[32m[20230207 15:24:09 @agent_ppo2.py:192][0m |          -0.0117 |          15.8506 |           0.1539 |
[32m[20230207 15:24:09 @agent_ppo2.py:192][0m |          -0.0090 |          15.8281 |           0.1539 |
[32m[20230207 15:24:09 @agent_ppo2.py:192][0m |          -0.0130 |          15.5731 |           0.1538 |
[32m[20230207 15:24:09 @agent_ppo2.py:192][0m |          -0.0122 |          15.4627 |           0.1540 |
[32m[20230207 15:24:09 @agent_ppo2.py:192][0m |          -0.0133 |          15.3818 |           0.1539 |
[32m[20230207 15:24:09 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:24:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -13.29
[32m[20230207 15:24:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 75.53
[32m[20230207 15:24:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 1.33
[32m[20230207 15:24:10 @agent_ppo2.py:150][0m Total time:      23.47 min
[32m[20230207 15:24:10 @agent_ppo2.py:152][0m 1224704 total steps have happened
[32m[20230207 15:24:10 @agent_ppo2.py:128][0m #------------------------ Iteration 598 --------------------------#
[32m[20230207 15:24:10 @agent_ppo2.py:134][0m Sampling time: 0.64 s by 1 slaves
[32m[20230207 15:24:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:10 @agent_ppo2.py:192][0m |           0.0001 |          20.4732 |           0.1516 |
[32m[20230207 15:24:10 @agent_ppo2.py:192][0m |          -0.0069 |           7.2944 |           0.1514 |
[32m[20230207 15:24:10 @agent_ppo2.py:192][0m |          -0.0066 |           6.5122 |           0.1513 |
[32m[20230207 15:24:11 @agent_ppo2.py:192][0m |          -0.0091 |           6.1043 |           0.1512 |
[32m[20230207 15:24:11 @agent_ppo2.py:192][0m |          -0.0078 |           5.8337 |           0.1511 |
[32m[20230207 15:24:11 @agent_ppo2.py:192][0m |          -0.0154 |           5.6958 |           0.1512 |
[32m[20230207 15:24:11 @agent_ppo2.py:192][0m |          -0.0117 |           5.5936 |           0.1510 |
[32m[20230207 15:24:11 @agent_ppo2.py:192][0m |          -0.0144 |           5.4439 |           0.1512 |
[32m[20230207 15:24:11 @agent_ppo2.py:192][0m |          -0.0132 |           5.2940 |           0.1510 |
[32m[20230207 15:24:11 @agent_ppo2.py:192][0m |          -0.0170 |           5.2942 |           0.1511 |
[32m[20230207 15:24:11 @agent_ppo2.py:137][0m Policy update time: 0.70 s
[32m[20230207 15:24:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: -33.06
[32m[20230207 15:24:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 60.79
[32m[20230207 15:24:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 15.84
[32m[20230207 15:24:12 @agent_ppo2.py:150][0m Total time:      23.51 min
[32m[20230207 15:24:12 @agent_ppo2.py:152][0m 1226752 total steps have happened
[32m[20230207 15:24:12 @agent_ppo2.py:128][0m #------------------------ Iteration 599 --------------------------#
[32m[20230207 15:24:13 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:24:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:13 @agent_ppo2.py:192][0m |           0.0070 |           3.1642 |           0.1461 |
[32m[20230207 15:24:13 @agent_ppo2.py:192][0m |           0.0035 |           2.6335 |           0.1463 |
[32m[20230207 15:24:13 @agent_ppo2.py:192][0m |          -0.0027 |           2.5262 |           0.1463 |
[32m[20230207 15:24:13 @agent_ppo2.py:192][0m |          -0.0166 |           2.4735 |           0.1463 |
[32m[20230207 15:24:13 @agent_ppo2.py:192][0m |          -0.0248 |           2.3944 |           0.1463 |
[32m[20230207 15:24:13 @agent_ppo2.py:192][0m |          -0.0112 |           2.3955 |           0.1460 |
[32m[20230207 15:24:13 @agent_ppo2.py:192][0m |          -0.0100 |           2.3203 |           0.1462 |
[32m[20230207 15:24:13 @agent_ppo2.py:192][0m |          -0.0065 |           2.3006 |           0.1461 |
[32m[20230207 15:24:13 @agent_ppo2.py:192][0m |          -0.0058 |           2.2733 |           0.1461 |
[32m[20230207 15:24:13 @agent_ppo2.py:192][0m |          -0.0176 |           2.2813 |           0.1461 |
[32m[20230207 15:24:13 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:24:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: 50.18
[32m[20230207 15:24:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 86.04
[32m[20230207 15:24:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -90.13
[32m[20230207 15:24:14 @agent_ppo2.py:150][0m Total time:      23.54 min
[32m[20230207 15:24:14 @agent_ppo2.py:152][0m 1228800 total steps have happened
[32m[20230207 15:24:14 @agent_ppo2.py:128][0m #------------------------ Iteration 600 --------------------------#
[32m[20230207 15:24:14 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:24:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:15 @agent_ppo2.py:192][0m |          -0.0006 |          27.1062 |           0.1469 |
[32m[20230207 15:24:15 @agent_ppo2.py:192][0m |          -0.0053 |          13.0180 |           0.1465 |
[32m[20230207 15:24:15 @agent_ppo2.py:192][0m |          -0.0184 |           9.5891 |           0.1466 |
[32m[20230207 15:24:15 @agent_ppo2.py:192][0m |          -0.0128 |           8.4855 |           0.1465 |
[32m[20230207 15:24:15 @agent_ppo2.py:192][0m |          -0.0133 |           7.6114 |           0.1464 |
[32m[20230207 15:24:15 @agent_ppo2.py:192][0m |          -0.0137 |           7.1910 |           0.1464 |
[32m[20230207 15:24:15 @agent_ppo2.py:192][0m |          -0.0186 |           6.6464 |           0.1464 |
[32m[20230207 15:24:15 @agent_ppo2.py:192][0m |          -0.0152 |           6.0304 |           0.1462 |
[32m[20230207 15:24:15 @agent_ppo2.py:192][0m |          -0.0208 |           5.7656 |           0.1461 |
[32m[20230207 15:24:15 @agent_ppo2.py:192][0m |          -0.0279 |           5.4973 |           0.1459 |
[32m[20230207 15:24:15 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:24:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -44.58
[32m[20230207 15:24:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 85.86
[32m[20230207 15:24:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -3.91
[32m[20230207 15:24:16 @agent_ppo2.py:150][0m Total time:      23.58 min
[32m[20230207 15:24:16 @agent_ppo2.py:152][0m 1230848 total steps have happened
[32m[20230207 15:24:16 @agent_ppo2.py:128][0m #------------------------ Iteration 601 --------------------------#
[32m[20230207 15:24:17 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:24:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:17 @agent_ppo2.py:192][0m |           0.0005 |          23.5381 |           0.1513 |
[32m[20230207 15:24:17 @agent_ppo2.py:192][0m |          -0.0042 |          12.1228 |           0.1513 |
[32m[20230207 15:24:17 @agent_ppo2.py:192][0m |          -0.0057 |           8.9283 |           0.1514 |
[32m[20230207 15:24:17 @agent_ppo2.py:192][0m |          -0.0072 |           8.0004 |           0.1513 |
[32m[20230207 15:24:17 @agent_ppo2.py:192][0m |          -0.0086 |           6.3248 |           0.1513 |
[32m[20230207 15:24:17 @agent_ppo2.py:192][0m |          -0.0097 |           5.7545 |           0.1512 |
[32m[20230207 15:24:17 @agent_ppo2.py:192][0m |          -0.0100 |           4.9753 |           0.1512 |
[32m[20230207 15:24:17 @agent_ppo2.py:192][0m |          -0.0109 |           4.5324 |           0.1512 |
[32m[20230207 15:24:17 @agent_ppo2.py:192][0m |          -0.0124 |           4.4610 |           0.1512 |
[32m[20230207 15:24:17 @agent_ppo2.py:192][0m |          -0.0123 |           4.0683 |           0.1511 |
[32m[20230207 15:24:17 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:24:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: 48.54
[32m[20230207 15:24:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 117.30
[32m[20230207 15:24:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 25.00
[32m[20230207 15:24:18 @agent_ppo2.py:150][0m Total time:      23.61 min
[32m[20230207 15:24:18 @agent_ppo2.py:152][0m 1232896 total steps have happened
[32m[20230207 15:24:18 @agent_ppo2.py:128][0m #------------------------ Iteration 602 --------------------------#
[32m[20230207 15:24:18 @agent_ppo2.py:134][0m Sampling time: 0.51 s by 1 slaves
[32m[20230207 15:24:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:19 @agent_ppo2.py:192][0m |          -0.0049 |          21.9790 |           0.1514 |
[32m[20230207 15:24:19 @agent_ppo2.py:192][0m |          -0.0021 |          12.3725 |           0.1511 |
[32m[20230207 15:24:19 @agent_ppo2.py:192][0m |          -0.0115 |           8.4390 |           0.1510 |
[32m[20230207 15:24:19 @agent_ppo2.py:192][0m |           0.0069 |           7.2919 |           0.1509 |
[32m[20230207 15:24:19 @agent_ppo2.py:192][0m |          -0.0130 |           6.7358 |           0.1509 |
[32m[20230207 15:24:19 @agent_ppo2.py:192][0m |          -0.0140 |           5.8157 |           0.1509 |
[32m[20230207 15:24:19 @agent_ppo2.py:192][0m |          -0.0158 |           5.3526 |           0.1509 |
[32m[20230207 15:24:19 @agent_ppo2.py:192][0m |          -0.0024 |           5.3941 |           0.1509 |
[32m[20230207 15:24:19 @agent_ppo2.py:192][0m |          -0.0205 |           4.9379 |           0.1509 |
[32m[20230207 15:24:19 @agent_ppo2.py:192][0m |          -0.0171 |           4.6045 |           0.1509 |
[32m[20230207 15:24:19 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:24:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: 24.22
[32m[20230207 15:24:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 134.21
[32m[20230207 15:24:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 20.92
[32m[20230207 15:24:20 @agent_ppo2.py:150][0m Total time:      23.64 min
[32m[20230207 15:24:20 @agent_ppo2.py:152][0m 1234944 total steps have happened
[32m[20230207 15:24:20 @agent_ppo2.py:128][0m #------------------------ Iteration 603 --------------------------#
[32m[20230207 15:24:21 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:24:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:21 @agent_ppo2.py:192][0m |          -0.0008 |           9.3095 |           0.1493 |
[32m[20230207 15:24:21 @agent_ppo2.py:192][0m |          -0.0044 |           6.2209 |           0.1490 |
[32m[20230207 15:24:21 @agent_ppo2.py:192][0m |          -0.0069 |           5.8957 |           0.1490 |
[32m[20230207 15:24:21 @agent_ppo2.py:192][0m |          -0.0089 |           5.6707 |           0.1493 |
[32m[20230207 15:24:21 @agent_ppo2.py:192][0m |          -0.0068 |           5.5296 |           0.1491 |
[32m[20230207 15:24:21 @agent_ppo2.py:192][0m |          -0.0054 |           5.4091 |           0.1491 |
[32m[20230207 15:24:21 @agent_ppo2.py:192][0m |          -0.0084 |           5.3408 |           0.1492 |
[32m[20230207 15:24:21 @agent_ppo2.py:192][0m |          -0.0076 |           5.2516 |           0.1491 |
[32m[20230207 15:24:21 @agent_ppo2.py:192][0m |          -0.0114 |           5.1826 |           0.1492 |
[32m[20230207 15:24:21 @agent_ppo2.py:192][0m |          -0.0083 |           5.1433 |           0.1491 |
[32m[20230207 15:24:21 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:24:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: 13.71
[32m[20230207 15:24:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 78.25
[32m[20230207 15:24:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -43.58
[32m[20230207 15:24:22 @agent_ppo2.py:150][0m Total time:      23.69 min
[32m[20230207 15:24:22 @agent_ppo2.py:152][0m 1236992 total steps have happened
[32m[20230207 15:24:22 @agent_ppo2.py:128][0m #------------------------ Iteration 604 --------------------------#
[32m[20230207 15:24:23 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:24:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:23 @agent_ppo2.py:192][0m |          -0.0042 |          10.8946 |           0.1519 |
[32m[20230207 15:24:23 @agent_ppo2.py:192][0m |          -0.0032 |           6.9817 |           0.1516 |
[32m[20230207 15:24:23 @agent_ppo2.py:192][0m |          -0.0043 |           4.6469 |           0.1517 |
[32m[20230207 15:24:24 @agent_ppo2.py:192][0m |          -0.0071 |           4.3027 |           0.1517 |
[32m[20230207 15:24:24 @agent_ppo2.py:192][0m |          -0.0085 |           3.6158 |           0.1515 |
[32m[20230207 15:24:24 @agent_ppo2.py:192][0m |          -0.0143 |           3.6416 |           0.1514 |
[32m[20230207 15:24:24 @agent_ppo2.py:192][0m |          -0.0104 |           3.2556 |           0.1516 |
[32m[20230207 15:24:24 @agent_ppo2.py:192][0m |          -0.0104 |           3.5389 |           0.1514 |
[32m[20230207 15:24:24 @agent_ppo2.py:192][0m |          -0.0115 |           2.8752 |           0.1515 |
[32m[20230207 15:24:24 @agent_ppo2.py:192][0m |          -0.0113 |           2.6524 |           0.1515 |
[32m[20230207 15:24:24 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:24:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: 4.91
[32m[20230207 15:24:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 66.80
[32m[20230207 15:24:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 12.54
[32m[20230207 15:24:25 @agent_ppo2.py:150][0m Total time:      23.73 min
[32m[20230207 15:24:25 @agent_ppo2.py:152][0m 1239040 total steps have happened
[32m[20230207 15:24:25 @agent_ppo2.py:128][0m #------------------------ Iteration 605 --------------------------#
[32m[20230207 15:24:25 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:24:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:25 @agent_ppo2.py:192][0m |          -0.0012 |          41.2801 |           0.1482 |
[32m[20230207 15:24:26 @agent_ppo2.py:192][0m |          -0.0037 |          21.1849 |           0.1480 |
[32m[20230207 15:24:26 @agent_ppo2.py:192][0m |          -0.0146 |          15.5678 |           0.1480 |
[32m[20230207 15:24:26 @agent_ppo2.py:192][0m |          -0.0104 |          13.3708 |           0.1479 |
[32m[20230207 15:24:26 @agent_ppo2.py:192][0m |          -0.0140 |          11.2277 |           0.1478 |
[32m[20230207 15:24:26 @agent_ppo2.py:192][0m |          -0.0122 |          10.7302 |           0.1478 |
[32m[20230207 15:24:26 @agent_ppo2.py:192][0m |          -0.0129 |           9.3777 |           0.1477 |
[32m[20230207 15:24:26 @agent_ppo2.py:192][0m |          -0.0175 |           9.2441 |           0.1478 |
[32m[20230207 15:24:26 @agent_ppo2.py:192][0m |          -0.0083 |           8.8679 |           0.1478 |
[32m[20230207 15:24:26 @agent_ppo2.py:192][0m |          -0.0090 |           8.3389 |           0.1477 |
[32m[20230207 15:24:26 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:24:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -76.91
[32m[20230207 15:24:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -16.60
[32m[20230207 15:24:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 46.27
[32m[20230207 15:24:27 @agent_ppo2.py:150][0m Total time:      23.76 min
[32m[20230207 15:24:27 @agent_ppo2.py:152][0m 1241088 total steps have happened
[32m[20230207 15:24:27 @agent_ppo2.py:128][0m #------------------------ Iteration 606 --------------------------#
[32m[20230207 15:24:27 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 15:24:27 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:27 @agent_ppo2.py:192][0m |          -0.0026 |          25.6983 |           0.1515 |
[32m[20230207 15:24:27 @agent_ppo2.py:192][0m |          -0.0089 |           9.2500 |           0.1515 |
[32m[20230207 15:24:28 @agent_ppo2.py:192][0m |          -0.0136 |           7.5018 |           0.1515 |
[32m[20230207 15:24:28 @agent_ppo2.py:192][0m |          -0.0222 |           5.8065 |           0.1514 |
[32m[20230207 15:24:28 @agent_ppo2.py:192][0m |          -0.0182 |           4.3597 |           0.1512 |
[32m[20230207 15:24:28 @agent_ppo2.py:192][0m |          -0.0195 |           3.7397 |           0.1513 |
[32m[20230207 15:24:28 @agent_ppo2.py:192][0m |          -0.0114 |           3.4279 |           0.1510 |
[32m[20230207 15:24:28 @agent_ppo2.py:192][0m |          -0.0182 |           3.1258 |           0.1512 |
[32m[20230207 15:24:28 @agent_ppo2.py:192][0m |          -0.0190 |           2.8726 |           0.1510 |
[32m[20230207 15:24:28 @agent_ppo2.py:192][0m |          -0.0208 |           2.7103 |           0.1511 |
[32m[20230207 15:24:28 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:24:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -72.90
[32m[20230207 15:24:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 20.06
[32m[20230207 15:24:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 36.01
[32m[20230207 15:24:29 @agent_ppo2.py:150][0m Total time:      23.79 min
[32m[20230207 15:24:29 @agent_ppo2.py:152][0m 1243136 total steps have happened
[32m[20230207 15:24:29 @agent_ppo2.py:128][0m #------------------------ Iteration 607 --------------------------#
[32m[20230207 15:24:29 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:24:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:29 @agent_ppo2.py:192][0m |           0.0001 |          20.3464 |           0.1540 |
[32m[20230207 15:24:29 @agent_ppo2.py:192][0m |          -0.0056 |          11.2978 |           0.1540 |
[32m[20230207 15:24:30 @agent_ppo2.py:192][0m |          -0.0077 |           8.7970 |           0.1539 |
[32m[20230207 15:24:30 @agent_ppo2.py:192][0m |          -0.0123 |           7.9550 |           0.1539 |
[32m[20230207 15:24:30 @agent_ppo2.py:192][0m |          -0.0102 |           7.0586 |           0.1538 |
[32m[20230207 15:24:30 @agent_ppo2.py:192][0m |          -0.0092 |           6.5994 |           0.1537 |
[32m[20230207 15:24:30 @agent_ppo2.py:192][0m |          -0.0133 |           6.3482 |           0.1537 |
[32m[20230207 15:24:30 @agent_ppo2.py:192][0m |          -0.0122 |           6.2740 |           0.1536 |
[32m[20230207 15:24:30 @agent_ppo2.py:192][0m |          -0.0162 |           5.7976 |           0.1537 |
[32m[20230207 15:24:30 @agent_ppo2.py:192][0m |          -0.0156 |           5.6080 |           0.1536 |
[32m[20230207 15:24:30 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230207 15:24:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -55.55
[32m[20230207 15:24:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 4.83
[32m[20230207 15:24:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -5.86
[32m[20230207 15:24:31 @agent_ppo2.py:150][0m Total time:      23.82 min
[32m[20230207 15:24:31 @agent_ppo2.py:152][0m 1245184 total steps have happened
[32m[20230207 15:24:31 @agent_ppo2.py:128][0m #------------------------ Iteration 608 --------------------------#
[32m[20230207 15:24:31 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:24:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:31 @agent_ppo2.py:192][0m |           0.0029 |          20.3793 |           0.1547 |
[32m[20230207 15:24:31 @agent_ppo2.py:192][0m |           0.0057 |           8.8453 |           0.1545 |
[32m[20230207 15:24:31 @agent_ppo2.py:192][0m |          -0.0146 |           6.0224 |           0.1544 |
[32m[20230207 15:24:31 @agent_ppo2.py:192][0m |          -0.0081 |           4.8369 |           0.1543 |
[32m[20230207 15:24:32 @agent_ppo2.py:192][0m |           0.0043 |           4.3195 |           0.1544 |
[32m[20230207 15:24:32 @agent_ppo2.py:192][0m |          -0.0060 |           3.9994 |           0.1540 |
[32m[20230207 15:24:32 @agent_ppo2.py:192][0m |          -0.0163 |           3.9217 |           0.1541 |
[32m[20230207 15:24:32 @agent_ppo2.py:192][0m |          -0.0050 |           3.8990 |           0.1542 |
[32m[20230207 15:24:32 @agent_ppo2.py:192][0m |          -0.0135 |           3.5416 |           0.1541 |
[32m[20230207 15:24:32 @agent_ppo2.py:192][0m |          -0.0176 |           3.1999 |           0.1542 |
[32m[20230207 15:24:32 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230207 15:24:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: -50.30
[32m[20230207 15:24:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 44.17
[32m[20230207 15:24:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 90.05
[32m[20230207 15:24:33 @agent_ppo2.py:150][0m Total time:      23.86 min
[32m[20230207 15:24:33 @agent_ppo2.py:152][0m 1247232 total steps have happened
[32m[20230207 15:24:33 @agent_ppo2.py:128][0m #------------------------ Iteration 609 --------------------------#
[32m[20230207 15:24:34 @agent_ppo2.py:134][0m Sampling time: 0.91 s by 1 slaves
[32m[20230207 15:24:34 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:34 @agent_ppo2.py:192][0m |           0.0010 |          22.5906 |           0.1547 |
[32m[20230207 15:24:34 @agent_ppo2.py:192][0m |          -0.0030 |           9.6227 |           0.1548 |
[32m[20230207 15:24:34 @agent_ppo2.py:192][0m |          -0.0050 |           7.0414 |           0.1546 |
[32m[20230207 15:24:34 @agent_ppo2.py:192][0m |          -0.0068 |           6.0213 |           0.1548 |
[32m[20230207 15:24:34 @agent_ppo2.py:192][0m |          -0.0084 |           5.3102 |           0.1547 |
[32m[20230207 15:24:34 @agent_ppo2.py:192][0m |          -0.0094 |           4.8553 |           0.1548 |
[32m[20230207 15:24:34 @agent_ppo2.py:192][0m |          -0.0095 |           4.5967 |           0.1547 |
[32m[20230207 15:24:34 @agent_ppo2.py:192][0m |          -0.0109 |           4.3095 |           0.1546 |
[32m[20230207 15:24:34 @agent_ppo2.py:192][0m |          -0.0117 |           4.1338 |           0.1546 |
[32m[20230207 15:24:34 @agent_ppo2.py:192][0m |          -0.0113 |           3.9762 |           0.1546 |
[32m[20230207 15:24:34 @agent_ppo2.py:137][0m Policy update time: 0.92 s
[32m[20230207 15:24:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: -60.62
[32m[20230207 15:24:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 37.90
[32m[20230207 15:24:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -11.01
[32m[20230207 15:24:35 @agent_ppo2.py:150][0m Total time:      23.90 min
[32m[20230207 15:24:35 @agent_ppo2.py:152][0m 1249280 total steps have happened
[32m[20230207 15:24:35 @agent_ppo2.py:128][0m #------------------------ Iteration 610 --------------------------#
[32m[20230207 15:24:36 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:24:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:36 @agent_ppo2.py:192][0m |           0.0006 |           2.2204 |           0.1512 |
[32m[20230207 15:24:36 @agent_ppo2.py:192][0m |           0.0128 |           1.7231 |           0.1512 |
[32m[20230207 15:24:36 @agent_ppo2.py:192][0m |          -0.0089 |           1.6181 |           0.1510 |
[32m[20230207 15:24:37 @agent_ppo2.py:192][0m |          -0.0075 |           1.5546 |           0.1509 |
[32m[20230207 15:24:37 @agent_ppo2.py:192][0m |          -0.0357 |           1.5816 |           0.1509 |
[32m[20230207 15:24:37 @agent_ppo2.py:192][0m |          -0.0075 |           1.4908 |           0.1510 |
[32m[20230207 15:24:37 @agent_ppo2.py:192][0m |          -0.0031 |           1.4540 |           0.1510 |
[32m[20230207 15:24:37 @agent_ppo2.py:192][0m |          -0.0113 |           1.4260 |           0.1512 |
[32m[20230207 15:24:37 @agent_ppo2.py:192][0m |          -0.0223 |           1.4109 |           0.1511 |
[32m[20230207 15:24:37 @agent_ppo2.py:192][0m |          -0.0149 |           1.3929 |           0.1511 |
[32m[20230207 15:24:37 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:24:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: 69.70
[32m[20230207 15:24:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 87.96
[32m[20230207 15:24:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -84.19
[32m[20230207 15:24:38 @agent_ppo2.py:150][0m Total time:      23.94 min
[32m[20230207 15:24:38 @agent_ppo2.py:152][0m 1251328 total steps have happened
[32m[20230207 15:24:38 @agent_ppo2.py:128][0m #------------------------ Iteration 611 --------------------------#
[32m[20230207 15:24:38 @agent_ppo2.py:134][0m Sampling time: 0.64 s by 1 slaves
[32m[20230207 15:24:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:38 @agent_ppo2.py:192][0m |           0.0055 |          19.5122 |           0.1551 |
[32m[20230207 15:24:38 @agent_ppo2.py:192][0m |           0.0046 |           5.8071 |           0.1551 |
[32m[20230207 15:24:38 @agent_ppo2.py:192][0m |          -0.0053 |           4.1504 |           0.1551 |
[32m[20230207 15:24:38 @agent_ppo2.py:192][0m |          -0.0079 |           3.6067 |           0.1553 |
[32m[20230207 15:24:39 @agent_ppo2.py:192][0m |          -0.0037 |           3.4350 |           0.1553 |
[32m[20230207 15:24:39 @agent_ppo2.py:192][0m |          -0.0100 |           3.2667 |           0.1553 |
[32m[20230207 15:24:39 @agent_ppo2.py:192][0m |          -0.0100 |           3.1834 |           0.1554 |
[32m[20230207 15:24:39 @agent_ppo2.py:192][0m |          -0.0104 |           3.0895 |           0.1555 |
[32m[20230207 15:24:39 @agent_ppo2.py:192][0m |          -0.0066 |           3.0328 |           0.1555 |
[32m[20230207 15:24:39 @agent_ppo2.py:192][0m |          -0.0109 |           2.9436 |           0.1555 |
[32m[20230207 15:24:39 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230207 15:24:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: 33.77
[32m[20230207 15:24:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 76.57
[32m[20230207 15:24:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -53.04
[32m[20230207 15:24:39 @agent_ppo2.py:150][0m Total time:      23.97 min
[32m[20230207 15:24:39 @agent_ppo2.py:152][0m 1253376 total steps have happened
[32m[20230207 15:24:39 @agent_ppo2.py:128][0m #------------------------ Iteration 612 --------------------------#
[32m[20230207 15:24:40 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:24:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:40 @agent_ppo2.py:192][0m |           0.0010 |           9.8626 |           0.1556 |
[32m[20230207 15:24:41 @agent_ppo2.py:192][0m |          -0.0045 |           7.7637 |           0.1554 |
[32m[20230207 15:24:41 @agent_ppo2.py:192][0m |          -0.0120 |           7.3223 |           0.1553 |
[32m[20230207 15:24:41 @agent_ppo2.py:192][0m |          -0.0040 |           7.2492 |           0.1552 |
[32m[20230207 15:24:41 @agent_ppo2.py:192][0m |          -0.0095 |           6.9018 |           0.1550 |
[32m[20230207 15:24:41 @agent_ppo2.py:192][0m |          -0.0152 |           6.7696 |           0.1550 |
[32m[20230207 15:24:41 @agent_ppo2.py:192][0m |          -0.0132 |           6.6531 |           0.1549 |
[32m[20230207 15:24:41 @agent_ppo2.py:192][0m |          -0.0126 |           6.5678 |           0.1548 |
[32m[20230207 15:24:41 @agent_ppo2.py:192][0m |          -0.0143 |           6.4728 |           0.1547 |
[32m[20230207 15:24:41 @agent_ppo2.py:192][0m |          -0.0176 |           6.3909 |           0.1547 |
[32m[20230207 15:24:41 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:24:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: 0.60
[32m[20230207 15:24:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 67.20
[32m[20230207 15:24:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 1.05
[32m[20230207 15:24:42 @agent_ppo2.py:150][0m Total time:      24.01 min
[32m[20230207 15:24:42 @agent_ppo2.py:152][0m 1255424 total steps have happened
[32m[20230207 15:24:42 @agent_ppo2.py:128][0m #------------------------ Iteration 613 --------------------------#
[32m[20230207 15:24:43 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:24:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:43 @agent_ppo2.py:192][0m |           0.0031 |           8.3752 |           0.1543 |
[32m[20230207 15:24:43 @agent_ppo2.py:192][0m |          -0.0013 |           5.4522 |           0.1542 |
[32m[20230207 15:24:43 @agent_ppo2.py:192][0m |          -0.0056 |           4.6668 |           0.1543 |
[32m[20230207 15:24:43 @agent_ppo2.py:192][0m |          -0.0288 |           4.3739 |           0.1540 |
[32m[20230207 15:24:43 @agent_ppo2.py:192][0m |           0.0018 |           4.0586 |           0.1539 |
[32m[20230207 15:24:43 @agent_ppo2.py:192][0m |          -0.0006 |           3.6473 |           0.1542 |
[32m[20230207 15:24:43 @agent_ppo2.py:192][0m |          -0.0107 |           3.4252 |           0.1541 |
[32m[20230207 15:24:43 @agent_ppo2.py:192][0m |           0.0000 |           3.3248 |           0.1542 |
[32m[20230207 15:24:44 @agent_ppo2.py:192][0m |          -0.0355 |           3.3506 |           0.1542 |
[32m[20230207 15:24:44 @agent_ppo2.py:192][0m |          -0.0137 |           3.2060 |           0.1541 |
[32m[20230207 15:24:44 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:24:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: 67.62
[32m[20230207 15:24:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 93.71
[32m[20230207 15:24:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 51.04
[32m[20230207 15:24:44 @agent_ppo2.py:150][0m Total time:      24.05 min
[32m[20230207 15:24:44 @agent_ppo2.py:152][0m 1257472 total steps have happened
[32m[20230207 15:24:44 @agent_ppo2.py:128][0m #------------------------ Iteration 614 --------------------------#
[32m[20230207 15:24:45 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230207 15:24:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:45 @agent_ppo2.py:192][0m |          -0.0044 |          20.2264 |           0.1584 |
[32m[20230207 15:24:45 @agent_ppo2.py:192][0m |          -0.0095 |           8.5900 |           0.1583 |
[32m[20230207 15:24:45 @agent_ppo2.py:192][0m |          -0.0130 |           6.6744 |           0.1582 |
[32m[20230207 15:24:45 @agent_ppo2.py:192][0m |          -0.0131 |           5.5725 |           0.1582 |
[32m[20230207 15:24:45 @agent_ppo2.py:192][0m |          -0.0209 |           5.4562 |           0.1582 |
[32m[20230207 15:24:45 @agent_ppo2.py:192][0m |          -0.0121 |           5.0591 |           0.1581 |
[32m[20230207 15:24:45 @agent_ppo2.py:192][0m |          -0.0194 |           4.8929 |           0.1581 |
[32m[20230207 15:24:45 @agent_ppo2.py:192][0m |          -0.0175 |           4.7155 |           0.1581 |
[32m[20230207 15:24:46 @agent_ppo2.py:192][0m |          -0.0254 |           4.4626 |           0.1581 |
[32m[20230207 15:24:46 @agent_ppo2.py:192][0m |          -0.0130 |           4.2899 |           0.1580 |
[32m[20230207 15:24:46 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230207 15:24:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -77.00
[32m[20230207 15:24:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -34.34
[32m[20230207 15:24:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 39.35
[32m[20230207 15:24:46 @agent_ppo2.py:150][0m Total time:      24.09 min
[32m[20230207 15:24:46 @agent_ppo2.py:152][0m 1259520 total steps have happened
[32m[20230207 15:24:46 @agent_ppo2.py:128][0m #------------------------ Iteration 615 --------------------------#
[32m[20230207 15:24:47 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:24:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:47 @agent_ppo2.py:192][0m |           0.0008 |          13.4068 |           0.1582 |
[32m[20230207 15:24:47 @agent_ppo2.py:192][0m |          -0.0020 |          12.0152 |           0.1580 |
[32m[20230207 15:24:48 @agent_ppo2.py:192][0m |          -0.0041 |          11.4819 |           0.1580 |
[32m[20230207 15:24:48 @agent_ppo2.py:192][0m |          -0.0050 |          11.2583 |           0.1579 |
[32m[20230207 15:24:48 @agent_ppo2.py:192][0m |          -0.0051 |          11.1517 |           0.1578 |
[32m[20230207 15:24:48 @agent_ppo2.py:192][0m |          -0.0068 |          10.9252 |           0.1577 |
[32m[20230207 15:24:48 @agent_ppo2.py:192][0m |          -0.0067 |          10.8640 |           0.1576 |
[32m[20230207 15:24:48 @agent_ppo2.py:192][0m |          -0.0076 |          10.6383 |           0.1575 |
[32m[20230207 15:24:48 @agent_ppo2.py:192][0m |          -0.0094 |          10.5709 |           0.1574 |
[32m[20230207 15:24:48 @agent_ppo2.py:192][0m |          -0.0087 |          10.4707 |           0.1574 |
[32m[20230207 15:24:48 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:24:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: -7.46
[32m[20230207 15:24:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 73.59
[32m[20230207 15:24:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -12.84
[32m[20230207 15:24:49 @agent_ppo2.py:150][0m Total time:      24.12 min
[32m[20230207 15:24:49 @agent_ppo2.py:152][0m 1261568 total steps have happened
[32m[20230207 15:24:49 @agent_ppo2.py:128][0m #------------------------ Iteration 616 --------------------------#
[32m[20230207 15:24:49 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:24:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:50 @agent_ppo2.py:192][0m |           0.0070 |          10.5997 |           0.1525 |
[32m[20230207 15:24:50 @agent_ppo2.py:192][0m |          -0.0049 |           6.4259 |           0.1526 |
[32m[20230207 15:24:50 @agent_ppo2.py:192][0m |          -0.0064 |           5.3956 |           0.1526 |
[32m[20230207 15:24:50 @agent_ppo2.py:192][0m |          -0.0184 |           5.0031 |           0.1526 |
[32m[20230207 15:24:50 @agent_ppo2.py:192][0m |          -0.0069 |           4.7933 |           0.1527 |
[32m[20230207 15:24:50 @agent_ppo2.py:192][0m |          -0.0034 |           4.5707 |           0.1526 |
[32m[20230207 15:24:50 @agent_ppo2.py:192][0m |          -0.0083 |           4.3699 |           0.1526 |
[32m[20230207 15:24:50 @agent_ppo2.py:192][0m |          -0.0071 |           4.2737 |           0.1526 |
[32m[20230207 15:24:50 @agent_ppo2.py:192][0m |          -0.0074 |           4.0892 |           0.1526 |
[32m[20230207 15:24:50 @agent_ppo2.py:192][0m |          -0.0079 |           3.9085 |           0.1526 |
[32m[20230207 15:24:50 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:24:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: -38.09
[32m[20230207 15:24:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 19.44
[32m[20230207 15:24:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -20.77
[32m[20230207 15:24:51 @agent_ppo2.py:150][0m Total time:      24.16 min
[32m[20230207 15:24:51 @agent_ppo2.py:152][0m 1263616 total steps have happened
[32m[20230207 15:24:51 @agent_ppo2.py:128][0m #------------------------ Iteration 617 --------------------------#
[32m[20230207 15:24:52 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:24:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:52 @agent_ppo2.py:192][0m |          -0.0018 |          34.6327 |           0.1542 |
[32m[20230207 15:24:52 @agent_ppo2.py:192][0m |          -0.0052 |          17.7257 |           0.1544 |
[32m[20230207 15:24:52 @agent_ppo2.py:192][0m |          -0.0068 |          15.3407 |           0.1543 |
[32m[20230207 15:24:52 @agent_ppo2.py:192][0m |          -0.0109 |          13.3113 |           0.1542 |
[32m[20230207 15:24:52 @agent_ppo2.py:192][0m |          -0.0090 |          12.0321 |           0.1541 |
[32m[20230207 15:24:52 @agent_ppo2.py:192][0m |          -0.0105 |          11.2262 |           0.1542 |
[32m[20230207 15:24:52 @agent_ppo2.py:192][0m |          -0.0138 |          10.4417 |           0.1541 |
[32m[20230207 15:24:52 @agent_ppo2.py:192][0m |          -0.0130 |           9.8916 |           0.1541 |
[32m[20230207 15:24:52 @agent_ppo2.py:192][0m |          -0.0126 |           9.3482 |           0.1541 |
[32m[20230207 15:24:52 @agent_ppo2.py:192][0m |          -0.0153 |           8.8097 |           0.1541 |
[32m[20230207 15:24:52 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230207 15:24:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: -75.95
[32m[20230207 15:24:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -0.68
[32m[20230207 15:24:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 45.72
[32m[20230207 15:24:53 @agent_ppo2.py:150][0m Total time:      24.20 min
[32m[20230207 15:24:53 @agent_ppo2.py:152][0m 1265664 total steps have happened
[32m[20230207 15:24:53 @agent_ppo2.py:128][0m #------------------------ Iteration 618 --------------------------#
[32m[20230207 15:24:54 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:24:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:54 @agent_ppo2.py:192][0m |           0.0193 |           9.1822 |           0.1583 |
[32m[20230207 15:24:54 @agent_ppo2.py:192][0m |          -0.0067 |           5.7394 |           0.1582 |
[32m[20230207 15:24:54 @agent_ppo2.py:192][0m |           0.0005 |           5.0544 |           0.1581 |
[32m[20230207 15:24:54 @agent_ppo2.py:192][0m |          -0.0030 |           4.6863 |           0.1581 |
[32m[20230207 15:24:54 @agent_ppo2.py:192][0m |          -0.0059 |           4.2681 |           0.1583 |
[32m[20230207 15:24:55 @agent_ppo2.py:192][0m |           0.0134 |           4.1389 |           0.1582 |
[32m[20230207 15:24:55 @agent_ppo2.py:192][0m |          -0.0127 |           3.9379 |           0.1582 |
[32m[20230207 15:24:55 @agent_ppo2.py:192][0m |          -0.0102 |           3.7545 |           0.1582 |
[32m[20230207 15:24:55 @agent_ppo2.py:192][0m |          -0.0094 |           3.5468 |           0.1582 |
[32m[20230207 15:24:55 @agent_ppo2.py:192][0m |          -0.0130 |           3.5143 |           0.1582 |
[32m[20230207 15:24:55 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:24:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: 46.43
[32m[20230207 15:24:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 51.98
[32m[20230207 15:24:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 104.89
[32m[20230207 15:24:56 @agent_ppo2.py:150][0m Total time:      24.24 min
[32m[20230207 15:24:56 @agent_ppo2.py:152][0m 1267712 total steps have happened
[32m[20230207 15:24:56 @agent_ppo2.py:128][0m #------------------------ Iteration 619 --------------------------#
[32m[20230207 15:24:56 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 15:24:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:56 @agent_ppo2.py:192][0m |          -0.0088 |          13.6771 |           0.1546 |
[32m[20230207 15:24:56 @agent_ppo2.py:192][0m |          -0.0140 |           7.3450 |           0.1548 |
[32m[20230207 15:24:56 @agent_ppo2.py:192][0m |          -0.0160 |           5.6425 |           0.1546 |
[32m[20230207 15:24:56 @agent_ppo2.py:192][0m |          -0.0150 |           5.3077 |           0.1546 |
[32m[20230207 15:24:56 @agent_ppo2.py:192][0m |          -0.0131 |           4.9631 |           0.1547 |
[32m[20230207 15:24:57 @agent_ppo2.py:192][0m |          -0.0128 |           4.7177 |           0.1547 |
[32m[20230207 15:24:57 @agent_ppo2.py:192][0m |          -0.0099 |           4.6023 |           0.1547 |
[32m[20230207 15:24:57 @agent_ppo2.py:192][0m |          -0.0177 |           4.2895 |           0.1547 |
[32m[20230207 15:24:57 @agent_ppo2.py:192][0m |          -0.0186 |           4.1850 |           0.1547 |
[32m[20230207 15:24:57 @agent_ppo2.py:192][0m |          -0.0136 |           4.0776 |           0.1547 |
[32m[20230207 15:24:57 @agent_ppo2.py:137][0m Policy update time: 0.58 s
[32m[20230207 15:24:57 @agent_ppo2.py:145][0m Average TRAINING episode reward: -44.59
[32m[20230207 15:24:57 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 86.05
[32m[20230207 15:24:57 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -64.23
[32m[20230207 15:24:57 @agent_ppo2.py:150][0m Total time:      24.27 min
[32m[20230207 15:24:57 @agent_ppo2.py:152][0m 1269760 total steps have happened
[32m[20230207 15:24:57 @agent_ppo2.py:128][0m #------------------------ Iteration 620 --------------------------#
[32m[20230207 15:24:58 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:24:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:24:58 @agent_ppo2.py:192][0m |           0.0133 |          11.5016 |           0.1575 |
[32m[20230207 15:24:58 @agent_ppo2.py:192][0m |          -0.0031 |           8.1594 |           0.1575 |
[32m[20230207 15:24:58 @agent_ppo2.py:192][0m |          -0.0244 |           7.0599 |           0.1572 |
[32m[20230207 15:24:58 @agent_ppo2.py:192][0m |          -0.0121 |           6.1435 |           0.1574 |
[32m[20230207 15:24:59 @agent_ppo2.py:192][0m |          -0.0012 |           5.4438 |           0.1573 |
[32m[20230207 15:24:59 @agent_ppo2.py:192][0m |          -0.0039 |           5.0239 |           0.1572 |
[32m[20230207 15:24:59 @agent_ppo2.py:192][0m |          -0.0053 |           4.9084 |           0.1573 |
[32m[20230207 15:24:59 @agent_ppo2.py:192][0m |          -0.0355 |           4.6076 |           0.1570 |
[32m[20230207 15:24:59 @agent_ppo2.py:192][0m |          -0.0582 |           4.6409 |           0.1565 |
[32m[20230207 15:24:59 @agent_ppo2.py:192][0m |          -0.0185 |           4.5206 |           0.1569 |
[32m[20230207 15:24:59 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:25:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: 101.21
[32m[20230207 15:25:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 123.38
[32m[20230207 15:25:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -38.36
[32m[20230207 15:25:00 @agent_ppo2.py:150][0m Total time:      24.31 min
[32m[20230207 15:25:00 @agent_ppo2.py:152][0m 1271808 total steps have happened
[32m[20230207 15:25:00 @agent_ppo2.py:128][0m #------------------------ Iteration 621 --------------------------#
[32m[20230207 15:25:00 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:25:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:00 @agent_ppo2.py:192][0m |          -0.0030 |           8.3811 |           0.1538 |
[32m[20230207 15:25:01 @agent_ppo2.py:192][0m |          -0.0048 |           4.6892 |           0.1539 |
[32m[20230207 15:25:01 @agent_ppo2.py:192][0m |          -0.0066 |           4.0629 |           0.1538 |
[32m[20230207 15:25:01 @agent_ppo2.py:192][0m |          -0.0071 |           3.6855 |           0.1538 |
[32m[20230207 15:25:01 @agent_ppo2.py:192][0m |          -0.0193 |           3.4923 |           0.1537 |
[32m[20230207 15:25:01 @agent_ppo2.py:192][0m |          -0.0210 |           3.3459 |           0.1538 |
[32m[20230207 15:25:01 @agent_ppo2.py:192][0m |          -0.0064 |           3.2161 |           0.1537 |
[32m[20230207 15:25:01 @agent_ppo2.py:192][0m |          -0.0114 |           3.0557 |           0.1537 |
[32m[20230207 15:25:01 @agent_ppo2.py:192][0m |          -0.0086 |           2.9747 |           0.1537 |
[32m[20230207 15:25:01 @agent_ppo2.py:192][0m |          -0.0107 |           2.8888 |           0.1537 |
[32m[20230207 15:25:01 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:25:02 @agent_ppo2.py:145][0m Average TRAINING episode reward: 45.56
[32m[20230207 15:25:02 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 165.73
[32m[20230207 15:25:02 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -6.30
[32m[20230207 15:25:02 @agent_ppo2.py:150][0m Total time:      24.35 min
[32m[20230207 15:25:02 @agent_ppo2.py:152][0m 1273856 total steps have happened
[32m[20230207 15:25:02 @agent_ppo2.py:128][0m #------------------------ Iteration 622 --------------------------#
[32m[20230207 15:25:03 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:25:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:03 @agent_ppo2.py:192][0m |          -0.0108 |           2.2524 |           0.1549 |
[32m[20230207 15:25:03 @agent_ppo2.py:192][0m |          -0.0116 |           1.6234 |           0.1550 |
[32m[20230207 15:25:03 @agent_ppo2.py:192][0m |          -0.0027 |           1.4686 |           0.1548 |
[32m[20230207 15:25:03 @agent_ppo2.py:192][0m |          -0.0309 |           1.3919 |           0.1548 |
[32m[20230207 15:25:03 @agent_ppo2.py:192][0m |           0.0064 |           1.3585 |           0.1546 |
[32m[20230207 15:25:03 @agent_ppo2.py:192][0m |           0.0023 |           1.3039 |           0.1546 |
[32m[20230207 15:25:03 @agent_ppo2.py:192][0m |          -0.0161 |           1.2700 |           0.1547 |
[32m[20230207 15:25:03 @agent_ppo2.py:192][0m |          -0.0188 |           1.2462 |           0.1547 |
[32m[20230207 15:25:03 @agent_ppo2.py:192][0m |          -0.0002 |           1.2250 |           0.1547 |
[32m[20230207 15:25:04 @agent_ppo2.py:192][0m |          -0.0051 |           1.2072 |           0.1548 |
[32m[20230207 15:25:04 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:25:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: 75.34
[32m[20230207 15:25:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 145.43
[32m[20230207 15:25:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -23.34
[32m[20230207 15:25:04 @agent_ppo2.py:150][0m Total time:      24.39 min
[32m[20230207 15:25:04 @agent_ppo2.py:152][0m 1275904 total steps have happened
[32m[20230207 15:25:04 @agent_ppo2.py:128][0m #------------------------ Iteration 623 --------------------------#
[32m[20230207 15:25:05 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:25:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:05 @agent_ppo2.py:192][0m |          -0.0015 |           7.1961 |           0.1599 |
[32m[20230207 15:25:05 @agent_ppo2.py:192][0m |          -0.0049 |           3.6967 |           0.1596 |
[32m[20230207 15:25:05 @agent_ppo2.py:192][0m |          -0.0076 |           2.8791 |           0.1595 |
[32m[20230207 15:25:05 @agent_ppo2.py:192][0m |          -0.0103 |           2.6724 |           0.1593 |
[32m[20230207 15:25:05 @agent_ppo2.py:192][0m |          -0.0111 |           2.4151 |           0.1593 |
[32m[20230207 15:25:05 @agent_ppo2.py:192][0m |          -0.0125 |           2.2805 |           0.1592 |
[32m[20230207 15:25:05 @agent_ppo2.py:192][0m |          -0.0126 |           2.1530 |           0.1591 |
[32m[20230207 15:25:06 @agent_ppo2.py:192][0m |          -0.0140 |           2.0895 |           0.1591 |
[32m[20230207 15:25:06 @agent_ppo2.py:192][0m |          -0.0144 |           2.0308 |           0.1590 |
[32m[20230207 15:25:06 @agent_ppo2.py:192][0m |          -0.0150 |           1.9047 |           0.1589 |
[32m[20230207 15:25:06 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230207 15:25:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: -29.19
[32m[20230207 15:25:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -25.99
[32m[20230207 15:25:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -12.09
[32m[20230207 15:25:06 @agent_ppo2.py:150][0m Total time:      24.42 min
[32m[20230207 15:25:06 @agent_ppo2.py:152][0m 1277952 total steps have happened
[32m[20230207 15:25:06 @agent_ppo2.py:128][0m #------------------------ Iteration 624 --------------------------#
[32m[20230207 15:25:07 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:25:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:07 @agent_ppo2.py:192][0m |           0.0073 |           3.3756 |           0.1552 |
[32m[20230207 15:25:07 @agent_ppo2.py:192][0m |          -0.0045 |           2.2388 |           0.1548 |
[32m[20230207 15:25:07 @agent_ppo2.py:192][0m |           0.0089 |           2.0914 |           0.1549 |
[32m[20230207 15:25:07 @agent_ppo2.py:192][0m |          -0.0099 |           1.9737 |           0.1551 |
[32m[20230207 15:25:07 @agent_ppo2.py:192][0m |          -0.0089 |           1.9024 |           0.1551 |
[32m[20230207 15:25:07 @agent_ppo2.py:192][0m |           0.0198 |           1.8280 |           0.1549 |
[32m[20230207 15:25:08 @agent_ppo2.py:192][0m |          -0.0126 |           1.8120 |           0.1548 |
[32m[20230207 15:25:08 @agent_ppo2.py:192][0m |          -0.0054 |           1.7537 |           0.1548 |
[32m[20230207 15:25:08 @agent_ppo2.py:192][0m |          -0.0158 |           1.7266 |           0.1547 |
[32m[20230207 15:25:08 @agent_ppo2.py:192][0m |          -0.0063 |           1.7043 |           0.1546 |
[32m[20230207 15:25:08 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:25:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: 40.45
[32m[20230207 15:25:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 41.82
[32m[20230207 15:25:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -68.09
[32m[20230207 15:25:08 @agent_ppo2.py:150][0m Total time:      24.45 min
[32m[20230207 15:25:08 @agent_ppo2.py:152][0m 1280000 total steps have happened
[32m[20230207 15:25:08 @agent_ppo2.py:128][0m #------------------------ Iteration 625 --------------------------#
[32m[20230207 15:25:09 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:25:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:09 @agent_ppo2.py:192][0m |           0.0017 |          10.1889 |           0.1568 |
[32m[20230207 15:25:09 @agent_ppo2.py:192][0m |          -0.0047 |           9.3991 |           0.1567 |
[32m[20230207 15:25:10 @agent_ppo2.py:192][0m |          -0.0057 |           9.2608 |           0.1568 |
[32m[20230207 15:25:10 @agent_ppo2.py:192][0m |          -0.0056 |           9.0995 |           0.1566 |
[32m[20230207 15:25:10 @agent_ppo2.py:192][0m |          -0.0096 |           9.0170 |           0.1565 |
[32m[20230207 15:25:10 @agent_ppo2.py:192][0m |          -0.0046 |           9.2741 |           0.1565 |
[32m[20230207 15:25:10 @agent_ppo2.py:192][0m |          -0.0096 |           8.8006 |           0.1564 |
[32m[20230207 15:25:10 @agent_ppo2.py:192][0m |          -0.0110 |           8.7076 |           0.1564 |
[32m[20230207 15:25:10 @agent_ppo2.py:192][0m |          -0.0124 |           8.5947 |           0.1565 |
[32m[20230207 15:25:10 @agent_ppo2.py:192][0m |          -0.0121 |           8.5173 |           0.1564 |
[32m[20230207 15:25:10 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:25:11 @agent_ppo2.py:145][0m Average TRAINING episode reward: 26.79
[32m[20230207 15:25:11 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 95.92
[32m[20230207 15:25:11 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -41.00
[32m[20230207 15:25:11 @agent_ppo2.py:150][0m Total time:      24.49 min
[32m[20230207 15:25:11 @agent_ppo2.py:152][0m 1282048 total steps have happened
[32m[20230207 15:25:11 @agent_ppo2.py:128][0m #------------------------ Iteration 626 --------------------------#
[32m[20230207 15:25:12 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:25:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:12 @agent_ppo2.py:192][0m |           0.0056 |           2.4256 |           0.1553 |
[32m[20230207 15:25:12 @agent_ppo2.py:192][0m |          -0.0040 |           1.7057 |           0.1553 |
[32m[20230207 15:25:12 @agent_ppo2.py:192][0m |           0.0047 |           1.6455 |           0.1552 |
[32m[20230207 15:25:12 @agent_ppo2.py:192][0m |          -0.0157 |           1.6084 |           0.1551 |
[32m[20230207 15:25:12 @agent_ppo2.py:192][0m |          -0.0017 |           1.5719 |           0.1550 |
[32m[20230207 15:25:12 @agent_ppo2.py:192][0m |          -0.0124 |           1.5554 |           0.1550 |
[32m[20230207 15:25:12 @agent_ppo2.py:192][0m |          -0.0064 |           1.5393 |           0.1550 |
[32m[20230207 15:25:12 @agent_ppo2.py:192][0m |          -0.0282 |           1.5339 |           0.1551 |
[32m[20230207 15:25:12 @agent_ppo2.py:192][0m |          -0.0130 |           1.5087 |           0.1546 |
[32m[20230207 15:25:12 @agent_ppo2.py:192][0m |          -0.0135 |           1.4922 |           0.1549 |
[32m[20230207 15:25:12 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:25:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: 43.37
[32m[20230207 15:25:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 44.40
[32m[20230207 15:25:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -75.97
[32m[20230207 15:25:13 @agent_ppo2.py:150][0m Total time:      24.54 min
[32m[20230207 15:25:13 @agent_ppo2.py:152][0m 1284096 total steps have happened
[32m[20230207 15:25:13 @agent_ppo2.py:128][0m #------------------------ Iteration 627 --------------------------#
[32m[20230207 15:25:14 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:25:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:14 @agent_ppo2.py:192][0m |           0.0028 |           9.4211 |           0.1590 |
[32m[20230207 15:25:14 @agent_ppo2.py:192][0m |          -0.0051 |           3.6296 |           0.1589 |
[32m[20230207 15:25:14 @agent_ppo2.py:192][0m |          -0.0080 |           3.3224 |           0.1587 |
[32m[20230207 15:25:15 @agent_ppo2.py:192][0m |          -0.0100 |           3.1607 |           0.1586 |
[32m[20230207 15:25:15 @agent_ppo2.py:192][0m |          -0.0105 |           3.0416 |           0.1587 |
[32m[20230207 15:25:15 @agent_ppo2.py:192][0m |          -0.0120 |           2.9503 |           0.1587 |
[32m[20230207 15:25:15 @agent_ppo2.py:192][0m |          -0.0130 |           2.8867 |           0.1586 |
[32m[20230207 15:25:15 @agent_ppo2.py:192][0m |          -0.0138 |           2.8361 |           0.1586 |
[32m[20230207 15:25:15 @agent_ppo2.py:192][0m |          -0.0139 |           2.7977 |           0.1585 |
[32m[20230207 15:25:15 @agent_ppo2.py:192][0m |          -0.0141 |           2.7717 |           0.1585 |
[32m[20230207 15:25:15 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:25:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -37.15
[32m[20230207 15:25:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 54.50
[32m[20230207 15:25:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -30.16
[32m[20230207 15:25:16 @agent_ppo2.py:150][0m Total time:      24.58 min
[32m[20230207 15:25:16 @agent_ppo2.py:152][0m 1286144 total steps have happened
[32m[20230207 15:25:16 @agent_ppo2.py:128][0m #------------------------ Iteration 628 --------------------------#
[32m[20230207 15:25:17 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:25:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:17 @agent_ppo2.py:192][0m |          -0.0083 |           4.4439 |           0.1553 |
[32m[20230207 15:25:17 @agent_ppo2.py:192][0m |          -0.0020 |           3.4503 |           0.1551 |
[32m[20230207 15:25:17 @agent_ppo2.py:192][0m |          -0.0061 |           3.0481 |           0.1548 |
[32m[20230207 15:25:17 @agent_ppo2.py:192][0m |           0.0071 |           2.9273 |           0.1547 |
[32m[20230207 15:25:17 @agent_ppo2.py:192][0m |          -0.0036 |           2.7885 |           0.1545 |
[32m[20230207 15:25:17 @agent_ppo2.py:192][0m |          -0.0225 |           2.8097 |           0.1545 |
[32m[20230207 15:25:17 @agent_ppo2.py:192][0m |          -0.0101 |           2.6754 |           0.1542 |
[32m[20230207 15:25:17 @agent_ppo2.py:192][0m |          -0.0126 |           2.5792 |           0.1543 |
[32m[20230207 15:25:17 @agent_ppo2.py:192][0m |           0.0078 |           2.6104 |           0.1544 |
[32m[20230207 15:25:17 @agent_ppo2.py:192][0m |          -0.0042 |           2.6150 |           0.1543 |
[32m[20230207 15:25:17 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:25:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: 34.51
[32m[20230207 15:25:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 73.14
[32m[20230207 15:25:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 68.16
[32m[20230207 15:25:18 @agent_ppo2.py:150][0m Total time:      24.62 min
[32m[20230207 15:25:18 @agent_ppo2.py:152][0m 1288192 total steps have happened
[32m[20230207 15:25:18 @agent_ppo2.py:128][0m #------------------------ Iteration 629 --------------------------#
[32m[20230207 15:25:19 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:25:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:19 @agent_ppo2.py:192][0m |           0.0021 |          11.1898 |           0.1578 |
[32m[20230207 15:25:19 @agent_ppo2.py:192][0m |          -0.0130 |           6.7590 |           0.1578 |
[32m[20230207 15:25:19 @agent_ppo2.py:192][0m |          -0.0089 |           5.7523 |           0.1576 |
[32m[20230207 15:25:19 @agent_ppo2.py:192][0m |          -0.0126 |           5.4561 |           0.1576 |
[32m[20230207 15:25:20 @agent_ppo2.py:192][0m |          -0.0113 |           5.6818 |           0.1577 |
[32m[20230207 15:25:20 @agent_ppo2.py:192][0m |          -0.0145 |           5.0259 |           0.1575 |
[32m[20230207 15:25:20 @agent_ppo2.py:192][0m |          -0.0147 |           4.8262 |           0.1574 |
[32m[20230207 15:25:20 @agent_ppo2.py:192][0m |          -0.0108 |           4.7715 |           0.1574 |
[32m[20230207 15:25:20 @agent_ppo2.py:192][0m |          -0.0179 |           4.5566 |           0.1572 |
[32m[20230207 15:25:20 @agent_ppo2.py:192][0m |          -0.0185 |           4.4802 |           0.1573 |
[32m[20230207 15:25:20 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:25:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: 37.30
[32m[20230207 15:25:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 127.10
[32m[20230207 15:25:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -69.31
[32m[20230207 15:25:21 @agent_ppo2.py:150][0m Total time:      24.66 min
[32m[20230207 15:25:21 @agent_ppo2.py:152][0m 1290240 total steps have happened
[32m[20230207 15:25:21 @agent_ppo2.py:128][0m #------------------------ Iteration 630 --------------------------#
[32m[20230207 15:25:21 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:25:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:21 @agent_ppo2.py:192][0m |           0.0049 |           2.9038 |           0.1519 |
[32m[20230207 15:25:22 @agent_ppo2.py:192][0m |           0.0064 |           2.1914 |           0.1518 |
[32m[20230207 15:25:22 @agent_ppo2.py:192][0m |           0.0112 |           2.0327 |           0.1518 |
[32m[20230207 15:25:22 @agent_ppo2.py:192][0m |          -0.0043 |           1.9098 |           0.1517 |
[32m[20230207 15:25:22 @agent_ppo2.py:192][0m |          -0.0133 |           1.7955 |           0.1516 |
[32m[20230207 15:25:22 @agent_ppo2.py:192][0m |          -0.0356 |           1.7287 |           0.1516 |
[32m[20230207 15:25:22 @agent_ppo2.py:192][0m |          -0.0160 |           1.6431 |           0.1515 |
[32m[20230207 15:25:22 @agent_ppo2.py:192][0m |           0.0096 |           1.5885 |           0.1514 |
[32m[20230207 15:25:22 @agent_ppo2.py:192][0m |          -0.0230 |           1.5521 |           0.1514 |
[32m[20230207 15:25:22 @agent_ppo2.py:192][0m |          -0.0117 |           1.5183 |           0.1513 |
[32m[20230207 15:25:22 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:25:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: 69.72
[32m[20230207 15:25:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 86.88
[32m[20230207 15:25:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 5.14
[32m[20230207 15:25:23 @agent_ppo2.py:150][0m Total time:      24.69 min
[32m[20230207 15:25:23 @agent_ppo2.py:152][0m 1292288 total steps have happened
[32m[20230207 15:25:23 @agent_ppo2.py:128][0m #------------------------ Iteration 631 --------------------------#
[32m[20230207 15:25:23 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:25:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:23 @agent_ppo2.py:192][0m |          -0.0043 |          32.0939 |           0.1565 |
[32m[20230207 15:25:23 @agent_ppo2.py:192][0m |          -0.0094 |           9.8646 |           0.1561 |
[32m[20230207 15:25:23 @agent_ppo2.py:192][0m |          -0.0029 |           6.6075 |           0.1564 |
[32m[20230207 15:25:24 @agent_ppo2.py:192][0m |          -0.0130 |           5.3634 |           0.1564 |
[32m[20230207 15:25:24 @agent_ppo2.py:192][0m |          -0.0022 |           4.5599 |           0.1562 |
[32m[20230207 15:25:24 @agent_ppo2.py:192][0m |          -0.0089 |           3.9691 |           0.1558 |
[32m[20230207 15:25:24 @agent_ppo2.py:192][0m |          -0.0151 |           3.7525 |           0.1559 |
[32m[20230207 15:25:24 @agent_ppo2.py:192][0m |          -0.0072 |           3.4347 |           0.1560 |
[32m[20230207 15:25:24 @agent_ppo2.py:192][0m |          -0.0171 |           3.2007 |           0.1559 |
[32m[20230207 15:25:24 @agent_ppo2.py:192][0m |          -0.0066 |           2.9091 |           0.1556 |
[32m[20230207 15:25:24 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:25:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -63.31
[32m[20230207 15:25:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 107.12
[32m[20230207 15:25:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 99.76
[32m[20230207 15:25:25 @agent_ppo2.py:150][0m Total time:      24.72 min
[32m[20230207 15:25:25 @agent_ppo2.py:152][0m 1294336 total steps have happened
[32m[20230207 15:25:25 @agent_ppo2.py:128][0m #------------------------ Iteration 632 --------------------------#
[32m[20230207 15:25:25 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:25:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:25 @agent_ppo2.py:192][0m |          -0.0022 |          34.2587 |           0.1582 |
[32m[20230207 15:25:25 @agent_ppo2.py:192][0m |          -0.0043 |          17.2267 |           0.1578 |
[32m[20230207 15:25:25 @agent_ppo2.py:192][0m |          -0.0094 |          15.1429 |           0.1581 |
[32m[20230207 15:25:25 @agent_ppo2.py:192][0m |          -0.0113 |          14.0055 |           0.1583 |
[32m[20230207 15:25:25 @agent_ppo2.py:192][0m |          -0.0134 |          13.6930 |           0.1582 |
[32m[20230207 15:25:25 @agent_ppo2.py:192][0m |          -0.0129 |          12.9998 |           0.1583 |
[32m[20230207 15:25:26 @agent_ppo2.py:192][0m |          -0.0159 |          12.6677 |           0.1581 |
[32m[20230207 15:25:26 @agent_ppo2.py:192][0m |          -0.0153 |          12.3394 |           0.1585 |
[32m[20230207 15:25:26 @agent_ppo2.py:192][0m |          -0.0171 |          12.4642 |           0.1581 |
[32m[20230207 15:25:26 @agent_ppo2.py:192][0m |          -0.0173 |          12.1042 |           0.1582 |
[32m[20230207 15:25:26 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:25:26 @agent_ppo2.py:145][0m Average TRAINING episode reward: -65.77
[32m[20230207 15:25:26 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 122.17
[32m[20230207 15:25:26 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -59.42
[32m[20230207 15:25:26 @agent_ppo2.py:150][0m Total time:      24.75 min
[32m[20230207 15:25:26 @agent_ppo2.py:152][0m 1296384 total steps have happened
[32m[20230207 15:25:26 @agent_ppo2.py:128][0m #------------------------ Iteration 633 --------------------------#
[32m[20230207 15:25:27 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:25:27 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:27 @agent_ppo2.py:192][0m |           0.0034 |           7.9627 |           0.1542 |
[32m[20230207 15:25:27 @agent_ppo2.py:192][0m |          -0.0010 |           2.8948 |           0.1543 |
[32m[20230207 15:25:28 @agent_ppo2.py:192][0m |          -0.0058 |           2.2832 |           0.1544 |
[32m[20230207 15:25:28 @agent_ppo2.py:192][0m |          -0.0049 |           2.0852 |           0.1543 |
[32m[20230207 15:25:28 @agent_ppo2.py:192][0m |          -0.0030 |           1.9709 |           0.1543 |
[32m[20230207 15:25:28 @agent_ppo2.py:192][0m |          -0.0086 |           1.8891 |           0.1543 |
[32m[20230207 15:25:28 @agent_ppo2.py:192][0m |          -0.0111 |           1.8122 |           0.1543 |
[32m[20230207 15:25:28 @agent_ppo2.py:192][0m |          -0.0107 |           1.7521 |           0.1543 |
[32m[20230207 15:25:28 @agent_ppo2.py:192][0m |          -0.0104 |           1.8443 |           0.1543 |
[32m[20230207 15:25:28 @agent_ppo2.py:192][0m |          -0.0096 |           1.6749 |           0.1541 |
[32m[20230207 15:25:28 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:25:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -25.93
[32m[20230207 15:25:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 93.75
[32m[20230207 15:25:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 105.69
[32m[20230207 15:25:29 @agent_ppo2.py:150][0m Total time:      24.79 min
[32m[20230207 15:25:29 @agent_ppo2.py:152][0m 1298432 total steps have happened
[32m[20230207 15:25:29 @agent_ppo2.py:128][0m #------------------------ Iteration 634 --------------------------#
[32m[20230207 15:25:30 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:25:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:30 @agent_ppo2.py:192][0m |          -0.0117 |          27.0015 |           0.1527 |
[32m[20230207 15:25:30 @agent_ppo2.py:192][0m |          -0.0154 |          12.1741 |           0.1526 |
[32m[20230207 15:25:30 @agent_ppo2.py:192][0m |          -0.0027 |           9.2161 |           0.1525 |
[32m[20230207 15:25:30 @agent_ppo2.py:192][0m |          -0.0041 |           8.0293 |           0.1526 |
[32m[20230207 15:25:30 @agent_ppo2.py:192][0m |          -0.0085 |           7.2488 |           0.1526 |
[32m[20230207 15:25:30 @agent_ppo2.py:192][0m |          -0.0131 |           6.9327 |           0.1526 |
[32m[20230207 15:25:30 @agent_ppo2.py:192][0m |          -0.0098 |           6.4033 |           0.1526 |
[32m[20230207 15:25:30 @agent_ppo2.py:192][0m |          -0.0090 |           5.9421 |           0.1526 |
[32m[20230207 15:25:31 @agent_ppo2.py:192][0m |          -0.0188 |           5.5920 |           0.1526 |
[32m[20230207 15:25:31 @agent_ppo2.py:192][0m |          -0.0281 |           5.3115 |           0.1527 |
[32m[20230207 15:25:31 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:25:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -8.08
[32m[20230207 15:25:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 114.55
[32m[20230207 15:25:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 16.34
[32m[20230207 15:25:31 @agent_ppo2.py:150][0m Total time:      24.84 min
[32m[20230207 15:25:31 @agent_ppo2.py:152][0m 1300480 total steps have happened
[32m[20230207 15:25:31 @agent_ppo2.py:128][0m #------------------------ Iteration 635 --------------------------#
[32m[20230207 15:25:32 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:25:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:32 @agent_ppo2.py:192][0m |           0.0014 |          12.8079 |           0.1507 |
[32m[20230207 15:25:32 @agent_ppo2.py:192][0m |          -0.0037 |           7.7405 |           0.1507 |
[32m[20230207 15:25:33 @agent_ppo2.py:192][0m |          -0.0091 |           7.0061 |           0.1509 |
[32m[20230207 15:25:33 @agent_ppo2.py:192][0m |          -0.0025 |           7.2143 |           0.1507 |
[32m[20230207 15:25:33 @agent_ppo2.py:192][0m |          -0.0046 |           6.2777 |           0.1508 |
[32m[20230207 15:25:33 @agent_ppo2.py:192][0m |           0.0030 |           6.1554 |           0.1508 |
[32m[20230207 15:25:33 @agent_ppo2.py:192][0m |          -0.0194 |           5.8955 |           0.1510 |
[32m[20230207 15:25:33 @agent_ppo2.py:192][0m |          -0.0111 |           5.6359 |           0.1510 |
[32m[20230207 15:25:33 @agent_ppo2.py:192][0m |           0.0061 |           5.5510 |           0.1510 |
[32m[20230207 15:25:33 @agent_ppo2.py:192][0m |          -0.0060 |           5.3354 |           0.1511 |
[32m[20230207 15:25:33 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:25:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: -14.33
[32m[20230207 15:25:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 89.05
[32m[20230207 15:25:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -66.76
[32m[20230207 15:25:34 @agent_ppo2.py:150][0m Total time:      24.88 min
[32m[20230207 15:25:34 @agent_ppo2.py:152][0m 1302528 total steps have happened
[32m[20230207 15:25:34 @agent_ppo2.py:128][0m #------------------------ Iteration 636 --------------------------#
[32m[20230207 15:25:35 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:25:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:35 @agent_ppo2.py:192][0m |           0.0036 |           6.4884 |           0.1592 |
[32m[20230207 15:25:35 @agent_ppo2.py:192][0m |          -0.0045 |           1.2771 |           0.1591 |
[32m[20230207 15:25:35 @agent_ppo2.py:192][0m |          -0.0071 |           1.0870 |           0.1591 |
[32m[20230207 15:25:35 @agent_ppo2.py:192][0m |          -0.0087 |           1.0334 |           0.1591 |
[32m[20230207 15:25:35 @agent_ppo2.py:192][0m |          -0.0107 |           0.9966 |           0.1590 |
[32m[20230207 15:25:35 @agent_ppo2.py:192][0m |          -0.0111 |           0.9659 |           0.1591 |
[32m[20230207 15:25:35 @agent_ppo2.py:192][0m |          -0.0116 |           0.9462 |           0.1590 |
[32m[20230207 15:25:35 @agent_ppo2.py:192][0m |          -0.0090 |           0.9264 |           0.1590 |
[32m[20230207 15:25:35 @agent_ppo2.py:192][0m |          -0.0118 |           0.9116 |           0.1590 |
[32m[20230207 15:25:35 @agent_ppo2.py:192][0m |          -0.0138 |           0.8977 |           0.1591 |
[32m[20230207 15:25:35 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:25:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -9.35
[32m[20230207 15:25:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 42.61
[32m[20230207 15:25:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -32.10
[32m[20230207 15:25:36 @agent_ppo2.py:150][0m Total time:      24.92 min
[32m[20230207 15:25:36 @agent_ppo2.py:152][0m 1304576 total steps have happened
[32m[20230207 15:25:36 @agent_ppo2.py:128][0m #------------------------ Iteration 637 --------------------------#
[32m[20230207 15:25:37 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:25:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:37 @agent_ppo2.py:192][0m |           0.0004 |          23.9123 |           0.1562 |
[32m[20230207 15:25:37 @agent_ppo2.py:192][0m |          -0.0044 |          11.8364 |           0.1560 |
[32m[20230207 15:25:37 @agent_ppo2.py:192][0m |          -0.0077 |           9.2162 |           0.1558 |
[32m[20230207 15:25:37 @agent_ppo2.py:192][0m |          -0.0109 |           8.1299 |           0.1558 |
[32m[20230207 15:25:37 @agent_ppo2.py:192][0m |          -0.0141 |           7.3734 |           0.1558 |
[32m[20230207 15:25:37 @agent_ppo2.py:192][0m |          -0.0137 |           6.7778 |           0.1557 |
[32m[20230207 15:25:37 @agent_ppo2.py:192][0m |          -0.0133 |           6.2226 |           0.1558 |
[32m[20230207 15:25:37 @agent_ppo2.py:192][0m |          -0.0195 |           5.6892 |           0.1556 |
[32m[20230207 15:25:37 @agent_ppo2.py:192][0m |          -0.0176 |           5.3276 |           0.1558 |
[32m[20230207 15:25:37 @agent_ppo2.py:192][0m |          -0.0194 |           5.1829 |           0.1557 |
[32m[20230207 15:25:37 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:25:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: -26.13
[32m[20230207 15:25:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 60.31
[32m[20230207 15:25:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -81.64
[32m[20230207 15:25:38 @agent_ppo2.py:150][0m Total time:      24.95 min
[32m[20230207 15:25:38 @agent_ppo2.py:152][0m 1306624 total steps have happened
[32m[20230207 15:25:38 @agent_ppo2.py:128][0m #------------------------ Iteration 638 --------------------------#
[32m[20230207 15:25:39 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:25:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:39 @agent_ppo2.py:192][0m |           0.0029 |           3.9137 |           0.1576 |
[32m[20230207 15:25:39 @agent_ppo2.py:192][0m |          -0.0037 |           2.0922 |           0.1575 |
[32m[20230207 15:25:39 @agent_ppo2.py:192][0m |          -0.0068 |           1.8782 |           0.1572 |
[32m[20230207 15:25:39 @agent_ppo2.py:192][0m |          -0.0092 |           1.7686 |           0.1571 |
[32m[20230207 15:25:40 @agent_ppo2.py:192][0m |          -0.0096 |           1.6958 |           0.1571 |
[32m[20230207 15:25:40 @agent_ppo2.py:192][0m |          -0.0092 |           1.6332 |           0.1569 |
[32m[20230207 15:25:40 @agent_ppo2.py:192][0m |          -0.0107 |           1.5917 |           0.1570 |
[32m[20230207 15:25:40 @agent_ppo2.py:192][0m |          -0.0126 |           1.5485 |           0.1568 |
[32m[20230207 15:25:40 @agent_ppo2.py:192][0m |          -0.0138 |           1.5115 |           0.1570 |
[32m[20230207 15:25:40 @agent_ppo2.py:192][0m |          -0.0123 |           1.4813 |           0.1570 |
[32m[20230207 15:25:40 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:25:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: 14.93
[32m[20230207 15:25:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 94.42
[32m[20230207 15:25:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -90.00
[32m[20230207 15:25:41 @agent_ppo2.py:150][0m Total time:      24.99 min
[32m[20230207 15:25:41 @agent_ppo2.py:152][0m 1308672 total steps have happened
[32m[20230207 15:25:41 @agent_ppo2.py:128][0m #------------------------ Iteration 639 --------------------------#
[32m[20230207 15:25:42 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:25:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:42 @agent_ppo2.py:192][0m |           0.0019 |           7.9919 |           0.1595 |
[32m[20230207 15:25:42 @agent_ppo2.py:192][0m |          -0.0044 |           3.9587 |           0.1595 |
[32m[20230207 15:25:42 @agent_ppo2.py:192][0m |          -0.0065 |           3.4744 |           0.1596 |
[32m[20230207 15:25:42 @agent_ppo2.py:192][0m |          -0.0077 |           3.2835 |           0.1596 |
[32m[20230207 15:25:42 @agent_ppo2.py:192][0m |          -0.0103 |           3.1214 |           0.1595 |
[32m[20230207 15:25:42 @agent_ppo2.py:192][0m |          -0.0100 |           2.9886 |           0.1596 |
[32m[20230207 15:25:42 @agent_ppo2.py:192][0m |          -0.0117 |           2.8632 |           0.1596 |
[32m[20230207 15:25:42 @agent_ppo2.py:192][0m |          -0.0123 |           2.7508 |           0.1596 |
[32m[20230207 15:25:42 @agent_ppo2.py:192][0m |          -0.0128 |           2.6271 |           0.1596 |
[32m[20230207 15:25:43 @agent_ppo2.py:192][0m |          -0.0132 |           2.5550 |           0.1596 |
[32m[20230207 15:25:43 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:25:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -32.06
[32m[20230207 15:25:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 123.30
[32m[20230207 15:25:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 116.06
[32m[20230207 15:25:43 @agent_ppo2.py:150][0m Total time:      25.04 min
[32m[20230207 15:25:43 @agent_ppo2.py:152][0m 1310720 total steps have happened
[32m[20230207 15:25:43 @agent_ppo2.py:128][0m #------------------------ Iteration 640 --------------------------#
[32m[20230207 15:25:44 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:25:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:44 @agent_ppo2.py:192][0m |          -0.0015 |          14.1782 |           0.1591 |
[32m[20230207 15:25:44 @agent_ppo2.py:192][0m |          -0.0071 |           6.0902 |           0.1587 |
[32m[20230207 15:25:45 @agent_ppo2.py:192][0m |          -0.0086 |           5.1798 |           0.1586 |
[32m[20230207 15:25:45 @agent_ppo2.py:192][0m |          -0.0089 |           4.7669 |           0.1585 |
[32m[20230207 15:25:45 @agent_ppo2.py:192][0m |          -0.0125 |           4.4537 |           0.1583 |
[32m[20230207 15:25:45 @agent_ppo2.py:192][0m |          -0.0118 |           4.1548 |           0.1583 |
[32m[20230207 15:25:45 @agent_ppo2.py:192][0m |          -0.0138 |           3.9269 |           0.1583 |
[32m[20230207 15:25:45 @agent_ppo2.py:192][0m |          -0.0135 |           3.6690 |           0.1581 |
[32m[20230207 15:25:45 @agent_ppo2.py:192][0m |          -0.0152 |           3.5079 |           0.1581 |
[32m[20230207 15:25:45 @agent_ppo2.py:192][0m |          -0.0142 |           3.2534 |           0.1580 |
[32m[20230207 15:25:45 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:25:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -48.54
[32m[20230207 15:25:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 168.71
[32m[20230207 15:25:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -30.78
[32m[20230207 15:25:46 @agent_ppo2.py:150][0m Total time:      25.07 min
[32m[20230207 15:25:46 @agent_ppo2.py:152][0m 1312768 total steps have happened
[32m[20230207 15:25:46 @agent_ppo2.py:128][0m #------------------------ Iteration 641 --------------------------#
[32m[20230207 15:25:46 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:25:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:47 @agent_ppo2.py:192][0m |          -0.0001 |           3.2968 |           0.1600 |
[32m[20230207 15:25:47 @agent_ppo2.py:192][0m |          -0.0049 |           2.2829 |           0.1600 |
[32m[20230207 15:25:47 @agent_ppo2.py:192][0m |          -0.0059 |           2.1079 |           0.1601 |
[32m[20230207 15:25:47 @agent_ppo2.py:192][0m |          -0.0093 |           1.9748 |           0.1599 |
[32m[20230207 15:25:47 @agent_ppo2.py:192][0m |          -0.0108 |           1.8840 |           0.1601 |
[32m[20230207 15:25:47 @agent_ppo2.py:192][0m |          -0.0116 |           1.8200 |           0.1600 |
[32m[20230207 15:25:47 @agent_ppo2.py:192][0m |          -0.0129 |           1.7542 |           0.1598 |
[32m[20230207 15:25:47 @agent_ppo2.py:192][0m |          -0.0159 |           1.6999 |           0.1600 |
[32m[20230207 15:25:47 @agent_ppo2.py:192][0m |          -0.0112 |           1.6741 |           0.1599 |
[32m[20230207 15:25:47 @agent_ppo2.py:192][0m |          -0.0142 |           1.6205 |           0.1598 |
[32m[20230207 15:25:47 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:25:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: 147.56
[32m[20230207 15:25:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 201.38
[32m[20230207 15:25:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -114.49
[32m[20230207 15:25:48 @agent_ppo2.py:150][0m Total time:      25.11 min
[32m[20230207 15:25:48 @agent_ppo2.py:152][0m 1314816 total steps have happened
[32m[20230207 15:25:48 @agent_ppo2.py:128][0m #------------------------ Iteration 642 --------------------------#
[32m[20230207 15:25:49 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:25:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:49 @agent_ppo2.py:192][0m |          -0.0003 |          21.4232 |           0.1576 |
[32m[20230207 15:25:49 @agent_ppo2.py:192][0m |          -0.0054 |           8.8078 |           0.1570 |
[32m[20230207 15:25:49 @agent_ppo2.py:192][0m |          -0.0079 |           6.4125 |           0.1573 |
[32m[20230207 15:25:49 @agent_ppo2.py:192][0m |          -0.0116 |           5.2021 |           0.1569 |
[32m[20230207 15:25:49 @agent_ppo2.py:192][0m |          -0.0130 |           4.3882 |           0.1571 |
[32m[20230207 15:25:49 @agent_ppo2.py:192][0m |          -0.0145 |           3.8965 |           0.1572 |
[32m[20230207 15:25:49 @agent_ppo2.py:192][0m |          -0.0149 |           3.2897 |           0.1568 |
[32m[20230207 15:25:49 @agent_ppo2.py:192][0m |          -0.0160 |           2.9642 |           0.1571 |
[32m[20230207 15:25:49 @agent_ppo2.py:192][0m |          -0.0169 |           2.6160 |           0.1568 |
[32m[20230207 15:25:49 @agent_ppo2.py:192][0m |          -0.0182 |           2.4184 |           0.1570 |
[32m[20230207 15:25:49 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:25:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: -77.45
[32m[20230207 15:25:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 63.10
[32m[20230207 15:25:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -10.79
[32m[20230207 15:25:50 @agent_ppo2.py:150][0m Total time:      25.15 min
[32m[20230207 15:25:50 @agent_ppo2.py:152][0m 1316864 total steps have happened
[32m[20230207 15:25:50 @agent_ppo2.py:128][0m #------------------------ Iteration 643 --------------------------#
[32m[20230207 15:25:51 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:25:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:51 @agent_ppo2.py:192][0m |           0.0132 |           8.2495 |           0.1544 |
[32m[20230207 15:25:51 @agent_ppo2.py:192][0m |          -0.0016 |           5.6421 |           0.1542 |
[32m[20230207 15:25:51 @agent_ppo2.py:192][0m |           0.0027 |           5.2429 |           0.1543 |
[32m[20230207 15:25:51 @agent_ppo2.py:192][0m |          -0.0052 |           5.1523 |           0.1542 |
[32m[20230207 15:25:51 @agent_ppo2.py:192][0m |           0.0112 |           4.9334 |           0.1540 |
[32m[20230207 15:25:51 @agent_ppo2.py:192][0m |           0.0119 |           4.9489 |           0.1539 |
[32m[20230207 15:25:51 @agent_ppo2.py:192][0m |           0.0086 |           4.8447 |           0.1540 |
[32m[20230207 15:25:52 @agent_ppo2.py:192][0m |          -0.0253 |           4.6299 |           0.1538 |
[32m[20230207 15:25:52 @agent_ppo2.py:192][0m |          -0.0040 |           4.5414 |           0.1538 |
[32m[20230207 15:25:52 @agent_ppo2.py:192][0m |          -0.0180 |           4.4043 |           0.1538 |
[32m[20230207 15:25:52 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:25:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: 21.44
[32m[20230207 15:25:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 32.96
[32m[20230207 15:25:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -65.19
[32m[20230207 15:25:52 @agent_ppo2.py:150][0m Total time:      25.18 min
[32m[20230207 15:25:52 @agent_ppo2.py:152][0m 1318912 total steps have happened
[32m[20230207 15:25:52 @agent_ppo2.py:128][0m #------------------------ Iteration 644 --------------------------#
[32m[20230207 15:25:53 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:25:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:53 @agent_ppo2.py:192][0m |          -0.0418 |           1.6884 |           0.1608 |
[32m[20230207 15:25:53 @agent_ppo2.py:192][0m |          -0.0076 |           1.2395 |           0.1608 |
[32m[20230207 15:25:53 @agent_ppo2.py:192][0m |           0.0032 |           1.1611 |           0.1609 |
[32m[20230207 15:25:53 @agent_ppo2.py:192][0m |          -0.0028 |           1.1098 |           0.1609 |
[32m[20230207 15:25:53 @agent_ppo2.py:192][0m |          -0.0249 |           1.0835 |           0.1610 |
[32m[20230207 15:25:54 @agent_ppo2.py:192][0m |          -0.0212 |           1.0478 |           0.1609 |
[32m[20230207 15:25:54 @agent_ppo2.py:192][0m |           0.0052 |           1.0315 |           0.1605 |
[32m[20230207 15:25:54 @agent_ppo2.py:192][0m |          -0.0269 |           1.0147 |           0.1605 |
[32m[20230207 15:25:54 @agent_ppo2.py:192][0m |          -0.0288 |           1.0004 |           0.1605 |
[32m[20230207 15:25:54 @agent_ppo2.py:192][0m |          -0.0089 |           0.9975 |           0.1607 |
[32m[20230207 15:25:54 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:25:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: 56.75
[32m[20230207 15:25:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 111.48
[32m[20230207 15:25:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 25.55
[32m[20230207 15:25:55 @agent_ppo2.py:150][0m Total time:      25.22 min
[32m[20230207 15:25:55 @agent_ppo2.py:152][0m 1320960 total steps have happened
[32m[20230207 15:25:55 @agent_ppo2.py:128][0m #------------------------ Iteration 645 --------------------------#
[32m[20230207 15:25:55 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 15:25:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:55 @agent_ppo2.py:192][0m |           0.0015 |          22.5121 |           0.1566 |
[32m[20230207 15:25:55 @agent_ppo2.py:192][0m |          -0.0047 |          10.8747 |           0.1568 |
[32m[20230207 15:25:55 @agent_ppo2.py:192][0m |          -0.0068 |           8.8881 |           0.1570 |
[32m[20230207 15:25:55 @agent_ppo2.py:192][0m |          -0.0071 |           8.0146 |           0.1568 |
[32m[20230207 15:25:55 @agent_ppo2.py:192][0m |          -0.0094 |           7.2122 |           0.1569 |
[32m[20230207 15:25:56 @agent_ppo2.py:192][0m |          -0.0101 |           6.7332 |           0.1569 |
[32m[20230207 15:25:56 @agent_ppo2.py:192][0m |          -0.0112 |           6.3590 |           0.1568 |
[32m[20230207 15:25:56 @agent_ppo2.py:192][0m |          -0.0118 |           5.9333 |           0.1567 |
[32m[20230207 15:25:56 @agent_ppo2.py:192][0m |          -0.0123 |           5.6985 |           0.1567 |
[32m[20230207 15:25:56 @agent_ppo2.py:192][0m |          -0.0131 |           5.5221 |           0.1568 |
[32m[20230207 15:25:56 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:25:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: 14.51
[32m[20230207 15:25:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 74.89
[32m[20230207 15:25:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -54.03
[32m[20230207 15:25:56 @agent_ppo2.py:150][0m Total time:      25.25 min
[32m[20230207 15:25:56 @agent_ppo2.py:152][0m 1323008 total steps have happened
[32m[20230207 15:25:56 @agent_ppo2.py:128][0m #------------------------ Iteration 646 --------------------------#
[32m[20230207 15:25:57 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:25:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:25:57 @agent_ppo2.py:192][0m |          -0.0007 |           8.6337 |           0.1584 |
[32m[20230207 15:25:57 @agent_ppo2.py:192][0m |          -0.0071 |           3.8037 |           0.1586 |
[32m[20230207 15:25:57 @agent_ppo2.py:192][0m |          -0.0065 |           2.8786 |           0.1586 |
[32m[20230207 15:25:57 @agent_ppo2.py:192][0m |          -0.0066 |           2.6252 |           0.1586 |
[32m[20230207 15:25:58 @agent_ppo2.py:192][0m |          -0.0123 |           2.4498 |           0.1584 |
[32m[20230207 15:25:58 @agent_ppo2.py:192][0m |          -0.0194 |           2.3662 |           0.1584 |
[32m[20230207 15:25:58 @agent_ppo2.py:192][0m |          -0.0134 |           2.2614 |           0.1584 |
[32m[20230207 15:25:58 @agent_ppo2.py:192][0m |          -0.0181 |           2.1882 |           0.1584 |
[32m[20230207 15:25:58 @agent_ppo2.py:192][0m |          -0.0134 |           2.1090 |           0.1583 |
[32m[20230207 15:25:58 @agent_ppo2.py:192][0m |          -0.0095 |           2.0610 |           0.1583 |
[32m[20230207 15:25:58 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:25:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: -17.82
[32m[20230207 15:25:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 39.74
[32m[20230207 15:25:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -56.49
[32m[20230207 15:25:59 @agent_ppo2.py:150][0m Total time:      25.29 min
[32m[20230207 15:25:59 @agent_ppo2.py:152][0m 1325056 total steps have happened
[32m[20230207 15:25:59 @agent_ppo2.py:128][0m #------------------------ Iteration 647 --------------------------#
[32m[20230207 15:26:00 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:26:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:00 @agent_ppo2.py:192][0m |           0.0006 |          41.0007 |           0.1594 |
[32m[20230207 15:26:00 @agent_ppo2.py:192][0m |          -0.0049 |          22.7761 |           0.1592 |
[32m[20230207 15:26:00 @agent_ppo2.py:192][0m |          -0.0072 |          19.0964 |           0.1589 |
[32m[20230207 15:26:00 @agent_ppo2.py:192][0m |          -0.0116 |          16.6787 |           0.1588 |
[32m[20230207 15:26:00 @agent_ppo2.py:192][0m |          -0.0138 |          14.8751 |           0.1584 |
[32m[20230207 15:26:00 @agent_ppo2.py:192][0m |          -0.0130 |          13.4509 |           0.1585 |
[32m[20230207 15:26:00 @agent_ppo2.py:192][0m |          -0.0151 |          12.6296 |           0.1584 |
[32m[20230207 15:26:00 @agent_ppo2.py:192][0m |          -0.0163 |          11.8550 |           0.1582 |
[32m[20230207 15:26:00 @agent_ppo2.py:192][0m |          -0.0168 |          10.9422 |           0.1584 |
[32m[20230207 15:26:00 @agent_ppo2.py:192][0m |          -0.0188 |          10.4794 |           0.1583 |
[32m[20230207 15:26:00 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:26:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -106.54
[32m[20230207 15:26:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -97.33
[32m[20230207 15:26:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -18.08
[32m[20230207 15:26:01 @agent_ppo2.py:150][0m Total time:      25.33 min
[32m[20230207 15:26:01 @agent_ppo2.py:152][0m 1327104 total steps have happened
[32m[20230207 15:26:01 @agent_ppo2.py:128][0m #------------------------ Iteration 648 --------------------------#
[32m[20230207 15:26:02 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:26:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:02 @agent_ppo2.py:192][0m |          -0.0001 |          17.2400 |           0.1567 |
[32m[20230207 15:26:02 @agent_ppo2.py:192][0m |          -0.0040 |           5.8006 |           0.1568 |
[32m[20230207 15:26:02 @agent_ppo2.py:192][0m |          -0.0084 |           4.0524 |           0.1566 |
[32m[20230207 15:26:02 @agent_ppo2.py:192][0m |          -0.0109 |           3.5598 |           0.1567 |
[32m[20230207 15:26:02 @agent_ppo2.py:192][0m |          -0.0129 |           3.3617 |           0.1566 |
[32m[20230207 15:26:02 @agent_ppo2.py:192][0m |          -0.0129 |           3.1861 |           0.1567 |
[32m[20230207 15:26:02 @agent_ppo2.py:192][0m |          -0.0125 |           3.0651 |           0.1567 |
[32m[20230207 15:26:02 @agent_ppo2.py:192][0m |          -0.0181 |           2.9366 |           0.1566 |
[32m[20230207 15:26:02 @agent_ppo2.py:192][0m |          -0.0167 |           2.8809 |           0.1565 |
[32m[20230207 15:26:02 @agent_ppo2.py:192][0m |          -0.0143 |           2.7963 |           0.1566 |
[32m[20230207 15:26:02 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:26:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -31.59
[32m[20230207 15:26:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 113.25
[32m[20230207 15:26:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 97.18
[32m[20230207 15:26:03 @agent_ppo2.py:150][0m Total time:      25.36 min
[32m[20230207 15:26:03 @agent_ppo2.py:152][0m 1329152 total steps have happened
[32m[20230207 15:26:03 @agent_ppo2.py:128][0m #------------------------ Iteration 649 --------------------------#
[32m[20230207 15:26:04 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230207 15:26:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:04 @agent_ppo2.py:192][0m |           0.0083 |          15.0926 |           0.1589 |
[32m[20230207 15:26:04 @agent_ppo2.py:192][0m |          -0.0098 |           7.2074 |           0.1589 |
[32m[20230207 15:26:04 @agent_ppo2.py:192][0m |          -0.0092 |           5.9727 |           0.1587 |
[32m[20230207 15:26:04 @agent_ppo2.py:192][0m |          -0.0207 |           5.4924 |           0.1586 |
[32m[20230207 15:26:04 @agent_ppo2.py:192][0m |          -0.0179 |           4.8219 |           0.1587 |
[32m[20230207 15:26:04 @agent_ppo2.py:192][0m |          -0.0074 |           4.7021 |           0.1585 |
[32m[20230207 15:26:04 @agent_ppo2.py:192][0m |          -0.0053 |           4.3615 |           0.1583 |
[32m[20230207 15:26:04 @agent_ppo2.py:192][0m |          -0.0170 |           4.1247 |           0.1584 |
[32m[20230207 15:26:04 @agent_ppo2.py:192][0m |           0.0039 |           3.9336 |           0.1581 |
[32m[20230207 15:26:04 @agent_ppo2.py:192][0m |          -0.0233 |           3.7574 |           0.1581 |
[32m[20230207 15:26:04 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230207 15:26:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -2.38
[32m[20230207 15:26:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 104.98
[32m[20230207 15:26:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -54.14
[32m[20230207 15:26:05 @agent_ppo2.py:150][0m Total time:      25.40 min
[32m[20230207 15:26:05 @agent_ppo2.py:152][0m 1331200 total steps have happened
[32m[20230207 15:26:05 @agent_ppo2.py:128][0m #------------------------ Iteration 650 --------------------------#
[32m[20230207 15:26:06 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:26:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:06 @agent_ppo2.py:192][0m |           0.0008 |           9.4404 |           0.1585 |
[32m[20230207 15:26:06 @agent_ppo2.py:192][0m |          -0.0041 |           2.7822 |           0.1585 |
[32m[20230207 15:26:06 @agent_ppo2.py:192][0m |          -0.0068 |           2.0512 |           0.1583 |
[32m[20230207 15:26:07 @agent_ppo2.py:192][0m |          -0.0081 |           1.8232 |           0.1584 |
[32m[20230207 15:26:07 @agent_ppo2.py:192][0m |          -0.0088 |           1.6708 |           0.1583 |
[32m[20230207 15:26:07 @agent_ppo2.py:192][0m |          -0.0105 |           1.5730 |           0.1585 |
[32m[20230207 15:26:07 @agent_ppo2.py:192][0m |          -0.0104 |           1.4994 |           0.1584 |
[32m[20230207 15:26:07 @agent_ppo2.py:192][0m |          -0.0116 |           1.4535 |           0.1584 |
[32m[20230207 15:26:07 @agent_ppo2.py:192][0m |          -0.0129 |           1.4083 |           0.1583 |
[32m[20230207 15:26:07 @agent_ppo2.py:192][0m |          -0.0134 |           1.3683 |           0.1583 |
[32m[20230207 15:26:07 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:26:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: 22.46
[32m[20230207 15:26:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 138.04
[32m[20230207 15:26:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -29.40
[32m[20230207 15:26:08 @agent_ppo2.py:150][0m Total time:      25.44 min
[32m[20230207 15:26:08 @agent_ppo2.py:152][0m 1333248 total steps have happened
[32m[20230207 15:26:08 @agent_ppo2.py:128][0m #------------------------ Iteration 651 --------------------------#
[32m[20230207 15:26:08 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:26:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:09 @agent_ppo2.py:192][0m |           0.0050 |          31.3256 |           0.1596 |
[32m[20230207 15:26:09 @agent_ppo2.py:192][0m |          -0.0015 |          23.3409 |           0.1593 |
[32m[20230207 15:26:09 @agent_ppo2.py:192][0m |          -0.0027 |          22.4050 |           0.1591 |
[32m[20230207 15:26:09 @agent_ppo2.py:192][0m |          -0.0097 |          22.7059 |           0.1591 |
[32m[20230207 15:26:09 @agent_ppo2.py:192][0m |          -0.0085 |          21.0037 |           0.1590 |
[32m[20230207 15:26:09 @agent_ppo2.py:192][0m |          -0.0101 |          22.4725 |           0.1589 |
[32m[20230207 15:26:09 @agent_ppo2.py:192][0m |          -0.0082 |          20.4963 |           0.1586 |
[32m[20230207 15:26:09 @agent_ppo2.py:192][0m |          -0.0117 |          20.3111 |           0.1587 |
[32m[20230207 15:26:09 @agent_ppo2.py:192][0m |          -0.0093 |          20.2095 |           0.1585 |
[32m[20230207 15:26:09 @agent_ppo2.py:192][0m |          -0.0100 |          20.0075 |           0.1584 |
[32m[20230207 15:26:09 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:26:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -77.76
[32m[20230207 15:26:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 58.73
[32m[20230207 15:26:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 31.64
[32m[20230207 15:26:10 @agent_ppo2.py:150][0m Total time:      25.48 min
[32m[20230207 15:26:10 @agent_ppo2.py:152][0m 1335296 total steps have happened
[32m[20230207 15:26:10 @agent_ppo2.py:128][0m #------------------------ Iteration 652 --------------------------#
[32m[20230207 15:26:11 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:26:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:11 @agent_ppo2.py:192][0m |           0.0071 |          19.6884 |           0.1499 |
[32m[20230207 15:26:11 @agent_ppo2.py:192][0m |          -0.0004 |          10.2956 |           0.1498 |
[32m[20230207 15:26:11 @agent_ppo2.py:192][0m |          -0.0054 |           7.5152 |           0.1497 |
[32m[20230207 15:26:11 @agent_ppo2.py:192][0m |          -0.0115 |           6.1139 |           0.1496 |
[32m[20230207 15:26:11 @agent_ppo2.py:192][0m |          -0.0063 |           5.5742 |           0.1496 |
[32m[20230207 15:26:11 @agent_ppo2.py:192][0m |          -0.0095 |           5.0384 |           0.1496 |
[32m[20230207 15:26:11 @agent_ppo2.py:192][0m |          -0.0142 |           4.5449 |           0.1495 |
[32m[20230207 15:26:11 @agent_ppo2.py:192][0m |          -0.0079 |           4.3986 |           0.1495 |
[32m[20230207 15:26:11 @agent_ppo2.py:192][0m |          -0.0151 |           4.0393 |           0.1495 |
[32m[20230207 15:26:11 @agent_ppo2.py:192][0m |          -0.0089 |           3.8234 |           0.1494 |
[32m[20230207 15:26:11 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:26:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: -9.36
[32m[20230207 15:26:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 66.54
[32m[20230207 15:26:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -94.13
[32m[20230207 15:26:12 @agent_ppo2.py:150][0m Total time:      25.51 min
[32m[20230207 15:26:12 @agent_ppo2.py:152][0m 1337344 total steps have happened
[32m[20230207 15:26:12 @agent_ppo2.py:128][0m #------------------------ Iteration 653 --------------------------#
[32m[20230207 15:26:12 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:26:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:12 @agent_ppo2.py:192][0m |          -0.0011 |          24.8496 |           0.1593 |
[32m[20230207 15:26:12 @agent_ppo2.py:192][0m |          -0.0075 |           8.0901 |           0.1591 |
[32m[20230207 15:26:12 @agent_ppo2.py:192][0m |          -0.0095 |           5.5731 |           0.1589 |
[32m[20230207 15:26:12 @agent_ppo2.py:192][0m |          -0.0088 |           4.4630 |           0.1588 |
[32m[20230207 15:26:13 @agent_ppo2.py:192][0m |          -0.0083 |           3.8756 |           0.1587 |
[32m[20230207 15:26:13 @agent_ppo2.py:192][0m |          -0.0117 |           3.4489 |           0.1587 |
[32m[20230207 15:26:13 @agent_ppo2.py:192][0m |          -0.0118 |           3.2344 |           0.1585 |
[32m[20230207 15:26:13 @agent_ppo2.py:192][0m |          -0.0135 |           2.9407 |           0.1586 |
[32m[20230207 15:26:13 @agent_ppo2.py:192][0m |          -0.0146 |           2.7669 |           0.1585 |
[32m[20230207 15:26:13 @agent_ppo2.py:192][0m |          -0.0153 |           2.6690 |           0.1584 |
[32m[20230207 15:26:13 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:26:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: -38.14
[32m[20230207 15:26:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 64.93
[32m[20230207 15:26:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -35.19
[32m[20230207 15:26:14 @agent_ppo2.py:150][0m Total time:      25.54 min
[32m[20230207 15:26:14 @agent_ppo2.py:152][0m 1339392 total steps have happened
[32m[20230207 15:26:14 @agent_ppo2.py:128][0m #------------------------ Iteration 654 --------------------------#
[32m[20230207 15:26:14 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:26:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:14 @agent_ppo2.py:192][0m |           0.0024 |          25.3193 |           0.1538 |
[32m[20230207 15:26:14 @agent_ppo2.py:192][0m |          -0.0049 |          12.8753 |           0.1538 |
[32m[20230207 15:26:14 @agent_ppo2.py:192][0m |          -0.0106 |          10.3310 |           0.1535 |
[32m[20230207 15:26:14 @agent_ppo2.py:192][0m |          -0.0112 |           9.5939 |           0.1534 |
[32m[20230207 15:26:15 @agent_ppo2.py:192][0m |          -0.0136 |           9.3052 |           0.1533 |
[32m[20230207 15:26:15 @agent_ppo2.py:192][0m |          -0.0155 |           8.7345 |           0.1533 |
[32m[20230207 15:26:15 @agent_ppo2.py:192][0m |          -0.0160 |           8.8005 |           0.1532 |
[32m[20230207 15:26:15 @agent_ppo2.py:192][0m |          -0.0153 |           8.2805 |           0.1531 |
[32m[20230207 15:26:15 @agent_ppo2.py:192][0m |          -0.0198 |           8.0499 |           0.1530 |
[32m[20230207 15:26:15 @agent_ppo2.py:192][0m |          -0.0199 |           8.0394 |           0.1529 |
[32m[20230207 15:26:15 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230207 15:26:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -84.99
[32m[20230207 15:26:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -0.65
[32m[20230207 15:26:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 110.03
[32m[20230207 15:26:16 @agent_ppo2.py:150][0m Total time:      25.57 min
[32m[20230207 15:26:16 @agent_ppo2.py:152][0m 1341440 total steps have happened
[32m[20230207 15:26:16 @agent_ppo2.py:128][0m #------------------------ Iteration 655 --------------------------#
[32m[20230207 15:26:16 @agent_ppo2.py:134][0m Sampling time: 0.51 s by 1 slaves
[32m[20230207 15:26:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:16 @agent_ppo2.py:192][0m |          -0.0022 |          39.7132 |           0.1606 |
[32m[20230207 15:26:16 @agent_ppo2.py:192][0m |          -0.0076 |          16.5482 |           0.1604 |
[32m[20230207 15:26:16 @agent_ppo2.py:192][0m |          -0.0090 |          13.9655 |           0.1605 |
[32m[20230207 15:26:16 @agent_ppo2.py:192][0m |          -0.0109 |          11.6609 |           0.1602 |
[32m[20230207 15:26:16 @agent_ppo2.py:192][0m |          -0.0134 |          10.0982 |           0.1604 |
[32m[20230207 15:26:16 @agent_ppo2.py:192][0m |          -0.0151 |           9.2174 |           0.1603 |
[32m[20230207 15:26:17 @agent_ppo2.py:192][0m |          -0.0162 |           8.3664 |           0.1604 |
[32m[20230207 15:26:17 @agent_ppo2.py:192][0m |          -0.0164 |           7.7402 |           0.1603 |
[32m[20230207 15:26:17 @agent_ppo2.py:192][0m |          -0.0177 |           7.1120 |           0.1603 |
[32m[20230207 15:26:17 @agent_ppo2.py:192][0m |          -0.0196 |           6.6344 |           0.1604 |
[32m[20230207 15:26:17 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:26:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: -52.59
[32m[20230207 15:26:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 29.05
[32m[20230207 15:26:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 6.35
[32m[20230207 15:26:17 @agent_ppo2.py:150][0m Total time:      25.60 min
[32m[20230207 15:26:17 @agent_ppo2.py:152][0m 1343488 total steps have happened
[32m[20230207 15:26:17 @agent_ppo2.py:128][0m #------------------------ Iteration 656 --------------------------#
[32m[20230207 15:26:18 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:26:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:18 @agent_ppo2.py:192][0m |           0.0017 |          22.5668 |           0.1560 |
[32m[20230207 15:26:18 @agent_ppo2.py:192][0m |           0.0029 |          13.6022 |           0.1558 |
[32m[20230207 15:26:19 @agent_ppo2.py:192][0m |          -0.0041 |          13.3162 |           0.1558 |
[32m[20230207 15:26:19 @agent_ppo2.py:192][0m |          -0.0121 |          12.3524 |           0.1555 |
[32m[20230207 15:26:19 @agent_ppo2.py:192][0m |          -0.0000 |          12.3746 |           0.1554 |
[32m[20230207 15:26:19 @agent_ppo2.py:192][0m |           0.0043 |          11.8096 |           0.1557 |
[32m[20230207 15:26:19 @agent_ppo2.py:192][0m |          -0.0301 |          11.9203 |           0.1556 |
[32m[20230207 15:26:19 @agent_ppo2.py:192][0m |          -0.0102 |          11.6784 |           0.1557 |
[32m[20230207 15:26:19 @agent_ppo2.py:192][0m |          -0.0193 |          11.6148 |           0.1558 |
[32m[20230207 15:26:19 @agent_ppo2.py:192][0m |          -0.0049 |          11.2519 |           0.1556 |
[32m[20230207 15:26:19 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:26:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -59.70
[32m[20230207 15:26:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 49.31
[32m[20230207 15:26:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 35.35
[32m[20230207 15:26:20 @agent_ppo2.py:150][0m Total time:      25.65 min
[32m[20230207 15:26:20 @agent_ppo2.py:152][0m 1345536 total steps have happened
[32m[20230207 15:26:20 @agent_ppo2.py:128][0m #------------------------ Iteration 657 --------------------------#
[32m[20230207 15:26:21 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230207 15:26:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:21 @agent_ppo2.py:192][0m |           0.0038 |          14.3324 |           0.1591 |
[32m[20230207 15:26:21 @agent_ppo2.py:192][0m |          -0.0042 |          11.6291 |           0.1590 |
[32m[20230207 15:26:21 @agent_ppo2.py:192][0m |          -0.0094 |          10.9750 |           0.1588 |
[32m[20230207 15:26:21 @agent_ppo2.py:192][0m |          -0.0077 |          10.5549 |           0.1588 |
[32m[20230207 15:26:21 @agent_ppo2.py:192][0m |          -0.0049 |          11.5997 |           0.1586 |
[32m[20230207 15:26:21 @agent_ppo2.py:192][0m |          -0.0043 |          10.5109 |           0.1585 |
[32m[20230207 15:26:21 @agent_ppo2.py:192][0m |          -0.0029 |          10.7231 |           0.1585 |
[32m[20230207 15:26:21 @agent_ppo2.py:192][0m |          -0.0134 |           9.9248 |           0.1583 |
[32m[20230207 15:26:21 @agent_ppo2.py:192][0m |          -0.0125 |           9.5569 |           0.1583 |
[32m[20230207 15:26:21 @agent_ppo2.py:192][0m |          -0.0044 |          10.5092 |           0.1582 |
[32m[20230207 15:26:21 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230207 15:26:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: -102.97
[32m[20230207 15:26:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 21.42
[32m[20230207 15:26:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 57.67
[32m[20230207 15:26:22 @agent_ppo2.py:150][0m Total time:      25.68 min
[32m[20230207 15:26:22 @agent_ppo2.py:152][0m 1347584 total steps have happened
[32m[20230207 15:26:22 @agent_ppo2.py:128][0m #------------------------ Iteration 658 --------------------------#
[32m[20230207 15:26:23 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:26:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:23 @agent_ppo2.py:192][0m |          -0.0006 |          21.1428 |           0.1560 |
[32m[20230207 15:26:23 @agent_ppo2.py:192][0m |          -0.0054 |          11.3552 |           0.1561 |
[32m[20230207 15:26:23 @agent_ppo2.py:192][0m |          -0.0080 |           9.4220 |           0.1559 |
[32m[20230207 15:26:23 @agent_ppo2.py:192][0m |          -0.0146 |           8.4588 |           0.1561 |
[32m[20230207 15:26:23 @agent_ppo2.py:192][0m |          -0.0141 |           7.7950 |           0.1559 |
[32m[20230207 15:26:23 @agent_ppo2.py:192][0m |          -0.0108 |           7.1445 |           0.1559 |
[32m[20230207 15:26:24 @agent_ppo2.py:192][0m |          -0.0167 |           6.7409 |           0.1560 |
[32m[20230207 15:26:24 @agent_ppo2.py:192][0m |          -0.0152 |           6.4219 |           0.1561 |
[32m[20230207 15:26:24 @agent_ppo2.py:192][0m |          -0.0150 |           6.3026 |           0.1559 |
[32m[20230207 15:26:24 @agent_ppo2.py:192][0m |          -0.0162 |           5.9252 |           0.1561 |
[32m[20230207 15:26:24 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:26:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -36.45
[32m[20230207 15:26:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 81.07
[32m[20230207 15:26:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -2.52
[32m[20230207 15:26:25 @agent_ppo2.py:150][0m Total time:      25.72 min
[32m[20230207 15:26:25 @agent_ppo2.py:152][0m 1349632 total steps have happened
[32m[20230207 15:26:25 @agent_ppo2.py:128][0m #------------------------ Iteration 659 --------------------------#
[32m[20230207 15:26:26 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:26:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:26 @agent_ppo2.py:192][0m |           0.0005 |          10.5886 |           0.1596 |
[32m[20230207 15:26:26 @agent_ppo2.py:192][0m |          -0.0024 |           5.0375 |           0.1596 |
[32m[20230207 15:26:26 @agent_ppo2.py:192][0m |          -0.0043 |           3.9529 |           0.1596 |
[32m[20230207 15:26:26 @agent_ppo2.py:192][0m |          -0.0061 |           3.4470 |           0.1596 |
[32m[20230207 15:26:26 @agent_ppo2.py:192][0m |          -0.0069 |           3.2153 |           0.1595 |
[32m[20230207 15:26:26 @agent_ppo2.py:192][0m |          -0.0081 |           3.0374 |           0.1596 |
[32m[20230207 15:26:26 @agent_ppo2.py:192][0m |          -0.0087 |           2.9077 |           0.1596 |
[32m[20230207 15:26:26 @agent_ppo2.py:192][0m |          -0.0091 |           2.8547 |           0.1596 |
[32m[20230207 15:26:26 @agent_ppo2.py:192][0m |          -0.0100 |           2.7272 |           0.1596 |
[32m[20230207 15:26:26 @agent_ppo2.py:192][0m |          -0.0102 |           2.6548 |           0.1596 |
[32m[20230207 15:26:26 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:26:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -15.98
[32m[20230207 15:26:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 74.98
[32m[20230207 15:26:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 122.74
[32m[20230207 15:26:27 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 122.74
[32m[20230207 15:26:27 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 122.74
[32m[20230207 15:26:27 @agent_ppo2.py:150][0m Total time:      25.77 min
[32m[20230207 15:26:27 @agent_ppo2.py:152][0m 1351680 total steps have happened
[32m[20230207 15:26:27 @agent_ppo2.py:128][0m #------------------------ Iteration 660 --------------------------#
[32m[20230207 15:26:28 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:26:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:28 @agent_ppo2.py:192][0m |           0.0080 |          13.4645 |           0.1536 |
[32m[20230207 15:26:28 @agent_ppo2.py:192][0m |           0.0121 |           8.2410 |           0.1535 |
[32m[20230207 15:26:28 @agent_ppo2.py:192][0m |           0.0276 |           6.9460 |           0.1533 |
[32m[20230207 15:26:28 @agent_ppo2.py:192][0m |           0.0094 |           6.3954 |           0.1529 |
[32m[20230207 15:26:28 @agent_ppo2.py:192][0m |          -0.0052 |           6.0174 |           0.1532 |
[32m[20230207 15:26:28 @agent_ppo2.py:192][0m |           0.0129 |           5.6404 |           0.1531 |
[32m[20230207 15:26:29 @agent_ppo2.py:192][0m |          -0.0149 |           5.4460 |           0.1529 |
[32m[20230207 15:26:29 @agent_ppo2.py:192][0m |          -0.0214 |           5.3252 |           0.1530 |
[32m[20230207 15:26:29 @agent_ppo2.py:192][0m |          -0.0151 |           5.1446 |           0.1529 |
[32m[20230207 15:26:29 @agent_ppo2.py:192][0m |          -0.0070 |           4.9138 |           0.1530 |
[32m[20230207 15:26:29 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:26:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: 55.60
[32m[20230207 15:26:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 83.63
[32m[20230207 15:26:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -0.19
[32m[20230207 15:26:29 @agent_ppo2.py:150][0m Total time:      25.80 min
[32m[20230207 15:26:29 @agent_ppo2.py:152][0m 1353728 total steps have happened
[32m[20230207 15:26:29 @agent_ppo2.py:128][0m #------------------------ Iteration 661 --------------------------#
[32m[20230207 15:26:30 @agent_ppo2.py:134][0m Sampling time: 0.73 s by 1 slaves
[32m[20230207 15:26:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:30 @agent_ppo2.py:192][0m |          -0.0004 |          20.3902 |           0.1582 |
[32m[20230207 15:26:30 @agent_ppo2.py:192][0m |          -0.0037 |           9.9006 |           0.1578 |
[32m[20230207 15:26:30 @agent_ppo2.py:192][0m |          -0.0062 |           7.7129 |           0.1577 |
[32m[20230207 15:26:30 @agent_ppo2.py:192][0m |          -0.0070 |           6.8483 |           0.1577 |
[32m[20230207 15:26:30 @agent_ppo2.py:192][0m |          -0.0082 |           6.3433 |           0.1577 |
[32m[20230207 15:26:31 @agent_ppo2.py:192][0m |          -0.0091 |           5.9437 |           0.1576 |
[32m[20230207 15:26:31 @agent_ppo2.py:192][0m |          -0.0100 |           5.6277 |           0.1576 |
[32m[20230207 15:26:31 @agent_ppo2.py:192][0m |          -0.0107 |           5.4069 |           0.1576 |
[32m[20230207 15:26:31 @agent_ppo2.py:192][0m |          -0.0111 |           5.1682 |           0.1577 |
[32m[20230207 15:26:31 @agent_ppo2.py:192][0m |          -0.0126 |           5.0753 |           0.1577 |
[32m[20230207 15:26:31 @agent_ppo2.py:137][0m Policy update time: 0.76 s
[32m[20230207 15:26:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -32.39
[32m[20230207 15:26:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 71.79
[32m[20230207 15:26:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -0.79
[32m[20230207 15:26:32 @agent_ppo2.py:150][0m Total time:      25.84 min
[32m[20230207 15:26:32 @agent_ppo2.py:152][0m 1355776 total steps have happened
[32m[20230207 15:26:32 @agent_ppo2.py:128][0m #------------------------ Iteration 662 --------------------------#
[32m[20230207 15:26:32 @agent_ppo2.py:134][0m Sampling time: 0.50 s by 1 slaves
[32m[20230207 15:26:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:32 @agent_ppo2.py:192][0m |          -0.0001 |          33.8883 |           0.1580 |
[32m[20230207 15:26:32 @agent_ppo2.py:192][0m |           0.0020 |          20.1586 |           0.1578 |
[32m[20230207 15:26:32 @agent_ppo2.py:192][0m |          -0.0245 |          16.4922 |           0.1577 |
[32m[20230207 15:26:32 @agent_ppo2.py:192][0m |          -0.0043 |          14.8085 |           0.1575 |
[32m[20230207 15:26:32 @agent_ppo2.py:192][0m |          -0.0200 |          13.4814 |           0.1574 |
[32m[20230207 15:26:32 @agent_ppo2.py:192][0m |          -0.0059 |          12.6515 |           0.1573 |
[32m[20230207 15:26:33 @agent_ppo2.py:192][0m |          -0.0397 |          12.7465 |           0.1573 |
[32m[20230207 15:26:33 @agent_ppo2.py:192][0m |          -0.0599 |          11.4344 |           0.1572 |
[32m[20230207 15:26:33 @agent_ppo2.py:192][0m |          -0.0102 |          10.8949 |           0.1571 |
[32m[20230207 15:26:33 @agent_ppo2.py:192][0m |          -0.0182 |          10.7727 |           0.1570 |
[32m[20230207 15:26:33 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:26:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: -62.17
[32m[20230207 15:26:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -20.44
[32m[20230207 15:26:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 108.75
[32m[20230207 15:26:33 @agent_ppo2.py:150][0m Total time:      25.87 min
[32m[20230207 15:26:33 @agent_ppo2.py:152][0m 1357824 total steps have happened
[32m[20230207 15:26:33 @agent_ppo2.py:128][0m #------------------------ Iteration 663 --------------------------#
[32m[20230207 15:26:34 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230207 15:26:34 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:34 @agent_ppo2.py:192][0m |          -0.0010 |          12.2453 |           0.1594 |
[32m[20230207 15:26:34 @agent_ppo2.py:192][0m |          -0.0085 |           6.9784 |           0.1590 |
[32m[20230207 15:26:34 @agent_ppo2.py:192][0m |          -0.0136 |           5.4995 |           0.1586 |
[32m[20230207 15:26:34 @agent_ppo2.py:192][0m |          -0.0152 |           4.8091 |           0.1584 |
[32m[20230207 15:26:34 @agent_ppo2.py:192][0m |          -0.0175 |           4.4102 |           0.1583 |
[32m[20230207 15:26:34 @agent_ppo2.py:192][0m |          -0.0188 |           4.1955 |           0.1582 |
[32m[20230207 15:26:34 @agent_ppo2.py:192][0m |          -0.0199 |           3.9549 |           0.1581 |
[32m[20230207 15:26:35 @agent_ppo2.py:192][0m |          -0.0203 |           3.7290 |           0.1580 |
[32m[20230207 15:26:35 @agent_ppo2.py:192][0m |          -0.0209 |           3.6215 |           0.1580 |
[32m[20230207 15:26:35 @agent_ppo2.py:192][0m |          -0.0208 |           3.4560 |           0.1578 |
[32m[20230207 15:26:35 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:26:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: 3.23
[32m[20230207 15:26:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 38.60
[32m[20230207 15:26:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 9.86
[32m[20230207 15:26:35 @agent_ppo2.py:150][0m Total time:      25.90 min
[32m[20230207 15:26:35 @agent_ppo2.py:152][0m 1359872 total steps have happened
[32m[20230207 15:26:35 @agent_ppo2.py:128][0m #------------------------ Iteration 664 --------------------------#
[32m[20230207 15:26:36 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:26:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:36 @agent_ppo2.py:192][0m |          -0.0001 |          10.4568 |           0.1585 |
[32m[20230207 15:26:37 @agent_ppo2.py:192][0m |          -0.0041 |           5.4171 |           0.1579 |
[32m[20230207 15:26:37 @agent_ppo2.py:192][0m |          -0.0066 |           4.8322 |           0.1580 |
[32m[20230207 15:26:37 @agent_ppo2.py:192][0m |          -0.0079 |           4.4940 |           0.1581 |
[32m[20230207 15:26:37 @agent_ppo2.py:192][0m |          -0.0087 |           4.3634 |           0.1579 |
[32m[20230207 15:26:37 @agent_ppo2.py:192][0m |          -0.0100 |           4.2932 |           0.1579 |
[32m[20230207 15:26:37 @agent_ppo2.py:192][0m |          -0.0097 |           4.1787 |           0.1577 |
[32m[20230207 15:26:37 @agent_ppo2.py:192][0m |          -0.0107 |           4.0483 |           0.1576 |
[32m[20230207 15:26:37 @agent_ppo2.py:192][0m |          -0.0109 |           4.0009 |           0.1577 |
[32m[20230207 15:26:37 @agent_ppo2.py:192][0m |          -0.0121 |           3.9225 |           0.1575 |
[32m[20230207 15:26:37 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:26:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: -29.77
[32m[20230207 15:26:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 58.13
[32m[20230207 15:26:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 44.80
[32m[20230207 15:26:38 @agent_ppo2.py:150][0m Total time:      25.95 min
[32m[20230207 15:26:38 @agent_ppo2.py:152][0m 1361920 total steps have happened
[32m[20230207 15:26:38 @agent_ppo2.py:128][0m #------------------------ Iteration 665 --------------------------#
[32m[20230207 15:26:39 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:26:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:39 @agent_ppo2.py:192][0m |           0.0032 |          19.6052 |           0.1602 |
[32m[20230207 15:26:39 @agent_ppo2.py:192][0m |          -0.0013 |           7.7634 |           0.1602 |
[32m[20230207 15:26:39 @agent_ppo2.py:192][0m |          -0.0014 |           6.0406 |           0.1598 |
[32m[20230207 15:26:39 @agent_ppo2.py:192][0m |          -0.0151 |           4.8174 |           0.1597 |
[32m[20230207 15:26:39 @agent_ppo2.py:192][0m |          -0.0219 |           4.3604 |           0.1596 |
[32m[20230207 15:26:39 @agent_ppo2.py:192][0m |          -0.0187 |           3.9415 |           0.1594 |
[32m[20230207 15:26:39 @agent_ppo2.py:192][0m |          -0.0239 |           3.7447 |           0.1593 |
[32m[20230207 15:26:39 @agent_ppo2.py:192][0m |          -0.0360 |           3.4782 |           0.1593 |
[32m[20230207 15:26:39 @agent_ppo2.py:192][0m |          -0.0378 |           3.2516 |           0.1592 |
[32m[20230207 15:26:39 @agent_ppo2.py:192][0m |          -0.0175 |           3.1936 |           0.1593 |
[32m[20230207 15:26:39 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:26:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: -53.50
[32m[20230207 15:26:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 28.20
[32m[20230207 15:26:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 41.72
[32m[20230207 15:26:40 @agent_ppo2.py:150][0m Total time:      25.98 min
[32m[20230207 15:26:40 @agent_ppo2.py:152][0m 1363968 total steps have happened
[32m[20230207 15:26:40 @agent_ppo2.py:128][0m #------------------------ Iteration 666 --------------------------#
[32m[20230207 15:26:41 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:26:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:41 @agent_ppo2.py:192][0m |           0.0002 |          16.6171 |           0.1611 |
[32m[20230207 15:26:41 @agent_ppo2.py:192][0m |          -0.0029 |           8.1329 |           0.1611 |
[32m[20230207 15:26:41 @agent_ppo2.py:192][0m |          -0.0038 |           6.5316 |           0.1611 |
[32m[20230207 15:26:41 @agent_ppo2.py:192][0m |          -0.0048 |           5.7318 |           0.1611 |
[32m[20230207 15:26:41 @agent_ppo2.py:192][0m |          -0.0076 |           5.3725 |           0.1609 |
[32m[20230207 15:26:41 @agent_ppo2.py:192][0m |          -0.0081 |           4.5854 |           0.1608 |
[32m[20230207 15:26:41 @agent_ppo2.py:192][0m |          -0.0094 |           4.2720 |           0.1609 |
[32m[20230207 15:26:41 @agent_ppo2.py:192][0m |          -0.0109 |           4.0243 |           0.1609 |
[32m[20230207 15:26:41 @agent_ppo2.py:192][0m |          -0.0104 |           3.7880 |           0.1609 |
[32m[20230207 15:26:41 @agent_ppo2.py:192][0m |          -0.0113 |           3.6584 |           0.1608 |
[32m[20230207 15:26:41 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:26:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: -70.31
[32m[20230207 15:26:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 23.99
[32m[20230207 15:26:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 9.18
[32m[20230207 15:26:42 @agent_ppo2.py:150][0m Total time:      26.02 min
[32m[20230207 15:26:42 @agent_ppo2.py:152][0m 1366016 total steps have happened
[32m[20230207 15:26:42 @agent_ppo2.py:128][0m #------------------------ Iteration 667 --------------------------#
[32m[20230207 15:26:43 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:26:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:43 @agent_ppo2.py:192][0m |           0.0179 |           2.0728 |           0.1579 |
[32m[20230207 15:26:43 @agent_ppo2.py:192][0m |          -0.0045 |           1.5948 |           0.1580 |
[32m[20230207 15:26:43 @agent_ppo2.py:192][0m |          -0.0106 |           1.5207 |           0.1579 |
[32m[20230207 15:26:43 @agent_ppo2.py:192][0m |          -0.0086 |           1.4712 |           0.1578 |
[32m[20230207 15:26:43 @agent_ppo2.py:192][0m |          -0.0008 |           1.4413 |           0.1579 |
[32m[20230207 15:26:44 @agent_ppo2.py:192][0m |          -0.0038 |           1.4068 |           0.1577 |
[32m[20230207 15:26:44 @agent_ppo2.py:192][0m |          -0.0115 |           1.4059 |           0.1576 |
[32m[20230207 15:26:44 @agent_ppo2.py:192][0m |           0.0034 |           1.3775 |           0.1576 |
[32m[20230207 15:26:44 @agent_ppo2.py:192][0m |          -0.0183 |           1.3777 |           0.1576 |
[32m[20230207 15:26:44 @agent_ppo2.py:192][0m |          -0.0206 |           1.3537 |           0.1574 |
[32m[20230207 15:26:44 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:26:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: 32.03
[32m[20230207 15:26:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 45.68
[32m[20230207 15:26:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -39.07
[32m[20230207 15:26:45 @agent_ppo2.py:150][0m Total time:      26.06 min
[32m[20230207 15:26:45 @agent_ppo2.py:152][0m 1368064 total steps have happened
[32m[20230207 15:26:45 @agent_ppo2.py:128][0m #------------------------ Iteration 668 --------------------------#
[32m[20230207 15:26:46 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:26:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:46 @agent_ppo2.py:192][0m |           0.0017 |          14.5352 |           0.1596 |
[32m[20230207 15:26:46 @agent_ppo2.py:192][0m |          -0.0054 |           6.5256 |           0.1596 |
[32m[20230207 15:26:46 @agent_ppo2.py:192][0m |          -0.0079 |           4.9650 |           0.1594 |
[32m[20230207 15:26:46 @agent_ppo2.py:192][0m |          -0.0092 |           4.4788 |           0.1593 |
[32m[20230207 15:26:46 @agent_ppo2.py:192][0m |          -0.0101 |           3.9782 |           0.1592 |
[32m[20230207 15:26:46 @agent_ppo2.py:192][0m |          -0.0111 |           3.6797 |           0.1592 |
[32m[20230207 15:26:46 @agent_ppo2.py:192][0m |          -0.0129 |           3.4432 |           0.1590 |
[32m[20230207 15:26:46 @agent_ppo2.py:192][0m |          -0.0136 |           3.2751 |           0.1590 |
[32m[20230207 15:26:46 @agent_ppo2.py:192][0m |          -0.0130 |           3.1454 |           0.1589 |
[32m[20230207 15:26:47 @agent_ppo2.py:192][0m |          -0.0134 |           3.0354 |           0.1588 |
[32m[20230207 15:26:47 @agent_ppo2.py:137][0m Policy update time: 0.92 s
[32m[20230207 15:26:47 @agent_ppo2.py:145][0m Average TRAINING episode reward: -33.06
[32m[20230207 15:26:47 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 71.81
[32m[20230207 15:26:47 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -67.67
[32m[20230207 15:26:47 @agent_ppo2.py:150][0m Total time:      26.10 min
[32m[20230207 15:26:47 @agent_ppo2.py:152][0m 1370112 total steps have happened
[32m[20230207 15:26:47 @agent_ppo2.py:128][0m #------------------------ Iteration 669 --------------------------#
[32m[20230207 15:26:48 @agent_ppo2.py:134][0m Sampling time: 0.72 s by 1 slaves
[32m[20230207 15:26:48 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:48 @agent_ppo2.py:192][0m |           0.0023 |          15.6799 |           0.1542 |
[32m[20230207 15:26:48 @agent_ppo2.py:192][0m |          -0.0021 |           6.5182 |           0.1541 |
[32m[20230207 15:26:48 @agent_ppo2.py:192][0m |          -0.0145 |           4.9890 |           0.1541 |
[32m[20230207 15:26:48 @agent_ppo2.py:192][0m |           0.0117 |           5.1585 |           0.1542 |
[32m[20230207 15:26:48 @agent_ppo2.py:192][0m |          -0.0006 |           4.3952 |           0.1541 |
[32m[20230207 15:26:48 @agent_ppo2.py:192][0m |          -0.0071 |           3.5972 |           0.1540 |
[32m[20230207 15:26:48 @agent_ppo2.py:192][0m |          -0.0065 |           3.3656 |           0.1539 |
[32m[20230207 15:26:48 @agent_ppo2.py:192][0m |          -0.0098 |           3.2793 |           0.1539 |
[32m[20230207 15:26:49 @agent_ppo2.py:192][0m |          -0.0094 |           3.1035 |           0.1538 |
[32m[20230207 15:26:49 @agent_ppo2.py:192][0m |          -0.0113 |           3.0479 |           0.1538 |
[32m[20230207 15:26:49 @agent_ppo2.py:137][0m Policy update time: 0.75 s
[32m[20230207 15:26:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: -4.09
[32m[20230207 15:26:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 10.34
[32m[20230207 15:26:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 29.60
[32m[20230207 15:26:50 @agent_ppo2.py:150][0m Total time:      26.14 min
[32m[20230207 15:26:50 @agent_ppo2.py:152][0m 1372160 total steps have happened
[32m[20230207 15:26:50 @agent_ppo2.py:128][0m #------------------------ Iteration 670 --------------------------#
[32m[20230207 15:26:50 @agent_ppo2.py:134][0m Sampling time: 0.50 s by 1 slaves
[32m[20230207 15:26:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:50 @agent_ppo2.py:192][0m |          -0.0053 |          16.2360 |           0.1554 |
[32m[20230207 15:26:50 @agent_ppo2.py:192][0m |          -0.0058 |           6.4939 |           0.1552 |
[32m[20230207 15:26:50 @agent_ppo2.py:192][0m |          -0.0129 |           5.0051 |           0.1552 |
[32m[20230207 15:26:50 @agent_ppo2.py:192][0m |          -0.0067 |           4.4334 |           0.1552 |
[32m[20230207 15:26:50 @agent_ppo2.py:192][0m |          -0.0137 |           4.2723 |           0.1552 |
[32m[20230207 15:26:50 @agent_ppo2.py:192][0m |          -0.0128 |           3.4974 |           0.1551 |
[32m[20230207 15:26:50 @agent_ppo2.py:192][0m |          -0.0145 |           3.1602 |           0.1553 |
[32m[20230207 15:26:50 @agent_ppo2.py:192][0m |          -0.0150 |           2.9902 |           0.1550 |
[32m[20230207 15:26:50 @agent_ppo2.py:192][0m |          -0.0178 |           2.8033 |           0.1551 |
[32m[20230207 15:26:51 @agent_ppo2.py:192][0m |          -0.0195 |           2.7058 |           0.1550 |
[32m[20230207 15:26:51 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:26:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: -47.91
[32m[20230207 15:26:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -2.90
[32m[20230207 15:26:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -38.03
[32m[20230207 15:26:51 @agent_ppo2.py:150][0m Total time:      26.17 min
[32m[20230207 15:26:51 @agent_ppo2.py:152][0m 1374208 total steps have happened
[32m[20230207 15:26:51 @agent_ppo2.py:128][0m #------------------------ Iteration 671 --------------------------#
[32m[20230207 15:26:52 @agent_ppo2.py:134][0m Sampling time: 0.72 s by 1 slaves
[32m[20230207 15:26:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:52 @agent_ppo2.py:192][0m |          -0.0012 |          15.0865 |           0.1531 |
[32m[20230207 15:26:52 @agent_ppo2.py:192][0m |          -0.0064 |           8.4740 |           0.1530 |
[32m[20230207 15:26:52 @agent_ppo2.py:192][0m |          -0.0074 |           6.1689 |           0.1529 |
[32m[20230207 15:26:52 @agent_ppo2.py:192][0m |          -0.0099 |           4.8725 |           0.1528 |
[32m[20230207 15:26:52 @agent_ppo2.py:192][0m |          -0.0111 |           4.1307 |           0.1527 |
[32m[20230207 15:26:53 @agent_ppo2.py:192][0m |          -0.0127 |           3.6288 |           0.1528 |
[32m[20230207 15:26:53 @agent_ppo2.py:192][0m |          -0.0138 |           3.2627 |           0.1526 |
[32m[20230207 15:26:53 @agent_ppo2.py:192][0m |          -0.0151 |           2.9665 |           0.1527 |
[32m[20230207 15:26:53 @agent_ppo2.py:192][0m |          -0.0161 |           2.7865 |           0.1527 |
[32m[20230207 15:26:53 @agent_ppo2.py:192][0m |          -0.0167 |           2.5769 |           0.1525 |
[32m[20230207 15:26:53 @agent_ppo2.py:137][0m Policy update time: 0.75 s
[32m[20230207 15:26:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: -67.75
[32m[20230207 15:26:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 49.96
[32m[20230207 15:26:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 28.24
[32m[20230207 15:26:54 @agent_ppo2.py:150][0m Total time:      26.21 min
[32m[20230207 15:26:54 @agent_ppo2.py:152][0m 1376256 total steps have happened
[32m[20230207 15:26:54 @agent_ppo2.py:128][0m #------------------------ Iteration 672 --------------------------#
[32m[20230207 15:26:54 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:26:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:55 @agent_ppo2.py:192][0m |          -0.0010 |           4.0310 |           0.1529 |
[32m[20230207 15:26:55 @agent_ppo2.py:192][0m |          -0.0088 |           2.6404 |           0.1527 |
[32m[20230207 15:26:55 @agent_ppo2.py:192][0m |          -0.0033 |           2.3240 |           0.1526 |
[32m[20230207 15:26:55 @agent_ppo2.py:192][0m |          -0.0058 |           2.1709 |           0.1526 |
[32m[20230207 15:26:55 @agent_ppo2.py:192][0m |          -0.0093 |           2.0440 |           0.1523 |
[32m[20230207 15:26:55 @agent_ppo2.py:192][0m |          -0.0099 |           1.9457 |           0.1523 |
[32m[20230207 15:26:55 @agent_ppo2.py:192][0m |          -0.0151 |           1.8804 |           0.1523 |
[32m[20230207 15:26:55 @agent_ppo2.py:192][0m |          -0.0208 |           1.8361 |           0.1523 |
[32m[20230207 15:26:55 @agent_ppo2.py:192][0m |          -0.0138 |           1.7592 |           0.1522 |
[32m[20230207 15:26:55 @agent_ppo2.py:192][0m |          -0.0081 |           1.7107 |           0.1523 |
[32m[20230207 15:26:55 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:26:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: 69.88
[32m[20230207 15:26:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 89.05
[32m[20230207 15:26:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 21.52
[32m[20230207 15:26:56 @agent_ppo2.py:150][0m Total time:      26.25 min
[32m[20230207 15:26:56 @agent_ppo2.py:152][0m 1378304 total steps have happened
[32m[20230207 15:26:56 @agent_ppo2.py:128][0m #------------------------ Iteration 673 --------------------------#
[32m[20230207 15:26:57 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:26:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:57 @agent_ppo2.py:192][0m |           0.0087 |           9.5909 |           0.1486 |
[32m[20230207 15:26:57 @agent_ppo2.py:192][0m |          -0.0019 |           5.8678 |           0.1484 |
[32m[20230207 15:26:57 @agent_ppo2.py:192][0m |          -0.0014 |           5.1706 |           0.1483 |
[32m[20230207 15:26:57 @agent_ppo2.py:192][0m |          -0.0027 |           4.8052 |           0.1483 |
[32m[20230207 15:26:57 @agent_ppo2.py:192][0m |          -0.0068 |           4.5883 |           0.1481 |
[32m[20230207 15:26:57 @agent_ppo2.py:192][0m |          -0.0021 |           4.3917 |           0.1483 |
[32m[20230207 15:26:57 @agent_ppo2.py:192][0m |           0.0058 |           4.2282 |           0.1481 |
[32m[20230207 15:26:57 @agent_ppo2.py:192][0m |          -0.0311 |           4.4001 |           0.1481 |
[32m[20230207 15:26:58 @agent_ppo2.py:192][0m |          -0.0343 |           4.2443 |           0.1482 |
[32m[20230207 15:26:58 @agent_ppo2.py:192][0m |          -0.0146 |           4.5487 |           0.1481 |
[32m[20230207 15:26:58 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:26:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: 43.40
[32m[20230207 15:26:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 75.91
[32m[20230207 15:26:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 76.43
[32m[20230207 15:26:58 @agent_ppo2.py:150][0m Total time:      26.29 min
[32m[20230207 15:26:58 @agent_ppo2.py:152][0m 1380352 total steps have happened
[32m[20230207 15:26:58 @agent_ppo2.py:128][0m #------------------------ Iteration 674 --------------------------#
[32m[20230207 15:26:59 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:26:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:26:59 @agent_ppo2.py:192][0m |           0.0038 |           3.7587 |           0.1523 |
[32m[20230207 15:26:59 @agent_ppo2.py:192][0m |          -0.0061 |           2.7659 |           0.1521 |
[32m[20230207 15:27:00 @agent_ppo2.py:192][0m |          -0.0095 |           2.5260 |           0.1520 |
[32m[20230207 15:27:00 @agent_ppo2.py:192][0m |           0.0010 |           2.3685 |           0.1518 |
[32m[20230207 15:27:00 @agent_ppo2.py:192][0m |          -0.0194 |           2.2513 |           0.1518 |
[32m[20230207 15:27:00 @agent_ppo2.py:192][0m |          -0.0083 |           2.2134 |           0.1518 |
[32m[20230207 15:27:00 @agent_ppo2.py:192][0m |          -0.0018 |           2.1131 |           0.1517 |
[32m[20230207 15:27:00 @agent_ppo2.py:192][0m |           0.0030 |           2.0838 |           0.1517 |
[32m[20230207 15:27:00 @agent_ppo2.py:192][0m |          -0.0295 |           2.0091 |           0.1516 |
[32m[20230207 15:27:00 @agent_ppo2.py:192][0m |          -0.0146 |           1.9517 |           0.1514 |
[32m[20230207 15:27:00 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:27:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: 30.77
[32m[20230207 15:27:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 56.30
[32m[20230207 15:27:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 76.90
[32m[20230207 15:27:01 @agent_ppo2.py:150][0m Total time:      26.33 min
[32m[20230207 15:27:01 @agent_ppo2.py:152][0m 1382400 total steps have happened
[32m[20230207 15:27:01 @agent_ppo2.py:128][0m #------------------------ Iteration 675 --------------------------#
[32m[20230207 15:27:02 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:27:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:02 @agent_ppo2.py:192][0m |          -0.0001 |          10.5744 |           0.1516 |
[32m[20230207 15:27:02 @agent_ppo2.py:192][0m |          -0.0060 |           4.1493 |           0.1516 |
[32m[20230207 15:27:02 @agent_ppo2.py:192][0m |          -0.0106 |           3.0981 |           0.1515 |
[32m[20230207 15:27:02 @agent_ppo2.py:192][0m |          -0.0088 |           2.9245 |           0.1515 |
[32m[20230207 15:27:02 @agent_ppo2.py:192][0m |          -0.0086 |           2.7038 |           0.1516 |
[32m[20230207 15:27:02 @agent_ppo2.py:192][0m |          -0.0132 |           2.6058 |           0.1516 |
[32m[20230207 15:27:02 @agent_ppo2.py:192][0m |          -0.0136 |           2.3870 |           0.1516 |
[32m[20230207 15:27:02 @agent_ppo2.py:192][0m |          -0.0102 |           2.3096 |           0.1516 |
[32m[20230207 15:27:02 @agent_ppo2.py:192][0m |          -0.0137 |           2.2594 |           0.1518 |
[32m[20230207 15:27:03 @agent_ppo2.py:192][0m |          -0.0156 |           2.2270 |           0.1518 |
[32m[20230207 15:27:03 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:27:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -31.88
[32m[20230207 15:27:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 86.82
[32m[20230207 15:27:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 104.56
[32m[20230207 15:27:03 @agent_ppo2.py:150][0m Total time:      26.37 min
[32m[20230207 15:27:03 @agent_ppo2.py:152][0m 1384448 total steps have happened
[32m[20230207 15:27:03 @agent_ppo2.py:128][0m #------------------------ Iteration 676 --------------------------#
[32m[20230207 15:27:04 @agent_ppo2.py:134][0m Sampling time: 0.69 s by 1 slaves
[32m[20230207 15:27:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:04 @agent_ppo2.py:192][0m |          -0.0000 |          16.4181 |           0.1544 |
[32m[20230207 15:27:04 @agent_ppo2.py:192][0m |          -0.0038 |          10.8480 |           0.1545 |
[32m[20230207 15:27:04 @agent_ppo2.py:192][0m |          -0.0078 |           7.1831 |           0.1545 |
[32m[20230207 15:27:04 @agent_ppo2.py:192][0m |          -0.0089 |           6.4525 |           0.1545 |
[32m[20230207 15:27:04 @agent_ppo2.py:192][0m |          -0.0099 |           5.8267 |           0.1544 |
[32m[20230207 15:27:04 @agent_ppo2.py:192][0m |          -0.0110 |           5.4357 |           0.1544 |
[32m[20230207 15:27:05 @agent_ppo2.py:192][0m |          -0.0117 |           5.0168 |           0.1543 |
[32m[20230207 15:27:05 @agent_ppo2.py:192][0m |          -0.0129 |           4.7889 |           0.1544 |
[32m[20230207 15:27:05 @agent_ppo2.py:192][0m |          -0.0132 |           4.6427 |           0.1543 |
[32m[20230207 15:27:05 @agent_ppo2.py:192][0m |          -0.0138 |           4.3089 |           0.1545 |
[32m[20230207 15:27:05 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230207 15:27:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: -11.22
[32m[20230207 15:27:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 80.91
[32m[20230207 15:27:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 35.73
[32m[20230207 15:27:06 @agent_ppo2.py:150][0m Total time:      26.41 min
[32m[20230207 15:27:06 @agent_ppo2.py:152][0m 1386496 total steps have happened
[32m[20230207 15:27:06 @agent_ppo2.py:128][0m #------------------------ Iteration 677 --------------------------#
[32m[20230207 15:27:06 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:27:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:07 @agent_ppo2.py:192][0m |           0.0048 |           6.0102 |           0.1558 |
[32m[20230207 15:27:07 @agent_ppo2.py:192][0m |          -0.0053 |           2.6910 |           0.1554 |
[32m[20230207 15:27:07 @agent_ppo2.py:192][0m |          -0.0078 |           2.3775 |           0.1556 |
[32m[20230207 15:27:07 @agent_ppo2.py:192][0m |          -0.0077 |           2.2432 |           0.1556 |
[32m[20230207 15:27:07 @agent_ppo2.py:192][0m |          -0.0070 |           2.1339 |           0.1555 |
[32m[20230207 15:27:07 @agent_ppo2.py:192][0m |          -0.0094 |           2.0941 |           0.1555 |
[32m[20230207 15:27:07 @agent_ppo2.py:192][0m |          -0.0118 |           2.0474 |           0.1555 |
[32m[20230207 15:27:07 @agent_ppo2.py:192][0m |          -0.0152 |           1.9801 |           0.1554 |
[32m[20230207 15:27:07 @agent_ppo2.py:192][0m |          -0.0107 |           1.9336 |           0.1553 |
[32m[20230207 15:27:07 @agent_ppo2.py:192][0m |          -0.0113 |           1.9153 |           0.1554 |
[32m[20230207 15:27:07 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:27:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: 24.40
[32m[20230207 15:27:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 121.52
[32m[20230207 15:27:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 78.49
[32m[20230207 15:27:08 @agent_ppo2.py:150][0m Total time:      26.45 min
[32m[20230207 15:27:08 @agent_ppo2.py:152][0m 1388544 total steps have happened
[32m[20230207 15:27:08 @agent_ppo2.py:128][0m #------------------------ Iteration 678 --------------------------#
[32m[20230207 15:27:09 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:27:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:09 @agent_ppo2.py:192][0m |          -0.0075 |           2.0493 |           0.1553 |
[32m[20230207 15:27:09 @agent_ppo2.py:192][0m |          -0.0180 |           1.6584 |           0.1554 |
[32m[20230207 15:27:09 @agent_ppo2.py:192][0m |          -0.0272 |           1.6267 |           0.1554 |
[32m[20230207 15:27:09 @agent_ppo2.py:192][0m |          -0.0157 |           1.5817 |           0.1553 |
[32m[20230207 15:27:09 @agent_ppo2.py:192][0m |          -0.0165 |           1.5505 |           0.1553 |
[32m[20230207 15:27:09 @agent_ppo2.py:192][0m |          -0.0017 |           1.5294 |           0.1555 |
[32m[20230207 15:27:09 @agent_ppo2.py:192][0m |          -0.0088 |           1.5152 |           0.1555 |
[32m[20230207 15:27:09 @agent_ppo2.py:192][0m |           0.0020 |           1.5130 |           0.1554 |
[32m[20230207 15:27:10 @agent_ppo2.py:192][0m |          -0.0269 |           1.5077 |           0.1553 |
[32m[20230207 15:27:10 @agent_ppo2.py:192][0m |          -0.0309 |           1.4787 |           0.1555 |
[32m[20230207 15:27:10 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:27:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: 77.15
[32m[20230207 15:27:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 87.57
[32m[20230207 15:27:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -45.99
[32m[20230207 15:27:10 @agent_ppo2.py:150][0m Total time:      26.49 min
[32m[20230207 15:27:10 @agent_ppo2.py:152][0m 1390592 total steps have happened
[32m[20230207 15:27:10 @agent_ppo2.py:128][0m #------------------------ Iteration 679 --------------------------#
[32m[20230207 15:27:11 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:27:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:11 @agent_ppo2.py:192][0m |          -0.0008 |           1.4703 |           0.1556 |
[32m[20230207 15:27:11 @agent_ppo2.py:192][0m |           0.0168 |           1.1168 |           0.1556 |
[32m[20230207 15:27:11 @agent_ppo2.py:192][0m |           0.0114 |           1.0407 |           0.1555 |
[32m[20230207 15:27:12 @agent_ppo2.py:192][0m |           0.0051 |           1.0026 |           0.1552 |
[32m[20230207 15:27:12 @agent_ppo2.py:192][0m |          -0.0154 |           0.9977 |           0.1552 |
[32m[20230207 15:27:12 @agent_ppo2.py:192][0m |          -0.0183 |           0.9850 |           0.1551 |
[32m[20230207 15:27:12 @agent_ppo2.py:192][0m |           0.0025 |           0.9522 |           0.1549 |
[32m[20230207 15:27:12 @agent_ppo2.py:192][0m |          -0.0041 |           0.9373 |           0.1550 |
[32m[20230207 15:27:12 @agent_ppo2.py:192][0m |          -0.0097 |           0.9325 |           0.1549 |
[32m[20230207 15:27:12 @agent_ppo2.py:192][0m |          -0.0252 |           0.9236 |           0.1547 |
[32m[20230207 15:27:12 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:27:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: 53.97
[32m[20230207 15:27:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 63.32
[32m[20230207 15:27:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -14.98
[32m[20230207 15:27:13 @agent_ppo2.py:150][0m Total time:      26.52 min
[32m[20230207 15:27:13 @agent_ppo2.py:152][0m 1392640 total steps have happened
[32m[20230207 15:27:13 @agent_ppo2.py:128][0m #------------------------ Iteration 680 --------------------------#
[32m[20230207 15:27:13 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:27:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:14 @agent_ppo2.py:192][0m |           0.0055 |           1.3963 |           0.1564 |
[32m[20230207 15:27:14 @agent_ppo2.py:192][0m |          -0.0139 |           1.1767 |           0.1563 |
[32m[20230207 15:27:14 @agent_ppo2.py:192][0m |           0.0002 |           1.1503 |           0.1562 |
[32m[20230207 15:27:14 @agent_ppo2.py:192][0m |          -0.0027 |           1.1345 |           0.1562 |
[32m[20230207 15:27:14 @agent_ppo2.py:192][0m |          -0.0048 |           1.0945 |           0.1562 |
[32m[20230207 15:27:14 @agent_ppo2.py:192][0m |          -0.0122 |           1.0827 |           0.1561 |
[32m[20230207 15:27:14 @agent_ppo2.py:192][0m |          -0.0014 |           1.0928 |           0.1561 |
[32m[20230207 15:27:14 @agent_ppo2.py:192][0m |          -0.0134 |           1.0429 |           0.1561 |
[32m[20230207 15:27:14 @agent_ppo2.py:192][0m |          -0.0089 |           1.0338 |           0.1561 |
[32m[20230207 15:27:14 @agent_ppo2.py:192][0m |          -0.0088 |           1.0257 |           0.1560 |
[32m[20230207 15:27:14 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:27:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: 90.08
[32m[20230207 15:27:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 101.23
[32m[20230207 15:27:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -21.74
[32m[20230207 15:27:15 @agent_ppo2.py:150][0m Total time:      26.57 min
[32m[20230207 15:27:15 @agent_ppo2.py:152][0m 1394688 total steps have happened
[32m[20230207 15:27:15 @agent_ppo2.py:128][0m #------------------------ Iteration 681 --------------------------#
[32m[20230207 15:27:16 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:27:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:16 @agent_ppo2.py:192][0m |           0.0111 |           8.0672 |           0.1564 |
[32m[20230207 15:27:16 @agent_ppo2.py:192][0m |          -0.0041 |           1.8381 |           0.1563 |
[32m[20230207 15:27:16 @agent_ppo2.py:192][0m |          -0.0052 |           1.4203 |           0.1563 |
[32m[20230207 15:27:16 @agent_ppo2.py:192][0m |          -0.0077 |           1.2977 |           0.1563 |
[32m[20230207 15:27:16 @agent_ppo2.py:192][0m |          -0.0084 |           1.2322 |           0.1562 |
[32m[20230207 15:27:16 @agent_ppo2.py:192][0m |          -0.0107 |           1.2695 |           0.1563 |
[32m[20230207 15:27:17 @agent_ppo2.py:192][0m |          -0.0086 |           1.1561 |           0.1563 |
[32m[20230207 15:27:17 @agent_ppo2.py:192][0m |          -0.0093 |           1.1106 |           0.1563 |
[32m[20230207 15:27:17 @agent_ppo2.py:192][0m |          -0.0123 |           1.0894 |           0.1562 |
[32m[20230207 15:27:17 @agent_ppo2.py:192][0m |          -0.0126 |           1.0711 |           0.1562 |
[32m[20230207 15:27:17 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:27:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: 8.00
[32m[20230207 15:27:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 78.16
[32m[20230207 15:27:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 82.59
[32m[20230207 15:27:18 @agent_ppo2.py:150][0m Total time:      26.61 min
[32m[20230207 15:27:18 @agent_ppo2.py:152][0m 1396736 total steps have happened
[32m[20230207 15:27:18 @agent_ppo2.py:128][0m #------------------------ Iteration 682 --------------------------#
[32m[20230207 15:27:18 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:27:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:18 @agent_ppo2.py:192][0m |           0.0187 |          15.0204 |           0.1572 |
[32m[20230207 15:27:19 @agent_ppo2.py:192][0m |          -0.0076 |           8.4928 |           0.1573 |
[32m[20230207 15:27:19 @agent_ppo2.py:192][0m |          -0.0047 |           7.1841 |           0.1573 |
[32m[20230207 15:27:19 @agent_ppo2.py:192][0m |          -0.0140 |           6.2496 |           0.1573 |
[32m[20230207 15:27:19 @agent_ppo2.py:192][0m |           0.0071 |           6.7381 |           0.1574 |
[32m[20230207 15:27:19 @agent_ppo2.py:192][0m |          -0.0076 |           5.5609 |           0.1573 |
[32m[20230207 15:27:19 @agent_ppo2.py:192][0m |          -0.0143 |           5.2677 |           0.1574 |
[32m[20230207 15:27:19 @agent_ppo2.py:192][0m |          -0.0266 |           5.0365 |           0.1574 |
[32m[20230207 15:27:19 @agent_ppo2.py:192][0m |          -0.0215 |           4.8259 |           0.1576 |
[32m[20230207 15:27:19 @agent_ppo2.py:192][0m |          -0.0204 |           4.6305 |           0.1577 |
[32m[20230207 15:27:19 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:27:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: 6.76
[32m[20230207 15:27:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 18.34
[32m[20230207 15:27:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -26.98
[32m[20230207 15:27:20 @agent_ppo2.py:150][0m Total time:      26.65 min
[32m[20230207 15:27:20 @agent_ppo2.py:152][0m 1398784 total steps have happened
[32m[20230207 15:27:20 @agent_ppo2.py:128][0m #------------------------ Iteration 683 --------------------------#
[32m[20230207 15:27:21 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 15:27:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:21 @agent_ppo2.py:192][0m |           0.0016 |          32.0293 |           0.1557 |
[32m[20230207 15:27:21 @agent_ppo2.py:192][0m |          -0.0036 |          12.6858 |           0.1555 |
[32m[20230207 15:27:21 @agent_ppo2.py:192][0m |          -0.0089 |          10.8295 |           0.1555 |
[32m[20230207 15:27:21 @agent_ppo2.py:192][0m |          -0.0120 |           9.3708 |           0.1555 |
[32m[20230207 15:27:21 @agent_ppo2.py:192][0m |          -0.0120 |           7.9669 |           0.1554 |
[32m[20230207 15:27:21 @agent_ppo2.py:192][0m |          -0.0134 |           7.5031 |           0.1554 |
[32m[20230207 15:27:21 @agent_ppo2.py:192][0m |          -0.0146 |           6.9839 |           0.1554 |
[32m[20230207 15:27:21 @agent_ppo2.py:192][0m |          -0.0153 |           6.5624 |           0.1554 |
[32m[20230207 15:27:21 @agent_ppo2.py:192][0m |          -0.0167 |           6.2624 |           0.1553 |
[32m[20230207 15:27:21 @agent_ppo2.py:192][0m |          -0.0176 |           5.9338 |           0.1554 |
[32m[20230207 15:27:21 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:27:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: -14.71
[32m[20230207 15:27:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 166.78
[32m[20230207 15:27:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -1.19
[32m[20230207 15:27:22 @agent_ppo2.py:150][0m Total time:      26.68 min
[32m[20230207 15:27:22 @agent_ppo2.py:152][0m 1400832 total steps have happened
[32m[20230207 15:27:22 @agent_ppo2.py:128][0m #------------------------ Iteration 684 --------------------------#
[32m[20230207 15:27:22 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:27:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:23 @agent_ppo2.py:192][0m |           0.0107 |          28.0091 |           0.1508 |
[32m[20230207 15:27:23 @agent_ppo2.py:192][0m |          -0.0074 |          12.9364 |           0.1504 |
[32m[20230207 15:27:23 @agent_ppo2.py:192][0m |           0.0012 |           8.9126 |           0.1503 |
[32m[20230207 15:27:23 @agent_ppo2.py:192][0m |          -0.0114 |           6.1264 |           0.1503 |
[32m[20230207 15:27:23 @agent_ppo2.py:192][0m |          -0.0088 |           5.2217 |           0.1502 |
[32m[20230207 15:27:23 @agent_ppo2.py:192][0m |          -0.0153 |           4.6501 |           0.1503 |
[32m[20230207 15:27:23 @agent_ppo2.py:192][0m |           0.0172 |           4.4393 |           0.1501 |
[32m[20230207 15:27:23 @agent_ppo2.py:192][0m |          -0.0090 |           4.6892 |           0.1503 |
[32m[20230207 15:27:23 @agent_ppo2.py:192][0m |          -0.0158 |           3.8675 |           0.1503 |
[32m[20230207 15:27:23 @agent_ppo2.py:192][0m |          -0.0159 |           3.7224 |           0.1504 |
[32m[20230207 15:27:23 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:27:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: -43.20
[32m[20230207 15:27:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 13.44
[32m[20230207 15:27:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 105.54
[32m[20230207 15:27:24 @agent_ppo2.py:150][0m Total time:      26.71 min
[32m[20230207 15:27:24 @agent_ppo2.py:152][0m 1402880 total steps have happened
[32m[20230207 15:27:24 @agent_ppo2.py:128][0m #------------------------ Iteration 685 --------------------------#
[32m[20230207 15:27:25 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:27:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:25 @agent_ppo2.py:192][0m |           0.0070 |           2.0391 |           0.1577 |
[32m[20230207 15:27:25 @agent_ppo2.py:192][0m |           0.0015 |           1.6855 |           0.1576 |
[32m[20230207 15:27:25 @agent_ppo2.py:192][0m |           0.0132 |           1.5972 |           0.1575 |
[32m[20230207 15:27:25 @agent_ppo2.py:192][0m |          -0.0065 |           1.5311 |           0.1577 |
[32m[20230207 15:27:25 @agent_ppo2.py:192][0m |          -0.0102 |           1.4870 |           0.1578 |
[32m[20230207 15:27:25 @agent_ppo2.py:192][0m |           0.0041 |           1.4623 |           0.1575 |
[32m[20230207 15:27:25 @agent_ppo2.py:192][0m |          -0.0037 |           1.4183 |           0.1576 |
[32m[20230207 15:27:25 @agent_ppo2.py:192][0m |          -0.0352 |           1.4451 |           0.1577 |
[32m[20230207 15:27:25 @agent_ppo2.py:192][0m |          -0.0193 |           1.3870 |           0.1577 |
[32m[20230207 15:27:25 @agent_ppo2.py:192][0m |           0.0059 |           1.3609 |           0.1577 |
[32m[20230207 15:27:25 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:27:26 @agent_ppo2.py:145][0m Average TRAINING episode reward: 30.89
[32m[20230207 15:27:26 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 48.81
[32m[20230207 15:27:26 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 83.28
[32m[20230207 15:27:26 @agent_ppo2.py:150][0m Total time:      26.75 min
[32m[20230207 15:27:26 @agent_ppo2.py:152][0m 1404928 total steps have happened
[32m[20230207 15:27:26 @agent_ppo2.py:128][0m #------------------------ Iteration 686 --------------------------#
[32m[20230207 15:27:27 @agent_ppo2.py:134][0m Sampling time: 0.54 s by 1 slaves
[32m[20230207 15:27:27 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:27 @agent_ppo2.py:192][0m |           0.0017 |          42.5852 |           0.1539 |
[32m[20230207 15:27:27 @agent_ppo2.py:192][0m |          -0.0083 |          17.0405 |           0.1537 |
[32m[20230207 15:27:27 @agent_ppo2.py:192][0m |          -0.0083 |          14.5594 |           0.1536 |
[32m[20230207 15:27:27 @agent_ppo2.py:192][0m |          -0.0220 |          12.8615 |           0.1537 |
[32m[20230207 15:27:27 @agent_ppo2.py:192][0m |          -0.0149 |          12.0347 |           0.1537 |
[32m[20230207 15:27:27 @agent_ppo2.py:192][0m |          -0.0125 |          10.7939 |           0.1537 |
[32m[20230207 15:27:27 @agent_ppo2.py:192][0m |          -0.0185 |          10.1052 |           0.1537 |
[32m[20230207 15:27:27 @agent_ppo2.py:192][0m |          -0.0271 |           9.6950 |           0.1536 |
[32m[20230207 15:27:27 @agent_ppo2.py:192][0m |          -0.0267 |           9.9306 |           0.1536 |
[32m[20230207 15:27:27 @agent_ppo2.py:192][0m |          -0.0228 |           9.0651 |           0.1536 |
[32m[20230207 15:27:27 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:27:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: -77.28
[32m[20230207 15:27:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -39.61
[32m[20230207 15:27:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 91.85
[32m[20230207 15:27:28 @agent_ppo2.py:150][0m Total time:      26.78 min
[32m[20230207 15:27:28 @agent_ppo2.py:152][0m 1406976 total steps have happened
[32m[20230207 15:27:28 @agent_ppo2.py:128][0m #------------------------ Iteration 687 --------------------------#
[32m[20230207 15:27:29 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:27:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:29 @agent_ppo2.py:192][0m |           0.0290 |          51.2372 |           0.1529 |
[32m[20230207 15:27:29 @agent_ppo2.py:192][0m |           0.0017 |          31.2264 |           0.1528 |
[32m[20230207 15:27:29 @agent_ppo2.py:192][0m |          -0.0093 |          21.9507 |           0.1527 |
[32m[20230207 15:27:29 @agent_ppo2.py:192][0m |           0.0120 |          18.5315 |           0.1526 |
[32m[20230207 15:27:29 @agent_ppo2.py:192][0m |          -0.0161 |          15.8802 |           0.1524 |
[32m[20230207 15:27:29 @agent_ppo2.py:192][0m |           0.0089 |          14.0381 |           0.1524 |
[32m[20230207 15:27:29 @agent_ppo2.py:192][0m |          -0.0216 |          13.3717 |           0.1516 |
[32m[20230207 15:27:29 @agent_ppo2.py:192][0m |          -0.0349 |          12.3217 |           0.1518 |
[32m[20230207 15:27:29 @agent_ppo2.py:192][0m |          -0.0276 |          11.4595 |           0.1519 |
[32m[20230207 15:27:29 @agent_ppo2.py:192][0m |          -0.0309 |          10.4249 |           0.1521 |
[32m[20230207 15:27:29 @agent_ppo2.py:137][0m Policy update time: 0.61 s
[32m[20230207 15:27:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -88.66
[32m[20230207 15:27:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -35.23
[32m[20230207 15:27:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 125.34
[32m[20230207 15:27:30 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 125.34
[32m[20230207 15:27:30 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 125.34
[32m[20230207 15:27:30 @agent_ppo2.py:150][0m Total time:      26.81 min
[32m[20230207 15:27:30 @agent_ppo2.py:152][0m 1409024 total steps have happened
[32m[20230207 15:27:30 @agent_ppo2.py:128][0m #------------------------ Iteration 688 --------------------------#
[32m[20230207 15:27:31 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:27:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:31 @agent_ppo2.py:192][0m |          -0.0084 |           2.4869 |           0.1572 |
[32m[20230207 15:27:31 @agent_ppo2.py:192][0m |           0.0025 |           2.2785 |           0.1569 |
[32m[20230207 15:27:31 @agent_ppo2.py:192][0m |          -0.0048 |           2.2030 |           0.1569 |
[32m[20230207 15:27:31 @agent_ppo2.py:192][0m |          -0.0133 |           2.1547 |           0.1571 |
[32m[20230207 15:27:31 @agent_ppo2.py:192][0m |          -0.0164 |           2.1216 |           0.1572 |
[32m[20230207 15:27:31 @agent_ppo2.py:192][0m |          -0.0087 |           2.0906 |           0.1573 |
[32m[20230207 15:27:31 @agent_ppo2.py:192][0m |           0.0034 |           2.0917 |           0.1570 |
[32m[20230207 15:27:31 @agent_ppo2.py:192][0m |          -0.0041 |           2.0623 |           0.1571 |
[32m[20230207 15:27:32 @agent_ppo2.py:192][0m |          -0.0032 |           2.0362 |           0.1571 |
[32m[20230207 15:27:32 @agent_ppo2.py:192][0m |          -0.0051 |           2.0237 |           0.1572 |
[32m[20230207 15:27:32 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:27:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: 41.91
[32m[20230207 15:27:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 66.06
[32m[20230207 15:27:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 21.15
[32m[20230207 15:27:32 @agent_ppo2.py:150][0m Total time:      26.85 min
[32m[20230207 15:27:32 @agent_ppo2.py:152][0m 1411072 total steps have happened
[32m[20230207 15:27:32 @agent_ppo2.py:128][0m #------------------------ Iteration 689 --------------------------#
[32m[20230207 15:27:33 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:27:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:33 @agent_ppo2.py:192][0m |          -0.0168 |           1.3334 |           0.1586 |
[32m[20230207 15:27:33 @agent_ppo2.py:192][0m |          -0.0156 |           1.0477 |           0.1583 |
[32m[20230207 15:27:34 @agent_ppo2.py:192][0m |          -0.0186 |           0.9706 |           0.1585 |
[32m[20230207 15:27:34 @agent_ppo2.py:192][0m |          -0.0148 |           0.9227 |           0.1585 |
[32m[20230207 15:27:34 @agent_ppo2.py:192][0m |          -0.0094 |           0.9068 |           0.1586 |
[32m[20230207 15:27:34 @agent_ppo2.py:192][0m |           0.0038 |           0.8788 |           0.1587 |
[32m[20230207 15:27:34 @agent_ppo2.py:192][0m |          -0.0094 |           0.8670 |           0.1587 |
[32m[20230207 15:27:34 @agent_ppo2.py:192][0m |          -0.0189 |           0.8563 |           0.1587 |
[32m[20230207 15:27:34 @agent_ppo2.py:192][0m |           0.0003 |           0.8527 |           0.1588 |
[32m[20230207 15:27:34 @agent_ppo2.py:192][0m |          -0.0143 |           0.8375 |           0.1587 |
[32m[20230207 15:27:34 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:27:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: 64.72
[32m[20230207 15:27:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 84.18
[32m[20230207 15:27:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -54.98
[32m[20230207 15:27:35 @agent_ppo2.py:150][0m Total time:      26.89 min
[32m[20230207 15:27:35 @agent_ppo2.py:152][0m 1413120 total steps have happened
[32m[20230207 15:27:35 @agent_ppo2.py:128][0m #------------------------ Iteration 690 --------------------------#
[32m[20230207 15:27:35 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:27:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:36 @agent_ppo2.py:192][0m |          -0.0012 |          15.4543 |           0.1578 |
[32m[20230207 15:27:36 @agent_ppo2.py:192][0m |          -0.0044 |           9.1943 |           0.1577 |
[32m[20230207 15:27:36 @agent_ppo2.py:192][0m |          -0.0037 |           8.0606 |           0.1577 |
[32m[20230207 15:27:36 @agent_ppo2.py:192][0m |          -0.0071 |           7.3126 |           0.1576 |
[32m[20230207 15:27:36 @agent_ppo2.py:192][0m |          -0.0036 |           7.2068 |           0.1575 |
[32m[20230207 15:27:36 @agent_ppo2.py:192][0m |          -0.0094 |           6.8174 |           0.1576 |
[32m[20230207 15:27:36 @agent_ppo2.py:192][0m |          -0.0093 |           6.3897 |           0.1575 |
[32m[20230207 15:27:36 @agent_ppo2.py:192][0m |          -0.0107 |           6.1863 |           0.1573 |
[32m[20230207 15:27:36 @agent_ppo2.py:192][0m |          -0.0113 |           5.8981 |           0.1575 |
[32m[20230207 15:27:36 @agent_ppo2.py:192][0m |          -0.0134 |           5.7074 |           0.1574 |
[32m[20230207 15:27:36 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:27:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: 21.96
[32m[20230207 15:27:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 101.45
[32m[20230207 15:27:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -37.23
[32m[20230207 15:27:37 @agent_ppo2.py:150][0m Total time:      26.93 min
[32m[20230207 15:27:37 @agent_ppo2.py:152][0m 1415168 total steps have happened
[32m[20230207 15:27:37 @agent_ppo2.py:128][0m #------------------------ Iteration 691 --------------------------#
[32m[20230207 15:27:38 @agent_ppo2.py:134][0m Sampling time: 0.71 s by 1 slaves
[32m[20230207 15:27:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:38 @agent_ppo2.py:192][0m |           0.0017 |          12.8449 |           0.1627 |
[32m[20230207 15:27:38 @agent_ppo2.py:192][0m |          -0.0055 |           5.2008 |           0.1623 |
[32m[20230207 15:27:38 @agent_ppo2.py:192][0m |          -0.0107 |           4.4612 |           0.1623 |
[32m[20230207 15:27:38 @agent_ppo2.py:192][0m |          -0.0101 |           4.0552 |           0.1621 |
[32m[20230207 15:27:38 @agent_ppo2.py:192][0m |          -0.0133 |           3.7613 |           0.1622 |
[32m[20230207 15:27:38 @agent_ppo2.py:192][0m |          -0.0110 |           3.5489 |           0.1622 |
[32m[20230207 15:27:38 @agent_ppo2.py:192][0m |          -0.0088 |           3.3631 |           0.1622 |
[32m[20230207 15:27:38 @agent_ppo2.py:192][0m |          -0.0092 |           3.2364 |           0.1621 |
[32m[20230207 15:27:38 @agent_ppo2.py:192][0m |          -0.0140 |           3.1519 |           0.1620 |
[32m[20230207 15:27:38 @agent_ppo2.py:192][0m |          -0.0150 |           3.0343 |           0.1622 |
[32m[20230207 15:27:38 @agent_ppo2.py:137][0m Policy update time: 0.74 s
[32m[20230207 15:27:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -8.23
[32m[20230207 15:27:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 62.20
[32m[20230207 15:27:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 49.56
[32m[20230207 15:27:39 @agent_ppo2.py:150][0m Total time:      26.97 min
[32m[20230207 15:27:39 @agent_ppo2.py:152][0m 1417216 total steps have happened
[32m[20230207 15:27:39 @agent_ppo2.py:128][0m #------------------------ Iteration 692 --------------------------#
[32m[20230207 15:27:40 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:27:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:40 @agent_ppo2.py:192][0m |          -0.0023 |          10.1611 |           0.1571 |
[32m[20230207 15:27:40 @agent_ppo2.py:192][0m |          -0.0052 |           7.1445 |           0.1569 |
[32m[20230207 15:27:40 @agent_ppo2.py:192][0m |          -0.0098 |           6.5505 |           0.1568 |
[32m[20230207 15:27:40 @agent_ppo2.py:192][0m |          -0.0103 |           6.2816 |           0.1566 |
[32m[20230207 15:27:40 @agent_ppo2.py:192][0m |          -0.0124 |           6.1054 |           0.1566 |
[32m[20230207 15:27:41 @agent_ppo2.py:192][0m |          -0.0133 |           5.9396 |           0.1565 |
[32m[20230207 15:27:41 @agent_ppo2.py:192][0m |          -0.0151 |           5.8489 |           0.1566 |
[32m[20230207 15:27:41 @agent_ppo2.py:192][0m |          -0.0144 |           5.6867 |           0.1564 |
[32m[20230207 15:27:41 @agent_ppo2.py:192][0m |          -0.0175 |           5.6684 |           0.1564 |
[32m[20230207 15:27:41 @agent_ppo2.py:192][0m |          -0.0154 |           5.6347 |           0.1565 |
[32m[20230207 15:27:41 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:27:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: -7.45
[32m[20230207 15:27:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 46.49
[32m[20230207 15:27:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 17.61
[32m[20230207 15:27:42 @agent_ppo2.py:150][0m Total time:      27.01 min
[32m[20230207 15:27:42 @agent_ppo2.py:152][0m 1419264 total steps have happened
[32m[20230207 15:27:42 @agent_ppo2.py:128][0m #------------------------ Iteration 693 --------------------------#
[32m[20230207 15:27:43 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:27:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:43 @agent_ppo2.py:192][0m |           0.0012 |          11.2922 |           0.1589 |
[32m[20230207 15:27:43 @agent_ppo2.py:192][0m |          -0.0064 |           6.4477 |           0.1589 |
[32m[20230207 15:27:43 @agent_ppo2.py:192][0m |          -0.0083 |           5.3551 |           0.1589 |
[32m[20230207 15:27:43 @agent_ppo2.py:192][0m |          -0.0085 |           4.9447 |           0.1588 |
[32m[20230207 15:27:43 @agent_ppo2.py:192][0m |          -0.0092 |           4.7193 |           0.1588 |
[32m[20230207 15:27:43 @agent_ppo2.py:192][0m |          -0.0105 |           4.3089 |           0.1587 |
[32m[20230207 15:27:43 @agent_ppo2.py:192][0m |          -0.0114 |           4.1102 |           0.1588 |
[32m[20230207 15:27:43 @agent_ppo2.py:192][0m |          -0.0113 |           3.8817 |           0.1587 |
[32m[20230207 15:27:43 @agent_ppo2.py:192][0m |          -0.0092 |           3.8002 |           0.1587 |
[32m[20230207 15:27:43 @agent_ppo2.py:192][0m |          -0.0111 |           3.6329 |           0.1587 |
[32m[20230207 15:27:43 @agent_ppo2.py:137][0m Policy update time: 0.93 s
[32m[20230207 15:27:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: -18.47
[32m[20230207 15:27:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 34.93
[32m[20230207 15:27:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 0.15
[32m[20230207 15:27:44 @agent_ppo2.py:150][0m Total time:      27.05 min
[32m[20230207 15:27:44 @agent_ppo2.py:152][0m 1421312 total steps have happened
[32m[20230207 15:27:44 @agent_ppo2.py:128][0m #------------------------ Iteration 694 --------------------------#
[32m[20230207 15:27:45 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:27:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:45 @agent_ppo2.py:192][0m |           0.0054 |           2.5134 |           0.1546 |
[32m[20230207 15:27:45 @agent_ppo2.py:192][0m |           0.0024 |           1.6796 |           0.1546 |
[32m[20230207 15:27:45 @agent_ppo2.py:192][0m |          -0.0012 |           1.4891 |           0.1546 |
[32m[20230207 15:27:45 @agent_ppo2.py:192][0m |          -0.0022 |           1.4141 |           0.1545 |
[32m[20230207 15:27:46 @agent_ppo2.py:192][0m |          -0.0127 |           1.3352 |           0.1544 |
[32m[20230207 15:27:46 @agent_ppo2.py:192][0m |          -0.0025 |           1.2917 |           0.1543 |
[32m[20230207 15:27:46 @agent_ppo2.py:192][0m |          -0.0051 |           1.2639 |           0.1543 |
[32m[20230207 15:27:46 @agent_ppo2.py:192][0m |          -0.0192 |           1.2389 |           0.1546 |
[32m[20230207 15:27:46 @agent_ppo2.py:192][0m |          -0.0199 |           1.2202 |           0.1545 |
[32m[20230207 15:27:46 @agent_ppo2.py:192][0m |          -0.0028 |           1.2075 |           0.1545 |
[32m[20230207 15:27:46 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:27:47 @agent_ppo2.py:145][0m Average TRAINING episode reward: 106.32
[32m[20230207 15:27:47 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 116.09
[32m[20230207 15:27:47 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -45.43
[32m[20230207 15:27:47 @agent_ppo2.py:150][0m Total time:      27.09 min
[32m[20230207 15:27:47 @agent_ppo2.py:152][0m 1423360 total steps have happened
[32m[20230207 15:27:47 @agent_ppo2.py:128][0m #------------------------ Iteration 695 --------------------------#
[32m[20230207 15:27:48 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:27:48 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:48 @agent_ppo2.py:192][0m |           0.0061 |           1.3541 |           0.1610 |
[32m[20230207 15:27:48 @agent_ppo2.py:192][0m |           0.0039 |           1.1717 |           0.1609 |
[32m[20230207 15:27:48 @agent_ppo2.py:192][0m |          -0.0106 |           1.1141 |           0.1609 |
[32m[20230207 15:27:48 @agent_ppo2.py:192][0m |          -0.0079 |           1.0825 |           0.1608 |
[32m[20230207 15:27:48 @agent_ppo2.py:192][0m |          -0.0292 |           1.0683 |           0.1608 |
[32m[20230207 15:27:48 @agent_ppo2.py:192][0m |          -0.0124 |           1.0522 |           0.1606 |
[32m[20230207 15:27:48 @agent_ppo2.py:192][0m |          -0.0238 |           1.0439 |           0.1607 |
[32m[20230207 15:27:48 @agent_ppo2.py:192][0m |           0.0086 |           1.0331 |           0.1608 |
[32m[20230207 15:27:48 @agent_ppo2.py:192][0m |           0.0050 |           1.0375 |           0.1608 |
[32m[20230207 15:27:48 @agent_ppo2.py:192][0m |          -0.0046 |           1.0211 |           0.1607 |
[32m[20230207 15:27:48 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:27:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: 86.94
[32m[20230207 15:27:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 119.07
[32m[20230207 15:27:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 103.52
[32m[20230207 15:27:49 @agent_ppo2.py:150][0m Total time:      27.13 min
[32m[20230207 15:27:49 @agent_ppo2.py:152][0m 1425408 total steps have happened
[32m[20230207 15:27:49 @agent_ppo2.py:128][0m #------------------------ Iteration 696 --------------------------#
[32m[20230207 15:27:50 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:27:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:50 @agent_ppo2.py:192][0m |           0.0035 |           7.2619 |           0.1624 |
[32m[20230207 15:27:50 @agent_ppo2.py:192][0m |          -0.0012 |           1.8960 |           0.1625 |
[32m[20230207 15:27:50 @agent_ppo2.py:192][0m |          -0.0022 |           1.6329 |           0.1623 |
[32m[20230207 15:27:50 @agent_ppo2.py:192][0m |          -0.0061 |           1.5515 |           0.1622 |
[32m[20230207 15:27:50 @agent_ppo2.py:192][0m |          -0.0064 |           1.4710 |           0.1623 |
[32m[20230207 15:27:51 @agent_ppo2.py:192][0m |          -0.0087 |           1.4495 |           0.1621 |
[32m[20230207 15:27:51 @agent_ppo2.py:192][0m |          -0.0097 |           1.3851 |           0.1621 |
[32m[20230207 15:27:51 @agent_ppo2.py:192][0m |          -0.0095 |           1.3761 |           0.1620 |
[32m[20230207 15:27:51 @agent_ppo2.py:192][0m |          -0.0107 |           1.3449 |           0.1620 |
[32m[20230207 15:27:51 @agent_ppo2.py:192][0m |          -0.0106 |           1.3141 |           0.1619 |
[32m[20230207 15:27:51 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:27:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: 39.26
[32m[20230207 15:27:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 119.00
[32m[20230207 15:27:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -72.95
[32m[20230207 15:27:52 @agent_ppo2.py:150][0m Total time:      27.17 min
[32m[20230207 15:27:52 @agent_ppo2.py:152][0m 1427456 total steps have happened
[32m[20230207 15:27:52 @agent_ppo2.py:128][0m #------------------------ Iteration 697 --------------------------#
[32m[20230207 15:27:52 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:27:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:52 @agent_ppo2.py:192][0m |          -0.0043 |          20.5227 |           0.1566 |
[32m[20230207 15:27:52 @agent_ppo2.py:192][0m |          -0.0072 |           6.4648 |           0.1565 |
[32m[20230207 15:27:52 @agent_ppo2.py:192][0m |          -0.0098 |           4.0613 |           0.1564 |
[32m[20230207 15:27:52 @agent_ppo2.py:192][0m |           0.0013 |           3.7144 |           0.1563 |
[32m[20230207 15:27:52 @agent_ppo2.py:192][0m |          -0.0078 |           3.1941 |           0.1560 |
[32m[20230207 15:27:52 @agent_ppo2.py:192][0m |          -0.0138 |           2.5643 |           0.1558 |
[32m[20230207 15:27:52 @agent_ppo2.py:192][0m |          -0.0111 |           2.3931 |           0.1561 |
[32m[20230207 15:27:53 @agent_ppo2.py:192][0m |          -0.0125 |           2.3906 |           0.1560 |
[32m[20230207 15:27:53 @agent_ppo2.py:192][0m |          -0.0191 |           2.1518 |           0.1560 |
[32m[20230207 15:27:53 @agent_ppo2.py:192][0m |          -0.0189 |           2.0649 |           0.1559 |
[32m[20230207 15:27:53 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:27:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: 4.38
[32m[20230207 15:27:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 113.82
[32m[20230207 15:27:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -36.29
[32m[20230207 15:27:53 @agent_ppo2.py:150][0m Total time:      27.20 min
[32m[20230207 15:27:53 @agent_ppo2.py:152][0m 1429504 total steps have happened
[32m[20230207 15:27:53 @agent_ppo2.py:128][0m #------------------------ Iteration 698 --------------------------#
[32m[20230207 15:27:54 @agent_ppo2.py:134][0m Sampling time: 0.66 s by 1 slaves
[32m[20230207 15:27:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:54 @agent_ppo2.py:192][0m |           0.0077 |          60.1588 |           0.1581 |
[32m[20230207 15:27:54 @agent_ppo2.py:192][0m |           0.0026 |          28.4161 |           0.1579 |
[32m[20230207 15:27:54 @agent_ppo2.py:192][0m |          -0.0069 |          22.6947 |           0.1576 |
[32m[20230207 15:27:54 @agent_ppo2.py:192][0m |          -0.0085 |          19.7782 |           0.1577 |
[32m[20230207 15:27:54 @agent_ppo2.py:192][0m |          -0.0103 |          17.3713 |           0.1575 |
[32m[20230207 15:27:54 @agent_ppo2.py:192][0m |          -0.0085 |          16.5062 |           0.1573 |
[32m[20230207 15:27:54 @agent_ppo2.py:192][0m |          -0.0093 |          14.9505 |           0.1574 |
[32m[20230207 15:27:54 @agent_ppo2.py:192][0m |          -0.0087 |          14.6456 |           0.1573 |
[32m[20230207 15:27:54 @agent_ppo2.py:192][0m |          -0.0130 |          13.5292 |           0.1571 |
[32m[20230207 15:27:55 @agent_ppo2.py:192][0m |          -0.0114 |          13.0430 |           0.1571 |
[32m[20230207 15:27:55 @agent_ppo2.py:137][0m Policy update time: 0.70 s
[32m[20230207 15:27:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: -98.65
[32m[20230207 15:27:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -74.23
[32m[20230207 15:27:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -73.81
[32m[20230207 15:27:55 @agent_ppo2.py:150][0m Total time:      27.24 min
[32m[20230207 15:27:55 @agent_ppo2.py:152][0m 1431552 total steps have happened
[32m[20230207 15:27:55 @agent_ppo2.py:128][0m #------------------------ Iteration 699 --------------------------#
[32m[20230207 15:27:56 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:27:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:56 @agent_ppo2.py:192][0m |          -0.0107 |           2.9995 |           0.1610 |
[32m[20230207 15:27:56 @agent_ppo2.py:192][0m |           0.0048 |           2.0201 |           0.1608 |
[32m[20230207 15:27:57 @agent_ppo2.py:192][0m |          -0.0006 |           1.8339 |           0.1609 |
[32m[20230207 15:27:57 @agent_ppo2.py:192][0m |          -0.0163 |           1.7479 |           0.1607 |
[32m[20230207 15:27:57 @agent_ppo2.py:192][0m |          -0.0451 |           1.6788 |           0.1606 |
[32m[20230207 15:27:57 @agent_ppo2.py:192][0m |          -0.0139 |           1.6811 |           0.1606 |
[32m[20230207 15:27:57 @agent_ppo2.py:192][0m |          -0.0041 |           1.5870 |           0.1606 |
[32m[20230207 15:27:57 @agent_ppo2.py:192][0m |          -0.0093 |           1.5736 |           0.1604 |
[32m[20230207 15:27:57 @agent_ppo2.py:192][0m |          -0.0126 |           1.4951 |           0.1606 |
[32m[20230207 15:27:57 @agent_ppo2.py:192][0m |          -0.0041 |           1.4679 |           0.1606 |
[32m[20230207 15:27:57 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:27:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: 24.03
[32m[20230207 15:27:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 28.48
[32m[20230207 15:27:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 81.14
[32m[20230207 15:27:58 @agent_ppo2.py:150][0m Total time:      27.28 min
[32m[20230207 15:27:58 @agent_ppo2.py:152][0m 1433600 total steps have happened
[32m[20230207 15:27:58 @agent_ppo2.py:128][0m #------------------------ Iteration 700 --------------------------#
[32m[20230207 15:27:59 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:27:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:27:59 @agent_ppo2.py:192][0m |          -0.0006 |          11.3042 |           0.1584 |
[32m[20230207 15:27:59 @agent_ppo2.py:192][0m |          -0.0070 |           3.4524 |           0.1584 |
[32m[20230207 15:27:59 @agent_ppo2.py:192][0m |          -0.0073 |           2.3353 |           0.1583 |
[32m[20230207 15:27:59 @agent_ppo2.py:192][0m |          -0.0092 |           2.0543 |           0.1583 |
[32m[20230207 15:27:59 @agent_ppo2.py:192][0m |          -0.0127 |           1.9321 |           0.1584 |
[32m[20230207 15:27:59 @agent_ppo2.py:192][0m |          -0.0116 |           1.8479 |           0.1584 |
[32m[20230207 15:27:59 @agent_ppo2.py:192][0m |          -0.0138 |           1.7652 |           0.1584 |
[32m[20230207 15:27:59 @agent_ppo2.py:192][0m |          -0.0141 |           1.7116 |           0.1584 |
[32m[20230207 15:27:59 @agent_ppo2.py:192][0m |          -0.0141 |           1.6839 |           0.1584 |
[32m[20230207 15:27:59 @agent_ppo2.py:192][0m |          -0.0150 |           1.6139 |           0.1584 |
[32m[20230207 15:27:59 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:28:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: 14.67
[32m[20230207 15:28:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 91.48
[32m[20230207 15:28:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 107.51
[32m[20230207 15:28:00 @agent_ppo2.py:150][0m Total time:      27.32 min
[32m[20230207 15:28:00 @agent_ppo2.py:152][0m 1435648 total steps have happened
[32m[20230207 15:28:00 @agent_ppo2.py:128][0m #------------------------ Iteration 701 --------------------------#
[32m[20230207 15:28:01 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:28:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:01 @agent_ppo2.py:192][0m |           0.0052 |           3.0355 |           0.1629 |
[32m[20230207 15:28:01 @agent_ppo2.py:192][0m |           0.0015 |           2.3050 |           0.1627 |
[32m[20230207 15:28:01 @agent_ppo2.py:192][0m |          -0.0044 |           2.1149 |           0.1625 |
[32m[20230207 15:28:01 @agent_ppo2.py:192][0m |           0.0011 |           1.9782 |           0.1625 |
[32m[20230207 15:28:01 @agent_ppo2.py:192][0m |          -0.0064 |           1.9088 |           0.1624 |
[32m[20230207 15:28:02 @agent_ppo2.py:192][0m |          -0.0063 |           1.8396 |           0.1625 |
[32m[20230207 15:28:02 @agent_ppo2.py:192][0m |          -0.0034 |           1.7767 |           0.1625 |
[32m[20230207 15:28:02 @agent_ppo2.py:192][0m |          -0.0048 |           1.7290 |           0.1625 |
[32m[20230207 15:28:02 @agent_ppo2.py:192][0m |          -0.0076 |           1.6969 |           0.1626 |
[32m[20230207 15:28:02 @agent_ppo2.py:192][0m |          -0.0147 |           1.6838 |           0.1626 |
[32m[20230207 15:28:02 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:28:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: 67.24
[32m[20230207 15:28:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 72.03
[32m[20230207 15:28:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -5.88
[32m[20230207 15:28:03 @agent_ppo2.py:150][0m Total time:      27.36 min
[32m[20230207 15:28:03 @agent_ppo2.py:152][0m 1437696 total steps have happened
[32m[20230207 15:28:03 @agent_ppo2.py:128][0m #------------------------ Iteration 702 --------------------------#
[32m[20230207 15:28:03 @agent_ppo2.py:134][0m Sampling time: 0.71 s by 1 slaves
[32m[20230207 15:28:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:04 @agent_ppo2.py:192][0m |           0.0002 |          11.1078 |           0.1614 |
[32m[20230207 15:28:04 @agent_ppo2.py:192][0m |          -0.0031 |           6.2853 |           0.1613 |
[32m[20230207 15:28:04 @agent_ppo2.py:192][0m |          -0.0068 |           4.8747 |           0.1609 |
[32m[20230207 15:28:04 @agent_ppo2.py:192][0m |          -0.0086 |           4.6237 |           0.1610 |
[32m[20230207 15:28:04 @agent_ppo2.py:192][0m |          -0.0111 |           4.0358 |           0.1610 |
[32m[20230207 15:28:04 @agent_ppo2.py:192][0m |          -0.0122 |           3.5507 |           0.1608 |
[32m[20230207 15:28:04 @agent_ppo2.py:192][0m |          -0.0138 |           3.4470 |           0.1607 |
[32m[20230207 15:28:04 @agent_ppo2.py:192][0m |          -0.0148 |           3.1779 |           0.1606 |
[32m[20230207 15:28:04 @agent_ppo2.py:192][0m |          -0.0140 |           3.1274 |           0.1606 |
[32m[20230207 15:28:04 @agent_ppo2.py:192][0m |          -0.0137 |           2.8953 |           0.1606 |
[32m[20230207 15:28:04 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230207 15:28:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -16.17
[32m[20230207 15:28:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 123.84
[32m[20230207 15:28:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 86.81
[32m[20230207 15:28:05 @agent_ppo2.py:150][0m Total time:      27.40 min
[32m[20230207 15:28:05 @agent_ppo2.py:152][0m 1439744 total steps have happened
[32m[20230207 15:28:05 @agent_ppo2.py:128][0m #------------------------ Iteration 703 --------------------------#
[32m[20230207 15:28:06 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:28:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:06 @agent_ppo2.py:192][0m |          -0.0057 |           3.6797 |           0.1584 |
[32m[20230207 15:28:06 @agent_ppo2.py:192][0m |          -0.0002 |           3.0076 |           0.1584 |
[32m[20230207 15:28:06 @agent_ppo2.py:192][0m |          -0.0069 |           2.8507 |           0.1582 |
[32m[20230207 15:28:06 @agent_ppo2.py:192][0m |          -0.0156 |           2.7836 |           0.1584 |
[32m[20230207 15:28:06 @agent_ppo2.py:192][0m |           0.0005 |           2.7355 |           0.1581 |
[32m[20230207 15:28:06 @agent_ppo2.py:192][0m |          -0.0050 |           2.6736 |           0.1582 |
[32m[20230207 15:28:06 @agent_ppo2.py:192][0m |          -0.0124 |           2.5888 |           0.1581 |
[32m[20230207 15:28:06 @agent_ppo2.py:192][0m |          -0.0101 |           2.5508 |           0.1582 |
[32m[20230207 15:28:06 @agent_ppo2.py:192][0m |          -0.0074 |           2.5058 |           0.1581 |
[32m[20230207 15:28:07 @agent_ppo2.py:192][0m |          -0.0132 |           2.4911 |           0.1581 |
[32m[20230207 15:28:07 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:28:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: 65.72
[32m[20230207 15:28:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 74.14
[32m[20230207 15:28:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -8.40
[32m[20230207 15:28:07 @agent_ppo2.py:150][0m Total time:      27.43 min
[32m[20230207 15:28:07 @agent_ppo2.py:152][0m 1441792 total steps have happened
[32m[20230207 15:28:07 @agent_ppo2.py:128][0m #------------------------ Iteration 704 --------------------------#
[32m[20230207 15:28:08 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:28:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:08 @agent_ppo2.py:192][0m |           0.0245 |           2.0030 |           0.1575 |
[32m[20230207 15:28:08 @agent_ppo2.py:192][0m |           0.0014 |           1.4183 |           0.1573 |
[32m[20230207 15:28:08 @agent_ppo2.py:192][0m |          -0.0059 |           1.3330 |           0.1572 |
[32m[20230207 15:28:08 @agent_ppo2.py:192][0m |          -0.0069 |           1.2829 |           0.1569 |
[32m[20230207 15:28:09 @agent_ppo2.py:192][0m |           0.0008 |           1.2597 |           0.1570 |
[32m[20230207 15:28:09 @agent_ppo2.py:192][0m |          -0.0141 |           1.2251 |           0.1569 |
[32m[20230207 15:28:09 @agent_ppo2.py:192][0m |          -0.0166 |           1.2038 |           0.1569 |
[32m[20230207 15:28:09 @agent_ppo2.py:192][0m |           0.0005 |           1.1869 |           0.1568 |
[32m[20230207 15:28:09 @agent_ppo2.py:192][0m |          -0.0049 |           1.1629 |           0.1569 |
[32m[20230207 15:28:09 @agent_ppo2.py:192][0m |          -0.0186 |           1.1448 |           0.1569 |
[32m[20230207 15:28:09 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:28:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: 102.55
[32m[20230207 15:28:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 121.79
[32m[20230207 15:28:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -48.94
[32m[20230207 15:28:09 @agent_ppo2.py:150][0m Total time:      27.47 min
[32m[20230207 15:28:09 @agent_ppo2.py:152][0m 1443840 total steps have happened
[32m[20230207 15:28:09 @agent_ppo2.py:128][0m #------------------------ Iteration 705 --------------------------#
[32m[20230207 15:28:10 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:28:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:10 @agent_ppo2.py:192][0m |           0.0033 |          12.3320 |           0.1611 |
[32m[20230207 15:28:10 @agent_ppo2.py:192][0m |          -0.0063 |           6.8064 |           0.1609 |
[32m[20230207 15:28:10 @agent_ppo2.py:192][0m |          -0.0106 |           5.1051 |           0.1608 |
[32m[20230207 15:28:10 @agent_ppo2.py:192][0m |          -0.0152 |           4.1124 |           0.1606 |
[32m[20230207 15:28:10 @agent_ppo2.py:192][0m |          -0.0102 |           3.8052 |           0.1604 |
[32m[20230207 15:28:10 @agent_ppo2.py:192][0m |          -0.0076 |           3.3941 |           0.1604 |
[32m[20230207 15:28:10 @agent_ppo2.py:192][0m |          -0.0125 |           3.1165 |           0.1604 |
[32m[20230207 15:28:11 @agent_ppo2.py:192][0m |          -0.0140 |           2.9412 |           0.1603 |
[32m[20230207 15:28:11 @agent_ppo2.py:192][0m |          -0.0176 |           2.7788 |           0.1603 |
[32m[20230207 15:28:11 @agent_ppo2.py:192][0m |          -0.0190 |           2.6789 |           0.1602 |
[32m[20230207 15:28:11 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:28:11 @agent_ppo2.py:145][0m Average TRAINING episode reward: -31.48
[32m[20230207 15:28:11 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 79.57
[32m[20230207 15:28:11 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 15.70
[32m[20230207 15:28:11 @agent_ppo2.py:150][0m Total time:      27.50 min
[32m[20230207 15:28:11 @agent_ppo2.py:152][0m 1445888 total steps have happened
[32m[20230207 15:28:11 @agent_ppo2.py:128][0m #------------------------ Iteration 706 --------------------------#
[32m[20230207 15:28:12 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:28:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:12 @agent_ppo2.py:192][0m |          -0.0070 |           1.7191 |           0.1623 |
[32m[20230207 15:28:12 @agent_ppo2.py:192][0m |           0.0006 |           1.5031 |           0.1623 |
[32m[20230207 15:28:13 @agent_ppo2.py:192][0m |          -0.0243 |           1.4704 |           0.1622 |
[32m[20230207 15:28:13 @agent_ppo2.py:192][0m |          -0.0096 |           1.4555 |           0.1622 |
[32m[20230207 15:28:13 @agent_ppo2.py:192][0m |          -0.0397 |           1.4175 |           0.1622 |
[32m[20230207 15:28:13 @agent_ppo2.py:192][0m |          -0.0026 |           1.3782 |           0.1621 |
[32m[20230207 15:28:13 @agent_ppo2.py:192][0m |          -0.0075 |           1.3480 |           0.1621 |
[32m[20230207 15:28:13 @agent_ppo2.py:192][0m |           0.0021 |           1.3602 |           0.1621 |
[32m[20230207 15:28:13 @agent_ppo2.py:192][0m |          -0.0188 |           1.3669 |           0.1621 |
[32m[20230207 15:28:13 @agent_ppo2.py:192][0m |          -0.0144 |           1.3220 |           0.1621 |
[32m[20230207 15:28:13 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:28:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: 53.51
[32m[20230207 15:28:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 74.29
[32m[20230207 15:28:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 24.83
[32m[20230207 15:28:14 @agent_ppo2.py:150][0m Total time:      27.54 min
[32m[20230207 15:28:14 @agent_ppo2.py:152][0m 1447936 total steps have happened
[32m[20230207 15:28:14 @agent_ppo2.py:128][0m #------------------------ Iteration 707 --------------------------#
[32m[20230207 15:28:15 @agent_ppo2.py:134][0m Sampling time: 0.75 s by 1 slaves
[32m[20230207 15:28:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:15 @agent_ppo2.py:192][0m |          -0.0012 |           6.0696 |           0.1592 |
[32m[20230207 15:28:15 @agent_ppo2.py:192][0m |          -0.0035 |           2.8162 |           0.1591 |
[32m[20230207 15:28:15 @agent_ppo2.py:192][0m |           0.0175 |           2.5086 |           0.1591 |
[32m[20230207 15:28:15 @agent_ppo2.py:192][0m |           0.0112 |           3.2464 |           0.1590 |
[32m[20230207 15:28:15 @agent_ppo2.py:192][0m |           0.0012 |           3.3321 |           0.1588 |
[32m[20230207 15:28:15 @agent_ppo2.py:192][0m |          -0.0061 |           2.1409 |           0.1587 |
[32m[20230207 15:28:15 @agent_ppo2.py:192][0m |           0.0025 |           2.0711 |           0.1588 |
[32m[20230207 15:28:15 @agent_ppo2.py:192][0m |          -0.0098 |           2.0070 |           0.1588 |
[32m[20230207 15:28:15 @agent_ppo2.py:192][0m |          -0.0087 |           1.9548 |           0.1588 |
[32m[20230207 15:28:15 @agent_ppo2.py:192][0m |          -0.0088 |           1.9218 |           0.1587 |
[32m[20230207 15:28:15 @agent_ppo2.py:137][0m Policy update time: 0.79 s
[32m[20230207 15:28:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -9.47
[32m[20230207 15:28:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 36.96
[32m[20230207 15:28:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 49.41
[32m[20230207 15:28:16 @agent_ppo2.py:150][0m Total time:      27.58 min
[32m[20230207 15:28:16 @agent_ppo2.py:152][0m 1449984 total steps have happened
[32m[20230207 15:28:16 @agent_ppo2.py:128][0m #------------------------ Iteration 708 --------------------------#
[32m[20230207 15:28:17 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:28:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:17 @agent_ppo2.py:192][0m |           0.0014 |           4.3496 |           0.1586 |
[32m[20230207 15:28:17 @agent_ppo2.py:192][0m |          -0.0045 |           1.3369 |           0.1586 |
[32m[20230207 15:28:17 @agent_ppo2.py:192][0m |          -0.0017 |           1.1761 |           0.1585 |
[32m[20230207 15:28:17 @agent_ppo2.py:192][0m |          -0.0078 |           1.1169 |           0.1585 |
[32m[20230207 15:28:17 @agent_ppo2.py:192][0m |          -0.0092 |           1.0737 |           0.1585 |
[32m[20230207 15:28:17 @agent_ppo2.py:192][0m |          -0.0103 |           1.0493 |           0.1584 |
[32m[20230207 15:28:18 @agent_ppo2.py:192][0m |          -0.0122 |           1.0213 |           0.1583 |
[32m[20230207 15:28:18 @agent_ppo2.py:192][0m |          -0.0113 |           1.0063 |           0.1581 |
[32m[20230207 15:28:18 @agent_ppo2.py:192][0m |          -0.0113 |           0.9845 |           0.1581 |
[32m[20230207 15:28:18 @agent_ppo2.py:192][0m |          -0.0146 |           0.9711 |           0.1581 |
[32m[20230207 15:28:18 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:28:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: 35.65
[32m[20230207 15:28:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 112.52
[32m[20230207 15:28:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 21.78
[32m[20230207 15:28:19 @agent_ppo2.py:150][0m Total time:      27.62 min
[32m[20230207 15:28:19 @agent_ppo2.py:152][0m 1452032 total steps have happened
[32m[20230207 15:28:19 @agent_ppo2.py:128][0m #------------------------ Iteration 709 --------------------------#
[32m[20230207 15:28:19 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:28:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:19 @agent_ppo2.py:192][0m |           0.0019 |          26.9563 |           0.1602 |
[32m[20230207 15:28:19 @agent_ppo2.py:192][0m |          -0.0054 |           9.1282 |           0.1600 |
[32m[20230207 15:28:19 @agent_ppo2.py:192][0m |          -0.0071 |           6.0111 |           0.1597 |
[32m[20230207 15:28:19 @agent_ppo2.py:192][0m |          -0.0098 |           4.9333 |           0.1595 |
[32m[20230207 15:28:19 @agent_ppo2.py:192][0m |          -0.0101 |           4.2837 |           0.1593 |
[32m[20230207 15:28:19 @agent_ppo2.py:192][0m |          -0.0126 |           3.7848 |           0.1591 |
[32m[20230207 15:28:20 @agent_ppo2.py:192][0m |          -0.0120 |           3.4878 |           0.1589 |
[32m[20230207 15:28:20 @agent_ppo2.py:192][0m |          -0.0143 |           3.2190 |           0.1589 |
[32m[20230207 15:28:20 @agent_ppo2.py:192][0m |          -0.0146 |           3.0647 |           0.1588 |
[32m[20230207 15:28:20 @agent_ppo2.py:192][0m |          -0.0141 |           2.9560 |           0.1587 |
[32m[20230207 15:28:20 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:28:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -0.72
[32m[20230207 15:28:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 79.28
[32m[20230207 15:28:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 41.33
[32m[20230207 15:28:20 @agent_ppo2.py:150][0m Total time:      27.65 min
[32m[20230207 15:28:20 @agent_ppo2.py:152][0m 1454080 total steps have happened
[32m[20230207 15:28:20 @agent_ppo2.py:128][0m #------------------------ Iteration 710 --------------------------#
[32m[20230207 15:28:21 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:28:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:21 @agent_ppo2.py:192][0m |           0.0008 |           1.1885 |           0.1557 |
[32m[20230207 15:28:21 @agent_ppo2.py:192][0m |          -0.0066 |           1.0071 |           0.1554 |
[32m[20230207 15:28:22 @agent_ppo2.py:192][0m |          -0.0158 |           0.9646 |           0.1555 |
[32m[20230207 15:28:22 @agent_ppo2.py:192][0m |          -0.0095 |           0.9371 |           0.1553 |
[32m[20230207 15:28:22 @agent_ppo2.py:192][0m |          -0.0136 |           0.9072 |           0.1551 |
[32m[20230207 15:28:22 @agent_ppo2.py:192][0m |           0.0222 |           0.9775 |           0.1553 |
[32m[20230207 15:28:22 @agent_ppo2.py:192][0m |           0.0044 |           0.8902 |           0.1549 |
[32m[20230207 15:28:22 @agent_ppo2.py:192][0m |          -0.0147 |           0.8690 |           0.1552 |
[32m[20230207 15:28:22 @agent_ppo2.py:192][0m |          -0.0094 |           0.8710 |           0.1551 |
[32m[20230207 15:28:22 @agent_ppo2.py:192][0m |          -0.0211 |           0.8571 |           0.1551 |
[32m[20230207 15:28:22 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:28:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: 33.76
[32m[20230207 15:28:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 70.53
[32m[20230207 15:28:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -64.67
[32m[20230207 15:28:23 @agent_ppo2.py:150][0m Total time:      27.69 min
[32m[20230207 15:28:23 @agent_ppo2.py:152][0m 1456128 total steps have happened
[32m[20230207 15:28:23 @agent_ppo2.py:128][0m #------------------------ Iteration 711 --------------------------#
[32m[20230207 15:28:24 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:28:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:24 @agent_ppo2.py:192][0m |          -0.0030 |           5.7481 |           0.1565 |
[32m[20230207 15:28:24 @agent_ppo2.py:192][0m |          -0.0055 |           2.6669 |           0.1562 |
[32m[20230207 15:28:24 @agent_ppo2.py:192][0m |          -0.0064 |           2.2638 |           0.1562 |
[32m[20230207 15:28:24 @agent_ppo2.py:192][0m |          -0.0089 |           2.0899 |           0.1560 |
[32m[20230207 15:28:24 @agent_ppo2.py:192][0m |          -0.0105 |           1.9679 |           0.1559 |
[32m[20230207 15:28:24 @agent_ppo2.py:192][0m |          -0.0089 |           1.8964 |           0.1558 |
[32m[20230207 15:28:24 @agent_ppo2.py:192][0m |          -0.0119 |           1.8506 |           0.1555 |
[32m[20230207 15:28:24 @agent_ppo2.py:192][0m |          -0.0139 |           1.8037 |           0.1555 |
[32m[20230207 15:28:24 @agent_ppo2.py:192][0m |          -0.0099 |           1.7723 |           0.1555 |
[32m[20230207 15:28:24 @agent_ppo2.py:192][0m |          -0.0138 |           1.7409 |           0.1554 |
[32m[20230207 15:28:24 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:28:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -16.88
[32m[20230207 15:28:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 34.43
[32m[20230207 15:28:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 28.46
[32m[20230207 15:28:25 @agent_ppo2.py:150][0m Total time:      27.73 min
[32m[20230207 15:28:25 @agent_ppo2.py:152][0m 1458176 total steps have happened
[32m[20230207 15:28:25 @agent_ppo2.py:128][0m #------------------------ Iteration 712 --------------------------#
[32m[20230207 15:28:26 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:28:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:26 @agent_ppo2.py:192][0m |          -0.0016 |           6.1160 |           0.1592 |
[32m[20230207 15:28:26 @agent_ppo2.py:192][0m |          -0.0067 |           2.3855 |           0.1591 |
[32m[20230207 15:28:26 @agent_ppo2.py:192][0m |          -0.0076 |           2.0509 |           0.1591 |
[32m[20230207 15:28:26 @agent_ppo2.py:192][0m |          -0.0069 |           1.9183 |           0.1589 |
[32m[20230207 15:28:27 @agent_ppo2.py:192][0m |          -0.0079 |           1.8641 |           0.1588 |
[32m[20230207 15:28:27 @agent_ppo2.py:192][0m |          -0.0061 |           1.7684 |           0.1587 |
[32m[20230207 15:28:27 @agent_ppo2.py:192][0m |          -0.0141 |           1.7187 |           0.1588 |
[32m[20230207 15:28:27 @agent_ppo2.py:192][0m |          -0.0120 |           1.6950 |           0.1587 |
[32m[20230207 15:28:27 @agent_ppo2.py:192][0m |          -0.0113 |           1.6118 |           0.1587 |
[32m[20230207 15:28:27 @agent_ppo2.py:192][0m |          -0.0115 |           1.5634 |           0.1586 |
[32m[20230207 15:28:27 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:28:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: 15.98
[32m[20230207 15:28:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 72.46
[32m[20230207 15:28:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 9.72
[32m[20230207 15:28:28 @agent_ppo2.py:150][0m Total time:      27.78 min
[32m[20230207 15:28:28 @agent_ppo2.py:152][0m 1460224 total steps have happened
[32m[20230207 15:28:28 @agent_ppo2.py:128][0m #------------------------ Iteration 713 --------------------------#
[32m[20230207 15:28:29 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:28:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:29 @agent_ppo2.py:192][0m |           0.0037 |           2.7385 |           0.1585 |
[32m[20230207 15:28:29 @agent_ppo2.py:192][0m |           0.0066 |           1.8694 |           0.1585 |
[32m[20230207 15:28:29 @agent_ppo2.py:192][0m |           0.0187 |           1.7445 |           0.1583 |
[32m[20230207 15:28:29 @agent_ppo2.py:192][0m |          -0.0008 |           1.6829 |           0.1583 |
[32m[20230207 15:28:29 @agent_ppo2.py:192][0m |          -0.0062 |           1.5711 |           0.1583 |
[32m[20230207 15:28:29 @agent_ppo2.py:192][0m |          -0.0062 |           1.5240 |           0.1582 |
[32m[20230207 15:28:29 @agent_ppo2.py:192][0m |          -0.0035 |           1.4952 |           0.1583 |
[32m[20230207 15:28:29 @agent_ppo2.py:192][0m |          -0.0117 |           1.4427 |           0.1583 |
[32m[20230207 15:28:29 @agent_ppo2.py:192][0m |          -0.0022 |           1.4221 |           0.1583 |
[32m[20230207 15:28:29 @agent_ppo2.py:192][0m |           0.0069 |           1.3908 |           0.1584 |
[32m[20230207 15:28:29 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:28:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: 54.66
[32m[20230207 15:28:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 81.70
[32m[20230207 15:28:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 26.51
[32m[20230207 15:28:30 @agent_ppo2.py:150][0m Total time:      27.82 min
[32m[20230207 15:28:30 @agent_ppo2.py:152][0m 1462272 total steps have happened
[32m[20230207 15:28:30 @agent_ppo2.py:128][0m #------------------------ Iteration 714 --------------------------#
[32m[20230207 15:28:31 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:28:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:31 @agent_ppo2.py:192][0m |          -0.0012 |          10.5145 |           0.1568 |
[32m[20230207 15:28:31 @agent_ppo2.py:192][0m |          -0.0024 |           4.3863 |           0.1568 |
[32m[20230207 15:28:31 @agent_ppo2.py:192][0m |          -0.0149 |           2.8215 |           0.1568 |
[32m[20230207 15:28:31 @agent_ppo2.py:192][0m |          -0.0084 |           2.4460 |           0.1567 |
[32m[20230207 15:28:31 @agent_ppo2.py:192][0m |          -0.0152 |           2.1173 |           0.1567 |
[32m[20230207 15:28:32 @agent_ppo2.py:192][0m |          -0.0075 |           1.8253 |           0.1566 |
[32m[20230207 15:28:32 @agent_ppo2.py:192][0m |          -0.0121 |           1.7683 |           0.1566 |
[32m[20230207 15:28:32 @agent_ppo2.py:192][0m |          -0.0077 |           1.5934 |           0.1566 |
[32m[20230207 15:28:32 @agent_ppo2.py:192][0m |          -0.0146 |           1.5147 |           0.1566 |
[32m[20230207 15:28:32 @agent_ppo2.py:192][0m |          -0.0113 |           1.4720 |           0.1565 |
[32m[20230207 15:28:32 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:28:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: 28.02
[32m[20230207 15:28:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 109.34
[32m[20230207 15:28:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -11.53
[32m[20230207 15:28:33 @agent_ppo2.py:150][0m Total time:      27.86 min
[32m[20230207 15:28:33 @agent_ppo2.py:152][0m 1464320 total steps have happened
[32m[20230207 15:28:33 @agent_ppo2.py:128][0m #------------------------ Iteration 715 --------------------------#
[32m[20230207 15:28:33 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 15:28:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:34 @agent_ppo2.py:192][0m |           0.0000 |          28.5440 |           0.1540 |
[32m[20230207 15:28:34 @agent_ppo2.py:192][0m |          -0.0045 |           9.8347 |           0.1538 |
[32m[20230207 15:28:34 @agent_ppo2.py:192][0m |          -0.0066 |           6.2189 |           0.1538 |
[32m[20230207 15:28:34 @agent_ppo2.py:192][0m |          -0.0076 |           4.5590 |           0.1539 |
[32m[20230207 15:28:34 @agent_ppo2.py:192][0m |          -0.0110 |           3.8269 |           0.1538 |
[32m[20230207 15:28:34 @agent_ppo2.py:192][0m |          -0.0106 |           3.3542 |           0.1537 |
[32m[20230207 15:28:34 @agent_ppo2.py:192][0m |          -0.0136 |           3.0517 |           0.1535 |
[32m[20230207 15:28:34 @agent_ppo2.py:192][0m |          -0.0137 |           2.8241 |           0.1535 |
[32m[20230207 15:28:34 @agent_ppo2.py:192][0m |          -0.0159 |           2.6437 |           0.1536 |
[32m[20230207 15:28:34 @agent_ppo2.py:192][0m |          -0.0156 |           2.4627 |           0.1535 |
[32m[20230207 15:28:34 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230207 15:28:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: -58.53
[32m[20230207 15:28:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 52.01
[32m[20230207 15:28:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 4.12
[32m[20230207 15:28:35 @agent_ppo2.py:150][0m Total time:      27.90 min
[32m[20230207 15:28:35 @agent_ppo2.py:152][0m 1466368 total steps have happened
[32m[20230207 15:28:35 @agent_ppo2.py:128][0m #------------------------ Iteration 716 --------------------------#
[32m[20230207 15:28:36 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:28:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:36 @agent_ppo2.py:192][0m |          -0.0019 |          30.2713 |           0.1610 |
[32m[20230207 15:28:36 @agent_ppo2.py:192][0m |          -0.0070 |          12.4799 |           0.1608 |
[32m[20230207 15:28:36 @agent_ppo2.py:192][0m |          -0.0101 |           9.8295 |           0.1608 |
[32m[20230207 15:28:36 @agent_ppo2.py:192][0m |          -0.0117 |           8.7131 |           0.1608 |
[32m[20230207 15:28:36 @agent_ppo2.py:192][0m |          -0.0116 |           7.7172 |           0.1607 |
[32m[20230207 15:28:36 @agent_ppo2.py:192][0m |          -0.0127 |           7.1030 |           0.1606 |
[32m[20230207 15:28:36 @agent_ppo2.py:192][0m |          -0.0134 |           6.6232 |           0.1606 |
[32m[20230207 15:28:37 @agent_ppo2.py:192][0m |          -0.0152 |           6.2345 |           0.1605 |
[32m[20230207 15:28:37 @agent_ppo2.py:192][0m |          -0.0142 |           5.7273 |           0.1607 |
[32m[20230207 15:28:37 @agent_ppo2.py:192][0m |          -0.0157 |           5.3895 |           0.1607 |
[32m[20230207 15:28:37 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:28:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: -63.99
[32m[20230207 15:28:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -5.01
[32m[20230207 15:28:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -9.02
[32m[20230207 15:28:38 @agent_ppo2.py:150][0m Total time:      27.94 min
[32m[20230207 15:28:38 @agent_ppo2.py:152][0m 1468416 total steps have happened
[32m[20230207 15:28:38 @agent_ppo2.py:128][0m #------------------------ Iteration 717 --------------------------#
[32m[20230207 15:28:38 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:28:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:38 @agent_ppo2.py:192][0m |          -0.0007 |           3.4569 |           0.1570 |
[32m[20230207 15:28:39 @agent_ppo2.py:192][0m |           0.0094 |           2.1600 |           0.1569 |
[32m[20230207 15:28:39 @agent_ppo2.py:192][0m |           0.0090 |           1.9461 |           0.1570 |
[32m[20230207 15:28:39 @agent_ppo2.py:192][0m |          -0.0145 |           1.8365 |           0.1568 |
[32m[20230207 15:28:39 @agent_ppo2.py:192][0m |          -0.0633 |           1.9524 |           0.1570 |
[32m[20230207 15:28:39 @agent_ppo2.py:192][0m |           0.0003 |           1.6847 |           0.1568 |
[32m[20230207 15:28:39 @agent_ppo2.py:192][0m |           0.0043 |           1.6037 |           0.1571 |
[32m[20230207 15:28:39 @agent_ppo2.py:192][0m |          -0.0069 |           1.5748 |           0.1571 |
[32m[20230207 15:28:39 @agent_ppo2.py:192][0m |          -0.0000 |           1.5124 |           0.1572 |
[32m[20230207 15:28:39 @agent_ppo2.py:192][0m |          -0.0095 |           1.4705 |           0.1572 |
[32m[20230207 15:28:39 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:28:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: 78.85
[32m[20230207 15:28:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 104.26
[32m[20230207 15:28:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -30.46
[32m[20230207 15:28:40 @agent_ppo2.py:150][0m Total time:      27.98 min
[32m[20230207 15:28:40 @agent_ppo2.py:152][0m 1470464 total steps have happened
[32m[20230207 15:28:40 @agent_ppo2.py:128][0m #------------------------ Iteration 718 --------------------------#
[32m[20230207 15:28:41 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:28:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:41 @agent_ppo2.py:192][0m |           0.0014 |           9.7008 |           0.1563 |
[32m[20230207 15:28:41 @agent_ppo2.py:192][0m |          -0.0020 |           4.9701 |           0.1563 |
[32m[20230207 15:28:41 @agent_ppo2.py:192][0m |          -0.0047 |           4.0800 |           0.1563 |
[32m[20230207 15:28:41 @agent_ppo2.py:192][0m |          -0.0071 |           3.7672 |           0.1560 |
[32m[20230207 15:28:41 @agent_ppo2.py:192][0m |          -0.0085 |           3.5202 |           0.1561 |
[32m[20230207 15:28:41 @agent_ppo2.py:192][0m |          -0.0097 |           3.3938 |           0.1560 |
[32m[20230207 15:28:42 @agent_ppo2.py:192][0m |          -0.0110 |           3.2346 |           0.1558 |
[32m[20230207 15:28:42 @agent_ppo2.py:192][0m |          -0.0113 |           3.1063 |           0.1557 |
[32m[20230207 15:28:42 @agent_ppo2.py:192][0m |          -0.0123 |           3.0088 |           0.1557 |
[32m[20230207 15:28:42 @agent_ppo2.py:192][0m |          -0.0127 |           2.9114 |           0.1554 |
[32m[20230207 15:28:42 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:28:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -22.19
[32m[20230207 15:28:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 80.46
[32m[20230207 15:28:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 143.04
[32m[20230207 15:28:43 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 143.04
[32m[20230207 15:28:43 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 143.04
[32m[20230207 15:28:43 @agent_ppo2.py:150][0m Total time:      28.02 min
[32m[20230207 15:28:43 @agent_ppo2.py:152][0m 1472512 total steps have happened
[32m[20230207 15:28:43 @agent_ppo2.py:128][0m #------------------------ Iteration 719 --------------------------#
[32m[20230207 15:28:43 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:28:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:43 @agent_ppo2.py:192][0m |          -0.0149 |           1.9713 |           0.1566 |
[32m[20230207 15:28:44 @agent_ppo2.py:192][0m |           0.0136 |           1.5083 |           0.1565 |
[32m[20230207 15:28:44 @agent_ppo2.py:192][0m |          -0.0126 |           1.4311 |           0.1565 |
[32m[20230207 15:28:44 @agent_ppo2.py:192][0m |          -0.0067 |           1.3951 |           0.1565 |
[32m[20230207 15:28:44 @agent_ppo2.py:192][0m |          -0.0067 |           1.3570 |           0.1562 |
[32m[20230207 15:28:44 @agent_ppo2.py:192][0m |          -0.0080 |           1.3353 |           0.1562 |
[32m[20230207 15:28:44 @agent_ppo2.py:192][0m |           0.0047 |           1.3221 |           0.1563 |
[32m[20230207 15:28:44 @agent_ppo2.py:192][0m |          -0.0112 |           1.2959 |           0.1562 |
[32m[20230207 15:28:44 @agent_ppo2.py:192][0m |          -0.0174 |           1.2792 |           0.1562 |
[32m[20230207 15:28:44 @agent_ppo2.py:192][0m |          -0.0121 |           1.2697 |           0.1561 |
[32m[20230207 15:28:44 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:28:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: 75.41
[32m[20230207 15:28:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 75.82
[32m[20230207 15:28:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -21.49
[32m[20230207 15:28:45 @agent_ppo2.py:150][0m Total time:      28.06 min
[32m[20230207 15:28:45 @agent_ppo2.py:152][0m 1474560 total steps have happened
[32m[20230207 15:28:45 @agent_ppo2.py:128][0m #------------------------ Iteration 720 --------------------------#
[32m[20230207 15:28:46 @agent_ppo2.py:134][0m Sampling time: 0.73 s by 1 slaves
[32m[20230207 15:28:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:46 @agent_ppo2.py:192][0m |          -0.0024 |          34.9560 |           0.1546 |
[32m[20230207 15:28:46 @agent_ppo2.py:192][0m |          -0.0038 |          17.0045 |           0.1545 |
[32m[20230207 15:28:46 @agent_ppo2.py:192][0m |          -0.0039 |          13.5145 |           0.1543 |
[32m[20230207 15:28:46 @agent_ppo2.py:192][0m |          -0.0081 |          11.6711 |           0.1543 |
[32m[20230207 15:28:46 @agent_ppo2.py:192][0m |          -0.0093 |           9.8796 |           0.1542 |
[32m[20230207 15:28:46 @agent_ppo2.py:192][0m |          -0.0101 |           8.6720 |           0.1541 |
[32m[20230207 15:28:46 @agent_ppo2.py:192][0m |          -0.0107 |           7.8143 |           0.1541 |
[32m[20230207 15:28:46 @agent_ppo2.py:192][0m |          -0.0122 |           7.4298 |           0.1539 |
[32m[20230207 15:28:46 @agent_ppo2.py:192][0m |          -0.0103 |           6.9789 |           0.1540 |
[32m[20230207 15:28:46 @agent_ppo2.py:192][0m |          -0.0113 |           6.4058 |           0.1538 |
[32m[20230207 15:28:46 @agent_ppo2.py:137][0m Policy update time: 0.75 s
[32m[20230207 15:28:47 @agent_ppo2.py:145][0m Average TRAINING episode reward: -42.57
[32m[20230207 15:28:47 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 64.94
[32m[20230207 15:28:47 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -48.57
[32m[20230207 15:28:47 @agent_ppo2.py:150][0m Total time:      28.10 min
[32m[20230207 15:28:47 @agent_ppo2.py:152][0m 1476608 total steps have happened
[32m[20230207 15:28:47 @agent_ppo2.py:128][0m #------------------------ Iteration 721 --------------------------#
[32m[20230207 15:28:48 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:28:48 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:48 @agent_ppo2.py:192][0m |          -0.0042 |           8.1977 |           0.1565 |
[32m[20230207 15:28:48 @agent_ppo2.py:192][0m |          -0.0102 |           2.6464 |           0.1564 |
[32m[20230207 15:28:48 @agent_ppo2.py:192][0m |          -0.0141 |           2.1665 |           0.1563 |
[32m[20230207 15:28:48 @agent_ppo2.py:192][0m |          -0.0116 |           1.9918 |           0.1562 |
[32m[20230207 15:28:48 @agent_ppo2.py:192][0m |          -0.0142 |           1.8524 |           0.1560 |
[32m[20230207 15:28:48 @agent_ppo2.py:192][0m |          -0.0182 |           1.7449 |           0.1560 |
[32m[20230207 15:28:49 @agent_ppo2.py:192][0m |          -0.0204 |           1.6612 |           0.1563 |
[32m[20230207 15:28:49 @agent_ppo2.py:192][0m |          -0.0152 |           1.5893 |           0.1560 |
[32m[20230207 15:28:49 @agent_ppo2.py:192][0m |          -0.0199 |           1.5548 |           0.1561 |
[32m[20230207 15:28:49 @agent_ppo2.py:192][0m |           0.1498 |           4.2873 |           0.1560 |
[32m[20230207 15:28:49 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:28:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: -15.62
[32m[20230207 15:28:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 115.56
[32m[20230207 15:28:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 21.39
[32m[20230207 15:28:50 @agent_ppo2.py:150][0m Total time:      28.14 min
[32m[20230207 15:28:50 @agent_ppo2.py:152][0m 1478656 total steps have happened
[32m[20230207 15:28:50 @agent_ppo2.py:128][0m #------------------------ Iteration 722 --------------------------#
[32m[20230207 15:28:50 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230207 15:28:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:50 @agent_ppo2.py:192][0m |           0.0028 |          35.0475 |           0.1600 |
[32m[20230207 15:28:50 @agent_ppo2.py:192][0m |          -0.0055 |          14.0214 |           0.1602 |
[32m[20230207 15:28:50 @agent_ppo2.py:192][0m |          -0.0087 |          10.6265 |           0.1600 |
[32m[20230207 15:28:50 @agent_ppo2.py:192][0m |          -0.0106 |           9.4153 |           0.1599 |
[32m[20230207 15:28:51 @agent_ppo2.py:192][0m |          -0.0125 |           8.6308 |           0.1598 |
[32m[20230207 15:28:51 @agent_ppo2.py:192][0m |          -0.0133 |           8.0940 |           0.1598 |
[32m[20230207 15:28:51 @agent_ppo2.py:192][0m |          -0.0143 |           7.5931 |           0.1598 |
[32m[20230207 15:28:51 @agent_ppo2.py:192][0m |          -0.0166 |           7.3750 |           0.1597 |
[32m[20230207 15:28:51 @agent_ppo2.py:192][0m |          -0.0170 |           6.9394 |           0.1597 |
[32m[20230207 15:28:51 @agent_ppo2.py:192][0m |          -0.0179 |           6.7210 |           0.1595 |
[32m[20230207 15:28:51 @agent_ppo2.py:137][0m Policy update time: 0.64 s
[32m[20230207 15:28:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: -13.25
[32m[20230207 15:28:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 114.53
[32m[20230207 15:28:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 14.26
[32m[20230207 15:28:52 @agent_ppo2.py:150][0m Total time:      28.17 min
[32m[20230207 15:28:52 @agent_ppo2.py:152][0m 1480704 total steps have happened
[32m[20230207 15:28:52 @agent_ppo2.py:128][0m #------------------------ Iteration 723 --------------------------#
[32m[20230207 15:28:52 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:28:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:53 @agent_ppo2.py:192][0m |          -0.0276 |           3.1044 |           0.1546 |
[32m[20230207 15:28:53 @agent_ppo2.py:192][0m |          -0.0123 |           2.1951 |           0.1542 |
[32m[20230207 15:28:53 @agent_ppo2.py:192][0m |           0.0044 |           2.0345 |           0.1543 |
[32m[20230207 15:28:53 @agent_ppo2.py:192][0m |          -0.0262 |           1.9608 |           0.1543 |
[32m[20230207 15:28:53 @agent_ppo2.py:192][0m |          -0.0173 |           1.8710 |           0.1545 |
[32m[20230207 15:28:53 @agent_ppo2.py:192][0m |           0.0106 |           1.8446 |           0.1542 |
[32m[20230207 15:28:53 @agent_ppo2.py:192][0m |           0.0011 |           1.7914 |           0.1544 |
[32m[20230207 15:28:53 @agent_ppo2.py:192][0m |          -0.0210 |           1.7896 |           0.1545 |
[32m[20230207 15:28:53 @agent_ppo2.py:192][0m |           0.0048 |           1.7461 |           0.1545 |
[32m[20230207 15:28:53 @agent_ppo2.py:192][0m |          -0.0465 |           1.9081 |           0.1546 |
[32m[20230207 15:28:53 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:28:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: 93.90
[32m[20230207 15:28:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 116.09
[32m[20230207 15:28:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 28.14
[32m[20230207 15:28:54 @agent_ppo2.py:150][0m Total time:      28.21 min
[32m[20230207 15:28:54 @agent_ppo2.py:152][0m 1482752 total steps have happened
[32m[20230207 15:28:54 @agent_ppo2.py:128][0m #------------------------ Iteration 724 --------------------------#
[32m[20230207 15:28:55 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:28:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:55 @agent_ppo2.py:192][0m |          -0.0131 |           1.8681 |           0.1552 |
[32m[20230207 15:28:55 @agent_ppo2.py:192][0m |          -0.0016 |           1.7275 |           0.1549 |
[32m[20230207 15:28:55 @agent_ppo2.py:192][0m |          -0.0047 |           1.6900 |           0.1547 |
[32m[20230207 15:28:55 @agent_ppo2.py:192][0m |          -0.0103 |           1.6732 |           0.1548 |
[32m[20230207 15:28:55 @agent_ppo2.py:192][0m |          -0.0187 |           1.6486 |           0.1547 |
[32m[20230207 15:28:55 @agent_ppo2.py:192][0m |          -0.0294 |           1.6458 |           0.1547 |
[32m[20230207 15:28:55 @agent_ppo2.py:192][0m |          -0.0175 |           1.6191 |           0.1547 |
[32m[20230207 15:28:55 @agent_ppo2.py:192][0m |          -0.0173 |           1.6062 |           0.1546 |
[32m[20230207 15:28:55 @agent_ppo2.py:192][0m |          -0.0070 |           1.6186 |           0.1545 |
[32m[20230207 15:28:55 @agent_ppo2.py:192][0m |          -0.0185 |           1.5685 |           0.1544 |
[32m[20230207 15:28:55 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:28:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: 68.76
[32m[20230207 15:28:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 89.86
[32m[20230207 15:28:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -33.31
[32m[20230207 15:28:56 @agent_ppo2.py:150][0m Total time:      28.25 min
[32m[20230207 15:28:56 @agent_ppo2.py:152][0m 1484800 total steps have happened
[32m[20230207 15:28:56 @agent_ppo2.py:128][0m #------------------------ Iteration 725 --------------------------#
[32m[20230207 15:28:57 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:28:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:28:57 @agent_ppo2.py:192][0m |          -0.0097 |           1.7463 |           0.1540 |
[32m[20230207 15:28:57 @agent_ppo2.py:192][0m |           0.0021 |           1.2734 |           0.1537 |
[32m[20230207 15:28:57 @agent_ppo2.py:192][0m |          -0.0144 |           1.2367 |           0.1534 |
[32m[20230207 15:28:57 @agent_ppo2.py:192][0m |          -0.0284 |           1.2074 |           0.1533 |
[32m[20230207 15:28:57 @agent_ppo2.py:192][0m |           0.0111 |           1.1656 |           0.1530 |
[32m[20230207 15:28:57 @agent_ppo2.py:192][0m |          -0.0120 |           1.1459 |           0.1529 |
[32m[20230207 15:28:57 @agent_ppo2.py:192][0m |          -0.0163 |           1.1295 |           0.1529 |
[32m[20230207 15:28:58 @agent_ppo2.py:192][0m |          -0.0204 |           1.1165 |           0.1528 |
[32m[20230207 15:28:58 @agent_ppo2.py:192][0m |          -0.0215 |           1.1160 |           0.1527 |
[32m[20230207 15:28:58 @agent_ppo2.py:192][0m |          -0.0156 |           1.0965 |           0.1527 |
[32m[20230207 15:28:58 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:28:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: 73.24
[32m[20230207 15:28:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 128.43
[32m[20230207 15:28:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -30.42
[32m[20230207 15:28:59 @agent_ppo2.py:150][0m Total time:      28.29 min
[32m[20230207 15:28:59 @agent_ppo2.py:152][0m 1486848 total steps have happened
[32m[20230207 15:28:59 @agent_ppo2.py:128][0m #------------------------ Iteration 726 --------------------------#
[32m[20230207 15:28:59 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:28:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:00 @agent_ppo2.py:192][0m |          -0.0007 |          12.0786 |           0.1576 |
[32m[20230207 15:29:00 @agent_ppo2.py:192][0m |          -0.0036 |           3.4682 |           0.1574 |
[32m[20230207 15:29:00 @agent_ppo2.py:192][0m |          -0.0082 |           2.6534 |           0.1574 |
[32m[20230207 15:29:00 @agent_ppo2.py:192][0m |          -0.0058 |           2.2319 |           0.1574 |
[32m[20230207 15:29:00 @agent_ppo2.py:192][0m |          -0.0106 |           2.0141 |           0.1574 |
[32m[20230207 15:29:00 @agent_ppo2.py:192][0m |          -0.0100 |           1.9075 |           0.1574 |
[32m[20230207 15:29:00 @agent_ppo2.py:192][0m |          -0.0107 |           1.7845 |           0.1573 |
[32m[20230207 15:29:00 @agent_ppo2.py:192][0m |          -0.0128 |           1.7425 |           0.1573 |
[32m[20230207 15:29:00 @agent_ppo2.py:192][0m |          -0.0112 |           1.6649 |           0.1572 |
[32m[20230207 15:29:00 @agent_ppo2.py:192][0m |          -0.0120 |           1.6139 |           0.1572 |
[32m[20230207 15:29:00 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:29:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -47.11
[32m[20230207 15:29:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 57.60
[32m[20230207 15:29:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -45.43
[32m[20230207 15:29:01 @agent_ppo2.py:150][0m Total time:      28.33 min
[32m[20230207 15:29:01 @agent_ppo2.py:152][0m 1488896 total steps have happened
[32m[20230207 15:29:01 @agent_ppo2.py:128][0m #------------------------ Iteration 727 --------------------------#
[32m[20230207 15:29:02 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:29:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:02 @agent_ppo2.py:192][0m |          -0.0020 |          12.2876 |           0.1553 |
[32m[20230207 15:29:02 @agent_ppo2.py:192][0m |          -0.0077 |           6.7277 |           0.1552 |
[32m[20230207 15:29:02 @agent_ppo2.py:192][0m |          -0.0106 |           5.3779 |           0.1551 |
[32m[20230207 15:29:02 @agent_ppo2.py:192][0m |          -0.0108 |           5.0771 |           0.1550 |
[32m[20230207 15:29:02 @agent_ppo2.py:192][0m |          -0.0143 |           4.4528 |           0.1550 |
[32m[20230207 15:29:02 @agent_ppo2.py:192][0m |          -0.0148 |           4.2814 |           0.1550 |
[32m[20230207 15:29:02 @agent_ppo2.py:192][0m |          -0.0153 |           4.1079 |           0.1550 |
[32m[20230207 15:29:02 @agent_ppo2.py:192][0m |          -0.0158 |           3.8322 |           0.1549 |
[32m[20230207 15:29:03 @agent_ppo2.py:192][0m |          -0.0162 |           3.4777 |           0.1548 |
[32m[20230207 15:29:03 @agent_ppo2.py:192][0m |          -0.0178 |           3.4897 |           0.1548 |
[32m[20230207 15:29:03 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:29:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -23.98
[32m[20230207 15:29:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 86.26
[32m[20230207 15:29:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 113.08
[32m[20230207 15:29:03 @agent_ppo2.py:150][0m Total time:      28.37 min
[32m[20230207 15:29:03 @agent_ppo2.py:152][0m 1490944 total steps have happened
[32m[20230207 15:29:03 @agent_ppo2.py:128][0m #------------------------ Iteration 728 --------------------------#
[32m[20230207 15:29:04 @agent_ppo2.py:134][0m Sampling time: 0.54 s by 1 slaves
[32m[20230207 15:29:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:04 @agent_ppo2.py:192][0m |          -0.0007 |          13.0613 |           0.1590 |
[32m[20230207 15:29:04 @agent_ppo2.py:192][0m |          -0.0052 |           5.4167 |           0.1589 |
[32m[20230207 15:29:04 @agent_ppo2.py:192][0m |          -0.0065 |           3.1869 |           0.1589 |
[32m[20230207 15:29:04 @agent_ppo2.py:192][0m |          -0.0084 |           2.7659 |           0.1589 |
[32m[20230207 15:29:04 @agent_ppo2.py:192][0m |          -0.0096 |           2.4479 |           0.1588 |
[32m[20230207 15:29:04 @agent_ppo2.py:192][0m |          -0.0109 |           2.3221 |           0.1586 |
[32m[20230207 15:29:04 @agent_ppo2.py:192][0m |          -0.0127 |           2.1687 |           0.1586 |
[32m[20230207 15:29:04 @agent_ppo2.py:192][0m |          -0.0112 |           2.1009 |           0.1585 |
[32m[20230207 15:29:04 @agent_ppo2.py:192][0m |          -0.0121 |           1.9900 |           0.1585 |
[32m[20230207 15:29:04 @agent_ppo2.py:192][0m |          -0.0129 |           1.9835 |           0.1584 |
[32m[20230207 15:29:04 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:29:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -1.89
[32m[20230207 15:29:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 48.16
[32m[20230207 15:29:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 21.82
[32m[20230207 15:29:05 @agent_ppo2.py:150][0m Total time:      28.40 min
[32m[20230207 15:29:05 @agent_ppo2.py:152][0m 1492992 total steps have happened
[32m[20230207 15:29:05 @agent_ppo2.py:128][0m #------------------------ Iteration 729 --------------------------#
[32m[20230207 15:29:06 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:29:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:06 @agent_ppo2.py:192][0m |          -0.0008 |           9.4942 |           0.1548 |
[32m[20230207 15:29:06 @agent_ppo2.py:192][0m |          -0.0036 |           4.5197 |           0.1545 |
[32m[20230207 15:29:06 @agent_ppo2.py:192][0m |          -0.0080 |           3.9563 |           0.1545 |
[32m[20230207 15:29:06 @agent_ppo2.py:192][0m |          -0.0088 |           3.5946 |           0.1542 |
[32m[20230207 15:29:07 @agent_ppo2.py:192][0m |          -0.0046 |           3.4666 |           0.1545 |
[32m[20230207 15:29:07 @agent_ppo2.py:192][0m |          -0.0065 |           3.3339 |           0.1542 |
[32m[20230207 15:29:07 @agent_ppo2.py:192][0m |          -0.0099 |           3.1577 |           0.1545 |
[32m[20230207 15:29:07 @agent_ppo2.py:192][0m |          -0.0103 |           3.0731 |           0.1543 |
[32m[20230207 15:29:07 @agent_ppo2.py:192][0m |          -0.0133 |           3.0316 |           0.1543 |
[32m[20230207 15:29:07 @agent_ppo2.py:192][0m |          -0.0128 |           2.9145 |           0.1542 |
[32m[20230207 15:29:07 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:29:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: 9.60
[32m[20230207 15:29:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 86.58
[32m[20230207 15:29:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -37.77
[32m[20230207 15:29:08 @agent_ppo2.py:150][0m Total time:      28.44 min
[32m[20230207 15:29:08 @agent_ppo2.py:152][0m 1495040 total steps have happened
[32m[20230207 15:29:08 @agent_ppo2.py:128][0m #------------------------ Iteration 730 --------------------------#
[32m[20230207 15:29:09 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:29:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:09 @agent_ppo2.py:192][0m |           0.0023 |           9.2296 |           0.1507 |
[32m[20230207 15:29:09 @agent_ppo2.py:192][0m |           0.0252 |           4.7209 |           0.1506 |
[32m[20230207 15:29:09 @agent_ppo2.py:192][0m |          -0.0009 |           4.1408 |           0.1504 |
[32m[20230207 15:29:09 @agent_ppo2.py:192][0m |          -0.0061 |           3.7900 |           0.1503 |
[32m[20230207 15:29:09 @agent_ppo2.py:192][0m |          -0.0011 |           3.5798 |           0.1502 |
[32m[20230207 15:29:09 @agent_ppo2.py:192][0m |          -0.0242 |           3.4497 |           0.1502 |
[32m[20230207 15:29:09 @agent_ppo2.py:192][0m |          -0.0172 |           3.2572 |           0.1499 |
[32m[20230207 15:29:09 @agent_ppo2.py:192][0m |          -0.0032 |           3.1158 |           0.1500 |
[32m[20230207 15:29:09 @agent_ppo2.py:192][0m |          -0.0083 |           3.0223 |           0.1500 |
[32m[20230207 15:29:09 @agent_ppo2.py:192][0m |          -0.0088 |           2.9643 |           0.1499 |
[32m[20230207 15:29:09 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:29:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: 95.26
[32m[20230207 15:29:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 135.60
[32m[20230207 15:29:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 9.42
[32m[20230207 15:29:10 @agent_ppo2.py:150][0m Total time:      28.48 min
[32m[20230207 15:29:10 @agent_ppo2.py:152][0m 1497088 total steps have happened
[32m[20230207 15:29:10 @agent_ppo2.py:128][0m #------------------------ Iteration 731 --------------------------#
[32m[20230207 15:29:11 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:29:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:11 @agent_ppo2.py:192][0m |          -0.0042 |           6.0866 |           0.1538 |
[32m[20230207 15:29:11 @agent_ppo2.py:192][0m |          -0.0051 |           2.0522 |           0.1535 |
[32m[20230207 15:29:11 @agent_ppo2.py:192][0m |          -0.0110 |           1.8297 |           0.1534 |
[32m[20230207 15:29:11 @agent_ppo2.py:192][0m |          -0.0110 |           1.7402 |           0.1534 |
[32m[20230207 15:29:11 @agent_ppo2.py:192][0m |          -0.0090 |           1.6920 |           0.1534 |
[32m[20230207 15:29:11 @agent_ppo2.py:192][0m |          -0.0091 |           1.6691 |           0.1532 |
[32m[20230207 15:29:11 @agent_ppo2.py:192][0m |          -0.0090 |           1.6390 |           0.1532 |
[32m[20230207 15:29:11 @agent_ppo2.py:192][0m |          -0.0121 |           1.5958 |           0.1531 |
[32m[20230207 15:29:12 @agent_ppo2.py:192][0m |          -0.0106 |           1.5689 |           0.1532 |
[32m[20230207 15:29:12 @agent_ppo2.py:192][0m |          -0.0127 |           1.5546 |           0.1531 |
[32m[20230207 15:29:12 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:29:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: 6.57
[32m[20230207 15:29:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 92.84
[32m[20230207 15:29:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -59.50
[32m[20230207 15:29:12 @agent_ppo2.py:150][0m Total time:      28.52 min
[32m[20230207 15:29:12 @agent_ppo2.py:152][0m 1499136 total steps have happened
[32m[20230207 15:29:12 @agent_ppo2.py:128][0m #------------------------ Iteration 732 --------------------------#
[32m[20230207 15:29:13 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230207 15:29:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:13 @agent_ppo2.py:192][0m |          -0.0008 |          15.7151 |           0.1518 |
[32m[20230207 15:29:13 @agent_ppo2.py:192][0m |          -0.0050 |           7.4677 |           0.1517 |
[32m[20230207 15:29:13 @agent_ppo2.py:192][0m |          -0.0075 |           5.0068 |           0.1517 |
[32m[20230207 15:29:13 @agent_ppo2.py:192][0m |          -0.0105 |           3.7447 |           0.1517 |
[32m[20230207 15:29:13 @agent_ppo2.py:192][0m |          -0.0112 |           3.0613 |           0.1517 |
[32m[20230207 15:29:13 @agent_ppo2.py:192][0m |          -0.0126 |           2.5695 |           0.1516 |
[32m[20230207 15:29:13 @agent_ppo2.py:192][0m |          -0.0123 |           2.3375 |           0.1517 |
[32m[20230207 15:29:13 @agent_ppo2.py:192][0m |          -0.0148 |           2.1332 |           0.1515 |
[32m[20230207 15:29:14 @agent_ppo2.py:192][0m |          -0.0148 |           2.0392 |           0.1515 |
[32m[20230207 15:29:14 @agent_ppo2.py:192][0m |          -0.0148 |           1.9708 |           0.1514 |
[32m[20230207 15:29:14 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:29:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: 13.63
[32m[20230207 15:29:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 98.95
[32m[20230207 15:29:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 5.91
[32m[20230207 15:29:14 @agent_ppo2.py:150][0m Total time:      28.55 min
[32m[20230207 15:29:14 @agent_ppo2.py:152][0m 1501184 total steps have happened
[32m[20230207 15:29:14 @agent_ppo2.py:128][0m #------------------------ Iteration 733 --------------------------#
[32m[20230207 15:29:15 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:29:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:15 @agent_ppo2.py:192][0m |          -0.0007 |           8.9267 |           0.1528 |
[32m[20230207 15:29:15 @agent_ppo2.py:192][0m |          -0.0102 |           5.4508 |           0.1527 |
[32m[20230207 15:29:15 @agent_ppo2.py:192][0m |          -0.0044 |           4.4579 |           0.1530 |
[32m[20230207 15:29:15 @agent_ppo2.py:192][0m |          -0.0106 |           3.5536 |           0.1530 |
[32m[20230207 15:29:15 @agent_ppo2.py:192][0m |          -0.0167 |           3.1161 |           0.1529 |
[32m[20230207 15:29:15 @agent_ppo2.py:192][0m |          -0.0178 |           2.7449 |           0.1529 |
[32m[20230207 15:29:16 @agent_ppo2.py:192][0m |          -0.0168 |           2.6813 |           0.1530 |
[32m[20230207 15:29:16 @agent_ppo2.py:192][0m |          -0.0163 |           2.4103 |           0.1527 |
[32m[20230207 15:29:16 @agent_ppo2.py:192][0m |          -0.0191 |           2.3006 |           0.1529 |
[32m[20230207 15:29:16 @agent_ppo2.py:192][0m |          -0.0200 |           2.2578 |           0.1529 |
[32m[20230207 15:29:16 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230207 15:29:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: 11.81
[32m[20230207 15:29:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 66.32
[32m[20230207 15:29:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 29.70
[32m[20230207 15:29:17 @agent_ppo2.py:150][0m Total time:      28.59 min
[32m[20230207 15:29:17 @agent_ppo2.py:152][0m 1503232 total steps have happened
[32m[20230207 15:29:17 @agent_ppo2.py:128][0m #------------------------ Iteration 734 --------------------------#
[32m[20230207 15:29:17 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:29:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:17 @agent_ppo2.py:192][0m |          -0.0057 |           3.0658 |           0.1507 |
[32m[20230207 15:29:18 @agent_ppo2.py:192][0m |          -0.0038 |           2.2663 |           0.1507 |
[32m[20230207 15:29:18 @agent_ppo2.py:192][0m |          -0.0032 |           2.1430 |           0.1507 |
[32m[20230207 15:29:18 @agent_ppo2.py:192][0m |          -0.0244 |           2.1020 |           0.1506 |
[32m[20230207 15:29:18 @agent_ppo2.py:192][0m |           0.0146 |           2.0450 |           0.1507 |
[32m[20230207 15:29:18 @agent_ppo2.py:192][0m |          -0.0110 |           1.9441 |           0.1506 |
[32m[20230207 15:29:18 @agent_ppo2.py:192][0m |          -0.0019 |           1.9050 |           0.1507 |
[32m[20230207 15:29:18 @agent_ppo2.py:192][0m |          -0.0011 |           1.8716 |           0.1502 |
[32m[20230207 15:29:18 @agent_ppo2.py:192][0m |          -0.0188 |           1.8514 |           0.1508 |
[32m[20230207 15:29:18 @agent_ppo2.py:192][0m |          -0.0109 |           1.8141 |           0.1506 |
[32m[20230207 15:29:18 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:29:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: 124.49
[32m[20230207 15:29:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 180.09
[32m[20230207 15:29:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -81.38
[32m[20230207 15:29:19 @agent_ppo2.py:150][0m Total time:      28.62 min
[32m[20230207 15:29:19 @agent_ppo2.py:152][0m 1505280 total steps have happened
[32m[20230207 15:29:19 @agent_ppo2.py:128][0m #------------------------ Iteration 735 --------------------------#
[32m[20230207 15:29:19 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:29:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:20 @agent_ppo2.py:192][0m |           0.0017 |           7.2409 |           0.1516 |
[32m[20230207 15:29:20 @agent_ppo2.py:192][0m |          -0.0002 |           3.8037 |           0.1512 |
[32m[20230207 15:29:20 @agent_ppo2.py:192][0m |           0.0002 |           3.2871 |           0.1510 |
[32m[20230207 15:29:20 @agent_ppo2.py:192][0m |          -0.0057 |           3.0596 |           0.1508 |
[32m[20230207 15:29:20 @agent_ppo2.py:192][0m |          -0.0131 |           2.9321 |           0.1511 |
[32m[20230207 15:29:20 @agent_ppo2.py:192][0m |           0.0458 |           2.9428 |           0.1509 |
[32m[20230207 15:29:20 @agent_ppo2.py:192][0m |          -0.0075 |           3.1196 |           0.1504 |
[32m[20230207 15:29:20 @agent_ppo2.py:192][0m |          -0.0023 |           2.6783 |           0.1507 |
[32m[20230207 15:29:20 @agent_ppo2.py:192][0m |          -0.0107 |           2.6010 |           0.1508 |
[32m[20230207 15:29:20 @agent_ppo2.py:192][0m |          -0.0090 |           2.5595 |           0.1507 |
[32m[20230207 15:29:20 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:29:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -2.20
[32m[20230207 15:29:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 12.51
[32m[20230207 15:29:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -36.75
[32m[20230207 15:29:21 @agent_ppo2.py:150][0m Total time:      28.66 min
[32m[20230207 15:29:21 @agent_ppo2.py:152][0m 1507328 total steps have happened
[32m[20230207 15:29:21 @agent_ppo2.py:128][0m #------------------------ Iteration 736 --------------------------#
[32m[20230207 15:29:22 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:29:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:22 @agent_ppo2.py:192][0m |          -0.0038 |           2.9382 |           0.1525 |
[32m[20230207 15:29:22 @agent_ppo2.py:192][0m |          -0.0067 |           1.2642 |           0.1524 |
[32m[20230207 15:29:22 @agent_ppo2.py:192][0m |          -0.0082 |           1.1087 |           0.1522 |
[32m[20230207 15:29:22 @agent_ppo2.py:192][0m |          -0.0113 |           1.0774 |           0.1522 |
[32m[20230207 15:29:22 @agent_ppo2.py:192][0m |          -0.0058 |           1.0432 |           0.1521 |
[32m[20230207 15:29:22 @agent_ppo2.py:192][0m |          -0.0114 |           1.0153 |           0.1522 |
[32m[20230207 15:29:22 @agent_ppo2.py:192][0m |          -0.0147 |           0.9935 |           0.1521 |
[32m[20230207 15:29:22 @agent_ppo2.py:192][0m |          -0.0142 |           0.9868 |           0.1520 |
[32m[20230207 15:29:22 @agent_ppo2.py:192][0m |          -0.0057 |           0.9777 |           0.1521 |
[32m[20230207 15:29:22 @agent_ppo2.py:192][0m |          -0.0132 |           0.9686 |           0.1520 |
[32m[20230207 15:29:22 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:29:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -8.10
[32m[20230207 15:29:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 55.05
[32m[20230207 15:29:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 55.11
[32m[20230207 15:29:23 @agent_ppo2.py:150][0m Total time:      28.70 min
[32m[20230207 15:29:23 @agent_ppo2.py:152][0m 1509376 total steps have happened
[32m[20230207 15:29:23 @agent_ppo2.py:128][0m #------------------------ Iteration 737 --------------------------#
[32m[20230207 15:29:24 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:29:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:24 @agent_ppo2.py:192][0m |           0.0027 |           7.4628 |           0.1529 |
[32m[20230207 15:29:24 @agent_ppo2.py:192][0m |          -0.0018 |           2.2379 |           0.1530 |
[32m[20230207 15:29:24 @agent_ppo2.py:192][0m |          -0.0066 |           1.9114 |           0.1528 |
[32m[20230207 15:29:24 @agent_ppo2.py:192][0m |          -0.0069 |           1.8496 |           0.1525 |
[32m[20230207 15:29:25 @agent_ppo2.py:192][0m |          -0.0109 |           1.7605 |           0.1527 |
[32m[20230207 15:29:25 @agent_ppo2.py:192][0m |          -0.0122 |           1.6754 |           0.1526 |
[32m[20230207 15:29:25 @agent_ppo2.py:192][0m |          -0.0127 |           1.6538 |           0.1526 |
[32m[20230207 15:29:25 @agent_ppo2.py:192][0m |          -0.0136 |           1.6192 |           0.1524 |
[32m[20230207 15:29:25 @agent_ppo2.py:192][0m |          -0.0140 |           1.5936 |           0.1522 |
[32m[20230207 15:29:25 @agent_ppo2.py:192][0m |          -0.0159 |           1.6273 |           0.1523 |
[32m[20230207 15:29:25 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:29:26 @agent_ppo2.py:145][0m Average TRAINING episode reward: -7.73
[32m[20230207 15:29:26 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 113.31
[32m[20230207 15:29:26 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 74.73
[32m[20230207 15:29:26 @agent_ppo2.py:150][0m Total time:      28.74 min
[32m[20230207 15:29:26 @agent_ppo2.py:152][0m 1511424 total steps have happened
[32m[20230207 15:29:26 @agent_ppo2.py:128][0m #------------------------ Iteration 738 --------------------------#
[32m[20230207 15:29:26 @agent_ppo2.py:134][0m Sampling time: 0.68 s by 1 slaves
[32m[20230207 15:29:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:27 @agent_ppo2.py:192][0m |          -0.0011 |          16.8702 |           0.1513 |
[32m[20230207 15:29:27 @agent_ppo2.py:192][0m |          -0.0106 |           5.6931 |           0.1512 |
[32m[20230207 15:29:27 @agent_ppo2.py:192][0m |          -0.0101 |           4.1619 |           0.1511 |
[32m[20230207 15:29:27 @agent_ppo2.py:192][0m |          -0.0226 |           3.5234 |           0.1509 |
[32m[20230207 15:29:27 @agent_ppo2.py:192][0m |          -0.0073 |           3.0409 |           0.1508 |
[32m[20230207 15:29:27 @agent_ppo2.py:192][0m |          -0.0138 |           2.6763 |           0.1506 |
[32m[20230207 15:29:27 @agent_ppo2.py:192][0m |          -0.0133 |           2.4743 |           0.1504 |
[32m[20230207 15:29:27 @agent_ppo2.py:192][0m |          -0.0214 |           2.4546 |           0.1504 |
[32m[20230207 15:29:27 @agent_ppo2.py:192][0m |          -0.0165 |           2.2043 |           0.1505 |
[32m[20230207 15:29:27 @agent_ppo2.py:192][0m |          -0.0172 |           2.1442 |           0.1504 |
[32m[20230207 15:29:27 @agent_ppo2.py:137][0m Policy update time: 0.69 s
[32m[20230207 15:29:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: 47.56
[32m[20230207 15:29:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 97.53
[32m[20230207 15:29:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 35.57
[32m[20230207 15:29:28 @agent_ppo2.py:150][0m Total time:      28.78 min
[32m[20230207 15:29:28 @agent_ppo2.py:152][0m 1513472 total steps have happened
[32m[20230207 15:29:28 @agent_ppo2.py:128][0m #------------------------ Iteration 739 --------------------------#
[32m[20230207 15:29:28 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:29:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:29 @agent_ppo2.py:192][0m |           0.0016 |          19.9578 |           0.1539 |
[32m[20230207 15:29:29 @agent_ppo2.py:192][0m |          -0.0079 |           6.5723 |           0.1532 |
[32m[20230207 15:29:29 @agent_ppo2.py:192][0m |          -0.0087 |           4.5030 |           0.1532 |
[32m[20230207 15:29:29 @agent_ppo2.py:192][0m |          -0.0120 |           3.6423 |           0.1532 |
[32m[20230207 15:29:29 @agent_ppo2.py:192][0m |          -0.0144 |           3.1375 |           0.1529 |
[32m[20230207 15:29:29 @agent_ppo2.py:192][0m |          -0.0147 |           2.8450 |           0.1527 |
[32m[20230207 15:29:29 @agent_ppo2.py:192][0m |          -0.0163 |           2.5975 |           0.1529 |
[32m[20230207 15:29:29 @agent_ppo2.py:192][0m |          -0.0178 |           2.3937 |           0.1529 |
[32m[20230207 15:29:29 @agent_ppo2.py:192][0m |          -0.0183 |           2.2491 |           0.1528 |
[32m[20230207 15:29:29 @agent_ppo2.py:192][0m |          -0.0183 |           2.0954 |           0.1526 |
[32m[20230207 15:29:29 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:29:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -48.09
[32m[20230207 15:29:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 58.12
[32m[20230207 15:29:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -30.77
[32m[20230207 15:29:30 @agent_ppo2.py:150][0m Total time:      28.81 min
[32m[20230207 15:29:30 @agent_ppo2.py:152][0m 1515520 total steps have happened
[32m[20230207 15:29:30 @agent_ppo2.py:128][0m #------------------------ Iteration 740 --------------------------#
[32m[20230207 15:29:31 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:29:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:31 @agent_ppo2.py:192][0m |           0.0043 |           7.8609 |           0.1488 |
[32m[20230207 15:29:31 @agent_ppo2.py:192][0m |          -0.0023 |           3.2542 |           0.1489 |
[32m[20230207 15:29:31 @agent_ppo2.py:192][0m |          -0.0105 |           2.6138 |           0.1488 |
[32m[20230207 15:29:31 @agent_ppo2.py:192][0m |          -0.0125 |           2.4326 |           0.1486 |
[32m[20230207 15:29:31 @agent_ppo2.py:192][0m |          -0.0158 |           2.2182 |           0.1487 |
[32m[20230207 15:29:31 @agent_ppo2.py:192][0m |          -0.0171 |           2.0999 |           0.1486 |
[32m[20230207 15:29:31 @agent_ppo2.py:192][0m |          -0.0169 |           2.0054 |           0.1486 |
[32m[20230207 15:29:31 @agent_ppo2.py:192][0m |          -0.0159 |           1.9652 |           0.1487 |
[32m[20230207 15:29:31 @agent_ppo2.py:192][0m |          -0.0185 |           1.9049 |           0.1486 |
[32m[20230207 15:29:31 @agent_ppo2.py:192][0m |          -0.0164 |           1.8601 |           0.1484 |
[32m[20230207 15:29:31 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:29:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -43.71
[32m[20230207 15:29:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 61.21
[32m[20230207 15:29:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -39.40
[32m[20230207 15:29:32 @agent_ppo2.py:150][0m Total time:      28.85 min
[32m[20230207 15:29:32 @agent_ppo2.py:152][0m 1517568 total steps have happened
[32m[20230207 15:29:32 @agent_ppo2.py:128][0m #------------------------ Iteration 741 --------------------------#
[32m[20230207 15:29:33 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:29:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:33 @agent_ppo2.py:192][0m |           0.0089 |           1.6926 |           0.1559 |
[32m[20230207 15:29:33 @agent_ppo2.py:192][0m |          -0.0039 |           1.4086 |           0.1558 |
[32m[20230207 15:29:33 @agent_ppo2.py:192][0m |           0.0008 |           1.3528 |           0.1558 |
[32m[20230207 15:29:33 @agent_ppo2.py:192][0m |          -0.0069 |           1.3118 |           0.1556 |
[32m[20230207 15:29:33 @agent_ppo2.py:192][0m |          -0.0360 |           1.3099 |           0.1556 |
[32m[20230207 15:29:34 @agent_ppo2.py:192][0m |          -0.0153 |           1.2679 |           0.1555 |
[32m[20230207 15:29:34 @agent_ppo2.py:192][0m |          -0.0089 |           1.2360 |           0.1556 |
[32m[20230207 15:29:34 @agent_ppo2.py:192][0m |          -0.0191 |           1.2229 |           0.1555 |
[32m[20230207 15:29:34 @agent_ppo2.py:192][0m |          -0.0169 |           1.2172 |           0.1556 |
[32m[20230207 15:29:34 @agent_ppo2.py:192][0m |          -0.0119 |           1.2107 |           0.1555 |
[32m[20230207 15:29:34 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:29:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: 68.39
[32m[20230207 15:29:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 104.21
[32m[20230207 15:29:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 30.19
[32m[20230207 15:29:34 @agent_ppo2.py:150][0m Total time:      28.89 min
[32m[20230207 15:29:34 @agent_ppo2.py:152][0m 1519616 total steps have happened
[32m[20230207 15:29:34 @agent_ppo2.py:128][0m #------------------------ Iteration 742 --------------------------#
[32m[20230207 15:29:35 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:29:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:35 @agent_ppo2.py:192][0m |          -0.0009 |          16.3843 |           0.1525 |
[32m[20230207 15:29:35 @agent_ppo2.py:192][0m |          -0.0193 |           9.0196 |           0.1522 |
[32m[20230207 15:29:35 @agent_ppo2.py:192][0m |          -0.0225 |           8.2693 |           0.1522 |
[32m[20230207 15:29:36 @agent_ppo2.py:192][0m |          -0.0485 |           8.1247 |           0.1521 |
[32m[20230207 15:29:36 @agent_ppo2.py:192][0m |          -0.0238 |           8.0539 |           0.1520 |
[32m[20230207 15:29:36 @agent_ppo2.py:192][0m |          -0.0040 |           7.1568 |           0.1518 |
[32m[20230207 15:29:36 @agent_ppo2.py:192][0m |           0.0016 |           6.9708 |           0.1518 |
[32m[20230207 15:29:36 @agent_ppo2.py:192][0m |          -0.0020 |           7.0112 |           0.1518 |
[32m[20230207 15:29:36 @agent_ppo2.py:192][0m |          -0.0093 |           6.6658 |           0.1519 |
[32m[20230207 15:29:36 @agent_ppo2.py:192][0m |          -0.0220 |           6.4634 |           0.1518 |
[32m[20230207 15:29:36 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:29:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: 112.95
[32m[20230207 15:29:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 169.16
[32m[20230207 15:29:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 23.47
[32m[20230207 15:29:37 @agent_ppo2.py:150][0m Total time:      28.93 min
[32m[20230207 15:29:37 @agent_ppo2.py:152][0m 1521664 total steps have happened
[32m[20230207 15:29:37 @agent_ppo2.py:128][0m #------------------------ Iteration 743 --------------------------#
[32m[20230207 15:29:37 @agent_ppo2.py:134][0m Sampling time: 0.69 s by 1 slaves
[32m[20230207 15:29:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:38 @agent_ppo2.py:192][0m |          -0.0018 |          37.4890 |           0.1494 |
[32m[20230207 15:29:38 @agent_ppo2.py:192][0m |          -0.0023 |          14.2617 |           0.1491 |
[32m[20230207 15:29:38 @agent_ppo2.py:192][0m |          -0.0067 |          10.7264 |           0.1490 |
[32m[20230207 15:29:38 @agent_ppo2.py:192][0m |          -0.0129 |           9.1084 |           0.1490 |
[32m[20230207 15:29:38 @agent_ppo2.py:192][0m |          -0.0102 |           7.9155 |           0.1490 |
[32m[20230207 15:29:38 @agent_ppo2.py:192][0m |          -0.0107 |           7.1251 |           0.1490 |
[32m[20230207 15:29:38 @agent_ppo2.py:192][0m |          -0.0147 |           6.5879 |           0.1492 |
[32m[20230207 15:29:38 @agent_ppo2.py:192][0m |          -0.0081 |           5.9292 |           0.1492 |
[32m[20230207 15:29:38 @agent_ppo2.py:192][0m |          -0.0146 |           5.7092 |           0.1492 |
[32m[20230207 15:29:38 @agent_ppo2.py:192][0m |          -0.0178 |           5.1592 |           0.1493 |
[32m[20230207 15:29:38 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230207 15:29:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -70.34
[32m[20230207 15:29:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 10.97
[32m[20230207 15:29:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -62.69
[32m[20230207 15:29:39 @agent_ppo2.py:150][0m Total time:      28.96 min
[32m[20230207 15:29:39 @agent_ppo2.py:152][0m 1523712 total steps have happened
[32m[20230207 15:29:39 @agent_ppo2.py:128][0m #------------------------ Iteration 744 --------------------------#
[32m[20230207 15:29:39 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 15:29:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:39 @agent_ppo2.py:192][0m |           0.0062 |          17.1175 |           0.1537 |
[32m[20230207 15:29:40 @agent_ppo2.py:192][0m |          -0.0024 |           8.5206 |           0.1537 |
[32m[20230207 15:29:40 @agent_ppo2.py:192][0m |          -0.0105 |           6.0697 |           0.1537 |
[32m[20230207 15:29:40 @agent_ppo2.py:192][0m |          -0.0069 |           5.1700 |           0.1536 |
[32m[20230207 15:29:40 @agent_ppo2.py:192][0m |          -0.0087 |           5.2611 |           0.1536 |
[32m[20230207 15:29:40 @agent_ppo2.py:192][0m |          -0.0137 |           4.3092 |           0.1536 |
[32m[20230207 15:29:40 @agent_ppo2.py:192][0m |          -0.0169 |           4.1547 |           0.1536 |
[32m[20230207 15:29:40 @agent_ppo2.py:192][0m |          -0.0150 |           3.8724 |           0.1535 |
[32m[20230207 15:29:40 @agent_ppo2.py:192][0m |          -0.0153 |           3.6536 |           0.1534 |
[32m[20230207 15:29:40 @agent_ppo2.py:192][0m |          -0.0178 |           3.5115 |           0.1534 |
[32m[20230207 15:29:40 @agent_ppo2.py:137][0m Policy update time: 0.58 s
[32m[20230207 15:29:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -64.23
[32m[20230207 15:29:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 17.69
[32m[20230207 15:29:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 29.79
[32m[20230207 15:29:41 @agent_ppo2.py:150][0m Total time:      28.99 min
[32m[20230207 15:29:41 @agent_ppo2.py:152][0m 1525760 total steps have happened
[32m[20230207 15:29:41 @agent_ppo2.py:128][0m #------------------------ Iteration 745 --------------------------#
[32m[20230207 15:29:41 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:29:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:41 @agent_ppo2.py:192][0m |           0.0061 |          38.8026 |           0.1510 |
[32m[20230207 15:29:41 @agent_ppo2.py:192][0m |          -0.0023 |          20.8790 |           0.1508 |
[32m[20230207 15:29:41 @agent_ppo2.py:192][0m |          -0.0103 |          16.6251 |           0.1507 |
[32m[20230207 15:29:41 @agent_ppo2.py:192][0m |           0.0015 |          14.4672 |           0.1506 |
[32m[20230207 15:29:41 @agent_ppo2.py:192][0m |           0.0027 |          13.2334 |           0.1506 |
[32m[20230207 15:29:42 @agent_ppo2.py:192][0m |          -0.0007 |          13.1717 |           0.1506 |
[32m[20230207 15:29:42 @agent_ppo2.py:192][0m |          -0.0101 |          11.6178 |           0.1505 |
[32m[20230207 15:29:42 @agent_ppo2.py:192][0m |          -0.0170 |          10.8740 |           0.1502 |
[32m[20230207 15:29:42 @agent_ppo2.py:192][0m |          -0.0101 |          10.6418 |           0.1503 |
[32m[20230207 15:29:42 @agent_ppo2.py:192][0m |          -0.0188 |          10.2659 |           0.1504 |
[32m[20230207 15:29:42 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:29:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -34.68
[32m[20230207 15:29:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 114.68
[32m[20230207 15:29:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 109.25
[32m[20230207 15:29:43 @agent_ppo2.py:150][0m Total time:      29.02 min
[32m[20230207 15:29:43 @agent_ppo2.py:152][0m 1527808 total steps have happened
[32m[20230207 15:29:43 @agent_ppo2.py:128][0m #------------------------ Iteration 746 --------------------------#
[32m[20230207 15:29:43 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:29:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:43 @agent_ppo2.py:192][0m |          -0.0093 |          14.3800 |           0.1544 |
[32m[20230207 15:29:44 @agent_ppo2.py:192][0m |          -0.0030 |           5.9382 |           0.1544 |
[32m[20230207 15:29:44 @agent_ppo2.py:192][0m |          -0.0019 |           4.8086 |           0.1543 |
[32m[20230207 15:29:44 @agent_ppo2.py:192][0m |          -0.0121 |           4.3169 |           0.1543 |
[32m[20230207 15:29:44 @agent_ppo2.py:192][0m |          -0.0043 |           3.9905 |           0.1543 |
[32m[20230207 15:29:44 @agent_ppo2.py:192][0m |          -0.0066 |           3.8071 |           0.1543 |
[32m[20230207 15:29:44 @agent_ppo2.py:192][0m |          -0.0225 |           3.5866 |           0.1543 |
[32m[20230207 15:29:44 @agent_ppo2.py:192][0m |          -0.0050 |           3.3725 |           0.1544 |
[32m[20230207 15:29:44 @agent_ppo2.py:192][0m |          -0.0108 |           3.3400 |           0.1544 |
[32m[20230207 15:29:44 @agent_ppo2.py:192][0m |          -0.0029 |           3.1581 |           0.1545 |
[32m[20230207 15:29:44 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:29:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: 20.97
[32m[20230207 15:29:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 106.44
[32m[20230207 15:29:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -56.28
[32m[20230207 15:29:45 @agent_ppo2.py:150][0m Total time:      29.06 min
[32m[20230207 15:29:45 @agent_ppo2.py:152][0m 1529856 total steps have happened
[32m[20230207 15:29:45 @agent_ppo2.py:128][0m #------------------------ Iteration 747 --------------------------#
[32m[20230207 15:29:46 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:29:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:46 @agent_ppo2.py:192][0m |          -0.0056 |          20.3630 |           0.1580 |
[32m[20230207 15:29:46 @agent_ppo2.py:192][0m |          -0.0059 |          11.0412 |           0.1579 |
[32m[20230207 15:29:46 @agent_ppo2.py:192][0m |          -0.0073 |           8.7903 |           0.1580 |
[32m[20230207 15:29:46 @agent_ppo2.py:192][0m |          -0.0147 |           7.7589 |           0.1578 |
[32m[20230207 15:29:46 @agent_ppo2.py:192][0m |          -0.0099 |           6.6286 |           0.1578 |
[32m[20230207 15:29:46 @agent_ppo2.py:192][0m |          -0.0081 |           6.1184 |           0.1578 |
[32m[20230207 15:29:46 @agent_ppo2.py:192][0m |          -0.0067 |           5.6281 |           0.1578 |
[32m[20230207 15:29:46 @agent_ppo2.py:192][0m |          -0.0052 |           5.1815 |           0.1578 |
[32m[20230207 15:29:46 @agent_ppo2.py:192][0m |          -0.0161 |           4.9924 |           0.1579 |
[32m[20230207 15:29:46 @agent_ppo2.py:192][0m |          -0.0083 |           4.6538 |           0.1578 |
[32m[20230207 15:29:46 @agent_ppo2.py:137][0m Policy update time: 0.70 s
[32m[20230207 15:29:47 @agent_ppo2.py:145][0m Average TRAINING episode reward: 27.74
[32m[20230207 15:29:47 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 80.73
[32m[20230207 15:29:47 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 23.22
[32m[20230207 15:29:47 @agent_ppo2.py:150][0m Total time:      29.10 min
[32m[20230207 15:29:47 @agent_ppo2.py:152][0m 1531904 total steps have happened
[32m[20230207 15:29:47 @agent_ppo2.py:128][0m #------------------------ Iteration 748 --------------------------#
[32m[20230207 15:29:48 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:29:48 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:48 @agent_ppo2.py:192][0m |          -0.0019 |          17.3455 |           0.1549 |
[32m[20230207 15:29:48 @agent_ppo2.py:192][0m |          -0.0074 |           9.8249 |           0.1547 |
[32m[20230207 15:29:48 @agent_ppo2.py:192][0m |          -0.0079 |           8.7280 |           0.1545 |
[32m[20230207 15:29:48 @agent_ppo2.py:192][0m |          -0.0086 |           8.1229 |           0.1544 |
[32m[20230207 15:29:48 @agent_ppo2.py:192][0m |          -0.0098 |           7.6938 |           0.1544 |
[32m[20230207 15:29:49 @agent_ppo2.py:192][0m |          -0.0108 |           7.5265 |           0.1544 |
[32m[20230207 15:29:49 @agent_ppo2.py:192][0m |          -0.0113 |           7.0223 |           0.1545 |
[32m[20230207 15:29:49 @agent_ppo2.py:192][0m |          -0.0130 |           6.8098 |           0.1543 |
[32m[20230207 15:29:49 @agent_ppo2.py:192][0m |          -0.0129 |           6.5371 |           0.1543 |
[32m[20230207 15:29:49 @agent_ppo2.py:192][0m |          -0.0143 |           6.3103 |           0.1543 |
[32m[20230207 15:29:49 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:29:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: 18.36
[32m[20230207 15:29:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 90.93
[32m[20230207 15:29:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -20.56
[32m[20230207 15:29:50 @agent_ppo2.py:150][0m Total time:      29.14 min
[32m[20230207 15:29:50 @agent_ppo2.py:152][0m 1533952 total steps have happened
[32m[20230207 15:29:50 @agent_ppo2.py:128][0m #------------------------ Iteration 749 --------------------------#
[32m[20230207 15:29:50 @agent_ppo2.py:134][0m Sampling time: 0.72 s by 1 slaves
[32m[20230207 15:29:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:51 @agent_ppo2.py:192][0m |           0.0020 |          39.1076 |           0.1580 |
[32m[20230207 15:29:51 @agent_ppo2.py:192][0m |          -0.0025 |          18.9957 |           0.1575 |
[32m[20230207 15:29:51 @agent_ppo2.py:192][0m |          -0.0059 |          13.0306 |           0.1577 |
[32m[20230207 15:29:51 @agent_ppo2.py:192][0m |          -0.0068 |          10.5801 |           0.1576 |
[32m[20230207 15:29:51 @agent_ppo2.py:192][0m |          -0.0108 |           8.8719 |           0.1575 |
[32m[20230207 15:29:51 @agent_ppo2.py:192][0m |          -0.0120 |           7.8042 |           0.1575 |
[32m[20230207 15:29:51 @agent_ppo2.py:192][0m |          -0.0124 |           7.0008 |           0.1574 |
[32m[20230207 15:29:51 @agent_ppo2.py:192][0m |          -0.0136 |           6.5506 |           0.1573 |
[32m[20230207 15:29:51 @agent_ppo2.py:192][0m |          -0.0148 |           6.3013 |           0.1574 |
[32m[20230207 15:29:51 @agent_ppo2.py:192][0m |          -0.0143 |           5.8673 |           0.1575 |
[32m[20230207 15:29:51 @agent_ppo2.py:137][0m Policy update time: 0.74 s
[32m[20230207 15:29:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: -85.74
[32m[20230207 15:29:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -32.64
[32m[20230207 15:29:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -7.31
[32m[20230207 15:29:52 @agent_ppo2.py:150][0m Total time:      29.18 min
[32m[20230207 15:29:52 @agent_ppo2.py:152][0m 1536000 total steps have happened
[32m[20230207 15:29:52 @agent_ppo2.py:128][0m #------------------------ Iteration 750 --------------------------#
[32m[20230207 15:29:53 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:29:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:53 @agent_ppo2.py:192][0m |           0.0057 |          16.6935 |           0.1563 |
[32m[20230207 15:29:53 @agent_ppo2.py:192][0m |          -0.0014 |          12.9615 |           0.1560 |
[32m[20230207 15:29:53 @agent_ppo2.py:192][0m |          -0.0049 |          12.5129 |           0.1560 |
[32m[20230207 15:29:53 @agent_ppo2.py:192][0m |          -0.0037 |          12.2443 |           0.1560 |
[32m[20230207 15:29:53 @agent_ppo2.py:192][0m |          -0.0070 |          11.6600 |           0.1558 |
[32m[20230207 15:29:53 @agent_ppo2.py:192][0m |          -0.0090 |          11.4526 |           0.1559 |
[32m[20230207 15:29:53 @agent_ppo2.py:192][0m |          -0.0088 |          11.1752 |           0.1560 |
[32m[20230207 15:29:53 @agent_ppo2.py:192][0m |          -0.0102 |          10.9594 |           0.1560 |
[32m[20230207 15:29:54 @agent_ppo2.py:192][0m |          -0.0101 |          10.7425 |           0.1558 |
[32m[20230207 15:29:54 @agent_ppo2.py:192][0m |          -0.0117 |          10.5396 |           0.1560 |
[32m[20230207 15:29:54 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:29:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: 43.59
[32m[20230207 15:29:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 125.94
[32m[20230207 15:29:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 86.95
[32m[20230207 15:29:54 @agent_ppo2.py:150][0m Total time:      29.22 min
[32m[20230207 15:29:54 @agent_ppo2.py:152][0m 1538048 total steps have happened
[32m[20230207 15:29:54 @agent_ppo2.py:128][0m #------------------------ Iteration 751 --------------------------#
[32m[20230207 15:29:55 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:29:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:55 @agent_ppo2.py:192][0m |          -0.0016 |          19.4530 |           0.1559 |
[32m[20230207 15:29:56 @agent_ppo2.py:192][0m |          -0.0024 |          12.7137 |           0.1559 |
[32m[20230207 15:29:56 @agent_ppo2.py:192][0m |          -0.0152 |          11.4303 |           0.1559 |
[32m[20230207 15:29:56 @agent_ppo2.py:192][0m |          -0.0049 |          10.5907 |           0.1558 |
[32m[20230207 15:29:56 @agent_ppo2.py:192][0m |          -0.0068 |          10.2591 |           0.1558 |
[32m[20230207 15:29:56 @agent_ppo2.py:192][0m |          -0.0105 |           9.9078 |           0.1556 |
[32m[20230207 15:29:56 @agent_ppo2.py:192][0m |           0.0001 |          11.3398 |           0.1557 |
[32m[20230207 15:29:56 @agent_ppo2.py:192][0m |          -0.0114 |           9.3755 |           0.1557 |
[32m[20230207 15:29:56 @agent_ppo2.py:192][0m |          -0.0054 |           9.0514 |           0.1557 |
[32m[20230207 15:29:56 @agent_ppo2.py:192][0m |          -0.0175 |           8.9606 |           0.1558 |
[32m[20230207 15:29:56 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:29:57 @agent_ppo2.py:145][0m Average TRAINING episode reward: -37.84
[32m[20230207 15:29:57 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 35.17
[32m[20230207 15:29:57 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 75.84
[32m[20230207 15:29:57 @agent_ppo2.py:150][0m Total time:      29.26 min
[32m[20230207 15:29:57 @agent_ppo2.py:152][0m 1540096 total steps have happened
[32m[20230207 15:29:57 @agent_ppo2.py:128][0m #------------------------ Iteration 752 --------------------------#
[32m[20230207 15:29:58 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230207 15:29:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:29:58 @agent_ppo2.py:192][0m |           0.0017 |          15.7425 |           0.1520 |
[32m[20230207 15:29:58 @agent_ppo2.py:192][0m |          -0.0028 |           8.6683 |           0.1521 |
[32m[20230207 15:29:58 @agent_ppo2.py:192][0m |          -0.0070 |           7.2495 |           0.1521 |
[32m[20230207 15:29:58 @agent_ppo2.py:192][0m |          -0.0093 |           6.6617 |           0.1520 |
[32m[20230207 15:29:58 @agent_ppo2.py:192][0m |          -0.0099 |           6.0792 |           0.1520 |
[32m[20230207 15:29:58 @agent_ppo2.py:192][0m |          -0.0123 |           5.6113 |           0.1518 |
[32m[20230207 15:29:58 @agent_ppo2.py:192][0m |          -0.0131 |           5.4001 |           0.1517 |
[32m[20230207 15:29:58 @agent_ppo2.py:192][0m |          -0.0133 |           5.0503 |           0.1517 |
[32m[20230207 15:29:58 @agent_ppo2.py:192][0m |          -0.0146 |           4.8723 |           0.1518 |
[32m[20230207 15:29:58 @agent_ppo2.py:192][0m |          -0.0152 |           4.7026 |           0.1516 |
[32m[20230207 15:29:58 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230207 15:29:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: -44.80
[32m[20230207 15:29:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 70.22
[32m[20230207 15:29:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 38.76
[32m[20230207 15:29:59 @agent_ppo2.py:150][0m Total time:      29.30 min
[32m[20230207 15:29:59 @agent_ppo2.py:152][0m 1542144 total steps have happened
[32m[20230207 15:29:59 @agent_ppo2.py:128][0m #------------------------ Iteration 753 --------------------------#
[32m[20230207 15:30:00 @agent_ppo2.py:134][0m Sampling time: 0.71 s by 1 slaves
[32m[20230207 15:30:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:00 @agent_ppo2.py:192][0m |           0.0024 |          23.1432 |           0.1574 |
[32m[20230207 15:30:00 @agent_ppo2.py:192][0m |          -0.0044 |          13.5996 |           0.1572 |
[32m[20230207 15:30:00 @agent_ppo2.py:192][0m |          -0.0073 |          10.3266 |           0.1570 |
[32m[20230207 15:30:00 @agent_ppo2.py:192][0m |          -0.0084 |           8.4743 |           0.1569 |
[32m[20230207 15:30:00 @agent_ppo2.py:192][0m |          -0.0097 |           7.5876 |           0.1567 |
[32m[20230207 15:30:00 @agent_ppo2.py:192][0m |          -0.0108 |           6.9216 |           0.1568 |
[32m[20230207 15:30:00 @agent_ppo2.py:192][0m |          -0.0116 |           6.3077 |           0.1566 |
[32m[20230207 15:30:00 @agent_ppo2.py:192][0m |          -0.0139 |           5.9224 |           0.1565 |
[32m[20230207 15:30:00 @agent_ppo2.py:192][0m |          -0.0149 |           5.5829 |           0.1565 |
[32m[20230207 15:30:01 @agent_ppo2.py:192][0m |          -0.0137 |           5.3409 |           0.1565 |
[32m[20230207 15:30:01 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230207 15:30:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: 5.00
[32m[20230207 15:30:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 109.79
[32m[20230207 15:30:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 92.18
[32m[20230207 15:30:01 @agent_ppo2.py:150][0m Total time:      29.33 min
[32m[20230207 15:30:01 @agent_ppo2.py:152][0m 1544192 total steps have happened
[32m[20230207 15:30:01 @agent_ppo2.py:128][0m #------------------------ Iteration 754 --------------------------#
[32m[20230207 15:30:02 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:30:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:02 @agent_ppo2.py:192][0m |           0.0015 |          17.6606 |           0.1544 |
[32m[20230207 15:30:02 @agent_ppo2.py:192][0m |          -0.0043 |           7.5609 |           0.1543 |
[32m[20230207 15:30:02 @agent_ppo2.py:192][0m |          -0.0083 |           5.3800 |           0.1541 |
[32m[20230207 15:30:02 @agent_ppo2.py:192][0m |          -0.0044 |           4.5824 |           0.1542 |
[32m[20230207 15:30:02 @agent_ppo2.py:192][0m |          -0.0103 |           3.9141 |           0.1540 |
[32m[20230207 15:30:02 @agent_ppo2.py:192][0m |          -0.0120 |           3.5948 |           0.1541 |
[32m[20230207 15:30:02 @agent_ppo2.py:192][0m |          -0.0089 |           3.4463 |           0.1539 |
[32m[20230207 15:30:02 @agent_ppo2.py:192][0m |          -0.0114 |           3.1897 |           0.1541 |
[32m[20230207 15:30:02 @agent_ppo2.py:192][0m |          -0.0115 |           3.0621 |           0.1538 |
[32m[20230207 15:30:02 @agent_ppo2.py:192][0m |          -0.0151 |           2.8737 |           0.1542 |
[32m[20230207 15:30:02 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:30:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: 3.13
[32m[20230207 15:30:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 78.43
[32m[20230207 15:30:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 17.48
[32m[20230207 15:30:03 @agent_ppo2.py:150][0m Total time:      29.36 min
[32m[20230207 15:30:03 @agent_ppo2.py:152][0m 1546240 total steps have happened
[32m[20230207 15:30:03 @agent_ppo2.py:128][0m #------------------------ Iteration 755 --------------------------#
[32m[20230207 15:30:04 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:30:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:04 @agent_ppo2.py:192][0m |          -0.0004 |          12.8985 |           0.1567 |
[32m[20230207 15:30:04 @agent_ppo2.py:192][0m |          -0.0063 |           8.9431 |           0.1566 |
[32m[20230207 15:30:04 @agent_ppo2.py:192][0m |          -0.0094 |           7.9967 |           0.1564 |
[32m[20230207 15:30:04 @agent_ppo2.py:192][0m |          -0.0092 |           7.3755 |           0.1562 |
[32m[20230207 15:30:04 @agent_ppo2.py:192][0m |          -0.0124 |           6.8213 |           0.1562 |
[32m[20230207 15:30:04 @agent_ppo2.py:192][0m |          -0.0129 |           6.4628 |           0.1561 |
[32m[20230207 15:30:05 @agent_ppo2.py:192][0m |          -0.0123 |           6.1539 |           0.1560 |
[32m[20230207 15:30:05 @agent_ppo2.py:192][0m |          -0.0120 |           6.0142 |           0.1559 |
[32m[20230207 15:30:05 @agent_ppo2.py:192][0m |          -0.0147 |           5.5181 |           0.1560 |
[32m[20230207 15:30:05 @agent_ppo2.py:192][0m |          -0.0162 |           5.2363 |           0.1558 |
[32m[20230207 15:30:05 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:30:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: -10.83
[32m[20230207 15:30:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 104.14
[32m[20230207 15:30:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -61.44
[32m[20230207 15:30:06 @agent_ppo2.py:150][0m Total time:      29.41 min
[32m[20230207 15:30:06 @agent_ppo2.py:152][0m 1548288 total steps have happened
[32m[20230207 15:30:06 @agent_ppo2.py:128][0m #------------------------ Iteration 756 --------------------------#
[32m[20230207 15:30:06 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:30:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:06 @agent_ppo2.py:192][0m |           0.0187 |          13.6984 |           0.1558 |
[32m[20230207 15:30:06 @agent_ppo2.py:192][0m |          -0.0161 |           9.4799 |           0.1553 |
[32m[20230207 15:30:07 @agent_ppo2.py:192][0m |          -0.0205 |           8.2359 |           0.1552 |
[32m[20230207 15:30:07 @agent_ppo2.py:192][0m |           0.0040 |           7.2458 |           0.1551 |
[32m[20230207 15:30:07 @agent_ppo2.py:192][0m |           0.0017 |           6.8387 |           0.1550 |
[32m[20230207 15:30:07 @agent_ppo2.py:192][0m |          -0.0051 |           6.8960 |           0.1549 |
[32m[20230207 15:30:07 @agent_ppo2.py:192][0m |          -0.0069 |           6.3172 |           0.1548 |
[32m[20230207 15:30:07 @agent_ppo2.py:192][0m |          -0.0076 |           5.7679 |           0.1548 |
[32m[20230207 15:30:07 @agent_ppo2.py:192][0m |          -0.0139 |           5.5633 |           0.1547 |
[32m[20230207 15:30:07 @agent_ppo2.py:192][0m |          -0.0107 |           5.3739 |           0.1546 |
[32m[20230207 15:30:07 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:30:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: 79.32
[32m[20230207 15:30:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 126.05
[32m[20230207 15:30:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -39.05
[32m[20230207 15:30:08 @agent_ppo2.py:150][0m Total time:      29.45 min
[32m[20230207 15:30:08 @agent_ppo2.py:152][0m 1550336 total steps have happened
[32m[20230207 15:30:08 @agent_ppo2.py:128][0m #------------------------ Iteration 757 --------------------------#
[32m[20230207 15:30:09 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:30:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:09 @agent_ppo2.py:192][0m |          -0.0093 |           1.2826 |           0.1527 |
[32m[20230207 15:30:09 @agent_ppo2.py:192][0m |          -0.0012 |           1.1647 |           0.1529 |
[32m[20230207 15:30:09 @agent_ppo2.py:192][0m |          -0.0085 |           1.1179 |           0.1528 |
[32m[20230207 15:30:09 @agent_ppo2.py:192][0m |          -0.0226 |           1.1080 |           0.1529 |
[32m[20230207 15:30:09 @agent_ppo2.py:192][0m |          -0.0298 |           1.0964 |           0.1531 |
[32m[20230207 15:30:09 @agent_ppo2.py:192][0m |          -0.0057 |           1.0701 |           0.1531 |
[32m[20230207 15:30:09 @agent_ppo2.py:192][0m |          -0.0139 |           1.0664 |           0.1532 |
[32m[20230207 15:30:09 @agent_ppo2.py:192][0m |          -0.0216 |           1.0515 |           0.1533 |
[32m[20230207 15:30:09 @agent_ppo2.py:192][0m |          -0.0060 |           1.0453 |           0.1533 |
[32m[20230207 15:30:10 @agent_ppo2.py:192][0m |          -0.0188 |           1.0328 |           0.1532 |
[32m[20230207 15:30:10 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:30:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: 60.09
[32m[20230207 15:30:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 60.87
[32m[20230207 15:30:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 85.69
[32m[20230207 15:30:10 @agent_ppo2.py:150][0m Total time:      29.49 min
[32m[20230207 15:30:10 @agent_ppo2.py:152][0m 1552384 total steps have happened
[32m[20230207 15:30:10 @agent_ppo2.py:128][0m #------------------------ Iteration 758 --------------------------#
[32m[20230207 15:30:11 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:30:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:11 @agent_ppo2.py:192][0m |           0.0014 |          20.5777 |           0.1579 |
[32m[20230207 15:30:11 @agent_ppo2.py:192][0m |          -0.0042 |           9.6600 |           0.1577 |
[32m[20230207 15:30:11 @agent_ppo2.py:192][0m |          -0.0074 |           7.8524 |           0.1577 |
[32m[20230207 15:30:11 @agent_ppo2.py:192][0m |          -0.0089 |           7.0307 |           0.1577 |
[32m[20230207 15:30:11 @agent_ppo2.py:192][0m |          -0.0090 |           6.6793 |           0.1574 |
[32m[20230207 15:30:11 @agent_ppo2.py:192][0m |          -0.0109 |           6.7585 |           0.1575 |
[32m[20230207 15:30:11 @agent_ppo2.py:192][0m |          -0.0128 |           6.1759 |           0.1573 |
[32m[20230207 15:30:12 @agent_ppo2.py:192][0m |          -0.0132 |           6.0008 |           0.1574 |
[32m[20230207 15:30:12 @agent_ppo2.py:192][0m |          -0.0147 |           5.7421 |           0.1573 |
[32m[20230207 15:30:12 @agent_ppo2.py:192][0m |          -0.0147 |           5.6462 |           0.1572 |
[32m[20230207 15:30:12 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230207 15:30:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: -59.01
[32m[20230207 15:30:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 53.57
[32m[20230207 15:30:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 89.89
[32m[20230207 15:30:12 @agent_ppo2.py:150][0m Total time:      29.52 min
[32m[20230207 15:30:12 @agent_ppo2.py:152][0m 1554432 total steps have happened
[32m[20230207 15:30:12 @agent_ppo2.py:128][0m #------------------------ Iteration 759 --------------------------#
[32m[20230207 15:30:13 @agent_ppo2.py:134][0m Sampling time: 0.66 s by 1 slaves
[32m[20230207 15:30:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:13 @agent_ppo2.py:192][0m |           0.0004 |          14.0024 |           0.1565 |
[32m[20230207 15:30:13 @agent_ppo2.py:192][0m |          -0.0036 |           7.9523 |           0.1563 |
[32m[20230207 15:30:13 @agent_ppo2.py:192][0m |          -0.0088 |           6.5234 |           0.1561 |
[32m[20230207 15:30:13 @agent_ppo2.py:192][0m |          -0.0080 |           5.7174 |           0.1561 |
[32m[20230207 15:30:13 @agent_ppo2.py:192][0m |          -0.0109 |           5.2265 |           0.1561 |
[32m[20230207 15:30:14 @agent_ppo2.py:192][0m |          -0.0116 |           4.8979 |           0.1559 |
[32m[20230207 15:30:14 @agent_ppo2.py:192][0m |          -0.0113 |           4.6509 |           0.1558 |
[32m[20230207 15:30:14 @agent_ppo2.py:192][0m |          -0.0096 |           4.4022 |           0.1558 |
[32m[20230207 15:30:14 @agent_ppo2.py:192][0m |          -0.0109 |           4.2489 |           0.1558 |
[32m[20230207 15:30:14 @agent_ppo2.py:192][0m |          -0.0160 |           4.0927 |           0.1557 |
[32m[20230207 15:30:14 @agent_ppo2.py:137][0m Policy update time: 0.70 s
[32m[20230207 15:30:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: 20.25
[32m[20230207 15:30:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 47.79
[32m[20230207 15:30:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 17.92
[32m[20230207 15:30:15 @agent_ppo2.py:150][0m Total time:      29.56 min
[32m[20230207 15:30:15 @agent_ppo2.py:152][0m 1556480 total steps have happened
[32m[20230207 15:30:15 @agent_ppo2.py:128][0m #------------------------ Iteration 760 --------------------------#
[32m[20230207 15:30:15 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:30:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:15 @agent_ppo2.py:192][0m |          -0.0032 |           8.2284 |           0.1559 |
[32m[20230207 15:30:16 @agent_ppo2.py:192][0m |          -0.0086 |           4.1037 |           0.1559 |
[32m[20230207 15:30:16 @agent_ppo2.py:192][0m |          -0.0110 |           3.7615 |           0.1558 |
[32m[20230207 15:30:16 @agent_ppo2.py:192][0m |           0.0006 |           3.5315 |           0.1557 |
[32m[20230207 15:30:16 @agent_ppo2.py:192][0m |          -0.0150 |           3.3727 |           0.1556 |
[32m[20230207 15:30:16 @agent_ppo2.py:192][0m |          -0.0019 |           3.5102 |           0.1557 |
[32m[20230207 15:30:16 @agent_ppo2.py:192][0m |          -0.0147 |           3.1621 |           0.1556 |
[32m[20230207 15:30:16 @agent_ppo2.py:192][0m |          -0.0164 |           3.0536 |           0.1556 |
[32m[20230207 15:30:16 @agent_ppo2.py:192][0m |          -0.0160 |           2.9797 |           0.1556 |
[32m[20230207 15:30:16 @agent_ppo2.py:192][0m |          -0.0159 |           2.9292 |           0.1555 |
[32m[20230207 15:30:16 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:30:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: -3.63
[32m[20230207 15:30:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 100.27
[32m[20230207 15:30:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -40.60
[32m[20230207 15:30:17 @agent_ppo2.py:150][0m Total time:      29.60 min
[32m[20230207 15:30:17 @agent_ppo2.py:152][0m 1558528 total steps have happened
[32m[20230207 15:30:17 @agent_ppo2.py:128][0m #------------------------ Iteration 761 --------------------------#
[32m[20230207 15:30:17 @agent_ppo2.py:134][0m Sampling time: 0.51 s by 1 slaves
[32m[20230207 15:30:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:18 @agent_ppo2.py:192][0m |          -0.0030 |          20.3295 |           0.1547 |
[32m[20230207 15:30:18 @agent_ppo2.py:192][0m |          -0.0096 |           7.6667 |           0.1545 |
[32m[20230207 15:30:18 @agent_ppo2.py:192][0m |          -0.0115 |           4.8040 |           0.1545 |
[32m[20230207 15:30:18 @agent_ppo2.py:192][0m |          -0.0125 |           3.8992 |           0.1544 |
[32m[20230207 15:30:18 @agent_ppo2.py:192][0m |          -0.0140 |           3.3402 |           0.1544 |
[32m[20230207 15:30:18 @agent_ppo2.py:192][0m |          -0.0167 |           2.8789 |           0.1544 |
[32m[20230207 15:30:18 @agent_ppo2.py:192][0m |          -0.0129 |           2.5648 |           0.1544 |
[32m[20230207 15:30:18 @agent_ppo2.py:192][0m |          -0.0168 |           2.3216 |           0.1543 |
[32m[20230207 15:30:18 @agent_ppo2.py:192][0m |          -0.0149 |           2.1354 |           0.1544 |
[32m[20230207 15:30:18 @agent_ppo2.py:192][0m |          -0.0199 |           1.9995 |           0.1543 |
[32m[20230207 15:30:18 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:30:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: -45.58
[32m[20230207 15:30:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 95.22
[32m[20230207 15:30:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 50.86
[32m[20230207 15:30:19 @agent_ppo2.py:150][0m Total time:      29.63 min
[32m[20230207 15:30:19 @agent_ppo2.py:152][0m 1560576 total steps have happened
[32m[20230207 15:30:19 @agent_ppo2.py:128][0m #------------------------ Iteration 762 --------------------------#
[32m[20230207 15:30:19 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:30:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:20 @agent_ppo2.py:192][0m |           0.0006 |          20.1963 |           0.1545 |
[32m[20230207 15:30:20 @agent_ppo2.py:192][0m |          -0.0055 |          10.1248 |           0.1543 |
[32m[20230207 15:30:20 @agent_ppo2.py:192][0m |          -0.0067 |           7.0008 |           0.1542 |
[32m[20230207 15:30:20 @agent_ppo2.py:192][0m |          -0.0092 |           6.0935 |           0.1541 |
[32m[20230207 15:30:20 @agent_ppo2.py:192][0m |          -0.0125 |           5.2634 |           0.1541 |
[32m[20230207 15:30:20 @agent_ppo2.py:192][0m |          -0.0102 |           4.8306 |           0.1539 |
[32m[20230207 15:30:20 @agent_ppo2.py:192][0m |          -0.0124 |           4.3983 |           0.1540 |
[32m[20230207 15:30:20 @agent_ppo2.py:192][0m |          -0.0140 |           4.1444 |           0.1541 |
[32m[20230207 15:30:20 @agent_ppo2.py:192][0m |          -0.0114 |           3.9207 |           0.1540 |
[32m[20230207 15:30:20 @agent_ppo2.py:192][0m |          -0.0161 |           3.6724 |           0.1540 |
[32m[20230207 15:30:20 @agent_ppo2.py:137][0m Policy update time: 0.67 s
[32m[20230207 15:30:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -51.71
[32m[20230207 15:30:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 84.94
[32m[20230207 15:30:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 82.56
[32m[20230207 15:30:21 @agent_ppo2.py:150][0m Total time:      29.66 min
[32m[20230207 15:30:21 @agent_ppo2.py:152][0m 1562624 total steps have happened
[32m[20230207 15:30:21 @agent_ppo2.py:128][0m #------------------------ Iteration 763 --------------------------#
[32m[20230207 15:30:22 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:30:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:22 @agent_ppo2.py:192][0m |          -0.0010 |          25.7266 |           0.1533 |
[32m[20230207 15:30:22 @agent_ppo2.py:192][0m |          -0.0057 |          15.1876 |           0.1529 |
[32m[20230207 15:30:22 @agent_ppo2.py:192][0m |          -0.0161 |          12.1808 |           0.1530 |
[32m[20230207 15:30:22 @agent_ppo2.py:192][0m |          -0.0072 |          10.9610 |           0.1527 |
[32m[20230207 15:30:22 @agent_ppo2.py:192][0m |          -0.0143 |          10.0398 |           0.1527 |
[32m[20230207 15:30:22 @agent_ppo2.py:192][0m |          -0.0104 |           9.9934 |           0.1528 |
[32m[20230207 15:30:22 @agent_ppo2.py:192][0m |          -0.0125 |           8.7506 |           0.1529 |
[32m[20230207 15:30:23 @agent_ppo2.py:192][0m |          -0.0129 |           8.0023 |           0.1528 |
[32m[20230207 15:30:23 @agent_ppo2.py:192][0m |          -0.0164 |           7.7371 |           0.1528 |
[32m[20230207 15:30:23 @agent_ppo2.py:192][0m |          -0.0130 |           7.1304 |           0.1528 |
[32m[20230207 15:30:23 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:30:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -36.12
[32m[20230207 15:30:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 28.81
[32m[20230207 15:30:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -59.61
[32m[20230207 15:30:23 @agent_ppo2.py:150][0m Total time:      29.70 min
[32m[20230207 15:30:23 @agent_ppo2.py:152][0m 1564672 total steps have happened
[32m[20230207 15:30:23 @agent_ppo2.py:128][0m #------------------------ Iteration 764 --------------------------#
[32m[20230207 15:30:24 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:30:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:24 @agent_ppo2.py:192][0m |           0.0034 |           6.7420 |           0.1528 |
[32m[20230207 15:30:24 @agent_ppo2.py:192][0m |          -0.0061 |           3.7158 |           0.1527 |
[32m[20230207 15:30:24 @agent_ppo2.py:192][0m |          -0.0068 |           3.0205 |           0.1526 |
[32m[20230207 15:30:24 @agent_ppo2.py:192][0m |          -0.0073 |           2.5988 |           0.1527 |
[32m[20230207 15:30:24 @agent_ppo2.py:192][0m |          -0.0090 |           2.3528 |           0.1524 |
[32m[20230207 15:30:25 @agent_ppo2.py:192][0m |          -0.0115 |           2.2391 |           0.1527 |
[32m[20230207 15:30:25 @agent_ppo2.py:192][0m |          -0.0146 |           2.0837 |           0.1528 |
[32m[20230207 15:30:25 @agent_ppo2.py:192][0m |          -0.0174 |           1.9960 |           0.1527 |
[32m[20230207 15:30:25 @agent_ppo2.py:192][0m |          -0.0170 |           1.8817 |           0.1529 |
[32m[20230207 15:30:25 @agent_ppo2.py:192][0m |          -0.0152 |           1.8297 |           0.1530 |
[32m[20230207 15:30:25 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:30:26 @agent_ppo2.py:145][0m Average TRAINING episode reward: 25.41
[32m[20230207 15:30:26 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 129.09
[32m[20230207 15:30:26 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -37.14
[32m[20230207 15:30:26 @agent_ppo2.py:150][0m Total time:      29.74 min
[32m[20230207 15:30:26 @agent_ppo2.py:152][0m 1566720 total steps have happened
[32m[20230207 15:30:26 @agent_ppo2.py:128][0m #------------------------ Iteration 765 --------------------------#
[32m[20230207 15:30:27 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:30:27 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:27 @agent_ppo2.py:192][0m |          -0.0157 |           3.5653 |           0.1546 |
[32m[20230207 15:30:27 @agent_ppo2.py:192][0m |           0.0036 |           2.5576 |           0.1545 |
[32m[20230207 15:30:27 @agent_ppo2.py:192][0m |          -0.0177 |           2.3481 |           0.1544 |
[32m[20230207 15:30:27 @agent_ppo2.py:192][0m |           0.0168 |           2.2822 |           0.1544 |
[32m[20230207 15:30:27 @agent_ppo2.py:192][0m |          -0.0095 |           2.1661 |           0.1543 |
[32m[20230207 15:30:27 @agent_ppo2.py:192][0m |          -0.0044 |           2.0759 |           0.1543 |
[32m[20230207 15:30:27 @agent_ppo2.py:192][0m |          -0.0185 |           2.0357 |           0.1542 |
[32m[20230207 15:30:27 @agent_ppo2.py:192][0m |           0.0056 |           2.0135 |           0.1544 |
[32m[20230207 15:30:27 @agent_ppo2.py:192][0m |          -0.0216 |           1.9728 |           0.1543 |
[32m[20230207 15:30:27 @agent_ppo2.py:192][0m |          -0.0000 |           1.9381 |           0.1541 |
[32m[20230207 15:30:27 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:30:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: 98.20
[32m[20230207 15:30:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 120.95
[32m[20230207 15:30:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 25.30
[32m[20230207 15:30:28 @agent_ppo2.py:150][0m Total time:      29.78 min
[32m[20230207 15:30:28 @agent_ppo2.py:152][0m 1568768 total steps have happened
[32m[20230207 15:30:28 @agent_ppo2.py:128][0m #------------------------ Iteration 766 --------------------------#
[32m[20230207 15:30:29 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:30:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:29 @agent_ppo2.py:192][0m |           0.0007 |          16.9840 |           0.1566 |
[32m[20230207 15:30:29 @agent_ppo2.py:192][0m |          -0.0030 |           7.8962 |           0.1567 |
[32m[20230207 15:30:29 @agent_ppo2.py:192][0m |          -0.0064 |           5.6719 |           0.1565 |
[32m[20230207 15:30:29 @agent_ppo2.py:192][0m |          -0.0081 |           4.6904 |           0.1565 |
[32m[20230207 15:30:29 @agent_ppo2.py:192][0m |          -0.0099 |           4.2151 |           0.1564 |
[32m[20230207 15:30:29 @agent_ppo2.py:192][0m |          -0.0112 |           3.9349 |           0.1564 |
[32m[20230207 15:30:29 @agent_ppo2.py:192][0m |          -0.0113 |           3.5162 |           0.1563 |
[32m[20230207 15:30:29 @agent_ppo2.py:192][0m |          -0.0133 |           3.0601 |           0.1562 |
[32m[20230207 15:30:29 @agent_ppo2.py:192][0m |          -0.0130 |           2.8431 |           0.1561 |
[32m[20230207 15:30:29 @agent_ppo2.py:192][0m |          -0.0145 |           2.7335 |           0.1560 |
[32m[20230207 15:30:29 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:30:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -4.90
[32m[20230207 15:30:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 69.21
[32m[20230207 15:30:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -44.87
[32m[20230207 15:30:30 @agent_ppo2.py:150][0m Total time:      29.82 min
[32m[20230207 15:30:30 @agent_ppo2.py:152][0m 1570816 total steps have happened
[32m[20230207 15:30:30 @agent_ppo2.py:128][0m #------------------------ Iteration 767 --------------------------#
[32m[20230207 15:30:31 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:30:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:31 @agent_ppo2.py:192][0m |           0.0106 |           2.4922 |           0.1577 |
[32m[20230207 15:30:31 @agent_ppo2.py:192][0m |           0.0005 |           2.1545 |           0.1575 |
[32m[20230207 15:30:31 @agent_ppo2.py:192][0m |          -0.0097 |           2.0527 |           0.1576 |
[32m[20230207 15:30:31 @agent_ppo2.py:192][0m |          -0.0141 |           1.9746 |           0.1575 |
[32m[20230207 15:30:31 @agent_ppo2.py:192][0m |          -0.0130 |           1.9071 |           0.1576 |
[32m[20230207 15:30:31 @agent_ppo2.py:192][0m |          -0.0078 |           1.8532 |           0.1576 |
[32m[20230207 15:30:32 @agent_ppo2.py:192][0m |          -0.0006 |           1.8707 |           0.1576 |
[32m[20230207 15:30:32 @agent_ppo2.py:192][0m |          -0.0087 |           1.8018 |           0.1570 |
[32m[20230207 15:30:32 @agent_ppo2.py:192][0m |          -0.0068 |           1.7324 |           0.1574 |
[32m[20230207 15:30:32 @agent_ppo2.py:192][0m |          -0.0132 |           1.7017 |           0.1575 |
[32m[20230207 15:30:32 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:30:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: 70.76
[32m[20230207 15:30:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 83.35
[32m[20230207 15:30:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 48.70
[32m[20230207 15:30:33 @agent_ppo2.py:150][0m Total time:      29.86 min
[32m[20230207 15:30:33 @agent_ppo2.py:152][0m 1572864 total steps have happened
[32m[20230207 15:30:33 @agent_ppo2.py:128][0m #------------------------ Iteration 768 --------------------------#
[32m[20230207 15:30:33 @agent_ppo2.py:134][0m Sampling time: 0.66 s by 1 slaves
[32m[20230207 15:30:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:33 @agent_ppo2.py:192][0m |           0.0024 |          17.1327 |           0.1613 |
[32m[20230207 15:30:33 @agent_ppo2.py:192][0m |          -0.0020 |           7.7039 |           0.1613 |
[32m[20230207 15:30:33 @agent_ppo2.py:192][0m |          -0.0053 |           5.9648 |           0.1611 |
[32m[20230207 15:30:33 @agent_ppo2.py:192][0m |          -0.0077 |           5.1951 |           0.1613 |
[32m[20230207 15:30:34 @agent_ppo2.py:192][0m |          -0.0075 |           4.9402 |           0.1612 |
[32m[20230207 15:30:34 @agent_ppo2.py:192][0m |          -0.0095 |           4.4592 |           0.1611 |
[32m[20230207 15:30:34 @agent_ppo2.py:192][0m |          -0.0102 |           4.2370 |           0.1611 |
[32m[20230207 15:30:34 @agent_ppo2.py:192][0m |          -0.0113 |           3.9675 |           0.1609 |
[32m[20230207 15:30:34 @agent_ppo2.py:192][0m |          -0.0115 |           3.7858 |           0.1609 |
[32m[20230207 15:30:34 @agent_ppo2.py:192][0m |          -0.0117 |           3.6903 |           0.1610 |
[32m[20230207 15:30:34 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230207 15:30:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: 14.57
[32m[20230207 15:30:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 98.78
[32m[20230207 15:30:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -59.46
[32m[20230207 15:30:34 @agent_ppo2.py:150][0m Total time:      29.89 min
[32m[20230207 15:30:34 @agent_ppo2.py:152][0m 1574912 total steps have happened
[32m[20230207 15:30:34 @agent_ppo2.py:128][0m #------------------------ Iteration 769 --------------------------#
[32m[20230207 15:30:35 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:30:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:35 @agent_ppo2.py:192][0m |           0.0025 |           1.6604 |           0.1564 |
[32m[20230207 15:30:35 @agent_ppo2.py:192][0m |           0.0201 |           1.5175 |           0.1562 |
[32m[20230207 15:30:36 @agent_ppo2.py:192][0m |          -0.0085 |           1.4347 |           0.1560 |
[32m[20230207 15:30:36 @agent_ppo2.py:192][0m |          -0.0185 |           1.3919 |           0.1562 |
[32m[20230207 15:30:36 @agent_ppo2.py:192][0m |          -0.0192 |           1.3855 |           0.1562 |
[32m[20230207 15:30:36 @agent_ppo2.py:192][0m |          -0.0013 |           1.3466 |           0.1558 |
[32m[20230207 15:30:36 @agent_ppo2.py:192][0m |          -0.0089 |           1.3076 |           0.1562 |
[32m[20230207 15:30:36 @agent_ppo2.py:192][0m |          -0.0142 |           1.2902 |           0.1562 |
[32m[20230207 15:30:36 @agent_ppo2.py:192][0m |          -0.0237 |           1.2824 |           0.1562 |
[32m[20230207 15:30:36 @agent_ppo2.py:192][0m |           0.0043 |           1.2677 |           0.1562 |
[32m[20230207 15:30:36 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:30:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: 41.77
[32m[20230207 15:30:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 49.37
[32m[20230207 15:30:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 14.82
[32m[20230207 15:30:37 @agent_ppo2.py:150][0m Total time:      29.92 min
[32m[20230207 15:30:37 @agent_ppo2.py:152][0m 1576960 total steps have happened
[32m[20230207 15:30:37 @agent_ppo2.py:128][0m #------------------------ Iteration 770 --------------------------#
[32m[20230207 15:30:37 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:30:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:37 @agent_ppo2.py:192][0m |          -0.0019 |          29.8962 |           0.1500 |
[32m[20230207 15:30:37 @agent_ppo2.py:192][0m |          -0.0118 |          16.3575 |           0.1501 |
[32m[20230207 15:30:37 @agent_ppo2.py:192][0m |          -0.0083 |          12.8706 |           0.1501 |
[32m[20230207 15:30:37 @agent_ppo2.py:192][0m |          -0.0103 |          11.1727 |           0.1501 |
[32m[20230207 15:30:38 @agent_ppo2.py:192][0m |          -0.0087 |          10.0387 |           0.1500 |
[32m[20230207 15:30:38 @agent_ppo2.py:192][0m |          -0.0166 |           9.3340 |           0.1501 |
[32m[20230207 15:30:38 @agent_ppo2.py:192][0m |          -0.0173 |           8.6421 |           0.1500 |
[32m[20230207 15:30:38 @agent_ppo2.py:192][0m |          -0.0168 |           8.0558 |           0.1501 |
[32m[20230207 15:30:38 @agent_ppo2.py:192][0m |          -0.0136 |           7.5748 |           0.1501 |
[32m[20230207 15:30:38 @agent_ppo2.py:192][0m |          -0.0173 |           7.4191 |           0.1501 |
[32m[20230207 15:30:38 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:30:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -30.42
[32m[20230207 15:30:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 11.35
[32m[20230207 15:30:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 21.86
[32m[20230207 15:30:39 @agent_ppo2.py:150][0m Total time:      29.96 min
[32m[20230207 15:30:39 @agent_ppo2.py:152][0m 1579008 total steps have happened
[32m[20230207 15:30:39 @agent_ppo2.py:128][0m #------------------------ Iteration 771 --------------------------#
[32m[20230207 15:30:39 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:30:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:39 @agent_ppo2.py:192][0m |           0.0003 |          51.3831 |           0.1554 |
[32m[20230207 15:30:39 @agent_ppo2.py:192][0m |          -0.0081 |          21.4517 |           0.1555 |
[32m[20230207 15:30:39 @agent_ppo2.py:192][0m |          -0.0092 |          15.5664 |           0.1554 |
[32m[20230207 15:30:40 @agent_ppo2.py:192][0m |          -0.0118 |          11.9347 |           0.1554 |
[32m[20230207 15:30:40 @agent_ppo2.py:192][0m |          -0.0144 |          10.4224 |           0.1554 |
[32m[20230207 15:30:40 @agent_ppo2.py:192][0m |          -0.0144 |           8.8482 |           0.1555 |
[32m[20230207 15:30:40 @agent_ppo2.py:192][0m |          -0.0168 |           8.2490 |           0.1555 |
[32m[20230207 15:30:40 @agent_ppo2.py:192][0m |          -0.0140 |           7.4500 |           0.1553 |
[32m[20230207 15:30:40 @agent_ppo2.py:192][0m |          -0.0172 |           6.7571 |           0.1554 |
[32m[20230207 15:30:40 @agent_ppo2.py:192][0m |          -0.0177 |           6.3260 |           0.1554 |
[32m[20230207 15:30:40 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:30:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: -109.10
[32m[20230207 15:30:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -75.25
[32m[20230207 15:30:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -78.11
[32m[20230207 15:30:40 @agent_ppo2.py:150][0m Total time:      29.99 min
[32m[20230207 15:30:40 @agent_ppo2.py:152][0m 1581056 total steps have happened
[32m[20230207 15:30:40 @agent_ppo2.py:128][0m #------------------------ Iteration 772 --------------------------#
[32m[20230207 15:30:41 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:30:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:41 @agent_ppo2.py:192][0m |           0.0012 |          10.6085 |           0.1565 |
[32m[20230207 15:30:41 @agent_ppo2.py:192][0m |          -0.0074 |           5.0664 |           0.1565 |
[32m[20230207 15:30:42 @agent_ppo2.py:192][0m |          -0.0096 |           4.0821 |           0.1566 |
[32m[20230207 15:30:42 @agent_ppo2.py:192][0m |          -0.0105 |           3.6173 |           0.1563 |
[32m[20230207 15:30:42 @agent_ppo2.py:192][0m |          -0.0093 |           3.2884 |           0.1565 |
[32m[20230207 15:30:42 @agent_ppo2.py:192][0m |          -0.0140 |           3.1238 |           0.1566 |
[32m[20230207 15:30:42 @agent_ppo2.py:192][0m |          -0.0146 |           2.9967 |           0.1568 |
[32m[20230207 15:30:42 @agent_ppo2.py:192][0m |          -0.0167 |           2.8556 |           0.1567 |
[32m[20230207 15:30:42 @agent_ppo2.py:192][0m |          -0.0152 |           2.7354 |           0.1568 |
[32m[20230207 15:30:42 @agent_ppo2.py:192][0m |          -0.0141 |           2.6512 |           0.1568 |
[32m[20230207 15:30:42 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:30:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: 39.15
[32m[20230207 15:30:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 114.36
[32m[20230207 15:30:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 69.44
[32m[20230207 15:30:43 @agent_ppo2.py:150][0m Total time:      30.03 min
[32m[20230207 15:30:43 @agent_ppo2.py:152][0m 1583104 total steps have happened
[32m[20230207 15:30:43 @agent_ppo2.py:128][0m #------------------------ Iteration 773 --------------------------#
[32m[20230207 15:30:44 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:30:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:44 @agent_ppo2.py:192][0m |          -0.0015 |          10.8890 |           0.1601 |
[32m[20230207 15:30:44 @agent_ppo2.py:192][0m |          -0.0068 |           8.1663 |           0.1595 |
[32m[20230207 15:30:44 @agent_ppo2.py:192][0m |          -0.0071 |           7.8460 |           0.1598 |
[32m[20230207 15:30:44 @agent_ppo2.py:192][0m |          -0.0090 |           7.6066 |           0.1595 |
[32m[20230207 15:30:44 @agent_ppo2.py:192][0m |          -0.0105 |           7.4585 |           0.1596 |
[32m[20230207 15:30:44 @agent_ppo2.py:192][0m |          -0.0105 |           7.3733 |           0.1595 |
[32m[20230207 15:30:44 @agent_ppo2.py:192][0m |          -0.0118 |           7.2806 |           0.1595 |
[32m[20230207 15:30:44 @agent_ppo2.py:192][0m |          -0.0119 |           7.1826 |           0.1595 |
[32m[20230207 15:30:44 @agent_ppo2.py:192][0m |          -0.0127 |           7.0839 |           0.1594 |
[32m[20230207 15:30:45 @agent_ppo2.py:192][0m |          -0.0138 |           6.9682 |           0.1595 |
[32m[20230207 15:30:45 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:30:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: -25.00
[32m[20230207 15:30:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 57.15
[32m[20230207 15:30:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 113.55
[32m[20230207 15:30:45 @agent_ppo2.py:150][0m Total time:      30.07 min
[32m[20230207 15:30:45 @agent_ppo2.py:152][0m 1585152 total steps have happened
[32m[20230207 15:30:45 @agent_ppo2.py:128][0m #------------------------ Iteration 774 --------------------------#
[32m[20230207 15:30:46 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:30:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:46 @agent_ppo2.py:192][0m |           0.0111 |          18.9750 |           0.1532 |
[32m[20230207 15:30:46 @agent_ppo2.py:192][0m |           0.0116 |          11.0695 |           0.1529 |
[32m[20230207 15:30:46 @agent_ppo2.py:192][0m |           0.0063 |           9.8108 |           0.1527 |
[32m[20230207 15:30:46 @agent_ppo2.py:192][0m |          -0.0062 |           8.8413 |           0.1528 |
[32m[20230207 15:30:47 @agent_ppo2.py:192][0m |          -0.0082 |           8.2330 |           0.1528 |
[32m[20230207 15:30:47 @agent_ppo2.py:192][0m |          -0.0036 |           7.6307 |           0.1527 |
[32m[20230207 15:30:47 @agent_ppo2.py:192][0m |           0.0016 |           7.1873 |           0.1526 |
[32m[20230207 15:30:47 @agent_ppo2.py:192][0m |          -0.0327 |           7.0656 |           0.1526 |
[32m[20230207 15:30:47 @agent_ppo2.py:192][0m |          -0.0174 |           7.4850 |           0.1527 |
[32m[20230207 15:30:47 @agent_ppo2.py:192][0m |          -0.0079 |           6.4786 |           0.1526 |
[32m[20230207 15:30:47 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:30:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: 82.98
[32m[20230207 15:30:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 96.98
[32m[20230207 15:30:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 80.60
[32m[20230207 15:30:48 @agent_ppo2.py:150][0m Total time:      30.11 min
[32m[20230207 15:30:48 @agent_ppo2.py:152][0m 1587200 total steps have happened
[32m[20230207 15:30:48 @agent_ppo2.py:128][0m #------------------------ Iteration 775 --------------------------#
[32m[20230207 15:30:49 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:30:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:49 @agent_ppo2.py:192][0m |           0.0019 |           7.7970 |           0.1561 |
[32m[20230207 15:30:49 @agent_ppo2.py:192][0m |          -0.0020 |           5.0508 |           0.1562 |
[32m[20230207 15:30:49 @agent_ppo2.py:192][0m |          -0.0093 |           4.2664 |           0.1562 |
[32m[20230207 15:30:49 @agent_ppo2.py:192][0m |          -0.0077 |           3.6367 |           0.1562 |
[32m[20230207 15:30:49 @agent_ppo2.py:192][0m |          -0.0087 |           3.3673 |           0.1561 |
[32m[20230207 15:30:49 @agent_ppo2.py:192][0m |          -0.0048 |           3.2461 |           0.1560 |
[32m[20230207 15:30:49 @agent_ppo2.py:192][0m |          -0.0097 |           3.1065 |           0.1561 |
[32m[20230207 15:30:49 @agent_ppo2.py:192][0m |          -0.0161 |           3.0403 |           0.1560 |
[32m[20230207 15:30:49 @agent_ppo2.py:192][0m |          -0.0134 |           2.8101 |           0.1560 |
[32m[20230207 15:30:49 @agent_ppo2.py:192][0m |          -0.0147 |           2.7350 |           0.1559 |
[32m[20230207 15:30:49 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:30:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: -11.06
[32m[20230207 15:30:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 53.66
[32m[20230207 15:30:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 17.87
[32m[20230207 15:30:50 @agent_ppo2.py:150][0m Total time:      30.15 min
[32m[20230207 15:30:50 @agent_ppo2.py:152][0m 1589248 total steps have happened
[32m[20230207 15:30:50 @agent_ppo2.py:128][0m #------------------------ Iteration 776 --------------------------#
[32m[20230207 15:30:51 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:30:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:51 @agent_ppo2.py:192][0m |          -0.0027 |          25.6905 |           0.1550 |
[32m[20230207 15:30:51 @agent_ppo2.py:192][0m |           0.0040 |          18.5126 |           0.1549 |
[32m[20230207 15:30:51 @agent_ppo2.py:192][0m |          -0.0094 |          16.8149 |           0.1550 |
[32m[20230207 15:30:51 @agent_ppo2.py:192][0m |          -0.0070 |          15.5638 |           0.1547 |
[32m[20230207 15:30:51 @agent_ppo2.py:192][0m |           0.0077 |          15.1617 |           0.1547 |
[32m[20230207 15:30:51 @agent_ppo2.py:192][0m |          -0.0023 |          13.7229 |           0.1549 |
[32m[20230207 15:30:52 @agent_ppo2.py:192][0m |          -0.0480 |          13.3945 |           0.1550 |
[32m[20230207 15:30:52 @agent_ppo2.py:192][0m |           0.0117 |          13.3408 |           0.1548 |
[32m[20230207 15:30:52 @agent_ppo2.py:192][0m |          -0.0006 |          12.1998 |           0.1548 |
[32m[20230207 15:30:52 @agent_ppo2.py:192][0m |          -0.0263 |          12.0751 |           0.1549 |
[32m[20230207 15:30:52 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:30:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: 37.90
[32m[20230207 15:30:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 64.70
[32m[20230207 15:30:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -70.84
[32m[20230207 15:30:52 @agent_ppo2.py:150][0m Total time:      30.19 min
[32m[20230207 15:30:52 @agent_ppo2.py:152][0m 1591296 total steps have happened
[32m[20230207 15:30:52 @agent_ppo2.py:128][0m #------------------------ Iteration 777 --------------------------#
[32m[20230207 15:30:53 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:30:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:53 @agent_ppo2.py:192][0m |           0.0032 |           7.6323 |           0.1614 |
[32m[20230207 15:30:54 @agent_ppo2.py:192][0m |          -0.0005 |           2.9186 |           0.1612 |
[32m[20230207 15:30:54 @agent_ppo2.py:192][0m |          -0.0011 |           2.4409 |           0.1612 |
[32m[20230207 15:30:54 @agent_ppo2.py:192][0m |          -0.0066 |           2.2551 |           0.1610 |
[32m[20230207 15:30:54 @agent_ppo2.py:192][0m |          -0.0063 |           2.0581 |           0.1610 |
[32m[20230207 15:30:54 @agent_ppo2.py:192][0m |          -0.0091 |           2.0022 |           0.1609 |
[32m[20230207 15:30:54 @agent_ppo2.py:192][0m |          -0.0106 |           1.8979 |           0.1608 |
[32m[20230207 15:30:54 @agent_ppo2.py:192][0m |          -0.0064 |           1.8681 |           0.1608 |
[32m[20230207 15:30:54 @agent_ppo2.py:192][0m |          -0.0098 |           1.8008 |           0.1607 |
[32m[20230207 15:30:54 @agent_ppo2.py:192][0m |          -0.0107 |           1.7551 |           0.1606 |
[32m[20230207 15:30:54 @agent_ppo2.py:137][0m Policy update time: 0.97 s
[32m[20230207 15:30:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: -9.18
[32m[20230207 15:30:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 42.61
[32m[20230207 15:30:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 101.27
[32m[20230207 15:30:55 @agent_ppo2.py:150][0m Total time:      30.23 min
[32m[20230207 15:30:55 @agent_ppo2.py:152][0m 1593344 total steps have happened
[32m[20230207 15:30:55 @agent_ppo2.py:128][0m #------------------------ Iteration 778 --------------------------#
[32m[20230207 15:30:56 @agent_ppo2.py:134][0m Sampling time: 0.76 s by 1 slaves
[32m[20230207 15:30:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:56 @agent_ppo2.py:192][0m |           0.0023 |          22.8033 |           0.1616 |
[32m[20230207 15:30:56 @agent_ppo2.py:192][0m |          -0.0036 |           8.3303 |           0.1615 |
[32m[20230207 15:30:56 @agent_ppo2.py:192][0m |          -0.0059 |           6.8671 |           0.1615 |
[32m[20230207 15:30:56 @agent_ppo2.py:192][0m |          -0.0076 |           6.1744 |           0.1613 |
[32m[20230207 15:30:56 @agent_ppo2.py:192][0m |          -0.0094 |           5.7029 |           0.1614 |
[32m[20230207 15:30:56 @agent_ppo2.py:192][0m |          -0.0095 |           5.2733 |           0.1613 |
[32m[20230207 15:30:56 @agent_ppo2.py:192][0m |          -0.0107 |           4.9966 |           0.1613 |
[32m[20230207 15:30:56 @agent_ppo2.py:192][0m |          -0.0117 |           4.7992 |           0.1613 |
[32m[20230207 15:30:57 @agent_ppo2.py:192][0m |          -0.0115 |           4.5473 |           0.1611 |
[32m[20230207 15:30:57 @agent_ppo2.py:192][0m |          -0.0128 |           4.5349 |           0.1612 |
[32m[20230207 15:30:57 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:30:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: -56.59
[32m[20230207 15:30:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 86.65
[32m[20230207 15:30:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -31.71
[32m[20230207 15:30:58 @agent_ppo2.py:150][0m Total time:      30.27 min
[32m[20230207 15:30:58 @agent_ppo2.py:152][0m 1595392 total steps have happened
[32m[20230207 15:30:58 @agent_ppo2.py:128][0m #------------------------ Iteration 779 --------------------------#
[32m[20230207 15:30:58 @agent_ppo2.py:134][0m Sampling time: 0.72 s by 1 slaves
[32m[20230207 15:30:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:30:58 @agent_ppo2.py:192][0m |          -0.0050 |          24.9729 |           0.1586 |
[32m[20230207 15:30:58 @agent_ppo2.py:192][0m |          -0.0051 |          12.6907 |           0.1585 |
[32m[20230207 15:30:59 @agent_ppo2.py:192][0m |          -0.0089 |          10.3949 |           0.1584 |
[32m[20230207 15:30:59 @agent_ppo2.py:192][0m |          -0.0079 |           9.1573 |           0.1583 |
[32m[20230207 15:30:59 @agent_ppo2.py:192][0m |          -0.0146 |           8.3403 |           0.1582 |
[32m[20230207 15:30:59 @agent_ppo2.py:192][0m |          -0.0124 |           7.9821 |           0.1583 |
[32m[20230207 15:30:59 @agent_ppo2.py:192][0m |          -0.0126 |           7.5681 |           0.1583 |
[32m[20230207 15:30:59 @agent_ppo2.py:192][0m |          -0.0121 |           7.1483 |           0.1582 |
[32m[20230207 15:30:59 @agent_ppo2.py:192][0m |          -0.0146 |           6.8406 |           0.1583 |
[32m[20230207 15:30:59 @agent_ppo2.py:192][0m |          -0.0174 |           6.5822 |           0.1582 |
[32m[20230207 15:30:59 @agent_ppo2.py:137][0m Policy update time: 0.76 s
[32m[20230207 15:31:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: 21.21
[32m[20230207 15:31:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 25.33
[32m[20230207 15:31:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -42.94
[32m[20230207 15:31:00 @agent_ppo2.py:150][0m Total time:      30.31 min
[32m[20230207 15:31:00 @agent_ppo2.py:152][0m 1597440 total steps have happened
[32m[20230207 15:31:00 @agent_ppo2.py:128][0m #------------------------ Iteration 780 --------------------------#
[32m[20230207 15:31:00 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230207 15:31:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:00 @agent_ppo2.py:192][0m |          -0.0177 |          36.5020 |           0.1643 |
[32m[20230207 15:31:00 @agent_ppo2.py:192][0m |          -0.0034 |          20.4258 |           0.1637 |
[32m[20230207 15:31:00 @agent_ppo2.py:192][0m |          -0.0031 |          16.3503 |           0.1637 |
[32m[20230207 15:31:00 @agent_ppo2.py:192][0m |          -0.0008 |          13.4945 |           0.1638 |
[32m[20230207 15:31:00 @agent_ppo2.py:192][0m |          -0.0105 |          12.2958 |           0.1638 |
[32m[20230207 15:31:00 @agent_ppo2.py:192][0m |          -0.0267 |          10.8751 |           0.1638 |
[32m[20230207 15:31:01 @agent_ppo2.py:192][0m |          -0.0019 |          10.2004 |           0.1636 |
[32m[20230207 15:31:01 @agent_ppo2.py:192][0m |          -0.0311 |           9.8163 |           0.1636 |
[32m[20230207 15:31:01 @agent_ppo2.py:192][0m |          -0.0407 |           9.6644 |           0.1636 |
[32m[20230207 15:31:01 @agent_ppo2.py:192][0m |          -0.0174 |           8.4293 |           0.1635 |
[32m[20230207 15:31:01 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:31:02 @agent_ppo2.py:145][0m Average TRAINING episode reward: -61.71
[32m[20230207 15:31:02 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 22.16
[32m[20230207 15:31:02 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -18.79
[32m[20230207 15:31:02 @agent_ppo2.py:150][0m Total time:      30.34 min
[32m[20230207 15:31:02 @agent_ppo2.py:152][0m 1599488 total steps have happened
[32m[20230207 15:31:02 @agent_ppo2.py:128][0m #------------------------ Iteration 781 --------------------------#
[32m[20230207 15:31:02 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:31:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:02 @agent_ppo2.py:192][0m |          -0.0018 |          18.1542 |           0.1578 |
[32m[20230207 15:31:02 @agent_ppo2.py:192][0m |          -0.0050 |           6.7597 |           0.1578 |
[32m[20230207 15:31:03 @agent_ppo2.py:192][0m |          -0.0064 |           4.9769 |           0.1576 |
[32m[20230207 15:31:03 @agent_ppo2.py:192][0m |          -0.0102 |           4.0898 |           0.1577 |
[32m[20230207 15:31:03 @agent_ppo2.py:192][0m |          -0.0099 |           3.6176 |           0.1575 |
[32m[20230207 15:31:03 @agent_ppo2.py:192][0m |          -0.0106 |           3.1609 |           0.1577 |
[32m[20230207 15:31:03 @agent_ppo2.py:192][0m |          -0.0104 |           2.9253 |           0.1575 |
[32m[20230207 15:31:03 @agent_ppo2.py:192][0m |          -0.0145 |           2.6690 |           0.1575 |
[32m[20230207 15:31:03 @agent_ppo2.py:192][0m |          -0.0130 |           2.5449 |           0.1576 |
[32m[20230207 15:31:03 @agent_ppo2.py:192][0m |          -0.0159 |           2.4054 |           0.1573 |
[32m[20230207 15:31:03 @agent_ppo2.py:137][0m Policy update time: 0.72 s
[32m[20230207 15:31:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: -36.41
[32m[20230207 15:31:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 60.40
[32m[20230207 15:31:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 102.07
[32m[20230207 15:31:04 @agent_ppo2.py:150][0m Total time:      30.38 min
[32m[20230207 15:31:04 @agent_ppo2.py:152][0m 1601536 total steps have happened
[32m[20230207 15:31:04 @agent_ppo2.py:128][0m #------------------------ Iteration 782 --------------------------#
[32m[20230207 15:31:05 @agent_ppo2.py:134][0m Sampling time: 0.75 s by 1 slaves
[32m[20230207 15:31:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:05 @agent_ppo2.py:192][0m |          -0.0017 |          11.2814 |           0.1600 |
[32m[20230207 15:31:05 @agent_ppo2.py:192][0m |          -0.0007 |           8.1435 |           0.1600 |
[32m[20230207 15:31:05 @agent_ppo2.py:192][0m |           0.0016 |           7.6345 |           0.1599 |
[32m[20230207 15:31:05 @agent_ppo2.py:192][0m |          -0.0038 |           6.8063 |           0.1599 |
[32m[20230207 15:31:05 @agent_ppo2.py:192][0m |          -0.0136 |           6.3652 |           0.1598 |
[32m[20230207 15:31:05 @agent_ppo2.py:192][0m |          -0.0064 |           6.0779 |           0.1597 |
[32m[20230207 15:31:05 @agent_ppo2.py:192][0m |          -0.0097 |           5.7618 |           0.1596 |
[32m[20230207 15:31:05 @agent_ppo2.py:192][0m |          -0.0118 |           5.5092 |           0.1596 |
[32m[20230207 15:31:05 @agent_ppo2.py:192][0m |          -0.0160 |           5.3272 |           0.1597 |
[32m[20230207 15:31:05 @agent_ppo2.py:192][0m |          -0.0076 |           5.0448 |           0.1595 |
[32m[20230207 15:31:05 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:31:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: 11.30
[32m[20230207 15:31:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 71.76
[32m[20230207 15:31:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -82.58
[32m[20230207 15:31:06 @agent_ppo2.py:150][0m Total time:      30.42 min
[32m[20230207 15:31:06 @agent_ppo2.py:152][0m 1603584 total steps have happened
[32m[20230207 15:31:06 @agent_ppo2.py:128][0m #------------------------ Iteration 783 --------------------------#
[32m[20230207 15:31:07 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:31:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:07 @agent_ppo2.py:192][0m |          -0.0044 |          14.0404 |           0.1545 |
[32m[20230207 15:31:07 @agent_ppo2.py:192][0m |           0.0016 |           8.4883 |           0.1545 |
[32m[20230207 15:31:07 @agent_ppo2.py:192][0m |           0.0027 |           7.8928 |           0.1543 |
[32m[20230207 15:31:07 @agent_ppo2.py:192][0m |          -0.0027 |           7.4152 |           0.1542 |
[32m[20230207 15:31:07 @agent_ppo2.py:192][0m |          -0.0103 |           7.1241 |           0.1541 |
[32m[20230207 15:31:07 @agent_ppo2.py:192][0m |           0.0007 |           7.2656 |           0.1540 |
[32m[20230207 15:31:08 @agent_ppo2.py:192][0m |          -0.0045 |           7.0028 |           0.1540 |
[32m[20230207 15:31:08 @agent_ppo2.py:192][0m |          -0.0082 |           6.6521 |           0.1539 |
[32m[20230207 15:31:08 @agent_ppo2.py:192][0m |          -0.0048 |           6.3980 |           0.1539 |
[32m[20230207 15:31:08 @agent_ppo2.py:192][0m |          -0.0129 |           6.2722 |           0.1539 |
[32m[20230207 15:31:08 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:31:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: 92.34
[32m[20230207 15:31:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 92.98
[32m[20230207 15:31:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 39.03
[32m[20230207 15:31:09 @agent_ppo2.py:150][0m Total time:      30.46 min
[32m[20230207 15:31:09 @agent_ppo2.py:152][0m 1605632 total steps have happened
[32m[20230207 15:31:09 @agent_ppo2.py:128][0m #------------------------ Iteration 784 --------------------------#
[32m[20230207 15:31:09 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:31:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:09 @agent_ppo2.py:192][0m |           0.0006 |          26.4967 |           0.1621 |
[32m[20230207 15:31:09 @agent_ppo2.py:192][0m |          -0.0070 |           7.9921 |           0.1620 |
[32m[20230207 15:31:09 @agent_ppo2.py:192][0m |          -0.0084 |           5.4758 |           0.1620 |
[32m[20230207 15:31:09 @agent_ppo2.py:192][0m |          -0.0111 |           4.3237 |           0.1619 |
[32m[20230207 15:31:09 @agent_ppo2.py:192][0m |          -0.0128 |           3.8610 |           0.1619 |
[32m[20230207 15:31:10 @agent_ppo2.py:192][0m |          -0.0127 |           3.3529 |           0.1618 |
[32m[20230207 15:31:10 @agent_ppo2.py:192][0m |          -0.0147 |           3.1342 |           0.1617 |
[32m[20230207 15:31:10 @agent_ppo2.py:192][0m |          -0.0154 |           2.8130 |           0.1616 |
[32m[20230207 15:31:10 @agent_ppo2.py:192][0m |          -0.0163 |           2.6732 |           0.1616 |
[32m[20230207 15:31:10 @agent_ppo2.py:192][0m |          -0.0168 |           2.4756 |           0.1615 |
[32m[20230207 15:31:10 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:31:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -70.87
[32m[20230207 15:31:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 27.09
[32m[20230207 15:31:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -53.06
[32m[20230207 15:31:10 @agent_ppo2.py:150][0m Total time:      30.48 min
[32m[20230207 15:31:10 @agent_ppo2.py:152][0m 1607680 total steps have happened
[32m[20230207 15:31:10 @agent_ppo2.py:128][0m #------------------------ Iteration 785 --------------------------#
[32m[20230207 15:31:11 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:31:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:11 @agent_ppo2.py:192][0m |           0.0006 |          10.1175 |           0.1551 |
[32m[20230207 15:31:11 @agent_ppo2.py:192][0m |          -0.0046 |           4.1631 |           0.1550 |
[32m[20230207 15:31:11 @agent_ppo2.py:192][0m |          -0.0043 |           3.5198 |           0.1548 |
[32m[20230207 15:31:12 @agent_ppo2.py:192][0m |          -0.0073 |           3.1774 |           0.1547 |
[32m[20230207 15:31:12 @agent_ppo2.py:192][0m |          -0.0078 |           2.9574 |           0.1548 |
[32m[20230207 15:31:12 @agent_ppo2.py:192][0m |          -0.0071 |           2.8470 |           0.1546 |
[32m[20230207 15:31:12 @agent_ppo2.py:192][0m |          -0.0080 |           2.7256 |           0.1546 |
[32m[20230207 15:31:12 @agent_ppo2.py:192][0m |          -0.0091 |           2.6204 |           0.1547 |
[32m[20230207 15:31:12 @agent_ppo2.py:192][0m |          -0.0080 |           2.5371 |           0.1546 |
[32m[20230207 15:31:12 @agent_ppo2.py:192][0m |           0.0046 |           2.4462 |           0.1547 |
[32m[20230207 15:31:12 @agent_ppo2.py:137][0m Policy update time: 0.96 s
[32m[20230207 15:31:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -7.05
[32m[20230207 15:31:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 58.28
[32m[20230207 15:31:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 66.89
[32m[20230207 15:31:13 @agent_ppo2.py:150][0m Total time:      30.53 min
[32m[20230207 15:31:13 @agent_ppo2.py:152][0m 1609728 total steps have happened
[32m[20230207 15:31:13 @agent_ppo2.py:128][0m #------------------------ Iteration 786 --------------------------#
[32m[20230207 15:31:14 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:31:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:14 @agent_ppo2.py:192][0m |           0.0002 |          17.5870 |           0.1569 |
[32m[20230207 15:31:14 @agent_ppo2.py:192][0m |          -0.0027 |          10.5559 |           0.1569 |
[32m[20230207 15:31:14 @agent_ppo2.py:192][0m |          -0.0064 |           8.8378 |           0.1566 |
[32m[20230207 15:31:14 @agent_ppo2.py:192][0m |          -0.0083 |           8.2590 |           0.1567 |
[32m[20230207 15:31:14 @agent_ppo2.py:192][0m |          -0.0090 |           7.7477 |           0.1565 |
[32m[20230207 15:31:14 @agent_ppo2.py:192][0m |          -0.0101 |           7.2638 |           0.1566 |
[32m[20230207 15:31:14 @agent_ppo2.py:192][0m |          -0.0109 |           6.9636 |           0.1566 |
[32m[20230207 15:31:15 @agent_ppo2.py:192][0m |          -0.0119 |           6.6837 |           0.1566 |
[32m[20230207 15:31:15 @agent_ppo2.py:192][0m |          -0.0125 |           6.5537 |           0.1566 |
[32m[20230207 15:31:15 @agent_ppo2.py:192][0m |          -0.0135 |           6.4206 |           0.1565 |
[32m[20230207 15:31:15 @agent_ppo2.py:137][0m Policy update time: 0.97 s
[32m[20230207 15:31:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: -59.15
[32m[20230207 15:31:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 18.62
[32m[20230207 15:31:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -55.93
[32m[20230207 15:31:15 @agent_ppo2.py:150][0m Total time:      30.57 min
[32m[20230207 15:31:15 @agent_ppo2.py:152][0m 1611776 total steps have happened
[32m[20230207 15:31:15 @agent_ppo2.py:128][0m #------------------------ Iteration 787 --------------------------#
[32m[20230207 15:31:16 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:31:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:16 @agent_ppo2.py:192][0m |          -0.0091 |           2.1687 |           0.1555 |
[32m[20230207 15:31:16 @agent_ppo2.py:192][0m |           0.0167 |           1.6749 |           0.1553 |
[32m[20230207 15:31:16 @agent_ppo2.py:192][0m |          -0.0102 |           1.5684 |           0.1554 |
[32m[20230207 15:31:16 @agent_ppo2.py:192][0m |          -0.0102 |           1.5644 |           0.1552 |
[32m[20230207 15:31:16 @agent_ppo2.py:192][0m |          -0.0048 |           1.4844 |           0.1552 |
[32m[20230207 15:31:17 @agent_ppo2.py:192][0m |           0.0110 |           1.4583 |           0.1552 |
[32m[20230207 15:31:17 @agent_ppo2.py:192][0m |          -0.0036 |           1.4306 |           0.1551 |
[32m[20230207 15:31:17 @agent_ppo2.py:192][0m |          -0.0013 |           1.4028 |           0.1549 |
[32m[20230207 15:31:17 @agent_ppo2.py:192][0m |          -0.0337 |           1.4161 |           0.1551 |
[32m[20230207 15:31:17 @agent_ppo2.py:192][0m |          -0.0214 |           1.3988 |           0.1550 |
[32m[20230207 15:31:17 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:31:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: 88.44
[32m[20230207 15:31:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 104.66
[32m[20230207 15:31:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -109.03
[32m[20230207 15:31:18 @agent_ppo2.py:150][0m Total time:      30.61 min
[32m[20230207 15:31:18 @agent_ppo2.py:152][0m 1613824 total steps have happened
[32m[20230207 15:31:18 @agent_ppo2.py:128][0m #------------------------ Iteration 788 --------------------------#
[32m[20230207 15:31:18 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:31:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:19 @agent_ppo2.py:192][0m |          -0.0164 |           3.6543 |           0.1562 |
[32m[20230207 15:31:19 @agent_ppo2.py:192][0m |           0.0013 |           2.8340 |           0.1555 |
[32m[20230207 15:31:19 @agent_ppo2.py:192][0m |          -0.0026 |           2.5622 |           0.1554 |
[32m[20230207 15:31:19 @agent_ppo2.py:192][0m |           0.0001 |           2.4123 |           0.1555 |
[32m[20230207 15:31:19 @agent_ppo2.py:192][0m |          -0.0093 |           2.2717 |           0.1555 |
[32m[20230207 15:31:19 @agent_ppo2.py:192][0m |          -0.0257 |           2.2496 |           0.1553 |
[32m[20230207 15:31:19 @agent_ppo2.py:192][0m |          -0.0054 |           2.1338 |           0.1553 |
[32m[20230207 15:31:19 @agent_ppo2.py:192][0m |          -0.0159 |           2.1090 |           0.1553 |
[32m[20230207 15:31:19 @agent_ppo2.py:192][0m |          -0.0240 |           2.0289 |           0.1554 |
[32m[20230207 15:31:19 @agent_ppo2.py:192][0m |          -0.0069 |           1.9730 |           0.1553 |
[32m[20230207 15:31:19 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:31:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: 47.57
[32m[20230207 15:31:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 73.70
[32m[20230207 15:31:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -58.11
[32m[20230207 15:31:20 @agent_ppo2.py:150][0m Total time:      30.65 min
[32m[20230207 15:31:20 @agent_ppo2.py:152][0m 1615872 total steps have happened
[32m[20230207 15:31:20 @agent_ppo2.py:128][0m #------------------------ Iteration 789 --------------------------#
[32m[20230207 15:31:21 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:31:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:21 @agent_ppo2.py:192][0m |          -0.0003 |           7.4848 |           0.1600 |
[32m[20230207 15:31:21 @agent_ppo2.py:192][0m |          -0.0043 |           3.0465 |           0.1598 |
[32m[20230207 15:31:21 @agent_ppo2.py:192][0m |          -0.0070 |           2.4975 |           0.1596 |
[32m[20230207 15:31:21 @agent_ppo2.py:192][0m |          -0.0105 |           2.2615 |           0.1597 |
[32m[20230207 15:31:21 @agent_ppo2.py:192][0m |          -0.0120 |           2.1311 |           0.1595 |
[32m[20230207 15:31:21 @agent_ppo2.py:192][0m |          -0.0129 |           2.0567 |           0.1594 |
[32m[20230207 15:31:21 @agent_ppo2.py:192][0m |          -0.0141 |           2.0013 |           0.1595 |
[32m[20230207 15:31:22 @agent_ppo2.py:192][0m |          -0.0147 |           1.8849 |           0.1593 |
[32m[20230207 15:31:22 @agent_ppo2.py:192][0m |          -0.0146 |           1.8301 |           0.1595 |
[32m[20230207 15:31:22 @agent_ppo2.py:192][0m |          -0.0151 |           1.8038 |           0.1595 |
[32m[20230207 15:31:22 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:31:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: 4.87
[32m[20230207 15:31:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 94.90
[32m[20230207 15:31:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 13.71
[32m[20230207 15:31:23 @agent_ppo2.py:150][0m Total time:      30.69 min
[32m[20230207 15:31:23 @agent_ppo2.py:152][0m 1617920 total steps have happened
[32m[20230207 15:31:23 @agent_ppo2.py:128][0m #------------------------ Iteration 790 --------------------------#
[32m[20230207 15:31:23 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:31:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:24 @agent_ppo2.py:192][0m |          -0.0013 |          13.1933 |           0.1582 |
[32m[20230207 15:31:24 @agent_ppo2.py:192][0m |          -0.0040 |           4.8476 |           0.1583 |
[32m[20230207 15:31:24 @agent_ppo2.py:192][0m |          -0.0106 |           3.4116 |           0.1579 |
[32m[20230207 15:31:24 @agent_ppo2.py:192][0m |          -0.0084 |           2.9073 |           0.1575 |
[32m[20230207 15:31:24 @agent_ppo2.py:192][0m |          -0.0110 |           2.8144 |           0.1578 |
[32m[20230207 15:31:24 @agent_ppo2.py:192][0m |          -0.0119 |           2.5351 |           0.1577 |
[32m[20230207 15:31:24 @agent_ppo2.py:192][0m |          -0.0140 |           2.4528 |           0.1575 |
[32m[20230207 15:31:24 @agent_ppo2.py:192][0m |          -0.0139 |           2.3845 |           0.1576 |
[32m[20230207 15:31:24 @agent_ppo2.py:192][0m |          -0.0129 |           2.3134 |           0.1576 |
[32m[20230207 15:31:24 @agent_ppo2.py:192][0m |          -0.0133 |           2.2672 |           0.1574 |
[32m[20230207 15:31:24 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:31:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -12.01
[32m[20230207 15:31:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 86.98
[32m[20230207 15:31:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -32.49
[32m[20230207 15:31:25 @agent_ppo2.py:150][0m Total time:      30.73 min
[32m[20230207 15:31:25 @agent_ppo2.py:152][0m 1619968 total steps have happened
[32m[20230207 15:31:25 @agent_ppo2.py:128][0m #------------------------ Iteration 791 --------------------------#
[32m[20230207 15:31:26 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:31:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:26 @agent_ppo2.py:192][0m |          -0.0092 |           3.2113 |           0.1546 |
[32m[20230207 15:31:26 @agent_ppo2.py:192][0m |          -0.0125 |           2.4725 |           0.1545 |
[32m[20230207 15:31:26 @agent_ppo2.py:192][0m |          -0.0112 |           2.3014 |           0.1544 |
[32m[20230207 15:31:26 @agent_ppo2.py:192][0m |          -0.0016 |           2.2122 |           0.1540 |
[32m[20230207 15:31:26 @agent_ppo2.py:192][0m |          -0.0073 |           2.1463 |           0.1541 |
[32m[20230207 15:31:26 @agent_ppo2.py:192][0m |          -0.0110 |           2.1386 |           0.1541 |
[32m[20230207 15:31:26 @agent_ppo2.py:192][0m |           0.0006 |           2.0762 |           0.1537 |
[32m[20230207 15:31:26 @agent_ppo2.py:192][0m |          -0.0203 |           2.0189 |           0.1540 |
[32m[20230207 15:31:26 @agent_ppo2.py:192][0m |          -0.0228 |           2.0008 |           0.1539 |
[32m[20230207 15:31:26 @agent_ppo2.py:192][0m |           0.0039 |           1.9897 |           0.1539 |
[32m[20230207 15:31:26 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:31:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: 93.06
[32m[20230207 15:31:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 108.45
[32m[20230207 15:31:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 14.77
[32m[20230207 15:31:27 @agent_ppo2.py:150][0m Total time:      30.76 min
[32m[20230207 15:31:27 @agent_ppo2.py:152][0m 1622016 total steps have happened
[32m[20230207 15:31:27 @agent_ppo2.py:128][0m #------------------------ Iteration 792 --------------------------#
[32m[20230207 15:31:28 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:31:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:28 @agent_ppo2.py:192][0m |          -0.0013 |          23.5770 |           0.1552 |
[32m[20230207 15:31:28 @agent_ppo2.py:192][0m |          -0.0052 |          12.4598 |           0.1549 |
[32m[20230207 15:31:28 @agent_ppo2.py:192][0m |          -0.0056 |           9.2511 |           0.1547 |
[32m[20230207 15:31:28 @agent_ppo2.py:192][0m |          -0.0111 |           7.3274 |           0.1548 |
[32m[20230207 15:31:28 @agent_ppo2.py:192][0m |          -0.0111 |           6.0401 |           0.1549 |
[32m[20230207 15:31:28 @agent_ppo2.py:192][0m |          -0.0117 |           5.4437 |           0.1549 |
[32m[20230207 15:31:28 @agent_ppo2.py:192][0m |          -0.0147 |           4.8089 |           0.1549 |
[32m[20230207 15:31:28 @agent_ppo2.py:192][0m |          -0.0172 |           4.4812 |           0.1547 |
[32m[20230207 15:31:28 @agent_ppo2.py:192][0m |          -0.0126 |           4.0424 |           0.1549 |
[32m[20230207 15:31:28 @agent_ppo2.py:192][0m |          -0.0161 |           3.7869 |           0.1549 |
[32m[20230207 15:31:28 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:31:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -52.48
[32m[20230207 15:31:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 60.82
[32m[20230207 15:31:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 13.22
[32m[20230207 15:31:29 @agent_ppo2.py:150][0m Total time:      30.80 min
[32m[20230207 15:31:29 @agent_ppo2.py:152][0m 1624064 total steps have happened
[32m[20230207 15:31:29 @agent_ppo2.py:128][0m #------------------------ Iteration 793 --------------------------#
[32m[20230207 15:31:30 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:31:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:30 @agent_ppo2.py:192][0m |           0.0017 |          20.3020 |           0.1577 |
[32m[20230207 15:31:30 @agent_ppo2.py:192][0m |          -0.0044 |           8.0485 |           0.1577 |
[32m[20230207 15:31:30 @agent_ppo2.py:192][0m |          -0.0075 |           5.6023 |           0.1577 |
[32m[20230207 15:31:30 @agent_ppo2.py:192][0m |          -0.0076 |           5.0749 |           0.1579 |
[32m[20230207 15:31:30 @agent_ppo2.py:192][0m |          -0.0090 |           4.7686 |           0.1580 |
[32m[20230207 15:31:30 @agent_ppo2.py:192][0m |          -0.0108 |           4.5240 |           0.1580 |
[32m[20230207 15:31:30 @agent_ppo2.py:192][0m |          -0.0093 |           4.3574 |           0.1581 |
[32m[20230207 15:31:30 @agent_ppo2.py:192][0m |          -0.0115 |           4.2296 |           0.1581 |
[32m[20230207 15:31:30 @agent_ppo2.py:192][0m |          -0.0103 |           4.0959 |           0.1583 |
[32m[20230207 15:31:30 @agent_ppo2.py:192][0m |          -0.0152 |           4.0128 |           0.1581 |
[32m[20230207 15:31:30 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:31:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -37.96
[32m[20230207 15:31:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 95.33
[32m[20230207 15:31:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 5.59
[32m[20230207 15:31:31 @agent_ppo2.py:150][0m Total time:      30.83 min
[32m[20230207 15:31:31 @agent_ppo2.py:152][0m 1626112 total steps have happened
[32m[20230207 15:31:31 @agent_ppo2.py:128][0m #------------------------ Iteration 794 --------------------------#
[32m[20230207 15:31:32 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:31:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:32 @agent_ppo2.py:192][0m |           0.0008 |          22.4515 |           0.1529 |
[32m[20230207 15:31:32 @agent_ppo2.py:192][0m |          -0.0053 |          10.8570 |           0.1526 |
[32m[20230207 15:31:32 @agent_ppo2.py:192][0m |          -0.0111 |           8.5613 |           0.1526 |
[32m[20230207 15:31:32 @agent_ppo2.py:192][0m |          -0.0073 |           7.5831 |           0.1524 |
[32m[20230207 15:31:32 @agent_ppo2.py:192][0m |          -0.0100 |           6.8424 |           0.1524 |
[32m[20230207 15:31:32 @agent_ppo2.py:192][0m |          -0.0108 |           6.3698 |           0.1524 |
[32m[20230207 15:31:32 @agent_ppo2.py:192][0m |          -0.0140 |           6.0876 |           0.1522 |
[32m[20230207 15:31:33 @agent_ppo2.py:192][0m |          -0.0176 |           5.9273 |           0.1522 |
[32m[20230207 15:31:33 @agent_ppo2.py:192][0m |          -0.0203 |           5.5795 |           0.1522 |
[32m[20230207 15:31:33 @agent_ppo2.py:192][0m |          -0.0177 |           5.3774 |           0.1522 |
[32m[20230207 15:31:33 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:31:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: -22.38
[32m[20230207 15:31:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 124.45
[32m[20230207 15:31:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 92.21
[32m[20230207 15:31:33 @agent_ppo2.py:150][0m Total time:      30.87 min
[32m[20230207 15:31:33 @agent_ppo2.py:152][0m 1628160 total steps have happened
[32m[20230207 15:31:33 @agent_ppo2.py:128][0m #------------------------ Iteration 795 --------------------------#
[32m[20230207 15:31:34 @agent_ppo2.py:134][0m Sampling time: 0.75 s by 1 slaves
[32m[20230207 15:31:34 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:34 @agent_ppo2.py:192][0m |           0.0018 |          12.9892 |           0.1536 |
[32m[20230207 15:31:34 @agent_ppo2.py:192][0m |          -0.0094 |           8.5499 |           0.1536 |
[32m[20230207 15:31:35 @agent_ppo2.py:192][0m |          -0.0004 |           7.5183 |           0.1535 |
[32m[20230207 15:31:35 @agent_ppo2.py:192][0m |          -0.0088 |           6.9177 |           0.1534 |
[32m[20230207 15:31:35 @agent_ppo2.py:192][0m |          -0.0029 |           6.6853 |           0.1533 |
[32m[20230207 15:31:35 @agent_ppo2.py:192][0m |          -0.0125 |           5.9601 |           0.1531 |
[32m[20230207 15:31:35 @agent_ppo2.py:192][0m |          -0.0126 |           5.4234 |           0.1532 |
[32m[20230207 15:31:35 @agent_ppo2.py:192][0m |          -0.0042 |           5.2069 |           0.1532 |
[32m[20230207 15:31:35 @agent_ppo2.py:192][0m |          -0.0160 |           4.8298 |           0.1531 |
[32m[20230207 15:31:35 @agent_ppo2.py:192][0m |          -0.0285 |           4.6238 |           0.1530 |
[32m[20230207 15:31:35 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:31:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -8.56
[32m[20230207 15:31:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 62.22
[32m[20230207 15:31:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -16.70
[32m[20230207 15:31:36 @agent_ppo2.py:150][0m Total time:      30.91 min
[32m[20230207 15:31:36 @agent_ppo2.py:152][0m 1630208 total steps have happened
[32m[20230207 15:31:36 @agent_ppo2.py:128][0m #------------------------ Iteration 796 --------------------------#
[32m[20230207 15:31:37 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:31:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:37 @agent_ppo2.py:192][0m |           0.0025 |           4.3324 |           0.1601 |
[32m[20230207 15:31:37 @agent_ppo2.py:192][0m |          -0.0027 |           2.2480 |           0.1601 |
[32m[20230207 15:31:37 @agent_ppo2.py:192][0m |          -0.0045 |           2.0842 |           0.1601 |
[32m[20230207 15:31:37 @agent_ppo2.py:192][0m |          -0.0079 |           1.9541 |           0.1602 |
[32m[20230207 15:31:37 @agent_ppo2.py:192][0m |          -0.0084 |           1.8739 |           0.1602 |
[32m[20230207 15:31:37 @agent_ppo2.py:192][0m |          -0.0094 |           1.8056 |           0.1603 |
[32m[20230207 15:31:37 @agent_ppo2.py:192][0m |          -0.0119 |           1.7491 |           0.1601 |
[32m[20230207 15:31:37 @agent_ppo2.py:192][0m |          -0.0125 |           1.6848 |           0.1601 |
[32m[20230207 15:31:37 @agent_ppo2.py:192][0m |          -0.0128 |           1.6440 |           0.1601 |
[32m[20230207 15:31:37 @agent_ppo2.py:192][0m |          -0.0129 |           1.6007 |           0.1602 |
[32m[20230207 15:31:37 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:31:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: -9.09
[32m[20230207 15:31:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 59.86
[32m[20230207 15:31:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 75.67
[32m[20230207 15:31:38 @agent_ppo2.py:150][0m Total time:      30.95 min
[32m[20230207 15:31:38 @agent_ppo2.py:152][0m 1632256 total steps have happened
[32m[20230207 15:31:38 @agent_ppo2.py:128][0m #------------------------ Iteration 797 --------------------------#
[32m[20230207 15:31:39 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:31:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:39 @agent_ppo2.py:192][0m |          -0.0046 |          18.8091 |           0.1546 |
[32m[20230207 15:31:39 @agent_ppo2.py:192][0m |          -0.0063 |          11.5144 |           0.1548 |
[32m[20230207 15:31:39 @agent_ppo2.py:192][0m |          -0.0093 |           9.8071 |           0.1548 |
[32m[20230207 15:31:39 @agent_ppo2.py:192][0m |          -0.0137 |           8.7505 |           0.1545 |
[32m[20230207 15:31:39 @agent_ppo2.py:192][0m |          -0.0076 |           8.3190 |           0.1545 |
[32m[20230207 15:31:39 @agent_ppo2.py:192][0m |          -0.0218 |           7.7339 |           0.1542 |
[32m[20230207 15:31:39 @agent_ppo2.py:192][0m |          -0.0112 |           7.1362 |           0.1542 |
[32m[20230207 15:31:39 @agent_ppo2.py:192][0m |          -0.0283 |           6.7843 |           0.1539 |
[32m[20230207 15:31:39 @agent_ppo2.py:192][0m |          -0.0216 |           6.7626 |           0.1539 |
[32m[20230207 15:31:39 @agent_ppo2.py:192][0m |          -0.0104 |           6.6082 |           0.1539 |
[32m[20230207 15:31:39 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:31:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: -50.53
[32m[20230207 15:31:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 74.62
[32m[20230207 15:31:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 46.54
[32m[20230207 15:31:40 @agent_ppo2.py:150][0m Total time:      30.98 min
[32m[20230207 15:31:40 @agent_ppo2.py:152][0m 1634304 total steps have happened
[32m[20230207 15:31:40 @agent_ppo2.py:128][0m #------------------------ Iteration 798 --------------------------#
[32m[20230207 15:31:41 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 15:31:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:41 @agent_ppo2.py:192][0m |          -0.0008 |          19.3269 |           0.1555 |
[32m[20230207 15:31:41 @agent_ppo2.py:192][0m |          -0.0008 |          11.6228 |           0.1552 |
[32m[20230207 15:31:41 @agent_ppo2.py:192][0m |          -0.0115 |           8.8406 |           0.1550 |
[32m[20230207 15:31:41 @agent_ppo2.py:192][0m |          -0.0046 |           7.8350 |           0.1551 |
[32m[20230207 15:31:41 @agent_ppo2.py:192][0m |          -0.0012 |           6.9285 |           0.1553 |
[32m[20230207 15:31:41 @agent_ppo2.py:192][0m |          -0.0073 |           6.0802 |           0.1552 |
[32m[20230207 15:31:41 @agent_ppo2.py:192][0m |          -0.0180 |           5.3692 |           0.1550 |
[32m[20230207 15:31:41 @agent_ppo2.py:192][0m |          -0.0151 |           4.9283 |           0.1552 |
[32m[20230207 15:31:41 @agent_ppo2.py:192][0m |          -0.0123 |           4.8588 |           0.1551 |
[32m[20230207 15:31:41 @agent_ppo2.py:192][0m |          -0.0171 |           4.3144 |           0.1550 |
[32m[20230207 15:31:41 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230207 15:31:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: -32.02
[32m[20230207 15:31:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 67.94
[32m[20230207 15:31:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 2.37
[32m[20230207 15:31:42 @agent_ppo2.py:150][0m Total time:      31.02 min
[32m[20230207 15:31:42 @agent_ppo2.py:152][0m 1636352 total steps have happened
[32m[20230207 15:31:42 @agent_ppo2.py:128][0m #------------------------ Iteration 799 --------------------------#
[32m[20230207 15:31:43 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:31:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:43 @agent_ppo2.py:192][0m |           0.0001 |          15.6988 |           0.1612 |
[32m[20230207 15:31:43 @agent_ppo2.py:192][0m |          -0.0039 |           6.0629 |           0.1609 |
[32m[20230207 15:31:44 @agent_ppo2.py:192][0m |          -0.0075 |           5.3537 |           0.1610 |
[32m[20230207 15:31:44 @agent_ppo2.py:192][0m |          -0.0097 |           4.8091 |           0.1608 |
[32m[20230207 15:31:44 @agent_ppo2.py:192][0m |          -0.0112 |           4.6304 |           0.1609 |
[32m[20230207 15:31:44 @agent_ppo2.py:192][0m |          -0.0123 |           4.2936 |           0.1607 |
[32m[20230207 15:31:44 @agent_ppo2.py:192][0m |          -0.0124 |           4.1368 |           0.1607 |
[32m[20230207 15:31:44 @agent_ppo2.py:192][0m |          -0.0133 |           4.0442 |           0.1607 |
[32m[20230207 15:31:44 @agent_ppo2.py:192][0m |          -0.0127 |           3.8068 |           0.1607 |
[32m[20230207 15:31:44 @agent_ppo2.py:192][0m |          -0.0151 |           3.6638 |           0.1606 |
[32m[20230207 15:31:44 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:31:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: -34.83
[32m[20230207 15:31:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 94.60
[32m[20230207 15:31:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -7.06
[32m[20230207 15:31:45 @agent_ppo2.py:150][0m Total time:      31.06 min
[32m[20230207 15:31:45 @agent_ppo2.py:152][0m 1638400 total steps have happened
[32m[20230207 15:31:45 @agent_ppo2.py:128][0m #------------------------ Iteration 800 --------------------------#
[32m[20230207 15:31:46 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:31:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:46 @agent_ppo2.py:192][0m |          -0.0001 |          17.4826 |           0.1604 |
[32m[20230207 15:31:46 @agent_ppo2.py:192][0m |          -0.0082 |          11.2650 |           0.1600 |
[32m[20230207 15:31:46 @agent_ppo2.py:192][0m |          -0.0102 |           9.8139 |           0.1602 |
[32m[20230207 15:31:46 @agent_ppo2.py:192][0m |          -0.0105 |           9.3022 |           0.1599 |
[32m[20230207 15:31:46 @agent_ppo2.py:192][0m |          -0.0116 |           8.6641 |           0.1600 |
[32m[20230207 15:31:46 @agent_ppo2.py:192][0m |          -0.0132 |           8.2426 |           0.1599 |
[32m[20230207 15:31:46 @agent_ppo2.py:192][0m |          -0.0137 |           8.0012 |           0.1600 |
[32m[20230207 15:31:46 @agent_ppo2.py:192][0m |          -0.0136 |           7.7434 |           0.1600 |
[32m[20230207 15:31:47 @agent_ppo2.py:192][0m |          -0.0146 |           7.4884 |           0.1600 |
[32m[20230207 15:31:47 @agent_ppo2.py:192][0m |          -0.0149 |           7.3337 |           0.1600 |
[32m[20230207 15:31:47 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:31:47 @agent_ppo2.py:145][0m Average TRAINING episode reward: -17.15
[32m[20230207 15:31:47 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 81.66
[32m[20230207 15:31:47 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -30.22
[32m[20230207 15:31:47 @agent_ppo2.py:150][0m Total time:      31.10 min
[32m[20230207 15:31:47 @agent_ppo2.py:152][0m 1640448 total steps have happened
[32m[20230207 15:31:47 @agent_ppo2.py:128][0m #------------------------ Iteration 801 --------------------------#
[32m[20230207 15:31:48 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:31:48 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:48 @agent_ppo2.py:192][0m |           0.0032 |          20.8570 |           0.1556 |
[32m[20230207 15:31:48 @agent_ppo2.py:192][0m |           0.0028 |          12.3051 |           0.1556 |
[32m[20230207 15:31:48 @agent_ppo2.py:192][0m |          -0.0011 |          10.6384 |           0.1555 |
[32m[20230207 15:31:49 @agent_ppo2.py:192][0m |          -0.0073 |          10.3201 |           0.1556 |
[32m[20230207 15:31:49 @agent_ppo2.py:192][0m |          -0.0240 |           9.9243 |           0.1554 |
[32m[20230207 15:31:49 @agent_ppo2.py:192][0m |           0.0054 |           8.7559 |           0.1552 |
[32m[20230207 15:31:49 @agent_ppo2.py:192][0m |          -0.0047 |           8.1310 |           0.1554 |
[32m[20230207 15:31:49 @agent_ppo2.py:192][0m |          -0.0156 |           7.8974 |           0.1552 |
[32m[20230207 15:31:49 @agent_ppo2.py:192][0m |          -0.0018 |           7.6038 |           0.1555 |
[32m[20230207 15:31:49 @agent_ppo2.py:192][0m |          -0.0282 |           7.4356 |           0.1554 |
[32m[20230207 15:31:49 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:31:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: 82.50
[32m[20230207 15:31:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 95.48
[32m[20230207 15:31:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -63.59
[32m[20230207 15:31:50 @agent_ppo2.py:150][0m Total time:      31.14 min
[32m[20230207 15:31:50 @agent_ppo2.py:152][0m 1642496 total steps have happened
[32m[20230207 15:31:50 @agent_ppo2.py:128][0m #------------------------ Iteration 802 --------------------------#
[32m[20230207 15:31:50 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:31:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:51 @agent_ppo2.py:192][0m |          -0.0005 |           9.3707 |           0.1543 |
[32m[20230207 15:31:51 @agent_ppo2.py:192][0m |          -0.0070 |           3.7538 |           0.1541 |
[32m[20230207 15:31:51 @agent_ppo2.py:192][0m |          -0.0090 |           3.3887 |           0.1541 |
[32m[20230207 15:31:51 @agent_ppo2.py:192][0m |          -0.0107 |           3.1396 |           0.1542 |
[32m[20230207 15:31:51 @agent_ppo2.py:192][0m |          -0.0117 |           2.9895 |           0.1541 |
[32m[20230207 15:31:51 @agent_ppo2.py:192][0m |          -0.0097 |           2.9444 |           0.1540 |
[32m[20230207 15:31:51 @agent_ppo2.py:192][0m |          -0.0121 |           2.7283 |           0.1540 |
[32m[20230207 15:31:51 @agent_ppo2.py:192][0m |          -0.0141 |           2.6626 |           0.1539 |
[32m[20230207 15:31:51 @agent_ppo2.py:192][0m |          -0.0127 |           2.5211 |           0.1538 |
[32m[20230207 15:31:51 @agent_ppo2.py:192][0m |          -0.0148 |           2.4440 |           0.1538 |
[32m[20230207 15:31:51 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:31:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: 2.19
[32m[20230207 15:31:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 65.73
[32m[20230207 15:31:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -44.14
[32m[20230207 15:31:52 @agent_ppo2.py:150][0m Total time:      31.18 min
[32m[20230207 15:31:52 @agent_ppo2.py:152][0m 1644544 total steps have happened
[32m[20230207 15:31:52 @agent_ppo2.py:128][0m #------------------------ Iteration 803 --------------------------#
[32m[20230207 15:31:52 @agent_ppo2.py:134][0m Sampling time: 0.66 s by 1 slaves
[32m[20230207 15:31:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:53 @agent_ppo2.py:192][0m |          -0.0042 |          24.3277 |           0.1562 |
[32m[20230207 15:31:53 @agent_ppo2.py:192][0m |          -0.0030 |          15.7073 |           0.1561 |
[32m[20230207 15:31:53 @agent_ppo2.py:192][0m |          -0.0089 |          12.2951 |           0.1561 |
[32m[20230207 15:31:53 @agent_ppo2.py:192][0m |          -0.0081 |          10.3775 |           0.1560 |
[32m[20230207 15:31:53 @agent_ppo2.py:192][0m |          -0.0015 |           9.8189 |           0.1561 |
[32m[20230207 15:31:53 @agent_ppo2.py:192][0m |          -0.0117 |           8.9219 |           0.1560 |
[32m[20230207 15:31:53 @agent_ppo2.py:192][0m |          -0.0077 |           8.5373 |           0.1560 |
[32m[20230207 15:31:53 @agent_ppo2.py:192][0m |          -0.0061 |           9.3689 |           0.1560 |
[32m[20230207 15:31:53 @agent_ppo2.py:192][0m |          -0.0146 |           7.5487 |           0.1562 |
[32m[20230207 15:31:53 @agent_ppo2.py:192][0m |          -0.0121 |           7.1361 |           0.1561 |
[32m[20230207 15:31:53 @agent_ppo2.py:137][0m Policy update time: 0.69 s
[32m[20230207 15:31:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: -30.68
[32m[20230207 15:31:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 48.29
[32m[20230207 15:31:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -33.80
[32m[20230207 15:31:54 @agent_ppo2.py:150][0m Total time:      31.21 min
[32m[20230207 15:31:54 @agent_ppo2.py:152][0m 1646592 total steps have happened
[32m[20230207 15:31:54 @agent_ppo2.py:128][0m #------------------------ Iteration 804 --------------------------#
[32m[20230207 15:31:55 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:31:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:55 @agent_ppo2.py:192][0m |           0.0100 |           3.0347 |           0.1554 |
[32m[20230207 15:31:55 @agent_ppo2.py:192][0m |          -0.0148 |           2.1681 |           0.1552 |
[32m[20230207 15:31:55 @agent_ppo2.py:192][0m |          -0.0060 |           1.9630 |           0.1550 |
[32m[20230207 15:31:55 @agent_ppo2.py:192][0m |           0.0015 |           1.8490 |           0.1550 |
[32m[20230207 15:31:55 @agent_ppo2.py:192][0m |          -0.0053 |           1.7767 |           0.1549 |
[32m[20230207 15:31:55 @agent_ppo2.py:192][0m |          -0.0180 |           1.7312 |           0.1549 |
[32m[20230207 15:31:55 @agent_ppo2.py:192][0m |          -0.0022 |           1.7135 |           0.1549 |
[32m[20230207 15:31:55 @agent_ppo2.py:192][0m |          -0.0119 |           1.6408 |           0.1549 |
[32m[20230207 15:31:56 @agent_ppo2.py:192][0m |           0.0123 |           1.6241 |           0.1548 |
[32m[20230207 15:31:56 @agent_ppo2.py:192][0m |          -0.0009 |           1.5915 |           0.1551 |
[32m[20230207 15:31:56 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:31:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: 57.20
[32m[20230207 15:31:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 61.09
[32m[20230207 15:31:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 21.10
[32m[20230207 15:31:56 @agent_ppo2.py:150][0m Total time:      31.25 min
[32m[20230207 15:31:56 @agent_ppo2.py:152][0m 1648640 total steps have happened
[32m[20230207 15:31:56 @agent_ppo2.py:128][0m #------------------------ Iteration 805 --------------------------#
[32m[20230207 15:31:57 @agent_ppo2.py:134][0m Sampling time: 0.75 s by 1 slaves
[32m[20230207 15:31:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:31:57 @agent_ppo2.py:192][0m |          -0.0013 |          26.8755 |           0.1637 |
[32m[20230207 15:31:57 @agent_ppo2.py:192][0m |          -0.0075 |          10.2802 |           0.1636 |
[32m[20230207 15:31:57 @agent_ppo2.py:192][0m |          -0.0114 |           8.2926 |           0.1635 |
[32m[20230207 15:31:57 @agent_ppo2.py:192][0m |          -0.0128 |           7.2585 |           0.1635 |
[32m[20230207 15:31:58 @agent_ppo2.py:192][0m |          -0.0137 |           6.8367 |           0.1635 |
[32m[20230207 15:31:58 @agent_ppo2.py:192][0m |          -0.0159 |           6.4388 |           0.1635 |
[32m[20230207 15:31:58 @agent_ppo2.py:192][0m |          -0.0156 |           6.3094 |           0.1637 |
[32m[20230207 15:31:58 @agent_ppo2.py:192][0m |          -0.0170 |           5.9061 |           0.1635 |
[32m[20230207 15:31:58 @agent_ppo2.py:192][0m |          -0.0195 |           5.7696 |           0.1636 |
[32m[20230207 15:31:58 @agent_ppo2.py:192][0m |          -0.0192 |           5.5562 |           0.1636 |
[32m[20230207 15:31:58 @agent_ppo2.py:137][0m Policy update time: 0.75 s
[32m[20230207 15:31:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: -77.99
[32m[20230207 15:31:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 58.39
[32m[20230207 15:31:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -4.50
[32m[20230207 15:31:59 @agent_ppo2.py:150][0m Total time:      31.29 min
[32m[20230207 15:31:59 @agent_ppo2.py:152][0m 1650688 total steps have happened
[32m[20230207 15:31:59 @agent_ppo2.py:128][0m #------------------------ Iteration 806 --------------------------#
[32m[20230207 15:32:00 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:32:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:00 @agent_ppo2.py:192][0m |           0.0020 |           6.9790 |           0.1611 |
[32m[20230207 15:32:00 @agent_ppo2.py:192][0m |          -0.0045 |           3.7416 |           0.1614 |
[32m[20230207 15:32:00 @agent_ppo2.py:192][0m |          -0.0062 |           3.0225 |           0.1612 |
[32m[20230207 15:32:00 @agent_ppo2.py:192][0m |          -0.0081 |           2.7077 |           0.1610 |
[32m[20230207 15:32:00 @agent_ppo2.py:192][0m |          -0.0021 |           2.4776 |           0.1612 |
[32m[20230207 15:32:00 @agent_ppo2.py:192][0m |          -0.0078 |           2.2988 |           0.1610 |
[32m[20230207 15:32:00 @agent_ppo2.py:192][0m |          -0.0090 |           2.1819 |           0.1606 |
[32m[20230207 15:32:00 @agent_ppo2.py:192][0m |          -0.0086 |           2.0129 |           0.1608 |
[32m[20230207 15:32:00 @agent_ppo2.py:192][0m |          -0.0130 |           1.9562 |           0.1609 |
[32m[20230207 15:32:00 @agent_ppo2.py:192][0m |          -0.0089 |           1.9075 |           0.1606 |
[32m[20230207 15:32:00 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:32:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -3.07
[32m[20230207 15:32:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 111.60
[32m[20230207 15:32:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 100.39
[32m[20230207 15:32:01 @agent_ppo2.py:150][0m Total time:      31.33 min
[32m[20230207 15:32:01 @agent_ppo2.py:152][0m 1652736 total steps have happened
[32m[20230207 15:32:01 @agent_ppo2.py:128][0m #------------------------ Iteration 807 --------------------------#
[32m[20230207 15:32:02 @agent_ppo2.py:134][0m Sampling time: 0.61 s by 1 slaves
[32m[20230207 15:32:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:02 @agent_ppo2.py:192][0m |          -0.0021 |          22.3694 |           0.1594 |
[32m[20230207 15:32:02 @agent_ppo2.py:192][0m |          -0.0026 |          12.9230 |           0.1592 |
[32m[20230207 15:32:02 @agent_ppo2.py:192][0m |          -0.0045 |           9.4791 |           0.1593 |
[32m[20230207 15:32:02 @agent_ppo2.py:192][0m |          -0.0064 |           7.6044 |           0.1592 |
[32m[20230207 15:32:02 @agent_ppo2.py:192][0m |          -0.0097 |           6.4974 |           0.1593 |
[32m[20230207 15:32:02 @agent_ppo2.py:192][0m |          -0.0081 |           6.7048 |           0.1592 |
[32m[20230207 15:32:02 @agent_ppo2.py:192][0m |          -0.0097 |           5.6512 |           0.1593 |
[32m[20230207 15:32:02 @agent_ppo2.py:192][0m |          -0.0117 |           5.0858 |           0.1592 |
[32m[20230207 15:32:02 @agent_ppo2.py:192][0m |          -0.0104 |           4.7909 |           0.1594 |
[32m[20230207 15:32:02 @agent_ppo2.py:192][0m |          -0.0114 |           4.6730 |           0.1592 |
[32m[20230207 15:32:02 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:32:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -13.40
[32m[20230207 15:32:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 46.84
[32m[20230207 15:32:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 84.98
[32m[20230207 15:32:03 @agent_ppo2.py:150][0m Total time:      31.37 min
[32m[20230207 15:32:03 @agent_ppo2.py:152][0m 1654784 total steps have happened
[32m[20230207 15:32:03 @agent_ppo2.py:128][0m #------------------------ Iteration 808 --------------------------#
[32m[20230207 15:32:04 @agent_ppo2.py:134][0m Sampling time: 0.51 s by 1 slaves
[32m[20230207 15:32:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:04 @agent_ppo2.py:192][0m |          -0.0018 |          47.3469 |           0.1582 |
[32m[20230207 15:32:04 @agent_ppo2.py:192][0m |          -0.0232 |          23.3408 |           0.1584 |
[32m[20230207 15:32:04 @agent_ppo2.py:192][0m |          -0.0041 |          18.1638 |           0.1582 |
[32m[20230207 15:32:04 @agent_ppo2.py:192][0m |          -0.0236 |          15.8432 |           0.1584 |
[32m[20230207 15:32:04 @agent_ppo2.py:192][0m |          -0.0153 |          14.5535 |           0.1583 |
[32m[20230207 15:32:04 @agent_ppo2.py:192][0m |           0.0050 |          12.6954 |           0.1582 |
[32m[20230207 15:32:04 @agent_ppo2.py:192][0m |          -0.0258 |          12.2604 |           0.1581 |
[32m[20230207 15:32:04 @agent_ppo2.py:192][0m |          -0.0199 |          11.2497 |           0.1579 |
[32m[20230207 15:32:04 @agent_ppo2.py:192][0m |          -0.0207 |          10.3191 |           0.1579 |
[32m[20230207 15:32:04 @agent_ppo2.py:192][0m |          -0.0083 |          10.1692 |           0.1579 |
[32m[20230207 15:32:04 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:32:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -28.36
[32m[20230207 15:32:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 120.15
[32m[20230207 15:32:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -13.27
[32m[20230207 15:32:05 @agent_ppo2.py:150][0m Total time:      31.40 min
[32m[20230207 15:32:05 @agent_ppo2.py:152][0m 1656832 total steps have happened
[32m[20230207 15:32:05 @agent_ppo2.py:128][0m #------------------------ Iteration 809 --------------------------#
[32m[20230207 15:32:06 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:32:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:06 @agent_ppo2.py:192][0m |          -0.0159 |          30.0621 |           0.1616 |
[32m[20230207 15:32:06 @agent_ppo2.py:192][0m |           0.0035 |          20.1899 |           0.1616 |
[32m[20230207 15:32:06 @agent_ppo2.py:192][0m |          -0.0001 |          18.3649 |           0.1616 |
[32m[20230207 15:32:06 @agent_ppo2.py:192][0m |          -0.0059 |          17.7520 |           0.1617 |
[32m[20230207 15:32:06 @agent_ppo2.py:192][0m |          -0.0108 |          17.1529 |           0.1617 |
[32m[20230207 15:32:06 @agent_ppo2.py:192][0m |           0.0036 |          16.9278 |           0.1619 |
[32m[20230207 15:32:06 @agent_ppo2.py:192][0m |          -0.0057 |          16.1640 |           0.1617 |
[32m[20230207 15:32:06 @agent_ppo2.py:192][0m |          -0.0117 |          16.1630 |           0.1618 |
[32m[20230207 15:32:07 @agent_ppo2.py:192][0m |          -0.0142 |          16.2034 |           0.1619 |
[32m[20230207 15:32:07 @agent_ppo2.py:192][0m |          -0.0071 |          15.4612 |           0.1617 |
[32m[20230207 15:32:07 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:32:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: 50.02
[32m[20230207 15:32:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 62.99
[32m[20230207 15:32:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 70.70
[32m[20230207 15:32:07 @agent_ppo2.py:150][0m Total time:      31.44 min
[32m[20230207 15:32:07 @agent_ppo2.py:152][0m 1658880 total steps have happened
[32m[20230207 15:32:07 @agent_ppo2.py:128][0m #------------------------ Iteration 810 --------------------------#
[32m[20230207 15:32:08 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:32:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:08 @agent_ppo2.py:192][0m |           0.0071 |          12.0226 |           0.1582 |
[32m[20230207 15:32:08 @agent_ppo2.py:192][0m |           0.0104 |           8.3742 |           0.1583 |
[32m[20230207 15:32:08 @agent_ppo2.py:192][0m |          -0.0054 |           7.3455 |           0.1578 |
[32m[20230207 15:32:09 @agent_ppo2.py:192][0m |           0.0318 |           7.8900 |           0.1580 |
[32m[20230207 15:32:09 @agent_ppo2.py:192][0m |          -0.0336 |           6.9448 |           0.1579 |
[32m[20230207 15:32:09 @agent_ppo2.py:192][0m |          -0.0072 |           6.2495 |           0.1579 |
[32m[20230207 15:32:09 @agent_ppo2.py:192][0m |          -0.0051 |           6.0265 |           0.1578 |
[32m[20230207 15:32:09 @agent_ppo2.py:192][0m |          -0.0064 |           5.8057 |           0.1578 |
[32m[20230207 15:32:09 @agent_ppo2.py:192][0m |          -0.0074 |           5.5980 |           0.1578 |
[32m[20230207 15:32:09 @agent_ppo2.py:192][0m |          -0.0029 |           5.4388 |           0.1579 |
[32m[20230207 15:32:09 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:32:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: 75.32
[32m[20230207 15:32:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 95.63
[32m[20230207 15:32:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 3.24
[32m[20230207 15:32:10 @agent_ppo2.py:150][0m Total time:      31.48 min
[32m[20230207 15:32:10 @agent_ppo2.py:152][0m 1660928 total steps have happened
[32m[20230207 15:32:10 @agent_ppo2.py:128][0m #------------------------ Iteration 811 --------------------------#
[32m[20230207 15:32:11 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:32:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:11 @agent_ppo2.py:192][0m |           0.0014 |          11.5000 |           0.1597 |
[32m[20230207 15:32:11 @agent_ppo2.py:192][0m |           0.0044 |           7.4455 |           0.1596 |
[32m[20230207 15:32:11 @agent_ppo2.py:192][0m |           0.0070 |           6.5507 |           0.1595 |
[32m[20230207 15:32:11 @agent_ppo2.py:192][0m |           0.0101 |           6.1781 |           0.1595 |
[32m[20230207 15:32:11 @agent_ppo2.py:192][0m |          -0.0318 |           5.8023 |           0.1596 |
[32m[20230207 15:32:11 @agent_ppo2.py:192][0m |          -0.0021 |           5.5746 |           0.1595 |
[32m[20230207 15:32:11 @agent_ppo2.py:192][0m |          -0.0059 |           5.3059 |           0.1594 |
[32m[20230207 15:32:11 @agent_ppo2.py:192][0m |          -0.0016 |           5.1255 |           0.1595 |
[32m[20230207 15:32:11 @agent_ppo2.py:192][0m |          -0.0048 |           4.9583 |           0.1596 |
[32m[20230207 15:32:12 @agent_ppo2.py:192][0m |          -0.0133 |           4.8340 |           0.1595 |
[32m[20230207 15:32:12 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:32:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: 74.99
[32m[20230207 15:32:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 84.57
[32m[20230207 15:32:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -55.44
[32m[20230207 15:32:12 @agent_ppo2.py:150][0m Total time:      31.52 min
[32m[20230207 15:32:12 @agent_ppo2.py:152][0m 1662976 total steps have happened
[32m[20230207 15:32:12 @agent_ppo2.py:128][0m #------------------------ Iteration 812 --------------------------#
[32m[20230207 15:32:13 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:32:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:13 @agent_ppo2.py:192][0m |          -0.0052 |           2.2586 |           0.1599 |
[32m[20230207 15:32:13 @agent_ppo2.py:192][0m |          -0.0099 |           1.8709 |           0.1598 |
[32m[20230207 15:32:13 @agent_ppo2.py:192][0m |           0.0007 |           1.7466 |           0.1599 |
[32m[20230207 15:32:13 @agent_ppo2.py:192][0m |          -0.0054 |           1.6914 |           0.1598 |
[32m[20230207 15:32:13 @agent_ppo2.py:192][0m |          -0.0042 |           1.6458 |           0.1597 |
[32m[20230207 15:32:13 @agent_ppo2.py:192][0m |          -0.0120 |           1.6255 |           0.1598 |
[32m[20230207 15:32:14 @agent_ppo2.py:192][0m |          -0.0324 |           1.6181 |           0.1599 |
[32m[20230207 15:32:14 @agent_ppo2.py:192][0m |          -0.0031 |           1.5716 |           0.1600 |
[32m[20230207 15:32:14 @agent_ppo2.py:192][0m |           0.0570 |           2.1756 |           0.1599 |
[32m[20230207 15:32:14 @agent_ppo2.py:192][0m |          -0.0119 |           1.5547 |           0.1596 |
[32m[20230207 15:32:14 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:32:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: 47.04
[32m[20230207 15:32:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 57.67
[32m[20230207 15:32:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 92.30
[32m[20230207 15:32:15 @agent_ppo2.py:150][0m Total time:      31.56 min
[32m[20230207 15:32:15 @agent_ppo2.py:152][0m 1665024 total steps have happened
[32m[20230207 15:32:15 @agent_ppo2.py:128][0m #------------------------ Iteration 813 --------------------------#
[32m[20230207 15:32:15 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230207 15:32:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:15 @agent_ppo2.py:192][0m |           0.0012 |          23.7639 |           0.1593 |
[32m[20230207 15:32:15 @agent_ppo2.py:192][0m |          -0.0053 |          10.3490 |           0.1593 |
[32m[20230207 15:32:15 @agent_ppo2.py:192][0m |          -0.0072 |           8.4574 |           0.1593 |
[32m[20230207 15:32:15 @agent_ppo2.py:192][0m |          -0.0087 |           7.5941 |           0.1593 |
[32m[20230207 15:32:16 @agent_ppo2.py:192][0m |          -0.0103 |           7.1496 |           0.1593 |
[32m[20230207 15:32:16 @agent_ppo2.py:192][0m |          -0.0116 |           6.8759 |           0.1595 |
[32m[20230207 15:32:16 @agent_ppo2.py:192][0m |          -0.0112 |           6.6253 |           0.1596 |
[32m[20230207 15:32:16 @agent_ppo2.py:192][0m |          -0.0125 |           5.9862 |           0.1596 |
[32m[20230207 15:32:16 @agent_ppo2.py:192][0m |          -0.0131 |           5.9123 |           0.1596 |
[32m[20230207 15:32:16 @agent_ppo2.py:192][0m |          -0.0138 |           5.6048 |           0.1596 |
[32m[20230207 15:32:16 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 15:32:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: -61.62
[32m[20230207 15:32:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 45.30
[32m[20230207 15:32:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -49.40
[32m[20230207 15:32:17 @agent_ppo2.py:150][0m Total time:      31.59 min
[32m[20230207 15:32:17 @agent_ppo2.py:152][0m 1667072 total steps have happened
[32m[20230207 15:32:17 @agent_ppo2.py:128][0m #------------------------ Iteration 814 --------------------------#
[32m[20230207 15:32:17 @agent_ppo2.py:134][0m Sampling time: 0.73 s by 1 slaves
[32m[20230207 15:32:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:17 @agent_ppo2.py:192][0m |           0.0002 |          13.4979 |           0.1615 |
[32m[20230207 15:32:18 @agent_ppo2.py:192][0m |          -0.0059 |           7.5681 |           0.1613 |
[32m[20230207 15:32:18 @agent_ppo2.py:192][0m |          -0.0088 |           6.0118 |           0.1612 |
[32m[20230207 15:32:18 @agent_ppo2.py:192][0m |          -0.0101 |           5.2199 |           0.1612 |
[32m[20230207 15:32:18 @agent_ppo2.py:192][0m |          -0.0111 |           4.5639 |           0.1610 |
[32m[20230207 15:32:18 @agent_ppo2.py:192][0m |          -0.0117 |           4.2133 |           0.1610 |
[32m[20230207 15:32:18 @agent_ppo2.py:192][0m |          -0.0133 |           3.9221 |           0.1609 |
[32m[20230207 15:32:18 @agent_ppo2.py:192][0m |          -0.0145 |           3.8377 |           0.1609 |
[32m[20230207 15:32:18 @agent_ppo2.py:192][0m |          -0.0148 |           3.5516 |           0.1609 |
[32m[20230207 15:32:18 @agent_ppo2.py:192][0m |          -0.0152 |           3.4228 |           0.1609 |
[32m[20230207 15:32:18 @agent_ppo2.py:137][0m Policy update time: 0.76 s
[32m[20230207 15:32:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: -70.43
[32m[20230207 15:32:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 38.46
[32m[20230207 15:32:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 17.81
[32m[20230207 15:32:19 @agent_ppo2.py:150][0m Total time:      31.62 min
[32m[20230207 15:32:19 @agent_ppo2.py:152][0m 1669120 total steps have happened
[32m[20230207 15:32:19 @agent_ppo2.py:128][0m #------------------------ Iteration 815 --------------------------#
[32m[20230207 15:32:19 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:32:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:20 @agent_ppo2.py:192][0m |           0.0009 |           2.1629 |           0.1613 |
[32m[20230207 15:32:20 @agent_ppo2.py:192][0m |           0.0022 |           1.7621 |           0.1611 |
[32m[20230207 15:32:20 @agent_ppo2.py:192][0m |          -0.0018 |           1.6501 |           0.1609 |
[32m[20230207 15:32:20 @agent_ppo2.py:192][0m |           0.0040 |           1.6065 |           0.1608 |
[32m[20230207 15:32:20 @agent_ppo2.py:192][0m |          -0.0254 |           1.5869 |           0.1607 |
[32m[20230207 15:32:20 @agent_ppo2.py:192][0m |           0.0110 |           1.5693 |           0.1609 |
[32m[20230207 15:32:20 @agent_ppo2.py:192][0m |          -0.0255 |           1.4780 |           0.1608 |
[32m[20230207 15:32:20 @agent_ppo2.py:192][0m |          -0.0193 |           1.4481 |           0.1606 |
[32m[20230207 15:32:20 @agent_ppo2.py:192][0m |          -0.0106 |           1.4300 |           0.1608 |
[32m[20230207 15:32:20 @agent_ppo2.py:192][0m |          -0.0313 |           1.4108 |           0.1606 |
[32m[20230207 15:32:20 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:32:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: 81.45
[32m[20230207 15:32:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 108.09
[32m[20230207 15:32:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 32.58
[32m[20230207 15:32:21 @agent_ppo2.py:150][0m Total time:      31.67 min
[32m[20230207 15:32:21 @agent_ppo2.py:152][0m 1671168 total steps have happened
[32m[20230207 15:32:21 @agent_ppo2.py:128][0m #------------------------ Iteration 816 --------------------------#
[32m[20230207 15:32:22 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230207 15:32:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:22 @agent_ppo2.py:192][0m |           0.0035 |          17.8036 |           0.1606 |
[32m[20230207 15:32:22 @agent_ppo2.py:192][0m |          -0.0068 |           5.4347 |           0.1603 |
[32m[20230207 15:32:22 @agent_ppo2.py:192][0m |          -0.0079 |           4.4631 |           0.1601 |
[32m[20230207 15:32:22 @agent_ppo2.py:192][0m |          -0.0128 |           3.9302 |           0.1600 |
[32m[20230207 15:32:22 @agent_ppo2.py:192][0m |          -0.0115 |           3.5926 |           0.1599 |
[32m[20230207 15:32:22 @agent_ppo2.py:192][0m |          -0.0117 |           3.4076 |           0.1599 |
[32m[20230207 15:32:22 @agent_ppo2.py:192][0m |          -0.0154 |           3.2888 |           0.1596 |
[32m[20230207 15:32:22 @agent_ppo2.py:192][0m |          -0.0150 |           3.1520 |           0.1595 |
[32m[20230207 15:32:22 @agent_ppo2.py:192][0m |          -0.0162 |           3.0011 |           0.1595 |
[32m[20230207 15:32:22 @agent_ppo2.py:192][0m |          -0.0155 |           2.8912 |           0.1594 |
[32m[20230207 15:32:22 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:32:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -21.86
[32m[20230207 15:32:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 111.66
[32m[20230207 15:32:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -32.78
[32m[20230207 15:32:23 @agent_ppo2.py:150][0m Total time:      31.70 min
[32m[20230207 15:32:23 @agent_ppo2.py:152][0m 1673216 total steps have happened
[32m[20230207 15:32:23 @agent_ppo2.py:128][0m #------------------------ Iteration 817 --------------------------#
[32m[20230207 15:32:24 @agent_ppo2.py:134][0m Sampling time: 0.75 s by 1 slaves
[32m[20230207 15:32:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:24 @agent_ppo2.py:192][0m |           0.0023 |          23.3774 |           0.1689 |
[32m[20230207 15:32:24 @agent_ppo2.py:192][0m |          -0.0048 |          11.9009 |           0.1686 |
[32m[20230207 15:32:24 @agent_ppo2.py:192][0m |          -0.0082 |          10.1831 |           0.1686 |
[32m[20230207 15:32:24 @agent_ppo2.py:192][0m |          -0.0092 |           9.0515 |           0.1683 |
[32m[20230207 15:32:24 @agent_ppo2.py:192][0m |          -0.0108 |           8.3976 |           0.1683 |
[32m[20230207 15:32:24 @agent_ppo2.py:192][0m |          -0.0125 |           7.8277 |           0.1680 |
[32m[20230207 15:32:24 @agent_ppo2.py:192][0m |          -0.0123 |           7.3879 |           0.1683 |
[32m[20230207 15:32:24 @agent_ppo2.py:192][0m |          -0.0137 |           7.0446 |           0.1681 |
[32m[20230207 15:32:24 @agent_ppo2.py:192][0m |          -0.0147 |           6.6187 |           0.1680 |
[32m[20230207 15:32:25 @agent_ppo2.py:192][0m |          -0.0157 |           6.4055 |           0.1679 |
[32m[20230207 15:32:25 @agent_ppo2.py:137][0m Policy update time: 0.78 s
[32m[20230207 15:32:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -58.26
[32m[20230207 15:32:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 26.24
[32m[20230207 15:32:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -13.22
[32m[20230207 15:32:25 @agent_ppo2.py:150][0m Total time:      31.74 min
[32m[20230207 15:32:25 @agent_ppo2.py:152][0m 1675264 total steps have happened
[32m[20230207 15:32:25 @agent_ppo2.py:128][0m #------------------------ Iteration 818 --------------------------#
[32m[20230207 15:32:26 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230207 15:32:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:26 @agent_ppo2.py:192][0m |          -0.0016 |          21.8905 |           0.1539 |
[32m[20230207 15:32:26 @agent_ppo2.py:192][0m |           0.0000 |           9.6113 |           0.1536 |
[32m[20230207 15:32:26 @agent_ppo2.py:192][0m |          -0.0143 |           8.1094 |           0.1534 |
[32m[20230207 15:32:26 @agent_ppo2.py:192][0m |           0.0033 |           6.2416 |           0.1532 |
[32m[20230207 15:32:26 @agent_ppo2.py:192][0m |          -0.0114 |           5.4592 |           0.1532 |
[32m[20230207 15:32:26 @agent_ppo2.py:192][0m |          -0.0169 |           4.8981 |           0.1531 |
[32m[20230207 15:32:26 @agent_ppo2.py:192][0m |          -0.0109 |           4.7918 |           0.1530 |
[32m[20230207 15:32:27 @agent_ppo2.py:192][0m |          -0.0142 |           4.3463 |           0.1530 |
[32m[20230207 15:32:27 @agent_ppo2.py:192][0m |          -0.0204 |           4.1602 |           0.1531 |
[32m[20230207 15:32:27 @agent_ppo2.py:192][0m |          -0.0215 |           3.9135 |           0.1530 |
[32m[20230207 15:32:27 @agent_ppo2.py:137][0m Policy update time: 0.64 s
[32m[20230207 15:32:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -53.47
[32m[20230207 15:32:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 97.53
[32m[20230207 15:32:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -74.19
[32m[20230207 15:32:27 @agent_ppo2.py:150][0m Total time:      31.77 min
[32m[20230207 15:32:27 @agent_ppo2.py:152][0m 1677312 total steps have happened
[32m[20230207 15:32:27 @agent_ppo2.py:128][0m #------------------------ Iteration 819 --------------------------#
[32m[20230207 15:32:28 @agent_ppo2.py:134][0m Sampling time: 0.74 s by 1 slaves
[32m[20230207 15:32:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:28 @agent_ppo2.py:192][0m |          -0.0043 |          11.4887 |           0.1606 |
[32m[20230207 15:32:28 @agent_ppo2.py:192][0m |          -0.0078 |           4.5334 |           0.1602 |
[32m[20230207 15:32:28 @agent_ppo2.py:192][0m |          -0.0115 |           3.8527 |           0.1600 |
[32m[20230207 15:32:28 @agent_ppo2.py:192][0m |          -0.0117 |           3.5388 |           0.1599 |
[32m[20230207 15:32:28 @agent_ppo2.py:192][0m |          -0.0124 |           3.3288 |           0.1600 |
[32m[20230207 15:32:28 @agent_ppo2.py:192][0m |          -0.0141 |           3.1711 |           0.1600 |
[32m[20230207 15:32:29 @agent_ppo2.py:192][0m |          -0.0158 |           3.0438 |           0.1599 |
[32m[20230207 15:32:29 @agent_ppo2.py:192][0m |          -0.0122 |           2.9538 |           0.1599 |
[32m[20230207 15:32:29 @agent_ppo2.py:192][0m |          -0.0169 |           2.8436 |           0.1597 |
[32m[20230207 15:32:29 @agent_ppo2.py:192][0m |          -0.0170 |           2.7726 |           0.1597 |
[32m[20230207 15:32:29 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:32:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -36.73
[32m[20230207 15:32:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 8.97
[32m[20230207 15:32:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 29.47
[32m[20230207 15:32:30 @agent_ppo2.py:150][0m Total time:      31.81 min
[32m[20230207 15:32:30 @agent_ppo2.py:152][0m 1679360 total steps have happened
[32m[20230207 15:32:30 @agent_ppo2.py:128][0m #------------------------ Iteration 820 --------------------------#
[32m[20230207 15:32:30 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:32:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:31 @agent_ppo2.py:192][0m |           0.0010 |          13.0884 |           0.1590 |
[32m[20230207 15:32:31 @agent_ppo2.py:192][0m |          -0.0023 |           7.7284 |           0.1589 |
[32m[20230207 15:32:31 @agent_ppo2.py:192][0m |          -0.0048 |           6.5378 |           0.1588 |
[32m[20230207 15:32:31 @agent_ppo2.py:192][0m |          -0.0068 |           5.9017 |           0.1588 |
[32m[20230207 15:32:31 @agent_ppo2.py:192][0m |          -0.0070 |           5.5511 |           0.1588 |
[32m[20230207 15:32:31 @agent_ppo2.py:192][0m |          -0.0094 |           5.2332 |           0.1586 |
[32m[20230207 15:32:31 @agent_ppo2.py:192][0m |          -0.0096 |           5.0141 |           0.1586 |
[32m[20230207 15:32:31 @agent_ppo2.py:192][0m |          -0.0107 |           4.8053 |           0.1587 |
[32m[20230207 15:32:31 @agent_ppo2.py:192][0m |          -0.0115 |           4.4482 |           0.1588 |
[32m[20230207 15:32:31 @agent_ppo2.py:192][0m |          -0.0116 |           4.2196 |           0.1588 |
[32m[20230207 15:32:31 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:32:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: 26.48
[32m[20230207 15:32:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 170.91
[32m[20230207 15:32:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 85.65
[32m[20230207 15:32:32 @agent_ppo2.py:150][0m Total time:      31.85 min
[32m[20230207 15:32:32 @agent_ppo2.py:152][0m 1681408 total steps have happened
[32m[20230207 15:32:32 @agent_ppo2.py:128][0m #------------------------ Iteration 821 --------------------------#
[32m[20230207 15:32:33 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:32:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:33 @agent_ppo2.py:192][0m |          -0.0093 |           2.2371 |           0.1590 |
[32m[20230207 15:32:33 @agent_ppo2.py:192][0m |          -0.0090 |           1.7455 |           0.1587 |
[32m[20230207 15:32:33 @agent_ppo2.py:192][0m |          -0.0180 |           1.6833 |           0.1585 |
[32m[20230207 15:32:33 @agent_ppo2.py:192][0m |          -0.0092 |           1.6507 |           0.1581 |
[32m[20230207 15:32:33 @agent_ppo2.py:192][0m |          -0.0294 |           1.6077 |           0.1580 |
[32m[20230207 15:32:33 @agent_ppo2.py:192][0m |          -0.0207 |           1.5785 |           0.1581 |
[32m[20230207 15:32:34 @agent_ppo2.py:192][0m |          -0.0097 |           1.5467 |           0.1581 |
[32m[20230207 15:32:34 @agent_ppo2.py:192][0m |          -0.0391 |           1.5639 |           0.1582 |
[32m[20230207 15:32:34 @agent_ppo2.py:192][0m |          -0.0270 |           1.5252 |           0.1579 |
[32m[20230207 15:32:34 @agent_ppo2.py:192][0m |          -0.0143 |           1.4987 |           0.1580 |
[32m[20230207 15:32:34 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:32:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: 117.59
[32m[20230207 15:32:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 121.42
[32m[20230207 15:32:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -96.99
[32m[20230207 15:32:34 @agent_ppo2.py:150][0m Total time:      31.88 min
[32m[20230207 15:32:34 @agent_ppo2.py:152][0m 1683456 total steps have happened
[32m[20230207 15:32:34 @agent_ppo2.py:128][0m #------------------------ Iteration 822 --------------------------#
[32m[20230207 15:32:35 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:32:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:35 @agent_ppo2.py:192][0m |          -0.0017 |           7.0165 |           0.1615 |
[32m[20230207 15:32:35 @agent_ppo2.py:192][0m |          -0.0069 |           4.6361 |           0.1612 |
[32m[20230207 15:32:35 @agent_ppo2.py:192][0m |          -0.0115 |           4.1754 |           0.1613 |
[32m[20230207 15:32:36 @agent_ppo2.py:192][0m |          -0.0117 |           3.9137 |           0.1612 |
[32m[20230207 15:32:36 @agent_ppo2.py:192][0m |          -0.0090 |           3.8116 |           0.1612 |
[32m[20230207 15:32:36 @agent_ppo2.py:192][0m |          -0.0122 |           3.6486 |           0.1611 |
[32m[20230207 15:32:36 @agent_ppo2.py:192][0m |          -0.0150 |           3.5432 |           0.1610 |
[32m[20230207 15:32:36 @agent_ppo2.py:192][0m |          -0.0156 |           3.4904 |           0.1608 |
[32m[20230207 15:32:36 @agent_ppo2.py:192][0m |          -0.0166 |           3.3741 |           0.1610 |
[32m[20230207 15:32:36 @agent_ppo2.py:192][0m |          -0.0162 |           3.4296 |           0.1609 |
[32m[20230207 15:32:36 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:32:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: -25.15
[32m[20230207 15:32:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 79.76
[32m[20230207 15:32:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 3.81
[32m[20230207 15:32:37 @agent_ppo2.py:150][0m Total time:      31.93 min
[32m[20230207 15:32:37 @agent_ppo2.py:152][0m 1685504 total steps have happened
[32m[20230207 15:32:37 @agent_ppo2.py:128][0m #------------------------ Iteration 823 --------------------------#
[32m[20230207 15:32:38 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:32:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:38 @agent_ppo2.py:192][0m |           0.0284 |           2.8130 |           0.1617 |
[32m[20230207 15:32:38 @agent_ppo2.py:192][0m |          -0.0114 |           1.8795 |           0.1613 |
[32m[20230207 15:32:38 @agent_ppo2.py:192][0m |          -0.0550 |           1.7443 |           0.1614 |
[32m[20230207 15:32:38 @agent_ppo2.py:192][0m |           0.0099 |           1.6660 |           0.1615 |
[32m[20230207 15:32:38 @agent_ppo2.py:192][0m |          -0.0175 |           1.6246 |           0.1616 |
[32m[20230207 15:32:38 @agent_ppo2.py:192][0m |          -0.0155 |           1.5777 |           0.1617 |
[32m[20230207 15:32:38 @agent_ppo2.py:192][0m |          -0.0075 |           1.5544 |           0.1615 |
[32m[20230207 15:32:38 @agent_ppo2.py:192][0m |          -0.0113 |           1.5441 |           0.1616 |
[32m[20230207 15:32:38 @agent_ppo2.py:192][0m |          -0.0068 |           1.5502 |           0.1615 |
[32m[20230207 15:32:39 @agent_ppo2.py:192][0m |          -0.0108 |           1.5095 |           0.1614 |
[32m[20230207 15:32:39 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:32:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: 60.54
[32m[20230207 15:32:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 116.77
[32m[20230207 15:32:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 12.55
[32m[20230207 15:32:39 @agent_ppo2.py:150][0m Total time:      31.96 min
[32m[20230207 15:32:39 @agent_ppo2.py:152][0m 1687552 total steps have happened
[32m[20230207 15:32:39 @agent_ppo2.py:128][0m #------------------------ Iteration 824 --------------------------#
[32m[20230207 15:32:40 @agent_ppo2.py:134][0m Sampling time: 0.91 s by 1 slaves
[32m[20230207 15:32:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:40 @agent_ppo2.py:192][0m |          -0.0011 |           6.8879 |           0.1681 |
[32m[20230207 15:32:40 @agent_ppo2.py:192][0m |          -0.0053 |           3.3036 |           0.1681 |
[32m[20230207 15:32:40 @agent_ppo2.py:192][0m |          -0.0072 |           2.8995 |           0.1681 |
[32m[20230207 15:32:40 @agent_ppo2.py:192][0m |          -0.0082 |           2.6755 |           0.1681 |
[32m[20230207 15:32:40 @agent_ppo2.py:192][0m |          -0.0086 |           2.5095 |           0.1682 |
[32m[20230207 15:32:41 @agent_ppo2.py:192][0m |          -0.0094 |           2.4025 |           0.1682 |
[32m[20230207 15:32:41 @agent_ppo2.py:192][0m |          -0.0110 |           2.2888 |           0.1682 |
[32m[20230207 15:32:41 @agent_ppo2.py:192][0m |          -0.0124 |           2.1454 |           0.1682 |
[32m[20230207 15:32:41 @agent_ppo2.py:192][0m |          -0.0130 |           2.1300 |           0.1681 |
[32m[20230207 15:32:41 @agent_ppo2.py:192][0m |          -0.0136 |           1.9784 |           0.1683 |
[32m[20230207 15:32:41 @agent_ppo2.py:137][0m Policy update time: 0.92 s
[32m[20230207 15:32:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: -8.45
[32m[20230207 15:32:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 93.69
[32m[20230207 15:32:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 9.25
[32m[20230207 15:32:42 @agent_ppo2.py:150][0m Total time:      32.01 min
[32m[20230207 15:32:42 @agent_ppo2.py:152][0m 1689600 total steps have happened
[32m[20230207 15:32:42 @agent_ppo2.py:128][0m #------------------------ Iteration 825 --------------------------#
[32m[20230207 15:32:43 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:32:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:43 @agent_ppo2.py:192][0m |           0.0187 |           6.3566 |           0.1629 |
[32m[20230207 15:32:43 @agent_ppo2.py:192][0m |          -0.0184 |           4.2457 |           0.1631 |
[32m[20230207 15:32:43 @agent_ppo2.py:192][0m |          -0.0020 |           3.9602 |           0.1630 |
[32m[20230207 15:32:43 @agent_ppo2.py:192][0m |          -0.0024 |           3.7014 |           0.1630 |
[32m[20230207 15:32:43 @agent_ppo2.py:192][0m |          -0.0277 |           3.5701 |           0.1631 |
[32m[20230207 15:32:43 @agent_ppo2.py:192][0m |          -0.0326 |           3.4990 |           0.1631 |
[32m[20230207 15:32:43 @agent_ppo2.py:192][0m |          -0.0017 |           3.8292 |           0.1631 |
[32m[20230207 15:32:43 @agent_ppo2.py:192][0m |           0.0154 |           3.4229 |           0.1623 |
[32m[20230207 15:32:43 @agent_ppo2.py:192][0m |          -0.0215 |           3.3441 |           0.1631 |
[32m[20230207 15:32:43 @agent_ppo2.py:192][0m |           0.0017 |           3.1756 |           0.1633 |
[32m[20230207 15:32:43 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:32:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: 115.25
[32m[20230207 15:32:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 121.31
[32m[20230207 15:32:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 105.71
[32m[20230207 15:32:44 @agent_ppo2.py:150][0m Total time:      32.05 min
[32m[20230207 15:32:44 @agent_ppo2.py:152][0m 1691648 total steps have happened
[32m[20230207 15:32:44 @agent_ppo2.py:128][0m #------------------------ Iteration 826 --------------------------#
[32m[20230207 15:32:45 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:32:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:45 @agent_ppo2.py:192][0m |          -0.0038 |          13.0703 |           0.1679 |
[32m[20230207 15:32:45 @agent_ppo2.py:192][0m |          -0.0054 |           9.3638 |           0.1678 |
[32m[20230207 15:32:45 @agent_ppo2.py:192][0m |          -0.0058 |           7.9543 |           0.1676 |
[32m[20230207 15:32:45 @agent_ppo2.py:192][0m |          -0.0073 |           7.0586 |           0.1676 |
[32m[20230207 15:32:45 @agent_ppo2.py:192][0m |          -0.0065 |           6.2280 |           0.1672 |
[32m[20230207 15:32:46 @agent_ppo2.py:192][0m |          -0.0091 |           5.5357 |           0.1673 |
[32m[20230207 15:32:46 @agent_ppo2.py:192][0m |          -0.0083 |           4.9657 |           0.1675 |
[32m[20230207 15:32:46 @agent_ppo2.py:192][0m |          -0.0085 |           4.6219 |           0.1674 |
[32m[20230207 15:32:46 @agent_ppo2.py:192][0m |          -0.0114 |           4.3067 |           0.1673 |
[32m[20230207 15:32:46 @agent_ppo2.py:192][0m |          -0.0092 |           4.0825 |           0.1674 |
[32m[20230207 15:32:46 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:32:47 @agent_ppo2.py:145][0m Average TRAINING episode reward: 23.59
[32m[20230207 15:32:47 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 116.92
[32m[20230207 15:32:47 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 62.79
[32m[20230207 15:32:47 @agent_ppo2.py:150][0m Total time:      32.09 min
[32m[20230207 15:32:47 @agent_ppo2.py:152][0m 1693696 total steps have happened
[32m[20230207 15:32:47 @agent_ppo2.py:128][0m #------------------------ Iteration 827 --------------------------#
[32m[20230207 15:32:47 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230207 15:32:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:47 @agent_ppo2.py:192][0m |           0.0061 |          29.3135 |           0.1632 |
[32m[20230207 15:32:47 @agent_ppo2.py:192][0m |          -0.0048 |          11.7067 |           0.1630 |
[32m[20230207 15:32:47 @agent_ppo2.py:192][0m |          -0.0100 |           8.5288 |           0.1627 |
[32m[20230207 15:32:48 @agent_ppo2.py:192][0m |          -0.0087 |           6.9361 |           0.1627 |
[32m[20230207 15:32:48 @agent_ppo2.py:192][0m |          -0.0066 |           6.1012 |           0.1628 |
[32m[20230207 15:32:48 @agent_ppo2.py:192][0m |          -0.0123 |           5.5041 |           0.1628 |
[32m[20230207 15:32:48 @agent_ppo2.py:192][0m |          -0.0138 |           5.1862 |           0.1626 |
[32m[20230207 15:32:48 @agent_ppo2.py:192][0m |          -0.0122 |           4.8353 |           0.1629 |
[32m[20230207 15:32:48 @agent_ppo2.py:192][0m |          -0.0187 |           4.5678 |           0.1628 |
[32m[20230207 15:32:48 @agent_ppo2.py:192][0m |          -0.0179 |           4.3858 |           0.1628 |
[32m[20230207 15:32:48 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:32:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: -26.24
[32m[20230207 15:32:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 104.55
[32m[20230207 15:32:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 143.04
[32m[20230207 15:32:49 @agent_ppo2.py:150][0m Total time:      32.12 min
[32m[20230207 15:32:49 @agent_ppo2.py:152][0m 1695744 total steps have happened
[32m[20230207 15:32:49 @agent_ppo2.py:128][0m #------------------------ Iteration 828 --------------------------#
[32m[20230207 15:32:49 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:32:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:49 @agent_ppo2.py:192][0m |           0.0054 |          10.5541 |           0.1629 |
[32m[20230207 15:32:49 @agent_ppo2.py:192][0m |           0.0002 |           5.8672 |           0.1627 |
[32m[20230207 15:32:49 @agent_ppo2.py:192][0m |          -0.0061 |           5.1125 |           0.1625 |
[32m[20230207 15:32:49 @agent_ppo2.py:192][0m |          -0.0080 |           4.6415 |           0.1624 |
[32m[20230207 15:32:50 @agent_ppo2.py:192][0m |          -0.0123 |           4.3881 |           0.1625 |
[32m[20230207 15:32:50 @agent_ppo2.py:192][0m |          -0.0100 |           4.2567 |           0.1624 |
[32m[20230207 15:32:50 @agent_ppo2.py:192][0m |          -0.0093 |           3.9799 |           0.1624 |
[32m[20230207 15:32:50 @agent_ppo2.py:192][0m |          -0.0137 |           3.8173 |           0.1623 |
[32m[20230207 15:32:50 @agent_ppo2.py:192][0m |          -0.0158 |           3.6100 |           0.1623 |
[32m[20230207 15:32:50 @agent_ppo2.py:192][0m |          -0.0176 |           3.4598 |           0.1623 |
[32m[20230207 15:32:50 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:32:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: 1.64
[32m[20230207 15:32:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 96.20
[32m[20230207 15:32:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 69.37
[32m[20230207 15:32:51 @agent_ppo2.py:150][0m Total time:      32.16 min
[32m[20230207 15:32:51 @agent_ppo2.py:152][0m 1697792 total steps have happened
[32m[20230207 15:32:51 @agent_ppo2.py:128][0m #------------------------ Iteration 829 --------------------------#
[32m[20230207 15:32:51 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:32:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:51 @agent_ppo2.py:192][0m |          -0.0132 |           4.2681 |           0.1592 |
[32m[20230207 15:32:52 @agent_ppo2.py:192][0m |          -0.0004 |           2.9587 |           0.1591 |
[32m[20230207 15:32:52 @agent_ppo2.py:192][0m |           0.0006 |           2.6820 |           0.1588 |
[32m[20230207 15:32:52 @agent_ppo2.py:192][0m |          -0.0246 |           2.5414 |           0.1590 |
[32m[20230207 15:32:52 @agent_ppo2.py:192][0m |           0.0006 |           2.4310 |           0.1590 |
[32m[20230207 15:32:52 @agent_ppo2.py:192][0m |          -0.0093 |           2.3663 |           0.1589 |
[32m[20230207 15:32:52 @agent_ppo2.py:192][0m |          -0.0133 |           2.3232 |           0.1590 |
[32m[20230207 15:32:52 @agent_ppo2.py:192][0m |          -0.0133 |           2.3551 |           0.1589 |
[32m[20230207 15:32:52 @agent_ppo2.py:192][0m |          -0.0001 |           2.4443 |           0.1585 |
[32m[20230207 15:32:52 @agent_ppo2.py:192][0m |           0.0049 |           2.2279 |           0.1592 |
[32m[20230207 15:32:52 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:32:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: 69.23
[32m[20230207 15:32:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 104.88
[32m[20230207 15:32:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -46.67
[32m[20230207 15:32:53 @agent_ppo2.py:150][0m Total time:      32.19 min
[32m[20230207 15:32:53 @agent_ppo2.py:152][0m 1699840 total steps have happened
[32m[20230207 15:32:53 @agent_ppo2.py:128][0m #------------------------ Iteration 830 --------------------------#
[32m[20230207 15:32:53 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:32:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:54 @agent_ppo2.py:192][0m |          -0.0036 |          10.3032 |           0.1593 |
[32m[20230207 15:32:54 @agent_ppo2.py:192][0m |          -0.0050 |           5.0383 |           0.1595 |
[32m[20230207 15:32:54 @agent_ppo2.py:192][0m |          -0.0071 |           3.9796 |           0.1594 |
[32m[20230207 15:32:54 @agent_ppo2.py:192][0m |          -0.0100 |           3.5451 |           0.1595 |
[32m[20230207 15:32:54 @agent_ppo2.py:192][0m |          -0.0142 |           3.0662 |           0.1595 |
[32m[20230207 15:32:54 @agent_ppo2.py:192][0m |           0.0244 |           2.8253 |           0.1595 |
[32m[20230207 15:32:54 @agent_ppo2.py:192][0m |           0.0016 |           2.8265 |           0.1597 |
[32m[20230207 15:32:54 @agent_ppo2.py:192][0m |           0.0011 |           2.5190 |           0.1597 |
[32m[20230207 15:32:54 @agent_ppo2.py:192][0m |          -0.0082 |           2.3533 |           0.1597 |
[32m[20230207 15:32:54 @agent_ppo2.py:192][0m |          -0.0151 |           2.1784 |           0.1597 |
[32m[20230207 15:32:54 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:32:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: 22.32
[32m[20230207 15:32:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 107.15
[32m[20230207 15:32:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 13.58
[32m[20230207 15:32:54 @agent_ppo2.py:150][0m Total time:      32.22 min
[32m[20230207 15:32:54 @agent_ppo2.py:152][0m 1701888 total steps have happened
[32m[20230207 15:32:54 @agent_ppo2.py:128][0m #------------------------ Iteration 831 --------------------------#
[32m[20230207 15:32:55 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:32:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:55 @agent_ppo2.py:192][0m |          -0.0017 |          14.4266 |           0.1613 |
[32m[20230207 15:32:55 @agent_ppo2.py:192][0m |          -0.0089 |           6.1975 |           0.1610 |
[32m[20230207 15:32:55 @agent_ppo2.py:192][0m |          -0.0111 |           4.5676 |           0.1608 |
[32m[20230207 15:32:55 @agent_ppo2.py:192][0m |          -0.0135 |           3.8442 |           0.1608 |
[32m[20230207 15:32:55 @agent_ppo2.py:192][0m |          -0.0146 |           3.3846 |           0.1607 |
[32m[20230207 15:32:56 @agent_ppo2.py:192][0m |          -0.0150 |           3.1068 |           0.1604 |
[32m[20230207 15:32:56 @agent_ppo2.py:192][0m |          -0.0162 |           2.8453 |           0.1605 |
[32m[20230207 15:32:56 @agent_ppo2.py:192][0m |          -0.0155 |           2.7217 |           0.1604 |
[32m[20230207 15:32:56 @agent_ppo2.py:192][0m |          -0.0173 |           2.6023 |           0.1603 |
[32m[20230207 15:32:56 @agent_ppo2.py:192][0m |          -0.0171 |           2.4577 |           0.1603 |
[32m[20230207 15:32:56 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230207 15:32:57 @agent_ppo2.py:145][0m Average TRAINING episode reward: -39.05
[32m[20230207 15:32:57 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 83.73
[32m[20230207 15:32:57 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 16.85
[32m[20230207 15:32:57 @agent_ppo2.py:150][0m Total time:      32.26 min
[32m[20230207 15:32:57 @agent_ppo2.py:152][0m 1703936 total steps have happened
[32m[20230207 15:32:57 @agent_ppo2.py:128][0m #------------------------ Iteration 832 --------------------------#
[32m[20230207 15:32:57 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:32:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:32:57 @agent_ppo2.py:192][0m |          -0.0020 |           9.6700 |           0.1630 |
[32m[20230207 15:32:58 @agent_ppo2.py:192][0m |          -0.0012 |           5.3015 |           0.1627 |
[32m[20230207 15:32:58 @agent_ppo2.py:192][0m |          -0.0042 |           4.1189 |           0.1626 |
[32m[20230207 15:32:58 @agent_ppo2.py:192][0m |          -0.0102 |           3.5789 |           0.1624 |
[32m[20230207 15:32:58 @agent_ppo2.py:192][0m |          -0.0118 |           3.1086 |           0.1622 |
[32m[20230207 15:32:58 @agent_ppo2.py:192][0m |          -0.0062 |           2.8693 |           0.1623 |
[32m[20230207 15:32:58 @agent_ppo2.py:192][0m |          -0.0112 |           2.6868 |           0.1623 |
[32m[20230207 15:32:58 @agent_ppo2.py:192][0m |          -0.0108 |           2.5388 |           0.1622 |
[32m[20230207 15:32:58 @agent_ppo2.py:192][0m |          -0.0136 |           2.4801 |           0.1623 |
[32m[20230207 15:32:58 @agent_ppo2.py:192][0m |          -0.0118 |           2.3356 |           0.1621 |
[32m[20230207 15:32:58 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:32:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: 22.25
[32m[20230207 15:32:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 112.39
[32m[20230207 15:32:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 53.87
[32m[20230207 15:32:59 @agent_ppo2.py:150][0m Total time:      32.30 min
[32m[20230207 15:32:59 @agent_ppo2.py:152][0m 1705984 total steps have happened
[32m[20230207 15:32:59 @agent_ppo2.py:128][0m #------------------------ Iteration 833 --------------------------#
[32m[20230207 15:33:00 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:33:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:00 @agent_ppo2.py:192][0m |           0.0001 |           1.9081 |           0.1644 |
[32m[20230207 15:33:00 @agent_ppo2.py:192][0m |           0.0027 |           1.3779 |           0.1642 |
[32m[20230207 15:33:00 @agent_ppo2.py:192][0m |          -0.0165 |           1.3111 |           0.1641 |
[32m[20230207 15:33:00 @agent_ppo2.py:192][0m |          -0.0105 |           1.2680 |           0.1641 |
[32m[20230207 15:33:00 @agent_ppo2.py:192][0m |          -0.0034 |           1.2442 |           0.1641 |
[32m[20230207 15:33:00 @agent_ppo2.py:192][0m |          -0.0119 |           1.2202 |           0.1643 |
[32m[20230207 15:33:00 @agent_ppo2.py:192][0m |          -0.0079 |           1.2059 |           0.1642 |
[32m[20230207 15:33:00 @agent_ppo2.py:192][0m |          -0.0117 |           1.1965 |           0.1643 |
[32m[20230207 15:33:01 @agent_ppo2.py:192][0m |          -0.0234 |           1.1890 |           0.1643 |
[32m[20230207 15:33:01 @agent_ppo2.py:192][0m |          -0.0191 |           1.1741 |           0.1643 |
[32m[20230207 15:33:01 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:33:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -1.01
[32m[20230207 15:33:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 13.43
[32m[20230207 15:33:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -64.18
[32m[20230207 15:33:01 @agent_ppo2.py:150][0m Total time:      32.33 min
[32m[20230207 15:33:01 @agent_ppo2.py:152][0m 1708032 total steps have happened
[32m[20230207 15:33:01 @agent_ppo2.py:128][0m #------------------------ Iteration 834 --------------------------#
[32m[20230207 15:33:02 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230207 15:33:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:02 @agent_ppo2.py:192][0m |           0.0024 |          10.2927 |           0.1656 |
[32m[20230207 15:33:02 @agent_ppo2.py:192][0m |          -0.0016 |           4.7531 |           0.1655 |
[32m[20230207 15:33:02 @agent_ppo2.py:192][0m |          -0.0095 |           3.7390 |           0.1654 |
[32m[20230207 15:33:02 @agent_ppo2.py:192][0m |          -0.0134 |           3.2597 |           0.1652 |
[32m[20230207 15:33:02 @agent_ppo2.py:192][0m |          -0.0129 |           2.8440 |           0.1651 |
[32m[20230207 15:33:02 @agent_ppo2.py:192][0m |          -0.0150 |           2.5744 |           0.1651 |
[32m[20230207 15:33:02 @agent_ppo2.py:192][0m |          -0.0102 |           2.4765 |           0.1648 |
[32m[20230207 15:33:02 @agent_ppo2.py:192][0m |          -0.0091 |           2.3071 |           0.1648 |
[32m[20230207 15:33:02 @agent_ppo2.py:192][0m |          -0.0175 |           2.1788 |           0.1647 |
[32m[20230207 15:33:03 @agent_ppo2.py:192][0m |          -0.0133 |           2.1274 |           0.1646 |
[32m[20230207 15:33:03 @agent_ppo2.py:137][0m Policy update time: 0.69 s
[32m[20230207 15:33:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: 71.79
[32m[20230207 15:33:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 190.04
[32m[20230207 15:33:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -18.38
[32m[20230207 15:33:03 @agent_ppo2.py:150][0m Total time:      32.37 min
[32m[20230207 15:33:03 @agent_ppo2.py:152][0m 1710080 total steps have happened
[32m[20230207 15:33:03 @agent_ppo2.py:128][0m #------------------------ Iteration 835 --------------------------#
[32m[20230207 15:33:04 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:33:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:04 @agent_ppo2.py:192][0m |          -0.0012 |          11.7522 |           0.1599 |
[32m[20230207 15:33:05 @agent_ppo2.py:192][0m |          -0.0075 |           7.6824 |           0.1596 |
[32m[20230207 15:33:05 @agent_ppo2.py:192][0m |          -0.0116 |           7.0088 |           0.1596 |
[32m[20230207 15:33:05 @agent_ppo2.py:192][0m |          -0.0076 |           6.6350 |           0.1592 |
[32m[20230207 15:33:05 @agent_ppo2.py:192][0m |          -0.0105 |           6.4258 |           0.1592 |
[32m[20230207 15:33:05 @agent_ppo2.py:192][0m |          -0.0097 |           5.9926 |           0.1591 |
[32m[20230207 15:33:05 @agent_ppo2.py:192][0m |          -0.0204 |           6.0776 |           0.1592 |
[32m[20230207 15:33:05 @agent_ppo2.py:192][0m |          -0.0135 |           5.6301 |           0.1587 |
[32m[20230207 15:33:05 @agent_ppo2.py:192][0m |          -0.0105 |           5.4157 |           0.1590 |
[32m[20230207 15:33:05 @agent_ppo2.py:192][0m |          -0.0165 |           5.3621 |           0.1588 |
[32m[20230207 15:33:05 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:33:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: -1.71
[32m[20230207 15:33:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 59.48
[32m[20230207 15:33:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 48.73
[32m[20230207 15:33:06 @agent_ppo2.py:150][0m Total time:      32.41 min
[32m[20230207 15:33:06 @agent_ppo2.py:152][0m 1712128 total steps have happened
[32m[20230207 15:33:06 @agent_ppo2.py:128][0m #------------------------ Iteration 836 --------------------------#
[32m[20230207 15:33:07 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:33:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:07 @agent_ppo2.py:192][0m |          -0.0006 |           3.5109 |           0.1613 |
[32m[20230207 15:33:07 @agent_ppo2.py:192][0m |          -0.0056 |           1.8865 |           0.1611 |
[32m[20230207 15:33:07 @agent_ppo2.py:192][0m |          -0.0068 |           1.6213 |           0.1611 |
[32m[20230207 15:33:07 @agent_ppo2.py:192][0m |          -0.0087 |           1.4832 |           0.1609 |
[32m[20230207 15:33:07 @agent_ppo2.py:192][0m |          -0.0021 |           1.4319 |           0.1610 |
[32m[20230207 15:33:07 @agent_ppo2.py:192][0m |          -0.0087 |           1.3510 |           0.1609 |
[32m[20230207 15:33:07 @agent_ppo2.py:192][0m |          -0.0092 |           1.3133 |           0.1609 |
[32m[20230207 15:33:08 @agent_ppo2.py:192][0m |          -0.0130 |           1.2862 |           0.1609 |
[32m[20230207 15:33:08 @agent_ppo2.py:192][0m |          -0.0086 |           1.2648 |           0.1608 |
[32m[20230207 15:33:08 @agent_ppo2.py:192][0m |          -0.0114 |           1.2401 |           0.1608 |
[32m[20230207 15:33:08 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:33:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: 14.62
[32m[20230207 15:33:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 80.54
[32m[20230207 15:33:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 11.81
[32m[20230207 15:33:09 @agent_ppo2.py:150][0m Total time:      32.46 min
[32m[20230207 15:33:09 @agent_ppo2.py:152][0m 1714176 total steps have happened
[32m[20230207 15:33:09 @agent_ppo2.py:128][0m #------------------------ Iteration 837 --------------------------#
[32m[20230207 15:33:09 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:33:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:10 @agent_ppo2.py:192][0m |           0.0028 |          23.0925 |           0.1647 |
[32m[20230207 15:33:10 @agent_ppo2.py:192][0m |          -0.0016 |          15.2903 |           0.1646 |
[32m[20230207 15:33:10 @agent_ppo2.py:192][0m |          -0.0137 |          14.3623 |           0.1645 |
[32m[20230207 15:33:10 @agent_ppo2.py:192][0m |          -0.0097 |          13.6755 |           0.1646 |
[32m[20230207 15:33:10 @agent_ppo2.py:192][0m |          -0.0114 |          13.2801 |           0.1644 |
[32m[20230207 15:33:10 @agent_ppo2.py:192][0m |          -0.0094 |          13.1389 |           0.1644 |
[32m[20230207 15:33:10 @agent_ppo2.py:192][0m |          -0.0095 |          13.0506 |           0.1644 |
[32m[20230207 15:33:10 @agent_ppo2.py:192][0m |          -0.0142 |          12.7716 |           0.1643 |
[32m[20230207 15:33:10 @agent_ppo2.py:192][0m |          -0.0130 |          12.6933 |           0.1641 |
[32m[20230207 15:33:10 @agent_ppo2.py:192][0m |          -0.0151 |          12.8547 |           0.1640 |
[32m[20230207 15:33:10 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:33:11 @agent_ppo2.py:145][0m Average TRAINING episode reward: -43.68
[32m[20230207 15:33:11 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 102.28
[32m[20230207 15:33:11 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 19.99
[32m[20230207 15:33:11 @agent_ppo2.py:150][0m Total time:      32.50 min
[32m[20230207 15:33:11 @agent_ppo2.py:152][0m 1716224 total steps have happened
[32m[20230207 15:33:11 @agent_ppo2.py:128][0m #------------------------ Iteration 838 --------------------------#
[32m[20230207 15:33:12 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230207 15:33:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:12 @agent_ppo2.py:192][0m |          -0.0000 |          13.6639 |           0.1590 |
[32m[20230207 15:33:12 @agent_ppo2.py:192][0m |          -0.0023 |           5.1276 |           0.1590 |
[32m[20230207 15:33:12 @agent_ppo2.py:192][0m |          -0.0036 |           3.6889 |           0.1588 |
[32m[20230207 15:33:12 @agent_ppo2.py:192][0m |          -0.0059 |           3.0188 |           0.1586 |
[32m[20230207 15:33:12 @agent_ppo2.py:192][0m |          -0.0100 |           2.6811 |           0.1587 |
[32m[20230207 15:33:12 @agent_ppo2.py:192][0m |          -0.0106 |           2.4083 |           0.1587 |
[32m[20230207 15:33:12 @agent_ppo2.py:192][0m |          -0.0117 |           2.2039 |           0.1584 |
[32m[20230207 15:33:12 @agent_ppo2.py:192][0m |          -0.0135 |           2.0943 |           0.1584 |
[32m[20230207 15:33:12 @agent_ppo2.py:192][0m |          -0.0141 |           1.9830 |           0.1584 |
[32m[20230207 15:33:12 @agent_ppo2.py:192][0m |          -0.0133 |           1.8783 |           0.1583 |
[32m[20230207 15:33:12 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230207 15:33:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -49.80
[32m[20230207 15:33:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 79.99
[32m[20230207 15:33:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 106.33
[32m[20230207 15:33:13 @agent_ppo2.py:150][0m Total time:      32.53 min
[32m[20230207 15:33:13 @agent_ppo2.py:152][0m 1718272 total steps have happened
[32m[20230207 15:33:13 @agent_ppo2.py:128][0m #------------------------ Iteration 839 --------------------------#
[32m[20230207 15:33:14 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:33:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:14 @agent_ppo2.py:192][0m |          -0.0020 |          13.6017 |           0.1623 |
[32m[20230207 15:33:14 @agent_ppo2.py:192][0m |          -0.0023 |           5.3365 |           0.1623 |
[32m[20230207 15:33:14 @agent_ppo2.py:192][0m |          -0.0085 |           3.9388 |           0.1622 |
[32m[20230207 15:33:14 @agent_ppo2.py:192][0m |          -0.0089 |           3.3568 |           0.1621 |
[32m[20230207 15:33:15 @agent_ppo2.py:192][0m |          -0.0123 |           3.0129 |           0.1621 |
[32m[20230207 15:33:15 @agent_ppo2.py:192][0m |          -0.0115 |           2.7829 |           0.1620 |
[32m[20230207 15:33:15 @agent_ppo2.py:192][0m |          -0.0148 |           2.6155 |           0.1619 |
[32m[20230207 15:33:15 @agent_ppo2.py:192][0m |          -0.0083 |           2.4442 |           0.1619 |
[32m[20230207 15:33:15 @agent_ppo2.py:192][0m |          -0.0142 |           2.3818 |           0.1619 |
[32m[20230207 15:33:15 @agent_ppo2.py:192][0m |          -0.0118 |           2.2477 |           0.1618 |
[32m[20230207 15:33:15 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:33:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -18.21
[32m[20230207 15:33:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 70.72
[32m[20230207 15:33:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -62.75
[32m[20230207 15:33:16 @agent_ppo2.py:150][0m Total time:      32.57 min
[32m[20230207 15:33:16 @agent_ppo2.py:152][0m 1720320 total steps have happened
[32m[20230207 15:33:16 @agent_ppo2.py:128][0m #------------------------ Iteration 840 --------------------------#
[32m[20230207 15:33:16 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:33:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:16 @agent_ppo2.py:192][0m |           0.0016 |          32.9856 |           0.1665 |
[32m[20230207 15:33:16 @agent_ppo2.py:192][0m |          -0.0103 |          15.7835 |           0.1658 |
[32m[20230207 15:33:17 @agent_ppo2.py:192][0m |          -0.0073 |          11.9854 |           0.1660 |
[32m[20230207 15:33:17 @agent_ppo2.py:192][0m |          -0.0141 |          10.3099 |           0.1659 |
[32m[20230207 15:33:17 @agent_ppo2.py:192][0m |          -0.0128 |           9.3908 |           0.1661 |
[32m[20230207 15:33:17 @agent_ppo2.py:192][0m |          -0.0121 |           8.7721 |           0.1661 |
[32m[20230207 15:33:17 @agent_ppo2.py:192][0m |          -0.0117 |           8.2010 |           0.1661 |
[32m[20230207 15:33:17 @agent_ppo2.py:192][0m |          -0.0169 |           7.8408 |           0.1659 |
[32m[20230207 15:33:17 @agent_ppo2.py:192][0m |          -0.0242 |           7.6071 |           0.1662 |
[32m[20230207 15:33:17 @agent_ppo2.py:192][0m |          -0.0195 |           7.4195 |           0.1658 |
[32m[20230207 15:33:17 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230207 15:33:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -51.15
[32m[20230207 15:33:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 52.29
[32m[20230207 15:33:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 60.19
[32m[20230207 15:33:18 @agent_ppo2.py:150][0m Total time:      32.61 min
[32m[20230207 15:33:18 @agent_ppo2.py:152][0m 1722368 total steps have happened
[32m[20230207 15:33:18 @agent_ppo2.py:128][0m #------------------------ Iteration 841 --------------------------#
[32m[20230207 15:33:18 @agent_ppo2.py:134][0m Sampling time: 0.61 s by 1 slaves
[32m[20230207 15:33:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:18 @agent_ppo2.py:192][0m |           0.0012 |          34.6151 |           0.1650 |
[32m[20230207 15:33:19 @agent_ppo2.py:192][0m |          -0.0049 |          18.3892 |           0.1650 |
[32m[20230207 15:33:19 @agent_ppo2.py:192][0m |          -0.0079 |          13.8779 |           0.1648 |
[32m[20230207 15:33:19 @agent_ppo2.py:192][0m |          -0.0099 |          11.5994 |           0.1646 |
[32m[20230207 15:33:19 @agent_ppo2.py:192][0m |          -0.0118 |           9.7709 |           0.1646 |
[32m[20230207 15:33:19 @agent_ppo2.py:192][0m |          -0.0128 |           8.7973 |           0.1645 |
[32m[20230207 15:33:19 @agent_ppo2.py:192][0m |          -0.0140 |           8.1192 |           0.1644 |
[32m[20230207 15:33:19 @agent_ppo2.py:192][0m |          -0.0146 |           7.6459 |           0.1645 |
[32m[20230207 15:33:19 @agent_ppo2.py:192][0m |          -0.0169 |           7.1018 |           0.1645 |
[32m[20230207 15:33:19 @agent_ppo2.py:192][0m |          -0.0170 |           6.6932 |           0.1645 |
[32m[20230207 15:33:19 @agent_ppo2.py:137][0m Policy update time: 0.64 s
[32m[20230207 15:33:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -48.48
[32m[20230207 15:33:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 94.48
[32m[20230207 15:33:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 61.23
[32m[20230207 15:33:20 @agent_ppo2.py:150][0m Total time:      32.64 min
[32m[20230207 15:33:20 @agent_ppo2.py:152][0m 1724416 total steps have happened
[32m[20230207 15:33:20 @agent_ppo2.py:128][0m #------------------------ Iteration 842 --------------------------#
[32m[20230207 15:33:21 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:33:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:21 @agent_ppo2.py:192][0m |          -0.0027 |          13.2119 |           0.1642 |
[32m[20230207 15:33:21 @agent_ppo2.py:192][0m |          -0.0064 |           9.0260 |           0.1645 |
[32m[20230207 15:33:21 @agent_ppo2.py:192][0m |           0.0031 |           8.1528 |           0.1643 |
[32m[20230207 15:33:21 @agent_ppo2.py:192][0m |          -0.0190 |           7.6451 |           0.1644 |
[32m[20230207 15:33:21 @agent_ppo2.py:192][0m |          -0.0132 |           7.3524 |           0.1643 |
[32m[20230207 15:33:21 @agent_ppo2.py:192][0m |          -0.0142 |           7.1607 |           0.1643 |
[32m[20230207 15:33:21 @agent_ppo2.py:192][0m |          -0.0067 |           7.0019 |           0.1646 |
[32m[20230207 15:33:21 @agent_ppo2.py:192][0m |           0.0011 |           6.7854 |           0.1645 |
[32m[20230207 15:33:21 @agent_ppo2.py:192][0m |          -0.0104 |           6.3330 |           0.1644 |
[32m[20230207 15:33:21 @agent_ppo2.py:192][0m |          -0.0225 |           6.2255 |           0.1645 |
[32m[20230207 15:33:21 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:33:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: 87.52
[32m[20230207 15:33:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 120.44
[32m[20230207 15:33:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 75.10
[32m[20230207 15:33:22 @agent_ppo2.py:150][0m Total time:      32.68 min
[32m[20230207 15:33:22 @agent_ppo2.py:152][0m 1726464 total steps have happened
[32m[20230207 15:33:22 @agent_ppo2.py:128][0m #------------------------ Iteration 843 --------------------------#
[32m[20230207 15:33:23 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:33:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:23 @agent_ppo2.py:192][0m |           0.0048 |           3.9622 |           0.1641 |
[32m[20230207 15:33:23 @agent_ppo2.py:192][0m |           0.0127 |           2.8125 |           0.1638 |
[32m[20230207 15:33:23 @agent_ppo2.py:192][0m |          -0.0006 |           2.5707 |           0.1636 |
[32m[20230207 15:33:23 @agent_ppo2.py:192][0m |           0.0027 |           2.4642 |           0.1635 |
[32m[20230207 15:33:23 @agent_ppo2.py:192][0m |          -0.0385 |           2.4062 |           0.1634 |
[32m[20230207 15:33:24 @agent_ppo2.py:192][0m |          -0.0037 |           2.3309 |           0.1632 |
[32m[20230207 15:33:24 @agent_ppo2.py:192][0m |          -0.0240 |           2.2662 |           0.1632 |
[32m[20230207 15:33:24 @agent_ppo2.py:192][0m |           0.0001 |           2.2023 |           0.1632 |
[32m[20230207 15:33:24 @agent_ppo2.py:192][0m |          -0.0124 |           2.0724 |           0.1633 |
[32m[20230207 15:33:24 @agent_ppo2.py:192][0m |          -0.0028 |           2.0052 |           0.1631 |
[32m[20230207 15:33:24 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:33:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: 57.54
[32m[20230207 15:33:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 69.00
[32m[20230207 15:33:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -110.15
[32m[20230207 15:33:25 @agent_ppo2.py:150][0m Total time:      32.72 min
[32m[20230207 15:33:25 @agent_ppo2.py:152][0m 1728512 total steps have happened
[32m[20230207 15:33:25 @agent_ppo2.py:128][0m #------------------------ Iteration 844 --------------------------#
[32m[20230207 15:33:25 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:33:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:26 @agent_ppo2.py:192][0m |           0.0016 |          12.5972 |           0.1702 |
[32m[20230207 15:33:26 @agent_ppo2.py:192][0m |          -0.0006 |           7.0092 |           0.1702 |
[32m[20230207 15:33:26 @agent_ppo2.py:192][0m |          -0.0049 |           6.2979 |           0.1701 |
[32m[20230207 15:33:26 @agent_ppo2.py:192][0m |          -0.0093 |           5.9119 |           0.1700 |
[32m[20230207 15:33:26 @agent_ppo2.py:192][0m |          -0.0106 |           5.4592 |           0.1699 |
[32m[20230207 15:33:26 @agent_ppo2.py:192][0m |          -0.0103 |           5.2157 |           0.1699 |
[32m[20230207 15:33:26 @agent_ppo2.py:192][0m |          -0.0103 |           5.0481 |           0.1698 |
[32m[20230207 15:33:26 @agent_ppo2.py:192][0m |          -0.0128 |           4.6855 |           0.1700 |
[32m[20230207 15:33:26 @agent_ppo2.py:192][0m |          -0.0125 |           4.4482 |           0.1698 |
[32m[20230207 15:33:26 @agent_ppo2.py:192][0m |          -0.0122 |           4.3468 |           0.1696 |
[32m[20230207 15:33:26 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:33:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -2.19
[32m[20230207 15:33:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 103.45
[32m[20230207 15:33:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -46.45
[32m[20230207 15:33:27 @agent_ppo2.py:150][0m Total time:      32.76 min
[32m[20230207 15:33:27 @agent_ppo2.py:152][0m 1730560 total steps have happened
[32m[20230207 15:33:27 @agent_ppo2.py:128][0m #------------------------ Iteration 845 --------------------------#
[32m[20230207 15:33:28 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:33:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:28 @agent_ppo2.py:192][0m |           0.0016 |           5.0161 |           0.1617 |
[32m[20230207 15:33:28 @agent_ppo2.py:192][0m |          -0.0145 |           3.5722 |           0.1615 |
[32m[20230207 15:33:28 @agent_ppo2.py:192][0m |          -0.0066 |           3.2900 |           0.1615 |
[32m[20230207 15:33:28 @agent_ppo2.py:192][0m |          -0.0063 |           3.1234 |           0.1617 |
[32m[20230207 15:33:28 @agent_ppo2.py:192][0m |          -0.0062 |           2.9888 |           0.1616 |
[32m[20230207 15:33:28 @agent_ppo2.py:192][0m |          -0.0038 |           2.8818 |           0.1616 |
[32m[20230207 15:33:28 @agent_ppo2.py:192][0m |          -0.0282 |           2.8268 |           0.1617 |
[32m[20230207 15:33:28 @agent_ppo2.py:192][0m |          -0.0109 |           2.7327 |           0.1615 |
[32m[20230207 15:33:28 @agent_ppo2.py:192][0m |          -0.0043 |           2.6490 |           0.1615 |
[32m[20230207 15:33:29 @agent_ppo2.py:192][0m |           0.0070 |           2.6147 |           0.1616 |
[32m[20230207 15:33:29 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:33:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: 22.37
[32m[20230207 15:33:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 50.64
[32m[20230207 15:33:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 101.14
[32m[20230207 15:33:29 @agent_ppo2.py:150][0m Total time:      32.80 min
[32m[20230207 15:33:29 @agent_ppo2.py:152][0m 1732608 total steps have happened
[32m[20230207 15:33:29 @agent_ppo2.py:128][0m #------------------------ Iteration 846 --------------------------#
[32m[20230207 15:33:30 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:33:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:30 @agent_ppo2.py:192][0m |          -0.0082 |          31.1682 |           0.1638 |
[32m[20230207 15:33:30 @agent_ppo2.py:192][0m |          -0.0049 |          10.0467 |           0.1639 |
[32m[20230207 15:33:30 @agent_ppo2.py:192][0m |          -0.0023 |           7.6359 |           0.1637 |
[32m[20230207 15:33:30 @agent_ppo2.py:192][0m |          -0.0121 |           6.9569 |           0.1639 |
[32m[20230207 15:33:30 @agent_ppo2.py:192][0m |          -0.0198 |           5.7486 |           0.1637 |
[32m[20230207 15:33:30 @agent_ppo2.py:192][0m |          -0.0140 |           5.4773 |           0.1638 |
[32m[20230207 15:33:30 @agent_ppo2.py:192][0m |          -0.0091 |           5.0575 |           0.1637 |
[32m[20230207 15:33:30 @agent_ppo2.py:192][0m |          -0.0201 |           4.6141 |           0.1635 |
[32m[20230207 15:33:30 @agent_ppo2.py:192][0m |          -0.0190 |           4.4305 |           0.1635 |
[32m[20230207 15:33:30 @agent_ppo2.py:192][0m |          -0.0137 |           4.1950 |           0.1635 |
[32m[20230207 15:33:30 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:33:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -94.26
[32m[20230207 15:33:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -26.18
[32m[20230207 15:33:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -23.20
[32m[20230207 15:33:31 @agent_ppo2.py:150][0m Total time:      32.83 min
[32m[20230207 15:33:31 @agent_ppo2.py:152][0m 1734656 total steps have happened
[32m[20230207 15:33:31 @agent_ppo2.py:128][0m #------------------------ Iteration 847 --------------------------#
[32m[20230207 15:33:32 @agent_ppo2.py:134][0m Sampling time: 0.54 s by 1 slaves
[32m[20230207 15:33:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:32 @agent_ppo2.py:192][0m |           0.0067 |          28.2620 |           0.1625 |
[32m[20230207 15:33:32 @agent_ppo2.py:192][0m |          -0.0056 |          10.9568 |           0.1619 |
[32m[20230207 15:33:32 @agent_ppo2.py:192][0m |          -0.0000 |           7.9838 |           0.1617 |
[32m[20230207 15:33:32 @agent_ppo2.py:192][0m |          -0.0107 |           6.6626 |           0.1617 |
[32m[20230207 15:33:32 @agent_ppo2.py:192][0m |          -0.0085 |           5.8352 |           0.1618 |
[32m[20230207 15:33:32 @agent_ppo2.py:192][0m |          -0.0094 |           5.4402 |           0.1616 |
[32m[20230207 15:33:32 @agent_ppo2.py:192][0m |          -0.0126 |           5.0821 |           0.1614 |
[32m[20230207 15:33:32 @agent_ppo2.py:192][0m |          -0.0214 |           4.8383 |           0.1614 |
[32m[20230207 15:33:32 @agent_ppo2.py:192][0m |          -0.0177 |           4.7149 |           0.1612 |
[32m[20230207 15:33:32 @agent_ppo2.py:192][0m |          -0.0078 |           4.4443 |           0.1610 |
[32m[20230207 15:33:32 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:33:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: -24.64
[32m[20230207 15:33:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 210.48
[32m[20230207 15:33:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -17.34
[32m[20230207 15:33:33 @agent_ppo2.py:150][0m Total time:      32.87 min
[32m[20230207 15:33:33 @agent_ppo2.py:152][0m 1736704 total steps have happened
[32m[20230207 15:33:33 @agent_ppo2.py:128][0m #------------------------ Iteration 848 --------------------------#
[32m[20230207 15:33:34 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:33:34 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:34 @agent_ppo2.py:192][0m |           0.0002 |          30.7208 |           0.1659 |
[32m[20230207 15:33:34 @agent_ppo2.py:192][0m |          -0.0048 |          18.3209 |           0.1660 |
[32m[20230207 15:33:34 @agent_ppo2.py:192][0m |          -0.0088 |          15.3594 |           0.1660 |
[32m[20230207 15:33:34 @agent_ppo2.py:192][0m |          -0.0088 |          13.6638 |           0.1663 |
[32m[20230207 15:33:34 @agent_ppo2.py:192][0m |          -0.0135 |          12.8268 |           0.1662 |
[32m[20230207 15:33:34 @agent_ppo2.py:192][0m |          -0.0149 |          12.2714 |           0.1663 |
[32m[20230207 15:33:34 @agent_ppo2.py:192][0m |          -0.0168 |          11.9111 |           0.1666 |
[32m[20230207 15:33:34 @agent_ppo2.py:192][0m |          -0.0187 |          11.4707 |           0.1666 |
[32m[20230207 15:33:34 @agent_ppo2.py:192][0m |          -0.0201 |          11.1049 |           0.1667 |
[32m[20230207 15:33:34 @agent_ppo2.py:192][0m |          -0.0200 |          11.0064 |           0.1667 |
[32m[20230207 15:33:34 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:33:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: -66.92
[32m[20230207 15:33:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 134.08
[32m[20230207 15:33:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -16.98
[32m[20230207 15:33:35 @agent_ppo2.py:150][0m Total time:      32.90 min
[32m[20230207 15:33:35 @agent_ppo2.py:152][0m 1738752 total steps have happened
[32m[20230207 15:33:35 @agent_ppo2.py:128][0m #------------------------ Iteration 849 --------------------------#
[32m[20230207 15:33:36 @agent_ppo2.py:134][0m Sampling time: 0.64 s by 1 slaves
[32m[20230207 15:33:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:36 @agent_ppo2.py:192][0m |           0.0057 |          22.1488 |           0.1638 |
[32m[20230207 15:33:36 @agent_ppo2.py:192][0m |          -0.0106 |          12.0838 |           0.1635 |
[32m[20230207 15:33:36 @agent_ppo2.py:192][0m |          -0.0126 |           8.7634 |           0.1634 |
[32m[20230207 15:33:36 @agent_ppo2.py:192][0m |          -0.0170 |           7.8030 |           0.1633 |
[32m[20230207 15:33:36 @agent_ppo2.py:192][0m |          -0.0141 |           6.7585 |           0.1631 |
[32m[20230207 15:33:36 @agent_ppo2.py:192][0m |          -0.0186 |           6.1106 |           0.1632 |
[32m[20230207 15:33:36 @agent_ppo2.py:192][0m |          -0.0218 |           5.7007 |           0.1632 |
[32m[20230207 15:33:36 @agent_ppo2.py:192][0m |          -0.0137 |           5.2776 |           0.1633 |
[32m[20230207 15:33:36 @agent_ppo2.py:192][0m |          -0.0215 |           4.8957 |           0.1632 |
[32m[20230207 15:33:36 @agent_ppo2.py:192][0m |          -0.0254 |           4.6906 |           0.1632 |
[32m[20230207 15:33:36 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230207 15:33:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: -53.90
[32m[20230207 15:33:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 31.22
[32m[20230207 15:33:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 49.00
[32m[20230207 15:33:37 @agent_ppo2.py:150][0m Total time:      32.93 min
[32m[20230207 15:33:37 @agent_ppo2.py:152][0m 1740800 total steps have happened
[32m[20230207 15:33:37 @agent_ppo2.py:128][0m #------------------------ Iteration 850 --------------------------#
[32m[20230207 15:33:38 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 15:33:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:38 @agent_ppo2.py:192][0m |           0.0040 |          22.2115 |           0.1659 |
[32m[20230207 15:33:38 @agent_ppo2.py:192][0m |          -0.0092 |          11.5085 |           0.1658 |
[32m[20230207 15:33:38 @agent_ppo2.py:192][0m |          -0.0152 |           8.5779 |           0.1657 |
[32m[20230207 15:33:38 @agent_ppo2.py:192][0m |          -0.0120 |           7.5875 |           0.1655 |
[32m[20230207 15:33:38 @agent_ppo2.py:192][0m |          -0.0093 |           6.7143 |           0.1656 |
[32m[20230207 15:33:38 @agent_ppo2.py:192][0m |          -0.0145 |           6.4927 |           0.1654 |
[32m[20230207 15:33:38 @agent_ppo2.py:192][0m |          -0.0115 |           6.0068 |           0.1654 |
[32m[20230207 15:33:38 @agent_ppo2.py:192][0m |          -0.0160 |           5.7691 |           0.1654 |
[32m[20230207 15:33:38 @agent_ppo2.py:192][0m |          -0.0188 |           5.4230 |           0.1651 |
[32m[20230207 15:33:38 @agent_ppo2.py:192][0m |          -0.0194 |           5.1438 |           0.1653 |
[32m[20230207 15:33:38 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:33:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: -62.45
[32m[20230207 15:33:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 106.13
[32m[20230207 15:33:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 27.88
[32m[20230207 15:33:39 @agent_ppo2.py:150][0m Total time:      32.97 min
[32m[20230207 15:33:39 @agent_ppo2.py:152][0m 1742848 total steps have happened
[32m[20230207 15:33:39 @agent_ppo2.py:128][0m #------------------------ Iteration 851 --------------------------#
[32m[20230207 15:33:40 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230207 15:33:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:40 @agent_ppo2.py:192][0m |           0.0032 |          19.1526 |           0.1656 |
[32m[20230207 15:33:40 @agent_ppo2.py:192][0m |          -0.0035 |           7.4441 |           0.1655 |
[32m[20230207 15:33:40 @agent_ppo2.py:192][0m |          -0.0068 |           5.3911 |           0.1654 |
[32m[20230207 15:33:40 @agent_ppo2.py:192][0m |          -0.0069 |           4.6310 |           0.1655 |
[32m[20230207 15:33:40 @agent_ppo2.py:192][0m |          -0.0108 |           4.1909 |           0.1655 |
[32m[20230207 15:33:40 @agent_ppo2.py:192][0m |          -0.0121 |           3.9309 |           0.1654 |
[32m[20230207 15:33:40 @agent_ppo2.py:192][0m |          -0.0126 |           3.7166 |           0.1653 |
[32m[20230207 15:33:40 @agent_ppo2.py:192][0m |          -0.0145 |           3.6120 |           0.1655 |
[32m[20230207 15:33:40 @agent_ppo2.py:192][0m |          -0.0144 |           3.4800 |           0.1653 |
[32m[20230207 15:33:40 @agent_ppo2.py:192][0m |          -0.0173 |           3.3272 |           0.1654 |
[32m[20230207 15:33:40 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:33:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: -44.26
[32m[20230207 15:33:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 35.84
[32m[20230207 15:33:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -100.86
[32m[20230207 15:33:41 @agent_ppo2.py:150][0m Total time:      33.00 min
[32m[20230207 15:33:41 @agent_ppo2.py:152][0m 1744896 total steps have happened
[32m[20230207 15:33:41 @agent_ppo2.py:128][0m #------------------------ Iteration 852 --------------------------#
[32m[20230207 15:33:42 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:33:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:42 @agent_ppo2.py:192][0m |           0.0021 |          21.0394 |           0.1639 |
[32m[20230207 15:33:42 @agent_ppo2.py:192][0m |          -0.0043 |          10.0615 |           0.1638 |
[32m[20230207 15:33:42 @agent_ppo2.py:192][0m |          -0.0070 |           7.7363 |           0.1636 |
[32m[20230207 15:33:42 @agent_ppo2.py:192][0m |          -0.0065 |           6.6363 |           0.1638 |
[32m[20230207 15:33:42 @agent_ppo2.py:192][0m |          -0.0208 |           5.9804 |           0.1637 |
[32m[20230207 15:33:42 @agent_ppo2.py:192][0m |          -0.0119 |           5.2996 |           0.1637 |
[32m[20230207 15:33:42 @agent_ppo2.py:192][0m |          -0.0132 |           4.9884 |           0.1637 |
[32m[20230207 15:33:42 @agent_ppo2.py:192][0m |          -0.0133 |           4.6663 |           0.1636 |
[32m[20230207 15:33:42 @agent_ppo2.py:192][0m |          -0.0199 |           4.4180 |           0.1634 |
[32m[20230207 15:33:42 @agent_ppo2.py:192][0m |          -0.0183 |           4.3824 |           0.1635 |
[32m[20230207 15:33:42 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230207 15:33:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: 22.39
[32m[20230207 15:33:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 71.94
[32m[20230207 15:33:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 70.77
[32m[20230207 15:33:43 @agent_ppo2.py:150][0m Total time:      33.03 min
[32m[20230207 15:33:43 @agent_ppo2.py:152][0m 1746944 total steps have happened
[32m[20230207 15:33:43 @agent_ppo2.py:128][0m #------------------------ Iteration 853 --------------------------#
[32m[20230207 15:33:44 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:33:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:44 @agent_ppo2.py:192][0m |           0.0005 |          16.0449 |           0.1686 |
[32m[20230207 15:33:44 @agent_ppo2.py:192][0m |          -0.0031 |           9.7736 |           0.1685 |
[32m[20230207 15:33:44 @agent_ppo2.py:192][0m |          -0.0047 |           8.5707 |           0.1685 |
[32m[20230207 15:33:44 @agent_ppo2.py:192][0m |          -0.0065 |           7.5782 |           0.1684 |
[32m[20230207 15:33:44 @agent_ppo2.py:192][0m |          -0.0055 |           6.6424 |           0.1684 |
[32m[20230207 15:33:44 @agent_ppo2.py:192][0m |          -0.0090 |           6.2554 |           0.1684 |
[32m[20230207 15:33:44 @agent_ppo2.py:192][0m |          -0.0089 |           5.7641 |           0.1684 |
[32m[20230207 15:33:44 @agent_ppo2.py:192][0m |          -0.0091 |           5.5268 |           0.1685 |
[32m[20230207 15:33:45 @agent_ppo2.py:192][0m |          -0.0108 |           5.5358 |           0.1685 |
[32m[20230207 15:33:45 @agent_ppo2.py:192][0m |          -0.0110 |           5.0891 |           0.1684 |
[32m[20230207 15:33:45 @agent_ppo2.py:137][0m Policy update time: 0.78 s
[32m[20230207 15:33:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: 14.62
[32m[20230207 15:33:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 64.28
[32m[20230207 15:33:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 43.56
[32m[20230207 15:33:45 @agent_ppo2.py:150][0m Total time:      33.07 min
[32m[20230207 15:33:45 @agent_ppo2.py:152][0m 1748992 total steps have happened
[32m[20230207 15:33:45 @agent_ppo2.py:128][0m #------------------------ Iteration 854 --------------------------#
[32m[20230207 15:33:46 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:33:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:46 @agent_ppo2.py:192][0m |          -0.0001 |          39.9472 |           0.1708 |
[32m[20230207 15:33:46 @agent_ppo2.py:192][0m |          -0.0051 |          29.8596 |           0.1704 |
[32m[20230207 15:33:46 @agent_ppo2.py:192][0m |          -0.0057 |          27.0073 |           0.1705 |
[32m[20230207 15:33:46 @agent_ppo2.py:192][0m |          -0.0108 |          25.3116 |           0.1707 |
[32m[20230207 15:33:46 @agent_ppo2.py:192][0m |          -0.0125 |          24.3067 |           0.1706 |
[32m[20230207 15:33:46 @agent_ppo2.py:192][0m |          -0.0119 |          23.9533 |           0.1706 |
[32m[20230207 15:33:46 @agent_ppo2.py:192][0m |          -0.0145 |          23.1628 |           0.1706 |
[32m[20230207 15:33:46 @agent_ppo2.py:192][0m |          -0.0164 |          23.1373 |           0.1705 |
[32m[20230207 15:33:46 @agent_ppo2.py:192][0m |          -0.0158 |          23.3349 |           0.1704 |
[32m[20230207 15:33:46 @agent_ppo2.py:192][0m |          -0.0158 |          22.1132 |           0.1706 |
[32m[20230207 15:33:46 @agent_ppo2.py:137][0m Policy update time: 0.53 s
[32m[20230207 15:33:47 @agent_ppo2.py:145][0m Average TRAINING episode reward: -81.81
[32m[20230207 15:33:47 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 53.69
[32m[20230207 15:33:47 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -54.26
[32m[20230207 15:33:47 @agent_ppo2.py:150][0m Total time:      33.10 min
[32m[20230207 15:33:47 @agent_ppo2.py:152][0m 1751040 total steps have happened
[32m[20230207 15:33:47 @agent_ppo2.py:128][0m #------------------------ Iteration 855 --------------------------#
[32m[20230207 15:33:48 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:33:48 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:48 @agent_ppo2.py:192][0m |           0.0018 |          25.6908 |           0.1683 |
[32m[20230207 15:33:48 @agent_ppo2.py:192][0m |          -0.0046 |          15.9076 |           0.1683 |
[32m[20230207 15:33:48 @agent_ppo2.py:192][0m |          -0.0066 |          12.6677 |           0.1683 |
[32m[20230207 15:33:48 @agent_ppo2.py:192][0m |          -0.0108 |          10.8415 |           0.1680 |
[32m[20230207 15:33:48 @agent_ppo2.py:192][0m |          -0.0115 |           9.5591 |           0.1680 |
[32m[20230207 15:33:48 @agent_ppo2.py:192][0m |          -0.0127 |           8.7148 |           0.1679 |
[32m[20230207 15:33:48 @agent_ppo2.py:192][0m |          -0.0123 |           7.7856 |           0.1678 |
[32m[20230207 15:33:48 @agent_ppo2.py:192][0m |          -0.0144 |           7.2548 |           0.1677 |
[32m[20230207 15:33:49 @agent_ppo2.py:192][0m |          -0.0155 |           6.7189 |           0.1676 |
[32m[20230207 15:33:49 @agent_ppo2.py:192][0m |          -0.0155 |           6.4469 |           0.1676 |
[32m[20230207 15:33:49 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:33:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: -38.91
[32m[20230207 15:33:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 127.14
[32m[20230207 15:33:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -41.16
[32m[20230207 15:33:49 @agent_ppo2.py:150][0m Total time:      33.13 min
[32m[20230207 15:33:49 @agent_ppo2.py:152][0m 1753088 total steps have happened
[32m[20230207 15:33:49 @agent_ppo2.py:128][0m #------------------------ Iteration 856 --------------------------#
[32m[20230207 15:33:50 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:33:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:50 @agent_ppo2.py:192][0m |          -0.0038 |          10.0411 |           0.1666 |
[32m[20230207 15:33:50 @agent_ppo2.py:192][0m |          -0.0123 |           5.7173 |           0.1667 |
[32m[20230207 15:33:50 @agent_ppo2.py:192][0m |          -0.0066 |           4.8653 |           0.1668 |
[32m[20230207 15:33:51 @agent_ppo2.py:192][0m |          -0.0078 |           4.4292 |           0.1667 |
[32m[20230207 15:33:51 @agent_ppo2.py:192][0m |          -0.0037 |           4.1140 |           0.1666 |
[32m[20230207 15:33:51 @agent_ppo2.py:192][0m |          -0.0157 |           3.8962 |           0.1666 |
[32m[20230207 15:33:51 @agent_ppo2.py:192][0m |          -0.0148 |           3.6843 |           0.1666 |
[32m[20230207 15:33:51 @agent_ppo2.py:192][0m |          -0.0126 |           3.5701 |           0.1665 |
[32m[20230207 15:33:51 @agent_ppo2.py:192][0m |          -0.0157 |           3.4618 |           0.1664 |
[32m[20230207 15:33:51 @agent_ppo2.py:192][0m |          -0.0144 |           3.3061 |           0.1666 |
[32m[20230207 15:33:51 @agent_ppo2.py:137][0m Policy update time: 0.92 s
[32m[20230207 15:33:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: -18.65
[32m[20230207 15:33:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 97.50
[32m[20230207 15:33:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 66.91
[32m[20230207 15:33:52 @agent_ppo2.py:150][0m Total time:      33.18 min
[32m[20230207 15:33:52 @agent_ppo2.py:152][0m 1755136 total steps have happened
[32m[20230207 15:33:52 @agent_ppo2.py:128][0m #------------------------ Iteration 857 --------------------------#
[32m[20230207 15:33:52 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:33:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:53 @agent_ppo2.py:192][0m |          -0.0011 |          11.5830 |           0.1695 |
[32m[20230207 15:33:53 @agent_ppo2.py:192][0m |          -0.0014 |           5.2476 |           0.1693 |
[32m[20230207 15:33:53 @agent_ppo2.py:192][0m |          -0.0029 |           4.1768 |           0.1690 |
[32m[20230207 15:33:53 @agent_ppo2.py:192][0m |          -0.0083 |           3.7964 |           0.1692 |
[32m[20230207 15:33:53 @agent_ppo2.py:192][0m |          -0.0118 |           3.6481 |           0.1689 |
[32m[20230207 15:33:53 @agent_ppo2.py:192][0m |          -0.0120 |           3.3950 |           0.1689 |
[32m[20230207 15:33:53 @agent_ppo2.py:192][0m |          -0.0153 |           3.2690 |           0.1691 |
[32m[20230207 15:33:53 @agent_ppo2.py:192][0m |          -0.0120 |           3.2317 |           0.1689 |
[32m[20230207 15:33:53 @agent_ppo2.py:192][0m |          -0.0177 |           3.1340 |           0.1687 |
[32m[20230207 15:33:53 @agent_ppo2.py:192][0m |          -0.0060 |           3.0304 |           0.1688 |
[32m[20230207 15:33:53 @agent_ppo2.py:137][0m Policy update time: 0.69 s
[32m[20230207 15:33:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: 61.45
[32m[20230207 15:33:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 171.46
[32m[20230207 15:33:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 72.95
[32m[20230207 15:33:54 @agent_ppo2.py:150][0m Total time:      33.21 min
[32m[20230207 15:33:54 @agent_ppo2.py:152][0m 1757184 total steps have happened
[32m[20230207 15:33:54 @agent_ppo2.py:128][0m #------------------------ Iteration 858 --------------------------#
[32m[20230207 15:33:55 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 15:33:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:55 @agent_ppo2.py:192][0m |           0.0002 |          51.0968 |           0.1665 |
[32m[20230207 15:33:55 @agent_ppo2.py:192][0m |          -0.0093 |          33.1175 |           0.1659 |
[32m[20230207 15:33:55 @agent_ppo2.py:192][0m |          -0.0114 |          26.9734 |           0.1657 |
[32m[20230207 15:33:55 @agent_ppo2.py:192][0m |          -0.0103 |          23.0308 |           0.1656 |
[32m[20230207 15:33:55 @agent_ppo2.py:192][0m |          -0.0131 |          21.3211 |           0.1654 |
[32m[20230207 15:33:55 @agent_ppo2.py:192][0m |          -0.0141 |          20.9053 |           0.1654 |
[32m[20230207 15:33:55 @agent_ppo2.py:192][0m |          -0.0147 |          19.8221 |           0.1653 |
[32m[20230207 15:33:55 @agent_ppo2.py:192][0m |          -0.0165 |          19.0152 |           0.1652 |
[32m[20230207 15:33:55 @agent_ppo2.py:192][0m |          -0.0178 |          18.5649 |           0.1651 |
[32m[20230207 15:33:55 @agent_ppo2.py:192][0m |          -0.0174 |          17.9477 |           0.1650 |
[32m[20230207 15:33:55 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230207 15:33:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: -77.39
[32m[20230207 15:33:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 20.32
[32m[20230207 15:33:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 50.22
[32m[20230207 15:33:56 @agent_ppo2.py:150][0m Total time:      33.25 min
[32m[20230207 15:33:56 @agent_ppo2.py:152][0m 1759232 total steps have happened
[32m[20230207 15:33:56 @agent_ppo2.py:128][0m #------------------------ Iteration 859 --------------------------#
[32m[20230207 15:33:57 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:33:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:33:57 @agent_ppo2.py:192][0m |          -0.0083 |           4.9305 |           0.1684 |
[32m[20230207 15:33:57 @agent_ppo2.py:192][0m |           0.0125 |           3.6185 |           0.1679 |
[32m[20230207 15:33:57 @agent_ppo2.py:192][0m |          -0.0048 |           3.3171 |           0.1685 |
[32m[20230207 15:33:57 @agent_ppo2.py:192][0m |          -0.0027 |           3.1828 |           0.1684 |
[32m[20230207 15:33:57 @agent_ppo2.py:192][0m |           0.0082 |           3.1163 |           0.1685 |
[32m[20230207 15:33:58 @agent_ppo2.py:192][0m |          -0.0166 |           3.0368 |           0.1683 |
[32m[20230207 15:33:58 @agent_ppo2.py:192][0m |          -0.0085 |           2.9470 |           0.1685 |
[32m[20230207 15:33:58 @agent_ppo2.py:192][0m |          -0.0134 |           2.8922 |           0.1684 |
[32m[20230207 15:33:58 @agent_ppo2.py:192][0m |          -0.0191 |           2.8733 |           0.1684 |
[32m[20230207 15:33:58 @agent_ppo2.py:192][0m |          -0.0098 |           2.8553 |           0.1683 |
[32m[20230207 15:33:58 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:33:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: 97.97
[32m[20230207 15:33:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 103.30
[32m[20230207 15:33:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 79.65
[32m[20230207 15:33:59 @agent_ppo2.py:150][0m Total time:      33.29 min
[32m[20230207 15:33:59 @agent_ppo2.py:152][0m 1761280 total steps have happened
[32m[20230207 15:33:59 @agent_ppo2.py:128][0m #------------------------ Iteration 860 --------------------------#
[32m[20230207 15:33:59 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:33:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:00 @agent_ppo2.py:192][0m |          -0.0047 |           9.2662 |           0.1661 |
[32m[20230207 15:34:00 @agent_ppo2.py:192][0m |          -0.0098 |           4.3131 |           0.1664 |
[32m[20230207 15:34:00 @agent_ppo2.py:192][0m |          -0.0135 |           3.8615 |           0.1664 |
[32m[20230207 15:34:00 @agent_ppo2.py:192][0m |           0.0011 |           3.6347 |           0.1664 |
[32m[20230207 15:34:00 @agent_ppo2.py:192][0m |          -0.0207 |           3.3553 |           0.1664 |
[32m[20230207 15:34:00 @agent_ppo2.py:192][0m |          -0.0499 |           3.3280 |           0.1665 |
[32m[20230207 15:34:00 @agent_ppo2.py:192][0m |          -0.0178 |           3.8329 |           0.1667 |
[32m[20230207 15:34:00 @agent_ppo2.py:192][0m |          -0.0108 |           3.2421 |           0.1669 |
[32m[20230207 15:34:00 @agent_ppo2.py:192][0m |           0.0050 |           2.9608 |           0.1668 |
[32m[20230207 15:34:00 @agent_ppo2.py:192][0m |          -0.0189 |           2.7752 |           0.1668 |
[32m[20230207 15:34:00 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:34:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: 104.43
[32m[20230207 15:34:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 138.53
[32m[20230207 15:34:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -88.37
[32m[20230207 15:34:01 @agent_ppo2.py:150][0m Total time:      33.33 min
[32m[20230207 15:34:01 @agent_ppo2.py:152][0m 1763328 total steps have happened
[32m[20230207 15:34:01 @agent_ppo2.py:128][0m #------------------------ Iteration 861 --------------------------#
[32m[20230207 15:34:02 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:34:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:02 @agent_ppo2.py:192][0m |          -0.0020 |           4.5284 |           0.1702 |
[32m[20230207 15:34:02 @agent_ppo2.py:192][0m |          -0.0081 |           2.6891 |           0.1697 |
[32m[20230207 15:34:02 @agent_ppo2.py:192][0m |          -0.0090 |           2.2539 |           0.1699 |
[32m[20230207 15:34:02 @agent_ppo2.py:192][0m |          -0.0088 |           2.1881 |           0.1697 |
[32m[20230207 15:34:02 @agent_ppo2.py:192][0m |          -0.0115 |           1.8847 |           0.1697 |
[32m[20230207 15:34:02 @agent_ppo2.py:192][0m |          -0.0123 |           1.5247 |           0.1695 |
[32m[20230207 15:34:02 @agent_ppo2.py:192][0m |          -0.0123 |           1.4158 |           0.1694 |
[32m[20230207 15:34:03 @agent_ppo2.py:192][0m |          -0.0146 |           1.3208 |           0.1693 |
[32m[20230207 15:34:03 @agent_ppo2.py:192][0m |          -0.0146 |           1.2812 |           0.1692 |
[32m[20230207 15:34:03 @agent_ppo2.py:192][0m |          -0.0150 |           1.2376 |           0.1693 |
[32m[20230207 15:34:03 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:34:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -8.95
[32m[20230207 15:34:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 103.84
[32m[20230207 15:34:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -62.50
[32m[20230207 15:34:03 @agent_ppo2.py:150][0m Total time:      33.37 min
[32m[20230207 15:34:03 @agent_ppo2.py:152][0m 1765376 total steps have happened
[32m[20230207 15:34:03 @agent_ppo2.py:128][0m #------------------------ Iteration 862 --------------------------#
[32m[20230207 15:34:04 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:34:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:04 @agent_ppo2.py:192][0m |           0.0010 |          11.3697 |           0.1632 |
[32m[20230207 15:34:04 @agent_ppo2.py:192][0m |          -0.0044 |           7.2020 |           0.1631 |
[32m[20230207 15:34:04 @agent_ppo2.py:192][0m |          -0.0072 |           5.4733 |           0.1630 |
[32m[20230207 15:34:04 @agent_ppo2.py:192][0m |          -0.0087 |           4.2364 |           0.1631 |
[32m[20230207 15:34:04 @agent_ppo2.py:192][0m |          -0.0105 |           3.5292 |           0.1631 |
[32m[20230207 15:34:05 @agent_ppo2.py:192][0m |          -0.0100 |           3.1677 |           0.1631 |
[32m[20230207 15:34:05 @agent_ppo2.py:192][0m |          -0.0097 |           2.8045 |           0.1629 |
[32m[20230207 15:34:05 @agent_ppo2.py:192][0m |          -0.0112 |           2.6157 |           0.1631 |
[32m[20230207 15:34:05 @agent_ppo2.py:192][0m |          -0.0132 |           2.4569 |           0.1630 |
[32m[20230207 15:34:05 @agent_ppo2.py:192][0m |          -0.0117 |           2.3134 |           0.1631 |
[32m[20230207 15:34:05 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:34:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: 2.09
[32m[20230207 15:34:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 19.81
[32m[20230207 15:34:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -9.68
[32m[20230207 15:34:06 @agent_ppo2.py:150][0m Total time:      33.41 min
[32m[20230207 15:34:06 @agent_ppo2.py:152][0m 1767424 total steps have happened
[32m[20230207 15:34:06 @agent_ppo2.py:128][0m #------------------------ Iteration 863 --------------------------#
[32m[20230207 15:34:06 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:34:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:06 @agent_ppo2.py:192][0m |          -0.0051 |          40.4642 |           0.1647 |
[32m[20230207 15:34:06 @agent_ppo2.py:192][0m |          -0.0086 |          29.4965 |           0.1644 |
[32m[20230207 15:34:06 @agent_ppo2.py:192][0m |          -0.0119 |          26.6028 |           0.1646 |
[32m[20230207 15:34:06 @agent_ppo2.py:192][0m |          -0.0139 |          25.0987 |           0.1644 |
[32m[20230207 15:34:06 @agent_ppo2.py:192][0m |          -0.0113 |          24.5521 |           0.1645 |
[32m[20230207 15:34:07 @agent_ppo2.py:192][0m |          -0.0113 |          22.9363 |           0.1643 |
[32m[20230207 15:34:07 @agent_ppo2.py:192][0m |          -0.0135 |          22.8271 |           0.1643 |
[32m[20230207 15:34:07 @agent_ppo2.py:192][0m |          -0.0156 |          20.8967 |           0.1643 |
[32m[20230207 15:34:07 @agent_ppo2.py:192][0m |          -0.0181 |          20.0857 |           0.1642 |
[32m[20230207 15:34:07 @agent_ppo2.py:192][0m |          -0.0196 |          19.4037 |           0.1643 |
[32m[20230207 15:34:07 @agent_ppo2.py:137][0m Policy update time: 0.53 s
[32m[20230207 15:34:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -58.60
[32m[20230207 15:34:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 72.19
[32m[20230207 15:34:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 12.61
[32m[20230207 15:34:08 @agent_ppo2.py:150][0m Total time:      33.44 min
[32m[20230207 15:34:08 @agent_ppo2.py:152][0m 1769472 total steps have happened
[32m[20230207 15:34:08 @agent_ppo2.py:128][0m #------------------------ Iteration 864 --------------------------#
[32m[20230207 15:34:08 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:34:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:09 @agent_ppo2.py:192][0m |           0.0024 |           9.4584 |           0.1659 |
[32m[20230207 15:34:09 @agent_ppo2.py:192][0m |          -0.0081 |           3.8665 |           0.1657 |
[32m[20230207 15:34:09 @agent_ppo2.py:192][0m |          -0.0094 |           3.1393 |           0.1660 |
[32m[20230207 15:34:09 @agent_ppo2.py:192][0m |          -0.0100 |           2.8040 |           0.1660 |
[32m[20230207 15:34:09 @agent_ppo2.py:192][0m |          -0.0138 |           2.5803 |           0.1659 |
[32m[20230207 15:34:09 @agent_ppo2.py:192][0m |          -0.0103 |           2.3939 |           0.1660 |
[32m[20230207 15:34:09 @agent_ppo2.py:192][0m |          -0.0105 |           2.2400 |           0.1659 |
[32m[20230207 15:34:09 @agent_ppo2.py:192][0m |          -0.0105 |           2.1288 |           0.1659 |
[32m[20230207 15:34:09 @agent_ppo2.py:192][0m |          -0.0139 |           2.0703 |           0.1659 |
[32m[20230207 15:34:09 @agent_ppo2.py:192][0m |          -0.0139 |           2.0075 |           0.1656 |
[32m[20230207 15:34:09 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:34:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: 35.33
[32m[20230207 15:34:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 126.24
[32m[20230207 15:34:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -49.13
[32m[20230207 15:34:10 @agent_ppo2.py:150][0m Total time:      33.48 min
[32m[20230207 15:34:10 @agent_ppo2.py:152][0m 1771520 total steps have happened
[32m[20230207 15:34:10 @agent_ppo2.py:128][0m #------------------------ Iteration 865 --------------------------#
[32m[20230207 15:34:11 @agent_ppo2.py:134][0m Sampling time: 0.66 s by 1 slaves
[32m[20230207 15:34:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:11 @agent_ppo2.py:192][0m |          -0.0012 |          28.1609 |           0.1669 |
[32m[20230207 15:34:11 @agent_ppo2.py:192][0m |          -0.0083 |          10.8565 |           0.1664 |
[32m[20230207 15:34:11 @agent_ppo2.py:192][0m |          -0.0082 |           9.3713 |           0.1664 |
[32m[20230207 15:34:11 @agent_ppo2.py:192][0m |          -0.0131 |           8.3508 |           0.1664 |
[32m[20230207 15:34:11 @agent_ppo2.py:192][0m |          -0.0154 |           7.5587 |           0.1663 |
[32m[20230207 15:34:11 @agent_ppo2.py:192][0m |          -0.0143 |           6.8550 |           0.1661 |
[32m[20230207 15:34:11 @agent_ppo2.py:192][0m |          -0.0131 |           6.1869 |           0.1659 |
[32m[20230207 15:34:11 @agent_ppo2.py:192][0m |          -0.0123 |           5.7377 |           0.1660 |
[32m[20230207 15:34:11 @agent_ppo2.py:192][0m |          -0.0194 |           5.3760 |           0.1659 |
[32m[20230207 15:34:11 @agent_ppo2.py:192][0m |          -0.0110 |           5.2087 |           0.1659 |
[32m[20230207 15:34:11 @agent_ppo2.py:137][0m Policy update time: 0.69 s
[32m[20230207 15:34:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: 23.23
[32m[20230207 15:34:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 112.18
[32m[20230207 15:34:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 28.72
[32m[20230207 15:34:12 @agent_ppo2.py:150][0m Total time:      33.52 min
[32m[20230207 15:34:12 @agent_ppo2.py:152][0m 1773568 total steps have happened
[32m[20230207 15:34:12 @agent_ppo2.py:128][0m #------------------------ Iteration 866 --------------------------#
[32m[20230207 15:34:13 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:34:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:13 @agent_ppo2.py:192][0m |           0.0028 |           2.7266 |           0.1637 |
[32m[20230207 15:34:13 @agent_ppo2.py:192][0m |          -0.0024 |           1.6710 |           0.1636 |
[32m[20230207 15:34:13 @agent_ppo2.py:192][0m |          -0.0132 |           1.5963 |           0.1637 |
[32m[20230207 15:34:13 @agent_ppo2.py:192][0m |          -0.0096 |           1.4924 |           0.1637 |
[32m[20230207 15:34:13 @agent_ppo2.py:192][0m |           0.0014 |           1.4671 |           0.1634 |
[32m[20230207 15:34:13 @agent_ppo2.py:192][0m |          -0.0230 |           1.4229 |           0.1634 |
[32m[20230207 15:34:14 @agent_ppo2.py:192][0m |          -0.0059 |           1.4044 |           0.1634 |
[32m[20230207 15:34:14 @agent_ppo2.py:192][0m |          -0.0108 |           1.3524 |           0.1625 |
[32m[20230207 15:34:14 @agent_ppo2.py:192][0m |          -0.0043 |           1.3320 |           0.1629 |
[32m[20230207 15:34:14 @agent_ppo2.py:192][0m |          -0.0171 |           1.3256 |           0.1629 |
[32m[20230207 15:34:14 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:34:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: 80.90
[32m[20230207 15:34:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 96.07
[32m[20230207 15:34:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 114.01
[32m[20230207 15:34:15 @agent_ppo2.py:150][0m Total time:      33.56 min
[32m[20230207 15:34:15 @agent_ppo2.py:152][0m 1775616 total steps have happened
[32m[20230207 15:34:15 @agent_ppo2.py:128][0m #------------------------ Iteration 867 --------------------------#
[32m[20230207 15:34:15 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:34:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:15 @agent_ppo2.py:192][0m |          -0.0040 |          11.4992 |           0.1624 |
[32m[20230207 15:34:16 @agent_ppo2.py:192][0m |          -0.0007 |           6.2223 |           0.1625 |
[32m[20230207 15:34:16 @agent_ppo2.py:192][0m |          -0.0096 |           5.6310 |           0.1627 |
[32m[20230207 15:34:16 @agent_ppo2.py:192][0m |          -0.0076 |           5.1565 |           0.1626 |
[32m[20230207 15:34:16 @agent_ppo2.py:192][0m |          -0.0104 |           4.8468 |           0.1626 |
[32m[20230207 15:34:16 @agent_ppo2.py:192][0m |          -0.0098 |           4.6686 |           0.1626 |
[32m[20230207 15:34:16 @agent_ppo2.py:192][0m |          -0.0148 |           4.4460 |           0.1627 |
[32m[20230207 15:34:16 @agent_ppo2.py:192][0m |          -0.0122 |           4.3042 |           0.1627 |
[32m[20230207 15:34:16 @agent_ppo2.py:192][0m |          -0.0145 |           4.2449 |           0.1628 |
[32m[20230207 15:34:16 @agent_ppo2.py:192][0m |          -0.0147 |           4.0414 |           0.1628 |
[32m[20230207 15:34:16 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:34:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: 32.82
[32m[20230207 15:34:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 111.41
[32m[20230207 15:34:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -4.38
[32m[20230207 15:34:17 @agent_ppo2.py:150][0m Total time:      33.60 min
[32m[20230207 15:34:17 @agent_ppo2.py:152][0m 1777664 total steps have happened
[32m[20230207 15:34:17 @agent_ppo2.py:128][0m #------------------------ Iteration 868 --------------------------#
[32m[20230207 15:34:18 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:34:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:18 @agent_ppo2.py:192][0m |          -0.0001 |          18.0807 |           0.1671 |
[32m[20230207 15:34:18 @agent_ppo2.py:192][0m |          -0.0043 |           7.1696 |           0.1673 |
[32m[20230207 15:34:18 @agent_ppo2.py:192][0m |          -0.0096 |           4.8998 |           0.1673 |
[32m[20230207 15:34:18 @agent_ppo2.py:192][0m |          -0.0109 |           3.9707 |           0.1672 |
[32m[20230207 15:34:18 @agent_ppo2.py:192][0m |          -0.0130 |           3.3196 |           0.1672 |
[32m[20230207 15:34:18 @agent_ppo2.py:192][0m |          -0.0121 |           2.9681 |           0.1673 |
[32m[20230207 15:34:18 @agent_ppo2.py:192][0m |          -0.0148 |           2.6821 |           0.1671 |
[32m[20230207 15:34:18 @agent_ppo2.py:192][0m |          -0.0150 |           2.4712 |           0.1672 |
[32m[20230207 15:34:18 @agent_ppo2.py:192][0m |          -0.0159 |           2.3467 |           0.1673 |
[32m[20230207 15:34:18 @agent_ppo2.py:192][0m |          -0.0167 |           2.1594 |           0.1673 |
[32m[20230207 15:34:18 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:34:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: -56.68
[32m[20230207 15:34:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 100.33
[32m[20230207 15:34:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -38.46
[32m[20230207 15:34:19 @agent_ppo2.py:150][0m Total time:      33.63 min
[32m[20230207 15:34:19 @agent_ppo2.py:152][0m 1779712 total steps have happened
[32m[20230207 15:34:19 @agent_ppo2.py:128][0m #------------------------ Iteration 869 --------------------------#
[32m[20230207 15:34:20 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:34:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:20 @agent_ppo2.py:192][0m |           0.0123 |           3.4908 |           0.1679 |
[32m[20230207 15:34:20 @agent_ppo2.py:192][0m |          -0.0008 |           2.3409 |           0.1677 |
[32m[20230207 15:34:20 @agent_ppo2.py:192][0m |          -0.0021 |           2.0286 |           0.1675 |
[32m[20230207 15:34:20 @agent_ppo2.py:192][0m |          -0.0190 |           1.9262 |           0.1674 |
[32m[20230207 15:34:20 @agent_ppo2.py:192][0m |          -0.0232 |           1.8168 |           0.1676 |
[32m[20230207 15:34:20 @agent_ppo2.py:192][0m |          -0.0631 |           1.7541 |           0.1675 |
[32m[20230207 15:34:20 @agent_ppo2.py:192][0m |          -0.0058 |           1.7116 |           0.1673 |
[32m[20230207 15:34:21 @agent_ppo2.py:192][0m |          -0.0151 |           1.7192 |           0.1674 |
[32m[20230207 15:34:21 @agent_ppo2.py:192][0m |           0.0346 |           1.7507 |           0.1677 |
[32m[20230207 15:34:21 @agent_ppo2.py:192][0m |          -0.0060 |           1.7358 |           0.1673 |
[32m[20230207 15:34:21 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:34:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: 65.11
[32m[20230207 15:34:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 94.73
[32m[20230207 15:34:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -12.65
[32m[20230207 15:34:21 @agent_ppo2.py:150][0m Total time:      33.67 min
[32m[20230207 15:34:21 @agent_ppo2.py:152][0m 1781760 total steps have happened
[32m[20230207 15:34:22 @agent_ppo2.py:128][0m #------------------------ Iteration 870 --------------------------#
[32m[20230207 15:34:22 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:34:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:22 @agent_ppo2.py:192][0m |           0.0126 |           7.0441 |           0.1682 |
[32m[20230207 15:34:23 @agent_ppo2.py:192][0m |          -0.0014 |           4.0747 |           0.1681 |
[32m[20230207 15:34:23 @agent_ppo2.py:192][0m |          -0.0109 |           3.4661 |           0.1681 |
[32m[20230207 15:34:23 @agent_ppo2.py:192][0m |          -0.0025 |           3.1671 |           0.1680 |
[32m[20230207 15:34:23 @agent_ppo2.py:192][0m |          -0.0128 |           3.0605 |           0.1679 |
[32m[20230207 15:34:23 @agent_ppo2.py:192][0m |          -0.0141 |           2.9854 |           0.1679 |
[32m[20230207 15:34:23 @agent_ppo2.py:192][0m |          -0.0198 |           2.9376 |           0.1679 |
[32m[20230207 15:34:23 @agent_ppo2.py:192][0m |          -0.0010 |           2.8964 |           0.1678 |
[32m[20230207 15:34:23 @agent_ppo2.py:192][0m |          -0.0103 |           2.8619 |           0.1678 |
[32m[20230207 15:34:23 @agent_ppo2.py:192][0m |          -0.0174 |           2.7855 |           0.1678 |
[32m[20230207 15:34:23 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:34:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: -6.15
[32m[20230207 15:34:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 107.99
[32m[20230207 15:34:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 6.02
[32m[20230207 15:34:24 @agent_ppo2.py:150][0m Total time:      33.71 min
[32m[20230207 15:34:24 @agent_ppo2.py:152][0m 1783808 total steps have happened
[32m[20230207 15:34:24 @agent_ppo2.py:128][0m #------------------------ Iteration 871 --------------------------#
[32m[20230207 15:34:25 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:34:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:25 @agent_ppo2.py:192][0m |          -0.0024 |           9.7222 |           0.1718 |
[32m[20230207 15:34:25 @agent_ppo2.py:192][0m |          -0.0071 |           7.1405 |           0.1716 |
[32m[20230207 15:34:25 @agent_ppo2.py:192][0m |          -0.0091 |           6.7985 |           0.1717 |
[32m[20230207 15:34:25 @agent_ppo2.py:192][0m |          -0.0108 |           6.5640 |           0.1717 |
[32m[20230207 15:34:25 @agent_ppo2.py:192][0m |          -0.0115 |           6.3855 |           0.1714 |
[32m[20230207 15:34:25 @agent_ppo2.py:192][0m |          -0.0130 |           6.2897 |           0.1715 |
[32m[20230207 15:34:26 @agent_ppo2.py:192][0m |          -0.0133 |           6.2039 |           0.1714 |
[32m[20230207 15:34:26 @agent_ppo2.py:192][0m |          -0.0135 |           6.0765 |           0.1714 |
[32m[20230207 15:34:26 @agent_ppo2.py:192][0m |          -0.0158 |           5.9997 |           0.1712 |
[32m[20230207 15:34:26 @agent_ppo2.py:192][0m |          -0.0157 |           5.9127 |           0.1713 |
[32m[20230207 15:34:26 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:34:26 @agent_ppo2.py:145][0m Average TRAINING episode reward: -34.96
[32m[20230207 15:34:26 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 125.89
[32m[20230207 15:34:26 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -68.52
[32m[20230207 15:34:26 @agent_ppo2.py:150][0m Total time:      33.75 min
[32m[20230207 15:34:26 @agent_ppo2.py:152][0m 1785856 total steps have happened
[32m[20230207 15:34:26 @agent_ppo2.py:128][0m #------------------------ Iteration 872 --------------------------#
[32m[20230207 15:34:27 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:34:27 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:28 @agent_ppo2.py:192][0m |           0.0030 |          22.3625 |           0.1674 |
[32m[20230207 15:34:28 @agent_ppo2.py:192][0m |          -0.0035 |          10.5914 |           0.1674 |
[32m[20230207 15:34:28 @agent_ppo2.py:192][0m |          -0.0043 |           8.0864 |           0.1672 |
[32m[20230207 15:34:28 @agent_ppo2.py:192][0m |          -0.0082 |           6.9104 |           0.1673 |
[32m[20230207 15:34:28 @agent_ppo2.py:192][0m |          -0.0080 |           6.1179 |           0.1672 |
[32m[20230207 15:34:28 @agent_ppo2.py:192][0m |          -0.0100 |           5.6489 |           0.1672 |
[32m[20230207 15:34:28 @agent_ppo2.py:192][0m |          -0.0108 |           5.0703 |           0.1672 |
[32m[20230207 15:34:28 @agent_ppo2.py:192][0m |          -0.0108 |           4.6842 |           0.1670 |
[32m[20230207 15:34:28 @agent_ppo2.py:192][0m |          -0.0136 |           4.3639 |           0.1671 |
[32m[20230207 15:34:28 @agent_ppo2.py:192][0m |          -0.0140 |           4.0723 |           0.1670 |
[32m[20230207 15:34:28 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:34:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -52.19
[32m[20230207 15:34:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 45.38
[32m[20230207 15:34:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -26.79
[32m[20230207 15:34:29 @agent_ppo2.py:150][0m Total time:      33.80 min
[32m[20230207 15:34:29 @agent_ppo2.py:152][0m 1787904 total steps have happened
[32m[20230207 15:34:29 @agent_ppo2.py:128][0m #------------------------ Iteration 873 --------------------------#
[32m[20230207 15:34:30 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:34:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:30 @agent_ppo2.py:192][0m |          -0.0000 |           8.6277 |           0.1674 |
[32m[20230207 15:34:30 @agent_ppo2.py:192][0m |          -0.0069 |           7.5731 |           0.1673 |
[32m[20230207 15:34:30 @agent_ppo2.py:192][0m |          -0.0066 |           7.2878 |           0.1672 |
[32m[20230207 15:34:30 @agent_ppo2.py:192][0m |          -0.0104 |           7.1444 |           0.1670 |
[32m[20230207 15:34:30 @agent_ppo2.py:192][0m |          -0.0108 |           7.0510 |           0.1672 |
[32m[20230207 15:34:30 @agent_ppo2.py:192][0m |          -0.0116 |           6.9350 |           0.1669 |
[32m[20230207 15:34:30 @agent_ppo2.py:192][0m |          -0.0118 |           6.8652 |           0.1668 |
[32m[20230207 15:34:31 @agent_ppo2.py:192][0m |          -0.0140 |           6.8177 |           0.1667 |
[32m[20230207 15:34:31 @agent_ppo2.py:192][0m |          -0.0146 |           6.7692 |           0.1669 |
[32m[20230207 15:34:31 @agent_ppo2.py:192][0m |          -0.0168 |           6.7282 |           0.1666 |
[32m[20230207 15:34:31 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:34:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: 24.74
[32m[20230207 15:34:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 147.53
[32m[20230207 15:34:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -3.10
[32m[20230207 15:34:31 @agent_ppo2.py:150][0m Total time:      33.84 min
[32m[20230207 15:34:31 @agent_ppo2.py:152][0m 1789952 total steps have happened
[32m[20230207 15:34:31 @agent_ppo2.py:128][0m #------------------------ Iteration 874 --------------------------#
[32m[20230207 15:34:32 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:34:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:32 @agent_ppo2.py:192][0m |           0.0032 |           8.4033 |           0.1680 |
[32m[20230207 15:34:32 @agent_ppo2.py:192][0m |           0.0080 |           5.7422 |           0.1680 |
[32m[20230207 15:34:33 @agent_ppo2.py:192][0m |           0.0018 |           4.9802 |           0.1675 |
[32m[20230207 15:34:33 @agent_ppo2.py:192][0m |          -0.0044 |           4.3904 |           0.1679 |
[32m[20230207 15:34:33 @agent_ppo2.py:192][0m |          -0.0085 |           3.8564 |           0.1676 |
[32m[20230207 15:34:33 @agent_ppo2.py:192][0m |          -0.0100 |           3.6602 |           0.1674 |
[32m[20230207 15:34:33 @agent_ppo2.py:192][0m |          -0.0044 |           3.9197 |           0.1676 |
[32m[20230207 15:34:33 @agent_ppo2.py:192][0m |          -0.0057 |           3.3278 |           0.1675 |
[32m[20230207 15:34:33 @agent_ppo2.py:192][0m |          -0.0088 |           3.2082 |           0.1672 |
[32m[20230207 15:34:33 @agent_ppo2.py:192][0m |          -0.0133 |           2.9225 |           0.1673 |
[32m[20230207 15:34:33 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:34:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: 44.14
[32m[20230207 15:34:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 95.01
[32m[20230207 15:34:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 92.74
[32m[20230207 15:34:34 @agent_ppo2.py:150][0m Total time:      33.88 min
[32m[20230207 15:34:34 @agent_ppo2.py:152][0m 1792000 total steps have happened
[32m[20230207 15:34:34 @agent_ppo2.py:128][0m #------------------------ Iteration 875 --------------------------#
[32m[20230207 15:34:35 @agent_ppo2.py:134][0m Sampling time: 0.71 s by 1 slaves
[32m[20230207 15:34:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:35 @agent_ppo2.py:192][0m |           0.0100 |          14.1394 |           0.1669 |
[32m[20230207 15:34:35 @agent_ppo2.py:192][0m |          -0.0004 |           6.5601 |           0.1668 |
[32m[20230207 15:34:35 @agent_ppo2.py:192][0m |          -0.0100 |           5.4216 |           0.1668 |
[32m[20230207 15:34:35 @agent_ppo2.py:192][0m |          -0.0109 |           4.8824 |           0.1666 |
[32m[20230207 15:34:35 @agent_ppo2.py:192][0m |          -0.0100 |           4.5299 |           0.1665 |
[32m[20230207 15:34:35 @agent_ppo2.py:192][0m |          -0.0101 |           4.4784 |           0.1663 |
[32m[20230207 15:34:35 @agent_ppo2.py:192][0m |          -0.0166 |           3.9365 |           0.1664 |
[32m[20230207 15:34:35 @agent_ppo2.py:192][0m |          -0.0172 |           3.9403 |           0.1663 |
[32m[20230207 15:34:35 @agent_ppo2.py:192][0m |          -0.0156 |           3.5313 |           0.1663 |
[32m[20230207 15:34:35 @agent_ppo2.py:192][0m |          -0.0179 |           3.4083 |           0.1664 |
[32m[20230207 15:34:35 @agent_ppo2.py:137][0m Policy update time: 0.72 s
[32m[20230207 15:34:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: 7.51
[32m[20230207 15:34:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 119.32
[32m[20230207 15:34:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -2.60
[32m[20230207 15:34:36 @agent_ppo2.py:150][0m Total time:      33.91 min
[32m[20230207 15:34:36 @agent_ppo2.py:152][0m 1794048 total steps have happened
[32m[20230207 15:34:36 @agent_ppo2.py:128][0m #------------------------ Iteration 876 --------------------------#
[32m[20230207 15:34:37 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:34:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:37 @agent_ppo2.py:192][0m |           0.0085 |          11.4009 |           0.1675 |
[32m[20230207 15:34:37 @agent_ppo2.py:192][0m |          -0.0064 |           5.9462 |           0.1673 |
[32m[20230207 15:34:37 @agent_ppo2.py:192][0m |          -0.0025 |           5.2313 |           0.1669 |
[32m[20230207 15:34:37 @agent_ppo2.py:192][0m |           0.0050 |           4.8545 |           0.1675 |
[32m[20230207 15:34:37 @agent_ppo2.py:192][0m |           0.0016 |           4.4469 |           0.1675 |
[32m[20230207 15:34:37 @agent_ppo2.py:192][0m |          -0.0001 |           4.5030 |           0.1675 |
[32m[20230207 15:34:37 @agent_ppo2.py:192][0m |          -0.0362 |           4.1713 |           0.1676 |
[32m[20230207 15:34:37 @agent_ppo2.py:192][0m |          -0.0046 |           3.9697 |           0.1669 |
[32m[20230207 15:34:37 @agent_ppo2.py:192][0m |           0.0013 |           4.0506 |           0.1673 |
[32m[20230207 15:34:37 @agent_ppo2.py:192][0m |           0.0026 |           3.8368 |           0.1677 |
[32m[20230207 15:34:37 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:34:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: 27.84
[32m[20230207 15:34:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 50.65
[32m[20230207 15:34:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 9.87
[32m[20230207 15:34:38 @agent_ppo2.py:150][0m Total time:      33.95 min
[32m[20230207 15:34:38 @agent_ppo2.py:152][0m 1796096 total steps have happened
[32m[20230207 15:34:38 @agent_ppo2.py:128][0m #------------------------ Iteration 877 --------------------------#
[32m[20230207 15:34:39 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:34:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:39 @agent_ppo2.py:192][0m |          -0.0080 |          21.0600 |           0.1598 |
[32m[20230207 15:34:39 @agent_ppo2.py:192][0m |          -0.0045 |           8.4197 |           0.1598 |
[32m[20230207 15:34:39 @agent_ppo2.py:192][0m |          -0.0087 |           6.8940 |           0.1597 |
[32m[20230207 15:34:39 @agent_ppo2.py:192][0m |          -0.0084 |           6.3917 |           0.1598 |
[32m[20230207 15:34:39 @agent_ppo2.py:192][0m |          -0.0171 |           5.6772 |           0.1598 |
[32m[20230207 15:34:39 @agent_ppo2.py:192][0m |          -0.0167 |           5.3489 |           0.1597 |
[32m[20230207 15:34:39 @agent_ppo2.py:192][0m |          -0.0203 |           4.9816 |           0.1597 |
[32m[20230207 15:34:39 @agent_ppo2.py:192][0m |          -0.0243 |           4.7294 |           0.1597 |
[32m[20230207 15:34:39 @agent_ppo2.py:192][0m |          -0.0199 |           4.5528 |           0.1597 |
[32m[20230207 15:34:40 @agent_ppo2.py:192][0m |          -0.0191 |           4.0938 |           0.1597 |
[32m[20230207 15:34:40 @agent_ppo2.py:137][0m Policy update time: 0.67 s
[32m[20230207 15:34:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: 26.25
[32m[20230207 15:34:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 72.77
[32m[20230207 15:34:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -93.21
[32m[20230207 15:34:40 @agent_ppo2.py:150][0m Total time:      33.98 min
[32m[20230207 15:34:40 @agent_ppo2.py:152][0m 1798144 total steps have happened
[32m[20230207 15:34:40 @agent_ppo2.py:128][0m #------------------------ Iteration 878 --------------------------#
[32m[20230207 15:34:41 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:34:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:41 @agent_ppo2.py:192][0m |           0.0093 |          12.7948 |           0.1678 |
[32m[20230207 15:34:41 @agent_ppo2.py:192][0m |           0.0010 |           9.4932 |           0.1680 |
[32m[20230207 15:34:41 @agent_ppo2.py:192][0m |           0.0028 |           8.2268 |           0.1681 |
[32m[20230207 15:34:41 @agent_ppo2.py:192][0m |          -0.0001 |           7.6618 |           0.1680 |
[32m[20230207 15:34:41 @agent_ppo2.py:192][0m |          -0.0052 |           7.2323 |           0.1679 |
[32m[20230207 15:34:42 @agent_ppo2.py:192][0m |          -0.0107 |           6.7472 |           0.1681 |
[32m[20230207 15:34:42 @agent_ppo2.py:192][0m |          -0.0196 |           6.4922 |           0.1681 |
[32m[20230207 15:34:42 @agent_ppo2.py:192][0m |          -0.0091 |           6.1814 |           0.1682 |
[32m[20230207 15:34:42 @agent_ppo2.py:192][0m |          -0.0076 |           5.9274 |           0.1682 |
[32m[20230207 15:34:42 @agent_ppo2.py:192][0m |          -0.0192 |           5.7128 |           0.1683 |
[32m[20230207 15:34:42 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:34:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -13.80
[32m[20230207 15:34:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 98.07
[32m[20230207 15:34:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 1.45
[32m[20230207 15:34:43 @agent_ppo2.py:150][0m Total time:      34.02 min
[32m[20230207 15:34:43 @agent_ppo2.py:152][0m 1800192 total steps have happened
[32m[20230207 15:34:43 @agent_ppo2.py:128][0m #------------------------ Iteration 879 --------------------------#
[32m[20230207 15:34:44 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:34:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:44 @agent_ppo2.py:192][0m |           0.0037 |          11.0693 |           0.1675 |
[32m[20230207 15:34:44 @agent_ppo2.py:192][0m |          -0.0058 |           5.8601 |           0.1674 |
[32m[20230207 15:34:44 @agent_ppo2.py:192][0m |          -0.0041 |           4.5173 |           0.1673 |
[32m[20230207 15:34:44 @agent_ppo2.py:192][0m |          -0.0094 |           3.8760 |           0.1671 |
[32m[20230207 15:34:44 @agent_ppo2.py:192][0m |          -0.0118 |           3.4575 |           0.1670 |
[32m[20230207 15:34:44 @agent_ppo2.py:192][0m |          -0.0096 |           3.2034 |           0.1669 |
[32m[20230207 15:34:44 @agent_ppo2.py:192][0m |          -0.0136 |           3.0461 |           0.1668 |
[32m[20230207 15:34:44 @agent_ppo2.py:192][0m |          -0.0123 |           2.8462 |           0.1667 |
[32m[20230207 15:34:44 @agent_ppo2.py:192][0m |          -0.0127 |           2.7505 |           0.1668 |
[32m[20230207 15:34:44 @agent_ppo2.py:192][0m |          -0.0155 |           2.6147 |           0.1666 |
[32m[20230207 15:34:44 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:34:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: -20.21
[32m[20230207 15:34:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 58.27
[32m[20230207 15:34:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 25.09
[32m[20230207 15:34:45 @agent_ppo2.py:150][0m Total time:      34.07 min
[32m[20230207 15:34:45 @agent_ppo2.py:152][0m 1802240 total steps have happened
[32m[20230207 15:34:45 @agent_ppo2.py:128][0m #------------------------ Iteration 880 --------------------------#
[32m[20230207 15:34:46 @agent_ppo2.py:134][0m Sampling time: 0.91 s by 1 slaves
[32m[20230207 15:34:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:46 @agent_ppo2.py:192][0m |          -0.0005 |           8.3044 |           0.1697 |
[32m[20230207 15:34:46 @agent_ppo2.py:192][0m |          -0.0059 |           5.2850 |           0.1694 |
[32m[20230207 15:34:47 @agent_ppo2.py:192][0m |          -0.0072 |           4.6440 |           0.1697 |
[32m[20230207 15:34:47 @agent_ppo2.py:192][0m |          -0.0093 |           4.3058 |           0.1695 |
[32m[20230207 15:34:47 @agent_ppo2.py:192][0m |          -0.0110 |           4.0428 |           0.1696 |
[32m[20230207 15:34:47 @agent_ppo2.py:192][0m |          -0.0117 |           3.8331 |           0.1697 |
[32m[20230207 15:34:47 @agent_ppo2.py:192][0m |          -0.0134 |           3.6760 |           0.1695 |
[32m[20230207 15:34:47 @agent_ppo2.py:192][0m |          -0.0130 |           3.5586 |           0.1696 |
[32m[20230207 15:34:47 @agent_ppo2.py:192][0m |          -0.0137 |           3.3937 |           0.1697 |
[32m[20230207 15:34:47 @agent_ppo2.py:192][0m |          -0.0160 |           3.2938 |           0.1697 |
[32m[20230207 15:34:47 @agent_ppo2.py:137][0m Policy update time: 0.92 s
[32m[20230207 15:34:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: 1.22
[32m[20230207 15:34:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 88.98
[32m[20230207 15:34:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 115.43
[32m[20230207 15:34:48 @agent_ppo2.py:150][0m Total time:      34.11 min
[32m[20230207 15:34:48 @agent_ppo2.py:152][0m 1804288 total steps have happened
[32m[20230207 15:34:48 @agent_ppo2.py:128][0m #------------------------ Iteration 881 --------------------------#
[32m[20230207 15:34:49 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:34:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:49 @agent_ppo2.py:192][0m |          -0.0286 |           2.4725 |           0.1725 |
[32m[20230207 15:34:49 @agent_ppo2.py:192][0m |           0.0100 |           2.2487 |           0.1724 |
[32m[20230207 15:34:49 @agent_ppo2.py:192][0m |          -0.0328 |           2.0364 |           0.1724 |
[32m[20230207 15:34:49 @agent_ppo2.py:192][0m |           0.0076 |           1.9978 |           0.1724 |
[32m[20230207 15:34:49 @agent_ppo2.py:192][0m |          -0.0071 |           1.9097 |           0.1725 |
[32m[20230207 15:34:49 @agent_ppo2.py:192][0m |          -0.0079 |           1.8636 |           0.1725 |
[32m[20230207 15:34:49 @agent_ppo2.py:192][0m |          -0.0270 |           1.8429 |           0.1729 |
[32m[20230207 15:34:49 @agent_ppo2.py:192][0m |          -0.0015 |           1.8726 |           0.1727 |
[32m[20230207 15:34:49 @agent_ppo2.py:192][0m |          -0.0234 |           1.8136 |           0.1727 |
[32m[20230207 15:34:50 @agent_ppo2.py:192][0m |           0.0092 |           1.7735 |           0.1729 |
[32m[20230207 15:34:50 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:34:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: 63.68
[32m[20230207 15:34:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 68.31
[32m[20230207 15:34:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -34.20
[32m[20230207 15:34:50 @agent_ppo2.py:150][0m Total time:      34.15 min
[32m[20230207 15:34:50 @agent_ppo2.py:152][0m 1806336 total steps have happened
[32m[20230207 15:34:50 @agent_ppo2.py:128][0m #------------------------ Iteration 882 --------------------------#
[32m[20230207 15:34:51 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:34:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:51 @agent_ppo2.py:192][0m |           0.0004 |           1.7645 |           0.1729 |
[32m[20230207 15:34:51 @agent_ppo2.py:192][0m |          -0.0035 |           1.4961 |           0.1725 |
[32m[20230207 15:34:51 @agent_ppo2.py:192][0m |          -0.0179 |           1.4620 |           0.1725 |
[32m[20230207 15:34:51 @agent_ppo2.py:192][0m |           0.0021 |           1.3943 |           0.1723 |
[32m[20230207 15:34:52 @agent_ppo2.py:192][0m |           0.0310 |           1.4653 |           0.1721 |
[32m[20230207 15:34:52 @agent_ppo2.py:192][0m |          -0.0259 |           1.3965 |           0.1721 |
[32m[20230207 15:34:52 @agent_ppo2.py:192][0m |          -0.0069 |           1.3394 |           0.1723 |
[32m[20230207 15:34:52 @agent_ppo2.py:192][0m |           0.0010 |           1.3223 |           0.1723 |
[32m[20230207 15:34:52 @agent_ppo2.py:192][0m |          -0.0041 |           1.3061 |           0.1725 |
[32m[20230207 15:34:52 @agent_ppo2.py:192][0m |          -0.0148 |           1.2674 |           0.1721 |
[32m[20230207 15:34:52 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:34:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: 103.86
[32m[20230207 15:34:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 123.27
[32m[20230207 15:34:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 41.32
[32m[20230207 15:34:53 @agent_ppo2.py:150][0m Total time:      34.19 min
[32m[20230207 15:34:53 @agent_ppo2.py:152][0m 1808384 total steps have happened
[32m[20230207 15:34:53 @agent_ppo2.py:128][0m #------------------------ Iteration 883 --------------------------#
[32m[20230207 15:34:53 @agent_ppo2.py:134][0m Sampling time: 0.72 s by 1 slaves
[32m[20230207 15:34:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:54 @agent_ppo2.py:192][0m |           0.0026 |           9.3079 |           0.1716 |
[32m[20230207 15:34:54 @agent_ppo2.py:192][0m |           0.0010 |           3.4610 |           0.1715 |
[32m[20230207 15:34:54 @agent_ppo2.py:192][0m |          -0.0013 |           2.7528 |           0.1713 |
[32m[20230207 15:34:54 @agent_ppo2.py:192][0m |          -0.0049 |           2.3831 |           0.1713 |
[32m[20230207 15:34:54 @agent_ppo2.py:192][0m |          -0.0064 |           2.1818 |           0.1712 |
[32m[20230207 15:34:54 @agent_ppo2.py:192][0m |          -0.0088 |           2.0531 |           0.1713 |
[32m[20230207 15:34:54 @agent_ppo2.py:192][0m |          -0.0102 |           1.9438 |           0.1713 |
[32m[20230207 15:34:54 @agent_ppo2.py:192][0m |          -0.0126 |           1.8754 |           0.1711 |
[32m[20230207 15:34:54 @agent_ppo2.py:192][0m |          -0.0134 |           1.8098 |           0.1712 |
[32m[20230207 15:34:54 @agent_ppo2.py:192][0m |          -0.0131 |           1.7567 |           0.1712 |
[32m[20230207 15:34:54 @agent_ppo2.py:137][0m Policy update time: 0.74 s
[32m[20230207 15:34:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: 5.73
[32m[20230207 15:34:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 40.63
[32m[20230207 15:34:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 63.33
[32m[20230207 15:34:55 @agent_ppo2.py:150][0m Total time:      34.23 min
[32m[20230207 15:34:55 @agent_ppo2.py:152][0m 1810432 total steps have happened
[32m[20230207 15:34:55 @agent_ppo2.py:128][0m #------------------------ Iteration 884 --------------------------#
[32m[20230207 15:34:56 @agent_ppo2.py:134][0m Sampling time: 0.73 s by 1 slaves
[32m[20230207 15:34:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:56 @agent_ppo2.py:192][0m |           0.0015 |          16.9193 |           0.1743 |
[32m[20230207 15:34:56 @agent_ppo2.py:192][0m |          -0.0039 |           6.0030 |           0.1741 |
[32m[20230207 15:34:56 @agent_ppo2.py:192][0m |          -0.0060 |           4.7120 |           0.1739 |
[32m[20230207 15:34:56 @agent_ppo2.py:192][0m |          -0.0073 |           3.8630 |           0.1740 |
[32m[20230207 15:34:56 @agent_ppo2.py:192][0m |          -0.0075 |           3.5009 |           0.1738 |
[32m[20230207 15:34:56 @agent_ppo2.py:192][0m |          -0.0088 |           3.1829 |           0.1738 |
[32m[20230207 15:34:56 @agent_ppo2.py:192][0m |          -0.0126 |           3.0061 |           0.1737 |
[32m[20230207 15:34:56 @agent_ppo2.py:192][0m |          -0.0094 |           2.8031 |           0.1737 |
[32m[20230207 15:34:56 @agent_ppo2.py:192][0m |          -0.0113 |           2.6269 |           0.1739 |
[32m[20230207 15:34:56 @agent_ppo2.py:192][0m |          -0.0111 |           2.4531 |           0.1739 |
[32m[20230207 15:34:56 @agent_ppo2.py:137][0m Policy update time: 0.74 s
[32m[20230207 15:34:57 @agent_ppo2.py:145][0m Average TRAINING episode reward: -50.68
[32m[20230207 15:34:57 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 84.98
[32m[20230207 15:34:57 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -62.50
[32m[20230207 15:34:57 @agent_ppo2.py:150][0m Total time:      34.26 min
[32m[20230207 15:34:57 @agent_ppo2.py:152][0m 1812480 total steps have happened
[32m[20230207 15:34:57 @agent_ppo2.py:128][0m #------------------------ Iteration 885 --------------------------#
[32m[20230207 15:34:58 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:34:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:34:58 @agent_ppo2.py:192][0m |          -0.0235 |           2.7918 |           0.1729 |
[32m[20230207 15:34:58 @agent_ppo2.py:192][0m |           0.0163 |           1.9407 |           0.1729 |
[32m[20230207 15:34:58 @agent_ppo2.py:192][0m |          -0.0066 |           1.7146 |           0.1726 |
[32m[20230207 15:34:58 @agent_ppo2.py:192][0m |          -0.0313 |           1.6130 |           0.1724 |
[32m[20230207 15:34:58 @agent_ppo2.py:192][0m |          -0.0029 |           1.5586 |           0.1724 |
[32m[20230207 15:34:58 @agent_ppo2.py:192][0m |          -0.0104 |           1.4817 |           0.1723 |
[32m[20230207 15:34:58 @agent_ppo2.py:192][0m |           0.0008 |           1.4385 |           0.1722 |
[32m[20230207 15:34:59 @agent_ppo2.py:192][0m |          -0.0074 |           1.4442 |           0.1720 |
[32m[20230207 15:34:59 @agent_ppo2.py:192][0m |          -0.0047 |           1.4416 |           0.1723 |
[32m[20230207 15:34:59 @agent_ppo2.py:192][0m |           0.0012 |           1.3788 |           0.1722 |
[32m[20230207 15:34:59 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:34:59 @agent_ppo2.py:145][0m Average TRAINING episode reward: 83.68
[32m[20230207 15:34:59 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 118.86
[32m[20230207 15:34:59 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 152.01
[32m[20230207 15:34:59 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 152.01
[32m[20230207 15:34:59 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 152.01
[32m[20230207 15:34:59 @agent_ppo2.py:150][0m Total time:      34.30 min
[32m[20230207 15:34:59 @agent_ppo2.py:152][0m 1814528 total steps have happened
[32m[20230207 15:34:59 @agent_ppo2.py:128][0m #------------------------ Iteration 886 --------------------------#
[32m[20230207 15:35:00 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:35:00 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:00 @agent_ppo2.py:192][0m |           0.0007 |          57.8543 |           0.1729 |
[32m[20230207 15:35:00 @agent_ppo2.py:192][0m |          -0.0063 |          24.5601 |           0.1726 |
[32m[20230207 15:35:01 @agent_ppo2.py:192][0m |          -0.0082 |          18.9105 |           0.1723 |
[32m[20230207 15:35:01 @agent_ppo2.py:192][0m |          -0.0110 |          15.9461 |           0.1721 |
[32m[20230207 15:35:01 @agent_ppo2.py:192][0m |          -0.0122 |          14.1100 |           0.1721 |
[32m[20230207 15:35:01 @agent_ppo2.py:192][0m |          -0.0134 |          12.4297 |           0.1721 |
[32m[20230207 15:35:01 @agent_ppo2.py:192][0m |          -0.0138 |          11.5261 |           0.1720 |
[32m[20230207 15:35:01 @agent_ppo2.py:192][0m |          -0.0160 |          10.8230 |           0.1719 |
[32m[20230207 15:35:01 @agent_ppo2.py:192][0m |          -0.0160 |          10.1439 |           0.1717 |
[32m[20230207 15:35:01 @agent_ppo2.py:192][0m |          -0.0165 |           9.3235 |           0.1718 |
[32m[20230207 15:35:01 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:35:02 @agent_ppo2.py:145][0m Average TRAINING episode reward: -71.82
[32m[20230207 15:35:02 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 60.40
[32m[20230207 15:35:02 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 122.45
[32m[20230207 15:35:02 @agent_ppo2.py:150][0m Total time:      34.35 min
[32m[20230207 15:35:02 @agent_ppo2.py:152][0m 1816576 total steps have happened
[32m[20230207 15:35:02 @agent_ppo2.py:128][0m #------------------------ Iteration 887 --------------------------#
[32m[20230207 15:35:03 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:35:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:03 @agent_ppo2.py:192][0m |          -0.0059 |           9.2990 |           0.1724 |
[32m[20230207 15:35:03 @agent_ppo2.py:192][0m |          -0.0027 |           5.6305 |           0.1719 |
[32m[20230207 15:35:03 @agent_ppo2.py:192][0m |          -0.0056 |           4.7674 |           0.1718 |
[32m[20230207 15:35:03 @agent_ppo2.py:192][0m |          -0.0127 |           4.4034 |           0.1718 |
[32m[20230207 15:35:03 @agent_ppo2.py:192][0m |          -0.0025 |           4.1419 |           0.1716 |
[32m[20230207 15:35:03 @agent_ppo2.py:192][0m |          -0.0144 |           3.8999 |           0.1716 |
[32m[20230207 15:35:03 @agent_ppo2.py:192][0m |          -0.0152 |           3.6939 |           0.1717 |
[32m[20230207 15:35:04 @agent_ppo2.py:192][0m |          -0.0108 |           3.5932 |           0.1716 |
[32m[20230207 15:35:04 @agent_ppo2.py:192][0m |          -0.0065 |           3.4849 |           0.1716 |
[32m[20230207 15:35:04 @agent_ppo2.py:192][0m |          -0.0098 |           3.3782 |           0.1715 |
[32m[20230207 15:35:04 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:35:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: -20.37
[32m[20230207 15:35:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 104.83
[32m[20230207 15:35:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -3.69
[32m[20230207 15:35:05 @agent_ppo2.py:150][0m Total time:      34.39 min
[32m[20230207 15:35:05 @agent_ppo2.py:152][0m 1818624 total steps have happened
[32m[20230207 15:35:05 @agent_ppo2.py:128][0m #------------------------ Iteration 888 --------------------------#
[32m[20230207 15:35:05 @agent_ppo2.py:134][0m Sampling time: 0.72 s by 1 slaves
[32m[20230207 15:35:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:05 @agent_ppo2.py:192][0m |          -0.0012 |          14.0904 |           0.1717 |
[32m[20230207 15:35:05 @agent_ppo2.py:192][0m |          -0.0058 |           9.0192 |           0.1718 |
[32m[20230207 15:35:06 @agent_ppo2.py:192][0m |          -0.0080 |           7.5709 |           0.1717 |
[32m[20230207 15:35:06 @agent_ppo2.py:192][0m |          -0.0114 |           7.0711 |           0.1716 |
[32m[20230207 15:35:06 @agent_ppo2.py:192][0m |          -0.0119 |           6.5211 |           0.1714 |
[32m[20230207 15:35:06 @agent_ppo2.py:192][0m |          -0.0143 |           6.2376 |           0.1715 |
[32m[20230207 15:35:06 @agent_ppo2.py:192][0m |          -0.0143 |           5.9819 |           0.1713 |
[32m[20230207 15:35:06 @agent_ppo2.py:192][0m |          -0.0148 |           5.7479 |           0.1713 |
[32m[20230207 15:35:06 @agent_ppo2.py:192][0m |          -0.0170 |           5.5744 |           0.1713 |
[32m[20230207 15:35:06 @agent_ppo2.py:192][0m |          -0.0157 |           5.4009 |           0.1712 |
[32m[20230207 15:35:06 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230207 15:35:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: 12.10
[32m[20230207 15:35:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 109.66
[32m[20230207 15:35:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -59.72
[32m[20230207 15:35:07 @agent_ppo2.py:150][0m Total time:      34.43 min
[32m[20230207 15:35:07 @agent_ppo2.py:152][0m 1820672 total steps have happened
[32m[20230207 15:35:07 @agent_ppo2.py:128][0m #------------------------ Iteration 889 --------------------------#
[32m[20230207 15:35:07 @agent_ppo2.py:134][0m Sampling time: 0.54 s by 1 slaves
[32m[20230207 15:35:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:07 @agent_ppo2.py:192][0m |           0.0014 |          25.4481 |           0.1733 |
[32m[20230207 15:35:07 @agent_ppo2.py:192][0m |          -0.0043 |          14.3054 |           0.1730 |
[32m[20230207 15:35:08 @agent_ppo2.py:192][0m |          -0.0075 |          12.4549 |           0.1730 |
[32m[20230207 15:35:08 @agent_ppo2.py:192][0m |          -0.0098 |          11.4949 |           0.1731 |
[32m[20230207 15:35:08 @agent_ppo2.py:192][0m |          -0.0109 |          10.3725 |           0.1730 |
[32m[20230207 15:35:08 @agent_ppo2.py:192][0m |          -0.0126 |           9.8015 |           0.1731 |
[32m[20230207 15:35:08 @agent_ppo2.py:192][0m |          -0.0122 |           8.9215 |           0.1733 |
[32m[20230207 15:35:08 @agent_ppo2.py:192][0m |          -0.0136 |           8.4569 |           0.1732 |
[32m[20230207 15:35:08 @agent_ppo2.py:192][0m |          -0.0139 |           7.8671 |           0.1733 |
[32m[20230207 15:35:08 @agent_ppo2.py:192][0m |          -0.0152 |           7.5191 |           0.1733 |
[32m[20230207 15:35:08 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:35:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: -25.64
[32m[20230207 15:35:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 94.97
[32m[20230207 15:35:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 0.56
[32m[20230207 15:35:09 @agent_ppo2.py:150][0m Total time:      34.46 min
[32m[20230207 15:35:09 @agent_ppo2.py:152][0m 1822720 total steps have happened
[32m[20230207 15:35:09 @agent_ppo2.py:128][0m #------------------------ Iteration 890 --------------------------#
[32m[20230207 15:35:09 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:35:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:09 @agent_ppo2.py:192][0m |           0.0009 |           9.5666 |           0.1755 |
[32m[20230207 15:35:09 @agent_ppo2.py:192][0m |          -0.0042 |           5.0311 |           0.1752 |
[32m[20230207 15:35:09 @agent_ppo2.py:192][0m |          -0.0083 |           3.8785 |           0.1752 |
[32m[20230207 15:35:10 @agent_ppo2.py:192][0m |          -0.0094 |           3.3519 |           0.1751 |
[32m[20230207 15:35:10 @agent_ppo2.py:192][0m |          -0.0104 |           3.1826 |           0.1752 |
[32m[20230207 15:35:10 @agent_ppo2.py:192][0m |          -0.0112 |           2.8993 |           0.1752 |
[32m[20230207 15:35:10 @agent_ppo2.py:192][0m |          -0.0128 |           2.7359 |           0.1752 |
[32m[20230207 15:35:10 @agent_ppo2.py:192][0m |          -0.0133 |           2.6102 |           0.1751 |
[32m[20230207 15:35:10 @agent_ppo2.py:192][0m |          -0.0140 |           2.4961 |           0.1752 |
[32m[20230207 15:35:10 @agent_ppo2.py:192][0m |          -0.0148 |           2.4109 |           0.1752 |
[32m[20230207 15:35:10 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:35:11 @agent_ppo2.py:145][0m Average TRAINING episode reward: 5.67
[32m[20230207 15:35:11 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 98.20
[32m[20230207 15:35:11 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 79.65
[32m[20230207 15:35:11 @agent_ppo2.py:150][0m Total time:      34.49 min
[32m[20230207 15:35:11 @agent_ppo2.py:152][0m 1824768 total steps have happened
[32m[20230207 15:35:11 @agent_ppo2.py:128][0m #------------------------ Iteration 891 --------------------------#
[32m[20230207 15:35:12 @agent_ppo2.py:134][0m Sampling time: 0.91 s by 1 slaves
[32m[20230207 15:35:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:12 @agent_ppo2.py:192][0m |          -0.0003 |          15.8293 |           0.1736 |
[32m[20230207 15:35:12 @agent_ppo2.py:192][0m |          -0.0064 |           7.6617 |           0.1740 |
[32m[20230207 15:35:12 @agent_ppo2.py:192][0m |          -0.0085 |           6.6042 |           0.1740 |
[32m[20230207 15:35:12 @agent_ppo2.py:192][0m |          -0.0088 |           5.9012 |           0.1738 |
[32m[20230207 15:35:12 @agent_ppo2.py:192][0m |          -0.0100 |           5.3421 |           0.1739 |
[32m[20230207 15:35:12 @agent_ppo2.py:192][0m |          -0.0120 |           5.0125 |           0.1738 |
[32m[20230207 15:35:12 @agent_ppo2.py:192][0m |          -0.0124 |           4.6729 |           0.1738 |
[32m[20230207 15:35:12 @agent_ppo2.py:192][0m |          -0.0137 |           4.3947 |           0.1738 |
[32m[20230207 15:35:12 @agent_ppo2.py:192][0m |          -0.0151 |           4.2392 |           0.1738 |
[32m[20230207 15:35:12 @agent_ppo2.py:192][0m |          -0.0152 |           4.0293 |           0.1736 |
[32m[20230207 15:35:12 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:35:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -21.22
[32m[20230207 15:35:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 151.88
[32m[20230207 15:35:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 16.65
[32m[20230207 15:35:13 @agent_ppo2.py:150][0m Total time:      34.53 min
[32m[20230207 15:35:13 @agent_ppo2.py:152][0m 1826816 total steps have happened
[32m[20230207 15:35:13 @agent_ppo2.py:128][0m #------------------------ Iteration 892 --------------------------#
[32m[20230207 15:35:14 @agent_ppo2.py:134][0m Sampling time: 0.74 s by 1 slaves
[32m[20230207 15:35:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:14 @agent_ppo2.py:192][0m |          -0.0016 |          28.1543 |           0.1700 |
[32m[20230207 15:35:14 @agent_ppo2.py:192][0m |          -0.0031 |          21.2942 |           0.1698 |
[32m[20230207 15:35:14 @agent_ppo2.py:192][0m |          -0.0140 |          19.0654 |           0.1697 |
[32m[20230207 15:35:14 @agent_ppo2.py:192][0m |          -0.0090 |          18.1771 |           0.1696 |
[32m[20230207 15:35:14 @agent_ppo2.py:192][0m |          -0.0111 |          17.4706 |           0.1694 |
[32m[20230207 15:35:14 @agent_ppo2.py:192][0m |          -0.0154 |          17.0960 |           0.1696 |
[32m[20230207 15:35:15 @agent_ppo2.py:192][0m |           0.0039 |          16.8668 |           0.1696 |
[32m[20230207 15:35:15 @agent_ppo2.py:192][0m |          -0.0161 |          16.7613 |           0.1693 |
[32m[20230207 15:35:15 @agent_ppo2.py:192][0m |          -0.0089 |          16.1286 |           0.1691 |
[32m[20230207 15:35:15 @agent_ppo2.py:192][0m |          -0.0033 |          16.0131 |           0.1694 |
[32m[20230207 15:35:15 @agent_ppo2.py:137][0m Policy update time: 0.74 s
[32m[20230207 15:35:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -56.02
[32m[20230207 15:35:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 73.09
[32m[20230207 15:35:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -25.88
[32m[20230207 15:35:16 @agent_ppo2.py:150][0m Total time:      34.57 min
[32m[20230207 15:35:16 @agent_ppo2.py:152][0m 1828864 total steps have happened
[32m[20230207 15:35:16 @agent_ppo2.py:128][0m #------------------------ Iteration 893 --------------------------#
[32m[20230207 15:35:16 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:35:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:17 @agent_ppo2.py:192][0m |          -0.0003 |          13.8887 |           0.1742 |
[32m[20230207 15:35:17 @agent_ppo2.py:192][0m |          -0.0012 |           6.6105 |           0.1739 |
[32m[20230207 15:35:17 @agent_ppo2.py:192][0m |          -0.0083 |           5.5789 |           0.1738 |
[32m[20230207 15:35:17 @agent_ppo2.py:192][0m |          -0.0140 |           4.9742 |           0.1738 |
[32m[20230207 15:35:17 @agent_ppo2.py:192][0m |          -0.0133 |           4.6940 |           0.1738 |
[32m[20230207 15:35:17 @agent_ppo2.py:192][0m |          -0.0114 |           4.4062 |           0.1735 |
[32m[20230207 15:35:17 @agent_ppo2.py:192][0m |          -0.0141 |           4.1598 |           0.1736 |
[32m[20230207 15:35:17 @agent_ppo2.py:192][0m |          -0.0150 |           4.0501 |           0.1736 |
[32m[20230207 15:35:17 @agent_ppo2.py:192][0m |          -0.0159 |           3.8449 |           0.1737 |
[32m[20230207 15:35:17 @agent_ppo2.py:192][0m |          -0.0184 |           3.6656 |           0.1735 |
[32m[20230207 15:35:17 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:35:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -5.35
[32m[20230207 15:35:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 99.58
[32m[20230207 15:35:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 92.80
[32m[20230207 15:35:18 @agent_ppo2.py:150][0m Total time:      34.62 min
[32m[20230207 15:35:18 @agent_ppo2.py:152][0m 1830912 total steps have happened
[32m[20230207 15:35:18 @agent_ppo2.py:128][0m #------------------------ Iteration 894 --------------------------#
[32m[20230207 15:35:19 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:35:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:19 @agent_ppo2.py:192][0m |          -0.0037 |          34.8136 |           0.1690 |
[32m[20230207 15:35:19 @agent_ppo2.py:192][0m |          -0.0147 |          19.7638 |           0.1689 |
[32m[20230207 15:35:19 @agent_ppo2.py:192][0m |          -0.0150 |          16.3529 |           0.1686 |
[32m[20230207 15:35:19 @agent_ppo2.py:192][0m |          -0.0039 |          15.5949 |           0.1687 |
[32m[20230207 15:35:19 @agent_ppo2.py:192][0m |           0.0152 |          16.5307 |           0.1684 |
[32m[20230207 15:35:19 @agent_ppo2.py:192][0m |          -0.0183 |          10.5778 |           0.1683 |
[32m[20230207 15:35:20 @agent_ppo2.py:192][0m |          -0.0100 |           9.2601 |           0.1683 |
[32m[20230207 15:35:20 @agent_ppo2.py:192][0m |          -0.0162 |           8.2240 |           0.1683 |
[32m[20230207 15:35:20 @agent_ppo2.py:192][0m |          -0.0173 |           7.5548 |           0.1682 |
[32m[20230207 15:35:20 @agent_ppo2.py:192][0m |          -0.0184 |           7.2079 |           0.1682 |
[32m[20230207 15:35:20 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:35:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -74.89
[32m[20230207 15:35:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -35.39
[32m[20230207 15:35:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 113.08
[32m[20230207 15:35:21 @agent_ppo2.py:150][0m Total time:      34.66 min
[32m[20230207 15:35:21 @agent_ppo2.py:152][0m 1832960 total steps have happened
[32m[20230207 15:35:21 @agent_ppo2.py:128][0m #------------------------ Iteration 895 --------------------------#
[32m[20230207 15:35:21 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:35:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:22 @agent_ppo2.py:192][0m |           0.0017 |           7.9752 |           0.1731 |
[32m[20230207 15:35:22 @agent_ppo2.py:192][0m |          -0.0049 |           5.3782 |           0.1728 |
[32m[20230207 15:35:22 @agent_ppo2.py:192][0m |          -0.0063 |           4.4367 |           0.1725 |
[32m[20230207 15:35:22 @agent_ppo2.py:192][0m |          -0.0099 |           3.8741 |           0.1723 |
[32m[20230207 15:35:22 @agent_ppo2.py:192][0m |          -0.0095 |           3.5371 |           0.1722 |
[32m[20230207 15:35:22 @agent_ppo2.py:192][0m |          -0.0097 |           3.2677 |           0.1722 |
[32m[20230207 15:35:22 @agent_ppo2.py:192][0m |          -0.0149 |           3.1561 |           0.1722 |
[32m[20230207 15:35:22 @agent_ppo2.py:192][0m |          -0.0157 |           3.0044 |           0.1720 |
[32m[20230207 15:35:22 @agent_ppo2.py:192][0m |          -0.0140 |           2.8790 |           0.1721 |
[32m[20230207 15:35:22 @agent_ppo2.py:192][0m |          -0.0157 |           2.7719 |           0.1723 |
[32m[20230207 15:35:22 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:35:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: 4.34
[32m[20230207 15:35:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 102.91
[32m[20230207 15:35:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 50.25
[32m[20230207 15:35:23 @agent_ppo2.py:150][0m Total time:      34.70 min
[32m[20230207 15:35:23 @agent_ppo2.py:152][0m 1835008 total steps have happened
[32m[20230207 15:35:23 @agent_ppo2.py:128][0m #------------------------ Iteration 896 --------------------------#
[32m[20230207 15:35:24 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 15:35:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:24 @agent_ppo2.py:192][0m |           0.0007 |          14.1918 |           0.1715 |
[32m[20230207 15:35:24 @agent_ppo2.py:192][0m |          -0.0056 |           6.9833 |           0.1710 |
[32m[20230207 15:35:24 @agent_ppo2.py:192][0m |          -0.0104 |           6.1987 |           0.1708 |
[32m[20230207 15:35:24 @agent_ppo2.py:192][0m |          -0.0104 |           5.7726 |           0.1708 |
[32m[20230207 15:35:24 @agent_ppo2.py:192][0m |          -0.0155 |           5.4562 |           0.1708 |
[32m[20230207 15:35:24 @agent_ppo2.py:192][0m |          -0.0131 |           5.0594 |           0.1706 |
[32m[20230207 15:35:24 @agent_ppo2.py:192][0m |          -0.0140 |           4.8152 |           0.1707 |
[32m[20230207 15:35:24 @agent_ppo2.py:192][0m |          -0.0156 |           4.6460 |           0.1706 |
[32m[20230207 15:35:24 @agent_ppo2.py:192][0m |          -0.0127 |           4.5382 |           0.1705 |
[32m[20230207 15:35:24 @agent_ppo2.py:192][0m |          -0.0175 |           4.3496 |           0.1707 |
[32m[20230207 15:35:24 @agent_ppo2.py:137][0m Policy update time: 0.72 s
[32m[20230207 15:35:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -3.64
[32m[20230207 15:35:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 124.38
[32m[20230207 15:35:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 24.28
[32m[20230207 15:35:25 @agent_ppo2.py:150][0m Total time:      34.74 min
[32m[20230207 15:35:25 @agent_ppo2.py:152][0m 1837056 total steps have happened
[32m[20230207 15:35:25 @agent_ppo2.py:128][0m #------------------------ Iteration 897 --------------------------#
[32m[20230207 15:35:26 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:35:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:26 @agent_ppo2.py:192][0m |           0.0050 |           9.8510 |           0.1737 |
[32m[20230207 15:35:26 @agent_ppo2.py:192][0m |          -0.0004 |           4.6423 |           0.1736 |
[32m[20230207 15:35:26 @agent_ppo2.py:192][0m |          -0.0004 |           3.6145 |           0.1737 |
[32m[20230207 15:35:27 @agent_ppo2.py:192][0m |          -0.0056 |           3.1719 |           0.1736 |
[32m[20230207 15:35:27 @agent_ppo2.py:192][0m |          -0.0082 |           2.8941 |           0.1737 |
[32m[20230207 15:35:27 @agent_ppo2.py:192][0m |          -0.0084 |           2.7964 |           0.1737 |
[32m[20230207 15:35:27 @agent_ppo2.py:192][0m |          -0.0113 |           2.6782 |           0.1735 |
[32m[20230207 15:35:27 @agent_ppo2.py:192][0m |          -0.0087 |           2.5540 |           0.1735 |
[32m[20230207 15:35:27 @agent_ppo2.py:192][0m |          -0.0107 |           2.5158 |           0.1736 |
[32m[20230207 15:35:27 @agent_ppo2.py:192][0m |          -0.0123 |           2.4516 |           0.1736 |
[32m[20230207 15:35:27 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:35:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: 41.57
[32m[20230207 15:35:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 113.32
[32m[20230207 15:35:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 69.73
[32m[20230207 15:35:28 @agent_ppo2.py:150][0m Total time:      34.78 min
[32m[20230207 15:35:28 @agent_ppo2.py:152][0m 1839104 total steps have happened
[32m[20230207 15:35:28 @agent_ppo2.py:128][0m #------------------------ Iteration 898 --------------------------#
[32m[20230207 15:35:28 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 15:35:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:29 @agent_ppo2.py:192][0m |          -0.0001 |          31.9812 |           0.1730 |
[32m[20230207 15:35:29 @agent_ppo2.py:192][0m |          -0.0049 |          16.7912 |           0.1729 |
[32m[20230207 15:35:29 @agent_ppo2.py:192][0m |          -0.0076 |          12.7635 |           0.1729 |
[32m[20230207 15:35:29 @agent_ppo2.py:192][0m |          -0.0090 |          11.2011 |           0.1727 |
[32m[20230207 15:35:29 @agent_ppo2.py:192][0m |          -0.0091 |          10.2337 |           0.1729 |
[32m[20230207 15:35:29 @agent_ppo2.py:192][0m |          -0.0125 |           9.5716 |           0.1728 |
[32m[20230207 15:35:29 @agent_ppo2.py:192][0m |          -0.0121 |           8.9746 |           0.1727 |
[32m[20230207 15:35:29 @agent_ppo2.py:192][0m |          -0.0132 |           8.4540 |           0.1728 |
[32m[20230207 15:35:29 @agent_ppo2.py:192][0m |          -0.0138 |           8.0740 |           0.1728 |
[32m[20230207 15:35:29 @agent_ppo2.py:192][0m |          -0.0149 |           7.7617 |           0.1727 |
[32m[20230207 15:35:29 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:35:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -6.18
[32m[20230207 15:35:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 92.50
[32m[20230207 15:35:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 109.13
[32m[20230207 15:35:30 @agent_ppo2.py:150][0m Total time:      34.81 min
[32m[20230207 15:35:30 @agent_ppo2.py:152][0m 1841152 total steps have happened
[32m[20230207 15:35:30 @agent_ppo2.py:128][0m #------------------------ Iteration 899 --------------------------#
[32m[20230207 15:35:31 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:35:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:31 @agent_ppo2.py:192][0m |           0.0090 |           9.1991 |           0.1729 |
[32m[20230207 15:35:31 @agent_ppo2.py:192][0m |          -0.0024 |           6.4801 |           0.1729 |
[32m[20230207 15:35:31 @agent_ppo2.py:192][0m |          -0.0065 |           6.1015 |           0.1727 |
[32m[20230207 15:35:31 @agent_ppo2.py:192][0m |          -0.0040 |           5.8927 |           0.1727 |
[32m[20230207 15:35:31 @agent_ppo2.py:192][0m |          -0.0028 |           5.6862 |           0.1727 |
[32m[20230207 15:35:31 @agent_ppo2.py:192][0m |           0.0165 |           5.5876 |           0.1727 |
[32m[20230207 15:35:31 @agent_ppo2.py:192][0m |           0.0019 |           5.4542 |           0.1727 |
[32m[20230207 15:35:31 @agent_ppo2.py:192][0m |          -0.0193 |           5.3829 |           0.1728 |
[32m[20230207 15:35:31 @agent_ppo2.py:192][0m |          -0.0034 |           5.5083 |           0.1728 |
[32m[20230207 15:35:31 @agent_ppo2.py:192][0m |           0.0245 |           7.2858 |           0.1727 |
[32m[20230207 15:35:31 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:35:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: 77.79
[32m[20230207 15:35:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 129.27
[32m[20230207 15:35:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 25.35
[32m[20230207 15:35:32 @agent_ppo2.py:150][0m Total time:      34.85 min
[32m[20230207 15:35:32 @agent_ppo2.py:152][0m 1843200 total steps have happened
[32m[20230207 15:35:32 @agent_ppo2.py:128][0m #------------------------ Iteration 900 --------------------------#
[32m[20230207 15:35:33 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 15:35:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:33 @agent_ppo2.py:192][0m |           0.0003 |          24.4565 |           0.1692 |
[32m[20230207 15:35:33 @agent_ppo2.py:192][0m |           0.0008 |           9.4896 |           0.1690 |
[32m[20230207 15:35:33 @agent_ppo2.py:192][0m |          -0.0102 |           7.5576 |           0.1688 |
[32m[20230207 15:35:33 @agent_ppo2.py:192][0m |          -0.0100 |           6.2970 |           0.1688 |
[32m[20230207 15:35:33 @agent_ppo2.py:192][0m |          -0.0170 |           5.5195 |           0.1687 |
[32m[20230207 15:35:33 @agent_ppo2.py:192][0m |          -0.0126 |           5.0843 |           0.1685 |
[32m[20230207 15:35:33 @agent_ppo2.py:192][0m |          -0.0211 |           4.7969 |           0.1686 |
[32m[20230207 15:35:33 @agent_ppo2.py:192][0m |          -0.0195 |           4.5387 |           0.1686 |
[32m[20230207 15:35:33 @agent_ppo2.py:192][0m |          -0.0210 |           4.3837 |           0.1686 |
[32m[20230207 15:35:33 @agent_ppo2.py:192][0m |          -0.0243 |           4.1900 |           0.1683 |
[32m[20230207 15:35:33 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:35:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: -56.63
[32m[20230207 15:35:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 94.03
[32m[20230207 15:35:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 6.99
[32m[20230207 15:35:34 @agent_ppo2.py:150][0m Total time:      34.88 min
[32m[20230207 15:35:34 @agent_ppo2.py:152][0m 1845248 total steps have happened
[32m[20230207 15:35:34 @agent_ppo2.py:128][0m #------------------------ Iteration 901 --------------------------#
[32m[20230207 15:35:35 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:35:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:35 @agent_ppo2.py:192][0m |           0.0015 |          16.2712 |           0.1732 |
[32m[20230207 15:35:35 @agent_ppo2.py:192][0m |          -0.0058 |           7.1197 |           0.1731 |
[32m[20230207 15:35:35 @agent_ppo2.py:192][0m |          -0.0041 |           5.2162 |           0.1731 |
[32m[20230207 15:35:35 @agent_ppo2.py:192][0m |          -0.0118 |           4.1460 |           0.1728 |
[32m[20230207 15:35:35 @agent_ppo2.py:192][0m |          -0.0102 |           3.6876 |           0.1727 |
[32m[20230207 15:35:35 @agent_ppo2.py:192][0m |          -0.0110 |           3.2884 |           0.1725 |
[32m[20230207 15:35:35 @agent_ppo2.py:192][0m |          -0.0144 |           3.1760 |           0.1724 |
[32m[20230207 15:35:35 @agent_ppo2.py:192][0m |          -0.0124 |           2.8612 |           0.1723 |
[32m[20230207 15:35:36 @agent_ppo2.py:192][0m |          -0.0162 |           2.7463 |           0.1722 |
[32m[20230207 15:35:36 @agent_ppo2.py:192][0m |          -0.0181 |           2.6337 |           0.1721 |
[32m[20230207 15:35:36 @agent_ppo2.py:137][0m Policy update time: 0.67 s
[32m[20230207 15:35:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: 19.06
[32m[20230207 15:35:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 98.63
[32m[20230207 15:35:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 36.13
[32m[20230207 15:35:36 @agent_ppo2.py:150][0m Total time:      34.92 min
[32m[20230207 15:35:36 @agent_ppo2.py:152][0m 1847296 total steps have happened
[32m[20230207 15:35:36 @agent_ppo2.py:128][0m #------------------------ Iteration 902 --------------------------#
[32m[20230207 15:35:37 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:35:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:37 @agent_ppo2.py:192][0m |          -0.0237 |           4.4724 |           0.1670 |
[32m[20230207 15:35:37 @agent_ppo2.py:192][0m |           0.0047 |           2.8428 |           0.1668 |
[32m[20230207 15:35:37 @agent_ppo2.py:192][0m |          -0.0173 |           2.6670 |           0.1667 |
[32m[20230207 15:35:37 @agent_ppo2.py:192][0m |          -0.0065 |           2.5431 |           0.1668 |
[32m[20230207 15:35:37 @agent_ppo2.py:192][0m |          -0.0111 |           2.4554 |           0.1666 |
[32m[20230207 15:35:38 @agent_ppo2.py:192][0m |           0.0219 |           2.7928 |           0.1667 |
[32m[20230207 15:35:38 @agent_ppo2.py:192][0m |          -0.0139 |           2.5007 |           0.1664 |
[32m[20230207 15:35:38 @agent_ppo2.py:192][0m |          -0.0078 |           2.3337 |           0.1666 |
[32m[20230207 15:35:38 @agent_ppo2.py:192][0m |          -0.0100 |           2.3019 |           0.1664 |
[32m[20230207 15:35:38 @agent_ppo2.py:192][0m |          -0.0025 |           2.3042 |           0.1665 |
[32m[20230207 15:35:38 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:35:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: 60.34
[32m[20230207 15:35:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 154.47
[32m[20230207 15:35:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 49.91
[32m[20230207 15:35:39 @agent_ppo2.py:150][0m Total time:      34.96 min
[32m[20230207 15:35:39 @agent_ppo2.py:152][0m 1849344 total steps have happened
[32m[20230207 15:35:39 @agent_ppo2.py:128][0m #------------------------ Iteration 903 --------------------------#
[32m[20230207 15:35:39 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:35:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:40 @agent_ppo2.py:192][0m |           0.0012 |          13.2310 |           0.1764 |
[32m[20230207 15:35:40 @agent_ppo2.py:192][0m |          -0.0019 |           8.6129 |           0.1766 |
[32m[20230207 15:35:40 @agent_ppo2.py:192][0m |          -0.0046 |           7.8625 |           0.1767 |
[32m[20230207 15:35:40 @agent_ppo2.py:192][0m |          -0.0074 |           7.4613 |           0.1765 |
[32m[20230207 15:35:40 @agent_ppo2.py:192][0m |          -0.0081 |           7.0396 |           0.1766 |
[32m[20230207 15:35:40 @agent_ppo2.py:192][0m |          -0.0093 |           6.8499 |           0.1767 |
[32m[20230207 15:35:40 @agent_ppo2.py:192][0m |          -0.0103 |           6.6773 |           0.1767 |
[32m[20230207 15:35:40 @agent_ppo2.py:192][0m |          -0.0106 |           6.4619 |           0.1766 |
[32m[20230207 15:35:40 @agent_ppo2.py:192][0m |          -0.0122 |           6.3230 |           0.1766 |
[32m[20230207 15:35:40 @agent_ppo2.py:192][0m |          -0.0119 |           6.2272 |           0.1766 |
[32m[20230207 15:35:40 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:35:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: 25.48
[32m[20230207 15:35:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 91.91
[32m[20230207 15:35:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -19.71
[32m[20230207 15:35:41 @agent_ppo2.py:150][0m Total time:      35.00 min
[32m[20230207 15:35:41 @agent_ppo2.py:152][0m 1851392 total steps have happened
[32m[20230207 15:35:41 @agent_ppo2.py:128][0m #------------------------ Iteration 904 --------------------------#
[32m[20230207 15:35:42 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:35:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:42 @agent_ppo2.py:192][0m |           0.0007 |          10.6759 |           0.1734 |
[32m[20230207 15:35:42 @agent_ppo2.py:192][0m |           0.0019 |           6.7499 |           0.1733 |
[32m[20230207 15:35:42 @agent_ppo2.py:192][0m |           0.0133 |           6.0966 |           0.1734 |
[32m[20230207 15:35:42 @agent_ppo2.py:192][0m |          -0.0074 |           5.7178 |           0.1731 |
[32m[20230207 15:35:42 @agent_ppo2.py:192][0m |          -0.0152 |           5.3161 |           0.1728 |
[32m[20230207 15:35:42 @agent_ppo2.py:192][0m |           0.0021 |           5.1078 |           0.1727 |
[32m[20230207 15:35:43 @agent_ppo2.py:192][0m |          -0.0064 |           5.0199 |           0.1728 |
[32m[20230207 15:35:43 @agent_ppo2.py:192][0m |          -0.0085 |           4.8673 |           0.1728 |
[32m[20230207 15:35:43 @agent_ppo2.py:192][0m |          -0.0090 |           4.7690 |           0.1727 |
[32m[20230207 15:35:43 @agent_ppo2.py:192][0m |           0.0122 |           4.7850 |           0.1727 |
[32m[20230207 15:35:43 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:35:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: 97.89
[32m[20230207 15:35:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 102.85
[32m[20230207 15:35:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 58.62
[32m[20230207 15:35:44 @agent_ppo2.py:150][0m Total time:      35.04 min
[32m[20230207 15:35:44 @agent_ppo2.py:152][0m 1853440 total steps have happened
[32m[20230207 15:35:44 @agent_ppo2.py:128][0m #------------------------ Iteration 905 --------------------------#
[32m[20230207 15:35:44 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:35:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:45 @agent_ppo2.py:192][0m |           0.0026 |          13.2221 |           0.1782 |
[32m[20230207 15:35:45 @agent_ppo2.py:192][0m |          -0.0019 |           8.1490 |           0.1779 |
[32m[20230207 15:35:45 @agent_ppo2.py:192][0m |          -0.0087 |           6.9799 |           0.1778 |
[32m[20230207 15:35:45 @agent_ppo2.py:192][0m |          -0.0109 |           6.3172 |           0.1779 |
[32m[20230207 15:35:45 @agent_ppo2.py:192][0m |          -0.0094 |           5.9376 |           0.1779 |
[32m[20230207 15:35:45 @agent_ppo2.py:192][0m |          -0.0100 |           5.5405 |           0.1778 |
[32m[20230207 15:35:45 @agent_ppo2.py:192][0m |          -0.0143 |           5.3719 |           0.1776 |
[32m[20230207 15:35:45 @agent_ppo2.py:192][0m |          -0.0145 |           5.1156 |           0.1777 |
[32m[20230207 15:35:45 @agent_ppo2.py:192][0m |          -0.0124 |           4.9357 |           0.1778 |
[32m[20230207 15:35:45 @agent_ppo2.py:192][0m |          -0.0143 |           4.7946 |           0.1778 |
[32m[20230207 15:35:45 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:35:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -31.71
[32m[20230207 15:35:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 183.87
[32m[20230207 15:35:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 17.96
[32m[20230207 15:35:46 @agent_ppo2.py:150][0m Total time:      35.08 min
[32m[20230207 15:35:46 @agent_ppo2.py:152][0m 1855488 total steps have happened
[32m[20230207 15:35:46 @agent_ppo2.py:128][0m #------------------------ Iteration 906 --------------------------#
[32m[20230207 15:35:47 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:35:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:47 @agent_ppo2.py:192][0m |           0.0019 |          20.0093 |           0.1685 |
[32m[20230207 15:35:47 @agent_ppo2.py:192][0m |          -0.0076 |          16.6507 |           0.1684 |
[32m[20230207 15:35:47 @agent_ppo2.py:192][0m |          -0.0022 |          17.1515 |           0.1688 |
[32m[20230207 15:35:47 @agent_ppo2.py:192][0m |          -0.0094 |          15.5891 |           0.1689 |
[32m[20230207 15:35:47 @agent_ppo2.py:192][0m |          -0.0108 |          15.3295 |           0.1688 |
[32m[20230207 15:35:47 @agent_ppo2.py:192][0m |          -0.0121 |          15.1243 |           0.1689 |
[32m[20230207 15:35:47 @agent_ppo2.py:192][0m |          -0.0166 |          14.7168 |           0.1688 |
[32m[20230207 15:35:47 @agent_ppo2.py:192][0m |          -0.0139 |          14.3695 |           0.1686 |
[32m[20230207 15:35:48 @agent_ppo2.py:192][0m |          -0.0138 |          14.1977 |           0.1689 |
[32m[20230207 15:35:48 @agent_ppo2.py:192][0m |          -0.0202 |          14.0517 |           0.1688 |
[32m[20230207 15:35:48 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:35:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: 36.55
[32m[20230207 15:35:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 132.94
[32m[20230207 15:35:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 54.10
[32m[20230207 15:35:48 @agent_ppo2.py:150][0m Total time:      35.12 min
[32m[20230207 15:35:48 @agent_ppo2.py:152][0m 1857536 total steps have happened
[32m[20230207 15:35:48 @agent_ppo2.py:128][0m #------------------------ Iteration 907 --------------------------#
[32m[20230207 15:35:49 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:35:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:49 @agent_ppo2.py:192][0m |           0.0019 |          13.2094 |           0.1732 |
[32m[20230207 15:35:49 @agent_ppo2.py:192][0m |          -0.0075 |           9.6782 |           0.1729 |
[32m[20230207 15:35:50 @agent_ppo2.py:192][0m |          -0.0112 |           8.6390 |           0.1725 |
[32m[20230207 15:35:50 @agent_ppo2.py:192][0m |          -0.0136 |           8.1942 |           0.1724 |
[32m[20230207 15:35:50 @agent_ppo2.py:192][0m |          -0.0142 |           7.9376 |           0.1724 |
[32m[20230207 15:35:50 @agent_ppo2.py:192][0m |          -0.0173 |           7.7325 |           0.1723 |
[32m[20230207 15:35:50 @agent_ppo2.py:192][0m |          -0.0178 |           7.6051 |           0.1724 |
[32m[20230207 15:35:50 @agent_ppo2.py:192][0m |          -0.0193 |           7.4789 |           0.1723 |
[32m[20230207 15:35:50 @agent_ppo2.py:192][0m |          -0.0201 |           7.3400 |           0.1725 |
[32m[20230207 15:35:50 @agent_ppo2.py:192][0m |          -0.0207 |           7.2055 |           0.1725 |
[32m[20230207 15:35:50 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:35:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: -15.60
[32m[20230207 15:35:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 135.39
[32m[20230207 15:35:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -32.82
[32m[20230207 15:35:51 @agent_ppo2.py:150][0m Total time:      35.16 min
[32m[20230207 15:35:51 @agent_ppo2.py:152][0m 1859584 total steps have happened
[32m[20230207 15:35:51 @agent_ppo2.py:128][0m #------------------------ Iteration 908 --------------------------#
[32m[20230207 15:35:51 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:35:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:51 @agent_ppo2.py:192][0m |           0.0130 |           8.3855 |           0.1773 |
[32m[20230207 15:35:51 @agent_ppo2.py:192][0m |          -0.0102 |           3.9838 |           0.1770 |
[32m[20230207 15:35:52 @agent_ppo2.py:192][0m |           0.0025 |           3.3808 |           0.1768 |
[32m[20230207 15:35:52 @agent_ppo2.py:192][0m |          -0.0152 |           3.1844 |           0.1768 |
[32m[20230207 15:35:52 @agent_ppo2.py:192][0m |          -0.0375 |           2.9429 |           0.1769 |
[32m[20230207 15:35:52 @agent_ppo2.py:192][0m |          -0.0210 |           2.7523 |           0.1765 |
[32m[20230207 15:35:52 @agent_ppo2.py:192][0m |          -0.0036 |           2.8023 |           0.1767 |
[32m[20230207 15:35:52 @agent_ppo2.py:192][0m |          -0.0235 |           2.6246 |           0.1766 |
[32m[20230207 15:35:52 @agent_ppo2.py:192][0m |          -0.0160 |           2.5044 |           0.1768 |
[32m[20230207 15:35:52 @agent_ppo2.py:192][0m |          -0.0234 |           2.5144 |           0.1768 |
[32m[20230207 15:35:52 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:35:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: -16.89
[32m[20230207 15:35:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 95.91
[32m[20230207 15:35:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 42.24
[32m[20230207 15:35:52 @agent_ppo2.py:150][0m Total time:      35.19 min
[32m[20230207 15:35:52 @agent_ppo2.py:152][0m 1861632 total steps have happened
[32m[20230207 15:35:52 @agent_ppo2.py:128][0m #------------------------ Iteration 909 --------------------------#
[32m[20230207 15:35:53 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:35:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:53 @agent_ppo2.py:192][0m |           0.0223 |           2.4500 |           0.1693 |
[32m[20230207 15:35:54 @agent_ppo2.py:192][0m |           0.0054 |           2.1475 |           0.1686 |
[32m[20230207 15:35:54 @agent_ppo2.py:192][0m |           0.0011 |           2.0491 |           0.1688 |
[32m[20230207 15:35:54 @agent_ppo2.py:192][0m |           0.0268 |           2.1334 |           0.1689 |
[32m[20230207 15:35:54 @agent_ppo2.py:192][0m |          -0.0005 |           1.9557 |           0.1690 |
[32m[20230207 15:35:54 @agent_ppo2.py:192][0m |          -0.0228 |           1.9418 |           0.1687 |
[32m[20230207 15:35:54 @agent_ppo2.py:192][0m |          -0.0122 |           1.8917 |           0.1690 |
[32m[20230207 15:35:54 @agent_ppo2.py:192][0m |          -0.0024 |           1.8503 |           0.1689 |
[32m[20230207 15:35:54 @agent_ppo2.py:192][0m |          -0.0086 |           1.8501 |           0.1693 |
[32m[20230207 15:35:54 @agent_ppo2.py:192][0m |          -0.0165 |           1.8232 |           0.1690 |
[32m[20230207 15:35:54 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:35:55 @agent_ppo2.py:145][0m Average TRAINING episode reward: 91.63
[32m[20230207 15:35:55 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 121.78
[32m[20230207 15:35:55 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 13.77
[32m[20230207 15:35:55 @agent_ppo2.py:150][0m Total time:      35.23 min
[32m[20230207 15:35:55 @agent_ppo2.py:152][0m 1863680 total steps have happened
[32m[20230207 15:35:55 @agent_ppo2.py:128][0m #------------------------ Iteration 910 --------------------------#
[32m[20230207 15:35:56 @agent_ppo2.py:134][0m Sampling time: 0.83 s by 1 slaves
[32m[20230207 15:35:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:56 @agent_ppo2.py:192][0m |          -0.0015 |           4.5663 |           0.1735 |
[32m[20230207 15:35:56 @agent_ppo2.py:192][0m |          -0.0005 |           3.7115 |           0.1735 |
[32m[20230207 15:35:56 @agent_ppo2.py:192][0m |          -0.0100 |           3.4048 |           0.1733 |
[32m[20230207 15:35:56 @agent_ppo2.py:192][0m |          -0.0089 |           3.1505 |           0.1733 |
[32m[20230207 15:35:56 @agent_ppo2.py:192][0m |          -0.0054 |           3.0285 |           0.1734 |
[32m[20230207 15:35:56 @agent_ppo2.py:192][0m |           0.0065 |           3.0041 |           0.1732 |
[32m[20230207 15:35:56 @agent_ppo2.py:192][0m |          -0.0117 |           2.9001 |           0.1732 |
[32m[20230207 15:35:56 @agent_ppo2.py:192][0m |          -0.0150 |           2.7808 |           0.1731 |
[32m[20230207 15:35:57 @agent_ppo2.py:192][0m |          -0.0069 |           2.7024 |           0.1732 |
[32m[20230207 15:35:57 @agent_ppo2.py:192][0m |          -0.0060 |           2.6854 |           0.1732 |
[32m[20230207 15:35:57 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:35:57 @agent_ppo2.py:145][0m Average TRAINING episode reward: 22.51
[32m[20230207 15:35:57 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 119.14
[32m[20230207 15:35:57 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 98.11
[32m[20230207 15:35:57 @agent_ppo2.py:150][0m Total time:      35.27 min
[32m[20230207 15:35:57 @agent_ppo2.py:152][0m 1865728 total steps have happened
[32m[20230207 15:35:57 @agent_ppo2.py:128][0m #------------------------ Iteration 911 --------------------------#
[32m[20230207 15:35:58 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:35:58 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:35:58 @agent_ppo2.py:192][0m |           0.0143 |           2.3314 |           0.1712 |
[32m[20230207 15:35:58 @agent_ppo2.py:192][0m |           0.0129 |           1.7232 |           0.1714 |
[32m[20230207 15:35:58 @agent_ppo2.py:192][0m |           0.0032 |           1.6256 |           0.1705 |
[32m[20230207 15:35:59 @agent_ppo2.py:192][0m |          -0.0117 |           1.5787 |           0.1711 |
[32m[20230207 15:35:59 @agent_ppo2.py:192][0m |          -0.0101 |           1.5462 |           0.1714 |
[32m[20230207 15:35:59 @agent_ppo2.py:192][0m |          -0.0173 |           1.5271 |           0.1714 |
[32m[20230207 15:35:59 @agent_ppo2.py:192][0m |          -0.0189 |           1.5042 |           0.1714 |
[32m[20230207 15:35:59 @agent_ppo2.py:192][0m |          -0.0237 |           1.5234 |           0.1712 |
[32m[20230207 15:35:59 @agent_ppo2.py:192][0m |          -0.0054 |           1.4746 |           0.1715 |
[32m[20230207 15:35:59 @agent_ppo2.py:192][0m |          -0.0084 |           1.4528 |           0.1715 |
[32m[20230207 15:35:59 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:36:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: 69.61
[32m[20230207 15:36:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 86.12
[32m[20230207 15:36:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 121.84
[32m[20230207 15:36:00 @agent_ppo2.py:150][0m Total time:      35.31 min
[32m[20230207 15:36:00 @agent_ppo2.py:152][0m 1867776 total steps have happened
[32m[20230207 15:36:00 @agent_ppo2.py:128][0m #------------------------ Iteration 912 --------------------------#
[32m[20230207 15:36:01 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:36:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:01 @agent_ppo2.py:192][0m |           0.0049 |           5.4361 |           0.1741 |
[32m[20230207 15:36:01 @agent_ppo2.py:192][0m |          -0.0052 |           2.9863 |           0.1739 |
[32m[20230207 15:36:01 @agent_ppo2.py:192][0m |          -0.0065 |           2.5768 |           0.1735 |
[32m[20230207 15:36:01 @agent_ppo2.py:192][0m |          -0.0087 |           2.4071 |           0.1734 |
[32m[20230207 15:36:01 @agent_ppo2.py:192][0m |          -0.0077 |           2.2943 |           0.1732 |
[32m[20230207 15:36:01 @agent_ppo2.py:192][0m |          -0.0113 |           2.2063 |           0.1733 |
[32m[20230207 15:36:01 @agent_ppo2.py:192][0m |          -0.0112 |           2.2147 |           0.1732 |
[32m[20230207 15:36:01 @agent_ppo2.py:192][0m |          -0.0124 |           2.1142 |           0.1731 |
[32m[20230207 15:36:01 @agent_ppo2.py:192][0m |          -0.0127 |           2.0824 |           0.1731 |
[32m[20230207 15:36:01 @agent_ppo2.py:192][0m |          -0.0139 |           2.0739 |           0.1730 |
[32m[20230207 15:36:01 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:36:02 @agent_ppo2.py:145][0m Average TRAINING episode reward: 22.20
[32m[20230207 15:36:02 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 137.93
[32m[20230207 15:36:02 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -83.15
[32m[20230207 15:36:02 @agent_ppo2.py:150][0m Total time:      35.35 min
[32m[20230207 15:36:02 @agent_ppo2.py:152][0m 1869824 total steps have happened
[32m[20230207 15:36:02 @agent_ppo2.py:128][0m #------------------------ Iteration 913 --------------------------#
[32m[20230207 15:36:03 @agent_ppo2.py:134][0m Sampling time: 0.76 s by 1 slaves
[32m[20230207 15:36:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:03 @agent_ppo2.py:192][0m |           0.0004 |          10.8083 |           0.1788 |
[32m[20230207 15:36:03 @agent_ppo2.py:192][0m |          -0.0063 |           6.3137 |           0.1787 |
[32m[20230207 15:36:03 @agent_ppo2.py:192][0m |          -0.0089 |           5.6520 |           0.1785 |
[32m[20230207 15:36:03 @agent_ppo2.py:192][0m |          -0.0106 |           5.2589 |           0.1787 |
[32m[20230207 15:36:03 @agent_ppo2.py:192][0m |          -0.0129 |           5.0261 |           0.1785 |
[32m[20230207 15:36:03 @agent_ppo2.py:192][0m |          -0.0133 |           4.7861 |           0.1786 |
[32m[20230207 15:36:03 @agent_ppo2.py:192][0m |          -0.0146 |           4.5675 |           0.1787 |
[32m[20230207 15:36:03 @agent_ppo2.py:192][0m |          -0.0155 |           4.4791 |           0.1786 |
[32m[20230207 15:36:04 @agent_ppo2.py:192][0m |          -0.0166 |           4.3537 |           0.1786 |
[32m[20230207 15:36:04 @agent_ppo2.py:192][0m |          -0.0175 |           4.2418 |           0.1787 |
[32m[20230207 15:36:04 @agent_ppo2.py:137][0m Policy update time: 0.79 s
[32m[20230207 15:36:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: 123.25
[32m[20230207 15:36:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 203.63
[32m[20230207 15:36:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 72.98
[32m[20230207 15:36:04 @agent_ppo2.py:150][0m Total time:      35.39 min
[32m[20230207 15:36:04 @agent_ppo2.py:152][0m 1871872 total steps have happened
[32m[20230207 15:36:04 @agent_ppo2.py:128][0m #------------------------ Iteration 914 --------------------------#
[32m[20230207 15:36:05 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:36:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:05 @agent_ppo2.py:192][0m |           0.0166 |           3.6192 |           0.1752 |
[32m[20230207 15:36:05 @agent_ppo2.py:192][0m |          -0.0086 |           2.5489 |           0.1751 |
[32m[20230207 15:36:05 @agent_ppo2.py:192][0m |          -0.0076 |           2.3681 |           0.1749 |
[32m[20230207 15:36:06 @agent_ppo2.py:192][0m |          -0.0031 |           2.2192 |           0.1748 |
[32m[20230207 15:36:06 @agent_ppo2.py:192][0m |          -0.0069 |           2.1142 |           0.1747 |
[32m[20230207 15:36:06 @agent_ppo2.py:192][0m |          -0.0139 |           2.0531 |           0.1748 |
[32m[20230207 15:36:06 @agent_ppo2.py:192][0m |          -0.0079 |           2.0025 |           0.1744 |
[32m[20230207 15:36:06 @agent_ppo2.py:192][0m |          -0.0061 |           1.9676 |           0.1744 |
[32m[20230207 15:36:06 @agent_ppo2.py:192][0m |           0.0045 |           1.9284 |           0.1746 |
[32m[20230207 15:36:06 @agent_ppo2.py:192][0m |          -0.0467 |           1.9110 |           0.1746 |
[32m[20230207 15:36:06 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:36:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: 141.34
[32m[20230207 15:36:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 165.75
[32m[20230207 15:36:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -30.49
[32m[20230207 15:36:07 @agent_ppo2.py:150][0m Total time:      35.43 min
[32m[20230207 15:36:07 @agent_ppo2.py:152][0m 1873920 total steps have happened
[32m[20230207 15:36:07 @agent_ppo2.py:128][0m #------------------------ Iteration 915 --------------------------#
[32m[20230207 15:36:07 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230207 15:36:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:08 @agent_ppo2.py:192][0m |           0.0023 |          13.9914 |           0.1783 |
[32m[20230207 15:36:08 @agent_ppo2.py:192][0m |          -0.0054 |           7.7165 |           0.1783 |
[32m[20230207 15:36:08 @agent_ppo2.py:192][0m |          -0.0063 |           6.3948 |           0.1782 |
[32m[20230207 15:36:08 @agent_ppo2.py:192][0m |          -0.0093 |           5.4850 |           0.1782 |
[32m[20230207 15:36:08 @agent_ppo2.py:192][0m |          -0.0124 |           5.5273 |           0.1780 |
[32m[20230207 15:36:08 @agent_ppo2.py:192][0m |          -0.0117 |           4.3051 |           0.1780 |
[32m[20230207 15:36:08 @agent_ppo2.py:192][0m |          -0.0135 |           3.9096 |           0.1779 |
[32m[20230207 15:36:08 @agent_ppo2.py:192][0m |          -0.0150 |           3.3155 |           0.1778 |
[32m[20230207 15:36:08 @agent_ppo2.py:192][0m |          -0.0154 |           2.8909 |           0.1778 |
[32m[20230207 15:36:08 @agent_ppo2.py:192][0m |          -0.0152 |           2.6194 |           0.1778 |
[32m[20230207 15:36:08 @agent_ppo2.py:137][0m Policy update time: 0.61 s
[32m[20230207 15:36:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: 16.36
[32m[20230207 15:36:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 115.92
[32m[20230207 15:36:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 25.89
[32m[20230207 15:36:09 @agent_ppo2.py:150][0m Total time:      35.46 min
[32m[20230207 15:36:09 @agent_ppo2.py:152][0m 1875968 total steps have happened
[32m[20230207 15:36:09 @agent_ppo2.py:128][0m #------------------------ Iteration 916 --------------------------#
[32m[20230207 15:36:10 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:36:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:10 @agent_ppo2.py:192][0m |           0.0008 |          17.1778 |           0.1739 |
[32m[20230207 15:36:10 @agent_ppo2.py:192][0m |          -0.0055 |          11.7844 |           0.1737 |
[32m[20230207 15:36:10 @agent_ppo2.py:192][0m |          -0.0067 |          10.4357 |           0.1734 |
[32m[20230207 15:36:10 @agent_ppo2.py:192][0m |          -0.0093 |           9.7878 |           0.1733 |
[32m[20230207 15:36:10 @agent_ppo2.py:192][0m |          -0.0112 |           9.1596 |           0.1735 |
[32m[20230207 15:36:10 @agent_ppo2.py:192][0m |          -0.0108 |           8.6113 |           0.1732 |
[32m[20230207 15:36:10 @agent_ppo2.py:192][0m |          -0.0138 |           8.5010 |           0.1731 |
[32m[20230207 15:36:11 @agent_ppo2.py:192][0m |          -0.0121 |           8.2411 |           0.1731 |
[32m[20230207 15:36:11 @agent_ppo2.py:192][0m |          -0.0147 |           8.0118 |           0.1730 |
[32m[20230207 15:36:11 @agent_ppo2.py:192][0m |          -0.0137 |           7.7799 |           0.1730 |
[32m[20230207 15:36:11 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:36:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: 2.34
[32m[20230207 15:36:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 184.60
[32m[20230207 15:36:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 24.69
[32m[20230207 15:36:12 @agent_ppo2.py:150][0m Total time:      35.51 min
[32m[20230207 15:36:12 @agent_ppo2.py:152][0m 1878016 total steps have happened
[32m[20230207 15:36:12 @agent_ppo2.py:128][0m #------------------------ Iteration 917 --------------------------#
[32m[20230207 15:36:12 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:36:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:13 @agent_ppo2.py:192][0m |           0.0046 |           6.2525 |           0.1792 |
[32m[20230207 15:36:13 @agent_ppo2.py:192][0m |           0.0064 |           3.6354 |           0.1791 |
[32m[20230207 15:36:13 @agent_ppo2.py:192][0m |          -0.0023 |           3.0253 |           0.1788 |
[32m[20230207 15:36:13 @agent_ppo2.py:192][0m |          -0.0071 |           2.7306 |           0.1788 |
[32m[20230207 15:36:13 @agent_ppo2.py:192][0m |          -0.0021 |           2.5876 |           0.1787 |
[32m[20230207 15:36:13 @agent_ppo2.py:192][0m |          -0.0044 |           2.4376 |           0.1785 |
[32m[20230207 15:36:13 @agent_ppo2.py:192][0m |          -0.0089 |           2.3550 |           0.1785 |
[32m[20230207 15:36:13 @agent_ppo2.py:192][0m |          -0.0085 |           2.2258 |           0.1785 |
[32m[20230207 15:36:13 @agent_ppo2.py:192][0m |          -0.0109 |           2.1618 |           0.1784 |
[32m[20230207 15:36:13 @agent_ppo2.py:192][0m |          -0.0109 |           2.1329 |           0.1786 |
[32m[20230207 15:36:13 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:36:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: 36.41
[32m[20230207 15:36:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 124.40
[32m[20230207 15:36:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 49.75
[32m[20230207 15:36:14 @agent_ppo2.py:150][0m Total time:      35.55 min
[32m[20230207 15:36:14 @agent_ppo2.py:152][0m 1880064 total steps have happened
[32m[20230207 15:36:14 @agent_ppo2.py:128][0m #------------------------ Iteration 918 --------------------------#
[32m[20230207 15:36:15 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:36:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:15 @agent_ppo2.py:192][0m |          -0.0085 |           2.6450 |           0.1773 |
[32m[20230207 15:36:15 @agent_ppo2.py:192][0m |          -0.0255 |           2.0890 |           0.1773 |
[32m[20230207 15:36:15 @agent_ppo2.py:192][0m |          -0.0102 |           1.9307 |           0.1770 |
[32m[20230207 15:36:15 @agent_ppo2.py:192][0m |           0.0429 |           2.2245 |           0.1770 |
[32m[20230207 15:36:15 @agent_ppo2.py:192][0m |          -0.0252 |           1.8288 |           0.1771 |
[32m[20230207 15:36:15 @agent_ppo2.py:192][0m |          -0.0089 |           1.7003 |           0.1769 |
[32m[20230207 15:36:15 @agent_ppo2.py:192][0m |           0.0096 |           1.6979 |           0.1770 |
[32m[20230207 15:36:16 @agent_ppo2.py:192][0m |          -0.0249 |           1.6018 |           0.1765 |
[32m[20230207 15:36:16 @agent_ppo2.py:192][0m |          -0.0121 |           1.5401 |           0.1767 |
[32m[20230207 15:36:16 @agent_ppo2.py:192][0m |          -0.0051 |           1.4959 |           0.1767 |
[32m[20230207 15:36:16 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:36:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: 83.29
[32m[20230207 15:36:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 117.81
[32m[20230207 15:36:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 52.42
[32m[20230207 15:36:16 @agent_ppo2.py:150][0m Total time:      35.59 min
[32m[20230207 15:36:16 @agent_ppo2.py:152][0m 1882112 total steps have happened
[32m[20230207 15:36:16 @agent_ppo2.py:128][0m #------------------------ Iteration 919 --------------------------#
[32m[20230207 15:36:17 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:36:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:17 @agent_ppo2.py:192][0m |          -0.0163 |           2.8403 |           0.1787 |
[32m[20230207 15:36:17 @agent_ppo2.py:192][0m |           0.0111 |           2.4037 |           0.1781 |
[32m[20230207 15:36:18 @agent_ppo2.py:192][0m |           0.0053 |           2.2387 |           0.1782 |
[32m[20230207 15:36:18 @agent_ppo2.py:192][0m |          -0.0059 |           2.1325 |           0.1783 |
[32m[20230207 15:36:18 @agent_ppo2.py:192][0m |          -0.0048 |           2.0712 |           0.1781 |
[32m[20230207 15:36:18 @agent_ppo2.py:192][0m |          -0.0227 |           2.0553 |           0.1782 |
[32m[20230207 15:36:18 @agent_ppo2.py:192][0m |          -0.0144 |           1.9958 |           0.1780 |
[32m[20230207 15:36:18 @agent_ppo2.py:192][0m |          -0.0068 |           1.9670 |           0.1779 |
[32m[20230207 15:36:18 @agent_ppo2.py:192][0m |          -0.0224 |           1.9349 |           0.1778 |
[32m[20230207 15:36:18 @agent_ppo2.py:192][0m |          -0.0266 |           1.8813 |           0.1774 |
[32m[20230207 15:36:18 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:36:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: 101.54
[32m[20230207 15:36:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 102.20
[32m[20230207 15:36:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 45.81
[32m[20230207 15:36:19 @agent_ppo2.py:150][0m Total time:      35.63 min
[32m[20230207 15:36:19 @agent_ppo2.py:152][0m 1884160 total steps have happened
[32m[20230207 15:36:19 @agent_ppo2.py:128][0m #------------------------ Iteration 920 --------------------------#
[32m[20230207 15:36:20 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:36:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:20 @agent_ppo2.py:192][0m |          -0.0077 |           1.7280 |           0.1761 |
[32m[20230207 15:36:20 @agent_ppo2.py:192][0m |          -0.0102 |           1.6013 |           0.1762 |
[32m[20230207 15:36:20 @agent_ppo2.py:192][0m |           0.0004 |           1.5689 |           0.1758 |
[32m[20230207 15:36:20 @agent_ppo2.py:192][0m |           0.0062 |           1.5484 |           0.1759 |
[32m[20230207 15:36:20 @agent_ppo2.py:192][0m |           0.0110 |           1.5364 |           0.1757 |
[32m[20230207 15:36:20 @agent_ppo2.py:192][0m |          -0.0177 |           1.5194 |           0.1753 |
[32m[20230207 15:36:20 @agent_ppo2.py:192][0m |          -0.0152 |           1.4867 |           0.1754 |
[32m[20230207 15:36:20 @agent_ppo2.py:192][0m |           0.0055 |           1.5564 |           0.1756 |
[32m[20230207 15:36:20 @agent_ppo2.py:192][0m |          -0.0166 |           1.4881 |           0.1753 |
[32m[20230207 15:36:21 @agent_ppo2.py:192][0m |          -0.0047 |           1.4683 |           0.1754 |
[32m[20230207 15:36:21 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:36:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: 125.28
[32m[20230207 15:36:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 133.21
[32m[20230207 15:36:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -32.46
[32m[20230207 15:36:21 @agent_ppo2.py:150][0m Total time:      35.66 min
[32m[20230207 15:36:21 @agent_ppo2.py:152][0m 1886208 total steps have happened
[32m[20230207 15:36:21 @agent_ppo2.py:128][0m #------------------------ Iteration 921 --------------------------#
[32m[20230207 15:36:22 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:36:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:22 @agent_ppo2.py:192][0m |          -0.0146 |           2.2533 |           0.1806 |
[32m[20230207 15:36:22 @agent_ppo2.py:192][0m |          -0.0012 |           1.7893 |           0.1801 |
[32m[20230207 15:36:22 @agent_ppo2.py:192][0m |          -0.0016 |           1.7061 |           0.1803 |
[32m[20230207 15:36:22 @agent_ppo2.py:192][0m |          -0.0050 |           1.6468 |           0.1805 |
[32m[20230207 15:36:22 @agent_ppo2.py:192][0m |          -0.0193 |           1.6080 |           0.1804 |
[32m[20230207 15:36:22 @agent_ppo2.py:192][0m |           0.0037 |           1.6009 |           0.1802 |
[32m[20230207 15:36:22 @agent_ppo2.py:192][0m |          -0.0269 |           1.5919 |           0.1801 |
[32m[20230207 15:36:23 @agent_ppo2.py:192][0m |          -0.0374 |           1.5566 |           0.1803 |
[32m[20230207 15:36:23 @agent_ppo2.py:192][0m |          -0.0156 |           1.5428 |           0.1802 |
[32m[20230207 15:36:23 @agent_ppo2.py:192][0m |          -0.0097 |           1.5257 |           0.1801 |
[32m[20230207 15:36:23 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:36:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: 170.90
[32m[20230207 15:36:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 184.11
[32m[20230207 15:36:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 45.65
[32m[20230207 15:36:24 @agent_ppo2.py:150][0m Total time:      35.71 min
[32m[20230207 15:36:24 @agent_ppo2.py:152][0m 1888256 total steps have happened
[32m[20230207 15:36:24 @agent_ppo2.py:128][0m #------------------------ Iteration 922 --------------------------#
[32m[20230207 15:36:24 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:36:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:24 @agent_ppo2.py:192][0m |           0.0000 |          13.8976 |           0.1748 |
[32m[20230207 15:36:24 @agent_ppo2.py:192][0m |          -0.0121 |           5.1721 |           0.1745 |
[32m[20230207 15:36:24 @agent_ppo2.py:192][0m |          -0.0129 |           3.8226 |           0.1744 |
[32m[20230207 15:36:24 @agent_ppo2.py:192][0m |          -0.0154 |           3.1220 |           0.1744 |
[32m[20230207 15:36:24 @agent_ppo2.py:192][0m |          -0.0126 |           2.7384 |           0.1741 |
[32m[20230207 15:36:24 @agent_ppo2.py:192][0m |          -0.0064 |           2.4911 |           0.1739 |
[32m[20230207 15:36:25 @agent_ppo2.py:192][0m |          -0.0153 |           2.3651 |           0.1740 |
[32m[20230207 15:36:25 @agent_ppo2.py:192][0m |          -0.0222 |           2.2217 |           0.1738 |
[32m[20230207 15:36:25 @agent_ppo2.py:192][0m |          -0.0094 |           2.0923 |           0.1739 |
[32m[20230207 15:36:25 @agent_ppo2.py:192][0m |          -0.0155 |           2.0662 |           0.1736 |
[32m[20230207 15:36:25 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:36:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -21.99
[32m[20230207 15:36:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 0.76
[32m[20230207 15:36:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 55.67
[32m[20230207 15:36:25 @agent_ppo2.py:150][0m Total time:      35.74 min
[32m[20230207 15:36:25 @agent_ppo2.py:152][0m 1890304 total steps have happened
[32m[20230207 15:36:25 @agent_ppo2.py:128][0m #------------------------ Iteration 923 --------------------------#
[32m[20230207 15:36:26 @agent_ppo2.py:134][0m Sampling time: 0.73 s by 1 slaves
[32m[20230207 15:36:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:26 @agent_ppo2.py:192][0m |          -0.0024 |           2.4870 |           0.1781 |
[32m[20230207 15:36:26 @agent_ppo2.py:192][0m |          -0.0050 |           1.9898 |           0.1779 |
[32m[20230207 15:36:26 @agent_ppo2.py:192][0m |          -0.0064 |           1.9039 |           0.1780 |
[32m[20230207 15:36:27 @agent_ppo2.py:192][0m |          -0.0073 |           1.8687 |           0.1778 |
[32m[20230207 15:36:27 @agent_ppo2.py:192][0m |          -0.0090 |           1.8379 |           0.1779 |
[32m[20230207 15:36:27 @agent_ppo2.py:192][0m |          -0.0095 |           1.8260 |           0.1779 |
[32m[20230207 15:36:27 @agent_ppo2.py:192][0m |          -0.0101 |           1.7970 |           0.1779 |
[32m[20230207 15:36:27 @agent_ppo2.py:192][0m |          -0.0105 |           1.7870 |           0.1778 |
[32m[20230207 15:36:27 @agent_ppo2.py:192][0m |          -0.0122 |           1.7533 |           0.1779 |
[32m[20230207 15:36:27 @agent_ppo2.py:192][0m |          -0.0120 |           1.7389 |           0.1777 |
[32m[20230207 15:36:27 @agent_ppo2.py:137][0m Policy update time: 0.75 s
[32m[20230207 15:36:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: 184.93
[32m[20230207 15:36:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 217.38
[32m[20230207 15:36:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -57.93
[32m[20230207 15:36:27 @agent_ppo2.py:150][0m Total time:      35.77 min
[32m[20230207 15:36:27 @agent_ppo2.py:152][0m 1892352 total steps have happened
[32m[20230207 15:36:27 @agent_ppo2.py:128][0m #------------------------ Iteration 924 --------------------------#
[32m[20230207 15:36:28 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:36:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:28 @agent_ppo2.py:192][0m |           0.0052 |           7.5272 |           0.1795 |
[32m[20230207 15:36:29 @agent_ppo2.py:192][0m |          -0.0053 |           2.6657 |           0.1793 |
[32m[20230207 15:36:29 @agent_ppo2.py:192][0m |          -0.0072 |           2.1563 |           0.1792 |
[32m[20230207 15:36:29 @agent_ppo2.py:192][0m |          -0.0076 |           1.9701 |           0.1792 |
[32m[20230207 15:36:29 @agent_ppo2.py:192][0m |          -0.0089 |           1.8738 |           0.1792 |
[32m[20230207 15:36:29 @agent_ppo2.py:192][0m |          -0.0104 |           1.6880 |           0.1791 |
[32m[20230207 15:36:29 @agent_ppo2.py:192][0m |          -0.0101 |           1.5972 |           0.1791 |
[32m[20230207 15:36:29 @agent_ppo2.py:192][0m |          -0.0114 |           1.5230 |           0.1790 |
[32m[20230207 15:36:29 @agent_ppo2.py:192][0m |          -0.0113 |           1.4807 |           0.1791 |
[32m[20230207 15:36:29 @agent_ppo2.py:192][0m |          -0.0126 |           1.4767 |           0.1791 |
[32m[20230207 15:36:29 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:36:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -1.11
[32m[20230207 15:36:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 58.25
[32m[20230207 15:36:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 19.03
[32m[20230207 15:36:30 @agent_ppo2.py:150][0m Total time:      35.81 min
[32m[20230207 15:36:30 @agent_ppo2.py:152][0m 1894400 total steps have happened
[32m[20230207 15:36:30 @agent_ppo2.py:128][0m #------------------------ Iteration 925 --------------------------#
[32m[20230207 15:36:31 @agent_ppo2.py:134][0m Sampling time: 0.72 s by 1 slaves
[32m[20230207 15:36:31 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:31 @agent_ppo2.py:192][0m |           0.0049 |          29.4509 |           0.1777 |
[32m[20230207 15:36:31 @agent_ppo2.py:192][0m |          -0.0056 |          13.6719 |           0.1776 |
[32m[20230207 15:36:31 @agent_ppo2.py:192][0m |          -0.0121 |          11.7363 |           0.1777 |
[32m[20230207 15:36:31 @agent_ppo2.py:192][0m |          -0.0080 |           8.7785 |           0.1774 |
[32m[20230207 15:36:31 @agent_ppo2.py:192][0m |          -0.0107 |           7.7783 |           0.1776 |
[32m[20230207 15:36:31 @agent_ppo2.py:192][0m |          -0.0105 |           7.3878 |           0.1775 |
[32m[20230207 15:36:31 @agent_ppo2.py:192][0m |          -0.0073 |           6.4623 |           0.1776 |
[32m[20230207 15:36:31 @agent_ppo2.py:192][0m |          -0.0176 |           6.0499 |           0.1775 |
[32m[20230207 15:36:31 @agent_ppo2.py:192][0m |          -0.0146 |           5.8928 |           0.1776 |
[32m[20230207 15:36:31 @agent_ppo2.py:192][0m |          -0.0070 |           5.4191 |           0.1775 |
[32m[20230207 15:36:31 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230207 15:36:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: -44.47
[32m[20230207 15:36:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 61.33
[32m[20230207 15:36:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -40.67
[32m[20230207 15:36:32 @agent_ppo2.py:150][0m Total time:      35.85 min
[32m[20230207 15:36:32 @agent_ppo2.py:152][0m 1896448 total steps have happened
[32m[20230207 15:36:32 @agent_ppo2.py:128][0m #------------------------ Iteration 926 --------------------------#
[32m[20230207 15:36:33 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:36:33 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:33 @agent_ppo2.py:192][0m |           0.0031 |          36.7056 |           0.1803 |
[32m[20230207 15:36:33 @agent_ppo2.py:192][0m |          -0.0061 |          18.2566 |           0.1799 |
[32m[20230207 15:36:33 @agent_ppo2.py:192][0m |          -0.0102 |          15.9335 |           0.1795 |
[32m[20230207 15:36:33 @agent_ppo2.py:192][0m |          -0.0131 |          15.0711 |           0.1793 |
[32m[20230207 15:36:33 @agent_ppo2.py:192][0m |          -0.0151 |          14.5236 |           0.1792 |
[32m[20230207 15:36:33 @agent_ppo2.py:192][0m |          -0.0164 |          14.1734 |           0.1791 |
[32m[20230207 15:36:33 @agent_ppo2.py:192][0m |          -0.0173 |          13.8383 |           0.1790 |
[32m[20230207 15:36:33 @agent_ppo2.py:192][0m |          -0.0187 |          13.5128 |           0.1790 |
[32m[20230207 15:36:33 @agent_ppo2.py:192][0m |          -0.0195 |          13.3788 |           0.1789 |
[32m[20230207 15:36:33 @agent_ppo2.py:192][0m |          -0.0190 |          13.0720 |           0.1788 |
[32m[20230207 15:36:33 @agent_ppo2.py:137][0m Policy update time: 0.52 s
[32m[20230207 15:36:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: -55.47
[32m[20230207 15:36:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 83.14
[32m[20230207 15:36:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 124.99
[32m[20230207 15:36:34 @agent_ppo2.py:150][0m Total time:      35.88 min
[32m[20230207 15:36:34 @agent_ppo2.py:152][0m 1898496 total steps have happened
[32m[20230207 15:36:34 @agent_ppo2.py:128][0m #------------------------ Iteration 927 --------------------------#
[32m[20230207 15:36:35 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:36:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:35 @agent_ppo2.py:192][0m |          -0.0061 |           2.9254 |           0.1808 |
[32m[20230207 15:36:35 @agent_ppo2.py:192][0m |          -0.0050 |           2.4330 |           0.1805 |
[32m[20230207 15:36:35 @agent_ppo2.py:192][0m |          -0.0117 |           2.3509 |           0.1805 |
[32m[20230207 15:36:35 @agent_ppo2.py:192][0m |          -0.0281 |           2.2642 |           0.1802 |
[32m[20230207 15:36:35 @agent_ppo2.py:192][0m |          -0.0001 |           2.2499 |           0.1801 |
[32m[20230207 15:36:35 @agent_ppo2.py:192][0m |          -0.0123 |           2.1811 |           0.1803 |
[32m[20230207 15:36:35 @agent_ppo2.py:192][0m |          -0.0121 |           2.1531 |           0.1803 |
[32m[20230207 15:36:35 @agent_ppo2.py:192][0m |          -0.0226 |           2.1262 |           0.1803 |
[32m[20230207 15:36:35 @agent_ppo2.py:192][0m |          -0.0139 |           2.1097 |           0.1804 |
[32m[20230207 15:36:35 @agent_ppo2.py:192][0m |          -0.0063 |           2.1421 |           0.1803 |
[32m[20230207 15:36:35 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:36:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: 135.79
[32m[20230207 15:36:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 144.70
[32m[20230207 15:36:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 73.17
[32m[20230207 15:36:36 @agent_ppo2.py:150][0m Total time:      35.92 min
[32m[20230207 15:36:36 @agent_ppo2.py:152][0m 1900544 total steps have happened
[32m[20230207 15:36:36 @agent_ppo2.py:128][0m #------------------------ Iteration 928 --------------------------#
[32m[20230207 15:36:37 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:36:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:37 @agent_ppo2.py:192][0m |          -0.0036 |          11.2462 |           0.1753 |
[32m[20230207 15:36:37 @agent_ppo2.py:192][0m |          -0.0088 |           5.1291 |           0.1744 |
[32m[20230207 15:36:37 @agent_ppo2.py:192][0m |           0.0034 |           4.1881 |           0.1746 |
[32m[20230207 15:36:37 @agent_ppo2.py:192][0m |           0.0033 |           3.4566 |           0.1750 |
[32m[20230207 15:36:37 @agent_ppo2.py:192][0m |          -0.0144 |           3.1890 |           0.1749 |
[32m[20230207 15:36:37 @agent_ppo2.py:192][0m |          -0.0073 |           3.0045 |           0.1750 |
[32m[20230207 15:36:37 @agent_ppo2.py:192][0m |          -0.0067 |           2.8900 |           0.1748 |
[32m[20230207 15:36:38 @agent_ppo2.py:192][0m |          -0.0216 |           2.8167 |           0.1746 |
[32m[20230207 15:36:38 @agent_ppo2.py:192][0m |          -0.0111 |           2.6949 |           0.1747 |
[32m[20230207 15:36:38 @agent_ppo2.py:192][0m |          -0.0184 |           2.6150 |           0.1745 |
[32m[20230207 15:36:38 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:36:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: 45.13
[32m[20230207 15:36:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 72.02
[32m[20230207 15:36:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 86.57
[32m[20230207 15:36:38 @agent_ppo2.py:150][0m Total time:      35.95 min
[32m[20230207 15:36:38 @agent_ppo2.py:152][0m 1902592 total steps have happened
[32m[20230207 15:36:38 @agent_ppo2.py:128][0m #------------------------ Iteration 929 --------------------------#
[32m[20230207 15:36:39 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 15:36:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:39 @agent_ppo2.py:192][0m |           0.0002 |          26.0247 |           0.1790 |
[32m[20230207 15:36:39 @agent_ppo2.py:192][0m |          -0.0040 |          13.7612 |           0.1788 |
[32m[20230207 15:36:39 @agent_ppo2.py:192][0m |          -0.0075 |           9.3344 |           0.1786 |
[32m[20230207 15:36:39 @agent_ppo2.py:192][0m |          -0.0088 |           7.7900 |           0.1786 |
[32m[20230207 15:36:40 @agent_ppo2.py:192][0m |          -0.0099 |           6.6472 |           0.1786 |
[32m[20230207 15:36:40 @agent_ppo2.py:192][0m |          -0.0117 |           5.8022 |           0.1785 |
[32m[20230207 15:36:40 @agent_ppo2.py:192][0m |          -0.0135 |           5.2129 |           0.1785 |
[32m[20230207 15:36:40 @agent_ppo2.py:192][0m |          -0.0133 |           4.7901 |           0.1784 |
[32m[20230207 15:36:40 @agent_ppo2.py:192][0m |          -0.0140 |           4.4744 |           0.1785 |
[32m[20230207 15:36:40 @agent_ppo2.py:192][0m |          -0.0142 |           4.2057 |           0.1784 |
[32m[20230207 15:36:40 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230207 15:36:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: 9.21
[32m[20230207 15:36:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 144.72
[32m[20230207 15:36:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 1.36
[32m[20230207 15:36:41 @agent_ppo2.py:150][0m Total time:      35.99 min
[32m[20230207 15:36:41 @agent_ppo2.py:152][0m 1904640 total steps have happened
[32m[20230207 15:36:41 @agent_ppo2.py:128][0m #------------------------ Iteration 930 --------------------------#
[32m[20230207 15:36:41 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:36:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:42 @agent_ppo2.py:192][0m |          -0.0078 |          11.7228 |           0.1802 |
[32m[20230207 15:36:42 @agent_ppo2.py:192][0m |          -0.0088 |           5.9674 |           0.1800 |
[32m[20230207 15:36:42 @agent_ppo2.py:192][0m |          -0.0056 |           5.2481 |           0.1796 |
[32m[20230207 15:36:42 @agent_ppo2.py:192][0m |          -0.0116 |           4.8055 |           0.1799 |
[32m[20230207 15:36:42 @agent_ppo2.py:192][0m |          -0.0199 |           4.4367 |           0.1798 |
[32m[20230207 15:36:42 @agent_ppo2.py:192][0m |          -0.0052 |           4.1611 |           0.1797 |
[32m[20230207 15:36:42 @agent_ppo2.py:192][0m |          -0.0131 |           3.9246 |           0.1798 |
[32m[20230207 15:36:42 @agent_ppo2.py:192][0m |          -0.0102 |           3.7590 |           0.1800 |
[32m[20230207 15:36:42 @agent_ppo2.py:192][0m |          -0.0166 |           3.5616 |           0.1798 |
[32m[20230207 15:36:42 @agent_ppo2.py:192][0m |          -0.0131 |           3.6727 |           0.1797 |
[32m[20230207 15:36:42 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:36:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: 37.32
[32m[20230207 15:36:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 122.97
[32m[20230207 15:36:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -33.07
[32m[20230207 15:36:43 @agent_ppo2.py:150][0m Total time:      36.03 min
[32m[20230207 15:36:43 @agent_ppo2.py:152][0m 1906688 total steps have happened
[32m[20230207 15:36:43 @agent_ppo2.py:128][0m #------------------------ Iteration 931 --------------------------#
[32m[20230207 15:36:44 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:36:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:44 @agent_ppo2.py:192][0m |           0.0014 |           7.8376 |           0.1813 |
[32m[20230207 15:36:44 @agent_ppo2.py:192][0m |          -0.0049 |           4.0979 |           0.1814 |
[32m[20230207 15:36:44 @agent_ppo2.py:192][0m |          -0.0073 |           3.5445 |           0.1811 |
[32m[20230207 15:36:44 @agent_ppo2.py:192][0m |          -0.0098 |           3.2528 |           0.1810 |
[32m[20230207 15:36:44 @agent_ppo2.py:192][0m |          -0.0116 |           3.0733 |           0.1811 |
[32m[20230207 15:36:44 @agent_ppo2.py:192][0m |          -0.0126 |           2.9205 |           0.1810 |
[32m[20230207 15:36:44 @agent_ppo2.py:192][0m |          -0.0131 |           2.8255 |           0.1809 |
[32m[20230207 15:36:45 @agent_ppo2.py:192][0m |          -0.0144 |           2.7181 |           0.1810 |
[32m[20230207 15:36:45 @agent_ppo2.py:192][0m |          -0.0158 |           2.5977 |           0.1809 |
[32m[20230207 15:36:45 @agent_ppo2.py:192][0m |          -0.0158 |           2.5228 |           0.1809 |
[32m[20230207 15:36:45 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:36:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -18.67
[32m[20230207 15:36:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 82.44
[32m[20230207 15:36:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -89.55
[32m[20230207 15:36:46 @agent_ppo2.py:150][0m Total time:      36.07 min
[32m[20230207 15:36:46 @agent_ppo2.py:152][0m 1908736 total steps have happened
[32m[20230207 15:36:46 @agent_ppo2.py:128][0m #------------------------ Iteration 932 --------------------------#
[32m[20230207 15:36:46 @agent_ppo2.py:134][0m Sampling time: 0.86 s by 1 slaves
[32m[20230207 15:36:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:47 @agent_ppo2.py:192][0m |          -0.0049 |           7.0548 |           0.1803 |
[32m[20230207 15:36:47 @agent_ppo2.py:192][0m |          -0.0067 |           4.6041 |           0.1801 |
[32m[20230207 15:36:47 @agent_ppo2.py:192][0m |          -0.0172 |           4.1418 |           0.1800 |
[32m[20230207 15:36:47 @agent_ppo2.py:192][0m |          -0.0057 |           3.7609 |           0.1796 |
[32m[20230207 15:36:47 @agent_ppo2.py:192][0m |          -0.0123 |           3.5003 |           0.1795 |
[32m[20230207 15:36:47 @agent_ppo2.py:192][0m |          -0.0228 |           3.4160 |           0.1795 |
[32m[20230207 15:36:47 @agent_ppo2.py:192][0m |          -0.0097 |           3.2024 |           0.1797 |
[32m[20230207 15:36:47 @agent_ppo2.py:192][0m |          -0.0165 |           3.1262 |           0.1795 |
[32m[20230207 15:36:47 @agent_ppo2.py:192][0m |          -0.0122 |           3.0377 |           0.1795 |
[32m[20230207 15:36:47 @agent_ppo2.py:192][0m |          -0.0065 |           3.0018 |           0.1794 |
[32m[20230207 15:36:47 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:36:48 @agent_ppo2.py:145][0m Average TRAINING episode reward: 3.89
[32m[20230207 15:36:48 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 101.20
[32m[20230207 15:36:48 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -31.19
[32m[20230207 15:36:48 @agent_ppo2.py:150][0m Total time:      36.12 min
[32m[20230207 15:36:48 @agent_ppo2.py:152][0m 1910784 total steps have happened
[32m[20230207 15:36:48 @agent_ppo2.py:128][0m #------------------------ Iteration 933 --------------------------#
[32m[20230207 15:36:49 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:36:49 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:49 @agent_ppo2.py:192][0m |          -0.0050 |           5.4566 |           0.1809 |
[32m[20230207 15:36:49 @agent_ppo2.py:192][0m |          -0.0052 |           4.0094 |           0.1805 |
[32m[20230207 15:36:49 @agent_ppo2.py:192][0m |          -0.0011 |           3.5818 |           0.1806 |
[32m[20230207 15:36:49 @agent_ppo2.py:192][0m |          -0.0151 |           3.2688 |           0.1804 |
[32m[20230207 15:36:49 @agent_ppo2.py:192][0m |          -0.0208 |           3.0839 |           0.1804 |
[32m[20230207 15:36:49 @agent_ppo2.py:192][0m |           0.0005 |           2.8879 |           0.1803 |
[32m[20230207 15:36:50 @agent_ppo2.py:192][0m |          -0.0139 |           2.7556 |           0.1805 |
[32m[20230207 15:36:50 @agent_ppo2.py:192][0m |          -0.0189 |           2.5919 |           0.1804 |
[32m[20230207 15:36:50 @agent_ppo2.py:192][0m |          -0.0112 |           2.4550 |           0.1803 |
[32m[20230207 15:36:50 @agent_ppo2.py:192][0m |          -0.0237 |           2.3261 |           0.1803 |
[32m[20230207 15:36:50 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:36:50 @agent_ppo2.py:145][0m Average TRAINING episode reward: 98.19
[32m[20230207 15:36:50 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 149.31
[32m[20230207 15:36:50 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -74.88
[32m[20230207 15:36:50 @agent_ppo2.py:150][0m Total time:      36.15 min
[32m[20230207 15:36:50 @agent_ppo2.py:152][0m 1912832 total steps have happened
[32m[20230207 15:36:50 @agent_ppo2.py:128][0m #------------------------ Iteration 934 --------------------------#
[32m[20230207 15:36:51 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:36:51 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:51 @agent_ppo2.py:192][0m |           0.0018 |          19.3692 |           0.1836 |
[32m[20230207 15:36:51 @agent_ppo2.py:192][0m |          -0.0093 |           5.5293 |           0.1832 |
[32m[20230207 15:36:51 @agent_ppo2.py:192][0m |          -0.0146 |           4.0548 |           0.1832 |
[32m[20230207 15:36:51 @agent_ppo2.py:192][0m |          -0.0154 |           3.4713 |           0.1831 |
[32m[20230207 15:36:51 @agent_ppo2.py:192][0m |          -0.0164 |           3.2462 |           0.1833 |
[32m[20230207 15:36:51 @agent_ppo2.py:192][0m |          -0.0188 |           3.0668 |           0.1831 |
[32m[20230207 15:36:51 @agent_ppo2.py:192][0m |          -0.0186 |           2.9655 |           0.1830 |
[32m[20230207 15:36:51 @agent_ppo2.py:192][0m |          -0.0222 |           2.8375 |           0.1831 |
[32m[20230207 15:36:51 @agent_ppo2.py:192][0m |          -0.0228 |           2.7458 |           0.1830 |
[32m[20230207 15:36:51 @agent_ppo2.py:192][0m |          -0.0231 |           2.6696 |           0.1830 |
[32m[20230207 15:36:51 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:36:52 @agent_ppo2.py:145][0m Average TRAINING episode reward: -51.32
[32m[20230207 15:36:52 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 89.99
[32m[20230207 15:36:52 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -27.93
[32m[20230207 15:36:52 @agent_ppo2.py:150][0m Total time:      36.18 min
[32m[20230207 15:36:52 @agent_ppo2.py:152][0m 1914880 total steps have happened
[32m[20230207 15:36:52 @agent_ppo2.py:128][0m #------------------------ Iteration 935 --------------------------#
[32m[20230207 15:36:53 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:36:53 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:53 @agent_ppo2.py:192][0m |          -0.0009 |          12.8420 |           0.1778 |
[32m[20230207 15:36:53 @agent_ppo2.py:192][0m |          -0.0066 |           6.5896 |           0.1775 |
[32m[20230207 15:36:53 @agent_ppo2.py:192][0m |          -0.0116 |           5.3271 |           0.1775 |
[32m[20230207 15:36:53 @agent_ppo2.py:192][0m |          -0.0140 |           4.3387 |           0.1774 |
[32m[20230207 15:36:53 @agent_ppo2.py:192][0m |          -0.0148 |           3.7443 |           0.1773 |
[32m[20230207 15:36:53 @agent_ppo2.py:192][0m |          -0.0155 |           3.2919 |           0.1772 |
[32m[20230207 15:36:53 @agent_ppo2.py:192][0m |          -0.0147 |           2.9759 |           0.1772 |
[32m[20230207 15:36:53 @agent_ppo2.py:192][0m |          -0.0181 |           2.7075 |           0.1771 |
[32m[20230207 15:36:53 @agent_ppo2.py:192][0m |          -0.0176 |           2.5384 |           0.1771 |
[32m[20230207 15:36:53 @agent_ppo2.py:192][0m |          -0.0187 |           2.3782 |           0.1770 |
[32m[20230207 15:36:53 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:36:54 @agent_ppo2.py:145][0m Average TRAINING episode reward: 60.22
[32m[20230207 15:36:54 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 191.20
[32m[20230207 15:36:54 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -7.19
[32m[20230207 15:36:54 @agent_ppo2.py:150][0m Total time:      36.21 min
[32m[20230207 15:36:54 @agent_ppo2.py:152][0m 1916928 total steps have happened
[32m[20230207 15:36:54 @agent_ppo2.py:128][0m #------------------------ Iteration 936 --------------------------#
[32m[20230207 15:36:55 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:36:55 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:55 @agent_ppo2.py:192][0m |           0.0054 |           3.3251 |           0.1792 |
[32m[20230207 15:36:55 @agent_ppo2.py:192][0m |          -0.0027 |           1.9213 |           0.1790 |
[32m[20230207 15:36:55 @agent_ppo2.py:192][0m |          -0.0285 |           1.8184 |           0.1792 |
[32m[20230207 15:36:55 @agent_ppo2.py:192][0m |           0.0109 |           1.7358 |           0.1795 |
[32m[20230207 15:36:55 @agent_ppo2.py:192][0m |           0.0035 |           1.6419 |           0.1793 |
[32m[20230207 15:36:55 @agent_ppo2.py:192][0m |          -0.0163 |           1.5808 |           0.1791 |
[32m[20230207 15:36:55 @agent_ppo2.py:192][0m |          -0.0190 |           1.5646 |           0.1793 |
[32m[20230207 15:36:55 @agent_ppo2.py:192][0m |          -0.0067 |           1.5515 |           0.1791 |
[32m[20230207 15:36:55 @agent_ppo2.py:192][0m |          -0.0083 |           1.5220 |           0.1792 |
[32m[20230207 15:36:55 @agent_ppo2.py:192][0m |          -0.0012 |           1.4987 |           0.1791 |
[32m[20230207 15:36:55 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:36:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: 67.49
[32m[20230207 15:36:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 69.84
[32m[20230207 15:36:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -66.59
[32m[20230207 15:36:56 @agent_ppo2.py:150][0m Total time:      36.25 min
[32m[20230207 15:36:56 @agent_ppo2.py:152][0m 1918976 total steps have happened
[32m[20230207 15:36:56 @agent_ppo2.py:128][0m #------------------------ Iteration 937 --------------------------#
[32m[20230207 15:36:57 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:36:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:57 @agent_ppo2.py:192][0m |           0.0084 |           2.6952 |           0.1791 |
[32m[20230207 15:36:57 @agent_ppo2.py:192][0m |          -0.0008 |           2.2448 |           0.1787 |
[32m[20230207 15:36:57 @agent_ppo2.py:192][0m |          -0.0050 |           2.1346 |           0.1786 |
[32m[20230207 15:36:57 @agent_ppo2.py:192][0m |          -0.0064 |           2.0339 |           0.1784 |
[32m[20230207 15:36:57 @agent_ppo2.py:192][0m |          -0.0165 |           1.9639 |           0.1784 |
[32m[20230207 15:36:57 @agent_ppo2.py:192][0m |          -0.0011 |           1.8835 |           0.1781 |
[32m[20230207 15:36:57 @agent_ppo2.py:192][0m |          -0.0025 |           1.8464 |           0.1783 |
[32m[20230207 15:36:57 @agent_ppo2.py:192][0m |          -0.0184 |           1.7812 |           0.1780 |
[32m[20230207 15:36:58 @agent_ppo2.py:192][0m |          -0.0168 |           1.7348 |           0.1782 |
[32m[20230207 15:36:58 @agent_ppo2.py:192][0m |          -0.0219 |           1.7028 |           0.1780 |
[32m[20230207 15:36:58 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:36:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: 157.16
[32m[20230207 15:36:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 178.46
[32m[20230207 15:36:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -31.82
[32m[20230207 15:36:58 @agent_ppo2.py:150][0m Total time:      36.29 min
[32m[20230207 15:36:58 @agent_ppo2.py:152][0m 1921024 total steps have happened
[32m[20230207 15:36:58 @agent_ppo2.py:128][0m #------------------------ Iteration 938 --------------------------#
[32m[20230207 15:36:59 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:36:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:36:59 @agent_ppo2.py:192][0m |           0.0015 |          11.0860 |           0.1825 |
[32m[20230207 15:36:59 @agent_ppo2.py:192][0m |          -0.0021 |           7.9150 |           0.1823 |
[32m[20230207 15:36:59 @agent_ppo2.py:192][0m |          -0.0030 |           7.6195 |           0.1822 |
[32m[20230207 15:37:00 @agent_ppo2.py:192][0m |          -0.0051 |           7.5064 |           0.1823 |
[32m[20230207 15:37:00 @agent_ppo2.py:192][0m |          -0.0051 |           7.5145 |           0.1821 |
[32m[20230207 15:37:00 @agent_ppo2.py:192][0m |          -0.0068 |           7.2638 |           0.1822 |
[32m[20230207 15:37:00 @agent_ppo2.py:192][0m |          -0.0071 |           7.1871 |           0.1820 |
[32m[20230207 15:37:00 @agent_ppo2.py:192][0m |          -0.0083 |           7.1427 |           0.1820 |
[32m[20230207 15:37:00 @agent_ppo2.py:192][0m |          -0.0087 |           7.0501 |           0.1820 |
[32m[20230207 15:37:00 @agent_ppo2.py:192][0m |          -0.0096 |           7.0679 |           0.1819 |
[32m[20230207 15:37:00 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:37:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: 34.36
[32m[20230207 15:37:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 110.06
[32m[20230207 15:37:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 113.30
[32m[20230207 15:37:01 @agent_ppo2.py:150][0m Total time:      36.33 min
[32m[20230207 15:37:01 @agent_ppo2.py:152][0m 1923072 total steps have happened
[32m[20230207 15:37:01 @agent_ppo2.py:128][0m #------------------------ Iteration 939 --------------------------#
[32m[20230207 15:37:01 @agent_ppo2.py:134][0m Sampling time: 0.73 s by 1 slaves
[32m[20230207 15:37:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:02 @agent_ppo2.py:192][0m |          -0.0020 |          24.5183 |           0.1785 |
[32m[20230207 15:37:02 @agent_ppo2.py:192][0m |          -0.0025 |          14.5708 |           0.1783 |
[32m[20230207 15:37:02 @agent_ppo2.py:192][0m |          -0.0068 |          10.9024 |           0.1783 |
[32m[20230207 15:37:02 @agent_ppo2.py:192][0m |          -0.0081 |           9.2029 |           0.1782 |
[32m[20230207 15:37:02 @agent_ppo2.py:192][0m |          -0.0120 |           7.5697 |           0.1780 |
[32m[20230207 15:37:02 @agent_ppo2.py:192][0m |          -0.0162 |           7.0097 |           0.1780 |
[32m[20230207 15:37:02 @agent_ppo2.py:192][0m |          -0.0069 |           6.2392 |           0.1780 |
[32m[20230207 15:37:02 @agent_ppo2.py:192][0m |          -0.0094 |           5.5878 |           0.1781 |
[32m[20230207 15:37:02 @agent_ppo2.py:192][0m |          -0.0135 |           5.1279 |           0.1781 |
[32m[20230207 15:37:02 @agent_ppo2.py:192][0m |          -0.0156 |           4.9314 |           0.1782 |
[32m[20230207 15:37:02 @agent_ppo2.py:137][0m Policy update time: 0.76 s
[32m[20230207 15:37:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: 8.12
[32m[20230207 15:37:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 32.82
[32m[20230207 15:37:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 27.19
[32m[20230207 15:37:03 @agent_ppo2.py:150][0m Total time:      36.36 min
[32m[20230207 15:37:03 @agent_ppo2.py:152][0m 1925120 total steps have happened
[32m[20230207 15:37:03 @agent_ppo2.py:128][0m #------------------------ Iteration 940 --------------------------#
[32m[20230207 15:37:04 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:37:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:04 @agent_ppo2.py:192][0m |           0.0097 |           5.9490 |           0.1769 |
[32m[20230207 15:37:04 @agent_ppo2.py:192][0m |          -0.0004 |           3.9889 |           0.1765 |
[32m[20230207 15:37:04 @agent_ppo2.py:192][0m |          -0.0069 |           3.6307 |           0.1767 |
[32m[20230207 15:37:04 @agent_ppo2.py:192][0m |          -0.0134 |           3.3930 |           0.1766 |
[32m[20230207 15:37:04 @agent_ppo2.py:192][0m |          -0.0087 |           3.2046 |           0.1766 |
[32m[20230207 15:37:04 @agent_ppo2.py:192][0m |          -0.0103 |           3.0423 |           0.1766 |
[32m[20230207 15:37:04 @agent_ppo2.py:192][0m |          -0.0072 |           4.2849 |           0.1765 |
[32m[20230207 15:37:04 @agent_ppo2.py:192][0m |          -0.0121 |           2.7570 |           0.1764 |
[32m[20230207 15:37:04 @agent_ppo2.py:192][0m |          -0.0187 |           2.6053 |           0.1765 |
[32m[20230207 15:37:04 @agent_ppo2.py:192][0m |          -0.0195 |           2.4949 |           0.1765 |
[32m[20230207 15:37:04 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:37:05 @agent_ppo2.py:145][0m Average TRAINING episode reward: 25.38
[32m[20230207 15:37:05 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 137.02
[32m[20230207 15:37:05 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 20.17
[32m[20230207 15:37:05 @agent_ppo2.py:150][0m Total time:      36.40 min
[32m[20230207 15:37:05 @agent_ppo2.py:152][0m 1927168 total steps have happened
[32m[20230207 15:37:05 @agent_ppo2.py:128][0m #------------------------ Iteration 941 --------------------------#
[32m[20230207 15:37:06 @agent_ppo2.py:134][0m Sampling time: 0.54 s by 1 slaves
[32m[20230207 15:37:06 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:06 @agent_ppo2.py:192][0m |           0.0034 |          21.6027 |           0.1820 |
[32m[20230207 15:37:06 @agent_ppo2.py:192][0m |          -0.0036 |          13.4539 |           0.1819 |
[32m[20230207 15:37:06 @agent_ppo2.py:192][0m |          -0.0067 |          10.4603 |           0.1819 |
[32m[20230207 15:37:06 @agent_ppo2.py:192][0m |          -0.0084 |           9.3463 |           0.1819 |
[32m[20230207 15:37:06 @agent_ppo2.py:192][0m |          -0.0141 |           8.1538 |           0.1817 |
[32m[20230207 15:37:06 @agent_ppo2.py:192][0m |          -0.0096 |           7.4872 |           0.1817 |
[32m[20230207 15:37:06 @agent_ppo2.py:192][0m |          -0.0128 |           6.9746 |           0.1818 |
[32m[20230207 15:37:06 @agent_ppo2.py:192][0m |          -0.0126 |           6.4799 |           0.1813 |
[32m[20230207 15:37:06 @agent_ppo2.py:192][0m |          -0.0161 |           6.1693 |           0.1819 |
[32m[20230207 15:37:06 @agent_ppo2.py:192][0m |          -0.0179 |           5.7620 |           0.1818 |
[32m[20230207 15:37:06 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:37:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: 25.97
[32m[20230207 15:37:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 117.19
[32m[20230207 15:37:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 88.67
[32m[20230207 15:37:07 @agent_ppo2.py:150][0m Total time:      36.43 min
[32m[20230207 15:37:07 @agent_ppo2.py:152][0m 1929216 total steps have happened
[32m[20230207 15:37:07 @agent_ppo2.py:128][0m #------------------------ Iteration 942 --------------------------#
[32m[20230207 15:37:08 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230207 15:37:08 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:08 @agent_ppo2.py:192][0m |          -0.0004 |          21.0823 |           0.1776 |
[32m[20230207 15:37:08 @agent_ppo2.py:192][0m |          -0.0086 |          12.2957 |           0.1772 |
[32m[20230207 15:37:08 @agent_ppo2.py:192][0m |          -0.0108 |          10.7576 |           0.1769 |
[32m[20230207 15:37:08 @agent_ppo2.py:192][0m |          -0.0142 |           8.8944 |           0.1767 |
[32m[20230207 15:37:08 @agent_ppo2.py:192][0m |          -0.0156 |           7.6949 |           0.1767 |
[32m[20230207 15:37:08 @agent_ppo2.py:192][0m |          -0.0170 |           7.0997 |           0.1763 |
[32m[20230207 15:37:08 @agent_ppo2.py:192][0m |          -0.0156 |           6.6636 |           0.1763 |
[32m[20230207 15:37:08 @agent_ppo2.py:192][0m |          -0.0168 |           6.2785 |           0.1760 |
[32m[20230207 15:37:08 @agent_ppo2.py:192][0m |          -0.0158 |           5.7242 |           0.1759 |
[32m[20230207 15:37:08 @agent_ppo2.py:192][0m |          -0.0187 |           5.4642 |           0.1759 |
[32m[20230207 15:37:08 @agent_ppo2.py:137][0m Policy update time: 0.64 s
[32m[20230207 15:37:09 @agent_ppo2.py:145][0m Average TRAINING episode reward: -64.82
[32m[20230207 15:37:09 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 30.09
[32m[20230207 15:37:09 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 159.38
[32m[20230207 15:37:09 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 159.38
[32m[20230207 15:37:09 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 159.38
[32m[20230207 15:37:09 @agent_ppo2.py:150][0m Total time:      36.47 min
[32m[20230207 15:37:09 @agent_ppo2.py:152][0m 1931264 total steps have happened
[32m[20230207 15:37:09 @agent_ppo2.py:128][0m #------------------------ Iteration 943 --------------------------#
[32m[20230207 15:37:10 @agent_ppo2.py:134][0m Sampling time: 0.87 s by 1 slaves
[32m[20230207 15:37:10 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:10 @agent_ppo2.py:192][0m |           0.0011 |          11.4433 |           0.1807 |
[32m[20230207 15:37:10 @agent_ppo2.py:192][0m |          -0.0028 |           6.4375 |           0.1809 |
[32m[20230207 15:37:10 @agent_ppo2.py:192][0m |          -0.0046 |           5.4688 |           0.1806 |
[32m[20230207 15:37:10 @agent_ppo2.py:192][0m |          -0.0057 |           4.9437 |           0.1805 |
[32m[20230207 15:37:10 @agent_ppo2.py:192][0m |          -0.0071 |           4.6168 |           0.1805 |
[32m[20230207 15:37:11 @agent_ppo2.py:192][0m |          -0.0083 |           4.3551 |           0.1805 |
[32m[20230207 15:37:11 @agent_ppo2.py:192][0m |          -0.0083 |           4.1233 |           0.1805 |
[32m[20230207 15:37:11 @agent_ppo2.py:192][0m |          -0.0102 |           3.9186 |           0.1803 |
[32m[20230207 15:37:11 @agent_ppo2.py:192][0m |          -0.0118 |           3.8066 |           0.1805 |
[32m[20230207 15:37:11 @agent_ppo2.py:192][0m |          -0.0127 |           3.6086 |           0.1805 |
[32m[20230207 15:37:11 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:37:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: 31.75
[32m[20230207 15:37:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 185.00
[32m[20230207 15:37:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 120.30
[32m[20230207 15:37:12 @agent_ppo2.py:150][0m Total time:      36.51 min
[32m[20230207 15:37:12 @agent_ppo2.py:152][0m 1933312 total steps have happened
[32m[20230207 15:37:12 @agent_ppo2.py:128][0m #------------------------ Iteration 944 --------------------------#
[32m[20230207 15:37:12 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:37:13 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:13 @agent_ppo2.py:192][0m |          -0.0094 |           3.9229 |           0.1743 |
[32m[20230207 15:37:13 @agent_ppo2.py:192][0m |          -0.0147 |           3.0349 |           0.1740 |
[32m[20230207 15:37:13 @agent_ppo2.py:192][0m |           0.0132 |           2.7737 |           0.1738 |
[32m[20230207 15:37:13 @agent_ppo2.py:192][0m |           0.0052 |           2.6224 |           0.1737 |
[32m[20230207 15:37:13 @agent_ppo2.py:192][0m |          -0.0045 |           2.5472 |           0.1731 |
[32m[20230207 15:37:13 @agent_ppo2.py:192][0m |          -0.0247 |           2.4755 |           0.1736 |
[32m[20230207 15:37:13 @agent_ppo2.py:192][0m |          -0.0021 |           2.4344 |           0.1735 |
[32m[20230207 15:37:13 @agent_ppo2.py:192][0m |          -0.0045 |           2.3550 |           0.1737 |
[32m[20230207 15:37:13 @agent_ppo2.py:192][0m |          -0.0043 |           2.2758 |           0.1734 |
[32m[20230207 15:37:13 @agent_ppo2.py:192][0m |          -0.0079 |           2.2500 |           0.1734 |
[32m[20230207 15:37:13 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:37:14 @agent_ppo2.py:145][0m Average TRAINING episode reward: 106.13
[32m[20230207 15:37:14 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 120.76
[32m[20230207 15:37:14 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 43.85
[32m[20230207 15:37:14 @agent_ppo2.py:150][0m Total time:      36.55 min
[32m[20230207 15:37:14 @agent_ppo2.py:152][0m 1935360 total steps have happened
[32m[20230207 15:37:14 @agent_ppo2.py:128][0m #------------------------ Iteration 945 --------------------------#
[32m[20230207 15:37:15 @agent_ppo2.py:134][0m Sampling time: 0.88 s by 1 slaves
[32m[20230207 15:37:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:15 @agent_ppo2.py:192][0m |          -0.0009 |          14.9426 |           0.1753 |
[32m[20230207 15:37:15 @agent_ppo2.py:192][0m |          -0.0087 |           6.6413 |           0.1749 |
[32m[20230207 15:37:15 @agent_ppo2.py:192][0m |          -0.0109 |           3.7947 |           0.1746 |
[32m[20230207 15:37:15 @agent_ppo2.py:192][0m |          -0.0133 |           3.2474 |           0.1747 |
[32m[20230207 15:37:16 @agent_ppo2.py:192][0m |          -0.0133 |           2.9041 |           0.1745 |
[32m[20230207 15:37:16 @agent_ppo2.py:192][0m |          -0.0191 |           2.7432 |           0.1745 |
[32m[20230207 15:37:16 @agent_ppo2.py:192][0m |          -0.0051 |           2.6560 |           0.1745 |
[32m[20230207 15:37:16 @agent_ppo2.py:192][0m |          -0.0137 |           2.5109 |           0.1745 |
[32m[20230207 15:37:16 @agent_ppo2.py:192][0m |          -0.0116 |           2.3536 |           0.1743 |
[32m[20230207 15:37:16 @agent_ppo2.py:192][0m |           0.0009 |           2.2831 |           0.1743 |
[32m[20230207 15:37:16 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 15:37:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: 25.87
[32m[20230207 15:37:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 88.22
[32m[20230207 15:37:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -68.49
[32m[20230207 15:37:16 @agent_ppo2.py:150][0m Total time:      36.59 min
[32m[20230207 15:37:16 @agent_ppo2.py:152][0m 1937408 total steps have happened
[32m[20230207 15:37:16 @agent_ppo2.py:128][0m #------------------------ Iteration 946 --------------------------#
[32m[20230207 15:37:17 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:37:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:17 @agent_ppo2.py:192][0m |          -0.0141 |           3.0181 |           0.1752 |
[32m[20230207 15:37:17 @agent_ppo2.py:192][0m |          -0.0057 |           2.3370 |           0.1751 |
[32m[20230207 15:37:18 @agent_ppo2.py:192][0m |           0.0023 |           2.1907 |           0.1751 |
[32m[20230207 15:37:18 @agent_ppo2.py:192][0m |           0.0010 |           2.0923 |           0.1751 |
[32m[20230207 15:37:18 @agent_ppo2.py:192][0m |          -0.0089 |           2.0112 |           0.1751 |
[32m[20230207 15:37:18 @agent_ppo2.py:192][0m |          -0.0092 |           1.9756 |           0.1751 |
[32m[20230207 15:37:18 @agent_ppo2.py:192][0m |           0.0250 |           2.0261 |           0.1751 |
[32m[20230207 15:37:18 @agent_ppo2.py:192][0m |          -0.0015 |           1.9177 |           0.1748 |
[32m[20230207 15:37:18 @agent_ppo2.py:192][0m |           0.0043 |           1.8892 |           0.1748 |
[32m[20230207 15:37:18 @agent_ppo2.py:192][0m |          -0.0041 |           1.8429 |           0.1749 |
[32m[20230207 15:37:18 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:37:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: 129.46
[32m[20230207 15:37:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 133.51
[32m[20230207 15:37:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 17.97
[32m[20230207 15:37:19 @agent_ppo2.py:150][0m Total time:      36.63 min
[32m[20230207 15:37:19 @agent_ppo2.py:152][0m 1939456 total steps have happened
[32m[20230207 15:37:19 @agent_ppo2.py:128][0m #------------------------ Iteration 947 --------------------------#
[32m[20230207 15:37:20 @agent_ppo2.py:134][0m Sampling time: 0.67 s by 1 slaves
[32m[20230207 15:37:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:20 @agent_ppo2.py:192][0m |          -0.0052 |          44.8147 |           0.1745 |
[32m[20230207 15:37:20 @agent_ppo2.py:192][0m |          -0.0053 |          14.4115 |           0.1741 |
[32m[20230207 15:37:20 @agent_ppo2.py:192][0m |          -0.0153 |           9.2689 |           0.1744 |
[32m[20230207 15:37:20 @agent_ppo2.py:192][0m |          -0.0183 |           7.2500 |           0.1741 |
[32m[20230207 15:37:20 @agent_ppo2.py:192][0m |          -0.0290 |           5.9170 |           0.1742 |
[32m[20230207 15:37:20 @agent_ppo2.py:192][0m |          -0.0066 |           5.5287 |           0.1742 |
[32m[20230207 15:37:20 @agent_ppo2.py:192][0m |          -0.0247 |           5.0593 |           0.1740 |
[32m[20230207 15:37:20 @agent_ppo2.py:192][0m |          -0.0251 |           4.6478 |           0.1738 |
[32m[20230207 15:37:20 @agent_ppo2.py:192][0m |          -0.0160 |           4.5241 |           0.1737 |
[32m[20230207 15:37:20 @agent_ppo2.py:192][0m |           0.0177 |           4.0595 |           0.1737 |
[32m[20230207 15:37:20 @agent_ppo2.py:137][0m Policy update time: 0.69 s
[32m[20230207 15:37:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -88.91
[32m[20230207 15:37:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -60.22
[32m[20230207 15:37:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 54.14
[32m[20230207 15:37:21 @agent_ppo2.py:150][0m Total time:      36.66 min
[32m[20230207 15:37:21 @agent_ppo2.py:152][0m 1941504 total steps have happened
[32m[20230207 15:37:21 @agent_ppo2.py:128][0m #------------------------ Iteration 948 --------------------------#
[32m[20230207 15:37:22 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:37:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:22 @agent_ppo2.py:192][0m |          -0.0019 |           8.6678 |           0.1746 |
[32m[20230207 15:37:22 @agent_ppo2.py:192][0m |          -0.0056 |           6.3423 |           0.1745 |
[32m[20230207 15:37:22 @agent_ppo2.py:192][0m |          -0.0085 |           5.6388 |           0.1742 |
[32m[20230207 15:37:22 @agent_ppo2.py:192][0m |          -0.0112 |           5.1526 |           0.1739 |
[32m[20230207 15:37:22 @agent_ppo2.py:192][0m |          -0.0152 |           4.8536 |           0.1738 |
[32m[20230207 15:37:22 @agent_ppo2.py:192][0m |          -0.0135 |           4.5981 |           0.1737 |
[32m[20230207 15:37:22 @agent_ppo2.py:192][0m |          -0.0132 |           4.4073 |           0.1735 |
[32m[20230207 15:37:22 @agent_ppo2.py:192][0m |          -0.0171 |           4.2195 |           0.1734 |
[32m[20230207 15:37:23 @agent_ppo2.py:192][0m |          -0.0162 |           4.0655 |           0.1734 |
[32m[20230207 15:37:23 @agent_ppo2.py:192][0m |          -0.0172 |           3.9366 |           0.1733 |
[32m[20230207 15:37:23 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:37:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -5.30
[32m[20230207 15:37:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 65.27
[32m[20230207 15:37:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -25.62
[32m[20230207 15:37:23 @agent_ppo2.py:150][0m Total time:      36.70 min
[32m[20230207 15:37:23 @agent_ppo2.py:152][0m 1943552 total steps have happened
[32m[20230207 15:37:23 @agent_ppo2.py:128][0m #------------------------ Iteration 949 --------------------------#
[32m[20230207 15:37:24 @agent_ppo2.py:134][0m Sampling time: 0.62 s by 1 slaves
[32m[20230207 15:37:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:24 @agent_ppo2.py:192][0m |          -0.0018 |          38.7815 |           0.1802 |
[32m[20230207 15:37:24 @agent_ppo2.py:192][0m |          -0.0092 |          16.5071 |           0.1800 |
[32m[20230207 15:37:24 @agent_ppo2.py:192][0m |          -0.0112 |          10.8697 |           0.1799 |
[32m[20230207 15:37:24 @agent_ppo2.py:192][0m |          -0.0132 |           7.7788 |           0.1799 |
[32m[20230207 15:37:24 @agent_ppo2.py:192][0m |          -0.0145 |           6.4067 |           0.1798 |
[32m[20230207 15:37:24 @agent_ppo2.py:192][0m |          -0.0151 |           5.5296 |           0.1797 |
[32m[20230207 15:37:24 @agent_ppo2.py:192][0m |          -0.0177 |           4.9616 |           0.1795 |
[32m[20230207 15:37:24 @agent_ppo2.py:192][0m |          -0.0174 |           4.5863 |           0.1795 |
[32m[20230207 15:37:25 @agent_ppo2.py:192][0m |          -0.0199 |           4.3453 |           0.1794 |
[32m[20230207 15:37:25 @agent_ppo2.py:192][0m |          -0.0203 |           4.0593 |           0.1792 |
[32m[20230207 15:37:25 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:37:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -57.66
[32m[20230207 15:37:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 62.98
[32m[20230207 15:37:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -65.82
[32m[20230207 15:37:25 @agent_ppo2.py:150][0m Total time:      36.73 min
[32m[20230207 15:37:25 @agent_ppo2.py:152][0m 1945600 total steps have happened
[32m[20230207 15:37:25 @agent_ppo2.py:128][0m #------------------------ Iteration 950 --------------------------#
[32m[20230207 15:37:26 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230207 15:37:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:26 @agent_ppo2.py:192][0m |          -0.0000 |          14.3156 |           0.1765 |
[32m[20230207 15:37:26 @agent_ppo2.py:192][0m |          -0.0079 |           7.4052 |           0.1764 |
[32m[20230207 15:37:26 @agent_ppo2.py:192][0m |          -0.0101 |           5.0267 |           0.1762 |
[32m[20230207 15:37:26 @agent_ppo2.py:192][0m |          -0.0121 |           4.4666 |           0.1761 |
[32m[20230207 15:37:26 @agent_ppo2.py:192][0m |          -0.0115 |           4.0911 |           0.1761 |
[32m[20230207 15:37:26 @agent_ppo2.py:192][0m |          -0.0147 |           3.8623 |           0.1758 |
[32m[20230207 15:37:26 @agent_ppo2.py:192][0m |          -0.0147 |           3.6083 |           0.1759 |
[32m[20230207 15:37:26 @agent_ppo2.py:192][0m |          -0.0163 |           3.4246 |           0.1758 |
[32m[20230207 15:37:26 @agent_ppo2.py:192][0m |          -0.0156 |           3.2952 |           0.1757 |
[32m[20230207 15:37:26 @agent_ppo2.py:192][0m |          -0.0172 |           3.1675 |           0.1758 |
[32m[20230207 15:37:26 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 15:37:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -3.24
[32m[20230207 15:37:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 37.95
[32m[20230207 15:37:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -37.44
[32m[20230207 15:37:27 @agent_ppo2.py:150][0m Total time:      36.77 min
[32m[20230207 15:37:27 @agent_ppo2.py:152][0m 1947648 total steps have happened
[32m[20230207 15:37:27 @agent_ppo2.py:128][0m #------------------------ Iteration 951 --------------------------#
[32m[20230207 15:37:28 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230207 15:37:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:28 @agent_ppo2.py:192][0m |           0.0009 |          24.3269 |           0.1749 |
[32m[20230207 15:37:28 @agent_ppo2.py:192][0m |          -0.0056 |          12.9708 |           0.1753 |
[32m[20230207 15:37:28 @agent_ppo2.py:192][0m |          -0.0075 |          10.6761 |           0.1748 |
[32m[20230207 15:37:28 @agent_ppo2.py:192][0m |          -0.0115 |           9.5628 |           0.1750 |
[32m[20230207 15:37:28 @agent_ppo2.py:192][0m |          -0.0136 |           8.8378 |           0.1747 |
[32m[20230207 15:37:28 @agent_ppo2.py:192][0m |          -0.0152 |           8.2233 |           0.1747 |
[32m[20230207 15:37:28 @agent_ppo2.py:192][0m |          -0.0164 |           7.7216 |           0.1747 |
[32m[20230207 15:37:28 @agent_ppo2.py:192][0m |          -0.0160 |           7.3267 |           0.1745 |
[32m[20230207 15:37:28 @agent_ppo2.py:192][0m |          -0.0178 |           6.8685 |           0.1746 |
[32m[20230207 15:37:28 @agent_ppo2.py:192][0m |          -0.0188 |           6.5775 |           0.1745 |
[32m[20230207 15:37:28 @agent_ppo2.py:137][0m Policy update time: 0.64 s
[32m[20230207 15:37:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: 61.30
[32m[20230207 15:37:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 196.54
[32m[20230207 15:37:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 26.42
[32m[20230207 15:37:29 @agent_ppo2.py:150][0m Total time:      36.80 min
[32m[20230207 15:37:29 @agent_ppo2.py:152][0m 1949696 total steps have happened
[32m[20230207 15:37:29 @agent_ppo2.py:128][0m #------------------------ Iteration 952 --------------------------#
[32m[20230207 15:37:30 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230207 15:37:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:30 @agent_ppo2.py:192][0m |          -0.0011 |          24.5955 |           0.1761 |
[32m[20230207 15:37:30 @agent_ppo2.py:192][0m |          -0.0068 |          12.1547 |           0.1760 |
[32m[20230207 15:37:30 @agent_ppo2.py:192][0m |          -0.0092 |           9.5447 |           0.1758 |
[32m[20230207 15:37:30 @agent_ppo2.py:192][0m |          -0.0107 |           8.4965 |           0.1758 |
[32m[20230207 15:37:30 @agent_ppo2.py:192][0m |          -0.0119 |           7.6577 |           0.1757 |
[32m[20230207 15:37:30 @agent_ppo2.py:192][0m |          -0.0132 |           7.1587 |           0.1756 |
[32m[20230207 15:37:31 @agent_ppo2.py:192][0m |          -0.0137 |           6.6264 |           0.1756 |
[32m[20230207 15:37:31 @agent_ppo2.py:192][0m |          -0.0150 |           6.3560 |           0.1755 |
[32m[20230207 15:37:31 @agent_ppo2.py:192][0m |          -0.0155 |           6.1058 |           0.1756 |
[32m[20230207 15:37:31 @agent_ppo2.py:192][0m |          -0.0157 |           5.7817 |           0.1755 |
[32m[20230207 15:37:31 @agent_ppo2.py:137][0m Policy update time: 0.97 s
[32m[20230207 15:37:32 @agent_ppo2.py:145][0m Average TRAINING episode reward: 7.07
[32m[20230207 15:37:32 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 126.02
[32m[20230207 15:37:32 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 166.99
[32m[20230207 15:37:32 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 166.99
[32m[20230207 15:37:32 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 166.99
[32m[20230207 15:37:32 @agent_ppo2.py:150][0m Total time:      36.84 min
[32m[20230207 15:37:32 @agent_ppo2.py:152][0m 1951744 total steps have happened
[32m[20230207 15:37:32 @agent_ppo2.py:128][0m #------------------------ Iteration 953 --------------------------#
[32m[20230207 15:37:32 @agent_ppo2.py:134][0m Sampling time: 0.74 s by 1 slaves
[32m[20230207 15:37:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:32 @agent_ppo2.py:192][0m |          -0.0010 |          16.9212 |           0.1745 |
[32m[20230207 15:37:33 @agent_ppo2.py:192][0m |          -0.0084 |          10.3441 |           0.1746 |
[32m[20230207 15:37:33 @agent_ppo2.py:192][0m |          -0.0096 |           8.5987 |           0.1747 |
[32m[20230207 15:37:33 @agent_ppo2.py:192][0m |          -0.0056 |           7.9793 |           0.1745 |
[32m[20230207 15:37:33 @agent_ppo2.py:192][0m |          -0.0159 |           7.3505 |           0.1745 |
[32m[20230207 15:37:33 @agent_ppo2.py:192][0m |          -0.0160 |           6.9667 |           0.1746 |
[32m[20230207 15:37:33 @agent_ppo2.py:192][0m |          -0.0166 |           6.7423 |           0.1745 |
[32m[20230207 15:37:33 @agent_ppo2.py:192][0m |          -0.0129 |           6.8402 |           0.1744 |
[32m[20230207 15:37:33 @agent_ppo2.py:192][0m |          -0.0181 |           6.2223 |           0.1741 |
[32m[20230207 15:37:33 @agent_ppo2.py:192][0m |          -0.0160 |           6.0320 |           0.1743 |
[32m[20230207 15:37:33 @agent_ppo2.py:137][0m Policy update time: 0.81 s
[32m[20230207 15:37:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: -20.05
[32m[20230207 15:37:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 69.85
[32m[20230207 15:37:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 31.10
[32m[20230207 15:37:34 @agent_ppo2.py:150][0m Total time:      36.88 min
[32m[20230207 15:37:34 @agent_ppo2.py:152][0m 1953792 total steps have happened
[32m[20230207 15:37:34 @agent_ppo2.py:128][0m #------------------------ Iteration 954 --------------------------#
[32m[20230207 15:37:35 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:37:35 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:35 @agent_ppo2.py:192][0m |          -0.0012 |          13.5600 |           0.1732 |
[32m[20230207 15:37:35 @agent_ppo2.py:192][0m |          -0.0007 |           7.5219 |           0.1730 |
[32m[20230207 15:37:35 @agent_ppo2.py:192][0m |          -0.0021 |           6.2786 |           0.1731 |
[32m[20230207 15:37:35 @agent_ppo2.py:192][0m |          -0.0092 |           5.6519 |           0.1728 |
[32m[20230207 15:37:35 @agent_ppo2.py:192][0m |          -0.0157 |           5.1491 |           0.1728 |
[32m[20230207 15:37:35 @agent_ppo2.py:192][0m |          -0.0101 |           4.8459 |           0.1728 |
[32m[20230207 15:37:35 @agent_ppo2.py:192][0m |          -0.0420 |           4.7610 |           0.1728 |
[32m[20230207 15:37:35 @agent_ppo2.py:192][0m |          -0.0163 |           4.8746 |           0.1728 |
[32m[20230207 15:37:36 @agent_ppo2.py:192][0m |          -0.0290 |           4.2924 |           0.1728 |
[32m[20230207 15:37:36 @agent_ppo2.py:192][0m |          -0.0207 |           4.0206 |           0.1728 |
[32m[20230207 15:37:36 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:37:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: 24.35
[32m[20230207 15:37:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 31.36
[32m[20230207 15:37:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -39.71
[32m[20230207 15:37:36 @agent_ppo2.py:150][0m Total time:      36.92 min
[32m[20230207 15:37:36 @agent_ppo2.py:152][0m 1955840 total steps have happened
[32m[20230207 15:37:36 @agent_ppo2.py:128][0m #------------------------ Iteration 955 --------------------------#
[32m[20230207 15:37:37 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 15:37:37 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:37 @agent_ppo2.py:192][0m |          -0.0062 |           3.1237 |           0.1685 |
[32m[20230207 15:37:37 @agent_ppo2.py:192][0m |          -0.0137 |           2.2958 |           0.1680 |
[32m[20230207 15:37:37 @agent_ppo2.py:192][0m |          -0.0042 |           2.0773 |           0.1678 |
[32m[20230207 15:37:37 @agent_ppo2.py:192][0m |          -0.0197 |           1.9197 |           0.1678 |
[32m[20230207 15:37:37 @agent_ppo2.py:192][0m |          -0.0026 |           1.8680 |           0.1676 |
[32m[20230207 15:37:37 @agent_ppo2.py:192][0m |          -0.0204 |           1.8260 |           0.1674 |
[32m[20230207 15:37:38 @agent_ppo2.py:192][0m |          -0.0001 |           1.7396 |           0.1676 |
[32m[20230207 15:37:38 @agent_ppo2.py:192][0m |          -0.0062 |           1.6956 |           0.1673 |
[32m[20230207 15:37:38 @agent_ppo2.py:192][0m |          -0.0231 |           1.6628 |           0.1672 |
[32m[20230207 15:37:38 @agent_ppo2.py:192][0m |           0.0014 |           1.6506 |           0.1674 |
[32m[20230207 15:37:38 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:37:39 @agent_ppo2.py:145][0m Average TRAINING episode reward: 106.51
[32m[20230207 15:37:39 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 127.62
[32m[20230207 15:37:39 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -10.26
[32m[20230207 15:37:39 @agent_ppo2.py:150][0m Total time:      36.96 min
[32m[20230207 15:37:39 @agent_ppo2.py:152][0m 1957888 total steps have happened
[32m[20230207 15:37:39 @agent_ppo2.py:128][0m #------------------------ Iteration 956 --------------------------#
[32m[20230207 15:37:39 @agent_ppo2.py:134][0m Sampling time: 0.52 s by 1 slaves
[32m[20230207 15:37:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:39 @agent_ppo2.py:192][0m |           0.0030 |          11.6549 |           0.1761 |
[32m[20230207 15:37:39 @agent_ppo2.py:192][0m |          -0.0039 |           3.7166 |           0.1760 |
[32m[20230207 15:37:39 @agent_ppo2.py:192][0m |          -0.0092 |           2.8166 |           0.1759 |
[32m[20230207 15:37:39 @agent_ppo2.py:192][0m |          -0.0101 |           2.5890 |           0.1761 |
[32m[20230207 15:37:39 @agent_ppo2.py:192][0m |          -0.0135 |           2.4161 |           0.1757 |
[32m[20230207 15:37:39 @agent_ppo2.py:192][0m |          -0.0145 |           2.3054 |           0.1756 |
[32m[20230207 15:37:40 @agent_ppo2.py:192][0m |          -0.0162 |           2.2081 |           0.1757 |
[32m[20230207 15:37:40 @agent_ppo2.py:192][0m |          -0.0142 |           2.1217 |           0.1754 |
[32m[20230207 15:37:40 @agent_ppo2.py:192][0m |          -0.0187 |           2.0715 |           0.1754 |
[32m[20230207 15:37:40 @agent_ppo2.py:192][0m |          -0.0190 |           1.9955 |           0.1755 |
[32m[20230207 15:37:40 @agent_ppo2.py:137][0m Policy update time: 0.56 s
[32m[20230207 15:37:41 @agent_ppo2.py:145][0m Average TRAINING episode reward: 65.15
[32m[20230207 15:37:41 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 188.59
[32m[20230207 15:37:41 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -10.70
[32m[20230207 15:37:41 @agent_ppo2.py:150][0m Total time:      36.99 min
[32m[20230207 15:37:41 @agent_ppo2.py:152][0m 1959936 total steps have happened
[32m[20230207 15:37:41 @agent_ppo2.py:128][0m #------------------------ Iteration 957 --------------------------#
[32m[20230207 15:37:41 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:37:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:41 @agent_ppo2.py:192][0m |          -0.0012 |          14.9601 |           0.1758 |
[32m[20230207 15:37:42 @agent_ppo2.py:192][0m |          -0.0063 |           6.6449 |           0.1755 |
[32m[20230207 15:37:42 @agent_ppo2.py:192][0m |          -0.0079 |           4.0132 |           0.1754 |
[32m[20230207 15:37:42 @agent_ppo2.py:192][0m |          -0.0087 |           3.1051 |           0.1753 |
[32m[20230207 15:37:42 @agent_ppo2.py:192][0m |          -0.0121 |           2.7284 |           0.1752 |
[32m[20230207 15:37:42 @agent_ppo2.py:192][0m |          -0.0106 |           2.5379 |           0.1751 |
[32m[20230207 15:37:42 @agent_ppo2.py:192][0m |          -0.0126 |           2.3256 |           0.1751 |
[32m[20230207 15:37:42 @agent_ppo2.py:192][0m |          -0.0129 |           2.2216 |           0.1752 |
[32m[20230207 15:37:42 @agent_ppo2.py:192][0m |          -0.0104 |           2.1364 |           0.1750 |
[32m[20230207 15:37:42 @agent_ppo2.py:192][0m |          -0.0159 |           2.0865 |           0.1750 |
[32m[20230207 15:37:42 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:37:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: 72.68
[32m[20230207 15:37:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 204.30
[32m[20230207 15:37:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -11.21
[32m[20230207 15:37:43 @agent_ppo2.py:150][0m Total time:      37.03 min
[32m[20230207 15:37:43 @agent_ppo2.py:152][0m 1961984 total steps have happened
[32m[20230207 15:37:43 @agent_ppo2.py:128][0m #------------------------ Iteration 958 --------------------------#
[32m[20230207 15:37:43 @agent_ppo2.py:134][0m Sampling time: 0.57 s by 1 slaves
[32m[20230207 15:37:43 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:43 @agent_ppo2.py:192][0m |          -0.0006 |          10.5215 |           0.1749 |
[32m[20230207 15:37:44 @agent_ppo2.py:192][0m |          -0.0102 |           3.8643 |           0.1748 |
[32m[20230207 15:37:44 @agent_ppo2.py:192][0m |          -0.0134 |           3.0806 |           0.1746 |
[32m[20230207 15:37:44 @agent_ppo2.py:192][0m |          -0.0188 |           2.8271 |           0.1747 |
[32m[20230207 15:37:44 @agent_ppo2.py:192][0m |          -0.0154 |           2.6261 |           0.1748 |
[32m[20230207 15:37:44 @agent_ppo2.py:192][0m |          -0.0162 |           2.5042 |           0.1748 |
[32m[20230207 15:37:44 @agent_ppo2.py:192][0m |          -0.0186 |           2.4535 |           0.1746 |
[32m[20230207 15:37:44 @agent_ppo2.py:192][0m |          -0.0167 |           2.2486 |           0.1748 |
[32m[20230207 15:37:44 @agent_ppo2.py:192][0m |          -0.0201 |           2.2619 |           0.1747 |
[32m[20230207 15:37:44 @agent_ppo2.py:192][0m |          -0.0197 |           2.1216 |           0.1748 |
[32m[20230207 15:37:44 @agent_ppo2.py:137][0m Policy update time: 0.64 s
[32m[20230207 15:37:45 @agent_ppo2.py:145][0m Average TRAINING episode reward: 22.54
[32m[20230207 15:37:45 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 139.21
[32m[20230207 15:37:45 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -9.03
[32m[20230207 15:37:45 @agent_ppo2.py:150][0m Total time:      37.06 min
[32m[20230207 15:37:45 @agent_ppo2.py:152][0m 1964032 total steps have happened
[32m[20230207 15:37:45 @agent_ppo2.py:128][0m #------------------------ Iteration 959 --------------------------#
[32m[20230207 15:37:45 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:37:45 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:45 @agent_ppo2.py:192][0m |          -0.0018 |          33.9243 |           0.1761 |
[32m[20230207 15:37:45 @agent_ppo2.py:192][0m |          -0.0028 |          12.1782 |           0.1761 |
[32m[20230207 15:37:45 @agent_ppo2.py:192][0m |          -0.0109 |           9.2580 |           0.1764 |
[32m[20230207 15:37:45 @agent_ppo2.py:192][0m |          -0.0134 |           8.0656 |           0.1764 |
[32m[20230207 15:37:45 @agent_ppo2.py:192][0m |          -0.0143 |           7.2220 |           0.1765 |
[32m[20230207 15:37:45 @agent_ppo2.py:192][0m |          -0.0147 |           6.7760 |           0.1764 |
[32m[20230207 15:37:45 @agent_ppo2.py:192][0m |          -0.0165 |           6.4077 |           0.1765 |
[32m[20230207 15:37:46 @agent_ppo2.py:192][0m |          -0.0167 |           6.0665 |           0.1763 |
[32m[20230207 15:37:46 @agent_ppo2.py:192][0m |          -0.0196 |           5.9504 |           0.1763 |
[32m[20230207 15:37:46 @agent_ppo2.py:192][0m |          -0.0204 |           5.7544 |           0.1764 |
[32m[20230207 15:37:46 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:37:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -21.77
[32m[20230207 15:37:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 127.46
[32m[20230207 15:37:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 128.46
[32m[20230207 15:37:46 @agent_ppo2.py:150][0m Total time:      37.09 min
[32m[20230207 15:37:46 @agent_ppo2.py:152][0m 1966080 total steps have happened
[32m[20230207 15:37:46 @agent_ppo2.py:128][0m #------------------------ Iteration 960 --------------------------#
[32m[20230207 15:37:47 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:37:47 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:47 @agent_ppo2.py:192][0m |           0.0036 |          28.6999 |           0.1768 |
[32m[20230207 15:37:47 @agent_ppo2.py:192][0m |          -0.0110 |          13.6276 |           0.1768 |
[32m[20230207 15:37:48 @agent_ppo2.py:192][0m |          -0.0090 |          11.4962 |           0.1764 |
[32m[20230207 15:37:48 @agent_ppo2.py:192][0m |          -0.0060 |          10.5293 |           0.1764 |
[32m[20230207 15:37:48 @agent_ppo2.py:192][0m |          -0.0121 |           9.9883 |           0.1763 |
[32m[20230207 15:37:48 @agent_ppo2.py:192][0m |          -0.0118 |           8.8879 |           0.1763 |
[32m[20230207 15:37:48 @agent_ppo2.py:192][0m |          -0.0160 |           8.3426 |           0.1762 |
[32m[20230207 15:37:48 @agent_ppo2.py:192][0m |          -0.0089 |           7.9818 |           0.1762 |
[32m[20230207 15:37:48 @agent_ppo2.py:192][0m |          -0.0188 |           7.6497 |           0.1759 |
[32m[20230207 15:37:48 @agent_ppo2.py:192][0m |          -0.0127 |           7.3291 |           0.1761 |
[32m[20230207 15:37:48 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:37:49 @agent_ppo2.py:145][0m Average TRAINING episode reward: -64.89
[32m[20230207 15:37:49 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 76.81
[32m[20230207 15:37:49 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -64.09
[32m[20230207 15:37:49 @agent_ppo2.py:150][0m Total time:      37.13 min
[32m[20230207 15:37:49 @agent_ppo2.py:152][0m 1968128 total steps have happened
[32m[20230207 15:37:49 @agent_ppo2.py:128][0m #------------------------ Iteration 961 --------------------------#
[32m[20230207 15:37:50 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:37:50 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:50 @agent_ppo2.py:192][0m |          -0.0060 |           2.6597 |           0.1773 |
[32m[20230207 15:37:50 @agent_ppo2.py:192][0m |          -0.0040 |           2.1621 |           0.1769 |
[32m[20230207 15:37:50 @agent_ppo2.py:192][0m |           0.0251 |           2.0254 |           0.1766 |
[32m[20230207 15:37:50 @agent_ppo2.py:192][0m |          -0.0159 |           1.8942 |           0.1768 |
[32m[20230207 15:37:50 @agent_ppo2.py:192][0m |          -0.0033 |           1.8046 |           0.1769 |
[32m[20230207 15:37:50 @agent_ppo2.py:192][0m |           0.0046 |           1.8026 |           0.1768 |
[32m[20230207 15:37:50 @agent_ppo2.py:192][0m |          -0.0145 |           1.7462 |           0.1767 |
[32m[20230207 15:37:50 @agent_ppo2.py:192][0m |          -0.0226 |           1.6606 |           0.1770 |
[32m[20230207 15:37:50 @agent_ppo2.py:192][0m |           0.0006 |           1.6041 |           0.1767 |
[32m[20230207 15:37:51 @agent_ppo2.py:192][0m |           0.0097 |           1.6254 |           0.1766 |
[32m[20230207 15:37:51 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230207 15:37:51 @agent_ppo2.py:145][0m Average TRAINING episode reward: 52.35
[32m[20230207 15:37:51 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 53.33
[32m[20230207 15:37:51 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -3.69
[32m[20230207 15:37:51 @agent_ppo2.py:150][0m Total time:      37.17 min
[32m[20230207 15:37:51 @agent_ppo2.py:152][0m 1970176 total steps have happened
[32m[20230207 15:37:51 @agent_ppo2.py:128][0m #------------------------ Iteration 962 --------------------------#
[32m[20230207 15:37:52 @agent_ppo2.py:134][0m Sampling time: 0.74 s by 1 slaves
[32m[20230207 15:37:52 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:52 @agent_ppo2.py:192][0m |           0.0008 |          14.0998 |           0.1752 |
[32m[20230207 15:37:52 @agent_ppo2.py:192][0m |          -0.0030 |           7.4321 |           0.1748 |
[32m[20230207 15:37:52 @agent_ppo2.py:192][0m |          -0.0056 |           5.7809 |           0.1750 |
[32m[20230207 15:37:52 @agent_ppo2.py:192][0m |          -0.0067 |           4.7537 |           0.1747 |
[32m[20230207 15:37:53 @agent_ppo2.py:192][0m |          -0.0087 |           4.1605 |           0.1751 |
[32m[20230207 15:37:53 @agent_ppo2.py:192][0m |          -0.0099 |           3.8692 |           0.1750 |
[32m[20230207 15:37:53 @agent_ppo2.py:192][0m |          -0.0114 |           3.5287 |           0.1752 |
[32m[20230207 15:37:53 @agent_ppo2.py:192][0m |          -0.0113 |           3.2318 |           0.1751 |
[32m[20230207 15:37:53 @agent_ppo2.py:192][0m |          -0.0127 |           3.1071 |           0.1752 |
[32m[20230207 15:37:53 @agent_ppo2.py:192][0m |          -0.0128 |           2.9032 |           0.1751 |
[32m[20230207 15:37:53 @agent_ppo2.py:137][0m Policy update time: 0.79 s
[32m[20230207 15:37:53 @agent_ppo2.py:145][0m Average TRAINING episode reward: 102.57
[32m[20230207 15:37:53 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 127.65
[32m[20230207 15:37:53 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -46.08
[32m[20230207 15:37:53 @agent_ppo2.py:150][0m Total time:      37.20 min
[32m[20230207 15:37:53 @agent_ppo2.py:152][0m 1972224 total steps have happened
[32m[20230207 15:37:53 @agent_ppo2.py:128][0m #------------------------ Iteration 963 --------------------------#
[32m[20230207 15:37:54 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:37:54 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:54 @agent_ppo2.py:192][0m |           0.0145 |          14.6396 |           0.1747 |
[32m[20230207 15:37:54 @agent_ppo2.py:192][0m |          -0.0130 |           6.0607 |           0.1745 |
[32m[20230207 15:37:54 @agent_ppo2.py:192][0m |          -0.0130 |           4.0480 |           0.1746 |
[32m[20230207 15:37:54 @agent_ppo2.py:192][0m |          -0.0120 |           3.3857 |           0.1744 |
[32m[20230207 15:37:54 @agent_ppo2.py:192][0m |          -0.0148 |           3.1719 |           0.1745 |
[32m[20230207 15:37:55 @agent_ppo2.py:192][0m |          -0.0165 |           2.7958 |           0.1744 |
[32m[20230207 15:37:55 @agent_ppo2.py:192][0m |          -0.0053 |           2.7121 |           0.1744 |
[32m[20230207 15:37:55 @agent_ppo2.py:192][0m |          -0.0125 |           2.5268 |           0.1745 |
[32m[20230207 15:37:55 @agent_ppo2.py:192][0m |          -0.0205 |           2.4215 |           0.1743 |
[32m[20230207 15:37:55 @agent_ppo2.py:192][0m |          -0.0232 |           2.3177 |           0.1745 |
[32m[20230207 15:37:55 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230207 15:37:56 @agent_ppo2.py:145][0m Average TRAINING episode reward: 70.88
[32m[20230207 15:37:56 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 161.38
[32m[20230207 15:37:56 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 44.73
[32m[20230207 15:37:56 @agent_ppo2.py:150][0m Total time:      37.24 min
[32m[20230207 15:37:56 @agent_ppo2.py:152][0m 1974272 total steps have happened
[32m[20230207 15:37:56 @agent_ppo2.py:128][0m #------------------------ Iteration 964 --------------------------#
[32m[20230207 15:37:56 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:37:56 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:57 @agent_ppo2.py:192][0m |           0.0001 |           3.4639 |           0.1758 |
[32m[20230207 15:37:57 @agent_ppo2.py:192][0m |          -0.0062 |           2.0756 |           0.1755 |
[32m[20230207 15:37:57 @agent_ppo2.py:192][0m |          -0.0101 |           1.8569 |           0.1751 |
[32m[20230207 15:37:57 @agent_ppo2.py:192][0m |          -0.0146 |           1.7903 |           0.1750 |
[32m[20230207 15:37:57 @agent_ppo2.py:192][0m |          -0.0176 |           1.6857 |           0.1749 |
[32m[20230207 15:37:57 @agent_ppo2.py:192][0m |          -0.0168 |           1.5915 |           0.1749 |
[32m[20230207 15:37:57 @agent_ppo2.py:192][0m |          -0.0161 |           1.5480 |           0.1747 |
[32m[20230207 15:37:57 @agent_ppo2.py:192][0m |          -0.0181 |           1.5178 |           0.1748 |
[32m[20230207 15:37:57 @agent_ppo2.py:192][0m |          -0.0176 |           1.4853 |           0.1747 |
[32m[20230207 15:37:57 @agent_ppo2.py:192][0m |          -0.0146 |           1.4450 |           0.1746 |
[32m[20230207 15:37:57 @agent_ppo2.py:137][0m Policy update time: 0.88 s
[32m[20230207 15:37:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: 25.01
[32m[20230207 15:37:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 101.65
[32m[20230207 15:37:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -33.17
[32m[20230207 15:37:58 @agent_ppo2.py:150][0m Total time:      37.28 min
[32m[20230207 15:37:58 @agent_ppo2.py:152][0m 1976320 total steps have happened
[32m[20230207 15:37:58 @agent_ppo2.py:128][0m #------------------------ Iteration 965 --------------------------#
[32m[20230207 15:37:58 @agent_ppo2.py:134][0m Sampling time: 0.65 s by 1 slaves
[32m[20230207 15:37:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:37:59 @agent_ppo2.py:192][0m |          -0.0024 |          12.4145 |           0.1757 |
[32m[20230207 15:37:59 @agent_ppo2.py:192][0m |          -0.0059 |           4.8730 |           0.1756 |
[32m[20230207 15:37:59 @agent_ppo2.py:192][0m |          -0.0045 |           3.5719 |           0.1755 |
[32m[20230207 15:37:59 @agent_ppo2.py:192][0m |           0.0078 |           3.1388 |           0.1755 |
[32m[20230207 15:37:59 @agent_ppo2.py:192][0m |          -0.0072 |           2.8876 |           0.1753 |
[32m[20230207 15:37:59 @agent_ppo2.py:192][0m |          -0.0141 |           2.7218 |           0.1751 |
[32m[20230207 15:37:59 @agent_ppo2.py:192][0m |          -0.0159 |           2.5805 |           0.1750 |
[32m[20230207 15:37:59 @agent_ppo2.py:192][0m |          -0.0191 |           2.4943 |           0.1749 |
[32m[20230207 15:37:59 @agent_ppo2.py:192][0m |          -0.0123 |           2.4289 |           0.1747 |
[32m[20230207 15:37:59 @agent_ppo2.py:192][0m |          -0.0159 |           2.3442 |           0.1748 |
[32m[20230207 15:37:59 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230207 15:38:00 @agent_ppo2.py:145][0m Average TRAINING episode reward: 26.57
[32m[20230207 15:38:00 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 89.74
[32m[20230207 15:38:00 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 52.86
[32m[20230207 15:38:00 @agent_ppo2.py:150][0m Total time:      37.31 min
[32m[20230207 15:38:00 @agent_ppo2.py:152][0m 1978368 total steps have happened
[32m[20230207 15:38:00 @agent_ppo2.py:128][0m #------------------------ Iteration 966 --------------------------#
[32m[20230207 15:38:01 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 15:38:01 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:01 @agent_ppo2.py:192][0m |          -0.0015 |          11.6895 |           0.1756 |
[32m[20230207 15:38:01 @agent_ppo2.py:192][0m |          -0.0053 |           4.1329 |           0.1760 |
[32m[20230207 15:38:01 @agent_ppo2.py:192][0m |          -0.0091 |           3.2659 |           0.1760 |
[32m[20230207 15:38:01 @agent_ppo2.py:192][0m |          -0.0104 |           2.9296 |           0.1758 |
[32m[20230207 15:38:01 @agent_ppo2.py:192][0m |          -0.0116 |           2.7000 |           0.1757 |
[32m[20230207 15:38:01 @agent_ppo2.py:192][0m |          -0.0122 |           2.5874 |           0.1757 |
[32m[20230207 15:38:01 @agent_ppo2.py:192][0m |          -0.0117 |           2.4599 |           0.1756 |
[32m[20230207 15:38:02 @agent_ppo2.py:192][0m |          -0.0137 |           2.3654 |           0.1755 |
[32m[20230207 15:38:02 @agent_ppo2.py:192][0m |          -0.0117 |           2.2813 |           0.1755 |
[32m[20230207 15:38:02 @agent_ppo2.py:192][0m |          -0.0079 |           2.2234 |           0.1754 |
[32m[20230207 15:38:02 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230207 15:38:02 @agent_ppo2.py:145][0m Average TRAINING episode reward: 3.96
[32m[20230207 15:38:02 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 69.68
[32m[20230207 15:38:02 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -34.65
[32m[20230207 15:38:02 @agent_ppo2.py:150][0m Total time:      37.35 min
[32m[20230207 15:38:02 @agent_ppo2.py:152][0m 1980416 total steps have happened
[32m[20230207 15:38:02 @agent_ppo2.py:128][0m #------------------------ Iteration 967 --------------------------#
[32m[20230207 15:38:03 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230207 15:38:03 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:03 @agent_ppo2.py:192][0m |           0.0003 |          20.1737 |           0.1731 |
[32m[20230207 15:38:03 @agent_ppo2.py:192][0m |          -0.0062 |          10.3076 |           0.1732 |
[32m[20230207 15:38:03 @agent_ppo2.py:192][0m |          -0.0096 |           9.4316 |           0.1731 |
[32m[20230207 15:38:03 @agent_ppo2.py:192][0m |          -0.0130 |           8.6190 |           0.1732 |
[32m[20230207 15:38:03 @agent_ppo2.py:192][0m |          -0.0126 |           7.9771 |           0.1731 |
[32m[20230207 15:38:03 @agent_ppo2.py:192][0m |          -0.0143 |           7.5034 |           0.1729 |
[32m[20230207 15:38:03 @agent_ppo2.py:192][0m |          -0.0112 |           7.1826 |           0.1729 |
[32m[20230207 15:38:03 @agent_ppo2.py:192][0m |          -0.0177 |           6.8610 |           0.1728 |
[32m[20230207 15:38:03 @agent_ppo2.py:192][0m |          -0.0174 |           6.6790 |           0.1730 |
[32m[20230207 15:38:03 @agent_ppo2.py:192][0m |          -0.0137 |           6.4023 |           0.1730 |
[32m[20230207 15:38:03 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:38:04 @agent_ppo2.py:145][0m Average TRAINING episode reward: 40.23
[32m[20230207 15:38:04 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 132.37
[32m[20230207 15:38:04 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 99.53
[32m[20230207 15:38:04 @agent_ppo2.py:150][0m Total time:      37.38 min
[32m[20230207 15:38:04 @agent_ppo2.py:152][0m 1982464 total steps have happened
[32m[20230207 15:38:04 @agent_ppo2.py:128][0m #------------------------ Iteration 968 --------------------------#
[32m[20230207 15:38:05 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 15:38:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:05 @agent_ppo2.py:192][0m |           0.0011 |           7.7717 |           0.1804 |
[32m[20230207 15:38:05 @agent_ppo2.py:192][0m |          -0.0012 |           2.8364 |           0.1805 |
[32m[20230207 15:38:05 @agent_ppo2.py:192][0m |          -0.0064 |           2.0817 |           0.1802 |
[32m[20230207 15:38:06 @agent_ppo2.py:192][0m |          -0.0072 |           1.8253 |           0.1800 |
[32m[20230207 15:38:06 @agent_ppo2.py:192][0m |          -0.0082 |           1.7145 |           0.1802 |
[32m[20230207 15:38:06 @agent_ppo2.py:192][0m |          -0.0097 |           1.6070 |           0.1801 |
[32m[20230207 15:38:06 @agent_ppo2.py:192][0m |          -0.0101 |           1.5287 |           0.1799 |
[32m[20230207 15:38:06 @agent_ppo2.py:192][0m |          -0.0113 |           1.4799 |           0.1799 |
[32m[20230207 15:38:06 @agent_ppo2.py:192][0m |          -0.0127 |           1.4610 |           0.1798 |
[32m[20230207 15:38:06 @agent_ppo2.py:192][0m |          -0.0132 |           1.4020 |           0.1797 |
[32m[20230207 15:38:06 @agent_ppo2.py:137][0m Policy update time: 0.87 s
[32m[20230207 15:38:07 @agent_ppo2.py:145][0m Average TRAINING episode reward: -16.19
[32m[20230207 15:38:07 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 49.68
[32m[20230207 15:38:07 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -53.86
[32m[20230207 15:38:07 @agent_ppo2.py:150][0m Total time:      37.42 min
[32m[20230207 15:38:07 @agent_ppo2.py:152][0m 1984512 total steps have happened
[32m[20230207 15:38:07 @agent_ppo2.py:128][0m #------------------------ Iteration 969 --------------------------#
[32m[20230207 15:38:07 @agent_ppo2.py:134][0m Sampling time: 0.54 s by 1 slaves
[32m[20230207 15:38:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:07 @agent_ppo2.py:192][0m |          -0.0004 |          30.7565 |           0.1704 |
[32m[20230207 15:38:07 @agent_ppo2.py:192][0m |          -0.0057 |           9.3032 |           0.1699 |
[32m[20230207 15:38:07 @agent_ppo2.py:192][0m |          -0.0047 |           6.1006 |           0.1699 |
[32m[20230207 15:38:07 @agent_ppo2.py:192][0m |          -0.0093 |           5.0310 |           0.1697 |
[32m[20230207 15:38:07 @agent_ppo2.py:192][0m |          -0.0085 |           4.4114 |           0.1699 |
[32m[20230207 15:38:07 @agent_ppo2.py:192][0m |          -0.0116 |           3.9471 |           0.1698 |
[32m[20230207 15:38:08 @agent_ppo2.py:192][0m |          -0.0107 |           3.6879 |           0.1699 |
[32m[20230207 15:38:08 @agent_ppo2.py:192][0m |          -0.0141 |           3.4781 |           0.1697 |
[32m[20230207 15:38:08 @agent_ppo2.py:192][0m |          -0.0124 |           3.2946 |           0.1698 |
[32m[20230207 15:38:08 @agent_ppo2.py:192][0m |          -0.0162 |           3.1040 |           0.1697 |
[32m[20230207 15:38:08 @agent_ppo2.py:137][0m Policy update time: 0.54 s
[32m[20230207 15:38:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -42.30
[32m[20230207 15:38:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 211.17
[32m[20230207 15:38:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -32.79
[32m[20230207 15:38:08 @agent_ppo2.py:150][0m Total time:      37.45 min
[32m[20230207 15:38:08 @agent_ppo2.py:152][0m 1986560 total steps have happened
[32m[20230207 15:38:08 @agent_ppo2.py:128][0m #------------------------ Iteration 970 --------------------------#
[32m[20230207 15:38:09 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:38:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:09 @agent_ppo2.py:192][0m |           0.0016 |          23.2617 |           0.1822 |
[32m[20230207 15:38:09 @agent_ppo2.py:192][0m |          -0.0051 |           8.2325 |           0.1820 |
[32m[20230207 15:38:09 @agent_ppo2.py:192][0m |          -0.0118 |           6.4194 |           0.1818 |
[32m[20230207 15:38:09 @agent_ppo2.py:192][0m |          -0.0136 |           5.9631 |           0.1820 |
[32m[20230207 15:38:09 @agent_ppo2.py:192][0m |          -0.0164 |           5.7031 |           0.1818 |
[32m[20230207 15:38:09 @agent_ppo2.py:192][0m |          -0.0174 |           5.4293 |           0.1818 |
[32m[20230207 15:38:09 @agent_ppo2.py:192][0m |          -0.0194 |           5.2843 |           0.1816 |
[32m[20230207 15:38:09 @agent_ppo2.py:192][0m |          -0.0189 |           5.1750 |           0.1816 |
[32m[20230207 15:38:09 @agent_ppo2.py:192][0m |          -0.0172 |           5.0722 |           0.1813 |
[32m[20230207 15:38:09 @agent_ppo2.py:192][0m |          -0.0205 |           5.0000 |           0.1815 |
[32m[20230207 15:38:09 @agent_ppo2.py:137][0m Policy update time: 0.53 s
[32m[20230207 15:38:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -36.94
[32m[20230207 15:38:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 107.23
[32m[20230207 15:38:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -58.91
[32m[20230207 15:38:10 @agent_ppo2.py:150][0m Total time:      37.48 min
[32m[20230207 15:38:10 @agent_ppo2.py:152][0m 1988608 total steps have happened
[32m[20230207 15:38:10 @agent_ppo2.py:128][0m #------------------------ Iteration 971 --------------------------#
[32m[20230207 15:38:10 @agent_ppo2.py:134][0m Sampling time: 0.63 s by 1 slaves
[32m[20230207 15:38:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:11 @agent_ppo2.py:192][0m |           0.0012 |          17.1055 |           0.1730 |
[32m[20230207 15:38:11 @agent_ppo2.py:192][0m |          -0.0019 |           8.9765 |           0.1729 |
[32m[20230207 15:38:11 @agent_ppo2.py:192][0m |          -0.0106 |           6.7411 |           0.1726 |
[32m[20230207 15:38:11 @agent_ppo2.py:192][0m |          -0.0134 |           6.1408 |           0.1725 |
[32m[20230207 15:38:11 @agent_ppo2.py:192][0m |          -0.0140 |           5.1793 |           0.1724 |
[32m[20230207 15:38:11 @agent_ppo2.py:192][0m |          -0.0066 |           4.9270 |           0.1723 |
[32m[20230207 15:38:11 @agent_ppo2.py:192][0m |           0.0015 |           4.6869 |           0.1723 |
[32m[20230207 15:38:11 @agent_ppo2.py:192][0m |          -0.0156 |           4.2332 |           0.1725 |
[32m[20230207 15:38:11 @agent_ppo2.py:192][0m |          -0.0156 |           3.8716 |           0.1724 |
[32m[20230207 15:38:11 @agent_ppo2.py:192][0m |          -0.0143 |           3.7205 |           0.1723 |
[32m[20230207 15:38:11 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230207 15:38:12 @agent_ppo2.py:145][0m Average TRAINING episode reward: 41.37
[32m[20230207 15:38:12 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 91.81
[32m[20230207 15:38:12 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 11.36
[32m[20230207 15:38:12 @agent_ppo2.py:150][0m Total time:      37.51 min
[32m[20230207 15:38:12 @agent_ppo2.py:152][0m 1990656 total steps have happened
[32m[20230207 15:38:12 @agent_ppo2.py:128][0m #------------------------ Iteration 972 --------------------------#
[32m[20230207 15:38:12 @agent_ppo2.py:134][0m Sampling time: 0.53 s by 1 slaves
[32m[20230207 15:38:12 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:12 @agent_ppo2.py:192][0m |           0.0003 |          17.2593 |           0.1711 |
[32m[20230207 15:38:12 @agent_ppo2.py:192][0m |          -0.0100 |           8.5979 |           0.1711 |
[32m[20230207 15:38:12 @agent_ppo2.py:192][0m |          -0.0104 |           7.1877 |           0.1710 |
[32m[20230207 15:38:12 @agent_ppo2.py:192][0m |          -0.0076 |           6.2027 |           0.1709 |
[32m[20230207 15:38:12 @agent_ppo2.py:192][0m |          -0.0135 |           5.7306 |           0.1708 |
[32m[20230207 15:38:13 @agent_ppo2.py:192][0m |          -0.0160 |           5.4150 |           0.1710 |
[32m[20230207 15:38:13 @agent_ppo2.py:192][0m |          -0.0180 |           5.1840 |           0.1708 |
[32m[20230207 15:38:13 @agent_ppo2.py:192][0m |          -0.0156 |           4.9544 |           0.1708 |
[32m[20230207 15:38:13 @agent_ppo2.py:192][0m |          -0.0132 |           4.8473 |           0.1708 |
[32m[20230207 15:38:13 @agent_ppo2.py:192][0m |          -0.0157 |           4.4525 |           0.1707 |
[32m[20230207 15:38:13 @agent_ppo2.py:137][0m Policy update time: 0.57 s
[32m[20230207 15:38:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: 18.78
[32m[20230207 15:38:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 169.38
[32m[20230207 15:38:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 5.96
[32m[20230207 15:38:13 @agent_ppo2.py:150][0m Total time:      37.53 min
[32m[20230207 15:38:13 @agent_ppo2.py:152][0m 1992704 total steps have happened
[32m[20230207 15:38:13 @agent_ppo2.py:128][0m #------------------------ Iteration 973 --------------------------#
[32m[20230207 15:38:14 @agent_ppo2.py:134][0m Sampling time: 0.69 s by 1 slaves
[32m[20230207 15:38:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:14 @agent_ppo2.py:192][0m |           0.0001 |          19.4397 |           0.1763 |
[32m[20230207 15:38:14 @agent_ppo2.py:192][0m |          -0.0094 |           6.7338 |           0.1756 |
[32m[20230207 15:38:14 @agent_ppo2.py:192][0m |          -0.0103 |           4.9937 |           0.1755 |
[32m[20230207 15:38:14 @agent_ppo2.py:192][0m |          -0.0115 |           4.3151 |           0.1754 |
[32m[20230207 15:38:14 @agent_ppo2.py:192][0m |          -0.0121 |           3.7810 |           0.1753 |
[32m[20230207 15:38:14 @agent_ppo2.py:192][0m |          -0.0148 |           3.6277 |           0.1753 |
[32m[20230207 15:38:15 @agent_ppo2.py:192][0m |          -0.0164 |           3.2262 |           0.1753 |
[32m[20230207 15:38:15 @agent_ppo2.py:192][0m |          -0.0158 |           3.0415 |           0.1754 |
[32m[20230207 15:38:15 @agent_ppo2.py:192][0m |          -0.0183 |           2.8815 |           0.1752 |
[32m[20230207 15:38:15 @agent_ppo2.py:192][0m |          -0.0165 |           2.7522 |           0.1753 |
[32m[20230207 15:38:15 @agent_ppo2.py:137][0m Policy update time: 0.74 s
[32m[20230207 15:38:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -48.03
[32m[20230207 15:38:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 34.76
[32m[20230207 15:38:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 48.22
[32m[20230207 15:38:16 @agent_ppo2.py:150][0m Total time:      37.57 min
[32m[20230207 15:38:16 @agent_ppo2.py:152][0m 1994752 total steps have happened
[32m[20230207 15:38:16 @agent_ppo2.py:128][0m #------------------------ Iteration 974 --------------------------#
[32m[20230207 15:38:16 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 15:38:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:16 @agent_ppo2.py:192][0m |           0.0014 |          29.5578 |           0.1738 |
[32m[20230207 15:38:16 @agent_ppo2.py:192][0m |          -0.0702 |          15.4450 |           0.1736 |
[32m[20230207 15:38:16 @agent_ppo2.py:192][0m |           0.0028 |          14.4759 |           0.1724 |
[32m[20230207 15:38:16 @agent_ppo2.py:192][0m |          -0.0056 |          10.2063 |           0.1731 |
[32m[20230207 15:38:16 @agent_ppo2.py:192][0m |          -0.0166 |           9.5467 |           0.1734 |
[32m[20230207 15:38:17 @agent_ppo2.py:192][0m |          -0.0145 |           8.2643 |           0.1733 |
[32m[20230207 15:38:17 @agent_ppo2.py:192][0m |          -0.0239 |           7.5204 |           0.1733 |
[32m[20230207 15:38:17 @agent_ppo2.py:192][0m |          -0.0152 |           6.9731 |           0.1733 |
[32m[20230207 15:38:17 @agent_ppo2.py:192][0m |          -0.0155 |           6.4090 |           0.1732 |
[32m[20230207 15:38:17 @agent_ppo2.py:192][0m |          -0.0149 |           5.9216 |           0.1732 |
[32m[20230207 15:38:17 @agent_ppo2.py:137][0m Policy update time: 0.62 s
[32m[20230207 15:38:17 @agent_ppo2.py:145][0m Average TRAINING episode reward: 39.35
[32m[20230207 15:38:17 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 134.24
[32m[20230207 15:38:17 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -59.63
[32m[20230207 15:38:17 @agent_ppo2.py:150][0m Total time:      37.60 min
[32m[20230207 15:38:17 @agent_ppo2.py:152][0m 1996800 total steps have happened
[32m[20230207 15:38:17 @agent_ppo2.py:128][0m #------------------------ Iteration 975 --------------------------#
[32m[20230207 15:38:18 @agent_ppo2.py:134][0m Sampling time: 0.50 s by 1 slaves
[32m[20230207 15:38:18 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:18 @agent_ppo2.py:192][0m |           0.0032 |          14.5618 |           0.1749 |
[32m[20230207 15:38:18 @agent_ppo2.py:192][0m |          -0.0027 |           4.5922 |           0.1746 |
[32m[20230207 15:38:18 @agent_ppo2.py:192][0m |          -0.0067 |           3.9541 |           0.1743 |
[32m[20230207 15:38:18 @agent_ppo2.py:192][0m |          -0.0050 |           3.4321 |           0.1743 |
[32m[20230207 15:38:18 @agent_ppo2.py:192][0m |          -0.0125 |           3.1729 |           0.1741 |
[32m[20230207 15:38:18 @agent_ppo2.py:192][0m |          -0.0122 |           3.0087 |           0.1740 |
[32m[20230207 15:38:18 @agent_ppo2.py:192][0m |          -0.0031 |           2.8661 |           0.1741 |
[32m[20230207 15:38:18 @agent_ppo2.py:192][0m |          -0.0136 |           2.7861 |           0.1741 |
[32m[20230207 15:38:18 @agent_ppo2.py:192][0m |          -0.0086 |           2.8764 |           0.1738 |
[32m[20230207 15:38:19 @agent_ppo2.py:192][0m |          -0.0130 |           2.6546 |           0.1738 |
[32m[20230207 15:38:19 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 15:38:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: 6.36
[32m[20230207 15:38:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 96.58
[32m[20230207 15:38:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -33.89
[32m[20230207 15:38:19 @agent_ppo2.py:150][0m Total time:      37.64 min
[32m[20230207 15:38:19 @agent_ppo2.py:152][0m 1998848 total steps have happened
[32m[20230207 15:38:19 @agent_ppo2.py:128][0m #------------------------ Iteration 976 --------------------------#
[32m[20230207 15:38:20 @agent_ppo2.py:134][0m Sampling time: 0.64 s by 1 slaves
[32m[20230207 15:38:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:20 @agent_ppo2.py:192][0m |           0.0008 |          33.2389 |           0.1714 |
[32m[20230207 15:38:20 @agent_ppo2.py:192][0m |          -0.0127 |          14.3401 |           0.1713 |
[32m[20230207 15:38:20 @agent_ppo2.py:192][0m |          -0.0085 |          10.9145 |           0.1712 |
[32m[20230207 15:38:20 @agent_ppo2.py:192][0m |          -0.0054 |           9.2928 |           0.1708 |
[32m[20230207 15:38:20 @agent_ppo2.py:192][0m |          -0.0088 |           8.1880 |           0.1708 |
[32m[20230207 15:38:20 @agent_ppo2.py:192][0m |          -0.0149 |           7.2466 |           0.1707 |
[32m[20230207 15:38:20 @agent_ppo2.py:192][0m |          -0.0178 |           6.7178 |           0.1705 |
[32m[20230207 15:38:21 @agent_ppo2.py:192][0m |          -0.0108 |           6.3185 |           0.1705 |
[32m[20230207 15:38:21 @agent_ppo2.py:192][0m |          -0.0231 |           6.0198 |           0.1704 |
[32m[20230207 15:38:21 @agent_ppo2.py:192][0m |          -0.0220 |           5.4061 |           0.1701 |
[32m[20230207 15:38:21 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230207 15:38:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -91.94
[32m[20230207 15:38:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -43.42
[32m[20230207 15:38:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -55.39
[32m[20230207 15:38:21 @agent_ppo2.py:150][0m Total time:      37.67 min
[32m[20230207 15:38:21 @agent_ppo2.py:152][0m 2000896 total steps have happened
[32m[20230207 15:38:21 @agent_ppo2.py:128][0m #------------------------ Iteration 977 --------------------------#
[32m[20230207 15:38:22 @agent_ppo2.py:134][0m Sampling time: 0.89 s by 1 slaves
[32m[20230207 15:38:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:22 @agent_ppo2.py:192][0m |           0.0012 |          20.3081 |           0.1750 |
[32m[20230207 15:38:22 @agent_ppo2.py:192][0m |          -0.0031 |           8.8987 |           0.1748 |
[32m[20230207 15:38:23 @agent_ppo2.py:192][0m |          -0.0075 |           6.1802 |           0.1748 |
[32m[20230207 15:38:23 @agent_ppo2.py:192][0m |          -0.0099 |           4.6829 |           0.1747 |
[32m[20230207 15:38:23 @agent_ppo2.py:192][0m |          -0.0123 |           3.7705 |           0.1745 |
[32m[20230207 15:38:23 @agent_ppo2.py:192][0m |          -0.0132 |           3.2349 |           0.1745 |
[32m[20230207 15:38:23 @agent_ppo2.py:192][0m |          -0.0149 |           2.9622 |           0.1745 |
[32m[20230207 15:38:23 @agent_ppo2.py:192][0m |          -0.0140 |           2.7454 |           0.1742 |
[32m[20230207 15:38:23 @agent_ppo2.py:192][0m |          -0.0125 |           2.6358 |           0.1743 |
[32m[20230207 15:38:23 @agent_ppo2.py:192][0m |          -0.0152 |           2.4392 |           0.1743 |
[32m[20230207 15:38:23 @agent_ppo2.py:137][0m Policy update time: 0.93 s
[32m[20230207 15:38:24 @agent_ppo2.py:145][0m Average TRAINING episode reward: -18.63
[32m[20230207 15:38:24 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 143.38
[32m[20230207 15:38:24 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -1.09
[32m[20230207 15:38:24 @agent_ppo2.py:150][0m Total time:      37.71 min
[32m[20230207 15:38:24 @agent_ppo2.py:152][0m 2002944 total steps have happened
[32m[20230207 15:38:24 @agent_ppo2.py:128][0m #------------------------ Iteration 978 --------------------------#
[32m[20230207 15:38:25 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 15:38:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:25 @agent_ppo2.py:192][0m |           0.0013 |          17.4780 |           0.1718 |
[32m[20230207 15:38:25 @agent_ppo2.py:192][0m |          -0.0040 |           5.9452 |           0.1719 |
[32m[20230207 15:38:25 @agent_ppo2.py:192][0m |          -0.0030 |           4.5027 |           0.1717 |
[32m[20230207 15:38:25 @agent_ppo2.py:192][0m |          -0.0058 |           4.0012 |           0.1719 |
[32m[20230207 15:38:25 @agent_ppo2.py:192][0m |          -0.0090 |           3.4969 |           0.1718 |
[32m[20230207 15:38:25 @agent_ppo2.py:192][0m |          -0.0088 |           3.2527 |           0.1717 |
[32m[20230207 15:38:25 @agent_ppo2.py:192][0m |          -0.0120 |           3.0039 |           0.1719 |
[32m[20230207 15:38:25 @agent_ppo2.py:192][0m |          -0.0121 |           2.8877 |           0.1718 |
[32m[20230207 15:38:25 @agent_ppo2.py:192][0m |          -0.0120 |           2.7789 |           0.1717 |
[32m[20230207 15:38:25 @agent_ppo2.py:192][0m |          -0.0097 |           2.5895 |           0.1718 |
[32m[20230207 15:38:25 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:38:26 @agent_ppo2.py:145][0m Average TRAINING episode reward: -15.69
[32m[20230207 15:38:26 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 238.20
[32m[20230207 15:38:26 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -49.64
[32m[20230207 15:38:26 @agent_ppo2.py:150][0m Total time:      37.75 min
[32m[20230207 15:38:26 @agent_ppo2.py:152][0m 2004992 total steps have happened
[32m[20230207 15:38:26 @agent_ppo2.py:128][0m #------------------------ Iteration 979 --------------------------#
[32m[20230207 15:38:27 @agent_ppo2.py:134][0m Sampling time: 0.77 s by 1 slaves
[32m[20230207 15:38:27 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:27 @agent_ppo2.py:192][0m |           0.0038 |          17.8307 |           0.1723 |
[32m[20230207 15:38:27 @agent_ppo2.py:192][0m |           0.0231 |          10.5009 |           0.1722 |
[32m[20230207 15:38:27 @agent_ppo2.py:192][0m |           0.0046 |           9.1697 |           0.1718 |
[32m[20230207 15:38:27 @agent_ppo2.py:192][0m |           0.0018 |           7.3850 |           0.1719 |
[32m[20230207 15:38:27 @agent_ppo2.py:192][0m |          -0.0140 |           6.6140 |           0.1720 |
[32m[20230207 15:38:27 @agent_ppo2.py:192][0m |          -0.0089 |           6.2263 |           0.1719 |
[32m[20230207 15:38:27 @agent_ppo2.py:192][0m |          -0.0100 |           5.5091 |           0.1721 |
[32m[20230207 15:38:27 @agent_ppo2.py:192][0m |          -0.0069 |           5.2065 |           0.1719 |
[32m[20230207 15:38:27 @agent_ppo2.py:192][0m |           0.0012 |           4.8479 |           0.1721 |
[32m[20230207 15:38:28 @agent_ppo2.py:192][0m |          -0.0097 |           4.5729 |           0.1721 |
[32m[20230207 15:38:28 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230207 15:38:28 @agent_ppo2.py:145][0m Average TRAINING episode reward: 104.48
[32m[20230207 15:38:28 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 111.68
[32m[20230207 15:38:28 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 36.68
[32m[20230207 15:38:28 @agent_ppo2.py:150][0m Total time:      37.79 min
[32m[20230207 15:38:28 @agent_ppo2.py:152][0m 2007040 total steps have happened
[32m[20230207 15:38:28 @agent_ppo2.py:128][0m #------------------------ Iteration 980 --------------------------#
[32m[20230207 15:38:29 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:38:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:29 @agent_ppo2.py:192][0m |           0.0010 |           3.0165 |           0.1777 |
[32m[20230207 15:38:29 @agent_ppo2.py:192][0m |          -0.0041 |           1.9996 |           0.1772 |
[32m[20230207 15:38:29 @agent_ppo2.py:192][0m |          -0.0067 |           1.7823 |           0.1772 |
[32m[20230207 15:38:30 @agent_ppo2.py:192][0m |          -0.0094 |           1.7166 |           0.1772 |
[32m[20230207 15:38:30 @agent_ppo2.py:192][0m |          -0.0112 |           1.6630 |           0.1773 |
[32m[20230207 15:38:30 @agent_ppo2.py:192][0m |          -0.0130 |           1.6263 |           0.1770 |
[32m[20230207 15:38:30 @agent_ppo2.py:192][0m |          -0.0141 |           1.6014 |           0.1772 |
[32m[20230207 15:38:30 @agent_ppo2.py:192][0m |          -0.0144 |           1.5786 |           0.1771 |
[32m[20230207 15:38:30 @agent_ppo2.py:192][0m |          -0.0153 |           1.5538 |           0.1773 |
[32m[20230207 15:38:30 @agent_ppo2.py:192][0m |          -0.0165 |           1.5380 |           0.1770 |
[32m[20230207 15:38:30 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 15:38:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: 144.02
[32m[20230207 15:38:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 204.92
[32m[20230207 15:38:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 31.77
[32m[20230207 15:38:31 @agent_ppo2.py:150][0m Total time:      37.83 min
[32m[20230207 15:38:31 @agent_ppo2.py:152][0m 2009088 total steps have happened
[32m[20230207 15:38:31 @agent_ppo2.py:128][0m #------------------------ Iteration 981 --------------------------#
[32m[20230207 15:38:32 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230207 15:38:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:32 @agent_ppo2.py:192][0m |           0.0049 |          16.8201 |           0.1760 |
[32m[20230207 15:38:32 @agent_ppo2.py:192][0m |           0.0055 |           9.8480 |           0.1755 |
[32m[20230207 15:38:32 @agent_ppo2.py:192][0m |          -0.0079 |           9.0660 |           0.1756 |
[32m[20230207 15:38:32 @agent_ppo2.py:192][0m |           0.0071 |           8.0996 |           0.1753 |
[32m[20230207 15:38:32 @agent_ppo2.py:192][0m |          -0.0079 |           7.7000 |           0.1754 |
[32m[20230207 15:38:32 @agent_ppo2.py:192][0m |          -0.0047 |           7.1391 |           0.1754 |
[32m[20230207 15:38:32 @agent_ppo2.py:192][0m |          -0.0011 |           6.9069 |           0.1754 |
[32m[20230207 15:38:32 @agent_ppo2.py:192][0m |          -0.0408 |           7.0702 |           0.1754 |
[32m[20230207 15:38:32 @agent_ppo2.py:192][0m |          -0.0191 |           7.3094 |           0.1755 |
[32m[20230207 15:38:32 @agent_ppo2.py:192][0m |           0.0206 |           6.4241 |           0.1755 |
[32m[20230207 15:38:32 @agent_ppo2.py:137][0m Policy update time: 0.83 s
[32m[20230207 15:38:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: 24.36
[32m[20230207 15:38:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 70.77
[32m[20230207 15:38:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -20.15
[32m[20230207 15:38:33 @agent_ppo2.py:150][0m Total time:      37.86 min
[32m[20230207 15:38:33 @agent_ppo2.py:152][0m 2011136 total steps have happened
[32m[20230207 15:38:33 @agent_ppo2.py:128][0m #------------------------ Iteration 982 --------------------------#
[32m[20230207 15:38:34 @agent_ppo2.py:134][0m Sampling time: 0.81 s by 1 slaves
[32m[20230207 15:38:34 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:34 @agent_ppo2.py:192][0m |           0.0045 |          21.3770 |           0.1762 |
[32m[20230207 15:38:34 @agent_ppo2.py:192][0m |          -0.0069 |          13.9020 |           0.1766 |
[32m[20230207 15:38:34 @agent_ppo2.py:192][0m |          -0.0051 |          11.8929 |           0.1766 |
[32m[20230207 15:38:34 @agent_ppo2.py:192][0m |          -0.0091 |          10.6159 |           0.1764 |
[32m[20230207 15:38:34 @agent_ppo2.py:192][0m |          -0.0050 |           9.3751 |           0.1762 |
[32m[20230207 15:38:34 @agent_ppo2.py:192][0m |          -0.0131 |           8.6931 |           0.1764 |
[32m[20230207 15:38:34 @agent_ppo2.py:192][0m |          -0.0154 |           8.4271 |           0.1763 |
[32m[20230207 15:38:35 @agent_ppo2.py:192][0m |          -0.0094 |           7.8543 |           0.1762 |
[32m[20230207 15:38:35 @agent_ppo2.py:192][0m |          -0.0122 |           7.5231 |           0.1763 |
[32m[20230207 15:38:35 @agent_ppo2.py:192][0m |          -0.0140 |           7.2651 |           0.1762 |
[32m[20230207 15:38:35 @agent_ppo2.py:137][0m Policy update time: 0.90 s
[32m[20230207 15:38:35 @agent_ppo2.py:145][0m Average TRAINING episode reward: 36.90
[32m[20230207 15:38:35 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 170.91
[32m[20230207 15:38:35 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 72.34
[32m[20230207 15:38:35 @agent_ppo2.py:150][0m Total time:      37.90 min
[32m[20230207 15:38:35 @agent_ppo2.py:152][0m 2013184 total steps have happened
[32m[20230207 15:38:35 @agent_ppo2.py:128][0m #------------------------ Iteration 983 --------------------------#
[32m[20230207 15:38:36 @agent_ppo2.py:134][0m Sampling time: 0.92 s by 1 slaves
[32m[20230207 15:38:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:37 @agent_ppo2.py:192][0m |          -0.0037 |          10.3884 |           0.1797 |
[32m[20230207 15:38:37 @agent_ppo2.py:192][0m |          -0.0052 |           5.0800 |           0.1796 |
[32m[20230207 15:38:37 @agent_ppo2.py:192][0m |          -0.0078 |           4.1221 |           0.1794 |
[32m[20230207 15:38:37 @agent_ppo2.py:192][0m |          -0.0117 |           3.6837 |           0.1791 |
[32m[20230207 15:38:37 @agent_ppo2.py:192][0m |          -0.0127 |           3.3428 |           0.1789 |
[32m[20230207 15:38:37 @agent_ppo2.py:192][0m |          -0.0139 |           3.0679 |           0.1788 |
[32m[20230207 15:38:37 @agent_ppo2.py:192][0m |          -0.0156 |           2.7452 |           0.1787 |
[32m[20230207 15:38:37 @agent_ppo2.py:192][0m |          -0.0178 |           2.6241 |           0.1784 |
[32m[20230207 15:38:37 @agent_ppo2.py:192][0m |          -0.0203 |           2.6355 |           0.1785 |
[32m[20230207 15:38:37 @agent_ppo2.py:192][0m |          -0.0184 |           2.5401 |           0.1783 |
[32m[20230207 15:38:37 @agent_ppo2.py:137][0m Policy update time: 0.91 s
[32m[20230207 15:38:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: -23.21
[32m[20230207 15:38:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 123.47
[32m[20230207 15:38:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: 135.09
[32m[20230207 15:38:38 @agent_ppo2.py:150][0m Total time:      37.95 min
[32m[20230207 15:38:38 @agent_ppo2.py:152][0m 2015232 total steps have happened
[32m[20230207 15:38:38 @agent_ppo2.py:128][0m #------------------------ Iteration 984 --------------------------#
[32m[20230207 15:38:39 @agent_ppo2.py:134][0m Sampling time: 0.76 s by 1 slaves
[32m[20230207 15:38:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:39 @agent_ppo2.py:192][0m |           0.0012 |          21.1900 |           0.1773 |
[32m[20230207 15:38:39 @agent_ppo2.py:192][0m |          -0.0051 |           8.9856 |           0.1775 |
[32m[20230207 15:38:39 @agent_ppo2.py:192][0m |          -0.0084 |           7.5490 |           0.1773 |
[32m[20230207 15:38:39 @agent_ppo2.py:192][0m |          -0.0113 |           6.7807 |           0.1772 |
[32m[20230207 15:38:39 @agent_ppo2.py:192][0m |          -0.0097 |           6.1994 |           0.1775 |
[32m[20230207 15:38:39 @agent_ppo2.py:192][0m |          -0.0124 |           5.7529 |           0.1775 |
[32m[20230207 15:38:39 @agent_ppo2.py:192][0m |          -0.0140 |           5.3463 |           0.1770 |
[32m[20230207 15:38:40 @agent_ppo2.py:192][0m |          -0.0165 |           5.0158 |           0.1773 |
[32m[20230207 15:38:40 @agent_ppo2.py:192][0m |          -0.0175 |           4.7647 |           0.1773 |
[32m[20230207 15:38:40 @agent_ppo2.py:192][0m |          -0.0158 |           4.5202 |           0.1775 |
[32m[20230207 15:38:40 @agent_ppo2.py:137][0m Policy update time: 0.82 s
[32m[20230207 15:38:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: 52.99
[32m[20230207 15:38:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: 120.05
[32m[20230207 15:38:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -18.83
[32m[20230207 15:38:40 @agent_ppo2.py:150][0m Total time:      37.98 min
[32m[20230207 15:38:40 @agent_ppo2.py:152][0m 2017280 total steps have happened
[32m[20230207 15:38:40 @agent_ppo2.py:128][0m #------------------------ Iteration 985 --------------------------#
[32m[20230207 15:38:41 @agent_ppo2.py:134][0m Sampling time: 0.80 s by 1 slaves
[32m[20230207 15:38:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 15:38:41 @agent_ppo2.py:192][0m |           0.0029 |           4.4889 |           0.1792 |
[32m[20230207 15:38:41 @agent_ppo2.py:192][0m |          -0.0044 |           3.1156 |           0.1789 |
[32m[20230207 15:38:41 @agent_ppo2.py:192][0m |          -0.0085 |           2.8912 |           0.1788 |
[32m[20230207 15:38:41 @agent_ppo2.py:192][0m |          -0.0087 |           2.7454 |           0.1787 |
[32m[20230207 15:38:42 @agent_ppo2.py:192][0m |          -0.0102 |           2.6769 |           0.1787 |
[32m[20230207 15:38:42 @agent_ppo2.py:192][0m |          -0.0122 |           2.6016 |           0.1788 |
[32m[20230207 15:38:42 @agent_ppo2.py:192][0m |          -0.0134 |           2.3903 |           0.1785 |
[32m[20230207 15:38:42 @agent_ppo2.py:192][0m |          -0.0136 |           2.3050 |           0.1785 |
[32m[20230207 15:38:42 @agent_ppo2.py:192][0m |          -0.0150 |           2.2395 |           0.1785 |
[32m[20230207 15:38:42 @agent_ppo2.py:192][0m |          -0.0158 |           2.1573 |           0.1784 |
[32m[20230207 15:38:42 @agent_ppo2.py:137][0m Policy update time: 0.82 s
