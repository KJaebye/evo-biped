[32m[20230209 04:33:56 @logger.py:106][0m Log file set to ./tmp/evo_bipedalwalker/easy/20230209_043356/log/evo_bipedalwalker_easy-20230209_043356.log
[32m[20230209 04:33:56 @agent_ppo2.py:128][0m #------------------------ Iteration 0 --------------------------#
[32m[20230209 04:33:57 @agent_ppo2.py:134][0m Sampling time: 0.61 s by 1 slaves
[32m[20230209 04:33:57 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:33:57 @agent_ppo2.py:193][0m |          -0.0081 |         304.4637 |           0.0424 |
[32m[20230209 04:33:57 @agent_ppo2.py:193][0m |          -0.0498 |         259.5993 |           0.0423 |
[32m[20230209 04:33:57 @agent_ppo2.py:193][0m |          -0.0680 |         250.8342 |           0.0433 |
[32m[20230209 04:33:57 @agent_ppo2.py:193][0m |          -0.0698 |         239.6915 |           0.0437 |
[32m[20230209 04:33:57 @agent_ppo2.py:193][0m |          -0.0670 |         228.8776 |           0.0437 |
[32m[20230209 04:33:57 @agent_ppo2.py:193][0m |          -0.0532 |         253.6517 |           0.0438 |
[32m[20230209 04:33:57 @agent_ppo2.py:193][0m |          -0.0728 |         212.5261 |           0.0440 |
[32m[20230209 04:33:58 @agent_ppo2.py:193][0m |          -0.0704 |         207.4861 |           0.0441 |
[32m[20230209 04:33:58 @agent_ppo2.py:193][0m |          -0.0584 |         209.2638 |           0.0439 |
[32m[20230209 04:33:58 @agent_ppo2.py:193][0m |          -0.0723 |         197.8351 |           0.0440 |
[32m[20230209 04:33:58 @agent_ppo2.py:137][0m Policy update time: 0.66 s
[32m[20230209 04:33:58 @agent_ppo2.py:145][0m Average TRAINING episode reward: -131.73
[32m[20230209 04:33:58 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -107.21
[32m[20230209 04:33:58 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -117.04
[32m[20230209 04:33:58 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -117.04
[32m[20230209 04:33:58 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -117.04
[32m[20230209 04:33:58 @agent_ppo2.py:150][0m Total time:       0.03 min
[32m[20230209 04:33:58 @agent_ppo2.py:152][0m 2048 total steps have happened
[32m[20230209 04:33:58 @agent_ppo2.py:128][0m #------------------------ Iteration 1 --------------------------#
[32m[20230209 04:33:59 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230209 04:33:59 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:33:59 @agent_ppo2.py:193][0m |           0.0179 |          21.8547 |           0.0614 |
[32m[20230209 04:33:59 @agent_ppo2.py:193][0m |          -0.0089 |          21.3015 |           0.0598 |
[32m[20230209 04:33:59 @agent_ppo2.py:193][0m |          -0.0148 |          21.2397 |           0.0602 |
[32m[20230209 04:34:00 @agent_ppo2.py:193][0m |          -0.0158 |          21.0904 |           0.0603 |
[32m[20230209 04:34:00 @agent_ppo2.py:193][0m |          -0.0161 |          21.0255 |           0.0605 |
[32m[20230209 04:34:00 @agent_ppo2.py:193][0m |          -0.0166 |          20.9842 |           0.0607 |
[32m[20230209 04:34:00 @agent_ppo2.py:193][0m |          -0.0171 |          20.8949 |           0.0607 |
[32m[20230209 04:34:00 @agent_ppo2.py:193][0m |          -0.0160 |          20.8223 |           0.0609 |
[32m[20230209 04:34:00 @agent_ppo2.py:193][0m |          -0.0154 |          20.7351 |           0.0599 |
[32m[20230209 04:34:00 @agent_ppo2.py:193][0m |          -0.0164 |          20.6292 |           0.0602 |
[32m[20230209 04:34:00 @agent_ppo2.py:137][0m Policy update time: 0.94 s
[32m[20230209 04:34:01 @agent_ppo2.py:145][0m Average TRAINING episode reward: -158.22
[32m[20230209 04:34:01 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -105.68
[32m[20230209 04:34:01 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -115.67
[32m[20230209 04:34:01 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -115.67
[32m[20230209 04:34:01 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -115.67
[32m[20230209 04:34:01 @agent_ppo2.py:150][0m Total time:       0.07 min
[32m[20230209 04:34:01 @agent_ppo2.py:152][0m 4096 total steps have happened
[32m[20230209 04:34:01 @agent_ppo2.py:128][0m #------------------------ Iteration 2 --------------------------#
[32m[20230209 04:34:02 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230209 04:34:02 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:02 @agent_ppo2.py:193][0m |           0.0072 |          77.1091 |           0.0596 |
[32m[20230209 04:34:02 @agent_ppo2.py:193][0m |          -0.0263 |          75.3811 |           0.0568 |
[32m[20230209 04:34:02 @agent_ppo2.py:193][0m |          -0.0258 |          75.0426 |           0.0575 |
[32m[20230209 04:34:02 @agent_ppo2.py:193][0m |          -0.0336 |          71.9690 |           0.0581 |
[32m[20230209 04:34:02 @agent_ppo2.py:193][0m |          -0.0339 |          70.2888 |           0.0585 |
[32m[20230209 04:34:02 @agent_ppo2.py:193][0m |          -0.0342 |          68.6309 |           0.0586 |
[32m[20230209 04:34:02 @agent_ppo2.py:193][0m |          -0.0356 |          67.1086 |           0.0586 |
[32m[20230209 04:34:02 @agent_ppo2.py:193][0m |          -0.0361 |          65.6053 |           0.0588 |
[32m[20230209 04:34:02 @agent_ppo2.py:193][0m |          -0.0357 |          64.3037 |           0.0589 |
[32m[20230209 04:34:02 @agent_ppo2.py:193][0m |          -0.0374 |          62.8587 |           0.0591 |
[32m[20230209 04:34:02 @agent_ppo2.py:137][0m Policy update time: 0.86 s
[32m[20230209 04:34:03 @agent_ppo2.py:145][0m Average TRAINING episode reward: -160.36
[32m[20230209 04:34:03 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.30
[32m[20230209 04:34:03 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -125.49
[32m[20230209 04:34:03 @agent_ppo2.py:150][0m Total time:       0.11 min
[32m[20230209 04:34:03 @agent_ppo2.py:152][0m 6144 total steps have happened
[32m[20230209 04:34:03 @agent_ppo2.py:128][0m #------------------------ Iteration 3 --------------------------#
[32m[20230209 04:34:04 @agent_ppo2.py:134][0m Sampling time: 0.98 s by 1 slaves
[32m[20230209 04:34:04 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:04 @agent_ppo2.py:193][0m |           0.0355 |          74.5127 |           0.0553 |
[32m[20230209 04:34:04 @agent_ppo2.py:193][0m |           0.0436 |          71.4368 |           0.0497 |
[32m[20230209 04:34:04 @agent_ppo2.py:193][0m |           0.0065 |          72.2647 |           0.0523 |
[32m[20230209 04:34:04 @agent_ppo2.py:193][0m |          -0.0089 |          67.0747 |           0.0541 |
[32m[20230209 04:34:04 @agent_ppo2.py:193][0m |          -0.0125 |          65.4625 |           0.0553 |
[32m[20230209 04:34:05 @agent_ppo2.py:193][0m |          -0.0175 |          63.9189 |           0.0560 |
[32m[20230209 04:34:05 @agent_ppo2.py:193][0m |          -0.0205 |          61.8348 |           0.0565 |
[32m[20230209 04:34:05 @agent_ppo2.py:193][0m |          -0.0220 |          60.3208 |           0.0566 |
[32m[20230209 04:34:05 @agent_ppo2.py:193][0m |          -0.0218 |          59.0160 |           0.0566 |
[32m[20230209 04:34:05 @agent_ppo2.py:193][0m |          -0.0254 |          57.3989 |           0.0566 |
[32m[20230209 04:34:05 @agent_ppo2.py:137][0m Policy update time: 1.00 s
[32m[20230209 04:34:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: -128.70
[32m[20230209 04:34:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.90
[32m[20230209 04:34:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -129.26
[32m[20230209 04:34:06 @agent_ppo2.py:150][0m Total time:       0.15 min
[32m[20230209 04:34:06 @agent_ppo2.py:152][0m 8192 total steps have happened
[32m[20230209 04:34:06 @agent_ppo2.py:128][0m #------------------------ Iteration 4 --------------------------#
[32m[20230209 04:34:07 @agent_ppo2.py:134][0m Sampling time: 0.98 s by 1 slaves
[32m[20230209 04:34:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:07 @agent_ppo2.py:193][0m |           0.1292 |         147.4547 |           0.0557 |
[32m[20230209 04:34:07 @agent_ppo2.py:193][0m |           0.1432 |         140.3395 |           0.0459 |
[32m[20230209 04:34:07 @agent_ppo2.py:193][0m |           0.0182 |         135.5983 |           0.0466 |
[32m[20230209 04:34:07 @agent_ppo2.py:193][0m |           0.0089 |         137.6230 |           0.0493 |
[32m[20230209 04:34:07 @agent_ppo2.py:193][0m |          -0.0051 |         134.7333 |           0.0519 |
[32m[20230209 04:34:07 @agent_ppo2.py:193][0m |          -0.0118 |         132.3131 |           0.0529 |
[32m[20230209 04:34:07 @agent_ppo2.py:193][0m |          -0.0193 |         132.9582 |           0.0537 |
[32m[20230209 04:34:07 @agent_ppo2.py:193][0m |          -0.0172 |         132.5182 |           0.0536 |
[32m[20230209 04:34:07 @agent_ppo2.py:193][0m |          -0.0133 |         129.8551 |           0.0540 |
[32m[20230209 04:34:08 @agent_ppo2.py:193][0m |          -0.0203 |         116.7571 |           0.0542 |
[32m[20230209 04:34:08 @agent_ppo2.py:137][0m Policy update time: 1.02 s
[32m[20230209 04:34:08 @agent_ppo2.py:145][0m Average TRAINING episode reward: -129.93
[32m[20230209 04:34:08 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.75
[32m[20230209 04:34:08 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -128.99
[32m[20230209 04:34:08 @agent_ppo2.py:150][0m Total time:       0.20 min
[32m[20230209 04:34:08 @agent_ppo2.py:152][0m 10240 total steps have happened
[32m[20230209 04:34:08 @agent_ppo2.py:128][0m #------------------------ Iteration 5 --------------------------#
[32m[20230209 04:34:09 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230209 04:34:09 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:09 @agent_ppo2.py:193][0m |           0.0092 |          91.5730 |           0.0612 |
[32m[20230209 04:34:09 @agent_ppo2.py:193][0m |           0.0126 |          83.5514 |           0.0584 |
[32m[20230209 04:34:09 @agent_ppo2.py:193][0m |          -0.0040 |          81.8535 |           0.0557 |
[32m[20230209 04:34:09 @agent_ppo2.py:193][0m |          -0.0173 |          81.1339 |           0.0537 |
[32m[20230209 04:34:09 @agent_ppo2.py:193][0m |          -0.0246 |          78.8589 |           0.0538 |
[32m[20230209 04:34:09 @agent_ppo2.py:193][0m |          -0.0328 |          77.1221 |           0.0526 |
[32m[20230209 04:34:09 @agent_ppo2.py:193][0m |          -0.0287 |          75.3585 |           0.0516 |
[32m[20230209 04:34:09 @agent_ppo2.py:193][0m |          -0.0388 |          73.3452 |           0.0525 |
[32m[20230209 04:34:09 @agent_ppo2.py:193][0m |          -0.0372 |          71.1900 |           0.0529 |
[32m[20230209 04:34:09 @agent_ppo2.py:193][0m |          -0.0386 |          69.0439 |           0.0528 |
[32m[20230209 04:34:09 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230209 04:34:10 @agent_ppo2.py:145][0m Average TRAINING episode reward: -132.57
[32m[20230209 04:34:10 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -113.08
[32m[20230209 04:34:10 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -139.87
[32m[20230209 04:34:10 @agent_ppo2.py:150][0m Total time:       0.23 min
[32m[20230209 04:34:10 @agent_ppo2.py:152][0m 12288 total steps have happened
[32m[20230209 04:34:10 @agent_ppo2.py:128][0m #------------------------ Iteration 6 --------------------------#
[32m[20230209 04:34:11 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230209 04:34:11 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:11 @agent_ppo2.py:193][0m |           0.0630 |          31.4029 |           0.0617 |
[32m[20230209 04:34:11 @agent_ppo2.py:193][0m |           0.0057 |          28.6682 |           0.0624 |
[32m[20230209 04:34:11 @agent_ppo2.py:193][0m |           0.0111 |          28.0402 |           0.0617 |
[32m[20230209 04:34:11 @agent_ppo2.py:193][0m |           0.0021 |          27.5727 |           0.0625 |
[32m[20230209 04:34:11 @agent_ppo2.py:193][0m |          -0.0024 |          27.1701 |           0.0619 |
[32m[20230209 04:34:11 @agent_ppo2.py:193][0m |          -0.0071 |          26.7624 |           0.0623 |
[32m[20230209 04:34:12 @agent_ppo2.py:193][0m |           0.0316 |          29.6175 |           0.0623 |
[32m[20230209 04:34:12 @agent_ppo2.py:193][0m |          -0.0148 |          26.0425 |           0.0632 |
[32m[20230209 04:34:12 @agent_ppo2.py:193][0m |          -0.0170 |          25.6405 |           0.0632 |
[32m[20230209 04:34:12 @agent_ppo2.py:193][0m |          -0.0115 |          25.3318 |           0.0627 |
[32m[20230209 04:34:12 @agent_ppo2.py:137][0m Policy update time: 1.15 s
[32m[20230209 04:34:13 @agent_ppo2.py:145][0m Average TRAINING episode reward: -182.16
[32m[20230209 04:34:13 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -119.21
[32m[20230209 04:34:13 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -147.98
[32m[20230209 04:34:13 @agent_ppo2.py:150][0m Total time:       0.27 min
[32m[20230209 04:34:13 @agent_ppo2.py:152][0m 14336 total steps have happened
[32m[20230209 04:34:13 @agent_ppo2.py:128][0m #------------------------ Iteration 7 --------------------------#
[32m[20230209 04:34:14 @agent_ppo2.py:134][0m Sampling time: 0.95 s by 1 slaves
[32m[20230209 04:34:14 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:14 @agent_ppo2.py:193][0m |           0.3513 |          35.5903 |           0.0621 |
[32m[20230209 04:34:14 @agent_ppo2.py:193][0m |           0.0066 |          34.4493 |           0.0597 |
[32m[20230209 04:34:14 @agent_ppo2.py:193][0m |           0.0284 |          43.8927 |           0.0604 |
[32m[20230209 04:34:14 @agent_ppo2.py:193][0m |           0.0435 |          31.4583 |           0.0585 |
[32m[20230209 04:34:14 @agent_ppo2.py:193][0m |          -0.0154 |          31.4363 |           0.0601 |
[32m[20230209 04:34:14 @agent_ppo2.py:193][0m |          -0.0069 |          29.6710 |           0.0599 |
[32m[20230209 04:34:15 @agent_ppo2.py:193][0m |           0.0011 |          35.5198 |           0.0606 |
[32m[20230209 04:34:15 @agent_ppo2.py:193][0m |          -0.0157 |          27.8627 |           0.0607 |
[32m[20230209 04:34:15 @agent_ppo2.py:193][0m |          -0.0026 |          27.1608 |           0.0607 |
[32m[20230209 04:34:15 @agent_ppo2.py:193][0m |          -0.0072 |          29.9737 |           0.0608 |
[32m[20230209 04:34:15 @agent_ppo2.py:137][0m Policy update time: 1.08 s
[32m[20230209 04:34:15 @agent_ppo2.py:145][0m Average TRAINING episode reward: -144.65
[32m[20230209 04:34:15 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -111.02
[32m[20230209 04:34:15 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -128.30
[32m[20230209 04:34:15 @agent_ppo2.py:150][0m Total time:       0.32 min
[32m[20230209 04:34:15 @agent_ppo2.py:152][0m 16384 total steps have happened
[32m[20230209 04:34:15 @agent_ppo2.py:128][0m #------------------------ Iteration 8 --------------------------#
[32m[20230209 04:34:16 @agent_ppo2.py:134][0m Sampling time: 0.90 s by 1 slaves
[32m[20230209 04:34:16 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:16 @agent_ppo2.py:193][0m |           0.1042 |         200.7692 |           0.0572 |
[32m[20230209 04:34:17 @agent_ppo2.py:193][0m |           0.0001 |         153.5185 |           0.0462 |
[32m[20230209 04:34:17 @agent_ppo2.py:193][0m |          -0.0129 |         133.6073 |           0.0463 |
[32m[20230209 04:34:17 @agent_ppo2.py:193][0m |          -0.0275 |         118.7744 |           0.0471 |
[32m[20230209 04:34:17 @agent_ppo2.py:193][0m |          -0.0279 |         108.9108 |           0.0479 |
[32m[20230209 04:34:17 @agent_ppo2.py:193][0m |          -0.0342 |         100.3327 |           0.0482 |
[32m[20230209 04:34:17 @agent_ppo2.py:193][0m |          -0.0355 |          92.9141 |           0.0485 |
[32m[20230209 04:34:17 @agent_ppo2.py:193][0m |          -0.0332 |          87.8659 |           0.0485 |
[32m[20230209 04:34:17 @agent_ppo2.py:193][0m |          -0.0337 |          82.6101 |           0.0486 |
[32m[20230209 04:34:17 @agent_ppo2.py:193][0m |          -0.0334 |          77.5383 |           0.0489 |
[32m[20230209 04:34:17 @agent_ppo2.py:137][0m Policy update time: 0.92 s
[32m[20230209 04:34:18 @agent_ppo2.py:145][0m Average TRAINING episode reward: -121.63
[32m[20230209 04:34:18 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.04
[32m[20230209 04:34:18 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -118.51
[32m[20230209 04:34:18 @agent_ppo2.py:150][0m Total time:       0.36 min
[32m[20230209 04:34:18 @agent_ppo2.py:152][0m 18432 total steps have happened
[32m[20230209 04:34:18 @agent_ppo2.py:128][0m #------------------------ Iteration 9 --------------------------#
[32m[20230209 04:34:19 @agent_ppo2.py:134][0m Sampling time: 0.74 s by 1 slaves
[32m[20230209 04:34:19 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:19 @agent_ppo2.py:193][0m |           0.0489 |          99.2842 |           0.0627 |
[32m[20230209 04:34:19 @agent_ppo2.py:193][0m |           0.0735 |          76.5550 |           0.0599 |
[32m[20230209 04:34:19 @agent_ppo2.py:193][0m |           0.0468 |          72.2512 |           0.0610 |
[32m[20230209 04:34:19 @agent_ppo2.py:193][0m |           0.0516 |          68.9003 |           0.0611 |
[32m[20230209 04:34:19 @agent_ppo2.py:193][0m |           0.0325 |          65.9742 |           0.0608 |
[32m[20230209 04:34:19 @agent_ppo2.py:193][0m |           0.0150 |          64.2138 |           0.0608 |
[32m[20230209 04:34:19 @agent_ppo2.py:193][0m |           0.0008 |          61.3759 |           0.0620 |
[32m[20230209 04:34:19 @agent_ppo2.py:193][0m |           0.0016 |          60.8983 |           0.0619 |
[32m[20230209 04:34:19 @agent_ppo2.py:193][0m |           0.0125 |          57.9583 |           0.0615 |
[32m[20230209 04:34:19 @agent_ppo2.py:193][0m |           0.0137 |          55.7567 |           0.0613 |
[32m[20230209 04:34:19 @agent_ppo2.py:137][0m Policy update time: 0.76 s
[32m[20230209 04:34:20 @agent_ppo2.py:145][0m Average TRAINING episode reward: -121.95
[32m[20230209 04:34:20 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -105.52
[32m[20230209 04:34:20 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -138.83
[32m[20230209 04:34:20 @agent_ppo2.py:150][0m Total time:       0.40 min
[32m[20230209 04:34:20 @agent_ppo2.py:152][0m 20480 total steps have happened
[32m[20230209 04:34:20 @agent_ppo2.py:128][0m #------------------------ Iteration 10 --------------------------#
[32m[20230209 04:34:21 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230209 04:34:21 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:21 @agent_ppo2.py:193][0m |           0.0310 |          52.4081 |           0.0620 |
[32m[20230209 04:34:21 @agent_ppo2.py:193][0m |          -0.0025 |          40.2506 |           0.0568 |
[32m[20230209 04:34:21 @agent_ppo2.py:193][0m |          -0.0189 |          37.3057 |           0.0555 |
[32m[20230209 04:34:21 @agent_ppo2.py:193][0m |          -0.0259 |          35.0370 |           0.0551 |
[32m[20230209 04:34:21 @agent_ppo2.py:193][0m |          -0.0336 |          33.5361 |           0.0547 |
[32m[20230209 04:34:21 @agent_ppo2.py:193][0m |          -0.0390 |          31.9770 |           0.0547 |
[32m[20230209 04:34:21 @agent_ppo2.py:193][0m |          -0.0391 |          30.7074 |           0.0544 |
[32m[20230209 04:34:21 @agent_ppo2.py:193][0m |          -0.0397 |          30.3677 |           0.0543 |
[32m[20230209 04:34:21 @agent_ppo2.py:193][0m |          -0.0399 |          29.1491 |           0.0543 |
[32m[20230209 04:34:22 @agent_ppo2.py:193][0m |          -0.0374 |          29.2683 |           0.0542 |
[32m[20230209 04:34:22 @agent_ppo2.py:137][0m Policy update time: 0.73 s
[32m[20230209 04:34:22 @agent_ppo2.py:145][0m Average TRAINING episode reward: -123.81
[32m[20230209 04:34:22 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -109.75
[32m[20230209 04:34:22 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -130.21
[32m[20230209 04:34:22 @agent_ppo2.py:150][0m Total time:       0.43 min
[32m[20230209 04:34:22 @agent_ppo2.py:152][0m 22528 total steps have happened
[32m[20230209 04:34:22 @agent_ppo2.py:128][0m #------------------------ Iteration 11 --------------------------#
[32m[20230209 04:34:23 @agent_ppo2.py:134][0m Sampling time: 0.66 s by 1 slaves
[32m[20230209 04:34:23 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:23 @agent_ppo2.py:193][0m |           0.0322 |          62.5875 |           0.0653 |
[32m[20230209 04:34:23 @agent_ppo2.py:193][0m |           0.0829 |          38.8298 |           0.0619 |
[32m[20230209 04:34:23 @agent_ppo2.py:193][0m |           0.0385 |          35.7352 |           0.0610 |
[32m[20230209 04:34:24 @agent_ppo2.py:193][0m |           0.0337 |          33.6437 |           0.0595 |
[32m[20230209 04:34:24 @agent_ppo2.py:193][0m |           0.0180 |          32.1027 |           0.0587 |
[32m[20230209 04:34:24 @agent_ppo2.py:193][0m |           0.0048 |          30.9133 |           0.0591 |
[32m[20230209 04:34:24 @agent_ppo2.py:193][0m |           0.0045 |          29.8951 |           0.0594 |
[32m[20230209 04:34:24 @agent_ppo2.py:193][0m |          -0.0031 |          28.9463 |           0.0589 |
[32m[20230209 04:34:24 @agent_ppo2.py:193][0m |           0.0069 |          28.1018 |           0.0586 |
[32m[20230209 04:34:24 @agent_ppo2.py:193][0m |          -0.0062 |          27.3049 |           0.0576 |
[32m[20230209 04:34:24 @agent_ppo2.py:137][0m Policy update time: 0.94 s
[32m[20230209 04:34:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -117.15
[32m[20230209 04:34:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.34
[32m[20230209 04:34:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -125.80
[32m[20230209 04:34:25 @agent_ppo2.py:150][0m Total time:       0.47 min
[32m[20230209 04:34:25 @agent_ppo2.py:152][0m 24576 total steps have happened
[32m[20230209 04:34:25 @agent_ppo2.py:128][0m #------------------------ Iteration 12 --------------------------#
[32m[20230209 04:34:25 @agent_ppo2.py:134][0m Sampling time: 0.73 s by 1 slaves
[32m[20230209 04:34:25 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:25 @agent_ppo2.py:193][0m |           0.0237 |          56.1139 |           0.0647 |
[32m[20230209 04:34:26 @agent_ppo2.py:193][0m |           0.0367 |          42.4877 |           0.0638 |
[32m[20230209 04:34:26 @agent_ppo2.py:193][0m |           0.0036 |          37.2141 |           0.0640 |
[32m[20230209 04:34:26 @agent_ppo2.py:193][0m |           0.0181 |          33.8199 |           0.0632 |
[32m[20230209 04:34:26 @agent_ppo2.py:193][0m |           0.0068 |          31.8855 |           0.0631 |
[32m[20230209 04:34:26 @agent_ppo2.py:193][0m |           0.0056 |          30.5477 |           0.0626 |
[32m[20230209 04:34:26 @agent_ppo2.py:193][0m |          -0.0066 |          28.2444 |           0.0628 |
[32m[20230209 04:34:26 @agent_ppo2.py:193][0m |          -0.0035 |          27.6112 |           0.0624 |
[32m[20230209 04:34:26 @agent_ppo2.py:193][0m |          -0.0066 |          26.0390 |           0.0618 |
[32m[20230209 04:34:26 @agent_ppo2.py:193][0m |          -0.0129 |          24.9480 |           0.0614 |
[32m[20230209 04:34:26 @agent_ppo2.py:137][0m Policy update time: 0.84 s
[32m[20230209 04:34:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -120.58
[32m[20230209 04:34:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -108.36
[32m[20230209 04:34:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -122.23
[32m[20230209 04:34:27 @agent_ppo2.py:150][0m Total time:       0.51 min
[32m[20230209 04:34:27 @agent_ppo2.py:152][0m 26624 total steps have happened
[32m[20230209 04:34:27 @agent_ppo2.py:128][0m #------------------------ Iteration 13 --------------------------#
[32m[20230209 04:34:28 @agent_ppo2.py:134][0m Sampling time: 0.79 s by 1 slaves
[32m[20230209 04:34:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:28 @agent_ppo2.py:193][0m |           0.0310 |          52.3305 |           0.0636 |
[32m[20230209 04:34:28 @agent_ppo2.py:193][0m |           0.0047 |          37.1275 |           0.0618 |
[32m[20230209 04:34:28 @agent_ppo2.py:193][0m |           0.0147 |          34.3633 |           0.0610 |
[32m[20230209 04:34:28 @agent_ppo2.py:193][0m |          -0.0052 |          32.1119 |           0.0590 |
[32m[20230209 04:34:28 @agent_ppo2.py:193][0m |          -0.0130 |          30.3312 |           0.0595 |
[32m[20230209 04:34:28 @agent_ppo2.py:193][0m |          -0.0156 |          28.3732 |           0.0584 |
[32m[20230209 04:34:28 @agent_ppo2.py:193][0m |          -0.0223 |          26.9135 |           0.0578 |
[32m[20230209 04:34:28 @agent_ppo2.py:193][0m |          -0.0251 |          25.6486 |           0.0575 |
[32m[20230209 04:34:28 @agent_ppo2.py:193][0m |          -0.0293 |          24.8757 |           0.0565 |
[32m[20230209 04:34:28 @agent_ppo2.py:193][0m |          -0.0260 |          22.6536 |           0.0560 |
[32m[20230209 04:34:28 @agent_ppo2.py:137][0m Policy update time: 0.85 s
[32m[20230209 04:34:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -121.05
[32m[20230209 04:34:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.20
[32m[20230209 04:34:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -125.27
[32m[20230209 04:34:29 @agent_ppo2.py:150][0m Total time:       0.54 min
[32m[20230209 04:34:29 @agent_ppo2.py:152][0m 28672 total steps have happened
[32m[20230209 04:34:29 @agent_ppo2.py:128][0m #------------------------ Iteration 14 --------------------------#
[32m[20230209 04:34:30 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230209 04:34:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:30 @agent_ppo2.py:193][0m |           0.0186 |          31.3021 |           0.0649 |
[32m[20230209 04:34:30 @agent_ppo2.py:193][0m |           0.0097 |          20.6485 |           0.0641 |
[32m[20230209 04:34:30 @agent_ppo2.py:193][0m |           0.0097 |          19.5746 |           0.0638 |
[32m[20230209 04:34:30 @agent_ppo2.py:193][0m |           0.0180 |          18.8108 |           0.0627 |
[32m[20230209 04:34:30 @agent_ppo2.py:193][0m |          -0.0015 |          18.3610 |           0.0626 |
[32m[20230209 04:34:30 @agent_ppo2.py:193][0m |          -0.0130 |          17.9075 |           0.0626 |
[32m[20230209 04:34:30 @agent_ppo2.py:193][0m |          -0.0055 |          17.5058 |           0.0618 |
[32m[20230209 04:34:30 @agent_ppo2.py:193][0m |          -0.0233 |          17.4257 |           0.0621 |
[32m[20230209 04:34:30 @agent_ppo2.py:193][0m |          -0.0296 |          16.8873 |           0.0617 |
[32m[20230209 04:34:30 @agent_ppo2.py:193][0m |          -0.0254 |          16.4718 |           0.0613 |
[32m[20230209 04:34:30 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230209 04:34:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -122.98
[32m[20230209 04:34:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -107.53
[32m[20230209 04:34:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -126.25
[32m[20230209 04:34:31 @agent_ppo2.py:150][0m Total time:       0.58 min
[32m[20230209 04:34:31 @agent_ppo2.py:152][0m 30720 total steps have happened
[32m[20230209 04:34:31 @agent_ppo2.py:128][0m #------------------------ Iteration 15 --------------------------#
[32m[20230209 04:34:31 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230209 04:34:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:32 @agent_ppo2.py:193][0m |           5.3022 |          42.9927 |           0.0642 |
[32m[20230209 04:34:32 @agent_ppo2.py:193][0m |           0.0249 |          26.3433 |           0.0620 |
[32m[20230209 04:34:32 @agent_ppo2.py:193][0m |           0.0137 |          23.2779 |           0.0594 |
[32m[20230209 04:34:32 @agent_ppo2.py:193][0m |           0.0086 |          21.4011 |           0.0576 |
[32m[20230209 04:34:32 @agent_ppo2.py:193][0m |          -0.0031 |          20.7411 |           0.0576 |
[32m[20230209 04:34:32 @agent_ppo2.py:193][0m |           0.0044 |          22.4450 |           0.0566 |
[32m[20230209 04:34:32 @agent_ppo2.py:193][0m |           0.0134 |          20.6559 |           0.0561 |
[32m[20230209 04:34:32 @agent_ppo2.py:193][0m |          -0.0224 |          20.0946 |           0.0561 |
[32m[20230209 04:34:32 @agent_ppo2.py:193][0m |          -0.0285 |          18.7178 |           0.0552 |
[32m[20230209 04:34:32 @agent_ppo2.py:193][0m |           0.5504 |          18.5498 |           0.0548 |
[32m[20230209 04:34:32 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230209 04:34:33 @agent_ppo2.py:145][0m Average TRAINING episode reward: -122.14
[32m[20230209 04:34:33 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -105.42
[32m[20230209 04:34:33 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -113.17
[32m[20230209 04:34:33 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -113.17
[32m[20230209 04:34:33 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -113.17
[32m[20230209 04:34:33 @agent_ppo2.py:150][0m Total time:       0.61 min
[32m[20230209 04:34:33 @agent_ppo2.py:152][0m 32768 total steps have happened
[32m[20230209 04:34:33 @agent_ppo2.py:128][0m #------------------------ Iteration 16 --------------------------#
[32m[20230209 04:34:34 @agent_ppo2.py:134][0m Sampling time: 1.04 s by 1 slaves
[32m[20230209 04:34:34 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:34 @agent_ppo2.py:193][0m |           0.0180 |          26.6884 |           0.0657 |
[32m[20230209 04:34:34 @agent_ppo2.py:193][0m |           0.0116 |          16.4610 |           0.0648 |
[32m[20230209 04:34:34 @agent_ppo2.py:193][0m |           0.0164 |          15.5726 |           0.0653 |
[32m[20230209 04:34:34 @agent_ppo2.py:193][0m |           0.0093 |          14.8502 |           0.0641 |
[32m[20230209 04:34:34 @agent_ppo2.py:193][0m |           0.0003 |          14.5420 |           0.0635 |
[32m[20230209 04:34:35 @agent_ppo2.py:193][0m |          -0.0097 |          15.0021 |           0.0637 |
[32m[20230209 04:34:35 @agent_ppo2.py:193][0m |          -0.0181 |          15.5513 |           0.0635 |
[32m[20230209 04:34:35 @agent_ppo2.py:193][0m |          -0.0064 |          13.6005 |           0.0630 |
[32m[20230209 04:34:35 @agent_ppo2.py:193][0m |          -0.0133 |          13.2342 |           0.0633 |
[32m[20230209 04:34:35 @agent_ppo2.py:193][0m |          -0.0170 |          13.0095 |           0.0629 |
[32m[20230209 04:34:35 @agent_ppo2.py:137][0m Policy update time: 1.20 s
[32m[20230209 04:34:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -129.45
[32m[20230209 04:34:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.58
[32m[20230209 04:34:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -113.19
[32m[20230209 04:34:36 @agent_ppo2.py:150][0m Total time:       0.65 min
[32m[20230209 04:34:36 @agent_ppo2.py:152][0m 34816 total steps have happened
[32m[20230209 04:34:36 @agent_ppo2.py:128][0m #------------------------ Iteration 17 --------------------------#
[32m[20230209 04:34:36 @agent_ppo2.py:134][0m Sampling time: 0.61 s by 1 slaves
[32m[20230209 04:34:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:36 @agent_ppo2.py:193][0m |           0.0286 |          39.1897 |           0.0657 |
[32m[20230209 04:34:36 @agent_ppo2.py:193][0m |           0.0182 |          25.8928 |           0.0647 |
[32m[20230209 04:34:36 @agent_ppo2.py:193][0m |           0.0035 |          23.8408 |           0.0645 |
[32m[20230209 04:34:37 @agent_ppo2.py:193][0m |           0.0137 |          23.8005 |           0.0640 |
[32m[20230209 04:34:37 @agent_ppo2.py:193][0m |           0.0104 |          21.9518 |           0.0637 |
[32m[20230209 04:34:37 @agent_ppo2.py:193][0m |           0.0076 |          21.4601 |           0.0634 |
[32m[20230209 04:34:37 @agent_ppo2.py:193][0m |           0.0012 |          20.7650 |           0.0633 |
[32m[20230209 04:34:37 @agent_ppo2.py:193][0m |           0.0023 |          20.3618 |           0.0634 |
[32m[20230209 04:34:37 @agent_ppo2.py:193][0m |          -0.0001 |          20.2758 |           0.0629 |
[32m[20230209 04:34:37 @agent_ppo2.py:193][0m |          -0.0050 |          19.9860 |           0.0625 |
[32m[20230209 04:34:37 @agent_ppo2.py:137][0m Policy update time: 0.75 s
[32m[20230209 04:34:38 @agent_ppo2.py:145][0m Average TRAINING episode reward: -128.92
[32m[20230209 04:34:38 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -108.28
[32m[20230209 04:34:38 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -110.41
[32m[20230209 04:34:38 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -110.41
[32m[20230209 04:34:38 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -110.41
[32m[20230209 04:34:38 @agent_ppo2.py:150][0m Total time:       0.69 min
[32m[20230209 04:34:38 @agent_ppo2.py:152][0m 36864 total steps have happened
[32m[20230209 04:34:38 @agent_ppo2.py:128][0m #------------------------ Iteration 18 --------------------------#
[32m[20230209 04:34:39 @agent_ppo2.py:134][0m Sampling time: 1.03 s by 1 slaves
[32m[20230209 04:34:39 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:39 @agent_ppo2.py:193][0m |           0.0359 |          33.3066 |           0.0655 |
[32m[20230209 04:34:39 @agent_ppo2.py:193][0m |           0.0271 |          17.5290 |           0.0643 |
[32m[20230209 04:34:39 @agent_ppo2.py:193][0m |           0.0149 |          15.4192 |           0.0646 |
[32m[20230209 04:34:39 @agent_ppo2.py:193][0m |           0.0141 |          14.7905 |           0.0649 |
[32m[20230209 04:34:39 @agent_ppo2.py:193][0m |           0.0109 |          14.0398 |           0.0644 |
[32m[20230209 04:34:39 @agent_ppo2.py:193][0m |           0.0053 |          13.5037 |           0.0641 |
[32m[20230209 04:34:39 @agent_ppo2.py:193][0m |           0.0025 |          13.2603 |           0.0641 |
[32m[20230209 04:34:40 @agent_ppo2.py:193][0m |          -0.0012 |          12.8648 |           0.0644 |
[32m[20230209 04:34:40 @agent_ppo2.py:193][0m |          -0.0004 |          12.5447 |           0.0641 |
[32m[20230209 04:34:40 @agent_ppo2.py:193][0m |          -0.0005 |          12.3828 |           0.0641 |
[32m[20230209 04:34:40 @agent_ppo2.py:137][0m Policy update time: 1.07 s
[32m[20230209 04:34:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: -118.21
[32m[20230209 04:34:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.15
[32m[20230209 04:34:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -110.95
[32m[20230209 04:34:40 @agent_ppo2.py:150][0m Total time:       0.73 min
[32m[20230209 04:34:40 @agent_ppo2.py:152][0m 38912 total steps have happened
[32m[20230209 04:34:40 @agent_ppo2.py:128][0m #------------------------ Iteration 19 --------------------------#
[32m[20230209 04:34:41 @agent_ppo2.py:134][0m Sampling time: 0.96 s by 1 slaves
[32m[20230209 04:34:41 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:41 @agent_ppo2.py:193][0m |           0.0672 |          24.8044 |           0.0633 |
[32m[20230209 04:34:42 @agent_ppo2.py:193][0m |           0.0057 |          12.9008 |           0.0629 |
[32m[20230209 04:34:42 @agent_ppo2.py:193][0m |           0.0010 |          12.2251 |           0.0616 |
[32m[20230209 04:34:42 @agent_ppo2.py:193][0m |          -0.0058 |          11.7668 |           0.0618 |
[32m[20230209 04:34:42 @agent_ppo2.py:193][0m |          -0.0108 |          11.4387 |           0.0611 |
[32m[20230209 04:34:42 @agent_ppo2.py:193][0m |          -0.0116 |          11.2068 |           0.0613 |
[32m[20230209 04:34:42 @agent_ppo2.py:193][0m |          -0.0169 |          11.1809 |           0.0611 |
[32m[20230209 04:34:42 @agent_ppo2.py:193][0m |          -0.0126 |          10.9458 |           0.0610 |
[32m[20230209 04:34:42 @agent_ppo2.py:193][0m |          -0.0211 |          10.8845 |           0.0611 |
[32m[20230209 04:34:42 @agent_ppo2.py:193][0m |           0.0174 |          10.7922 |           0.0612 |
[32m[20230209 04:34:42 @agent_ppo2.py:137][0m Policy update time: 1.08 s
[32m[20230209 04:34:43 @agent_ppo2.py:145][0m Average TRAINING episode reward: -134.76
[32m[20230209 04:34:43 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.58
[32m[20230209 04:34:43 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -113.20
[32m[20230209 04:34:43 @agent_ppo2.py:150][0m Total time:       0.78 min
[32m[20230209 04:34:43 @agent_ppo2.py:152][0m 40960 total steps have happened
[32m[20230209 04:34:43 @agent_ppo2.py:128][0m #------------------------ Iteration 20 --------------------------#
[32m[20230209 04:34:44 @agent_ppo2.py:134][0m Sampling time: 0.60 s by 1 slaves
[32m[20230209 04:34:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:34:44 @agent_ppo2.py:193][0m |           0.0757 |          16.6359 |           0.0621 |
[32m[20230209 04:34:44 @agent_ppo2.py:193][0m |           0.1170 |           7.3681 |           0.0615 |
[32m[20230209 04:34:44 @agent_ppo2.py:193][0m |           0.0056 |           5.9963 |           0.0603 |
[32m[20230209 04:34:44 @agent_ppo2.py:193][0m |           0.3525 |           5.6961 |           0.0572 |
[32m[20230209 04:34:44 @agent_ppo2.py:193][0m |          -0.0128 |           5.2957 |           0.0574 |
[32m[20230209 04:34:44 @agent_ppo2.py:193][0m |          -0.0227 |           5.0545 |           0.0578 |
[32m[20230209 04:34:44 @agent_ppo2.py:193][0m |          -0.0262 |           4.9072 |           0.0579 |
[32m[20230209 04:34:44 @agent_ppo2.py:193][0m |          -0.0278 |           4.8665 |           0.0583 |
[32m[20230209 04:34:44 @agent_ppo2.py:193][0m |          -0.0304 |           4.5659 |           0.0580 |
[32m[20230209 04:34:44 @agent_ppo2.py:193][0m |          -0.0311 |           4.4407 |           0.0584 |
[32m[20230209 04:34:44 @agent_ppo2.py:137][0m Policy update time: 0.70 s
