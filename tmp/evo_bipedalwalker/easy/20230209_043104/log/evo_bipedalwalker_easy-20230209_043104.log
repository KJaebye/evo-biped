[32m[20230209 04:31:04 @logger.py:106][0m Log file set to ./tmp/evo_bipedalwalker/easy/20230209_043104/log/evo_bipedalwalker_easy-20230209_043104.log
[32m[20230209 04:31:04 @agent_ppo2.py:128][0m #------------------------ Iteration 0 --------------------------#
[32m[20230209 04:31:05 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230209 04:31:05 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:31:05 @agent_ppo2.py:193][0m |          -0.0020 |         482.3031 |           0.0649 |
[32m[20230209 04:31:05 @agent_ppo2.py:193][0m |           0.0074 |         494.7201 |           0.0648 |
[32m[20230209 04:31:05 @agent_ppo2.py:193][0m |          -0.0041 |         462.1495 |           0.0648 |
[32m[20230209 04:31:05 @agent_ppo2.py:193][0m |          -0.0031 |         440.0414 |           0.0648 |
[32m[20230209 04:31:05 @agent_ppo2.py:193][0m |          -0.0022 |         421.9908 |           0.0647 |
[32m[20230209 04:31:05 @agent_ppo2.py:193][0m |           0.0002 |         410.5371 |           0.0647 |
[32m[20230209 04:31:05 @agent_ppo2.py:193][0m |          -0.0009 |         400.8073 |           0.0647 |
[32m[20230209 04:31:05 @agent_ppo2.py:193][0m |          -0.0044 |         383.8429 |           0.0646 |
[32m[20230209 04:31:05 @agent_ppo2.py:193][0m |          -0.0023 |         382.6819 |           0.0646 |
[32m[20230209 04:31:05 @agent_ppo2.py:193][0m |          -0.0058 |         366.6456 |           0.0646 |
[32m[20230209 04:31:05 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230209 04:31:06 @agent_ppo2.py:145][0m Average TRAINING episode reward: -121.79
[32m[20230209 04:31:06 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -97.69
[32m[20230209 04:31:06 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -99.92
[32m[20230209 04:31:06 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -99.92
[32m[20230209 04:31:06 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -99.92
[32m[20230209 04:31:06 @agent_ppo2.py:150][0m Total time:       0.04 min
[32m[20230209 04:31:06 @agent_ppo2.py:152][0m 2048 total steps have happened
[32m[20230209 04:31:06 @agent_ppo2.py:128][0m #------------------------ Iteration 1 --------------------------#
[32m[20230209 04:31:07 @agent_ppo2.py:134][0m Sampling time: 0.75 s by 1 slaves
[32m[20230209 04:31:07 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:31:07 @agent_ppo2.py:193][0m |          -0.0014 |         168.0395 |           0.0634 |
[32m[20230209 04:31:07 @agent_ppo2.py:193][0m |          -0.0028 |         164.3542 |           0.0634 |
[32m[20230209 04:31:07 @agent_ppo2.py:193][0m |           0.0004 |         163.9810 |           0.0634 |
[32m[20230209 04:31:08 @agent_ppo2.py:193][0m |          -0.0031 |         160.5892 |           0.0634 |
[32m[20230209 04:31:08 @agent_ppo2.py:193][0m |          -0.0011 |         160.8482 |           0.0634 |
[32m[20230209 04:31:08 @agent_ppo2.py:193][0m |          -0.0055 |         157.6658 |           0.0634 |
[32m[20230209 04:31:08 @agent_ppo2.py:193][0m |          -0.0049 |         158.0952 |           0.0634 |
[32m[20230209 04:31:08 @agent_ppo2.py:193][0m |          -0.0076 |         156.1010 |           0.0634 |
[32m[20230209 04:31:08 @agent_ppo2.py:193][0m |          -0.0050 |         158.8274 |           0.0634 |
[32m[20230209 04:31:08 @agent_ppo2.py:193][0m |          -0.0088 |         155.7024 |           0.0634 |
[32m[20230209 04:31:08 @agent_ppo2.py:137][0m Policy update time: 0.75 s
