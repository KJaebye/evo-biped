[32m[20230207 13:26:14 @logger.py:106][0m Log file set to ./tmp/evo_bipedalwalker/easy/20230207_132614/log/evo_bipedalwalker_easy-20230207_132614.log
[32m[20230207 13:26:14 @agent_ppo2.py:128][0m #------------------------ Iteration 0 --------------------------#
[32m[20230207 13:26:15 @agent_ppo2.py:134][0m Sampling time: 0.59 s by 1 slaves
[32m[20230207 13:26:15 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:15 @agent_ppo2.py:192][0m |          -0.0001 |         377.6521 |           0.0652 |
[32m[20230207 13:26:15 @agent_ppo2.py:192][0m |          -0.0085 |         373.1604 |           0.0654 |
[32m[20230207 13:26:15 @agent_ppo2.py:192][0m |          -0.0130 |         365.7394 |           0.0654 |
[32m[20230207 13:26:15 @agent_ppo2.py:192][0m |          -0.0149 |         354.0757 |           0.0653 |
[32m[20230207 13:26:15 @agent_ppo2.py:192][0m |          -0.0165 |         340.8475 |           0.0653 |
[32m[20230207 13:26:15 @agent_ppo2.py:192][0m |          -0.0183 |         326.2249 |           0.0652 |
[32m[20230207 13:26:15 @agent_ppo2.py:192][0m |          -0.0198 |         312.2230 |           0.0651 |
[32m[20230207 13:26:15 @agent_ppo2.py:192][0m |          -0.0212 |         301.7096 |           0.0651 |
[32m[20230207 13:26:15 @agent_ppo2.py:192][0m |          -0.0220 |         292.3101 |           0.0650 |
[32m[20230207 13:26:15 @agent_ppo2.py:192][0m |          -0.0219 |         285.9427 |           0.0650 |
[32m[20230207 13:26:15 @agent_ppo2.py:137][0m Policy update time: 0.65 s
[32m[20230207 13:26:16 @agent_ppo2.py:145][0m Average TRAINING episode reward: -137.43
[32m[20230207 13:26:16 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -104.23
[32m[20230207 13:26:16 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -123.13
[32m[20230207 13:26:16 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -123.13
[32m[20230207 13:26:16 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -123.13
[32m[20230207 13:26:16 @agent_ppo2.py:150][0m Total time:       0.04 min
[32m[20230207 13:26:16 @agent_ppo2.py:152][0m 2048 total steps have happened
[32m[20230207 13:26:16 @agent_ppo2.py:128][0m #------------------------ Iteration 1 --------------------------#
[32m[20230207 13:26:17 @agent_ppo2.py:134][0m Sampling time: 0.84 s by 1 slaves
[32m[20230207 13:26:17 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:17 @agent_ppo2.py:192][0m |           0.0015 |         293.9442 |           0.0647 |
[32m[20230207 13:26:17 @agent_ppo2.py:192][0m |          -0.0080 |         274.1883 |           0.0646 |
[32m[20230207 13:26:17 @agent_ppo2.py:192][0m |          -0.0123 |         262.1209 |           0.0646 |
[32m[20230207 13:26:18 @agent_ppo2.py:192][0m |          -0.0140 |         254.5688 |           0.0646 |
[32m[20230207 13:26:18 @agent_ppo2.py:192][0m |          -0.0163 |         246.0456 |           0.0644 |
[32m[20230207 13:26:18 @agent_ppo2.py:192][0m |          -0.0173 |         239.4574 |           0.0643 |
[32m[20230207 13:26:18 @agent_ppo2.py:192][0m |          -0.0196 |         232.4235 |           0.0643 |
[32m[20230207 13:26:18 @agent_ppo2.py:192][0m |          -0.0204 |         227.1816 |           0.0642 |
[32m[20230207 13:26:18 @agent_ppo2.py:192][0m |          -0.0215 |         222.6904 |           0.0642 |
[32m[20230207 13:26:18 @agent_ppo2.py:192][0m |          -0.0218 |         219.7167 |           0.0642 |
[32m[20230207 13:26:18 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230207 13:26:19 @agent_ppo2.py:145][0m Average TRAINING episode reward: -123.76
[32m[20230207 13:26:19 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.00
[32m[20230207 13:26:19 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -129.33
[32m[20230207 13:26:19 @agent_ppo2.py:150][0m Total time:       0.08 min
[32m[20230207 13:26:19 @agent_ppo2.py:152][0m 4096 total steps have happened
[32m[20230207 13:26:19 @agent_ppo2.py:128][0m #------------------------ Iteration 2 --------------------------#
[32m[20230207 13:26:20 @agent_ppo2.py:134][0m Sampling time: 0.69 s by 1 slaves
[32m[20230207 13:26:20 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:20 @agent_ppo2.py:192][0m |          -0.0003 |         104.9709 |           0.0640 |
[32m[20230207 13:26:20 @agent_ppo2.py:192][0m |          -0.0110 |         101.5651 |           0.0635 |
[32m[20230207 13:26:20 @agent_ppo2.py:192][0m |          -0.0141 |         100.8335 |           0.0633 |
[32m[20230207 13:26:20 @agent_ppo2.py:192][0m |          -0.0186 |          97.2841 |           0.0632 |
[32m[20230207 13:26:20 @agent_ppo2.py:192][0m |          -0.0193 |          96.5912 |           0.0628 |
[32m[20230207 13:26:20 @agent_ppo2.py:192][0m |          -0.0197 |          96.4545 |           0.0626 |
[32m[20230207 13:26:20 @agent_ppo2.py:192][0m |          -0.0183 |          95.4622 |           0.0624 |
[32m[20230207 13:26:20 @agent_ppo2.py:192][0m |          -0.0234 |          93.0639 |           0.0623 |
[32m[20230207 13:26:20 @agent_ppo2.py:192][0m |          -0.0235 |          92.2348 |           0.0622 |
[32m[20230207 13:26:20 @agent_ppo2.py:192][0m |          -0.0218 |          92.4537 |           0.0621 |
[32m[20230207 13:26:20 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230207 13:26:21 @agent_ppo2.py:145][0m Average TRAINING episode reward: -134.59
[32m[20230207 13:26:21 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -99.90
[32m[20230207 13:26:21 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -149.82
[32m[20230207 13:26:21 @agent_ppo2.py:150][0m Total time:       0.11 min
[32m[20230207 13:26:21 @agent_ppo2.py:152][0m 6144 total steps have happened
[32m[20230207 13:26:21 @agent_ppo2.py:128][0m #------------------------ Iteration 3 --------------------------#
[32m[20230207 13:26:22 @agent_ppo2.py:134][0m Sampling time: 0.92 s by 1 slaves
[32m[20230207 13:26:22 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:22 @agent_ppo2.py:192][0m |          -0.0016 |         229.7438 |           0.0611 |
[32m[20230207 13:26:22 @agent_ppo2.py:192][0m |          -0.0100 |         220.4611 |           0.0610 |
[32m[20230207 13:26:22 @agent_ppo2.py:192][0m |          -0.0171 |         210.4716 |           0.0609 |
[32m[20230207 13:26:22 @agent_ppo2.py:192][0m |          -0.0211 |         200.1636 |           0.0609 |
[32m[20230207 13:26:22 @agent_ppo2.py:192][0m |          -0.0242 |         191.4497 |           0.0609 |
[32m[20230207 13:26:22 @agent_ppo2.py:192][0m |          -0.0266 |         183.8044 |           0.0610 |
[32m[20230207 13:26:22 @agent_ppo2.py:192][0m |          -0.0191 |         183.5359 |           0.0608 |
[32m[20230207 13:26:22 @agent_ppo2.py:192][0m |          -0.0237 |         173.0995 |           0.0608 |
[32m[20230207 13:26:23 @agent_ppo2.py:192][0m |          -0.0225 |         168.3450 |           0.0608 |
[32m[20230207 13:26:23 @agent_ppo2.py:192][0m |          -0.0297 |         159.5819 |           0.0608 |
[32m[20230207 13:26:23 @agent_ppo2.py:137][0m Policy update time: 0.79 s
[32m[20230207 13:26:23 @agent_ppo2.py:145][0m Average TRAINING episode reward: -113.48
[32m[20230207 13:26:23 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.30
[32m[20230207 13:26:23 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -123.94
[32m[20230207 13:26:23 @agent_ppo2.py:150][0m Total time:       0.15 min
[32m[20230207 13:26:23 @agent_ppo2.py:152][0m 8192 total steps have happened
[32m[20230207 13:26:23 @agent_ppo2.py:128][0m #------------------------ Iteration 4 --------------------------#
[32m[20230207 13:26:24 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 13:26:24 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:24 @agent_ppo2.py:192][0m |           0.0008 |         115.8256 |           0.0607 |
[32m[20230207 13:26:24 @agent_ppo2.py:192][0m |          -0.0063 |         109.1411 |           0.0602 |
[32m[20230207 13:26:24 @agent_ppo2.py:192][0m |          -0.0158 |         104.3545 |           0.0602 |
[32m[20230207 13:26:24 @agent_ppo2.py:192][0m |          -0.0099 |         106.5791 |           0.0598 |
[32m[20230207 13:26:24 @agent_ppo2.py:192][0m |          -0.0147 |         102.0824 |           0.0597 |
[32m[20230207 13:26:24 @agent_ppo2.py:192][0m |          -0.0171 |          98.8857 |           0.0595 |
[32m[20230207 13:26:24 @agent_ppo2.py:192][0m |          -0.0188 |          98.3085 |           0.0597 |
[32m[20230207 13:26:24 @agent_ppo2.py:192][0m |          -0.0216 |          96.2206 |           0.0597 |
[32m[20230207 13:26:25 @agent_ppo2.py:192][0m |          -0.0192 |          94.8657 |           0.0597 |
[32m[20230207 13:26:25 @agent_ppo2.py:192][0m |          -0.0206 |          95.9230 |           0.0596 |
[32m[20230207 13:26:25 @agent_ppo2.py:137][0m Policy update time: 0.71 s
[32m[20230207 13:26:25 @agent_ppo2.py:145][0m Average TRAINING episode reward: -123.57
[32m[20230207 13:26:25 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.72
[32m[20230207 13:26:25 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -114.36
[32m[20230207 13:26:25 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -114.36
[32m[20230207 13:26:25 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -114.36
[32m[20230207 13:26:25 @agent_ppo2.py:150][0m Total time:       0.18 min
[32m[20230207 13:26:25 @agent_ppo2.py:152][0m 10240 total steps have happened
[32m[20230207 13:26:25 @agent_ppo2.py:128][0m #------------------------ Iteration 5 --------------------------#
[32m[20230207 13:26:26 @agent_ppo2.py:134][0m Sampling time: 0.82 s by 1 slaves
[32m[20230207 13:26:26 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:26 @agent_ppo2.py:192][0m |           0.0098 |          87.2316 |           0.0584 |
[32m[20230207 13:26:26 @agent_ppo2.py:192][0m |           0.0007 |          64.8103 |           0.0580 |
[32m[20230207 13:26:26 @agent_ppo2.py:192][0m |          -0.0040 |          66.9738 |           0.0576 |
[32m[20230207 13:26:26 @agent_ppo2.py:192][0m |          -0.0121 |          60.4942 |           0.0574 |
[32m[20230207 13:26:26 @agent_ppo2.py:192][0m |          -0.0086 |          61.1630 |           0.0572 |
[32m[20230207 13:26:27 @agent_ppo2.py:192][0m |          -0.0086 |          61.8562 |           0.0572 |
[32m[20230207 13:26:27 @agent_ppo2.py:192][0m |          -0.0170 |          58.7290 |           0.0571 |
[32m[20230207 13:26:27 @agent_ppo2.py:192][0m |          -0.0166 |          58.5194 |           0.0571 |
[32m[20230207 13:26:27 @agent_ppo2.py:192][0m |          -0.0219 |          58.0521 |           0.0572 |
[32m[20230207 13:26:27 @agent_ppo2.py:192][0m |          -0.0177 |          58.1629 |           0.0572 |
[32m[20230207 13:26:27 @agent_ppo2.py:137][0m Policy update time: 0.75 s
[32m[20230207 13:26:27 @agent_ppo2.py:145][0m Average TRAINING episode reward: -175.11
[32m[20230207 13:26:27 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -105.31
[32m[20230207 13:26:27 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -135.10
[32m[20230207 13:26:27 @agent_ppo2.py:150][0m Total time:       0.22 min
[32m[20230207 13:26:27 @agent_ppo2.py:152][0m 12288 total steps have happened
[32m[20230207 13:26:27 @agent_ppo2.py:128][0m #------------------------ Iteration 6 --------------------------#
[32m[20230207 13:26:28 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 13:26:28 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:28 @agent_ppo2.py:192][0m |           0.0040 |         236.7969 |           0.0582 |
[32m[20230207 13:26:28 @agent_ppo2.py:192][0m |          -0.0076 |         221.6516 |           0.0580 |
[32m[20230207 13:26:28 @agent_ppo2.py:192][0m |          -0.0140 |         211.1706 |           0.0578 |
[32m[20230207 13:26:28 @agent_ppo2.py:192][0m |          -0.0183 |         201.6160 |           0.0576 |
[32m[20230207 13:26:28 @agent_ppo2.py:192][0m |          -0.0213 |         196.1322 |           0.0576 |
[32m[20230207 13:26:28 @agent_ppo2.py:192][0m |          -0.0249 |         188.5311 |           0.0574 |
[32m[20230207 13:26:28 @agent_ppo2.py:192][0m |          -0.0233 |         181.8449 |           0.0573 |
[32m[20230207 13:26:28 @agent_ppo2.py:192][0m |          -0.0281 |         177.6031 |           0.0571 |
[32m[20230207 13:26:28 @agent_ppo2.py:192][0m |          -0.0228 |         173.4367 |           0.0572 |
[32m[20230207 13:26:28 @agent_ppo2.py:192][0m |          -0.0288 |         165.8057 |           0.0572 |
[32m[20230207 13:26:28 @agent_ppo2.py:137][0m Policy update time: 0.52 s
[32m[20230207 13:26:29 @agent_ppo2.py:145][0m Average TRAINING episode reward: -120.50
[32m[20230207 13:26:29 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.64
[32m[20230207 13:26:29 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -116.96
[32m[20230207 13:26:29 @agent_ppo2.py:150][0m Total time:       0.25 min
[32m[20230207 13:26:29 @agent_ppo2.py:152][0m 14336 total steps have happened
[32m[20230207 13:26:29 @agent_ppo2.py:128][0m #------------------------ Iteration 7 --------------------------#
[32m[20230207 13:26:30 @agent_ppo2.py:134][0m Sampling time: 0.85 s by 1 slaves
[32m[20230207 13:26:30 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:30 @agent_ppo2.py:192][0m |           0.0026 |         122.1714 |           0.0561 |
[32m[20230207 13:26:30 @agent_ppo2.py:192][0m |          -0.0082 |         113.7393 |           0.0560 |
[32m[20230207 13:26:30 @agent_ppo2.py:192][0m |          -0.0107 |         109.4272 |           0.0557 |
[32m[20230207 13:26:30 @agent_ppo2.py:192][0m |          -0.0150 |         103.9748 |           0.0557 |
[32m[20230207 13:26:30 @agent_ppo2.py:192][0m |          -0.0170 |          99.5392 |           0.0556 |
[32m[20230207 13:26:30 @agent_ppo2.py:192][0m |          -0.0181 |          95.4413 |           0.0557 |
[32m[20230207 13:26:31 @agent_ppo2.py:192][0m |          -0.0191 |          91.8557 |           0.0557 |
[32m[20230207 13:26:31 @agent_ppo2.py:192][0m |          -0.0212 |          87.6359 |           0.0555 |
[32m[20230207 13:26:31 @agent_ppo2.py:192][0m |          -0.0196 |          85.5809 |           0.0556 |
[32m[20230207 13:26:31 @agent_ppo2.py:192][0m |          -0.0241 |          81.7301 |           0.0555 |
[32m[20230207 13:26:31 @agent_ppo2.py:137][0m Policy update time: 0.96 s
[32m[20230207 13:26:31 @agent_ppo2.py:145][0m Average TRAINING episode reward: -117.21
[32m[20230207 13:26:31 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.64
[32m[20230207 13:26:31 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -119.79
[32m[20230207 13:26:31 @agent_ppo2.py:150][0m Total time:       0.29 min
[32m[20230207 13:26:31 @agent_ppo2.py:152][0m 16384 total steps have happened
[32m[20230207 13:26:31 @agent_ppo2.py:128][0m #------------------------ Iteration 8 --------------------------#
[32m[20230207 13:26:32 @agent_ppo2.py:134][0m Sampling time: 0.70 s by 1 slaves
[32m[20230207 13:26:32 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:32 @agent_ppo2.py:192][0m |          -0.0046 |          67.0678 |           0.0541 |
[32m[20230207 13:26:32 @agent_ppo2.py:192][0m |          -0.0109 |          60.8443 |           0.0541 |
[32m[20230207 13:26:32 @agent_ppo2.py:192][0m |          -0.0192 |          57.9523 |           0.0542 |
[32m[20230207 13:26:32 @agent_ppo2.py:192][0m |           0.0181 |          76.1913 |           0.0541 |
[32m[20230207 13:26:33 @agent_ppo2.py:192][0m |          -0.0284 |          53.5278 |           0.0541 |
[32m[20230207 13:26:33 @agent_ppo2.py:192][0m |          -0.0274 |          51.9204 |           0.0540 |
[32m[20230207 13:26:33 @agent_ppo2.py:192][0m |           0.0133 |          67.9004 |           0.0540 |
[32m[20230207 13:26:33 @agent_ppo2.py:192][0m |           0.0154 |          55.1270 |           0.0535 |
[32m[20230207 13:26:33 @agent_ppo2.py:192][0m |           0.0158 |          55.5461 |           0.0534 |
[32m[20230207 13:26:33 @agent_ppo2.py:192][0m |          -0.0333 |          46.4043 |           0.0537 |
[32m[20230207 13:26:33 @agent_ppo2.py:137][0m Policy update time: 0.96 s
[32m[20230207 13:26:34 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.40
[32m[20230207 13:26:34 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -102.43
[32m[20230207 13:26:34 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -111.03
[32m[20230207 13:26:34 @evo_bipedalwalker_agent.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -111.03
[32m[20230207 13:26:34 @evo_bipedalwalker_agent.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -111.03
[32m[20230207 13:26:34 @agent_ppo2.py:150][0m Total time:       0.33 min
[32m[20230207 13:26:34 @agent_ppo2.py:152][0m 18432 total steps have happened
[32m[20230207 13:26:34 @agent_ppo2.py:128][0m #------------------------ Iteration 9 --------------------------#
[32m[20230207 13:26:34 @agent_ppo2.py:134][0m Sampling time: 0.58 s by 1 slaves
[32m[20230207 13:26:34 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:34 @agent_ppo2.py:192][0m |           0.0004 |          76.1453 |           0.0576 |
[32m[20230207 13:26:34 @agent_ppo2.py:192][0m |          -0.0128 |          70.5412 |           0.0571 |
[32m[20230207 13:26:35 @agent_ppo2.py:192][0m |          -0.0176 |          70.0163 |           0.0566 |
[32m[20230207 13:26:35 @agent_ppo2.py:192][0m |          -0.0221 |          66.5588 |           0.0562 |
[32m[20230207 13:26:35 @agent_ppo2.py:192][0m |          -0.0228 |          65.5807 |           0.0560 |
[32m[20230207 13:26:35 @agent_ppo2.py:192][0m |          -0.0279 |          64.3226 |           0.0558 |
[32m[20230207 13:26:35 @agent_ppo2.py:192][0m |          -0.0291 |          63.3070 |           0.0556 |
[32m[20230207 13:26:35 @agent_ppo2.py:192][0m |          -0.0317 |          61.0843 |           0.0556 |
[32m[20230207 13:26:35 @agent_ppo2.py:192][0m |          -0.0321 |          61.3493 |           0.0555 |
[32m[20230207 13:26:35 @agent_ppo2.py:192][0m |          -0.0299 |          60.5413 |           0.0554 |
[32m[20230207 13:26:35 @agent_ppo2.py:137][0m Policy update time: 0.60 s
[32m[20230207 13:26:36 @agent_ppo2.py:145][0m Average TRAINING episode reward: -114.62
[32m[20230207 13:26:36 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.53
[32m[20230207 13:26:36 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -144.79
[32m[20230207 13:26:36 @agent_ppo2.py:150][0m Total time:       0.36 min
[32m[20230207 13:26:36 @agent_ppo2.py:152][0m 20480 total steps have happened
[32m[20230207 13:26:36 @agent_ppo2.py:128][0m #------------------------ Iteration 10 --------------------------#
[32m[20230207 13:26:36 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 13:26:36 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:36 @agent_ppo2.py:192][0m |          -0.0025 |          87.2158 |           0.0526 |
[32m[20230207 13:26:36 @agent_ppo2.py:192][0m |          -0.0116 |          79.6892 |           0.0525 |
[32m[20230207 13:26:36 @agent_ppo2.py:192][0m |          -0.0202 |          75.5814 |           0.0524 |
[32m[20230207 13:26:36 @agent_ppo2.py:192][0m |          -0.0211 |          72.8948 |           0.0523 |
[32m[20230207 13:26:36 @agent_ppo2.py:192][0m |          -0.0263 |          69.3554 |           0.0523 |
[32m[20230207 13:26:36 @agent_ppo2.py:192][0m |          -0.0243 |          67.8630 |           0.0523 |
[32m[20230207 13:26:37 @agent_ppo2.py:192][0m |          -0.0292 |          65.8521 |           0.0522 |
[32m[20230207 13:26:37 @agent_ppo2.py:192][0m |          -0.0294 |          64.3971 |           0.0523 |
[32m[20230207 13:26:37 @agent_ppo2.py:192][0m |          -0.0368 |          61.8481 |           0.0521 |
[32m[20230207 13:26:37 @agent_ppo2.py:192][0m |          -0.0302 |          61.6192 |           0.0521 |
[32m[20230207 13:26:37 @agent_ppo2.py:137][0m Policy update time: 0.59 s
[32m[20230207 13:26:37 @agent_ppo2.py:145][0m Average TRAINING episode reward: -125.03
[32m[20230207 13:26:37 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.04
[32m[20230207 13:26:37 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -117.22
[32m[20230207 13:26:37 @agent_ppo2.py:150][0m Total time:       0.38 min
[32m[20230207 13:26:37 @agent_ppo2.py:152][0m 22528 total steps have happened
[32m[20230207 13:26:37 @agent_ppo2.py:128][0m #------------------------ Iteration 11 --------------------------#
[32m[20230207 13:26:38 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230207 13:26:38 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:38 @agent_ppo2.py:192][0m |           0.0022 |          39.5455 |           0.0516 |
[32m[20230207 13:26:38 @agent_ppo2.py:192][0m |          -0.0074 |          33.7910 |           0.0515 |
[32m[20230207 13:26:38 @agent_ppo2.py:192][0m |          -0.0119 |          31.6393 |           0.0513 |
[32m[20230207 13:26:38 @agent_ppo2.py:192][0m |          -0.0130 |          30.2536 |           0.0511 |
[32m[20230207 13:26:38 @agent_ppo2.py:192][0m |          -0.0170 |          29.4119 |           0.0509 |
[32m[20230207 13:26:39 @agent_ppo2.py:192][0m |          -0.0165 |          28.5028 |           0.0510 |
[32m[20230207 13:26:39 @agent_ppo2.py:192][0m |          -0.0207 |          28.1156 |           0.0509 |
[32m[20230207 13:26:39 @agent_ppo2.py:192][0m |          -0.0231 |          27.3503 |           0.0509 |
[32m[20230207 13:26:39 @agent_ppo2.py:192][0m |          -0.0231 |          26.7475 |           0.0508 |
[32m[20230207 13:26:39 @agent_ppo2.py:192][0m |          -0.0234 |          26.0517 |           0.0508 |
[32m[20230207 13:26:39 @agent_ppo2.py:137][0m Policy update time: 0.89 s
[32m[20230207 13:26:40 @agent_ppo2.py:145][0m Average TRAINING episode reward: -127.31
[32m[20230207 13:26:40 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -101.74
[32m[20230207 13:26:40 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -166.12
[32m[20230207 13:26:40 @agent_ppo2.py:150][0m Total time:       0.42 min
[32m[20230207 13:26:40 @agent_ppo2.py:152][0m 24576 total steps have happened
[32m[20230207 13:26:40 @agent_ppo2.py:128][0m #------------------------ Iteration 12 --------------------------#
[32m[20230207 13:26:40 @agent_ppo2.py:134][0m Sampling time: 0.74 s by 1 slaves
[32m[20230207 13:26:40 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:40 @agent_ppo2.py:192][0m |           0.0032 |          61.6782 |           0.0502 |
[32m[20230207 13:26:41 @agent_ppo2.py:192][0m |          -0.0124 |          53.1766 |           0.0499 |
[32m[20230207 13:26:41 @agent_ppo2.py:192][0m |          -0.0154 |          49.9707 |           0.0497 |
[32m[20230207 13:26:41 @agent_ppo2.py:192][0m |          -0.0199 |          47.4600 |           0.0498 |
[32m[20230207 13:26:41 @agent_ppo2.py:192][0m |          -0.0240 |          45.2800 |           0.0495 |
[32m[20230207 13:26:41 @agent_ppo2.py:192][0m |          -0.0245 |          43.3723 |           0.0495 |
[32m[20230207 13:26:41 @agent_ppo2.py:192][0m |          -0.0246 |          41.8074 |           0.0494 |
[32m[20230207 13:26:41 @agent_ppo2.py:192][0m |          -0.0296 |          40.5066 |           0.0494 |
[32m[20230207 13:26:41 @agent_ppo2.py:192][0m |          -0.0304 |          38.6399 |           0.0493 |
[32m[20230207 13:26:41 @agent_ppo2.py:192][0m |          -0.0297 |          37.2319 |           0.0493 |
[32m[20230207 13:26:41 @agent_ppo2.py:137][0m Policy update time: 0.79 s
[32m[20230207 13:26:42 @agent_ppo2.py:145][0m Average TRAINING episode reward: -118.97
[32m[20230207 13:26:42 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -100.79
[32m[20230207 13:26:42 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -112.92
[32m[20230207 13:26:42 @agent_ppo2.py:150][0m Total time:       0.46 min
[32m[20230207 13:26:42 @agent_ppo2.py:152][0m 26624 total steps have happened
[32m[20230207 13:26:42 @agent_ppo2.py:128][0m #------------------------ Iteration 13 --------------------------#
[32m[20230207 13:26:42 @agent_ppo2.py:134][0m Sampling time: 0.56 s by 1 slaves
[32m[20230207 13:26:42 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:42 @agent_ppo2.py:192][0m |          -0.0011 |          45.0872 |           0.0506 |
[32m[20230207 13:26:42 @agent_ppo2.py:192][0m |          -0.0100 |          36.4362 |           0.0504 |
[32m[20230207 13:26:42 @agent_ppo2.py:192][0m |          -0.0189 |          33.4360 |           0.0501 |
[32m[20230207 13:26:43 @agent_ppo2.py:192][0m |          -0.0160 |          31.6654 |           0.0500 |
[32m[20230207 13:26:43 @agent_ppo2.py:192][0m |          -0.0217 |          29.7060 |           0.0496 |
[32m[20230207 13:26:43 @agent_ppo2.py:192][0m |          -0.0239 |          28.0798 |           0.0496 |
[32m[20230207 13:26:43 @agent_ppo2.py:192][0m |          -0.0277 |          27.0231 |           0.0495 |
[32m[20230207 13:26:43 @agent_ppo2.py:192][0m |          -0.0287 |          26.2863 |           0.0494 |
[32m[20230207 13:26:43 @agent_ppo2.py:192][0m |          -0.0297 |          24.8014 |           0.0493 |
[32m[20230207 13:26:43 @agent_ppo2.py:192][0m |          -0.0329 |          23.7271 |           0.0492 |
[32m[20230207 13:26:43 @agent_ppo2.py:137][0m Policy update time: 0.63 s
[32m[20230207 13:26:44 @agent_ppo2.py:145][0m Average TRAINING episode reward: -118.03
[32m[20230207 13:26:44 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -105.45
[32m[20230207 13:26:44 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -113.62
[32m[20230207 13:26:44 @agent_ppo2.py:150][0m Total time:       0.49 min
[32m[20230207 13:26:44 @agent_ppo2.py:152][0m 28672 total steps have happened
[32m[20230207 13:26:44 @agent_ppo2.py:128][0m #------------------------ Iteration 14 --------------------------#
[32m[20230207 13:26:44 @agent_ppo2.py:134][0m Sampling time: 0.75 s by 1 slaves
[32m[20230207 13:26:44 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:44 @agent_ppo2.py:192][0m |           0.0206 |          18.0942 |           0.0471 |
[32m[20230207 13:26:45 @agent_ppo2.py:192][0m |           0.0029 |          15.4548 |           0.0463 |
[32m[20230207 13:26:45 @agent_ppo2.py:192][0m |          -0.0129 |          14.4099 |           0.0469 |
[32m[20230207 13:26:45 @agent_ppo2.py:192][0m |          -0.0254 |          13.6514 |           0.0469 |
[32m[20230207 13:26:45 @agent_ppo2.py:192][0m |          -0.0306 |          13.1802 |           0.0469 |
[32m[20230207 13:26:45 @agent_ppo2.py:192][0m |          -0.0335 |          13.1826 |           0.0469 |
[32m[20230207 13:26:45 @agent_ppo2.py:192][0m |           0.0062 |          12.7490 |           0.0464 |
[32m[20230207 13:26:45 @agent_ppo2.py:192][0m |          -0.0487 |          13.3981 |           0.0465 |
[32m[20230207 13:26:45 @agent_ppo2.py:192][0m |          -0.0092 |          12.0684 |           0.0467 |
[32m[20230207 13:26:45 @agent_ppo2.py:192][0m |          -0.0307 |          11.5620 |           0.0467 |
[32m[20230207 13:26:45 @agent_ppo2.py:137][0m Policy update time: 0.68 s
[32m[20230207 13:26:46 @agent_ppo2.py:145][0m Average TRAINING episode reward: -157.54
[32m[20230207 13:26:46 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.59
[32m[20230207 13:26:46 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -160.41
[32m[20230207 13:26:46 @agent_ppo2.py:150][0m Total time:       0.53 min
[32m[20230207 13:26:46 @agent_ppo2.py:152][0m 30720 total steps have happened
[32m[20230207 13:26:46 @agent_ppo2.py:128][0m #------------------------ Iteration 15 --------------------------#
[32m[20230207 13:26:46 @agent_ppo2.py:134][0m Sampling time: 0.61 s by 1 slaves
[32m[20230207 13:26:46 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:46 @agent_ppo2.py:192][0m |           0.0031 |          74.4383 |           0.0478 |
[32m[20230207 13:26:46 @agent_ppo2.py:192][0m |          -0.0120 |          61.3067 |           0.0478 |
[32m[20230207 13:26:47 @agent_ppo2.py:192][0m |          -0.0181 |          56.3957 |           0.0477 |
[32m[20230207 13:26:47 @agent_ppo2.py:192][0m |          -0.0194 |          52.8732 |           0.0477 |
[32m[20230207 13:26:47 @agent_ppo2.py:192][0m |          -0.0201 |          48.6262 |           0.0477 |
[32m[20230207 13:26:47 @agent_ppo2.py:192][0m |          -0.0190 |          45.2899 |           0.0476 |
[32m[20230207 13:26:47 @agent_ppo2.py:192][0m |          -0.0222 |          42.5519 |           0.0476 |
[32m[20230207 13:26:47 @agent_ppo2.py:192][0m |          -0.0232 |          39.7095 |           0.0475 |
[32m[20230207 13:26:47 @agent_ppo2.py:192][0m |          -0.0280 |          38.0941 |           0.0475 |
[32m[20230207 13:26:47 @agent_ppo2.py:192][0m |          -0.0274 |          36.3781 |           0.0476 |
[32m[20230207 13:26:47 @agent_ppo2.py:137][0m Policy update time: 0.55 s
[32m[20230207 13:26:47 @agent_ppo2.py:145][0m Average TRAINING episode reward: -123.78
[32m[20230207 13:26:47 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -103.18
[32m[20230207 13:26:47 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -111.66
[32m[20230207 13:26:47 @agent_ppo2.py:150][0m Total time:       0.55 min
[32m[20230207 13:26:47 @agent_ppo2.py:152][0m 32768 total steps have happened
[32m[20230207 13:26:47 @agent_ppo2.py:128][0m #------------------------ Iteration 16 --------------------------#
[32m[20230207 13:26:48 @agent_ppo2.py:134][0m Sampling time: 0.55 s by 1 slaves
[32m[20230207 13:26:48 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230207 13:26:48 @agent_ppo2.py:192][0m |          -0.0020 |          35.1768 |           0.0469 |
[32m[20230207 13:26:48 @agent_ppo2.py:192][0m |          -0.0095 |          23.1121 |           0.0469 |
[32m[20230207 13:26:48 @agent_ppo2.py:192][0m |          -0.0143 |          20.6709 |           0.0468 |
[32m[20230207 13:26:48 @agent_ppo2.py:192][0m |          -0.0175 |          18.8963 |           0.0467 |
[32m[20230207 13:26:48 @agent_ppo2.py:192][0m |          -0.0221 |          17.5682 |           0.0469 |
[32m[20230207 13:26:48 @agent_ppo2.py:192][0m |          -0.0246 |          16.5971 |           0.0468 |
[32m[20230207 13:26:48 @agent_ppo2.py:192][0m |          -0.0265 |          15.8220 |           0.0467 |
