[32m[20230209 04:19:28 @logger.py:106][0m Log file set to ./tmp/evo_bipedalwalker/easy/20230209_041928/log/evo_bipedalwalker_easy-20230209_041928.log
[32m[20230209 04:19:28 @agent_ppo2.py:128][0m #------------------------ Iteration 0 --------------------------#
[32m[20230209 04:19:29 @agent_ppo2.py:134][0m Sampling time: 0.78 s by 1 slaves
[32m[20230209 04:19:29 @agent_ppo2.py:168][0m |      policy_loss |       value_loss |          entropy |
[32m[20230209 04:19:29 @agent_ppo2.py:193][0m |           0.0283 |         314.2424 |           0.0639 |
[32m[20230209 04:19:29 @agent_ppo2.py:193][0m |           0.0060 |         281.6431 |           0.0639 |
[32m[20230209 04:19:29 @agent_ppo2.py:193][0m |          -0.0028 |         262.7964 |           0.0639 |
[32m[20230209 04:19:29 @agent_ppo2.py:193][0m |          -0.0088 |         248.6097 |           0.0640 |
[32m[20230209 04:19:29 @agent_ppo2.py:193][0m |          -0.0081 |         239.0698 |           0.0640 |
[32m[20230209 04:19:29 @agent_ppo2.py:193][0m |          -0.0048 |         231.4328 |           0.0640 |
[32m[20230209 04:19:29 @agent_ppo2.py:193][0m |          -0.0099 |         224.3792 |           0.0640 |
[32m[20230209 04:19:29 @agent_ppo2.py:193][0m |          -0.0107 |         218.1703 |           0.0640 |
[32m[20230209 04:19:29 @agent_ppo2.py:193][0m |          -0.0106 |         212.7839 |           0.0640 |
[32m[20230209 04:19:29 @agent_ppo2.py:193][0m |          -0.0128 |         207.9158 |           0.0641 |
[32m[20230209 04:19:29 @agent_ppo2.py:137][0m Policy update time: 0.80 s
[32m[20230209 04:19:30 @agent_ppo2.py:145][0m Average TRAINING episode reward: -116.42
[32m[20230209 04:19:30 @agent_ppo2.py:146][0m Maximum TRAINING episode reward: -98.73
[32m[20230209 04:19:30 @agent_ppo2.py:147][0m Average EVALUATION episode reward: -92.91
[32m[20230209 04:19:30 @evo_bipedalwalker_agent.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -92.91
[32m[20230209 04:19:30 @evo_bipedalwalker_agent.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -92.91
[32m[20230209 04:19:30 @agent_ppo2.py:150][0m Total time:       0.04 min
[32m[20230209 04:19:30 @agent_ppo2.py:152][0m 2048 total steps have happened
[32m[20230209 04:19:30 @agent_ppo2.py:128][0m #------------------------ Iteration 1 --------------------------#
