[32m[20230113 19:44:33 @logger.py:105][0m Log file set to ./tmp/bipedalwalker/easy/20230113_194433/log/bipedalwalker_easy-20230113_194433.log
[32m[20230113 19:44:33 @agent_ppo2.py:122][0m #------------------------ Iteration 0 --------------------------#
[32m[20230113 19:44:33 @agent_ppo2.py:128][0m Sampling time: 0.56 s by 1 slaves
[32m[20230113 19:44:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:33 @agent_ppo2.py:186][0m |           0.0142 |          70.1775 |           1.8194 |
[32m[20230113 19:44:33 @agent_ppo2.py:186][0m |          -0.0036 |          59.6496 |           1.8191 |
[32m[20230113 19:44:33 @agent_ppo2.py:186][0m |          -0.0044 |          57.3377 |           1.8190 |
[32m[20230113 19:44:33 @agent_ppo2.py:186][0m |          -0.0046 |          56.1084 |           1.8190 |
[32m[20230113 19:44:33 @agent_ppo2.py:186][0m |          -0.0048 |          55.1936 |           1.8188 |
[32m[20230113 19:44:34 @agent_ppo2.py:186][0m |          -0.0055 |          54.3322 |           1.8184 |
[32m[20230113 19:44:34 @agent_ppo2.py:186][0m |           0.0132 |          68.0317 |           1.8181 |
[32m[20230113 19:44:34 @agent_ppo2.py:186][0m |          -0.0057 |          52.4398 |           1.8181 |
[32m[20230113 19:44:34 @agent_ppo2.py:186][0m |          -0.0047 |          51.7002 |           1.8176 |
[32m[20230113 19:44:34 @agent_ppo2.py:186][0m |          -0.0089 |          50.6788 |           1.8170 |
[32m[20230113 19:44:34 @agent_ppo2.py:131][0m Policy update time: 0.57 s
[32m[20230113 19:44:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: -113.80
[32m[20230113 19:44:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -102.16
[32m[20230113 19:44:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -92.57
[32m[20230113 19:44:34 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -92.57
[32m[20230113 19:44:34 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -92.57
[32m[20230113 19:44:34 @agent_ppo2.py:144][0m Total time:       0.02 min
[32m[20230113 19:44:34 @agent_ppo2.py:146][0m 2048 total steps have happened
[32m[20230113 19:44:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1 --------------------------#
[32m[20230113 19:44:35 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:44:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:35 @agent_ppo2.py:186][0m |          -0.0017 |          53.9133 |           1.8829 |
[32m[20230113 19:44:35 @agent_ppo2.py:186][0m |          -0.0031 |          50.0529 |           1.8827 |
[32m[20230113 19:44:35 @agent_ppo2.py:186][0m |           0.0024 |          51.4674 |           1.8825 |
[32m[20230113 19:44:35 @agent_ppo2.py:186][0m |          -0.0037 |          48.3941 |           1.8821 |
[32m[20230113 19:44:35 @agent_ppo2.py:186][0m |           0.0006 |          48.8080 |           1.8820 |
[32m[20230113 19:44:35 @agent_ppo2.py:186][0m |          -0.0045 |          47.1639 |           1.8819 |
[32m[20230113 19:44:35 @agent_ppo2.py:186][0m |          -0.0058 |          46.5085 |           1.8814 |
[32m[20230113 19:44:35 @agent_ppo2.py:186][0m |          -0.0061 |          45.8425 |           1.8815 |
[32m[20230113 19:44:35 @agent_ppo2.py:186][0m |          -0.0070 |          45.2209 |           1.8812 |
[32m[20230113 19:44:35 @agent_ppo2.py:186][0m |          -0.0033 |          46.4456 |           1.8808 |
[32m[20230113 19:44:35 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 19:44:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: -112.12
[32m[20230113 19:44:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -99.98
[32m[20230113 19:44:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -92.31
[32m[20230113 19:44:36 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -92.31
[32m[20230113 19:44:36 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -92.31
[32m[20230113 19:44:36 @agent_ppo2.py:144][0m Total time:       0.05 min
[32m[20230113 19:44:36 @agent_ppo2.py:146][0m 4096 total steps have happened
[32m[20230113 19:44:36 @agent_ppo2.py:122][0m #------------------------ Iteration 2 --------------------------#
[32m[20230113 19:44:36 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:44:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:36 @agent_ppo2.py:186][0m |          -0.0005 |          25.3314 |           1.8693 |
[32m[20230113 19:44:36 @agent_ppo2.py:186][0m |          -0.0027 |          23.8002 |           1.8697 |
[32m[20230113 19:44:36 @agent_ppo2.py:186][0m |           0.0009 |          23.8063 |           1.8699 |
[32m[20230113 19:44:36 @agent_ppo2.py:186][0m |          -0.0037 |          22.5893 |           1.8706 |
[32m[20230113 19:44:36 @agent_ppo2.py:186][0m |          -0.0023 |          22.1530 |           1.8710 |
[32m[20230113 19:44:36 @agent_ppo2.py:186][0m |          -0.0045 |          21.6697 |           1.8714 |
[32m[20230113 19:44:36 @agent_ppo2.py:186][0m |          -0.0056 |          21.2502 |           1.8717 |
[32m[20230113 19:44:36 @agent_ppo2.py:186][0m |          -0.0046 |          21.2102 |           1.8716 |
[32m[20230113 19:44:37 @agent_ppo2.py:186][0m |          -0.0023 |          21.2478 |           1.8714 |
[32m[20230113 19:44:37 @agent_ppo2.py:186][0m |          -0.0071 |          20.1049 |           1.8713 |
[32m[20230113 19:44:37 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:44:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: -110.62
[32m[20230113 19:44:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -107.99
[32m[20230113 19:44:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -92.46
[32m[20230113 19:44:37 @agent_ppo2.py:144][0m Total time:       0.07 min
[32m[20230113 19:44:37 @agent_ppo2.py:146][0m 6144 total steps have happened
[32m[20230113 19:44:37 @agent_ppo2.py:122][0m #------------------------ Iteration 3 --------------------------#
[32m[20230113 19:44:37 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 19:44:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:37 @agent_ppo2.py:186][0m |          -0.0003 |         117.2919 |           1.8777 |
[32m[20230113 19:44:37 @agent_ppo2.py:186][0m |          -0.0001 |          93.8644 |           1.8778 |
[32m[20230113 19:44:37 @agent_ppo2.py:186][0m |          -0.0017 |          85.1706 |           1.8777 |
[32m[20230113 19:44:37 @agent_ppo2.py:186][0m |          -0.0044 |          80.1964 |           1.8775 |
[32m[20230113 19:44:37 @agent_ppo2.py:186][0m |          -0.0015 |          78.4103 |           1.8774 |
[32m[20230113 19:44:37 @agent_ppo2.py:186][0m |          -0.0055 |          73.7198 |           1.8770 |
[32m[20230113 19:44:38 @agent_ppo2.py:186][0m |          -0.0079 |          70.7844 |           1.8765 |
[32m[20230113 19:44:38 @agent_ppo2.py:186][0m |          -0.0066 |          69.7171 |           1.8760 |
[32m[20230113 19:44:38 @agent_ppo2.py:186][0m |          -0.0091 |          66.4307 |           1.8753 |
[32m[20230113 19:44:38 @agent_ppo2.py:186][0m |          -0.0088 |          64.4681 |           1.8751 |
[32m[20230113 19:44:38 @agent_ppo2.py:131][0m Policy update time: 0.34 s
[32m[20230113 19:44:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: -117.52
[32m[20230113 19:44:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -103.38
[32m[20230113 19:44:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -91.74
[32m[20230113 19:44:38 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -91.74
[32m[20230113 19:44:38 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -91.74
[32m[20230113 19:44:38 @agent_ppo2.py:144][0m Total time:       0.09 min
[32m[20230113 19:44:38 @agent_ppo2.py:146][0m 8192 total steps have happened
[32m[20230113 19:44:38 @agent_ppo2.py:122][0m #------------------------ Iteration 4 --------------------------#
[32m[20230113 19:44:39 @agent_ppo2.py:128][0m Sampling time: 0.57 s by 1 slaves
[32m[20230113 19:44:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:39 @agent_ppo2.py:186][0m |           0.0002 |          54.1961 |           1.8641 |
[32m[20230113 19:44:39 @agent_ppo2.py:186][0m |          -0.0015 |          49.6032 |           1.8638 |
[32m[20230113 19:44:39 @agent_ppo2.py:186][0m |          -0.0041 |          47.4360 |           1.8642 |
[32m[20230113 19:44:39 @agent_ppo2.py:186][0m |          -0.0066 |          46.0922 |           1.8638 |
[32m[20230113 19:44:39 @agent_ppo2.py:186][0m |          -0.0086 |          44.9237 |           1.8634 |
[32m[20230113 19:44:39 @agent_ppo2.py:186][0m |          -0.0089 |          44.2897 |           1.8631 |
[32m[20230113 19:44:39 @agent_ppo2.py:186][0m |          -0.0098 |          42.9729 |           1.8626 |
[32m[20230113 19:44:39 @agent_ppo2.py:186][0m |          -0.0118 |          41.8389 |           1.8624 |
[32m[20230113 19:44:39 @agent_ppo2.py:186][0m |          -0.0101 |          41.5217 |           1.8619 |
[32m[20230113 19:44:39 @agent_ppo2.py:186][0m |          -0.0109 |          39.9426 |           1.8617 |
[32m[20230113 19:44:39 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 19:44:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: -111.35
[32m[20230113 19:44:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -99.46
[32m[20230113 19:44:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -92.93
[32m[20230113 19:44:39 @agent_ppo2.py:144][0m Total time:       0.11 min
[32m[20230113 19:44:39 @agent_ppo2.py:146][0m 10240 total steps have happened
[32m[20230113 19:44:39 @agent_ppo2.py:122][0m #------------------------ Iteration 5 --------------------------#
[32m[20230113 19:44:40 @agent_ppo2.py:128][0m Sampling time: 0.57 s by 1 slaves
[32m[20230113 19:44:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:40 @agent_ppo2.py:186][0m |          -0.0001 |          54.2558 |           1.9005 |
[32m[20230113 19:44:40 @agent_ppo2.py:186][0m |          -0.0014 |          50.7612 |           1.9008 |
[32m[20230113 19:44:40 @agent_ppo2.py:186][0m |          -0.0028 |          48.5040 |           1.9007 |
[32m[20230113 19:44:40 @agent_ppo2.py:186][0m |          -0.0042 |          46.4027 |           1.9002 |
[32m[20230113 19:44:40 @agent_ppo2.py:186][0m |          -0.0055 |          44.5431 |           1.8999 |
[32m[20230113 19:44:40 @agent_ppo2.py:186][0m |          -0.0061 |          42.7801 |           1.8986 |
[32m[20230113 19:44:40 @agent_ppo2.py:186][0m |          -0.0065 |          41.1791 |           1.8982 |
[32m[20230113 19:44:40 @agent_ppo2.py:186][0m |          -0.0069 |          39.6621 |           1.8979 |
[32m[20230113 19:44:40 @agent_ppo2.py:186][0m |          -0.0075 |          38.1819 |           1.8969 |
[32m[20230113 19:44:41 @agent_ppo2.py:186][0m |          -0.0077 |          36.8601 |           1.8961 |
[32m[20230113 19:44:41 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 19:44:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: -102.77
[32m[20230113 19:44:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -98.17
[32m[20230113 19:44:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -97.80
[32m[20230113 19:44:41 @agent_ppo2.py:144][0m Total time:       0.14 min
[32m[20230113 19:44:41 @agent_ppo2.py:146][0m 12288 total steps have happened
[32m[20230113 19:44:41 @agent_ppo2.py:122][0m #------------------------ Iteration 6 --------------------------#
[32m[20230113 19:44:41 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 19:44:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:41 @agent_ppo2.py:186][0m |           0.0126 |         118.0698 |           1.8468 |
[32m[20230113 19:44:41 @agent_ppo2.py:186][0m |          -0.0098 |          94.4358 |           1.8458 |
[32m[20230113 19:44:41 @agent_ppo2.py:186][0m |           0.0207 |          94.7651 |           1.8451 |
[32m[20230113 19:44:41 @agent_ppo2.py:186][0m |           0.0017 |          86.7113 |           1.8439 |
[32m[20230113 19:44:41 @agent_ppo2.py:186][0m |          -0.0104 |          77.6117 |           1.8425 |
[32m[20230113 19:44:41 @agent_ppo2.py:186][0m |          -0.0113 |          74.0281 |           1.8413 |
[32m[20230113 19:44:41 @agent_ppo2.py:186][0m |          -0.0142 |          70.7132 |           1.8399 |
[32m[20230113 19:44:41 @agent_ppo2.py:186][0m |           0.0097 |          74.8925 |           1.8392 |
[32m[20230113 19:44:42 @agent_ppo2.py:186][0m |           0.0029 |          71.1873 |           1.8382 |
[32m[20230113 19:44:42 @agent_ppo2.py:186][0m |          -0.0180 |          62.3449 |           1.8370 |
[32m[20230113 19:44:42 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 19:44:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: -108.45
[32m[20230113 19:44:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -102.64
[32m[20230113 19:44:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -92.43
[32m[20230113 19:44:42 @agent_ppo2.py:144][0m Total time:       0.15 min
[32m[20230113 19:44:42 @agent_ppo2.py:146][0m 14336 total steps have happened
[32m[20230113 19:44:42 @agent_ppo2.py:122][0m #------------------------ Iteration 7 --------------------------#
[32m[20230113 19:44:42 @agent_ppo2.py:128][0m Sampling time: 0.55 s by 1 slaves
[32m[20230113 19:44:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:43 @agent_ppo2.py:186][0m |          -0.0023 |          26.0787 |           1.8766 |
[32m[20230113 19:44:43 @agent_ppo2.py:186][0m |          -0.0070 |          23.9251 |           1.8767 |
[32m[20230113 19:44:43 @agent_ppo2.py:186][0m |           0.0006 |          24.9455 |           1.8768 |
[32m[20230113 19:44:43 @agent_ppo2.py:186][0m |          -0.0056 |          22.6961 |           1.8766 |
[32m[20230113 19:44:43 @agent_ppo2.py:186][0m |          -0.0107 |          22.1840 |           1.8767 |
[32m[20230113 19:44:43 @agent_ppo2.py:186][0m |          -0.0062 |          21.6926 |           1.8766 |
[32m[20230113 19:44:43 @agent_ppo2.py:186][0m |          -0.0069 |          21.5437 |           1.8760 |
[32m[20230113 19:44:43 @agent_ppo2.py:186][0m |          -0.0091 |          20.7487 |           1.8754 |
[32m[20230113 19:44:43 @agent_ppo2.py:186][0m |          -0.0109 |          20.3008 |           1.8750 |
[32m[20230113 19:44:43 @agent_ppo2.py:186][0m |          -0.0094 |          19.9050 |           1.8747 |
[32m[20230113 19:44:43 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 19:44:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: -98.58
[32m[20230113 19:44:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -94.48
[32m[20230113 19:44:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -93.96
[32m[20230113 19:44:43 @agent_ppo2.py:144][0m Total time:       0.18 min
[32m[20230113 19:44:43 @agent_ppo2.py:146][0m 16384 total steps have happened
[32m[20230113 19:44:43 @agent_ppo2.py:122][0m #------------------------ Iteration 8 --------------------------#
[32m[20230113 19:44:44 @agent_ppo2.py:128][0m Sampling time: 0.57 s by 1 slaves
[32m[20230113 19:44:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:44 @agent_ppo2.py:186][0m |          -0.0022 |          35.7648 |           1.8958 |
[32m[20230113 19:44:44 @agent_ppo2.py:186][0m |           0.0007 |          33.9515 |           1.8957 |
[32m[20230113 19:44:44 @agent_ppo2.py:186][0m |          -0.0069 |          31.3143 |           1.8957 |
[32m[20230113 19:44:44 @agent_ppo2.py:186][0m |          -0.0066 |          30.1872 |           1.8958 |
[32m[20230113 19:44:44 @agent_ppo2.py:186][0m |          -0.0108 |          29.0893 |           1.8954 |
[32m[20230113 19:44:44 @agent_ppo2.py:186][0m |          -0.0075 |          28.2282 |           1.8953 |
[32m[20230113 19:44:44 @agent_ppo2.py:186][0m |          -0.0056 |          27.3488 |           1.8946 |
[32m[20230113 19:44:44 @agent_ppo2.py:186][0m |          -0.0009 |          27.8284 |           1.8945 |
[32m[20230113 19:44:44 @agent_ppo2.py:186][0m |          -0.0052 |          25.5703 |           1.8941 |
[32m[20230113 19:44:44 @agent_ppo2.py:186][0m |          -0.0096 |          24.5209 |           1.8938 |
[32m[20230113 19:44:44 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 19:44:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: -108.39
[32m[20230113 19:44:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -106.71
[32m[20230113 19:44:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -93.86
[32m[20230113 19:44:45 @agent_ppo2.py:144][0m Total time:       0.20 min
[32m[20230113 19:44:45 @agent_ppo2.py:146][0m 18432 total steps have happened
[32m[20230113 19:44:45 @agent_ppo2.py:122][0m #------------------------ Iteration 9 --------------------------#
[32m[20230113 19:44:45 @agent_ppo2.py:128][0m Sampling time: 0.59 s by 1 slaves
[32m[20230113 19:44:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:45 @agent_ppo2.py:186][0m |           0.0009 |          34.2004 |           1.8746 |
[32m[20230113 19:44:45 @agent_ppo2.py:186][0m |          -0.0016 |          28.9249 |           1.8741 |
[32m[20230113 19:44:46 @agent_ppo2.py:186][0m |          -0.0036 |          26.3606 |           1.8735 |
[32m[20230113 19:44:46 @agent_ppo2.py:186][0m |          -0.0049 |          24.5691 |           1.8729 |
[32m[20230113 19:44:46 @agent_ppo2.py:186][0m |          -0.0061 |          23.3665 |           1.8722 |
[32m[20230113 19:44:46 @agent_ppo2.py:186][0m |          -0.0068 |          22.2375 |           1.8715 |
[32m[20230113 19:44:46 @agent_ppo2.py:186][0m |          -0.0098 |          21.1972 |           1.8709 |
[32m[20230113 19:44:46 @agent_ppo2.py:186][0m |          -0.0092 |          20.5189 |           1.8703 |
[32m[20230113 19:44:46 @agent_ppo2.py:186][0m |          -0.0107 |          19.6189 |           1.8698 |
[32m[20230113 19:44:46 @agent_ppo2.py:186][0m |          -0.0113 |          18.7769 |           1.8688 |
[32m[20230113 19:44:46 @agent_ppo2.py:131][0m Policy update time: 0.57 s
[32m[20230113 19:44:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: -108.18
[32m[20230113 19:44:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -99.45
[32m[20230113 19:44:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -10.52
[32m[20230113 19:44:46 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: -10.52
[32m[20230113 19:44:46 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -10.52
[32m[20230113 19:44:46 @agent_ppo2.py:144][0m Total time:       0.23 min
[32m[20230113 19:44:46 @agent_ppo2.py:146][0m 20480 total steps have happened
[32m[20230113 19:44:46 @agent_ppo2.py:122][0m #------------------------ Iteration 10 --------------------------#
[32m[20230113 19:44:47 @agent_ppo2.py:128][0m Sampling time: 0.55 s by 1 slaves
[32m[20230113 19:44:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:47 @agent_ppo2.py:186][0m |           0.0003 |          43.0453 |           1.8707 |
[32m[20230113 19:44:47 @agent_ppo2.py:186][0m |          -0.0009 |          32.5245 |           1.8704 |
[32m[20230113 19:44:47 @agent_ppo2.py:186][0m |          -0.0013 |          30.2084 |           1.8703 |
[32m[20230113 19:44:47 @agent_ppo2.py:186][0m |          -0.0037 |          28.4424 |           1.8701 |
[32m[20230113 19:44:47 @agent_ppo2.py:186][0m |          -0.0038 |          27.4919 |           1.8698 |
[32m[20230113 19:44:47 @agent_ppo2.py:186][0m |          -0.0048 |          26.6943 |           1.8694 |
[32m[20230113 19:44:47 @agent_ppo2.py:186][0m |          -0.0048 |          26.3549 |           1.8691 |
[32m[20230113 19:44:47 @agent_ppo2.py:186][0m |          -0.0051 |          25.7064 |           1.8694 |
[32m[20230113 19:44:47 @agent_ppo2.py:186][0m |          -0.0068 |          24.7869 |           1.8692 |
[32m[20230113 19:44:48 @agent_ppo2.py:186][0m |          -0.0064 |          24.5045 |           1.8692 |
[32m[20230113 19:44:48 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 19:44:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: -113.06
[32m[20230113 19:44:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -106.64
[32m[20230113 19:44:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -96.87
[32m[20230113 19:44:48 @agent_ppo2.py:144][0m Total time:       0.25 min
[32m[20230113 19:44:48 @agent_ppo2.py:146][0m 22528 total steps have happened
[32m[20230113 19:44:48 @agent_ppo2.py:122][0m #------------------------ Iteration 11 --------------------------#
[32m[20230113 19:44:48 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:44:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:48 @agent_ppo2.py:186][0m |           0.0001 |           5.1946 |           1.8808 |
[32m[20230113 19:44:49 @agent_ppo2.py:186][0m |          -0.0011 |           4.2063 |           1.8815 |
[32m[20230113 19:44:49 @agent_ppo2.py:186][0m |          -0.0023 |           3.9185 |           1.8816 |
[32m[20230113 19:44:49 @agent_ppo2.py:186][0m |          -0.0035 |           3.7385 |           1.8815 |
[32m[20230113 19:44:49 @agent_ppo2.py:186][0m |          -0.0045 |           3.6483 |           1.8810 |
[32m[20230113 19:44:49 @agent_ppo2.py:186][0m |          -0.0056 |           3.5518 |           1.8807 |
[32m[20230113 19:44:49 @agent_ppo2.py:186][0m |          -0.0064 |           3.5038 |           1.8801 |
[32m[20230113 19:44:49 @agent_ppo2.py:186][0m |          -0.0070 |           3.4243 |           1.8800 |
[32m[20230113 19:44:49 @agent_ppo2.py:186][0m |          -0.0076 |           3.3790 |           1.8800 |
[32m[20230113 19:44:49 @agent_ppo2.py:186][0m |          -0.0079 |           3.3389 |           1.8802 |
[32m[20230113 19:44:49 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:44:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: -104.23
[32m[20230113 19:44:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -101.50
[32m[20230113 19:44:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -27.67
[32m[20230113 19:44:49 @agent_ppo2.py:144][0m Total time:       0.28 min
[32m[20230113 19:44:49 @agent_ppo2.py:146][0m 24576 total steps have happened
[32m[20230113 19:44:49 @agent_ppo2.py:122][0m #------------------------ Iteration 12 --------------------------#
[32m[20230113 19:44:50 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:44:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:50 @agent_ppo2.py:186][0m |          -0.0001 |           5.1482 |           1.9319 |
[32m[20230113 19:44:50 @agent_ppo2.py:186][0m |          -0.0010 |           3.5629 |           1.9325 |
[32m[20230113 19:44:50 @agent_ppo2.py:186][0m |          -0.0020 |           3.2933 |           1.9331 |
[32m[20230113 19:44:50 @agent_ppo2.py:186][0m |          -0.0029 |           3.1706 |           1.9336 |
[32m[20230113 19:44:50 @agent_ppo2.py:186][0m |          -0.0038 |           3.1018 |           1.9338 |
[32m[20230113 19:44:50 @agent_ppo2.py:186][0m |          -0.0048 |           3.0655 |           1.9342 |
[32m[20230113 19:44:50 @agent_ppo2.py:186][0m |          -0.0056 |           3.0326 |           1.9347 |
[32m[20230113 19:44:50 @agent_ppo2.py:186][0m |          -0.0063 |           2.9987 |           1.9348 |
[32m[20230113 19:44:50 @agent_ppo2.py:186][0m |          -0.0069 |           2.9748 |           1.9350 |
[32m[20230113 19:44:50 @agent_ppo2.py:186][0m |          -0.0077 |           2.9505 |           1.9356 |
[32m[20230113 19:44:50 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:44:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: -94.93
[32m[20230113 19:44:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -91.94
[32m[20230113 19:44:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -112.34
[32m[20230113 19:44:51 @agent_ppo2.py:144][0m Total time:       0.31 min
[32m[20230113 19:44:51 @agent_ppo2.py:146][0m 26624 total steps have happened
[32m[20230113 19:44:51 @agent_ppo2.py:122][0m #------------------------ Iteration 13 --------------------------#
[32m[20230113 19:44:51 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:44:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:52 @agent_ppo2.py:186][0m |          -0.0003 |           3.3134 |           1.9014 |
[32m[20230113 19:44:52 @agent_ppo2.py:186][0m |          -0.0025 |           2.8540 |           1.9019 |
[32m[20230113 19:44:52 @agent_ppo2.py:186][0m |          -0.0046 |           2.7366 |           1.9018 |
[32m[20230113 19:44:52 @agent_ppo2.py:186][0m |          -0.0061 |           2.6481 |           1.9013 |
[32m[20230113 19:44:52 @agent_ppo2.py:186][0m |          -0.0069 |           2.5824 |           1.9017 |
[32m[20230113 19:44:52 @agent_ppo2.py:186][0m |          -0.0074 |           2.6075 |           1.9021 |
[32m[20230113 19:44:52 @agent_ppo2.py:186][0m |          -0.0081 |           2.5247 |           1.9025 |
[32m[20230113 19:44:52 @agent_ppo2.py:186][0m |          -0.0085 |           2.4903 |           1.9029 |
[32m[20230113 19:44:52 @agent_ppo2.py:186][0m |          -0.0090 |           2.4598 |           1.9034 |
[32m[20230113 19:44:52 @agent_ppo2.py:186][0m |          -0.0093 |           2.4211 |           1.9040 |
[32m[20230113 19:44:52 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:44:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: -101.37
[32m[20230113 19:44:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -98.39
[32m[20230113 19:44:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -36.21
[32m[20230113 19:44:52 @agent_ppo2.py:144][0m Total time:       0.33 min
[32m[20230113 19:44:52 @agent_ppo2.py:146][0m 28672 total steps have happened
[32m[20230113 19:44:52 @agent_ppo2.py:122][0m #------------------------ Iteration 14 --------------------------#
[32m[20230113 19:44:53 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:44:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:53 @agent_ppo2.py:186][0m |           0.0004 |           7.9909 |           1.9422 |
[32m[20230113 19:44:53 @agent_ppo2.py:186][0m |          -0.0008 |           5.6086 |           1.9426 |
[32m[20230113 19:44:53 @agent_ppo2.py:186][0m |          -0.0025 |           5.1854 |           1.9433 |
[32m[20230113 19:44:53 @agent_ppo2.py:186][0m |          -0.0035 |           5.0145 |           1.9437 |
[32m[20230113 19:44:53 @agent_ppo2.py:186][0m |          -0.0045 |           4.8966 |           1.9435 |
[32m[20230113 19:44:53 @agent_ppo2.py:186][0m |          -0.0054 |           4.7591 |           1.9436 |
[32m[20230113 19:44:53 @agent_ppo2.py:186][0m |          -0.0059 |           4.5967 |           1.9435 |
[32m[20230113 19:44:53 @agent_ppo2.py:186][0m |          -0.0065 |           4.5098 |           1.9436 |
[32m[20230113 19:44:53 @agent_ppo2.py:186][0m |          -0.0066 |           4.4235 |           1.9434 |
[32m[20230113 19:44:53 @agent_ppo2.py:186][0m |          -0.0072 |           4.3077 |           1.9437 |
[32m[20230113 19:44:53 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:44:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: -95.73
[32m[20230113 19:44:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -89.16
[32m[20230113 19:44:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -121.31
[32m[20230113 19:44:54 @agent_ppo2.py:144][0m Total time:       0.35 min
[32m[20230113 19:44:54 @agent_ppo2.py:146][0m 30720 total steps have happened
[32m[20230113 19:44:54 @agent_ppo2.py:122][0m #------------------------ Iteration 15 --------------------------#
[32m[20230113 19:44:54 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:44:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:54 @agent_ppo2.py:186][0m |          -0.0001 |           1.6327 |           1.9593 |
[32m[20230113 19:44:54 @agent_ppo2.py:186][0m |          -0.0015 |           1.3655 |           1.9591 |
[32m[20230113 19:44:55 @agent_ppo2.py:186][0m |          -0.0032 |           1.3256 |           1.9592 |
[32m[20230113 19:44:55 @agent_ppo2.py:186][0m |          -0.0043 |           1.2985 |           1.9591 |
[32m[20230113 19:44:55 @agent_ppo2.py:186][0m |          -0.0054 |           1.2814 |           1.9586 |
[32m[20230113 19:44:55 @agent_ppo2.py:186][0m |          -0.0062 |           1.2692 |           1.9587 |
[32m[20230113 19:44:55 @agent_ppo2.py:186][0m |          -0.0068 |           1.2564 |           1.9583 |
[32m[20230113 19:44:55 @agent_ppo2.py:186][0m |          -0.0072 |           1.2487 |           1.9589 |
[32m[20230113 19:44:55 @agent_ppo2.py:186][0m |          -0.0077 |           1.2392 |           1.9594 |
[32m[20230113 19:44:55 @agent_ppo2.py:186][0m |          -0.0082 |           1.2357 |           1.9596 |
[32m[20230113 19:44:55 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:44:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: -90.30
[32m[20230113 19:44:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -87.89
[32m[20230113 19:44:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -95.09
[32m[20230113 19:44:55 @agent_ppo2.py:144][0m Total time:       0.38 min
[32m[20230113 19:44:55 @agent_ppo2.py:146][0m 32768 total steps have happened
[32m[20230113 19:44:55 @agent_ppo2.py:122][0m #------------------------ Iteration 16 --------------------------#
[32m[20230113 19:44:56 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:44:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:56 @agent_ppo2.py:186][0m |           0.0004 |           8.1544 |           1.9400 |
[32m[20230113 19:44:56 @agent_ppo2.py:186][0m |          -0.0003 |           6.6466 |           1.9405 |
[32m[20230113 19:44:56 @agent_ppo2.py:186][0m |          -0.0028 |           6.3210 |           1.9403 |
[32m[20230113 19:44:56 @agent_ppo2.py:186][0m |          -0.0054 |           6.0507 |           1.9394 |
[32m[20230113 19:44:56 @agent_ppo2.py:186][0m |          -0.0053 |           5.8525 |           1.9385 |
[32m[20230113 19:44:56 @agent_ppo2.py:186][0m |           0.0055 |           5.7698 |           1.9381 |
[32m[20230113 19:44:56 @agent_ppo2.py:186][0m |          -0.0064 |           5.5383 |           1.9375 |
[32m[20230113 19:44:56 @agent_ppo2.py:186][0m |          -0.0293 |           5.6873 |           1.9367 |
[32m[20230113 19:44:56 @agent_ppo2.py:186][0m |           0.1427 |           5.3016 |           1.9362 |
[32m[20230113 19:44:56 @agent_ppo2.py:186][0m |          -0.0078 |           5.1901 |           1.9346 |
[32m[20230113 19:44:56 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:44:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: -103.23
[32m[20230113 19:44:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -95.29
[32m[20230113 19:44:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -76.15
[32m[20230113 19:44:57 @agent_ppo2.py:144][0m Total time:       0.40 min
[32m[20230113 19:44:57 @agent_ppo2.py:146][0m 34816 total steps have happened
[32m[20230113 19:44:57 @agent_ppo2.py:122][0m #------------------------ Iteration 17 --------------------------#
[32m[20230113 19:44:57 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:44:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:57 @agent_ppo2.py:186][0m |           0.0000 |           1.6448 |           1.9702 |
[32m[20230113 19:44:57 @agent_ppo2.py:186][0m |          -0.0007 |           1.4438 |           1.9706 |
[32m[20230113 19:44:58 @agent_ppo2.py:186][0m |          -0.0015 |           1.4200 |           1.9713 |
[32m[20230113 19:44:58 @agent_ppo2.py:186][0m |          -0.0022 |           1.4183 |           1.9715 |
[32m[20230113 19:44:58 @agent_ppo2.py:186][0m |          -0.0027 |           1.4135 |           1.9720 |
[32m[20230113 19:44:58 @agent_ppo2.py:186][0m |          -0.0031 |           1.3876 |           1.9722 |
[32m[20230113 19:44:58 @agent_ppo2.py:186][0m |          -0.0035 |           1.3897 |           1.9728 |
[32m[20230113 19:44:58 @agent_ppo2.py:186][0m |          -0.0039 |           1.3937 |           1.9730 |
[32m[20230113 19:44:58 @agent_ppo2.py:186][0m |          -0.0041 |           1.3613 |           1.9736 |
[32m[20230113 19:44:58 @agent_ppo2.py:186][0m |          -0.0044 |           1.3653 |           1.9740 |
[32m[20230113 19:44:58 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:44:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: -94.26
[32m[20230113 19:44:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -93.63
[32m[20230113 19:44:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -49.53
[32m[20230113 19:44:58 @agent_ppo2.py:144][0m Total time:       0.43 min
[32m[20230113 19:44:58 @agent_ppo2.py:146][0m 36864 total steps have happened
[32m[20230113 19:44:58 @agent_ppo2.py:122][0m #------------------------ Iteration 18 --------------------------#
[32m[20230113 19:44:59 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:44:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:44:59 @agent_ppo2.py:186][0m |          -0.0005 |           1.6765 |           1.9431 |
[32m[20230113 19:44:59 @agent_ppo2.py:186][0m |          -0.0027 |           1.1789 |           1.9428 |
[32m[20230113 19:44:59 @agent_ppo2.py:186][0m |          -0.0047 |           1.1504 |           1.9425 |
[32m[20230113 19:44:59 @agent_ppo2.py:186][0m |          -0.0062 |           1.1088 |           1.9420 |
[32m[20230113 19:44:59 @agent_ppo2.py:186][0m |          -0.0069 |           1.0926 |           1.9417 |
[32m[20230113 19:44:59 @agent_ppo2.py:186][0m |          -0.0075 |           1.0876 |           1.9419 |
[32m[20230113 19:44:59 @agent_ppo2.py:186][0m |          -0.0080 |           1.0817 |           1.9416 |
[32m[20230113 19:44:59 @agent_ppo2.py:186][0m |          -0.0085 |           1.0742 |           1.9414 |
[32m[20230113 19:44:59 @agent_ppo2.py:186][0m |          -0.0087 |           1.0677 |           1.9411 |
[32m[20230113 19:44:59 @agent_ppo2.py:186][0m |          -0.0091 |           1.0629 |           1.9412 |
[32m[20230113 19:44:59 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: -91.35
[32m[20230113 19:45:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -89.07
[32m[20230113 19:45:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -52.64
[32m[20230113 19:45:00 @agent_ppo2.py:144][0m Total time:       0.46 min
[32m[20230113 19:45:00 @agent_ppo2.py:146][0m 38912 total steps have happened
[32m[20230113 19:45:00 @agent_ppo2.py:122][0m #------------------------ Iteration 19 --------------------------#
[32m[20230113 19:45:01 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:45:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:01 @agent_ppo2.py:186][0m |           0.0045 |           5.0846 |           1.8835 |
[32m[20230113 19:45:01 @agent_ppo2.py:186][0m |          -0.0033 |           3.3596 |           1.8838 |
[32m[20230113 19:45:01 @agent_ppo2.py:186][0m |          -0.0035 |           3.0759 |           1.8841 |
[32m[20230113 19:45:01 @agent_ppo2.py:186][0m |          -0.0030 |           3.0811 |           1.8841 |
[32m[20230113 19:45:01 @agent_ppo2.py:186][0m |          -0.0068 |           2.8934 |           1.8839 |
[32m[20230113 19:45:01 @agent_ppo2.py:186][0m |          -0.0061 |           2.7716 |           1.8838 |
[32m[20230113 19:45:01 @agent_ppo2.py:186][0m |          -0.0053 |           2.7105 |           1.8834 |
[32m[20230113 19:45:01 @agent_ppo2.py:186][0m |          -0.0064 |           2.6940 |           1.8831 |
[32m[20230113 19:45:01 @agent_ppo2.py:186][0m |          -0.0044 |           2.7638 |           1.8833 |
[32m[20230113 19:45:01 @agent_ppo2.py:186][0m |          -0.0082 |           2.5588 |           1.8827 |
[32m[20230113 19:45:01 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 19:45:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: -100.74
[32m[20230113 19:45:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -85.43
[32m[20230113 19:45:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -62.15
[32m[20230113 19:45:02 @agent_ppo2.py:144][0m Total time:       0.48 min
[32m[20230113 19:45:02 @agent_ppo2.py:146][0m 40960 total steps have happened
[32m[20230113 19:45:02 @agent_ppo2.py:122][0m #------------------------ Iteration 20 --------------------------#
[32m[20230113 19:45:02 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:45:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:02 @agent_ppo2.py:186][0m |           0.0001 |           2.4909 |           1.9397 |
[32m[20230113 19:45:02 @agent_ppo2.py:186][0m |          -0.0013 |           1.5309 |           1.9399 |
[32m[20230113 19:45:02 @agent_ppo2.py:186][0m |          -0.0031 |           1.3663 |           1.9396 |
[32m[20230113 19:45:02 @agent_ppo2.py:186][0m |          -0.0042 |           1.3263 |           1.9394 |
[32m[20230113 19:45:02 @agent_ppo2.py:186][0m |          -0.0048 |           1.2580 |           1.9393 |
[32m[20230113 19:45:02 @agent_ppo2.py:186][0m |          -0.0052 |           1.2264 |           1.9388 |
[32m[20230113 19:45:03 @agent_ppo2.py:186][0m |          -0.0056 |           1.1941 |           1.9388 |
[32m[20230113 19:45:03 @agent_ppo2.py:186][0m |          -0.0060 |           1.1687 |           1.9387 |
[32m[20230113 19:45:03 @agent_ppo2.py:186][0m |          -0.0061 |           1.1518 |           1.9388 |
[32m[20230113 19:45:03 @agent_ppo2.py:186][0m |          -0.0065 |           1.1420 |           1.9386 |
[32m[20230113 19:45:03 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: -87.37
[32m[20230113 19:45:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -86.29
[32m[20230113 19:45:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -55.61
[32m[20230113 19:45:03 @agent_ppo2.py:144][0m Total time:       0.51 min
[32m[20230113 19:45:03 @agent_ppo2.py:146][0m 43008 total steps have happened
[32m[20230113 19:45:03 @agent_ppo2.py:122][0m #------------------------ Iteration 21 --------------------------#
[32m[20230113 19:45:04 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:45:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:04 @agent_ppo2.py:186][0m |           0.0030 |           6.1954 |           1.8974 |
[32m[20230113 19:45:04 @agent_ppo2.py:186][0m |          -0.0033 |           3.9199 |           1.8975 |
[32m[20230113 19:45:04 @agent_ppo2.py:186][0m |          -0.0050 |           3.6089 |           1.8971 |
[32m[20230113 19:45:04 @agent_ppo2.py:186][0m |          -0.0049 |           3.4320 |           1.8964 |
[32m[20230113 19:45:04 @agent_ppo2.py:186][0m |          -0.0076 |           3.3387 |           1.8961 |
[32m[20230113 19:45:04 @agent_ppo2.py:186][0m |          -0.0094 |           3.2785 |           1.8956 |
[32m[20230113 19:45:04 @agent_ppo2.py:186][0m |          -0.0050 |           3.3172 |           1.8949 |
[32m[20230113 19:45:04 @agent_ppo2.py:186][0m |          -0.0101 |           3.1622 |           1.8943 |
[32m[20230113 19:45:04 @agent_ppo2.py:186][0m |          -0.0108 |           2.9143 |           1.8938 |
[32m[20230113 19:45:04 @agent_ppo2.py:186][0m |          -0.0119 |           2.8500 |           1.8932 |
[32m[20230113 19:45:04 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 19:45:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: -102.07
[32m[20230113 19:45:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -93.90
[32m[20230113 19:45:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -50.27
[32m[20230113 19:45:05 @agent_ppo2.py:144][0m Total time:       0.54 min
[32m[20230113 19:45:05 @agent_ppo2.py:146][0m 45056 total steps have happened
[32m[20230113 19:45:05 @agent_ppo2.py:122][0m #------------------------ Iteration 22 --------------------------#
[32m[20230113 19:45:05 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:45:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:05 @agent_ppo2.py:186][0m |          -0.0004 |           8.2817 |           1.9868 |
[32m[20230113 19:45:05 @agent_ppo2.py:186][0m |          -0.0021 |           5.0019 |           1.9871 |
[32m[20230113 19:45:06 @agent_ppo2.py:186][0m |          -0.0036 |           4.2757 |           1.9879 |
[32m[20230113 19:45:06 @agent_ppo2.py:186][0m |          -0.0037 |           3.8357 |           1.9887 |
[32m[20230113 19:45:06 @agent_ppo2.py:186][0m |          -0.0040 |           3.6408 |           1.9894 |
[32m[20230113 19:45:06 @agent_ppo2.py:186][0m |          -0.0046 |           3.4786 |           1.9904 |
[32m[20230113 19:45:06 @agent_ppo2.py:186][0m |          -0.0045 |           3.3741 |           1.9909 |
[32m[20230113 19:45:06 @agent_ppo2.py:186][0m |          -0.0052 |           3.1815 |           1.9921 |
[32m[20230113 19:45:06 @agent_ppo2.py:186][0m |          -0.0057 |           3.0956 |           1.9926 |
[32m[20230113 19:45:06 @agent_ppo2.py:186][0m |          -0.0058 |           3.1161 |           1.9933 |
[32m[20230113 19:45:06 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 19:45:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: -97.36
[32m[20230113 19:45:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -88.15
[32m[20230113 19:45:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -90.52
[32m[20230113 19:45:06 @agent_ppo2.py:144][0m Total time:       0.56 min
[32m[20230113 19:45:06 @agent_ppo2.py:146][0m 47104 total steps have happened
[32m[20230113 19:45:06 @agent_ppo2.py:122][0m #------------------------ Iteration 23 --------------------------#
[32m[20230113 19:45:07 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:45:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:07 @agent_ppo2.py:186][0m |          -0.0004 |           6.0171 |           1.9509 |
[32m[20230113 19:45:07 @agent_ppo2.py:186][0m |          -0.0068 |           4.5980 |           1.9515 |
[32m[20230113 19:45:07 @agent_ppo2.py:186][0m |          -0.0069 |           3.9934 |           1.9512 |
[32m[20230113 19:45:07 @agent_ppo2.py:186][0m |          -0.0073 |           3.8109 |           1.9521 |
[32m[20230113 19:45:07 @agent_ppo2.py:186][0m |          -0.0060 |           3.4653 |           1.9524 |
[32m[20230113 19:45:07 @agent_ppo2.py:186][0m |          -0.0093 |           3.3284 |           1.9529 |
[32m[20230113 19:45:07 @agent_ppo2.py:186][0m |          -0.0106 |           3.3331 |           1.9531 |
[32m[20230113 19:45:07 @agent_ppo2.py:186][0m |          -0.0098 |           3.1905 |           1.9536 |
[32m[20230113 19:45:07 @agent_ppo2.py:186][0m |          -0.0114 |           3.0963 |           1.9538 |
[32m[20230113 19:45:07 @agent_ppo2.py:186][0m |          -0.0110 |           2.9826 |           1.9541 |
[32m[20230113 19:45:07 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:45:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: -92.57
[32m[20230113 19:45:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -87.84
[32m[20230113 19:45:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -51.80
[32m[20230113 19:45:08 @agent_ppo2.py:144][0m Total time:       0.59 min
[32m[20230113 19:45:08 @agent_ppo2.py:146][0m 49152 total steps have happened
[32m[20230113 19:45:08 @agent_ppo2.py:122][0m #------------------------ Iteration 24 --------------------------#
[32m[20230113 19:45:09 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:45:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:09 @agent_ppo2.py:186][0m |           0.0001 |           3.8337 |           1.9933 |
[32m[20230113 19:45:09 @agent_ppo2.py:186][0m |          -0.0006 |           3.3239 |           1.9932 |
[32m[20230113 19:45:09 @agent_ppo2.py:186][0m |          -0.0012 |           3.2387 |           1.9935 |
[32m[20230113 19:45:09 @agent_ppo2.py:186][0m |          -0.0019 |           3.1663 |           1.9936 |
[32m[20230113 19:45:09 @agent_ppo2.py:186][0m |          -0.0029 |           3.1698 |           1.9941 |
[32m[20230113 19:45:09 @agent_ppo2.py:186][0m |          -0.0035 |           3.0740 |           1.9943 |
[32m[20230113 19:45:09 @agent_ppo2.py:186][0m |          -0.0042 |           3.0446 |           1.9943 |
[32m[20230113 19:45:09 @agent_ppo2.py:186][0m |          -0.0046 |           3.0381 |           1.9945 |
[32m[20230113 19:45:09 @agent_ppo2.py:186][0m |          -0.0048 |           3.0427 |           1.9950 |
[32m[20230113 19:45:09 @agent_ppo2.py:186][0m |          -0.0056 |           3.0216 |           1.9949 |
[32m[20230113 19:45:09 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: -90.01
[32m[20230113 19:45:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -89.03
[32m[20230113 19:45:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -62.59
[32m[20230113 19:45:10 @agent_ppo2.py:144][0m Total time:       0.62 min
[32m[20230113 19:45:10 @agent_ppo2.py:146][0m 51200 total steps have happened
[32m[20230113 19:45:10 @agent_ppo2.py:122][0m #------------------------ Iteration 25 --------------------------#
[32m[20230113 19:45:10 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:45:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:10 @agent_ppo2.py:186][0m |          -0.0007 |           2.9579 |           2.0358 |
[32m[20230113 19:45:10 @agent_ppo2.py:186][0m |          -0.0037 |           2.0895 |           2.0362 |
[32m[20230113 19:45:10 @agent_ppo2.py:186][0m |          -0.0054 |           1.7778 |           2.0357 |
[32m[20230113 19:45:10 @agent_ppo2.py:186][0m |          -0.0062 |           1.6287 |           2.0363 |
[32m[20230113 19:45:10 @agent_ppo2.py:186][0m |          -0.0067 |           1.5582 |           2.0360 |
[32m[20230113 19:45:10 @agent_ppo2.py:186][0m |          -0.0079 |           1.4995 |           2.0356 |
[32m[20230113 19:45:11 @agent_ppo2.py:186][0m |          -0.0081 |           1.4627 |           2.0354 |
[32m[20230113 19:45:11 @agent_ppo2.py:186][0m |          -0.0087 |           1.4229 |           2.0354 |
[32m[20230113 19:45:11 @agent_ppo2.py:186][0m |          -0.0089 |           1.3890 |           2.0357 |
[32m[20230113 19:45:11 @agent_ppo2.py:186][0m |          -0.0093 |           1.3697 |           2.0357 |
[32m[20230113 19:45:11 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: -89.76
[32m[20230113 19:45:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -89.48
[32m[20230113 19:45:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -61.46
[32m[20230113 19:45:11 @agent_ppo2.py:144][0m Total time:       0.64 min
[32m[20230113 19:45:11 @agent_ppo2.py:146][0m 53248 total steps have happened
[32m[20230113 19:45:11 @agent_ppo2.py:122][0m #------------------------ Iteration 26 --------------------------#
[32m[20230113 19:45:12 @agent_ppo2.py:128][0m Sampling time: 0.56 s by 1 slaves
[32m[20230113 19:45:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:12 @agent_ppo2.py:186][0m |          -0.0004 |           6.5920 |           1.9528 |
[32m[20230113 19:45:12 @agent_ppo2.py:186][0m |          -0.0023 |           3.3480 |           1.9531 |
[32m[20230113 19:45:12 @agent_ppo2.py:186][0m |          -0.0047 |           2.9404 |           1.9525 |
[32m[20230113 19:45:12 @agent_ppo2.py:186][0m |          -0.0043 |           2.7825 |           1.9522 |
[32m[20230113 19:45:12 @agent_ppo2.py:186][0m |          -0.0051 |           2.6182 |           1.9517 |
[32m[20230113 19:45:12 @agent_ppo2.py:186][0m |          -0.0073 |           2.4168 |           1.9517 |
[32m[20230113 19:45:12 @agent_ppo2.py:186][0m |          -0.0072 |           2.2920 |           1.9509 |
[32m[20230113 19:45:12 @agent_ppo2.py:186][0m |          -0.0064 |           2.2862 |           1.9505 |
[32m[20230113 19:45:12 @agent_ppo2.py:186][0m |          -0.0094 |           2.2605 |           1.9499 |
[32m[20230113 19:45:12 @agent_ppo2.py:186][0m |          -0.0091 |           2.1474 |           1.9495 |
[32m[20230113 19:45:12 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:45:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: -93.99
[32m[20230113 19:45:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -79.71
[32m[20230113 19:45:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -69.68
[32m[20230113 19:45:13 @agent_ppo2.py:144][0m Total time:       0.67 min
[32m[20230113 19:45:13 @agent_ppo2.py:146][0m 55296 total steps have happened
[32m[20230113 19:45:13 @agent_ppo2.py:122][0m #------------------------ Iteration 27 --------------------------#
[32m[20230113 19:45:13 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:45:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:13 @agent_ppo2.py:186][0m |           0.0000 |           1.3220 |           1.9750 |
[32m[20230113 19:45:13 @agent_ppo2.py:186][0m |          -0.0018 |           1.2480 |           1.9753 |
[32m[20230113 19:45:13 @agent_ppo2.py:186][0m |          -0.0034 |           1.1734 |           1.9752 |
[32m[20230113 19:45:13 @agent_ppo2.py:186][0m |          -0.0050 |           1.1466 |           1.9750 |
[32m[20230113 19:45:13 @agent_ppo2.py:186][0m |          -0.0066 |           1.1369 |           1.9746 |
[32m[20230113 19:45:14 @agent_ppo2.py:186][0m |          -0.0075 |           1.1123 |           1.9744 |
[32m[20230113 19:45:14 @agent_ppo2.py:186][0m |          -0.0082 |           1.1075 |           1.9747 |
[32m[20230113 19:45:14 @agent_ppo2.py:186][0m |          -0.0087 |           1.0946 |           1.9752 |
[32m[20230113 19:45:14 @agent_ppo2.py:186][0m |          -0.0094 |           1.0959 |           1.9758 |
[32m[20230113 19:45:14 @agent_ppo2.py:186][0m |          -0.0098 |           1.0852 |           1.9766 |
[32m[20230113 19:45:14 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: -83.61
[32m[20230113 19:45:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -83.21
[32m[20230113 19:45:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -70.34
[32m[20230113 19:45:14 @agent_ppo2.py:144][0m Total time:       0.69 min
[32m[20230113 19:45:14 @agent_ppo2.py:146][0m 57344 total steps have happened
[32m[20230113 19:45:14 @agent_ppo2.py:122][0m #------------------------ Iteration 28 --------------------------#
[32m[20230113 19:45:15 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:45:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:15 @agent_ppo2.py:186][0m |          -0.0001 |           9.9580 |           2.0584 |
[32m[20230113 19:45:15 @agent_ppo2.py:186][0m |          -0.0029 |           2.2308 |           2.0584 |
[32m[20230113 19:45:15 @agent_ppo2.py:186][0m |          -0.0050 |           1.9063 |           2.0584 |
[32m[20230113 19:45:15 @agent_ppo2.py:186][0m |          -0.0060 |           1.7430 |           2.0584 |
[32m[20230113 19:45:15 @agent_ppo2.py:186][0m |          -0.0072 |           1.6108 |           2.0579 |
[32m[20230113 19:45:15 @agent_ppo2.py:186][0m |          -0.0082 |           1.5386 |           2.0580 |
[32m[20230113 19:45:15 @agent_ppo2.py:186][0m |          -0.0088 |           1.5033 |           2.0579 |
[32m[20230113 19:45:15 @agent_ppo2.py:186][0m |          -0.0091 |           1.4315 |           2.0576 |
[32m[20230113 19:45:15 @agent_ppo2.py:186][0m |          -0.0097 |           1.4021 |           2.0575 |
[32m[20230113 19:45:15 @agent_ppo2.py:186][0m |          -0.0099 |           1.3821 |           2.0572 |
[32m[20230113 19:45:15 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 19:45:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: -90.73
[32m[20230113 19:45:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -74.82
[32m[20230113 19:45:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -76.21
[32m[20230113 19:45:16 @agent_ppo2.py:144][0m Total time:       0.72 min
[32m[20230113 19:45:16 @agent_ppo2.py:146][0m 59392 total steps have happened
[32m[20230113 19:45:16 @agent_ppo2.py:122][0m #------------------------ Iteration 29 --------------------------#
[32m[20230113 19:45:16 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:45:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:16 @agent_ppo2.py:186][0m |           0.0001 |           1.0798 |           2.0544 |
[32m[20230113 19:45:16 @agent_ppo2.py:186][0m |          -0.0009 |           0.9270 |           2.0553 |
[32m[20230113 19:45:16 @agent_ppo2.py:186][0m |          -0.0020 |           0.8806 |           2.0556 |
[32m[20230113 19:45:16 @agent_ppo2.py:186][0m |          -0.0028 |           0.8426 |           2.0560 |
[32m[20230113 19:45:16 @agent_ppo2.py:186][0m |          -0.0037 |           0.8277 |           2.0564 |
[32m[20230113 19:45:16 @agent_ppo2.py:186][0m |          -0.0042 |           0.8224 |           2.0559 |
[32m[20230113 19:45:16 @agent_ppo2.py:186][0m |          -0.0049 |           0.8090 |           2.0566 |
[32m[20230113 19:45:17 @agent_ppo2.py:186][0m |          -0.0054 |           0.8023 |           2.0565 |
[32m[20230113 19:45:17 @agent_ppo2.py:186][0m |          -0.0058 |           0.8015 |           2.0564 |
[32m[20230113 19:45:17 @agent_ppo2.py:186][0m |          -0.0064 |           0.7917 |           2.0564 |
[32m[20230113 19:45:17 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: -77.86
[32m[20230113 19:45:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -77.37
[32m[20230113 19:45:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -71.92
[32m[20230113 19:45:17 @agent_ppo2.py:144][0m Total time:       0.74 min
[32m[20230113 19:45:17 @agent_ppo2.py:146][0m 61440 total steps have happened
[32m[20230113 19:45:17 @agent_ppo2.py:122][0m #------------------------ Iteration 30 --------------------------#
[32m[20230113 19:45:18 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:45:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:18 @agent_ppo2.py:186][0m |           0.0004 |           0.9226 |           2.0443 |
[32m[20230113 19:45:18 @agent_ppo2.py:186][0m |          -0.0008 |           0.8517 |           2.0442 |
[32m[20230113 19:45:18 @agent_ppo2.py:186][0m |          -0.0021 |           0.8289 |           2.0447 |
[32m[20230113 19:45:18 @agent_ppo2.py:186][0m |          -0.0030 |           0.8130 |           2.0450 |
[32m[20230113 19:45:18 @agent_ppo2.py:186][0m |          -0.0040 |           0.8135 |           2.0451 |
[32m[20230113 19:45:18 @agent_ppo2.py:186][0m |          -0.0049 |           0.7998 |           2.0460 |
[32m[20230113 19:45:18 @agent_ppo2.py:186][0m |          -0.0055 |           0.7928 |           2.0465 |
[32m[20230113 19:45:18 @agent_ppo2.py:186][0m |          -0.0062 |           0.7861 |           2.0471 |
[32m[20230113 19:45:18 @agent_ppo2.py:186][0m |          -0.0071 |           0.7863 |           2.0471 |
[32m[20230113 19:45:18 @agent_ppo2.py:186][0m |          -0.0079 |           0.7732 |           2.0477 |
[32m[20230113 19:45:18 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: -73.80
[32m[20230113 19:45:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -73.09
[32m[20230113 19:45:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -79.62
[32m[20230113 19:45:19 @agent_ppo2.py:144][0m Total time:       0.77 min
[32m[20230113 19:45:19 @agent_ppo2.py:146][0m 63488 total steps have happened
[32m[20230113 19:45:19 @agent_ppo2.py:122][0m #------------------------ Iteration 31 --------------------------#
[32m[20230113 19:45:19 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:45:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:19 @agent_ppo2.py:186][0m |           0.0002 |           0.8328 |           2.0132 |
[32m[20230113 19:45:19 @agent_ppo2.py:186][0m |          -0.0006 |           0.7666 |           2.0134 |
[32m[20230113 19:45:19 @agent_ppo2.py:186][0m |          -0.0021 |           0.7597 |           2.0137 |
[32m[20230113 19:45:19 @agent_ppo2.py:186][0m |          -0.0032 |           0.7519 |           2.0138 |
[32m[20230113 19:45:19 @agent_ppo2.py:186][0m |          -0.0041 |           0.7458 |           2.0138 |
[32m[20230113 19:45:19 @agent_ppo2.py:186][0m |          -0.0047 |           0.7428 |           2.0143 |
[32m[20230113 19:45:19 @agent_ppo2.py:186][0m |          -0.0053 |           0.7401 |           2.0144 |
[32m[20230113 19:45:19 @agent_ppo2.py:186][0m |          -0.0058 |           0.7376 |           2.0150 |
[32m[20230113 19:45:19 @agent_ppo2.py:186][0m |          -0.0063 |           0.7345 |           2.0157 |
[32m[20230113 19:45:20 @agent_ppo2.py:186][0m |          -0.0066 |           0.7336 |           2.0160 |
[32m[20230113 19:45:20 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:45:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: -75.60
[32m[20230113 19:45:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -70.65
[32m[20230113 19:45:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -69.90
[32m[20230113 19:45:20 @agent_ppo2.py:144][0m Total time:       0.79 min
[32m[20230113 19:45:20 @agent_ppo2.py:146][0m 65536 total steps have happened
[32m[20230113 19:45:20 @agent_ppo2.py:122][0m #------------------------ Iteration 32 --------------------------#
[32m[20230113 19:45:20 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:45:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:21 @agent_ppo2.py:186][0m |          -0.0000 |           0.8174 |           2.0800 |
[32m[20230113 19:45:21 @agent_ppo2.py:186][0m |          -0.0014 |           0.7690 |           2.0804 |
[32m[20230113 19:45:21 @agent_ppo2.py:186][0m |          -0.0030 |           0.7612 |           2.0809 |
[32m[20230113 19:45:21 @agent_ppo2.py:186][0m |          -0.0043 |           0.7506 |           2.0807 |
[32m[20230113 19:45:21 @agent_ppo2.py:186][0m |          -0.0059 |           0.7489 |           2.0815 |
[32m[20230113 19:45:21 @agent_ppo2.py:186][0m |          -0.0065 |           0.7417 |           2.0808 |
[32m[20230113 19:45:21 @agent_ppo2.py:186][0m |          -0.0071 |           0.7427 |           2.0819 |
[32m[20230113 19:45:21 @agent_ppo2.py:186][0m |          -0.0073 |           0.7376 |           2.0821 |
[32m[20230113 19:45:21 @agent_ppo2.py:186][0m |          -0.0079 |           0.7297 |           2.0829 |
[32m[20230113 19:45:21 @agent_ppo2.py:186][0m |          -0.0082 |           0.7254 |           2.0840 |
[32m[20230113 19:45:21 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: -69.71
[32m[20230113 19:45:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -66.45
[32m[20230113 19:45:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -113.66
[32m[20230113 19:45:21 @agent_ppo2.py:144][0m Total time:       0.81 min
[32m[20230113 19:45:21 @agent_ppo2.py:146][0m 67584 total steps have happened
[32m[20230113 19:45:21 @agent_ppo2.py:122][0m #------------------------ Iteration 33 --------------------------#
[32m[20230113 19:45:22 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:45:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:22 @agent_ppo2.py:186][0m |          -0.0001 |           0.8418 |           2.1325 |
[32m[20230113 19:45:22 @agent_ppo2.py:186][0m |          -0.0021 |           0.7772 |           2.1329 |
[32m[20230113 19:45:22 @agent_ppo2.py:186][0m |          -0.0041 |           0.7552 |           2.1329 |
[32m[20230113 19:45:22 @agent_ppo2.py:186][0m |          -0.0058 |           0.7506 |           2.1324 |
[32m[20230113 19:45:22 @agent_ppo2.py:186][0m |          -0.0065 |           0.7433 |           2.1320 |
[32m[20230113 19:45:22 @agent_ppo2.py:186][0m |          -0.0072 |           0.7392 |           2.1333 |
[32m[20230113 19:45:22 @agent_ppo2.py:186][0m |          -0.0078 |           0.7377 |           2.1335 |
[32m[20230113 19:45:22 @agent_ppo2.py:186][0m |          -0.0082 |           0.7328 |           2.1344 |
[32m[20230113 19:45:22 @agent_ppo2.py:186][0m |          -0.0086 |           0.7306 |           2.1356 |
[32m[20230113 19:45:22 @agent_ppo2.py:186][0m |          -0.0089 |           0.7296 |           2.1367 |
[32m[20230113 19:45:22 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: -68.40
[32m[20230113 19:45:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -68.33
[32m[20230113 19:45:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -107.63
[32m[20230113 19:45:23 @agent_ppo2.py:144][0m Total time:       0.83 min
[32m[20230113 19:45:23 @agent_ppo2.py:146][0m 69632 total steps have happened
[32m[20230113 19:45:23 @agent_ppo2.py:122][0m #------------------------ Iteration 34 --------------------------#
[32m[20230113 19:45:23 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:45:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:23 @agent_ppo2.py:186][0m |          -0.0002 |           0.7764 |           2.0978 |
[32m[20230113 19:45:23 @agent_ppo2.py:186][0m |          -0.0025 |           0.7483 |           2.0987 |
[32m[20230113 19:45:23 @agent_ppo2.py:186][0m |          -0.0044 |           0.7404 |           2.0995 |
[32m[20230113 19:45:23 @agent_ppo2.py:186][0m |          -0.0062 |           0.7380 |           2.1003 |
[32m[20230113 19:45:23 @agent_ppo2.py:186][0m |          -0.0074 |           0.7313 |           2.1004 |
[32m[20230113 19:45:23 @agent_ppo2.py:186][0m |          -0.0082 |           0.7299 |           2.1014 |
[32m[20230113 19:45:23 @agent_ppo2.py:186][0m |          -0.0088 |           0.7257 |           2.1017 |
[32m[20230113 19:45:23 @agent_ppo2.py:186][0m |          -0.0093 |           0.7241 |           2.1030 |
[32m[20230113 19:45:24 @agent_ppo2.py:186][0m |          -0.0101 |           0.7221 |           2.1039 |
[32m[20230113 19:45:24 @agent_ppo2.py:186][0m |          -0.0104 |           0.7200 |           2.1056 |
[32m[20230113 19:45:24 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: -69.71
[32m[20230113 19:45:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -65.34
[32m[20230113 19:45:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -71.55
[32m[20230113 19:45:24 @agent_ppo2.py:144][0m Total time:       0.86 min
[32m[20230113 19:45:24 @agent_ppo2.py:146][0m 71680 total steps have happened
[32m[20230113 19:45:24 @agent_ppo2.py:122][0m #------------------------ Iteration 35 --------------------------#
[32m[20230113 19:45:25 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:45:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:25 @agent_ppo2.py:186][0m |           0.0005 |          24.7925 |           2.1480 |
[32m[20230113 19:45:25 @agent_ppo2.py:186][0m |          -0.0017 |          19.8710 |           2.1470 |
[32m[20230113 19:45:25 @agent_ppo2.py:186][0m |          -0.0036 |          16.5035 |           2.1458 |
[32m[20230113 19:45:25 @agent_ppo2.py:186][0m |          -0.0052 |          13.1037 |           2.1436 |
[32m[20230113 19:45:25 @agent_ppo2.py:186][0m |          -0.0059 |          10.6803 |           2.1422 |
[32m[20230113 19:45:25 @agent_ppo2.py:186][0m |          -0.0069 |           8.6648 |           2.1407 |
[32m[20230113 19:45:25 @agent_ppo2.py:186][0m |          -0.0075 |           6.8358 |           2.1395 |
[32m[20230113 19:45:25 @agent_ppo2.py:186][0m |          -0.0079 |           5.2163 |           2.1381 |
[32m[20230113 19:45:25 @agent_ppo2.py:186][0m |          -0.0085 |           3.9945 |           2.1370 |
[32m[20230113 19:45:25 @agent_ppo2.py:186][0m |          -0.0087 |           3.1344 |           2.1360 |
[32m[20230113 19:45:25 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 19:45:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: -76.88
[32m[20230113 19:45:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -56.51
[32m[20230113 19:45:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -57.88
[32m[20230113 19:45:25 @agent_ppo2.py:144][0m Total time:       0.88 min
[32m[20230113 19:45:25 @agent_ppo2.py:146][0m 73728 total steps have happened
[32m[20230113 19:45:25 @agent_ppo2.py:122][0m #------------------------ Iteration 36 --------------------------#
[32m[20230113 19:45:26 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:45:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:26 @agent_ppo2.py:186][0m |          -0.0001 |           1.4525 |           2.1348 |
[32m[20230113 19:45:26 @agent_ppo2.py:186][0m |          -0.0017 |           1.0327 |           2.1350 |
[32m[20230113 19:45:26 @agent_ppo2.py:186][0m |          -0.0033 |           0.9597 |           2.1352 |
[32m[20230113 19:45:26 @agent_ppo2.py:186][0m |          -0.0045 |           0.9244 |           2.1352 |
[32m[20230113 19:45:26 @agent_ppo2.py:186][0m |          -0.0054 |           0.8971 |           2.1354 |
[32m[20230113 19:45:26 @agent_ppo2.py:186][0m |          -0.0059 |           0.8830 |           2.1361 |
[32m[20230113 19:45:26 @agent_ppo2.py:186][0m |          -0.0066 |           0.8678 |           2.1365 |
[32m[20230113 19:45:26 @agent_ppo2.py:186][0m |          -0.0073 |           0.8518 |           2.1375 |
[32m[20230113 19:45:26 @agent_ppo2.py:186][0m |          -0.0078 |           0.8480 |           2.1382 |
[32m[20230113 19:45:26 @agent_ppo2.py:186][0m |          -0.0082 |           0.8380 |           2.1393 |
[32m[20230113 19:45:26 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:45:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: -55.36
[32m[20230113 19:45:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -54.72
[32m[20230113 19:45:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -63.59
[32m[20230113 19:45:27 @agent_ppo2.py:144][0m Total time:       0.91 min
[32m[20230113 19:45:27 @agent_ppo2.py:146][0m 75776 total steps have happened
[32m[20230113 19:45:27 @agent_ppo2.py:122][0m #------------------------ Iteration 37 --------------------------#
[32m[20230113 19:45:27 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:45:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:28 @agent_ppo2.py:186][0m |          -0.0002 |           1.0472 |           2.1534 |
[32m[20230113 19:45:28 @agent_ppo2.py:186][0m |          -0.0030 |           0.8738 |           2.1527 |
[32m[20230113 19:45:28 @agent_ppo2.py:186][0m |          -0.0058 |           0.8428 |           2.1518 |
[32m[20230113 19:45:28 @agent_ppo2.py:186][0m |          -0.0071 |           0.8234 |           2.1518 |
[32m[20230113 19:45:28 @agent_ppo2.py:186][0m |          -0.0080 |           0.8118 |           2.1532 |
[32m[20230113 19:45:28 @agent_ppo2.py:186][0m |          -0.0088 |           0.8052 |           2.1537 |
[32m[20230113 19:45:28 @agent_ppo2.py:186][0m |          -0.0095 |           0.7971 |           2.1541 |
[32m[20230113 19:45:28 @agent_ppo2.py:186][0m |          -0.0097 |           0.7900 |           2.1553 |
[32m[20230113 19:45:28 @agent_ppo2.py:186][0m |          -0.0104 |           0.7875 |           2.1558 |
[32m[20230113 19:45:28 @agent_ppo2.py:186][0m |          -0.0108 |           0.7840 |           2.1569 |
[32m[20230113 19:45:28 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: -60.96
[32m[20230113 19:45:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -55.69
[32m[20230113 19:45:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -74.01
[32m[20230113 19:45:28 @agent_ppo2.py:144][0m Total time:       0.93 min
[32m[20230113 19:45:28 @agent_ppo2.py:146][0m 77824 total steps have happened
[32m[20230113 19:45:28 @agent_ppo2.py:122][0m #------------------------ Iteration 38 --------------------------#
[32m[20230113 19:45:29 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:45:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:29 @agent_ppo2.py:186][0m |           0.0003 |           3.3563 |           2.1523 |
[32m[20230113 19:45:29 @agent_ppo2.py:186][0m |          -0.0023 |           2.4078 |           2.1525 |
[32m[20230113 19:45:29 @agent_ppo2.py:186][0m |          -0.0045 |           1.9063 |           2.1529 |
[32m[20230113 19:45:29 @agent_ppo2.py:186][0m |          -0.0062 |           1.7189 |           2.1523 |
[32m[20230113 19:45:29 @agent_ppo2.py:186][0m |          -0.0073 |           1.5686 |           2.1532 |
[32m[20230113 19:45:29 @agent_ppo2.py:186][0m |          -0.0083 |           1.4938 |           2.1530 |
[32m[20230113 19:45:29 @agent_ppo2.py:186][0m |          -0.0089 |           1.4432 |           2.1529 |
[32m[20230113 19:45:29 @agent_ppo2.py:186][0m |          -0.0096 |           1.3991 |           2.1531 |
[32m[20230113 19:45:29 @agent_ppo2.py:186][0m |          -0.0102 |           1.3770 |           2.1528 |
[32m[20230113 19:45:29 @agent_ppo2.py:186][0m |          -0.0105 |           1.3397 |           2.1531 |
[32m[20230113 19:45:29 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: -48.23
[32m[20230113 19:45:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -45.03
[32m[20230113 19:45:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -41.60
[32m[20230113 19:45:30 @agent_ppo2.py:144][0m Total time:       0.96 min
[32m[20230113 19:45:30 @agent_ppo2.py:146][0m 79872 total steps have happened
[32m[20230113 19:45:30 @agent_ppo2.py:122][0m #------------------------ Iteration 39 --------------------------#
[32m[20230113 19:45:30 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:45:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:30 @agent_ppo2.py:186][0m |          -0.0001 |           0.9188 |           2.2167 |
[32m[20230113 19:45:31 @agent_ppo2.py:186][0m |          -0.0031 |           0.8313 |           2.2160 |
[32m[20230113 19:45:31 @agent_ppo2.py:186][0m |          -0.0058 |           0.7870 |           2.2152 |
[32m[20230113 19:45:31 @agent_ppo2.py:186][0m |          -0.0076 |           0.7608 |           2.2140 |
[32m[20230113 19:45:31 @agent_ppo2.py:186][0m |          -0.0088 |           0.7433 |           2.2123 |
[32m[20230113 19:45:31 @agent_ppo2.py:186][0m |          -0.0098 |           0.7288 |           2.2110 |
[32m[20230113 19:45:31 @agent_ppo2.py:186][0m |          -0.0103 |           0.7158 |           2.2103 |
[32m[20230113 19:45:31 @agent_ppo2.py:186][0m |          -0.0112 |           0.7061 |           2.2100 |
[32m[20230113 19:45:31 @agent_ppo2.py:186][0m |          -0.0116 |           0.6996 |           2.2092 |
[32m[20230113 19:45:31 @agent_ppo2.py:186][0m |          -0.0121 |           0.6916 |           2.2085 |
[32m[20230113 19:45:31 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: -49.06
[32m[20230113 19:45:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -44.64
[32m[20230113 19:45:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -15.06
[32m[20230113 19:45:31 @agent_ppo2.py:144][0m Total time:       0.98 min
[32m[20230113 19:45:31 @agent_ppo2.py:146][0m 81920 total steps have happened
[32m[20230113 19:45:31 @agent_ppo2.py:122][0m #------------------------ Iteration 40 --------------------------#
[32m[20230113 19:45:32 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:45:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:32 @agent_ppo2.py:186][0m |          -0.0004 |           6.9107 |           2.1731 |
[32m[20230113 19:45:32 @agent_ppo2.py:186][0m |          -0.0035 |           2.1519 |           2.1711 |
[32m[20230113 19:45:32 @agent_ppo2.py:186][0m |          -0.0067 |           1.8893 |           2.1695 |
[32m[20230113 19:45:32 @agent_ppo2.py:186][0m |          -0.0083 |           1.7448 |           2.1678 |
[32m[20230113 19:45:32 @agent_ppo2.py:186][0m |          -0.0084 |           1.6674 |           2.1669 |
[32m[20230113 19:45:32 @agent_ppo2.py:186][0m |          -0.0088 |           1.5955 |           2.1656 |
[32m[20230113 19:45:32 @agent_ppo2.py:186][0m |          -0.0103 |           1.5377 |           2.1652 |
[32m[20230113 19:45:32 @agent_ppo2.py:186][0m |          -0.0113 |           1.5018 |           2.1644 |
[32m[20230113 19:45:32 @agent_ppo2.py:186][0m |          -0.0118 |           1.4459 |           2.1640 |
[32m[20230113 19:45:32 @agent_ppo2.py:186][0m |          -0.0118 |           1.4114 |           2.1637 |
[32m[20230113 19:45:32 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: -68.09
[32m[20230113 19:45:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -46.67
[32m[20230113 19:45:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -24.63
[32m[20230113 19:45:33 @agent_ppo2.py:144][0m Total time:       1.00 min
[32m[20230113 19:45:33 @agent_ppo2.py:146][0m 83968 total steps have happened
[32m[20230113 19:45:33 @agent_ppo2.py:122][0m #------------------------ Iteration 41 --------------------------#
[32m[20230113 19:45:33 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:45:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:33 @agent_ppo2.py:186][0m |           0.0001 |           1.6934 |           2.1546 |
[32m[20230113 19:45:33 @agent_ppo2.py:186][0m |          -0.0015 |           1.0197 |           2.1553 |
[32m[20230113 19:45:34 @agent_ppo2.py:186][0m |          -0.0032 |           0.9478 |           2.1549 |
[32m[20230113 19:45:34 @agent_ppo2.py:186][0m |          -0.0048 |           0.9022 |           2.1547 |
[32m[20230113 19:45:34 @agent_ppo2.py:186][0m |          -0.0056 |           0.8726 |           2.1548 |
[32m[20230113 19:45:34 @agent_ppo2.py:186][0m |          -0.0061 |           0.8523 |           2.1545 |
[32m[20230113 19:45:34 @agent_ppo2.py:186][0m |          -0.0065 |           0.8356 |           2.1549 |
[32m[20230113 19:45:34 @agent_ppo2.py:186][0m |          -0.0071 |           0.8213 |           2.1547 |
[32m[20230113 19:45:34 @agent_ppo2.py:186][0m |          -0.0073 |           0.8052 |           2.1551 |
[32m[20230113 19:45:34 @agent_ppo2.py:186][0m |          -0.0078 |           0.7922 |           2.1553 |
[32m[20230113 19:45:34 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:45:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: -38.05
[32m[20230113 19:45:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -36.86
[32m[20230113 19:45:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -41.58
[32m[20230113 19:45:34 @agent_ppo2.py:144][0m Total time:       1.03 min
[32m[20230113 19:45:34 @agent_ppo2.py:146][0m 86016 total steps have happened
[32m[20230113 19:45:34 @agent_ppo2.py:122][0m #------------------------ Iteration 42 --------------------------#
[32m[20230113 19:45:35 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:45:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:35 @agent_ppo2.py:186][0m |          -0.0013 |           1.0060 |           2.1688 |
[32m[20230113 19:45:35 @agent_ppo2.py:186][0m |          -0.0049 |           0.9220 |           2.1689 |
[32m[20230113 19:45:35 @agent_ppo2.py:186][0m |          -0.0072 |           0.8706 |           2.1682 |
[32m[20230113 19:45:35 @agent_ppo2.py:186][0m |          -0.0081 |           0.8495 |           2.1688 |
[32m[20230113 19:45:35 @agent_ppo2.py:186][0m |          -0.0091 |           0.8345 |           2.1700 |
[32m[20230113 19:45:35 @agent_ppo2.py:186][0m |          -0.0097 |           0.8218 |           2.1706 |
[32m[20230113 19:45:35 @agent_ppo2.py:186][0m |          -0.0103 |           0.8130 |           2.1715 |
[32m[20230113 19:45:35 @agent_ppo2.py:186][0m |          -0.0108 |           0.8070 |           2.1721 |
[32m[20230113 19:45:35 @agent_ppo2.py:186][0m |          -0.0113 |           0.8007 |           2.1727 |
[32m[20230113 19:45:35 @agent_ppo2.py:186][0m |          -0.0119 |           0.7936 |           2.1734 |
[32m[20230113 19:45:35 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: -38.27
[32m[20230113 19:45:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -34.91
[32m[20230113 19:45:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 2.51
[32m[20230113 19:45:36 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 2.51
[32m[20230113 19:45:36 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 2.51
[32m[20230113 19:45:36 @agent_ppo2.py:144][0m Total time:       1.05 min
[32m[20230113 19:45:36 @agent_ppo2.py:146][0m 88064 total steps have happened
[32m[20230113 19:45:36 @agent_ppo2.py:122][0m #------------------------ Iteration 43 --------------------------#
[32m[20230113 19:45:36 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:45:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:36 @agent_ppo2.py:186][0m |           0.0010 |           9.8280 |           2.1744 |
[32m[20230113 19:45:36 @agent_ppo2.py:186][0m |          -0.0013 |           2.3817 |           2.1729 |
[32m[20230113 19:45:36 @agent_ppo2.py:186][0m |          -0.0025 |           1.7227 |           2.1722 |
[32m[20230113 19:45:37 @agent_ppo2.py:186][0m |          -0.0027 |           1.4782 |           2.1718 |
[32m[20230113 19:45:37 @agent_ppo2.py:186][0m |          -0.0047 |           1.3859 |           2.1715 |
[32m[20230113 19:45:37 @agent_ppo2.py:186][0m |          -0.0075 |           1.3244 |           2.1704 |
[32m[20230113 19:45:37 @agent_ppo2.py:186][0m |          -0.0083 |           1.2879 |           2.1704 |
[32m[20230113 19:45:37 @agent_ppo2.py:186][0m |          -0.0086 |           1.2491 |           2.1696 |
[32m[20230113 19:45:37 @agent_ppo2.py:186][0m |          -0.0092 |           1.2358 |           2.1688 |
[32m[20230113 19:45:37 @agent_ppo2.py:186][0m |          -0.0087 |           1.2120 |           2.1691 |
[32m[20230113 19:45:37 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 19:45:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: -49.01
[32m[20230113 19:45:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -13.95
[32m[20230113 19:45:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -37.38
[32m[20230113 19:45:37 @agent_ppo2.py:144][0m Total time:       1.08 min
[32m[20230113 19:45:37 @agent_ppo2.py:146][0m 90112 total steps have happened
[32m[20230113 19:45:37 @agent_ppo2.py:122][0m #------------------------ Iteration 44 --------------------------#
[32m[20230113 19:45:38 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:45:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:38 @agent_ppo2.py:186][0m |          -0.0002 |           1.9686 |           2.2058 |
[32m[20230113 19:45:38 @agent_ppo2.py:186][0m |          -0.0031 |           1.6276 |           2.2051 |
[32m[20230113 19:45:38 @agent_ppo2.py:186][0m |          -0.0055 |           1.5903 |           2.2033 |
[32m[20230113 19:45:38 @agent_ppo2.py:186][0m |          -0.0066 |           1.5539 |           2.2025 |
[32m[20230113 19:45:38 @agent_ppo2.py:186][0m |          -0.0075 |           1.5323 |           2.2013 |
[32m[20230113 19:45:38 @agent_ppo2.py:186][0m |          -0.0081 |           1.5124 |           2.2014 |
[32m[20230113 19:45:38 @agent_ppo2.py:186][0m |          -0.0088 |           1.4942 |           2.2005 |
[32m[20230113 19:45:38 @agent_ppo2.py:186][0m |          -0.0093 |           1.4815 |           2.2001 |
[32m[20230113 19:45:38 @agent_ppo2.py:186][0m |          -0.0097 |           1.4722 |           2.2001 |
[32m[20230113 19:45:38 @agent_ppo2.py:186][0m |          -0.0102 |           1.4599 |           2.1998 |
[32m[20230113 19:45:38 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: -18.31
[32m[20230113 19:45:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -15.03
[32m[20230113 19:45:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 59.00
[32m[20230113 19:45:39 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 59.00
[32m[20230113 19:45:39 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 59.00
[32m[20230113 19:45:39 @agent_ppo2.py:144][0m Total time:       1.10 min
[32m[20230113 19:45:39 @agent_ppo2.py:146][0m 92160 total steps have happened
[32m[20230113 19:45:39 @agent_ppo2.py:122][0m #------------------------ Iteration 45 --------------------------#
[32m[20230113 19:45:39 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:45:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:39 @agent_ppo2.py:186][0m |          -0.0000 |           1.1989 |           2.2235 |
[32m[20230113 19:45:39 @agent_ppo2.py:186][0m |          -0.0022 |           1.0261 |           2.2228 |
[32m[20230113 19:45:39 @agent_ppo2.py:186][0m |          -0.0044 |           0.9913 |           2.2211 |
[32m[20230113 19:45:39 @agent_ppo2.py:186][0m |          -0.0063 |           0.9723 |           2.2198 |
[32m[20230113 19:45:40 @agent_ppo2.py:186][0m |          -0.0078 |           0.9609 |           2.2177 |
[32m[20230113 19:45:40 @agent_ppo2.py:186][0m |          -0.0092 |           0.9480 |           2.2185 |
[32m[20230113 19:45:40 @agent_ppo2.py:186][0m |          -0.0102 |           0.9401 |           2.2175 |
[32m[20230113 19:45:40 @agent_ppo2.py:186][0m |          -0.0106 |           0.9349 |           2.2179 |
[32m[20230113 19:45:40 @agent_ppo2.py:186][0m |          -0.0116 |           0.9277 |           2.2182 |
[32m[20230113 19:45:40 @agent_ppo2.py:186][0m |          -0.0120 |           0.9218 |           2.2188 |
[32m[20230113 19:45:40 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: -21.24
[32m[20230113 19:45:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -21.21
[32m[20230113 19:45:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 110.38
[32m[20230113 19:45:40 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 110.38
[32m[20230113 19:45:40 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 110.38
[32m[20230113 19:45:40 @agent_ppo2.py:144][0m Total time:       1.13 min
[32m[20230113 19:45:40 @agent_ppo2.py:146][0m 94208 total steps have happened
[32m[20230113 19:45:40 @agent_ppo2.py:122][0m #------------------------ Iteration 46 --------------------------#
[32m[20230113 19:45:41 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:45:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:41 @agent_ppo2.py:186][0m |          -0.0018 |           1.2336 |           2.1942 |
[32m[20230113 19:45:41 @agent_ppo2.py:186][0m |          -0.0062 |           1.0969 |           2.1929 |
[32m[20230113 19:45:41 @agent_ppo2.py:186][0m |          -0.0080 |           1.0441 |           2.1914 |
[32m[20230113 19:45:41 @agent_ppo2.py:186][0m |          -0.0096 |           1.0280 |           2.1900 |
[32m[20230113 19:45:41 @agent_ppo2.py:186][0m |          -0.0105 |           1.0126 |           2.1889 |
[32m[20230113 19:45:41 @agent_ppo2.py:186][0m |          -0.0114 |           0.9966 |           2.1884 |
[32m[20230113 19:45:41 @agent_ppo2.py:186][0m |          -0.0122 |           0.9854 |           2.1884 |
[32m[20230113 19:45:41 @agent_ppo2.py:186][0m |          -0.0128 |           0.9817 |           2.1876 |
[32m[20230113 19:45:41 @agent_ppo2.py:186][0m |          -0.0132 |           0.9720 |           2.1874 |
[32m[20230113 19:45:41 @agent_ppo2.py:186][0m |          -0.0135 |           0.9658 |           2.1875 |
[32m[20230113 19:45:41 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: -12.69
[32m[20230113 19:45:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: -12.47
[32m[20230113 19:45:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 28.78
[32m[20230113 19:45:42 @agent_ppo2.py:144][0m Total time:       1.15 min
[32m[20230113 19:45:42 @agent_ppo2.py:146][0m 96256 total steps have happened
[32m[20230113 19:45:42 @agent_ppo2.py:122][0m #------------------------ Iteration 47 --------------------------#
[32m[20230113 19:45:42 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:45:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:42 @agent_ppo2.py:186][0m |          -0.0006 |           1.0052 |           2.2769 |
[32m[20230113 19:45:42 @agent_ppo2.py:186][0m |          -0.0051 |           0.8170 |           2.2767 |
[32m[20230113 19:45:42 @agent_ppo2.py:186][0m |          -0.0079 |           0.7895 |           2.2752 |
[32m[20230113 19:45:42 @agent_ppo2.py:186][0m |          -0.0098 |           0.7712 |           2.2739 |
[32m[20230113 19:45:42 @agent_ppo2.py:186][0m |          -0.0105 |           0.7574 |           2.2734 |
[32m[20230113 19:45:42 @agent_ppo2.py:186][0m |          -0.0117 |           0.7490 |           2.2733 |
[32m[20230113 19:45:42 @agent_ppo2.py:186][0m |          -0.0123 |           0.7388 |           2.2736 |
[32m[20230113 19:45:42 @agent_ppo2.py:186][0m |          -0.0127 |           0.7309 |           2.2729 |
[32m[20230113 19:45:43 @agent_ppo2.py:186][0m |          -0.0135 |           0.7263 |           2.2735 |
[32m[20230113 19:45:43 @agent_ppo2.py:186][0m |          -0.0140 |           0.7194 |           2.2737 |
[32m[20230113 19:45:43 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 2.45
[32m[20230113 19:45:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 8.82
[32m[20230113 19:45:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -77.52
[32m[20230113 19:45:43 @agent_ppo2.py:144][0m Total time:       1.17 min
[32m[20230113 19:45:43 @agent_ppo2.py:146][0m 98304 total steps have happened
[32m[20230113 19:45:43 @agent_ppo2.py:122][0m #------------------------ Iteration 48 --------------------------#
[32m[20230113 19:45:43 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 19:45:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:43 @agent_ppo2.py:186][0m |           0.0037 |           8.8857 |           2.1564 |
[32m[20230113 19:45:43 @agent_ppo2.py:186][0m |           0.0025 |           4.4248 |           2.1553 |
[32m[20230113 19:45:43 @agent_ppo2.py:186][0m |          -0.0040 |           3.9298 |           2.1548 |
[32m[20230113 19:45:43 @agent_ppo2.py:186][0m |          -0.0060 |           3.5328 |           2.1542 |
[32m[20230113 19:45:44 @agent_ppo2.py:186][0m |          -0.0047 |           3.3181 |           2.1528 |
[32m[20230113 19:45:44 @agent_ppo2.py:186][0m |          -0.0011 |           3.0507 |           2.1519 |
[32m[20230113 19:45:44 @agent_ppo2.py:186][0m |          -0.0081 |           2.9211 |           2.1507 |
[32m[20230113 19:45:44 @agent_ppo2.py:186][0m |          -0.0090 |           2.6936 |           2.1504 |
[32m[20230113 19:45:44 @agent_ppo2.py:186][0m |          -0.0073 |           2.4801 |           2.1500 |
[32m[20230113 19:45:44 @agent_ppo2.py:186][0m |          -0.0068 |           2.4114 |           2.1486 |
[32m[20230113 19:45:44 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 19:45:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: -41.10
[32m[20230113 19:45:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 8.94
[32m[20230113 19:45:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 38.12
[32m[20230113 19:45:44 @agent_ppo2.py:144][0m Total time:       1.19 min
[32m[20230113 19:45:44 @agent_ppo2.py:146][0m 100352 total steps have happened
[32m[20230113 19:45:44 @agent_ppo2.py:122][0m #------------------------ Iteration 49 --------------------------#
[32m[20230113 19:45:45 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 19:45:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:45 @agent_ppo2.py:186][0m |          -0.0022 |          13.3949 |           2.2019 |
[32m[20230113 19:45:45 @agent_ppo2.py:186][0m |          -0.0081 |           9.4600 |           2.2000 |
[32m[20230113 19:45:45 @agent_ppo2.py:186][0m |          -0.0043 |           8.5624 |           2.1980 |
[32m[20230113 19:45:45 @agent_ppo2.py:186][0m |          -0.0088 |           7.8637 |           2.1969 |
[32m[20230113 19:45:45 @agent_ppo2.py:186][0m |          -0.0072 |           7.1427 |           2.1950 |
[32m[20230113 19:45:45 @agent_ppo2.py:186][0m |          -0.0135 |           6.7292 |           2.1946 |
[32m[20230113 19:45:45 @agent_ppo2.py:186][0m |          -0.0129 |           6.8006 |           2.1938 |
[32m[20230113 19:45:45 @agent_ppo2.py:186][0m |          -0.0104 |           6.2666 |           2.1926 |
[32m[20230113 19:45:45 @agent_ppo2.py:186][0m |          -0.0126 |           6.0873 |           2.1916 |
[32m[20230113 19:45:45 @agent_ppo2.py:186][0m |          -0.0154 |           5.8925 |           2.1911 |
[32m[20230113 19:45:45 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 19:45:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: -52.56
[32m[20230113 19:45:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 21.93
[32m[20230113 19:45:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 115.84
[32m[20230113 19:45:45 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 115.84
[32m[20230113 19:45:45 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 115.84
[32m[20230113 19:45:45 @agent_ppo2.py:144][0m Total time:       1.21 min
[32m[20230113 19:45:45 @agent_ppo2.py:146][0m 102400 total steps have happened
[32m[20230113 19:45:45 @agent_ppo2.py:122][0m #------------------------ Iteration 50 --------------------------#
[32m[20230113 19:45:46 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:45:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:46 @agent_ppo2.py:186][0m |          -0.0002 |           3.9549 |           2.1747 |
[32m[20230113 19:45:46 @agent_ppo2.py:186][0m |          -0.0035 |           2.2093 |           2.1735 |
[32m[20230113 19:45:46 @agent_ppo2.py:186][0m |          -0.0047 |           2.0896 |           2.1724 |
[32m[20230113 19:45:46 @agent_ppo2.py:186][0m |          -0.0054 |           2.0034 |           2.1719 |
[32m[20230113 19:45:46 @agent_ppo2.py:186][0m |          -0.0060 |           1.9514 |           2.1719 |
[32m[20230113 19:45:46 @agent_ppo2.py:186][0m |          -0.0065 |           1.9151 |           2.1733 |
[32m[20230113 19:45:46 @agent_ppo2.py:186][0m |          -0.0073 |           1.8791 |           2.1731 |
[32m[20230113 19:45:46 @agent_ppo2.py:186][0m |          -0.0072 |           1.8519 |           2.1737 |
[32m[20230113 19:45:46 @agent_ppo2.py:186][0m |          -0.0078 |           1.8072 |           2.1736 |
[32m[20230113 19:45:46 @agent_ppo2.py:186][0m |          -0.0082 |           1.7751 |           2.1740 |
[32m[20230113 19:45:46 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:45:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 18.95
[32m[20230113 19:45:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 27.09
[32m[20230113 19:45:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 122.07
[32m[20230113 19:45:47 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 122.07
[32m[20230113 19:45:47 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 122.07
[32m[20230113 19:45:47 @agent_ppo2.py:144][0m Total time:       1.24 min
[32m[20230113 19:45:47 @agent_ppo2.py:146][0m 104448 total steps have happened
[32m[20230113 19:45:47 @agent_ppo2.py:122][0m #------------------------ Iteration 51 --------------------------#
[32m[20230113 19:45:47 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:45:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:47 @agent_ppo2.py:186][0m |          -0.0001 |           1.1123 |           2.2466 |
[32m[20230113 19:45:47 @agent_ppo2.py:186][0m |          -0.0021 |           0.9558 |           2.2478 |
[32m[20230113 19:45:48 @agent_ppo2.py:186][0m |          -0.0043 |           0.9339 |           2.2498 |
[32m[20230113 19:45:48 @agent_ppo2.py:186][0m |          -0.0062 |           0.9108 |           2.2512 |
[32m[20230113 19:45:48 @agent_ppo2.py:186][0m |          -0.0075 |           0.9008 |           2.2515 |
[32m[20230113 19:45:48 @agent_ppo2.py:186][0m |          -0.0087 |           0.8883 |           2.2538 |
[32m[20230113 19:45:48 @agent_ppo2.py:186][0m |          -0.0096 |           0.8836 |           2.2557 |
[32m[20230113 19:45:48 @agent_ppo2.py:186][0m |          -0.0103 |           0.8744 |           2.2575 |
[32m[20230113 19:45:48 @agent_ppo2.py:186][0m |          -0.0111 |           0.8669 |           2.2592 |
[32m[20230113 19:45:48 @agent_ppo2.py:186][0m |          -0.0116 |           0.8608 |           2.2606 |
[32m[20230113 19:45:48 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:45:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 5.31
[32m[20230113 19:45:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 18.04
[32m[20230113 19:45:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 137.18
[32m[20230113 19:45:48 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 137.18
[32m[20230113 19:45:48 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 137.18
[32m[20230113 19:45:48 @agent_ppo2.py:144][0m Total time:       1.26 min
[32m[20230113 19:45:48 @agent_ppo2.py:146][0m 106496 total steps have happened
[32m[20230113 19:45:48 @agent_ppo2.py:122][0m #------------------------ Iteration 52 --------------------------#
[32m[20230113 19:45:49 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:45:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:49 @agent_ppo2.py:186][0m |          -0.0007 |           0.9493 |           2.2658 |
[32m[20230113 19:45:49 @agent_ppo2.py:186][0m |          -0.0053 |           0.8626 |           2.2647 |
[32m[20230113 19:45:49 @agent_ppo2.py:186][0m |          -0.0078 |           0.8281 |           2.2620 |
[32m[20230113 19:45:49 @agent_ppo2.py:186][0m |          -0.0088 |           0.8094 |           2.2611 |
[32m[20230113 19:45:49 @agent_ppo2.py:186][0m |          -0.0100 |           0.7943 |           2.2601 |
[32m[20230113 19:45:49 @agent_ppo2.py:186][0m |          -0.0103 |           0.7833 |           2.2595 |
[32m[20230113 19:45:49 @agent_ppo2.py:186][0m |          -0.0110 |           0.7725 |           2.2603 |
[32m[20230113 19:45:49 @agent_ppo2.py:186][0m |          -0.0115 |           0.7633 |           2.2607 |
[32m[20230113 19:45:49 @agent_ppo2.py:186][0m |          -0.0119 |           0.7551 |           2.2609 |
[32m[20230113 19:45:49 @agent_ppo2.py:186][0m |          -0.0125 |           0.7465 |           2.2609 |
[32m[20230113 19:45:49 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 27.41
[32m[20230113 19:45:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 30.48
[32m[20230113 19:45:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 136.09
[32m[20230113 19:45:50 @agent_ppo2.py:144][0m Total time:       1.29 min
[32m[20230113 19:45:50 @agent_ppo2.py:146][0m 108544 total steps have happened
[32m[20230113 19:45:50 @agent_ppo2.py:122][0m #------------------------ Iteration 53 --------------------------#
[32m[20230113 19:45:50 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 19:45:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:50 @agent_ppo2.py:186][0m |          -0.0040 |           7.9911 |           2.1878 |
[32m[20230113 19:45:50 @agent_ppo2.py:186][0m |          -0.0058 |           3.5867 |           2.1870 |
[32m[20230113 19:45:50 @agent_ppo2.py:186][0m |           0.0050 |           4.1159 |           2.1864 |
[32m[20230113 19:45:50 @agent_ppo2.py:186][0m |          -0.0106 |           2.7084 |           2.1846 |
[32m[20230113 19:45:50 @agent_ppo2.py:186][0m |          -0.0132 |           2.4817 |           2.1842 |
[32m[20230113 19:45:50 @agent_ppo2.py:186][0m |          -0.0129 |           2.4026 |           2.1837 |
[32m[20230113 19:45:50 @agent_ppo2.py:186][0m |          -0.0115 |           2.3001 |           2.1827 |
[32m[20230113 19:45:50 @agent_ppo2.py:186][0m |          -0.0149 |           2.2412 |           2.1821 |
[32m[20230113 19:45:50 @agent_ppo2.py:186][0m |          -0.0124 |           2.2062 |           2.1815 |
[32m[20230113 19:45:50 @agent_ppo2.py:186][0m |          -0.0140 |           2.1581 |           2.1813 |
[32m[20230113 19:45:50 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 19:45:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: -11.67
[32m[20230113 19:45:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 42.02
[32m[20230113 19:45:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 122.29
[32m[20230113 19:45:51 @agent_ppo2.py:144][0m Total time:       1.31 min
[32m[20230113 19:45:51 @agent_ppo2.py:146][0m 110592 total steps have happened
[32m[20230113 19:45:51 @agent_ppo2.py:122][0m #------------------------ Iteration 54 --------------------------#
[32m[20230113 19:45:51 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:45:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:52 @agent_ppo2.py:186][0m |           0.0004 |           1.8268 |           2.2770 |
[32m[20230113 19:45:52 @agent_ppo2.py:186][0m |          -0.0032 |           1.2857 |           2.2744 |
[32m[20230113 19:45:52 @agent_ppo2.py:186][0m |          -0.0050 |           1.2253 |           2.2738 |
[32m[20230113 19:45:52 @agent_ppo2.py:186][0m |          -0.0068 |           1.1910 |           2.2740 |
[32m[20230113 19:45:52 @agent_ppo2.py:186][0m |          -0.0080 |           1.1677 |           2.2736 |
[32m[20230113 19:45:52 @agent_ppo2.py:186][0m |          -0.0088 |           1.1490 |           2.2738 |
[32m[20230113 19:45:52 @agent_ppo2.py:186][0m |          -0.0097 |           1.1359 |           2.2746 |
[32m[20230113 19:45:52 @agent_ppo2.py:186][0m |          -0.0099 |           1.1261 |           2.2755 |
[32m[20230113 19:45:52 @agent_ppo2.py:186][0m |          -0.0106 |           1.1225 |           2.2765 |
[32m[20230113 19:45:52 @agent_ppo2.py:186][0m |          -0.0113 |           1.1107 |           2.2774 |
[32m[20230113 19:45:52 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:45:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 51.14
[32m[20230113 19:45:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 54.88
[32m[20230113 19:45:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 122.83
[32m[20230113 19:45:52 @agent_ppo2.py:144][0m Total time:       1.33 min
[32m[20230113 19:45:52 @agent_ppo2.py:146][0m 112640 total steps have happened
[32m[20230113 19:45:52 @agent_ppo2.py:122][0m #------------------------ Iteration 55 --------------------------#
[32m[20230113 19:45:53 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:45:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:53 @agent_ppo2.py:186][0m |          -0.0038 |           6.1486 |           2.3146 |
[32m[20230113 19:45:53 @agent_ppo2.py:186][0m |          -0.0084 |           2.1647 |           2.3100 |
[32m[20230113 19:45:53 @agent_ppo2.py:186][0m |          -0.0093 |           1.6950 |           2.3091 |
[32m[20230113 19:45:53 @agent_ppo2.py:186][0m |          -0.0104 |           1.5341 |           2.3077 |
[32m[20230113 19:45:53 @agent_ppo2.py:186][0m |          -0.0105 |           1.4982 |           2.3064 |
[32m[20230113 19:45:53 @agent_ppo2.py:186][0m |          -0.0116 |           1.4350 |           2.3046 |
[32m[20230113 19:45:53 @agent_ppo2.py:186][0m |          -0.0115 |           1.3647 |           2.3036 |
[32m[20230113 19:45:53 @agent_ppo2.py:186][0m |          -0.0123 |           1.3578 |           2.3025 |
[32m[20230113 19:45:53 @agent_ppo2.py:186][0m |          -0.0125 |           1.3298 |           2.3020 |
[32m[20230113 19:45:53 @agent_ppo2.py:186][0m |          -0.0127 |           1.3259 |           2.3010 |
[32m[20230113 19:45:53 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:45:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 2.20
[32m[20230113 19:45:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 66.16
[32m[20230113 19:45:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 128.49
[32m[20230113 19:45:54 @agent_ppo2.py:144][0m Total time:       1.35 min
[32m[20230113 19:45:54 @agent_ppo2.py:146][0m 114688 total steps have happened
[32m[20230113 19:45:54 @agent_ppo2.py:122][0m #------------------------ Iteration 56 --------------------------#
[32m[20230113 19:45:54 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:45:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:54 @agent_ppo2.py:186][0m |          -0.0010 |           1.1199 |           2.2645 |
[32m[20230113 19:45:54 @agent_ppo2.py:186][0m |          -0.0050 |           0.9557 |           2.2633 |
[32m[20230113 19:45:55 @agent_ppo2.py:186][0m |          -0.0070 |           0.8987 |           2.2628 |
[32m[20230113 19:45:55 @agent_ppo2.py:186][0m |          -0.0081 |           0.8767 |           2.2617 |
[32m[20230113 19:45:55 @agent_ppo2.py:186][0m |          -0.0095 |           0.8577 |           2.2625 |
[32m[20230113 19:45:55 @agent_ppo2.py:186][0m |          -0.0105 |           0.8469 |           2.2621 |
[32m[20230113 19:45:55 @agent_ppo2.py:186][0m |          -0.0109 |           0.8336 |           2.2620 |
[32m[20230113 19:45:55 @agent_ppo2.py:186][0m |          -0.0117 |           0.8234 |           2.2621 |
[32m[20230113 19:45:55 @agent_ppo2.py:186][0m |          -0.0121 |           0.8135 |           2.2622 |
[32m[20230113 19:45:55 @agent_ppo2.py:186][0m |          -0.0126 |           0.8055 |           2.2625 |
[32m[20230113 19:45:55 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:45:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 59.38
[32m[20230113 19:45:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 71.56
[32m[20230113 19:45:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 121.85
[32m[20230113 19:45:55 @agent_ppo2.py:144][0m Total time:       1.38 min
[32m[20230113 19:45:55 @agent_ppo2.py:146][0m 116736 total steps have happened
[32m[20230113 19:45:55 @agent_ppo2.py:122][0m #------------------------ Iteration 57 --------------------------#
[32m[20230113 19:45:56 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:45:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:56 @agent_ppo2.py:186][0m |           0.0021 |           5.5042 |           2.2595 |
[32m[20230113 19:45:56 @agent_ppo2.py:186][0m |          -0.0054 |           3.9179 |           2.2571 |
[32m[20230113 19:45:56 @agent_ppo2.py:186][0m |          -0.0099 |           3.4517 |           2.2564 |
[32m[20230113 19:45:56 @agent_ppo2.py:186][0m |          -0.0113 |           3.3186 |           2.2551 |
[32m[20230113 19:45:56 @agent_ppo2.py:186][0m |          -0.0075 |           3.2391 |           2.2551 |
[32m[20230113 19:45:56 @agent_ppo2.py:186][0m |          -0.0108 |           3.2626 |           2.2536 |
[32m[20230113 19:45:56 @agent_ppo2.py:186][0m |          -0.0140 |           3.1124 |           2.2545 |
[32m[20230113 19:45:56 @agent_ppo2.py:186][0m |          -0.0060 |           3.1125 |           2.2541 |
[32m[20230113 19:45:56 @agent_ppo2.py:186][0m |          -0.0150 |           3.0554 |           2.2534 |
[32m[20230113 19:45:56 @agent_ppo2.py:186][0m |          -0.0138 |           2.9146 |           2.2539 |
[32m[20230113 19:45:56 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 19:45:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 10.39
[32m[20230113 19:45:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 68.55
[32m[20230113 19:45:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 135.02
[32m[20230113 19:45:57 @agent_ppo2.py:144][0m Total time:       1.40 min
[32m[20230113 19:45:57 @agent_ppo2.py:146][0m 118784 total steps have happened
[32m[20230113 19:45:57 @agent_ppo2.py:122][0m #------------------------ Iteration 58 --------------------------#
[32m[20230113 19:45:57 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:45:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:57 @agent_ppo2.py:186][0m |          -0.0003 |           1.9268 |           2.2847 |
[32m[20230113 19:45:57 @agent_ppo2.py:186][0m |          -0.0048 |           1.3461 |           2.2831 |
[32m[20230113 19:45:57 @agent_ppo2.py:186][0m |          -0.0078 |           1.2293 |           2.2808 |
[32m[20230113 19:45:57 @agent_ppo2.py:186][0m |          -0.0091 |           1.2001 |           2.2795 |
[32m[20230113 19:45:58 @agent_ppo2.py:186][0m |          -0.0100 |           1.1549 |           2.2776 |
[32m[20230113 19:45:58 @agent_ppo2.py:186][0m |          -0.0107 |           1.1396 |           2.2761 |
[32m[20230113 19:45:58 @agent_ppo2.py:186][0m |          -0.0111 |           1.1186 |           2.2772 |
[32m[20230113 19:45:58 @agent_ppo2.py:186][0m |          -0.0120 |           1.0966 |           2.2765 |
[32m[20230113 19:45:58 @agent_ppo2.py:186][0m |          -0.0123 |           1.0777 |           2.2765 |
[32m[20230113 19:45:58 @agent_ppo2.py:186][0m |          -0.0124 |           1.0648 |           2.2766 |
[32m[20230113 19:45:58 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:45:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 50.28
[32m[20230113 19:45:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 56.42
[32m[20230113 19:45:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 116.62
[32m[20230113 19:45:58 @agent_ppo2.py:144][0m Total time:       1.43 min
[32m[20230113 19:45:58 @agent_ppo2.py:146][0m 120832 total steps have happened
[32m[20230113 19:45:58 @agent_ppo2.py:122][0m #------------------------ Iteration 59 --------------------------#
[32m[20230113 19:45:59 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 19:45:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:45:59 @agent_ppo2.py:186][0m |          -0.0018 |          24.1935 |           2.2617 |
[32m[20230113 19:45:59 @agent_ppo2.py:186][0m |          -0.0015 |           8.6163 |           2.2625 |
[32m[20230113 19:45:59 @agent_ppo2.py:186][0m |          -0.0041 |           4.9520 |           2.2627 |
[32m[20230113 19:45:59 @agent_ppo2.py:186][0m |          -0.0067 |           4.2678 |           2.2630 |
[32m[20230113 19:45:59 @agent_ppo2.py:186][0m |          -0.0064 |           3.7016 |           2.2629 |
[32m[20230113 19:45:59 @agent_ppo2.py:186][0m |          -0.0037 |           3.3830 |           2.2618 |
[32m[20230113 19:45:59 @agent_ppo2.py:186][0m |          -0.0099 |           3.0424 |           2.2623 |
[32m[20230113 19:45:59 @agent_ppo2.py:186][0m |          -0.0082 |           2.7702 |           2.2620 |
[32m[20230113 19:45:59 @agent_ppo2.py:186][0m |          -0.0040 |           2.5030 |           2.2611 |
[32m[20230113 19:45:59 @agent_ppo2.py:186][0m |          -0.0098 |           2.3709 |           2.2613 |
[32m[20230113 19:45:59 @agent_ppo2.py:131][0m Policy update time: 0.34 s
[32m[20230113 19:45:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: -7.72
[32m[20230113 19:45:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 72.36
[32m[20230113 19:45:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 120.89
[32m[20230113 19:45:59 @agent_ppo2.py:144][0m Total time:       1.45 min
[32m[20230113 19:45:59 @agent_ppo2.py:146][0m 122880 total steps have happened
[32m[20230113 19:45:59 @agent_ppo2.py:122][0m #------------------------ Iteration 60 --------------------------#
[32m[20230113 19:46:00 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:46:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:00 @agent_ppo2.py:186][0m |          -0.0012 |          11.6127 |           2.2699 |
[32m[20230113 19:46:00 @agent_ppo2.py:186][0m |          -0.0047 |           6.2262 |           2.2683 |
[32m[20230113 19:46:00 @agent_ppo2.py:186][0m |          -0.0093 |           4.9532 |           2.2667 |
[32m[20230113 19:46:00 @agent_ppo2.py:186][0m |          -0.0109 |           4.6940 |           2.2655 |
[32m[20230113 19:46:00 @agent_ppo2.py:186][0m |          -0.0113 |           4.5413 |           2.2658 |
[32m[20230113 19:46:00 @agent_ppo2.py:186][0m |          -0.0135 |           4.5125 |           2.2644 |
[32m[20230113 19:46:00 @agent_ppo2.py:186][0m |          -0.0132 |           4.4175 |           2.2642 |
[32m[20230113 19:46:00 @agent_ppo2.py:186][0m |          -0.0146 |           4.3198 |           2.2642 |
[32m[20230113 19:46:00 @agent_ppo2.py:186][0m |          -0.0143 |           4.2270 |           2.2641 |
[32m[20230113 19:46:00 @agent_ppo2.py:186][0m |          -0.0153 |           4.1629 |           2.2641 |
[32m[20230113 19:46:00 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:46:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 22.64
[32m[20230113 19:46:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 90.43
[32m[20230113 19:46:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 126.74
[32m[20230113 19:46:01 @agent_ppo2.py:144][0m Total time:       1.47 min
[32m[20230113 19:46:01 @agent_ppo2.py:146][0m 124928 total steps have happened
[32m[20230113 19:46:01 @agent_ppo2.py:122][0m #------------------------ Iteration 61 --------------------------#
[32m[20230113 19:46:01 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:46:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:01 @agent_ppo2.py:186][0m |          -0.0022 |           2.8537 |           2.2952 |
[32m[20230113 19:46:01 @agent_ppo2.py:186][0m |          -0.0071 |           2.5391 |           2.2962 |
[32m[20230113 19:46:01 @agent_ppo2.py:186][0m |          -0.0094 |           2.4083 |           2.2973 |
[32m[20230113 19:46:02 @agent_ppo2.py:186][0m |          -0.0106 |           2.3180 |           2.2994 |
[32m[20230113 19:46:02 @agent_ppo2.py:186][0m |          -0.0113 |           2.2490 |           2.3005 |
[32m[20230113 19:46:02 @agent_ppo2.py:186][0m |          -0.0116 |           2.1874 |           2.3011 |
[32m[20230113 19:46:02 @agent_ppo2.py:186][0m |          -0.0117 |           2.1611 |           2.3023 |
[32m[20230113 19:46:02 @agent_ppo2.py:186][0m |          -0.0130 |           2.1303 |           2.3038 |
[32m[20230113 19:46:02 @agent_ppo2.py:186][0m |          -0.0131 |           2.0926 |           2.3051 |
[32m[20230113 19:46:02 @agent_ppo2.py:186][0m |          -0.0136 |           2.0712 |           2.3064 |
[32m[20230113 19:46:02 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:46:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 64.78
[32m[20230113 19:46:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 65.92
[32m[20230113 19:46:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 108.70
[32m[20230113 19:46:02 @agent_ppo2.py:144][0m Total time:       1.50 min
[32m[20230113 19:46:02 @agent_ppo2.py:146][0m 126976 total steps have happened
[32m[20230113 19:46:02 @agent_ppo2.py:122][0m #------------------------ Iteration 62 --------------------------#
[32m[20230113 19:46:03 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:03 @agent_ppo2.py:186][0m |          -0.0009 |           1.8599 |           2.3602 |
[32m[20230113 19:46:03 @agent_ppo2.py:186][0m |          -0.0049 |           1.6532 |           2.3547 |
[32m[20230113 19:46:03 @agent_ppo2.py:186][0m |          -0.0074 |           1.6047 |           2.3564 |
[32m[20230113 19:46:03 @agent_ppo2.py:186][0m |          -0.0089 |           1.5637 |           2.3536 |
[32m[20230113 19:46:03 @agent_ppo2.py:186][0m |          -0.0101 |           1.5270 |           2.3553 |
[32m[20230113 19:46:03 @agent_ppo2.py:186][0m |          -0.0110 |           1.4967 |           2.3575 |
[32m[20230113 19:46:03 @agent_ppo2.py:186][0m |          -0.0117 |           1.4778 |           2.3573 |
[32m[20230113 19:46:03 @agent_ppo2.py:186][0m |          -0.0122 |           1.4596 |           2.3592 |
[32m[20230113 19:46:03 @agent_ppo2.py:186][0m |          -0.0127 |           1.4399 |           2.3586 |
[32m[20230113 19:46:03 @agent_ppo2.py:186][0m |          -0.0135 |           1.4255 |           2.3617 |
[32m[20230113 19:46:03 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 95.87
[32m[20230113 19:46:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 100.60
[32m[20230113 19:46:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 111.55
[32m[20230113 19:46:04 @agent_ppo2.py:144][0m Total time:       1.52 min
[32m[20230113 19:46:04 @agent_ppo2.py:146][0m 129024 total steps have happened
[32m[20230113 19:46:04 @agent_ppo2.py:122][0m #------------------------ Iteration 63 --------------------------#
[32m[20230113 19:46:04 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:46:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:04 @agent_ppo2.py:186][0m |          -0.0022 |           1.8041 |           2.3483 |
[32m[20230113 19:46:04 @agent_ppo2.py:186][0m |          -0.0075 |           1.6526 |           2.3484 |
[32m[20230113 19:46:04 @agent_ppo2.py:186][0m |          -0.0098 |           1.6190 |           2.3480 |
[32m[20230113 19:46:04 @agent_ppo2.py:186][0m |          -0.0114 |           1.5993 |           2.3489 |
[32m[20230113 19:46:04 @agent_ppo2.py:186][0m |          -0.0123 |           1.5735 |           2.3488 |
[32m[20230113 19:46:04 @agent_ppo2.py:186][0m |          -0.0133 |           1.5486 |           2.3493 |
[32m[20230113 19:46:05 @agent_ppo2.py:186][0m |          -0.0138 |           1.5359 |           2.3504 |
[32m[20230113 19:46:05 @agent_ppo2.py:186][0m |          -0.0143 |           1.5157 |           2.3512 |
[32m[20230113 19:46:05 @agent_ppo2.py:186][0m |          -0.0149 |           1.5045 |           2.3528 |
[32m[20230113 19:46:05 @agent_ppo2.py:186][0m |          -0.0154 |           1.4955 |           2.3525 |
[32m[20230113 19:46:05 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 89.05
[32m[20230113 19:46:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 107.31
[32m[20230113 19:46:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 121.27
[32m[20230113 19:46:05 @agent_ppo2.py:144][0m Total time:       1.54 min
[32m[20230113 19:46:05 @agent_ppo2.py:146][0m 131072 total steps have happened
[32m[20230113 19:46:05 @agent_ppo2.py:122][0m #------------------------ Iteration 64 --------------------------#
[32m[20230113 19:46:06 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:06 @agent_ppo2.py:186][0m |           0.0000 |          13.6339 |           2.3461 |
[32m[20230113 19:46:06 @agent_ppo2.py:186][0m |          -0.0030 |           4.8883 |           2.3451 |
[32m[20230113 19:46:06 @agent_ppo2.py:186][0m |          -0.0084 |           4.3334 |           2.3441 |
[32m[20230113 19:46:06 @agent_ppo2.py:186][0m |          -0.0057 |           4.1041 |           2.3423 |
[32m[20230113 19:46:06 @agent_ppo2.py:186][0m |          -0.0081 |           3.6102 |           2.3442 |
[32m[20230113 19:46:06 @agent_ppo2.py:186][0m |          -0.0082 |           3.5250 |           2.3444 |
[32m[20230113 19:46:06 @agent_ppo2.py:186][0m |          -0.0081 |           3.4065 |           2.3459 |
[32m[20230113 19:46:06 @agent_ppo2.py:186][0m |          -0.0092 |           3.3137 |           2.3459 |
[32m[20230113 19:46:06 @agent_ppo2.py:186][0m |          -0.0103 |           3.2371 |           2.3464 |
[32m[20230113 19:46:06 @agent_ppo2.py:186][0m |          -0.0110 |           3.1743 |           2.3470 |
[32m[20230113 19:46:06 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 68.56
[32m[20230113 19:46:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 133.09
[32m[20230113 19:46:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 131.39
[32m[20230113 19:46:07 @agent_ppo2.py:144][0m Total time:       1.57 min
[32m[20230113 19:46:07 @agent_ppo2.py:146][0m 133120 total steps have happened
[32m[20230113 19:46:07 @agent_ppo2.py:122][0m #------------------------ Iteration 65 --------------------------#
[32m[20230113 19:46:07 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 19:46:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:07 @agent_ppo2.py:186][0m |           0.0007 |          16.5285 |           2.3582 |
[32m[20230113 19:46:07 @agent_ppo2.py:186][0m |          -0.0025 |           3.8582 |           2.3566 |
[32m[20230113 19:46:07 @agent_ppo2.py:186][0m |          -0.0049 |           2.9509 |           2.3564 |
[32m[20230113 19:46:07 @agent_ppo2.py:186][0m |          -0.0060 |           2.7451 |           2.3555 |
[32m[20230113 19:46:07 @agent_ppo2.py:186][0m |          -0.0065 |           2.6096 |           2.3554 |
[32m[20230113 19:46:07 @agent_ppo2.py:186][0m |          -0.0073 |           2.5710 |           2.3547 |
[32m[20230113 19:46:07 @agent_ppo2.py:186][0m |          -0.0082 |           2.4733 |           2.3548 |
[32m[20230113 19:46:07 @agent_ppo2.py:186][0m |          -0.0079 |           2.4093 |           2.3533 |
[32m[20230113 19:46:07 @agent_ppo2.py:186][0m |          -0.0082 |           2.3590 |           2.3535 |
[32m[20230113 19:46:07 @agent_ppo2.py:186][0m |          -0.0091 |           2.3100 |           2.3524 |
[32m[20230113 19:46:07 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 19:46:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 52.24
[32m[20230113 19:46:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 119.32
[32m[20230113 19:46:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 157.36
[32m[20230113 19:46:08 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 157.36
[32m[20230113 19:46:08 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 157.36
[32m[20230113 19:46:08 @agent_ppo2.py:144][0m Total time:       1.59 min
[32m[20230113 19:46:08 @agent_ppo2.py:146][0m 135168 total steps have happened
[32m[20230113 19:46:08 @agent_ppo2.py:122][0m #------------------------ Iteration 66 --------------------------#
[32m[20230113 19:46:08 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:46:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:09 @agent_ppo2.py:186][0m |           0.0002 |          43.6498 |           2.3715 |
[32m[20230113 19:46:09 @agent_ppo2.py:186][0m |          -0.0005 |          21.6020 |           2.3713 |
[32m[20230113 19:46:09 @agent_ppo2.py:186][0m |          -0.0020 |          17.2179 |           2.3707 |
[32m[20230113 19:46:09 @agent_ppo2.py:186][0m |          -0.0039 |          14.9687 |           2.3694 |
[32m[20230113 19:46:09 @agent_ppo2.py:186][0m |          -0.0068 |          12.9147 |           2.3686 |
[32m[20230113 19:46:09 @agent_ppo2.py:186][0m |          -0.0071 |          11.5267 |           2.3677 |
[32m[20230113 19:46:09 @agent_ppo2.py:186][0m |          -0.0084 |          10.4512 |           2.3662 |
[32m[20230113 19:46:09 @agent_ppo2.py:186][0m |          -0.0084 |           9.2643 |           2.3658 |
[32m[20230113 19:46:09 @agent_ppo2.py:186][0m |          -0.0064 |           8.2712 |           2.3652 |
[32m[20230113 19:46:09 @agent_ppo2.py:186][0m |          -0.0065 |           7.7190 |           2.3646 |
[32m[20230113 19:46:09 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 19:46:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 2.50
[32m[20230113 19:46:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 99.59
[32m[20230113 19:46:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 126.84
[32m[20230113 19:46:09 @agent_ppo2.py:144][0m Total time:       1.61 min
[32m[20230113 19:46:09 @agent_ppo2.py:146][0m 137216 total steps have happened
[32m[20230113 19:46:09 @agent_ppo2.py:122][0m #------------------------ Iteration 67 --------------------------#
[32m[20230113 19:46:10 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:46:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:10 @agent_ppo2.py:186][0m |          -0.0010 |          13.3824 |           2.3991 |
[32m[20230113 19:46:10 @agent_ppo2.py:186][0m |          -0.0052 |           5.1207 |           2.3973 |
[32m[20230113 19:46:10 @agent_ppo2.py:186][0m |          -0.0094 |           4.2991 |           2.3961 |
[32m[20230113 19:46:10 @agent_ppo2.py:186][0m |          -0.0091 |           3.8388 |           2.3972 |
[32m[20230113 19:46:10 @agent_ppo2.py:186][0m |          -0.0114 |           3.6641 |           2.3966 |
[32m[20230113 19:46:10 @agent_ppo2.py:186][0m |          -0.0115 |           3.3874 |           2.3957 |
[32m[20230113 19:46:10 @agent_ppo2.py:186][0m |          -0.0129 |           3.2962 |           2.3961 |
[32m[20230113 19:46:10 @agent_ppo2.py:186][0m |          -0.0128 |           3.1268 |           2.3956 |
[32m[20230113 19:46:10 @agent_ppo2.py:186][0m |          -0.0130 |           3.0565 |           2.3960 |
[32m[20230113 19:46:10 @agent_ppo2.py:186][0m |          -0.0117 |           2.9850 |           2.3957 |
[32m[20230113 19:46:10 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 19:46:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 69.40
[32m[20230113 19:46:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 141.28
[32m[20230113 19:46:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 73.18
[32m[20230113 19:46:11 @agent_ppo2.py:144][0m Total time:       1.64 min
[32m[20230113 19:46:11 @agent_ppo2.py:146][0m 139264 total steps have happened
[32m[20230113 19:46:11 @agent_ppo2.py:122][0m #------------------------ Iteration 68 --------------------------#
[32m[20230113 19:46:11 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 19:46:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:11 @agent_ppo2.py:186][0m |           0.0037 |          25.8312 |           2.3709 |
[32m[20230113 19:46:11 @agent_ppo2.py:186][0m |          -0.0017 |          18.1596 |           2.3703 |
[32m[20230113 19:46:11 @agent_ppo2.py:186][0m |          -0.0052 |          15.6741 |           2.3685 |
[32m[20230113 19:46:11 @agent_ppo2.py:186][0m |          -0.0097 |          14.1880 |           2.3680 |
[32m[20230113 19:46:11 @agent_ppo2.py:186][0m |          -0.0081 |          13.8351 |           2.3666 |
[32m[20230113 19:46:11 @agent_ppo2.py:186][0m |          -0.0051 |          13.0693 |           2.3662 |
[32m[20230113 19:46:11 @agent_ppo2.py:186][0m |          -0.0093 |          12.4473 |           2.3655 |
[32m[20230113 19:46:11 @agent_ppo2.py:186][0m |          -0.0094 |          12.0060 |           2.3651 |
[32m[20230113 19:46:11 @agent_ppo2.py:186][0m |          -0.0109 |          11.6680 |           2.3652 |
[32m[20230113 19:46:11 @agent_ppo2.py:186][0m |          -0.0041 |          11.9506 |           2.3650 |
[32m[20230113 19:46:11 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 19:46:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: -16.08
[32m[20230113 19:46:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 105.75
[32m[20230113 19:46:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 151.09
[32m[20230113 19:46:12 @agent_ppo2.py:144][0m Total time:       1.66 min
[32m[20230113 19:46:12 @agent_ppo2.py:146][0m 141312 total steps have happened
[32m[20230113 19:46:12 @agent_ppo2.py:122][0m #------------------------ Iteration 69 --------------------------#
[32m[20230113 19:46:12 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:46:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:12 @agent_ppo2.py:186][0m |          -0.0003 |           5.5212 |           2.4466 |
[32m[20230113 19:46:13 @agent_ppo2.py:186][0m |          -0.0047 |           3.4757 |           2.4476 |
[32m[20230113 19:46:13 @agent_ppo2.py:186][0m |          -0.0067 |           3.2183 |           2.4469 |
[32m[20230113 19:46:13 @agent_ppo2.py:186][0m |          -0.0085 |           3.0841 |           2.4473 |
[32m[20230113 19:46:13 @agent_ppo2.py:186][0m |          -0.0090 |           2.9763 |           2.4477 |
[32m[20230113 19:46:13 @agent_ppo2.py:186][0m |          -0.0088 |           2.9264 |           2.4485 |
[32m[20230113 19:46:13 @agent_ppo2.py:186][0m |          -0.0099 |           2.8664 |           2.4493 |
[32m[20230113 19:46:13 @agent_ppo2.py:186][0m |          -0.0101 |           2.7990 |           2.4505 |
[32m[20230113 19:46:13 @agent_ppo2.py:186][0m |          -0.0105 |           2.7694 |           2.4520 |
[32m[20230113 19:46:13 @agent_ppo2.py:186][0m |          -0.0109 |           2.7297 |           2.4531 |
[32m[20230113 19:46:13 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 84.53
[32m[20230113 19:46:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 87.24
[32m[20230113 19:46:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 155.04
[32m[20230113 19:46:13 @agent_ppo2.py:144][0m Total time:       1.68 min
[32m[20230113 19:46:13 @agent_ppo2.py:146][0m 143360 total steps have happened
[32m[20230113 19:46:13 @agent_ppo2.py:122][0m #------------------------ Iteration 70 --------------------------#
[32m[20230113 19:46:14 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:46:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:14 @agent_ppo2.py:186][0m |           0.0003 |           2.2211 |           2.4791 |
[32m[20230113 19:46:14 @agent_ppo2.py:186][0m |          -0.0017 |           1.8304 |           2.4777 |
[32m[20230113 19:46:14 @agent_ppo2.py:186][0m |          -0.0037 |           1.7528 |           2.4760 |
[32m[20230113 19:46:14 @agent_ppo2.py:186][0m |          -0.0050 |           1.7132 |           2.4744 |
[32m[20230113 19:46:14 @agent_ppo2.py:186][0m |          -0.0066 |           1.6829 |           2.4745 |
[32m[20230113 19:46:14 @agent_ppo2.py:186][0m |          -0.0074 |           1.6621 |           2.4741 |
[32m[20230113 19:46:14 @agent_ppo2.py:186][0m |          -0.0079 |           1.6422 |           2.4735 |
[32m[20230113 19:46:14 @agent_ppo2.py:186][0m |          -0.0089 |           1.6268 |           2.4735 |
[32m[20230113 19:46:14 @agent_ppo2.py:186][0m |          -0.0092 |           1.6076 |           2.4736 |
[32m[20230113 19:46:14 @agent_ppo2.py:186][0m |          -0.0098 |           1.6026 |           2.4737 |
[32m[20230113 19:46:14 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 102.76
[32m[20230113 19:46:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 124.54
[32m[20230113 19:46:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 147.10
[32m[20230113 19:46:15 @agent_ppo2.py:144][0m Total time:       1.70 min
[32m[20230113 19:46:15 @agent_ppo2.py:146][0m 145408 total steps have happened
[32m[20230113 19:46:15 @agent_ppo2.py:122][0m #------------------------ Iteration 71 --------------------------#
[32m[20230113 19:46:15 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:15 @agent_ppo2.py:186][0m |          -0.0008 |           1.5999 |           2.4551 |
[32m[20230113 19:46:15 @agent_ppo2.py:186][0m |          -0.0044 |           1.4675 |           2.4553 |
[32m[20230113 19:46:15 @agent_ppo2.py:186][0m |          -0.0063 |           1.4220 |           2.4547 |
[32m[20230113 19:46:15 @agent_ppo2.py:186][0m |          -0.0076 |           1.3892 |           2.4539 |
[32m[20230113 19:46:16 @agent_ppo2.py:186][0m |          -0.0090 |           1.3678 |           2.4545 |
[32m[20230113 19:46:16 @agent_ppo2.py:186][0m |          -0.0096 |           1.3507 |           2.4539 |
[32m[20230113 19:46:16 @agent_ppo2.py:186][0m |          -0.0104 |           1.3372 |           2.4545 |
[32m[20230113 19:46:16 @agent_ppo2.py:186][0m |          -0.0111 |           1.3270 |           2.4543 |
[32m[20230113 19:46:16 @agent_ppo2.py:186][0m |          -0.0118 |           1.3130 |           2.4550 |
[32m[20230113 19:46:16 @agent_ppo2.py:186][0m |          -0.0122 |           1.3000 |           2.4550 |
[32m[20230113 19:46:16 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 105.37
[32m[20230113 19:46:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 109.83
[32m[20230113 19:46:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 157.22
[32m[20230113 19:46:16 @agent_ppo2.py:144][0m Total time:       1.73 min
[32m[20230113 19:46:16 @agent_ppo2.py:146][0m 147456 total steps have happened
[32m[20230113 19:46:16 @agent_ppo2.py:122][0m #------------------------ Iteration 72 --------------------------#
[32m[20230113 19:46:17 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 19:46:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:17 @agent_ppo2.py:186][0m |           0.0027 |          11.2455 |           2.4576 |
[32m[20230113 19:46:17 @agent_ppo2.py:186][0m |          -0.0025 |           7.5339 |           2.4571 |
[32m[20230113 19:46:17 @agent_ppo2.py:186][0m |          -0.0046 |           6.5049 |           2.4568 |
[32m[20230113 19:46:17 @agent_ppo2.py:186][0m |          -0.0067 |           5.9665 |           2.4556 |
[32m[20230113 19:46:17 @agent_ppo2.py:186][0m |          -0.0073 |           5.5639 |           2.4547 |
[32m[20230113 19:46:17 @agent_ppo2.py:186][0m |          -0.0085 |           5.2092 |           2.4540 |
[32m[20230113 19:46:17 @agent_ppo2.py:186][0m |          -0.0089 |           4.9311 |           2.4527 |
[32m[20230113 19:46:17 @agent_ppo2.py:186][0m |          -0.0102 |           4.6278 |           2.4527 |
[32m[20230113 19:46:17 @agent_ppo2.py:186][0m |          -0.0107 |           4.4066 |           2.4521 |
[32m[20230113 19:46:17 @agent_ppo2.py:186][0m |          -0.0093 |           4.1227 |           2.4517 |
[32m[20230113 19:46:17 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 19:46:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 37.14
[32m[20230113 19:46:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 111.10
[32m[20230113 19:46:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 164.18
[32m[20230113 19:46:17 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 164.18
[32m[20230113 19:46:17 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 164.18
[32m[20230113 19:46:17 @agent_ppo2.py:144][0m Total time:       1.75 min
[32m[20230113 19:46:17 @agent_ppo2.py:146][0m 149504 total steps have happened
[32m[20230113 19:46:17 @agent_ppo2.py:122][0m #------------------------ Iteration 73 --------------------------#
[32m[20230113 19:46:18 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 19:46:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:18 @agent_ppo2.py:186][0m |           0.0013 |           9.3017 |           2.4547 |
[32m[20230113 19:46:18 @agent_ppo2.py:186][0m |          -0.0019 |           5.8959 |           2.4542 |
[32m[20230113 19:46:18 @agent_ppo2.py:186][0m |          -0.0064 |           5.6105 |           2.4526 |
[32m[20230113 19:46:18 @agent_ppo2.py:186][0m |          -0.0062 |           5.1857 |           2.4525 |
[32m[20230113 19:46:18 @agent_ppo2.py:186][0m |          -0.0049 |           4.9615 |           2.4511 |
[32m[20230113 19:46:18 @agent_ppo2.py:186][0m |          -0.0090 |           4.8620 |           2.4508 |
[32m[20230113 19:46:18 @agent_ppo2.py:186][0m |          -0.0080 |           4.7043 |           2.4504 |
[32m[20230113 19:46:18 @agent_ppo2.py:186][0m |          -0.0105 |           4.6020 |           2.4511 |
[32m[20230113 19:46:18 @agent_ppo2.py:186][0m |          -0.0117 |           4.5967 |           2.4501 |
[32m[20230113 19:46:18 @agent_ppo2.py:186][0m |          -0.0102 |           4.5305 |           2.4485 |
[32m[20230113 19:46:18 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 19:46:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 51.59
[32m[20230113 19:46:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 123.18
[32m[20230113 19:46:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 160.42
[32m[20230113 19:46:19 @agent_ppo2.py:144][0m Total time:       1.77 min
[32m[20230113 19:46:19 @agent_ppo2.py:146][0m 151552 total steps have happened
[32m[20230113 19:46:19 @agent_ppo2.py:122][0m #------------------------ Iteration 74 --------------------------#
[32m[20230113 19:46:19 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:46:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:19 @agent_ppo2.py:186][0m |           0.0009 |           3.3464 |           2.4152 |
[32m[20230113 19:46:19 @agent_ppo2.py:186][0m |          -0.0015 |           2.4740 |           2.4184 |
[32m[20230113 19:46:19 @agent_ppo2.py:186][0m |          -0.0036 |           2.3447 |           2.4189 |
[32m[20230113 19:46:19 @agent_ppo2.py:186][0m |          -0.0049 |           2.2529 |           2.4191 |
[32m[20230113 19:46:20 @agent_ppo2.py:186][0m |          -0.0070 |           2.1772 |           2.4196 |
[32m[20230113 19:46:20 @agent_ppo2.py:186][0m |          -0.0079 |           2.1463 |           2.4194 |
[32m[20230113 19:46:20 @agent_ppo2.py:186][0m |          -0.0086 |           2.1084 |           2.4214 |
[32m[20230113 19:46:20 @agent_ppo2.py:186][0m |          -0.0094 |           2.0588 |           2.4207 |
[32m[20230113 19:46:20 @agent_ppo2.py:186][0m |          -0.0097 |           2.0401 |           2.4220 |
[32m[20230113 19:46:20 @agent_ppo2.py:186][0m |          -0.0103 |           2.0026 |           2.4231 |
[32m[20230113 19:46:20 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 109.69
[32m[20230113 19:46:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 115.98
[32m[20230113 19:46:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 156.38
[32m[20230113 19:46:20 @agent_ppo2.py:144][0m Total time:       1.79 min
[32m[20230113 19:46:20 @agent_ppo2.py:146][0m 153600 total steps have happened
[32m[20230113 19:46:20 @agent_ppo2.py:122][0m #------------------------ Iteration 75 --------------------------#
[32m[20230113 19:46:21 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:21 @agent_ppo2.py:186][0m |          -0.0004 |           2.2005 |           2.4457 |
[32m[20230113 19:46:21 @agent_ppo2.py:186][0m |          -0.0045 |           1.9459 |           2.4461 |
[32m[20230113 19:46:21 @agent_ppo2.py:186][0m |          -0.0071 |           1.8107 |           2.4460 |
[32m[20230113 19:46:21 @agent_ppo2.py:186][0m |          -0.0091 |           1.7286 |           2.4481 |
[32m[20230113 19:46:21 @agent_ppo2.py:186][0m |          -0.0104 |           1.7035 |           2.4490 |
[32m[20230113 19:46:21 @agent_ppo2.py:186][0m |          -0.0113 |           1.6476 |           2.4503 |
[32m[20230113 19:46:21 @agent_ppo2.py:186][0m |          -0.0127 |           1.6151 |           2.4515 |
[32m[20230113 19:46:21 @agent_ppo2.py:186][0m |          -0.0131 |           1.5923 |           2.4524 |
[32m[20230113 19:46:21 @agent_ppo2.py:186][0m |          -0.0139 |           1.5724 |           2.4540 |
[32m[20230113 19:46:21 @agent_ppo2.py:186][0m |          -0.0147 |           1.5475 |           2.4553 |
[32m[20230113 19:46:21 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 122.75
[32m[20230113 19:46:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 123.17
[32m[20230113 19:46:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 166.05
[32m[20230113 19:46:22 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 166.05
[32m[20230113 19:46:22 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 166.05
[32m[20230113 19:46:22 @agent_ppo2.py:144][0m Total time:       1.82 min
[32m[20230113 19:46:22 @agent_ppo2.py:146][0m 155648 total steps have happened
[32m[20230113 19:46:22 @agent_ppo2.py:122][0m #------------------------ Iteration 76 --------------------------#
[32m[20230113 19:46:22 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:22 @agent_ppo2.py:186][0m |          -0.0005 |           1.7656 |           2.5158 |
[32m[20230113 19:46:22 @agent_ppo2.py:186][0m |          -0.0041 |           1.6505 |           2.5166 |
[32m[20230113 19:46:22 @agent_ppo2.py:186][0m |          -0.0063 |           1.6153 |           2.5148 |
[32m[20230113 19:46:22 @agent_ppo2.py:186][0m |          -0.0078 |           1.5874 |           2.5163 |
[32m[20230113 19:46:22 @agent_ppo2.py:186][0m |          -0.0092 |           1.5672 |           2.5156 |
[32m[20230113 19:46:22 @agent_ppo2.py:186][0m |          -0.0100 |           1.5423 |           2.5166 |
[32m[20230113 19:46:22 @agent_ppo2.py:186][0m |          -0.0104 |           1.5329 |           2.5163 |
[32m[20230113 19:46:23 @agent_ppo2.py:186][0m |          -0.0108 |           1.5195 |           2.5173 |
[32m[20230113 19:46:23 @agent_ppo2.py:186][0m |          -0.0112 |           1.5204 |           2.5167 |
[32m[20230113 19:46:23 @agent_ppo2.py:186][0m |          -0.0117 |           1.5014 |           2.5180 |
[32m[20230113 19:46:23 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:46:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 129.14
[32m[20230113 19:46:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 146.71
[32m[20230113 19:46:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 194.23
[32m[20230113 19:46:23 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 194.23
[32m[20230113 19:46:23 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 194.23
[32m[20230113 19:46:23 @agent_ppo2.py:144][0m Total time:       1.84 min
[32m[20230113 19:46:23 @agent_ppo2.py:146][0m 157696 total steps have happened
[32m[20230113 19:46:23 @agent_ppo2.py:122][0m #------------------------ Iteration 77 --------------------------#
[32m[20230113 19:46:24 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:24 @agent_ppo2.py:186][0m |           0.0010 |           1.9919 |           2.5801 |
[32m[20230113 19:46:24 @agent_ppo2.py:186][0m |          -0.0019 |           1.8203 |           2.5808 |
[32m[20230113 19:46:24 @agent_ppo2.py:186][0m |          -0.0039 |           1.7806 |           2.5813 |
[32m[20230113 19:46:24 @agent_ppo2.py:186][0m |          -0.0051 |           1.7446 |           2.5817 |
[32m[20230113 19:46:24 @agent_ppo2.py:186][0m |          -0.0057 |           1.7251 |           2.5825 |
[32m[20230113 19:46:24 @agent_ppo2.py:186][0m |          -0.0067 |           1.7085 |           2.5829 |
[32m[20230113 19:46:24 @agent_ppo2.py:186][0m |          -0.0072 |           1.6896 |           2.5840 |
[32m[20230113 19:46:24 @agent_ppo2.py:186][0m |          -0.0077 |           1.6806 |           2.5849 |
[32m[20230113 19:46:24 @agent_ppo2.py:186][0m |          -0.0080 |           1.6720 |           2.5848 |
[32m[20230113 19:46:24 @agent_ppo2.py:186][0m |          -0.0086 |           1.6575 |           2.5869 |
[32m[20230113 19:46:24 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 145.14
[32m[20230113 19:46:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 159.27
[32m[20230113 19:46:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 206.63
[32m[20230113 19:46:25 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 206.63
[32m[20230113 19:46:25 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 206.63
[32m[20230113 19:46:25 @agent_ppo2.py:144][0m Total time:       1.87 min
[32m[20230113 19:46:25 @agent_ppo2.py:146][0m 159744 total steps have happened
[32m[20230113 19:46:25 @agent_ppo2.py:122][0m #------------------------ Iteration 78 --------------------------#
[32m[20230113 19:46:25 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:25 @agent_ppo2.py:186][0m |          -0.0010 |           1.9134 |           2.5739 |
[32m[20230113 19:46:25 @agent_ppo2.py:186][0m |          -0.0053 |           1.7188 |           2.5688 |
[32m[20230113 19:46:25 @agent_ppo2.py:186][0m |          -0.0070 |           1.6597 |           2.5674 |
[32m[20230113 19:46:25 @agent_ppo2.py:186][0m |          -0.0082 |           1.6285 |           2.5662 |
[32m[20230113 19:46:25 @agent_ppo2.py:186][0m |          -0.0086 |           1.5988 |           2.5666 |
[32m[20230113 19:46:25 @agent_ppo2.py:186][0m |          -0.0092 |           1.5865 |           2.5650 |
[32m[20230113 19:46:25 @agent_ppo2.py:186][0m |          -0.0103 |           1.5721 |           2.5651 |
[32m[20230113 19:46:25 @agent_ppo2.py:186][0m |          -0.0103 |           1.5544 |           2.5645 |
[32m[20230113 19:46:25 @agent_ppo2.py:186][0m |          -0.0110 |           1.5392 |           2.5641 |
[32m[20230113 19:46:25 @agent_ppo2.py:186][0m |          -0.0113 |           1.5306 |           2.5643 |
[32m[20230113 19:46:25 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 141.95
[32m[20230113 19:46:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 146.55
[32m[20230113 19:46:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 193.99
[32m[20230113 19:46:26 @agent_ppo2.py:144][0m Total time:       1.89 min
[32m[20230113 19:46:26 @agent_ppo2.py:146][0m 161792 total steps have happened
[32m[20230113 19:46:26 @agent_ppo2.py:122][0m #------------------------ Iteration 79 --------------------------#
[32m[20230113 19:46:26 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:46:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:27 @agent_ppo2.py:186][0m |          -0.0001 |           1.8771 |           2.5499 |
[32m[20230113 19:46:27 @agent_ppo2.py:186][0m |          -0.0039 |           1.7665 |           2.5501 |
[32m[20230113 19:46:27 @agent_ppo2.py:186][0m |          -0.0059 |           1.7443 |           2.5491 |
[32m[20230113 19:46:27 @agent_ppo2.py:186][0m |          -0.0075 |           1.7240 |           2.5472 |
[32m[20230113 19:46:27 @agent_ppo2.py:186][0m |          -0.0084 |           1.7048 |           2.5460 |
[32m[20230113 19:46:27 @agent_ppo2.py:186][0m |          -0.0090 |           1.6969 |           2.5459 |
[32m[20230113 19:46:27 @agent_ppo2.py:186][0m |          -0.0095 |           1.6919 |           2.5459 |
[32m[20230113 19:46:27 @agent_ppo2.py:186][0m |          -0.0097 |           1.6781 |           2.5456 |
[32m[20230113 19:46:27 @agent_ppo2.py:186][0m |          -0.0107 |           1.6640 |           2.5456 |
[32m[20230113 19:46:27 @agent_ppo2.py:186][0m |          -0.0109 |           1.6533 |           2.5456 |
[32m[20230113 19:46:27 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 152.71
[32m[20230113 19:46:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 161.13
[32m[20230113 19:46:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 211.25
[32m[20230113 19:46:27 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 211.25
[32m[20230113 19:46:27 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 211.25
[32m[20230113 19:46:27 @agent_ppo2.py:144][0m Total time:       1.91 min
[32m[20230113 19:46:27 @agent_ppo2.py:146][0m 163840 total steps have happened
[32m[20230113 19:46:27 @agent_ppo2.py:122][0m #------------------------ Iteration 80 --------------------------#
[32m[20230113 19:46:28 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:46:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:28 @agent_ppo2.py:186][0m |           0.0001 |          15.2840 |           2.5506 |
[32m[20230113 19:46:28 @agent_ppo2.py:186][0m |          -0.0025 |           5.2126 |           2.5497 |
[32m[20230113 19:46:28 @agent_ppo2.py:186][0m |          -0.0030 |           4.7698 |           2.5478 |
[32m[20230113 19:46:28 @agent_ppo2.py:186][0m |          -0.0010 |           4.5295 |           2.5464 |
[32m[20230113 19:46:28 @agent_ppo2.py:186][0m |          -0.0056 |           4.3856 |           2.5440 |
[32m[20230113 19:46:28 @agent_ppo2.py:186][0m |          -0.0045 |           4.3429 |           2.5446 |
[32m[20230113 19:46:28 @agent_ppo2.py:186][0m |          -0.0044 |           4.3679 |           2.5447 |
[32m[20230113 19:46:28 @agent_ppo2.py:186][0m |          -0.0070 |           4.1096 |           2.5429 |
[32m[20230113 19:46:28 @agent_ppo2.py:186][0m |          -0.0041 |           4.0054 |           2.5416 |
[32m[20230113 19:46:28 @agent_ppo2.py:186][0m |          -0.0079 |           3.8477 |           2.5419 |
[32m[20230113 19:46:28 @agent_ppo2.py:131][0m Policy update time: 0.55 s
[32m[20230113 19:46:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 71.30
[32m[20230113 19:46:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 159.26
[32m[20230113 19:46:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 113.95
[32m[20230113 19:46:29 @agent_ppo2.py:144][0m Total time:       1.94 min
[32m[20230113 19:46:29 @agent_ppo2.py:146][0m 165888 total steps have happened
[32m[20230113 19:46:29 @agent_ppo2.py:122][0m #------------------------ Iteration 81 --------------------------#
[32m[20230113 19:46:29 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:46:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:29 @agent_ppo2.py:186][0m |          -0.0006 |          44.8541 |           2.5773 |
[32m[20230113 19:46:29 @agent_ppo2.py:186][0m |          -0.0074 |          31.5189 |           2.5732 |
[32m[20230113 19:46:30 @agent_ppo2.py:186][0m |          -0.0077 |          28.2600 |           2.5749 |
[32m[20230113 19:46:30 @agent_ppo2.py:186][0m |          -0.0079 |          26.5387 |           2.5739 |
[32m[20230113 19:46:30 @agent_ppo2.py:186][0m |          -0.0093 |          25.3834 |           2.5757 |
[32m[20230113 19:46:30 @agent_ppo2.py:186][0m |          -0.0099 |          25.0638 |           2.5773 |
[32m[20230113 19:46:30 @agent_ppo2.py:186][0m |          -0.0111 |          23.3307 |           2.5783 |
[32m[20230113 19:46:30 @agent_ppo2.py:186][0m |          -0.0117 |          22.8459 |           2.5785 |
[32m[20230113 19:46:30 @agent_ppo2.py:186][0m |          -0.0110 |          22.1470 |           2.5801 |
[32m[20230113 19:46:30 @agent_ppo2.py:186][0m |          -0.0126 |          21.8633 |           2.5806 |
[32m[20230113 19:46:30 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:46:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: -3.14
[32m[20230113 19:46:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 147.67
[32m[20230113 19:46:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 219.78
[32m[20230113 19:46:30 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 219.78
[32m[20230113 19:46:30 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 219.78
[32m[20230113 19:46:30 @agent_ppo2.py:144][0m Total time:       1.96 min
[32m[20230113 19:46:30 @agent_ppo2.py:146][0m 167936 total steps have happened
[32m[20230113 19:46:30 @agent_ppo2.py:122][0m #------------------------ Iteration 82 --------------------------#
[32m[20230113 19:46:31 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:31 @agent_ppo2.py:186][0m |          -0.0007 |           4.4988 |           2.5514 |
[32m[20230113 19:46:31 @agent_ppo2.py:186][0m |          -0.0039 |           3.1649 |           2.5502 |
[32m[20230113 19:46:31 @agent_ppo2.py:186][0m |          -0.0053 |           2.9968 |           2.5517 |
[32m[20230113 19:46:31 @agent_ppo2.py:186][0m |          -0.0062 |           2.9013 |           2.5535 |
[32m[20230113 19:46:31 @agent_ppo2.py:186][0m |          -0.0070 |           2.8426 |           2.5541 |
[32m[20230113 19:46:31 @agent_ppo2.py:186][0m |          -0.0076 |           2.7899 |           2.5548 |
[32m[20230113 19:46:31 @agent_ppo2.py:186][0m |          -0.0080 |           2.7419 |           2.5555 |
[32m[20230113 19:46:31 @agent_ppo2.py:186][0m |          -0.0080 |           2.6969 |           2.5569 |
[32m[20230113 19:46:31 @agent_ppo2.py:186][0m |          -0.0087 |           2.6617 |           2.5568 |
[32m[20230113 19:46:31 @agent_ppo2.py:186][0m |          -0.0077 |           2.6257 |           2.5590 |
[32m[20230113 19:46:31 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:46:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 143.27
[32m[20230113 19:46:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 147.69
[32m[20230113 19:46:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 185.29
[32m[20230113 19:46:32 @agent_ppo2.py:144][0m Total time:       1.99 min
[32m[20230113 19:46:32 @agent_ppo2.py:146][0m 169984 total steps have happened
[32m[20230113 19:46:32 @agent_ppo2.py:122][0m #------------------------ Iteration 83 --------------------------#
[32m[20230113 19:46:32 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 19:46:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:32 @agent_ppo2.py:186][0m |           0.0014 |          36.2820 |           2.5477 |
[32m[20230113 19:46:32 @agent_ppo2.py:186][0m |          -0.0082 |          28.8597 |           2.5397 |
[32m[20230113 19:46:32 @agent_ppo2.py:186][0m |          -0.0098 |          25.0142 |           2.5393 |
[32m[20230113 19:46:32 @agent_ppo2.py:186][0m |          -0.0103 |          21.9748 |           2.5347 |
[32m[20230113 19:46:32 @agent_ppo2.py:186][0m |          -0.0131 |          20.5944 |           2.5345 |
[32m[20230113 19:46:32 @agent_ppo2.py:186][0m |          -0.0111 |          19.2838 |           2.5342 |
[32m[20230113 19:46:32 @agent_ppo2.py:186][0m |          -0.0150 |          18.5272 |           2.5350 |
[32m[20230113 19:46:33 @agent_ppo2.py:186][0m |          -0.0131 |          18.3535 |           2.5342 |
[32m[20230113 19:46:33 @agent_ppo2.py:186][0m |          -0.0137 |          17.8391 |           2.5356 |
[32m[20230113 19:46:33 @agent_ppo2.py:186][0m |          -0.0113 |          17.1944 |           2.5351 |
[32m[20230113 19:46:33 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 19:46:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 75.63
[32m[20230113 19:46:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 141.56
[32m[20230113 19:46:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 174.31
[32m[20230113 19:46:33 @agent_ppo2.py:144][0m Total time:       2.01 min
[32m[20230113 19:46:33 @agent_ppo2.py:146][0m 172032 total steps have happened
[32m[20230113 19:46:33 @agent_ppo2.py:122][0m #------------------------ Iteration 84 --------------------------#
[32m[20230113 19:46:34 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:34 @agent_ppo2.py:186][0m |          -0.0005 |           3.4019 |           2.6017 |
[32m[20230113 19:46:34 @agent_ppo2.py:186][0m |          -0.0044 |           2.4474 |           2.6010 |
[32m[20230113 19:46:34 @agent_ppo2.py:186][0m |          -0.0061 |           2.3340 |           2.6003 |
[32m[20230113 19:46:34 @agent_ppo2.py:186][0m |          -0.0078 |           2.2493 |           2.6009 |
[32m[20230113 19:46:34 @agent_ppo2.py:186][0m |          -0.0089 |           2.1975 |           2.6022 |
[32m[20230113 19:46:34 @agent_ppo2.py:186][0m |          -0.0095 |           2.1526 |           2.6005 |
[32m[20230113 19:46:34 @agent_ppo2.py:186][0m |          -0.0106 |           2.1141 |           2.6011 |
[32m[20230113 19:46:34 @agent_ppo2.py:186][0m |          -0.0113 |           2.0814 |           2.6026 |
[32m[20230113 19:46:34 @agent_ppo2.py:186][0m |          -0.0117 |           2.0472 |           2.6021 |
[32m[20230113 19:46:34 @agent_ppo2.py:186][0m |          -0.0125 |           2.0212 |           2.6017 |
[32m[20230113 19:46:34 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 134.01
[32m[20230113 19:46:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 140.23
[32m[20230113 19:46:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 190.95
[32m[20230113 19:46:35 @agent_ppo2.py:144][0m Total time:       2.03 min
[32m[20230113 19:46:35 @agent_ppo2.py:146][0m 174080 total steps have happened
[32m[20230113 19:46:35 @agent_ppo2.py:122][0m #------------------------ Iteration 85 --------------------------#
[32m[20230113 19:46:35 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:35 @agent_ppo2.py:186][0m |          -0.0004 |           2.2589 |           2.5827 |
[32m[20230113 19:46:35 @agent_ppo2.py:186][0m |          -0.0041 |           2.0676 |           2.5800 |
[32m[20230113 19:46:35 @agent_ppo2.py:186][0m |          -0.0060 |           1.9765 |           2.5799 |
[32m[20230113 19:46:35 @agent_ppo2.py:186][0m |          -0.0073 |           1.9174 |           2.5792 |
[32m[20230113 19:46:35 @agent_ppo2.py:186][0m |          -0.0083 |           1.8769 |           2.5800 |
[32m[20230113 19:46:35 @agent_ppo2.py:186][0m |          -0.0091 |           1.8377 |           2.5811 |
[32m[20230113 19:46:35 @agent_ppo2.py:186][0m |          -0.0098 |           1.8114 |           2.5823 |
[32m[20230113 19:46:35 @agent_ppo2.py:186][0m |          -0.0106 |           1.7870 |           2.5816 |
[32m[20230113 19:46:35 @agent_ppo2.py:186][0m |          -0.0109 |           1.7643 |           2.5824 |
[32m[20230113 19:46:35 @agent_ppo2.py:186][0m |          -0.0117 |           1.7516 |           2.5831 |
[32m[20230113 19:46:35 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 158.09
[32m[20230113 19:46:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 158.51
[32m[20230113 19:46:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 205.56
[32m[20230113 19:46:36 @agent_ppo2.py:144][0m Total time:       2.06 min
[32m[20230113 19:46:36 @agent_ppo2.py:146][0m 176128 total steps have happened
[32m[20230113 19:46:36 @agent_ppo2.py:122][0m #------------------------ Iteration 86 --------------------------#
[32m[20230113 19:46:36 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 19:46:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:36 @agent_ppo2.py:186][0m |          -0.0001 |          25.2280 |           2.6555 |
[32m[20230113 19:46:36 @agent_ppo2.py:186][0m |          -0.0040 |           6.2169 |           2.6524 |
[32m[20230113 19:46:36 @agent_ppo2.py:186][0m |          -0.0057 |           4.8329 |           2.6508 |
[32m[20230113 19:46:36 @agent_ppo2.py:186][0m |          -0.0069 |           4.0676 |           2.6477 |
[32m[20230113 19:46:37 @agent_ppo2.py:186][0m |          -0.0082 |           3.7316 |           2.6460 |
[32m[20230113 19:46:37 @agent_ppo2.py:186][0m |          -0.0085 |           3.4984 |           2.6453 |
[32m[20230113 19:46:37 @agent_ppo2.py:186][0m |          -0.0097 |           3.3579 |           2.6429 |
[32m[20230113 19:46:37 @agent_ppo2.py:186][0m |          -0.0082 |           3.1842 |           2.6426 |
[32m[20230113 19:46:37 @agent_ppo2.py:186][0m |          -0.0098 |           3.1376 |           2.6413 |
[32m[20230113 19:46:37 @agent_ppo2.py:186][0m |          -0.0103 |           2.9954 |           2.6396 |
[32m[20230113 19:46:37 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 19:46:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 68.70
[32m[20230113 19:46:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 167.95
[32m[20230113 19:46:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 192.80
[32m[20230113 19:46:37 @agent_ppo2.py:144][0m Total time:       2.08 min
[32m[20230113 19:46:37 @agent_ppo2.py:146][0m 178176 total steps have happened
[32m[20230113 19:46:37 @agent_ppo2.py:122][0m #------------------------ Iteration 87 --------------------------#
[32m[20230113 19:46:38 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:38 @agent_ppo2.py:186][0m |          -0.0015 |           2.2002 |           2.6339 |
[32m[20230113 19:46:38 @agent_ppo2.py:186][0m |          -0.0056 |           2.0086 |           2.6301 |
[32m[20230113 19:46:38 @agent_ppo2.py:186][0m |          -0.0076 |           1.9314 |           2.6295 |
[32m[20230113 19:46:38 @agent_ppo2.py:186][0m |          -0.0087 |           1.8772 |           2.6270 |
[32m[20230113 19:46:38 @agent_ppo2.py:186][0m |          -0.0094 |           1.8367 |           2.6254 |
[32m[20230113 19:46:38 @agent_ppo2.py:186][0m |          -0.0100 |           1.8114 |           2.6253 |
[32m[20230113 19:46:38 @agent_ppo2.py:186][0m |          -0.0105 |           1.7891 |           2.6251 |
[32m[20230113 19:46:38 @agent_ppo2.py:186][0m |          -0.0112 |           1.7650 |           2.6246 |
[32m[20230113 19:46:38 @agent_ppo2.py:186][0m |          -0.0113 |           1.7539 |           2.6237 |
[32m[20230113 19:46:38 @agent_ppo2.py:186][0m |          -0.0119 |           1.7385 |           2.6249 |
[32m[20230113 19:46:38 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:46:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 137.81
[32m[20230113 19:46:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 142.44
[32m[20230113 19:46:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 204.07
[32m[20230113 19:46:39 @agent_ppo2.py:144][0m Total time:       2.10 min
[32m[20230113 19:46:39 @agent_ppo2.py:146][0m 180224 total steps have happened
[32m[20230113 19:46:39 @agent_ppo2.py:122][0m #------------------------ Iteration 88 --------------------------#
[32m[20230113 19:46:39 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:39 @agent_ppo2.py:186][0m |           0.0011 |           2.4460 |           2.7180 |
[32m[20230113 19:46:39 @agent_ppo2.py:186][0m |          -0.0026 |           2.0847 |           2.7167 |
[32m[20230113 19:46:39 @agent_ppo2.py:186][0m |          -0.0052 |           1.9796 |           2.7151 |
[32m[20230113 19:46:39 @agent_ppo2.py:186][0m |          -0.0065 |           1.9152 |           2.7141 |
[32m[20230113 19:46:39 @agent_ppo2.py:186][0m |          -0.0079 |           1.8671 |           2.7137 |
[32m[20230113 19:46:39 @agent_ppo2.py:186][0m |          -0.0081 |           1.8276 |           2.7118 |
[32m[20230113 19:46:39 @agent_ppo2.py:186][0m |          -0.0090 |           1.7934 |           2.7118 |
[32m[20230113 19:46:39 @agent_ppo2.py:186][0m |          -0.0097 |           1.7671 |           2.7111 |
[32m[20230113 19:46:40 @agent_ppo2.py:186][0m |          -0.0107 |           1.7413 |           2.7099 |
[32m[20230113 19:46:40 @agent_ppo2.py:186][0m |          -0.0112 |           1.7286 |           2.7095 |
[32m[20230113 19:46:40 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 180.76
[32m[20230113 19:46:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 190.07
[32m[20230113 19:46:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 210.05
[32m[20230113 19:46:40 @agent_ppo2.py:144][0m Total time:       2.12 min
[32m[20230113 19:46:40 @agent_ppo2.py:146][0m 182272 total steps have happened
[32m[20230113 19:46:40 @agent_ppo2.py:122][0m #------------------------ Iteration 89 --------------------------#
[32m[20230113 19:46:41 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:41 @agent_ppo2.py:186][0m |           0.0003 |           2.3165 |           2.5945 |
[32m[20230113 19:46:41 @agent_ppo2.py:186][0m |          -0.0028 |           2.1762 |           2.5935 |
[32m[20230113 19:46:41 @agent_ppo2.py:186][0m |          -0.0047 |           2.1081 |           2.5910 |
[32m[20230113 19:46:41 @agent_ppo2.py:186][0m |          -0.0057 |           2.0606 |           2.5913 |
[32m[20230113 19:46:41 @agent_ppo2.py:186][0m |          -0.0070 |           2.0236 |           2.5897 |
[32m[20230113 19:46:41 @agent_ppo2.py:186][0m |          -0.0079 |           1.9879 |           2.5908 |
[32m[20230113 19:46:41 @agent_ppo2.py:186][0m |          -0.0083 |           1.9659 |           2.5898 |
[32m[20230113 19:46:41 @agent_ppo2.py:186][0m |          -0.0090 |           1.9416 |           2.5911 |
[32m[20230113 19:46:41 @agent_ppo2.py:186][0m |          -0.0095 |           1.9227 |           2.5901 |
[32m[20230113 19:46:41 @agent_ppo2.py:186][0m |          -0.0099 |           1.8962 |           2.5912 |
[32m[20230113 19:46:41 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 172.16
[32m[20230113 19:46:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 197.58
[32m[20230113 19:46:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 119.18
[32m[20230113 19:46:41 @agent_ppo2.py:144][0m Total time:       2.15 min
[32m[20230113 19:46:41 @agent_ppo2.py:146][0m 184320 total steps have happened
[32m[20230113 19:46:41 @agent_ppo2.py:122][0m #------------------------ Iteration 90 --------------------------#
[32m[20230113 19:46:42 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 19:46:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:42 @agent_ppo2.py:186][0m |           0.0004 |          17.1218 |           2.6058 |
[32m[20230113 19:46:42 @agent_ppo2.py:186][0m |          -0.0023 |           7.6492 |           2.6032 |
[32m[20230113 19:46:42 @agent_ppo2.py:186][0m |          -0.0070 |           6.4881 |           2.6013 |
[32m[20230113 19:46:42 @agent_ppo2.py:186][0m |          -0.0079 |           5.8226 |           2.6005 |
[32m[20230113 19:46:42 @agent_ppo2.py:186][0m |          -0.0090 |           5.4355 |           2.6000 |
[32m[20230113 19:46:42 @agent_ppo2.py:186][0m |          -0.0079 |           5.0824 |           2.6014 |
[32m[20230113 19:46:42 @agent_ppo2.py:186][0m |          -0.0064 |           4.9269 |           2.5999 |
[32m[20230113 19:46:42 @agent_ppo2.py:186][0m |          -0.0099 |           4.9319 |           2.5999 |
[32m[20230113 19:46:42 @agent_ppo2.py:186][0m |          -0.0099 |           4.7216 |           2.5992 |
[32m[20230113 19:46:42 @agent_ppo2.py:186][0m |          -0.0107 |           4.5428 |           2.5996 |
[32m[20230113 19:46:42 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 19:46:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 46.08
[32m[20230113 19:46:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 164.51
[32m[20230113 19:46:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.69
[32m[20230113 19:46:43 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 244.69
[32m[20230113 19:46:43 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 244.69
[32m[20230113 19:46:43 @agent_ppo2.py:144][0m Total time:       2.17 min
[32m[20230113 19:46:43 @agent_ppo2.py:146][0m 186368 total steps have happened
[32m[20230113 19:46:43 @agent_ppo2.py:122][0m #------------------------ Iteration 91 --------------------------#
[32m[20230113 19:46:43 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:43 @agent_ppo2.py:186][0m |           0.0001 |           2.5159 |           2.6677 |
[32m[20230113 19:46:43 @agent_ppo2.py:186][0m |          -0.0041 |           2.0243 |           2.6672 |
[32m[20230113 19:46:43 @agent_ppo2.py:186][0m |          -0.0056 |           1.8623 |           2.6680 |
[32m[20230113 19:46:43 @agent_ppo2.py:186][0m |          -0.0069 |           1.7488 |           2.6698 |
[32m[20230113 19:46:43 @agent_ppo2.py:186][0m |          -0.0075 |           1.6816 |           2.6690 |
[32m[20230113 19:46:43 @agent_ppo2.py:186][0m |          -0.0082 |           1.6285 |           2.6695 |
[32m[20230113 19:46:43 @agent_ppo2.py:186][0m |          -0.0090 |           1.5823 |           2.6708 |
[32m[20230113 19:46:43 @agent_ppo2.py:186][0m |          -0.0095 |           1.5414 |           2.6716 |
[32m[20230113 19:46:43 @agent_ppo2.py:186][0m |          -0.0097 |           1.5147 |           2.6729 |
[32m[20230113 19:46:44 @agent_ppo2.py:186][0m |          -0.0103 |           1.4982 |           2.6731 |
[32m[20230113 19:46:44 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:46:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 190.30
[32m[20230113 19:46:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 195.81
[32m[20230113 19:46:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 222.55
[32m[20230113 19:46:44 @agent_ppo2.py:144][0m Total time:       2.19 min
[32m[20230113 19:46:44 @agent_ppo2.py:146][0m 188416 total steps have happened
[32m[20230113 19:46:44 @agent_ppo2.py:122][0m #------------------------ Iteration 92 --------------------------#
[32m[20230113 19:46:44 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 19:46:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:44 @agent_ppo2.py:186][0m |          -0.0000 |          44.5344 |           2.6389 |
[32m[20230113 19:46:45 @agent_ppo2.py:186][0m |          -0.0068 |          24.4993 |           2.6336 |
[32m[20230113 19:46:45 @agent_ppo2.py:186][0m |          -0.0070 |          22.4823 |           2.6334 |
[32m[20230113 19:46:45 @agent_ppo2.py:186][0m |          -0.0090 |          20.3827 |           2.6320 |
[32m[20230113 19:46:45 @agent_ppo2.py:186][0m |          -0.0094 |          19.1571 |           2.6307 |
[32m[20230113 19:46:45 @agent_ppo2.py:186][0m |          -0.0093 |          18.4364 |           2.6314 |
[32m[20230113 19:46:45 @agent_ppo2.py:186][0m |          -0.0113 |          17.5619 |           2.6312 |
[32m[20230113 19:46:45 @agent_ppo2.py:186][0m |          -0.0106 |          17.4545 |           2.6315 |
[32m[20230113 19:46:45 @agent_ppo2.py:186][0m |          -0.0119 |          16.3869 |           2.6319 |
[32m[20230113 19:46:45 @agent_ppo2.py:186][0m |          -0.0109 |          16.5166 |           2.6310 |
[32m[20230113 19:46:45 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 19:46:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 30.59
[32m[20230113 19:46:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 184.97
[32m[20230113 19:46:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 219.33
[32m[20230113 19:46:45 @agent_ppo2.py:144][0m Total time:       2.21 min
[32m[20230113 19:46:45 @agent_ppo2.py:146][0m 190464 total steps have happened
[32m[20230113 19:46:45 @agent_ppo2.py:122][0m #------------------------ Iteration 93 --------------------------#
[32m[20230113 19:46:46 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:46:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:46 @agent_ppo2.py:186][0m |           0.0005 |           4.7464 |           2.6050 |
[32m[20230113 19:46:46 @agent_ppo2.py:186][0m |          -0.0037 |           3.1167 |           2.6023 |
[32m[20230113 19:46:46 @agent_ppo2.py:186][0m |          -0.0059 |           2.9636 |           2.6001 |
[32m[20230113 19:46:46 @agent_ppo2.py:186][0m |          -0.0072 |           2.8707 |           2.6011 |
[32m[20230113 19:46:46 @agent_ppo2.py:186][0m |          -0.0081 |           2.8164 |           2.6014 |
[32m[20230113 19:46:46 @agent_ppo2.py:186][0m |          -0.0089 |           2.7757 |           2.6031 |
[32m[20230113 19:46:46 @agent_ppo2.py:186][0m |          -0.0096 |           2.7375 |           2.6009 |
[32m[20230113 19:46:46 @agent_ppo2.py:186][0m |          -0.0100 |           2.7061 |           2.6026 |
[32m[20230113 19:46:46 @agent_ppo2.py:186][0m |          -0.0106 |           2.6808 |           2.6029 |
[32m[20230113 19:46:46 @agent_ppo2.py:186][0m |          -0.0111 |           2.6599 |           2.6028 |
[32m[20230113 19:46:46 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:46:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 171.55
[32m[20230113 19:46:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 177.50
[32m[20230113 19:46:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 208.26
[32m[20230113 19:46:47 @agent_ppo2.py:144][0m Total time:       2.24 min
[32m[20230113 19:46:47 @agent_ppo2.py:146][0m 192512 total steps have happened
[32m[20230113 19:46:47 @agent_ppo2.py:122][0m #------------------------ Iteration 94 --------------------------#
[32m[20230113 19:46:47 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:46:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:47 @agent_ppo2.py:186][0m |          -0.0003 |          36.9477 |           2.6554 |
[32m[20230113 19:46:47 @agent_ppo2.py:186][0m |          -0.0050 |          16.6951 |           2.6537 |
[32m[20230113 19:46:47 @agent_ppo2.py:186][0m |          -0.0064 |          12.2748 |           2.6534 |
[32m[20230113 19:46:47 @agent_ppo2.py:186][0m |          -0.0076 |          10.2677 |           2.6524 |
[32m[20230113 19:46:47 @agent_ppo2.py:186][0m |          -0.0084 |           8.9383 |           2.6508 |
[32m[20230113 19:46:47 @agent_ppo2.py:186][0m |          -0.0099 |           8.0928 |           2.6506 |
[32m[20230113 19:46:48 @agent_ppo2.py:186][0m |          -0.0099 |           7.6556 |           2.6493 |
[32m[20230113 19:46:48 @agent_ppo2.py:186][0m |          -0.0112 |           7.2139 |           2.6492 |
[32m[20230113 19:46:48 @agent_ppo2.py:186][0m |          -0.0111 |           6.9746 |           2.6485 |
[32m[20230113 19:46:48 @agent_ppo2.py:186][0m |          -0.0111 |           6.6348 |           2.6479 |
[32m[20230113 19:46:48 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:46:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 16.28
[32m[20230113 19:46:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 165.58
[32m[20230113 19:46:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 225.70
[32m[20230113 19:46:48 @agent_ppo2.py:144][0m Total time:       2.26 min
[32m[20230113 19:46:48 @agent_ppo2.py:146][0m 194560 total steps have happened
[32m[20230113 19:46:48 @agent_ppo2.py:122][0m #------------------------ Iteration 95 --------------------------#
[32m[20230113 19:46:49 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:46:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:49 @agent_ppo2.py:186][0m |           0.0001 |           2.6946 |           2.7024 |
[32m[20230113 19:46:49 @agent_ppo2.py:186][0m |          -0.0030 |           2.3682 |           2.7016 |
[32m[20230113 19:46:49 @agent_ppo2.py:186][0m |          -0.0048 |           2.2483 |           2.7006 |
[32m[20230113 19:46:49 @agent_ppo2.py:186][0m |          -0.0061 |           2.1797 |           2.7007 |
[32m[20230113 19:46:49 @agent_ppo2.py:186][0m |          -0.0069 |           2.1268 |           2.7018 |
[32m[20230113 19:46:49 @agent_ppo2.py:186][0m |          -0.0079 |           2.0888 |           2.7020 |
[32m[20230113 19:46:49 @agent_ppo2.py:186][0m |          -0.0084 |           2.0536 |           2.7008 |
[32m[20230113 19:46:49 @agent_ppo2.py:186][0m |          -0.0093 |           2.0291 |           2.7010 |
[32m[20230113 19:46:49 @agent_ppo2.py:186][0m |          -0.0098 |           2.0098 |           2.7008 |
[32m[20230113 19:46:49 @agent_ppo2.py:186][0m |          -0.0103 |           1.9828 |           2.7008 |
[32m[20230113 19:46:49 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 175.91
[32m[20230113 19:46:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 176.00
[32m[20230113 19:46:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 214.82
[32m[20230113 19:46:50 @agent_ppo2.py:144][0m Total time:       2.28 min
[32m[20230113 19:46:50 @agent_ppo2.py:146][0m 196608 total steps have happened
[32m[20230113 19:46:50 @agent_ppo2.py:122][0m #------------------------ Iteration 96 --------------------------#
[32m[20230113 19:46:50 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:50 @agent_ppo2.py:186][0m |          -0.0004 |           2.3580 |           2.7059 |
[32m[20230113 19:46:50 @agent_ppo2.py:186][0m |          -0.0045 |           2.2624 |           2.7018 |
[32m[20230113 19:46:50 @agent_ppo2.py:186][0m |          -0.0062 |           2.2221 |           2.6999 |
[32m[20230113 19:46:50 @agent_ppo2.py:186][0m |          -0.0074 |           2.1864 |           2.6992 |
[32m[20230113 19:46:50 @agent_ppo2.py:186][0m |          -0.0081 |           2.1643 |           2.7025 |
[32m[20230113 19:46:50 @agent_ppo2.py:186][0m |          -0.0087 |           2.1446 |           2.7020 |
[32m[20230113 19:46:50 @agent_ppo2.py:186][0m |          -0.0093 |           2.1280 |           2.7015 |
[32m[20230113 19:46:50 @agent_ppo2.py:186][0m |          -0.0098 |           2.1147 |           2.7034 |
[32m[20230113 19:46:50 @agent_ppo2.py:186][0m |          -0.0100 |           2.1031 |           2.7040 |
[32m[20230113 19:46:51 @agent_ppo2.py:186][0m |          -0.0108 |           2.0959 |           2.7053 |
[32m[20230113 19:46:51 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 147.34
[32m[20230113 19:46:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 152.25
[32m[20230113 19:46:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 217.03
[32m[20230113 19:46:51 @agent_ppo2.py:144][0m Total time:       2.31 min
[32m[20230113 19:46:51 @agent_ppo2.py:146][0m 198656 total steps have happened
[32m[20230113 19:46:51 @agent_ppo2.py:122][0m #------------------------ Iteration 97 --------------------------#
[32m[20230113 19:46:51 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:52 @agent_ppo2.py:186][0m |          -0.0002 |           3.2572 |           2.7081 |
[32m[20230113 19:46:52 @agent_ppo2.py:186][0m |          -0.0056 |           2.7119 |           2.7038 |
[32m[20230113 19:46:52 @agent_ppo2.py:186][0m |          -0.0081 |           2.5639 |           2.7027 |
[32m[20230113 19:46:52 @agent_ppo2.py:186][0m |          -0.0097 |           2.4807 |           2.7031 |
[32m[20230113 19:46:52 @agent_ppo2.py:186][0m |          -0.0108 |           2.4267 |           2.7039 |
[32m[20230113 19:46:52 @agent_ppo2.py:186][0m |          -0.0122 |           2.3742 |           2.7036 |
[32m[20230113 19:46:52 @agent_ppo2.py:186][0m |          -0.0127 |           2.3269 |           2.7030 |
[32m[20230113 19:46:52 @agent_ppo2.py:186][0m |          -0.0134 |           2.3077 |           2.7051 |
[32m[20230113 19:46:52 @agent_ppo2.py:186][0m |          -0.0134 |           2.2661 |           2.7053 |
[32m[20230113 19:46:52 @agent_ppo2.py:186][0m |          -0.0141 |           2.2452 |           2.7055 |
[32m[20230113 19:46:52 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 172.77
[32m[20230113 19:46:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 191.55
[32m[20230113 19:46:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 223.52
[32m[20230113 19:46:52 @agent_ppo2.py:144][0m Total time:       2.33 min
[32m[20230113 19:46:52 @agent_ppo2.py:146][0m 200704 total steps have happened
[32m[20230113 19:46:52 @agent_ppo2.py:122][0m #------------------------ Iteration 98 --------------------------#
[32m[20230113 19:46:53 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:53 @agent_ppo2.py:186][0m |          -0.0005 |           2.5806 |           2.7491 |
[32m[20230113 19:46:53 @agent_ppo2.py:186][0m |          -0.0043 |           2.4865 |           2.7484 |
[32m[20230113 19:46:53 @agent_ppo2.py:186][0m |          -0.0060 |           2.4290 |           2.7461 |
[32m[20230113 19:46:53 @agent_ppo2.py:186][0m |          -0.0068 |           2.3964 |           2.7465 |
[32m[20230113 19:46:53 @agent_ppo2.py:186][0m |          -0.0079 |           2.3627 |           2.7463 |
[32m[20230113 19:46:53 @agent_ppo2.py:186][0m |          -0.0084 |           2.3403 |           2.7464 |
[32m[20230113 19:46:53 @agent_ppo2.py:186][0m |          -0.0087 |           2.3208 |           2.7473 |
[32m[20230113 19:46:53 @agent_ppo2.py:186][0m |          -0.0091 |           2.2998 |           2.7471 |
[32m[20230113 19:46:53 @agent_ppo2.py:186][0m |          -0.0094 |           2.2810 |           2.7483 |
[32m[20230113 19:46:53 @agent_ppo2.py:186][0m |          -0.0099 |           2.2738 |           2.7486 |
[32m[20230113 19:46:53 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 165.49
[32m[20230113 19:46:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 166.36
[32m[20230113 19:46:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.68
[32m[20230113 19:46:54 @agent_ppo2.py:144][0m Total time:       2.35 min
[32m[20230113 19:46:54 @agent_ppo2.py:146][0m 202752 total steps have happened
[32m[20230113 19:46:54 @agent_ppo2.py:122][0m #------------------------ Iteration 99 --------------------------#
[32m[20230113 19:46:54 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:46:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:54 @agent_ppo2.py:186][0m |          -0.0011 |          11.9772 |           2.7309 |
[32m[20230113 19:46:54 @agent_ppo2.py:186][0m |          -0.0035 |           6.8201 |           2.7299 |
[32m[20230113 19:46:55 @agent_ppo2.py:186][0m |          -0.0057 |           6.3316 |           2.7283 |
[32m[20230113 19:46:55 @agent_ppo2.py:186][0m |          -0.0063 |           5.7979 |           2.7270 |
[32m[20230113 19:46:55 @agent_ppo2.py:186][0m |          -0.0070 |           5.4053 |           2.7261 |
[32m[20230113 19:46:55 @agent_ppo2.py:186][0m |          -0.0077 |           5.0315 |           2.7251 |
[32m[20230113 19:46:55 @agent_ppo2.py:186][0m |          -0.0087 |           4.7134 |           2.7246 |
[32m[20230113 19:46:55 @agent_ppo2.py:186][0m |          -0.0090 |           4.5802 |           2.7223 |
[32m[20230113 19:46:55 @agent_ppo2.py:186][0m |          -0.0090 |           4.4302 |           2.7229 |
[32m[20230113 19:46:55 @agent_ppo2.py:186][0m |          -0.0094 |           4.2370 |           2.7218 |
[32m[20230113 19:46:55 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:46:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 88.25
[32m[20230113 19:46:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 184.91
[32m[20230113 19:46:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.71
[32m[20230113 19:46:55 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 244.71
[32m[20230113 19:46:55 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 244.71
[32m[20230113 19:46:55 @agent_ppo2.py:144][0m Total time:       2.38 min
[32m[20230113 19:46:55 @agent_ppo2.py:146][0m 204800 total steps have happened
[32m[20230113 19:46:55 @agent_ppo2.py:122][0m #------------------------ Iteration 100 --------------------------#
[32m[20230113 19:46:56 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 19:46:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:56 @agent_ppo2.py:186][0m |          -0.0002 |          15.5216 |           2.6881 |
[32m[20230113 19:46:56 @agent_ppo2.py:186][0m |           0.0049 |           4.4097 |           2.6851 |
[32m[20230113 19:46:56 @agent_ppo2.py:186][0m |          -0.0076 |           3.5901 |           2.6822 |
[32m[20230113 19:46:56 @agent_ppo2.py:186][0m |           0.0007 |           3.3269 |           2.6810 |
[32m[20230113 19:46:56 @agent_ppo2.py:186][0m |          -0.0024 |           3.1590 |           2.6774 |
[32m[20230113 19:46:56 @agent_ppo2.py:186][0m |          -0.0103 |           3.0931 |           2.6745 |
[32m[20230113 19:46:56 @agent_ppo2.py:186][0m |          -0.0111 |           2.9784 |           2.6717 |
[32m[20230113 19:46:56 @agent_ppo2.py:186][0m |          -0.0113 |           2.9133 |           2.6707 |
[32m[20230113 19:46:56 @agent_ppo2.py:186][0m |          -0.0115 |           2.8786 |           2.6699 |
[32m[20230113 19:46:56 @agent_ppo2.py:186][0m |          -0.0127 |           2.8514 |           2.6679 |
[32m[20230113 19:46:56 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 19:46:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 103.34
[32m[20230113 19:46:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 192.73
[32m[20230113 19:46:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 207.73
[32m[20230113 19:46:57 @agent_ppo2.py:144][0m Total time:       2.40 min
[32m[20230113 19:46:57 @agent_ppo2.py:146][0m 206848 total steps have happened
[32m[20230113 19:46:57 @agent_ppo2.py:122][0m #------------------------ Iteration 101 --------------------------#
[32m[20230113 19:46:57 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:57 @agent_ppo2.py:186][0m |           0.0005 |           2.8390 |           2.6965 |
[32m[20230113 19:46:57 @agent_ppo2.py:186][0m |          -0.0038 |           2.6330 |           2.6941 |
[32m[20230113 19:46:57 @agent_ppo2.py:186][0m |          -0.0073 |           2.5554 |           2.6929 |
[32m[20230113 19:46:57 @agent_ppo2.py:186][0m |          -0.0085 |           2.5051 |           2.6922 |
[32m[20230113 19:46:57 @agent_ppo2.py:186][0m |          -0.0096 |           2.4704 |           2.6921 |
[32m[20230113 19:46:57 @agent_ppo2.py:186][0m |          -0.0102 |           2.4439 |           2.6924 |
[32m[20230113 19:46:57 @agent_ppo2.py:186][0m |          -0.0110 |           2.4188 |           2.6931 |
[32m[20230113 19:46:57 @agent_ppo2.py:186][0m |          -0.0116 |           2.4013 |           2.6921 |
[32m[20230113 19:46:58 @agent_ppo2.py:186][0m |          -0.0122 |           2.3814 |           2.6929 |
[32m[20230113 19:46:58 @agent_ppo2.py:186][0m |          -0.0130 |           2.3618 |           2.6940 |
[32m[20230113 19:46:58 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 165.01
[32m[20230113 19:46:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 170.64
[32m[20230113 19:46:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.65
[32m[20230113 19:46:58 @agent_ppo2.py:144][0m Total time:       2.42 min
[32m[20230113 19:46:58 @agent_ppo2.py:146][0m 208896 total steps have happened
[32m[20230113 19:46:58 @agent_ppo2.py:122][0m #------------------------ Iteration 102 --------------------------#
[32m[20230113 19:46:59 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:46:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:46:59 @agent_ppo2.py:186][0m |           0.0004 |           2.1312 |           2.7425 |
[32m[20230113 19:46:59 @agent_ppo2.py:186][0m |          -0.0023 |           2.0251 |           2.7411 |
[32m[20230113 19:46:59 @agent_ppo2.py:186][0m |          -0.0043 |           1.9830 |           2.7398 |
[32m[20230113 19:46:59 @agent_ppo2.py:186][0m |          -0.0053 |           1.9531 |           2.7405 |
[32m[20230113 19:46:59 @agent_ppo2.py:186][0m |          -0.0065 |           1.9356 |           2.7394 |
[32m[20230113 19:46:59 @agent_ppo2.py:186][0m |          -0.0074 |           1.9225 |           2.7390 |
[32m[20230113 19:46:59 @agent_ppo2.py:186][0m |          -0.0077 |           1.8998 |           2.7395 |
[32m[20230113 19:46:59 @agent_ppo2.py:186][0m |          -0.0082 |           1.8886 |           2.7406 |
[32m[20230113 19:46:59 @agent_ppo2.py:186][0m |          -0.0089 |           1.8812 |           2.7402 |
[32m[20230113 19:46:59 @agent_ppo2.py:186][0m |          -0.0094 |           1.8677 |           2.7408 |
[32m[20230113 19:46:59 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:46:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 174.76
[32m[20230113 19:46:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 186.35
[32m[20230113 19:46:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.50
[32m[20230113 19:46:59 @agent_ppo2.py:144][0m Total time:       2.45 min
[32m[20230113 19:46:59 @agent_ppo2.py:146][0m 210944 total steps have happened
[32m[20230113 19:46:59 @agent_ppo2.py:122][0m #------------------------ Iteration 103 --------------------------#
[32m[20230113 19:47:00 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:00 @agent_ppo2.py:186][0m |          -0.0008 |           3.2070 |           2.7738 |
[32m[20230113 19:47:00 @agent_ppo2.py:186][0m |          -0.0052 |           3.0035 |           2.7735 |
[32m[20230113 19:47:00 @agent_ppo2.py:186][0m |          -0.0072 |           2.9316 |           2.7712 |
[32m[20230113 19:47:00 @agent_ppo2.py:186][0m |          -0.0083 |           2.8970 |           2.7697 |
[32m[20230113 19:47:00 @agent_ppo2.py:186][0m |          -0.0090 |           2.8607 |           2.7716 |
[32m[20230113 19:47:00 @agent_ppo2.py:186][0m |          -0.0098 |           2.8318 |           2.7701 |
[32m[20230113 19:47:00 @agent_ppo2.py:186][0m |          -0.0103 |           2.8134 |           2.7685 |
[32m[20230113 19:47:00 @agent_ppo2.py:186][0m |          -0.0110 |           2.7919 |           2.7688 |
[32m[20230113 19:47:00 @agent_ppo2.py:186][0m |          -0.0115 |           2.7697 |           2.7671 |
[32m[20230113 19:47:00 @agent_ppo2.py:186][0m |          -0.0124 |           2.7561 |           2.7673 |
[32m[20230113 19:47:00 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 167.04
[32m[20230113 19:47:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 178.34
[32m[20230113 19:47:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.94
[32m[20230113 19:47:01 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 244.94
[32m[20230113 19:47:01 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 244.94
[32m[20230113 19:47:01 @agent_ppo2.py:144][0m Total time:       2.47 min
[32m[20230113 19:47:01 @agent_ppo2.py:146][0m 212992 total steps have happened
[32m[20230113 19:47:01 @agent_ppo2.py:122][0m #------------------------ Iteration 104 --------------------------#
[32m[20230113 19:47:01 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 19:47:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:01 @agent_ppo2.py:186][0m |           0.0003 |           8.2442 |           2.7504 |
[32m[20230113 19:47:01 @agent_ppo2.py:186][0m |          -0.0022 |           5.5878 |           2.7474 |
[32m[20230113 19:47:01 @agent_ppo2.py:186][0m |          -0.0036 |           4.9305 |           2.7449 |
[32m[20230113 19:47:01 @agent_ppo2.py:186][0m |          -0.0048 |           4.4199 |           2.7434 |
[32m[20230113 19:47:02 @agent_ppo2.py:186][0m |          -0.0072 |           4.2671 |           2.7439 |
[32m[20230113 19:47:02 @agent_ppo2.py:186][0m |          -0.0062 |           3.9220 |           2.7474 |
[32m[20230113 19:47:02 @agent_ppo2.py:186][0m |          -0.0071 |           3.7929 |           2.7442 |
[32m[20230113 19:47:02 @agent_ppo2.py:186][0m |          -0.0082 |           3.6019 |           2.7478 |
[32m[20230113 19:47:02 @agent_ppo2.py:186][0m |          -0.0090 |           3.4345 |           2.7481 |
[32m[20230113 19:47:02 @agent_ppo2.py:186][0m |          -0.0102 |           3.2683 |           2.7480 |
[32m[20230113 19:47:02 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 19:47:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 99.78
[32m[20230113 19:47:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 197.31
[32m[20230113 19:47:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.50
[32m[20230113 19:47:02 @agent_ppo2.py:144][0m Total time:       2.49 min
[32m[20230113 19:47:02 @agent_ppo2.py:146][0m 215040 total steps have happened
[32m[20230113 19:47:02 @agent_ppo2.py:122][0m #------------------------ Iteration 105 --------------------------#
[32m[20230113 19:47:03 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 19:47:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:03 @agent_ppo2.py:186][0m |          -0.0025 |          12.0947 |           2.7525 |
[32m[20230113 19:47:03 @agent_ppo2.py:186][0m |          -0.0075 |           4.9956 |           2.7486 |
[32m[20230113 19:47:03 @agent_ppo2.py:186][0m |          -0.0095 |           3.9824 |           2.7466 |
[32m[20230113 19:47:03 @agent_ppo2.py:186][0m |          -0.0060 |           3.5107 |           2.7445 |
[32m[20230113 19:47:03 @agent_ppo2.py:186][0m |          -0.0116 |           3.1623 |           2.7431 |
[32m[20230113 19:47:03 @agent_ppo2.py:186][0m |          -0.0111 |           2.9911 |           2.7415 |
[32m[20230113 19:47:03 @agent_ppo2.py:186][0m |          -0.0112 |           2.8156 |           2.7413 |
[32m[20230113 19:47:03 @agent_ppo2.py:186][0m |          -0.0143 |           2.6992 |           2.7395 |
[32m[20230113 19:47:03 @agent_ppo2.py:186][0m |          -0.0152 |           2.5674 |           2.7375 |
[32m[20230113 19:47:03 @agent_ppo2.py:186][0m |          -0.0122 |           2.5035 |           2.7367 |
[32m[20230113 19:47:03 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 19:47:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 109.12
[32m[20230113 19:47:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 185.61
[32m[20230113 19:47:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.67
[32m[20230113 19:47:03 @agent_ppo2.py:144][0m Total time:       2.51 min
[32m[20230113 19:47:03 @agent_ppo2.py:146][0m 217088 total steps have happened
[32m[20230113 19:47:03 @agent_ppo2.py:122][0m #------------------------ Iteration 106 --------------------------#
[32m[20230113 19:47:04 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 19:47:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:04 @agent_ppo2.py:186][0m |           0.0007 |          17.3989 |           2.7304 |
[32m[20230113 19:47:04 @agent_ppo2.py:186][0m |          -0.0034 |          10.0622 |           2.7276 |
[32m[20230113 19:47:04 @agent_ppo2.py:186][0m |          -0.0073 |           8.3254 |           2.7237 |
[32m[20230113 19:47:04 @agent_ppo2.py:186][0m |          -0.0089 |           7.4437 |           2.7215 |
[32m[20230113 19:47:04 @agent_ppo2.py:186][0m |          -0.0098 |           6.9211 |           2.7195 |
[32m[20230113 19:47:04 @agent_ppo2.py:186][0m |          -0.0093 |           6.5473 |           2.7157 |
[32m[20230113 19:47:04 @agent_ppo2.py:186][0m |          -0.0114 |           6.4205 |           2.7148 |
[32m[20230113 19:47:04 @agent_ppo2.py:186][0m |          -0.0119 |           6.3065 |           2.7154 |
[32m[20230113 19:47:04 @agent_ppo2.py:186][0m |          -0.0123 |           6.0522 |           2.7136 |
[32m[20230113 19:47:04 @agent_ppo2.py:186][0m |          -0.0126 |           5.9968 |           2.7125 |
[32m[20230113 19:47:04 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 19:47:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 69.57
[32m[20230113 19:47:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 191.86
[32m[20230113 19:47:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.09
[32m[20230113 19:47:05 @agent_ppo2.py:144][0m Total time:       2.53 min
[32m[20230113 19:47:05 @agent_ppo2.py:146][0m 219136 total steps have happened
[32m[20230113 19:47:05 @agent_ppo2.py:122][0m #------------------------ Iteration 107 --------------------------#
[32m[20230113 19:47:05 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:47:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:05 @agent_ppo2.py:186][0m |          -0.0016 |          62.7605 |           2.6894 |
[32m[20230113 19:47:05 @agent_ppo2.py:186][0m |          -0.0068 |          47.8933 |           2.6865 |
[32m[20230113 19:47:05 @agent_ppo2.py:186][0m |          -0.0090 |          42.3092 |           2.6838 |
[32m[20230113 19:47:05 @agent_ppo2.py:186][0m |          -0.0108 |          38.9927 |           2.6818 |
[32m[20230113 19:47:05 @agent_ppo2.py:186][0m |          -0.0116 |          36.7123 |           2.6817 |
[32m[20230113 19:47:05 @agent_ppo2.py:186][0m |          -0.0119 |          35.6176 |           2.6818 |
[32m[20230113 19:47:05 @agent_ppo2.py:186][0m |          -0.0123 |          34.6111 |           2.6811 |
[32m[20230113 19:47:05 @agent_ppo2.py:186][0m |          -0.0139 |          32.8339 |           2.6815 |
[32m[20230113 19:47:06 @agent_ppo2.py:186][0m |          -0.0134 |          32.0572 |           2.6814 |
[32m[20230113 19:47:06 @agent_ppo2.py:186][0m |          -0.0140 |          31.6346 |           2.6812 |
[32m[20230113 19:47:06 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:47:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 23.74
[32m[20230113 19:47:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 208.63
[32m[20230113 19:47:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 105.47
[32m[20230113 19:47:06 @agent_ppo2.py:144][0m Total time:       2.56 min
[32m[20230113 19:47:06 @agent_ppo2.py:146][0m 221184 total steps have happened
[32m[20230113 19:47:06 @agent_ppo2.py:122][0m #------------------------ Iteration 108 --------------------------#
[32m[20230113 19:47:06 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 19:47:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:07 @agent_ppo2.py:186][0m |           0.0024 |          24.4025 |           2.7642 |
[32m[20230113 19:47:07 @agent_ppo2.py:186][0m |          -0.0027 |          16.9873 |           2.7625 |
[32m[20230113 19:47:07 @agent_ppo2.py:186][0m |          -0.0049 |          14.8022 |           2.7605 |
[32m[20230113 19:47:07 @agent_ppo2.py:186][0m |          -0.0052 |          13.8073 |           2.7600 |
[32m[20230113 19:47:07 @agent_ppo2.py:186][0m |          -0.0093 |          13.0067 |           2.7617 |
[32m[20230113 19:47:07 @agent_ppo2.py:186][0m |          -0.0086 |          12.1176 |           2.7611 |
[32m[20230113 19:47:07 @agent_ppo2.py:186][0m |          -0.0087 |          11.6069 |           2.7604 |
[32m[20230113 19:47:07 @agent_ppo2.py:186][0m |          -0.0101 |          10.8314 |           2.7611 |
[32m[20230113 19:47:07 @agent_ppo2.py:186][0m |          -0.0098 |          10.3559 |           2.7621 |
[32m[20230113 19:47:07 @agent_ppo2.py:186][0m |          -0.0094 |           9.8399 |           2.7620 |
[32m[20230113 19:47:07 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 19:47:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 128.03
[32m[20230113 19:47:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 187.12
[32m[20230113 19:47:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.45
[32m[20230113 19:47:07 @agent_ppo2.py:144][0m Total time:       2.58 min
[32m[20230113 19:47:07 @agent_ppo2.py:146][0m 223232 total steps have happened
[32m[20230113 19:47:07 @agent_ppo2.py:122][0m #------------------------ Iteration 109 --------------------------#
[32m[20230113 19:47:08 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:47:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:08 @agent_ppo2.py:186][0m |           0.0010 |           7.2508 |           2.8055 |
[32m[20230113 19:47:08 @agent_ppo2.py:186][0m |          -0.0024 |           5.3759 |           2.8043 |
[32m[20230113 19:47:08 @agent_ppo2.py:186][0m |          -0.0046 |           4.8674 |           2.7999 |
[32m[20230113 19:47:08 @agent_ppo2.py:186][0m |          -0.0061 |           4.5932 |           2.7982 |
[32m[20230113 19:47:08 @agent_ppo2.py:186][0m |          -0.0069 |           4.3924 |           2.7984 |
[32m[20230113 19:47:08 @agent_ppo2.py:186][0m |          -0.0077 |           4.2425 |           2.7981 |
[32m[20230113 19:47:08 @agent_ppo2.py:186][0m |          -0.0081 |           4.1237 |           2.7975 |
[32m[20230113 19:47:08 @agent_ppo2.py:186][0m |          -0.0084 |           4.0158 |           2.7974 |
[32m[20230113 19:47:08 @agent_ppo2.py:186][0m |          -0.0088 |           3.9296 |           2.7965 |
[32m[20230113 19:47:08 @agent_ppo2.py:186][0m |          -0.0096 |           3.8635 |           2.7980 |
[32m[20230113 19:47:08 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 188.60
[32m[20230113 19:47:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 202.94
[32m[20230113 19:47:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.51
[32m[20230113 19:47:09 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 248.51
[32m[20230113 19:47:09 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 248.51
[32m[20230113 19:47:09 @agent_ppo2.py:144][0m Total time:       2.60 min
[32m[20230113 19:47:09 @agent_ppo2.py:146][0m 225280 total steps have happened
[32m[20230113 19:47:09 @agent_ppo2.py:122][0m #------------------------ Iteration 110 --------------------------#
[32m[20230113 19:47:09 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:09 @agent_ppo2.py:186][0m |           0.0005 |          11.2260 |           2.7688 |
[32m[20230113 19:47:09 @agent_ppo2.py:186][0m |          -0.0017 |           9.6676 |           2.7648 |
[32m[20230113 19:47:09 @agent_ppo2.py:186][0m |          -0.0048 |           8.8172 |           2.7626 |
[32m[20230113 19:47:10 @agent_ppo2.py:186][0m |          -0.0062 |           8.2863 |           2.7599 |
[32m[20230113 19:47:10 @agent_ppo2.py:186][0m |          -0.0075 |           7.9169 |           2.7582 |
[32m[20230113 19:47:10 @agent_ppo2.py:186][0m |          -0.0083 |           7.5628 |           2.7575 |
[32m[20230113 19:47:10 @agent_ppo2.py:186][0m |          -0.0090 |           7.2853 |           2.7552 |
[32m[20230113 19:47:10 @agent_ppo2.py:186][0m |          -0.0090 |           7.0409 |           2.7550 |
[32m[20230113 19:47:10 @agent_ppo2.py:186][0m |          -0.0101 |           6.8016 |           2.7550 |
[32m[20230113 19:47:10 @agent_ppo2.py:186][0m |          -0.0102 |           6.6915 |           2.7545 |
[32m[20230113 19:47:10 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:47:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 198.73
[32m[20230113 19:47:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.24
[32m[20230113 19:47:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 90.56
[32m[20230113 19:47:10 @agent_ppo2.py:144][0m Total time:       2.63 min
[32m[20230113 19:47:10 @agent_ppo2.py:146][0m 227328 total steps have happened
[32m[20230113 19:47:10 @agent_ppo2.py:122][0m #------------------------ Iteration 111 --------------------------#
[32m[20230113 19:47:11 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:47:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:11 @agent_ppo2.py:186][0m |          -0.0056 |          26.7258 |           2.7720 |
[32m[20230113 19:47:11 @agent_ppo2.py:186][0m |          -0.0078 |          15.5635 |           2.7711 |
[32m[20230113 19:47:11 @agent_ppo2.py:186][0m |          -0.0038 |          11.7286 |           2.7687 |
[32m[20230113 19:47:11 @agent_ppo2.py:186][0m |          -0.0035 |           9.7599 |           2.7686 |
[32m[20230113 19:47:11 @agent_ppo2.py:186][0m |           0.0074 |           9.1680 |           2.7670 |
[32m[20230113 19:47:11 @agent_ppo2.py:186][0m |          -0.0127 |           8.3995 |           2.7667 |
[32m[20230113 19:47:11 @agent_ppo2.py:186][0m |          -0.0126 |           7.9043 |           2.7655 |
[32m[20230113 19:47:11 @agent_ppo2.py:186][0m |          -0.0091 |           7.5497 |           2.7647 |
[32m[20230113 19:47:11 @agent_ppo2.py:186][0m |          -0.0107 |           7.1952 |           2.7654 |
[32m[20230113 19:47:11 @agent_ppo2.py:186][0m |          -0.0077 |           6.9230 |           2.7636 |
[32m[20230113 19:47:11 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:47:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 115.38
[32m[20230113 19:47:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 207.12
[32m[20230113 19:47:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.40
[32m[20230113 19:47:12 @agent_ppo2.py:144][0m Total time:       2.65 min
[32m[20230113 19:47:12 @agent_ppo2.py:146][0m 229376 total steps have happened
[32m[20230113 19:47:12 @agent_ppo2.py:122][0m #------------------------ Iteration 112 --------------------------#
[32m[20230113 19:47:12 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:12 @agent_ppo2.py:186][0m |          -0.0006 |          13.9391 |           2.7687 |
[32m[20230113 19:47:12 @agent_ppo2.py:186][0m |          -0.0042 |           9.5770 |           2.7656 |
[32m[20230113 19:47:12 @agent_ppo2.py:186][0m |          -0.0064 |           8.1422 |           2.7617 |
[32m[20230113 19:47:12 @agent_ppo2.py:186][0m |          -0.0061 |           7.0286 |           2.7621 |
[32m[20230113 19:47:12 @agent_ppo2.py:186][0m |          -0.0080 |           6.2565 |           2.7598 |
[32m[20230113 19:47:12 @agent_ppo2.py:186][0m |          -0.0082 |           5.6223 |           2.7592 |
[32m[20230113 19:47:12 @agent_ppo2.py:186][0m |          -0.0088 |           5.2638 |           2.7583 |
[32m[20230113 19:47:12 @agent_ppo2.py:186][0m |          -0.0107 |           4.8119 |           2.7589 |
[32m[20230113 19:47:13 @agent_ppo2.py:186][0m |          -0.0111 |           4.5418 |           2.7576 |
[32m[20230113 19:47:13 @agent_ppo2.py:186][0m |          -0.0117 |           4.3565 |           2.7571 |
[32m[20230113 19:47:13 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:47:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 140.28
[32m[20230113 19:47:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 180.79
[32m[20230113 19:47:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 140.20
[32m[20230113 19:47:13 @agent_ppo2.py:144][0m Total time:       2.68 min
[32m[20230113 19:47:13 @agent_ppo2.py:146][0m 231424 total steps have happened
[32m[20230113 19:47:13 @agent_ppo2.py:122][0m #------------------------ Iteration 113 --------------------------#
[32m[20230113 19:47:14 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:14 @agent_ppo2.py:186][0m |           0.0003 |          12.7383 |           2.7646 |
[32m[20230113 19:47:14 @agent_ppo2.py:186][0m |          -0.0040 |           7.9808 |           2.7639 |
[32m[20230113 19:47:14 @agent_ppo2.py:186][0m |          -0.0056 |           7.2890 |           2.7615 |
[32m[20230113 19:47:14 @agent_ppo2.py:186][0m |          -0.0070 |           6.7597 |           2.7605 |
[32m[20230113 19:47:14 @agent_ppo2.py:186][0m |          -0.0076 |           6.3953 |           2.7584 |
[32m[20230113 19:47:14 @agent_ppo2.py:186][0m |          -0.0082 |           6.1077 |           2.7607 |
[32m[20230113 19:47:14 @agent_ppo2.py:186][0m |          -0.0090 |           5.9088 |           2.7617 |
[32m[20230113 19:47:14 @agent_ppo2.py:186][0m |          -0.0099 |           5.7445 |           2.7617 |
[32m[20230113 19:47:14 @agent_ppo2.py:186][0m |          -0.0103 |           5.6088 |           2.7621 |
[32m[20230113 19:47:14 @agent_ppo2.py:186][0m |          -0.0108 |           5.4876 |           2.7620 |
[32m[20230113 19:47:14 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 200.31
[32m[20230113 19:47:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 203.60
[32m[20230113 19:47:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.57
[32m[20230113 19:47:15 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 250.57
[32m[20230113 19:47:15 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 250.57
[32m[20230113 19:47:15 @agent_ppo2.py:144][0m Total time:       2.70 min
[32m[20230113 19:47:15 @agent_ppo2.py:146][0m 233472 total steps have happened
[32m[20230113 19:47:15 @agent_ppo2.py:122][0m #------------------------ Iteration 114 --------------------------#
[32m[20230113 19:47:15 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:15 @agent_ppo2.py:186][0m |          -0.0002 |           5.8562 |           2.7274 |
[32m[20230113 19:47:15 @agent_ppo2.py:186][0m |          -0.0050 |           4.9830 |           2.7242 |
[32m[20230113 19:47:15 @agent_ppo2.py:186][0m |          -0.0079 |           4.7377 |           2.7222 |
[32m[20230113 19:47:15 @agent_ppo2.py:186][0m |          -0.0097 |           4.5964 |           2.7205 |
[32m[20230113 19:47:15 @agent_ppo2.py:186][0m |          -0.0109 |           4.4787 |           2.7204 |
[32m[20230113 19:47:15 @agent_ppo2.py:186][0m |          -0.0118 |           4.3872 |           2.7188 |
[32m[20230113 19:47:15 @agent_ppo2.py:186][0m |          -0.0122 |           4.3109 |           2.7169 |
[32m[20230113 19:47:15 @agent_ppo2.py:186][0m |          -0.0129 |           4.2433 |           2.7165 |
[32m[20230113 19:47:15 @agent_ppo2.py:186][0m |          -0.0133 |           4.1795 |           2.7162 |
[32m[20230113 19:47:15 @agent_ppo2.py:186][0m |          -0.0140 |           4.1211 |           2.7155 |
[32m[20230113 19:47:15 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 190.95
[32m[20230113 19:47:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 218.52
[32m[20230113 19:47:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 52.17
[32m[20230113 19:47:16 @agent_ppo2.py:144][0m Total time:       2.72 min
[32m[20230113 19:47:16 @agent_ppo2.py:146][0m 235520 total steps have happened
[32m[20230113 19:47:16 @agent_ppo2.py:122][0m #------------------------ Iteration 115 --------------------------#
[32m[20230113 19:47:16 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:16 @agent_ppo2.py:186][0m |          -0.0008 |           2.6381 |           2.7754 |
[32m[20230113 19:47:16 @agent_ppo2.py:186][0m |          -0.0047 |           2.2545 |           2.7725 |
[32m[20230113 19:47:16 @agent_ppo2.py:186][0m |          -0.0065 |           2.1493 |           2.7703 |
[32m[20230113 19:47:17 @agent_ppo2.py:186][0m |          -0.0080 |           2.0763 |           2.7691 |
[32m[20230113 19:47:17 @agent_ppo2.py:186][0m |          -0.0090 |           2.0294 |           2.7683 |
[32m[20230113 19:47:17 @agent_ppo2.py:186][0m |          -0.0096 |           1.9845 |           2.7665 |
[32m[20230113 19:47:17 @agent_ppo2.py:186][0m |          -0.0104 |           1.9538 |           2.7669 |
[32m[20230113 19:47:17 @agent_ppo2.py:186][0m |          -0.0110 |           1.9237 |           2.7641 |
[32m[20230113 19:47:17 @agent_ppo2.py:186][0m |          -0.0115 |           1.9015 |           2.7647 |
[32m[20230113 19:47:17 @agent_ppo2.py:186][0m |          -0.0122 |           1.8816 |           2.7641 |
[32m[20230113 19:47:17 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:47:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 218.08
[32m[20230113 19:47:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.47
[32m[20230113 19:47:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.09
[32m[20230113 19:47:17 @agent_ppo2.py:144][0m Total time:       2.74 min
[32m[20230113 19:47:17 @agent_ppo2.py:146][0m 237568 total steps have happened
[32m[20230113 19:47:17 @agent_ppo2.py:122][0m #------------------------ Iteration 116 --------------------------#
[32m[20230113 19:47:18 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 19:47:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:18 @agent_ppo2.py:186][0m |          -0.0027 |          29.8969 |           2.6989 |
[32m[20230113 19:47:18 @agent_ppo2.py:186][0m |          -0.0101 |          16.9162 |           2.6920 |
[32m[20230113 19:47:18 @agent_ppo2.py:186][0m |          -0.0122 |          15.3140 |           2.6924 |
[32m[20230113 19:47:18 @agent_ppo2.py:186][0m |          -0.0122 |          14.2132 |           2.6930 |
[32m[20230113 19:47:18 @agent_ppo2.py:186][0m |          -0.0132 |          13.4381 |           2.6931 |
[32m[20230113 19:47:18 @agent_ppo2.py:186][0m |          -0.0137 |          12.1193 |           2.6926 |
[32m[20230113 19:47:18 @agent_ppo2.py:186][0m |          -0.0144 |          11.3284 |           2.6916 |
[32m[20230113 19:47:18 @agent_ppo2.py:186][0m |          -0.0150 |          10.8992 |           2.6916 |
[32m[20230113 19:47:18 @agent_ppo2.py:186][0m |          -0.0149 |          10.3796 |           2.6907 |
[32m[20230113 19:47:18 @agent_ppo2.py:186][0m |          -0.0160 |          10.1324 |           2.6901 |
[32m[20230113 19:47:18 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 19:47:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 61.60
[32m[20230113 19:47:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 176.15
[32m[20230113 19:47:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 125.26
[32m[20230113 19:47:18 @agent_ppo2.py:144][0m Total time:       2.76 min
[32m[20230113 19:47:18 @agent_ppo2.py:146][0m 239616 total steps have happened
[32m[20230113 19:47:18 @agent_ppo2.py:122][0m #------------------------ Iteration 117 --------------------------#
[32m[20230113 19:47:19 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:47:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:19 @agent_ppo2.py:186][0m |           0.0013 |          16.4575 |           2.7343 |
[32m[20230113 19:47:19 @agent_ppo2.py:186][0m |          -0.0036 |           8.6885 |           2.7303 |
[32m[20230113 19:47:19 @agent_ppo2.py:186][0m |          -0.0071 |           7.2288 |           2.7262 |
[32m[20230113 19:47:19 @agent_ppo2.py:186][0m |          -0.0085 |           6.3762 |           2.7263 |
[32m[20230113 19:47:19 @agent_ppo2.py:186][0m |          -0.0099 |           5.8095 |           2.7216 |
[32m[20230113 19:47:19 @agent_ppo2.py:186][0m |          -0.0118 |           5.3013 |           2.7213 |
[32m[20230113 19:47:19 @agent_ppo2.py:186][0m |          -0.0099 |           5.0582 |           2.7200 |
[32m[20230113 19:47:19 @agent_ppo2.py:186][0m |          -0.0118 |           4.6873 |           2.7182 |
[32m[20230113 19:47:19 @agent_ppo2.py:186][0m |          -0.0122 |           4.4637 |           2.7208 |
[32m[20230113 19:47:19 @agent_ppo2.py:186][0m |          -0.0133 |           4.3226 |           2.7189 |
[32m[20230113 19:47:19 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:47:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 137.44
[32m[20230113 19:47:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 202.72
[32m[20230113 19:47:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.22
[32m[20230113 19:47:20 @agent_ppo2.py:144][0m Total time:       2.78 min
[32m[20230113 19:47:20 @agent_ppo2.py:146][0m 241664 total steps have happened
[32m[20230113 19:47:20 @agent_ppo2.py:122][0m #------------------------ Iteration 118 --------------------------#
[32m[20230113 19:47:20 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:47:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:20 @agent_ppo2.py:186][0m |          -0.0012 |          11.7244 |           2.7997 |
[32m[20230113 19:47:20 @agent_ppo2.py:186][0m |          -0.0014 |           7.1621 |           2.7984 |
[32m[20230113 19:47:20 @agent_ppo2.py:186][0m |          -0.0052 |           6.2024 |           2.7929 |
[32m[20230113 19:47:20 @agent_ppo2.py:186][0m |          -0.0049 |           5.8914 |           2.7943 |
[32m[20230113 19:47:20 @agent_ppo2.py:186][0m |          -0.0086 |           5.4230 |           2.7925 |
[32m[20230113 19:47:20 @agent_ppo2.py:186][0m |          -0.0071 |           5.2445 |           2.7899 |
[32m[20230113 19:47:21 @agent_ppo2.py:186][0m |          -0.0083 |           5.0779 |           2.7890 |
[32m[20230113 19:47:21 @agent_ppo2.py:186][0m |          -0.0092 |           4.8648 |           2.7904 |
[32m[20230113 19:47:21 @agent_ppo2.py:186][0m |          -0.0092 |           4.8153 |           2.7872 |
[32m[20230113 19:47:21 @agent_ppo2.py:186][0m |          -0.0098 |           4.5060 |           2.7876 |
[32m[20230113 19:47:21 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:47:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 85.74
[32m[20230113 19:47:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 184.00
[32m[20230113 19:47:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.26
[32m[20230113 19:47:21 @agent_ppo2.py:144][0m Total time:       2.81 min
[32m[20230113 19:47:21 @agent_ppo2.py:146][0m 243712 total steps have happened
[32m[20230113 19:47:21 @agent_ppo2.py:122][0m #------------------------ Iteration 119 --------------------------#
[32m[20230113 19:47:22 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:22 @agent_ppo2.py:186][0m |           0.0010 |           4.4028 |           2.7301 |
[32m[20230113 19:47:22 @agent_ppo2.py:186][0m |          -0.0015 |           4.0741 |           2.7290 |
[32m[20230113 19:47:22 @agent_ppo2.py:186][0m |          -0.0036 |           3.9287 |           2.7280 |
[32m[20230113 19:47:22 @agent_ppo2.py:186][0m |          -0.0042 |           3.8560 |           2.7276 |
[32m[20230113 19:47:22 @agent_ppo2.py:186][0m |          -0.0055 |           3.7915 |           2.7251 |
[32m[20230113 19:47:22 @agent_ppo2.py:186][0m |          -0.0059 |           3.7362 |           2.7254 |
[32m[20230113 19:47:22 @agent_ppo2.py:186][0m |          -0.0065 |           3.7122 |           2.7234 |
[32m[20230113 19:47:22 @agent_ppo2.py:186][0m |          -0.0073 |           3.6619 |           2.7237 |
[32m[20230113 19:47:22 @agent_ppo2.py:186][0m |          -0.0073 |           3.6342 |           2.7230 |
[32m[20230113 19:47:22 @agent_ppo2.py:186][0m |          -0.0080 |           3.6145 |           2.7221 |
[32m[20230113 19:47:22 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 189.21
[32m[20230113 19:47:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 208.58
[32m[20230113 19:47:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.18
[32m[20230113 19:47:23 @agent_ppo2.py:144][0m Total time:       2.83 min
[32m[20230113 19:47:23 @agent_ppo2.py:146][0m 245760 total steps have happened
[32m[20230113 19:47:23 @agent_ppo2.py:122][0m #------------------------ Iteration 120 --------------------------#
[32m[20230113 19:47:23 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:23 @agent_ppo2.py:186][0m |          -0.0001 |           3.4419 |           2.7189 |
[32m[20230113 19:47:23 @agent_ppo2.py:186][0m |          -0.0026 |           3.2269 |           2.7158 |
[32m[20230113 19:47:23 @agent_ppo2.py:186][0m |          -0.0048 |           3.1296 |           2.7147 |
[32m[20230113 19:47:23 @agent_ppo2.py:186][0m |          -0.0064 |           3.0561 |           2.7129 |
[32m[20230113 19:47:23 @agent_ppo2.py:186][0m |          -0.0070 |           3.0022 |           2.7097 |
[32m[20230113 19:47:23 @agent_ppo2.py:186][0m |          -0.0078 |           2.9592 |           2.7102 |
[32m[20230113 19:47:23 @agent_ppo2.py:186][0m |          -0.0091 |           2.9201 |           2.7085 |
[32m[20230113 19:47:23 @agent_ppo2.py:186][0m |          -0.0095 |           2.8772 |           2.7082 |
[32m[20230113 19:47:23 @agent_ppo2.py:186][0m |          -0.0102 |           2.8467 |           2.7080 |
[32m[20230113 19:47:24 @agent_ppo2.py:186][0m |          -0.0105 |           2.8134 |           2.7070 |
[32m[20230113 19:47:24 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 199.41
[32m[20230113 19:47:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 202.53
[32m[20230113 19:47:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.70
[32m[20230113 19:47:24 @agent_ppo2.py:144][0m Total time:       2.86 min
[32m[20230113 19:47:24 @agent_ppo2.py:146][0m 247808 total steps have happened
[32m[20230113 19:47:24 @agent_ppo2.py:122][0m #------------------------ Iteration 121 --------------------------#
[32m[20230113 19:47:24 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 19:47:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:24 @agent_ppo2.py:186][0m |           0.0004 |          18.0911 |           2.7107 |
[32m[20230113 19:47:24 @agent_ppo2.py:186][0m |          -0.0005 |          10.4094 |           2.7104 |
[32m[20230113 19:47:24 @agent_ppo2.py:186][0m |          -0.0034 |           8.6421 |           2.7105 |
[32m[20230113 19:47:24 @agent_ppo2.py:186][0m |          -0.0050 |           7.7865 |           2.7091 |
[32m[20230113 19:47:24 @agent_ppo2.py:186][0m |          -0.0064 |           6.8441 |           2.7084 |
[32m[20230113 19:47:25 @agent_ppo2.py:186][0m |          -0.0082 |           6.2711 |           2.7071 |
[32m[20230113 19:47:25 @agent_ppo2.py:186][0m |          -0.0085 |           5.8027 |           2.7068 |
[32m[20230113 19:47:25 @agent_ppo2.py:186][0m |          -0.0100 |           5.5610 |           2.7059 |
[32m[20230113 19:47:25 @agent_ppo2.py:186][0m |          -0.0101 |           5.3984 |           2.7050 |
[32m[20230113 19:47:25 @agent_ppo2.py:186][0m |          -0.0104 |           5.2704 |           2.7047 |
[32m[20230113 19:47:25 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 19:47:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 89.56
[32m[20230113 19:47:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 218.83
[32m[20230113 19:47:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 229.54
[32m[20230113 19:47:25 @agent_ppo2.py:144][0m Total time:       2.88 min
[32m[20230113 19:47:25 @agent_ppo2.py:146][0m 249856 total steps have happened
[32m[20230113 19:47:25 @agent_ppo2.py:122][0m #------------------------ Iteration 122 --------------------------#
[32m[20230113 19:47:26 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:26 @agent_ppo2.py:186][0m |          -0.0014 |           4.1254 |           2.7301 |
[32m[20230113 19:47:26 @agent_ppo2.py:186][0m |          -0.0052 |           3.3940 |           2.7255 |
[32m[20230113 19:47:26 @agent_ppo2.py:186][0m |          -0.0067 |           3.1839 |           2.7257 |
[32m[20230113 19:47:26 @agent_ppo2.py:186][0m |          -0.0074 |           3.0576 |           2.7257 |
[32m[20230113 19:47:26 @agent_ppo2.py:186][0m |          -0.0082 |           2.9634 |           2.7269 |
[32m[20230113 19:47:26 @agent_ppo2.py:186][0m |          -0.0088 |           2.9017 |           2.7303 |
[32m[20230113 19:47:26 @agent_ppo2.py:186][0m |          -0.0098 |           2.8350 |           2.7300 |
[32m[20230113 19:47:26 @agent_ppo2.py:186][0m |          -0.0104 |           2.7807 |           2.7330 |
[32m[20230113 19:47:26 @agent_ppo2.py:186][0m |          -0.0110 |           2.7333 |           2.7326 |
[32m[20230113 19:47:26 @agent_ppo2.py:186][0m |          -0.0116 |           2.6929 |           2.7338 |
[32m[20230113 19:47:26 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 192.83
[32m[20230113 19:47:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 197.49
[32m[20230113 19:47:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 138.71
[32m[20230113 19:47:26 @agent_ppo2.py:144][0m Total time:       2.90 min
[32m[20230113 19:47:26 @agent_ppo2.py:146][0m 251904 total steps have happened
[32m[20230113 19:47:26 @agent_ppo2.py:122][0m #------------------------ Iteration 123 --------------------------#
[32m[20230113 19:47:27 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:47:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:27 @agent_ppo2.py:186][0m |          -0.0064 |          17.2708 |           2.7084 |
[32m[20230113 19:47:27 @agent_ppo2.py:186][0m |          -0.0003 |          10.0891 |           2.7057 |
[32m[20230113 19:47:27 @agent_ppo2.py:186][0m |           0.0013 |           8.6404 |           2.7024 |
[32m[20230113 19:47:27 @agent_ppo2.py:186][0m |          -0.0112 |           7.4342 |           2.7021 |
[32m[20230113 19:47:27 @agent_ppo2.py:186][0m |          -0.0095 |           6.7652 |           2.7005 |
[32m[20230113 19:47:27 @agent_ppo2.py:186][0m |          -0.0100 |           6.6588 |           2.6992 |
[32m[20230113 19:47:27 @agent_ppo2.py:186][0m |          -0.0125 |           6.1373 |           2.6983 |
[32m[20230113 19:47:27 @agent_ppo2.py:186][0m |          -0.0047 |           6.4450 |           2.6965 |
[32m[20230113 19:47:27 @agent_ppo2.py:186][0m |          -0.0110 |           6.0405 |           2.6958 |
[32m[20230113 19:47:27 @agent_ppo2.py:186][0m |          -0.0124 |           5.5631 |           2.6948 |
[32m[20230113 19:47:27 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 19:47:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 41.39
[32m[20230113 19:47:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 190.86
[32m[20230113 19:47:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 215.94
[32m[20230113 19:47:28 @agent_ppo2.py:144][0m Total time:       2.92 min
[32m[20230113 19:47:28 @agent_ppo2.py:146][0m 253952 total steps have happened
[32m[20230113 19:47:28 @agent_ppo2.py:122][0m #------------------------ Iteration 124 --------------------------#
[32m[20230113 19:47:28 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:47:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:28 @agent_ppo2.py:186][0m |          -0.0008 |          11.4188 |           2.7241 |
[32m[20230113 19:47:28 @agent_ppo2.py:186][0m |          -0.0027 |           6.5788 |           2.7192 |
[32m[20230113 19:47:29 @agent_ppo2.py:186][0m |          -0.0044 |           5.5605 |           2.7197 |
[32m[20230113 19:47:29 @agent_ppo2.py:186][0m |           0.0053 |           5.0930 |           2.7190 |
[32m[20230113 19:47:29 @agent_ppo2.py:186][0m |          -0.0021 |           4.8502 |           2.7194 |
[32m[20230113 19:47:29 @agent_ppo2.py:186][0m |          -0.0106 |           4.7389 |           2.7183 |
[32m[20230113 19:47:29 @agent_ppo2.py:186][0m |          -0.0119 |           4.5520 |           2.7166 |
[32m[20230113 19:47:29 @agent_ppo2.py:186][0m |          -0.0093 |           4.6871 |           2.7154 |
[32m[20230113 19:47:29 @agent_ppo2.py:186][0m |          -0.0055 |           4.6221 |           2.7142 |
[32m[20230113 19:47:29 @agent_ppo2.py:186][0m |          -0.0158 |           4.2589 |           2.7132 |
[32m[20230113 19:47:29 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 19:47:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 109.43
[32m[20230113 19:47:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 184.20
[32m[20230113 19:47:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 228.37
[32m[20230113 19:47:29 @agent_ppo2.py:144][0m Total time:       2.95 min
[32m[20230113 19:47:29 @agent_ppo2.py:146][0m 256000 total steps have happened
[32m[20230113 19:47:29 @agent_ppo2.py:122][0m #------------------------ Iteration 125 --------------------------#
[32m[20230113 19:47:30 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:30 @agent_ppo2.py:186][0m |          -0.0001 |           9.8639 |           2.7150 |
[32m[20230113 19:47:30 @agent_ppo2.py:186][0m |          -0.0040 |           8.1555 |           2.7101 |
[32m[20230113 19:47:30 @agent_ppo2.py:186][0m |          -0.0058 |           7.2151 |           2.7083 |
[32m[20230113 19:47:30 @agent_ppo2.py:186][0m |          -0.0075 |           6.1300 |           2.7084 |
[32m[20230113 19:47:30 @agent_ppo2.py:186][0m |          -0.0084 |           5.5000 |           2.7091 |
[32m[20230113 19:47:30 @agent_ppo2.py:186][0m |          -0.0093 |           5.0398 |           2.7092 |
[32m[20230113 19:47:30 @agent_ppo2.py:186][0m |          -0.0095 |           4.8228 |           2.7076 |
[32m[20230113 19:47:30 @agent_ppo2.py:186][0m |          -0.0099 |           4.6180 |           2.7095 |
[32m[20230113 19:47:30 @agent_ppo2.py:186][0m |          -0.0103 |           4.5375 |           2.7093 |
[32m[20230113 19:47:30 @agent_ppo2.py:186][0m |          -0.0111 |           4.3794 |           2.7090 |
[32m[20230113 19:47:30 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 168.93
[32m[20230113 19:47:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 176.95
[32m[20230113 19:47:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 223.74
[32m[20230113 19:47:31 @agent_ppo2.py:144][0m Total time:       2.97 min
[32m[20230113 19:47:31 @agent_ppo2.py:146][0m 258048 total steps have happened
[32m[20230113 19:47:31 @agent_ppo2.py:122][0m #------------------------ Iteration 126 --------------------------#
[32m[20230113 19:47:31 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:47:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:31 @agent_ppo2.py:186][0m |           0.0020 |          12.7132 |           2.7135 |
[32m[20230113 19:47:31 @agent_ppo2.py:186][0m |          -0.0043 |           8.6373 |           2.7120 |
[32m[20230113 19:47:31 @agent_ppo2.py:186][0m |          -0.0025 |           7.6527 |           2.7118 |
[32m[20230113 19:47:31 @agent_ppo2.py:186][0m |          -0.0071 |           6.9854 |           2.7115 |
[32m[20230113 19:47:32 @agent_ppo2.py:186][0m |          -0.0052 |           6.6542 |           2.7090 |
[32m[20230113 19:47:32 @agent_ppo2.py:186][0m |          -0.0072 |           6.2161 |           2.7089 |
[32m[20230113 19:47:32 @agent_ppo2.py:186][0m |          -0.0067 |           5.9236 |           2.7089 |
[32m[20230113 19:47:32 @agent_ppo2.py:186][0m |          -0.0080 |           5.6617 |           2.7091 |
[32m[20230113 19:47:32 @agent_ppo2.py:186][0m |          -0.0076 |           5.5292 |           2.7084 |
[32m[20230113 19:47:32 @agent_ppo2.py:186][0m |          -0.0018 |           5.3361 |           2.7072 |
[32m[20230113 19:47:32 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 19:47:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 93.11
[32m[20230113 19:47:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 190.49
[32m[20230113 19:47:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -30.39
[32m[20230113 19:47:32 @agent_ppo2.py:144][0m Total time:       2.99 min
[32m[20230113 19:47:32 @agent_ppo2.py:146][0m 260096 total steps have happened
[32m[20230113 19:47:32 @agent_ppo2.py:122][0m #------------------------ Iteration 127 --------------------------#
[32m[20230113 19:47:33 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:33 @agent_ppo2.py:186][0m |           0.0009 |           4.2277 |           2.7543 |
[32m[20230113 19:47:33 @agent_ppo2.py:186][0m |          -0.0018 |           3.8285 |           2.7521 |
[32m[20230113 19:47:33 @agent_ppo2.py:186][0m |          -0.0038 |           3.7008 |           2.7535 |
[32m[20230113 19:47:33 @agent_ppo2.py:186][0m |          -0.0054 |           3.6378 |           2.7527 |
[32m[20230113 19:47:33 @agent_ppo2.py:186][0m |          -0.0071 |           3.5706 |           2.7516 |
[32m[20230113 19:47:33 @agent_ppo2.py:186][0m |          -0.0080 |           3.5323 |           2.7529 |
[32m[20230113 19:47:33 @agent_ppo2.py:186][0m |          -0.0087 |           3.4935 |           2.7524 |
[32m[20230113 19:47:33 @agent_ppo2.py:186][0m |          -0.0095 |           3.4680 |           2.7518 |
[32m[20230113 19:47:33 @agent_ppo2.py:186][0m |          -0.0098 |           3.4357 |           2.7527 |
[32m[20230113 19:47:33 @agent_ppo2.py:186][0m |          -0.0106 |           3.4042 |           2.7532 |
[32m[20230113 19:47:33 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 180.41
[32m[20230113 19:47:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 198.24
[32m[20230113 19:47:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.78
[32m[20230113 19:47:33 @agent_ppo2.py:144][0m Total time:       3.02 min
[32m[20230113 19:47:33 @agent_ppo2.py:146][0m 262144 total steps have happened
[32m[20230113 19:47:33 @agent_ppo2.py:122][0m #------------------------ Iteration 128 --------------------------#
[32m[20230113 19:47:34 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:34 @agent_ppo2.py:186][0m |          -0.0008 |           2.7336 |           2.7672 |
[32m[20230113 19:47:34 @agent_ppo2.py:186][0m |          -0.0048 |           2.4822 |           2.7643 |
[32m[20230113 19:47:34 @agent_ppo2.py:186][0m |          -0.0072 |           2.3704 |           2.7598 |
[32m[20230113 19:47:34 @agent_ppo2.py:186][0m |          -0.0080 |           2.2992 |           2.7594 |
[32m[20230113 19:47:34 @agent_ppo2.py:186][0m |          -0.0089 |           2.2510 |           2.7580 |
[32m[20230113 19:47:34 @agent_ppo2.py:186][0m |          -0.0099 |           2.1924 |           2.7598 |
[32m[20230113 19:47:34 @agent_ppo2.py:186][0m |          -0.0104 |           2.1525 |           2.7597 |
[32m[20230113 19:47:34 @agent_ppo2.py:186][0m |          -0.0109 |           2.1284 |           2.7591 |
[32m[20230113 19:47:34 @agent_ppo2.py:186][0m |          -0.0114 |           2.0869 |           2.7603 |
[32m[20230113 19:47:34 @agent_ppo2.py:186][0m |          -0.0117 |           2.0593 |           2.7598 |
[32m[20230113 19:47:34 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 206.66
[32m[20230113 19:47:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 210.05
[32m[20230113 19:47:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -65.07
[32m[20230113 19:47:35 @agent_ppo2.py:144][0m Total time:       3.04 min
[32m[20230113 19:47:35 @agent_ppo2.py:146][0m 264192 total steps have happened
[32m[20230113 19:47:35 @agent_ppo2.py:122][0m #------------------------ Iteration 129 --------------------------#
[32m[20230113 19:47:35 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:47:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:35 @agent_ppo2.py:186][0m |          -0.0033 |          18.9316 |           2.7036 |
[32m[20230113 19:47:35 @agent_ppo2.py:186][0m |          -0.0065 |           9.8802 |           2.7030 |
[32m[20230113 19:47:35 @agent_ppo2.py:186][0m |          -0.0067 |           7.9542 |           2.7025 |
[32m[20230113 19:47:36 @agent_ppo2.py:186][0m |          -0.0097 |           6.9046 |           2.7032 |
[32m[20230113 19:47:36 @agent_ppo2.py:186][0m |          -0.0105 |           6.3003 |           2.7035 |
[32m[20230113 19:47:36 @agent_ppo2.py:186][0m |          -0.0108 |           5.8516 |           2.7023 |
[32m[20230113 19:47:36 @agent_ppo2.py:186][0m |          -0.0031 |           6.0858 |           2.7039 |
[32m[20230113 19:47:36 @agent_ppo2.py:186][0m |          -0.0085 |           5.6241 |           2.7020 |
[32m[20230113 19:47:36 @agent_ppo2.py:186][0m |          -0.0092 |           5.2497 |           2.7039 |
[32m[20230113 19:47:36 @agent_ppo2.py:186][0m |          -0.0127 |           5.1134 |           2.7037 |
[32m[20230113 19:47:36 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:47:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 92.25
[32m[20230113 19:47:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 205.33
[32m[20230113 19:47:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.53
[32m[20230113 19:47:36 @agent_ppo2.py:144][0m Total time:       3.06 min
[32m[20230113 19:47:36 @agent_ppo2.py:146][0m 266240 total steps have happened
[32m[20230113 19:47:36 @agent_ppo2.py:122][0m #------------------------ Iteration 130 --------------------------#
[32m[20230113 19:47:37 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:47:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:37 @agent_ppo2.py:186][0m |           0.0000 |          19.4705 |           2.7896 |
[32m[20230113 19:47:37 @agent_ppo2.py:186][0m |          -0.0029 |          10.5576 |           2.7862 |
[32m[20230113 19:47:37 @agent_ppo2.py:186][0m |          -0.0045 |           9.3730 |           2.7863 |
[32m[20230113 19:47:37 @agent_ppo2.py:186][0m |          -0.0057 |           8.5171 |           2.7842 |
[32m[20230113 19:47:37 @agent_ppo2.py:186][0m |          -0.0060 |           8.0572 |           2.7827 |
[32m[20230113 19:47:37 @agent_ppo2.py:186][0m |          -0.0074 |           7.6764 |           2.7816 |
[32m[20230113 19:47:37 @agent_ppo2.py:186][0m |          -0.0080 |           7.5723 |           2.7802 |
[32m[20230113 19:47:37 @agent_ppo2.py:186][0m |          -0.0081 |           7.2168 |           2.7821 |
[32m[20230113 19:47:37 @agent_ppo2.py:186][0m |          -0.0087 |           7.0822 |           2.7790 |
[32m[20230113 19:47:37 @agent_ppo2.py:186][0m |          -0.0092 |           6.7918 |           2.7798 |
[32m[20230113 19:47:37 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 19:47:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 114.25
[32m[20230113 19:47:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.94
[32m[20230113 19:47:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.21
[32m[20230113 19:47:38 @agent_ppo2.py:144][0m Total time:       3.09 min
[32m[20230113 19:47:38 @agent_ppo2.py:146][0m 268288 total steps have happened
[32m[20230113 19:47:38 @agent_ppo2.py:122][0m #------------------------ Iteration 131 --------------------------#
[32m[20230113 19:47:38 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 19:47:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:38 @agent_ppo2.py:186][0m |          -0.0005 |          13.7097 |           2.7393 |
[32m[20230113 19:47:38 @agent_ppo2.py:186][0m |          -0.0053 |          11.6877 |           2.7385 |
[32m[20230113 19:47:38 @agent_ppo2.py:186][0m |          -0.0078 |          10.2587 |           2.7380 |
[32m[20230113 19:47:38 @agent_ppo2.py:186][0m |          -0.0090 |           9.7965 |           2.7360 |
[32m[20230113 19:47:38 @agent_ppo2.py:186][0m |          -0.0113 |           9.3787 |           2.7373 |
[32m[20230113 19:47:38 @agent_ppo2.py:186][0m |          -0.0118 |           9.2364 |           2.7366 |
[32m[20230113 19:47:38 @agent_ppo2.py:186][0m |          -0.0124 |           8.8764 |           2.7358 |
[32m[20230113 19:47:38 @agent_ppo2.py:186][0m |          -0.0123 |           8.7734 |           2.7367 |
[32m[20230113 19:47:39 @agent_ppo2.py:186][0m |          -0.0134 |           8.3638 |           2.7369 |
[32m[20230113 19:47:39 @agent_ppo2.py:186][0m |          -0.0140 |           8.2519 |           2.7371 |
[32m[20230113 19:47:39 @agent_ppo2.py:131][0m Policy update time: 0.40 s
[32m[20230113 19:47:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 112.55
[32m[20230113 19:47:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 194.60
[32m[20230113 19:47:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 220.42
[32m[20230113 19:47:39 @agent_ppo2.py:144][0m Total time:       3.11 min
[32m[20230113 19:47:39 @agent_ppo2.py:146][0m 270336 total steps have happened
[32m[20230113 19:47:39 @agent_ppo2.py:122][0m #------------------------ Iteration 132 --------------------------#
[32m[20230113 19:47:40 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:47:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:40 @agent_ppo2.py:186][0m |           0.0002 |           4.1185 |           2.6802 |
[32m[20230113 19:47:40 @agent_ppo2.py:186][0m |          -0.0042 |           3.7759 |           2.6756 |
[32m[20230113 19:47:40 @agent_ppo2.py:186][0m |          -0.0068 |           3.5996 |           2.6743 |
[32m[20230113 19:47:40 @agent_ppo2.py:186][0m |          -0.0083 |           3.4952 |           2.6735 |
[32m[20230113 19:47:40 @agent_ppo2.py:186][0m |          -0.0096 |           3.4201 |           2.6714 |
[32m[20230113 19:47:40 @agent_ppo2.py:186][0m |          -0.0101 |           3.3584 |           2.6713 |
[32m[20230113 19:47:40 @agent_ppo2.py:186][0m |          -0.0107 |           3.3058 |           2.6701 |
[32m[20230113 19:47:40 @agent_ppo2.py:186][0m |          -0.0110 |           3.2746 |           2.6704 |
[32m[20230113 19:47:40 @agent_ppo2.py:186][0m |          -0.0115 |           3.2353 |           2.6686 |
[32m[20230113 19:47:40 @agent_ppo2.py:186][0m |          -0.0121 |           3.2032 |           2.6695 |
[32m[20230113 19:47:40 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 195.61
[32m[20230113 19:47:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 202.99
[32m[20230113 19:47:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 224.22
[32m[20230113 19:47:40 @agent_ppo2.py:144][0m Total time:       3.13 min
[32m[20230113 19:47:40 @agent_ppo2.py:146][0m 272384 total steps have happened
[32m[20230113 19:47:40 @agent_ppo2.py:122][0m #------------------------ Iteration 133 --------------------------#
[32m[20230113 19:47:41 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 19:47:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:41 @agent_ppo2.py:186][0m |          -0.0021 |          34.0711 |           2.7135 |
[32m[20230113 19:47:41 @agent_ppo2.py:186][0m |          -0.0062 |          15.7242 |           2.7101 |
[32m[20230113 19:47:41 @agent_ppo2.py:186][0m |          -0.0088 |          13.6352 |           2.7071 |
[32m[20230113 19:47:41 @agent_ppo2.py:186][0m |          -0.0128 |          12.5044 |           2.7090 |
[32m[20230113 19:47:41 @agent_ppo2.py:186][0m |          -0.0150 |          11.6166 |           2.7099 |
[32m[20230113 19:47:41 @agent_ppo2.py:186][0m |          -0.0155 |          11.0070 |           2.7123 |
[32m[20230113 19:47:41 @agent_ppo2.py:186][0m |          -0.0136 |          10.4926 |           2.7106 |
[32m[20230113 19:47:41 @agent_ppo2.py:186][0m |          -0.0126 |          10.0382 |           2.7129 |
[32m[20230113 19:47:41 @agent_ppo2.py:186][0m |          -0.0165 |           9.8085 |           2.7120 |
[32m[20230113 19:47:41 @agent_ppo2.py:186][0m |          -0.0167 |           9.6652 |           2.7130 |
[32m[20230113 19:47:41 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 19:47:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 26.93
[32m[20230113 19:47:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 189.22
[32m[20230113 19:47:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.91
[32m[20230113 19:47:42 @agent_ppo2.py:144][0m Total time:       3.15 min
[32m[20230113 19:47:42 @agent_ppo2.py:146][0m 274432 total steps have happened
[32m[20230113 19:47:42 @agent_ppo2.py:122][0m #------------------------ Iteration 134 --------------------------#
[32m[20230113 19:47:42 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:42 @agent_ppo2.py:186][0m |           0.0005 |           8.0802 |           2.7135 |
[32m[20230113 19:47:42 @agent_ppo2.py:186][0m |          -0.0006 |           6.8401 |           2.7143 |
[32m[20230113 19:47:42 @agent_ppo2.py:186][0m |          -0.0027 |           6.3306 |           2.7139 |
[32m[20230113 19:47:42 @agent_ppo2.py:186][0m |          -0.0042 |           6.0313 |           2.7116 |
[32m[20230113 19:47:42 @agent_ppo2.py:186][0m |          -0.0053 |           5.7665 |           2.7113 |
[32m[20230113 19:47:42 @agent_ppo2.py:186][0m |          -0.0061 |           5.5838 |           2.7110 |
[32m[20230113 19:47:42 @agent_ppo2.py:186][0m |          -0.0069 |           5.4013 |           2.7089 |
[32m[20230113 19:47:43 @agent_ppo2.py:186][0m |          -0.0074 |           5.2615 |           2.7100 |
[32m[20230113 19:47:43 @agent_ppo2.py:186][0m |          -0.0077 |           5.1385 |           2.7080 |
[32m[20230113 19:47:43 @agent_ppo2.py:186][0m |          -0.0081 |           5.0295 |           2.7082 |
[32m[20230113 19:47:43 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 191.64
[32m[20230113 19:47:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 215.51
[32m[20230113 19:47:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 116.88
[32m[20230113 19:47:43 @agent_ppo2.py:144][0m Total time:       3.18 min
[32m[20230113 19:47:43 @agent_ppo2.py:146][0m 276480 total steps have happened
[32m[20230113 19:47:43 @agent_ppo2.py:122][0m #------------------------ Iteration 135 --------------------------#
[32m[20230113 19:47:44 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:44 @agent_ppo2.py:186][0m |           0.0001 |           4.8207 |           2.7503 |
[32m[20230113 19:47:44 @agent_ppo2.py:186][0m |          -0.0043 |           4.2441 |           2.7504 |
[32m[20230113 19:47:44 @agent_ppo2.py:186][0m |          -0.0059 |           4.0757 |           2.7535 |
[32m[20230113 19:47:44 @agent_ppo2.py:186][0m |          -0.0071 |           3.9486 |           2.7527 |
[32m[20230113 19:47:44 @agent_ppo2.py:186][0m |          -0.0085 |           3.8361 |           2.7538 |
[32m[20230113 19:47:44 @agent_ppo2.py:186][0m |          -0.0094 |           3.7414 |           2.7529 |
[32m[20230113 19:47:44 @agent_ppo2.py:186][0m |          -0.0100 |           3.6575 |           2.7542 |
[32m[20230113 19:47:44 @agent_ppo2.py:186][0m |          -0.0109 |           3.5631 |           2.7548 |
[32m[20230113 19:47:44 @agent_ppo2.py:186][0m |          -0.0111 |           3.4824 |           2.7540 |
[32m[20230113 19:47:44 @agent_ppo2.py:186][0m |          -0.0116 |           3.4147 |           2.7549 |
[32m[20230113 19:47:44 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 200.14
[32m[20230113 19:47:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.70
[32m[20230113 19:47:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.49
[32m[20230113 19:47:45 @agent_ppo2.py:144][0m Total time:       3.20 min
[32m[20230113 19:47:45 @agent_ppo2.py:146][0m 278528 total steps have happened
[32m[20230113 19:47:45 @agent_ppo2.py:122][0m #------------------------ Iteration 136 --------------------------#
[32m[20230113 19:47:45 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:45 @agent_ppo2.py:186][0m |           0.0004 |           3.6334 |           2.8346 |
[32m[20230113 19:47:45 @agent_ppo2.py:186][0m |          -0.0022 |           3.4096 |           2.8325 |
[32m[20230113 19:47:45 @agent_ppo2.py:186][0m |          -0.0045 |           3.2986 |           2.8310 |
[32m[20230113 19:47:45 @agent_ppo2.py:186][0m |          -0.0057 |           3.2365 |           2.8317 |
[32m[20230113 19:47:45 @agent_ppo2.py:186][0m |          -0.0068 |           3.1738 |           2.8289 |
[32m[20230113 19:47:45 @agent_ppo2.py:186][0m |          -0.0072 |           3.1348 |           2.8294 |
[32m[20230113 19:47:45 @agent_ppo2.py:186][0m |          -0.0082 |           3.1145 |           2.8287 |
[32m[20230113 19:47:45 @agent_ppo2.py:186][0m |          -0.0087 |           3.0686 |           2.8273 |
[32m[20230113 19:47:45 @agent_ppo2.py:186][0m |          -0.0091 |           3.0417 |           2.8254 |
[32m[20230113 19:47:46 @agent_ppo2.py:186][0m |          -0.0096 |           3.0128 |           2.8261 |
[32m[20230113 19:47:46 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 210.62
[32m[20230113 19:47:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.62
[32m[20230113 19:47:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.93
[32m[20230113 19:47:46 @agent_ppo2.py:144][0m Total time:       3.22 min
[32m[20230113 19:47:46 @agent_ppo2.py:146][0m 280576 total steps have happened
[32m[20230113 19:47:46 @agent_ppo2.py:122][0m #------------------------ Iteration 137 --------------------------#
[32m[20230113 19:47:46 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 19:47:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:46 @agent_ppo2.py:186][0m |          -0.0051 |          28.1486 |           2.6715 |
[32m[20230113 19:47:46 @agent_ppo2.py:186][0m |          -0.0015 |          10.7011 |           2.6685 |
[32m[20230113 19:47:46 @agent_ppo2.py:186][0m |          -0.0079 |           8.7237 |           2.6627 |
[32m[20230113 19:47:46 @agent_ppo2.py:186][0m |          -0.0077 |           7.2856 |           2.6601 |
[32m[20230113 19:47:47 @agent_ppo2.py:186][0m |          -0.0147 |           6.7763 |           2.6594 |
[32m[20230113 19:47:47 @agent_ppo2.py:186][0m |          -0.0169 |           6.2068 |           2.6598 |
[32m[20230113 19:47:47 @agent_ppo2.py:186][0m |          -0.0152 |           5.7252 |           2.6596 |
[32m[20230113 19:47:47 @agent_ppo2.py:186][0m |          -0.0133 |           5.3964 |           2.6603 |
[32m[20230113 19:47:47 @agent_ppo2.py:186][0m |          -0.0162 |           5.4498 |           2.6602 |
[32m[20230113 19:47:47 @agent_ppo2.py:186][0m |          -0.0197 |           4.8129 |           2.6602 |
[32m[20230113 19:47:47 @agent_ppo2.py:131][0m Policy update time: 0.34 s
[32m[20230113 19:47:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 8.77
[32m[20230113 19:47:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 42.13
[32m[20230113 19:47:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.14
[32m[20230113 19:47:47 @agent_ppo2.py:144][0m Total time:       3.24 min
[32m[20230113 19:47:47 @agent_ppo2.py:146][0m 282624 total steps have happened
[32m[20230113 19:47:47 @agent_ppo2.py:122][0m #------------------------ Iteration 138 --------------------------#
[32m[20230113 19:47:48 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 19:47:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:48 @agent_ppo2.py:186][0m |           0.0045 |          21.8672 |           2.7114 |
[32m[20230113 19:47:48 @agent_ppo2.py:186][0m |          -0.0062 |          15.6485 |           2.7111 |
[32m[20230113 19:47:48 @agent_ppo2.py:186][0m |          -0.0094 |          12.5775 |           2.7080 |
[32m[20230113 19:47:48 @agent_ppo2.py:186][0m |          -0.0099 |          11.4104 |           2.7058 |
[32m[20230113 19:47:48 @agent_ppo2.py:186][0m |          -0.0125 |          10.4533 |           2.7034 |
[32m[20230113 19:47:48 @agent_ppo2.py:186][0m |          -0.0121 |          10.4252 |           2.7029 |
[32m[20230113 19:47:48 @agent_ppo2.py:186][0m |          -0.0111 |           9.5330 |           2.7006 |
[32m[20230113 19:47:48 @agent_ppo2.py:186][0m |          -0.0156 |           8.9021 |           2.6997 |
[32m[20230113 19:47:48 @agent_ppo2.py:186][0m |          -0.0136 |           8.3271 |           2.7012 |
[32m[20230113 19:47:48 @agent_ppo2.py:186][0m |          -0.0144 |           7.9076 |           2.6992 |
[32m[20230113 19:47:48 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 19:47:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 103.98
[32m[20230113 19:47:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 194.06
[32m[20230113 19:47:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.99
[32m[20230113 19:47:48 @agent_ppo2.py:144][0m Total time:       3.26 min
[32m[20230113 19:47:48 @agent_ppo2.py:146][0m 284672 total steps have happened
[32m[20230113 19:47:48 @agent_ppo2.py:122][0m #------------------------ Iteration 139 --------------------------#
[32m[20230113 19:47:49 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:49 @agent_ppo2.py:186][0m |          -0.0004 |           5.5735 |           2.7294 |
[32m[20230113 19:47:49 @agent_ppo2.py:186][0m |          -0.0053 |           3.6852 |           2.7260 |
[32m[20230113 19:47:49 @agent_ppo2.py:186][0m |          -0.0067 |           3.5060 |           2.7262 |
[32m[20230113 19:47:49 @agent_ppo2.py:186][0m |          -0.0075 |           3.3816 |           2.7245 |
[32m[20230113 19:47:49 @agent_ppo2.py:186][0m |          -0.0081 |           3.2863 |           2.7249 |
[32m[20230113 19:47:49 @agent_ppo2.py:186][0m |          -0.0084 |           3.2011 |           2.7260 |
[32m[20230113 19:47:49 @agent_ppo2.py:186][0m |          -0.0091 |           3.1384 |           2.7250 |
[32m[20230113 19:47:49 @agent_ppo2.py:186][0m |          -0.0091 |           3.0731 |           2.7247 |
[32m[20230113 19:47:49 @agent_ppo2.py:186][0m |          -0.0101 |           3.0305 |           2.7238 |
[32m[20230113 19:47:49 @agent_ppo2.py:186][0m |          -0.0102 |           2.9831 |           2.7239 |
[32m[20230113 19:47:49 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 212.63
[32m[20230113 19:47:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.34
[32m[20230113 19:47:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.95
[32m[20230113 19:47:50 @agent_ppo2.py:144][0m Total time:       3.29 min
[32m[20230113 19:47:50 @agent_ppo2.py:146][0m 286720 total steps have happened
[32m[20230113 19:47:50 @agent_ppo2.py:122][0m #------------------------ Iteration 140 --------------------------#
[32m[20230113 19:47:50 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:47:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:51 @agent_ppo2.py:186][0m |           0.0017 |          30.5604 |           2.7520 |
[32m[20230113 19:47:51 @agent_ppo2.py:186][0m |           0.0000 |          17.5319 |           2.7525 |
[32m[20230113 19:47:51 @agent_ppo2.py:186][0m |          -0.0045 |          14.8063 |           2.7498 |
[32m[20230113 19:47:51 @agent_ppo2.py:186][0m |          -0.0048 |          14.0254 |           2.7497 |
[32m[20230113 19:47:51 @agent_ppo2.py:186][0m |          -0.0062 |          12.9321 |           2.7468 |
[32m[20230113 19:47:51 @agent_ppo2.py:186][0m |          -0.0072 |          13.4951 |           2.7461 |
[32m[20230113 19:47:51 @agent_ppo2.py:186][0m |          -0.0081 |          12.2682 |           2.7453 |
[32m[20230113 19:47:51 @agent_ppo2.py:186][0m |          -0.0076 |          11.9927 |           2.7431 |
[32m[20230113 19:47:51 @agent_ppo2.py:186][0m |          -0.0072 |          11.5655 |           2.7423 |
[32m[20230113 19:47:51 @agent_ppo2.py:186][0m |          -0.0100 |          11.4291 |           2.7405 |
[32m[20230113 19:47:51 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 19:47:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 50.78
[32m[20230113 19:47:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 214.36
[32m[20230113 19:47:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.83
[32m[20230113 19:47:51 @agent_ppo2.py:144][0m Total time:       3.31 min
[32m[20230113 19:47:51 @agent_ppo2.py:146][0m 288768 total steps have happened
[32m[20230113 19:47:51 @agent_ppo2.py:122][0m #------------------------ Iteration 141 --------------------------#
[32m[20230113 19:47:52 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:52 @agent_ppo2.py:186][0m |           0.0005 |           4.3113 |           2.7188 |
[32m[20230113 19:47:52 @agent_ppo2.py:186][0m |          -0.0030 |           3.8023 |           2.7189 |
[32m[20230113 19:47:52 @agent_ppo2.py:186][0m |          -0.0053 |           3.6951 |           2.7183 |
[32m[20230113 19:47:52 @agent_ppo2.py:186][0m |          -0.0068 |           3.6392 |           2.7208 |
[32m[20230113 19:47:52 @agent_ppo2.py:186][0m |          -0.0076 |           3.5866 |           2.7195 |
[32m[20230113 19:47:52 @agent_ppo2.py:186][0m |          -0.0084 |           3.5426 |           2.7200 |
[32m[20230113 19:47:52 @agent_ppo2.py:186][0m |          -0.0085 |           3.5231 |           2.7203 |
[32m[20230113 19:47:52 @agent_ppo2.py:186][0m |          -0.0094 |           3.4953 |           2.7193 |
[32m[20230113 19:47:52 @agent_ppo2.py:186][0m |          -0.0093 |           3.4618 |           2.7204 |
[32m[20230113 19:47:52 @agent_ppo2.py:186][0m |          -0.0099 |           3.4337 |           2.7214 |
[32m[20230113 19:47:52 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 201.70
[32m[20230113 19:47:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.22
[32m[20230113 19:47:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 258.20
[32m[20230113 19:47:53 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 258.20
[32m[20230113 19:47:53 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 258.20
[32m[20230113 19:47:53 @agent_ppo2.py:144][0m Total time:       3.34 min
[32m[20230113 19:47:53 @agent_ppo2.py:146][0m 290816 total steps have happened
[32m[20230113 19:47:53 @agent_ppo2.py:122][0m #------------------------ Iteration 142 --------------------------#
[32m[20230113 19:47:53 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:53 @agent_ppo2.py:186][0m |           0.0012 |           4.9225 |           2.7701 |
[32m[20230113 19:47:53 @agent_ppo2.py:186][0m |          -0.0015 |           4.0956 |           2.7684 |
[32m[20230113 19:47:53 @agent_ppo2.py:186][0m |          -0.0025 |           3.9157 |           2.7639 |
[32m[20230113 19:47:54 @agent_ppo2.py:186][0m |          -0.0040 |           3.7718 |           2.7644 |
[32m[20230113 19:47:54 @agent_ppo2.py:186][0m |          -0.0053 |           3.6561 |           2.7632 |
[32m[20230113 19:47:54 @agent_ppo2.py:186][0m |          -0.0049 |           3.5328 |           2.7611 |
[32m[20230113 19:47:54 @agent_ppo2.py:186][0m |          -0.0060 |           3.4396 |           2.7634 |
[32m[20230113 19:47:54 @agent_ppo2.py:186][0m |          -0.0069 |           3.3785 |           2.7604 |
[32m[20230113 19:47:54 @agent_ppo2.py:186][0m |          -0.0076 |           3.3307 |           2.7627 |
[32m[20230113 19:47:54 @agent_ppo2.py:186][0m |          -0.0085 |           3.2771 |           2.7604 |
[32m[20230113 19:47:54 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 206.16
[32m[20230113 19:47:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 210.02
[32m[20230113 19:47:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.83
[32m[20230113 19:47:54 @agent_ppo2.py:144][0m Total time:       3.36 min
[32m[20230113 19:47:54 @agent_ppo2.py:146][0m 292864 total steps have happened
[32m[20230113 19:47:54 @agent_ppo2.py:122][0m #------------------------ Iteration 143 --------------------------#
[32m[20230113 19:47:55 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:55 @agent_ppo2.py:186][0m |          -0.0001 |           3.7286 |           2.8748 |
[32m[20230113 19:47:55 @agent_ppo2.py:186][0m |          -0.0046 |           3.5308 |           2.8714 |
[32m[20230113 19:47:55 @agent_ppo2.py:186][0m |          -0.0077 |           3.4375 |           2.8701 |
[32m[20230113 19:47:55 @agent_ppo2.py:186][0m |          -0.0089 |           3.3747 |           2.8685 |
[32m[20230113 19:47:55 @agent_ppo2.py:186][0m |          -0.0095 |           3.3348 |           2.8701 |
[32m[20230113 19:47:55 @agent_ppo2.py:186][0m |          -0.0103 |           3.3008 |           2.8701 |
[32m[20230113 19:47:55 @agent_ppo2.py:186][0m |          -0.0111 |           3.2726 |           2.8683 |
[32m[20230113 19:47:55 @agent_ppo2.py:186][0m |          -0.0119 |           3.2424 |           2.8704 |
[32m[20230113 19:47:55 @agent_ppo2.py:186][0m |          -0.0122 |           3.2149 |           2.8683 |
[32m[20230113 19:47:55 @agent_ppo2.py:186][0m |          -0.0119 |           3.1981 |           2.8680 |
[32m[20230113 19:47:55 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:47:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 199.39
[32m[20230113 19:47:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 204.87
[32m[20230113 19:47:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 80.39
[32m[20230113 19:47:56 @agent_ppo2.py:144][0m Total time:       3.38 min
[32m[20230113 19:47:56 @agent_ppo2.py:146][0m 294912 total steps have happened
[32m[20230113 19:47:56 @agent_ppo2.py:122][0m #------------------------ Iteration 144 --------------------------#
[32m[20230113 19:47:56 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:47:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:56 @agent_ppo2.py:186][0m |           0.0004 |           2.9476 |           2.8684 |
[32m[20230113 19:47:56 @agent_ppo2.py:186][0m |          -0.0025 |           2.8184 |           2.8635 |
[32m[20230113 19:47:56 @agent_ppo2.py:186][0m |          -0.0043 |           2.7162 |           2.8584 |
[32m[20230113 19:47:56 @agent_ppo2.py:186][0m |          -0.0052 |           2.6557 |           2.8535 |
[32m[20230113 19:47:56 @agent_ppo2.py:186][0m |          -0.0060 |           2.6117 |           2.8500 |
[32m[20230113 19:47:56 @agent_ppo2.py:186][0m |          -0.0069 |           2.5764 |           2.8479 |
[32m[20230113 19:47:56 @agent_ppo2.py:186][0m |          -0.0075 |           2.5437 |           2.8464 |
[32m[20230113 19:47:57 @agent_ppo2.py:186][0m |          -0.0081 |           2.5175 |           2.8448 |
[32m[20230113 19:47:57 @agent_ppo2.py:186][0m |          -0.0084 |           2.4883 |           2.8425 |
[32m[20230113 19:47:57 @agent_ppo2.py:186][0m |          -0.0090 |           2.4621 |           2.8397 |
[32m[20230113 19:47:57 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:47:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 193.38
[32m[20230113 19:47:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 196.23
[32m[20230113 19:47:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 255.13
[32m[20230113 19:47:57 @agent_ppo2.py:144][0m Total time:       3.41 min
[32m[20230113 19:47:57 @agent_ppo2.py:146][0m 296960 total steps have happened
[32m[20230113 19:47:57 @agent_ppo2.py:122][0m #------------------------ Iteration 145 --------------------------#
[32m[20230113 19:47:58 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 19:47:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:58 @agent_ppo2.py:186][0m |          -0.0014 |          10.6653 |           2.7256 |
[32m[20230113 19:47:58 @agent_ppo2.py:186][0m |          -0.0056 |           6.6846 |           2.7205 |
[32m[20230113 19:47:58 @agent_ppo2.py:186][0m |          -0.0066 |           5.5214 |           2.7191 |
[32m[20230113 19:47:58 @agent_ppo2.py:186][0m |          -0.0074 |           5.0014 |           2.7202 |
[32m[20230113 19:47:58 @agent_ppo2.py:186][0m |          -0.0072 |           4.8309 |           2.7173 |
[32m[20230113 19:47:58 @agent_ppo2.py:186][0m |          -0.0087 |           4.5285 |           2.7188 |
[32m[20230113 19:47:58 @agent_ppo2.py:186][0m |          -0.0093 |           4.4337 |           2.7203 |
[32m[20230113 19:47:58 @agent_ppo2.py:186][0m |          -0.0100 |           4.5601 |           2.7172 |
[32m[20230113 19:47:58 @agent_ppo2.py:186][0m |          -0.0102 |           4.3218 |           2.7175 |
[32m[20230113 19:47:58 @agent_ppo2.py:186][0m |          -0.0111 |           4.2011 |           2.7163 |
[32m[20230113 19:47:58 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 19:47:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 131.54
[32m[20230113 19:47:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 196.73
[32m[20230113 19:47:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 258.49
[32m[20230113 19:47:58 @agent_ppo2.py:110][0m [4m[34mCRITICAL[0m Get the best episode reward: 258.49
[32m[20230113 19:47:58 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 258.49
[32m[20230113 19:47:58 @agent_ppo2.py:144][0m Total time:       3.43 min
[32m[20230113 19:47:58 @agent_ppo2.py:146][0m 299008 total steps have happened
[32m[20230113 19:47:58 @agent_ppo2.py:122][0m #------------------------ Iteration 146 --------------------------#
[32m[20230113 19:47:59 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 19:47:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:47:59 @agent_ppo2.py:186][0m |           0.0069 |          14.0496 |           2.7183 |
[32m[20230113 19:47:59 @agent_ppo2.py:186][0m |          -0.0029 |           6.9491 |           2.7139 |
[32m[20230113 19:47:59 @agent_ppo2.py:186][0m |          -0.0057 |           5.1211 |           2.7084 |
[32m[20230113 19:47:59 @agent_ppo2.py:186][0m |          -0.0082 |           4.4451 |           2.7068 |
[32m[20230113 19:47:59 @agent_ppo2.py:186][0m |          -0.0065 |           4.1628 |           2.7049 |
[32m[20230113 19:47:59 @agent_ppo2.py:186][0m |          -0.0083 |           4.0432 |           2.7049 |
[32m[20230113 19:47:59 @agent_ppo2.py:186][0m |          -0.0059 |           3.8110 |           2.7017 |
[32m[20230113 19:47:59 @agent_ppo2.py:186][0m |          -0.0082 |           3.9674 |           2.7003 |
[32m[20230113 19:47:59 @agent_ppo2.py:186][0m |          -0.0133 |           3.6222 |           2.6999 |
[32m[20230113 19:47:59 @agent_ppo2.py:186][0m |          -0.0119 |           3.5124 |           2.6979 |
[32m[20230113 19:47:59 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 19:48:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 133.85
[32m[20230113 19:48:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.64
[32m[20230113 19:48:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.40
[32m[20230113 19:48:00 @agent_ppo2.py:144][0m Total time:       3.45 min
[32m[20230113 19:48:00 @agent_ppo2.py:146][0m 301056 total steps have happened
[32m[20230113 19:48:00 @agent_ppo2.py:122][0m #------------------------ Iteration 147 --------------------------#
[32m[20230113 19:48:00 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:48:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:00 @agent_ppo2.py:186][0m |           0.0007 |           2.9092 |           2.7645 |
[32m[20230113 19:48:00 @agent_ppo2.py:186][0m |          -0.0016 |           2.6653 |           2.7667 |
[32m[20230113 19:48:00 @agent_ppo2.py:186][0m |          -0.0036 |           2.5628 |           2.7671 |
[32m[20230113 19:48:00 @agent_ppo2.py:186][0m |          -0.0044 |           2.4946 |           2.7679 |
[32m[20230113 19:48:00 @agent_ppo2.py:186][0m |          -0.0051 |           2.4584 |           2.7690 |
[32m[20230113 19:48:00 @agent_ppo2.py:186][0m |          -0.0057 |           2.4238 |           2.7663 |
[32m[20230113 19:48:01 @agent_ppo2.py:186][0m |          -0.0065 |           2.3983 |           2.7659 |
[32m[20230113 19:48:01 @agent_ppo2.py:186][0m |          -0.0069 |           2.3736 |           2.7659 |
[32m[20230113 19:48:01 @agent_ppo2.py:186][0m |          -0.0074 |           2.3476 |           2.7658 |
[32m[20230113 19:48:01 @agent_ppo2.py:186][0m |          -0.0081 |           2.3237 |           2.7650 |
[32m[20230113 19:48:01 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:48:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 187.96
[32m[20230113 19:48:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 196.67
[32m[20230113 19:48:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.17
[32m[20230113 19:48:01 @agent_ppo2.py:144][0m Total time:       3.48 min
[32m[20230113 19:48:01 @agent_ppo2.py:146][0m 303104 total steps have happened
[32m[20230113 19:48:01 @agent_ppo2.py:122][0m #------------------------ Iteration 148 --------------------------#
[32m[20230113 19:48:02 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:02 @agent_ppo2.py:186][0m |          -0.0007 |           9.7257 |           2.7580 |
[32m[20230113 19:48:02 @agent_ppo2.py:186][0m |          -0.0051 |           6.3077 |           2.7507 |
[32m[20230113 19:48:02 @agent_ppo2.py:186][0m |          -0.0069 |           5.3917 |           2.7508 |
[32m[20230113 19:48:02 @agent_ppo2.py:186][0m |          -0.0078 |           4.9760 |           2.7505 |
[32m[20230113 19:48:02 @agent_ppo2.py:186][0m |          -0.0088 |           4.6350 |           2.7506 |
[32m[20230113 19:48:02 @agent_ppo2.py:186][0m |          -0.0091 |           4.4785 |           2.7507 |
[32m[20230113 19:48:02 @agent_ppo2.py:186][0m |          -0.0102 |           4.3094 |           2.7512 |
[32m[20230113 19:48:02 @agent_ppo2.py:186][0m |          -0.0106 |           4.1616 |           2.7516 |
[32m[20230113 19:48:02 @agent_ppo2.py:186][0m |          -0.0108 |           4.0532 |           2.7531 |
[32m[20230113 19:48:02 @agent_ppo2.py:186][0m |          -0.0111 |           4.0042 |           2.7533 |
[32m[20230113 19:48:02 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:48:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 194.21
[32m[20230113 19:48:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 199.93
[32m[20230113 19:48:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 255.17
[32m[20230113 19:48:02 @agent_ppo2.py:144][0m Total time:       3.50 min
[32m[20230113 19:48:02 @agent_ppo2.py:146][0m 305152 total steps have happened
[32m[20230113 19:48:02 @agent_ppo2.py:122][0m #------------------------ Iteration 149 --------------------------#
[32m[20230113 19:48:03 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:03 @agent_ppo2.py:186][0m |           0.0001 |           4.2015 |           2.8349 |
[32m[20230113 19:48:03 @agent_ppo2.py:186][0m |          -0.0041 |           3.9419 |           2.8317 |
[32m[20230113 19:48:03 @agent_ppo2.py:186][0m |          -0.0057 |           3.8232 |           2.8301 |
[32m[20230113 19:48:03 @agent_ppo2.py:186][0m |          -0.0076 |           3.7496 |           2.8315 |
[32m[20230113 19:48:03 @agent_ppo2.py:186][0m |          -0.0087 |           3.7192 |           2.8310 |
[32m[20230113 19:48:03 @agent_ppo2.py:186][0m |          -0.0093 |           3.6594 |           2.8317 |
[32m[20230113 19:48:03 @agent_ppo2.py:186][0m |          -0.0105 |           3.6309 |           2.8329 |
[32m[20230113 19:48:03 @agent_ppo2.py:186][0m |          -0.0104 |           3.5955 |           2.8331 |
[32m[20230113 19:48:03 @agent_ppo2.py:186][0m |          -0.0108 |           3.5604 |           2.8349 |
[32m[20230113 19:48:03 @agent_ppo2.py:186][0m |          -0.0120 |           3.5431 |           2.8345 |
[32m[20230113 19:48:03 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 210.47
[32m[20230113 19:48:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.90
[32m[20230113 19:48:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.37
[32m[20230113 19:48:04 @agent_ppo2.py:144][0m Total time:       3.52 min
[32m[20230113 19:48:04 @agent_ppo2.py:146][0m 307200 total steps have happened
[32m[20230113 19:48:04 @agent_ppo2.py:122][0m #------------------------ Iteration 150 --------------------------#
[32m[20230113 19:48:04 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:48:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:05 @agent_ppo2.py:186][0m |          -0.0046 |          12.8195 |           2.7217 |
[32m[20230113 19:48:05 @agent_ppo2.py:186][0m |          -0.0053 |           8.3796 |           2.7186 |
[32m[20230113 19:48:05 @agent_ppo2.py:186][0m |          -0.0080 |           7.8987 |           2.7175 |
[32m[20230113 19:48:05 @agent_ppo2.py:186][0m |          -0.0082 |           7.6660 |           2.7162 |
[32m[20230113 19:48:05 @agent_ppo2.py:186][0m |          -0.0127 |           7.3528 |           2.7138 |
[32m[20230113 19:48:05 @agent_ppo2.py:186][0m |          -0.0079 |           7.6206 |           2.7126 |
[32m[20230113 19:48:05 @agent_ppo2.py:186][0m |          -0.0100 |           7.2285 |           2.7113 |
[32m[20230113 19:48:05 @agent_ppo2.py:186][0m |          -0.0118 |           7.1510 |           2.7113 |
[32m[20230113 19:48:05 @agent_ppo2.py:186][0m |          -0.0100 |           7.4216 |           2.7110 |
[32m[20230113 19:48:05 @agent_ppo2.py:186][0m |          -0.0146 |           7.0292 |           2.7094 |
[32m[20230113 19:48:05 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 19:48:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 101.71
[32m[20230113 19:48:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 215.37
[32m[20230113 19:48:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.86
[32m[20230113 19:48:05 @agent_ppo2.py:144][0m Total time:       3.55 min
[32m[20230113 19:48:05 @agent_ppo2.py:146][0m 309248 total steps have happened
[32m[20230113 19:48:05 @agent_ppo2.py:122][0m #------------------------ Iteration 151 --------------------------#
[32m[20230113 19:48:06 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 19:48:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:06 @agent_ppo2.py:186][0m |          -0.0032 |          19.7279 |           2.7705 |
[32m[20230113 19:48:06 @agent_ppo2.py:186][0m |          -0.0085 |           9.9416 |           2.7698 |
[32m[20230113 19:48:06 @agent_ppo2.py:186][0m |          -0.0093 |           8.2512 |           2.7724 |
[32m[20230113 19:48:06 @agent_ppo2.py:186][0m |          -0.0109 |           7.0062 |           2.7735 |
[32m[20230113 19:48:06 @agent_ppo2.py:186][0m |          -0.0111 |           6.6178 |           2.7719 |
[32m[20230113 19:48:06 @agent_ppo2.py:186][0m |          -0.0133 |           5.9024 |           2.7717 |
[32m[20230113 19:48:06 @agent_ppo2.py:186][0m |          -0.0139 |           5.5700 |           2.7720 |
[32m[20230113 19:48:06 @agent_ppo2.py:186][0m |          -0.0143 |           5.3954 |           2.7729 |
[32m[20230113 19:48:06 @agent_ppo2.py:186][0m |          -0.0151 |           5.0808 |           2.7723 |
[32m[20230113 19:48:06 @agent_ppo2.py:186][0m |          -0.0143 |           4.9406 |           2.7710 |
[32m[20230113 19:48:06 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 19:48:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 99.90
[32m[20230113 19:48:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 207.09
[32m[20230113 19:48:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 153.75
[32m[20230113 19:48:07 @agent_ppo2.py:144][0m Total time:       3.57 min
[32m[20230113 19:48:07 @agent_ppo2.py:146][0m 311296 total steps have happened
[32m[20230113 19:48:07 @agent_ppo2.py:122][0m #------------------------ Iteration 152 --------------------------#
[32m[20230113 19:48:07 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:07 @agent_ppo2.py:186][0m |           0.0001 |           3.4695 |           2.8332 |
[32m[20230113 19:48:07 @agent_ppo2.py:186][0m |          -0.0025 |           2.9573 |           2.8334 |
[32m[20230113 19:48:07 @agent_ppo2.py:186][0m |          -0.0044 |           2.8635 |           2.8319 |
[32m[20230113 19:48:07 @agent_ppo2.py:186][0m |          -0.0060 |           2.8049 |           2.8292 |
[32m[20230113 19:48:07 @agent_ppo2.py:186][0m |          -0.0071 |           2.7611 |           2.8309 |
[32m[20230113 19:48:07 @agent_ppo2.py:186][0m |          -0.0081 |           2.7235 |           2.8280 |
[32m[20230113 19:48:07 @agent_ppo2.py:186][0m |          -0.0088 |           2.6921 |           2.8277 |
[32m[20230113 19:48:07 @agent_ppo2.py:186][0m |          -0.0092 |           2.6714 |           2.8252 |
[32m[20230113 19:48:08 @agent_ppo2.py:186][0m |          -0.0102 |           2.6421 |           2.8261 |
[32m[20230113 19:48:08 @agent_ppo2.py:186][0m |          -0.0107 |           2.6175 |           2.8277 |
[32m[20230113 19:48:08 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.98
[32m[20230113 19:48:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.39
[32m[20230113 19:48:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 254.55
[32m[20230113 19:48:08 @agent_ppo2.py:144][0m Total time:       3.59 min
[32m[20230113 19:48:08 @agent_ppo2.py:146][0m 313344 total steps have happened
[32m[20230113 19:48:08 @agent_ppo2.py:122][0m #------------------------ Iteration 153 --------------------------#
[32m[20230113 19:48:08 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 19:48:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:08 @agent_ppo2.py:186][0m |           0.0002 |          24.4274 |           2.7708 |
[32m[20230113 19:48:08 @agent_ppo2.py:186][0m |           0.0004 |          10.8342 |           2.7670 |
[32m[20230113 19:48:08 @agent_ppo2.py:186][0m |          -0.0080 |           8.1718 |           2.7636 |
[32m[20230113 19:48:09 @agent_ppo2.py:186][0m |          -0.0135 |           6.7516 |           2.7630 |
[32m[20230113 19:48:09 @agent_ppo2.py:186][0m |          -0.0125 |           6.4264 |           2.7630 |
[32m[20230113 19:48:09 @agent_ppo2.py:186][0m |          -0.0032 |           5.8560 |           2.7625 |
[32m[20230113 19:48:09 @agent_ppo2.py:186][0m |          -0.0040 |           5.5052 |           2.7642 |
[32m[20230113 19:48:09 @agent_ppo2.py:186][0m |          -0.0037 |           5.1925 |           2.7599 |
[32m[20230113 19:48:09 @agent_ppo2.py:186][0m |          -0.0170 |           4.8260 |           2.7584 |
[32m[20230113 19:48:09 @agent_ppo2.py:186][0m |          -0.0089 |           4.5690 |           2.7590 |
[32m[20230113 19:48:09 @agent_ppo2.py:131][0m Policy update time: 0.34 s
[32m[20230113 19:48:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 13.50
[32m[20230113 19:48:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 190.21
[32m[20230113 19:48:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 103.01
[32m[20230113 19:48:09 @agent_ppo2.py:144][0m Total time:       3.61 min
[32m[20230113 19:48:09 @agent_ppo2.py:146][0m 315392 total steps have happened
[32m[20230113 19:48:09 @agent_ppo2.py:122][0m #------------------------ Iteration 154 --------------------------#
[32m[20230113 19:48:10 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:10 @agent_ppo2.py:186][0m |          -0.0011 |           5.0240 |           2.8213 |
[32m[20230113 19:48:10 @agent_ppo2.py:186][0m |          -0.0069 |           4.2870 |           2.8157 |
[32m[20230113 19:48:10 @agent_ppo2.py:186][0m |          -0.0091 |           4.1080 |           2.8160 |
[32m[20230113 19:48:10 @agent_ppo2.py:186][0m |          -0.0106 |           3.9730 |           2.8153 |
[32m[20230113 19:48:10 @agent_ppo2.py:186][0m |          -0.0117 |           3.8818 |           2.8146 |
[32m[20230113 19:48:10 @agent_ppo2.py:186][0m |          -0.0123 |           3.8117 |           2.8136 |
[32m[20230113 19:48:10 @agent_ppo2.py:186][0m |          -0.0126 |           3.7370 |           2.8126 |
[32m[20230113 19:48:10 @agent_ppo2.py:186][0m |          -0.0131 |           3.6918 |           2.8123 |
[32m[20230113 19:48:10 @agent_ppo2.py:186][0m |          -0.0137 |           3.6516 |           2.8118 |
[32m[20230113 19:48:10 @agent_ppo2.py:186][0m |          -0.0140 |           3.5765 |           2.8111 |
[32m[20230113 19:48:10 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 189.72
[32m[20230113 19:48:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 208.47
[32m[20230113 19:48:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.85
[32m[20230113 19:48:10 @agent_ppo2.py:144][0m Total time:       3.63 min
[32m[20230113 19:48:10 @agent_ppo2.py:146][0m 317440 total steps have happened
[32m[20230113 19:48:10 @agent_ppo2.py:122][0m #------------------------ Iteration 155 --------------------------#
[32m[20230113 19:48:11 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:48:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:11 @agent_ppo2.py:186][0m |          -0.0005 |           3.7849 |           2.8119 |
[32m[20230113 19:48:11 @agent_ppo2.py:186][0m |          -0.0053 |           3.2219 |           2.8126 |
[32m[20230113 19:48:11 @agent_ppo2.py:186][0m |          -0.0077 |           3.1438 |           2.8101 |
[32m[20230113 19:48:11 @agent_ppo2.py:186][0m |          -0.0086 |           3.0906 |           2.8101 |
[32m[20230113 19:48:11 @agent_ppo2.py:186][0m |          -0.0097 |           3.0510 |           2.8093 |
[32m[20230113 19:48:11 @agent_ppo2.py:186][0m |          -0.0107 |           3.0243 |           2.8098 |
[32m[20230113 19:48:11 @agent_ppo2.py:186][0m |          -0.0113 |           2.9817 |           2.8087 |
[32m[20230113 19:48:11 @agent_ppo2.py:186][0m |          -0.0118 |           2.9647 |           2.8098 |
[32m[20230113 19:48:11 @agent_ppo2.py:186][0m |          -0.0122 |           2.9422 |           2.8087 |
[32m[20230113 19:48:11 @agent_ppo2.py:186][0m |          -0.0122 |           2.9077 |           2.8088 |
[32m[20230113 19:48:11 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:48:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 200.76
[32m[20230113 19:48:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 203.64
[32m[20230113 19:48:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.06
[32m[20230113 19:48:12 @agent_ppo2.py:144][0m Total time:       3.66 min
[32m[20230113 19:48:12 @agent_ppo2.py:146][0m 319488 total steps have happened
[32m[20230113 19:48:12 @agent_ppo2.py:122][0m #------------------------ Iteration 156 --------------------------#
[32m[20230113 19:48:12 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:12 @agent_ppo2.py:186][0m |           0.0007 |           2.7353 |           2.7636 |
[32m[20230113 19:48:12 @agent_ppo2.py:186][0m |          -0.0013 |           2.5381 |           2.7612 |
[32m[20230113 19:48:13 @agent_ppo2.py:186][0m |          -0.0029 |           2.4681 |           2.7582 |
[32m[20230113 19:48:13 @agent_ppo2.py:186][0m |          -0.0047 |           2.4184 |           2.7573 |
[32m[20230113 19:48:13 @agent_ppo2.py:186][0m |          -0.0056 |           2.3783 |           2.7549 |
[32m[20230113 19:48:13 @agent_ppo2.py:186][0m |          -0.0062 |           2.3456 |           2.7541 |
[32m[20230113 19:48:13 @agent_ppo2.py:186][0m |          -0.0071 |           2.3237 |           2.7534 |
[32m[20230113 19:48:13 @agent_ppo2.py:186][0m |          -0.0079 |           2.2961 |           2.7532 |
[32m[20230113 19:48:13 @agent_ppo2.py:186][0m |          -0.0085 |           2.2843 |           2.7514 |
[32m[20230113 19:48:13 @agent_ppo2.py:186][0m |          -0.0091 |           2.2559 |           2.7499 |
[32m[20230113 19:48:13 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 195.79
[32m[20230113 19:48:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 201.86
[32m[20230113 19:48:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.33
[32m[20230113 19:48:13 @agent_ppo2.py:144][0m Total time:       3.68 min
[32m[20230113 19:48:13 @agent_ppo2.py:146][0m 321536 total steps have happened
[32m[20230113 19:48:13 @agent_ppo2.py:122][0m #------------------------ Iteration 157 --------------------------#
[32m[20230113 19:48:14 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:14 @agent_ppo2.py:186][0m |          -0.0014 |           2.9150 |           2.7757 |
[32m[20230113 19:48:14 @agent_ppo2.py:186][0m |          -0.0080 |           2.6851 |           2.7738 |
[32m[20230113 19:48:14 @agent_ppo2.py:186][0m |          -0.0100 |           2.5854 |           2.7712 |
[32m[20230113 19:48:14 @agent_ppo2.py:186][0m |          -0.0113 |           2.5108 |           2.7723 |
[32m[20230113 19:48:14 @agent_ppo2.py:186][0m |          -0.0124 |           2.4575 |           2.7715 |
[32m[20230113 19:48:14 @agent_ppo2.py:186][0m |          -0.0124 |           2.4174 |           2.7701 |
[32m[20230113 19:48:14 @agent_ppo2.py:186][0m |          -0.0134 |           2.3866 |           2.7706 |
[32m[20230113 19:48:14 @agent_ppo2.py:186][0m |          -0.0138 |           2.3556 |           2.7704 |
[32m[20230113 19:48:14 @agent_ppo2.py:186][0m |          -0.0144 |           2.3358 |           2.7700 |
[32m[20230113 19:48:14 @agent_ppo2.py:186][0m |          -0.0150 |           2.3109 |           2.7714 |
[32m[20230113 19:48:14 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:48:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 197.49
[32m[20230113 19:48:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 212.52
[32m[20230113 19:48:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.49
[32m[20230113 19:48:15 @agent_ppo2.py:144][0m Total time:       3.70 min
[32m[20230113 19:48:15 @agent_ppo2.py:146][0m 323584 total steps have happened
[32m[20230113 19:48:15 @agent_ppo2.py:122][0m #------------------------ Iteration 158 --------------------------#
[32m[20230113 19:48:15 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:15 @agent_ppo2.py:186][0m |           0.0002 |           3.2658 |           2.7888 |
[32m[20230113 19:48:15 @agent_ppo2.py:186][0m |          -0.0008 |           3.1659 |           2.7863 |
[32m[20230113 19:48:15 @agent_ppo2.py:186][0m |          -0.0032 |           3.1130 |           2.7871 |
[32m[20230113 19:48:15 @agent_ppo2.py:186][0m |          -0.0042 |           3.0788 |           2.7871 |
[32m[20230113 19:48:15 @agent_ppo2.py:186][0m |          -0.0054 |           3.0609 |           2.7890 |
[32m[20230113 19:48:16 @agent_ppo2.py:186][0m |          -0.0059 |           3.0303 |           2.7882 |
[32m[20230113 19:48:16 @agent_ppo2.py:186][0m |          -0.0069 |           3.0094 |           2.7891 |
[32m[20230113 19:48:16 @agent_ppo2.py:186][0m |          -0.0071 |           2.9897 |           2.7904 |
[32m[20230113 19:48:16 @agent_ppo2.py:186][0m |          -0.0077 |           2.9701 |           2.7920 |
[32m[20230113 19:48:16 @agent_ppo2.py:186][0m |          -0.0080 |           2.9549 |           2.7913 |
[32m[20230113 19:48:16 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 213.14
[32m[20230113 19:48:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.74
[32m[20230113 19:48:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.27
[32m[20230113 19:48:16 @agent_ppo2.py:144][0m Total time:       3.73 min
[32m[20230113 19:48:16 @agent_ppo2.py:146][0m 325632 total steps have happened
[32m[20230113 19:48:16 @agent_ppo2.py:122][0m #------------------------ Iteration 159 --------------------------#
[32m[20230113 19:48:17 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:48:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:17 @agent_ppo2.py:186][0m |          -0.0007 |           8.9905 |           2.8115 |
[32m[20230113 19:48:17 @agent_ppo2.py:186][0m |          -0.0049 |           6.4787 |           2.8126 |
[32m[20230113 19:48:17 @agent_ppo2.py:186][0m |          -0.0052 |           5.9453 |           2.8132 |
[32m[20230113 19:48:17 @agent_ppo2.py:186][0m |          -0.0085 |           5.6800 |           2.8146 |
[32m[20230113 19:48:17 @agent_ppo2.py:186][0m |          -0.0035 |           5.4389 |           2.8152 |
[32m[20230113 19:48:17 @agent_ppo2.py:186][0m |          -0.0086 |           5.5486 |           2.8159 |
[32m[20230113 19:48:17 @agent_ppo2.py:186][0m |          -0.0055 |           5.3273 |           2.8165 |
[32m[20230113 19:48:17 @agent_ppo2.py:186][0m |          -0.0035 |           5.3748 |           2.8152 |
[32m[20230113 19:48:17 @agent_ppo2.py:186][0m |          -0.0127 |           5.0878 |           2.8169 |
[32m[20230113 19:48:17 @agent_ppo2.py:186][0m |          -0.0054 |           5.1282 |           2.8170 |
[32m[20230113 19:48:17 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 19:48:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 96.68
[32m[20230113 19:48:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 206.37
[32m[20230113 19:48:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.42
[32m[20230113 19:48:18 @agent_ppo2.py:144][0m Total time:       3.75 min
[32m[20230113 19:48:18 @agent_ppo2.py:146][0m 327680 total steps have happened
[32m[20230113 19:48:18 @agent_ppo2.py:122][0m #------------------------ Iteration 160 --------------------------#
[32m[20230113 19:48:18 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:18 @agent_ppo2.py:186][0m |           0.0000 |           3.8400 |           2.8620 |
[32m[20230113 19:48:18 @agent_ppo2.py:186][0m |          -0.0060 |           3.4803 |           2.8606 |
[32m[20230113 19:48:18 @agent_ppo2.py:186][0m |          -0.0076 |           3.4093 |           2.8605 |
[32m[20230113 19:48:18 @agent_ppo2.py:186][0m |          -0.0084 |           3.3650 |           2.8609 |
[32m[20230113 19:48:18 @agent_ppo2.py:186][0m |          -0.0089 |           3.3199 |           2.8623 |
[32m[20230113 19:48:18 @agent_ppo2.py:186][0m |          -0.0090 |           3.2908 |           2.8621 |
[32m[20230113 19:48:18 @agent_ppo2.py:186][0m |          -0.0101 |           3.2512 |           2.8620 |
[32m[20230113 19:48:18 @agent_ppo2.py:186][0m |          -0.0102 |           3.2156 |           2.8622 |
[32m[20230113 19:48:19 @agent_ppo2.py:186][0m |          -0.0107 |           3.1833 |           2.8627 |
[32m[20230113 19:48:19 @agent_ppo2.py:186][0m |          -0.0112 |           3.1504 |           2.8635 |
[32m[20230113 19:48:19 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 206.38
[32m[20230113 19:48:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 207.58
[32m[20230113 19:48:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.40
[32m[20230113 19:48:19 @agent_ppo2.py:144][0m Total time:       3.77 min
[32m[20230113 19:48:19 @agent_ppo2.py:146][0m 329728 total steps have happened
[32m[20230113 19:48:19 @agent_ppo2.py:122][0m #------------------------ Iteration 161 --------------------------#
[32m[20230113 19:48:19 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 19:48:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:19 @agent_ppo2.py:186][0m |           0.0005 |          36.8396 |           2.7748 |
[32m[20230113 19:48:19 @agent_ppo2.py:186][0m |          -0.0043 |          20.1432 |           2.7691 |
[32m[20230113 19:48:19 @agent_ppo2.py:186][0m |          -0.0059 |          16.6441 |           2.7653 |
[32m[20230113 19:48:20 @agent_ppo2.py:186][0m |          -0.0098 |          15.2218 |           2.7616 |
[32m[20230113 19:48:20 @agent_ppo2.py:186][0m |          -0.0094 |          13.3025 |           2.7608 |
[32m[20230113 19:48:20 @agent_ppo2.py:186][0m |          -0.0119 |          13.4108 |           2.7581 |
[32m[20230113 19:48:20 @agent_ppo2.py:186][0m |          -0.0109 |          12.4682 |           2.7575 |
[32m[20230113 19:48:20 @agent_ppo2.py:186][0m |          -0.0111 |          10.7349 |           2.7546 |
[32m[20230113 19:48:20 @agent_ppo2.py:186][0m |          -0.0113 |          10.1169 |           2.7538 |
[32m[20230113 19:48:20 @agent_ppo2.py:186][0m |          -0.0143 |           9.6652 |           2.7533 |
[32m[20230113 19:48:20 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 19:48:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 36.94
[32m[20230113 19:48:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 120.88
[32m[20230113 19:48:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.46
[32m[20230113 19:48:20 @agent_ppo2.py:144][0m Total time:       3.79 min
[32m[20230113 19:48:20 @agent_ppo2.py:146][0m 331776 total steps have happened
[32m[20230113 19:48:20 @agent_ppo2.py:122][0m #------------------------ Iteration 162 --------------------------#
[32m[20230113 19:48:21 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:21 @agent_ppo2.py:186][0m |          -0.0013 |           4.0518 |           2.8736 |
[32m[20230113 19:48:21 @agent_ppo2.py:186][0m |          -0.0056 |           3.6023 |           2.8707 |
[32m[20230113 19:48:21 @agent_ppo2.py:186][0m |          -0.0081 |           3.4897 |           2.8670 |
[32m[20230113 19:48:21 @agent_ppo2.py:186][0m |          -0.0092 |           3.4283 |           2.8658 |
[32m[20230113 19:48:21 @agent_ppo2.py:186][0m |          -0.0100 |           3.3903 |           2.8647 |
[32m[20230113 19:48:21 @agent_ppo2.py:186][0m |          -0.0112 |           3.3435 |           2.8634 |
[32m[20230113 19:48:21 @agent_ppo2.py:186][0m |          -0.0116 |           3.3031 |           2.8629 |
[32m[20230113 19:48:21 @agent_ppo2.py:186][0m |          -0.0126 |           3.2746 |           2.8609 |
[32m[20230113 19:48:21 @agent_ppo2.py:186][0m |          -0.0128 |           3.2498 |           2.8606 |
[32m[20230113 19:48:21 @agent_ppo2.py:186][0m |          -0.0131 |           3.2131 |           2.8590 |
[32m[20230113 19:48:21 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 193.14
[32m[20230113 19:48:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 198.59
[32m[20230113 19:48:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.11
[32m[20230113 19:48:22 @agent_ppo2.py:144][0m Total time:       3.82 min
[32m[20230113 19:48:22 @agent_ppo2.py:146][0m 333824 total steps have happened
[32m[20230113 19:48:22 @agent_ppo2.py:122][0m #------------------------ Iteration 163 --------------------------#
[32m[20230113 19:48:22 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:22 @agent_ppo2.py:186][0m |          -0.0005 |           6.3496 |           2.8083 |
[32m[20230113 19:48:22 @agent_ppo2.py:186][0m |          -0.0045 |           4.8894 |           2.8086 |
[32m[20230113 19:48:22 @agent_ppo2.py:186][0m |          -0.0070 |           4.6818 |           2.8094 |
[32m[20230113 19:48:22 @agent_ppo2.py:186][0m |          -0.0080 |           4.4780 |           2.8092 |
[32m[20230113 19:48:22 @agent_ppo2.py:186][0m |          -0.0088 |           4.3862 |           2.8107 |
[32m[20230113 19:48:22 @agent_ppo2.py:186][0m |          -0.0097 |           4.2998 |           2.8088 |
[32m[20230113 19:48:22 @agent_ppo2.py:186][0m |          -0.0107 |           4.2355 |           2.8113 |
[32m[20230113 19:48:22 @agent_ppo2.py:186][0m |          -0.0106 |           4.1802 |           2.8097 |
[32m[20230113 19:48:23 @agent_ppo2.py:186][0m |          -0.0113 |           4.1353 |           2.8109 |
[32m[20230113 19:48:23 @agent_ppo2.py:186][0m |          -0.0121 |           4.0969 |           2.8124 |
[32m[20230113 19:48:23 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 201.27
[32m[20230113 19:48:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 210.77
[32m[20230113 19:48:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 229.27
[32m[20230113 19:48:23 @agent_ppo2.py:144][0m Total time:       3.84 min
[32m[20230113 19:48:23 @agent_ppo2.py:146][0m 335872 total steps have happened
[32m[20230113 19:48:23 @agent_ppo2.py:122][0m #------------------------ Iteration 164 --------------------------#
[32m[20230113 19:48:23 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:24 @agent_ppo2.py:186][0m |          -0.0003 |           2.8919 |           2.8408 |
[32m[20230113 19:48:24 @agent_ppo2.py:186][0m |          -0.0028 |           2.4569 |           2.8413 |
[32m[20230113 19:48:24 @agent_ppo2.py:186][0m |          -0.0041 |           2.3638 |           2.8406 |
[32m[20230113 19:48:24 @agent_ppo2.py:186][0m |          -0.0048 |           2.3202 |           2.8422 |
[32m[20230113 19:48:24 @agent_ppo2.py:186][0m |          -0.0058 |           2.2868 |           2.8436 |
[32m[20230113 19:48:24 @agent_ppo2.py:186][0m |          -0.0066 |           2.2656 |           2.8450 |
[32m[20230113 19:48:24 @agent_ppo2.py:186][0m |          -0.0072 |           2.2458 |           2.8458 |
[32m[20230113 19:48:24 @agent_ppo2.py:186][0m |          -0.0077 |           2.2279 |           2.8474 |
[32m[20230113 19:48:24 @agent_ppo2.py:186][0m |          -0.0082 |           2.2169 |           2.8480 |
[32m[20230113 19:48:24 @agent_ppo2.py:186][0m |          -0.0088 |           2.2044 |           2.8488 |
[32m[20230113 19:48:24 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 190.40
[32m[20230113 19:48:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 193.63
[32m[20230113 19:48:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 113.36
[32m[20230113 19:48:24 @agent_ppo2.py:144][0m Total time:       3.86 min
[32m[20230113 19:48:24 @agent_ppo2.py:146][0m 337920 total steps have happened
[32m[20230113 19:48:24 @agent_ppo2.py:122][0m #------------------------ Iteration 165 --------------------------#
[32m[20230113 19:48:25 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 19:48:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:25 @agent_ppo2.py:186][0m |          -0.0031 |          22.6675 |           2.8615 |
[32m[20230113 19:48:25 @agent_ppo2.py:186][0m |          -0.0081 |           5.5152 |           2.8598 |
[32m[20230113 19:48:25 @agent_ppo2.py:186][0m |          -0.0090 |           4.1465 |           2.8588 |
[32m[20230113 19:48:25 @agent_ppo2.py:186][0m |          -0.0002 |           3.8721 |           2.8576 |
[32m[20230113 19:48:25 @agent_ppo2.py:186][0m |          -0.0105 |           3.3630 |           2.8557 |
[32m[20230113 19:48:25 @agent_ppo2.py:186][0m |          -0.0090 |           3.1531 |           2.8547 |
[32m[20230113 19:48:25 @agent_ppo2.py:186][0m |          -0.0107 |           3.1053 |           2.8535 |
[32m[20230113 19:48:25 @agent_ppo2.py:186][0m |           0.0010 |           2.9448 |           2.8526 |
[32m[20230113 19:48:25 @agent_ppo2.py:186][0m |          -0.0127 |           2.7811 |           2.8522 |
[32m[20230113 19:48:25 @agent_ppo2.py:186][0m |          -0.0134 |           2.7663 |           2.8513 |
[32m[20230113 19:48:25 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 19:48:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 102.66
[32m[20230113 19:48:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 191.03
[32m[20230113 19:48:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 220.32
[32m[20230113 19:48:26 @agent_ppo2.py:144][0m Total time:       3.88 min
[32m[20230113 19:48:26 @agent_ppo2.py:146][0m 339968 total steps have happened
[32m[20230113 19:48:26 @agent_ppo2.py:122][0m #------------------------ Iteration 166 --------------------------#
[32m[20230113 19:48:26 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:26 @agent_ppo2.py:186][0m |           0.0003 |           3.1426 |           2.8161 |
[32m[20230113 19:48:26 @agent_ppo2.py:186][0m |          -0.0030 |           2.8603 |           2.8133 |
[32m[20230113 19:48:26 @agent_ppo2.py:186][0m |          -0.0053 |           2.7956 |           2.8082 |
[32m[20230113 19:48:26 @agent_ppo2.py:186][0m |          -0.0071 |           2.7405 |           2.8076 |
[32m[20230113 19:48:26 @agent_ppo2.py:186][0m |          -0.0082 |           2.7045 |           2.8066 |
[32m[20230113 19:48:26 @agent_ppo2.py:186][0m |          -0.0090 |           2.6795 |           2.8061 |
[32m[20230113 19:48:26 @agent_ppo2.py:186][0m |          -0.0100 |           2.6548 |           2.8071 |
[32m[20230113 19:48:26 @agent_ppo2.py:186][0m |          -0.0106 |           2.6348 |           2.8050 |
[32m[20230113 19:48:27 @agent_ppo2.py:186][0m |          -0.0111 |           2.6111 |           2.8057 |
[32m[20230113 19:48:27 @agent_ppo2.py:186][0m |          -0.0115 |           2.5914 |           2.8051 |
[32m[20230113 19:48:27 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 213.83
[32m[20230113 19:48:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.26
[32m[20230113 19:48:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.74
[32m[20230113 19:48:27 @agent_ppo2.py:144][0m Total time:       3.91 min
[32m[20230113 19:48:27 @agent_ppo2.py:146][0m 342016 total steps have happened
[32m[20230113 19:48:27 @agent_ppo2.py:122][0m #------------------------ Iteration 167 --------------------------#
[32m[20230113 19:48:28 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:28 @agent_ppo2.py:186][0m |           0.0008 |           2.3546 |           2.9385 |
[32m[20230113 19:48:28 @agent_ppo2.py:186][0m |          -0.0018 |           2.2379 |           2.9384 |
[32m[20230113 19:48:28 @agent_ppo2.py:186][0m |          -0.0039 |           2.1967 |           2.9356 |
[32m[20230113 19:48:28 @agent_ppo2.py:186][0m |          -0.0051 |           2.1712 |           2.9344 |
[32m[20230113 19:48:28 @agent_ppo2.py:186][0m |          -0.0059 |           2.1483 |           2.9351 |
[32m[20230113 19:48:28 @agent_ppo2.py:186][0m |          -0.0067 |           2.1342 |           2.9348 |
[32m[20230113 19:48:28 @agent_ppo2.py:186][0m |          -0.0074 |           2.1226 |           2.9337 |
[32m[20230113 19:48:28 @agent_ppo2.py:186][0m |          -0.0076 |           2.1169 |           2.9339 |
[32m[20230113 19:48:28 @agent_ppo2.py:186][0m |          -0.0083 |           2.1020 |           2.9344 |
[32m[20230113 19:48:28 @agent_ppo2.py:186][0m |          -0.0087 |           2.0958 |           2.9344 |
[32m[20230113 19:48:28 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 210.37
[32m[20230113 19:48:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 211.83
[32m[20230113 19:48:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.64
[32m[20230113 19:48:28 @agent_ppo2.py:144][0m Total time:       3.93 min
[32m[20230113 19:48:28 @agent_ppo2.py:146][0m 344064 total steps have happened
[32m[20230113 19:48:28 @agent_ppo2.py:122][0m #------------------------ Iteration 168 --------------------------#
[32m[20230113 19:48:29 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:29 @agent_ppo2.py:186][0m |           0.0002 |           2.9451 |           2.9286 |
[32m[20230113 19:48:29 @agent_ppo2.py:186][0m |          -0.0034 |           2.7453 |           2.9258 |
[32m[20230113 19:48:29 @agent_ppo2.py:186][0m |          -0.0042 |           2.6730 |           2.9208 |
[32m[20230113 19:48:29 @agent_ppo2.py:186][0m |          -0.0055 |           2.6518 |           2.9206 |
[32m[20230113 19:48:29 @agent_ppo2.py:186][0m |          -0.0065 |           2.6131 |           2.9168 |
[32m[20230113 19:48:29 @agent_ppo2.py:186][0m |          -0.0074 |           2.5853 |           2.9169 |
[32m[20230113 19:48:29 @agent_ppo2.py:186][0m |          -0.0078 |           2.5649 |           2.9137 |
[32m[20230113 19:48:29 @agent_ppo2.py:186][0m |          -0.0079 |           2.5499 |           2.9143 |
[32m[20230113 19:48:29 @agent_ppo2.py:186][0m |          -0.0089 |           2.5284 |           2.9130 |
[32m[20230113 19:48:29 @agent_ppo2.py:186][0m |          -0.0090 |           2.5126 |           2.9108 |
[32m[20230113 19:48:29 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 208.74
[32m[20230113 19:48:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.35
[32m[20230113 19:48:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 228.23
[32m[20230113 19:48:30 @agent_ppo2.py:144][0m Total time:       3.96 min
[32m[20230113 19:48:30 @agent_ppo2.py:146][0m 346112 total steps have happened
[32m[20230113 19:48:30 @agent_ppo2.py:122][0m #------------------------ Iteration 169 --------------------------#
[32m[20230113 19:48:30 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:30 @agent_ppo2.py:186][0m |           0.0017 |           3.4388 |           2.8872 |
[32m[20230113 19:48:30 @agent_ppo2.py:186][0m |          -0.0027 |           3.1935 |           2.8886 |
[32m[20230113 19:48:31 @agent_ppo2.py:186][0m |          -0.0041 |           3.0703 |           2.8886 |
[32m[20230113 19:48:31 @agent_ppo2.py:186][0m |          -0.0052 |           2.9943 |           2.8872 |
[32m[20230113 19:48:31 @agent_ppo2.py:186][0m |          -0.0066 |           2.9412 |           2.8880 |
[32m[20230113 19:48:31 @agent_ppo2.py:186][0m |          -0.0067 |           2.8916 |           2.8885 |
[32m[20230113 19:48:31 @agent_ppo2.py:186][0m |          -0.0081 |           2.8488 |           2.8881 |
[32m[20230113 19:48:31 @agent_ppo2.py:186][0m |          -0.0086 |           2.8117 |           2.8872 |
[32m[20230113 19:48:31 @agent_ppo2.py:186][0m |          -0.0091 |           2.7716 |           2.8888 |
[32m[20230113 19:48:31 @agent_ppo2.py:186][0m |          -0.0090 |           2.7344 |           2.8883 |
[32m[20230113 19:48:31 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 202.28
[32m[20230113 19:48:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 202.94
[32m[20230113 19:48:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.16
[32m[20230113 19:48:31 @agent_ppo2.py:144][0m Total time:       3.98 min
[32m[20230113 19:48:31 @agent_ppo2.py:146][0m 348160 total steps have happened
[32m[20230113 19:48:31 @agent_ppo2.py:122][0m #------------------------ Iteration 170 --------------------------#
[32m[20230113 19:48:32 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:48:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:32 @agent_ppo2.py:186][0m |           0.0003 |           3.0967 |           2.8457 |
[32m[20230113 19:48:32 @agent_ppo2.py:186][0m |          -0.0025 |           2.7748 |           2.8421 |
[32m[20230113 19:48:32 @agent_ppo2.py:186][0m |          -0.0051 |           2.6432 |           2.8390 |
[32m[20230113 19:48:32 @agent_ppo2.py:186][0m |          -0.0060 |           2.5721 |           2.8388 |
[32m[20230113 19:48:32 @agent_ppo2.py:186][0m |          -0.0066 |           2.5207 |           2.8379 |
[32m[20230113 19:48:32 @agent_ppo2.py:186][0m |          -0.0074 |           2.4649 |           2.8394 |
[32m[20230113 19:48:32 @agent_ppo2.py:186][0m |          -0.0080 |           2.4291 |           2.8384 |
[32m[20230113 19:48:32 @agent_ppo2.py:186][0m |          -0.0084 |           2.3929 |           2.8388 |
[32m[20230113 19:48:32 @agent_ppo2.py:186][0m |          -0.0088 |           2.3661 |           2.8390 |
[32m[20230113 19:48:32 @agent_ppo2.py:186][0m |          -0.0092 |           2.3334 |           2.8397 |
[32m[20230113 19:48:32 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 204.24
[32m[20230113 19:48:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 215.05
[32m[20230113 19:48:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.35
[32m[20230113 19:48:33 @agent_ppo2.py:144][0m Total time:       4.00 min
[32m[20230113 19:48:33 @agent_ppo2.py:146][0m 350208 total steps have happened
[32m[20230113 19:48:33 @agent_ppo2.py:122][0m #------------------------ Iteration 171 --------------------------#
[32m[20230113 19:48:33 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:33 @agent_ppo2.py:186][0m |          -0.0000 |           3.3020 |           2.9374 |
[32m[20230113 19:48:33 @agent_ppo2.py:186][0m |          -0.0027 |           3.0384 |           2.9348 |
[32m[20230113 19:48:33 @agent_ppo2.py:186][0m |          -0.0053 |           2.9724 |           2.9329 |
[32m[20230113 19:48:33 @agent_ppo2.py:186][0m |          -0.0069 |           2.9276 |           2.9304 |
[32m[20230113 19:48:33 @agent_ppo2.py:186][0m |          -0.0074 |           2.8937 |           2.9288 |
[32m[20230113 19:48:34 @agent_ppo2.py:186][0m |          -0.0079 |           2.8757 |           2.9268 |
[32m[20230113 19:48:34 @agent_ppo2.py:186][0m |          -0.0086 |           2.8712 |           2.9291 |
[32m[20230113 19:48:34 @agent_ppo2.py:186][0m |          -0.0093 |           2.8365 |           2.9261 |
[32m[20230113 19:48:34 @agent_ppo2.py:186][0m |          -0.0093 |           2.8172 |           2.9279 |
[32m[20230113 19:48:34 @agent_ppo2.py:186][0m |          -0.0100 |           2.8018 |           2.9263 |
[32m[20230113 19:48:34 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 214.62
[32m[20230113 19:48:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.36
[32m[20230113 19:48:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 221.62
[32m[20230113 19:48:34 @agent_ppo2.py:144][0m Total time:       4.03 min
[32m[20230113 19:48:34 @agent_ppo2.py:146][0m 352256 total steps have happened
[32m[20230113 19:48:34 @agent_ppo2.py:122][0m #------------------------ Iteration 172 --------------------------#
[32m[20230113 19:48:35 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:35 @agent_ppo2.py:186][0m |           0.0004 |           3.4348 |           2.8318 |
[32m[20230113 19:48:35 @agent_ppo2.py:186][0m |          -0.0037 |           3.2373 |           2.8322 |
[32m[20230113 19:48:35 @agent_ppo2.py:186][0m |          -0.0054 |           3.1334 |           2.8317 |
[32m[20230113 19:48:35 @agent_ppo2.py:186][0m |          -0.0067 |           3.0410 |           2.8320 |
[32m[20230113 19:48:35 @agent_ppo2.py:186][0m |          -0.0073 |           2.8970 |           2.8305 |
[32m[20230113 19:48:35 @agent_ppo2.py:186][0m |          -0.0080 |           2.7084 |           2.8294 |
[32m[20230113 19:48:35 @agent_ppo2.py:186][0m |          -0.0088 |           2.4915 |           2.8294 |
[32m[20230113 19:48:35 @agent_ppo2.py:186][0m |          -0.0092 |           2.2892 |           2.8291 |
[32m[20230113 19:48:35 @agent_ppo2.py:186][0m |          -0.0095 |           2.2071 |           2.8286 |
[32m[20230113 19:48:35 @agent_ppo2.py:186][0m |          -0.0100 |           2.0963 |           2.8280 |
[32m[20230113 19:48:35 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 199.73
[32m[20230113 19:48:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 204.00
[32m[20230113 19:48:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.40
[32m[20230113 19:48:36 @agent_ppo2.py:144][0m Total time:       4.05 min
[32m[20230113 19:48:36 @agent_ppo2.py:146][0m 354304 total steps have happened
[32m[20230113 19:48:36 @agent_ppo2.py:122][0m #------------------------ Iteration 173 --------------------------#
[32m[20230113 19:48:36 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:36 @agent_ppo2.py:186][0m |           0.0006 |           3.2796 |           2.9850 |
[32m[20230113 19:48:36 @agent_ppo2.py:186][0m |          -0.0033 |           2.9667 |           2.9830 |
[32m[20230113 19:48:36 @agent_ppo2.py:186][0m |          -0.0047 |           2.8682 |           2.9818 |
[32m[20230113 19:48:36 @agent_ppo2.py:186][0m |          -0.0060 |           2.8207 |           2.9794 |
[32m[20230113 19:48:36 @agent_ppo2.py:186][0m |          -0.0075 |           2.7565 |           2.9824 |
[32m[20230113 19:48:36 @agent_ppo2.py:186][0m |          -0.0075 |           2.7076 |           2.9802 |
[32m[20230113 19:48:36 @agent_ppo2.py:186][0m |          -0.0079 |           2.6749 |           2.9819 |
[32m[20230113 19:48:36 @agent_ppo2.py:186][0m |          -0.0088 |           2.6311 |           2.9836 |
[32m[20230113 19:48:37 @agent_ppo2.py:186][0m |          -0.0097 |           2.6081 |           2.9818 |
[32m[20230113 19:48:37 @agent_ppo2.py:186][0m |          -0.0096 |           2.5410 |           2.9842 |
[32m[20230113 19:48:37 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 213.57
[32m[20230113 19:48:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.15
[32m[20230113 19:48:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.24
[32m[20230113 19:48:37 @agent_ppo2.py:144][0m Total time:       4.07 min
[32m[20230113 19:48:37 @agent_ppo2.py:146][0m 356352 total steps have happened
[32m[20230113 19:48:37 @agent_ppo2.py:122][0m #------------------------ Iteration 174 --------------------------#
[32m[20230113 19:48:38 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:38 @agent_ppo2.py:186][0m |          -0.0001 |           2.9146 |           3.0006 |
[32m[20230113 19:48:38 @agent_ppo2.py:186][0m |          -0.0043 |           2.7204 |           2.9934 |
[32m[20230113 19:48:38 @agent_ppo2.py:186][0m |          -0.0065 |           2.6492 |           2.9933 |
[32m[20230113 19:48:38 @agent_ppo2.py:186][0m |          -0.0076 |           2.5834 |           2.9946 |
[32m[20230113 19:48:38 @agent_ppo2.py:186][0m |          -0.0086 |           2.5325 |           2.9952 |
[32m[20230113 19:48:38 @agent_ppo2.py:186][0m |          -0.0093 |           2.4975 |           2.9977 |
[32m[20230113 19:48:38 @agent_ppo2.py:186][0m |          -0.0100 |           2.4645 |           2.9988 |
[32m[20230113 19:48:38 @agent_ppo2.py:186][0m |          -0.0108 |           2.4293 |           2.9985 |
[32m[20230113 19:48:38 @agent_ppo2.py:186][0m |          -0.0112 |           2.4037 |           3.0007 |
[32m[20230113 19:48:38 @agent_ppo2.py:186][0m |          -0.0116 |           2.3818 |           3.0015 |
[32m[20230113 19:48:38 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 204.75
[32m[20230113 19:48:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 208.14
[32m[20230113 19:48:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 227.28
[32m[20230113 19:48:38 @agent_ppo2.py:144][0m Total time:       4.10 min
[32m[20230113 19:48:38 @agent_ppo2.py:146][0m 358400 total steps have happened
[32m[20230113 19:48:38 @agent_ppo2.py:122][0m #------------------------ Iteration 175 --------------------------#
[32m[20230113 19:48:39 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:39 @agent_ppo2.py:186][0m |           0.0002 |           2.9768 |           2.9202 |
[32m[20230113 19:48:39 @agent_ppo2.py:186][0m |          -0.0028 |           2.8966 |           2.9175 |
[32m[20230113 19:48:39 @agent_ppo2.py:186][0m |          -0.0049 |           2.8496 |           2.9155 |
[32m[20230113 19:48:39 @agent_ppo2.py:186][0m |          -0.0066 |           2.8168 |           2.9140 |
[32m[20230113 19:48:39 @agent_ppo2.py:186][0m |          -0.0070 |           2.7961 |           2.9144 |
[32m[20230113 19:48:39 @agent_ppo2.py:186][0m |          -0.0077 |           2.7883 |           2.9138 |
[32m[20230113 19:48:39 @agent_ppo2.py:186][0m |          -0.0082 |           2.7669 |           2.9137 |
[32m[20230113 19:48:39 @agent_ppo2.py:186][0m |          -0.0087 |           2.7525 |           2.9149 |
[32m[20230113 19:48:39 @agent_ppo2.py:186][0m |          -0.0092 |           2.7485 |           2.9147 |
[32m[20230113 19:48:39 @agent_ppo2.py:186][0m |          -0.0092 |           2.7415 |           2.9154 |
[32m[20230113 19:48:39 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:48:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 217.40
[32m[20230113 19:48:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.77
[32m[20230113 19:48:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.03
[32m[20230113 19:48:40 @agent_ppo2.py:144][0m Total time:       4.12 min
[32m[20230113 19:48:40 @agent_ppo2.py:146][0m 360448 total steps have happened
[32m[20230113 19:48:40 @agent_ppo2.py:122][0m #------------------------ Iteration 176 --------------------------#
[32m[20230113 19:48:40 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 19:48:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:40 @agent_ppo2.py:186][0m |          -0.0011 |          26.4714 |           2.9243 |
[32m[20230113 19:48:40 @agent_ppo2.py:186][0m |          -0.0069 |           9.0779 |           2.9236 |
[32m[20230113 19:48:40 @agent_ppo2.py:186][0m |          -0.0080 |           6.6997 |           2.9214 |
[32m[20230113 19:48:41 @agent_ppo2.py:186][0m |          -0.0103 |           5.9083 |           2.9193 |
[32m[20230113 19:48:41 @agent_ppo2.py:186][0m |          -0.0089 |           5.0533 |           2.9192 |
[32m[20230113 19:48:41 @agent_ppo2.py:186][0m |          -0.0069 |           5.0160 |           2.9171 |
[32m[20230113 19:48:41 @agent_ppo2.py:186][0m |          -0.0121 |           4.3189 |           2.9151 |
[32m[20230113 19:48:41 @agent_ppo2.py:186][0m |           0.0086 |           4.2011 |           2.9157 |
[32m[20230113 19:48:41 @agent_ppo2.py:186][0m |           0.0121 |           3.9985 |           2.9128 |
[32m[20230113 19:48:41 @agent_ppo2.py:186][0m |          -0.0110 |           3.8384 |           2.9071 |
[32m[20230113 19:48:41 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 19:48:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 130.82
[32m[20230113 19:48:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.08
[32m[20230113 19:48:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.65
[32m[20230113 19:48:41 @agent_ppo2.py:144][0m Total time:       4.14 min
[32m[20230113 19:48:41 @agent_ppo2.py:146][0m 362496 total steps have happened
[32m[20230113 19:48:41 @agent_ppo2.py:122][0m #------------------------ Iteration 177 --------------------------#
[32m[20230113 19:48:42 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:42 @agent_ppo2.py:186][0m |          -0.0004 |           3.8666 |           3.0131 |
[32m[20230113 19:48:42 @agent_ppo2.py:186][0m |          -0.0037 |           3.3098 |           3.0097 |
[32m[20230113 19:48:42 @agent_ppo2.py:186][0m |          -0.0050 |           3.1966 |           3.0096 |
[32m[20230113 19:48:42 @agent_ppo2.py:186][0m |          -0.0064 |           3.1277 |           3.0085 |
[32m[20230113 19:48:42 @agent_ppo2.py:186][0m |          -0.0065 |           3.0775 |           3.0062 |
[32m[20230113 19:48:42 @agent_ppo2.py:186][0m |          -0.0083 |           3.0265 |           3.0082 |
[32m[20230113 19:48:42 @agent_ppo2.py:186][0m |          -0.0088 |           3.0040 |           3.0106 |
[32m[20230113 19:48:42 @agent_ppo2.py:186][0m |          -0.0093 |           2.9636 |           3.0099 |
[32m[20230113 19:48:42 @agent_ppo2.py:186][0m |          -0.0102 |           2.9418 |           3.0101 |
[32m[20230113 19:48:42 @agent_ppo2.py:186][0m |          -0.0104 |           2.9148 |           3.0105 |
[32m[20230113 19:48:42 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 205.35
[32m[20230113 19:48:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 214.86
[32m[20230113 19:48:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.25
[32m[20230113 19:48:43 @agent_ppo2.py:144][0m Total time:       4.17 min
[32m[20230113 19:48:43 @agent_ppo2.py:146][0m 364544 total steps have happened
[32m[20230113 19:48:43 @agent_ppo2.py:122][0m #------------------------ Iteration 178 --------------------------#
[32m[20230113 19:48:43 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:48:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:43 @agent_ppo2.py:186][0m |          -0.0010 |          15.9011 |           2.9242 |
[32m[20230113 19:48:43 @agent_ppo2.py:186][0m |          -0.0030 |           7.6074 |           2.9175 |
[32m[20230113 19:48:43 @agent_ppo2.py:186][0m |          -0.0070 |           5.4544 |           2.9161 |
[32m[20230113 19:48:43 @agent_ppo2.py:186][0m |          -0.0068 |           4.5937 |           2.9128 |
[32m[20230113 19:48:43 @agent_ppo2.py:186][0m |          -0.0093 |           4.0504 |           2.9135 |
[32m[20230113 19:48:44 @agent_ppo2.py:186][0m |          -0.0084 |           3.8055 |           2.9117 |
[32m[20230113 19:48:44 @agent_ppo2.py:186][0m |          -0.0103 |           3.5825 |           2.9120 |
[32m[20230113 19:48:44 @agent_ppo2.py:186][0m |          -0.0087 |           3.5899 |           2.9113 |
[32m[20230113 19:48:44 @agent_ppo2.py:186][0m |          -0.0115 |           3.3366 |           2.9105 |
[32m[20230113 19:48:44 @agent_ppo2.py:186][0m |          -0.0089 |           3.2791 |           2.9106 |
[32m[20230113 19:48:44 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 19:48:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 118.49
[32m[20230113 19:48:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 211.64
[32m[20230113 19:48:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 167.16
[32m[20230113 19:48:44 @agent_ppo2.py:144][0m Total time:       4.19 min
[32m[20230113 19:48:44 @agent_ppo2.py:146][0m 366592 total steps have happened
[32m[20230113 19:48:44 @agent_ppo2.py:122][0m #------------------------ Iteration 179 --------------------------#
[32m[20230113 19:48:45 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:45 @agent_ppo2.py:186][0m |           0.0010 |           5.1317 |           2.9412 |
[32m[20230113 19:48:45 @agent_ppo2.py:186][0m |          -0.0039 |           4.2992 |           2.9386 |
[32m[20230113 19:48:45 @agent_ppo2.py:186][0m |          -0.0053 |           4.1843 |           2.9373 |
[32m[20230113 19:48:45 @agent_ppo2.py:186][0m |          -0.0069 |           4.0779 |           2.9373 |
[32m[20230113 19:48:45 @agent_ppo2.py:186][0m |          -0.0076 |           4.0382 |           2.9388 |
[32m[20230113 19:48:45 @agent_ppo2.py:186][0m |          -0.0077 |           3.9930 |           2.9398 |
[32m[20230113 19:48:45 @agent_ppo2.py:186][0m |          -0.0088 |           3.9332 |           2.9388 |
[32m[20230113 19:48:45 @agent_ppo2.py:186][0m |          -0.0088 |           3.9271 |           2.9392 |
[32m[20230113 19:48:45 @agent_ppo2.py:186][0m |          -0.0094 |           3.8591 |           2.9408 |
[32m[20230113 19:48:45 @agent_ppo2.py:186][0m |          -0.0099 |           3.8209 |           2.9400 |
[32m[20230113 19:48:45 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 214.25
[32m[20230113 19:48:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.62
[32m[20230113 19:48:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.51
[32m[20230113 19:48:46 @agent_ppo2.py:144][0m Total time:       4.22 min
[32m[20230113 19:48:46 @agent_ppo2.py:146][0m 368640 total steps have happened
[32m[20230113 19:48:46 @agent_ppo2.py:122][0m #------------------------ Iteration 180 --------------------------#
[32m[20230113 19:48:46 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 19:48:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:46 @agent_ppo2.py:186][0m |          -0.0006 |           9.1595 |           2.8098 |
[32m[20230113 19:48:46 @agent_ppo2.py:186][0m |          -0.0061 |           6.2800 |           2.8053 |
[32m[20230113 19:48:46 @agent_ppo2.py:186][0m |          -0.0078 |           4.9892 |           2.8012 |
[32m[20230113 19:48:46 @agent_ppo2.py:186][0m |          -0.0093 |           4.4286 |           2.7990 |
[32m[20230113 19:48:46 @agent_ppo2.py:186][0m |          -0.0098 |           4.0108 |           2.7969 |
[32m[20230113 19:48:46 @agent_ppo2.py:186][0m |          -0.0101 |           3.6694 |           2.7962 |
[32m[20230113 19:48:46 @agent_ppo2.py:186][0m |          -0.0094 |           3.3660 |           2.7944 |
[32m[20230113 19:48:46 @agent_ppo2.py:186][0m |          -0.0124 |           3.0680 |           2.7944 |
[32m[20230113 19:48:46 @agent_ppo2.py:186][0m |          -0.0123 |           2.8700 |           2.7929 |
[32m[20230113 19:48:46 @agent_ppo2.py:186][0m |          -0.0115 |           2.7732 |           2.7936 |
[32m[20230113 19:48:46 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 19:48:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 121.90
[32m[20230113 19:48:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 199.19
[32m[20230113 19:48:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.91
[32m[20230113 19:48:47 @agent_ppo2.py:144][0m Total time:       4.24 min
[32m[20230113 19:48:47 @agent_ppo2.py:146][0m 370688 total steps have happened
[32m[20230113 19:48:47 @agent_ppo2.py:122][0m #------------------------ Iteration 181 --------------------------#
[32m[20230113 19:48:47 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 19:48:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:47 @agent_ppo2.py:186][0m |           0.0003 |          18.5688 |           2.9127 |
[32m[20230113 19:48:47 @agent_ppo2.py:186][0m |          -0.0018 |          12.4802 |           2.9112 |
[32m[20230113 19:48:47 @agent_ppo2.py:186][0m |          -0.0042 |          10.0087 |           2.9072 |
[32m[20230113 19:48:47 @agent_ppo2.py:186][0m |          -0.0059 |           8.3889 |           2.9042 |
[32m[20230113 19:48:47 @agent_ppo2.py:186][0m |          -0.0074 |           7.3151 |           2.9028 |
[32m[20230113 19:48:47 @agent_ppo2.py:186][0m |          -0.0084 |           6.6225 |           2.9011 |
[32m[20230113 19:48:48 @agent_ppo2.py:186][0m |          -0.0090 |           6.1758 |           2.8992 |
[32m[20230113 19:48:48 @agent_ppo2.py:186][0m |          -0.0092 |           5.9014 |           2.8988 |
[32m[20230113 19:48:48 @agent_ppo2.py:186][0m |          -0.0101 |           5.3954 |           2.8964 |
[32m[20230113 19:48:48 @agent_ppo2.py:186][0m |          -0.0103 |           5.1412 |           2.8960 |
[32m[20230113 19:48:48 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 19:48:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 117.37
[32m[20230113 19:48:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 199.00
[32m[20230113 19:48:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.65
[32m[20230113 19:48:48 @agent_ppo2.py:144][0m Total time:       4.26 min
[32m[20230113 19:48:48 @agent_ppo2.py:146][0m 372736 total steps have happened
[32m[20230113 19:48:48 @agent_ppo2.py:122][0m #------------------------ Iteration 182 --------------------------#
[32m[20230113 19:48:49 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:49 @agent_ppo2.py:186][0m |           0.0000 |           2.7179 |           2.9401 |
[32m[20230113 19:48:49 @agent_ppo2.py:186][0m |          -0.0038 |           2.3757 |           2.9403 |
[32m[20230113 19:48:49 @agent_ppo2.py:186][0m |          -0.0060 |           2.2179 |           2.9379 |
[32m[20230113 19:48:49 @agent_ppo2.py:186][0m |          -0.0070 |           2.1131 |           2.9386 |
[32m[20230113 19:48:49 @agent_ppo2.py:186][0m |          -0.0078 |           2.0364 |           2.9404 |
[32m[20230113 19:48:49 @agent_ppo2.py:186][0m |          -0.0082 |           1.9768 |           2.9394 |
[32m[20230113 19:48:49 @agent_ppo2.py:186][0m |          -0.0086 |           1.9352 |           2.9382 |
[32m[20230113 19:48:49 @agent_ppo2.py:186][0m |          -0.0092 |           1.8957 |           2.9399 |
[32m[20230113 19:48:49 @agent_ppo2.py:186][0m |          -0.0097 |           1.8594 |           2.9397 |
[32m[20230113 19:48:49 @agent_ppo2.py:186][0m |          -0.0106 |           1.8353 |           2.9386 |
[32m[20230113 19:48:49 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:48:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 207.78
[32m[20230113 19:48:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 214.21
[32m[20230113 19:48:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.20
[32m[20230113 19:48:49 @agent_ppo2.py:144][0m Total time:       4.28 min
[32m[20230113 19:48:49 @agent_ppo2.py:146][0m 374784 total steps have happened
[32m[20230113 19:48:49 @agent_ppo2.py:122][0m #------------------------ Iteration 183 --------------------------#
[32m[20230113 19:48:50 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:50 @agent_ppo2.py:186][0m |          -0.0000 |           5.6308 |           2.9312 |
[32m[20230113 19:48:50 @agent_ppo2.py:186][0m |          -0.0037 |           4.7718 |           2.9274 |
[32m[20230113 19:48:50 @agent_ppo2.py:186][0m |          -0.0066 |           4.5420 |           2.9261 |
[32m[20230113 19:48:50 @agent_ppo2.py:186][0m |          -0.0082 |           4.4209 |           2.9245 |
[32m[20230113 19:48:50 @agent_ppo2.py:186][0m |          -0.0095 |           4.3073 |           2.9226 |
[32m[20230113 19:48:50 @agent_ppo2.py:186][0m |          -0.0098 |           4.2461 |           2.9206 |
[32m[20230113 19:48:50 @agent_ppo2.py:186][0m |          -0.0105 |           4.1795 |           2.9193 |
[32m[20230113 19:48:50 @agent_ppo2.py:186][0m |          -0.0112 |           4.1155 |           2.9188 |
[32m[20230113 19:48:50 @agent_ppo2.py:186][0m |          -0.0113 |           4.0387 |           2.9184 |
[32m[20230113 19:48:50 @agent_ppo2.py:186][0m |          -0.0116 |           3.9878 |           2.9184 |
[32m[20230113 19:48:50 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 197.17
[32m[20230113 19:48:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 201.07
[32m[20230113 19:48:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.08
[32m[20230113 19:48:51 @agent_ppo2.py:144][0m Total time:       4.31 min
[32m[20230113 19:48:51 @agent_ppo2.py:146][0m 376832 total steps have happened
[32m[20230113 19:48:51 @agent_ppo2.py:122][0m #------------------------ Iteration 184 --------------------------#
[32m[20230113 19:48:51 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:48:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:52 @agent_ppo2.py:186][0m |           0.0007 |           9.2632 |           2.9359 |
[32m[20230113 19:48:52 @agent_ppo2.py:186][0m |          -0.0035 |           6.8366 |           2.9310 |
[32m[20230113 19:48:52 @agent_ppo2.py:186][0m |          -0.0065 |           5.6544 |           2.9290 |
[32m[20230113 19:48:52 @agent_ppo2.py:186][0m |          -0.0084 |           5.1353 |           2.9278 |
[32m[20230113 19:48:52 @agent_ppo2.py:186][0m |          -0.0091 |           4.7945 |           2.9262 |
[32m[20230113 19:48:52 @agent_ppo2.py:186][0m |          -0.0102 |           4.6379 |           2.9271 |
[32m[20230113 19:48:52 @agent_ppo2.py:186][0m |          -0.0102 |           4.5102 |           2.9273 |
[32m[20230113 19:48:52 @agent_ppo2.py:186][0m |          -0.0103 |           4.5704 |           2.9270 |
[32m[20230113 19:48:52 @agent_ppo2.py:186][0m |          -0.0109 |           4.1827 |           2.9289 |
[32m[20230113 19:48:52 @agent_ppo2.py:186][0m |          -0.0118 |           4.0642 |           2.9300 |
[32m[20230113 19:48:52 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 19:48:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 105.52
[32m[20230113 19:48:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 200.95
[32m[20230113 19:48:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 124.78
[32m[20230113 19:48:52 @agent_ppo2.py:144][0m Total time:       4.33 min
[32m[20230113 19:48:52 @agent_ppo2.py:146][0m 378880 total steps have happened
[32m[20230113 19:48:52 @agent_ppo2.py:122][0m #------------------------ Iteration 185 --------------------------#
[32m[20230113 19:48:53 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 19:48:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:53 @agent_ppo2.py:186][0m |          -0.0002 |          18.6491 |           2.9267 |
[32m[20230113 19:48:53 @agent_ppo2.py:186][0m |          -0.0049 |           9.8330 |           2.9265 |
[32m[20230113 19:48:53 @agent_ppo2.py:186][0m |          -0.0102 |           8.3859 |           2.9261 |
[32m[20230113 19:48:53 @agent_ppo2.py:186][0m |          -0.0082 |           7.9360 |           2.9259 |
[32m[20230113 19:48:53 @agent_ppo2.py:186][0m |          -0.0128 |           7.5049 |           2.9239 |
[32m[20230113 19:48:53 @agent_ppo2.py:186][0m |          -0.0145 |           7.3434 |           2.9241 |
[32m[20230113 19:48:53 @agent_ppo2.py:186][0m |          -0.0099 |           7.1995 |           2.9232 |
[32m[20230113 19:48:53 @agent_ppo2.py:186][0m |          -0.0160 |           6.9682 |           2.9227 |
[32m[20230113 19:48:53 @agent_ppo2.py:186][0m |          -0.0173 |           6.8424 |           2.9224 |
[32m[20230113 19:48:53 @agent_ppo2.py:186][0m |          -0.0122 |           6.7440 |           2.9235 |
[32m[20230113 19:48:53 @agent_ppo2.py:131][0m Policy update time: 0.40 s
[32m[20230113 19:48:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 108.12
[32m[20230113 19:48:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 205.21
[32m[20230113 19:48:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.57
[32m[20230113 19:48:54 @agent_ppo2.py:144][0m Total time:       4.35 min
[32m[20230113 19:48:54 @agent_ppo2.py:146][0m 380928 total steps have happened
[32m[20230113 19:48:54 @agent_ppo2.py:122][0m #------------------------ Iteration 186 --------------------------#
[32m[20230113 19:48:54 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:54 @agent_ppo2.py:186][0m |          -0.0015 |           8.0061 |           2.9320 |
[32m[20230113 19:48:54 @agent_ppo2.py:186][0m |          -0.0066 |           6.3524 |           2.9304 |
[32m[20230113 19:48:54 @agent_ppo2.py:186][0m |          -0.0096 |           5.9391 |           2.9289 |
[32m[20230113 19:48:54 @agent_ppo2.py:186][0m |          -0.0111 |           5.6534 |           2.9278 |
[32m[20230113 19:48:54 @agent_ppo2.py:186][0m |          -0.0118 |           5.4058 |           2.9276 |
[32m[20230113 19:48:54 @agent_ppo2.py:186][0m |          -0.0126 |           5.2056 |           2.9284 |
[32m[20230113 19:48:54 @agent_ppo2.py:186][0m |          -0.0133 |           5.0501 |           2.9289 |
[32m[20230113 19:48:54 @agent_ppo2.py:186][0m |          -0.0139 |           4.9524 |           2.9285 |
[32m[20230113 19:48:54 @agent_ppo2.py:186][0m |          -0.0142 |           4.7801 |           2.9286 |
[32m[20230113 19:48:55 @agent_ppo2.py:186][0m |          -0.0141 |           4.6990 |           2.9299 |
[32m[20230113 19:48:55 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 190.83
[32m[20230113 19:48:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 204.56
[32m[20230113 19:48:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.92
[32m[20230113 19:48:55 @agent_ppo2.py:144][0m Total time:       4.37 min
[32m[20230113 19:48:55 @agent_ppo2.py:146][0m 382976 total steps have happened
[32m[20230113 19:48:55 @agent_ppo2.py:122][0m #------------------------ Iteration 187 --------------------------#
[32m[20230113 19:48:55 @agent_ppo2.py:128][0m Sampling time: 0.55 s by 1 slaves
[32m[20230113 19:48:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:56 @agent_ppo2.py:186][0m |          -0.0012 |          27.2870 |           2.9473 |
[32m[20230113 19:48:56 @agent_ppo2.py:186][0m |          -0.0068 |          14.8202 |           2.9412 |
[32m[20230113 19:48:56 @agent_ppo2.py:186][0m |          -0.0072 |          12.0607 |           2.9374 |
[32m[20230113 19:48:56 @agent_ppo2.py:186][0m |          -0.0097 |          10.7522 |           2.9348 |
[32m[20230113 19:48:56 @agent_ppo2.py:186][0m |          -0.0099 |          10.0011 |           2.9335 |
[32m[20230113 19:48:56 @agent_ppo2.py:186][0m |          -0.0111 |           9.4368 |           2.9334 |
[32m[20230113 19:48:56 @agent_ppo2.py:186][0m |          -0.0106 |           9.1282 |           2.9315 |
[32m[20230113 19:48:56 @agent_ppo2.py:186][0m |          -0.0110 |           8.8370 |           2.9309 |
[32m[20230113 19:48:56 @agent_ppo2.py:186][0m |          -0.0115 |           8.4894 |           2.9300 |
[32m[20230113 19:48:56 @agent_ppo2.py:186][0m |          -0.0127 |           8.2789 |           2.9288 |
[32m[20230113 19:48:56 @agent_ppo2.py:131][0m Policy update time: 0.55 s
[32m[20230113 19:48:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 62.51
[32m[20230113 19:48:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.75
[32m[20230113 19:48:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.37
[32m[20230113 19:48:57 @agent_ppo2.py:144][0m Total time:       4.40 min
[32m[20230113 19:48:57 @agent_ppo2.py:146][0m 385024 total steps have happened
[32m[20230113 19:48:57 @agent_ppo2.py:122][0m #------------------------ Iteration 188 --------------------------#
[32m[20230113 19:48:57 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:48:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:57 @agent_ppo2.py:186][0m |           0.0000 |           5.6366 |           2.9585 |
[32m[20230113 19:48:57 @agent_ppo2.py:186][0m |          -0.0037 |           5.1236 |           2.9567 |
[32m[20230113 19:48:57 @agent_ppo2.py:186][0m |          -0.0050 |           5.0269 |           2.9555 |
[32m[20230113 19:48:57 @agent_ppo2.py:186][0m |          -0.0061 |           4.9663 |           2.9547 |
[32m[20230113 19:48:57 @agent_ppo2.py:186][0m |          -0.0072 |           4.9123 |           2.9532 |
[32m[20230113 19:48:57 @agent_ppo2.py:186][0m |          -0.0057 |           4.9181 |           2.9538 |
[32m[20230113 19:48:57 @agent_ppo2.py:186][0m |          -0.0075 |           4.8441 |           2.9546 |
[32m[20230113 19:48:57 @agent_ppo2.py:186][0m |          -0.0092 |           4.8151 |           2.9546 |
[32m[20230113 19:48:57 @agent_ppo2.py:186][0m |          -0.0086 |           4.7796 |           2.9544 |
[32m[20230113 19:48:57 @agent_ppo2.py:186][0m |          -0.0079 |           4.7883 |           2.9552 |
[32m[20230113 19:48:57 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:48:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.80
[32m[20230113 19:48:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.92
[32m[20230113 19:48:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.87
[32m[20230113 19:48:58 @agent_ppo2.py:144][0m Total time:       4.42 min
[32m[20230113 19:48:58 @agent_ppo2.py:146][0m 387072 total steps have happened
[32m[20230113 19:48:58 @agent_ppo2.py:122][0m #------------------------ Iteration 189 --------------------------#
[32m[20230113 19:48:58 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:48:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:48:59 @agent_ppo2.py:186][0m |           0.0011 |          16.5950 |           2.9721 |
[32m[20230113 19:48:59 @agent_ppo2.py:186][0m |          -0.0021 |           8.2767 |           2.9717 |
[32m[20230113 19:48:59 @agent_ppo2.py:186][0m |          -0.0043 |           7.0372 |           2.9686 |
[32m[20230113 19:48:59 @agent_ppo2.py:186][0m |          -0.0062 |           6.4979 |           2.9692 |
[32m[20230113 19:48:59 @agent_ppo2.py:186][0m |          -0.0089 |           6.0945 |           2.9689 |
[32m[20230113 19:48:59 @agent_ppo2.py:186][0m |          -0.0112 |           5.8512 |           2.9692 |
[32m[20230113 19:48:59 @agent_ppo2.py:186][0m |          -0.0089 |           5.7052 |           2.9691 |
[32m[20230113 19:48:59 @agent_ppo2.py:186][0m |          -0.0099 |           5.6320 |           2.9705 |
[32m[20230113 19:48:59 @agent_ppo2.py:186][0m |          -0.0123 |           5.4956 |           2.9707 |
[32m[20230113 19:48:59 @agent_ppo2.py:186][0m |          -0.0127 |           5.3505 |           2.9690 |
[32m[20230113 19:48:59 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:48:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 97.14
[32m[20230113 19:48:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 198.91
[32m[20230113 19:48:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.28
[32m[20230113 19:48:59 @agent_ppo2.py:144][0m Total time:       4.45 min
[32m[20230113 19:48:59 @agent_ppo2.py:146][0m 389120 total steps have happened
[32m[20230113 19:48:59 @agent_ppo2.py:122][0m #------------------------ Iteration 190 --------------------------#
[32m[20230113 19:49:00 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 19:49:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:00 @agent_ppo2.py:186][0m |           0.0122 |          42.4739 |           2.9347 |
[32m[20230113 19:49:00 @agent_ppo2.py:186][0m |           0.0021 |          23.1424 |           2.9312 |
[32m[20230113 19:49:00 @agent_ppo2.py:186][0m |          -0.0064 |          18.2224 |           2.9319 |
[32m[20230113 19:49:00 @agent_ppo2.py:186][0m |          -0.0046 |          17.5401 |           2.9331 |
[32m[20230113 19:49:00 @agent_ppo2.py:186][0m |          -0.0068 |          16.2705 |           2.9322 |
[32m[20230113 19:49:00 @agent_ppo2.py:186][0m |          -0.0054 |          15.0015 |           2.9322 |
[32m[20230113 19:49:00 @agent_ppo2.py:186][0m |          -0.0091 |          14.2399 |           2.9307 |
[32m[20230113 19:49:00 @agent_ppo2.py:186][0m |          -0.0067 |          13.6099 |           2.9288 |
[32m[20230113 19:49:00 @agent_ppo2.py:186][0m |          -0.0106 |          13.1590 |           2.9280 |
[32m[20230113 19:49:00 @agent_ppo2.py:186][0m |          -0.0078 |          12.5821 |           2.9272 |
[32m[20230113 19:49:00 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 19:49:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 29.52
[32m[20230113 19:49:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 35.04
[32m[20230113 19:49:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.57
[32m[20230113 19:49:01 @agent_ppo2.py:144][0m Total time:       4.47 min
[32m[20230113 19:49:01 @agent_ppo2.py:146][0m 391168 total steps have happened
[32m[20230113 19:49:01 @agent_ppo2.py:122][0m #------------------------ Iteration 191 --------------------------#
[32m[20230113 19:49:01 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:01 @agent_ppo2.py:186][0m |           0.0001 |          13.0990 |           2.9010 |
[32m[20230113 19:49:01 @agent_ppo2.py:186][0m |          -0.0037 |           7.7364 |           2.8992 |
[32m[20230113 19:49:01 @agent_ppo2.py:186][0m |          -0.0059 |           6.7290 |           2.8967 |
[32m[20230113 19:49:01 @agent_ppo2.py:186][0m |          -0.0075 |           6.0973 |           2.8961 |
[32m[20230113 19:49:01 @agent_ppo2.py:186][0m |          -0.0083 |           5.6521 |           2.8961 |
[32m[20230113 19:49:01 @agent_ppo2.py:186][0m |          -0.0090 |           5.3420 |           2.8969 |
[32m[20230113 19:49:01 @agent_ppo2.py:186][0m |          -0.0096 |           5.0914 |           2.8975 |
[32m[20230113 19:49:01 @agent_ppo2.py:186][0m |          -0.0097 |           4.9316 |           2.8968 |
[32m[20230113 19:49:01 @agent_ppo2.py:186][0m |          -0.0101 |           4.7585 |           2.8975 |
[32m[20230113 19:49:01 @agent_ppo2.py:186][0m |          -0.0103 |           4.6520 |           2.9005 |
[32m[20230113 19:49:01 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 195.17
[32m[20230113 19:49:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 202.64
[32m[20230113 19:49:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.57
[32m[20230113 19:49:02 @agent_ppo2.py:144][0m Total time:       4.49 min
[32m[20230113 19:49:02 @agent_ppo2.py:146][0m 393216 total steps have happened
[32m[20230113 19:49:02 @agent_ppo2.py:122][0m #------------------------ Iteration 192 --------------------------#
[32m[20230113 19:49:02 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:03 @agent_ppo2.py:186][0m |           0.0002 |           3.6303 |           2.9608 |
[32m[20230113 19:49:03 @agent_ppo2.py:186][0m |          -0.0042 |           3.1043 |           2.9640 |
[32m[20230113 19:49:03 @agent_ppo2.py:186][0m |          -0.0064 |           2.7774 |           2.9630 |
[32m[20230113 19:49:03 @agent_ppo2.py:186][0m |          -0.0075 |           2.5850 |           2.9595 |
[32m[20230113 19:49:03 @agent_ppo2.py:186][0m |          -0.0081 |           2.4427 |           2.9560 |
[32m[20230113 19:49:03 @agent_ppo2.py:186][0m |          -0.0084 |           2.3456 |           2.9592 |
[32m[20230113 19:49:03 @agent_ppo2.py:186][0m |          -0.0092 |           2.2718 |           2.9554 |
[32m[20230113 19:49:03 @agent_ppo2.py:186][0m |          -0.0100 |           2.1994 |           2.9554 |
[32m[20230113 19:49:03 @agent_ppo2.py:186][0m |          -0.0100 |           2.1513 |           2.9559 |
[32m[20230113 19:49:03 @agent_ppo2.py:186][0m |          -0.0106 |           2.1012 |           2.9561 |
[32m[20230113 19:49:03 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 198.33
[32m[20230113 19:49:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 201.35
[32m[20230113 19:49:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 107.76
[32m[20230113 19:49:03 @agent_ppo2.py:144][0m Total time:       4.51 min
[32m[20230113 19:49:03 @agent_ppo2.py:146][0m 395264 total steps have happened
[32m[20230113 19:49:03 @agent_ppo2.py:122][0m #------------------------ Iteration 193 --------------------------#
[32m[20230113 19:49:04 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:49:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:04 @agent_ppo2.py:186][0m |           0.0011 |           3.3569 |           2.9994 |
[32m[20230113 19:49:04 @agent_ppo2.py:186][0m |          -0.0018 |           3.0940 |           3.0007 |
[32m[20230113 19:49:04 @agent_ppo2.py:186][0m |          -0.0040 |           3.0168 |           3.0005 |
[32m[20230113 19:49:04 @agent_ppo2.py:186][0m |          -0.0056 |           2.9604 |           3.0005 |
[32m[20230113 19:49:04 @agent_ppo2.py:186][0m |          -0.0065 |           2.9310 |           2.9998 |
[32m[20230113 19:49:04 @agent_ppo2.py:186][0m |          -0.0071 |           2.8975 |           3.0002 |
[32m[20230113 19:49:04 @agent_ppo2.py:186][0m |          -0.0078 |           2.8859 |           3.0006 |
[32m[20230113 19:49:04 @agent_ppo2.py:186][0m |          -0.0077 |           2.8539 |           3.0011 |
[32m[20230113 19:49:04 @agent_ppo2.py:186][0m |          -0.0086 |           2.8401 |           3.0022 |
[32m[20230113 19:49:04 @agent_ppo2.py:186][0m |          -0.0092 |           2.8251 |           3.0014 |
[32m[20230113 19:49:04 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 209.29
[32m[20230113 19:49:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.87
[32m[20230113 19:49:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.67
[32m[20230113 19:49:05 @agent_ppo2.py:144][0m Total time:       4.53 min
[32m[20230113 19:49:05 @agent_ppo2.py:146][0m 397312 total steps have happened
[32m[20230113 19:49:05 @agent_ppo2.py:122][0m #------------------------ Iteration 194 --------------------------#
[32m[20230113 19:49:05 @agent_ppo2.py:128][0m Sampling time: 0.55 s by 1 slaves
[32m[20230113 19:49:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:05 @agent_ppo2.py:186][0m |           0.0007 |          14.2579 |           2.9801 |
[32m[20230113 19:49:05 @agent_ppo2.py:186][0m |          -0.0048 |           7.8890 |           2.9798 |
[32m[20230113 19:49:05 @agent_ppo2.py:186][0m |          -0.0069 |           6.1761 |           2.9796 |
[32m[20230113 19:49:05 @agent_ppo2.py:186][0m |          -0.0086 |           5.4852 |           2.9793 |
[32m[20230113 19:49:05 @agent_ppo2.py:186][0m |          -0.0104 |           5.1706 |           2.9786 |
[32m[20230113 19:49:06 @agent_ppo2.py:186][0m |          -0.0111 |           4.9020 |           2.9777 |
[32m[20230113 19:49:06 @agent_ppo2.py:186][0m |          -0.0113 |           4.7405 |           2.9768 |
[32m[20230113 19:49:06 @agent_ppo2.py:186][0m |          -0.0116 |           4.6716 |           2.9765 |
[32m[20230113 19:49:06 @agent_ppo2.py:186][0m |          -0.0123 |           4.5009 |           2.9781 |
[32m[20230113 19:49:06 @agent_ppo2.py:186][0m |          -0.0129 |           4.3563 |           2.9772 |
[32m[20230113 19:49:06 @agent_ppo2.py:131][0m Policy update time: 0.55 s
[32m[20230113 19:49:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 108.42
[32m[20230113 19:49:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 192.88
[32m[20230113 19:49:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.84
[32m[20230113 19:49:06 @agent_ppo2.py:144][0m Total time:       4.56 min
[32m[20230113 19:49:06 @agent_ppo2.py:146][0m 399360 total steps have happened
[32m[20230113 19:49:06 @agent_ppo2.py:122][0m #------------------------ Iteration 195 --------------------------#
[32m[20230113 19:49:07 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:07 @agent_ppo2.py:186][0m |          -0.0005 |           3.4058 |           3.0576 |
[32m[20230113 19:49:07 @agent_ppo2.py:186][0m |          -0.0064 |           3.1200 |           3.0547 |
[32m[20230113 19:49:07 @agent_ppo2.py:186][0m |          -0.0092 |           3.0081 |           3.0501 |
[32m[20230113 19:49:07 @agent_ppo2.py:186][0m |          -0.0101 |           2.9304 |           3.0483 |
[32m[20230113 19:49:07 @agent_ppo2.py:186][0m |          -0.0110 |           2.8744 |           3.0471 |
[32m[20230113 19:49:07 @agent_ppo2.py:186][0m |          -0.0123 |           2.8285 |           3.0474 |
[32m[20230113 19:49:07 @agent_ppo2.py:186][0m |          -0.0128 |           2.7875 |           3.0464 |
[32m[20230113 19:49:07 @agent_ppo2.py:186][0m |          -0.0133 |           2.7518 |           3.0462 |
[32m[20230113 19:49:07 @agent_ppo2.py:186][0m |          -0.0142 |           2.7231 |           3.0455 |
[32m[20230113 19:49:07 @agent_ppo2.py:186][0m |          -0.0144 |           2.6958 |           3.0456 |
[32m[20230113 19:49:07 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 193.88
[32m[20230113 19:49:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 201.16
[32m[20230113 19:49:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.86
[32m[20230113 19:49:08 @agent_ppo2.py:144][0m Total time:       4.58 min
[32m[20230113 19:49:08 @agent_ppo2.py:146][0m 401408 total steps have happened
[32m[20230113 19:49:08 @agent_ppo2.py:122][0m #------------------------ Iteration 196 --------------------------#
[32m[20230113 19:49:08 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:08 @agent_ppo2.py:186][0m |           0.0005 |           3.5718 |           3.0113 |
[32m[20230113 19:49:08 @agent_ppo2.py:186][0m |          -0.0033 |           3.4767 |           3.0093 |
[32m[20230113 19:49:08 @agent_ppo2.py:186][0m |          -0.0046 |           3.4213 |           3.0097 |
[32m[20230113 19:49:08 @agent_ppo2.py:186][0m |          -0.0057 |           3.3696 |           3.0092 |
[32m[20230113 19:49:08 @agent_ppo2.py:186][0m |          -0.0067 |           3.3489 |           3.0099 |
[32m[20230113 19:49:08 @agent_ppo2.py:186][0m |          -0.0075 |           3.2915 |           3.0107 |
[32m[20230113 19:49:08 @agent_ppo2.py:186][0m |          -0.0078 |           3.2597 |           3.0099 |
[32m[20230113 19:49:08 @agent_ppo2.py:186][0m |          -0.0087 |           3.2189 |           3.0111 |
[32m[20230113 19:49:09 @agent_ppo2.py:186][0m |          -0.0095 |           3.1865 |           3.0106 |
[32m[20230113 19:49:09 @agent_ppo2.py:186][0m |          -0.0098 |           3.1625 |           3.0115 |
[32m[20230113 19:49:09 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 208.80
[32m[20230113 19:49:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 211.12
[32m[20230113 19:49:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.26
[32m[20230113 19:49:09 @agent_ppo2.py:144][0m Total time:       4.61 min
[32m[20230113 19:49:09 @agent_ppo2.py:146][0m 403456 total steps have happened
[32m[20230113 19:49:09 @agent_ppo2.py:122][0m #------------------------ Iteration 197 --------------------------#
[32m[20230113 19:49:10 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:49:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:10 @agent_ppo2.py:186][0m |          -0.0012 |          16.9033 |           2.9946 |
[32m[20230113 19:49:10 @agent_ppo2.py:186][0m |          -0.0047 |          10.3260 |           2.9961 |
[32m[20230113 19:49:10 @agent_ppo2.py:186][0m |          -0.0053 |           8.7655 |           2.9967 |
[32m[20230113 19:49:10 @agent_ppo2.py:186][0m |          -0.0089 |           7.8527 |           2.9958 |
[32m[20230113 19:49:10 @agent_ppo2.py:186][0m |          -0.0053 |           7.3236 |           2.9960 |
[32m[20230113 19:49:10 @agent_ppo2.py:186][0m |          -0.0101 |           6.8877 |           2.9958 |
[32m[20230113 19:49:10 @agent_ppo2.py:186][0m |          -0.0077 |           6.5741 |           2.9961 |
[32m[20230113 19:49:10 @agent_ppo2.py:186][0m |          -0.0087 |           6.3830 |           2.9958 |
[32m[20230113 19:49:10 @agent_ppo2.py:186][0m |          -0.0100 |           6.2867 |           2.9954 |
[32m[20230113 19:49:10 @agent_ppo2.py:186][0m |          -0.0114 |           5.9198 |           2.9942 |
[32m[20230113 19:49:10 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 19:49:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 125.52
[32m[20230113 19:49:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.97
[32m[20230113 19:49:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.70
[32m[20230113 19:49:11 @agent_ppo2.py:144][0m Total time:       4.63 min
[32m[20230113 19:49:11 @agent_ppo2.py:146][0m 405504 total steps have happened
[32m[20230113 19:49:11 @agent_ppo2.py:122][0m #------------------------ Iteration 198 --------------------------#
[32m[20230113 19:49:11 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:49:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:11 @agent_ppo2.py:186][0m |          -0.0025 |          19.8060 |           3.0066 |
[32m[20230113 19:49:11 @agent_ppo2.py:186][0m |          -0.0028 |          13.2689 |           3.0068 |
[32m[20230113 19:49:11 @agent_ppo2.py:186][0m |          -0.0097 |          12.3482 |           3.0033 |
[32m[20230113 19:49:11 @agent_ppo2.py:186][0m |          -0.0097 |          11.5474 |           3.0028 |
[32m[20230113 19:49:11 @agent_ppo2.py:186][0m |          -0.0135 |          11.2133 |           3.0000 |
[32m[20230113 19:49:11 @agent_ppo2.py:186][0m |          -0.0126 |          10.5719 |           3.0001 |
[32m[20230113 19:49:11 @agent_ppo2.py:186][0m |          -0.0156 |           9.8630 |           2.9991 |
[32m[20230113 19:49:11 @agent_ppo2.py:186][0m |          -0.0172 |           9.5423 |           2.9977 |
[32m[20230113 19:49:11 @agent_ppo2.py:186][0m |          -0.0148 |           9.3266 |           2.9975 |
[32m[20230113 19:49:11 @agent_ppo2.py:186][0m |          -0.0170 |           9.0010 |           2.9972 |
[32m[20230113 19:49:11 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:49:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 135.15
[32m[20230113 19:49:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 205.40
[32m[20230113 19:49:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.34
[32m[20230113 19:49:12 @agent_ppo2.py:144][0m Total time:       4.66 min
[32m[20230113 19:49:12 @agent_ppo2.py:146][0m 407552 total steps have happened
[32m[20230113 19:49:12 @agent_ppo2.py:122][0m #------------------------ Iteration 199 --------------------------#
[32m[20230113 19:49:12 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:12 @agent_ppo2.py:186][0m |          -0.0006 |           3.6875 |           3.0455 |
[32m[20230113 19:49:13 @agent_ppo2.py:186][0m |          -0.0064 |           3.2789 |           3.0428 |
[32m[20230113 19:49:13 @agent_ppo2.py:186][0m |          -0.0088 |           3.0986 |           3.0399 |
[32m[20230113 19:49:13 @agent_ppo2.py:186][0m |          -0.0098 |           2.9815 |           3.0428 |
[32m[20230113 19:49:13 @agent_ppo2.py:186][0m |          -0.0112 |           2.9259 |           3.0427 |
[32m[20230113 19:49:13 @agent_ppo2.py:186][0m |          -0.0117 |           2.8635 |           3.0450 |
[32m[20230113 19:49:13 @agent_ppo2.py:186][0m |          -0.0121 |           2.8231 |           3.0446 |
[32m[20230113 19:49:13 @agent_ppo2.py:186][0m |          -0.0129 |           2.7935 |           3.0454 |
[32m[20230113 19:49:13 @agent_ppo2.py:186][0m |          -0.0132 |           2.7600 |           3.0462 |
[32m[20230113 19:49:13 @agent_ppo2.py:186][0m |          -0.0139 |           2.7359 |           3.0481 |
[32m[20230113 19:49:13 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 210.98
[32m[20230113 19:49:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 213.31
[32m[20230113 19:49:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.15
[32m[20230113 19:49:13 @agent_ppo2.py:144][0m Total time:       4.68 min
[32m[20230113 19:49:13 @agent_ppo2.py:146][0m 409600 total steps have happened
[32m[20230113 19:49:13 @agent_ppo2.py:122][0m #------------------------ Iteration 200 --------------------------#
[32m[20230113 19:49:14 @agent_ppo2.py:128][0m Sampling time: 0.56 s by 1 slaves
[32m[20230113 19:49:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:14 @agent_ppo2.py:186][0m |          -0.0022 |          10.9613 |           3.0564 |
[32m[20230113 19:49:14 @agent_ppo2.py:186][0m |          -0.0083 |           6.4385 |           3.0523 |
[32m[20230113 19:49:14 @agent_ppo2.py:186][0m |          -0.0095 |           5.8943 |           3.0490 |
[32m[20230113 19:49:14 @agent_ppo2.py:186][0m |          -0.0110 |           5.7986 |           3.0513 |
[32m[20230113 19:49:14 @agent_ppo2.py:186][0m |          -0.0114 |           5.5677 |           3.0499 |
[32m[20230113 19:49:14 @agent_ppo2.py:186][0m |          -0.0118 |           5.6550 |           3.0489 |
[32m[20230113 19:49:14 @agent_ppo2.py:186][0m |          -0.0127 |           5.3565 |           3.0506 |
[32m[20230113 19:49:14 @agent_ppo2.py:186][0m |          -0.0129 |           5.2658 |           3.0499 |
[32m[20230113 19:49:14 @agent_ppo2.py:186][0m |          -0.0133 |           5.2062 |           3.0514 |
[32m[20230113 19:49:14 @agent_ppo2.py:186][0m |          -0.0139 |           5.1593 |           3.0509 |
[32m[20230113 19:49:14 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 19:49:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 104.66
[32m[20230113 19:49:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 198.52
[32m[20230113 19:49:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.06
[32m[20230113 19:49:15 @agent_ppo2.py:144][0m Total time:       4.70 min
[32m[20230113 19:49:15 @agent_ppo2.py:146][0m 411648 total steps have happened
[32m[20230113 19:49:15 @agent_ppo2.py:122][0m #------------------------ Iteration 201 --------------------------#
[32m[20230113 19:49:15 @agent_ppo2.py:128][0m Sampling time: 0.55 s by 1 slaves
[32m[20230113 19:49:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:16 @agent_ppo2.py:186][0m |          -0.0011 |          19.1813 |           3.0821 |
[32m[20230113 19:49:16 @agent_ppo2.py:186][0m |          -0.0057 |           8.2701 |           3.0728 |
[32m[20230113 19:49:16 @agent_ppo2.py:186][0m |          -0.0079 |           7.3448 |           3.0718 |
[32m[20230113 19:49:16 @agent_ppo2.py:186][0m |          -0.0092 |           6.8072 |           3.0727 |
[32m[20230113 19:49:16 @agent_ppo2.py:186][0m |          -0.0090 |           6.5041 |           3.0738 |
[32m[20230113 19:49:16 @agent_ppo2.py:186][0m |          -0.0106 |           6.2449 |           3.0707 |
[32m[20230113 19:49:16 @agent_ppo2.py:186][0m |          -0.0113 |           6.1146 |           3.0713 |
[32m[20230113 19:49:16 @agent_ppo2.py:186][0m |          -0.0120 |           5.9278 |           3.0682 |
[32m[20230113 19:49:16 @agent_ppo2.py:186][0m |          -0.0124 |           5.8101 |           3.0682 |
[32m[20230113 19:49:16 @agent_ppo2.py:186][0m |          -0.0127 |           5.7323 |           3.0695 |
[32m[20230113 19:49:16 @agent_ppo2.py:131][0m Policy update time: 0.55 s
[32m[20230113 19:49:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 119.47
[32m[20230113 19:49:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 211.17
[32m[20230113 19:49:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.48
[32m[20230113 19:49:16 @agent_ppo2.py:144][0m Total time:       4.73 min
[32m[20230113 19:49:16 @agent_ppo2.py:146][0m 413696 total steps have happened
[32m[20230113 19:49:16 @agent_ppo2.py:122][0m #------------------------ Iteration 202 --------------------------#
[32m[20230113 19:49:17 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:17 @agent_ppo2.py:186][0m |           0.0013 |           4.1185 |           3.0274 |
[32m[20230113 19:49:17 @agent_ppo2.py:186][0m |          -0.0007 |           3.7225 |           3.0240 |
[32m[20230113 19:49:17 @agent_ppo2.py:186][0m |          -0.0028 |           3.6213 |           3.0210 |
[32m[20230113 19:49:17 @agent_ppo2.py:186][0m |          -0.0035 |           3.5635 |           3.0156 |
[32m[20230113 19:49:17 @agent_ppo2.py:186][0m |          -0.0050 |           3.5259 |           3.0150 |
[32m[20230113 19:49:17 @agent_ppo2.py:186][0m |          -0.0056 |           3.5032 |           3.0127 |
[32m[20230113 19:49:17 @agent_ppo2.py:186][0m |          -0.0061 |           3.4616 |           3.0099 |
[32m[20230113 19:49:17 @agent_ppo2.py:186][0m |          -0.0076 |           3.4431 |           3.0094 |
[32m[20230113 19:49:17 @agent_ppo2.py:186][0m |          -0.0080 |           3.4383 |           3.0098 |
[32m[20230113 19:49:17 @agent_ppo2.py:186][0m |          -0.0085 |           3.4113 |           3.0072 |
[32m[20230113 19:49:17 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 215.42
[32m[20230113 19:49:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.00
[32m[20230113 19:49:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.65
[32m[20230113 19:49:18 @agent_ppo2.py:144][0m Total time:       4.75 min
[32m[20230113 19:49:18 @agent_ppo2.py:146][0m 415744 total steps have happened
[32m[20230113 19:49:18 @agent_ppo2.py:122][0m #------------------------ Iteration 203 --------------------------#
[32m[20230113 19:49:18 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:18 @agent_ppo2.py:186][0m |           0.0003 |           2.9673 |           3.0060 |
[32m[20230113 19:49:18 @agent_ppo2.py:186][0m |          -0.0023 |           2.7981 |           3.0042 |
[32m[20230113 19:49:18 @agent_ppo2.py:186][0m |          -0.0040 |           2.7368 |           3.0024 |
[32m[20230113 19:49:18 @agent_ppo2.py:186][0m |          -0.0049 |           2.6921 |           3.0006 |
[32m[20230113 19:49:19 @agent_ppo2.py:186][0m |          -0.0056 |           2.6566 |           3.0001 |
[32m[20230113 19:49:19 @agent_ppo2.py:186][0m |          -0.0062 |           2.6245 |           3.0002 |
[32m[20230113 19:49:19 @agent_ppo2.py:186][0m |          -0.0070 |           2.6006 |           3.0009 |
[32m[20230113 19:49:19 @agent_ppo2.py:186][0m |          -0.0069 |           2.5723 |           2.9996 |
[32m[20230113 19:49:19 @agent_ppo2.py:186][0m |          -0.0076 |           2.5544 |           2.9998 |
[32m[20230113 19:49:19 @agent_ppo2.py:186][0m |          -0.0082 |           2.5418 |           3.0004 |
[32m[20230113 19:49:19 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 201.07
[32m[20230113 19:49:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 202.16
[32m[20230113 19:49:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.16
[32m[20230113 19:49:19 @agent_ppo2.py:144][0m Total time:       4.78 min
[32m[20230113 19:49:19 @agent_ppo2.py:146][0m 417792 total steps have happened
[32m[20230113 19:49:19 @agent_ppo2.py:122][0m #------------------------ Iteration 204 --------------------------#
[32m[20230113 19:49:20 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:49:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:20 @agent_ppo2.py:186][0m |          -0.0003 |           3.2876 |           3.1409 |
[32m[20230113 19:49:20 @agent_ppo2.py:186][0m |          -0.0033 |           3.1460 |           3.1378 |
[32m[20230113 19:49:20 @agent_ppo2.py:186][0m |          -0.0060 |           3.0600 |           3.1363 |
[32m[20230113 19:49:20 @agent_ppo2.py:186][0m |          -0.0069 |           3.0031 |           3.1348 |
[32m[20230113 19:49:20 @agent_ppo2.py:186][0m |          -0.0082 |           2.9731 |           3.1322 |
[32m[20230113 19:49:20 @agent_ppo2.py:186][0m |          -0.0084 |           2.9396 |           3.1326 |
[32m[20230113 19:49:20 @agent_ppo2.py:186][0m |          -0.0091 |           2.9219 |           3.1340 |
[32m[20230113 19:49:20 @agent_ppo2.py:186][0m |          -0.0095 |           2.8995 |           3.1342 |
[32m[20230113 19:49:20 @agent_ppo2.py:186][0m |          -0.0099 |           2.8909 |           3.1339 |
[32m[20230113 19:49:20 @agent_ppo2.py:186][0m |          -0.0103 |           2.8767 |           3.1352 |
[32m[20230113 19:49:20 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 204.57
[32m[20230113 19:49:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 206.14
[32m[20230113 19:49:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.05
[32m[20230113 19:49:21 @agent_ppo2.py:144][0m Total time:       4.80 min
[32m[20230113 19:49:21 @agent_ppo2.py:146][0m 419840 total steps have happened
[32m[20230113 19:49:21 @agent_ppo2.py:122][0m #------------------------ Iteration 205 --------------------------#
[32m[20230113 19:49:21 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:21 @agent_ppo2.py:186][0m |          -0.0026 |           4.2291 |           2.9526 |
[32m[20230113 19:49:21 @agent_ppo2.py:186][0m |          -0.0034 |           4.0907 |           2.9449 |
[32m[20230113 19:49:21 @agent_ppo2.py:186][0m |          -0.0047 |           4.0251 |           2.9429 |
[32m[20230113 19:49:21 @agent_ppo2.py:186][0m |          -0.0060 |           3.9499 |           2.9421 |
[32m[20230113 19:49:21 @agent_ppo2.py:186][0m |          -0.0074 |           3.8448 |           2.9403 |
[32m[20230113 19:49:21 @agent_ppo2.py:186][0m |          -0.0100 |           3.7544 |           2.9369 |
[32m[20230113 19:49:21 @agent_ppo2.py:186][0m |          -0.0101 |           3.7267 |           2.9382 |
[32m[20230113 19:49:21 @agent_ppo2.py:186][0m |          -0.0109 |           3.7132 |           2.9398 |
[32m[20230113 19:49:22 @agent_ppo2.py:186][0m |          -0.0099 |           3.6605 |           2.9395 |
[32m[20230113 19:49:22 @agent_ppo2.py:186][0m |          -0.0090 |           3.6534 |           2.9408 |
[32m[20230113 19:49:22 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 209.98
[32m[20230113 19:49:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.35
[32m[20230113 19:49:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 225.42
[32m[20230113 19:49:22 @agent_ppo2.py:144][0m Total time:       4.82 min
[32m[20230113 19:49:22 @agent_ppo2.py:146][0m 421888 total steps have happened
[32m[20230113 19:49:22 @agent_ppo2.py:122][0m #------------------------ Iteration 206 --------------------------#
[32m[20230113 19:49:23 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:23 @agent_ppo2.py:186][0m |          -0.0017 |           3.3741 |           3.0769 |
[32m[20230113 19:49:23 @agent_ppo2.py:186][0m |          -0.0044 |           3.0903 |           3.0739 |
[32m[20230113 19:49:23 @agent_ppo2.py:186][0m |          -0.0058 |           2.9821 |           3.0742 |
[32m[20230113 19:49:23 @agent_ppo2.py:186][0m |          -0.0068 |           2.9154 |           3.0727 |
[32m[20230113 19:49:23 @agent_ppo2.py:186][0m |          -0.0073 |           2.8710 |           3.0718 |
[32m[20230113 19:49:23 @agent_ppo2.py:186][0m |          -0.0078 |           2.8205 |           3.0713 |
[32m[20230113 19:49:23 @agent_ppo2.py:186][0m |          -0.0083 |           2.7834 |           3.0720 |
[32m[20230113 19:49:23 @agent_ppo2.py:186][0m |          -0.0087 |           2.7438 |           3.0715 |
[32m[20230113 19:49:23 @agent_ppo2.py:186][0m |          -0.0091 |           2.7036 |           3.0707 |
[32m[20230113 19:49:23 @agent_ppo2.py:186][0m |          -0.0095 |           2.6801 |           3.0711 |
[32m[20230113 19:49:23 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 199.35
[32m[20230113 19:49:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 201.49
[32m[20230113 19:49:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.40
[32m[20230113 19:49:23 @agent_ppo2.py:144][0m Total time:       4.85 min
[32m[20230113 19:49:23 @agent_ppo2.py:146][0m 423936 total steps have happened
[32m[20230113 19:49:23 @agent_ppo2.py:122][0m #------------------------ Iteration 207 --------------------------#
[32m[20230113 19:49:24 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:24 @agent_ppo2.py:186][0m |          -0.0004 |           3.9112 |           3.0537 |
[32m[20230113 19:49:24 @agent_ppo2.py:186][0m |          -0.0052 |           3.1445 |           3.0533 |
[32m[20230113 19:49:24 @agent_ppo2.py:186][0m |          -0.0080 |           3.0318 |           3.0546 |
[32m[20230113 19:49:24 @agent_ppo2.py:186][0m |          -0.0091 |           2.9474 |           3.0568 |
[32m[20230113 19:49:24 @agent_ppo2.py:186][0m |          -0.0108 |           2.8992 |           3.0592 |
[32m[20230113 19:49:24 @agent_ppo2.py:186][0m |          -0.0117 |           2.8479 |           3.0598 |
[32m[20230113 19:49:24 @agent_ppo2.py:186][0m |          -0.0124 |           2.8135 |           3.0600 |
[32m[20230113 19:49:24 @agent_ppo2.py:186][0m |          -0.0128 |           2.8028 |           3.0593 |
[32m[20230113 19:49:24 @agent_ppo2.py:186][0m |          -0.0133 |           2.7624 |           3.0605 |
[32m[20230113 19:49:24 @agent_ppo2.py:186][0m |          -0.0138 |           2.7378 |           3.0614 |
[32m[20230113 19:49:24 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 210.31
[32m[20230113 19:49:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.39
[32m[20230113 19:49:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 228.44
[32m[20230113 19:49:25 @agent_ppo2.py:144][0m Total time:       4.87 min
[32m[20230113 19:49:25 @agent_ppo2.py:146][0m 425984 total steps have happened
[32m[20230113 19:49:25 @agent_ppo2.py:122][0m #------------------------ Iteration 208 --------------------------#
[32m[20230113 19:49:25 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:49:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:25 @agent_ppo2.py:186][0m |           0.0000 |           3.3147 |           3.1833 |
[32m[20230113 19:49:26 @agent_ppo2.py:186][0m |          -0.0044 |           3.0971 |           3.1782 |
[32m[20230113 19:49:26 @agent_ppo2.py:186][0m |          -0.0063 |           2.9892 |           3.1779 |
[32m[20230113 19:49:26 @agent_ppo2.py:186][0m |          -0.0068 |           2.9082 |           3.1775 |
[32m[20230113 19:49:26 @agent_ppo2.py:186][0m |          -0.0080 |           2.8455 |           3.1782 |
[32m[20230113 19:49:26 @agent_ppo2.py:186][0m |          -0.0085 |           2.7899 |           3.1789 |
[32m[20230113 19:49:26 @agent_ppo2.py:186][0m |          -0.0087 |           2.7336 |           3.1797 |
[32m[20230113 19:49:26 @agent_ppo2.py:186][0m |          -0.0092 |           2.6842 |           3.1812 |
[32m[20230113 19:49:26 @agent_ppo2.py:186][0m |          -0.0101 |           2.6503 |           3.1824 |
[32m[20230113 19:49:26 @agent_ppo2.py:186][0m |          -0.0105 |           2.6061 |           3.1821 |
[32m[20230113 19:49:26 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 193.87
[32m[20230113 19:49:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 198.18
[32m[20230113 19:49:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.22
[32m[20230113 19:49:26 @agent_ppo2.py:144][0m Total time:       4.90 min
[32m[20230113 19:49:26 @agent_ppo2.py:146][0m 428032 total steps have happened
[32m[20230113 19:49:26 @agent_ppo2.py:122][0m #------------------------ Iteration 209 --------------------------#
[32m[20230113 19:49:27 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:27 @agent_ppo2.py:186][0m |          -0.0000 |           3.5984 |           3.1097 |
[32m[20230113 19:49:27 @agent_ppo2.py:186][0m |          -0.0028 |           3.4579 |           3.0993 |
[32m[20230113 19:49:27 @agent_ppo2.py:186][0m |          -0.0044 |           3.3707 |           3.1016 |
[32m[20230113 19:49:27 @agent_ppo2.py:186][0m |          -0.0058 |           3.3026 |           3.0992 |
[32m[20230113 19:49:27 @agent_ppo2.py:186][0m |          -0.0068 |           3.2583 |           3.0971 |
[32m[20230113 19:49:27 @agent_ppo2.py:186][0m |          -0.0078 |           3.2279 |           3.0972 |
[32m[20230113 19:49:27 @agent_ppo2.py:186][0m |          -0.0081 |           3.1894 |           3.0975 |
[32m[20230113 19:49:27 @agent_ppo2.py:186][0m |          -0.0093 |           3.1547 |           3.0960 |
[32m[20230113 19:49:27 @agent_ppo2.py:186][0m |          -0.0097 |           3.1300 |           3.0984 |
[32m[20230113 19:49:27 @agent_ppo2.py:186][0m |          -0.0100 |           3.1059 |           3.0995 |
[32m[20230113 19:49:27 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 205.52
[32m[20230113 19:49:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 210.08
[32m[20230113 19:49:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.37
[32m[20230113 19:49:28 @agent_ppo2.py:144][0m Total time:       4.92 min
[32m[20230113 19:49:28 @agent_ppo2.py:146][0m 430080 total steps have happened
[32m[20230113 19:49:28 @agent_ppo2.py:122][0m #------------------------ Iteration 210 --------------------------#
[32m[20230113 19:49:28 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:28 @agent_ppo2.py:186][0m |          -0.0001 |           2.8968 |           3.1780 |
[32m[20230113 19:49:28 @agent_ppo2.py:186][0m |          -0.0050 |           2.3092 |           3.1758 |
[32m[20230113 19:49:28 @agent_ppo2.py:186][0m |          -0.0073 |           2.1552 |           3.1773 |
[32m[20230113 19:49:28 @agent_ppo2.py:186][0m |          -0.0086 |           2.0578 |           3.1780 |
[32m[20230113 19:49:28 @agent_ppo2.py:186][0m |          -0.0098 |           1.9754 |           3.1801 |
[32m[20230113 19:49:29 @agent_ppo2.py:186][0m |          -0.0113 |           1.8904 |           3.1822 |
[32m[20230113 19:49:29 @agent_ppo2.py:186][0m |          -0.0106 |           1.9434 |           3.1821 |
[32m[20230113 19:49:29 @agent_ppo2.py:186][0m |          -0.0108 |           1.8754 |           3.1832 |
[32m[20230113 19:49:29 @agent_ppo2.py:186][0m |          -0.0124 |           1.8441 |           3.1846 |
[32m[20230113 19:49:29 @agent_ppo2.py:186][0m |          -0.0112 |           1.8657 |           3.1870 |
[32m[20230113 19:49:29 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 213.98
[32m[20230113 19:49:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.61
[32m[20230113 19:49:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.55
[32m[20230113 19:49:29 @agent_ppo2.py:144][0m Total time:       4.94 min
[32m[20230113 19:49:29 @agent_ppo2.py:146][0m 432128 total steps have happened
[32m[20230113 19:49:29 @agent_ppo2.py:122][0m #------------------------ Iteration 211 --------------------------#
[32m[20230113 19:49:30 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:49:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:30 @agent_ppo2.py:186][0m |          -0.0043 |           4.5744 |           3.1834 |
[32m[20230113 19:49:30 @agent_ppo2.py:186][0m |          -0.0046 |           4.3820 |           3.1788 |
[32m[20230113 19:49:30 @agent_ppo2.py:186][0m |          -0.0043 |           4.3125 |           3.1764 |
[32m[20230113 19:49:30 @agent_ppo2.py:186][0m |          -0.0087 |           4.2375 |           3.1745 |
[32m[20230113 19:49:30 @agent_ppo2.py:186][0m |          -0.0113 |           4.1537 |           3.1740 |
[32m[20230113 19:49:30 @agent_ppo2.py:186][0m |          -0.0074 |           4.0902 |           3.1755 |
[32m[20230113 19:49:30 @agent_ppo2.py:186][0m |          -0.0140 |           4.0470 |           3.1750 |
[32m[20230113 19:49:30 @agent_ppo2.py:186][0m |          -0.0123 |           3.9961 |           3.1741 |
[32m[20230113 19:49:30 @agent_ppo2.py:186][0m |          -0.0121 |           3.9610 |           3.1732 |
[32m[20230113 19:49:30 @agent_ppo2.py:186][0m |          -0.0126 |           3.9322 |           3.1743 |
[32m[20230113 19:49:30 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:49:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.46
[32m[20230113 19:49:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.11
[32m[20230113 19:49:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.36
[32m[20230113 19:49:31 @agent_ppo2.py:144][0m Total time:       4.97 min
[32m[20230113 19:49:31 @agent_ppo2.py:146][0m 434176 total steps have happened
[32m[20230113 19:49:31 @agent_ppo2.py:122][0m #------------------------ Iteration 212 --------------------------#
[32m[20230113 19:49:31 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:31 @agent_ppo2.py:186][0m |           0.0003 |           3.5097 |           3.1759 |
[32m[20230113 19:49:31 @agent_ppo2.py:186][0m |          -0.0035 |           3.3446 |           3.1730 |
[32m[20230113 19:49:31 @agent_ppo2.py:186][0m |          -0.0061 |           3.2741 |           3.1700 |
[32m[20230113 19:49:31 @agent_ppo2.py:186][0m |          -0.0072 |           3.2357 |           3.1694 |
[32m[20230113 19:49:31 @agent_ppo2.py:186][0m |          -0.0081 |           3.2121 |           3.1702 |
[32m[20230113 19:49:31 @agent_ppo2.py:186][0m |          -0.0087 |           3.1812 |           3.1691 |
[32m[20230113 19:49:31 @agent_ppo2.py:186][0m |          -0.0094 |           3.1639 |           3.1686 |
[32m[20230113 19:49:31 @agent_ppo2.py:186][0m |          -0.0099 |           3.1537 |           3.1702 |
[32m[20230113 19:49:31 @agent_ppo2.py:186][0m |          -0.0106 |           3.1178 |           3.1711 |
[32m[20230113 19:49:31 @agent_ppo2.py:186][0m |          -0.0108 |           3.1198 |           3.1710 |
[32m[20230113 19:49:31 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 204.67
[32m[20230113 19:49:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 210.96
[32m[20230113 19:49:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.64
[32m[20230113 19:49:32 @agent_ppo2.py:144][0m Total time:       4.99 min
[32m[20230113 19:49:32 @agent_ppo2.py:146][0m 436224 total steps have happened
[32m[20230113 19:49:32 @agent_ppo2.py:122][0m #------------------------ Iteration 213 --------------------------#
[32m[20230113 19:49:32 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:33 @agent_ppo2.py:186][0m |          -0.0001 |           3.8069 |           3.2559 |
[32m[20230113 19:49:33 @agent_ppo2.py:186][0m |          -0.0041 |           3.5845 |           3.2529 |
[32m[20230113 19:49:33 @agent_ppo2.py:186][0m |          -0.0060 |           3.4882 |           3.2522 |
[32m[20230113 19:49:33 @agent_ppo2.py:186][0m |          -0.0068 |           3.4208 |           3.2499 |
[32m[20230113 19:49:33 @agent_ppo2.py:186][0m |          -0.0079 |           3.3529 |           3.2501 |
[32m[20230113 19:49:33 @agent_ppo2.py:186][0m |          -0.0087 |           3.3225 |           3.2493 |
[32m[20230113 19:49:33 @agent_ppo2.py:186][0m |          -0.0090 |           3.2610 |           3.2469 |
[32m[20230113 19:49:33 @agent_ppo2.py:186][0m |          -0.0088 |           3.1865 |           3.2469 |
[32m[20230113 19:49:33 @agent_ppo2.py:186][0m |          -0.0098 |           3.1204 |           3.2457 |
[32m[20230113 19:49:33 @agent_ppo2.py:186][0m |          -0.0103 |           3.0798 |           3.2452 |
[32m[20230113 19:49:33 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 218.62
[32m[20230113 19:49:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.17
[32m[20230113 19:49:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 225.08
[32m[20230113 19:49:33 @agent_ppo2.py:144][0m Total time:       5.01 min
[32m[20230113 19:49:33 @agent_ppo2.py:146][0m 438272 total steps have happened
[32m[20230113 19:49:33 @agent_ppo2.py:122][0m #------------------------ Iteration 214 --------------------------#
[32m[20230113 19:49:34 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:34 @agent_ppo2.py:186][0m |           0.0001 |           3.0211 |           3.2562 |
[32m[20230113 19:49:34 @agent_ppo2.py:186][0m |          -0.0033 |           2.8724 |           3.2501 |
[32m[20230113 19:49:34 @agent_ppo2.py:186][0m |          -0.0047 |           2.8030 |           3.2517 |
[32m[20230113 19:49:34 @agent_ppo2.py:186][0m |          -0.0057 |           2.7655 |           3.2505 |
[32m[20230113 19:49:34 @agent_ppo2.py:186][0m |          -0.0070 |           2.7419 |           3.2516 |
[32m[20230113 19:49:34 @agent_ppo2.py:186][0m |          -0.0075 |           2.6907 |           3.2507 |
[32m[20230113 19:49:34 @agent_ppo2.py:186][0m |          -0.0080 |           2.6797 |           3.2531 |
[32m[20230113 19:49:34 @agent_ppo2.py:186][0m |          -0.0085 |           2.6561 |           3.2516 |
[32m[20230113 19:49:34 @agent_ppo2.py:186][0m |          -0.0087 |           2.6389 |           3.2538 |
[32m[20230113 19:49:34 @agent_ppo2.py:186][0m |          -0.0092 |           2.6141 |           3.2554 |
[32m[20230113 19:49:34 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:49:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.81
[32m[20230113 19:49:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.85
[32m[20230113 19:49:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 160.58
[32m[20230113 19:49:35 @agent_ppo2.py:144][0m Total time:       5.04 min
[32m[20230113 19:49:35 @agent_ppo2.py:146][0m 440320 total steps have happened
[32m[20230113 19:49:35 @agent_ppo2.py:122][0m #------------------------ Iteration 215 --------------------------#
[32m[20230113 19:49:35 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 19:49:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:35 @agent_ppo2.py:186][0m |           0.0065 |           9.4283 |           3.1914 |
[32m[20230113 19:49:35 @agent_ppo2.py:186][0m |          -0.0010 |           4.7058 |           3.1907 |
[32m[20230113 19:49:35 @agent_ppo2.py:186][0m |          -0.0118 |           4.1275 |           3.1910 |
[32m[20230113 19:49:35 @agent_ppo2.py:186][0m |          -0.0132 |           3.8512 |           3.1884 |
[32m[20230113 19:49:35 @agent_ppo2.py:186][0m |          -0.0134 |           3.5399 |           3.1880 |
[32m[20230113 19:49:35 @agent_ppo2.py:186][0m |          -0.0151 |           3.4685 |           3.1891 |
[32m[20230113 19:49:35 @agent_ppo2.py:186][0m |          -0.0140 |           3.2621 |           3.1885 |
[32m[20230113 19:49:35 @agent_ppo2.py:186][0m |          -0.0076 |           3.3050 |           3.1875 |
[32m[20230113 19:49:35 @agent_ppo2.py:186][0m |          -0.0120 |           3.3090 |           3.1875 |
[32m[20230113 19:49:36 @agent_ppo2.py:186][0m |          -0.0094 |           3.1493 |           3.1869 |
[32m[20230113 19:49:36 @agent_ppo2.py:131][0m Policy update time: 0.40 s
[32m[20230113 19:49:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 110.52
[32m[20230113 19:49:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.67
[32m[20230113 19:49:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.82
[32m[20230113 19:49:36 @agent_ppo2.py:144][0m Total time:       5.06 min
[32m[20230113 19:49:36 @agent_ppo2.py:146][0m 442368 total steps have happened
[32m[20230113 19:49:36 @agent_ppo2.py:122][0m #------------------------ Iteration 216 --------------------------#
[32m[20230113 19:49:36 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:37 @agent_ppo2.py:186][0m |           0.0001 |           4.6318 |           3.2218 |
[32m[20230113 19:49:37 @agent_ppo2.py:186][0m |          -0.0039 |           4.2951 |           3.2143 |
[32m[20230113 19:49:37 @agent_ppo2.py:186][0m |          -0.0053 |           4.1740 |           3.2123 |
[32m[20230113 19:49:37 @agent_ppo2.py:186][0m |          -0.0062 |           4.1189 |           3.2102 |
[32m[20230113 19:49:37 @agent_ppo2.py:186][0m |          -0.0074 |           4.0598 |           3.2107 |
[32m[20230113 19:49:37 @agent_ppo2.py:186][0m |          -0.0079 |           4.0211 |           3.2126 |
[32m[20230113 19:49:37 @agent_ppo2.py:186][0m |          -0.0087 |           3.9755 |           3.2119 |
[32m[20230113 19:49:37 @agent_ppo2.py:186][0m |          -0.0091 |           3.9665 |           3.2107 |
[32m[20230113 19:49:37 @agent_ppo2.py:186][0m |          -0.0094 |           3.9417 |           3.2136 |
[32m[20230113 19:49:37 @agent_ppo2.py:186][0m |          -0.0096 |           3.9189 |           3.2130 |
[32m[20230113 19:49:37 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 209.86
[32m[20230113 19:49:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 218.10
[32m[20230113 19:49:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.44
[32m[20230113 19:49:37 @agent_ppo2.py:144][0m Total time:       5.08 min
[32m[20230113 19:49:37 @agent_ppo2.py:146][0m 444416 total steps have happened
[32m[20230113 19:49:37 @agent_ppo2.py:122][0m #------------------------ Iteration 217 --------------------------#
[32m[20230113 19:49:38 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:38 @agent_ppo2.py:186][0m |           0.0001 |           2.4760 |           3.2776 |
[32m[20230113 19:49:38 @agent_ppo2.py:186][0m |          -0.0031 |           2.3146 |           3.2730 |
[32m[20230113 19:49:38 @agent_ppo2.py:186][0m |          -0.0049 |           2.2533 |           3.2720 |
[32m[20230113 19:49:38 @agent_ppo2.py:186][0m |          -0.0063 |           2.2136 |           3.2695 |
[32m[20230113 19:49:38 @agent_ppo2.py:186][0m |          -0.0064 |           2.1825 |           3.2724 |
[32m[20230113 19:49:38 @agent_ppo2.py:186][0m |          -0.0075 |           2.1393 |           3.2710 |
[32m[20230113 19:49:38 @agent_ppo2.py:186][0m |          -0.0082 |           2.1151 |           3.2719 |
[32m[20230113 19:49:38 @agent_ppo2.py:186][0m |          -0.0085 |           2.0961 |           3.2729 |
[32m[20230113 19:49:38 @agent_ppo2.py:186][0m |          -0.0089 |           2.0750 |           3.2723 |
[32m[20230113 19:49:38 @agent_ppo2.py:186][0m |          -0.0096 |           2.0631 |           3.2730 |
[32m[20230113 19:49:38 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 218.38
[32m[20230113 19:49:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.17
[32m[20230113 19:49:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.33
[32m[20230113 19:49:39 @agent_ppo2.py:144][0m Total time:       5.10 min
[32m[20230113 19:49:39 @agent_ppo2.py:146][0m 446464 total steps have happened
[32m[20230113 19:49:39 @agent_ppo2.py:122][0m #------------------------ Iteration 218 --------------------------#
[32m[20230113 19:49:39 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 19:49:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:39 @agent_ppo2.py:186][0m |           0.0002 |          16.5846 |           3.1795 |
[32m[20230113 19:49:39 @agent_ppo2.py:186][0m |          -0.0086 |           7.7144 |           3.1742 |
[32m[20230113 19:49:39 @agent_ppo2.py:186][0m |          -0.0089 |           6.2639 |           3.1727 |
[32m[20230113 19:49:39 @agent_ppo2.py:186][0m |           0.0028 |           5.7459 |           3.1703 |
[32m[20230113 19:49:39 @agent_ppo2.py:186][0m |          -0.0148 |           5.1396 |           3.1670 |
[32m[20230113 19:49:39 @agent_ppo2.py:186][0m |          -0.0117 |           4.8076 |           3.1689 |
[32m[20230113 19:49:39 @agent_ppo2.py:186][0m |          -0.0173 |           4.5696 |           3.1665 |
[32m[20230113 19:49:39 @agent_ppo2.py:186][0m |          -0.0157 |           4.3413 |           3.1655 |
[32m[20230113 19:49:39 @agent_ppo2.py:186][0m |          -0.0131 |           4.3095 |           3.1643 |
[32m[20230113 19:49:39 @agent_ppo2.py:186][0m |          -0.0162 |           4.1300 |           3.1650 |
[32m[20230113 19:49:39 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 19:49:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 82.50
[32m[20230113 19:49:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 182.11
[32m[20230113 19:49:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 95.85
[32m[20230113 19:49:40 @agent_ppo2.py:144][0m Total time:       5.12 min
[32m[20230113 19:49:40 @agent_ppo2.py:146][0m 448512 total steps have happened
[32m[20230113 19:49:40 @agent_ppo2.py:122][0m #------------------------ Iteration 219 --------------------------#
[32m[20230113 19:49:40 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:41 @agent_ppo2.py:186][0m |           0.0031 |           3.7001 |           3.2528 |
[32m[20230113 19:49:41 @agent_ppo2.py:186][0m |          -0.0016 |           3.4543 |           3.2534 |
[32m[20230113 19:49:41 @agent_ppo2.py:186][0m |          -0.0034 |           3.3850 |           3.2523 |
[32m[20230113 19:49:41 @agent_ppo2.py:186][0m |          -0.0034 |           3.3819 |           3.2525 |
[32m[20230113 19:49:41 @agent_ppo2.py:186][0m |          -0.0057 |           3.3130 |           3.2516 |
[32m[20230113 19:49:41 @agent_ppo2.py:186][0m |          -0.0048 |           3.4139 |           3.2531 |
[32m[20230113 19:49:41 @agent_ppo2.py:186][0m |          -0.0097 |           3.2678 |           3.2526 |
[32m[20230113 19:49:41 @agent_ppo2.py:186][0m |          -0.0089 |           3.2425 |           3.2522 |
[32m[20230113 19:49:41 @agent_ppo2.py:186][0m |          -0.0087 |           3.2260 |           3.2525 |
[32m[20230113 19:49:41 @agent_ppo2.py:186][0m |          -0.0073 |           3.3481 |           3.2521 |
[32m[20230113 19:49:41 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.35
[32m[20230113 19:49:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.99
[32m[20230113 19:49:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.86
[32m[20230113 19:49:41 @agent_ppo2.py:144][0m Total time:       5.15 min
[32m[20230113 19:49:41 @agent_ppo2.py:146][0m 450560 total steps have happened
[32m[20230113 19:49:41 @agent_ppo2.py:122][0m #------------------------ Iteration 220 --------------------------#
[32m[20230113 19:49:42 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 19:49:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:42 @agent_ppo2.py:186][0m |          -0.0051 |           8.2003 |           3.1545 |
[32m[20230113 19:49:42 @agent_ppo2.py:186][0m |          -0.0089 |           5.6235 |           3.1505 |
[32m[20230113 19:49:42 @agent_ppo2.py:186][0m |          -0.0117 |           5.1094 |           3.1488 |
[32m[20230113 19:49:42 @agent_ppo2.py:186][0m |          -0.0155 |           4.9131 |           3.1488 |
[32m[20230113 19:49:42 @agent_ppo2.py:186][0m |          -0.0146 |           5.0038 |           3.1479 |
[32m[20230113 19:49:42 @agent_ppo2.py:186][0m |          -0.0165 |           4.6551 |           3.1485 |
[32m[20230113 19:49:42 @agent_ppo2.py:186][0m |          -0.0191 |           4.6226 |           3.1462 |
[32m[20230113 19:49:42 @agent_ppo2.py:186][0m |          -0.0130 |           4.7824 |           3.1462 |
[32m[20230113 19:49:42 @agent_ppo2.py:186][0m |          -0.0006 |           4.4094 |           3.1439 |
[32m[20230113 19:49:42 @agent_ppo2.py:186][0m |          -0.0158 |           4.3858 |           3.1438 |
[32m[20230113 19:49:42 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 19:49:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 122.00
[32m[20230113 19:49:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.37
[32m[20230113 19:49:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.95
[32m[20230113 19:49:43 @agent_ppo2.py:144][0m Total time:       5.17 min
[32m[20230113 19:49:43 @agent_ppo2.py:146][0m 452608 total steps have happened
[32m[20230113 19:49:43 @agent_ppo2.py:122][0m #------------------------ Iteration 221 --------------------------#
[32m[20230113 19:49:43 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:49:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:43 @agent_ppo2.py:186][0m |           0.0005 |           5.4516 |           3.2855 |
[32m[20230113 19:49:43 @agent_ppo2.py:186][0m |          -0.0018 |           5.0943 |           3.2852 |
[32m[20230113 19:49:43 @agent_ppo2.py:186][0m |          -0.0031 |           4.9293 |           3.2822 |
[32m[20230113 19:49:43 @agent_ppo2.py:186][0m |          -0.0037 |           4.7761 |           3.2804 |
[32m[20230113 19:49:43 @agent_ppo2.py:186][0m |          -0.0047 |           4.6578 |           3.2812 |
[32m[20230113 19:49:43 @agent_ppo2.py:186][0m |          -0.0054 |           4.5620 |           3.2798 |
[32m[20230113 19:49:43 @agent_ppo2.py:186][0m |          -0.0059 |           4.4529 |           3.2789 |
[32m[20230113 19:49:44 @agent_ppo2.py:186][0m |          -0.0064 |           4.4057 |           3.2807 |
[32m[20230113 19:49:44 @agent_ppo2.py:186][0m |          -0.0070 |           4.3112 |           3.2799 |
[32m[20230113 19:49:44 @agent_ppo2.py:186][0m |          -0.0072 |           4.2517 |           3.2800 |
[32m[20230113 19:49:44 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.99
[32m[20230113 19:49:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.18
[32m[20230113 19:49:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.63
[32m[20230113 19:49:44 @agent_ppo2.py:144][0m Total time:       5.19 min
[32m[20230113 19:49:44 @agent_ppo2.py:146][0m 454656 total steps have happened
[32m[20230113 19:49:44 @agent_ppo2.py:122][0m #------------------------ Iteration 222 --------------------------#
[32m[20230113 19:49:45 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:45 @agent_ppo2.py:186][0m |          -0.0006 |           3.6293 |           3.3030 |
[32m[20230113 19:49:45 @agent_ppo2.py:186][0m |          -0.0051 |           3.3127 |           3.2973 |
[32m[20230113 19:49:45 @agent_ppo2.py:186][0m |          -0.0068 |           3.1565 |           3.2948 |
[32m[20230113 19:49:45 @agent_ppo2.py:186][0m |          -0.0081 |           3.0748 |           3.2968 |
[32m[20230113 19:49:45 @agent_ppo2.py:186][0m |          -0.0090 |           3.0108 |           3.2972 |
[32m[20230113 19:49:45 @agent_ppo2.py:186][0m |          -0.0100 |           2.9713 |           3.3005 |
[32m[20230113 19:49:45 @agent_ppo2.py:186][0m |          -0.0099 |           2.9375 |           3.2998 |
[32m[20230113 19:49:45 @agent_ppo2.py:186][0m |          -0.0109 |           2.8965 |           3.3032 |
[32m[20230113 19:49:45 @agent_ppo2.py:186][0m |          -0.0115 |           2.8699 |           3.3043 |
[32m[20230113 19:49:45 @agent_ppo2.py:186][0m |          -0.0120 |           2.8428 |           3.3053 |
[32m[20230113 19:49:45 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 198.55
[32m[20230113 19:49:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 207.37
[32m[20230113 19:49:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.62
[32m[20230113 19:49:45 @agent_ppo2.py:144][0m Total time:       5.22 min
[32m[20230113 19:49:45 @agent_ppo2.py:146][0m 456704 total steps have happened
[32m[20230113 19:49:45 @agent_ppo2.py:122][0m #------------------------ Iteration 223 --------------------------#
[32m[20230113 19:49:46 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:46 @agent_ppo2.py:186][0m |           0.0000 |           4.0794 |           3.2645 |
[32m[20230113 19:49:46 @agent_ppo2.py:186][0m |          -0.0041 |           3.9356 |           3.2624 |
[32m[20230113 19:49:46 @agent_ppo2.py:186][0m |          -0.0059 |           3.8699 |           3.2615 |
[32m[20230113 19:49:46 @agent_ppo2.py:186][0m |          -0.0074 |           3.8163 |           3.2642 |
[32m[20230113 19:49:46 @agent_ppo2.py:186][0m |          -0.0084 |           3.7918 |           3.2627 |
[32m[20230113 19:49:46 @agent_ppo2.py:186][0m |          -0.0092 |           3.7454 |           3.2651 |
[32m[20230113 19:49:46 @agent_ppo2.py:186][0m |          -0.0096 |           3.7250 |           3.2660 |
[32m[20230113 19:49:46 @agent_ppo2.py:186][0m |          -0.0103 |           3.6982 |           3.2656 |
[32m[20230113 19:49:46 @agent_ppo2.py:186][0m |          -0.0104 |           3.6778 |           3.2668 |
[32m[20230113 19:49:46 @agent_ppo2.py:186][0m |          -0.0112 |           3.6607 |           3.2663 |
[32m[20230113 19:49:46 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 205.75
[32m[20230113 19:49:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.16
[32m[20230113 19:49:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.21
[32m[20230113 19:49:47 @agent_ppo2.py:144][0m Total time:       5.24 min
[32m[20230113 19:49:47 @agent_ppo2.py:146][0m 458752 total steps have happened
[32m[20230113 19:49:47 @agent_ppo2.py:122][0m #------------------------ Iteration 224 --------------------------#
[32m[20230113 19:49:47 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:49:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:47 @agent_ppo2.py:186][0m |          -0.0064 |          64.1705 |           3.2584 |
[32m[20230113 19:49:48 @agent_ppo2.py:186][0m |          -0.0140 |          34.4461 |           3.2504 |
[32m[20230113 19:49:48 @agent_ppo2.py:186][0m |          -0.0178 |          27.0596 |           3.2482 |
[32m[20230113 19:49:48 @agent_ppo2.py:186][0m |          -0.0161 |          22.5587 |           3.2452 |
[32m[20230113 19:49:48 @agent_ppo2.py:186][0m |          -0.0186 |          19.9732 |           3.2443 |
[32m[20230113 19:49:48 @agent_ppo2.py:186][0m |           0.0103 |          18.9397 |           3.2429 |
[32m[20230113 19:49:48 @agent_ppo2.py:186][0m |          -0.0217 |          17.1279 |           3.2408 |
[32m[20230113 19:49:48 @agent_ppo2.py:186][0m |          -0.0138 |          16.2559 |           3.2418 |
[32m[20230113 19:49:48 @agent_ppo2.py:186][0m |          -0.0058 |          14.9002 |           3.2381 |
[32m[20230113 19:49:48 @agent_ppo2.py:186][0m |          -0.0098 |          14.8273 |           3.2373 |
[32m[20230113 19:49:48 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:49:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: -19.20
[32m[20230113 19:49:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 57.98
[32m[20230113 19:49:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.23
[32m[20230113 19:49:48 @agent_ppo2.py:144][0m Total time:       5.26 min
[32m[20230113 19:49:48 @agent_ppo2.py:146][0m 460800 total steps have happened
[32m[20230113 19:49:48 @agent_ppo2.py:122][0m #------------------------ Iteration 225 --------------------------#
[32m[20230113 19:49:49 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:49 @agent_ppo2.py:186][0m |           0.0007 |           4.9540 |           3.2817 |
[32m[20230113 19:49:49 @agent_ppo2.py:186][0m |          -0.0022 |           3.8016 |           3.2787 |
[32m[20230113 19:49:49 @agent_ppo2.py:186][0m |          -0.0044 |           3.5969 |           3.2786 |
[32m[20230113 19:49:49 @agent_ppo2.py:186][0m |          -0.0049 |           3.4546 |           3.2774 |
[32m[20230113 19:49:49 @agent_ppo2.py:186][0m |          -0.0052 |           3.3909 |           3.2795 |
[32m[20230113 19:49:49 @agent_ppo2.py:186][0m |          -0.0065 |           3.3350 |           3.2777 |
[32m[20230113 19:49:49 @agent_ppo2.py:186][0m |          -0.0074 |           3.2913 |           3.2798 |
[32m[20230113 19:49:49 @agent_ppo2.py:186][0m |          -0.0078 |           3.2669 |           3.2784 |
[32m[20230113 19:49:49 @agent_ppo2.py:186][0m |          -0.0077 |           3.2352 |           3.2793 |
[32m[20230113 19:49:49 @agent_ppo2.py:186][0m |          -0.0086 |           3.2150 |           3.2798 |
[32m[20230113 19:49:49 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 215.15
[32m[20230113 19:49:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 215.37
[32m[20230113 19:49:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.63
[32m[20230113 19:49:50 @agent_ppo2.py:144][0m Total time:       5.29 min
[32m[20230113 19:49:50 @agent_ppo2.py:146][0m 462848 total steps have happened
[32m[20230113 19:49:50 @agent_ppo2.py:122][0m #------------------------ Iteration 226 --------------------------#
[32m[20230113 19:49:50 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:50 @agent_ppo2.py:186][0m |           0.0010 |           3.9711 |           3.2846 |
[32m[20230113 19:49:50 @agent_ppo2.py:186][0m |          -0.0007 |           3.6066 |           3.2830 |
[32m[20230113 19:49:50 @agent_ppo2.py:186][0m |          -0.0026 |           3.4665 |           3.2825 |
[32m[20230113 19:49:50 @agent_ppo2.py:186][0m |          -0.0037 |           3.3867 |           3.2835 |
[32m[20230113 19:49:50 @agent_ppo2.py:186][0m |          -0.0046 |           3.3376 |           3.2826 |
[32m[20230113 19:49:51 @agent_ppo2.py:186][0m |          -0.0050 |           3.2951 |           3.2825 |
[32m[20230113 19:49:51 @agent_ppo2.py:186][0m |          -0.0054 |           3.2560 |           3.2826 |
[32m[20230113 19:49:51 @agent_ppo2.py:186][0m |          -0.0057 |           3.2282 |           3.2843 |
[32m[20230113 19:49:51 @agent_ppo2.py:186][0m |          -0.0065 |           3.1970 |           3.2835 |
[32m[20230113 19:49:51 @agent_ppo2.py:186][0m |          -0.0066 |           3.1740 |           3.2846 |
[32m[20230113 19:49:51 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 212.86
[32m[20230113 19:49:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.50
[32m[20230113 19:49:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.47
[32m[20230113 19:49:51 @agent_ppo2.py:144][0m Total time:       5.31 min
[32m[20230113 19:49:51 @agent_ppo2.py:146][0m 464896 total steps have happened
[32m[20230113 19:49:51 @agent_ppo2.py:122][0m #------------------------ Iteration 227 --------------------------#
[32m[20230113 19:49:52 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:49:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:52 @agent_ppo2.py:186][0m |           0.0182 |          14.8158 |           3.1754 |
[32m[20230113 19:49:52 @agent_ppo2.py:186][0m |          -0.0060 |           5.7948 |           3.1759 |
[32m[20230113 19:49:52 @agent_ppo2.py:186][0m |          -0.0091 |           5.1120 |           3.1758 |
[32m[20230113 19:49:52 @agent_ppo2.py:186][0m |          -0.0048 |           4.6689 |           3.1750 |
[32m[20230113 19:49:52 @agent_ppo2.py:186][0m |          -0.0080 |           4.4547 |           3.1729 |
[32m[20230113 19:49:52 @agent_ppo2.py:186][0m |          -0.0091 |           4.3367 |           3.1728 |
[32m[20230113 19:49:52 @agent_ppo2.py:186][0m |          -0.0144 |           4.2411 |           3.1714 |
[32m[20230113 19:49:52 @agent_ppo2.py:186][0m |          -0.0100 |           4.1708 |           3.1697 |
[32m[20230113 19:49:52 @agent_ppo2.py:186][0m |          -0.0060 |           4.1319 |           3.1704 |
[32m[20230113 19:49:52 @agent_ppo2.py:186][0m |          -0.0116 |           4.0460 |           3.1692 |
[32m[20230113 19:49:52 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:49:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 146.91
[32m[20230113 19:49:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.11
[32m[20230113 19:49:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.02
[32m[20230113 19:49:53 @agent_ppo2.py:144][0m Total time:       5.33 min
[32m[20230113 19:49:53 @agent_ppo2.py:146][0m 466944 total steps have happened
[32m[20230113 19:49:53 @agent_ppo2.py:122][0m #------------------------ Iteration 228 --------------------------#
[32m[20230113 19:49:53 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:53 @agent_ppo2.py:186][0m |          -0.0008 |           4.8313 |           3.3005 |
[32m[20230113 19:49:53 @agent_ppo2.py:186][0m |          -0.0039 |           4.0092 |           3.2955 |
[32m[20230113 19:49:53 @agent_ppo2.py:186][0m |          -0.0067 |           3.7703 |           3.2936 |
[32m[20230113 19:49:53 @agent_ppo2.py:186][0m |          -0.0087 |           3.6315 |           3.2924 |
[32m[20230113 19:49:53 @agent_ppo2.py:186][0m |          -0.0092 |           3.5661 |           3.2915 |
[32m[20230113 19:49:53 @agent_ppo2.py:186][0m |          -0.0103 |           3.5029 |           3.2907 |
[32m[20230113 19:49:53 @agent_ppo2.py:186][0m |          -0.0102 |           3.4624 |           3.2921 |
[32m[20230113 19:49:53 @agent_ppo2.py:186][0m |          -0.0110 |           3.4304 |           3.2923 |
[32m[20230113 19:49:53 @agent_ppo2.py:186][0m |          -0.0113 |           3.4142 |           3.2918 |
[32m[20230113 19:49:53 @agent_ppo2.py:186][0m |          -0.0116 |           3.3634 |           3.2911 |
[32m[20230113 19:49:53 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 213.05
[32m[20230113 19:49:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.06
[32m[20230113 19:49:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.07
[32m[20230113 19:49:54 @agent_ppo2.py:144][0m Total time:       5.36 min
[32m[20230113 19:49:54 @agent_ppo2.py:146][0m 468992 total steps have happened
[32m[20230113 19:49:54 @agent_ppo2.py:122][0m #------------------------ Iteration 229 --------------------------#
[32m[20230113 19:49:54 @agent_ppo2.py:128][0m Sampling time: 0.31 s by 1 slaves
[32m[20230113 19:49:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:54 @agent_ppo2.py:186][0m |          -0.0039 |          19.2236 |           3.2556 |
[32m[20230113 19:49:54 @agent_ppo2.py:186][0m |           0.0029 |           8.3466 |           3.2522 |
[32m[20230113 19:49:54 @agent_ppo2.py:186][0m |          -0.0115 |           7.1768 |           3.2479 |
[32m[20230113 19:49:54 @agent_ppo2.py:186][0m |          -0.0107 |           7.1219 |           3.2463 |
[32m[20230113 19:49:54 @agent_ppo2.py:186][0m |          -0.0144 |           6.5947 |           3.2448 |
[32m[20230113 19:49:54 @agent_ppo2.py:186][0m |          -0.0176 |           6.3570 |           3.2446 |
[32m[20230113 19:49:54 @agent_ppo2.py:186][0m |          -0.0156 |           6.4409 |           3.2416 |
[32m[20230113 19:49:55 @agent_ppo2.py:186][0m |          -0.0157 |           6.1262 |           3.2418 |
[32m[20230113 19:49:55 @agent_ppo2.py:186][0m |          -0.0192 |           6.0471 |           3.2399 |
[32m[20230113 19:49:55 @agent_ppo2.py:186][0m |          -0.0196 |           5.8420 |           3.2385 |
[32m[20230113 19:49:55 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 19:49:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 92.39
[32m[20230113 19:49:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.80
[32m[20230113 19:49:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.20
[32m[20230113 19:49:55 @agent_ppo2.py:144][0m Total time:       5.37 min
[32m[20230113 19:49:55 @agent_ppo2.py:146][0m 471040 total steps have happened
[32m[20230113 19:49:55 @agent_ppo2.py:122][0m #------------------------ Iteration 230 --------------------------#
[32m[20230113 19:49:55 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:49:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:56 @agent_ppo2.py:186][0m |           0.0001 |           7.4257 |           3.3274 |
[32m[20230113 19:49:56 @agent_ppo2.py:186][0m |          -0.0045 |           5.4887 |           3.3249 |
[32m[20230113 19:49:56 @agent_ppo2.py:186][0m |          -0.0074 |           5.2723 |           3.3235 |
[32m[20230113 19:49:56 @agent_ppo2.py:186][0m |          -0.0086 |           5.0408 |           3.3228 |
[32m[20230113 19:49:56 @agent_ppo2.py:186][0m |          -0.0099 |           4.9189 |           3.3221 |
[32m[20230113 19:49:56 @agent_ppo2.py:186][0m |          -0.0107 |           4.8655 |           3.3231 |
[32m[20230113 19:49:56 @agent_ppo2.py:186][0m |          -0.0114 |           4.7479 |           3.3227 |
[32m[20230113 19:49:56 @agent_ppo2.py:186][0m |          -0.0113 |           4.6552 |           3.3223 |
[32m[20230113 19:49:56 @agent_ppo2.py:186][0m |          -0.0120 |           4.6106 |           3.3228 |
[32m[20230113 19:49:56 @agent_ppo2.py:186][0m |          -0.0122 |           4.5785 |           3.3232 |
[32m[20230113 19:49:56 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 210.16
[32m[20230113 19:49:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 218.46
[32m[20230113 19:49:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.31
[32m[20230113 19:49:56 @agent_ppo2.py:144][0m Total time:       5.40 min
[32m[20230113 19:49:56 @agent_ppo2.py:146][0m 473088 total steps have happened
[32m[20230113 19:49:56 @agent_ppo2.py:122][0m #------------------------ Iteration 231 --------------------------#
[32m[20230113 19:49:57 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:49:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:57 @agent_ppo2.py:186][0m |          -0.0002 |           4.6068 |           3.3061 |
[32m[20230113 19:49:57 @agent_ppo2.py:186][0m |          -0.0050 |           4.2455 |           3.2993 |
[32m[20230113 19:49:57 @agent_ppo2.py:186][0m |          -0.0058 |           4.2122 |           3.2973 |
[32m[20230113 19:49:57 @agent_ppo2.py:186][0m |          -0.0075 |           4.1033 |           3.2972 |
[32m[20230113 19:49:57 @agent_ppo2.py:186][0m |          -0.0084 |           4.0944 |           3.2977 |
[32m[20230113 19:49:57 @agent_ppo2.py:186][0m |          -0.0097 |           4.0404 |           3.2989 |
[32m[20230113 19:49:57 @agent_ppo2.py:186][0m |          -0.0102 |           3.9991 |           3.3000 |
[32m[20230113 19:49:57 @agent_ppo2.py:186][0m |          -0.0095 |           3.9866 |           3.3013 |
[32m[20230113 19:49:57 @agent_ppo2.py:186][0m |          -0.0109 |           3.9608 |           3.2990 |
[32m[20230113 19:49:57 @agent_ppo2.py:186][0m |          -0.0116 |           3.9453 |           3.2998 |
[32m[20230113 19:49:57 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:49:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 209.52
[32m[20230113 19:49:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.76
[32m[20230113 19:49:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.77
[32m[20230113 19:49:58 @agent_ppo2.py:144][0m Total time:       5.42 min
[32m[20230113 19:49:58 @agent_ppo2.py:146][0m 475136 total steps have happened
[32m[20230113 19:49:58 @agent_ppo2.py:122][0m #------------------------ Iteration 232 --------------------------#
[32m[20230113 19:49:58 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:49:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:49:58 @agent_ppo2.py:186][0m |           0.0029 |          24.2935 |           3.3006 |
[32m[20230113 19:49:59 @agent_ppo2.py:186][0m |          -0.0007 |          10.8596 |           3.2955 |
[32m[20230113 19:49:59 @agent_ppo2.py:186][0m |          -0.0019 |           8.4369 |           3.3000 |
[32m[20230113 19:49:59 @agent_ppo2.py:186][0m |          -0.0072 |           7.4280 |           3.2973 |
[32m[20230113 19:49:59 @agent_ppo2.py:186][0m |          -0.0083 |           6.7097 |           3.2956 |
[32m[20230113 19:49:59 @agent_ppo2.py:186][0m |          -0.0054 |           6.4252 |           3.2995 |
[32m[20230113 19:49:59 @agent_ppo2.py:186][0m |          -0.0096 |           6.0180 |           3.2989 |
[32m[20230113 19:49:59 @agent_ppo2.py:186][0m |          -0.0106 |           5.8011 |           3.2980 |
[32m[20230113 19:49:59 @agent_ppo2.py:186][0m |          -0.0089 |           5.6932 |           3.3001 |
[32m[20230113 19:49:59 @agent_ppo2.py:186][0m |          -0.0123 |           5.3692 |           3.2999 |
[32m[20230113 19:49:59 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 19:49:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 132.78
[32m[20230113 19:49:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.55
[32m[20230113 19:49:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.72
[32m[20230113 19:49:59 @agent_ppo2.py:144][0m Total time:       5.45 min
[32m[20230113 19:49:59 @agent_ppo2.py:146][0m 477184 total steps have happened
[32m[20230113 19:49:59 @agent_ppo2.py:122][0m #------------------------ Iteration 233 --------------------------#
[32m[20230113 19:50:00 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 19:50:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:00 @agent_ppo2.py:186][0m |           0.0004 |          40.6944 |           3.3459 |
[32m[20230113 19:50:00 @agent_ppo2.py:186][0m |          -0.0054 |          22.7984 |           3.3419 |
[32m[20230113 19:50:00 @agent_ppo2.py:186][0m |          -0.0055 |          18.4207 |           3.3403 |
[32m[20230113 19:50:00 @agent_ppo2.py:186][0m |          -0.0081 |          16.5124 |           3.3402 |
[32m[20230113 19:50:00 @agent_ppo2.py:186][0m |          -0.0093 |          15.2406 |           3.3374 |
[32m[20230113 19:50:00 @agent_ppo2.py:186][0m |          -0.0112 |          14.1241 |           3.3359 |
[32m[20230113 19:50:00 @agent_ppo2.py:186][0m |          -0.0113 |          14.1766 |           3.3362 |
[32m[20230113 19:50:00 @agent_ppo2.py:186][0m |          -0.0121 |          12.6999 |           3.3350 |
[32m[20230113 19:50:00 @agent_ppo2.py:186][0m |          -0.0129 |          12.4167 |           3.3326 |
[32m[20230113 19:50:00 @agent_ppo2.py:186][0m |          -0.0136 |          11.6190 |           3.3327 |
[32m[20230113 19:50:00 @agent_ppo2.py:131][0m Policy update time: 0.40 s
[32m[20230113 19:50:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 42.24
[32m[20230113 19:50:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.91
[32m[20230113 19:50:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.01
[32m[20230113 19:50:01 @agent_ppo2.py:144][0m Total time:       5.47 min
[32m[20230113 19:50:01 @agent_ppo2.py:146][0m 479232 total steps have happened
[32m[20230113 19:50:01 @agent_ppo2.py:122][0m #------------------------ Iteration 234 --------------------------#
[32m[20230113 19:50:01 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:01 @agent_ppo2.py:186][0m |          -0.0029 |           8.3612 |           3.2830 |
[32m[20230113 19:50:01 @agent_ppo2.py:186][0m |          -0.0023 |           7.1958 |           3.2820 |
[32m[20230113 19:50:01 @agent_ppo2.py:186][0m |          -0.0041 |           7.0360 |           3.2836 |
[32m[20230113 19:50:01 @agent_ppo2.py:186][0m |          -0.0044 |           6.8462 |           3.2850 |
[32m[20230113 19:50:01 @agent_ppo2.py:186][0m |          -0.0076 |           6.7551 |           3.2835 |
[32m[20230113 19:50:01 @agent_ppo2.py:186][0m |          -0.0060 |           6.7287 |           3.2862 |
[32m[20230113 19:50:01 @agent_ppo2.py:186][0m |          -0.0077 |           6.5989 |           3.2877 |
[32m[20230113 19:50:01 @agent_ppo2.py:186][0m |          -0.0076 |           6.5927 |           3.2882 |
[32m[20230113 19:50:01 @agent_ppo2.py:186][0m |          -0.0084 |           6.5551 |           3.2904 |
[32m[20230113 19:50:02 @agent_ppo2.py:186][0m |          -0.0083 |           6.4877 |           3.2923 |
[32m[20230113 19:50:02 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.39
[32m[20230113 19:50:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.26
[32m[20230113 19:50:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 229.38
[32m[20230113 19:50:02 @agent_ppo2.py:144][0m Total time:       5.49 min
[32m[20230113 19:50:02 @agent_ppo2.py:146][0m 481280 total steps have happened
[32m[20230113 19:50:02 @agent_ppo2.py:122][0m #------------------------ Iteration 235 --------------------------#
[32m[20230113 19:50:02 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:03 @agent_ppo2.py:186][0m |           0.0009 |           5.3045 |           3.3696 |
[32m[20230113 19:50:03 @agent_ppo2.py:186][0m |          -0.0032 |           5.0037 |           3.3688 |
[32m[20230113 19:50:03 @agent_ppo2.py:186][0m |          -0.0057 |           4.9295 |           3.3663 |
[32m[20230113 19:50:03 @agent_ppo2.py:186][0m |          -0.0048 |           4.8898 |           3.3653 |
[32m[20230113 19:50:03 @agent_ppo2.py:186][0m |          -0.0079 |           4.8100 |           3.3641 |
[32m[20230113 19:50:03 @agent_ppo2.py:186][0m |          -0.0093 |           4.7159 |           3.3635 |
[32m[20230113 19:50:03 @agent_ppo2.py:186][0m |          -0.0104 |           4.6910 |           3.3633 |
[32m[20230113 19:50:03 @agent_ppo2.py:186][0m |          -0.0108 |           4.6458 |           3.3631 |
[32m[20230113 19:50:03 @agent_ppo2.py:186][0m |          -0.0103 |           4.6649 |           3.3619 |
[32m[20230113 19:50:03 @agent_ppo2.py:186][0m |          -0.0116 |           4.5827 |           3.3608 |
[32m[20230113 19:50:03 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 207.95
[32m[20230113 19:50:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.29
[32m[20230113 19:50:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.67
[32m[20230113 19:50:03 @agent_ppo2.py:144][0m Total time:       5.51 min
[32m[20230113 19:50:03 @agent_ppo2.py:146][0m 483328 total steps have happened
[32m[20230113 19:50:03 @agent_ppo2.py:122][0m #------------------------ Iteration 236 --------------------------#
[32m[20230113 19:50:04 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 19:50:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:04 @agent_ppo2.py:186][0m |          -0.0004 |          21.8033 |           3.2738 |
[32m[20230113 19:50:04 @agent_ppo2.py:186][0m |          -0.0036 |          14.1292 |           3.2723 |
[32m[20230113 19:50:04 @agent_ppo2.py:186][0m |          -0.0012 |          11.9754 |           3.2700 |
[32m[20230113 19:50:04 @agent_ppo2.py:186][0m |          -0.0090 |          11.0119 |           3.2683 |
[32m[20230113 19:50:04 @agent_ppo2.py:186][0m |          -0.0055 |          10.8293 |           3.2677 |
[32m[20230113 19:50:04 @agent_ppo2.py:186][0m |          -0.0105 |           9.8951 |           3.2666 |
[32m[20230113 19:50:04 @agent_ppo2.py:186][0m |          -0.0097 |           9.5922 |           3.2641 |
[32m[20230113 19:50:04 @agent_ppo2.py:186][0m |          -0.0140 |           8.8443 |           3.2646 |
[32m[20230113 19:50:04 @agent_ppo2.py:186][0m |          -0.0085 |           8.5849 |           3.2634 |
[32m[20230113 19:50:04 @agent_ppo2.py:186][0m |          -0.0123 |           8.3118 |           3.2631 |
[32m[20230113 19:50:04 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 19:50:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 81.46
[32m[20230113 19:50:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 212.39
[32m[20230113 19:50:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.35
[32m[20230113 19:50:04 @agent_ppo2.py:144][0m Total time:       5.53 min
[32m[20230113 19:50:04 @agent_ppo2.py:146][0m 485376 total steps have happened
[32m[20230113 19:50:04 @agent_ppo2.py:122][0m #------------------------ Iteration 237 --------------------------#
[32m[20230113 19:50:05 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:05 @agent_ppo2.py:186][0m |           0.0004 |           4.8196 |           3.3545 |
[32m[20230113 19:50:05 @agent_ppo2.py:186][0m |          -0.0031 |           4.1237 |           3.3517 |
[32m[20230113 19:50:05 @agent_ppo2.py:186][0m |          -0.0051 |           3.9773 |           3.3493 |
[32m[20230113 19:50:05 @agent_ppo2.py:186][0m |          -0.0067 |           3.8913 |           3.3479 |
[32m[20230113 19:50:05 @agent_ppo2.py:186][0m |          -0.0084 |           3.8414 |           3.3476 |
[32m[20230113 19:50:05 @agent_ppo2.py:186][0m |          -0.0090 |           3.7953 |           3.3468 |
[32m[20230113 19:50:05 @agent_ppo2.py:186][0m |          -0.0096 |           3.7519 |           3.3458 |
[32m[20230113 19:50:05 @agent_ppo2.py:186][0m |          -0.0103 |           3.7083 |           3.3446 |
[32m[20230113 19:50:05 @agent_ppo2.py:186][0m |          -0.0106 |           3.6709 |           3.3452 |
[32m[20230113 19:50:05 @agent_ppo2.py:186][0m |          -0.0111 |           3.6390 |           3.3447 |
[32m[20230113 19:50:05 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.51
[32m[20230113 19:50:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 216.84
[32m[20230113 19:50:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 154.80
[32m[20230113 19:50:06 @agent_ppo2.py:144][0m Total time:       5.55 min
[32m[20230113 19:50:06 @agent_ppo2.py:146][0m 487424 total steps have happened
[32m[20230113 19:50:06 @agent_ppo2.py:122][0m #------------------------ Iteration 238 --------------------------#
[32m[20230113 19:50:06 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:06 @agent_ppo2.py:186][0m |          -0.0015 |           4.7523 |           3.3883 |
[32m[20230113 19:50:06 @agent_ppo2.py:186][0m |          -0.0073 |           4.4986 |           3.3834 |
[32m[20230113 19:50:06 @agent_ppo2.py:186][0m |          -0.0097 |           4.3991 |           3.3808 |
[32m[20230113 19:50:07 @agent_ppo2.py:186][0m |          -0.0099 |           4.3196 |           3.3785 |
[32m[20230113 19:50:07 @agent_ppo2.py:186][0m |          -0.0102 |           4.2485 |           3.3783 |
[32m[20230113 19:50:07 @agent_ppo2.py:186][0m |          -0.0113 |           4.2032 |           3.3762 |
[32m[20230113 19:50:07 @agent_ppo2.py:186][0m |          -0.0118 |           4.1717 |           3.3766 |
[32m[20230113 19:50:07 @agent_ppo2.py:186][0m |          -0.0122 |           4.1261 |           3.3762 |
[32m[20230113 19:50:07 @agent_ppo2.py:186][0m |          -0.0121 |           4.0914 |           3.3762 |
[32m[20230113 19:50:07 @agent_ppo2.py:186][0m |          -0.0121 |           4.0617 |           3.3767 |
[32m[20230113 19:50:07 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.32
[32m[20230113 19:50:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.58
[32m[20230113 19:50:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.68
[32m[20230113 19:50:07 @agent_ppo2.py:144][0m Total time:       5.58 min
[32m[20230113 19:50:07 @agent_ppo2.py:146][0m 489472 total steps have happened
[32m[20230113 19:50:07 @agent_ppo2.py:122][0m #------------------------ Iteration 239 --------------------------#
[32m[20230113 19:50:08 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:50:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:08 @agent_ppo2.py:186][0m |          -0.0002 |          12.0602 |           3.3473 |
[32m[20230113 19:50:08 @agent_ppo2.py:186][0m |          -0.0047 |           8.6379 |           3.3397 |
[32m[20230113 19:50:08 @agent_ppo2.py:186][0m |          -0.0065 |           7.8670 |           3.3320 |
[32m[20230113 19:50:08 @agent_ppo2.py:186][0m |          -0.0075 |           7.3942 |           3.3348 |
[32m[20230113 19:50:08 @agent_ppo2.py:186][0m |          -0.0092 |           7.4234 |           3.3319 |
[32m[20230113 19:50:08 @agent_ppo2.py:186][0m |          -0.0098 |           7.0312 |           3.3307 |
[32m[20230113 19:50:08 @agent_ppo2.py:186][0m |          -0.0103 |           6.9057 |           3.3305 |
[32m[20230113 19:50:08 @agent_ppo2.py:186][0m |          -0.0107 |           6.7711 |           3.3279 |
[32m[20230113 19:50:08 @agent_ppo2.py:186][0m |          -0.0112 |           6.5643 |           3.3260 |
[32m[20230113 19:50:08 @agent_ppo2.py:186][0m |          -0.0110 |           6.6544 |           3.3286 |
[32m[20230113 19:50:08 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:50:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 116.94
[32m[20230113 19:50:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.81
[32m[20230113 19:50:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.56
[32m[20230113 19:50:09 @agent_ppo2.py:144][0m Total time:       5.60 min
[32m[20230113 19:50:09 @agent_ppo2.py:146][0m 491520 total steps have happened
[32m[20230113 19:50:09 @agent_ppo2.py:122][0m #------------------------ Iteration 240 --------------------------#
[32m[20230113 19:50:09 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:09 @agent_ppo2.py:186][0m |          -0.0020 |           3.8454 |           3.3850 |
[32m[20230113 19:50:09 @agent_ppo2.py:186][0m |          -0.0064 |           3.4408 |           3.3816 |
[32m[20230113 19:50:09 @agent_ppo2.py:186][0m |          -0.0078 |           3.2944 |           3.3792 |
[32m[20230113 19:50:09 @agent_ppo2.py:186][0m |          -0.0087 |           3.2047 |           3.3833 |
[32m[20230113 19:50:09 @agent_ppo2.py:186][0m |          -0.0098 |           3.1143 |           3.3809 |
[32m[20230113 19:50:10 @agent_ppo2.py:186][0m |          -0.0107 |           3.0594 |           3.3822 |
[32m[20230113 19:50:10 @agent_ppo2.py:186][0m |          -0.0110 |           3.0020 |           3.3812 |
[32m[20230113 19:50:10 @agent_ppo2.py:186][0m |          -0.0117 |           2.9578 |           3.3822 |
[32m[20230113 19:50:10 @agent_ppo2.py:186][0m |          -0.0118 |           2.9196 |           3.3822 |
[32m[20230113 19:50:10 @agent_ppo2.py:186][0m |          -0.0128 |           2.8811 |           3.3816 |
[32m[20230113 19:50:10 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 211.12
[32m[20230113 19:50:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 211.75
[32m[20230113 19:50:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.95
[32m[20230113 19:50:10 @agent_ppo2.py:144][0m Total time:       5.63 min
[32m[20230113 19:50:10 @agent_ppo2.py:146][0m 493568 total steps have happened
[32m[20230113 19:50:10 @agent_ppo2.py:122][0m #------------------------ Iteration 241 --------------------------#
[32m[20230113 19:50:11 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:11 @agent_ppo2.py:186][0m |          -0.0007 |           3.9022 |           3.3588 |
[32m[20230113 19:50:11 @agent_ppo2.py:186][0m |          -0.0036 |           3.6056 |           3.3555 |
[32m[20230113 19:50:11 @agent_ppo2.py:186][0m |          -0.0055 |           3.5376 |           3.3487 |
[32m[20230113 19:50:11 @agent_ppo2.py:186][0m |          -0.0065 |           3.5003 |           3.3534 |
[32m[20230113 19:50:11 @agent_ppo2.py:186][0m |          -0.0077 |           3.4562 |           3.3506 |
[32m[20230113 19:50:11 @agent_ppo2.py:186][0m |          -0.0082 |           3.4270 |           3.3505 |
[32m[20230113 19:50:11 @agent_ppo2.py:186][0m |          -0.0094 |           3.4030 |           3.3510 |
[32m[20230113 19:50:11 @agent_ppo2.py:186][0m |          -0.0097 |           3.3822 |           3.3496 |
[32m[20230113 19:50:11 @agent_ppo2.py:186][0m |          -0.0100 |           3.3598 |           3.3470 |
[32m[20230113 19:50:11 @agent_ppo2.py:186][0m |          -0.0107 |           3.3332 |           3.3476 |
[32m[20230113 19:50:11 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 208.50
[32m[20230113 19:50:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.74
[32m[20230113 19:50:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.87
[32m[20230113 19:50:12 @agent_ppo2.py:144][0m Total time:       5.65 min
[32m[20230113 19:50:12 @agent_ppo2.py:146][0m 495616 total steps have happened
[32m[20230113 19:50:12 @agent_ppo2.py:122][0m #------------------------ Iteration 242 --------------------------#
[32m[20230113 19:50:12 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:12 @agent_ppo2.py:186][0m |           0.0003 |           3.1508 |           3.3261 |
[32m[20230113 19:50:12 @agent_ppo2.py:186][0m |          -0.0047 |           3.0209 |           3.3246 |
[32m[20230113 19:50:12 @agent_ppo2.py:186][0m |          -0.0067 |           2.9549 |           3.3250 |
[32m[20230113 19:50:12 @agent_ppo2.py:186][0m |          -0.0082 |           2.9199 |           3.3233 |
[32m[20230113 19:50:12 @agent_ppo2.py:186][0m |          -0.0093 |           2.8846 |           3.3253 |
[32m[20230113 19:50:12 @agent_ppo2.py:186][0m |          -0.0102 |           2.8631 |           3.3248 |
[32m[20230113 19:50:12 @agent_ppo2.py:186][0m |          -0.0106 |           2.8353 |           3.3225 |
[32m[20230113 19:50:12 @agent_ppo2.py:186][0m |          -0.0111 |           2.8222 |           3.3245 |
[32m[20230113 19:50:12 @agent_ppo2.py:186][0m |          -0.0115 |           2.7935 |           3.3249 |
[32m[20230113 19:50:12 @agent_ppo2.py:186][0m |          -0.0124 |           2.7785 |           3.3267 |
[32m[20230113 19:50:12 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 198.89
[32m[20230113 19:50:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 207.92
[32m[20230113 19:50:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.82
[32m[20230113 19:50:13 @agent_ppo2.py:144][0m Total time:       5.67 min
[32m[20230113 19:50:13 @agent_ppo2.py:146][0m 497664 total steps have happened
[32m[20230113 19:50:13 @agent_ppo2.py:122][0m #------------------------ Iteration 243 --------------------------#
[32m[20230113 19:50:13 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:50:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:14 @agent_ppo2.py:186][0m |          -0.0006 |          10.0980 |           3.3787 |
[32m[20230113 19:50:14 @agent_ppo2.py:186][0m |          -0.0025 |           5.9522 |           3.3755 |
[32m[20230113 19:50:14 @agent_ppo2.py:186][0m |          -0.0030 |           5.5132 |           3.3746 |
[32m[20230113 19:50:14 @agent_ppo2.py:186][0m |          -0.0040 |           5.3172 |           3.3729 |
[32m[20230113 19:50:14 @agent_ppo2.py:186][0m |          -0.0082 |           5.1785 |           3.3703 |
[32m[20230113 19:50:14 @agent_ppo2.py:186][0m |          -0.0092 |           5.1016 |           3.3689 |
[32m[20230113 19:50:14 @agent_ppo2.py:186][0m |          -0.0096 |           5.0134 |           3.3700 |
[32m[20230113 19:50:14 @agent_ppo2.py:186][0m |          -0.0064 |           4.9783 |           3.3697 |
[32m[20230113 19:50:14 @agent_ppo2.py:186][0m |          -0.0078 |           4.9587 |           3.3675 |
[32m[20230113 19:50:14 @agent_ppo2.py:186][0m |          -0.0082 |           4.8922 |           3.3666 |
[32m[20230113 19:50:14 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 19:50:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 127.78
[32m[20230113 19:50:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.41
[32m[20230113 19:50:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.34
[32m[20230113 19:50:14 @agent_ppo2.py:144][0m Total time:       5.70 min
[32m[20230113 19:50:14 @agent_ppo2.py:146][0m 499712 total steps have happened
[32m[20230113 19:50:14 @agent_ppo2.py:122][0m #------------------------ Iteration 244 --------------------------#
[32m[20230113 19:50:15 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:50:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:15 @agent_ppo2.py:186][0m |          -0.0008 |           5.6693 |           3.3486 |
[32m[20230113 19:50:15 @agent_ppo2.py:186][0m |          -0.0050 |           5.2934 |           3.3414 |
[32m[20230113 19:50:15 @agent_ppo2.py:186][0m |          -0.0069 |           5.1035 |           3.3384 |
[32m[20230113 19:50:15 @agent_ppo2.py:186][0m |          -0.0015 |           5.0442 |           3.3334 |
[32m[20230113 19:50:15 @agent_ppo2.py:186][0m |          -0.0053 |           5.1337 |           3.3318 |
[32m[20230113 19:50:15 @agent_ppo2.py:186][0m |          -0.0078 |           4.7796 |           3.3291 |
[32m[20230113 19:50:15 @agent_ppo2.py:186][0m |          -0.0088 |           4.6906 |           3.3295 |
[32m[20230113 19:50:15 @agent_ppo2.py:186][0m |          -0.0113 |           4.6133 |           3.3284 |
[32m[20230113 19:50:15 @agent_ppo2.py:186][0m |          -0.0116 |           4.5238 |           3.3291 |
[32m[20230113 19:50:15 @agent_ppo2.py:186][0m |          -0.0067 |           4.5581 |           3.3246 |
[32m[20230113 19:50:15 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 217.00
[32m[20230113 19:50:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.76
[32m[20230113 19:50:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.64
[32m[20230113 19:50:16 @agent_ppo2.py:144][0m Total time:       5.72 min
[32m[20230113 19:50:16 @agent_ppo2.py:146][0m 501760 total steps have happened
[32m[20230113 19:50:16 @agent_ppo2.py:122][0m #------------------------ Iteration 245 --------------------------#
[32m[20230113 19:50:16 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:50:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:16 @agent_ppo2.py:186][0m |          -0.0013 |           5.2381 |           3.3970 |
[32m[20230113 19:50:16 @agent_ppo2.py:186][0m |          -0.0071 |           4.7513 |           3.3969 |
[32m[20230113 19:50:16 @agent_ppo2.py:186][0m |          -0.0093 |           4.6394 |           3.3990 |
[32m[20230113 19:50:16 @agent_ppo2.py:186][0m |          -0.0099 |           4.5363 |           3.4001 |
[32m[20230113 19:50:17 @agent_ppo2.py:186][0m |          -0.0108 |           4.4909 |           3.4023 |
[32m[20230113 19:50:17 @agent_ppo2.py:186][0m |          -0.0114 |           4.4606 |           3.4019 |
[32m[20230113 19:50:17 @agent_ppo2.py:186][0m |          -0.0118 |           4.4190 |           3.4025 |
[32m[20230113 19:50:17 @agent_ppo2.py:186][0m |          -0.0127 |           4.3893 |           3.4033 |
[32m[20230113 19:50:17 @agent_ppo2.py:186][0m |          -0.0119 |           4.3962 |           3.4020 |
[32m[20230113 19:50:17 @agent_ppo2.py:186][0m |          -0.0129 |           4.3491 |           3.4003 |
[32m[20230113 19:50:17 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:50:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.37
[32m[20230113 19:50:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.56
[32m[20230113 19:50:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.00
[32m[20230113 19:50:17 @agent_ppo2.py:144][0m Total time:       5.74 min
[32m[20230113 19:50:17 @agent_ppo2.py:146][0m 503808 total steps have happened
[32m[20230113 19:50:17 @agent_ppo2.py:122][0m #------------------------ Iteration 246 --------------------------#
[32m[20230113 19:50:18 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:50:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:18 @agent_ppo2.py:186][0m |           0.0007 |           6.6868 |           3.5026 |
[32m[20230113 19:50:18 @agent_ppo2.py:186][0m |          -0.0033 |           6.2806 |           3.4980 |
[32m[20230113 19:50:18 @agent_ppo2.py:186][0m |          -0.0055 |           6.0947 |           3.4947 |
[32m[20230113 19:50:18 @agent_ppo2.py:186][0m |          -0.0059 |           5.9997 |           3.4965 |
[32m[20230113 19:50:18 @agent_ppo2.py:186][0m |          -0.0076 |           5.9380 |           3.4950 |
[32m[20230113 19:50:18 @agent_ppo2.py:186][0m |          -0.0089 |           5.8772 |           3.4936 |
[32m[20230113 19:50:18 @agent_ppo2.py:186][0m |          -0.0101 |           5.8156 |           3.4941 |
[32m[20230113 19:50:18 @agent_ppo2.py:186][0m |          -0.0100 |           5.7606 |           3.4953 |
[32m[20230113 19:50:18 @agent_ppo2.py:186][0m |          -0.0107 |           5.7417 |           3.4950 |
[32m[20230113 19:50:18 @agent_ppo2.py:186][0m |          -0.0113 |           5.6981 |           3.4948 |
[32m[20230113 19:50:18 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:50:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.16
[32m[20230113 19:50:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.70
[32m[20230113 19:50:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.43
[32m[20230113 19:50:19 @agent_ppo2.py:144][0m Total time:       5.77 min
[32m[20230113 19:50:19 @agent_ppo2.py:146][0m 505856 total steps have happened
[32m[20230113 19:50:19 @agent_ppo2.py:122][0m #------------------------ Iteration 247 --------------------------#
[32m[20230113 19:50:19 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:19 @agent_ppo2.py:186][0m |           0.0002 |           3.2347 |           3.4805 |
[32m[20230113 19:50:19 @agent_ppo2.py:186][0m |          -0.0052 |           3.1280 |           3.4778 |
[32m[20230113 19:50:19 @agent_ppo2.py:186][0m |          -0.0074 |           3.0791 |           3.4791 |
[32m[20230113 19:50:19 @agent_ppo2.py:186][0m |          -0.0082 |           3.0558 |           3.4784 |
[32m[20230113 19:50:19 @agent_ppo2.py:186][0m |          -0.0093 |           3.0403 |           3.4793 |
[32m[20230113 19:50:19 @agent_ppo2.py:186][0m |          -0.0109 |           3.0207 |           3.4805 |
[32m[20230113 19:50:19 @agent_ppo2.py:186][0m |          -0.0110 |           3.0215 |           3.4802 |
[32m[20230113 19:50:19 @agent_ppo2.py:186][0m |          -0.0112 |           3.0016 |           3.4816 |
[32m[20230113 19:50:19 @agent_ppo2.py:186][0m |          -0.0115 |           2.9845 |           3.4820 |
[32m[20230113 19:50:20 @agent_ppo2.py:186][0m |          -0.0124 |           2.9775 |           3.4803 |
[32m[20230113 19:50:20 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 205.44
[32m[20230113 19:50:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 211.99
[32m[20230113 19:50:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.11
[32m[20230113 19:50:20 @agent_ppo2.py:144][0m Total time:       5.79 min
[32m[20230113 19:50:20 @agent_ppo2.py:146][0m 507904 total steps have happened
[32m[20230113 19:50:20 @agent_ppo2.py:122][0m #------------------------ Iteration 248 --------------------------#
[32m[20230113 19:50:20 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:21 @agent_ppo2.py:186][0m |           0.0007 |           4.6322 |           3.5331 |
[32m[20230113 19:50:21 @agent_ppo2.py:186][0m |          -0.0012 |           4.4267 |           3.5314 |
[32m[20230113 19:50:21 @agent_ppo2.py:186][0m |          -0.0022 |           4.3225 |           3.5270 |
[32m[20230113 19:50:21 @agent_ppo2.py:186][0m |          -0.0058 |           4.1842 |           3.5241 |
[32m[20230113 19:50:21 @agent_ppo2.py:186][0m |          -0.0057 |           4.1205 |           3.5259 |
[32m[20230113 19:50:21 @agent_ppo2.py:186][0m |          -0.0065 |           4.1250 |           3.5298 |
[32m[20230113 19:50:21 @agent_ppo2.py:186][0m |          -0.0071 |           4.0335 |           3.5290 |
[32m[20230113 19:50:21 @agent_ppo2.py:186][0m |          -0.0078 |           3.9968 |           3.5285 |
[32m[20230113 19:50:21 @agent_ppo2.py:186][0m |          -0.0071 |           3.9721 |           3.5281 |
[32m[20230113 19:50:21 @agent_ppo2.py:186][0m |          -0.0085 |           3.9307 |           3.5281 |
[32m[20230113 19:50:21 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 207.74
[32m[20230113 19:50:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.28
[32m[20230113 19:50:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.76
[32m[20230113 19:50:21 @agent_ppo2.py:144][0m Total time:       5.81 min
[32m[20230113 19:50:21 @agent_ppo2.py:146][0m 509952 total steps have happened
[32m[20230113 19:50:21 @agent_ppo2.py:122][0m #------------------------ Iteration 249 --------------------------#
[32m[20230113 19:50:22 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:50:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:22 @agent_ppo2.py:186][0m |          -0.0052 |           5.8858 |           3.4871 |
[32m[20230113 19:50:22 @agent_ppo2.py:186][0m |          -0.0002 |           5.5751 |           3.4822 |
[32m[20230113 19:50:22 @agent_ppo2.py:186][0m |          -0.0038 |           5.3697 |           3.4818 |
[32m[20230113 19:50:22 @agent_ppo2.py:186][0m |           0.0014 |           5.1488 |           3.4805 |
[32m[20230113 19:50:22 @agent_ppo2.py:186][0m |          -0.0043 |           4.9451 |           3.4809 |
[32m[20230113 19:50:22 @agent_ppo2.py:186][0m |          -0.0134 |           4.8155 |           3.4801 |
[32m[20230113 19:50:22 @agent_ppo2.py:186][0m |          -0.0153 |           4.6772 |           3.4816 |
[32m[20230113 19:50:22 @agent_ppo2.py:186][0m |           0.0090 |           4.6929 |           3.4806 |
[32m[20230113 19:50:22 @agent_ppo2.py:186][0m |          -0.0042 |           4.5318 |           3.4756 |
[32m[20230113 19:50:22 @agent_ppo2.py:186][0m |          -0.0192 |           4.4796 |           3.4798 |
[32m[20230113 19:50:22 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.69
[32m[20230113 19:50:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.56
[32m[20230113 19:50:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 146.76
[32m[20230113 19:50:23 @agent_ppo2.py:144][0m Total time:       5.84 min
[32m[20230113 19:50:23 @agent_ppo2.py:146][0m 512000 total steps have happened
[32m[20230113 19:50:23 @agent_ppo2.py:122][0m #------------------------ Iteration 250 --------------------------#
[32m[20230113 19:50:23 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:23 @agent_ppo2.py:186][0m |          -0.0002 |           3.6125 |           3.4618 |
[32m[20230113 19:50:23 @agent_ppo2.py:186][0m |          -0.0025 |           3.3131 |           3.4596 |
[32m[20230113 19:50:23 @agent_ppo2.py:186][0m |          -0.0057 |           3.2069 |           3.4608 |
[32m[20230113 19:50:24 @agent_ppo2.py:186][0m |          -0.0056 |           3.1360 |           3.4596 |
[32m[20230113 19:50:24 @agent_ppo2.py:186][0m |          -0.0077 |           3.0933 |           3.4603 |
[32m[20230113 19:50:24 @agent_ppo2.py:186][0m |          -0.0088 |           3.0603 |           3.4604 |
[32m[20230113 19:50:24 @agent_ppo2.py:186][0m |          -0.0081 |           3.0592 |           3.4609 |
[32m[20230113 19:50:24 @agent_ppo2.py:186][0m |          -0.0094 |           2.9968 |           3.4592 |
[32m[20230113 19:50:24 @agent_ppo2.py:186][0m |          -0.0101 |           2.9762 |           3.4591 |
[32m[20230113 19:50:24 @agent_ppo2.py:186][0m |          -0.0101 |           2.9476 |           3.4625 |
[32m[20230113 19:50:24 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.93
[32m[20230113 19:50:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.30
[32m[20230113 19:50:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.39
[32m[20230113 19:50:24 @agent_ppo2.py:144][0m Total time:       5.86 min
[32m[20230113 19:50:24 @agent_ppo2.py:146][0m 514048 total steps have happened
[32m[20230113 19:50:24 @agent_ppo2.py:122][0m #------------------------ Iteration 251 --------------------------#
[32m[20230113 19:50:25 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 19:50:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:25 @agent_ppo2.py:186][0m |           0.0035 |          31.3502 |           3.4613 |
[32m[20230113 19:50:25 @agent_ppo2.py:186][0m |          -0.0067 |          17.6981 |           3.4580 |
[32m[20230113 19:50:25 @agent_ppo2.py:186][0m |          -0.0068 |          14.5184 |           3.4561 |
[32m[20230113 19:50:25 @agent_ppo2.py:186][0m |          -0.0077 |          11.7243 |           3.4565 |
[32m[20230113 19:50:25 @agent_ppo2.py:186][0m |          -0.0070 |          10.2576 |           3.4536 |
[32m[20230113 19:50:25 @agent_ppo2.py:186][0m |          -0.0097 |           9.7893 |           3.4499 |
[32m[20230113 19:50:25 @agent_ppo2.py:186][0m |          -0.0099 |           9.5762 |           3.4500 |
[32m[20230113 19:50:25 @agent_ppo2.py:186][0m |          -0.0106 |           8.5569 |           3.4483 |
[32m[20230113 19:50:25 @agent_ppo2.py:186][0m |          -0.0097 |           8.6168 |           3.4490 |
[32m[20230113 19:50:25 @agent_ppo2.py:186][0m |          -0.0117 |           8.2202 |           3.4464 |
[32m[20230113 19:50:25 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 19:50:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 100.47
[32m[20230113 19:50:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 200.60
[32m[20230113 19:50:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.84
[32m[20230113 19:50:25 @agent_ppo2.py:144][0m Total time:       5.88 min
[32m[20230113 19:50:25 @agent_ppo2.py:146][0m 516096 total steps have happened
[32m[20230113 19:50:25 @agent_ppo2.py:122][0m #------------------------ Iteration 252 --------------------------#
[32m[20230113 19:50:26 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:26 @agent_ppo2.py:186][0m |           0.0004 |           3.7821 |           3.5207 |
[32m[20230113 19:50:26 @agent_ppo2.py:186][0m |          -0.0029 |           3.3471 |           3.5188 |
[32m[20230113 19:50:26 @agent_ppo2.py:186][0m |          -0.0027 |           3.2139 |           3.5196 |
[32m[20230113 19:50:26 @agent_ppo2.py:186][0m |          -0.0037 |           3.1849 |           3.5219 |
[32m[20230113 19:50:26 @agent_ppo2.py:186][0m |          -0.0061 |           3.0705 |           3.5219 |
[32m[20230113 19:50:26 @agent_ppo2.py:186][0m |          -0.0064 |           3.0228 |           3.5230 |
[32m[20230113 19:50:26 @agent_ppo2.py:186][0m |          -0.0072 |           2.9664 |           3.5216 |
[32m[20230113 19:50:26 @agent_ppo2.py:186][0m |          -0.0075 |           2.9346 |           3.5231 |
[32m[20230113 19:50:26 @agent_ppo2.py:186][0m |          -0.0075 |           2.9112 |           3.5245 |
[32m[20230113 19:50:26 @agent_ppo2.py:186][0m |          -0.0085 |           2.8796 |           3.5220 |
[32m[20230113 19:50:26 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 214.65
[32m[20230113 19:50:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.07
[32m[20230113 19:50:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 150.22
[32m[20230113 19:50:27 @agent_ppo2.py:144][0m Total time:       5.90 min
[32m[20230113 19:50:27 @agent_ppo2.py:146][0m 518144 total steps have happened
[32m[20230113 19:50:27 @agent_ppo2.py:122][0m #------------------------ Iteration 253 --------------------------#
[32m[20230113 19:50:27 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 19:50:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:27 @agent_ppo2.py:186][0m |           0.0000 |          24.2741 |           3.4703 |
[32m[20230113 19:50:27 @agent_ppo2.py:186][0m |          -0.0063 |          17.4021 |           3.4684 |
[32m[20230113 19:50:27 @agent_ppo2.py:186][0m |          -0.0123 |          15.9534 |           3.4694 |
[32m[20230113 19:50:27 @agent_ppo2.py:186][0m |          -0.0090 |          14.5939 |           3.4696 |
[32m[20230113 19:50:27 @agent_ppo2.py:186][0m |          -0.0109 |          12.9229 |           3.4705 |
[32m[20230113 19:50:27 @agent_ppo2.py:186][0m |          -0.0115 |          11.8883 |           3.4738 |
[32m[20230113 19:50:27 @agent_ppo2.py:186][0m |          -0.0090 |          10.7685 |           3.4743 |
[32m[20230113 19:50:28 @agent_ppo2.py:186][0m |          -0.0117 |          10.2888 |           3.4747 |
[32m[20230113 19:50:28 @agent_ppo2.py:186][0m |          -0.0157 |           9.8785 |           3.4734 |
[32m[20230113 19:50:28 @agent_ppo2.py:186][0m |          -0.0145 |           8.7532 |           3.4765 |
[32m[20230113 19:50:28 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 19:50:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 124.02
[32m[20230113 19:50:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 209.20
[32m[20230113 19:50:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.97
[32m[20230113 19:50:28 @agent_ppo2.py:144][0m Total time:       5.92 min
[32m[20230113 19:50:28 @agent_ppo2.py:146][0m 520192 total steps have happened
[32m[20230113 19:50:28 @agent_ppo2.py:122][0m #------------------------ Iteration 254 --------------------------#
[32m[20230113 19:50:28 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 19:50:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:28 @agent_ppo2.py:186][0m |          -0.0017 |          34.0495 |           3.3921 |
[32m[20230113 19:50:28 @agent_ppo2.py:186][0m |          -0.0096 |          20.4272 |           3.3861 |
[32m[20230113 19:50:29 @agent_ppo2.py:186][0m |          -0.0090 |          15.4091 |           3.3883 |
[32m[20230113 19:50:29 @agent_ppo2.py:186][0m |          -0.0114 |          12.4459 |           3.3899 |
[32m[20230113 19:50:29 @agent_ppo2.py:186][0m |           0.0132 |          11.1887 |           3.3883 |
[32m[20230113 19:50:29 @agent_ppo2.py:186][0m |          -0.0154 |          10.3018 |           3.3858 |
[32m[20230113 19:50:29 @agent_ppo2.py:186][0m |          -0.0129 |           9.2507 |           3.3873 |
[32m[20230113 19:50:29 @agent_ppo2.py:186][0m |          -0.0140 |           8.3612 |           3.3904 |
[32m[20230113 19:50:29 @agent_ppo2.py:186][0m |          -0.0192 |           7.8506 |           3.3872 |
[32m[20230113 19:50:29 @agent_ppo2.py:186][0m |          -0.0090 |           7.6730 |           3.3903 |
[32m[20230113 19:50:29 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 19:50:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 104.78
[32m[20230113 19:50:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.29
[32m[20230113 19:50:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.04
[32m[20230113 19:50:29 @agent_ppo2.py:144][0m Total time:       5.94 min
[32m[20230113 19:50:29 @agent_ppo2.py:146][0m 522240 total steps have happened
[32m[20230113 19:50:29 @agent_ppo2.py:122][0m #------------------------ Iteration 255 --------------------------#
[32m[20230113 19:50:30 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:50:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:30 @agent_ppo2.py:186][0m |           0.0019 |          20.9101 |           3.5585 |
[32m[20230113 19:50:30 @agent_ppo2.py:186][0m |          -0.0028 |          13.6835 |           3.5572 |
[32m[20230113 19:50:30 @agent_ppo2.py:186][0m |          -0.0045 |          11.1389 |           3.5592 |
[32m[20230113 19:50:30 @agent_ppo2.py:186][0m |          -0.0033 |           9.5728 |           3.5597 |
[32m[20230113 19:50:30 @agent_ppo2.py:186][0m |          -0.0080 |           8.5700 |           3.5591 |
[32m[20230113 19:50:30 @agent_ppo2.py:186][0m |          -0.0073 |           7.5834 |           3.5606 |
[32m[20230113 19:50:30 @agent_ppo2.py:186][0m |          -0.0100 |           6.7904 |           3.5608 |
[32m[20230113 19:50:30 @agent_ppo2.py:186][0m |          -0.0109 |           6.2472 |           3.5605 |
[32m[20230113 19:50:30 @agent_ppo2.py:186][0m |          -0.0102 |           5.7964 |           3.5598 |
[32m[20230113 19:50:30 @agent_ppo2.py:186][0m |          -0.0111 |           5.2884 |           3.5598 |
[32m[20230113 19:50:30 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 19:50:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 113.17
[32m[20230113 19:50:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 208.52
[32m[20230113 19:50:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.22
[32m[20230113 19:50:31 @agent_ppo2.py:144][0m Total time:       5.97 min
[32m[20230113 19:50:31 @agent_ppo2.py:146][0m 524288 total steps have happened
[32m[20230113 19:50:31 @agent_ppo2.py:122][0m #------------------------ Iteration 256 --------------------------#
[32m[20230113 19:50:31 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:31 @agent_ppo2.py:186][0m |          -0.0001 |          10.4096 |           3.5405 |
[32m[20230113 19:50:31 @agent_ppo2.py:186][0m |          -0.0052 |           8.6989 |           3.5360 |
[32m[20230113 19:50:31 @agent_ppo2.py:186][0m |          -0.0072 |           8.0624 |           3.5349 |
[32m[20230113 19:50:31 @agent_ppo2.py:186][0m |          -0.0086 |           7.6020 |           3.5355 |
[32m[20230113 19:50:31 @agent_ppo2.py:186][0m |          -0.0088 |           7.2140 |           3.5373 |
[32m[20230113 19:50:31 @agent_ppo2.py:186][0m |          -0.0097 |           6.9327 |           3.5342 |
[32m[20230113 19:50:32 @agent_ppo2.py:186][0m |          -0.0107 |           6.6790 |           3.5357 |
[32m[20230113 19:50:32 @agent_ppo2.py:186][0m |          -0.0113 |           6.5134 |           3.5393 |
[32m[20230113 19:50:32 @agent_ppo2.py:186][0m |          -0.0117 |           6.3644 |           3.5344 |
[32m[20230113 19:50:32 @agent_ppo2.py:186][0m |          -0.0126 |           6.2520 |           3.5354 |
[32m[20230113 19:50:32 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.26
[32m[20230113 19:50:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.15
[32m[20230113 19:50:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.03
[32m[20230113 19:50:32 @agent_ppo2.py:144][0m Total time:       5.99 min
[32m[20230113 19:50:32 @agent_ppo2.py:146][0m 526336 total steps have happened
[32m[20230113 19:50:32 @agent_ppo2.py:122][0m #------------------------ Iteration 257 --------------------------#
[32m[20230113 19:50:32 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 19:50:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:33 @agent_ppo2.py:186][0m |          -0.0017 |          34.6166 |           3.6278 |
[32m[20230113 19:50:33 @agent_ppo2.py:186][0m |          -0.0088 |          17.8034 |           3.6242 |
[32m[20230113 19:50:33 @agent_ppo2.py:186][0m |          -0.0087 |          15.7777 |           3.6205 |
[32m[20230113 19:50:33 @agent_ppo2.py:186][0m |          -0.0111 |          14.7301 |           3.6197 |
[32m[20230113 19:50:33 @agent_ppo2.py:186][0m |          -0.0128 |          13.7139 |           3.6154 |
[32m[20230113 19:50:33 @agent_ppo2.py:186][0m |          -0.0128 |          12.9202 |           3.6146 |
[32m[20230113 19:50:33 @agent_ppo2.py:186][0m |          -0.0135 |          12.5122 |           3.6133 |
[32m[20230113 19:50:33 @agent_ppo2.py:186][0m |          -0.0145 |          11.9790 |           3.6120 |
[32m[20230113 19:50:33 @agent_ppo2.py:186][0m |          -0.0152 |          11.6920 |           3.6104 |
[32m[20230113 19:50:33 @agent_ppo2.py:186][0m |          -0.0178 |          11.3787 |           3.6093 |
[32m[20230113 19:50:33 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 19:50:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 29.17
[32m[20230113 19:50:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.94
[32m[20230113 19:50:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.10
[32m[20230113 19:50:33 @agent_ppo2.py:144][0m Total time:       6.01 min
[32m[20230113 19:50:33 @agent_ppo2.py:146][0m 528384 total steps have happened
[32m[20230113 19:50:33 @agent_ppo2.py:122][0m #------------------------ Iteration 258 --------------------------#
[32m[20230113 19:50:34 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 19:50:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:34 @agent_ppo2.py:186][0m |           0.0002 |          22.2492 |           3.6068 |
[32m[20230113 19:50:34 @agent_ppo2.py:186][0m |          -0.0031 |          15.0020 |           3.6025 |
[32m[20230113 19:50:34 @agent_ppo2.py:186][0m |          -0.0011 |          13.5367 |           3.5980 |
[32m[20230113 19:50:34 @agent_ppo2.py:186][0m |          -0.0086 |          12.7061 |           3.5981 |
[32m[20230113 19:50:34 @agent_ppo2.py:186][0m |          -0.0071 |          12.2495 |           3.5941 |
[32m[20230113 19:50:34 @agent_ppo2.py:186][0m |          -0.0078 |          11.7006 |           3.5929 |
[32m[20230113 19:50:34 @agent_ppo2.py:186][0m |          -0.0119 |          11.5840 |           3.5923 |
[32m[20230113 19:50:34 @agent_ppo2.py:186][0m |          -0.0113 |          11.0707 |           3.5892 |
[32m[20230113 19:50:34 @agent_ppo2.py:186][0m |          -0.0127 |          10.4616 |           3.5906 |
[32m[20230113 19:50:34 @agent_ppo2.py:186][0m |          -0.0106 |          10.2965 |           3.5870 |
[32m[20230113 19:50:34 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 19:50:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 98.85
[32m[20230113 19:50:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.32
[32m[20230113 19:50:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.48
[32m[20230113 19:50:34 @agent_ppo2.py:144][0m Total time:       6.03 min
[32m[20230113 19:50:34 @agent_ppo2.py:146][0m 530432 total steps have happened
[32m[20230113 19:50:34 @agent_ppo2.py:122][0m #------------------------ Iteration 259 --------------------------#
[32m[20230113 19:50:35 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:35 @agent_ppo2.py:186][0m |          -0.0007 |           5.5370 |           3.5341 |
[32m[20230113 19:50:35 @agent_ppo2.py:186][0m |          -0.0002 |           4.3244 |           3.5356 |
[32m[20230113 19:50:35 @agent_ppo2.py:186][0m |          -0.0024 |           4.0077 |           3.5348 |
[32m[20230113 19:50:35 @agent_ppo2.py:186][0m |          -0.0048 |           3.8764 |           3.5341 |
[32m[20230113 19:50:35 @agent_ppo2.py:186][0m |          -0.0044 |           3.7829 |           3.5340 |
[32m[20230113 19:50:35 @agent_ppo2.py:186][0m |          -0.0076 |           3.7080 |           3.5346 |
[32m[20230113 19:50:35 @agent_ppo2.py:186][0m |          -0.0055 |           3.6523 |           3.5355 |
[32m[20230113 19:50:35 @agent_ppo2.py:186][0m |          -0.0065 |           3.6027 |           3.5361 |
[32m[20230113 19:50:35 @agent_ppo2.py:186][0m |          -0.0080 |           3.5548 |           3.5360 |
[32m[20230113 19:50:35 @agent_ppo2.py:186][0m |          -0.0085 |           3.5255 |           3.5342 |
[32m[20230113 19:50:35 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.71
[32m[20230113 19:50:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.44
[32m[20230113 19:50:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.41
[32m[20230113 19:50:36 @agent_ppo2.py:144][0m Total time:       6.05 min
[32m[20230113 19:50:36 @agent_ppo2.py:146][0m 532480 total steps have happened
[32m[20230113 19:50:36 @agent_ppo2.py:122][0m #------------------------ Iteration 260 --------------------------#
[32m[20230113 19:50:36 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:50:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:36 @agent_ppo2.py:186][0m |           0.0047 |           5.3029 |           3.5919 |
[32m[20230113 19:50:36 @agent_ppo2.py:186][0m |           0.0025 |           4.8868 |           3.5857 |
[32m[20230113 19:50:36 @agent_ppo2.py:186][0m |          -0.0027 |           4.5763 |           3.5818 |
[32m[20230113 19:50:37 @agent_ppo2.py:186][0m |          -0.0021 |           4.5677 |           3.5829 |
[32m[20230113 19:50:37 @agent_ppo2.py:186][0m |          -0.0058 |           4.4597 |           3.5788 |
[32m[20230113 19:50:37 @agent_ppo2.py:186][0m |          -0.0038 |           4.4387 |           3.5827 |
[32m[20230113 19:50:37 @agent_ppo2.py:186][0m |          -0.0074 |           4.3568 |           3.5817 |
[32m[20230113 19:50:37 @agent_ppo2.py:186][0m |          -0.0066 |           4.3259 |           3.5831 |
[32m[20230113 19:50:37 @agent_ppo2.py:186][0m |          -0.0047 |           4.3682 |           3.5818 |
[32m[20230113 19:50:37 @agent_ppo2.py:186][0m |          -0.0082 |           4.2683 |           3.5841 |
[32m[20230113 19:50:37 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.83
[32m[20230113 19:50:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.82
[32m[20230113 19:50:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 145.10
[32m[20230113 19:50:37 @agent_ppo2.py:144][0m Total time:       6.08 min
[32m[20230113 19:50:37 @agent_ppo2.py:146][0m 534528 total steps have happened
[32m[20230113 19:50:37 @agent_ppo2.py:122][0m #------------------------ Iteration 261 --------------------------#
[32m[20230113 19:50:38 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:50:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:38 @agent_ppo2.py:186][0m |           0.0048 |           5.5859 |           3.5549 |
[32m[20230113 19:50:38 @agent_ppo2.py:186][0m |          -0.0029 |           4.0374 |           3.5529 |
[32m[20230113 19:50:38 @agent_ppo2.py:186][0m |          -0.0042 |           3.7774 |           3.5490 |
[32m[20230113 19:50:38 @agent_ppo2.py:186][0m |          -0.0048 |           3.6141 |           3.5472 |
[32m[20230113 19:50:38 @agent_ppo2.py:186][0m |          -0.0068 |           3.5619 |           3.5471 |
[32m[20230113 19:50:38 @agent_ppo2.py:186][0m |          -0.0020 |           3.6237 |           3.5434 |
[32m[20230113 19:50:38 @agent_ppo2.py:186][0m |          -0.0075 |           3.4459 |           3.5421 |
[32m[20230113 19:50:38 @agent_ppo2.py:186][0m |          -0.0092 |           3.3697 |           3.5417 |
[32m[20230113 19:50:38 @agent_ppo2.py:186][0m |          -0.0087 |           3.3435 |           3.5415 |
[32m[20230113 19:50:38 @agent_ppo2.py:186][0m |          -0.0113 |           3.2979 |           3.5391 |
[32m[20230113 19:50:38 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.55
[32m[20230113 19:50:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.14
[32m[20230113 19:50:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.07
[32m[20230113 19:50:39 @agent_ppo2.py:144][0m Total time:       6.10 min
[32m[20230113 19:50:39 @agent_ppo2.py:146][0m 536576 total steps have happened
[32m[20230113 19:50:39 @agent_ppo2.py:122][0m #------------------------ Iteration 262 --------------------------#
[32m[20230113 19:50:39 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:50:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:39 @agent_ppo2.py:186][0m |          -0.0004 |           4.6240 |           3.6148 |
[32m[20230113 19:50:39 @agent_ppo2.py:186][0m |          -0.0027 |           4.4259 |           3.6094 |
[32m[20230113 19:50:39 @agent_ppo2.py:186][0m |          -0.0055 |           4.3564 |           3.6034 |
[32m[20230113 19:50:39 @agent_ppo2.py:186][0m |          -0.0049 |           4.3291 |           3.6041 |
[32m[20230113 19:50:39 @agent_ppo2.py:186][0m |          -0.0061 |           4.3200 |           3.6014 |
[32m[20230113 19:50:39 @agent_ppo2.py:186][0m |          -0.0074 |           4.2253 |           3.5998 |
[32m[20230113 19:50:39 @agent_ppo2.py:186][0m |          -0.0076 |           4.2073 |           3.5987 |
[32m[20230113 19:50:39 @agent_ppo2.py:186][0m |          -0.0090 |           4.1675 |           3.5988 |
[32m[20230113 19:50:39 @agent_ppo2.py:186][0m |          -0.0083 |           4.1538 |           3.5991 |
[32m[20230113 19:50:40 @agent_ppo2.py:186][0m |          -0.0097 |           4.1629 |           3.5972 |
[32m[20230113 19:50:40 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.70
[32m[20230113 19:50:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.78
[32m[20230113 19:50:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 227.14
[32m[20230113 19:50:40 @agent_ppo2.py:144][0m Total time:       6.12 min
[32m[20230113 19:50:40 @agent_ppo2.py:146][0m 538624 total steps have happened
[32m[20230113 19:50:40 @agent_ppo2.py:122][0m #------------------------ Iteration 263 --------------------------#
[32m[20230113 19:50:40 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 19:50:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:40 @agent_ppo2.py:186][0m |          -0.0004 |          15.6420 |           3.5987 |
[32m[20230113 19:50:40 @agent_ppo2.py:186][0m |          -0.0033 |           7.9551 |           3.5938 |
[32m[20230113 19:50:40 @agent_ppo2.py:186][0m |          -0.0078 |           6.7619 |           3.5863 |
[32m[20230113 19:50:40 @agent_ppo2.py:186][0m |          -0.0090 |           6.3286 |           3.5871 |
[32m[20230113 19:50:41 @agent_ppo2.py:186][0m |          -0.0105 |           5.9913 |           3.5831 |
[32m[20230113 19:50:41 @agent_ppo2.py:186][0m |          -0.0108 |           5.7556 |           3.5816 |
[32m[20230113 19:50:41 @agent_ppo2.py:186][0m |          -0.0116 |           5.6225 |           3.5809 |
[32m[20230113 19:50:41 @agent_ppo2.py:186][0m |          -0.0125 |           5.4236 |           3.5791 |
[32m[20230113 19:50:41 @agent_ppo2.py:186][0m |          -0.0132 |           5.3755 |           3.5779 |
[32m[20230113 19:50:41 @agent_ppo2.py:186][0m |          -0.0126 |           5.1933 |           3.5775 |
[32m[20230113 19:50:41 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 19:50:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 92.63
[32m[20230113 19:50:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.16
[32m[20230113 19:50:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 226.98
[32m[20230113 19:50:41 @agent_ppo2.py:144][0m Total time:       6.14 min
[32m[20230113 19:50:41 @agent_ppo2.py:146][0m 540672 total steps have happened
[32m[20230113 19:50:41 @agent_ppo2.py:122][0m #------------------------ Iteration 264 --------------------------#
[32m[20230113 19:50:41 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 19:50:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:41 @agent_ppo2.py:186][0m |          -0.0014 |          61.3280 |           3.5493 |
[32m[20230113 19:50:42 @agent_ppo2.py:186][0m |           0.0541 |          47.6720 |           3.5472 |
[32m[20230113 19:50:42 @agent_ppo2.py:186][0m |          -0.0103 |          24.9106 |           3.5454 |
[32m[20230113 19:50:42 @agent_ppo2.py:186][0m |          -0.0109 |          16.8538 |           3.5419 |
[32m[20230113 19:50:42 @agent_ppo2.py:186][0m |          -0.0146 |          15.0627 |           3.5440 |
[32m[20230113 19:50:42 @agent_ppo2.py:186][0m |          -0.0051 |          13.1166 |           3.5419 |
[32m[20230113 19:50:42 @agent_ppo2.py:186][0m |          -0.0153 |          12.0121 |           3.5419 |
[32m[20230113 19:50:42 @agent_ppo2.py:186][0m |          -0.0180 |          11.4083 |           3.5431 |
[32m[20230113 19:50:42 @agent_ppo2.py:186][0m |          -0.0153 |          10.7212 |           3.5413 |
[32m[20230113 19:50:42 @agent_ppo2.py:186][0m |          -0.0168 |          10.2714 |           3.5420 |
[32m[20230113 19:50:42 @agent_ppo2.py:131][0m Policy update time: 0.34 s
[32m[20230113 19:50:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 20.58
[32m[20230113 19:50:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 216.92
[32m[20230113 19:50:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 213.40
[32m[20230113 19:50:42 @agent_ppo2.py:144][0m Total time:       6.16 min
[32m[20230113 19:50:42 @agent_ppo2.py:146][0m 542720 total steps have happened
[32m[20230113 19:50:42 @agent_ppo2.py:122][0m #------------------------ Iteration 265 --------------------------#
[32m[20230113 19:50:43 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:43 @agent_ppo2.py:186][0m |           0.0032 |          16.0589 |           3.6012 |
[32m[20230113 19:50:43 @agent_ppo2.py:186][0m |          -0.0044 |          11.6321 |           3.6017 |
[32m[20230113 19:50:43 @agent_ppo2.py:186][0m |          -0.0022 |          11.1613 |           3.5987 |
[32m[20230113 19:50:43 @agent_ppo2.py:186][0m |          -0.0063 |          10.6240 |           3.6003 |
[32m[20230113 19:50:43 @agent_ppo2.py:186][0m |          -0.0104 |          10.1682 |           3.5976 |
[32m[20230113 19:50:43 @agent_ppo2.py:186][0m |          -0.0076 |           9.9487 |           3.6009 |
[32m[20230113 19:50:43 @agent_ppo2.py:186][0m |          -0.0076 |           9.8249 |           3.5986 |
[32m[20230113 19:50:43 @agent_ppo2.py:186][0m |          -0.0071 |           9.7050 |           3.5994 |
[32m[20230113 19:50:43 @agent_ppo2.py:186][0m |          -0.0111 |           9.4037 |           3.5997 |
[32m[20230113 19:50:43 @agent_ppo2.py:186][0m |          -0.0091 |           9.3804 |           3.5976 |
[32m[20230113 19:50:43 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.02
[32m[20230113 19:50:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.95
[32m[20230113 19:50:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.92
[32m[20230113 19:50:44 @agent_ppo2.py:144][0m Total time:       6.18 min
[32m[20230113 19:50:44 @agent_ppo2.py:146][0m 544768 total steps have happened
[32m[20230113 19:50:44 @agent_ppo2.py:122][0m #------------------------ Iteration 266 --------------------------#
[32m[20230113 19:50:44 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 19:50:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:44 @agent_ppo2.py:186][0m |          -0.0010 |          41.3369 |           3.5996 |
[32m[20230113 19:50:44 @agent_ppo2.py:186][0m |          -0.0075 |          17.6751 |           3.5961 |
[32m[20230113 19:50:44 @agent_ppo2.py:186][0m |          -0.0102 |          13.2379 |           3.5930 |
[32m[20230113 19:50:44 @agent_ppo2.py:186][0m |          -0.0118 |          10.8221 |           3.5909 |
[32m[20230113 19:50:44 @agent_ppo2.py:186][0m |          -0.0132 |           9.6365 |           3.5882 |
[32m[20230113 19:50:44 @agent_ppo2.py:186][0m |          -0.0140 |           8.6985 |           3.5874 |
[32m[20230113 19:50:44 @agent_ppo2.py:186][0m |          -0.0145 |           8.1522 |           3.5855 |
[32m[20230113 19:50:44 @agent_ppo2.py:186][0m |          -0.0154 |           7.4647 |           3.5850 |
[32m[20230113 19:50:44 @agent_ppo2.py:186][0m |          -0.0161 |           7.0301 |           3.5860 |
[32m[20230113 19:50:44 @agent_ppo2.py:186][0m |          -0.0163 |           6.9513 |           3.5848 |
[32m[20230113 19:50:44 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 19:50:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 90.86
[32m[20230113 19:50:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.82
[32m[20230113 19:50:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.45
[32m[20230113 19:50:45 @agent_ppo2.py:144][0m Total time:       6.20 min
[32m[20230113 19:50:45 @agent_ppo2.py:146][0m 546816 total steps have happened
[32m[20230113 19:50:45 @agent_ppo2.py:122][0m #------------------------ Iteration 267 --------------------------#
[32m[20230113 19:50:45 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:50:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:45 @agent_ppo2.py:186][0m |          -0.0063 |          33.9955 |           3.5301 |
[32m[20230113 19:50:45 @agent_ppo2.py:186][0m |          -0.0103 |          27.3399 |           3.5236 |
[32m[20230113 19:50:46 @agent_ppo2.py:186][0m |          -0.0140 |          25.2886 |           3.5204 |
[32m[20230113 19:50:46 @agent_ppo2.py:186][0m |          -0.0084 |          24.5214 |           3.5168 |
[32m[20230113 19:50:46 @agent_ppo2.py:186][0m |          -0.0098 |          24.2831 |           3.5123 |
[32m[20230113 19:50:46 @agent_ppo2.py:186][0m |          -0.0121 |          22.7068 |           3.5112 |
[32m[20230113 19:50:46 @agent_ppo2.py:186][0m |          -0.0112 |          22.1271 |           3.5120 |
[32m[20230113 19:50:46 @agent_ppo2.py:186][0m |          -0.0182 |          21.5426 |           3.5103 |
[32m[20230113 19:50:46 @agent_ppo2.py:186][0m |          -0.0140 |          20.7341 |           3.5083 |
[32m[20230113 19:50:46 @agent_ppo2.py:186][0m |          -0.0148 |          20.3028 |           3.5088 |
[32m[20230113 19:50:46 @agent_ppo2.py:131][0m Policy update time: 0.55 s
[32m[20230113 19:50:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 122.46
[32m[20230113 19:50:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.17
[32m[20230113 19:50:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.82
[32m[20230113 19:50:46 @agent_ppo2.py:144][0m Total time:       6.23 min
[32m[20230113 19:50:46 @agent_ppo2.py:146][0m 548864 total steps have happened
[32m[20230113 19:50:46 @agent_ppo2.py:122][0m #------------------------ Iteration 268 --------------------------#
[32m[20230113 19:50:47 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:50:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:47 @agent_ppo2.py:186][0m |          -0.0016 |          18.8558 |           3.5686 |
[32m[20230113 19:50:47 @agent_ppo2.py:186][0m |          -0.0047 |          13.9611 |           3.5672 |
[32m[20230113 19:50:47 @agent_ppo2.py:186][0m |          -0.0052 |          11.6969 |           3.5676 |
[32m[20230113 19:50:47 @agent_ppo2.py:186][0m |          -0.0061 |          10.7964 |           3.5689 |
[32m[20230113 19:50:47 @agent_ppo2.py:186][0m |          -0.0085 |           9.5879 |           3.5684 |
[32m[20230113 19:50:47 @agent_ppo2.py:186][0m |          -0.0086 |           9.4534 |           3.5643 |
[32m[20230113 19:50:47 @agent_ppo2.py:186][0m |          -0.0084 |           8.8813 |           3.5660 |
[32m[20230113 19:50:47 @agent_ppo2.py:186][0m |          -0.0104 |           8.3181 |           3.5663 |
[32m[20230113 19:50:47 @agent_ppo2.py:186][0m |          -0.0089 |           7.8490 |           3.5680 |
[32m[20230113 19:50:47 @agent_ppo2.py:186][0m |          -0.0111 |           7.7935 |           3.5661 |
[32m[20230113 19:50:47 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 19:50:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 127.82
[32m[20230113 19:50:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.65
[32m[20230113 19:50:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.32
[32m[20230113 19:50:48 @agent_ppo2.py:144][0m Total time:       6.25 min
[32m[20230113 19:50:48 @agent_ppo2.py:146][0m 550912 total steps have happened
[32m[20230113 19:50:48 @agent_ppo2.py:122][0m #------------------------ Iteration 269 --------------------------#
[32m[20230113 19:50:48 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 19:50:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:48 @agent_ppo2.py:186][0m |           0.0008 |          19.8011 |           3.4644 |
[32m[20230113 19:50:48 @agent_ppo2.py:186][0m |          -0.0006 |          13.1587 |           3.4629 |
[32m[20230113 19:50:48 @agent_ppo2.py:186][0m |          -0.0042 |          10.6952 |           3.4594 |
[32m[20230113 19:50:48 @agent_ppo2.py:186][0m |          -0.0068 |           9.6313 |           3.4573 |
[32m[20230113 19:50:48 @agent_ppo2.py:186][0m |          -0.0057 |           8.8434 |           3.4553 |
[32m[20230113 19:50:48 @agent_ppo2.py:186][0m |          -0.0086 |           8.2282 |           3.4541 |
[32m[20230113 19:50:49 @agent_ppo2.py:186][0m |          -0.0091 |           7.8001 |           3.4541 |
[32m[20230113 19:50:49 @agent_ppo2.py:186][0m |          -0.0110 |           7.3435 |           3.4558 |
[32m[20230113 19:50:49 @agent_ppo2.py:186][0m |          -0.0092 |           7.1315 |           3.4559 |
[32m[20230113 19:50:49 @agent_ppo2.py:186][0m |          -0.0102 |           6.7310 |           3.4556 |
[32m[20230113 19:50:49 @agent_ppo2.py:131][0m Policy update time: 0.40 s
[32m[20230113 19:50:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 125.60
[32m[20230113 19:50:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 201.88
[32m[20230113 19:50:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 229.23
[32m[20230113 19:50:49 @agent_ppo2.py:144][0m Total time:       6.28 min
[32m[20230113 19:50:49 @agent_ppo2.py:146][0m 552960 total steps have happened
[32m[20230113 19:50:49 @agent_ppo2.py:122][0m #------------------------ Iteration 270 --------------------------#
[32m[20230113 19:50:49 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 19:50:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:50 @agent_ppo2.py:186][0m |          -0.0007 |          32.3813 |           3.4751 |
[32m[20230113 19:50:50 @agent_ppo2.py:186][0m |           0.0013 |          14.9176 |           3.4693 |
[32m[20230113 19:50:50 @agent_ppo2.py:186][0m |          -0.0076 |          10.0915 |           3.4608 |
[32m[20230113 19:50:50 @agent_ppo2.py:186][0m |          -0.0051 |           9.0600 |           3.4636 |
[32m[20230113 19:50:50 @agent_ppo2.py:186][0m |          -0.0051 |           8.2111 |           3.4621 |
[32m[20230113 19:50:50 @agent_ppo2.py:186][0m |          -0.0137 |           7.6852 |           3.4595 |
[32m[20230113 19:50:50 @agent_ppo2.py:186][0m |          -0.0167 |           7.1734 |           3.4596 |
[32m[20230113 19:50:50 @agent_ppo2.py:186][0m |          -0.0135 |           7.1160 |           3.4566 |
[32m[20230113 19:50:50 @agent_ppo2.py:186][0m |          -0.0116 |           6.9467 |           3.4566 |
[32m[20230113 19:50:50 @agent_ppo2.py:186][0m |          -0.0164 |           6.4625 |           3.4547 |
[32m[20230113 19:50:50 @agent_ppo2.py:131][0m Policy update time: 0.40 s
[32m[20230113 19:50:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 107.23
[32m[20230113 19:50:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 192.19
[32m[20230113 19:50:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.15
[32m[20230113 19:50:50 @agent_ppo2.py:144][0m Total time:       6.30 min
[32m[20230113 19:50:50 @agent_ppo2.py:146][0m 555008 total steps have happened
[32m[20230113 19:50:50 @agent_ppo2.py:122][0m #------------------------ Iteration 271 --------------------------#
[32m[20230113 19:50:51 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:50:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:51 @agent_ppo2.py:186][0m |           0.0047 |          10.2080 |           3.5906 |
[32m[20230113 19:50:51 @agent_ppo2.py:186][0m |          -0.0015 |           6.6671 |           3.5898 |
[32m[20230113 19:50:51 @agent_ppo2.py:186][0m |          -0.0019 |           6.4408 |           3.5891 |
[32m[20230113 19:50:51 @agent_ppo2.py:186][0m |          -0.0027 |           6.3047 |           3.5887 |
[32m[20230113 19:50:51 @agent_ppo2.py:186][0m |          -0.0088 |           5.6598 |           3.5873 |
[32m[20230113 19:50:51 @agent_ppo2.py:186][0m |          -0.0068 |           5.5276 |           3.5882 |
[32m[20230113 19:50:51 @agent_ppo2.py:186][0m |          -0.0118 |           5.3828 |           3.5897 |
[32m[20230113 19:50:51 @agent_ppo2.py:186][0m |          -0.0325 |           5.4525 |           3.5903 |
[32m[20230113 19:50:51 @agent_ppo2.py:186][0m |          -0.0098 |           5.2432 |           3.5910 |
[32m[20230113 19:50:51 @agent_ppo2.py:186][0m |          -0.0118 |           5.1413 |           3.5916 |
[32m[20230113 19:50:51 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:50:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 214.51
[32m[20230113 19:50:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.52
[32m[20230113 19:50:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 229.43
[32m[20230113 19:50:52 @agent_ppo2.py:144][0m Total time:       6.32 min
[32m[20230113 19:50:52 @agent_ppo2.py:146][0m 557056 total steps have happened
[32m[20230113 19:50:52 @agent_ppo2.py:122][0m #------------------------ Iteration 272 --------------------------#
[32m[20230113 19:50:52 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:52 @agent_ppo2.py:186][0m |          -0.0014 |           3.5622 |           3.6704 |
[32m[20230113 19:50:52 @agent_ppo2.py:186][0m |          -0.0046 |           3.3679 |           3.6641 |
[32m[20230113 19:50:52 @agent_ppo2.py:186][0m |          -0.0066 |           3.2685 |           3.6717 |
[32m[20230113 19:50:52 @agent_ppo2.py:186][0m |          -0.0073 |           3.2094 |           3.6680 |
[32m[20230113 19:50:53 @agent_ppo2.py:186][0m |          -0.0080 |           3.1604 |           3.6751 |
[32m[20230113 19:50:53 @agent_ppo2.py:186][0m |          -0.0092 |           3.1339 |           3.6769 |
[32m[20230113 19:50:53 @agent_ppo2.py:186][0m |          -0.0099 |           3.1050 |           3.6772 |
[32m[20230113 19:50:53 @agent_ppo2.py:186][0m |          -0.0102 |           3.0732 |           3.6786 |
[32m[20230113 19:50:53 @agent_ppo2.py:186][0m |          -0.0109 |           3.0515 |           3.6791 |
[32m[20230113 19:50:53 @agent_ppo2.py:186][0m |          -0.0116 |           3.0295 |           3.6798 |
[32m[20230113 19:50:53 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 218.61
[32m[20230113 19:50:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.44
[32m[20230113 19:50:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.50
[32m[20230113 19:50:53 @agent_ppo2.py:144][0m Total time:       6.34 min
[32m[20230113 19:50:53 @agent_ppo2.py:146][0m 559104 total steps have happened
[32m[20230113 19:50:53 @agent_ppo2.py:122][0m #------------------------ Iteration 273 --------------------------#
[32m[20230113 19:50:54 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:54 @agent_ppo2.py:186][0m |           0.0004 |           3.6083 |           3.6203 |
[32m[20230113 19:50:54 @agent_ppo2.py:186][0m |          -0.0049 |           3.2796 |           3.6127 |
[32m[20230113 19:50:54 @agent_ppo2.py:186][0m |          -0.0072 |           3.1517 |           3.6075 |
[32m[20230113 19:50:54 @agent_ppo2.py:186][0m |          -0.0077 |           3.0793 |           3.6087 |
[32m[20230113 19:50:54 @agent_ppo2.py:186][0m |          -0.0089 |           3.0196 |           3.6081 |
[32m[20230113 19:50:54 @agent_ppo2.py:186][0m |          -0.0092 |           2.9733 |           3.6053 |
[32m[20230113 19:50:54 @agent_ppo2.py:186][0m |          -0.0099 |           2.9324 |           3.6058 |
[32m[20230113 19:50:54 @agent_ppo2.py:186][0m |          -0.0106 |           2.8974 |           3.6036 |
[32m[20230113 19:50:54 @agent_ppo2.py:186][0m |          -0.0106 |           2.8780 |           3.6031 |
[32m[20230113 19:50:54 @agent_ppo2.py:186][0m |          -0.0113 |           2.8374 |           3.6045 |
[32m[20230113 19:50:54 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:50:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 218.72
[32m[20230113 19:50:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.33
[32m[20230113 19:50:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.81
[32m[20230113 19:50:55 @agent_ppo2.py:144][0m Total time:       6.37 min
[32m[20230113 19:50:55 @agent_ppo2.py:146][0m 561152 total steps have happened
[32m[20230113 19:50:55 @agent_ppo2.py:122][0m #------------------------ Iteration 274 --------------------------#
[32m[20230113 19:50:55 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:55 @agent_ppo2.py:186][0m |           0.0005 |           3.6369 |           3.5495 |
[32m[20230113 19:50:55 @agent_ppo2.py:186][0m |          -0.0031 |           3.3669 |           3.5465 |
[32m[20230113 19:50:55 @agent_ppo2.py:186][0m |          -0.0049 |           3.2589 |           3.5413 |
[32m[20230113 19:50:55 @agent_ppo2.py:186][0m |          -0.0052 |           3.1815 |           3.5454 |
[32m[20230113 19:50:55 @agent_ppo2.py:186][0m |          -0.0057 |           3.1418 |           3.5440 |
[32m[20230113 19:50:55 @agent_ppo2.py:186][0m |          -0.0068 |           3.0931 |           3.5470 |
[32m[20230113 19:50:55 @agent_ppo2.py:186][0m |          -0.0064 |           3.0724 |           3.5476 |
[32m[20230113 19:50:55 @agent_ppo2.py:186][0m |          -0.0077 |           3.0371 |           3.5476 |
[32m[20230113 19:50:56 @agent_ppo2.py:186][0m |          -0.0082 |           3.0172 |           3.5484 |
[32m[20230113 19:50:56 @agent_ppo2.py:186][0m |          -0.0085 |           2.9891 |           3.5490 |
[32m[20230113 19:50:56 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:50:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 217.89
[32m[20230113 19:50:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.50
[32m[20230113 19:50:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.71
[32m[20230113 19:50:56 @agent_ppo2.py:144][0m Total time:       6.39 min
[32m[20230113 19:50:56 @agent_ppo2.py:146][0m 563200 total steps have happened
[32m[20230113 19:50:56 @agent_ppo2.py:122][0m #------------------------ Iteration 275 --------------------------#
[32m[20230113 19:50:56 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:57 @agent_ppo2.py:186][0m |          -0.0010 |           3.3169 |           3.6865 |
[32m[20230113 19:50:57 @agent_ppo2.py:186][0m |          -0.0050 |           3.1993 |           3.6825 |
[32m[20230113 19:50:57 @agent_ppo2.py:186][0m |          -0.0064 |           3.1539 |           3.6824 |
[32m[20230113 19:50:57 @agent_ppo2.py:186][0m |          -0.0079 |           3.1061 |           3.6784 |
[32m[20230113 19:50:57 @agent_ppo2.py:186][0m |          -0.0084 |           3.0776 |           3.6791 |
[32m[20230113 19:50:57 @agent_ppo2.py:186][0m |          -0.0089 |           3.0652 |           3.6790 |
[32m[20230113 19:50:57 @agent_ppo2.py:186][0m |          -0.0091 |           3.0435 |           3.6768 |
[32m[20230113 19:50:57 @agent_ppo2.py:186][0m |          -0.0101 |           3.0107 |           3.6779 |
[32m[20230113 19:50:57 @agent_ppo2.py:186][0m |          -0.0099 |           2.9863 |           3.6801 |
[32m[20230113 19:50:57 @agent_ppo2.py:186][0m |          -0.0103 |           3.0010 |           3.6794 |
[32m[20230113 19:50:57 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 213.93
[32m[20230113 19:50:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.27
[32m[20230113 19:50:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.73
[32m[20230113 19:50:57 @agent_ppo2.py:144][0m Total time:       6.41 min
[32m[20230113 19:50:57 @agent_ppo2.py:146][0m 565248 total steps have happened
[32m[20230113 19:50:57 @agent_ppo2.py:122][0m #------------------------ Iteration 276 --------------------------#
[32m[20230113 19:50:58 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:50:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:58 @agent_ppo2.py:186][0m |          -0.0009 |           2.6164 |           3.6648 |
[32m[20230113 19:50:58 @agent_ppo2.py:186][0m |          -0.0036 |           2.4758 |           3.6571 |
[32m[20230113 19:50:58 @agent_ppo2.py:186][0m |          -0.0061 |           2.4053 |           3.6527 |
[32m[20230113 19:50:58 @agent_ppo2.py:186][0m |          -0.0075 |           2.3525 |           3.6514 |
[32m[20230113 19:50:58 @agent_ppo2.py:186][0m |          -0.0081 |           2.3180 |           3.6493 |
[32m[20230113 19:50:58 @agent_ppo2.py:186][0m |          -0.0087 |           2.2769 |           3.6491 |
[32m[20230113 19:50:58 @agent_ppo2.py:186][0m |          -0.0095 |           2.2505 |           3.6475 |
[32m[20230113 19:50:58 @agent_ppo2.py:186][0m |          -0.0099 |           2.2287 |           3.6499 |
[32m[20230113 19:50:58 @agent_ppo2.py:186][0m |          -0.0106 |           2.2031 |           3.6477 |
[32m[20230113 19:50:58 @agent_ppo2.py:186][0m |          -0.0109 |           2.1811 |           3.6495 |
[32m[20230113 19:50:58 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:50:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 214.14
[32m[20230113 19:50:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.89
[32m[20230113 19:50:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.88
[32m[20230113 19:50:59 @agent_ppo2.py:144][0m Total time:       6.44 min
[32m[20230113 19:50:59 @agent_ppo2.py:146][0m 567296 total steps have happened
[32m[20230113 19:50:59 @agent_ppo2.py:122][0m #------------------------ Iteration 277 --------------------------#
[32m[20230113 19:50:59 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 19:50:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:50:59 @agent_ppo2.py:186][0m |           0.0004 |          28.2828 |           3.6551 |
[32m[20230113 19:50:59 @agent_ppo2.py:186][0m |          -0.0035 |          13.4386 |           3.6534 |
[32m[20230113 19:50:59 @agent_ppo2.py:186][0m |          -0.0086 |          10.9240 |           3.6506 |
[32m[20230113 19:50:59 @agent_ppo2.py:186][0m |          -0.0099 |           8.6954 |           3.6494 |
[32m[20230113 19:50:59 @agent_ppo2.py:186][0m |          -0.0117 |           7.0074 |           3.6474 |
[32m[20230113 19:50:59 @agent_ppo2.py:186][0m |          -0.0125 |           6.3929 |           3.6448 |
[32m[20230113 19:50:59 @agent_ppo2.py:186][0m |          -0.0135 |           6.1106 |           3.6450 |
[32m[20230113 19:51:00 @agent_ppo2.py:186][0m |          -0.0151 |           5.9038 |           3.6443 |
[32m[20230113 19:51:00 @agent_ppo2.py:186][0m |          -0.0141 |           5.6658 |           3.6412 |
[32m[20230113 19:51:00 @agent_ppo2.py:186][0m |          -0.0162 |           5.5420 |           3.6430 |
[32m[20230113 19:51:00 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 19:51:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 111.79
[32m[20230113 19:51:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.98
[32m[20230113 19:51:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.57
[32m[20230113 19:51:00 @agent_ppo2.py:144][0m Total time:       6.46 min
[32m[20230113 19:51:00 @agent_ppo2.py:146][0m 569344 total steps have happened
[32m[20230113 19:51:00 @agent_ppo2.py:122][0m #------------------------ Iteration 278 --------------------------#
[32m[20230113 19:51:01 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:01 @agent_ppo2.py:186][0m |           0.0002 |           5.7765 |           3.6899 |
[32m[20230113 19:51:01 @agent_ppo2.py:186][0m |          -0.0023 |           3.5080 |           3.6902 |
[32m[20230113 19:51:01 @agent_ppo2.py:186][0m |          -0.0037 |           3.1382 |           3.6886 |
[32m[20230113 19:51:01 @agent_ppo2.py:186][0m |          -0.0046 |           2.9559 |           3.6880 |
[32m[20230113 19:51:01 @agent_ppo2.py:186][0m |          -0.0056 |           2.8620 |           3.6887 |
[32m[20230113 19:51:01 @agent_ppo2.py:186][0m |          -0.0063 |           2.8093 |           3.6896 |
[32m[20230113 19:51:01 @agent_ppo2.py:186][0m |          -0.0067 |           2.7367 |           3.6887 |
[32m[20230113 19:51:01 @agent_ppo2.py:186][0m |          -0.0070 |           2.7106 |           3.6903 |
[32m[20230113 19:51:01 @agent_ppo2.py:186][0m |          -0.0074 |           2.6671 |           3.6911 |
[32m[20230113 19:51:01 @agent_ppo2.py:186][0m |          -0.0077 |           2.6386 |           3.6939 |
[32m[20230113 19:51:01 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.13
[32m[20230113 19:51:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 218.70
[32m[20230113 19:51:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.20
[32m[20230113 19:51:01 @agent_ppo2.py:144][0m Total time:       6.48 min
[32m[20230113 19:51:01 @agent_ppo2.py:146][0m 571392 total steps have happened
[32m[20230113 19:51:01 @agent_ppo2.py:122][0m #------------------------ Iteration 279 --------------------------#
[32m[20230113 19:51:02 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:02 @agent_ppo2.py:186][0m |          -0.0004 |           5.1823 |           3.6462 |
[32m[20230113 19:51:02 @agent_ppo2.py:186][0m |          -0.0056 |           5.0429 |           3.6440 |
[32m[20230113 19:51:02 @agent_ppo2.py:186][0m |          -0.0097 |           5.0023 |           3.6443 |
[32m[20230113 19:51:02 @agent_ppo2.py:186][0m |          -0.0093 |           4.9678 |           3.6428 |
[32m[20230113 19:51:02 @agent_ppo2.py:186][0m |          -0.0110 |           4.9377 |           3.6401 |
[32m[20230113 19:51:02 @agent_ppo2.py:186][0m |          -0.0113 |           4.8922 |           3.6407 |
[32m[20230113 19:51:02 @agent_ppo2.py:186][0m |          -0.0115 |           4.8937 |           3.6422 |
[32m[20230113 19:51:02 @agent_ppo2.py:186][0m |          -0.0136 |           4.8723 |           3.6407 |
[32m[20230113 19:51:02 @agent_ppo2.py:186][0m |          -0.0106 |           4.9112 |           3.6422 |
[32m[20230113 19:51:02 @agent_ppo2.py:186][0m |          -0.0123 |           4.8573 |           3.6435 |
[32m[20230113 19:51:02 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.47
[32m[20230113 19:51:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.38
[32m[20230113 19:51:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.53
[32m[20230113 19:51:03 @agent_ppo2.py:144][0m Total time:       6.51 min
[32m[20230113 19:51:03 @agent_ppo2.py:146][0m 573440 total steps have happened
[32m[20230113 19:51:03 @agent_ppo2.py:122][0m #------------------------ Iteration 280 --------------------------#
[32m[20230113 19:51:03 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:03 @agent_ppo2.py:186][0m |           0.0004 |           3.6838 |           3.7253 |
[32m[20230113 19:51:03 @agent_ppo2.py:186][0m |          -0.0041 |           3.5497 |           3.7223 |
[32m[20230113 19:51:04 @agent_ppo2.py:186][0m |          -0.0061 |           3.4629 |           3.7226 |
[32m[20230113 19:51:04 @agent_ppo2.py:186][0m |          -0.0071 |           3.4129 |           3.7207 |
[32m[20230113 19:51:04 @agent_ppo2.py:186][0m |          -0.0076 |           3.3963 |           3.7189 |
[32m[20230113 19:51:04 @agent_ppo2.py:186][0m |          -0.0084 |           3.3626 |           3.7193 |
[32m[20230113 19:51:04 @agent_ppo2.py:186][0m |          -0.0094 |           3.3236 |           3.7187 |
[32m[20230113 19:51:04 @agent_ppo2.py:186][0m |          -0.0100 |           3.3078 |           3.7187 |
[32m[20230113 19:51:04 @agent_ppo2.py:186][0m |          -0.0106 |           3.2878 |           3.7202 |
[32m[20230113 19:51:04 @agent_ppo2.py:186][0m |          -0.0107 |           3.2654 |           3.7177 |
[32m[20230113 19:51:04 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.92
[32m[20230113 19:51:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.27
[32m[20230113 19:51:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.06
[32m[20230113 19:51:04 @agent_ppo2.py:144][0m Total time:       6.53 min
[32m[20230113 19:51:04 @agent_ppo2.py:146][0m 575488 total steps have happened
[32m[20230113 19:51:04 @agent_ppo2.py:122][0m #------------------------ Iteration 281 --------------------------#
[32m[20230113 19:51:05 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:05 @agent_ppo2.py:186][0m |          -0.0006 |           3.2630 |           3.5635 |
[32m[20230113 19:51:05 @agent_ppo2.py:186][0m |          -0.0061 |           3.1204 |           3.5577 |
[32m[20230113 19:51:05 @agent_ppo2.py:186][0m |          -0.0079 |           3.0578 |           3.5584 |
[32m[20230113 19:51:05 @agent_ppo2.py:186][0m |          -0.0091 |           3.0176 |           3.5529 |
[32m[20230113 19:51:05 @agent_ppo2.py:186][0m |          -0.0102 |           2.9895 |           3.5532 |
[32m[20230113 19:51:05 @agent_ppo2.py:186][0m |          -0.0117 |           2.9651 |           3.5518 |
[32m[20230113 19:51:05 @agent_ppo2.py:186][0m |          -0.0116 |           2.9380 |           3.5520 |
[32m[20230113 19:51:05 @agent_ppo2.py:186][0m |          -0.0127 |           2.9227 |           3.5504 |
[32m[20230113 19:51:05 @agent_ppo2.py:186][0m |          -0.0132 |           2.9035 |           3.5530 |
[32m[20230113 19:51:05 @agent_ppo2.py:186][0m |          -0.0136 |           2.8934 |           3.5519 |
[32m[20230113 19:51:05 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 209.12
[32m[20230113 19:51:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.15
[32m[20230113 19:51:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.42
[32m[20230113 19:51:06 @agent_ppo2.py:144][0m Total time:       6.55 min
[32m[20230113 19:51:06 @agent_ppo2.py:146][0m 577536 total steps have happened
[32m[20230113 19:51:06 @agent_ppo2.py:122][0m #------------------------ Iteration 282 --------------------------#
[32m[20230113 19:51:06 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:51:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:06 @agent_ppo2.py:186][0m |          -0.0029 |           4.8070 |           3.6300 |
[32m[20230113 19:51:06 @agent_ppo2.py:186][0m |          -0.0037 |           4.7129 |           3.6247 |
[32m[20230113 19:51:06 @agent_ppo2.py:186][0m |          -0.0071 |           4.6102 |           3.6187 |
[32m[20230113 19:51:06 @agent_ppo2.py:186][0m |          -0.0064 |           4.6464 |           3.6173 |
[32m[20230113 19:51:06 @agent_ppo2.py:186][0m |          -0.0070 |           4.6320 |           3.6149 |
[32m[20230113 19:51:06 @agent_ppo2.py:186][0m |          -0.0097 |           4.5074 |           3.6158 |
[32m[20230113 19:51:07 @agent_ppo2.py:186][0m |          -0.0107 |           4.4889 |           3.6162 |
[32m[20230113 19:51:07 @agent_ppo2.py:186][0m |          -0.0107 |           4.4621 |           3.6114 |
[32m[20230113 19:51:07 @agent_ppo2.py:186][0m |          -0.0106 |           4.4443 |           3.6146 |
[32m[20230113 19:51:07 @agent_ppo2.py:186][0m |          -0.0128 |           4.4308 |           3.6146 |
[32m[20230113 19:51:07 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 217.32
[32m[20230113 19:51:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.98
[32m[20230113 19:51:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 135.45
[32m[20230113 19:51:07 @agent_ppo2.py:144][0m Total time:       6.58 min
[32m[20230113 19:51:07 @agent_ppo2.py:146][0m 579584 total steps have happened
[32m[20230113 19:51:07 @agent_ppo2.py:122][0m #------------------------ Iteration 283 --------------------------#
[32m[20230113 19:51:08 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:51:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:08 @agent_ppo2.py:186][0m |          -0.0000 |           5.2661 |           3.6860 |
[32m[20230113 19:51:08 @agent_ppo2.py:186][0m |          -0.0048 |           5.0977 |           3.6745 |
[32m[20230113 19:51:08 @agent_ppo2.py:186][0m |          -0.0079 |           5.0200 |           3.6751 |
[32m[20230113 19:51:08 @agent_ppo2.py:186][0m |          -0.0089 |           4.9765 |           3.6755 |
[32m[20230113 19:51:08 @agent_ppo2.py:186][0m |          -0.0095 |           4.9236 |           3.6789 |
[32m[20230113 19:51:08 @agent_ppo2.py:186][0m |          -0.0086 |           5.0377 |           3.6813 |
[32m[20230113 19:51:08 @agent_ppo2.py:186][0m |          -0.0100 |           4.9300 |           3.6777 |
[32m[20230113 19:51:08 @agent_ppo2.py:186][0m |          -0.0125 |           4.8395 |           3.6828 |
[32m[20230113 19:51:08 @agent_ppo2.py:186][0m |          -0.0130 |           4.8015 |           3.6799 |
[32m[20230113 19:51:08 @agent_ppo2.py:186][0m |          -0.0129 |           4.8175 |           3.6851 |
[32m[20230113 19:51:08 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.57
[32m[20230113 19:51:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.74
[32m[20230113 19:51:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.66
[32m[20230113 19:51:09 @agent_ppo2.py:144][0m Total time:       6.60 min
[32m[20230113 19:51:09 @agent_ppo2.py:146][0m 581632 total steps have happened
[32m[20230113 19:51:09 @agent_ppo2.py:122][0m #------------------------ Iteration 284 --------------------------#
[32m[20230113 19:51:09 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 19:51:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:09 @agent_ppo2.py:186][0m |          -0.0016 |          14.0589 |           3.5846 |
[32m[20230113 19:51:09 @agent_ppo2.py:186][0m |          -0.0062 |           6.7448 |           3.5849 |
[32m[20230113 19:51:09 @agent_ppo2.py:186][0m |          -0.0057 |           6.1824 |           3.5842 |
[32m[20230113 19:51:09 @agent_ppo2.py:186][0m |          -0.0094 |           5.7921 |           3.5817 |
[32m[20230113 19:51:09 @agent_ppo2.py:186][0m |           0.0420 |           6.8432 |           3.5834 |
[32m[20230113 19:51:09 @agent_ppo2.py:186][0m |          -0.0096 |           5.3958 |           3.5771 |
[32m[20230113 19:51:09 @agent_ppo2.py:186][0m |          -0.0115 |           5.2634 |           3.5757 |
[32m[20230113 19:51:09 @agent_ppo2.py:186][0m |           0.0046 |           6.3982 |           3.5755 |
[32m[20230113 19:51:09 @agent_ppo2.py:186][0m |          -0.0054 |           5.2098 |           3.5756 |
[32m[20230113 19:51:09 @agent_ppo2.py:186][0m |          -0.0124 |           4.8247 |           3.5733 |
[32m[20230113 19:51:09 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 19:51:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 116.79
[32m[20230113 19:51:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.86
[32m[20230113 19:51:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 133.22
[32m[20230113 19:51:10 @agent_ppo2.py:144][0m Total time:       6.62 min
[32m[20230113 19:51:10 @agent_ppo2.py:146][0m 583680 total steps have happened
[32m[20230113 19:51:10 @agent_ppo2.py:122][0m #------------------------ Iteration 285 --------------------------#
[32m[20230113 19:51:10 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 19:51:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:10 @agent_ppo2.py:186][0m |           0.0032 |          13.1874 |           3.5966 |
[32m[20230113 19:51:10 @agent_ppo2.py:186][0m |          -0.0029 |           7.0245 |           3.5935 |
[32m[20230113 19:51:10 @agent_ppo2.py:186][0m |           0.0098 |           6.4805 |           3.5977 |
[32m[20230113 19:51:10 @agent_ppo2.py:186][0m |          -0.0009 |           6.4739 |           3.5963 |
[32m[20230113 19:51:10 @agent_ppo2.py:186][0m |          -0.0100 |           5.6102 |           3.5967 |
[32m[20230113 19:51:10 @agent_ppo2.py:186][0m |           0.0022 |           5.4024 |           3.5957 |
[32m[20230113 19:51:10 @agent_ppo2.py:186][0m |          -0.0061 |           5.4232 |           3.5958 |
[32m[20230113 19:51:11 @agent_ppo2.py:186][0m |          -0.0086 |           5.2191 |           3.5962 |
[32m[20230113 19:51:11 @agent_ppo2.py:186][0m |          -0.0056 |           5.1059 |           3.5940 |
[32m[20230113 19:51:11 @agent_ppo2.py:186][0m |          -0.0113 |           5.0045 |           3.5948 |
[32m[20230113 19:51:11 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 19:51:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 114.70
[32m[20230113 19:51:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.54
[32m[20230113 19:51:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.56
[32m[20230113 19:51:11 @agent_ppo2.py:144][0m Total time:       6.64 min
[32m[20230113 19:51:11 @agent_ppo2.py:146][0m 585728 total steps have happened
[32m[20230113 19:51:11 @agent_ppo2.py:122][0m #------------------------ Iteration 286 --------------------------#
[32m[20230113 19:51:11 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 19:51:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:12 @agent_ppo2.py:186][0m |          -0.0014 |          16.0482 |           3.5764 |
[32m[20230113 19:51:12 @agent_ppo2.py:186][0m |          -0.0074 |           8.9360 |           3.5734 |
[32m[20230113 19:51:12 @agent_ppo2.py:186][0m |          -0.0109 |           5.8668 |           3.5732 |
[32m[20230113 19:51:12 @agent_ppo2.py:186][0m |          -0.0123 |           4.3866 |           3.5729 |
[32m[20230113 19:51:12 @agent_ppo2.py:186][0m |          -0.0127 |           3.6061 |           3.5735 |
[32m[20230113 19:51:12 @agent_ppo2.py:186][0m |          -0.0132 |           3.1660 |           3.5732 |
[32m[20230113 19:51:12 @agent_ppo2.py:186][0m |          -0.0136 |           2.9078 |           3.5722 |
[32m[20230113 19:51:12 @agent_ppo2.py:186][0m |          -0.0141 |           2.7202 |           3.5729 |
[32m[20230113 19:51:12 @agent_ppo2.py:186][0m |          -0.0146 |           2.5651 |           3.5730 |
[32m[20230113 19:51:12 @agent_ppo2.py:186][0m |          -0.0152 |           2.4829 |           3.5738 |
[32m[20230113 19:51:12 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 19:51:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 129.17
[32m[20230113 19:51:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.61
[32m[20230113 19:51:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 229.05
[32m[20230113 19:51:12 @agent_ppo2.py:144][0m Total time:       6.66 min
[32m[20230113 19:51:12 @agent_ppo2.py:146][0m 587776 total steps have happened
[32m[20230113 19:51:12 @agent_ppo2.py:122][0m #------------------------ Iteration 287 --------------------------#
[32m[20230113 19:51:13 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:13 @agent_ppo2.py:186][0m |          -0.0007 |           8.6993 |           3.6782 |
[32m[20230113 19:51:13 @agent_ppo2.py:186][0m |          -0.0052 |           6.0048 |           3.6740 |
[32m[20230113 19:51:13 @agent_ppo2.py:186][0m |          -0.0065 |           5.5970 |           3.6674 |
[32m[20230113 19:51:13 @agent_ppo2.py:186][0m |          -0.0082 |           5.3858 |           3.6657 |
[32m[20230113 19:51:13 @agent_ppo2.py:186][0m |          -0.0088 |           5.1881 |           3.6642 |
[32m[20230113 19:51:13 @agent_ppo2.py:186][0m |          -0.0099 |           5.0377 |           3.6634 |
[32m[20230113 19:51:13 @agent_ppo2.py:186][0m |          -0.0103 |           4.9318 |           3.6631 |
[32m[20230113 19:51:13 @agent_ppo2.py:186][0m |          -0.0108 |           4.8511 |           3.6621 |
[32m[20230113 19:51:13 @agent_ppo2.py:186][0m |          -0.0115 |           4.7579 |           3.6605 |
[32m[20230113 19:51:13 @agent_ppo2.py:186][0m |          -0.0116 |           4.7116 |           3.6578 |
[32m[20230113 19:51:13 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 214.82
[32m[20230113 19:51:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 215.71
[32m[20230113 19:51:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 229.40
[32m[20230113 19:51:14 @agent_ppo2.py:144][0m Total time:       6.69 min
[32m[20230113 19:51:14 @agent_ppo2.py:146][0m 589824 total steps have happened
[32m[20230113 19:51:14 @agent_ppo2.py:122][0m #------------------------ Iteration 288 --------------------------#
[32m[20230113 19:51:14 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:14 @agent_ppo2.py:186][0m |           0.0008 |           3.1210 |           3.6952 |
[32m[20230113 19:51:14 @agent_ppo2.py:186][0m |          -0.0029 |           2.8005 |           3.6922 |
[32m[20230113 19:51:14 @agent_ppo2.py:186][0m |          -0.0045 |           2.7167 |           3.6904 |
[32m[20230113 19:51:14 @agent_ppo2.py:186][0m |          -0.0059 |           2.6586 |           3.6862 |
[32m[20230113 19:51:14 @agent_ppo2.py:186][0m |          -0.0065 |           2.6252 |           3.6875 |
[32m[20230113 19:51:14 @agent_ppo2.py:186][0m |          -0.0069 |           2.5976 |           3.6889 |
[32m[20230113 19:51:15 @agent_ppo2.py:186][0m |          -0.0079 |           2.5587 |           3.6862 |
[32m[20230113 19:51:15 @agent_ppo2.py:186][0m |          -0.0077 |           2.5533 |           3.6887 |
[32m[20230113 19:51:15 @agent_ppo2.py:186][0m |          -0.0084 |           2.5252 |           3.6876 |
[32m[20230113 19:51:15 @agent_ppo2.py:186][0m |          -0.0093 |           2.5120 |           3.6880 |
[32m[20230113 19:51:15 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.40
[32m[20230113 19:51:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.22
[32m[20230113 19:51:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.36
[32m[20230113 19:51:15 @agent_ppo2.py:144][0m Total time:       6.71 min
[32m[20230113 19:51:15 @agent_ppo2.py:146][0m 591872 total steps have happened
[32m[20230113 19:51:15 @agent_ppo2.py:122][0m #------------------------ Iteration 289 --------------------------#
[32m[20230113 19:51:16 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:16 @agent_ppo2.py:186][0m |          -0.0004 |           5.3731 |           3.6989 |
[32m[20230113 19:51:16 @agent_ppo2.py:186][0m |          -0.0051 |           3.8905 |           3.6953 |
[32m[20230113 19:51:16 @agent_ppo2.py:186][0m |          -0.0078 |           3.6438 |           3.7040 |
[32m[20230113 19:51:16 @agent_ppo2.py:186][0m |          -0.0091 |           3.4925 |           3.7026 |
[32m[20230113 19:51:16 @agent_ppo2.py:186][0m |          -0.0105 |           3.3754 |           3.7025 |
[32m[20230113 19:51:16 @agent_ppo2.py:186][0m |          -0.0112 |           3.2809 |           3.7026 |
[32m[20230113 19:51:16 @agent_ppo2.py:186][0m |          -0.0121 |           3.2005 |           3.7027 |
[32m[20230113 19:51:16 @agent_ppo2.py:186][0m |          -0.0128 |           3.1416 |           3.7028 |
[32m[20230113 19:51:16 @agent_ppo2.py:186][0m |          -0.0129 |           3.0933 |           3.7011 |
[32m[20230113 19:51:16 @agent_ppo2.py:186][0m |          -0.0133 |           3.0290 |           3.7051 |
[32m[20230113 19:51:16 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 212.87
[32m[20230113 19:51:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 213.35
[32m[20230113 19:51:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.02
[32m[20230113 19:51:17 @agent_ppo2.py:144][0m Total time:       6.73 min
[32m[20230113 19:51:17 @agent_ppo2.py:146][0m 593920 total steps have happened
[32m[20230113 19:51:17 @agent_ppo2.py:122][0m #------------------------ Iteration 290 --------------------------#
[32m[20230113 19:51:17 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:17 @agent_ppo2.py:186][0m |           0.0001 |           2.9132 |           3.6325 |
[32m[20230113 19:51:17 @agent_ppo2.py:186][0m |          -0.0034 |           2.5802 |           3.6272 |
[32m[20230113 19:51:17 @agent_ppo2.py:186][0m |          -0.0056 |           2.3957 |           3.6241 |
[32m[20230113 19:51:17 @agent_ppo2.py:186][0m |          -0.0064 |           2.3014 |           3.6221 |
[32m[20230113 19:51:17 @agent_ppo2.py:186][0m |          -0.0075 |           2.2181 |           3.6244 |
[32m[20230113 19:51:17 @agent_ppo2.py:186][0m |          -0.0084 |           2.1448 |           3.6230 |
[32m[20230113 19:51:17 @agent_ppo2.py:186][0m |          -0.0090 |           2.0918 |           3.6222 |
[32m[20230113 19:51:17 @agent_ppo2.py:186][0m |          -0.0096 |           2.0324 |           3.6225 |
[32m[20230113 19:51:17 @agent_ppo2.py:186][0m |          -0.0102 |           1.9851 |           3.6205 |
[32m[20230113 19:51:17 @agent_ppo2.py:186][0m |          -0.0102 |           1.9437 |           3.6188 |
[32m[20230113 19:51:17 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 207.80
[32m[20230113 19:51:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 211.64
[32m[20230113 19:51:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.44
[32m[20230113 19:51:18 @agent_ppo2.py:144][0m Total time:       6.76 min
[32m[20230113 19:51:18 @agent_ppo2.py:146][0m 595968 total steps have happened
[32m[20230113 19:51:18 @agent_ppo2.py:122][0m #------------------------ Iteration 291 --------------------------#
[32m[20230113 19:51:18 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:51:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:18 @agent_ppo2.py:186][0m |           0.0018 |           4.9394 |           3.6217 |
[32m[20230113 19:51:19 @agent_ppo2.py:186][0m |           0.0005 |           4.6759 |           3.6203 |
[32m[20230113 19:51:19 @agent_ppo2.py:186][0m |          -0.0061 |           4.5808 |           3.6186 |
[32m[20230113 19:51:19 @agent_ppo2.py:186][0m |          -0.0017 |           4.5207 |           3.6118 |
[32m[20230113 19:51:19 @agent_ppo2.py:186][0m |          -0.0113 |           4.4888 |           3.6099 |
[32m[20230113 19:51:19 @agent_ppo2.py:186][0m |           0.0004 |           4.5428 |           3.6068 |
[32m[20230113 19:51:19 @agent_ppo2.py:186][0m |          -0.0107 |           4.4393 |           3.6076 |
[32m[20230113 19:51:19 @agent_ppo2.py:186][0m |          -0.0085 |           4.3916 |           3.6076 |
[32m[20230113 19:51:19 @agent_ppo2.py:186][0m |          -0.0151 |           4.3701 |           3.6079 |
[32m[20230113 19:51:19 @agent_ppo2.py:186][0m |           0.0265 |           5.6654 |           3.6066 |
[32m[20230113 19:51:19 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.83
[32m[20230113 19:51:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.30
[32m[20230113 19:51:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.93
[32m[20230113 19:51:19 @agent_ppo2.py:144][0m Total time:       6.78 min
[32m[20230113 19:51:19 @agent_ppo2.py:146][0m 598016 total steps have happened
[32m[20230113 19:51:19 @agent_ppo2.py:122][0m #------------------------ Iteration 292 --------------------------#
[32m[20230113 19:51:20 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:51:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:20 @agent_ppo2.py:186][0m |          -0.0013 |           2.9816 |           3.6892 |
[32m[20230113 19:51:20 @agent_ppo2.py:186][0m |          -0.0056 |           2.7651 |           3.6838 |
[32m[20230113 19:51:20 @agent_ppo2.py:186][0m |          -0.0077 |           2.7138 |           3.6832 |
[32m[20230113 19:51:20 @agent_ppo2.py:186][0m |          -0.0083 |           2.6733 |           3.6814 |
[32m[20230113 19:51:20 @agent_ppo2.py:186][0m |          -0.0092 |           2.6438 |           3.6789 |
[32m[20230113 19:51:20 @agent_ppo2.py:186][0m |          -0.0098 |           2.6348 |           3.6776 |
[32m[20230113 19:51:20 @agent_ppo2.py:186][0m |          -0.0104 |           2.6119 |           3.6787 |
[32m[20230113 19:51:20 @agent_ppo2.py:186][0m |          -0.0108 |           2.6070 |           3.6760 |
[32m[20230113 19:51:20 @agent_ppo2.py:186][0m |          -0.0106 |           2.5856 |           3.6768 |
[32m[20230113 19:51:20 @agent_ppo2.py:186][0m |          -0.0115 |           2.5792 |           3.6771 |
[32m[20230113 19:51:20 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 19:51:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 212.51
[32m[20230113 19:51:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 216.47
[32m[20230113 19:51:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 138.75
[32m[20230113 19:51:21 @agent_ppo2.py:144][0m Total time:       6.80 min
[32m[20230113 19:51:21 @agent_ppo2.py:146][0m 600064 total steps have happened
[32m[20230113 19:51:21 @agent_ppo2.py:122][0m #------------------------ Iteration 293 --------------------------#
[32m[20230113 19:51:21 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:21 @agent_ppo2.py:186][0m |          -0.0023 |           2.7685 |           3.6059 |
[32m[20230113 19:51:21 @agent_ppo2.py:186][0m |          -0.0038 |           2.6945 |           3.5993 |
[32m[20230113 19:51:21 @agent_ppo2.py:186][0m |          -0.0065 |           2.6497 |           3.5983 |
[32m[20230113 19:51:21 @agent_ppo2.py:186][0m |          -0.0057 |           2.6166 |           3.5975 |
[32m[20230113 19:51:21 @agent_ppo2.py:186][0m |          -0.0052 |           2.5963 |           3.5989 |
[32m[20230113 19:51:22 @agent_ppo2.py:186][0m |          -0.0063 |           2.5798 |           3.6011 |
[32m[20230113 19:51:22 @agent_ppo2.py:186][0m |          -0.0081 |           2.5623 |           3.6014 |
[32m[20230113 19:51:22 @agent_ppo2.py:186][0m |          -0.0080 |           2.5479 |           3.6031 |
[32m[20230113 19:51:22 @agent_ppo2.py:186][0m |          -0.0080 |           2.5247 |           3.6046 |
[32m[20230113 19:51:22 @agent_ppo2.py:186][0m |          -0.0077 |           2.5091 |           3.6043 |
[32m[20230113 19:51:22 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 215.33
[32m[20230113 19:51:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.74
[32m[20230113 19:51:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 226.02
[32m[20230113 19:51:22 @agent_ppo2.py:144][0m Total time:       6.83 min
[32m[20230113 19:51:22 @agent_ppo2.py:146][0m 602112 total steps have happened
[32m[20230113 19:51:22 @agent_ppo2.py:122][0m #------------------------ Iteration 294 --------------------------#
[32m[20230113 19:51:23 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:51:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:23 @agent_ppo2.py:186][0m |          -0.0006 |           4.8871 |           3.6087 |
[32m[20230113 19:51:23 @agent_ppo2.py:186][0m |          -0.0031 |           4.6922 |           3.6057 |
[32m[20230113 19:51:23 @agent_ppo2.py:186][0m |          -0.0057 |           4.5655 |           3.6050 |
[32m[20230113 19:51:23 @agent_ppo2.py:186][0m |          -0.0081 |           4.5014 |           3.6041 |
[32m[20230113 19:51:23 @agent_ppo2.py:186][0m |          -0.0082 |           4.4255 |           3.5998 |
[32m[20230113 19:51:23 @agent_ppo2.py:186][0m |          -0.0073 |           4.3786 |           3.6032 |
[32m[20230113 19:51:23 @agent_ppo2.py:186][0m |          -0.0079 |           4.3689 |           3.6003 |
[32m[20230113 19:51:23 @agent_ppo2.py:186][0m |          -0.0106 |           4.2918 |           3.6018 |
[32m[20230113 19:51:23 @agent_ppo2.py:186][0m |          -0.0062 |           4.4895 |           3.6028 |
[32m[20230113 19:51:23 @agent_ppo2.py:186][0m |          -0.0108 |           4.2066 |           3.6049 |
[32m[20230113 19:51:23 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.32
[32m[20230113 19:51:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.88
[32m[20230113 19:51:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 222.46
[32m[20230113 19:51:24 @agent_ppo2.py:144][0m Total time:       6.85 min
[32m[20230113 19:51:24 @agent_ppo2.py:146][0m 604160 total steps have happened
[32m[20230113 19:51:24 @agent_ppo2.py:122][0m #------------------------ Iteration 295 --------------------------#
[32m[20230113 19:51:24 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:51:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:24 @agent_ppo2.py:186][0m |           0.0012 |           3.7466 |           3.6333 |
[32m[20230113 19:51:24 @agent_ppo2.py:186][0m |          -0.0005 |           3.6353 |           3.6331 |
[32m[20230113 19:51:24 @agent_ppo2.py:186][0m |          -0.0021 |           3.5885 |           3.6365 |
[32m[20230113 19:51:24 @agent_ppo2.py:186][0m |          -0.0053 |           3.4812 |           3.6356 |
[32m[20230113 19:51:24 @agent_ppo2.py:186][0m |          -0.0067 |           3.4704 |           3.6364 |
[32m[20230113 19:51:24 @agent_ppo2.py:186][0m |          -0.0059 |           3.4635 |           3.6343 |
[32m[20230113 19:51:24 @agent_ppo2.py:186][0m |          -0.0066 |           3.4443 |           3.6352 |
[32m[20230113 19:51:24 @agent_ppo2.py:186][0m |          -0.0082 |           3.3904 |           3.6354 |
[32m[20230113 19:51:25 @agent_ppo2.py:186][0m |          -0.0108 |           3.3776 |           3.6363 |
[32m[20230113 19:51:25 @agent_ppo2.py:186][0m |          -0.0089 |           3.3724 |           3.6361 |
[32m[20230113 19:51:25 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 209.39
[32m[20230113 19:51:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.06
[32m[20230113 19:51:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 224.90
[32m[20230113 19:51:25 @agent_ppo2.py:144][0m Total time:       6.87 min
[32m[20230113 19:51:25 @agent_ppo2.py:146][0m 606208 total steps have happened
[32m[20230113 19:51:25 @agent_ppo2.py:122][0m #------------------------ Iteration 296 --------------------------#
[32m[20230113 19:51:25 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:26 @agent_ppo2.py:186][0m |          -0.0007 |           3.6211 |           3.6666 |
[32m[20230113 19:51:26 @agent_ppo2.py:186][0m |          -0.0036 |           3.4312 |           3.6630 |
[32m[20230113 19:51:26 @agent_ppo2.py:186][0m |          -0.0055 |           3.3225 |           3.6610 |
[32m[20230113 19:51:26 @agent_ppo2.py:186][0m |          -0.0064 |           3.2751 |           3.6582 |
[32m[20230113 19:51:26 @agent_ppo2.py:186][0m |          -0.0076 |           3.2026 |           3.6585 |
[32m[20230113 19:51:26 @agent_ppo2.py:186][0m |          -0.0081 |           3.1438 |           3.6560 |
[32m[20230113 19:51:26 @agent_ppo2.py:186][0m |          -0.0081 |           3.0763 |           3.6532 |
[32m[20230113 19:51:26 @agent_ppo2.py:186][0m |          -0.0086 |           2.9974 |           3.6505 |
[32m[20230113 19:51:26 @agent_ppo2.py:186][0m |          -0.0088 |           2.9055 |           3.6512 |
[32m[20230113 19:51:26 @agent_ppo2.py:186][0m |          -0.0094 |           2.8539 |           3.6518 |
[32m[20230113 19:51:26 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 211.63
[32m[20230113 19:51:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 215.09
[32m[20230113 19:51:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.54
[32m[20230113 19:51:26 @agent_ppo2.py:144][0m Total time:       6.90 min
[32m[20230113 19:51:26 @agent_ppo2.py:146][0m 608256 total steps have happened
[32m[20230113 19:51:26 @agent_ppo2.py:122][0m #------------------------ Iteration 297 --------------------------#
[32m[20230113 19:51:27 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:51:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:27 @agent_ppo2.py:186][0m |           0.0063 |           5.5839 |           3.6399 |
[32m[20230113 19:51:27 @agent_ppo2.py:186][0m |           0.0014 |           5.3708 |           3.6304 |
[32m[20230113 19:51:27 @agent_ppo2.py:186][0m |          -0.0074 |           4.9655 |           3.6259 |
[32m[20230113 19:51:27 @agent_ppo2.py:186][0m |          -0.0066 |           4.9843 |           3.6211 |
[32m[20230113 19:51:27 @agent_ppo2.py:186][0m |          -0.0046 |           5.0081 |           3.6157 |
[32m[20230113 19:51:27 @agent_ppo2.py:186][0m |          -0.0094 |           4.8408 |           3.6148 |
[32m[20230113 19:51:27 @agent_ppo2.py:186][0m |          -0.0099 |           4.8141 |           3.6146 |
[32m[20230113 19:51:27 @agent_ppo2.py:186][0m |          -0.0086 |           4.8875 |           3.6142 |
[32m[20230113 19:51:27 @agent_ppo2.py:186][0m |          -0.0085 |           4.8196 |           3.6116 |
[32m[20230113 19:51:27 @agent_ppo2.py:186][0m |          -0.0105 |           4.6819 |           3.6127 |
[32m[20230113 19:51:27 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 218.27
[32m[20230113 19:51:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.56
[32m[20230113 19:51:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.71
[32m[20230113 19:51:28 @agent_ppo2.py:144][0m Total time:       6.92 min
[32m[20230113 19:51:28 @agent_ppo2.py:146][0m 610304 total steps have happened
[32m[20230113 19:51:28 @agent_ppo2.py:122][0m #------------------------ Iteration 298 --------------------------#
[32m[20230113 19:51:28 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 19:51:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:28 @agent_ppo2.py:186][0m |           0.0040 |          11.2337 |           3.6259 |
[32m[20230113 19:51:28 @agent_ppo2.py:186][0m |          -0.0032 |           6.1390 |           3.6231 |
[32m[20230113 19:51:28 @agent_ppo2.py:186][0m |          -0.0077 |           4.9957 |           3.6221 |
[32m[20230113 19:51:28 @agent_ppo2.py:186][0m |          -0.0089 |           4.7309 |           3.6231 |
[32m[20230113 19:51:28 @agent_ppo2.py:186][0m |          -0.0093 |           4.5588 |           3.6229 |
[32m[20230113 19:51:29 @agent_ppo2.py:186][0m |          -0.0111 |           4.3679 |           3.6207 |
[32m[20230113 19:51:29 @agent_ppo2.py:186][0m |          -0.0126 |           4.2273 |           3.6199 |
[32m[20230113 19:51:29 @agent_ppo2.py:186][0m |          -0.0131 |           4.1186 |           3.6178 |
[32m[20230113 19:51:29 @agent_ppo2.py:186][0m |          -0.0145 |           3.9920 |           3.6190 |
[32m[20230113 19:51:29 @agent_ppo2.py:186][0m |          -0.0093 |           4.1181 |           3.6185 |
[32m[20230113 19:51:29 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 19:51:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 120.48
[32m[20230113 19:51:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 209.11
[32m[20230113 19:51:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 218.47
[32m[20230113 19:51:29 @agent_ppo2.py:144][0m Total time:       6.94 min
[32m[20230113 19:51:29 @agent_ppo2.py:146][0m 612352 total steps have happened
[32m[20230113 19:51:29 @agent_ppo2.py:122][0m #------------------------ Iteration 299 --------------------------#
[32m[20230113 19:51:30 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:30 @agent_ppo2.py:186][0m |           0.0013 |           6.5363 |           3.7514 |
[32m[20230113 19:51:30 @agent_ppo2.py:186][0m |          -0.0014 |           5.1497 |           3.7480 |
[32m[20230113 19:51:30 @agent_ppo2.py:186][0m |          -0.0048 |           4.8350 |           3.7471 |
[32m[20230113 19:51:30 @agent_ppo2.py:186][0m |          -0.0052 |           4.6114 |           3.7451 |
[32m[20230113 19:51:30 @agent_ppo2.py:186][0m |          -0.0067 |           4.3489 |           3.7432 |
[32m[20230113 19:51:30 @agent_ppo2.py:186][0m |          -0.0070 |           4.2183 |           3.7406 |
[32m[20230113 19:51:30 @agent_ppo2.py:186][0m |          -0.0074 |           4.1103 |           3.7397 |
[32m[20230113 19:51:30 @agent_ppo2.py:186][0m |          -0.0085 |           4.0040 |           3.7387 |
[32m[20230113 19:51:30 @agent_ppo2.py:186][0m |          -0.0092 |           3.9533 |           3.7392 |
[32m[20230113 19:51:30 @agent_ppo2.py:186][0m |          -0.0091 |           3.8715 |           3.7345 |
[32m[20230113 19:51:30 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 206.70
[32m[20230113 19:51:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 212.62
[32m[20230113 19:51:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 219.33
[32m[20230113 19:51:31 @agent_ppo2.py:144][0m Total time:       6.97 min
[32m[20230113 19:51:31 @agent_ppo2.py:146][0m 614400 total steps have happened
[32m[20230113 19:51:31 @agent_ppo2.py:122][0m #------------------------ Iteration 300 --------------------------#
[32m[20230113 19:51:31 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:31 @agent_ppo2.py:186][0m |           0.0018 |           2.7988 |           3.6588 |
[32m[20230113 19:51:31 @agent_ppo2.py:186][0m |          -0.0031 |           2.6089 |           3.6563 |
[32m[20230113 19:51:31 @agent_ppo2.py:186][0m |          -0.0045 |           2.5300 |           3.6553 |
[32m[20230113 19:51:31 @agent_ppo2.py:186][0m |          -0.0057 |           2.4846 |           3.6583 |
[32m[20230113 19:51:31 @agent_ppo2.py:186][0m |          -0.0066 |           2.4328 |           3.6574 |
[32m[20230113 19:51:31 @agent_ppo2.py:186][0m |          -0.0073 |           2.4068 |           3.6600 |
[32m[20230113 19:51:31 @agent_ppo2.py:186][0m |          -0.0080 |           2.3808 |           3.6599 |
[32m[20230113 19:51:31 @agent_ppo2.py:186][0m |          -0.0078 |           2.3642 |           3.6606 |
[32m[20230113 19:51:31 @agent_ppo2.py:186][0m |          -0.0085 |           2.3450 |           3.6607 |
[32m[20230113 19:51:32 @agent_ppo2.py:186][0m |          -0.0088 |           2.3204 |           3.6601 |
[32m[20230113 19:51:32 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 210.74
[32m[20230113 19:51:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 211.02
[32m[20230113 19:51:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 228.13
[32m[20230113 19:51:32 @agent_ppo2.py:144][0m Total time:       6.99 min
[32m[20230113 19:51:32 @agent_ppo2.py:146][0m 616448 total steps have happened
[32m[20230113 19:51:32 @agent_ppo2.py:122][0m #------------------------ Iteration 301 --------------------------#
[32m[20230113 19:51:32 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:33 @agent_ppo2.py:186][0m |           0.0016 |           3.4225 |           3.6488 |
[32m[20230113 19:51:33 @agent_ppo2.py:186][0m |          -0.0023 |           3.2786 |           3.6460 |
[32m[20230113 19:51:33 @agent_ppo2.py:186][0m |          -0.0064 |           3.1411 |           3.6421 |
[32m[20230113 19:51:33 @agent_ppo2.py:186][0m |          -0.0046 |           3.1153 |           3.6429 |
[32m[20230113 19:51:33 @agent_ppo2.py:186][0m |          -0.0061 |           3.0462 |           3.6435 |
[32m[20230113 19:51:33 @agent_ppo2.py:186][0m |          -0.0061 |           3.0598 |           3.6430 |
[32m[20230113 19:51:33 @agent_ppo2.py:186][0m |          -0.0073 |           3.0224 |           3.6429 |
[32m[20230113 19:51:33 @agent_ppo2.py:186][0m |          -0.0080 |           2.9709 |           3.6444 |
[32m[20230113 19:51:33 @agent_ppo2.py:186][0m |          -0.0084 |           2.9745 |           3.6441 |
[32m[20230113 19:51:33 @agent_ppo2.py:186][0m |          -0.0087 |           2.9350 |           3.6453 |
[32m[20230113 19:51:33 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 215.62
[32m[20230113 19:51:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.06
[32m[20230113 19:51:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 227.06
[32m[20230113 19:51:33 @agent_ppo2.py:144][0m Total time:       7.01 min
[32m[20230113 19:51:33 @agent_ppo2.py:146][0m 618496 total steps have happened
[32m[20230113 19:51:33 @agent_ppo2.py:122][0m #------------------------ Iteration 302 --------------------------#
[32m[20230113 19:51:34 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:34 @agent_ppo2.py:186][0m |           0.0000 |           2.9584 |           3.7238 |
[32m[20230113 19:51:34 @agent_ppo2.py:186][0m |          -0.0045 |           2.6724 |           3.7209 |
[32m[20230113 19:51:34 @agent_ppo2.py:186][0m |          -0.0065 |           2.5229 |           3.7146 |
[32m[20230113 19:51:34 @agent_ppo2.py:186][0m |          -0.0077 |           2.4319 |           3.7133 |
[32m[20230113 19:51:34 @agent_ppo2.py:186][0m |          -0.0086 |           2.3566 |           3.7110 |
[32m[20230113 19:51:34 @agent_ppo2.py:186][0m |          -0.0094 |           2.2937 |           3.7104 |
[32m[20230113 19:51:34 @agent_ppo2.py:186][0m |          -0.0099 |           2.2430 |           3.7083 |
[32m[20230113 19:51:34 @agent_ppo2.py:186][0m |          -0.0104 |           2.1850 |           3.7068 |
[32m[20230113 19:51:34 @agent_ppo2.py:186][0m |          -0.0106 |           2.1381 |           3.7065 |
[32m[20230113 19:51:34 @agent_ppo2.py:186][0m |          -0.0111 |           2.0916 |           3.7026 |
[32m[20230113 19:51:34 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 215.25
[32m[20230113 19:51:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.25
[32m[20230113 19:51:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.62
[32m[20230113 19:51:35 @agent_ppo2.py:144][0m Total time:       7.04 min
[32m[20230113 19:51:35 @agent_ppo2.py:146][0m 620544 total steps have happened
[32m[20230113 19:51:35 @agent_ppo2.py:122][0m #------------------------ Iteration 303 --------------------------#
[32m[20230113 19:51:35 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:35 @agent_ppo2.py:186][0m |          -0.0002 |           2.3111 |           3.7135 |
[32m[20230113 19:51:35 @agent_ppo2.py:186][0m |          -0.0038 |           2.2035 |           3.7069 |
[32m[20230113 19:51:35 @agent_ppo2.py:186][0m |          -0.0049 |           2.1360 |           3.7022 |
[32m[20230113 19:51:36 @agent_ppo2.py:186][0m |          -0.0056 |           2.0960 |           3.7004 |
[32m[20230113 19:51:36 @agent_ppo2.py:186][0m |          -0.0065 |           2.0710 |           3.6972 |
[32m[20230113 19:51:36 @agent_ppo2.py:186][0m |          -0.0070 |           2.0400 |           3.6957 |
[32m[20230113 19:51:36 @agent_ppo2.py:186][0m |          -0.0073 |           2.0249 |           3.6927 |
[32m[20230113 19:51:36 @agent_ppo2.py:186][0m |          -0.0077 |           2.0076 |           3.6949 |
[32m[20230113 19:51:36 @agent_ppo2.py:186][0m |          -0.0080 |           1.9793 |           3.6919 |
[32m[20230113 19:51:36 @agent_ppo2.py:186][0m |          -0.0086 |           1.9773 |           3.6908 |
[32m[20230113 19:51:36 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 210.86
[32m[20230113 19:51:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 211.81
[32m[20230113 19:51:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 228.91
[32m[20230113 19:51:36 @agent_ppo2.py:144][0m Total time:       7.06 min
[32m[20230113 19:51:36 @agent_ppo2.py:146][0m 622592 total steps have happened
[32m[20230113 19:51:36 @agent_ppo2.py:122][0m #------------------------ Iteration 304 --------------------------#
[32m[20230113 19:51:37 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:51:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:37 @agent_ppo2.py:186][0m |          -0.0022 |          11.6052 |           3.6798 |
[32m[20230113 19:51:37 @agent_ppo2.py:186][0m |          -0.0066 |           6.8003 |           3.6747 |
[32m[20230113 19:51:37 @agent_ppo2.py:186][0m |          -0.0061 |           6.3179 |           3.6700 |
[32m[20230113 19:51:37 @agent_ppo2.py:186][0m |          -0.0086 |           5.6454 |           3.6680 |
[32m[20230113 19:51:37 @agent_ppo2.py:186][0m |          -0.0088 |           5.1948 |           3.6691 |
[32m[20230113 19:51:37 @agent_ppo2.py:186][0m |          -0.0097 |           4.9001 |           3.6663 |
[32m[20230113 19:51:37 @agent_ppo2.py:186][0m |          -0.0109 |           4.5231 |           3.6656 |
[32m[20230113 19:51:37 @agent_ppo2.py:186][0m |          -0.0095 |           4.4426 |           3.6633 |
[32m[20230113 19:51:37 @agent_ppo2.py:186][0m |          -0.0112 |           4.2356 |           3.6648 |
[32m[20230113 19:51:37 @agent_ppo2.py:186][0m |          -0.0105 |           4.1381 |           3.6621 |
[32m[20230113 19:51:37 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 19:51:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 112.19
[32m[20230113 19:51:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.43
[32m[20230113 19:51:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 226.47
[32m[20230113 19:51:38 @agent_ppo2.py:144][0m Total time:       7.09 min
[32m[20230113 19:51:38 @agent_ppo2.py:146][0m 624640 total steps have happened
[32m[20230113 19:51:38 @agent_ppo2.py:122][0m #------------------------ Iteration 305 --------------------------#
[32m[20230113 19:51:38 @agent_ppo2.py:128][0m Sampling time: 0.35 s by 1 slaves
[32m[20230113 19:51:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:38 @agent_ppo2.py:186][0m |          -0.0007 |          15.9060 |           3.5824 |
[32m[20230113 19:51:38 @agent_ppo2.py:186][0m |          -0.0068 |           8.1777 |           3.5812 |
[32m[20230113 19:51:38 @agent_ppo2.py:186][0m |          -0.0051 |           6.9363 |           3.5787 |
[32m[20230113 19:51:38 @agent_ppo2.py:186][0m |          -0.0108 |           6.2921 |           3.5784 |
[32m[20230113 19:51:38 @agent_ppo2.py:186][0m |          -0.0088 |           6.0601 |           3.5824 |
[32m[20230113 19:51:38 @agent_ppo2.py:186][0m |          -0.0131 |           5.8855 |           3.5821 |
[32m[20230113 19:51:38 @agent_ppo2.py:186][0m |          -0.0157 |           5.6708 |           3.5809 |
[32m[20230113 19:51:38 @agent_ppo2.py:186][0m |          -0.0140 |           5.5439 |           3.5788 |
[32m[20230113 19:51:38 @agent_ppo2.py:186][0m |          -0.0148 |           5.5568 |           3.5799 |
[32m[20230113 19:51:38 @agent_ppo2.py:186][0m |          -0.0142 |           5.2745 |           3.5812 |
[32m[20230113 19:51:38 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 19:51:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 92.05
[32m[20230113 19:51:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 191.93
[32m[20230113 19:51:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.65
[32m[20230113 19:51:39 @agent_ppo2.py:144][0m Total time:       7.11 min
[32m[20230113 19:51:39 @agent_ppo2.py:146][0m 626688 total steps have happened
[32m[20230113 19:51:39 @agent_ppo2.py:122][0m #------------------------ Iteration 306 --------------------------#
[32m[20230113 19:51:39 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:39 @agent_ppo2.py:186][0m |           0.0002 |           3.1100 |           3.6154 |
[32m[20230113 19:51:39 @agent_ppo2.py:186][0m |          -0.0032 |           2.9059 |           3.6117 |
[32m[20230113 19:51:40 @agent_ppo2.py:186][0m |          -0.0061 |           2.8335 |           3.6078 |
[32m[20230113 19:51:40 @agent_ppo2.py:186][0m |          -0.0072 |           2.7904 |           3.6081 |
[32m[20230113 19:51:40 @agent_ppo2.py:186][0m |          -0.0086 |           2.7665 |           3.6073 |
[32m[20230113 19:51:40 @agent_ppo2.py:186][0m |          -0.0094 |           2.7324 |           3.6099 |
[32m[20230113 19:51:40 @agent_ppo2.py:186][0m |          -0.0102 |           2.7103 |           3.6078 |
[32m[20230113 19:51:40 @agent_ppo2.py:186][0m |          -0.0104 |           2.6947 |           3.6102 |
[32m[20230113 19:51:40 @agent_ppo2.py:186][0m |          -0.0113 |           2.6776 |           3.6090 |
[32m[20230113 19:51:40 @agent_ppo2.py:186][0m |          -0.0116 |           2.6710 |           3.6084 |
[32m[20230113 19:51:40 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 205.28
[32m[20230113 19:51:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 214.43
[32m[20230113 19:51:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.56
[32m[20230113 19:51:40 @agent_ppo2.py:144][0m Total time:       7.13 min
[32m[20230113 19:51:40 @agent_ppo2.py:146][0m 628736 total steps have happened
[32m[20230113 19:51:40 @agent_ppo2.py:122][0m #------------------------ Iteration 307 --------------------------#
[32m[20230113 19:51:41 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:51:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:41 @agent_ppo2.py:186][0m |          -0.0007 |           3.4225 |           3.7311 |
[32m[20230113 19:51:41 @agent_ppo2.py:186][0m |          -0.0053 |           3.2808 |           3.7269 |
[32m[20230113 19:51:41 @agent_ppo2.py:186][0m |          -0.0067 |           3.2199 |           3.7236 |
[32m[20230113 19:51:41 @agent_ppo2.py:186][0m |          -0.0082 |           3.1690 |           3.7261 |
[32m[20230113 19:51:41 @agent_ppo2.py:186][0m |          -0.0090 |           3.1427 |           3.7247 |
[32m[20230113 19:51:41 @agent_ppo2.py:186][0m |          -0.0093 |           3.1102 |           3.7195 |
[32m[20230113 19:51:41 @agent_ppo2.py:186][0m |          -0.0101 |           3.0834 |           3.7231 |
[32m[20230113 19:51:41 @agent_ppo2.py:186][0m |          -0.0104 |           3.0606 |           3.7210 |
[32m[20230113 19:51:41 @agent_ppo2.py:186][0m |          -0.0111 |           3.0446 |           3.7222 |
[32m[20230113 19:51:41 @agent_ppo2.py:186][0m |          -0.0116 |           3.0140 |           3.7215 |
[32m[20230113 19:51:41 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 205.54
[32m[20230113 19:51:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 212.03
[32m[20230113 19:51:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 217.33
[32m[20230113 19:51:42 @agent_ppo2.py:144][0m Total time:       7.15 min
[32m[20230113 19:51:42 @agent_ppo2.py:146][0m 630784 total steps have happened
[32m[20230113 19:51:42 @agent_ppo2.py:122][0m #------------------------ Iteration 308 --------------------------#
[32m[20230113 19:51:42 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:51:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:42 @agent_ppo2.py:186][0m |          -0.0004 |           4.4091 |           3.6249 |
[32m[20230113 19:51:42 @agent_ppo2.py:186][0m |          -0.0006 |           4.2748 |           3.6225 |
[32m[20230113 19:51:42 @agent_ppo2.py:186][0m |          -0.0073 |           4.0786 |           3.6206 |
[32m[20230113 19:51:42 @agent_ppo2.py:186][0m |          -0.0098 |           4.0122 |           3.6170 |
[32m[20230113 19:51:42 @agent_ppo2.py:186][0m |          -0.0085 |           3.9824 |           3.6194 |
[32m[20230113 19:51:42 @agent_ppo2.py:186][0m |          -0.0127 |           3.9135 |           3.6201 |
[32m[20230113 19:51:43 @agent_ppo2.py:186][0m |          -0.0123 |           3.8714 |           3.6207 |
[32m[20230113 19:51:43 @agent_ppo2.py:186][0m |          -0.0138 |           3.8534 |           3.6200 |
[32m[20230113 19:51:43 @agent_ppo2.py:186][0m |          -0.0120 |           3.8359 |           3.6228 |
[32m[20230113 19:51:43 @agent_ppo2.py:186][0m |          -0.0121 |           3.8135 |           3.6235 |
[32m[20230113 19:51:43 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.31
[32m[20230113 19:51:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.49
[32m[20230113 19:51:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 204.91
[32m[20230113 19:51:43 @agent_ppo2.py:144][0m Total time:       7.18 min
[32m[20230113 19:51:43 @agent_ppo2.py:146][0m 632832 total steps have happened
[32m[20230113 19:51:43 @agent_ppo2.py:122][0m #------------------------ Iteration 309 --------------------------#
[32m[20230113 19:51:44 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:44 @agent_ppo2.py:186][0m |           0.0004 |           3.5412 |           3.7073 |
[32m[20230113 19:51:44 @agent_ppo2.py:186][0m |          -0.0034 |           3.4352 |           3.7046 |
[32m[20230113 19:51:44 @agent_ppo2.py:186][0m |          -0.0041 |           3.4133 |           3.7038 |
[32m[20230113 19:51:44 @agent_ppo2.py:186][0m |          -0.0047 |           3.3826 |           3.7053 |
[32m[20230113 19:51:44 @agent_ppo2.py:186][0m |          -0.0056 |           3.3434 |           3.7010 |
[32m[20230113 19:51:44 @agent_ppo2.py:186][0m |          -0.0068 |           3.3364 |           3.7008 |
[32m[20230113 19:51:44 @agent_ppo2.py:186][0m |          -0.0067 |           3.2950 |           3.7019 |
[32m[20230113 19:51:44 @agent_ppo2.py:186][0m |          -0.0072 |           3.3052 |           3.7005 |
[32m[20230113 19:51:44 @agent_ppo2.py:186][0m |          -0.0076 |           3.2811 |           3.7016 |
[32m[20230113 19:51:44 @agent_ppo2.py:186][0m |          -0.0086 |           3.2607 |           3.6999 |
[32m[20230113 19:51:44 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 19:51:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 214.32
[32m[20230113 19:51:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 218.13
[32m[20230113 19:51:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 222.68
[32m[20230113 19:51:45 @agent_ppo2.py:144][0m Total time:       7.20 min
[32m[20230113 19:51:45 @agent_ppo2.py:146][0m 634880 total steps have happened
[32m[20230113 19:51:45 @agent_ppo2.py:122][0m #------------------------ Iteration 310 --------------------------#
[32m[20230113 19:51:45 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:45 @agent_ppo2.py:186][0m |           0.0006 |           3.3182 |           3.7090 |
[32m[20230113 19:51:45 @agent_ppo2.py:186][0m |          -0.0022 |           3.2646 |           3.7051 |
[32m[20230113 19:51:45 @agent_ppo2.py:186][0m |          -0.0042 |           3.2071 |           3.7027 |
[32m[20230113 19:51:45 @agent_ppo2.py:186][0m |          -0.0061 |           3.1476 |           3.7003 |
[32m[20230113 19:51:45 @agent_ppo2.py:186][0m |          -0.0072 |           3.1177 |           3.7014 |
[32m[20230113 19:51:45 @agent_ppo2.py:186][0m |          -0.0078 |           3.0899 |           3.7002 |
[32m[20230113 19:51:45 @agent_ppo2.py:186][0m |          -0.0086 |           3.0815 |           3.6976 |
[32m[20230113 19:51:45 @agent_ppo2.py:186][0m |          -0.0090 |           3.0614 |           3.6981 |
[32m[20230113 19:51:46 @agent_ppo2.py:186][0m |          -0.0093 |           3.0199 |           3.6988 |
[32m[20230113 19:51:46 @agent_ppo2.py:186][0m |          -0.0100 |           2.9952 |           3.6973 |
[32m[20230113 19:51:46 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 212.21
[32m[20230113 19:51:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 213.24
[32m[20230113 19:51:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 228.50
[32m[20230113 19:51:46 @agent_ppo2.py:144][0m Total time:       7.22 min
[32m[20230113 19:51:46 @agent_ppo2.py:146][0m 636928 total steps have happened
[32m[20230113 19:51:46 @agent_ppo2.py:122][0m #------------------------ Iteration 311 --------------------------#
[32m[20230113 19:51:46 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:51:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:47 @agent_ppo2.py:186][0m |          -0.0018 |           7.7821 |           3.5760 |
[32m[20230113 19:51:47 @agent_ppo2.py:186][0m |          -0.0047 |           4.1844 |           3.5753 |
[32m[20230113 19:51:47 @agent_ppo2.py:186][0m |          -0.0052 |           3.7152 |           3.5744 |
[32m[20230113 19:51:47 @agent_ppo2.py:186][0m |          -0.0072 |           3.4550 |           3.5736 |
[32m[20230113 19:51:47 @agent_ppo2.py:186][0m |          -0.0101 |           3.2801 |           3.5735 |
[32m[20230113 19:51:47 @agent_ppo2.py:186][0m |          -0.0104 |           3.1633 |           3.5741 |
[32m[20230113 19:51:47 @agent_ppo2.py:186][0m |          -0.0052 |           3.1681 |           3.5781 |
[32m[20230113 19:51:47 @agent_ppo2.py:186][0m |          -0.0117 |           3.0179 |           3.5741 |
[32m[20230113 19:51:47 @agent_ppo2.py:186][0m |          -0.0114 |           2.9511 |           3.5743 |
[32m[20230113 19:51:47 @agent_ppo2.py:186][0m |          -0.0076 |           2.9179 |           3.5761 |
[32m[20230113 19:51:47 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 19:51:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 115.48
[32m[20230113 19:51:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.92
[32m[20230113 19:51:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 226.16
[32m[20230113 19:51:47 @agent_ppo2.py:144][0m Total time:       7.25 min
[32m[20230113 19:51:47 @agent_ppo2.py:146][0m 638976 total steps have happened
[32m[20230113 19:51:47 @agent_ppo2.py:122][0m #------------------------ Iteration 312 --------------------------#
[32m[20230113 19:51:48 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:51:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:48 @agent_ppo2.py:186][0m |          -0.0026 |           3.1677 |           3.6176 |
[32m[20230113 19:51:48 @agent_ppo2.py:186][0m |          -0.0054 |           3.0148 |           3.6130 |
[32m[20230113 19:51:48 @agent_ppo2.py:186][0m |          -0.0018 |           2.9410 |           3.6093 |
[32m[20230113 19:51:48 @agent_ppo2.py:186][0m |          -0.0105 |           2.8921 |           3.6068 |
[32m[20230113 19:51:48 @agent_ppo2.py:186][0m |          -0.0126 |           2.8573 |           3.6060 |
[32m[20230113 19:51:48 @agent_ppo2.py:186][0m |          -0.0054 |           2.8312 |           3.6016 |
[32m[20230113 19:51:48 @agent_ppo2.py:186][0m |          -0.0096 |           2.8001 |           3.6043 |
[32m[20230113 19:51:48 @agent_ppo2.py:186][0m |          -0.0065 |           2.7762 |           3.6022 |
[32m[20230113 19:51:48 @agent_ppo2.py:186][0m |          -0.0111 |           2.7620 |           3.6024 |
[32m[20230113 19:51:48 @agent_ppo2.py:186][0m |          -0.0150 |           2.7448 |           3.6007 |
[32m[20230113 19:51:48 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.32
[32m[20230113 19:51:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.08
[32m[20230113 19:51:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 225.32
[32m[20230113 19:51:49 @agent_ppo2.py:144][0m Total time:       7.27 min
[32m[20230113 19:51:49 @agent_ppo2.py:146][0m 641024 total steps have happened
[32m[20230113 19:51:49 @agent_ppo2.py:122][0m #------------------------ Iteration 313 --------------------------#
[32m[20230113 19:51:49 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:51:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:49 @agent_ppo2.py:186][0m |           0.0013 |           3.9425 |           3.6664 |
[32m[20230113 19:51:49 @agent_ppo2.py:186][0m |          -0.0014 |           3.6418 |           3.6643 |
[32m[20230113 19:51:50 @agent_ppo2.py:186][0m |          -0.0017 |           3.5219 |           3.6631 |
[32m[20230113 19:51:50 @agent_ppo2.py:186][0m |          -0.0017 |           3.2812 |           3.6622 |
[32m[20230113 19:51:50 @agent_ppo2.py:186][0m |          -0.0055 |           3.1235 |           3.6608 |
[32m[20230113 19:51:50 @agent_ppo2.py:186][0m |          -0.0050 |           3.0429 |           3.6618 |
[32m[20230113 19:51:50 @agent_ppo2.py:186][0m |          -0.0024 |           2.9980 |           3.6611 |
[32m[20230113 19:51:50 @agent_ppo2.py:186][0m |          -0.0030 |           2.9274 |           3.6626 |
[32m[20230113 19:51:50 @agent_ppo2.py:186][0m |          -0.0039 |           2.8588 |           3.6636 |
[32m[20230113 19:51:50 @agent_ppo2.py:186][0m |          -0.0063 |           2.8229 |           3.6599 |
[32m[20230113 19:51:50 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 211.64
[32m[20230113 19:51:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.53
[32m[20230113 19:51:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.28
[32m[20230113 19:51:50 @agent_ppo2.py:144][0m Total time:       7.30 min
[32m[20230113 19:51:50 @agent_ppo2.py:146][0m 643072 total steps have happened
[32m[20230113 19:51:50 @agent_ppo2.py:122][0m #------------------------ Iteration 314 --------------------------#
[32m[20230113 19:51:51 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:51 @agent_ppo2.py:186][0m |          -0.0006 |           3.8866 |           3.7117 |
[32m[20230113 19:51:51 @agent_ppo2.py:186][0m |          -0.0041 |           3.5079 |           3.7000 |
[32m[20230113 19:51:51 @agent_ppo2.py:186][0m |          -0.0063 |           3.3282 |           3.7012 |
[32m[20230113 19:51:51 @agent_ppo2.py:186][0m |          -0.0077 |           3.2497 |           3.6973 |
[32m[20230113 19:51:51 @agent_ppo2.py:186][0m |          -0.0087 |           3.1456 |           3.6963 |
[32m[20230113 19:51:51 @agent_ppo2.py:186][0m |          -0.0092 |           3.0861 |           3.6961 |
[32m[20230113 19:51:51 @agent_ppo2.py:186][0m |          -0.0099 |           3.0233 |           3.6943 |
[32m[20230113 19:51:51 @agent_ppo2.py:186][0m |          -0.0102 |           2.9654 |           3.6950 |
[32m[20230113 19:51:51 @agent_ppo2.py:186][0m |          -0.0110 |           2.9110 |           3.6973 |
[32m[20230113 19:51:51 @agent_ppo2.py:186][0m |          -0.0114 |           2.8785 |           3.6948 |
[32m[20230113 19:51:51 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 204.19
[32m[20230113 19:51:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 208.75
[32m[20230113 19:51:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.79
[32m[20230113 19:51:52 @agent_ppo2.py:144][0m Total time:       7.32 min
[32m[20230113 19:51:52 @agent_ppo2.py:146][0m 645120 total steps have happened
[32m[20230113 19:51:52 @agent_ppo2.py:122][0m #------------------------ Iteration 315 --------------------------#
[32m[20230113 19:51:52 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:51:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:52 @agent_ppo2.py:186][0m |          -0.0004 |           5.0420 |           3.5945 |
[32m[20230113 19:51:52 @agent_ppo2.py:186][0m |          -0.0143 |           4.7066 |           3.5942 |
[32m[20230113 19:51:52 @agent_ppo2.py:186][0m |          -0.0061 |           4.5565 |           3.5930 |
[32m[20230113 19:51:52 @agent_ppo2.py:186][0m |          -0.0053 |           4.4606 |           3.5921 |
[32m[20230113 19:51:52 @agent_ppo2.py:186][0m |          -0.0080 |           4.4136 |           3.5921 |
[32m[20230113 19:51:52 @agent_ppo2.py:186][0m |          -0.0022 |           4.6206 |           3.5933 |
[32m[20230113 19:51:53 @agent_ppo2.py:186][0m |          -0.0106 |           4.2961 |           3.5912 |
[32m[20230113 19:51:53 @agent_ppo2.py:186][0m |          -0.0131 |           4.2522 |           3.5924 |
[32m[20230113 19:51:53 @agent_ppo2.py:186][0m |          -0.0092 |           4.3630 |           3.5925 |
[32m[20230113 19:51:53 @agent_ppo2.py:186][0m |          -0.0131 |           4.2011 |           3.5907 |
[32m[20230113 19:51:53 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.80
[32m[20230113 19:51:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.32
[32m[20230113 19:51:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.77
[32m[20230113 19:51:53 @agent_ppo2.py:144][0m Total time:       7.34 min
[32m[20230113 19:51:53 @agent_ppo2.py:146][0m 647168 total steps have happened
[32m[20230113 19:51:53 @agent_ppo2.py:122][0m #------------------------ Iteration 316 --------------------------#
[32m[20230113 19:51:54 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:54 @agent_ppo2.py:186][0m |           0.0004 |           3.1659 |           3.7608 |
[32m[20230113 19:51:54 @agent_ppo2.py:186][0m |          -0.0025 |           3.0820 |           3.7582 |
[32m[20230113 19:51:54 @agent_ppo2.py:186][0m |          -0.0044 |           2.9993 |           3.7534 |
[32m[20230113 19:51:54 @agent_ppo2.py:186][0m |          -0.0061 |           2.9638 |           3.7500 |
[32m[20230113 19:51:54 @agent_ppo2.py:186][0m |          -0.0070 |           2.9073 |           3.7472 |
[32m[20230113 19:51:54 @agent_ppo2.py:186][0m |          -0.0081 |           2.8872 |           3.7449 |
[32m[20230113 19:51:54 @agent_ppo2.py:186][0m |          -0.0081 |           2.8582 |           3.7441 |
[32m[20230113 19:51:54 @agent_ppo2.py:186][0m |          -0.0094 |           2.8355 |           3.7406 |
[32m[20230113 19:51:54 @agent_ppo2.py:186][0m |          -0.0099 |           2.8183 |           3.7401 |
[32m[20230113 19:51:54 @agent_ppo2.py:186][0m |          -0.0101 |           2.8019 |           3.7399 |
[32m[20230113 19:51:54 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:51:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 213.74
[32m[20230113 19:51:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 215.35
[32m[20230113 19:51:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 222.70
[32m[20230113 19:51:55 @agent_ppo2.py:144][0m Total time:       7.37 min
[32m[20230113 19:51:55 @agent_ppo2.py:146][0m 649216 total steps have happened
[32m[20230113 19:51:55 @agent_ppo2.py:122][0m #------------------------ Iteration 317 --------------------------#
[32m[20230113 19:51:55 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:55 @agent_ppo2.py:186][0m |           0.0009 |           2.6701 |           3.6485 |
[32m[20230113 19:51:55 @agent_ppo2.py:186][0m |          -0.0029 |           2.5394 |           3.6482 |
[32m[20230113 19:51:55 @agent_ppo2.py:186][0m |          -0.0046 |           2.4728 |           3.6469 |
[32m[20230113 19:51:55 @agent_ppo2.py:186][0m |          -0.0064 |           2.4389 |           3.6481 |
[32m[20230113 19:51:55 @agent_ppo2.py:186][0m |          -0.0077 |           2.3950 |           3.6455 |
[32m[20230113 19:51:55 @agent_ppo2.py:186][0m |          -0.0079 |           2.3847 |           3.6468 |
[32m[20230113 19:51:55 @agent_ppo2.py:186][0m |          -0.0091 |           2.3586 |           3.6462 |
[32m[20230113 19:51:55 @agent_ppo2.py:186][0m |          -0.0097 |           2.3520 |           3.6467 |
[32m[20230113 19:51:55 @agent_ppo2.py:186][0m |          -0.0105 |           2.3305 |           3.6460 |
[32m[20230113 19:51:55 @agent_ppo2.py:186][0m |          -0.0110 |           2.3248 |           3.6442 |
[32m[20230113 19:51:55 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.84
[32m[20230113 19:51:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.48
[32m[20230113 19:51:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.82
[32m[20230113 19:51:56 @agent_ppo2.py:144][0m Total time:       7.39 min
[32m[20230113 19:51:56 @agent_ppo2.py:146][0m 651264 total steps have happened
[32m[20230113 19:51:56 @agent_ppo2.py:122][0m #------------------------ Iteration 318 --------------------------#
[32m[20230113 19:51:56 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:56 @agent_ppo2.py:186][0m |          -0.0003 |           2.9880 |           3.6246 |
[32m[20230113 19:51:57 @agent_ppo2.py:186][0m |          -0.0030 |           2.7988 |           3.6192 |
[32m[20230113 19:51:57 @agent_ppo2.py:186][0m |          -0.0043 |           2.7253 |           3.6173 |
[32m[20230113 19:51:57 @agent_ppo2.py:186][0m |          -0.0057 |           2.6727 |           3.6164 |
[32m[20230113 19:51:57 @agent_ppo2.py:186][0m |          -0.0059 |           2.6417 |           3.6168 |
[32m[20230113 19:51:57 @agent_ppo2.py:186][0m |          -0.0075 |           2.6193 |           3.6176 |
[32m[20230113 19:51:57 @agent_ppo2.py:186][0m |          -0.0076 |           2.5968 |           3.6154 |
[32m[20230113 19:51:57 @agent_ppo2.py:186][0m |          -0.0083 |           2.5867 |           3.6150 |
[32m[20230113 19:51:57 @agent_ppo2.py:186][0m |          -0.0087 |           2.5679 |           3.6150 |
[32m[20230113 19:51:57 @agent_ppo2.py:186][0m |          -0.0099 |           2.5295 |           3.6156 |
[32m[20230113 19:51:57 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 202.82
[32m[20230113 19:51:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 203.29
[32m[20230113 19:51:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.22
[32m[20230113 19:51:57 @agent_ppo2.py:144][0m Total time:       7.41 min
[32m[20230113 19:51:57 @agent_ppo2.py:146][0m 653312 total steps have happened
[32m[20230113 19:51:57 @agent_ppo2.py:122][0m #------------------------ Iteration 319 --------------------------#
[32m[20230113 19:51:58 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:58 @agent_ppo2.py:186][0m |          -0.0009 |           4.6445 |           3.6629 |
[32m[20230113 19:51:58 @agent_ppo2.py:186][0m |          -0.0047 |           4.5033 |           3.6590 |
[32m[20230113 19:51:58 @agent_ppo2.py:186][0m |          -0.0055 |           4.4245 |           3.6568 |
[32m[20230113 19:51:58 @agent_ppo2.py:186][0m |          -0.0062 |           4.3695 |           3.6600 |
[32m[20230113 19:51:58 @agent_ppo2.py:186][0m |          -0.0073 |           4.2926 |           3.6612 |
[32m[20230113 19:51:58 @agent_ppo2.py:186][0m |          -0.0093 |           4.2701 |           3.6619 |
[32m[20230113 19:51:58 @agent_ppo2.py:186][0m |          -0.0066 |           4.2965 |           3.6636 |
[32m[20230113 19:51:58 @agent_ppo2.py:186][0m |          -0.0089 |           4.1951 |           3.6642 |
[32m[20230113 19:51:58 @agent_ppo2.py:186][0m |          -0.0070 |           4.2119 |           3.6658 |
[32m[20230113 19:51:58 @agent_ppo2.py:186][0m |          -0.0097 |           4.1799 |           3.6649 |
[32m[20230113 19:51:58 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:51:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.94
[32m[20230113 19:51:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.43
[32m[20230113 19:51:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.29
[32m[20230113 19:51:59 @agent_ppo2.py:144][0m Total time:       7.44 min
[32m[20230113 19:51:59 @agent_ppo2.py:146][0m 655360 total steps have happened
[32m[20230113 19:51:59 @agent_ppo2.py:122][0m #------------------------ Iteration 320 --------------------------#
[32m[20230113 19:51:59 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:51:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:51:59 @agent_ppo2.py:186][0m |           0.0010 |           2.9889 |           3.7160 |
[32m[20230113 19:51:59 @agent_ppo2.py:186][0m |          -0.0035 |           2.7713 |           3.7168 |
[32m[20230113 19:51:59 @agent_ppo2.py:186][0m |          -0.0054 |           2.7030 |           3.7156 |
[32m[20230113 19:51:59 @agent_ppo2.py:186][0m |          -0.0060 |           2.6368 |           3.7139 |
[32m[20230113 19:51:59 @agent_ppo2.py:186][0m |          -0.0072 |           2.6053 |           3.7157 |
[32m[20230113 19:52:00 @agent_ppo2.py:186][0m |          -0.0078 |           2.5676 |           3.7161 |
[32m[20230113 19:52:00 @agent_ppo2.py:186][0m |          -0.0079 |           2.5528 |           3.7182 |
[32m[20230113 19:52:00 @agent_ppo2.py:186][0m |          -0.0084 |           2.5137 |           3.7169 |
[32m[20230113 19:52:00 @agent_ppo2.py:186][0m |          -0.0093 |           2.4976 |           3.7177 |
[32m[20230113 19:52:00 @agent_ppo2.py:186][0m |          -0.0098 |           2.4828 |           3.7188 |
[32m[20230113 19:52:00 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 207.23
[32m[20230113 19:52:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.18
[32m[20230113 19:52:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.77
[32m[20230113 19:52:00 @agent_ppo2.py:144][0m Total time:       7.46 min
[32m[20230113 19:52:00 @agent_ppo2.py:146][0m 657408 total steps have happened
[32m[20230113 19:52:00 @agent_ppo2.py:122][0m #------------------------ Iteration 321 --------------------------#
[32m[20230113 19:52:01 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:01 @agent_ppo2.py:186][0m |          -0.0023 |           4.5927 |           3.6865 |
[32m[20230113 19:52:01 @agent_ppo2.py:186][0m |          -0.0067 |           4.3815 |           3.6800 |
[32m[20230113 19:52:01 @agent_ppo2.py:186][0m |          -0.0054 |           4.3342 |           3.6791 |
[32m[20230113 19:52:01 @agent_ppo2.py:186][0m |          -0.0092 |           4.2379 |           3.6759 |
[32m[20230113 19:52:01 @agent_ppo2.py:186][0m |          -0.0098 |           4.1596 |           3.6726 |
[32m[20230113 19:52:01 @agent_ppo2.py:186][0m |          -0.0097 |           4.1426 |           3.6727 |
[32m[20230113 19:52:01 @agent_ppo2.py:186][0m |          -0.0095 |           4.1141 |           3.6718 |
[32m[20230113 19:52:01 @agent_ppo2.py:186][0m |          -0.0121 |           4.0465 |           3.6722 |
[32m[20230113 19:52:01 @agent_ppo2.py:186][0m |          -0.0110 |           4.0269 |           3.6707 |
[32m[20230113 19:52:01 @agent_ppo2.py:186][0m |          -0.0120 |           3.9758 |           3.6699 |
[32m[20230113 19:52:01 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.30
[32m[20230113 19:52:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.05
[32m[20230113 19:52:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.86
[32m[20230113 19:52:02 @agent_ppo2.py:144][0m Total time:       7.48 min
[32m[20230113 19:52:02 @agent_ppo2.py:146][0m 659456 total steps have happened
[32m[20230113 19:52:02 @agent_ppo2.py:122][0m #------------------------ Iteration 322 --------------------------#
[32m[20230113 19:52:02 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:52:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:02 @agent_ppo2.py:186][0m |           0.0024 |           2.5966 |           3.6625 |
[32m[20230113 19:52:02 @agent_ppo2.py:186][0m |          -0.0028 |           2.4463 |           3.6583 |
[32m[20230113 19:52:02 @agent_ppo2.py:186][0m |          -0.0031 |           2.3596 |           3.6552 |
[32m[20230113 19:52:02 @agent_ppo2.py:186][0m |          -0.0055 |           2.3180 |           3.6515 |
[32m[20230113 19:52:02 @agent_ppo2.py:186][0m |          -0.0048 |           2.3024 |           3.6504 |
[32m[20230113 19:52:02 @agent_ppo2.py:186][0m |          -0.0010 |           2.3423 |           3.6494 |
[32m[20230113 19:52:02 @agent_ppo2.py:186][0m |          -0.0021 |           2.3162 |           3.6465 |
[32m[20230113 19:52:02 @agent_ppo2.py:186][0m |          -0.0066 |           2.2503 |           3.6441 |
[32m[20230113 19:52:02 @agent_ppo2.py:186][0m |          -0.0065 |           2.2197 |           3.6417 |
[32m[20230113 19:52:02 @agent_ppo2.py:186][0m |          -0.0075 |           2.2116 |           3.6403 |
[32m[20230113 19:52:02 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.52
[32m[20230113 19:52:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.26
[32m[20230113 19:52:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.28
[32m[20230113 19:52:03 @agent_ppo2.py:144][0m Total time:       7.51 min
[32m[20230113 19:52:03 @agent_ppo2.py:146][0m 661504 total steps have happened
[32m[20230113 19:52:03 @agent_ppo2.py:122][0m #------------------------ Iteration 323 --------------------------#
[32m[20230113 19:52:03 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:03 @agent_ppo2.py:186][0m |           0.0011 |           3.4464 |           3.6323 |
[32m[20230113 19:52:04 @agent_ppo2.py:186][0m |          -0.0055 |           3.3326 |           3.6259 |
[32m[20230113 19:52:04 @agent_ppo2.py:186][0m |          -0.0080 |           3.2705 |           3.6243 |
[32m[20230113 19:52:04 @agent_ppo2.py:186][0m |          -0.0073 |           3.2481 |           3.6255 |
[32m[20230113 19:52:04 @agent_ppo2.py:186][0m |          -0.0106 |           3.2035 |           3.6241 |
[32m[20230113 19:52:04 @agent_ppo2.py:186][0m |          -0.0138 |           3.1800 |           3.6236 |
[32m[20230113 19:52:04 @agent_ppo2.py:186][0m |          -0.0095 |           3.1511 |           3.6221 |
[32m[20230113 19:52:04 @agent_ppo2.py:186][0m |          -0.0105 |           3.1433 |           3.6233 |
[32m[20230113 19:52:04 @agent_ppo2.py:186][0m |          -0.0116 |           3.1080 |           3.6226 |
[32m[20230113 19:52:04 @agent_ppo2.py:186][0m |          -0.0141 |           3.1047 |           3.6215 |
[32m[20230113 19:52:04 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 217.67
[32m[20230113 19:52:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.14
[32m[20230113 19:52:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.73
[32m[20230113 19:52:04 @agent_ppo2.py:144][0m Total time:       7.53 min
[32m[20230113 19:52:04 @agent_ppo2.py:146][0m 663552 total steps have happened
[32m[20230113 19:52:04 @agent_ppo2.py:122][0m #------------------------ Iteration 324 --------------------------#
[32m[20230113 19:52:05 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:05 @agent_ppo2.py:186][0m |          -0.0037 |           4.4620 |           3.6091 |
[32m[20230113 19:52:05 @agent_ppo2.py:186][0m |          -0.0050 |           4.3034 |           3.6057 |
[32m[20230113 19:52:05 @agent_ppo2.py:186][0m |           0.0035 |           4.4759 |           3.6065 |
[32m[20230113 19:52:05 @agent_ppo2.py:186][0m |          -0.0077 |           4.1620 |           3.6034 |
[32m[20230113 19:52:05 @agent_ppo2.py:186][0m |          -0.0086 |           4.1309 |           3.6067 |
[32m[20230113 19:52:05 @agent_ppo2.py:186][0m |          -0.0093 |           4.1018 |           3.6054 |
[32m[20230113 19:52:05 @agent_ppo2.py:186][0m |          -0.0097 |           4.0905 |           3.6050 |
[32m[20230113 19:52:05 @agent_ppo2.py:186][0m |          -0.0066 |           4.0442 |           3.6083 |
[32m[20230113 19:52:05 @agent_ppo2.py:186][0m |          -0.0030 |           4.1948 |           3.6058 |
[32m[20230113 19:52:05 @agent_ppo2.py:186][0m |          -0.0125 |           3.9907 |           3.6083 |
[32m[20230113 19:52:05 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.90
[32m[20230113 19:52:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.55
[32m[20230113 19:52:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.76
[32m[20230113 19:52:06 @agent_ppo2.py:144][0m Total time:       7.55 min
[32m[20230113 19:52:06 @agent_ppo2.py:146][0m 665600 total steps have happened
[32m[20230113 19:52:06 @agent_ppo2.py:122][0m #------------------------ Iteration 325 --------------------------#
[32m[20230113 19:52:06 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:52:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:06 @agent_ppo2.py:186][0m |           0.0009 |           3.5643 |           3.6679 |
[32m[20230113 19:52:06 @agent_ppo2.py:186][0m |          -0.0032 |           3.2723 |           3.6674 |
[32m[20230113 19:52:06 @agent_ppo2.py:186][0m |          -0.0040 |           3.0807 |           3.6630 |
[32m[20230113 19:52:06 @agent_ppo2.py:186][0m |          -0.0049 |           2.9989 |           3.6636 |
[32m[20230113 19:52:06 @agent_ppo2.py:186][0m |          -0.0057 |           2.8938 |           3.6628 |
[32m[20230113 19:52:06 @agent_ppo2.py:186][0m |          -0.0058 |           2.8376 |           3.6624 |
[32m[20230113 19:52:07 @agent_ppo2.py:186][0m |          -0.0062 |           2.7631 |           3.6607 |
[32m[20230113 19:52:07 @agent_ppo2.py:186][0m |          -0.0065 |           2.6878 |           3.6632 |
[32m[20230113 19:52:07 @agent_ppo2.py:186][0m |          -0.0071 |           2.6627 |           3.6617 |
[32m[20230113 19:52:07 @agent_ppo2.py:186][0m |          -0.0076 |           2.5536 |           3.6620 |
[32m[20230113 19:52:07 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 207.33
[32m[20230113 19:52:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 209.69
[32m[20230113 19:52:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.44
[32m[20230113 19:52:07 @agent_ppo2.py:144][0m Total time:       7.58 min
[32m[20230113 19:52:07 @agent_ppo2.py:146][0m 667648 total steps have happened
[32m[20230113 19:52:07 @agent_ppo2.py:122][0m #------------------------ Iteration 326 --------------------------#
[32m[20230113 19:52:08 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:08 @agent_ppo2.py:186][0m |           0.0153 |           9.6126 |           3.5388 |
[32m[20230113 19:52:08 @agent_ppo2.py:186][0m |          -0.0096 |           4.6777 |           3.5348 |
[32m[20230113 19:52:08 @agent_ppo2.py:186][0m |          -0.0078 |           3.9404 |           3.5315 |
[32m[20230113 19:52:08 @agent_ppo2.py:186][0m |          -0.0018 |           3.7300 |           3.5296 |
[32m[20230113 19:52:08 @agent_ppo2.py:186][0m |          -0.0059 |           3.6205 |           3.5268 |
[32m[20230113 19:52:08 @agent_ppo2.py:186][0m |          -0.0089 |           3.4300 |           3.5270 |
[32m[20230113 19:52:08 @agent_ppo2.py:186][0m |           0.0017 |           3.4446 |           3.5279 |
[32m[20230113 19:52:08 @agent_ppo2.py:186][0m |          -0.0254 |           3.2980 |           3.5287 |
[32m[20230113 19:52:08 @agent_ppo2.py:186][0m |          -0.0132 |           3.1593 |           3.5291 |
[32m[20230113 19:52:08 @agent_ppo2.py:186][0m |          -0.0151 |           3.0850 |           3.5298 |
[32m[20230113 19:52:08 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:52:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.97
[32m[20230113 19:52:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.56
[32m[20230113 19:52:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.45
[32m[20230113 19:52:08 @agent_ppo2.py:144][0m Total time:       7.60 min
[32m[20230113 19:52:08 @agent_ppo2.py:146][0m 669696 total steps have happened
[32m[20230113 19:52:08 @agent_ppo2.py:122][0m #------------------------ Iteration 327 --------------------------#
[32m[20230113 19:52:09 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 19:52:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:09 @agent_ppo2.py:186][0m |           0.0004 |          14.8987 |           3.6895 |
[32m[20230113 19:52:09 @agent_ppo2.py:186][0m |          -0.0041 |           7.4728 |           3.6827 |
[32m[20230113 19:52:09 @agent_ppo2.py:186][0m |          -0.0083 |           6.2855 |           3.6826 |
[32m[20230113 19:52:09 @agent_ppo2.py:186][0m |          -0.0098 |           5.8540 |           3.6789 |
[32m[20230113 19:52:09 @agent_ppo2.py:186][0m |          -0.0112 |           5.5319 |           3.6772 |
[32m[20230113 19:52:09 @agent_ppo2.py:186][0m |          -0.0122 |           5.4070 |           3.6761 |
[32m[20230113 19:52:09 @agent_ppo2.py:186][0m |          -0.0124 |           5.1554 |           3.6754 |
[32m[20230113 19:52:09 @agent_ppo2.py:186][0m |          -0.0134 |           5.1826 |           3.6755 |
[32m[20230113 19:52:09 @agent_ppo2.py:186][0m |          -0.0132 |           5.0518 |           3.6748 |
[32m[20230113 19:52:09 @agent_ppo2.py:186][0m |          -0.0141 |           4.8004 |           3.6766 |
[32m[20230113 19:52:09 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 19:52:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 134.96
[32m[20230113 19:52:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 210.19
[32m[20230113 19:52:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.26
[32m[20230113 19:52:10 @agent_ppo2.py:144][0m Total time:       7.62 min
[32m[20230113 19:52:10 @agent_ppo2.py:146][0m 671744 total steps have happened
[32m[20230113 19:52:10 @agent_ppo2.py:122][0m #------------------------ Iteration 328 --------------------------#
[32m[20230113 19:52:10 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:52:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:10 @agent_ppo2.py:186][0m |           0.0006 |           4.7849 |           3.6372 |
[32m[20230113 19:52:10 @agent_ppo2.py:186][0m |          -0.0041 |           4.2638 |           3.6352 |
[32m[20230113 19:52:10 @agent_ppo2.py:186][0m |          -0.0029 |           4.1147 |           3.6339 |
[32m[20230113 19:52:10 @agent_ppo2.py:186][0m |          -0.0044 |           4.0063 |           3.6347 |
[32m[20230113 19:52:10 @agent_ppo2.py:186][0m |          -0.0027 |           4.0775 |           3.6322 |
[32m[20230113 19:52:11 @agent_ppo2.py:186][0m |          -0.0053 |           3.8672 |           3.6337 |
[32m[20230113 19:52:11 @agent_ppo2.py:186][0m |          -0.0108 |           3.7519 |           3.6309 |
[32m[20230113 19:52:11 @agent_ppo2.py:186][0m |          -0.0075 |           3.7075 |           3.6274 |
[32m[20230113 19:52:11 @agent_ppo2.py:186][0m |          -0.0105 |           3.6530 |           3.6319 |
[32m[20230113 19:52:11 @agent_ppo2.py:186][0m |          -0.0101 |           3.5942 |           3.6329 |
[32m[20230113 19:52:11 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:52:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.60
[32m[20230113 19:52:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.34
[32m[20230113 19:52:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 99.27
[32m[20230113 19:52:11 @agent_ppo2.py:144][0m Total time:       7.64 min
[32m[20230113 19:52:11 @agent_ppo2.py:146][0m 673792 total steps have happened
[32m[20230113 19:52:11 @agent_ppo2.py:122][0m #------------------------ Iteration 329 --------------------------#
[32m[20230113 19:52:11 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:52:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:12 @agent_ppo2.py:186][0m |          -0.0000 |           3.1891 |           3.6934 |
[32m[20230113 19:52:12 @agent_ppo2.py:186][0m |          -0.0047 |           3.0520 |           3.6869 |
[32m[20230113 19:52:12 @agent_ppo2.py:186][0m |          -0.0057 |           2.9966 |           3.6865 |
[32m[20230113 19:52:12 @agent_ppo2.py:186][0m |          -0.0067 |           2.9498 |           3.6836 |
[32m[20230113 19:52:12 @agent_ppo2.py:186][0m |          -0.0071 |           2.9178 |           3.6826 |
[32m[20230113 19:52:12 @agent_ppo2.py:186][0m |          -0.0075 |           2.8938 |           3.6826 |
[32m[20230113 19:52:12 @agent_ppo2.py:186][0m |          -0.0077 |           2.8646 |           3.6845 |
[32m[20230113 19:52:12 @agent_ppo2.py:186][0m |          -0.0087 |           2.8384 |           3.6833 |
[32m[20230113 19:52:12 @agent_ppo2.py:186][0m |          -0.0091 |           2.8475 |           3.6820 |
[32m[20230113 19:52:12 @agent_ppo2.py:186][0m |          -0.0093 |           2.8158 |           3.6824 |
[32m[20230113 19:52:12 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:52:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.02
[32m[20230113 19:52:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.24
[32m[20230113 19:52:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.90
[32m[20230113 19:52:12 @agent_ppo2.py:144][0m Total time:       7.66 min
[32m[20230113 19:52:12 @agent_ppo2.py:146][0m 675840 total steps have happened
[32m[20230113 19:52:12 @agent_ppo2.py:122][0m #------------------------ Iteration 330 --------------------------#
[32m[20230113 19:52:13 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:52:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:13 @agent_ppo2.py:186][0m |          -0.0017 |           5.0361 |           3.7169 |
[32m[20230113 19:52:13 @agent_ppo2.py:186][0m |          -0.0075 |           4.7988 |           3.7085 |
[32m[20230113 19:52:13 @agent_ppo2.py:186][0m |          -0.0097 |           4.7055 |           3.7026 |
[32m[20230113 19:52:13 @agent_ppo2.py:186][0m |          -0.0110 |           4.5973 |           3.7001 |
[32m[20230113 19:52:13 @agent_ppo2.py:186][0m |          -0.0117 |           4.5425 |           3.6952 |
[32m[20230113 19:52:13 @agent_ppo2.py:186][0m |          -0.0127 |           4.4700 |           3.6921 |
[32m[20230113 19:52:13 @agent_ppo2.py:186][0m |          -0.0130 |           4.4206 |           3.6943 |
[32m[20230113 19:52:13 @agent_ppo2.py:186][0m |          -0.0137 |           4.3764 |           3.6914 |
[32m[20230113 19:52:13 @agent_ppo2.py:186][0m |          -0.0141 |           4.3201 |           3.6909 |
[32m[20230113 19:52:13 @agent_ppo2.py:186][0m |          -0.0141 |           4.3103 |           3.6868 |
[32m[20230113 19:52:13 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.66
[32m[20230113 19:52:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.99
[32m[20230113 19:52:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.48
[32m[20230113 19:52:14 @agent_ppo2.py:144][0m Total time:       7.69 min
[32m[20230113 19:52:14 @agent_ppo2.py:146][0m 677888 total steps have happened
[32m[20230113 19:52:14 @agent_ppo2.py:122][0m #------------------------ Iteration 331 --------------------------#
[32m[20230113 19:52:14 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:52:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:14 @agent_ppo2.py:186][0m |           0.0001 |           5.1840 |           3.6225 |
[32m[20230113 19:52:14 @agent_ppo2.py:186][0m |          -0.0055 |           4.7755 |           3.6203 |
[32m[20230113 19:52:14 @agent_ppo2.py:186][0m |          -0.0068 |           4.6124 |           3.6237 |
[32m[20230113 19:52:14 @agent_ppo2.py:186][0m |          -0.0070 |           4.5061 |           3.6243 |
[32m[20230113 19:52:14 @agent_ppo2.py:186][0m |          -0.0089 |           4.4035 |           3.6224 |
[32m[20230113 19:52:15 @agent_ppo2.py:186][0m |          -0.0077 |           4.3851 |           3.6232 |
[32m[20230113 19:52:15 @agent_ppo2.py:186][0m |          -0.0095 |           4.2710 |           3.6236 |
[32m[20230113 19:52:15 @agent_ppo2.py:186][0m |          -0.0100 |           4.2050 |           3.6257 |
[32m[20230113 19:52:15 @agent_ppo2.py:186][0m |          -0.0106 |           4.1540 |           3.6247 |
[32m[20230113 19:52:15 @agent_ppo2.py:186][0m |          -0.0105 |           4.1140 |           3.6230 |
[32m[20230113 19:52:15 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:52:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.61
[32m[20230113 19:52:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.43
[32m[20230113 19:52:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 112.62
[32m[20230113 19:52:15 @agent_ppo2.py:144][0m Total time:       7.71 min
[32m[20230113 19:52:15 @agent_ppo2.py:146][0m 679936 total steps have happened
[32m[20230113 19:52:15 @agent_ppo2.py:122][0m #------------------------ Iteration 332 --------------------------#
[32m[20230113 19:52:16 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:16 @agent_ppo2.py:186][0m |          -0.0015 |           4.6416 |           3.6007 |
[32m[20230113 19:52:16 @agent_ppo2.py:186][0m |          -0.0066 |           4.3188 |           3.5960 |
[32m[20230113 19:52:16 @agent_ppo2.py:186][0m |          -0.0073 |           4.2081 |           3.5901 |
[32m[20230113 19:52:16 @agent_ppo2.py:186][0m |          -0.0073 |           4.1757 |           3.5910 |
[32m[20230113 19:52:16 @agent_ppo2.py:186][0m |          -0.0087 |           4.0089 |           3.5890 |
[32m[20230113 19:52:16 @agent_ppo2.py:186][0m |          -0.0079 |           4.0377 |           3.5907 |
[32m[20230113 19:52:16 @agent_ppo2.py:186][0m |          -0.0109 |           3.9575 |           3.5938 |
[32m[20230113 19:52:16 @agent_ppo2.py:186][0m |          -0.0124 |           3.8452 |           3.5936 |
[32m[20230113 19:52:16 @agent_ppo2.py:186][0m |          -0.0133 |           3.7838 |           3.5953 |
[32m[20230113 19:52:16 @agent_ppo2.py:186][0m |          -0.0135 |           3.7412 |           3.5943 |
[32m[20230113 19:52:16 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:52:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.40
[32m[20230113 19:52:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.28
[32m[20230113 19:52:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.21
[32m[20230113 19:52:16 @agent_ppo2.py:144][0m Total time:       7.73 min
[32m[20230113 19:52:16 @agent_ppo2.py:146][0m 681984 total steps have happened
[32m[20230113 19:52:16 @agent_ppo2.py:122][0m #------------------------ Iteration 333 --------------------------#
[32m[20230113 19:52:17 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:52:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:17 @agent_ppo2.py:186][0m |           0.0015 |           2.8917 |           3.6095 |
[32m[20230113 19:52:17 @agent_ppo2.py:186][0m |          -0.0015 |           2.6748 |           3.6060 |
[32m[20230113 19:52:17 @agent_ppo2.py:186][0m |          -0.0042 |           2.5430 |           3.6053 |
[32m[20230113 19:52:17 @agent_ppo2.py:186][0m |          -0.0054 |           2.4736 |           3.6028 |
[32m[20230113 19:52:17 @agent_ppo2.py:186][0m |          -0.0064 |           2.4171 |           3.5994 |
[32m[20230113 19:52:17 @agent_ppo2.py:186][0m |          -0.0071 |           2.3680 |           3.6014 |
[32m[20230113 19:52:17 @agent_ppo2.py:186][0m |          -0.0072 |           2.3242 |           3.5982 |
[32m[20230113 19:52:17 @agent_ppo2.py:186][0m |          -0.0077 |           2.2803 |           3.5985 |
[32m[20230113 19:52:17 @agent_ppo2.py:186][0m |          -0.0085 |           2.2440 |           3.5987 |
[32m[20230113 19:52:17 @agent_ppo2.py:186][0m |          -0.0088 |           2.2071 |           3.5953 |
[32m[20230113 19:52:17 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:52:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.96
[32m[20230113 19:52:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.95
[32m[20230113 19:52:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.97
[32m[20230113 19:52:18 @agent_ppo2.py:144][0m Total time:       7.75 min
[32m[20230113 19:52:18 @agent_ppo2.py:146][0m 684032 total steps have happened
[32m[20230113 19:52:18 @agent_ppo2.py:122][0m #------------------------ Iteration 334 --------------------------#
[32m[20230113 19:52:18 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:52:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:18 @agent_ppo2.py:186][0m |          -0.0033 |           9.4472 |           3.6838 |
[32m[20230113 19:52:19 @agent_ppo2.py:186][0m |          -0.0071 |           6.4990 |           3.6740 |
[32m[20230113 19:52:19 @agent_ppo2.py:186][0m |          -0.0089 |           6.0209 |           3.6734 |
[32m[20230113 19:52:19 @agent_ppo2.py:186][0m |          -0.0106 |           5.7442 |           3.6711 |
[32m[20230113 19:52:19 @agent_ppo2.py:186][0m |          -0.0110 |           5.5166 |           3.6699 |
[32m[20230113 19:52:19 @agent_ppo2.py:186][0m |          -0.0115 |           5.3915 |           3.6661 |
[32m[20230113 19:52:19 @agent_ppo2.py:186][0m |          -0.0124 |           5.2761 |           3.6676 |
[32m[20230113 19:52:19 @agent_ppo2.py:186][0m |          -0.0127 |           5.1544 |           3.6654 |
[32m[20230113 19:52:19 @agent_ppo2.py:186][0m |          -0.0131 |           5.0945 |           3.6634 |
[32m[20230113 19:52:19 @agent_ppo2.py:186][0m |          -0.0137 |           5.0052 |           3.6641 |
[32m[20230113 19:52:19 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 19:52:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 133.25
[32m[20230113 19:52:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.84
[32m[20230113 19:52:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.77
[32m[20230113 19:52:19 @agent_ppo2.py:144][0m Total time:       7.78 min
[32m[20230113 19:52:19 @agent_ppo2.py:146][0m 686080 total steps have happened
[32m[20230113 19:52:19 @agent_ppo2.py:122][0m #------------------------ Iteration 335 --------------------------#
[32m[20230113 19:52:20 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:52:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:20 @agent_ppo2.py:186][0m |          -0.0061 |           4.8869 |           3.5870 |
[32m[20230113 19:52:20 @agent_ppo2.py:186][0m |           0.0232 |           5.5649 |           3.5798 |
[32m[20230113 19:52:20 @agent_ppo2.py:186][0m |           0.0010 |           4.5091 |           3.5718 |
[32m[20230113 19:52:20 @agent_ppo2.py:186][0m |          -0.0154 |           4.3843 |           3.5766 |
[32m[20230113 19:52:20 @agent_ppo2.py:186][0m |          -0.0102 |           4.3537 |           3.5778 |
[32m[20230113 19:52:20 @agent_ppo2.py:186][0m |          -0.0089 |           4.2955 |           3.5747 |
[32m[20230113 19:52:20 @agent_ppo2.py:186][0m |          -0.0145 |           4.2923 |           3.5730 |
[32m[20230113 19:52:20 @agent_ppo2.py:186][0m |          -0.0086 |           4.2721 |           3.5736 |
[32m[20230113 19:52:20 @agent_ppo2.py:186][0m |          -0.0160 |           4.2412 |           3.5705 |
[32m[20230113 19:52:20 @agent_ppo2.py:186][0m |          -0.0263 |           4.1955 |           3.5720 |
[32m[20230113 19:52:20 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:52:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.46
[32m[20230113 19:52:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.05
[32m[20230113 19:52:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.66
[32m[20230113 19:52:21 @agent_ppo2.py:144][0m Total time:       7.80 min
[32m[20230113 19:52:21 @agent_ppo2.py:146][0m 688128 total steps have happened
[32m[20230113 19:52:21 @agent_ppo2.py:122][0m #------------------------ Iteration 336 --------------------------#
[32m[20230113 19:52:21 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:52:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:21 @agent_ppo2.py:186][0m |           0.0001 |           4.8572 |           3.5451 |
[32m[20230113 19:52:21 @agent_ppo2.py:186][0m |          -0.0037 |           4.2132 |           3.5416 |
[32m[20230113 19:52:21 @agent_ppo2.py:186][0m |          -0.0056 |           4.0131 |           3.5390 |
[32m[20230113 19:52:21 @agent_ppo2.py:186][0m |          -0.0065 |           3.8821 |           3.5367 |
[32m[20230113 19:52:21 @agent_ppo2.py:186][0m |          -0.0072 |           3.7864 |           3.5366 |
[32m[20230113 19:52:21 @agent_ppo2.py:186][0m |          -0.0072 |           3.7276 |           3.5369 |
[32m[20230113 19:52:22 @agent_ppo2.py:186][0m |          -0.0074 |           3.6750 |           3.5370 |
[32m[20230113 19:52:22 @agent_ppo2.py:186][0m |          -0.0088 |           3.6077 |           3.5379 |
[32m[20230113 19:52:22 @agent_ppo2.py:186][0m |          -0.0083 |           3.5567 |           3.5379 |
[32m[20230113 19:52:22 @agent_ppo2.py:186][0m |          -0.0087 |           3.5248 |           3.5385 |
[32m[20230113 19:52:22 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:52:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 218.78
[32m[20230113 19:52:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.86
[32m[20230113 19:52:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.53
[32m[20230113 19:52:22 @agent_ppo2.py:144][0m Total time:       7.83 min
[32m[20230113 19:52:22 @agent_ppo2.py:146][0m 690176 total steps have happened
[32m[20230113 19:52:22 @agent_ppo2.py:122][0m #------------------------ Iteration 337 --------------------------#
[32m[20230113 19:52:23 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:52:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:23 @agent_ppo2.py:186][0m |           0.0057 |           8.3656 |           3.6384 |
[32m[20230113 19:52:23 @agent_ppo2.py:186][0m |          -0.0054 |           4.6099 |           3.6384 |
[32m[20230113 19:52:23 @agent_ppo2.py:186][0m |          -0.0097 |           4.2166 |           3.6386 |
[32m[20230113 19:52:23 @agent_ppo2.py:186][0m |          -0.0102 |           4.0672 |           3.6341 |
[32m[20230113 19:52:23 @agent_ppo2.py:186][0m |          -0.0110 |           4.0165 |           3.6336 |
[32m[20230113 19:52:23 @agent_ppo2.py:186][0m |          -0.0146 |           3.8996 |           3.6347 |
[32m[20230113 19:52:23 @agent_ppo2.py:186][0m |          -0.0121 |           3.8770 |           3.6332 |
[32m[20230113 19:52:23 @agent_ppo2.py:186][0m |          -0.0157 |           3.7764 |           3.6351 |
[32m[20230113 19:52:23 @agent_ppo2.py:186][0m |          -0.0132 |           3.7849 |           3.6335 |
[32m[20230113 19:52:23 @agent_ppo2.py:186][0m |          -0.0177 |           3.7276 |           3.6327 |
[32m[20230113 19:52:23 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:52:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 103.09
[32m[20230113 19:52:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 215.88
[32m[20230113 19:52:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.28
[32m[20230113 19:52:24 @agent_ppo2.py:144][0m Total time:       7.85 min
[32m[20230113 19:52:24 @agent_ppo2.py:146][0m 692224 total steps have happened
[32m[20230113 19:52:24 @agent_ppo2.py:122][0m #------------------------ Iteration 338 --------------------------#
[32m[20230113 19:52:24 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:52:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:24 @agent_ppo2.py:186][0m |           0.0000 |           3.3709 |           3.6414 |
[32m[20230113 19:52:24 @agent_ppo2.py:186][0m |          -0.0053 |           3.2662 |           3.6374 |
[32m[20230113 19:52:24 @agent_ppo2.py:186][0m |          -0.0081 |           3.2344 |           3.6315 |
[32m[20230113 19:52:24 @agent_ppo2.py:186][0m |          -0.0095 |           3.1921 |           3.6320 |
[32m[20230113 19:52:24 @agent_ppo2.py:186][0m |          -0.0103 |           3.1512 |           3.6311 |
[32m[20230113 19:52:24 @agent_ppo2.py:186][0m |          -0.0108 |           3.1308 |           3.6318 |
[32m[20230113 19:52:24 @agent_ppo2.py:186][0m |          -0.0118 |           3.0885 |           3.6295 |
[32m[20230113 19:52:24 @agent_ppo2.py:186][0m |          -0.0122 |           3.0764 |           3.6327 |
[32m[20230113 19:52:24 @agent_ppo2.py:186][0m |          -0.0126 |           3.0648 |           3.6321 |
[32m[20230113 19:52:25 @agent_ppo2.py:186][0m |          -0.0131 |           3.0415 |           3.6330 |
[32m[20230113 19:52:25 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:52:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 211.56
[32m[20230113 19:52:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 216.83
[32m[20230113 19:52:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.65
[32m[20230113 19:52:25 @agent_ppo2.py:144][0m Total time:       7.87 min
[32m[20230113 19:52:25 @agent_ppo2.py:146][0m 694272 total steps have happened
[32m[20230113 19:52:25 @agent_ppo2.py:122][0m #------------------------ Iteration 339 --------------------------#
[32m[20230113 19:52:25 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:26 @agent_ppo2.py:186][0m |          -0.0001 |           5.8705 |           3.6557 |
[32m[20230113 19:52:26 @agent_ppo2.py:186][0m |          -0.0023 |           5.6443 |           3.6557 |
[32m[20230113 19:52:26 @agent_ppo2.py:186][0m |          -0.0044 |           5.5131 |           3.6550 |
[32m[20230113 19:52:26 @agent_ppo2.py:186][0m |          -0.0055 |           5.4520 |           3.6511 |
[32m[20230113 19:52:26 @agent_ppo2.py:186][0m |          -0.0072 |           5.3720 |           3.6518 |
[32m[20230113 19:52:26 @agent_ppo2.py:186][0m |          -0.0067 |           5.3694 |           3.6517 |
[32m[20230113 19:52:26 @agent_ppo2.py:186][0m |          -0.0072 |           5.2724 |           3.6486 |
[32m[20230113 19:52:26 @agent_ppo2.py:186][0m |          -0.0090 |           5.2259 |           3.6528 |
[32m[20230113 19:52:26 @agent_ppo2.py:186][0m |          -0.0090 |           5.2039 |           3.6506 |
[32m[20230113 19:52:26 @agent_ppo2.py:186][0m |          -0.0072 |           5.2398 |           3.6500 |
[32m[20230113 19:52:26 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 217.09
[32m[20230113 19:52:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.67
[32m[20230113 19:52:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.10
[32m[20230113 19:52:26 @agent_ppo2.py:144][0m Total time:       7.90 min
[32m[20230113 19:52:26 @agent_ppo2.py:146][0m 696320 total steps have happened
[32m[20230113 19:52:26 @agent_ppo2.py:122][0m #------------------------ Iteration 340 --------------------------#
[32m[20230113 19:52:27 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:27 @agent_ppo2.py:186][0m |           0.0008 |           5.0044 |           3.6536 |
[32m[20230113 19:52:27 @agent_ppo2.py:186][0m |          -0.0048 |           4.5258 |           3.6450 |
[32m[20230113 19:52:27 @agent_ppo2.py:186][0m |          -0.0068 |           4.4068 |           3.6460 |
[32m[20230113 19:52:27 @agent_ppo2.py:186][0m |          -0.0085 |           4.3333 |           3.6455 |
[32m[20230113 19:52:27 @agent_ppo2.py:186][0m |          -0.0096 |           4.2578 |           3.6456 |
[32m[20230113 19:52:27 @agent_ppo2.py:186][0m |          -0.0066 |           4.3867 |           3.6419 |
[32m[20230113 19:52:27 @agent_ppo2.py:186][0m |          -0.0112 |           4.1549 |           3.6416 |
[32m[20230113 19:52:27 @agent_ppo2.py:186][0m |          -0.0110 |           4.1162 |           3.6415 |
[32m[20230113 19:52:27 @agent_ppo2.py:186][0m |          -0.0119 |           4.0735 |           3.6426 |
[32m[20230113 19:52:27 @agent_ppo2.py:186][0m |          -0.0126 |           4.0510 |           3.6379 |
[32m[20230113 19:52:27 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.40
[32m[20230113 19:52:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.45
[32m[20230113 19:52:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.49
[32m[20230113 19:52:28 @agent_ppo2.py:144][0m Total time:       7.92 min
[32m[20230113 19:52:28 @agent_ppo2.py:146][0m 698368 total steps have happened
[32m[20230113 19:52:28 @agent_ppo2.py:122][0m #------------------------ Iteration 341 --------------------------#
[32m[20230113 19:52:28 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:28 @agent_ppo2.py:186][0m |          -0.0022 |           5.4224 |           3.6315 |
[32m[20230113 19:52:28 @agent_ppo2.py:186][0m |          -0.0046 |           5.1524 |           3.6226 |
[32m[20230113 19:52:28 @agent_ppo2.py:186][0m |          -0.0040 |           5.0428 |           3.6235 |
[32m[20230113 19:52:28 @agent_ppo2.py:186][0m |          -0.0012 |           5.1959 |           3.6224 |
[32m[20230113 19:52:28 @agent_ppo2.py:186][0m |          -0.0047 |           4.9681 |           3.6221 |
[32m[20230113 19:52:29 @agent_ppo2.py:186][0m |          -0.0070 |           4.8545 |           3.6211 |
[32m[20230113 19:52:29 @agent_ppo2.py:186][0m |          -0.0073 |           4.8332 |           3.6190 |
[32m[20230113 19:52:29 @agent_ppo2.py:186][0m |          -0.0072 |           4.7299 |           3.6213 |
[32m[20230113 19:52:29 @agent_ppo2.py:186][0m |          -0.0117 |           4.6851 |           3.6213 |
[32m[20230113 19:52:29 @agent_ppo2.py:186][0m |          -0.0056 |           4.9979 |           3.6222 |
[32m[20230113 19:52:29 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.57
[32m[20230113 19:52:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.50
[32m[20230113 19:52:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.43
[32m[20230113 19:52:29 @agent_ppo2.py:144][0m Total time:       7.94 min
[32m[20230113 19:52:29 @agent_ppo2.py:146][0m 700416 total steps have happened
[32m[20230113 19:52:29 @agent_ppo2.py:122][0m #------------------------ Iteration 342 --------------------------#
[32m[20230113 19:52:30 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:30 @agent_ppo2.py:186][0m |           0.0027 |           4.4816 |           3.6201 |
[32m[20230113 19:52:30 @agent_ppo2.py:186][0m |          -0.0040 |           4.2467 |           3.6150 |
[32m[20230113 19:52:30 @agent_ppo2.py:186][0m |          -0.0130 |           4.1369 |           3.6139 |
[32m[20230113 19:52:30 @agent_ppo2.py:186][0m |          -0.0026 |           4.0605 |           3.6088 |
[32m[20230113 19:52:30 @agent_ppo2.py:186][0m |          -0.0076 |           4.0002 |           3.6091 |
[32m[20230113 19:52:30 @agent_ppo2.py:186][0m |          -0.0170 |           3.9522 |           3.6103 |
[32m[20230113 19:52:30 @agent_ppo2.py:186][0m |          -0.0054 |           3.9197 |           3.6088 |
[32m[20230113 19:52:30 @agent_ppo2.py:186][0m |          -0.0064 |           3.8579 |           3.6046 |
[32m[20230113 19:52:30 @agent_ppo2.py:186][0m |          -0.0096 |           3.8450 |           3.6066 |
[32m[20230113 19:52:30 @agent_ppo2.py:186][0m |          -0.0154 |           3.8046 |           3.6038 |
[32m[20230113 19:52:30 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.90
[32m[20230113 19:52:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.06
[32m[20230113 19:52:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 229.95
[32m[20230113 19:52:31 @agent_ppo2.py:144][0m Total time:       7.97 min
[32m[20230113 19:52:31 @agent_ppo2.py:146][0m 702464 total steps have happened
[32m[20230113 19:52:31 @agent_ppo2.py:122][0m #------------------------ Iteration 343 --------------------------#
[32m[20230113 19:52:31 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 19:52:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:31 @agent_ppo2.py:186][0m |          -0.0004 |           9.3429 |           3.6116 |
[32m[20230113 19:52:31 @agent_ppo2.py:186][0m |          -0.0045 |           5.0631 |           3.6088 |
[32m[20230113 19:52:31 @agent_ppo2.py:186][0m |          -0.0078 |           5.2590 |           3.6073 |
[32m[20230113 19:52:31 @agent_ppo2.py:186][0m |          -0.0092 |           4.3939 |           3.6045 |
[32m[20230113 19:52:31 @agent_ppo2.py:186][0m |          -0.0108 |           4.3284 |           3.6022 |
[32m[20230113 19:52:31 @agent_ppo2.py:186][0m |          -0.0117 |           3.8692 |           3.5999 |
[32m[20230113 19:52:31 @agent_ppo2.py:186][0m |          -0.0122 |           3.7657 |           3.5980 |
[32m[20230113 19:52:31 @agent_ppo2.py:186][0m |          -0.0135 |           3.7551 |           3.5956 |
[32m[20230113 19:52:31 @agent_ppo2.py:186][0m |          -0.0138 |           3.5496 |           3.5966 |
[32m[20230113 19:52:31 @agent_ppo2.py:186][0m |          -0.0139 |           3.5173 |           3.5937 |
[32m[20230113 19:52:31 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 19:52:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 132.68
[32m[20230113 19:52:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.73
[32m[20230113 19:52:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.57
[32m[20230113 19:52:32 @agent_ppo2.py:144][0m Total time:       7.99 min
[32m[20230113 19:52:32 @agent_ppo2.py:146][0m 704512 total steps have happened
[32m[20230113 19:52:32 @agent_ppo2.py:122][0m #------------------------ Iteration 344 --------------------------#
[32m[20230113 19:52:32 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:52:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:32 @agent_ppo2.py:186][0m |          -0.0001 |           3.3517 |           3.6069 |
[32m[20230113 19:52:32 @agent_ppo2.py:186][0m |          -0.0058 |           3.1177 |           3.6037 |
[32m[20230113 19:52:32 @agent_ppo2.py:186][0m |          -0.0087 |           3.0315 |           3.6025 |
[32m[20230113 19:52:32 @agent_ppo2.py:186][0m |          -0.0103 |           2.9560 |           3.6022 |
[32m[20230113 19:52:33 @agent_ppo2.py:186][0m |          -0.0102 |           2.8915 |           3.6000 |
[32m[20230113 19:52:33 @agent_ppo2.py:186][0m |          -0.0094 |           2.8599 |           3.6002 |
[32m[20230113 19:52:33 @agent_ppo2.py:186][0m |          -0.0108 |           2.7912 |           3.6002 |
[32m[20230113 19:52:33 @agent_ppo2.py:186][0m |          -0.0104 |           2.7664 |           3.6012 |
[32m[20230113 19:52:33 @agent_ppo2.py:186][0m |          -0.0118 |           2.7188 |           3.6010 |
[32m[20230113 19:52:33 @agent_ppo2.py:186][0m |          -0.0125 |           2.6947 |           3.6001 |
[32m[20230113 19:52:33 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:52:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.09
[32m[20230113 19:52:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.55
[32m[20230113 19:52:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.72
[32m[20230113 19:52:33 @agent_ppo2.py:144][0m Total time:       8.01 min
[32m[20230113 19:52:33 @agent_ppo2.py:146][0m 706560 total steps have happened
[32m[20230113 19:52:33 @agent_ppo2.py:122][0m #------------------------ Iteration 345 --------------------------#
[32m[20230113 19:52:34 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:34 @agent_ppo2.py:186][0m |           0.0082 |          16.8413 |           3.6398 |
[32m[20230113 19:52:34 @agent_ppo2.py:186][0m |          -0.0062 |           6.8742 |           3.6335 |
[32m[20230113 19:52:34 @agent_ppo2.py:186][0m |          -0.0110 |           5.7809 |           3.6335 |
[32m[20230113 19:52:34 @agent_ppo2.py:186][0m |          -0.0133 |           5.3005 |           3.6287 |
[32m[20230113 19:52:34 @agent_ppo2.py:186][0m |          -0.0148 |           5.0367 |           3.6255 |
[32m[20230113 19:52:34 @agent_ppo2.py:186][0m |          -0.0162 |           4.8209 |           3.6242 |
[32m[20230113 19:52:34 @agent_ppo2.py:186][0m |          -0.0165 |           4.5758 |           3.6225 |
[32m[20230113 19:52:34 @agent_ppo2.py:186][0m |          -0.0163 |           4.4579 |           3.6209 |
[32m[20230113 19:52:34 @agent_ppo2.py:186][0m |          -0.0184 |           4.2980 |           3.6207 |
[32m[20230113 19:52:34 @agent_ppo2.py:186][0m |          -0.0156 |           4.1774 |           3.6183 |
[32m[20230113 19:52:34 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:52:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 145.44
[32m[20230113 19:52:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 216.72
[32m[20230113 19:52:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.39
[32m[20230113 19:52:35 @agent_ppo2.py:144][0m Total time:       8.03 min
[32m[20230113 19:52:35 @agent_ppo2.py:146][0m 708608 total steps have happened
[32m[20230113 19:52:35 @agent_ppo2.py:122][0m #------------------------ Iteration 346 --------------------------#
[32m[20230113 19:52:35 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:52:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:35 @agent_ppo2.py:186][0m |          -0.0005 |           6.4082 |           3.6766 |
[32m[20230113 19:52:35 @agent_ppo2.py:186][0m |          -0.0034 |           5.9164 |           3.6755 |
[32m[20230113 19:52:35 @agent_ppo2.py:186][0m |          -0.0077 |           5.7717 |           3.6710 |
[32m[20230113 19:52:35 @agent_ppo2.py:186][0m |          -0.0098 |           5.6837 |           3.6703 |
[32m[20230113 19:52:35 @agent_ppo2.py:186][0m |          -0.0115 |           5.6433 |           3.6694 |
[32m[20230113 19:52:35 @agent_ppo2.py:186][0m |          -0.0102 |           5.5993 |           3.6692 |
[32m[20230113 19:52:35 @agent_ppo2.py:186][0m |          -0.0125 |           5.5467 |           3.6689 |
[32m[20230113 19:52:35 @agent_ppo2.py:186][0m |          -0.0088 |           5.8914 |           3.6682 |
[32m[20230113 19:52:35 @agent_ppo2.py:186][0m |          -0.0132 |           5.4678 |           3.6682 |
[32m[20230113 19:52:35 @agent_ppo2.py:186][0m |          -0.0069 |           5.6139 |           3.6686 |
[32m[20230113 19:52:35 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:52:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.44
[32m[20230113 19:52:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.84
[32m[20230113 19:52:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.47
[32m[20230113 19:52:36 @agent_ppo2.py:144][0m Total time:       8.06 min
[32m[20230113 19:52:36 @agent_ppo2.py:146][0m 710656 total steps have happened
[32m[20230113 19:52:36 @agent_ppo2.py:122][0m #------------------------ Iteration 347 --------------------------#
[32m[20230113 19:52:36 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 19:52:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:36 @agent_ppo2.py:186][0m |           0.0024 |          12.6766 |           3.7047 |
[32m[20230113 19:52:36 @agent_ppo2.py:186][0m |          -0.0031 |           6.6299 |           3.7004 |
[32m[20230113 19:52:37 @agent_ppo2.py:186][0m |          -0.0064 |           5.8804 |           3.6962 |
[32m[20230113 19:52:37 @agent_ppo2.py:186][0m |          -0.0099 |           5.6215 |           3.6948 |
[32m[20230113 19:52:37 @agent_ppo2.py:186][0m |          -0.0099 |           5.3843 |           3.6931 |
[32m[20230113 19:52:37 @agent_ppo2.py:186][0m |          -0.0122 |           5.1939 |           3.6909 |
[32m[20230113 19:52:37 @agent_ppo2.py:186][0m |          -0.0129 |           5.0486 |           3.6887 |
[32m[20230113 19:52:37 @agent_ppo2.py:186][0m |          -0.0116 |           5.1088 |           3.6865 |
[32m[20230113 19:52:37 @agent_ppo2.py:186][0m |          -0.0113 |           4.8333 |           3.6858 |
[32m[20230113 19:52:37 @agent_ppo2.py:186][0m |          -0.0146 |           4.6892 |           3.6842 |
[32m[20230113 19:52:37 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 19:52:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 142.28
[32m[20230113 19:52:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.82
[32m[20230113 19:52:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 60.08
[32m[20230113 19:52:37 @agent_ppo2.py:144][0m Total time:       8.08 min
[32m[20230113 19:52:37 @agent_ppo2.py:146][0m 712704 total steps have happened
[32m[20230113 19:52:37 @agent_ppo2.py:122][0m #------------------------ Iteration 348 --------------------------#
[32m[20230113 19:52:38 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:52:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:38 @agent_ppo2.py:186][0m |           0.0006 |           5.7217 |           3.6327 |
[32m[20230113 19:52:38 @agent_ppo2.py:186][0m |          -0.0029 |           5.5375 |           3.6299 |
[32m[20230113 19:52:38 @agent_ppo2.py:186][0m |          -0.0054 |           5.3683 |           3.6266 |
[32m[20230113 19:52:38 @agent_ppo2.py:186][0m |          -0.0061 |           5.2866 |           3.6262 |
[32m[20230113 19:52:38 @agent_ppo2.py:186][0m |          -0.0078 |           5.2512 |           3.6226 |
[32m[20230113 19:52:38 @agent_ppo2.py:186][0m |          -0.0083 |           5.1699 |           3.6252 |
[32m[20230113 19:52:38 @agent_ppo2.py:186][0m |          -0.0090 |           5.1140 |           3.6219 |
[32m[20230113 19:52:38 @agent_ppo2.py:186][0m |          -0.0103 |           5.0767 |           3.6226 |
[32m[20230113 19:52:38 @agent_ppo2.py:186][0m |          -0.0110 |           5.0850 |           3.6218 |
[32m[20230113 19:52:38 @agent_ppo2.py:186][0m |          -0.0112 |           5.0177 |           3.6212 |
[32m[20230113 19:52:38 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:52:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.11
[32m[20230113 19:52:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.10
[32m[20230113 19:52:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 79.45
[32m[20230113 19:52:39 @agent_ppo2.py:144][0m Total time:       8.10 min
[32m[20230113 19:52:39 @agent_ppo2.py:146][0m 714752 total steps have happened
[32m[20230113 19:52:39 @agent_ppo2.py:122][0m #------------------------ Iteration 349 --------------------------#
[32m[20230113 19:52:39 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:52:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:39 @agent_ppo2.py:186][0m |           0.0004 |           3.8906 |           3.6833 |
[32m[20230113 19:52:39 @agent_ppo2.py:186][0m |          -0.0025 |           3.6912 |           3.6788 |
[32m[20230113 19:52:39 @agent_ppo2.py:186][0m |          -0.0051 |           3.6111 |           3.6701 |
[32m[20230113 19:52:39 @agent_ppo2.py:186][0m |          -0.0067 |           3.5417 |           3.6674 |
[32m[20230113 19:52:39 @agent_ppo2.py:186][0m |          -0.0077 |           3.5002 |           3.6671 |
[32m[20230113 19:52:39 @agent_ppo2.py:186][0m |          -0.0078 |           3.4645 |           3.6676 |
[32m[20230113 19:52:39 @agent_ppo2.py:186][0m |          -0.0085 |           3.4501 |           3.6657 |
[32m[20230113 19:52:39 @agent_ppo2.py:186][0m |          -0.0092 |           3.4117 |           3.6687 |
[32m[20230113 19:52:39 @agent_ppo2.py:186][0m |          -0.0091 |           3.3983 |           3.6667 |
[32m[20230113 19:52:39 @agent_ppo2.py:186][0m |          -0.0098 |           3.3666 |           3.6701 |
[32m[20230113 19:52:39 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:52:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 215.55
[32m[20230113 19:52:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 218.92
[32m[20230113 19:52:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.15
[32m[20230113 19:52:40 @agent_ppo2.py:144][0m Total time:       8.12 min
[32m[20230113 19:52:40 @agent_ppo2.py:146][0m 716800 total steps have happened
[32m[20230113 19:52:40 @agent_ppo2.py:122][0m #------------------------ Iteration 350 --------------------------#
[32m[20230113 19:52:40 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:40 @agent_ppo2.py:186][0m |           0.0001 |           4.8880 |           3.7210 |
[32m[20230113 19:52:41 @agent_ppo2.py:186][0m |          -0.0051 |           4.6837 |           3.7173 |
[32m[20230113 19:52:41 @agent_ppo2.py:186][0m |          -0.0077 |           4.4862 |           3.7147 |
[32m[20230113 19:52:41 @agent_ppo2.py:186][0m |          -0.0087 |           4.2833 |           3.7115 |
[32m[20230113 19:52:41 @agent_ppo2.py:186][0m |          -0.0098 |           4.1207 |           3.7116 |
[32m[20230113 19:52:41 @agent_ppo2.py:186][0m |          -0.0110 |           4.0416 |           3.7108 |
[32m[20230113 19:52:41 @agent_ppo2.py:186][0m |          -0.0117 |           3.8542 |           3.7123 |
[32m[20230113 19:52:41 @agent_ppo2.py:186][0m |          -0.0099 |           3.7751 |           3.7103 |
[32m[20230113 19:52:41 @agent_ppo2.py:186][0m |          -0.0127 |           3.6292 |           3.7109 |
[32m[20230113 19:52:41 @agent_ppo2.py:186][0m |          -0.0124 |           3.5662 |           3.7123 |
[32m[20230113 19:52:41 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.11
[32m[20230113 19:52:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.19
[32m[20230113 19:52:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.10
[32m[20230113 19:52:41 @agent_ppo2.py:144][0m Total time:       8.15 min
[32m[20230113 19:52:41 @agent_ppo2.py:146][0m 718848 total steps have happened
[32m[20230113 19:52:41 @agent_ppo2.py:122][0m #------------------------ Iteration 351 --------------------------#
[32m[20230113 19:52:42 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:52:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:42 @agent_ppo2.py:186][0m |          -0.0001 |           5.9834 |           3.6985 |
[32m[20230113 19:52:42 @agent_ppo2.py:186][0m |          -0.0063 |           5.7475 |           3.6882 |
[32m[20230113 19:52:42 @agent_ppo2.py:186][0m |          -0.0085 |           5.6678 |           3.6850 |
[32m[20230113 19:52:42 @agent_ppo2.py:186][0m |          -0.0090 |           5.5947 |           3.6820 |
[32m[20230113 19:52:42 @agent_ppo2.py:186][0m |          -0.0101 |           5.5423 |           3.6810 |
[32m[20230113 19:52:42 @agent_ppo2.py:186][0m |          -0.0107 |           5.4723 |           3.6829 |
[32m[20230113 19:52:42 @agent_ppo2.py:186][0m |          -0.0110 |           5.4610 |           3.6809 |
[32m[20230113 19:52:42 @agent_ppo2.py:186][0m |          -0.0113 |           5.4191 |           3.6796 |
[32m[20230113 19:52:42 @agent_ppo2.py:186][0m |          -0.0118 |           5.3775 |           3.6799 |
[32m[20230113 19:52:42 @agent_ppo2.py:186][0m |          -0.0120 |           5.3372 |           3.6807 |
[32m[20230113 19:52:42 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:52:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.22
[32m[20230113 19:52:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.96
[32m[20230113 19:52:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 133.80
[32m[20230113 19:52:43 @agent_ppo2.py:144][0m Total time:       8.17 min
[32m[20230113 19:52:43 @agent_ppo2.py:146][0m 720896 total steps have happened
[32m[20230113 19:52:43 @agent_ppo2.py:122][0m #------------------------ Iteration 352 --------------------------#
[32m[20230113 19:52:43 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:43 @agent_ppo2.py:186][0m |          -0.0038 |           5.8199 |           3.6782 |
[32m[20230113 19:52:43 @agent_ppo2.py:186][0m |          -0.0076 |           4.8913 |           3.6790 |
[32m[20230113 19:52:43 @agent_ppo2.py:186][0m |          -0.0059 |           4.4395 |           3.6790 |
[32m[20230113 19:52:43 @agent_ppo2.py:186][0m |          -0.0069 |           4.2150 |           3.6809 |
[32m[20230113 19:52:43 @agent_ppo2.py:186][0m |          -0.0097 |           4.0068 |           3.6810 |
[32m[20230113 19:52:43 @agent_ppo2.py:186][0m |          -0.0122 |           3.8065 |           3.6802 |
[32m[20230113 19:52:43 @agent_ppo2.py:186][0m |          -0.0118 |           3.6926 |           3.6833 |
[32m[20230113 19:52:43 @agent_ppo2.py:186][0m |          -0.0066 |           3.9021 |           3.6858 |
[32m[20230113 19:52:44 @agent_ppo2.py:186][0m |          -0.0139 |           3.5347 |           3.6846 |
[32m[20230113 19:52:44 @agent_ppo2.py:186][0m |          -0.0044 |           3.9401 |           3.6874 |
[32m[20230113 19:52:44 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 212.17
[32m[20230113 19:52:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.93
[32m[20230113 19:52:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.95
[32m[20230113 19:52:44 @agent_ppo2.py:144][0m Total time:       8.19 min
[32m[20230113 19:52:44 @agent_ppo2.py:146][0m 722944 total steps have happened
[32m[20230113 19:52:44 @agent_ppo2.py:122][0m #------------------------ Iteration 353 --------------------------#
[32m[20230113 19:52:44 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:45 @agent_ppo2.py:186][0m |          -0.0008 |           5.7701 |           3.6897 |
[32m[20230113 19:52:45 @agent_ppo2.py:186][0m |          -0.0048 |           5.4174 |           3.6880 |
[32m[20230113 19:52:45 @agent_ppo2.py:186][0m |          -0.0066 |           5.2794 |           3.6897 |
[32m[20230113 19:52:45 @agent_ppo2.py:186][0m |          -0.0063 |           5.1820 |           3.6882 |
[32m[20230113 19:52:45 @agent_ppo2.py:186][0m |          -0.0096 |           5.0802 |           3.6904 |
[32m[20230113 19:52:45 @agent_ppo2.py:186][0m |          -0.0067 |           5.0727 |           3.6886 |
[32m[20230113 19:52:45 @agent_ppo2.py:186][0m |          -0.0099 |           4.9185 |           3.6886 |
[32m[20230113 19:52:45 @agent_ppo2.py:186][0m |          -0.0112 |           4.8512 |           3.6895 |
[32m[20230113 19:52:45 @agent_ppo2.py:186][0m |          -0.0101 |           4.7946 |           3.6884 |
[32m[20230113 19:52:45 @agent_ppo2.py:186][0m |          -0.0117 |           4.7395 |           3.6891 |
[32m[20230113 19:52:45 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:52:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.07
[32m[20230113 19:52:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.41
[32m[20230113 19:52:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.95
[32m[20230113 19:52:45 @agent_ppo2.py:144][0m Total time:       8.21 min
[32m[20230113 19:52:45 @agent_ppo2.py:146][0m 724992 total steps have happened
[32m[20230113 19:52:45 @agent_ppo2.py:122][0m #------------------------ Iteration 354 --------------------------#
[32m[20230113 19:52:46 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:52:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:46 @agent_ppo2.py:186][0m |           0.0011 |           6.0733 |           3.5719 |
[32m[20230113 19:52:46 @agent_ppo2.py:186][0m |          -0.0037 |           5.6047 |           3.5657 |
[32m[20230113 19:52:46 @agent_ppo2.py:186][0m |          -0.0064 |           5.4198 |           3.5620 |
[32m[20230113 19:52:46 @agent_ppo2.py:186][0m |          -0.0069 |           5.2840 |           3.5623 |
[32m[20230113 19:52:46 @agent_ppo2.py:186][0m |          -0.0090 |           5.2188 |           3.5609 |
[32m[20230113 19:52:46 @agent_ppo2.py:186][0m |          -0.0086 |           5.0976 |           3.5637 |
[32m[20230113 19:52:46 @agent_ppo2.py:186][0m |          -0.0085 |           5.0774 |           3.5609 |
[32m[20230113 19:52:46 @agent_ppo2.py:186][0m |          -0.0081 |           5.0295 |           3.5627 |
[32m[20230113 19:52:46 @agent_ppo2.py:186][0m |          -0.0076 |           4.9200 |           3.5629 |
[32m[20230113 19:52:46 @agent_ppo2.py:186][0m |          -0.0092 |           4.8096 |           3.5600 |
[32m[20230113 19:52:46 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:52:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.92
[32m[20230113 19:52:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.15
[32m[20230113 19:52:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.11
[32m[20230113 19:52:47 @agent_ppo2.py:144][0m Total time:       8.23 min
[32m[20230113 19:52:47 @agent_ppo2.py:146][0m 727040 total steps have happened
[32m[20230113 19:52:47 @agent_ppo2.py:122][0m #------------------------ Iteration 355 --------------------------#
[32m[20230113 19:52:47 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:47 @agent_ppo2.py:186][0m |           0.0058 |           5.1160 |           3.7380 |
[32m[20230113 19:52:47 @agent_ppo2.py:186][0m |           0.0016 |           4.9172 |           3.7293 |
[32m[20230113 19:52:47 @agent_ppo2.py:186][0m |           0.0011 |           4.7558 |           3.7266 |
[32m[20230113 19:52:47 @agent_ppo2.py:186][0m |          -0.0090 |           4.6137 |           3.7327 |
[32m[20230113 19:52:47 @agent_ppo2.py:186][0m |          -0.0078 |           4.5211 |           3.7287 |
[32m[20230113 19:52:47 @agent_ppo2.py:186][0m |          -0.0129 |           4.5084 |           3.7311 |
[32m[20230113 19:52:47 @agent_ppo2.py:186][0m |          -0.0114 |           4.4603 |           3.7361 |
[32m[20230113 19:52:48 @agent_ppo2.py:186][0m |          -0.0141 |           4.4175 |           3.7321 |
[32m[20230113 19:52:48 @agent_ppo2.py:186][0m |          -0.0128 |           4.3846 |           3.7343 |
[32m[20230113 19:52:48 @agent_ppo2.py:186][0m |          -0.0084 |           4.3834 |           3.7324 |
[32m[20230113 19:52:48 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.83
[32m[20230113 19:52:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.88
[32m[20230113 19:52:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.66
[32m[20230113 19:52:48 @agent_ppo2.py:144][0m Total time:       8.26 min
[32m[20230113 19:52:48 @agent_ppo2.py:146][0m 729088 total steps have happened
[32m[20230113 19:52:48 @agent_ppo2.py:122][0m #------------------------ Iteration 356 --------------------------#
[32m[20230113 19:52:48 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:52:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:49 @agent_ppo2.py:186][0m |           0.0019 |           5.5049 |           3.7017 |
[32m[20230113 19:52:49 @agent_ppo2.py:186][0m |          -0.0027 |           5.3093 |           3.7028 |
[32m[20230113 19:52:49 @agent_ppo2.py:186][0m |          -0.0064 |           5.1876 |           3.7008 |
[32m[20230113 19:52:49 @agent_ppo2.py:186][0m |          -0.0069 |           5.1245 |           3.7011 |
[32m[20230113 19:52:49 @agent_ppo2.py:186][0m |          -0.0079 |           5.0640 |           3.7002 |
[32m[20230113 19:52:49 @agent_ppo2.py:186][0m |          -0.0091 |           5.0173 |           3.7012 |
[32m[20230113 19:52:49 @agent_ppo2.py:186][0m |          -0.0085 |           5.0348 |           3.7002 |
[32m[20230113 19:52:49 @agent_ppo2.py:186][0m |          -0.0083 |           5.0095 |           3.7015 |
[32m[20230113 19:52:49 @agent_ppo2.py:186][0m |          -0.0121 |           4.9700 |           3.7001 |
[32m[20230113 19:52:49 @agent_ppo2.py:186][0m |          -0.0106 |           4.8777 |           3.7027 |
[32m[20230113 19:52:49 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:52:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.24
[32m[20230113 19:52:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.88
[32m[20230113 19:52:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.55
[32m[20230113 19:52:49 @agent_ppo2.py:144][0m Total time:       8.28 min
[32m[20230113 19:52:49 @agent_ppo2.py:146][0m 731136 total steps have happened
[32m[20230113 19:52:49 @agent_ppo2.py:122][0m #------------------------ Iteration 357 --------------------------#
[32m[20230113 19:52:50 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:50 @agent_ppo2.py:186][0m |          -0.0018 |           5.4528 |           3.8361 |
[32m[20230113 19:52:50 @agent_ppo2.py:186][0m |          -0.0078 |           5.1568 |           3.8244 |
[32m[20230113 19:52:50 @agent_ppo2.py:186][0m |          -0.0082 |           5.0227 |           3.8244 |
[32m[20230113 19:52:50 @agent_ppo2.py:186][0m |          -0.0109 |           4.9610 |           3.8215 |
[32m[20230113 19:52:50 @agent_ppo2.py:186][0m |          -0.0079 |           4.8809 |           3.8234 |
[32m[20230113 19:52:50 @agent_ppo2.py:186][0m |          -0.0034 |           5.0385 |           3.8271 |
[32m[20230113 19:52:50 @agent_ppo2.py:186][0m |          -0.0123 |           4.7820 |           3.8242 |
[32m[20230113 19:52:50 @agent_ppo2.py:186][0m |          -0.0135 |           4.7356 |           3.8215 |
[32m[20230113 19:52:50 @agent_ppo2.py:186][0m |          -0.0074 |           4.8016 |           3.8228 |
[32m[20230113 19:52:50 @agent_ppo2.py:186][0m |          -0.0065 |           4.7426 |           3.8207 |
[32m[20230113 19:52:50 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.52
[32m[20230113 19:52:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.19
[32m[20230113 19:52:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.69
[32m[20230113 19:52:51 @agent_ppo2.py:144][0m Total time:       8.30 min
[32m[20230113 19:52:51 @agent_ppo2.py:146][0m 733184 total steps have happened
[32m[20230113 19:52:51 @agent_ppo2.py:122][0m #------------------------ Iteration 358 --------------------------#
[32m[20230113 19:52:51 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:51 @agent_ppo2.py:186][0m |           0.0055 |           3.8175 |           3.8274 |
[32m[20230113 19:52:51 @agent_ppo2.py:186][0m |          -0.0011 |           3.5679 |           3.8244 |
[32m[20230113 19:52:51 @agent_ppo2.py:186][0m |          -0.0034 |           3.4904 |           3.8246 |
[32m[20230113 19:52:51 @agent_ppo2.py:186][0m |          -0.0058 |           3.4284 |           3.8222 |
[32m[20230113 19:52:51 @agent_ppo2.py:186][0m |          -0.0068 |           3.3767 |           3.8227 |
[32m[20230113 19:52:51 @agent_ppo2.py:186][0m |          -0.0038 |           3.3669 |           3.8238 |
[32m[20230113 19:52:52 @agent_ppo2.py:186][0m |          -0.0026 |           3.4200 |           3.8237 |
[32m[20230113 19:52:52 @agent_ppo2.py:186][0m |          -0.0055 |           3.2713 |           3.8217 |
[32m[20230113 19:52:52 @agent_ppo2.py:186][0m |          -0.0088 |           3.2524 |           3.8234 |
[32m[20230113 19:52:52 @agent_ppo2.py:186][0m |          -0.0066 |           3.2547 |           3.8234 |
[32m[20230113 19:52:52 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:52:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.79
[32m[20230113 19:52:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.42
[32m[20230113 19:52:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.30
[32m[20230113 19:52:52 @agent_ppo2.py:144][0m Total time:       8.33 min
[32m[20230113 19:52:52 @agent_ppo2.py:146][0m 735232 total steps have happened
[32m[20230113 19:52:52 @agent_ppo2.py:122][0m #------------------------ Iteration 359 --------------------------#
[32m[20230113 19:52:53 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:52:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:53 @agent_ppo2.py:186][0m |          -0.0004 |           4.6561 |           3.8522 |
[32m[20230113 19:52:53 @agent_ppo2.py:186][0m |          -0.0033 |           4.5233 |           3.8457 |
[32m[20230113 19:52:53 @agent_ppo2.py:186][0m |          -0.0051 |           4.4790 |           3.8405 |
[32m[20230113 19:52:53 @agent_ppo2.py:186][0m |          -0.0072 |           4.3820 |           3.8436 |
[32m[20230113 19:52:53 @agent_ppo2.py:186][0m |          -0.0080 |           4.3556 |           3.8379 |
[32m[20230113 19:52:53 @agent_ppo2.py:186][0m |          -0.0070 |           4.3116 |           3.8423 |
[32m[20230113 19:52:53 @agent_ppo2.py:186][0m |          -0.0093 |           4.3144 |           3.8398 |
[32m[20230113 19:52:53 @agent_ppo2.py:186][0m |          -0.0088 |           4.2954 |           3.8388 |
[32m[20230113 19:52:53 @agent_ppo2.py:186][0m |          -0.0084 |           4.2474 |           3.8395 |
[32m[20230113 19:52:53 @agent_ppo2.py:186][0m |          -0.0062 |           4.3089 |           3.8390 |
[32m[20230113 19:52:53 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:52:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.36
[32m[20230113 19:52:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.31
[32m[20230113 19:52:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.70
[32m[20230113 19:52:53 @agent_ppo2.py:144][0m Total time:       8.35 min
[32m[20230113 19:52:53 @agent_ppo2.py:146][0m 737280 total steps have happened
[32m[20230113 19:52:53 @agent_ppo2.py:122][0m #------------------------ Iteration 360 --------------------------#
[32m[20230113 19:52:54 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:52:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:54 @agent_ppo2.py:186][0m |           0.0036 |           3.8661 |           3.8283 |
[32m[20230113 19:52:54 @agent_ppo2.py:186][0m |          -0.0057 |           3.4933 |           3.8251 |
[32m[20230113 19:52:54 @agent_ppo2.py:186][0m |          -0.0064 |           3.3450 |           3.8237 |
[32m[20230113 19:52:54 @agent_ppo2.py:186][0m |          -0.0078 |           3.3380 |           3.8249 |
[32m[20230113 19:52:54 @agent_ppo2.py:186][0m |          -0.0075 |           3.2526 |           3.8238 |
[32m[20230113 19:52:54 @agent_ppo2.py:186][0m |          -0.0089 |           3.2097 |           3.8250 |
[32m[20230113 19:52:54 @agent_ppo2.py:186][0m |          -0.0100 |           3.1487 |           3.8242 |
[32m[20230113 19:52:54 @agent_ppo2.py:186][0m |          -0.0109 |           3.1228 |           3.8228 |
[32m[20230113 19:52:54 @agent_ppo2.py:186][0m |          -0.0105 |           3.0934 |           3.8229 |
[32m[20230113 19:52:54 @agent_ppo2.py:186][0m |          -0.0100 |           3.0786 |           3.8246 |
[32m[20230113 19:52:54 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:52:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 218.66
[32m[20230113 19:52:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.00
[32m[20230113 19:52:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 183.36
[32m[20230113 19:52:55 @agent_ppo2.py:144][0m Total time:       8.37 min
[32m[20230113 19:52:55 @agent_ppo2.py:146][0m 739328 total steps have happened
[32m[20230113 19:52:55 @agent_ppo2.py:122][0m #------------------------ Iteration 361 --------------------------#
[32m[20230113 19:52:55 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:52:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:55 @agent_ppo2.py:186][0m |           0.0020 |           3.3631 |           3.7772 |
[32m[20230113 19:52:55 @agent_ppo2.py:186][0m |          -0.0055 |           3.1753 |           3.7695 |
[32m[20230113 19:52:55 @agent_ppo2.py:186][0m |          -0.0079 |           3.0752 |           3.7682 |
[32m[20230113 19:52:56 @agent_ppo2.py:186][0m |          -0.0066 |           3.0628 |           3.7707 |
[32m[20230113 19:52:56 @agent_ppo2.py:186][0m |          -0.0092 |           3.0092 |           3.7691 |
[32m[20230113 19:52:56 @agent_ppo2.py:186][0m |          -0.0084 |           3.0137 |           3.7694 |
[32m[20230113 19:52:56 @agent_ppo2.py:186][0m |          -0.0089 |           2.9517 |           3.7693 |
[32m[20230113 19:52:56 @agent_ppo2.py:186][0m |          -0.0107 |           2.9387 |           3.7708 |
[32m[20230113 19:52:56 @agent_ppo2.py:186][0m |          -0.0101 |           2.9257 |           3.7717 |
[32m[20230113 19:52:56 @agent_ppo2.py:186][0m |          -0.0108 |           2.9055 |           3.7713 |
[32m[20230113 19:52:56 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 210.40
[32m[20230113 19:52:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.44
[32m[20230113 19:52:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.81
[32m[20230113 19:52:56 @agent_ppo2.py:144][0m Total time:       8.39 min
[32m[20230113 19:52:56 @agent_ppo2.py:146][0m 741376 total steps have happened
[32m[20230113 19:52:56 @agent_ppo2.py:122][0m #------------------------ Iteration 362 --------------------------#
[32m[20230113 19:52:57 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:52:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:57 @agent_ppo2.py:186][0m |          -0.0001 |           5.4090 |           3.8554 |
[32m[20230113 19:52:57 @agent_ppo2.py:186][0m |          -0.0009 |           5.2382 |           3.8493 |
[32m[20230113 19:52:57 @agent_ppo2.py:186][0m |          -0.0058 |           4.9700 |           3.8468 |
[32m[20230113 19:52:57 @agent_ppo2.py:186][0m |          -0.0051 |           4.9503 |           3.8468 |
[32m[20230113 19:52:57 @agent_ppo2.py:186][0m |          -0.0070 |           4.7975 |           3.8439 |
[32m[20230113 19:52:57 @agent_ppo2.py:186][0m |          -0.0059 |           4.8165 |           3.8442 |
[32m[20230113 19:52:57 @agent_ppo2.py:186][0m |          -0.0083 |           4.7018 |           3.8449 |
[32m[20230113 19:52:57 @agent_ppo2.py:186][0m |          -0.0082 |           4.6580 |           3.8417 |
[32m[20230113 19:52:57 @agent_ppo2.py:186][0m |          -0.0090 |           4.6256 |           3.8412 |
[32m[20230113 19:52:57 @agent_ppo2.py:186][0m |          -0.0068 |           4.6491 |           3.8407 |
[32m[20230113 19:52:57 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.21
[32m[20230113 19:52:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.39
[32m[20230113 19:52:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.26
[32m[20230113 19:52:58 @agent_ppo2.py:144][0m Total time:       8.42 min
[32m[20230113 19:52:58 @agent_ppo2.py:146][0m 743424 total steps have happened
[32m[20230113 19:52:58 @agent_ppo2.py:122][0m #------------------------ Iteration 363 --------------------------#
[32m[20230113 19:52:58 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:52:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:52:58 @agent_ppo2.py:186][0m |          -0.0003 |           4.5005 |           3.7148 |
[32m[20230113 19:52:58 @agent_ppo2.py:186][0m |          -0.0065 |           4.1904 |           3.7036 |
[32m[20230113 19:52:58 @agent_ppo2.py:186][0m |          -0.0081 |           4.0227 |           3.7010 |
[32m[20230113 19:52:58 @agent_ppo2.py:186][0m |          -0.0099 |           3.9297 |           3.7032 |
[32m[20230113 19:52:58 @agent_ppo2.py:186][0m |          -0.0095 |           3.8590 |           3.7011 |
[32m[20230113 19:52:58 @agent_ppo2.py:186][0m |          -0.0112 |           3.7780 |           3.7006 |
[32m[20230113 19:52:58 @agent_ppo2.py:186][0m |          -0.0107 |           3.7552 |           3.6975 |
[32m[20230113 19:52:58 @agent_ppo2.py:186][0m |          -0.0104 |           3.6650 |           3.6985 |
[32m[20230113 19:52:58 @agent_ppo2.py:186][0m |          -0.0124 |           3.5959 |           3.6982 |
[32m[20230113 19:52:59 @agent_ppo2.py:186][0m |          -0.0127 |           3.5645 |           3.6963 |
[32m[20230113 19:52:59 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:52:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.31
[32m[20230113 19:52:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.15
[32m[20230113 19:52:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.77
[32m[20230113 19:52:59 @agent_ppo2.py:144][0m Total time:       8.44 min
[32m[20230113 19:52:59 @agent_ppo2.py:146][0m 745472 total steps have happened
[32m[20230113 19:52:59 @agent_ppo2.py:122][0m #------------------------ Iteration 364 --------------------------#
[32m[20230113 19:52:59 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:52:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:00 @agent_ppo2.py:186][0m |           0.0019 |           2.9193 |           3.7574 |
[32m[20230113 19:53:00 @agent_ppo2.py:186][0m |          -0.0048 |           2.7270 |           3.7591 |
[32m[20230113 19:53:00 @agent_ppo2.py:186][0m |           0.0000 |           2.7553 |           3.7571 |
[32m[20230113 19:53:00 @agent_ppo2.py:186][0m |          -0.0019 |           2.5370 |           3.7565 |
[32m[20230113 19:53:00 @agent_ppo2.py:186][0m |          -0.0061 |           2.4247 |           3.7547 |
[32m[20230113 19:53:00 @agent_ppo2.py:186][0m |          -0.0070 |           2.3609 |           3.7541 |
[32m[20230113 19:53:00 @agent_ppo2.py:186][0m |          -0.0054 |           2.2949 |           3.7539 |
[32m[20230113 19:53:00 @agent_ppo2.py:186][0m |          -0.0068 |           2.2456 |           3.7540 |
[32m[20230113 19:53:00 @agent_ppo2.py:186][0m |          -0.0072 |           2.1933 |           3.7529 |
[32m[20230113 19:53:00 @agent_ppo2.py:186][0m |          -0.0095 |           2.1764 |           3.7520 |
[32m[20230113 19:53:00 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:53:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.16
[32m[20230113 19:53:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.65
[32m[20230113 19:53:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.58
[32m[20230113 19:53:00 @agent_ppo2.py:144][0m Total time:       8.46 min
[32m[20230113 19:53:00 @agent_ppo2.py:146][0m 747520 total steps have happened
[32m[20230113 19:53:00 @agent_ppo2.py:122][0m #------------------------ Iteration 365 --------------------------#
[32m[20230113 19:53:01 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:53:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:01 @agent_ppo2.py:186][0m |          -0.0009 |           5.4191 |           3.7828 |
[32m[20230113 19:53:01 @agent_ppo2.py:186][0m |          -0.0045 |           5.1336 |           3.7762 |
[32m[20230113 19:53:01 @agent_ppo2.py:186][0m |          -0.0062 |           4.9734 |           3.7675 |
[32m[20230113 19:53:01 @agent_ppo2.py:186][0m |          -0.0066 |           4.9445 |           3.7744 |
[32m[20230113 19:53:01 @agent_ppo2.py:186][0m |          -0.0059 |           4.9706 |           3.7680 |
[32m[20230113 19:53:01 @agent_ppo2.py:186][0m |          -0.0108 |           4.7829 |           3.7700 |
[32m[20230113 19:53:01 @agent_ppo2.py:186][0m |          -0.0104 |           4.7614 |           3.7709 |
[32m[20230113 19:53:01 @agent_ppo2.py:186][0m |          -0.0111 |           4.7544 |           3.7717 |
[32m[20230113 19:53:01 @agent_ppo2.py:186][0m |          -0.0089 |           4.7799 |           3.7701 |
[32m[20230113 19:53:01 @agent_ppo2.py:186][0m |          -0.0103 |           4.7262 |           3.7715 |
[32m[20230113 19:53:01 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.22
[32m[20230113 19:53:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.33
[32m[20230113 19:53:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.73
[32m[20230113 19:53:02 @agent_ppo2.py:144][0m Total time:       8.48 min
[32m[20230113 19:53:02 @agent_ppo2.py:146][0m 749568 total steps have happened
[32m[20230113 19:53:02 @agent_ppo2.py:122][0m #------------------------ Iteration 366 --------------------------#
[32m[20230113 19:53:02 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:53:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:02 @agent_ppo2.py:186][0m |           0.0021 |          10.3787 |           3.8228 |
[32m[20230113 19:53:02 @agent_ppo2.py:186][0m |          -0.0030 |           4.0797 |           3.8203 |
[32m[20230113 19:53:02 @agent_ppo2.py:186][0m |          -0.0057 |           3.5368 |           3.8178 |
[32m[20230113 19:53:02 @agent_ppo2.py:186][0m |          -0.0065 |           3.3063 |           3.8143 |
[32m[20230113 19:53:02 @agent_ppo2.py:186][0m |          -0.0071 |           3.1706 |           3.8146 |
[32m[20230113 19:53:02 @agent_ppo2.py:186][0m |          -0.0075 |           3.0845 |           3.8141 |
[32m[20230113 19:53:03 @agent_ppo2.py:186][0m |          -0.0085 |           2.9850 |           3.8120 |
[32m[20230113 19:53:03 @agent_ppo2.py:186][0m |          -0.0079 |           2.8807 |           3.8116 |
[32m[20230113 19:53:03 @agent_ppo2.py:186][0m |          -0.0090 |           2.8152 |           3.8133 |
[32m[20230113 19:53:03 @agent_ppo2.py:186][0m |          -0.0091 |           2.7820 |           3.8134 |
[32m[20230113 19:53:03 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:53:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 81.86
[32m[20230113 19:53:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.43
[32m[20230113 19:53:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.25
[32m[20230113 19:53:03 @agent_ppo2.py:144][0m Total time:       8.51 min
[32m[20230113 19:53:03 @agent_ppo2.py:146][0m 751616 total steps have happened
[32m[20230113 19:53:03 @agent_ppo2.py:122][0m #------------------------ Iteration 367 --------------------------#
[32m[20230113 19:53:04 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:53:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:04 @agent_ppo2.py:186][0m |           0.0000 |           3.6817 |           3.8337 |
[32m[20230113 19:53:04 @agent_ppo2.py:186][0m |          -0.0021 |           3.4793 |           3.8321 |
[32m[20230113 19:53:04 @agent_ppo2.py:186][0m |          -0.0042 |           3.4070 |           3.8291 |
[32m[20230113 19:53:04 @agent_ppo2.py:186][0m |          -0.0045 |           3.3602 |           3.8299 |
[32m[20230113 19:53:04 @agent_ppo2.py:186][0m |          -0.0059 |           3.3234 |           3.8275 |
[32m[20230113 19:53:04 @agent_ppo2.py:186][0m |          -0.0061 |           3.2630 |           3.8274 |
[32m[20230113 19:53:04 @agent_ppo2.py:186][0m |          -0.0083 |           3.1912 |           3.8278 |
[32m[20230113 19:53:04 @agent_ppo2.py:186][0m |          -0.0082 |           3.1513 |           3.8309 |
[32m[20230113 19:53:04 @agent_ppo2.py:186][0m |          -0.0090 |           3.1144 |           3.8303 |
[32m[20230113 19:53:04 @agent_ppo2.py:186][0m |          -0.0092 |           3.1051 |           3.8299 |
[32m[20230113 19:53:04 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:53:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.59
[32m[20230113 19:53:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.03
[32m[20230113 19:53:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.70
[32m[20230113 19:53:04 @agent_ppo2.py:144][0m Total time:       8.53 min
[32m[20230113 19:53:04 @agent_ppo2.py:146][0m 753664 total steps have happened
[32m[20230113 19:53:04 @agent_ppo2.py:122][0m #------------------------ Iteration 368 --------------------------#
[32m[20230113 19:53:05 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:53:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:05 @agent_ppo2.py:186][0m |          -0.0004 |           5.2512 |           3.8041 |
[32m[20230113 19:53:05 @agent_ppo2.py:186][0m |          -0.0031 |           5.1389 |           3.8023 |
[32m[20230113 19:53:05 @agent_ppo2.py:186][0m |          -0.0025 |           5.1930 |           3.8000 |
[32m[20230113 19:53:05 @agent_ppo2.py:186][0m |          -0.0075 |           4.9885 |           3.7999 |
[32m[20230113 19:53:05 @agent_ppo2.py:186][0m |          -0.0081 |           4.9259 |           3.7992 |
[32m[20230113 19:53:05 @agent_ppo2.py:186][0m |          -0.0065 |           5.0984 |           3.8031 |
[32m[20230113 19:53:05 @agent_ppo2.py:186][0m |          -0.0065 |           4.9144 |           3.8029 |
[32m[20230113 19:53:05 @agent_ppo2.py:186][0m |          -0.0088 |           4.8609 |           3.8049 |
[32m[20230113 19:53:05 @agent_ppo2.py:186][0m |          -0.0092 |           4.7909 |           3.8039 |
[32m[20230113 19:53:05 @agent_ppo2.py:186][0m |          -0.0100 |           4.7994 |           3.8063 |
[32m[20230113 19:53:05 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:53:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.26
[32m[20230113 19:53:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.59
[32m[20230113 19:53:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.48
[32m[20230113 19:53:06 @agent_ppo2.py:144][0m Total time:       8.55 min
[32m[20230113 19:53:06 @agent_ppo2.py:146][0m 755712 total steps have happened
[32m[20230113 19:53:06 @agent_ppo2.py:122][0m #------------------------ Iteration 369 --------------------------#
[32m[20230113 19:53:06 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:53:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:06 @agent_ppo2.py:186][0m |           0.0002 |           5.5092 |           3.8825 |
[32m[20230113 19:53:06 @agent_ppo2.py:186][0m |          -0.0030 |           5.3999 |           3.8769 |
[32m[20230113 19:53:06 @agent_ppo2.py:186][0m |          -0.0048 |           5.2907 |           3.8694 |
[32m[20230113 19:53:06 @agent_ppo2.py:186][0m |          -0.0062 |           5.2097 |           3.8678 |
[32m[20230113 19:53:06 @agent_ppo2.py:186][0m |          -0.0057 |           5.2070 |           3.8650 |
[32m[20230113 19:53:07 @agent_ppo2.py:186][0m |          -0.0075 |           5.1187 |           3.8649 |
[32m[20230113 19:53:07 @agent_ppo2.py:186][0m |          -0.0085 |           5.0747 |           3.8606 |
[32m[20230113 19:53:07 @agent_ppo2.py:186][0m |          -0.0086 |           5.0717 |           3.8617 |
[32m[20230113 19:53:07 @agent_ppo2.py:186][0m |          -0.0094 |           5.0665 |           3.8580 |
[32m[20230113 19:53:07 @agent_ppo2.py:186][0m |          -0.0104 |           5.0212 |           3.8570 |
[32m[20230113 19:53:07 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:53:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.23
[32m[20230113 19:53:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.87
[32m[20230113 19:53:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.17
[32m[20230113 19:53:07 @agent_ppo2.py:144][0m Total time:       8.58 min
[32m[20230113 19:53:07 @agent_ppo2.py:146][0m 757760 total steps have happened
[32m[20230113 19:53:07 @agent_ppo2.py:122][0m #------------------------ Iteration 370 --------------------------#
[32m[20230113 19:53:08 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:53:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:08 @agent_ppo2.py:186][0m |           0.0006 |           5.2771 |           3.8412 |
[32m[20230113 19:53:08 @agent_ppo2.py:186][0m |          -0.0039 |           5.0106 |           3.8407 |
[32m[20230113 19:53:08 @agent_ppo2.py:186][0m |          -0.0060 |           4.9104 |           3.8371 |
[32m[20230113 19:53:08 @agent_ppo2.py:186][0m |          -0.0085 |           4.7563 |           3.8392 |
[32m[20230113 19:53:08 @agent_ppo2.py:186][0m |          -0.0089 |           4.7246 |           3.8396 |
[32m[20230113 19:53:08 @agent_ppo2.py:186][0m |          -0.0091 |           4.6400 |           3.8391 |
[32m[20230113 19:53:08 @agent_ppo2.py:186][0m |          -0.0100 |           4.6047 |           3.8377 |
[32m[20230113 19:53:08 @agent_ppo2.py:186][0m |          -0.0113 |           4.5278 |           3.8395 |
[32m[20230113 19:53:08 @agent_ppo2.py:186][0m |          -0.0123 |           4.4912 |           3.8370 |
[32m[20230113 19:53:08 @agent_ppo2.py:186][0m |          -0.0134 |           4.4357 |           3.8378 |
[32m[20230113 19:53:08 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 214.12
[32m[20230113 19:53:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.64
[32m[20230113 19:53:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.93
[32m[20230113 19:53:08 @agent_ppo2.py:144][0m Total time:       8.60 min
[32m[20230113 19:53:08 @agent_ppo2.py:146][0m 759808 total steps have happened
[32m[20230113 19:53:08 @agent_ppo2.py:122][0m #------------------------ Iteration 371 --------------------------#
[32m[20230113 19:53:09 @agent_ppo2.py:128][0m Sampling time: 0.31 s by 1 slaves
[32m[20230113 19:53:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:09 @agent_ppo2.py:186][0m |          -0.0011 |          16.7253 |           3.7978 |
[32m[20230113 19:53:09 @agent_ppo2.py:186][0m |          -0.0065 |           7.5829 |           3.7949 |
[32m[20230113 19:53:09 @agent_ppo2.py:186][0m |          -0.0083 |           6.1151 |           3.7947 |
[32m[20230113 19:53:09 @agent_ppo2.py:186][0m |          -0.0107 |           5.7367 |           3.7881 |
[32m[20230113 19:53:09 @agent_ppo2.py:186][0m |          -0.0111 |           5.3121 |           3.7906 |
[32m[20230113 19:53:09 @agent_ppo2.py:186][0m |          -0.0119 |           5.0906 |           3.7901 |
[32m[20230113 19:53:09 @agent_ppo2.py:186][0m |          -0.0128 |           4.9233 |           3.7928 |
[32m[20230113 19:53:09 @agent_ppo2.py:186][0m |          -0.0129 |           4.8038 |           3.7907 |
[32m[20230113 19:53:09 @agent_ppo2.py:186][0m |          -0.0139 |           4.6888 |           3.7878 |
[32m[20230113 19:53:09 @agent_ppo2.py:186][0m |          -0.0146 |           4.5807 |           3.7903 |
[32m[20230113 19:53:09 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 19:53:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 97.23
[32m[20230113 19:53:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.41
[32m[20230113 19:53:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.48
[32m[20230113 19:53:10 @agent_ppo2.py:144][0m Total time:       8.62 min
[32m[20230113 19:53:10 @agent_ppo2.py:146][0m 761856 total steps have happened
[32m[20230113 19:53:10 @agent_ppo2.py:122][0m #------------------------ Iteration 372 --------------------------#
[32m[20230113 19:53:10 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:53:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:10 @agent_ppo2.py:186][0m |          -0.0009 |          11.3659 |           3.8819 |
[32m[20230113 19:53:10 @agent_ppo2.py:186][0m |          -0.0026 |           6.2665 |           3.8775 |
[32m[20230113 19:53:10 @agent_ppo2.py:186][0m |          -0.0044 |           5.7674 |           3.8712 |
[32m[20230113 19:53:10 @agent_ppo2.py:186][0m |          -0.0064 |           5.5452 |           3.8686 |
[32m[20230113 19:53:10 @agent_ppo2.py:186][0m |          -0.0063 |           5.0210 |           3.8679 |
[32m[20230113 19:53:10 @agent_ppo2.py:186][0m |          -0.0068 |           4.8712 |           3.8680 |
[32m[20230113 19:53:10 @agent_ppo2.py:186][0m |          -0.0089 |           4.8172 |           3.8671 |
[32m[20230113 19:53:10 @agent_ppo2.py:186][0m |          -0.0076 |           4.5722 |           3.8677 |
[32m[20230113 19:53:10 @agent_ppo2.py:186][0m |          -0.0086 |           4.3591 |           3.8650 |
[32m[20230113 19:53:10 @agent_ppo2.py:186][0m |          -0.0113 |           4.2491 |           3.8673 |
[32m[20230113 19:53:10 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:53:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 163.03
[32m[20230113 19:53:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 208.94
[32m[20230113 19:53:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.52
[32m[20230113 19:53:11 @agent_ppo2.py:144][0m Total time:       8.64 min
[32m[20230113 19:53:11 @agent_ppo2.py:146][0m 763904 total steps have happened
[32m[20230113 19:53:11 @agent_ppo2.py:122][0m #------------------------ Iteration 373 --------------------------#
[32m[20230113 19:53:11 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 19:53:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:11 @agent_ppo2.py:186][0m |          -0.0018 |          29.9437 |           3.8178 |
[32m[20230113 19:53:11 @agent_ppo2.py:186][0m |          -0.0079 |          19.6370 |           3.8088 |
[32m[20230113 19:53:11 @agent_ppo2.py:186][0m |          -0.0108 |          15.6855 |           3.8086 |
[32m[20230113 19:53:11 @agent_ppo2.py:186][0m |          -0.0110 |          13.5980 |           3.8063 |
[32m[20230113 19:53:11 @agent_ppo2.py:186][0m |          -0.0123 |          11.8696 |           3.8034 |
[32m[20230113 19:53:11 @agent_ppo2.py:186][0m |          -0.0136 |          10.7463 |           3.8025 |
[32m[20230113 19:53:11 @agent_ppo2.py:186][0m |          -0.0140 |           9.9030 |           3.8017 |
[32m[20230113 19:53:12 @agent_ppo2.py:186][0m |          -0.0149 |           9.2863 |           3.8004 |
[32m[20230113 19:53:12 @agent_ppo2.py:186][0m |          -0.0150 |           8.8276 |           3.7997 |
[32m[20230113 19:53:12 @agent_ppo2.py:186][0m |          -0.0157 |           8.6152 |           3.8005 |
[32m[20230113 19:53:12 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 19:53:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 114.13
[32m[20230113 19:53:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.20
[32m[20230113 19:53:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.28
[32m[20230113 19:53:12 @agent_ppo2.py:144][0m Total time:       8.66 min
[32m[20230113 19:53:12 @agent_ppo2.py:146][0m 765952 total steps have happened
[32m[20230113 19:53:12 @agent_ppo2.py:122][0m #------------------------ Iteration 374 --------------------------#
[32m[20230113 19:53:12 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:53:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:13 @agent_ppo2.py:186][0m |           0.0005 |           7.4028 |           3.8288 |
[32m[20230113 19:53:13 @agent_ppo2.py:186][0m |          -0.0026 |           6.6137 |           3.8278 |
[32m[20230113 19:53:13 @agent_ppo2.py:186][0m |          -0.0050 |           6.3437 |           3.8243 |
[32m[20230113 19:53:13 @agent_ppo2.py:186][0m |          -0.0066 |           6.2283 |           3.8230 |
[32m[20230113 19:53:13 @agent_ppo2.py:186][0m |          -0.0086 |           6.0221 |           3.8233 |
[32m[20230113 19:53:13 @agent_ppo2.py:186][0m |          -0.0085 |           5.9049 |           3.8223 |
[32m[20230113 19:53:13 @agent_ppo2.py:186][0m |          -0.0088 |           5.9211 |           3.8208 |
[32m[20230113 19:53:13 @agent_ppo2.py:186][0m |          -0.0107 |           5.7179 |           3.8236 |
[32m[20230113 19:53:13 @agent_ppo2.py:186][0m |          -0.0087 |           5.7712 |           3.8242 |
[32m[20230113 19:53:13 @agent_ppo2.py:186][0m |          -0.0098 |           5.6092 |           3.8259 |
[32m[20230113 19:53:13 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:53:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.50
[32m[20230113 19:53:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.04
[32m[20230113 19:53:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.14
[32m[20230113 19:53:13 @agent_ppo2.py:144][0m Total time:       8.68 min
[32m[20230113 19:53:13 @agent_ppo2.py:146][0m 768000 total steps have happened
[32m[20230113 19:53:13 @agent_ppo2.py:122][0m #------------------------ Iteration 375 --------------------------#
[32m[20230113 19:53:14 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:53:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:14 @agent_ppo2.py:186][0m |           0.0003 |           4.8954 |           3.8185 |
[32m[20230113 19:53:14 @agent_ppo2.py:186][0m |          -0.0030 |           4.5819 |           3.8171 |
[32m[20230113 19:53:14 @agent_ppo2.py:186][0m |          -0.0062 |           4.4485 |           3.8151 |
[32m[20230113 19:53:14 @agent_ppo2.py:186][0m |          -0.0074 |           4.3664 |           3.8134 |
[32m[20230113 19:53:14 @agent_ppo2.py:186][0m |          -0.0086 |           4.2867 |           3.8160 |
[32m[20230113 19:53:14 @agent_ppo2.py:186][0m |          -0.0089 |           4.2277 |           3.8139 |
[32m[20230113 19:53:14 @agent_ppo2.py:186][0m |          -0.0104 |           4.1614 |           3.8134 |
[32m[20230113 19:53:14 @agent_ppo2.py:186][0m |          -0.0103 |           4.1169 |           3.8132 |
[32m[20230113 19:53:14 @agent_ppo2.py:186][0m |          -0.0113 |           4.0754 |           3.8149 |
[32m[20230113 19:53:14 @agent_ppo2.py:186][0m |          -0.0117 |           4.0223 |           3.8167 |
[32m[20230113 19:53:14 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.58
[32m[20230113 19:53:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.61
[32m[20230113 19:53:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.53
[32m[20230113 19:53:15 @agent_ppo2.py:144][0m Total time:       8.70 min
[32m[20230113 19:53:15 @agent_ppo2.py:146][0m 770048 total steps have happened
[32m[20230113 19:53:15 @agent_ppo2.py:122][0m #------------------------ Iteration 376 --------------------------#
[32m[20230113 19:53:15 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:53:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:15 @agent_ppo2.py:186][0m |           0.0006 |           6.8366 |           3.9250 |
[32m[20230113 19:53:15 @agent_ppo2.py:186][0m |          -0.0035 |           6.4370 |           3.9183 |
[32m[20230113 19:53:15 @agent_ppo2.py:186][0m |          -0.0046 |           6.2962 |           3.9145 |
[32m[20230113 19:53:15 @agent_ppo2.py:186][0m |          -0.0061 |           6.2063 |           3.9144 |
[32m[20230113 19:53:15 @agent_ppo2.py:186][0m |          -0.0068 |           6.1337 |           3.9124 |
[32m[20230113 19:53:15 @agent_ppo2.py:186][0m |          -0.0082 |           6.0572 |           3.9127 |
[32m[20230113 19:53:15 @agent_ppo2.py:186][0m |          -0.0098 |           5.9631 |           3.9116 |
[32m[20230113 19:53:16 @agent_ppo2.py:186][0m |          -0.0099 |           5.9292 |           3.9143 |
[32m[20230113 19:53:16 @agent_ppo2.py:186][0m |          -0.0107 |           5.8747 |           3.9114 |
[32m[20230113 19:53:16 @agent_ppo2.py:186][0m |          -0.0089 |           5.8871 |           3.9095 |
[32m[20230113 19:53:16 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:53:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.89
[32m[20230113 19:53:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.03
[32m[20230113 19:53:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 117.83
[32m[20230113 19:53:16 @agent_ppo2.py:144][0m Total time:       8.72 min
[32m[20230113 19:53:16 @agent_ppo2.py:146][0m 772096 total steps have happened
[32m[20230113 19:53:16 @agent_ppo2.py:122][0m #------------------------ Iteration 377 --------------------------#
[32m[20230113 19:53:16 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:53:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:17 @agent_ppo2.py:186][0m |           0.0041 |           4.3665 |           3.8260 |
[32m[20230113 19:53:17 @agent_ppo2.py:186][0m |          -0.0035 |           4.1507 |           3.8237 |
[32m[20230113 19:53:17 @agent_ppo2.py:186][0m |          -0.0068 |           4.0421 |           3.8226 |
[32m[20230113 19:53:17 @agent_ppo2.py:186][0m |          -0.0100 |           3.9792 |           3.8221 |
[32m[20230113 19:53:17 @agent_ppo2.py:186][0m |          -0.0088 |           3.9378 |           3.8186 |
[32m[20230113 19:53:17 @agent_ppo2.py:186][0m |          -0.0111 |           3.9307 |           3.8176 |
[32m[20230113 19:53:17 @agent_ppo2.py:186][0m |          -0.0118 |           3.8759 |           3.8171 |
[32m[20230113 19:53:17 @agent_ppo2.py:186][0m |          -0.0135 |           3.8434 |           3.8152 |
[32m[20230113 19:53:17 @agent_ppo2.py:186][0m |          -0.0088 |           3.9136 |           3.8158 |
[32m[20230113 19:53:17 @agent_ppo2.py:186][0m |          -0.0126 |           3.7886 |           3.8179 |
[32m[20230113 19:53:17 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:53:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.01
[32m[20230113 19:53:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.36
[32m[20230113 19:53:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.04
[32m[20230113 19:53:17 @agent_ppo2.py:144][0m Total time:       8.75 min
[32m[20230113 19:53:17 @agent_ppo2.py:146][0m 774144 total steps have happened
[32m[20230113 19:53:17 @agent_ppo2.py:122][0m #------------------------ Iteration 378 --------------------------#
[32m[20230113 19:53:18 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:53:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:18 @agent_ppo2.py:186][0m |           0.0008 |           5.7436 |           3.8031 |
[32m[20230113 19:53:18 @agent_ppo2.py:186][0m |          -0.0017 |           5.5143 |           3.8017 |
[32m[20230113 19:53:18 @agent_ppo2.py:186][0m |          -0.0037 |           5.4313 |           3.8004 |
[32m[20230113 19:53:18 @agent_ppo2.py:186][0m |          -0.0067 |           5.2792 |           3.8000 |
[32m[20230113 19:53:18 @agent_ppo2.py:186][0m |          -0.0073 |           5.2239 |           3.8010 |
[32m[20230113 19:53:18 @agent_ppo2.py:186][0m |          -0.0088 |           5.1030 |           3.7978 |
[32m[20230113 19:53:18 @agent_ppo2.py:186][0m |          -0.0090 |           5.0389 |           3.7958 |
[32m[20230113 19:53:18 @agent_ppo2.py:186][0m |          -0.0093 |           4.9630 |           3.7959 |
[32m[20230113 19:53:18 @agent_ppo2.py:186][0m |          -0.0110 |           4.8288 |           3.7956 |
[32m[20230113 19:53:18 @agent_ppo2.py:186][0m |          -0.0107 |           4.7662 |           3.7960 |
[32m[20230113 19:53:18 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.45
[32m[20230113 19:53:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.00
[32m[20230113 19:53:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.37
[32m[20230113 19:53:19 @agent_ppo2.py:144][0m Total time:       8.77 min
[32m[20230113 19:53:19 @agent_ppo2.py:146][0m 776192 total steps have happened
[32m[20230113 19:53:19 @agent_ppo2.py:122][0m #------------------------ Iteration 379 --------------------------#
[32m[20230113 19:53:19 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:53:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:19 @agent_ppo2.py:186][0m |           0.0006 |           5.1579 |           3.8711 |
[32m[20230113 19:53:19 @agent_ppo2.py:186][0m |          -0.0035 |           4.7712 |           3.8640 |
[32m[20230113 19:53:19 @agent_ppo2.py:186][0m |          -0.0052 |           4.6498 |           3.8633 |
[32m[20230113 19:53:19 @agent_ppo2.py:186][0m |          -0.0066 |           4.5545 |           3.8630 |
[32m[20230113 19:53:19 @agent_ppo2.py:186][0m |          -0.0068 |           4.5206 |           3.8591 |
[32m[20230113 19:53:19 @agent_ppo2.py:186][0m |          -0.0079 |           4.4702 |           3.8587 |
[32m[20230113 19:53:19 @agent_ppo2.py:186][0m |          -0.0079 |           4.4115 |           3.8554 |
[32m[20230113 19:53:20 @agent_ppo2.py:186][0m |          -0.0088 |           4.3612 |           3.8584 |
[32m[20230113 19:53:20 @agent_ppo2.py:186][0m |          -0.0096 |           4.3414 |           3.8575 |
[32m[20230113 19:53:20 @agent_ppo2.py:186][0m |          -0.0105 |           4.2948 |           3.8549 |
[32m[20230113 19:53:20 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.12
[32m[20230113 19:53:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.37
[32m[20230113 19:53:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 141.10
[32m[20230113 19:53:20 @agent_ppo2.py:144][0m Total time:       8.79 min
[32m[20230113 19:53:20 @agent_ppo2.py:146][0m 778240 total steps have happened
[32m[20230113 19:53:20 @agent_ppo2.py:122][0m #------------------------ Iteration 380 --------------------------#
[32m[20230113 19:53:21 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:53:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:21 @agent_ppo2.py:186][0m |          -0.0022 |           4.8119 |           3.8783 |
[32m[20230113 19:53:21 @agent_ppo2.py:186][0m |          -0.0046 |           4.6395 |           3.8745 |
[32m[20230113 19:53:21 @agent_ppo2.py:186][0m |          -0.0013 |           4.6276 |           3.8745 |
[32m[20230113 19:53:21 @agent_ppo2.py:186][0m |          -0.0023 |           4.5006 |           3.8765 |
[32m[20230113 19:53:21 @agent_ppo2.py:186][0m |          -0.0082 |           4.4700 |           3.8752 |
[32m[20230113 19:53:21 @agent_ppo2.py:186][0m |          -0.0061 |           4.4226 |           3.8752 |
[32m[20230113 19:53:21 @agent_ppo2.py:186][0m |          -0.0042 |           4.4370 |           3.8759 |
[32m[20230113 19:53:21 @agent_ppo2.py:186][0m |          -0.0035 |           4.5271 |           3.8769 |
[32m[20230113 19:53:21 @agent_ppo2.py:186][0m |          -0.0086 |           4.3450 |           3.8774 |
[32m[20230113 19:53:21 @agent_ppo2.py:186][0m |          -0.0102 |           4.3193 |           3.8774 |
[32m[20230113 19:53:21 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:53:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.89
[32m[20230113 19:53:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.18
[32m[20230113 19:53:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.99
[32m[20230113 19:53:21 @agent_ppo2.py:144][0m Total time:       8.81 min
[32m[20230113 19:53:21 @agent_ppo2.py:146][0m 780288 total steps have happened
[32m[20230113 19:53:21 @agent_ppo2.py:122][0m #------------------------ Iteration 381 --------------------------#
[32m[20230113 19:53:22 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:53:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:22 @agent_ppo2.py:186][0m |          -0.0005 |           4.9909 |           3.9344 |
[32m[20230113 19:53:22 @agent_ppo2.py:186][0m |          -0.0039 |           4.7677 |           3.9241 |
[32m[20230113 19:53:22 @agent_ppo2.py:186][0m |          -0.0067 |           4.6912 |           3.9262 |
[32m[20230113 19:53:22 @agent_ppo2.py:186][0m |          -0.0080 |           4.6119 |           3.9240 |
[32m[20230113 19:53:22 @agent_ppo2.py:186][0m |          -0.0095 |           4.5835 |           3.9236 |
[32m[20230113 19:53:22 @agent_ppo2.py:186][0m |          -0.0096 |           4.5469 |           3.9220 |
[32m[20230113 19:53:22 @agent_ppo2.py:186][0m |          -0.0100 |           4.5288 |           3.9226 |
[32m[20230113 19:53:22 @agent_ppo2.py:186][0m |          -0.0109 |           4.5485 |           3.9193 |
[32m[20230113 19:53:22 @agent_ppo2.py:186][0m |          -0.0109 |           4.4391 |           3.9211 |
[32m[20230113 19:53:22 @agent_ppo2.py:186][0m |          -0.0111 |           4.4307 |           3.9174 |
[32m[20230113 19:53:22 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:53:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.05
[32m[20230113 19:53:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.21
[32m[20230113 19:53:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 158.03
[32m[20230113 19:53:23 @agent_ppo2.py:144][0m Total time:       8.84 min
[32m[20230113 19:53:23 @agent_ppo2.py:146][0m 782336 total steps have happened
[32m[20230113 19:53:23 @agent_ppo2.py:122][0m #------------------------ Iteration 382 --------------------------#
[32m[20230113 19:53:23 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:53:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:23 @agent_ppo2.py:186][0m |           0.0006 |           4.6698 |           3.9507 |
[32m[20230113 19:53:23 @agent_ppo2.py:186][0m |          -0.0030 |           4.4497 |           3.9465 |
[32m[20230113 19:53:23 @agent_ppo2.py:186][0m |          -0.0051 |           4.3398 |           3.9417 |
[32m[20230113 19:53:24 @agent_ppo2.py:186][0m |          -0.0064 |           4.2530 |           3.9421 |
[32m[20230113 19:53:24 @agent_ppo2.py:186][0m |          -0.0070 |           4.1748 |           3.9396 |
[32m[20230113 19:53:24 @agent_ppo2.py:186][0m |          -0.0076 |           4.1337 |           3.9422 |
[32m[20230113 19:53:24 @agent_ppo2.py:186][0m |          -0.0081 |           4.0686 |           3.9419 |
[32m[20230113 19:53:24 @agent_ppo2.py:186][0m |          -0.0085 |           4.0513 |           3.9418 |
[32m[20230113 19:53:24 @agent_ppo2.py:186][0m |          -0.0087 |           3.9769 |           3.9406 |
[32m[20230113 19:53:24 @agent_ppo2.py:186][0m |          -0.0093 |           3.9399 |           3.9408 |
[32m[20230113 19:53:24 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.05
[32m[20230113 19:53:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.92
[32m[20230113 19:53:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.96
[32m[20230113 19:53:24 @agent_ppo2.py:144][0m Total time:       8.86 min
[32m[20230113 19:53:24 @agent_ppo2.py:146][0m 784384 total steps have happened
[32m[20230113 19:53:24 @agent_ppo2.py:122][0m #------------------------ Iteration 383 --------------------------#
[32m[20230113 19:53:25 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:53:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:25 @agent_ppo2.py:186][0m |           0.0017 |           5.8616 |           3.8931 |
[32m[20230113 19:53:25 @agent_ppo2.py:186][0m |          -0.0067 |           5.4208 |           3.8890 |
[32m[20230113 19:53:25 @agent_ppo2.py:186][0m |          -0.0049 |           5.3451 |           3.8868 |
[32m[20230113 19:53:25 @agent_ppo2.py:186][0m |          -0.0045 |           5.3242 |           3.8832 |
[32m[20230113 19:53:25 @agent_ppo2.py:186][0m |          -0.0106 |           5.0376 |           3.8834 |
[32m[20230113 19:53:25 @agent_ppo2.py:186][0m |          -0.0089 |           4.9880 |           3.8804 |
[32m[20230113 19:53:25 @agent_ppo2.py:186][0m |          -0.0105 |           4.9484 |           3.8778 |
[32m[20230113 19:53:25 @agent_ppo2.py:186][0m |          -0.0091 |           4.9217 |           3.8792 |
[32m[20230113 19:53:25 @agent_ppo2.py:186][0m |          -0.0101 |           4.9129 |           3.8777 |
[32m[20230113 19:53:25 @agent_ppo2.py:186][0m |          -0.0148 |           4.8622 |           3.8771 |
[32m[20230113 19:53:25 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:53:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.61
[32m[20230113 19:53:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.08
[32m[20230113 19:53:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.83
[32m[20230113 19:53:26 @agent_ppo2.py:144][0m Total time:       8.88 min
[32m[20230113 19:53:26 @agent_ppo2.py:146][0m 786432 total steps have happened
[32m[20230113 19:53:26 @agent_ppo2.py:122][0m #------------------------ Iteration 384 --------------------------#
[32m[20230113 19:53:26 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:53:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:26 @agent_ppo2.py:186][0m |           0.0026 |          11.7865 |           3.9699 |
[32m[20230113 19:53:26 @agent_ppo2.py:186][0m |          -0.0029 |           5.8622 |           3.9682 |
[32m[20230113 19:53:26 @agent_ppo2.py:186][0m |          -0.0061 |           5.0860 |           3.9649 |
[32m[20230113 19:53:26 @agent_ppo2.py:186][0m |          -0.0074 |           4.6454 |           3.9645 |
[32m[20230113 19:53:26 @agent_ppo2.py:186][0m |          -0.0089 |           4.3623 |           3.9606 |
[32m[20230113 19:53:26 @agent_ppo2.py:186][0m |          -0.0108 |           4.1886 |           3.9599 |
[32m[20230113 19:53:26 @agent_ppo2.py:186][0m |          -0.0111 |           4.1195 |           3.9596 |
[32m[20230113 19:53:26 @agent_ppo2.py:186][0m |          -0.0119 |           4.0345 |           3.9588 |
[32m[20230113 19:53:26 @agent_ppo2.py:186][0m |          -0.0119 |           3.8611 |           3.9589 |
[32m[20230113 19:53:26 @agent_ppo2.py:186][0m |          -0.0120 |           3.8836 |           3.9571 |
[32m[20230113 19:53:26 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 74.33
[32m[20230113 19:53:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.20
[32m[20230113 19:53:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 113.83
[32m[20230113 19:53:27 @agent_ppo2.py:144][0m Total time:       8.90 min
[32m[20230113 19:53:27 @agent_ppo2.py:146][0m 788480 total steps have happened
[32m[20230113 19:53:27 @agent_ppo2.py:122][0m #------------------------ Iteration 385 --------------------------#
[32m[20230113 19:53:27 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 19:53:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:27 @agent_ppo2.py:186][0m |          -0.0021 |          10.2771 |           3.8601 |
[32m[20230113 19:53:27 @agent_ppo2.py:186][0m |           0.0005 |           7.0721 |           3.8597 |
[32m[20230113 19:53:27 @agent_ppo2.py:186][0m |          -0.0073 |           6.1530 |           3.8583 |
[32m[20230113 19:53:27 @agent_ppo2.py:186][0m |           0.0107 |           5.7492 |           3.8587 |
[32m[20230113 19:53:27 @agent_ppo2.py:186][0m |          -0.0069 |           5.2696 |           3.8526 |
[32m[20230113 19:53:27 @agent_ppo2.py:186][0m |          -0.0142 |           4.7600 |           3.8534 |
[32m[20230113 19:53:27 @agent_ppo2.py:186][0m |          -0.0110 |           4.5430 |           3.8494 |
[32m[20230113 19:53:27 @agent_ppo2.py:186][0m |          -0.0133 |           4.3816 |           3.8501 |
[32m[20230113 19:53:28 @agent_ppo2.py:186][0m |          -0.0201 |           4.2834 |           3.8451 |
[32m[20230113 19:53:28 @agent_ppo2.py:186][0m |          -0.0152 |           4.0790 |           3.8482 |
[32m[20230113 19:53:28 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 19:53:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 133.12
[32m[20230113 19:53:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.89
[32m[20230113 19:53:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.44
[32m[20230113 19:53:28 @agent_ppo2.py:144][0m Total time:       8.92 min
[32m[20230113 19:53:28 @agent_ppo2.py:146][0m 790528 total steps have happened
[32m[20230113 19:53:28 @agent_ppo2.py:122][0m #------------------------ Iteration 386 --------------------------#
[32m[20230113 19:53:28 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:53:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:29 @agent_ppo2.py:186][0m |          -0.0025 |           6.4251 |           3.8139 |
[32m[20230113 19:53:29 @agent_ppo2.py:186][0m |           0.0016 |           5.8313 |           3.8096 |
[32m[20230113 19:53:29 @agent_ppo2.py:186][0m |           0.0018 |           5.6912 |           3.8093 |
[32m[20230113 19:53:29 @agent_ppo2.py:186][0m |          -0.0050 |           5.5115 |           3.8074 |
[32m[20230113 19:53:29 @agent_ppo2.py:186][0m |          -0.0059 |           5.4041 |           3.8091 |
[32m[20230113 19:53:29 @agent_ppo2.py:186][0m |          -0.0088 |           5.3495 |           3.8076 |
[32m[20230113 19:53:29 @agent_ppo2.py:186][0m |          -0.0219 |           5.2581 |           3.8091 |
[32m[20230113 19:53:29 @agent_ppo2.py:186][0m |           0.0641 |          11.0792 |           3.8108 |
[32m[20230113 19:53:29 @agent_ppo2.py:186][0m |          -0.0118 |           5.6161 |           3.8041 |
[32m[20230113 19:53:29 @agent_ppo2.py:186][0m |          -0.0104 |           5.1539 |           3.8109 |
[32m[20230113 19:53:29 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:53:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.04
[32m[20230113 19:53:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.42
[32m[20230113 19:53:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 50.89
[32m[20230113 19:53:29 @agent_ppo2.py:144][0m Total time:       8.94 min
[32m[20230113 19:53:29 @agent_ppo2.py:146][0m 792576 total steps have happened
[32m[20230113 19:53:29 @agent_ppo2.py:122][0m #------------------------ Iteration 387 --------------------------#
[32m[20230113 19:53:30 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 19:53:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:30 @agent_ppo2.py:186][0m |          -0.0070 |          26.8865 |           3.9870 |
[32m[20230113 19:53:30 @agent_ppo2.py:186][0m |          -0.0103 |          20.5249 |           3.9789 |
[32m[20230113 19:53:30 @agent_ppo2.py:186][0m |          -0.0127 |          20.7683 |           3.9805 |
[32m[20230113 19:53:30 @agent_ppo2.py:186][0m |          -0.0126 |          18.1717 |           3.9777 |
[32m[20230113 19:53:30 @agent_ppo2.py:186][0m |          -0.0169 |          17.6546 |           3.9815 |
[32m[20230113 19:53:30 @agent_ppo2.py:186][0m |          -0.0171 |          17.1741 |           3.9759 |
[32m[20230113 19:53:30 @agent_ppo2.py:186][0m |          -0.0006 |          21.7930 |           3.9774 |
[32m[20230113 19:53:30 @agent_ppo2.py:186][0m |          -0.0137 |          16.9027 |           3.9717 |
[32m[20230113 19:53:30 @agent_ppo2.py:186][0m |          -0.0072 |          17.2035 |           3.9721 |
[32m[20230113 19:53:30 @agent_ppo2.py:186][0m |          -0.0195 |          16.3285 |           3.9733 |
[32m[20230113 19:53:30 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 19:53:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 90.74
[32m[20230113 19:53:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.02
[32m[20230113 19:53:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.66
[32m[20230113 19:53:31 @agent_ppo2.py:144][0m Total time:       8.97 min
[32m[20230113 19:53:31 @agent_ppo2.py:146][0m 794624 total steps have happened
[32m[20230113 19:53:31 @agent_ppo2.py:122][0m #------------------------ Iteration 388 --------------------------#
[32m[20230113 19:53:31 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 19:53:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:31 @agent_ppo2.py:186][0m |          -0.0011 |          24.4740 |           4.0571 |
[32m[20230113 19:53:31 @agent_ppo2.py:186][0m |          -0.0050 |          13.3888 |           4.0509 |
[32m[20230113 19:53:31 @agent_ppo2.py:186][0m |          -0.0055 |           9.6906 |           4.0530 |
[32m[20230113 19:53:31 @agent_ppo2.py:186][0m |          -0.0074 |           8.5317 |           4.0515 |
[32m[20230113 19:53:31 @agent_ppo2.py:186][0m |          -0.0080 |           7.9171 |           4.0444 |
[32m[20230113 19:53:31 @agent_ppo2.py:186][0m |          -0.0097 |           7.4039 |           4.0476 |
[32m[20230113 19:53:31 @agent_ppo2.py:186][0m |          -0.0102 |           7.2685 |           4.0481 |
[32m[20230113 19:53:31 @agent_ppo2.py:186][0m |          -0.0104 |           6.8445 |           4.0461 |
[32m[20230113 19:53:31 @agent_ppo2.py:186][0m |          -0.0113 |           6.5757 |           4.0457 |
[32m[20230113 19:53:32 @agent_ppo2.py:186][0m |          -0.0122 |           6.4057 |           4.0432 |
[32m[20230113 19:53:32 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 19:53:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 153.60
[32m[20230113 19:53:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.25
[32m[20230113 19:53:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.82
[32m[20230113 19:53:32 @agent_ppo2.py:144][0m Total time:       8.99 min
[32m[20230113 19:53:32 @agent_ppo2.py:146][0m 796672 total steps have happened
[32m[20230113 19:53:32 @agent_ppo2.py:122][0m #------------------------ Iteration 389 --------------------------#
[32m[20230113 19:53:32 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:53:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:32 @agent_ppo2.py:186][0m |          -0.0096 |          12.1167 |           3.8635 |
[32m[20230113 19:53:33 @agent_ppo2.py:186][0m |          -0.0038 |           8.8704 |           3.8612 |
[32m[20230113 19:53:33 @agent_ppo2.py:186][0m |          -0.0024 |           8.4645 |           3.8628 |
[32m[20230113 19:53:33 @agent_ppo2.py:186][0m |          -0.0028 |           7.7911 |           3.8613 |
[32m[20230113 19:53:33 @agent_ppo2.py:186][0m |          -0.0097 |           7.3615 |           3.8609 |
[32m[20230113 19:53:33 @agent_ppo2.py:186][0m |          -0.0071 |           7.0259 |           3.8616 |
[32m[20230113 19:53:33 @agent_ppo2.py:186][0m |          -0.0094 |           6.7510 |           3.8637 |
[32m[20230113 19:53:33 @agent_ppo2.py:186][0m |          -0.0125 |           6.5262 |           3.8631 |
[32m[20230113 19:53:33 @agent_ppo2.py:186][0m |          -0.0130 |           6.3785 |           3.8651 |
[32m[20230113 19:53:33 @agent_ppo2.py:186][0m |          -0.0048 |           6.4197 |           3.8639 |
[32m[20230113 19:53:33 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.05
[32m[20230113 19:53:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.14
[32m[20230113 19:53:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.78
[32m[20230113 19:53:33 @agent_ppo2.py:144][0m Total time:       9.01 min
[32m[20230113 19:53:33 @agent_ppo2.py:146][0m 798720 total steps have happened
[32m[20230113 19:53:33 @agent_ppo2.py:122][0m #------------------------ Iteration 390 --------------------------#
[32m[20230113 19:53:34 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:53:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:34 @agent_ppo2.py:186][0m |          -0.0009 |           7.0056 |           3.9632 |
[32m[20230113 19:53:34 @agent_ppo2.py:186][0m |          -0.0038 |           6.4260 |           3.9608 |
[32m[20230113 19:53:34 @agent_ppo2.py:186][0m |          -0.0077 |           6.2403 |           3.9513 |
[32m[20230113 19:53:34 @agent_ppo2.py:186][0m |          -0.0076 |           6.1219 |           3.9471 |
[32m[20230113 19:53:34 @agent_ppo2.py:186][0m |          -0.0086 |           6.0227 |           3.9453 |
[32m[20230113 19:53:34 @agent_ppo2.py:186][0m |          -0.0117 |           5.9794 |           3.9399 |
[32m[20230113 19:53:34 @agent_ppo2.py:186][0m |          -0.0119 |           5.9056 |           3.9406 |
[32m[20230113 19:53:34 @agent_ppo2.py:186][0m |          -0.0111 |           5.8413 |           3.9398 |
[32m[20230113 19:53:34 @agent_ppo2.py:186][0m |          -0.0067 |           5.8396 |           3.9371 |
[32m[20230113 19:53:34 @agent_ppo2.py:186][0m |          -0.0082 |           5.8403 |           3.9353 |
[32m[20230113 19:53:34 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.46
[32m[20230113 19:53:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.84
[32m[20230113 19:53:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.14
[32m[20230113 19:53:35 @agent_ppo2.py:144][0m Total time:       9.03 min
[32m[20230113 19:53:35 @agent_ppo2.py:146][0m 800768 total steps have happened
[32m[20230113 19:53:35 @agent_ppo2.py:122][0m #------------------------ Iteration 391 --------------------------#
[32m[20230113 19:53:35 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:53:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:35 @agent_ppo2.py:186][0m |          -0.0039 |           6.2427 |           3.9846 |
[32m[20230113 19:53:35 @agent_ppo2.py:186][0m |          -0.0052 |           6.0327 |           3.9726 |
[32m[20230113 19:53:35 @agent_ppo2.py:186][0m |          -0.0090 |           5.7536 |           3.9728 |
[32m[20230113 19:53:35 @agent_ppo2.py:186][0m |          -0.0097 |           5.6272 |           3.9774 |
[32m[20230113 19:53:35 @agent_ppo2.py:186][0m |          -0.0090 |           5.6100 |           3.9731 |
[32m[20230113 19:53:35 @agent_ppo2.py:186][0m |          -0.0097 |           5.6484 |           3.9735 |
[32m[20230113 19:53:35 @agent_ppo2.py:186][0m |          -0.0127 |           5.4627 |           3.9763 |
[32m[20230113 19:53:36 @agent_ppo2.py:186][0m |          -0.0130 |           5.4366 |           3.9749 |
[32m[20230113 19:53:36 @agent_ppo2.py:186][0m |          -0.0132 |           5.4123 |           3.9772 |
[32m[20230113 19:53:36 @agent_ppo2.py:186][0m |          -0.0128 |           5.3499 |           3.9738 |
[32m[20230113 19:53:36 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:53:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.46
[32m[20230113 19:53:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.61
[32m[20230113 19:53:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.02
[32m[20230113 19:53:36 @agent_ppo2.py:144][0m Total time:       9.06 min
[32m[20230113 19:53:36 @agent_ppo2.py:146][0m 802816 total steps have happened
[32m[20230113 19:53:36 @agent_ppo2.py:122][0m #------------------------ Iteration 392 --------------------------#
[32m[20230113 19:53:36 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:53:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:37 @agent_ppo2.py:186][0m |           0.0018 |           5.0217 |           3.9169 |
[32m[20230113 19:53:37 @agent_ppo2.py:186][0m |          -0.0075 |           4.6979 |           3.9121 |
[32m[20230113 19:53:37 @agent_ppo2.py:186][0m |          -0.0083 |           4.6676 |           3.9046 |
[32m[20230113 19:53:37 @agent_ppo2.py:186][0m |          -0.0083 |           4.5918 |           3.9020 |
[32m[20230113 19:53:37 @agent_ppo2.py:186][0m |          -0.0109 |           4.5486 |           3.8989 |
[32m[20230113 19:53:37 @agent_ppo2.py:186][0m |          -0.0088 |           4.5960 |           3.8972 |
[32m[20230113 19:53:37 @agent_ppo2.py:186][0m |          -0.0148 |           4.4853 |           3.8963 |
[32m[20230113 19:53:37 @agent_ppo2.py:186][0m |          -0.0045 |           4.6843 |           3.8937 |
[32m[20230113 19:53:37 @agent_ppo2.py:186][0m |          -0.0104 |           4.4259 |           3.8929 |
[32m[20230113 19:53:37 @agent_ppo2.py:186][0m |          -0.0126 |           4.4132 |           3.8921 |
[32m[20230113 19:53:37 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:53:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.51
[32m[20230113 19:53:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.71
[32m[20230113 19:53:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 145.85
[32m[20230113 19:53:37 @agent_ppo2.py:144][0m Total time:       9.08 min
[32m[20230113 19:53:37 @agent_ppo2.py:146][0m 804864 total steps have happened
[32m[20230113 19:53:37 @agent_ppo2.py:122][0m #------------------------ Iteration 393 --------------------------#
[32m[20230113 19:53:38 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:53:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:38 @agent_ppo2.py:186][0m |           0.0029 |           3.8039 |           3.9894 |
[32m[20230113 19:53:38 @agent_ppo2.py:186][0m |          -0.0017 |           3.7420 |           3.9880 |
[32m[20230113 19:53:38 @agent_ppo2.py:186][0m |          -0.0080 |           3.5199 |           3.9846 |
[32m[20230113 19:53:38 @agent_ppo2.py:186][0m |          -0.0068 |           3.6199 |           3.9878 |
[32m[20230113 19:53:38 @agent_ppo2.py:186][0m |          -0.0019 |           3.5400 |           3.9841 |
[32m[20230113 19:53:38 @agent_ppo2.py:186][0m |          -0.0099 |           3.4077 |           3.9865 |
[32m[20230113 19:53:38 @agent_ppo2.py:186][0m |          -0.0108 |           3.3802 |           3.9848 |
[32m[20230113 19:53:38 @agent_ppo2.py:186][0m |          -0.0104 |           3.3778 |           3.9856 |
[32m[20230113 19:53:38 @agent_ppo2.py:186][0m |          -0.0126 |           3.3350 |           3.9842 |
[32m[20230113 19:53:38 @agent_ppo2.py:186][0m |          -0.0122 |           3.3356 |           3.9849 |
[32m[20230113 19:53:38 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:53:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.83
[32m[20230113 19:53:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.22
[32m[20230113 19:53:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.39
[32m[20230113 19:53:39 @agent_ppo2.py:144][0m Total time:       9.10 min
[32m[20230113 19:53:39 @agent_ppo2.py:146][0m 806912 total steps have happened
[32m[20230113 19:53:39 @agent_ppo2.py:122][0m #------------------------ Iteration 394 --------------------------#
[32m[20230113 19:53:39 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:53:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:39 @agent_ppo2.py:186][0m |           0.0034 |           5.4642 |           3.9268 |
[32m[20230113 19:53:39 @agent_ppo2.py:186][0m |          -0.0021 |           5.2500 |           3.9290 |
[32m[20230113 19:53:39 @agent_ppo2.py:186][0m |          -0.0059 |           5.1553 |           3.9246 |
[32m[20230113 19:53:40 @agent_ppo2.py:186][0m |          -0.0108 |           5.1320 |           3.9199 |
[32m[20230113 19:53:40 @agent_ppo2.py:186][0m |          -0.0090 |           5.1216 |           3.9185 |
[32m[20230113 19:53:40 @agent_ppo2.py:186][0m |          -0.0102 |           5.0335 |           3.9175 |
[32m[20230113 19:53:40 @agent_ppo2.py:186][0m |          -0.0107 |           5.0136 |           3.9166 |
[32m[20230113 19:53:40 @agent_ppo2.py:186][0m |          -0.0040 |           5.3787 |           3.9137 |
[32m[20230113 19:53:40 @agent_ppo2.py:186][0m |          -0.0013 |           5.2680 |           3.9119 |
[32m[20230113 19:53:40 @agent_ppo2.py:186][0m |          -0.0103 |           4.9364 |           3.9131 |
[32m[20230113 19:53:40 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:53:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.12
[32m[20230113 19:53:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.35
[32m[20230113 19:53:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.17
[32m[20230113 19:53:40 @agent_ppo2.py:144][0m Total time:       9.13 min
[32m[20230113 19:53:40 @agent_ppo2.py:146][0m 808960 total steps have happened
[32m[20230113 19:53:40 @agent_ppo2.py:122][0m #------------------------ Iteration 395 --------------------------#
[32m[20230113 19:53:41 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:53:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:41 @agent_ppo2.py:186][0m |           0.0111 |           5.0249 |           3.9445 |
[32m[20230113 19:53:41 @agent_ppo2.py:186][0m |          -0.0047 |           3.7317 |           3.9300 |
[32m[20230113 19:53:41 @agent_ppo2.py:186][0m |          -0.0083 |           3.4400 |           3.9336 |
[32m[20230113 19:53:41 @agent_ppo2.py:186][0m |          -0.0134 |           3.3183 |           3.9275 |
[32m[20230113 19:53:41 @agent_ppo2.py:186][0m |          -0.0119 |           3.2155 |           3.9261 |
[32m[20230113 19:53:41 @agent_ppo2.py:186][0m |          -0.0143 |           3.1713 |           3.9268 |
[32m[20230113 19:53:41 @agent_ppo2.py:186][0m |          -0.0143 |           3.1113 |           3.9286 |
[32m[20230113 19:53:41 @agent_ppo2.py:186][0m |          -0.0154 |           3.0717 |           3.9249 |
[32m[20230113 19:53:41 @agent_ppo2.py:186][0m |          -0.0071 |           3.1556 |           3.9273 |
[32m[20230113 19:53:41 @agent_ppo2.py:186][0m |          -0.0145 |           2.9960 |           3.9223 |
[32m[20230113 19:53:41 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:53:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.91
[32m[20230113 19:53:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.11
[32m[20230113 19:53:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.87
[32m[20230113 19:53:42 @agent_ppo2.py:144][0m Total time:       9.15 min
[32m[20230113 19:53:42 @agent_ppo2.py:146][0m 811008 total steps have happened
[32m[20230113 19:53:42 @agent_ppo2.py:122][0m #------------------------ Iteration 396 --------------------------#
[32m[20230113 19:53:42 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 19:53:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:42 @agent_ppo2.py:186][0m |           0.0029 |          10.8511 |           4.0214 |
[32m[20230113 19:53:42 @agent_ppo2.py:186][0m |          -0.0046 |           4.7171 |           4.0173 |
[32m[20230113 19:53:42 @agent_ppo2.py:186][0m |          -0.0098 |           4.2967 |           4.0118 |
[32m[20230113 19:53:42 @agent_ppo2.py:186][0m |          -0.0094 |           4.2047 |           4.0153 |
[32m[20230113 19:53:42 @agent_ppo2.py:186][0m |          -0.0139 |           3.9869 |           4.0109 |
[32m[20230113 19:53:42 @agent_ppo2.py:186][0m |          -0.0140 |           3.8086 |           4.0111 |
[32m[20230113 19:53:42 @agent_ppo2.py:186][0m |          -0.0109 |           3.7304 |           4.0088 |
[32m[20230113 19:53:42 @agent_ppo2.py:186][0m |          -0.0110 |           3.6994 |           4.0094 |
[32m[20230113 19:53:42 @agent_ppo2.py:186][0m |          -0.0160 |           3.6668 |           4.0093 |
[32m[20230113 19:53:42 @agent_ppo2.py:186][0m |          -0.0158 |           3.6293 |           4.0110 |
[32m[20230113 19:53:42 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 19:53:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 117.99
[32m[20230113 19:53:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.11
[32m[20230113 19:53:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 149.86
[32m[20230113 19:53:43 @agent_ppo2.py:144][0m Total time:       9.17 min
[32m[20230113 19:53:43 @agent_ppo2.py:146][0m 813056 total steps have happened
[32m[20230113 19:53:43 @agent_ppo2.py:122][0m #------------------------ Iteration 397 --------------------------#
[32m[20230113 19:53:43 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:53:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:43 @agent_ppo2.py:186][0m |          -0.0010 |           3.6915 |           3.9551 |
[32m[20230113 19:53:43 @agent_ppo2.py:186][0m |          -0.0028 |           3.2663 |           3.9477 |
[32m[20230113 19:53:43 @agent_ppo2.py:186][0m |          -0.0055 |           3.1362 |           3.9419 |
[32m[20230113 19:53:43 @agent_ppo2.py:186][0m |          -0.0089 |           2.9689 |           3.9415 |
[32m[20230113 19:53:43 @agent_ppo2.py:186][0m |          -0.0095 |           2.8968 |           3.9394 |
[32m[20230113 19:53:43 @agent_ppo2.py:186][0m |          -0.0105 |           2.8394 |           3.9359 |
[32m[20230113 19:53:43 @agent_ppo2.py:186][0m |          -0.0115 |           2.7921 |           3.9361 |
[32m[20230113 19:53:44 @agent_ppo2.py:186][0m |          -0.0143 |           2.7420 |           3.9327 |
[32m[20230113 19:53:44 @agent_ppo2.py:186][0m |          -0.0151 |           2.7168 |           3.9317 |
[32m[20230113 19:53:44 @agent_ppo2.py:186][0m |          -0.0074 |           2.8548 |           3.9309 |
[32m[20230113 19:53:44 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:53:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.66
[32m[20230113 19:53:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.80
[32m[20230113 19:53:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.95
[32m[20230113 19:53:44 @agent_ppo2.py:144][0m Total time:       9.19 min
[32m[20230113 19:53:44 @agent_ppo2.py:146][0m 815104 total steps have happened
[32m[20230113 19:53:44 @agent_ppo2.py:122][0m #------------------------ Iteration 398 --------------------------#
[32m[20230113 19:53:44 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:53:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:45 @agent_ppo2.py:186][0m |          -0.0006 |           5.6191 |           3.9570 |
[32m[20230113 19:53:45 @agent_ppo2.py:186][0m |          -0.0057 |           5.3040 |           3.9497 |
[32m[20230113 19:53:45 @agent_ppo2.py:186][0m |          -0.0075 |           5.1698 |           3.9440 |
[32m[20230113 19:53:45 @agent_ppo2.py:186][0m |          -0.0091 |           5.1035 |           3.9435 |
[32m[20230113 19:53:45 @agent_ppo2.py:186][0m |          -0.0093 |           5.0024 |           3.9474 |
[32m[20230113 19:53:45 @agent_ppo2.py:186][0m |          -0.0124 |           4.9320 |           3.9462 |
[32m[20230113 19:53:45 @agent_ppo2.py:186][0m |          -0.0106 |           4.8827 |           3.9463 |
[32m[20230113 19:53:45 @agent_ppo2.py:186][0m |          -0.0122 |           4.8344 |           3.9466 |
[32m[20230113 19:53:45 @agent_ppo2.py:186][0m |          -0.0121 |           4.8448 |           3.9487 |
[32m[20230113 19:53:45 @agent_ppo2.py:186][0m |          -0.0126 |           4.7726 |           3.9466 |
[32m[20230113 19:53:45 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.27
[32m[20230113 19:53:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.69
[32m[20230113 19:53:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 112.10
[32m[20230113 19:53:45 @agent_ppo2.py:144][0m Total time:       9.21 min
[32m[20230113 19:53:45 @agent_ppo2.py:146][0m 817152 total steps have happened
[32m[20230113 19:53:45 @agent_ppo2.py:122][0m #------------------------ Iteration 399 --------------------------#
[32m[20230113 19:53:46 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:53:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:46 @agent_ppo2.py:186][0m |           0.0054 |           4.9456 |           3.8536 |
[32m[20230113 19:53:46 @agent_ppo2.py:186][0m |           0.0009 |           4.7580 |           3.8525 |
[32m[20230113 19:53:46 @agent_ppo2.py:186][0m |          -0.0059 |           4.4618 |           3.8487 |
[32m[20230113 19:53:46 @agent_ppo2.py:186][0m |          -0.0075 |           4.3309 |           3.8458 |
[32m[20230113 19:53:46 @agent_ppo2.py:186][0m |          -0.0093 |           4.2564 |           3.8459 |
[32m[20230113 19:53:46 @agent_ppo2.py:186][0m |          -0.0050 |           4.2746 |           3.8481 |
[32m[20230113 19:53:46 @agent_ppo2.py:186][0m |          -0.0115 |           4.1445 |           3.8464 |
[32m[20230113 19:53:46 @agent_ppo2.py:186][0m |          -0.0072 |           4.1491 |           3.8468 |
[32m[20230113 19:53:46 @agent_ppo2.py:186][0m |          -0.0101 |           4.1370 |           3.8441 |
[32m[20230113 19:53:46 @agent_ppo2.py:186][0m |          -0.0083 |           4.0437 |           3.8459 |
[32m[20230113 19:53:46 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:53:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.46
[32m[20230113 19:53:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.21
[32m[20230113 19:53:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.68
[32m[20230113 19:53:47 @agent_ppo2.py:144][0m Total time:       9.23 min
[32m[20230113 19:53:47 @agent_ppo2.py:146][0m 819200 total steps have happened
[32m[20230113 19:53:47 @agent_ppo2.py:122][0m #------------------------ Iteration 400 --------------------------#
[32m[20230113 19:53:47 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:53:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:47 @agent_ppo2.py:186][0m |          -0.0004 |           5.1597 |           3.9552 |
[32m[20230113 19:53:47 @agent_ppo2.py:186][0m |          -0.0075 |           4.9163 |           3.9493 |
[32m[20230113 19:53:47 @agent_ppo2.py:186][0m |          -0.0103 |           4.8300 |           3.9491 |
[32m[20230113 19:53:47 @agent_ppo2.py:186][0m |          -0.0032 |           4.8810 |           3.9431 |
[32m[20230113 19:53:47 @agent_ppo2.py:186][0m |          -0.0078 |           4.8147 |           3.9460 |
[32m[20230113 19:53:47 @agent_ppo2.py:186][0m |          -0.0127 |           4.6428 |           3.9445 |
[32m[20230113 19:53:47 @agent_ppo2.py:186][0m |          -0.0093 |           4.7658 |           3.9461 |
[32m[20230113 19:53:47 @agent_ppo2.py:186][0m |          -0.0099 |           4.5709 |           3.9431 |
[32m[20230113 19:53:47 @agent_ppo2.py:186][0m |          -0.0119 |           4.5261 |           3.9441 |
[32m[20230113 19:53:48 @agent_ppo2.py:186][0m |          -0.0138 |           4.4890 |           3.9440 |
[32m[20230113 19:53:48 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.76
[32m[20230113 19:53:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.86
[32m[20230113 19:53:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.53
[32m[20230113 19:53:48 @agent_ppo2.py:144][0m Total time:       9.26 min
[32m[20230113 19:53:48 @agent_ppo2.py:146][0m 821248 total steps have happened
[32m[20230113 19:53:48 @agent_ppo2.py:122][0m #------------------------ Iteration 401 --------------------------#
[32m[20230113 19:53:48 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:53:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:49 @agent_ppo2.py:186][0m |           0.0018 |           5.1592 |           4.0189 |
[32m[20230113 19:53:49 @agent_ppo2.py:186][0m |          -0.0008 |           4.8555 |           4.0130 |
[32m[20230113 19:53:49 @agent_ppo2.py:186][0m |          -0.0031 |           4.7202 |           4.0095 |
[32m[20230113 19:53:49 @agent_ppo2.py:186][0m |          -0.0045 |           4.5431 |           4.0101 |
[32m[20230113 19:53:49 @agent_ppo2.py:186][0m |          -0.0053 |           4.3971 |           4.0061 |
[32m[20230113 19:53:49 @agent_ppo2.py:186][0m |          -0.0064 |           4.3112 |           4.0050 |
[32m[20230113 19:53:49 @agent_ppo2.py:186][0m |          -0.0064 |           4.1924 |           4.0073 |
[32m[20230113 19:53:49 @agent_ppo2.py:186][0m |          -0.0079 |           4.1419 |           4.0080 |
[32m[20230113 19:53:49 @agent_ppo2.py:186][0m |          -0.0084 |           4.0133 |           4.0063 |
[32m[20230113 19:53:49 @agent_ppo2.py:186][0m |          -0.0090 |           3.9391 |           4.0047 |
[32m[20230113 19:53:49 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.92
[32m[20230113 19:53:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.88
[32m[20230113 19:53:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.23
[32m[20230113 19:53:49 @agent_ppo2.py:144][0m Total time:       9.28 min
[32m[20230113 19:53:49 @agent_ppo2.py:146][0m 823296 total steps have happened
[32m[20230113 19:53:49 @agent_ppo2.py:122][0m #------------------------ Iteration 402 --------------------------#
[32m[20230113 19:53:50 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:53:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:50 @agent_ppo2.py:186][0m |          -0.0022 |           4.9947 |           3.9193 |
[32m[20230113 19:53:50 @agent_ppo2.py:186][0m |          -0.0078 |           4.6101 |           3.9142 |
[32m[20230113 19:53:50 @agent_ppo2.py:186][0m |          -0.0109 |           4.4901 |           3.9116 |
[32m[20230113 19:53:50 @agent_ppo2.py:186][0m |          -0.0099 |           4.4420 |           3.9085 |
[32m[20230113 19:53:50 @agent_ppo2.py:186][0m |          -0.0099 |           4.3507 |           3.9073 |
[32m[20230113 19:53:50 @agent_ppo2.py:186][0m |          -0.0113 |           4.3099 |           3.9055 |
[32m[20230113 19:53:50 @agent_ppo2.py:186][0m |          -0.0104 |           4.2512 |           3.9085 |
[32m[20230113 19:53:50 @agent_ppo2.py:186][0m |          -0.0126 |           4.2205 |           3.9072 |
[32m[20230113 19:53:50 @agent_ppo2.py:186][0m |          -0.0094 |           4.1656 |           3.9065 |
[32m[20230113 19:53:50 @agent_ppo2.py:186][0m |          -0.0130 |           4.1230 |           3.9064 |
[32m[20230113 19:53:50 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.40
[32m[20230113 19:53:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.55
[32m[20230113 19:53:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 175.93
[32m[20230113 19:53:51 @agent_ppo2.py:144][0m Total time:       9.30 min
[32m[20230113 19:53:51 @agent_ppo2.py:146][0m 825344 total steps have happened
[32m[20230113 19:53:51 @agent_ppo2.py:122][0m #------------------------ Iteration 403 --------------------------#
[32m[20230113 19:53:51 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:53:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:51 @agent_ppo2.py:186][0m |          -0.0032 |           5.5023 |           3.9777 |
[32m[20230113 19:53:51 @agent_ppo2.py:186][0m |          -0.0055 |           5.2585 |           3.9778 |
[32m[20230113 19:53:51 @agent_ppo2.py:186][0m |          -0.0064 |           5.1763 |           3.9763 |
[32m[20230113 19:53:51 @agent_ppo2.py:186][0m |          -0.0108 |           5.0919 |           3.9762 |
[32m[20230113 19:53:51 @agent_ppo2.py:186][0m |          -0.0057 |           5.1605 |           3.9766 |
[32m[20230113 19:53:51 @agent_ppo2.py:186][0m |          -0.0070 |           5.0357 |           3.9760 |
[32m[20230113 19:53:51 @agent_ppo2.py:186][0m |          -0.0105 |           4.9677 |           3.9761 |
[32m[20230113 19:53:52 @agent_ppo2.py:186][0m |          -0.0105 |           5.0205 |           3.9719 |
[32m[20230113 19:53:52 @agent_ppo2.py:186][0m |          -0.0091 |           5.0680 |           3.9733 |
[32m[20230113 19:53:52 @agent_ppo2.py:186][0m |          -0.0117 |           4.8714 |           3.9747 |
[32m[20230113 19:53:52 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.09
[32m[20230113 19:53:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.81
[32m[20230113 19:53:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.01
[32m[20230113 19:53:52 @agent_ppo2.py:144][0m Total time:       9.32 min
[32m[20230113 19:53:52 @agent_ppo2.py:146][0m 827392 total steps have happened
[32m[20230113 19:53:52 @agent_ppo2.py:122][0m #------------------------ Iteration 404 --------------------------#
[32m[20230113 19:53:53 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:53:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:53 @agent_ppo2.py:186][0m |          -0.0000 |           3.3882 |           3.9299 |
[32m[20230113 19:53:53 @agent_ppo2.py:186][0m |          -0.0013 |           3.3069 |           3.9248 |
[32m[20230113 19:53:53 @agent_ppo2.py:186][0m |          -0.0046 |           3.2095 |           3.9245 |
[32m[20230113 19:53:53 @agent_ppo2.py:186][0m |          -0.0057 |           3.1566 |           3.9230 |
[32m[20230113 19:53:53 @agent_ppo2.py:186][0m |          -0.0061 |           3.1289 |           3.9205 |
[32m[20230113 19:53:53 @agent_ppo2.py:186][0m |          -0.0042 |           3.1647 |           3.9189 |
[32m[20230113 19:53:53 @agent_ppo2.py:186][0m |          -0.0084 |           3.0887 |           3.9175 |
[32m[20230113 19:53:53 @agent_ppo2.py:186][0m |          -0.0072 |           3.0697 |           3.9152 |
[32m[20230113 19:53:53 @agent_ppo2.py:186][0m |          -0.0079 |           3.0403 |           3.9147 |
[32m[20230113 19:53:53 @agent_ppo2.py:186][0m |          -0.0084 |           3.0169 |           3.9141 |
[32m[20230113 19:53:53 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:53:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.74
[32m[20230113 19:53:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.16
[32m[20230113 19:53:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.86
[32m[20230113 19:53:53 @agent_ppo2.py:144][0m Total time:       9.35 min
[32m[20230113 19:53:53 @agent_ppo2.py:146][0m 829440 total steps have happened
[32m[20230113 19:53:53 @agent_ppo2.py:122][0m #------------------------ Iteration 405 --------------------------#
[32m[20230113 19:53:54 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:53:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:54 @agent_ppo2.py:186][0m |           0.0015 |           4.9584 |           3.9080 |
[32m[20230113 19:53:54 @agent_ppo2.py:186][0m |          -0.0030 |           4.7735 |           3.9061 |
[32m[20230113 19:53:54 @agent_ppo2.py:186][0m |          -0.0061 |           4.6856 |           3.8991 |
[32m[20230113 19:53:54 @agent_ppo2.py:186][0m |          -0.0063 |           4.6529 |           3.8955 |
[32m[20230113 19:53:54 @agent_ppo2.py:186][0m |          -0.0076 |           4.5767 |           3.8972 |
[32m[20230113 19:53:54 @agent_ppo2.py:186][0m |          -0.0092 |           4.5479 |           3.8977 |
[32m[20230113 19:53:54 @agent_ppo2.py:186][0m |          -0.0077 |           4.5373 |           3.8964 |
[32m[20230113 19:53:54 @agent_ppo2.py:186][0m |          -0.0101 |           4.4815 |           3.8922 |
[32m[20230113 19:53:54 @agent_ppo2.py:186][0m |          -0.0111 |           4.4387 |           3.8921 |
[32m[20230113 19:53:54 @agent_ppo2.py:186][0m |          -0.0110 |           4.4102 |           3.8928 |
[32m[20230113 19:53:54 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.76
[32m[20230113 19:53:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.00
[32m[20230113 19:53:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.42
[32m[20230113 19:53:55 @agent_ppo2.py:144][0m Total time:       9.37 min
[32m[20230113 19:53:55 @agent_ppo2.py:146][0m 831488 total steps have happened
[32m[20230113 19:53:55 @agent_ppo2.py:122][0m #------------------------ Iteration 406 --------------------------#
[32m[20230113 19:53:55 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:53:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:55 @agent_ppo2.py:186][0m |           0.0004 |           5.8154 |           3.9523 |
[32m[20230113 19:53:55 @agent_ppo2.py:186][0m |          -0.0029 |           5.3568 |           3.9513 |
[32m[20230113 19:53:55 @agent_ppo2.py:186][0m |          -0.0085 |           5.1292 |           3.9518 |
[32m[20230113 19:53:55 @agent_ppo2.py:186][0m |          -0.0077 |           5.0580 |           3.9473 |
[32m[20230113 19:53:56 @agent_ppo2.py:186][0m |          -0.0081 |           4.9691 |           3.9454 |
[32m[20230113 19:53:56 @agent_ppo2.py:186][0m |          -0.0104 |           4.8591 |           3.9390 |
[32m[20230113 19:53:56 @agent_ppo2.py:186][0m |          -0.0104 |           4.6809 |           3.9390 |
[32m[20230113 19:53:56 @agent_ppo2.py:186][0m |          -0.0130 |           4.5728 |           3.9382 |
[32m[20230113 19:53:56 @agent_ppo2.py:186][0m |          -0.0107 |           4.5600 |           3.9382 |
[32m[20230113 19:53:56 @agent_ppo2.py:186][0m |          -0.0113 |           4.5101 |           3.9366 |
[32m[20230113 19:53:56 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.04
[32m[20230113 19:53:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.04
[32m[20230113 19:53:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.26
[32m[20230113 19:53:56 @agent_ppo2.py:144][0m Total time:       9.39 min
[32m[20230113 19:53:56 @agent_ppo2.py:146][0m 833536 total steps have happened
[32m[20230113 19:53:56 @agent_ppo2.py:122][0m #------------------------ Iteration 407 --------------------------#
[32m[20230113 19:53:57 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:53:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:57 @agent_ppo2.py:186][0m |           0.0024 |           5.4473 |           3.8643 |
[32m[20230113 19:53:57 @agent_ppo2.py:186][0m |          -0.0070 |           5.1827 |           3.8533 |
[32m[20230113 19:53:57 @agent_ppo2.py:186][0m |          -0.0075 |           5.0732 |           3.8528 |
[32m[20230113 19:53:57 @agent_ppo2.py:186][0m |          -0.0032 |           5.1165 |           3.8456 |
[32m[20230113 19:53:57 @agent_ppo2.py:186][0m |          -0.0074 |           4.9692 |           3.8482 |
[32m[20230113 19:53:57 @agent_ppo2.py:186][0m |           0.0013 |           5.3474 |           3.8455 |
[32m[20230113 19:53:57 @agent_ppo2.py:186][0m |          -0.0123 |           4.8863 |           3.8433 |
[32m[20230113 19:53:57 @agent_ppo2.py:186][0m |          -0.0134 |           4.8100 |           3.8453 |
[32m[20230113 19:53:57 @agent_ppo2.py:186][0m |          -0.0130 |           4.7826 |           3.8438 |
[32m[20230113 19:53:57 @agent_ppo2.py:186][0m |          -0.0082 |           4.7772 |           3.8442 |
[32m[20230113 19:53:57 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:53:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.14
[32m[20230113 19:53:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.04
[32m[20230113 19:53:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.56
[32m[20230113 19:53:57 @agent_ppo2.py:144][0m Total time:       9.42 min
[32m[20230113 19:53:57 @agent_ppo2.py:146][0m 835584 total steps have happened
[32m[20230113 19:53:57 @agent_ppo2.py:122][0m #------------------------ Iteration 408 --------------------------#
[32m[20230113 19:53:58 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:53:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:58 @agent_ppo2.py:186][0m |          -0.0001 |           3.5546 |           3.8900 |
[32m[20230113 19:53:58 @agent_ppo2.py:186][0m |          -0.0050 |           3.2769 |           3.8878 |
[32m[20230113 19:53:58 @agent_ppo2.py:186][0m |          -0.0081 |           3.1111 |           3.8849 |
[32m[20230113 19:53:58 @agent_ppo2.py:186][0m |          -0.0087 |           3.0313 |           3.8853 |
[32m[20230113 19:53:58 @agent_ppo2.py:186][0m |          -0.0103 |           2.9082 |           3.8809 |
[32m[20230113 19:53:58 @agent_ppo2.py:186][0m |          -0.0117 |           2.8304 |           3.8852 |
[32m[20230113 19:53:58 @agent_ppo2.py:186][0m |          -0.0128 |           2.7604 |           3.8835 |
[32m[20230113 19:53:58 @agent_ppo2.py:186][0m |          -0.0132 |           2.6992 |           3.8834 |
[32m[20230113 19:53:58 @agent_ppo2.py:186][0m |          -0.0138 |           2.6434 |           3.8829 |
[32m[20230113 19:53:58 @agent_ppo2.py:186][0m |          -0.0120 |           2.6503 |           3.8803 |
[32m[20230113 19:53:58 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:53:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.95
[32m[20230113 19:53:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.28
[32m[20230113 19:53:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.46
[32m[20230113 19:53:59 @agent_ppo2.py:144][0m Total time:       9.44 min
[32m[20230113 19:53:59 @agent_ppo2.py:146][0m 837632 total steps have happened
[32m[20230113 19:53:59 @agent_ppo2.py:122][0m #------------------------ Iteration 409 --------------------------#
[32m[20230113 19:53:59 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:53:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:53:59 @agent_ppo2.py:186][0m |          -0.0023 |           6.7701 |           3.9594 |
[32m[20230113 19:54:00 @agent_ppo2.py:186][0m |          -0.0047 |           4.6322 |           3.9567 |
[32m[20230113 19:54:00 @agent_ppo2.py:186][0m |          -0.0069 |           4.0948 |           3.9601 |
[32m[20230113 19:54:00 @agent_ppo2.py:186][0m |          -0.0083 |           3.7863 |           3.9580 |
[32m[20230113 19:54:00 @agent_ppo2.py:186][0m |          -0.0110 |           3.5542 |           3.9552 |
[32m[20230113 19:54:00 @agent_ppo2.py:186][0m |          -0.0107 |           3.4407 |           3.9561 |
[32m[20230113 19:54:00 @agent_ppo2.py:186][0m |          -0.0119 |           3.3554 |           3.9577 |
[32m[20230113 19:54:00 @agent_ppo2.py:186][0m |          -0.0127 |           3.2746 |           3.9567 |
[32m[20230113 19:54:00 @agent_ppo2.py:186][0m |          -0.0132 |           3.2113 |           3.9585 |
[32m[20230113 19:54:00 @agent_ppo2.py:186][0m |          -0.0131 |           3.1398 |           3.9561 |
[32m[20230113 19:54:00 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.98
[32m[20230113 19:54:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.25
[32m[20230113 19:54:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.36
[32m[20230113 19:54:00 @agent_ppo2.py:144][0m Total time:       9.46 min
[32m[20230113 19:54:00 @agent_ppo2.py:146][0m 839680 total steps have happened
[32m[20230113 19:54:00 @agent_ppo2.py:122][0m #------------------------ Iteration 410 --------------------------#
[32m[20230113 19:54:01 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 19:54:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:01 @agent_ppo2.py:186][0m |           0.0022 |          23.5441 |           3.9563 |
[32m[20230113 19:54:01 @agent_ppo2.py:186][0m |          -0.0013 |          11.0251 |           3.9536 |
[32m[20230113 19:54:01 @agent_ppo2.py:186][0m |          -0.0037 |           8.9517 |           3.9507 |
[32m[20230113 19:54:01 @agent_ppo2.py:186][0m |          -0.0060 |           7.9882 |           3.9485 |
[32m[20230113 19:54:01 @agent_ppo2.py:186][0m |          -0.0101 |           7.3514 |           3.9476 |
[32m[20230113 19:54:01 @agent_ppo2.py:186][0m |          -0.0098 |           6.9949 |           3.9474 |
[32m[20230113 19:54:01 @agent_ppo2.py:186][0m |          -0.0126 |           6.7789 |           3.9447 |
[32m[20230113 19:54:01 @agent_ppo2.py:186][0m |          -0.0130 |           6.3901 |           3.9434 |
[32m[20230113 19:54:01 @agent_ppo2.py:186][0m |          -0.0123 |           6.1759 |           3.9431 |
[32m[20230113 19:54:01 @agent_ppo2.py:186][0m |          -0.0141 |           5.9697 |           3.9425 |
[32m[20230113 19:54:01 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 19:54:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 92.16
[32m[20230113 19:54:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.30
[32m[20230113 19:54:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.10
[32m[20230113 19:54:01 @agent_ppo2.py:144][0m Total time:       9.48 min
[32m[20230113 19:54:01 @agent_ppo2.py:146][0m 841728 total steps have happened
[32m[20230113 19:54:01 @agent_ppo2.py:122][0m #------------------------ Iteration 411 --------------------------#
[32m[20230113 19:54:02 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:54:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:02 @agent_ppo2.py:186][0m |           0.0017 |           5.7077 |           3.9403 |
[32m[20230113 19:54:02 @agent_ppo2.py:186][0m |          -0.0033 |           4.5818 |           3.9434 |
[32m[20230113 19:54:02 @agent_ppo2.py:186][0m |          -0.0062 |           4.3029 |           3.9396 |
[32m[20230113 19:54:02 @agent_ppo2.py:186][0m |          -0.0076 |           4.0989 |           3.9403 |
[32m[20230113 19:54:02 @agent_ppo2.py:186][0m |          -0.0086 |           3.9398 |           3.9420 |
[32m[20230113 19:54:02 @agent_ppo2.py:186][0m |          -0.0098 |           3.8180 |           3.9436 |
[32m[20230113 19:54:02 @agent_ppo2.py:186][0m |          -0.0099 |           3.6898 |           3.9443 |
[32m[20230113 19:54:02 @agent_ppo2.py:186][0m |          -0.0108 |           3.5957 |           3.9455 |
[32m[20230113 19:54:02 @agent_ppo2.py:186][0m |          -0.0116 |           3.5199 |           3.9485 |
[32m[20230113 19:54:02 @agent_ppo2.py:186][0m |          -0.0112 |           3.4482 |           3.9505 |
[32m[20230113 19:54:02 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:54:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 215.80
[32m[20230113 19:54:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 216.41
[32m[20230113 19:54:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.38
[32m[20230113 19:54:03 @agent_ppo2.py:144][0m Total time:       9.50 min
[32m[20230113 19:54:03 @agent_ppo2.py:146][0m 843776 total steps have happened
[32m[20230113 19:54:03 @agent_ppo2.py:122][0m #------------------------ Iteration 412 --------------------------#
[32m[20230113 19:54:03 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:03 @agent_ppo2.py:186][0m |          -0.0002 |           6.0964 |           3.9702 |
[32m[20230113 19:54:03 @agent_ppo2.py:186][0m |          -0.0047 |           5.6166 |           3.9629 |
[32m[20230113 19:54:03 @agent_ppo2.py:186][0m |          -0.0072 |           5.4476 |           3.9611 |
[32m[20230113 19:54:03 @agent_ppo2.py:186][0m |          -0.0094 |           5.3308 |           3.9574 |
[32m[20230113 19:54:04 @agent_ppo2.py:186][0m |          -0.0098 |           5.2561 |           3.9596 |
[32m[20230113 19:54:04 @agent_ppo2.py:186][0m |          -0.0102 |           5.2038 |           3.9580 |
[32m[20230113 19:54:04 @agent_ppo2.py:186][0m |          -0.0112 |           5.1178 |           3.9584 |
[32m[20230113 19:54:04 @agent_ppo2.py:186][0m |          -0.0117 |           5.0624 |           3.9595 |
[32m[20230113 19:54:04 @agent_ppo2.py:186][0m |          -0.0116 |           5.0104 |           3.9592 |
[32m[20230113 19:54:04 @agent_ppo2.py:186][0m |          -0.0126 |           4.9696 |           3.9579 |
[32m[20230113 19:54:04 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.48
[32m[20230113 19:54:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.41
[32m[20230113 19:54:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.05
[32m[20230113 19:54:04 @agent_ppo2.py:144][0m Total time:       9.53 min
[32m[20230113 19:54:04 @agent_ppo2.py:146][0m 845824 total steps have happened
[32m[20230113 19:54:04 @agent_ppo2.py:122][0m #------------------------ Iteration 413 --------------------------#
[32m[20230113 19:54:05 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:05 @agent_ppo2.py:186][0m |          -0.0003 |           4.4049 |           3.9268 |
[32m[20230113 19:54:05 @agent_ppo2.py:186][0m |          -0.0049 |           4.0980 |           3.9244 |
[32m[20230113 19:54:05 @agent_ppo2.py:186][0m |          -0.0067 |           3.9564 |           3.9224 |
[32m[20230113 19:54:05 @agent_ppo2.py:186][0m |          -0.0088 |           3.8733 |           3.9224 |
[32m[20230113 19:54:05 @agent_ppo2.py:186][0m |          -0.0098 |           3.7866 |           3.9233 |
[32m[20230113 19:54:05 @agent_ppo2.py:186][0m |          -0.0103 |           3.7433 |           3.9224 |
[32m[20230113 19:54:05 @agent_ppo2.py:186][0m |          -0.0111 |           3.6858 |           3.9194 |
[32m[20230113 19:54:05 @agent_ppo2.py:186][0m |          -0.0117 |           3.6444 |           3.9223 |
[32m[20230113 19:54:05 @agent_ppo2.py:186][0m |          -0.0120 |           3.5991 |           3.9231 |
[32m[20230113 19:54:05 @agent_ppo2.py:186][0m |          -0.0127 |           3.5550 |           3.9215 |
[32m[20230113 19:54:05 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.37
[32m[20230113 19:54:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.69
[32m[20230113 19:54:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.59
[32m[20230113 19:54:06 @agent_ppo2.py:144][0m Total time:       9.55 min
[32m[20230113 19:54:06 @agent_ppo2.py:146][0m 847872 total steps have happened
[32m[20230113 19:54:06 @agent_ppo2.py:122][0m #------------------------ Iteration 414 --------------------------#
[32m[20230113 19:54:06 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:54:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:06 @agent_ppo2.py:186][0m |          -0.0002 |           4.9479 |           3.9700 |
[32m[20230113 19:54:06 @agent_ppo2.py:186][0m |          -0.0028 |           4.7481 |           3.9694 |
[32m[20230113 19:54:06 @agent_ppo2.py:186][0m |          -0.0062 |           4.5732 |           3.9647 |
[32m[20230113 19:54:06 @agent_ppo2.py:186][0m |          -0.0063 |           4.4721 |           3.9627 |
[32m[20230113 19:54:06 @agent_ppo2.py:186][0m |          -0.0092 |           4.3681 |           3.9601 |
[32m[20230113 19:54:06 @agent_ppo2.py:186][0m |          -0.0097 |           4.2810 |           3.9591 |
[32m[20230113 19:54:06 @agent_ppo2.py:186][0m |          -0.0090 |           4.2676 |           3.9589 |
[32m[20230113 19:54:06 @agent_ppo2.py:186][0m |          -0.0103 |           4.1726 |           3.9573 |
[32m[20230113 19:54:06 @agent_ppo2.py:186][0m |          -0.0111 |           4.1286 |           3.9557 |
[32m[20230113 19:54:07 @agent_ppo2.py:186][0m |          -0.0118 |           4.0185 |           3.9575 |
[32m[20230113 19:54:07 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:54:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.61
[32m[20230113 19:54:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.33
[32m[20230113 19:54:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.05
[32m[20230113 19:54:07 @agent_ppo2.py:144][0m Total time:       9.57 min
[32m[20230113 19:54:07 @agent_ppo2.py:146][0m 849920 total steps have happened
[32m[20230113 19:54:07 @agent_ppo2.py:122][0m #------------------------ Iteration 415 --------------------------#
[32m[20230113 19:54:07 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:54:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:08 @agent_ppo2.py:186][0m |           0.0001 |           5.5289 |           4.0115 |
[32m[20230113 19:54:08 @agent_ppo2.py:186][0m |          -0.0057 |           4.9976 |           4.0086 |
[32m[20230113 19:54:08 @agent_ppo2.py:186][0m |          -0.0093 |           4.8610 |           4.0064 |
[32m[20230113 19:54:08 @agent_ppo2.py:186][0m |          -0.0091 |           4.8386 |           3.9993 |
[32m[20230113 19:54:08 @agent_ppo2.py:186][0m |          -0.0109 |           4.7266 |           3.9959 |
[32m[20230113 19:54:08 @agent_ppo2.py:186][0m |          -0.0112 |           4.6743 |           3.9944 |
[32m[20230113 19:54:08 @agent_ppo2.py:186][0m |          -0.0119 |           4.6182 |           3.9901 |
[32m[20230113 19:54:08 @agent_ppo2.py:186][0m |          -0.0129 |           4.5618 |           3.9902 |
[32m[20230113 19:54:08 @agent_ppo2.py:186][0m |          -0.0125 |           4.5678 |           3.9882 |
[32m[20230113 19:54:08 @agent_ppo2.py:186][0m |          -0.0142 |           4.4734 |           3.9846 |
[32m[20230113 19:54:08 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:54:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.56
[32m[20230113 19:54:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.01
[32m[20230113 19:54:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.74
[32m[20230113 19:54:08 @agent_ppo2.py:144][0m Total time:       9.60 min
[32m[20230113 19:54:08 @agent_ppo2.py:146][0m 851968 total steps have happened
[32m[20230113 19:54:08 @agent_ppo2.py:122][0m #------------------------ Iteration 416 --------------------------#
[32m[20230113 19:54:09 @agent_ppo2.py:128][0m Sampling time: 0.35 s by 1 slaves
[32m[20230113 19:54:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:09 @agent_ppo2.py:186][0m |          -0.0019 |           7.1225 |           3.8665 |
[32m[20230113 19:54:09 @agent_ppo2.py:186][0m |          -0.0012 |           2.7612 |           3.8580 |
[32m[20230113 19:54:09 @agent_ppo2.py:186][0m |          -0.0065 |           2.4878 |           3.8589 |
[32m[20230113 19:54:09 @agent_ppo2.py:186][0m |          -0.0016 |           2.3390 |           3.8590 |
[32m[20230113 19:54:09 @agent_ppo2.py:186][0m |          -0.0055 |           2.1716 |           3.8552 |
[32m[20230113 19:54:09 @agent_ppo2.py:186][0m |          -0.0147 |           2.0257 |           3.8552 |
[32m[20230113 19:54:09 @agent_ppo2.py:186][0m |          -0.0112 |           1.9138 |           3.8524 |
[32m[20230113 19:54:09 @agent_ppo2.py:186][0m |          -0.0099 |           1.9483 |           3.8520 |
[32m[20230113 19:54:09 @agent_ppo2.py:186][0m |          -0.0183 |           1.8353 |           3.8504 |
[32m[20230113 19:54:09 @agent_ppo2.py:186][0m |          -0.0191 |           1.7305 |           3.8497 |
[32m[20230113 19:54:09 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 19:54:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 99.80
[32m[20230113 19:54:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 216.50
[32m[20230113 19:54:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.25
[32m[20230113 19:54:10 @agent_ppo2.py:144][0m Total time:       9.62 min
[32m[20230113 19:54:10 @agent_ppo2.py:146][0m 854016 total steps have happened
[32m[20230113 19:54:10 @agent_ppo2.py:122][0m #------------------------ Iteration 417 --------------------------#
[32m[20230113 19:54:10 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:54:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:10 @agent_ppo2.py:186][0m |           0.0034 |           7.1408 |           3.8044 |
[32m[20230113 19:54:10 @agent_ppo2.py:186][0m |          -0.0028 |           6.3750 |           3.7955 |
[32m[20230113 19:54:10 @agent_ppo2.py:186][0m |          -0.0015 |           5.8340 |           3.7885 |
[32m[20230113 19:54:10 @agent_ppo2.py:186][0m |          -0.0079 |           5.4651 |           3.7874 |
[32m[20230113 19:54:10 @agent_ppo2.py:186][0m |           0.0004 |           5.8803 |           3.7850 |
[32m[20230113 19:54:10 @agent_ppo2.py:186][0m |          -0.0098 |           5.1778 |           3.7866 |
[32m[20230113 19:54:10 @agent_ppo2.py:186][0m |          -0.0098 |           5.0668 |           3.7847 |
[32m[20230113 19:54:10 @agent_ppo2.py:186][0m |          -0.0106 |           4.9951 |           3.7829 |
[32m[20230113 19:54:10 @agent_ppo2.py:186][0m |          -0.0075 |           4.9465 |           3.7825 |
[32m[20230113 19:54:10 @agent_ppo2.py:186][0m |          -0.0121 |           4.8275 |           3.7824 |
[32m[20230113 19:54:10 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:54:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.79
[32m[20230113 19:54:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.32
[32m[20230113 19:54:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.12
[32m[20230113 19:54:11 @agent_ppo2.py:144][0m Total time:       9.64 min
[32m[20230113 19:54:11 @agent_ppo2.py:146][0m 856064 total steps have happened
[32m[20230113 19:54:11 @agent_ppo2.py:122][0m #------------------------ Iteration 418 --------------------------#
[32m[20230113 19:54:11 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:54:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:12 @agent_ppo2.py:186][0m |           0.0021 |           8.2152 |           3.9306 |
[32m[20230113 19:54:12 @agent_ppo2.py:186][0m |           0.0185 |           5.6549 |           3.9275 |
[32m[20230113 19:54:12 @agent_ppo2.py:186][0m |           0.0158 |           5.1829 |           3.9263 |
[32m[20230113 19:54:12 @agent_ppo2.py:186][0m |          -0.0068 |           4.6410 |           3.9170 |
[32m[20230113 19:54:12 @agent_ppo2.py:186][0m |          -0.0071 |           4.2277 |           3.9193 |
[32m[20230113 19:54:12 @agent_ppo2.py:186][0m |          -0.0040 |           4.1216 |           3.9196 |
[32m[20230113 19:54:12 @agent_ppo2.py:186][0m |          -0.0127 |           4.2225 |           3.9178 |
[32m[20230113 19:54:12 @agent_ppo2.py:186][0m |          -0.0113 |           4.2681 |           3.9188 |
[32m[20230113 19:54:12 @agent_ppo2.py:186][0m |          -0.0044 |           3.8577 |           3.9191 |
[32m[20230113 19:54:12 @agent_ppo2.py:186][0m |           0.0070 |           3.8427 |           3.9149 |
[32m[20230113 19:54:12 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 19:54:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 117.46
[32m[20230113 19:54:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.20
[32m[20230113 19:54:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.18
[32m[20230113 19:54:12 @agent_ppo2.py:144][0m Total time:       9.66 min
[32m[20230113 19:54:12 @agent_ppo2.py:146][0m 858112 total steps have happened
[32m[20230113 19:54:12 @agent_ppo2.py:122][0m #------------------------ Iteration 419 --------------------------#
[32m[20230113 19:54:13 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:13 @agent_ppo2.py:186][0m |           0.0009 |           6.0718 |           3.9155 |
[32m[20230113 19:54:13 @agent_ppo2.py:186][0m |          -0.0015 |           5.8721 |           3.9185 |
[32m[20230113 19:54:13 @agent_ppo2.py:186][0m |          -0.0036 |           5.6645 |           3.9141 |
[32m[20230113 19:54:13 @agent_ppo2.py:186][0m |          -0.0043 |           5.5960 |           3.9143 |
[32m[20230113 19:54:13 @agent_ppo2.py:186][0m |          -0.0051 |           5.5435 |           3.9133 |
[32m[20230113 19:54:13 @agent_ppo2.py:186][0m |          -0.0056 |           5.5406 |           3.9142 |
[32m[20230113 19:54:13 @agent_ppo2.py:186][0m |          -0.0053 |           5.4846 |           3.9136 |
[32m[20230113 19:54:13 @agent_ppo2.py:186][0m |          -0.0070 |           5.4414 |           3.9138 |
[32m[20230113 19:54:13 @agent_ppo2.py:186][0m |          -0.0077 |           5.3855 |           3.9145 |
[32m[20230113 19:54:13 @agent_ppo2.py:186][0m |          -0.0073 |           5.3526 |           3.9177 |
[32m[20230113 19:54:13 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.84
[32m[20230113 19:54:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.06
[32m[20230113 19:54:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.50
[32m[20230113 19:54:14 @agent_ppo2.py:144][0m Total time:       9.69 min
[32m[20230113 19:54:14 @agent_ppo2.py:146][0m 860160 total steps have happened
[32m[20230113 19:54:14 @agent_ppo2.py:122][0m #------------------------ Iteration 420 --------------------------#
[32m[20230113 19:54:14 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:54:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:14 @agent_ppo2.py:186][0m |          -0.0008 |           3.9044 |           3.9035 |
[32m[20230113 19:54:14 @agent_ppo2.py:186][0m |          -0.0062 |           3.7545 |           3.8980 |
[32m[20230113 19:54:14 @agent_ppo2.py:186][0m |          -0.0078 |           3.6564 |           3.8980 |
[32m[20230113 19:54:14 @agent_ppo2.py:186][0m |          -0.0087 |           3.5921 |           3.8993 |
[32m[20230113 19:54:14 @agent_ppo2.py:186][0m |          -0.0097 |           3.5827 |           3.8984 |
[32m[20230113 19:54:15 @agent_ppo2.py:186][0m |          -0.0100 |           3.5315 |           3.8978 |
[32m[20230113 19:54:15 @agent_ppo2.py:186][0m |          -0.0104 |           3.4989 |           3.9010 |
[32m[20230113 19:54:15 @agent_ppo2.py:186][0m |          -0.0111 |           3.4816 |           3.8992 |
[32m[20230113 19:54:15 @agent_ppo2.py:186][0m |          -0.0117 |           3.4580 |           3.9004 |
[32m[20230113 19:54:15 @agent_ppo2.py:186][0m |          -0.0120 |           3.4150 |           3.8991 |
[32m[20230113 19:54:15 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.61
[32m[20230113 19:54:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.02
[32m[20230113 19:54:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.15
[32m[20230113 19:54:15 @agent_ppo2.py:144][0m Total time:       9.71 min
[32m[20230113 19:54:15 @agent_ppo2.py:146][0m 862208 total steps have happened
[32m[20230113 19:54:15 @agent_ppo2.py:122][0m #------------------------ Iteration 421 --------------------------#
[32m[20230113 19:54:16 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:16 @agent_ppo2.py:186][0m |          -0.0002 |           6.8407 |           3.8839 |
[32m[20230113 19:54:16 @agent_ppo2.py:186][0m |          -0.0036 |           6.1705 |           3.8758 |
[32m[20230113 19:54:16 @agent_ppo2.py:186][0m |          -0.0090 |           5.9265 |           3.8692 |
[32m[20230113 19:54:16 @agent_ppo2.py:186][0m |          -0.0089 |           5.8150 |           3.8641 |
[32m[20230113 19:54:16 @agent_ppo2.py:186][0m |          -0.0100 |           5.7530 |           3.8615 |
[32m[20230113 19:54:16 @agent_ppo2.py:186][0m |          -0.0082 |           5.7652 |           3.8564 |
[32m[20230113 19:54:16 @agent_ppo2.py:186][0m |          -0.0103 |           5.6431 |           3.8560 |
[32m[20230113 19:54:16 @agent_ppo2.py:186][0m |          -0.0098 |           5.5393 |           3.8537 |
[32m[20230113 19:54:16 @agent_ppo2.py:186][0m |          -0.0095 |           5.5165 |           3.8515 |
[32m[20230113 19:54:16 @agent_ppo2.py:186][0m |          -0.0186 |           5.4556 |           3.8505 |
[32m[20230113 19:54:16 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:54:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.73
[32m[20230113 19:54:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.48
[32m[20230113 19:54:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.84
[32m[20230113 19:54:17 @agent_ppo2.py:144][0m Total time:       9.73 min
[32m[20230113 19:54:17 @agent_ppo2.py:146][0m 864256 total steps have happened
[32m[20230113 19:54:17 @agent_ppo2.py:122][0m #------------------------ Iteration 422 --------------------------#
[32m[20230113 19:54:17 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:17 @agent_ppo2.py:186][0m |           0.0013 |           3.8694 |           3.9041 |
[32m[20230113 19:54:17 @agent_ppo2.py:186][0m |          -0.0035 |           3.6404 |           3.9012 |
[32m[20230113 19:54:17 @agent_ppo2.py:186][0m |          -0.0045 |           3.6449 |           3.8988 |
[32m[20230113 19:54:17 @agent_ppo2.py:186][0m |          -0.0070 |           3.5127 |           3.8974 |
[32m[20230113 19:54:17 @agent_ppo2.py:186][0m |          -0.0078 |           3.3872 |           3.8978 |
[32m[20230113 19:54:17 @agent_ppo2.py:186][0m |          -0.0099 |           3.3418 |           3.9005 |
[32m[20230113 19:54:17 @agent_ppo2.py:186][0m |          -0.0098 |           3.3068 |           3.9016 |
[32m[20230113 19:54:17 @agent_ppo2.py:186][0m |          -0.0103 |           3.3135 |           3.9020 |
[32m[20230113 19:54:17 @agent_ppo2.py:186][0m |          -0.0115 |           3.2561 |           3.9028 |
[32m[20230113 19:54:17 @agent_ppo2.py:186][0m |          -0.0114 |           3.2344 |           3.9006 |
[32m[20230113 19:54:17 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:54:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.14
[32m[20230113 19:54:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.29
[32m[20230113 19:54:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.48
[32m[20230113 19:54:18 @agent_ppo2.py:144][0m Total time:       9.76 min
[32m[20230113 19:54:18 @agent_ppo2.py:146][0m 866304 total steps have happened
[32m[20230113 19:54:18 @agent_ppo2.py:122][0m #------------------------ Iteration 423 --------------------------#
[32m[20230113 19:54:18 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:54:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:18 @agent_ppo2.py:186][0m |          -0.0007 |           3.8836 |           3.9449 |
[32m[20230113 19:54:18 @agent_ppo2.py:186][0m |          -0.0046 |           3.7218 |           3.9376 |
[32m[20230113 19:54:19 @agent_ppo2.py:186][0m |          -0.0058 |           3.6341 |           3.9350 |
[32m[20230113 19:54:19 @agent_ppo2.py:186][0m |          -0.0074 |           3.5685 |           3.9350 |
[32m[20230113 19:54:19 @agent_ppo2.py:186][0m |          -0.0082 |           3.5169 |           3.9318 |
[32m[20230113 19:54:19 @agent_ppo2.py:186][0m |          -0.0088 |           3.4968 |           3.9323 |
[32m[20230113 19:54:19 @agent_ppo2.py:186][0m |          -0.0098 |           3.4560 |           3.9332 |
[32m[20230113 19:54:19 @agent_ppo2.py:186][0m |          -0.0098 |           3.4285 |           3.9316 |
[32m[20230113 19:54:19 @agent_ppo2.py:186][0m |          -0.0105 |           3.4063 |           3.9326 |
[32m[20230113 19:54:19 @agent_ppo2.py:186][0m |          -0.0113 |           3.3797 |           3.9318 |
[32m[20230113 19:54:19 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:54:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.41
[32m[20230113 19:54:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.08
[32m[20230113 19:54:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.08
[32m[20230113 19:54:19 @agent_ppo2.py:144][0m Total time:       9.78 min
[32m[20230113 19:54:19 @agent_ppo2.py:146][0m 868352 total steps have happened
[32m[20230113 19:54:19 @agent_ppo2.py:122][0m #------------------------ Iteration 424 --------------------------#
[32m[20230113 19:54:20 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:54:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:20 @agent_ppo2.py:186][0m |          -0.0021 |           5.2409 |           3.9181 |
[32m[20230113 19:54:20 @agent_ppo2.py:186][0m |          -0.0080 |           4.6657 |           3.9114 |
[32m[20230113 19:54:20 @agent_ppo2.py:186][0m |          -0.0104 |           4.4606 |           3.9057 |
[32m[20230113 19:54:20 @agent_ppo2.py:186][0m |          -0.0121 |           4.3019 |           3.9032 |
[32m[20230113 19:54:20 @agent_ppo2.py:186][0m |          -0.0130 |           4.1286 |           3.9013 |
[32m[20230113 19:54:20 @agent_ppo2.py:186][0m |          -0.0131 |           4.0320 |           3.9009 |
[32m[20230113 19:54:20 @agent_ppo2.py:186][0m |          -0.0131 |           3.9566 |           3.8979 |
[32m[20230113 19:54:20 @agent_ppo2.py:186][0m |          -0.0130 |           3.8047 |           3.8997 |
[32m[20230113 19:54:20 @agent_ppo2.py:186][0m |          -0.0148 |           3.6532 |           3.8953 |
[32m[20230113 19:54:20 @agent_ppo2.py:186][0m |          -0.0130 |           3.5347 |           3.8957 |
[32m[20230113 19:54:20 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.65
[32m[20230113 19:54:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.68
[32m[20230113 19:54:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.43
[32m[20230113 19:54:21 @agent_ppo2.py:144][0m Total time:       9.80 min
[32m[20230113 19:54:21 @agent_ppo2.py:146][0m 870400 total steps have happened
[32m[20230113 19:54:21 @agent_ppo2.py:122][0m #------------------------ Iteration 425 --------------------------#
[32m[20230113 19:54:21 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:54:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:21 @agent_ppo2.py:186][0m |          -0.0014 |           5.7955 |           3.9019 |
[32m[20230113 19:54:21 @agent_ppo2.py:186][0m |          -0.0046 |           5.4481 |           3.8984 |
[32m[20230113 19:54:21 @agent_ppo2.py:186][0m |          -0.0038 |           5.3827 |           3.8910 |
[32m[20230113 19:54:21 @agent_ppo2.py:186][0m |          -0.0027 |           5.3871 |           3.8926 |
[32m[20230113 19:54:21 @agent_ppo2.py:186][0m |          -0.0082 |           5.0562 |           3.8940 |
[32m[20230113 19:54:21 @agent_ppo2.py:186][0m |          -0.0064 |           5.0805 |           3.8900 |
[32m[20230113 19:54:21 @agent_ppo2.py:186][0m |          -0.0103 |           4.9376 |           3.8881 |
[32m[20230113 19:54:21 @agent_ppo2.py:186][0m |          -0.0069 |           4.9972 |           3.8878 |
[32m[20230113 19:54:22 @agent_ppo2.py:186][0m |          -0.0097 |           4.8640 |           3.8847 |
[32m[20230113 19:54:22 @agent_ppo2.py:186][0m |          -0.0108 |           4.7907 |           3.8848 |
[32m[20230113 19:54:22 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:54:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.26
[32m[20230113 19:54:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.68
[32m[20230113 19:54:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -84.21
[32m[20230113 19:54:22 @agent_ppo2.py:144][0m Total time:       9.82 min
[32m[20230113 19:54:22 @agent_ppo2.py:146][0m 872448 total steps have happened
[32m[20230113 19:54:22 @agent_ppo2.py:122][0m #------------------------ Iteration 426 --------------------------#
[32m[20230113 19:54:22 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:23 @agent_ppo2.py:186][0m |           0.0052 |           6.1860 |           3.9628 |
[32m[20230113 19:54:23 @agent_ppo2.py:186][0m |          -0.0050 |           5.5935 |           3.9609 |
[32m[20230113 19:54:23 @agent_ppo2.py:186][0m |          -0.0061 |           5.4513 |           3.9592 |
[32m[20230113 19:54:23 @agent_ppo2.py:186][0m |          -0.0078 |           5.3311 |           3.9610 |
[32m[20230113 19:54:23 @agent_ppo2.py:186][0m |          -0.0106 |           5.2816 |           3.9589 |
[32m[20230113 19:54:23 @agent_ppo2.py:186][0m |          -0.0098 |           5.2225 |           3.9575 |
[32m[20230113 19:54:23 @agent_ppo2.py:186][0m |          -0.0127 |           5.1189 |           3.9557 |
[32m[20230113 19:54:23 @agent_ppo2.py:186][0m |          -0.0115 |           5.0786 |           3.9561 |
[32m[20230113 19:54:23 @agent_ppo2.py:186][0m |          -0.0127 |           5.0213 |           3.9558 |
[32m[20230113 19:54:23 @agent_ppo2.py:186][0m |          -0.0153 |           4.9773 |           3.9555 |
[32m[20230113 19:54:23 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:54:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.49
[32m[20230113 19:54:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.53
[32m[20230113 19:54:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.00
[32m[20230113 19:54:23 @agent_ppo2.py:144][0m Total time:       9.85 min
[32m[20230113 19:54:23 @agent_ppo2.py:146][0m 874496 total steps have happened
[32m[20230113 19:54:23 @agent_ppo2.py:122][0m #------------------------ Iteration 427 --------------------------#
[32m[20230113 19:54:24 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:54:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:24 @agent_ppo2.py:186][0m |           0.0007 |           3.2383 |           3.8720 |
[32m[20230113 19:54:24 @agent_ppo2.py:186][0m |          -0.0031 |           3.0197 |           3.8701 |
[32m[20230113 19:54:24 @agent_ppo2.py:186][0m |          -0.0051 |           2.9163 |           3.8677 |
[32m[20230113 19:54:24 @agent_ppo2.py:186][0m |          -0.0067 |           2.8557 |           3.8643 |
[32m[20230113 19:54:24 @agent_ppo2.py:186][0m |          -0.0070 |           2.8107 |           3.8664 |
[32m[20230113 19:54:24 @agent_ppo2.py:186][0m |          -0.0073 |           2.7679 |           3.8663 |
[32m[20230113 19:54:24 @agent_ppo2.py:186][0m |          -0.0084 |           2.7458 |           3.8663 |
[32m[20230113 19:54:24 @agent_ppo2.py:186][0m |          -0.0088 |           2.7163 |           3.8688 |
[32m[20230113 19:54:24 @agent_ppo2.py:186][0m |          -0.0092 |           2.6892 |           3.8686 |
[32m[20230113 19:54:24 @agent_ppo2.py:186][0m |          -0.0094 |           2.6724 |           3.8691 |
[32m[20230113 19:54:24 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:54:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 217.89
[32m[20230113 19:54:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.72
[32m[20230113 19:54:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.83
[32m[20230113 19:54:25 @agent_ppo2.py:144][0m Total time:       9.87 min
[32m[20230113 19:54:25 @agent_ppo2.py:146][0m 876544 total steps have happened
[32m[20230113 19:54:25 @agent_ppo2.py:122][0m #------------------------ Iteration 428 --------------------------#
[32m[20230113 19:54:25 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:54:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:25 @agent_ppo2.py:186][0m |          -0.0017 |          15.3825 |           3.9583 |
[32m[20230113 19:54:25 @agent_ppo2.py:186][0m |          -0.0078 |           9.1323 |           3.9521 |
[32m[20230113 19:54:25 @agent_ppo2.py:186][0m |          -0.0096 |           8.0187 |           3.9496 |
[32m[20230113 19:54:26 @agent_ppo2.py:186][0m |          -0.0093 |           7.7075 |           3.9478 |
[32m[20230113 19:54:26 @agent_ppo2.py:186][0m |          -0.0105 |           7.4954 |           3.9450 |
[32m[20230113 19:54:26 @agent_ppo2.py:186][0m |          -0.0123 |           7.2464 |           3.9435 |
[32m[20230113 19:54:26 @agent_ppo2.py:186][0m |          -0.0121 |           7.1780 |           3.9424 |
[32m[20230113 19:54:26 @agent_ppo2.py:186][0m |          -0.0128 |           7.1416 |           3.9422 |
[32m[20230113 19:54:26 @agent_ppo2.py:186][0m |          -0.0132 |           6.9753 |           3.9423 |
[32m[20230113 19:54:26 @agent_ppo2.py:186][0m |          -0.0119 |           6.9218 |           3.9391 |
[32m[20230113 19:54:26 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 19:54:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 125.93
[32m[20230113 19:54:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.69
[32m[20230113 19:54:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.03
[32m[20230113 19:54:26 @agent_ppo2.py:144][0m Total time:       9.89 min
[32m[20230113 19:54:26 @agent_ppo2.py:146][0m 878592 total steps have happened
[32m[20230113 19:54:26 @agent_ppo2.py:122][0m #------------------------ Iteration 429 --------------------------#
[32m[20230113 19:54:27 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:54:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:27 @agent_ppo2.py:186][0m |           0.0010 |           6.1502 |           3.9568 |
[32m[20230113 19:54:27 @agent_ppo2.py:186][0m |          -0.0000 |           5.8642 |           3.9544 |
[32m[20230113 19:54:27 @agent_ppo2.py:186][0m |          -0.0045 |           5.4266 |           3.9555 |
[32m[20230113 19:54:27 @agent_ppo2.py:186][0m |          -0.0076 |           5.2641 |           3.9552 |
[32m[20230113 19:54:27 @agent_ppo2.py:186][0m |          -0.0066 |           5.1776 |           3.9553 |
[32m[20230113 19:54:27 @agent_ppo2.py:186][0m |          -0.0062 |           5.1714 |           3.9530 |
[32m[20230113 19:54:27 @agent_ppo2.py:186][0m |          -0.0085 |           5.0008 |           3.9565 |
[32m[20230113 19:54:27 @agent_ppo2.py:186][0m |          -0.0098 |           4.9224 |           3.9546 |
[32m[20230113 19:54:27 @agent_ppo2.py:186][0m |          -0.0082 |           4.8774 |           3.9572 |
[32m[20230113 19:54:27 @agent_ppo2.py:186][0m |          -0.0114 |           4.7897 |           3.9564 |
[32m[20230113 19:54:27 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.13
[32m[20230113 19:54:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.20
[32m[20230113 19:54:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.49
[32m[20230113 19:54:28 @agent_ppo2.py:144][0m Total time:       9.92 min
[32m[20230113 19:54:28 @agent_ppo2.py:146][0m 880640 total steps have happened
[32m[20230113 19:54:28 @agent_ppo2.py:122][0m #------------------------ Iteration 430 --------------------------#
[32m[20230113 19:54:28 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:54:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:28 @agent_ppo2.py:186][0m |          -0.0019 |           3.1815 |           3.9497 |
[32m[20230113 19:54:28 @agent_ppo2.py:186][0m |          -0.0062 |           2.9226 |           3.9436 |
[32m[20230113 19:54:28 @agent_ppo2.py:186][0m |          -0.0075 |           2.8289 |           3.9420 |
[32m[20230113 19:54:28 @agent_ppo2.py:186][0m |          -0.0093 |           2.7719 |           3.9397 |
[32m[20230113 19:54:28 @agent_ppo2.py:186][0m |          -0.0099 |           2.7212 |           3.9401 |
[32m[20230113 19:54:28 @agent_ppo2.py:186][0m |          -0.0104 |           2.6878 |           3.9354 |
[32m[20230113 19:54:28 @agent_ppo2.py:186][0m |          -0.0111 |           2.6537 |           3.9341 |
[32m[20230113 19:54:28 @agent_ppo2.py:186][0m |          -0.0116 |           2.6260 |           3.9333 |
[32m[20230113 19:54:29 @agent_ppo2.py:186][0m |          -0.0120 |           2.6025 |           3.9321 |
[32m[20230113 19:54:29 @agent_ppo2.py:186][0m |          -0.0124 |           2.5529 |           3.9304 |
[32m[20230113 19:54:29 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:54:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 214.68
[32m[20230113 19:54:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 218.83
[32m[20230113 19:54:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.05
[32m[20230113 19:54:29 @agent_ppo2.py:144][0m Total time:       9.94 min
[32m[20230113 19:54:29 @agent_ppo2.py:146][0m 882688 total steps have happened
[32m[20230113 19:54:29 @agent_ppo2.py:122][0m #------------------------ Iteration 431 --------------------------#
[32m[20230113 19:54:29 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:54:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:30 @agent_ppo2.py:186][0m |          -0.0022 |          17.0232 |           3.9334 |
[32m[20230113 19:54:30 @agent_ppo2.py:186][0m |          -0.0089 |           6.9933 |           3.9297 |
[32m[20230113 19:54:30 @agent_ppo2.py:186][0m |          -0.0108 |           5.0053 |           3.9293 |
[32m[20230113 19:54:30 @agent_ppo2.py:186][0m |          -0.0120 |           4.1134 |           3.9292 |
[32m[20230113 19:54:30 @agent_ppo2.py:186][0m |          -0.0131 |           3.6102 |           3.9299 |
[32m[20230113 19:54:30 @agent_ppo2.py:186][0m |          -0.0133 |           3.2294 |           3.9270 |
[32m[20230113 19:54:30 @agent_ppo2.py:186][0m |          -0.0143 |           3.0684 |           3.9287 |
[32m[20230113 19:54:30 @agent_ppo2.py:186][0m |          -0.0146 |           2.7745 |           3.9287 |
[32m[20230113 19:54:30 @agent_ppo2.py:186][0m |          -0.0147 |           2.6099 |           3.9260 |
[32m[20230113 19:54:30 @agent_ppo2.py:186][0m |          -0.0152 |           2.4517 |           3.9276 |
[32m[20230113 19:54:30 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:54:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 166.03
[32m[20230113 19:54:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 213.82
[32m[20230113 19:54:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 254.09
[32m[20230113 19:54:30 @agent_ppo2.py:144][0m Total time:       9.96 min
[32m[20230113 19:54:30 @agent_ppo2.py:146][0m 884736 total steps have happened
[32m[20230113 19:54:30 @agent_ppo2.py:122][0m #------------------------ Iteration 432 --------------------------#
[32m[20230113 19:54:31 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:31 @agent_ppo2.py:186][0m |           0.0010 |           7.8918 |           3.8630 |
[32m[20230113 19:54:31 @agent_ppo2.py:186][0m |          -0.0067 |           6.6014 |           3.8612 |
[32m[20230113 19:54:31 @agent_ppo2.py:186][0m |          -0.0068 |           6.3980 |           3.8594 |
[32m[20230113 19:54:31 @agent_ppo2.py:186][0m |          -0.0065 |           6.3901 |           3.8564 |
[32m[20230113 19:54:31 @agent_ppo2.py:186][0m |          -0.0113 |           6.1052 |           3.8596 |
[32m[20230113 19:54:31 @agent_ppo2.py:186][0m |          -0.0099 |           6.0062 |           3.8586 |
[32m[20230113 19:54:31 @agent_ppo2.py:186][0m |          -0.0113 |           5.9357 |           3.8564 |
[32m[20230113 19:54:31 @agent_ppo2.py:186][0m |          -0.0110 |           5.8418 |           3.8556 |
[32m[20230113 19:54:31 @agent_ppo2.py:186][0m |          -0.0094 |           5.7984 |           3.8540 |
[32m[20230113 19:54:31 @agent_ppo2.py:186][0m |          -0.0087 |           5.8628 |           3.8545 |
[32m[20230113 19:54:31 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.45
[32m[20230113 19:54:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.85
[32m[20230113 19:54:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.46
[32m[20230113 19:54:32 @agent_ppo2.py:144][0m Total time:       9.99 min
[32m[20230113 19:54:32 @agent_ppo2.py:146][0m 886784 total steps have happened
[32m[20230113 19:54:32 @agent_ppo2.py:122][0m #------------------------ Iteration 433 --------------------------#
[32m[20230113 19:54:32 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:54:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:32 @agent_ppo2.py:186][0m |          -0.0022 |           6.2455 |           3.9609 |
[32m[20230113 19:54:32 @agent_ppo2.py:186][0m |          -0.0068 |           5.9469 |           3.9592 |
[32m[20230113 19:54:32 @agent_ppo2.py:186][0m |          -0.0031 |           5.7757 |           3.9576 |
[32m[20230113 19:54:32 @agent_ppo2.py:186][0m |          -0.0058 |           5.6817 |           3.9572 |
[32m[20230113 19:54:32 @agent_ppo2.py:186][0m |          -0.0052 |           5.7279 |           3.9542 |
[32m[20230113 19:54:32 @agent_ppo2.py:186][0m |          -0.0106 |           5.5533 |           3.9537 |
[32m[20230113 19:54:32 @agent_ppo2.py:186][0m |           0.0123 |           6.5004 |           3.9546 |
[32m[20230113 19:54:33 @agent_ppo2.py:186][0m |          -0.0062 |           5.6806 |           3.9547 |
[32m[20230113 19:54:33 @agent_ppo2.py:186][0m |          -0.0048 |           5.3933 |           3.9562 |
[32m[20230113 19:54:33 @agent_ppo2.py:186][0m |          -0.0109 |           5.3475 |           3.9548 |
[32m[20230113 19:54:33 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.82
[32m[20230113 19:54:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.44
[32m[20230113 19:54:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 156.12
[32m[20230113 19:54:33 @agent_ppo2.py:144][0m Total time:      10.01 min
[32m[20230113 19:54:33 @agent_ppo2.py:146][0m 888832 total steps have happened
[32m[20230113 19:54:33 @agent_ppo2.py:122][0m #------------------------ Iteration 434 --------------------------#
[32m[20230113 19:54:33 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:54:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:34 @agent_ppo2.py:186][0m |          -0.0014 |           5.0546 |           3.8864 |
[32m[20230113 19:54:34 @agent_ppo2.py:186][0m |          -0.0082 |           4.5719 |           3.8793 |
[32m[20230113 19:54:34 @agent_ppo2.py:186][0m |          -0.0102 |           4.3678 |           3.8710 |
[32m[20230113 19:54:34 @agent_ppo2.py:186][0m |          -0.0091 |           4.2760 |           3.8728 |
[32m[20230113 19:54:34 @agent_ppo2.py:186][0m |          -0.0149 |           4.1399 |           3.8704 |
[32m[20230113 19:54:34 @agent_ppo2.py:186][0m |          -0.0136 |           4.0501 |           3.8684 |
[32m[20230113 19:54:34 @agent_ppo2.py:186][0m |          -0.0111 |           3.9802 |           3.8685 |
[32m[20230113 19:54:34 @agent_ppo2.py:186][0m |          -0.0092 |           4.0788 |           3.8680 |
[32m[20230113 19:54:34 @agent_ppo2.py:186][0m |          -0.0140 |           3.8029 |           3.8682 |
[32m[20230113 19:54:34 @agent_ppo2.py:186][0m |          -0.0139 |           3.7190 |           3.8680 |
[32m[20230113 19:54:34 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.66
[32m[20230113 19:54:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.51
[32m[20230113 19:54:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.03
[32m[20230113 19:54:34 @agent_ppo2.py:144][0m Total time:      10.03 min
[32m[20230113 19:54:34 @agent_ppo2.py:146][0m 890880 total steps have happened
[32m[20230113 19:54:34 @agent_ppo2.py:122][0m #------------------------ Iteration 435 --------------------------#
[32m[20230113 19:54:35 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:35 @agent_ppo2.py:186][0m |           0.0002 |           4.8914 |           3.8539 |
[32m[20230113 19:54:35 @agent_ppo2.py:186][0m |          -0.0039 |           4.6182 |           3.8517 |
[32m[20230113 19:54:35 @agent_ppo2.py:186][0m |          -0.0067 |           4.4884 |           3.8557 |
[32m[20230113 19:54:35 @agent_ppo2.py:186][0m |          -0.0085 |           4.3899 |           3.8547 |
[32m[20230113 19:54:35 @agent_ppo2.py:186][0m |          -0.0080 |           4.3176 |           3.8515 |
[32m[20230113 19:54:35 @agent_ppo2.py:186][0m |          -0.0106 |           4.2305 |           3.8529 |
[32m[20230113 19:54:35 @agent_ppo2.py:186][0m |          -0.0111 |           4.1837 |           3.8512 |
[32m[20230113 19:54:35 @agent_ppo2.py:186][0m |          -0.0109 |           4.1383 |           3.8517 |
[32m[20230113 19:54:35 @agent_ppo2.py:186][0m |          -0.0128 |           4.0849 |           3.8515 |
[32m[20230113 19:54:35 @agent_ppo2.py:186][0m |          -0.0107 |           4.0510 |           3.8502 |
[32m[20230113 19:54:35 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:54:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.26
[32m[20230113 19:54:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.39
[32m[20230113 19:54:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.88
[32m[20230113 19:54:36 @agent_ppo2.py:144][0m Total time:      10.05 min
[32m[20230113 19:54:36 @agent_ppo2.py:146][0m 892928 total steps have happened
[32m[20230113 19:54:36 @agent_ppo2.py:122][0m #------------------------ Iteration 436 --------------------------#
[32m[20230113 19:54:36 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:36 @agent_ppo2.py:186][0m |          -0.0019 |           5.5589 |           3.8464 |
[32m[20230113 19:54:36 @agent_ppo2.py:186][0m |          -0.0084 |           5.1821 |           3.8482 |
[32m[20230113 19:54:36 @agent_ppo2.py:186][0m |           0.0014 |           5.0978 |           3.8467 |
[32m[20230113 19:54:36 @agent_ppo2.py:186][0m |          -0.0132 |           4.9780 |           3.8447 |
[32m[20230113 19:54:36 @agent_ppo2.py:186][0m |          -0.0138 |           4.9138 |           3.8456 |
[32m[20230113 19:54:36 @agent_ppo2.py:186][0m |           0.0075 |           5.4722 |           3.8445 |
[32m[20230113 19:54:37 @agent_ppo2.py:186][0m |          -0.0154 |           4.8932 |           3.8379 |
[32m[20230113 19:54:37 @agent_ppo2.py:186][0m |          -0.0023 |           4.7614 |           3.8446 |
[32m[20230113 19:54:37 @agent_ppo2.py:186][0m |          -0.0186 |           4.7150 |           3.8469 |
[32m[20230113 19:54:37 @agent_ppo2.py:186][0m |          -0.0149 |           4.6855 |           3.8459 |
[32m[20230113 19:54:37 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:54:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.73
[32m[20230113 19:54:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.24
[32m[20230113 19:54:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.42
[32m[20230113 19:54:37 @agent_ppo2.py:144][0m Total time:      10.08 min
[32m[20230113 19:54:37 @agent_ppo2.py:146][0m 894976 total steps have happened
[32m[20230113 19:54:37 @agent_ppo2.py:122][0m #------------------------ Iteration 437 --------------------------#
[32m[20230113 19:54:38 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:54:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:38 @agent_ppo2.py:186][0m |          -0.0003 |           5.4317 |           3.9349 |
[32m[20230113 19:54:38 @agent_ppo2.py:186][0m |          -0.0050 |           5.1067 |           3.9371 |
[32m[20230113 19:54:38 @agent_ppo2.py:186][0m |          -0.0075 |           4.9556 |           3.9300 |
[32m[20230113 19:54:38 @agent_ppo2.py:186][0m |          -0.0094 |           4.8701 |           3.9345 |
[32m[20230113 19:54:38 @agent_ppo2.py:186][0m |          -0.0095 |           4.7674 |           3.9353 |
[32m[20230113 19:54:38 @agent_ppo2.py:186][0m |          -0.0113 |           4.6985 |           3.9337 |
[32m[20230113 19:54:38 @agent_ppo2.py:186][0m |          -0.0117 |           4.6682 |           3.9358 |
[32m[20230113 19:54:38 @agent_ppo2.py:186][0m |          -0.0104 |           4.6111 |           3.9346 |
[32m[20230113 19:54:38 @agent_ppo2.py:186][0m |          -0.0128 |           4.5565 |           3.9342 |
[32m[20230113 19:54:38 @agent_ppo2.py:186][0m |          -0.0126 |           4.5400 |           3.9363 |
[32m[20230113 19:54:38 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.90
[32m[20230113 19:54:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.50
[32m[20230113 19:54:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.38
[32m[20230113 19:54:38 @agent_ppo2.py:144][0m Total time:      10.10 min
[32m[20230113 19:54:38 @agent_ppo2.py:146][0m 897024 total steps have happened
[32m[20230113 19:54:38 @agent_ppo2.py:122][0m #------------------------ Iteration 438 --------------------------#
[32m[20230113 19:54:39 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:54:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:39 @agent_ppo2.py:186][0m |           0.0003 |           5.4376 |           3.9686 |
[32m[20230113 19:54:39 @agent_ppo2.py:186][0m |          -0.0030 |           5.2290 |           3.9670 |
[32m[20230113 19:54:39 @agent_ppo2.py:186][0m |          -0.0048 |           5.1274 |           3.9632 |
[32m[20230113 19:54:39 @agent_ppo2.py:186][0m |          -0.0059 |           5.0354 |           3.9632 |
[32m[20230113 19:54:39 @agent_ppo2.py:186][0m |          -0.0061 |           4.9717 |           3.9599 |
[32m[20230113 19:54:39 @agent_ppo2.py:186][0m |          -0.0075 |           4.9065 |           3.9602 |
[32m[20230113 19:54:39 @agent_ppo2.py:186][0m |          -0.0082 |           4.8541 |           3.9630 |
[32m[20230113 19:54:39 @agent_ppo2.py:186][0m |          -0.0090 |           4.8101 |           3.9611 |
[32m[20230113 19:54:39 @agent_ppo2.py:186][0m |          -0.0091 |           4.7772 |           3.9602 |
[32m[20230113 19:54:39 @agent_ppo2.py:186][0m |          -0.0098 |           4.7342 |           3.9584 |
[32m[20230113 19:54:39 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.18
[32m[20230113 19:54:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.30
[32m[20230113 19:54:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.79
[32m[20230113 19:54:40 @agent_ppo2.py:144][0m Total time:      10.12 min
[32m[20230113 19:54:40 @agent_ppo2.py:146][0m 899072 total steps have happened
[32m[20230113 19:54:40 @agent_ppo2.py:122][0m #------------------------ Iteration 439 --------------------------#
[32m[20230113 19:54:40 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:40 @agent_ppo2.py:186][0m |          -0.0003 |           3.2518 |           4.0546 |
[32m[20230113 19:54:40 @agent_ppo2.py:186][0m |          -0.0036 |           3.1038 |           4.0445 |
[32m[20230113 19:54:40 @agent_ppo2.py:186][0m |          -0.0055 |           3.0333 |           4.0437 |
[32m[20230113 19:54:40 @agent_ppo2.py:186][0m |          -0.0066 |           2.9631 |           4.0455 |
[32m[20230113 19:54:41 @agent_ppo2.py:186][0m |          -0.0069 |           2.9119 |           4.0419 |
[32m[20230113 19:54:41 @agent_ppo2.py:186][0m |          -0.0082 |           2.8715 |           4.0432 |
[32m[20230113 19:54:41 @agent_ppo2.py:186][0m |          -0.0084 |           2.8424 |           4.0397 |
[32m[20230113 19:54:41 @agent_ppo2.py:186][0m |          -0.0092 |           2.8115 |           4.0376 |
[32m[20230113 19:54:41 @agent_ppo2.py:186][0m |          -0.0098 |           2.7890 |           4.0406 |
[32m[20230113 19:54:41 @agent_ppo2.py:186][0m |          -0.0103 |           2.7724 |           4.0385 |
[32m[20230113 19:54:41 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.85
[32m[20230113 19:54:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.23
[32m[20230113 19:54:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.78
[32m[20230113 19:54:41 @agent_ppo2.py:144][0m Total time:      10.14 min
[32m[20230113 19:54:41 @agent_ppo2.py:146][0m 901120 total steps have happened
[32m[20230113 19:54:41 @agent_ppo2.py:122][0m #------------------------ Iteration 440 --------------------------#
[32m[20230113 19:54:42 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:54:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:42 @agent_ppo2.py:186][0m |           0.0007 |           4.8250 |           3.9993 |
[32m[20230113 19:54:42 @agent_ppo2.py:186][0m |          -0.0020 |           4.5076 |           3.9981 |
[32m[20230113 19:54:42 @agent_ppo2.py:186][0m |          -0.0034 |           4.3497 |           4.0007 |
[32m[20230113 19:54:42 @agent_ppo2.py:186][0m |          -0.0042 |           4.2428 |           4.0014 |
[32m[20230113 19:54:42 @agent_ppo2.py:186][0m |          -0.0048 |           4.1386 |           4.0005 |
[32m[20230113 19:54:42 @agent_ppo2.py:186][0m |          -0.0067 |           4.0365 |           3.9995 |
[32m[20230113 19:54:42 @agent_ppo2.py:186][0m |          -0.0075 |           3.9616 |           3.9996 |
[32m[20230113 19:54:42 @agent_ppo2.py:186][0m |          -0.0067 |           3.9216 |           3.9980 |
[32m[20230113 19:54:42 @agent_ppo2.py:186][0m |          -0.0074 |           3.7952 |           3.9980 |
[32m[20230113 19:54:42 @agent_ppo2.py:186][0m |          -0.0077 |           3.6765 |           3.9987 |
[32m[20230113 19:54:42 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:54:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.05
[32m[20230113 19:54:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.10
[32m[20230113 19:54:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.67
[32m[20230113 19:54:43 @agent_ppo2.py:144][0m Total time:      10.17 min
[32m[20230113 19:54:43 @agent_ppo2.py:146][0m 903168 total steps have happened
[32m[20230113 19:54:43 @agent_ppo2.py:122][0m #------------------------ Iteration 441 --------------------------#
[32m[20230113 19:54:43 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:43 @agent_ppo2.py:186][0m |          -0.0006 |           5.7009 |           4.0005 |
[32m[20230113 19:54:43 @agent_ppo2.py:186][0m |          -0.0081 |           5.3464 |           3.9932 |
[32m[20230113 19:54:43 @agent_ppo2.py:186][0m |          -0.0069 |           5.1291 |           3.9913 |
[32m[20230113 19:54:43 @agent_ppo2.py:186][0m |          -0.0075 |           5.0261 |           3.9936 |
[32m[20230113 19:54:43 @agent_ppo2.py:186][0m |          -0.0082 |           4.9481 |           3.9906 |
[32m[20230113 19:54:43 @agent_ppo2.py:186][0m |          -0.0104 |           4.8213 |           3.9938 |
[32m[20230113 19:54:43 @agent_ppo2.py:186][0m |          -0.0066 |           4.9017 |           3.9908 |
[32m[20230113 19:54:43 @agent_ppo2.py:186][0m |          -0.0096 |           4.7910 |           3.9930 |
[32m[20230113 19:54:43 @agent_ppo2.py:186][0m |          -0.0106 |           4.6829 |           3.9900 |
[32m[20230113 19:54:43 @agent_ppo2.py:186][0m |          -0.0116 |           4.6826 |           3.9899 |
[32m[20230113 19:54:43 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:54:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.19
[32m[20230113 19:54:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.23
[32m[20230113 19:54:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.34
[32m[20230113 19:54:44 @agent_ppo2.py:144][0m Total time:      10.19 min
[32m[20230113 19:54:44 @agent_ppo2.py:146][0m 905216 total steps have happened
[32m[20230113 19:54:44 @agent_ppo2.py:122][0m #------------------------ Iteration 442 --------------------------#
[32m[20230113 19:54:44 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:54:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:44 @agent_ppo2.py:186][0m |          -0.0030 |          13.1319 |           3.9388 |
[32m[20230113 19:54:45 @agent_ppo2.py:186][0m |          -0.0069 |           8.1314 |           3.9354 |
[32m[20230113 19:54:45 @agent_ppo2.py:186][0m |          -0.0078 |           7.1704 |           3.9266 |
[32m[20230113 19:54:45 @agent_ppo2.py:186][0m |          -0.0097 |           6.5249 |           3.9203 |
[32m[20230113 19:54:45 @agent_ppo2.py:186][0m |          -0.0123 |           6.1492 |           3.9188 |
[32m[20230113 19:54:45 @agent_ppo2.py:186][0m |          -0.0120 |           5.7886 |           3.9158 |
[32m[20230113 19:54:45 @agent_ppo2.py:186][0m |          -0.0123 |           5.4887 |           3.9117 |
[32m[20230113 19:54:45 @agent_ppo2.py:186][0m |          -0.0134 |           5.2539 |           3.9123 |
[32m[20230113 19:54:45 @agent_ppo2.py:186][0m |          -0.0150 |           5.2156 |           3.9087 |
[32m[20230113 19:54:45 @agent_ppo2.py:186][0m |          -0.0104 |           4.9375 |           3.9068 |
[32m[20230113 19:54:45 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 19:54:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 123.77
[32m[20230113 19:54:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.58
[32m[20230113 19:54:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 149.69
[32m[20230113 19:54:45 @agent_ppo2.py:144][0m Total time:      10.21 min
[32m[20230113 19:54:45 @agent_ppo2.py:146][0m 907264 total steps have happened
[32m[20230113 19:54:45 @agent_ppo2.py:122][0m #------------------------ Iteration 443 --------------------------#
[32m[20230113 19:54:46 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:46 @agent_ppo2.py:186][0m |          -0.0007 |           4.2505 |           3.8885 |
[32m[20230113 19:54:46 @agent_ppo2.py:186][0m |          -0.0041 |           3.7686 |           3.8832 |
[32m[20230113 19:54:46 @agent_ppo2.py:186][0m |          -0.0052 |           3.6187 |           3.8839 |
[32m[20230113 19:54:46 @agent_ppo2.py:186][0m |          -0.0048 |           3.4770 |           3.8850 |
[32m[20230113 19:54:46 @agent_ppo2.py:186][0m |          -0.0069 |           3.4533 |           3.8847 |
[32m[20230113 19:54:46 @agent_ppo2.py:186][0m |          -0.0104 |           3.3785 |           3.8852 |
[32m[20230113 19:54:46 @agent_ppo2.py:186][0m |          -0.0069 |           3.4393 |           3.8874 |
[32m[20230113 19:54:46 @agent_ppo2.py:186][0m |          -0.0098 |           3.2938 |           3.8867 |
[32m[20230113 19:54:46 @agent_ppo2.py:186][0m |          -0.0087 |           3.2248 |           3.8867 |
[32m[20230113 19:54:46 @agent_ppo2.py:186][0m |          -0.0108 |           3.1746 |           3.8869 |
[32m[20230113 19:54:46 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:54:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.44
[32m[20230113 19:54:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.80
[32m[20230113 19:54:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.12
[32m[20230113 19:54:47 @agent_ppo2.py:144][0m Total time:      10.24 min
[32m[20230113 19:54:47 @agent_ppo2.py:146][0m 909312 total steps have happened
[32m[20230113 19:54:47 @agent_ppo2.py:122][0m #------------------------ Iteration 444 --------------------------#
[32m[20230113 19:54:47 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:47 @agent_ppo2.py:186][0m |           0.0000 |           3.1860 |           3.9096 |
[32m[20230113 19:54:47 @agent_ppo2.py:186][0m |          -0.0030 |           3.0317 |           3.9088 |
[32m[20230113 19:54:47 @agent_ppo2.py:186][0m |          -0.0062 |           2.9814 |           3.9056 |
[32m[20230113 19:54:47 @agent_ppo2.py:186][0m |          -0.0080 |           2.8662 |           3.9047 |
[32m[20230113 19:54:47 @agent_ppo2.py:186][0m |          -0.0060 |           2.8213 |           3.9048 |
[32m[20230113 19:54:48 @agent_ppo2.py:186][0m |          -0.0101 |           2.7450 |           3.9021 |
[32m[20230113 19:54:48 @agent_ppo2.py:186][0m |          -0.0106 |           2.7143 |           3.9095 |
[32m[20230113 19:54:48 @agent_ppo2.py:186][0m |          -0.0108 |           2.7033 |           3.9070 |
[32m[20230113 19:54:48 @agent_ppo2.py:186][0m |          -0.0085 |           2.7025 |           3.9072 |
[32m[20230113 19:54:48 @agent_ppo2.py:186][0m |          -0.0122 |           2.6355 |           3.9095 |
[32m[20230113 19:54:48 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:54:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.61
[32m[20230113 19:54:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.65
[32m[20230113 19:54:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 152.53
[32m[20230113 19:54:48 @agent_ppo2.py:144][0m Total time:      10.26 min
[32m[20230113 19:54:48 @agent_ppo2.py:146][0m 911360 total steps have happened
[32m[20230113 19:54:48 @agent_ppo2.py:122][0m #------------------------ Iteration 445 --------------------------#
[32m[20230113 19:54:48 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:54:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:49 @agent_ppo2.py:186][0m |           0.0017 |           5.8089 |           3.9324 |
[32m[20230113 19:54:49 @agent_ppo2.py:186][0m |          -0.0054 |           5.2694 |           3.9244 |
[32m[20230113 19:54:49 @agent_ppo2.py:186][0m |          -0.0064 |           5.1393 |           3.9239 |
[32m[20230113 19:54:49 @agent_ppo2.py:186][0m |          -0.0060 |           5.0779 |           3.9214 |
[32m[20230113 19:54:49 @agent_ppo2.py:186][0m |          -0.0106 |           5.0375 |           3.9234 |
[32m[20230113 19:54:49 @agent_ppo2.py:186][0m |          -0.0085 |           4.9577 |           3.9189 |
[32m[20230113 19:54:49 @agent_ppo2.py:186][0m |          -0.0090 |           4.9327 |           3.9177 |
[32m[20230113 19:54:49 @agent_ppo2.py:186][0m |          -0.0079 |           4.9084 |           3.9165 |
[32m[20230113 19:54:49 @agent_ppo2.py:186][0m |          -0.0075 |           4.9966 |           3.9146 |
[32m[20230113 19:54:49 @agent_ppo2.py:186][0m |          -0.0103 |           4.8266 |           3.9145 |
[32m[20230113 19:54:49 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.35
[32m[20230113 19:54:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.06
[32m[20230113 19:54:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.56
[32m[20230113 19:54:49 @agent_ppo2.py:144][0m Total time:      10.28 min
[32m[20230113 19:54:49 @agent_ppo2.py:146][0m 913408 total steps have happened
[32m[20230113 19:54:49 @agent_ppo2.py:122][0m #------------------------ Iteration 446 --------------------------#
[32m[20230113 19:54:50 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:54:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:50 @agent_ppo2.py:186][0m |          -0.0008 |           5.0760 |           3.9029 |
[32m[20230113 19:54:50 @agent_ppo2.py:186][0m |          -0.0029 |           4.8405 |           3.8985 |
[32m[20230113 19:54:50 @agent_ppo2.py:186][0m |          -0.0039 |           4.7247 |           3.8989 |
[32m[20230113 19:54:50 @agent_ppo2.py:186][0m |          -0.0051 |           4.6066 |           3.9045 |
[32m[20230113 19:54:50 @agent_ppo2.py:186][0m |          -0.0057 |           4.5413 |           3.9020 |
[32m[20230113 19:54:50 @agent_ppo2.py:186][0m |          -0.0069 |           4.5270 |           3.9064 |
[32m[20230113 19:54:50 @agent_ppo2.py:186][0m |          -0.0080 |           4.4078 |           3.9010 |
[32m[20230113 19:54:50 @agent_ppo2.py:186][0m |          -0.0072 |           4.3848 |           3.9057 |
[32m[20230113 19:54:50 @agent_ppo2.py:186][0m |          -0.0074 |           4.3688 |           3.9057 |
[32m[20230113 19:54:50 @agent_ppo2.py:186][0m |          -0.0098 |           4.2869 |           3.9052 |
[32m[20230113 19:54:50 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.87
[32m[20230113 19:54:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.55
[32m[20230113 19:54:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 121.30
[32m[20230113 19:54:51 @agent_ppo2.py:144][0m Total time:      10.30 min
[32m[20230113 19:54:51 @agent_ppo2.py:146][0m 915456 total steps have happened
[32m[20230113 19:54:51 @agent_ppo2.py:122][0m #------------------------ Iteration 447 --------------------------#
[32m[20230113 19:54:51 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:54:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:51 @agent_ppo2.py:186][0m |          -0.0008 |           5.4241 |           3.9272 |
[32m[20230113 19:54:51 @agent_ppo2.py:186][0m |          -0.0045 |           4.9853 |           3.9189 |
[32m[20230113 19:54:51 @agent_ppo2.py:186][0m |          -0.0048 |           4.8387 |           3.9259 |
[32m[20230113 19:54:51 @agent_ppo2.py:186][0m |          -0.0074 |           4.7209 |           3.9271 |
[32m[20230113 19:54:51 @agent_ppo2.py:186][0m |          -0.0085 |           4.6466 |           3.9290 |
[32m[20230113 19:54:51 @agent_ppo2.py:186][0m |          -0.0088 |           4.5227 |           3.9303 |
[32m[20230113 19:54:51 @agent_ppo2.py:186][0m |          -0.0094 |           4.4702 |           3.9303 |
[32m[20230113 19:54:51 @agent_ppo2.py:186][0m |          -0.0094 |           4.3969 |           3.9312 |
[32m[20230113 19:54:51 @agent_ppo2.py:186][0m |          -0.0106 |           4.3103 |           3.9303 |
[32m[20230113 19:54:52 @agent_ppo2.py:186][0m |          -0.0112 |           4.2338 |           3.9327 |
[32m[20230113 19:54:52 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:54:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.35
[32m[20230113 19:54:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.71
[32m[20230113 19:54:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.32
[32m[20230113 19:54:52 @agent_ppo2.py:144][0m Total time:      10.32 min
[32m[20230113 19:54:52 @agent_ppo2.py:146][0m 917504 total steps have happened
[32m[20230113 19:54:52 @agent_ppo2.py:122][0m #------------------------ Iteration 448 --------------------------#
[32m[20230113 19:54:52 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 19:54:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:52 @agent_ppo2.py:186][0m |          -0.0002 |          17.9602 |           3.8883 |
[32m[20230113 19:54:52 @agent_ppo2.py:186][0m |          -0.0061 |           7.5209 |           3.8880 |
[32m[20230113 19:54:52 @agent_ppo2.py:186][0m |          -0.0064 |           6.0045 |           3.8854 |
[32m[20230113 19:54:52 @agent_ppo2.py:186][0m |          -0.0094 |           5.1852 |           3.8813 |
[32m[20230113 19:54:53 @agent_ppo2.py:186][0m |          -0.0099 |           4.5196 |           3.8806 |
[32m[20230113 19:54:53 @agent_ppo2.py:186][0m |          -0.0094 |           4.2650 |           3.8798 |
[32m[20230113 19:54:53 @agent_ppo2.py:186][0m |          -0.0104 |           4.0447 |           3.8786 |
[32m[20230113 19:54:53 @agent_ppo2.py:186][0m |          -0.0122 |           3.6482 |           3.8749 |
[32m[20230113 19:54:53 @agent_ppo2.py:186][0m |          -0.0140 |           3.4084 |           3.8743 |
[32m[20230113 19:54:53 @agent_ppo2.py:186][0m |          -0.0133 |           3.4870 |           3.8736 |
[32m[20230113 19:54:53 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 19:54:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 65.81
[32m[20230113 19:54:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 112.36
[32m[20230113 19:54:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.98
[32m[20230113 19:54:53 @agent_ppo2.py:144][0m Total time:      10.34 min
[32m[20230113 19:54:53 @agent_ppo2.py:146][0m 919552 total steps have happened
[32m[20230113 19:54:53 @agent_ppo2.py:122][0m #------------------------ Iteration 449 --------------------------#
[32m[20230113 19:54:54 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:54:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:54 @agent_ppo2.py:186][0m |           0.0006 |           6.7553 |           3.7945 |
[32m[20230113 19:54:54 @agent_ppo2.py:186][0m |          -0.0029 |           6.0502 |           3.7854 |
[32m[20230113 19:54:54 @agent_ppo2.py:186][0m |          -0.0057 |           5.7314 |           3.7787 |
[32m[20230113 19:54:54 @agent_ppo2.py:186][0m |          -0.0064 |           5.5965 |           3.7802 |
[32m[20230113 19:54:54 @agent_ppo2.py:186][0m |          -0.0090 |           5.4819 |           3.7736 |
[32m[20230113 19:54:54 @agent_ppo2.py:186][0m |          -0.0090 |           5.3677 |           3.7755 |
[32m[20230113 19:54:54 @agent_ppo2.py:186][0m |          -0.0096 |           5.2589 |           3.7703 |
[32m[20230113 19:54:54 @agent_ppo2.py:186][0m |          -0.0101 |           5.2279 |           3.7678 |
[32m[20230113 19:54:54 @agent_ppo2.py:186][0m |          -0.0104 |           5.1310 |           3.7658 |
[32m[20230113 19:54:54 @agent_ppo2.py:186][0m |          -0.0114 |           5.0694 |           3.7621 |
[32m[20230113 19:54:54 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.32
[32m[20230113 19:54:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.72
[32m[20230113 19:54:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.53
[32m[20230113 19:54:54 @agent_ppo2.py:144][0m Total time:      10.36 min
[32m[20230113 19:54:54 @agent_ppo2.py:146][0m 921600 total steps have happened
[32m[20230113 19:54:54 @agent_ppo2.py:122][0m #------------------------ Iteration 450 --------------------------#
[32m[20230113 19:54:55 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:54:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:55 @agent_ppo2.py:186][0m |           0.0004 |           5.3657 |           3.8795 |
[32m[20230113 19:54:55 @agent_ppo2.py:186][0m |          -0.0032 |           4.9947 |           3.8807 |
[32m[20230113 19:54:55 @agent_ppo2.py:186][0m |          -0.0065 |           4.8502 |           3.8784 |
[32m[20230113 19:54:55 @agent_ppo2.py:186][0m |          -0.0062 |           4.7889 |           3.8765 |
[32m[20230113 19:54:55 @agent_ppo2.py:186][0m |          -0.0077 |           4.6682 |           3.8759 |
[32m[20230113 19:54:55 @agent_ppo2.py:186][0m |          -0.0078 |           4.6392 |           3.8771 |
[32m[20230113 19:54:55 @agent_ppo2.py:186][0m |          -0.0086 |           4.5928 |           3.8767 |
[32m[20230113 19:54:55 @agent_ppo2.py:186][0m |          -0.0092 |           4.5602 |           3.8755 |
[32m[20230113 19:54:55 @agent_ppo2.py:186][0m |          -0.0096 |           4.5278 |           3.8777 |
[32m[20230113 19:54:55 @agent_ppo2.py:186][0m |          -0.0093 |           4.4694 |           3.8775 |
[32m[20230113 19:54:55 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:54:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.37
[32m[20230113 19:54:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.50
[32m[20230113 19:54:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.27
[32m[20230113 19:54:56 @agent_ppo2.py:144][0m Total time:      10.39 min
[32m[20230113 19:54:56 @agent_ppo2.py:146][0m 923648 total steps have happened
[32m[20230113 19:54:56 @agent_ppo2.py:122][0m #------------------------ Iteration 451 --------------------------#
[32m[20230113 19:54:56 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:54:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:56 @agent_ppo2.py:186][0m |          -0.0016 |          11.7712 |           3.8161 |
[32m[20230113 19:54:56 @agent_ppo2.py:186][0m |          -0.0061 |           6.9110 |           3.8185 |
[32m[20230113 19:54:56 @agent_ppo2.py:186][0m |          -0.0101 |           5.7072 |           3.8184 |
[32m[20230113 19:54:56 @agent_ppo2.py:186][0m |          -0.0014 |           5.4088 |           3.8176 |
[32m[20230113 19:54:56 @agent_ppo2.py:186][0m |          -0.0120 |           4.7964 |           3.8177 |
[32m[20230113 19:54:56 @agent_ppo2.py:186][0m |          -0.0158 |           4.6088 |           3.8180 |
[32m[20230113 19:54:57 @agent_ppo2.py:186][0m |          -0.0156 |           4.4672 |           3.8173 |
[32m[20230113 19:54:57 @agent_ppo2.py:186][0m |          -0.0126 |           4.2690 |           3.8196 |
[32m[20230113 19:54:57 @agent_ppo2.py:186][0m |          -0.0049 |           4.2934 |           3.8181 |
[32m[20230113 19:54:57 @agent_ppo2.py:186][0m |          -0.0154 |           4.0473 |           3.8200 |
[32m[20230113 19:54:57 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:54:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 141.97
[32m[20230113 19:54:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 206.66
[32m[20230113 19:54:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.98
[32m[20230113 19:54:57 @agent_ppo2.py:144][0m Total time:      10.41 min
[32m[20230113 19:54:57 @agent_ppo2.py:146][0m 925696 total steps have happened
[32m[20230113 19:54:57 @agent_ppo2.py:122][0m #------------------------ Iteration 452 --------------------------#
[32m[20230113 19:54:58 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:54:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:58 @agent_ppo2.py:186][0m |           0.0015 |           5.9213 |           3.9945 |
[32m[20230113 19:54:58 @agent_ppo2.py:186][0m |          -0.0065 |           5.4175 |           3.9935 |
[32m[20230113 19:54:58 @agent_ppo2.py:186][0m |          -0.0087 |           5.2396 |           3.9851 |
[32m[20230113 19:54:58 @agent_ppo2.py:186][0m |          -0.0104 |           5.1434 |           3.9863 |
[32m[20230113 19:54:58 @agent_ppo2.py:186][0m |          -0.0119 |           5.0210 |           3.9864 |
[32m[20230113 19:54:58 @agent_ppo2.py:186][0m |          -0.0132 |           4.9007 |           3.9874 |
[32m[20230113 19:54:58 @agent_ppo2.py:186][0m |          -0.0127 |           4.8093 |           3.9878 |
[32m[20230113 19:54:58 @agent_ppo2.py:186][0m |          -0.0147 |           4.7163 |           3.9869 |
[32m[20230113 19:54:58 @agent_ppo2.py:186][0m |          -0.0150 |           4.6640 |           3.9884 |
[32m[20230113 19:54:58 @agent_ppo2.py:186][0m |          -0.0162 |           4.5842 |           3.9885 |
[32m[20230113 19:54:58 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:54:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.70
[32m[20230113 19:54:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.74
[32m[20230113 19:54:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.77
[32m[20230113 19:54:58 @agent_ppo2.py:144][0m Total time:      10.43 min
[32m[20230113 19:54:58 @agent_ppo2.py:146][0m 927744 total steps have happened
[32m[20230113 19:54:58 @agent_ppo2.py:122][0m #------------------------ Iteration 453 --------------------------#
[32m[20230113 19:54:59 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:54:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:54:59 @agent_ppo2.py:186][0m |           0.0017 |           5.5484 |           3.9105 |
[32m[20230113 19:54:59 @agent_ppo2.py:186][0m |          -0.0036 |           5.0490 |           3.9133 |
[32m[20230113 19:54:59 @agent_ppo2.py:186][0m |          -0.0150 |           4.9046 |           3.9130 |
[32m[20230113 19:54:59 @agent_ppo2.py:186][0m |          -0.0147 |           4.8059 |           3.9090 |
[32m[20230113 19:54:59 @agent_ppo2.py:186][0m |          -0.0101 |           4.7010 |           3.9043 |
[32m[20230113 19:54:59 @agent_ppo2.py:186][0m |          -0.0123 |           4.6395 |           3.9026 |
[32m[20230113 19:54:59 @agent_ppo2.py:186][0m |          -0.0128 |           4.5890 |           3.9015 |
[32m[20230113 19:54:59 @agent_ppo2.py:186][0m |          -0.0107 |           4.5428 |           3.9007 |
[32m[20230113 19:54:59 @agent_ppo2.py:186][0m |          -0.0129 |           4.5057 |           3.8975 |
[32m[20230113 19:54:59 @agent_ppo2.py:186][0m |          -0.0145 |           4.4593 |           3.8983 |
[32m[20230113 19:54:59 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:55:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.61
[32m[20230113 19:55:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.13
[32m[20230113 19:55:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 124.88
[32m[20230113 19:55:00 @agent_ppo2.py:144][0m Total time:      10.45 min
[32m[20230113 19:55:00 @agent_ppo2.py:146][0m 929792 total steps have happened
[32m[20230113 19:55:00 @agent_ppo2.py:122][0m #------------------------ Iteration 454 --------------------------#
[32m[20230113 19:55:00 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:00 @agent_ppo2.py:186][0m |          -0.0014 |           5.6406 |           3.8331 |
[32m[20230113 19:55:00 @agent_ppo2.py:186][0m |          -0.0063 |           5.2678 |           3.8284 |
[32m[20230113 19:55:00 @agent_ppo2.py:186][0m |          -0.0124 |           4.9633 |           3.8232 |
[32m[20230113 19:55:00 @agent_ppo2.py:186][0m |          -0.0088 |           4.9373 |           3.8216 |
[32m[20230113 19:55:00 @agent_ppo2.py:186][0m |          -0.0130 |           4.6953 |           3.8188 |
[32m[20230113 19:55:00 @agent_ppo2.py:186][0m |          -0.0126 |           4.5913 |           3.8141 |
[32m[20230113 19:55:00 @agent_ppo2.py:186][0m |          -0.0162 |           4.5160 |           3.8142 |
[32m[20230113 19:55:01 @agent_ppo2.py:186][0m |          -0.0155 |           4.4379 |           3.8164 |
[32m[20230113 19:55:01 @agent_ppo2.py:186][0m |          -0.0185 |           4.4016 |           3.8128 |
[32m[20230113 19:55:01 @agent_ppo2.py:186][0m |          -0.0105 |           4.4976 |           3.8156 |
[32m[20230113 19:55:01 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:55:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.35
[32m[20230113 19:55:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.94
[32m[20230113 19:55:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.54
[32m[20230113 19:55:01 @agent_ppo2.py:144][0m Total time:      10.47 min
[32m[20230113 19:55:01 @agent_ppo2.py:146][0m 931840 total steps have happened
[32m[20230113 19:55:01 @agent_ppo2.py:122][0m #------------------------ Iteration 455 --------------------------#
[32m[20230113 19:55:02 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:55:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:02 @agent_ppo2.py:186][0m |           0.0004 |           4.6691 |           3.9554 |
[32m[20230113 19:55:02 @agent_ppo2.py:186][0m |          -0.0037 |           4.4746 |           3.9517 |
[32m[20230113 19:55:02 @agent_ppo2.py:186][0m |          -0.0060 |           4.2700 |           3.9520 |
[32m[20230113 19:55:02 @agent_ppo2.py:186][0m |          -0.0074 |           4.1685 |           3.9513 |
[32m[20230113 19:55:02 @agent_ppo2.py:186][0m |          -0.0079 |           4.1267 |           3.9505 |
[32m[20230113 19:55:02 @agent_ppo2.py:186][0m |          -0.0088 |           4.0465 |           3.9500 |
[32m[20230113 19:55:02 @agent_ppo2.py:186][0m |          -0.0093 |           4.0046 |           3.9491 |
[32m[20230113 19:55:02 @agent_ppo2.py:186][0m |          -0.0091 |           3.9874 |           3.9504 |
[32m[20230113 19:55:02 @agent_ppo2.py:186][0m |          -0.0099 |           3.9429 |           3.9493 |
[32m[20230113 19:55:02 @agent_ppo2.py:186][0m |          -0.0103 |           3.9121 |           3.9496 |
[32m[20230113 19:55:02 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:55:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.69
[32m[20230113 19:55:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.84
[32m[20230113 19:55:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 125.70
[32m[20230113 19:55:02 @agent_ppo2.py:144][0m Total time:      10.50 min
[32m[20230113 19:55:02 @agent_ppo2.py:146][0m 933888 total steps have happened
[32m[20230113 19:55:02 @agent_ppo2.py:122][0m #------------------------ Iteration 456 --------------------------#
[32m[20230113 19:55:03 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:55:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:03 @agent_ppo2.py:186][0m |          -0.0052 |           5.9138 |           3.9411 |
[32m[20230113 19:55:03 @agent_ppo2.py:186][0m |           0.0116 |           6.2740 |           3.9330 |
[32m[20230113 19:55:03 @agent_ppo2.py:186][0m |           0.0128 |           6.0700 |           3.9243 |
[32m[20230113 19:55:03 @agent_ppo2.py:186][0m |           0.0115 |           6.1122 |           3.9137 |
[32m[20230113 19:55:03 @agent_ppo2.py:186][0m |          -0.0054 |           5.3681 |           3.9188 |
[32m[20230113 19:55:03 @agent_ppo2.py:186][0m |          -0.0086 |           5.3148 |           3.9160 |
[32m[20230113 19:55:03 @agent_ppo2.py:186][0m |          -0.0105 |           5.2251 |           3.9142 |
[32m[20230113 19:55:03 @agent_ppo2.py:186][0m |          -0.0017 |           5.3189 |           3.9113 |
[32m[20230113 19:55:03 @agent_ppo2.py:186][0m |          -0.0099 |           5.1426 |           3.9112 |
[32m[20230113 19:55:03 @agent_ppo2.py:186][0m |          -0.0101 |           5.1250 |           3.9104 |
[32m[20230113 19:55:03 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:55:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.28
[32m[20230113 19:55:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.05
[32m[20230113 19:55:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.27
[32m[20230113 19:55:04 @agent_ppo2.py:144][0m Total time:      10.52 min
[32m[20230113 19:55:04 @agent_ppo2.py:146][0m 935936 total steps have happened
[32m[20230113 19:55:04 @agent_ppo2.py:122][0m #------------------------ Iteration 457 --------------------------#
[32m[20230113 19:55:04 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:04 @agent_ppo2.py:186][0m |          -0.0004 |           6.1289 |           3.8543 |
[32m[20230113 19:55:04 @agent_ppo2.py:186][0m |          -0.0019 |           5.9903 |           3.8504 |
[32m[20230113 19:55:04 @agent_ppo2.py:186][0m |          -0.0025 |           5.9259 |           3.8511 |
[32m[20230113 19:55:04 @agent_ppo2.py:186][0m |          -0.0068 |           5.6908 |           3.8505 |
[32m[20230113 19:55:04 @agent_ppo2.py:186][0m |          -0.0073 |           5.6779 |           3.8512 |
[32m[20230113 19:55:04 @agent_ppo2.py:186][0m |          -0.0092 |           5.5486 |           3.8490 |
[32m[20230113 19:55:05 @agent_ppo2.py:186][0m |          -0.0102 |           5.4994 |           3.8484 |
[32m[20230113 19:55:05 @agent_ppo2.py:186][0m |          -0.0096 |           5.4925 |           3.8497 |
[32m[20230113 19:55:05 @agent_ppo2.py:186][0m |          -0.0101 |           5.4312 |           3.8484 |
[32m[20230113 19:55:05 @agent_ppo2.py:186][0m |          -0.0115 |           5.3836 |           3.8498 |
[32m[20230113 19:55:05 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:55:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.23
[32m[20230113 19:55:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.32
[32m[20230113 19:55:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.82
[32m[20230113 19:55:05 @agent_ppo2.py:144][0m Total time:      10.54 min
[32m[20230113 19:55:05 @agent_ppo2.py:146][0m 937984 total steps have happened
[32m[20230113 19:55:05 @agent_ppo2.py:122][0m #------------------------ Iteration 458 --------------------------#
[32m[20230113 19:55:06 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:55:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:06 @agent_ppo2.py:186][0m |          -0.0020 |          15.0646 |           3.9574 |
[32m[20230113 19:55:06 @agent_ppo2.py:186][0m |          -0.0073 |           6.4354 |           3.9595 |
[32m[20230113 19:55:06 @agent_ppo2.py:186][0m |          -0.0089 |           6.0344 |           3.9564 |
[32m[20230113 19:55:06 @agent_ppo2.py:186][0m |          -0.0106 |           5.8844 |           3.9557 |
[32m[20230113 19:55:06 @agent_ppo2.py:186][0m |          -0.0043 |           5.7501 |           3.9539 |
[32m[20230113 19:55:06 @agent_ppo2.py:186][0m |          -0.0114 |           5.6178 |           3.9500 |
[32m[20230113 19:55:06 @agent_ppo2.py:186][0m |          -0.0115 |           5.5597 |           3.9515 |
[32m[20230113 19:55:06 @agent_ppo2.py:186][0m |          -0.0141 |           5.4457 |           3.9466 |
[32m[20230113 19:55:06 @agent_ppo2.py:186][0m |          -0.0082 |           6.0234 |           3.9460 |
[32m[20230113 19:55:06 @agent_ppo2.py:186][0m |          -0.0138 |           5.4241 |           3.9458 |
[32m[20230113 19:55:06 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 19:55:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 127.02
[32m[20230113 19:55:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.07
[32m[20230113 19:55:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.92
[32m[20230113 19:55:07 @agent_ppo2.py:144][0m Total time:      10.57 min
[32m[20230113 19:55:07 @agent_ppo2.py:146][0m 940032 total steps have happened
[32m[20230113 19:55:07 @agent_ppo2.py:122][0m #------------------------ Iteration 459 --------------------------#
[32m[20230113 19:55:07 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:55:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:07 @agent_ppo2.py:186][0m |           0.0004 |           5.3629 |           3.9553 |
[32m[20230113 19:55:07 @agent_ppo2.py:186][0m |          -0.0060 |           4.9414 |           3.9496 |
[32m[20230113 19:55:07 @agent_ppo2.py:186][0m |          -0.0077 |           4.8328 |           3.9466 |
[32m[20230113 19:55:07 @agent_ppo2.py:186][0m |          -0.0106 |           4.6742 |           3.9466 |
[32m[20230113 19:55:07 @agent_ppo2.py:186][0m |          -0.0109 |           4.5835 |           3.9441 |
[32m[20230113 19:55:07 @agent_ppo2.py:186][0m |          -0.0114 |           4.4815 |           3.9422 |
[32m[20230113 19:55:07 @agent_ppo2.py:186][0m |          -0.0117 |           4.3994 |           3.9422 |
[32m[20230113 19:55:07 @agent_ppo2.py:186][0m |          -0.0125 |           4.3267 |           3.9423 |
[32m[20230113 19:55:07 @agent_ppo2.py:186][0m |          -0.0124 |           4.2957 |           3.9406 |
[32m[20230113 19:55:07 @agent_ppo2.py:186][0m |          -0.0130 |           4.2133 |           3.9415 |
[32m[20230113 19:55:07 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:55:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.68
[32m[20230113 19:55:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.87
[32m[20230113 19:55:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.56
[32m[20230113 19:55:08 @agent_ppo2.py:144][0m Total time:      10.59 min
[32m[20230113 19:55:08 @agent_ppo2.py:146][0m 942080 total steps have happened
[32m[20230113 19:55:08 @agent_ppo2.py:122][0m #------------------------ Iteration 460 --------------------------#
[32m[20230113 19:55:08 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:55:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:08 @agent_ppo2.py:186][0m |          -0.0012 |           5.1379 |           3.8887 |
[32m[20230113 19:55:08 @agent_ppo2.py:186][0m |          -0.0063 |           4.8724 |           3.8829 |
[32m[20230113 19:55:09 @agent_ppo2.py:186][0m |          -0.0081 |           4.6697 |           3.8834 |
[32m[20230113 19:55:09 @agent_ppo2.py:186][0m |          -0.0063 |           4.6365 |           3.8808 |
[32m[20230113 19:55:09 @agent_ppo2.py:186][0m |          -0.0071 |           4.4907 |           3.8840 |
[32m[20230113 19:55:09 @agent_ppo2.py:186][0m |          -0.0113 |           4.3755 |           3.8851 |
[32m[20230113 19:55:09 @agent_ppo2.py:186][0m |          -0.0053 |           4.5098 |           3.8851 |
[32m[20230113 19:55:09 @agent_ppo2.py:186][0m |          -0.0119 |           4.2685 |           3.8833 |
[32m[20230113 19:55:09 @agent_ppo2.py:186][0m |          -0.0109 |           4.2995 |           3.8832 |
[32m[20230113 19:55:09 @agent_ppo2.py:186][0m |          -0.0139 |           4.1252 |           3.8852 |
[32m[20230113 19:55:09 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:55:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.72
[32m[20230113 19:55:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.21
[32m[20230113 19:55:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 140.81
[32m[20230113 19:55:09 @agent_ppo2.py:144][0m Total time:      10.61 min
[32m[20230113 19:55:09 @agent_ppo2.py:146][0m 944128 total steps have happened
[32m[20230113 19:55:09 @agent_ppo2.py:122][0m #------------------------ Iteration 461 --------------------------#
[32m[20230113 19:55:10 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:55:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:10 @agent_ppo2.py:186][0m |          -0.0004 |           6.0437 |           3.9446 |
[32m[20230113 19:55:10 @agent_ppo2.py:186][0m |          -0.0045 |           5.7299 |           3.9314 |
[32m[20230113 19:55:10 @agent_ppo2.py:186][0m |          -0.0077 |           5.5806 |           3.9267 |
[32m[20230113 19:55:10 @agent_ppo2.py:186][0m |          -0.0093 |           5.4808 |           3.9281 |
[32m[20230113 19:55:10 @agent_ppo2.py:186][0m |          -0.0077 |           5.4106 |           3.9286 |
[32m[20230113 19:55:10 @agent_ppo2.py:186][0m |          -0.0101 |           5.3661 |           3.9224 |
[32m[20230113 19:55:10 @agent_ppo2.py:186][0m |          -0.0084 |           5.3216 |           3.9269 |
[32m[20230113 19:55:10 @agent_ppo2.py:186][0m |          -0.0107 |           5.2569 |           3.9237 |
[32m[20230113 19:55:10 @agent_ppo2.py:186][0m |          -0.0093 |           5.2922 |           3.9287 |
[32m[20230113 19:55:10 @agent_ppo2.py:186][0m |          -0.0114 |           5.1597 |           3.9252 |
[32m[20230113 19:55:10 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:55:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.82
[32m[20230113 19:55:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.17
[32m[20230113 19:55:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.30
[32m[20230113 19:55:11 @agent_ppo2.py:144][0m Total time:      10.63 min
[32m[20230113 19:55:11 @agent_ppo2.py:146][0m 946176 total steps have happened
[32m[20230113 19:55:11 @agent_ppo2.py:122][0m #------------------------ Iteration 462 --------------------------#
[32m[20230113 19:55:11 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:55:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:11 @agent_ppo2.py:186][0m |           0.0004 |           6.6402 |           3.8925 |
[32m[20230113 19:55:11 @agent_ppo2.py:186][0m |          -0.0045 |           6.3336 |           3.8915 |
[32m[20230113 19:55:11 @agent_ppo2.py:186][0m |          -0.0060 |           6.1412 |           3.8817 |
[32m[20230113 19:55:11 @agent_ppo2.py:186][0m |          -0.0078 |           6.0818 |           3.8842 |
[32m[20230113 19:55:11 @agent_ppo2.py:186][0m |          -0.0100 |           5.9026 |           3.8859 |
[32m[20230113 19:55:11 @agent_ppo2.py:186][0m |          -0.0113 |           5.8272 |           3.8856 |
[32m[20230113 19:55:11 @agent_ppo2.py:186][0m |          -0.0114 |           5.8135 |           3.8825 |
[32m[20230113 19:55:11 @agent_ppo2.py:186][0m |          -0.0131 |           5.7504 |           3.8830 |
[32m[20230113 19:55:11 @agent_ppo2.py:186][0m |          -0.0132 |           5.7134 |           3.8819 |
[32m[20230113 19:55:11 @agent_ppo2.py:186][0m |          -0.0130 |           5.6772 |           3.8810 |
[32m[20230113 19:55:11 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:55:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.13
[32m[20230113 19:55:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.48
[32m[20230113 19:55:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.29
[32m[20230113 19:55:12 @agent_ppo2.py:144][0m Total time:      10.65 min
[32m[20230113 19:55:12 @agent_ppo2.py:146][0m 948224 total steps have happened
[32m[20230113 19:55:12 @agent_ppo2.py:122][0m #------------------------ Iteration 463 --------------------------#
[32m[20230113 19:55:12 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:55:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:12 @agent_ppo2.py:186][0m |          -0.0042 |           4.7753 |           3.8250 |
[32m[20230113 19:55:12 @agent_ppo2.py:186][0m |          -0.0065 |           4.3389 |           3.8142 |
[32m[20230113 19:55:12 @agent_ppo2.py:186][0m |          -0.0091 |           4.1335 |           3.8200 |
[32m[20230113 19:55:12 @agent_ppo2.py:186][0m |          -0.0072 |           3.9789 |           3.8138 |
[32m[20230113 19:55:13 @agent_ppo2.py:186][0m |          -0.0052 |           3.8942 |           3.8132 |
[32m[20230113 19:55:13 @agent_ppo2.py:186][0m |          -0.0107 |           3.7631 |           3.8150 |
[32m[20230113 19:55:13 @agent_ppo2.py:186][0m |          -0.0096 |           3.6873 |           3.8154 |
[32m[20230113 19:55:13 @agent_ppo2.py:186][0m |          -0.0128 |           3.5840 |           3.8133 |
[32m[20230113 19:55:13 @agent_ppo2.py:186][0m |          -0.0142 |           3.5195 |           3.8134 |
[32m[20230113 19:55:13 @agent_ppo2.py:186][0m |          -0.0123 |           3.4540 |           3.8141 |
[32m[20230113 19:55:13 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:55:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.86
[32m[20230113 19:55:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.81
[32m[20230113 19:55:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.94
[32m[20230113 19:55:13 @agent_ppo2.py:144][0m Total time:      10.68 min
[32m[20230113 19:55:13 @agent_ppo2.py:146][0m 950272 total steps have happened
[32m[20230113 19:55:13 @agent_ppo2.py:122][0m #------------------------ Iteration 464 --------------------------#
[32m[20230113 19:55:14 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:55:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:14 @agent_ppo2.py:186][0m |           0.0003 |           4.6596 |           3.9533 |
[32m[20230113 19:55:14 @agent_ppo2.py:186][0m |          -0.0044 |           4.2565 |           3.9420 |
[32m[20230113 19:55:14 @agent_ppo2.py:186][0m |          -0.0042 |           4.2113 |           3.9410 |
[32m[20230113 19:55:14 @agent_ppo2.py:186][0m |          -0.0065 |           3.9266 |           3.9376 |
[32m[20230113 19:55:14 @agent_ppo2.py:186][0m |          -0.0108 |           3.7646 |           3.9397 |
[32m[20230113 19:55:14 @agent_ppo2.py:186][0m |          -0.0108 |           3.6826 |           3.9366 |
[32m[20230113 19:55:14 @agent_ppo2.py:186][0m |          -0.0098 |           3.6418 |           3.9364 |
[32m[20230113 19:55:14 @agent_ppo2.py:186][0m |          -0.0086 |           3.5806 |           3.9366 |
[32m[20230113 19:55:14 @agent_ppo2.py:186][0m |          -0.0094 |           3.5266 |           3.9373 |
[32m[20230113 19:55:14 @agent_ppo2.py:186][0m |          -0.0101 |           3.4737 |           3.9352 |
[32m[20230113 19:55:14 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:55:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.48
[32m[20230113 19:55:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.57
[32m[20230113 19:55:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.42
[32m[20230113 19:55:15 @agent_ppo2.py:144][0m Total time:      10.70 min
[32m[20230113 19:55:15 @agent_ppo2.py:146][0m 952320 total steps have happened
[32m[20230113 19:55:15 @agent_ppo2.py:122][0m #------------------------ Iteration 465 --------------------------#
[32m[20230113 19:55:15 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:15 @agent_ppo2.py:186][0m |          -0.0033 |           6.4352 |           3.8494 |
[32m[20230113 19:55:15 @agent_ppo2.py:186][0m |          -0.0068 |           6.1328 |           3.8421 |
[32m[20230113 19:55:15 @agent_ppo2.py:186][0m |          -0.0098 |           6.0253 |           3.8473 |
[32m[20230113 19:55:15 @agent_ppo2.py:186][0m |          -0.0106 |           5.8849 |           3.8437 |
[32m[20230113 19:55:15 @agent_ppo2.py:186][0m |          -0.0072 |           5.7996 |           3.8421 |
[32m[20230113 19:55:15 @agent_ppo2.py:186][0m |          -0.0113 |           5.7299 |           3.8415 |
[32m[20230113 19:55:15 @agent_ppo2.py:186][0m |          -0.0081 |           5.6781 |           3.8467 |
[32m[20230113 19:55:15 @agent_ppo2.py:186][0m |          -0.0222 |           5.6947 |           3.8416 |
[32m[20230113 19:55:15 @agent_ppo2.py:186][0m |          -0.0207 |           5.5975 |           3.8467 |
[32m[20230113 19:55:15 @agent_ppo2.py:186][0m |           0.0831 |          10.2809 |           3.8465 |
[32m[20230113 19:55:15 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:55:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.06
[32m[20230113 19:55:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.53
[32m[20230113 19:55:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 145.06
[32m[20230113 19:55:16 @agent_ppo2.py:144][0m Total time:      10.72 min
[32m[20230113 19:55:16 @agent_ppo2.py:146][0m 954368 total steps have happened
[32m[20230113 19:55:16 @agent_ppo2.py:122][0m #------------------------ Iteration 466 --------------------------#
[32m[20230113 19:55:16 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:17 @agent_ppo2.py:186][0m |           0.0014 |           6.2705 |           3.9060 |
[32m[20230113 19:55:17 @agent_ppo2.py:186][0m |          -0.0072 |           5.0828 |           3.9043 |
[32m[20230113 19:55:17 @agent_ppo2.py:186][0m |          -0.0067 |           4.8973 |           3.9070 |
[32m[20230113 19:55:17 @agent_ppo2.py:186][0m |          -0.0086 |           4.7327 |           3.9035 |
[32m[20230113 19:55:17 @agent_ppo2.py:186][0m |          -0.0084 |           4.6278 |           3.9078 |
[32m[20230113 19:55:17 @agent_ppo2.py:186][0m |          -0.0107 |           4.5413 |           3.9065 |
[32m[20230113 19:55:17 @agent_ppo2.py:186][0m |          -0.0078 |           4.6070 |           3.9066 |
[32m[20230113 19:55:17 @agent_ppo2.py:186][0m |          -0.0139 |           4.4156 |           3.9082 |
[32m[20230113 19:55:17 @agent_ppo2.py:186][0m |          -0.0145 |           4.3476 |           3.9045 |
[32m[20230113 19:55:17 @agent_ppo2.py:186][0m |          -0.0139 |           4.3127 |           3.9062 |
[32m[20230113 19:55:17 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:55:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.21
[32m[20230113 19:55:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.29
[32m[20230113 19:55:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.47
[32m[20230113 19:55:17 @agent_ppo2.py:144][0m Total time:      10.75 min
[32m[20230113 19:55:17 @agent_ppo2.py:146][0m 956416 total steps have happened
[32m[20230113 19:55:17 @agent_ppo2.py:122][0m #------------------------ Iteration 467 --------------------------#
[32m[20230113 19:55:18 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:55:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:18 @agent_ppo2.py:186][0m |          -0.0011 |           6.0196 |           4.0103 |
[32m[20230113 19:55:18 @agent_ppo2.py:186][0m |           0.0015 |           5.8054 |           4.0014 |
[32m[20230113 19:55:18 @agent_ppo2.py:186][0m |          -0.0086 |           5.5862 |           3.9975 |
[32m[20230113 19:55:18 @agent_ppo2.py:186][0m |          -0.0073 |           5.5511 |           3.9922 |
[32m[20230113 19:55:18 @agent_ppo2.py:186][0m |          -0.0020 |           5.6329 |           3.9887 |
[32m[20230113 19:55:18 @agent_ppo2.py:186][0m |          -0.0095 |           5.3602 |           3.9859 |
[32m[20230113 19:55:18 @agent_ppo2.py:186][0m |          -0.0058 |           5.3193 |           3.9892 |
[32m[20230113 19:55:18 @agent_ppo2.py:186][0m |          -0.0096 |           5.2358 |           3.9869 |
[32m[20230113 19:55:18 @agent_ppo2.py:186][0m |          -0.0129 |           5.2083 |           3.9889 |
[32m[20230113 19:55:18 @agent_ppo2.py:186][0m |          -0.0082 |           5.2719 |           3.9895 |
[32m[20230113 19:55:18 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:55:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.95
[32m[20230113 19:55:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.45
[32m[20230113 19:55:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.43
[32m[20230113 19:55:19 @agent_ppo2.py:144][0m Total time:      10.77 min
[32m[20230113 19:55:19 @agent_ppo2.py:146][0m 958464 total steps have happened
[32m[20230113 19:55:19 @agent_ppo2.py:122][0m #------------------------ Iteration 468 --------------------------#
[32m[20230113 19:55:19 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:19 @agent_ppo2.py:186][0m |           0.0011 |           5.3315 |           3.8355 |
[32m[20230113 19:55:19 @agent_ppo2.py:186][0m |          -0.0043 |           5.0485 |           3.8364 |
[32m[20230113 19:55:19 @agent_ppo2.py:186][0m |          -0.0084 |           4.9228 |           3.8356 |
[32m[20230113 19:55:19 @agent_ppo2.py:186][0m |          -0.0064 |           4.8433 |           3.8295 |
[32m[20230113 19:55:19 @agent_ppo2.py:186][0m |          -0.0074 |           4.7990 |           3.8305 |
[32m[20230113 19:55:19 @agent_ppo2.py:186][0m |          -0.0069 |           4.7496 |           3.8299 |
[32m[20230113 19:55:19 @agent_ppo2.py:186][0m |          -0.0068 |           4.6818 |           3.8298 |
[32m[20230113 19:55:19 @agent_ppo2.py:186][0m |          -0.0092 |           4.6199 |           3.8290 |
[32m[20230113 19:55:20 @agent_ppo2.py:186][0m |          -0.0081 |           4.6120 |           3.8286 |
[32m[20230113 19:55:20 @agent_ppo2.py:186][0m |          -0.0012 |           4.9332 |           3.8258 |
[32m[20230113 19:55:20 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:55:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.30
[32m[20230113 19:55:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.48
[32m[20230113 19:55:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.44
[32m[20230113 19:55:20 @agent_ppo2.py:144][0m Total time:      10.79 min
[32m[20230113 19:55:20 @agent_ppo2.py:146][0m 960512 total steps have happened
[32m[20230113 19:55:20 @agent_ppo2.py:122][0m #------------------------ Iteration 469 --------------------------#
[32m[20230113 19:55:20 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 19:55:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:20 @agent_ppo2.py:186][0m |          -0.0047 |          16.6101 |           3.8654 |
[32m[20230113 19:55:21 @agent_ppo2.py:186][0m |          -0.0061 |           4.8294 |           3.8637 |
[32m[20230113 19:55:21 @agent_ppo2.py:186][0m |          -0.0110 |           4.4012 |           3.8643 |
[32m[20230113 19:55:21 @agent_ppo2.py:186][0m |           0.0264 |           4.4179 |           3.8598 |
[32m[20230113 19:55:21 @agent_ppo2.py:186][0m |          -0.0068 |           4.0527 |           3.8556 |
[32m[20230113 19:55:21 @agent_ppo2.py:186][0m |          -0.0112 |           3.8980 |           3.8577 |
[32m[20230113 19:55:21 @agent_ppo2.py:186][0m |          -0.0118 |           3.7501 |           3.8562 |
[32m[20230113 19:55:21 @agent_ppo2.py:186][0m |          -0.0116 |           3.7216 |           3.8558 |
[32m[20230113 19:55:21 @agent_ppo2.py:186][0m |           0.0129 |           7.1095 |           3.8572 |
[32m[20230113 19:55:21 @agent_ppo2.py:186][0m |          -0.0160 |           4.1118 |           3.8580 |
[32m[20230113 19:55:21 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 19:55:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 134.48
[32m[20230113 19:55:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.95
[32m[20230113 19:55:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 129.17
[32m[20230113 19:55:21 @agent_ppo2.py:144][0m Total time:      10.81 min
[32m[20230113 19:55:21 @agent_ppo2.py:146][0m 962560 total steps have happened
[32m[20230113 19:55:21 @agent_ppo2.py:122][0m #------------------------ Iteration 470 --------------------------#
[32m[20230113 19:55:21 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 19:55:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:22 @agent_ppo2.py:186][0m |           0.0012 |          24.9181 |           3.9441 |
[32m[20230113 19:55:22 @agent_ppo2.py:186][0m |          -0.0045 |           8.6593 |           3.9397 |
[32m[20230113 19:55:22 @agent_ppo2.py:186][0m |          -0.0055 |           7.3031 |           3.9414 |
[32m[20230113 19:55:22 @agent_ppo2.py:186][0m |          -0.0081 |           6.7949 |           3.9376 |
[32m[20230113 19:55:22 @agent_ppo2.py:186][0m |          -0.0089 |           6.4350 |           3.9391 |
[32m[20230113 19:55:22 @agent_ppo2.py:186][0m |          -0.0096 |           6.2085 |           3.9380 |
[32m[20230113 19:55:22 @agent_ppo2.py:186][0m |          -0.0112 |           6.0248 |           3.9372 |
[32m[20230113 19:55:22 @agent_ppo2.py:186][0m |          -0.0121 |           5.7458 |           3.9382 |
[32m[20230113 19:55:22 @agent_ppo2.py:186][0m |          -0.0123 |           5.5728 |           3.9377 |
[32m[20230113 19:55:22 @agent_ppo2.py:186][0m |          -0.0131 |           5.4174 |           3.9375 |
[32m[20230113 19:55:22 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 19:55:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 10.36
[32m[20230113 19:55:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.75
[32m[20230113 19:55:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.50
[32m[20230113 19:55:22 @agent_ppo2.py:144][0m Total time:      10.83 min
[32m[20230113 19:55:22 @agent_ppo2.py:146][0m 964608 total steps have happened
[32m[20230113 19:55:22 @agent_ppo2.py:122][0m #------------------------ Iteration 471 --------------------------#
[32m[20230113 19:55:22 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 19:55:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:23 @agent_ppo2.py:186][0m |           0.0001 |          38.9019 |           4.0016 |
[32m[20230113 19:55:23 @agent_ppo2.py:186][0m |          -0.0061 |          25.4752 |           3.9976 |
[32m[20230113 19:55:23 @agent_ppo2.py:186][0m |          -0.0088 |          21.7185 |           3.9946 |
[32m[20230113 19:55:23 @agent_ppo2.py:186][0m |          -0.0107 |          19.2680 |           3.9946 |
[32m[20230113 19:55:23 @agent_ppo2.py:186][0m |          -0.0111 |          17.5474 |           3.9964 |
[32m[20230113 19:55:23 @agent_ppo2.py:186][0m |          -0.0125 |          16.4076 |           3.9935 |
[32m[20230113 19:55:23 @agent_ppo2.py:186][0m |          -0.0135 |          15.3909 |           3.9909 |
[32m[20230113 19:55:23 @agent_ppo2.py:186][0m |          -0.0153 |          14.4780 |           3.9934 |
[32m[20230113 19:55:23 @agent_ppo2.py:186][0m |          -0.0138 |          13.8954 |           3.9943 |
[32m[20230113 19:55:23 @agent_ppo2.py:186][0m |          -0.0151 |          13.2814 |           3.9923 |
[32m[20230113 19:55:23 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 19:55:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 97.47
[32m[20230113 19:55:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.63
[32m[20230113 19:55:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.12
[32m[20230113 19:55:23 @agent_ppo2.py:144][0m Total time:      10.84 min
[32m[20230113 19:55:23 @agent_ppo2.py:146][0m 966656 total steps have happened
[32m[20230113 19:55:23 @agent_ppo2.py:122][0m #------------------------ Iteration 472 --------------------------#
[32m[20230113 19:55:24 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:55:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:24 @agent_ppo2.py:186][0m |           0.0005 |          10.7767 |           3.8503 |
[32m[20230113 19:55:24 @agent_ppo2.py:186][0m |          -0.0019 |           7.7315 |           3.8483 |
[32m[20230113 19:55:24 @agent_ppo2.py:186][0m |          -0.0056 |           7.1020 |           3.8445 |
[32m[20230113 19:55:24 @agent_ppo2.py:186][0m |          -0.0077 |           6.7299 |           3.8473 |
[32m[20230113 19:55:24 @agent_ppo2.py:186][0m |          -0.0098 |           6.4050 |           3.8440 |
[32m[20230113 19:55:24 @agent_ppo2.py:186][0m |          -0.0096 |           6.1743 |           3.8447 |
[32m[20230113 19:55:24 @agent_ppo2.py:186][0m |          -0.0110 |           5.9906 |           3.8445 |
[32m[20230113 19:55:24 @agent_ppo2.py:186][0m |          -0.0118 |           5.8109 |           3.8419 |
[32m[20230113 19:55:24 @agent_ppo2.py:186][0m |          -0.0111 |           5.6661 |           3.8399 |
[32m[20230113 19:55:24 @agent_ppo2.py:186][0m |          -0.0123 |           5.5609 |           3.8436 |
[32m[20230113 19:55:24 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:55:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.69
[32m[20230113 19:55:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.38
[32m[20230113 19:55:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.35
[32m[20230113 19:55:25 @agent_ppo2.py:144][0m Total time:      10.87 min
[32m[20230113 19:55:25 @agent_ppo2.py:146][0m 968704 total steps have happened
[32m[20230113 19:55:25 @agent_ppo2.py:122][0m #------------------------ Iteration 473 --------------------------#
[32m[20230113 19:55:25 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:55:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:25 @agent_ppo2.py:186][0m |          -0.0029 |          12.9411 |           3.9822 |
[32m[20230113 19:55:25 @agent_ppo2.py:186][0m |          -0.0055 |           8.4049 |           3.9866 |
[32m[20230113 19:55:25 @agent_ppo2.py:186][0m |          -0.0104 |           7.6798 |           3.9889 |
[32m[20230113 19:55:25 @agent_ppo2.py:186][0m |          -0.0062 |           7.3832 |           3.9869 |
[32m[20230113 19:55:25 @agent_ppo2.py:186][0m |          -0.0129 |           7.0533 |           3.9848 |
[32m[20230113 19:55:25 @agent_ppo2.py:186][0m |          -0.0132 |           6.8188 |           3.9852 |
[32m[20230113 19:55:25 @agent_ppo2.py:186][0m |          -0.0139 |           6.6357 |           3.9866 |
[32m[20230113 19:55:25 @agent_ppo2.py:186][0m |          -0.0147 |           6.4856 |           3.9874 |
[32m[20230113 19:55:26 @agent_ppo2.py:186][0m |          -0.0140 |           6.3618 |           3.9874 |
[32m[20230113 19:55:26 @agent_ppo2.py:186][0m |          -0.0126 |           6.5235 |           3.9884 |
[32m[20230113 19:55:26 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 19:55:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 119.59
[32m[20230113 19:55:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.77
[32m[20230113 19:55:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 133.30
[32m[20230113 19:55:26 @agent_ppo2.py:144][0m Total time:      10.89 min
[32m[20230113 19:55:26 @agent_ppo2.py:146][0m 970752 total steps have happened
[32m[20230113 19:55:26 @agent_ppo2.py:122][0m #------------------------ Iteration 474 --------------------------#
[32m[20230113 19:55:26 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:55:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:27 @agent_ppo2.py:186][0m |           0.0004 |           5.3909 |           3.9406 |
[32m[20230113 19:55:27 @agent_ppo2.py:186][0m |          -0.0041 |           4.8553 |           3.9404 |
[32m[20230113 19:55:27 @agent_ppo2.py:186][0m |          -0.0054 |           4.5563 |           3.9391 |
[32m[20230113 19:55:27 @agent_ppo2.py:186][0m |          -0.0063 |           4.3540 |           3.9377 |
[32m[20230113 19:55:27 @agent_ppo2.py:186][0m |          -0.0078 |           4.1444 |           3.9348 |
[32m[20230113 19:55:27 @agent_ppo2.py:186][0m |          -0.0048 |           4.1338 |           3.9370 |
[32m[20230113 19:55:27 @agent_ppo2.py:186][0m |          -0.0073 |           3.9698 |           3.9344 |
[32m[20230113 19:55:27 @agent_ppo2.py:186][0m |          -0.0098 |           3.7536 |           3.9367 |
[32m[20230113 19:55:27 @agent_ppo2.py:186][0m |          -0.0112 |           3.6752 |           3.9371 |
[32m[20230113 19:55:27 @agent_ppo2.py:186][0m |          -0.0085 |           3.6038 |           3.9356 |
[32m[20230113 19:55:27 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:55:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.11
[32m[20230113 19:55:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.17
[32m[20230113 19:55:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.35
[32m[20230113 19:55:27 @agent_ppo2.py:144][0m Total time:      10.91 min
[32m[20230113 19:55:27 @agent_ppo2.py:146][0m 972800 total steps have happened
[32m[20230113 19:55:27 @agent_ppo2.py:122][0m #------------------------ Iteration 475 --------------------------#
[32m[20230113 19:55:28 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:28 @agent_ppo2.py:186][0m |          -0.0052 |           4.9446 |           3.8751 |
[32m[20230113 19:55:28 @agent_ppo2.py:186][0m |           0.0005 |           4.6847 |           3.8684 |
[32m[20230113 19:55:28 @agent_ppo2.py:186][0m |          -0.0053 |           4.4161 |           3.8698 |
[32m[20230113 19:55:28 @agent_ppo2.py:186][0m |          -0.0084 |           4.3094 |           3.8690 |
[32m[20230113 19:55:28 @agent_ppo2.py:186][0m |          -0.0115 |           4.2117 |           3.8671 |
[32m[20230113 19:55:28 @agent_ppo2.py:186][0m |          -0.0159 |           4.1723 |           3.8665 |
[32m[20230113 19:55:28 @agent_ppo2.py:186][0m |          -0.0134 |           4.1132 |           3.8614 |
[32m[20230113 19:55:28 @agent_ppo2.py:186][0m |          -0.0111 |           4.0346 |           3.8671 |
[32m[20230113 19:55:28 @agent_ppo2.py:186][0m |          -0.0091 |           3.9946 |           3.8679 |
[32m[20230113 19:55:28 @agent_ppo2.py:186][0m |          -0.0135 |           3.9411 |           3.8653 |
[32m[20230113 19:55:28 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:55:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.02
[32m[20230113 19:55:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.95
[32m[20230113 19:55:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 169.85
[32m[20230113 19:55:29 @agent_ppo2.py:144][0m Total time:      10.94 min
[32m[20230113 19:55:29 @agent_ppo2.py:146][0m 974848 total steps have happened
[32m[20230113 19:55:29 @agent_ppo2.py:122][0m #------------------------ Iteration 476 --------------------------#
[32m[20230113 19:55:29 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:29 @agent_ppo2.py:186][0m |           0.0005 |           5.9008 |           3.8762 |
[32m[20230113 19:55:29 @agent_ppo2.py:186][0m |          -0.0098 |           5.1963 |           3.8709 |
[32m[20230113 19:55:29 @agent_ppo2.py:186][0m |           0.0016 |           4.8891 |           3.8675 |
[32m[20230113 19:55:29 @agent_ppo2.py:186][0m |          -0.0158 |           4.5634 |           3.8656 |
[32m[20230113 19:55:29 @agent_ppo2.py:186][0m |          -0.0116 |           4.3187 |           3.8658 |
[32m[20230113 19:55:29 @agent_ppo2.py:186][0m |          -0.0159 |           4.1276 |           3.8656 |
[32m[20230113 19:55:30 @agent_ppo2.py:186][0m |           0.0017 |           4.0214 |           3.8640 |
[32m[20230113 19:55:30 @agent_ppo2.py:186][0m |          -0.0158 |           3.8374 |           3.8611 |
[32m[20230113 19:55:30 @agent_ppo2.py:186][0m |          -0.0121 |           3.7375 |           3.8601 |
[32m[20230113 19:55:30 @agent_ppo2.py:186][0m |          -0.0159 |           3.6460 |           3.8625 |
[32m[20230113 19:55:30 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:55:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.48
[32m[20230113 19:55:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.46
[32m[20230113 19:55:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 190.73
[32m[20230113 19:55:30 @agent_ppo2.py:144][0m Total time:      10.96 min
[32m[20230113 19:55:30 @agent_ppo2.py:146][0m 976896 total steps have happened
[32m[20230113 19:55:30 @agent_ppo2.py:122][0m #------------------------ Iteration 477 --------------------------#
[32m[20230113 19:55:30 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:31 @agent_ppo2.py:186][0m |          -0.0038 |           5.0033 |           3.9894 |
[32m[20230113 19:55:31 @agent_ppo2.py:186][0m |          -0.0093 |           4.7162 |           3.9882 |
[32m[20230113 19:55:31 @agent_ppo2.py:186][0m |          -0.0078 |           4.5623 |           3.9885 |
[32m[20230113 19:55:31 @agent_ppo2.py:186][0m |          -0.0087 |           4.4648 |           3.9885 |
[32m[20230113 19:55:31 @agent_ppo2.py:186][0m |          -0.0116 |           4.3711 |           3.9885 |
[32m[20230113 19:55:31 @agent_ppo2.py:186][0m |          -0.0103 |           4.3116 |           3.9872 |
[32m[20230113 19:55:31 @agent_ppo2.py:186][0m |          -0.0097 |           4.2433 |           3.9857 |
[32m[20230113 19:55:31 @agent_ppo2.py:186][0m |          -0.0113 |           4.2040 |           3.9871 |
[32m[20230113 19:55:31 @agent_ppo2.py:186][0m |          -0.0127 |           4.1778 |           3.9875 |
[32m[20230113 19:55:31 @agent_ppo2.py:186][0m |          -0.0121 |           4.1422 |           3.9866 |
[32m[20230113 19:55:31 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:55:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.05
[32m[20230113 19:55:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.00
[32m[20230113 19:55:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 152.90
[32m[20230113 19:55:31 @agent_ppo2.py:144][0m Total time:      10.98 min
[32m[20230113 19:55:31 @agent_ppo2.py:146][0m 978944 total steps have happened
[32m[20230113 19:55:31 @agent_ppo2.py:122][0m #------------------------ Iteration 478 --------------------------#
[32m[20230113 19:55:32 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 19:55:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:32 @agent_ppo2.py:186][0m |          -0.0006 |          35.0672 |           3.9154 |
[32m[20230113 19:55:32 @agent_ppo2.py:186][0m |          -0.0072 |          12.1611 |           3.9093 |
[32m[20230113 19:55:32 @agent_ppo2.py:186][0m |          -0.0082 |           8.2292 |           3.9081 |
[32m[20230113 19:55:32 @agent_ppo2.py:186][0m |          -0.0099 |           7.0245 |           3.9054 |
[32m[20230113 19:55:32 @agent_ppo2.py:186][0m |          -0.0116 |           6.0953 |           3.9096 |
[32m[20230113 19:55:32 @agent_ppo2.py:186][0m |          -0.0131 |           5.4136 |           3.9050 |
[32m[20230113 19:55:32 @agent_ppo2.py:186][0m |          -0.0133 |           5.0996 |           3.9043 |
[32m[20230113 19:55:32 @agent_ppo2.py:186][0m |          -0.0139 |           4.8567 |           3.9039 |
[32m[20230113 19:55:32 @agent_ppo2.py:186][0m |          -0.0152 |           4.5419 |           3.9047 |
[32m[20230113 19:55:32 @agent_ppo2.py:186][0m |          -0.0154 |           4.3058 |           3.9035 |
[32m[20230113 19:55:32 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 19:55:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 72.03
[32m[20230113 19:55:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 96.95
[32m[20230113 19:55:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 158.18
[32m[20230113 19:55:33 @agent_ppo2.py:144][0m Total time:      11.00 min
[32m[20230113 19:55:33 @agent_ppo2.py:146][0m 980992 total steps have happened
[32m[20230113 19:55:33 @agent_ppo2.py:122][0m #------------------------ Iteration 479 --------------------------#
[32m[20230113 19:55:33 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:55:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:33 @agent_ppo2.py:186][0m |           0.0035 |           8.1150 |           3.9605 |
[32m[20230113 19:55:33 @agent_ppo2.py:186][0m |           0.0017 |           6.8457 |           3.9580 |
[32m[20230113 19:55:33 @agent_ppo2.py:186][0m |          -0.0035 |           6.1781 |           3.9506 |
[32m[20230113 19:55:33 @agent_ppo2.py:186][0m |          -0.0028 |           5.9987 |           3.9531 |
[32m[20230113 19:55:33 @agent_ppo2.py:186][0m |          -0.0092 |           5.6479 |           3.9529 |
[32m[20230113 19:55:33 @agent_ppo2.py:186][0m |          -0.0091 |           5.4709 |           3.9509 |
[32m[20230113 19:55:33 @agent_ppo2.py:186][0m |          -0.0129 |           5.2922 |           3.9559 |
[32m[20230113 19:55:33 @agent_ppo2.py:186][0m |          -0.0132 |           5.2227 |           3.9556 |
[32m[20230113 19:55:33 @agent_ppo2.py:186][0m |          -0.0119 |           5.0821 |           3.9552 |
[32m[20230113 19:55:33 @agent_ppo2.py:186][0m |          -0.0155 |           4.9930 |           3.9572 |
[32m[20230113 19:55:33 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:55:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.39
[32m[20230113 19:55:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.95
[32m[20230113 19:55:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.40
[32m[20230113 19:55:34 @agent_ppo2.py:144][0m Total time:      11.02 min
[32m[20230113 19:55:34 @agent_ppo2.py:146][0m 983040 total steps have happened
[32m[20230113 19:55:34 @agent_ppo2.py:122][0m #------------------------ Iteration 480 --------------------------#
[32m[20230113 19:55:34 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:34 @agent_ppo2.py:186][0m |          -0.0048 |           6.4604 |           4.0410 |
[32m[20230113 19:55:35 @agent_ppo2.py:186][0m |          -0.0073 |           5.8783 |           4.0409 |
[32m[20230113 19:55:35 @agent_ppo2.py:186][0m |          -0.0089 |           5.6562 |           4.0439 |
[32m[20230113 19:55:35 @agent_ppo2.py:186][0m |          -0.0176 |           5.5887 |           4.0424 |
[32m[20230113 19:55:35 @agent_ppo2.py:186][0m |          -0.0037 |           5.4613 |           4.0399 |
[32m[20230113 19:55:35 @agent_ppo2.py:186][0m |           0.0090 |           5.6941 |           4.0392 |
[32m[20230113 19:55:35 @agent_ppo2.py:186][0m |          -0.0031 |           5.3043 |           4.0383 |
[32m[20230113 19:55:35 @agent_ppo2.py:186][0m |          -0.0147 |           5.1956 |           4.0361 |
[32m[20230113 19:55:35 @agent_ppo2.py:186][0m |          -0.0071 |           5.1515 |           4.0331 |
[32m[20230113 19:55:35 @agent_ppo2.py:186][0m |          -0.0202 |           5.0934 |           4.0329 |
[32m[20230113 19:55:35 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:55:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.21
[32m[20230113 19:55:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.20
[32m[20230113 19:55:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.33
[32m[20230113 19:55:35 @agent_ppo2.py:144][0m Total time:      11.05 min
[32m[20230113 19:55:35 @agent_ppo2.py:146][0m 985088 total steps have happened
[32m[20230113 19:55:35 @agent_ppo2.py:122][0m #------------------------ Iteration 481 --------------------------#
[32m[20230113 19:55:36 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 19:55:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:36 @agent_ppo2.py:186][0m |           0.0014 |           9.7426 |           3.9377 |
[32m[20230113 19:55:36 @agent_ppo2.py:186][0m |          -0.0091 |           3.7492 |           3.9334 |
[32m[20230113 19:55:36 @agent_ppo2.py:186][0m |          -0.0073 |           2.7834 |           3.9311 |
[32m[20230113 19:55:36 @agent_ppo2.py:186][0m |          -0.0100 |           2.4786 |           3.9310 |
[32m[20230113 19:55:36 @agent_ppo2.py:186][0m |          -0.0126 |           2.2127 |           3.9284 |
[32m[20230113 19:55:36 @agent_ppo2.py:186][0m |          -0.0146 |           2.0371 |           3.9294 |
[32m[20230113 19:55:36 @agent_ppo2.py:186][0m |          -0.0142 |           1.8892 |           3.9308 |
[32m[20230113 19:55:36 @agent_ppo2.py:186][0m |          -0.0157 |           1.7817 |           3.9296 |
[32m[20230113 19:55:36 @agent_ppo2.py:186][0m |          -0.0086 |           1.6874 |           3.9338 |
[32m[20230113 19:55:36 @agent_ppo2.py:186][0m |          -0.0161 |           1.6683 |           3.9315 |
[32m[20230113 19:55:36 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 19:55:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 153.83
[32m[20230113 19:55:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.63
[32m[20230113 19:55:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.29
[32m[20230113 19:55:37 @agent_ppo2.py:144][0m Total time:      11.07 min
[32m[20230113 19:55:37 @agent_ppo2.py:146][0m 987136 total steps have happened
[32m[20230113 19:55:37 @agent_ppo2.py:122][0m #------------------------ Iteration 482 --------------------------#
[32m[20230113 19:55:37 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:55:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:37 @agent_ppo2.py:186][0m |           0.0131 |           8.9342 |           4.0163 |
[32m[20230113 19:55:37 @agent_ppo2.py:186][0m |          -0.0082 |           6.2948 |           4.0120 |
[32m[20230113 19:55:37 @agent_ppo2.py:186][0m |          -0.0025 |           5.9689 |           4.0084 |
[32m[20230113 19:55:37 @agent_ppo2.py:186][0m |          -0.0070 |           5.4496 |           4.0047 |
[32m[20230113 19:55:37 @agent_ppo2.py:186][0m |          -0.0072 |           5.2364 |           4.0036 |
[32m[20230113 19:55:37 @agent_ppo2.py:186][0m |          -0.0107 |           5.1358 |           4.0015 |
[32m[20230113 19:55:37 @agent_ppo2.py:186][0m |          -0.0076 |           4.8837 |           4.0019 |
[32m[20230113 19:55:38 @agent_ppo2.py:186][0m |          -0.0146 |           4.9540 |           4.0006 |
[32m[20230113 19:55:38 @agent_ppo2.py:186][0m |          -0.0114 |           4.7266 |           3.9993 |
[32m[20230113 19:55:38 @agent_ppo2.py:186][0m |          -0.0156 |           4.6361 |           3.9973 |
[32m[20230113 19:55:38 @agent_ppo2.py:131][0m Policy update time: 0.55 s
[32m[20230113 19:55:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 126.63
[32m[20230113 19:55:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.04
[32m[20230113 19:55:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.21
[32m[20230113 19:55:38 @agent_ppo2.py:144][0m Total time:      11.09 min
[32m[20230113 19:55:38 @agent_ppo2.py:146][0m 989184 total steps have happened
[32m[20230113 19:55:38 @agent_ppo2.py:122][0m #------------------------ Iteration 483 --------------------------#
[32m[20230113 19:55:39 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:55:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:39 @agent_ppo2.py:186][0m |           0.0010 |          10.4965 |           4.1267 |
[32m[20230113 19:55:39 @agent_ppo2.py:186][0m |          -0.0020 |           8.4844 |           4.1223 |
[32m[20230113 19:55:39 @agent_ppo2.py:186][0m |          -0.0033 |           7.7620 |           4.1160 |
[32m[20230113 19:55:39 @agent_ppo2.py:186][0m |          -0.0044 |           7.4060 |           4.1152 |
[32m[20230113 19:55:39 @agent_ppo2.py:186][0m |          -0.0051 |           7.1571 |           4.1154 |
[32m[20230113 19:55:39 @agent_ppo2.py:186][0m |          -0.0061 |           6.8874 |           4.1173 |
[32m[20230113 19:55:39 @agent_ppo2.py:186][0m |          -0.0066 |           6.6579 |           4.1165 |
[32m[20230113 19:55:39 @agent_ppo2.py:186][0m |          -0.0075 |           6.4872 |           4.1123 |
[32m[20230113 19:55:39 @agent_ppo2.py:186][0m |          -0.0079 |           6.4031 |           4.1145 |
[32m[20230113 19:55:39 @agent_ppo2.py:186][0m |          -0.0084 |           6.2247 |           4.1118 |
[32m[20230113 19:55:39 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 19:55:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 132.80
[32m[20230113 19:55:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.32
[32m[20230113 19:55:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.26
[32m[20230113 19:55:40 @agent_ppo2.py:144][0m Total time:      11.12 min
[32m[20230113 19:55:40 @agent_ppo2.py:146][0m 991232 total steps have happened
[32m[20230113 19:55:40 @agent_ppo2.py:122][0m #------------------------ Iteration 484 --------------------------#
[32m[20230113 19:55:40 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:40 @agent_ppo2.py:186][0m |          -0.0008 |           6.6911 |           3.9361 |
[32m[20230113 19:55:40 @agent_ppo2.py:186][0m |          -0.0079 |           6.0875 |           3.9338 |
[32m[20230113 19:55:40 @agent_ppo2.py:186][0m |          -0.0112 |           5.9311 |           3.9302 |
[32m[20230113 19:55:40 @agent_ppo2.py:186][0m |          -0.0092 |           5.7347 |           3.9262 |
[32m[20230113 19:55:40 @agent_ppo2.py:186][0m |          -0.0122 |           5.7105 |           3.9277 |
[32m[20230113 19:55:40 @agent_ppo2.py:186][0m |          -0.0117 |           5.6488 |           3.9266 |
[32m[20230113 19:55:40 @agent_ppo2.py:186][0m |          -0.0162 |           5.5348 |           3.9281 |
[32m[20230113 19:55:40 @agent_ppo2.py:186][0m |          -0.0118 |           5.4586 |           3.9276 |
[32m[20230113 19:55:40 @agent_ppo2.py:186][0m |          -0.0095 |           5.4459 |           3.9320 |
[32m[20230113 19:55:40 @agent_ppo2.py:186][0m |          -0.0143 |           5.4261 |           3.9299 |
[32m[20230113 19:55:40 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:55:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.63
[32m[20230113 19:55:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.96
[32m[20230113 19:55:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.69
[32m[20230113 19:55:41 @agent_ppo2.py:144][0m Total time:      11.14 min
[32m[20230113 19:55:41 @agent_ppo2.py:146][0m 993280 total steps have happened
[32m[20230113 19:55:41 @agent_ppo2.py:122][0m #------------------------ Iteration 485 --------------------------#
[32m[20230113 19:55:41 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:41 @agent_ppo2.py:186][0m |           0.0002 |           5.5483 |           3.9918 |
[32m[20230113 19:55:41 @agent_ppo2.py:186][0m |          -0.0052 |           5.0402 |           3.9840 |
[32m[20230113 19:55:41 @agent_ppo2.py:186][0m |          -0.0061 |           4.8827 |           3.9799 |
[32m[20230113 19:55:42 @agent_ppo2.py:186][0m |          -0.0086 |           4.7924 |           3.9795 |
[32m[20230113 19:55:42 @agent_ppo2.py:186][0m |          -0.0086 |           4.6983 |           3.9809 |
[32m[20230113 19:55:42 @agent_ppo2.py:186][0m |          -0.0089 |           4.6192 |           3.9771 |
[32m[20230113 19:55:42 @agent_ppo2.py:186][0m |          -0.0097 |           4.5485 |           3.9760 |
[32m[20230113 19:55:42 @agent_ppo2.py:186][0m |          -0.0108 |           4.5253 |           3.9780 |
[32m[20230113 19:55:42 @agent_ppo2.py:186][0m |          -0.0118 |           4.4618 |           3.9758 |
[32m[20230113 19:55:42 @agent_ppo2.py:186][0m |          -0.0114 |           4.4128 |           3.9751 |
[32m[20230113 19:55:42 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:55:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.23
[32m[20230113 19:55:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.21
[32m[20230113 19:55:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.00
[32m[20230113 19:55:42 @agent_ppo2.py:144][0m Total time:      11.16 min
[32m[20230113 19:55:42 @agent_ppo2.py:146][0m 995328 total steps have happened
[32m[20230113 19:55:42 @agent_ppo2.py:122][0m #------------------------ Iteration 486 --------------------------#
[32m[20230113 19:55:43 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:43 @agent_ppo2.py:186][0m |          -0.0027 |           5.2521 |           4.0504 |
[32m[20230113 19:55:43 @agent_ppo2.py:186][0m |          -0.0048 |           4.8451 |           4.0428 |
[32m[20230113 19:55:43 @agent_ppo2.py:186][0m |          -0.0075 |           4.6277 |           4.0463 |
[32m[20230113 19:55:43 @agent_ppo2.py:186][0m |          -0.0104 |           4.5097 |           4.0433 |
[32m[20230113 19:55:43 @agent_ppo2.py:186][0m |          -0.0087 |           4.3676 |           4.0465 |
[32m[20230113 19:55:43 @agent_ppo2.py:186][0m |          -0.0084 |           4.3393 |           4.0484 |
[32m[20230113 19:55:43 @agent_ppo2.py:186][0m |          -0.0082 |           4.2190 |           4.0469 |
[32m[20230113 19:55:43 @agent_ppo2.py:186][0m |          -0.0105 |           4.1320 |           4.0474 |
[32m[20230113 19:55:43 @agent_ppo2.py:186][0m |          -0.0084 |           4.1644 |           4.0479 |
[32m[20230113 19:55:43 @agent_ppo2.py:186][0m |          -0.0117 |           3.9704 |           4.0502 |
[32m[20230113 19:55:43 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:55:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.18
[32m[20230113 19:55:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.66
[32m[20230113 19:55:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.30
[32m[20230113 19:55:44 @agent_ppo2.py:144][0m Total time:      11.18 min
[32m[20230113 19:55:44 @agent_ppo2.py:146][0m 997376 total steps have happened
[32m[20230113 19:55:44 @agent_ppo2.py:122][0m #------------------------ Iteration 487 --------------------------#
[32m[20230113 19:55:44 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:55:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:44 @agent_ppo2.py:186][0m |           0.0002 |           5.4099 |           4.1019 |
[32m[20230113 19:55:44 @agent_ppo2.py:186][0m |          -0.0033 |           5.1054 |           4.1027 |
[32m[20230113 19:55:44 @agent_ppo2.py:186][0m |          -0.0044 |           4.8957 |           4.1034 |
[32m[20230113 19:55:44 @agent_ppo2.py:186][0m |          -0.0060 |           4.7328 |           4.1005 |
[32m[20230113 19:55:44 @agent_ppo2.py:186][0m |          -0.0068 |           4.6430 |           4.1004 |
[32m[20230113 19:55:44 @agent_ppo2.py:186][0m |          -0.0077 |           4.5703 |           4.1009 |
[32m[20230113 19:55:44 @agent_ppo2.py:186][0m |          -0.0088 |           4.4957 |           4.1010 |
[32m[20230113 19:55:44 @agent_ppo2.py:186][0m |          -0.0092 |           4.4601 |           4.1018 |
[32m[20230113 19:55:44 @agent_ppo2.py:186][0m |          -0.0095 |           4.4182 |           4.1019 |
[32m[20230113 19:55:44 @agent_ppo2.py:186][0m |          -0.0103 |           4.3532 |           4.1038 |
[32m[20230113 19:55:44 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:55:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.81
[32m[20230113 19:55:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.26
[32m[20230113 19:55:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.42
[32m[20230113 19:55:45 @agent_ppo2.py:144][0m Total time:      11.21 min
[32m[20230113 19:55:45 @agent_ppo2.py:146][0m 999424 total steps have happened
[32m[20230113 19:55:45 @agent_ppo2.py:122][0m #------------------------ Iteration 488 --------------------------#
[32m[20230113 19:55:45 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:55:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:45 @agent_ppo2.py:186][0m |           0.0017 |           5.7584 |           4.1514 |
[32m[20230113 19:55:45 @agent_ppo2.py:186][0m |          -0.0026 |           5.2432 |           4.1356 |
[32m[20230113 19:55:46 @agent_ppo2.py:186][0m |          -0.0039 |           5.0251 |           4.1361 |
[32m[20230113 19:55:46 @agent_ppo2.py:186][0m |          -0.0056 |           4.9244 |           4.1291 |
[32m[20230113 19:55:46 @agent_ppo2.py:186][0m |          -0.0065 |           4.7581 |           4.1356 |
[32m[20230113 19:55:46 @agent_ppo2.py:186][0m |          -0.0096 |           4.6241 |           4.1310 |
[32m[20230113 19:55:46 @agent_ppo2.py:186][0m |          -0.0071 |           4.5366 |           4.1263 |
[32m[20230113 19:55:46 @agent_ppo2.py:186][0m |          -0.0080 |           4.4574 |           4.1283 |
[32m[20230113 19:55:46 @agent_ppo2.py:186][0m |          -0.0092 |           4.3490 |           4.1269 |
[32m[20230113 19:55:46 @agent_ppo2.py:186][0m |          -0.0086 |           4.2858 |           4.1244 |
[32m[20230113 19:55:46 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:55:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.91
[32m[20230113 19:55:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.82
[32m[20230113 19:55:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.30
[32m[20230113 19:55:46 @agent_ppo2.py:144][0m Total time:      11.23 min
[32m[20230113 19:55:46 @agent_ppo2.py:146][0m 1001472 total steps have happened
[32m[20230113 19:55:46 @agent_ppo2.py:122][0m #------------------------ Iteration 489 --------------------------#
[32m[20230113 19:55:47 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:47 @agent_ppo2.py:186][0m |          -0.0046 |           6.8150 |           4.0474 |
[32m[20230113 19:55:47 @agent_ppo2.py:186][0m |          -0.0014 |           6.2964 |           4.0415 |
[32m[20230113 19:55:47 @agent_ppo2.py:186][0m |          -0.0099 |           6.0340 |           4.0419 |
[32m[20230113 19:55:47 @agent_ppo2.py:186][0m |          -0.0051 |           5.9008 |           4.0403 |
[32m[20230113 19:55:47 @agent_ppo2.py:186][0m |          -0.0135 |           5.7805 |           4.0358 |
[32m[20230113 19:55:47 @agent_ppo2.py:186][0m |          -0.0114 |           5.6797 |           4.0368 |
[32m[20230113 19:55:47 @agent_ppo2.py:186][0m |          -0.0078 |           5.8112 |           4.0372 |
[32m[20230113 19:55:47 @agent_ppo2.py:186][0m |          -0.0119 |           5.5421 |           4.0336 |
[32m[20230113 19:55:47 @agent_ppo2.py:186][0m |          -0.0127 |           5.4959 |           4.0355 |
[32m[20230113 19:55:47 @agent_ppo2.py:186][0m |          -0.0130 |           5.4430 |           4.0374 |
[32m[20230113 19:55:47 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:55:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.57
[32m[20230113 19:55:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.97
[32m[20230113 19:55:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 108.11
[32m[20230113 19:55:47 @agent_ppo2.py:144][0m Total time:      11.25 min
[32m[20230113 19:55:47 @agent_ppo2.py:146][0m 1003520 total steps have happened
[32m[20230113 19:55:47 @agent_ppo2.py:122][0m #------------------------ Iteration 490 --------------------------#
[32m[20230113 19:55:48 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 19:55:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:48 @agent_ppo2.py:186][0m |          -0.0040 |           9.9709 |           4.0088 |
[32m[20230113 19:55:48 @agent_ppo2.py:186][0m |          -0.0073 |           4.7238 |           4.0073 |
[32m[20230113 19:55:48 @agent_ppo2.py:186][0m |          -0.0024 |           4.2844 |           4.0094 |
[32m[20230113 19:55:48 @agent_ppo2.py:186][0m |          -0.0097 |           3.9603 |           4.0066 |
[32m[20230113 19:55:48 @agent_ppo2.py:186][0m |          -0.0056 |           3.8573 |           4.0058 |
[32m[20230113 19:55:48 @agent_ppo2.py:186][0m |          -0.0086 |           3.9127 |           4.0054 |
[32m[20230113 19:55:48 @agent_ppo2.py:186][0m |          -0.0113 |           3.5690 |           4.0025 |
[32m[20230113 19:55:48 @agent_ppo2.py:186][0m |          -0.0125 |           3.4805 |           4.0001 |
[32m[20230113 19:55:48 @agent_ppo2.py:186][0m |          -0.0115 |           3.4406 |           3.9996 |
[32m[20230113 19:55:48 @agent_ppo2.py:186][0m |          -0.0113 |           3.4080 |           4.0006 |
[32m[20230113 19:55:48 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 19:55:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 159.81
[32m[20230113 19:55:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.89
[32m[20230113 19:55:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.73
[32m[20230113 19:55:49 @agent_ppo2.py:144][0m Total time:      11.27 min
[32m[20230113 19:55:49 @agent_ppo2.py:146][0m 1005568 total steps have happened
[32m[20230113 19:55:49 @agent_ppo2.py:122][0m #------------------------ Iteration 491 --------------------------#
[32m[20230113 19:55:49 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:55:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:49 @agent_ppo2.py:186][0m |           0.0000 |          14.8198 |           4.0370 |
[32m[20230113 19:55:49 @agent_ppo2.py:186][0m |          -0.0042 |           9.7287 |           4.0328 |
[32m[20230113 19:55:49 @agent_ppo2.py:186][0m |          -0.0070 |           8.5074 |           4.0272 |
[32m[20230113 19:55:50 @agent_ppo2.py:186][0m |          -0.0089 |           7.8466 |           4.0237 |
[32m[20230113 19:55:50 @agent_ppo2.py:186][0m |          -0.0094 |           7.3179 |           4.0226 |
[32m[20230113 19:55:50 @agent_ppo2.py:186][0m |          -0.0114 |           6.9009 |           4.0193 |
[32m[20230113 19:55:50 @agent_ppo2.py:186][0m |          -0.0119 |           6.5430 |           4.0222 |
[32m[20230113 19:55:50 @agent_ppo2.py:186][0m |          -0.0126 |           6.3255 |           4.0178 |
[32m[20230113 19:55:50 @agent_ppo2.py:186][0m |          -0.0117 |           6.1887 |           4.0169 |
[32m[20230113 19:55:50 @agent_ppo2.py:186][0m |          -0.0130 |           5.9543 |           4.0208 |
[32m[20230113 19:55:50 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 19:55:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 131.10
[32m[20230113 19:55:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.05
[32m[20230113 19:55:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.63
[32m[20230113 19:55:50 @agent_ppo2.py:144][0m Total time:      11.29 min
[32m[20230113 19:55:50 @agent_ppo2.py:146][0m 1007616 total steps have happened
[32m[20230113 19:55:50 @agent_ppo2.py:122][0m #------------------------ Iteration 492 --------------------------#
[32m[20230113 19:55:51 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 19:55:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:51 @agent_ppo2.py:186][0m |           0.0011 |          18.1059 |           4.0283 |
[32m[20230113 19:55:51 @agent_ppo2.py:186][0m |          -0.0038 |          12.0497 |           4.0217 |
[32m[20230113 19:55:51 @agent_ppo2.py:186][0m |          -0.0058 |          10.0091 |           4.0192 |
[32m[20230113 19:55:51 @agent_ppo2.py:186][0m |          -0.0073 |           8.6561 |           4.0177 |
[32m[20230113 19:55:51 @agent_ppo2.py:186][0m |          -0.0081 |           7.7919 |           4.0203 |
[32m[20230113 19:55:51 @agent_ppo2.py:186][0m |          -0.0079 |           7.3674 |           4.0206 |
[32m[20230113 19:55:51 @agent_ppo2.py:186][0m |          -0.0106 |           6.9858 |           4.0237 |
[32m[20230113 19:55:51 @agent_ppo2.py:186][0m |          -0.0106 |           6.7486 |           4.0231 |
[32m[20230113 19:55:51 @agent_ppo2.py:186][0m |          -0.0109 |           6.5291 |           4.0264 |
[32m[20230113 19:55:51 @agent_ppo2.py:186][0m |          -0.0116 |           6.3551 |           4.0297 |
[32m[20230113 19:55:51 @agent_ppo2.py:131][0m Policy update time: 0.34 s
[32m[20230113 19:55:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 94.07
[32m[20230113 19:55:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 213.11
[32m[20230113 19:55:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.93
[32m[20230113 19:55:51 @agent_ppo2.py:144][0m Total time:      11.31 min
[32m[20230113 19:55:51 @agent_ppo2.py:146][0m 1009664 total steps have happened
[32m[20230113 19:55:51 @agent_ppo2.py:122][0m #------------------------ Iteration 493 --------------------------#
[32m[20230113 19:55:52 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:52 @agent_ppo2.py:186][0m |          -0.0004 |           7.4759 |           4.0924 |
[32m[20230113 19:55:52 @agent_ppo2.py:186][0m |          -0.0056 |           6.3924 |           4.0908 |
[32m[20230113 19:55:52 @agent_ppo2.py:186][0m |          -0.0086 |           6.1017 |           4.0905 |
[32m[20230113 19:55:52 @agent_ppo2.py:186][0m |          -0.0076 |           5.8423 |           4.0885 |
[32m[20230113 19:55:52 @agent_ppo2.py:186][0m |          -0.0077 |           5.7052 |           4.0859 |
[32m[20230113 19:55:52 @agent_ppo2.py:186][0m |          -0.0091 |           5.6040 |           4.0877 |
[32m[20230113 19:55:52 @agent_ppo2.py:186][0m |          -0.0104 |           5.5463 |           4.0850 |
[32m[20230113 19:55:52 @agent_ppo2.py:186][0m |          -0.0108 |           5.4289 |           4.0845 |
[32m[20230113 19:55:52 @agent_ppo2.py:186][0m |          -0.0090 |           5.3332 |           4.0830 |
[32m[20230113 19:55:52 @agent_ppo2.py:186][0m |          -0.0113 |           5.2191 |           4.0876 |
[32m[20230113 19:55:52 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:55:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.39
[32m[20230113 19:55:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.66
[32m[20230113 19:55:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 150.53
[32m[20230113 19:55:53 @agent_ppo2.py:144][0m Total time:      11.34 min
[32m[20230113 19:55:53 @agent_ppo2.py:146][0m 1011712 total steps have happened
[32m[20230113 19:55:53 @agent_ppo2.py:122][0m #------------------------ Iteration 494 --------------------------#
[32m[20230113 19:55:53 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 19:55:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:53 @agent_ppo2.py:186][0m |           0.0000 |          24.0026 |           4.1278 |
[32m[20230113 19:55:53 @agent_ppo2.py:186][0m |          -0.0052 |          12.7016 |           4.1243 |
[32m[20230113 19:55:53 @agent_ppo2.py:186][0m |          -0.0061 |          10.5094 |           4.1248 |
[32m[20230113 19:55:53 @agent_ppo2.py:186][0m |          -0.0080 |           9.3838 |           4.1225 |
[32m[20230113 19:55:53 @agent_ppo2.py:186][0m |          -0.0089 |           8.7287 |           4.1203 |
[32m[20230113 19:55:54 @agent_ppo2.py:186][0m |          -0.0105 |           8.4460 |           4.1206 |
[32m[20230113 19:55:54 @agent_ppo2.py:186][0m |          -0.0099 |           8.0649 |           4.1185 |
[32m[20230113 19:55:54 @agent_ppo2.py:186][0m |          -0.0100 |           7.9376 |           4.1149 |
[32m[20230113 19:55:54 @agent_ppo2.py:186][0m |          -0.0120 |           7.5839 |           4.1148 |
[32m[20230113 19:55:54 @agent_ppo2.py:186][0m |          -0.0119 |           7.4556 |           4.1154 |
[32m[20230113 19:55:54 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 19:55:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 143.71
[32m[20230113 19:55:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.03
[32m[20230113 19:55:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.37
[32m[20230113 19:55:54 @agent_ppo2.py:144][0m Total time:      11.36 min
[32m[20230113 19:55:54 @agent_ppo2.py:146][0m 1013760 total steps have happened
[32m[20230113 19:55:54 @agent_ppo2.py:122][0m #------------------------ Iteration 495 --------------------------#
[32m[20230113 19:55:55 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:55:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:55 @agent_ppo2.py:186][0m |          -0.0008 |           7.4826 |           3.9333 |
[32m[20230113 19:55:55 @agent_ppo2.py:186][0m |          -0.0054 |           5.9904 |           3.9341 |
[32m[20230113 19:55:55 @agent_ppo2.py:186][0m |          -0.0083 |           5.6682 |           3.9309 |
[32m[20230113 19:55:55 @agent_ppo2.py:186][0m |          -0.0085 |           5.5061 |           3.9298 |
[32m[20230113 19:55:55 @agent_ppo2.py:186][0m |          -0.0104 |           5.3937 |           3.9310 |
[32m[20230113 19:55:55 @agent_ppo2.py:186][0m |          -0.0108 |           5.3184 |           3.9294 |
[32m[20230113 19:55:55 @agent_ppo2.py:186][0m |          -0.0119 |           5.2155 |           3.9298 |
[32m[20230113 19:55:55 @agent_ppo2.py:186][0m |          -0.0126 |           5.1341 |           3.9293 |
[32m[20230113 19:55:55 @agent_ppo2.py:186][0m |          -0.0133 |           5.1097 |           3.9306 |
[32m[20230113 19:55:55 @agent_ppo2.py:186][0m |          -0.0132 |           5.0753 |           3.9325 |
[32m[20230113 19:55:55 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:55:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.47
[32m[20230113 19:55:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.21
[32m[20230113 19:55:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 122.16
[32m[20230113 19:55:56 @agent_ppo2.py:144][0m Total time:      11.38 min
[32m[20230113 19:55:56 @agent_ppo2.py:146][0m 1015808 total steps have happened
[32m[20230113 19:55:56 @agent_ppo2.py:122][0m #------------------------ Iteration 496 --------------------------#
[32m[20230113 19:55:56 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:55:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:56 @agent_ppo2.py:186][0m |           0.0012 |           5.9733 |           4.1182 |
[32m[20230113 19:55:56 @agent_ppo2.py:186][0m |          -0.0019 |           5.5479 |           4.1058 |
[32m[20230113 19:55:56 @agent_ppo2.py:186][0m |          -0.0035 |           5.3250 |           4.1113 |
[32m[20230113 19:55:56 @agent_ppo2.py:186][0m |          -0.0044 |           5.1726 |           4.1057 |
[32m[20230113 19:55:56 @agent_ppo2.py:186][0m |          -0.0053 |           5.1063 |           4.1094 |
[32m[20230113 19:55:56 @agent_ppo2.py:186][0m |          -0.0061 |           4.9884 |           4.1042 |
[32m[20230113 19:55:56 @agent_ppo2.py:186][0m |          -0.0058 |           4.9126 |           4.1073 |
[32m[20230113 19:55:56 @agent_ppo2.py:186][0m |          -0.0074 |           4.8245 |           4.1036 |
[32m[20230113 19:55:56 @agent_ppo2.py:186][0m |          -0.0070 |           4.7702 |           4.1099 |
[32m[20230113 19:55:56 @agent_ppo2.py:186][0m |          -0.0076 |           4.7028 |           4.1014 |
[32m[20230113 19:55:56 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:55:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.72
[32m[20230113 19:55:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.26
[32m[20230113 19:55:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.16
[32m[20230113 19:55:57 @agent_ppo2.py:144][0m Total time:      11.41 min
[32m[20230113 19:55:57 @agent_ppo2.py:146][0m 1017856 total steps have happened
[32m[20230113 19:55:57 @agent_ppo2.py:122][0m #------------------------ Iteration 497 --------------------------#
[32m[20230113 19:55:57 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:55:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:57 @agent_ppo2.py:186][0m |           0.0009 |          10.5477 |           4.1641 |
[32m[20230113 19:55:57 @agent_ppo2.py:186][0m |          -0.0036 |           7.0966 |           4.1668 |
[32m[20230113 19:55:58 @agent_ppo2.py:186][0m |          -0.0037 |           6.4530 |           4.1625 |
[32m[20230113 19:55:58 @agent_ppo2.py:186][0m |          -0.0054 |           6.0020 |           4.1587 |
[32m[20230113 19:55:58 @agent_ppo2.py:186][0m |          -0.0053 |           5.8500 |           4.1600 |
[32m[20230113 19:55:58 @agent_ppo2.py:186][0m |          -0.0059 |           5.5520 |           4.1592 |
[32m[20230113 19:55:58 @agent_ppo2.py:186][0m |          -0.0037 |           5.5107 |           4.1582 |
[32m[20230113 19:55:58 @agent_ppo2.py:186][0m |          -0.0084 |           5.3020 |           4.1615 |
[32m[20230113 19:55:58 @agent_ppo2.py:186][0m |          -0.0089 |           5.1433 |           4.1587 |
[32m[20230113 19:55:58 @agent_ppo2.py:186][0m |          -0.0095 |           5.0495 |           4.1600 |
[32m[20230113 19:55:58 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:55:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 110.00
[32m[20230113 19:55:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.28
[32m[20230113 19:55:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.71
[32m[20230113 19:55:58 @agent_ppo2.py:144][0m Total time:      11.43 min
[32m[20230113 19:55:58 @agent_ppo2.py:146][0m 1019904 total steps have happened
[32m[20230113 19:55:58 @agent_ppo2.py:122][0m #------------------------ Iteration 498 --------------------------#
[32m[20230113 19:55:59 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:55:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:55:59 @agent_ppo2.py:186][0m |          -0.0009 |           5.0961 |           4.1502 |
[32m[20230113 19:55:59 @agent_ppo2.py:186][0m |          -0.0046 |           4.7379 |           4.1426 |
[32m[20230113 19:55:59 @agent_ppo2.py:186][0m |          -0.0082 |           4.5056 |           4.1403 |
[32m[20230113 19:55:59 @agent_ppo2.py:186][0m |          -0.0097 |           4.3574 |           4.1393 |
[32m[20230113 19:55:59 @agent_ppo2.py:186][0m |          -0.0084 |           4.3029 |           4.1407 |
[32m[20230113 19:55:59 @agent_ppo2.py:186][0m |          -0.0113 |           4.1580 |           4.1424 |
[32m[20230113 19:55:59 @agent_ppo2.py:186][0m |          -0.0111 |           4.0898 |           4.1426 |
[32m[20230113 19:55:59 @agent_ppo2.py:186][0m |          -0.0115 |           4.0173 |           4.1455 |
[32m[20230113 19:55:59 @agent_ppo2.py:186][0m |          -0.0120 |           3.9477 |           4.1399 |
[32m[20230113 19:55:59 @agent_ppo2.py:186][0m |          -0.0120 |           3.9097 |           4.1453 |
[32m[20230113 19:55:59 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.24
[32m[20230113 19:56:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.28
[32m[20230113 19:56:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.29
[32m[20230113 19:56:00 @agent_ppo2.py:144][0m Total time:      11.45 min
[32m[20230113 19:56:00 @agent_ppo2.py:146][0m 1021952 total steps have happened
[32m[20230113 19:56:00 @agent_ppo2.py:122][0m #------------------------ Iteration 499 --------------------------#
[32m[20230113 19:56:00 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:00 @agent_ppo2.py:186][0m |           0.0023 |           5.7324 |           4.1193 |
[32m[20230113 19:56:00 @agent_ppo2.py:186][0m |          -0.0030 |           5.2114 |           4.1090 |
[32m[20230113 19:56:00 @agent_ppo2.py:186][0m |          -0.0048 |           4.9838 |           4.1130 |
[32m[20230113 19:56:00 @agent_ppo2.py:186][0m |           0.0040 |           5.1561 |           4.1097 |
[32m[20230113 19:56:00 @agent_ppo2.py:186][0m |          -0.0015 |           4.7250 |           4.1042 |
[32m[20230113 19:56:00 @agent_ppo2.py:186][0m |          -0.0083 |           4.5909 |           4.1059 |
[32m[20230113 19:56:00 @agent_ppo2.py:186][0m |          -0.0012 |           5.0945 |           4.1084 |
[32m[20230113 19:56:00 @agent_ppo2.py:186][0m |          -0.0054 |           4.4951 |           4.1044 |
[32m[20230113 19:56:01 @agent_ppo2.py:186][0m |          -0.0014 |           4.4882 |           4.1050 |
[32m[20230113 19:56:01 @agent_ppo2.py:186][0m |          -0.0071 |           4.3495 |           4.1039 |
[32m[20230113 19:56:01 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.48
[32m[20230113 19:56:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.97
[32m[20230113 19:56:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.16
[32m[20230113 19:56:01 @agent_ppo2.py:104][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 258.49
[32m[20230113 19:56:01 @agent_ppo2.py:144][0m Total time:      11.47 min
[32m[20230113 19:56:01 @agent_ppo2.py:146][0m 1024000 total steps have happened
[32m[20230113 19:56:01 @agent_ppo2.py:122][0m #------------------------ Iteration 500 --------------------------#
[32m[20230113 19:56:01 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:02 @agent_ppo2.py:186][0m |          -0.0032 |           3.3740 |           4.1158 |
[32m[20230113 19:56:02 @agent_ppo2.py:186][0m |          -0.0004 |           2.9926 |           4.1127 |
[32m[20230113 19:56:02 @agent_ppo2.py:186][0m |          -0.0064 |           2.7867 |           4.1095 |
[32m[20230113 19:56:02 @agent_ppo2.py:186][0m |          -0.0074 |           2.6765 |           4.1037 |
[32m[20230113 19:56:02 @agent_ppo2.py:186][0m |          -0.0080 |           2.5917 |           4.1032 |
[32m[20230113 19:56:02 @agent_ppo2.py:186][0m |          -0.0052 |           2.5746 |           4.1022 |
[32m[20230113 19:56:02 @agent_ppo2.py:186][0m |          -0.0086 |           2.4927 |           4.1011 |
[32m[20230113 19:56:02 @agent_ppo2.py:186][0m |          -0.0102 |           2.4568 |           4.0983 |
[32m[20230113 19:56:02 @agent_ppo2.py:186][0m |          -0.0088 |           2.4087 |           4.0981 |
[32m[20230113 19:56:02 @agent_ppo2.py:186][0m |          -0.0078 |           2.3889 |           4.0961 |
[32m[20230113 19:56:02 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:56:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.07
[32m[20230113 19:56:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.87
[32m[20230113 19:56:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.52
[32m[20230113 19:56:02 @agent_ppo2.py:144][0m Total time:      11.50 min
[32m[20230113 19:56:02 @agent_ppo2.py:146][0m 1026048 total steps have happened
[32m[20230113 19:56:02 @agent_ppo2.py:122][0m #------------------------ Iteration 501 --------------------------#
[32m[20230113 19:56:03 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:56:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:03 @agent_ppo2.py:186][0m |          -0.0011 |           6.6290 |           3.9854 |
[32m[20230113 19:56:03 @agent_ppo2.py:186][0m |           0.0010 |           6.5024 |           3.9835 |
[32m[20230113 19:56:03 @agent_ppo2.py:186][0m |          -0.0037 |           6.1690 |           3.9836 |
[32m[20230113 19:56:03 @agent_ppo2.py:186][0m |          -0.0040 |           6.0242 |           3.9836 |
[32m[20230113 19:56:03 @agent_ppo2.py:186][0m |          -0.0070 |           5.8315 |           3.9794 |
[32m[20230113 19:56:03 @agent_ppo2.py:186][0m |          -0.0076 |           5.7102 |           3.9790 |
[32m[20230113 19:56:03 @agent_ppo2.py:186][0m |          -0.0070 |           5.7495 |           3.9777 |
[32m[20230113 19:56:03 @agent_ppo2.py:186][0m |          -0.0117 |           5.5565 |           3.9757 |
[32m[20230113 19:56:03 @agent_ppo2.py:186][0m |          -0.0130 |           5.5320 |           3.9758 |
[32m[20230113 19:56:03 @agent_ppo2.py:186][0m |          -0.0122 |           5.4692 |           3.9735 |
[32m[20230113 19:56:03 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:56:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.13
[32m[20230113 19:56:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.14
[32m[20230113 19:56:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.81
[32m[20230113 19:56:04 @agent_ppo2.py:144][0m Total time:      11.52 min
[32m[20230113 19:56:04 @agent_ppo2.py:146][0m 1028096 total steps have happened
[32m[20230113 19:56:04 @agent_ppo2.py:122][0m #------------------------ Iteration 502 --------------------------#
[32m[20230113 19:56:04 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:56:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:04 @agent_ppo2.py:186][0m |           0.0002 |           6.0532 |           4.0226 |
[32m[20230113 19:56:04 @agent_ppo2.py:186][0m |          -0.0099 |           5.7033 |           4.0040 |
[32m[20230113 19:56:04 @agent_ppo2.py:186][0m |          -0.0095 |           5.5534 |           4.0123 |
[32m[20230113 19:56:04 @agent_ppo2.py:186][0m |          -0.0060 |           5.4676 |           4.0086 |
[32m[20230113 19:56:04 @agent_ppo2.py:186][0m |          -0.0098 |           5.3966 |           4.0089 |
[32m[20230113 19:56:04 @agent_ppo2.py:186][0m |          -0.0093 |           5.3058 |           4.0074 |
[32m[20230113 19:56:04 @agent_ppo2.py:186][0m |          -0.0080 |           5.4140 |           4.0066 |
[32m[20230113 19:56:05 @agent_ppo2.py:186][0m |          -0.0151 |           5.1878 |           4.0047 |
[32m[20230113 19:56:05 @agent_ppo2.py:186][0m |          -0.0138 |           5.1343 |           4.0028 |
[32m[20230113 19:56:05 @agent_ppo2.py:186][0m |          -0.0138 |           5.1000 |           3.9989 |
[32m[20230113 19:56:05 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:56:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.54
[32m[20230113 19:56:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.03
[32m[20230113 19:56:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.45
[32m[20230113 19:56:05 @agent_ppo2.py:144][0m Total time:      11.54 min
[32m[20230113 19:56:05 @agent_ppo2.py:146][0m 1030144 total steps have happened
[32m[20230113 19:56:05 @agent_ppo2.py:122][0m #------------------------ Iteration 503 --------------------------#
[32m[20230113 19:56:05 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:06 @agent_ppo2.py:186][0m |          -0.0021 |           6.0859 |           4.1919 |
[32m[20230113 19:56:06 @agent_ppo2.py:186][0m |          -0.0053 |           5.5730 |           4.1852 |
[32m[20230113 19:56:06 @agent_ppo2.py:186][0m |          -0.0081 |           5.3567 |           4.1890 |
[32m[20230113 19:56:06 @agent_ppo2.py:186][0m |          -0.0096 |           5.1705 |           4.1844 |
[32m[20230113 19:56:06 @agent_ppo2.py:186][0m |          -0.0096 |           5.0324 |           4.1859 |
[32m[20230113 19:56:06 @agent_ppo2.py:186][0m |          -0.0117 |           4.8327 |           4.1843 |
[32m[20230113 19:56:06 @agent_ppo2.py:186][0m |          -0.0113 |           4.7149 |           4.1872 |
[32m[20230113 19:56:06 @agent_ppo2.py:186][0m |          -0.0124 |           4.6379 |           4.1869 |
[32m[20230113 19:56:06 @agent_ppo2.py:186][0m |          -0.0103 |           4.5993 |           4.1861 |
[32m[20230113 19:56:06 @agent_ppo2.py:186][0m |          -0.0130 |           4.5070 |           4.1869 |
[32m[20230113 19:56:06 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.14
[32m[20230113 19:56:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.52
[32m[20230113 19:56:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.22
[32m[20230113 19:56:06 @agent_ppo2.py:144][0m Total time:      11.56 min
[32m[20230113 19:56:06 @agent_ppo2.py:146][0m 1032192 total steps have happened
[32m[20230113 19:56:06 @agent_ppo2.py:122][0m #------------------------ Iteration 504 --------------------------#
[32m[20230113 19:56:07 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:07 @agent_ppo2.py:186][0m |          -0.0020 |           5.8334 |           4.0930 |
[32m[20230113 19:56:07 @agent_ppo2.py:186][0m |          -0.0055 |           5.3372 |           4.0862 |
[32m[20230113 19:56:07 @agent_ppo2.py:186][0m |          -0.0060 |           5.1668 |           4.0819 |
[32m[20230113 19:56:07 @agent_ppo2.py:186][0m |          -0.0056 |           5.0981 |           4.0815 |
[32m[20230113 19:56:07 @agent_ppo2.py:186][0m |          -0.0005 |           5.2044 |           4.0810 |
[32m[20230113 19:56:07 @agent_ppo2.py:186][0m |          -0.0076 |           4.9294 |           4.0756 |
[32m[20230113 19:56:07 @agent_ppo2.py:186][0m |          -0.0115 |           4.8738 |           4.0782 |
[32m[20230113 19:56:07 @agent_ppo2.py:186][0m |          -0.0091 |           4.8338 |           4.0760 |
[32m[20230113 19:56:07 @agent_ppo2.py:186][0m |          -0.0003 |           4.9752 |           4.0780 |
[32m[20230113 19:56:07 @agent_ppo2.py:186][0m |          -0.0053 |           4.8431 |           4.0782 |
[32m[20230113 19:56:07 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:56:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.91
[32m[20230113 19:56:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.41
[32m[20230113 19:56:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.42
[32m[20230113 19:56:08 @agent_ppo2.py:144][0m Total time:      11.59 min
[32m[20230113 19:56:08 @agent_ppo2.py:146][0m 1034240 total steps have happened
[32m[20230113 19:56:08 @agent_ppo2.py:122][0m #------------------------ Iteration 505 --------------------------#
[32m[20230113 19:56:08 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:56:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:08 @agent_ppo2.py:186][0m |           0.0020 |           6.3803 |           4.0619 |
[32m[20230113 19:56:08 @agent_ppo2.py:186][0m |          -0.0014 |           5.9177 |           4.0611 |
[32m[20230113 19:56:08 @agent_ppo2.py:186][0m |          -0.0053 |           5.6149 |           4.0598 |
[32m[20230113 19:56:08 @agent_ppo2.py:186][0m |          -0.0074 |           5.3351 |           4.0597 |
[32m[20230113 19:56:08 @agent_ppo2.py:186][0m |          -0.0079 |           5.1181 |           4.0577 |
[32m[20230113 19:56:09 @agent_ppo2.py:186][0m |          -0.0104 |           4.9200 |           4.0603 |
[32m[20230113 19:56:09 @agent_ppo2.py:186][0m |          -0.0092 |           4.7877 |           4.0609 |
[32m[20230113 19:56:09 @agent_ppo2.py:186][0m |          -0.0097 |           4.6732 |           4.0581 |
[32m[20230113 19:56:09 @agent_ppo2.py:186][0m |          -0.0107 |           4.5148 |           4.0597 |
[32m[20230113 19:56:09 @agent_ppo2.py:186][0m |          -0.0113 |           4.4350 |           4.0623 |
[32m[20230113 19:56:09 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:56:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.04
[32m[20230113 19:56:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.56
[32m[20230113 19:56:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.18
[32m[20230113 19:56:09 @agent_ppo2.py:144][0m Total time:      11.61 min
[32m[20230113 19:56:09 @agent_ppo2.py:146][0m 1036288 total steps have happened
[32m[20230113 19:56:09 @agent_ppo2.py:122][0m #------------------------ Iteration 506 --------------------------#
[32m[20230113 19:56:10 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:56:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:10 @agent_ppo2.py:186][0m |          -0.0030 |           5.4245 |           4.1014 |
[32m[20230113 19:56:10 @agent_ppo2.py:186][0m |          -0.0011 |           4.8458 |           4.0924 |
[32m[20230113 19:56:10 @agent_ppo2.py:186][0m |          -0.0107 |           4.6143 |           4.0835 |
[32m[20230113 19:56:10 @agent_ppo2.py:186][0m |          -0.0044 |           4.6040 |           4.0868 |
[32m[20230113 19:56:10 @agent_ppo2.py:186][0m |          -0.0072 |           4.4290 |           4.0786 |
[32m[20230113 19:56:10 @agent_ppo2.py:186][0m |          -0.0105 |           4.3259 |           4.0773 |
[32m[20230113 19:56:10 @agent_ppo2.py:186][0m |          -0.0093 |           4.3054 |           4.0811 |
[32m[20230113 19:56:10 @agent_ppo2.py:186][0m |          -0.0133 |           4.2109 |           4.0767 |
[32m[20230113 19:56:10 @agent_ppo2.py:186][0m |          -0.0138 |           4.1903 |           4.0771 |
[32m[20230113 19:56:10 @agent_ppo2.py:186][0m |          -0.0171 |           4.1445 |           4.0711 |
[32m[20230113 19:56:10 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:56:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.12
[32m[20230113 19:56:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.22
[32m[20230113 19:56:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.03
[32m[20230113 19:56:11 @agent_ppo2.py:144][0m Total time:      11.63 min
[32m[20230113 19:56:11 @agent_ppo2.py:146][0m 1038336 total steps have happened
[32m[20230113 19:56:11 @agent_ppo2.py:122][0m #------------------------ Iteration 507 --------------------------#
[32m[20230113 19:56:11 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 19:56:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:11 @agent_ppo2.py:186][0m |           0.0010 |          26.4155 |           4.0449 |
[32m[20230113 19:56:11 @agent_ppo2.py:186][0m |          -0.0101 |           9.2571 |           4.0437 |
[32m[20230113 19:56:11 @agent_ppo2.py:186][0m |          -0.0136 |           7.4899 |           4.0387 |
[32m[20230113 19:56:11 @agent_ppo2.py:186][0m |          -0.0163 |           6.5479 |           4.0395 |
[32m[20230113 19:56:11 @agent_ppo2.py:186][0m |          -0.0165 |           6.2118 |           4.0368 |
[32m[20230113 19:56:11 @agent_ppo2.py:186][0m |          -0.0141 |           5.9611 |           4.0384 |
[32m[20230113 19:56:11 @agent_ppo2.py:186][0m |          -0.0160 |           5.8790 |           4.0378 |
[32m[20230113 19:56:11 @agent_ppo2.py:186][0m |          -0.0188 |           5.5507 |           4.0349 |
[32m[20230113 19:56:11 @agent_ppo2.py:186][0m |          -0.0192 |           5.3762 |           4.0334 |
[32m[20230113 19:56:11 @agent_ppo2.py:186][0m |          -0.0175 |           5.3739 |           4.0373 |
[32m[20230113 19:56:11 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 19:56:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 99.29
[32m[20230113 19:56:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.62
[32m[20230113 19:56:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.62
[32m[20230113 19:56:12 @agent_ppo2.py:144][0m Total time:      11.65 min
[32m[20230113 19:56:12 @agent_ppo2.py:146][0m 1040384 total steps have happened
[32m[20230113 19:56:12 @agent_ppo2.py:122][0m #------------------------ Iteration 508 --------------------------#
[32m[20230113 19:56:12 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 19:56:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:12 @agent_ppo2.py:186][0m |           0.0023 |          16.6073 |           4.2437 |
[32m[20230113 19:56:12 @agent_ppo2.py:186][0m |          -0.0012 |           6.7059 |           4.2402 |
[32m[20230113 19:56:12 @agent_ppo2.py:186][0m |          -0.0037 |           5.7088 |           4.2420 |
[32m[20230113 19:56:12 @agent_ppo2.py:186][0m |          -0.0029 |           5.4174 |           4.2392 |
[32m[20230113 19:56:12 @agent_ppo2.py:186][0m |          -0.0064 |           5.1669 |           4.2403 |
[32m[20230113 19:56:12 @agent_ppo2.py:186][0m |          -0.0072 |           4.9185 |           4.2412 |
[32m[20230113 19:56:12 @agent_ppo2.py:186][0m |          -0.0057 |           4.8603 |           4.2411 |
[32m[20230113 19:56:12 @agent_ppo2.py:186][0m |          -0.0069 |           4.6593 |           4.2405 |
[32m[20230113 19:56:12 @agent_ppo2.py:186][0m |          -0.0082 |           4.5337 |           4.2414 |
[32m[20230113 19:56:12 @agent_ppo2.py:186][0m |          -0.0077 |           4.5336 |           4.2403 |
[32m[20230113 19:56:12 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 19:56:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 123.84
[32m[20230113 19:56:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.01
[32m[20230113 19:56:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.25
[32m[20230113 19:56:13 @agent_ppo2.py:144][0m Total time:      11.67 min
[32m[20230113 19:56:13 @agent_ppo2.py:146][0m 1042432 total steps have happened
[32m[20230113 19:56:13 @agent_ppo2.py:122][0m #------------------------ Iteration 509 --------------------------#
[32m[20230113 19:56:13 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:56:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:13 @agent_ppo2.py:186][0m |           0.0010 |           7.3593 |           4.1700 |
[32m[20230113 19:56:13 @agent_ppo2.py:186][0m |          -0.0034 |           6.3171 |           4.1681 |
[32m[20230113 19:56:13 @agent_ppo2.py:186][0m |          -0.0052 |           5.9778 |           4.1676 |
[32m[20230113 19:56:13 @agent_ppo2.py:186][0m |          -0.0072 |           5.8167 |           4.1656 |
[32m[20230113 19:56:14 @agent_ppo2.py:186][0m |          -0.0087 |           5.6670 |           4.1638 |
[32m[20230113 19:56:14 @agent_ppo2.py:186][0m |          -0.0102 |           5.5365 |           4.1632 |
[32m[20230113 19:56:14 @agent_ppo2.py:186][0m |          -0.0106 |           5.4271 |           4.1643 |
[32m[20230113 19:56:14 @agent_ppo2.py:186][0m |          -0.0094 |           5.4282 |           4.1635 |
[32m[20230113 19:56:14 @agent_ppo2.py:186][0m |          -0.0114 |           5.2522 |           4.1632 |
[32m[20230113 19:56:14 @agent_ppo2.py:186][0m |          -0.0116 |           5.2099 |           4.1655 |
[32m[20230113 19:56:14 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:56:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.33
[32m[20230113 19:56:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.34
[32m[20230113 19:56:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.54
[32m[20230113 19:56:14 @agent_ppo2.py:144][0m Total time:      11.69 min
[32m[20230113 19:56:14 @agent_ppo2.py:146][0m 1044480 total steps have happened
[32m[20230113 19:56:14 @agent_ppo2.py:122][0m #------------------------ Iteration 510 --------------------------#
[32m[20230113 19:56:15 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 19:56:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:15 @agent_ppo2.py:186][0m |          -0.0011 |          23.0117 |           4.0178 |
[32m[20230113 19:56:15 @agent_ppo2.py:186][0m |          -0.0043 |          15.3051 |           4.0093 |
[32m[20230113 19:56:15 @agent_ppo2.py:186][0m |          -0.0099 |          11.6935 |           4.0072 |
[32m[20230113 19:56:15 @agent_ppo2.py:186][0m |          -0.0083 |           9.7643 |           4.0048 |
[32m[20230113 19:56:15 @agent_ppo2.py:186][0m |          -0.0108 |           8.3472 |           4.0027 |
[32m[20230113 19:56:15 @agent_ppo2.py:186][0m |          -0.0118 |           7.4470 |           4.0023 |
[32m[20230113 19:56:15 @agent_ppo2.py:186][0m |          -0.0109 |           6.9858 |           3.9990 |
[32m[20230113 19:56:15 @agent_ppo2.py:186][0m |          -0.0124 |           6.5980 |           4.0019 |
[32m[20230113 19:56:15 @agent_ppo2.py:186][0m |          -0.0113 |           6.1650 |           4.0022 |
[32m[20230113 19:56:15 @agent_ppo2.py:186][0m |          -0.0138 |           6.0065 |           4.0014 |
[32m[20230113 19:56:15 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 19:56:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 135.68
[32m[20230113 19:56:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.96
[32m[20230113 19:56:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.34
[32m[20230113 19:56:16 @agent_ppo2.py:144][0m Total time:      11.72 min
[32m[20230113 19:56:16 @agent_ppo2.py:146][0m 1046528 total steps have happened
[32m[20230113 19:56:16 @agent_ppo2.py:122][0m #------------------------ Iteration 511 --------------------------#
[32m[20230113 19:56:16 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 19:56:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:16 @agent_ppo2.py:186][0m |          -0.0007 |          36.0969 |           4.1932 |
[32m[20230113 19:56:16 @agent_ppo2.py:186][0m |          -0.0056 |          17.2658 |           4.1890 |
[32m[20230113 19:56:16 @agent_ppo2.py:186][0m |          -0.0073 |          13.6395 |           4.1856 |
[32m[20230113 19:56:16 @agent_ppo2.py:186][0m |          -0.0095 |          11.9061 |           4.1825 |
[32m[20230113 19:56:16 @agent_ppo2.py:186][0m |          -0.0101 |          10.9963 |           4.1845 |
[32m[20230113 19:56:16 @agent_ppo2.py:186][0m |          -0.0114 |          10.5866 |           4.1784 |
[32m[20230113 19:56:16 @agent_ppo2.py:186][0m |          -0.0115 |           9.8808 |           4.1805 |
[32m[20230113 19:56:16 @agent_ppo2.py:186][0m |          -0.0122 |           9.2908 |           4.1759 |
[32m[20230113 19:56:16 @agent_ppo2.py:186][0m |          -0.0134 |           8.9579 |           4.1798 |
[32m[20230113 19:56:16 @agent_ppo2.py:186][0m |          -0.0141 |           8.7289 |           4.1790 |
[32m[20230113 19:56:16 @agent_ppo2.py:131][0m Policy update time: 0.34 s
[32m[20230113 19:56:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 107.24
[32m[20230113 19:56:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.72
[32m[20230113 19:56:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.23
[32m[20230113 19:56:17 @agent_ppo2.py:144][0m Total time:      11.74 min
[32m[20230113 19:56:17 @agent_ppo2.py:146][0m 1048576 total steps have happened
[32m[20230113 19:56:17 @agent_ppo2.py:122][0m #------------------------ Iteration 512 --------------------------#
[32m[20230113 19:56:17 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:56:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:17 @agent_ppo2.py:186][0m |          -0.0010 |           8.1995 |           4.0920 |
[32m[20230113 19:56:17 @agent_ppo2.py:186][0m |          -0.0067 |           6.7178 |           4.0889 |
[32m[20230113 19:56:17 @agent_ppo2.py:186][0m |          -0.0083 |           6.2352 |           4.0831 |
[32m[20230113 19:56:17 @agent_ppo2.py:186][0m |          -0.0124 |           5.8845 |           4.0781 |
[32m[20230113 19:56:17 @agent_ppo2.py:186][0m |          -0.0113 |           5.6928 |           4.0830 |
[32m[20230113 19:56:18 @agent_ppo2.py:186][0m |          -0.0121 |           5.5046 |           4.0776 |
[32m[20230113 19:56:18 @agent_ppo2.py:186][0m |          -0.0113 |           5.3445 |           4.0806 |
[32m[20230113 19:56:18 @agent_ppo2.py:186][0m |          -0.0147 |           5.2153 |           4.0802 |
[32m[20230113 19:56:18 @agent_ppo2.py:186][0m |          -0.0145 |           5.1578 |           4.0802 |
[32m[20230113 19:56:18 @agent_ppo2.py:186][0m |          -0.0158 |           5.0294 |           4.0813 |
[32m[20230113 19:56:18 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:56:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.01
[32m[20230113 19:56:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.51
[32m[20230113 19:56:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.94
[32m[20230113 19:56:18 @agent_ppo2.py:144][0m Total time:      11.76 min
[32m[20230113 19:56:18 @agent_ppo2.py:146][0m 1050624 total steps have happened
[32m[20230113 19:56:18 @agent_ppo2.py:122][0m #------------------------ Iteration 513 --------------------------#
[32m[20230113 19:56:19 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:19 @agent_ppo2.py:186][0m |          -0.0020 |           5.2969 |           4.1356 |
[32m[20230113 19:56:19 @agent_ppo2.py:186][0m |          -0.0051 |           4.9821 |           4.1295 |
[32m[20230113 19:56:19 @agent_ppo2.py:186][0m |          -0.0107 |           4.7120 |           4.1240 |
[32m[20230113 19:56:19 @agent_ppo2.py:186][0m |          -0.0113 |           4.5627 |           4.1280 |
[32m[20230113 19:56:19 @agent_ppo2.py:186][0m |          -0.0140 |           4.4635 |           4.1195 |
[32m[20230113 19:56:19 @agent_ppo2.py:186][0m |          -0.0128 |           4.3166 |           4.1202 |
[32m[20230113 19:56:19 @agent_ppo2.py:186][0m |          -0.0137 |           4.2353 |           4.1194 |
[32m[20230113 19:56:19 @agent_ppo2.py:186][0m |          -0.0128 |           4.1422 |           4.1175 |
[32m[20230113 19:56:19 @agent_ppo2.py:186][0m |          -0.0128 |           4.1487 |           4.1178 |
[32m[20230113 19:56:19 @agent_ppo2.py:186][0m |          -0.0128 |           4.0228 |           4.1195 |
[32m[20230113 19:56:19 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.26
[32m[20230113 19:56:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.26
[32m[20230113 19:56:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.41
[32m[20230113 19:56:19 @agent_ppo2.py:144][0m Total time:      11.78 min
[32m[20230113 19:56:19 @agent_ppo2.py:146][0m 1052672 total steps have happened
[32m[20230113 19:56:19 @agent_ppo2.py:122][0m #------------------------ Iteration 514 --------------------------#
[32m[20230113 19:56:20 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:20 @agent_ppo2.py:186][0m |           0.0157 |           9.3566 |           4.1629 |
[32m[20230113 19:56:20 @agent_ppo2.py:186][0m |          -0.0061 |           7.5871 |           4.1552 |
[32m[20230113 19:56:20 @agent_ppo2.py:186][0m |          -0.0089 |           7.0342 |           4.1532 |
[32m[20230113 19:56:20 @agent_ppo2.py:186][0m |          -0.0110 |           6.7549 |           4.1546 |
[32m[20230113 19:56:20 @agent_ppo2.py:186][0m |          -0.0104 |           6.5581 |           4.1550 |
[32m[20230113 19:56:20 @agent_ppo2.py:186][0m |          -0.0090 |           6.4887 |           4.1567 |
[32m[20230113 19:56:20 @agent_ppo2.py:186][0m |          -0.0155 |           6.3189 |           4.1604 |
[32m[20230113 19:56:20 @agent_ppo2.py:186][0m |          -0.0156 |           6.2032 |           4.1561 |
[32m[20230113 19:56:20 @agent_ppo2.py:186][0m |          -0.0173 |           6.1105 |           4.1592 |
[32m[20230113 19:56:20 @agent_ppo2.py:186][0m |          -0.0014 |           6.5999 |           4.1633 |
[32m[20230113 19:56:20 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:56:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.54
[32m[20230113 19:56:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.65
[32m[20230113 19:56:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.61
[32m[20230113 19:56:21 @agent_ppo2.py:144][0m Total time:      11.80 min
[32m[20230113 19:56:21 @agent_ppo2.py:146][0m 1054720 total steps have happened
[32m[20230113 19:56:21 @agent_ppo2.py:122][0m #------------------------ Iteration 515 --------------------------#
[32m[20230113 19:56:21 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:21 @agent_ppo2.py:186][0m |           0.0015 |           5.8425 |           4.1719 |
[32m[20230113 19:56:21 @agent_ppo2.py:186][0m |          -0.0045 |           5.3184 |           4.1738 |
[32m[20230113 19:56:21 @agent_ppo2.py:186][0m |          -0.0071 |           5.0692 |           4.1674 |
[32m[20230113 19:56:22 @agent_ppo2.py:186][0m |          -0.0080 |           4.9418 |           4.1663 |
[32m[20230113 19:56:22 @agent_ppo2.py:186][0m |          -0.0097 |           4.8236 |           4.1640 |
[32m[20230113 19:56:22 @agent_ppo2.py:186][0m |          -0.0101 |           4.7198 |           4.1655 |
[32m[20230113 19:56:22 @agent_ppo2.py:186][0m |          -0.0115 |           4.6233 |           4.1661 |
[32m[20230113 19:56:22 @agent_ppo2.py:186][0m |          -0.0091 |           4.6708 |           4.1670 |
[32m[20230113 19:56:22 @agent_ppo2.py:186][0m |          -0.0130 |           4.5078 |           4.1633 |
[32m[20230113 19:56:22 @agent_ppo2.py:186][0m |          -0.0113 |           4.4267 |           4.1616 |
[32m[20230113 19:56:22 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.98
[32m[20230113 19:56:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.45
[32m[20230113 19:56:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.05
[32m[20230113 19:56:22 @agent_ppo2.py:144][0m Total time:      11.83 min
[32m[20230113 19:56:22 @agent_ppo2.py:146][0m 1056768 total steps have happened
[32m[20230113 19:56:22 @agent_ppo2.py:122][0m #------------------------ Iteration 516 --------------------------#
[32m[20230113 19:56:23 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:23 @agent_ppo2.py:186][0m |          -0.0010 |           5.0148 |           4.1200 |
[32m[20230113 19:56:23 @agent_ppo2.py:186][0m |          -0.0098 |           4.7303 |           4.1089 |
[32m[20230113 19:56:23 @agent_ppo2.py:186][0m |          -0.0107 |           4.5736 |           4.1107 |
[32m[20230113 19:56:23 @agent_ppo2.py:186][0m |           0.0003 |           4.8055 |           4.1072 |
[32m[20230113 19:56:23 @agent_ppo2.py:186][0m |          -0.0121 |           4.4078 |           4.1071 |
[32m[20230113 19:56:23 @agent_ppo2.py:186][0m |          -0.0124 |           4.3659 |           4.1087 |
[32m[20230113 19:56:23 @agent_ppo2.py:186][0m |          -0.0086 |           4.3125 |           4.1083 |
[32m[20230113 19:56:23 @agent_ppo2.py:186][0m |          -0.0155 |           4.2679 |           4.1075 |
[32m[20230113 19:56:23 @agent_ppo2.py:186][0m |          -0.0018 |           4.5496 |           4.1093 |
[32m[20230113 19:56:23 @agent_ppo2.py:186][0m |          -0.0146 |           4.2273 |           4.1066 |
[32m[20230113 19:56:23 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.71
[32m[20230113 19:56:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.01
[32m[20230113 19:56:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.01
[32m[20230113 19:56:24 @agent_ppo2.py:144][0m Total time:      11.85 min
[32m[20230113 19:56:24 @agent_ppo2.py:146][0m 1058816 total steps have happened
[32m[20230113 19:56:24 @agent_ppo2.py:122][0m #------------------------ Iteration 517 --------------------------#
[32m[20230113 19:56:24 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 19:56:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:24 @agent_ppo2.py:186][0m |          -0.0009 |          12.8400 |           4.2362 |
[32m[20230113 19:56:24 @agent_ppo2.py:186][0m |          -0.0062 |           6.7879 |           4.2292 |
[32m[20230113 19:56:24 @agent_ppo2.py:186][0m |          -0.0103 |           6.0067 |           4.2282 |
[32m[20230113 19:56:24 @agent_ppo2.py:186][0m |          -0.0124 |           5.5761 |           4.2226 |
[32m[20230113 19:56:24 @agent_ppo2.py:186][0m |          -0.0095 |           5.1977 |           4.2229 |
[32m[20230113 19:56:24 @agent_ppo2.py:186][0m |          -0.0106 |           4.8561 |           4.2242 |
[32m[20230113 19:56:24 @agent_ppo2.py:186][0m |          -0.0131 |           4.5844 |           4.2204 |
[32m[20230113 19:56:24 @agent_ppo2.py:186][0m |          -0.0115 |           4.4585 |           4.2232 |
[32m[20230113 19:56:24 @agent_ppo2.py:186][0m |          -0.0109 |           4.2263 |           4.2156 |
[32m[20230113 19:56:24 @agent_ppo2.py:186][0m |          -0.0118 |           3.9891 |           4.2180 |
[32m[20230113 19:56:24 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 19:56:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 157.38
[32m[20230113 19:56:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.13
[32m[20230113 19:56:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.45
[32m[20230113 19:56:25 @agent_ppo2.py:144][0m Total time:      11.87 min
[32m[20230113 19:56:25 @agent_ppo2.py:146][0m 1060864 total steps have happened
[32m[20230113 19:56:25 @agent_ppo2.py:122][0m #------------------------ Iteration 518 --------------------------#
[32m[20230113 19:56:25 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:56:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:25 @agent_ppo2.py:186][0m |          -0.0002 |           6.2938 |           4.1679 |
[32m[20230113 19:56:25 @agent_ppo2.py:186][0m |          -0.0044 |           5.3409 |           4.1616 |
[32m[20230113 19:56:25 @agent_ppo2.py:186][0m |          -0.0061 |           5.0502 |           4.1624 |
[32m[20230113 19:56:26 @agent_ppo2.py:186][0m |          -0.0088 |           4.9668 |           4.1619 |
[32m[20230113 19:56:26 @agent_ppo2.py:186][0m |          -0.0100 |           4.8826 |           4.1593 |
[32m[20230113 19:56:26 @agent_ppo2.py:186][0m |          -0.0087 |           4.8744 |           4.1597 |
[32m[20230113 19:56:26 @agent_ppo2.py:186][0m |          -0.0139 |           4.7669 |           4.1636 |
[32m[20230113 19:56:26 @agent_ppo2.py:186][0m |          -0.0136 |           4.6981 |           4.1615 |
[32m[20230113 19:56:26 @agent_ppo2.py:186][0m |          -0.0093 |           4.6633 |           4.1642 |
[32m[20230113 19:56:26 @agent_ppo2.py:186][0m |          -0.0068 |           4.8795 |           4.1653 |
[32m[20230113 19:56:26 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:56:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.24
[32m[20230113 19:56:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.51
[32m[20230113 19:56:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.83
[32m[20230113 19:56:26 @agent_ppo2.py:144][0m Total time:      11.89 min
[32m[20230113 19:56:26 @agent_ppo2.py:146][0m 1062912 total steps have happened
[32m[20230113 19:56:26 @agent_ppo2.py:122][0m #------------------------ Iteration 519 --------------------------#
[32m[20230113 19:56:27 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:27 @agent_ppo2.py:186][0m |           0.0008 |           6.5296 |           4.1039 |
[32m[20230113 19:56:27 @agent_ppo2.py:186][0m |          -0.0050 |           5.8388 |           4.0981 |
[32m[20230113 19:56:27 @agent_ppo2.py:186][0m |          -0.0070 |           5.5191 |           4.1005 |
[32m[20230113 19:56:27 @agent_ppo2.py:186][0m |          -0.0083 |           5.2826 |           4.0996 |
[32m[20230113 19:56:27 @agent_ppo2.py:186][0m |          -0.0071 |           5.2188 |           4.1008 |
[32m[20230113 19:56:27 @agent_ppo2.py:186][0m |          -0.0094 |           5.0533 |           4.1006 |
[32m[20230113 19:56:27 @agent_ppo2.py:186][0m |          -0.0104 |           4.9372 |           4.1005 |
[32m[20230113 19:56:27 @agent_ppo2.py:186][0m |          -0.0114 |           4.8540 |           4.1020 |
[32m[20230113 19:56:27 @agent_ppo2.py:186][0m |          -0.0128 |           4.7708 |           4.1040 |
[32m[20230113 19:56:27 @agent_ppo2.py:186][0m |          -0.0119 |           4.7335 |           4.1040 |
[32m[20230113 19:56:27 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:56:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.86
[32m[20230113 19:56:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.08
[32m[20230113 19:56:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.96
[32m[20230113 19:56:28 @agent_ppo2.py:144][0m Total time:      11.92 min
[32m[20230113 19:56:28 @agent_ppo2.py:146][0m 1064960 total steps have happened
[32m[20230113 19:56:28 @agent_ppo2.py:122][0m #------------------------ Iteration 520 --------------------------#
[32m[20230113 19:56:28 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:28 @agent_ppo2.py:186][0m |           0.0005 |           6.3028 |           4.2192 |
[32m[20230113 19:56:28 @agent_ppo2.py:186][0m |          -0.0033 |           5.9521 |           4.2101 |
[32m[20230113 19:56:28 @agent_ppo2.py:186][0m |          -0.0013 |           5.8698 |           4.2096 |
[32m[20230113 19:56:28 @agent_ppo2.py:186][0m |          -0.0049 |           5.6683 |           4.2023 |
[32m[20230113 19:56:28 @agent_ppo2.py:186][0m |          -0.0061 |           5.5556 |           4.2022 |
[32m[20230113 19:56:28 @agent_ppo2.py:186][0m |          -0.0075 |           5.3974 |           4.2004 |
[32m[20230113 19:56:28 @agent_ppo2.py:186][0m |          -0.0080 |           5.3090 |           4.1972 |
[32m[20230113 19:56:28 @agent_ppo2.py:186][0m |          -0.0089 |           5.2609 |           4.1989 |
[32m[20230113 19:56:28 @agent_ppo2.py:186][0m |          -0.0079 |           5.3201 |           4.2000 |
[32m[20230113 19:56:29 @agent_ppo2.py:186][0m |          -0.0092 |           5.1449 |           4.1962 |
[32m[20230113 19:56:29 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.76
[32m[20230113 19:56:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.93
[32m[20230113 19:56:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.84
[32m[20230113 19:56:29 @agent_ppo2.py:144][0m Total time:      11.94 min
[32m[20230113 19:56:29 @agent_ppo2.py:146][0m 1067008 total steps have happened
[32m[20230113 19:56:29 @agent_ppo2.py:122][0m #------------------------ Iteration 521 --------------------------#
[32m[20230113 19:56:29 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:56:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:30 @agent_ppo2.py:186][0m |           0.0002 |           5.7978 |           4.1681 |
[32m[20230113 19:56:30 @agent_ppo2.py:186][0m |          -0.0046 |           5.4366 |           4.1615 |
[32m[20230113 19:56:30 @agent_ppo2.py:186][0m |          -0.0060 |           5.1679 |           4.1614 |
[32m[20230113 19:56:30 @agent_ppo2.py:186][0m |          -0.0068 |           5.0304 |           4.1589 |
[32m[20230113 19:56:30 @agent_ppo2.py:186][0m |          -0.0093 |           4.7947 |           4.1572 |
[32m[20230113 19:56:30 @agent_ppo2.py:186][0m |          -0.0105 |           4.6576 |           4.1543 |
[32m[20230113 19:56:30 @agent_ppo2.py:186][0m |          -0.0115 |           4.5457 |           4.1561 |
[32m[20230113 19:56:30 @agent_ppo2.py:186][0m |          -0.0108 |           4.4432 |           4.1553 |
[32m[20230113 19:56:30 @agent_ppo2.py:186][0m |          -0.0123 |           4.3249 |           4.1505 |
[32m[20230113 19:56:30 @agent_ppo2.py:186][0m |          -0.0134 |           4.2170 |           4.1521 |
[32m[20230113 19:56:30 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.49
[32m[20230113 19:56:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.66
[32m[20230113 19:56:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.09
[32m[20230113 19:56:30 @agent_ppo2.py:144][0m Total time:      11.96 min
[32m[20230113 19:56:30 @agent_ppo2.py:146][0m 1069056 total steps have happened
[32m[20230113 19:56:30 @agent_ppo2.py:122][0m #------------------------ Iteration 522 --------------------------#
[32m[20230113 19:56:31 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:56:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:31 @agent_ppo2.py:186][0m |           0.0005 |           5.6244 |           4.1718 |
[32m[20230113 19:56:31 @agent_ppo2.py:186][0m |          -0.0027 |           5.2501 |           4.1687 |
[32m[20230113 19:56:31 @agent_ppo2.py:186][0m |          -0.0050 |           5.0133 |           4.1683 |
[32m[20230113 19:56:31 @agent_ppo2.py:186][0m |          -0.0063 |           4.8621 |           4.1685 |
[32m[20230113 19:56:31 @agent_ppo2.py:186][0m |          -0.0061 |           4.7632 |           4.1694 |
[32m[20230113 19:56:31 @agent_ppo2.py:186][0m |          -0.0075 |           4.6365 |           4.1703 |
[32m[20230113 19:56:31 @agent_ppo2.py:186][0m |          -0.0082 |           4.5685 |           4.1701 |
[32m[20230113 19:56:31 @agent_ppo2.py:186][0m |          -0.0085 |           4.4783 |           4.1704 |
[32m[20230113 19:56:31 @agent_ppo2.py:186][0m |          -0.0098 |           4.4050 |           4.1697 |
[32m[20230113 19:56:31 @agent_ppo2.py:186][0m |          -0.0103 |           4.3533 |           4.1695 |
[32m[20230113 19:56:31 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:56:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.67
[32m[20230113 19:56:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.25
[32m[20230113 19:56:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.61
[32m[20230113 19:56:32 @agent_ppo2.py:144][0m Total time:      11.98 min
[32m[20230113 19:56:32 @agent_ppo2.py:146][0m 1071104 total steps have happened
[32m[20230113 19:56:32 @agent_ppo2.py:122][0m #------------------------ Iteration 523 --------------------------#
[32m[20230113 19:56:32 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:32 @agent_ppo2.py:186][0m |           0.0008 |           5.9451 |           4.1363 |
[32m[20230113 19:56:32 @agent_ppo2.py:186][0m |          -0.0026 |           5.4157 |           4.1335 |
[32m[20230113 19:56:32 @agent_ppo2.py:186][0m |          -0.0058 |           4.8871 |           4.1291 |
[32m[20230113 19:56:32 @agent_ppo2.py:186][0m |          -0.0048 |           4.3698 |           4.1310 |
[32m[20230113 19:56:32 @agent_ppo2.py:186][0m |          -0.0069 |           3.7890 |           4.1273 |
[32m[20230113 19:56:32 @agent_ppo2.py:186][0m |          -0.0072 |           3.6387 |           4.1283 |
[32m[20230113 19:56:32 @agent_ppo2.py:186][0m |          -0.0078 |           3.5004 |           4.1281 |
[32m[20230113 19:56:32 @agent_ppo2.py:186][0m |          -0.0094 |           3.4272 |           4.1263 |
[32m[20230113 19:56:33 @agent_ppo2.py:186][0m |          -0.0088 |           3.3320 |           4.1266 |
[32m[20230113 19:56:33 @agent_ppo2.py:186][0m |          -0.0117 |           3.2657 |           4.1266 |
[32m[20230113 19:56:33 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.18
[32m[20230113 19:56:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.59
[32m[20230113 19:56:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.24
[32m[20230113 19:56:33 @agent_ppo2.py:144][0m Total time:      12.01 min
[32m[20230113 19:56:33 @agent_ppo2.py:146][0m 1073152 total steps have happened
[32m[20230113 19:56:33 @agent_ppo2.py:122][0m #------------------------ Iteration 524 --------------------------#
[32m[20230113 19:56:33 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:34 @agent_ppo2.py:186][0m |           0.0021 |           6.1211 |           4.1318 |
[32m[20230113 19:56:34 @agent_ppo2.py:186][0m |          -0.0110 |           5.1350 |           4.1227 |
[32m[20230113 19:56:34 @agent_ppo2.py:186][0m |          -0.0079 |           4.5337 |           4.1270 |
[32m[20230113 19:56:34 @agent_ppo2.py:186][0m |          -0.0195 |           4.2933 |           4.1205 |
[32m[20230113 19:56:34 @agent_ppo2.py:186][0m |          -0.0148 |           3.9429 |           4.1243 |
[32m[20230113 19:56:34 @agent_ppo2.py:186][0m |          -0.0172 |           3.7524 |           4.1190 |
[32m[20230113 19:56:34 @agent_ppo2.py:186][0m |          -0.0189 |           3.6618 |           4.1175 |
[32m[20230113 19:56:34 @agent_ppo2.py:186][0m |          -0.0146 |           3.5371 |           4.1195 |
[32m[20230113 19:56:34 @agent_ppo2.py:186][0m |          -0.0151 |           3.4596 |           4.1179 |
[32m[20230113 19:56:34 @agent_ppo2.py:186][0m |          -0.0215 |           3.3597 |           4.1151 |
[32m[20230113 19:56:34 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:56:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.79
[32m[20230113 19:56:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.36
[32m[20230113 19:56:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.87
[32m[20230113 19:56:34 @agent_ppo2.py:144][0m Total time:      12.03 min
[32m[20230113 19:56:34 @agent_ppo2.py:146][0m 1075200 total steps have happened
[32m[20230113 19:56:34 @agent_ppo2.py:122][0m #------------------------ Iteration 525 --------------------------#
[32m[20230113 19:56:35 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:56:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:35 @agent_ppo2.py:186][0m |           0.0012 |          13.5957 |           4.2197 |
[32m[20230113 19:56:35 @agent_ppo2.py:186][0m |          -0.0021 |          10.0840 |           4.2114 |
[32m[20230113 19:56:35 @agent_ppo2.py:186][0m |          -0.0047 |           9.0707 |           4.2116 |
[32m[20230113 19:56:35 @agent_ppo2.py:186][0m |          -0.0065 |           8.3504 |           4.2156 |
[32m[20230113 19:56:35 @agent_ppo2.py:186][0m |          -0.0062 |           7.7913 |           4.2157 |
[32m[20230113 19:56:35 @agent_ppo2.py:186][0m |          -0.0040 |           7.3897 |           4.2101 |
[32m[20230113 19:56:35 @agent_ppo2.py:186][0m |          -0.0082 |           6.9266 |           4.2181 |
[32m[20230113 19:56:35 @agent_ppo2.py:186][0m |          -0.0071 |           6.6931 |           4.2156 |
[32m[20230113 19:56:35 @agent_ppo2.py:186][0m |          -0.0096 |           6.4537 |           4.2182 |
[32m[20230113 19:56:35 @agent_ppo2.py:186][0m |          -0.0127 |           6.1492 |           4.2176 |
[32m[20230113 19:56:35 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:56:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.97
[32m[20230113 19:56:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.55
[32m[20230113 19:56:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.83
[32m[20230113 19:56:36 @agent_ppo2.py:144][0m Total time:      12.05 min
[32m[20230113 19:56:36 @agent_ppo2.py:146][0m 1077248 total steps have happened
[32m[20230113 19:56:36 @agent_ppo2.py:122][0m #------------------------ Iteration 526 --------------------------#
[32m[20230113 19:56:36 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:56:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:36 @agent_ppo2.py:186][0m |          -0.0011 |           6.7202 |           4.2508 |
[32m[20230113 19:56:36 @agent_ppo2.py:186][0m |          -0.0069 |           5.7801 |           4.2375 |
[32m[20230113 19:56:36 @agent_ppo2.py:186][0m |          -0.0076 |           5.5460 |           4.2395 |
[32m[20230113 19:56:36 @agent_ppo2.py:186][0m |          -0.0107 |           5.4094 |           4.2352 |
[32m[20230113 19:56:36 @agent_ppo2.py:186][0m |          -0.0100 |           5.2896 |           4.2377 |
[32m[20230113 19:56:36 @agent_ppo2.py:186][0m |          -0.0104 |           5.1440 |           4.2361 |
[32m[20230113 19:56:37 @agent_ppo2.py:186][0m |          -0.0096 |           5.1018 |           4.2366 |
[32m[20230113 19:56:37 @agent_ppo2.py:186][0m |          -0.0117 |           4.9443 |           4.2358 |
[32m[20230113 19:56:37 @agent_ppo2.py:186][0m |          -0.0136 |           4.9199 |           4.2377 |
[32m[20230113 19:56:37 @agent_ppo2.py:186][0m |          -0.0139 |           4.8143 |           4.2365 |
[32m[20230113 19:56:37 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:56:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.96
[32m[20230113 19:56:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.28
[32m[20230113 19:56:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 158.96
[32m[20230113 19:56:37 @agent_ppo2.py:144][0m Total time:      12.07 min
[32m[20230113 19:56:37 @agent_ppo2.py:146][0m 1079296 total steps have happened
[32m[20230113 19:56:37 @agent_ppo2.py:122][0m #------------------------ Iteration 527 --------------------------#
[32m[20230113 19:56:37 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 19:56:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:38 @agent_ppo2.py:186][0m |          -0.0024 |          13.2508 |           4.1314 |
[32m[20230113 19:56:38 @agent_ppo2.py:186][0m |          -0.0066 |           7.1584 |           4.1313 |
[32m[20230113 19:56:38 @agent_ppo2.py:186][0m |          -0.0030 |           5.8462 |           4.1262 |
[32m[20230113 19:56:38 @agent_ppo2.py:186][0m |          -0.0139 |           5.4382 |           4.1236 |
[32m[20230113 19:56:38 @agent_ppo2.py:186][0m |          -0.0136 |           5.0423 |           4.1222 |
[32m[20230113 19:56:38 @agent_ppo2.py:186][0m |          -0.0044 |           4.8326 |           4.1180 |
[32m[20230113 19:56:38 @agent_ppo2.py:186][0m |          -0.0152 |           4.7235 |           4.1175 |
[32m[20230113 19:56:38 @agent_ppo2.py:186][0m |          -0.0197 |           4.4251 |           4.1179 |
[32m[20230113 19:56:38 @agent_ppo2.py:186][0m |          -0.0176 |           4.1986 |           4.1169 |
[32m[20230113 19:56:38 @agent_ppo2.py:186][0m |          -0.0194 |           4.1844 |           4.1149 |
[32m[20230113 19:56:38 @agent_ppo2.py:131][0m Policy update time: 0.40 s
[32m[20230113 19:56:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 134.57
[32m[20230113 19:56:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.79
[32m[20230113 19:56:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.70
[32m[20230113 19:56:38 @agent_ppo2.py:144][0m Total time:      12.09 min
[32m[20230113 19:56:38 @agent_ppo2.py:146][0m 1081344 total steps have happened
[32m[20230113 19:56:38 @agent_ppo2.py:122][0m #------------------------ Iteration 528 --------------------------#
[32m[20230113 19:56:39 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:56:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:39 @agent_ppo2.py:186][0m |           0.0014 |           5.6897 |           4.2242 |
[32m[20230113 19:56:39 @agent_ppo2.py:186][0m |          -0.0015 |           5.0613 |           4.2277 |
[32m[20230113 19:56:39 @agent_ppo2.py:186][0m |          -0.0044 |           4.8260 |           4.2249 |
[32m[20230113 19:56:39 @agent_ppo2.py:186][0m |          -0.0068 |           4.7598 |           4.2215 |
[32m[20230113 19:56:39 @agent_ppo2.py:186][0m |          -0.0056 |           4.5703 |           4.2227 |
[32m[20230113 19:56:39 @agent_ppo2.py:186][0m |          -0.0062 |           4.4919 |           4.2194 |
[32m[20230113 19:56:39 @agent_ppo2.py:186][0m |          -0.0085 |           4.3588 |           4.2185 |
[32m[20230113 19:56:39 @agent_ppo2.py:186][0m |          -0.0094 |           4.2817 |           4.2176 |
[32m[20230113 19:56:39 @agent_ppo2.py:186][0m |          -0.0101 |           4.2119 |           4.2151 |
[32m[20230113 19:56:39 @agent_ppo2.py:186][0m |          -0.0104 |           4.1643 |           4.2161 |
[32m[20230113 19:56:39 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:56:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.33
[32m[20230113 19:56:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.77
[32m[20230113 19:56:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.78
[32m[20230113 19:56:40 @agent_ppo2.py:144][0m Total time:      12.12 min
[32m[20230113 19:56:40 @agent_ppo2.py:146][0m 1083392 total steps have happened
[32m[20230113 19:56:40 @agent_ppo2.py:122][0m #------------------------ Iteration 529 --------------------------#
[32m[20230113 19:56:40 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:40 @agent_ppo2.py:186][0m |           0.0007 |           5.8803 |           4.3120 |
[32m[20230113 19:56:40 @agent_ppo2.py:186][0m |          -0.0032 |           5.3274 |           4.3132 |
[32m[20230113 19:56:40 @agent_ppo2.py:186][0m |          -0.0063 |           5.0626 |           4.3102 |
[32m[20230113 19:56:40 @agent_ppo2.py:186][0m |          -0.0073 |           4.8728 |           4.3114 |
[32m[20230113 19:56:40 @agent_ppo2.py:186][0m |          -0.0088 |           4.7575 |           4.3108 |
[32m[20230113 19:56:40 @agent_ppo2.py:186][0m |          -0.0090 |           4.5888 |           4.3108 |
[32m[20230113 19:56:40 @agent_ppo2.py:186][0m |          -0.0098 |           4.4979 |           4.3102 |
[32m[20230113 19:56:40 @agent_ppo2.py:186][0m |          -0.0098 |           4.3551 |           4.3105 |
[32m[20230113 19:56:40 @agent_ppo2.py:186][0m |          -0.0102 |           4.2420 |           4.3118 |
[32m[20230113 19:56:41 @agent_ppo2.py:186][0m |          -0.0103 |           4.1234 |           4.3120 |
[32m[20230113 19:56:41 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.50
[32m[20230113 19:56:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.27
[32m[20230113 19:56:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.67
[32m[20230113 19:56:41 @agent_ppo2.py:144][0m Total time:      12.14 min
[32m[20230113 19:56:41 @agent_ppo2.py:146][0m 1085440 total steps have happened
[32m[20230113 19:56:41 @agent_ppo2.py:122][0m #------------------------ Iteration 530 --------------------------#
[32m[20230113 19:56:41 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:56:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:42 @agent_ppo2.py:186][0m |           0.0252 |           7.6938 |           4.2661 |
[32m[20230113 19:56:42 @agent_ppo2.py:186][0m |          -0.0060 |           5.9620 |           4.2540 |
[32m[20230113 19:56:42 @agent_ppo2.py:186][0m |          -0.0069 |           5.6236 |           4.2563 |
[32m[20230113 19:56:42 @agent_ppo2.py:186][0m |          -0.0087 |           5.4800 |           4.2555 |
[32m[20230113 19:56:42 @agent_ppo2.py:186][0m |          -0.0112 |           5.3905 |           4.2576 |
[32m[20230113 19:56:42 @agent_ppo2.py:186][0m |          -0.0069 |           5.4502 |           4.2546 |
[32m[20230113 19:56:42 @agent_ppo2.py:186][0m |          -0.0085 |           5.3998 |           4.2552 |
[32m[20230113 19:56:42 @agent_ppo2.py:186][0m |          -0.0089 |           5.2260 |           4.2532 |
[32m[20230113 19:56:42 @agent_ppo2.py:186][0m |          -0.0057 |           5.5846 |           4.2550 |
[32m[20230113 19:56:42 @agent_ppo2.py:186][0m |          -0.0133 |           5.1137 |           4.2483 |
[32m[20230113 19:56:42 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:56:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.34
[32m[20230113 19:56:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.88
[32m[20230113 19:56:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.96
[32m[20230113 19:56:42 @agent_ppo2.py:144][0m Total time:      12.16 min
[32m[20230113 19:56:42 @agent_ppo2.py:146][0m 1087488 total steps have happened
[32m[20230113 19:56:42 @agent_ppo2.py:122][0m #------------------------ Iteration 531 --------------------------#
[32m[20230113 19:56:43 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:43 @agent_ppo2.py:186][0m |           0.0004 |           5.8201 |           4.2848 |
[32m[20230113 19:56:43 @agent_ppo2.py:186][0m |          -0.0043 |           5.2751 |           4.2846 |
[32m[20230113 19:56:43 @agent_ppo2.py:186][0m |          -0.0085 |           5.0154 |           4.2792 |
[32m[20230113 19:56:43 @agent_ppo2.py:186][0m |          -0.0099 |           4.8041 |           4.2760 |
[32m[20230113 19:56:43 @agent_ppo2.py:186][0m |          -0.0120 |           4.6839 |           4.2763 |
[32m[20230113 19:56:43 @agent_ppo2.py:186][0m |          -0.0132 |           4.5860 |           4.2746 |
[32m[20230113 19:56:43 @agent_ppo2.py:186][0m |          -0.0140 |           4.4900 |           4.2765 |
[32m[20230113 19:56:43 @agent_ppo2.py:186][0m |          -0.0150 |           4.3969 |           4.2729 |
[32m[20230113 19:56:43 @agent_ppo2.py:186][0m |          -0.0151 |           4.3502 |           4.2752 |
[32m[20230113 19:56:43 @agent_ppo2.py:186][0m |          -0.0164 |           4.2686 |           4.2745 |
[32m[20230113 19:56:43 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:56:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.59
[32m[20230113 19:56:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.35
[32m[20230113 19:56:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.41
[32m[20230113 19:56:44 @agent_ppo2.py:144][0m Total time:      12.18 min
[32m[20230113 19:56:44 @agent_ppo2.py:146][0m 1089536 total steps have happened
[32m[20230113 19:56:44 @agent_ppo2.py:122][0m #------------------------ Iteration 532 --------------------------#
[32m[20230113 19:56:44 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:56:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:44 @agent_ppo2.py:186][0m |          -0.0040 |           5.1190 |           4.2220 |
[32m[20230113 19:56:44 @agent_ppo2.py:186][0m |          -0.0059 |           4.7227 |           4.2166 |
[32m[20230113 19:56:44 @agent_ppo2.py:186][0m |          -0.0066 |           4.5054 |           4.2137 |
[32m[20230113 19:56:44 @agent_ppo2.py:186][0m |          -0.0107 |           4.3785 |           4.2140 |
[32m[20230113 19:56:44 @agent_ppo2.py:186][0m |          -0.0112 |           4.2333 |           4.2138 |
[32m[20230113 19:56:44 @agent_ppo2.py:186][0m |          -0.0141 |           4.1552 |           4.2147 |
[32m[20230113 19:56:44 @agent_ppo2.py:186][0m |          -0.0119 |           4.0314 |           4.2153 |
[32m[20230113 19:56:45 @agent_ppo2.py:186][0m |          -0.0134 |           4.0016 |           4.2143 |
[32m[20230113 19:56:45 @agent_ppo2.py:186][0m |          -0.0144 |           3.8878 |           4.2123 |
[32m[20230113 19:56:45 @agent_ppo2.py:186][0m |          -0.0177 |           3.9094 |           4.2148 |
[32m[20230113 19:56:45 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:56:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.49
[32m[20230113 19:56:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.02
[32m[20230113 19:56:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.34
[32m[20230113 19:56:45 @agent_ppo2.py:144][0m Total time:      12.21 min
[32m[20230113 19:56:45 @agent_ppo2.py:146][0m 1091584 total steps have happened
[32m[20230113 19:56:45 @agent_ppo2.py:122][0m #------------------------ Iteration 533 --------------------------#
[32m[20230113 19:56:46 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:56:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:46 @agent_ppo2.py:186][0m |          -0.0019 |          20.5737 |           4.1725 |
[32m[20230113 19:56:46 @agent_ppo2.py:186][0m |          -0.0062 |          12.4443 |           4.1703 |
[32m[20230113 19:56:46 @agent_ppo2.py:186][0m |          -0.0065 |          10.4534 |           4.1695 |
[32m[20230113 19:56:46 @agent_ppo2.py:186][0m |          -0.0091 |           9.3659 |           4.1668 |
[32m[20230113 19:56:46 @agent_ppo2.py:186][0m |          -0.0132 |           8.4096 |           4.1705 |
[32m[20230113 19:56:46 @agent_ppo2.py:186][0m |          -0.0153 |           8.0459 |           4.1686 |
[32m[20230113 19:56:46 @agent_ppo2.py:186][0m |          -0.0110 |           7.3470 |           4.1732 |
[32m[20230113 19:56:46 @agent_ppo2.py:186][0m |          -0.0167 |           6.9101 |           4.1733 |
[32m[20230113 19:56:46 @agent_ppo2.py:186][0m |          -0.0180 |           6.6219 |           4.1751 |
[32m[20230113 19:56:46 @agent_ppo2.py:186][0m |          -0.0180 |           6.4159 |           4.1724 |
[32m[20230113 19:56:46 @agent_ppo2.py:131][0m Policy update time: 0.55 s
[32m[20230113 19:56:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 138.26
[32m[20230113 19:56:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.20
[32m[20230113 19:56:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.57
[32m[20230113 19:56:47 @agent_ppo2.py:144][0m Total time:      12.23 min
[32m[20230113 19:56:47 @agent_ppo2.py:146][0m 1093632 total steps have happened
[32m[20230113 19:56:47 @agent_ppo2.py:122][0m #------------------------ Iteration 534 --------------------------#
[32m[20230113 19:56:47 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:56:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:47 @agent_ppo2.py:186][0m |           0.0040 |           6.0286 |           4.2116 |
[32m[20230113 19:56:47 @agent_ppo2.py:186][0m |          -0.0059 |           5.1905 |           4.2044 |
[32m[20230113 19:56:47 @agent_ppo2.py:186][0m |          -0.0012 |           4.6885 |           4.2063 |
[32m[20230113 19:56:47 @agent_ppo2.py:186][0m |          -0.0036 |           4.3654 |           4.2116 |
[32m[20230113 19:56:47 @agent_ppo2.py:186][0m |          -0.0058 |           4.0827 |           4.2078 |
[32m[20230113 19:56:47 @agent_ppo2.py:186][0m |          -0.0025 |           4.0461 |           4.2105 |
[32m[20230113 19:56:47 @agent_ppo2.py:186][0m |          -0.0018 |           3.6562 |           4.2069 |
[32m[20230113 19:56:47 @agent_ppo2.py:186][0m |          -0.0131 |           3.5038 |           4.2090 |
[32m[20230113 19:56:47 @agent_ppo2.py:186][0m |          -0.0093 |           3.4165 |           4.2120 |
[32m[20230113 19:56:48 @agent_ppo2.py:186][0m |          -0.0084 |           3.2500 |           4.2084 |
[32m[20230113 19:56:48 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:56:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.46
[32m[20230113 19:56:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.00
[32m[20230113 19:56:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.00
[32m[20230113 19:56:48 @agent_ppo2.py:144][0m Total time:      12.26 min
[32m[20230113 19:56:48 @agent_ppo2.py:146][0m 1095680 total steps have happened
[32m[20230113 19:56:48 @agent_ppo2.py:122][0m #------------------------ Iteration 535 --------------------------#
[32m[20230113 19:56:48 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:56:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:48 @agent_ppo2.py:186][0m |          -0.0014 |           6.3303 |           4.2534 |
[32m[20230113 19:56:49 @agent_ppo2.py:186][0m |          -0.0025 |           5.5852 |           4.2461 |
[32m[20230113 19:56:49 @agent_ppo2.py:186][0m |          -0.0074 |           5.1925 |           4.2442 |
[32m[20230113 19:56:49 @agent_ppo2.py:186][0m |          -0.0105 |           4.9858 |           4.2475 |
[32m[20230113 19:56:49 @agent_ppo2.py:186][0m |          -0.0102 |           4.8218 |           4.2437 |
[32m[20230113 19:56:49 @agent_ppo2.py:186][0m |          -0.0093 |           4.7771 |           4.2462 |
[32m[20230113 19:56:49 @agent_ppo2.py:186][0m |          -0.0071 |           4.7293 |           4.2451 |
[32m[20230113 19:56:49 @agent_ppo2.py:186][0m |          -0.0109 |           4.4647 |           4.2468 |
[32m[20230113 19:56:49 @agent_ppo2.py:186][0m |          -0.0147 |           4.3755 |           4.2466 |
[32m[20230113 19:56:49 @agent_ppo2.py:186][0m |          -0.0134 |           4.5392 |           4.2466 |
[32m[20230113 19:56:49 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:56:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.18
[32m[20230113 19:56:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.87
[32m[20230113 19:56:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.33
[32m[20230113 19:56:49 @agent_ppo2.py:144][0m Total time:      12.28 min
[32m[20230113 19:56:49 @agent_ppo2.py:146][0m 1097728 total steps have happened
[32m[20230113 19:56:49 @agent_ppo2.py:122][0m #------------------------ Iteration 536 --------------------------#
[32m[20230113 19:56:50 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 19:56:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:50 @agent_ppo2.py:186][0m |          -0.0018 |          12.8579 |           4.2566 |
[32m[20230113 19:56:50 @agent_ppo2.py:186][0m |          -0.0054 |           4.7976 |           4.2554 |
[32m[20230113 19:56:50 @agent_ppo2.py:186][0m |          -0.0055 |           4.1099 |           4.2547 |
[32m[20230113 19:56:50 @agent_ppo2.py:186][0m |          -0.0087 |           3.6281 |           4.2518 |
[32m[20230113 19:56:50 @agent_ppo2.py:186][0m |          -0.0107 |           3.2874 |           4.2495 |
[32m[20230113 19:56:50 @agent_ppo2.py:186][0m |          -0.0094 |           3.1536 |           4.2507 |
[32m[20230113 19:56:50 @agent_ppo2.py:186][0m |          -0.0122 |           2.9693 |           4.2490 |
[32m[20230113 19:56:50 @agent_ppo2.py:186][0m |          -0.0108 |           2.9253 |           4.2438 |
[32m[20230113 19:56:50 @agent_ppo2.py:186][0m |          -0.0116 |           2.8021 |           4.2494 |
[32m[20230113 19:56:50 @agent_ppo2.py:186][0m |          -0.0149 |           2.6722 |           4.2464 |
[32m[20230113 19:56:50 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 19:56:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 113.63
[32m[20230113 19:56:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.34
[32m[20230113 19:56:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.58
[32m[20230113 19:56:50 @agent_ppo2.py:144][0m Total time:      12.30 min
[32m[20230113 19:56:50 @agent_ppo2.py:146][0m 1099776 total steps have happened
[32m[20230113 19:56:50 @agent_ppo2.py:122][0m #------------------------ Iteration 537 --------------------------#
[32m[20230113 19:56:51 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:56:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:51 @agent_ppo2.py:186][0m |           0.0006 |           7.5753 |           4.2664 |
[32m[20230113 19:56:51 @agent_ppo2.py:186][0m |          -0.0041 |           6.7032 |           4.2642 |
[32m[20230113 19:56:51 @agent_ppo2.py:186][0m |          -0.0070 |           6.3907 |           4.2593 |
[32m[20230113 19:56:51 @agent_ppo2.py:186][0m |          -0.0088 |           6.0751 |           4.2610 |
[32m[20230113 19:56:51 @agent_ppo2.py:186][0m |          -0.0107 |           5.8774 |           4.2591 |
[32m[20230113 19:56:51 @agent_ppo2.py:186][0m |          -0.0109 |           5.6620 |           4.2591 |
[32m[20230113 19:56:51 @agent_ppo2.py:186][0m |          -0.0110 |           5.5598 |           4.2622 |
[32m[20230113 19:56:51 @agent_ppo2.py:186][0m |          -0.0120 |           5.3862 |           4.2624 |
[32m[20230113 19:56:51 @agent_ppo2.py:186][0m |          -0.0132 |           5.2726 |           4.2648 |
[32m[20230113 19:56:51 @agent_ppo2.py:186][0m |          -0.0132 |           5.1870 |           4.2614 |
[32m[20230113 19:56:51 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.33
[32m[20230113 19:56:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.92
[32m[20230113 19:56:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.91
[32m[20230113 19:56:52 @agent_ppo2.py:144][0m Total time:      12.32 min
[32m[20230113 19:56:52 @agent_ppo2.py:146][0m 1101824 total steps have happened
[32m[20230113 19:56:52 @agent_ppo2.py:122][0m #------------------------ Iteration 538 --------------------------#
[32m[20230113 19:56:52 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:52 @agent_ppo2.py:186][0m |          -0.0012 |           6.3408 |           4.2675 |
[32m[20230113 19:56:52 @agent_ppo2.py:186][0m |          -0.0035 |           6.0527 |           4.2608 |
[32m[20230113 19:56:52 @agent_ppo2.py:186][0m |          -0.0021 |           5.8874 |           4.2575 |
[32m[20230113 19:56:53 @agent_ppo2.py:186][0m |          -0.0111 |           5.7928 |           4.2505 |
[32m[20230113 19:56:53 @agent_ppo2.py:186][0m |          -0.0099 |           5.7311 |           4.2523 |
[32m[20230113 19:56:53 @agent_ppo2.py:186][0m |          -0.0070 |           5.6520 |           4.2511 |
[32m[20230113 19:56:53 @agent_ppo2.py:186][0m |          -0.0074 |           5.6300 |           4.2500 |
[32m[20230113 19:56:53 @agent_ppo2.py:186][0m |          -0.0128 |           5.5767 |           4.2509 |
[32m[20230113 19:56:53 @agent_ppo2.py:186][0m |          -0.0119 |           5.5067 |           4.2485 |
[32m[20230113 19:56:53 @agent_ppo2.py:186][0m |          -0.0139 |           5.4852 |           4.2489 |
[32m[20230113 19:56:53 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.61
[32m[20230113 19:56:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.38
[32m[20230113 19:56:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.79
[32m[20230113 19:56:53 @agent_ppo2.py:144][0m Total time:      12.34 min
[32m[20230113 19:56:53 @agent_ppo2.py:146][0m 1103872 total steps have happened
[32m[20230113 19:56:53 @agent_ppo2.py:122][0m #------------------------ Iteration 539 --------------------------#
[32m[20230113 19:56:54 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:54 @agent_ppo2.py:186][0m |          -0.0013 |           5.7531 |           4.3329 |
[32m[20230113 19:56:54 @agent_ppo2.py:186][0m |          -0.0043 |           5.3218 |           4.3301 |
[32m[20230113 19:56:54 @agent_ppo2.py:186][0m |          -0.0072 |           5.0474 |           4.3275 |
[32m[20230113 19:56:54 @agent_ppo2.py:186][0m |          -0.0078 |           4.8600 |           4.3305 |
[32m[20230113 19:56:54 @agent_ppo2.py:186][0m |          -0.0096 |           4.7318 |           4.3285 |
[32m[20230113 19:56:54 @agent_ppo2.py:186][0m |          -0.0099 |           4.6244 |           4.3245 |
[32m[20230113 19:56:54 @agent_ppo2.py:186][0m |          -0.0089 |           4.5693 |           4.3239 |
[32m[20230113 19:56:54 @agent_ppo2.py:186][0m |          -0.0104 |           4.4252 |           4.3202 |
[32m[20230113 19:56:54 @agent_ppo2.py:186][0m |          -0.0117 |           4.3458 |           4.3259 |
[32m[20230113 19:56:54 @agent_ppo2.py:186][0m |          -0.0124 |           4.2655 |           4.3226 |
[32m[20230113 19:56:54 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:56:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.39
[32m[20230113 19:56:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.65
[32m[20230113 19:56:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.06
[32m[20230113 19:56:55 @agent_ppo2.py:144][0m Total time:      12.37 min
[32m[20230113 19:56:55 @agent_ppo2.py:146][0m 1105920 total steps have happened
[32m[20230113 19:56:55 @agent_ppo2.py:122][0m #------------------------ Iteration 540 --------------------------#
[32m[20230113 19:56:55 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:56:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:55 @agent_ppo2.py:186][0m |           0.0087 |           6.7206 |           4.3000 |
[32m[20230113 19:56:55 @agent_ppo2.py:186][0m |          -0.0094 |           5.7556 |           4.2922 |
[32m[20230113 19:56:55 @agent_ppo2.py:186][0m |          -0.0078 |           5.4512 |           4.2879 |
[32m[20230113 19:56:55 @agent_ppo2.py:186][0m |          -0.0078 |           5.2429 |           4.2871 |
[32m[20230113 19:56:55 @agent_ppo2.py:186][0m |          -0.0121 |           5.0535 |           4.2858 |
[32m[20230113 19:56:55 @agent_ppo2.py:186][0m |          -0.0097 |           4.9486 |           4.2837 |
[32m[20230113 19:56:55 @agent_ppo2.py:186][0m |          -0.0087 |           4.8606 |           4.2825 |
[32m[20230113 19:56:55 @agent_ppo2.py:186][0m |          -0.0156 |           4.6589 |           4.2820 |
[32m[20230113 19:56:55 @agent_ppo2.py:186][0m |          -0.0122 |           4.5060 |           4.2796 |
[32m[20230113 19:56:55 @agent_ppo2.py:186][0m |          -0.0122 |           4.3722 |           4.2787 |
[32m[20230113 19:56:55 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.35
[32m[20230113 19:56:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.46
[32m[20230113 19:56:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.04
[32m[20230113 19:56:56 @agent_ppo2.py:144][0m Total time:      12.39 min
[32m[20230113 19:56:56 @agent_ppo2.py:146][0m 1107968 total steps have happened
[32m[20230113 19:56:56 @agent_ppo2.py:122][0m #------------------------ Iteration 541 --------------------------#
[32m[20230113 19:56:56 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:56:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:56 @agent_ppo2.py:186][0m |          -0.0002 |           5.8156 |           4.2446 |
[32m[20230113 19:56:56 @agent_ppo2.py:186][0m |          -0.0068 |           5.0141 |           4.2388 |
[32m[20230113 19:56:57 @agent_ppo2.py:186][0m |          -0.0080 |           4.6961 |           4.2384 |
[32m[20230113 19:56:57 @agent_ppo2.py:186][0m |          -0.0100 |           4.4810 |           4.2359 |
[32m[20230113 19:56:57 @agent_ppo2.py:186][0m |          -0.0111 |           4.3975 |           4.2348 |
[32m[20230113 19:56:57 @agent_ppo2.py:186][0m |          -0.0120 |           4.2760 |           4.2321 |
[32m[20230113 19:56:57 @agent_ppo2.py:186][0m |          -0.0110 |           4.2288 |           4.2308 |
[32m[20230113 19:56:57 @agent_ppo2.py:186][0m |          -0.0126 |           4.1364 |           4.2308 |
[32m[20230113 19:56:57 @agent_ppo2.py:186][0m |          -0.0122 |           4.1463 |           4.2315 |
[32m[20230113 19:56:57 @agent_ppo2.py:186][0m |          -0.0129 |           4.0439 |           4.2274 |
[32m[20230113 19:56:57 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:56:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.94
[32m[20230113 19:56:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.82
[32m[20230113 19:56:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.76
[32m[20230113 19:56:57 @agent_ppo2.py:144][0m Total time:      12.41 min
[32m[20230113 19:56:57 @agent_ppo2.py:146][0m 1110016 total steps have happened
[32m[20230113 19:56:57 @agent_ppo2.py:122][0m #------------------------ Iteration 542 --------------------------#
[32m[20230113 19:56:58 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:56:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:58 @agent_ppo2.py:186][0m |           0.0001 |           4.8631 |           4.3529 |
[32m[20230113 19:56:58 @agent_ppo2.py:186][0m |          -0.0032 |           4.4642 |           4.3509 |
[32m[20230113 19:56:58 @agent_ppo2.py:186][0m |          -0.0052 |           4.3101 |           4.3455 |
[32m[20230113 19:56:58 @agent_ppo2.py:186][0m |          -0.0071 |           4.1577 |           4.3445 |
[32m[20230113 19:56:58 @agent_ppo2.py:186][0m |          -0.0079 |           3.9990 |           4.3413 |
[32m[20230113 19:56:58 @agent_ppo2.py:186][0m |          -0.0084 |           3.9158 |           4.3433 |
[32m[20230113 19:56:58 @agent_ppo2.py:186][0m |          -0.0088 |           3.8159 |           4.3382 |
[32m[20230113 19:56:58 @agent_ppo2.py:186][0m |          -0.0096 |           3.7644 |           4.3351 |
[32m[20230113 19:56:58 @agent_ppo2.py:186][0m |          -0.0105 |           3.6447 |           4.3352 |
[32m[20230113 19:56:58 @agent_ppo2.py:186][0m |          -0.0101 |           3.5705 |           4.3339 |
[32m[20230113 19:56:58 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:56:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.95
[32m[20230113 19:56:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.04
[32m[20230113 19:56:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.07
[32m[20230113 19:56:59 @agent_ppo2.py:144][0m Total time:      12.43 min
[32m[20230113 19:56:59 @agent_ppo2.py:146][0m 1112064 total steps have happened
[32m[20230113 19:56:59 @agent_ppo2.py:122][0m #------------------------ Iteration 543 --------------------------#
[32m[20230113 19:56:59 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:56:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:56:59 @agent_ppo2.py:186][0m |           0.0023 |          17.3440 |           4.2731 |
[32m[20230113 19:56:59 @agent_ppo2.py:186][0m |          -0.0048 |           9.6221 |           4.2760 |
[32m[20230113 19:56:59 @agent_ppo2.py:186][0m |          -0.0061 |           8.5712 |           4.2760 |
[32m[20230113 19:56:59 @agent_ppo2.py:186][0m |          -0.0077 |           7.8485 |           4.2760 |
[32m[20230113 19:56:59 @agent_ppo2.py:186][0m |          -0.0076 |           7.2537 |           4.2747 |
[32m[20230113 19:56:59 @agent_ppo2.py:186][0m |          -0.0095 |           7.3928 |           4.2764 |
[32m[20230113 19:56:59 @agent_ppo2.py:186][0m |          -0.0103 |           6.6746 |           4.2751 |
[32m[20230113 19:56:59 @agent_ppo2.py:186][0m |          -0.0105 |           6.4926 |           4.2766 |
[32m[20230113 19:56:59 @agent_ppo2.py:186][0m |          -0.0116 |           6.2013 |           4.2768 |
[32m[20230113 19:57:00 @agent_ppo2.py:186][0m |          -0.0108 |           5.9576 |           4.2780 |
[32m[20230113 19:57:00 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 169.29
[32m[20230113 19:57:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.23
[32m[20230113 19:57:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.83
[32m[20230113 19:57:00 @agent_ppo2.py:144][0m Total time:      12.46 min
[32m[20230113 19:57:00 @agent_ppo2.py:146][0m 1114112 total steps have happened
[32m[20230113 19:57:00 @agent_ppo2.py:122][0m #------------------------ Iteration 544 --------------------------#
[32m[20230113 19:57:00 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:00 @agent_ppo2.py:186][0m |          -0.0055 |           6.7601 |           4.2260 |
[32m[20230113 19:57:01 @agent_ppo2.py:186][0m |          -0.0026 |           5.7970 |           4.2178 |
[32m[20230113 19:57:01 @agent_ppo2.py:186][0m |          -0.0076 |           5.2877 |           4.2149 |
[32m[20230113 19:57:01 @agent_ppo2.py:186][0m |          -0.0089 |           4.9135 |           4.2137 |
[32m[20230113 19:57:01 @agent_ppo2.py:186][0m |          -0.0117 |           4.6198 |           4.2127 |
[32m[20230113 19:57:01 @agent_ppo2.py:186][0m |          -0.0171 |           4.4368 |           4.2155 |
[32m[20230113 19:57:01 @agent_ppo2.py:186][0m |          -0.0138 |           4.2332 |           4.2115 |
[32m[20230113 19:57:01 @agent_ppo2.py:186][0m |          -0.0139 |           4.0487 |           4.2082 |
[32m[20230113 19:57:01 @agent_ppo2.py:186][0m |          -0.0107 |           3.9475 |           4.2087 |
[32m[20230113 19:57:01 @agent_ppo2.py:186][0m |          -0.0133 |           3.8334 |           4.2077 |
[32m[20230113 19:57:01 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.99
[32m[20230113 19:57:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.64
[32m[20230113 19:57:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.92
[32m[20230113 19:57:01 @agent_ppo2.py:144][0m Total time:      12.48 min
[32m[20230113 19:57:01 @agent_ppo2.py:146][0m 1116160 total steps have happened
[32m[20230113 19:57:01 @agent_ppo2.py:122][0m #------------------------ Iteration 545 --------------------------#
[32m[20230113 19:57:02 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:02 @agent_ppo2.py:186][0m |           0.0019 |           6.3331 |           4.2098 |
[32m[20230113 19:57:02 @agent_ppo2.py:186][0m |           0.0000 |           5.8696 |           4.2087 |
[32m[20230113 19:57:02 @agent_ppo2.py:186][0m |          -0.0087 |           5.3783 |           4.2079 |
[32m[20230113 19:57:02 @agent_ppo2.py:186][0m |          -0.0073 |           5.2198 |           4.2082 |
[32m[20230113 19:57:02 @agent_ppo2.py:186][0m |          -0.0077 |           5.2405 |           4.2085 |
[32m[20230113 19:57:02 @agent_ppo2.py:186][0m |          -0.0099 |           5.0074 |           4.2071 |
[32m[20230113 19:57:02 @agent_ppo2.py:186][0m |          -0.0117 |           4.9054 |           4.2050 |
[32m[20230113 19:57:02 @agent_ppo2.py:186][0m |          -0.0135 |           4.8446 |           4.2083 |
[32m[20230113 19:57:02 @agent_ppo2.py:186][0m |          -0.0106 |           4.8015 |           4.2099 |
[32m[20230113 19:57:02 @agent_ppo2.py:186][0m |          -0.0158 |           4.6625 |           4.2078 |
[32m[20230113 19:57:02 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.43
[32m[20230113 19:57:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.35
[32m[20230113 19:57:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.25
[32m[20230113 19:57:03 @agent_ppo2.py:144][0m Total time:      12.50 min
[32m[20230113 19:57:03 @agent_ppo2.py:146][0m 1118208 total steps have happened
[32m[20230113 19:57:03 @agent_ppo2.py:122][0m #------------------------ Iteration 546 --------------------------#
[32m[20230113 19:57:03 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:03 @agent_ppo2.py:186][0m |           0.0007 |           5.2098 |           4.2997 |
[32m[20230113 19:57:03 @agent_ppo2.py:186][0m |          -0.0064 |           4.6067 |           4.3001 |
[32m[20230113 19:57:03 @agent_ppo2.py:186][0m |          -0.0071 |           4.1894 |           4.2953 |
[32m[20230113 19:57:03 @agent_ppo2.py:186][0m |          -0.0080 |           4.0022 |           4.2938 |
[32m[20230113 19:57:03 @agent_ppo2.py:186][0m |          -0.0071 |           3.7679 |           4.2931 |
[32m[20230113 19:57:03 @agent_ppo2.py:186][0m |          -0.0090 |           3.6667 |           4.2951 |
[32m[20230113 19:57:03 @agent_ppo2.py:186][0m |          -0.0102 |           3.5391 |           4.2927 |
[32m[20230113 19:57:03 @agent_ppo2.py:186][0m |          -0.0099 |           3.5135 |           4.2898 |
[32m[20230113 19:57:04 @agent_ppo2.py:186][0m |          -0.0061 |           3.4302 |           4.2913 |
[32m[20230113 19:57:04 @agent_ppo2.py:186][0m |          -0.0097 |           3.3972 |           4.2854 |
[32m[20230113 19:57:04 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:57:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.20
[32m[20230113 19:57:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.34
[32m[20230113 19:57:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.73
[32m[20230113 19:57:04 @agent_ppo2.py:144][0m Total time:      12.52 min
[32m[20230113 19:57:04 @agent_ppo2.py:146][0m 1120256 total steps have happened
[32m[20230113 19:57:04 @agent_ppo2.py:122][0m #------------------------ Iteration 547 --------------------------#
[32m[20230113 19:57:04 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:57:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:05 @agent_ppo2.py:186][0m |          -0.0002 |           5.2355 |           4.2582 |
[32m[20230113 19:57:05 @agent_ppo2.py:186][0m |          -0.0052 |           4.7722 |           4.2556 |
[32m[20230113 19:57:05 @agent_ppo2.py:186][0m |          -0.0058 |           4.8317 |           4.2526 |
[32m[20230113 19:57:05 @agent_ppo2.py:186][0m |          -0.0106 |           4.4926 |           4.2499 |
[32m[20230113 19:57:05 @agent_ppo2.py:186][0m |          -0.0079 |           4.4498 |           4.2499 |
[32m[20230113 19:57:05 @agent_ppo2.py:186][0m |          -0.0093 |           4.2992 |           4.2499 |
[32m[20230113 19:57:05 @agent_ppo2.py:186][0m |          -0.0102 |           4.2325 |           4.2489 |
[32m[20230113 19:57:05 @agent_ppo2.py:186][0m |          -0.0123 |           4.1535 |           4.2493 |
[32m[20230113 19:57:05 @agent_ppo2.py:186][0m |          -0.0108 |           4.1842 |           4.2513 |
[32m[20230113 19:57:05 @agent_ppo2.py:186][0m |          -0.0130 |           4.0794 |           4.2498 |
[32m[20230113 19:57:05 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.31
[32m[20230113 19:57:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.77
[32m[20230113 19:57:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.78
[32m[20230113 19:57:05 @agent_ppo2.py:144][0m Total time:      12.55 min
[32m[20230113 19:57:05 @agent_ppo2.py:146][0m 1122304 total steps have happened
[32m[20230113 19:57:05 @agent_ppo2.py:122][0m #------------------------ Iteration 548 --------------------------#
[32m[20230113 19:57:06 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:57:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:06 @agent_ppo2.py:186][0m |          -0.0009 |           5.7026 |           4.2426 |
[32m[20230113 19:57:06 @agent_ppo2.py:186][0m |          -0.0048 |           5.2501 |           4.2424 |
[32m[20230113 19:57:06 @agent_ppo2.py:186][0m |          -0.0069 |           4.8665 |           4.2430 |
[32m[20230113 19:57:06 @agent_ppo2.py:186][0m |          -0.0076 |           4.6087 |           4.2409 |
[32m[20230113 19:57:06 @agent_ppo2.py:186][0m |          -0.0103 |           4.4339 |           4.2400 |
[32m[20230113 19:57:06 @agent_ppo2.py:186][0m |          -0.0102 |           4.3484 |           4.2419 |
[32m[20230113 19:57:06 @agent_ppo2.py:186][0m |          -0.0110 |           4.2554 |           4.2388 |
[32m[20230113 19:57:06 @agent_ppo2.py:186][0m |          -0.0113 |           4.2041 |           4.2378 |
[32m[20230113 19:57:06 @agent_ppo2.py:186][0m |          -0.0120 |           4.1718 |           4.2403 |
[32m[20230113 19:57:06 @agent_ppo2.py:186][0m |          -0.0126 |           4.1147 |           4.2374 |
[32m[20230113 19:57:06 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:57:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.87
[32m[20230113 19:57:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.18
[32m[20230113 19:57:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.16
[32m[20230113 19:57:07 @agent_ppo2.py:144][0m Total time:      12.57 min
[32m[20230113 19:57:07 @agent_ppo2.py:146][0m 1124352 total steps have happened
[32m[20230113 19:57:07 @agent_ppo2.py:122][0m #------------------------ Iteration 549 --------------------------#
[32m[20230113 19:57:07 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:07 @agent_ppo2.py:186][0m |           0.0003 |           6.3925 |           4.2944 |
[32m[20230113 19:57:07 @agent_ppo2.py:186][0m |          -0.0075 |           5.6252 |           4.2874 |
[32m[20230113 19:57:07 @agent_ppo2.py:186][0m |          -0.0074 |           5.3864 |           4.2812 |
[32m[20230113 19:57:07 @agent_ppo2.py:186][0m |          -0.0096 |           5.2266 |           4.2844 |
[32m[20230113 19:57:07 @agent_ppo2.py:186][0m |          -0.0100 |           5.1560 |           4.2779 |
[32m[20230113 19:57:07 @agent_ppo2.py:186][0m |          -0.0126 |           4.9891 |           4.2810 |
[32m[20230113 19:57:07 @agent_ppo2.py:186][0m |          -0.0131 |           4.8952 |           4.2744 |
[32m[20230113 19:57:07 @agent_ppo2.py:186][0m |          -0.0112 |           4.8710 |           4.2749 |
[32m[20230113 19:57:08 @agent_ppo2.py:186][0m |          -0.0111 |           4.8059 |           4.2742 |
[32m[20230113 19:57:08 @agent_ppo2.py:186][0m |          -0.0160 |           4.6713 |           4.2718 |
[32m[20230113 19:57:08 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:57:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.91
[32m[20230113 19:57:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.97
[32m[20230113 19:57:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.34
[32m[20230113 19:57:08 @agent_ppo2.py:144][0m Total time:      12.59 min
[32m[20230113 19:57:08 @agent_ppo2.py:146][0m 1126400 total steps have happened
[32m[20230113 19:57:08 @agent_ppo2.py:122][0m #------------------------ Iteration 550 --------------------------#
[32m[20230113 19:57:08 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:09 @agent_ppo2.py:186][0m |           0.0001 |           6.9302 |           4.3399 |
[32m[20230113 19:57:09 @agent_ppo2.py:186][0m |          -0.0082 |           6.4005 |           4.3373 |
[32m[20230113 19:57:09 @agent_ppo2.py:186][0m |          -0.0084 |           6.1796 |           4.3367 |
[32m[20230113 19:57:09 @agent_ppo2.py:186][0m |          -0.0042 |           6.0372 |           4.3366 |
[32m[20230113 19:57:09 @agent_ppo2.py:186][0m |          -0.0098 |           5.9067 |           4.3383 |
[32m[20230113 19:57:09 @agent_ppo2.py:186][0m |          -0.0069 |           5.8767 |           4.3369 |
[32m[20230113 19:57:09 @agent_ppo2.py:186][0m |          -0.0096 |           5.7904 |           4.3330 |
[32m[20230113 19:57:09 @agent_ppo2.py:186][0m |          -0.0067 |           5.7305 |           4.3331 |
[32m[20230113 19:57:09 @agent_ppo2.py:186][0m |          -0.0044 |           5.6329 |           4.3351 |
[32m[20230113 19:57:09 @agent_ppo2.py:186][0m |          -0.0117 |           5.6191 |           4.3383 |
[32m[20230113 19:57:09 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:57:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 215.85
[32m[20230113 19:57:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.24
[32m[20230113 19:57:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.87
[32m[20230113 19:57:09 @agent_ppo2.py:144][0m Total time:      12.61 min
[32m[20230113 19:57:09 @agent_ppo2.py:146][0m 1128448 total steps have happened
[32m[20230113 19:57:09 @agent_ppo2.py:122][0m #------------------------ Iteration 551 --------------------------#
[32m[20230113 19:57:10 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:10 @agent_ppo2.py:186][0m |          -0.0041 |           6.1390 |           4.3564 |
[32m[20230113 19:57:10 @agent_ppo2.py:186][0m |          -0.0139 |           5.6369 |           4.3417 |
[32m[20230113 19:57:10 @agent_ppo2.py:186][0m |          -0.0102 |           5.2795 |           4.3485 |
[32m[20230113 19:57:10 @agent_ppo2.py:186][0m |          -0.0129 |           5.0041 |           4.3454 |
[32m[20230113 19:57:10 @agent_ppo2.py:186][0m |          -0.0122 |           4.7520 |           4.3401 |
[32m[20230113 19:57:10 @agent_ppo2.py:186][0m |          -0.0126 |           4.5325 |           4.3428 |
[32m[20230113 19:57:10 @agent_ppo2.py:186][0m |          -0.0117 |           4.3471 |           4.3404 |
[32m[20230113 19:57:10 @agent_ppo2.py:186][0m |          -0.0123 |           4.1873 |           4.3413 |
[32m[20230113 19:57:10 @agent_ppo2.py:186][0m |          -0.0083 |           4.0760 |           4.3376 |
[32m[20230113 19:57:10 @agent_ppo2.py:186][0m |          -0.0115 |           3.8888 |           4.3339 |
[32m[20230113 19:57:10 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:57:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.56
[32m[20230113 19:57:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.38
[32m[20230113 19:57:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.49
[32m[20230113 19:57:11 @agent_ppo2.py:144][0m Total time:      12.64 min
[32m[20230113 19:57:11 @agent_ppo2.py:146][0m 1130496 total steps have happened
[32m[20230113 19:57:11 @agent_ppo2.py:122][0m #------------------------ Iteration 552 --------------------------#
[32m[20230113 19:57:11 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:57:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:11 @agent_ppo2.py:186][0m |          -0.0030 |           7.0334 |           4.2985 |
[32m[20230113 19:57:11 @agent_ppo2.py:186][0m |          -0.0098 |           6.4414 |           4.2882 |
[32m[20230113 19:57:11 @agent_ppo2.py:186][0m |          -0.0070 |           6.4252 |           4.2866 |
[32m[20230113 19:57:11 @agent_ppo2.py:186][0m |          -0.0064 |           6.2234 |           4.2839 |
[32m[20230113 19:57:11 @agent_ppo2.py:186][0m |          -0.0078 |           6.0169 |           4.2827 |
[32m[20230113 19:57:11 @agent_ppo2.py:186][0m |          -0.0140 |           5.8109 |           4.2820 |
[32m[20230113 19:57:12 @agent_ppo2.py:186][0m |          -0.0115 |           5.7025 |           4.2817 |
[32m[20230113 19:57:12 @agent_ppo2.py:186][0m |          -0.0120 |           5.7009 |           4.2823 |
[32m[20230113 19:57:12 @agent_ppo2.py:186][0m |          -0.0130 |           5.5601 |           4.2827 |
[32m[20230113 19:57:12 @agent_ppo2.py:186][0m |          -0.0146 |           5.5663 |           4.2800 |
[32m[20230113 19:57:12 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:57:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.86
[32m[20230113 19:57:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.97
[32m[20230113 19:57:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.06
[32m[20230113 19:57:12 @agent_ppo2.py:144][0m Total time:      12.66 min
[32m[20230113 19:57:12 @agent_ppo2.py:146][0m 1132544 total steps have happened
[32m[20230113 19:57:12 @agent_ppo2.py:122][0m #------------------------ Iteration 553 --------------------------#
[32m[20230113 19:57:12 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 19:57:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:13 @agent_ppo2.py:186][0m |          -0.0017 |          17.0237 |           4.3410 |
[32m[20230113 19:57:13 @agent_ppo2.py:186][0m |          -0.0105 |           6.8840 |           4.3283 |
[32m[20230113 19:57:13 @agent_ppo2.py:186][0m |          -0.0130 |           5.6678 |           4.3275 |
[32m[20230113 19:57:13 @agent_ppo2.py:186][0m |          -0.0142 |           5.2169 |           4.3278 |
[32m[20230113 19:57:13 @agent_ppo2.py:186][0m |          -0.0122 |           4.8919 |           4.3173 |
[32m[20230113 19:57:13 @agent_ppo2.py:186][0m |          -0.0142 |           4.6026 |           4.3211 |
[32m[20230113 19:57:13 @agent_ppo2.py:186][0m |          -0.0149 |           4.3793 |           4.3222 |
[32m[20230113 19:57:13 @agent_ppo2.py:186][0m |          -0.0128 |           4.3020 |           4.3131 |
[32m[20230113 19:57:13 @agent_ppo2.py:186][0m |          -0.0169 |           4.1740 |           4.3155 |
[32m[20230113 19:57:13 @agent_ppo2.py:186][0m |          -0.0175 |           4.0491 |           4.3172 |
[32m[20230113 19:57:13 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 19:57:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 119.36
[32m[20230113 19:57:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.27
[32m[20230113 19:57:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.21
[32m[20230113 19:57:13 @agent_ppo2.py:144][0m Total time:      12.68 min
[32m[20230113 19:57:13 @agent_ppo2.py:146][0m 1134592 total steps have happened
[32m[20230113 19:57:13 @agent_ppo2.py:122][0m #------------------------ Iteration 554 --------------------------#
[32m[20230113 19:57:14 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:14 @agent_ppo2.py:186][0m |          -0.0004 |           5.5933 |           4.3616 |
[32m[20230113 19:57:14 @agent_ppo2.py:186][0m |           0.0015 |           5.5715 |           4.3561 |
[32m[20230113 19:57:14 @agent_ppo2.py:186][0m |          -0.0103 |           4.9646 |           4.3457 |
[32m[20230113 19:57:14 @agent_ppo2.py:186][0m |          -0.0085 |           4.8303 |           4.3452 |
[32m[20230113 19:57:14 @agent_ppo2.py:186][0m |          -0.0107 |           4.7242 |           4.3480 |
[32m[20230113 19:57:14 @agent_ppo2.py:186][0m |          -0.0131 |           4.6714 |           4.3438 |
[32m[20230113 19:57:14 @agent_ppo2.py:186][0m |          -0.0109 |           4.6409 |           4.3452 |
[32m[20230113 19:57:14 @agent_ppo2.py:186][0m |          -0.0117 |           4.5572 |           4.3434 |
[32m[20230113 19:57:14 @agent_ppo2.py:186][0m |          -0.0059 |           4.6482 |           4.3419 |
[32m[20230113 19:57:14 @agent_ppo2.py:186][0m |          -0.0134 |           4.4816 |           4.3368 |
[32m[20230113 19:57:14 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.07
[32m[20230113 19:57:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.59
[32m[20230113 19:57:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.78
[32m[20230113 19:57:15 @agent_ppo2.py:144][0m Total time:      12.70 min
[32m[20230113 19:57:15 @agent_ppo2.py:146][0m 1136640 total steps have happened
[32m[20230113 19:57:15 @agent_ppo2.py:122][0m #------------------------ Iteration 555 --------------------------#
[32m[20230113 19:57:15 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:15 @agent_ppo2.py:186][0m |          -0.0282 |           6.2100 |           4.2467 |
[32m[20230113 19:57:15 @agent_ppo2.py:186][0m |          -0.0092 |           5.6362 |           4.2431 |
[32m[20230113 19:57:15 @agent_ppo2.py:186][0m |          -0.0009 |           5.4255 |           4.2408 |
[32m[20230113 19:57:15 @agent_ppo2.py:186][0m |          -0.0064 |           5.2474 |           4.2371 |
[32m[20230113 19:57:15 @agent_ppo2.py:186][0m |          -0.0109 |           5.1326 |           4.2370 |
[32m[20230113 19:57:15 @agent_ppo2.py:186][0m |          -0.0192 |           5.0429 |           4.2346 |
[32m[20230113 19:57:15 @agent_ppo2.py:186][0m |          -0.0091 |           4.9492 |           4.2341 |
[32m[20230113 19:57:15 @agent_ppo2.py:186][0m |          -0.0160 |           4.9095 |           4.2332 |
[32m[20230113 19:57:15 @agent_ppo2.py:186][0m |          -0.0123 |           4.8161 |           4.2354 |
[32m[20230113 19:57:16 @agent_ppo2.py:186][0m |          -0.0181 |           4.7748 |           4.2372 |
[32m[20230113 19:57:16 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:57:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.49
[32m[20230113 19:57:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.77
[32m[20230113 19:57:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.12
[32m[20230113 19:57:16 @agent_ppo2.py:144][0m Total time:      12.72 min
[32m[20230113 19:57:16 @agent_ppo2.py:146][0m 1138688 total steps have happened
[32m[20230113 19:57:16 @agent_ppo2.py:122][0m #------------------------ Iteration 556 --------------------------#
[32m[20230113 19:57:16 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:16 @agent_ppo2.py:186][0m |           0.0106 |           7.9844 |           4.3385 |
[32m[20230113 19:57:17 @agent_ppo2.py:186][0m |          -0.0103 |           6.1575 |           4.3374 |
[32m[20230113 19:57:17 @agent_ppo2.py:186][0m |          -0.0073 |           5.8862 |           4.3337 |
[32m[20230113 19:57:17 @agent_ppo2.py:186][0m |          -0.0136 |           5.7687 |           4.3368 |
[32m[20230113 19:57:17 @agent_ppo2.py:186][0m |          -0.0084 |           5.7232 |           4.3373 |
[32m[20230113 19:57:17 @agent_ppo2.py:186][0m |          -0.0038 |           5.8875 |           4.3366 |
[32m[20230113 19:57:17 @agent_ppo2.py:186][0m |          -0.0142 |           5.5365 |           4.3362 |
[32m[20230113 19:57:17 @agent_ppo2.py:186][0m |          -0.0194 |           5.4969 |           4.3382 |
[32m[20230113 19:57:17 @agent_ppo2.py:186][0m |          -0.0137 |           5.4533 |           4.3350 |
[32m[20230113 19:57:17 @agent_ppo2.py:186][0m |          -0.0157 |           5.3931 |           4.3360 |
[32m[20230113 19:57:17 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:57:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.09
[32m[20230113 19:57:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.78
[32m[20230113 19:57:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.79
[32m[20230113 19:57:17 @agent_ppo2.py:144][0m Total time:      12.75 min
[32m[20230113 19:57:17 @agent_ppo2.py:146][0m 1140736 total steps have happened
[32m[20230113 19:57:17 @agent_ppo2.py:122][0m #------------------------ Iteration 557 --------------------------#
[32m[20230113 19:57:18 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:18 @agent_ppo2.py:186][0m |           0.0006 |           5.7816 |           4.3467 |
[32m[20230113 19:57:18 @agent_ppo2.py:186][0m |          -0.0002 |           5.5182 |           4.3388 |
[32m[20230113 19:57:18 @agent_ppo2.py:186][0m |          -0.0062 |           5.1264 |           4.3378 |
[32m[20230113 19:57:18 @agent_ppo2.py:186][0m |          -0.0083 |           4.9365 |           4.3388 |
[32m[20230113 19:57:18 @agent_ppo2.py:186][0m |          -0.0055 |           4.9074 |           4.3347 |
[32m[20230113 19:57:18 @agent_ppo2.py:186][0m |          -0.0064 |           4.9486 |           4.3324 |
[32m[20230113 19:57:18 @agent_ppo2.py:186][0m |          -0.0071 |           4.7634 |           4.3298 |
[32m[20230113 19:57:18 @agent_ppo2.py:186][0m |          -0.0081 |           4.7212 |           4.3335 |
[32m[20230113 19:57:18 @agent_ppo2.py:186][0m |          -0.0072 |           4.7179 |           4.3306 |
[32m[20230113 19:57:18 @agent_ppo2.py:186][0m |          -0.0104 |           4.6037 |           4.3284 |
[32m[20230113 19:57:18 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.55
[32m[20230113 19:57:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.24
[32m[20230113 19:57:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.37
[32m[20230113 19:57:19 @agent_ppo2.py:144][0m Total time:      12.77 min
[32m[20230113 19:57:19 @agent_ppo2.py:146][0m 1142784 total steps have happened
[32m[20230113 19:57:19 @agent_ppo2.py:122][0m #------------------------ Iteration 558 --------------------------#
[32m[20230113 19:57:19 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:19 @agent_ppo2.py:186][0m |           0.0006 |           5.1454 |           4.3021 |
[32m[20230113 19:57:19 @agent_ppo2.py:186][0m |          -0.0045 |           4.3527 |           4.2968 |
[32m[20230113 19:57:19 @agent_ppo2.py:186][0m |          -0.0072 |           3.9915 |           4.2963 |
[32m[20230113 19:57:19 @agent_ppo2.py:186][0m |          -0.0070 |           3.7589 |           4.2992 |
[32m[20230113 19:57:19 @agent_ppo2.py:186][0m |          -0.0084 |           3.5575 |           4.2939 |
[32m[20230113 19:57:19 @agent_ppo2.py:186][0m |          -0.0094 |           3.4143 |           4.3001 |
[32m[20230113 19:57:19 @agent_ppo2.py:186][0m |          -0.0101 |           3.3030 |           4.2967 |
[32m[20230113 19:57:19 @agent_ppo2.py:186][0m |          -0.0112 |           3.1962 |           4.2965 |
[32m[20230113 19:57:20 @agent_ppo2.py:186][0m |          -0.0090 |           3.1890 |           4.2963 |
[32m[20230113 19:57:20 @agent_ppo2.py:186][0m |          -0.0121 |           3.0364 |           4.2925 |
[32m[20230113 19:57:20 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:57:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.74
[32m[20230113 19:57:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.04
[32m[20230113 19:57:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.25
[32m[20230113 19:57:20 @agent_ppo2.py:144][0m Total time:      12.79 min
[32m[20230113 19:57:20 @agent_ppo2.py:146][0m 1144832 total steps have happened
[32m[20230113 19:57:20 @agent_ppo2.py:122][0m #------------------------ Iteration 559 --------------------------#
[32m[20230113 19:57:20 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:21 @agent_ppo2.py:186][0m |           0.0010 |           5.0674 |           4.4412 |
[32m[20230113 19:57:21 @agent_ppo2.py:186][0m |          -0.0035 |           4.3556 |           4.4354 |
[32m[20230113 19:57:21 @agent_ppo2.py:186][0m |          -0.0052 |           4.0245 |           4.4321 |
[32m[20230113 19:57:21 @agent_ppo2.py:186][0m |          -0.0068 |           3.7265 |           4.4334 |
[32m[20230113 19:57:21 @agent_ppo2.py:186][0m |          -0.0080 |           3.5126 |           4.4351 |
[32m[20230113 19:57:21 @agent_ppo2.py:186][0m |          -0.0083 |           3.3846 |           4.4324 |
[32m[20230113 19:57:21 @agent_ppo2.py:186][0m |          -0.0091 |           3.2201 |           4.4337 |
[32m[20230113 19:57:21 @agent_ppo2.py:186][0m |          -0.0105 |           3.0956 |           4.4311 |
[32m[20230113 19:57:21 @agent_ppo2.py:186][0m |          -0.0100 |           2.9453 |           4.4329 |
[32m[20230113 19:57:21 @agent_ppo2.py:186][0m |          -0.0118 |           2.8434 |           4.4334 |
[32m[20230113 19:57:21 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.14
[32m[20230113 19:57:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.87
[32m[20230113 19:57:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.83
[32m[20230113 19:57:21 @agent_ppo2.py:144][0m Total time:      12.81 min
[32m[20230113 19:57:21 @agent_ppo2.py:146][0m 1146880 total steps have happened
[32m[20230113 19:57:21 @agent_ppo2.py:122][0m #------------------------ Iteration 560 --------------------------#
[32m[20230113 19:57:22 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:22 @agent_ppo2.py:186][0m |           0.0019 |           6.6887 |           4.2859 |
[32m[20230113 19:57:22 @agent_ppo2.py:186][0m |          -0.0054 |           5.4769 |           4.2787 |
[32m[20230113 19:57:22 @agent_ppo2.py:186][0m |          -0.0101 |           5.1711 |           4.2806 |
[32m[20230113 19:57:22 @agent_ppo2.py:186][0m |          -0.0105 |           5.0112 |           4.2811 |
[32m[20230113 19:57:22 @agent_ppo2.py:186][0m |          -0.0112 |           4.9074 |           4.2806 |
[32m[20230113 19:57:22 @agent_ppo2.py:186][0m |          -0.0134 |           4.8754 |           4.2829 |
[32m[20230113 19:57:22 @agent_ppo2.py:186][0m |          -0.0151 |           4.7782 |           4.2830 |
[32m[20230113 19:57:22 @agent_ppo2.py:186][0m |          -0.0082 |           4.8938 |           4.2848 |
[32m[20230113 19:57:22 @agent_ppo2.py:186][0m |          -0.0114 |           4.7374 |           4.2855 |
[32m[20230113 19:57:22 @agent_ppo2.py:186][0m |          -0.0130 |           4.7178 |           4.2844 |
[32m[20230113 19:57:22 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:57:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.03
[32m[20230113 19:57:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.74
[32m[20230113 19:57:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.73
[32m[20230113 19:57:23 @agent_ppo2.py:144][0m Total time:      12.84 min
[32m[20230113 19:57:23 @agent_ppo2.py:146][0m 1148928 total steps have happened
[32m[20230113 19:57:23 @agent_ppo2.py:122][0m #------------------------ Iteration 561 --------------------------#
[32m[20230113 19:57:23 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:57:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:23 @agent_ppo2.py:186][0m |           0.0013 |           5.7677 |           4.3166 |
[32m[20230113 19:57:23 @agent_ppo2.py:186][0m |          -0.0026 |           5.1469 |           4.3162 |
[32m[20230113 19:57:23 @agent_ppo2.py:186][0m |          -0.0041 |           4.9822 |           4.3206 |
[32m[20230113 19:57:23 @agent_ppo2.py:186][0m |          -0.0078 |           4.8040 |           4.3212 |
[32m[20230113 19:57:23 @agent_ppo2.py:186][0m |          -0.0056 |           4.7377 |           4.3211 |
[32m[20230113 19:57:24 @agent_ppo2.py:186][0m |          -0.0095 |           4.6160 |           4.3202 |
[32m[20230113 19:57:24 @agent_ppo2.py:186][0m |          -0.0108 |           4.5093 |           4.3231 |
[32m[20230113 19:57:24 @agent_ppo2.py:186][0m |          -0.0113 |           4.4840 |           4.3228 |
[32m[20230113 19:57:24 @agent_ppo2.py:186][0m |          -0.0109 |           4.4514 |           4.3239 |
[32m[20230113 19:57:24 @agent_ppo2.py:186][0m |          -0.0125 |           4.3377 |           4.3234 |
[32m[20230113 19:57:24 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:57:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.23
[32m[20230113 19:57:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.58
[32m[20230113 19:57:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.07
[32m[20230113 19:57:24 @agent_ppo2.py:144][0m Total time:      12.86 min
[32m[20230113 19:57:24 @agent_ppo2.py:146][0m 1150976 total steps have happened
[32m[20230113 19:57:24 @agent_ppo2.py:122][0m #------------------------ Iteration 562 --------------------------#
[32m[20230113 19:57:25 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:25 @agent_ppo2.py:186][0m |          -0.0018 |           5.6574 |           4.3092 |
[32m[20230113 19:57:25 @agent_ppo2.py:186][0m |          -0.0065 |           5.3428 |           4.3043 |
[32m[20230113 19:57:25 @agent_ppo2.py:186][0m |           0.0099 |           5.4446 |           4.3052 |
[32m[20230113 19:57:25 @agent_ppo2.py:186][0m |           0.0014 |           5.2494 |           4.3014 |
[32m[20230113 19:57:25 @agent_ppo2.py:186][0m |          -0.0004 |           5.0613 |           4.3058 |
[32m[20230113 19:57:25 @agent_ppo2.py:186][0m |          -0.0112 |           4.9861 |           4.3055 |
[32m[20230113 19:57:25 @agent_ppo2.py:186][0m |          -0.0071 |           4.9506 |           4.3045 |
[32m[20230113 19:57:25 @agent_ppo2.py:186][0m |          -0.0048 |           4.8602 |           4.3046 |
[32m[20230113 19:57:25 @agent_ppo2.py:186][0m |          -0.0146 |           4.8165 |           4.3048 |
[32m[20230113 19:57:25 @agent_ppo2.py:186][0m |          -0.0093 |           4.7992 |           4.3062 |
[32m[20230113 19:57:25 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:57:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.70
[32m[20230113 19:57:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.83
[32m[20230113 19:57:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.75
[32m[20230113 19:57:26 @agent_ppo2.py:144][0m Total time:      12.88 min
[32m[20230113 19:57:26 @agent_ppo2.py:146][0m 1153024 total steps have happened
[32m[20230113 19:57:26 @agent_ppo2.py:122][0m #------------------------ Iteration 563 --------------------------#
[32m[20230113 19:57:26 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:26 @agent_ppo2.py:186][0m |          -0.0005 |           5.0598 |           4.3249 |
[32m[20230113 19:57:26 @agent_ppo2.py:186][0m |          -0.0040 |           4.7224 |           4.3238 |
[32m[20230113 19:57:26 @agent_ppo2.py:186][0m |          -0.0065 |           4.4716 |           4.3230 |
[32m[20230113 19:57:26 @agent_ppo2.py:186][0m |          -0.0078 |           4.3063 |           4.3209 |
[32m[20230113 19:57:26 @agent_ppo2.py:186][0m |          -0.0081 |           4.2067 |           4.3200 |
[32m[20230113 19:57:26 @agent_ppo2.py:186][0m |          -0.0081 |           4.1693 |           4.3139 |
[32m[20230113 19:57:26 @agent_ppo2.py:186][0m |          -0.0097 |           4.0423 |           4.3158 |
[32m[20230113 19:57:26 @agent_ppo2.py:186][0m |          -0.0093 |           4.0196 |           4.3140 |
[32m[20230113 19:57:26 @agent_ppo2.py:186][0m |          -0.0102 |           3.9377 |           4.3136 |
[32m[20230113 19:57:26 @agent_ppo2.py:186][0m |          -0.0105 |           3.8827 |           4.3133 |
[32m[20230113 19:57:26 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.54
[32m[20230113 19:57:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.38
[32m[20230113 19:57:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.04
[32m[20230113 19:57:27 @agent_ppo2.py:144][0m Total time:      12.91 min
[32m[20230113 19:57:27 @agent_ppo2.py:146][0m 1155072 total steps have happened
[32m[20230113 19:57:27 @agent_ppo2.py:122][0m #------------------------ Iteration 564 --------------------------#
[32m[20230113 19:57:27 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:57:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:27 @agent_ppo2.py:186][0m |           0.0024 |           6.5512 |           4.3713 |
[32m[20230113 19:57:27 @agent_ppo2.py:186][0m |          -0.0052 |           5.4628 |           4.3662 |
[32m[20230113 19:57:28 @agent_ppo2.py:186][0m |          -0.0044 |           5.2170 |           4.3589 |
[32m[20230113 19:57:28 @agent_ppo2.py:186][0m |          -0.0061 |           4.8616 |           4.3554 |
[32m[20230113 19:57:28 @agent_ppo2.py:186][0m |          -0.0084 |           4.6947 |           4.3556 |
[32m[20230113 19:57:28 @agent_ppo2.py:186][0m |          -0.0075 |           4.5563 |           4.3555 |
[32m[20230113 19:57:28 @agent_ppo2.py:186][0m |          -0.0108 |           4.3942 |           4.3527 |
[32m[20230113 19:57:28 @agent_ppo2.py:186][0m |          -0.0091 |           4.4215 |           4.3551 |
[32m[20230113 19:57:28 @agent_ppo2.py:186][0m |          -0.0091 |           4.2220 |           4.3545 |
[32m[20230113 19:57:28 @agent_ppo2.py:186][0m |          -0.0126 |           4.1221 |           4.3553 |
[32m[20230113 19:57:28 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:57:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.82
[32m[20230113 19:57:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.15
[32m[20230113 19:57:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.33
[32m[20230113 19:57:28 @agent_ppo2.py:144][0m Total time:      12.93 min
[32m[20230113 19:57:28 @agent_ppo2.py:146][0m 1157120 total steps have happened
[32m[20230113 19:57:28 @agent_ppo2.py:122][0m #------------------------ Iteration 565 --------------------------#
[32m[20230113 19:57:29 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:29 @agent_ppo2.py:186][0m |          -0.0024 |           5.2797 |           4.3848 |
[32m[20230113 19:57:29 @agent_ppo2.py:186][0m |          -0.0080 |           4.6445 |           4.3783 |
[32m[20230113 19:57:29 @agent_ppo2.py:186][0m |          -0.0095 |           4.4622 |           4.3769 |
[32m[20230113 19:57:29 @agent_ppo2.py:186][0m |          -0.0101 |           4.3611 |           4.3688 |
[32m[20230113 19:57:29 @agent_ppo2.py:186][0m |          -0.0141 |           4.3009 |           4.3679 |
[32m[20230113 19:57:29 @agent_ppo2.py:186][0m |          -0.0040 |           4.6351 |           4.3664 |
[32m[20230113 19:57:29 @agent_ppo2.py:186][0m |          -0.0115 |           4.1720 |           4.3640 |
[32m[20230113 19:57:29 @agent_ppo2.py:186][0m |          -0.0120 |           4.1070 |           4.3647 |
[32m[20230113 19:57:29 @agent_ppo2.py:186][0m |          -0.0142 |           4.0925 |           4.3637 |
[32m[20230113 19:57:29 @agent_ppo2.py:186][0m |          -0.0143 |           4.0397 |           4.3616 |
[32m[20230113 19:57:29 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:57:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.04
[32m[20230113 19:57:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.43
[32m[20230113 19:57:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.04
[32m[20230113 19:57:30 @agent_ppo2.py:144][0m Total time:      12.95 min
[32m[20230113 19:57:30 @agent_ppo2.py:146][0m 1159168 total steps have happened
[32m[20230113 19:57:30 @agent_ppo2.py:122][0m #------------------------ Iteration 566 --------------------------#
[32m[20230113 19:57:30 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:57:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:30 @agent_ppo2.py:186][0m |           0.0027 |           4.5179 |           4.3580 |
[32m[20230113 19:57:30 @agent_ppo2.py:186][0m |          -0.0012 |           3.8751 |           4.3563 |
[32m[20230113 19:57:30 @agent_ppo2.py:186][0m |          -0.0055 |           3.6421 |           4.3556 |
[32m[20230113 19:57:30 @agent_ppo2.py:186][0m |          -0.0064 |           3.6909 |           4.3554 |
[32m[20230113 19:57:30 @agent_ppo2.py:186][0m |          -0.0083 |           3.3824 |           4.3543 |
[32m[20230113 19:57:30 @agent_ppo2.py:186][0m |          -0.0093 |           3.2596 |           4.3535 |
[32m[20230113 19:57:30 @agent_ppo2.py:186][0m |          -0.0021 |           3.4640 |           4.3535 |
[32m[20230113 19:57:31 @agent_ppo2.py:186][0m |          -0.0075 |           3.1733 |           4.3554 |
[32m[20230113 19:57:31 @agent_ppo2.py:186][0m |          -0.0099 |           3.0545 |           4.3559 |
[32m[20230113 19:57:31 @agent_ppo2.py:186][0m |          -0.0104 |           3.0102 |           4.3544 |
[32m[20230113 19:57:31 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:57:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 215.23
[32m[20230113 19:57:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.06
[32m[20230113 19:57:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.25
[32m[20230113 19:57:31 @agent_ppo2.py:144][0m Total time:      12.97 min
[32m[20230113 19:57:31 @agent_ppo2.py:146][0m 1161216 total steps have happened
[32m[20230113 19:57:31 @agent_ppo2.py:122][0m #------------------------ Iteration 567 --------------------------#
[32m[20230113 19:57:32 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:57:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:32 @agent_ppo2.py:186][0m |           0.0002 |           9.4850 |           4.4186 |
[32m[20230113 19:57:32 @agent_ppo2.py:186][0m |          -0.0025 |           5.9463 |           4.4150 |
[32m[20230113 19:57:32 @agent_ppo2.py:186][0m |          -0.0046 |           5.4017 |           4.4135 |
[32m[20230113 19:57:32 @agent_ppo2.py:186][0m |          -0.0055 |           5.1914 |           4.4139 |
[32m[20230113 19:57:32 @agent_ppo2.py:186][0m |          -0.0056 |           5.0502 |           4.4104 |
[32m[20230113 19:57:32 @agent_ppo2.py:186][0m |          -0.0069 |           4.8874 |           4.4093 |
[32m[20230113 19:57:32 @agent_ppo2.py:186][0m |          -0.0082 |           4.8028 |           4.4089 |
[32m[20230113 19:57:32 @agent_ppo2.py:186][0m |          -0.0080 |           4.7493 |           4.4061 |
[32m[20230113 19:57:32 @agent_ppo2.py:186][0m |          -0.0095 |           4.6731 |           4.4061 |
[32m[20230113 19:57:32 @agent_ppo2.py:186][0m |          -0.0104 |           4.6647 |           4.4054 |
[32m[20230113 19:57:32 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:57:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 112.00
[32m[20230113 19:57:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.57
[32m[20230113 19:57:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.59
[32m[20230113 19:57:32 @agent_ppo2.py:144][0m Total time:      13.00 min
[32m[20230113 19:57:32 @agent_ppo2.py:146][0m 1163264 total steps have happened
[32m[20230113 19:57:32 @agent_ppo2.py:122][0m #------------------------ Iteration 568 --------------------------#
[32m[20230113 19:57:33 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:33 @agent_ppo2.py:186][0m |          -0.0004 |           6.2999 |           4.2676 |
[32m[20230113 19:57:33 @agent_ppo2.py:186][0m |          -0.0032 |           5.6499 |           4.2694 |
[32m[20230113 19:57:33 @agent_ppo2.py:186][0m |          -0.0036 |           5.3817 |           4.2689 |
[32m[20230113 19:57:33 @agent_ppo2.py:186][0m |          -0.0086 |           5.1802 |           4.2708 |
[32m[20230113 19:57:33 @agent_ppo2.py:186][0m |          -0.0081 |           5.0603 |           4.2751 |
[32m[20230113 19:57:33 @agent_ppo2.py:186][0m |          -0.0099 |           4.9466 |           4.2722 |
[32m[20230113 19:57:33 @agent_ppo2.py:186][0m |          -0.0073 |           4.8433 |           4.2771 |
[32m[20230113 19:57:33 @agent_ppo2.py:186][0m |          -0.0076 |           4.7840 |           4.2742 |
[32m[20230113 19:57:33 @agent_ppo2.py:186][0m |          -0.0112 |           4.6725 |           4.2762 |
[32m[20230113 19:57:33 @agent_ppo2.py:186][0m |          -0.0129 |           4.5620 |           4.2761 |
[32m[20230113 19:57:33 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.87
[32m[20230113 19:57:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.99
[32m[20230113 19:57:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.81
[32m[20230113 19:57:34 @agent_ppo2.py:144][0m Total time:      13.02 min
[32m[20230113 19:57:34 @agent_ppo2.py:146][0m 1165312 total steps have happened
[32m[20230113 19:57:34 @agent_ppo2.py:122][0m #------------------------ Iteration 569 --------------------------#
[32m[20230113 19:57:34 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:57:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:34 @agent_ppo2.py:186][0m |          -0.0023 |           5.6958 |           4.2838 |
[32m[20230113 19:57:34 @agent_ppo2.py:186][0m |          -0.0025 |           5.3147 |           4.2724 |
[32m[20230113 19:57:34 @agent_ppo2.py:186][0m |          -0.0089 |           4.8606 |           4.2705 |
[32m[20230113 19:57:35 @agent_ppo2.py:186][0m |          -0.0105 |           4.6680 |           4.2641 |
[32m[20230113 19:57:35 @agent_ppo2.py:186][0m |          -0.0103 |           4.4898 |           4.2636 |
[32m[20230113 19:57:35 @agent_ppo2.py:186][0m |          -0.0088 |           4.3192 |           4.2630 |
[32m[20230113 19:57:35 @agent_ppo2.py:186][0m |          -0.0130 |           4.2191 |           4.2616 |
[32m[20230113 19:57:35 @agent_ppo2.py:186][0m |          -0.0130 |           4.1484 |           4.2575 |
[32m[20230113 19:57:35 @agent_ppo2.py:186][0m |          -0.0120 |           4.0725 |           4.2581 |
[32m[20230113 19:57:35 @agent_ppo2.py:186][0m |          -0.0170 |           4.0391 |           4.2581 |
[32m[20230113 19:57:35 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:57:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.65
[32m[20230113 19:57:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.97
[32m[20230113 19:57:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.56
[32m[20230113 19:57:35 @agent_ppo2.py:144][0m Total time:      13.04 min
[32m[20230113 19:57:35 @agent_ppo2.py:146][0m 1167360 total steps have happened
[32m[20230113 19:57:35 @agent_ppo2.py:122][0m #------------------------ Iteration 570 --------------------------#
[32m[20230113 19:57:36 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:36 @agent_ppo2.py:186][0m |           0.0015 |           5.9344 |           4.3720 |
[32m[20230113 19:57:36 @agent_ppo2.py:186][0m |          -0.0005 |           5.1430 |           4.3729 |
[32m[20230113 19:57:36 @agent_ppo2.py:186][0m |          -0.0036 |           4.7021 |           4.3672 |
[32m[20230113 19:57:36 @agent_ppo2.py:186][0m |          -0.0055 |           4.3862 |           4.3671 |
[32m[20230113 19:57:36 @agent_ppo2.py:186][0m |          -0.0058 |           4.2785 |           4.3619 |
[32m[20230113 19:57:36 @agent_ppo2.py:186][0m |          -0.0064 |           3.9707 |           4.3633 |
[32m[20230113 19:57:36 @agent_ppo2.py:186][0m |          -0.0072 |           3.8916 |           4.3620 |
[32m[20230113 19:57:36 @agent_ppo2.py:186][0m |          -0.0078 |           3.6660 |           4.3605 |
[32m[20230113 19:57:36 @agent_ppo2.py:186][0m |          -0.0079 |           3.5754 |           4.3593 |
[32m[20230113 19:57:36 @agent_ppo2.py:186][0m |          -0.0086 |           3.5107 |           4.3570 |
[32m[20230113 19:57:36 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:57:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.61
[32m[20230113 19:57:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.78
[32m[20230113 19:57:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.13
[32m[20230113 19:57:37 @agent_ppo2.py:144][0m Total time:      13.07 min
[32m[20230113 19:57:37 @agent_ppo2.py:146][0m 1169408 total steps have happened
[32m[20230113 19:57:37 @agent_ppo2.py:122][0m #------------------------ Iteration 571 --------------------------#
[32m[20230113 19:57:37 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:57:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:37 @agent_ppo2.py:186][0m |           0.0051 |           5.3791 |           4.3035 |
[32m[20230113 19:57:37 @agent_ppo2.py:186][0m |          -0.0050 |           4.4118 |           4.2928 |
[32m[20230113 19:57:37 @agent_ppo2.py:186][0m |          -0.0059 |           4.0119 |           4.2885 |
[32m[20230113 19:57:37 @agent_ppo2.py:186][0m |          -0.0083 |           3.7672 |           4.2853 |
[32m[20230113 19:57:37 @agent_ppo2.py:186][0m |          -0.0089 |           3.5687 |           4.2819 |
[32m[20230113 19:57:37 @agent_ppo2.py:186][0m |          -0.0074 |           3.4790 |           4.2789 |
[32m[20230113 19:57:37 @agent_ppo2.py:186][0m |          -0.0125 |           3.3726 |           4.2801 |
[32m[20230113 19:57:37 @agent_ppo2.py:186][0m |          -0.0075 |           3.2578 |           4.2783 |
[32m[20230113 19:57:37 @agent_ppo2.py:186][0m |          -0.0144 |           3.1817 |           4.2779 |
[32m[20230113 19:57:38 @agent_ppo2.py:186][0m |          -0.0068 |           3.1090 |           4.2704 |
[32m[20230113 19:57:38 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:57:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.17
[32m[20230113 19:57:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.44
[32m[20230113 19:57:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.20
[32m[20230113 19:57:38 @agent_ppo2.py:144][0m Total time:      13.09 min
[32m[20230113 19:57:38 @agent_ppo2.py:146][0m 1171456 total steps have happened
[32m[20230113 19:57:38 @agent_ppo2.py:122][0m #------------------------ Iteration 572 --------------------------#
[32m[20230113 19:57:38 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:38 @agent_ppo2.py:186][0m |          -0.0001 |           6.0079 |           4.2436 |
[32m[20230113 19:57:39 @agent_ppo2.py:186][0m |          -0.0054 |           5.3295 |           4.2341 |
[32m[20230113 19:57:39 @agent_ppo2.py:186][0m |          -0.0065 |           5.0823 |           4.2275 |
[32m[20230113 19:57:39 @agent_ppo2.py:186][0m |          -0.0080 |           4.9604 |           4.2249 |
[32m[20230113 19:57:39 @agent_ppo2.py:186][0m |          -0.0084 |           4.8555 |           4.2265 |
[32m[20230113 19:57:39 @agent_ppo2.py:186][0m |          -0.0098 |           4.7553 |           4.2241 |
[32m[20230113 19:57:39 @agent_ppo2.py:186][0m |          -0.0096 |           4.6606 |           4.2224 |
[32m[20230113 19:57:39 @agent_ppo2.py:186][0m |          -0.0098 |           4.5680 |           4.2271 |
[32m[20230113 19:57:39 @agent_ppo2.py:186][0m |          -0.0104 |           4.5283 |           4.2227 |
[32m[20230113 19:57:39 @agent_ppo2.py:186][0m |          -0.0107 |           4.4547 |           4.2239 |
[32m[20230113 19:57:39 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:57:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.99
[32m[20230113 19:57:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.71
[32m[20230113 19:57:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.97
[32m[20230113 19:57:39 @agent_ppo2.py:144][0m Total time:      13.11 min
[32m[20230113 19:57:39 @agent_ppo2.py:146][0m 1173504 total steps have happened
[32m[20230113 19:57:39 @agent_ppo2.py:122][0m #------------------------ Iteration 573 --------------------------#
[32m[20230113 19:57:40 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:40 @agent_ppo2.py:186][0m |          -0.0006 |           6.4885 |           4.3392 |
[32m[20230113 19:57:40 @agent_ppo2.py:186][0m |          -0.0040 |           5.8227 |           4.3252 |
[32m[20230113 19:57:40 @agent_ppo2.py:186][0m |          -0.0062 |           5.4501 |           4.3263 |
[32m[20230113 19:57:40 @agent_ppo2.py:186][0m |          -0.0069 |           5.2665 |           4.3257 |
[32m[20230113 19:57:40 @agent_ppo2.py:186][0m |          -0.0081 |           5.0478 |           4.3217 |
[32m[20230113 19:57:40 @agent_ppo2.py:186][0m |          -0.0086 |           4.9150 |           4.3211 |
[32m[20230113 19:57:40 @agent_ppo2.py:186][0m |          -0.0080 |           4.9766 |           4.3176 |
[32m[20230113 19:57:40 @agent_ppo2.py:186][0m |          -0.0098 |           4.7997 |           4.3213 |
[32m[20230113 19:57:40 @agent_ppo2.py:186][0m |          -0.0093 |           4.7702 |           4.3159 |
[32m[20230113 19:57:40 @agent_ppo2.py:186][0m |          -0.0106 |           4.6658 |           4.3205 |
[32m[20230113 19:57:40 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.83
[32m[20230113 19:57:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.28
[32m[20230113 19:57:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.56
[32m[20230113 19:57:41 @agent_ppo2.py:144][0m Total time:      13.13 min
[32m[20230113 19:57:41 @agent_ppo2.py:146][0m 1175552 total steps have happened
[32m[20230113 19:57:41 @agent_ppo2.py:122][0m #------------------------ Iteration 574 --------------------------#
[32m[20230113 19:57:41 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:57:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:41 @agent_ppo2.py:186][0m |          -0.0019 |           5.7089 |           4.2223 |
[32m[20230113 19:57:41 @agent_ppo2.py:186][0m |          -0.0006 |           5.0964 |           4.2137 |
[32m[20230113 19:57:41 @agent_ppo2.py:186][0m |          -0.0014 |           4.9159 |           4.2115 |
[32m[20230113 19:57:41 @agent_ppo2.py:186][0m |          -0.0049 |           4.7789 |           4.2080 |
[32m[20230113 19:57:41 @agent_ppo2.py:186][0m |          -0.0051 |           4.6260 |           4.2087 |
[32m[20230113 19:57:41 @agent_ppo2.py:186][0m |          -0.0111 |           4.5283 |           4.2067 |
[32m[20230113 19:57:41 @agent_ppo2.py:186][0m |          -0.0138 |           4.4856 |           4.2061 |
[32m[20230113 19:57:41 @agent_ppo2.py:186][0m |          -0.0098 |           4.4161 |           4.2023 |
[32m[20230113 19:57:42 @agent_ppo2.py:186][0m |          -0.0048 |           4.4490 |           4.2036 |
[32m[20230113 19:57:42 @agent_ppo2.py:186][0m |          -0.0055 |           4.3670 |           4.2010 |
[32m[20230113 19:57:42 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:57:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.79
[32m[20230113 19:57:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.74
[32m[20230113 19:57:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.49
[32m[20230113 19:57:42 @agent_ppo2.py:144][0m Total time:      13.16 min
[32m[20230113 19:57:42 @agent_ppo2.py:146][0m 1177600 total steps have happened
[32m[20230113 19:57:42 @agent_ppo2.py:122][0m #------------------------ Iteration 575 --------------------------#
[32m[20230113 19:57:42 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:57:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:43 @agent_ppo2.py:186][0m |          -0.0019 |           5.5065 |           4.3751 |
[32m[20230113 19:57:43 @agent_ppo2.py:186][0m |          -0.0061 |           4.4287 |           4.3658 |
[32m[20230113 19:57:43 @agent_ppo2.py:186][0m |          -0.0076 |           4.0086 |           4.3645 |
[32m[20230113 19:57:43 @agent_ppo2.py:186][0m |          -0.0091 |           3.7941 |           4.3587 |
[32m[20230113 19:57:43 @agent_ppo2.py:186][0m |          -0.0090 |           3.5927 |           4.3571 |
[32m[20230113 19:57:43 @agent_ppo2.py:186][0m |          -0.0112 |           3.4680 |           4.3534 |
[32m[20230113 19:57:43 @agent_ppo2.py:186][0m |          -0.0128 |           3.3654 |           4.3470 |
[32m[20230113 19:57:43 @agent_ppo2.py:186][0m |          -0.0115 |           3.3002 |           4.3468 |
[32m[20230113 19:57:43 @agent_ppo2.py:186][0m |          -0.0121 |           3.2044 |           4.3430 |
[32m[20230113 19:57:43 @agent_ppo2.py:186][0m |          -0.0127 |           3.1498 |           4.3419 |
[32m[20230113 19:57:43 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:57:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.49
[32m[20230113 19:57:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.29
[32m[20230113 19:57:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.74
[32m[20230113 19:57:43 @agent_ppo2.py:144][0m Total time:      13.18 min
[32m[20230113 19:57:43 @agent_ppo2.py:146][0m 1179648 total steps have happened
[32m[20230113 19:57:43 @agent_ppo2.py:122][0m #------------------------ Iteration 576 --------------------------#
[32m[20230113 19:57:44 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:57:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:44 @agent_ppo2.py:186][0m |          -0.0028 |           5.4791 |           4.1908 |
[32m[20230113 19:57:44 @agent_ppo2.py:186][0m |          -0.0065 |           4.7353 |           4.1889 |
[32m[20230113 19:57:44 @agent_ppo2.py:186][0m |          -0.0107 |           4.2422 |           4.1851 |
[32m[20230113 19:57:44 @agent_ppo2.py:186][0m |          -0.0151 |           3.9350 |           4.1857 |
[32m[20230113 19:57:44 @agent_ppo2.py:186][0m |          -0.0136 |           3.7648 |           4.1836 |
[32m[20230113 19:57:44 @agent_ppo2.py:186][0m |          -0.0133 |           3.5918 |           4.1824 |
[32m[20230113 19:57:44 @agent_ppo2.py:186][0m |          -0.0142 |           3.4758 |           4.1821 |
[32m[20230113 19:57:44 @agent_ppo2.py:186][0m |          -0.0051 |           3.6352 |           4.1804 |
[32m[20230113 19:57:44 @agent_ppo2.py:186][0m |          -0.0095 |           3.2690 |           4.1809 |
[32m[20230113 19:57:44 @agent_ppo2.py:186][0m |          -0.0145 |           3.2331 |           4.1813 |
[32m[20230113 19:57:44 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:57:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.59
[32m[20230113 19:57:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.35
[32m[20230113 19:57:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.17
[32m[20230113 19:57:45 @agent_ppo2.py:144][0m Total time:      13.20 min
[32m[20230113 19:57:45 @agent_ppo2.py:146][0m 1181696 total steps have happened
[32m[20230113 19:57:45 @agent_ppo2.py:122][0m #------------------------ Iteration 577 --------------------------#
[32m[20230113 19:57:45 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:45 @agent_ppo2.py:186][0m |          -0.0126 |           5.6513 |           4.1680 |
[32m[20230113 19:57:45 @agent_ppo2.py:186][0m |          -0.0136 |           5.1070 |           4.1631 |
[32m[20230113 19:57:45 @agent_ppo2.py:186][0m |          -0.0020 |           4.9168 |           4.1591 |
[32m[20230113 19:57:45 @agent_ppo2.py:186][0m |          -0.0000 |           4.7417 |           4.1641 |
[32m[20230113 19:57:45 @agent_ppo2.py:186][0m |          -0.0014 |           4.5631 |           4.1597 |
[32m[20230113 19:57:45 @agent_ppo2.py:186][0m |           0.0105 |           4.6109 |           4.1604 |
[32m[20230113 19:57:45 @agent_ppo2.py:186][0m |          -0.0073 |           4.4077 |           4.1540 |
[32m[20230113 19:57:45 @agent_ppo2.py:186][0m |          -0.0146 |           4.2949 |           4.1635 |
[32m[20230113 19:57:46 @agent_ppo2.py:186][0m |          -0.0233 |           4.2938 |           4.1615 |
[32m[20230113 19:57:46 @agent_ppo2.py:186][0m |          -0.0095 |           4.2211 |           4.1621 |
[32m[20230113 19:57:46 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.64
[32m[20230113 19:57:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.67
[32m[20230113 19:57:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.61
[32m[20230113 19:57:46 @agent_ppo2.py:144][0m Total time:      13.22 min
[32m[20230113 19:57:46 @agent_ppo2.py:146][0m 1183744 total steps have happened
[32m[20230113 19:57:46 @agent_ppo2.py:122][0m #------------------------ Iteration 578 --------------------------#
[32m[20230113 19:57:46 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:47 @agent_ppo2.py:186][0m |           0.0011 |           6.4572 |           4.2445 |
[32m[20230113 19:57:47 @agent_ppo2.py:186][0m |          -0.0046 |           5.7463 |           4.2304 |
[32m[20230113 19:57:47 @agent_ppo2.py:186][0m |          -0.0068 |           5.4332 |           4.2269 |
[32m[20230113 19:57:47 @agent_ppo2.py:186][0m |          -0.0078 |           5.2272 |           4.2288 |
[32m[20230113 19:57:47 @agent_ppo2.py:186][0m |          -0.0073 |           5.1326 |           4.2326 |
[32m[20230113 19:57:47 @agent_ppo2.py:186][0m |          -0.0102 |           5.0016 |           4.2275 |
[32m[20230113 19:57:47 @agent_ppo2.py:186][0m |          -0.0098 |           4.8947 |           4.2348 |
[32m[20230113 19:57:47 @agent_ppo2.py:186][0m |          -0.0099 |           4.8575 |           4.2291 |
[32m[20230113 19:57:47 @agent_ppo2.py:186][0m |          -0.0092 |           4.8210 |           4.2294 |
[32m[20230113 19:57:47 @agent_ppo2.py:186][0m |          -0.0106 |           4.7438 |           4.2345 |
[32m[20230113 19:57:47 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:57:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.45
[32m[20230113 19:57:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.81
[32m[20230113 19:57:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.66
[32m[20230113 19:57:47 @agent_ppo2.py:144][0m Total time:      13.25 min
[32m[20230113 19:57:47 @agent_ppo2.py:146][0m 1185792 total steps have happened
[32m[20230113 19:57:47 @agent_ppo2.py:122][0m #------------------------ Iteration 579 --------------------------#
[32m[20230113 19:57:48 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:57:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:48 @agent_ppo2.py:186][0m |          -0.0006 |           5.8696 |           4.3739 |
[32m[20230113 19:57:48 @agent_ppo2.py:186][0m |          -0.0065 |           5.3523 |           4.3628 |
[32m[20230113 19:57:48 @agent_ppo2.py:186][0m |          -0.0058 |           5.1875 |           4.3614 |
[32m[20230113 19:57:48 @agent_ppo2.py:186][0m |          -0.0059 |           5.0621 |           4.3596 |
[32m[20230113 19:57:48 @agent_ppo2.py:186][0m |          -0.0103 |           4.9177 |           4.3540 |
[32m[20230113 19:57:48 @agent_ppo2.py:186][0m |          -0.0078 |           4.8216 |           4.3597 |
[32m[20230113 19:57:48 @agent_ppo2.py:186][0m |          -0.0085 |           4.7485 |           4.3591 |
[32m[20230113 19:57:48 @agent_ppo2.py:186][0m |          -0.0095 |           4.6725 |           4.3587 |
[32m[20230113 19:57:48 @agent_ppo2.py:186][0m |          -0.0111 |           4.6236 |           4.3578 |
[32m[20230113 19:57:48 @agent_ppo2.py:186][0m |          -0.0103 |           4.6064 |           4.3603 |
[32m[20230113 19:57:48 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.47
[32m[20230113 19:57:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.13
[32m[20230113 19:57:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.94
[32m[20230113 19:57:49 @agent_ppo2.py:144][0m Total time:      13.27 min
[32m[20230113 19:57:49 @agent_ppo2.py:146][0m 1187840 total steps have happened
[32m[20230113 19:57:49 @agent_ppo2.py:122][0m #------------------------ Iteration 580 --------------------------#
[32m[20230113 19:57:49 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:57:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:49 @agent_ppo2.py:186][0m |           0.0013 |           5.8061 |           4.2916 |
[32m[20230113 19:57:49 @agent_ppo2.py:186][0m |          -0.0031 |           4.7443 |           4.2837 |
[32m[20230113 19:57:49 @agent_ppo2.py:186][0m |          -0.0082 |           4.1851 |           4.2788 |
[32m[20230113 19:57:49 @agent_ppo2.py:186][0m |          -0.0095 |           3.8632 |           4.2812 |
[32m[20230113 19:57:49 @agent_ppo2.py:186][0m |          -0.0111 |           3.6324 |           4.2791 |
[32m[20230113 19:57:49 @agent_ppo2.py:186][0m |          -0.0098 |           3.4592 |           4.2782 |
[32m[20230113 19:57:49 @agent_ppo2.py:186][0m |          -0.0104 |           3.3260 |           4.2765 |
[32m[20230113 19:57:49 @agent_ppo2.py:186][0m |          -0.0119 |           3.2069 |           4.2800 |
[32m[20230113 19:57:49 @agent_ppo2.py:186][0m |          -0.0137 |           3.1314 |           4.2798 |
[32m[20230113 19:57:50 @agent_ppo2.py:186][0m |          -0.0126 |           3.0515 |           4.2818 |
[32m[20230113 19:57:50 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:57:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.55
[32m[20230113 19:57:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.96
[32m[20230113 19:57:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.91
[32m[20230113 19:57:50 @agent_ppo2.py:144][0m Total time:      13.29 min
[32m[20230113 19:57:50 @agent_ppo2.py:146][0m 1189888 total steps have happened
[32m[20230113 19:57:50 @agent_ppo2.py:122][0m #------------------------ Iteration 581 --------------------------#
[32m[20230113 19:57:50 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:57:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:50 @agent_ppo2.py:186][0m |           0.0004 |           6.1314 |           4.2635 |
[32m[20230113 19:57:50 @agent_ppo2.py:186][0m |          -0.0031 |           5.6189 |           4.2554 |
[32m[20230113 19:57:51 @agent_ppo2.py:186][0m |          -0.0072 |           5.4007 |           4.2503 |
[32m[20230113 19:57:51 @agent_ppo2.py:186][0m |          -0.0066 |           5.2868 |           4.2446 |
[32m[20230113 19:57:51 @agent_ppo2.py:186][0m |          -0.0075 |           5.2589 |           4.2480 |
[32m[20230113 19:57:51 @agent_ppo2.py:186][0m |          -0.0085 |           5.1133 |           4.2471 |
[32m[20230113 19:57:51 @agent_ppo2.py:186][0m |          -0.0091 |           5.0403 |           4.2457 |
[32m[20230113 19:57:51 @agent_ppo2.py:186][0m |          -0.0087 |           4.9590 |           4.2470 |
[32m[20230113 19:57:51 @agent_ppo2.py:186][0m |          -0.0112 |           4.9126 |           4.2480 |
[32m[20230113 19:57:51 @agent_ppo2.py:186][0m |          -0.0106 |           4.8788 |           4.2471 |
[32m[20230113 19:57:51 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:57:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.58
[32m[20230113 19:57:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.64
[32m[20230113 19:57:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.71
[32m[20230113 19:57:51 @agent_ppo2.py:144][0m Total time:      13.31 min
[32m[20230113 19:57:51 @agent_ppo2.py:146][0m 1191936 total steps have happened
[32m[20230113 19:57:51 @agent_ppo2.py:122][0m #------------------------ Iteration 582 --------------------------#
[32m[20230113 19:57:52 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:52 @agent_ppo2.py:186][0m |          -0.0006 |           6.2831 |           4.2558 |
[32m[20230113 19:57:52 @agent_ppo2.py:186][0m |           0.0119 |           6.0924 |           4.2499 |
[32m[20230113 19:57:52 @agent_ppo2.py:186][0m |          -0.0045 |           5.3847 |           4.2569 |
[32m[20230113 19:57:52 @agent_ppo2.py:186][0m |          -0.0098 |           5.1097 |           4.2506 |
[32m[20230113 19:57:52 @agent_ppo2.py:186][0m |          -0.0074 |           4.9319 |           4.2524 |
[32m[20230113 19:57:52 @agent_ppo2.py:186][0m |          -0.0091 |           4.8232 |           4.2502 |
[32m[20230113 19:57:52 @agent_ppo2.py:186][0m |          -0.0010 |           5.0242 |           4.2517 |
[32m[20230113 19:57:52 @agent_ppo2.py:186][0m |          -0.0082 |           4.6941 |           4.2508 |
[32m[20230113 19:57:52 @agent_ppo2.py:186][0m |          -0.0136 |           4.5928 |           4.2541 |
[32m[20230113 19:57:52 @agent_ppo2.py:186][0m |          -0.0131 |           4.5257 |           4.2524 |
[32m[20230113 19:57:52 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:57:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.46
[32m[20230113 19:57:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.57
[32m[20230113 19:57:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.79
[32m[20230113 19:57:53 @agent_ppo2.py:144][0m Total time:      13.33 min
[32m[20230113 19:57:53 @agent_ppo2.py:146][0m 1193984 total steps have happened
[32m[20230113 19:57:53 @agent_ppo2.py:122][0m #------------------------ Iteration 583 --------------------------#
[32m[20230113 19:57:53 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:53 @agent_ppo2.py:186][0m |           0.0035 |           6.0881 |           4.2942 |
[32m[20230113 19:57:53 @agent_ppo2.py:186][0m |          -0.0052 |           5.2622 |           4.2885 |
[32m[20230113 19:57:53 @agent_ppo2.py:186][0m |          -0.0074 |           4.9011 |           4.2883 |
[32m[20230113 19:57:53 @agent_ppo2.py:186][0m |          -0.0105 |           4.7026 |           4.2883 |
[32m[20230113 19:57:53 @agent_ppo2.py:186][0m |          -0.0114 |           4.4855 |           4.2894 |
[32m[20230113 19:57:53 @agent_ppo2.py:186][0m |          -0.0119 |           4.3810 |           4.2889 |
[32m[20230113 19:57:53 @agent_ppo2.py:186][0m |          -0.0125 |           4.2310 |           4.2917 |
[32m[20230113 19:57:53 @agent_ppo2.py:186][0m |          -0.0142 |           4.1182 |           4.2910 |
[32m[20230113 19:57:53 @agent_ppo2.py:186][0m |          -0.0128 |           4.0320 |           4.2885 |
[32m[20230113 19:57:53 @agent_ppo2.py:186][0m |          -0.0127 |           4.0944 |           4.2918 |
[32m[20230113 19:57:53 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:57:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.91
[32m[20230113 19:57:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.38
[32m[20230113 19:57:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.20
[32m[20230113 19:57:54 @agent_ppo2.py:144][0m Total time:      13.36 min
[32m[20230113 19:57:54 @agent_ppo2.py:146][0m 1196032 total steps have happened
[32m[20230113 19:57:54 @agent_ppo2.py:122][0m #------------------------ Iteration 584 --------------------------#
[32m[20230113 19:57:54 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:54 @agent_ppo2.py:186][0m |          -0.0024 |           6.1786 |           4.3077 |
[32m[20230113 19:57:54 @agent_ppo2.py:186][0m |           0.0071 |           5.9905 |           4.3049 |
[32m[20230113 19:57:55 @agent_ppo2.py:186][0m |          -0.0075 |           5.6197 |           4.3023 |
[32m[20230113 19:57:55 @agent_ppo2.py:186][0m |          -0.0084 |           5.3147 |           4.2977 |
[32m[20230113 19:57:55 @agent_ppo2.py:186][0m |          -0.0087 |           5.1562 |           4.2998 |
[32m[20230113 19:57:55 @agent_ppo2.py:186][0m |           0.0078 |           5.7812 |           4.3001 |
[32m[20230113 19:57:55 @agent_ppo2.py:186][0m |          -0.0095 |           5.1013 |           4.2962 |
[32m[20230113 19:57:55 @agent_ppo2.py:186][0m |          -0.0127 |           4.8678 |           4.2977 |
[32m[20230113 19:57:55 @agent_ppo2.py:186][0m |          -0.0129 |           4.7663 |           4.2976 |
[32m[20230113 19:57:55 @agent_ppo2.py:186][0m |          -0.0153 |           4.7374 |           4.2964 |
[32m[20230113 19:57:55 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.30
[32m[20230113 19:57:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.86
[32m[20230113 19:57:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.37
[32m[20230113 19:57:55 @agent_ppo2.py:144][0m Total time:      13.38 min
[32m[20230113 19:57:55 @agent_ppo2.py:146][0m 1198080 total steps have happened
[32m[20230113 19:57:55 @agent_ppo2.py:122][0m #------------------------ Iteration 585 --------------------------#
[32m[20230113 19:57:56 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:57:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:56 @agent_ppo2.py:186][0m |          -0.0000 |           5.7518 |           4.3800 |
[32m[20230113 19:57:56 @agent_ppo2.py:186][0m |           0.0072 |           5.1847 |           4.3808 |
[32m[20230113 19:57:56 @agent_ppo2.py:186][0m |          -0.0146 |           4.7664 |           4.3780 |
[32m[20230113 19:57:56 @agent_ppo2.py:186][0m |          -0.0082 |           4.5589 |           4.3755 |
[32m[20230113 19:57:56 @agent_ppo2.py:186][0m |          -0.0201 |           4.4038 |           4.3792 |
[32m[20230113 19:57:56 @agent_ppo2.py:186][0m |           0.0025 |           4.3441 |           4.3784 |
[32m[20230113 19:57:56 @agent_ppo2.py:186][0m |          -0.0173 |           4.3901 |           4.3769 |
[32m[20230113 19:57:56 @agent_ppo2.py:186][0m |          -0.0138 |           4.1366 |           4.3792 |
[32m[20230113 19:57:56 @agent_ppo2.py:186][0m |          -0.0224 |           4.0743 |           4.3793 |
[32m[20230113 19:57:56 @agent_ppo2.py:186][0m |          -0.0173 |           4.0901 |           4.3805 |
[32m[20230113 19:57:56 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:57:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.13
[32m[20230113 19:57:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.41
[32m[20230113 19:57:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.48
[32m[20230113 19:57:57 @agent_ppo2.py:144][0m Total time:      13.40 min
[32m[20230113 19:57:57 @agent_ppo2.py:146][0m 1200128 total steps have happened
[32m[20230113 19:57:57 @agent_ppo2.py:122][0m #------------------------ Iteration 586 --------------------------#
[32m[20230113 19:57:57 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 19:57:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:57 @agent_ppo2.py:186][0m |          -0.0015 |          13.0513 |           4.3971 |
[32m[20230113 19:57:57 @agent_ppo2.py:186][0m |          -0.0022 |           6.3230 |           4.3962 |
[32m[20230113 19:57:57 @agent_ppo2.py:186][0m |          -0.0093 |           4.9715 |           4.3936 |
[32m[20230113 19:57:57 @agent_ppo2.py:186][0m |          -0.0077 |           4.2646 |           4.3950 |
[32m[20230113 19:57:57 @agent_ppo2.py:186][0m |          -0.0109 |           3.8683 |           4.3935 |
[32m[20230113 19:57:57 @agent_ppo2.py:186][0m |          -0.0132 |           3.5453 |           4.3937 |
[32m[20230113 19:57:57 @agent_ppo2.py:186][0m |          -0.0125 |           3.3258 |           4.3940 |
[32m[20230113 19:57:57 @agent_ppo2.py:186][0m |          -0.0102 |           3.1008 |           4.3941 |
[32m[20230113 19:57:57 @agent_ppo2.py:186][0m |          -0.0129 |           2.9198 |           4.3948 |
[32m[20230113 19:57:57 @agent_ppo2.py:186][0m |          -0.0109 |           2.7879 |           4.3952 |
[32m[20230113 19:57:57 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 19:57:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 117.16
[32m[20230113 19:57:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.21
[32m[20230113 19:57:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.13
[32m[20230113 19:57:58 @agent_ppo2.py:144][0m Total time:      13.42 min
[32m[20230113 19:57:58 @agent_ppo2.py:146][0m 1202176 total steps have happened
[32m[20230113 19:57:58 @agent_ppo2.py:122][0m #------------------------ Iteration 587 --------------------------#
[32m[20230113 19:57:58 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:57:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:57:58 @agent_ppo2.py:186][0m |           0.0027 |           6.9775 |           4.5198 |
[32m[20230113 19:57:58 @agent_ppo2.py:186][0m |          -0.0029 |           6.2919 |           4.5196 |
[32m[20230113 19:57:58 @agent_ppo2.py:186][0m |          -0.0042 |           5.9574 |           4.5154 |
[32m[20230113 19:57:58 @agent_ppo2.py:186][0m |          -0.0025 |           5.8483 |           4.5153 |
[32m[20230113 19:57:58 @agent_ppo2.py:186][0m |          -0.0056 |           5.5801 |           4.5156 |
[32m[20230113 19:57:58 @agent_ppo2.py:186][0m |          -0.0070 |           5.3831 |           4.5162 |
[32m[20230113 19:57:58 @agent_ppo2.py:186][0m |          -0.0060 |           5.3621 |           4.5122 |
[32m[20230113 19:57:59 @agent_ppo2.py:186][0m |          -0.0091 |           5.1800 |           4.5113 |
[32m[20230113 19:57:59 @agent_ppo2.py:186][0m |          -0.0099 |           5.1003 |           4.5157 |
[32m[20230113 19:57:59 @agent_ppo2.py:186][0m |          -0.0083 |           5.0588 |           4.5158 |
[32m[20230113 19:57:59 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:57:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.02
[32m[20230113 19:57:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.23
[32m[20230113 19:57:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.14
[32m[20230113 19:57:59 @agent_ppo2.py:144][0m Total time:      13.44 min
[32m[20230113 19:57:59 @agent_ppo2.py:146][0m 1204224 total steps have happened
[32m[20230113 19:57:59 @agent_ppo2.py:122][0m #------------------------ Iteration 588 --------------------------#
[32m[20230113 19:57:59 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:58:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:00 @agent_ppo2.py:186][0m |          -0.0008 |           5.2670 |           4.4809 |
[32m[20230113 19:58:00 @agent_ppo2.py:186][0m |          -0.0080 |           4.5854 |           4.4740 |
[32m[20230113 19:58:00 @agent_ppo2.py:186][0m |          -0.0083 |           4.3370 |           4.4692 |
[32m[20230113 19:58:00 @agent_ppo2.py:186][0m |          -0.0084 |           4.0950 |           4.4672 |
[32m[20230113 19:58:00 @agent_ppo2.py:186][0m |          -0.0065 |           4.1025 |           4.4665 |
[32m[20230113 19:58:00 @agent_ppo2.py:186][0m |          -0.0100 |           3.8536 |           4.4678 |
[32m[20230113 19:58:00 @agent_ppo2.py:186][0m |          -0.0072 |           3.9180 |           4.4671 |
[32m[20230113 19:58:00 @agent_ppo2.py:186][0m |          -0.0115 |           3.6742 |           4.4677 |
[32m[20230113 19:58:00 @agent_ppo2.py:186][0m |          -0.0112 |           3.5880 |           4.4658 |
[32m[20230113 19:58:00 @agent_ppo2.py:186][0m |          -0.0104 |           3.5322 |           4.4670 |
[32m[20230113 19:58:00 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:58:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.76
[32m[20230113 19:58:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.29
[32m[20230113 19:58:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.71
[32m[20230113 19:58:00 @agent_ppo2.py:144][0m Total time:      13.46 min
[32m[20230113 19:58:00 @agent_ppo2.py:146][0m 1206272 total steps have happened
[32m[20230113 19:58:00 @agent_ppo2.py:122][0m #------------------------ Iteration 589 --------------------------#
[32m[20230113 19:58:01 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:01 @agent_ppo2.py:186][0m |          -0.0018 |           5.0345 |           4.4607 |
[32m[20230113 19:58:01 @agent_ppo2.py:186][0m |          -0.0054 |           4.3864 |           4.4573 |
[32m[20230113 19:58:01 @agent_ppo2.py:186][0m |          -0.0069 |           4.1542 |           4.4532 |
[32m[20230113 19:58:01 @agent_ppo2.py:186][0m |          -0.0077 |           3.9917 |           4.4533 |
[32m[20230113 19:58:01 @agent_ppo2.py:186][0m |          -0.0096 |           3.8936 |           4.4560 |
[32m[20230113 19:58:01 @agent_ppo2.py:186][0m |          -0.0100 |           3.8131 |           4.4526 |
[32m[20230113 19:58:01 @agent_ppo2.py:186][0m |          -0.0101 |           3.7602 |           4.4486 |
[32m[20230113 19:58:01 @agent_ppo2.py:186][0m |          -0.0104 |           3.6664 |           4.4516 |
[32m[20230113 19:58:01 @agent_ppo2.py:186][0m |          -0.0115 |           3.6227 |           4.4499 |
[32m[20230113 19:58:01 @agent_ppo2.py:186][0m |          -0.0113 |           3.5550 |           4.4485 |
[32m[20230113 19:58:01 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:58:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.67
[32m[20230113 19:58:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.32
[32m[20230113 19:58:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.29
[32m[20230113 19:58:02 @agent_ppo2.py:144][0m Total time:      13.49 min
[32m[20230113 19:58:02 @agent_ppo2.py:146][0m 1208320 total steps have happened
[32m[20230113 19:58:02 @agent_ppo2.py:122][0m #------------------------ Iteration 590 --------------------------#
[32m[20230113 19:58:02 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:58:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:02 @agent_ppo2.py:186][0m |           0.0068 |           6.6605 |           4.4029 |
[32m[20230113 19:58:02 @agent_ppo2.py:186][0m |          -0.0113 |           5.9438 |           4.3888 |
[32m[20230113 19:58:02 @agent_ppo2.py:186][0m |          -0.0077 |           5.6020 |           4.3823 |
[32m[20230113 19:58:02 @agent_ppo2.py:186][0m |          -0.0064 |           5.4265 |           4.3823 |
[32m[20230113 19:58:02 @agent_ppo2.py:186][0m |          -0.0110 |           5.3093 |           4.3789 |
[32m[20230113 19:58:02 @agent_ppo2.py:186][0m |          -0.0108 |           5.1546 |           4.3753 |
[32m[20230113 19:58:02 @agent_ppo2.py:186][0m |          -0.0130 |           5.0441 |           4.3745 |
[32m[20230113 19:58:03 @agent_ppo2.py:186][0m |          -0.0084 |           5.1186 |           4.3740 |
[32m[20230113 19:58:03 @agent_ppo2.py:186][0m |          -0.0147 |           4.8786 |           4.3707 |
[32m[20230113 19:58:03 @agent_ppo2.py:186][0m |          -0.0165 |           4.8406 |           4.3738 |
[32m[20230113 19:58:03 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.94
[32m[20230113 19:58:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.68
[32m[20230113 19:58:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.60
[32m[20230113 19:58:03 @agent_ppo2.py:144][0m Total time:      13.51 min
[32m[20230113 19:58:03 @agent_ppo2.py:146][0m 1210368 total steps have happened
[32m[20230113 19:58:03 @agent_ppo2.py:122][0m #------------------------ Iteration 591 --------------------------#
[32m[20230113 19:58:03 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:58:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:04 @agent_ppo2.py:186][0m |          -0.0048 |           6.7197 |           4.4019 |
[32m[20230113 19:58:04 @agent_ppo2.py:186][0m |          -0.0021 |           5.6697 |           4.3928 |
[32m[20230113 19:58:04 @agent_ppo2.py:186][0m |          -0.0018 |           5.3664 |           4.3877 |
[32m[20230113 19:58:04 @agent_ppo2.py:186][0m |          -0.0102 |           5.1671 |           4.3856 |
[32m[20230113 19:58:04 @agent_ppo2.py:186][0m |          -0.0219 |           5.1171 |           4.3849 |
[32m[20230113 19:58:04 @agent_ppo2.py:186][0m |          -0.0030 |           4.9423 |           4.3850 |
[32m[20230113 19:58:04 @agent_ppo2.py:186][0m |           0.0289 |           5.3824 |           4.3876 |
[32m[20230113 19:58:04 @agent_ppo2.py:186][0m |           0.0059 |           4.9873 |           4.3528 |
[32m[20230113 19:58:04 @agent_ppo2.py:186][0m |          -0.0161 |           4.7027 |           4.3788 |
[32m[20230113 19:58:04 @agent_ppo2.py:186][0m |          -0.0132 |           4.6398 |           4.3785 |
[32m[20230113 19:58:04 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:58:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.05
[32m[20230113 19:58:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.07
[32m[20230113 19:58:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.01
[32m[20230113 19:58:04 @agent_ppo2.py:144][0m Total time:      13.53 min
[32m[20230113 19:58:04 @agent_ppo2.py:146][0m 1212416 total steps have happened
[32m[20230113 19:58:04 @agent_ppo2.py:122][0m #------------------------ Iteration 592 --------------------------#
[32m[20230113 19:58:05 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:58:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:05 @agent_ppo2.py:186][0m |           0.0125 |           5.8977 |           4.3333 |
[32m[20230113 19:58:05 @agent_ppo2.py:186][0m |          -0.0008 |           5.3201 |           4.3326 |
[32m[20230113 19:58:05 @agent_ppo2.py:186][0m |          -0.0081 |           5.0791 |           4.3284 |
[32m[20230113 19:58:05 @agent_ppo2.py:186][0m |          -0.0066 |           4.9208 |           4.3280 |
[32m[20230113 19:58:05 @agent_ppo2.py:186][0m |          -0.0115 |           4.7604 |           4.3257 |
[32m[20230113 19:58:05 @agent_ppo2.py:186][0m |          -0.0185 |           4.7950 |           4.3270 |
[32m[20230113 19:58:05 @agent_ppo2.py:186][0m |          -0.0056 |           4.5833 |           4.3261 |
[32m[20230113 19:58:05 @agent_ppo2.py:186][0m |          -0.0172 |           4.5230 |           4.3246 |
[32m[20230113 19:58:05 @agent_ppo2.py:186][0m |           0.0613 |           6.5257 |           4.3262 |
[32m[20230113 19:58:05 @agent_ppo2.py:186][0m |          -0.0050 |           4.5801 |           4.3175 |
[32m[20230113 19:58:05 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.11
[32m[20230113 19:58:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.59
[32m[20230113 19:58:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.33
[32m[20230113 19:58:06 @agent_ppo2.py:144][0m Total time:      13.55 min
[32m[20230113 19:58:06 @agent_ppo2.py:146][0m 1214464 total steps have happened
[32m[20230113 19:58:06 @agent_ppo2.py:122][0m #------------------------ Iteration 593 --------------------------#
[32m[20230113 19:58:06 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:58:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:06 @agent_ppo2.py:186][0m |          -0.0003 |          15.2021 |           4.4401 |
[32m[20230113 19:58:06 @agent_ppo2.py:186][0m |           0.0029 |           8.0581 |           4.4368 |
[32m[20230113 19:58:06 @agent_ppo2.py:186][0m |          -0.0112 |           6.4196 |           4.4352 |
[32m[20230113 19:58:06 @agent_ppo2.py:186][0m |          -0.0071 |           6.0489 |           4.4354 |
[32m[20230113 19:58:06 @agent_ppo2.py:186][0m |          -0.0116 |           5.3951 |           4.4385 |
[32m[20230113 19:58:06 @agent_ppo2.py:186][0m |          -0.0136 |           5.0822 |           4.4369 |
[32m[20230113 19:58:07 @agent_ppo2.py:186][0m |          -0.0089 |           4.9042 |           4.4386 |
[32m[20230113 19:58:07 @agent_ppo2.py:186][0m |          -0.0139 |           4.7921 |           4.4345 |
[32m[20230113 19:58:07 @agent_ppo2.py:186][0m |          -0.0142 |           4.6562 |           4.4399 |
[32m[20230113 19:58:07 @agent_ppo2.py:186][0m |          -0.0154 |           4.5827 |           4.4398 |
[32m[20230113 19:58:07 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:58:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 110.25
[32m[20230113 19:58:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.60
[32m[20230113 19:58:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.82
[32m[20230113 19:58:07 @agent_ppo2.py:144][0m Total time:      13.57 min
[32m[20230113 19:58:07 @agent_ppo2.py:146][0m 1216512 total steps have happened
[32m[20230113 19:58:07 @agent_ppo2.py:122][0m #------------------------ Iteration 594 --------------------------#
[32m[20230113 19:58:07 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:58:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:08 @agent_ppo2.py:186][0m |          -0.0038 |           6.2951 |           4.4061 |
[32m[20230113 19:58:08 @agent_ppo2.py:186][0m |          -0.0038 |           5.7094 |           4.3978 |
[32m[20230113 19:58:08 @agent_ppo2.py:186][0m |          -0.0073 |           5.4733 |           4.3967 |
[32m[20230113 19:58:08 @agent_ppo2.py:186][0m |          -0.0066 |           5.3301 |           4.4009 |
[32m[20230113 19:58:08 @agent_ppo2.py:186][0m |          -0.0064 |           5.2693 |           4.4003 |
[32m[20230113 19:58:08 @agent_ppo2.py:186][0m |           0.0017 |           5.3256 |           4.4028 |
[32m[20230113 19:58:08 @agent_ppo2.py:186][0m |          -0.0133 |           5.0170 |           4.3993 |
[32m[20230113 19:58:08 @agent_ppo2.py:186][0m |          -0.0107 |           4.8833 |           4.4014 |
[32m[20230113 19:58:08 @agent_ppo2.py:186][0m |          -0.0106 |           4.7902 |           4.3986 |
[32m[20230113 19:58:08 @agent_ppo2.py:186][0m |          -0.0086 |           4.7554 |           4.4006 |
[32m[20230113 19:58:08 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.00
[32m[20230113 19:58:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.94
[32m[20230113 19:58:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.56
[32m[20230113 19:58:08 @agent_ppo2.py:144][0m Total time:      13.60 min
[32m[20230113 19:58:08 @agent_ppo2.py:146][0m 1218560 total steps have happened
[32m[20230113 19:58:08 @agent_ppo2.py:122][0m #------------------------ Iteration 595 --------------------------#
[32m[20230113 19:58:09 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:09 @agent_ppo2.py:186][0m |          -0.0010 |           6.0993 |           4.4578 |
[32m[20230113 19:58:09 @agent_ppo2.py:186][0m |          -0.0053 |           5.6145 |           4.4488 |
[32m[20230113 19:58:09 @agent_ppo2.py:186][0m |          -0.0088 |           5.3596 |           4.4535 |
[32m[20230113 19:58:09 @agent_ppo2.py:186][0m |          -0.0104 |           5.1825 |           4.4562 |
[32m[20230113 19:58:09 @agent_ppo2.py:186][0m |          -0.0114 |           5.0126 |           4.4558 |
[32m[20230113 19:58:09 @agent_ppo2.py:186][0m |          -0.0126 |           4.8962 |           4.4560 |
[32m[20230113 19:58:09 @agent_ppo2.py:186][0m |          -0.0130 |           4.7651 |           4.4575 |
[32m[20230113 19:58:09 @agent_ppo2.py:186][0m |          -0.0130 |           4.6620 |           4.4590 |
[32m[20230113 19:58:09 @agent_ppo2.py:186][0m |          -0.0130 |           4.5849 |           4.4581 |
[32m[20230113 19:58:09 @agent_ppo2.py:186][0m |          -0.0143 |           4.4724 |           4.4592 |
[32m[20230113 19:58:09 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.08
[32m[20230113 19:58:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.09
[32m[20230113 19:58:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.23
[32m[20230113 19:58:10 @agent_ppo2.py:144][0m Total time:      13.62 min
[32m[20230113 19:58:10 @agent_ppo2.py:146][0m 1220608 total steps have happened
[32m[20230113 19:58:10 @agent_ppo2.py:122][0m #------------------------ Iteration 596 --------------------------#
[32m[20230113 19:58:10 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:58:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:10 @agent_ppo2.py:186][0m |          -0.0001 |           6.9468 |           4.4920 |
[32m[20230113 19:58:10 @agent_ppo2.py:186][0m |          -0.0056 |           6.0301 |           4.4842 |
[32m[20230113 19:58:10 @agent_ppo2.py:186][0m |          -0.0069 |           5.5610 |           4.4806 |
[32m[20230113 19:58:10 @agent_ppo2.py:186][0m |          -0.0082 |           5.2720 |           4.4816 |
[32m[20230113 19:58:10 @agent_ppo2.py:186][0m |          -0.0083 |           5.0840 |           4.4808 |
[32m[20230113 19:58:10 @agent_ppo2.py:186][0m |          -0.0092 |           4.9142 |           4.4793 |
[32m[20230113 19:58:10 @agent_ppo2.py:186][0m |          -0.0095 |           4.7746 |           4.4751 |
[32m[20230113 19:58:11 @agent_ppo2.py:186][0m |          -0.0107 |           4.6173 |           4.4754 |
[32m[20230113 19:58:11 @agent_ppo2.py:186][0m |          -0.0111 |           4.4938 |           4.4779 |
[32m[20230113 19:58:11 @agent_ppo2.py:186][0m |          -0.0107 |           4.3707 |           4.4736 |
[32m[20230113 19:58:11 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:58:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.34
[32m[20230113 19:58:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.34
[32m[20230113 19:58:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.90
[32m[20230113 19:58:11 @agent_ppo2.py:144][0m Total time:      13.64 min
[32m[20230113 19:58:11 @agent_ppo2.py:146][0m 1222656 total steps have happened
[32m[20230113 19:58:11 @agent_ppo2.py:122][0m #------------------------ Iteration 597 --------------------------#
[32m[20230113 19:58:11 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:58:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:12 @agent_ppo2.py:186][0m |          -0.0004 |           5.7380 |           4.4081 |
[32m[20230113 19:58:12 @agent_ppo2.py:186][0m |          -0.0038 |           5.0729 |           4.4046 |
[32m[20230113 19:58:12 @agent_ppo2.py:186][0m |          -0.0050 |           4.7911 |           4.4010 |
[32m[20230113 19:58:12 @agent_ppo2.py:186][0m |          -0.0068 |           4.5846 |           4.3977 |
[32m[20230113 19:58:12 @agent_ppo2.py:186][0m |          -0.0072 |           4.4623 |           4.3973 |
[32m[20230113 19:58:12 @agent_ppo2.py:186][0m |          -0.0103 |           4.3814 |           4.3945 |
[32m[20230113 19:58:12 @agent_ppo2.py:186][0m |          -0.0092 |           4.3681 |           4.3945 |
[32m[20230113 19:58:12 @agent_ppo2.py:186][0m |          -0.0103 |           4.2227 |           4.3967 |
[32m[20230113 19:58:12 @agent_ppo2.py:186][0m |          -0.0106 |           4.1601 |           4.3968 |
[32m[20230113 19:58:12 @agent_ppo2.py:186][0m |          -0.0138 |           4.1106 |           4.3969 |
[32m[20230113 19:58:12 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.62
[32m[20230113 19:58:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.24
[32m[20230113 19:58:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.08
[32m[20230113 19:58:12 @agent_ppo2.py:144][0m Total time:      13.66 min
[32m[20230113 19:58:12 @agent_ppo2.py:146][0m 1224704 total steps have happened
[32m[20230113 19:58:12 @agent_ppo2.py:122][0m #------------------------ Iteration 598 --------------------------#
[32m[20230113 19:58:13 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:58:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:13 @agent_ppo2.py:186][0m |          -0.0015 |           6.3464 |           4.5548 |
[32m[20230113 19:58:13 @agent_ppo2.py:186][0m |          -0.0047 |           5.1840 |           4.5518 |
[32m[20230113 19:58:13 @agent_ppo2.py:186][0m |          -0.0070 |           4.7820 |           4.5518 |
[32m[20230113 19:58:13 @agent_ppo2.py:186][0m |          -0.0028 |           4.6579 |           4.5456 |
[32m[20230113 19:58:13 @agent_ppo2.py:186][0m |          -0.0104 |           4.3182 |           4.5497 |
[32m[20230113 19:58:13 @agent_ppo2.py:186][0m |          -0.0126 |           4.1867 |           4.5524 |
[32m[20230113 19:58:13 @agent_ppo2.py:186][0m |          -0.0129 |           4.1156 |           4.5479 |
[32m[20230113 19:58:13 @agent_ppo2.py:186][0m |          -0.0135 |           3.9093 |           4.5492 |
[32m[20230113 19:58:13 @agent_ppo2.py:186][0m |          -0.0140 |           3.7859 |           4.5500 |
[32m[20230113 19:58:13 @agent_ppo2.py:186][0m |          -0.0124 |           3.7241 |           4.5487 |
[32m[20230113 19:58:13 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.39
[32m[20230113 19:58:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.14
[32m[20230113 19:58:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.61
[32m[20230113 19:58:14 @agent_ppo2.py:144][0m Total time:      13.69 min
[32m[20230113 19:58:14 @agent_ppo2.py:146][0m 1226752 total steps have happened
[32m[20230113 19:58:14 @agent_ppo2.py:122][0m #------------------------ Iteration 599 --------------------------#
[32m[20230113 19:58:14 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:14 @agent_ppo2.py:186][0m |           0.0008 |           6.0359 |           4.5509 |
[32m[20230113 19:58:14 @agent_ppo2.py:186][0m |          -0.0041 |           5.3399 |           4.5485 |
[32m[20230113 19:58:14 @agent_ppo2.py:186][0m |          -0.0055 |           4.8357 |           4.5447 |
[32m[20230113 19:58:14 @agent_ppo2.py:186][0m |          -0.0097 |           4.5102 |           4.5418 |
[32m[20230113 19:58:14 @agent_ppo2.py:186][0m |          -0.0087 |           4.2753 |           4.5400 |
[32m[20230113 19:58:14 @agent_ppo2.py:186][0m |          -0.0108 |           4.1285 |           4.5393 |
[32m[20230113 19:58:14 @agent_ppo2.py:186][0m |          -0.0110 |           3.9576 |           4.5366 |
[32m[20230113 19:58:15 @agent_ppo2.py:186][0m |          -0.0129 |           3.8556 |           4.5362 |
[32m[20230113 19:58:15 @agent_ppo2.py:186][0m |          -0.0126 |           3.6968 |           4.5345 |
[32m[20230113 19:58:15 @agent_ppo2.py:186][0m |          -0.0136 |           3.6484 |           4.5333 |
[32m[20230113 19:58:15 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:58:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.49
[32m[20230113 19:58:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.24
[32m[20230113 19:58:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.35
[32m[20230113 19:58:15 @agent_ppo2.py:144][0m Total time:      13.71 min
[32m[20230113 19:58:15 @agent_ppo2.py:146][0m 1228800 total steps have happened
[32m[20230113 19:58:15 @agent_ppo2.py:122][0m #------------------------ Iteration 600 --------------------------#
[32m[20230113 19:58:16 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:16 @agent_ppo2.py:186][0m |           0.0014 |           8.2008 |           4.3838 |
[32m[20230113 19:58:16 @agent_ppo2.py:186][0m |          -0.0040 |           6.3458 |           4.3718 |
[32m[20230113 19:58:16 @agent_ppo2.py:186][0m |          -0.0069 |           5.8322 |           4.3756 |
[32m[20230113 19:58:16 @agent_ppo2.py:186][0m |          -0.0070 |           5.5667 |           4.3681 |
[32m[20230113 19:58:16 @agent_ppo2.py:186][0m |          -0.0082 |           5.3790 |           4.3673 |
[32m[20230113 19:58:16 @agent_ppo2.py:186][0m |          -0.0086 |           5.2252 |           4.3668 |
[32m[20230113 19:58:16 @agent_ppo2.py:186][0m |          -0.0114 |           5.0861 |           4.3655 |
[32m[20230113 19:58:16 @agent_ppo2.py:186][0m |          -0.0108 |           5.0006 |           4.3642 |
[32m[20230113 19:58:16 @agent_ppo2.py:186][0m |          -0.0125 |           4.8623 |           4.3654 |
[32m[20230113 19:58:16 @agent_ppo2.py:186][0m |          -0.0119 |           4.7819 |           4.3643 |
[32m[20230113 19:58:16 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:58:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.81
[32m[20230113 19:58:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.89
[32m[20230113 19:58:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.75
[32m[20230113 19:58:16 @agent_ppo2.py:144][0m Total time:      13.73 min
[32m[20230113 19:58:16 @agent_ppo2.py:146][0m 1230848 total steps have happened
[32m[20230113 19:58:16 @agent_ppo2.py:122][0m #------------------------ Iteration 601 --------------------------#
[32m[20230113 19:58:17 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:17 @agent_ppo2.py:186][0m |           0.0021 |           7.3891 |           4.6388 |
[32m[20230113 19:58:17 @agent_ppo2.py:186][0m |          -0.0049 |           6.1008 |           4.6288 |
[32m[20230113 19:58:17 @agent_ppo2.py:186][0m |          -0.0071 |           5.5525 |           4.6310 |
[32m[20230113 19:58:17 @agent_ppo2.py:186][0m |          -0.0082 |           5.2203 |           4.6259 |
[32m[20230113 19:58:17 @agent_ppo2.py:186][0m |          -0.0091 |           4.9704 |           4.6228 |
[32m[20230113 19:58:17 @agent_ppo2.py:186][0m |          -0.0109 |           4.7222 |           4.6251 |
[32m[20230113 19:58:17 @agent_ppo2.py:186][0m |          -0.0109 |           4.5336 |           4.6223 |
[32m[20230113 19:58:17 @agent_ppo2.py:186][0m |          -0.0116 |           4.4676 |           4.6229 |
[32m[20230113 19:58:17 @agent_ppo2.py:186][0m |          -0.0131 |           4.2514 |           4.6209 |
[32m[20230113 19:58:17 @agent_ppo2.py:186][0m |          -0.0131 |           4.1557 |           4.6233 |
[32m[20230113 19:58:17 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.88
[32m[20230113 19:58:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.24
[32m[20230113 19:58:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.36
[32m[20230113 19:58:18 @agent_ppo2.py:144][0m Total time:      13.75 min
[32m[20230113 19:58:18 @agent_ppo2.py:146][0m 1232896 total steps have happened
[32m[20230113 19:58:18 @agent_ppo2.py:122][0m #------------------------ Iteration 602 --------------------------#
[32m[20230113 19:58:18 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:18 @agent_ppo2.py:186][0m |           0.0007 |           9.7288 |           4.4985 |
[32m[20230113 19:58:18 @agent_ppo2.py:186][0m |          -0.0032 |           6.8246 |           4.4989 |
[32m[20230113 19:58:18 @agent_ppo2.py:186][0m |          -0.0126 |           6.1044 |           4.4961 |
[32m[20230113 19:58:18 @agent_ppo2.py:186][0m |          -0.0099 |           5.4801 |           4.4962 |
[32m[20230113 19:58:18 @agent_ppo2.py:186][0m |          -0.0090 |           5.1491 |           4.4907 |
[32m[20230113 19:58:19 @agent_ppo2.py:186][0m |          -0.0124 |           4.9062 |           4.4954 |
[32m[20230113 19:58:19 @agent_ppo2.py:186][0m |          -0.0259 |           4.8338 |           4.4958 |
[32m[20230113 19:58:19 @agent_ppo2.py:186][0m |          -0.0081 |           4.7460 |           4.4947 |
[32m[20230113 19:58:19 @agent_ppo2.py:186][0m |          -0.0217 |           4.5370 |           4.4960 |
[32m[20230113 19:58:19 @agent_ppo2.py:186][0m |          -0.0072 |           4.5239 |           4.4973 |
[32m[20230113 19:58:19 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:58:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.67
[32m[20230113 19:58:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.94
[32m[20230113 19:58:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.27
[32m[20230113 19:58:19 @agent_ppo2.py:144][0m Total time:      13.78 min
[32m[20230113 19:58:19 @agent_ppo2.py:146][0m 1234944 total steps have happened
[32m[20230113 19:58:19 @agent_ppo2.py:122][0m #------------------------ Iteration 603 --------------------------#
[32m[20230113 19:58:20 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:20 @agent_ppo2.py:186][0m |          -0.0011 |           6.5930 |           4.5413 |
[32m[20230113 19:58:20 @agent_ppo2.py:186][0m |          -0.0066 |           5.8113 |           4.5334 |
[32m[20230113 19:58:20 @agent_ppo2.py:186][0m |          -0.0085 |           5.3949 |           4.5306 |
[32m[20230113 19:58:20 @agent_ppo2.py:186][0m |          -0.0098 |           5.1010 |           4.5266 |
[32m[20230113 19:58:20 @agent_ppo2.py:186][0m |          -0.0106 |           4.8414 |           4.5281 |
[32m[20230113 19:58:20 @agent_ppo2.py:186][0m |          -0.0109 |           4.6985 |           4.5257 |
[32m[20230113 19:58:20 @agent_ppo2.py:186][0m |          -0.0106 |           4.6121 |           4.5261 |
[32m[20230113 19:58:20 @agent_ppo2.py:186][0m |          -0.0116 |           4.5036 |           4.5240 |
[32m[20230113 19:58:20 @agent_ppo2.py:186][0m |          -0.0126 |           4.4005 |           4.5219 |
[32m[20230113 19:58:20 @agent_ppo2.py:186][0m |          -0.0128 |           4.3338 |           4.5243 |
[32m[20230113 19:58:20 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:58:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.13
[32m[20230113 19:58:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.03
[32m[20230113 19:58:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.27
[32m[20230113 19:58:20 @agent_ppo2.py:144][0m Total time:      13.80 min
[32m[20230113 19:58:20 @agent_ppo2.py:146][0m 1236992 total steps have happened
[32m[20230113 19:58:20 @agent_ppo2.py:122][0m #------------------------ Iteration 604 --------------------------#
[32m[20230113 19:58:21 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:21 @agent_ppo2.py:186][0m |           0.0079 |           6.5264 |           4.4273 |
[32m[20230113 19:58:21 @agent_ppo2.py:186][0m |          -0.0107 |           5.3016 |           4.4223 |
[32m[20230113 19:58:21 @agent_ppo2.py:186][0m |          -0.0033 |           5.0072 |           4.4204 |
[32m[20230113 19:58:21 @agent_ppo2.py:186][0m |          -0.0057 |           4.7767 |           4.4224 |
[32m[20230113 19:58:21 @agent_ppo2.py:186][0m |          -0.0091 |           4.4600 |           4.4199 |
[32m[20230113 19:58:21 @agent_ppo2.py:186][0m |          -0.0057 |           4.4669 |           4.4201 |
[32m[20230113 19:58:21 @agent_ppo2.py:186][0m |           0.0050 |           4.4658 |           4.4203 |
[32m[20230113 19:58:21 @agent_ppo2.py:186][0m |          -0.0136 |           4.0379 |           4.4195 |
[32m[20230113 19:58:21 @agent_ppo2.py:186][0m |          -0.0041 |           3.9948 |           4.4216 |
[32m[20230113 19:58:21 @agent_ppo2.py:186][0m |          -0.0134 |           3.7759 |           4.4208 |
[32m[20230113 19:58:21 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:58:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.60
[32m[20230113 19:58:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.31
[32m[20230113 19:58:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.56
[32m[20230113 19:58:22 @agent_ppo2.py:144][0m Total time:      13.82 min
[32m[20230113 19:58:22 @agent_ppo2.py:146][0m 1239040 total steps have happened
[32m[20230113 19:58:22 @agent_ppo2.py:122][0m #------------------------ Iteration 605 --------------------------#
[32m[20230113 19:58:22 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:58:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:22 @agent_ppo2.py:186][0m |           0.0002 |           7.6031 |           4.4884 |
[32m[20230113 19:58:22 @agent_ppo2.py:186][0m |          -0.0049 |           6.2221 |           4.4770 |
[32m[20230113 19:58:22 @agent_ppo2.py:186][0m |          -0.0033 |           5.8547 |           4.4800 |
[32m[20230113 19:58:22 @agent_ppo2.py:186][0m |          -0.0059 |           5.5822 |           4.4749 |
[32m[20230113 19:58:23 @agent_ppo2.py:186][0m |          -0.0075 |           5.3221 |           4.4756 |
[32m[20230113 19:58:23 @agent_ppo2.py:186][0m |          -0.0081 |           5.1662 |           4.4716 |
[32m[20230113 19:58:23 @agent_ppo2.py:186][0m |          -0.0091 |           4.9551 |           4.4778 |
[32m[20230113 19:58:23 @agent_ppo2.py:186][0m |          -0.0070 |           4.8557 |           4.4752 |
[32m[20230113 19:58:23 @agent_ppo2.py:186][0m |          -0.0077 |           4.6764 |           4.4750 |
[32m[20230113 19:58:23 @agent_ppo2.py:186][0m |          -0.0101 |           4.4660 |           4.4743 |
[32m[20230113 19:58:23 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:58:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.82
[32m[20230113 19:58:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.08
[32m[20230113 19:58:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.56
[32m[20230113 19:58:23 @agent_ppo2.py:144][0m Total time:      13.84 min
[32m[20230113 19:58:23 @agent_ppo2.py:146][0m 1241088 total steps have happened
[32m[20230113 19:58:23 @agent_ppo2.py:122][0m #------------------------ Iteration 606 --------------------------#
[32m[20230113 19:58:24 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 19:58:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:24 @agent_ppo2.py:186][0m |          -0.0084 |           6.6970 |           4.5103 |
[32m[20230113 19:58:24 @agent_ppo2.py:186][0m |          -0.0002 |           5.4531 |           4.5023 |
[32m[20230113 19:58:24 @agent_ppo2.py:186][0m |          -0.0037 |           4.8825 |           4.4987 |
[32m[20230113 19:58:24 @agent_ppo2.py:186][0m |          -0.0043 |           4.5736 |           4.5006 |
[32m[20230113 19:58:24 @agent_ppo2.py:186][0m |          -0.0077 |           4.2117 |           4.4957 |
[32m[20230113 19:58:24 @agent_ppo2.py:186][0m |          -0.0063 |           3.9448 |           4.4954 |
[32m[20230113 19:58:24 @agent_ppo2.py:186][0m |          -0.0070 |           3.8301 |           4.4921 |
[32m[20230113 19:58:24 @agent_ppo2.py:186][0m |          -0.0107 |           3.7404 |           4.4916 |
[32m[20230113 19:58:24 @agent_ppo2.py:186][0m |          -0.0074 |           3.6668 |           4.4894 |
[32m[20230113 19:58:24 @agent_ppo2.py:186][0m |          -0.0066 |           3.4691 |           4.4902 |
[32m[20230113 19:58:24 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 19:58:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.56
[32m[20230113 19:58:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.17
[32m[20230113 19:58:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.96
[32m[20230113 19:58:24 @agent_ppo2.py:144][0m Total time:      13.86 min
[32m[20230113 19:58:24 @agent_ppo2.py:146][0m 1243136 total steps have happened
[32m[20230113 19:58:24 @agent_ppo2.py:122][0m #------------------------ Iteration 607 --------------------------#
[32m[20230113 19:58:25 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 19:58:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:25 @agent_ppo2.py:186][0m |           0.0005 |           5.4931 |           4.4574 |
[32m[20230113 19:58:25 @agent_ppo2.py:186][0m |          -0.0022 |           4.7424 |           4.4512 |
[32m[20230113 19:58:25 @agent_ppo2.py:186][0m |          -0.0038 |           4.4759 |           4.4533 |
[32m[20230113 19:58:25 @agent_ppo2.py:186][0m |          -0.0107 |           4.1830 |           4.4554 |
[32m[20230113 19:58:25 @agent_ppo2.py:186][0m |          -0.0081 |           4.1078 |           4.4586 |
[32m[20230113 19:58:25 @agent_ppo2.py:186][0m |          -0.0082 |           3.9998 |           4.4520 |
[32m[20230113 19:58:25 @agent_ppo2.py:186][0m |          -0.0103 |           3.8998 |           4.4554 |
[32m[20230113 19:58:25 @agent_ppo2.py:186][0m |          -0.0115 |           3.7198 |           4.4517 |
[32m[20230113 19:58:25 @agent_ppo2.py:186][0m |          -0.0106 |           3.7340 |           4.4532 |
[32m[20230113 19:58:25 @agent_ppo2.py:186][0m |          -0.0126 |           3.5953 |           4.4544 |
[32m[20230113 19:58:25 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:58:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.99
[32m[20230113 19:58:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.58
[32m[20230113 19:58:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.76
[32m[20230113 19:58:26 @agent_ppo2.py:144][0m Total time:      13.89 min
[32m[20230113 19:58:26 @agent_ppo2.py:146][0m 1245184 total steps have happened
[32m[20230113 19:58:26 @agent_ppo2.py:122][0m #------------------------ Iteration 608 --------------------------#
[32m[20230113 19:58:26 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:58:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:26 @agent_ppo2.py:186][0m |           0.0000 |           5.1979 |           4.4482 |
[32m[20230113 19:58:26 @agent_ppo2.py:186][0m |          -0.0040 |           4.5980 |           4.4501 |
[32m[20230113 19:58:26 @agent_ppo2.py:186][0m |          -0.0058 |           4.1648 |           4.4442 |
[32m[20230113 19:58:26 @agent_ppo2.py:186][0m |          -0.0023 |           3.9339 |           4.4391 |
[32m[20230113 19:58:26 @agent_ppo2.py:186][0m |          -0.0093 |           3.7180 |           4.4393 |
[32m[20230113 19:58:26 @agent_ppo2.py:186][0m |          -0.0079 |           3.5206 |           4.4390 |
[32m[20230113 19:58:27 @agent_ppo2.py:186][0m |          -0.0107 |           3.4697 |           4.4378 |
[32m[20230113 19:58:27 @agent_ppo2.py:186][0m |          -0.0082 |           3.4797 |           4.4384 |
[32m[20230113 19:58:27 @agent_ppo2.py:186][0m |          -0.0099 |           3.2539 |           4.4343 |
[32m[20230113 19:58:27 @agent_ppo2.py:186][0m |          -0.0064 |           3.4241 |           4.4348 |
[32m[20230113 19:58:27 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.29
[32m[20230113 19:58:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.02
[32m[20230113 19:58:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.90
[32m[20230113 19:58:27 @agent_ppo2.py:144][0m Total time:      13.91 min
[32m[20230113 19:58:27 @agent_ppo2.py:146][0m 1247232 total steps have happened
[32m[20230113 19:58:27 @agent_ppo2.py:122][0m #------------------------ Iteration 609 --------------------------#
[32m[20230113 19:58:28 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:58:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:28 @agent_ppo2.py:186][0m |           0.0013 |           7.3728 |           4.5908 |
[32m[20230113 19:58:28 @agent_ppo2.py:186][0m |          -0.0050 |           6.4360 |           4.5868 |
[32m[20230113 19:58:28 @agent_ppo2.py:186][0m |          -0.0082 |           6.1505 |           4.5840 |
[32m[20230113 19:58:28 @agent_ppo2.py:186][0m |          -0.0108 |           5.9378 |           4.5801 |
[32m[20230113 19:58:28 @agent_ppo2.py:186][0m |          -0.0106 |           5.8042 |           4.5783 |
[32m[20230113 19:58:28 @agent_ppo2.py:186][0m |          -0.0113 |           5.7361 |           4.5790 |
[32m[20230113 19:58:28 @agent_ppo2.py:186][0m |          -0.0118 |           5.6512 |           4.5777 |
[32m[20230113 19:58:28 @agent_ppo2.py:186][0m |          -0.0119 |           5.5554 |           4.5745 |
[32m[20230113 19:58:28 @agent_ppo2.py:186][0m |          -0.0144 |           5.4830 |           4.5777 |
[32m[20230113 19:58:28 @agent_ppo2.py:186][0m |          -0.0152 |           5.4225 |           4.5759 |
[32m[20230113 19:58:28 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:58:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.04
[32m[20230113 19:58:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.07
[32m[20230113 19:58:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.03
[32m[20230113 19:58:28 @agent_ppo2.py:144][0m Total time:      13.93 min
[32m[20230113 19:58:28 @agent_ppo2.py:146][0m 1249280 total steps have happened
[32m[20230113 19:58:28 @agent_ppo2.py:122][0m #------------------------ Iteration 610 --------------------------#
[32m[20230113 19:58:29 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 19:58:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:29 @agent_ppo2.py:186][0m |           0.0007 |          14.7084 |           4.5445 |
[32m[20230113 19:58:29 @agent_ppo2.py:186][0m |          -0.0056 |           5.9521 |           4.5415 |
[32m[20230113 19:58:29 @agent_ppo2.py:186][0m |          -0.0068 |           5.0213 |           4.5381 |
[32m[20230113 19:58:29 @agent_ppo2.py:186][0m |          -0.0080 |           4.5651 |           4.5313 |
[32m[20230113 19:58:29 @agent_ppo2.py:186][0m |          -0.0098 |           4.3166 |           4.5330 |
[32m[20230113 19:58:29 @agent_ppo2.py:186][0m |          -0.0100 |           4.2062 |           4.5274 |
[32m[20230113 19:58:29 @agent_ppo2.py:186][0m |          -0.0115 |           4.0707 |           4.5244 |
[32m[20230113 19:58:29 @agent_ppo2.py:186][0m |          -0.0117 |           3.9673 |           4.5228 |
[32m[20230113 19:58:29 @agent_ppo2.py:186][0m |          -0.0117 |           3.9038 |           4.5207 |
[32m[20230113 19:58:29 @agent_ppo2.py:186][0m |          -0.0138 |           3.8163 |           4.5177 |
[32m[20230113 19:58:29 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 19:58:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 155.67
[32m[20230113 19:58:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.97
[32m[20230113 19:58:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 157.52
[32m[20230113 19:58:30 @agent_ppo2.py:144][0m Total time:      13.95 min
[32m[20230113 19:58:30 @agent_ppo2.py:146][0m 1251328 total steps have happened
[32m[20230113 19:58:30 @agent_ppo2.py:122][0m #------------------------ Iteration 611 --------------------------#
[32m[20230113 19:58:30 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:58:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:30 @agent_ppo2.py:186][0m |          -0.0024 |          25.5387 |           4.4751 |
[32m[20230113 19:58:30 @agent_ppo2.py:186][0m |          -0.0102 |           8.4432 |           4.4666 |
[32m[20230113 19:58:30 @agent_ppo2.py:186][0m |          -0.0116 |           6.9600 |           4.4698 |
[32m[20230113 19:58:30 @agent_ppo2.py:186][0m |          -0.0123 |           6.1749 |           4.4682 |
[32m[20230113 19:58:30 @agent_ppo2.py:186][0m |          -0.0126 |           5.7378 |           4.4691 |
[32m[20230113 19:58:30 @agent_ppo2.py:186][0m |          -0.0152 |           5.3684 |           4.4692 |
[32m[20230113 19:58:30 @agent_ppo2.py:186][0m |          -0.0150 |           5.1305 |           4.4704 |
[32m[20230113 19:58:30 @agent_ppo2.py:186][0m |          -0.0147 |           4.9153 |           4.4667 |
[32m[20230113 19:58:31 @agent_ppo2.py:186][0m |          -0.0149 |           4.7296 |           4.4677 |
[32m[20230113 19:58:31 @agent_ppo2.py:186][0m |          -0.0168 |           4.5420 |           4.4689 |
[32m[20230113 19:58:31 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:58:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 158.98
[32m[20230113 19:58:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 218.93
[32m[20230113 19:58:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.22
[32m[20230113 19:58:31 @agent_ppo2.py:144][0m Total time:      13.97 min
[32m[20230113 19:58:31 @agent_ppo2.py:146][0m 1253376 total steps have happened
[32m[20230113 19:58:31 @agent_ppo2.py:122][0m #------------------------ Iteration 612 --------------------------#
[32m[20230113 19:58:31 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:58:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:32 @agent_ppo2.py:186][0m |           0.0012 |           4.5121 |           4.4314 |
[32m[20230113 19:58:32 @agent_ppo2.py:186][0m |          -0.0029 |           3.9159 |           4.4303 |
[32m[20230113 19:58:32 @agent_ppo2.py:186][0m |          -0.0052 |           3.6550 |           4.4290 |
[32m[20230113 19:58:32 @agent_ppo2.py:186][0m |          -0.0075 |           3.5015 |           4.4222 |
[32m[20230113 19:58:32 @agent_ppo2.py:186][0m |          -0.0071 |           3.3816 |           4.4266 |
[32m[20230113 19:58:32 @agent_ppo2.py:186][0m |          -0.0092 |           3.2839 |           4.4246 |
[32m[20230113 19:58:32 @agent_ppo2.py:186][0m |          -0.0098 |           3.1853 |           4.4259 |
[32m[20230113 19:58:32 @agent_ppo2.py:186][0m |          -0.0105 |           3.1071 |           4.4240 |
[32m[20230113 19:58:32 @agent_ppo2.py:186][0m |          -0.0107 |           3.0433 |           4.4248 |
[32m[20230113 19:58:32 @agent_ppo2.py:186][0m |          -0.0114 |           2.9780 |           4.4225 |
[32m[20230113 19:58:32 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:58:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.01
[32m[20230113 19:58:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 218.33
[32m[20230113 19:58:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.13
[32m[20230113 19:58:32 @agent_ppo2.py:144][0m Total time:      14.00 min
[32m[20230113 19:58:32 @agent_ppo2.py:146][0m 1255424 total steps have happened
[32m[20230113 19:58:32 @agent_ppo2.py:122][0m #------------------------ Iteration 613 --------------------------#
[32m[20230113 19:58:33 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:58:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:33 @agent_ppo2.py:186][0m |          -0.0011 |           5.6716 |           4.4444 |
[32m[20230113 19:58:33 @agent_ppo2.py:186][0m |          -0.0048 |           5.2765 |           4.4405 |
[32m[20230113 19:58:33 @agent_ppo2.py:186][0m |          -0.0067 |           5.0735 |           4.4391 |
[32m[20230113 19:58:33 @agent_ppo2.py:186][0m |          -0.0081 |           4.8847 |           4.4396 |
[32m[20230113 19:58:33 @agent_ppo2.py:186][0m |          -0.0083 |           4.7331 |           4.4386 |
[32m[20230113 19:58:33 @agent_ppo2.py:186][0m |          -0.0093 |           4.5238 |           4.4367 |
[32m[20230113 19:58:33 @agent_ppo2.py:186][0m |          -0.0097 |           4.4191 |           4.4356 |
[32m[20230113 19:58:33 @agent_ppo2.py:186][0m |          -0.0112 |           4.2539 |           4.4359 |
[32m[20230113 19:58:33 @agent_ppo2.py:186][0m |          -0.0120 |           4.1168 |           4.4358 |
[32m[20230113 19:58:33 @agent_ppo2.py:186][0m |          -0.0123 |           4.0336 |           4.4348 |
[32m[20230113 19:58:33 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:58:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.30
[32m[20230113 19:58:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.53
[32m[20230113 19:58:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 228.44
[32m[20230113 19:58:34 @agent_ppo2.py:144][0m Total time:      14.02 min
[32m[20230113 19:58:34 @agent_ppo2.py:146][0m 1257472 total steps have happened
[32m[20230113 19:58:34 @agent_ppo2.py:122][0m #------------------------ Iteration 614 --------------------------#
[32m[20230113 19:58:34 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:58:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:34 @agent_ppo2.py:186][0m |           0.0059 |           9.5393 |           4.4183 |
[32m[20230113 19:58:34 @agent_ppo2.py:186][0m |          -0.0027 |           5.4428 |           4.4181 |
[32m[20230113 19:58:34 @agent_ppo2.py:186][0m |          -0.0057 |           4.7838 |           4.4184 |
[32m[20230113 19:58:34 @agent_ppo2.py:186][0m |          -0.0038 |           4.3758 |           4.4158 |
[32m[20230113 19:58:34 @agent_ppo2.py:186][0m |          -0.0072 |           4.1601 |           4.4163 |
[32m[20230113 19:58:34 @agent_ppo2.py:186][0m |          -0.0036 |           3.9565 |           4.4130 |
[32m[20230113 19:58:35 @agent_ppo2.py:186][0m |          -0.0081 |           3.8130 |           4.4149 |
[32m[20230113 19:58:35 @agent_ppo2.py:186][0m |          -0.0053 |           3.6864 |           4.4119 |
[32m[20230113 19:58:35 @agent_ppo2.py:186][0m |          -0.0102 |           3.5521 |           4.4116 |
[32m[20230113 19:58:35 @agent_ppo2.py:186][0m |          -0.0111 |           3.4600 |           4.4126 |
[32m[20230113 19:58:35 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:58:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 165.07
[32m[20230113 19:58:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.66
[32m[20230113 19:58:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.25
[32m[20230113 19:58:35 @agent_ppo2.py:144][0m Total time:      14.04 min
[32m[20230113 19:58:35 @agent_ppo2.py:146][0m 1259520 total steps have happened
[32m[20230113 19:58:35 @agent_ppo2.py:122][0m #------------------------ Iteration 615 --------------------------#
[32m[20230113 19:58:35 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 19:58:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:36 @agent_ppo2.py:186][0m |          -0.0030 |           5.9669 |           4.5423 |
[32m[20230113 19:58:36 @agent_ppo2.py:186][0m |          -0.0068 |           5.1876 |           4.5301 |
[32m[20230113 19:58:36 @agent_ppo2.py:186][0m |          -0.0098 |           4.8771 |           4.5274 |
[32m[20230113 19:58:36 @agent_ppo2.py:186][0m |          -0.0125 |           4.6815 |           4.5287 |
[32m[20230113 19:58:36 @agent_ppo2.py:186][0m |          -0.0135 |           4.5640 |           4.5246 |
[32m[20230113 19:58:36 @agent_ppo2.py:186][0m |          -0.0121 |           4.4560 |           4.5237 |
[32m[20230113 19:58:36 @agent_ppo2.py:186][0m |          -0.0151 |           4.3534 |           4.5245 |
[32m[20230113 19:58:36 @agent_ppo2.py:186][0m |          -0.0136 |           4.2753 |           4.5234 |
[32m[20230113 19:58:36 @agent_ppo2.py:186][0m |          -0.0156 |           4.1595 |           4.5230 |
[32m[20230113 19:58:36 @agent_ppo2.py:186][0m |          -0.0150 |           4.0949 |           4.5233 |
[32m[20230113 19:58:36 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 19:58:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.98
[32m[20230113 19:58:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.82
[32m[20230113 19:58:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.55
[32m[20230113 19:58:36 @agent_ppo2.py:144][0m Total time:      14.06 min
[32m[20230113 19:58:36 @agent_ppo2.py:146][0m 1261568 total steps have happened
[32m[20230113 19:58:36 @agent_ppo2.py:122][0m #------------------------ Iteration 616 --------------------------#
[32m[20230113 19:58:37 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:37 @agent_ppo2.py:186][0m |          -0.0019 |           6.3354 |           4.4808 |
[32m[20230113 19:58:37 @agent_ppo2.py:186][0m |          -0.0077 |           5.2226 |           4.4764 |
[32m[20230113 19:58:37 @agent_ppo2.py:186][0m |          -0.0096 |           4.7932 |           4.4775 |
[32m[20230113 19:58:37 @agent_ppo2.py:186][0m |          -0.0114 |           4.6415 |           4.4729 |
[32m[20230113 19:58:37 @agent_ppo2.py:186][0m |          -0.0101 |           4.4599 |           4.4732 |
[32m[20230113 19:58:37 @agent_ppo2.py:186][0m |          -0.0119 |           4.3990 |           4.4714 |
[32m[20230113 19:58:37 @agent_ppo2.py:186][0m |          -0.0122 |           4.2691 |           4.4683 |
[32m[20230113 19:58:37 @agent_ppo2.py:186][0m |          -0.0115 |           4.2186 |           4.4683 |
[32m[20230113 19:58:37 @agent_ppo2.py:186][0m |          -0.0131 |           4.1192 |           4.4674 |
[32m[20230113 19:58:37 @agent_ppo2.py:186][0m |          -0.0144 |           4.0459 |           4.4658 |
[32m[20230113 19:58:37 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.32
[32m[20230113 19:58:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.80
[32m[20230113 19:58:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.02
[32m[20230113 19:58:38 @agent_ppo2.py:144][0m Total time:      14.09 min
[32m[20230113 19:58:38 @agent_ppo2.py:146][0m 1263616 total steps have happened
[32m[20230113 19:58:38 @agent_ppo2.py:122][0m #------------------------ Iteration 617 --------------------------#
[32m[20230113 19:58:38 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:58:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:38 @agent_ppo2.py:186][0m |           0.0040 |           6.2687 |           4.4348 |
[32m[20230113 19:58:38 @agent_ppo2.py:186][0m |          -0.0071 |           5.9042 |           4.4331 |
[32m[20230113 19:58:38 @agent_ppo2.py:186][0m |          -0.0089 |           5.7059 |           4.4292 |
[32m[20230113 19:58:38 @agent_ppo2.py:186][0m |          -0.0078 |           5.5915 |           4.4319 |
[32m[20230113 19:58:38 @agent_ppo2.py:186][0m |          -0.0119 |           5.5305 |           4.4312 |
[32m[20230113 19:58:38 @agent_ppo2.py:186][0m |          -0.0130 |           5.4029 |           4.4332 |
[32m[20230113 19:58:38 @agent_ppo2.py:186][0m |          -0.0180 |           5.3254 |           4.4342 |
[32m[20230113 19:58:39 @agent_ppo2.py:186][0m |          -0.0153 |           5.2579 |           4.4351 |
[32m[20230113 19:58:39 @agent_ppo2.py:186][0m |          -0.0126 |           5.1789 |           4.4392 |
[32m[20230113 19:58:39 @agent_ppo2.py:186][0m |          -0.0162 |           5.1385 |           4.4406 |
[32m[20230113 19:58:39 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.36
[32m[20230113 19:58:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.69
[32m[20230113 19:58:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.70
[32m[20230113 19:58:39 @agent_ppo2.py:144][0m Total time:      14.11 min
[32m[20230113 19:58:39 @agent_ppo2.py:146][0m 1265664 total steps have happened
[32m[20230113 19:58:39 @agent_ppo2.py:122][0m #------------------------ Iteration 618 --------------------------#
[32m[20230113 19:58:40 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:58:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:40 @agent_ppo2.py:186][0m |           0.0059 |           5.3476 |           4.4040 |
[32m[20230113 19:58:40 @agent_ppo2.py:186][0m |          -0.0034 |           4.6250 |           4.3954 |
[32m[20230113 19:58:40 @agent_ppo2.py:186][0m |          -0.0072 |           4.2773 |           4.3943 |
[32m[20230113 19:58:40 @agent_ppo2.py:186][0m |          -0.0098 |           4.1231 |           4.3886 |
[32m[20230113 19:58:40 @agent_ppo2.py:186][0m |           0.0093 |           4.6077 |           4.3879 |
[32m[20230113 19:58:40 @agent_ppo2.py:186][0m |          -0.0003 |           4.0871 |           4.3787 |
[32m[20230113 19:58:40 @agent_ppo2.py:186][0m |          -0.0136 |           3.8717 |           4.3765 |
[32m[20230113 19:58:40 @agent_ppo2.py:186][0m |          -0.0164 |           3.7894 |           4.3795 |
[32m[20230113 19:58:40 @agent_ppo2.py:186][0m |          -0.0154 |           3.7485 |           4.3793 |
[32m[20230113 19:58:40 @agent_ppo2.py:186][0m |           0.0001 |           3.9269 |           4.3828 |
[32m[20230113 19:58:40 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.58
[32m[20230113 19:58:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.91
[32m[20230113 19:58:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.74
[32m[20230113 19:58:40 @agent_ppo2.py:144][0m Total time:      14.13 min
[32m[20230113 19:58:40 @agent_ppo2.py:146][0m 1267712 total steps have happened
[32m[20230113 19:58:40 @agent_ppo2.py:122][0m #------------------------ Iteration 619 --------------------------#
[32m[20230113 19:58:41 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:41 @agent_ppo2.py:186][0m |          -0.0013 |           6.8812 |           4.5460 |
[32m[20230113 19:58:41 @agent_ppo2.py:186][0m |          -0.0040 |           5.7063 |           4.5441 |
[32m[20230113 19:58:41 @agent_ppo2.py:186][0m |          -0.0080 |           5.1638 |           4.5400 |
[32m[20230113 19:58:41 @agent_ppo2.py:186][0m |          -0.0078 |           4.8543 |           4.5386 |
[32m[20230113 19:58:41 @agent_ppo2.py:186][0m |          -0.0084 |           4.6541 |           4.5421 |
[32m[20230113 19:58:41 @agent_ppo2.py:186][0m |          -0.0095 |           4.4988 |           4.5441 |
[32m[20230113 19:58:41 @agent_ppo2.py:186][0m |          -0.0094 |           4.3274 |           4.5423 |
[32m[20230113 19:58:41 @agent_ppo2.py:186][0m |          -0.0121 |           4.1822 |           4.5447 |
[32m[20230113 19:58:41 @agent_ppo2.py:186][0m |          -0.0119 |           4.1000 |           4.5447 |
[32m[20230113 19:58:41 @agent_ppo2.py:186][0m |          -0.0095 |           3.9429 |           4.5442 |
[32m[20230113 19:58:41 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:58:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.52
[32m[20230113 19:58:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.84
[32m[20230113 19:58:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.52
[32m[20230113 19:58:42 @agent_ppo2.py:144][0m Total time:      14.15 min
[32m[20230113 19:58:42 @agent_ppo2.py:146][0m 1269760 total steps have happened
[32m[20230113 19:58:42 @agent_ppo2.py:122][0m #------------------------ Iteration 620 --------------------------#
[32m[20230113 19:58:42 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 19:58:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:42 @agent_ppo2.py:186][0m |           0.0007 |          13.9067 |           4.3710 |
[32m[20230113 19:58:42 @agent_ppo2.py:186][0m |          -0.0052 |           6.5849 |           4.3712 |
[32m[20230113 19:58:42 @agent_ppo2.py:186][0m |          -0.0016 |           4.9779 |           4.3654 |
[32m[20230113 19:58:42 @agent_ppo2.py:186][0m |          -0.0075 |           4.3679 |           4.3571 |
[32m[20230113 19:58:42 @agent_ppo2.py:186][0m |          -0.0068 |           3.9261 |           4.3566 |
[32m[20230113 19:58:42 @agent_ppo2.py:186][0m |          -0.0123 |           3.6753 |           4.3561 |
[32m[20230113 19:58:42 @agent_ppo2.py:186][0m |          -0.0130 |           3.4729 |           4.3509 |
[32m[20230113 19:58:42 @agent_ppo2.py:186][0m |          -0.0126 |           3.3675 |           4.3551 |
[32m[20230113 19:58:42 @agent_ppo2.py:186][0m |          -0.0072 |           3.2486 |           4.3526 |
[32m[20230113 19:58:42 @agent_ppo2.py:186][0m |          -0.0134 |           3.1365 |           4.3509 |
[32m[20230113 19:58:42 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 19:58:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 128.00
[32m[20230113 19:58:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.61
[32m[20230113 19:58:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.26
[32m[20230113 19:58:43 @agent_ppo2.py:144][0m Total time:      14.17 min
[32m[20230113 19:58:43 @agent_ppo2.py:146][0m 1271808 total steps have happened
[32m[20230113 19:58:43 @agent_ppo2.py:122][0m #------------------------ Iteration 621 --------------------------#
[32m[20230113 19:58:43 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:43 @agent_ppo2.py:186][0m |          -0.0011 |           6.3400 |           4.4523 |
[32m[20230113 19:58:43 @agent_ppo2.py:186][0m |          -0.0064 |           5.3523 |           4.4462 |
[32m[20230113 19:58:44 @agent_ppo2.py:186][0m |          -0.0103 |           4.9924 |           4.4448 |
[32m[20230113 19:58:44 @agent_ppo2.py:186][0m |          -0.0124 |           4.8064 |           4.4436 |
[32m[20230113 19:58:44 @agent_ppo2.py:186][0m |          -0.0117 |           4.6317 |           4.4433 |
[32m[20230113 19:58:44 @agent_ppo2.py:186][0m |          -0.0057 |           4.6488 |           4.4461 |
[32m[20230113 19:58:44 @agent_ppo2.py:186][0m |          -0.0136 |           4.3691 |           4.4428 |
[32m[20230113 19:58:44 @agent_ppo2.py:186][0m |          -0.0115 |           4.2854 |           4.4439 |
[32m[20230113 19:58:44 @agent_ppo2.py:186][0m |          -0.0153 |           4.1690 |           4.4403 |
[32m[20230113 19:58:44 @agent_ppo2.py:186][0m |          -0.0138 |           4.1236 |           4.4434 |
[32m[20230113 19:58:44 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:58:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.80
[32m[20230113 19:58:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.71
[32m[20230113 19:58:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.09
[32m[20230113 19:58:44 @agent_ppo2.py:144][0m Total time:      14.19 min
[32m[20230113 19:58:44 @agent_ppo2.py:146][0m 1273856 total steps have happened
[32m[20230113 19:58:44 @agent_ppo2.py:122][0m #------------------------ Iteration 622 --------------------------#
[32m[20230113 19:58:45 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:45 @agent_ppo2.py:186][0m |          -0.0007 |           6.7288 |           4.4479 |
[32m[20230113 19:58:45 @agent_ppo2.py:186][0m |          -0.0023 |           6.1875 |           4.4356 |
[32m[20230113 19:58:45 @agent_ppo2.py:186][0m |          -0.0152 |           6.0405 |           4.4381 |
[32m[20230113 19:58:45 @agent_ppo2.py:186][0m |          -0.0386 |           5.9425 |           4.4387 |
[32m[20230113 19:58:45 @agent_ppo2.py:186][0m |           0.0029 |           5.8344 |           4.4163 |
[32m[20230113 19:58:45 @agent_ppo2.py:186][0m |           0.0111 |           5.7426 |           4.4256 |
[32m[20230113 19:58:45 @agent_ppo2.py:186][0m |          -0.0171 |           5.6046 |           4.4363 |
[32m[20230113 19:58:45 @agent_ppo2.py:186][0m |          -0.0225 |           5.6095 |           4.4379 |
[32m[20230113 19:58:45 @agent_ppo2.py:186][0m |          -0.0113 |           5.4392 |           4.4365 |
[32m[20230113 19:58:45 @agent_ppo2.py:186][0m |          -0.0169 |           5.3780 |           4.4358 |
[32m[20230113 19:58:45 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:58:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.18
[32m[20230113 19:58:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.70
[32m[20230113 19:58:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.78
[32m[20230113 19:58:46 @agent_ppo2.py:144][0m Total time:      14.22 min
[32m[20230113 19:58:46 @agent_ppo2.py:146][0m 1275904 total steps have happened
[32m[20230113 19:58:46 @agent_ppo2.py:122][0m #------------------------ Iteration 623 --------------------------#
[32m[20230113 19:58:46 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:46 @agent_ppo2.py:186][0m |          -0.0020 |           6.3076 |           4.5957 |
[32m[20230113 19:58:46 @agent_ppo2.py:186][0m |          -0.0085 |           5.7935 |           4.5881 |
[32m[20230113 19:58:46 @agent_ppo2.py:186][0m |          -0.0101 |           5.4762 |           4.5867 |
[32m[20230113 19:58:46 @agent_ppo2.py:186][0m |          -0.0126 |           5.2572 |           4.5824 |
[32m[20230113 19:58:46 @agent_ppo2.py:186][0m |          -0.0126 |           5.0783 |           4.5855 |
[32m[20230113 19:58:46 @agent_ppo2.py:186][0m |          -0.0134 |           4.8134 |           4.5796 |
[32m[20230113 19:58:46 @agent_ppo2.py:186][0m |          -0.0145 |           4.6204 |           4.5800 |
[32m[20230113 19:58:46 @agent_ppo2.py:186][0m |          -0.0149 |           4.5300 |           4.5789 |
[32m[20230113 19:58:47 @agent_ppo2.py:186][0m |          -0.0147 |           4.4351 |           4.5779 |
[32m[20230113 19:58:47 @agent_ppo2.py:186][0m |          -0.0143 |           4.3477 |           4.5764 |
[32m[20230113 19:58:47 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:58:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.07
[32m[20230113 19:58:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.73
[32m[20230113 19:58:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.44
[32m[20230113 19:58:47 @agent_ppo2.py:144][0m Total time:      14.24 min
[32m[20230113 19:58:47 @agent_ppo2.py:146][0m 1277952 total steps have happened
[32m[20230113 19:58:47 @agent_ppo2.py:122][0m #------------------------ Iteration 624 --------------------------#
[32m[20230113 19:58:47 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:58:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:48 @agent_ppo2.py:186][0m |           0.0014 |           5.5835 |           4.4737 |
[32m[20230113 19:58:48 @agent_ppo2.py:186][0m |          -0.0030 |           5.1659 |           4.4701 |
[32m[20230113 19:58:48 @agent_ppo2.py:186][0m |          -0.0063 |           4.9530 |           4.4669 |
[32m[20230113 19:58:48 @agent_ppo2.py:186][0m |          -0.0034 |           4.7843 |           4.4649 |
[32m[20230113 19:58:48 @agent_ppo2.py:186][0m |          -0.0110 |           4.6340 |           4.4627 |
[32m[20230113 19:58:48 @agent_ppo2.py:186][0m |          -0.0120 |           4.5572 |           4.4588 |
[32m[20230113 19:58:48 @agent_ppo2.py:186][0m |          -0.0098 |           4.4551 |           4.4554 |
[32m[20230113 19:58:48 @agent_ppo2.py:186][0m |          -0.0100 |           4.3773 |           4.4515 |
[32m[20230113 19:58:48 @agent_ppo2.py:186][0m |           0.0005 |           4.7946 |           4.4508 |
[32m[20230113 19:58:48 @agent_ppo2.py:186][0m |          -0.0064 |           4.2958 |           4.4476 |
[32m[20230113 19:58:48 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:58:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.00
[32m[20230113 19:58:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.46
[32m[20230113 19:58:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.62
[32m[20230113 19:58:48 @agent_ppo2.py:144][0m Total time:      14.26 min
[32m[20230113 19:58:48 @agent_ppo2.py:146][0m 1280000 total steps have happened
[32m[20230113 19:58:48 @agent_ppo2.py:122][0m #------------------------ Iteration 625 --------------------------#
[32m[20230113 19:58:49 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 19:58:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:49 @agent_ppo2.py:186][0m |          -0.0012 |          27.0121 |           4.4071 |
[32m[20230113 19:58:49 @agent_ppo2.py:186][0m |          -0.0084 |          12.0382 |           4.4006 |
[32m[20230113 19:58:49 @agent_ppo2.py:186][0m |          -0.0098 |           9.1873 |           4.3987 |
[32m[20230113 19:58:49 @agent_ppo2.py:186][0m |          -0.0108 |           8.0715 |           4.3983 |
[32m[20230113 19:58:49 @agent_ppo2.py:186][0m |          -0.0116 |           7.5801 |           4.3995 |
[32m[20230113 19:58:49 @agent_ppo2.py:186][0m |          -0.0136 |           7.1147 |           4.3976 |
[32m[20230113 19:58:49 @agent_ppo2.py:186][0m |          -0.0144 |           6.7266 |           4.3997 |
[32m[20230113 19:58:49 @agent_ppo2.py:186][0m |          -0.0153 |           6.5110 |           4.3986 |
[32m[20230113 19:58:49 @agent_ppo2.py:186][0m |          -0.0156 |           6.3655 |           4.3972 |
[32m[20230113 19:58:49 @agent_ppo2.py:186][0m |          -0.0167 |           6.2086 |           4.4019 |
[32m[20230113 19:58:49 @agent_ppo2.py:131][0m Policy update time: 0.34 s
[32m[20230113 19:58:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 112.26
[32m[20230113 19:58:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.81
[32m[20230113 19:58:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.60
[32m[20230113 19:58:49 @agent_ppo2.py:144][0m Total time:      14.28 min
[32m[20230113 19:58:49 @agent_ppo2.py:146][0m 1282048 total steps have happened
[32m[20230113 19:58:49 @agent_ppo2.py:122][0m #------------------------ Iteration 626 --------------------------#
[32m[20230113 19:58:50 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 19:58:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:50 @agent_ppo2.py:186][0m |          -0.0111 |           6.0250 |           4.4388 |
[32m[20230113 19:58:50 @agent_ppo2.py:186][0m |          -0.0084 |           5.5173 |           4.4414 |
[32m[20230113 19:58:50 @agent_ppo2.py:186][0m |           0.0203 |           6.2409 |           4.4405 |
[32m[20230113 19:58:50 @agent_ppo2.py:186][0m |          -0.0076 |           5.0766 |           4.4337 |
[32m[20230113 19:58:50 @agent_ppo2.py:186][0m |          -0.0105 |           4.9624 |           4.4411 |
[32m[20230113 19:58:50 @agent_ppo2.py:186][0m |          -0.0109 |           4.8525 |           4.4418 |
[32m[20230113 19:58:50 @agent_ppo2.py:186][0m |          -0.0040 |           4.7646 |           4.4458 |
[32m[20230113 19:58:50 @agent_ppo2.py:186][0m |          -0.0093 |           4.7038 |           4.4423 |
[32m[20230113 19:58:50 @agent_ppo2.py:186][0m |          -0.0085 |           4.6777 |           4.4439 |
[32m[20230113 19:58:50 @agent_ppo2.py:186][0m |          -0.0052 |           4.6141 |           4.4444 |
[32m[20230113 19:58:50 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:58:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.97
[32m[20230113 19:58:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.98
[32m[20230113 19:58:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.18
[32m[20230113 19:58:51 @agent_ppo2.py:144][0m Total time:      14.30 min
[32m[20230113 19:58:51 @agent_ppo2.py:146][0m 1284096 total steps have happened
[32m[20230113 19:58:51 @agent_ppo2.py:122][0m #------------------------ Iteration 627 --------------------------#
[32m[20230113 19:58:51 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:58:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:51 @agent_ppo2.py:186][0m |           0.0017 |           6.2879 |           4.5267 |
[32m[20230113 19:58:51 @agent_ppo2.py:186][0m |          -0.0016 |           5.5191 |           4.5226 |
[32m[20230113 19:58:51 @agent_ppo2.py:186][0m |          -0.0038 |           5.1248 |           4.5175 |
[32m[20230113 19:58:51 @agent_ppo2.py:186][0m |          -0.0047 |           4.9080 |           4.5163 |
[32m[20230113 19:58:51 @agent_ppo2.py:186][0m |          -0.0059 |           4.6828 |           4.5108 |
[32m[20230113 19:58:52 @agent_ppo2.py:186][0m |          -0.0071 |           4.5290 |           4.5076 |
[32m[20230113 19:58:52 @agent_ppo2.py:186][0m |          -0.0079 |           4.3911 |           4.5091 |
[32m[20230113 19:58:52 @agent_ppo2.py:186][0m |          -0.0074 |           4.2848 |           4.5048 |
[32m[20230113 19:58:52 @agent_ppo2.py:186][0m |          -0.0088 |           4.1931 |           4.5056 |
[32m[20230113 19:58:52 @agent_ppo2.py:186][0m |          -0.0090 |           4.1262 |           4.4997 |
[32m[20230113 19:58:52 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 19:58:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.36
[32m[20230113 19:58:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.43
[32m[20230113 19:58:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.25
[32m[20230113 19:58:52 @agent_ppo2.py:144][0m Total time:      14.33 min
[32m[20230113 19:58:52 @agent_ppo2.py:146][0m 1286144 total steps have happened
[32m[20230113 19:58:52 @agent_ppo2.py:122][0m #------------------------ Iteration 628 --------------------------#
[32m[20230113 19:58:53 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:58:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:53 @agent_ppo2.py:186][0m |          -0.0011 |           5.9669 |           4.4704 |
[32m[20230113 19:58:53 @agent_ppo2.py:186][0m |          -0.0037 |           5.5597 |           4.4682 |
[32m[20230113 19:58:53 @agent_ppo2.py:186][0m |          -0.0041 |           5.3648 |           4.4663 |
[32m[20230113 19:58:53 @agent_ppo2.py:186][0m |          -0.0092 |           5.1703 |           4.4630 |
[32m[20230113 19:58:53 @agent_ppo2.py:186][0m |          -0.0077 |           5.0943 |           4.4633 |
[32m[20230113 19:58:53 @agent_ppo2.py:186][0m |          -0.0102 |           4.9651 |           4.4640 |
[32m[20230113 19:58:53 @agent_ppo2.py:186][0m |          -0.0107 |           4.8724 |           4.4603 |
[32m[20230113 19:58:53 @agent_ppo2.py:186][0m |          -0.0118 |           4.8041 |           4.4613 |
[32m[20230113 19:58:53 @agent_ppo2.py:186][0m |          -0.0091 |           4.8164 |           4.4604 |
[32m[20230113 19:58:53 @agent_ppo2.py:186][0m |          -0.0119 |           4.6643 |           4.4583 |
[32m[20230113 19:58:53 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:58:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.90
[32m[20230113 19:58:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.83
[32m[20230113 19:58:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.60
[32m[20230113 19:58:53 @agent_ppo2.py:144][0m Total time:      14.35 min
[32m[20230113 19:58:53 @agent_ppo2.py:146][0m 1288192 total steps have happened
[32m[20230113 19:58:53 @agent_ppo2.py:122][0m #------------------------ Iteration 629 --------------------------#
[32m[20230113 19:58:54 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:58:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:54 @agent_ppo2.py:186][0m |          -0.0012 |           6.4774 |           4.3708 |
[32m[20230113 19:58:54 @agent_ppo2.py:186][0m |          -0.0066 |           5.7413 |           4.3632 |
[32m[20230113 19:58:54 @agent_ppo2.py:186][0m |          -0.0041 |           5.4308 |           4.3558 |
[32m[20230113 19:58:54 @agent_ppo2.py:186][0m |          -0.0051 |           5.1976 |           4.3561 |
[32m[20230113 19:58:54 @agent_ppo2.py:186][0m |          -0.0080 |           5.0083 |           4.3527 |
[32m[20230113 19:58:54 @agent_ppo2.py:186][0m |          -0.0135 |           4.8263 |           4.3552 |
[32m[20230113 19:58:54 @agent_ppo2.py:186][0m |          -0.0160 |           4.7158 |           4.3527 |
[32m[20230113 19:58:54 @agent_ppo2.py:186][0m |          -0.0121 |           4.5809 |           4.3527 |
[32m[20230113 19:58:54 @agent_ppo2.py:186][0m |          -0.0102 |           4.4956 |           4.3497 |
[32m[20230113 19:58:54 @agent_ppo2.py:186][0m |          -0.0097 |           4.3924 |           4.3584 |
[32m[20230113 19:58:54 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 19:58:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.33
[32m[20230113 19:58:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.17
[32m[20230113 19:58:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.71
[32m[20230113 19:58:55 @agent_ppo2.py:144][0m Total time:      14.37 min
[32m[20230113 19:58:55 @agent_ppo2.py:146][0m 1290240 total steps have happened
[32m[20230113 19:58:55 @agent_ppo2.py:122][0m #------------------------ Iteration 630 --------------------------#
[32m[20230113 19:58:55 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:58:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:55 @agent_ppo2.py:186][0m |          -0.0026 |           5.4760 |           4.5985 |
[32m[20230113 19:58:55 @agent_ppo2.py:186][0m |          -0.0054 |           5.1954 |           4.5961 |
[32m[20230113 19:58:55 @agent_ppo2.py:186][0m |          -0.0076 |           5.0645 |           4.5896 |
[32m[20230113 19:58:55 @agent_ppo2.py:186][0m |          -0.0122 |           4.7725 |           4.5883 |
[32m[20230113 19:58:56 @agent_ppo2.py:186][0m |          -0.0117 |           4.6669 |           4.5897 |
[32m[20230113 19:58:56 @agent_ppo2.py:186][0m |          -0.0111 |           4.6311 |           4.5858 |
[32m[20230113 19:58:56 @agent_ppo2.py:186][0m |          -0.0121 |           4.5590 |           4.5871 |
[32m[20230113 19:58:56 @agent_ppo2.py:186][0m |          -0.0124 |           4.4864 |           4.5851 |
[32m[20230113 19:58:56 @agent_ppo2.py:186][0m |          -0.0141 |           4.4585 |           4.5850 |
[32m[20230113 19:58:56 @agent_ppo2.py:186][0m |          -0.0131 |           4.3948 |           4.5840 |
[32m[20230113 19:58:56 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.68
[32m[20230113 19:58:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.58
[32m[20230113 19:58:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.95
[32m[20230113 19:58:56 @agent_ppo2.py:144][0m Total time:      14.39 min
[32m[20230113 19:58:56 @agent_ppo2.py:146][0m 1292288 total steps have happened
[32m[20230113 19:58:56 @agent_ppo2.py:122][0m #------------------------ Iteration 631 --------------------------#
[32m[20230113 19:58:57 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:58:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:57 @agent_ppo2.py:186][0m |           0.0018 |           6.1677 |           4.5583 |
[32m[20230113 19:58:57 @agent_ppo2.py:186][0m |          -0.0061 |           5.4296 |           4.5477 |
[32m[20230113 19:58:57 @agent_ppo2.py:186][0m |          -0.0070 |           5.2536 |           4.5536 |
[32m[20230113 19:58:57 @agent_ppo2.py:186][0m |          -0.0068 |           5.1263 |           4.5512 |
[32m[20230113 19:58:57 @agent_ppo2.py:186][0m |          -0.0097 |           5.0520 |           4.5501 |
[32m[20230113 19:58:57 @agent_ppo2.py:186][0m |          -0.0083 |           4.9229 |           4.5517 |
[32m[20230113 19:58:57 @agent_ppo2.py:186][0m |          -0.0123 |           4.8256 |           4.5515 |
[32m[20230113 19:58:57 @agent_ppo2.py:186][0m |          -0.0086 |           4.8434 |           4.5527 |
[32m[20230113 19:58:57 @agent_ppo2.py:186][0m |          -0.0123 |           4.7323 |           4.5534 |
[32m[20230113 19:58:57 @agent_ppo2.py:186][0m |          -0.0133 |           4.6865 |           4.5526 |
[32m[20230113 19:58:57 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:58:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.35
[32m[20230113 19:58:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.55
[32m[20230113 19:58:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.53
[32m[20230113 19:58:57 @agent_ppo2.py:144][0m Total time:      14.42 min
[32m[20230113 19:58:57 @agent_ppo2.py:146][0m 1294336 total steps have happened
[32m[20230113 19:58:57 @agent_ppo2.py:122][0m #------------------------ Iteration 632 --------------------------#
[32m[20230113 19:58:58 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:58:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:58 @agent_ppo2.py:186][0m |           0.0007 |           5.5027 |           4.4981 |
[32m[20230113 19:58:58 @agent_ppo2.py:186][0m |          -0.0036 |           5.1178 |           4.4929 |
[32m[20230113 19:58:58 @agent_ppo2.py:186][0m |          -0.0076 |           4.8256 |           4.4852 |
[32m[20230113 19:58:58 @agent_ppo2.py:186][0m |          -0.0092 |           4.6967 |           4.4789 |
[32m[20230113 19:58:58 @agent_ppo2.py:186][0m |          -0.0095 |           4.6178 |           4.4734 |
[32m[20230113 19:58:58 @agent_ppo2.py:186][0m |          -0.0106 |           4.4807 |           4.4753 |
[32m[20230113 19:58:58 @agent_ppo2.py:186][0m |          -0.0087 |           4.4274 |           4.4685 |
[32m[20230113 19:58:58 @agent_ppo2.py:186][0m |          -0.0124 |           4.2752 |           4.4685 |
[32m[20230113 19:58:58 @agent_ppo2.py:186][0m |          -0.0117 |           4.2173 |           4.4631 |
[32m[20230113 19:58:58 @agent_ppo2.py:186][0m |          -0.0122 |           4.1560 |           4.4608 |
[32m[20230113 19:58:58 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:58:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.24
[32m[20230113 19:58:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.57
[32m[20230113 19:58:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.17
[32m[20230113 19:58:59 @agent_ppo2.py:144][0m Total time:      14.44 min
[32m[20230113 19:58:59 @agent_ppo2.py:146][0m 1296384 total steps have happened
[32m[20230113 19:58:59 @agent_ppo2.py:122][0m #------------------------ Iteration 633 --------------------------#
[32m[20230113 19:58:59 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:58:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:58:59 @agent_ppo2.py:186][0m |          -0.0072 |           6.9797 |           4.2899 |
[32m[20230113 19:58:59 @agent_ppo2.py:186][0m |          -0.0128 |           5.8677 |           4.2794 |
[32m[20230113 19:58:59 @agent_ppo2.py:186][0m |          -0.0092 |           5.6125 |           4.2800 |
[32m[20230113 19:58:59 @agent_ppo2.py:186][0m |          -0.0092 |           5.4728 |           4.2779 |
[32m[20230113 19:58:59 @agent_ppo2.py:186][0m |          -0.0023 |           5.3279 |           4.2795 |
[32m[20230113 19:59:00 @agent_ppo2.py:186][0m |          -0.0114 |           5.1915 |           4.2748 |
[32m[20230113 19:59:00 @agent_ppo2.py:186][0m |          -0.0092 |           5.1176 |           4.2740 |
[32m[20230113 19:59:00 @agent_ppo2.py:186][0m |          -0.0072 |           5.0403 |           4.2720 |
[32m[20230113 19:59:00 @agent_ppo2.py:186][0m |          -0.0136 |           4.9569 |           4.2719 |
[32m[20230113 19:59:00 @agent_ppo2.py:186][0m |           0.0024 |           4.9834 |           4.2741 |
[32m[20230113 19:59:00 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.18
[32m[20230113 19:59:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.10
[32m[20230113 19:59:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.57
[32m[20230113 19:59:00 @agent_ppo2.py:144][0m Total time:      14.46 min
[32m[20230113 19:59:00 @agent_ppo2.py:146][0m 1298432 total steps have happened
[32m[20230113 19:59:00 @agent_ppo2.py:122][0m #------------------------ Iteration 634 --------------------------#
[32m[20230113 19:59:01 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 19:59:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:01 @agent_ppo2.py:186][0m |          -0.0044 |          14.1580 |           4.4596 |
[32m[20230113 19:59:01 @agent_ppo2.py:186][0m |          -0.0028 |           5.6719 |           4.4534 |
[32m[20230113 19:59:01 @agent_ppo2.py:186][0m |          -0.0167 |           4.8162 |           4.4492 |
[32m[20230113 19:59:01 @agent_ppo2.py:186][0m |          -0.0017 |           4.4021 |           4.4449 |
[32m[20230113 19:59:01 @agent_ppo2.py:186][0m |          -0.0111 |           4.1223 |           4.4410 |
[32m[20230113 19:59:01 @agent_ppo2.py:186][0m |          -0.0134 |           3.9236 |           4.4374 |
[32m[20230113 19:59:01 @agent_ppo2.py:186][0m |          -0.0078 |           3.7791 |           4.4360 |
[32m[20230113 19:59:01 @agent_ppo2.py:186][0m |          -0.0092 |           3.7077 |           4.4336 |
[32m[20230113 19:59:01 @agent_ppo2.py:186][0m |          -0.0126 |           3.6270 |           4.4307 |
[32m[20230113 19:59:01 @agent_ppo2.py:186][0m |          -0.0087 |           3.5623 |           4.4261 |
[32m[20230113 19:59:01 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 19:59:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 140.52
[32m[20230113 19:59:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.74
[32m[20230113 19:59:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.30
[32m[20230113 19:59:02 @agent_ppo2.py:144][0m Total time:      14.48 min
[32m[20230113 19:59:02 @agent_ppo2.py:146][0m 1300480 total steps have happened
[32m[20230113 19:59:02 @agent_ppo2.py:122][0m #------------------------ Iteration 635 --------------------------#
[32m[20230113 19:59:02 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:02 @agent_ppo2.py:186][0m |          -0.0009 |           6.1411 |           4.3466 |
[32m[20230113 19:59:02 @agent_ppo2.py:186][0m |          -0.0043 |           5.1814 |           4.3473 |
[32m[20230113 19:59:02 @agent_ppo2.py:186][0m |          -0.0091 |           4.6879 |           4.3434 |
[32m[20230113 19:59:02 @agent_ppo2.py:186][0m |          -0.0106 |           4.4267 |           4.3409 |
[32m[20230113 19:59:02 @agent_ppo2.py:186][0m |          -0.0114 |           4.1844 |           4.3431 |
[32m[20230113 19:59:02 @agent_ppo2.py:186][0m |          -0.0118 |           3.9765 |           4.3420 |
[32m[20230113 19:59:02 @agent_ppo2.py:186][0m |          -0.0111 |           3.8349 |           4.3417 |
[32m[20230113 19:59:02 @agent_ppo2.py:186][0m |          -0.0138 |           3.6881 |           4.3437 |
[32m[20230113 19:59:02 @agent_ppo2.py:186][0m |          -0.0135 |           3.5574 |           4.3433 |
[32m[20230113 19:59:03 @agent_ppo2.py:186][0m |          -0.0142 |           3.4588 |           4.3459 |
[32m[20230113 19:59:03 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:59:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.31
[32m[20230113 19:59:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.60
[32m[20230113 19:59:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.28
[32m[20230113 19:59:03 @agent_ppo2.py:144][0m Total time:      14.51 min
[32m[20230113 19:59:03 @agent_ppo2.py:146][0m 1302528 total steps have happened
[32m[20230113 19:59:03 @agent_ppo2.py:122][0m #------------------------ Iteration 636 --------------------------#
[32m[20230113 19:59:03 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:03 @agent_ppo2.py:186][0m |           0.0071 |           5.7965 |           4.3589 |
[32m[20230113 19:59:04 @agent_ppo2.py:186][0m |          -0.0050 |           5.0156 |           4.3525 |
[32m[20230113 19:59:04 @agent_ppo2.py:186][0m |          -0.0080 |           4.8496 |           4.3576 |
[32m[20230113 19:59:04 @agent_ppo2.py:186][0m |          -0.0012 |           4.8028 |           4.3596 |
[32m[20230113 19:59:04 @agent_ppo2.py:186][0m |          -0.0097 |           4.6111 |           4.3613 |
[32m[20230113 19:59:04 @agent_ppo2.py:186][0m |          -0.0088 |           4.5430 |           4.3615 |
[32m[20230113 19:59:04 @agent_ppo2.py:186][0m |           0.0011 |           4.8115 |           4.3667 |
[32m[20230113 19:59:04 @agent_ppo2.py:186][0m |          -0.0148 |           4.4514 |           4.3631 |
[32m[20230113 19:59:04 @agent_ppo2.py:186][0m |          -0.0005 |           4.5574 |           4.3671 |
[32m[20230113 19:59:04 @agent_ppo2.py:186][0m |          -0.0062 |           4.3527 |           4.3674 |
[32m[20230113 19:59:04 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.15
[32m[20230113 19:59:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.77
[32m[20230113 19:59:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.77
[32m[20230113 19:59:04 @agent_ppo2.py:144][0m Total time:      14.53 min
[32m[20230113 19:59:04 @agent_ppo2.py:146][0m 1304576 total steps have happened
[32m[20230113 19:59:04 @agent_ppo2.py:122][0m #------------------------ Iteration 637 --------------------------#
[32m[20230113 19:59:05 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:05 @agent_ppo2.py:186][0m |           0.0057 |           5.9539 |           4.3137 |
[32m[20230113 19:59:05 @agent_ppo2.py:186][0m |          -0.0092 |           4.7276 |           4.3049 |
[32m[20230113 19:59:05 @agent_ppo2.py:186][0m |          -0.0088 |           4.2475 |           4.3050 |
[32m[20230113 19:59:05 @agent_ppo2.py:186][0m |          -0.0177 |           4.0664 |           4.2994 |
[32m[20230113 19:59:05 @agent_ppo2.py:186][0m |          -0.0189 |           4.0063 |           4.3034 |
[32m[20230113 19:59:05 @agent_ppo2.py:186][0m |           0.0085 |           3.9346 |           4.2991 |
[32m[20230113 19:59:05 @agent_ppo2.py:186][0m |          -0.0175 |           4.0083 |           4.2962 |
[32m[20230113 19:59:05 @agent_ppo2.py:186][0m |          -0.0150 |           3.6668 |           4.3009 |
[32m[20230113 19:59:05 @agent_ppo2.py:186][0m |          -0.0134 |           3.5515 |           4.2998 |
[32m[20230113 19:59:05 @agent_ppo2.py:186][0m |          -0.0194 |           3.4951 |           4.2965 |
[32m[20230113 19:59:05 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.28
[32m[20230113 19:59:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.34
[32m[20230113 19:59:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.58
[32m[20230113 19:59:06 @agent_ppo2.py:144][0m Total time:      14.55 min
[32m[20230113 19:59:06 @agent_ppo2.py:146][0m 1306624 total steps have happened
[32m[20230113 19:59:06 @agent_ppo2.py:122][0m #------------------------ Iteration 638 --------------------------#
[32m[20230113 19:59:06 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:06 @agent_ppo2.py:186][0m |          -0.0026 |           7.1148 |           4.3962 |
[32m[20230113 19:59:06 @agent_ppo2.py:186][0m |          -0.0121 |           6.3815 |           4.3961 |
[32m[20230113 19:59:06 @agent_ppo2.py:186][0m |          -0.0128 |           6.1642 |           4.3933 |
[32m[20230113 19:59:06 @agent_ppo2.py:186][0m |          -0.0061 |           5.9945 |           4.3908 |
[32m[20230113 19:59:06 @agent_ppo2.py:186][0m |          -0.0113 |           5.8167 |           4.3899 |
[32m[20230113 19:59:06 @agent_ppo2.py:186][0m |          -0.0134 |           5.7903 |           4.3903 |
[32m[20230113 19:59:06 @agent_ppo2.py:186][0m |          -0.0104 |           5.6799 |           4.3871 |
[32m[20230113 19:59:06 @agent_ppo2.py:186][0m |          -0.0145 |           5.5925 |           4.3873 |
[32m[20230113 19:59:07 @agent_ppo2.py:186][0m |          -0.0120 |           5.6167 |           4.3888 |
[32m[20230113 19:59:07 @agent_ppo2.py:186][0m |          -0.0139 |           5.5101 |           4.3882 |
[32m[20230113 19:59:07 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.79
[32m[20230113 19:59:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.25
[32m[20230113 19:59:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.67
[32m[20230113 19:59:07 @agent_ppo2.py:144][0m Total time:      14.57 min
[32m[20230113 19:59:07 @agent_ppo2.py:146][0m 1308672 total steps have happened
[32m[20230113 19:59:07 @agent_ppo2.py:122][0m #------------------------ Iteration 639 --------------------------#
[32m[20230113 19:59:07 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:08 @agent_ppo2.py:186][0m |          -0.0053 |           6.4310 |           4.3640 |
[32m[20230113 19:59:08 @agent_ppo2.py:186][0m |          -0.0068 |           5.6439 |           4.3618 |
[32m[20230113 19:59:08 @agent_ppo2.py:186][0m |          -0.0026 |           5.2856 |           4.3622 |
[32m[20230113 19:59:08 @agent_ppo2.py:186][0m |          -0.0099 |           5.0963 |           4.3608 |
[32m[20230113 19:59:08 @agent_ppo2.py:186][0m |          -0.0129 |           4.9453 |           4.3586 |
[32m[20230113 19:59:08 @agent_ppo2.py:186][0m |          -0.0090 |           4.8275 |           4.3583 |
[32m[20230113 19:59:08 @agent_ppo2.py:186][0m |          -0.0097 |           4.7199 |           4.3571 |
[32m[20230113 19:59:08 @agent_ppo2.py:186][0m |          -0.0125 |           4.6538 |           4.3570 |
[32m[20230113 19:59:08 @agent_ppo2.py:186][0m |          -0.0080 |           4.5507 |           4.3566 |
[32m[20230113 19:59:08 @agent_ppo2.py:186][0m |          -0.0096 |           4.4779 |           4.3558 |
[32m[20230113 19:59:08 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.36
[32m[20230113 19:59:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.96
[32m[20230113 19:59:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.47
[32m[20230113 19:59:08 @agent_ppo2.py:144][0m Total time:      14.60 min
[32m[20230113 19:59:08 @agent_ppo2.py:146][0m 1310720 total steps have happened
[32m[20230113 19:59:08 @agent_ppo2.py:122][0m #------------------------ Iteration 640 --------------------------#
[32m[20230113 19:59:09 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:09 @agent_ppo2.py:186][0m |           0.0010 |           6.1065 |           4.5391 |
[32m[20230113 19:59:09 @agent_ppo2.py:186][0m |          -0.0024 |           5.3788 |           4.5338 |
[32m[20230113 19:59:09 @agent_ppo2.py:186][0m |          -0.0035 |           4.9980 |           4.5282 |
[32m[20230113 19:59:09 @agent_ppo2.py:186][0m |          -0.0041 |           4.7367 |           4.5259 |
[32m[20230113 19:59:09 @agent_ppo2.py:186][0m |          -0.0062 |           4.4685 |           4.5280 |
[32m[20230113 19:59:09 @agent_ppo2.py:186][0m |          -0.0082 |           4.3565 |           4.5256 |
[32m[20230113 19:59:09 @agent_ppo2.py:186][0m |          -0.0097 |           4.1889 |           4.5213 |
[32m[20230113 19:59:09 @agent_ppo2.py:186][0m |          -0.0092 |           4.1137 |           4.5226 |
[32m[20230113 19:59:09 @agent_ppo2.py:186][0m |          -0.0105 |           3.9990 |           4.5184 |
[32m[20230113 19:59:09 @agent_ppo2.py:186][0m |          -0.0099 |           3.9176 |           4.5187 |
[32m[20230113 19:59:09 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.28
[32m[20230113 19:59:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.03
[32m[20230113 19:59:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.80
[32m[20230113 19:59:10 @agent_ppo2.py:144][0m Total time:      14.62 min
[32m[20230113 19:59:10 @agent_ppo2.py:146][0m 1312768 total steps have happened
[32m[20230113 19:59:10 @agent_ppo2.py:122][0m #------------------------ Iteration 641 --------------------------#
[32m[20230113 19:59:10 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:59:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:10 @agent_ppo2.py:186][0m |          -0.0000 |           5.7365 |           4.3616 |
[32m[20230113 19:59:10 @agent_ppo2.py:186][0m |          -0.0020 |           5.0696 |           4.3614 |
[32m[20230113 19:59:10 @agent_ppo2.py:186][0m |          -0.0051 |           4.7787 |           4.3600 |
[32m[20230113 19:59:10 @agent_ppo2.py:186][0m |          -0.0069 |           4.6722 |           4.3649 |
[32m[20230113 19:59:10 @agent_ppo2.py:186][0m |          -0.0076 |           4.5129 |           4.3631 |
[32m[20230113 19:59:10 @agent_ppo2.py:186][0m |          -0.0090 |           4.3922 |           4.3655 |
[32m[20230113 19:59:10 @agent_ppo2.py:186][0m |          -0.0096 |           4.2832 |           4.3703 |
[32m[20230113 19:59:11 @agent_ppo2.py:186][0m |          -0.0109 |           4.2124 |           4.3734 |
[32m[20230113 19:59:11 @agent_ppo2.py:186][0m |          -0.0114 |           4.1538 |           4.3753 |
[32m[20230113 19:59:11 @agent_ppo2.py:186][0m |          -0.0117 |           4.0613 |           4.3757 |
[32m[20230113 19:59:11 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.29
[32m[20230113 19:59:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.12
[32m[20230113 19:59:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.79
[32m[20230113 19:59:11 @agent_ppo2.py:144][0m Total time:      14.64 min
[32m[20230113 19:59:11 @agent_ppo2.py:146][0m 1314816 total steps have happened
[32m[20230113 19:59:11 @agent_ppo2.py:122][0m #------------------------ Iteration 642 --------------------------#
[32m[20230113 19:59:12 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 19:59:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:12 @agent_ppo2.py:186][0m |           0.0004 |           5.6725 |           4.4383 |
[32m[20230113 19:59:12 @agent_ppo2.py:186][0m |          -0.0049 |           5.1816 |           4.4332 |
[32m[20230113 19:59:12 @agent_ppo2.py:186][0m |          -0.0082 |           4.9492 |           4.4282 |
[32m[20230113 19:59:12 @agent_ppo2.py:186][0m |          -0.0090 |           4.7707 |           4.4267 |
[32m[20230113 19:59:12 @agent_ppo2.py:186][0m |          -0.0098 |           4.6431 |           4.4275 |
[32m[20230113 19:59:12 @agent_ppo2.py:186][0m |          -0.0111 |           4.5346 |           4.4273 |
[32m[20230113 19:59:12 @agent_ppo2.py:186][0m |          -0.0105 |           4.4526 |           4.4274 |
[32m[20230113 19:59:12 @agent_ppo2.py:186][0m |          -0.0116 |           4.3456 |           4.4273 |
[32m[20230113 19:59:12 @agent_ppo2.py:186][0m |          -0.0126 |           4.2627 |           4.4275 |
[32m[20230113 19:59:12 @agent_ppo2.py:186][0m |          -0.0130 |           4.1617 |           4.4276 |
[32m[20230113 19:59:12 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.43
[32m[20230113 19:59:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.53
[32m[20230113 19:59:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 148.34
[32m[20230113 19:59:12 @agent_ppo2.py:144][0m Total time:      14.66 min
[32m[20230113 19:59:12 @agent_ppo2.py:146][0m 1316864 total steps have happened
[32m[20230113 19:59:12 @agent_ppo2.py:122][0m #------------------------ Iteration 643 --------------------------#
[32m[20230113 19:59:13 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:13 @agent_ppo2.py:186][0m |           0.0046 |           5.9685 |           4.4678 |
[32m[20230113 19:59:13 @agent_ppo2.py:186][0m |          -0.0127 |           5.5164 |           4.4647 |
[32m[20230113 19:59:13 @agent_ppo2.py:186][0m |          -0.0008 |           5.4446 |           4.4570 |
[32m[20230113 19:59:13 @agent_ppo2.py:186][0m |          -0.0146 |           5.1953 |           4.4630 |
[32m[20230113 19:59:13 @agent_ppo2.py:186][0m |          -0.0109 |           5.0311 |           4.4577 |
[32m[20230113 19:59:13 @agent_ppo2.py:186][0m |          -0.0119 |           4.9367 |           4.4602 |
[32m[20230113 19:59:13 @agent_ppo2.py:186][0m |           0.0213 |           6.1549 |           4.4622 |
[32m[20230113 19:59:13 @agent_ppo2.py:186][0m |          -0.0158 |           5.0843 |           4.4540 |
[32m[20230113 19:59:13 @agent_ppo2.py:186][0m |          -0.0108 |           4.6931 |           4.4595 |
[32m[20230113 19:59:13 @agent_ppo2.py:186][0m |          -0.0187 |           4.6400 |           4.4605 |
[32m[20230113 19:59:13 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.23
[32m[20230113 19:59:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.78
[32m[20230113 19:59:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.84
[32m[20230113 19:59:14 @agent_ppo2.py:144][0m Total time:      14.69 min
[32m[20230113 19:59:14 @agent_ppo2.py:146][0m 1318912 total steps have happened
[32m[20230113 19:59:14 @agent_ppo2.py:122][0m #------------------------ Iteration 644 --------------------------#
[32m[20230113 19:59:14 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:14 @agent_ppo2.py:186][0m |          -0.0005 |           4.3063 |           4.4558 |
[32m[20230113 19:59:14 @agent_ppo2.py:186][0m |          -0.0130 |           3.6013 |           4.4482 |
[32m[20230113 19:59:14 @agent_ppo2.py:186][0m |          -0.0073 |           3.4261 |           4.4499 |
[32m[20230113 19:59:14 @agent_ppo2.py:186][0m |          -0.0153 |           3.3194 |           4.4472 |
[32m[20230113 19:59:14 @agent_ppo2.py:186][0m |          -0.0083 |           3.1608 |           4.4438 |
[32m[20230113 19:59:15 @agent_ppo2.py:186][0m |          -0.0108 |           3.0475 |           4.4476 |
[32m[20230113 19:59:15 @agent_ppo2.py:186][0m |          -0.0105 |           2.9843 |           4.4453 |
[32m[20230113 19:59:15 @agent_ppo2.py:186][0m |          -0.0222 |           2.9300 |           4.4456 |
[32m[20230113 19:59:15 @agent_ppo2.py:186][0m |          -0.0114 |           2.8792 |           4.4463 |
[32m[20230113 19:59:15 @agent_ppo2.py:186][0m |          -0.0051 |           2.9974 |           4.4454 |
[32m[20230113 19:59:15 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.07
[32m[20230113 19:59:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.66
[32m[20230113 19:59:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.32
[32m[20230113 19:59:15 @agent_ppo2.py:144][0m Total time:      14.71 min
[32m[20230113 19:59:15 @agent_ppo2.py:146][0m 1320960 total steps have happened
[32m[20230113 19:59:15 @agent_ppo2.py:122][0m #------------------------ Iteration 645 --------------------------#
[32m[20230113 19:59:16 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:16 @agent_ppo2.py:186][0m |           0.0006 |           5.1717 |           4.6127 |
[32m[20230113 19:59:16 @agent_ppo2.py:186][0m |          -0.0057 |           4.5142 |           4.6131 |
[32m[20230113 19:59:16 @agent_ppo2.py:186][0m |          -0.0069 |           4.2698 |           4.6081 |
[32m[20230113 19:59:16 @agent_ppo2.py:186][0m |          -0.0084 |           4.0694 |           4.6095 |
[32m[20230113 19:59:16 @agent_ppo2.py:186][0m |          -0.0082 |           3.9761 |           4.6074 |
[32m[20230113 19:59:16 @agent_ppo2.py:186][0m |          -0.0122 |           3.8195 |           4.6054 |
[32m[20230113 19:59:16 @agent_ppo2.py:186][0m |          -0.0121 |           3.7285 |           4.6041 |
[32m[20230113 19:59:16 @agent_ppo2.py:186][0m |          -0.0130 |           3.6732 |           4.6020 |
[32m[20230113 19:59:16 @agent_ppo2.py:186][0m |          -0.0121 |           3.6092 |           4.6048 |
[32m[20230113 19:59:16 @agent_ppo2.py:186][0m |          -0.0136 |           3.5346 |           4.6047 |
[32m[20230113 19:59:16 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.97
[32m[20230113 19:59:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.36
[32m[20230113 19:59:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.75
[32m[20230113 19:59:16 @agent_ppo2.py:144][0m Total time:      14.73 min
[32m[20230113 19:59:16 @agent_ppo2.py:146][0m 1323008 total steps have happened
[32m[20230113 19:59:16 @agent_ppo2.py:122][0m #------------------------ Iteration 646 --------------------------#
[32m[20230113 19:59:17 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:17 @agent_ppo2.py:186][0m |           0.0003 |           6.5025 |           4.5271 |
[32m[20230113 19:59:17 @agent_ppo2.py:186][0m |          -0.0042 |           5.5492 |           4.5253 |
[32m[20230113 19:59:17 @agent_ppo2.py:186][0m |          -0.0072 |           4.9921 |           4.5213 |
[32m[20230113 19:59:17 @agent_ppo2.py:186][0m |          -0.0107 |           4.6899 |           4.5227 |
[32m[20230113 19:59:17 @agent_ppo2.py:186][0m |          -0.0080 |           4.3737 |           4.5184 |
[32m[20230113 19:59:17 @agent_ppo2.py:186][0m |          -0.0094 |           4.1947 |           4.5271 |
[32m[20230113 19:59:17 @agent_ppo2.py:186][0m |          -0.0137 |           3.9749 |           4.5222 |
[32m[20230113 19:59:17 @agent_ppo2.py:186][0m |          -0.0161 |           3.8576 |           4.5202 |
[32m[20230113 19:59:17 @agent_ppo2.py:186][0m |          -0.0081 |           3.7865 |           4.5234 |
[32m[20230113 19:59:17 @agent_ppo2.py:186][0m |          -0.0133 |           3.6244 |           4.5239 |
[32m[20230113 19:59:17 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.39
[32m[20230113 19:59:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.13
[32m[20230113 19:59:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.18
[32m[20230113 19:59:18 @agent_ppo2.py:144][0m Total time:      14.75 min
[32m[20230113 19:59:18 @agent_ppo2.py:146][0m 1325056 total steps have happened
[32m[20230113 19:59:18 @agent_ppo2.py:122][0m #------------------------ Iteration 647 --------------------------#
[32m[20230113 19:59:18 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:18 @agent_ppo2.py:186][0m |           0.0093 |           6.6069 |           4.4257 |
[32m[20230113 19:59:18 @agent_ppo2.py:186][0m |          -0.0007 |           5.7159 |           4.4227 |
[32m[20230113 19:59:18 @agent_ppo2.py:186][0m |          -0.0100 |           5.3137 |           4.4177 |
[32m[20230113 19:59:18 @agent_ppo2.py:186][0m |           0.0030 |           5.5335 |           4.4218 |
[32m[20230113 19:59:18 @agent_ppo2.py:186][0m |          -0.0036 |           5.0569 |           4.4185 |
[32m[20230113 19:59:19 @agent_ppo2.py:186][0m |          -0.0070 |           4.8995 |           4.4233 |
[32m[20230113 19:59:19 @agent_ppo2.py:186][0m |          -0.0036 |           5.1440 |           4.4239 |
[32m[20230113 19:59:19 @agent_ppo2.py:186][0m |          -0.0095 |           4.7137 |           4.4223 |
[32m[20230113 19:59:19 @agent_ppo2.py:186][0m |          -0.0143 |           4.6542 |           4.4260 |
[32m[20230113 19:59:19 @agent_ppo2.py:186][0m |          -0.0080 |           4.5888 |           4.4247 |
[32m[20230113 19:59:19 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.13
[32m[20230113 19:59:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.24
[32m[20230113 19:59:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.63
[32m[20230113 19:59:19 @agent_ppo2.py:144][0m Total time:      14.78 min
[32m[20230113 19:59:19 @agent_ppo2.py:146][0m 1327104 total steps have happened
[32m[20230113 19:59:19 @agent_ppo2.py:122][0m #------------------------ Iteration 648 --------------------------#
[32m[20230113 19:59:20 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:20 @agent_ppo2.py:186][0m |          -0.0041 |           3.6649 |           4.6149 |
[32m[20230113 19:59:20 @agent_ppo2.py:186][0m |          -0.0007 |           3.4721 |           4.6096 |
[32m[20230113 19:59:20 @agent_ppo2.py:186][0m |           0.0150 |           3.8152 |           4.6037 |
[32m[20230113 19:59:20 @agent_ppo2.py:186][0m |          -0.0092 |           3.2671 |           4.5968 |
[32m[20230113 19:59:20 @agent_ppo2.py:186][0m |          -0.0079 |           3.1252 |           4.5934 |
[32m[20230113 19:59:20 @agent_ppo2.py:186][0m |          -0.0108 |           3.0784 |           4.5925 |
[32m[20230113 19:59:20 @agent_ppo2.py:186][0m |          -0.0167 |           3.0358 |           4.5898 |
[32m[20230113 19:59:20 @agent_ppo2.py:186][0m |          -0.0087 |           2.9868 |           4.5881 |
[32m[20230113 19:59:20 @agent_ppo2.py:186][0m |          -0.0099 |           2.9430 |           4.5879 |
[32m[20230113 19:59:20 @agent_ppo2.py:186][0m |          -0.0081 |           2.9064 |           4.5872 |
[32m[20230113 19:59:20 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.25
[32m[20230113 19:59:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.15
[32m[20230113 19:59:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.59
[32m[20230113 19:59:20 @agent_ppo2.py:144][0m Total time:      14.80 min
[32m[20230113 19:59:20 @agent_ppo2.py:146][0m 1329152 total steps have happened
[32m[20230113 19:59:20 @agent_ppo2.py:122][0m #------------------------ Iteration 649 --------------------------#
[32m[20230113 19:59:21 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 19:59:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:21 @agent_ppo2.py:186][0m |          -0.0023 |          13.8129 |           4.4941 |
[32m[20230113 19:59:21 @agent_ppo2.py:186][0m |          -0.0056 |           5.5239 |           4.4972 |
[32m[20230113 19:59:21 @agent_ppo2.py:186][0m |          -0.0075 |           4.8623 |           4.4948 |
[32m[20230113 19:59:21 @agent_ppo2.py:186][0m |          -0.0105 |           4.4043 |           4.4960 |
[32m[20230113 19:59:21 @agent_ppo2.py:186][0m |          -0.0113 |           4.0548 |           4.4939 |
[32m[20230113 19:59:21 @agent_ppo2.py:186][0m |          -0.0168 |           3.8585 |           4.4926 |
[32m[20230113 19:59:21 @agent_ppo2.py:186][0m |          -0.0162 |           3.7083 |           4.4924 |
[32m[20230113 19:59:21 @agent_ppo2.py:186][0m |          -0.0099 |           3.5460 |           4.4931 |
[32m[20230113 19:59:21 @agent_ppo2.py:186][0m |          -0.0174 |           3.3702 |           4.4935 |
[32m[20230113 19:59:21 @agent_ppo2.py:186][0m |          -0.0171 |           3.2596 |           4.4967 |
[32m[20230113 19:59:21 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 19:59:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 125.38
[32m[20230113 19:59:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.13
[32m[20230113 19:59:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.51
[32m[20230113 19:59:22 @agent_ppo2.py:144][0m Total time:      14.82 min
[32m[20230113 19:59:22 @agent_ppo2.py:146][0m 1331200 total steps have happened
[32m[20230113 19:59:22 @agent_ppo2.py:122][0m #------------------------ Iteration 650 --------------------------#
[32m[20230113 19:59:22 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 19:59:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:22 @agent_ppo2.py:186][0m |          -0.0053 |          15.1084 |           4.5427 |
[32m[20230113 19:59:22 @agent_ppo2.py:186][0m |          -0.0094 |          11.9072 |           4.5379 |
[32m[20230113 19:59:22 @agent_ppo2.py:186][0m |          -0.0102 |          10.1193 |           4.5335 |
[32m[20230113 19:59:22 @agent_ppo2.py:186][0m |          -0.0123 |           9.5568 |           4.5357 |
[32m[20230113 19:59:22 @agent_ppo2.py:186][0m |          -0.0116 |           9.1404 |           4.5323 |
[32m[20230113 19:59:22 @agent_ppo2.py:186][0m |          -0.0147 |           8.4338 |           4.5303 |
[32m[20230113 19:59:22 @agent_ppo2.py:186][0m |          -0.0083 |           8.3122 |           4.5352 |
[32m[20230113 19:59:23 @agent_ppo2.py:186][0m |          -0.0158 |           7.9365 |           4.5318 |
[32m[20230113 19:59:23 @agent_ppo2.py:186][0m |          -0.0169 |           7.4143 |           4.5325 |
[32m[20230113 19:59:23 @agent_ppo2.py:186][0m |          -0.0166 |           7.2874 |           4.5329 |
[32m[20230113 19:59:23 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 19:59:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 142.30
[32m[20230113 19:59:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.24
[32m[20230113 19:59:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.95
[32m[20230113 19:59:23 @agent_ppo2.py:144][0m Total time:      14.84 min
[32m[20230113 19:59:23 @agent_ppo2.py:146][0m 1333248 total steps have happened
[32m[20230113 19:59:23 @agent_ppo2.py:122][0m #------------------------ Iteration 651 --------------------------#
[32m[20230113 19:59:23 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:24 @agent_ppo2.py:186][0m |           0.0063 |           6.2399 |           4.5570 |
[32m[20230113 19:59:24 @agent_ppo2.py:186][0m |          -0.0074 |           5.2643 |           4.5579 |
[32m[20230113 19:59:24 @agent_ppo2.py:186][0m |          -0.0082 |           4.9710 |           4.5625 |
[32m[20230113 19:59:24 @agent_ppo2.py:186][0m |          -0.0056 |           4.9119 |           4.5662 |
[32m[20230113 19:59:24 @agent_ppo2.py:186][0m |          -0.0089 |           4.5550 |           4.5666 |
[32m[20230113 19:59:24 @agent_ppo2.py:186][0m |          -0.0088 |           4.4963 |           4.5666 |
[32m[20230113 19:59:24 @agent_ppo2.py:186][0m |          -0.0108 |           4.2666 |           4.5682 |
[32m[20230113 19:59:24 @agent_ppo2.py:186][0m |          -0.0093 |           4.1590 |           4.5676 |
[32m[20230113 19:59:24 @agent_ppo2.py:186][0m |          -0.0102 |           4.0882 |           4.5684 |
[32m[20230113 19:59:24 @agent_ppo2.py:186][0m |          -0.0057 |           4.1152 |           4.5672 |
[32m[20230113 19:59:24 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.79
[32m[20230113 19:59:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.04
[32m[20230113 19:59:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.14
[32m[20230113 19:59:24 @agent_ppo2.py:144][0m Total time:      14.86 min
[32m[20230113 19:59:24 @agent_ppo2.py:146][0m 1335296 total steps have happened
[32m[20230113 19:59:24 @agent_ppo2.py:122][0m #------------------------ Iteration 652 --------------------------#
[32m[20230113 19:59:25 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:25 @agent_ppo2.py:186][0m |          -0.0025 |           6.5450 |           4.5357 |
[32m[20230113 19:59:25 @agent_ppo2.py:186][0m |          -0.0059 |           5.9719 |           4.5301 |
[32m[20230113 19:59:25 @agent_ppo2.py:186][0m |          -0.0073 |           5.5752 |           4.5270 |
[32m[20230113 19:59:25 @agent_ppo2.py:186][0m |          -0.0116 |           5.2874 |           4.5259 |
[32m[20230113 19:59:25 @agent_ppo2.py:186][0m |          -0.0094 |           5.1141 |           4.5213 |
[32m[20230113 19:59:25 @agent_ppo2.py:186][0m |          -0.0139 |           5.0449 |           4.5203 |
[32m[20230113 19:59:25 @agent_ppo2.py:186][0m |          -0.0108 |           4.8545 |           4.5172 |
[32m[20230113 19:59:25 @agent_ppo2.py:186][0m |          -0.0022 |           5.6981 |           4.5188 |
[32m[20230113 19:59:25 @agent_ppo2.py:186][0m |          -0.0120 |           4.6859 |           4.5173 |
[32m[20230113 19:59:25 @agent_ppo2.py:186][0m |          -0.0114 |           4.5668 |           4.5177 |
[32m[20230113 19:59:25 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.66
[32m[20230113 19:59:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.14
[32m[20230113 19:59:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.34
[32m[20230113 19:59:26 @agent_ppo2.py:144][0m Total time:      14.89 min
[32m[20230113 19:59:26 @agent_ppo2.py:146][0m 1337344 total steps have happened
[32m[20230113 19:59:26 @agent_ppo2.py:122][0m #------------------------ Iteration 653 --------------------------#
[32m[20230113 19:59:26 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:26 @agent_ppo2.py:186][0m |          -0.0038 |           6.5906 |           4.4615 |
[32m[20230113 19:59:26 @agent_ppo2.py:186][0m |           0.0008 |           5.6766 |           4.4601 |
[32m[20230113 19:59:26 @agent_ppo2.py:186][0m |           0.0434 |           6.5976 |           4.4562 |
[32m[20230113 19:59:26 @agent_ppo2.py:186][0m |          -0.0096 |           5.6002 |           4.4510 |
[32m[20230113 19:59:26 @agent_ppo2.py:186][0m |          -0.0081 |           5.1681 |           4.4529 |
[32m[20230113 19:59:26 @agent_ppo2.py:186][0m |          -0.0135 |           5.0237 |           4.4532 |
[32m[20230113 19:59:26 @agent_ppo2.py:186][0m |          -0.0084 |           4.9171 |           4.4520 |
[32m[20230113 19:59:27 @agent_ppo2.py:186][0m |          -0.0176 |           4.8623 |           4.4529 |
[32m[20230113 19:59:27 @agent_ppo2.py:186][0m |          -0.0183 |           4.7927 |           4.4519 |
[32m[20230113 19:59:27 @agent_ppo2.py:186][0m |          -0.0190 |           4.7199 |           4.4498 |
[32m[20230113 19:59:27 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.15
[32m[20230113 19:59:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.32
[32m[20230113 19:59:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.41
[32m[20230113 19:59:27 @agent_ppo2.py:144][0m Total time:      14.91 min
[32m[20230113 19:59:27 @agent_ppo2.py:146][0m 1339392 total steps have happened
[32m[20230113 19:59:27 @agent_ppo2.py:122][0m #------------------------ Iteration 654 --------------------------#
[32m[20230113 19:59:27 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:28 @agent_ppo2.py:186][0m |          -0.0000 |           6.3131 |           4.6217 |
[32m[20230113 19:59:28 @agent_ppo2.py:186][0m |          -0.0044 |           4.6682 |           4.6162 |
[32m[20230113 19:59:28 @agent_ppo2.py:186][0m |          -0.0075 |           4.1134 |           4.6131 |
[32m[20230113 19:59:28 @agent_ppo2.py:186][0m |          -0.0079 |           3.7700 |           4.6144 |
[32m[20230113 19:59:28 @agent_ppo2.py:186][0m |          -0.0100 |           3.5739 |           4.6152 |
[32m[20230113 19:59:28 @agent_ppo2.py:186][0m |          -0.0105 |           3.4667 |           4.6119 |
[32m[20230113 19:59:28 @agent_ppo2.py:186][0m |          -0.0119 |           3.3306 |           4.6105 |
[32m[20230113 19:59:28 @agent_ppo2.py:186][0m |          -0.0126 |           3.2370 |           4.6121 |
[32m[20230113 19:59:28 @agent_ppo2.py:186][0m |          -0.0134 |           3.1481 |           4.6130 |
[32m[20230113 19:59:28 @agent_ppo2.py:186][0m |          -0.0142 |           3.0741 |           4.6124 |
[32m[20230113 19:59:28 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.84
[32m[20230113 19:59:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.93
[32m[20230113 19:59:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.87
[32m[20230113 19:59:28 @agent_ppo2.py:144][0m Total time:      14.93 min
[32m[20230113 19:59:28 @agent_ppo2.py:146][0m 1341440 total steps have happened
[32m[20230113 19:59:28 @agent_ppo2.py:122][0m #------------------------ Iteration 655 --------------------------#
[32m[20230113 19:59:29 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:29 @agent_ppo2.py:186][0m |           0.0007 |           4.7241 |           4.5361 |
[32m[20230113 19:59:29 @agent_ppo2.py:186][0m |          -0.0058 |           4.1410 |           4.5312 |
[32m[20230113 19:59:29 @agent_ppo2.py:186][0m |          -0.0072 |           3.9398 |           4.5348 |
[32m[20230113 19:59:29 @agent_ppo2.py:186][0m |          -0.0084 |           3.7694 |           4.5269 |
[32m[20230113 19:59:29 @agent_ppo2.py:186][0m |          -0.0096 |           3.6718 |           4.5321 |
[32m[20230113 19:59:29 @agent_ppo2.py:186][0m |          -0.0110 |           3.6046 |           4.5264 |
[32m[20230113 19:59:29 @agent_ppo2.py:186][0m |          -0.0131 |           3.4387 |           4.5261 |
[32m[20230113 19:59:29 @agent_ppo2.py:186][0m |          -0.0131 |           3.3946 |           4.5261 |
[32m[20230113 19:59:29 @agent_ppo2.py:186][0m |          -0.0132 |           3.2978 |           4.5215 |
[32m[20230113 19:59:29 @agent_ppo2.py:186][0m |          -0.0142 |           3.2307 |           4.5223 |
[32m[20230113 19:59:29 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.66
[32m[20230113 19:59:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.97
[32m[20230113 19:59:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.60
[32m[20230113 19:59:30 @agent_ppo2.py:144][0m Total time:      14.95 min
[32m[20230113 19:59:30 @agent_ppo2.py:146][0m 1343488 total steps have happened
[32m[20230113 19:59:30 @agent_ppo2.py:122][0m #------------------------ Iteration 656 --------------------------#
[32m[20230113 19:59:30 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:59:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:30 @agent_ppo2.py:186][0m |           0.0014 |           6.5131 |           4.5697 |
[32m[20230113 19:59:30 @agent_ppo2.py:186][0m |          -0.0092 |           5.0034 |           4.5657 |
[32m[20230113 19:59:30 @agent_ppo2.py:186][0m |          -0.0106 |           4.3109 |           4.5602 |
[32m[20230113 19:59:30 @agent_ppo2.py:186][0m |          -0.0138 |           3.7202 |           4.5543 |
[32m[20230113 19:59:30 @agent_ppo2.py:186][0m |          -0.0131 |           3.4064 |           4.5536 |
[32m[20230113 19:59:30 @agent_ppo2.py:186][0m |          -0.0143 |           3.2092 |           4.5512 |
[32m[20230113 19:59:30 @agent_ppo2.py:186][0m |          -0.0146 |           3.0214 |           4.5515 |
[32m[20230113 19:59:30 @agent_ppo2.py:186][0m |          -0.0146 |           2.9098 |           4.5511 |
[32m[20230113 19:59:31 @agent_ppo2.py:186][0m |          -0.0157 |           2.8395 |           4.5485 |
[32m[20230113 19:59:31 @agent_ppo2.py:186][0m |          -0.0184 |           2.7644 |           4.5471 |
[32m[20230113 19:59:31 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.03
[32m[20230113 19:59:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.35
[32m[20230113 19:59:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.25
[32m[20230113 19:59:31 @agent_ppo2.py:144][0m Total time:      14.97 min
[32m[20230113 19:59:31 @agent_ppo2.py:146][0m 1345536 total steps have happened
[32m[20230113 19:59:31 @agent_ppo2.py:122][0m #------------------------ Iteration 657 --------------------------#
[32m[20230113 19:59:31 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:32 @agent_ppo2.py:186][0m |          -0.0000 |           6.3270 |           4.6665 |
[32m[20230113 19:59:32 @agent_ppo2.py:186][0m |          -0.0041 |           5.7078 |           4.6629 |
[32m[20230113 19:59:32 @agent_ppo2.py:186][0m |          -0.0057 |           5.3881 |           4.6649 |
[32m[20230113 19:59:32 @agent_ppo2.py:186][0m |          -0.0065 |           5.2439 |           4.6643 |
[32m[20230113 19:59:32 @agent_ppo2.py:186][0m |          -0.0078 |           5.0993 |           4.6646 |
[32m[20230113 19:59:32 @agent_ppo2.py:186][0m |          -0.0087 |           4.9700 |           4.6651 |
[32m[20230113 19:59:32 @agent_ppo2.py:186][0m |          -0.0088 |           4.8951 |           4.6664 |
[32m[20230113 19:59:32 @agent_ppo2.py:186][0m |          -0.0101 |           4.8255 |           4.6654 |
[32m[20230113 19:59:32 @agent_ppo2.py:186][0m |          -0.0102 |           4.7369 |           4.6670 |
[32m[20230113 19:59:32 @agent_ppo2.py:186][0m |          -0.0109 |           4.6744 |           4.6683 |
[32m[20230113 19:59:32 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:59:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.40
[32m[20230113 19:59:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.43
[32m[20230113 19:59:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.37
[32m[20230113 19:59:32 @agent_ppo2.py:144][0m Total time:      15.00 min
[32m[20230113 19:59:32 @agent_ppo2.py:146][0m 1347584 total steps have happened
[32m[20230113 19:59:32 @agent_ppo2.py:122][0m #------------------------ Iteration 658 --------------------------#
[32m[20230113 19:59:33 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:33 @agent_ppo2.py:186][0m |           0.0014 |           6.7137 |           4.6465 |
[32m[20230113 19:59:33 @agent_ppo2.py:186][0m |          -0.0027 |           5.9630 |           4.6397 |
[32m[20230113 19:59:33 @agent_ppo2.py:186][0m |          -0.0048 |           5.6066 |           4.6382 |
[32m[20230113 19:59:33 @agent_ppo2.py:186][0m |          -0.0059 |           5.3498 |           4.6371 |
[32m[20230113 19:59:33 @agent_ppo2.py:186][0m |          -0.0073 |           5.1859 |           4.6345 |
[32m[20230113 19:59:33 @agent_ppo2.py:186][0m |          -0.0079 |           5.0935 |           4.6356 |
[32m[20230113 19:59:33 @agent_ppo2.py:186][0m |          -0.0095 |           4.9984 |           4.6370 |
[32m[20230113 19:59:33 @agent_ppo2.py:186][0m |          -0.0091 |           4.8763 |           4.6373 |
[32m[20230113 19:59:33 @agent_ppo2.py:186][0m |          -0.0096 |           4.7869 |           4.6383 |
[32m[20230113 19:59:33 @agent_ppo2.py:186][0m |          -0.0108 |           4.7426 |           4.6372 |
[32m[20230113 19:59:33 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.74
[32m[20230113 19:59:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.85
[32m[20230113 19:59:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.05
[32m[20230113 19:59:34 @agent_ppo2.py:144][0m Total time:      15.02 min
[32m[20230113 19:59:34 @agent_ppo2.py:146][0m 1349632 total steps have happened
[32m[20230113 19:59:34 @agent_ppo2.py:122][0m #------------------------ Iteration 659 --------------------------#
[32m[20230113 19:59:34 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:34 @agent_ppo2.py:186][0m |          -0.0005 |           5.6151 |           4.5055 |
[32m[20230113 19:59:34 @agent_ppo2.py:186][0m |          -0.0047 |           5.1839 |           4.5025 |
[32m[20230113 19:59:34 @agent_ppo2.py:186][0m |          -0.0091 |           4.9589 |           4.4958 |
[32m[20230113 19:59:34 @agent_ppo2.py:186][0m |          -0.0084 |           4.8110 |           4.4934 |
[32m[20230113 19:59:34 @agent_ppo2.py:186][0m |          -0.0102 |           4.6610 |           4.4932 |
[32m[20230113 19:59:34 @agent_ppo2.py:186][0m |          -0.0091 |           4.5086 |           4.4893 |
[32m[20230113 19:59:34 @agent_ppo2.py:186][0m |          -0.0086 |           4.4595 |           4.4894 |
[32m[20230113 19:59:34 @agent_ppo2.py:186][0m |          -0.0126 |           4.3390 |           4.4887 |
[32m[20230113 19:59:35 @agent_ppo2.py:186][0m |          -0.0155 |           4.2771 |           4.4862 |
[32m[20230113 19:59:35 @agent_ppo2.py:186][0m |          -0.0161 |           4.2191 |           4.4892 |
[32m[20230113 19:59:35 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.16
[32m[20230113 19:59:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.13
[32m[20230113 19:59:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.88
[32m[20230113 19:59:35 @agent_ppo2.py:144][0m Total time:      15.04 min
[32m[20230113 19:59:35 @agent_ppo2.py:146][0m 1351680 total steps have happened
[32m[20230113 19:59:35 @agent_ppo2.py:122][0m #------------------------ Iteration 660 --------------------------#
[32m[20230113 19:59:35 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:36 @agent_ppo2.py:186][0m |          -0.0031 |           5.5742 |           4.5982 |
[32m[20230113 19:59:36 @agent_ppo2.py:186][0m |          -0.0095 |           5.0211 |           4.5952 |
[32m[20230113 19:59:36 @agent_ppo2.py:186][0m |          -0.0110 |           4.7761 |           4.5984 |
[32m[20230113 19:59:36 @agent_ppo2.py:186][0m |          -0.0079 |           4.5865 |           4.5928 |
[32m[20230113 19:59:36 @agent_ppo2.py:186][0m |          -0.0040 |           4.6455 |           4.5922 |
[32m[20230113 19:59:36 @agent_ppo2.py:186][0m |          -0.0071 |           4.4238 |           4.5956 |
[32m[20230113 19:59:36 @agent_ppo2.py:186][0m |          -0.0126 |           4.2962 |           4.5974 |
[32m[20230113 19:59:36 @agent_ppo2.py:186][0m |          -0.0095 |           4.1715 |           4.5988 |
[32m[20230113 19:59:36 @agent_ppo2.py:186][0m |          -0.0113 |           4.0706 |           4.5950 |
[32m[20230113 19:59:36 @agent_ppo2.py:186][0m |          -0.0154 |           3.9456 |           4.5991 |
[32m[20230113 19:59:36 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.13
[32m[20230113 19:59:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.55
[32m[20230113 19:59:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.86
[32m[20230113 19:59:36 @agent_ppo2.py:144][0m Total time:      15.06 min
[32m[20230113 19:59:36 @agent_ppo2.py:146][0m 1353728 total steps have happened
[32m[20230113 19:59:36 @agent_ppo2.py:122][0m #------------------------ Iteration 661 --------------------------#
[32m[20230113 19:59:37 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:37 @agent_ppo2.py:186][0m |          -0.0015 |           6.1948 |           4.6605 |
[32m[20230113 19:59:37 @agent_ppo2.py:186][0m |          -0.0047 |           5.4343 |           4.6501 |
[32m[20230113 19:59:37 @agent_ppo2.py:186][0m |          -0.0070 |           5.1394 |           4.6493 |
[32m[20230113 19:59:37 @agent_ppo2.py:186][0m |          -0.0093 |           4.9811 |           4.6480 |
[32m[20230113 19:59:37 @agent_ppo2.py:186][0m |          -0.0104 |           4.8474 |           4.6466 |
[32m[20230113 19:59:37 @agent_ppo2.py:186][0m |          -0.0116 |           4.7826 |           4.6479 |
[32m[20230113 19:59:37 @agent_ppo2.py:186][0m |          -0.0125 |           4.6555 |           4.6442 |
[32m[20230113 19:59:37 @agent_ppo2.py:186][0m |          -0.0120 |           4.5963 |           4.6461 |
[32m[20230113 19:59:37 @agent_ppo2.py:186][0m |          -0.0126 |           4.5198 |           4.6464 |
[32m[20230113 19:59:37 @agent_ppo2.py:186][0m |          -0.0122 |           4.5182 |           4.6461 |
[32m[20230113 19:59:37 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.60
[32m[20230113 19:59:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.06
[32m[20230113 19:59:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.58
[32m[20230113 19:59:38 @agent_ppo2.py:144][0m Total time:      15.08 min
[32m[20230113 19:59:38 @agent_ppo2.py:146][0m 1355776 total steps have happened
[32m[20230113 19:59:38 @agent_ppo2.py:122][0m #------------------------ Iteration 662 --------------------------#
[32m[20230113 19:59:38 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:38 @agent_ppo2.py:186][0m |           0.0005 |           6.6065 |           4.6745 |
[32m[20230113 19:59:38 @agent_ppo2.py:186][0m |          -0.0043 |           5.8041 |           4.6701 |
[32m[20230113 19:59:38 @agent_ppo2.py:186][0m |          -0.0028 |           5.5388 |           4.6666 |
[32m[20230113 19:59:38 @agent_ppo2.py:186][0m |          -0.0080 |           5.3188 |           4.6667 |
[32m[20230113 19:59:38 @agent_ppo2.py:186][0m |          -0.0069 |           5.1540 |           4.6656 |
[32m[20230113 19:59:38 @agent_ppo2.py:186][0m |          -0.0106 |           5.0495 |           4.6643 |
[32m[20230113 19:59:38 @agent_ppo2.py:186][0m |          -0.0062 |           4.9489 |           4.6634 |
[32m[20230113 19:59:38 @agent_ppo2.py:186][0m |          -0.0106 |           4.8304 |           4.6630 |
[32m[20230113 19:59:39 @agent_ppo2.py:186][0m |          -0.0106 |           4.7550 |           4.6616 |
[32m[20230113 19:59:39 @agent_ppo2.py:186][0m |          -0.0137 |           4.6759 |           4.6603 |
[32m[20230113 19:59:39 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.56
[32m[20230113 19:59:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.94
[32m[20230113 19:59:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.42
[32m[20230113 19:59:39 @agent_ppo2.py:144][0m Total time:      15.11 min
[32m[20230113 19:59:39 @agent_ppo2.py:146][0m 1357824 total steps have happened
[32m[20230113 19:59:39 @agent_ppo2.py:122][0m #------------------------ Iteration 663 --------------------------#
[32m[20230113 19:59:39 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:40 @agent_ppo2.py:186][0m |          -0.0009 |           5.8140 |           4.6534 |
[32m[20230113 19:59:40 @agent_ppo2.py:186][0m |          -0.0068 |           5.2620 |           4.6486 |
[32m[20230113 19:59:40 @agent_ppo2.py:186][0m |          -0.0074 |           5.0395 |           4.6432 |
[32m[20230113 19:59:40 @agent_ppo2.py:186][0m |          -0.0096 |           4.8103 |           4.6426 |
[32m[20230113 19:59:40 @agent_ppo2.py:186][0m |          -0.0107 |           4.6887 |           4.6432 |
[32m[20230113 19:59:40 @agent_ppo2.py:186][0m |          -0.0106 |           4.5069 |           4.6387 |
[32m[20230113 19:59:40 @agent_ppo2.py:186][0m |          -0.0112 |           4.4046 |           4.6390 |
[32m[20230113 19:59:40 @agent_ppo2.py:186][0m |          -0.0112 |           4.2838 |           4.6397 |
[32m[20230113 19:59:40 @agent_ppo2.py:186][0m |          -0.0122 |           4.1912 |           4.6375 |
[32m[20230113 19:59:40 @agent_ppo2.py:186][0m |          -0.0128 |           4.0946 |           4.6372 |
[32m[20230113 19:59:40 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.60
[32m[20230113 19:59:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.04
[32m[20230113 19:59:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.42
[32m[20230113 19:59:40 @agent_ppo2.py:144][0m Total time:      15.13 min
[32m[20230113 19:59:40 @agent_ppo2.py:146][0m 1359872 total steps have happened
[32m[20230113 19:59:40 @agent_ppo2.py:122][0m #------------------------ Iteration 664 --------------------------#
[32m[20230113 19:59:41 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:59:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:41 @agent_ppo2.py:186][0m |          -0.0044 |           4.5665 |           4.4395 |
[32m[20230113 19:59:41 @agent_ppo2.py:186][0m |          -0.0092 |           3.8976 |           4.4307 |
[32m[20230113 19:59:41 @agent_ppo2.py:186][0m |          -0.0012 |           3.5839 |           4.4244 |
[32m[20230113 19:59:41 @agent_ppo2.py:186][0m |          -0.0033 |           3.4884 |           4.4232 |
[32m[20230113 19:59:41 @agent_ppo2.py:186][0m |          -0.0157 |           3.2804 |           4.4221 |
[32m[20230113 19:59:41 @agent_ppo2.py:186][0m |          -0.0055 |           3.1603 |           4.4262 |
[32m[20230113 19:59:41 @agent_ppo2.py:186][0m |          -0.0134 |           3.0673 |           4.4191 |
[32m[20230113 19:59:41 @agent_ppo2.py:186][0m |          -0.0172 |           2.9812 |           4.4242 |
[32m[20230113 19:59:41 @agent_ppo2.py:186][0m |          -0.0221 |           2.9526 |           4.4267 |
[32m[20230113 19:59:41 @agent_ppo2.py:186][0m |          -0.0227 |           2.8725 |           4.4240 |
[32m[20230113 19:59:41 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:59:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.37
[32m[20230113 19:59:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.52
[32m[20230113 19:59:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.63
[32m[20230113 19:59:42 @agent_ppo2.py:144][0m Total time:      15.15 min
[32m[20230113 19:59:42 @agent_ppo2.py:146][0m 1361920 total steps have happened
[32m[20230113 19:59:42 @agent_ppo2.py:122][0m #------------------------ Iteration 665 --------------------------#
[32m[20230113 19:59:42 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:42 @agent_ppo2.py:186][0m |          -0.0019 |           5.3954 |           4.5354 |
[32m[20230113 19:59:42 @agent_ppo2.py:186][0m |          -0.0030 |           4.6169 |           4.5309 |
[32m[20230113 19:59:42 @agent_ppo2.py:186][0m |          -0.0055 |           4.2876 |           4.5229 |
[32m[20230113 19:59:42 @agent_ppo2.py:186][0m |          -0.0089 |           4.0551 |           4.5234 |
[32m[20230113 19:59:42 @agent_ppo2.py:186][0m |          -0.0083 |           3.8793 |           4.5230 |
[32m[20230113 19:59:42 @agent_ppo2.py:186][0m |          -0.0101 |           3.7235 |           4.5207 |
[32m[20230113 19:59:42 @agent_ppo2.py:186][0m |          -0.0066 |           3.6461 |           4.5180 |
[32m[20230113 19:59:42 @agent_ppo2.py:186][0m |          -0.0101 |           3.5117 |           4.5195 |
[32m[20230113 19:59:43 @agent_ppo2.py:186][0m |          -0.0097 |           3.4072 |           4.5151 |
[32m[20230113 19:59:43 @agent_ppo2.py:186][0m |          -0.0108 |           3.3049 |           4.5178 |
[32m[20230113 19:59:43 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.67
[32m[20230113 19:59:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.05
[32m[20230113 19:59:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.89
[32m[20230113 19:59:43 @agent_ppo2.py:144][0m Total time:      15.17 min
[32m[20230113 19:59:43 @agent_ppo2.py:146][0m 1363968 total steps have happened
[32m[20230113 19:59:43 @agent_ppo2.py:122][0m #------------------------ Iteration 666 --------------------------#
[32m[20230113 19:59:43 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:44 @agent_ppo2.py:186][0m |           0.0000 |          12.6854 |           4.7375 |
[32m[20230113 19:59:44 @agent_ppo2.py:186][0m |          -0.0034 |           7.1607 |           4.7327 |
[32m[20230113 19:59:44 @agent_ppo2.py:186][0m |          -0.0032 |           6.6336 |           4.7339 |
[32m[20230113 19:59:44 @agent_ppo2.py:186][0m |          -0.0062 |           6.3565 |           4.7352 |
[32m[20230113 19:59:44 @agent_ppo2.py:186][0m |          -0.0070 |           6.0766 |           4.7331 |
[32m[20230113 19:59:44 @agent_ppo2.py:186][0m |          -0.0086 |           5.8404 |           4.7368 |
[32m[20230113 19:59:44 @agent_ppo2.py:186][0m |          -0.0069 |           5.6920 |           4.7368 |
[32m[20230113 19:59:44 @agent_ppo2.py:186][0m |          -0.0084 |           5.5890 |           4.7362 |
[32m[20230113 19:59:44 @agent_ppo2.py:186][0m |          -0.0090 |           5.4202 |           4.7379 |
[32m[20230113 19:59:44 @agent_ppo2.py:186][0m |          -0.0070 |           5.3133 |           4.7364 |
[32m[20230113 19:59:44 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 109.82
[32m[20230113 19:59:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.61
[32m[20230113 19:59:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.41
[32m[20230113 19:59:44 @agent_ppo2.py:144][0m Total time:      15.20 min
[32m[20230113 19:59:44 @agent_ppo2.py:146][0m 1366016 total steps have happened
[32m[20230113 19:59:44 @agent_ppo2.py:122][0m #------------------------ Iteration 667 --------------------------#
[32m[20230113 19:59:45 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:59:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:45 @agent_ppo2.py:186][0m |           0.0037 |           6.6720 |           4.6215 |
[32m[20230113 19:59:45 @agent_ppo2.py:186][0m |          -0.0092 |           5.7184 |           4.6205 |
[32m[20230113 19:59:45 @agent_ppo2.py:186][0m |          -0.0092 |           5.2304 |           4.6185 |
[32m[20230113 19:59:45 @agent_ppo2.py:186][0m |          -0.0085 |           4.9206 |           4.6144 |
[32m[20230113 19:59:45 @agent_ppo2.py:186][0m |          -0.0100 |           4.6901 |           4.6155 |
[32m[20230113 19:59:45 @agent_ppo2.py:186][0m |          -0.0154 |           4.5339 |           4.6138 |
[32m[20230113 19:59:45 @agent_ppo2.py:186][0m |          -0.0215 |           4.3518 |           4.6117 |
[32m[20230113 19:59:45 @agent_ppo2.py:186][0m |          -0.0199 |           4.2315 |           4.6121 |
[32m[20230113 19:59:45 @agent_ppo2.py:186][0m |          -0.0181 |           4.1211 |           4.6074 |
[32m[20230113 19:59:45 @agent_ppo2.py:186][0m |          -0.0104 |           4.0159 |           4.6086 |
[32m[20230113 19:59:45 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:59:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.17
[32m[20230113 19:59:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.52
[32m[20230113 19:59:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.67
[32m[20230113 19:59:46 @agent_ppo2.py:144][0m Total time:      15.22 min
[32m[20230113 19:59:46 @agent_ppo2.py:146][0m 1368064 total steps have happened
[32m[20230113 19:59:46 @agent_ppo2.py:122][0m #------------------------ Iteration 668 --------------------------#
[32m[20230113 19:59:46 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:46 @agent_ppo2.py:186][0m |          -0.0003 |           6.7579 |           4.5492 |
[32m[20230113 19:59:46 @agent_ppo2.py:186][0m |          -0.0040 |           5.8837 |           4.5468 |
[32m[20230113 19:59:46 @agent_ppo2.py:186][0m |          -0.0077 |           5.3614 |           4.5394 |
[32m[20230113 19:59:46 @agent_ppo2.py:186][0m |          -0.0048 |           5.2648 |           4.5416 |
[32m[20230113 19:59:46 @agent_ppo2.py:186][0m |          -0.0090 |           4.8956 |           4.5398 |
[32m[20230113 19:59:46 @agent_ppo2.py:186][0m |          -0.0090 |           4.8362 |           4.5385 |
[32m[20230113 19:59:46 @agent_ppo2.py:186][0m |          -0.0095 |           4.5812 |           4.5433 |
[32m[20230113 19:59:46 @agent_ppo2.py:186][0m |          -0.0099 |           4.5259 |           4.5413 |
[32m[20230113 19:59:46 @agent_ppo2.py:186][0m |          -0.0123 |           4.3684 |           4.5425 |
[32m[20230113 19:59:47 @agent_ppo2.py:186][0m |          -0.0107 |           4.2461 |           4.5415 |
[32m[20230113 19:59:47 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.68
[32m[20230113 19:59:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.88
[32m[20230113 19:59:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.66
[32m[20230113 19:59:47 @agent_ppo2.py:144][0m Total time:      15.24 min
[32m[20230113 19:59:47 @agent_ppo2.py:146][0m 1370112 total steps have happened
[32m[20230113 19:59:47 @agent_ppo2.py:122][0m #------------------------ Iteration 669 --------------------------#
[32m[20230113 19:59:47 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 19:59:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:47 @agent_ppo2.py:186][0m |          -0.0002 |           6.5363 |           4.6361 |
[32m[20230113 19:59:48 @agent_ppo2.py:186][0m |          -0.0003 |           6.0074 |           4.6381 |
[32m[20230113 19:59:48 @agent_ppo2.py:186][0m |          -0.0030 |           5.5355 |           4.6385 |
[32m[20230113 19:59:48 @agent_ppo2.py:186][0m |          -0.0061 |           5.1128 |           4.6395 |
[32m[20230113 19:59:48 @agent_ppo2.py:186][0m |          -0.0048 |           4.7933 |           4.6422 |
[32m[20230113 19:59:48 @agent_ppo2.py:186][0m |          -0.0076 |           4.5714 |           4.6463 |
[32m[20230113 19:59:48 @agent_ppo2.py:186][0m |          -0.0080 |           4.3025 |           4.6436 |
[32m[20230113 19:59:48 @agent_ppo2.py:186][0m |          -0.0091 |           4.1423 |           4.6451 |
[32m[20230113 19:59:48 @agent_ppo2.py:186][0m |          -0.0091 |           3.9862 |           4.6436 |
[32m[20230113 19:59:48 @agent_ppo2.py:186][0m |          -0.0101 |           3.8422 |           4.6436 |
[32m[20230113 19:59:48 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 19:59:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.89
[32m[20230113 19:59:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.66
[32m[20230113 19:59:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.84
[32m[20230113 19:59:48 @agent_ppo2.py:144][0m Total time:      15.26 min
[32m[20230113 19:59:48 @agent_ppo2.py:146][0m 1372160 total steps have happened
[32m[20230113 19:59:48 @agent_ppo2.py:122][0m #------------------------ Iteration 670 --------------------------#
[32m[20230113 19:59:49 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:59:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:49 @agent_ppo2.py:186][0m |          -0.0004 |           7.7530 |           4.5636 |
[32m[20230113 19:59:49 @agent_ppo2.py:186][0m |          -0.0012 |           6.9368 |           4.5597 |
[32m[20230113 19:59:49 @agent_ppo2.py:186][0m |          -0.0049 |           6.4362 |           4.5590 |
[32m[20230113 19:59:49 @agent_ppo2.py:186][0m |          -0.0064 |           6.2580 |           4.5542 |
[32m[20230113 19:59:49 @agent_ppo2.py:186][0m |          -0.0076 |           6.0266 |           4.5511 |
[32m[20230113 19:59:49 @agent_ppo2.py:186][0m |          -0.0095 |           5.9386 |           4.5494 |
[32m[20230113 19:59:49 @agent_ppo2.py:186][0m |          -0.0102 |           5.8194 |           4.5518 |
[32m[20230113 19:59:49 @agent_ppo2.py:186][0m |          -0.0104 |           5.6747 |           4.5501 |
[32m[20230113 19:59:49 @agent_ppo2.py:186][0m |          -0.0109 |           5.6072 |           4.5503 |
[32m[20230113 19:59:49 @agent_ppo2.py:186][0m |          -0.0121 |           5.5255 |           4.5495 |
[32m[20230113 19:59:49 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:59:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 210.49
[32m[20230113 19:59:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.31
[32m[20230113 19:59:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.25
[32m[20230113 19:59:50 @agent_ppo2.py:144][0m Total time:      15.28 min
[32m[20230113 19:59:50 @agent_ppo2.py:146][0m 1374208 total steps have happened
[32m[20230113 19:59:50 @agent_ppo2.py:122][0m #------------------------ Iteration 671 --------------------------#
[32m[20230113 19:59:50 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:50 @agent_ppo2.py:186][0m |          -0.0021 |           6.1013 |           4.5539 |
[32m[20230113 19:59:50 @agent_ppo2.py:186][0m |          -0.0044 |           5.7240 |           4.5378 |
[32m[20230113 19:59:50 @agent_ppo2.py:186][0m |          -0.0077 |           5.3609 |           4.5396 |
[32m[20230113 19:59:50 @agent_ppo2.py:186][0m |          -0.0091 |           5.1714 |           4.5366 |
[32m[20230113 19:59:50 @agent_ppo2.py:186][0m |          -0.0089 |           5.0322 |           4.5374 |
[32m[20230113 19:59:50 @agent_ppo2.py:186][0m |          -0.0104 |           4.9006 |           4.5396 |
[32m[20230113 19:59:50 @agent_ppo2.py:186][0m |          -0.0112 |           4.7506 |           4.5354 |
[32m[20230113 19:59:50 @agent_ppo2.py:186][0m |          -0.0116 |           4.6361 |           4.5380 |
[32m[20230113 19:59:51 @agent_ppo2.py:186][0m |          -0.0115 |           4.6052 |           4.5373 |
[32m[20230113 19:59:51 @agent_ppo2.py:186][0m |          -0.0145 |           4.4630 |           4.5320 |
[32m[20230113 19:59:51 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.86
[32m[20230113 19:59:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.18
[32m[20230113 19:59:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.44
[32m[20230113 19:59:51 @agent_ppo2.py:144][0m Total time:      15.31 min
[32m[20230113 19:59:51 @agent_ppo2.py:146][0m 1376256 total steps have happened
[32m[20230113 19:59:51 @agent_ppo2.py:122][0m #------------------------ Iteration 672 --------------------------#
[32m[20230113 19:59:51 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:51 @agent_ppo2.py:186][0m |          -0.0001 |           6.3683 |           4.5509 |
[32m[20230113 19:59:52 @agent_ppo2.py:186][0m |          -0.0060 |           5.3012 |           4.5421 |
[32m[20230113 19:59:52 @agent_ppo2.py:186][0m |          -0.0068 |           4.8116 |           4.5365 |
[32m[20230113 19:59:52 @agent_ppo2.py:186][0m |          -0.0086 |           4.5170 |           4.5376 |
[32m[20230113 19:59:52 @agent_ppo2.py:186][0m |          -0.0080 |           4.2810 |           4.5412 |
[32m[20230113 19:59:52 @agent_ppo2.py:186][0m |          -0.0104 |           4.1158 |           4.5375 |
[32m[20230113 19:59:52 @agent_ppo2.py:186][0m |          -0.0111 |           3.9911 |           4.5353 |
[32m[20230113 19:59:52 @agent_ppo2.py:186][0m |          -0.0106 |           3.9024 |           4.5372 |
[32m[20230113 19:59:52 @agent_ppo2.py:186][0m |          -0.0112 |           3.8024 |           4.5329 |
[32m[20230113 19:59:52 @agent_ppo2.py:186][0m |          -0.0122 |           3.7455 |           4.5330 |
[32m[20230113 19:59:52 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.73
[32m[20230113 19:59:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.35
[32m[20230113 19:59:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.11
[32m[20230113 19:59:52 @agent_ppo2.py:144][0m Total time:      15.33 min
[32m[20230113 19:59:52 @agent_ppo2.py:146][0m 1378304 total steps have happened
[32m[20230113 19:59:52 @agent_ppo2.py:122][0m #------------------------ Iteration 673 --------------------------#
[32m[20230113 19:59:53 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:53 @agent_ppo2.py:186][0m |           0.0010 |           5.5245 |           4.6675 |
[32m[20230113 19:59:53 @agent_ppo2.py:186][0m |          -0.0076 |           5.1457 |           4.6613 |
[32m[20230113 19:59:53 @agent_ppo2.py:186][0m |          -0.0084 |           5.0331 |           4.6605 |
[32m[20230113 19:59:53 @agent_ppo2.py:186][0m |          -0.0122 |           4.7648 |           4.6549 |
[32m[20230113 19:59:53 @agent_ppo2.py:186][0m |          -0.0106 |           4.6516 |           4.6600 |
[32m[20230113 19:59:53 @agent_ppo2.py:186][0m |          -0.0121 |           4.5557 |           4.6564 |
[32m[20230113 19:59:53 @agent_ppo2.py:186][0m |          -0.0135 |           4.4534 |           4.6510 |
[32m[20230113 19:59:53 @agent_ppo2.py:186][0m |          -0.0131 |           4.3943 |           4.6560 |
[32m[20230113 19:59:53 @agent_ppo2.py:186][0m |          -0.0130 |           4.3267 |           4.6571 |
[32m[20230113 19:59:53 @agent_ppo2.py:186][0m |          -0.0109 |           4.2898 |           4.6594 |
[32m[20230113 19:59:53 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.32
[32m[20230113 19:59:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.18
[32m[20230113 19:59:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.81
[32m[20230113 19:59:54 @agent_ppo2.py:144][0m Total time:      15.35 min
[32m[20230113 19:59:54 @agent_ppo2.py:146][0m 1380352 total steps have happened
[32m[20230113 19:59:54 @agent_ppo2.py:122][0m #------------------------ Iteration 674 --------------------------#
[32m[20230113 19:59:54 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 19:59:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:54 @agent_ppo2.py:186][0m |          -0.0022 |           5.4577 |           4.5899 |
[32m[20230113 19:59:54 @agent_ppo2.py:186][0m |          -0.0078 |           5.0069 |           4.5763 |
[32m[20230113 19:59:54 @agent_ppo2.py:186][0m |          -0.0104 |           4.7134 |           4.5763 |
[32m[20230113 19:59:54 @agent_ppo2.py:186][0m |          -0.0106 |           4.5383 |           4.5751 |
[32m[20230113 19:59:54 @agent_ppo2.py:186][0m |          -0.0118 |           4.4220 |           4.5779 |
[32m[20230113 19:59:54 @agent_ppo2.py:186][0m |          -0.0110 |           4.3639 |           4.5773 |
[32m[20230113 19:59:54 @agent_ppo2.py:186][0m |          -0.0132 |           4.1995 |           4.5738 |
[32m[20230113 19:59:54 @agent_ppo2.py:186][0m |          -0.0130 |           4.1424 |           4.5779 |
[32m[20230113 19:59:54 @agent_ppo2.py:186][0m |          -0.0139 |           4.0452 |           4.5787 |
[32m[20230113 19:59:55 @agent_ppo2.py:186][0m |          -0.0143 |           3.9972 |           4.5746 |
[32m[20230113 19:59:55 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 19:59:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.50
[32m[20230113 19:59:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.87
[32m[20230113 19:59:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.34
[32m[20230113 19:59:55 @agent_ppo2.py:144][0m Total time:      15.37 min
[32m[20230113 19:59:55 @agent_ppo2.py:146][0m 1382400 total steps have happened
[32m[20230113 19:59:55 @agent_ppo2.py:122][0m #------------------------ Iteration 675 --------------------------#
[32m[20230113 19:59:55 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:56 @agent_ppo2.py:186][0m |           0.0005 |           3.7525 |           4.5629 |
[32m[20230113 19:59:56 @agent_ppo2.py:186][0m |          -0.0042 |           3.2238 |           4.5579 |
[32m[20230113 19:59:56 @agent_ppo2.py:186][0m |          -0.0078 |           3.0550 |           4.5499 |
[32m[20230113 19:59:56 @agent_ppo2.py:186][0m |          -0.0083 |           2.8959 |           4.5455 |
[32m[20230113 19:59:56 @agent_ppo2.py:186][0m |          -0.0079 |           2.8134 |           4.5449 |
[32m[20230113 19:59:56 @agent_ppo2.py:186][0m |          -0.0102 |           2.7984 |           4.5405 |
[32m[20230113 19:59:56 @agent_ppo2.py:186][0m |          -0.0102 |           2.6921 |           4.5368 |
[32m[20230113 19:59:56 @agent_ppo2.py:186][0m |          -0.0113 |           2.6347 |           4.5357 |
[32m[20230113 19:59:56 @agent_ppo2.py:186][0m |          -0.0096 |           2.6136 |           4.5345 |
[32m[20230113 19:59:56 @agent_ppo2.py:186][0m |          -0.0120 |           2.5576 |           4.5298 |
[32m[20230113 19:59:56 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.21
[32m[20230113 19:59:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.46
[32m[20230113 19:59:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.52
[32m[20230113 19:59:56 @agent_ppo2.py:144][0m Total time:      15.40 min
[32m[20230113 19:59:56 @agent_ppo2.py:146][0m 1384448 total steps have happened
[32m[20230113 19:59:56 @agent_ppo2.py:122][0m #------------------------ Iteration 676 --------------------------#
[32m[20230113 19:59:57 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 19:59:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:57 @agent_ppo2.py:186][0m |          -0.0014 |           5.5600 |           4.5679 |
[32m[20230113 19:59:57 @agent_ppo2.py:186][0m |          -0.0080 |           4.9931 |           4.5612 |
[32m[20230113 19:59:57 @agent_ppo2.py:186][0m |          -0.0047 |           4.7562 |           4.5614 |
[32m[20230113 19:59:57 @agent_ppo2.py:186][0m |          -0.0032 |           4.6132 |           4.5570 |
[32m[20230113 19:59:57 @agent_ppo2.py:186][0m |          -0.0119 |           4.4821 |           4.5576 |
[32m[20230113 19:59:57 @agent_ppo2.py:186][0m |          -0.0075 |           4.3945 |           4.5594 |
[32m[20230113 19:59:57 @agent_ppo2.py:186][0m |          -0.0072 |           4.2783 |           4.5599 |
[32m[20230113 19:59:57 @agent_ppo2.py:186][0m |          -0.0095 |           4.1975 |           4.5616 |
[32m[20230113 19:59:57 @agent_ppo2.py:186][0m |          -0.0082 |           4.1464 |           4.5575 |
[32m[20230113 19:59:57 @agent_ppo2.py:186][0m |          -0.0095 |           4.0776 |           4.5597 |
[32m[20230113 19:59:57 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 19:59:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.72
[32m[20230113 19:59:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.09
[32m[20230113 19:59:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.61
[32m[20230113 19:59:58 @agent_ppo2.py:144][0m Total time:      15.42 min
[32m[20230113 19:59:58 @agent_ppo2.py:146][0m 1386496 total steps have happened
[32m[20230113 19:59:58 @agent_ppo2.py:122][0m #------------------------ Iteration 677 --------------------------#
[32m[20230113 19:59:58 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 19:59:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 19:59:58 @agent_ppo2.py:186][0m |          -0.0017 |           5.4023 |           4.5690 |
[32m[20230113 19:59:58 @agent_ppo2.py:186][0m |          -0.0063 |           4.7361 |           4.5646 |
[32m[20230113 19:59:58 @agent_ppo2.py:186][0m |          -0.0052 |           4.4612 |           4.5605 |
[32m[20230113 19:59:58 @agent_ppo2.py:186][0m |          -0.0078 |           4.2066 |           4.5632 |
[32m[20230113 19:59:58 @agent_ppo2.py:186][0m |          -0.0098 |           3.9743 |           4.5592 |
[32m[20230113 19:59:58 @agent_ppo2.py:186][0m |          -0.0138 |           3.8026 |           4.5598 |
[32m[20230113 19:59:59 @agent_ppo2.py:186][0m |          -0.0082 |           3.8654 |           4.5548 |
[32m[20230113 19:59:59 @agent_ppo2.py:186][0m |          -0.0134 |           3.5557 |           4.5529 |
[32m[20230113 19:59:59 @agent_ppo2.py:186][0m |          -0.0142 |           3.3991 |           4.5545 |
[32m[20230113 19:59:59 @agent_ppo2.py:186][0m |          -0.0122 |           3.4385 |           4.5544 |
[32m[20230113 19:59:59 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 19:59:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.66
[32m[20230113 19:59:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.89
[32m[20230113 19:59:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.04
[32m[20230113 19:59:59 @agent_ppo2.py:144][0m Total time:      15.44 min
[32m[20230113 19:59:59 @agent_ppo2.py:146][0m 1388544 total steps have happened
[32m[20230113 19:59:59 @agent_ppo2.py:122][0m #------------------------ Iteration 678 --------------------------#
[32m[20230113 20:00:00 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:00:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:00 @agent_ppo2.py:186][0m |          -0.0003 |           4.5364 |           4.6288 |
[32m[20230113 20:00:00 @agent_ppo2.py:186][0m |          -0.0052 |           4.1017 |           4.6193 |
[32m[20230113 20:00:00 @agent_ppo2.py:186][0m |          -0.0064 |           3.9077 |           4.6092 |
[32m[20230113 20:00:00 @agent_ppo2.py:186][0m |          -0.0091 |           3.8238 |           4.6152 |
[32m[20230113 20:00:00 @agent_ppo2.py:186][0m |          -0.0097 |           3.6652 |           4.6149 |
[32m[20230113 20:00:00 @agent_ppo2.py:186][0m |          -0.0095 |           3.5875 |           4.6150 |
[32m[20230113 20:00:00 @agent_ppo2.py:186][0m |          -0.0105 |           3.4569 |           4.6136 |
[32m[20230113 20:00:00 @agent_ppo2.py:186][0m |          -0.0113 |           3.3816 |           4.6100 |
[32m[20230113 20:00:00 @agent_ppo2.py:186][0m |          -0.0108 |           3.3125 |           4.6094 |
[32m[20230113 20:00:00 @agent_ppo2.py:186][0m |          -0.0120 |           3.2576 |           4.6093 |
[32m[20230113 20:00:00 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 214.51
[32m[20230113 20:00:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 216.40
[32m[20230113 20:00:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.89
[32m[20230113 20:00:00 @agent_ppo2.py:144][0m Total time:      15.47 min
[32m[20230113 20:00:00 @agent_ppo2.py:146][0m 1390592 total steps have happened
[32m[20230113 20:00:00 @agent_ppo2.py:122][0m #------------------------ Iteration 679 --------------------------#
[32m[20230113 20:00:01 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:00:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:01 @agent_ppo2.py:186][0m |          -0.0016 |           6.3752 |           4.6796 |
[32m[20230113 20:00:01 @agent_ppo2.py:186][0m |          -0.0040 |           5.4505 |           4.6715 |
[32m[20230113 20:00:01 @agent_ppo2.py:186][0m |          -0.0070 |           5.2356 |           4.6719 |
[32m[20230113 20:00:01 @agent_ppo2.py:186][0m |          -0.0097 |           4.8881 |           4.6658 |
[32m[20230113 20:00:01 @agent_ppo2.py:186][0m |          -0.0083 |           4.6900 |           4.6666 |
[32m[20230113 20:00:01 @agent_ppo2.py:186][0m |          -0.0113 |           4.4563 |           4.6650 |
[32m[20230113 20:00:01 @agent_ppo2.py:186][0m |          -0.0102 |           4.2452 |           4.6648 |
[32m[20230113 20:00:01 @agent_ppo2.py:186][0m |          -0.0104 |           4.0772 |           4.6609 |
[32m[20230113 20:00:01 @agent_ppo2.py:186][0m |          -0.0106 |           3.9548 |           4.6591 |
[32m[20230113 20:00:01 @agent_ppo2.py:186][0m |          -0.0130 |           3.8164 |           4.6614 |
[32m[20230113 20:00:01 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:00:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.13
[32m[20230113 20:00:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.22
[32m[20230113 20:00:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.25
[32m[20230113 20:00:02 @agent_ppo2.py:144][0m Total time:      15.49 min
[32m[20230113 20:00:02 @agent_ppo2.py:146][0m 1392640 total steps have happened
[32m[20230113 20:00:02 @agent_ppo2.py:122][0m #------------------------ Iteration 680 --------------------------#
[32m[20230113 20:00:02 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:02 @agent_ppo2.py:186][0m |           0.0002 |           5.8527 |           4.6456 |
[32m[20230113 20:00:02 @agent_ppo2.py:186][0m |          -0.0023 |           4.9070 |           4.6331 |
[32m[20230113 20:00:02 @agent_ppo2.py:186][0m |          -0.0017 |           4.3928 |           4.6437 |
[32m[20230113 20:00:02 @agent_ppo2.py:186][0m |          -0.0026 |           4.0025 |           4.6426 |
[32m[20230113 20:00:03 @agent_ppo2.py:186][0m |          -0.0080 |           3.6601 |           4.6425 |
[32m[20230113 20:00:03 @agent_ppo2.py:186][0m |          -0.0071 |           3.4271 |           4.6420 |
[32m[20230113 20:00:03 @agent_ppo2.py:186][0m |          -0.0117 |           3.2928 |           4.6433 |
[32m[20230113 20:00:03 @agent_ppo2.py:186][0m |          -0.0057 |           3.2528 |           4.6466 |
[32m[20230113 20:00:03 @agent_ppo2.py:186][0m |          -0.0105 |           3.0527 |           4.6440 |
[32m[20230113 20:00:03 @agent_ppo2.py:186][0m |          -0.0080 |           3.0117 |           4.6454 |
[32m[20230113 20:00:03 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.98
[32m[20230113 20:00:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.48
[32m[20230113 20:00:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.42
[32m[20230113 20:00:03 @agent_ppo2.py:144][0m Total time:      15.51 min
[32m[20230113 20:00:03 @agent_ppo2.py:146][0m 1394688 total steps have happened
[32m[20230113 20:00:03 @agent_ppo2.py:122][0m #------------------------ Iteration 681 --------------------------#
[32m[20230113 20:00:04 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:04 @agent_ppo2.py:186][0m |          -0.0006 |           6.4213 |           4.5683 |
[32m[20230113 20:00:04 @agent_ppo2.py:186][0m |          -0.0050 |           5.3050 |           4.5617 |
[32m[20230113 20:00:04 @agent_ppo2.py:186][0m |          -0.0071 |           5.0119 |           4.5608 |
[32m[20230113 20:00:04 @agent_ppo2.py:186][0m |          -0.0101 |           4.8092 |           4.5563 |
[32m[20230113 20:00:04 @agent_ppo2.py:186][0m |          -0.0107 |           4.5905 |           4.5594 |
[32m[20230113 20:00:04 @agent_ppo2.py:186][0m |          -0.0126 |           4.4274 |           4.5571 |
[32m[20230113 20:00:04 @agent_ppo2.py:186][0m |          -0.0069 |           4.3912 |           4.5590 |
[32m[20230113 20:00:04 @agent_ppo2.py:186][0m |          -0.0143 |           4.2729 |           4.5602 |
[32m[20230113 20:00:04 @agent_ppo2.py:186][0m |          -0.0125 |           4.1842 |           4.5597 |
[32m[20230113 20:00:04 @agent_ppo2.py:186][0m |          -0.0134 |           4.0869 |           4.5629 |
[32m[20230113 20:00:04 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.07
[32m[20230113 20:00:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.93
[32m[20230113 20:00:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.38
[32m[20230113 20:00:05 @agent_ppo2.py:144][0m Total time:      15.53 min
[32m[20230113 20:00:05 @agent_ppo2.py:146][0m 1396736 total steps have happened
[32m[20230113 20:00:05 @agent_ppo2.py:122][0m #------------------------ Iteration 682 --------------------------#
[32m[20230113 20:00:05 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:05 @agent_ppo2.py:186][0m |           0.0003 |           5.8856 |           4.5877 |
[32m[20230113 20:00:05 @agent_ppo2.py:186][0m |          -0.0047 |           5.3851 |           4.5843 |
[32m[20230113 20:00:05 @agent_ppo2.py:186][0m |          -0.0072 |           5.1099 |           4.5850 |
[32m[20230113 20:00:05 @agent_ppo2.py:186][0m |          -0.0075 |           4.9457 |           4.5827 |
[32m[20230113 20:00:05 @agent_ppo2.py:186][0m |          -0.0097 |           4.7806 |           4.5814 |
[32m[20230113 20:00:05 @agent_ppo2.py:186][0m |          -0.0106 |           4.6904 |           4.5818 |
[32m[20230113 20:00:05 @agent_ppo2.py:186][0m |          -0.0109 |           4.6323 |           4.5817 |
[32m[20230113 20:00:05 @agent_ppo2.py:186][0m |          -0.0121 |           4.5930 |           4.5837 |
[32m[20230113 20:00:05 @agent_ppo2.py:186][0m |          -0.0109 |           4.4616 |           4.5842 |
[32m[20230113 20:00:05 @agent_ppo2.py:186][0m |          -0.0125 |           4.4452 |           4.5860 |
[32m[20230113 20:00:05 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.81
[32m[20230113 20:00:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.05
[32m[20230113 20:00:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.14
[32m[20230113 20:00:06 @agent_ppo2.py:144][0m Total time:      15.56 min
[32m[20230113 20:00:06 @agent_ppo2.py:146][0m 1398784 total steps have happened
[32m[20230113 20:00:06 @agent_ppo2.py:122][0m #------------------------ Iteration 683 --------------------------#
[32m[20230113 20:00:06 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:00:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:06 @agent_ppo2.py:186][0m |           0.0026 |           3.6690 |           4.6025 |
[32m[20230113 20:00:07 @agent_ppo2.py:186][0m |           0.0086 |           3.3991 |           4.6022 |
[32m[20230113 20:00:07 @agent_ppo2.py:186][0m |          -0.0087 |           2.9056 |           4.5961 |
[32m[20230113 20:00:07 @agent_ppo2.py:186][0m |          -0.0110 |           2.7606 |           4.5972 |
[32m[20230113 20:00:07 @agent_ppo2.py:186][0m |          -0.0078 |           2.6594 |           4.5921 |
[32m[20230113 20:00:07 @agent_ppo2.py:186][0m |          -0.0089 |           2.6055 |           4.5884 |
[32m[20230113 20:00:07 @agent_ppo2.py:186][0m |          -0.0110 |           2.5672 |           4.5889 |
[32m[20230113 20:00:07 @agent_ppo2.py:186][0m |          -0.0078 |           2.5280 |           4.5866 |
[32m[20230113 20:00:07 @agent_ppo2.py:186][0m |          -0.0124 |           2.5051 |           4.5841 |
[32m[20230113 20:00:07 @agent_ppo2.py:186][0m |          -0.0152 |           2.4600 |           4.5823 |
[32m[20230113 20:00:07 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.07
[32m[20230113 20:00:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.98
[32m[20230113 20:00:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.08
[32m[20230113 20:00:07 @agent_ppo2.py:144][0m Total time:      15.58 min
[32m[20230113 20:00:07 @agent_ppo2.py:146][0m 1400832 total steps have happened
[32m[20230113 20:00:07 @agent_ppo2.py:122][0m #------------------------ Iteration 684 --------------------------#
[32m[20230113 20:00:08 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:08 @agent_ppo2.py:186][0m |          -0.0012 |           5.7197 |           4.6251 |
[32m[20230113 20:00:08 @agent_ppo2.py:186][0m |          -0.0036 |           4.6853 |           4.6163 |
[32m[20230113 20:00:08 @agent_ppo2.py:186][0m |          -0.0081 |           4.1464 |           4.6198 |
[32m[20230113 20:00:08 @agent_ppo2.py:186][0m |          -0.0082 |           3.8751 |           4.6186 |
[32m[20230113 20:00:08 @agent_ppo2.py:186][0m |          -0.0089 |           3.6489 |           4.6215 |
[32m[20230113 20:00:08 @agent_ppo2.py:186][0m |          -0.0114 |           3.4097 |           4.6209 |
[32m[20230113 20:00:08 @agent_ppo2.py:186][0m |          -0.0107 |           3.2590 |           4.6211 |
[32m[20230113 20:00:08 @agent_ppo2.py:186][0m |          -0.0112 |           3.1338 |           4.6218 |
[32m[20230113 20:00:08 @agent_ppo2.py:186][0m |          -0.0116 |           3.0515 |           4.6251 |
[32m[20230113 20:00:08 @agent_ppo2.py:186][0m |          -0.0121 |           2.9790 |           4.6215 |
[32m[20230113 20:00:08 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:00:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.70
[32m[20230113 20:00:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.70
[32m[20230113 20:00:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.92
[32m[20230113 20:00:09 @agent_ppo2.py:144][0m Total time:      15.60 min
[32m[20230113 20:00:09 @agent_ppo2.py:146][0m 1402880 total steps have happened
[32m[20230113 20:00:09 @agent_ppo2.py:122][0m #------------------------ Iteration 685 --------------------------#
[32m[20230113 20:00:09 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:00:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:09 @agent_ppo2.py:186][0m |           0.0105 |           6.1389 |           4.6929 |
[32m[20230113 20:00:09 @agent_ppo2.py:186][0m |          -0.0047 |           4.9224 |           4.6861 |
[32m[20230113 20:00:09 @agent_ppo2.py:186][0m |          -0.0145 |           4.4316 |           4.6778 |
[32m[20230113 20:00:09 @agent_ppo2.py:186][0m |          -0.0148 |           4.1424 |           4.6851 |
[32m[20230113 20:00:09 @agent_ppo2.py:186][0m |          -0.0118 |           3.8251 |           4.6804 |
[32m[20230113 20:00:09 @agent_ppo2.py:186][0m |          -0.0085 |           3.6681 |           4.6780 |
[32m[20230113 20:00:09 @agent_ppo2.py:186][0m |          -0.0173 |           3.5239 |           4.6776 |
[32m[20230113 20:00:10 @agent_ppo2.py:186][0m |          -0.0108 |           3.3461 |           4.6798 |
[32m[20230113 20:00:10 @agent_ppo2.py:186][0m |          -0.0164 |           3.2431 |           4.6754 |
[32m[20230113 20:00:10 @agent_ppo2.py:186][0m |          -0.0132 |           3.1195 |           4.6762 |
[32m[20230113 20:00:10 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.17
[32m[20230113 20:00:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.11
[32m[20230113 20:00:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.08
[32m[20230113 20:00:10 @agent_ppo2.py:144][0m Total time:      15.62 min
[32m[20230113 20:00:10 @agent_ppo2.py:146][0m 1404928 total steps have happened
[32m[20230113 20:00:10 @agent_ppo2.py:122][0m #------------------------ Iteration 686 --------------------------#
[32m[20230113 20:00:10 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:00:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:11 @agent_ppo2.py:186][0m |           0.0174 |           6.2375 |           4.6357 |
[32m[20230113 20:00:11 @agent_ppo2.py:186][0m |          -0.0023 |           5.2463 |           4.6251 |
[32m[20230113 20:00:11 @agent_ppo2.py:186][0m |          -0.0080 |           4.9687 |           4.6293 |
[32m[20230113 20:00:11 @agent_ppo2.py:186][0m |          -0.0121 |           4.7933 |           4.6316 |
[32m[20230113 20:00:11 @agent_ppo2.py:186][0m |          -0.0040 |           4.6117 |           4.6286 |
[32m[20230113 20:00:11 @agent_ppo2.py:186][0m |          -0.0107 |           4.5113 |           4.6358 |
[32m[20230113 20:00:11 @agent_ppo2.py:186][0m |          -0.0132 |           4.3699 |           4.6337 |
[32m[20230113 20:00:11 @agent_ppo2.py:186][0m |          -0.0115 |           4.2627 |           4.6345 |
[32m[20230113 20:00:11 @agent_ppo2.py:186][0m |          -0.0076 |           4.2275 |           4.6325 |
[32m[20230113 20:00:11 @agent_ppo2.py:186][0m |          -0.0205 |           4.0833 |           4.6389 |
[32m[20230113 20:00:11 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.74
[32m[20230113 20:00:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.17
[32m[20230113 20:00:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.48
[32m[20230113 20:00:11 @agent_ppo2.py:144][0m Total time:      15.65 min
[32m[20230113 20:00:11 @agent_ppo2.py:146][0m 1406976 total steps have happened
[32m[20230113 20:00:11 @agent_ppo2.py:122][0m #------------------------ Iteration 687 --------------------------#
[32m[20230113 20:00:12 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:12 @agent_ppo2.py:186][0m |          -0.0000 |           3.2851 |           4.7021 |
[32m[20230113 20:00:12 @agent_ppo2.py:186][0m |          -0.0043 |           2.6328 |           4.6947 |
[32m[20230113 20:00:12 @agent_ppo2.py:186][0m |          -0.0079 |           2.4633 |           4.6913 |
[32m[20230113 20:00:12 @agent_ppo2.py:186][0m |          -0.0072 |           2.3291 |           4.6908 |
[32m[20230113 20:00:12 @agent_ppo2.py:186][0m |          -0.0089 |           2.1942 |           4.6861 |
[32m[20230113 20:00:12 @agent_ppo2.py:186][0m |          -0.0073 |           2.1826 |           4.6877 |
[32m[20230113 20:00:12 @agent_ppo2.py:186][0m |          -0.0104 |           2.0847 |           4.6868 |
[32m[20230113 20:00:12 @agent_ppo2.py:186][0m |          -0.0111 |           2.0276 |           4.6854 |
[32m[20230113 20:00:12 @agent_ppo2.py:186][0m |          -0.0117 |           1.9972 |           4.6838 |
[32m[20230113 20:00:12 @agent_ppo2.py:186][0m |          -0.0100 |           1.9387 |           4.6846 |
[32m[20230113 20:00:12 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.56
[32m[20230113 20:00:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.00
[32m[20230113 20:00:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.06
[32m[20230113 20:00:13 @agent_ppo2.py:144][0m Total time:      15.67 min
[32m[20230113 20:00:13 @agent_ppo2.py:146][0m 1409024 total steps have happened
[32m[20230113 20:00:13 @agent_ppo2.py:122][0m #------------------------ Iteration 688 --------------------------#
[32m[20230113 20:00:13 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:00:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:13 @agent_ppo2.py:186][0m |           0.0013 |           3.7185 |           4.5618 |
[32m[20230113 20:00:13 @agent_ppo2.py:186][0m |          -0.0070 |           3.1155 |           4.5642 |
[32m[20230113 20:00:13 @agent_ppo2.py:186][0m |          -0.0072 |           2.9230 |           4.5582 |
[32m[20230113 20:00:13 @agent_ppo2.py:186][0m |          -0.0137 |           2.7622 |           4.5581 |
[32m[20230113 20:00:14 @agent_ppo2.py:186][0m |          -0.0058 |           2.6656 |           4.5545 |
[32m[20230113 20:00:14 @agent_ppo2.py:186][0m |          -0.0120 |           2.5884 |           4.5547 |
[32m[20230113 20:00:14 @agent_ppo2.py:186][0m |          -0.0106 |           2.4979 |           4.5564 |
[32m[20230113 20:00:14 @agent_ppo2.py:186][0m |          -0.0132 |           2.4501 |           4.5550 |
[32m[20230113 20:00:14 @agent_ppo2.py:186][0m |          -0.0128 |           2.3943 |           4.5553 |
[32m[20230113 20:00:14 @agent_ppo2.py:186][0m |          -0.0213 |           2.3648 |           4.5546 |
[32m[20230113 20:00:14 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 20:00:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.86
[32m[20230113 20:00:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.40
[32m[20230113 20:00:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.96
[32m[20230113 20:00:14 @agent_ppo2.py:144][0m Total time:      15.69 min
[32m[20230113 20:00:14 @agent_ppo2.py:146][0m 1411072 total steps have happened
[32m[20230113 20:00:14 @agent_ppo2.py:122][0m #------------------------ Iteration 689 --------------------------#
[32m[20230113 20:00:15 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:15 @agent_ppo2.py:186][0m |           0.0004 |           5.5304 |           4.7390 |
[32m[20230113 20:00:15 @agent_ppo2.py:186][0m |          -0.0021 |           4.9384 |           4.7340 |
[32m[20230113 20:00:15 @agent_ppo2.py:186][0m |          -0.0040 |           4.6035 |           4.7349 |
[32m[20230113 20:00:15 @agent_ppo2.py:186][0m |          -0.0056 |           4.4519 |           4.7322 |
[32m[20230113 20:00:15 @agent_ppo2.py:186][0m |          -0.0069 |           4.2945 |           4.7269 |
[32m[20230113 20:00:15 @agent_ppo2.py:186][0m |          -0.0074 |           4.1770 |           4.7304 |
[32m[20230113 20:00:15 @agent_ppo2.py:186][0m |          -0.0084 |           4.0228 |           4.7292 |
[32m[20230113 20:00:15 @agent_ppo2.py:186][0m |          -0.0085 |           3.9588 |           4.7281 |
[32m[20230113 20:00:15 @agent_ppo2.py:186][0m |          -0.0096 |           3.8606 |           4.7285 |
[32m[20230113 20:00:15 @agent_ppo2.py:186][0m |          -0.0101 |           3.7719 |           4.7280 |
[32m[20230113 20:00:15 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:00:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.23
[32m[20230113 20:00:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.00
[32m[20230113 20:00:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.80
[32m[20230113 20:00:16 @agent_ppo2.py:144][0m Total time:      15.72 min
[32m[20230113 20:00:16 @agent_ppo2.py:146][0m 1413120 total steps have happened
[32m[20230113 20:00:16 @agent_ppo2.py:122][0m #------------------------ Iteration 690 --------------------------#
[32m[20230113 20:00:16 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:00:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:16 @agent_ppo2.py:186][0m |          -0.0021 |          22.7680 |           4.7282 |
[32m[20230113 20:00:16 @agent_ppo2.py:186][0m |          -0.0066 |           5.5961 |           4.7289 |
[32m[20230113 20:00:16 @agent_ppo2.py:186][0m |          -0.0083 |           4.4288 |           4.7290 |
[32m[20230113 20:00:16 @agent_ppo2.py:186][0m |          -0.0039 |           4.1382 |           4.7280 |
[32m[20230113 20:00:16 @agent_ppo2.py:186][0m |          -0.0100 |           3.9695 |           4.7282 |
[32m[20230113 20:00:16 @agent_ppo2.py:186][0m |          -0.0077 |           3.7593 |           4.7275 |
[32m[20230113 20:00:16 @agent_ppo2.py:186][0m |          -0.0130 |           3.7098 |           4.7252 |
[32m[20230113 20:00:16 @agent_ppo2.py:186][0m |          -0.0102 |           3.5348 |           4.7254 |
[32m[20230113 20:00:16 @agent_ppo2.py:186][0m |          -0.0159 |           3.4414 |           4.7243 |
[32m[20230113 20:00:16 @agent_ppo2.py:186][0m |          -0.0112 |           3.4100 |           4.7249 |
[32m[20230113 20:00:16 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:00:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 49.45
[32m[20230113 20:00:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.67
[32m[20230113 20:00:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.78
[32m[20230113 20:00:17 @agent_ppo2.py:144][0m Total time:      15.74 min
[32m[20230113 20:00:17 @agent_ppo2.py:146][0m 1415168 total steps have happened
[32m[20230113 20:00:17 @agent_ppo2.py:122][0m #------------------------ Iteration 691 --------------------------#
[32m[20230113 20:00:17 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:00:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:17 @agent_ppo2.py:186][0m |          -0.0007 |           3.4450 |           4.6012 |
[32m[20230113 20:00:17 @agent_ppo2.py:186][0m |          -0.0067 |           2.9505 |           4.5995 |
[32m[20230113 20:00:17 @agent_ppo2.py:186][0m |          -0.0090 |           2.7682 |           4.6005 |
[32m[20230113 20:00:17 @agent_ppo2.py:186][0m |          -0.0132 |           2.6511 |           4.5951 |
[32m[20230113 20:00:18 @agent_ppo2.py:186][0m |          -0.0032 |           2.8858 |           4.5941 |
[32m[20230113 20:00:18 @agent_ppo2.py:186][0m |          -0.0049 |           2.5908 |           4.5926 |
[32m[20230113 20:00:18 @agent_ppo2.py:186][0m |          -0.0090 |           2.3995 |           4.5889 |
[32m[20230113 20:00:18 @agent_ppo2.py:186][0m |          -0.0145 |           2.3141 |           4.5899 |
[32m[20230113 20:00:18 @agent_ppo2.py:186][0m |          -0.0151 |           2.2486 |           4.5891 |
[32m[20230113 20:00:18 @agent_ppo2.py:186][0m |          -0.0102 |           2.2461 |           4.5856 |
[32m[20230113 20:00:18 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.73
[32m[20230113 20:00:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.19
[32m[20230113 20:00:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.72
[32m[20230113 20:00:18 @agent_ppo2.py:144][0m Total time:      15.76 min
[32m[20230113 20:00:18 @agent_ppo2.py:146][0m 1417216 total steps have happened
[32m[20230113 20:00:18 @agent_ppo2.py:122][0m #------------------------ Iteration 692 --------------------------#
[32m[20230113 20:00:19 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 20:00:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:19 @agent_ppo2.py:186][0m |          -0.0023 |          13.3493 |           4.6063 |
[32m[20230113 20:00:19 @agent_ppo2.py:186][0m |          -0.0075 |           6.8392 |           4.6049 |
[32m[20230113 20:00:19 @agent_ppo2.py:186][0m |          -0.0025 |           6.4906 |           4.6078 |
[32m[20230113 20:00:19 @agent_ppo2.py:186][0m |          -0.0050 |           5.5543 |           4.6033 |
[32m[20230113 20:00:19 @agent_ppo2.py:186][0m |          -0.0119 |           4.9700 |           4.6037 |
[32m[20230113 20:00:19 @agent_ppo2.py:186][0m |          -0.0087 |           4.8322 |           4.6050 |
[32m[20230113 20:00:19 @agent_ppo2.py:186][0m |          -0.0162 |           4.6040 |           4.6031 |
[32m[20230113 20:00:19 @agent_ppo2.py:186][0m |          -0.0021 |           4.4912 |           4.6045 |
[32m[20230113 20:00:19 @agent_ppo2.py:186][0m |          -0.0075 |           4.7502 |           4.6046 |
[32m[20230113 20:00:19 @agent_ppo2.py:186][0m |          -0.0164 |           4.3056 |           4.6029 |
[32m[20230113 20:00:19 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:00:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 126.83
[32m[20230113 20:00:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.48
[32m[20230113 20:00:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.71
[32m[20230113 20:00:20 @agent_ppo2.py:144][0m Total time:      15.78 min
[32m[20230113 20:00:20 @agent_ppo2.py:146][0m 1419264 total steps have happened
[32m[20230113 20:00:20 @agent_ppo2.py:122][0m #------------------------ Iteration 693 --------------------------#
[32m[20230113 20:00:20 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:20 @agent_ppo2.py:186][0m |           0.0022 |           5.9063 |           4.5497 |
[32m[20230113 20:00:20 @agent_ppo2.py:186][0m |          -0.0019 |           5.1410 |           4.5441 |
[32m[20230113 20:00:20 @agent_ppo2.py:186][0m |          -0.0095 |           4.8112 |           4.5414 |
[32m[20230113 20:00:20 @agent_ppo2.py:186][0m |          -0.0221 |           4.6122 |           4.5367 |
[32m[20230113 20:00:20 @agent_ppo2.py:186][0m |          -0.0082 |           4.3939 |           4.5390 |
[32m[20230113 20:00:20 @agent_ppo2.py:186][0m |          -0.0064 |           4.2993 |           4.5367 |
[32m[20230113 20:00:20 @agent_ppo2.py:186][0m |          -0.0087 |           4.1831 |           4.5325 |
[32m[20230113 20:00:20 @agent_ppo2.py:186][0m |          -0.0116 |           4.0506 |           4.5355 |
[32m[20230113 20:00:21 @agent_ppo2.py:186][0m |          -0.0052 |           4.0077 |           4.5301 |
[32m[20230113 20:00:21 @agent_ppo2.py:186][0m |          -0.0189 |           3.9030 |           4.5258 |
[32m[20230113 20:00:21 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.85
[32m[20230113 20:00:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.49
[32m[20230113 20:00:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.53
[32m[20230113 20:00:21 @agent_ppo2.py:144][0m Total time:      15.81 min
[32m[20230113 20:00:21 @agent_ppo2.py:146][0m 1421312 total steps have happened
[32m[20230113 20:00:21 @agent_ppo2.py:122][0m #------------------------ Iteration 694 --------------------------#
[32m[20230113 20:00:21 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:00:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:22 @agent_ppo2.py:186][0m |          -0.0005 |           4.1205 |           4.7204 |
[32m[20230113 20:00:22 @agent_ppo2.py:186][0m |          -0.0064 |           3.5989 |           4.7154 |
[32m[20230113 20:00:22 @agent_ppo2.py:186][0m |          -0.0082 |           3.4154 |           4.7118 |
[32m[20230113 20:00:22 @agent_ppo2.py:186][0m |          -0.0095 |           3.2727 |           4.7129 |
[32m[20230113 20:00:22 @agent_ppo2.py:186][0m |          -0.0103 |           3.1936 |           4.7126 |
[32m[20230113 20:00:22 @agent_ppo2.py:186][0m |          -0.0110 |           3.1032 |           4.7153 |
[32m[20230113 20:00:22 @agent_ppo2.py:186][0m |          -0.0115 |           3.0168 |           4.7109 |
[32m[20230113 20:00:22 @agent_ppo2.py:186][0m |          -0.0121 |           2.9621 |           4.7159 |
[32m[20230113 20:00:22 @agent_ppo2.py:186][0m |          -0.0123 |           2.9193 |           4.7145 |
[32m[20230113 20:00:22 @agent_ppo2.py:186][0m |          -0.0128 |           2.8526 |           4.7135 |
[32m[20230113 20:00:22 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 218.08
[32m[20230113 20:00:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 218.19
[32m[20230113 20:00:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.63
[32m[20230113 20:00:22 @agent_ppo2.py:144][0m Total time:      15.83 min
[32m[20230113 20:00:22 @agent_ppo2.py:146][0m 1423360 total steps have happened
[32m[20230113 20:00:22 @agent_ppo2.py:122][0m #------------------------ Iteration 695 --------------------------#
[32m[20230113 20:00:23 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:00:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:23 @agent_ppo2.py:186][0m |          -0.0002 |           5.0918 |           4.7299 |
[32m[20230113 20:00:23 @agent_ppo2.py:186][0m |          -0.0008 |           4.5464 |           4.7200 |
[32m[20230113 20:00:23 @agent_ppo2.py:186][0m |          -0.0035 |           4.0819 |           4.7185 |
[32m[20230113 20:00:23 @agent_ppo2.py:186][0m |          -0.0098 |           3.7120 |           4.7132 |
[32m[20230113 20:00:23 @agent_ppo2.py:186][0m |          -0.0102 |           3.4997 |           4.7142 |
[32m[20230113 20:00:23 @agent_ppo2.py:186][0m |          -0.0089 |           3.3502 |           4.7092 |
[32m[20230113 20:00:23 @agent_ppo2.py:186][0m |          -0.0105 |           3.1994 |           4.7074 |
[32m[20230113 20:00:23 @agent_ppo2.py:186][0m |          -0.0126 |           3.0966 |           4.7072 |
[32m[20230113 20:00:23 @agent_ppo2.py:186][0m |          -0.0117 |           2.9936 |           4.7072 |
[32m[20230113 20:00:23 @agent_ppo2.py:186][0m |          -0.0122 |           2.9003 |           4.7032 |
[32m[20230113 20:00:23 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.46
[32m[20230113 20:00:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.58
[32m[20230113 20:00:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.29
[32m[20230113 20:00:24 @agent_ppo2.py:144][0m Total time:      15.85 min
[32m[20230113 20:00:24 @agent_ppo2.py:146][0m 1425408 total steps have happened
[32m[20230113 20:00:24 @agent_ppo2.py:122][0m #------------------------ Iteration 696 --------------------------#
[32m[20230113 20:00:24 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:24 @agent_ppo2.py:186][0m |          -0.0010 |           5.9621 |           4.6681 |
[32m[20230113 20:00:24 @agent_ppo2.py:186][0m |          -0.0016 |           4.9113 |           4.6614 |
[32m[20230113 20:00:24 @agent_ppo2.py:186][0m |          -0.0056 |           4.5226 |           4.6567 |
[32m[20230113 20:00:24 @agent_ppo2.py:186][0m |          -0.0090 |           4.3328 |           4.6558 |
[32m[20230113 20:00:24 @agent_ppo2.py:186][0m |          -0.0080 |           4.1467 |           4.6537 |
[32m[20230113 20:00:25 @agent_ppo2.py:186][0m |          -0.0005 |           4.2802 |           4.6554 |
[32m[20230113 20:00:25 @agent_ppo2.py:186][0m |          -0.0107 |           3.9373 |           4.6557 |
[32m[20230113 20:00:25 @agent_ppo2.py:186][0m |          -0.0084 |           3.8256 |           4.6585 |
[32m[20230113 20:00:25 @agent_ppo2.py:186][0m |          -0.0113 |           3.7383 |           4.6584 |
[32m[20230113 20:00:25 @agent_ppo2.py:186][0m |          -0.0145 |           3.6856 |           4.6570 |
[32m[20230113 20:00:25 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.87
[32m[20230113 20:00:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.51
[32m[20230113 20:00:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.78
[32m[20230113 20:00:25 @agent_ppo2.py:144][0m Total time:      15.88 min
[32m[20230113 20:00:25 @agent_ppo2.py:146][0m 1427456 total steps have happened
[32m[20230113 20:00:25 @agent_ppo2.py:122][0m #------------------------ Iteration 697 --------------------------#
[32m[20230113 20:00:26 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:26 @agent_ppo2.py:186][0m |           0.0025 |           5.3660 |           4.7643 |
[32m[20230113 20:00:26 @agent_ppo2.py:186][0m |          -0.0034 |           4.8019 |           4.7586 |
[32m[20230113 20:00:26 @agent_ppo2.py:186][0m |          -0.0062 |           4.5475 |           4.7593 |
[32m[20230113 20:00:26 @agent_ppo2.py:186][0m |          -0.0089 |           4.2925 |           4.7596 |
[32m[20230113 20:00:26 @agent_ppo2.py:186][0m |          -0.0104 |           4.1885 |           4.7554 |
[32m[20230113 20:00:26 @agent_ppo2.py:186][0m |          -0.0121 |           4.1121 |           4.7566 |
[32m[20230113 20:00:26 @agent_ppo2.py:186][0m |          -0.0110 |           4.0394 |           4.7543 |
[32m[20230113 20:00:26 @agent_ppo2.py:186][0m |          -0.0133 |           4.0090 |           4.7558 |
[32m[20230113 20:00:26 @agent_ppo2.py:186][0m |          -0.0124 |           3.8951 |           4.7556 |
[32m[20230113 20:00:26 @agent_ppo2.py:186][0m |          -0.0110 |           3.8433 |           4.7533 |
[32m[20230113 20:00:26 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.37
[32m[20230113 20:00:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.29
[32m[20230113 20:00:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.08
[32m[20230113 20:00:26 @agent_ppo2.py:144][0m Total time:      15.90 min
[32m[20230113 20:00:26 @agent_ppo2.py:146][0m 1429504 total steps have happened
[32m[20230113 20:00:26 @agent_ppo2.py:122][0m #------------------------ Iteration 698 --------------------------#
[32m[20230113 20:00:27 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:00:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:27 @agent_ppo2.py:186][0m |          -0.0041 |           5.4122 |           4.6233 |
[32m[20230113 20:00:27 @agent_ppo2.py:186][0m |          -0.0016 |           4.9096 |           4.6171 |
[32m[20230113 20:00:27 @agent_ppo2.py:186][0m |          -0.0091 |           4.6014 |           4.6232 |
[32m[20230113 20:00:27 @agent_ppo2.py:186][0m |          -0.0047 |           4.3920 |           4.6252 |
[32m[20230113 20:00:27 @agent_ppo2.py:186][0m |           0.0429 |           4.7573 |           4.6278 |
[32m[20230113 20:00:27 @agent_ppo2.py:186][0m |          -0.0162 |           4.1712 |           4.6137 |
[32m[20230113 20:00:27 @agent_ppo2.py:186][0m |          -0.0136 |           3.9348 |           4.6218 |
[32m[20230113 20:00:27 @agent_ppo2.py:186][0m |          -0.0104 |           3.8239 |           4.6306 |
[32m[20230113 20:00:27 @agent_ppo2.py:186][0m |          -0.0122 |           3.8166 |           4.6279 |
[32m[20230113 20:00:27 @agent_ppo2.py:186][0m |          -0.0170 |           3.7521 |           4.6269 |
[32m[20230113 20:00:27 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.79
[32m[20230113 20:00:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.48
[32m[20230113 20:00:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.11
[32m[20230113 20:00:28 @agent_ppo2.py:144][0m Total time:      15.92 min
[32m[20230113 20:00:28 @agent_ppo2.py:146][0m 1431552 total steps have happened
[32m[20230113 20:00:28 @agent_ppo2.py:122][0m #------------------------ Iteration 699 --------------------------#
[32m[20230113 20:00:28 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 20:00:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:28 @agent_ppo2.py:186][0m |           0.0008 |          11.2412 |           4.7747 |
[32m[20230113 20:00:28 @agent_ppo2.py:186][0m |          -0.0018 |           3.8957 |           4.7710 |
[32m[20230113 20:00:29 @agent_ppo2.py:186][0m |           0.0048 |           3.6778 |           4.7720 |
[32m[20230113 20:00:29 @agent_ppo2.py:186][0m |          -0.0084 |           3.1895 |           4.7717 |
[32m[20230113 20:00:29 @agent_ppo2.py:186][0m |          -0.0056 |           3.0016 |           4.7714 |
[32m[20230113 20:00:29 @agent_ppo2.py:186][0m |          -0.0079 |           2.8343 |           4.7687 |
[32m[20230113 20:00:29 @agent_ppo2.py:186][0m |          -0.0079 |           2.7250 |           4.7698 |
[32m[20230113 20:00:29 @agent_ppo2.py:186][0m |           0.0029 |           2.8789 |           4.7699 |
[32m[20230113 20:00:29 @agent_ppo2.py:186][0m |          -0.0097 |           2.6024 |           4.7664 |
[32m[20230113 20:00:29 @agent_ppo2.py:186][0m |          -0.0109 |           2.4681 |           4.7686 |
[32m[20230113 20:00:29 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:00:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 103.89
[32m[20230113 20:00:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.57
[32m[20230113 20:00:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.25
[32m[20230113 20:00:29 @agent_ppo2.py:144][0m Total time:      15.95 min
[32m[20230113 20:00:29 @agent_ppo2.py:146][0m 1433600 total steps have happened
[32m[20230113 20:00:29 @agent_ppo2.py:122][0m #------------------------ Iteration 700 --------------------------#
[32m[20230113 20:00:30 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:30 @agent_ppo2.py:186][0m |          -0.0013 |           5.1604 |           4.8333 |
[32m[20230113 20:00:30 @agent_ppo2.py:186][0m |          -0.0078 |           4.1635 |           4.8222 |
[32m[20230113 20:00:30 @agent_ppo2.py:186][0m |          -0.0101 |           3.7649 |           4.8180 |
[32m[20230113 20:00:30 @agent_ppo2.py:186][0m |          -0.0110 |           3.5249 |           4.8204 |
[32m[20230113 20:00:30 @agent_ppo2.py:186][0m |          -0.0120 |           3.3444 |           4.8197 |
[32m[20230113 20:00:30 @agent_ppo2.py:186][0m |          -0.0130 |           3.2051 |           4.8150 |
[32m[20230113 20:00:30 @agent_ppo2.py:186][0m |          -0.0137 |           3.1249 |           4.8178 |
[32m[20230113 20:00:30 @agent_ppo2.py:186][0m |          -0.0144 |           3.0162 |           4.8154 |
[32m[20230113 20:00:30 @agent_ppo2.py:186][0m |          -0.0153 |           2.9470 |           4.8160 |
[32m[20230113 20:00:30 @agent_ppo2.py:186][0m |          -0.0154 |           2.8712 |           4.8148 |
[32m[20230113 20:00:30 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.02
[32m[20230113 20:00:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.34
[32m[20230113 20:00:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.99
[32m[20230113 20:00:31 @agent_ppo2.py:144][0m Total time:      15.97 min
[32m[20230113 20:00:31 @agent_ppo2.py:146][0m 1435648 total steps have happened
[32m[20230113 20:00:31 @agent_ppo2.py:122][0m #------------------------ Iteration 701 --------------------------#
[32m[20230113 20:00:31 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:00:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:31 @agent_ppo2.py:186][0m |           0.0001 |           6.0262 |           4.6906 |
[32m[20230113 20:00:31 @agent_ppo2.py:186][0m |          -0.0056 |           5.2685 |           4.6852 |
[32m[20230113 20:00:31 @agent_ppo2.py:186][0m |          -0.0040 |           5.0466 |           4.6856 |
[32m[20230113 20:00:31 @agent_ppo2.py:186][0m |          -0.0108 |           4.7106 |           4.6876 |
[32m[20230113 20:00:31 @agent_ppo2.py:186][0m |          -0.0082 |           4.6173 |           4.6873 |
[32m[20230113 20:00:31 @agent_ppo2.py:186][0m |          -0.0117 |           4.4813 |           4.6895 |
[32m[20230113 20:00:31 @agent_ppo2.py:186][0m |          -0.0113 |           4.3980 |           4.6878 |
[32m[20230113 20:00:31 @agent_ppo2.py:186][0m |          -0.0138 |           4.3501 |           4.6900 |
[32m[20230113 20:00:32 @agent_ppo2.py:186][0m |          -0.0133 |           4.2619 |           4.6914 |
[32m[20230113 20:00:32 @agent_ppo2.py:186][0m |          -0.0105 |           4.2512 |           4.6940 |
[32m[20230113 20:00:32 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.03
[32m[20230113 20:00:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.20
[32m[20230113 20:00:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.77
[32m[20230113 20:00:32 @agent_ppo2.py:144][0m Total time:      15.99 min
[32m[20230113 20:00:32 @agent_ppo2.py:146][0m 1437696 total steps have happened
[32m[20230113 20:00:32 @agent_ppo2.py:122][0m #------------------------ Iteration 702 --------------------------#
[32m[20230113 20:00:32 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:33 @agent_ppo2.py:186][0m |           0.0015 |           5.3807 |           4.8142 |
[32m[20230113 20:00:33 @agent_ppo2.py:186][0m |          -0.0059 |           4.9625 |           4.8134 |
[32m[20230113 20:00:33 @agent_ppo2.py:186][0m |          -0.0212 |           4.8432 |           4.8081 |
[32m[20230113 20:00:33 @agent_ppo2.py:186][0m |          -0.0362 |           4.6879 |           4.8076 |
[32m[20230113 20:00:33 @agent_ppo2.py:186][0m |          -0.0299 |           4.5723 |           4.8102 |
[32m[20230113 20:00:33 @agent_ppo2.py:186][0m |           0.0015 |           4.5249 |           4.8122 |
[32m[20230113 20:00:33 @agent_ppo2.py:186][0m |          -0.0035 |           4.4683 |           4.8114 |
[32m[20230113 20:00:33 @agent_ppo2.py:186][0m |          -0.0056 |           4.3742 |           4.8107 |
[32m[20230113 20:00:33 @agent_ppo2.py:186][0m |          -0.0143 |           4.3412 |           4.8097 |
[32m[20230113 20:00:33 @agent_ppo2.py:186][0m |          -0.0196 |           4.3253 |           4.8140 |
[32m[20230113 20:00:33 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.58
[32m[20230113 20:00:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.83
[32m[20230113 20:00:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.05
[32m[20230113 20:00:33 @agent_ppo2.py:144][0m Total time:      16.01 min
[32m[20230113 20:00:33 @agent_ppo2.py:146][0m 1439744 total steps have happened
[32m[20230113 20:00:33 @agent_ppo2.py:122][0m #------------------------ Iteration 703 --------------------------#
[32m[20230113 20:00:34 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:34 @agent_ppo2.py:186][0m |          -0.0020 |           5.2731 |           4.8341 |
[32m[20230113 20:00:34 @agent_ppo2.py:186][0m |          -0.0032 |           4.6457 |           4.8322 |
[32m[20230113 20:00:34 @agent_ppo2.py:186][0m |          -0.0058 |           4.3026 |           4.8301 |
[32m[20230113 20:00:34 @agent_ppo2.py:186][0m |          -0.0040 |           4.3705 |           4.8227 |
[32m[20230113 20:00:34 @agent_ppo2.py:186][0m |          -0.0056 |           4.0377 |           4.8230 |
[32m[20230113 20:00:34 @agent_ppo2.py:186][0m |          -0.0096 |           3.8593 |           4.8238 |
[32m[20230113 20:00:34 @agent_ppo2.py:186][0m |          -0.0116 |           3.7758 |           4.8219 |
[32m[20230113 20:00:34 @agent_ppo2.py:186][0m |          -0.0097 |           3.6969 |           4.8251 |
[32m[20230113 20:00:34 @agent_ppo2.py:186][0m |          -0.0130 |           3.5960 |           4.8246 |
[32m[20230113 20:00:34 @agent_ppo2.py:186][0m |          -0.0088 |           3.5051 |           4.8240 |
[32m[20230113 20:00:34 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.25
[32m[20230113 20:00:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.70
[32m[20230113 20:00:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.18
[32m[20230113 20:00:35 @agent_ppo2.py:144][0m Total time:      16.04 min
[32m[20230113 20:00:35 @agent_ppo2.py:146][0m 1441792 total steps have happened
[32m[20230113 20:00:35 @agent_ppo2.py:122][0m #------------------------ Iteration 704 --------------------------#
[32m[20230113 20:00:35 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:00:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:35 @agent_ppo2.py:186][0m |           0.0013 |           4.2507 |           4.8713 |
[32m[20230113 20:00:35 @agent_ppo2.py:186][0m |          -0.0038 |           3.5457 |           4.8544 |
[32m[20230113 20:00:35 @agent_ppo2.py:186][0m |          -0.0063 |           3.3148 |           4.8542 |
[32m[20230113 20:00:35 @agent_ppo2.py:186][0m |          -0.0079 |           3.1403 |           4.8563 |
[32m[20230113 20:00:35 @agent_ppo2.py:186][0m |          -0.0093 |           3.0419 |           4.8568 |
[32m[20230113 20:00:35 @agent_ppo2.py:186][0m |          -0.0096 |           2.9641 |           4.8569 |
[32m[20230113 20:00:36 @agent_ppo2.py:186][0m |          -0.0107 |           2.9196 |           4.8597 |
[32m[20230113 20:00:36 @agent_ppo2.py:186][0m |          -0.0119 |           2.8558 |           4.8592 |
[32m[20230113 20:00:36 @agent_ppo2.py:186][0m |          -0.0124 |           2.7898 |           4.8586 |
[32m[20230113 20:00:36 @agent_ppo2.py:186][0m |          -0.0133 |           2.7635 |           4.8577 |
[32m[20230113 20:00:36 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.91
[32m[20230113 20:00:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.49
[32m[20230113 20:00:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.32
[32m[20230113 20:00:36 @agent_ppo2.py:144][0m Total time:      16.06 min
[32m[20230113 20:00:36 @agent_ppo2.py:146][0m 1443840 total steps have happened
[32m[20230113 20:00:36 @agent_ppo2.py:122][0m #------------------------ Iteration 705 --------------------------#
[32m[20230113 20:00:37 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:00:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:37 @agent_ppo2.py:186][0m |          -0.0032 |           5.3809 |           4.7751 |
[32m[20230113 20:00:37 @agent_ppo2.py:186][0m |          -0.0016 |           4.9900 |           4.7737 |
[32m[20230113 20:00:37 @agent_ppo2.py:186][0m |          -0.0041 |           4.8347 |           4.7690 |
[32m[20230113 20:00:37 @agent_ppo2.py:186][0m |          -0.0009 |           4.7364 |           4.7676 |
[32m[20230113 20:00:37 @agent_ppo2.py:186][0m |          -0.0094 |           4.6753 |           4.7705 |
[32m[20230113 20:00:37 @agent_ppo2.py:186][0m |          -0.0103 |           4.5847 |           4.7698 |
[32m[20230113 20:00:37 @agent_ppo2.py:186][0m |          -0.0041 |           4.4974 |           4.7724 |
[32m[20230113 20:00:37 @agent_ppo2.py:186][0m |          -0.0002 |           4.4342 |           4.7718 |
[32m[20230113 20:00:37 @agent_ppo2.py:186][0m |          -0.0077 |           4.3938 |           4.7720 |
[32m[20230113 20:00:37 @agent_ppo2.py:186][0m |          -0.0087 |           4.3397 |           4.7756 |
[32m[20230113 20:00:37 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:00:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.18
[32m[20230113 20:00:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.39
[32m[20230113 20:00:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.42
[32m[20230113 20:00:37 @agent_ppo2.py:144][0m Total time:      16.08 min
[32m[20230113 20:00:37 @agent_ppo2.py:146][0m 1445888 total steps have happened
[32m[20230113 20:00:37 @agent_ppo2.py:122][0m #------------------------ Iteration 706 --------------------------#
[32m[20230113 20:00:38 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:38 @agent_ppo2.py:186][0m |           0.0003 |           5.8230 |           4.7643 |
[32m[20230113 20:00:38 @agent_ppo2.py:186][0m |           0.0083 |           5.2609 |           4.7548 |
[32m[20230113 20:00:38 @agent_ppo2.py:186][0m |          -0.0031 |           4.7570 |           4.7486 |
[32m[20230113 20:00:38 @agent_ppo2.py:186][0m |          -0.0161 |           4.4554 |           4.7517 |
[32m[20230113 20:00:38 @agent_ppo2.py:186][0m |          -0.0112 |           4.2848 |           4.7497 |
[32m[20230113 20:00:38 @agent_ppo2.py:186][0m |          -0.0096 |           4.1280 |           4.7472 |
[32m[20230113 20:00:38 @agent_ppo2.py:186][0m |          -0.0095 |           4.0024 |           4.7439 |
[32m[20230113 20:00:38 @agent_ppo2.py:186][0m |          -0.0170 |           3.9665 |           4.7414 |
[32m[20230113 20:00:38 @agent_ppo2.py:186][0m |          -0.0114 |           3.8722 |           4.7343 |
[32m[20230113 20:00:38 @agent_ppo2.py:186][0m |          -0.0170 |           3.7479 |           4.7364 |
[32m[20230113 20:00:38 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:00:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.58
[32m[20230113 20:00:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.64
[32m[20230113 20:00:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.07
[32m[20230113 20:00:39 @agent_ppo2.py:144][0m Total time:      16.10 min
[32m[20230113 20:00:39 @agent_ppo2.py:146][0m 1447936 total steps have happened
[32m[20230113 20:00:39 @agent_ppo2.py:122][0m #------------------------ Iteration 707 --------------------------#
[32m[20230113 20:00:39 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:39 @agent_ppo2.py:186][0m |          -0.0012 |           5.2356 |           4.8081 |
[32m[20230113 20:00:39 @agent_ppo2.py:186][0m |          -0.0062 |           4.5485 |           4.7955 |
[32m[20230113 20:00:39 @agent_ppo2.py:186][0m |          -0.0079 |           4.2737 |           4.7944 |
[32m[20230113 20:00:39 @agent_ppo2.py:186][0m |          -0.0093 |           4.1041 |           4.7934 |
[32m[20230113 20:00:39 @agent_ppo2.py:186][0m |          -0.0106 |           3.9173 |           4.7903 |
[32m[20230113 20:00:40 @agent_ppo2.py:186][0m |          -0.0109 |           3.7955 |           4.7925 |
[32m[20230113 20:00:40 @agent_ppo2.py:186][0m |          -0.0122 |           3.7605 |           4.7892 |
[32m[20230113 20:00:40 @agent_ppo2.py:186][0m |          -0.0131 |           3.6817 |           4.7896 |
[32m[20230113 20:00:40 @agent_ppo2.py:186][0m |          -0.0126 |           3.5776 |           4.7901 |
[32m[20230113 20:00:40 @agent_ppo2.py:186][0m |          -0.0134 |           3.5240 |           4.7887 |
[32m[20230113 20:00:40 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.65
[32m[20230113 20:00:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.44
[32m[20230113 20:00:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.95
[32m[20230113 20:00:40 @agent_ppo2.py:144][0m Total time:      16.13 min
[32m[20230113 20:00:40 @agent_ppo2.py:146][0m 1449984 total steps have happened
[32m[20230113 20:00:40 @agent_ppo2.py:122][0m #------------------------ Iteration 708 --------------------------#
[32m[20230113 20:00:41 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:00:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:41 @agent_ppo2.py:186][0m |          -0.0007 |           5.2012 |           4.8596 |
[32m[20230113 20:00:41 @agent_ppo2.py:186][0m |          -0.0054 |           4.0722 |           4.8534 |
[32m[20230113 20:00:41 @agent_ppo2.py:186][0m |          -0.0070 |           3.6186 |           4.8513 |
[32m[20230113 20:00:41 @agent_ppo2.py:186][0m |          -0.0085 |           3.3472 |           4.8484 |
[32m[20230113 20:00:41 @agent_ppo2.py:186][0m |          -0.0094 |           3.1077 |           4.8503 |
[32m[20230113 20:00:41 @agent_ppo2.py:186][0m |          -0.0104 |           2.9610 |           4.8486 |
[32m[20230113 20:00:41 @agent_ppo2.py:186][0m |          -0.0109 |           2.8422 |           4.8480 |
[32m[20230113 20:00:41 @agent_ppo2.py:186][0m |          -0.0117 |           2.7620 |           4.8508 |
[32m[20230113 20:00:41 @agent_ppo2.py:186][0m |          -0.0119 |           2.6964 |           4.8534 |
[32m[20230113 20:00:41 @agent_ppo2.py:186][0m |          -0.0124 |           2.6323 |           4.8518 |
[32m[20230113 20:00:41 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:00:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.20
[32m[20230113 20:00:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.36
[32m[20230113 20:00:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.42
[32m[20230113 20:00:41 @agent_ppo2.py:144][0m Total time:      16.15 min
[32m[20230113 20:00:41 @agent_ppo2.py:146][0m 1452032 total steps have happened
[32m[20230113 20:00:41 @agent_ppo2.py:122][0m #------------------------ Iteration 709 --------------------------#
[32m[20230113 20:00:42 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:00:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:42 @agent_ppo2.py:186][0m |          -0.0008 |           5.3668 |           4.8272 |
[32m[20230113 20:00:42 @agent_ppo2.py:186][0m |          -0.0062 |           4.7163 |           4.8229 |
[32m[20230113 20:00:42 @agent_ppo2.py:186][0m |          -0.0084 |           4.5072 |           4.8146 |
[32m[20230113 20:00:42 @agent_ppo2.py:186][0m |          -0.0091 |           4.3651 |           4.8190 |
[32m[20230113 20:00:42 @agent_ppo2.py:186][0m |          -0.0099 |           4.2203 |           4.8155 |
[32m[20230113 20:00:42 @agent_ppo2.py:186][0m |          -0.0114 |           4.1020 |           4.8183 |
[32m[20230113 20:00:42 @agent_ppo2.py:186][0m |          -0.0102 |           4.0261 |           4.8163 |
[32m[20230113 20:00:42 @agent_ppo2.py:186][0m |          -0.0117 |           3.9363 |           4.8149 |
[32m[20230113 20:00:42 @agent_ppo2.py:186][0m |          -0.0117 |           3.9247 |           4.8166 |
[32m[20230113 20:00:42 @agent_ppo2.py:186][0m |          -0.0114 |           3.8043 |           4.8163 |
[32m[20230113 20:00:42 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 216.34
[32m[20230113 20:00:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.79
[32m[20230113 20:00:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.87
[32m[20230113 20:00:43 @agent_ppo2.py:144][0m Total time:      16.17 min
[32m[20230113 20:00:43 @agent_ppo2.py:146][0m 1454080 total steps have happened
[32m[20230113 20:00:43 @agent_ppo2.py:122][0m #------------------------ Iteration 710 --------------------------#
[32m[20230113 20:00:43 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:00:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:43 @agent_ppo2.py:186][0m |          -0.0004 |           6.1101 |           4.9769 |
[32m[20230113 20:00:43 @agent_ppo2.py:186][0m |          -0.0058 |           5.4480 |           4.9673 |
[32m[20230113 20:00:43 @agent_ppo2.py:186][0m |          -0.0095 |           4.9405 |           4.9611 |
[32m[20230113 20:00:43 @agent_ppo2.py:186][0m |          -0.0111 |           4.6628 |           4.9588 |
[32m[20230113 20:00:43 @agent_ppo2.py:186][0m |          -0.0106 |           4.4944 |           4.9609 |
[32m[20230113 20:00:43 @agent_ppo2.py:186][0m |          -0.0116 |           4.1772 |           4.9535 |
[32m[20230113 20:00:44 @agent_ppo2.py:186][0m |          -0.0125 |           4.0571 |           4.9552 |
[32m[20230113 20:00:44 @agent_ppo2.py:186][0m |          -0.0132 |           3.9836 |           4.9510 |
[32m[20230113 20:00:44 @agent_ppo2.py:186][0m |          -0.0161 |           3.8727 |           4.9514 |
[32m[20230113 20:00:44 @agent_ppo2.py:186][0m |          -0.0150 |           3.7354 |           4.9496 |
[32m[20230113 20:00:44 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:00:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.33
[32m[20230113 20:00:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.63
[32m[20230113 20:00:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.74
[32m[20230113 20:00:44 @agent_ppo2.py:144][0m Total time:      16.19 min
[32m[20230113 20:00:44 @agent_ppo2.py:146][0m 1456128 total steps have happened
[32m[20230113 20:00:44 @agent_ppo2.py:122][0m #------------------------ Iteration 711 --------------------------#
[32m[20230113 20:00:45 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:00:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:45 @agent_ppo2.py:186][0m |          -0.0004 |           5.8663 |           4.7958 |
[32m[20230113 20:00:45 @agent_ppo2.py:186][0m |          -0.0032 |           4.5422 |           4.7905 |
[32m[20230113 20:00:45 @agent_ppo2.py:186][0m |          -0.0064 |           4.1639 |           4.7818 |
[32m[20230113 20:00:45 @agent_ppo2.py:186][0m |          -0.0103 |           3.8577 |           4.7773 |
[32m[20230113 20:00:45 @agent_ppo2.py:186][0m |          -0.0122 |           3.6983 |           4.7773 |
[32m[20230113 20:00:45 @agent_ppo2.py:186][0m |          -0.0116 |           3.6043 |           4.7735 |
[32m[20230113 20:00:45 @agent_ppo2.py:186][0m |          -0.0110 |           3.4089 |           4.7699 |
[32m[20230113 20:00:45 @agent_ppo2.py:186][0m |          -0.0145 |           3.3022 |           4.7684 |
[32m[20230113 20:00:45 @agent_ppo2.py:186][0m |          -0.0157 |           3.2307 |           4.7654 |
[32m[20230113 20:00:45 @agent_ppo2.py:186][0m |          -0.0132 |           3.1469 |           4.7689 |
[32m[20230113 20:00:45 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:00:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.52
[32m[20230113 20:00:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.45
[32m[20230113 20:00:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.27
[32m[20230113 20:00:45 @agent_ppo2.py:144][0m Total time:      16.21 min
[32m[20230113 20:00:45 @agent_ppo2.py:146][0m 1458176 total steps have happened
[32m[20230113 20:00:45 @agent_ppo2.py:122][0m #------------------------ Iteration 712 --------------------------#
[32m[20230113 20:00:46 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:00:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:46 @agent_ppo2.py:186][0m |           0.0017 |           6.1028 |           4.8870 |
[32m[20230113 20:00:46 @agent_ppo2.py:186][0m |          -0.0068 |           5.3924 |           4.8772 |
[32m[20230113 20:00:46 @agent_ppo2.py:186][0m |          -0.0078 |           5.1761 |           4.8777 |
[32m[20230113 20:00:46 @agent_ppo2.py:186][0m |          -0.0092 |           5.0112 |           4.8753 |
[32m[20230113 20:00:46 @agent_ppo2.py:186][0m |          -0.0106 |           4.8623 |           4.8795 |
[32m[20230113 20:00:46 @agent_ppo2.py:186][0m |          -0.0068 |           4.8819 |           4.8753 |
[32m[20230113 20:00:46 @agent_ppo2.py:186][0m |          -0.0108 |           4.6978 |           4.8808 |
[32m[20230113 20:00:46 @agent_ppo2.py:186][0m |          -0.0099 |           4.7219 |           4.8790 |
[32m[20230113 20:00:46 @agent_ppo2.py:186][0m |          -0.0127 |           4.5861 |           4.8744 |
[32m[20230113 20:00:46 @agent_ppo2.py:186][0m |          -0.0136 |           4.5272 |           4.8808 |
[32m[20230113 20:00:46 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:00:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.33
[32m[20230113 20:00:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.05
[32m[20230113 20:00:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.20
[32m[20230113 20:00:47 @agent_ppo2.py:144][0m Total time:      16.24 min
[32m[20230113 20:00:47 @agent_ppo2.py:146][0m 1460224 total steps have happened
[32m[20230113 20:00:47 @agent_ppo2.py:122][0m #------------------------ Iteration 713 --------------------------#
[32m[20230113 20:00:47 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:47 @agent_ppo2.py:186][0m |          -0.0186 |           5.2328 |           4.9386 |
[32m[20230113 20:00:47 @agent_ppo2.py:186][0m |          -0.0115 |           4.3888 |           4.9244 |
[32m[20230113 20:00:47 @agent_ppo2.py:186][0m |           0.0022 |           4.0562 |           4.9289 |
[32m[20230113 20:00:47 @agent_ppo2.py:186][0m |          -0.0174 |           3.8110 |           4.9297 |
[32m[20230113 20:00:47 @agent_ppo2.py:186][0m |          -0.0140 |           3.6024 |           4.9228 |
[32m[20230113 20:00:48 @agent_ppo2.py:186][0m |          -0.0197 |           3.4879 |           4.9246 |
[32m[20230113 20:00:48 @agent_ppo2.py:186][0m |          -0.0005 |           3.3494 |           4.9214 |
[32m[20230113 20:00:48 @agent_ppo2.py:186][0m |          -0.0143 |           3.2654 |           4.9206 |
[32m[20230113 20:00:48 @agent_ppo2.py:186][0m |          -0.0118 |           3.1112 |           4.9191 |
[32m[20230113 20:00:48 @agent_ppo2.py:186][0m |          -0.0332 |           3.0658 |           4.9200 |
[32m[20230113 20:00:48 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.10
[32m[20230113 20:00:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.54
[32m[20230113 20:00:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.86
[32m[20230113 20:00:48 @agent_ppo2.py:144][0m Total time:      16.26 min
[32m[20230113 20:00:48 @agent_ppo2.py:146][0m 1462272 total steps have happened
[32m[20230113 20:00:48 @agent_ppo2.py:122][0m #------------------------ Iteration 714 --------------------------#
[32m[20230113 20:00:49 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:00:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:49 @agent_ppo2.py:186][0m |          -0.0023 |           6.0601 |           4.9113 |
[32m[20230113 20:00:49 @agent_ppo2.py:186][0m |          -0.0027 |           4.5863 |           4.9039 |
[32m[20230113 20:00:49 @agent_ppo2.py:186][0m |          -0.0065 |           3.8486 |           4.9078 |
[32m[20230113 20:00:49 @agent_ppo2.py:186][0m |          -0.0065 |           3.4003 |           4.9083 |
[32m[20230113 20:00:49 @agent_ppo2.py:186][0m |          -0.0109 |           3.1584 |           4.9088 |
[32m[20230113 20:00:49 @agent_ppo2.py:186][0m |          -0.0087 |           2.9350 |           4.9070 |
[32m[20230113 20:00:49 @agent_ppo2.py:186][0m |          -0.0115 |           2.7783 |           4.9086 |
[32m[20230113 20:00:49 @agent_ppo2.py:186][0m |          -0.0120 |           2.6641 |           4.9104 |
[32m[20230113 20:00:49 @agent_ppo2.py:186][0m |          -0.0123 |           2.5626 |           4.9139 |
[32m[20230113 20:00:49 @agent_ppo2.py:186][0m |          -0.0123 |           2.4596 |           4.9135 |
[32m[20230113 20:00:49 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:00:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.69
[32m[20230113 20:00:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.11
[32m[20230113 20:00:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.63
[32m[20230113 20:00:49 @agent_ppo2.py:144][0m Total time:      16.28 min
[32m[20230113 20:00:49 @agent_ppo2.py:146][0m 1464320 total steps have happened
[32m[20230113 20:00:49 @agent_ppo2.py:122][0m #------------------------ Iteration 715 --------------------------#
[32m[20230113 20:00:50 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:50 @agent_ppo2.py:186][0m |           0.0035 |           5.1105 |           4.8144 |
[32m[20230113 20:00:50 @agent_ppo2.py:186][0m |          -0.0042 |           3.8942 |           4.8098 |
[32m[20230113 20:00:50 @agent_ppo2.py:186][0m |          -0.0111 |           3.4045 |           4.8126 |
[32m[20230113 20:00:50 @agent_ppo2.py:186][0m |          -0.0107 |           3.1854 |           4.8119 |
[32m[20230113 20:00:50 @agent_ppo2.py:186][0m |          -0.0129 |           2.9452 |           4.8137 |
[32m[20230113 20:00:50 @agent_ppo2.py:186][0m |          -0.0061 |           2.7272 |           4.8163 |
[32m[20230113 20:00:50 @agent_ppo2.py:186][0m |          -0.0184 |           2.6274 |           4.8140 |
[32m[20230113 20:00:50 @agent_ppo2.py:186][0m |          -0.0126 |           2.6153 |           4.8165 |
[32m[20230113 20:00:50 @agent_ppo2.py:186][0m |          -0.0245 |           2.4926 |           4.8116 |
[32m[20230113 20:00:50 @agent_ppo2.py:186][0m |          -0.0282 |           2.4848 |           4.8128 |
[32m[20230113 20:00:50 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.81
[32m[20230113 20:00:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.26
[32m[20230113 20:00:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.44
[32m[20230113 20:00:51 @agent_ppo2.py:144][0m Total time:      16.30 min
[32m[20230113 20:00:51 @agent_ppo2.py:146][0m 1466368 total steps have happened
[32m[20230113 20:00:51 @agent_ppo2.py:122][0m #------------------------ Iteration 716 --------------------------#
[32m[20230113 20:00:51 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:00:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:51 @agent_ppo2.py:186][0m |          -0.0046 |           5.5409 |           4.9292 |
[32m[20230113 20:00:51 @agent_ppo2.py:186][0m |          -0.0003 |           4.9299 |           4.9326 |
[32m[20230113 20:00:51 @agent_ppo2.py:186][0m |          -0.0045 |           4.6690 |           4.9344 |
[32m[20230113 20:00:51 @agent_ppo2.py:186][0m |          -0.0002 |           4.5091 |           4.9307 |
[32m[20230113 20:00:52 @agent_ppo2.py:186][0m |          -0.0053 |           4.3695 |           4.9349 |
[32m[20230113 20:00:52 @agent_ppo2.py:186][0m |          -0.0082 |           4.1397 |           4.9258 |
[32m[20230113 20:00:52 @agent_ppo2.py:186][0m |          -0.0119 |           4.0850 |           4.9337 |
[32m[20230113 20:00:52 @agent_ppo2.py:186][0m |          -0.0079 |           3.9492 |           4.9328 |
[32m[20230113 20:00:52 @agent_ppo2.py:186][0m |          -0.0079 |           3.8936 |           4.9248 |
[32m[20230113 20:00:52 @agent_ppo2.py:186][0m |          -0.0157 |           3.8209 |           4.9347 |
[32m[20230113 20:00:52 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.90
[32m[20230113 20:00:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.41
[32m[20230113 20:00:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.49
[32m[20230113 20:00:52 @agent_ppo2.py:144][0m Total time:      16.33 min
[32m[20230113 20:00:52 @agent_ppo2.py:146][0m 1468416 total steps have happened
[32m[20230113 20:00:52 @agent_ppo2.py:122][0m #------------------------ Iteration 717 --------------------------#
[32m[20230113 20:00:53 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:00:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:53 @agent_ppo2.py:186][0m |          -0.0169 |           4.2499 |           4.9524 |
[32m[20230113 20:00:53 @agent_ppo2.py:186][0m |          -0.0232 |           3.6513 |           4.9556 |
[32m[20230113 20:00:53 @agent_ppo2.py:186][0m |           0.0258 |           3.6717 |           4.9522 |
[32m[20230113 20:00:53 @agent_ppo2.py:186][0m |          -0.0072 |           3.4265 |           4.9473 |
[32m[20230113 20:00:53 @agent_ppo2.py:186][0m |          -0.0010 |           3.0768 |           4.9471 |
[32m[20230113 20:00:53 @agent_ppo2.py:186][0m |          -0.0023 |           2.9540 |           4.9463 |
[32m[20230113 20:00:53 @agent_ppo2.py:186][0m |          -0.0668 |           3.0671 |           4.9458 |
[32m[20230113 20:00:53 @agent_ppo2.py:186][0m |          -0.0023 |           2.9481 |           4.9385 |
[32m[20230113 20:00:53 @agent_ppo2.py:186][0m |          -0.0129 |           2.7130 |           4.9453 |
[32m[20230113 20:00:53 @agent_ppo2.py:186][0m |          -0.0113 |           2.6650 |           4.9472 |
[32m[20230113 20:00:53 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:00:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.91
[32m[20230113 20:00:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.11
[32m[20230113 20:00:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.12
[32m[20230113 20:00:54 @agent_ppo2.py:144][0m Total time:      16.35 min
[32m[20230113 20:00:54 @agent_ppo2.py:146][0m 1470464 total steps have happened
[32m[20230113 20:00:54 @agent_ppo2.py:122][0m #------------------------ Iteration 718 --------------------------#
[32m[20230113 20:00:54 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:00:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:54 @agent_ppo2.py:186][0m |           0.0021 |           5.8820 |           4.9493 |
[32m[20230113 20:00:54 @agent_ppo2.py:186][0m |          -0.0056 |           4.2882 |           4.9452 |
[32m[20230113 20:00:54 @agent_ppo2.py:186][0m |          -0.0061 |           3.8622 |           4.9430 |
[32m[20230113 20:00:54 @agent_ppo2.py:186][0m |          -0.0100 |           3.5697 |           4.9369 |
[32m[20230113 20:00:54 @agent_ppo2.py:186][0m |          -0.0107 |           3.4062 |           4.9398 |
[32m[20230113 20:00:54 @agent_ppo2.py:186][0m |          -0.0095 |           3.3078 |           4.9368 |
[32m[20230113 20:00:54 @agent_ppo2.py:186][0m |          -0.0096 |           3.1872 |           4.9367 |
[32m[20230113 20:00:54 @agent_ppo2.py:186][0m |          -0.0103 |           3.1337 |           4.9364 |
[32m[20230113 20:00:54 @agent_ppo2.py:186][0m |          -0.0118 |           3.0313 |           4.9367 |
[32m[20230113 20:00:54 @agent_ppo2.py:186][0m |          -0.0107 |           2.9507 |           4.9379 |
[32m[20230113 20:00:54 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:00:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.71
[32m[20230113 20:00:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.58
[32m[20230113 20:00:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.56
[32m[20230113 20:00:55 @agent_ppo2.py:144][0m Total time:      16.37 min
[32m[20230113 20:00:55 @agent_ppo2.py:146][0m 1472512 total steps have happened
[32m[20230113 20:00:55 @agent_ppo2.py:122][0m #------------------------ Iteration 719 --------------------------#
[32m[20230113 20:00:55 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:00:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:55 @agent_ppo2.py:186][0m |          -0.0044 |           5.6793 |           5.0078 |
[32m[20230113 20:00:55 @agent_ppo2.py:186][0m |          -0.0054 |           4.7859 |           5.0067 |
[32m[20230113 20:00:55 @agent_ppo2.py:186][0m |          -0.0085 |           4.4625 |           5.0087 |
[32m[20230113 20:00:56 @agent_ppo2.py:186][0m |          -0.0135 |           4.3039 |           5.0052 |
[32m[20230113 20:00:56 @agent_ppo2.py:186][0m |          -0.0107 |           4.1969 |           5.0109 |
[32m[20230113 20:00:56 @agent_ppo2.py:186][0m |          -0.0101 |           4.0195 |           5.0082 |
[32m[20230113 20:00:56 @agent_ppo2.py:186][0m |          -0.0173 |           3.9298 |           5.0095 |
[32m[20230113 20:00:56 @agent_ppo2.py:186][0m |          -0.0104 |           4.0969 |           5.0102 |
[32m[20230113 20:00:56 @agent_ppo2.py:186][0m |          -0.0150 |           3.7967 |           5.0085 |
[32m[20230113 20:00:56 @agent_ppo2.py:186][0m |          -0.0087 |           4.1703 |           5.0103 |
[32m[20230113 20:00:56 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:00:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.92
[32m[20230113 20:00:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.31
[32m[20230113 20:00:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.01
[32m[20230113 20:00:56 @agent_ppo2.py:144][0m Total time:      16.39 min
[32m[20230113 20:00:56 @agent_ppo2.py:146][0m 1474560 total steps have happened
[32m[20230113 20:00:56 @agent_ppo2.py:122][0m #------------------------ Iteration 720 --------------------------#
[32m[20230113 20:00:57 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:00:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:57 @agent_ppo2.py:186][0m |          -0.0056 |           5.7918 |           5.0405 |
[32m[20230113 20:00:57 @agent_ppo2.py:186][0m |          -0.0037 |           5.1086 |           5.0396 |
[32m[20230113 20:00:57 @agent_ppo2.py:186][0m |           0.0036 |           5.6827 |           5.0343 |
[32m[20230113 20:00:57 @agent_ppo2.py:186][0m |          -0.0003 |           4.8974 |           5.0319 |
[32m[20230113 20:00:57 @agent_ppo2.py:186][0m |          -0.0016 |           4.5852 |           5.0338 |
[32m[20230113 20:00:57 @agent_ppo2.py:186][0m |          -0.0092 |           4.4374 |           5.0339 |
[32m[20230113 20:00:57 @agent_ppo2.py:186][0m |          -0.0154 |           4.3431 |           5.0288 |
[32m[20230113 20:00:57 @agent_ppo2.py:186][0m |           0.0002 |           4.2895 |           5.0316 |
[32m[20230113 20:00:57 @agent_ppo2.py:186][0m |          -0.0024 |           4.2702 |           5.0361 |
[32m[20230113 20:00:57 @agent_ppo2.py:186][0m |          -0.0119 |           4.1552 |           5.0282 |
[32m[20230113 20:00:57 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:00:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.56
[32m[20230113 20:00:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.90
[32m[20230113 20:00:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.79
[32m[20230113 20:00:58 @agent_ppo2.py:144][0m Total time:      16.42 min
[32m[20230113 20:00:58 @agent_ppo2.py:146][0m 1476608 total steps have happened
[32m[20230113 20:00:58 @agent_ppo2.py:122][0m #------------------------ Iteration 721 --------------------------#
[32m[20230113 20:00:58 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:00:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:58 @agent_ppo2.py:186][0m |          -0.0003 |           5.2622 |           5.1513 |
[32m[20230113 20:00:58 @agent_ppo2.py:186][0m |          -0.0027 |           4.7943 |           5.1386 |
[32m[20230113 20:00:58 @agent_ppo2.py:186][0m |          -0.0049 |           4.4829 |           5.1419 |
[32m[20230113 20:00:58 @agent_ppo2.py:186][0m |          -0.0089 |           4.2880 |           5.1406 |
[32m[20230113 20:00:58 @agent_ppo2.py:186][0m |          -0.0094 |           4.1627 |           5.1429 |
[32m[20230113 20:00:58 @agent_ppo2.py:186][0m |          -0.0105 |           4.0655 |           5.1321 |
[32m[20230113 20:00:58 @agent_ppo2.py:186][0m |          -0.0099 |           3.9411 |           5.1324 |
[32m[20230113 20:00:58 @agent_ppo2.py:186][0m |          -0.0110 |           3.8811 |           5.1249 |
[32m[20230113 20:00:58 @agent_ppo2.py:186][0m |          -0.0103 |           3.7705 |           5.1261 |
[32m[20230113 20:00:58 @agent_ppo2.py:186][0m |          -0.0134 |           3.7476 |           5.1227 |
[32m[20230113 20:00:58 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:00:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.28
[32m[20230113 20:00:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.48
[32m[20230113 20:00:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.61
[32m[20230113 20:00:59 @agent_ppo2.py:144][0m Total time:      16.44 min
[32m[20230113 20:00:59 @agent_ppo2.py:146][0m 1478656 total steps have happened
[32m[20230113 20:00:59 @agent_ppo2.py:122][0m #------------------------ Iteration 722 --------------------------#
[32m[20230113 20:00:59 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:00:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:00:59 @agent_ppo2.py:186][0m |          -0.0033 |           9.9408 |           5.0331 |
[32m[20230113 20:00:59 @agent_ppo2.py:186][0m |          -0.0099 |           4.2548 |           5.0249 |
[32m[20230113 20:00:59 @agent_ppo2.py:186][0m |          -0.0133 |           3.7271 |           5.0248 |
[32m[20230113 20:01:00 @agent_ppo2.py:186][0m |          -0.0127 |           3.4526 |           5.0249 |
[32m[20230113 20:01:00 @agent_ppo2.py:186][0m |          -0.0156 |           3.1950 |           5.0260 |
[32m[20230113 20:01:00 @agent_ppo2.py:186][0m |          -0.0135 |           2.9939 |           5.0244 |
[32m[20230113 20:01:00 @agent_ppo2.py:186][0m |          -0.0168 |           2.8834 |           5.0294 |
[32m[20230113 20:01:00 @agent_ppo2.py:186][0m |          -0.0167 |           2.7302 |           5.0286 |
[32m[20230113 20:01:00 @agent_ppo2.py:186][0m |          -0.0197 |           2.6282 |           5.0280 |
[32m[20230113 20:01:00 @agent_ppo2.py:186][0m |          -0.0195 |           2.5325 |           5.0299 |
[32m[20230113 20:01:00 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:01:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 154.73
[32m[20230113 20:01:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.76
[32m[20230113 20:01:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.17
[32m[20230113 20:01:00 @agent_ppo2.py:144][0m Total time:      16.46 min
[32m[20230113 20:01:00 @agent_ppo2.py:146][0m 1480704 total steps have happened
[32m[20230113 20:01:00 @agent_ppo2.py:122][0m #------------------------ Iteration 723 --------------------------#
[32m[20230113 20:01:01 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:01:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:01 @agent_ppo2.py:186][0m |           0.0013 |           5.1355 |           5.0320 |
[32m[20230113 20:01:01 @agent_ppo2.py:186][0m |          -0.0009 |           4.4964 |           5.0310 |
[32m[20230113 20:01:01 @agent_ppo2.py:186][0m |          -0.0049 |           4.3126 |           5.0267 |
[32m[20230113 20:01:01 @agent_ppo2.py:186][0m |          -0.0055 |           4.1993 |           5.0233 |
[32m[20230113 20:01:01 @agent_ppo2.py:186][0m |          -0.0082 |           4.0637 |           5.0214 |
[32m[20230113 20:01:01 @agent_ppo2.py:186][0m |          -0.0091 |           3.9327 |           5.0203 |
[32m[20230113 20:01:01 @agent_ppo2.py:186][0m |          -0.0085 |           3.7950 |           5.0170 |
[32m[20230113 20:01:01 @agent_ppo2.py:186][0m |          -0.0098 |           3.7193 |           5.0182 |
[32m[20230113 20:01:01 @agent_ppo2.py:186][0m |          -0.0106 |           3.6338 |           5.0158 |
[32m[20230113 20:01:01 @agent_ppo2.py:186][0m |          -0.0108 |           3.5758 |           5.0102 |
[32m[20230113 20:01:01 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:01:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.57
[32m[20230113 20:01:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.90
[32m[20230113 20:01:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.57
[32m[20230113 20:01:02 @agent_ppo2.py:144][0m Total time:      16.48 min
[32m[20230113 20:01:02 @agent_ppo2.py:146][0m 1482752 total steps have happened
[32m[20230113 20:01:02 @agent_ppo2.py:122][0m #------------------------ Iteration 724 --------------------------#
[32m[20230113 20:01:02 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:02 @agent_ppo2.py:186][0m |           0.0052 |           5.7878 |           4.9357 |
[32m[20230113 20:01:02 @agent_ppo2.py:186][0m |          -0.0047 |           5.1557 |           4.9180 |
[32m[20230113 20:01:02 @agent_ppo2.py:186][0m |          -0.0093 |           4.9079 |           4.9166 |
[32m[20230113 20:01:02 @agent_ppo2.py:186][0m |          -0.0086 |           4.7034 |           4.9109 |
[32m[20230113 20:01:02 @agent_ppo2.py:186][0m |          -0.0141 |           4.6700 |           4.9139 |
[32m[20230113 20:01:02 @agent_ppo2.py:186][0m |          -0.0145 |           4.5697 |           4.9085 |
[32m[20230113 20:01:02 @agent_ppo2.py:186][0m |          -0.0122 |           4.4765 |           4.9065 |
[32m[20230113 20:01:02 @agent_ppo2.py:186][0m |          -0.0099 |           4.4707 |           4.9099 |
[32m[20230113 20:01:02 @agent_ppo2.py:186][0m |          -0.0122 |           4.3950 |           4.9044 |
[32m[20230113 20:01:02 @agent_ppo2.py:186][0m |          -0.0198 |           4.3173 |           4.9012 |
[32m[20230113 20:01:02 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:01:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.65
[32m[20230113 20:01:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.93
[32m[20230113 20:01:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.24
[32m[20230113 20:01:03 @agent_ppo2.py:144][0m Total time:      16.51 min
[32m[20230113 20:01:03 @agent_ppo2.py:146][0m 1484800 total steps have happened
[32m[20230113 20:01:03 @agent_ppo2.py:122][0m #------------------------ Iteration 725 --------------------------#
[32m[20230113 20:01:03 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:03 @agent_ppo2.py:186][0m |          -0.0030 |           5.5237 |           4.9385 |
[32m[20230113 20:01:03 @agent_ppo2.py:186][0m |          -0.0079 |           5.0752 |           4.9287 |
[32m[20230113 20:01:04 @agent_ppo2.py:186][0m |          -0.0005 |           4.9452 |           4.9250 |
[32m[20230113 20:01:04 @agent_ppo2.py:186][0m |          -0.0067 |           4.8703 |           4.9204 |
[32m[20230113 20:01:04 @agent_ppo2.py:186][0m |          -0.0081 |           4.6254 |           4.9216 |
[32m[20230113 20:01:04 @agent_ppo2.py:186][0m |          -0.0086 |           4.5355 |           4.9196 |
[32m[20230113 20:01:04 @agent_ppo2.py:186][0m |          -0.0112 |           4.4594 |           4.9195 |
[32m[20230113 20:01:04 @agent_ppo2.py:186][0m |          -0.0120 |           4.3732 |           4.9164 |
[32m[20230113 20:01:04 @agent_ppo2.py:186][0m |          -0.0139 |           4.3360 |           4.9144 |
[32m[20230113 20:01:04 @agent_ppo2.py:186][0m |          -0.0131 |           4.2203 |           4.9169 |
[32m[20230113 20:01:04 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.97
[32m[20230113 20:01:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.81
[32m[20230113 20:01:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.97
[32m[20230113 20:01:04 @agent_ppo2.py:144][0m Total time:      16.53 min
[32m[20230113 20:01:04 @agent_ppo2.py:146][0m 1486848 total steps have happened
[32m[20230113 20:01:04 @agent_ppo2.py:122][0m #------------------------ Iteration 726 --------------------------#
[32m[20230113 20:01:05 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:05 @agent_ppo2.py:186][0m |           0.0007 |           5.9249 |           4.8960 |
[32m[20230113 20:01:05 @agent_ppo2.py:186][0m |          -0.0048 |           5.1411 |           4.8906 |
[32m[20230113 20:01:05 @agent_ppo2.py:186][0m |          -0.0075 |           4.7999 |           4.8950 |
[32m[20230113 20:01:05 @agent_ppo2.py:186][0m |          -0.0114 |           4.5607 |           4.8907 |
[32m[20230113 20:01:05 @agent_ppo2.py:186][0m |          -0.0163 |           4.4257 |           4.8916 |
[32m[20230113 20:01:05 @agent_ppo2.py:186][0m |          -0.0101 |           4.3047 |           4.8986 |
[32m[20230113 20:01:05 @agent_ppo2.py:186][0m |          -0.0108 |           4.2443 |           4.8950 |
[32m[20230113 20:01:05 @agent_ppo2.py:186][0m |          -0.0111 |           4.1575 |           4.9004 |
[32m[20230113 20:01:05 @agent_ppo2.py:186][0m |          -0.0131 |           3.9999 |           4.9027 |
[32m[20230113 20:01:05 @agent_ppo2.py:186][0m |          -0.0104 |           3.9886 |           4.9010 |
[32m[20230113 20:01:05 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.55
[32m[20230113 20:01:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.03
[32m[20230113 20:01:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.77
[32m[20230113 20:01:06 @agent_ppo2.py:144][0m Total time:      16.55 min
[32m[20230113 20:01:06 @agent_ppo2.py:146][0m 1488896 total steps have happened
[32m[20230113 20:01:06 @agent_ppo2.py:122][0m #------------------------ Iteration 727 --------------------------#
[32m[20230113 20:01:06 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:06 @agent_ppo2.py:186][0m |           0.0022 |           6.7644 |           5.0534 |
[32m[20230113 20:01:06 @agent_ppo2.py:186][0m |          -0.0053 |           5.6728 |           5.0458 |
[32m[20230113 20:01:06 @agent_ppo2.py:186][0m |          -0.0075 |           5.2739 |           5.0434 |
[32m[20230113 20:01:06 @agent_ppo2.py:186][0m |          -0.0081 |           5.1336 |           5.0489 |
[32m[20230113 20:01:06 @agent_ppo2.py:186][0m |          -0.0081 |           5.0197 |           5.0454 |
[32m[20230113 20:01:06 @agent_ppo2.py:186][0m |          -0.0064 |           5.0710 |           5.0496 |
[32m[20230113 20:01:06 @agent_ppo2.py:186][0m |          -0.0105 |           4.7754 |           5.0419 |
[32m[20230113 20:01:06 @agent_ppo2.py:186][0m |          -0.0111 |           4.6974 |           5.0450 |
[32m[20230113 20:01:06 @agent_ppo2.py:186][0m |          -0.0112 |           4.6836 |           5.0425 |
[32m[20230113 20:01:06 @agent_ppo2.py:186][0m |          -0.0110 |           4.6259 |           5.0490 |
[32m[20230113 20:01:06 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:01:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.69
[32m[20230113 20:01:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.00
[32m[20230113 20:01:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.93
[32m[20230113 20:01:07 @agent_ppo2.py:144][0m Total time:      16.57 min
[32m[20230113 20:01:07 @agent_ppo2.py:146][0m 1490944 total steps have happened
[32m[20230113 20:01:07 @agent_ppo2.py:122][0m #------------------------ Iteration 728 --------------------------#
[32m[20230113 20:01:07 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:01:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:07 @agent_ppo2.py:186][0m |          -0.0079 |           6.2535 |           4.8888 |
[32m[20230113 20:01:07 @agent_ppo2.py:186][0m |          -0.0188 |           4.6639 |           4.8843 |
[32m[20230113 20:01:07 @agent_ppo2.py:186][0m |           0.0063 |           3.9082 |           4.8831 |
[32m[20230113 20:01:08 @agent_ppo2.py:186][0m |          -0.0149 |           3.4151 |           4.8793 |
[32m[20230113 20:01:08 @agent_ppo2.py:186][0m |          -0.0088 |           3.0948 |           4.8866 |
[32m[20230113 20:01:08 @agent_ppo2.py:186][0m |          -0.0148 |           2.8962 |           4.8824 |
[32m[20230113 20:01:08 @agent_ppo2.py:186][0m |          -0.0015 |           2.7255 |           4.8794 |
[32m[20230113 20:01:08 @agent_ppo2.py:186][0m |          -0.0158 |           2.6213 |           4.8839 |
[32m[20230113 20:01:08 @agent_ppo2.py:186][0m |          -0.0036 |           2.4665 |           4.8834 |
[32m[20230113 20:01:08 @agent_ppo2.py:186][0m |          -0.0109 |           2.3583 |           4.8865 |
[32m[20230113 20:01:08 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:01:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.16
[32m[20230113 20:01:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.25
[32m[20230113 20:01:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.41
[32m[20230113 20:01:08 @agent_ppo2.py:144][0m Total time:      16.59 min
[32m[20230113 20:01:08 @agent_ppo2.py:146][0m 1492992 total steps have happened
[32m[20230113 20:01:08 @agent_ppo2.py:122][0m #------------------------ Iteration 729 --------------------------#
[32m[20230113 20:01:09 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:01:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:09 @agent_ppo2.py:186][0m |          -0.0018 |           5.7722 |           5.0340 |
[32m[20230113 20:01:09 @agent_ppo2.py:186][0m |          -0.0023 |           5.1763 |           5.0246 |
[32m[20230113 20:01:09 @agent_ppo2.py:186][0m |          -0.0028 |           4.9630 |           5.0234 |
[32m[20230113 20:01:09 @agent_ppo2.py:186][0m |          -0.0077 |           4.8337 |           5.0262 |
[32m[20230113 20:01:09 @agent_ppo2.py:186][0m |          -0.0101 |           4.7787 |           5.0256 |
[32m[20230113 20:01:09 @agent_ppo2.py:186][0m |          -0.0155 |           4.7231 |           5.0261 |
[32m[20230113 20:01:09 @agent_ppo2.py:186][0m |          -0.0030 |           4.6245 |           5.0265 |
[32m[20230113 20:01:09 @agent_ppo2.py:186][0m |          -0.0099 |           4.5398 |           5.0241 |
[32m[20230113 20:01:09 @agent_ppo2.py:186][0m |           0.0181 |           5.5338 |           5.0216 |
[32m[20230113 20:01:09 @agent_ppo2.py:186][0m |           0.0161 |           6.2586 |           5.0198 |
[32m[20230113 20:01:09 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:01:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.32
[32m[20230113 20:01:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.71
[32m[20230113 20:01:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.94
[32m[20230113 20:01:10 @agent_ppo2.py:144][0m Total time:      16.62 min
[32m[20230113 20:01:10 @agent_ppo2.py:146][0m 1495040 total steps have happened
[32m[20230113 20:01:10 @agent_ppo2.py:122][0m #------------------------ Iteration 730 --------------------------#
[32m[20230113 20:01:10 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:01:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:10 @agent_ppo2.py:186][0m |          -0.0005 |           5.9493 |           5.0314 |
[32m[20230113 20:01:10 @agent_ppo2.py:186][0m |          -0.0074 |           4.8376 |           5.0313 |
[32m[20230113 20:01:10 @agent_ppo2.py:186][0m |          -0.0086 |           4.4431 |           5.0312 |
[32m[20230113 20:01:10 @agent_ppo2.py:186][0m |          -0.0110 |           4.2159 |           5.0346 |
[32m[20230113 20:01:10 @agent_ppo2.py:186][0m |          -0.0125 |           4.0176 |           5.0332 |
[32m[20230113 20:01:10 @agent_ppo2.py:186][0m |          -0.0126 |           3.8885 |           5.0320 |
[32m[20230113 20:01:10 @agent_ppo2.py:186][0m |          -0.0132 |           3.7611 |           5.0303 |
[32m[20230113 20:01:10 @agent_ppo2.py:186][0m |          -0.0145 |           3.6798 |           5.0263 |
[32m[20230113 20:01:10 @agent_ppo2.py:186][0m |          -0.0131 |           3.6147 |           5.0287 |
[32m[20230113 20:01:10 @agent_ppo2.py:186][0m |          -0.0142 |           3.5566 |           5.0239 |
[32m[20230113 20:01:10 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.52
[32m[20230113 20:01:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.52
[32m[20230113 20:01:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.45
[32m[20230113 20:01:11 @agent_ppo2.py:144][0m Total time:      16.64 min
[32m[20230113 20:01:11 @agent_ppo2.py:146][0m 1497088 total steps have happened
[32m[20230113 20:01:11 @agent_ppo2.py:122][0m #------------------------ Iteration 731 --------------------------#
[32m[20230113 20:01:11 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:11 @agent_ppo2.py:186][0m |           0.0002 |           6.1360 |           5.0374 |
[32m[20230113 20:01:11 @agent_ppo2.py:186][0m |          -0.0035 |           5.2440 |           5.0291 |
[32m[20230113 20:01:12 @agent_ppo2.py:186][0m |          -0.0069 |           4.6811 |           5.0334 |
[32m[20230113 20:01:12 @agent_ppo2.py:186][0m |          -0.0080 |           4.3666 |           5.0310 |
[32m[20230113 20:01:12 @agent_ppo2.py:186][0m |          -0.0086 |           4.1799 |           5.0357 |
[32m[20230113 20:01:12 @agent_ppo2.py:186][0m |          -0.0090 |           4.0073 |           5.0343 |
[32m[20230113 20:01:12 @agent_ppo2.py:186][0m |          -0.0089 |           3.8819 |           5.0372 |
[32m[20230113 20:01:12 @agent_ppo2.py:186][0m |          -0.0113 |           3.7648 |           5.0344 |
[32m[20230113 20:01:12 @agent_ppo2.py:186][0m |          -0.0106 |           3.6860 |           5.0367 |
[32m[20230113 20:01:12 @agent_ppo2.py:186][0m |          -0.0086 |           3.6729 |           5.0420 |
[32m[20230113 20:01:12 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:01:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.97
[32m[20230113 20:01:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.39
[32m[20230113 20:01:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.10
[32m[20230113 20:01:12 @agent_ppo2.py:144][0m Total time:      16.66 min
[32m[20230113 20:01:12 @agent_ppo2.py:146][0m 1499136 total steps have happened
[32m[20230113 20:01:12 @agent_ppo2.py:122][0m #------------------------ Iteration 732 --------------------------#
[32m[20230113 20:01:13 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:01:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:13 @agent_ppo2.py:186][0m |          -0.0013 |          15.0345 |           4.9876 |
[32m[20230113 20:01:13 @agent_ppo2.py:186][0m |          -0.0078 |           6.4044 |           4.9814 |
[32m[20230113 20:01:13 @agent_ppo2.py:186][0m |          -0.0085 |           5.6540 |           4.9816 |
[32m[20230113 20:01:13 @agent_ppo2.py:186][0m |          -0.0098 |           5.2033 |           4.9796 |
[32m[20230113 20:01:13 @agent_ppo2.py:186][0m |          -0.0107 |           5.1211 |           4.9791 |
[32m[20230113 20:01:13 @agent_ppo2.py:186][0m |          -0.0120 |           4.8143 |           4.9762 |
[32m[20230113 20:01:13 @agent_ppo2.py:186][0m |          -0.0129 |           4.6749 |           4.9755 |
[32m[20230113 20:01:13 @agent_ppo2.py:186][0m |          -0.0135 |           4.5771 |           4.9689 |
[32m[20230113 20:01:13 @agent_ppo2.py:186][0m |          -0.0137 |           4.5157 |           4.9681 |
[32m[20230113 20:01:13 @agent_ppo2.py:186][0m |          -0.0126 |           4.4107 |           4.9674 |
[32m[20230113 20:01:13 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 111.69
[32m[20230113 20:01:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.11
[32m[20230113 20:01:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.26
[32m[20230113 20:01:14 @agent_ppo2.py:144][0m Total time:      16.68 min
[32m[20230113 20:01:14 @agent_ppo2.py:146][0m 1501184 total steps have happened
[32m[20230113 20:01:14 @agent_ppo2.py:122][0m #------------------------ Iteration 733 --------------------------#
[32m[20230113 20:01:14 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:14 @agent_ppo2.py:186][0m |          -0.0066 |           5.5940 |           4.9717 |
[32m[20230113 20:01:14 @agent_ppo2.py:186][0m |          -0.0039 |           4.7697 |           4.9599 |
[32m[20230113 20:01:14 @agent_ppo2.py:186][0m |          -0.0196 |           4.4849 |           4.9559 |
[32m[20230113 20:01:14 @agent_ppo2.py:186][0m |          -0.0189 |           4.2505 |           4.9516 |
[32m[20230113 20:01:14 @agent_ppo2.py:186][0m |          -0.0079 |           4.0801 |           4.9519 |
[32m[20230113 20:01:14 @agent_ppo2.py:186][0m |          -0.0233 |           3.9677 |           4.9550 |
[32m[20230113 20:01:14 @agent_ppo2.py:186][0m |          -0.0178 |           3.8932 |           4.9515 |
[32m[20230113 20:01:14 @agent_ppo2.py:186][0m |          -0.0213 |           3.8112 |           4.9521 |
[32m[20230113 20:01:14 @agent_ppo2.py:186][0m |          -0.0128 |           3.6826 |           4.9538 |
[32m[20230113 20:01:14 @agent_ppo2.py:186][0m |          -0.0105 |           3.6235 |           4.9587 |
[32m[20230113 20:01:14 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.28
[32m[20230113 20:01:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.74
[32m[20230113 20:01:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.62
[32m[20230113 20:01:15 @agent_ppo2.py:144][0m Total time:      16.71 min
[32m[20230113 20:01:15 @agent_ppo2.py:146][0m 1503232 total steps have happened
[32m[20230113 20:01:15 @agent_ppo2.py:122][0m #------------------------ Iteration 734 --------------------------#
[32m[20230113 20:01:15 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:01:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:15 @agent_ppo2.py:186][0m |          -0.0036 |           5.5935 |           5.0196 |
[32m[20230113 20:01:16 @agent_ppo2.py:186][0m |          -0.0028 |           4.7864 |           5.0151 |
[32m[20230113 20:01:16 @agent_ppo2.py:186][0m |          -0.0088 |           4.5166 |           5.0136 |
[32m[20230113 20:01:16 @agent_ppo2.py:186][0m |          -0.0073 |           4.3404 |           5.0145 |
[32m[20230113 20:01:16 @agent_ppo2.py:186][0m |          -0.0110 |           4.1876 |           5.0127 |
[32m[20230113 20:01:16 @agent_ppo2.py:186][0m |          -0.0166 |           4.0836 |           5.0163 |
[32m[20230113 20:01:16 @agent_ppo2.py:186][0m |          -0.0123 |           3.9776 |           5.0158 |
[32m[20230113 20:01:16 @agent_ppo2.py:186][0m |          -0.0106 |           3.9955 |           5.0130 |
[32m[20230113 20:01:16 @agent_ppo2.py:186][0m |          -0.0227 |           3.8100 |           5.0127 |
[32m[20230113 20:01:16 @agent_ppo2.py:186][0m |          -0.0131 |           3.7498 |           5.0103 |
[32m[20230113 20:01:16 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:01:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.78
[32m[20230113 20:01:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.99
[32m[20230113 20:01:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.29
[32m[20230113 20:01:16 @agent_ppo2.py:144][0m Total time:      16.73 min
[32m[20230113 20:01:16 @agent_ppo2.py:146][0m 1505280 total steps have happened
[32m[20230113 20:01:16 @agent_ppo2.py:122][0m #------------------------ Iteration 735 --------------------------#
[32m[20230113 20:01:17 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:17 @agent_ppo2.py:186][0m |          -0.0000 |           4.6187 |           5.0247 |
[32m[20230113 20:01:17 @agent_ppo2.py:186][0m |          -0.0050 |           4.1842 |           5.0254 |
[32m[20230113 20:01:17 @agent_ppo2.py:186][0m |          -0.0068 |           4.0248 |           5.0244 |
[32m[20230113 20:01:17 @agent_ppo2.py:186][0m |          -0.0054 |           4.0207 |           5.0232 |
[32m[20230113 20:01:17 @agent_ppo2.py:186][0m |          -0.0085 |           3.8082 |           5.0229 |
[32m[20230113 20:01:17 @agent_ppo2.py:186][0m |          -0.0102 |           3.7003 |           5.0202 |
[32m[20230113 20:01:17 @agent_ppo2.py:186][0m |          -0.0108 |           3.6309 |           5.0228 |
[32m[20230113 20:01:17 @agent_ppo2.py:186][0m |          -0.0115 |           3.6267 |           5.0219 |
[32m[20230113 20:01:17 @agent_ppo2.py:186][0m |          -0.0128 |           3.5869 |           5.0201 |
[32m[20230113 20:01:17 @agent_ppo2.py:186][0m |          -0.0123 |           3.5062 |           5.0201 |
[32m[20230113 20:01:17 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:01:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.57
[32m[20230113 20:01:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.89
[32m[20230113 20:01:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.30
[32m[20230113 20:01:18 @agent_ppo2.py:144][0m Total time:      16.75 min
[32m[20230113 20:01:18 @agent_ppo2.py:146][0m 1507328 total steps have happened
[32m[20230113 20:01:18 @agent_ppo2.py:122][0m #------------------------ Iteration 736 --------------------------#
[32m[20230113 20:01:18 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:18 @agent_ppo2.py:186][0m |          -0.0012 |           6.1389 |           4.9726 |
[32m[20230113 20:01:18 @agent_ppo2.py:186][0m |          -0.0089 |           5.0204 |           4.9719 |
[32m[20230113 20:01:18 @agent_ppo2.py:186][0m |          -0.0017 |           4.6721 |           4.9699 |
[32m[20230113 20:01:18 @agent_ppo2.py:186][0m |           0.0027 |           4.7249 |           4.9712 |
[32m[20230113 20:01:18 @agent_ppo2.py:186][0m |          -0.0043 |           4.2487 |           4.9735 |
[32m[20230113 20:01:18 @agent_ppo2.py:186][0m |          -0.0022 |           4.2332 |           4.9749 |
[32m[20230113 20:01:18 @agent_ppo2.py:186][0m |           0.0063 |           4.2848 |           4.9717 |
[32m[20230113 20:01:18 @agent_ppo2.py:186][0m |          -0.0167 |           3.9314 |           4.9666 |
[32m[20230113 20:01:18 @agent_ppo2.py:186][0m |          -0.0122 |           3.8502 |           4.9713 |
[32m[20230113 20:01:18 @agent_ppo2.py:186][0m |          -0.0034 |           3.8163 |           4.9715 |
[32m[20230113 20:01:18 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:01:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.51
[32m[20230113 20:01:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.20
[32m[20230113 20:01:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.39
[32m[20230113 20:01:19 @agent_ppo2.py:144][0m Total time:      16.77 min
[32m[20230113 20:01:19 @agent_ppo2.py:146][0m 1509376 total steps have happened
[32m[20230113 20:01:19 @agent_ppo2.py:122][0m #------------------------ Iteration 737 --------------------------#
[32m[20230113 20:01:19 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:01:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:19 @agent_ppo2.py:186][0m |          -0.0044 |           5.9431 |           4.9559 |
[32m[20230113 20:01:20 @agent_ppo2.py:186][0m |          -0.0070 |           4.7564 |           4.9523 |
[32m[20230113 20:01:20 @agent_ppo2.py:186][0m |          -0.0086 |           4.1872 |           4.9502 |
[32m[20230113 20:01:20 @agent_ppo2.py:186][0m |          -0.0097 |           3.9083 |           4.9474 |
[32m[20230113 20:01:20 @agent_ppo2.py:186][0m |          -0.0071 |           3.6774 |           4.9450 |
[32m[20230113 20:01:20 @agent_ppo2.py:186][0m |          -0.0106 |           3.5071 |           4.9475 |
[32m[20230113 20:01:20 @agent_ppo2.py:186][0m |          -0.0144 |           3.3222 |           4.9468 |
[32m[20230113 20:01:20 @agent_ppo2.py:186][0m |          -0.0154 |           3.2035 |           4.9434 |
[32m[20230113 20:01:20 @agent_ppo2.py:186][0m |          -0.0163 |           3.0959 |           4.9432 |
[32m[20230113 20:01:20 @agent_ppo2.py:186][0m |          -0.0154 |           2.9885 |           4.9444 |
[32m[20230113 20:01:20 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:01:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.72
[32m[20230113 20:01:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.82
[32m[20230113 20:01:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.07
[32m[20230113 20:01:20 @agent_ppo2.py:144][0m Total time:      16.80 min
[32m[20230113 20:01:20 @agent_ppo2.py:146][0m 1511424 total steps have happened
[32m[20230113 20:01:20 @agent_ppo2.py:122][0m #------------------------ Iteration 738 --------------------------#
[32m[20230113 20:01:21 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 20:01:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:21 @agent_ppo2.py:186][0m |          -0.0010 |           5.8723 |           5.0718 |
[32m[20230113 20:01:21 @agent_ppo2.py:186][0m |           0.0018 |           5.3657 |           5.0711 |
[32m[20230113 20:01:21 @agent_ppo2.py:186][0m |          -0.0041 |           4.7810 |           5.0696 |
[32m[20230113 20:01:21 @agent_ppo2.py:186][0m |          -0.0053 |           4.5864 |           5.0742 |
[32m[20230113 20:01:21 @agent_ppo2.py:186][0m |          -0.0085 |           4.4121 |           5.0741 |
[32m[20230113 20:01:21 @agent_ppo2.py:186][0m |          -0.0073 |           4.3162 |           5.0740 |
[32m[20230113 20:01:21 @agent_ppo2.py:186][0m |          -0.0080 |           4.3161 |           5.0764 |
[32m[20230113 20:01:21 @agent_ppo2.py:186][0m |          -0.0090 |           4.1408 |           5.0774 |
[32m[20230113 20:01:21 @agent_ppo2.py:186][0m |          -0.0104 |           4.0778 |           5.0741 |
[32m[20230113 20:01:21 @agent_ppo2.py:186][0m |          -0.0111 |           4.0228 |           5.0794 |
[32m[20230113 20:01:21 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:01:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.95
[32m[20230113 20:01:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.63
[32m[20230113 20:01:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.78
[32m[20230113 20:01:22 @agent_ppo2.py:144][0m Total time:      16.82 min
[32m[20230113 20:01:22 @agent_ppo2.py:146][0m 1513472 total steps have happened
[32m[20230113 20:01:22 @agent_ppo2.py:122][0m #------------------------ Iteration 739 --------------------------#
[32m[20230113 20:01:22 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:22 @agent_ppo2.py:186][0m |           0.0007 |           5.3509 |           5.1752 |
[32m[20230113 20:01:22 @agent_ppo2.py:186][0m |           0.0005 |           4.7326 |           5.1651 |
[32m[20230113 20:01:22 @agent_ppo2.py:186][0m |          -0.0058 |           4.3657 |           5.1669 |
[32m[20230113 20:01:22 @agent_ppo2.py:186][0m |          -0.0097 |           4.1932 |           5.1659 |
[32m[20230113 20:01:22 @agent_ppo2.py:186][0m |          -0.0083 |           4.0674 |           5.1625 |
[32m[20230113 20:01:22 @agent_ppo2.py:186][0m |          -0.0074 |           3.9571 |           5.1670 |
[32m[20230113 20:01:22 @agent_ppo2.py:186][0m |          -0.0093 |           3.8896 |           5.1671 |
[32m[20230113 20:01:22 @agent_ppo2.py:186][0m |          -0.0097 |           3.7822 |           5.1651 |
[32m[20230113 20:01:23 @agent_ppo2.py:186][0m |          -0.0024 |           4.2912 |           5.1694 |
[32m[20230113 20:01:23 @agent_ppo2.py:186][0m |          -0.0108 |           3.6365 |           5.1612 |
[32m[20230113 20:01:23 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:01:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.82
[32m[20230113 20:01:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.40
[32m[20230113 20:01:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.35
[32m[20230113 20:01:23 @agent_ppo2.py:144][0m Total time:      16.84 min
[32m[20230113 20:01:23 @agent_ppo2.py:146][0m 1515520 total steps have happened
[32m[20230113 20:01:23 @agent_ppo2.py:122][0m #------------------------ Iteration 740 --------------------------#
[32m[20230113 20:01:23 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:24 @agent_ppo2.py:186][0m |          -0.0053 |           6.0378 |           5.1227 |
[32m[20230113 20:01:24 @agent_ppo2.py:186][0m |          -0.0074 |           4.9749 |           5.1117 |
[32m[20230113 20:01:24 @agent_ppo2.py:186][0m |          -0.0076 |           4.5918 |           5.1135 |
[32m[20230113 20:01:24 @agent_ppo2.py:186][0m |          -0.0091 |           4.3264 |           5.1153 |
[32m[20230113 20:01:24 @agent_ppo2.py:186][0m |          -0.0116 |           4.1495 |           5.1168 |
[32m[20230113 20:01:24 @agent_ppo2.py:186][0m |          -0.0088 |           3.9844 |           5.1139 |
[32m[20230113 20:01:24 @agent_ppo2.py:186][0m |          -0.0098 |           3.8559 |           5.1104 |
[32m[20230113 20:01:24 @agent_ppo2.py:186][0m |          -0.0122 |           3.7431 |           5.1109 |
[32m[20230113 20:01:24 @agent_ppo2.py:186][0m |          -0.0125 |           3.6766 |           5.1134 |
[32m[20230113 20:01:24 @agent_ppo2.py:186][0m |          -0.0129 |           3.6131 |           5.1117 |
[32m[20230113 20:01:24 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.77
[32m[20230113 20:01:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.36
[32m[20230113 20:01:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.48
[32m[20230113 20:01:24 @agent_ppo2.py:144][0m Total time:      16.86 min
[32m[20230113 20:01:24 @agent_ppo2.py:146][0m 1517568 total steps have happened
[32m[20230113 20:01:24 @agent_ppo2.py:122][0m #------------------------ Iteration 741 --------------------------#
[32m[20230113 20:01:25 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:25 @agent_ppo2.py:186][0m |           0.0000 |           5.9511 |           5.0355 |
[32m[20230113 20:01:25 @agent_ppo2.py:186][0m |          -0.0059 |           5.0816 |           5.0273 |
[32m[20230113 20:01:25 @agent_ppo2.py:186][0m |          -0.0112 |           4.7644 |           5.0145 |
[32m[20230113 20:01:25 @agent_ppo2.py:186][0m |          -0.0123 |           4.5578 |           5.0138 |
[32m[20230113 20:01:25 @agent_ppo2.py:186][0m |          -0.0113 |           4.3841 |           5.0145 |
[32m[20230113 20:01:25 @agent_ppo2.py:186][0m |          -0.0074 |           4.2289 |           5.0112 |
[32m[20230113 20:01:25 @agent_ppo2.py:186][0m |          -0.0035 |           4.5843 |           5.0086 |
[32m[20230113 20:01:25 @agent_ppo2.py:186][0m |          -0.0173 |           3.9903 |           5.0104 |
[32m[20230113 20:01:25 @agent_ppo2.py:186][0m |          -0.0103 |           3.8998 |           5.0105 |
[32m[20230113 20:01:25 @agent_ppo2.py:186][0m |          -0.0164 |           3.8421 |           5.0083 |
[32m[20230113 20:01:25 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.61
[32m[20230113 20:01:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.59
[32m[20230113 20:01:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.99
[32m[20230113 20:01:26 @agent_ppo2.py:144][0m Total time:      16.88 min
[32m[20230113 20:01:26 @agent_ppo2.py:146][0m 1519616 total steps have happened
[32m[20230113 20:01:26 @agent_ppo2.py:122][0m #------------------------ Iteration 742 --------------------------#
[32m[20230113 20:01:26 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:26 @agent_ppo2.py:186][0m |           0.0020 |           6.7528 |           5.1496 |
[32m[20230113 20:01:26 @agent_ppo2.py:186][0m |          -0.0028 |           5.6792 |           5.1320 |
[32m[20230113 20:01:26 @agent_ppo2.py:186][0m |          -0.0075 |           5.2869 |           5.1297 |
[32m[20230113 20:01:26 @agent_ppo2.py:186][0m |          -0.0062 |           5.0235 |           5.1237 |
[32m[20230113 20:01:26 @agent_ppo2.py:186][0m |          -0.0124 |           4.6926 |           5.1250 |
[32m[20230113 20:01:26 @agent_ppo2.py:186][0m |          -0.0089 |           4.5085 |           5.1267 |
[32m[20230113 20:01:26 @agent_ppo2.py:186][0m |          -0.0120 |           4.2983 |           5.1249 |
[32m[20230113 20:01:26 @agent_ppo2.py:186][0m |          -0.0075 |           4.2517 |           5.1274 |
[32m[20230113 20:01:26 @agent_ppo2.py:186][0m |          -0.0136 |           4.1034 |           5.1174 |
[32m[20230113 20:01:27 @agent_ppo2.py:186][0m |          -0.0101 |           4.0464 |           5.1210 |
[32m[20230113 20:01:27 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.05
[32m[20230113 20:01:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.02
[32m[20230113 20:01:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.69
[32m[20230113 20:01:27 @agent_ppo2.py:144][0m Total time:      16.91 min
[32m[20230113 20:01:27 @agent_ppo2.py:146][0m 1521664 total steps have happened
[32m[20230113 20:01:27 @agent_ppo2.py:122][0m #------------------------ Iteration 743 --------------------------#
[32m[20230113 20:01:27 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:27 @agent_ppo2.py:186][0m |          -0.0009 |           5.1895 |           5.1545 |
[32m[20230113 20:01:28 @agent_ppo2.py:186][0m |          -0.0048 |           4.6403 |           5.1461 |
[32m[20230113 20:01:28 @agent_ppo2.py:186][0m |          -0.0073 |           4.3854 |           5.1454 |
[32m[20230113 20:01:28 @agent_ppo2.py:186][0m |          -0.0102 |           4.2515 |           5.1464 |
[32m[20230113 20:01:28 @agent_ppo2.py:186][0m |          -0.0113 |           4.1479 |           5.1473 |
[32m[20230113 20:01:28 @agent_ppo2.py:186][0m |          -0.0118 |           4.0568 |           5.1427 |
[32m[20230113 20:01:28 @agent_ppo2.py:186][0m |          -0.0120 |           3.9829 |           5.1444 |
[32m[20230113 20:01:28 @agent_ppo2.py:186][0m |          -0.0133 |           3.9071 |           5.1438 |
[32m[20230113 20:01:28 @agent_ppo2.py:186][0m |          -0.0133 |           3.8142 |           5.1435 |
[32m[20230113 20:01:28 @agent_ppo2.py:186][0m |          -0.0142 |           3.7364 |           5.1420 |
[32m[20230113 20:01:28 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.06
[32m[20230113 20:01:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.34
[32m[20230113 20:01:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.84
[32m[20230113 20:01:28 @agent_ppo2.py:144][0m Total time:      16.93 min
[32m[20230113 20:01:28 @agent_ppo2.py:146][0m 1523712 total steps have happened
[32m[20230113 20:01:28 @agent_ppo2.py:122][0m #------------------------ Iteration 744 --------------------------#
[32m[20230113 20:01:29 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:01:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:29 @agent_ppo2.py:186][0m |           0.0006 |           7.8965 |           4.9977 |
[32m[20230113 20:01:29 @agent_ppo2.py:186][0m |          -0.0027 |           6.4403 |           4.9911 |
[32m[20230113 20:01:29 @agent_ppo2.py:186][0m |          -0.0071 |           5.5918 |           4.9886 |
[32m[20230113 20:01:29 @agent_ppo2.py:186][0m |          -0.0055 |           5.2744 |           4.9805 |
[32m[20230113 20:01:29 @agent_ppo2.py:186][0m |          -0.0104 |           4.8029 |           4.9757 |
[32m[20230113 20:01:29 @agent_ppo2.py:186][0m |          -0.0099 |           4.5620 |           4.9765 |
[32m[20230113 20:01:29 @agent_ppo2.py:186][0m |          -0.0069 |           4.3929 |           4.9695 |
[32m[20230113 20:01:29 @agent_ppo2.py:186][0m |          -0.0120 |           4.2077 |           4.9691 |
[32m[20230113 20:01:29 @agent_ppo2.py:186][0m |          -0.0136 |           4.1027 |           4.9629 |
[32m[20230113 20:01:29 @agent_ppo2.py:186][0m |          -0.0113 |           3.9919 |           4.9605 |
[32m[20230113 20:01:29 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:01:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 215.60
[32m[20230113 20:01:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.47
[32m[20230113 20:01:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.31
[32m[20230113 20:01:30 @agent_ppo2.py:144][0m Total time:      16.95 min
[32m[20230113 20:01:30 @agent_ppo2.py:146][0m 1525760 total steps have happened
[32m[20230113 20:01:30 @agent_ppo2.py:122][0m #------------------------ Iteration 745 --------------------------#
[32m[20230113 20:01:30 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:30 @agent_ppo2.py:186][0m |           0.0093 |           7.0455 |           4.9704 |
[32m[20230113 20:01:30 @agent_ppo2.py:186][0m |          -0.0094 |           5.6383 |           4.9631 |
[32m[20230113 20:01:30 @agent_ppo2.py:186][0m |          -0.0111 |           5.1297 |           4.9638 |
[32m[20230113 20:01:30 @agent_ppo2.py:186][0m |          -0.0127 |           4.9258 |           4.9632 |
[32m[20230113 20:01:30 @agent_ppo2.py:186][0m |          -0.0081 |           4.7647 |           4.9626 |
[32m[20230113 20:01:30 @agent_ppo2.py:186][0m |          -0.0128 |           4.5621 |           4.9592 |
[32m[20230113 20:01:30 @agent_ppo2.py:186][0m |          -0.0048 |           4.4830 |           4.9612 |
[32m[20230113 20:01:30 @agent_ppo2.py:186][0m |          -0.0186 |           4.3901 |           4.9598 |
[32m[20230113 20:01:30 @agent_ppo2.py:186][0m |          -0.0167 |           4.3764 |           4.9592 |
[32m[20230113 20:01:31 @agent_ppo2.py:186][0m |          -0.0127 |           4.2248 |           4.9607 |
[32m[20230113 20:01:31 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.33
[32m[20230113 20:01:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.08
[32m[20230113 20:01:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.78
[32m[20230113 20:01:31 @agent_ppo2.py:144][0m Total time:      16.97 min
[32m[20230113 20:01:31 @agent_ppo2.py:146][0m 1527808 total steps have happened
[32m[20230113 20:01:31 @agent_ppo2.py:122][0m #------------------------ Iteration 746 --------------------------#
[32m[20230113 20:01:31 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:01:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:31 @agent_ppo2.py:186][0m |          -0.0012 |           6.2021 |           5.1869 |
[32m[20230113 20:01:32 @agent_ppo2.py:186][0m |          -0.0052 |           5.9182 |           5.1796 |
[32m[20230113 20:01:32 @agent_ppo2.py:186][0m |          -0.0072 |           5.5290 |           5.1791 |
[32m[20230113 20:01:32 @agent_ppo2.py:186][0m |          -0.0083 |           5.3989 |           5.1794 |
[32m[20230113 20:01:32 @agent_ppo2.py:186][0m |          -0.0096 |           5.2733 |           5.1790 |
[32m[20230113 20:01:32 @agent_ppo2.py:186][0m |          -0.0093 |           5.2287 |           5.1807 |
[32m[20230113 20:01:32 @agent_ppo2.py:186][0m |          -0.0110 |           5.1370 |           5.1798 |
[32m[20230113 20:01:32 @agent_ppo2.py:186][0m |          -0.0113 |           5.0571 |           5.1784 |
[32m[20230113 20:01:32 @agent_ppo2.py:186][0m |          -0.0119 |           5.0193 |           5.1776 |
[32m[20230113 20:01:32 @agent_ppo2.py:186][0m |          -0.0120 |           4.9649 |           5.1763 |
[32m[20230113 20:01:32 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:01:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.91
[32m[20230113 20:01:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.02
[32m[20230113 20:01:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.04
[32m[20230113 20:01:32 @agent_ppo2.py:144][0m Total time:      16.99 min
[32m[20230113 20:01:32 @agent_ppo2.py:146][0m 1529856 total steps have happened
[32m[20230113 20:01:32 @agent_ppo2.py:122][0m #------------------------ Iteration 747 --------------------------#
[32m[20230113 20:01:33 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:01:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:33 @agent_ppo2.py:186][0m |           0.0015 |           9.9763 |           5.1463 |
[32m[20230113 20:01:33 @agent_ppo2.py:186][0m |          -0.0048 |           5.1737 |           5.1374 |
[32m[20230113 20:01:33 @agent_ppo2.py:186][0m |          -0.0060 |           4.5680 |           5.1335 |
[32m[20230113 20:01:33 @agent_ppo2.py:186][0m |          -0.0094 |           4.3219 |           5.1277 |
[32m[20230113 20:01:33 @agent_ppo2.py:186][0m |          -0.0125 |           3.9946 |           5.1283 |
[32m[20230113 20:01:33 @agent_ppo2.py:186][0m |          -0.0102 |           3.8670 |           5.1271 |
[32m[20230113 20:01:33 @agent_ppo2.py:186][0m |          -0.0108 |           3.6725 |           5.1288 |
[32m[20230113 20:01:33 @agent_ppo2.py:186][0m |          -0.0107 |           3.5614 |           5.1276 |
[32m[20230113 20:01:33 @agent_ppo2.py:186][0m |          -0.0130 |           3.4588 |           5.1263 |
[32m[20230113 20:01:33 @agent_ppo2.py:186][0m |          -0.0146 |           3.3770 |           5.1262 |
[32m[20230113 20:01:33 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:01:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 119.25
[32m[20230113 20:01:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.55
[32m[20230113 20:01:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.53
[32m[20230113 20:01:34 @agent_ppo2.py:144][0m Total time:      17.02 min
[32m[20230113 20:01:34 @agent_ppo2.py:146][0m 1531904 total steps have happened
[32m[20230113 20:01:34 @agent_ppo2.py:122][0m #------------------------ Iteration 748 --------------------------#
[32m[20230113 20:01:34 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:34 @agent_ppo2.py:186][0m |           0.0009 |           5.9398 |           5.1232 |
[32m[20230113 20:01:34 @agent_ppo2.py:186][0m |          -0.0040 |           5.2671 |           5.1154 |
[32m[20230113 20:01:34 @agent_ppo2.py:186][0m |          -0.0078 |           5.0748 |           5.1158 |
[32m[20230113 20:01:34 @agent_ppo2.py:186][0m |          -0.0076 |           4.8762 |           5.1103 |
[32m[20230113 20:01:34 @agent_ppo2.py:186][0m |          -0.0144 |           4.7605 |           5.1128 |
[32m[20230113 20:01:34 @agent_ppo2.py:186][0m |          -0.0142 |           4.6390 |           5.1106 |
[32m[20230113 20:01:34 @agent_ppo2.py:186][0m |          -0.0178 |           4.5234 |           5.1099 |
[32m[20230113 20:01:34 @agent_ppo2.py:186][0m |          -0.0153 |           4.4757 |           5.1096 |
[32m[20230113 20:01:34 @agent_ppo2.py:186][0m |          -0.0205 |           4.3782 |           5.1073 |
[32m[20230113 20:01:35 @agent_ppo2.py:186][0m |          -0.0161 |           4.3383 |           5.1070 |
[32m[20230113 20:01:35 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.60
[32m[20230113 20:01:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.21
[32m[20230113 20:01:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.62
[32m[20230113 20:01:35 @agent_ppo2.py:144][0m Total time:      17.04 min
[32m[20230113 20:01:35 @agent_ppo2.py:146][0m 1533952 total steps have happened
[32m[20230113 20:01:35 @agent_ppo2.py:122][0m #------------------------ Iteration 749 --------------------------#
[32m[20230113 20:01:35 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:35 @agent_ppo2.py:186][0m |           0.0006 |           5.2731 |           4.9974 |
[32m[20230113 20:01:35 @agent_ppo2.py:186][0m |          -0.0064 |           4.5891 |           4.9869 |
[32m[20230113 20:01:36 @agent_ppo2.py:186][0m |          -0.0090 |           4.2169 |           4.9897 |
[32m[20230113 20:01:36 @agent_ppo2.py:186][0m |          -0.0106 |           3.9863 |           4.9853 |
[32m[20230113 20:01:36 @agent_ppo2.py:186][0m |          -0.0114 |           3.8177 |           4.9859 |
[32m[20230113 20:01:36 @agent_ppo2.py:186][0m |          -0.0119 |           3.6839 |           4.9783 |
[32m[20230113 20:01:36 @agent_ppo2.py:186][0m |          -0.0130 |           3.5809 |           4.9806 |
[32m[20230113 20:01:36 @agent_ppo2.py:186][0m |          -0.0127 |           3.4236 |           4.9833 |
[32m[20230113 20:01:36 @agent_ppo2.py:186][0m |          -0.0132 |           3.3370 |           4.9759 |
[32m[20230113 20:01:36 @agent_ppo2.py:186][0m |          -0.0150 |           3.2430 |           4.9800 |
[32m[20230113 20:01:36 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.54
[32m[20230113 20:01:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.30
[32m[20230113 20:01:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.66
[32m[20230113 20:01:36 @agent_ppo2.py:144][0m Total time:      17.06 min
[32m[20230113 20:01:36 @agent_ppo2.py:146][0m 1536000 total steps have happened
[32m[20230113 20:01:36 @agent_ppo2.py:122][0m #------------------------ Iteration 750 --------------------------#
[32m[20230113 20:01:37 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:37 @agent_ppo2.py:186][0m |          -0.0017 |          16.2604 |           5.0774 |
[32m[20230113 20:01:37 @agent_ppo2.py:186][0m |          -0.0098 |           5.9637 |           5.0683 |
[32m[20230113 20:01:37 @agent_ppo2.py:186][0m |          -0.0048 |           5.2293 |           5.0765 |
[32m[20230113 20:01:37 @agent_ppo2.py:186][0m |          -0.0092 |           4.8082 |           5.0650 |
[32m[20230113 20:01:37 @agent_ppo2.py:186][0m |          -0.0107 |           4.4161 |           5.0748 |
[32m[20230113 20:01:37 @agent_ppo2.py:186][0m |          -0.0126 |           4.2311 |           5.0670 |
[32m[20230113 20:01:37 @agent_ppo2.py:186][0m |          -0.0133 |           4.1242 |           5.0689 |
[32m[20230113 20:01:37 @agent_ppo2.py:186][0m |          -0.0135 |           3.8992 |           5.0689 |
[32m[20230113 20:01:37 @agent_ppo2.py:186][0m |          -0.0138 |           3.7731 |           5.0687 |
[32m[20230113 20:01:37 @agent_ppo2.py:186][0m |          -0.0127 |           3.6537 |           5.0686 |
[32m[20230113 20:01:37 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 112.68
[32m[20230113 20:01:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.68
[32m[20230113 20:01:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.66
[32m[20230113 20:01:38 @agent_ppo2.py:144][0m Total time:      17.08 min
[32m[20230113 20:01:38 @agent_ppo2.py:146][0m 1538048 total steps have happened
[32m[20230113 20:01:38 @agent_ppo2.py:122][0m #------------------------ Iteration 751 --------------------------#
[32m[20230113 20:01:38 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:38 @agent_ppo2.py:186][0m |           0.0060 |           5.8207 |           4.9399 |
[32m[20230113 20:01:38 @agent_ppo2.py:186][0m |          -0.0055 |           4.9845 |           4.9256 |
[32m[20230113 20:01:38 @agent_ppo2.py:186][0m |          -0.0162 |           4.6550 |           4.9273 |
[32m[20230113 20:01:38 @agent_ppo2.py:186][0m |          -0.0121 |           4.4724 |           4.9199 |
[32m[20230113 20:01:38 @agent_ppo2.py:186][0m |          -0.0120 |           4.3358 |           4.9256 |
[32m[20230113 20:01:38 @agent_ppo2.py:186][0m |          -0.0125 |           4.2199 |           4.9196 |
[32m[20230113 20:01:38 @agent_ppo2.py:186][0m |          -0.0133 |           4.1048 |           4.9279 |
[32m[20230113 20:01:38 @agent_ppo2.py:186][0m |          -0.0059 |           4.4610 |           4.9255 |
[32m[20230113 20:01:38 @agent_ppo2.py:186][0m |          -0.0141 |           4.0522 |           4.9202 |
[32m[20230113 20:01:38 @agent_ppo2.py:186][0m |          -0.0137 |           3.9929 |           4.9240 |
[32m[20230113 20:01:38 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.91
[32m[20230113 20:01:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.45
[32m[20230113 20:01:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.87
[32m[20230113 20:01:39 @agent_ppo2.py:144][0m Total time:      17.11 min
[32m[20230113 20:01:39 @agent_ppo2.py:146][0m 1540096 total steps have happened
[32m[20230113 20:01:39 @agent_ppo2.py:122][0m #------------------------ Iteration 752 --------------------------#
[32m[20230113 20:01:39 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:39 @agent_ppo2.py:186][0m |          -0.0003 |           6.2921 |           5.1609 |
[32m[20230113 20:01:39 @agent_ppo2.py:186][0m |          -0.0056 |           5.4715 |           5.1604 |
[32m[20230113 20:01:40 @agent_ppo2.py:186][0m |          -0.0070 |           5.1485 |           5.1622 |
[32m[20230113 20:01:40 @agent_ppo2.py:186][0m |          -0.0077 |           4.8629 |           5.1607 |
[32m[20230113 20:01:40 @agent_ppo2.py:186][0m |          -0.0100 |           4.6695 |           5.1585 |
[32m[20230113 20:01:40 @agent_ppo2.py:186][0m |          -0.0116 |           4.4652 |           5.1591 |
[32m[20230113 20:01:40 @agent_ppo2.py:186][0m |          -0.0123 |           4.3453 |           5.1581 |
[32m[20230113 20:01:40 @agent_ppo2.py:186][0m |          -0.0122 |           4.2158 |           5.1592 |
[32m[20230113 20:01:40 @agent_ppo2.py:186][0m |          -0.0142 |           4.1294 |           5.1594 |
[32m[20230113 20:01:40 @agent_ppo2.py:186][0m |          -0.0147 |           4.0555 |           5.1621 |
[32m[20230113 20:01:40 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.70
[32m[20230113 20:01:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.35
[32m[20230113 20:01:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.30
[32m[20230113 20:01:40 @agent_ppo2.py:144][0m Total time:      17.13 min
[32m[20230113 20:01:40 @agent_ppo2.py:146][0m 1542144 total steps have happened
[32m[20230113 20:01:40 @agent_ppo2.py:122][0m #------------------------ Iteration 753 --------------------------#
[32m[20230113 20:01:41 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:41 @agent_ppo2.py:186][0m |          -0.0001 |           5.2563 |           5.1615 |
[32m[20230113 20:01:41 @agent_ppo2.py:186][0m |          -0.0045 |           4.6419 |           5.1535 |
[32m[20230113 20:01:41 @agent_ppo2.py:186][0m |          -0.0056 |           4.3821 |           5.1478 |
[32m[20230113 20:01:41 @agent_ppo2.py:186][0m |          -0.0071 |           4.2492 |           5.1437 |
[32m[20230113 20:01:41 @agent_ppo2.py:186][0m |          -0.0088 |           4.1229 |           5.1439 |
[32m[20230113 20:01:41 @agent_ppo2.py:186][0m |          -0.0091 |           4.0320 |           5.1415 |
[32m[20230113 20:01:41 @agent_ppo2.py:186][0m |          -0.0099 |           3.9432 |           5.1397 |
[32m[20230113 20:01:41 @agent_ppo2.py:186][0m |          -0.0099 |           3.8826 |           5.1398 |
[32m[20230113 20:01:41 @agent_ppo2.py:186][0m |          -0.0111 |           3.8110 |           5.1362 |
[32m[20230113 20:01:41 @agent_ppo2.py:186][0m |          -0.0115 |           3.7427 |           5.1347 |
[32m[20230113 20:01:41 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.45
[32m[20230113 20:01:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.85
[32m[20230113 20:01:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.88
[32m[20230113 20:01:42 @agent_ppo2.py:144][0m Total time:      17.15 min
[32m[20230113 20:01:42 @agent_ppo2.py:146][0m 1544192 total steps have happened
[32m[20230113 20:01:42 @agent_ppo2.py:122][0m #------------------------ Iteration 754 --------------------------#
[32m[20230113 20:01:42 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:42 @agent_ppo2.py:186][0m |           0.0002 |           7.4342 |           5.1261 |
[32m[20230113 20:01:42 @agent_ppo2.py:186][0m |          -0.0012 |           5.6617 |           5.1200 |
[32m[20230113 20:01:42 @agent_ppo2.py:186][0m |          -0.0048 |           4.8927 |           5.1226 |
[32m[20230113 20:01:42 @agent_ppo2.py:186][0m |          -0.0080 |           4.5411 |           5.1269 |
[32m[20230113 20:01:42 @agent_ppo2.py:186][0m |          -0.0099 |           4.3340 |           5.1219 |
[32m[20230113 20:01:42 @agent_ppo2.py:186][0m |          -0.0082 |           4.2080 |           5.1276 |
[32m[20230113 20:01:42 @agent_ppo2.py:186][0m |          -0.0111 |           4.0137 |           5.1262 |
[32m[20230113 20:01:42 @agent_ppo2.py:186][0m |          -0.0113 |           3.8990 |           5.1219 |
[32m[20230113 20:01:42 @agent_ppo2.py:186][0m |          -0.0112 |           3.7694 |           5.1290 |
[32m[20230113 20:01:42 @agent_ppo2.py:186][0m |          -0.0126 |           3.7095 |           5.1256 |
[32m[20230113 20:01:42 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.92
[32m[20230113 20:01:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.41
[32m[20230113 20:01:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.16
[32m[20230113 20:01:43 @agent_ppo2.py:144][0m Total time:      17.17 min
[32m[20230113 20:01:43 @agent_ppo2.py:146][0m 1546240 total steps have happened
[32m[20230113 20:01:43 @agent_ppo2.py:122][0m #------------------------ Iteration 755 --------------------------#
[32m[20230113 20:01:43 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:43 @agent_ppo2.py:186][0m |          -0.0006 |           6.5962 |           5.0995 |
[32m[20230113 20:01:43 @agent_ppo2.py:186][0m |          -0.0019 |           5.4374 |           5.0914 |
[32m[20230113 20:01:43 @agent_ppo2.py:186][0m |          -0.0062 |           4.7210 |           5.0879 |
[32m[20230113 20:01:44 @agent_ppo2.py:186][0m |          -0.0081 |           4.3568 |           5.0865 |
[32m[20230113 20:01:44 @agent_ppo2.py:186][0m |          -0.0070 |           4.0950 |           5.0828 |
[32m[20230113 20:01:44 @agent_ppo2.py:186][0m |          -0.0105 |           3.8896 |           5.0853 |
[32m[20230113 20:01:44 @agent_ppo2.py:186][0m |          -0.0107 |           3.7216 |           5.0821 |
[32m[20230113 20:01:44 @agent_ppo2.py:186][0m |          -0.0095 |           3.5886 |           5.0813 |
[32m[20230113 20:01:44 @agent_ppo2.py:186][0m |          -0.0117 |           3.4684 |           5.0812 |
[32m[20230113 20:01:44 @agent_ppo2.py:186][0m |          -0.0105 |           3.3892 |           5.0822 |
[32m[20230113 20:01:44 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:01:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.21
[32m[20230113 20:01:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.29
[32m[20230113 20:01:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.28
[32m[20230113 20:01:44 @agent_ppo2.py:144][0m Total time:      17.19 min
[32m[20230113 20:01:44 @agent_ppo2.py:146][0m 1548288 total steps have happened
[32m[20230113 20:01:44 @agent_ppo2.py:122][0m #------------------------ Iteration 756 --------------------------#
[32m[20230113 20:01:45 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:45 @agent_ppo2.py:186][0m |          -0.0014 |           7.0878 |           5.0787 |
[32m[20230113 20:01:45 @agent_ppo2.py:186][0m |          -0.0077 |           6.2448 |           5.0663 |
[32m[20230113 20:01:45 @agent_ppo2.py:186][0m |          -0.0074 |           5.9474 |           5.0731 |
[32m[20230113 20:01:45 @agent_ppo2.py:186][0m |          -0.0086 |           5.8332 |           5.0718 |
[32m[20230113 20:01:45 @agent_ppo2.py:186][0m |          -0.0139 |           5.5355 |           5.0693 |
[32m[20230113 20:01:45 @agent_ppo2.py:186][0m |          -0.0113 |           5.3686 |           5.0696 |
[32m[20230113 20:01:45 @agent_ppo2.py:186][0m |          -0.0104 |           5.3946 |           5.0788 |
[32m[20230113 20:01:45 @agent_ppo2.py:186][0m |          -0.0107 |           5.4043 |           5.0748 |
[32m[20230113 20:01:45 @agent_ppo2.py:186][0m |          -0.0125 |           5.1935 |           5.0781 |
[32m[20230113 20:01:45 @agent_ppo2.py:186][0m |          -0.0110 |           4.9657 |           5.0819 |
[32m[20230113 20:01:45 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:01:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.13
[32m[20230113 20:01:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.05
[32m[20230113 20:01:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.48
[32m[20230113 20:01:45 @agent_ppo2.py:144][0m Total time:      17.22 min
[32m[20230113 20:01:45 @agent_ppo2.py:146][0m 1550336 total steps have happened
[32m[20230113 20:01:45 @agent_ppo2.py:122][0m #------------------------ Iteration 757 --------------------------#
[32m[20230113 20:01:46 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:46 @agent_ppo2.py:186][0m |          -0.0000 |           5.1931 |           5.0820 |
[32m[20230113 20:01:46 @agent_ppo2.py:186][0m |          -0.0047 |           4.3944 |           5.0717 |
[32m[20230113 20:01:46 @agent_ppo2.py:186][0m |          -0.0082 |           4.0444 |           5.0641 |
[32m[20230113 20:01:46 @agent_ppo2.py:186][0m |          -0.0095 |           3.7907 |           5.0647 |
[32m[20230113 20:01:46 @agent_ppo2.py:186][0m |          -0.0102 |           3.6165 |           5.0612 |
[32m[20230113 20:01:46 @agent_ppo2.py:186][0m |          -0.0108 |           3.4991 |           5.0599 |
[32m[20230113 20:01:46 @agent_ppo2.py:186][0m |          -0.0119 |           3.4049 |           5.0610 |
[32m[20230113 20:01:46 @agent_ppo2.py:186][0m |          -0.0119 |           3.2909 |           5.0609 |
[32m[20230113 20:01:46 @agent_ppo2.py:186][0m |          -0.0123 |           3.2035 |           5.0556 |
[32m[20230113 20:01:46 @agent_ppo2.py:186][0m |          -0.0133 |           3.0982 |           5.0539 |
[32m[20230113 20:01:46 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:01:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.68
[32m[20230113 20:01:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.29
[32m[20230113 20:01:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.98
[32m[20230113 20:01:47 @agent_ppo2.py:144][0m Total time:      17.24 min
[32m[20230113 20:01:47 @agent_ppo2.py:146][0m 1552384 total steps have happened
[32m[20230113 20:01:47 @agent_ppo2.py:122][0m #------------------------ Iteration 758 --------------------------#
[32m[20230113 20:01:47 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:47 @agent_ppo2.py:186][0m |          -0.0036 |           6.7716 |           5.1170 |
[32m[20230113 20:01:47 @agent_ppo2.py:186][0m |          -0.0136 |           5.4549 |           5.1051 |
[32m[20230113 20:01:47 @agent_ppo2.py:186][0m |          -0.0026 |           4.8343 |           5.1114 |
[32m[20230113 20:01:47 @agent_ppo2.py:186][0m |          -0.0128 |           4.4424 |           5.1094 |
[32m[20230113 20:01:47 @agent_ppo2.py:186][0m |          -0.0113 |           4.1217 |           5.1123 |
[32m[20230113 20:01:48 @agent_ppo2.py:186][0m |          -0.0106 |           3.8835 |           5.1124 |
[32m[20230113 20:01:48 @agent_ppo2.py:186][0m |          -0.0101 |           3.7180 |           5.1127 |
[32m[20230113 20:01:48 @agent_ppo2.py:186][0m |          -0.0094 |           3.5814 |           5.1151 |
[32m[20230113 20:01:48 @agent_ppo2.py:186][0m |          -0.0129 |           3.5006 |           5.1121 |
[32m[20230113 20:01:48 @agent_ppo2.py:186][0m |          -0.0163 |           3.3228 |           5.1123 |
[32m[20230113 20:01:48 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.54
[32m[20230113 20:01:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.16
[32m[20230113 20:01:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.51
[32m[20230113 20:01:48 @agent_ppo2.py:144][0m Total time:      17.26 min
[32m[20230113 20:01:48 @agent_ppo2.py:146][0m 1554432 total steps have happened
[32m[20230113 20:01:48 @agent_ppo2.py:122][0m #------------------------ Iteration 759 --------------------------#
[32m[20230113 20:01:49 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:49 @agent_ppo2.py:186][0m |           0.0020 |           6.5134 |           5.2004 |
[32m[20230113 20:01:49 @agent_ppo2.py:186][0m |          -0.0049 |           5.4069 |           5.1946 |
[32m[20230113 20:01:49 @agent_ppo2.py:186][0m |          -0.0069 |           5.0150 |           5.1998 |
[32m[20230113 20:01:49 @agent_ppo2.py:186][0m |          -0.0098 |           4.7822 |           5.1910 |
[32m[20230113 20:01:49 @agent_ppo2.py:186][0m |          -0.0098 |           4.5907 |           5.1923 |
[32m[20230113 20:01:49 @agent_ppo2.py:186][0m |          -0.0106 |           4.4658 |           5.1939 |
[32m[20230113 20:01:49 @agent_ppo2.py:186][0m |          -0.0121 |           4.2880 |           5.1874 |
[32m[20230113 20:01:49 @agent_ppo2.py:186][0m |          -0.0122 |           4.2032 |           5.1943 |
[32m[20230113 20:01:49 @agent_ppo2.py:186][0m |          -0.0127 |           4.0768 |           5.1965 |
[32m[20230113 20:01:49 @agent_ppo2.py:186][0m |          -0.0140 |           3.9637 |           5.1980 |
[32m[20230113 20:01:49 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:01:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.72
[32m[20230113 20:01:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.09
[32m[20230113 20:01:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.20
[32m[20230113 20:01:49 @agent_ppo2.py:144][0m Total time:      17.28 min
[32m[20230113 20:01:49 @agent_ppo2.py:146][0m 1556480 total steps have happened
[32m[20230113 20:01:49 @agent_ppo2.py:122][0m #------------------------ Iteration 760 --------------------------#
[32m[20230113 20:01:50 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:01:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:50 @agent_ppo2.py:186][0m |          -0.0013 |           6.2624 |           5.2074 |
[32m[20230113 20:01:50 @agent_ppo2.py:186][0m |          -0.0054 |           5.6456 |           5.2010 |
[32m[20230113 20:01:50 @agent_ppo2.py:186][0m |          -0.0055 |           5.4705 |           5.1934 |
[32m[20230113 20:01:50 @agent_ppo2.py:186][0m |          -0.0070 |           5.1680 |           5.1916 |
[32m[20230113 20:01:50 @agent_ppo2.py:186][0m |          -0.0118 |           4.9818 |           5.1885 |
[32m[20230113 20:01:50 @agent_ppo2.py:186][0m |          -0.0059 |           4.8184 |           5.1842 |
[32m[20230113 20:01:50 @agent_ppo2.py:186][0m |          -0.0123 |           4.7042 |           5.1821 |
[32m[20230113 20:01:50 @agent_ppo2.py:186][0m |          -0.0131 |           4.5496 |           5.1800 |
[32m[20230113 20:01:50 @agent_ppo2.py:186][0m |          -0.0066 |           4.6324 |           5.1786 |
[32m[20230113 20:01:50 @agent_ppo2.py:186][0m |          -0.0136 |           4.2957 |           5.1776 |
[32m[20230113 20:01:50 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:01:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.07
[32m[20230113 20:01:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.24
[32m[20230113 20:01:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.99
[32m[20230113 20:01:51 @agent_ppo2.py:144][0m Total time:      17.30 min
[32m[20230113 20:01:51 @agent_ppo2.py:146][0m 1558528 total steps have happened
[32m[20230113 20:01:51 @agent_ppo2.py:122][0m #------------------------ Iteration 761 --------------------------#
[32m[20230113 20:01:51 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:01:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:51 @agent_ppo2.py:186][0m |          -0.0004 |           7.6103 |           5.0982 |
[32m[20230113 20:01:51 @agent_ppo2.py:186][0m |          -0.0002 |           6.8269 |           5.1016 |
[32m[20230113 20:01:51 @agent_ppo2.py:186][0m |          -0.0051 |           6.3816 |           5.0999 |
[32m[20230113 20:01:51 @agent_ppo2.py:186][0m |          -0.0083 |           5.7809 |           5.0938 |
[32m[20230113 20:01:51 @agent_ppo2.py:186][0m |          -0.0101 |           5.5806 |           5.0970 |
[32m[20230113 20:01:51 @agent_ppo2.py:186][0m |          -0.0089 |           5.4099 |           5.0945 |
[32m[20230113 20:01:51 @agent_ppo2.py:186][0m |          -0.0075 |           5.3214 |           5.1000 |
[32m[20230113 20:01:51 @agent_ppo2.py:186][0m |          -0.0101 |           5.1749 |           5.1017 |
[32m[20230113 20:01:52 @agent_ppo2.py:186][0m |          -0.0129 |           5.0785 |           5.0992 |
[32m[20230113 20:01:52 @agent_ppo2.py:186][0m |          -0.0096 |           4.9853 |           5.1005 |
[32m[20230113 20:01:52 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:01:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.37
[32m[20230113 20:01:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.72
[32m[20230113 20:01:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.14
[32m[20230113 20:01:52 @agent_ppo2.py:144][0m Total time:      17.32 min
[32m[20230113 20:01:52 @agent_ppo2.py:146][0m 1560576 total steps have happened
[32m[20230113 20:01:52 @agent_ppo2.py:122][0m #------------------------ Iteration 762 --------------------------#
[32m[20230113 20:01:52 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:52 @agent_ppo2.py:186][0m |          -0.0009 |           5.5442 |           5.2591 |
[32m[20230113 20:01:53 @agent_ppo2.py:186][0m |          -0.0047 |           4.8115 |           5.2543 |
[32m[20230113 20:01:53 @agent_ppo2.py:186][0m |          -0.0066 |           4.5237 |           5.2494 |
[32m[20230113 20:01:53 @agent_ppo2.py:186][0m |          -0.0079 |           4.2738 |           5.2459 |
[32m[20230113 20:01:53 @agent_ppo2.py:186][0m |          -0.0098 |           4.2336 |           5.2484 |
[32m[20230113 20:01:53 @agent_ppo2.py:186][0m |          -0.0102 |           4.0723 |           5.2454 |
[32m[20230113 20:01:53 @agent_ppo2.py:186][0m |          -0.0082 |           3.9445 |           5.2426 |
[32m[20230113 20:01:53 @agent_ppo2.py:186][0m |          -0.0096 |           3.8981 |           5.2409 |
[32m[20230113 20:01:53 @agent_ppo2.py:186][0m |          -0.0114 |           3.8550 |           5.2442 |
[32m[20230113 20:01:53 @agent_ppo2.py:186][0m |          -0.0126 |           3.7973 |           5.2394 |
[32m[20230113 20:01:53 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.29
[32m[20230113 20:01:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.38
[32m[20230113 20:01:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.83
[32m[20230113 20:01:53 @agent_ppo2.py:144][0m Total time:      17.35 min
[32m[20230113 20:01:53 @agent_ppo2.py:146][0m 1562624 total steps have happened
[32m[20230113 20:01:53 @agent_ppo2.py:122][0m #------------------------ Iteration 763 --------------------------#
[32m[20230113 20:01:54 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:54 @agent_ppo2.py:186][0m |          -0.0013 |           6.9970 |           5.1627 |
[32m[20230113 20:01:54 @agent_ppo2.py:186][0m |          -0.0060 |           5.8816 |           5.1622 |
[32m[20230113 20:01:54 @agent_ppo2.py:186][0m |          -0.0069 |           5.5075 |           5.1621 |
[32m[20230113 20:01:54 @agent_ppo2.py:186][0m |          -0.0075 |           5.2363 |           5.1616 |
[32m[20230113 20:01:54 @agent_ppo2.py:186][0m |          -0.0082 |           4.9979 |           5.1566 |
[32m[20230113 20:01:54 @agent_ppo2.py:186][0m |          -0.0128 |           4.8418 |           5.1597 |
[32m[20230113 20:01:54 @agent_ppo2.py:186][0m |          -0.0117 |           4.7439 |           5.1606 |
[32m[20230113 20:01:54 @agent_ppo2.py:186][0m |          -0.0104 |           4.7034 |           5.1542 |
[32m[20230113 20:01:54 @agent_ppo2.py:186][0m |          -0.0121 |           4.5704 |           5.1523 |
[32m[20230113 20:01:54 @agent_ppo2.py:186][0m |          -0.0112 |           4.5899 |           5.1570 |
[32m[20230113 20:01:54 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:01:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.23
[32m[20230113 20:01:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.05
[32m[20230113 20:01:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.45
[32m[20230113 20:01:55 @agent_ppo2.py:144][0m Total time:      17.37 min
[32m[20230113 20:01:55 @agent_ppo2.py:146][0m 1564672 total steps have happened
[32m[20230113 20:01:55 @agent_ppo2.py:122][0m #------------------------ Iteration 764 --------------------------#
[32m[20230113 20:01:55 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:55 @agent_ppo2.py:186][0m |          -0.0048 |           5.8152 |           5.1631 |
[32m[20230113 20:01:55 @agent_ppo2.py:186][0m |          -0.0066 |           5.4000 |           5.1551 |
[32m[20230113 20:01:55 @agent_ppo2.py:186][0m |          -0.0108 |           5.1069 |           5.1566 |
[32m[20230113 20:01:55 @agent_ppo2.py:186][0m |          -0.0127 |           4.9389 |           5.1542 |
[32m[20230113 20:01:55 @agent_ppo2.py:186][0m |          -0.0137 |           4.8517 |           5.1541 |
[32m[20230113 20:01:55 @agent_ppo2.py:186][0m |          -0.0132 |           4.7314 |           5.1537 |
[32m[20230113 20:01:55 @agent_ppo2.py:186][0m |          -0.0140 |           4.6446 |           5.1504 |
[32m[20230113 20:01:55 @agent_ppo2.py:186][0m |          -0.0154 |           4.5735 |           5.1506 |
[32m[20230113 20:01:55 @agent_ppo2.py:186][0m |          -0.0150 |           4.5019 |           5.1493 |
[32m[20230113 20:01:55 @agent_ppo2.py:186][0m |          -0.0165 |           4.4249 |           5.1494 |
[32m[20230113 20:01:55 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:01:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.03
[32m[20230113 20:01:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.37
[32m[20230113 20:01:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.99
[32m[20230113 20:01:56 @agent_ppo2.py:144][0m Total time:      17.39 min
[32m[20230113 20:01:56 @agent_ppo2.py:146][0m 1566720 total steps have happened
[32m[20230113 20:01:56 @agent_ppo2.py:122][0m #------------------------ Iteration 765 --------------------------#
[32m[20230113 20:01:56 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:56 @agent_ppo2.py:186][0m |          -0.0010 |           6.6051 |           5.1921 |
[32m[20230113 20:01:56 @agent_ppo2.py:186][0m |          -0.0057 |           5.5345 |           5.1810 |
[32m[20230113 20:01:56 @agent_ppo2.py:186][0m |          -0.0074 |           5.0957 |           5.1755 |
[32m[20230113 20:01:57 @agent_ppo2.py:186][0m |          -0.0086 |           4.8087 |           5.1712 |
[32m[20230113 20:01:57 @agent_ppo2.py:186][0m |          -0.0104 |           4.6015 |           5.1671 |
[32m[20230113 20:01:57 @agent_ppo2.py:186][0m |          -0.0111 |           4.3986 |           5.1656 |
[32m[20230113 20:01:57 @agent_ppo2.py:186][0m |          -0.0126 |           4.2648 |           5.1629 |
[32m[20230113 20:01:57 @agent_ppo2.py:186][0m |          -0.0135 |           4.1356 |           5.1615 |
[32m[20230113 20:01:57 @agent_ppo2.py:186][0m |          -0.0142 |           4.0351 |           5.1587 |
[32m[20230113 20:01:57 @agent_ppo2.py:186][0m |          -0.0118 |           3.9901 |           5.1554 |
[32m[20230113 20:01:57 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:01:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.52
[32m[20230113 20:01:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.81
[32m[20230113 20:01:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.72
[32m[20230113 20:01:57 @agent_ppo2.py:144][0m Total time:      17.41 min
[32m[20230113 20:01:57 @agent_ppo2.py:146][0m 1568768 total steps have happened
[32m[20230113 20:01:57 @agent_ppo2.py:122][0m #------------------------ Iteration 766 --------------------------#
[32m[20230113 20:01:58 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:01:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:58 @agent_ppo2.py:186][0m |           0.0019 |           6.3022 |           5.0795 |
[32m[20230113 20:01:58 @agent_ppo2.py:186][0m |          -0.0028 |           5.2036 |           5.0723 |
[32m[20230113 20:01:58 @agent_ppo2.py:186][0m |          -0.0024 |           4.8591 |           5.0716 |
[32m[20230113 20:01:58 @agent_ppo2.py:186][0m |          -0.0082 |           4.5472 |           5.0699 |
[32m[20230113 20:01:58 @agent_ppo2.py:186][0m |          -0.0078 |           4.4295 |           5.0683 |
[32m[20230113 20:01:58 @agent_ppo2.py:186][0m |          -0.0085 |           4.4165 |           5.0682 |
[32m[20230113 20:01:58 @agent_ppo2.py:186][0m |          -0.0130 |           4.1963 |           5.0682 |
[32m[20230113 20:01:58 @agent_ppo2.py:186][0m |          -0.0111 |           4.0432 |           5.0642 |
[32m[20230113 20:01:58 @agent_ppo2.py:186][0m |          -0.0089 |           4.0486 |           5.0634 |
[32m[20230113 20:01:58 @agent_ppo2.py:186][0m |          -0.0135 |           3.8165 |           5.0693 |
[32m[20230113 20:01:58 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:01:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.34
[32m[20230113 20:01:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.67
[32m[20230113 20:01:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.88
[32m[20230113 20:01:59 @agent_ppo2.py:144][0m Total time:      17.43 min
[32m[20230113 20:01:59 @agent_ppo2.py:146][0m 1570816 total steps have happened
[32m[20230113 20:01:59 @agent_ppo2.py:122][0m #------------------------ Iteration 767 --------------------------#
[32m[20230113 20:01:59 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:01:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:01:59 @agent_ppo2.py:186][0m |           0.0127 |           6.1184 |           5.1327 |
[32m[20230113 20:01:59 @agent_ppo2.py:186][0m |          -0.0005 |           4.6384 |           5.1211 |
[32m[20230113 20:01:59 @agent_ppo2.py:186][0m |          -0.0090 |           4.1866 |           5.1279 |
[32m[20230113 20:01:59 @agent_ppo2.py:186][0m |          -0.0116 |           3.9626 |           5.1240 |
[32m[20230113 20:01:59 @agent_ppo2.py:186][0m |          -0.0131 |           3.7641 |           5.1246 |
[32m[20230113 20:01:59 @agent_ppo2.py:186][0m |          -0.0104 |           3.6836 |           5.1249 |
[32m[20230113 20:01:59 @agent_ppo2.py:186][0m |          -0.0089 |           3.5468 |           5.1225 |
[32m[20230113 20:01:59 @agent_ppo2.py:186][0m |          -0.0130 |           3.4335 |           5.1281 |
[32m[20230113 20:01:59 @agent_ppo2.py:186][0m |          -0.0127 |           3.3295 |           5.1245 |
[32m[20230113 20:01:59 @agent_ppo2.py:186][0m |          -0.0191 |           3.2630 |           5.1261 |
[32m[20230113 20:01:59 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.47
[32m[20230113 20:02:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.95
[32m[20230113 20:02:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.94
[32m[20230113 20:02:00 @agent_ppo2.py:144][0m Total time:      17.45 min
[32m[20230113 20:02:00 @agent_ppo2.py:146][0m 1572864 total steps have happened
[32m[20230113 20:02:00 @agent_ppo2.py:122][0m #------------------------ Iteration 768 --------------------------#
[32m[20230113 20:02:00 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:02:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:00 @agent_ppo2.py:186][0m |          -0.0030 |           6.4782 |           5.2785 |
[32m[20230113 20:02:00 @agent_ppo2.py:186][0m |          -0.0035 |           5.4306 |           5.2788 |
[32m[20230113 20:02:00 @agent_ppo2.py:186][0m |          -0.0157 |           4.8729 |           5.2765 |
[32m[20230113 20:02:00 @agent_ppo2.py:186][0m |          -0.0038 |           4.5749 |           5.2762 |
[32m[20230113 20:02:00 @agent_ppo2.py:186][0m |          -0.0117 |           4.3505 |           5.2770 |
[32m[20230113 20:02:01 @agent_ppo2.py:186][0m |          -0.0065 |           4.1793 |           5.2746 |
[32m[20230113 20:02:01 @agent_ppo2.py:186][0m |          -0.0173 |           4.0822 |           5.2735 |
[32m[20230113 20:02:01 @agent_ppo2.py:186][0m |          -0.0134 |           3.8959 |           5.2774 |
[32m[20230113 20:02:01 @agent_ppo2.py:186][0m |          -0.0095 |           3.7812 |           5.2750 |
[32m[20230113 20:02:01 @agent_ppo2.py:186][0m |          -0.0101 |           3.6895 |           5.2755 |
[32m[20230113 20:02:01 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.66
[32m[20230113 20:02:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.92
[32m[20230113 20:02:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.72
[32m[20230113 20:02:01 @agent_ppo2.py:144][0m Total time:      17.48 min
[32m[20230113 20:02:01 @agent_ppo2.py:146][0m 1574912 total steps have happened
[32m[20230113 20:02:01 @agent_ppo2.py:122][0m #------------------------ Iteration 769 --------------------------#
[32m[20230113 20:02:02 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:02 @agent_ppo2.py:186][0m |           0.0071 |           6.5114 |           5.1218 |
[32m[20230113 20:02:02 @agent_ppo2.py:186][0m |          -0.0077 |           5.5238 |           5.1051 |
[32m[20230113 20:02:02 @agent_ppo2.py:186][0m |          -0.0076 |           5.2992 |           5.1117 |
[32m[20230113 20:02:02 @agent_ppo2.py:186][0m |          -0.0040 |           5.3265 |           5.1135 |
[32m[20230113 20:02:02 @agent_ppo2.py:186][0m |          -0.0133 |           4.9678 |           5.1077 |
[32m[20230113 20:02:02 @agent_ppo2.py:186][0m |          -0.0104 |           4.8545 |           5.1091 |
[32m[20230113 20:02:02 @agent_ppo2.py:186][0m |          -0.0147 |           4.7805 |           5.1051 |
[32m[20230113 20:02:02 @agent_ppo2.py:186][0m |          -0.0054 |           4.7578 |           5.1089 |
[32m[20230113 20:02:02 @agent_ppo2.py:186][0m |          -0.0093 |           4.5252 |           5.1097 |
[32m[20230113 20:02:02 @agent_ppo2.py:186][0m |          -0.0124 |           4.4208 |           5.1066 |
[32m[20230113 20:02:02 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.68
[32m[20230113 20:02:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.80
[32m[20230113 20:02:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.00
[32m[20230113 20:02:02 @agent_ppo2.py:144][0m Total time:      17.50 min
[32m[20230113 20:02:02 @agent_ppo2.py:146][0m 1576960 total steps have happened
[32m[20230113 20:02:02 @agent_ppo2.py:122][0m #------------------------ Iteration 770 --------------------------#
[32m[20230113 20:02:03 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:03 @agent_ppo2.py:186][0m |          -0.0030 |           6.6613 |           5.2098 |
[32m[20230113 20:02:03 @agent_ppo2.py:186][0m |          -0.0069 |           5.5385 |           5.1941 |
[32m[20230113 20:02:03 @agent_ppo2.py:186][0m |          -0.0074 |           5.1327 |           5.1934 |
[32m[20230113 20:02:03 @agent_ppo2.py:186][0m |          -0.0099 |           4.9409 |           5.1885 |
[32m[20230113 20:02:03 @agent_ppo2.py:186][0m |          -0.0125 |           4.8309 |           5.1901 |
[32m[20230113 20:02:03 @agent_ppo2.py:186][0m |          -0.0099 |           4.6835 |           5.1859 |
[32m[20230113 20:02:03 @agent_ppo2.py:186][0m |          -0.0126 |           4.5591 |           5.1829 |
[32m[20230113 20:02:03 @agent_ppo2.py:186][0m |          -0.0140 |           4.4691 |           5.1850 |
[32m[20230113 20:02:03 @agent_ppo2.py:186][0m |          -0.0090 |           4.4464 |           5.1854 |
[32m[20230113 20:02:03 @agent_ppo2.py:186][0m |          -0.0121 |           4.3071 |           5.1825 |
[32m[20230113 20:02:03 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.37
[32m[20230113 20:02:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.40
[32m[20230113 20:02:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.87
[32m[20230113 20:02:04 @agent_ppo2.py:144][0m Total time:      17.52 min
[32m[20230113 20:02:04 @agent_ppo2.py:146][0m 1579008 total steps have happened
[32m[20230113 20:02:04 @agent_ppo2.py:122][0m #------------------------ Iteration 771 --------------------------#
[32m[20230113 20:02:04 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:04 @agent_ppo2.py:186][0m |          -0.0002 |           5.5455 |           5.1253 |
[32m[20230113 20:02:04 @agent_ppo2.py:186][0m |          -0.0041 |           3.9834 |           5.1143 |
[32m[20230113 20:02:04 @agent_ppo2.py:186][0m |          -0.0062 |           3.5262 |           5.1136 |
[32m[20230113 20:02:04 @agent_ppo2.py:186][0m |          -0.0079 |           3.3447 |           5.1082 |
[32m[20230113 20:02:04 @agent_ppo2.py:186][0m |          -0.0085 |           3.0680 |           5.1068 |
[32m[20230113 20:02:04 @agent_ppo2.py:186][0m |          -0.0096 |           2.9549 |           5.1051 |
[32m[20230113 20:02:04 @agent_ppo2.py:186][0m |          -0.0103 |           2.8802 |           5.0991 |
[32m[20230113 20:02:05 @agent_ppo2.py:186][0m |          -0.0107 |           2.7137 |           5.1023 |
[32m[20230113 20:02:05 @agent_ppo2.py:186][0m |          -0.0118 |           2.6280 |           5.1000 |
[32m[20230113 20:02:05 @agent_ppo2.py:186][0m |          -0.0129 |           2.5599 |           5.0993 |
[32m[20230113 20:02:05 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.65
[32m[20230113 20:02:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.73
[32m[20230113 20:02:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.62
[32m[20230113 20:02:05 @agent_ppo2.py:144][0m Total time:      17.54 min
[32m[20230113 20:02:05 @agent_ppo2.py:146][0m 1581056 total steps have happened
[32m[20230113 20:02:05 @agent_ppo2.py:122][0m #------------------------ Iteration 772 --------------------------#
[32m[20230113 20:02:05 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:06 @agent_ppo2.py:186][0m |           0.0002 |           5.3421 |           5.1192 |
[32m[20230113 20:02:06 @agent_ppo2.py:186][0m |          -0.0092 |           4.3485 |           5.1022 |
[32m[20230113 20:02:06 @agent_ppo2.py:186][0m |          -0.0091 |           3.9786 |           5.1010 |
[32m[20230113 20:02:06 @agent_ppo2.py:186][0m |          -0.0132 |           3.7959 |           5.0998 |
[32m[20230113 20:02:06 @agent_ppo2.py:186][0m |          -0.0114 |           3.6104 |           5.0979 |
[32m[20230113 20:02:06 @agent_ppo2.py:186][0m |          -0.0154 |           3.5126 |           5.0925 |
[32m[20230113 20:02:06 @agent_ppo2.py:186][0m |          -0.0170 |           3.4405 |           5.0980 |
[32m[20230113 20:02:06 @agent_ppo2.py:186][0m |          -0.0194 |           3.3371 |           5.0982 |
[32m[20230113 20:02:06 @agent_ppo2.py:186][0m |          -0.0146 |           3.2783 |           5.0914 |
[32m[20230113 20:02:06 @agent_ppo2.py:186][0m |          -0.0155 |           3.2012 |           5.0904 |
[32m[20230113 20:02:06 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.94
[32m[20230113 20:02:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.94
[32m[20230113 20:02:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.87
[32m[20230113 20:02:06 @agent_ppo2.py:144][0m Total time:      17.56 min
[32m[20230113 20:02:06 @agent_ppo2.py:146][0m 1583104 total steps have happened
[32m[20230113 20:02:06 @agent_ppo2.py:122][0m #------------------------ Iteration 773 --------------------------#
[32m[20230113 20:02:07 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:07 @agent_ppo2.py:186][0m |           0.0071 |          13.2200 |           5.1530 |
[32m[20230113 20:02:07 @agent_ppo2.py:186][0m |          -0.0060 |           5.0983 |           5.1524 |
[32m[20230113 20:02:07 @agent_ppo2.py:186][0m |          -0.0099 |           4.3883 |           5.1529 |
[32m[20230113 20:02:07 @agent_ppo2.py:186][0m |          -0.0071 |           4.0646 |           5.1540 |
[32m[20230113 20:02:07 @agent_ppo2.py:186][0m |          -0.0097 |           3.7631 |           5.1528 |
[32m[20230113 20:02:07 @agent_ppo2.py:186][0m |          -0.0115 |           3.5791 |           5.1536 |
[32m[20230113 20:02:07 @agent_ppo2.py:186][0m |          -0.0110 |           3.4413 |           5.1538 |
[32m[20230113 20:02:07 @agent_ppo2.py:186][0m |          -0.0103 |           3.3241 |           5.1550 |
[32m[20230113 20:02:07 @agent_ppo2.py:186][0m |          -0.0147 |           3.1552 |           5.1556 |
[32m[20230113 20:02:07 @agent_ppo2.py:186][0m |          -0.0081 |           3.2107 |           5.1544 |
[32m[20230113 20:02:07 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 160.42
[32m[20230113 20:02:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.30
[32m[20230113 20:02:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.92
[32m[20230113 20:02:08 @agent_ppo2.py:144][0m Total time:      17.58 min
[32m[20230113 20:02:08 @agent_ppo2.py:146][0m 1585152 total steps have happened
[32m[20230113 20:02:08 @agent_ppo2.py:122][0m #------------------------ Iteration 774 --------------------------#
[32m[20230113 20:02:08 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:02:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:08 @agent_ppo2.py:186][0m |          -0.0015 |           6.0220 |           5.1107 |
[32m[20230113 20:02:08 @agent_ppo2.py:186][0m |          -0.0062 |           4.7970 |           5.1022 |
[32m[20230113 20:02:08 @agent_ppo2.py:186][0m |          -0.0076 |           4.3919 |           5.1008 |
[32m[20230113 20:02:08 @agent_ppo2.py:186][0m |          -0.0091 |           4.0765 |           5.0988 |
[32m[20230113 20:02:08 @agent_ppo2.py:186][0m |          -0.0086 |           3.7763 |           5.0979 |
[32m[20230113 20:02:08 @agent_ppo2.py:186][0m |          -0.0097 |           3.5117 |           5.0995 |
[32m[20230113 20:02:08 @agent_ppo2.py:186][0m |          -0.0100 |           3.3397 |           5.1042 |
[32m[20230113 20:02:08 @agent_ppo2.py:186][0m |          -0.0106 |           3.1677 |           5.0990 |
[32m[20230113 20:02:08 @agent_ppo2.py:186][0m |          -0.0119 |           3.0502 |           5.1031 |
[32m[20230113 20:02:08 @agent_ppo2.py:186][0m |          -0.0118 |           2.9896 |           5.1028 |
[32m[20230113 20:02:08 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:02:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.82
[32m[20230113 20:02:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.92
[32m[20230113 20:02:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.68
[32m[20230113 20:02:09 @agent_ppo2.py:144][0m Total time:      17.61 min
[32m[20230113 20:02:09 @agent_ppo2.py:146][0m 1587200 total steps have happened
[32m[20230113 20:02:09 @agent_ppo2.py:122][0m #------------------------ Iteration 775 --------------------------#
[32m[20230113 20:02:09 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:09 @agent_ppo2.py:186][0m |          -0.0001 |           6.6947 |           5.1277 |
[32m[20230113 20:02:09 @agent_ppo2.py:186][0m |          -0.0052 |           5.8851 |           5.1100 |
[32m[20230113 20:02:10 @agent_ppo2.py:186][0m |          -0.0063 |           5.5510 |           5.1187 |
[32m[20230113 20:02:10 @agent_ppo2.py:186][0m |          -0.0052 |           5.4282 |           5.1149 |
[32m[20230113 20:02:10 @agent_ppo2.py:186][0m |          -0.0093 |           5.1304 |           5.1126 |
[32m[20230113 20:02:10 @agent_ppo2.py:186][0m |          -0.0106 |           4.9718 |           5.1142 |
[32m[20230113 20:02:10 @agent_ppo2.py:186][0m |          -0.0085 |           4.9481 |           5.1136 |
[32m[20230113 20:02:10 @agent_ppo2.py:186][0m |          -0.0112 |           4.7738 |           5.1155 |
[32m[20230113 20:02:10 @agent_ppo2.py:186][0m |          -0.0104 |           4.7202 |           5.1156 |
[32m[20230113 20:02:10 @agent_ppo2.py:186][0m |          -0.0121 |           4.6098 |           5.1179 |
[32m[20230113 20:02:10 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.15
[32m[20230113 20:02:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.45
[32m[20230113 20:02:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.55
[32m[20230113 20:02:10 @agent_ppo2.py:144][0m Total time:      17.63 min
[32m[20230113 20:02:10 @agent_ppo2.py:146][0m 1589248 total steps have happened
[32m[20230113 20:02:10 @agent_ppo2.py:122][0m #------------------------ Iteration 776 --------------------------#
[32m[20230113 20:02:11 @agent_ppo2.py:128][0m Sampling time: 0.35 s by 1 slaves
[32m[20230113 20:02:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:11 @agent_ppo2.py:186][0m |          -0.0004 |          15.3180 |           5.0893 |
[32m[20230113 20:02:11 @agent_ppo2.py:186][0m |          -0.0096 |           6.8309 |           5.0852 |
[32m[20230113 20:02:11 @agent_ppo2.py:186][0m |          -0.0125 |           5.1030 |           5.0846 |
[32m[20230113 20:02:11 @agent_ppo2.py:186][0m |          -0.0111 |           4.6827 |           5.0899 |
[32m[20230113 20:02:11 @agent_ppo2.py:186][0m |          -0.0115 |           4.1834 |           5.0888 |
[32m[20230113 20:02:11 @agent_ppo2.py:186][0m |          -0.0082 |           3.9932 |           5.0894 |
[32m[20230113 20:02:11 @agent_ppo2.py:186][0m |          -0.0121 |           3.9281 |           5.0877 |
[32m[20230113 20:02:11 @agent_ppo2.py:186][0m |          -0.0189 |           3.6820 |           5.0848 |
[32m[20230113 20:02:11 @agent_ppo2.py:186][0m |          -0.0179 |           3.5498 |           5.0859 |
[32m[20230113 20:02:11 @agent_ppo2.py:186][0m |          -0.0158 |           3.5491 |           5.0862 |
[32m[20230113 20:02:11 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 20:02:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 121.49
[32m[20230113 20:02:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.25
[32m[20230113 20:02:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.91
[32m[20230113 20:02:11 @agent_ppo2.py:144][0m Total time:      17.65 min
[32m[20230113 20:02:11 @agent_ppo2.py:146][0m 1591296 total steps have happened
[32m[20230113 20:02:11 @agent_ppo2.py:122][0m #------------------------ Iteration 777 --------------------------#
[32m[20230113 20:02:12 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:12 @agent_ppo2.py:186][0m |          -0.0001 |           6.3318 |           5.1187 |
[32m[20230113 20:02:12 @agent_ppo2.py:186][0m |          -0.0102 |           5.7129 |           5.1200 |
[32m[20230113 20:02:12 @agent_ppo2.py:186][0m |          -0.0103 |           5.2096 |           5.1189 |
[32m[20230113 20:02:12 @agent_ppo2.py:186][0m |          -0.0092 |           4.9601 |           5.1225 |
[32m[20230113 20:02:12 @agent_ppo2.py:186][0m |          -0.0130 |           4.8334 |           5.1275 |
[32m[20230113 20:02:12 @agent_ppo2.py:186][0m |           0.0016 |           5.6581 |           5.1230 |
[32m[20230113 20:02:12 @agent_ppo2.py:186][0m |          -0.0132 |           4.6762 |           5.1263 |
[32m[20230113 20:02:12 @agent_ppo2.py:186][0m |          -0.0074 |           4.4884 |           5.1220 |
[32m[20230113 20:02:12 @agent_ppo2.py:186][0m |          -0.0054 |           4.4647 |           5.1226 |
[32m[20230113 20:02:12 @agent_ppo2.py:186][0m |          -0.0145 |           4.3659 |           5.1192 |
[32m[20230113 20:02:12 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.08
[32m[20230113 20:02:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.01
[32m[20230113 20:02:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.33
[32m[20230113 20:02:13 @agent_ppo2.py:144][0m Total time:      17.67 min
[32m[20230113 20:02:13 @agent_ppo2.py:146][0m 1593344 total steps have happened
[32m[20230113 20:02:13 @agent_ppo2.py:122][0m #------------------------ Iteration 778 --------------------------#
[32m[20230113 20:02:13 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:13 @agent_ppo2.py:186][0m |          -0.0022 |           6.7931 |           5.1862 |
[32m[20230113 20:02:13 @agent_ppo2.py:186][0m |          -0.0055 |           5.6324 |           5.1719 |
[32m[20230113 20:02:13 @agent_ppo2.py:186][0m |          -0.0045 |           5.2257 |           5.1713 |
[32m[20230113 20:02:13 @agent_ppo2.py:186][0m |          -0.0109 |           4.8388 |           5.1684 |
[32m[20230113 20:02:13 @agent_ppo2.py:186][0m |          -0.0093 |           4.6234 |           5.1709 |
[32m[20230113 20:02:13 @agent_ppo2.py:186][0m |          -0.0108 |           4.5045 |           5.1666 |
[32m[20230113 20:02:13 @agent_ppo2.py:186][0m |          -0.0126 |           4.3950 |           5.1675 |
[32m[20230113 20:02:13 @agent_ppo2.py:186][0m |          -0.0107 |           4.2591 |           5.1674 |
[32m[20230113 20:02:13 @agent_ppo2.py:186][0m |          -0.0104 |           4.1834 |           5.1697 |
[32m[20230113 20:02:14 @agent_ppo2.py:186][0m |          -0.0158 |           4.0735 |           5.1686 |
[32m[20230113 20:02:14 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.02
[32m[20230113 20:02:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.51
[32m[20230113 20:02:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.89
[32m[20230113 20:02:14 @agent_ppo2.py:144][0m Total time:      17.69 min
[32m[20230113 20:02:14 @agent_ppo2.py:146][0m 1595392 total steps have happened
[32m[20230113 20:02:14 @agent_ppo2.py:122][0m #------------------------ Iteration 779 --------------------------#
[32m[20230113 20:02:14 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:14 @agent_ppo2.py:186][0m |          -0.0011 |           6.2111 |           5.2350 |
[32m[20230113 20:02:14 @agent_ppo2.py:186][0m |          -0.0056 |           5.1517 |           5.2232 |
[32m[20230113 20:02:15 @agent_ppo2.py:186][0m |          -0.0078 |           4.8010 |           5.2255 |
[32m[20230113 20:02:15 @agent_ppo2.py:186][0m |          -0.0095 |           4.4933 |           5.2268 |
[32m[20230113 20:02:15 @agent_ppo2.py:186][0m |          -0.0100 |           4.2798 |           5.2262 |
[32m[20230113 20:02:15 @agent_ppo2.py:186][0m |          -0.0108 |           4.1231 |           5.2292 |
[32m[20230113 20:02:15 @agent_ppo2.py:186][0m |          -0.0104 |           4.0158 |           5.2241 |
[32m[20230113 20:02:15 @agent_ppo2.py:186][0m |          -0.0108 |           3.8680 |           5.2245 |
[32m[20230113 20:02:15 @agent_ppo2.py:186][0m |          -0.0131 |           3.7098 |           5.2256 |
[32m[20230113 20:02:15 @agent_ppo2.py:186][0m |          -0.0138 |           3.5601 |           5.2251 |
[32m[20230113 20:02:15 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.02
[32m[20230113 20:02:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.83
[32m[20230113 20:02:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.52
[32m[20230113 20:02:15 @agent_ppo2.py:144][0m Total time:      17.71 min
[32m[20230113 20:02:15 @agent_ppo2.py:146][0m 1597440 total steps have happened
[32m[20230113 20:02:15 @agent_ppo2.py:122][0m #------------------------ Iteration 780 --------------------------#
[32m[20230113 20:02:16 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 20:02:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:16 @agent_ppo2.py:186][0m |           0.0003 |          31.4848 |           5.3317 |
[32m[20230113 20:02:16 @agent_ppo2.py:186][0m |          -0.0061 |          12.6149 |           5.3304 |
[32m[20230113 20:02:16 @agent_ppo2.py:186][0m |          -0.0066 |          10.8464 |           5.3306 |
[32m[20230113 20:02:16 @agent_ppo2.py:186][0m |          -0.0094 |           9.7149 |           5.3275 |
[32m[20230113 20:02:16 @agent_ppo2.py:186][0m |          -0.0100 |           8.8137 |           5.3302 |
[32m[20230113 20:02:16 @agent_ppo2.py:186][0m |          -0.0148 |           8.3929 |           5.3285 |
[32m[20230113 20:02:16 @agent_ppo2.py:186][0m |          -0.0162 |           7.6933 |           5.3294 |
[32m[20230113 20:02:16 @agent_ppo2.py:186][0m |          -0.0184 |           7.3131 |           5.3309 |
[32m[20230113 20:02:16 @agent_ppo2.py:186][0m |          -0.0181 |           7.0837 |           5.3312 |
[32m[20230113 20:02:16 @agent_ppo2.py:186][0m |          -0.0195 |           6.6957 |           5.3308 |
[32m[20230113 20:02:16 @agent_ppo2.py:131][0m Policy update time: 0.34 s
[32m[20230113 20:02:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 96.57
[32m[20230113 20:02:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 216.29
[32m[20230113 20:02:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.91
[32m[20230113 20:02:16 @agent_ppo2.py:144][0m Total time:      17.73 min
[32m[20230113 20:02:16 @agent_ppo2.py:146][0m 1599488 total steps have happened
[32m[20230113 20:02:16 @agent_ppo2.py:122][0m #------------------------ Iteration 781 --------------------------#
[32m[20230113 20:02:17 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:02:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:17 @agent_ppo2.py:186][0m |          -0.0013 |           5.7845 |           5.2877 |
[32m[20230113 20:02:17 @agent_ppo2.py:186][0m |          -0.0087 |           4.9000 |           5.2817 |
[32m[20230113 20:02:17 @agent_ppo2.py:186][0m |          -0.0105 |           4.4853 |           5.2802 |
[32m[20230113 20:02:17 @agent_ppo2.py:186][0m |          -0.0127 |           4.2078 |           5.2773 |
[32m[20230113 20:02:17 @agent_ppo2.py:186][0m |          -0.0105 |           4.0862 |           5.2707 |
[32m[20230113 20:02:17 @agent_ppo2.py:186][0m |          -0.0133 |           3.8985 |           5.2782 |
[32m[20230113 20:02:17 @agent_ppo2.py:186][0m |          -0.0123 |           3.8121 |           5.2761 |
[32m[20230113 20:02:17 @agent_ppo2.py:186][0m |          -0.0137 |           3.6835 |           5.2748 |
[32m[20230113 20:02:17 @agent_ppo2.py:186][0m |          -0.0154 |           3.6080 |           5.2749 |
[32m[20230113 20:02:17 @agent_ppo2.py:186][0m |          -0.0162 |           3.5310 |           5.2729 |
[32m[20230113 20:02:17 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.87
[32m[20230113 20:02:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.16
[32m[20230113 20:02:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.37
[32m[20230113 20:02:18 @agent_ppo2.py:144][0m Total time:      17.75 min
[32m[20230113 20:02:18 @agent_ppo2.py:146][0m 1601536 total steps have happened
[32m[20230113 20:02:18 @agent_ppo2.py:122][0m #------------------------ Iteration 782 --------------------------#
[32m[20230113 20:02:18 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:18 @agent_ppo2.py:186][0m |           0.0016 |           5.9743 |           5.3022 |
[32m[20230113 20:02:18 @agent_ppo2.py:186][0m |          -0.0039 |           5.4285 |           5.3029 |
[32m[20230113 20:02:18 @agent_ppo2.py:186][0m |          -0.0074 |           5.0558 |           5.3005 |
[32m[20230113 20:02:18 @agent_ppo2.py:186][0m |          -0.0096 |           4.8490 |           5.3014 |
[32m[20230113 20:02:18 @agent_ppo2.py:186][0m |          -0.0131 |           4.7143 |           5.2962 |
[32m[20230113 20:02:18 @agent_ppo2.py:186][0m |          -0.0110 |           4.7152 |           5.2958 |
[32m[20230113 20:02:18 @agent_ppo2.py:186][0m |          -0.0098 |           4.5141 |           5.2916 |
[32m[20230113 20:02:18 @agent_ppo2.py:186][0m |          -0.0126 |           4.3907 |           5.2934 |
[32m[20230113 20:02:19 @agent_ppo2.py:186][0m |          -0.0147 |           4.3200 |           5.2932 |
[32m[20230113 20:02:19 @agent_ppo2.py:186][0m |          -0.0153 |           4.2933 |           5.2928 |
[32m[20230113 20:02:19 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.67
[32m[20230113 20:02:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.62
[32m[20230113 20:02:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.27
[32m[20230113 20:02:19 @agent_ppo2.py:144][0m Total time:      17.77 min
[32m[20230113 20:02:19 @agent_ppo2.py:146][0m 1603584 total steps have happened
[32m[20230113 20:02:19 @agent_ppo2.py:122][0m #------------------------ Iteration 783 --------------------------#
[32m[20230113 20:02:19 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:02:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:19 @agent_ppo2.py:186][0m |          -0.0002 |           5.9952 |           5.2209 |
[32m[20230113 20:02:20 @agent_ppo2.py:186][0m |          -0.0021 |           5.3566 |           5.2133 |
[32m[20230113 20:02:20 @agent_ppo2.py:186][0m |          -0.0042 |           5.1396 |           5.2105 |
[32m[20230113 20:02:20 @agent_ppo2.py:186][0m |          -0.0102 |           4.7849 |           5.2111 |
[32m[20230113 20:02:20 @agent_ppo2.py:186][0m |          -0.0073 |           4.5525 |           5.2140 |
[32m[20230113 20:02:20 @agent_ppo2.py:186][0m |          -0.0075 |           4.5704 |           5.2155 |
[32m[20230113 20:02:20 @agent_ppo2.py:186][0m |          -0.0088 |           4.3041 |           5.2160 |
[32m[20230113 20:02:20 @agent_ppo2.py:186][0m |          -0.0121 |           4.1883 |           5.2159 |
[32m[20230113 20:02:20 @agent_ppo2.py:186][0m |          -0.0114 |           4.0903 |           5.2208 |
[32m[20230113 20:02:20 @agent_ppo2.py:186][0m |          -0.0059 |           4.2771 |           5.2208 |
[32m[20230113 20:02:20 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:02:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.22
[32m[20230113 20:02:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.28
[32m[20230113 20:02:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.30
[32m[20230113 20:02:20 @agent_ppo2.py:144][0m Total time:      17.80 min
[32m[20230113 20:02:20 @agent_ppo2.py:146][0m 1605632 total steps have happened
[32m[20230113 20:02:20 @agent_ppo2.py:122][0m #------------------------ Iteration 784 --------------------------#
[32m[20230113 20:02:21 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:02:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:21 @agent_ppo2.py:186][0m |          -0.0031 |           5.8074 |           5.4687 |
[32m[20230113 20:02:21 @agent_ppo2.py:186][0m |          -0.0066 |           5.0252 |           5.4651 |
[32m[20230113 20:02:21 @agent_ppo2.py:186][0m |          -0.0057 |           4.6166 |           5.4628 |
[32m[20230113 20:02:21 @agent_ppo2.py:186][0m |          -0.0102 |           4.3620 |           5.4659 |
[32m[20230113 20:02:21 @agent_ppo2.py:186][0m |          -0.0114 |           4.2331 |           5.4651 |
[32m[20230113 20:02:21 @agent_ppo2.py:186][0m |          -0.0102 |           4.1710 |           5.4666 |
[32m[20230113 20:02:21 @agent_ppo2.py:186][0m |          -0.0137 |           4.0203 |           5.4646 |
[32m[20230113 20:02:21 @agent_ppo2.py:186][0m |          -0.0139 |           4.0081 |           5.4675 |
[32m[20230113 20:02:21 @agent_ppo2.py:186][0m |          -0.0108 |           4.0804 |           5.4661 |
[32m[20230113 20:02:21 @agent_ppo2.py:186][0m |          -0.0148 |           3.8197 |           5.4659 |
[32m[20230113 20:02:21 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.77
[32m[20230113 20:02:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.46
[32m[20230113 20:02:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.33
[32m[20230113 20:02:22 @agent_ppo2.py:144][0m Total time:      17.82 min
[32m[20230113 20:02:22 @agent_ppo2.py:146][0m 1607680 total steps have happened
[32m[20230113 20:02:22 @agent_ppo2.py:122][0m #------------------------ Iteration 785 --------------------------#
[32m[20230113 20:02:22 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:22 @agent_ppo2.py:186][0m |          -0.0022 |           6.3993 |           5.3576 |
[32m[20230113 20:02:22 @agent_ppo2.py:186][0m |          -0.0055 |           5.4857 |           5.3527 |
[32m[20230113 20:02:22 @agent_ppo2.py:186][0m |          -0.0076 |           5.1952 |           5.3541 |
[32m[20230113 20:02:22 @agent_ppo2.py:186][0m |          -0.0101 |           4.9649 |           5.3555 |
[32m[20230113 20:02:22 @agent_ppo2.py:186][0m |          -0.0105 |           4.8070 |           5.3538 |
[32m[20230113 20:02:22 @agent_ppo2.py:186][0m |          -0.0087 |           4.8023 |           5.3543 |
[32m[20230113 20:02:22 @agent_ppo2.py:186][0m |          -0.0105 |           4.6483 |           5.3547 |
[32m[20230113 20:02:22 @agent_ppo2.py:186][0m |          -0.0113 |           4.5684 |           5.3554 |
[32m[20230113 20:02:22 @agent_ppo2.py:186][0m |          -0.0134 |           4.5022 |           5.3546 |
[32m[20230113 20:02:23 @agent_ppo2.py:186][0m |          -0.0135 |           4.4678 |           5.3536 |
[32m[20230113 20:02:23 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.19
[32m[20230113 20:02:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.51
[32m[20230113 20:02:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.58
[32m[20230113 20:02:23 @agent_ppo2.py:144][0m Total time:      17.84 min
[32m[20230113 20:02:23 @agent_ppo2.py:146][0m 1609728 total steps have happened
[32m[20230113 20:02:23 @agent_ppo2.py:122][0m #------------------------ Iteration 786 --------------------------#
[32m[20230113 20:02:23 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:23 @agent_ppo2.py:186][0m |          -0.0001 |           5.3870 |           5.3602 |
[32m[20230113 20:02:24 @agent_ppo2.py:186][0m |          -0.0051 |           4.3386 |           5.3534 |
[32m[20230113 20:02:24 @agent_ppo2.py:186][0m |          -0.0076 |           3.5467 |           5.3594 |
[32m[20230113 20:02:24 @agent_ppo2.py:186][0m |          -0.0065 |           3.0860 |           5.3592 |
[32m[20230113 20:02:24 @agent_ppo2.py:186][0m |          -0.0089 |           2.7926 |           5.3583 |
[32m[20230113 20:02:24 @agent_ppo2.py:186][0m |          -0.0093 |           2.5217 |           5.3591 |
[32m[20230113 20:02:24 @agent_ppo2.py:186][0m |          -0.0077 |           2.4206 |           5.3594 |
[32m[20230113 20:02:24 @agent_ppo2.py:186][0m |          -0.0110 |           2.2935 |           5.3597 |
[32m[20230113 20:02:24 @agent_ppo2.py:186][0m |          -0.0100 |           2.2454 |           5.3597 |
[32m[20230113 20:02:24 @agent_ppo2.py:186][0m |          -0.0106 |           2.1526 |           5.3589 |
[32m[20230113 20:02:24 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:02:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.67
[32m[20230113 20:02:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.57
[32m[20230113 20:02:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.29
[32m[20230113 20:02:24 @agent_ppo2.py:144][0m Total time:      17.86 min
[32m[20230113 20:02:24 @agent_ppo2.py:146][0m 1611776 total steps have happened
[32m[20230113 20:02:24 @agent_ppo2.py:122][0m #------------------------ Iteration 787 --------------------------#
[32m[20230113 20:02:25 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:25 @agent_ppo2.py:186][0m |           0.0013 |           5.1681 |           5.2846 |
[32m[20230113 20:02:25 @agent_ppo2.py:186][0m |          -0.0060 |           4.4983 |           5.2857 |
[32m[20230113 20:02:25 @agent_ppo2.py:186][0m |          -0.0054 |           4.2111 |           5.2833 |
[32m[20230113 20:02:25 @agent_ppo2.py:186][0m |          -0.0089 |           4.0128 |           5.2788 |
[32m[20230113 20:02:25 @agent_ppo2.py:186][0m |          -0.0070 |           3.9866 |           5.2812 |
[32m[20230113 20:02:25 @agent_ppo2.py:186][0m |          -0.0085 |           3.7233 |           5.2781 |
[32m[20230113 20:02:25 @agent_ppo2.py:186][0m |          -0.0110 |           3.6294 |           5.2789 |
[32m[20230113 20:02:25 @agent_ppo2.py:186][0m |          -0.0140 |           3.5681 |           5.2810 |
[32m[20230113 20:02:25 @agent_ppo2.py:186][0m |          -0.0121 |           3.4591 |           5.2820 |
[32m[20230113 20:02:25 @agent_ppo2.py:186][0m |          -0.0124 |           3.4268 |           5.2802 |
[32m[20230113 20:02:25 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:02:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.35
[32m[20230113 20:02:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.60
[32m[20230113 20:02:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.40
[32m[20230113 20:02:26 @agent_ppo2.py:144][0m Total time:      17.88 min
[32m[20230113 20:02:26 @agent_ppo2.py:146][0m 1613824 total steps have happened
[32m[20230113 20:02:26 @agent_ppo2.py:122][0m #------------------------ Iteration 788 --------------------------#
[32m[20230113 20:02:26 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:26 @agent_ppo2.py:186][0m |          -0.0038 |           6.3594 |           5.2470 |
[32m[20230113 20:02:26 @agent_ppo2.py:186][0m |          -0.0078 |           4.9862 |           5.2373 |
[32m[20230113 20:02:26 @agent_ppo2.py:186][0m |          -0.0104 |           4.5178 |           5.2318 |
[32m[20230113 20:02:26 @agent_ppo2.py:186][0m |          -0.0098 |           4.2998 |           5.2259 |
[32m[20230113 20:02:26 @agent_ppo2.py:186][0m |           0.0051 |           4.1487 |           5.2243 |
[32m[20230113 20:02:26 @agent_ppo2.py:186][0m |          -0.0125 |           3.7979 |           5.2248 |
[32m[20230113 20:02:26 @agent_ppo2.py:186][0m |          -0.0154 |           3.6185 |           5.2223 |
[32m[20230113 20:02:26 @agent_ppo2.py:186][0m |          -0.0173 |           3.5511 |           5.2233 |
[32m[20230113 20:02:26 @agent_ppo2.py:186][0m |          -0.0160 |           3.4449 |           5.2202 |
[32m[20230113 20:02:27 @agent_ppo2.py:186][0m |          -0.0165 |           3.3439 |           5.2191 |
[32m[20230113 20:02:27 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.90
[32m[20230113 20:02:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.70
[32m[20230113 20:02:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.20
[32m[20230113 20:02:27 @agent_ppo2.py:144][0m Total time:      17.91 min
[32m[20230113 20:02:27 @agent_ppo2.py:146][0m 1615872 total steps have happened
[32m[20230113 20:02:27 @agent_ppo2.py:122][0m #------------------------ Iteration 789 --------------------------#
[32m[20230113 20:02:27 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:27 @agent_ppo2.py:186][0m |          -0.0014 |           5.9323 |           5.3152 |
[32m[20230113 20:02:28 @agent_ppo2.py:186][0m |          -0.0061 |           5.2057 |           5.3079 |
[32m[20230113 20:02:28 @agent_ppo2.py:186][0m |          -0.0074 |           4.8539 |           5.3072 |
[32m[20230113 20:02:28 @agent_ppo2.py:186][0m |          -0.0094 |           4.6857 |           5.3089 |
[32m[20230113 20:02:28 @agent_ppo2.py:186][0m |          -0.0089 |           4.6023 |           5.3056 |
[32m[20230113 20:02:28 @agent_ppo2.py:186][0m |          -0.0090 |           4.3445 |           5.3066 |
[32m[20230113 20:02:28 @agent_ppo2.py:186][0m |          -0.0108 |           4.3059 |           5.3100 |
[32m[20230113 20:02:28 @agent_ppo2.py:186][0m |          -0.0109 |           4.1441 |           5.3053 |
[32m[20230113 20:02:28 @agent_ppo2.py:186][0m |          -0.0103 |           4.1135 |           5.3106 |
[32m[20230113 20:02:28 @agent_ppo2.py:186][0m |          -0.0113 |           3.9964 |           5.3017 |
[32m[20230113 20:02:28 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.87
[32m[20230113 20:02:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.18
[32m[20230113 20:02:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.71
[32m[20230113 20:02:28 @agent_ppo2.py:144][0m Total time:      17.93 min
[32m[20230113 20:02:28 @agent_ppo2.py:146][0m 1617920 total steps have happened
[32m[20230113 20:02:28 @agent_ppo2.py:122][0m #------------------------ Iteration 790 --------------------------#
[32m[20230113 20:02:29 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:02:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:29 @agent_ppo2.py:186][0m |           0.0009 |           5.8231 |           5.3727 |
[32m[20230113 20:02:29 @agent_ppo2.py:186][0m |          -0.0025 |           5.0790 |           5.3686 |
[32m[20230113 20:02:29 @agent_ppo2.py:186][0m |          -0.0048 |           4.6771 |           5.3663 |
[32m[20230113 20:02:29 @agent_ppo2.py:186][0m |          -0.0064 |           4.4534 |           5.3609 |
[32m[20230113 20:02:29 @agent_ppo2.py:186][0m |          -0.0075 |           4.2465 |           5.3605 |
[32m[20230113 20:02:29 @agent_ppo2.py:186][0m |          -0.0094 |           4.0993 |           5.3569 |
[32m[20230113 20:02:29 @agent_ppo2.py:186][0m |          -0.0100 |           3.9879 |           5.3528 |
[32m[20230113 20:02:29 @agent_ppo2.py:186][0m |          -0.0113 |           3.8780 |           5.3515 |
[32m[20230113 20:02:29 @agent_ppo2.py:186][0m |          -0.0121 |           3.8061 |           5.3501 |
[32m[20230113 20:02:29 @agent_ppo2.py:186][0m |          -0.0125 |           3.7464 |           5.3494 |
[32m[20230113 20:02:29 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:02:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.95
[32m[20230113 20:02:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.76
[32m[20230113 20:02:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.10
[32m[20230113 20:02:30 @agent_ppo2.py:144][0m Total time:      17.95 min
[32m[20230113 20:02:30 @agent_ppo2.py:146][0m 1619968 total steps have happened
[32m[20230113 20:02:30 @agent_ppo2.py:122][0m #------------------------ Iteration 791 --------------------------#
[32m[20230113 20:02:30 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:02:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:30 @agent_ppo2.py:186][0m |          -0.0022 |           5.9976 |           5.2839 |
[32m[20230113 20:02:30 @agent_ppo2.py:186][0m |          -0.0067 |           5.1356 |           5.2714 |
[32m[20230113 20:02:30 @agent_ppo2.py:186][0m |          -0.0104 |           4.7251 |           5.2691 |
[32m[20230113 20:02:30 @agent_ppo2.py:186][0m |          -0.0127 |           4.4250 |           5.2695 |
[32m[20230113 20:02:30 @agent_ppo2.py:186][0m |          -0.0101 |           4.3659 |           5.2682 |
[32m[20230113 20:02:30 @agent_ppo2.py:186][0m |          -0.0128 |           4.1822 |           5.2618 |
[32m[20230113 20:02:30 @agent_ppo2.py:186][0m |          -0.0147 |           3.9983 |           5.2673 |
[32m[20230113 20:02:30 @agent_ppo2.py:186][0m |          -0.0136 |           3.9352 |           5.2648 |
[32m[20230113 20:02:30 @agent_ppo2.py:186][0m |          -0.0157 |           3.7915 |           5.2591 |
[32m[20230113 20:02:31 @agent_ppo2.py:186][0m |          -0.0165 |           3.7386 |           5.2643 |
[32m[20230113 20:02:31 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:02:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.91
[32m[20230113 20:02:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.51
[32m[20230113 20:02:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.98
[32m[20230113 20:02:31 @agent_ppo2.py:144][0m Total time:      17.97 min
[32m[20230113 20:02:31 @agent_ppo2.py:146][0m 1622016 total steps have happened
[32m[20230113 20:02:31 @agent_ppo2.py:122][0m #------------------------ Iteration 792 --------------------------#
[32m[20230113 20:02:31 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:31 @agent_ppo2.py:186][0m |          -0.0027 |           6.5191 |           5.3443 |
[32m[20230113 20:02:32 @agent_ppo2.py:186][0m |           0.0058 |           5.8684 |           5.3340 |
[32m[20230113 20:02:32 @agent_ppo2.py:186][0m |          -0.0068 |           5.4282 |           5.3256 |
[32m[20230113 20:02:32 @agent_ppo2.py:186][0m |          -0.0100 |           5.1878 |           5.3317 |
[32m[20230113 20:02:32 @agent_ppo2.py:186][0m |          -0.0104 |           4.9575 |           5.3318 |
[32m[20230113 20:02:32 @agent_ppo2.py:186][0m |          -0.0119 |           4.7657 |           5.3316 |
[32m[20230113 20:02:32 @agent_ppo2.py:186][0m |          -0.0152 |           4.6352 |           5.3345 |
[32m[20230113 20:02:32 @agent_ppo2.py:186][0m |          -0.0159 |           4.5248 |           5.3307 |
[32m[20230113 20:02:32 @agent_ppo2.py:186][0m |          -0.0145 |           4.3867 |           5.3302 |
[32m[20230113 20:02:32 @agent_ppo2.py:186][0m |          -0.0075 |           4.2955 |           5.3286 |
[32m[20230113 20:02:32 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.28
[32m[20230113 20:02:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.68
[32m[20230113 20:02:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.67
[32m[20230113 20:02:32 @agent_ppo2.py:144][0m Total time:      18.00 min
[32m[20230113 20:02:32 @agent_ppo2.py:146][0m 1624064 total steps have happened
[32m[20230113 20:02:32 @agent_ppo2.py:122][0m #------------------------ Iteration 793 --------------------------#
[32m[20230113 20:02:33 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:33 @agent_ppo2.py:186][0m |           0.0010 |           6.2147 |           5.4059 |
[32m[20230113 20:02:33 @agent_ppo2.py:186][0m |          -0.0056 |           4.3871 |           5.3997 |
[32m[20230113 20:02:33 @agent_ppo2.py:186][0m |          -0.0086 |           3.8030 |           5.4021 |
[32m[20230113 20:02:33 @agent_ppo2.py:186][0m |          -0.0091 |           3.4513 |           5.3972 |
[32m[20230113 20:02:33 @agent_ppo2.py:186][0m |          -0.0099 |           3.2276 |           5.3953 |
[32m[20230113 20:02:33 @agent_ppo2.py:186][0m |          -0.0134 |           3.0669 |           5.3937 |
[32m[20230113 20:02:33 @agent_ppo2.py:186][0m |          -0.0130 |           2.9787 |           5.3949 |
[32m[20230113 20:02:33 @agent_ppo2.py:186][0m |          -0.0106 |           2.9158 |           5.3912 |
[32m[20230113 20:02:33 @agent_ppo2.py:186][0m |          -0.0075 |           2.9178 |           5.3929 |
[32m[20230113 20:02:33 @agent_ppo2.py:186][0m |          -0.0166 |           2.7793 |           5.3889 |
[32m[20230113 20:02:33 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.59
[32m[20230113 20:02:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.34
[32m[20230113 20:02:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.66
[32m[20230113 20:02:34 @agent_ppo2.py:144][0m Total time:      18.02 min
[32m[20230113 20:02:34 @agent_ppo2.py:146][0m 1626112 total steps have happened
[32m[20230113 20:02:34 @agent_ppo2.py:122][0m #------------------------ Iteration 794 --------------------------#
[32m[20230113 20:02:34 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:34 @agent_ppo2.py:186][0m |           0.0001 |           5.8112 |           5.3554 |
[32m[20230113 20:02:34 @agent_ppo2.py:186][0m |          -0.0050 |           4.8776 |           5.3493 |
[32m[20230113 20:02:34 @agent_ppo2.py:186][0m |          -0.0071 |           4.5325 |           5.3465 |
[32m[20230113 20:02:34 @agent_ppo2.py:186][0m |          -0.0087 |           4.2740 |           5.3460 |
[32m[20230113 20:02:34 @agent_ppo2.py:186][0m |          -0.0097 |           4.1242 |           5.3468 |
[32m[20230113 20:02:34 @agent_ppo2.py:186][0m |          -0.0106 |           3.9791 |           5.3482 |
[32m[20230113 20:02:34 @agent_ppo2.py:186][0m |          -0.0113 |           3.9001 |           5.3447 |
[32m[20230113 20:02:34 @agent_ppo2.py:186][0m |          -0.0121 |           3.8039 |           5.3476 |
[32m[20230113 20:02:34 @agent_ppo2.py:186][0m |          -0.0131 |           3.7402 |           5.3442 |
[32m[20230113 20:02:34 @agent_ppo2.py:186][0m |          -0.0136 |           3.6603 |           5.3496 |
[32m[20230113 20:02:34 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.04
[32m[20230113 20:02:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.70
[32m[20230113 20:02:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.04
[32m[20230113 20:02:35 @agent_ppo2.py:144][0m Total time:      18.04 min
[32m[20230113 20:02:35 @agent_ppo2.py:146][0m 1628160 total steps have happened
[32m[20230113 20:02:35 @agent_ppo2.py:122][0m #------------------------ Iteration 795 --------------------------#
[32m[20230113 20:02:35 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:02:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:35 @agent_ppo2.py:186][0m |           0.0027 |           5.8534 |           5.2962 |
[32m[20230113 20:02:35 @agent_ppo2.py:186][0m |          -0.0106 |           4.8390 |           5.2799 |
[32m[20230113 20:02:36 @agent_ppo2.py:186][0m |          -0.0091 |           4.4195 |           5.2810 |
[32m[20230113 20:02:36 @agent_ppo2.py:186][0m |          -0.0119 |           4.1135 |           5.2841 |
[32m[20230113 20:02:36 @agent_ppo2.py:186][0m |          -0.0143 |           3.9482 |           5.2884 |
[32m[20230113 20:02:36 @agent_ppo2.py:186][0m |          -0.0104 |           3.8266 |           5.2894 |
[32m[20230113 20:02:36 @agent_ppo2.py:186][0m |          -0.0131 |           3.6875 |           5.2849 |
[32m[20230113 20:02:36 @agent_ppo2.py:186][0m |          -0.0153 |           3.5764 |           5.2886 |
[32m[20230113 20:02:36 @agent_ppo2.py:186][0m |          -0.0139 |           3.6094 |           5.2892 |
[32m[20230113 20:02:36 @agent_ppo2.py:186][0m |          -0.0155 |           3.4258 |           5.2901 |
[32m[20230113 20:02:36 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:02:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.19
[32m[20230113 20:02:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.70
[32m[20230113 20:02:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.40
[32m[20230113 20:02:36 @agent_ppo2.py:144][0m Total time:      18.06 min
[32m[20230113 20:02:36 @agent_ppo2.py:146][0m 1630208 total steps have happened
[32m[20230113 20:02:36 @agent_ppo2.py:122][0m #------------------------ Iteration 796 --------------------------#
[32m[20230113 20:02:37 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:37 @agent_ppo2.py:186][0m |           0.0029 |           5.5894 |           5.4709 |
[32m[20230113 20:02:37 @agent_ppo2.py:186][0m |           0.0068 |           4.6958 |           5.4642 |
[32m[20230113 20:02:37 @agent_ppo2.py:186][0m |           0.0096 |           4.3824 |           5.4610 |
[32m[20230113 20:02:37 @agent_ppo2.py:186][0m |          -0.0148 |           4.1354 |           5.4570 |
[32m[20230113 20:02:37 @agent_ppo2.py:186][0m |          -0.0087 |           3.9770 |           5.4574 |
[32m[20230113 20:02:37 @agent_ppo2.py:186][0m |          -0.0105 |           3.8150 |           5.4571 |
[32m[20230113 20:02:37 @agent_ppo2.py:186][0m |          -0.0015 |           3.7078 |           5.4570 |
[32m[20230113 20:02:37 @agent_ppo2.py:186][0m |          -0.0096 |           3.6156 |           5.4497 |
[32m[20230113 20:02:37 @agent_ppo2.py:186][0m |           0.0217 |           3.6265 |           5.4522 |
[32m[20230113 20:02:37 @agent_ppo2.py:186][0m |          -0.0346 |           3.6422 |           5.4493 |
[32m[20230113 20:02:37 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.68
[32m[20230113 20:02:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.96
[32m[20230113 20:02:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.05
[32m[20230113 20:02:38 @agent_ppo2.py:144][0m Total time:      18.08 min
[32m[20230113 20:02:38 @agent_ppo2.py:146][0m 1632256 total steps have happened
[32m[20230113 20:02:38 @agent_ppo2.py:122][0m #------------------------ Iteration 797 --------------------------#
[32m[20230113 20:02:38 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:38 @agent_ppo2.py:186][0m |           0.0006 |           5.1450 |           5.5751 |
[32m[20230113 20:02:38 @agent_ppo2.py:186][0m |          -0.0066 |           4.3979 |           5.5733 |
[32m[20230113 20:02:38 @agent_ppo2.py:186][0m |          -0.0093 |           4.0532 |           5.5644 |
[32m[20230113 20:02:38 @agent_ppo2.py:186][0m |          -0.0103 |           3.8237 |           5.5591 |
[32m[20230113 20:02:38 @agent_ppo2.py:186][0m |          -0.0124 |           3.6416 |           5.5635 |
[32m[20230113 20:02:38 @agent_ppo2.py:186][0m |          -0.0138 |           3.5020 |           5.5598 |
[32m[20230113 20:02:38 @agent_ppo2.py:186][0m |          -0.0142 |           3.3890 |           5.5585 |
[32m[20230113 20:02:38 @agent_ppo2.py:186][0m |          -0.0146 |           3.3221 |           5.5584 |
[32m[20230113 20:02:38 @agent_ppo2.py:186][0m |          -0.0160 |           3.2227 |           5.5567 |
[32m[20230113 20:02:38 @agent_ppo2.py:186][0m |          -0.0161 |           3.1632 |           5.5546 |
[32m[20230113 20:02:38 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.05
[32m[20230113 20:02:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.83
[32m[20230113 20:02:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.64
[32m[20230113 20:02:39 @agent_ppo2.py:144][0m Total time:      18.10 min
[32m[20230113 20:02:39 @agent_ppo2.py:146][0m 1634304 total steps have happened
[32m[20230113 20:02:39 @agent_ppo2.py:122][0m #------------------------ Iteration 798 --------------------------#
[32m[20230113 20:02:39 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:39 @agent_ppo2.py:186][0m |          -0.0008 |           5.7057 |           5.5598 |
[32m[20230113 20:02:39 @agent_ppo2.py:186][0m |          -0.0063 |           4.6151 |           5.5508 |
[32m[20230113 20:02:39 @agent_ppo2.py:186][0m |          -0.0088 |           4.1006 |           5.5467 |
[32m[20230113 20:02:40 @agent_ppo2.py:186][0m |          -0.0105 |           3.9128 |           5.5450 |
[32m[20230113 20:02:40 @agent_ppo2.py:186][0m |          -0.0121 |           3.7162 |           5.5468 |
[32m[20230113 20:02:40 @agent_ppo2.py:186][0m |          -0.0122 |           3.6190 |           5.5482 |
[32m[20230113 20:02:40 @agent_ppo2.py:186][0m |          -0.0149 |           3.4695 |           5.5494 |
[32m[20230113 20:02:40 @agent_ppo2.py:186][0m |          -0.0139 |           3.3701 |           5.5510 |
[32m[20230113 20:02:40 @agent_ppo2.py:186][0m |          -0.0146 |           3.3139 |           5.5528 |
[32m[20230113 20:02:40 @agent_ppo2.py:186][0m |          -0.0163 |           3.2095 |           5.5541 |
[32m[20230113 20:02:40 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.02
[32m[20230113 20:02:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.14
[32m[20230113 20:02:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.06
[32m[20230113 20:02:40 @agent_ppo2.py:144][0m Total time:      18.13 min
[32m[20230113 20:02:40 @agent_ppo2.py:146][0m 1636352 total steps have happened
[32m[20230113 20:02:40 @agent_ppo2.py:122][0m #------------------------ Iteration 799 --------------------------#
[32m[20230113 20:02:41 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:41 @agent_ppo2.py:186][0m |          -0.0023 |           5.3329 |           5.5080 |
[32m[20230113 20:02:41 @agent_ppo2.py:186][0m |           0.0009 |           4.9127 |           5.5067 |
[32m[20230113 20:02:41 @agent_ppo2.py:186][0m |          -0.0026 |           4.3907 |           5.5002 |
[32m[20230113 20:02:41 @agent_ppo2.py:186][0m |          -0.0012 |           4.3586 |           5.5049 |
[32m[20230113 20:02:41 @agent_ppo2.py:186][0m |          -0.0122 |           3.8813 |           5.4986 |
[32m[20230113 20:02:41 @agent_ppo2.py:186][0m |          -0.0113 |           3.7364 |           5.5047 |
[32m[20230113 20:02:41 @agent_ppo2.py:186][0m |          -0.0114 |           3.6499 |           5.5046 |
[32m[20230113 20:02:41 @agent_ppo2.py:186][0m |          -0.0137 |           3.4899 |           5.5036 |
[32m[20230113 20:02:41 @agent_ppo2.py:186][0m |          -0.0153 |           3.4034 |           5.5079 |
[32m[20230113 20:02:41 @agent_ppo2.py:186][0m |          -0.0142 |           3.2878 |           5.5119 |
[32m[20230113 20:02:41 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.02
[32m[20230113 20:02:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.47
[32m[20230113 20:02:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.56
[32m[20230113 20:02:42 @agent_ppo2.py:144][0m Total time:      18.15 min
[32m[20230113 20:02:42 @agent_ppo2.py:146][0m 1638400 total steps have happened
[32m[20230113 20:02:42 @agent_ppo2.py:122][0m #------------------------ Iteration 800 --------------------------#
[32m[20230113 20:02:42 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:42 @agent_ppo2.py:186][0m |          -0.0026 |           5.1086 |           5.5591 |
[32m[20230113 20:02:42 @agent_ppo2.py:186][0m |          -0.0035 |           3.8689 |           5.5594 |
[32m[20230113 20:02:42 @agent_ppo2.py:186][0m |          -0.0056 |           3.3771 |           5.5598 |
[32m[20230113 20:02:42 @agent_ppo2.py:186][0m |          -0.0063 |           3.1470 |           5.5580 |
[32m[20230113 20:02:42 @agent_ppo2.py:186][0m |          -0.0077 |           2.9794 |           5.5631 |
[32m[20230113 20:02:42 @agent_ppo2.py:186][0m |          -0.0088 |           2.8411 |           5.5620 |
[32m[20230113 20:02:42 @agent_ppo2.py:186][0m |          -0.0092 |           2.7455 |           5.5637 |
[32m[20230113 20:02:42 @agent_ppo2.py:186][0m |          -0.0090 |           2.6575 |           5.5605 |
[32m[20230113 20:02:42 @agent_ppo2.py:186][0m |          -0.0109 |           2.5748 |           5.5601 |
[32m[20230113 20:02:42 @agent_ppo2.py:186][0m |          -0.0108 |           2.4983 |           5.5622 |
[32m[20230113 20:02:42 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.38
[32m[20230113 20:02:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.85
[32m[20230113 20:02:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.24
[32m[20230113 20:02:43 @agent_ppo2.py:144][0m Total time:      18.17 min
[32m[20230113 20:02:43 @agent_ppo2.py:146][0m 1640448 total steps have happened
[32m[20230113 20:02:43 @agent_ppo2.py:122][0m #------------------------ Iteration 801 --------------------------#
[32m[20230113 20:02:43 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:43 @agent_ppo2.py:186][0m |           0.0004 |           6.0014 |           5.5383 |
[32m[20230113 20:02:43 @agent_ppo2.py:186][0m |          -0.0014 |           4.9136 |           5.5316 |
[32m[20230113 20:02:43 @agent_ppo2.py:186][0m |          -0.0056 |           4.2499 |           5.5315 |
[32m[20230113 20:02:43 @agent_ppo2.py:186][0m |          -0.0046 |           3.9806 |           5.5290 |
[32m[20230113 20:02:44 @agent_ppo2.py:186][0m |          -0.0061 |           3.7221 |           5.5289 |
[32m[20230113 20:02:44 @agent_ppo2.py:186][0m |          -0.0061 |           3.5345 |           5.5272 |
[32m[20230113 20:02:44 @agent_ppo2.py:186][0m |          -0.0107 |           3.3408 |           5.5273 |
[32m[20230113 20:02:44 @agent_ppo2.py:186][0m |          -0.0088 |           3.2600 |           5.5210 |
[32m[20230113 20:02:44 @agent_ppo2.py:186][0m |          -0.0154 |           3.1351 |           5.5232 |
[32m[20230113 20:02:44 @agent_ppo2.py:186][0m |          -0.0121 |           3.0599 |           5.5256 |
[32m[20230113 20:02:44 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.93
[32m[20230113 20:02:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.09
[32m[20230113 20:02:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 133.66
[32m[20230113 20:02:44 @agent_ppo2.py:144][0m Total time:      18.19 min
[32m[20230113 20:02:44 @agent_ppo2.py:146][0m 1642496 total steps have happened
[32m[20230113 20:02:44 @agent_ppo2.py:122][0m #------------------------ Iteration 802 --------------------------#
[32m[20230113 20:02:45 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:02:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:45 @agent_ppo2.py:186][0m |          -0.0025 |           4.7934 |           5.5705 |
[32m[20230113 20:02:45 @agent_ppo2.py:186][0m |          -0.0068 |           3.8684 |           5.5573 |
[32m[20230113 20:02:45 @agent_ppo2.py:186][0m |          -0.0123 |           3.4829 |           5.5585 |
[32m[20230113 20:02:45 @agent_ppo2.py:186][0m |          -0.0088 |           3.2263 |           5.5578 |
[32m[20230113 20:02:45 @agent_ppo2.py:186][0m |          -0.0098 |           3.0110 |           5.5584 |
[32m[20230113 20:02:45 @agent_ppo2.py:186][0m |          -0.0100 |           2.8777 |           5.5657 |
[32m[20230113 20:02:45 @agent_ppo2.py:186][0m |          -0.0114 |           2.7012 |           5.5614 |
[32m[20230113 20:02:45 @agent_ppo2.py:186][0m |          -0.0139 |           2.5727 |           5.5622 |
[32m[20230113 20:02:45 @agent_ppo2.py:186][0m |          -0.0145 |           2.4678 |           5.5646 |
[32m[20230113 20:02:45 @agent_ppo2.py:186][0m |          -0.0142 |           2.3577 |           5.5653 |
[32m[20230113 20:02:45 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:02:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.37
[32m[20230113 20:02:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.26
[32m[20230113 20:02:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.73
[32m[20230113 20:02:45 @agent_ppo2.py:144][0m Total time:      18.21 min
[32m[20230113 20:02:45 @agent_ppo2.py:146][0m 1644544 total steps have happened
[32m[20230113 20:02:45 @agent_ppo2.py:122][0m #------------------------ Iteration 803 --------------------------#
[32m[20230113 20:02:46 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:46 @agent_ppo2.py:186][0m |          -0.0026 |           7.1317 |           5.7293 |
[32m[20230113 20:02:46 @agent_ppo2.py:186][0m |          -0.0091 |           5.9752 |           5.7201 |
[32m[20230113 20:02:46 @agent_ppo2.py:186][0m |          -0.0113 |           5.6341 |           5.7128 |
[32m[20230113 20:02:46 @agent_ppo2.py:186][0m |          -0.0122 |           5.3376 |           5.7144 |
[32m[20230113 20:02:46 @agent_ppo2.py:186][0m |          -0.0123 |           5.1815 |           5.7086 |
[32m[20230113 20:02:46 @agent_ppo2.py:186][0m |          -0.0138 |           4.9991 |           5.7110 |
[32m[20230113 20:02:46 @agent_ppo2.py:186][0m |          -0.0151 |           4.8665 |           5.7073 |
[32m[20230113 20:02:46 @agent_ppo2.py:186][0m |          -0.0141 |           4.8378 |           5.7065 |
[32m[20230113 20:02:46 @agent_ppo2.py:186][0m |          -0.0146 |           4.6784 |           5.7110 |
[32m[20230113 20:02:46 @agent_ppo2.py:186][0m |          -0.0150 |           4.5798 |           5.7086 |
[32m[20230113 20:02:46 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.93
[32m[20230113 20:02:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.58
[32m[20230113 20:02:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.00
[32m[20230113 20:02:47 @agent_ppo2.py:144][0m Total time:      18.24 min
[32m[20230113 20:02:47 @agent_ppo2.py:146][0m 1646592 total steps have happened
[32m[20230113 20:02:47 @agent_ppo2.py:122][0m #------------------------ Iteration 804 --------------------------#
[32m[20230113 20:02:47 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:47 @agent_ppo2.py:186][0m |          -0.0056 |           6.5215 |           5.6159 |
[32m[20230113 20:02:47 @agent_ppo2.py:186][0m |          -0.0104 |           5.2371 |           5.6116 |
[32m[20230113 20:02:47 @agent_ppo2.py:186][0m |          -0.0045 |           4.8103 |           5.6104 |
[32m[20230113 20:02:47 @agent_ppo2.py:186][0m |          -0.0104 |           4.5437 |           5.6125 |
[32m[20230113 20:02:47 @agent_ppo2.py:186][0m |          -0.0144 |           4.1757 |           5.6165 |
[32m[20230113 20:02:47 @agent_ppo2.py:186][0m |          -0.0170 |           4.0112 |           5.6190 |
[32m[20230113 20:02:47 @agent_ppo2.py:186][0m |          -0.0083 |           3.8090 |           5.6164 |
[32m[20230113 20:02:48 @agent_ppo2.py:186][0m |          -0.0103 |           3.6392 |           5.6218 |
[32m[20230113 20:02:48 @agent_ppo2.py:186][0m |          -0.0134 |           3.4946 |           5.6245 |
[32m[20230113 20:02:48 @agent_ppo2.py:186][0m |          -0.0035 |           3.7528 |           5.6245 |
[32m[20230113 20:02:48 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.62
[32m[20230113 20:02:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.10
[32m[20230113 20:02:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.62
[32m[20230113 20:02:48 @agent_ppo2.py:144][0m Total time:      18.26 min
[32m[20230113 20:02:48 @agent_ppo2.py:146][0m 1648640 total steps have happened
[32m[20230113 20:02:48 @agent_ppo2.py:122][0m #------------------------ Iteration 805 --------------------------#
[32m[20230113 20:02:48 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:02:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:49 @agent_ppo2.py:186][0m |          -0.0006 |           6.1365 |           5.8079 |
[32m[20230113 20:02:49 @agent_ppo2.py:186][0m |          -0.0041 |           5.2080 |           5.8047 |
[32m[20230113 20:02:49 @agent_ppo2.py:186][0m |          -0.0080 |           4.7258 |           5.7913 |
[32m[20230113 20:02:49 @agent_ppo2.py:186][0m |          -0.0097 |           4.3616 |           5.7918 |
[32m[20230113 20:02:49 @agent_ppo2.py:186][0m |          -0.0097 |           4.1834 |           5.7848 |
[32m[20230113 20:02:49 @agent_ppo2.py:186][0m |          -0.0103 |           3.9964 |           5.7847 |
[32m[20230113 20:02:49 @agent_ppo2.py:186][0m |          -0.0108 |           3.8230 |           5.7814 |
[32m[20230113 20:02:49 @agent_ppo2.py:186][0m |          -0.0117 |           3.7183 |           5.7829 |
[32m[20230113 20:02:49 @agent_ppo2.py:186][0m |          -0.0115 |           3.6235 |           5.7833 |
[32m[20230113 20:02:49 @agent_ppo2.py:186][0m |          -0.0116 |           3.5735 |           5.7795 |
[32m[20230113 20:02:49 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.34
[32m[20230113 20:02:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.44
[32m[20230113 20:02:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.55
[32m[20230113 20:02:49 @agent_ppo2.py:144][0m Total time:      18.28 min
[32m[20230113 20:02:49 @agent_ppo2.py:146][0m 1650688 total steps have happened
[32m[20230113 20:02:49 @agent_ppo2.py:122][0m #------------------------ Iteration 806 --------------------------#
[32m[20230113 20:02:50 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:50 @agent_ppo2.py:186][0m |           0.0033 |           6.3192 |           5.5931 |
[32m[20230113 20:02:50 @agent_ppo2.py:186][0m |          -0.0037 |           5.0596 |           5.5893 |
[32m[20230113 20:02:50 @agent_ppo2.py:186][0m |          -0.0070 |           4.3805 |           5.5917 |
[32m[20230113 20:02:50 @agent_ppo2.py:186][0m |          -0.0046 |           4.1492 |           5.5910 |
[32m[20230113 20:02:50 @agent_ppo2.py:186][0m |          -0.0092 |           3.7744 |           5.5878 |
[32m[20230113 20:02:50 @agent_ppo2.py:186][0m |          -0.0090 |           3.5819 |           5.5870 |
[32m[20230113 20:02:50 @agent_ppo2.py:186][0m |          -0.0096 |           3.4158 |           5.5895 |
[32m[20230113 20:02:50 @agent_ppo2.py:186][0m |          -0.0113 |           3.3106 |           5.5854 |
[32m[20230113 20:02:50 @agent_ppo2.py:186][0m |          -0.0108 |           3.2254 |           5.5849 |
[32m[20230113 20:02:50 @agent_ppo2.py:186][0m |          -0.0129 |           3.0854 |           5.5857 |
[32m[20230113 20:02:50 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.72
[32m[20230113 20:02:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.55
[32m[20230113 20:02:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.45
[32m[20230113 20:02:51 @agent_ppo2.py:144][0m Total time:      18.30 min
[32m[20230113 20:02:51 @agent_ppo2.py:146][0m 1652736 total steps have happened
[32m[20230113 20:02:51 @agent_ppo2.py:122][0m #------------------------ Iteration 807 --------------------------#
[32m[20230113 20:02:51 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:51 @agent_ppo2.py:186][0m |           0.0013 |           6.0779 |           5.7117 |
[32m[20230113 20:02:51 @agent_ppo2.py:186][0m |          -0.0101 |           5.2142 |           5.7152 |
[32m[20230113 20:02:51 @agent_ppo2.py:186][0m |          -0.0070 |           4.7760 |           5.7083 |
[32m[20230113 20:02:51 @agent_ppo2.py:186][0m |          -0.0090 |           4.4474 |           5.7039 |
[32m[20230113 20:02:51 @agent_ppo2.py:186][0m |          -0.0078 |           4.2903 |           5.7027 |
[32m[20230113 20:02:51 @agent_ppo2.py:186][0m |           0.0003 |           4.5035 |           5.7031 |
[32m[20230113 20:02:51 @agent_ppo2.py:186][0m |          -0.0127 |           3.9109 |           5.6960 |
[32m[20230113 20:02:51 @agent_ppo2.py:186][0m |          -0.0141 |           3.6244 |           5.6931 |
[32m[20230113 20:02:52 @agent_ppo2.py:186][0m |          -0.0153 |           3.4892 |           5.6944 |
[32m[20230113 20:02:52 @agent_ppo2.py:186][0m |          -0.0191 |           3.3770 |           5.6930 |
[32m[20230113 20:02:52 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.89
[32m[20230113 20:02:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.07
[32m[20230113 20:02:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.26
[32m[20230113 20:02:52 @agent_ppo2.py:144][0m Total time:      18.32 min
[32m[20230113 20:02:52 @agent_ppo2.py:146][0m 1654784 total steps have happened
[32m[20230113 20:02:52 @agent_ppo2.py:122][0m #------------------------ Iteration 808 --------------------------#
[32m[20230113 20:02:52 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:52 @agent_ppo2.py:186][0m |          -0.0002 |           6.9168 |           5.6695 |
[32m[20230113 20:02:53 @agent_ppo2.py:186][0m |          -0.0100 |           5.8015 |           5.6675 |
[32m[20230113 20:02:53 @agent_ppo2.py:186][0m |          -0.0096 |           5.2607 |           5.6676 |
[32m[20230113 20:02:53 @agent_ppo2.py:186][0m |          -0.0136 |           4.9768 |           5.6629 |
[32m[20230113 20:02:53 @agent_ppo2.py:186][0m |          -0.0083 |           4.7995 |           5.6653 |
[32m[20230113 20:02:53 @agent_ppo2.py:186][0m |          -0.0125 |           4.5633 |           5.6700 |
[32m[20230113 20:02:53 @agent_ppo2.py:186][0m |          -0.0172 |           4.3845 |           5.6687 |
[32m[20230113 20:02:53 @agent_ppo2.py:186][0m |          -0.0138 |           4.2495 |           5.6736 |
[32m[20230113 20:02:53 @agent_ppo2.py:186][0m |          -0.0137 |           4.1184 |           5.6706 |
[32m[20230113 20:02:53 @agent_ppo2.py:186][0m |          -0.0162 |           4.0345 |           5.6716 |
[32m[20230113 20:02:53 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:02:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.13
[32m[20230113 20:02:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.37
[32m[20230113 20:02:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.57
[32m[20230113 20:02:53 @agent_ppo2.py:144][0m Total time:      18.35 min
[32m[20230113 20:02:53 @agent_ppo2.py:146][0m 1656832 total steps have happened
[32m[20230113 20:02:53 @agent_ppo2.py:122][0m #------------------------ Iteration 809 --------------------------#
[32m[20230113 20:02:54 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:54 @agent_ppo2.py:186][0m |           0.0004 |           5.8857 |           5.8423 |
[32m[20230113 20:02:54 @agent_ppo2.py:186][0m |          -0.0059 |           3.9576 |           5.8375 |
[32m[20230113 20:02:54 @agent_ppo2.py:186][0m |          -0.0098 |           3.3826 |           5.8371 |
[32m[20230113 20:02:54 @agent_ppo2.py:186][0m |          -0.0107 |           3.0789 |           5.8362 |
[32m[20230113 20:02:54 @agent_ppo2.py:186][0m |          -0.0128 |           2.8684 |           5.8343 |
[32m[20230113 20:02:54 @agent_ppo2.py:186][0m |          -0.0123 |           2.7203 |           5.8336 |
[32m[20230113 20:02:54 @agent_ppo2.py:186][0m |          -0.0142 |           2.5996 |           5.8336 |
[32m[20230113 20:02:54 @agent_ppo2.py:186][0m |          -0.0151 |           2.5138 |           5.8338 |
[32m[20230113 20:02:54 @agent_ppo2.py:186][0m |          -0.0144 |           2.4122 |           5.8368 |
[32m[20230113 20:02:54 @agent_ppo2.py:186][0m |          -0.0170 |           2.3460 |           5.8379 |
[32m[20230113 20:02:54 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.52
[32m[20230113 20:02:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.90
[32m[20230113 20:02:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.09
[32m[20230113 20:02:55 @agent_ppo2.py:144][0m Total time:      18.37 min
[32m[20230113 20:02:55 @agent_ppo2.py:146][0m 1658880 total steps have happened
[32m[20230113 20:02:55 @agent_ppo2.py:122][0m #------------------------ Iteration 810 --------------------------#
[32m[20230113 20:02:55 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:55 @agent_ppo2.py:186][0m |          -0.0007 |           6.1559 |           5.7741 |
[32m[20230113 20:02:55 @agent_ppo2.py:186][0m |          -0.0049 |           5.0527 |           5.7628 |
[32m[20230113 20:02:55 @agent_ppo2.py:186][0m |          -0.0101 |           4.5489 |           5.7595 |
[32m[20230113 20:02:55 @agent_ppo2.py:186][0m |          -0.0095 |           4.2463 |           5.7543 |
[32m[20230113 20:02:55 @agent_ppo2.py:186][0m |          -0.0117 |           4.0043 |           5.7517 |
[32m[20230113 20:02:55 @agent_ppo2.py:186][0m |          -0.0111 |           3.8663 |           5.7475 |
[32m[20230113 20:02:55 @agent_ppo2.py:186][0m |          -0.0119 |           3.7788 |           5.7462 |
[32m[20230113 20:02:55 @agent_ppo2.py:186][0m |          -0.0118 |           3.7503 |           5.7472 |
[32m[20230113 20:02:55 @agent_ppo2.py:186][0m |          -0.0154 |           3.5423 |           5.7429 |
[32m[20230113 20:02:56 @agent_ppo2.py:186][0m |          -0.0156 |           3.4909 |           5.7419 |
[32m[20230113 20:02:56 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:02:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.67
[32m[20230113 20:02:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.67
[32m[20230113 20:02:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.91
[32m[20230113 20:02:56 @agent_ppo2.py:144][0m Total time:      18.39 min
[32m[20230113 20:02:56 @agent_ppo2.py:146][0m 1660928 total steps have happened
[32m[20230113 20:02:56 @agent_ppo2.py:122][0m #------------------------ Iteration 811 --------------------------#
[32m[20230113 20:02:56 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:02:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:56 @agent_ppo2.py:186][0m |          -0.0009 |           5.7481 |           5.6791 |
[32m[20230113 20:02:56 @agent_ppo2.py:186][0m |          -0.0056 |           5.0334 |           5.6684 |
[32m[20230113 20:02:57 @agent_ppo2.py:186][0m |          -0.0069 |           4.5736 |           5.6533 |
[32m[20230113 20:02:57 @agent_ppo2.py:186][0m |          -0.0089 |           4.2864 |           5.6532 |
[32m[20230113 20:02:57 @agent_ppo2.py:186][0m |          -0.0102 |           4.0949 |           5.6472 |
[32m[20230113 20:02:57 @agent_ppo2.py:186][0m |          -0.0106 |           3.9697 |           5.6491 |
[32m[20230113 20:02:57 @agent_ppo2.py:186][0m |          -0.0098 |           3.8451 |           5.6450 |
[32m[20230113 20:02:57 @agent_ppo2.py:186][0m |          -0.0099 |           3.7455 |           5.6467 |
[32m[20230113 20:02:57 @agent_ppo2.py:186][0m |          -0.0107 |           3.6538 |           5.6433 |
[32m[20230113 20:02:57 @agent_ppo2.py:186][0m |          -0.0118 |           3.5936 |           5.6384 |
[32m[20230113 20:02:57 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:02:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.41
[32m[20230113 20:02:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.10
[32m[20230113 20:02:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.51
[32m[20230113 20:02:57 @agent_ppo2.py:144][0m Total time:      18.41 min
[32m[20230113 20:02:57 @agent_ppo2.py:146][0m 1662976 total steps have happened
[32m[20230113 20:02:57 @agent_ppo2.py:122][0m #------------------------ Iteration 812 --------------------------#
[32m[20230113 20:02:58 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:02:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:58 @agent_ppo2.py:186][0m |          -0.0000 |           5.4987 |           5.5374 |
[32m[20230113 20:02:58 @agent_ppo2.py:186][0m |          -0.0052 |           4.7530 |           5.5321 |
[32m[20230113 20:02:58 @agent_ppo2.py:186][0m |          -0.0063 |           4.3320 |           5.5272 |
[32m[20230113 20:02:58 @agent_ppo2.py:186][0m |          -0.0058 |           4.1357 |           5.5294 |
[32m[20230113 20:02:58 @agent_ppo2.py:186][0m |          -0.0131 |           3.9192 |           5.5281 |
[32m[20230113 20:02:58 @agent_ppo2.py:186][0m |          -0.0092 |           3.7515 |           5.5276 |
[32m[20230113 20:02:58 @agent_ppo2.py:186][0m |          -0.0178 |           3.6437 |           5.5319 |
[32m[20230113 20:02:58 @agent_ppo2.py:186][0m |          -0.0159 |           3.5111 |           5.5312 |
[32m[20230113 20:02:58 @agent_ppo2.py:186][0m |          -0.0098 |           3.4607 |           5.5358 |
[32m[20230113 20:02:58 @agent_ppo2.py:186][0m |          -0.0105 |           3.3511 |           5.5336 |
[32m[20230113 20:02:58 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:02:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.77
[32m[20230113 20:02:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.43
[32m[20230113 20:02:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.51
[32m[20230113 20:02:58 @agent_ppo2.py:144][0m Total time:      18.43 min
[32m[20230113 20:02:58 @agent_ppo2.py:146][0m 1665024 total steps have happened
[32m[20230113 20:02:58 @agent_ppo2.py:122][0m #------------------------ Iteration 813 --------------------------#
[32m[20230113 20:02:59 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:02:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:02:59 @agent_ppo2.py:186][0m |          -0.0054 |           4.6209 |           5.7069 |
[32m[20230113 20:02:59 @agent_ppo2.py:186][0m |          -0.0148 |           3.5525 |           5.6961 |
[32m[20230113 20:02:59 @agent_ppo2.py:186][0m |          -0.0091 |           3.2587 |           5.6935 |
[32m[20230113 20:02:59 @agent_ppo2.py:186][0m |          -0.0171 |           3.0798 |           5.6915 |
[32m[20230113 20:02:59 @agent_ppo2.py:186][0m |          -0.0109 |           2.9066 |           5.6932 |
[32m[20230113 20:02:59 @agent_ppo2.py:186][0m |           0.0095 |           3.1611 |           5.6905 |
[32m[20230113 20:02:59 @agent_ppo2.py:186][0m |          -0.0189 |           2.7970 |           5.6917 |
[32m[20230113 20:02:59 @agent_ppo2.py:186][0m |          -0.0134 |           2.6812 |           5.6905 |
[32m[20230113 20:02:59 @agent_ppo2.py:186][0m |           0.0050 |           2.6229 |           5.6887 |
[32m[20230113 20:02:59 @agent_ppo2.py:186][0m |          -0.0070 |           2.5568 |           5.6889 |
[32m[20230113 20:02:59 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:03:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.93
[32m[20230113 20:03:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.67
[32m[20230113 20:03:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.05
[32m[20230113 20:03:00 @agent_ppo2.py:144][0m Total time:      18.45 min
[32m[20230113 20:03:00 @agent_ppo2.py:146][0m 1667072 total steps have happened
[32m[20230113 20:03:00 @agent_ppo2.py:122][0m #------------------------ Iteration 814 --------------------------#
[32m[20230113 20:03:00 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:03:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:00 @agent_ppo2.py:186][0m |          -0.0049 |           5.1361 |           5.7299 |
[32m[20230113 20:03:00 @agent_ppo2.py:186][0m |           0.0014 |           4.6515 |           5.7273 |
[32m[20230113 20:03:00 @agent_ppo2.py:186][0m |          -0.0109 |           4.0228 |           5.7155 |
[32m[20230113 20:03:00 @agent_ppo2.py:186][0m |          -0.0128 |           3.8072 |           5.7182 |
[32m[20230113 20:03:01 @agent_ppo2.py:186][0m |          -0.0099 |           3.6393 |           5.7199 |
[32m[20230113 20:03:01 @agent_ppo2.py:186][0m |          -0.0144 |           3.5018 |           5.7180 |
[32m[20230113 20:03:01 @agent_ppo2.py:186][0m |          -0.0142 |           3.3757 |           5.7142 |
[32m[20230113 20:03:01 @agent_ppo2.py:186][0m |          -0.0138 |           3.2807 |           5.7124 |
[32m[20230113 20:03:01 @agent_ppo2.py:186][0m |          -0.0078 |           3.2365 |           5.7117 |
[32m[20230113 20:03:01 @agent_ppo2.py:186][0m |          -0.0205 |           3.1869 |           5.7131 |
[32m[20230113 20:03:01 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:03:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.56
[32m[20230113 20:03:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.80
[32m[20230113 20:03:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.51
[32m[20230113 20:03:01 @agent_ppo2.py:144][0m Total time:      18.48 min
[32m[20230113 20:03:01 @agent_ppo2.py:146][0m 1669120 total steps have happened
[32m[20230113 20:03:01 @agent_ppo2.py:122][0m #------------------------ Iteration 815 --------------------------#
[32m[20230113 20:03:02 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:03:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:02 @agent_ppo2.py:186][0m |           0.0007 |           6.2916 |           5.7637 |
[32m[20230113 20:03:02 @agent_ppo2.py:186][0m |          -0.0055 |           5.2926 |           5.7599 |
[32m[20230113 20:03:02 @agent_ppo2.py:186][0m |          -0.0086 |           4.8390 |           5.7562 |
[32m[20230113 20:03:02 @agent_ppo2.py:186][0m |          -0.0115 |           4.4709 |           5.7622 |
[32m[20230113 20:03:02 @agent_ppo2.py:186][0m |          -0.0099 |           4.2177 |           5.7573 |
[32m[20230113 20:03:02 @agent_ppo2.py:186][0m |          -0.0146 |           4.0522 |           5.7569 |
[32m[20230113 20:03:02 @agent_ppo2.py:186][0m |          -0.0142 |           3.8544 |           5.7580 |
[32m[20230113 20:03:02 @agent_ppo2.py:186][0m |          -0.0133 |           3.7632 |           5.7531 |
[32m[20230113 20:03:02 @agent_ppo2.py:186][0m |          -0.0135 |           3.6053 |           5.7565 |
[32m[20230113 20:03:02 @agent_ppo2.py:186][0m |          -0.0149 |           3.4604 |           5.7566 |
[32m[20230113 20:03:02 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:03:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.87
[32m[20230113 20:03:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.17
[32m[20230113 20:03:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.60
[32m[20230113 20:03:02 @agent_ppo2.py:144][0m Total time:      18.50 min
[32m[20230113 20:03:02 @agent_ppo2.py:146][0m 1671168 total steps have happened
[32m[20230113 20:03:02 @agent_ppo2.py:122][0m #------------------------ Iteration 816 --------------------------#
[32m[20230113 20:03:03 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:03:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:03 @agent_ppo2.py:186][0m |           0.0386 |           6.5788 |           5.6582 |
[32m[20230113 20:03:03 @agent_ppo2.py:186][0m |          -0.0085 |           5.5821 |           5.5974 |
[32m[20230113 20:03:03 @agent_ppo2.py:186][0m |          -0.0081 |           4.4802 |           5.6292 |
[32m[20230113 20:03:03 @agent_ppo2.py:186][0m |          -0.0027 |           4.2003 |           5.6377 |
[32m[20230113 20:03:03 @agent_ppo2.py:186][0m |           0.0049 |           3.8136 |           5.6330 |
[32m[20230113 20:03:03 @agent_ppo2.py:186][0m |          -0.0157 |           3.5400 |           5.6271 |
[32m[20230113 20:03:03 @agent_ppo2.py:186][0m |          -0.0299 |           3.3532 |           5.6293 |
[32m[20230113 20:03:03 @agent_ppo2.py:186][0m |          -0.0161 |           3.2020 |           5.6167 |
[32m[20230113 20:03:03 @agent_ppo2.py:186][0m |          -0.0203 |           3.1034 |           5.6248 |
[32m[20230113 20:03:03 @agent_ppo2.py:186][0m |          -0.0166 |           2.9500 |           5.6236 |
[32m[20230113 20:03:03 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.46
[32m[20230113 20:03:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.83
[32m[20230113 20:03:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.73
[32m[20230113 20:03:04 @agent_ppo2.py:144][0m Total time:      18.52 min
[32m[20230113 20:03:04 @agent_ppo2.py:146][0m 1673216 total steps have happened
[32m[20230113 20:03:04 @agent_ppo2.py:122][0m #------------------------ Iteration 817 --------------------------#
[32m[20230113 20:03:04 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:03:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:04 @agent_ppo2.py:186][0m |           0.0041 |           5.9732 |           5.7105 |
[32m[20230113 20:03:04 @agent_ppo2.py:186][0m |          -0.0048 |           4.7270 |           5.7078 |
[32m[20230113 20:03:04 @agent_ppo2.py:186][0m |          -0.0113 |           4.2508 |           5.7057 |
[32m[20230113 20:03:04 @agent_ppo2.py:186][0m |          -0.0108 |           3.9713 |           5.7082 |
[32m[20230113 20:03:04 @agent_ppo2.py:186][0m |          -0.0159 |           3.7731 |           5.7093 |
[32m[20230113 20:03:05 @agent_ppo2.py:186][0m |          -0.0134 |           3.5974 |           5.7072 |
[32m[20230113 20:03:05 @agent_ppo2.py:186][0m |          -0.0135 |           3.4570 |           5.7023 |
[32m[20230113 20:03:05 @agent_ppo2.py:186][0m |          -0.0145 |           3.3763 |           5.7041 |
[32m[20230113 20:03:05 @agent_ppo2.py:186][0m |          -0.0095 |           3.3805 |           5.7064 |
[32m[20230113 20:03:05 @agent_ppo2.py:186][0m |          -0.0116 |           3.2004 |           5.7016 |
[32m[20230113 20:03:05 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.27
[32m[20230113 20:03:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.69
[32m[20230113 20:03:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.83
[32m[20230113 20:03:05 @agent_ppo2.py:144][0m Total time:      18.54 min
[32m[20230113 20:03:05 @agent_ppo2.py:146][0m 1675264 total steps have happened
[32m[20230113 20:03:05 @agent_ppo2.py:122][0m #------------------------ Iteration 818 --------------------------#
[32m[20230113 20:03:06 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:03:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:06 @agent_ppo2.py:186][0m |           0.0060 |           5.9971 |           5.7172 |
[32m[20230113 20:03:06 @agent_ppo2.py:186][0m |          -0.0001 |           4.8829 |           5.7089 |
[32m[20230113 20:03:06 @agent_ppo2.py:186][0m |          -0.0065 |           4.5367 |           5.6999 |
[32m[20230113 20:03:06 @agent_ppo2.py:186][0m |          -0.0047 |           4.4290 |           5.7000 |
[32m[20230113 20:03:06 @agent_ppo2.py:186][0m |          -0.0096 |           4.1573 |           5.7009 |
[32m[20230113 20:03:06 @agent_ppo2.py:186][0m |          -0.0171 |           4.0171 |           5.6947 |
[32m[20230113 20:03:06 @agent_ppo2.py:186][0m |          -0.0026 |           3.9968 |           5.6994 |
[32m[20230113 20:03:06 @agent_ppo2.py:186][0m |          -0.0126 |           3.8172 |           5.7003 |
[32m[20230113 20:03:06 @agent_ppo2.py:186][0m |          -0.0180 |           3.7182 |           5.7016 |
[32m[20230113 20:03:06 @agent_ppo2.py:186][0m |          -0.0136 |           3.6120 |           5.6965 |
[32m[20230113 20:03:06 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:03:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.28
[32m[20230113 20:03:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.03
[32m[20230113 20:03:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.24
[32m[20230113 20:03:06 @agent_ppo2.py:144][0m Total time:      18.56 min
[32m[20230113 20:03:06 @agent_ppo2.py:146][0m 1677312 total steps have happened
[32m[20230113 20:03:06 @agent_ppo2.py:122][0m #------------------------ Iteration 819 --------------------------#
[32m[20230113 20:03:07 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:03:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:07 @agent_ppo2.py:186][0m |           0.0004 |           7.6484 |           5.8607 |
[32m[20230113 20:03:07 @agent_ppo2.py:186][0m |          -0.0047 |           5.8043 |           5.8612 |
[32m[20230113 20:03:07 @agent_ppo2.py:186][0m |          -0.0079 |           5.0544 |           5.8598 |
[32m[20230113 20:03:07 @agent_ppo2.py:186][0m |          -0.0087 |           4.5980 |           5.8539 |
[32m[20230113 20:03:07 @agent_ppo2.py:186][0m |          -0.0083 |           4.3143 |           5.8540 |
[32m[20230113 20:03:07 @agent_ppo2.py:186][0m |          -0.0104 |           4.0631 |           5.8534 |
[32m[20230113 20:03:07 @agent_ppo2.py:186][0m |          -0.0113 |           3.8263 |           5.8554 |
[32m[20230113 20:03:07 @agent_ppo2.py:186][0m |          -0.0113 |           3.6213 |           5.8536 |
[32m[20230113 20:03:07 @agent_ppo2.py:186][0m |          -0.0116 |           3.4372 |           5.8569 |
[32m[20230113 20:03:07 @agent_ppo2.py:186][0m |          -0.0131 |           3.3385 |           5.8552 |
[32m[20230113 20:03:07 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:03:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.49
[32m[20230113 20:03:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.51
[32m[20230113 20:03:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.48
[32m[20230113 20:03:08 @agent_ppo2.py:144][0m Total time:      18.59 min
[32m[20230113 20:03:08 @agent_ppo2.py:146][0m 1679360 total steps have happened
[32m[20230113 20:03:08 @agent_ppo2.py:122][0m #------------------------ Iteration 820 --------------------------#
[32m[20230113 20:03:08 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:03:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:08 @agent_ppo2.py:186][0m |           0.0150 |           5.7953 |           5.6837 |
[32m[20230113 20:03:08 @agent_ppo2.py:186][0m |           0.0036 |           5.0293 |           5.6738 |
[32m[20230113 20:03:08 @agent_ppo2.py:186][0m |          -0.0034 |           4.6490 |           5.6741 |
[32m[20230113 20:03:08 @agent_ppo2.py:186][0m |           0.0066 |           4.3844 |           5.6734 |
[32m[20230113 20:03:08 @agent_ppo2.py:186][0m |          -0.0297 |           4.3761 |           5.6634 |
[32m[20230113 20:03:08 @agent_ppo2.py:186][0m |           0.0089 |           4.2028 |           5.6586 |
[32m[20230113 20:03:08 @agent_ppo2.py:186][0m |          -0.0074 |           3.9945 |           5.6573 |
[32m[20230113 20:03:08 @agent_ppo2.py:186][0m |          -0.0063 |           3.9339 |           5.6631 |
[32m[20230113 20:03:09 @agent_ppo2.py:186][0m |          -0.0230 |           3.8664 |           5.6653 |
[32m[20230113 20:03:09 @agent_ppo2.py:186][0m |          -0.0053 |           3.8162 |           5.6633 |
[32m[20230113 20:03:09 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:03:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.18
[32m[20230113 20:03:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.42
[32m[20230113 20:03:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.04
[32m[20230113 20:03:09 @agent_ppo2.py:144][0m Total time:      18.61 min
[32m[20230113 20:03:09 @agent_ppo2.py:146][0m 1681408 total steps have happened
[32m[20230113 20:03:09 @agent_ppo2.py:122][0m #------------------------ Iteration 821 --------------------------#
[32m[20230113 20:03:09 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:03:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:10 @agent_ppo2.py:186][0m |          -0.0047 |           5.7149 |           5.6923 |
[32m[20230113 20:03:10 @agent_ppo2.py:186][0m |          -0.0081 |           4.4433 |           5.6852 |
[32m[20230113 20:03:10 @agent_ppo2.py:186][0m |          -0.0091 |           3.9491 |           5.6882 |
[32m[20230113 20:03:10 @agent_ppo2.py:186][0m |          -0.0127 |           3.7040 |           5.6860 |
[32m[20230113 20:03:10 @agent_ppo2.py:186][0m |          -0.0099 |           3.5082 |           5.6865 |
[32m[20230113 20:03:10 @agent_ppo2.py:186][0m |          -0.0119 |           3.3705 |           5.6888 |
[32m[20230113 20:03:10 @agent_ppo2.py:186][0m |          -0.0144 |           3.2466 |           5.6846 |
[32m[20230113 20:03:10 @agent_ppo2.py:186][0m |          -0.0107 |           3.1370 |           5.6838 |
[32m[20230113 20:03:10 @agent_ppo2.py:186][0m |          -0.0161 |           3.0597 |           5.6825 |
[32m[20230113 20:03:10 @agent_ppo2.py:186][0m |          -0.0139 |           3.0365 |           5.6835 |
[32m[20230113 20:03:10 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:03:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.64
[32m[20230113 20:03:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.93
[32m[20230113 20:03:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.41
[32m[20230113 20:03:10 @agent_ppo2.py:144][0m Total time:      18.63 min
[32m[20230113 20:03:10 @agent_ppo2.py:146][0m 1683456 total steps have happened
[32m[20230113 20:03:10 @agent_ppo2.py:122][0m #------------------------ Iteration 822 --------------------------#
[32m[20230113 20:03:11 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:03:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:11 @agent_ppo2.py:186][0m |           0.0009 |           5.3798 |           5.7882 |
[32m[20230113 20:03:11 @agent_ppo2.py:186][0m |          -0.0053 |           4.3675 |           5.7772 |
[32m[20230113 20:03:11 @agent_ppo2.py:186][0m |          -0.0096 |           3.9571 |           5.7823 |
[32m[20230113 20:03:11 @agent_ppo2.py:186][0m |          -0.0107 |           3.6755 |           5.7804 |
[32m[20230113 20:03:11 @agent_ppo2.py:186][0m |          -0.0114 |           3.5128 |           5.7685 |
[32m[20230113 20:03:11 @agent_ppo2.py:186][0m |          -0.0122 |           3.3545 |           5.7756 |
[32m[20230113 20:03:11 @agent_ppo2.py:186][0m |          -0.0136 |           3.2435 |           5.7768 |
[32m[20230113 20:03:11 @agent_ppo2.py:186][0m |          -0.0134 |           3.0944 |           5.7780 |
[32m[20230113 20:03:11 @agent_ppo2.py:186][0m |          -0.0132 |           3.0225 |           5.7816 |
[32m[20230113 20:03:11 @agent_ppo2.py:186][0m |          -0.0146 |           2.9283 |           5.7793 |
[32m[20230113 20:03:11 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:03:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.11
[32m[20230113 20:03:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.62
[32m[20230113 20:03:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 182.84
[32m[20230113 20:03:12 @agent_ppo2.py:144][0m Total time:      18.65 min
[32m[20230113 20:03:12 @agent_ppo2.py:146][0m 1685504 total steps have happened
[32m[20230113 20:03:12 @agent_ppo2.py:122][0m #------------------------ Iteration 823 --------------------------#
[32m[20230113 20:03:12 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:03:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:12 @agent_ppo2.py:186][0m |           0.0034 |          16.8830 |           5.6902 |
[32m[20230113 20:03:12 @agent_ppo2.py:186][0m |          -0.0001 |           7.2535 |           5.6759 |
[32m[20230113 20:03:12 @agent_ppo2.py:186][0m |          -0.0056 |           6.1436 |           5.6829 |
[32m[20230113 20:03:12 @agent_ppo2.py:186][0m |          -0.0093 |           5.7385 |           5.6714 |
[32m[20230113 20:03:12 @agent_ppo2.py:186][0m |          -0.0148 |           5.2738 |           5.6701 |
[32m[20230113 20:03:12 @agent_ppo2.py:186][0m |          -0.0105 |           5.0511 |           5.6685 |
[32m[20230113 20:03:12 @agent_ppo2.py:186][0m |          -0.0154 |           4.8848 |           5.6655 |
[32m[20230113 20:03:12 @agent_ppo2.py:186][0m |          -0.0161 |           4.6708 |           5.6663 |
[32m[20230113 20:03:12 @agent_ppo2.py:186][0m |          -0.0147 |           4.5742 |           5.6661 |
[32m[20230113 20:03:13 @agent_ppo2.py:186][0m |          -0.0170 |           4.4953 |           5.6655 |
[32m[20230113 20:03:13 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:03:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 118.65
[32m[20230113 20:03:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.10
[32m[20230113 20:03:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 145.85
[32m[20230113 20:03:13 @agent_ppo2.py:144][0m Total time:      18.67 min
[32m[20230113 20:03:13 @agent_ppo2.py:146][0m 1687552 total steps have happened
[32m[20230113 20:03:13 @agent_ppo2.py:122][0m #------------------------ Iteration 824 --------------------------#
[32m[20230113 20:03:13 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:03:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:14 @agent_ppo2.py:186][0m |          -0.0003 |           6.4565 |           5.7123 |
[32m[20230113 20:03:14 @agent_ppo2.py:186][0m |          -0.0092 |           5.1221 |           5.7024 |
[32m[20230113 20:03:14 @agent_ppo2.py:186][0m |          -0.0107 |           4.7532 |           5.7061 |
[32m[20230113 20:03:14 @agent_ppo2.py:186][0m |          -0.0132 |           4.4752 |           5.7102 |
[32m[20230113 20:03:14 @agent_ppo2.py:186][0m |           0.0011 |           4.5376 |           5.7127 |
[32m[20230113 20:03:14 @agent_ppo2.py:186][0m |          -0.0119 |           4.1748 |           5.7044 |
[32m[20230113 20:03:14 @agent_ppo2.py:186][0m |          -0.0157 |           4.0131 |           5.7116 |
[32m[20230113 20:03:14 @agent_ppo2.py:186][0m |          -0.0157 |           3.8704 |           5.7086 |
[32m[20230113 20:03:14 @agent_ppo2.py:186][0m |          -0.0188 |           3.7772 |           5.7144 |
[32m[20230113 20:03:14 @agent_ppo2.py:186][0m |          -0.0154 |           3.6896 |           5.7116 |
[32m[20230113 20:03:14 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.24
[32m[20230113 20:03:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.59
[32m[20230113 20:03:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.42
[32m[20230113 20:03:14 @agent_ppo2.py:144][0m Total time:      18.70 min
[32m[20230113 20:03:14 @agent_ppo2.py:146][0m 1689600 total steps have happened
[32m[20230113 20:03:14 @agent_ppo2.py:122][0m #------------------------ Iteration 825 --------------------------#
[32m[20230113 20:03:15 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:03:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:15 @agent_ppo2.py:186][0m |          -0.0005 |           5.5962 |           5.6796 |
[32m[20230113 20:03:15 @agent_ppo2.py:186][0m |          -0.0008 |           4.8866 |           5.6850 |
[32m[20230113 20:03:15 @agent_ppo2.py:186][0m |          -0.0036 |           4.5117 |           5.6835 |
[32m[20230113 20:03:15 @agent_ppo2.py:186][0m |          -0.0066 |           4.3190 |           5.6845 |
[32m[20230113 20:03:15 @agent_ppo2.py:186][0m |          -0.0074 |           4.0176 |           5.6792 |
[32m[20230113 20:03:15 @agent_ppo2.py:186][0m |          -0.0093 |           3.8634 |           5.6849 |
[32m[20230113 20:03:15 @agent_ppo2.py:186][0m |          -0.0131 |           3.6979 |           5.6823 |
[32m[20230113 20:03:15 @agent_ppo2.py:186][0m |          -0.0073 |           3.6674 |           5.6918 |
[32m[20230113 20:03:15 @agent_ppo2.py:186][0m |          -0.0132 |           3.5024 |           5.6854 |
[32m[20230113 20:03:15 @agent_ppo2.py:186][0m |          -0.0172 |           3.4358 |           5.6885 |
[32m[20230113 20:03:15 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.26
[32m[20230113 20:03:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.06
[32m[20230113 20:03:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.42
[32m[20230113 20:03:16 @agent_ppo2.py:144][0m Total time:      18.72 min
[32m[20230113 20:03:16 @agent_ppo2.py:146][0m 1691648 total steps have happened
[32m[20230113 20:03:16 @agent_ppo2.py:122][0m #------------------------ Iteration 826 --------------------------#
[32m[20230113 20:03:16 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:03:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:16 @agent_ppo2.py:186][0m |           0.0018 |           4.2526 |           5.6584 |
[32m[20230113 20:03:16 @agent_ppo2.py:186][0m |          -0.0052 |           3.2777 |           5.6476 |
[32m[20230113 20:03:16 @agent_ppo2.py:186][0m |          -0.0042 |           2.8118 |           5.6487 |
[32m[20230113 20:03:16 @agent_ppo2.py:186][0m |          -0.0060 |           2.5067 |           5.6501 |
[32m[20230113 20:03:16 @agent_ppo2.py:186][0m |          -0.0098 |           2.3320 |           5.6515 |
[32m[20230113 20:03:16 @agent_ppo2.py:186][0m |          -0.0090 |           2.2178 |           5.6479 |
[32m[20230113 20:03:16 @agent_ppo2.py:186][0m |          -0.0089 |           2.1307 |           5.6437 |
[32m[20230113 20:03:16 @agent_ppo2.py:186][0m |          -0.0134 |           2.0534 |           5.6423 |
[32m[20230113 20:03:17 @agent_ppo2.py:186][0m |          -0.0119 |           1.9828 |           5.6434 |
[32m[20230113 20:03:17 @agent_ppo2.py:186][0m |          -0.0137 |           1.9363 |           5.6406 |
[32m[20230113 20:03:17 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:03:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.14
[32m[20230113 20:03:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.75
[32m[20230113 20:03:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.96
[32m[20230113 20:03:17 @agent_ppo2.py:144][0m Total time:      18.74 min
[32m[20230113 20:03:17 @agent_ppo2.py:146][0m 1693696 total steps have happened
[32m[20230113 20:03:17 @agent_ppo2.py:122][0m #------------------------ Iteration 827 --------------------------#
[32m[20230113 20:03:17 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:03:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:18 @agent_ppo2.py:186][0m |          -0.0007 |           7.4367 |           5.7132 |
[32m[20230113 20:03:18 @agent_ppo2.py:186][0m |          -0.0025 |           6.3599 |           5.7046 |
[32m[20230113 20:03:18 @agent_ppo2.py:186][0m |          -0.0084 |           5.7110 |           5.6893 |
[32m[20230113 20:03:18 @agent_ppo2.py:186][0m |          -0.0089 |           5.3543 |           5.6953 |
[32m[20230113 20:03:18 @agent_ppo2.py:186][0m |          -0.0072 |           5.0375 |           5.6935 |
[32m[20230113 20:03:18 @agent_ppo2.py:186][0m |          -0.0083 |           4.9006 |           5.6923 |
[32m[20230113 20:03:18 @agent_ppo2.py:186][0m |          -0.0044 |           4.7734 |           5.6908 |
[32m[20230113 20:03:18 @agent_ppo2.py:186][0m |          -0.0069 |           4.5905 |           5.6875 |
[32m[20230113 20:03:18 @agent_ppo2.py:186][0m |          -0.0080 |           4.5012 |           5.6854 |
[32m[20230113 20:03:18 @agent_ppo2.py:186][0m |          -0.0124 |           4.3537 |           5.6881 |
[32m[20230113 20:03:18 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:03:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.08
[32m[20230113 20:03:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.43
[32m[20230113 20:03:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 149.98
[32m[20230113 20:03:18 @agent_ppo2.py:144][0m Total time:      18.76 min
[32m[20230113 20:03:18 @agent_ppo2.py:146][0m 1695744 total steps have happened
[32m[20230113 20:03:18 @agent_ppo2.py:122][0m #------------------------ Iteration 828 --------------------------#
[32m[20230113 20:03:19 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:03:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:19 @agent_ppo2.py:186][0m |           0.0008 |           6.7633 |           5.8307 |
[32m[20230113 20:03:19 @agent_ppo2.py:186][0m |          -0.0036 |           5.6193 |           5.8263 |
[32m[20230113 20:03:19 @agent_ppo2.py:186][0m |          -0.0051 |           5.1998 |           5.8198 |
[32m[20230113 20:03:19 @agent_ppo2.py:186][0m |          -0.0073 |           4.9358 |           5.8170 |
[32m[20230113 20:03:19 @agent_ppo2.py:186][0m |          -0.0076 |           4.6171 |           5.8139 |
[32m[20230113 20:03:19 @agent_ppo2.py:186][0m |          -0.0092 |           4.3982 |           5.8149 |
[32m[20230113 20:03:19 @agent_ppo2.py:186][0m |          -0.0108 |           4.2514 |           5.8092 |
[32m[20230113 20:03:19 @agent_ppo2.py:186][0m |          -0.0108 |           4.1171 |           5.8134 |
[32m[20230113 20:03:19 @agent_ppo2.py:186][0m |          -0.0118 |           4.0254 |           5.8131 |
[32m[20230113 20:03:19 @agent_ppo2.py:186][0m |          -0.0124 |           3.9344 |           5.8090 |
[32m[20230113 20:03:19 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:03:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.54
[32m[20230113 20:03:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.89
[32m[20230113 20:03:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.54
[32m[20230113 20:03:20 @agent_ppo2.py:144][0m Total time:      18.79 min
[32m[20230113 20:03:20 @agent_ppo2.py:146][0m 1697792 total steps have happened
[32m[20230113 20:03:20 @agent_ppo2.py:122][0m #------------------------ Iteration 829 --------------------------#
[32m[20230113 20:03:20 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:03:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:20 @agent_ppo2.py:186][0m |           0.0068 |           6.2689 |           5.7236 |
[32m[20230113 20:03:20 @agent_ppo2.py:186][0m |          -0.0042 |           5.3439 |           5.7143 |
[32m[20230113 20:03:20 @agent_ppo2.py:186][0m |          -0.0084 |           4.9488 |           5.7011 |
[32m[20230113 20:03:20 @agent_ppo2.py:186][0m |          -0.0117 |           4.6628 |           5.7095 |
[32m[20230113 20:03:20 @agent_ppo2.py:186][0m |          -0.0121 |           4.4231 |           5.7046 |
[32m[20230113 20:03:20 @agent_ppo2.py:186][0m |          -0.0140 |           4.2472 |           5.7046 |
[32m[20230113 20:03:21 @agent_ppo2.py:186][0m |          -0.0100 |           4.1061 |           5.7100 |
[32m[20230113 20:03:21 @agent_ppo2.py:186][0m |          -0.0117 |           3.9812 |           5.7059 |
[32m[20230113 20:03:21 @agent_ppo2.py:186][0m |          -0.0137 |           3.9041 |           5.7035 |
[32m[20230113 20:03:21 @agent_ppo2.py:186][0m |          -0.0113 |           3.8261 |           5.7093 |
[32m[20230113 20:03:21 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:03:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.40
[32m[20230113 20:03:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.97
[32m[20230113 20:03:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 142.71
[32m[20230113 20:03:21 @agent_ppo2.py:144][0m Total time:      18.81 min
[32m[20230113 20:03:21 @agent_ppo2.py:146][0m 1699840 total steps have happened
[32m[20230113 20:03:21 @agent_ppo2.py:122][0m #------------------------ Iteration 830 --------------------------#
[32m[20230113 20:03:21 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 20:03:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:22 @agent_ppo2.py:186][0m |           0.0011 |          19.9447 |           5.7728 |
[32m[20230113 20:03:22 @agent_ppo2.py:186][0m |          -0.0038 |           9.5313 |           5.7723 |
[32m[20230113 20:03:22 @agent_ppo2.py:186][0m |          -0.0077 |           7.7427 |           5.7687 |
[32m[20230113 20:03:22 @agent_ppo2.py:186][0m |          -0.0113 |           6.9246 |           5.7625 |
[32m[20230113 20:03:22 @agent_ppo2.py:186][0m |          -0.0127 |           6.3085 |           5.7567 |
[32m[20230113 20:03:22 @agent_ppo2.py:186][0m |          -0.0143 |           5.9163 |           5.7522 |
[32m[20230113 20:03:22 @agent_ppo2.py:186][0m |          -0.0154 |           5.5719 |           5.7478 |
[32m[20230113 20:03:22 @agent_ppo2.py:186][0m |          -0.0165 |           5.3437 |           5.7458 |
[32m[20230113 20:03:22 @agent_ppo2.py:186][0m |          -0.0172 |           5.1266 |           5.7423 |
[32m[20230113 20:03:22 @agent_ppo2.py:186][0m |          -0.0178 |           4.8226 |           5.7401 |
[32m[20230113 20:03:22 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 20:03:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 42.61
[32m[20230113 20:03:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.88
[32m[20230113 20:03:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.45
[32m[20230113 20:03:22 @agent_ppo2.py:144][0m Total time:      18.83 min
[32m[20230113 20:03:22 @agent_ppo2.py:146][0m 1701888 total steps have happened
[32m[20230113 20:03:22 @agent_ppo2.py:122][0m #------------------------ Iteration 831 --------------------------#
[32m[20230113 20:03:23 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:03:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:23 @agent_ppo2.py:186][0m |          -0.0002 |           8.1085 |           5.6124 |
[32m[20230113 20:03:23 @agent_ppo2.py:186][0m |          -0.0011 |           6.2992 |           5.6058 |
[32m[20230113 20:03:23 @agent_ppo2.py:186][0m |          -0.0065 |           5.4893 |           5.6081 |
[32m[20230113 20:03:23 @agent_ppo2.py:186][0m |          -0.0033 |           4.9847 |           5.6071 |
[32m[20230113 20:03:23 @agent_ppo2.py:186][0m |          -0.0082 |           4.6042 |           5.6071 |
[32m[20230113 20:03:23 @agent_ppo2.py:186][0m |          -0.0057 |           4.4176 |           5.6006 |
[32m[20230113 20:03:23 @agent_ppo2.py:186][0m |          -0.0157 |           4.1179 |           5.6033 |
[32m[20230113 20:03:23 @agent_ppo2.py:186][0m |          -0.0059 |           3.9195 |           5.5981 |
[32m[20230113 20:03:23 @agent_ppo2.py:186][0m |          -0.0151 |           3.7847 |           5.6001 |
[32m[20230113 20:03:23 @agent_ppo2.py:186][0m |          -0.0125 |           3.6459 |           5.6030 |
[32m[20230113 20:03:23 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:03:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.09
[32m[20230113 20:03:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.36
[32m[20230113 20:03:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 177.68
[32m[20230113 20:03:24 @agent_ppo2.py:144][0m Total time:      18.85 min
[32m[20230113 20:03:24 @agent_ppo2.py:146][0m 1703936 total steps have happened
[32m[20230113 20:03:24 @agent_ppo2.py:122][0m #------------------------ Iteration 832 --------------------------#
[32m[20230113 20:03:24 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:03:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:24 @agent_ppo2.py:186][0m |          -0.0007 |           5.9437 |           5.7467 |
[32m[20230113 20:03:24 @agent_ppo2.py:186][0m |          -0.0051 |           4.7160 |           5.7336 |
[32m[20230113 20:03:24 @agent_ppo2.py:186][0m |          -0.0065 |           4.2153 |           5.7298 |
[32m[20230113 20:03:24 @agent_ppo2.py:186][0m |          -0.0050 |           3.8761 |           5.7300 |
[32m[20230113 20:03:24 @agent_ppo2.py:186][0m |          -0.0083 |           3.6292 |           5.7132 |
[32m[20230113 20:03:24 @agent_ppo2.py:186][0m |          -0.0095 |           3.4131 |           5.7208 |
[32m[20230113 20:03:24 @agent_ppo2.py:186][0m |          -0.0093 |           3.2408 |           5.7234 |
[32m[20230113 20:03:24 @agent_ppo2.py:186][0m |          -0.0094 |           3.1353 |           5.7118 |
[32m[20230113 20:03:24 @agent_ppo2.py:186][0m |          -0.0106 |           2.9875 |           5.7149 |
[32m[20230113 20:03:24 @agent_ppo2.py:186][0m |          -0.0114 |           2.8964 |           5.7151 |
[32m[20230113 20:03:24 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.34
[32m[20230113 20:03:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.07
[32m[20230113 20:03:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.14
[32m[20230113 20:03:25 @agent_ppo2.py:144][0m Total time:      18.87 min
[32m[20230113 20:03:25 @agent_ppo2.py:146][0m 1705984 total steps have happened
[32m[20230113 20:03:25 @agent_ppo2.py:122][0m #------------------------ Iteration 833 --------------------------#
[32m[20230113 20:03:25 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:03:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:25 @agent_ppo2.py:186][0m |          -0.0006 |           6.8885 |           5.6121 |
[32m[20230113 20:03:25 @agent_ppo2.py:186][0m |          -0.0058 |           5.5164 |           5.6097 |
[32m[20230113 20:03:25 @agent_ppo2.py:186][0m |          -0.0085 |           5.1243 |           5.6078 |
[32m[20230113 20:03:26 @agent_ppo2.py:186][0m |          -0.0109 |           4.9411 |           5.6026 |
[32m[20230113 20:03:26 @agent_ppo2.py:186][0m |          -0.0106 |           4.5942 |           5.6030 |
[32m[20230113 20:03:26 @agent_ppo2.py:186][0m |          -0.0123 |           4.4162 |           5.6025 |
[32m[20230113 20:03:26 @agent_ppo2.py:186][0m |          -0.0135 |           4.2133 |           5.6049 |
[32m[20230113 20:03:26 @agent_ppo2.py:186][0m |          -0.0129 |           4.1009 |           5.6029 |
[32m[20230113 20:03:26 @agent_ppo2.py:186][0m |          -0.0134 |           3.9643 |           5.6026 |
[32m[20230113 20:03:26 @agent_ppo2.py:186][0m |          -0.0134 |           3.8110 |           5.6056 |
[32m[20230113 20:03:26 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.79
[32m[20230113 20:03:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.01
[32m[20230113 20:03:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.07
[32m[20230113 20:03:26 @agent_ppo2.py:144][0m Total time:      18.89 min
[32m[20230113 20:03:26 @agent_ppo2.py:146][0m 1708032 total steps have happened
[32m[20230113 20:03:26 @agent_ppo2.py:122][0m #------------------------ Iteration 834 --------------------------#
[32m[20230113 20:03:27 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:03:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:27 @agent_ppo2.py:186][0m |           0.0005 |           5.9449 |           5.7340 |
[32m[20230113 20:03:27 @agent_ppo2.py:186][0m |          -0.0021 |           5.1545 |           5.7207 |
[32m[20230113 20:03:27 @agent_ppo2.py:186][0m |          -0.0059 |           4.8612 |           5.7126 |
[32m[20230113 20:03:27 @agent_ppo2.py:186][0m |          -0.0063 |           4.6334 |           5.7103 |
[32m[20230113 20:03:27 @agent_ppo2.py:186][0m |          -0.0071 |           4.4508 |           5.7132 |
[32m[20230113 20:03:27 @agent_ppo2.py:186][0m |          -0.0087 |           4.3150 |           5.7071 |
[32m[20230113 20:03:27 @agent_ppo2.py:186][0m |          -0.0093 |           4.1998 |           5.7055 |
[32m[20230113 20:03:27 @agent_ppo2.py:186][0m |          -0.0105 |           4.0758 |           5.7084 |
[32m[20230113 20:03:27 @agent_ppo2.py:186][0m |          -0.0118 |           3.9658 |           5.7044 |
[32m[20230113 20:03:27 @agent_ppo2.py:186][0m |          -0.0118 |           3.8697 |           5.7008 |
[32m[20230113 20:03:27 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.57
[32m[20230113 20:03:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.69
[32m[20230113 20:03:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.50
[32m[20230113 20:03:27 @agent_ppo2.py:144][0m Total time:      18.92 min
[32m[20230113 20:03:27 @agent_ppo2.py:146][0m 1710080 total steps have happened
[32m[20230113 20:03:27 @agent_ppo2.py:122][0m #------------------------ Iteration 835 --------------------------#
[32m[20230113 20:03:28 @agent_ppo2.py:128][0m Sampling time: 0.35 s by 1 slaves
[32m[20230113 20:03:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:28 @agent_ppo2.py:186][0m |          -0.0023 |          16.5465 |           5.7246 |
[32m[20230113 20:03:28 @agent_ppo2.py:186][0m |          -0.0077 |           7.9620 |           5.7127 |
[32m[20230113 20:03:28 @agent_ppo2.py:186][0m |          -0.0098 |           6.3251 |           5.7135 |
[32m[20230113 20:03:28 @agent_ppo2.py:186][0m |          -0.0114 |           5.5433 |           5.7077 |
[32m[20230113 20:03:28 @agent_ppo2.py:186][0m |          -0.0131 |           5.0932 |           5.7049 |
[32m[20230113 20:03:28 @agent_ppo2.py:186][0m |          -0.0143 |           4.7751 |           5.7088 |
[32m[20230113 20:03:28 @agent_ppo2.py:186][0m |          -0.0151 |           4.5468 |           5.7047 |
[32m[20230113 20:03:28 @agent_ppo2.py:186][0m |          -0.0164 |           4.3045 |           5.7049 |
[32m[20230113 20:03:28 @agent_ppo2.py:186][0m |          -0.0159 |           4.1668 |           5.7018 |
[32m[20230113 20:03:28 @agent_ppo2.py:186][0m |          -0.0173 |           3.9502 |           5.6980 |
[32m[20230113 20:03:28 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 20:03:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 117.63
[32m[20230113 20:03:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.48
[32m[20230113 20:03:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.76
[32m[20230113 20:03:29 @agent_ppo2.py:144][0m Total time:      18.93 min
[32m[20230113 20:03:29 @agent_ppo2.py:146][0m 1712128 total steps have happened
[32m[20230113 20:03:29 @agent_ppo2.py:122][0m #------------------------ Iteration 836 --------------------------#
[32m[20230113 20:03:29 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:03:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:29 @agent_ppo2.py:186][0m |           0.0007 |          31.6866 |           5.6019 |
[32m[20230113 20:03:29 @agent_ppo2.py:186][0m |          -0.0047 |          18.8504 |           5.5995 |
[32m[20230113 20:03:29 @agent_ppo2.py:186][0m |          -0.0061 |          14.0932 |           5.5903 |
[32m[20230113 20:03:29 @agent_ppo2.py:186][0m |          -0.0088 |          11.5266 |           5.5918 |
[32m[20230113 20:03:29 @agent_ppo2.py:186][0m |          -0.0093 |           9.7649 |           5.5886 |
[32m[20230113 20:03:29 @agent_ppo2.py:186][0m |          -0.0107 |           8.4118 |           5.5884 |
[32m[20230113 20:03:29 @agent_ppo2.py:186][0m |          -0.0113 |           7.4433 |           5.5879 |
[32m[20230113 20:03:29 @agent_ppo2.py:186][0m |          -0.0102 |           6.7052 |           5.5906 |
[32m[20230113 20:03:29 @agent_ppo2.py:186][0m |          -0.0123 |           6.1212 |           5.5857 |
[32m[20230113 20:03:29 @agent_ppo2.py:186][0m |          -0.0128 |           5.6338 |           5.5848 |
[32m[20230113 20:03:29 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.07
[32m[20230113 20:03:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.92
[32m[20230113 20:03:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.06
[32m[20230113 20:03:30 @agent_ppo2.py:144][0m Total time:      18.96 min
[32m[20230113 20:03:30 @agent_ppo2.py:146][0m 1714176 total steps have happened
[32m[20230113 20:03:30 @agent_ppo2.py:122][0m #------------------------ Iteration 837 --------------------------#
[32m[20230113 20:03:30 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:03:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:30 @agent_ppo2.py:186][0m |          -0.0026 |          29.6658 |           5.7744 |
[32m[20230113 20:03:30 @agent_ppo2.py:186][0m |          -0.0090 |          15.8220 |           5.7641 |
[32m[20230113 20:03:30 @agent_ppo2.py:186][0m |          -0.0087 |          11.9938 |           5.7522 |
[32m[20230113 20:03:30 @agent_ppo2.py:186][0m |          -0.0114 |          10.1393 |           5.7561 |
[32m[20230113 20:03:30 @agent_ppo2.py:186][0m |          -0.0157 |           9.2443 |           5.7520 |
[32m[20230113 20:03:31 @agent_ppo2.py:186][0m |           0.0009 |          10.3463 |           5.7547 |
[32m[20230113 20:03:31 @agent_ppo2.py:186][0m |          -0.0071 |           8.3221 |           5.7536 |
[32m[20230113 20:03:31 @agent_ppo2.py:186][0m |          -0.0154 |           8.0123 |           5.7536 |
[32m[20230113 20:03:31 @agent_ppo2.py:186][0m |          -0.0193 |           7.7130 |           5.7525 |
[32m[20230113 20:03:31 @agent_ppo2.py:186][0m |          -0.0190 |           7.3427 |           5.7509 |
[32m[20230113 20:03:31 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 20:03:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 133.79
[32m[20230113 20:03:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.14
[32m[20230113 20:03:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.65
[32m[20230113 20:03:31 @agent_ppo2.py:144][0m Total time:      18.98 min
[32m[20230113 20:03:31 @agent_ppo2.py:146][0m 1716224 total steps have happened
[32m[20230113 20:03:31 @agent_ppo2.py:122][0m #------------------------ Iteration 838 --------------------------#
[32m[20230113 20:03:32 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:03:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:32 @agent_ppo2.py:186][0m |           0.0012 |           8.1108 |           5.6112 |
[32m[20230113 20:03:32 @agent_ppo2.py:186][0m |          -0.0044 |           6.3008 |           5.6100 |
[32m[20230113 20:03:32 @agent_ppo2.py:186][0m |          -0.0083 |           5.6472 |           5.6051 |
[32m[20230113 20:03:32 @agent_ppo2.py:186][0m |          -0.0079 |           5.2437 |           5.6053 |
[32m[20230113 20:03:32 @agent_ppo2.py:186][0m |          -0.0084 |           4.9218 |           5.6000 |
[32m[20230113 20:03:32 @agent_ppo2.py:186][0m |          -0.0097 |           4.6310 |           5.5990 |
[32m[20230113 20:03:32 @agent_ppo2.py:186][0m |          -0.0108 |           4.4479 |           5.5952 |
[32m[20230113 20:03:32 @agent_ppo2.py:186][0m |          -0.0112 |           4.2863 |           5.5982 |
[32m[20230113 20:03:32 @agent_ppo2.py:186][0m |          -0.0115 |           4.1515 |           5.5932 |
[32m[20230113 20:03:32 @agent_ppo2.py:186][0m |          -0.0125 |           4.0819 |           5.5922 |
[32m[20230113 20:03:32 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.06
[32m[20230113 20:03:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.17
[32m[20230113 20:03:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 145.55
[32m[20230113 20:03:32 @agent_ppo2.py:144][0m Total time:      19.00 min
[32m[20230113 20:03:32 @agent_ppo2.py:146][0m 1718272 total steps have happened
[32m[20230113 20:03:32 @agent_ppo2.py:122][0m #------------------------ Iteration 839 --------------------------#
[32m[20230113 20:03:33 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:03:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:33 @agent_ppo2.py:186][0m |          -0.0036 |           6.5613 |           5.7443 |
[32m[20230113 20:03:33 @agent_ppo2.py:186][0m |          -0.0071 |           5.6953 |           5.7383 |
[32m[20230113 20:03:33 @agent_ppo2.py:186][0m |          -0.0154 |           5.3852 |           5.7346 |
[32m[20230113 20:03:33 @agent_ppo2.py:186][0m |          -0.0134 |           5.1578 |           5.7405 |
[32m[20230113 20:03:33 @agent_ppo2.py:186][0m |          -0.0058 |           5.0906 |           5.7392 |
[32m[20230113 20:03:33 @agent_ppo2.py:186][0m |          -0.0194 |           4.8854 |           5.7384 |
[32m[20230113 20:03:33 @agent_ppo2.py:186][0m |          -0.0051 |           4.6874 |           5.7407 |
[32m[20230113 20:03:33 @agent_ppo2.py:186][0m |          -0.0163 |           4.5051 |           5.7362 |
[32m[20230113 20:03:33 @agent_ppo2.py:186][0m |           0.0010 |           5.4552 |           5.7403 |
[32m[20230113 20:03:33 @agent_ppo2.py:186][0m |          -0.0100 |           4.4054 |           5.7360 |
[32m[20230113 20:03:33 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.99
[32m[20230113 20:03:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.12
[32m[20230113 20:03:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.45
[32m[20230113 20:03:34 @agent_ppo2.py:144][0m Total time:      19.02 min
[32m[20230113 20:03:34 @agent_ppo2.py:146][0m 1720320 total steps have happened
[32m[20230113 20:03:34 @agent_ppo2.py:122][0m #------------------------ Iteration 840 --------------------------#
[32m[20230113 20:03:34 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:03:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:34 @agent_ppo2.py:186][0m |          -0.0014 |          15.9537 |           5.8059 |
[32m[20230113 20:03:34 @agent_ppo2.py:186][0m |          -0.0062 |          10.6578 |           5.7988 |
[32m[20230113 20:03:34 @agent_ppo2.py:186][0m |          -0.0090 |           8.7324 |           5.7948 |
[32m[20230113 20:03:34 @agent_ppo2.py:186][0m |          -0.0096 |           7.5248 |           5.7931 |
[32m[20230113 20:03:34 @agent_ppo2.py:186][0m |          -0.0102 |           6.8802 |           5.7931 |
[32m[20230113 20:03:34 @agent_ppo2.py:186][0m |          -0.0116 |           6.4102 |           5.7831 |
[32m[20230113 20:03:35 @agent_ppo2.py:186][0m |          -0.0113 |           6.0204 |           5.7899 |
[32m[20230113 20:03:35 @agent_ppo2.py:186][0m |          -0.0119 |           5.7561 |           5.7827 |
[32m[20230113 20:03:35 @agent_ppo2.py:186][0m |          -0.0137 |           5.4739 |           5.7854 |
[32m[20230113 20:03:35 @agent_ppo2.py:186][0m |          -0.0147 |           5.2541 |           5.7852 |
[32m[20230113 20:03:35 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 20:03:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 139.98
[32m[20230113 20:03:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.84
[32m[20230113 20:03:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.77
[32m[20230113 20:03:35 @agent_ppo2.py:144][0m Total time:      19.04 min
[32m[20230113 20:03:35 @agent_ppo2.py:146][0m 1722368 total steps have happened
[32m[20230113 20:03:35 @agent_ppo2.py:122][0m #------------------------ Iteration 841 --------------------------#
[32m[20230113 20:03:36 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:03:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:36 @agent_ppo2.py:186][0m |          -0.0008 |           6.3036 |           5.7185 |
[32m[20230113 20:03:36 @agent_ppo2.py:186][0m |          -0.0070 |           5.4671 |           5.7094 |
[32m[20230113 20:03:36 @agent_ppo2.py:186][0m |          -0.0064 |           5.1266 |           5.7003 |
[32m[20230113 20:03:36 @agent_ppo2.py:186][0m |          -0.0103 |           4.8435 |           5.6973 |
[32m[20230113 20:03:36 @agent_ppo2.py:186][0m |          -0.0136 |           4.7122 |           5.6940 |
[32m[20230113 20:03:36 @agent_ppo2.py:186][0m |          -0.0147 |           4.5479 |           5.6925 |
[32m[20230113 20:03:36 @agent_ppo2.py:186][0m |          -0.0172 |           4.4120 |           5.6958 |
[32m[20230113 20:03:36 @agent_ppo2.py:186][0m |          -0.0125 |           4.3227 |           5.6938 |
[32m[20230113 20:03:36 @agent_ppo2.py:186][0m |          -0.0137 |           4.2335 |           5.6903 |
[32m[20230113 20:03:36 @agent_ppo2.py:186][0m |          -0.0167 |           4.1078 |           5.6863 |
[32m[20230113 20:03:36 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:03:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.85
[32m[20230113 20:03:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.51
[32m[20230113 20:03:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.57
[32m[20230113 20:03:36 @agent_ppo2.py:144][0m Total time:      19.06 min
[32m[20230113 20:03:36 @agent_ppo2.py:146][0m 1724416 total steps have happened
[32m[20230113 20:03:36 @agent_ppo2.py:122][0m #------------------------ Iteration 842 --------------------------#
[32m[20230113 20:03:37 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:03:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:37 @agent_ppo2.py:186][0m |           0.0015 |           7.3920 |           5.7682 |
[32m[20230113 20:03:37 @agent_ppo2.py:186][0m |          -0.0067 |           5.7435 |           5.7627 |
[32m[20230113 20:03:37 @agent_ppo2.py:186][0m |          -0.0079 |           5.3448 |           5.7630 |
[32m[20230113 20:03:37 @agent_ppo2.py:186][0m |          -0.0103 |           5.0206 |           5.7579 |
[32m[20230113 20:03:37 @agent_ppo2.py:186][0m |          -0.0108 |           4.8173 |           5.7559 |
[32m[20230113 20:03:37 @agent_ppo2.py:186][0m |          -0.0117 |           4.6277 |           5.7577 |
[32m[20230113 20:03:37 @agent_ppo2.py:186][0m |          -0.0126 |           4.5554 |           5.7550 |
[32m[20230113 20:03:37 @agent_ppo2.py:186][0m |          -0.0150 |           4.3923 |           5.7568 |
[32m[20230113 20:03:37 @agent_ppo2.py:186][0m |          -0.0163 |           4.2976 |           5.7564 |
[32m[20230113 20:03:37 @agent_ppo2.py:186][0m |          -0.0116 |           4.2616 |           5.7546 |
[32m[20230113 20:03:37 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.30
[32m[20230113 20:03:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.78
[32m[20230113 20:03:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 170.34
[32m[20230113 20:03:38 @agent_ppo2.py:144][0m Total time:      19.09 min
[32m[20230113 20:03:38 @agent_ppo2.py:146][0m 1726464 total steps have happened
[32m[20230113 20:03:38 @agent_ppo2.py:122][0m #------------------------ Iteration 843 --------------------------#
[32m[20230113 20:03:38 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:03:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:38 @agent_ppo2.py:186][0m |          -0.0005 |           5.6926 |           5.7941 |
[32m[20230113 20:03:38 @agent_ppo2.py:186][0m |          -0.0049 |           4.8995 |           5.7918 |
[32m[20230113 20:03:38 @agent_ppo2.py:186][0m |          -0.0072 |           4.6190 |           5.7888 |
[32m[20230113 20:03:38 @agent_ppo2.py:186][0m |          -0.0093 |           4.4149 |           5.7808 |
[32m[20230113 20:03:38 @agent_ppo2.py:186][0m |          -0.0123 |           4.2290 |           5.7726 |
[32m[20230113 20:03:38 @agent_ppo2.py:186][0m |          -0.0120 |           4.0774 |           5.7716 |
[32m[20230113 20:03:38 @agent_ppo2.py:186][0m |          -0.0117 |           4.0289 |           5.7659 |
[32m[20230113 20:03:39 @agent_ppo2.py:186][0m |          -0.0134 |           3.9112 |           5.7631 |
[32m[20230113 20:03:39 @agent_ppo2.py:186][0m |          -0.0153 |           3.8223 |           5.7636 |
[32m[20230113 20:03:39 @agent_ppo2.py:186][0m |          -0.0148 |           3.7080 |           5.7619 |
[32m[20230113 20:03:39 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:03:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.30
[32m[20230113 20:03:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.26
[32m[20230113 20:03:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.93
[32m[20230113 20:03:39 @agent_ppo2.py:144][0m Total time:      19.11 min
[32m[20230113 20:03:39 @agent_ppo2.py:146][0m 1728512 total steps have happened
[32m[20230113 20:03:39 @agent_ppo2.py:122][0m #------------------------ Iteration 844 --------------------------#
[32m[20230113 20:03:39 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 20:03:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:39 @agent_ppo2.py:186][0m |          -0.0004 |          15.7448 |           5.6760 |
[32m[20230113 20:03:40 @agent_ppo2.py:186][0m |          -0.0058 |           6.1220 |           5.6747 |
[32m[20230113 20:03:40 @agent_ppo2.py:186][0m |          -0.0102 |           4.3019 |           5.6661 |
[32m[20230113 20:03:40 @agent_ppo2.py:186][0m |          -0.0148 |           3.8016 |           5.6644 |
[32m[20230113 20:03:40 @agent_ppo2.py:186][0m |          -0.0137 |           3.5095 |           5.6594 |
[32m[20230113 20:03:40 @agent_ppo2.py:186][0m |          -0.0146 |           3.4076 |           5.6595 |
[32m[20230113 20:03:40 @agent_ppo2.py:186][0m |          -0.0089 |           3.2388 |           5.6557 |
[32m[20230113 20:03:40 @agent_ppo2.py:186][0m |          -0.0100 |           3.1089 |           5.6495 |
[32m[20230113 20:03:40 @agent_ppo2.py:186][0m |          -0.0194 |           3.0098 |           5.6527 |
[32m[20230113 20:03:40 @agent_ppo2.py:186][0m |          -0.0147 |           3.0014 |           5.6520 |
[32m[20230113 20:03:40 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:03:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 148.48
[32m[20230113 20:03:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.23
[32m[20230113 20:03:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.88
[32m[20230113 20:03:40 @agent_ppo2.py:144][0m Total time:      19.13 min
[32m[20230113 20:03:40 @agent_ppo2.py:146][0m 1730560 total steps have happened
[32m[20230113 20:03:40 @agent_ppo2.py:122][0m #------------------------ Iteration 845 --------------------------#
[32m[20230113 20:03:41 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:03:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:41 @agent_ppo2.py:186][0m |          -0.0010 |           7.5769 |           5.8146 |
[32m[20230113 20:03:41 @agent_ppo2.py:186][0m |          -0.0083 |           4.9460 |           5.8126 |
[32m[20230113 20:03:41 @agent_ppo2.py:186][0m |          -0.0090 |           4.3314 |           5.8080 |
[32m[20230113 20:03:41 @agent_ppo2.py:186][0m |          -0.0140 |           4.0258 |           5.8074 |
[32m[20230113 20:03:41 @agent_ppo2.py:186][0m |          -0.0122 |           3.8227 |           5.8108 |
[32m[20230113 20:03:41 @agent_ppo2.py:186][0m |          -0.0123 |           3.6297 |           5.8062 |
[32m[20230113 20:03:41 @agent_ppo2.py:186][0m |          -0.0148 |           3.4691 |           5.8087 |
[32m[20230113 20:03:41 @agent_ppo2.py:186][0m |          -0.0159 |           3.3458 |           5.8034 |
[32m[20230113 20:03:41 @agent_ppo2.py:186][0m |          -0.0147 |           3.2790 |           5.8077 |
[32m[20230113 20:03:41 @agent_ppo2.py:186][0m |          -0.0161 |           3.1669 |           5.8056 |
[32m[20230113 20:03:41 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:03:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.90
[32m[20230113 20:03:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.99
[32m[20230113 20:03:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.88
[32m[20230113 20:03:42 @agent_ppo2.py:144][0m Total time:      19.15 min
[32m[20230113 20:03:42 @agent_ppo2.py:146][0m 1732608 total steps have happened
[32m[20230113 20:03:42 @agent_ppo2.py:122][0m #------------------------ Iteration 846 --------------------------#
[32m[20230113 20:03:42 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:03:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:42 @agent_ppo2.py:186][0m |           0.0018 |          16.0961 |           5.7819 |
[32m[20230113 20:03:42 @agent_ppo2.py:186][0m |          -0.0086 |           7.9791 |           5.7780 |
[32m[20230113 20:03:42 @agent_ppo2.py:186][0m |          -0.0066 |           6.9083 |           5.7769 |
[32m[20230113 20:03:42 @agent_ppo2.py:186][0m |          -0.0101 |           6.2261 |           5.7784 |
[32m[20230113 20:03:42 @agent_ppo2.py:186][0m |          -0.0090 |           5.6530 |           5.7698 |
[32m[20230113 20:03:42 @agent_ppo2.py:186][0m |          -0.0163 |           5.3123 |           5.7733 |
[32m[20230113 20:03:42 @agent_ppo2.py:186][0m |          -0.0184 |           5.1244 |           5.7746 |
[32m[20230113 20:03:42 @agent_ppo2.py:186][0m |          -0.0136 |           4.8495 |           5.7754 |
[32m[20230113 20:03:42 @agent_ppo2.py:186][0m |          -0.0114 |           4.6941 |           5.7750 |
[32m[20230113 20:03:42 @agent_ppo2.py:186][0m |          -0.0181 |           4.6479 |           5.7730 |
[32m[20230113 20:03:42 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:03:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 114.87
[32m[20230113 20:03:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.02
[32m[20230113 20:03:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.70
[32m[20230113 20:03:43 @agent_ppo2.py:144][0m Total time:      19.17 min
[32m[20230113 20:03:43 @agent_ppo2.py:146][0m 1734656 total steps have happened
[32m[20230113 20:03:43 @agent_ppo2.py:122][0m #------------------------ Iteration 847 --------------------------#
[32m[20230113 20:03:43 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:03:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:43 @agent_ppo2.py:186][0m |          -0.0002 |           5.3604 |           5.6676 |
[32m[20230113 20:03:43 @agent_ppo2.py:186][0m |           0.0008 |           3.8667 |           5.6551 |
[32m[20230113 20:03:43 @agent_ppo2.py:186][0m |          -0.0054 |           3.5734 |           5.6453 |
[32m[20230113 20:03:43 @agent_ppo2.py:186][0m |          -0.0106 |           3.3468 |           5.6500 |
[32m[20230113 20:03:43 @agent_ppo2.py:186][0m |          -0.0084 |           3.1666 |           5.6477 |
[32m[20230113 20:03:43 @agent_ppo2.py:186][0m |          -0.0107 |           3.0899 |           5.6424 |
[32m[20230113 20:03:43 @agent_ppo2.py:186][0m |          -0.0086 |           2.9439 |           5.6423 |
[32m[20230113 20:03:43 @agent_ppo2.py:186][0m |          -0.0161 |           2.8944 |           5.6428 |
[32m[20230113 20:03:43 @agent_ppo2.py:186][0m |          -0.0096 |           2.8277 |           5.6418 |
[32m[20230113 20:03:44 @agent_ppo2.py:186][0m |          -0.0125 |           2.7356 |           5.6400 |
[32m[20230113 20:03:44 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:03:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.26
[32m[20230113 20:03:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.22
[32m[20230113 20:03:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.15
[32m[20230113 20:03:44 @agent_ppo2.py:144][0m Total time:      19.19 min
[32m[20230113 20:03:44 @agent_ppo2.py:146][0m 1736704 total steps have happened
[32m[20230113 20:03:44 @agent_ppo2.py:122][0m #------------------------ Iteration 848 --------------------------#
[32m[20230113 20:03:44 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:03:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:44 @agent_ppo2.py:186][0m |          -0.0000 |           6.9549 |           5.8416 |
[32m[20230113 20:03:45 @agent_ppo2.py:186][0m |          -0.0046 |           5.1922 |           5.8278 |
[32m[20230113 20:03:45 @agent_ppo2.py:186][0m |          -0.0071 |           4.6050 |           5.8259 |
[32m[20230113 20:03:45 @agent_ppo2.py:186][0m |          -0.0113 |           4.2300 |           5.8232 |
[32m[20230113 20:03:45 @agent_ppo2.py:186][0m |          -0.0104 |           3.9967 |           5.8156 |
[32m[20230113 20:03:45 @agent_ppo2.py:186][0m |          -0.0105 |           3.8275 |           5.8149 |
[32m[20230113 20:03:45 @agent_ppo2.py:186][0m |          -0.0122 |           3.7154 |           5.8165 |
[32m[20230113 20:03:45 @agent_ppo2.py:186][0m |          -0.0148 |           3.5712 |           5.8182 |
[32m[20230113 20:03:45 @agent_ppo2.py:186][0m |          -0.0158 |           3.4501 |           5.8126 |
[32m[20230113 20:03:45 @agent_ppo2.py:186][0m |          -0.0129 |           3.3853 |           5.8110 |
[32m[20230113 20:03:45 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:03:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.97
[32m[20230113 20:03:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.87
[32m[20230113 20:03:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.83
[32m[20230113 20:03:45 @agent_ppo2.py:144][0m Total time:      19.21 min
[32m[20230113 20:03:45 @agent_ppo2.py:146][0m 1738752 total steps have happened
[32m[20230113 20:03:45 @agent_ppo2.py:122][0m #------------------------ Iteration 849 --------------------------#
[32m[20230113 20:03:46 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:03:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:46 @agent_ppo2.py:186][0m |          -0.0089 |           5.9093 |           5.5717 |
[32m[20230113 20:03:46 @agent_ppo2.py:186][0m |          -0.0046 |           4.6786 |           5.5622 |
[32m[20230113 20:03:46 @agent_ppo2.py:186][0m |          -0.0005 |           4.2216 |           5.5642 |
[32m[20230113 20:03:46 @agent_ppo2.py:186][0m |          -0.0006 |           4.0296 |           5.5636 |
[32m[20230113 20:03:46 @agent_ppo2.py:186][0m |          -0.0132 |           3.8590 |           5.5641 |
[32m[20230113 20:03:46 @agent_ppo2.py:186][0m |          -0.0128 |           3.6894 |           5.5612 |
[32m[20230113 20:03:46 @agent_ppo2.py:186][0m |          -0.0094 |           3.5639 |           5.5655 |
[32m[20230113 20:03:46 @agent_ppo2.py:186][0m |          -0.0178 |           3.4631 |           5.5619 |
[32m[20230113 20:03:46 @agent_ppo2.py:186][0m |          -0.0153 |           3.4808 |           5.5647 |
[32m[20230113 20:03:46 @agent_ppo2.py:186][0m |          -0.0103 |           3.3644 |           5.5601 |
[32m[20230113 20:03:46 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:03:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.70
[32m[20230113 20:03:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.27
[32m[20230113 20:03:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.13
[32m[20230113 20:03:47 @agent_ppo2.py:144][0m Total time:      19.23 min
[32m[20230113 20:03:47 @agent_ppo2.py:146][0m 1740800 total steps have happened
[32m[20230113 20:03:47 @agent_ppo2.py:122][0m #------------------------ Iteration 850 --------------------------#
[32m[20230113 20:03:47 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:03:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:47 @agent_ppo2.py:186][0m |          -0.0002 |           5.1143 |           5.6124 |
[32m[20230113 20:03:47 @agent_ppo2.py:186][0m |          -0.0066 |           4.5359 |           5.6003 |
[32m[20230113 20:03:47 @agent_ppo2.py:186][0m |          -0.0094 |           4.2550 |           5.6069 |
[32m[20230113 20:03:47 @agent_ppo2.py:186][0m |          -0.0105 |           4.0950 |           5.6066 |
[32m[20230113 20:03:47 @agent_ppo2.py:186][0m |          -0.0110 |           3.9981 |           5.6005 |
[32m[20230113 20:03:47 @agent_ppo2.py:186][0m |          -0.0125 |           3.8683 |           5.6031 |
[32m[20230113 20:03:47 @agent_ppo2.py:186][0m |          -0.0136 |           3.7655 |           5.6049 |
[32m[20230113 20:03:47 @agent_ppo2.py:186][0m |          -0.0140 |           3.7388 |           5.6061 |
[32m[20230113 20:03:47 @agent_ppo2.py:186][0m |          -0.0155 |           3.6169 |           5.6029 |
[32m[20230113 20:03:48 @agent_ppo2.py:186][0m |          -0.0159 |           3.5024 |           5.6063 |
[32m[20230113 20:03:48 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:03:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.79
[32m[20230113 20:03:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.98
[32m[20230113 20:03:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.50
[32m[20230113 20:03:48 @agent_ppo2.py:144][0m Total time:      19.26 min
[32m[20230113 20:03:48 @agent_ppo2.py:146][0m 1742848 total steps have happened
[32m[20230113 20:03:48 @agent_ppo2.py:122][0m #------------------------ Iteration 851 --------------------------#
[32m[20230113 20:03:48 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 20:03:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:49 @agent_ppo2.py:186][0m |           0.0019 |          13.9405 |           5.7419 |
[32m[20230113 20:03:49 @agent_ppo2.py:186][0m |          -0.0071 |           8.2029 |           5.7352 |
[32m[20230113 20:03:49 @agent_ppo2.py:186][0m |          -0.0064 |           7.1209 |           5.7241 |
[32m[20230113 20:03:49 @agent_ppo2.py:186][0m |          -0.0094 |           6.6897 |           5.7265 |
[32m[20230113 20:03:49 @agent_ppo2.py:186][0m |          -0.0088 |           6.2246 |           5.7288 |
[32m[20230113 20:03:49 @agent_ppo2.py:186][0m |          -0.0111 |           5.9607 |           5.7272 |
[32m[20230113 20:03:49 @agent_ppo2.py:186][0m |          -0.0125 |           5.8000 |           5.7246 |
[32m[20230113 20:03:49 @agent_ppo2.py:186][0m |          -0.0121 |           5.5641 |           5.7215 |
[32m[20230113 20:03:49 @agent_ppo2.py:186][0m |          -0.0143 |           5.3708 |           5.7229 |
[32m[20230113 20:03:49 @agent_ppo2.py:186][0m |          -0.0135 |           5.2345 |           5.7216 |
[32m[20230113 20:03:49 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 20:03:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 129.76
[32m[20230113 20:03:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.59
[32m[20230113 20:03:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.21
[32m[20230113 20:03:49 @agent_ppo2.py:144][0m Total time:      19.28 min
[32m[20230113 20:03:49 @agent_ppo2.py:146][0m 1744896 total steps have happened
[32m[20230113 20:03:49 @agent_ppo2.py:122][0m #------------------------ Iteration 852 --------------------------#
[32m[20230113 20:03:50 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:03:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:50 @agent_ppo2.py:186][0m |           0.0012 |           6.3515 |           5.7139 |
[32m[20230113 20:03:50 @agent_ppo2.py:186][0m |          -0.0047 |           5.2604 |           5.7198 |
[32m[20230113 20:03:50 @agent_ppo2.py:186][0m |          -0.0069 |           4.7428 |           5.7201 |
[32m[20230113 20:03:50 @agent_ppo2.py:186][0m |          -0.0083 |           4.5142 |           5.7240 |
[32m[20230113 20:03:50 @agent_ppo2.py:186][0m |          -0.0098 |           4.3331 |           5.7280 |
[32m[20230113 20:03:50 @agent_ppo2.py:186][0m |          -0.0105 |           4.2032 |           5.7290 |
[32m[20230113 20:03:50 @agent_ppo2.py:186][0m |          -0.0109 |           4.0739 |           5.7298 |
[32m[20230113 20:03:50 @agent_ppo2.py:186][0m |          -0.0121 |           3.9580 |           5.7316 |
[32m[20230113 20:03:50 @agent_ppo2.py:186][0m |          -0.0135 |           3.8509 |           5.7327 |
[32m[20230113 20:03:50 @agent_ppo2.py:186][0m |          -0.0132 |           3.7831 |           5.7362 |
[32m[20230113 20:03:50 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:03:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.98
[32m[20230113 20:03:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.71
[32m[20230113 20:03:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.38
[32m[20230113 20:03:51 @agent_ppo2.py:144][0m Total time:      19.30 min
[32m[20230113 20:03:51 @agent_ppo2.py:146][0m 1746944 total steps have happened
[32m[20230113 20:03:51 @agent_ppo2.py:122][0m #------------------------ Iteration 853 --------------------------#
[32m[20230113 20:03:51 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:03:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:51 @agent_ppo2.py:186][0m |          -0.0051 |           6.5124 |           5.6370 |
[32m[20230113 20:03:51 @agent_ppo2.py:186][0m |          -0.0092 |           5.3919 |           5.6425 |
[32m[20230113 20:03:51 @agent_ppo2.py:186][0m |          -0.0093 |           4.9358 |           5.6377 |
[32m[20230113 20:03:51 @agent_ppo2.py:186][0m |          -0.0073 |           4.7031 |           5.6405 |
[32m[20230113 20:03:51 @agent_ppo2.py:186][0m |          -0.0109 |           4.4577 |           5.6367 |
[32m[20230113 20:03:51 @agent_ppo2.py:186][0m |          -0.0110 |           4.3360 |           5.6368 |
[32m[20230113 20:03:51 @agent_ppo2.py:186][0m |          -0.0117 |           4.2049 |           5.6317 |
[32m[20230113 20:03:52 @agent_ppo2.py:186][0m |          -0.0081 |           4.0660 |           5.6357 |
[32m[20230113 20:03:52 @agent_ppo2.py:186][0m |          -0.0113 |           3.9773 |           5.6301 |
[32m[20230113 20:03:52 @agent_ppo2.py:186][0m |          -0.0078 |           3.9953 |           5.6329 |
[32m[20230113 20:03:52 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.36
[32m[20230113 20:03:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.39
[32m[20230113 20:03:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.71
[32m[20230113 20:03:52 @agent_ppo2.py:144][0m Total time:      19.32 min
[32m[20230113 20:03:52 @agent_ppo2.py:146][0m 1748992 total steps have happened
[32m[20230113 20:03:52 @agent_ppo2.py:122][0m #------------------------ Iteration 854 --------------------------#
[32m[20230113 20:03:52 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:03:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:53 @agent_ppo2.py:186][0m |          -0.0001 |           5.9683 |           5.7573 |
[32m[20230113 20:03:53 @agent_ppo2.py:186][0m |          -0.0069 |           4.8091 |           5.7525 |
[32m[20230113 20:03:53 @agent_ppo2.py:186][0m |          -0.0026 |           4.4459 |           5.7553 |
[32m[20230113 20:03:53 @agent_ppo2.py:186][0m |          -0.0077 |           4.1281 |           5.7553 |
[32m[20230113 20:03:53 @agent_ppo2.py:186][0m |          -0.0114 |           3.9168 |           5.7578 |
[32m[20230113 20:03:53 @agent_ppo2.py:186][0m |          -0.0096 |           3.8081 |           5.7588 |
[32m[20230113 20:03:53 @agent_ppo2.py:186][0m |          -0.0126 |           3.6798 |           5.7585 |
[32m[20230113 20:03:53 @agent_ppo2.py:186][0m |          -0.0099 |           3.6761 |           5.7573 |
[32m[20230113 20:03:53 @agent_ppo2.py:186][0m |          -0.0120 |           3.5425 |           5.7590 |
[32m[20230113 20:03:53 @agent_ppo2.py:186][0m |          -0.0142 |           3.4541 |           5.7598 |
[32m[20230113 20:03:53 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.80
[32m[20230113 20:03:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.90
[32m[20230113 20:03:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.75
[32m[20230113 20:03:53 @agent_ppo2.py:144][0m Total time:      19.35 min
[32m[20230113 20:03:53 @agent_ppo2.py:146][0m 1751040 total steps have happened
[32m[20230113 20:03:53 @agent_ppo2.py:122][0m #------------------------ Iteration 855 --------------------------#
[32m[20230113 20:03:54 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:03:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:54 @agent_ppo2.py:186][0m |           0.0007 |          12.4897 |           5.9400 |
[32m[20230113 20:03:54 @agent_ppo2.py:186][0m |          -0.0067 |           6.0373 |           5.9385 |
[32m[20230113 20:03:54 @agent_ppo2.py:186][0m |          -0.0075 |           4.8666 |           5.9409 |
[32m[20230113 20:03:54 @agent_ppo2.py:186][0m |          -0.0107 |           4.3385 |           5.9401 |
[32m[20230113 20:03:54 @agent_ppo2.py:186][0m |          -0.0121 |           4.0836 |           5.9415 |
[32m[20230113 20:03:54 @agent_ppo2.py:186][0m |          -0.0115 |           3.8420 |           5.9414 |
[32m[20230113 20:03:54 @agent_ppo2.py:186][0m |          -0.0121 |           3.6591 |           5.9433 |
[32m[20230113 20:03:54 @agent_ppo2.py:186][0m |          -0.0134 |           3.5412 |           5.9433 |
[32m[20230113 20:03:54 @agent_ppo2.py:186][0m |          -0.0140 |           3.4302 |           5.9440 |
[32m[20230113 20:03:54 @agent_ppo2.py:186][0m |          -0.0140 |           3.3751 |           5.9411 |
[32m[20230113 20:03:54 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 20:03:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 138.40
[32m[20230113 20:03:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.95
[32m[20230113 20:03:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.55
[32m[20230113 20:03:54 @agent_ppo2.py:144][0m Total time:      19.37 min
[32m[20230113 20:03:54 @agent_ppo2.py:146][0m 1753088 total steps have happened
[32m[20230113 20:03:54 @agent_ppo2.py:122][0m #------------------------ Iteration 856 --------------------------#
[32m[20230113 20:03:55 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:03:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:55 @agent_ppo2.py:186][0m |          -0.0028 |           7.9256 |           5.7812 |
[32m[20230113 20:03:55 @agent_ppo2.py:186][0m |          -0.0032 |           6.6115 |           5.7722 |
[32m[20230113 20:03:55 @agent_ppo2.py:186][0m |          -0.0111 |           6.1469 |           5.7642 |
[32m[20230113 20:03:55 @agent_ppo2.py:186][0m |          -0.0089 |           5.7858 |           5.7542 |
[32m[20230113 20:03:55 @agent_ppo2.py:186][0m |          -0.0153 |           5.5242 |           5.7513 |
[32m[20230113 20:03:55 @agent_ppo2.py:186][0m |          -0.0089 |           5.3357 |           5.7498 |
[32m[20230113 20:03:55 @agent_ppo2.py:186][0m |          -0.0133 |           5.2412 |           5.7441 |
[32m[20230113 20:03:55 @agent_ppo2.py:186][0m |          -0.0115 |           5.1941 |           5.7438 |
[32m[20230113 20:03:55 @agent_ppo2.py:186][0m |          -0.0117 |           4.9865 |           5.7388 |
[32m[20230113 20:03:55 @agent_ppo2.py:186][0m |          -0.0154 |           4.8858 |           5.7398 |
[32m[20230113 20:03:55 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.33
[32m[20230113 20:03:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.23
[32m[20230113 20:03:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.12
[32m[20230113 20:03:56 @agent_ppo2.py:144][0m Total time:      19.39 min
[32m[20230113 20:03:56 @agent_ppo2.py:146][0m 1755136 total steps have happened
[32m[20230113 20:03:56 @agent_ppo2.py:122][0m #------------------------ Iteration 857 --------------------------#
[32m[20230113 20:03:56 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:03:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:56 @agent_ppo2.py:186][0m |          -0.0007 |           7.0238 |           5.7808 |
[32m[20230113 20:03:56 @agent_ppo2.py:186][0m |          -0.0045 |           5.4486 |           5.7802 |
[32m[20230113 20:03:56 @agent_ppo2.py:186][0m |          -0.0086 |           4.9293 |           5.7788 |
[32m[20230113 20:03:56 @agent_ppo2.py:186][0m |          -0.0113 |           4.6461 |           5.7767 |
[32m[20230113 20:03:56 @agent_ppo2.py:186][0m |          -0.0080 |           4.5008 |           5.7785 |
[32m[20230113 20:03:57 @agent_ppo2.py:186][0m |          -0.0118 |           4.2449 |           5.7706 |
[32m[20230113 20:03:57 @agent_ppo2.py:186][0m |          -0.0108 |           4.1378 |           5.7715 |
[32m[20230113 20:03:57 @agent_ppo2.py:186][0m |          -0.0123 |           4.0100 |           5.7731 |
[32m[20230113 20:03:57 @agent_ppo2.py:186][0m |          -0.0106 |           3.9005 |           5.7725 |
[32m[20230113 20:03:57 @agent_ppo2.py:186][0m |          -0.0120 |           3.7877 |           5.7723 |
[32m[20230113 20:03:57 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:03:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.70
[32m[20230113 20:03:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.48
[32m[20230113 20:03:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.33
[32m[20230113 20:03:57 @agent_ppo2.py:144][0m Total time:      19.41 min
[32m[20230113 20:03:57 @agent_ppo2.py:146][0m 1757184 total steps have happened
[32m[20230113 20:03:57 @agent_ppo2.py:122][0m #------------------------ Iteration 858 --------------------------#
[32m[20230113 20:03:58 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:03:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:58 @agent_ppo2.py:186][0m |           0.0011 |           6.4308 |           5.6998 |
[32m[20230113 20:03:58 @agent_ppo2.py:186][0m |          -0.0052 |           5.5522 |           5.6964 |
[32m[20230113 20:03:58 @agent_ppo2.py:186][0m |          -0.0061 |           5.1389 |           5.6983 |
[32m[20230113 20:03:58 @agent_ppo2.py:186][0m |          -0.0064 |           4.8760 |           5.6986 |
[32m[20230113 20:03:58 @agent_ppo2.py:186][0m |          -0.0088 |           4.7211 |           5.6942 |
[32m[20230113 20:03:58 @agent_ppo2.py:186][0m |          -0.0077 |           4.6491 |           5.6958 |
[32m[20230113 20:03:58 @agent_ppo2.py:186][0m |          -0.0083 |           4.4393 |           5.6954 |
[32m[20230113 20:03:58 @agent_ppo2.py:186][0m |          -0.0113 |           4.3185 |           5.6899 |
[32m[20230113 20:03:58 @agent_ppo2.py:186][0m |          -0.0090 |           4.2756 |           5.6960 |
[32m[20230113 20:03:58 @agent_ppo2.py:186][0m |          -0.0094 |           4.1518 |           5.6936 |
[32m[20230113 20:03:58 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:03:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.28
[32m[20230113 20:03:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.74
[32m[20230113 20:03:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 150.06
[32m[20230113 20:03:59 @agent_ppo2.py:144][0m Total time:      19.43 min
[32m[20230113 20:03:59 @agent_ppo2.py:146][0m 1759232 total steps have happened
[32m[20230113 20:03:59 @agent_ppo2.py:122][0m #------------------------ Iteration 859 --------------------------#
[32m[20230113 20:03:59 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:03:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:03:59 @agent_ppo2.py:186][0m |          -0.0022 |           7.0761 |           6.0247 |
[32m[20230113 20:03:59 @agent_ppo2.py:186][0m |          -0.0062 |           5.8349 |           6.0180 |
[32m[20230113 20:03:59 @agent_ppo2.py:186][0m |          -0.0082 |           5.2131 |           6.0190 |
[32m[20230113 20:03:59 @agent_ppo2.py:186][0m |          -0.0095 |           4.7904 |           6.0197 |
[32m[20230113 20:03:59 @agent_ppo2.py:186][0m |          -0.0106 |           4.4999 |           6.0192 |
[32m[20230113 20:03:59 @agent_ppo2.py:186][0m |          -0.0102 |           4.2680 |           6.0210 |
[32m[20230113 20:03:59 @agent_ppo2.py:186][0m |          -0.0110 |           4.0969 |           6.0125 |
[32m[20230113 20:03:59 @agent_ppo2.py:186][0m |          -0.0115 |           3.9410 |           6.0204 |
[32m[20230113 20:03:59 @agent_ppo2.py:186][0m |          -0.0127 |           3.8196 |           6.0165 |
[32m[20230113 20:03:59 @agent_ppo2.py:186][0m |          -0.0127 |           3.7324 |           6.0187 |
[32m[20230113 20:03:59 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:04:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.03
[32m[20230113 20:04:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.46
[32m[20230113 20:04:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.26
[32m[20230113 20:04:00 @agent_ppo2.py:144][0m Total time:      19.45 min
[32m[20230113 20:04:00 @agent_ppo2.py:146][0m 1761280 total steps have happened
[32m[20230113 20:04:00 @agent_ppo2.py:122][0m #------------------------ Iteration 860 --------------------------#
[32m[20230113 20:04:00 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 20:04:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:00 @agent_ppo2.py:186][0m |           0.0024 |          18.3518 |           5.8958 |
[32m[20230113 20:04:00 @agent_ppo2.py:186][0m |          -0.0012 |           6.3538 |           5.9021 |
[32m[20230113 20:04:00 @agent_ppo2.py:186][0m |          -0.0079 |           5.0005 |           5.9028 |
[32m[20230113 20:04:00 @agent_ppo2.py:186][0m |          -0.0000 |           4.3953 |           5.9002 |
[32m[20230113 20:04:00 @agent_ppo2.py:186][0m |          -0.0078 |           4.0651 |           5.8940 |
[32m[20230113 20:04:00 @agent_ppo2.py:186][0m |          -0.0105 |           3.7926 |           5.9010 |
[32m[20230113 20:04:01 @agent_ppo2.py:186][0m |          -0.0071 |           3.6308 |           5.8934 |
[32m[20230113 20:04:01 @agent_ppo2.py:186][0m |          -0.0116 |           3.4742 |           5.8959 |
[32m[20230113 20:04:01 @agent_ppo2.py:186][0m |          -0.0020 |           3.4058 |           5.8921 |
[32m[20230113 20:04:01 @agent_ppo2.py:186][0m |          -0.0146 |           3.2962 |           5.8909 |
[32m[20230113 20:04:01 @agent_ppo2.py:131][0m Policy update time: 0.40 s
[32m[20230113 20:04:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 143.73
[32m[20230113 20:04:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.37
[32m[20230113 20:04:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.75
[32m[20230113 20:04:01 @agent_ppo2.py:144][0m Total time:      19.47 min
[32m[20230113 20:04:01 @agent_ppo2.py:146][0m 1763328 total steps have happened
[32m[20230113 20:04:01 @agent_ppo2.py:122][0m #------------------------ Iteration 861 --------------------------#
[32m[20230113 20:04:01 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:02 @agent_ppo2.py:186][0m |          -0.0007 |           6.0796 |           5.7711 |
[32m[20230113 20:04:02 @agent_ppo2.py:186][0m |          -0.0038 |           5.0744 |           5.7628 |
[32m[20230113 20:04:02 @agent_ppo2.py:186][0m |          -0.0050 |           4.7490 |           5.7558 |
[32m[20230113 20:04:02 @agent_ppo2.py:186][0m |          -0.0064 |           4.3698 |           5.7545 |
[32m[20230113 20:04:02 @agent_ppo2.py:186][0m |          -0.0080 |           4.1584 |           5.7524 |
[32m[20230113 20:04:02 @agent_ppo2.py:186][0m |          -0.0083 |           4.0144 |           5.7478 |
[32m[20230113 20:04:02 @agent_ppo2.py:186][0m |          -0.0098 |           3.9094 |           5.7483 |
[32m[20230113 20:04:02 @agent_ppo2.py:186][0m |          -0.0097 |           3.7776 |           5.7468 |
[32m[20230113 20:04:02 @agent_ppo2.py:186][0m |          -0.0106 |           3.6618 |           5.7399 |
[32m[20230113 20:04:02 @agent_ppo2.py:186][0m |          -0.0090 |           3.5878 |           5.7397 |
[32m[20230113 20:04:02 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.45
[32m[20230113 20:04:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.87
[32m[20230113 20:04:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.96
[32m[20230113 20:04:02 @agent_ppo2.py:144][0m Total time:      19.50 min
[32m[20230113 20:04:02 @agent_ppo2.py:146][0m 1765376 total steps have happened
[32m[20230113 20:04:02 @agent_ppo2.py:122][0m #------------------------ Iteration 862 --------------------------#
[32m[20230113 20:04:03 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:03 @agent_ppo2.py:186][0m |           0.0036 |          11.0799 |           5.7855 |
[32m[20230113 20:04:03 @agent_ppo2.py:186][0m |           0.0040 |           6.2133 |           5.7946 |
[32m[20230113 20:04:03 @agent_ppo2.py:186][0m |          -0.0123 |           4.9930 |           5.7922 |
[32m[20230113 20:04:03 @agent_ppo2.py:186][0m |          -0.0094 |           4.4242 |           5.7914 |
[32m[20230113 20:04:03 @agent_ppo2.py:186][0m |          -0.0114 |           4.0166 |           5.7936 |
[32m[20230113 20:04:03 @agent_ppo2.py:186][0m |           0.0248 |           3.8845 |           5.7930 |
[32m[20230113 20:04:03 @agent_ppo2.py:186][0m |           0.0180 |           3.8288 |           5.7727 |
[32m[20230113 20:04:03 @agent_ppo2.py:186][0m |          -0.0098 |           3.4322 |           5.7767 |
[32m[20230113 20:04:03 @agent_ppo2.py:186][0m |           0.0085 |           3.3610 |           5.7820 |
[32m[20230113 20:04:03 @agent_ppo2.py:186][0m |          -0.0075 |           3.4368 |           5.7873 |
[32m[20230113 20:04:03 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:04:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 158.06
[32m[20230113 20:04:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.33
[32m[20230113 20:04:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.68
[32m[20230113 20:04:04 @agent_ppo2.py:144][0m Total time:      19.52 min
[32m[20230113 20:04:04 @agent_ppo2.py:146][0m 1767424 total steps have happened
[32m[20230113 20:04:04 @agent_ppo2.py:122][0m #------------------------ Iteration 863 --------------------------#
[32m[20230113 20:04:04 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:04 @agent_ppo2.py:186][0m |           0.0003 |           5.7625 |           5.7727 |
[32m[20230113 20:04:04 @agent_ppo2.py:186][0m |          -0.0062 |           4.6613 |           5.7716 |
[32m[20230113 20:04:04 @agent_ppo2.py:186][0m |          -0.0066 |           4.2152 |           5.7656 |
[32m[20230113 20:04:04 @agent_ppo2.py:186][0m |          -0.0104 |           3.9450 |           5.7554 |
[32m[20230113 20:04:04 @agent_ppo2.py:186][0m |          -0.0125 |           3.7059 |           5.7542 |
[32m[20230113 20:04:04 @agent_ppo2.py:186][0m |          -0.0137 |           3.5698 |           5.7502 |
[32m[20230113 20:04:04 @agent_ppo2.py:186][0m |          -0.0130 |           3.3910 |           5.7456 |
[32m[20230113 20:04:04 @agent_ppo2.py:186][0m |          -0.0136 |           3.2721 |           5.7423 |
[32m[20230113 20:04:05 @agent_ppo2.py:186][0m |          -0.0141 |           3.1594 |           5.7421 |
[32m[20230113 20:04:05 @agent_ppo2.py:186][0m |          -0.0133 |           3.0769 |           5.7398 |
[32m[20230113 20:04:05 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.97
[32m[20230113 20:04:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.92
[32m[20230113 20:04:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.02
[32m[20230113 20:04:05 @agent_ppo2.py:144][0m Total time:      19.54 min
[32m[20230113 20:04:05 @agent_ppo2.py:146][0m 1769472 total steps have happened
[32m[20230113 20:04:05 @agent_ppo2.py:122][0m #------------------------ Iteration 864 --------------------------#
[32m[20230113 20:04:05 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:06 @agent_ppo2.py:186][0m |          -0.0006 |           5.9037 |           5.7520 |
[32m[20230113 20:04:06 @agent_ppo2.py:186][0m |          -0.0082 |           4.4112 |           5.7394 |
[32m[20230113 20:04:06 @agent_ppo2.py:186][0m |          -0.0109 |           3.9761 |           5.7460 |
[32m[20230113 20:04:06 @agent_ppo2.py:186][0m |          -0.0111 |           3.7022 |           5.7442 |
[32m[20230113 20:04:06 @agent_ppo2.py:186][0m |          -0.0139 |           3.5873 |           5.7502 |
[32m[20230113 20:04:06 @agent_ppo2.py:186][0m |          -0.0125 |           3.3422 |           5.7525 |
[32m[20230113 20:04:06 @agent_ppo2.py:186][0m |          -0.0131 |           3.1753 |           5.7541 |
[32m[20230113 20:04:06 @agent_ppo2.py:186][0m |          -0.0113 |           3.0771 |           5.7568 |
[32m[20230113 20:04:06 @agent_ppo2.py:186][0m |          -0.0146 |           2.9574 |           5.7547 |
[32m[20230113 20:04:06 @agent_ppo2.py:186][0m |          -0.0149 |           2.8872 |           5.7598 |
[32m[20230113 20:04:06 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:04:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.65
[32m[20230113 20:04:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.88
[32m[20230113 20:04:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.16
[32m[20230113 20:04:06 @agent_ppo2.py:144][0m Total time:      19.56 min
[32m[20230113 20:04:06 @agent_ppo2.py:146][0m 1771520 total steps have happened
[32m[20230113 20:04:06 @agent_ppo2.py:122][0m #------------------------ Iteration 865 --------------------------#
[32m[20230113 20:04:07 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:07 @agent_ppo2.py:186][0m |           0.0040 |           6.4519 |           5.7026 |
[32m[20230113 20:04:07 @agent_ppo2.py:186][0m |          -0.0054 |           5.1771 |           5.6965 |
[32m[20230113 20:04:07 @agent_ppo2.py:186][0m |          -0.0072 |           4.6636 |           5.6911 |
[32m[20230113 20:04:07 @agent_ppo2.py:186][0m |          -0.0060 |           4.1922 |           5.6826 |
[32m[20230113 20:04:07 @agent_ppo2.py:186][0m |          -0.0117 |           3.9692 |           5.6812 |
[32m[20230113 20:04:07 @agent_ppo2.py:186][0m |          -0.0084 |           3.7473 |           5.6789 |
[32m[20230113 20:04:07 @agent_ppo2.py:186][0m |          -0.0064 |           3.6292 |           5.6887 |
[32m[20230113 20:04:07 @agent_ppo2.py:186][0m |          -0.0091 |           3.4855 |           5.6846 |
[32m[20230113 20:04:07 @agent_ppo2.py:186][0m |          -0.0160 |           3.4034 |           5.6865 |
[32m[20230113 20:04:07 @agent_ppo2.py:186][0m |          -0.0089 |           3.2298 |           5.6877 |
[32m[20230113 20:04:07 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:04:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.42
[32m[20230113 20:04:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.52
[32m[20230113 20:04:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.53
[32m[20230113 20:04:08 @agent_ppo2.py:144][0m Total time:      19.58 min
[32m[20230113 20:04:08 @agent_ppo2.py:146][0m 1773568 total steps have happened
[32m[20230113 20:04:08 @agent_ppo2.py:122][0m #------------------------ Iteration 866 --------------------------#
[32m[20230113 20:04:08 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:08 @agent_ppo2.py:186][0m |          -0.0010 |           5.9994 |           5.9393 |
[32m[20230113 20:04:08 @agent_ppo2.py:186][0m |          -0.0025 |           5.2859 |           5.9321 |
[32m[20230113 20:04:08 @agent_ppo2.py:186][0m |          -0.0065 |           4.8187 |           5.9248 |
[32m[20230113 20:04:08 @agent_ppo2.py:186][0m |          -0.0073 |           4.6166 |           5.9240 |
[32m[20230113 20:04:08 @agent_ppo2.py:186][0m |          -0.0082 |           4.4917 |           5.9197 |
[32m[20230113 20:04:08 @agent_ppo2.py:186][0m |          -0.0080 |           4.3332 |           5.9110 |
[32m[20230113 20:04:08 @agent_ppo2.py:186][0m |          -0.0134 |           4.2243 |           5.9155 |
[32m[20230113 20:04:08 @agent_ppo2.py:186][0m |          -0.0097 |           4.0731 |           5.9119 |
[32m[20230113 20:04:08 @agent_ppo2.py:186][0m |          -0.0144 |           3.9662 |           5.9080 |
[32m[20230113 20:04:09 @agent_ppo2.py:186][0m |          -0.0128 |           3.8951 |           5.9070 |
[32m[20230113 20:04:09 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:04:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.62
[32m[20230113 20:04:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.79
[32m[20230113 20:04:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.60
[32m[20230113 20:04:09 @agent_ppo2.py:144][0m Total time:      19.61 min
[32m[20230113 20:04:09 @agent_ppo2.py:146][0m 1775616 total steps have happened
[32m[20230113 20:04:09 @agent_ppo2.py:122][0m #------------------------ Iteration 867 --------------------------#
[32m[20230113 20:04:09 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:09 @agent_ppo2.py:186][0m |          -0.0032 |           5.0022 |           5.7878 |
[32m[20230113 20:04:10 @agent_ppo2.py:186][0m |          -0.0099 |           4.4281 |           5.7854 |
[32m[20230113 20:04:10 @agent_ppo2.py:186][0m |          -0.0063 |           4.2150 |           5.7782 |
[32m[20230113 20:04:10 @agent_ppo2.py:186][0m |          -0.0072 |           4.0966 |           5.7816 |
[32m[20230113 20:04:10 @agent_ppo2.py:186][0m |          -0.0128 |           3.8554 |           5.7869 |
[32m[20230113 20:04:10 @agent_ppo2.py:186][0m |          -0.0138 |           3.7484 |           5.7885 |
[32m[20230113 20:04:10 @agent_ppo2.py:186][0m |          -0.0160 |           3.6424 |           5.7883 |
[32m[20230113 20:04:10 @agent_ppo2.py:186][0m |          -0.0083 |           3.5632 |           5.7911 |
[32m[20230113 20:04:10 @agent_ppo2.py:186][0m |          -0.0112 |           3.5301 |           5.7931 |
[32m[20230113 20:04:10 @agent_ppo2.py:186][0m |          -0.0133 |           3.4395 |           5.7943 |
[32m[20230113 20:04:10 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:04:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.91
[32m[20230113 20:04:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.16
[32m[20230113 20:04:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.97
[32m[20230113 20:04:10 @agent_ppo2.py:144][0m Total time:      19.63 min
[32m[20230113 20:04:10 @agent_ppo2.py:146][0m 1777664 total steps have happened
[32m[20230113 20:04:10 @agent_ppo2.py:122][0m #------------------------ Iteration 868 --------------------------#
[32m[20230113 20:04:11 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:11 @agent_ppo2.py:186][0m |           0.0006 |           5.3558 |           5.9017 |
[32m[20230113 20:04:11 @agent_ppo2.py:186][0m |          -0.0028 |           4.7977 |           5.8965 |
[32m[20230113 20:04:11 @agent_ppo2.py:186][0m |          -0.0051 |           4.4907 |           5.8860 |
[32m[20230113 20:04:11 @agent_ppo2.py:186][0m |          -0.0070 |           4.2647 |           5.8885 |
[32m[20230113 20:04:11 @agent_ppo2.py:186][0m |          -0.0073 |           4.1298 |           5.8814 |
[32m[20230113 20:04:11 @agent_ppo2.py:186][0m |          -0.0082 |           4.0316 |           5.8826 |
[32m[20230113 20:04:11 @agent_ppo2.py:186][0m |          -0.0081 |           3.8977 |           5.8795 |
[32m[20230113 20:04:11 @agent_ppo2.py:186][0m |          -0.0093 |           3.7695 |           5.8776 |
[32m[20230113 20:04:11 @agent_ppo2.py:186][0m |          -0.0098 |           3.7107 |           5.8774 |
[32m[20230113 20:04:11 @agent_ppo2.py:186][0m |          -0.0099 |           3.6248 |           5.8777 |
[32m[20230113 20:04:11 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:04:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.13
[32m[20230113 20:04:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.02
[32m[20230113 20:04:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 159.76
[32m[20230113 20:04:11 @agent_ppo2.py:144][0m Total time:      19.65 min
[32m[20230113 20:04:11 @agent_ppo2.py:146][0m 1779712 total steps have happened
[32m[20230113 20:04:11 @agent_ppo2.py:122][0m #------------------------ Iteration 869 --------------------------#
[32m[20230113 20:04:12 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:12 @agent_ppo2.py:186][0m |           0.0038 |           5.0203 |           5.8245 |
[32m[20230113 20:04:12 @agent_ppo2.py:186][0m |          -0.0040 |           4.5946 |           5.8250 |
[32m[20230113 20:04:12 @agent_ppo2.py:186][0m |          -0.0045 |           4.2748 |           5.8220 |
[32m[20230113 20:04:12 @agent_ppo2.py:186][0m |          -0.0053 |           4.0843 |           5.8158 |
[32m[20230113 20:04:12 @agent_ppo2.py:186][0m |          -0.0066 |           3.9122 |           5.8122 |
[32m[20230113 20:04:12 @agent_ppo2.py:186][0m |          -0.0091 |           3.8184 |           5.8063 |
[32m[20230113 20:04:12 @agent_ppo2.py:186][0m |          -0.0088 |           3.7539 |           5.8096 |
[32m[20230113 20:04:12 @agent_ppo2.py:186][0m |          -0.0109 |           3.6038 |           5.8094 |
[32m[20230113 20:04:12 @agent_ppo2.py:186][0m |          -0.0125 |           3.5248 |           5.8008 |
[32m[20230113 20:04:12 @agent_ppo2.py:186][0m |          -0.0125 |           3.4571 |           5.8051 |
[32m[20230113 20:04:12 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:04:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.69
[32m[20230113 20:04:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.47
[32m[20230113 20:04:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.91
[32m[20230113 20:04:13 @agent_ppo2.py:144][0m Total time:      19.67 min
[32m[20230113 20:04:13 @agent_ppo2.py:146][0m 1781760 total steps have happened
[32m[20230113 20:04:13 @agent_ppo2.py:122][0m #------------------------ Iteration 870 --------------------------#
[32m[20230113 20:04:13 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:04:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:13 @agent_ppo2.py:186][0m |           0.0004 |          12.5528 |           5.7905 |
[32m[20230113 20:04:13 @agent_ppo2.py:186][0m |          -0.0090 |           6.0846 |           5.7853 |
[32m[20230113 20:04:13 @agent_ppo2.py:186][0m |          -0.0094 |           5.0803 |           5.7884 |
[32m[20230113 20:04:13 @agent_ppo2.py:186][0m |          -0.0126 |           4.5662 |           5.7840 |
[32m[20230113 20:04:14 @agent_ppo2.py:186][0m |          -0.0113 |           4.3828 |           5.7802 |
[32m[20230113 20:04:14 @agent_ppo2.py:186][0m |          -0.0143 |           4.1368 |           5.7797 |
[32m[20230113 20:04:14 @agent_ppo2.py:186][0m |          -0.0150 |           3.9577 |           5.7795 |
[32m[20230113 20:04:14 @agent_ppo2.py:186][0m |          -0.0163 |           3.7038 |           5.7767 |
[32m[20230113 20:04:14 @agent_ppo2.py:186][0m |          -0.0172 |           3.6157 |           5.7766 |
[32m[20230113 20:04:14 @agent_ppo2.py:186][0m |          -0.0174 |           3.5706 |           5.7761 |
[32m[20230113 20:04:14 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 20:04:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 126.46
[32m[20230113 20:04:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.89
[32m[20230113 20:04:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.21
[32m[20230113 20:04:14 @agent_ppo2.py:144][0m Total time:      19.69 min
[32m[20230113 20:04:14 @agent_ppo2.py:146][0m 1783808 total steps have happened
[32m[20230113 20:04:14 @agent_ppo2.py:122][0m #------------------------ Iteration 871 --------------------------#
[32m[20230113 20:04:15 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:15 @agent_ppo2.py:186][0m |           0.0028 |           6.7416 |           5.8053 |
[32m[20230113 20:04:15 @agent_ppo2.py:186][0m |          -0.0024 |           5.0997 |           5.8026 |
[32m[20230113 20:04:15 @agent_ppo2.py:186][0m |          -0.0035 |           4.6148 |           5.7979 |
[32m[20230113 20:04:15 @agent_ppo2.py:186][0m |          -0.0072 |           4.2852 |           5.7968 |
[32m[20230113 20:04:15 @agent_ppo2.py:186][0m |          -0.0089 |           4.0813 |           5.7945 |
[32m[20230113 20:04:15 @agent_ppo2.py:186][0m |          -0.0105 |           3.9473 |           5.7919 |
[32m[20230113 20:04:15 @agent_ppo2.py:186][0m |          -0.0081 |           3.8132 |           5.7869 |
[32m[20230113 20:04:15 @agent_ppo2.py:186][0m |          -0.0101 |           3.7175 |           5.7870 |
[32m[20230113 20:04:15 @agent_ppo2.py:186][0m |          -0.0109 |           3.6523 |           5.7871 |
[32m[20230113 20:04:15 @agent_ppo2.py:186][0m |          -0.0117 |           3.5742 |           5.7832 |
[32m[20230113 20:04:15 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.80
[32m[20230113 20:04:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.77
[32m[20230113 20:04:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.97
[32m[20230113 20:04:15 @agent_ppo2.py:144][0m Total time:      19.72 min
[32m[20230113 20:04:15 @agent_ppo2.py:146][0m 1785856 total steps have happened
[32m[20230113 20:04:15 @agent_ppo2.py:122][0m #------------------------ Iteration 872 --------------------------#
[32m[20230113 20:04:16 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:04:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:16 @agent_ppo2.py:186][0m |           0.0005 |           6.2415 |           5.8214 |
[32m[20230113 20:04:16 @agent_ppo2.py:186][0m |          -0.0048 |           5.1780 |           5.8203 |
[32m[20230113 20:04:16 @agent_ppo2.py:186][0m |          -0.0080 |           4.7436 |           5.8179 |
[32m[20230113 20:04:16 @agent_ppo2.py:186][0m |          -0.0075 |           4.4684 |           5.8127 |
[32m[20230113 20:04:16 @agent_ppo2.py:186][0m |          -0.0065 |           4.3034 |           5.8103 |
[32m[20230113 20:04:16 @agent_ppo2.py:186][0m |          -0.0081 |           4.0961 |           5.8124 |
[32m[20230113 20:04:16 @agent_ppo2.py:186][0m |          -0.0060 |           4.0886 |           5.8018 |
[32m[20230113 20:04:16 @agent_ppo2.py:186][0m |          -0.0111 |           3.8977 |           5.8044 |
[32m[20230113 20:04:16 @agent_ppo2.py:186][0m |          -0.0130 |           3.8066 |           5.8026 |
[32m[20230113 20:04:16 @agent_ppo2.py:186][0m |          -0.0130 |           3.7545 |           5.8030 |
[32m[20230113 20:04:16 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:04:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.23
[32m[20230113 20:04:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.40
[32m[20230113 20:04:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.03
[32m[20230113 20:04:17 @agent_ppo2.py:144][0m Total time:      19.74 min
[32m[20230113 20:04:17 @agent_ppo2.py:146][0m 1787904 total steps have happened
[32m[20230113 20:04:17 @agent_ppo2.py:122][0m #------------------------ Iteration 873 --------------------------#
[32m[20230113 20:04:17 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:04:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:17 @agent_ppo2.py:186][0m |          -0.0013 |           6.0726 |           5.8400 |
[32m[20230113 20:04:17 @agent_ppo2.py:186][0m |          -0.0038 |           4.6948 |           5.8255 |
[32m[20230113 20:04:17 @agent_ppo2.py:186][0m |          -0.0060 |           4.2926 |           5.8329 |
[32m[20230113 20:04:18 @agent_ppo2.py:186][0m |          -0.0079 |           4.1431 |           5.8302 |
[32m[20230113 20:04:18 @agent_ppo2.py:186][0m |          -0.0082 |           3.9452 |           5.8249 |
[32m[20230113 20:04:18 @agent_ppo2.py:186][0m |          -0.0104 |           3.9254 |           5.8310 |
[32m[20230113 20:04:18 @agent_ppo2.py:186][0m |          -0.0132 |           3.7243 |           5.8316 |
[32m[20230113 20:04:18 @agent_ppo2.py:186][0m |          -0.0128 |           3.6671 |           5.8326 |
[32m[20230113 20:04:18 @agent_ppo2.py:186][0m |          -0.0151 |           3.5837 |           5.8352 |
[32m[20230113 20:04:18 @agent_ppo2.py:186][0m |          -0.0142 |           3.5965 |           5.8337 |
[32m[20230113 20:04:18 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:04:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.03
[32m[20230113 20:04:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 221.78
[32m[20230113 20:04:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 97.80
[32m[20230113 20:04:18 @agent_ppo2.py:144][0m Total time:      19.76 min
[32m[20230113 20:04:18 @agent_ppo2.py:146][0m 1789952 total steps have happened
[32m[20230113 20:04:18 @agent_ppo2.py:122][0m #------------------------ Iteration 874 --------------------------#
[32m[20230113 20:04:19 @agent_ppo2.py:128][0m Sampling time: 0.31 s by 1 slaves
[32m[20230113 20:04:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:19 @agent_ppo2.py:186][0m |          -0.0042 |          14.2069 |           5.7638 |
[32m[20230113 20:04:19 @agent_ppo2.py:186][0m |          -0.0106 |           8.8172 |           5.7581 |
[32m[20230113 20:04:19 @agent_ppo2.py:186][0m |          -0.0184 |           5.9707 |           5.7566 |
[32m[20230113 20:04:19 @agent_ppo2.py:186][0m |          -0.0187 |           5.0698 |           5.7568 |
[32m[20230113 20:04:19 @agent_ppo2.py:186][0m |          -0.0177 |           4.6117 |           5.7510 |
[32m[20230113 20:04:19 @agent_ppo2.py:186][0m |          -0.0247 |           4.2594 |           5.7503 |
[32m[20230113 20:04:19 @agent_ppo2.py:186][0m |          -0.0169 |           4.0899 |           5.7547 |
[32m[20230113 20:04:19 @agent_ppo2.py:186][0m |          -0.0228 |           3.8410 |           5.7536 |
[32m[20230113 20:04:19 @agent_ppo2.py:186][0m |          -0.0246 |           3.6174 |           5.7493 |
[32m[20230113 20:04:19 @agent_ppo2.py:186][0m |          -0.0257 |           3.4579 |           5.7463 |
[32m[20230113 20:04:19 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 20:04:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 94.78
[32m[20230113 20:04:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.98
[32m[20230113 20:04:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.43
[32m[20230113 20:04:19 @agent_ppo2.py:144][0m Total time:      19.78 min
[32m[20230113 20:04:19 @agent_ppo2.py:146][0m 1792000 total steps have happened
[32m[20230113 20:04:19 @agent_ppo2.py:122][0m #------------------------ Iteration 875 --------------------------#
[32m[20230113 20:04:20 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:04:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:20 @agent_ppo2.py:186][0m |           0.0051 |           5.8503 |           5.7752 |
[32m[20230113 20:04:20 @agent_ppo2.py:186][0m |           0.0056 |           4.6160 |           5.7736 |
[32m[20230113 20:04:20 @agent_ppo2.py:186][0m |          -0.0123 |           4.1582 |           5.7765 |
[32m[20230113 20:04:20 @agent_ppo2.py:186][0m |          -0.0069 |           4.0736 |           5.7776 |
[32m[20230113 20:04:20 @agent_ppo2.py:186][0m |          -0.0025 |           3.8725 |           5.7724 |
[32m[20230113 20:04:20 @agent_ppo2.py:186][0m |          -0.0082 |           3.6402 |           5.7684 |
[32m[20230113 20:04:20 @agent_ppo2.py:186][0m |          -0.0079 |           3.5524 |           5.7745 |
[32m[20230113 20:04:20 @agent_ppo2.py:186][0m |          -0.0078 |           3.4451 |           5.7774 |
[32m[20230113 20:04:20 @agent_ppo2.py:186][0m |          -0.0132 |           3.3577 |           5.7736 |
[32m[20230113 20:04:20 @agent_ppo2.py:186][0m |          -0.0071 |           3.3197 |           5.7714 |
[32m[20230113 20:04:20 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:04:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.78
[32m[20230113 20:04:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.76
[32m[20230113 20:04:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 151.90
[32m[20230113 20:04:21 @agent_ppo2.py:144][0m Total time:      19.80 min
[32m[20230113 20:04:21 @agent_ppo2.py:146][0m 1794048 total steps have happened
[32m[20230113 20:04:21 @agent_ppo2.py:122][0m #------------------------ Iteration 876 --------------------------#
[32m[20230113 20:04:21 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:21 @agent_ppo2.py:186][0m |          -0.0007 |           5.5633 |           5.8548 |
[32m[20230113 20:04:21 @agent_ppo2.py:186][0m |          -0.0046 |           4.5311 |           5.8527 |
[32m[20230113 20:04:21 @agent_ppo2.py:186][0m |          -0.0078 |           4.0778 |           5.8485 |
[32m[20230113 20:04:21 @agent_ppo2.py:186][0m |          -0.0095 |           3.8788 |           5.8485 |
[32m[20230113 20:04:21 @agent_ppo2.py:186][0m |          -0.0102 |           3.7026 |           5.8460 |
[32m[20230113 20:04:21 @agent_ppo2.py:186][0m |          -0.0122 |           3.5577 |           5.8495 |
[32m[20230113 20:04:21 @agent_ppo2.py:186][0m |          -0.0121 |           3.4324 |           5.8499 |
[32m[20230113 20:04:21 @agent_ppo2.py:186][0m |          -0.0122 |           3.3544 |           5.8503 |
[32m[20230113 20:04:21 @agent_ppo2.py:186][0m |          -0.0141 |           3.2442 |           5.8532 |
[32m[20230113 20:04:22 @agent_ppo2.py:186][0m |          -0.0148 |           3.1451 |           5.8569 |
[32m[20230113 20:04:22 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.28
[32m[20230113 20:04:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.50
[32m[20230113 20:04:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.28
[32m[20230113 20:04:22 @agent_ppo2.py:144][0m Total time:      19.82 min
[32m[20230113 20:04:22 @agent_ppo2.py:146][0m 1796096 total steps have happened
[32m[20230113 20:04:22 @agent_ppo2.py:122][0m #------------------------ Iteration 877 --------------------------#
[32m[20230113 20:04:22 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:22 @agent_ppo2.py:186][0m |           0.0004 |           5.7892 |           5.8955 |
[32m[20230113 20:04:23 @agent_ppo2.py:186][0m |          -0.0038 |           4.4104 |           5.8841 |
[32m[20230113 20:04:23 @agent_ppo2.py:186][0m |          -0.0057 |           3.9972 |           5.8776 |
[32m[20230113 20:04:23 @agent_ppo2.py:186][0m |          -0.0079 |           3.7017 |           5.8767 |
[32m[20230113 20:04:23 @agent_ppo2.py:186][0m |          -0.0078 |           3.4968 |           5.8774 |
[32m[20230113 20:04:23 @agent_ppo2.py:186][0m |          -0.0098 |           3.3716 |           5.8794 |
[32m[20230113 20:04:23 @agent_ppo2.py:186][0m |          -0.0102 |           3.2688 |           5.8773 |
[32m[20230113 20:04:23 @agent_ppo2.py:186][0m |          -0.0103 |           3.1422 |           5.8767 |
[32m[20230113 20:04:23 @agent_ppo2.py:186][0m |          -0.0111 |           3.0851 |           5.8721 |
[32m[20230113 20:04:23 @agent_ppo2.py:186][0m |          -0.0110 |           3.0131 |           5.8765 |
[32m[20230113 20:04:23 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.24
[32m[20230113 20:04:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.28
[32m[20230113 20:04:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.35
[32m[20230113 20:04:23 @agent_ppo2.py:144][0m Total time:      19.84 min
[32m[20230113 20:04:23 @agent_ppo2.py:146][0m 1798144 total steps have happened
[32m[20230113 20:04:23 @agent_ppo2.py:122][0m #------------------------ Iteration 878 --------------------------#
[32m[20230113 20:04:24 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:24 @agent_ppo2.py:186][0m |           0.0068 |           5.3594 |           5.8574 |
[32m[20230113 20:04:24 @agent_ppo2.py:186][0m |          -0.0020 |           4.3934 |           5.8441 |
[32m[20230113 20:04:24 @agent_ppo2.py:186][0m |          -0.0044 |           3.9452 |           5.8442 |
[32m[20230113 20:04:24 @agent_ppo2.py:186][0m |          -0.0038 |           3.7015 |           5.8364 |
[32m[20230113 20:04:24 @agent_ppo2.py:186][0m |          -0.0064 |           3.5052 |           5.8422 |
[32m[20230113 20:04:24 @agent_ppo2.py:186][0m |          -0.0101 |           3.3489 |           5.8334 |
[32m[20230113 20:04:24 @agent_ppo2.py:186][0m |          -0.0077 |           3.2806 |           5.8314 |
[32m[20230113 20:04:24 @agent_ppo2.py:186][0m |          -0.0107 |           3.1302 |           5.8314 |
[32m[20230113 20:04:24 @agent_ppo2.py:186][0m |          -0.0133 |           3.1394 |           5.8308 |
[32m[20230113 20:04:24 @agent_ppo2.py:186][0m |          -0.0106 |           2.9432 |           5.8293 |
[32m[20230113 20:04:24 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:04:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.61
[32m[20230113 20:04:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.47
[32m[20230113 20:04:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.93
[32m[20230113 20:04:25 @agent_ppo2.py:144][0m Total time:      19.87 min
[32m[20230113 20:04:25 @agent_ppo2.py:146][0m 1800192 total steps have happened
[32m[20230113 20:04:25 @agent_ppo2.py:122][0m #------------------------ Iteration 879 --------------------------#
[32m[20230113 20:04:25 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:25 @agent_ppo2.py:186][0m |           0.0049 |           6.0726 |           5.6124 |
[32m[20230113 20:04:25 @agent_ppo2.py:186][0m |           0.0014 |           4.9959 |           5.6088 |
[32m[20230113 20:04:25 @agent_ppo2.py:186][0m |          -0.0043 |           4.4542 |           5.6067 |
[32m[20230113 20:04:25 @agent_ppo2.py:186][0m |          -0.0066 |           4.2697 |           5.6072 |
[32m[20230113 20:04:25 @agent_ppo2.py:186][0m |          -0.0019 |           4.0560 |           5.6014 |
[32m[20230113 20:04:25 @agent_ppo2.py:186][0m |          -0.0126 |           3.9444 |           5.6038 |
[32m[20230113 20:04:25 @agent_ppo2.py:186][0m |          -0.0118 |           3.8518 |           5.6050 |
[32m[20230113 20:04:25 @agent_ppo2.py:186][0m |          -0.0118 |           3.7460 |           5.6055 |
[32m[20230113 20:04:25 @agent_ppo2.py:186][0m |          -0.0041 |           3.8969 |           5.6070 |
[32m[20230113 20:04:25 @agent_ppo2.py:186][0m |          -0.0120 |           3.6022 |           5.6062 |
[32m[20230113 20:04:25 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.52
[32m[20230113 20:04:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.88
[32m[20230113 20:04:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.29
[32m[20230113 20:04:26 @agent_ppo2.py:144][0m Total time:      19.89 min
[32m[20230113 20:04:26 @agent_ppo2.py:146][0m 1802240 total steps have happened
[32m[20230113 20:04:26 @agent_ppo2.py:122][0m #------------------------ Iteration 880 --------------------------#
[32m[20230113 20:04:26 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 20:04:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:26 @agent_ppo2.py:186][0m |           0.0040 |          23.1596 |           5.8881 |
[32m[20230113 20:04:26 @agent_ppo2.py:186][0m |          -0.0031 |          11.7792 |           5.8748 |
[32m[20230113 20:04:26 @agent_ppo2.py:186][0m |          -0.0118 |           9.8716 |           5.8692 |
[32m[20230113 20:04:26 @agent_ppo2.py:186][0m |          -0.0109 |           8.7619 |           5.8648 |
[32m[20230113 20:04:26 @agent_ppo2.py:186][0m |          -0.0111 |           8.0996 |           5.8584 |
[32m[20230113 20:04:26 @agent_ppo2.py:186][0m |          -0.0146 |           7.4887 |           5.8548 |
[32m[20230113 20:04:26 @agent_ppo2.py:186][0m |          -0.0161 |           7.0360 |           5.8542 |
[32m[20230113 20:04:26 @agent_ppo2.py:186][0m |          -0.0152 |           6.6305 |           5.8538 |
[32m[20230113 20:04:27 @agent_ppo2.py:186][0m |          -0.0175 |           6.2086 |           5.8477 |
[32m[20230113 20:04:27 @agent_ppo2.py:186][0m |          -0.0168 |           5.9344 |           5.8484 |
[32m[20230113 20:04:27 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:04:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 111.69
[32m[20230113 20:04:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.73
[32m[20230113 20:04:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 91.63
[32m[20230113 20:04:27 @agent_ppo2.py:144][0m Total time:      19.91 min
[32m[20230113 20:04:27 @agent_ppo2.py:146][0m 1804288 total steps have happened
[32m[20230113 20:04:27 @agent_ppo2.py:122][0m #------------------------ Iteration 881 --------------------------#
[32m[20230113 20:04:27 @agent_ppo2.py:128][0m Sampling time: 0.31 s by 1 slaves
[32m[20230113 20:04:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:27 @agent_ppo2.py:186][0m |          -0.0006 |          16.1364 |           5.7569 |
[32m[20230113 20:04:27 @agent_ppo2.py:186][0m |          -0.0098 |           8.1119 |           5.7346 |
[32m[20230113 20:04:27 @agent_ppo2.py:186][0m |          -0.0092 |           6.2977 |           5.7327 |
[32m[20230113 20:04:27 @agent_ppo2.py:186][0m |          -0.0023 |           5.9882 |           5.7368 |
[32m[20230113 20:04:27 @agent_ppo2.py:186][0m |          -0.0091 |           5.1454 |           5.7334 |
[32m[20230113 20:04:27 @agent_ppo2.py:186][0m |          -0.0096 |           4.9795 |           5.7330 |
[32m[20230113 20:04:27 @agent_ppo2.py:186][0m |          -0.0135 |           4.5059 |           5.7267 |
[32m[20230113 20:04:27 @agent_ppo2.py:186][0m |          -0.0035 |           4.4978 |           5.7324 |
[32m[20230113 20:04:27 @agent_ppo2.py:186][0m |          -0.0091 |           4.1942 |           5.7361 |
[32m[20230113 20:04:27 @agent_ppo2.py:186][0m |          -0.0142 |           3.9495 |           5.7308 |
[32m[20230113 20:04:27 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 20:04:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 98.98
[32m[20230113 20:04:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.84
[32m[20230113 20:04:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.44
[32m[20230113 20:04:28 @agent_ppo2.py:144][0m Total time:      19.92 min
[32m[20230113 20:04:28 @agent_ppo2.py:146][0m 1806336 total steps have happened
[32m[20230113 20:04:28 @agent_ppo2.py:122][0m #------------------------ Iteration 882 --------------------------#
[32m[20230113 20:04:28 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:04:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:28 @agent_ppo2.py:186][0m |          -0.0006 |           7.4059 |           5.8782 |
[32m[20230113 20:04:28 @agent_ppo2.py:186][0m |          -0.0026 |           6.1805 |           5.8666 |
[32m[20230113 20:04:29 @agent_ppo2.py:186][0m |          -0.0063 |           5.5563 |           5.8631 |
[32m[20230113 20:04:29 @agent_ppo2.py:186][0m |          -0.0049 |           5.1978 |           5.8626 |
[32m[20230113 20:04:29 @agent_ppo2.py:186][0m |          -0.0084 |           4.8689 |           5.8601 |
[32m[20230113 20:04:29 @agent_ppo2.py:186][0m |          -0.0048 |           4.8229 |           5.8554 |
[32m[20230113 20:04:29 @agent_ppo2.py:186][0m |          -0.0084 |           4.5396 |           5.8537 |
[32m[20230113 20:04:29 @agent_ppo2.py:186][0m |          -0.0074 |           4.4052 |           5.8502 |
[32m[20230113 20:04:29 @agent_ppo2.py:186][0m |          -0.0107 |           4.3226 |           5.8529 |
[32m[20230113 20:04:29 @agent_ppo2.py:186][0m |          -0.0089 |           4.2095 |           5.8446 |
[32m[20230113 20:04:29 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.76
[32m[20230113 20:04:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.19
[32m[20230113 20:04:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 167.94
[32m[20230113 20:04:29 @agent_ppo2.py:144][0m Total time:      19.94 min
[32m[20230113 20:04:29 @agent_ppo2.py:146][0m 1808384 total steps have happened
[32m[20230113 20:04:29 @agent_ppo2.py:122][0m #------------------------ Iteration 883 --------------------------#
[32m[20230113 20:04:30 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 20:04:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:30 @agent_ppo2.py:186][0m |          -0.0022 |          20.7143 |           5.6092 |
[32m[20230113 20:04:30 @agent_ppo2.py:186][0m |           0.0028 |          14.8606 |           5.5956 |
[32m[20230113 20:04:30 @agent_ppo2.py:186][0m |          -0.0064 |          11.2843 |           5.5828 |
[32m[20230113 20:04:30 @agent_ppo2.py:186][0m |          -0.0087 |          10.0664 |           5.5901 |
[32m[20230113 20:04:30 @agent_ppo2.py:186][0m |          -0.0022 |           9.7810 |           5.5953 |
[32m[20230113 20:04:30 @agent_ppo2.py:186][0m |          -0.0009 |           9.1115 |           5.5935 |
[32m[20230113 20:04:30 @agent_ppo2.py:186][0m |          -0.0105 |           8.1034 |           5.5913 |
[32m[20230113 20:04:30 @agent_ppo2.py:186][0m |          -0.0145 |           7.7215 |           5.5906 |
[32m[20230113 20:04:30 @agent_ppo2.py:186][0m |          -0.0134 |           7.3141 |           5.5887 |
[32m[20230113 20:04:30 @agent_ppo2.py:186][0m |          -0.0112 |           7.3012 |           5.5881 |
[32m[20230113 20:04:30 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:04:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 131.30
[32m[20230113 20:04:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 215.72
[32m[20230113 20:04:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.79
[32m[20230113 20:04:30 @agent_ppo2.py:144][0m Total time:      19.96 min
[32m[20230113 20:04:30 @agent_ppo2.py:146][0m 1810432 total steps have happened
[32m[20230113 20:04:30 @agent_ppo2.py:122][0m #------------------------ Iteration 884 --------------------------#
[32m[20230113 20:04:31 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:31 @agent_ppo2.py:186][0m |          -0.0027 |           7.7521 |           5.6014 |
[32m[20230113 20:04:31 @agent_ppo2.py:186][0m |          -0.0045 |           6.0078 |           5.5914 |
[32m[20230113 20:04:31 @agent_ppo2.py:186][0m |          -0.0090 |           5.4563 |           5.5850 |
[32m[20230113 20:04:31 @agent_ppo2.py:186][0m |          -0.0077 |           5.1122 |           5.5786 |
[32m[20230113 20:04:31 @agent_ppo2.py:186][0m |          -0.0102 |           4.9037 |           5.5845 |
[32m[20230113 20:04:31 @agent_ppo2.py:186][0m |          -0.0073 |           4.7240 |           5.5753 |
[32m[20230113 20:04:31 @agent_ppo2.py:186][0m |          -0.0105 |           4.5326 |           5.5795 |
[32m[20230113 20:04:31 @agent_ppo2.py:186][0m |          -0.0096 |           4.5966 |           5.5743 |
[32m[20230113 20:04:31 @agent_ppo2.py:186][0m |          -0.0141 |           4.3573 |           5.5782 |
[32m[20230113 20:04:31 @agent_ppo2.py:186][0m |          -0.0118 |           4.3017 |           5.5741 |
[32m[20230113 20:04:31 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.10
[32m[20230113 20:04:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.22
[32m[20230113 20:04:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.92
[32m[20230113 20:04:32 @agent_ppo2.py:144][0m Total time:      19.99 min
[32m[20230113 20:04:32 @agent_ppo2.py:146][0m 1812480 total steps have happened
[32m[20230113 20:04:32 @agent_ppo2.py:122][0m #------------------------ Iteration 885 --------------------------#
[32m[20230113 20:04:32 @agent_ppo2.py:128][0m Sampling time: 0.31 s by 1 slaves
[32m[20230113 20:04:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:32 @agent_ppo2.py:186][0m |          -0.0053 |          27.9847 |           5.6177 |
[32m[20230113 20:04:32 @agent_ppo2.py:186][0m |          -0.0160 |          15.2668 |           5.6089 |
[32m[20230113 20:04:32 @agent_ppo2.py:186][0m |           0.0053 |          13.5468 |           5.6064 |
[32m[20230113 20:04:32 @agent_ppo2.py:186][0m |           0.0022 |          12.0675 |           5.6083 |
[32m[20230113 20:04:32 @agent_ppo2.py:186][0m |          -0.0103 |           9.9927 |           5.5939 |
[32m[20230113 20:04:32 @agent_ppo2.py:186][0m |          -0.0153 |           8.3887 |           5.5981 |
[32m[20230113 20:04:32 @agent_ppo2.py:186][0m |          -0.0101 |           7.9244 |           5.5992 |
[32m[20230113 20:04:32 @agent_ppo2.py:186][0m |          -0.0115 |           7.5367 |           5.5744 |
[32m[20230113 20:04:32 @agent_ppo2.py:186][0m |          -0.0157 |           6.8288 |           5.5874 |
[32m[20230113 20:04:32 @agent_ppo2.py:186][0m |           0.0084 |           6.4442 |           5.5947 |
[32m[20230113 20:04:32 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 20:04:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 99.82
[32m[20230113 20:04:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.43
[32m[20230113 20:04:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.23
[32m[20230113 20:04:33 @agent_ppo2.py:144][0m Total time:      20.00 min
[32m[20230113 20:04:33 @agent_ppo2.py:146][0m 1814528 total steps have happened
[32m[20230113 20:04:33 @agent_ppo2.py:122][0m #------------------------ Iteration 886 --------------------------#
[32m[20230113 20:04:33 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:04:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:33 @agent_ppo2.py:186][0m |           0.0013 |           8.0499 |           5.7430 |
[32m[20230113 20:04:33 @agent_ppo2.py:186][0m |          -0.0054 |           5.7828 |           5.7406 |
[32m[20230113 20:04:33 @agent_ppo2.py:186][0m |          -0.0091 |           4.9035 |           5.7389 |
[32m[20230113 20:04:33 @agent_ppo2.py:186][0m |          -0.0112 |           4.4219 |           5.7412 |
[32m[20230113 20:04:33 @agent_ppo2.py:186][0m |          -0.0102 |           4.1277 |           5.7366 |
[32m[20230113 20:04:34 @agent_ppo2.py:186][0m |          -0.0105 |           3.8648 |           5.7406 |
[32m[20230113 20:04:34 @agent_ppo2.py:186][0m |          -0.0131 |           3.6702 |           5.7380 |
[32m[20230113 20:04:34 @agent_ppo2.py:186][0m |          -0.0143 |           3.4840 |           5.7413 |
[32m[20230113 20:04:34 @agent_ppo2.py:186][0m |          -0.0143 |           3.3434 |           5.7419 |
[32m[20230113 20:04:34 @agent_ppo2.py:186][0m |          -0.0154 |           3.2410 |           5.7406 |
[32m[20230113 20:04:34 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:04:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.87
[32m[20230113 20:04:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.28
[32m[20230113 20:04:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 134.48
[32m[20230113 20:04:34 @agent_ppo2.py:144][0m Total time:      20.02 min
[32m[20230113 20:04:34 @agent_ppo2.py:146][0m 1816576 total steps have happened
[32m[20230113 20:04:34 @agent_ppo2.py:122][0m #------------------------ Iteration 887 --------------------------#
[32m[20230113 20:04:34 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:35 @agent_ppo2.py:186][0m |           0.0010 |           5.8099 |           5.6689 |
[32m[20230113 20:04:35 @agent_ppo2.py:186][0m |          -0.0010 |           4.8416 |           5.6598 |
[32m[20230113 20:04:35 @agent_ppo2.py:186][0m |          -0.0042 |           4.4464 |           5.6564 |
[32m[20230113 20:04:35 @agent_ppo2.py:186][0m |          -0.0045 |           4.1473 |           5.6490 |
[32m[20230113 20:04:35 @agent_ppo2.py:186][0m |          -0.0063 |           3.9078 |           5.6474 |
[32m[20230113 20:04:35 @agent_ppo2.py:186][0m |          -0.0075 |           3.7235 |           5.6432 |
[32m[20230113 20:04:35 @agent_ppo2.py:186][0m |          -0.0090 |           3.5701 |           5.6410 |
[32m[20230113 20:04:35 @agent_ppo2.py:186][0m |          -0.0109 |           3.4602 |           5.6401 |
[32m[20230113 20:04:35 @agent_ppo2.py:186][0m |          -0.0097 |           3.3733 |           5.6394 |
[32m[20230113 20:04:35 @agent_ppo2.py:186][0m |          -0.0117 |           3.2774 |           5.6358 |
[32m[20230113 20:04:35 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:04:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.73
[32m[20230113 20:04:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.59
[32m[20230113 20:04:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 130.48
[32m[20230113 20:04:35 @agent_ppo2.py:144][0m Total time:      20.04 min
[32m[20230113 20:04:35 @agent_ppo2.py:146][0m 1818624 total steps have happened
[32m[20230113 20:04:35 @agent_ppo2.py:122][0m #------------------------ Iteration 888 --------------------------#
[32m[20230113 20:04:36 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:36 @agent_ppo2.py:186][0m |           0.0002 |           6.5544 |           5.8640 |
[32m[20230113 20:04:36 @agent_ppo2.py:186][0m |          -0.0043 |           5.3318 |           5.8594 |
[32m[20230113 20:04:36 @agent_ppo2.py:186][0m |          -0.0072 |           4.7556 |           5.8581 |
[32m[20230113 20:04:36 @agent_ppo2.py:186][0m |          -0.0088 |           4.4280 |           5.8512 |
[32m[20230113 20:04:36 @agent_ppo2.py:186][0m |          -0.0106 |           4.2054 |           5.8591 |
[32m[20230113 20:04:36 @agent_ppo2.py:186][0m |          -0.0117 |           4.0260 |           5.8561 |
[32m[20230113 20:04:36 @agent_ppo2.py:186][0m |          -0.0124 |           3.8566 |           5.8559 |
[32m[20230113 20:04:36 @agent_ppo2.py:186][0m |          -0.0131 |           3.7084 |           5.8588 |
[32m[20230113 20:04:36 @agent_ppo2.py:186][0m |          -0.0140 |           3.6032 |           5.8538 |
[32m[20230113 20:04:36 @agent_ppo2.py:186][0m |          -0.0144 |           3.4870 |           5.8563 |
[32m[20230113 20:04:36 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.63
[32m[20230113 20:04:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.32
[32m[20230113 20:04:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.43
[32m[20230113 20:04:37 @agent_ppo2.py:144][0m Total time:      20.07 min
[32m[20230113 20:04:37 @agent_ppo2.py:146][0m 1820672 total steps have happened
[32m[20230113 20:04:37 @agent_ppo2.py:122][0m #------------------------ Iteration 889 --------------------------#
[32m[20230113 20:04:37 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:37 @agent_ppo2.py:186][0m |           0.0033 |           6.4566 |           5.7856 |
[32m[20230113 20:04:37 @agent_ppo2.py:186][0m |           0.0040 |           5.5206 |           5.7786 |
[32m[20230113 20:04:37 @agent_ppo2.py:186][0m |          -0.0058 |           4.9368 |           5.7733 |
[32m[20230113 20:04:37 @agent_ppo2.py:186][0m |          -0.0103 |           4.5485 |           5.7678 |
[32m[20230113 20:04:37 @agent_ppo2.py:186][0m |          -0.0122 |           4.2894 |           5.7636 |
[32m[20230113 20:04:37 @agent_ppo2.py:186][0m |          -0.0106 |           4.1571 |           5.7632 |
[32m[20230113 20:04:37 @agent_ppo2.py:186][0m |          -0.0061 |           4.0461 |           5.7634 |
[32m[20230113 20:04:37 @agent_ppo2.py:186][0m |          -0.0116 |           3.9091 |           5.7617 |
[32m[20230113 20:04:37 @agent_ppo2.py:186][0m |          -0.0126 |           3.8020 |           5.7637 |
[32m[20230113 20:04:37 @agent_ppo2.py:186][0m |          -0.0158 |           3.6948 |           5.7630 |
[32m[20230113 20:04:37 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:04:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.77
[32m[20230113 20:04:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.20
[32m[20230113 20:04:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.25
[32m[20230113 20:04:38 @agent_ppo2.py:144][0m Total time:      20.09 min
[32m[20230113 20:04:38 @agent_ppo2.py:146][0m 1822720 total steps have happened
[32m[20230113 20:04:38 @agent_ppo2.py:122][0m #------------------------ Iteration 890 --------------------------#
[32m[20230113 20:04:38 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:04:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:38 @agent_ppo2.py:186][0m |          -0.0028 |          11.2294 |           5.6549 |
[32m[20230113 20:04:38 @agent_ppo2.py:186][0m |          -0.0080 |           5.6667 |           5.6379 |
[32m[20230113 20:04:38 @agent_ppo2.py:186][0m |           0.0220 |           5.1812 |           5.6417 |
[32m[20230113 20:04:38 @agent_ppo2.py:186][0m |          -0.0128 |           4.4214 |           5.6361 |
[32m[20230113 20:04:38 @agent_ppo2.py:186][0m |          -0.0150 |           4.0860 |           5.6313 |
[32m[20230113 20:04:38 @agent_ppo2.py:186][0m |          -0.0161 |           3.8247 |           5.6348 |
[32m[20230113 20:04:38 @agent_ppo2.py:186][0m |           0.0049 |           3.6668 |           5.6331 |
[32m[20230113 20:04:39 @agent_ppo2.py:186][0m |          -0.0072 |           3.6835 |           5.6353 |
[32m[20230113 20:04:39 @agent_ppo2.py:186][0m |          -0.0195 |           3.4637 |           5.6276 |
[32m[20230113 20:04:39 @agent_ppo2.py:186][0m |          -0.0193 |           3.2793 |           5.6313 |
[32m[20230113 20:04:39 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 20:04:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 118.93
[32m[20230113 20:04:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 220.76
[32m[20230113 20:04:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.16
[32m[20230113 20:04:39 @agent_ppo2.py:144][0m Total time:      20.11 min
[32m[20230113 20:04:39 @agent_ppo2.py:146][0m 1824768 total steps have happened
[32m[20230113 20:04:39 @agent_ppo2.py:122][0m #------------------------ Iteration 891 --------------------------#
[32m[20230113 20:04:40 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:04:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:40 @agent_ppo2.py:186][0m |           0.0020 |          15.5518 |           5.7289 |
[32m[20230113 20:04:40 @agent_ppo2.py:186][0m |          -0.0077 |           9.9650 |           5.7234 |
[32m[20230113 20:04:40 @agent_ppo2.py:186][0m |          -0.0100 |           8.5369 |           5.7219 |
[32m[20230113 20:04:40 @agent_ppo2.py:186][0m |          -0.0110 |           8.0468 |           5.7197 |
[32m[20230113 20:04:40 @agent_ppo2.py:186][0m |          -0.0112 |           7.2182 |           5.7170 |
[32m[20230113 20:04:40 @agent_ppo2.py:186][0m |          -0.0126 |           6.8622 |           5.7189 |
[32m[20230113 20:04:40 @agent_ppo2.py:186][0m |          -0.0122 |           6.5184 |           5.7143 |
[32m[20230113 20:04:40 @agent_ppo2.py:186][0m |          -0.0139 |           6.2684 |           5.7133 |
[32m[20230113 20:04:40 @agent_ppo2.py:186][0m |          -0.0144 |           5.9331 |           5.7188 |
[32m[20230113 20:04:40 @agent_ppo2.py:186][0m |          -0.0137 |           5.7815 |           5.7107 |
[32m[20230113 20:04:40 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 20:04:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 96.92
[32m[20230113 20:04:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.67
[32m[20230113 20:04:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.48
[32m[20230113 20:04:40 @agent_ppo2.py:144][0m Total time:      20.13 min
[32m[20230113 20:04:40 @agent_ppo2.py:146][0m 1826816 total steps have happened
[32m[20230113 20:04:40 @agent_ppo2.py:122][0m #------------------------ Iteration 892 --------------------------#
[32m[20230113 20:04:41 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:41 @agent_ppo2.py:186][0m |           0.0071 |           6.1804 |           5.7428 |
[32m[20230113 20:04:41 @agent_ppo2.py:186][0m |          -0.0035 |           5.2190 |           5.7416 |
[32m[20230113 20:04:41 @agent_ppo2.py:186][0m |          -0.0037 |           4.8587 |           5.7305 |
[32m[20230113 20:04:41 @agent_ppo2.py:186][0m |          -0.0059 |           4.5730 |           5.7262 |
[32m[20230113 20:04:41 @agent_ppo2.py:186][0m |          -0.0182 |           4.3203 |           5.7252 |
[32m[20230113 20:04:41 @agent_ppo2.py:186][0m |          -0.0145 |           4.1143 |           5.7194 |
[32m[20230113 20:04:41 @agent_ppo2.py:186][0m |          -0.0197 |           4.0125 |           5.7222 |
[32m[20230113 20:04:41 @agent_ppo2.py:186][0m |          -0.0165 |           3.8155 |           5.7216 |
[32m[20230113 20:04:41 @agent_ppo2.py:186][0m |          -0.0111 |           3.6809 |           5.7204 |
[32m[20230113 20:04:41 @agent_ppo2.py:186][0m |          -0.0228 |           3.6660 |           5.7189 |
[32m[20230113 20:04:41 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:04:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.65
[32m[20230113 20:04:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.77
[32m[20230113 20:04:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.95
[32m[20230113 20:04:42 @agent_ppo2.py:144][0m Total time:      20.15 min
[32m[20230113 20:04:42 @agent_ppo2.py:146][0m 1828864 total steps have happened
[32m[20230113 20:04:42 @agent_ppo2.py:122][0m #------------------------ Iteration 893 --------------------------#
[32m[20230113 20:04:42 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:42 @agent_ppo2.py:186][0m |           0.0010 |           7.6792 |           5.7116 |
[32m[20230113 20:04:42 @agent_ppo2.py:186][0m |          -0.0108 |           5.8192 |           5.7216 |
[32m[20230113 20:04:42 @agent_ppo2.py:186][0m |          -0.0082 |           5.1143 |           5.7161 |
[32m[20230113 20:04:42 @agent_ppo2.py:186][0m |          -0.0162 |           4.7329 |           5.7130 |
[32m[20230113 20:04:42 @agent_ppo2.py:186][0m |          -0.0123 |           4.4980 |           5.7109 |
[32m[20230113 20:04:42 @agent_ppo2.py:186][0m |          -0.0146 |           4.2144 |           5.7104 |
[32m[20230113 20:04:43 @agent_ppo2.py:186][0m |          -0.0175 |           4.2199 |           5.7130 |
[32m[20230113 20:04:43 @agent_ppo2.py:186][0m |          -0.0098 |           3.9573 |           5.7123 |
[32m[20230113 20:04:43 @agent_ppo2.py:186][0m |          -0.0110 |           3.8462 |           5.7106 |
[32m[20230113 20:04:43 @agent_ppo2.py:186][0m |          -0.0171 |           3.7774 |           5.7086 |
[32m[20230113 20:04:43 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:04:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.08
[32m[20230113 20:04:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.71
[32m[20230113 20:04:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.79
[32m[20230113 20:04:43 @agent_ppo2.py:144][0m Total time:      20.18 min
[32m[20230113 20:04:43 @agent_ppo2.py:146][0m 1830912 total steps have happened
[32m[20230113 20:04:43 @agent_ppo2.py:122][0m #------------------------ Iteration 894 --------------------------#
[32m[20230113 20:04:43 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:04:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:44 @agent_ppo2.py:186][0m |           0.0002 |          14.5034 |           5.6686 |
[32m[20230113 20:04:44 @agent_ppo2.py:186][0m |          -0.0028 |           9.6343 |           5.6396 |
[32m[20230113 20:04:44 @agent_ppo2.py:186][0m |          -0.0068 |           8.1209 |           5.6594 |
[32m[20230113 20:04:44 @agent_ppo2.py:186][0m |          -0.0129 |           7.1657 |           5.6530 |
[32m[20230113 20:04:44 @agent_ppo2.py:186][0m |          -0.0113 |           6.6697 |           5.6549 |
[32m[20230113 20:04:44 @agent_ppo2.py:186][0m |          -0.0117 |           6.1069 |           5.6501 |
[32m[20230113 20:04:44 @agent_ppo2.py:186][0m |          -0.0144 |           5.5745 |           5.6456 |
[32m[20230113 20:04:44 @agent_ppo2.py:186][0m |          -0.0171 |           5.7066 |           5.6501 |
[32m[20230113 20:04:44 @agent_ppo2.py:186][0m |          -0.0166 |           4.9204 |           5.6431 |
[32m[20230113 20:04:44 @agent_ppo2.py:186][0m |          -0.0146 |           4.9370 |           5.6478 |
[32m[20230113 20:04:44 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 20:04:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 127.25
[32m[20230113 20:04:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 218.52
[32m[20230113 20:04:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.31
[32m[20230113 20:04:44 @agent_ppo2.py:144][0m Total time:      20.19 min
[32m[20230113 20:04:44 @agent_ppo2.py:146][0m 1832960 total steps have happened
[32m[20230113 20:04:44 @agent_ppo2.py:122][0m #------------------------ Iteration 895 --------------------------#
[32m[20230113 20:04:45 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:45 @agent_ppo2.py:186][0m |          -0.0031 |           5.7766 |           5.7395 |
[32m[20230113 20:04:45 @agent_ppo2.py:186][0m |          -0.0077 |           5.1298 |           5.7346 |
[32m[20230113 20:04:45 @agent_ppo2.py:186][0m |          -0.0084 |           4.7445 |           5.7326 |
[32m[20230113 20:04:45 @agent_ppo2.py:186][0m |          -0.0060 |           4.3994 |           5.7289 |
[32m[20230113 20:04:45 @agent_ppo2.py:186][0m |          -0.0093 |           4.1350 |           5.7299 |
[32m[20230113 20:04:45 @agent_ppo2.py:186][0m |          -0.0141 |           3.8687 |           5.7211 |
[32m[20230113 20:04:45 @agent_ppo2.py:186][0m |          -0.0150 |           3.7376 |           5.7234 |
[32m[20230113 20:04:45 @agent_ppo2.py:186][0m |          -0.0049 |           3.6600 |           5.7237 |
[32m[20230113 20:04:45 @agent_ppo2.py:186][0m |          -0.0092 |           3.4873 |           5.7186 |
[32m[20230113 20:04:45 @agent_ppo2.py:186][0m |          -0.0099 |           3.4640 |           5.7181 |
[32m[20230113 20:04:45 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:04:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.46
[32m[20230113 20:04:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.65
[32m[20230113 20:04:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.07
[32m[20230113 20:04:46 @agent_ppo2.py:144][0m Total time:      20.22 min
[32m[20230113 20:04:46 @agent_ppo2.py:146][0m 1835008 total steps have happened
[32m[20230113 20:04:46 @agent_ppo2.py:122][0m #------------------------ Iteration 896 --------------------------#
[32m[20230113 20:04:46 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:46 @agent_ppo2.py:186][0m |           0.0008 |           5.7162 |           5.7278 |
[32m[20230113 20:04:46 @agent_ppo2.py:186][0m |          -0.0047 |           4.7027 |           5.7193 |
[32m[20230113 20:04:46 @agent_ppo2.py:186][0m |          -0.0077 |           4.2713 |           5.7079 |
[32m[20230113 20:04:46 @agent_ppo2.py:186][0m |          -0.0095 |           3.9713 |           5.7102 |
[32m[20230113 20:04:46 @agent_ppo2.py:186][0m |          -0.0109 |           3.7559 |           5.7026 |
[32m[20230113 20:04:46 @agent_ppo2.py:186][0m |          -0.0121 |           3.6172 |           5.7033 |
[32m[20230113 20:04:46 @agent_ppo2.py:186][0m |          -0.0124 |           3.4540 |           5.6973 |
[32m[20230113 20:04:46 @agent_ppo2.py:186][0m |          -0.0136 |           3.3933 |           5.6993 |
[32m[20230113 20:04:46 @agent_ppo2.py:186][0m |          -0.0138 |           3.2788 |           5.6954 |
[32m[20230113 20:04:46 @agent_ppo2.py:186][0m |          -0.0142 |           3.1810 |           5.6951 |
[32m[20230113 20:04:46 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:04:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.11
[32m[20230113 20:04:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.35
[32m[20230113 20:04:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 171.02
[32m[20230113 20:04:47 @agent_ppo2.py:144][0m Total time:      20.24 min
[32m[20230113 20:04:47 @agent_ppo2.py:146][0m 1837056 total steps have happened
[32m[20230113 20:04:47 @agent_ppo2.py:122][0m #------------------------ Iteration 897 --------------------------#
[32m[20230113 20:04:47 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:04:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:47 @agent_ppo2.py:186][0m |          -0.0003 |           5.0358 |           5.7950 |
[32m[20230113 20:04:47 @agent_ppo2.py:186][0m |          -0.0049 |           4.0186 |           5.7886 |
[32m[20230113 20:04:47 @agent_ppo2.py:186][0m |          -0.0051 |           3.7050 |           5.7874 |
[32m[20230113 20:04:47 @agent_ppo2.py:186][0m |          -0.0082 |           3.4938 |           5.7914 |
[32m[20230113 20:04:48 @agent_ppo2.py:186][0m |          -0.0098 |           3.3565 |           5.7914 |
[32m[20230113 20:04:48 @agent_ppo2.py:186][0m |          -0.0079 |           3.2829 |           5.7892 |
[32m[20230113 20:04:48 @agent_ppo2.py:186][0m |          -0.0094 |           3.1655 |           5.7979 |
[32m[20230113 20:04:48 @agent_ppo2.py:186][0m |          -0.0101 |           3.1086 |           5.7888 |
[32m[20230113 20:04:48 @agent_ppo2.py:186][0m |          -0.0103 |           3.0460 |           5.7885 |
[32m[20230113 20:04:48 @agent_ppo2.py:186][0m |          -0.0113 |           2.9947 |           5.7962 |
[32m[20230113 20:04:48 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:04:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.58
[32m[20230113 20:04:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.89
[32m[20230113 20:04:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.73
[32m[20230113 20:04:48 @agent_ppo2.py:144][0m Total time:      20.26 min
[32m[20230113 20:04:48 @agent_ppo2.py:146][0m 1839104 total steps have happened
[32m[20230113 20:04:48 @agent_ppo2.py:122][0m #------------------------ Iteration 898 --------------------------#
[32m[20230113 20:04:49 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:04:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:49 @agent_ppo2.py:186][0m |          -0.0016 |           4.7942 |           5.6690 |
[32m[20230113 20:04:49 @agent_ppo2.py:186][0m |          -0.0047 |           3.6482 |           5.6715 |
[32m[20230113 20:04:49 @agent_ppo2.py:186][0m |          -0.0068 |           3.2189 |           5.6732 |
[32m[20230113 20:04:49 @agent_ppo2.py:186][0m |          -0.0070 |           2.9663 |           5.6740 |
[32m[20230113 20:04:49 @agent_ppo2.py:186][0m |          -0.0074 |           2.8386 |           5.6762 |
[32m[20230113 20:04:49 @agent_ppo2.py:186][0m |          -0.0046 |           2.6987 |           5.6757 |
[32m[20230113 20:04:49 @agent_ppo2.py:186][0m |          -0.0106 |           2.5961 |           5.6725 |
[32m[20230113 20:04:49 @agent_ppo2.py:186][0m |          -0.0096 |           2.4669 |           5.6792 |
[32m[20230113 20:04:49 @agent_ppo2.py:186][0m |          -0.0102 |           2.3819 |           5.6746 |
[32m[20230113 20:04:49 @agent_ppo2.py:186][0m |          -0.0147 |           2.3421 |           5.6776 |
[32m[20230113 20:04:49 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:04:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.47
[32m[20230113 20:04:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.80
[32m[20230113 20:04:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.07
[32m[20230113 20:04:49 @agent_ppo2.py:144][0m Total time:      20.28 min
[32m[20230113 20:04:49 @agent_ppo2.py:146][0m 1841152 total steps have happened
[32m[20230113 20:04:49 @agent_ppo2.py:122][0m #------------------------ Iteration 899 --------------------------#
[32m[20230113 20:04:50 @agent_ppo2.py:128][0m Sampling time: 0.62 s by 1 slaves
[32m[20230113 20:04:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:50 @agent_ppo2.py:186][0m |           0.0006 |           5.6956 |           5.9058 |
[32m[20230113 20:04:50 @agent_ppo2.py:186][0m |          -0.0032 |           4.2330 |           5.8961 |
[32m[20230113 20:04:50 @agent_ppo2.py:186][0m |          -0.0058 |           3.6780 |           5.8980 |
[32m[20230113 20:04:50 @agent_ppo2.py:186][0m |          -0.0074 |           3.3905 |           5.8873 |
[32m[20230113 20:04:50 @agent_ppo2.py:186][0m |          -0.0087 |           3.1822 |           5.8843 |
[32m[20230113 20:04:50 @agent_ppo2.py:186][0m |          -0.0100 |           3.0297 |           5.8843 |
[32m[20230113 20:04:51 @agent_ppo2.py:186][0m |          -0.0111 |           2.8829 |           5.8873 |
[32m[20230113 20:04:51 @agent_ppo2.py:186][0m |          -0.0118 |           2.8081 |           5.8828 |
[32m[20230113 20:04:51 @agent_ppo2.py:186][0m |          -0.0127 |           2.7267 |           5.8857 |
[32m[20230113 20:04:51 @agent_ppo2.py:186][0m |          -0.0128 |           2.6477 |           5.8781 |
[32m[20230113 20:04:51 @agent_ppo2.py:131][0m Policy update time: 0.58 s
[32m[20230113 20:04:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.85
[32m[20230113 20:04:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.57
[32m[20230113 20:04:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.32
[32m[20230113 20:04:51 @agent_ppo2.py:144][0m Total time:      20.31 min
[32m[20230113 20:04:51 @agent_ppo2.py:146][0m 1843200 total steps have happened
[32m[20230113 20:04:51 @agent_ppo2.py:122][0m #------------------------ Iteration 900 --------------------------#
[32m[20230113 20:04:52 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:52 @agent_ppo2.py:186][0m |           0.0005 |           6.4060 |           5.6461 |
[32m[20230113 20:04:52 @agent_ppo2.py:186][0m |          -0.0043 |           5.0163 |           5.6424 |
[32m[20230113 20:04:52 @agent_ppo2.py:186][0m |          -0.0039 |           4.3253 |           5.6351 |
[32m[20230113 20:04:52 @agent_ppo2.py:186][0m |          -0.0083 |           3.8949 |           5.6319 |
[32m[20230113 20:04:52 @agent_ppo2.py:186][0m |          -0.0084 |           3.5831 |           5.6397 |
[32m[20230113 20:04:52 @agent_ppo2.py:186][0m |          -0.0072 |           3.4114 |           5.6366 |
[32m[20230113 20:04:52 @agent_ppo2.py:186][0m |          -0.0062 |           3.2323 |           5.6405 |
[32m[20230113 20:04:52 @agent_ppo2.py:186][0m |          -0.0111 |           3.1048 |           5.6338 |
[32m[20230113 20:04:52 @agent_ppo2.py:186][0m |          -0.0113 |           2.9892 |           5.6420 |
[32m[20230113 20:04:52 @agent_ppo2.py:186][0m |          -0.0092 |           2.9017 |           5.6382 |
[32m[20230113 20:04:52 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:04:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.31
[32m[20230113 20:04:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.84
[32m[20230113 20:04:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.61
[32m[20230113 20:04:52 @agent_ppo2.py:144][0m Total time:      20.33 min
[32m[20230113 20:04:52 @agent_ppo2.py:146][0m 1845248 total steps have happened
[32m[20230113 20:04:52 @agent_ppo2.py:122][0m #------------------------ Iteration 901 --------------------------#
[32m[20230113 20:04:53 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:04:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:53 @agent_ppo2.py:186][0m |          -0.0047 |           5.7200 |           5.7183 |
[32m[20230113 20:04:53 @agent_ppo2.py:186][0m |          -0.0075 |           4.5794 |           5.7141 |
[32m[20230113 20:04:53 @agent_ppo2.py:186][0m |          -0.0089 |           4.1361 |           5.7074 |
[32m[20230113 20:04:53 @agent_ppo2.py:186][0m |          -0.0111 |           3.8836 |           5.7087 |
[32m[20230113 20:04:53 @agent_ppo2.py:186][0m |          -0.0037 |           3.8408 |           5.7048 |
[32m[20230113 20:04:53 @agent_ppo2.py:186][0m |          -0.0089 |           3.5897 |           5.7028 |
[32m[20230113 20:04:53 @agent_ppo2.py:186][0m |          -0.0095 |           3.4757 |           5.7027 |
[32m[20230113 20:04:53 @agent_ppo2.py:186][0m |          -0.0116 |           3.3781 |           5.7061 |
[32m[20230113 20:04:53 @agent_ppo2.py:186][0m |          -0.0141 |           3.2987 |           5.7042 |
[32m[20230113 20:04:53 @agent_ppo2.py:186][0m |          -0.0129 |           3.2479 |           5.7008 |
[32m[20230113 20:04:53 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.29
[32m[20230113 20:04:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.65
[32m[20230113 20:04:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.35
[32m[20230113 20:04:54 @agent_ppo2.py:144][0m Total time:      20.35 min
[32m[20230113 20:04:54 @agent_ppo2.py:146][0m 1847296 total steps have happened
[32m[20230113 20:04:54 @agent_ppo2.py:122][0m #------------------------ Iteration 902 --------------------------#
[32m[20230113 20:04:54 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:54 @agent_ppo2.py:186][0m |           0.0004 |           6.4334 |           5.7786 |
[32m[20230113 20:04:54 @agent_ppo2.py:186][0m |          -0.0039 |           5.0007 |           5.7699 |
[32m[20230113 20:04:54 @agent_ppo2.py:186][0m |          -0.0059 |           4.4798 |           5.7690 |
[32m[20230113 20:04:54 @agent_ppo2.py:186][0m |          -0.0077 |           4.1446 |           5.7607 |
[32m[20230113 20:04:54 @agent_ppo2.py:186][0m |          -0.0080 |           3.9001 |           5.7584 |
[32m[20230113 20:04:54 @agent_ppo2.py:186][0m |          -0.0103 |           3.7310 |           5.7466 |
[32m[20230113 20:04:54 @agent_ppo2.py:186][0m |          -0.0103 |           3.5930 |           5.7480 |
[32m[20230113 20:04:55 @agent_ppo2.py:186][0m |          -0.0105 |           3.4940 |           5.7450 |
[32m[20230113 20:04:55 @agent_ppo2.py:186][0m |          -0.0124 |           3.3984 |           5.7388 |
[32m[20230113 20:04:55 @agent_ppo2.py:186][0m |          -0.0126 |           3.2996 |           5.7422 |
[32m[20230113 20:04:55 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.96
[32m[20230113 20:04:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.59
[32m[20230113 20:04:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.67
[32m[20230113 20:04:55 @agent_ppo2.py:144][0m Total time:      20.37 min
[32m[20230113 20:04:55 @agent_ppo2.py:146][0m 1849344 total steps have happened
[32m[20230113 20:04:55 @agent_ppo2.py:122][0m #------------------------ Iteration 903 --------------------------#
[32m[20230113 20:04:55 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:56 @agent_ppo2.py:186][0m |          -0.0026 |           6.0133 |           5.6465 |
[32m[20230113 20:04:56 @agent_ppo2.py:186][0m |          -0.0004 |           5.0260 |           5.6398 |
[32m[20230113 20:04:56 @agent_ppo2.py:186][0m |          -0.0064 |           4.5535 |           5.6337 |
[32m[20230113 20:04:56 @agent_ppo2.py:186][0m |          -0.0074 |           4.2264 |           5.6355 |
[32m[20230113 20:04:56 @agent_ppo2.py:186][0m |          -0.0087 |           4.0171 |           5.6350 |
[32m[20230113 20:04:56 @agent_ppo2.py:186][0m |          -0.0086 |           3.7584 |           5.6357 |
[32m[20230113 20:04:56 @agent_ppo2.py:186][0m |          -0.0077 |           3.6259 |           5.6381 |
[32m[20230113 20:04:56 @agent_ppo2.py:186][0m |          -0.0128 |           3.4236 |           5.6325 |
[32m[20230113 20:04:56 @agent_ppo2.py:186][0m |          -0.0144 |           3.3214 |           5.6336 |
[32m[20230113 20:04:56 @agent_ppo2.py:186][0m |          -0.0088 |           3.2631 |           5.6392 |
[32m[20230113 20:04:56 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.73
[32m[20230113 20:04:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.46
[32m[20230113 20:04:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.15
[32m[20230113 20:04:56 @agent_ppo2.py:144][0m Total time:      20.40 min
[32m[20230113 20:04:56 @agent_ppo2.py:146][0m 1851392 total steps have happened
[32m[20230113 20:04:56 @agent_ppo2.py:122][0m #------------------------ Iteration 904 --------------------------#
[32m[20230113 20:04:57 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:57 @agent_ppo2.py:186][0m |          -0.0017 |           5.3341 |           5.7898 |
[32m[20230113 20:04:57 @agent_ppo2.py:186][0m |          -0.0055 |           4.1836 |           5.7880 |
[32m[20230113 20:04:57 @agent_ppo2.py:186][0m |          -0.0071 |           3.8328 |           5.7885 |
[32m[20230113 20:04:57 @agent_ppo2.py:186][0m |          -0.0091 |           3.6097 |           5.7920 |
[32m[20230113 20:04:57 @agent_ppo2.py:186][0m |          -0.0103 |           3.4516 |           5.7903 |
[32m[20230113 20:04:57 @agent_ppo2.py:186][0m |          -0.0111 |           3.3204 |           5.7930 |
[32m[20230113 20:04:57 @agent_ppo2.py:186][0m |          -0.0116 |           3.2421 |           5.7925 |
[32m[20230113 20:04:57 @agent_ppo2.py:186][0m |          -0.0115 |           3.1411 |           5.7958 |
[32m[20230113 20:04:57 @agent_ppo2.py:186][0m |          -0.0128 |           3.0472 |           5.7931 |
[32m[20230113 20:04:57 @agent_ppo2.py:186][0m |          -0.0129 |           3.0005 |           5.7969 |
[32m[20230113 20:04:57 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:04:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.82
[32m[20230113 20:04:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.63
[32m[20230113 20:04:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.62
[32m[20230113 20:04:58 @agent_ppo2.py:144][0m Total time:      20.42 min
[32m[20230113 20:04:58 @agent_ppo2.py:146][0m 1853440 total steps have happened
[32m[20230113 20:04:58 @agent_ppo2.py:122][0m #------------------------ Iteration 905 --------------------------#
[32m[20230113 20:04:58 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:58 @agent_ppo2.py:186][0m |           0.0026 |           5.2906 |           5.4738 |
[32m[20230113 20:04:58 @agent_ppo2.py:186][0m |          -0.0097 |           4.1498 |           5.4676 |
[32m[20230113 20:04:58 @agent_ppo2.py:186][0m |           0.0037 |           3.9172 |           5.4689 |
[32m[20230113 20:04:58 @agent_ppo2.py:186][0m |          -0.0143 |           3.5913 |           5.4629 |
[32m[20230113 20:04:58 @agent_ppo2.py:186][0m |          -0.0030 |           3.4423 |           5.4562 |
[32m[20230113 20:04:58 @agent_ppo2.py:186][0m |          -0.0220 |           3.1203 |           5.4609 |
[32m[20230113 20:04:58 @agent_ppo2.py:186][0m |          -0.0211 |           2.9912 |           5.4565 |
[32m[20230113 20:04:58 @agent_ppo2.py:186][0m |          -0.0013 |           2.9266 |           5.4560 |
[32m[20230113 20:04:59 @agent_ppo2.py:186][0m |          -0.0116 |           2.8150 |           5.4574 |
[32m[20230113 20:04:59 @agent_ppo2.py:186][0m |          -0.0112 |           2.7503 |           5.4553 |
[32m[20230113 20:04:59 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:04:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.49
[32m[20230113 20:04:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.17
[32m[20230113 20:04:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.51
[32m[20230113 20:04:59 @agent_ppo2.py:144][0m Total time:      20.44 min
[32m[20230113 20:04:59 @agent_ppo2.py:146][0m 1855488 total steps have happened
[32m[20230113 20:04:59 @agent_ppo2.py:122][0m #------------------------ Iteration 906 --------------------------#
[32m[20230113 20:04:59 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:04:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:04:59 @agent_ppo2.py:186][0m |           0.0004 |           6.4721 |           5.6504 |
[32m[20230113 20:05:00 @agent_ppo2.py:186][0m |           0.0081 |           6.3840 |           5.6408 |
[32m[20230113 20:05:00 @agent_ppo2.py:186][0m |          -0.0102 |           5.3234 |           5.6448 |
[32m[20230113 20:05:00 @agent_ppo2.py:186][0m |          -0.0130 |           4.9959 |           5.6459 |
[32m[20230113 20:05:00 @agent_ppo2.py:186][0m |          -0.0115 |           4.8160 |           5.6428 |
[32m[20230113 20:05:00 @agent_ppo2.py:186][0m |          -0.0118 |           4.6386 |           5.6414 |
[32m[20230113 20:05:00 @agent_ppo2.py:186][0m |          -0.0121 |           4.4741 |           5.6451 |
[32m[20230113 20:05:00 @agent_ppo2.py:186][0m |          -0.0083 |           4.3683 |           5.6451 |
[32m[20230113 20:05:00 @agent_ppo2.py:186][0m |          -0.0080 |           4.3075 |           5.6481 |
[32m[20230113 20:05:00 @agent_ppo2.py:186][0m |          -0.0133 |           4.2127 |           5.6464 |
[32m[20230113 20:05:00 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:05:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.79
[32m[20230113 20:05:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.57
[32m[20230113 20:05:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.94
[32m[20230113 20:05:00 @agent_ppo2.py:144][0m Total time:      20.46 min
[32m[20230113 20:05:00 @agent_ppo2.py:146][0m 1857536 total steps have happened
[32m[20230113 20:05:00 @agent_ppo2.py:122][0m #------------------------ Iteration 907 --------------------------#
[32m[20230113 20:05:01 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:05:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:01 @agent_ppo2.py:186][0m |          -0.0014 |          13.3422 |           5.7786 |
[32m[20230113 20:05:01 @agent_ppo2.py:186][0m |          -0.0082 |           8.0545 |           5.7644 |
[32m[20230113 20:05:01 @agent_ppo2.py:186][0m |          -0.0132 |           6.8789 |           5.7690 |
[32m[20230113 20:05:01 @agent_ppo2.py:186][0m |          -0.0153 |           6.4403 |           5.7683 |
[32m[20230113 20:05:01 @agent_ppo2.py:186][0m |          -0.0150 |           6.1447 |           5.7642 |
[32m[20230113 20:05:01 @agent_ppo2.py:186][0m |          -0.0156 |           5.5252 |           5.7634 |
[32m[20230113 20:05:01 @agent_ppo2.py:186][0m |          -0.0234 |           5.2889 |           5.7595 |
[32m[20230113 20:05:01 @agent_ppo2.py:186][0m |          -0.0164 |           5.1996 |           5.7573 |
[32m[20230113 20:05:01 @agent_ppo2.py:186][0m |          -0.0240 |           5.0543 |           5.7601 |
[32m[20230113 20:05:01 @agent_ppo2.py:186][0m |          -0.0122 |           5.1697 |           5.7543 |
[32m[20230113 20:05:01 @agent_ppo2.py:131][0m Policy update time: 0.34 s
[32m[20230113 20:05:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 113.13
[32m[20230113 20:05:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.77
[32m[20230113 20:05:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.99
[32m[20230113 20:05:01 @agent_ppo2.py:144][0m Total time:      20.48 min
[32m[20230113 20:05:01 @agent_ppo2.py:146][0m 1859584 total steps have happened
[32m[20230113 20:05:01 @agent_ppo2.py:122][0m #------------------------ Iteration 908 --------------------------#
[32m[20230113 20:05:02 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:02 @agent_ppo2.py:186][0m |          -0.0045 |           5.8257 |           5.7003 |
[32m[20230113 20:05:02 @agent_ppo2.py:186][0m |          -0.0115 |           4.9946 |           5.6857 |
[32m[20230113 20:05:02 @agent_ppo2.py:186][0m |          -0.0044 |           4.5972 |           5.6950 |
[32m[20230113 20:05:02 @agent_ppo2.py:186][0m |          -0.0097 |           4.3063 |           5.6904 |
[32m[20230113 20:05:02 @agent_ppo2.py:186][0m |          -0.0040 |           4.1286 |           5.6851 |
[32m[20230113 20:05:02 @agent_ppo2.py:186][0m |          -0.0081 |           4.0024 |           5.6840 |
[32m[20230113 20:05:02 @agent_ppo2.py:186][0m |          -0.0126 |           3.9140 |           5.6820 |
[32m[20230113 20:05:02 @agent_ppo2.py:186][0m |          -0.0154 |           3.8159 |           5.6821 |
[32m[20230113 20:05:02 @agent_ppo2.py:186][0m |          -0.0130 |           3.7224 |           5.6832 |
[32m[20230113 20:05:02 @agent_ppo2.py:186][0m |          -0.0106 |           3.6654 |           5.6799 |
[32m[20230113 20:05:02 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:05:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.40
[32m[20230113 20:05:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.52
[32m[20230113 20:05:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.93
[32m[20230113 20:05:03 @agent_ppo2.py:144][0m Total time:      20.50 min
[32m[20230113 20:05:03 @agent_ppo2.py:146][0m 1861632 total steps have happened
[32m[20230113 20:05:03 @agent_ppo2.py:122][0m #------------------------ Iteration 909 --------------------------#
[32m[20230113 20:05:03 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:05:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:03 @agent_ppo2.py:186][0m |          -0.0004 |           5.5098 |           5.8222 |
[32m[20230113 20:05:03 @agent_ppo2.py:186][0m |          -0.0066 |           4.6713 |           5.8056 |
[32m[20230113 20:05:03 @agent_ppo2.py:186][0m |          -0.0080 |           4.3376 |           5.8046 |
[32m[20230113 20:05:03 @agent_ppo2.py:186][0m |          -0.0133 |           4.0650 |           5.7949 |
[32m[20230113 20:05:03 @agent_ppo2.py:186][0m |          -0.0147 |           3.8927 |           5.7976 |
[32m[20230113 20:05:03 @agent_ppo2.py:186][0m |          -0.0161 |           3.7704 |           5.7939 |
[32m[20230113 20:05:03 @agent_ppo2.py:186][0m |          -0.0115 |           3.7307 |           5.8013 |
[32m[20230113 20:05:04 @agent_ppo2.py:186][0m |          -0.0155 |           3.5828 |           5.8000 |
[32m[20230113 20:05:04 @agent_ppo2.py:186][0m |          -0.0182 |           3.5217 |           5.7980 |
[32m[20230113 20:05:04 @agent_ppo2.py:186][0m |          -0.0161 |           3.4554 |           5.7955 |
[32m[20230113 20:05:04 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:05:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.05
[32m[20230113 20:05:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.17
[32m[20230113 20:05:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.16
[32m[20230113 20:05:04 @agent_ppo2.py:144][0m Total time:      20.52 min
[32m[20230113 20:05:04 @agent_ppo2.py:146][0m 1863680 total steps have happened
[32m[20230113 20:05:04 @agent_ppo2.py:122][0m #------------------------ Iteration 910 --------------------------#
[32m[20230113 20:05:04 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:05 @agent_ppo2.py:186][0m |           0.0007 |           6.5325 |           5.8716 |
[32m[20230113 20:05:05 @agent_ppo2.py:186][0m |          -0.0048 |           5.6543 |           5.8575 |
[32m[20230113 20:05:05 @agent_ppo2.py:186][0m |          -0.0070 |           5.2689 |           5.8507 |
[32m[20230113 20:05:05 @agent_ppo2.py:186][0m |          -0.0080 |           5.0761 |           5.8584 |
[32m[20230113 20:05:05 @agent_ppo2.py:186][0m |          -0.0090 |           4.8981 |           5.8528 |
[32m[20230113 20:05:05 @agent_ppo2.py:186][0m |          -0.0094 |           4.7400 |           5.8563 |
[32m[20230113 20:05:05 @agent_ppo2.py:186][0m |          -0.0100 |           4.6235 |           5.8555 |
[32m[20230113 20:05:05 @agent_ppo2.py:186][0m |          -0.0106 |           4.5207 |           5.8505 |
[32m[20230113 20:05:05 @agent_ppo2.py:186][0m |          -0.0112 |           4.4262 |           5.8587 |
[32m[20230113 20:05:05 @agent_ppo2.py:186][0m |          -0.0117 |           4.3461 |           5.8450 |
[32m[20230113 20:05:05 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:05:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.51
[32m[20230113 20:05:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.19
[32m[20230113 20:05:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.26
[32m[20230113 20:05:05 @agent_ppo2.py:144][0m Total time:      20.55 min
[32m[20230113 20:05:05 @agent_ppo2.py:146][0m 1865728 total steps have happened
[32m[20230113 20:05:05 @agent_ppo2.py:122][0m #------------------------ Iteration 911 --------------------------#
[32m[20230113 20:05:06 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:05:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:06 @agent_ppo2.py:186][0m |           0.0012 |           5.4958 |           6.0438 |
[32m[20230113 20:05:06 @agent_ppo2.py:186][0m |          -0.0032 |           4.6917 |           6.0389 |
[32m[20230113 20:05:06 @agent_ppo2.py:186][0m |          -0.0052 |           4.2628 |           6.0344 |
[32m[20230113 20:05:06 @agent_ppo2.py:186][0m |          -0.0071 |           4.0109 |           6.0293 |
[32m[20230113 20:05:06 @agent_ppo2.py:186][0m |          -0.0080 |           3.8265 |           6.0342 |
[32m[20230113 20:05:06 @agent_ppo2.py:186][0m |          -0.0086 |           3.6731 |           6.0261 |
[32m[20230113 20:05:06 @agent_ppo2.py:186][0m |          -0.0098 |           3.5385 |           6.0223 |
[32m[20230113 20:05:06 @agent_ppo2.py:186][0m |          -0.0106 |           3.4617 |           6.0257 |
[32m[20230113 20:05:06 @agent_ppo2.py:186][0m |          -0.0115 |           3.3460 |           6.0228 |
[32m[20230113 20:05:06 @agent_ppo2.py:186][0m |          -0.0121 |           3.2462 |           6.0204 |
[32m[20230113 20:05:06 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:05:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.33
[32m[20230113 20:05:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.75
[32m[20230113 20:05:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.87
[32m[20230113 20:05:07 @agent_ppo2.py:144][0m Total time:      20.57 min
[32m[20230113 20:05:07 @agent_ppo2.py:146][0m 1867776 total steps have happened
[32m[20230113 20:05:07 @agent_ppo2.py:122][0m #------------------------ Iteration 912 --------------------------#
[32m[20230113 20:05:07 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:05:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:07 @agent_ppo2.py:186][0m |          -0.0016 |           4.8669 |           5.7771 |
[32m[20230113 20:05:07 @agent_ppo2.py:186][0m |          -0.0183 |           4.3994 |           5.7645 |
[32m[20230113 20:05:07 @agent_ppo2.py:186][0m |          -0.0090 |           4.0966 |           5.7668 |
[32m[20230113 20:05:07 @agent_ppo2.py:186][0m |          -0.0068 |           3.8641 |           5.7655 |
[32m[20230113 20:05:07 @agent_ppo2.py:186][0m |           0.0035 |           3.7494 |           5.7635 |
[32m[20230113 20:05:07 @agent_ppo2.py:186][0m |          -0.0204 |           3.7278 |           5.7635 |
[32m[20230113 20:05:07 @agent_ppo2.py:186][0m |          -0.0333 |           3.6117 |           5.7667 |
[32m[20230113 20:05:07 @agent_ppo2.py:186][0m |           0.0036 |           3.5958 |           5.7485 |
[32m[20230113 20:05:07 @agent_ppo2.py:186][0m |          -0.0255 |           3.3876 |           5.7608 |
[32m[20230113 20:05:08 @agent_ppo2.py:186][0m |           0.0023 |           3.3395 |           5.7626 |
[32m[20230113 20:05:08 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:05:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.83
[32m[20230113 20:05:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.53
[32m[20230113 20:05:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.80
[32m[20230113 20:05:08 @agent_ppo2.py:144][0m Total time:      20.59 min
[32m[20230113 20:05:08 @agent_ppo2.py:146][0m 1869824 total steps have happened
[32m[20230113 20:05:08 @agent_ppo2.py:122][0m #------------------------ Iteration 913 --------------------------#
[32m[20230113 20:05:08 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:08 @agent_ppo2.py:186][0m |          -0.0006 |           7.9982 |           5.8270 |
[32m[20230113 20:05:09 @agent_ppo2.py:186][0m |          -0.0066 |           5.6276 |           5.8202 |
[32m[20230113 20:05:09 @agent_ppo2.py:186][0m |          -0.0102 |           4.8754 |           5.8174 |
[32m[20230113 20:05:09 @agent_ppo2.py:186][0m |          -0.0081 |           4.4771 |           5.8157 |
[32m[20230113 20:05:09 @agent_ppo2.py:186][0m |          -0.0074 |           4.1628 |           5.8165 |
[32m[20230113 20:05:09 @agent_ppo2.py:186][0m |          -0.0109 |           3.9951 |           5.8140 |
[32m[20230113 20:05:09 @agent_ppo2.py:186][0m |          -0.0113 |           3.7910 |           5.8135 |
[32m[20230113 20:05:09 @agent_ppo2.py:186][0m |          -0.0120 |           3.6861 |           5.8134 |
[32m[20230113 20:05:09 @agent_ppo2.py:186][0m |          -0.0122 |           3.5575 |           5.8133 |
[32m[20230113 20:05:09 @agent_ppo2.py:186][0m |          -0.0149 |           3.5002 |           5.8137 |
[32m[20230113 20:05:09 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:05:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.72
[32m[20230113 20:05:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.59
[32m[20230113 20:05:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.82
[32m[20230113 20:05:09 @agent_ppo2.py:144][0m Total time:      20.61 min
[32m[20230113 20:05:09 @agent_ppo2.py:146][0m 1871872 total steps have happened
[32m[20230113 20:05:09 @agent_ppo2.py:122][0m #------------------------ Iteration 914 --------------------------#
[32m[20230113 20:05:10 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:05:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:10 @agent_ppo2.py:186][0m |           0.0085 |           6.6812 |           5.9090 |
[32m[20230113 20:05:10 @agent_ppo2.py:186][0m |          -0.0052 |           5.0970 |           5.9042 |
[32m[20230113 20:05:10 @agent_ppo2.py:186][0m |          -0.0065 |           4.5813 |           5.9074 |
[32m[20230113 20:05:10 @agent_ppo2.py:186][0m |          -0.0072 |           4.2862 |           5.8990 |
[32m[20230113 20:05:10 @agent_ppo2.py:186][0m |          -0.0116 |           4.0825 |           5.8922 |
[32m[20230113 20:05:10 @agent_ppo2.py:186][0m |          -0.0089 |           3.9111 |           5.8936 |
[32m[20230113 20:05:10 @agent_ppo2.py:186][0m |          -0.0110 |           3.8620 |           5.8884 |
[32m[20230113 20:05:10 @agent_ppo2.py:186][0m |          -0.0088 |           3.7237 |           5.8929 |
[32m[20230113 20:05:10 @agent_ppo2.py:186][0m |          -0.0126 |           3.5850 |           5.8830 |
[32m[20230113 20:05:10 @agent_ppo2.py:186][0m |          -0.0108 |           3.5788 |           5.8887 |
[32m[20230113 20:05:10 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:05:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.57
[32m[20230113 20:05:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.43
[32m[20230113 20:05:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.07
[32m[20230113 20:05:11 @agent_ppo2.py:144][0m Total time:      20.63 min
[32m[20230113 20:05:11 @agent_ppo2.py:146][0m 1873920 total steps have happened
[32m[20230113 20:05:11 @agent_ppo2.py:122][0m #------------------------ Iteration 915 --------------------------#
[32m[20230113 20:05:11 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:11 @agent_ppo2.py:186][0m |           0.0002 |           5.8470 |           5.8454 |
[32m[20230113 20:05:11 @agent_ppo2.py:186][0m |          -0.0053 |           4.7585 |           5.8412 |
[32m[20230113 20:05:11 @agent_ppo2.py:186][0m |          -0.0071 |           4.2722 |           5.8385 |
[32m[20230113 20:05:11 @agent_ppo2.py:186][0m |          -0.0084 |           3.9819 |           5.8411 |
[32m[20230113 20:05:11 @agent_ppo2.py:186][0m |          -0.0098 |           3.7757 |           5.8337 |
[32m[20230113 20:05:11 @agent_ppo2.py:186][0m |          -0.0100 |           3.5541 |           5.8415 |
[32m[20230113 20:05:11 @agent_ppo2.py:186][0m |          -0.0101 |           3.4662 |           5.8393 |
[32m[20230113 20:05:11 @agent_ppo2.py:186][0m |          -0.0116 |           3.3448 |           5.8384 |
[32m[20230113 20:05:11 @agent_ppo2.py:186][0m |          -0.0119 |           3.2686 |           5.8440 |
[32m[20230113 20:05:11 @agent_ppo2.py:186][0m |          -0.0136 |           3.1678 |           5.8435 |
[32m[20230113 20:05:11 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:05:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.95
[32m[20230113 20:05:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.99
[32m[20230113 20:05:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.00
[32m[20230113 20:05:12 @agent_ppo2.py:144][0m Total time:      20.66 min
[32m[20230113 20:05:12 @agent_ppo2.py:146][0m 1875968 total steps have happened
[32m[20230113 20:05:12 @agent_ppo2.py:122][0m #------------------------ Iteration 916 --------------------------#
[32m[20230113 20:05:12 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:05:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:12 @agent_ppo2.py:186][0m |          -0.0011 |           5.7587 |           5.7303 |
[32m[20230113 20:05:13 @agent_ppo2.py:186][0m |           0.0052 |           4.4464 |           5.7214 |
[32m[20230113 20:05:13 @agent_ppo2.py:186][0m |          -0.0005 |           3.9566 |           5.7172 |
[32m[20230113 20:05:13 @agent_ppo2.py:186][0m |          -0.0076 |           3.6572 |           5.7141 |
[32m[20230113 20:05:13 @agent_ppo2.py:186][0m |          -0.0150 |           3.4208 |           5.7113 |
[32m[20230113 20:05:13 @agent_ppo2.py:186][0m |          -0.0147 |           3.2731 |           5.7084 |
[32m[20230113 20:05:13 @agent_ppo2.py:186][0m |          -0.0169 |           3.1129 |           5.7140 |
[32m[20230113 20:05:13 @agent_ppo2.py:186][0m |          -0.0122 |           3.0014 |           5.7089 |
[32m[20230113 20:05:13 @agent_ppo2.py:186][0m |          -0.0140 |           2.9337 |           5.7104 |
[32m[20230113 20:05:13 @agent_ppo2.py:186][0m |          -0.0111 |           2.8717 |           5.7096 |
[32m[20230113 20:05:13 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:05:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.24
[32m[20230113 20:05:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.30
[32m[20230113 20:05:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.62
[32m[20230113 20:05:13 @agent_ppo2.py:144][0m Total time:      20.68 min
[32m[20230113 20:05:13 @agent_ppo2.py:146][0m 1878016 total steps have happened
[32m[20230113 20:05:13 @agent_ppo2.py:122][0m #------------------------ Iteration 917 --------------------------#
[32m[20230113 20:05:14 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:05:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:14 @agent_ppo2.py:186][0m |          -0.0010 |           6.0325 |           5.8971 |
[32m[20230113 20:05:14 @agent_ppo2.py:186][0m |          -0.0058 |           4.8867 |           5.8884 |
[32m[20230113 20:05:14 @agent_ppo2.py:186][0m |          -0.0079 |           4.4900 |           5.8920 |
[32m[20230113 20:05:14 @agent_ppo2.py:186][0m |          -0.0110 |           4.2086 |           5.8834 |
[32m[20230113 20:05:14 @agent_ppo2.py:186][0m |          -0.0110 |           4.0202 |           5.8826 |
[32m[20230113 20:05:14 @agent_ppo2.py:186][0m |          -0.0132 |           3.8280 |           5.8835 |
[32m[20230113 20:05:14 @agent_ppo2.py:186][0m |          -0.0132 |           3.6756 |           5.8828 |
[32m[20230113 20:05:14 @agent_ppo2.py:186][0m |          -0.0132 |           3.5517 |           5.8907 |
[32m[20230113 20:05:14 @agent_ppo2.py:186][0m |          -0.0141 |           3.4238 |           5.8854 |
[32m[20230113 20:05:14 @agent_ppo2.py:186][0m |          -0.0148 |           3.3468 |           5.8905 |
[32m[20230113 20:05:14 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:05:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.36
[32m[20230113 20:05:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.84
[32m[20230113 20:05:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.43
[32m[20230113 20:05:15 @agent_ppo2.py:144][0m Total time:      20.70 min
[32m[20230113 20:05:15 @agent_ppo2.py:146][0m 1880064 total steps have happened
[32m[20230113 20:05:15 @agent_ppo2.py:122][0m #------------------------ Iteration 918 --------------------------#
[32m[20230113 20:05:15 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:15 @agent_ppo2.py:186][0m |           0.0006 |           5.6108 |           5.8035 |
[32m[20230113 20:05:15 @agent_ppo2.py:186][0m |          -0.0039 |           4.8941 |           5.8045 |
[32m[20230113 20:05:15 @agent_ppo2.py:186][0m |          -0.0065 |           4.3683 |           5.8001 |
[32m[20230113 20:05:15 @agent_ppo2.py:186][0m |          -0.0076 |           4.0829 |           5.7982 |
[32m[20230113 20:05:15 @agent_ppo2.py:186][0m |          -0.0088 |           3.8705 |           5.8004 |
[32m[20230113 20:05:15 @agent_ppo2.py:186][0m |          -0.0096 |           3.7174 |           5.7999 |
[32m[20230113 20:05:15 @agent_ppo2.py:186][0m |          -0.0091 |           3.5721 |           5.8024 |
[32m[20230113 20:05:15 @agent_ppo2.py:186][0m |          -0.0098 |           3.4608 |           5.8042 |
[32m[20230113 20:05:15 @agent_ppo2.py:186][0m |          -0.0104 |           3.3688 |           5.8059 |
[32m[20230113 20:05:16 @agent_ppo2.py:186][0m |          -0.0117 |           3.2693 |           5.8053 |
[32m[20230113 20:05:16 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:05:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.86
[32m[20230113 20:05:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.96
[32m[20230113 20:05:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.56
[32m[20230113 20:05:16 @agent_ppo2.py:144][0m Total time:      20.72 min
[32m[20230113 20:05:16 @agent_ppo2.py:146][0m 1882112 total steps have happened
[32m[20230113 20:05:16 @agent_ppo2.py:122][0m #------------------------ Iteration 919 --------------------------#
[32m[20230113 20:05:16 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:05:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:16 @agent_ppo2.py:186][0m |           0.0034 |           6.6114 |           5.9216 |
[32m[20230113 20:05:16 @agent_ppo2.py:186][0m |          -0.0040 |           5.2189 |           5.9121 |
[32m[20230113 20:05:17 @agent_ppo2.py:186][0m |          -0.0075 |           4.7646 |           5.9147 |
[32m[20230113 20:05:17 @agent_ppo2.py:186][0m |          -0.0084 |           4.4617 |           5.9164 |
[32m[20230113 20:05:17 @agent_ppo2.py:186][0m |          -0.0050 |           4.2950 |           5.9161 |
[32m[20230113 20:05:17 @agent_ppo2.py:186][0m |          -0.0091 |           4.0611 |           5.9194 |
[32m[20230113 20:05:17 @agent_ppo2.py:186][0m |          -0.0087 |           3.9372 |           5.9178 |
[32m[20230113 20:05:17 @agent_ppo2.py:186][0m |          -0.0109 |           3.7969 |           5.9178 |
[32m[20230113 20:05:17 @agent_ppo2.py:186][0m |          -0.0111 |           3.7333 |           5.9165 |
[32m[20230113 20:05:17 @agent_ppo2.py:186][0m |          -0.0123 |           3.5819 |           5.9207 |
[32m[20230113 20:05:17 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:05:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.14
[32m[20230113 20:05:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.39
[32m[20230113 20:05:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.40
[32m[20230113 20:05:17 @agent_ppo2.py:144][0m Total time:      20.74 min
[32m[20230113 20:05:17 @agent_ppo2.py:146][0m 1884160 total steps have happened
[32m[20230113 20:05:17 @agent_ppo2.py:122][0m #------------------------ Iteration 920 --------------------------#
[32m[20230113 20:05:18 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:18 @agent_ppo2.py:186][0m |           0.0001 |           5.8139 |           5.8431 |
[32m[20230113 20:05:18 @agent_ppo2.py:186][0m |          -0.0116 |           4.7904 |           5.8365 |
[32m[20230113 20:05:18 @agent_ppo2.py:186][0m |          -0.0114 |           4.3284 |           5.8300 |
[32m[20230113 20:05:18 @agent_ppo2.py:186][0m |          -0.0159 |           4.0244 |           5.8335 |
[32m[20230113 20:05:18 @agent_ppo2.py:186][0m |          -0.0157 |           3.8553 |           5.8298 |
[32m[20230113 20:05:18 @agent_ppo2.py:186][0m |          -0.0166 |           3.7789 |           5.8290 |
[32m[20230113 20:05:18 @agent_ppo2.py:186][0m |          -0.0152 |           3.5900 |           5.8246 |
[32m[20230113 20:05:18 @agent_ppo2.py:186][0m |          -0.0137 |           3.4196 |           5.8258 |
[32m[20230113 20:05:18 @agent_ppo2.py:186][0m |          -0.0182 |           3.3277 |           5.8282 |
[32m[20230113 20:05:18 @agent_ppo2.py:186][0m |          -0.0156 |           3.2480 |           5.8260 |
[32m[20230113 20:05:18 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:05:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.43
[32m[20230113 20:05:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.86
[32m[20230113 20:05:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.42
[32m[20230113 20:05:19 @agent_ppo2.py:144][0m Total time:      20.77 min
[32m[20230113 20:05:19 @agent_ppo2.py:146][0m 1886208 total steps have happened
[32m[20230113 20:05:19 @agent_ppo2.py:122][0m #------------------------ Iteration 921 --------------------------#
[32m[20230113 20:05:19 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:05:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:19 @agent_ppo2.py:186][0m |          -0.0011 |           5.6439 |           5.8312 |
[32m[20230113 20:05:19 @agent_ppo2.py:186][0m |          -0.0054 |           4.6356 |           5.8231 |
[32m[20230113 20:05:19 @agent_ppo2.py:186][0m |          -0.0072 |           4.2090 |           5.8146 |
[32m[20230113 20:05:19 @agent_ppo2.py:186][0m |          -0.0090 |           3.9853 |           5.8162 |
[32m[20230113 20:05:19 @agent_ppo2.py:186][0m |          -0.0106 |           3.7848 |           5.8109 |
[32m[20230113 20:05:19 @agent_ppo2.py:186][0m |          -0.0110 |           3.6517 |           5.8128 |
[32m[20230113 20:05:19 @agent_ppo2.py:186][0m |          -0.0126 |           3.5398 |           5.8133 |
[32m[20230113 20:05:19 @agent_ppo2.py:186][0m |          -0.0139 |           3.4154 |           5.8089 |
[32m[20230113 20:05:19 @agent_ppo2.py:186][0m |          -0.0140 |           3.3700 |           5.8091 |
[32m[20230113 20:05:19 @agent_ppo2.py:186][0m |          -0.0133 |           3.2826 |           5.8123 |
[32m[20230113 20:05:19 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:05:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.58
[32m[20230113 20:05:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.94
[32m[20230113 20:05:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.19
[32m[20230113 20:05:20 @agent_ppo2.py:144][0m Total time:      20.79 min
[32m[20230113 20:05:20 @agent_ppo2.py:146][0m 1888256 total steps have happened
[32m[20230113 20:05:20 @agent_ppo2.py:122][0m #------------------------ Iteration 922 --------------------------#
[32m[20230113 20:05:20 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:20 @agent_ppo2.py:186][0m |          -0.0000 |           5.8202 |           5.9098 |
[32m[20230113 20:05:20 @agent_ppo2.py:186][0m |          -0.0040 |           4.8440 |           5.9004 |
[32m[20230113 20:05:21 @agent_ppo2.py:186][0m |          -0.0066 |           4.4998 |           5.9018 |
[32m[20230113 20:05:21 @agent_ppo2.py:186][0m |          -0.0082 |           4.2778 |           5.9024 |
[32m[20230113 20:05:21 @agent_ppo2.py:186][0m |          -0.0090 |           4.0385 |           5.9024 |
[32m[20230113 20:05:21 @agent_ppo2.py:186][0m |          -0.0100 |           3.9796 |           5.9043 |
[32m[20230113 20:05:21 @agent_ppo2.py:186][0m |          -0.0115 |           3.7496 |           5.9045 |
[32m[20230113 20:05:21 @agent_ppo2.py:186][0m |          -0.0120 |           3.6497 |           5.9061 |
[32m[20230113 20:05:21 @agent_ppo2.py:186][0m |          -0.0128 |           3.5485 |           5.9079 |
[32m[20230113 20:05:21 @agent_ppo2.py:186][0m |          -0.0127 |           3.5014 |           5.9066 |
[32m[20230113 20:05:21 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:05:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.79
[32m[20230113 20:05:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.54
[32m[20230113 20:05:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.59
[32m[20230113 20:05:21 @agent_ppo2.py:144][0m Total time:      20.81 min
[32m[20230113 20:05:21 @agent_ppo2.py:146][0m 1890304 total steps have happened
[32m[20230113 20:05:21 @agent_ppo2.py:122][0m #------------------------ Iteration 923 --------------------------#
[32m[20230113 20:05:22 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:05:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:22 @agent_ppo2.py:186][0m |           0.0008 |           6.3766 |           6.0596 |
[32m[20230113 20:05:22 @agent_ppo2.py:186][0m |          -0.0028 |           4.8887 |           6.0606 |
[32m[20230113 20:05:22 @agent_ppo2.py:186][0m |          -0.0052 |           4.3654 |           6.0536 |
[32m[20230113 20:05:22 @agent_ppo2.py:186][0m |          -0.0070 |           4.0001 |           6.0537 |
[32m[20230113 20:05:22 @agent_ppo2.py:186][0m |          -0.0084 |           3.7716 |           6.0453 |
[32m[20230113 20:05:22 @agent_ppo2.py:186][0m |          -0.0097 |           3.5561 |           6.0490 |
[32m[20230113 20:05:22 @agent_ppo2.py:186][0m |          -0.0107 |           3.3766 |           6.0467 |
[32m[20230113 20:05:22 @agent_ppo2.py:186][0m |          -0.0116 |           3.2204 |           6.0440 |
[32m[20230113 20:05:22 @agent_ppo2.py:186][0m |          -0.0125 |           3.1093 |           6.0428 |
[32m[20230113 20:05:22 @agent_ppo2.py:186][0m |          -0.0117 |           3.0237 |           6.0432 |
[32m[20230113 20:05:22 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:05:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.37
[32m[20230113 20:05:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.75
[32m[20230113 20:05:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.89
[32m[20230113 20:05:23 @agent_ppo2.py:144][0m Total time:      20.83 min
[32m[20230113 20:05:23 @agent_ppo2.py:146][0m 1892352 total steps have happened
[32m[20230113 20:05:23 @agent_ppo2.py:122][0m #------------------------ Iteration 924 --------------------------#
[32m[20230113 20:05:23 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:05:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:23 @agent_ppo2.py:186][0m |           0.0024 |           6.3584 |           5.9570 |
[32m[20230113 20:05:23 @agent_ppo2.py:186][0m |          -0.0058 |           4.8933 |           5.9569 |
[32m[20230113 20:05:23 @agent_ppo2.py:186][0m |          -0.0065 |           4.3942 |           5.9611 |
[32m[20230113 20:05:23 @agent_ppo2.py:186][0m |          -0.0083 |           4.0668 |           5.9550 |
[32m[20230113 20:05:23 @agent_ppo2.py:186][0m |          -0.0114 |           3.8400 |           5.9570 |
[32m[20230113 20:05:23 @agent_ppo2.py:186][0m |          -0.0111 |           3.6117 |           5.9547 |
[32m[20230113 20:05:23 @agent_ppo2.py:186][0m |          -0.0108 |           3.4580 |           5.9555 |
[32m[20230113 20:05:23 @agent_ppo2.py:186][0m |          -0.0112 |           3.3657 |           5.9524 |
[32m[20230113 20:05:23 @agent_ppo2.py:186][0m |          -0.0134 |           3.2427 |           5.9547 |
[32m[20230113 20:05:23 @agent_ppo2.py:186][0m |          -0.0137 |           3.0495 |           5.9553 |
[32m[20230113 20:05:23 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:05:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.07
[32m[20230113 20:05:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.41
[32m[20230113 20:05:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.98
[32m[20230113 20:05:24 @agent_ppo2.py:144][0m Total time:      20.86 min
[32m[20230113 20:05:24 @agent_ppo2.py:146][0m 1894400 total steps have happened
[32m[20230113 20:05:24 @agent_ppo2.py:122][0m #------------------------ Iteration 925 --------------------------#
[32m[20230113 20:05:24 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:05:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:24 @agent_ppo2.py:186][0m |           0.0020 |          10.2790 |           6.1172 |
[32m[20230113 20:05:24 @agent_ppo2.py:186][0m |          -0.0051 |           4.6860 |           6.1076 |
[32m[20230113 20:05:24 @agent_ppo2.py:186][0m |          -0.0099 |           3.8892 |           6.1047 |
[32m[20230113 20:05:25 @agent_ppo2.py:186][0m |          -0.0119 |           3.5554 |           6.0996 |
[32m[20230113 20:05:25 @agent_ppo2.py:186][0m |          -0.0114 |           3.3103 |           6.0927 |
[32m[20230113 20:05:25 @agent_ppo2.py:186][0m |          -0.0149 |           3.0595 |           6.0934 |
[32m[20230113 20:05:25 @agent_ppo2.py:186][0m |          -0.0148 |           2.9057 |           6.0881 |
[32m[20230113 20:05:25 @agent_ppo2.py:186][0m |          -0.0147 |           2.7631 |           6.0892 |
[32m[20230113 20:05:25 @agent_ppo2.py:186][0m |          -0.0173 |           2.6431 |           6.0850 |
[32m[20230113 20:05:25 @agent_ppo2.py:186][0m |          -0.0174 |           2.5516 |           6.0844 |
[32m[20230113 20:05:25 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:05:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 163.37
[32m[20230113 20:05:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.84
[32m[20230113 20:05:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.38
[32m[20230113 20:05:25 @agent_ppo2.py:144][0m Total time:      20.88 min
[32m[20230113 20:05:25 @agent_ppo2.py:146][0m 1896448 total steps have happened
[32m[20230113 20:05:25 @agent_ppo2.py:122][0m #------------------------ Iteration 926 --------------------------#
[32m[20230113 20:05:26 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:05:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:26 @agent_ppo2.py:186][0m |           0.0024 |           5.7034 |           6.0209 |
[32m[20230113 20:05:26 @agent_ppo2.py:186][0m |          -0.0037 |           4.4805 |           6.0066 |
[32m[20230113 20:05:26 @agent_ppo2.py:186][0m |          -0.0057 |           4.0865 |           6.0041 |
[32m[20230113 20:05:26 @agent_ppo2.py:186][0m |          -0.0121 |           3.8094 |           5.9976 |
[32m[20230113 20:05:26 @agent_ppo2.py:186][0m |          -0.0120 |           3.5993 |           5.9902 |
[32m[20230113 20:05:26 @agent_ppo2.py:186][0m |          -0.0145 |           3.5601 |           5.9906 |
[32m[20230113 20:05:26 @agent_ppo2.py:186][0m |          -0.0134 |           3.3690 |           5.9927 |
[32m[20230113 20:05:26 @agent_ppo2.py:186][0m |          -0.0151 |           3.2810 |           5.9887 |
[32m[20230113 20:05:26 @agent_ppo2.py:186][0m |          -0.0130 |           3.2392 |           5.9881 |
[32m[20230113 20:05:26 @agent_ppo2.py:186][0m |          -0.0101 |           3.1584 |           5.9884 |
[32m[20230113 20:05:26 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:05:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.85
[32m[20230113 20:05:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.19
[32m[20230113 20:05:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.03
[32m[20230113 20:05:27 @agent_ppo2.py:144][0m Total time:      20.90 min
[32m[20230113 20:05:27 @agent_ppo2.py:146][0m 1898496 total steps have happened
[32m[20230113 20:05:27 @agent_ppo2.py:122][0m #------------------------ Iteration 927 --------------------------#
[32m[20230113 20:05:27 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:05:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:27 @agent_ppo2.py:186][0m |           0.0003 |           6.0225 |           5.8740 |
[32m[20230113 20:05:27 @agent_ppo2.py:186][0m |          -0.0074 |           4.8883 |           5.8853 |
[32m[20230113 20:05:27 @agent_ppo2.py:186][0m |          -0.0109 |           4.4094 |           5.8807 |
[32m[20230113 20:05:27 @agent_ppo2.py:186][0m |          -0.0122 |           4.0795 |           5.8764 |
[32m[20230113 20:05:27 @agent_ppo2.py:186][0m |          -0.0118 |           3.8838 |           5.8736 |
[32m[20230113 20:05:27 @agent_ppo2.py:186][0m |          -0.0112 |           3.7416 |           5.8724 |
[32m[20230113 20:05:27 @agent_ppo2.py:186][0m |          -0.0139 |           3.5856 |           5.8720 |
[32m[20230113 20:05:27 @agent_ppo2.py:186][0m |          -0.0137 |           3.4718 |           5.8709 |
[32m[20230113 20:05:27 @agent_ppo2.py:186][0m |          -0.0138 |           3.4029 |           5.8726 |
[32m[20230113 20:05:27 @agent_ppo2.py:186][0m |          -0.0160 |           3.2942 |           5.8757 |
[32m[20230113 20:05:27 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:05:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.27
[32m[20230113 20:05:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.29
[32m[20230113 20:05:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.63
[32m[20230113 20:05:28 @agent_ppo2.py:144][0m Total time:      20.92 min
[32m[20230113 20:05:28 @agent_ppo2.py:146][0m 1900544 total steps have happened
[32m[20230113 20:05:28 @agent_ppo2.py:122][0m #------------------------ Iteration 928 --------------------------#
[32m[20230113 20:05:28 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:28 @agent_ppo2.py:186][0m |          -0.0020 |           6.7909 |           5.9362 |
[32m[20230113 20:05:28 @agent_ppo2.py:186][0m |          -0.0050 |           4.7942 |           5.9259 |
[32m[20230113 20:05:29 @agent_ppo2.py:186][0m |          -0.0073 |           4.1666 |           5.9240 |
[32m[20230113 20:05:29 @agent_ppo2.py:186][0m |          -0.0079 |           3.7402 |           5.9261 |
[32m[20230113 20:05:29 @agent_ppo2.py:186][0m |          -0.0127 |           3.4387 |           5.9275 |
[32m[20230113 20:05:29 @agent_ppo2.py:186][0m |          -0.0136 |           3.2406 |           5.9284 |
[32m[20230113 20:05:29 @agent_ppo2.py:186][0m |          -0.0138 |           3.1174 |           5.9300 |
[32m[20230113 20:05:29 @agent_ppo2.py:186][0m |          -0.0147 |           2.9208 |           5.9284 |
[32m[20230113 20:05:29 @agent_ppo2.py:186][0m |          -0.0159 |           2.8283 |           5.9288 |
[32m[20230113 20:05:29 @agent_ppo2.py:186][0m |          -0.0143 |           2.7971 |           5.9313 |
[32m[20230113 20:05:29 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:05:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.72
[32m[20230113 20:05:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.15
[32m[20230113 20:05:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.30
[32m[20230113 20:05:29 @agent_ppo2.py:144][0m Total time:      20.94 min
[32m[20230113 20:05:29 @agent_ppo2.py:146][0m 1902592 total steps have happened
[32m[20230113 20:05:29 @agent_ppo2.py:122][0m #------------------------ Iteration 929 --------------------------#
[32m[20230113 20:05:30 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:05:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:30 @agent_ppo2.py:186][0m |           0.0004 |           5.7861 |           6.0288 |
[32m[20230113 20:05:30 @agent_ppo2.py:186][0m |          -0.0054 |           4.4454 |           6.0164 |
[32m[20230113 20:05:30 @agent_ppo2.py:186][0m |          -0.0077 |           3.9393 |           6.0193 |
[32m[20230113 20:05:30 @agent_ppo2.py:186][0m |          -0.0081 |           3.7770 |           6.0177 |
[32m[20230113 20:05:30 @agent_ppo2.py:186][0m |          -0.0066 |           3.5711 |           6.0228 |
[32m[20230113 20:05:30 @agent_ppo2.py:186][0m |          -0.0104 |           3.3872 |           6.0226 |
[32m[20230113 20:05:30 @agent_ppo2.py:186][0m |          -0.0114 |           3.2307 |           6.0216 |
[32m[20230113 20:05:30 @agent_ppo2.py:186][0m |          -0.0100 |           3.2051 |           6.0262 |
[32m[20230113 20:05:30 @agent_ppo2.py:186][0m |          -0.0112 |           3.0628 |           6.0208 |
[32m[20230113 20:05:30 @agent_ppo2.py:186][0m |          -0.0139 |           2.9823 |           6.0227 |
[32m[20230113 20:05:30 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:05:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.93
[32m[20230113 20:05:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.10
[32m[20230113 20:05:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.46
[32m[20230113 20:05:31 @agent_ppo2.py:144][0m Total time:      20.97 min
[32m[20230113 20:05:31 @agent_ppo2.py:146][0m 1904640 total steps have happened
[32m[20230113 20:05:31 @agent_ppo2.py:122][0m #------------------------ Iteration 930 --------------------------#
[32m[20230113 20:05:31 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:05:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:31 @agent_ppo2.py:186][0m |           0.0021 |           5.7711 |           6.0146 |
[32m[20230113 20:05:31 @agent_ppo2.py:186][0m |          -0.0061 |           4.3536 |           6.0057 |
[32m[20230113 20:05:31 @agent_ppo2.py:186][0m |          -0.0087 |           3.7547 |           6.0083 |
[32m[20230113 20:05:31 @agent_ppo2.py:186][0m |          -0.0101 |           3.4300 |           6.0059 |
[32m[20230113 20:05:31 @agent_ppo2.py:186][0m |          -0.0118 |           3.1921 |           6.0056 |
[32m[20230113 20:05:31 @agent_ppo2.py:186][0m |          -0.0142 |           2.9983 |           6.0015 |
[32m[20230113 20:05:31 @agent_ppo2.py:186][0m |          -0.0145 |           2.8697 |           6.0005 |
[32m[20230113 20:05:31 @agent_ppo2.py:186][0m |          -0.0152 |           2.7337 |           6.0018 |
[32m[20230113 20:05:31 @agent_ppo2.py:186][0m |          -0.0155 |           2.6415 |           5.9988 |
[32m[20230113 20:05:31 @agent_ppo2.py:186][0m |          -0.0167 |           2.5563 |           5.9951 |
[32m[20230113 20:05:31 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:05:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.03
[32m[20230113 20:05:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.82
[32m[20230113 20:05:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.99
[32m[20230113 20:05:32 @agent_ppo2.py:144][0m Total time:      20.99 min
[32m[20230113 20:05:32 @agent_ppo2.py:146][0m 1906688 total steps have happened
[32m[20230113 20:05:32 @agent_ppo2.py:122][0m #------------------------ Iteration 931 --------------------------#
[32m[20230113 20:05:32 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 20:05:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:32 @agent_ppo2.py:186][0m |           0.0017 |          15.8012 |           6.0060 |
[32m[20230113 20:05:32 @agent_ppo2.py:186][0m |          -0.0073 |           6.7794 |           5.9989 |
[32m[20230113 20:05:32 @agent_ppo2.py:186][0m |          -0.0091 |           5.0636 |           5.9912 |
[32m[20230113 20:05:32 @agent_ppo2.py:186][0m |          -0.0121 |           4.3654 |           5.9896 |
[32m[20230113 20:05:32 @agent_ppo2.py:186][0m |          -0.0136 |           3.8256 |           5.9873 |
[32m[20230113 20:05:32 @agent_ppo2.py:186][0m |          -0.0115 |           3.4306 |           5.9874 |
[32m[20230113 20:05:33 @agent_ppo2.py:186][0m |          -0.0155 |           3.2086 |           5.9857 |
[32m[20230113 20:05:33 @agent_ppo2.py:186][0m |          -0.0153 |           3.0116 |           5.9866 |
[32m[20230113 20:05:33 @agent_ppo2.py:186][0m |          -0.0137 |           2.8069 |           5.9837 |
[32m[20230113 20:05:33 @agent_ppo2.py:186][0m |          -0.0080 |           2.6736 |           5.9809 |
[32m[20230113 20:05:33 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 20:05:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 131.61
[32m[20230113 20:05:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.76
[32m[20230113 20:05:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.76
[32m[20230113 20:05:33 @agent_ppo2.py:144][0m Total time:      21.01 min
[32m[20230113 20:05:33 @agent_ppo2.py:146][0m 1908736 total steps have happened
[32m[20230113 20:05:33 @agent_ppo2.py:122][0m #------------------------ Iteration 932 --------------------------#
[32m[20230113 20:05:33 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:05:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:34 @agent_ppo2.py:186][0m |          -0.0028 |           9.4685 |           5.8740 |
[32m[20230113 20:05:34 @agent_ppo2.py:186][0m |          -0.0054 |           7.0234 |           5.8717 |
[32m[20230113 20:05:34 @agent_ppo2.py:186][0m |          -0.0045 |           6.0414 |           5.8638 |
[32m[20230113 20:05:34 @agent_ppo2.py:186][0m |          -0.0085 |           5.2523 |           5.8652 |
[32m[20230113 20:05:34 @agent_ppo2.py:186][0m |          -0.0006 |           5.3217 |           5.8598 |
[32m[20230113 20:05:34 @agent_ppo2.py:186][0m |          -0.0135 |           4.5611 |           5.8532 |
[32m[20230113 20:05:34 @agent_ppo2.py:186][0m |          -0.0156 |           4.2983 |           5.8555 |
[32m[20230113 20:05:34 @agent_ppo2.py:186][0m |          -0.0158 |           4.0929 |           5.8564 |
[32m[20230113 20:05:34 @agent_ppo2.py:186][0m |          -0.0147 |           3.9298 |           5.8553 |
[32m[20230113 20:05:34 @agent_ppo2.py:186][0m |          -0.0156 |           3.7460 |           5.8526 |
[32m[20230113 20:05:34 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:05:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.83
[32m[20230113 20:05:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.59
[32m[20230113 20:05:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.93
[32m[20230113 20:05:34 @agent_ppo2.py:144][0m Total time:      21.03 min
[32m[20230113 20:05:34 @agent_ppo2.py:146][0m 1910784 total steps have happened
[32m[20230113 20:05:34 @agent_ppo2.py:122][0m #------------------------ Iteration 933 --------------------------#
[32m[20230113 20:05:35 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:05:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:35 @agent_ppo2.py:186][0m |          -0.0008 |           6.6476 |           5.7518 |
[32m[20230113 20:05:35 @agent_ppo2.py:186][0m |           0.0000 |           4.7147 |           5.7401 |
[32m[20230113 20:05:35 @agent_ppo2.py:186][0m |          -0.0032 |           4.3305 |           5.7457 |
[32m[20230113 20:05:35 @agent_ppo2.py:186][0m |          -0.0070 |           3.9148 |           5.7421 |
[32m[20230113 20:05:35 @agent_ppo2.py:186][0m |          -0.0164 |           3.6703 |           5.7467 |
[32m[20230113 20:05:35 @agent_ppo2.py:186][0m |          -0.0112 |           3.3842 |           5.7456 |
[32m[20230113 20:05:35 @agent_ppo2.py:186][0m |          -0.0134 |           3.3159 |           5.7501 |
[32m[20230113 20:05:35 @agent_ppo2.py:186][0m |          -0.0158 |           3.1759 |           5.7528 |
[32m[20230113 20:05:35 @agent_ppo2.py:186][0m |          -0.0154 |           2.9909 |           5.7520 |
[32m[20230113 20:05:35 @agent_ppo2.py:186][0m |          -0.0121 |           2.9228 |           5.7548 |
[32m[20230113 20:05:35 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:05:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.25
[32m[20230113 20:05:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.81
[32m[20230113 20:05:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.48
[32m[20230113 20:05:36 @agent_ppo2.py:144][0m Total time:      21.05 min
[32m[20230113 20:05:36 @agent_ppo2.py:146][0m 1912832 total steps have happened
[32m[20230113 20:05:36 @agent_ppo2.py:122][0m #------------------------ Iteration 934 --------------------------#
[32m[20230113 20:05:36 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:05:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:36 @agent_ppo2.py:186][0m |           0.0012 |           6.2514 |           6.0045 |
[32m[20230113 20:05:36 @agent_ppo2.py:186][0m |          -0.0043 |           5.0058 |           5.9814 |
[32m[20230113 20:05:36 @agent_ppo2.py:186][0m |          -0.0109 |           4.5792 |           5.9703 |
[32m[20230113 20:05:36 @agent_ppo2.py:186][0m |          -0.0114 |           4.3015 |           5.9912 |
[32m[20230113 20:05:36 @agent_ppo2.py:186][0m |          -0.0123 |           4.0626 |           5.9749 |
[32m[20230113 20:05:36 @agent_ppo2.py:186][0m |          -0.0148 |           3.9886 |           5.9857 |
[32m[20230113 20:05:36 @agent_ppo2.py:186][0m |          -0.0129 |           3.7641 |           5.9777 |
[32m[20230113 20:05:37 @agent_ppo2.py:186][0m |          -0.0093 |           3.6487 |           5.9785 |
[32m[20230113 20:05:37 @agent_ppo2.py:186][0m |          -0.0114 |           3.5385 |           5.9831 |
[32m[20230113 20:05:37 @agent_ppo2.py:186][0m |          -0.0191 |           3.4765 |           5.9818 |
[32m[20230113 20:05:37 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:05:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.76
[32m[20230113 20:05:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.40
[32m[20230113 20:05:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.57
[32m[20230113 20:05:37 @agent_ppo2.py:144][0m Total time:      21.07 min
[32m[20230113 20:05:37 @agent_ppo2.py:146][0m 1914880 total steps have happened
[32m[20230113 20:05:37 @agent_ppo2.py:122][0m #------------------------ Iteration 935 --------------------------#
[32m[20230113 20:05:37 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:38 @agent_ppo2.py:186][0m |           0.0020 |           4.8864 |           6.0666 |
[32m[20230113 20:05:38 @agent_ppo2.py:186][0m |          -0.0048 |           4.2001 |           6.0633 |
[32m[20230113 20:05:38 @agent_ppo2.py:186][0m |          -0.0071 |           3.8942 |           6.0527 |
[32m[20230113 20:05:38 @agent_ppo2.py:186][0m |          -0.0081 |           3.7492 |           6.0543 |
[32m[20230113 20:05:38 @agent_ppo2.py:186][0m |          -0.0085 |           3.5712 |           6.0500 |
[32m[20230113 20:05:38 @agent_ppo2.py:186][0m |          -0.0117 |           3.4659 |           6.0523 |
[32m[20230113 20:05:38 @agent_ppo2.py:186][0m |          -0.0113 |           3.3587 |           6.0477 |
[32m[20230113 20:05:38 @agent_ppo2.py:186][0m |          -0.0122 |           3.3171 |           6.0492 |
[32m[20230113 20:05:38 @agent_ppo2.py:186][0m |          -0.0121 |           3.2046 |           6.0508 |
[32m[20230113 20:05:38 @agent_ppo2.py:186][0m |          -0.0103 |           3.2081 |           6.0421 |
[32m[20230113 20:05:38 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:05:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.71
[32m[20230113 20:05:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.59
[32m[20230113 20:05:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.80
[32m[20230113 20:05:38 @agent_ppo2.py:144][0m Total time:      21.10 min
[32m[20230113 20:05:38 @agent_ppo2.py:146][0m 1916928 total steps have happened
[32m[20230113 20:05:38 @agent_ppo2.py:122][0m #------------------------ Iteration 936 --------------------------#
[32m[20230113 20:05:39 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 20:05:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:39 @agent_ppo2.py:186][0m |          -0.0001 |           9.8079 |           5.9912 |
[32m[20230113 20:05:39 @agent_ppo2.py:186][0m |          -0.0013 |           4.7016 |           5.9845 |
[32m[20230113 20:05:39 @agent_ppo2.py:186][0m |          -0.0062 |           4.2600 |           5.9783 |
[32m[20230113 20:05:39 @agent_ppo2.py:186][0m |          -0.0088 |           3.9272 |           5.9787 |
[32m[20230113 20:05:39 @agent_ppo2.py:186][0m |          -0.0083 |           3.7374 |           5.9778 |
[32m[20230113 20:05:39 @agent_ppo2.py:186][0m |          -0.0079 |           3.5986 |           5.9705 |
[32m[20230113 20:05:39 @agent_ppo2.py:186][0m |          -0.0097 |           3.4234 |           5.9721 |
[32m[20230113 20:05:39 @agent_ppo2.py:186][0m |          -0.0111 |           3.2984 |           5.9678 |
[32m[20230113 20:05:39 @agent_ppo2.py:186][0m |          -0.0110 |           3.2080 |           5.9701 |
[32m[20230113 20:05:39 @agent_ppo2.py:186][0m |          -0.0128 |           3.1691 |           5.9649 |
[32m[20230113 20:05:39 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 20:05:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 140.73
[32m[20230113 20:05:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.61
[32m[20230113 20:05:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.90
[32m[20230113 20:05:40 @agent_ppo2.py:144][0m Total time:      21.12 min
[32m[20230113 20:05:40 @agent_ppo2.py:146][0m 1918976 total steps have happened
[32m[20230113 20:05:40 @agent_ppo2.py:122][0m #------------------------ Iteration 937 --------------------------#
[32m[20230113 20:05:40 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:05:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:40 @agent_ppo2.py:186][0m |          -0.0012 |           5.9667 |           6.1931 |
[32m[20230113 20:05:40 @agent_ppo2.py:186][0m |          -0.0080 |           4.8469 |           6.1814 |
[32m[20230113 20:05:40 @agent_ppo2.py:186][0m |          -0.0104 |           4.3665 |           6.1832 |
[32m[20230113 20:05:40 @agent_ppo2.py:186][0m |          -0.0114 |           4.0343 |           6.1772 |
[32m[20230113 20:05:40 @agent_ppo2.py:186][0m |          -0.0137 |           3.8324 |           6.1751 |
[32m[20230113 20:05:41 @agent_ppo2.py:186][0m |          -0.0124 |           3.6332 |           6.1728 |
[32m[20230113 20:05:41 @agent_ppo2.py:186][0m |          -0.0144 |           3.4644 |           6.1725 |
[32m[20230113 20:05:41 @agent_ppo2.py:186][0m |          -0.0152 |           3.3765 |           6.1742 |
[32m[20230113 20:05:41 @agent_ppo2.py:186][0m |          -0.0157 |           3.2942 |           6.1657 |
[32m[20230113 20:05:41 @agent_ppo2.py:186][0m |          -0.0163 |           3.1869 |           6.1650 |
[32m[20230113 20:05:41 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:05:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.39
[32m[20230113 20:05:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.84
[32m[20230113 20:05:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.36
[32m[20230113 20:05:41 @agent_ppo2.py:144][0m Total time:      21.14 min
[32m[20230113 20:05:41 @agent_ppo2.py:146][0m 1921024 total steps have happened
[32m[20230113 20:05:41 @agent_ppo2.py:122][0m #------------------------ Iteration 938 --------------------------#
[32m[20230113 20:05:42 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:05:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:42 @agent_ppo2.py:186][0m |           0.0044 |           5.4615 |           6.1371 |
[32m[20230113 20:05:42 @agent_ppo2.py:186][0m |          -0.0032 |           4.1968 |           6.1396 |
[32m[20230113 20:05:42 @agent_ppo2.py:186][0m |          -0.0099 |           3.7627 |           6.1399 |
[32m[20230113 20:05:42 @agent_ppo2.py:186][0m |          -0.0043 |           3.7992 |           6.1414 |
[32m[20230113 20:05:42 @agent_ppo2.py:186][0m |          -0.0094 |           3.3615 |           6.1372 |
[32m[20230113 20:05:42 @agent_ppo2.py:186][0m |          -0.0056 |           3.2967 |           6.1381 |
[32m[20230113 20:05:42 @agent_ppo2.py:186][0m |          -0.0140 |           3.0866 |           6.1377 |
[32m[20230113 20:05:42 @agent_ppo2.py:186][0m |          -0.0130 |           2.9764 |           6.1409 |
[32m[20230113 20:05:42 @agent_ppo2.py:186][0m |          -0.0144 |           2.8972 |           6.1388 |
[32m[20230113 20:05:42 @agent_ppo2.py:186][0m |          -0.0195 |           2.8221 |           6.1393 |
[32m[20230113 20:05:42 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:05:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.68
[32m[20230113 20:05:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.20
[32m[20230113 20:05:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.96
[32m[20230113 20:05:42 @agent_ppo2.py:144][0m Total time:      21.16 min
[32m[20230113 20:05:42 @agent_ppo2.py:146][0m 1923072 total steps have happened
[32m[20230113 20:05:42 @agent_ppo2.py:122][0m #------------------------ Iteration 939 --------------------------#
[32m[20230113 20:05:43 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:05:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:43 @agent_ppo2.py:186][0m |           0.0018 |           5.5538 |           6.0549 |
[32m[20230113 20:05:43 @agent_ppo2.py:186][0m |          -0.0051 |           4.8638 |           6.0502 |
[32m[20230113 20:05:43 @agent_ppo2.py:186][0m |          -0.0137 |           4.6595 |           6.0488 |
[32m[20230113 20:05:43 @agent_ppo2.py:186][0m |          -0.0089 |           4.3489 |           6.0454 |
[32m[20230113 20:05:43 @agent_ppo2.py:186][0m |          -0.0077 |           4.2220 |           6.0380 |
[32m[20230113 20:05:43 @agent_ppo2.py:186][0m |          -0.0122 |           4.1061 |           6.0398 |
[32m[20230113 20:05:43 @agent_ppo2.py:186][0m |          -0.0137 |           3.9529 |           6.0360 |
[32m[20230113 20:05:43 @agent_ppo2.py:186][0m |          -0.0103 |           3.8672 |           6.0330 |
[32m[20230113 20:05:43 @agent_ppo2.py:186][0m |          -0.0106 |           3.7868 |           6.0329 |
[32m[20230113 20:05:43 @agent_ppo2.py:186][0m |          -0.0123 |           3.6789 |           6.0328 |
[32m[20230113 20:05:43 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:05:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.05
[32m[20230113 20:05:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.27
[32m[20230113 20:05:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.40
[32m[20230113 20:05:44 @agent_ppo2.py:144][0m Total time:      21.18 min
[32m[20230113 20:05:44 @agent_ppo2.py:146][0m 1925120 total steps have happened
[32m[20230113 20:05:44 @agent_ppo2.py:122][0m #------------------------ Iteration 940 --------------------------#
[32m[20230113 20:05:44 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:05:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:44 @agent_ppo2.py:186][0m |          -0.0030 |           5.5925 |           5.9676 |
[32m[20230113 20:05:44 @agent_ppo2.py:186][0m |          -0.0113 |           4.6063 |           5.9566 |
[32m[20230113 20:05:44 @agent_ppo2.py:186][0m |          -0.0162 |           4.2696 |           5.9628 |
[32m[20230113 20:05:44 @agent_ppo2.py:186][0m |          -0.0043 |           3.9532 |           5.9589 |
[32m[20230113 20:05:44 @agent_ppo2.py:186][0m |          -0.0138 |           3.7291 |           5.9644 |
[32m[20230113 20:05:44 @agent_ppo2.py:186][0m |          -0.0111 |           3.5602 |           5.9703 |
[32m[20230113 20:05:44 @agent_ppo2.py:186][0m |          -0.0097 |           3.5053 |           5.9641 |
[32m[20230113 20:05:44 @agent_ppo2.py:186][0m |          -0.0150 |           3.2906 |           5.9704 |
[32m[20230113 20:05:44 @agent_ppo2.py:186][0m |          -0.0149 |           3.2278 |           5.9702 |
[32m[20230113 20:05:45 @agent_ppo2.py:186][0m |          -0.0203 |           3.1713 |           5.9697 |
[32m[20230113 20:05:45 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:05:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.46
[32m[20230113 20:05:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.61
[32m[20230113 20:05:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.07
[32m[20230113 20:05:45 @agent_ppo2.py:144][0m Total time:      21.21 min
[32m[20230113 20:05:45 @agent_ppo2.py:146][0m 1927168 total steps have happened
[32m[20230113 20:05:45 @agent_ppo2.py:122][0m #------------------------ Iteration 941 --------------------------#
[32m[20230113 20:05:45 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:45 @agent_ppo2.py:186][0m |           0.0004 |           6.8792 |           6.1756 |
[32m[20230113 20:05:46 @agent_ppo2.py:186][0m |          -0.0043 |           5.0284 |           6.1720 |
[32m[20230113 20:05:46 @agent_ppo2.py:186][0m |          -0.0064 |           4.3358 |           6.1707 |
[32m[20230113 20:05:46 @agent_ppo2.py:186][0m |          -0.0072 |           3.8976 |           6.1753 |
[32m[20230113 20:05:46 @agent_ppo2.py:186][0m |          -0.0088 |           3.6579 |           6.1750 |
[32m[20230113 20:05:46 @agent_ppo2.py:186][0m |          -0.0088 |           3.4750 |           6.1784 |
[32m[20230113 20:05:46 @agent_ppo2.py:186][0m |          -0.0092 |           3.3096 |           6.1740 |
[32m[20230113 20:05:46 @agent_ppo2.py:186][0m |          -0.0096 |           3.1712 |           6.1733 |
[32m[20230113 20:05:46 @agent_ppo2.py:186][0m |          -0.0105 |           3.0580 |           6.1758 |
[32m[20230113 20:05:46 @agent_ppo2.py:186][0m |          -0.0105 |           2.9846 |           6.1757 |
[32m[20230113 20:05:46 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:05:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.79
[32m[20230113 20:05:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.55
[32m[20230113 20:05:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.64
[32m[20230113 20:05:46 @agent_ppo2.py:144][0m Total time:      21.23 min
[32m[20230113 20:05:46 @agent_ppo2.py:146][0m 1929216 total steps have happened
[32m[20230113 20:05:46 @agent_ppo2.py:122][0m #------------------------ Iteration 942 --------------------------#
[32m[20230113 20:05:47 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:05:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:47 @agent_ppo2.py:186][0m |          -0.0002 |           5.9825 |           6.0564 |
[32m[20230113 20:05:47 @agent_ppo2.py:186][0m |          -0.0060 |           5.0166 |           6.0498 |
[32m[20230113 20:05:47 @agent_ppo2.py:186][0m |          -0.0094 |           4.6169 |           6.0465 |
[32m[20230113 20:05:47 @agent_ppo2.py:186][0m |          -0.0082 |           4.2696 |           6.0481 |
[32m[20230113 20:05:47 @agent_ppo2.py:186][0m |          -0.0091 |           4.1398 |           6.0430 |
[32m[20230113 20:05:47 @agent_ppo2.py:186][0m |          -0.0142 |           3.9885 |           6.0463 |
[32m[20230113 20:05:47 @agent_ppo2.py:186][0m |          -0.0080 |           3.9214 |           6.0413 |
[32m[20230113 20:05:47 @agent_ppo2.py:186][0m |          -0.0085 |           3.8629 |           6.0469 |
[32m[20230113 20:05:47 @agent_ppo2.py:186][0m |          -0.0124 |           3.7913 |           6.0411 |
[32m[20230113 20:05:47 @agent_ppo2.py:186][0m |          -0.0105 |           3.7710 |           6.0477 |
[32m[20230113 20:05:47 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:05:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.97
[32m[20230113 20:05:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.18
[32m[20230113 20:05:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.69
[32m[20230113 20:05:48 @agent_ppo2.py:144][0m Total time:      21.25 min
[32m[20230113 20:05:48 @agent_ppo2.py:146][0m 1931264 total steps have happened
[32m[20230113 20:05:48 @agent_ppo2.py:122][0m #------------------------ Iteration 943 --------------------------#
[32m[20230113 20:05:48 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:48 @agent_ppo2.py:186][0m |           0.0001 |           5.7069 |           6.1654 |
[32m[20230113 20:05:48 @agent_ppo2.py:186][0m |          -0.0049 |           4.0334 |           6.1528 |
[32m[20230113 20:05:48 @agent_ppo2.py:186][0m |          -0.0098 |           3.4687 |           6.1524 |
[32m[20230113 20:05:48 @agent_ppo2.py:186][0m |          -0.0102 |           3.1542 |           6.1474 |
[32m[20230113 20:05:48 @agent_ppo2.py:186][0m |          -0.0112 |           2.9404 |           6.1428 |
[32m[20230113 20:05:48 @agent_ppo2.py:186][0m |          -0.0114 |           2.7476 |           6.1361 |
[32m[20230113 20:05:48 @agent_ppo2.py:186][0m |          -0.0137 |           2.5957 |           6.1391 |
[32m[20230113 20:05:48 @agent_ppo2.py:186][0m |          -0.0097 |           2.5546 |           6.1347 |
[32m[20230113 20:05:48 @agent_ppo2.py:186][0m |          -0.0139 |           2.4199 |           6.1355 |
[32m[20230113 20:05:48 @agent_ppo2.py:186][0m |          -0.0135 |           2.3490 |           6.1351 |
[32m[20230113 20:05:48 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:05:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.51
[32m[20230113 20:05:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.14
[32m[20230113 20:05:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.84
[32m[20230113 20:05:49 @agent_ppo2.py:144][0m Total time:      21.27 min
[32m[20230113 20:05:49 @agent_ppo2.py:146][0m 1933312 total steps have happened
[32m[20230113 20:05:49 @agent_ppo2.py:122][0m #------------------------ Iteration 944 --------------------------#
[32m[20230113 20:05:49 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:49 @agent_ppo2.py:186][0m |          -0.0013 |           6.5480 |           6.0115 |
[32m[20230113 20:05:49 @agent_ppo2.py:186][0m |          -0.0007 |           5.0583 |           6.0034 |
[32m[20230113 20:05:50 @agent_ppo2.py:186][0m |          -0.0088 |           4.3540 |           5.9960 |
[32m[20230113 20:05:50 @agent_ppo2.py:186][0m |          -0.0103 |           3.9177 |           6.0039 |
[32m[20230113 20:05:50 @agent_ppo2.py:186][0m |          -0.0123 |           3.7442 |           6.0057 |
[32m[20230113 20:05:50 @agent_ppo2.py:186][0m |          -0.0120 |           3.5713 |           6.0099 |
[32m[20230113 20:05:50 @agent_ppo2.py:186][0m |          -0.0113 |           3.4409 |           6.0060 |
[32m[20230113 20:05:50 @agent_ppo2.py:186][0m |          -0.0146 |           3.3144 |           6.0094 |
[32m[20230113 20:05:50 @agent_ppo2.py:186][0m |          -0.0160 |           3.2121 |           6.0105 |
[32m[20230113 20:05:50 @agent_ppo2.py:186][0m |          -0.0150 |           3.1474 |           6.0122 |
[32m[20230113 20:05:50 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:05:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.12
[32m[20230113 20:05:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.09
[32m[20230113 20:05:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.66
[32m[20230113 20:05:50 @agent_ppo2.py:144][0m Total time:      21.29 min
[32m[20230113 20:05:50 @agent_ppo2.py:146][0m 1935360 total steps have happened
[32m[20230113 20:05:50 @agent_ppo2.py:122][0m #------------------------ Iteration 945 --------------------------#
[32m[20230113 20:05:51 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:51 @agent_ppo2.py:186][0m |          -0.0013 |           5.7733 |           6.1426 |
[32m[20230113 20:05:51 @agent_ppo2.py:186][0m |          -0.0043 |           4.6154 |           6.1401 |
[32m[20230113 20:05:51 @agent_ppo2.py:186][0m |          -0.0102 |           4.2051 |           6.1346 |
[32m[20230113 20:05:51 @agent_ppo2.py:186][0m |          -0.0117 |           3.9304 |           6.1360 |
[32m[20230113 20:05:51 @agent_ppo2.py:186][0m |          -0.0109 |           3.6508 |           6.1379 |
[32m[20230113 20:05:51 @agent_ppo2.py:186][0m |          -0.0134 |           3.5492 |           6.1364 |
[32m[20230113 20:05:51 @agent_ppo2.py:186][0m |          -0.0140 |           3.4414 |           6.1310 |
[32m[20230113 20:05:51 @agent_ppo2.py:186][0m |          -0.0131 |           3.3427 |           6.1335 |
[32m[20230113 20:05:51 @agent_ppo2.py:186][0m |          -0.0153 |           3.2229 |           6.1326 |
[32m[20230113 20:05:51 @agent_ppo2.py:186][0m |          -0.0167 |           3.1947 |           6.1295 |
[32m[20230113 20:05:51 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:05:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.89
[32m[20230113 20:05:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.70
[32m[20230113 20:05:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.18
[32m[20230113 20:05:51 @agent_ppo2.py:144][0m Total time:      21.32 min
[32m[20230113 20:05:51 @agent_ppo2.py:146][0m 1937408 total steps have happened
[32m[20230113 20:05:51 @agent_ppo2.py:122][0m #------------------------ Iteration 946 --------------------------#
[32m[20230113 20:05:52 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:52 @agent_ppo2.py:186][0m |          -0.0012 |           5.5350 |           6.2717 |
[32m[20230113 20:05:52 @agent_ppo2.py:186][0m |          -0.0049 |           4.5628 |           6.2499 |
[32m[20230113 20:05:52 @agent_ppo2.py:186][0m |          -0.0104 |           4.0509 |           6.2478 |
[32m[20230113 20:05:52 @agent_ppo2.py:186][0m |          -0.0151 |           3.7183 |           6.2499 |
[32m[20230113 20:05:52 @agent_ppo2.py:186][0m |          -0.0126 |           3.4850 |           6.2466 |
[32m[20230113 20:05:52 @agent_ppo2.py:186][0m |          -0.0122 |           3.3346 |           6.2560 |
[32m[20230113 20:05:52 @agent_ppo2.py:186][0m |          -0.0155 |           3.1885 |           6.2474 |
[32m[20230113 20:05:52 @agent_ppo2.py:186][0m |          -0.0114 |           3.1846 |           6.2541 |
[32m[20230113 20:05:52 @agent_ppo2.py:186][0m |          -0.0160 |           3.0588 |           6.2494 |
[32m[20230113 20:05:52 @agent_ppo2.py:186][0m |          -0.0148 |           2.8974 |           6.2488 |
[32m[20230113 20:05:52 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:05:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.61
[32m[20230113 20:05:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.81
[32m[20230113 20:05:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.76
[32m[20230113 20:05:53 @agent_ppo2.py:144][0m Total time:      21.34 min
[32m[20230113 20:05:53 @agent_ppo2.py:146][0m 1939456 total steps have happened
[32m[20230113 20:05:53 @agent_ppo2.py:122][0m #------------------------ Iteration 947 --------------------------#
[32m[20230113 20:05:53 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:05:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:53 @agent_ppo2.py:186][0m |          -0.0031 |           6.5676 |           6.2452 |
[32m[20230113 20:05:53 @agent_ppo2.py:186][0m |          -0.0058 |           5.6744 |           6.2390 |
[32m[20230113 20:05:53 @agent_ppo2.py:186][0m |          -0.0122 |           4.9929 |           6.2359 |
[32m[20230113 20:05:53 @agent_ppo2.py:186][0m |          -0.0089 |           4.7050 |           6.2379 |
[32m[20230113 20:05:53 @agent_ppo2.py:186][0m |          -0.0094 |           4.3599 |           6.2328 |
[32m[20230113 20:05:54 @agent_ppo2.py:186][0m |          -0.0140 |           4.1117 |           6.2347 |
[32m[20230113 20:05:54 @agent_ppo2.py:186][0m |          -0.0120 |           3.9213 |           6.2328 |
[32m[20230113 20:05:54 @agent_ppo2.py:186][0m |          -0.0189 |           3.7673 |           6.2365 |
[32m[20230113 20:05:54 @agent_ppo2.py:186][0m |          -0.0181 |           3.6255 |           6.2348 |
[32m[20230113 20:05:54 @agent_ppo2.py:186][0m |          -0.0121 |           3.5407 |           6.2378 |
[32m[20230113 20:05:54 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:05:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.24
[32m[20230113 20:05:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.08
[32m[20230113 20:05:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.30
[32m[20230113 20:05:54 @agent_ppo2.py:144][0m Total time:      21.36 min
[32m[20230113 20:05:54 @agent_ppo2.py:146][0m 1941504 total steps have happened
[32m[20230113 20:05:54 @agent_ppo2.py:122][0m #------------------------ Iteration 948 --------------------------#
[32m[20230113 20:05:55 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:05:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:55 @agent_ppo2.py:186][0m |          -0.0011 |           6.3453 |           6.2126 |
[32m[20230113 20:05:55 @agent_ppo2.py:186][0m |          -0.0065 |           5.0549 |           6.1891 |
[32m[20230113 20:05:55 @agent_ppo2.py:186][0m |          -0.0071 |           4.5826 |           6.1916 |
[32m[20230113 20:05:55 @agent_ppo2.py:186][0m |          -0.0110 |           4.2401 |           6.1891 |
[32m[20230113 20:05:55 @agent_ppo2.py:186][0m |          -0.0094 |           4.0325 |           6.1850 |
[32m[20230113 20:05:55 @agent_ppo2.py:186][0m |          -0.0121 |           3.8256 |           6.1839 |
[32m[20230113 20:05:55 @agent_ppo2.py:186][0m |          -0.0138 |           3.6814 |           6.1779 |
[32m[20230113 20:05:55 @agent_ppo2.py:186][0m |          -0.0117 |           3.5423 |           6.1770 |
[32m[20230113 20:05:55 @agent_ppo2.py:186][0m |          -0.0135 |           3.4635 |           6.1744 |
[32m[20230113 20:05:55 @agent_ppo2.py:186][0m |          -0.0135 |           3.3763 |           6.1767 |
[32m[20230113 20:05:55 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:05:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.10
[32m[20230113 20:05:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.16
[32m[20230113 20:05:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.65
[32m[20230113 20:05:55 @agent_ppo2.py:144][0m Total time:      21.38 min
[32m[20230113 20:05:55 @agent_ppo2.py:146][0m 1943552 total steps have happened
[32m[20230113 20:05:55 @agent_ppo2.py:122][0m #------------------------ Iteration 949 --------------------------#
[32m[20230113 20:05:56 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:05:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:56 @agent_ppo2.py:186][0m |           0.0052 |           6.1183 |           6.2084 |
[32m[20230113 20:05:56 @agent_ppo2.py:186][0m |          -0.0075 |           4.8288 |           6.1949 |
[32m[20230113 20:05:56 @agent_ppo2.py:186][0m |          -0.0119 |           4.3866 |           6.1913 |
[32m[20230113 20:05:56 @agent_ppo2.py:186][0m |          -0.0080 |           4.1960 |           6.1939 |
[32m[20230113 20:05:56 @agent_ppo2.py:186][0m |          -0.0115 |           4.0309 |           6.1942 |
[32m[20230113 20:05:56 @agent_ppo2.py:186][0m |           0.0072 |           4.5409 |           6.1956 |
[32m[20230113 20:05:56 @agent_ppo2.py:186][0m |          -0.0075 |           3.9130 |           6.1854 |
[32m[20230113 20:05:56 @agent_ppo2.py:186][0m |          -0.0101 |           3.6915 |           6.1973 |
[32m[20230113 20:05:56 @agent_ppo2.py:186][0m |          -0.0060 |           3.5955 |           6.1955 |
[32m[20230113 20:05:56 @agent_ppo2.py:186][0m |          -0.0073 |           3.5173 |           6.1981 |
[32m[20230113 20:05:56 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:05:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 220.71
[32m[20230113 20:05:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.06
[32m[20230113 20:05:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.29
[32m[20230113 20:05:57 @agent_ppo2.py:144][0m Total time:      21.40 min
[32m[20230113 20:05:57 @agent_ppo2.py:146][0m 1945600 total steps have happened
[32m[20230113 20:05:57 @agent_ppo2.py:122][0m #------------------------ Iteration 950 --------------------------#
[32m[20230113 20:05:57 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:05:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:57 @agent_ppo2.py:186][0m |          -0.0011 |           5.2818 |           6.2882 |
[32m[20230113 20:05:57 @agent_ppo2.py:186][0m |          -0.0043 |           4.4456 |           6.2652 |
[32m[20230113 20:05:57 @agent_ppo2.py:186][0m |          -0.0060 |           4.0686 |           6.2761 |
[32m[20230113 20:05:57 @agent_ppo2.py:186][0m |          -0.0065 |           3.7881 |           6.2669 |
[32m[20230113 20:05:57 @agent_ppo2.py:186][0m |          -0.0078 |           3.6466 |           6.2705 |
[32m[20230113 20:05:57 @agent_ppo2.py:186][0m |          -0.0095 |           3.4664 |           6.2693 |
[32m[20230113 20:05:58 @agent_ppo2.py:186][0m |          -0.0118 |           3.3944 |           6.2690 |
[32m[20230113 20:05:58 @agent_ppo2.py:186][0m |          -0.0104 |           3.2586 |           6.2735 |
[32m[20230113 20:05:58 @agent_ppo2.py:186][0m |          -0.0061 |           3.2611 |           6.2704 |
[32m[20230113 20:05:58 @agent_ppo2.py:186][0m |          -0.0082 |           3.1086 |           6.2711 |
[32m[20230113 20:05:58 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:05:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.70
[32m[20230113 20:05:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.26
[32m[20230113 20:05:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.77
[32m[20230113 20:05:58 @agent_ppo2.py:144][0m Total time:      21.42 min
[32m[20230113 20:05:58 @agent_ppo2.py:146][0m 1947648 total steps have happened
[32m[20230113 20:05:58 @agent_ppo2.py:122][0m #------------------------ Iteration 951 --------------------------#
[32m[20230113 20:05:58 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:05:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:05:59 @agent_ppo2.py:186][0m |           0.0002 |           6.6304 |           6.4217 |
[32m[20230113 20:05:59 @agent_ppo2.py:186][0m |          -0.0040 |           4.6350 |           6.4115 |
[32m[20230113 20:05:59 @agent_ppo2.py:186][0m |          -0.0100 |           3.9637 |           6.4080 |
[32m[20230113 20:05:59 @agent_ppo2.py:186][0m |          -0.0113 |           3.6508 |           6.4072 |
[32m[20230113 20:05:59 @agent_ppo2.py:186][0m |          -0.0111 |           3.4525 |           6.4078 |
[32m[20230113 20:05:59 @agent_ppo2.py:186][0m |          -0.0104 |           3.3253 |           6.4043 |
[32m[20230113 20:05:59 @agent_ppo2.py:186][0m |          -0.0123 |           3.1553 |           6.4030 |
[32m[20230113 20:05:59 @agent_ppo2.py:186][0m |          -0.0152 |           3.0410 |           6.3997 |
[32m[20230113 20:05:59 @agent_ppo2.py:186][0m |          -0.0118 |           2.9817 |           6.4000 |
[32m[20230113 20:05:59 @agent_ppo2.py:186][0m |          -0.0137 |           2.8492 |           6.3989 |
[32m[20230113 20:05:59 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:05:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.59
[32m[20230113 20:05:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.91
[32m[20230113 20:05:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.33
[32m[20230113 20:05:59 @agent_ppo2.py:144][0m Total time:      21.45 min
[32m[20230113 20:05:59 @agent_ppo2.py:146][0m 1949696 total steps have happened
[32m[20230113 20:05:59 @agent_ppo2.py:122][0m #------------------------ Iteration 952 --------------------------#
[32m[20230113 20:06:00 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:00 @agent_ppo2.py:186][0m |           0.0010 |           6.3808 |           6.3054 |
[32m[20230113 20:06:00 @agent_ppo2.py:186][0m |          -0.0187 |           5.0446 |           6.2926 |
[32m[20230113 20:06:00 @agent_ppo2.py:186][0m |          -0.0164 |           4.5277 |           6.2925 |
[32m[20230113 20:06:00 @agent_ppo2.py:186][0m |          -0.0140 |           4.0420 |           6.2929 |
[32m[20230113 20:06:00 @agent_ppo2.py:186][0m |          -0.0193 |           3.7875 |           6.2923 |
[32m[20230113 20:06:00 @agent_ppo2.py:186][0m |          -0.0107 |           3.6067 |           6.2904 |
[32m[20230113 20:06:00 @agent_ppo2.py:186][0m |          -0.0094 |           3.4525 |           6.2859 |
[32m[20230113 20:06:00 @agent_ppo2.py:186][0m |          -0.0141 |           3.3047 |           6.2893 |
[32m[20230113 20:06:00 @agent_ppo2.py:186][0m |          -0.0101 |           3.1995 |           6.2885 |
[32m[20230113 20:06:00 @agent_ppo2.py:186][0m |          -0.0170 |           3.0513 |           6.2888 |
[32m[20230113 20:06:00 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.82
[32m[20230113 20:06:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.88
[32m[20230113 20:06:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.53
[32m[20230113 20:06:01 @agent_ppo2.py:144][0m Total time:      21.47 min
[32m[20230113 20:06:01 @agent_ppo2.py:146][0m 1951744 total steps have happened
[32m[20230113 20:06:01 @agent_ppo2.py:122][0m #------------------------ Iteration 953 --------------------------#
[32m[20230113 20:06:01 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:06:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:01 @agent_ppo2.py:186][0m |          -0.0014 |           6.5800 |           6.2439 |
[32m[20230113 20:06:01 @agent_ppo2.py:186][0m |          -0.0025 |           5.0425 |           6.2417 |
[32m[20230113 20:06:01 @agent_ppo2.py:186][0m |          -0.0058 |           4.4742 |           6.2312 |
[32m[20230113 20:06:01 @agent_ppo2.py:186][0m |          -0.0060 |           4.2005 |           6.2345 |
[32m[20230113 20:06:01 @agent_ppo2.py:186][0m |          -0.0074 |           4.0345 |           6.2315 |
[32m[20230113 20:06:01 @agent_ppo2.py:186][0m |          -0.0063 |           3.9269 |           6.2267 |
[32m[20230113 20:06:01 @agent_ppo2.py:186][0m |          -0.0064 |           3.8153 |           6.2272 |
[32m[20230113 20:06:01 @agent_ppo2.py:186][0m |          -0.0091 |           3.6564 |           6.2233 |
[32m[20230113 20:06:01 @agent_ppo2.py:186][0m |          -0.0100 |           3.5799 |           6.2216 |
[32m[20230113 20:06:02 @agent_ppo2.py:186][0m |          -0.0103 |           3.5520 |           6.2218 |
[32m[20230113 20:06:02 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:06:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.97
[32m[20230113 20:06:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.04
[32m[20230113 20:06:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.29
[32m[20230113 20:06:02 @agent_ppo2.py:144][0m Total time:      21.49 min
[32m[20230113 20:06:02 @agent_ppo2.py:146][0m 1953792 total steps have happened
[32m[20230113 20:06:02 @agent_ppo2.py:122][0m #------------------------ Iteration 954 --------------------------#
[32m[20230113 20:06:02 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:06:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:02 @agent_ppo2.py:186][0m |           0.0107 |           6.7380 |           6.2101 |
[32m[20230113 20:06:03 @agent_ppo2.py:186][0m |           0.0006 |           4.7074 |           6.2051 |
[32m[20230113 20:06:03 @agent_ppo2.py:186][0m |          -0.0045 |           3.9504 |           6.2121 |
[32m[20230113 20:06:03 @agent_ppo2.py:186][0m |          -0.0123 |           3.5104 |           6.2114 |
[32m[20230113 20:06:03 @agent_ppo2.py:186][0m |          -0.0092 |           3.2281 |           6.2125 |
[32m[20230113 20:06:03 @agent_ppo2.py:186][0m |          -0.0135 |           3.0136 |           6.2155 |
[32m[20230113 20:06:03 @agent_ppo2.py:186][0m |          -0.0150 |           2.8401 |           6.2056 |
[32m[20230113 20:06:03 @agent_ppo2.py:186][0m |          -0.0069 |           2.7268 |           6.2118 |
[32m[20230113 20:06:03 @agent_ppo2.py:186][0m |          -0.0164 |           2.6080 |           6.2046 |
[32m[20230113 20:06:03 @agent_ppo2.py:186][0m |          -0.0154 |           2.5308 |           6.2108 |
[32m[20230113 20:06:03 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:06:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.57
[32m[20230113 20:06:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.00
[32m[20230113 20:06:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.17
[32m[20230113 20:06:03 @agent_ppo2.py:144][0m Total time:      21.51 min
[32m[20230113 20:06:03 @agent_ppo2.py:146][0m 1955840 total steps have happened
[32m[20230113 20:06:03 @agent_ppo2.py:122][0m #------------------------ Iteration 955 --------------------------#
[32m[20230113 20:06:04 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:04 @agent_ppo2.py:186][0m |           0.0004 |           5.0791 |           6.4647 |
[32m[20230113 20:06:04 @agent_ppo2.py:186][0m |          -0.0055 |           3.9431 |           6.4552 |
[32m[20230113 20:06:04 @agent_ppo2.py:186][0m |          -0.0077 |           3.6342 |           6.4489 |
[32m[20230113 20:06:04 @agent_ppo2.py:186][0m |          -0.0094 |           3.4394 |           6.4470 |
[32m[20230113 20:06:04 @agent_ppo2.py:186][0m |          -0.0108 |           3.3344 |           6.4465 |
[32m[20230113 20:06:04 @agent_ppo2.py:186][0m |          -0.0106 |           3.1672 |           6.4475 |
[32m[20230113 20:06:04 @agent_ppo2.py:186][0m |          -0.0114 |           3.0797 |           6.4475 |
[32m[20230113 20:06:04 @agent_ppo2.py:186][0m |          -0.0133 |           2.9822 |           6.4497 |
[32m[20230113 20:06:04 @agent_ppo2.py:186][0m |          -0.0133 |           2.9100 |           6.4481 |
[32m[20230113 20:06:04 @agent_ppo2.py:186][0m |          -0.0138 |           2.8474 |           6.4518 |
[32m[20230113 20:06:04 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.50
[32m[20230113 20:06:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.93
[32m[20230113 20:06:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.67
[32m[20230113 20:06:05 @agent_ppo2.py:144][0m Total time:      21.53 min
[32m[20230113 20:06:05 @agent_ppo2.py:146][0m 1957888 total steps have happened
[32m[20230113 20:06:05 @agent_ppo2.py:122][0m #------------------------ Iteration 956 --------------------------#
[32m[20230113 20:06:05 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:05 @agent_ppo2.py:186][0m |           0.0002 |           5.0861 |           6.3594 |
[32m[20230113 20:06:05 @agent_ppo2.py:186][0m |          -0.0064 |           4.1299 |           6.3547 |
[32m[20230113 20:06:05 @agent_ppo2.py:186][0m |          -0.0091 |           3.7753 |           6.3522 |
[32m[20230113 20:06:05 @agent_ppo2.py:186][0m |          -0.0094 |           3.4921 |           6.3463 |
[32m[20230113 20:06:05 @agent_ppo2.py:186][0m |          -0.0119 |           3.3035 |           6.3465 |
[32m[20230113 20:06:05 @agent_ppo2.py:186][0m |          -0.0126 |           3.1471 |           6.3444 |
[32m[20230113 20:06:05 @agent_ppo2.py:186][0m |          -0.0138 |           3.0361 |           6.3416 |
[32m[20230113 20:06:05 @agent_ppo2.py:186][0m |          -0.0145 |           2.8926 |           6.3433 |
[32m[20230113 20:06:05 @agent_ppo2.py:186][0m |          -0.0141 |           2.8152 |           6.3439 |
[32m[20230113 20:06:05 @agent_ppo2.py:186][0m |          -0.0142 |           2.7263 |           6.3434 |
[32m[20230113 20:06:05 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.21
[32m[20230113 20:06:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.24
[32m[20230113 20:06:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.88
[32m[20230113 20:06:06 @agent_ppo2.py:144][0m Total time:      21.55 min
[32m[20230113 20:06:06 @agent_ppo2.py:146][0m 1959936 total steps have happened
[32m[20230113 20:06:06 @agent_ppo2.py:122][0m #------------------------ Iteration 957 --------------------------#
[32m[20230113 20:06:06 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:06 @agent_ppo2.py:186][0m |           0.0016 |           5.7773 |           6.3071 |
[32m[20230113 20:06:06 @agent_ppo2.py:186][0m |          -0.0092 |           4.3640 |           6.3017 |
[32m[20230113 20:06:06 @agent_ppo2.py:186][0m |          -0.0081 |           3.9063 |           6.2938 |
[32m[20230113 20:06:07 @agent_ppo2.py:186][0m |          -0.0124 |           3.5786 |           6.2914 |
[32m[20230113 20:06:07 @agent_ppo2.py:186][0m |          -0.0117 |           3.3755 |           6.2857 |
[32m[20230113 20:06:07 @agent_ppo2.py:186][0m |          -0.0145 |           3.1919 |           6.2927 |
[32m[20230113 20:06:07 @agent_ppo2.py:186][0m |          -0.0137 |           3.0526 |           6.2917 |
[32m[20230113 20:06:07 @agent_ppo2.py:186][0m |          -0.0173 |           2.9332 |           6.2872 |
[32m[20230113 20:06:07 @agent_ppo2.py:186][0m |          -0.0151 |           2.8295 |           6.2853 |
[32m[20230113 20:06:07 @agent_ppo2.py:186][0m |          -0.0161 |           2.7861 |           6.2888 |
[32m[20230113 20:06:07 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:06:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.92
[32m[20230113 20:06:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.09
[32m[20230113 20:06:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.41
[32m[20230113 20:06:07 @agent_ppo2.py:144][0m Total time:      21.58 min
[32m[20230113 20:06:07 @agent_ppo2.py:146][0m 1961984 total steps have happened
[32m[20230113 20:06:07 @agent_ppo2.py:122][0m #------------------------ Iteration 958 --------------------------#
[32m[20230113 20:06:08 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:06:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:08 @agent_ppo2.py:186][0m |           0.0011 |           6.4946 |           6.1600 |
[32m[20230113 20:06:08 @agent_ppo2.py:186][0m |          -0.0080 |           5.4649 |           6.1552 |
[32m[20230113 20:06:08 @agent_ppo2.py:186][0m |          -0.0053 |           5.2209 |           6.1501 |
[32m[20230113 20:06:08 @agent_ppo2.py:186][0m |          -0.0115 |           4.8374 |           6.1467 |
[32m[20230113 20:06:08 @agent_ppo2.py:186][0m |          -0.0125 |           4.6660 |           6.1457 |
[32m[20230113 20:06:08 @agent_ppo2.py:186][0m |          -0.0132 |           4.4383 |           6.1434 |
[32m[20230113 20:06:08 @agent_ppo2.py:186][0m |          -0.0229 |           4.3123 |           6.1418 |
[32m[20230113 20:06:08 @agent_ppo2.py:186][0m |          -0.0333 |           4.2376 |           6.1370 |
[32m[20230113 20:06:08 @agent_ppo2.py:186][0m |          -0.0115 |           4.1255 |           6.1437 |
[32m[20230113 20:06:08 @agent_ppo2.py:186][0m |          -0.0201 |           4.0136 |           6.1391 |
[32m[20230113 20:06:08 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:06:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.43
[32m[20230113 20:06:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.53
[32m[20230113 20:06:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.94
[32m[20230113 20:06:08 @agent_ppo2.py:144][0m Total time:      21.60 min
[32m[20230113 20:06:08 @agent_ppo2.py:146][0m 1964032 total steps have happened
[32m[20230113 20:06:08 @agent_ppo2.py:122][0m #------------------------ Iteration 959 --------------------------#
[32m[20230113 20:06:09 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:06:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:09 @agent_ppo2.py:186][0m |          -0.0002 |           6.5600 |           6.3505 |
[32m[20230113 20:06:09 @agent_ppo2.py:186][0m |          -0.0048 |           5.2803 |           6.3438 |
[32m[20230113 20:06:09 @agent_ppo2.py:186][0m |          -0.0051 |           5.0377 |           6.3365 |
[32m[20230113 20:06:09 @agent_ppo2.py:186][0m |          -0.0096 |           4.5970 |           6.3414 |
[32m[20230113 20:06:09 @agent_ppo2.py:186][0m |          -0.0102 |           4.2471 |           6.3393 |
[32m[20230113 20:06:09 @agent_ppo2.py:186][0m |          -0.0084 |           4.1006 |           6.3453 |
[32m[20230113 20:06:09 @agent_ppo2.py:186][0m |          -0.0108 |           3.9570 |           6.3373 |
[32m[20230113 20:06:09 @agent_ppo2.py:186][0m |          -0.0093 |           3.8694 |           6.3436 |
[32m[20230113 20:06:09 @agent_ppo2.py:186][0m |          -0.0131 |           3.7184 |           6.3395 |
[32m[20230113 20:06:09 @agent_ppo2.py:186][0m |          -0.0139 |           3.6294 |           6.3422 |
[32m[20230113 20:06:09 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:06:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.29
[32m[20230113 20:06:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.60
[32m[20230113 20:06:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.38
[32m[20230113 20:06:10 @agent_ppo2.py:144][0m Total time:      21.62 min
[32m[20230113 20:06:10 @agent_ppo2.py:146][0m 1966080 total steps have happened
[32m[20230113 20:06:10 @agent_ppo2.py:122][0m #------------------------ Iteration 960 --------------------------#
[32m[20230113 20:06:10 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:06:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:10 @agent_ppo2.py:186][0m |          -0.0046 |           5.8893 |           6.2958 |
[32m[20230113 20:06:10 @agent_ppo2.py:186][0m |          -0.0157 |           4.0654 |           6.2881 |
[32m[20230113 20:06:10 @agent_ppo2.py:186][0m |          -0.0015 |           3.4120 |           6.2962 |
[32m[20230113 20:06:10 @agent_ppo2.py:186][0m |          -0.0028 |           3.1225 |           6.2829 |
[32m[20230113 20:06:10 @agent_ppo2.py:186][0m |          -0.0144 |           2.9109 |           6.2826 |
[32m[20230113 20:06:10 @agent_ppo2.py:186][0m |          -0.0137 |           2.7529 |           6.2831 |
[32m[20230113 20:06:11 @agent_ppo2.py:186][0m |          -0.0207 |           2.8456 |           6.2739 |
[32m[20230113 20:06:11 @agent_ppo2.py:186][0m |          -0.0122 |           2.7500 |           6.2753 |
[32m[20230113 20:06:11 @agent_ppo2.py:186][0m |          -0.0263 |           2.5223 |           6.2754 |
[32m[20230113 20:06:11 @agent_ppo2.py:186][0m |          -0.0041 |           2.4530 |           6.2642 |
[32m[20230113 20:06:11 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:06:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.81
[32m[20230113 20:06:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.60
[32m[20230113 20:06:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.19
[32m[20230113 20:06:11 @agent_ppo2.py:144][0m Total time:      21.64 min
[32m[20230113 20:06:11 @agent_ppo2.py:146][0m 1968128 total steps have happened
[32m[20230113 20:06:11 @agent_ppo2.py:122][0m #------------------------ Iteration 961 --------------------------#
[32m[20230113 20:06:12 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:06:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:12 @agent_ppo2.py:186][0m |          -0.0035 |           4.9410 |           6.2159 |
[32m[20230113 20:06:12 @agent_ppo2.py:186][0m |          -0.0061 |           3.9474 |           6.2129 |
[32m[20230113 20:06:12 @agent_ppo2.py:186][0m |          -0.0120 |           3.5749 |           6.2109 |
[32m[20230113 20:06:12 @agent_ppo2.py:186][0m |          -0.0081 |           3.4770 |           6.2160 |
[32m[20230113 20:06:12 @agent_ppo2.py:186][0m |          -0.0111 |           3.1733 |           6.2090 |
[32m[20230113 20:06:12 @agent_ppo2.py:186][0m |          -0.0130 |           2.9728 |           6.2150 |
[32m[20230113 20:06:12 @agent_ppo2.py:186][0m |          -0.0160 |           2.8597 |           6.2133 |
[32m[20230113 20:06:12 @agent_ppo2.py:186][0m |          -0.0136 |           2.7905 |           6.2102 |
[32m[20230113 20:06:12 @agent_ppo2.py:186][0m |          -0.0114 |           2.7169 |           6.2112 |
[32m[20230113 20:06:12 @agent_ppo2.py:186][0m |          -0.0182 |           2.6119 |           6.2069 |
[32m[20230113 20:06:12 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:06:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.21
[32m[20230113 20:06:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.32
[32m[20230113 20:06:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.64
[32m[20230113 20:06:12 @agent_ppo2.py:144][0m Total time:      21.66 min
[32m[20230113 20:06:12 @agent_ppo2.py:146][0m 1970176 total steps have happened
[32m[20230113 20:06:12 @agent_ppo2.py:122][0m #------------------------ Iteration 962 --------------------------#
[32m[20230113 20:06:13 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:13 @agent_ppo2.py:186][0m |          -0.0016 |           5.4411 |           6.3685 |
[32m[20230113 20:06:13 @agent_ppo2.py:186][0m |          -0.0070 |           4.4847 |           6.3513 |
[32m[20230113 20:06:13 @agent_ppo2.py:186][0m |          -0.0091 |           4.2322 |           6.3490 |
[32m[20230113 20:06:13 @agent_ppo2.py:186][0m |          -0.0100 |           3.9948 |           6.3547 |
[32m[20230113 20:06:13 @agent_ppo2.py:186][0m |          -0.0105 |           4.0128 |           6.3452 |
[32m[20230113 20:06:13 @agent_ppo2.py:186][0m |          -0.0117 |           3.8070 |           6.3500 |
[32m[20230113 20:06:13 @agent_ppo2.py:186][0m |          -0.0115 |           3.6760 |           6.3440 |
[32m[20230113 20:06:13 @agent_ppo2.py:186][0m |          -0.0124 |           3.6120 |           6.3468 |
[32m[20230113 20:06:13 @agent_ppo2.py:186][0m |          -0.0125 |           3.6154 |           6.3480 |
[32m[20230113 20:06:13 @agent_ppo2.py:186][0m |          -0.0128 |           3.4756 |           6.3529 |
[32m[20230113 20:06:13 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.92
[32m[20230113 20:06:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.45
[32m[20230113 20:06:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.28
[32m[20230113 20:06:14 @agent_ppo2.py:144][0m Total time:      21.69 min
[32m[20230113 20:06:14 @agent_ppo2.py:146][0m 1972224 total steps have happened
[32m[20230113 20:06:14 @agent_ppo2.py:122][0m #------------------------ Iteration 963 --------------------------#
[32m[20230113 20:06:14 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:06:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:14 @agent_ppo2.py:186][0m |          -0.0007 |           5.6644 |           6.3043 |
[32m[20230113 20:06:14 @agent_ppo2.py:186][0m |          -0.0006 |           4.5379 |           6.2940 |
[32m[20230113 20:06:14 @agent_ppo2.py:186][0m |          -0.0062 |           4.1542 |           6.2910 |
[32m[20230113 20:06:14 @agent_ppo2.py:186][0m |          -0.0082 |           3.8552 |           6.2996 |
[32m[20230113 20:06:14 @agent_ppo2.py:186][0m |          -0.0083 |           3.6992 |           6.2920 |
[32m[20230113 20:06:14 @agent_ppo2.py:186][0m |          -0.0098 |           3.6225 |           6.2873 |
[32m[20230113 20:06:14 @agent_ppo2.py:186][0m |          -0.0104 |           3.4470 |           6.2891 |
[32m[20230113 20:06:15 @agent_ppo2.py:186][0m |          -0.0091 |           3.3760 |           6.2936 |
[32m[20230113 20:06:15 @agent_ppo2.py:186][0m |          -0.0136 |           3.3039 |           6.2856 |
[32m[20230113 20:06:15 @agent_ppo2.py:186][0m |          -0.0121 |           3.1789 |           6.2882 |
[32m[20230113 20:06:15 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.93
[32m[20230113 20:06:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.69
[32m[20230113 20:06:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 148.98
[32m[20230113 20:06:15 @agent_ppo2.py:144][0m Total time:      21.71 min
[32m[20230113 20:06:15 @agent_ppo2.py:146][0m 1974272 total steps have happened
[32m[20230113 20:06:15 @agent_ppo2.py:122][0m #------------------------ Iteration 964 --------------------------#
[32m[20230113 20:06:16 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:06:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:16 @agent_ppo2.py:186][0m |          -0.0003 |           6.1394 |           6.2311 |
[32m[20230113 20:06:16 @agent_ppo2.py:186][0m |          -0.0055 |           4.9096 |           6.2079 |
[32m[20230113 20:06:16 @agent_ppo2.py:186][0m |          -0.0083 |           4.5714 |           6.2150 |
[32m[20230113 20:06:16 @agent_ppo2.py:186][0m |          -0.0099 |           4.2603 |           6.2002 |
[32m[20230113 20:06:16 @agent_ppo2.py:186][0m |          -0.0112 |           4.0626 |           6.2015 |
[32m[20230113 20:06:16 @agent_ppo2.py:186][0m |          -0.0121 |           3.9253 |           6.1971 |
[32m[20230113 20:06:16 @agent_ppo2.py:186][0m |          -0.0126 |           3.7871 |           6.1922 |
[32m[20230113 20:06:16 @agent_ppo2.py:186][0m |          -0.0147 |           3.6979 |           6.1934 |
[32m[20230113 20:06:16 @agent_ppo2.py:186][0m |          -0.0146 |           3.5683 |           6.1929 |
[32m[20230113 20:06:16 @agent_ppo2.py:186][0m |          -0.0148 |           3.5298 |           6.1884 |
[32m[20230113 20:06:16 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:06:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.33
[32m[20230113 20:06:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.89
[32m[20230113 20:06:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 138.26
[32m[20230113 20:06:16 @agent_ppo2.py:144][0m Total time:      21.73 min
[32m[20230113 20:06:16 @agent_ppo2.py:146][0m 1976320 total steps have happened
[32m[20230113 20:06:16 @agent_ppo2.py:122][0m #------------------------ Iteration 965 --------------------------#
[32m[20230113 20:06:17 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:06:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:17 @agent_ppo2.py:186][0m |          -0.0041 |          17.1749 |           6.3747 |
[32m[20230113 20:06:17 @agent_ppo2.py:186][0m |          -0.0038 |           5.8679 |           6.3657 |
[32m[20230113 20:06:17 @agent_ppo2.py:186][0m |          -0.0070 |           4.9592 |           6.3675 |
[32m[20230113 20:06:17 @agent_ppo2.py:186][0m |          -0.0097 |           4.5642 |           6.3517 |
[32m[20230113 20:06:17 @agent_ppo2.py:186][0m |          -0.0110 |           4.1960 |           6.3494 |
[32m[20230113 20:06:17 @agent_ppo2.py:186][0m |          -0.0145 |           3.9592 |           6.3481 |
[32m[20230113 20:06:17 @agent_ppo2.py:186][0m |          -0.0139 |           3.7706 |           6.3483 |
[32m[20230113 20:06:17 @agent_ppo2.py:186][0m |          -0.0146 |           3.5962 |           6.3419 |
[32m[20230113 20:06:17 @agent_ppo2.py:186][0m |          -0.0180 |           3.4374 |           6.3447 |
[32m[20230113 20:06:17 @agent_ppo2.py:186][0m |          -0.0189 |           3.3005 |           6.3422 |
[32m[20230113 20:06:17 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:06:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 110.70
[32m[20230113 20:06:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.53
[32m[20230113 20:06:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.55
[32m[20230113 20:06:17 @agent_ppo2.py:144][0m Total time:      21.75 min
[32m[20230113 20:06:17 @agent_ppo2.py:146][0m 1978368 total steps have happened
[32m[20230113 20:06:17 @agent_ppo2.py:122][0m #------------------------ Iteration 966 --------------------------#
[32m[20230113 20:06:18 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:06:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:18 @agent_ppo2.py:186][0m |          -0.0011 |           4.8617 |           6.1926 |
[32m[20230113 20:06:18 @agent_ppo2.py:186][0m |          -0.0101 |           3.8286 |           6.1807 |
[32m[20230113 20:06:18 @agent_ppo2.py:186][0m |          -0.0121 |           3.3042 |           6.1810 |
[32m[20230113 20:06:18 @agent_ppo2.py:186][0m |          -0.0137 |           2.9712 |           6.1759 |
[32m[20230113 20:06:18 @agent_ppo2.py:186][0m |          -0.0141 |           2.7456 |           6.1735 |
[32m[20230113 20:06:18 @agent_ppo2.py:186][0m |          -0.0159 |           2.5913 |           6.1699 |
[32m[20230113 20:06:18 @agent_ppo2.py:186][0m |          -0.0176 |           2.4614 |           6.1681 |
[32m[20230113 20:06:18 @agent_ppo2.py:186][0m |          -0.0175 |           2.3416 |           6.1704 |
[32m[20230113 20:06:18 @agent_ppo2.py:186][0m |          -0.0183 |           2.2479 |           6.1691 |
[32m[20230113 20:06:18 @agent_ppo2.py:186][0m |          -0.0186 |           2.1786 |           6.1665 |
[32m[20230113 20:06:18 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:06:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 219.91
[32m[20230113 20:06:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.14
[32m[20230113 20:06:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.17
[32m[20230113 20:06:19 @agent_ppo2.py:144][0m Total time:      21.77 min
[32m[20230113 20:06:19 @agent_ppo2.py:146][0m 1980416 total steps have happened
[32m[20230113 20:06:19 @agent_ppo2.py:122][0m #------------------------ Iteration 967 --------------------------#
[32m[20230113 20:06:19 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:06:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:19 @agent_ppo2.py:186][0m |           0.0015 |           6.1134 |           6.3335 |
[32m[20230113 20:06:19 @agent_ppo2.py:186][0m |          -0.0025 |           5.1688 |           6.3373 |
[32m[20230113 20:06:19 @agent_ppo2.py:186][0m |          -0.0068 |           4.7734 |           6.3260 |
[32m[20230113 20:06:19 @agent_ppo2.py:186][0m |          -0.0084 |           4.5547 |           6.3278 |
[32m[20230113 20:06:20 @agent_ppo2.py:186][0m |          -0.0104 |           4.2788 |           6.3295 |
[32m[20230113 20:06:20 @agent_ppo2.py:186][0m |          -0.0131 |           4.1530 |           6.3258 |
[32m[20230113 20:06:20 @agent_ppo2.py:186][0m |          -0.0133 |           4.0248 |           6.3320 |
[32m[20230113 20:06:20 @agent_ppo2.py:186][0m |          -0.0137 |           3.9201 |           6.3302 |
[32m[20230113 20:06:20 @agent_ppo2.py:186][0m |          -0.0117 |           3.8357 |           6.3320 |
[32m[20230113 20:06:20 @agent_ppo2.py:186][0m |          -0.0134 |           3.7540 |           6.3284 |
[32m[20230113 20:06:20 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:06:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.51
[32m[20230113 20:06:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.42
[32m[20230113 20:06:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 174.32
[32m[20230113 20:06:20 @agent_ppo2.py:144][0m Total time:      21.79 min
[32m[20230113 20:06:20 @agent_ppo2.py:146][0m 1982464 total steps have happened
[32m[20230113 20:06:20 @agent_ppo2.py:122][0m #------------------------ Iteration 968 --------------------------#
[32m[20230113 20:06:21 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:06:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:21 @agent_ppo2.py:186][0m |          -0.0077 |          23.0120 |           6.2180 |
[32m[20230113 20:06:21 @agent_ppo2.py:186][0m |          -0.0104 |           8.6018 |           6.2082 |
[32m[20230113 20:06:21 @agent_ppo2.py:186][0m |           0.0111 |           7.2631 |           6.2015 |
[32m[20230113 20:06:21 @agent_ppo2.py:186][0m |          -0.0150 |           6.5524 |           6.1925 |
[32m[20230113 20:06:21 @agent_ppo2.py:186][0m |          -0.0169 |           6.0644 |           6.1953 |
[32m[20230113 20:06:21 @agent_ppo2.py:186][0m |          -0.0178 |           5.5650 |           6.1927 |
[32m[20230113 20:06:21 @agent_ppo2.py:186][0m |          -0.0196 |           5.1706 |           6.1904 |
[32m[20230113 20:06:21 @agent_ppo2.py:186][0m |          -0.0189 |           4.9270 |           6.1891 |
[32m[20230113 20:06:21 @agent_ppo2.py:186][0m |          -0.0213 |           4.7899 |           6.1860 |
[32m[20230113 20:06:21 @agent_ppo2.py:186][0m |           0.0080 |           4.7398 |           6.1854 |
[32m[20230113 20:06:21 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:06:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 65.32
[32m[20230113 20:06:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.19
[32m[20230113 20:06:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.60
[32m[20230113 20:06:21 @agent_ppo2.py:144][0m Total time:      21.81 min
[32m[20230113 20:06:21 @agent_ppo2.py:146][0m 1984512 total steps have happened
[32m[20230113 20:06:21 @agent_ppo2.py:122][0m #------------------------ Iteration 969 --------------------------#
[32m[20230113 20:06:22 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:22 @agent_ppo2.py:186][0m |           0.0015 |          11.8421 |           6.2071 |
[32m[20230113 20:06:22 @agent_ppo2.py:186][0m |           0.0017 |           8.5912 |           6.2040 |
[32m[20230113 20:06:22 @agent_ppo2.py:186][0m |          -0.0049 |           7.3960 |           6.2011 |
[32m[20230113 20:06:22 @agent_ppo2.py:186][0m |          -0.0102 |           6.6676 |           6.1957 |
[32m[20230113 20:06:22 @agent_ppo2.py:186][0m |          -0.0202 |           6.3439 |           6.1924 |
[32m[20230113 20:06:22 @agent_ppo2.py:186][0m |          -0.0141 |           5.9900 |           6.1947 |
[32m[20230113 20:06:22 @agent_ppo2.py:186][0m |          -0.0132 |           5.5986 |           6.1927 |
[32m[20230113 20:06:22 @agent_ppo2.py:186][0m |          -0.0117 |           5.3889 |           6.1958 |
[32m[20230113 20:06:22 @agent_ppo2.py:186][0m |          -0.0101 |           5.2240 |           6.1920 |
[32m[20230113 20:06:22 @agent_ppo2.py:186][0m |          -0.0094 |           5.1726 |           6.1981 |
[32m[20230113 20:06:22 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.62
[32m[20230113 20:06:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.03
[32m[20230113 20:06:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.55
[32m[20230113 20:06:23 @agent_ppo2.py:144][0m Total time:      21.84 min
[32m[20230113 20:06:23 @agent_ppo2.py:146][0m 1986560 total steps have happened
[32m[20230113 20:06:23 @agent_ppo2.py:122][0m #------------------------ Iteration 970 --------------------------#
[32m[20230113 20:06:23 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:06:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:23 @agent_ppo2.py:186][0m |           0.0021 |           6.2620 |           6.2319 |
[32m[20230113 20:06:23 @agent_ppo2.py:186][0m |          -0.0027 |           4.5920 |           6.2290 |
[32m[20230113 20:06:23 @agent_ppo2.py:186][0m |          -0.0052 |           4.0747 |           6.2283 |
[32m[20230113 20:06:23 @agent_ppo2.py:186][0m |          -0.0139 |           3.7958 |           6.2282 |
[32m[20230113 20:06:23 @agent_ppo2.py:186][0m |          -0.0146 |           3.6554 |           6.2283 |
[32m[20230113 20:06:23 @agent_ppo2.py:186][0m |          -0.0097 |           3.5069 |           6.2280 |
[32m[20230113 20:06:23 @agent_ppo2.py:186][0m |          -0.0089 |           3.5697 |           6.2291 |
[32m[20230113 20:06:23 @agent_ppo2.py:186][0m |          -0.0143 |           3.3265 |           6.2244 |
[32m[20230113 20:06:24 @agent_ppo2.py:186][0m |          -0.0204 |           3.2487 |           6.2238 |
[32m[20230113 20:06:24 @agent_ppo2.py:186][0m |          -0.0112 |           3.1839 |           6.2289 |
[32m[20230113 20:06:24 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:06:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.66
[32m[20230113 20:06:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.93
[32m[20230113 20:06:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.32
[32m[20230113 20:06:24 @agent_ppo2.py:144][0m Total time:      21.86 min
[32m[20230113 20:06:24 @agent_ppo2.py:146][0m 1988608 total steps have happened
[32m[20230113 20:06:24 @agent_ppo2.py:122][0m #------------------------ Iteration 971 --------------------------#
[32m[20230113 20:06:24 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:06:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:24 @agent_ppo2.py:186][0m |          -0.0004 |           6.5943 |           6.4572 |
[32m[20230113 20:06:25 @agent_ppo2.py:186][0m |          -0.0068 |           5.0173 |           6.4511 |
[32m[20230113 20:06:25 @agent_ppo2.py:186][0m |          -0.0076 |           4.3104 |           6.4466 |
[32m[20230113 20:06:25 @agent_ppo2.py:186][0m |          -0.0092 |           3.9310 |           6.4486 |
[32m[20230113 20:06:25 @agent_ppo2.py:186][0m |          -0.0112 |           3.5766 |           6.4483 |
[32m[20230113 20:06:25 @agent_ppo2.py:186][0m |          -0.0118 |           3.3963 |           6.4527 |
[32m[20230113 20:06:25 @agent_ppo2.py:186][0m |          -0.0134 |           3.1886 |           6.4519 |
[32m[20230113 20:06:25 @agent_ppo2.py:186][0m |          -0.0127 |           3.0715 |           6.4517 |
[32m[20230113 20:06:25 @agent_ppo2.py:186][0m |          -0.0130 |           2.9515 |           6.4499 |
[32m[20230113 20:06:25 @agent_ppo2.py:186][0m |          -0.0139 |           2.8419 |           6.4529 |
[32m[20230113 20:06:25 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:06:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.97
[32m[20230113 20:06:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.92
[32m[20230113 20:06:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.18
[32m[20230113 20:06:25 @agent_ppo2.py:144][0m Total time:      21.88 min
[32m[20230113 20:06:25 @agent_ppo2.py:146][0m 1990656 total steps have happened
[32m[20230113 20:06:25 @agent_ppo2.py:122][0m #------------------------ Iteration 972 --------------------------#
[32m[20230113 20:06:26 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:06:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:26 @agent_ppo2.py:186][0m |          -0.0022 |          12.3134 |           6.3052 |
[32m[20230113 20:06:26 @agent_ppo2.py:186][0m |          -0.0080 |           6.4092 |           6.2974 |
[32m[20230113 20:06:26 @agent_ppo2.py:186][0m |          -0.0105 |           5.3485 |           6.2980 |
[32m[20230113 20:06:26 @agent_ppo2.py:186][0m |          -0.0111 |           4.8723 |           6.2907 |
[32m[20230113 20:06:26 @agent_ppo2.py:186][0m |          -0.0136 |           4.4571 |           6.2913 |
[32m[20230113 20:06:26 @agent_ppo2.py:186][0m |          -0.0135 |           4.1030 |           6.2868 |
[32m[20230113 20:06:26 @agent_ppo2.py:186][0m |          -0.0144 |           3.8616 |           6.2849 |
[32m[20230113 20:06:26 @agent_ppo2.py:186][0m |          -0.0152 |           3.6870 |           6.2791 |
[32m[20230113 20:06:26 @agent_ppo2.py:186][0m |          -0.0159 |           3.5154 |           6.2785 |
[32m[20230113 20:06:26 @agent_ppo2.py:186][0m |          -0.0166 |           3.3942 |           6.2784 |
[32m[20230113 20:06:26 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 20:06:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 138.45
[32m[20230113 20:06:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.74
[32m[20230113 20:06:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.22
[32m[20230113 20:06:27 @agent_ppo2.py:144][0m Total time:      21.90 min
[32m[20230113 20:06:27 @agent_ppo2.py:146][0m 1992704 total steps have happened
[32m[20230113 20:06:27 @agent_ppo2.py:122][0m #------------------------ Iteration 973 --------------------------#
[32m[20230113 20:06:27 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:27 @agent_ppo2.py:186][0m |          -0.0009 |           6.6766 |           6.4204 |
[32m[20230113 20:06:27 @agent_ppo2.py:186][0m |          -0.0068 |           5.0154 |           6.4218 |
[32m[20230113 20:06:27 @agent_ppo2.py:186][0m |          -0.0069 |           4.5015 |           6.4282 |
[32m[20230113 20:06:27 @agent_ppo2.py:186][0m |          -0.0076 |           4.1957 |           6.4220 |
[32m[20230113 20:06:27 @agent_ppo2.py:186][0m |          -0.0086 |           4.0699 |           6.4259 |
[32m[20230113 20:06:27 @agent_ppo2.py:186][0m |          -0.0117 |           3.8113 |           6.4298 |
[32m[20230113 20:06:27 @agent_ppo2.py:186][0m |          -0.0133 |           3.6801 |           6.4320 |
[32m[20230113 20:06:27 @agent_ppo2.py:186][0m |          -0.0120 |           3.5955 |           6.4356 |
[32m[20230113 20:06:28 @agent_ppo2.py:186][0m |          -0.0153 |           3.5194 |           6.4340 |
[32m[20230113 20:06:28 @agent_ppo2.py:186][0m |          -0.0119 |           3.4094 |           6.4368 |
[32m[20230113 20:06:28 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:06:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.38
[32m[20230113 20:06:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.45
[32m[20230113 20:06:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.67
[32m[20230113 20:06:28 @agent_ppo2.py:144][0m Total time:      21.92 min
[32m[20230113 20:06:28 @agent_ppo2.py:146][0m 1994752 total steps have happened
[32m[20230113 20:06:28 @agent_ppo2.py:122][0m #------------------------ Iteration 974 --------------------------#
[32m[20230113 20:06:28 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:29 @agent_ppo2.py:186][0m |           0.0055 |           6.1407 |           6.4170 |
[32m[20230113 20:06:29 @agent_ppo2.py:186][0m |          -0.0110 |           4.7883 |           6.4057 |
[32m[20230113 20:06:29 @agent_ppo2.py:186][0m |          -0.0153 |           4.1756 |           6.4025 |
[32m[20230113 20:06:29 @agent_ppo2.py:186][0m |          -0.0155 |           3.7925 |           6.4078 |
[32m[20230113 20:06:29 @agent_ppo2.py:186][0m |          -0.0075 |           3.5912 |           6.4050 |
[32m[20230113 20:06:29 @agent_ppo2.py:186][0m |          -0.0178 |           3.4465 |           6.4070 |
[32m[20230113 20:06:29 @agent_ppo2.py:186][0m |          -0.0219 |           3.2377 |           6.4116 |
[32m[20230113 20:06:29 @agent_ppo2.py:186][0m |          -0.0206 |           3.0993 |           6.4090 |
[32m[20230113 20:06:29 @agent_ppo2.py:186][0m |          -0.0274 |           3.0616 |           6.4167 |
[32m[20230113 20:06:29 @agent_ppo2.py:186][0m |          -0.0156 |           3.0044 |           6.4128 |
[32m[20230113 20:06:29 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.69
[32m[20230113 20:06:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.04
[32m[20230113 20:06:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.58
[32m[20230113 20:06:29 @agent_ppo2.py:144][0m Total time:      21.95 min
[32m[20230113 20:06:29 @agent_ppo2.py:146][0m 1996800 total steps have happened
[32m[20230113 20:06:29 @agent_ppo2.py:122][0m #------------------------ Iteration 975 --------------------------#
[32m[20230113 20:06:30 @agent_ppo2.py:128][0m Sampling time: 0.35 s by 1 slaves
[32m[20230113 20:06:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:30 @agent_ppo2.py:186][0m |           0.0026 |          12.8976 |           6.5370 |
[32m[20230113 20:06:30 @agent_ppo2.py:186][0m |          -0.0036 |           6.7442 |           6.5217 |
[32m[20230113 20:06:30 @agent_ppo2.py:186][0m |          -0.0107 |           5.6330 |           6.5189 |
[32m[20230113 20:06:30 @agent_ppo2.py:186][0m |          -0.0084 |           4.9984 |           6.5239 |
[32m[20230113 20:06:30 @agent_ppo2.py:186][0m |          -0.0096 |           4.4657 |           6.5136 |
[32m[20230113 20:06:30 @agent_ppo2.py:186][0m |          -0.0131 |           4.1684 |           6.5081 |
[32m[20230113 20:06:30 @agent_ppo2.py:186][0m |          -0.0153 |           3.9270 |           6.5177 |
[32m[20230113 20:06:30 @agent_ppo2.py:186][0m |          -0.0151 |           3.6797 |           6.5108 |
[32m[20230113 20:06:30 @agent_ppo2.py:186][0m |          -0.0156 |           3.5062 |           6.5113 |
[32m[20230113 20:06:30 @agent_ppo2.py:186][0m |          -0.0146 |           3.3295 |           6.5176 |
[32m[20230113 20:06:30 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:06:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 118.52
[32m[20230113 20:06:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.11
[32m[20230113 20:06:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.38
[32m[20230113 20:06:30 @agent_ppo2.py:144][0m Total time:      21.96 min
[32m[20230113 20:06:30 @agent_ppo2.py:146][0m 1998848 total steps have happened
[32m[20230113 20:06:30 @agent_ppo2.py:122][0m #------------------------ Iteration 976 --------------------------#
[32m[20230113 20:06:31 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:31 @agent_ppo2.py:186][0m |          -0.0008 |           5.7406 |           6.4996 |
[32m[20230113 20:06:31 @agent_ppo2.py:186][0m |          -0.0018 |           4.4508 |           6.4898 |
[32m[20230113 20:06:31 @agent_ppo2.py:186][0m |          -0.0077 |           3.9416 |           6.4794 |
[32m[20230113 20:06:31 @agent_ppo2.py:186][0m |          -0.0070 |           3.6040 |           6.4850 |
[32m[20230113 20:06:31 @agent_ppo2.py:186][0m |          -0.0101 |           3.4019 |           6.4815 |
[32m[20230113 20:06:31 @agent_ppo2.py:186][0m |          -0.0118 |           3.2197 |           6.4777 |
[32m[20230113 20:06:31 @agent_ppo2.py:186][0m |          -0.0095 |           3.1672 |           6.4792 |
[32m[20230113 20:06:31 @agent_ppo2.py:186][0m |          -0.0103 |           3.0191 |           6.4740 |
[32m[20230113 20:06:31 @agent_ppo2.py:186][0m |          -0.0143 |           2.9125 |           6.4727 |
[32m[20230113 20:06:31 @agent_ppo2.py:186][0m |          -0.0137 |           2.8263 |           6.4754 |
[32m[20230113 20:06:31 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.42
[32m[20230113 20:06:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.60
[32m[20230113 20:06:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.20
[32m[20230113 20:06:32 @agent_ppo2.py:144][0m Total time:      21.99 min
[32m[20230113 20:06:32 @agent_ppo2.py:146][0m 2000896 total steps have happened
[32m[20230113 20:06:32 @agent_ppo2.py:122][0m #------------------------ Iteration 977 --------------------------#
[32m[20230113 20:06:32 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:32 @agent_ppo2.py:186][0m |          -0.0006 |           5.9948 |           6.3280 |
[32m[20230113 20:06:32 @agent_ppo2.py:186][0m |          -0.0075 |           4.8090 |           6.3138 |
[32m[20230113 20:06:32 @agent_ppo2.py:186][0m |          -0.0034 |           4.3314 |           6.3177 |
[32m[20230113 20:06:32 @agent_ppo2.py:186][0m |          -0.0107 |           4.0611 |           6.3068 |
[32m[20230113 20:06:32 @agent_ppo2.py:186][0m |          -0.0112 |           3.8439 |           6.3077 |
[32m[20230113 20:06:32 @agent_ppo2.py:186][0m |          -0.0104 |           3.6962 |           6.3093 |
[32m[20230113 20:06:32 @agent_ppo2.py:186][0m |          -0.0130 |           3.5604 |           6.3008 |
[32m[20230113 20:06:32 @agent_ppo2.py:186][0m |          -0.0155 |           3.4718 |           6.2925 |
[32m[20230113 20:06:33 @agent_ppo2.py:186][0m |          -0.0141 |           3.4009 |           6.2990 |
[32m[20230113 20:06:33 @agent_ppo2.py:186][0m |          -0.0171 |           3.3196 |           6.2937 |
[32m[20230113 20:06:33 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.82
[32m[20230113 20:06:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.77
[32m[20230113 20:06:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.61
[32m[20230113 20:06:33 @agent_ppo2.py:144][0m Total time:      22.01 min
[32m[20230113 20:06:33 @agent_ppo2.py:146][0m 2002944 total steps have happened
[32m[20230113 20:06:33 @agent_ppo2.py:122][0m #------------------------ Iteration 978 --------------------------#
[32m[20230113 20:06:33 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:33 @agent_ppo2.py:186][0m |          -0.0032 |           5.0909 |           6.4028 |
[32m[20230113 20:06:34 @agent_ppo2.py:186][0m |          -0.0055 |           4.4332 |           6.3938 |
[32m[20230113 20:06:34 @agent_ppo2.py:186][0m |          -0.0077 |           4.0141 |           6.3932 |
[32m[20230113 20:06:34 @agent_ppo2.py:186][0m |          -0.0133 |           3.7635 |           6.3874 |
[32m[20230113 20:06:34 @agent_ppo2.py:186][0m |          -0.0136 |           3.5886 |           6.3961 |
[32m[20230113 20:06:34 @agent_ppo2.py:186][0m |          -0.0154 |           3.5013 |           6.3908 |
[32m[20230113 20:06:34 @agent_ppo2.py:186][0m |          -0.0155 |           3.4023 |           6.3869 |
[32m[20230113 20:06:34 @agent_ppo2.py:186][0m |          -0.0136 |           3.4456 |           6.3885 |
[32m[20230113 20:06:34 @agent_ppo2.py:186][0m |          -0.0151 |           3.2792 |           6.3907 |
[32m[20230113 20:06:34 @agent_ppo2.py:186][0m |          -0.0182 |           3.2249 |           6.3890 |
[32m[20230113 20:06:34 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.15
[32m[20230113 20:06:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.71
[32m[20230113 20:06:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.96
[32m[20230113 20:06:34 @agent_ppo2.py:144][0m Total time:      22.03 min
[32m[20230113 20:06:34 @agent_ppo2.py:146][0m 2004992 total steps have happened
[32m[20230113 20:06:34 @agent_ppo2.py:122][0m #------------------------ Iteration 979 --------------------------#
[32m[20230113 20:06:35 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:06:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:35 @agent_ppo2.py:186][0m |          -0.0006 |          33.0355 |           6.4762 |
[32m[20230113 20:06:35 @agent_ppo2.py:186][0m |          -0.0088 |          16.5277 |           6.4664 |
[32m[20230113 20:06:35 @agent_ppo2.py:186][0m |          -0.0122 |          14.4170 |           6.4713 |
[32m[20230113 20:06:35 @agent_ppo2.py:186][0m |          -0.0128 |          13.5224 |           6.4650 |
[32m[20230113 20:06:35 @agent_ppo2.py:186][0m |          -0.0117 |          12.6681 |           6.4643 |
[32m[20230113 20:06:35 @agent_ppo2.py:186][0m |          -0.0154 |          11.6814 |           6.4572 |
[32m[20230113 20:06:35 @agent_ppo2.py:186][0m |          -0.0079 |          11.3496 |           6.4559 |
[32m[20230113 20:06:35 @agent_ppo2.py:186][0m |          -0.0123 |          10.4878 |           6.4592 |
[32m[20230113 20:06:35 @agent_ppo2.py:186][0m |          -0.0185 |           9.9454 |           6.4563 |
[32m[20230113 20:06:35 @agent_ppo2.py:186][0m |          -0.0163 |           9.4921 |           6.4514 |
[32m[20230113 20:06:35 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 20:06:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 13.36
[32m[20230113 20:06:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 68.67
[32m[20230113 20:06:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 151.73
[32m[20230113 20:06:35 @agent_ppo2.py:144][0m Total time:      22.05 min
[32m[20230113 20:06:35 @agent_ppo2.py:146][0m 2007040 total steps have happened
[32m[20230113 20:06:35 @agent_ppo2.py:122][0m #------------------------ Iteration 980 --------------------------#
[32m[20230113 20:06:36 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 20:06:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:36 @agent_ppo2.py:186][0m |           0.0012 |          21.0544 |           6.4576 |
[32m[20230113 20:06:36 @agent_ppo2.py:186][0m |          -0.0007 |          14.6738 |           6.4535 |
[32m[20230113 20:06:36 @agent_ppo2.py:186][0m |          -0.0042 |          12.7420 |           6.4440 |
[32m[20230113 20:06:36 @agent_ppo2.py:186][0m |          -0.0069 |          11.5925 |           6.4432 |
[32m[20230113 20:06:36 @agent_ppo2.py:186][0m |          -0.0073 |          10.8726 |           6.4449 |
[32m[20230113 20:06:36 @agent_ppo2.py:186][0m |          -0.0094 |          10.3070 |           6.4434 |
[32m[20230113 20:06:36 @agent_ppo2.py:186][0m |          -0.0086 |           9.8854 |           6.4381 |
[32m[20230113 20:06:36 @agent_ppo2.py:186][0m |          -0.0101 |           9.5891 |           6.4394 |
[32m[20230113 20:06:36 @agent_ppo2.py:186][0m |          -0.0118 |           9.1952 |           6.4407 |
[32m[20230113 20:06:36 @agent_ppo2.py:186][0m |          -0.0124 |           8.9686 |           6.4398 |
[32m[20230113 20:06:36 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 20:06:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 143.57
[32m[20230113 20:06:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.16
[32m[20230113 20:06:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.67
[32m[20230113 20:06:37 @agent_ppo2.py:144][0m Total time:      22.07 min
[32m[20230113 20:06:37 @agent_ppo2.py:146][0m 2009088 total steps have happened
[32m[20230113 20:06:37 @agent_ppo2.py:122][0m #------------------------ Iteration 981 --------------------------#
[32m[20230113 20:06:37 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 20:06:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:38 @agent_ppo2.py:186][0m |          -0.0007 |          24.3986 |           6.4798 |
[32m[20230113 20:06:38 @agent_ppo2.py:186][0m |          -0.0068 |          17.0962 |           6.4775 |
[32m[20230113 20:06:38 @agent_ppo2.py:186][0m |          -0.0077 |          13.3518 |           6.4765 |
[32m[20230113 20:06:38 @agent_ppo2.py:186][0m |          -0.0086 |          11.2496 |           6.4700 |
[32m[20230113 20:06:38 @agent_ppo2.py:186][0m |          -0.0093 |           9.7423 |           6.4674 |
[32m[20230113 20:06:38 @agent_ppo2.py:186][0m |          -0.0133 |           8.8475 |           6.4620 |
[32m[20230113 20:06:38 @agent_ppo2.py:186][0m |          -0.0102 |           8.0385 |           6.4612 |
[32m[20230113 20:06:38 @agent_ppo2.py:186][0m |          -0.0081 |           7.5567 |           6.4592 |
[32m[20230113 20:06:38 @agent_ppo2.py:186][0m |          -0.0117 |           6.9866 |           6.4588 |
[32m[20230113 20:06:38 @agent_ppo2.py:186][0m |          -0.0141 |           6.4948 |           6.4601 |
[32m[20230113 20:06:38 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 20:06:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 139.04
[32m[20230113 20:06:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.67
[32m[20230113 20:06:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.38
[32m[20230113 20:06:38 @agent_ppo2.py:144][0m Total time:      22.10 min
[32m[20230113 20:06:38 @agent_ppo2.py:146][0m 2011136 total steps have happened
[32m[20230113 20:06:38 @agent_ppo2.py:122][0m #------------------------ Iteration 982 --------------------------#
[32m[20230113 20:06:39 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:06:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:39 @agent_ppo2.py:186][0m |          -0.0036 |          10.0035 |           6.3626 |
[32m[20230113 20:06:39 @agent_ppo2.py:186][0m |          -0.0003 |           7.2369 |           6.3606 |
[32m[20230113 20:06:39 @agent_ppo2.py:186][0m |          -0.0266 |           7.0079 |           6.3644 |
[32m[20230113 20:06:39 @agent_ppo2.py:186][0m |          -0.0180 |           6.2670 |           6.3594 |
[32m[20230113 20:06:39 @agent_ppo2.py:186][0m |          -0.0189 |           5.8501 |           6.3750 |
[32m[20230113 20:06:39 @agent_ppo2.py:186][0m |          -0.0152 |           5.5620 |           6.3697 |
[32m[20230113 20:06:39 @agent_ppo2.py:186][0m |          -0.0044 |           5.4667 |           6.3705 |
[32m[20230113 20:06:39 @agent_ppo2.py:186][0m |          -0.0174 |           5.1783 |           6.3770 |
[32m[20230113 20:06:39 @agent_ppo2.py:186][0m |          -0.0100 |           5.1514 |           6.3753 |
[32m[20230113 20:06:39 @agent_ppo2.py:186][0m |          -0.0145 |           4.9081 |           6.3732 |
[32m[20230113 20:06:39 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:06:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.83
[32m[20230113 20:06:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.97
[32m[20230113 20:06:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 147.05
[32m[20230113 20:06:40 @agent_ppo2.py:144][0m Total time:      22.12 min
[32m[20230113 20:06:40 @agent_ppo2.py:146][0m 2013184 total steps have happened
[32m[20230113 20:06:40 @agent_ppo2.py:122][0m #------------------------ Iteration 983 --------------------------#
[32m[20230113 20:06:40 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:40 @agent_ppo2.py:186][0m |          -0.0008 |           7.0672 |           6.4011 |
[32m[20230113 20:06:40 @agent_ppo2.py:186][0m |          -0.0052 |           5.3968 |           6.3912 |
[32m[20230113 20:06:40 @agent_ppo2.py:186][0m |          -0.0117 |           4.9802 |           6.3896 |
[32m[20230113 20:06:40 @agent_ppo2.py:186][0m |          -0.0086 |           4.6527 |           6.3833 |
[32m[20230113 20:06:40 @agent_ppo2.py:186][0m |          -0.0129 |           4.4657 |           6.3894 |
[32m[20230113 20:06:40 @agent_ppo2.py:186][0m |          -0.0096 |           4.2269 |           6.3868 |
[32m[20230113 20:06:40 @agent_ppo2.py:186][0m |          -0.0147 |           4.1930 |           6.3845 |
[32m[20230113 20:06:40 @agent_ppo2.py:186][0m |          -0.0125 |           3.9861 |           6.3879 |
[32m[20230113 20:06:40 @agent_ppo2.py:186][0m |          -0.0056 |           4.0323 |           6.3864 |
[32m[20230113 20:06:40 @agent_ppo2.py:186][0m |          -0.0153 |           3.7715 |           6.3863 |
[32m[20230113 20:06:40 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.80
[32m[20230113 20:06:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.09
[32m[20230113 20:06:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.59
[32m[20230113 20:06:41 @agent_ppo2.py:144][0m Total time:      22.14 min
[32m[20230113 20:06:41 @agent_ppo2.py:146][0m 2015232 total steps have happened
[32m[20230113 20:06:41 @agent_ppo2.py:122][0m #------------------------ Iteration 984 --------------------------#
[32m[20230113 20:06:41 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:06:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:41 @agent_ppo2.py:186][0m |           0.0026 |          15.5859 |           6.5585 |
[32m[20230113 20:06:41 @agent_ppo2.py:186][0m |          -0.0074 |           7.9195 |           6.5463 |
[32m[20230113 20:06:42 @agent_ppo2.py:186][0m |          -0.0024 |           6.6256 |           6.5490 |
[32m[20230113 20:06:42 @agent_ppo2.py:186][0m |          -0.0112 |           6.0211 |           6.5446 |
[32m[20230113 20:06:42 @agent_ppo2.py:186][0m |          -0.0112 |           5.6497 |           6.5448 |
[32m[20230113 20:06:42 @agent_ppo2.py:186][0m |          -0.0081 |           5.4512 |           6.5462 |
[32m[20230113 20:06:42 @agent_ppo2.py:186][0m |          -0.0115 |           5.1438 |           6.5423 |
[32m[20230113 20:06:42 @agent_ppo2.py:186][0m |          -0.0129 |           4.9380 |           6.5452 |
[32m[20230113 20:06:42 @agent_ppo2.py:186][0m |          -0.0164 |           4.7316 |           6.5477 |
[32m[20230113 20:06:42 @agent_ppo2.py:186][0m |          -0.0175 |           4.5054 |           6.5459 |
[32m[20230113 20:06:42 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:06:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 107.62
[32m[20230113 20:06:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.23
[32m[20230113 20:06:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.78
[32m[20230113 20:06:42 @agent_ppo2.py:144][0m Total time:      22.16 min
[32m[20230113 20:06:42 @agent_ppo2.py:146][0m 2017280 total steps have happened
[32m[20230113 20:06:42 @agent_ppo2.py:122][0m #------------------------ Iteration 985 --------------------------#
[32m[20230113 20:06:43 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:06:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:43 @agent_ppo2.py:186][0m |           0.0017 |           6.2562 |           6.4991 |
[32m[20230113 20:06:43 @agent_ppo2.py:186][0m |          -0.0059 |           4.9764 |           6.4905 |
[32m[20230113 20:06:43 @agent_ppo2.py:186][0m |          -0.0094 |           4.5819 |           6.4905 |
[32m[20230113 20:06:43 @agent_ppo2.py:186][0m |          -0.0089 |           4.2635 |           6.4878 |
[32m[20230113 20:06:43 @agent_ppo2.py:186][0m |          -0.0110 |           4.0606 |           6.4831 |
[32m[20230113 20:06:43 @agent_ppo2.py:186][0m |          -0.0130 |           3.8671 |           6.4829 |
[32m[20230113 20:06:43 @agent_ppo2.py:186][0m |          -0.0124 |           3.7395 |           6.4852 |
[32m[20230113 20:06:43 @agent_ppo2.py:186][0m |          -0.0165 |           3.6325 |           6.4806 |
[32m[20230113 20:06:43 @agent_ppo2.py:186][0m |          -0.0159 |           3.5029 |           6.4802 |
[32m[20230113 20:06:43 @agent_ppo2.py:186][0m |          -0.0155 |           3.3972 |           6.4845 |
[32m[20230113 20:06:43 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:06:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.42
[32m[20230113 20:06:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.86
[32m[20230113 20:06:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.33
[32m[20230113 20:06:43 @agent_ppo2.py:144][0m Total time:      22.18 min
[32m[20230113 20:06:43 @agent_ppo2.py:146][0m 2019328 total steps have happened
[32m[20230113 20:06:43 @agent_ppo2.py:122][0m #------------------------ Iteration 986 --------------------------#
[32m[20230113 20:06:44 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:44 @agent_ppo2.py:186][0m |           0.0004 |           5.7375 |           6.4812 |
[32m[20230113 20:06:44 @agent_ppo2.py:186][0m |          -0.0029 |           4.9086 |           6.4693 |
[32m[20230113 20:06:44 @agent_ppo2.py:186][0m |           0.0002 |           4.5926 |           6.4590 |
[32m[20230113 20:06:44 @agent_ppo2.py:186][0m |          -0.0098 |           4.2754 |           6.4612 |
[32m[20230113 20:06:44 @agent_ppo2.py:186][0m |          -0.0106 |           4.1008 |           6.4637 |
[32m[20230113 20:06:44 @agent_ppo2.py:186][0m |          -0.0107 |           3.9502 |           6.4649 |
[32m[20230113 20:06:44 @agent_ppo2.py:186][0m |          -0.0131 |           3.8397 |           6.4690 |
[32m[20230113 20:06:44 @agent_ppo2.py:186][0m |          -0.0160 |           3.7758 |           6.4674 |
[32m[20230113 20:06:44 @agent_ppo2.py:186][0m |          -0.0137 |           3.6743 |           6.4680 |
[32m[20230113 20:06:44 @agent_ppo2.py:186][0m |          -0.0174 |           3.6228 |           6.4669 |
[32m[20230113 20:06:44 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.61
[32m[20230113 20:06:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.60
[32m[20230113 20:06:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.97
[32m[20230113 20:06:45 @agent_ppo2.py:144][0m Total time:      22.20 min
[32m[20230113 20:06:45 @agent_ppo2.py:146][0m 2021376 total steps have happened
[32m[20230113 20:06:45 @agent_ppo2.py:122][0m #------------------------ Iteration 987 --------------------------#
[32m[20230113 20:06:45 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:06:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:45 @agent_ppo2.py:186][0m |          -0.0048 |           6.0248 |           6.4930 |
[32m[20230113 20:06:45 @agent_ppo2.py:186][0m |          -0.0101 |           5.1890 |           6.4811 |
[32m[20230113 20:06:45 @agent_ppo2.py:186][0m |          -0.0121 |           4.7806 |           6.4776 |
[32m[20230113 20:06:45 @agent_ppo2.py:186][0m |          -0.0119 |           4.5524 |           6.4670 |
[32m[20230113 20:06:45 @agent_ppo2.py:186][0m |          -0.0108 |           4.3812 |           6.4780 |
[32m[20230113 20:06:45 @agent_ppo2.py:186][0m |          -0.0116 |           4.3245 |           6.4734 |
[32m[20230113 20:06:46 @agent_ppo2.py:186][0m |          -0.0129 |           4.2764 |           6.4699 |
[32m[20230113 20:06:46 @agent_ppo2.py:186][0m |          -0.0159 |           3.9595 |           6.4681 |
[32m[20230113 20:06:46 @agent_ppo2.py:186][0m |          -0.0125 |           3.9164 |           6.4711 |
[32m[20230113 20:06:46 @agent_ppo2.py:186][0m |          -0.0147 |           3.7825 |           6.4802 |
[32m[20230113 20:06:46 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:06:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.68
[32m[20230113 20:06:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.04
[32m[20230113 20:06:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.06
[32m[20230113 20:06:46 @agent_ppo2.py:144][0m Total time:      22.22 min
[32m[20230113 20:06:46 @agent_ppo2.py:146][0m 2023424 total steps have happened
[32m[20230113 20:06:46 @agent_ppo2.py:122][0m #------------------------ Iteration 988 --------------------------#
[32m[20230113 20:06:46 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:06:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:47 @agent_ppo2.py:186][0m |          -0.0003 |           5.5636 |           6.5272 |
[32m[20230113 20:06:47 @agent_ppo2.py:186][0m |          -0.0095 |           4.4956 |           6.5196 |
[32m[20230113 20:06:47 @agent_ppo2.py:186][0m |          -0.0102 |           4.0846 |           6.5185 |
[32m[20230113 20:06:47 @agent_ppo2.py:186][0m |          -0.0110 |           3.8416 |           6.5139 |
[32m[20230113 20:06:47 @agent_ppo2.py:186][0m |          -0.0163 |           3.7272 |           6.5088 |
[32m[20230113 20:06:47 @agent_ppo2.py:186][0m |          -0.0166 |           3.4628 |           6.5087 |
[32m[20230113 20:06:47 @agent_ppo2.py:186][0m |          -0.0014 |           3.4327 |           6.5034 |
[32m[20230113 20:06:47 @agent_ppo2.py:186][0m |          -0.0120 |           3.2285 |           6.4986 |
[32m[20230113 20:06:47 @agent_ppo2.py:186][0m |          -0.0135 |           3.1267 |           6.5060 |
[32m[20230113 20:06:47 @agent_ppo2.py:186][0m |          -0.0132 |           3.0577 |           6.5035 |
[32m[20230113 20:06:47 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:06:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.34
[32m[20230113 20:06:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.38
[32m[20230113 20:06:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.59
[32m[20230113 20:06:47 @agent_ppo2.py:144][0m Total time:      22.25 min
[32m[20230113 20:06:47 @agent_ppo2.py:146][0m 2025472 total steps have happened
[32m[20230113 20:06:47 @agent_ppo2.py:122][0m #------------------------ Iteration 989 --------------------------#
[32m[20230113 20:06:48 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:06:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:48 @agent_ppo2.py:186][0m |          -0.0029 |           7.0197 |           6.4041 |
[32m[20230113 20:06:48 @agent_ppo2.py:186][0m |          -0.0044 |           5.7527 |           6.3925 |
[32m[20230113 20:06:48 @agent_ppo2.py:186][0m |          -0.0092 |           5.1154 |           6.3981 |
[32m[20230113 20:06:48 @agent_ppo2.py:186][0m |          -0.0035 |           4.8429 |           6.3999 |
[32m[20230113 20:06:48 @agent_ppo2.py:186][0m |          -0.0161 |           4.5819 |           6.3908 |
[32m[20230113 20:06:48 @agent_ppo2.py:186][0m |          -0.0142 |           4.4308 |           6.3932 |
[32m[20230113 20:06:48 @agent_ppo2.py:186][0m |          -0.0087 |           4.5767 |           6.3909 |
[32m[20230113 20:06:48 @agent_ppo2.py:186][0m |          -0.0175 |           4.3127 |           6.3909 |
[32m[20230113 20:06:48 @agent_ppo2.py:186][0m |          -0.0133 |           4.1171 |           6.3960 |
[32m[20230113 20:06:48 @agent_ppo2.py:186][0m |          -0.0300 |           4.0099 |           6.3903 |
[32m[20230113 20:06:48 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:06:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.58
[32m[20230113 20:06:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.01
[32m[20230113 20:06:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.77
[32m[20230113 20:06:49 @agent_ppo2.py:144][0m Total time:      22.27 min
[32m[20230113 20:06:49 @agent_ppo2.py:146][0m 2027520 total steps have happened
[32m[20230113 20:06:49 @agent_ppo2.py:122][0m #------------------------ Iteration 990 --------------------------#
[32m[20230113 20:06:49 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 20:06:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:49 @agent_ppo2.py:186][0m |          -0.0008 |          19.4072 |           6.7525 |
[32m[20230113 20:06:49 @agent_ppo2.py:186][0m |          -0.0026 |           6.4965 |           6.7501 |
[32m[20230113 20:06:49 @agent_ppo2.py:186][0m |          -0.0065 |           4.7788 |           6.7516 |
[32m[20230113 20:06:49 @agent_ppo2.py:186][0m |          -0.0090 |           4.2230 |           6.7446 |
[32m[20230113 20:06:49 @agent_ppo2.py:186][0m |          -0.0108 |           3.8299 |           6.7465 |
[32m[20230113 20:06:49 @agent_ppo2.py:186][0m |          -0.0111 |           3.5210 |           6.7433 |
[32m[20230113 20:06:49 @agent_ppo2.py:186][0m |          -0.0142 |           3.3239 |           6.7432 |
[32m[20230113 20:06:49 @agent_ppo2.py:186][0m |          -0.0141 |           3.1537 |           6.7411 |
[32m[20230113 20:06:49 @agent_ppo2.py:186][0m |          -0.0110 |           2.9959 |           6.7376 |
[32m[20230113 20:06:49 @agent_ppo2.py:186][0m |          -0.0142 |           2.8314 |           6.7344 |
[32m[20230113 20:06:49 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:06:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 122.01
[32m[20230113 20:06:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.33
[32m[20230113 20:06:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 155.53
[32m[20230113 20:06:50 @agent_ppo2.py:144][0m Total time:      22.28 min
[32m[20230113 20:06:50 @agent_ppo2.py:146][0m 2029568 total steps have happened
[32m[20230113 20:06:50 @agent_ppo2.py:122][0m #------------------------ Iteration 991 --------------------------#
[32m[20230113 20:06:50 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:50 @agent_ppo2.py:186][0m |          -0.0003 |           7.8741 |           6.5248 |
[32m[20230113 20:06:50 @agent_ppo2.py:186][0m |          -0.0071 |           6.2282 |           6.5097 |
[32m[20230113 20:06:50 @agent_ppo2.py:186][0m |          -0.0105 |           5.6401 |           6.5090 |
[32m[20230113 20:06:50 @agent_ppo2.py:186][0m |          -0.0104 |           5.2733 |           6.5167 |
[32m[20230113 20:06:50 @agent_ppo2.py:186][0m |          -0.0111 |           5.0199 |           6.5153 |
[32m[20230113 20:06:50 @agent_ppo2.py:186][0m |          -0.0150 |           4.7963 |           6.5138 |
[32m[20230113 20:06:50 @agent_ppo2.py:186][0m |          -0.0061 |           4.7002 |           6.5106 |
[32m[20230113 20:06:50 @agent_ppo2.py:186][0m |          -0.0146 |           4.4942 |           6.5139 |
[32m[20230113 20:06:50 @agent_ppo2.py:186][0m |          -0.0122 |           4.3757 |           6.5160 |
[32m[20230113 20:06:51 @agent_ppo2.py:186][0m |          -0.0125 |           4.3513 |           6.5169 |
[32m[20230113 20:06:51 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.90
[32m[20230113 20:06:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.76
[32m[20230113 20:06:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 130.49
[32m[20230113 20:06:51 @agent_ppo2.py:144][0m Total time:      22.30 min
[32m[20230113 20:06:51 @agent_ppo2.py:146][0m 2031616 total steps have happened
[32m[20230113 20:06:51 @agent_ppo2.py:122][0m #------------------------ Iteration 992 --------------------------#
[32m[20230113 20:06:51 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:06:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:51 @agent_ppo2.py:186][0m |          -0.0004 |           6.3497 |           6.7017 |
[32m[20230113 20:06:51 @agent_ppo2.py:186][0m |          -0.0038 |           4.4391 |           6.6994 |
[32m[20230113 20:06:51 @agent_ppo2.py:186][0m |          -0.0075 |           3.8661 |           6.6944 |
[32m[20230113 20:06:52 @agent_ppo2.py:186][0m |          -0.0095 |           3.5748 |           6.6919 |
[32m[20230113 20:06:52 @agent_ppo2.py:186][0m |          -0.0092 |           3.3987 |           6.6930 |
[32m[20230113 20:06:52 @agent_ppo2.py:186][0m |          -0.0109 |           3.2259 |           6.6841 |
[32m[20230113 20:06:52 @agent_ppo2.py:186][0m |          -0.0115 |           3.1327 |           6.6868 |
[32m[20230113 20:06:52 @agent_ppo2.py:186][0m |          -0.0108 |           3.0122 |           6.6898 |
[32m[20230113 20:06:52 @agent_ppo2.py:186][0m |          -0.0113 |           2.8979 |           6.6908 |
[32m[20230113 20:06:52 @agent_ppo2.py:186][0m |          -0.0133 |           2.8133 |           6.6901 |
[32m[20230113 20:06:52 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:06:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.44
[32m[20230113 20:06:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.49
[32m[20230113 20:06:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.43
[32m[20230113 20:06:52 @agent_ppo2.py:144][0m Total time:      22.33 min
[32m[20230113 20:06:52 @agent_ppo2.py:146][0m 2033664 total steps have happened
[32m[20230113 20:06:52 @agent_ppo2.py:122][0m #------------------------ Iteration 993 --------------------------#
[32m[20230113 20:06:53 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:06:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:53 @agent_ppo2.py:186][0m |          -0.0029 |           6.9629 |           6.7195 |
[32m[20230113 20:06:53 @agent_ppo2.py:186][0m |          -0.0095 |           5.6544 |           6.7121 |
[32m[20230113 20:06:53 @agent_ppo2.py:186][0m |          -0.0067 |           5.1142 |           6.7141 |
[32m[20230113 20:06:53 @agent_ppo2.py:186][0m |          -0.0107 |           4.7546 |           6.7146 |
[32m[20230113 20:06:53 @agent_ppo2.py:186][0m |          -0.0136 |           4.4833 |           6.7061 |
[32m[20230113 20:06:53 @agent_ppo2.py:186][0m |          -0.0096 |           4.3564 |           6.7093 |
[32m[20230113 20:06:53 @agent_ppo2.py:186][0m |          -0.0138 |           4.1488 |           6.7059 |
[32m[20230113 20:06:53 @agent_ppo2.py:186][0m |          -0.0124 |           4.0797 |           6.7045 |
[32m[20230113 20:06:53 @agent_ppo2.py:186][0m |          -0.0169 |           3.9645 |           6.7050 |
[32m[20230113 20:06:53 @agent_ppo2.py:186][0m |          -0.0143 |           3.9094 |           6.7001 |
[32m[20230113 20:06:53 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:06:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.49
[32m[20230113 20:06:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.86
[32m[20230113 20:06:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.65
[32m[20230113 20:06:53 @agent_ppo2.py:144][0m Total time:      22.35 min
[32m[20230113 20:06:53 @agent_ppo2.py:146][0m 2035712 total steps have happened
[32m[20230113 20:06:53 @agent_ppo2.py:122][0m #------------------------ Iteration 994 --------------------------#
[32m[20230113 20:06:54 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:06:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:54 @agent_ppo2.py:186][0m |           0.0027 |           5.8888 |           6.6951 |
[32m[20230113 20:06:54 @agent_ppo2.py:186][0m |          -0.0016 |           4.6933 |           6.6780 |
[32m[20230113 20:06:54 @agent_ppo2.py:186][0m |          -0.0108 |           4.1678 |           6.6768 |
[32m[20230113 20:06:54 @agent_ppo2.py:186][0m |          -0.0088 |           3.8818 |           6.6668 |
[32m[20230113 20:06:54 @agent_ppo2.py:186][0m |          -0.0136 |           3.6031 |           6.6720 |
[32m[20230113 20:06:54 @agent_ppo2.py:186][0m |          -0.0121 |           3.3781 |           6.6667 |
[32m[20230113 20:06:54 @agent_ppo2.py:186][0m |          -0.0088 |           3.2260 |           6.6623 |
[32m[20230113 20:06:54 @agent_ppo2.py:186][0m |          -0.0096 |           3.2195 |           6.6608 |
[32m[20230113 20:06:54 @agent_ppo2.py:186][0m |          -0.0152 |           2.9767 |           6.6588 |
[32m[20230113 20:06:54 @agent_ppo2.py:186][0m |          -0.0145 |           2.8859 |           6.6616 |
[32m[20230113 20:06:54 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:06:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.20
[32m[20230113 20:06:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.94
[32m[20230113 20:06:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.13
[32m[20230113 20:06:55 @agent_ppo2.py:144][0m Total time:      22.37 min
[32m[20230113 20:06:55 @agent_ppo2.py:146][0m 2037760 total steps have happened
[32m[20230113 20:06:55 @agent_ppo2.py:122][0m #------------------------ Iteration 995 --------------------------#
[32m[20230113 20:06:55 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:55 @agent_ppo2.py:186][0m |          -0.0001 |           5.4386 |           6.5681 |
[32m[20230113 20:06:55 @agent_ppo2.py:186][0m |          -0.0069 |           4.4820 |           6.5638 |
[32m[20230113 20:06:55 @agent_ppo2.py:186][0m |          -0.0098 |           4.1310 |           6.5537 |
[32m[20230113 20:06:55 @agent_ppo2.py:186][0m |          -0.0115 |           3.9493 |           6.5543 |
[32m[20230113 20:06:55 @agent_ppo2.py:186][0m |          -0.0121 |           3.7395 |           6.5530 |
[32m[20230113 20:06:56 @agent_ppo2.py:186][0m |          -0.0125 |           3.6265 |           6.5523 |
[32m[20230113 20:06:56 @agent_ppo2.py:186][0m |          -0.0139 |           3.5525 |           6.5549 |
[32m[20230113 20:06:56 @agent_ppo2.py:186][0m |          -0.0128 |           3.4464 |           6.5448 |
[32m[20230113 20:06:56 @agent_ppo2.py:186][0m |          -0.0140 |           3.3345 |           6.5517 |
[32m[20230113 20:06:56 @agent_ppo2.py:186][0m |          -0.0152 |           3.2813 |           6.5570 |
[32m[20230113 20:06:56 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.54
[32m[20230113 20:06:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.16
[32m[20230113 20:06:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 160.59
[32m[20230113 20:06:56 @agent_ppo2.py:144][0m Total time:      22.39 min
[32m[20230113 20:06:56 @agent_ppo2.py:146][0m 2039808 total steps have happened
[32m[20230113 20:06:56 @agent_ppo2.py:122][0m #------------------------ Iteration 996 --------------------------#
[32m[20230113 20:06:57 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:06:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:57 @agent_ppo2.py:186][0m |           0.0017 |           5.8334 |           6.5258 |
[32m[20230113 20:06:57 @agent_ppo2.py:186][0m |           0.0060 |           4.7057 |           6.5153 |
[32m[20230113 20:06:57 @agent_ppo2.py:186][0m |          -0.0008 |           4.0680 |           6.4984 |
[32m[20230113 20:06:57 @agent_ppo2.py:186][0m |           0.0175 |           4.6430 |           6.4918 |
[32m[20230113 20:06:57 @agent_ppo2.py:186][0m |          -0.0096 |           4.0860 |           6.4874 |
[32m[20230113 20:06:57 @agent_ppo2.py:186][0m |          -0.0035 |           3.5149 |           6.4885 |
[32m[20230113 20:06:57 @agent_ppo2.py:186][0m |          -0.0113 |           3.3279 |           6.4878 |
[32m[20230113 20:06:57 @agent_ppo2.py:186][0m |          -0.0057 |           3.2355 |           6.4891 |
[32m[20230113 20:06:57 @agent_ppo2.py:186][0m |          -0.0066 |           3.1779 |           6.4848 |
[32m[20230113 20:06:57 @agent_ppo2.py:186][0m |          -0.0106 |           3.0850 |           6.4846 |
[32m[20230113 20:06:57 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:06:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.08
[32m[20230113 20:06:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.74
[32m[20230113 20:06:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.10
[32m[20230113 20:06:57 @agent_ppo2.py:144][0m Total time:      22.41 min
[32m[20230113 20:06:57 @agent_ppo2.py:146][0m 2041856 total steps have happened
[32m[20230113 20:06:57 @agent_ppo2.py:122][0m #------------------------ Iteration 997 --------------------------#
[32m[20230113 20:06:58 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:06:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:58 @agent_ppo2.py:186][0m |           0.0012 |          21.4432 |           6.5459 |
[32m[20230113 20:06:58 @agent_ppo2.py:186][0m |          -0.0029 |           9.1578 |           6.5491 |
[32m[20230113 20:06:58 @agent_ppo2.py:186][0m |          -0.0074 |           7.4189 |           6.5425 |
[32m[20230113 20:06:58 @agent_ppo2.py:186][0m |          -0.0088 |           6.4608 |           6.5378 |
[32m[20230113 20:06:58 @agent_ppo2.py:186][0m |          -0.0107 |           6.0047 |           6.5341 |
[32m[20230113 20:06:58 @agent_ppo2.py:186][0m |          -0.0118 |           5.5693 |           6.5319 |
[32m[20230113 20:06:58 @agent_ppo2.py:186][0m |          -0.0130 |           5.3577 |           6.5306 |
[32m[20230113 20:06:58 @agent_ppo2.py:186][0m |          -0.0143 |           5.1636 |           6.5280 |
[32m[20230113 20:06:58 @agent_ppo2.py:186][0m |          -0.0154 |           4.9194 |           6.5274 |
[32m[20230113 20:06:58 @agent_ppo2.py:186][0m |          -0.0165 |           4.8755 |           6.5242 |
[32m[20230113 20:06:58 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:06:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 28.21
[32m[20230113 20:06:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.73
[32m[20230113 20:06:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.91
[32m[20230113 20:06:58 @agent_ppo2.py:144][0m Total time:      22.43 min
[32m[20230113 20:06:58 @agent_ppo2.py:146][0m 2043904 total steps have happened
[32m[20230113 20:06:58 @agent_ppo2.py:122][0m #------------------------ Iteration 998 --------------------------#
[32m[20230113 20:06:59 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:06:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:06:59 @agent_ppo2.py:186][0m |           0.0006 |           8.3257 |           6.5054 |
[32m[20230113 20:06:59 @agent_ppo2.py:186][0m |          -0.0052 |           5.5965 |           6.4977 |
[32m[20230113 20:06:59 @agent_ppo2.py:186][0m |          -0.0071 |           4.9791 |           6.4939 |
[32m[20230113 20:06:59 @agent_ppo2.py:186][0m |          -0.0091 |           4.6364 |           6.4968 |
[32m[20230113 20:06:59 @agent_ppo2.py:186][0m |          -0.0100 |           4.3559 |           6.4964 |
[32m[20230113 20:06:59 @agent_ppo2.py:186][0m |          -0.0104 |           4.1498 |           6.4920 |
[32m[20230113 20:06:59 @agent_ppo2.py:186][0m |          -0.0105 |           3.9945 |           6.4893 |
[32m[20230113 20:06:59 @agent_ppo2.py:186][0m |          -0.0111 |           3.8528 |           6.4950 |
[32m[20230113 20:06:59 @agent_ppo2.py:186][0m |          -0.0123 |           3.7181 |           6.4896 |
[32m[20230113 20:06:59 @agent_ppo2.py:186][0m |          -0.0124 |           3.5899 |           6.4845 |
[32m[20230113 20:06:59 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:07:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.41
[32m[20230113 20:07:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.53
[32m[20230113 20:07:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.32
[32m[20230113 20:07:00 @agent_ppo2.py:144][0m Total time:      22.45 min
[32m[20230113 20:07:00 @agent_ppo2.py:146][0m 2045952 total steps have happened
[32m[20230113 20:07:00 @agent_ppo2.py:122][0m #------------------------ Iteration 999 --------------------------#
[32m[20230113 20:07:00 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:07:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:00 @agent_ppo2.py:186][0m |           0.0004 |           6.1912 |           6.6305 |
[32m[20230113 20:07:00 @agent_ppo2.py:186][0m |          -0.0048 |           5.1866 |           6.6246 |
[32m[20230113 20:07:00 @agent_ppo2.py:186][0m |          -0.0076 |           4.7482 |           6.6208 |
[32m[20230113 20:07:00 @agent_ppo2.py:186][0m |          -0.0086 |           4.5319 |           6.6155 |
[32m[20230113 20:07:00 @agent_ppo2.py:186][0m |          -0.0110 |           4.3414 |           6.6150 |
[32m[20230113 20:07:00 @agent_ppo2.py:186][0m |          -0.0107 |           4.2105 |           6.6166 |
[32m[20230113 20:07:00 @agent_ppo2.py:186][0m |          -0.0122 |           4.0769 |           6.6113 |
[32m[20230113 20:07:00 @agent_ppo2.py:186][0m |          -0.0123 |           3.9700 |           6.6132 |
[32m[20230113 20:07:01 @agent_ppo2.py:186][0m |          -0.0137 |           3.8743 |           6.6160 |
[32m[20230113 20:07:01 @agent_ppo2.py:186][0m |          -0.0134 |           3.8094 |           6.6148 |
[32m[20230113 20:07:01 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:07:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.26
[32m[20230113 20:07:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.71
[32m[20230113 20:07:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.08
[32m[20230113 20:07:01 @agent_ppo2.py:104][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 258.49
[32m[20230113 20:07:01 @agent_ppo2.py:144][0m Total time:      22.47 min
[32m[20230113 20:07:01 @agent_ppo2.py:146][0m 2048000 total steps have happened
[32m[20230113 20:07:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1000 --------------------------#
[32m[20230113 20:07:01 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:07:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:01 @agent_ppo2.py:186][0m |          -0.0005 |           6.0784 |           6.7177 |
[32m[20230113 20:07:02 @agent_ppo2.py:186][0m |          -0.0056 |           5.0592 |           6.7012 |
[32m[20230113 20:07:02 @agent_ppo2.py:186][0m |          -0.0076 |           4.6031 |           6.7055 |
[32m[20230113 20:07:02 @agent_ppo2.py:186][0m |          -0.0092 |           4.3550 |           6.7062 |
[32m[20230113 20:07:02 @agent_ppo2.py:186][0m |          -0.0096 |           4.1147 |           6.6977 |
[32m[20230113 20:07:02 @agent_ppo2.py:186][0m |          -0.0110 |           3.9398 |           6.6998 |
[32m[20230113 20:07:02 @agent_ppo2.py:186][0m |          -0.0114 |           3.8228 |           6.6958 |
[32m[20230113 20:07:02 @agent_ppo2.py:186][0m |          -0.0111 |           3.7510 |           6.7023 |
[32m[20230113 20:07:02 @agent_ppo2.py:186][0m |          -0.0123 |           3.5792 |           6.6931 |
[32m[20230113 20:07:02 @agent_ppo2.py:186][0m |          -0.0124 |           3.5176 |           6.6920 |
[32m[20230113 20:07:02 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:07:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.91
[32m[20230113 20:07:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.14
[32m[20230113 20:07:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 178.77
[32m[20230113 20:07:02 @agent_ppo2.py:144][0m Total time:      22.49 min
[32m[20230113 20:07:02 @agent_ppo2.py:146][0m 2050048 total steps have happened
[32m[20230113 20:07:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1001 --------------------------#
[32m[20230113 20:07:03 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:03 @agent_ppo2.py:186][0m |           0.0014 |           6.1990 |           6.5698 |
[32m[20230113 20:07:03 @agent_ppo2.py:186][0m |          -0.0045 |           5.0605 |           6.5544 |
[32m[20230113 20:07:03 @agent_ppo2.py:186][0m |          -0.0064 |           4.5876 |           6.5479 |
[32m[20230113 20:07:03 @agent_ppo2.py:186][0m |          -0.0079 |           4.3331 |           6.5471 |
[32m[20230113 20:07:03 @agent_ppo2.py:186][0m |          -0.0090 |           4.1617 |           6.5496 |
[32m[20230113 20:07:03 @agent_ppo2.py:186][0m |          -0.0108 |           4.0513 |           6.5433 |
[32m[20230113 20:07:03 @agent_ppo2.py:186][0m |          -0.0119 |           3.8892 |           6.5520 |
[32m[20230113 20:07:03 @agent_ppo2.py:186][0m |          -0.0127 |           3.7906 |           6.5396 |
[32m[20230113 20:07:03 @agent_ppo2.py:186][0m |          -0.0119 |           3.6937 |           6.5405 |
[32m[20230113 20:07:03 @agent_ppo2.py:186][0m |          -0.0133 |           3.6091 |           6.5430 |
[32m[20230113 20:07:03 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:07:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.60
[32m[20230113 20:07:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.97
[32m[20230113 20:07:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.87
[32m[20230113 20:07:03 @agent_ppo2.py:144][0m Total time:      22.52 min
[32m[20230113 20:07:03 @agent_ppo2.py:146][0m 2052096 total steps have happened
[32m[20230113 20:07:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1002 --------------------------#
[32m[20230113 20:07:04 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:04 @agent_ppo2.py:186][0m |          -0.0011 |           6.1490 |           6.6035 |
[32m[20230113 20:07:04 @agent_ppo2.py:186][0m |          -0.0078 |           4.2242 |           6.5989 |
[32m[20230113 20:07:04 @agent_ppo2.py:186][0m |          -0.0112 |           3.6796 |           6.5990 |
[32m[20230113 20:07:04 @agent_ppo2.py:186][0m |          -0.0089 |           3.3999 |           6.5924 |
[32m[20230113 20:07:04 @agent_ppo2.py:186][0m |          -0.0098 |           3.1915 |           6.5973 |
[32m[20230113 20:07:04 @agent_ppo2.py:186][0m |          -0.0118 |           3.0283 |           6.5989 |
[32m[20230113 20:07:04 @agent_ppo2.py:186][0m |          -0.0096 |           2.9472 |           6.5981 |
[32m[20230113 20:07:04 @agent_ppo2.py:186][0m |          -0.0148 |           2.8152 |           6.5922 |
[32m[20230113 20:07:04 @agent_ppo2.py:186][0m |          -0.0163 |           2.7269 |           6.5990 |
[32m[20230113 20:07:04 @agent_ppo2.py:186][0m |          -0.0175 |           2.6703 |           6.5946 |
[32m[20230113 20:07:04 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:07:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.59
[32m[20230113 20:07:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.62
[32m[20230113 20:07:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.84
[32m[20230113 20:07:05 @agent_ppo2.py:144][0m Total time:      22.54 min
[32m[20230113 20:07:05 @agent_ppo2.py:146][0m 2054144 total steps have happened
[32m[20230113 20:07:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1003 --------------------------#
[32m[20230113 20:07:05 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:07:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:05 @agent_ppo2.py:186][0m |          -0.0003 |           6.3654 |           6.6182 |
[32m[20230113 20:07:05 @agent_ppo2.py:186][0m |          -0.0043 |           5.1251 |           6.6113 |
[32m[20230113 20:07:05 @agent_ppo2.py:186][0m |          -0.0086 |           4.5385 |           6.6060 |
[32m[20230113 20:07:05 @agent_ppo2.py:186][0m |          -0.0124 |           4.2306 |           6.6035 |
[32m[20230113 20:07:05 @agent_ppo2.py:186][0m |          -0.0106 |           4.0159 |           6.6022 |
[32m[20230113 20:07:05 @agent_ppo2.py:186][0m |          -0.0105 |           3.8501 |           6.6016 |
[32m[20230113 20:07:06 @agent_ppo2.py:186][0m |          -0.0101 |           3.7715 |           6.5997 |
[32m[20230113 20:07:06 @agent_ppo2.py:186][0m |          -0.0123 |           3.6135 |           6.5925 |
[32m[20230113 20:07:06 @agent_ppo2.py:186][0m |          -0.0134 |           3.5131 |           6.5933 |
[32m[20230113 20:07:06 @agent_ppo2.py:186][0m |          -0.0137 |           3.4586 |           6.5927 |
[32m[20230113 20:07:06 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:07:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.36
[32m[20230113 20:07:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.87
[32m[20230113 20:07:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.48
[32m[20230113 20:07:06 @agent_ppo2.py:144][0m Total time:      22.56 min
[32m[20230113 20:07:06 @agent_ppo2.py:146][0m 2056192 total steps have happened
[32m[20230113 20:07:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1004 --------------------------#
[32m[20230113 20:07:06 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:07 @agent_ppo2.py:186][0m |          -0.0015 |           6.4051 |           6.7187 |
[32m[20230113 20:07:07 @agent_ppo2.py:186][0m |          -0.0045 |           5.1199 |           6.7100 |
[32m[20230113 20:07:07 @agent_ppo2.py:186][0m |          -0.0067 |           4.6399 |           6.7013 |
[32m[20230113 20:07:07 @agent_ppo2.py:186][0m |          -0.0039 |           4.3063 |           6.7023 |
[32m[20230113 20:07:07 @agent_ppo2.py:186][0m |          -0.0085 |           4.1240 |           6.6975 |
[32m[20230113 20:07:07 @agent_ppo2.py:186][0m |          -0.0090 |           3.9914 |           6.6934 |
[32m[20230113 20:07:07 @agent_ppo2.py:186][0m |          -0.0103 |           3.8051 |           6.7024 |
[32m[20230113 20:07:07 @agent_ppo2.py:186][0m |          -0.0118 |           3.7392 |           6.7005 |
[32m[20230113 20:07:07 @agent_ppo2.py:186][0m |          -0.0129 |           3.6276 |           6.7010 |
[32m[20230113 20:07:07 @agent_ppo2.py:186][0m |          -0.0131 |           3.6096 |           6.6990 |
[32m[20230113 20:07:07 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:07:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.94
[32m[20230113 20:07:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.89
[32m[20230113 20:07:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.79
[32m[20230113 20:07:07 @agent_ppo2.py:144][0m Total time:      22.58 min
[32m[20230113 20:07:07 @agent_ppo2.py:146][0m 2058240 total steps have happened
[32m[20230113 20:07:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1005 --------------------------#
[32m[20230113 20:07:08 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:08 @agent_ppo2.py:186][0m |          -0.0069 |           5.7213 |           6.3753 |
[32m[20230113 20:07:08 @agent_ppo2.py:186][0m |          -0.0147 |           4.8993 |           6.3583 |
[32m[20230113 20:07:08 @agent_ppo2.py:186][0m |          -0.0098 |           4.4998 |           6.3660 |
[32m[20230113 20:07:08 @agent_ppo2.py:186][0m |          -0.0106 |           4.2616 |           6.3530 |
[32m[20230113 20:07:08 @agent_ppo2.py:186][0m |           0.0026 |           4.0180 |           6.3675 |
[32m[20230113 20:07:08 @agent_ppo2.py:186][0m |          -0.0156 |           3.9215 |           6.3586 |
[32m[20230113 20:07:08 @agent_ppo2.py:186][0m |          -0.0111 |           3.7889 |           6.3651 |
[32m[20230113 20:07:08 @agent_ppo2.py:186][0m |          -0.0064 |           3.8089 |           6.3630 |
[32m[20230113 20:07:08 @agent_ppo2.py:186][0m |          -0.0264 |           3.6009 |           6.3659 |
[32m[20230113 20:07:08 @agent_ppo2.py:186][0m |          -0.0192 |           3.5022 |           6.3637 |
[32m[20230113 20:07:08 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:07:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.51
[32m[20230113 20:07:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.51
[32m[20230113 20:07:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 143.95
[32m[20230113 20:07:09 @agent_ppo2.py:144][0m Total time:      22.60 min
[32m[20230113 20:07:09 @agent_ppo2.py:146][0m 2060288 total steps have happened
[32m[20230113 20:07:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1006 --------------------------#
[32m[20230113 20:07:09 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:07:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:09 @agent_ppo2.py:186][0m |          -0.0023 |           6.0480 |           6.7031 |
[32m[20230113 20:07:09 @agent_ppo2.py:186][0m |          -0.0076 |           5.1842 |           6.7020 |
[32m[20230113 20:07:09 @agent_ppo2.py:186][0m |          -0.0075 |           4.7053 |           6.6994 |
[32m[20230113 20:07:09 @agent_ppo2.py:186][0m |          -0.0111 |           4.4088 |           6.7004 |
[32m[20230113 20:07:09 @agent_ppo2.py:186][0m |          -0.0111 |           4.2817 |           6.6938 |
[32m[20230113 20:07:09 @agent_ppo2.py:186][0m |          -0.0121 |           4.1343 |           6.6940 |
[32m[20230113 20:07:09 @agent_ppo2.py:186][0m |          -0.0121 |           3.9973 |           6.6933 |
[32m[20230113 20:07:09 @agent_ppo2.py:186][0m |          -0.0155 |           3.9288 |           6.6907 |
[32m[20230113 20:07:09 @agent_ppo2.py:186][0m |          -0.0161 |           3.8740 |           6.6935 |
[32m[20230113 20:07:09 @agent_ppo2.py:186][0m |          -0.0158 |           3.7149 |           6.6916 |
[32m[20230113 20:07:09 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:07:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.35
[32m[20230113 20:07:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.18
[32m[20230113 20:07:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.35
[32m[20230113 20:07:10 @agent_ppo2.py:144][0m Total time:      22.62 min
[32m[20230113 20:07:10 @agent_ppo2.py:146][0m 2062336 total steps have happened
[32m[20230113 20:07:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1007 --------------------------#
[32m[20230113 20:07:10 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:10 @agent_ppo2.py:186][0m |          -0.0008 |           5.3369 |           6.5936 |
[32m[20230113 20:07:10 @agent_ppo2.py:186][0m |          -0.0063 |           4.4828 |           6.5888 |
[32m[20230113 20:07:10 @agent_ppo2.py:186][0m |          -0.0083 |           4.0842 |           6.5879 |
[32m[20230113 20:07:11 @agent_ppo2.py:186][0m |          -0.0103 |           3.8274 |           6.5834 |
[32m[20230113 20:07:11 @agent_ppo2.py:186][0m |          -0.0113 |           3.6439 |           6.5806 |
[32m[20230113 20:07:11 @agent_ppo2.py:186][0m |          -0.0116 |           3.4920 |           6.5830 |
[32m[20230113 20:07:11 @agent_ppo2.py:186][0m |          -0.0128 |           3.3778 |           6.5861 |
[32m[20230113 20:07:11 @agent_ppo2.py:186][0m |          -0.0135 |           3.3081 |           6.5772 |
[32m[20230113 20:07:11 @agent_ppo2.py:186][0m |          -0.0139 |           3.2157 |           6.5870 |
[32m[20230113 20:07:11 @agent_ppo2.py:186][0m |          -0.0141 |           3.1397 |           6.5803 |
[32m[20230113 20:07:11 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:07:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.70
[32m[20230113 20:07:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.06
[32m[20230113 20:07:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.64
[32m[20230113 20:07:11 @agent_ppo2.py:144][0m Total time:      22.64 min
[32m[20230113 20:07:11 @agent_ppo2.py:146][0m 2064384 total steps have happened
[32m[20230113 20:07:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1008 --------------------------#
[32m[20230113 20:07:12 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:07:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:12 @agent_ppo2.py:186][0m |          -0.0007 |           5.8789 |           6.5328 |
[32m[20230113 20:07:12 @agent_ppo2.py:186][0m |          -0.0029 |           4.6245 |           6.5193 |
[32m[20230113 20:07:12 @agent_ppo2.py:186][0m |          -0.0070 |           4.1387 |           6.5230 |
[32m[20230113 20:07:12 @agent_ppo2.py:186][0m |          -0.0061 |           3.8466 |           6.5224 |
[32m[20230113 20:07:12 @agent_ppo2.py:186][0m |          -0.0087 |           3.6319 |           6.5143 |
[32m[20230113 20:07:12 @agent_ppo2.py:186][0m |          -0.0107 |           3.4792 |           6.5153 |
[32m[20230113 20:07:12 @agent_ppo2.py:186][0m |          -0.0117 |           3.3514 |           6.5110 |
[32m[20230113 20:07:12 @agent_ppo2.py:186][0m |          -0.0118 |           3.2302 |           6.5121 |
[32m[20230113 20:07:12 @agent_ppo2.py:186][0m |          -0.0139 |           3.1244 |           6.5134 |
[32m[20230113 20:07:12 @agent_ppo2.py:186][0m |          -0.0140 |           3.0461 |           6.5173 |
[32m[20230113 20:07:12 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:07:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.35
[32m[20230113 20:07:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.72
[32m[20230113 20:07:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.17
[32m[20230113 20:07:12 @agent_ppo2.py:144][0m Total time:      22.67 min
[32m[20230113 20:07:12 @agent_ppo2.py:146][0m 2066432 total steps have happened
[32m[20230113 20:07:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1009 --------------------------#
[32m[20230113 20:07:13 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:07:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:13 @agent_ppo2.py:186][0m |          -0.0013 |           7.4103 |           6.7082 |
[32m[20230113 20:07:13 @agent_ppo2.py:186][0m |          -0.0061 |           5.5855 |           6.7057 |
[32m[20230113 20:07:13 @agent_ppo2.py:186][0m |          -0.0073 |           4.7391 |           6.6987 |
[32m[20230113 20:07:13 @agent_ppo2.py:186][0m |          -0.0093 |           4.2548 |           6.6934 |
[32m[20230113 20:07:13 @agent_ppo2.py:186][0m |          -0.0089 |           3.9481 |           6.6965 |
[32m[20230113 20:07:13 @agent_ppo2.py:186][0m |          -0.0100 |           3.6872 |           6.6981 |
[32m[20230113 20:07:13 @agent_ppo2.py:186][0m |          -0.0113 |           3.4869 |           6.6931 |
[32m[20230113 20:07:13 @agent_ppo2.py:186][0m |          -0.0120 |           3.3556 |           6.6974 |
[32m[20230113 20:07:13 @agent_ppo2.py:186][0m |          -0.0138 |           3.2502 |           6.6902 |
[32m[20230113 20:07:13 @agent_ppo2.py:186][0m |          -0.0145 |           3.1243 |           6.6922 |
[32m[20230113 20:07:13 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:07:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.82
[32m[20230113 20:07:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.43
[32m[20230113 20:07:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.85
[32m[20230113 20:07:14 @agent_ppo2.py:144][0m Total time:      22.69 min
[32m[20230113 20:07:14 @agent_ppo2.py:146][0m 2068480 total steps have happened
[32m[20230113 20:07:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1010 --------------------------#
[32m[20230113 20:07:14 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 20:07:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:14 @agent_ppo2.py:186][0m |           0.0032 |          35.1182 |           6.6578 |
[32m[20230113 20:07:14 @agent_ppo2.py:186][0m |          -0.0058 |           6.6321 |           6.6600 |
[32m[20230113 20:07:14 @agent_ppo2.py:186][0m |          -0.0108 |           5.0445 |           6.6541 |
[32m[20230113 20:07:14 @agent_ppo2.py:186][0m |          -0.0121 |           4.5181 |           6.6514 |
[32m[20230113 20:07:14 @agent_ppo2.py:186][0m |          -0.0115 |           4.1907 |           6.6509 |
[32m[20230113 20:07:14 @agent_ppo2.py:186][0m |          -0.0144 |           3.9405 |           6.6465 |
[32m[20230113 20:07:14 @agent_ppo2.py:186][0m |          -0.0139 |           3.6736 |           6.6469 |
[32m[20230113 20:07:14 @agent_ppo2.py:186][0m |          -0.0148 |           3.5391 |           6.6466 |
[32m[20230113 20:07:14 @agent_ppo2.py:186][0m |          -0.0150 |           3.3708 |           6.6459 |
[32m[20230113 20:07:14 @agent_ppo2.py:186][0m |          -0.0131 |           3.2130 |           6.6464 |
[32m[20230113 20:07:14 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 20:07:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 115.70
[32m[20230113 20:07:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.51
[32m[20230113 20:07:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 103.60
[32m[20230113 20:07:15 @agent_ppo2.py:144][0m Total time:      22.71 min
[32m[20230113 20:07:15 @agent_ppo2.py:146][0m 2070528 total steps have happened
[32m[20230113 20:07:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1011 --------------------------#
[32m[20230113 20:07:15 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:07:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:15 @agent_ppo2.py:186][0m |          -0.0003 |           7.7444 |           6.5329 |
[32m[20230113 20:07:15 @agent_ppo2.py:186][0m |          -0.0041 |           5.9538 |           6.5282 |
[32m[20230113 20:07:16 @agent_ppo2.py:186][0m |          -0.0080 |           5.2340 |           6.5238 |
[32m[20230113 20:07:16 @agent_ppo2.py:186][0m |          -0.0101 |           4.9405 |           6.5203 |
[32m[20230113 20:07:16 @agent_ppo2.py:186][0m |          -0.0102 |           4.6881 |           6.5174 |
[32m[20230113 20:07:16 @agent_ppo2.py:186][0m |          -0.0137 |           4.4985 |           6.5164 |
[32m[20230113 20:07:16 @agent_ppo2.py:186][0m |          -0.0136 |           4.3362 |           6.5156 |
[32m[20230113 20:07:16 @agent_ppo2.py:186][0m |          -0.0137 |           4.2348 |           6.5094 |
[32m[20230113 20:07:16 @agent_ppo2.py:186][0m |          -0.0152 |           4.0924 |           6.5152 |
[32m[20230113 20:07:16 @agent_ppo2.py:186][0m |          -0.0152 |           4.0560 |           6.5185 |
[32m[20230113 20:07:16 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:07:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.42
[32m[20230113 20:07:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.27
[32m[20230113 20:07:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.36
[32m[20230113 20:07:16 @agent_ppo2.py:144][0m Total time:      22.73 min
[32m[20230113 20:07:16 @agent_ppo2.py:146][0m 2072576 total steps have happened
[32m[20230113 20:07:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1012 --------------------------#
[32m[20230113 20:07:17 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:07:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:17 @agent_ppo2.py:186][0m |          -0.0010 |          17.0267 |           6.6469 |
[32m[20230113 20:07:17 @agent_ppo2.py:186][0m |          -0.0073 |           6.7206 |           6.6312 |
[32m[20230113 20:07:17 @agent_ppo2.py:186][0m |          -0.0104 |           5.4179 |           6.6238 |
[32m[20230113 20:07:17 @agent_ppo2.py:186][0m |          -0.0117 |           4.9066 |           6.6203 |
[32m[20230113 20:07:17 @agent_ppo2.py:186][0m |          -0.0148 |           4.5313 |           6.6164 |
[32m[20230113 20:07:17 @agent_ppo2.py:186][0m |          -0.0150 |           4.3187 |           6.6157 |
[32m[20230113 20:07:17 @agent_ppo2.py:186][0m |          -0.0141 |           4.1702 |           6.6185 |
[32m[20230113 20:07:17 @agent_ppo2.py:186][0m |          -0.0163 |           3.9894 |           6.6105 |
[32m[20230113 20:07:17 @agent_ppo2.py:186][0m |          -0.0161 |           3.8706 |           6.6081 |
[32m[20230113 20:07:17 @agent_ppo2.py:186][0m |          -0.0159 |           3.7895 |           6.6040 |
[32m[20230113 20:07:17 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:07:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 108.43
[32m[20230113 20:07:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.92
[32m[20230113 20:07:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.98
[32m[20230113 20:07:18 @agent_ppo2.py:144][0m Total time:      22.75 min
[32m[20230113 20:07:18 @agent_ppo2.py:146][0m 2074624 total steps have happened
[32m[20230113 20:07:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1013 --------------------------#
[32m[20230113 20:07:18 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 20:07:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:18 @agent_ppo2.py:186][0m |           0.0018 |          25.2933 |           6.5079 |
[32m[20230113 20:07:18 @agent_ppo2.py:186][0m |          -0.0033 |           6.4106 |           6.4966 |
[32m[20230113 20:07:18 @agent_ppo2.py:186][0m |          -0.0097 |           5.0318 |           6.4944 |
[32m[20230113 20:07:18 @agent_ppo2.py:186][0m |          -0.0082 |           4.4251 |           6.4850 |
[32m[20230113 20:07:18 @agent_ppo2.py:186][0m |          -0.0087 |           3.9911 |           6.4923 |
[32m[20230113 20:07:18 @agent_ppo2.py:186][0m |          -0.0106 |           3.7053 |           6.4928 |
[32m[20230113 20:07:18 @agent_ppo2.py:186][0m |          -0.0145 |           3.4530 |           6.4888 |
[32m[20230113 20:07:18 @agent_ppo2.py:186][0m |          -0.0175 |           3.2765 |           6.4886 |
[32m[20230113 20:07:18 @agent_ppo2.py:186][0m |          -0.0191 |           3.0888 |           6.4873 |
[32m[20230113 20:07:18 @agent_ppo2.py:186][0m |          -0.0188 |           2.9932 |           6.4865 |
[32m[20230113 20:07:18 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 20:07:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 118.95
[32m[20230113 20:07:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.62
[32m[20230113 20:07:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.20
[32m[20230113 20:07:19 @agent_ppo2.py:144][0m Total time:      22.77 min
[32m[20230113 20:07:19 @agent_ppo2.py:146][0m 2076672 total steps have happened
[32m[20230113 20:07:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1014 --------------------------#
[32m[20230113 20:07:19 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:19 @agent_ppo2.py:186][0m |           0.0020 |           5.8575 |           6.5749 |
[32m[20230113 20:07:19 @agent_ppo2.py:186][0m |          -0.0061 |           4.6635 |           6.5706 |
[32m[20230113 20:07:19 @agent_ppo2.py:186][0m |          -0.0099 |           4.2072 |           6.5691 |
[32m[20230113 20:07:19 @agent_ppo2.py:186][0m |          -0.0099 |           3.9297 |           6.5619 |
[32m[20230113 20:07:19 @agent_ppo2.py:186][0m |          -0.0072 |           3.9169 |           6.5577 |
[32m[20230113 20:07:19 @agent_ppo2.py:186][0m |          -0.0112 |           3.6483 |           6.5508 |
[32m[20230113 20:07:19 @agent_ppo2.py:186][0m |          -0.0139 |           3.5487 |           6.5514 |
[32m[20230113 20:07:19 @agent_ppo2.py:186][0m |          -0.0158 |           3.4448 |           6.5467 |
[32m[20230113 20:07:20 @agent_ppo2.py:186][0m |          -0.0149 |           3.3540 |           6.5446 |
[32m[20230113 20:07:20 @agent_ppo2.py:186][0m |          -0.0127 |           3.2937 |           6.5420 |
[32m[20230113 20:07:20 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:07:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.20
[32m[20230113 20:07:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.69
[32m[20230113 20:07:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.48
[32m[20230113 20:07:20 @agent_ppo2.py:144][0m Total time:      22.79 min
[32m[20230113 20:07:20 @agent_ppo2.py:146][0m 2078720 total steps have happened
[32m[20230113 20:07:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1015 --------------------------#
[32m[20230113 20:07:20 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:07:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:20 @agent_ppo2.py:186][0m |          -0.0029 |          34.6316 |           6.6163 |
[32m[20230113 20:07:21 @agent_ppo2.py:186][0m |          -0.0038 |          17.6970 |           6.6060 |
[32m[20230113 20:07:21 @agent_ppo2.py:186][0m |          -0.0130 |          13.8256 |           6.6005 |
[32m[20230113 20:07:21 @agent_ppo2.py:186][0m |          -0.0054 |          11.8642 |           6.6015 |
[32m[20230113 20:07:21 @agent_ppo2.py:186][0m |          -0.0079 |          10.2628 |           6.6013 |
[32m[20230113 20:07:21 @agent_ppo2.py:186][0m |          -0.0128 |           9.3722 |           6.5936 |
[32m[20230113 20:07:21 @agent_ppo2.py:186][0m |          -0.0114 |           8.6680 |           6.5950 |
[32m[20230113 20:07:21 @agent_ppo2.py:186][0m |          -0.0173 |           7.7164 |           6.5933 |
[32m[20230113 20:07:21 @agent_ppo2.py:186][0m |          -0.0099 |           7.1753 |           6.5906 |
[32m[20230113 20:07:21 @agent_ppo2.py:186][0m |          -0.0161 |           6.6895 |           6.5852 |
[32m[20230113 20:07:21 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:07:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 75.07
[32m[20230113 20:07:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.20
[32m[20230113 20:07:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.88
[32m[20230113 20:07:21 @agent_ppo2.py:144][0m Total time:      22.81 min
[32m[20230113 20:07:21 @agent_ppo2.py:146][0m 2080768 total steps have happened
[32m[20230113 20:07:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1016 --------------------------#
[32m[20230113 20:07:22 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:22 @agent_ppo2.py:186][0m |           0.0066 |           7.7042 |           6.6139 |
[32m[20230113 20:07:22 @agent_ppo2.py:186][0m |           0.0015 |           5.9428 |           6.6027 |
[32m[20230113 20:07:22 @agent_ppo2.py:186][0m |          -0.0203 |           5.3387 |           6.5870 |
[32m[20230113 20:07:22 @agent_ppo2.py:186][0m |          -0.0149 |           5.0731 |           6.5966 |
[32m[20230113 20:07:22 @agent_ppo2.py:186][0m |          -0.0191 |           4.7189 |           6.5920 |
[32m[20230113 20:07:22 @agent_ppo2.py:186][0m |          -0.0180 |           4.4885 |           6.5869 |
[32m[20230113 20:07:22 @agent_ppo2.py:186][0m |          -0.0092 |           4.3531 |           6.5958 |
[32m[20230113 20:07:22 @agent_ppo2.py:186][0m |          -0.0098 |           4.1529 |           6.5922 |
[32m[20230113 20:07:22 @agent_ppo2.py:186][0m |          -0.0131 |           4.0607 |           6.5913 |
[32m[20230113 20:07:22 @agent_ppo2.py:186][0m |          -0.0190 |           3.9349 |           6.5850 |
[32m[20230113 20:07:22 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:07:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.48
[32m[20230113 20:07:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.92
[32m[20230113 20:07:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.18
[32m[20230113 20:07:23 @agent_ppo2.py:144][0m Total time:      22.83 min
[32m[20230113 20:07:23 @agent_ppo2.py:146][0m 2082816 total steps have happened
[32m[20230113 20:07:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1017 --------------------------#
[32m[20230113 20:07:23 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:23 @agent_ppo2.py:186][0m |          -0.0013 |           6.9409 |           6.4450 |
[32m[20230113 20:07:23 @agent_ppo2.py:186][0m |          -0.0041 |           5.2810 |           6.4339 |
[32m[20230113 20:07:23 @agent_ppo2.py:186][0m |          -0.0045 |           4.6223 |           6.4274 |
[32m[20230113 20:07:23 @agent_ppo2.py:186][0m |          -0.0098 |           4.1456 |           6.4329 |
[32m[20230113 20:07:23 @agent_ppo2.py:186][0m |          -0.0112 |           3.8730 |           6.4275 |
[32m[20230113 20:07:23 @agent_ppo2.py:186][0m |          -0.0138 |           3.6682 |           6.4305 |
[32m[20230113 20:07:23 @agent_ppo2.py:186][0m |          -0.0138 |           3.5172 |           6.4336 |
[32m[20230113 20:07:23 @agent_ppo2.py:186][0m |          -0.0117 |           3.3759 |           6.4299 |
[32m[20230113 20:07:23 @agent_ppo2.py:186][0m |          -0.0153 |           3.2553 |           6.4327 |
[32m[20230113 20:07:23 @agent_ppo2.py:186][0m |          -0.0165 |           3.1811 |           6.4398 |
[32m[20230113 20:07:23 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:07:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.82
[32m[20230113 20:07:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.25
[32m[20230113 20:07:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.27
[32m[20230113 20:07:24 @agent_ppo2.py:144][0m Total time:      22.86 min
[32m[20230113 20:07:24 @agent_ppo2.py:146][0m 2084864 total steps have happened
[32m[20230113 20:07:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1018 --------------------------#
[32m[20230113 20:07:24 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:07:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:24 @agent_ppo2.py:186][0m |           0.0066 |          19.0755 |           6.6258 |
[32m[20230113 20:07:24 @agent_ppo2.py:186][0m |          -0.0014 |           6.3245 |           6.6229 |
[32m[20230113 20:07:24 @agent_ppo2.py:186][0m |          -0.0037 |           5.1662 |           6.6236 |
[32m[20230113 20:07:24 @agent_ppo2.py:186][0m |          -0.0063 |           4.6605 |           6.6287 |
[32m[20230113 20:07:25 @agent_ppo2.py:186][0m |          -0.0042 |           4.3025 |           6.6226 |
[32m[20230113 20:07:25 @agent_ppo2.py:186][0m |          -0.0072 |           4.0319 |           6.6266 |
[32m[20230113 20:07:25 @agent_ppo2.py:186][0m |          -0.0070 |           3.7722 |           6.6245 |
[32m[20230113 20:07:25 @agent_ppo2.py:186][0m |          -0.0090 |           3.6044 |           6.6282 |
[32m[20230113 20:07:25 @agent_ppo2.py:186][0m |          -0.0101 |           3.4615 |           6.6264 |
[32m[20230113 20:07:25 @agent_ppo2.py:186][0m |          -0.0097 |           3.3307 |           6.6248 |
[32m[20230113 20:07:25 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:07:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 140.40
[32m[20230113 20:07:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.49
[32m[20230113 20:07:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 23.54
[32m[20230113 20:07:25 @agent_ppo2.py:144][0m Total time:      22.88 min
[32m[20230113 20:07:25 @agent_ppo2.py:146][0m 2086912 total steps have happened
[32m[20230113 20:07:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1019 --------------------------#
[32m[20230113 20:07:26 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:26 @agent_ppo2.py:186][0m |          -0.0020 |           6.3268 |           6.6433 |
[32m[20230113 20:07:26 @agent_ppo2.py:186][0m |          -0.0031 |           5.0761 |           6.6354 |
[32m[20230113 20:07:26 @agent_ppo2.py:186][0m |          -0.0095 |           4.6145 |           6.6366 |
[32m[20230113 20:07:26 @agent_ppo2.py:186][0m |          -0.0104 |           4.2217 |           6.6336 |
[32m[20230113 20:07:26 @agent_ppo2.py:186][0m |          -0.0109 |           3.9484 |           6.6327 |
[32m[20230113 20:07:26 @agent_ppo2.py:186][0m |          -0.0118 |           3.7427 |           6.6453 |
[32m[20230113 20:07:26 @agent_ppo2.py:186][0m |          -0.0146 |           3.5808 |           6.6427 |
[32m[20230113 20:07:26 @agent_ppo2.py:186][0m |          -0.0126 |           3.4296 |           6.6368 |
[32m[20230113 20:07:26 @agent_ppo2.py:186][0m |          -0.0154 |           3.3346 |           6.6412 |
[32m[20230113 20:07:26 @agent_ppo2.py:186][0m |          -0.0129 |           3.2260 |           6.6452 |
[32m[20230113 20:07:26 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:07:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.25
[32m[20230113 20:07:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.82
[32m[20230113 20:07:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.35
[32m[20230113 20:07:26 @agent_ppo2.py:144][0m Total time:      22.90 min
[32m[20230113 20:07:26 @agent_ppo2.py:146][0m 2088960 total steps have happened
[32m[20230113 20:07:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1020 --------------------------#
[32m[20230113 20:07:27 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:27 @agent_ppo2.py:186][0m |          -0.0000 |           5.0591 |           6.7199 |
[32m[20230113 20:07:27 @agent_ppo2.py:186][0m |          -0.0063 |           4.1221 |           6.7031 |
[32m[20230113 20:07:27 @agent_ppo2.py:186][0m |          -0.0086 |           3.8481 |           6.7121 |
[32m[20230113 20:07:27 @agent_ppo2.py:186][0m |          -0.0102 |           3.5914 |           6.7103 |
[32m[20230113 20:07:27 @agent_ppo2.py:186][0m |          -0.0111 |           3.4442 |           6.7060 |
[32m[20230113 20:07:27 @agent_ppo2.py:186][0m |          -0.0124 |           3.3691 |           6.7135 |
[32m[20230113 20:07:27 @agent_ppo2.py:186][0m |          -0.0130 |           3.2459 |           6.7149 |
[32m[20230113 20:07:27 @agent_ppo2.py:186][0m |          -0.0147 |           3.1618 |           6.7092 |
[32m[20230113 20:07:27 @agent_ppo2.py:186][0m |          -0.0148 |           3.0861 |           6.7142 |
[32m[20230113 20:07:27 @agent_ppo2.py:186][0m |          -0.0153 |           3.0473 |           6.7123 |
[32m[20230113 20:07:27 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:07:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.53
[32m[20230113 20:07:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.93
[32m[20230113 20:07:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 151.05
[32m[20230113 20:07:28 @agent_ppo2.py:144][0m Total time:      22.92 min
[32m[20230113 20:07:28 @agent_ppo2.py:146][0m 2091008 total steps have happened
[32m[20230113 20:07:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1021 --------------------------#
[32m[20230113 20:07:28 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:28 @agent_ppo2.py:186][0m |          -0.0026 |           5.5929 |           6.6557 |
[32m[20230113 20:07:28 @agent_ppo2.py:186][0m |          -0.0025 |           4.1894 |           6.6500 |
[32m[20230113 20:07:28 @agent_ppo2.py:186][0m |          -0.0097 |           3.6303 |           6.6516 |
[32m[20230113 20:07:28 @agent_ppo2.py:186][0m |          -0.0096 |           3.3818 |           6.6448 |
[32m[20230113 20:07:28 @agent_ppo2.py:186][0m |          -0.0129 |           3.1464 |           6.6454 |
[32m[20230113 20:07:28 @agent_ppo2.py:186][0m |          -0.0056 |           2.9685 |           6.6396 |
[32m[20230113 20:07:29 @agent_ppo2.py:186][0m |          -0.0088 |           2.8279 |           6.6440 |
[32m[20230113 20:07:29 @agent_ppo2.py:186][0m |          -0.0198 |           2.7357 |           6.6347 |
[32m[20230113 20:07:29 @agent_ppo2.py:186][0m |          -0.0125 |           2.6290 |           6.6359 |
[32m[20230113 20:07:29 @agent_ppo2.py:186][0m |          -0.0130 |           2.5842 |           6.6394 |
[32m[20230113 20:07:29 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:07:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.53
[32m[20230113 20:07:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.39
[32m[20230113 20:07:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.18
[32m[20230113 20:07:29 @agent_ppo2.py:144][0m Total time:      22.94 min
[32m[20230113 20:07:29 @agent_ppo2.py:146][0m 2093056 total steps have happened
[32m[20230113 20:07:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1022 --------------------------#
[32m[20230113 20:07:29 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:07:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:30 @agent_ppo2.py:186][0m |          -0.0030 |          13.8740 |           6.5792 |
[32m[20230113 20:07:30 @agent_ppo2.py:186][0m |          -0.0055 |           6.1849 |           6.5774 |
[32m[20230113 20:07:30 @agent_ppo2.py:186][0m |          -0.0042 |           5.3879 |           6.5739 |
[32m[20230113 20:07:30 @agent_ppo2.py:186][0m |          -0.0123 |           4.8516 |           6.5746 |
[32m[20230113 20:07:30 @agent_ppo2.py:186][0m |          -0.0134 |           4.4819 |           6.5690 |
[32m[20230113 20:07:30 @agent_ppo2.py:186][0m |          -0.0152 |           4.3134 |           6.5722 |
[32m[20230113 20:07:30 @agent_ppo2.py:186][0m |          -0.0184 |           3.9958 |           6.5716 |
[32m[20230113 20:07:30 @agent_ppo2.py:186][0m |          -0.0163 |           3.8328 |           6.5675 |
[32m[20230113 20:07:30 @agent_ppo2.py:186][0m |          -0.0184 |           3.6691 |           6.5693 |
[32m[20230113 20:07:30 @agent_ppo2.py:186][0m |          -0.0165 |           3.5580 |           6.5700 |
[32m[20230113 20:07:30 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:07:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 158.00
[32m[20230113 20:07:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.92
[32m[20230113 20:07:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.13
[32m[20230113 20:07:30 @agent_ppo2.py:144][0m Total time:      22.96 min
[32m[20230113 20:07:30 @agent_ppo2.py:146][0m 2095104 total steps have happened
[32m[20230113 20:07:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1023 --------------------------#
[32m[20230113 20:07:31 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:07:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:31 @agent_ppo2.py:186][0m |           0.0024 |           7.3916 |           6.6733 |
[32m[20230113 20:07:31 @agent_ppo2.py:186][0m |          -0.0034 |           5.4665 |           6.6738 |
[32m[20230113 20:07:31 @agent_ppo2.py:186][0m |          -0.0064 |           4.8268 |           6.6690 |
[32m[20230113 20:07:31 @agent_ppo2.py:186][0m |          -0.0058 |           4.4348 |           6.6670 |
[32m[20230113 20:07:31 @agent_ppo2.py:186][0m |          -0.0085 |           4.1408 |           6.6759 |
[32m[20230113 20:07:31 @agent_ppo2.py:186][0m |          -0.0092 |           3.8936 |           6.6677 |
[32m[20230113 20:07:31 @agent_ppo2.py:186][0m |          -0.0108 |           3.7444 |           6.6675 |
[32m[20230113 20:07:31 @agent_ppo2.py:186][0m |          -0.0112 |           3.5842 |           6.6740 |
[32m[20230113 20:07:31 @agent_ppo2.py:186][0m |          -0.0128 |           3.4699 |           6.6801 |
[32m[20230113 20:07:31 @agent_ppo2.py:186][0m |          -0.0138 |           3.3478 |           6.6728 |
[32m[20230113 20:07:31 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:07:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.94
[32m[20230113 20:07:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.18
[32m[20230113 20:07:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.44
[32m[20230113 20:07:32 @agent_ppo2.py:144][0m Total time:      22.98 min
[32m[20230113 20:07:32 @agent_ppo2.py:146][0m 2097152 total steps have happened
[32m[20230113 20:07:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1024 --------------------------#
[32m[20230113 20:07:32 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:07:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:32 @agent_ppo2.py:186][0m |          -0.0027 |           5.1757 |           6.6885 |
[32m[20230113 20:07:32 @agent_ppo2.py:186][0m |          -0.0063 |           3.8516 |           6.6711 |
[32m[20230113 20:07:32 @agent_ppo2.py:186][0m |          -0.0060 |           3.4529 |           6.6767 |
[32m[20230113 20:07:32 @agent_ppo2.py:186][0m |          -0.0061 |           3.3071 |           6.6724 |
[32m[20230113 20:07:32 @agent_ppo2.py:186][0m |          -0.0106 |           3.1390 |           6.6680 |
[32m[20230113 20:07:32 @agent_ppo2.py:186][0m |          -0.0079 |           3.0289 |           6.6807 |
[32m[20230113 20:07:32 @agent_ppo2.py:186][0m |          -0.0112 |           2.9569 |           6.6700 |
[32m[20230113 20:07:32 @agent_ppo2.py:186][0m |          -0.0117 |           2.8818 |           6.6804 |
[32m[20230113 20:07:32 @agent_ppo2.py:186][0m |          -0.0103 |           2.8239 |           6.6760 |
[32m[20230113 20:07:33 @agent_ppo2.py:186][0m |          -0.0110 |           2.7490 |           6.6671 |
[32m[20230113 20:07:33 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:07:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.93
[32m[20230113 20:07:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.40
[32m[20230113 20:07:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.61
[32m[20230113 20:07:33 @agent_ppo2.py:144][0m Total time:      23.01 min
[32m[20230113 20:07:33 @agent_ppo2.py:146][0m 2099200 total steps have happened
[32m[20230113 20:07:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1025 --------------------------#
[32m[20230113 20:07:33 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:07:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:33 @agent_ppo2.py:186][0m |           0.0063 |           6.7673 |           6.7765 |
[32m[20230113 20:07:34 @agent_ppo2.py:186][0m |          -0.0101 |           5.6337 |           6.7595 |
[32m[20230113 20:07:34 @agent_ppo2.py:186][0m |          -0.0171 |           5.3036 |           6.7584 |
[32m[20230113 20:07:34 @agent_ppo2.py:186][0m |          -0.0147 |           4.9044 |           6.7560 |
[32m[20230113 20:07:34 @agent_ppo2.py:186][0m |          -0.0188 |           4.5431 |           6.7603 |
[32m[20230113 20:07:34 @agent_ppo2.py:186][0m |          -0.0098 |           4.3464 |           6.7487 |
[32m[20230113 20:07:34 @agent_ppo2.py:186][0m |          -0.0075 |           4.1395 |           6.7644 |
[32m[20230113 20:07:34 @agent_ppo2.py:186][0m |          -0.0263 |           3.9949 |           6.7532 |
[32m[20230113 20:07:34 @agent_ppo2.py:186][0m |          -0.0141 |           3.9061 |           6.7512 |
[32m[20230113 20:07:34 @agent_ppo2.py:186][0m |          -0.0212 |           3.7876 |           6.7514 |
[32m[20230113 20:07:34 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:07:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.11
[32m[20230113 20:07:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.28
[32m[20230113 20:07:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 129.64
[32m[20230113 20:07:34 @agent_ppo2.py:144][0m Total time:      23.03 min
[32m[20230113 20:07:34 @agent_ppo2.py:146][0m 2101248 total steps have happened
[32m[20230113 20:07:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1026 --------------------------#
[32m[20230113 20:07:35 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 20:07:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:35 @agent_ppo2.py:186][0m |          -0.0006 |          19.1100 |           6.6584 |
[32m[20230113 20:07:35 @agent_ppo2.py:186][0m |          -0.0059 |           7.9671 |           6.6544 |
[32m[20230113 20:07:35 @agent_ppo2.py:186][0m |          -0.0088 |           6.2227 |           6.6657 |
[32m[20230113 20:07:35 @agent_ppo2.py:186][0m |          -0.0105 |           5.3648 |           6.6574 |
[32m[20230113 20:07:35 @agent_ppo2.py:186][0m |          -0.0115 |           4.8213 |           6.6559 |
[32m[20230113 20:07:35 @agent_ppo2.py:186][0m |          -0.0127 |           4.3824 |           6.6506 |
[32m[20230113 20:07:35 @agent_ppo2.py:186][0m |          -0.0140 |           4.1183 |           6.6506 |
[32m[20230113 20:07:35 @agent_ppo2.py:186][0m |          -0.0134 |           4.0139 |           6.6489 |
[32m[20230113 20:07:35 @agent_ppo2.py:186][0m |          -0.0144 |           3.6893 |           6.6431 |
[32m[20230113 20:07:35 @agent_ppo2.py:186][0m |          -0.0146 |           3.5761 |           6.6482 |
[32m[20230113 20:07:35 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:07:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 85.38
[32m[20230113 20:07:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.75
[32m[20230113 20:07:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.36
[32m[20230113 20:07:36 @agent_ppo2.py:144][0m Total time:      23.05 min
[32m[20230113 20:07:36 @agent_ppo2.py:146][0m 2103296 total steps have happened
[32m[20230113 20:07:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1027 --------------------------#
[32m[20230113 20:07:36 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 20:07:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:36 @agent_ppo2.py:186][0m |          -0.0001 |          54.3343 |           6.9247 |
[32m[20230113 20:07:36 @agent_ppo2.py:186][0m |          -0.0070 |          24.6705 |           6.9147 |
[32m[20230113 20:07:36 @agent_ppo2.py:186][0m |          -0.0098 |          18.4383 |           6.9124 |
[32m[20230113 20:07:36 @agent_ppo2.py:186][0m |          -0.0123 |          15.6007 |           6.9043 |
[32m[20230113 20:07:36 @agent_ppo2.py:186][0m |          -0.0139 |          13.4673 |           6.8977 |
[32m[20230113 20:07:36 @agent_ppo2.py:186][0m |          -0.0153 |          12.0478 |           6.8972 |
[32m[20230113 20:07:36 @agent_ppo2.py:186][0m |          -0.0160 |          11.1469 |           6.8939 |
[32m[20230113 20:07:36 @agent_ppo2.py:186][0m |          -0.0172 |          10.5368 |           6.8918 |
[32m[20230113 20:07:36 @agent_ppo2.py:186][0m |          -0.0180 |          10.0444 |           6.8931 |
[32m[20230113 20:07:36 @agent_ppo2.py:186][0m |          -0.0192 |           9.5306 |           6.8912 |
[32m[20230113 20:07:36 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 20:07:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 23.55
[32m[20230113 20:07:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 47.21
[32m[20230113 20:07:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.36
[32m[20230113 20:07:37 @agent_ppo2.py:144][0m Total time:      23.07 min
[32m[20230113 20:07:37 @agent_ppo2.py:146][0m 2105344 total steps have happened
[32m[20230113 20:07:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1028 --------------------------#
[32m[20230113 20:07:37 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:07:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:37 @agent_ppo2.py:186][0m |           0.0012 |          11.1478 |           6.4968 |
[32m[20230113 20:07:37 @agent_ppo2.py:186][0m |          -0.0099 |           7.3394 |           6.4906 |
[32m[20230113 20:07:37 @agent_ppo2.py:186][0m |          -0.0084 |           6.4992 |           6.4871 |
[32m[20230113 20:07:37 @agent_ppo2.py:186][0m |          -0.0087 |           6.0873 |           6.4837 |
[32m[20230113 20:07:37 @agent_ppo2.py:186][0m |          -0.0135 |           5.7457 |           6.4794 |
[32m[20230113 20:07:37 @agent_ppo2.py:186][0m |          -0.0123 |           5.4835 |           6.4799 |
[32m[20230113 20:07:38 @agent_ppo2.py:186][0m |          -0.0125 |           5.2589 |           6.4776 |
[32m[20230113 20:07:38 @agent_ppo2.py:186][0m |          -0.0100 |           5.2614 |           6.4777 |
[32m[20230113 20:07:38 @agent_ppo2.py:186][0m |          -0.0094 |           5.0116 |           6.4727 |
[32m[20230113 20:07:38 @agent_ppo2.py:186][0m |          -0.0129 |           4.7508 |           6.4737 |
[32m[20230113 20:07:38 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:07:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.89
[32m[20230113 20:07:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.66
[32m[20230113 20:07:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 94.52
[32m[20230113 20:07:38 @agent_ppo2.py:144][0m Total time:      23.09 min
[32m[20230113 20:07:38 @agent_ppo2.py:146][0m 2107392 total steps have happened
[32m[20230113 20:07:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1029 --------------------------#
[32m[20230113 20:07:38 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:07:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:39 @agent_ppo2.py:186][0m |          -0.0000 |          17.6320 |           6.7104 |
[32m[20230113 20:07:39 @agent_ppo2.py:186][0m |          -0.0052 |          10.8995 |           6.7006 |
[32m[20230113 20:07:39 @agent_ppo2.py:186][0m |          -0.0074 |           9.0944 |           6.6992 |
[32m[20230113 20:07:39 @agent_ppo2.py:186][0m |          -0.0126 |           7.9548 |           6.6944 |
[32m[20230113 20:07:39 @agent_ppo2.py:186][0m |          -0.0130 |           7.2727 |           6.6958 |
[32m[20230113 20:07:39 @agent_ppo2.py:186][0m |          -0.0142 |           6.6828 |           6.6895 |
[32m[20230113 20:07:39 @agent_ppo2.py:186][0m |          -0.0166 |           6.1529 |           6.6876 |
[32m[20230113 20:07:39 @agent_ppo2.py:186][0m |          -0.0159 |           5.7580 |           6.6901 |
[32m[20230113 20:07:39 @agent_ppo2.py:186][0m |          -0.0180 |           5.3954 |           6.6890 |
[32m[20230113 20:07:39 @agent_ppo2.py:186][0m |          -0.0176 |           5.1515 |           6.6861 |
[32m[20230113 20:07:39 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 20:07:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 137.72
[32m[20230113 20:07:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.61
[32m[20230113 20:07:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.02
[32m[20230113 20:07:39 @agent_ppo2.py:144][0m Total time:      23.11 min
[32m[20230113 20:07:39 @agent_ppo2.py:146][0m 2109440 total steps have happened
[32m[20230113 20:07:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1030 --------------------------#
[32m[20230113 20:07:40 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:07:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:40 @agent_ppo2.py:186][0m |           0.0004 |           9.8067 |           6.6620 |
[32m[20230113 20:07:40 @agent_ppo2.py:186][0m |          -0.0062 |           6.7682 |           6.6493 |
[32m[20230113 20:07:40 @agent_ppo2.py:186][0m |          -0.0092 |           5.8623 |           6.6422 |
[32m[20230113 20:07:40 @agent_ppo2.py:186][0m |          -0.0117 |           5.2973 |           6.6387 |
[32m[20230113 20:07:40 @agent_ppo2.py:186][0m |          -0.0125 |           4.9091 |           6.6355 |
[32m[20230113 20:07:40 @agent_ppo2.py:186][0m |          -0.0138 |           4.6269 |           6.6339 |
[32m[20230113 20:07:40 @agent_ppo2.py:186][0m |          -0.0147 |           4.3647 |           6.6345 |
[32m[20230113 20:07:40 @agent_ppo2.py:186][0m |          -0.0148 |           4.1814 |           6.6328 |
[32m[20230113 20:07:40 @agent_ppo2.py:186][0m |          -0.0158 |           4.0259 |           6.6321 |
[32m[20230113 20:07:40 @agent_ppo2.py:186][0m |          -0.0170 |           3.9350 |           6.6281 |
[32m[20230113 20:07:40 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:07:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.57
[32m[20230113 20:07:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.27
[32m[20230113 20:07:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.11
[32m[20230113 20:07:41 @agent_ppo2.py:144][0m Total time:      23.13 min
[32m[20230113 20:07:41 @agent_ppo2.py:146][0m 2111488 total steps have happened
[32m[20230113 20:07:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1031 --------------------------#
[32m[20230113 20:07:41 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:41 @agent_ppo2.py:186][0m |           0.0002 |           9.0661 |           6.6204 |
[32m[20230113 20:07:41 @agent_ppo2.py:186][0m |          -0.0060 |           6.8582 |           6.6282 |
[32m[20230113 20:07:41 @agent_ppo2.py:186][0m |          -0.0099 |           6.1488 |           6.6368 |
[32m[20230113 20:07:41 @agent_ppo2.py:186][0m |          -0.0109 |           5.7182 |           6.6289 |
[32m[20230113 20:07:41 @agent_ppo2.py:186][0m |          -0.0117 |           5.4457 |           6.6296 |
[32m[20230113 20:07:41 @agent_ppo2.py:186][0m |          -0.0131 |           5.1397 |           6.6389 |
[32m[20230113 20:07:41 @agent_ppo2.py:186][0m |          -0.0134 |           4.9314 |           6.6345 |
[32m[20230113 20:07:41 @agent_ppo2.py:186][0m |          -0.0134 |           4.8046 |           6.6341 |
[32m[20230113 20:07:41 @agent_ppo2.py:186][0m |          -0.0145 |           4.6159 |           6.6384 |
[32m[20230113 20:07:41 @agent_ppo2.py:186][0m |          -0.0148 |           4.4767 |           6.6396 |
[32m[20230113 20:07:41 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:07:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.23
[32m[20230113 20:07:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.24
[32m[20230113 20:07:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 140.34
[32m[20230113 20:07:42 @agent_ppo2.py:144][0m Total time:      23.15 min
[32m[20230113 20:07:42 @agent_ppo2.py:146][0m 2113536 total steps have happened
[32m[20230113 20:07:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1032 --------------------------#
[32m[20230113 20:07:42 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:07:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:42 @agent_ppo2.py:186][0m |          -0.0063 |          13.8951 |           6.5104 |
[32m[20230113 20:07:42 @agent_ppo2.py:186][0m |          -0.0097 |           9.0569 |           6.4994 |
[32m[20230113 20:07:42 @agent_ppo2.py:186][0m |          -0.0115 |           7.5965 |           6.5020 |
[32m[20230113 20:07:42 @agent_ppo2.py:186][0m |          -0.0064 |           6.9839 |           6.5020 |
[32m[20230113 20:07:42 @agent_ppo2.py:186][0m |          -0.0123 |           6.5528 |           6.5014 |
[32m[20230113 20:07:43 @agent_ppo2.py:186][0m |           0.0093 |           6.2173 |           6.4990 |
[32m[20230113 20:07:43 @agent_ppo2.py:186][0m |          -0.0128 |           5.7776 |           6.4967 |
[32m[20230113 20:07:43 @agent_ppo2.py:186][0m |          -0.0121 |           5.7678 |           6.4963 |
[32m[20230113 20:07:43 @agent_ppo2.py:186][0m |          -0.0145 |           5.3502 |           6.4970 |
[32m[20230113 20:07:43 @agent_ppo2.py:186][0m |          -0.0068 |           5.1350 |           6.4974 |
[32m[20230113 20:07:43 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:07:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 110.18
[32m[20230113 20:07:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.98
[32m[20230113 20:07:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 176.91
[32m[20230113 20:07:43 @agent_ppo2.py:144][0m Total time:      23.18 min
[32m[20230113 20:07:43 @agent_ppo2.py:146][0m 2115584 total steps have happened
[32m[20230113 20:07:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1033 --------------------------#
[32m[20230113 20:07:43 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 20:07:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:44 @agent_ppo2.py:186][0m |          -0.0012 |          18.1515 |           6.5722 |
[32m[20230113 20:07:44 @agent_ppo2.py:186][0m |          -0.0080 |          10.2636 |           6.5710 |
[32m[20230113 20:07:44 @agent_ppo2.py:186][0m |          -0.0126 |           8.4730 |           6.5676 |
[32m[20230113 20:07:44 @agent_ppo2.py:186][0m |          -0.0158 |           7.5139 |           6.5656 |
[32m[20230113 20:07:44 @agent_ppo2.py:186][0m |          -0.0155 |           6.9548 |           6.5648 |
[32m[20230113 20:07:44 @agent_ppo2.py:186][0m |          -0.0175 |           6.4661 |           6.5645 |
[32m[20230113 20:07:44 @agent_ppo2.py:186][0m |          -0.0204 |           6.1774 |           6.5653 |
[32m[20230113 20:07:44 @agent_ppo2.py:186][0m |          -0.0178 |           5.9783 |           6.5687 |
[32m[20230113 20:07:44 @agent_ppo2.py:186][0m |          -0.0195 |           5.6450 |           6.5653 |
[32m[20230113 20:07:44 @agent_ppo2.py:186][0m |          -0.0198 |           5.5306 |           6.5626 |
[32m[20230113 20:07:44 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 20:07:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 133.52
[32m[20230113 20:07:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.94
[32m[20230113 20:07:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.30
[32m[20230113 20:07:44 @agent_ppo2.py:144][0m Total time:      23.19 min
[32m[20230113 20:07:44 @agent_ppo2.py:146][0m 2117632 total steps have happened
[32m[20230113 20:07:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1034 --------------------------#
[32m[20230113 20:07:45 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:07:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:45 @agent_ppo2.py:186][0m |           0.0001 |          38.5628 |           6.7113 |
[32m[20230113 20:07:45 @agent_ppo2.py:186][0m |          -0.0086 |          20.1991 |           6.7050 |
[32m[20230113 20:07:45 @agent_ppo2.py:186][0m |          -0.0074 |          15.6890 |           6.7180 |
[32m[20230113 20:07:45 @agent_ppo2.py:186][0m |          -0.0129 |          12.7394 |           6.7146 |
[32m[20230113 20:07:45 @agent_ppo2.py:186][0m |          -0.0118 |          10.6301 |           6.7205 |
[32m[20230113 20:07:45 @agent_ppo2.py:186][0m |          -0.0159 |           9.7430 |           6.7138 |
[32m[20230113 20:07:45 @agent_ppo2.py:186][0m |          -0.0168 |           8.6781 |           6.7204 |
[32m[20230113 20:07:45 @agent_ppo2.py:186][0m |          -0.0155 |           8.0384 |           6.7156 |
[32m[20230113 20:07:45 @agent_ppo2.py:186][0m |          -0.0160 |           7.5126 |           6.7133 |
[32m[20230113 20:07:45 @agent_ppo2.py:186][0m |          -0.0153 |           7.1605 |           6.7143 |
[32m[20230113 20:07:45 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:07:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 87.29
[32m[20230113 20:07:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.19
[32m[20230113 20:07:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.00
[32m[20230113 20:07:46 @agent_ppo2.py:144][0m Total time:      23.22 min
[32m[20230113 20:07:46 @agent_ppo2.py:146][0m 2119680 total steps have happened
[32m[20230113 20:07:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1035 --------------------------#
[32m[20230113 20:07:46 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:07:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:46 @agent_ppo2.py:186][0m |          -0.0001 |          14.8704 |           6.5921 |
[32m[20230113 20:07:46 @agent_ppo2.py:186][0m |          -0.0063 |           9.7177 |           6.5917 |
[32m[20230113 20:07:46 @agent_ppo2.py:186][0m |          -0.0095 |           8.3304 |           6.5914 |
[32m[20230113 20:07:46 @agent_ppo2.py:186][0m |          -0.0092 |           7.6080 |           6.5837 |
[32m[20230113 20:07:46 @agent_ppo2.py:186][0m |          -0.0130 |           7.1599 |           6.5764 |
[32m[20230113 20:07:46 @agent_ppo2.py:186][0m |          -0.0146 |           6.6899 |           6.5782 |
[32m[20230113 20:07:46 @agent_ppo2.py:186][0m |          -0.0152 |           6.3868 |           6.5778 |
[32m[20230113 20:07:47 @agent_ppo2.py:186][0m |          -0.0161 |           6.1350 |           6.5751 |
[32m[20230113 20:07:47 @agent_ppo2.py:186][0m |          -0.0162 |           5.9018 |           6.5721 |
[32m[20230113 20:07:47 @agent_ppo2.py:186][0m |          -0.0166 |           5.8119 |           6.5758 |
[32m[20230113 20:07:47 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 20:07:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 101.70
[32m[20230113 20:07:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.19
[32m[20230113 20:07:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.04
[32m[20230113 20:07:47 @agent_ppo2.py:144][0m Total time:      23.24 min
[32m[20230113 20:07:47 @agent_ppo2.py:146][0m 2121728 total steps have happened
[32m[20230113 20:07:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1036 --------------------------#
[32m[20230113 20:07:47 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 20:07:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:47 @agent_ppo2.py:186][0m |          -0.0050 |          18.1344 |           6.6552 |
[32m[20230113 20:07:47 @agent_ppo2.py:186][0m |          -0.0023 |          12.7502 |           6.6502 |
[32m[20230113 20:07:48 @agent_ppo2.py:186][0m |          -0.0084 |          10.1482 |           6.6448 |
[32m[20230113 20:07:48 @agent_ppo2.py:186][0m |          -0.0200 |           8.5782 |           6.6412 |
[32m[20230113 20:07:48 @agent_ppo2.py:186][0m |          -0.0203 |           7.6878 |           6.6363 |
[32m[20230113 20:07:48 @agent_ppo2.py:186][0m |          -0.0183 |           6.9848 |           6.6382 |
[32m[20230113 20:07:48 @agent_ppo2.py:186][0m |          -0.0207 |           6.4796 |           6.6410 |
[32m[20230113 20:07:48 @agent_ppo2.py:186][0m |          -0.0194 |           6.0180 |           6.6422 |
[32m[20230113 20:07:48 @agent_ppo2.py:186][0m |          -0.0216 |           5.7280 |           6.6372 |
[32m[20230113 20:07:48 @agent_ppo2.py:186][0m |          -0.0190 |           5.4200 |           6.6415 |
[32m[20230113 20:07:48 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 20:07:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 151.35
[32m[20230113 20:07:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.05
[32m[20230113 20:07:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.95
[32m[20230113 20:07:48 @agent_ppo2.py:144][0m Total time:      23.26 min
[32m[20230113 20:07:48 @agent_ppo2.py:146][0m 2123776 total steps have happened
[32m[20230113 20:07:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1037 --------------------------#
[32m[20230113 20:07:49 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 20:07:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:49 @agent_ppo2.py:186][0m |           0.0013 |          27.9029 |           6.6557 |
[32m[20230113 20:07:49 @agent_ppo2.py:186][0m |          -0.0032 |          19.5679 |           6.6599 |
[32m[20230113 20:07:49 @agent_ppo2.py:186][0m |          -0.0062 |          16.3503 |           6.6540 |
[32m[20230113 20:07:49 @agent_ppo2.py:186][0m |          -0.0080 |          15.2741 |           6.6604 |
[32m[20230113 20:07:49 @agent_ppo2.py:186][0m |          -0.0066 |          14.1015 |           6.6612 |
[32m[20230113 20:07:49 @agent_ppo2.py:186][0m |          -0.0095 |          13.5390 |           6.6506 |
[32m[20230113 20:07:49 @agent_ppo2.py:186][0m |          -0.0107 |          12.7485 |           6.6582 |
[32m[20230113 20:07:49 @agent_ppo2.py:186][0m |          -0.0090 |          12.1330 |           6.6611 |
[32m[20230113 20:07:49 @agent_ppo2.py:186][0m |          -0.0114 |          11.7178 |           6.6624 |
[32m[20230113 20:07:49 @agent_ppo2.py:186][0m |          -0.0135 |          11.3457 |           6.6604 |
[32m[20230113 20:07:49 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 20:07:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 131.22
[32m[20230113 20:07:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.70
[32m[20230113 20:07:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.53
[32m[20230113 20:07:49 @agent_ppo2.py:144][0m Total time:      23.28 min
[32m[20230113 20:07:49 @agent_ppo2.py:146][0m 2125824 total steps have happened
[32m[20230113 20:07:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1038 --------------------------#
[32m[20230113 20:07:50 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:50 @agent_ppo2.py:186][0m |           0.0010 |           8.8194 |           6.7270 |
[32m[20230113 20:07:50 @agent_ppo2.py:186][0m |          -0.0041 |           7.1332 |           6.7153 |
[32m[20230113 20:07:50 @agent_ppo2.py:186][0m |          -0.0080 |           6.3872 |           6.7056 |
[32m[20230113 20:07:50 @agent_ppo2.py:186][0m |          -0.0080 |           6.0737 |           6.7059 |
[32m[20230113 20:07:50 @agent_ppo2.py:186][0m |          -0.0101 |           5.7805 |           6.7022 |
[32m[20230113 20:07:50 @agent_ppo2.py:186][0m |          -0.0107 |           5.5769 |           6.7019 |
[32m[20230113 20:07:50 @agent_ppo2.py:186][0m |          -0.0106 |           5.3746 |           6.7015 |
[32m[20230113 20:07:50 @agent_ppo2.py:186][0m |          -0.0107 |           5.2258 |           6.7020 |
[32m[20230113 20:07:50 @agent_ppo2.py:186][0m |          -0.0110 |           5.1609 |           6.6957 |
[32m[20230113 20:07:50 @agent_ppo2.py:186][0m |          -0.0110 |           4.9847 |           6.6973 |
[32m[20230113 20:07:50 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:07:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.73
[32m[20230113 20:07:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.46
[32m[20230113 20:07:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 70.25
[32m[20230113 20:07:51 @agent_ppo2.py:144][0m Total time:      23.30 min
[32m[20230113 20:07:51 @agent_ppo2.py:146][0m 2127872 total steps have happened
[32m[20230113 20:07:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1039 --------------------------#
[32m[20230113 20:07:51 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 20:07:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:51 @agent_ppo2.py:186][0m |          -0.0020 |          29.1053 |           6.7692 |
[32m[20230113 20:07:51 @agent_ppo2.py:186][0m |          -0.0081 |          20.5399 |           6.7543 |
[32m[20230113 20:07:51 @agent_ppo2.py:186][0m |          -0.0104 |          17.6796 |           6.7434 |
[32m[20230113 20:07:51 @agent_ppo2.py:186][0m |          -0.0115 |          15.4992 |           6.7398 |
[32m[20230113 20:07:51 @agent_ppo2.py:186][0m |          -0.0127 |          14.2778 |           6.7436 |
[32m[20230113 20:07:51 @agent_ppo2.py:186][0m |          -0.0144 |          13.5065 |           6.7388 |
[32m[20230113 20:07:51 @agent_ppo2.py:186][0m |          -0.0149 |          12.6529 |           6.7421 |
[32m[20230113 20:07:51 @agent_ppo2.py:186][0m |          -0.0169 |          11.9277 |           6.7403 |
[32m[20230113 20:07:51 @agent_ppo2.py:186][0m |          -0.0190 |          11.5093 |           6.7404 |
[32m[20230113 20:07:51 @agent_ppo2.py:186][0m |          -0.0173 |          11.1132 |           6.7415 |
[32m[20230113 20:07:51 @agent_ppo2.py:131][0m Policy update time: 0.40 s
[32m[20230113 20:07:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 147.59
[32m[20230113 20:07:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.98
[32m[20230113 20:07:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.13
[32m[20230113 20:07:52 @agent_ppo2.py:144][0m Total time:      23.32 min
[32m[20230113 20:07:52 @agent_ppo2.py:146][0m 2129920 total steps have happened
[32m[20230113 20:07:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1040 --------------------------#
[32m[20230113 20:07:52 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:07:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:52 @agent_ppo2.py:186][0m |           0.0014 |          22.1616 |           6.7361 |
[32m[20230113 20:07:52 @agent_ppo2.py:186][0m |          -0.0068 |          12.4427 |           6.7361 |
[32m[20230113 20:07:52 @agent_ppo2.py:186][0m |          -0.0115 |          10.5527 |           6.7227 |
[32m[20230113 20:07:52 @agent_ppo2.py:186][0m |          -0.0099 |           9.3749 |           6.7145 |
[32m[20230113 20:07:52 @agent_ppo2.py:186][0m |          -0.0114 |           8.6179 |           6.7225 |
[32m[20230113 20:07:53 @agent_ppo2.py:186][0m |          -0.0152 |           8.0756 |           6.7230 |
[32m[20230113 20:07:53 @agent_ppo2.py:186][0m |          -0.0142 |           7.4814 |           6.7227 |
[32m[20230113 20:07:53 @agent_ppo2.py:186][0m |          -0.0117 |           7.1094 |           6.7228 |
[32m[20230113 20:07:53 @agent_ppo2.py:186][0m |          -0.0165 |           6.7925 |           6.7250 |
[32m[20230113 20:07:53 @agent_ppo2.py:186][0m |          -0.0147 |           6.5639 |           6.7246 |
[32m[20230113 20:07:53 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:07:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.79
[32m[20230113 20:07:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.00
[32m[20230113 20:07:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.59
[32m[20230113 20:07:53 @agent_ppo2.py:144][0m Total time:      23.34 min
[32m[20230113 20:07:53 @agent_ppo2.py:146][0m 2131968 total steps have happened
[32m[20230113 20:07:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1041 --------------------------#
[32m[20230113 20:07:54 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:07:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:54 @agent_ppo2.py:186][0m |           0.0001 |          11.5995 |           6.8687 |
[32m[20230113 20:07:54 @agent_ppo2.py:186][0m |           0.0003 |           7.2826 |           6.8607 |
[32m[20230113 20:07:54 @agent_ppo2.py:186][0m |          -0.0080 |           6.2866 |           6.8638 |
[32m[20230113 20:07:54 @agent_ppo2.py:186][0m |          -0.0068 |           5.7149 |           6.8601 |
[32m[20230113 20:07:54 @agent_ppo2.py:186][0m |          -0.0183 |           5.3445 |           6.8594 |
[32m[20230113 20:07:54 @agent_ppo2.py:186][0m |          -0.0050 |           5.0601 |           6.8628 |
[32m[20230113 20:07:54 @agent_ppo2.py:186][0m |          -0.0176 |           4.8920 |           6.8572 |
[32m[20230113 20:07:54 @agent_ppo2.py:186][0m |          -0.0122 |           4.7248 |           6.8534 |
[32m[20230113 20:07:54 @agent_ppo2.py:186][0m |          -0.0156 |           4.4662 |           6.8592 |
[32m[20230113 20:07:54 @agent_ppo2.py:186][0m |          -0.0103 |           4.3453 |           6.8497 |
[32m[20230113 20:07:54 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:07:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.11
[32m[20230113 20:07:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.72
[32m[20230113 20:07:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.13
[32m[20230113 20:07:54 @agent_ppo2.py:144][0m Total time:      23.36 min
[32m[20230113 20:07:54 @agent_ppo2.py:146][0m 2134016 total steps have happened
[32m[20230113 20:07:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1042 --------------------------#
[32m[20230113 20:07:55 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:07:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:55 @agent_ppo2.py:186][0m |          -0.0024 |          19.0982 |           6.7436 |
[32m[20230113 20:07:55 @agent_ppo2.py:186][0m |          -0.0092 |           9.0777 |           6.7439 |
[32m[20230113 20:07:55 @agent_ppo2.py:186][0m |          -0.0109 |           7.6438 |           6.7311 |
[32m[20230113 20:07:55 @agent_ppo2.py:186][0m |          -0.0128 |           6.6738 |           6.7363 |
[32m[20230113 20:07:55 @agent_ppo2.py:186][0m |          -0.0143 |           6.0729 |           6.7323 |
[32m[20230113 20:07:55 @agent_ppo2.py:186][0m |          -0.0150 |           5.5590 |           6.7291 |
[32m[20230113 20:07:55 @agent_ppo2.py:186][0m |          -0.0154 |           5.2235 |           6.7274 |
[32m[20230113 20:07:55 @agent_ppo2.py:186][0m |          -0.0162 |           4.9501 |           6.7295 |
[32m[20230113 20:07:55 @agent_ppo2.py:186][0m |          -0.0167 |           4.7231 |           6.7275 |
[32m[20230113 20:07:55 @agent_ppo2.py:186][0m |          -0.0168 |           4.5272 |           6.7266 |
[32m[20230113 20:07:55 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:07:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 137.50
[32m[20230113 20:07:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.17
[32m[20230113 20:07:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.36
[32m[20230113 20:07:56 @agent_ppo2.py:144][0m Total time:      23.39 min
[32m[20230113 20:07:56 @agent_ppo2.py:146][0m 2136064 total steps have happened
[32m[20230113 20:07:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1043 --------------------------#
[32m[20230113 20:07:56 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:07:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:56 @agent_ppo2.py:186][0m |           0.0025 |          11.7779 |           6.8072 |
[32m[20230113 20:07:56 @agent_ppo2.py:186][0m |          -0.0068 |           8.3881 |           6.7962 |
[32m[20230113 20:07:56 @agent_ppo2.py:186][0m |          -0.0103 |           7.3458 |           6.7897 |
[32m[20230113 20:07:56 @agent_ppo2.py:186][0m |          -0.0101 |           6.8282 |           6.7962 |
[32m[20230113 20:07:57 @agent_ppo2.py:186][0m |          -0.0147 |           6.2634 |           6.7901 |
[32m[20230113 20:07:57 @agent_ppo2.py:186][0m |          -0.0132 |           6.0220 |           6.7924 |
[32m[20230113 20:07:57 @agent_ppo2.py:186][0m |          -0.0140 |           5.6147 |           6.7918 |
[32m[20230113 20:07:57 @agent_ppo2.py:186][0m |          -0.0138 |           5.3817 |           6.7949 |
[32m[20230113 20:07:57 @agent_ppo2.py:186][0m |          -0.0147 |           5.1656 |           6.7921 |
[32m[20230113 20:07:57 @agent_ppo2.py:186][0m |          -0.0156 |           4.9661 |           6.7952 |
[32m[20230113 20:07:57 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:07:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.46
[32m[20230113 20:07:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.79
[32m[20230113 20:07:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.92
[32m[20230113 20:07:57 @agent_ppo2.py:144][0m Total time:      23.41 min
[32m[20230113 20:07:57 @agent_ppo2.py:146][0m 2138112 total steps have happened
[32m[20230113 20:07:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1044 --------------------------#
[32m[20230113 20:07:58 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:07:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:58 @agent_ppo2.py:186][0m |          -0.0010 |           5.4790 |           6.6466 |
[32m[20230113 20:07:58 @agent_ppo2.py:186][0m |          -0.0073 |           4.6366 |           6.6330 |
[32m[20230113 20:07:58 @agent_ppo2.py:186][0m |          -0.0060 |           4.4050 |           6.6299 |
[32m[20230113 20:07:58 @agent_ppo2.py:186][0m |          -0.0148 |           3.9965 |           6.6313 |
[32m[20230113 20:07:58 @agent_ppo2.py:186][0m |          -0.0110 |           3.7939 |           6.6270 |
[32m[20230113 20:07:58 @agent_ppo2.py:186][0m |          -0.0130 |           3.6827 |           6.6322 |
[32m[20230113 20:07:58 @agent_ppo2.py:186][0m |          -0.0091 |           3.5357 |           6.6278 |
[32m[20230113 20:07:58 @agent_ppo2.py:186][0m |          -0.0145 |           3.3770 |           6.6347 |
[32m[20230113 20:07:58 @agent_ppo2.py:186][0m |          -0.0141 |           3.2858 |           6.6309 |
[32m[20230113 20:07:58 @agent_ppo2.py:186][0m |          -0.0110 |           3.1748 |           6.6350 |
[32m[20230113 20:07:58 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:07:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.47
[32m[20230113 20:07:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.66
[32m[20230113 20:07:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.34
[32m[20230113 20:07:58 @agent_ppo2.py:144][0m Total time:      23.43 min
[32m[20230113 20:07:58 @agent_ppo2.py:146][0m 2140160 total steps have happened
[32m[20230113 20:07:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1045 --------------------------#
[32m[20230113 20:07:59 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:07:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:07:59 @agent_ppo2.py:186][0m |          -0.0010 |           7.3859 |           6.8565 |
[32m[20230113 20:07:59 @agent_ppo2.py:186][0m |          -0.0057 |           5.5329 |           6.8556 |
[32m[20230113 20:07:59 @agent_ppo2.py:186][0m |          -0.0101 |           4.9777 |           6.8520 |
[32m[20230113 20:07:59 @agent_ppo2.py:186][0m |          -0.0030 |           4.6690 |           6.8512 |
[32m[20230113 20:07:59 @agent_ppo2.py:186][0m |          -0.0114 |           4.3906 |           6.8473 |
[32m[20230113 20:07:59 @agent_ppo2.py:186][0m |          -0.0126 |           4.1728 |           6.8483 |
[32m[20230113 20:07:59 @agent_ppo2.py:186][0m |          -0.0162 |           4.0460 |           6.8493 |
[32m[20230113 20:07:59 @agent_ppo2.py:186][0m |          -0.0112 |           3.9180 |           6.8409 |
[32m[20230113 20:07:59 @agent_ppo2.py:186][0m |          -0.0108 |           3.8238 |           6.8420 |
[32m[20230113 20:07:59 @agent_ppo2.py:186][0m |          -0.0149 |           3.7295 |           6.8430 |
[32m[20230113 20:07:59 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:08:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.88
[32m[20230113 20:08:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.81
[32m[20230113 20:08:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.44
[32m[20230113 20:08:00 @agent_ppo2.py:144][0m Total time:      23.45 min
[32m[20230113 20:08:00 @agent_ppo2.py:146][0m 2142208 total steps have happened
[32m[20230113 20:08:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1046 --------------------------#
[32m[20230113 20:08:00 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:00 @agent_ppo2.py:186][0m |          -0.0063 |           6.7572 |           6.6550 |
[32m[20230113 20:08:00 @agent_ppo2.py:186][0m |          -0.0120 |           5.6686 |           6.6388 |
[32m[20230113 20:08:00 @agent_ppo2.py:186][0m |          -0.0034 |           5.1315 |           6.6377 |
[32m[20230113 20:08:00 @agent_ppo2.py:186][0m |          -0.0146 |           4.8277 |           6.6388 |
[32m[20230113 20:08:00 @agent_ppo2.py:186][0m |          -0.0141 |           4.6275 |           6.6295 |
[32m[20230113 20:08:00 @agent_ppo2.py:186][0m |          -0.0105 |           4.4804 |           6.6287 |
[32m[20230113 20:08:00 @agent_ppo2.py:186][0m |          -0.0069 |           4.4772 |           6.6299 |
[32m[20230113 20:08:00 @agent_ppo2.py:186][0m |          -0.0045 |           4.2175 |           6.6208 |
[32m[20230113 20:08:01 @agent_ppo2.py:186][0m |          -0.0137 |           4.0927 |           6.6280 |
[32m[20230113 20:08:01 @agent_ppo2.py:186][0m |          -0.0260 |           4.0274 |           6.6207 |
[32m[20230113 20:08:01 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.03
[32m[20230113 20:08:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.86
[32m[20230113 20:08:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.90
[32m[20230113 20:08:01 @agent_ppo2.py:144][0m Total time:      23.47 min
[32m[20230113 20:08:01 @agent_ppo2.py:146][0m 2144256 total steps have happened
[32m[20230113 20:08:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1047 --------------------------#
[32m[20230113 20:08:01 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:08:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:02 @agent_ppo2.py:186][0m |          -0.0013 |           7.7903 |           6.6838 |
[32m[20230113 20:08:02 @agent_ppo2.py:186][0m |          -0.0050 |           5.5825 |           6.6685 |
[32m[20230113 20:08:02 @agent_ppo2.py:186][0m |          -0.0071 |           4.9125 |           6.6663 |
[32m[20230113 20:08:02 @agent_ppo2.py:186][0m |          -0.0031 |           4.6423 |           6.6782 |
[32m[20230113 20:08:02 @agent_ppo2.py:186][0m |          -0.0130 |           4.2998 |           6.6778 |
[32m[20230113 20:08:02 @agent_ppo2.py:186][0m |           0.0102 |           4.7495 |           6.6729 |
[32m[20230113 20:08:02 @agent_ppo2.py:186][0m |          -0.0097 |           4.0398 |           6.6845 |
[32m[20230113 20:08:02 @agent_ppo2.py:186][0m |          -0.0160 |           3.7817 |           6.6849 |
[32m[20230113 20:08:02 @agent_ppo2.py:186][0m |          -0.0355 |           3.6248 |           6.6914 |
[32m[20230113 20:08:02 @agent_ppo2.py:186][0m |          -0.0171 |           3.5266 |           6.6876 |
[32m[20230113 20:08:02 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:08:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.00
[32m[20230113 20:08:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.00
[32m[20230113 20:08:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.60
[32m[20230113 20:08:02 @agent_ppo2.py:144][0m Total time:      23.50 min
[32m[20230113 20:08:02 @agent_ppo2.py:146][0m 2146304 total steps have happened
[32m[20230113 20:08:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1048 --------------------------#
[32m[20230113 20:08:03 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:08:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:03 @agent_ppo2.py:186][0m |          -0.0008 |           6.4670 |           6.9523 |
[32m[20230113 20:08:03 @agent_ppo2.py:186][0m |          -0.0080 |           5.2351 |           6.9527 |
[32m[20230113 20:08:03 @agent_ppo2.py:186][0m |          -0.0095 |           4.7482 |           6.9539 |
[32m[20230113 20:08:03 @agent_ppo2.py:186][0m |          -0.0104 |           4.4524 |           6.9512 |
[32m[20230113 20:08:03 @agent_ppo2.py:186][0m |          -0.0113 |           4.2582 |           6.9501 |
[32m[20230113 20:08:03 @agent_ppo2.py:186][0m |          -0.0128 |           4.0141 |           6.9482 |
[32m[20230113 20:08:03 @agent_ppo2.py:186][0m |          -0.0135 |           3.9069 |           6.9481 |
[32m[20230113 20:08:03 @agent_ppo2.py:186][0m |          -0.0139 |           3.7970 |           6.9466 |
[32m[20230113 20:08:03 @agent_ppo2.py:186][0m |          -0.0129 |           3.6881 |           6.9486 |
[32m[20230113 20:08:03 @agent_ppo2.py:186][0m |          -0.0150 |           3.6021 |           6.9455 |
[32m[20230113 20:08:03 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:08:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.36
[32m[20230113 20:08:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.20
[32m[20230113 20:08:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.82
[32m[20230113 20:08:04 @agent_ppo2.py:144][0m Total time:      23.52 min
[32m[20230113 20:08:04 @agent_ppo2.py:146][0m 2148352 total steps have happened
[32m[20230113 20:08:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1049 --------------------------#
[32m[20230113 20:08:04 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:08:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:04 @agent_ppo2.py:186][0m |           0.0007 |           5.9884 |           6.6859 |
[32m[20230113 20:08:04 @agent_ppo2.py:186][0m |          -0.0022 |           4.8374 |           6.6871 |
[32m[20230113 20:08:04 @agent_ppo2.py:186][0m |          -0.0084 |           4.3307 |           6.6839 |
[32m[20230113 20:08:04 @agent_ppo2.py:186][0m |          -0.0172 |           4.0984 |           6.6912 |
[32m[20230113 20:08:04 @agent_ppo2.py:186][0m |          -0.0128 |           3.8976 |           6.6930 |
[32m[20230113 20:08:04 @agent_ppo2.py:186][0m |          -0.0123 |           3.6863 |           6.6892 |
[32m[20230113 20:08:04 @agent_ppo2.py:186][0m |          -0.0077 |           3.5334 |           6.6930 |
[32m[20230113 20:08:04 @agent_ppo2.py:186][0m |          -0.0125 |           3.4140 |           6.6993 |
[32m[20230113 20:08:04 @agent_ppo2.py:186][0m |          -0.0055 |           3.3313 |           6.7009 |
[32m[20230113 20:08:04 @agent_ppo2.py:186][0m |          -0.0154 |           3.2408 |           6.7027 |
[32m[20230113 20:08:04 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:08:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.91
[32m[20230113 20:08:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.45
[32m[20230113 20:08:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.77
[32m[20230113 20:08:05 @agent_ppo2.py:144][0m Total time:      23.54 min
[32m[20230113 20:08:05 @agent_ppo2.py:146][0m 2150400 total steps have happened
[32m[20230113 20:08:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1050 --------------------------#
[32m[20230113 20:08:05 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:08:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:05 @agent_ppo2.py:186][0m |           0.0041 |           6.0512 |           6.9712 |
[32m[20230113 20:08:05 @agent_ppo2.py:186][0m |          -0.0048 |           4.9554 |           6.9616 |
[32m[20230113 20:08:05 @agent_ppo2.py:186][0m |          -0.0066 |           4.5143 |           6.9542 |
[32m[20230113 20:08:06 @agent_ppo2.py:186][0m |          -0.0094 |           4.2336 |           6.9513 |
[32m[20230113 20:08:06 @agent_ppo2.py:186][0m |          -0.0102 |           3.9952 |           6.9547 |
[32m[20230113 20:08:06 @agent_ppo2.py:186][0m |          -0.0113 |           3.8543 |           6.9608 |
[32m[20230113 20:08:06 @agent_ppo2.py:186][0m |          -0.0090 |           3.7175 |           6.9588 |
[32m[20230113 20:08:06 @agent_ppo2.py:186][0m |          -0.0119 |           3.6020 |           6.9586 |
[32m[20230113 20:08:06 @agent_ppo2.py:186][0m |          -0.0115 |           3.5183 |           6.9600 |
[32m[20230113 20:08:06 @agent_ppo2.py:186][0m |          -0.0122 |           3.3904 |           6.9646 |
[32m[20230113 20:08:06 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:08:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.47
[32m[20230113 20:08:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.79
[32m[20230113 20:08:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.43
[32m[20230113 20:08:06 @agent_ppo2.py:144][0m Total time:      23.56 min
[32m[20230113 20:08:06 @agent_ppo2.py:146][0m 2152448 total steps have happened
[32m[20230113 20:08:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1051 --------------------------#
[32m[20230113 20:08:07 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:07 @agent_ppo2.py:186][0m |          -0.0009 |           4.7903 |           6.8909 |
[32m[20230113 20:08:07 @agent_ppo2.py:186][0m |          -0.0064 |           3.8121 |           6.8824 |
[32m[20230113 20:08:07 @agent_ppo2.py:186][0m |          -0.0086 |           3.4230 |           6.8885 |
[32m[20230113 20:08:07 @agent_ppo2.py:186][0m |          -0.0124 |           3.1623 |           6.8842 |
[32m[20230113 20:08:07 @agent_ppo2.py:186][0m |          -0.0125 |           3.0566 |           6.8855 |
[32m[20230113 20:08:07 @agent_ppo2.py:186][0m |          -0.0136 |           2.8942 |           6.8874 |
[32m[20230113 20:08:07 @agent_ppo2.py:186][0m |          -0.0155 |           2.7837 |           6.8851 |
[32m[20230113 20:08:07 @agent_ppo2.py:186][0m |          -0.0137 |           2.7363 |           6.8865 |
[32m[20230113 20:08:07 @agent_ppo2.py:186][0m |          -0.0157 |           2.6415 |           6.8857 |
[32m[20230113 20:08:07 @agent_ppo2.py:186][0m |          -0.0160 |           2.5475 |           6.8933 |
[32m[20230113 20:08:07 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.30
[32m[20230113 20:08:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.47
[32m[20230113 20:08:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.68
[32m[20230113 20:08:07 @agent_ppo2.py:144][0m Total time:      23.58 min
[32m[20230113 20:08:07 @agent_ppo2.py:146][0m 2154496 total steps have happened
[32m[20230113 20:08:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1052 --------------------------#
[32m[20230113 20:08:08 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:08:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:08 @agent_ppo2.py:186][0m |           0.0006 |           4.7432 |           7.0726 |
[32m[20230113 20:08:08 @agent_ppo2.py:186][0m |          -0.0024 |           3.9902 |           7.0652 |
[32m[20230113 20:08:08 @agent_ppo2.py:186][0m |          -0.0050 |           3.6211 |           7.0602 |
[32m[20230113 20:08:08 @agent_ppo2.py:186][0m |          -0.0074 |           3.3823 |           7.0578 |
[32m[20230113 20:08:08 @agent_ppo2.py:186][0m |          -0.0080 |           3.2463 |           7.0552 |
[32m[20230113 20:08:08 @agent_ppo2.py:186][0m |          -0.0100 |           3.1570 |           7.0564 |
[32m[20230113 20:08:08 @agent_ppo2.py:186][0m |          -0.0105 |           3.0380 |           7.0465 |
[32m[20230113 20:08:08 @agent_ppo2.py:186][0m |          -0.0121 |           2.9706 |           7.0628 |
[32m[20230113 20:08:08 @agent_ppo2.py:186][0m |          -0.0100 |           2.9110 |           7.0580 |
[32m[20230113 20:08:08 @agent_ppo2.py:186][0m |          -0.0119 |           2.8376 |           7.0509 |
[32m[20230113 20:08:08 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:08:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.54
[32m[20230113 20:08:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.84
[32m[20230113 20:08:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.20
[32m[20230113 20:08:09 @agent_ppo2.py:144][0m Total time:      23.60 min
[32m[20230113 20:08:09 @agent_ppo2.py:146][0m 2156544 total steps have happened
[32m[20230113 20:08:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1053 --------------------------#
[32m[20230113 20:08:09 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:08:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:09 @agent_ppo2.py:186][0m |          -0.0010 |           5.9412 |           7.0164 |
[32m[20230113 20:08:09 @agent_ppo2.py:186][0m |          -0.0005 |           5.1839 |           7.0001 |
[32m[20230113 20:08:09 @agent_ppo2.py:186][0m |          -0.0036 |           5.1313 |           7.0055 |
[32m[20230113 20:08:09 @agent_ppo2.py:186][0m |          -0.0019 |           4.6595 |           7.0066 |
[32m[20230113 20:08:09 @agent_ppo2.py:186][0m |          -0.0111 |           4.3606 |           7.0022 |
[32m[20230113 20:08:10 @agent_ppo2.py:186][0m |          -0.0076 |           4.2041 |           7.0119 |
[32m[20230113 20:08:10 @agent_ppo2.py:186][0m |          -0.0102 |           4.1283 |           6.9995 |
[32m[20230113 20:08:10 @agent_ppo2.py:186][0m |          -0.0077 |           4.0014 |           7.0031 |
[32m[20230113 20:08:10 @agent_ppo2.py:186][0m |          -0.0089 |           3.9043 |           7.0075 |
[32m[20230113 20:08:10 @agent_ppo2.py:186][0m |          -0.0144 |           3.7921 |           6.9979 |
[32m[20230113 20:08:10 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:08:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.23
[32m[20230113 20:08:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.14
[32m[20230113 20:08:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.94
[32m[20230113 20:08:10 @agent_ppo2.py:144][0m Total time:      23.63 min
[32m[20230113 20:08:10 @agent_ppo2.py:146][0m 2158592 total steps have happened
[32m[20230113 20:08:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1054 --------------------------#
[32m[20230113 20:08:10 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:08:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:11 @agent_ppo2.py:186][0m |           0.0034 |           6.6749 |           7.0603 |
[32m[20230113 20:08:11 @agent_ppo2.py:186][0m |          -0.0039 |           5.6850 |           7.0604 |
[32m[20230113 20:08:11 @agent_ppo2.py:186][0m |          -0.0008 |           5.5009 |           7.0537 |
[32m[20230113 20:08:11 @agent_ppo2.py:186][0m |          -0.0068 |           4.9196 |           7.0533 |
[32m[20230113 20:08:11 @agent_ppo2.py:186][0m |          -0.0078 |           4.6595 |           7.0595 |
[32m[20230113 20:08:11 @agent_ppo2.py:186][0m |          -0.0109 |           4.3897 |           7.0554 |
[32m[20230113 20:08:11 @agent_ppo2.py:186][0m |          -0.0093 |           4.2085 |           7.0569 |
[32m[20230113 20:08:11 @agent_ppo2.py:186][0m |          -0.0148 |           4.0296 |           7.0559 |
[32m[20230113 20:08:11 @agent_ppo2.py:186][0m |          -0.0089 |           3.9754 |           7.0549 |
[32m[20230113 20:08:11 @agent_ppo2.py:186][0m |          -0.0097 |           3.8723 |           7.0560 |
[32m[20230113 20:08:11 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:08:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.85
[32m[20230113 20:08:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.24
[32m[20230113 20:08:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 139.60
[32m[20230113 20:08:11 @agent_ppo2.py:144][0m Total time:      23.65 min
[32m[20230113 20:08:11 @agent_ppo2.py:146][0m 2160640 total steps have happened
[32m[20230113 20:08:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1055 --------------------------#
[32m[20230113 20:08:12 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 20:08:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:12 @agent_ppo2.py:186][0m |          -0.0027 |          14.8241 |           6.9391 |
[32m[20230113 20:08:12 @agent_ppo2.py:186][0m |          -0.0082 |           8.4532 |           6.9319 |
[32m[20230113 20:08:12 @agent_ppo2.py:186][0m |          -0.0106 |           5.9874 |           6.9282 |
[32m[20230113 20:08:12 @agent_ppo2.py:186][0m |          -0.0096 |           5.2485 |           6.9216 |
[32m[20230113 20:08:12 @agent_ppo2.py:186][0m |          -0.0125 |           4.8182 |           6.9221 |
[32m[20230113 20:08:12 @agent_ppo2.py:186][0m |          -0.0105 |           4.5139 |           6.9216 |
[32m[20230113 20:08:12 @agent_ppo2.py:186][0m |          -0.0136 |           4.3611 |           6.9208 |
[32m[20230113 20:08:12 @agent_ppo2.py:186][0m |          -0.0151 |           4.1860 |           6.9180 |
[32m[20230113 20:08:12 @agent_ppo2.py:186][0m |          -0.0192 |           4.0221 |           6.9169 |
[32m[20230113 20:08:12 @agent_ppo2.py:186][0m |          -0.0178 |           3.9264 |           6.9128 |
[32m[20230113 20:08:12 @agent_ppo2.py:131][0m Policy update time: 0.34 s
[32m[20230113 20:08:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 108.18
[32m[20230113 20:08:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.08
[32m[20230113 20:08:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.41
[32m[20230113 20:08:12 @agent_ppo2.py:144][0m Total time:      23.66 min
[32m[20230113 20:08:12 @agent_ppo2.py:146][0m 2162688 total steps have happened
[32m[20230113 20:08:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1056 --------------------------#
[32m[20230113 20:08:13 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:08:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:13 @agent_ppo2.py:186][0m |          -0.0009 |           6.0081 |           7.1519 |
[32m[20230113 20:08:13 @agent_ppo2.py:186][0m |          -0.0058 |           4.4632 |           7.1408 |
[32m[20230113 20:08:13 @agent_ppo2.py:186][0m |          -0.0089 |           4.0192 |           7.1298 |
[32m[20230113 20:08:13 @agent_ppo2.py:186][0m |          -0.0120 |           3.7115 |           7.1311 |
[32m[20230113 20:08:13 @agent_ppo2.py:186][0m |          -0.0126 |           3.5192 |           7.1291 |
[32m[20230113 20:08:13 @agent_ppo2.py:186][0m |          -0.0115 |           3.3348 |           7.1272 |
[32m[20230113 20:08:13 @agent_ppo2.py:186][0m |          -0.0122 |           3.2091 |           7.1293 |
[32m[20230113 20:08:13 @agent_ppo2.py:186][0m |          -0.0133 |           3.0992 |           7.1263 |
[32m[20230113 20:08:13 @agent_ppo2.py:186][0m |          -0.0120 |           3.0234 |           7.1259 |
[32m[20230113 20:08:13 @agent_ppo2.py:186][0m |          -0.0148 |           2.9260 |           7.1317 |
[32m[20230113 20:08:13 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:08:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.07
[32m[20230113 20:08:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.99
[32m[20230113 20:08:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.33
[32m[20230113 20:08:14 @agent_ppo2.py:144][0m Total time:      23.69 min
[32m[20230113 20:08:14 @agent_ppo2.py:146][0m 2164736 total steps have happened
[32m[20230113 20:08:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1057 --------------------------#
[32m[20230113 20:08:14 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:14 @agent_ppo2.py:186][0m |           0.0013 |           6.3973 |           7.3250 |
[32m[20230113 20:08:14 @agent_ppo2.py:186][0m |          -0.0045 |           5.5293 |           7.3191 |
[32m[20230113 20:08:14 @agent_ppo2.py:186][0m |          -0.0074 |           4.9969 |           7.3135 |
[32m[20230113 20:08:14 @agent_ppo2.py:186][0m |          -0.0092 |           4.7041 |           7.3163 |
[32m[20230113 20:08:14 @agent_ppo2.py:186][0m |          -0.0106 |           4.4538 |           7.3121 |
[32m[20230113 20:08:14 @agent_ppo2.py:186][0m |          -0.0113 |           4.2610 |           7.3141 |
[32m[20230113 20:08:14 @agent_ppo2.py:186][0m |          -0.0119 |           4.1127 |           7.3142 |
[32m[20230113 20:08:15 @agent_ppo2.py:186][0m |          -0.0130 |           4.0277 |           7.3124 |
[32m[20230113 20:08:15 @agent_ppo2.py:186][0m |          -0.0132 |           3.9169 |           7.3065 |
[32m[20230113 20:08:15 @agent_ppo2.py:186][0m |          -0.0144 |           3.8238 |           7.3069 |
[32m[20230113 20:08:15 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.82
[32m[20230113 20:08:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.48
[32m[20230113 20:08:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.22
[32m[20230113 20:08:15 @agent_ppo2.py:144][0m Total time:      23.71 min
[32m[20230113 20:08:15 @agent_ppo2.py:146][0m 2166784 total steps have happened
[32m[20230113 20:08:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1058 --------------------------#
[32m[20230113 20:08:15 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:08:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:16 @agent_ppo2.py:186][0m |          -0.0029 |           6.8188 |           7.0523 |
[32m[20230113 20:08:16 @agent_ppo2.py:186][0m |          -0.0064 |           5.3689 |           7.0478 |
[32m[20230113 20:08:16 @agent_ppo2.py:186][0m |          -0.0099 |           4.9385 |           7.0393 |
[32m[20230113 20:08:16 @agent_ppo2.py:186][0m |          -0.0078 |           4.6434 |           7.0350 |
[32m[20230113 20:08:16 @agent_ppo2.py:186][0m |          -0.0111 |           4.5090 |           7.0278 |
[32m[20230113 20:08:16 @agent_ppo2.py:186][0m |          -0.0110 |           4.3089 |           7.0231 |
[32m[20230113 20:08:16 @agent_ppo2.py:186][0m |          -0.0169 |           4.1843 |           7.0297 |
[32m[20230113 20:08:16 @agent_ppo2.py:186][0m |          -0.0129 |           4.0665 |           7.0213 |
[32m[20230113 20:08:16 @agent_ppo2.py:186][0m |          -0.0166 |           4.0606 |           7.0241 |
[32m[20230113 20:08:16 @agent_ppo2.py:186][0m |          -0.0146 |           3.9127 |           7.0208 |
[32m[20230113 20:08:16 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:08:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.96
[32m[20230113 20:08:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.32
[32m[20230113 20:08:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.47
[32m[20230113 20:08:16 @agent_ppo2.py:144][0m Total time:      23.73 min
[32m[20230113 20:08:16 @agent_ppo2.py:146][0m 2168832 total steps have happened
[32m[20230113 20:08:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1059 --------------------------#
[32m[20230113 20:08:17 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:17 @agent_ppo2.py:186][0m |           0.0073 |           5.2074 |           7.1179 |
[32m[20230113 20:08:17 @agent_ppo2.py:186][0m |          -0.0009 |           4.5703 |           7.1245 |
[32m[20230113 20:08:17 @agent_ppo2.py:186][0m |          -0.0017 |           4.2845 |           7.1218 |
[32m[20230113 20:08:17 @agent_ppo2.py:186][0m |          -0.0057 |           4.1170 |           7.1186 |
[32m[20230113 20:08:17 @agent_ppo2.py:186][0m |          -0.0138 |           3.9434 |           7.1171 |
[32m[20230113 20:08:17 @agent_ppo2.py:186][0m |          -0.0113 |           3.8382 |           7.1171 |
[32m[20230113 20:08:17 @agent_ppo2.py:186][0m |          -0.0129 |           3.7164 |           7.1224 |
[32m[20230113 20:08:17 @agent_ppo2.py:186][0m |          -0.0131 |           3.6161 |           7.1220 |
[32m[20230113 20:08:17 @agent_ppo2.py:186][0m |          -0.0170 |           3.5403 |           7.1281 |
[32m[20230113 20:08:17 @agent_ppo2.py:186][0m |          -0.0103 |           3.4928 |           7.1230 |
[32m[20230113 20:08:17 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.35
[32m[20230113 20:08:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.27
[32m[20230113 20:08:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.09
[32m[20230113 20:08:18 @agent_ppo2.py:144][0m Total time:      23.75 min
[32m[20230113 20:08:18 @agent_ppo2.py:146][0m 2170880 total steps have happened
[32m[20230113 20:08:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1060 --------------------------#
[32m[20230113 20:08:18 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:08:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:18 @agent_ppo2.py:186][0m |          -0.0057 |           5.6310 |           7.0299 |
[32m[20230113 20:08:18 @agent_ppo2.py:186][0m |          -0.0073 |           4.6535 |           7.0255 |
[32m[20230113 20:08:18 @agent_ppo2.py:186][0m |          -0.0104 |           4.1895 |           7.0218 |
[32m[20230113 20:08:18 @agent_ppo2.py:186][0m |          -0.0036 |           3.9126 |           7.0272 |
[32m[20230113 20:08:18 @agent_ppo2.py:186][0m |          -0.0058 |           3.7716 |           7.0249 |
[32m[20230113 20:08:18 @agent_ppo2.py:186][0m |          -0.0053 |           3.6362 |           7.0264 |
[32m[20230113 20:08:18 @agent_ppo2.py:186][0m |          -0.0064 |           3.5273 |           7.0255 |
[32m[20230113 20:08:18 @agent_ppo2.py:186][0m |          -0.0128 |           3.3671 |           7.0330 |
[32m[20230113 20:08:18 @agent_ppo2.py:186][0m |          -0.0102 |           3.3120 |           7.0288 |
[32m[20230113 20:08:18 @agent_ppo2.py:186][0m |          -0.0108 |           3.3033 |           7.0244 |
[32m[20230113 20:08:18 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:08:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.84
[32m[20230113 20:08:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.46
[32m[20230113 20:08:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.48
[32m[20230113 20:08:19 @agent_ppo2.py:144][0m Total time:      23.77 min
[32m[20230113 20:08:19 @agent_ppo2.py:146][0m 2172928 total steps have happened
[32m[20230113 20:08:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1061 --------------------------#
[32m[20230113 20:08:19 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:19 @agent_ppo2.py:186][0m |           0.0045 |           7.0244 |           7.0146 |
[32m[20230113 20:08:19 @agent_ppo2.py:186][0m |          -0.0098 |           5.6343 |           7.0111 |
[32m[20230113 20:08:19 @agent_ppo2.py:186][0m |          -0.0051 |           5.1379 |           7.0071 |
[32m[20230113 20:08:20 @agent_ppo2.py:186][0m |          -0.0080 |           4.8103 |           6.9984 |
[32m[20230113 20:08:20 @agent_ppo2.py:186][0m |          -0.0097 |           4.5154 |           7.0076 |
[32m[20230113 20:08:20 @agent_ppo2.py:186][0m |          -0.0135 |           4.3418 |           7.0043 |
[32m[20230113 20:08:20 @agent_ppo2.py:186][0m |          -0.0264 |           4.2021 |           6.9986 |
[32m[20230113 20:08:20 @agent_ppo2.py:186][0m |          -0.0160 |           4.1496 |           7.0003 |
[32m[20230113 20:08:20 @agent_ppo2.py:186][0m |          -0.0102 |           3.8913 |           6.9994 |
[32m[20230113 20:08:20 @agent_ppo2.py:186][0m |          -0.0189 |           3.8268 |           6.9918 |
[32m[20230113 20:08:20 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:08:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.91
[32m[20230113 20:08:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.93
[32m[20230113 20:08:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.03
[32m[20230113 20:08:20 @agent_ppo2.py:144][0m Total time:      23.79 min
[32m[20230113 20:08:20 @agent_ppo2.py:146][0m 2174976 total steps have happened
[32m[20230113 20:08:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1062 --------------------------#
[32m[20230113 20:08:21 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:08:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:21 @agent_ppo2.py:186][0m |           0.0022 |           6.2319 |           7.2286 |
[32m[20230113 20:08:21 @agent_ppo2.py:186][0m |          -0.0042 |           4.8937 |           7.2330 |
[32m[20230113 20:08:21 @agent_ppo2.py:186][0m |          -0.0064 |           4.3412 |           7.2243 |
[32m[20230113 20:08:21 @agent_ppo2.py:186][0m |          -0.0096 |           4.0911 |           7.2239 |
[32m[20230113 20:08:21 @agent_ppo2.py:186][0m |          -0.0090 |           3.9050 |           7.2205 |
[32m[20230113 20:08:21 @agent_ppo2.py:186][0m |          -0.0107 |           3.7359 |           7.2201 |
[32m[20230113 20:08:21 @agent_ppo2.py:186][0m |          -0.0108 |           3.6542 |           7.2213 |
[32m[20230113 20:08:21 @agent_ppo2.py:186][0m |          -0.0123 |           3.5264 |           7.2180 |
[32m[20230113 20:08:21 @agent_ppo2.py:186][0m |          -0.0123 |           3.4257 |           7.2136 |
[32m[20230113 20:08:21 @agent_ppo2.py:186][0m |          -0.0124 |           3.3471 |           7.2191 |
[32m[20230113 20:08:21 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:08:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.88
[32m[20230113 20:08:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.55
[32m[20230113 20:08:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.28
[32m[20230113 20:08:21 @agent_ppo2.py:144][0m Total time:      23.81 min
[32m[20230113 20:08:21 @agent_ppo2.py:146][0m 2177024 total steps have happened
[32m[20230113 20:08:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1063 --------------------------#
[32m[20230113 20:08:22 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:22 @agent_ppo2.py:186][0m |          -0.0001 |           5.6808 |           7.1347 |
[32m[20230113 20:08:22 @agent_ppo2.py:186][0m |          -0.0056 |           4.4196 |           7.1218 |
[32m[20230113 20:08:22 @agent_ppo2.py:186][0m |          -0.0109 |           4.0161 |           7.1181 |
[32m[20230113 20:08:22 @agent_ppo2.py:186][0m |          -0.0087 |           3.6662 |           7.1106 |
[32m[20230113 20:08:22 @agent_ppo2.py:186][0m |          -0.0095 |           3.4424 |           7.1153 |
[32m[20230113 20:08:22 @agent_ppo2.py:186][0m |          -0.0116 |           3.2863 |           7.1109 |
[32m[20230113 20:08:22 @agent_ppo2.py:186][0m |          -0.0123 |           3.1494 |           7.1167 |
[32m[20230113 20:08:22 @agent_ppo2.py:186][0m |          -0.0101 |           3.0447 |           7.1174 |
[32m[20230113 20:08:22 @agent_ppo2.py:186][0m |          -0.0153 |           2.9376 |           7.1131 |
[32m[20230113 20:08:22 @agent_ppo2.py:186][0m |          -0.0085 |           2.9762 |           7.1153 |
[32m[20230113 20:08:22 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:08:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.69
[32m[20230113 20:08:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.23
[32m[20230113 20:08:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.85
[32m[20230113 20:08:23 @agent_ppo2.py:144][0m Total time:      23.84 min
[32m[20230113 20:08:23 @agent_ppo2.py:146][0m 2179072 total steps have happened
[32m[20230113 20:08:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1064 --------------------------#
[32m[20230113 20:08:23 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:23 @agent_ppo2.py:186][0m |          -0.0015 |           6.9444 |           7.2720 |
[32m[20230113 20:08:23 @agent_ppo2.py:186][0m |          -0.0060 |           5.6656 |           7.2581 |
[32m[20230113 20:08:23 @agent_ppo2.py:186][0m |          -0.0073 |           5.1489 |           7.2604 |
[32m[20230113 20:08:23 @agent_ppo2.py:186][0m |          -0.0088 |           4.7993 |           7.2593 |
[32m[20230113 20:08:23 @agent_ppo2.py:186][0m |          -0.0111 |           4.5524 |           7.2609 |
[32m[20230113 20:08:23 @agent_ppo2.py:186][0m |          -0.0109 |           4.2643 |           7.2596 |
[32m[20230113 20:08:24 @agent_ppo2.py:186][0m |          -0.0119 |           4.1105 |           7.2586 |
[32m[20230113 20:08:24 @agent_ppo2.py:186][0m |          -0.0131 |           3.9936 |           7.2600 |
[32m[20230113 20:08:24 @agent_ppo2.py:186][0m |          -0.0121 |           3.9333 |           7.2581 |
[32m[20230113 20:08:24 @agent_ppo2.py:186][0m |          -0.0129 |           3.7905 |           7.2580 |
[32m[20230113 20:08:24 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.92
[32m[20230113 20:08:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.74
[32m[20230113 20:08:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.77
[32m[20230113 20:08:24 @agent_ppo2.py:144][0m Total time:      23.86 min
[32m[20230113 20:08:24 @agent_ppo2.py:146][0m 2181120 total steps have happened
[32m[20230113 20:08:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1065 --------------------------#
[32m[20230113 20:08:25 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:25 @agent_ppo2.py:186][0m |          -0.0005 |           6.4261 |           7.2247 |
[32m[20230113 20:08:25 @agent_ppo2.py:186][0m |          -0.0056 |           5.3077 |           7.2230 |
[32m[20230113 20:08:25 @agent_ppo2.py:186][0m |          -0.0083 |           4.7156 |           7.2177 |
[32m[20230113 20:08:25 @agent_ppo2.py:186][0m |          -0.0096 |           4.3988 |           7.2182 |
[32m[20230113 20:08:25 @agent_ppo2.py:186][0m |          -0.0090 |           4.1261 |           7.2126 |
[32m[20230113 20:08:25 @agent_ppo2.py:186][0m |          -0.0114 |           3.9729 |           7.2158 |
[32m[20230113 20:08:25 @agent_ppo2.py:186][0m |          -0.0126 |           3.7640 |           7.2178 |
[32m[20230113 20:08:25 @agent_ppo2.py:186][0m |          -0.0124 |           3.6794 |           7.2129 |
[32m[20230113 20:08:25 @agent_ppo2.py:186][0m |          -0.0128 |           3.5411 |           7.2158 |
[32m[20230113 20:08:25 @agent_ppo2.py:186][0m |          -0.0135 |           3.4077 |           7.2116 |
[32m[20230113 20:08:25 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.32
[32m[20230113 20:08:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.54
[32m[20230113 20:08:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.87
[32m[20230113 20:08:25 @agent_ppo2.py:144][0m Total time:      23.88 min
[32m[20230113 20:08:25 @agent_ppo2.py:146][0m 2183168 total steps have happened
[32m[20230113 20:08:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1066 --------------------------#
[32m[20230113 20:08:26 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:08:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:26 @agent_ppo2.py:186][0m |           0.0025 |           4.7302 |           7.2953 |
[32m[20230113 20:08:26 @agent_ppo2.py:186][0m |          -0.0016 |           3.4143 |           7.2818 |
[32m[20230113 20:08:26 @agent_ppo2.py:186][0m |          -0.0003 |           3.1825 |           7.2748 |
[32m[20230113 20:08:26 @agent_ppo2.py:186][0m |          -0.0096 |           2.8870 |           7.2719 |
[32m[20230113 20:08:26 @agent_ppo2.py:186][0m |          -0.0087 |           2.7970 |           7.2642 |
[32m[20230113 20:08:26 @agent_ppo2.py:186][0m |          -0.0111 |           2.6921 |           7.2574 |
[32m[20230113 20:08:26 @agent_ppo2.py:186][0m |          -0.0073 |           2.6137 |           7.2586 |
[32m[20230113 20:08:26 @agent_ppo2.py:186][0m |          -0.0102 |           2.5757 |           7.2500 |
[32m[20230113 20:08:26 @agent_ppo2.py:186][0m |          -0.0132 |           2.4728 |           7.2524 |
[32m[20230113 20:08:26 @agent_ppo2.py:186][0m |          -0.0131 |           2.4299 |           7.2488 |
[32m[20230113 20:08:26 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.84
[32m[20230113 20:08:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.87
[32m[20230113 20:08:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.56
[32m[20230113 20:08:27 @agent_ppo2.py:144][0m Total time:      23.90 min
[32m[20230113 20:08:27 @agent_ppo2.py:146][0m 2185216 total steps have happened
[32m[20230113 20:08:27 @agent_ppo2.py:122][0m #------------------------ Iteration 1067 --------------------------#
[32m[20230113 20:08:27 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:27 @agent_ppo2.py:186][0m |           0.0006 |           5.2655 |           7.2145 |
[32m[20230113 20:08:27 @agent_ppo2.py:186][0m |           0.0003 |           4.3519 |           7.2071 |
[32m[20230113 20:08:27 @agent_ppo2.py:186][0m |          -0.0047 |           3.9629 |           7.1969 |
[32m[20230113 20:08:27 @agent_ppo2.py:186][0m |          -0.0042 |           3.7595 |           7.1916 |
[32m[20230113 20:08:27 @agent_ppo2.py:186][0m |          -0.0099 |           3.5798 |           7.1809 |
[32m[20230113 20:08:27 @agent_ppo2.py:186][0m |          -0.0079 |           3.5013 |           7.1877 |
[32m[20230113 20:08:27 @agent_ppo2.py:186][0m |          -0.0070 |           3.3690 |           7.1845 |
[32m[20230113 20:08:28 @agent_ppo2.py:186][0m |          -0.0104 |           3.2844 |           7.1833 |
[32m[20230113 20:08:28 @agent_ppo2.py:186][0m |          -0.0115 |           3.2055 |           7.1777 |
[32m[20230113 20:08:28 @agent_ppo2.py:186][0m |          -0.0137 |           3.1407 |           7.1818 |
[32m[20230113 20:08:28 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.68
[32m[20230113 20:08:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.55
[32m[20230113 20:08:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.79
[32m[20230113 20:08:28 @agent_ppo2.py:144][0m Total time:      23.92 min
[32m[20230113 20:08:28 @agent_ppo2.py:146][0m 2187264 total steps have happened
[32m[20230113 20:08:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1068 --------------------------#
[32m[20230113 20:08:28 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:29 @agent_ppo2.py:186][0m |           0.0007 |           6.0171 |           7.3374 |
[32m[20230113 20:08:29 @agent_ppo2.py:186][0m |          -0.0028 |           4.6961 |           7.3406 |
[32m[20230113 20:08:29 @agent_ppo2.py:186][0m |          -0.0063 |           4.3253 |           7.3395 |
[32m[20230113 20:08:29 @agent_ppo2.py:186][0m |          -0.0083 |           4.1194 |           7.3378 |
[32m[20230113 20:08:29 @agent_ppo2.py:186][0m |          -0.0092 |           3.9512 |           7.3340 |
[32m[20230113 20:08:29 @agent_ppo2.py:186][0m |          -0.0095 |           3.8368 |           7.3266 |
[32m[20230113 20:08:29 @agent_ppo2.py:186][0m |          -0.0112 |           3.7293 |           7.3307 |
[32m[20230113 20:08:29 @agent_ppo2.py:186][0m |          -0.0124 |           3.6597 |           7.3251 |
[32m[20230113 20:08:29 @agent_ppo2.py:186][0m |          -0.0118 |           3.5902 |           7.3307 |
[32m[20230113 20:08:29 @agent_ppo2.py:186][0m |          -0.0119 |           3.5182 |           7.3257 |
[32m[20230113 20:08:29 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.92
[32m[20230113 20:08:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.67
[32m[20230113 20:08:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.85
[32m[20230113 20:08:29 @agent_ppo2.py:144][0m Total time:      23.95 min
[32m[20230113 20:08:29 @agent_ppo2.py:146][0m 2189312 total steps have happened
[32m[20230113 20:08:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1069 --------------------------#
[32m[20230113 20:08:30 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:30 @agent_ppo2.py:186][0m |           0.0078 |           5.3421 |           7.0373 |
[32m[20230113 20:08:30 @agent_ppo2.py:186][0m |           0.0022 |           4.3646 |           7.0401 |
[32m[20230113 20:08:30 @agent_ppo2.py:186][0m |          -0.0047 |           3.9426 |           7.0398 |
[32m[20230113 20:08:30 @agent_ppo2.py:186][0m |          -0.0075 |           3.6649 |           7.0368 |
[32m[20230113 20:08:30 @agent_ppo2.py:186][0m |          -0.0055 |           3.4856 |           7.0377 |
[32m[20230113 20:08:30 @agent_ppo2.py:186][0m |          -0.0068 |           3.4012 |           7.0401 |
[32m[20230113 20:08:30 @agent_ppo2.py:186][0m |          -0.0129 |           3.2617 |           7.0426 |
[32m[20230113 20:08:30 @agent_ppo2.py:186][0m |          -0.0128 |           3.1608 |           7.0399 |
[32m[20230113 20:08:30 @agent_ppo2.py:186][0m |          -0.0073 |           3.0625 |           7.0414 |
[32m[20230113 20:08:30 @agent_ppo2.py:186][0m |          -0.0198 |           3.0143 |           7.0401 |
[32m[20230113 20:08:30 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:08:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.07
[32m[20230113 20:08:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.05
[32m[20230113 20:08:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.17
[32m[20230113 20:08:31 @agent_ppo2.py:144][0m Total time:      23.97 min
[32m[20230113 20:08:31 @agent_ppo2.py:146][0m 2191360 total steps have happened
[32m[20230113 20:08:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1070 --------------------------#
[32m[20230113 20:08:31 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:31 @agent_ppo2.py:186][0m |          -0.0039 |           6.0401 |           7.1216 |
[32m[20230113 20:08:31 @agent_ppo2.py:186][0m |           0.0093 |           5.3018 |           7.1153 |
[32m[20230113 20:08:31 @agent_ppo2.py:186][0m |          -0.0025 |           4.6396 |           7.1095 |
[32m[20230113 20:08:31 @agent_ppo2.py:186][0m |          -0.0102 |           4.2809 |           7.1158 |
[32m[20230113 20:08:31 @agent_ppo2.py:186][0m |          -0.0034 |           4.0859 |           7.1068 |
[32m[20230113 20:08:31 @agent_ppo2.py:186][0m |          -0.0088 |           3.8465 |           7.1101 |
[32m[20230113 20:08:31 @agent_ppo2.py:186][0m |          -0.0043 |           3.7098 |           7.1040 |
[32m[20230113 20:08:31 @agent_ppo2.py:186][0m |          -0.0072 |           3.5791 |           7.1090 |
[32m[20230113 20:08:32 @agent_ppo2.py:186][0m |          -0.0087 |           3.4584 |           7.1048 |
[32m[20230113 20:08:32 @agent_ppo2.py:186][0m |          -0.0098 |           3.3677 |           7.1011 |
[32m[20230113 20:08:32 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:08:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.72
[32m[20230113 20:08:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.16
[32m[20230113 20:08:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.82
[32m[20230113 20:08:32 @agent_ppo2.py:144][0m Total time:      23.99 min
[32m[20230113 20:08:32 @agent_ppo2.py:146][0m 2193408 total steps have happened
[32m[20230113 20:08:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1071 --------------------------#
[32m[20230113 20:08:32 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:08:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:33 @agent_ppo2.py:186][0m |           0.0018 |           4.8268 |           7.1428 |
[32m[20230113 20:08:33 @agent_ppo2.py:186][0m |          -0.0064 |           3.8449 |           7.1348 |
[32m[20230113 20:08:33 @agent_ppo2.py:186][0m |          -0.0072 |           3.4611 |           7.1352 |
[32m[20230113 20:08:33 @agent_ppo2.py:186][0m |          -0.0080 |           3.2316 |           7.1346 |
[32m[20230113 20:08:33 @agent_ppo2.py:186][0m |          -0.0121 |           3.0552 |           7.1267 |
[32m[20230113 20:08:33 @agent_ppo2.py:186][0m |          -0.0090 |           2.9284 |           7.1292 |
[32m[20230113 20:08:33 @agent_ppo2.py:186][0m |          -0.0103 |           2.7812 |           7.1247 |
[32m[20230113 20:08:33 @agent_ppo2.py:186][0m |          -0.0107 |           2.7137 |           7.1238 |
[32m[20230113 20:08:33 @agent_ppo2.py:186][0m |          -0.0144 |           2.6045 |           7.1223 |
[32m[20230113 20:08:33 @agent_ppo2.py:186][0m |          -0.0157 |           2.5372 |           7.1253 |
[32m[20230113 20:08:33 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:08:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 214.40
[32m[20230113 20:08:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.14
[32m[20230113 20:08:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 153.50
[32m[20230113 20:08:33 @agent_ppo2.py:144][0m Total time:      24.01 min
[32m[20230113 20:08:33 @agent_ppo2.py:146][0m 2195456 total steps have happened
[32m[20230113 20:08:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1072 --------------------------#
[32m[20230113 20:08:34 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:34 @agent_ppo2.py:186][0m |           0.0021 |           6.2648 |           7.1990 |
[32m[20230113 20:08:34 @agent_ppo2.py:186][0m |          -0.0051 |           4.9555 |           7.1853 |
[32m[20230113 20:08:34 @agent_ppo2.py:186][0m |          -0.0076 |           4.5942 |           7.1742 |
[32m[20230113 20:08:34 @agent_ppo2.py:186][0m |          -0.0086 |           4.2733 |           7.1771 |
[32m[20230113 20:08:34 @agent_ppo2.py:186][0m |          -0.0108 |           4.1142 |           7.1763 |
[32m[20230113 20:08:34 @agent_ppo2.py:186][0m |          -0.0126 |           3.9505 |           7.1768 |
[32m[20230113 20:08:34 @agent_ppo2.py:186][0m |          -0.0132 |           3.8277 |           7.1727 |
[32m[20230113 20:08:34 @agent_ppo2.py:186][0m |          -0.0144 |           3.6781 |           7.1751 |
[32m[20230113 20:08:34 @agent_ppo2.py:186][0m |          -0.0143 |           3.6154 |           7.1650 |
[32m[20230113 20:08:34 @agent_ppo2.py:186][0m |          -0.0154 |           3.5334 |           7.1678 |
[32m[20230113 20:08:34 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.40
[32m[20230113 20:08:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.91
[32m[20230113 20:08:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.69
[32m[20230113 20:08:35 @agent_ppo2.py:144][0m Total time:      24.03 min
[32m[20230113 20:08:35 @agent_ppo2.py:146][0m 2197504 total steps have happened
[32m[20230113 20:08:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1073 --------------------------#
[32m[20230113 20:08:35 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:35 @agent_ppo2.py:186][0m |          -0.0093 |           6.3864 |           6.9641 |
[32m[20230113 20:08:35 @agent_ppo2.py:186][0m |          -0.0099 |           4.4780 |           6.9599 |
[32m[20230113 20:08:35 @agent_ppo2.py:186][0m |          -0.0055 |           3.9967 |           6.9556 |
[32m[20230113 20:08:35 @agent_ppo2.py:186][0m |          -0.0089 |           3.6856 |           6.9590 |
[32m[20230113 20:08:35 @agent_ppo2.py:186][0m |          -0.0202 |           3.5645 |           6.9553 |
[32m[20230113 20:08:35 @agent_ppo2.py:186][0m |          -0.0061 |           3.4076 |           6.9482 |
[32m[20230113 20:08:35 @agent_ppo2.py:186][0m |          -0.0239 |           3.4189 |           6.9538 |
[32m[20230113 20:08:35 @agent_ppo2.py:186][0m |          -0.0065 |           3.2609 |           6.9499 |
[32m[20230113 20:08:35 @agent_ppo2.py:186][0m |          -0.0174 |           3.0926 |           6.9484 |
[32m[20230113 20:08:35 @agent_ppo2.py:186][0m |          -0.0133 |           3.0388 |           6.9513 |
[32m[20230113 20:08:35 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:08:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.29
[32m[20230113 20:08:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.57
[32m[20230113 20:08:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.13
[32m[20230113 20:08:36 @agent_ppo2.py:144][0m Total time:      24.06 min
[32m[20230113 20:08:36 @agent_ppo2.py:146][0m 2199552 total steps have happened
[32m[20230113 20:08:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1074 --------------------------#
[32m[20230113 20:08:36 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:36 @agent_ppo2.py:186][0m |          -0.0044 |           5.1918 |           7.0392 |
[32m[20230113 20:08:36 @agent_ppo2.py:186][0m |          -0.0087 |           4.0487 |           7.0267 |
[32m[20230113 20:08:37 @agent_ppo2.py:186][0m |          -0.0021 |           3.5821 |           7.0244 |
[32m[20230113 20:08:37 @agent_ppo2.py:186][0m |          -0.0086 |           3.3188 |           7.0236 |
[32m[20230113 20:08:37 @agent_ppo2.py:186][0m |          -0.0106 |           3.0537 |           7.0134 |
[32m[20230113 20:08:37 @agent_ppo2.py:186][0m |          -0.0016 |           2.9417 |           7.0187 |
[32m[20230113 20:08:37 @agent_ppo2.py:186][0m |          -0.0078 |           2.7842 |           7.0028 |
[32m[20230113 20:08:37 @agent_ppo2.py:186][0m |          -0.0109 |           2.6525 |           7.0190 |
[32m[20230113 20:08:37 @agent_ppo2.py:186][0m |          -0.0119 |           2.6286 |           7.0149 |
[32m[20230113 20:08:37 @agent_ppo2.py:186][0m |          -0.0137 |           2.4714 |           7.0129 |
[32m[20230113 20:08:37 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.97
[32m[20230113 20:08:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.42
[32m[20230113 20:08:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.36
[32m[20230113 20:08:37 @agent_ppo2.py:144][0m Total time:      24.08 min
[32m[20230113 20:08:37 @agent_ppo2.py:146][0m 2201600 total steps have happened
[32m[20230113 20:08:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1075 --------------------------#
[32m[20230113 20:08:38 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:38 @agent_ppo2.py:186][0m |           0.0002 |           6.3283 |           7.0280 |
[32m[20230113 20:08:38 @agent_ppo2.py:186][0m |          -0.0117 |           4.8337 |           7.0275 |
[32m[20230113 20:08:38 @agent_ppo2.py:186][0m |          -0.0153 |           4.2425 |           7.0267 |
[32m[20230113 20:08:38 @agent_ppo2.py:186][0m |           0.0013 |           3.8874 |           7.0249 |
[32m[20230113 20:08:38 @agent_ppo2.py:186][0m |          -0.0176 |           3.6051 |           7.0265 |
[32m[20230113 20:08:38 @agent_ppo2.py:186][0m |          -0.0127 |           3.4755 |           7.0279 |
[32m[20230113 20:08:38 @agent_ppo2.py:186][0m |          -0.0123 |           3.3366 |           7.0309 |
[32m[20230113 20:08:38 @agent_ppo2.py:186][0m |          -0.0229 |           3.2182 |           7.0274 |
[32m[20230113 20:08:38 @agent_ppo2.py:186][0m |          -0.0085 |           3.1311 |           7.0284 |
[32m[20230113 20:08:38 @agent_ppo2.py:186][0m |          -0.0249 |           3.0489 |           7.0167 |
[32m[20230113 20:08:38 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.36
[32m[20230113 20:08:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.46
[32m[20230113 20:08:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.05
[32m[20230113 20:08:39 @agent_ppo2.py:144][0m Total time:      24.10 min
[32m[20230113 20:08:39 @agent_ppo2.py:146][0m 2203648 total steps have happened
[32m[20230113 20:08:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1076 --------------------------#
[32m[20230113 20:08:39 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:08:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:39 @agent_ppo2.py:186][0m |           0.0095 |           5.7338 |           7.1082 |
[32m[20230113 20:08:39 @agent_ppo2.py:186][0m |          -0.0088 |           4.3945 |           7.0953 |
[32m[20230113 20:08:39 @agent_ppo2.py:186][0m |          -0.0034 |           4.0468 |           7.0967 |
[32m[20230113 20:08:39 @agent_ppo2.py:186][0m |          -0.0071 |           3.7447 |           7.0919 |
[32m[20230113 20:08:39 @agent_ppo2.py:186][0m |          -0.0114 |           3.5881 |           7.0978 |
[32m[20230113 20:08:39 @agent_ppo2.py:186][0m |          -0.0177 |           3.4790 |           7.0921 |
[32m[20230113 20:08:39 @agent_ppo2.py:186][0m |          -0.0161 |           3.4326 |           7.1007 |
[32m[20230113 20:08:39 @agent_ppo2.py:186][0m |          -0.0121 |           3.3241 |           7.0944 |
[32m[20230113 20:08:39 @agent_ppo2.py:186][0m |          -0.0164 |           3.2545 |           7.0952 |
[32m[20230113 20:08:39 @agent_ppo2.py:186][0m |          -0.0124 |           3.1882 |           7.0990 |
[32m[20230113 20:08:39 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:08:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.32
[32m[20230113 20:08:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.38
[32m[20230113 20:08:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.23
[32m[20230113 20:08:40 @agent_ppo2.py:144][0m Total time:      24.12 min
[32m[20230113 20:08:40 @agent_ppo2.py:146][0m 2205696 total steps have happened
[32m[20230113 20:08:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1077 --------------------------#
[32m[20230113 20:08:40 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:40 @agent_ppo2.py:186][0m |          -0.0004 |           6.1750 |           7.2317 |
[32m[20230113 20:08:40 @agent_ppo2.py:186][0m |          -0.0024 |           4.8847 |           7.2169 |
[32m[20230113 20:08:40 @agent_ppo2.py:186][0m |          -0.0083 |           4.2703 |           7.2103 |
[32m[20230113 20:08:41 @agent_ppo2.py:186][0m |          -0.0048 |           3.9660 |           7.2085 |
[32m[20230113 20:08:41 @agent_ppo2.py:186][0m |          -0.0104 |           3.7051 |           7.2152 |
[32m[20230113 20:08:41 @agent_ppo2.py:186][0m |          -0.0061 |           3.6014 |           7.2152 |
[32m[20230113 20:08:41 @agent_ppo2.py:186][0m |          -0.0114 |           3.2904 |           7.2226 |
[32m[20230113 20:08:41 @agent_ppo2.py:186][0m |          -0.0127 |           3.1544 |           7.2102 |
[32m[20230113 20:08:41 @agent_ppo2.py:186][0m |          -0.0130 |           3.0491 |           7.2223 |
[32m[20230113 20:08:41 @agent_ppo2.py:186][0m |          -0.0150 |           2.9625 |           7.2182 |
[32m[20230113 20:08:41 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.36
[32m[20230113 20:08:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.41
[32m[20230113 20:08:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.61
[32m[20230113 20:08:41 @agent_ppo2.py:144][0m Total time:      24.14 min
[32m[20230113 20:08:41 @agent_ppo2.py:146][0m 2207744 total steps have happened
[32m[20230113 20:08:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1078 --------------------------#
[32m[20230113 20:08:42 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:42 @agent_ppo2.py:186][0m |           0.0032 |           6.3790 |           7.1505 |
[32m[20230113 20:08:42 @agent_ppo2.py:186][0m |          -0.0083 |           4.8556 |           7.1394 |
[32m[20230113 20:08:42 @agent_ppo2.py:186][0m |          -0.0133 |           4.3176 |           7.1301 |
[32m[20230113 20:08:42 @agent_ppo2.py:186][0m |          -0.0103 |           4.0435 |           7.1306 |
[32m[20230113 20:08:42 @agent_ppo2.py:186][0m |          -0.0093 |           3.8521 |           7.1246 |
[32m[20230113 20:08:42 @agent_ppo2.py:186][0m |          -0.0116 |           3.6632 |           7.1248 |
[32m[20230113 20:08:42 @agent_ppo2.py:186][0m |          -0.0100 |           3.5720 |           7.1234 |
[32m[20230113 20:08:42 @agent_ppo2.py:186][0m |          -0.0148 |           3.4409 |           7.1216 |
[32m[20230113 20:08:42 @agent_ppo2.py:186][0m |          -0.0171 |           3.3436 |           7.1165 |
[32m[20230113 20:08:42 @agent_ppo2.py:186][0m |          -0.0156 |           3.2030 |           7.1237 |
[32m[20230113 20:08:42 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:08:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.90
[32m[20230113 20:08:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.47
[32m[20230113 20:08:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.72
[32m[20230113 20:08:42 @agent_ppo2.py:144][0m Total time:      24.17 min
[32m[20230113 20:08:42 @agent_ppo2.py:146][0m 2209792 total steps have happened
[32m[20230113 20:08:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1079 --------------------------#
[32m[20230113 20:08:43 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:43 @agent_ppo2.py:186][0m |           0.0005 |           5.3884 |           7.2242 |
[32m[20230113 20:08:43 @agent_ppo2.py:186][0m |          -0.0047 |           4.0768 |           7.2231 |
[32m[20230113 20:08:43 @agent_ppo2.py:186][0m |          -0.0085 |           3.5763 |           7.2145 |
[32m[20230113 20:08:43 @agent_ppo2.py:186][0m |          -0.0093 |           3.3595 |           7.2137 |
[32m[20230113 20:08:43 @agent_ppo2.py:186][0m |          -0.0097 |           3.2204 |           7.2095 |
[32m[20230113 20:08:43 @agent_ppo2.py:186][0m |          -0.0092 |           3.1501 |           7.2103 |
[32m[20230113 20:08:43 @agent_ppo2.py:186][0m |          -0.0098 |           2.9570 |           7.2128 |
[32m[20230113 20:08:43 @agent_ppo2.py:186][0m |          -0.0137 |           2.8829 |           7.2108 |
[32m[20230113 20:08:43 @agent_ppo2.py:186][0m |          -0.0144 |           2.8083 |           7.2102 |
[32m[20230113 20:08:43 @agent_ppo2.py:186][0m |          -0.0159 |           2.7193 |           7.2089 |
[32m[20230113 20:08:43 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:08:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.63
[32m[20230113 20:08:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.98
[32m[20230113 20:08:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.76
[32m[20230113 20:08:44 @agent_ppo2.py:144][0m Total time:      24.19 min
[32m[20230113 20:08:44 @agent_ppo2.py:146][0m 2211840 total steps have happened
[32m[20230113 20:08:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1080 --------------------------#
[32m[20230113 20:08:44 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:44 @agent_ppo2.py:186][0m |           0.0009 |           5.2780 |           7.3024 |
[32m[20230113 20:08:44 @agent_ppo2.py:186][0m |          -0.0044 |           4.2201 |           7.3002 |
[32m[20230113 20:08:44 @agent_ppo2.py:186][0m |          -0.0117 |           3.7763 |           7.3025 |
[32m[20230113 20:08:44 @agent_ppo2.py:186][0m |          -0.0052 |           3.5298 |           7.2990 |
[32m[20230113 20:08:45 @agent_ppo2.py:186][0m |          -0.0095 |           3.3878 |           7.2992 |
[32m[20230113 20:08:45 @agent_ppo2.py:186][0m |          -0.0135 |           3.1519 |           7.2954 |
[32m[20230113 20:08:45 @agent_ppo2.py:186][0m |          -0.0158 |           3.0498 |           7.2975 |
[32m[20230113 20:08:45 @agent_ppo2.py:186][0m |          -0.0173 |           2.9350 |           7.2972 |
[32m[20230113 20:08:45 @agent_ppo2.py:186][0m |          -0.0164 |           2.8488 |           7.2878 |
[32m[20230113 20:08:45 @agent_ppo2.py:186][0m |          -0.0183 |           2.7639 |           7.2925 |
[32m[20230113 20:08:45 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.05
[32m[20230113 20:08:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.53
[32m[20230113 20:08:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.81
[32m[20230113 20:08:45 @agent_ppo2.py:144][0m Total time:      24.21 min
[32m[20230113 20:08:45 @agent_ppo2.py:146][0m 2213888 total steps have happened
[32m[20230113 20:08:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1081 --------------------------#
[32m[20230113 20:08:46 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:08:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:46 @agent_ppo2.py:186][0m |          -0.0024 |           6.5159 |           7.0992 |
[32m[20230113 20:08:46 @agent_ppo2.py:186][0m |          -0.0062 |           5.3623 |           7.1001 |
[32m[20230113 20:08:46 @agent_ppo2.py:186][0m |          -0.0033 |           4.8562 |           7.0963 |
[32m[20230113 20:08:46 @agent_ppo2.py:186][0m |          -0.0089 |           4.5181 |           7.0950 |
[32m[20230113 20:08:46 @agent_ppo2.py:186][0m |          -0.0036 |           4.2812 |           7.0942 |
[32m[20230113 20:08:46 @agent_ppo2.py:186][0m |          -0.0100 |           4.0992 |           7.0841 |
[32m[20230113 20:08:46 @agent_ppo2.py:186][0m |          -0.0091 |           3.9354 |           7.0802 |
[32m[20230113 20:08:46 @agent_ppo2.py:186][0m |          -0.0073 |           3.7875 |           7.0766 |
[32m[20230113 20:08:46 @agent_ppo2.py:186][0m |          -0.0129 |           3.6574 |           7.0794 |
[32m[20230113 20:08:46 @agent_ppo2.py:186][0m |          -0.0142 |           3.5654 |           7.0822 |
[32m[20230113 20:08:46 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.90
[32m[20230113 20:08:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.07
[32m[20230113 20:08:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.57
[32m[20230113 20:08:46 @agent_ppo2.py:144][0m Total time:      24.23 min
[32m[20230113 20:08:46 @agent_ppo2.py:146][0m 2215936 total steps have happened
[32m[20230113 20:08:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1082 --------------------------#
[32m[20230113 20:08:47 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:47 @agent_ppo2.py:186][0m |           0.0003 |           5.7988 |           7.2048 |
[32m[20230113 20:08:47 @agent_ppo2.py:186][0m |          -0.0042 |           4.7422 |           7.1910 |
[32m[20230113 20:08:47 @agent_ppo2.py:186][0m |          -0.0056 |           4.3811 |           7.1870 |
[32m[20230113 20:08:47 @agent_ppo2.py:186][0m |          -0.0075 |           4.1169 |           7.1778 |
[32m[20230113 20:08:47 @agent_ppo2.py:186][0m |          -0.0073 |           3.9067 |           7.1799 |
[32m[20230113 20:08:47 @agent_ppo2.py:186][0m |          -0.0088 |           3.7944 |           7.1785 |
[32m[20230113 20:08:47 @agent_ppo2.py:186][0m |          -0.0101 |           3.6469 |           7.1731 |
[32m[20230113 20:08:47 @agent_ppo2.py:186][0m |          -0.0106 |           3.5312 |           7.1751 |
[32m[20230113 20:08:47 @agent_ppo2.py:186][0m |          -0.0116 |           3.4367 |           7.1745 |
[32m[20230113 20:08:47 @agent_ppo2.py:186][0m |          -0.0114 |           3.3267 |           7.1656 |
[32m[20230113 20:08:47 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.72
[32m[20230113 20:08:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.26
[32m[20230113 20:08:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.56
[32m[20230113 20:08:48 @agent_ppo2.py:144][0m Total time:      24.25 min
[32m[20230113 20:08:48 @agent_ppo2.py:146][0m 2217984 total steps have happened
[32m[20230113 20:08:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1083 --------------------------#
[32m[20230113 20:08:48 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 20:08:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:48 @agent_ppo2.py:186][0m |          -0.0034 |          13.4418 |           7.2034 |
[32m[20230113 20:08:48 @agent_ppo2.py:186][0m |          -0.0063 |           6.0106 |           7.1995 |
[32m[20230113 20:08:48 @agent_ppo2.py:186][0m |          -0.0062 |           4.8165 |           7.2024 |
[32m[20230113 20:08:48 @agent_ppo2.py:186][0m |          -0.0098 |           4.2585 |           7.1923 |
[32m[20230113 20:08:48 @agent_ppo2.py:186][0m |          -0.0099 |           3.9003 |           7.1970 |
[32m[20230113 20:08:48 @agent_ppo2.py:186][0m |          -0.0075 |           3.6421 |           7.1949 |
[32m[20230113 20:08:48 @agent_ppo2.py:186][0m |          -0.0105 |           3.5073 |           7.1981 |
[32m[20230113 20:08:48 @agent_ppo2.py:186][0m |          -0.0111 |           3.3712 |           7.1946 |
[32m[20230113 20:08:49 @agent_ppo2.py:186][0m |          -0.0086 |           3.2906 |           7.1946 |
[32m[20230113 20:08:49 @agent_ppo2.py:186][0m |          -0.0005 |           3.2000 |           7.1984 |
[32m[20230113 20:08:49 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:08:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 147.25
[32m[20230113 20:08:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.08
[32m[20230113 20:08:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.93
[32m[20230113 20:08:49 @agent_ppo2.py:144][0m Total time:      24.27 min
[32m[20230113 20:08:49 @agent_ppo2.py:146][0m 2220032 total steps have happened
[32m[20230113 20:08:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1084 --------------------------#
[32m[20230113 20:08:49 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:08:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:49 @agent_ppo2.py:186][0m |          -0.0017 |          17.1987 |           7.0643 |
[32m[20230113 20:08:50 @agent_ppo2.py:186][0m |          -0.0099 |           7.0753 |           7.0536 |
[32m[20230113 20:08:50 @agent_ppo2.py:186][0m |          -0.0119 |           5.3600 |           7.0531 |
[32m[20230113 20:08:50 @agent_ppo2.py:186][0m |          -0.0154 |           4.5732 |           7.0521 |
[32m[20230113 20:08:50 @agent_ppo2.py:186][0m |          -0.0156 |           4.0487 |           7.0502 |
[32m[20230113 20:08:50 @agent_ppo2.py:186][0m |          -0.0175 |           3.7486 |           7.0495 |
[32m[20230113 20:08:50 @agent_ppo2.py:186][0m |          -0.0176 |           3.4747 |           7.0504 |
[32m[20230113 20:08:50 @agent_ppo2.py:186][0m |          -0.0179 |           3.2798 |           7.0480 |
[32m[20230113 20:08:50 @agent_ppo2.py:186][0m |          -0.0194 |           3.1477 |           7.0525 |
[32m[20230113 20:08:50 @agent_ppo2.py:186][0m |          -0.0201 |           3.0113 |           7.0524 |
[32m[20230113 20:08:50 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:08:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 142.80
[32m[20230113 20:08:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.40
[32m[20230113 20:08:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.27
[32m[20230113 20:08:50 @agent_ppo2.py:144][0m Total time:      24.29 min
[32m[20230113 20:08:50 @agent_ppo2.py:146][0m 2222080 total steps have happened
[32m[20230113 20:08:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1085 --------------------------#
[32m[20230113 20:08:51 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:51 @agent_ppo2.py:186][0m |          -0.0032 |           6.0881 |           7.0380 |
[32m[20230113 20:08:51 @agent_ppo2.py:186][0m |          -0.0083 |           4.3340 |           7.0348 |
[32m[20230113 20:08:51 @agent_ppo2.py:186][0m |          -0.0069 |           3.7158 |           7.0348 |
[32m[20230113 20:08:51 @agent_ppo2.py:186][0m |          -0.0121 |           3.3938 |           7.0279 |
[32m[20230113 20:08:51 @agent_ppo2.py:186][0m |          -0.0114 |           3.1578 |           7.0257 |
[32m[20230113 20:08:51 @agent_ppo2.py:186][0m |          -0.0136 |           3.0008 |           7.0193 |
[32m[20230113 20:08:51 @agent_ppo2.py:186][0m |          -0.0167 |           2.8941 |           7.0223 |
[32m[20230113 20:08:51 @agent_ppo2.py:186][0m |          -0.0102 |           2.7656 |           7.0202 |
[32m[20230113 20:08:51 @agent_ppo2.py:186][0m |          -0.0176 |           2.6795 |           7.0240 |
[32m[20230113 20:08:51 @agent_ppo2.py:186][0m |          -0.0149 |           2.5947 |           7.0197 |
[32m[20230113 20:08:51 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.98
[32m[20230113 20:08:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.17
[32m[20230113 20:08:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.78
[32m[20230113 20:08:52 @agent_ppo2.py:144][0m Total time:      24.32 min
[32m[20230113 20:08:52 @agent_ppo2.py:146][0m 2224128 total steps have happened
[32m[20230113 20:08:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1086 --------------------------#
[32m[20230113 20:08:52 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:52 @agent_ppo2.py:186][0m |          -0.0009 |           5.5697 |           7.2782 |
[32m[20230113 20:08:52 @agent_ppo2.py:186][0m |          -0.0058 |           4.4973 |           7.2783 |
[32m[20230113 20:08:52 @agent_ppo2.py:186][0m |          -0.0078 |           4.1505 |           7.2773 |
[32m[20230113 20:08:52 @agent_ppo2.py:186][0m |          -0.0110 |           3.8142 |           7.2752 |
[32m[20230113 20:08:52 @agent_ppo2.py:186][0m |          -0.0119 |           3.6310 |           7.2783 |
[32m[20230113 20:08:52 @agent_ppo2.py:186][0m |          -0.0138 |           3.4555 |           7.2745 |
[32m[20230113 20:08:52 @agent_ppo2.py:186][0m |          -0.0132 |           3.3784 |           7.2685 |
[32m[20230113 20:08:52 @agent_ppo2.py:186][0m |          -0.0140 |           3.2180 |           7.2724 |
[32m[20230113 20:08:52 @agent_ppo2.py:186][0m |          -0.0152 |           3.1166 |           7.2697 |
[32m[20230113 20:08:52 @agent_ppo2.py:186][0m |          -0.0154 |           3.0241 |           7.2722 |
[32m[20230113 20:08:52 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:08:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.68
[32m[20230113 20:08:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.78
[32m[20230113 20:08:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.96
[32m[20230113 20:08:53 @agent_ppo2.py:144][0m Total time:      24.34 min
[32m[20230113 20:08:53 @agent_ppo2.py:146][0m 2226176 total steps have happened
[32m[20230113 20:08:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1087 --------------------------#
[32m[20230113 20:08:53 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:53 @agent_ppo2.py:186][0m |          -0.0012 |           6.7681 |           7.2014 |
[32m[20230113 20:08:53 @agent_ppo2.py:186][0m |          -0.0034 |           5.1245 |           7.1883 |
[32m[20230113 20:08:53 @agent_ppo2.py:186][0m |          -0.0047 |           4.4729 |           7.1980 |
[32m[20230113 20:08:54 @agent_ppo2.py:186][0m |          -0.0088 |           4.1357 |           7.1978 |
[32m[20230113 20:08:54 @agent_ppo2.py:186][0m |          -0.0118 |           3.9126 |           7.1899 |
[32m[20230113 20:08:54 @agent_ppo2.py:186][0m |          -0.0077 |           3.7117 |           7.1890 |
[32m[20230113 20:08:54 @agent_ppo2.py:186][0m |          -0.0068 |           3.6937 |           7.2075 |
[32m[20230113 20:08:54 @agent_ppo2.py:186][0m |          -0.0089 |           3.4930 |           7.1982 |
[32m[20230113 20:08:54 @agent_ppo2.py:186][0m |          -0.0124 |           3.3521 |           7.2035 |
[32m[20230113 20:08:54 @agent_ppo2.py:186][0m |          -0.0120 |           3.2577 |           7.2070 |
[32m[20230113 20:08:54 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:08:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.46
[32m[20230113 20:08:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.87
[32m[20230113 20:08:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.90
[32m[20230113 20:08:54 @agent_ppo2.py:144][0m Total time:      24.36 min
[32m[20230113 20:08:54 @agent_ppo2.py:146][0m 2228224 total steps have happened
[32m[20230113 20:08:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1088 --------------------------#
[32m[20230113 20:08:55 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:08:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:55 @agent_ppo2.py:186][0m |           0.0000 |           6.1223 |           7.3845 |
[32m[20230113 20:08:55 @agent_ppo2.py:186][0m |          -0.0067 |           5.1766 |           7.3777 |
[32m[20230113 20:08:55 @agent_ppo2.py:186][0m |          -0.0089 |           4.6786 |           7.3779 |
[32m[20230113 20:08:55 @agent_ppo2.py:186][0m |          -0.0105 |           4.4296 |           7.3742 |
[32m[20230113 20:08:55 @agent_ppo2.py:186][0m |          -0.0104 |           4.1728 |           7.3751 |
[32m[20230113 20:08:55 @agent_ppo2.py:186][0m |          -0.0108 |           4.0101 |           7.3764 |
[32m[20230113 20:08:55 @agent_ppo2.py:186][0m |          -0.0105 |           3.8844 |           7.3777 |
[32m[20230113 20:08:55 @agent_ppo2.py:186][0m |          -0.0118 |           3.8085 |           7.3826 |
[32m[20230113 20:08:55 @agent_ppo2.py:186][0m |          -0.0135 |           3.6336 |           7.3783 |
[32m[20230113 20:08:55 @agent_ppo2.py:186][0m |          -0.0126 |           3.5761 |           7.3824 |
[32m[20230113 20:08:55 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.83
[32m[20230113 20:08:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.74
[32m[20230113 20:08:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.07
[32m[20230113 20:08:56 @agent_ppo2.py:144][0m Total time:      24.38 min
[32m[20230113 20:08:56 @agent_ppo2.py:146][0m 2230272 total steps have happened
[32m[20230113 20:08:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1089 --------------------------#
[32m[20230113 20:08:56 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:56 @agent_ppo2.py:186][0m |          -0.0014 |          19.0238 |           7.2891 |
[32m[20230113 20:08:56 @agent_ppo2.py:186][0m |          -0.0080 |           7.9522 |           7.2737 |
[32m[20230113 20:08:56 @agent_ppo2.py:186][0m |          -0.0103 |           6.3153 |           7.2765 |
[32m[20230113 20:08:56 @agent_ppo2.py:186][0m |          -0.0123 |           5.5130 |           7.2779 |
[32m[20230113 20:08:56 @agent_ppo2.py:186][0m |          -0.0134 |           5.0565 |           7.2744 |
[32m[20230113 20:08:56 @agent_ppo2.py:186][0m |          -0.0150 |           4.6248 |           7.2748 |
[32m[20230113 20:08:56 @agent_ppo2.py:186][0m |          -0.0152 |           4.3330 |           7.2751 |
[32m[20230113 20:08:56 @agent_ppo2.py:186][0m |          -0.0157 |           4.0406 |           7.2707 |
[32m[20230113 20:08:56 @agent_ppo2.py:186][0m |          -0.0161 |           3.9034 |           7.2738 |
[32m[20230113 20:08:56 @agent_ppo2.py:186][0m |          -0.0182 |           3.6837 |           7.2753 |
[32m[20230113 20:08:56 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:08:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 68.54
[32m[20230113 20:08:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.78
[32m[20230113 20:08:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 170.12
[32m[20230113 20:08:57 @agent_ppo2.py:144][0m Total time:      24.40 min
[32m[20230113 20:08:57 @agent_ppo2.py:146][0m 2232320 total steps have happened
[32m[20230113 20:08:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1090 --------------------------#
[32m[20230113 20:08:57 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 20:08:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:57 @agent_ppo2.py:186][0m |          -0.0020 |          33.8253 |           7.2056 |
[32m[20230113 20:08:57 @agent_ppo2.py:186][0m |           0.0338 |          20.6053 |           7.1987 |
[32m[20230113 20:08:57 @agent_ppo2.py:186][0m |           0.0002 |          16.4218 |           7.1596 |
[32m[20230113 20:08:57 @agent_ppo2.py:186][0m |          -0.0146 |          14.1618 |           7.1838 |
[32m[20230113 20:08:57 @agent_ppo2.py:186][0m |          -0.0130 |          12.4446 |           7.1820 |
[32m[20230113 20:08:57 @agent_ppo2.py:186][0m |          -0.0056 |          11.6873 |           7.1862 |
[32m[20230113 20:08:57 @agent_ppo2.py:186][0m |          -0.0167 |          10.4506 |           7.1795 |
[32m[20230113 20:08:57 @agent_ppo2.py:186][0m |          -0.0175 |           9.5476 |           7.1790 |
[32m[20230113 20:08:57 @agent_ppo2.py:186][0m |          -0.0194 |           8.7978 |           7.1759 |
[32m[20230113 20:08:57 @agent_ppo2.py:186][0m |          -0.0175 |           8.3898 |           7.1842 |
[32m[20230113 20:08:57 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:08:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 119.79
[32m[20230113 20:08:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.62
[32m[20230113 20:08:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.14
[32m[20230113 20:08:58 @agent_ppo2.py:144][0m Total time:      24.42 min
[32m[20230113 20:08:58 @agent_ppo2.py:146][0m 2234368 total steps have happened
[32m[20230113 20:08:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1091 --------------------------#
[32m[20230113 20:08:58 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:08:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:08:58 @agent_ppo2.py:186][0m |           0.0022 |           6.8987 |           7.2021 |
[32m[20230113 20:08:58 @agent_ppo2.py:186][0m |          -0.0093 |           4.1661 |           7.1836 |
[32m[20230113 20:08:59 @agent_ppo2.py:186][0m |          -0.0122 |           3.6763 |           7.1861 |
[32m[20230113 20:08:59 @agent_ppo2.py:186][0m |          -0.0113 |           3.4221 |           7.1818 |
[32m[20230113 20:08:59 @agent_ppo2.py:186][0m |          -0.0125 |           3.1993 |           7.1825 |
[32m[20230113 20:08:59 @agent_ppo2.py:186][0m |          -0.0140 |           3.0621 |           7.1781 |
[32m[20230113 20:08:59 @agent_ppo2.py:186][0m |          -0.0157 |           2.9340 |           7.1852 |
[32m[20230113 20:08:59 @agent_ppo2.py:186][0m |          -0.0153 |           2.8142 |           7.1854 |
[32m[20230113 20:08:59 @agent_ppo2.py:186][0m |          -0.0164 |           2.7244 |           7.1808 |
[32m[20230113 20:08:59 @agent_ppo2.py:186][0m |          -0.0182 |           2.6329 |           7.1774 |
[32m[20230113 20:08:59 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:08:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.21
[32m[20230113 20:08:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.64
[32m[20230113 20:08:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.09
[32m[20230113 20:08:59 @agent_ppo2.py:144][0m Total time:      24.44 min
[32m[20230113 20:08:59 @agent_ppo2.py:146][0m 2236416 total steps have happened
[32m[20230113 20:08:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1092 --------------------------#
[32m[20230113 20:09:00 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:09:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:00 @agent_ppo2.py:186][0m |           0.0004 |          20.0589 |           7.2122 |
[32m[20230113 20:09:00 @agent_ppo2.py:186][0m |          -0.0015 |          10.6728 |           7.2027 |
[32m[20230113 20:09:00 @agent_ppo2.py:186][0m |          -0.0081 |           8.6811 |           7.1943 |
[32m[20230113 20:09:00 @agent_ppo2.py:186][0m |          -0.0108 |           7.2680 |           7.2009 |
[32m[20230113 20:09:00 @agent_ppo2.py:186][0m |          -0.0102 |           6.5095 |           7.2003 |
[32m[20230113 20:09:00 @agent_ppo2.py:186][0m |          -0.0118 |           5.9831 |           7.2004 |
[32m[20230113 20:09:00 @agent_ppo2.py:186][0m |          -0.0153 |           5.7221 |           7.1982 |
[32m[20230113 20:09:00 @agent_ppo2.py:186][0m |          -0.0163 |           5.4546 |           7.1992 |
[32m[20230113 20:09:00 @agent_ppo2.py:186][0m |          -0.0165 |           5.1520 |           7.2017 |
[32m[20230113 20:09:00 @agent_ppo2.py:186][0m |          -0.0196 |           5.0793 |           7.1985 |
[32m[20230113 20:09:00 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:09:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 114.00
[32m[20230113 20:09:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.44
[32m[20230113 20:09:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.09
[32m[20230113 20:09:00 @agent_ppo2.py:144][0m Total time:      24.47 min
[32m[20230113 20:09:00 @agent_ppo2.py:146][0m 2238464 total steps have happened
[32m[20230113 20:09:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1093 --------------------------#
[32m[20230113 20:09:01 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 20:09:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:01 @agent_ppo2.py:186][0m |           0.0016 |          13.7980 |           7.3391 |
[32m[20230113 20:09:01 @agent_ppo2.py:186][0m |          -0.0054 |           6.9163 |           7.3328 |
[32m[20230113 20:09:01 @agent_ppo2.py:186][0m |          -0.0111 |           5.6708 |           7.3286 |
[32m[20230113 20:09:01 @agent_ppo2.py:186][0m |          -0.0118 |           5.3391 |           7.3227 |
[32m[20230113 20:09:01 @agent_ppo2.py:186][0m |          -0.0172 |           4.6991 |           7.3148 |
[32m[20230113 20:09:01 @agent_ppo2.py:186][0m |          -0.0158 |           4.4244 |           7.3124 |
[32m[20230113 20:09:01 @agent_ppo2.py:186][0m |          -0.0158 |           4.2226 |           7.3064 |
[32m[20230113 20:09:01 @agent_ppo2.py:186][0m |          -0.0156 |           3.9774 |           7.3032 |
[32m[20230113 20:09:01 @agent_ppo2.py:186][0m |          -0.0163 |           3.8019 |           7.3060 |
[32m[20230113 20:09:01 @agent_ppo2.py:186][0m |          -0.0211 |           3.6607 |           7.3064 |
[32m[20230113 20:09:01 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:09:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 145.26
[32m[20230113 20:09:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.69
[32m[20230113 20:09:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.48
[32m[20230113 20:09:02 @agent_ppo2.py:144][0m Total time:      24.49 min
[32m[20230113 20:09:02 @agent_ppo2.py:146][0m 2240512 total steps have happened
[32m[20230113 20:09:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1094 --------------------------#
[32m[20230113 20:09:02 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:09:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:02 @agent_ppo2.py:186][0m |           0.0003 |           5.7362 |           7.1527 |
[32m[20230113 20:09:02 @agent_ppo2.py:186][0m |          -0.0038 |           4.6725 |           7.1455 |
[32m[20230113 20:09:02 @agent_ppo2.py:186][0m |          -0.0027 |           4.1687 |           7.1381 |
[32m[20230113 20:09:02 @agent_ppo2.py:186][0m |          -0.0068 |           3.8427 |           7.1274 |
[32m[20230113 20:09:02 @agent_ppo2.py:186][0m |          -0.0091 |           3.5982 |           7.1225 |
[32m[20230113 20:09:02 @agent_ppo2.py:186][0m |          -0.0094 |           3.4538 |           7.1245 |
[32m[20230113 20:09:02 @agent_ppo2.py:186][0m |          -0.0104 |           3.3146 |           7.1120 |
[32m[20230113 20:09:03 @agent_ppo2.py:186][0m |          -0.0100 |           3.2238 |           7.1144 |
[32m[20230113 20:09:03 @agent_ppo2.py:186][0m |          -0.0116 |           3.1168 |           7.1178 |
[32m[20230113 20:09:03 @agent_ppo2.py:186][0m |          -0.0119 |           3.0260 |           7.1085 |
[32m[20230113 20:09:03 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:09:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.47
[32m[20230113 20:09:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.22
[32m[20230113 20:09:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.78
[32m[20230113 20:09:03 @agent_ppo2.py:144][0m Total time:      24.51 min
[32m[20230113 20:09:03 @agent_ppo2.py:146][0m 2242560 total steps have happened
[32m[20230113 20:09:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1095 --------------------------#
[32m[20230113 20:09:03 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:09:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:04 @agent_ppo2.py:186][0m |          -0.0002 |           9.2311 |           7.1357 |
[32m[20230113 20:09:04 @agent_ppo2.py:186][0m |          -0.0118 |           7.2844 |           7.1223 |
[32m[20230113 20:09:04 @agent_ppo2.py:186][0m |          -0.0137 |           6.5051 |           7.1174 |
[32m[20230113 20:09:04 @agent_ppo2.py:186][0m |          -0.0153 |           5.9939 |           7.1107 |
[32m[20230113 20:09:04 @agent_ppo2.py:186][0m |          -0.0120 |           5.7552 |           7.1170 |
[32m[20230113 20:09:04 @agent_ppo2.py:186][0m |          -0.0141 |           5.4401 |           7.1123 |
[32m[20230113 20:09:04 @agent_ppo2.py:186][0m |          -0.0170 |           5.2554 |           7.1117 |
[32m[20230113 20:09:04 @agent_ppo2.py:186][0m |          -0.0144 |           5.0509 |           7.1097 |
[32m[20230113 20:09:04 @agent_ppo2.py:186][0m |          -0.0169 |           4.9083 |           7.1077 |
[32m[20230113 20:09:04 @agent_ppo2.py:186][0m |          -0.0149 |           4.7806 |           7.1080 |
[32m[20230113 20:09:04 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:09:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.50
[32m[20230113 20:09:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.88
[32m[20230113 20:09:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.07
[32m[20230113 20:09:04 @agent_ppo2.py:144][0m Total time:      24.53 min
[32m[20230113 20:09:04 @agent_ppo2.py:146][0m 2244608 total steps have happened
[32m[20230113 20:09:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1096 --------------------------#
[32m[20230113 20:09:05 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:09:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:05 @agent_ppo2.py:186][0m |          -0.0006 |           5.4869 |           7.1707 |
[32m[20230113 20:09:05 @agent_ppo2.py:186][0m |          -0.0043 |           4.3116 |           7.1730 |
[32m[20230113 20:09:05 @agent_ppo2.py:186][0m |          -0.0076 |           3.8248 |           7.1678 |
[32m[20230113 20:09:05 @agent_ppo2.py:186][0m |          -0.0109 |           3.5657 |           7.1643 |
[32m[20230113 20:09:05 @agent_ppo2.py:186][0m |          -0.0175 |           3.4599 |           7.1656 |
[32m[20230113 20:09:05 @agent_ppo2.py:186][0m |          -0.0121 |           3.2857 |           7.1630 |
[32m[20230113 20:09:05 @agent_ppo2.py:186][0m |          -0.0120 |           3.1793 |           7.1504 |
[32m[20230113 20:09:05 @agent_ppo2.py:186][0m |          -0.0140 |           3.0615 |           7.1563 |
[32m[20230113 20:09:05 @agent_ppo2.py:186][0m |          -0.0150 |           3.0026 |           7.1573 |
[32m[20230113 20:09:05 @agent_ppo2.py:186][0m |          -0.0146 |           2.9038 |           7.1571 |
[32m[20230113 20:09:05 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:09:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.68
[32m[20230113 20:09:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.75
[32m[20230113 20:09:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.66
[32m[20230113 20:09:06 @agent_ppo2.py:144][0m Total time:      24.55 min
[32m[20230113 20:09:06 @agent_ppo2.py:146][0m 2246656 total steps have happened
[32m[20230113 20:09:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1097 --------------------------#
[32m[20230113 20:09:06 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:09:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:06 @agent_ppo2.py:186][0m |          -0.0005 |           6.7721 |           7.2665 |
[32m[20230113 20:09:06 @agent_ppo2.py:186][0m |          -0.0061 |           5.2245 |           7.2567 |
[32m[20230113 20:09:06 @agent_ppo2.py:186][0m |          -0.0092 |           4.5618 |           7.2487 |
[32m[20230113 20:09:06 @agent_ppo2.py:186][0m |          -0.0095 |           4.2295 |           7.2627 |
[32m[20230113 20:09:06 @agent_ppo2.py:186][0m |          -0.0119 |           3.9806 |           7.2527 |
[32m[20230113 20:09:06 @agent_ppo2.py:186][0m |          -0.0127 |           3.8265 |           7.2517 |
[32m[20230113 20:09:06 @agent_ppo2.py:186][0m |          -0.0132 |           3.6382 |           7.2565 |
[32m[20230113 20:09:07 @agent_ppo2.py:186][0m |          -0.0142 |           3.5368 |           7.2597 |
[32m[20230113 20:09:07 @agent_ppo2.py:186][0m |          -0.0158 |           3.4085 |           7.2563 |
[32m[20230113 20:09:07 @agent_ppo2.py:186][0m |          -0.0158 |           3.3357 |           7.2555 |
[32m[20230113 20:09:07 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.54
[32m[20230113 20:09:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.31
[32m[20230113 20:09:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.03
[32m[20230113 20:09:07 @agent_ppo2.py:144][0m Total time:      24.57 min
[32m[20230113 20:09:07 @agent_ppo2.py:146][0m 2248704 total steps have happened
[32m[20230113 20:09:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1098 --------------------------#
[32m[20230113 20:09:07 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:09:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:08 @agent_ppo2.py:186][0m |          -0.0002 |           5.6188 |           7.2193 |
[32m[20230113 20:09:08 @agent_ppo2.py:186][0m |          -0.0041 |           4.7842 |           7.2226 |
[32m[20230113 20:09:08 @agent_ppo2.py:186][0m |          -0.0063 |           4.3997 |           7.2122 |
[32m[20230113 20:09:08 @agent_ppo2.py:186][0m |          -0.0090 |           4.1781 |           7.2175 |
[32m[20230113 20:09:08 @agent_ppo2.py:186][0m |          -0.0105 |           3.9956 |           7.2122 |
[32m[20230113 20:09:08 @agent_ppo2.py:186][0m |          -0.0113 |           3.8536 |           7.2142 |
[32m[20230113 20:09:08 @agent_ppo2.py:186][0m |          -0.0117 |           3.7149 |           7.2155 |
[32m[20230113 20:09:08 @agent_ppo2.py:186][0m |          -0.0127 |           3.6174 |           7.2180 |
[32m[20230113 20:09:08 @agent_ppo2.py:186][0m |          -0.0124 |           3.5179 |           7.2138 |
[32m[20230113 20:09:08 @agent_ppo2.py:186][0m |          -0.0127 |           3.4352 |           7.2128 |
[32m[20230113 20:09:08 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:09:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.81
[32m[20230113 20:09:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.03
[32m[20230113 20:09:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 229.88
[32m[20230113 20:09:08 @agent_ppo2.py:144][0m Total time:      24.60 min
[32m[20230113 20:09:08 @agent_ppo2.py:146][0m 2250752 total steps have happened
[32m[20230113 20:09:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1099 --------------------------#
[32m[20230113 20:09:09 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:09:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:09 @agent_ppo2.py:186][0m |          -0.0002 |           4.6397 |           7.1154 |
[32m[20230113 20:09:09 @agent_ppo2.py:186][0m |          -0.0072 |           4.1391 |           7.1006 |
[32m[20230113 20:09:09 @agent_ppo2.py:186][0m |          -0.0084 |           3.9258 |           7.1116 |
[32m[20230113 20:09:09 @agent_ppo2.py:186][0m |          -0.0104 |           3.6412 |           7.0997 |
[32m[20230113 20:09:09 @agent_ppo2.py:186][0m |          -0.0179 |           3.4488 |           7.1080 |
[32m[20230113 20:09:09 @agent_ppo2.py:186][0m |          -0.0081 |           3.3145 |           7.1122 |
[32m[20230113 20:09:09 @agent_ppo2.py:186][0m |          -0.0086 |           3.1972 |           7.1201 |
[32m[20230113 20:09:09 @agent_ppo2.py:186][0m |          -0.0079 |           3.1555 |           7.1158 |
[32m[20230113 20:09:09 @agent_ppo2.py:186][0m |          -0.0091 |           3.0690 |           7.1164 |
[32m[20230113 20:09:09 @agent_ppo2.py:186][0m |          -0.0178 |           2.9712 |           7.1099 |
[32m[20230113 20:09:09 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.18
[32m[20230113 20:09:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.89
[32m[20230113 20:09:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.11
[32m[20230113 20:09:10 @agent_ppo2.py:144][0m Total time:      24.62 min
[32m[20230113 20:09:10 @agent_ppo2.py:146][0m 2252800 total steps have happened
[32m[20230113 20:09:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1100 --------------------------#
[32m[20230113 20:09:10 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:09:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:10 @agent_ppo2.py:186][0m |           0.0054 |          10.4855 |           7.2313 |
[32m[20230113 20:09:10 @agent_ppo2.py:186][0m |          -0.0024 |           5.2698 |           7.2212 |
[32m[20230113 20:09:10 @agent_ppo2.py:186][0m |          -0.0062 |           4.3822 |           7.2226 |
[32m[20230113 20:09:10 @agent_ppo2.py:186][0m |          -0.0074 |           4.0778 |           7.2151 |
[32m[20230113 20:09:10 @agent_ppo2.py:186][0m |          -0.0091 |           3.8365 |           7.2187 |
[32m[20230113 20:09:10 @agent_ppo2.py:186][0m |          -0.0119 |           3.6505 |           7.2165 |
[32m[20230113 20:09:10 @agent_ppo2.py:186][0m |          -0.0118 |           3.5174 |           7.2154 |
[32m[20230113 20:09:10 @agent_ppo2.py:186][0m |          -0.0117 |           3.3979 |           7.2137 |
[32m[20230113 20:09:10 @agent_ppo2.py:186][0m |          -0.0123 |           3.3029 |           7.2178 |
[32m[20230113 20:09:11 @agent_ppo2.py:186][0m |          -0.0132 |           3.1868 |           7.2170 |
[32m[20230113 20:09:11 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:09:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 155.24
[32m[20230113 20:09:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 219.69
[32m[20230113 20:09:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.62
[32m[20230113 20:09:11 @agent_ppo2.py:144][0m Total time:      24.64 min
[32m[20230113 20:09:11 @agent_ppo2.py:146][0m 2254848 total steps have happened
[32m[20230113 20:09:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1101 --------------------------#
[32m[20230113 20:09:11 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:09:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:11 @agent_ppo2.py:186][0m |          -0.0005 |           5.6917 |           7.2803 |
[32m[20230113 20:09:12 @agent_ppo2.py:186][0m |          -0.0015 |           4.6260 |           7.2791 |
[32m[20230113 20:09:12 @agent_ppo2.py:186][0m |          -0.0051 |           4.2362 |           7.2791 |
[32m[20230113 20:09:12 @agent_ppo2.py:186][0m |          -0.0112 |           4.0011 |           7.2781 |
[32m[20230113 20:09:12 @agent_ppo2.py:186][0m |          -0.0106 |           3.7973 |           7.2718 |
[32m[20230113 20:09:12 @agent_ppo2.py:186][0m |          -0.0142 |           3.7013 |           7.2753 |
[32m[20230113 20:09:12 @agent_ppo2.py:186][0m |          -0.0113 |           3.6057 |           7.2730 |
[32m[20230113 20:09:12 @agent_ppo2.py:186][0m |          -0.0132 |           3.5147 |           7.2647 |
[32m[20230113 20:09:12 @agent_ppo2.py:186][0m |          -0.0164 |           3.4132 |           7.2645 |
[32m[20230113 20:09:12 @agent_ppo2.py:186][0m |          -0.0153 |           3.3548 |           7.2710 |
[32m[20230113 20:09:12 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.65
[32m[20230113 20:09:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.53
[32m[20230113 20:09:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.92
[32m[20230113 20:09:12 @agent_ppo2.py:144][0m Total time:      24.66 min
[32m[20230113 20:09:12 @agent_ppo2.py:146][0m 2256896 total steps have happened
[32m[20230113 20:09:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1102 --------------------------#
[32m[20230113 20:09:13 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:09:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:13 @agent_ppo2.py:186][0m |           0.0068 |           9.2187 |           7.1865 |
[32m[20230113 20:09:13 @agent_ppo2.py:186][0m |          -0.0104 |           6.1376 |           7.1858 |
[32m[20230113 20:09:13 @agent_ppo2.py:186][0m |           0.0002 |           5.2055 |           7.1741 |
[32m[20230113 20:09:13 @agent_ppo2.py:186][0m |          -0.0075 |           4.5372 |           7.1720 |
[32m[20230113 20:09:13 @agent_ppo2.py:186][0m |          -0.0147 |           4.1426 |           7.1722 |
[32m[20230113 20:09:13 @agent_ppo2.py:186][0m |          -0.0042 |           3.8786 |           7.1699 |
[32m[20230113 20:09:13 @agent_ppo2.py:186][0m |          -0.0044 |           3.8384 |           7.1739 |
[32m[20230113 20:09:13 @agent_ppo2.py:186][0m |          -0.0052 |           3.5771 |           7.1762 |
[32m[20230113 20:09:13 @agent_ppo2.py:186][0m |          -0.0094 |           3.3433 |           7.1760 |
[32m[20230113 20:09:13 @agent_ppo2.py:186][0m |          -0.0037 |           3.2713 |           7.1808 |
[32m[20230113 20:09:13 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:09:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 206.06
[32m[20230113 20:09:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.39
[32m[20230113 20:09:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.53
[32m[20230113 20:09:14 @agent_ppo2.py:144][0m Total time:      24.68 min
[32m[20230113 20:09:14 @agent_ppo2.py:146][0m 2258944 total steps have happened
[32m[20230113 20:09:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1103 --------------------------#
[32m[20230113 20:09:14 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:09:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:14 @agent_ppo2.py:186][0m |           0.0008 |           7.3552 |           7.2295 |
[32m[20230113 20:09:14 @agent_ppo2.py:186][0m |          -0.0064 |           5.3018 |           7.2248 |
[32m[20230113 20:09:14 @agent_ppo2.py:186][0m |          -0.0102 |           4.6355 |           7.2268 |
[32m[20230113 20:09:14 @agent_ppo2.py:186][0m |          -0.0114 |           4.2451 |           7.2234 |
[32m[20230113 20:09:14 @agent_ppo2.py:186][0m |          -0.0136 |           3.9884 |           7.2209 |
[32m[20230113 20:09:14 @agent_ppo2.py:186][0m |          -0.0155 |           3.7529 |           7.2226 |
[32m[20230113 20:09:14 @agent_ppo2.py:186][0m |          -0.0153 |           3.6178 |           7.2215 |
[32m[20230113 20:09:14 @agent_ppo2.py:186][0m |          -0.0168 |           3.4678 |           7.2201 |
[32m[20230113 20:09:14 @agent_ppo2.py:186][0m |          -0.0172 |           3.3400 |           7.2209 |
[32m[20230113 20:09:14 @agent_ppo2.py:186][0m |          -0.0180 |           3.2565 |           7.2176 |
[32m[20230113 20:09:14 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.61
[32m[20230113 20:09:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.07
[32m[20230113 20:09:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.79
[32m[20230113 20:09:15 @agent_ppo2.py:144][0m Total time:      24.71 min
[32m[20230113 20:09:15 @agent_ppo2.py:146][0m 2260992 total steps have happened
[32m[20230113 20:09:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1104 --------------------------#
[32m[20230113 20:09:15 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:09:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:15 @agent_ppo2.py:186][0m |           0.0035 |           7.0428 |           7.2840 |
[32m[20230113 20:09:15 @agent_ppo2.py:186][0m |          -0.0021 |           5.2963 |           7.2702 |
[32m[20230113 20:09:15 @agent_ppo2.py:186][0m |          -0.0056 |           4.7487 |           7.2684 |
[32m[20230113 20:09:16 @agent_ppo2.py:186][0m |          -0.0018 |           4.4711 |           7.2723 |
[32m[20230113 20:09:16 @agent_ppo2.py:186][0m |          -0.0114 |           3.9564 |           7.2716 |
[32m[20230113 20:09:16 @agent_ppo2.py:186][0m |          -0.0140 |           3.7261 |           7.2659 |
[32m[20230113 20:09:16 @agent_ppo2.py:186][0m |          -0.0159 |           3.5137 |           7.2702 |
[32m[20230113 20:09:16 @agent_ppo2.py:186][0m |          -0.0117 |           3.3336 |           7.2693 |
[32m[20230113 20:09:16 @agent_ppo2.py:186][0m |          -0.0193 |           3.1719 |           7.2617 |
[32m[20230113 20:09:16 @agent_ppo2.py:186][0m |          -0.0163 |           3.0815 |           7.2669 |
[32m[20230113 20:09:16 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:09:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.77
[32m[20230113 20:09:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.30
[32m[20230113 20:09:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.88
[32m[20230113 20:09:16 @agent_ppo2.py:144][0m Total time:      24.73 min
[32m[20230113 20:09:16 @agent_ppo2.py:146][0m 2263040 total steps have happened
[32m[20230113 20:09:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1105 --------------------------#
[32m[20230113 20:09:17 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:09:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:17 @agent_ppo2.py:186][0m |           0.0025 |           5.5337 |           7.3371 |
[32m[20230113 20:09:17 @agent_ppo2.py:186][0m |           0.0024 |           4.3608 |           7.3348 |
[32m[20230113 20:09:17 @agent_ppo2.py:186][0m |          -0.0040 |           3.9847 |           7.3379 |
[32m[20230113 20:09:17 @agent_ppo2.py:186][0m |           0.0033 |           3.8000 |           7.3332 |
[32m[20230113 20:09:17 @agent_ppo2.py:186][0m |          -0.0083 |           3.5726 |           7.3358 |
[32m[20230113 20:09:17 @agent_ppo2.py:186][0m |          -0.0086 |           3.4301 |           7.3318 |
[32m[20230113 20:09:17 @agent_ppo2.py:186][0m |          -0.0051 |           3.4544 |           7.3298 |
[32m[20230113 20:09:17 @agent_ppo2.py:186][0m |          -0.0198 |           3.2245 |           7.3323 |
[32m[20230113 20:09:17 @agent_ppo2.py:186][0m |          -0.0046 |           3.2364 |           7.3343 |
[32m[20230113 20:09:17 @agent_ppo2.py:186][0m |          -0.0108 |           3.0367 |           7.3314 |
[32m[20230113 20:09:17 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.84
[32m[20230113 20:09:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.57
[32m[20230113 20:09:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 228.56
[32m[20230113 20:09:18 @agent_ppo2.py:144][0m Total time:      24.75 min
[32m[20230113 20:09:18 @agent_ppo2.py:146][0m 2265088 total steps have happened
[32m[20230113 20:09:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1106 --------------------------#
[32m[20230113 20:09:18 @agent_ppo2.py:128][0m Sampling time: 0.31 s by 1 slaves
[32m[20230113 20:09:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:18 @agent_ppo2.py:186][0m |          -0.0027 |          25.5736 |           7.2066 |
[32m[20230113 20:09:18 @agent_ppo2.py:186][0m |          -0.0092 |           8.9437 |           7.1945 |
[32m[20230113 20:09:18 @agent_ppo2.py:186][0m |          -0.0119 |           6.4496 |           7.1873 |
[32m[20230113 20:09:18 @agent_ppo2.py:186][0m |          -0.0131 |           5.2299 |           7.1932 |
[32m[20230113 20:09:18 @agent_ppo2.py:186][0m |          -0.0149 |           4.7251 |           7.1904 |
[32m[20230113 20:09:18 @agent_ppo2.py:186][0m |          -0.0154 |           4.3110 |           7.1878 |
[32m[20230113 20:09:18 @agent_ppo2.py:186][0m |          -0.0174 |           4.0531 |           7.1876 |
[32m[20230113 20:09:18 @agent_ppo2.py:186][0m |          -0.0161 |           3.9035 |           7.1868 |
[32m[20230113 20:09:18 @agent_ppo2.py:186][0m |          -0.0164 |           3.6471 |           7.1863 |
[32m[20230113 20:09:18 @agent_ppo2.py:186][0m |          -0.0185 |           3.5004 |           7.1849 |
[32m[20230113 20:09:18 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 20:09:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 90.34
[32m[20230113 20:09:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 217.97
[32m[20230113 20:09:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.81
[32m[20230113 20:09:19 @agent_ppo2.py:144][0m Total time:      24.77 min
[32m[20230113 20:09:19 @agent_ppo2.py:146][0m 2267136 total steps have happened
[32m[20230113 20:09:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1107 --------------------------#
[32m[20230113 20:09:19 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:09:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:19 @agent_ppo2.py:186][0m |          -0.0022 |           6.1810 |           7.3149 |
[32m[20230113 20:09:19 @agent_ppo2.py:186][0m |          -0.0056 |           4.7068 |           7.3103 |
[32m[20230113 20:09:19 @agent_ppo2.py:186][0m |          -0.0078 |           4.2629 |           7.3080 |
[32m[20230113 20:09:19 @agent_ppo2.py:186][0m |          -0.0101 |           4.0060 |           7.2989 |
[32m[20230113 20:09:19 @agent_ppo2.py:186][0m |          -0.0061 |           3.8052 |           7.2919 |
[32m[20230113 20:09:19 @agent_ppo2.py:186][0m |          -0.0135 |           3.6298 |           7.2968 |
[32m[20230113 20:09:19 @agent_ppo2.py:186][0m |          -0.0106 |           3.4856 |           7.2887 |
[32m[20230113 20:09:19 @agent_ppo2.py:186][0m |          -0.0116 |           3.3525 |           7.2846 |
[32m[20230113 20:09:19 @agent_ppo2.py:186][0m |          -0.0150 |           3.2376 |           7.2881 |
[32m[20230113 20:09:19 @agent_ppo2.py:186][0m |          -0.0144 |           3.1693 |           7.2817 |
[32m[20230113 20:09:19 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.08
[32m[20230113 20:09:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.60
[32m[20230113 20:09:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.63
[32m[20230113 20:09:20 @agent_ppo2.py:144][0m Total time:      24.79 min
[32m[20230113 20:09:20 @agent_ppo2.py:146][0m 2269184 total steps have happened
[32m[20230113 20:09:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1108 --------------------------#
[32m[20230113 20:09:20 @agent_ppo2.py:128][0m Sampling time: 0.53 s by 1 slaves
[32m[20230113 20:09:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:21 @agent_ppo2.py:186][0m |          -0.0027 |          14.3236 |           7.1428 |
[32m[20230113 20:09:21 @agent_ppo2.py:186][0m |          -0.0127 |           8.3498 |           7.1240 |
[32m[20230113 20:09:21 @agent_ppo2.py:186][0m |          -0.0150 |           6.6323 |           7.1195 |
[32m[20230113 20:09:21 @agent_ppo2.py:186][0m |          -0.0155 |           5.7634 |           7.1180 |
[32m[20230113 20:09:21 @agent_ppo2.py:186][0m |          -0.0195 |           5.2117 |           7.1213 |
[32m[20230113 20:09:21 @agent_ppo2.py:186][0m |          -0.0185 |           4.7975 |           7.1192 |
[32m[20230113 20:09:21 @agent_ppo2.py:186][0m |          -0.0168 |           4.5438 |           7.1143 |
[32m[20230113 20:09:21 @agent_ppo2.py:186][0m |          -0.0200 |           4.2764 |           7.1207 |
[32m[20230113 20:09:21 @agent_ppo2.py:186][0m |          -0.0213 |           4.1198 |           7.1183 |
[32m[20230113 20:09:21 @agent_ppo2.py:186][0m |          -0.0218 |           3.9096 |           7.1177 |
[32m[20230113 20:09:21 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 20:09:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 136.14
[32m[20230113 20:09:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.73
[32m[20230113 20:09:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.62
[32m[20230113 20:09:21 @agent_ppo2.py:144][0m Total time:      24.81 min
[32m[20230113 20:09:21 @agent_ppo2.py:146][0m 2271232 total steps have happened
[32m[20230113 20:09:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1109 --------------------------#
[32m[20230113 20:09:22 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:09:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:22 @agent_ppo2.py:186][0m |          -0.0075 |           6.9214 |           7.3223 |
[32m[20230113 20:09:22 @agent_ppo2.py:186][0m |          -0.0061 |           4.3967 |           7.3124 |
[32m[20230113 20:09:22 @agent_ppo2.py:186][0m |          -0.0123 |           3.8491 |           7.3074 |
[32m[20230113 20:09:22 @agent_ppo2.py:186][0m |          -0.0112 |           3.4914 |           7.3082 |
[32m[20230113 20:09:22 @agent_ppo2.py:186][0m |          -0.0153 |           3.2810 |           7.3069 |
[32m[20230113 20:09:22 @agent_ppo2.py:186][0m |          -0.0068 |           3.1828 |           7.3099 |
[32m[20230113 20:09:22 @agent_ppo2.py:186][0m |          -0.0023 |           2.9931 |           7.3084 |
[32m[20230113 20:09:22 @agent_ppo2.py:186][0m |          -0.0181 |           2.8562 |           7.3046 |
[32m[20230113 20:09:22 @agent_ppo2.py:186][0m |          -0.0092 |           2.7786 |           7.3059 |
[32m[20230113 20:09:22 @agent_ppo2.py:186][0m |          -0.0155 |           2.6671 |           7.3065 |
[32m[20230113 20:09:22 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.31
[32m[20230113 20:09:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.08
[32m[20230113 20:09:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.09
[32m[20230113 20:09:23 @agent_ppo2.py:144][0m Total time:      24.84 min
[32m[20230113 20:09:23 @agent_ppo2.py:146][0m 2273280 total steps have happened
[32m[20230113 20:09:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1110 --------------------------#
[32m[20230113 20:09:23 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:09:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:23 @agent_ppo2.py:186][0m |          -0.0078 |           6.3999 |           7.1177 |
[32m[20230113 20:09:23 @agent_ppo2.py:186][0m |           0.0007 |           5.1214 |           7.1107 |
[32m[20230113 20:09:23 @agent_ppo2.py:186][0m |          -0.0040 |           4.4416 |           7.1109 |
[32m[20230113 20:09:23 @agent_ppo2.py:186][0m |          -0.0122 |           4.0268 |           7.1095 |
[32m[20230113 20:09:23 @agent_ppo2.py:186][0m |           0.0056 |           3.7259 |           7.1112 |
[32m[20230113 20:09:23 @agent_ppo2.py:186][0m |          -0.0052 |           3.4645 |           7.0954 |
[32m[20230113 20:09:23 @agent_ppo2.py:186][0m |          -0.0037 |           3.1668 |           7.0985 |
[32m[20230113 20:09:24 @agent_ppo2.py:186][0m |          -0.0054 |           2.9959 |           7.0969 |
[32m[20230113 20:09:24 @agent_ppo2.py:186][0m |          -0.0123 |           2.9148 |           7.0918 |
[32m[20230113 20:09:24 @agent_ppo2.py:186][0m |          -0.0115 |           2.7923 |           7.0919 |
[32m[20230113 20:09:24 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:09:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.95
[32m[20230113 20:09:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.60
[32m[20230113 20:09:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.87
[32m[20230113 20:09:24 @agent_ppo2.py:144][0m Total time:      24.86 min
[32m[20230113 20:09:24 @agent_ppo2.py:146][0m 2275328 total steps have happened
[32m[20230113 20:09:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1111 --------------------------#
[32m[20230113 20:09:24 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:09:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:25 @agent_ppo2.py:186][0m |           0.0014 |           5.9720 |           7.2028 |
[32m[20230113 20:09:25 @agent_ppo2.py:186][0m |          -0.0049 |           5.1014 |           7.1994 |
[32m[20230113 20:09:25 @agent_ppo2.py:186][0m |          -0.0068 |           4.7806 |           7.2054 |
[32m[20230113 20:09:25 @agent_ppo2.py:186][0m |          -0.0087 |           4.5281 |           7.2063 |
[32m[20230113 20:09:25 @agent_ppo2.py:186][0m |          -0.0098 |           4.3454 |           7.1983 |
[32m[20230113 20:09:25 @agent_ppo2.py:186][0m |          -0.0111 |           4.1665 |           7.2067 |
[32m[20230113 20:09:25 @agent_ppo2.py:186][0m |          -0.0119 |           4.0589 |           7.2007 |
[32m[20230113 20:09:25 @agent_ppo2.py:186][0m |          -0.0125 |           3.9565 |           7.2053 |
[32m[20230113 20:09:25 @agent_ppo2.py:186][0m |          -0.0130 |           3.8438 |           7.2023 |
[32m[20230113 20:09:25 @agent_ppo2.py:186][0m |          -0.0135 |           3.7695 |           7.2076 |
[32m[20230113 20:09:25 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.81
[32m[20230113 20:09:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.37
[32m[20230113 20:09:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.21
[32m[20230113 20:09:25 @agent_ppo2.py:144][0m Total time:      24.88 min
[32m[20230113 20:09:25 @agent_ppo2.py:146][0m 2277376 total steps have happened
[32m[20230113 20:09:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1112 --------------------------#
[32m[20230113 20:09:26 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:09:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:26 @agent_ppo2.py:186][0m |           0.0002 |           5.4506 |           7.3292 |
[32m[20230113 20:09:26 @agent_ppo2.py:186][0m |          -0.0044 |           4.3721 |           7.3150 |
[32m[20230113 20:09:26 @agent_ppo2.py:186][0m |          -0.0066 |           3.9891 |           7.3137 |
[32m[20230113 20:09:26 @agent_ppo2.py:186][0m |          -0.0091 |           3.7784 |           7.3056 |
[32m[20230113 20:09:26 @agent_ppo2.py:186][0m |          -0.0087 |           3.5776 |           7.3076 |
[32m[20230113 20:09:26 @agent_ppo2.py:186][0m |          -0.0108 |           3.4231 |           7.3071 |
[32m[20230113 20:09:26 @agent_ppo2.py:186][0m |          -0.0129 |           3.3075 |           7.3032 |
[32m[20230113 20:09:26 @agent_ppo2.py:186][0m |          -0.0098 |           3.2349 |           7.3049 |
[32m[20230113 20:09:26 @agent_ppo2.py:186][0m |          -0.0100 |           3.1102 |           7.2997 |
[32m[20230113 20:09:26 @agent_ppo2.py:186][0m |          -0.0117 |           3.0321 |           7.3058 |
[32m[20230113 20:09:26 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:09:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 218.82
[32m[20230113 20:09:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.43
[32m[20230113 20:09:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.64
[32m[20230113 20:09:27 @agent_ppo2.py:144][0m Total time:      24.90 min
[32m[20230113 20:09:27 @agent_ppo2.py:146][0m 2279424 total steps have happened
[32m[20230113 20:09:27 @agent_ppo2.py:122][0m #------------------------ Iteration 1113 --------------------------#
[32m[20230113 20:09:27 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:09:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:27 @agent_ppo2.py:186][0m |          -0.0040 |           6.2029 |           7.1886 |
[32m[20230113 20:09:27 @agent_ppo2.py:186][0m |          -0.0031 |           5.1978 |           7.1730 |
[32m[20230113 20:09:27 @agent_ppo2.py:186][0m |           0.0020 |           4.7227 |           7.1568 |
[32m[20230113 20:09:27 @agent_ppo2.py:186][0m |          -0.0074 |           4.2441 |           7.1696 |
[32m[20230113 20:09:27 @agent_ppo2.py:186][0m |          -0.0105 |           3.9660 |           7.1492 |
[32m[20230113 20:09:27 @agent_ppo2.py:186][0m |          -0.0166 |           3.8093 |           7.1462 |
[32m[20230113 20:09:27 @agent_ppo2.py:186][0m |          -0.0100 |           3.6419 |           7.1481 |
[32m[20230113 20:09:27 @agent_ppo2.py:186][0m |          -0.0124 |           3.5127 |           7.1468 |
[32m[20230113 20:09:28 @agent_ppo2.py:186][0m |          -0.0105 |           3.4118 |           7.1377 |
[32m[20230113 20:09:28 @agent_ppo2.py:186][0m |          -0.0135 |           3.4214 |           7.1363 |
[32m[20230113 20:09:28 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:09:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.84
[32m[20230113 20:09:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.24
[32m[20230113 20:09:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.00
[32m[20230113 20:09:28 @agent_ppo2.py:144][0m Total time:      24.92 min
[32m[20230113 20:09:28 @agent_ppo2.py:146][0m 2281472 total steps have happened
[32m[20230113 20:09:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1114 --------------------------#
[32m[20230113 20:09:28 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:09:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:29 @agent_ppo2.py:186][0m |           0.0022 |           6.1955 |           7.3909 |
[32m[20230113 20:09:29 @agent_ppo2.py:186][0m |          -0.0058 |           5.3040 |           7.3777 |
[32m[20230113 20:09:29 @agent_ppo2.py:186][0m |          -0.0067 |           4.9292 |           7.3689 |
[32m[20230113 20:09:29 @agent_ppo2.py:186][0m |          -0.0087 |           4.7148 |           7.3697 |
[32m[20230113 20:09:29 @agent_ppo2.py:186][0m |          -0.0102 |           4.4215 |           7.3645 |
[32m[20230113 20:09:29 @agent_ppo2.py:186][0m |          -0.0119 |           4.2176 |           7.3595 |
[32m[20230113 20:09:29 @agent_ppo2.py:186][0m |          -0.0101 |           4.0984 |           7.3532 |
[32m[20230113 20:09:29 @agent_ppo2.py:186][0m |          -0.0128 |           3.9250 |           7.3595 |
[32m[20230113 20:09:29 @agent_ppo2.py:186][0m |          -0.0127 |           3.8481 |           7.3512 |
[32m[20230113 20:09:29 @agent_ppo2.py:186][0m |          -0.0138 |           3.7780 |           7.3501 |
[32m[20230113 20:09:29 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.41
[32m[20230113 20:09:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.65
[32m[20230113 20:09:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.37
[32m[20230113 20:09:29 @agent_ppo2.py:144][0m Total time:      24.95 min
[32m[20230113 20:09:29 @agent_ppo2.py:146][0m 2283520 total steps have happened
[32m[20230113 20:09:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1115 --------------------------#
[32m[20230113 20:09:30 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:09:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:30 @agent_ppo2.py:186][0m |          -0.0014 |           6.5499 |           7.1507 |
[32m[20230113 20:09:30 @agent_ppo2.py:186][0m |          -0.0041 |           5.2680 |           7.1462 |
[32m[20230113 20:09:30 @agent_ppo2.py:186][0m |          -0.0083 |           4.7065 |           7.1447 |
[32m[20230113 20:09:30 @agent_ppo2.py:186][0m |          -0.0111 |           4.3414 |           7.1470 |
[32m[20230113 20:09:30 @agent_ppo2.py:186][0m |          -0.0098 |           4.2464 |           7.1467 |
[32m[20230113 20:09:30 @agent_ppo2.py:186][0m |          -0.0126 |           3.8603 |           7.1494 |
[32m[20230113 20:09:30 @agent_ppo2.py:186][0m |          -0.0151 |           3.7332 |           7.1482 |
[32m[20230113 20:09:30 @agent_ppo2.py:186][0m |          -0.0116 |           3.6114 |           7.1480 |
[32m[20230113 20:09:30 @agent_ppo2.py:186][0m |          -0.0163 |           3.4676 |           7.1543 |
[32m[20230113 20:09:30 @agent_ppo2.py:186][0m |          -0.0117 |           3.3792 |           7.1484 |
[32m[20230113 20:09:30 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:09:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.27
[32m[20230113 20:09:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.01
[32m[20230113 20:09:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.51
[32m[20230113 20:09:31 @agent_ppo2.py:144][0m Total time:      24.97 min
[32m[20230113 20:09:31 @agent_ppo2.py:146][0m 2285568 total steps have happened
[32m[20230113 20:09:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1116 --------------------------#
[32m[20230113 20:09:31 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:09:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:31 @agent_ppo2.py:186][0m |           0.0031 |           5.6176 |           7.1865 |
[32m[20230113 20:09:31 @agent_ppo2.py:186][0m |          -0.0032 |           4.2501 |           7.1742 |
[32m[20230113 20:09:31 @agent_ppo2.py:186][0m |          -0.0055 |           3.8216 |           7.1751 |
[32m[20230113 20:09:31 @agent_ppo2.py:186][0m |          -0.0085 |           3.6437 |           7.1703 |
[32m[20230113 20:09:31 @agent_ppo2.py:186][0m |          -0.0099 |           3.5463 |           7.1780 |
[32m[20230113 20:09:31 @agent_ppo2.py:186][0m |          -0.0104 |           3.3727 |           7.1679 |
[32m[20230113 20:09:31 @agent_ppo2.py:186][0m |          -0.0106 |           3.3344 |           7.1723 |
[32m[20230113 20:09:31 @agent_ppo2.py:186][0m |          -0.0108 |           3.2936 |           7.1727 |
[32m[20230113 20:09:31 @agent_ppo2.py:186][0m |          -0.0086 |           3.1727 |           7.1683 |
[32m[20230113 20:09:32 @agent_ppo2.py:186][0m |          -0.0137 |           3.0873 |           7.1732 |
[32m[20230113 20:09:32 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:09:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.63
[32m[20230113 20:09:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.08
[32m[20230113 20:09:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.81
[32m[20230113 20:09:32 @agent_ppo2.py:144][0m Total time:      24.99 min
[32m[20230113 20:09:32 @agent_ppo2.py:146][0m 2287616 total steps have happened
[32m[20230113 20:09:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1117 --------------------------#
[32m[20230113 20:09:32 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:09:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:32 @agent_ppo2.py:186][0m |           0.0005 |           6.1463 |           7.1016 |
[32m[20230113 20:09:32 @agent_ppo2.py:186][0m |          -0.0034 |           5.1651 |           7.1039 |
[32m[20230113 20:09:33 @agent_ppo2.py:186][0m |          -0.0065 |           4.7602 |           7.0926 |
[32m[20230113 20:09:33 @agent_ppo2.py:186][0m |          -0.0065 |           4.4416 |           7.0936 |
[32m[20230113 20:09:33 @agent_ppo2.py:186][0m |          -0.0079 |           4.2201 |           7.0929 |
[32m[20230113 20:09:33 @agent_ppo2.py:186][0m |          -0.0104 |           4.0736 |           7.0883 |
[32m[20230113 20:09:33 @agent_ppo2.py:186][0m |          -0.0105 |           3.9255 |           7.0885 |
[32m[20230113 20:09:33 @agent_ppo2.py:186][0m |          -0.0122 |           3.7983 |           7.0861 |
[32m[20230113 20:09:33 @agent_ppo2.py:186][0m |          -0.0123 |           3.6774 |           7.0893 |
[32m[20230113 20:09:33 @agent_ppo2.py:186][0m |          -0.0126 |           3.5767 |           7.0844 |
[32m[20230113 20:09:33 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:09:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.30
[32m[20230113 20:09:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.56
[32m[20230113 20:09:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 137.22
[32m[20230113 20:09:33 @agent_ppo2.py:144][0m Total time:      25.01 min
[32m[20230113 20:09:33 @agent_ppo2.py:146][0m 2289664 total steps have happened
[32m[20230113 20:09:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1118 --------------------------#
[32m[20230113 20:09:34 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:09:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:34 @agent_ppo2.py:186][0m |          -0.0013 |           6.2423 |           7.3265 |
[32m[20230113 20:09:34 @agent_ppo2.py:186][0m |          -0.0054 |           5.3085 |           7.3116 |
[32m[20230113 20:09:34 @agent_ppo2.py:186][0m |          -0.0083 |           4.7566 |           7.3152 |
[32m[20230113 20:09:34 @agent_ppo2.py:186][0m |          -0.0094 |           4.4458 |           7.3150 |
[32m[20230113 20:09:34 @agent_ppo2.py:186][0m |          -0.0107 |           4.2245 |           7.2994 |
[32m[20230113 20:09:34 @agent_ppo2.py:186][0m |          -0.0116 |           4.0952 |           7.3010 |
[32m[20230113 20:09:34 @agent_ppo2.py:186][0m |          -0.0118 |           3.8951 |           7.3014 |
[32m[20230113 20:09:34 @agent_ppo2.py:186][0m |          -0.0129 |           3.8260 |           7.2957 |
[32m[20230113 20:09:34 @agent_ppo2.py:186][0m |          -0.0138 |           3.6844 |           7.2990 |
[32m[20230113 20:09:34 @agent_ppo2.py:186][0m |          -0.0134 |           3.6285 |           7.2964 |
[32m[20230113 20:09:34 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.13
[32m[20230113 20:09:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.38
[32m[20230113 20:09:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.27
[32m[20230113 20:09:35 @agent_ppo2.py:144][0m Total time:      25.03 min
[32m[20230113 20:09:35 @agent_ppo2.py:146][0m 2291712 total steps have happened
[32m[20230113 20:09:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1119 --------------------------#
[32m[20230113 20:09:35 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:09:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:35 @agent_ppo2.py:186][0m |           0.0011 |           6.4472 |           7.2648 |
[32m[20230113 20:09:35 @agent_ppo2.py:186][0m |          -0.0057 |           4.7429 |           7.2418 |
[32m[20230113 20:09:35 @agent_ppo2.py:186][0m |          -0.0035 |           4.1396 |           7.2373 |
[32m[20230113 20:09:35 @agent_ppo2.py:186][0m |          -0.0103 |           3.8495 |           7.2329 |
[32m[20230113 20:09:35 @agent_ppo2.py:186][0m |          -0.0108 |           3.6542 |           7.2372 |
[32m[20230113 20:09:35 @agent_ppo2.py:186][0m |          -0.0154 |           3.4971 |           7.2355 |
[32m[20230113 20:09:35 @agent_ppo2.py:186][0m |          -0.0089 |           3.3828 |           7.2385 |
[32m[20230113 20:09:35 @agent_ppo2.py:186][0m |          -0.0127 |           3.2544 |           7.2380 |
[32m[20230113 20:09:35 @agent_ppo2.py:186][0m |          -0.0145 |           3.1764 |           7.2330 |
[32m[20230113 20:09:35 @agent_ppo2.py:186][0m |          -0.0140 |           3.0731 |           7.2334 |
[32m[20230113 20:09:35 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:09:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.29
[32m[20230113 20:09:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.62
[32m[20230113 20:09:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.86
[32m[20230113 20:09:36 @agent_ppo2.py:144][0m Total time:      25.06 min
[32m[20230113 20:09:36 @agent_ppo2.py:146][0m 2293760 total steps have happened
[32m[20230113 20:09:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1120 --------------------------#
[32m[20230113 20:09:36 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:09:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:36 @agent_ppo2.py:186][0m |           0.0001 |           5.8752 |           7.2471 |
[32m[20230113 20:09:36 @agent_ppo2.py:186][0m |          -0.0039 |           4.7406 |           7.2432 |
[32m[20230113 20:09:36 @agent_ppo2.py:186][0m |          -0.0051 |           4.2395 |           7.2520 |
[32m[20230113 20:09:37 @agent_ppo2.py:186][0m |          -0.0072 |           3.9364 |           7.2435 |
[32m[20230113 20:09:37 @agent_ppo2.py:186][0m |          -0.0076 |           3.7513 |           7.2446 |
[32m[20230113 20:09:37 @agent_ppo2.py:186][0m |          -0.0085 |           3.5272 |           7.2474 |
[32m[20230113 20:09:37 @agent_ppo2.py:186][0m |          -0.0090 |           3.4292 |           7.2482 |
[32m[20230113 20:09:37 @agent_ppo2.py:186][0m |          -0.0096 |           3.2855 |           7.2485 |
[32m[20230113 20:09:37 @agent_ppo2.py:186][0m |          -0.0097 |           3.2063 |           7.2486 |
[32m[20230113 20:09:37 @agent_ppo2.py:186][0m |          -0.0112 |           3.1445 |           7.2452 |
[32m[20230113 20:09:37 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.15
[32m[20230113 20:09:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.88
[32m[20230113 20:09:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.87
[32m[20230113 20:09:37 @agent_ppo2.py:144][0m Total time:      25.08 min
[32m[20230113 20:09:37 @agent_ppo2.py:146][0m 2295808 total steps have happened
[32m[20230113 20:09:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1121 --------------------------#
[32m[20230113 20:09:38 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:09:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:38 @agent_ppo2.py:186][0m |          -0.0022 |           5.7819 |           7.2168 |
[32m[20230113 20:09:38 @agent_ppo2.py:186][0m |          -0.0065 |           5.4749 |           7.2008 |
[32m[20230113 20:09:38 @agent_ppo2.py:186][0m |          -0.0096 |           4.8905 |           7.1924 |
[32m[20230113 20:09:38 @agent_ppo2.py:186][0m |          -0.0050 |           4.8337 |           7.1917 |
[32m[20230113 20:09:38 @agent_ppo2.py:186][0m |          -0.0074 |           4.7317 |           7.1870 |
[32m[20230113 20:09:38 @agent_ppo2.py:186][0m |          -0.0161 |           4.3447 |           7.1848 |
[32m[20230113 20:09:38 @agent_ppo2.py:186][0m |          -0.0140 |           4.1989 |           7.1846 |
[32m[20230113 20:09:38 @agent_ppo2.py:186][0m |          -0.0158 |           4.1402 |           7.1820 |
[32m[20230113 20:09:38 @agent_ppo2.py:186][0m |          -0.0148 |           4.0902 |           7.1854 |
[32m[20230113 20:09:38 @agent_ppo2.py:186][0m |          -0.0199 |           4.0302 |           7.1843 |
[32m[20230113 20:09:38 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:09:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.62
[32m[20230113 20:09:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.95
[32m[20230113 20:09:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.73
[32m[20230113 20:09:38 @agent_ppo2.py:144][0m Total time:      25.10 min
[32m[20230113 20:09:38 @agent_ppo2.py:146][0m 2297856 total steps have happened
[32m[20230113 20:09:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1122 --------------------------#
[32m[20230113 20:09:39 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:09:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:39 @agent_ppo2.py:186][0m |           0.0011 |           5.8419 |           7.1945 |
[32m[20230113 20:09:39 @agent_ppo2.py:186][0m |          -0.0064 |           4.1846 |           7.1890 |
[32m[20230113 20:09:39 @agent_ppo2.py:186][0m |          -0.0078 |           3.6103 |           7.1843 |
[32m[20230113 20:09:39 @agent_ppo2.py:186][0m |          -0.0098 |           3.3147 |           7.1832 |
[32m[20230113 20:09:39 @agent_ppo2.py:186][0m |          -0.0108 |           3.0689 |           7.1747 |
[32m[20230113 20:09:39 @agent_ppo2.py:186][0m |          -0.0113 |           2.8780 |           7.1758 |
[32m[20230113 20:09:39 @agent_ppo2.py:186][0m |          -0.0129 |           2.7499 |           7.1711 |
[32m[20230113 20:09:39 @agent_ppo2.py:186][0m |          -0.0139 |           2.6350 |           7.1734 |
[32m[20230113 20:09:39 @agent_ppo2.py:186][0m |          -0.0149 |           2.5560 |           7.1735 |
[32m[20230113 20:09:39 @agent_ppo2.py:186][0m |          -0.0152 |           2.4550 |           7.1698 |
[32m[20230113 20:09:39 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:09:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.20
[32m[20230113 20:09:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.72
[32m[20230113 20:09:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.62
[32m[20230113 20:09:40 @agent_ppo2.py:144][0m Total time:      25.12 min
[32m[20230113 20:09:40 @agent_ppo2.py:146][0m 2299904 total steps have happened
[32m[20230113 20:09:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1123 --------------------------#
[32m[20230113 20:09:40 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:09:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:40 @agent_ppo2.py:186][0m |          -0.0015 |          14.0204 |           7.1415 |
[32m[20230113 20:09:40 @agent_ppo2.py:186][0m |          -0.0160 |           8.0086 |           7.1244 |
[32m[20230113 20:09:40 @agent_ppo2.py:186][0m |          -0.0135 |           6.1803 |           7.1145 |
[32m[20230113 20:09:40 @agent_ppo2.py:186][0m |          -0.0137 |           5.1604 |           7.1203 |
[32m[20230113 20:09:40 @agent_ppo2.py:186][0m |          -0.0171 |           4.5983 |           7.1154 |
[32m[20230113 20:09:40 @agent_ppo2.py:186][0m |          -0.0128 |           4.0889 |           7.1193 |
[32m[20230113 20:09:40 @agent_ppo2.py:186][0m |          -0.0149 |           3.8340 |           7.1144 |
[32m[20230113 20:09:40 @agent_ppo2.py:186][0m |          -0.0205 |           3.5458 |           7.1146 |
[32m[20230113 20:09:40 @agent_ppo2.py:186][0m |          -0.0221 |           3.3954 |           7.1156 |
[32m[20230113 20:09:41 @agent_ppo2.py:186][0m |          -0.0224 |           3.2464 |           7.1184 |
[32m[20230113 20:09:41 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 20:09:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 135.00
[32m[20230113 20:09:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.99
[32m[20230113 20:09:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.01
[32m[20230113 20:09:41 @agent_ppo2.py:144][0m Total time:      25.14 min
[32m[20230113 20:09:41 @agent_ppo2.py:146][0m 2301952 total steps have happened
[32m[20230113 20:09:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1124 --------------------------#
[32m[20230113 20:09:41 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:09:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:42 @agent_ppo2.py:186][0m |          -0.0015 |           7.0955 |           7.2133 |
[32m[20230113 20:09:42 @agent_ppo2.py:186][0m |          -0.0081 |           5.2405 |           7.2136 |
[32m[20230113 20:09:42 @agent_ppo2.py:186][0m |          -0.0143 |           4.6061 |           7.2085 |
[32m[20230113 20:09:42 @agent_ppo2.py:186][0m |          -0.0090 |           4.2405 |           7.1971 |
[32m[20230113 20:09:42 @agent_ppo2.py:186][0m |          -0.0110 |           3.9948 |           7.2009 |
[32m[20230113 20:09:42 @agent_ppo2.py:186][0m |          -0.0138 |           3.8197 |           7.1988 |
[32m[20230113 20:09:42 @agent_ppo2.py:186][0m |          -0.0176 |           3.6585 |           7.1955 |
[32m[20230113 20:09:42 @agent_ppo2.py:186][0m |          -0.0134 |           3.5491 |           7.1937 |
[32m[20230113 20:09:42 @agent_ppo2.py:186][0m |          -0.0174 |           3.4180 |           7.1933 |
[32m[20230113 20:09:42 @agent_ppo2.py:186][0m |          -0.0184 |           3.3550 |           7.1915 |
[32m[20230113 20:09:42 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:09:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.85
[32m[20230113 20:09:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 224.39
[32m[20230113 20:09:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 171.46
[32m[20230113 20:09:42 @agent_ppo2.py:144][0m Total time:      25.16 min
[32m[20230113 20:09:42 @agent_ppo2.py:146][0m 2304000 total steps have happened
[32m[20230113 20:09:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1125 --------------------------#
[32m[20230113 20:09:43 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:09:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:43 @agent_ppo2.py:186][0m |           0.0007 |           7.3720 |           7.2434 |
[32m[20230113 20:09:43 @agent_ppo2.py:186][0m |          -0.0048 |           5.7442 |           7.2325 |
[32m[20230113 20:09:43 @agent_ppo2.py:186][0m |          -0.0091 |           5.1401 |           7.2262 |
[32m[20230113 20:09:43 @agent_ppo2.py:186][0m |          -0.0058 |           4.8162 |           7.2348 |
[32m[20230113 20:09:43 @agent_ppo2.py:186][0m |          -0.0098 |           4.6500 |           7.2297 |
[32m[20230113 20:09:43 @agent_ppo2.py:186][0m |          -0.0098 |           4.4270 |           7.2276 |
[32m[20230113 20:09:43 @agent_ppo2.py:186][0m |          -0.0129 |           4.2589 |           7.2339 |
[32m[20230113 20:09:43 @agent_ppo2.py:186][0m |          -0.0143 |           4.1177 |           7.2272 |
[32m[20230113 20:09:43 @agent_ppo2.py:186][0m |          -0.0166 |           4.0133 |           7.2265 |
[32m[20230113 20:09:43 @agent_ppo2.py:186][0m |          -0.0141 |           3.9442 |           7.2265 |
[32m[20230113 20:09:43 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:09:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.34
[32m[20230113 20:09:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.44
[32m[20230113 20:09:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.93
[32m[20230113 20:09:44 @agent_ppo2.py:144][0m Total time:      25.18 min
[32m[20230113 20:09:44 @agent_ppo2.py:146][0m 2306048 total steps have happened
[32m[20230113 20:09:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1126 --------------------------#
[32m[20230113 20:09:44 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:09:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:44 @agent_ppo2.py:186][0m |           0.0008 |           9.1807 |           7.2911 |
[32m[20230113 20:09:44 @agent_ppo2.py:186][0m |          -0.0060 |           5.1176 |           7.2812 |
[32m[20230113 20:09:44 @agent_ppo2.py:186][0m |          -0.0073 |           4.3184 |           7.2695 |
[32m[20230113 20:09:44 @agent_ppo2.py:186][0m |          -0.0094 |           3.8581 |           7.2633 |
[32m[20230113 20:09:44 @agent_ppo2.py:186][0m |          -0.0111 |           3.5203 |           7.2556 |
[32m[20230113 20:09:44 @agent_ppo2.py:186][0m |          -0.0117 |           3.2628 |           7.2507 |
[32m[20230113 20:09:44 @agent_ppo2.py:186][0m |          -0.0131 |           3.0071 |           7.2479 |
[32m[20230113 20:09:44 @agent_ppo2.py:186][0m |          -0.0139 |           2.8609 |           7.2456 |
[32m[20230113 20:09:44 @agent_ppo2.py:186][0m |          -0.0149 |           2.7916 |           7.2398 |
[32m[20230113 20:09:44 @agent_ppo2.py:186][0m |          -0.0154 |           2.6531 |           7.2411 |
[32m[20230113 20:09:44 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:09:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 165.16
[32m[20230113 20:09:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.05
[32m[20230113 20:09:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.40
[32m[20230113 20:09:45 @agent_ppo2.py:144][0m Total time:      25.20 min
[32m[20230113 20:09:45 @agent_ppo2.py:146][0m 2308096 total steps have happened
[32m[20230113 20:09:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1127 --------------------------#
[32m[20230113 20:09:45 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:09:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:45 @agent_ppo2.py:186][0m |           0.0009 |           5.2800 |           7.3782 |
[32m[20230113 20:09:45 @agent_ppo2.py:186][0m |          -0.0049 |           4.4547 |           7.3714 |
[32m[20230113 20:09:45 @agent_ppo2.py:186][0m |          -0.0059 |           4.0943 |           7.3607 |
[32m[20230113 20:09:46 @agent_ppo2.py:186][0m |          -0.0078 |           3.8552 |           7.3545 |
[32m[20230113 20:09:46 @agent_ppo2.py:186][0m |          -0.0098 |           3.7182 |           7.3571 |
[32m[20230113 20:09:46 @agent_ppo2.py:186][0m |          -0.0106 |           3.5915 |           7.3527 |
[32m[20230113 20:09:46 @agent_ppo2.py:186][0m |          -0.0111 |           3.5071 |           7.3444 |
[32m[20230113 20:09:46 @agent_ppo2.py:186][0m |          -0.0114 |           3.3956 |           7.3403 |
[32m[20230113 20:09:46 @agent_ppo2.py:186][0m |          -0.0128 |           3.3129 |           7.3410 |
[32m[20230113 20:09:46 @agent_ppo2.py:186][0m |          -0.0137 |           3.2608 |           7.3410 |
[32m[20230113 20:09:46 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:09:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.46
[32m[20230113 20:09:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.78
[32m[20230113 20:09:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.46
[32m[20230113 20:09:46 @agent_ppo2.py:144][0m Total time:      25.23 min
[32m[20230113 20:09:46 @agent_ppo2.py:146][0m 2310144 total steps have happened
[32m[20230113 20:09:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1128 --------------------------#
[32m[20230113 20:09:47 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:09:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:47 @agent_ppo2.py:186][0m |          -0.0098 |           6.6049 |           7.0425 |
[32m[20230113 20:09:47 @agent_ppo2.py:186][0m |          -0.0086 |           4.7709 |           7.0305 |
[32m[20230113 20:09:47 @agent_ppo2.py:186][0m |           0.0173 |           4.1990 |           7.0313 |
[32m[20230113 20:09:47 @agent_ppo2.py:186][0m |          -0.0138 |           3.8287 |           7.0272 |
[32m[20230113 20:09:47 @agent_ppo2.py:186][0m |          -0.0096 |           3.7012 |           7.0161 |
[32m[20230113 20:09:47 @agent_ppo2.py:186][0m |          -0.0360 |           3.6388 |           7.0317 |
[32m[20230113 20:09:47 @agent_ppo2.py:186][0m |          -0.0224 |           3.5855 |           6.9968 |
[32m[20230113 20:09:47 @agent_ppo2.py:186][0m |          -0.0095 |           3.2904 |           7.0153 |
[32m[20230113 20:09:47 @agent_ppo2.py:186][0m |          -0.0064 |           3.2647 |           7.0150 |
[32m[20230113 20:09:47 @agent_ppo2.py:186][0m |          -0.0128 |           3.1343 |           7.0182 |
[32m[20230113 20:09:47 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:09:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 221.13
[32m[20230113 20:09:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.21
[32m[20230113 20:09:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.22
[32m[20230113 20:09:48 @agent_ppo2.py:144][0m Total time:      25.25 min
[32m[20230113 20:09:48 @agent_ppo2.py:146][0m 2312192 total steps have happened
[32m[20230113 20:09:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1129 --------------------------#
[32m[20230113 20:09:48 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:09:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:48 @agent_ppo2.py:186][0m |          -0.0036 |           5.6485 |           7.2863 |
[32m[20230113 20:09:48 @agent_ppo2.py:186][0m |          -0.0078 |           4.7873 |           7.2718 |
[32m[20230113 20:09:48 @agent_ppo2.py:186][0m |          -0.0095 |           4.4336 |           7.2676 |
[32m[20230113 20:09:48 @agent_ppo2.py:186][0m |          -0.0121 |           4.1390 |           7.2628 |
[32m[20230113 20:09:48 @agent_ppo2.py:186][0m |          -0.0131 |           3.9364 |           7.2612 |
[32m[20230113 20:09:48 @agent_ppo2.py:186][0m |          -0.0136 |           3.8305 |           7.2555 |
[32m[20230113 20:09:48 @agent_ppo2.py:186][0m |          -0.0137 |           3.7202 |           7.2551 |
[32m[20230113 20:09:48 @agent_ppo2.py:186][0m |          -0.0155 |           3.6224 |           7.2526 |
[32m[20230113 20:09:48 @agent_ppo2.py:186][0m |          -0.0158 |           3.5713 |           7.2549 |
[32m[20230113 20:09:48 @agent_ppo2.py:186][0m |          -0.0169 |           3.4624 |           7.2529 |
[32m[20230113 20:09:48 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:09:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.02
[32m[20230113 20:09:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.17
[32m[20230113 20:09:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.39
[32m[20230113 20:09:49 @agent_ppo2.py:144][0m Total time:      25.27 min
[32m[20230113 20:09:49 @agent_ppo2.py:146][0m 2314240 total steps have happened
[32m[20230113 20:09:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1130 --------------------------#
[32m[20230113 20:09:49 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:09:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:49 @agent_ppo2.py:186][0m |           0.0022 |           6.1370 |           6.9462 |
[32m[20230113 20:09:49 @agent_ppo2.py:186][0m |          -0.0089 |           4.6213 |           6.9385 |
[32m[20230113 20:09:50 @agent_ppo2.py:186][0m |          -0.0094 |           4.0414 |           6.9232 |
[32m[20230113 20:09:50 @agent_ppo2.py:186][0m |          -0.0101 |           3.6644 |           6.9245 |
[32m[20230113 20:09:50 @agent_ppo2.py:186][0m |          -0.0098 |           3.3508 |           6.9152 |
[32m[20230113 20:09:50 @agent_ppo2.py:186][0m |          -0.0184 |           3.1248 |           6.9212 |
[32m[20230113 20:09:50 @agent_ppo2.py:186][0m |          -0.0099 |           2.9220 |           6.9237 |
[32m[20230113 20:09:50 @agent_ppo2.py:186][0m |          -0.0184 |           2.7685 |           6.9147 |
[32m[20230113 20:09:50 @agent_ppo2.py:186][0m |          -0.0185 |           2.6390 |           6.9132 |
[32m[20230113 20:09:50 @agent_ppo2.py:186][0m |          -0.0131 |           2.4750 |           6.9166 |
[32m[20230113 20:09:50 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:09:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.63
[32m[20230113 20:09:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.87
[32m[20230113 20:09:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.12
[32m[20230113 20:09:50 @agent_ppo2.py:144][0m Total time:      25.29 min
[32m[20230113 20:09:50 @agent_ppo2.py:146][0m 2316288 total steps have happened
[32m[20230113 20:09:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1131 --------------------------#
[32m[20230113 20:09:51 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 20:09:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:51 @agent_ppo2.py:186][0m |           0.0110 |          18.9289 |           7.0094 |
[32m[20230113 20:09:51 @agent_ppo2.py:186][0m |          -0.0158 |           6.3396 |           7.0171 |
[32m[20230113 20:09:51 @agent_ppo2.py:186][0m |          -0.0119 |           4.9300 |           7.0129 |
[32m[20230113 20:09:51 @agent_ppo2.py:186][0m |          -0.0184 |           4.2369 |           7.0115 |
[32m[20230113 20:09:51 @agent_ppo2.py:186][0m |          -0.0204 |           3.8109 |           7.0068 |
[32m[20230113 20:09:51 @agent_ppo2.py:186][0m |           0.0085 |           3.5094 |           6.9993 |
[32m[20230113 20:09:51 @agent_ppo2.py:186][0m |          -0.0118 |           3.3290 |           6.9872 |
[32m[20230113 20:09:51 @agent_ppo2.py:186][0m |          -0.0211 |           3.1351 |           6.9931 |
[32m[20230113 20:09:51 @agent_ppo2.py:186][0m |           0.0022 |           3.0322 |           6.9919 |
[32m[20230113 20:09:51 @agent_ppo2.py:186][0m |          -0.0239 |           3.0442 |           6.9916 |
[32m[20230113 20:09:51 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 20:09:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 114.55
[32m[20230113 20:09:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.71
[32m[20230113 20:09:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.69
[32m[20230113 20:09:51 @agent_ppo2.py:144][0m Total time:      25.31 min
[32m[20230113 20:09:51 @agent_ppo2.py:146][0m 2318336 total steps have happened
[32m[20230113 20:09:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1132 --------------------------#
[32m[20230113 20:09:52 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:09:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:52 @agent_ppo2.py:186][0m |           0.0011 |           8.3677 |           7.1236 |
[32m[20230113 20:09:52 @agent_ppo2.py:186][0m |          -0.0042 |           6.4880 |           7.1229 |
[32m[20230113 20:09:52 @agent_ppo2.py:186][0m |          -0.0073 |           5.8055 |           7.1164 |
[32m[20230113 20:09:52 @agent_ppo2.py:186][0m |          -0.0094 |           5.3582 |           7.1112 |
[32m[20230113 20:09:52 @agent_ppo2.py:186][0m |          -0.0111 |           5.1077 |           7.1124 |
[32m[20230113 20:09:52 @agent_ppo2.py:186][0m |          -0.0118 |           4.9128 |           7.1116 |
[32m[20230113 20:09:52 @agent_ppo2.py:186][0m |          -0.0122 |           4.7348 |           7.1114 |
[32m[20230113 20:09:52 @agent_ppo2.py:186][0m |          -0.0139 |           4.5858 |           7.1077 |
[32m[20230113 20:09:52 @agent_ppo2.py:186][0m |          -0.0127 |           4.4627 |           7.1068 |
[32m[20230113 20:09:52 @agent_ppo2.py:186][0m |          -0.0143 |           4.3826 |           7.1056 |
[32m[20230113 20:09:52 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.74
[32m[20230113 20:09:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.40
[32m[20230113 20:09:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.20
[32m[20230113 20:09:53 @agent_ppo2.py:144][0m Total time:      25.34 min
[32m[20230113 20:09:53 @agent_ppo2.py:146][0m 2320384 total steps have happened
[32m[20230113 20:09:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1133 --------------------------#
[32m[20230113 20:09:53 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:09:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:53 @agent_ppo2.py:186][0m |          -0.0117 |           6.0031 |           6.9630 |
[32m[20230113 20:09:53 @agent_ppo2.py:186][0m |           0.0021 |           4.9888 |           6.9531 |
[32m[20230113 20:09:53 @agent_ppo2.py:186][0m |          -0.0048 |           4.5778 |           6.9532 |
[32m[20230113 20:09:53 @agent_ppo2.py:186][0m |          -0.0186 |           4.2528 |           6.9524 |
[32m[20230113 20:09:53 @agent_ppo2.py:186][0m |          -0.0154 |           4.0182 |           6.9559 |
[32m[20230113 20:09:53 @agent_ppo2.py:186][0m |          -0.0139 |           3.8618 |           6.9579 |
[32m[20230113 20:09:53 @agent_ppo2.py:186][0m |          -0.0185 |           3.6945 |           6.9575 |
[32m[20230113 20:09:54 @agent_ppo2.py:186][0m |          -0.0027 |           3.5496 |           6.9552 |
[32m[20230113 20:09:54 @agent_ppo2.py:186][0m |          -0.0117 |           3.4494 |           6.9568 |
[32m[20230113 20:09:54 @agent_ppo2.py:186][0m |          -0.0026 |           3.3873 |           6.9572 |
[32m[20230113 20:09:54 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:09:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.21
[32m[20230113 20:09:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.37
[32m[20230113 20:09:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.69
[32m[20230113 20:09:54 @agent_ppo2.py:144][0m Total time:      25.36 min
[32m[20230113 20:09:54 @agent_ppo2.py:146][0m 2322432 total steps have happened
[32m[20230113 20:09:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1134 --------------------------#
[32m[20230113 20:09:54 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:09:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:55 @agent_ppo2.py:186][0m |           0.0142 |           5.9724 |           7.0553 |
[32m[20230113 20:09:55 @agent_ppo2.py:186][0m |           0.0270 |           4.3643 |           7.0434 |
[32m[20230113 20:09:55 @agent_ppo2.py:186][0m |           0.0066 |           3.8754 |           7.0179 |
[32m[20230113 20:09:55 @agent_ppo2.py:186][0m |           0.0080 |           3.4237 |           7.0307 |
[32m[20230113 20:09:55 @agent_ppo2.py:186][0m |          -0.0050 |           3.2029 |           7.0287 |
[32m[20230113 20:09:55 @agent_ppo2.py:186][0m |          -0.0140 |           3.0157 |           7.0260 |
[32m[20230113 20:09:55 @agent_ppo2.py:186][0m |          -0.0079 |           2.8608 |           7.0303 |
[32m[20230113 20:09:55 @agent_ppo2.py:186][0m |          -0.0113 |           2.7507 |           7.0316 |
[32m[20230113 20:09:55 @agent_ppo2.py:186][0m |          -0.0079 |           2.6369 |           7.0306 |
[32m[20230113 20:09:55 @agent_ppo2.py:186][0m |          -0.0092 |           2.5378 |           7.0301 |
[32m[20230113 20:09:55 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:09:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.72
[32m[20230113 20:09:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.08
[32m[20230113 20:09:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.68
[32m[20230113 20:09:55 @agent_ppo2.py:144][0m Total time:      25.38 min
[32m[20230113 20:09:55 @agent_ppo2.py:146][0m 2324480 total steps have happened
[32m[20230113 20:09:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1135 --------------------------#
[32m[20230113 20:09:56 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:09:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:56 @agent_ppo2.py:186][0m |           0.0028 |           6.7086 |           7.0283 |
[32m[20230113 20:09:56 @agent_ppo2.py:186][0m |          -0.0083 |           5.2391 |           7.0193 |
[32m[20230113 20:09:56 @agent_ppo2.py:186][0m |          -0.0031 |           4.7341 |           7.0180 |
[32m[20230113 20:09:56 @agent_ppo2.py:186][0m |          -0.0075 |           4.4707 |           7.0130 |
[32m[20230113 20:09:56 @agent_ppo2.py:186][0m |          -0.0181 |           4.2223 |           7.0138 |
[32m[20230113 20:09:56 @agent_ppo2.py:186][0m |          -0.0044 |           4.1235 |           7.0105 |
[32m[20230113 20:09:56 @agent_ppo2.py:186][0m |          -0.0160 |           3.9878 |           7.0085 |
[32m[20230113 20:09:56 @agent_ppo2.py:186][0m |          -0.0162 |           3.8551 |           7.0096 |
[32m[20230113 20:09:56 @agent_ppo2.py:186][0m |          -0.0153 |           3.7740 |           7.0057 |
[32m[20230113 20:09:56 @agent_ppo2.py:186][0m |          -0.0190 |           3.7124 |           7.0082 |
[32m[20230113 20:09:56 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.39
[32m[20230113 20:09:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.06
[32m[20230113 20:09:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.95
[32m[20230113 20:09:57 @agent_ppo2.py:144][0m Total time:      25.40 min
[32m[20230113 20:09:57 @agent_ppo2.py:146][0m 2326528 total steps have happened
[32m[20230113 20:09:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1136 --------------------------#
[32m[20230113 20:09:57 @agent_ppo2.py:128][0m Sampling time: 0.35 s by 1 slaves
[32m[20230113 20:09:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:57 @agent_ppo2.py:186][0m |           0.0034 |          26.9773 |           7.2428 |
[32m[20230113 20:09:57 @agent_ppo2.py:186][0m |          -0.0041 |           9.9592 |           7.2321 |
[32m[20230113 20:09:57 @agent_ppo2.py:186][0m |          -0.0074 |           7.8680 |           7.2339 |
[32m[20230113 20:09:57 @agent_ppo2.py:186][0m |          -0.0099 |           6.8397 |           7.2324 |
[32m[20230113 20:09:57 @agent_ppo2.py:186][0m |          -0.0106 |           6.2877 |           7.2330 |
[32m[20230113 20:09:57 @agent_ppo2.py:186][0m |          -0.0123 |           5.7003 |           7.2331 |
[32m[20230113 20:09:57 @agent_ppo2.py:186][0m |          -0.0127 |           5.0520 |           7.2370 |
[32m[20230113 20:09:57 @agent_ppo2.py:186][0m |          -0.0136 |           4.8838 |           7.2346 |
[32m[20230113 20:09:57 @agent_ppo2.py:186][0m |          -0.0139 |           4.7645 |           7.2361 |
[32m[20230113 20:09:57 @agent_ppo2.py:186][0m |          -0.0150 |           4.3171 |           7.2392 |
[32m[20230113 20:09:57 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:09:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 36.29
[32m[20230113 20:09:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.02
[32m[20230113 20:09:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.62
[32m[20230113 20:09:58 @agent_ppo2.py:144][0m Total time:      25.42 min
[32m[20230113 20:09:58 @agent_ppo2.py:146][0m 2328576 total steps have happened
[32m[20230113 20:09:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1137 --------------------------#
[32m[20230113 20:09:58 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:09:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:09:58 @agent_ppo2.py:186][0m |          -0.0024 |           7.5657 |           7.1526 |
[32m[20230113 20:09:58 @agent_ppo2.py:186][0m |          -0.0089 |           6.0351 |           7.1428 |
[32m[20230113 20:09:58 @agent_ppo2.py:186][0m |          -0.0061 |           5.3304 |           7.1333 |
[32m[20230113 20:09:58 @agent_ppo2.py:186][0m |          -0.0083 |           4.9689 |           7.1273 |
[32m[20230113 20:09:58 @agent_ppo2.py:186][0m |           0.0271 |           5.3538 |           7.1293 |
[32m[20230113 20:09:59 @agent_ppo2.py:186][0m |          -0.0189 |           4.7951 |           7.1200 |
[32m[20230113 20:09:59 @agent_ppo2.py:186][0m |          -0.0267 |           4.3092 |           7.1143 |
[32m[20230113 20:09:59 @agent_ppo2.py:186][0m |          -0.0234 |           4.1613 |           7.1171 |
[32m[20230113 20:09:59 @agent_ppo2.py:186][0m |          -0.0118 |           3.9981 |           7.1193 |
[32m[20230113 20:09:59 @agent_ppo2.py:186][0m |          -0.0059 |           3.9282 |           7.1167 |
[32m[20230113 20:09:59 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:09:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.15
[32m[20230113 20:09:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.05
[32m[20230113 20:09:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.28
[32m[20230113 20:09:59 @agent_ppo2.py:144][0m Total time:      25.44 min
[32m[20230113 20:09:59 @agent_ppo2.py:146][0m 2330624 total steps have happened
[32m[20230113 20:09:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1138 --------------------------#
[32m[20230113 20:10:00 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:10:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:00 @agent_ppo2.py:186][0m |          -0.0007 |           5.3294 |           7.0639 |
[32m[20230113 20:10:00 @agent_ppo2.py:186][0m |          -0.0064 |           4.3021 |           7.0587 |
[32m[20230113 20:10:00 @agent_ppo2.py:186][0m |          -0.0091 |           3.8957 |           7.0506 |
[32m[20230113 20:10:00 @agent_ppo2.py:186][0m |          -0.0116 |           3.6871 |           7.0509 |
[32m[20230113 20:10:00 @agent_ppo2.py:186][0m |          -0.0123 |           3.5253 |           7.0492 |
[32m[20230113 20:10:00 @agent_ppo2.py:186][0m |          -0.0135 |           3.3777 |           7.0535 |
[32m[20230113 20:10:00 @agent_ppo2.py:186][0m |          -0.0138 |           3.2773 |           7.0524 |
[32m[20230113 20:10:00 @agent_ppo2.py:186][0m |          -0.0153 |           3.1690 |           7.0462 |
[32m[20230113 20:10:00 @agent_ppo2.py:186][0m |          -0.0139 |           3.0852 |           7.0516 |
[32m[20230113 20:10:00 @agent_ppo2.py:186][0m |          -0.0156 |           3.0412 |           7.0509 |
[32m[20230113 20:10:00 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:10:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.39
[32m[20230113 20:10:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.42
[32m[20230113 20:10:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.20
[32m[20230113 20:10:00 @agent_ppo2.py:144][0m Total time:      25.46 min
[32m[20230113 20:10:00 @agent_ppo2.py:146][0m 2332672 total steps have happened
[32m[20230113 20:10:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1139 --------------------------#
[32m[20230113 20:10:01 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 20:10:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:01 @agent_ppo2.py:186][0m |           0.0064 |          11.1571 |           7.0515 |
[32m[20230113 20:10:01 @agent_ppo2.py:186][0m |          -0.0006 |           7.6887 |           7.0369 |
[32m[20230113 20:10:01 @agent_ppo2.py:186][0m |          -0.0076 |           6.1753 |           7.0340 |
[32m[20230113 20:10:01 @agent_ppo2.py:186][0m |          -0.0075 |           5.3973 |           7.0436 |
[32m[20230113 20:10:01 @agent_ppo2.py:186][0m |          -0.0102 |           4.8609 |           7.0384 |
[32m[20230113 20:10:01 @agent_ppo2.py:186][0m |          -0.0005 |           4.8184 |           7.0454 |
[32m[20230113 20:10:01 @agent_ppo2.py:186][0m |          -0.0141 |           4.3494 |           7.0360 |
[32m[20230113 20:10:01 @agent_ppo2.py:186][0m |          -0.0120 |           4.1484 |           7.0397 |
[32m[20230113 20:10:01 @agent_ppo2.py:186][0m |          -0.0120 |           3.9243 |           7.0349 |
[32m[20230113 20:10:01 @agent_ppo2.py:186][0m |          -0.0158 |           3.7931 |           7.0392 |
[32m[20230113 20:10:01 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 20:10:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 125.45
[32m[20230113 20:10:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.01
[32m[20230113 20:10:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.42
[32m[20230113 20:10:02 @agent_ppo2.py:144][0m Total time:      25.49 min
[32m[20230113 20:10:02 @agent_ppo2.py:146][0m 2334720 total steps have happened
[32m[20230113 20:10:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1140 --------------------------#
[32m[20230113 20:10:02 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:10:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:02 @agent_ppo2.py:186][0m |           0.0015 |           7.2135 |           7.3171 |
[32m[20230113 20:10:02 @agent_ppo2.py:186][0m |          -0.0057 |           5.3728 |           7.3141 |
[32m[20230113 20:10:03 @agent_ppo2.py:186][0m |          -0.0057 |           4.6849 |           7.3078 |
[32m[20230113 20:10:03 @agent_ppo2.py:186][0m |          -0.0081 |           4.2538 |           7.3045 |
[32m[20230113 20:10:03 @agent_ppo2.py:186][0m |          -0.0097 |           3.9017 |           7.3022 |
[32m[20230113 20:10:03 @agent_ppo2.py:186][0m |          -0.0111 |           3.6742 |           7.2968 |
[32m[20230113 20:10:03 @agent_ppo2.py:186][0m |          -0.0123 |           3.4911 |           7.2906 |
[32m[20230113 20:10:03 @agent_ppo2.py:186][0m |          -0.0074 |           3.3509 |           7.2900 |
[32m[20230113 20:10:03 @agent_ppo2.py:186][0m |          -0.0122 |           3.2026 |           7.2852 |
[32m[20230113 20:10:03 @agent_ppo2.py:186][0m |          -0.0089 |           3.1638 |           7.2899 |
[32m[20230113 20:10:03 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:10:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.44
[32m[20230113 20:10:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.31
[32m[20230113 20:10:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.33
[32m[20230113 20:10:03 @agent_ppo2.py:144][0m Total time:      25.51 min
[32m[20230113 20:10:03 @agent_ppo2.py:146][0m 2336768 total steps have happened
[32m[20230113 20:10:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1141 --------------------------#
[32m[20230113 20:10:04 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:10:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:04 @agent_ppo2.py:186][0m |          -0.0002 |           7.0234 |           7.1637 |
[32m[20230113 20:10:04 @agent_ppo2.py:186][0m |          -0.0030 |           5.8139 |           7.1534 |
[32m[20230113 20:10:04 @agent_ppo2.py:186][0m |          -0.0081 |           5.1361 |           7.1508 |
[32m[20230113 20:10:04 @agent_ppo2.py:186][0m |          -0.0083 |           4.7735 |           7.1493 |
[32m[20230113 20:10:04 @agent_ppo2.py:186][0m |          -0.0089 |           4.5535 |           7.1450 |
[32m[20230113 20:10:04 @agent_ppo2.py:186][0m |          -0.0085 |           4.2761 |           7.1498 |
[32m[20230113 20:10:04 @agent_ppo2.py:186][0m |          -0.0093 |           4.0787 |           7.1532 |
[32m[20230113 20:10:04 @agent_ppo2.py:186][0m |          -0.0114 |           3.9684 |           7.1488 |
[32m[20230113 20:10:04 @agent_ppo2.py:186][0m |          -0.0158 |           3.8312 |           7.1405 |
[32m[20230113 20:10:04 @agent_ppo2.py:186][0m |          -0.0055 |           3.7615 |           7.1487 |
[32m[20230113 20:10:04 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:10:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.51
[32m[20230113 20:10:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.81
[32m[20230113 20:10:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.08
[32m[20230113 20:10:05 @agent_ppo2.py:144][0m Total time:      25.53 min
[32m[20230113 20:10:05 @agent_ppo2.py:146][0m 2338816 total steps have happened
[32m[20230113 20:10:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1142 --------------------------#
[32m[20230113 20:10:05 @agent_ppo2.py:128][0m Sampling time: 0.35 s by 1 slaves
[32m[20230113 20:10:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:05 @agent_ppo2.py:186][0m |          -0.0015 |          19.4588 |           7.2915 |
[32m[20230113 20:10:05 @agent_ppo2.py:186][0m |          -0.0083 |           8.1301 |           7.2771 |
[32m[20230113 20:10:05 @agent_ppo2.py:186][0m |          -0.0103 |           5.6252 |           7.2663 |
[32m[20230113 20:10:05 @agent_ppo2.py:186][0m |          -0.0124 |           4.5592 |           7.2624 |
[32m[20230113 20:10:05 @agent_ppo2.py:186][0m |          -0.0127 |           4.1173 |           7.2634 |
[32m[20230113 20:10:05 @agent_ppo2.py:186][0m |          -0.0140 |           3.6674 |           7.2610 |
[32m[20230113 20:10:05 @agent_ppo2.py:186][0m |          -0.0146 |           3.3956 |           7.2595 |
[32m[20230113 20:10:05 @agent_ppo2.py:186][0m |          -0.0154 |           3.1823 |           7.2601 |
[32m[20230113 20:10:05 @agent_ppo2.py:186][0m |          -0.0162 |           3.0104 |           7.2618 |
[32m[20230113 20:10:05 @agent_ppo2.py:186][0m |          -0.0161 |           2.8839 |           7.2558 |
[32m[20230113 20:10:05 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 20:10:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 137.41
[32m[20230113 20:10:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.26
[32m[20230113 20:10:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.94
[32m[20230113 20:10:06 @agent_ppo2.py:144][0m Total time:      25.55 min
[32m[20230113 20:10:06 @agent_ppo2.py:146][0m 2340864 total steps have happened
[32m[20230113 20:10:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1143 --------------------------#
[32m[20230113 20:10:06 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:10:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:06 @agent_ppo2.py:186][0m |          -0.0002 |           7.6915 |           7.0828 |
[32m[20230113 20:10:06 @agent_ppo2.py:186][0m |          -0.0040 |           5.7032 |           7.0777 |
[32m[20230113 20:10:06 @agent_ppo2.py:186][0m |          -0.0049 |           5.0502 |           7.0808 |
[32m[20230113 20:10:06 @agent_ppo2.py:186][0m |          -0.0090 |           4.5587 |           7.0781 |
[32m[20230113 20:10:06 @agent_ppo2.py:186][0m |          -0.0101 |           4.2319 |           7.0775 |
[32m[20230113 20:10:06 @agent_ppo2.py:186][0m |          -0.0103 |           3.9789 |           7.0761 |
[32m[20230113 20:10:06 @agent_ppo2.py:186][0m |          -0.0097 |           3.8139 |           7.0734 |
[32m[20230113 20:10:06 @agent_ppo2.py:186][0m |          -0.0132 |           3.6648 |           7.0798 |
[32m[20230113 20:10:07 @agent_ppo2.py:186][0m |          -0.0137 |           3.5094 |           7.0818 |
[32m[20230113 20:10:07 @agent_ppo2.py:186][0m |          -0.0148 |           3.3873 |           7.0788 |
[32m[20230113 20:10:07 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:10:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.93
[32m[20230113 20:10:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.40
[32m[20230113 20:10:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.94
[32m[20230113 20:10:07 @agent_ppo2.py:144][0m Total time:      25.57 min
[32m[20230113 20:10:07 @agent_ppo2.py:146][0m 2342912 total steps have happened
[32m[20230113 20:10:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1144 --------------------------#
[32m[20230113 20:10:07 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:10:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:07 @agent_ppo2.py:186][0m |           0.0000 |           6.7991 |           7.2656 |
[32m[20230113 20:10:08 @agent_ppo2.py:186][0m |          -0.0050 |           5.4150 |           7.2546 |
[32m[20230113 20:10:08 @agent_ppo2.py:186][0m |          -0.0086 |           4.8935 |           7.2473 |
[32m[20230113 20:10:08 @agent_ppo2.py:186][0m |          -0.0100 |           4.5451 |           7.2406 |
[32m[20230113 20:10:08 @agent_ppo2.py:186][0m |          -0.0089 |           4.2951 |           7.2509 |
[32m[20230113 20:10:08 @agent_ppo2.py:186][0m |          -0.0109 |           4.1544 |           7.2457 |
[32m[20230113 20:10:08 @agent_ppo2.py:186][0m |          -0.0125 |           3.9544 |           7.2420 |
[32m[20230113 20:10:08 @agent_ppo2.py:186][0m |          -0.0125 |           3.8137 |           7.2521 |
[32m[20230113 20:10:08 @agent_ppo2.py:186][0m |          -0.0153 |           3.7158 |           7.2432 |
[32m[20230113 20:10:08 @agent_ppo2.py:186][0m |          -0.0147 |           3.6535 |           7.2488 |
[32m[20230113 20:10:08 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:10:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.83
[32m[20230113 20:10:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.50
[32m[20230113 20:10:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.95
[32m[20230113 20:10:08 @agent_ppo2.py:144][0m Total time:      25.60 min
[32m[20230113 20:10:08 @agent_ppo2.py:146][0m 2344960 total steps have happened
[32m[20230113 20:10:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1145 --------------------------#
[32m[20230113 20:10:09 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 20:10:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:09 @agent_ppo2.py:186][0m |          -0.0012 |          11.9871 |           7.0313 |
[32m[20230113 20:10:09 @agent_ppo2.py:186][0m |          -0.0061 |           6.1925 |           7.0301 |
[32m[20230113 20:10:09 @agent_ppo2.py:186][0m |          -0.0072 |           5.3433 |           7.0224 |
[32m[20230113 20:10:09 @agent_ppo2.py:186][0m |          -0.0106 |           4.9453 |           7.0273 |
[32m[20230113 20:10:09 @agent_ppo2.py:186][0m |          -0.0120 |           4.5481 |           7.0245 |
[32m[20230113 20:10:09 @agent_ppo2.py:186][0m |          -0.0102 |           4.2982 |           7.0269 |
[32m[20230113 20:10:09 @agent_ppo2.py:186][0m |          -0.0129 |           4.1080 |           7.0262 |
[32m[20230113 20:10:09 @agent_ppo2.py:186][0m |          -0.0128 |           3.9483 |           7.0264 |
[32m[20230113 20:10:09 @agent_ppo2.py:186][0m |          -0.0146 |           3.8688 |           7.0199 |
[32m[20230113 20:10:09 @agent_ppo2.py:186][0m |          -0.0143 |           3.7294 |           7.0258 |
[32m[20230113 20:10:09 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 20:10:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 128.67
[32m[20230113 20:10:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.10
[32m[20230113 20:10:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.25
[32m[20230113 20:10:10 @agent_ppo2.py:144][0m Total time:      25.62 min
[32m[20230113 20:10:10 @agent_ppo2.py:146][0m 2347008 total steps have happened
[32m[20230113 20:10:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1146 --------------------------#
[32m[20230113 20:10:10 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:10:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:10 @agent_ppo2.py:186][0m |           0.0015 |           7.0508 |           7.2133 |
[32m[20230113 20:10:10 @agent_ppo2.py:186][0m |          -0.0063 |           6.0397 |           7.2143 |
[32m[20230113 20:10:10 @agent_ppo2.py:186][0m |          -0.0127 |           5.5326 |           7.2064 |
[32m[20230113 20:10:10 @agent_ppo2.py:186][0m |          -0.0104 |           5.2338 |           7.2062 |
[32m[20230113 20:10:10 @agent_ppo2.py:186][0m |          -0.0122 |           4.9013 |           7.1974 |
[32m[20230113 20:10:10 @agent_ppo2.py:186][0m |           0.0066 |           4.8072 |           7.2013 |
[32m[20230113 20:10:10 @agent_ppo2.py:186][0m |          -0.0133 |           4.5727 |           7.2025 |
[32m[20230113 20:10:10 @agent_ppo2.py:186][0m |          -0.0212 |           4.4455 |           7.2043 |
[32m[20230113 20:10:10 @agent_ppo2.py:186][0m |          -0.0029 |           4.5559 |           7.2106 |
[32m[20230113 20:10:11 @agent_ppo2.py:186][0m |          -0.0318 |           4.4499 |           7.2026 |
[32m[20230113 20:10:11 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:10:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.61
[32m[20230113 20:10:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.29
[32m[20230113 20:10:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 140.36
[32m[20230113 20:10:11 @agent_ppo2.py:144][0m Total time:      25.64 min
[32m[20230113 20:10:11 @agent_ppo2.py:146][0m 2349056 total steps have happened
[32m[20230113 20:10:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1147 --------------------------#
[32m[20230113 20:10:11 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 20:10:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:11 @agent_ppo2.py:186][0m |          -0.0015 |          17.1896 |           7.1920 |
[32m[20230113 20:10:11 @agent_ppo2.py:186][0m |          -0.0089 |          10.5501 |           7.1881 |
[32m[20230113 20:10:12 @agent_ppo2.py:186][0m |          -0.0117 |           8.6053 |           7.1849 |
[32m[20230113 20:10:12 @agent_ppo2.py:186][0m |          -0.0142 |           7.1232 |           7.1790 |
[32m[20230113 20:10:12 @agent_ppo2.py:186][0m |          -0.0137 |           6.6547 |           7.1810 |
[32m[20230113 20:10:12 @agent_ppo2.py:186][0m |          -0.0132 |           5.7810 |           7.1748 |
[32m[20230113 20:10:12 @agent_ppo2.py:186][0m |          -0.0153 |           5.3946 |           7.1761 |
[32m[20230113 20:10:12 @agent_ppo2.py:186][0m |          -0.0164 |           5.2436 |           7.1604 |
[32m[20230113 20:10:12 @agent_ppo2.py:186][0m |          -0.0172 |           4.8065 |           7.1659 |
[32m[20230113 20:10:12 @agent_ppo2.py:186][0m |          -0.0150 |           4.5967 |           7.1655 |
[32m[20230113 20:10:12 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:10:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 96.98
[32m[20230113 20:10:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.20
[32m[20230113 20:10:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.34
[32m[20230113 20:10:12 @agent_ppo2.py:144][0m Total time:      25.66 min
[32m[20230113 20:10:12 @agent_ppo2.py:146][0m 2351104 total steps have happened
[32m[20230113 20:10:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1148 --------------------------#
[32m[20230113 20:10:13 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:10:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:13 @agent_ppo2.py:186][0m |           0.0000 |          17.3584 |           7.1000 |
[32m[20230113 20:10:13 @agent_ppo2.py:186][0m |          -0.0054 |           9.6705 |           7.0915 |
[32m[20230113 20:10:13 @agent_ppo2.py:186][0m |          -0.0067 |           7.4299 |           7.0904 |
[32m[20230113 20:10:13 @agent_ppo2.py:186][0m |          -0.0111 |           6.2726 |           7.0855 |
[32m[20230113 20:10:13 @agent_ppo2.py:186][0m |          -0.0111 |           5.5509 |           7.0859 |
[32m[20230113 20:10:13 @agent_ppo2.py:186][0m |          -0.0099 |           5.0812 |           7.0779 |
[32m[20230113 20:10:13 @agent_ppo2.py:186][0m |          -0.0119 |           4.6861 |           7.0792 |
[32m[20230113 20:10:13 @agent_ppo2.py:186][0m |          -0.0154 |           4.3628 |           7.0738 |
[32m[20230113 20:10:13 @agent_ppo2.py:186][0m |          -0.0145 |           4.1352 |           7.0736 |
[32m[20230113 20:10:13 @agent_ppo2.py:186][0m |          -0.0169 |           3.9635 |           7.0709 |
[32m[20230113 20:10:13 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 20:10:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 138.75
[32m[20230113 20:10:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.01
[32m[20230113 20:10:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.67
[32m[20230113 20:10:13 @agent_ppo2.py:144][0m Total time:      25.68 min
[32m[20230113 20:10:13 @agent_ppo2.py:146][0m 2353152 total steps have happened
[32m[20230113 20:10:13 @agent_ppo2.py:122][0m #------------------------ Iteration 1149 --------------------------#
[32m[20230113 20:10:14 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:10:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:14 @agent_ppo2.py:186][0m |           0.0000 |          12.7480 |           7.1583 |
[32m[20230113 20:10:14 @agent_ppo2.py:186][0m |          -0.0053 |           8.2471 |           7.1537 |
[32m[20230113 20:10:14 @agent_ppo2.py:186][0m |          -0.0060 |           7.1186 |           7.1470 |
[32m[20230113 20:10:14 @agent_ppo2.py:186][0m |          -0.0098 |           6.4665 |           7.1548 |
[32m[20230113 20:10:14 @agent_ppo2.py:186][0m |          -0.0096 |           6.0468 |           7.1490 |
[32m[20230113 20:10:14 @agent_ppo2.py:186][0m |          -0.0125 |           5.7521 |           7.1452 |
[32m[20230113 20:10:14 @agent_ppo2.py:186][0m |          -0.0105 |           5.4853 |           7.1472 |
[32m[20230113 20:10:14 @agent_ppo2.py:186][0m |          -0.0134 |           5.2804 |           7.1416 |
[32m[20230113 20:10:14 @agent_ppo2.py:186][0m |          -0.0142 |           5.0985 |           7.1460 |
[32m[20230113 20:10:14 @agent_ppo2.py:186][0m |          -0.0146 |           4.9485 |           7.1375 |
[32m[20230113 20:10:14 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:10:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.22
[32m[20230113 20:10:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.16
[32m[20230113 20:10:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 151.18
[32m[20230113 20:10:15 @agent_ppo2.py:144][0m Total time:      25.70 min
[32m[20230113 20:10:15 @agent_ppo2.py:146][0m 2355200 total steps have happened
[32m[20230113 20:10:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1150 --------------------------#
[32m[20230113 20:10:15 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:10:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:15 @agent_ppo2.py:186][0m |          -0.0026 |          23.4479 |           7.3251 |
[32m[20230113 20:10:15 @agent_ppo2.py:186][0m |          -0.0070 |          12.5673 |           7.3113 |
[32m[20230113 20:10:15 @agent_ppo2.py:186][0m |          -0.0092 |           9.9868 |           7.3072 |
[32m[20230113 20:10:15 @agent_ppo2.py:186][0m |          -0.0105 |           8.6009 |           7.3077 |
[32m[20230113 20:10:15 @agent_ppo2.py:186][0m |          -0.0123 |           7.7227 |           7.3090 |
[32m[20230113 20:10:16 @agent_ppo2.py:186][0m |          -0.0124 |           7.0765 |           7.3104 |
[32m[20230113 20:10:16 @agent_ppo2.py:186][0m |          -0.0117 |           6.6580 |           7.3126 |
[32m[20230113 20:10:16 @agent_ppo2.py:186][0m |          -0.0148 |           6.2698 |           7.3155 |
[32m[20230113 20:10:16 @agent_ppo2.py:186][0m |          -0.0134 |           6.0659 |           7.3151 |
[32m[20230113 20:10:16 @agent_ppo2.py:186][0m |          -0.0147 |           5.7151 |           7.3121 |
[32m[20230113 20:10:16 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:10:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 119.19
[32m[20230113 20:10:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.88
[32m[20230113 20:10:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.21
[32m[20230113 20:10:16 @agent_ppo2.py:144][0m Total time:      25.73 min
[32m[20230113 20:10:16 @agent_ppo2.py:146][0m 2357248 total steps have happened
[32m[20230113 20:10:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1151 --------------------------#
[32m[20230113 20:10:17 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:10:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:17 @agent_ppo2.py:186][0m |           0.0408 |           6.7179 |           7.0045 |
[32m[20230113 20:10:17 @agent_ppo2.py:186][0m |          -0.0031 |           5.7919 |           6.9769 |
[32m[20230113 20:10:17 @agent_ppo2.py:186][0m |          -0.0614 |           5.6042 |           6.9780 |
[32m[20230113 20:10:17 @agent_ppo2.py:186][0m |          -0.0011 |           5.4691 |           6.9692 |
[32m[20230113 20:10:17 @agent_ppo2.py:186][0m |           0.0208 |           4.7690 |           6.9819 |
[32m[20230113 20:10:17 @agent_ppo2.py:186][0m |          -0.0076 |           4.4002 |           6.9844 |
[32m[20230113 20:10:17 @agent_ppo2.py:186][0m |          -0.0111 |           4.2251 |           6.9812 |
[32m[20230113 20:10:17 @agent_ppo2.py:186][0m |          -0.0616 |           4.1741 |           6.9794 |
[32m[20230113 20:10:17 @agent_ppo2.py:186][0m |          -0.0175 |           4.0393 |           6.9807 |
[32m[20230113 20:10:17 @agent_ppo2.py:186][0m |           0.0096 |           3.9237 |           6.9893 |
[32m[20230113 20:10:17 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:10:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.93
[32m[20230113 20:10:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.47
[32m[20230113 20:10:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.64
[32m[20230113 20:10:17 @agent_ppo2.py:144][0m Total time:      25.75 min
[32m[20230113 20:10:17 @agent_ppo2.py:146][0m 2359296 total steps have happened
[32m[20230113 20:10:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1152 --------------------------#
[32m[20230113 20:10:18 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:10:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:18 @agent_ppo2.py:186][0m |           0.0017 |          25.4844 |           7.2227 |
[32m[20230113 20:10:18 @agent_ppo2.py:186][0m |          -0.0025 |          14.5479 |           7.2270 |
[32m[20230113 20:10:18 @agent_ppo2.py:186][0m |          -0.0040 |          12.6049 |           7.2223 |
[32m[20230113 20:10:18 @agent_ppo2.py:186][0m |          -0.0069 |          11.5319 |           7.2235 |
[32m[20230113 20:10:18 @agent_ppo2.py:186][0m |          -0.0084 |          10.8922 |           7.2282 |
[32m[20230113 20:10:18 @agent_ppo2.py:186][0m |          -0.0082 |          10.0844 |           7.2180 |
[32m[20230113 20:10:18 @agent_ppo2.py:186][0m |          -0.0099 |           9.5721 |           7.2177 |
[32m[20230113 20:10:18 @agent_ppo2.py:186][0m |          -0.0097 |           9.0369 |           7.2235 |
[32m[20230113 20:10:18 @agent_ppo2.py:186][0m |          -0.0101 |           8.6529 |           7.2180 |
[32m[20230113 20:10:18 @agent_ppo2.py:186][0m |          -0.0117 |           8.3437 |           7.2168 |
[32m[20230113 20:10:18 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 20:10:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 139.62
[32m[20230113 20:10:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.36
[32m[20230113 20:10:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 26.38
[32m[20230113 20:10:19 @agent_ppo2.py:144][0m Total time:      25.77 min
[32m[20230113 20:10:19 @agent_ppo2.py:146][0m 2361344 total steps have happened
[32m[20230113 20:10:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1153 --------------------------#
[32m[20230113 20:10:19 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:10:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:19 @agent_ppo2.py:186][0m |           0.0012 |           7.1206 |           7.0488 |
[32m[20230113 20:10:19 @agent_ppo2.py:186][0m |          -0.0090 |           5.4309 |           7.0351 |
[32m[20230113 20:10:19 @agent_ppo2.py:186][0m |          -0.0165 |           4.8902 |           7.0336 |
[32m[20230113 20:10:19 @agent_ppo2.py:186][0m |          -0.0094 |           4.6046 |           7.0292 |
[32m[20230113 20:10:20 @agent_ppo2.py:186][0m |          -0.0254 |           4.3827 |           7.0255 |
[32m[20230113 20:10:20 @agent_ppo2.py:186][0m |          -0.0156 |           4.1763 |           7.0287 |
[32m[20230113 20:10:20 @agent_ppo2.py:186][0m |          -0.0130 |           4.0461 |           7.0255 |
[32m[20230113 20:10:20 @agent_ppo2.py:186][0m |          -0.0074 |           3.9342 |           7.0244 |
[32m[20230113 20:10:20 @agent_ppo2.py:186][0m |          -0.0268 |           4.0680 |           7.0208 |
[32m[20230113 20:10:20 @agent_ppo2.py:186][0m |          -0.0094 |           3.8340 |           7.0186 |
[32m[20230113 20:10:20 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:10:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.74
[32m[20230113 20:10:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.51
[32m[20230113 20:10:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 142.21
[32m[20230113 20:10:20 @agent_ppo2.py:144][0m Total time:      25.79 min
[32m[20230113 20:10:20 @agent_ppo2.py:146][0m 2363392 total steps have happened
[32m[20230113 20:10:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1154 --------------------------#
[32m[20230113 20:10:21 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:10:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:21 @agent_ppo2.py:186][0m |           0.0248 |           6.8346 |           7.1816 |
[32m[20230113 20:10:21 @agent_ppo2.py:186][0m |          -0.0028 |           5.0858 |           7.1760 |
[32m[20230113 20:10:21 @agent_ppo2.py:186][0m |          -0.0090 |           4.4433 |           7.1749 |
[32m[20230113 20:10:21 @agent_ppo2.py:186][0m |          -0.0120 |           4.1360 |           7.1756 |
[32m[20230113 20:10:21 @agent_ppo2.py:186][0m |          -0.0108 |           3.9150 |           7.1705 |
[32m[20230113 20:10:21 @agent_ppo2.py:186][0m |          -0.0145 |           3.7638 |           7.1706 |
[32m[20230113 20:10:21 @agent_ppo2.py:186][0m |          -0.0141 |           3.6329 |           7.1689 |
[32m[20230113 20:10:21 @agent_ppo2.py:186][0m |          -0.0083 |           3.7309 |           7.1743 |
[32m[20230113 20:10:21 @agent_ppo2.py:186][0m |          -0.0149 |           3.4474 |           7.1614 |
[32m[20230113 20:10:21 @agent_ppo2.py:186][0m |          -0.0218 |           3.3664 |           7.1705 |
[32m[20230113 20:10:21 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:10:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.81
[32m[20230113 20:10:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.10
[32m[20230113 20:10:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.29
[32m[20230113 20:10:21 @agent_ppo2.py:144][0m Total time:      25.81 min
[32m[20230113 20:10:21 @agent_ppo2.py:146][0m 2365440 total steps have happened
[32m[20230113 20:10:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1155 --------------------------#
[32m[20230113 20:10:22 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 20:10:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:22 @agent_ppo2.py:186][0m |           0.0052 |          28.4474 |           7.1323 |
[32m[20230113 20:10:22 @agent_ppo2.py:186][0m |          -0.0046 |          11.5107 |           7.1270 |
[32m[20230113 20:10:22 @agent_ppo2.py:186][0m |          -0.0142 |           9.7609 |           7.1192 |
[32m[20230113 20:10:22 @agent_ppo2.py:186][0m |          -0.0176 |           8.9188 |           7.1114 |
[32m[20230113 20:10:22 @agent_ppo2.py:186][0m |          -0.0142 |           8.0712 |           7.1062 |
[32m[20230113 20:10:22 @agent_ppo2.py:186][0m |          -0.0169 |           7.4562 |           7.0971 |
[32m[20230113 20:10:22 @agent_ppo2.py:186][0m |          -0.0183 |           7.0755 |           7.1010 |
[32m[20230113 20:10:22 @agent_ppo2.py:186][0m |          -0.0138 |           6.8833 |           7.0929 |
[32m[20230113 20:10:22 @agent_ppo2.py:186][0m |          -0.0049 |           6.1223 |           7.0943 |
[32m[20230113 20:10:22 @agent_ppo2.py:186][0m |          -0.0100 |           5.7989 |           7.0852 |
[32m[20230113 20:10:22 @agent_ppo2.py:131][0m Policy update time: 0.40 s
[32m[20230113 20:10:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 57.22
[32m[20230113 20:10:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.78
[32m[20230113 20:10:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.38
[32m[20230113 20:10:23 @agent_ppo2.py:144][0m Total time:      25.83 min
[32m[20230113 20:10:23 @agent_ppo2.py:146][0m 2367488 total steps have happened
[32m[20230113 20:10:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1156 --------------------------#
[32m[20230113 20:10:23 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:10:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:23 @agent_ppo2.py:186][0m |          -0.0002 |          27.3356 |           7.2903 |
[32m[20230113 20:10:23 @agent_ppo2.py:186][0m |          -0.0057 |          17.5519 |           7.2845 |
[32m[20230113 20:10:23 @agent_ppo2.py:186][0m |          -0.0062 |          12.6425 |           7.2825 |
[32m[20230113 20:10:23 @agent_ppo2.py:186][0m |          -0.0095 |          10.3866 |           7.2817 |
[32m[20230113 20:10:23 @agent_ppo2.py:186][0m |          -0.0092 |           8.9556 |           7.2820 |
[32m[20230113 20:10:23 @agent_ppo2.py:186][0m |          -0.0119 |           7.9370 |           7.2777 |
[32m[20230113 20:10:23 @agent_ppo2.py:186][0m |          -0.0123 |           7.4910 |           7.2736 |
[32m[20230113 20:10:24 @agent_ppo2.py:186][0m |          -0.0141 |           6.8430 |           7.2655 |
[32m[20230113 20:10:24 @agent_ppo2.py:186][0m |          -0.0143 |           6.5099 |           7.2648 |
[32m[20230113 20:10:24 @agent_ppo2.py:186][0m |          -0.0135 |           6.1837 |           7.2631 |
[32m[20230113 20:10:24 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:10:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 120.95
[32m[20230113 20:10:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.31
[32m[20230113 20:10:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.98
[32m[20230113 20:10:24 @agent_ppo2.py:144][0m Total time:      25.86 min
[32m[20230113 20:10:24 @agent_ppo2.py:146][0m 2369536 total steps have happened
[32m[20230113 20:10:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1157 --------------------------#
[32m[20230113 20:10:24 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:10:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:25 @agent_ppo2.py:186][0m |          -0.0022 |           6.9890 |           7.0245 |
[32m[20230113 20:10:25 @agent_ppo2.py:186][0m |          -0.0082 |           5.3887 |           7.0154 |
[32m[20230113 20:10:25 @agent_ppo2.py:186][0m |          -0.0123 |           4.9007 |           7.0055 |
[32m[20230113 20:10:25 @agent_ppo2.py:186][0m |          -0.0137 |           4.5937 |           7.0105 |
[32m[20230113 20:10:25 @agent_ppo2.py:186][0m |          -0.0150 |           4.3429 |           7.0043 |
[32m[20230113 20:10:25 @agent_ppo2.py:186][0m |          -0.0174 |           4.1498 |           7.0070 |
[32m[20230113 20:10:25 @agent_ppo2.py:186][0m |          -0.0172 |           4.0200 |           7.0088 |
[32m[20230113 20:10:25 @agent_ppo2.py:186][0m |          -0.0177 |           3.8670 |           7.0053 |
[32m[20230113 20:10:25 @agent_ppo2.py:186][0m |          -0.0206 |           3.7484 |           7.0029 |
[32m[20230113 20:10:25 @agent_ppo2.py:186][0m |          -0.0197 |           3.6588 |           7.0052 |
[32m[20230113 20:10:25 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:10:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.51
[32m[20230113 20:10:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.26
[32m[20230113 20:10:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.20
[32m[20230113 20:10:25 @agent_ppo2.py:144][0m Total time:      25.88 min
[32m[20230113 20:10:25 @agent_ppo2.py:146][0m 2371584 total steps have happened
[32m[20230113 20:10:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1158 --------------------------#
[32m[20230113 20:10:26 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:10:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:26 @agent_ppo2.py:186][0m |          -0.0024 |           9.6989 |           7.2022 |
[32m[20230113 20:10:26 @agent_ppo2.py:186][0m |          -0.0081 |           7.0257 |           7.1810 |
[32m[20230113 20:10:26 @agent_ppo2.py:186][0m |          -0.0092 |           6.2408 |           7.1746 |
[32m[20230113 20:10:26 @agent_ppo2.py:186][0m |          -0.0109 |           5.8022 |           7.1747 |
[32m[20230113 20:10:26 @agent_ppo2.py:186][0m |          -0.0079 |           5.5042 |           7.1801 |
[32m[20230113 20:10:26 @agent_ppo2.py:186][0m |          -0.0123 |           5.1756 |           7.1740 |
[32m[20230113 20:10:26 @agent_ppo2.py:186][0m |          -0.0123 |           4.9183 |           7.1774 |
[32m[20230113 20:10:26 @agent_ppo2.py:186][0m |          -0.0129 |           4.7278 |           7.1726 |
[32m[20230113 20:10:26 @agent_ppo2.py:186][0m |          -0.0158 |           4.6283 |           7.1692 |
[32m[20230113 20:10:26 @agent_ppo2.py:186][0m |          -0.0124 |           4.4677 |           7.1829 |
[32m[20230113 20:10:26 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:10:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.09
[32m[20230113 20:10:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.26
[32m[20230113 20:10:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 5.94
[32m[20230113 20:10:26 @agent_ppo2.py:144][0m Total time:      25.90 min
[32m[20230113 20:10:26 @agent_ppo2.py:146][0m 2373632 total steps have happened
[32m[20230113 20:10:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1159 --------------------------#
[32m[20230113 20:10:27 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:10:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:27 @agent_ppo2.py:186][0m |          -0.0007 |           6.4697 |           7.3934 |
[32m[20230113 20:10:27 @agent_ppo2.py:186][0m |          -0.0079 |           5.2349 |           7.3769 |
[32m[20230113 20:10:27 @agent_ppo2.py:186][0m |          -0.0133 |           4.7611 |           7.3760 |
[32m[20230113 20:10:27 @agent_ppo2.py:186][0m |          -0.0068 |           4.4090 |           7.3787 |
[32m[20230113 20:10:27 @agent_ppo2.py:186][0m |          -0.0132 |           4.2664 |           7.3869 |
[32m[20230113 20:10:27 @agent_ppo2.py:186][0m |          -0.0100 |           4.0641 |           7.3777 |
[32m[20230113 20:10:27 @agent_ppo2.py:186][0m |          -0.0160 |           3.8738 |           7.3810 |
[32m[20230113 20:10:27 @agent_ppo2.py:186][0m |          -0.0116 |           3.8499 |           7.3774 |
[32m[20230113 20:10:27 @agent_ppo2.py:186][0m |          -0.0148 |           3.7037 |           7.3852 |
[32m[20230113 20:10:27 @agent_ppo2.py:186][0m |          -0.0139 |           3.5859 |           7.3825 |
[32m[20230113 20:10:27 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:10:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.92
[32m[20230113 20:10:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.78
[32m[20230113 20:10:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.17
[32m[20230113 20:10:28 @agent_ppo2.py:144][0m Total time:      25.92 min
[32m[20230113 20:10:28 @agent_ppo2.py:146][0m 2375680 total steps have happened
[32m[20230113 20:10:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1160 --------------------------#
[32m[20230113 20:10:28 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:10:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:28 @agent_ppo2.py:186][0m |          -0.0011 |           7.0921 |           7.1750 |
[32m[20230113 20:10:28 @agent_ppo2.py:186][0m |          -0.0054 |           5.5949 |           7.1680 |
[32m[20230113 20:10:28 @agent_ppo2.py:186][0m |          -0.0082 |           5.2499 |           7.1650 |
[32m[20230113 20:10:28 @agent_ppo2.py:186][0m |          -0.0068 |           4.8025 |           7.1671 |
[32m[20230113 20:10:28 @agent_ppo2.py:186][0m |          -0.0102 |           4.6327 |           7.1635 |
[32m[20230113 20:10:28 @agent_ppo2.py:186][0m |          -0.0096 |           4.4898 |           7.1673 |
[32m[20230113 20:10:29 @agent_ppo2.py:186][0m |          -0.0116 |           4.2996 |           7.1709 |
[32m[20230113 20:10:29 @agent_ppo2.py:186][0m |          -0.0146 |           4.2255 |           7.1667 |
[32m[20230113 20:10:29 @agent_ppo2.py:186][0m |          -0.0148 |           4.1433 |           7.1656 |
[32m[20230113 20:10:29 @agent_ppo2.py:186][0m |          -0.0137 |           4.0213 |           7.1639 |
[32m[20230113 20:10:29 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:10:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.50
[32m[20230113 20:10:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.20
[32m[20230113 20:10:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.55
[32m[20230113 20:10:29 @agent_ppo2.py:144][0m Total time:      25.94 min
[32m[20230113 20:10:29 @agent_ppo2.py:146][0m 2377728 total steps have happened
[32m[20230113 20:10:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1161 --------------------------#
[32m[20230113 20:10:29 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:10:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:30 @agent_ppo2.py:186][0m |           0.0012 |           6.8090 |           7.3890 |
[32m[20230113 20:10:30 @agent_ppo2.py:186][0m |          -0.0013 |           5.4226 |           7.3858 |
[32m[20230113 20:10:30 @agent_ppo2.py:186][0m |          -0.0026 |           4.8563 |           7.3832 |
[32m[20230113 20:10:30 @agent_ppo2.py:186][0m |          -0.0045 |           4.5530 |           7.3760 |
[32m[20230113 20:10:30 @agent_ppo2.py:186][0m |          -0.0061 |           4.3474 |           7.3732 |
[32m[20230113 20:10:30 @agent_ppo2.py:186][0m |          -0.0077 |           4.1339 |           7.3743 |
[32m[20230113 20:10:30 @agent_ppo2.py:186][0m |          -0.0082 |           4.0185 |           7.3719 |
[32m[20230113 20:10:30 @agent_ppo2.py:186][0m |          -0.0092 |           3.9037 |           7.3688 |
[32m[20230113 20:10:30 @agent_ppo2.py:186][0m |          -0.0089 |           3.8042 |           7.3729 |
[32m[20230113 20:10:30 @agent_ppo2.py:186][0m |          -0.0105 |           3.6915 |           7.3664 |
[32m[20230113 20:10:30 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:10:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.74
[32m[20230113 20:10:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.35
[32m[20230113 20:10:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.97
[32m[20230113 20:10:30 @agent_ppo2.py:144][0m Total time:      25.96 min
[32m[20230113 20:10:30 @agent_ppo2.py:146][0m 2379776 total steps have happened
[32m[20230113 20:10:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1162 --------------------------#
[32m[20230113 20:10:31 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:10:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:31 @agent_ppo2.py:186][0m |           0.0001 |          16.8699 |           7.2337 |
[32m[20230113 20:10:31 @agent_ppo2.py:186][0m |          -0.0023 |           9.4045 |           7.2309 |
[32m[20230113 20:10:31 @agent_ppo2.py:186][0m |          -0.0065 |           7.7081 |           7.2239 |
[32m[20230113 20:10:31 @agent_ppo2.py:186][0m |          -0.0084 |           6.8309 |           7.2289 |
[32m[20230113 20:10:31 @agent_ppo2.py:186][0m |          -0.0082 |           6.3327 |           7.2272 |
[32m[20230113 20:10:31 @agent_ppo2.py:186][0m |          -0.0119 |           5.8948 |           7.2266 |
[32m[20230113 20:10:31 @agent_ppo2.py:186][0m |          -0.0121 |           5.7529 |           7.2214 |
[32m[20230113 20:10:31 @agent_ppo2.py:186][0m |          -0.0132 |           5.3314 |           7.2213 |
[32m[20230113 20:10:31 @agent_ppo2.py:186][0m |          -0.0126 |           5.1684 |           7.2201 |
[32m[20230113 20:10:31 @agent_ppo2.py:186][0m |          -0.0102 |           5.0211 |           7.2241 |
[32m[20230113 20:10:31 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:10:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 161.64
[32m[20230113 20:10:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.14
[32m[20230113 20:10:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.01
[32m[20230113 20:10:32 @agent_ppo2.py:144][0m Total time:      25.98 min
[32m[20230113 20:10:32 @agent_ppo2.py:146][0m 2381824 total steps have happened
[32m[20230113 20:10:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1163 --------------------------#
[32m[20230113 20:10:32 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 20:10:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:32 @agent_ppo2.py:186][0m |           0.0007 |          19.3957 |           7.3433 |
[32m[20230113 20:10:32 @agent_ppo2.py:186][0m |          -0.0036 |          12.8050 |           7.3406 |
[32m[20230113 20:10:32 @agent_ppo2.py:186][0m |          -0.0093 |          10.3810 |           7.3319 |
[32m[20230113 20:10:32 @agent_ppo2.py:186][0m |          -0.0102 |           9.3203 |           7.3271 |
[32m[20230113 20:10:32 @agent_ppo2.py:186][0m |          -0.0116 |           8.5629 |           7.3238 |
[32m[20230113 20:10:32 @agent_ppo2.py:186][0m |          -0.0100 |           8.1431 |           7.3200 |
[32m[20230113 20:10:32 @agent_ppo2.py:186][0m |          -0.0085 |           7.7311 |           7.3201 |
[32m[20230113 20:10:33 @agent_ppo2.py:186][0m |          -0.0141 |           7.3995 |           7.3183 |
[32m[20230113 20:10:33 @agent_ppo2.py:186][0m |          -0.0157 |           7.0242 |           7.3120 |
[32m[20230113 20:10:33 @agent_ppo2.py:186][0m |          -0.0164 |           6.7111 |           7.3108 |
[32m[20230113 20:10:33 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 20:10:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 144.99
[32m[20230113 20:10:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.60
[32m[20230113 20:10:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.67
[32m[20230113 20:10:33 @agent_ppo2.py:144][0m Total time:      26.01 min
[32m[20230113 20:10:33 @agent_ppo2.py:146][0m 2383872 total steps have happened
[32m[20230113 20:10:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1164 --------------------------#
[32m[20230113 20:10:33 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:10:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:34 @agent_ppo2.py:186][0m |          -0.0032 |           7.2348 |           7.1726 |
[32m[20230113 20:10:34 @agent_ppo2.py:186][0m |          -0.0033 |           5.6778 |           7.1564 |
[32m[20230113 20:10:34 @agent_ppo2.py:186][0m |          -0.0104 |           5.2286 |           7.1548 |
[32m[20230113 20:10:34 @agent_ppo2.py:186][0m |          -0.0118 |           4.8982 |           7.1493 |
[32m[20230113 20:10:34 @agent_ppo2.py:186][0m |          -0.0127 |           4.7025 |           7.1575 |
[32m[20230113 20:10:34 @agent_ppo2.py:186][0m |          -0.0124 |           4.5463 |           7.1541 |
[32m[20230113 20:10:34 @agent_ppo2.py:186][0m |          -0.0092 |           4.4115 |           7.1496 |
[32m[20230113 20:10:34 @agent_ppo2.py:186][0m |          -0.0131 |           4.2308 |           7.1476 |
[32m[20230113 20:10:34 @agent_ppo2.py:186][0m |          -0.0145 |           4.1356 |           7.1489 |
[32m[20230113 20:10:34 @agent_ppo2.py:186][0m |          -0.0144 |           4.1214 |           7.1488 |
[32m[20230113 20:10:34 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:10:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.80
[32m[20230113 20:10:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.58
[32m[20230113 20:10:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.66
[32m[20230113 20:10:34 @agent_ppo2.py:144][0m Total time:      26.03 min
[32m[20230113 20:10:34 @agent_ppo2.py:146][0m 2385920 total steps have happened
[32m[20230113 20:10:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1165 --------------------------#
[32m[20230113 20:10:35 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:10:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:35 @agent_ppo2.py:186][0m |           0.0005 |          10.4050 |           7.1362 |
[32m[20230113 20:10:35 @agent_ppo2.py:186][0m |          -0.0097 |           6.5116 |           7.1129 |
[32m[20230113 20:10:35 @agent_ppo2.py:186][0m |          -0.0143 |           5.7673 |           7.1142 |
[32m[20230113 20:10:35 @agent_ppo2.py:186][0m |          -0.0176 |           5.3679 |           7.1043 |
[32m[20230113 20:10:35 @agent_ppo2.py:186][0m |          -0.0156 |           5.0152 |           7.1077 |
[32m[20230113 20:10:35 @agent_ppo2.py:186][0m |          -0.0141 |           4.8132 |           7.1011 |
[32m[20230113 20:10:35 @agent_ppo2.py:186][0m |          -0.0166 |           4.6459 |           7.1036 |
[32m[20230113 20:10:35 @agent_ppo2.py:186][0m |          -0.0158 |           4.4818 |           7.1011 |
[32m[20230113 20:10:35 @agent_ppo2.py:186][0m |          -0.0196 |           4.3576 |           7.0987 |
[32m[20230113 20:10:35 @agent_ppo2.py:186][0m |          -0.0174 |           4.2505 |           7.0927 |
[32m[20230113 20:10:35 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:10:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.29
[32m[20230113 20:10:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.17
[32m[20230113 20:10:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.32
[32m[20230113 20:10:36 @agent_ppo2.py:144][0m Total time:      26.05 min
[32m[20230113 20:10:36 @agent_ppo2.py:146][0m 2387968 total steps have happened
[32m[20230113 20:10:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1166 --------------------------#
[32m[20230113 20:10:36 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:10:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:36 @agent_ppo2.py:186][0m |           0.0001 |           7.1819 |           7.3807 |
[32m[20230113 20:10:36 @agent_ppo2.py:186][0m |          -0.0055 |           5.7254 |           7.3625 |
[32m[20230113 20:10:36 @agent_ppo2.py:186][0m |          -0.0082 |           5.2327 |           7.3593 |
[32m[20230113 20:10:36 @agent_ppo2.py:186][0m |          -0.0107 |           4.9500 |           7.3539 |
[32m[20230113 20:10:36 @agent_ppo2.py:186][0m |          -0.0098 |           4.7290 |           7.3435 |
[32m[20230113 20:10:36 @agent_ppo2.py:186][0m |          -0.0123 |           4.5380 |           7.3442 |
[32m[20230113 20:10:36 @agent_ppo2.py:186][0m |          -0.0117 |           4.4552 |           7.3434 |
[32m[20230113 20:10:36 @agent_ppo2.py:186][0m |          -0.0139 |           4.2983 |           7.3378 |
[32m[20230113 20:10:36 @agent_ppo2.py:186][0m |          -0.0142 |           4.2160 |           7.3391 |
[32m[20230113 20:10:36 @agent_ppo2.py:186][0m |          -0.0151 |           4.0988 |           7.3376 |
[32m[20230113 20:10:36 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:10:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.84
[32m[20230113 20:10:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.44
[32m[20230113 20:10:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.42
[32m[20230113 20:10:37 @agent_ppo2.py:144][0m Total time:      26.07 min
[32m[20230113 20:10:37 @agent_ppo2.py:146][0m 2390016 total steps have happened
[32m[20230113 20:10:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1167 --------------------------#
[32m[20230113 20:10:37 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:10:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:37 @agent_ppo2.py:186][0m |           0.0033 |           6.6215 |           7.2767 |
[32m[20230113 20:10:37 @agent_ppo2.py:186][0m |          -0.0044 |           5.7757 |           7.2553 |
[32m[20230113 20:10:37 @agent_ppo2.py:186][0m |          -0.0094 |           5.3496 |           7.2512 |
[32m[20230113 20:10:38 @agent_ppo2.py:186][0m |          -0.0115 |           5.0410 |           7.2423 |
[32m[20230113 20:10:38 @agent_ppo2.py:186][0m |          -0.0103 |           4.8292 |           7.2432 |
[32m[20230113 20:10:38 @agent_ppo2.py:186][0m |          -0.0116 |           4.6680 |           7.2382 |
[32m[20230113 20:10:38 @agent_ppo2.py:186][0m |          -0.0131 |           4.4789 |           7.2373 |
[32m[20230113 20:10:38 @agent_ppo2.py:186][0m |          -0.0142 |           4.4128 |           7.2296 |
[32m[20230113 20:10:38 @agent_ppo2.py:186][0m |          -0.0137 |           4.2858 |           7.2290 |
[32m[20230113 20:10:38 @agent_ppo2.py:186][0m |          -0.0172 |           4.1552 |           7.2221 |
[32m[20230113 20:10:38 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:10:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.85
[32m[20230113 20:10:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.64
[32m[20230113 20:10:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.93
[32m[20230113 20:10:38 @agent_ppo2.py:144][0m Total time:      26.09 min
[32m[20230113 20:10:38 @agent_ppo2.py:146][0m 2392064 total steps have happened
[32m[20230113 20:10:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1168 --------------------------#
[32m[20230113 20:10:38 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:10:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:39 @agent_ppo2.py:186][0m |           0.0270 |          89.9811 |           7.0439 |
[32m[20230113 20:10:39 @agent_ppo2.py:186][0m |          -0.0099 |          46.7129 |           7.0223 |
[32m[20230113 20:10:39 @agent_ppo2.py:186][0m |          -0.0072 |          32.5465 |           7.0146 |
[32m[20230113 20:10:39 @agent_ppo2.py:186][0m |          -0.0102 |          24.2301 |           7.0272 |
[32m[20230113 20:10:39 @agent_ppo2.py:186][0m |          -0.0136 |          17.8874 |           7.0379 |
[32m[20230113 20:10:39 @agent_ppo2.py:186][0m |          -0.0147 |          13.7475 |           7.0370 |
[32m[20230113 20:10:39 @agent_ppo2.py:186][0m |           0.0036 |          11.2682 |           7.0377 |
[32m[20230113 20:10:39 @agent_ppo2.py:186][0m |          -0.0159 |           9.1841 |           7.0390 |
[32m[20230113 20:10:39 @agent_ppo2.py:186][0m |          -0.0170 |           7.8183 |           7.0342 |
[32m[20230113 20:10:39 @agent_ppo2.py:186][0m |          -0.0162 |           7.1033 |           7.0393 |
[32m[20230113 20:10:39 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 20:10:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 112.38
[32m[20230113 20:10:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.50
[32m[20230113 20:10:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 139.32
[32m[20230113 20:10:39 @agent_ppo2.py:144][0m Total time:      26.11 min
[32m[20230113 20:10:39 @agent_ppo2.py:146][0m 2394112 total steps have happened
[32m[20230113 20:10:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1169 --------------------------#
[32m[20230113 20:10:40 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:10:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:40 @agent_ppo2.py:186][0m |          -0.0002 |           8.9744 |           7.1887 |
[32m[20230113 20:10:40 @agent_ppo2.py:186][0m |          -0.0067 |           5.9898 |           7.1923 |
[32m[20230113 20:10:40 @agent_ppo2.py:186][0m |          -0.0102 |           5.1672 |           7.1897 |
[32m[20230113 20:10:40 @agent_ppo2.py:186][0m |          -0.0121 |           4.7464 |           7.1899 |
[32m[20230113 20:10:40 @agent_ppo2.py:186][0m |          -0.0133 |           4.4074 |           7.1956 |
[32m[20230113 20:10:40 @agent_ppo2.py:186][0m |          -0.0147 |           4.1580 |           7.1924 |
[32m[20230113 20:10:40 @agent_ppo2.py:186][0m |          -0.0159 |           3.9513 |           7.1948 |
[32m[20230113 20:10:40 @agent_ppo2.py:186][0m |          -0.0156 |           3.8067 |           7.1896 |
[32m[20230113 20:10:40 @agent_ppo2.py:186][0m |          -0.0180 |           3.6678 |           7.1996 |
[32m[20230113 20:10:40 @agent_ppo2.py:186][0m |          -0.0179 |           3.5467 |           7.2004 |
[32m[20230113 20:10:40 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:10:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.34
[32m[20230113 20:10:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.64
[32m[20230113 20:10:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 130.32
[32m[20230113 20:10:40 @agent_ppo2.py:144][0m Total time:      26.13 min
[32m[20230113 20:10:40 @agent_ppo2.py:146][0m 2396160 total steps have happened
[32m[20230113 20:10:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1170 --------------------------#
[32m[20230113 20:10:41 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:10:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:41 @agent_ppo2.py:186][0m |          -0.0004 |           8.0232 |           7.3562 |
[32m[20230113 20:10:41 @agent_ppo2.py:186][0m |          -0.0061 |           6.4045 |           7.3519 |
[32m[20230113 20:10:41 @agent_ppo2.py:186][0m |          -0.0083 |           5.7658 |           7.3486 |
[32m[20230113 20:10:41 @agent_ppo2.py:186][0m |          -0.0108 |           5.3964 |           7.3458 |
[32m[20230113 20:10:41 @agent_ppo2.py:186][0m |          -0.0113 |           5.1586 |           7.3405 |
[32m[20230113 20:10:41 @agent_ppo2.py:186][0m |          -0.0125 |           4.9747 |           7.3389 |
[32m[20230113 20:10:41 @agent_ppo2.py:186][0m |          -0.0125 |           4.7705 |           7.3418 |
[32m[20230113 20:10:41 @agent_ppo2.py:186][0m |          -0.0135 |           4.6386 |           7.3363 |
[32m[20230113 20:10:41 @agent_ppo2.py:186][0m |          -0.0138 |           4.5405 |           7.3359 |
[32m[20230113 20:10:41 @agent_ppo2.py:186][0m |          -0.0147 |           4.4095 |           7.3356 |
[32m[20230113 20:10:41 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:10:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.66
[32m[20230113 20:10:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.57
[32m[20230113 20:10:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.14
[32m[20230113 20:10:42 @agent_ppo2.py:144][0m Total time:      26.15 min
[32m[20230113 20:10:42 @agent_ppo2.py:146][0m 2398208 total steps have happened
[32m[20230113 20:10:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1171 --------------------------#
[32m[20230113 20:10:42 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:10:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:42 @agent_ppo2.py:186][0m |           0.0004 |           7.3655 |           7.1791 |
[32m[20230113 20:10:42 @agent_ppo2.py:186][0m |          -0.0019 |           5.5075 |           7.1683 |
[32m[20230113 20:10:42 @agent_ppo2.py:186][0m |          -0.0082 |           4.9284 |           7.1732 |
[32m[20230113 20:10:42 @agent_ppo2.py:186][0m |          -0.0076 |           4.5618 |           7.1643 |
[32m[20230113 20:10:42 @agent_ppo2.py:186][0m |          -0.0084 |           4.3427 |           7.1653 |
[32m[20230113 20:10:42 @agent_ppo2.py:186][0m |          -0.0078 |           4.1845 |           7.1631 |
[32m[20230113 20:10:42 @agent_ppo2.py:186][0m |          -0.0095 |           4.0455 |           7.1556 |
[32m[20230113 20:10:42 @agent_ppo2.py:186][0m |          -0.0098 |           3.9115 |           7.1581 |
[32m[20230113 20:10:42 @agent_ppo2.py:186][0m |          -0.0078 |           3.8101 |           7.1563 |
[32m[20230113 20:10:42 @agent_ppo2.py:186][0m |          -0.0072 |           3.7248 |           7.1559 |
[32m[20230113 20:10:42 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:10:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.66
[32m[20230113 20:10:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.09
[32m[20230113 20:10:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.58
[32m[20230113 20:10:43 @agent_ppo2.py:144][0m Total time:      26.17 min
[32m[20230113 20:10:43 @agent_ppo2.py:146][0m 2400256 total steps have happened
[32m[20230113 20:10:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1172 --------------------------#
[32m[20230113 20:10:43 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:10:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:43 @agent_ppo2.py:186][0m |          -0.0012 |           7.6447 |           7.2978 |
[32m[20230113 20:10:43 @agent_ppo2.py:186][0m |          -0.0055 |           5.8913 |           7.2892 |
[32m[20230113 20:10:43 @agent_ppo2.py:186][0m |          -0.0078 |           5.2552 |           7.2922 |
[32m[20230113 20:10:44 @agent_ppo2.py:186][0m |          -0.0091 |           4.8654 |           7.2884 |
[32m[20230113 20:10:44 @agent_ppo2.py:186][0m |          -0.0098 |           4.5771 |           7.2883 |
[32m[20230113 20:10:44 @agent_ppo2.py:186][0m |          -0.0106 |           4.3094 |           7.2818 |
[32m[20230113 20:10:44 @agent_ppo2.py:186][0m |          -0.0109 |           4.0587 |           7.2916 |
[32m[20230113 20:10:44 @agent_ppo2.py:186][0m |          -0.0108 |           3.7888 |           7.2896 |
[32m[20230113 20:10:44 @agent_ppo2.py:186][0m |          -0.0125 |           3.5626 |           7.2876 |
[32m[20230113 20:10:44 @agent_ppo2.py:186][0m |          -0.0124 |           3.3955 |           7.2928 |
[32m[20230113 20:10:44 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:10:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.06
[32m[20230113 20:10:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.94
[32m[20230113 20:10:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.12
[32m[20230113 20:10:44 @agent_ppo2.py:144][0m Total time:      26.19 min
[32m[20230113 20:10:44 @agent_ppo2.py:146][0m 2402304 total steps have happened
[32m[20230113 20:10:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1173 --------------------------#
[32m[20230113 20:10:45 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:10:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:45 @agent_ppo2.py:186][0m |           0.0022 |           6.4107 |           7.1693 |
[32m[20230113 20:10:45 @agent_ppo2.py:186][0m |          -0.0023 |           5.5728 |           7.1638 |
[32m[20230113 20:10:45 @agent_ppo2.py:186][0m |          -0.0065 |           5.1954 |           7.1462 |
[32m[20230113 20:10:45 @agent_ppo2.py:186][0m |          -0.0073 |           4.9610 |           7.1585 |
[32m[20230113 20:10:45 @agent_ppo2.py:186][0m |          -0.0088 |           4.7721 |           7.1528 |
[32m[20230113 20:10:45 @agent_ppo2.py:186][0m |          -0.0108 |           4.6163 |           7.1555 |
[32m[20230113 20:10:45 @agent_ppo2.py:186][0m |          -0.0113 |           4.4731 |           7.1546 |
[32m[20230113 20:10:45 @agent_ppo2.py:186][0m |          -0.0119 |           4.4049 |           7.1535 |
[32m[20230113 20:10:45 @agent_ppo2.py:186][0m |          -0.0131 |           4.2697 |           7.1582 |
[32m[20230113 20:10:45 @agent_ppo2.py:186][0m |          -0.0132 |           4.1865 |           7.1627 |
[32m[20230113 20:10:45 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:10:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.76
[32m[20230113 20:10:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.76
[32m[20230113 20:10:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 148.59
[32m[20230113 20:10:46 @agent_ppo2.py:144][0m Total time:      26.22 min
[32m[20230113 20:10:46 @agent_ppo2.py:146][0m 2404352 total steps have happened
[32m[20230113 20:10:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1174 --------------------------#
[32m[20230113 20:10:46 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:10:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:46 @agent_ppo2.py:186][0m |          -0.0018 |           5.6275 |           7.1364 |
[32m[20230113 20:10:46 @agent_ppo2.py:186][0m |          -0.0070 |           4.5269 |           7.1307 |
[32m[20230113 20:10:46 @agent_ppo2.py:186][0m |          -0.0109 |           3.9731 |           7.1186 |
[32m[20230113 20:10:46 @agent_ppo2.py:186][0m |          -0.0127 |           3.6847 |           7.1112 |
[32m[20230113 20:10:46 @agent_ppo2.py:186][0m |          -0.0112 |           3.6031 |           7.1138 |
[32m[20230113 20:10:46 @agent_ppo2.py:186][0m |          -0.0143 |           3.3896 |           7.1131 |
[32m[20230113 20:10:46 @agent_ppo2.py:186][0m |          -0.0119 |           3.2471 |           7.1049 |
[32m[20230113 20:10:46 @agent_ppo2.py:186][0m |          -0.0175 |           3.1209 |           7.1087 |
[32m[20230113 20:10:46 @agent_ppo2.py:186][0m |          -0.0113 |           3.0611 |           7.1053 |
[32m[20230113 20:10:46 @agent_ppo2.py:186][0m |          -0.0157 |           2.9972 |           7.1060 |
[32m[20230113 20:10:46 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:10:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.08
[32m[20230113 20:10:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.43
[32m[20230113 20:10:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.74
[32m[20230113 20:10:47 @agent_ppo2.py:144][0m Total time:      26.24 min
[32m[20230113 20:10:47 @agent_ppo2.py:146][0m 2406400 total steps have happened
[32m[20230113 20:10:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1175 --------------------------#
[32m[20230113 20:10:47 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:10:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:47 @agent_ppo2.py:186][0m |          -0.0003 |           6.3121 |           7.1375 |
[32m[20230113 20:10:47 @agent_ppo2.py:186][0m |          -0.0076 |           4.9061 |           7.1347 |
[32m[20230113 20:10:47 @agent_ppo2.py:186][0m |          -0.0099 |           4.3062 |           7.1281 |
[32m[20230113 20:10:47 @agent_ppo2.py:186][0m |          -0.0110 |           3.9470 |           7.1318 |
[32m[20230113 20:10:47 @agent_ppo2.py:186][0m |          -0.0093 |           3.8274 |           7.1306 |
[32m[20230113 20:10:48 @agent_ppo2.py:186][0m |          -0.0123 |           3.4964 |           7.1240 |
[32m[20230113 20:10:48 @agent_ppo2.py:186][0m |          -0.0135 |           3.3605 |           7.1239 |
[32m[20230113 20:10:48 @agent_ppo2.py:186][0m |          -0.0135 |           3.2359 |           7.1274 |
[32m[20230113 20:10:48 @agent_ppo2.py:186][0m |          -0.0133 |           3.1564 |           7.1240 |
[32m[20230113 20:10:48 @agent_ppo2.py:186][0m |          -0.0165 |           3.1195 |           7.1283 |
[32m[20230113 20:10:48 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:10:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.26
[32m[20230113 20:10:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.82
[32m[20230113 20:10:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.02
[32m[20230113 20:10:48 @agent_ppo2.py:144][0m Total time:      26.26 min
[32m[20230113 20:10:48 @agent_ppo2.py:146][0m 2408448 total steps have happened
[32m[20230113 20:10:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1176 --------------------------#
[32m[20230113 20:10:49 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:10:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:49 @agent_ppo2.py:186][0m |           0.0111 |           6.7655 |           7.1567 |
[32m[20230113 20:10:49 @agent_ppo2.py:186][0m |          -0.0061 |           4.8235 |           7.1395 |
[32m[20230113 20:10:49 @agent_ppo2.py:186][0m |          -0.0103 |           4.1509 |           7.1475 |
[32m[20230113 20:10:49 @agent_ppo2.py:186][0m |          -0.0138 |           3.7677 |           7.1388 |
[32m[20230113 20:10:49 @agent_ppo2.py:186][0m |          -0.0115 |           3.5376 |           7.1327 |
[32m[20230113 20:10:49 @agent_ppo2.py:186][0m |          -0.0172 |           3.3578 |           7.1398 |
[32m[20230113 20:10:49 @agent_ppo2.py:186][0m |          -0.0075 |           3.2188 |           7.1368 |
[32m[20230113 20:10:49 @agent_ppo2.py:186][0m |          -0.0200 |           3.0933 |           7.1365 |
[32m[20230113 20:10:49 @agent_ppo2.py:186][0m |          -0.0168 |           3.0234 |           7.1322 |
[32m[20230113 20:10:49 @agent_ppo2.py:186][0m |          -0.0109 |           2.9398 |           7.1361 |
[32m[20230113 20:10:49 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:10:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.80
[32m[20230113 20:10:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.08
[32m[20230113 20:10:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.77
[32m[20230113 20:10:49 @agent_ppo2.py:144][0m Total time:      26.28 min
[32m[20230113 20:10:49 @agent_ppo2.py:146][0m 2410496 total steps have happened
[32m[20230113 20:10:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1177 --------------------------#
[32m[20230113 20:10:50 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:10:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:50 @agent_ppo2.py:186][0m |           0.0006 |          10.6432 |           7.3117 |
[32m[20230113 20:10:50 @agent_ppo2.py:186][0m |          -0.0051 |           5.1505 |           7.2915 |
[32m[20230113 20:10:50 @agent_ppo2.py:186][0m |          -0.0114 |           4.0659 |           7.2872 |
[32m[20230113 20:10:50 @agent_ppo2.py:186][0m |          -0.0112 |           3.7323 |           7.2820 |
[32m[20230113 20:10:50 @agent_ppo2.py:186][0m |          -0.0148 |           3.3387 |           7.2706 |
[32m[20230113 20:10:50 @agent_ppo2.py:186][0m |          -0.0144 |           3.2110 |           7.2688 |
[32m[20230113 20:10:50 @agent_ppo2.py:186][0m |          -0.0151 |           3.1954 |           7.2658 |
[32m[20230113 20:10:50 @agent_ppo2.py:186][0m |          -0.0181 |           2.8658 |           7.2647 |
[32m[20230113 20:10:50 @agent_ppo2.py:186][0m |          -0.0191 |           2.7883 |           7.2621 |
[32m[20230113 20:10:50 @agent_ppo2.py:186][0m |          -0.0189 |           2.6767 |           7.2596 |
[32m[20230113 20:10:50 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 20:10:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 132.31
[32m[20230113 20:10:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.52
[32m[20230113 20:10:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.41
[32m[20230113 20:10:51 @agent_ppo2.py:144][0m Total time:      26.30 min
[32m[20230113 20:10:51 @agent_ppo2.py:146][0m 2412544 total steps have happened
[32m[20230113 20:10:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1178 --------------------------#
[32m[20230113 20:10:51 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:10:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:51 @agent_ppo2.py:186][0m |           0.0036 |           5.6719 |           7.2164 |
[32m[20230113 20:10:51 @agent_ppo2.py:186][0m |          -0.0037 |           4.8245 |           7.2082 |
[32m[20230113 20:10:51 @agent_ppo2.py:186][0m |          -0.0100 |           4.4028 |           7.2054 |
[32m[20230113 20:10:51 @agent_ppo2.py:186][0m |          -0.0042 |           4.2053 |           7.2032 |
[32m[20230113 20:10:51 @agent_ppo2.py:186][0m |          -0.0102 |           3.9849 |           7.2118 |
[32m[20230113 20:10:51 @agent_ppo2.py:186][0m |          -0.0106 |           3.8558 |           7.2069 |
[32m[20230113 20:10:51 @agent_ppo2.py:186][0m |          -0.0120 |           3.7460 |           7.2084 |
[32m[20230113 20:10:51 @agent_ppo2.py:186][0m |          -0.0134 |           3.6675 |           7.2062 |
[32m[20230113 20:10:51 @agent_ppo2.py:186][0m |          -0.0122 |           3.5529 |           7.2055 |
[32m[20230113 20:10:51 @agent_ppo2.py:186][0m |          -0.0120 |           3.4850 |           7.2122 |
[32m[20230113 20:10:51 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:10:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.74
[32m[20230113 20:10:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.69
[32m[20230113 20:10:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.72
[32m[20230113 20:10:52 @agent_ppo2.py:144][0m Total time:      26.32 min
[32m[20230113 20:10:52 @agent_ppo2.py:146][0m 2414592 total steps have happened
[32m[20230113 20:10:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1179 --------------------------#
[32m[20230113 20:10:52 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:10:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:52 @agent_ppo2.py:186][0m |          -0.0003 |           6.7613 |           7.3211 |
[32m[20230113 20:10:52 @agent_ppo2.py:186][0m |          -0.0066 |           5.6757 |           7.3143 |
[32m[20230113 20:10:52 @agent_ppo2.py:186][0m |          -0.0096 |           5.1630 |           7.3117 |
[32m[20230113 20:10:52 @agent_ppo2.py:186][0m |          -0.0101 |           4.7814 |           7.3083 |
[32m[20230113 20:10:53 @agent_ppo2.py:186][0m |          -0.0087 |           4.4515 |           7.3144 |
[32m[20230113 20:10:53 @agent_ppo2.py:186][0m |          -0.0105 |           4.2905 |           7.3076 |
[32m[20230113 20:10:53 @agent_ppo2.py:186][0m |          -0.0125 |           4.0809 |           7.3131 |
[32m[20230113 20:10:53 @agent_ppo2.py:186][0m |          -0.0119 |           3.9200 |           7.3094 |
[32m[20230113 20:10:53 @agent_ppo2.py:186][0m |          -0.0157 |           3.7863 |           7.3133 |
[32m[20230113 20:10:53 @agent_ppo2.py:186][0m |          -0.0107 |           3.7117 |           7.3154 |
[32m[20230113 20:10:53 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:10:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.14
[32m[20230113 20:10:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.46
[32m[20230113 20:10:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.01
[32m[20230113 20:10:53 @agent_ppo2.py:144][0m Total time:      26.34 min
[32m[20230113 20:10:53 @agent_ppo2.py:146][0m 2416640 total steps have happened
[32m[20230113 20:10:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1180 --------------------------#
[32m[20230113 20:10:54 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:10:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:54 @agent_ppo2.py:186][0m |          -0.0002 |           5.9769 |           7.3741 |
[32m[20230113 20:10:54 @agent_ppo2.py:186][0m |          -0.0059 |           4.9423 |           7.3572 |
[32m[20230113 20:10:54 @agent_ppo2.py:186][0m |          -0.0074 |           4.5037 |           7.3600 |
[32m[20230113 20:10:54 @agent_ppo2.py:186][0m |          -0.0088 |           4.2155 |           7.3591 |
[32m[20230113 20:10:54 @agent_ppo2.py:186][0m |          -0.0098 |           4.0958 |           7.3630 |
[32m[20230113 20:10:54 @agent_ppo2.py:186][0m |          -0.0108 |           3.8751 |           7.3576 |
[32m[20230113 20:10:54 @agent_ppo2.py:186][0m |          -0.0113 |           3.7609 |           7.3526 |
[32m[20230113 20:10:54 @agent_ppo2.py:186][0m |          -0.0123 |           3.6338 |           7.3552 |
[32m[20230113 20:10:54 @agent_ppo2.py:186][0m |          -0.0125 |           3.6064 |           7.3585 |
[32m[20230113 20:10:54 @agent_ppo2.py:186][0m |          -0.0131 |           3.4977 |           7.3541 |
[32m[20230113 20:10:54 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:10:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.25
[32m[20230113 20:10:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.67
[32m[20230113 20:10:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.73
[32m[20230113 20:10:54 @agent_ppo2.py:144][0m Total time:      26.36 min
[32m[20230113 20:10:54 @agent_ppo2.py:146][0m 2418688 total steps have happened
[32m[20230113 20:10:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1181 --------------------------#
[32m[20230113 20:10:55 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:10:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:55 @agent_ppo2.py:186][0m |           0.0041 |           5.5588 |           7.2420 |
[32m[20230113 20:10:55 @agent_ppo2.py:186][0m |          -0.0049 |           4.6796 |           7.2274 |
[32m[20230113 20:10:55 @agent_ppo2.py:186][0m |          -0.0065 |           4.3344 |           7.2282 |
[32m[20230113 20:10:55 @agent_ppo2.py:186][0m |          -0.0108 |           4.1543 |           7.2288 |
[32m[20230113 20:10:55 @agent_ppo2.py:186][0m |          -0.0106 |           3.9894 |           7.2231 |
[32m[20230113 20:10:55 @agent_ppo2.py:186][0m |          -0.0114 |           3.8908 |           7.2254 |
[32m[20230113 20:10:55 @agent_ppo2.py:186][0m |          -0.0156 |           3.7604 |           7.2290 |
[32m[20230113 20:10:55 @agent_ppo2.py:186][0m |          -0.0198 |           3.7027 |           7.2285 |
[32m[20230113 20:10:55 @agent_ppo2.py:186][0m |          -0.0140 |           3.5795 |           7.2303 |
[32m[20230113 20:10:55 @agent_ppo2.py:186][0m |          -0.0164 |           3.5838 |           7.2323 |
[32m[20230113 20:10:55 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:10:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.33
[32m[20230113 20:10:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.44
[32m[20230113 20:10:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 160.27
[32m[20230113 20:10:56 @agent_ppo2.py:144][0m Total time:      26.39 min
[32m[20230113 20:10:56 @agent_ppo2.py:146][0m 2420736 total steps have happened
[32m[20230113 20:10:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1182 --------------------------#
[32m[20230113 20:10:56 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:10:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:56 @agent_ppo2.py:186][0m |           0.0019 |           6.4305 |           7.5926 |
[32m[20230113 20:10:56 @agent_ppo2.py:186][0m |          -0.0066 |           5.4943 |           7.5810 |
[32m[20230113 20:10:56 @agent_ppo2.py:186][0m |          -0.0098 |           4.7946 |           7.5821 |
[32m[20230113 20:10:56 @agent_ppo2.py:186][0m |          -0.0117 |           4.3831 |           7.5749 |
[32m[20230113 20:10:56 @agent_ppo2.py:186][0m |          -0.0119 |           4.1474 |           7.5793 |
[32m[20230113 20:10:56 @agent_ppo2.py:186][0m |          -0.0136 |           3.9832 |           7.5768 |
[32m[20230113 20:10:56 @agent_ppo2.py:186][0m |          -0.0151 |           3.7999 |           7.5843 |
[32m[20230113 20:10:57 @agent_ppo2.py:186][0m |          -0.0157 |           3.6653 |           7.5766 |
[32m[20230113 20:10:57 @agent_ppo2.py:186][0m |          -0.0170 |           3.5703 |           7.5766 |
[32m[20230113 20:10:57 @agent_ppo2.py:186][0m |          -0.0181 |           3.4499 |           7.5836 |
[32m[20230113 20:10:57 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:10:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.95
[32m[20230113 20:10:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.89
[32m[20230113 20:10:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.77
[32m[20230113 20:10:57 @agent_ppo2.py:144][0m Total time:      26.41 min
[32m[20230113 20:10:57 @agent_ppo2.py:146][0m 2422784 total steps have happened
[32m[20230113 20:10:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1183 --------------------------#
[32m[20230113 20:10:57 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:10:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:58 @agent_ppo2.py:186][0m |          -0.0016 |           6.7692 |           7.5332 |
[32m[20230113 20:10:58 @agent_ppo2.py:186][0m |          -0.0052 |           4.6248 |           7.5242 |
[32m[20230113 20:10:58 @agent_ppo2.py:186][0m |          -0.0066 |           4.0010 |           7.5174 |
[32m[20230113 20:10:58 @agent_ppo2.py:186][0m |          -0.0107 |           3.6277 |           7.5091 |
[32m[20230113 20:10:58 @agent_ppo2.py:186][0m |          -0.0107 |           3.3643 |           7.5004 |
[32m[20230113 20:10:58 @agent_ppo2.py:186][0m |          -0.0112 |           3.1800 |           7.5035 |
[32m[20230113 20:10:58 @agent_ppo2.py:186][0m |          -0.0118 |           3.0505 |           7.5002 |
[32m[20230113 20:10:58 @agent_ppo2.py:186][0m |          -0.0128 |           2.9166 |           7.4965 |
[32m[20230113 20:10:58 @agent_ppo2.py:186][0m |          -0.0138 |           2.8471 |           7.4901 |
[32m[20230113 20:10:58 @agent_ppo2.py:186][0m |          -0.0141 |           2.7559 |           7.4922 |
[32m[20230113 20:10:58 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:10:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.86
[32m[20230113 20:10:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.82
[32m[20230113 20:10:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.97
[32m[20230113 20:10:58 @agent_ppo2.py:144][0m Total time:      26.43 min
[32m[20230113 20:10:58 @agent_ppo2.py:146][0m 2424832 total steps have happened
[32m[20230113 20:10:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1184 --------------------------#
[32m[20230113 20:10:59 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:10:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:10:59 @agent_ppo2.py:186][0m |           0.0003 |           7.5383 |           7.3960 |
[32m[20230113 20:10:59 @agent_ppo2.py:186][0m |          -0.0091 |           5.8496 |           7.3884 |
[32m[20230113 20:10:59 @agent_ppo2.py:186][0m |          -0.0078 |           5.1877 |           7.3878 |
[32m[20230113 20:10:59 @agent_ppo2.py:186][0m |          -0.0062 |           5.0089 |           7.3783 |
[32m[20230113 20:10:59 @agent_ppo2.py:186][0m |          -0.0096 |           4.4813 |           7.3787 |
[32m[20230113 20:10:59 @agent_ppo2.py:186][0m |          -0.0131 |           4.2654 |           7.3818 |
[32m[20230113 20:10:59 @agent_ppo2.py:186][0m |          -0.0115 |           4.0582 |           7.3839 |
[32m[20230113 20:10:59 @agent_ppo2.py:186][0m |          -0.0131 |           4.2100 |           7.3779 |
[32m[20230113 20:10:59 @agent_ppo2.py:186][0m |          -0.0154 |           3.8634 |           7.3621 |
[32m[20230113 20:10:59 @agent_ppo2.py:186][0m |          -0.0160 |           3.7059 |           7.3781 |
[32m[20230113 20:10:59 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:11:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.19
[32m[20230113 20:11:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.74
[32m[20230113 20:11:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.99
[32m[20230113 20:11:00 @agent_ppo2.py:144][0m Total time:      26.45 min
[32m[20230113 20:11:00 @agent_ppo2.py:146][0m 2426880 total steps have happened
[32m[20230113 20:11:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1185 --------------------------#
[32m[20230113 20:11:00 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:11:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:00 @agent_ppo2.py:186][0m |           0.0004 |           6.4396 |           7.5477 |
[32m[20230113 20:11:00 @agent_ppo2.py:186][0m |          -0.0045 |           5.4251 |           7.5369 |
[32m[20230113 20:11:00 @agent_ppo2.py:186][0m |          -0.0071 |           5.0152 |           7.5305 |
[32m[20230113 20:11:00 @agent_ppo2.py:186][0m |          -0.0083 |           4.7405 |           7.5343 |
[32m[20230113 20:11:00 @agent_ppo2.py:186][0m |          -0.0106 |           4.6084 |           7.5356 |
[32m[20230113 20:11:00 @agent_ppo2.py:186][0m |          -0.0100 |           4.4343 |           7.5344 |
[32m[20230113 20:11:00 @agent_ppo2.py:186][0m |          -0.0107 |           4.3042 |           7.5294 |
[32m[20230113 20:11:00 @agent_ppo2.py:186][0m |          -0.0123 |           4.1941 |           7.5322 |
[32m[20230113 20:11:00 @agent_ppo2.py:186][0m |          -0.0119 |           4.1078 |           7.5236 |
[32m[20230113 20:11:00 @agent_ppo2.py:186][0m |          -0.0133 |           4.0254 |           7.5274 |
[32m[20230113 20:11:00 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:11:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.59
[32m[20230113 20:11:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.71
[32m[20230113 20:11:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.73
[32m[20230113 20:11:01 @agent_ppo2.py:144][0m Total time:      26.47 min
[32m[20230113 20:11:01 @agent_ppo2.py:146][0m 2428928 total steps have happened
[32m[20230113 20:11:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1186 --------------------------#
[32m[20230113 20:11:01 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:01 @agent_ppo2.py:186][0m |           0.0004 |           6.8314 |           7.4706 |
[32m[20230113 20:11:01 @agent_ppo2.py:186][0m |          -0.0050 |           5.5807 |           7.4628 |
[32m[20230113 20:11:01 @agent_ppo2.py:186][0m |          -0.0080 |           5.0225 |           7.4592 |
[32m[20230113 20:11:02 @agent_ppo2.py:186][0m |          -0.0099 |           4.7104 |           7.4612 |
[32m[20230113 20:11:02 @agent_ppo2.py:186][0m |          -0.0112 |           4.4221 |           7.4575 |
[32m[20230113 20:11:02 @agent_ppo2.py:186][0m |          -0.0121 |           4.2150 |           7.4571 |
[32m[20230113 20:11:02 @agent_ppo2.py:186][0m |          -0.0138 |           4.0673 |           7.4596 |
[32m[20230113 20:11:02 @agent_ppo2.py:186][0m |          -0.0134 |           3.8849 |           7.4561 |
[32m[20230113 20:11:02 @agent_ppo2.py:186][0m |          -0.0149 |           3.7492 |           7.4631 |
[32m[20230113 20:11:02 @agent_ppo2.py:186][0m |          -0.0158 |           3.6210 |           7.4588 |
[32m[20230113 20:11:02 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.70
[32m[20230113 20:11:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.71
[32m[20230113 20:11:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.31
[32m[20230113 20:11:02 @agent_ppo2.py:144][0m Total time:      26.49 min
[32m[20230113 20:11:02 @agent_ppo2.py:146][0m 2430976 total steps have happened
[32m[20230113 20:11:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1187 --------------------------#
[32m[20230113 20:11:03 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:03 @agent_ppo2.py:186][0m |          -0.0011 |           7.4177 |           7.4010 |
[32m[20230113 20:11:03 @agent_ppo2.py:186][0m |          -0.0052 |           5.4512 |           7.3939 |
[32m[20230113 20:11:03 @agent_ppo2.py:186][0m |          -0.0120 |           4.8132 |           7.3892 |
[32m[20230113 20:11:03 @agent_ppo2.py:186][0m |          -0.0077 |           4.5233 |           7.3847 |
[32m[20230113 20:11:03 @agent_ppo2.py:186][0m |          -0.0189 |           4.2792 |           7.3872 |
[32m[20230113 20:11:03 @agent_ppo2.py:186][0m |          -0.0228 |           4.0843 |           7.3899 |
[32m[20230113 20:11:03 @agent_ppo2.py:186][0m |          -0.0015 |           3.9169 |           7.3885 |
[32m[20230113 20:11:03 @agent_ppo2.py:186][0m |          -0.0062 |           3.8036 |           7.3807 |
[32m[20230113 20:11:03 @agent_ppo2.py:186][0m |          -0.0227 |           3.6824 |           7.3861 |
[32m[20230113 20:11:03 @agent_ppo2.py:186][0m |          -0.0151 |           3.5903 |           7.3906 |
[32m[20230113 20:11:03 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:11:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.61
[32m[20230113 20:11:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.31
[32m[20230113 20:11:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.37
[32m[20230113 20:11:03 @agent_ppo2.py:144][0m Total time:      26.51 min
[32m[20230113 20:11:03 @agent_ppo2.py:146][0m 2433024 total steps have happened
[32m[20230113 20:11:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1188 --------------------------#
[32m[20230113 20:11:04 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:04 @agent_ppo2.py:186][0m |          -0.0028 |           5.5580 |           7.4177 |
[32m[20230113 20:11:04 @agent_ppo2.py:186][0m |          -0.0070 |           4.3875 |           7.4085 |
[32m[20230113 20:11:04 @agent_ppo2.py:186][0m |          -0.0454 |           3.9830 |           7.4023 |
[32m[20230113 20:11:04 @agent_ppo2.py:186][0m |          -0.0134 |           3.6860 |           7.3923 |
[32m[20230113 20:11:04 @agent_ppo2.py:186][0m |          -0.0233 |           3.5156 |           7.3952 |
[32m[20230113 20:11:04 @agent_ppo2.py:186][0m |          -0.0058 |           3.3488 |           7.3808 |
[32m[20230113 20:11:04 @agent_ppo2.py:186][0m |          -0.0122 |           3.2093 |           7.3919 |
[32m[20230113 20:11:04 @agent_ppo2.py:186][0m |          -0.0174 |           3.0924 |           7.3888 |
[32m[20230113 20:11:04 @agent_ppo2.py:186][0m |          -0.0157 |           3.0277 |           7.3865 |
[32m[20230113 20:11:04 @agent_ppo2.py:186][0m |          -0.0172 |           2.9545 |           7.3883 |
[32m[20230113 20:11:04 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:11:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.46
[32m[20230113 20:11:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.17
[32m[20230113 20:11:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 160.05
[32m[20230113 20:11:05 @agent_ppo2.py:144][0m Total time:      26.54 min
[32m[20230113 20:11:05 @agent_ppo2.py:146][0m 2435072 total steps have happened
[32m[20230113 20:11:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1189 --------------------------#
[32m[20230113 20:11:05 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 20:11:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:05 @agent_ppo2.py:186][0m |           0.0046 |          14.2847 |           7.4170 |
[32m[20230113 20:11:05 @agent_ppo2.py:186][0m |          -0.0043 |           9.0133 |           7.4134 |
[32m[20230113 20:11:05 @agent_ppo2.py:186][0m |          -0.0103 |           7.2785 |           7.4083 |
[32m[20230113 20:11:05 @agent_ppo2.py:186][0m |          -0.0090 |           6.3399 |           7.4099 |
[32m[20230113 20:11:05 @agent_ppo2.py:186][0m |          -0.0017 |           5.8875 |           7.4115 |
[32m[20230113 20:11:06 @agent_ppo2.py:186][0m |          -0.0088 |           5.6622 |           7.4073 |
[32m[20230113 20:11:06 @agent_ppo2.py:186][0m |          -0.0136 |           5.1905 |           7.4023 |
[32m[20230113 20:11:06 @agent_ppo2.py:186][0m |          -0.0167 |           5.0049 |           7.3984 |
[32m[20230113 20:11:06 @agent_ppo2.py:186][0m |          -0.0174 |           4.7413 |           7.3980 |
[32m[20230113 20:11:06 @agent_ppo2.py:186][0m |          -0.0124 |           4.5930 |           7.3926 |
[32m[20230113 20:11:06 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 20:11:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 135.64
[32m[20230113 20:11:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.48
[32m[20230113 20:11:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.42
[32m[20230113 20:11:06 @agent_ppo2.py:144][0m Total time:      26.56 min
[32m[20230113 20:11:06 @agent_ppo2.py:146][0m 2437120 total steps have happened
[32m[20230113 20:11:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1190 --------------------------#
[32m[20230113 20:11:07 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:07 @agent_ppo2.py:186][0m |           0.0018 |           6.3822 |           7.4914 |
[32m[20230113 20:11:07 @agent_ppo2.py:186][0m |          -0.0040 |           5.2193 |           7.4728 |
[32m[20230113 20:11:07 @agent_ppo2.py:186][0m |          -0.0074 |           4.7504 |           7.4799 |
[32m[20230113 20:11:07 @agent_ppo2.py:186][0m |          -0.0096 |           4.4425 |           7.4747 |
[32m[20230113 20:11:07 @agent_ppo2.py:186][0m |          -0.0111 |           4.2513 |           7.4720 |
[32m[20230113 20:11:07 @agent_ppo2.py:186][0m |          -0.0110 |           4.0560 |           7.4759 |
[32m[20230113 20:11:07 @agent_ppo2.py:186][0m |          -0.0119 |           3.9175 |           7.4703 |
[32m[20230113 20:11:07 @agent_ppo2.py:186][0m |          -0.0137 |           3.8091 |           7.4684 |
[32m[20230113 20:11:07 @agent_ppo2.py:186][0m |          -0.0150 |           3.7024 |           7.4734 |
[32m[20230113 20:11:07 @agent_ppo2.py:186][0m |          -0.0150 |           3.6032 |           7.4699 |
[32m[20230113 20:11:07 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:11:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.09
[32m[20230113 20:11:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.14
[32m[20230113 20:11:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.17
[32m[20230113 20:11:07 @agent_ppo2.py:144][0m Total time:      26.58 min
[32m[20230113 20:11:07 @agent_ppo2.py:146][0m 2439168 total steps have happened
[32m[20230113 20:11:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1191 --------------------------#
[32m[20230113 20:11:08 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:11:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:08 @agent_ppo2.py:186][0m |           0.0009 |           5.2867 |           7.3984 |
[32m[20230113 20:11:08 @agent_ppo2.py:186][0m |          -0.0052 |           4.3050 |           7.3893 |
[32m[20230113 20:11:08 @agent_ppo2.py:186][0m |          -0.0077 |           3.9023 |           7.3919 |
[32m[20230113 20:11:08 @agent_ppo2.py:186][0m |          -0.0080 |           3.6404 |           7.3836 |
[32m[20230113 20:11:08 @agent_ppo2.py:186][0m |          -0.0092 |           3.4228 |           7.3890 |
[32m[20230113 20:11:08 @agent_ppo2.py:186][0m |          -0.0114 |           3.2758 |           7.3834 |
[32m[20230113 20:11:08 @agent_ppo2.py:186][0m |          -0.0128 |           3.1702 |           7.3810 |
[32m[20230113 20:11:08 @agent_ppo2.py:186][0m |          -0.0148 |           3.0631 |           7.3849 |
[32m[20230113 20:11:08 @agent_ppo2.py:186][0m |          -0.0146 |           2.9874 |           7.3830 |
[32m[20230113 20:11:08 @agent_ppo2.py:186][0m |          -0.0136 |           2.9222 |           7.3796 |
[32m[20230113 20:11:08 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:11:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.66
[32m[20230113 20:11:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.02
[32m[20230113 20:11:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.93
[32m[20230113 20:11:09 @agent_ppo2.py:144][0m Total time:      26.60 min
[32m[20230113 20:11:09 @agent_ppo2.py:146][0m 2441216 total steps have happened
[32m[20230113 20:11:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1192 --------------------------#
[32m[20230113 20:11:09 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:09 @agent_ppo2.py:186][0m |           0.0082 |           7.1578 |           7.5183 |
[32m[20230113 20:11:09 @agent_ppo2.py:186][0m |          -0.0050 |           5.5068 |           7.5102 |
[32m[20230113 20:11:09 @agent_ppo2.py:186][0m |          -0.0120 |           5.0564 |           7.5124 |
[32m[20230113 20:11:09 @agent_ppo2.py:186][0m |          -0.0140 |           4.8692 |           7.5158 |
[32m[20230113 20:11:09 @agent_ppo2.py:186][0m |          -0.0103 |           4.6632 |           7.5154 |
[32m[20230113 20:11:09 @agent_ppo2.py:186][0m |          -0.0068 |           4.5650 |           7.5167 |
[32m[20230113 20:11:09 @agent_ppo2.py:186][0m |           0.0025 |           4.9846 |           7.5155 |
[32m[20230113 20:11:09 @agent_ppo2.py:186][0m |          -0.0021 |           4.5762 |           7.5185 |
[32m[20230113 20:11:10 @agent_ppo2.py:186][0m |          -0.0112 |           4.3039 |           7.5157 |
[32m[20230113 20:11:10 @agent_ppo2.py:186][0m |          -0.0104 |           4.0536 |           7.5163 |
[32m[20230113 20:11:10 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.20
[32m[20230113 20:11:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.27
[32m[20230113 20:11:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.15
[32m[20230113 20:11:10 @agent_ppo2.py:144][0m Total time:      26.62 min
[32m[20230113 20:11:10 @agent_ppo2.py:146][0m 2443264 total steps have happened
[32m[20230113 20:11:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1193 --------------------------#
[32m[20230113 20:11:10 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:11:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:10 @agent_ppo2.py:186][0m |           0.0025 |           6.9258 |           7.4465 |
[32m[20230113 20:11:10 @agent_ppo2.py:186][0m |           0.0029 |           5.3246 |           7.4251 |
[32m[20230113 20:11:11 @agent_ppo2.py:186][0m |          -0.0030 |           4.6749 |           7.4279 |
[32m[20230113 20:11:11 @agent_ppo2.py:186][0m |          -0.0138 |           4.1570 |           7.4212 |
[32m[20230113 20:11:11 @agent_ppo2.py:186][0m |          -0.0088 |           3.8679 |           7.4130 |
[32m[20230113 20:11:11 @agent_ppo2.py:186][0m |          -0.0145 |           3.6047 |           7.4142 |
[32m[20230113 20:11:11 @agent_ppo2.py:186][0m |          -0.0151 |           3.4141 |           7.4054 |
[32m[20230113 20:11:11 @agent_ppo2.py:186][0m |          -0.0130 |           3.2461 |           7.4073 |
[32m[20230113 20:11:11 @agent_ppo2.py:186][0m |          -0.0092 |           3.1535 |           7.4075 |
[32m[20230113 20:11:11 @agent_ppo2.py:186][0m |          -0.0136 |           2.9944 |           7.4039 |
[32m[20230113 20:11:11 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:11:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.61
[32m[20230113 20:11:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.86
[32m[20230113 20:11:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.64
[32m[20230113 20:11:11 @agent_ppo2.py:144][0m Total time:      26.64 min
[32m[20230113 20:11:11 @agent_ppo2.py:146][0m 2445312 total steps have happened
[32m[20230113 20:11:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1194 --------------------------#
[32m[20230113 20:11:12 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:12 @agent_ppo2.py:186][0m |          -0.0045 |           5.8402 |           7.3902 |
[32m[20230113 20:11:12 @agent_ppo2.py:186][0m |          -0.0060 |           4.8196 |           7.3878 |
[32m[20230113 20:11:12 @agent_ppo2.py:186][0m |          -0.0060 |           4.4231 |           7.3863 |
[32m[20230113 20:11:12 @agent_ppo2.py:186][0m |          -0.0073 |           4.1713 |           7.3871 |
[32m[20230113 20:11:12 @agent_ppo2.py:186][0m |          -0.0136 |           3.9544 |           7.3844 |
[32m[20230113 20:11:12 @agent_ppo2.py:186][0m |          -0.0138 |           3.8699 |           7.3764 |
[32m[20230113 20:11:12 @agent_ppo2.py:186][0m |          -0.0094 |           3.8465 |           7.3776 |
[32m[20230113 20:11:12 @agent_ppo2.py:186][0m |          -0.0143 |           3.6146 |           7.3787 |
[32m[20230113 20:11:12 @agent_ppo2.py:186][0m |          -0.0215 |           3.5577 |           7.3810 |
[32m[20230113 20:11:12 @agent_ppo2.py:186][0m |          -0.0098 |           3.5386 |           7.3717 |
[32m[20230113 20:11:12 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.67
[32m[20230113 20:11:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.31
[32m[20230113 20:11:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 150.34
[32m[20230113 20:11:12 @agent_ppo2.py:144][0m Total time:      26.66 min
[32m[20230113 20:11:12 @agent_ppo2.py:146][0m 2447360 total steps have happened
[32m[20230113 20:11:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1195 --------------------------#
[32m[20230113 20:11:13 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:13 @agent_ppo2.py:186][0m |           0.0012 |           4.6592 |           7.7469 |
[32m[20230113 20:11:13 @agent_ppo2.py:186][0m |          -0.0031 |           4.0535 |           7.7344 |
[32m[20230113 20:11:13 @agent_ppo2.py:186][0m |          -0.0059 |           3.7718 |           7.7188 |
[32m[20230113 20:11:13 @agent_ppo2.py:186][0m |          -0.0074 |           3.6346 |           7.7197 |
[32m[20230113 20:11:13 @agent_ppo2.py:186][0m |          -0.0095 |           3.4352 |           7.7162 |
[32m[20230113 20:11:13 @agent_ppo2.py:186][0m |          -0.0096 |           3.3187 |           7.7124 |
[32m[20230113 20:11:13 @agent_ppo2.py:186][0m |          -0.0099 |           3.2213 |           7.7116 |
[32m[20230113 20:11:13 @agent_ppo2.py:186][0m |          -0.0114 |           3.1414 |           7.7062 |
[32m[20230113 20:11:13 @agent_ppo2.py:186][0m |          -0.0125 |           3.0686 |           7.7050 |
[32m[20230113 20:11:13 @agent_ppo2.py:186][0m |          -0.0130 |           2.9969 |           7.6976 |
[32m[20230113 20:11:13 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.83
[32m[20230113 20:11:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.20
[32m[20230113 20:11:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.41
[32m[20230113 20:11:14 @agent_ppo2.py:144][0m Total time:      26.69 min
[32m[20230113 20:11:14 @agent_ppo2.py:146][0m 2449408 total steps have happened
[32m[20230113 20:11:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1196 --------------------------#
[32m[20230113 20:11:14 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:14 @agent_ppo2.py:186][0m |           0.0012 |           6.2777 |           7.3341 |
[32m[20230113 20:11:14 @agent_ppo2.py:186][0m |          -0.0096 |           4.8569 |           7.3286 |
[32m[20230113 20:11:14 @agent_ppo2.py:186][0m |          -0.0127 |           4.3997 |           7.3255 |
[32m[20230113 20:11:14 @agent_ppo2.py:186][0m |          -0.0139 |           4.1550 |           7.3173 |
[32m[20230113 20:11:14 @agent_ppo2.py:186][0m |          -0.0104 |           3.9578 |           7.3242 |
[32m[20230113 20:11:14 @agent_ppo2.py:186][0m |          -0.0205 |           3.7563 |           7.3187 |
[32m[20230113 20:11:14 @agent_ppo2.py:186][0m |          -0.0139 |           3.5650 |           7.3121 |
[32m[20230113 20:11:14 @agent_ppo2.py:186][0m |          -0.0221 |           3.4497 |           7.3088 |
[32m[20230113 20:11:15 @agent_ppo2.py:186][0m |          -0.0171 |           3.2697 |           7.3142 |
[32m[20230113 20:11:15 @agent_ppo2.py:186][0m |          -0.0225 |           3.1832 |           7.3099 |
[32m[20230113 20:11:15 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.56
[32m[20230113 20:11:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.56
[32m[20230113 20:11:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.38
[32m[20230113 20:11:15 @agent_ppo2.py:144][0m Total time:      26.71 min
[32m[20230113 20:11:15 @agent_ppo2.py:146][0m 2451456 total steps have happened
[32m[20230113 20:11:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1197 --------------------------#
[32m[20230113 20:11:15 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:11:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:15 @agent_ppo2.py:186][0m |          -0.0009 |           6.9187 |           7.4334 |
[32m[20230113 20:11:15 @agent_ppo2.py:186][0m |          -0.0052 |           5.6066 |           7.4255 |
[32m[20230113 20:11:16 @agent_ppo2.py:186][0m |          -0.0074 |           4.9874 |           7.4215 |
[32m[20230113 20:11:16 @agent_ppo2.py:186][0m |          -0.0054 |           4.6567 |           7.4204 |
[32m[20230113 20:11:16 @agent_ppo2.py:186][0m |          -0.0101 |           4.3697 |           7.4128 |
[32m[20230113 20:11:16 @agent_ppo2.py:186][0m |          -0.0114 |           4.1548 |           7.4146 |
[32m[20230113 20:11:16 @agent_ppo2.py:186][0m |          -0.0134 |           3.9820 |           7.4292 |
[32m[20230113 20:11:16 @agent_ppo2.py:186][0m |          -0.0109 |           3.8872 |           7.4209 |
[32m[20230113 20:11:16 @agent_ppo2.py:186][0m |          -0.0139 |           3.8107 |           7.4186 |
[32m[20230113 20:11:16 @agent_ppo2.py:186][0m |          -0.0105 |           3.8081 |           7.4187 |
[32m[20230113 20:11:16 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:11:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.44
[32m[20230113 20:11:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.56
[32m[20230113 20:11:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 85.45
[32m[20230113 20:11:16 @agent_ppo2.py:144][0m Total time:      26.73 min
[32m[20230113 20:11:16 @agent_ppo2.py:146][0m 2453504 total steps have happened
[32m[20230113 20:11:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1198 --------------------------#
[32m[20230113 20:11:17 @agent_ppo2.py:128][0m Sampling time: 0.35 s by 1 slaves
[32m[20230113 20:11:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:17 @agent_ppo2.py:186][0m |          -0.0007 |          13.2232 |           7.3913 |
[32m[20230113 20:11:17 @agent_ppo2.py:186][0m |          -0.0028 |           7.2404 |           7.3775 |
[32m[20230113 20:11:17 @agent_ppo2.py:186][0m |          -0.0056 |           5.5786 |           7.3874 |
[32m[20230113 20:11:17 @agent_ppo2.py:186][0m |          -0.0052 |           4.8575 |           7.3854 |
[32m[20230113 20:11:17 @agent_ppo2.py:186][0m |          -0.0095 |           4.3659 |           7.3810 |
[32m[20230113 20:11:17 @agent_ppo2.py:186][0m |          -0.0097 |           4.0849 |           7.3853 |
[32m[20230113 20:11:17 @agent_ppo2.py:186][0m |          -0.0096 |           3.7596 |           7.3796 |
[32m[20230113 20:11:17 @agent_ppo2.py:186][0m |          -0.0105 |           3.6046 |           7.3768 |
[32m[20230113 20:11:17 @agent_ppo2.py:186][0m |          -0.0103 |           3.3321 |           7.3765 |
[32m[20230113 20:11:17 @agent_ppo2.py:186][0m |          -0.0117 |           3.2550 |           7.3814 |
[32m[20230113 20:11:17 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:11:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 131.82
[32m[20230113 20:11:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.40
[32m[20230113 20:11:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.15
[32m[20230113 20:11:17 @agent_ppo2.py:144][0m Total time:      26.75 min
[32m[20230113 20:11:17 @agent_ppo2.py:146][0m 2455552 total steps have happened
[32m[20230113 20:11:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1199 --------------------------#
[32m[20230113 20:11:18 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:11:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:18 @agent_ppo2.py:186][0m |           0.0059 |           6.6826 |           7.3115 |
[32m[20230113 20:11:18 @agent_ppo2.py:186][0m |          -0.0062 |           5.0912 |           7.2852 |
[32m[20230113 20:11:18 @agent_ppo2.py:186][0m |          -0.0181 |           4.6144 |           7.2920 |
[32m[20230113 20:11:18 @agent_ppo2.py:186][0m |          -0.0108 |           4.3068 |           7.2894 |
[32m[20230113 20:11:18 @agent_ppo2.py:186][0m |          -0.0161 |           4.0804 |           7.2918 |
[32m[20230113 20:11:18 @agent_ppo2.py:186][0m |          -0.0146 |           3.9733 |           7.2958 |
[32m[20230113 20:11:18 @agent_ppo2.py:186][0m |          -0.0180 |           3.8429 |           7.2926 |
[32m[20230113 20:11:18 @agent_ppo2.py:186][0m |          -0.0172 |           3.7804 |           7.2921 |
[32m[20230113 20:11:18 @agent_ppo2.py:186][0m |          -0.0196 |           3.6327 |           7.2937 |
[32m[20230113 20:11:18 @agent_ppo2.py:186][0m |          -0.0173 |           3.5943 |           7.2936 |
[32m[20230113 20:11:18 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:11:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.05
[32m[20230113 20:11:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.23
[32m[20230113 20:11:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.91
[32m[20230113 20:11:19 @agent_ppo2.py:144][0m Total time:      26.77 min
[32m[20230113 20:11:19 @agent_ppo2.py:146][0m 2457600 total steps have happened
[32m[20230113 20:11:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1200 --------------------------#
[32m[20230113 20:11:19 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:19 @agent_ppo2.py:186][0m |           0.0013 |           5.3340 |           7.5160 |
[32m[20230113 20:11:19 @agent_ppo2.py:186][0m |          -0.0045 |           4.0027 |           7.5006 |
[32m[20230113 20:11:19 @agent_ppo2.py:186][0m |          -0.0079 |           3.5253 |           7.4913 |
[32m[20230113 20:11:19 @agent_ppo2.py:186][0m |          -0.0081 |           3.2201 |           7.4837 |
[32m[20230113 20:11:19 @agent_ppo2.py:186][0m |          -0.0093 |           2.9622 |           7.4816 |
[32m[20230113 20:11:19 @agent_ppo2.py:186][0m |          -0.0108 |           2.8091 |           7.4799 |
[32m[20230113 20:11:20 @agent_ppo2.py:186][0m |          -0.0107 |           2.6882 |           7.4849 |
[32m[20230113 20:11:20 @agent_ppo2.py:186][0m |          -0.0121 |           2.5742 |           7.4777 |
[32m[20230113 20:11:20 @agent_ppo2.py:186][0m |          -0.0121 |           2.4742 |           7.4764 |
[32m[20230113 20:11:20 @agent_ppo2.py:186][0m |          -0.0125 |           2.4007 |           7.4780 |
[32m[20230113 20:11:20 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.07
[32m[20230113 20:11:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.47
[32m[20230113 20:11:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.56
[32m[20230113 20:11:20 @agent_ppo2.py:144][0m Total time:      26.79 min
[32m[20230113 20:11:20 @agent_ppo2.py:146][0m 2459648 total steps have happened
[32m[20230113 20:11:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1201 --------------------------#
[32m[20230113 20:11:20 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:21 @agent_ppo2.py:186][0m |          -0.0035 |           7.0598 |           7.3335 |
[32m[20230113 20:11:21 @agent_ppo2.py:186][0m |           0.0019 |           5.6594 |           7.3303 |
[32m[20230113 20:11:21 @agent_ppo2.py:186][0m |          -0.0083 |           5.0247 |           7.3206 |
[32m[20230113 20:11:21 @agent_ppo2.py:186][0m |          -0.0119 |           4.6945 |           7.3237 |
[32m[20230113 20:11:21 @agent_ppo2.py:186][0m |          -0.0220 |           4.4657 |           7.3230 |
[32m[20230113 20:11:21 @agent_ppo2.py:186][0m |          -0.0097 |           4.3016 |           7.3183 |
[32m[20230113 20:11:21 @agent_ppo2.py:186][0m |          -0.0082 |           4.1116 |           7.3157 |
[32m[20230113 20:11:21 @agent_ppo2.py:186][0m |           0.0075 |           4.5812 |           7.3177 |
[32m[20230113 20:11:21 @agent_ppo2.py:186][0m |          -0.0191 |           4.0457 |           7.3119 |
[32m[20230113 20:11:21 @agent_ppo2.py:186][0m |          -0.0163 |           3.8398 |           7.3090 |
[32m[20230113 20:11:21 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.73
[32m[20230113 20:11:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.11
[32m[20230113 20:11:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.49
[32m[20230113 20:11:21 @agent_ppo2.py:144][0m Total time:      26.81 min
[32m[20230113 20:11:21 @agent_ppo2.py:146][0m 2461696 total steps have happened
[32m[20230113 20:11:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1202 --------------------------#
[32m[20230113 20:11:22 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:22 @agent_ppo2.py:186][0m |          -0.0007 |           8.1685 |           7.3031 |
[32m[20230113 20:11:22 @agent_ppo2.py:186][0m |          -0.0070 |           6.0012 |           7.2888 |
[32m[20230113 20:11:22 @agent_ppo2.py:186][0m |          -0.0093 |           5.1878 |           7.2877 |
[32m[20230113 20:11:22 @agent_ppo2.py:186][0m |          -0.0103 |           4.8465 |           7.2758 |
[32m[20230113 20:11:22 @agent_ppo2.py:186][0m |          -0.0107 |           4.4489 |           7.2868 |
[32m[20230113 20:11:22 @agent_ppo2.py:186][0m |          -0.0122 |           4.2281 |           7.2744 |
[32m[20230113 20:11:22 @agent_ppo2.py:186][0m |          -0.0121 |           4.0600 |           7.2748 |
[32m[20230113 20:11:22 @agent_ppo2.py:186][0m |          -0.0145 |           3.8539 |           7.2730 |
[32m[20230113 20:11:22 @agent_ppo2.py:186][0m |          -0.0126 |           3.7377 |           7.2738 |
[32m[20230113 20:11:22 @agent_ppo2.py:186][0m |          -0.0143 |           3.6184 |           7.2669 |
[32m[20230113 20:11:22 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.71
[32m[20230113 20:11:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.31
[32m[20230113 20:11:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.65
[32m[20230113 20:11:23 @agent_ppo2.py:144][0m Total time:      26.83 min
[32m[20230113 20:11:23 @agent_ppo2.py:146][0m 2463744 total steps have happened
[32m[20230113 20:11:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1203 --------------------------#
[32m[20230113 20:11:23 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:23 @agent_ppo2.py:186][0m |          -0.0019 |           5.5695 |           7.3375 |
[32m[20230113 20:11:23 @agent_ppo2.py:186][0m |          -0.0094 |           4.7117 |           7.3408 |
[32m[20230113 20:11:23 @agent_ppo2.py:186][0m |          -0.0108 |           4.3284 |           7.3405 |
[32m[20230113 20:11:23 @agent_ppo2.py:186][0m |          -0.0119 |           4.1530 |           7.3349 |
[32m[20230113 20:11:23 @agent_ppo2.py:186][0m |          -0.0137 |           3.9967 |           7.3339 |
[32m[20230113 20:11:23 @agent_ppo2.py:186][0m |          -0.0129 |           3.9176 |           7.3304 |
[32m[20230113 20:11:23 @agent_ppo2.py:186][0m |          -0.0157 |           3.8238 |           7.3384 |
[32m[20230113 20:11:23 @agent_ppo2.py:186][0m |          -0.0160 |           3.6831 |           7.3239 |
[32m[20230113 20:11:23 @agent_ppo2.py:186][0m |          -0.0173 |           3.6937 |           7.3403 |
[32m[20230113 20:11:23 @agent_ppo2.py:186][0m |          -0.0161 |           3.6161 |           7.3302 |
[32m[20230113 20:11:23 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.74
[32m[20230113 20:11:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.09
[32m[20230113 20:11:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.88
[32m[20230113 20:11:24 @agent_ppo2.py:144][0m Total time:      26.86 min
[32m[20230113 20:11:24 @agent_ppo2.py:146][0m 2465792 total steps have happened
[32m[20230113 20:11:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1204 --------------------------#
[32m[20230113 20:11:24 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:24 @agent_ppo2.py:186][0m |           0.0015 |           5.5413 |           7.5515 |
[32m[20230113 20:11:24 @agent_ppo2.py:186][0m |          -0.0051 |           4.6075 |           7.5367 |
[32m[20230113 20:11:24 @agent_ppo2.py:186][0m |          -0.0083 |           4.2045 |           7.5277 |
[32m[20230113 20:11:25 @agent_ppo2.py:186][0m |          -0.0094 |           3.9192 |           7.5315 |
[32m[20230113 20:11:25 @agent_ppo2.py:186][0m |          -0.0112 |           3.7544 |           7.5296 |
[32m[20230113 20:11:25 @agent_ppo2.py:186][0m |          -0.0120 |           3.6036 |           7.5250 |
[32m[20230113 20:11:25 @agent_ppo2.py:186][0m |          -0.0126 |           3.4600 |           7.5327 |
[32m[20230113 20:11:25 @agent_ppo2.py:186][0m |          -0.0131 |           3.3668 |           7.5256 |
[32m[20230113 20:11:25 @agent_ppo2.py:186][0m |          -0.0136 |           3.2684 |           7.5284 |
[32m[20230113 20:11:25 @agent_ppo2.py:186][0m |          -0.0133 |           3.1963 |           7.5296 |
[32m[20230113 20:11:25 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.19
[32m[20230113 20:11:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.12
[32m[20230113 20:11:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.00
[32m[20230113 20:11:25 @agent_ppo2.py:144][0m Total time:      26.88 min
[32m[20230113 20:11:25 @agent_ppo2.py:146][0m 2467840 total steps have happened
[32m[20230113 20:11:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1205 --------------------------#
[32m[20230113 20:11:26 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:11:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:26 @agent_ppo2.py:186][0m |          -0.0015 |           5.9094 |           7.4618 |
[32m[20230113 20:11:26 @agent_ppo2.py:186][0m |          -0.0062 |           4.9109 |           7.4551 |
[32m[20230113 20:11:26 @agent_ppo2.py:186][0m |          -0.0048 |           4.4551 |           7.4466 |
[32m[20230113 20:11:26 @agent_ppo2.py:186][0m |          -0.0140 |           4.2029 |           7.4476 |
[32m[20230113 20:11:26 @agent_ppo2.py:186][0m |          -0.0031 |           4.0228 |           7.4501 |
[32m[20230113 20:11:26 @agent_ppo2.py:186][0m |          -0.0170 |           3.8447 |           7.4572 |
[32m[20230113 20:11:26 @agent_ppo2.py:186][0m |          -0.0144 |           3.7479 |           7.4553 |
[32m[20230113 20:11:26 @agent_ppo2.py:186][0m |          -0.0173 |           3.6308 |           7.4541 |
[32m[20230113 20:11:26 @agent_ppo2.py:186][0m |          -0.0280 |           3.6162 |           7.4573 |
[32m[20230113 20:11:26 @agent_ppo2.py:186][0m |          -0.0140 |           3.5375 |           7.4548 |
[32m[20230113 20:11:26 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:11:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.75
[32m[20230113 20:11:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.50
[32m[20230113 20:11:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.16
[32m[20230113 20:11:26 @agent_ppo2.py:144][0m Total time:      26.90 min
[32m[20230113 20:11:26 @agent_ppo2.py:146][0m 2469888 total steps have happened
[32m[20230113 20:11:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1206 --------------------------#
[32m[20230113 20:11:27 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:27 @agent_ppo2.py:186][0m |           0.0015 |           5.8196 |           7.6930 |
[32m[20230113 20:11:27 @agent_ppo2.py:186][0m |          -0.0034 |           4.7571 |           7.6930 |
[32m[20230113 20:11:27 @agent_ppo2.py:186][0m |          -0.0080 |           4.3657 |           7.6877 |
[32m[20230113 20:11:27 @agent_ppo2.py:186][0m |          -0.0098 |           4.0434 |           7.6849 |
[32m[20230113 20:11:27 @agent_ppo2.py:186][0m |          -0.0112 |           3.8752 |           7.6787 |
[32m[20230113 20:11:27 @agent_ppo2.py:186][0m |          -0.0119 |           3.7001 |           7.6815 |
[32m[20230113 20:11:27 @agent_ppo2.py:186][0m |          -0.0126 |           3.6149 |           7.6806 |
[32m[20230113 20:11:27 @agent_ppo2.py:186][0m |          -0.0134 |           3.5167 |           7.6788 |
[32m[20230113 20:11:27 @agent_ppo2.py:186][0m |          -0.0143 |           3.4034 |           7.6782 |
[32m[20230113 20:11:27 @agent_ppo2.py:186][0m |          -0.0153 |           3.3471 |           7.6764 |
[32m[20230113 20:11:27 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.42
[32m[20230113 20:11:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.55
[32m[20230113 20:11:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.42
[32m[20230113 20:11:28 @agent_ppo2.py:144][0m Total time:      26.92 min
[32m[20230113 20:11:28 @agent_ppo2.py:146][0m 2471936 total steps have happened
[32m[20230113 20:11:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1207 --------------------------#
[32m[20230113 20:11:28 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:11:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:28 @agent_ppo2.py:186][0m |          -0.0009 |           6.7123 |           7.4788 |
[32m[20230113 20:11:28 @agent_ppo2.py:186][0m |          -0.0069 |           5.1850 |           7.4582 |
[32m[20230113 20:11:28 @agent_ppo2.py:186][0m |          -0.0059 |           4.5320 |           7.4660 |
[32m[20230113 20:11:28 @agent_ppo2.py:186][0m |          -0.0061 |           4.2223 |           7.4627 |
[32m[20230113 20:11:28 @agent_ppo2.py:186][0m |          -0.0117 |           3.9341 |           7.4565 |
[32m[20230113 20:11:29 @agent_ppo2.py:186][0m |          -0.0102 |           3.7336 |           7.4616 |
[32m[20230113 20:11:29 @agent_ppo2.py:186][0m |          -0.0097 |           3.4636 |           7.4580 |
[32m[20230113 20:11:29 @agent_ppo2.py:186][0m |          -0.0168 |           3.3056 |           7.4635 |
[32m[20230113 20:11:29 @agent_ppo2.py:186][0m |          -0.0167 |           3.1703 |           7.4658 |
[32m[20230113 20:11:29 @agent_ppo2.py:186][0m |          -0.0124 |           3.0743 |           7.4685 |
[32m[20230113 20:11:29 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:11:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.09
[32m[20230113 20:11:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.18
[32m[20230113 20:11:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.32
[32m[20230113 20:11:29 @agent_ppo2.py:144][0m Total time:      26.94 min
[32m[20230113 20:11:29 @agent_ppo2.py:146][0m 2473984 total steps have happened
[32m[20230113 20:11:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1208 --------------------------#
[32m[20230113 20:11:30 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:30 @agent_ppo2.py:186][0m |          -0.0005 |           6.5657 |           7.4992 |
[32m[20230113 20:11:30 @agent_ppo2.py:186][0m |          -0.0062 |           5.1952 |           7.4778 |
[32m[20230113 20:11:30 @agent_ppo2.py:186][0m |          -0.0103 |           4.7229 |           7.4887 |
[32m[20230113 20:11:30 @agent_ppo2.py:186][0m |          -0.0091 |           4.3598 |           7.4759 |
[32m[20230113 20:11:30 @agent_ppo2.py:186][0m |          -0.0080 |           4.1304 |           7.4900 |
[32m[20230113 20:11:30 @agent_ppo2.py:186][0m |          -0.0117 |           3.9113 |           7.4855 |
[32m[20230113 20:11:30 @agent_ppo2.py:186][0m |          -0.0115 |           3.7130 |           7.4864 |
[32m[20230113 20:11:30 @agent_ppo2.py:186][0m |          -0.0145 |           3.5730 |           7.4793 |
[32m[20230113 20:11:30 @agent_ppo2.py:186][0m |          -0.0129 |           3.4662 |           7.4826 |
[32m[20230113 20:11:30 @agent_ppo2.py:186][0m |          -0.0157 |           3.3703 |           7.4848 |
[32m[20230113 20:11:30 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.75
[32m[20230113 20:11:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.36
[32m[20230113 20:11:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.92
[32m[20230113 20:11:30 @agent_ppo2.py:144][0m Total time:      26.96 min
[32m[20230113 20:11:30 @agent_ppo2.py:146][0m 2476032 total steps have happened
[32m[20230113 20:11:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1209 --------------------------#
[32m[20230113 20:11:31 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 20:11:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:31 @agent_ppo2.py:186][0m |          -0.0020 |          13.8266 |           7.6011 |
[32m[20230113 20:11:31 @agent_ppo2.py:186][0m |          -0.0070 |           5.5800 |           7.5723 |
[32m[20230113 20:11:31 @agent_ppo2.py:186][0m |          -0.0118 |           4.6858 |           7.5759 |
[32m[20230113 20:11:31 @agent_ppo2.py:186][0m |          -0.0141 |           4.3504 |           7.5735 |
[32m[20230113 20:11:31 @agent_ppo2.py:186][0m |          -0.0148 |           4.1171 |           7.5695 |
[32m[20230113 20:11:31 @agent_ppo2.py:186][0m |          -0.0159 |           3.9650 |           7.5627 |
[32m[20230113 20:11:31 @agent_ppo2.py:186][0m |          -0.0163 |           3.8327 |           7.5623 |
[32m[20230113 20:11:31 @agent_ppo2.py:186][0m |          -0.0177 |           3.7220 |           7.5557 |
[32m[20230113 20:11:31 @agent_ppo2.py:186][0m |          -0.0175 |           3.6480 |           7.5571 |
[32m[20230113 20:11:31 @agent_ppo2.py:186][0m |          -0.0178 |           3.5457 |           7.5497 |
[32m[20230113 20:11:31 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 20:11:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 158.77
[32m[20230113 20:11:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.75
[32m[20230113 20:11:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.47
[32m[20230113 20:11:32 @agent_ppo2.py:144][0m Total time:      26.98 min
[32m[20230113 20:11:32 @agent_ppo2.py:146][0m 2478080 total steps have happened
[32m[20230113 20:11:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1210 --------------------------#
[32m[20230113 20:11:32 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:32 @agent_ppo2.py:186][0m |          -0.0024 |           6.3541 |           7.4316 |
[32m[20230113 20:11:32 @agent_ppo2.py:186][0m |          -0.0072 |           5.1515 |           7.4368 |
[32m[20230113 20:11:32 @agent_ppo2.py:186][0m |          -0.0093 |           4.7114 |           7.4333 |
[32m[20230113 20:11:32 @agent_ppo2.py:186][0m |          -0.0103 |           4.3436 |           7.4349 |
[32m[20230113 20:11:32 @agent_ppo2.py:186][0m |          -0.0103 |           4.1645 |           7.4284 |
[32m[20230113 20:11:32 @agent_ppo2.py:186][0m |          -0.0142 |           3.9675 |           7.4314 |
[32m[20230113 20:11:32 @agent_ppo2.py:186][0m |          -0.0169 |           3.8278 |           7.4346 |
[32m[20230113 20:11:32 @agent_ppo2.py:186][0m |          -0.0125 |           3.6790 |           7.4311 |
[32m[20230113 20:11:32 @agent_ppo2.py:186][0m |          -0.0172 |           3.5532 |           7.4293 |
[32m[20230113 20:11:32 @agent_ppo2.py:186][0m |          -0.0158 |           3.4428 |           7.4284 |
[32m[20230113 20:11:32 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.33
[32m[20230113 20:11:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.09
[32m[20230113 20:11:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.44
[32m[20230113 20:11:33 @agent_ppo2.py:144][0m Total time:      27.00 min
[32m[20230113 20:11:33 @agent_ppo2.py:146][0m 2480128 total steps have happened
[32m[20230113 20:11:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1211 --------------------------#
[32m[20230113 20:11:33 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:33 @agent_ppo2.py:186][0m |           0.0050 |           7.0942 |           7.5101 |
[32m[20230113 20:11:33 @agent_ppo2.py:186][0m |          -0.0022 |           6.0074 |           7.5053 |
[32m[20230113 20:11:33 @agent_ppo2.py:186][0m |          -0.0018 |           5.5042 |           7.4993 |
[32m[20230113 20:11:33 @agent_ppo2.py:186][0m |          -0.0102 |           5.2766 |           7.4931 |
[32m[20230113 20:11:33 @agent_ppo2.py:186][0m |          -0.0109 |           4.9880 |           7.4936 |
[32m[20230113 20:11:34 @agent_ppo2.py:186][0m |          -0.0080 |           4.7935 |           7.4877 |
[32m[20230113 20:11:34 @agent_ppo2.py:186][0m |           0.0020 |           5.2803 |           7.4936 |
[32m[20230113 20:11:34 @agent_ppo2.py:186][0m |          -0.0004 |           5.4086 |           7.4865 |
[32m[20230113 20:11:34 @agent_ppo2.py:186][0m |          -0.0105 |           4.6171 |           7.4862 |
[32m[20230113 20:11:34 @agent_ppo2.py:186][0m |          -0.0137 |           4.3556 |           7.4854 |
[32m[20230113 20:11:34 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.79
[32m[20230113 20:11:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.97
[32m[20230113 20:11:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.37
[32m[20230113 20:11:34 @agent_ppo2.py:144][0m Total time:      27.03 min
[32m[20230113 20:11:34 @agent_ppo2.py:146][0m 2482176 total steps have happened
[32m[20230113 20:11:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1212 --------------------------#
[32m[20230113 20:11:35 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:35 @agent_ppo2.py:186][0m |           0.0023 |           5.5841 |           7.6313 |
[32m[20230113 20:11:35 @agent_ppo2.py:186][0m |          -0.0020 |           4.5492 |           7.6393 |
[32m[20230113 20:11:35 @agent_ppo2.py:186][0m |          -0.0051 |           4.1142 |           7.6289 |
[32m[20230113 20:11:35 @agent_ppo2.py:186][0m |          -0.0061 |           3.8933 |           7.6341 |
[32m[20230113 20:11:35 @agent_ppo2.py:186][0m |          -0.0083 |           3.6596 |           7.6297 |
[32m[20230113 20:11:35 @agent_ppo2.py:186][0m |          -0.0083 |           3.5831 |           7.6279 |
[32m[20230113 20:11:35 @agent_ppo2.py:186][0m |          -0.0093 |           3.4147 |           7.6274 |
[32m[20230113 20:11:35 @agent_ppo2.py:186][0m |          -0.0104 |           3.3215 |           7.6229 |
[32m[20230113 20:11:35 @agent_ppo2.py:186][0m |          -0.0112 |           3.2426 |           7.6282 |
[32m[20230113 20:11:35 @agent_ppo2.py:186][0m |          -0.0117 |           3.1692 |           7.6244 |
[32m[20230113 20:11:35 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.54
[32m[20230113 20:11:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.71
[32m[20230113 20:11:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.20
[32m[20230113 20:11:35 @agent_ppo2.py:144][0m Total time:      27.05 min
[32m[20230113 20:11:35 @agent_ppo2.py:146][0m 2484224 total steps have happened
[32m[20230113 20:11:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1213 --------------------------#
[32m[20230113 20:11:36 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:36 @agent_ppo2.py:186][0m |          -0.0001 |           5.9013 |           7.6437 |
[32m[20230113 20:11:36 @agent_ppo2.py:186][0m |           0.0012 |           3.9958 |           7.6360 |
[32m[20230113 20:11:36 @agent_ppo2.py:186][0m |          -0.0045 |           3.5458 |           7.6413 |
[32m[20230113 20:11:36 @agent_ppo2.py:186][0m |          -0.0079 |           3.2497 |           7.6355 |
[32m[20230113 20:11:36 @agent_ppo2.py:186][0m |          -0.0142 |           3.1092 |           7.6356 |
[32m[20230113 20:11:36 @agent_ppo2.py:186][0m |          -0.0101 |           2.9638 |           7.6295 |
[32m[20230113 20:11:36 @agent_ppo2.py:186][0m |          -0.0154 |           2.8360 |           7.6211 |
[32m[20230113 20:11:36 @agent_ppo2.py:186][0m |          -0.0164 |           2.7630 |           7.6310 |
[32m[20230113 20:11:36 @agent_ppo2.py:186][0m |          -0.0194 |           2.6971 |           7.6370 |
[32m[20230113 20:11:36 @agent_ppo2.py:186][0m |          -0.0105 |           2.6453 |           7.6329 |
[32m[20230113 20:11:36 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.18
[32m[20230113 20:11:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.80
[32m[20230113 20:11:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.04
[32m[20230113 20:11:37 @agent_ppo2.py:144][0m Total time:      27.07 min
[32m[20230113 20:11:37 @agent_ppo2.py:146][0m 2486272 total steps have happened
[32m[20230113 20:11:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1214 --------------------------#
[32m[20230113 20:11:37 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:11:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:37 @agent_ppo2.py:186][0m |           0.0013 |           7.3864 |           7.7529 |
[32m[20230113 20:11:37 @agent_ppo2.py:186][0m |          -0.0060 |           5.6624 |           7.7492 |
[32m[20230113 20:11:37 @agent_ppo2.py:186][0m |          -0.0072 |           5.0542 |           7.7488 |
[32m[20230113 20:11:37 @agent_ppo2.py:186][0m |          -0.0085 |           4.6567 |           7.7480 |
[32m[20230113 20:11:37 @agent_ppo2.py:186][0m |          -0.0095 |           4.4505 |           7.7517 |
[32m[20230113 20:11:37 @agent_ppo2.py:186][0m |          -0.0119 |           4.2512 |           7.7491 |
[32m[20230113 20:11:37 @agent_ppo2.py:186][0m |          -0.0135 |           4.0785 |           7.7468 |
[32m[20230113 20:11:37 @agent_ppo2.py:186][0m |          -0.0128 |           3.9125 |           7.7471 |
[32m[20230113 20:11:37 @agent_ppo2.py:186][0m |          -0.0147 |           3.8279 |           7.7448 |
[32m[20230113 20:11:38 @agent_ppo2.py:186][0m |          -0.0150 |           3.6718 |           7.7457 |
[32m[20230113 20:11:38 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:11:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.65
[32m[20230113 20:11:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.15
[32m[20230113 20:11:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.38
[32m[20230113 20:11:38 @agent_ppo2.py:144][0m Total time:      27.09 min
[32m[20230113 20:11:38 @agent_ppo2.py:146][0m 2488320 total steps have happened
[32m[20230113 20:11:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1215 --------------------------#
[32m[20230113 20:11:38 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:38 @agent_ppo2.py:186][0m |           0.0054 |           6.5679 |           7.4985 |
[32m[20230113 20:11:38 @agent_ppo2.py:186][0m |          -0.0029 |           5.2612 |           7.4980 |
[32m[20230113 20:11:39 @agent_ppo2.py:186][0m |          -0.0157 |           4.7356 |           7.4879 |
[32m[20230113 20:11:39 @agent_ppo2.py:186][0m |           0.0100 |           4.5675 |           7.4868 |
[32m[20230113 20:11:39 @agent_ppo2.py:186][0m |          -0.0195 |           4.2414 |           7.4748 |
[32m[20230113 20:11:39 @agent_ppo2.py:186][0m |          -0.0092 |           3.9363 |           7.4814 |
[32m[20230113 20:11:39 @agent_ppo2.py:186][0m |          -0.0056 |           3.8174 |           7.4787 |
[32m[20230113 20:11:39 @agent_ppo2.py:186][0m |          -0.0280 |           3.6967 |           7.4918 |
[32m[20230113 20:11:39 @agent_ppo2.py:186][0m |          -0.0194 |           3.5240 |           7.4872 |
[32m[20230113 20:11:39 @agent_ppo2.py:186][0m |          -0.0076 |           3.3689 |           7.4911 |
[32m[20230113 20:11:39 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:11:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.77
[32m[20230113 20:11:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.35
[32m[20230113 20:11:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.42
[32m[20230113 20:11:39 @agent_ppo2.py:144][0m Total time:      27.11 min
[32m[20230113 20:11:39 @agent_ppo2.py:146][0m 2490368 total steps have happened
[32m[20230113 20:11:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1216 --------------------------#
[32m[20230113 20:11:40 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:11:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:40 @agent_ppo2.py:186][0m |          -0.0006 |           6.6529 |           7.6676 |
[32m[20230113 20:11:40 @agent_ppo2.py:186][0m |          -0.0051 |           4.8854 |           7.6605 |
[32m[20230113 20:11:40 @agent_ppo2.py:186][0m |          -0.0067 |           4.4255 |           7.6516 |
[32m[20230113 20:11:40 @agent_ppo2.py:186][0m |          -0.0070 |           4.1497 |           7.6531 |
[32m[20230113 20:11:40 @agent_ppo2.py:186][0m |          -0.0080 |           4.0709 |           7.6514 |
[32m[20230113 20:11:40 @agent_ppo2.py:186][0m |          -0.0099 |           3.8763 |           7.6509 |
[32m[20230113 20:11:40 @agent_ppo2.py:186][0m |          -0.0102 |           3.8121 |           7.6476 |
[32m[20230113 20:11:40 @agent_ppo2.py:186][0m |          -0.0112 |           3.7807 |           7.6511 |
[32m[20230113 20:11:40 @agent_ppo2.py:186][0m |          -0.0104 |           3.6005 |           7.6377 |
[32m[20230113 20:11:40 @agent_ppo2.py:186][0m |          -0.0101 |           3.5645 |           7.6429 |
[32m[20230113 20:11:40 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.03
[32m[20230113 20:11:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.66
[32m[20230113 20:11:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.45
[32m[20230113 20:11:41 @agent_ppo2.py:144][0m Total time:      27.13 min
[32m[20230113 20:11:41 @agent_ppo2.py:146][0m 2492416 total steps have happened
[32m[20230113 20:11:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1217 --------------------------#
[32m[20230113 20:11:41 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:41 @agent_ppo2.py:186][0m |           0.0039 |           5.4047 |           7.5512 |
[32m[20230113 20:11:41 @agent_ppo2.py:186][0m |          -0.0048 |           4.5786 |           7.5341 |
[32m[20230113 20:11:41 @agent_ppo2.py:186][0m |           0.0058 |           4.4616 |           7.5245 |
[32m[20230113 20:11:41 @agent_ppo2.py:186][0m |          -0.0097 |           4.0876 |           7.5141 |
[32m[20230113 20:11:41 @agent_ppo2.py:186][0m |          -0.0148 |           3.8673 |           7.5163 |
[32m[20230113 20:11:41 @agent_ppo2.py:186][0m |          -0.0010 |           3.8245 |           7.5193 |
[32m[20230113 20:11:41 @agent_ppo2.py:186][0m |          -0.0062 |           3.6477 |           7.5215 |
[32m[20230113 20:11:41 @agent_ppo2.py:186][0m |          -0.0099 |           3.5044 |           7.5170 |
[32m[20230113 20:11:41 @agent_ppo2.py:186][0m |           0.0125 |           4.3699 |           7.5174 |
[32m[20230113 20:11:41 @agent_ppo2.py:186][0m |          -0.0093 |           3.3902 |           7.5106 |
[32m[20230113 20:11:41 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:11:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.27
[32m[20230113 20:11:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.02
[32m[20230113 20:11:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.88
[32m[20230113 20:11:42 @agent_ppo2.py:144][0m Total time:      27.15 min
[32m[20230113 20:11:42 @agent_ppo2.py:146][0m 2494464 total steps have happened
[32m[20230113 20:11:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1218 --------------------------#
[32m[20230113 20:11:42 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:42 @agent_ppo2.py:186][0m |          -0.0021 |           6.0971 |           7.5943 |
[32m[20230113 20:11:42 @agent_ppo2.py:186][0m |          -0.0047 |           5.3367 |           7.5749 |
[32m[20230113 20:11:42 @agent_ppo2.py:186][0m |          -0.0089 |           4.9121 |           7.5799 |
[32m[20230113 20:11:42 @agent_ppo2.py:186][0m |          -0.0150 |           4.5150 |           7.5747 |
[32m[20230113 20:11:43 @agent_ppo2.py:186][0m |          -0.0122 |           4.3291 |           7.5635 |
[32m[20230113 20:11:43 @agent_ppo2.py:186][0m |          -0.0118 |           4.1188 |           7.5717 |
[32m[20230113 20:11:43 @agent_ppo2.py:186][0m |          -0.0159 |           3.9965 |           7.5659 |
[32m[20230113 20:11:43 @agent_ppo2.py:186][0m |          -0.0179 |           3.8226 |           7.5684 |
[32m[20230113 20:11:43 @agent_ppo2.py:186][0m |          -0.0169 |           3.7160 |           7.5683 |
[32m[20230113 20:11:43 @agent_ppo2.py:186][0m |          -0.0156 |           3.6317 |           7.5668 |
[32m[20230113 20:11:43 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:11:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.35
[32m[20230113 20:11:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.67
[32m[20230113 20:11:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.12
[32m[20230113 20:11:43 @agent_ppo2.py:144][0m Total time:      27.18 min
[32m[20230113 20:11:43 @agent_ppo2.py:146][0m 2496512 total steps have happened
[32m[20230113 20:11:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1219 --------------------------#
[32m[20230113 20:11:44 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:44 @agent_ppo2.py:186][0m |           0.0012 |           6.3952 |           7.6078 |
[32m[20230113 20:11:44 @agent_ppo2.py:186][0m |          -0.0044 |           5.1741 |           7.6219 |
[32m[20230113 20:11:44 @agent_ppo2.py:186][0m |          -0.0080 |           4.4909 |           7.6215 |
[32m[20230113 20:11:44 @agent_ppo2.py:186][0m |          -0.0085 |           4.1446 |           7.6101 |
[32m[20230113 20:11:44 @agent_ppo2.py:186][0m |          -0.0098 |           3.9336 |           7.6233 |
[32m[20230113 20:11:44 @agent_ppo2.py:186][0m |          -0.0122 |           3.7216 |           7.6183 |
[32m[20230113 20:11:44 @agent_ppo2.py:186][0m |          -0.0133 |           3.5256 |           7.6222 |
[32m[20230113 20:11:44 @agent_ppo2.py:186][0m |          -0.0132 |           3.3934 |           7.6155 |
[32m[20230113 20:11:44 @agent_ppo2.py:186][0m |          -0.0127 |           3.2854 |           7.6186 |
[32m[20230113 20:11:44 @agent_ppo2.py:186][0m |          -0.0118 |           3.2161 |           7.6186 |
[32m[20230113 20:11:44 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.75
[32m[20230113 20:11:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.31
[32m[20230113 20:11:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.71
[32m[20230113 20:11:44 @agent_ppo2.py:144][0m Total time:      27.20 min
[32m[20230113 20:11:44 @agent_ppo2.py:146][0m 2498560 total steps have happened
[32m[20230113 20:11:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1220 --------------------------#
[32m[20230113 20:11:45 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:45 @agent_ppo2.py:186][0m |           0.0003 |           6.2716 |           7.6463 |
[32m[20230113 20:11:45 @agent_ppo2.py:186][0m |          -0.0061 |           4.6623 |           7.6400 |
[32m[20230113 20:11:45 @agent_ppo2.py:186][0m |          -0.0087 |           4.1915 |           7.6393 |
[32m[20230113 20:11:45 @agent_ppo2.py:186][0m |          -0.0096 |           3.8302 |           7.6345 |
[32m[20230113 20:11:45 @agent_ppo2.py:186][0m |          -0.0104 |           3.6685 |           7.6407 |
[32m[20230113 20:11:45 @agent_ppo2.py:186][0m |          -0.0118 |           3.4678 |           7.6383 |
[32m[20230113 20:11:45 @agent_ppo2.py:186][0m |          -0.0120 |           3.3296 |           7.6374 |
[32m[20230113 20:11:45 @agent_ppo2.py:186][0m |          -0.0128 |           3.2014 |           7.6387 |
[32m[20230113 20:11:45 @agent_ppo2.py:186][0m |          -0.0135 |           3.0983 |           7.6365 |
[32m[20230113 20:11:45 @agent_ppo2.py:186][0m |          -0.0143 |           3.0051 |           7.6417 |
[32m[20230113 20:11:45 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.24
[32m[20230113 20:11:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.29
[32m[20230113 20:11:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.11
[32m[20230113 20:11:46 @agent_ppo2.py:144][0m Total time:      27.22 min
[32m[20230113 20:11:46 @agent_ppo2.py:146][0m 2500608 total steps have happened
[32m[20230113 20:11:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1221 --------------------------#
[32m[20230113 20:11:46 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:46 @agent_ppo2.py:186][0m |          -0.0008 |           6.2998 |           7.6755 |
[32m[20230113 20:11:46 @agent_ppo2.py:186][0m |          -0.0072 |           4.8479 |           7.6845 |
[32m[20230113 20:11:46 @agent_ppo2.py:186][0m |          -0.0094 |           4.1412 |           7.6733 |
[32m[20230113 20:11:46 @agent_ppo2.py:186][0m |          -0.0084 |           3.7855 |           7.6786 |
[32m[20230113 20:11:46 @agent_ppo2.py:186][0m |          -0.0119 |           3.5030 |           7.6740 |
[32m[20230113 20:11:46 @agent_ppo2.py:186][0m |          -0.0124 |           3.3552 |           7.6709 |
[32m[20230113 20:11:46 @agent_ppo2.py:186][0m |          -0.0139 |           3.2251 |           7.6703 |
[32m[20230113 20:11:46 @agent_ppo2.py:186][0m |          -0.0134 |           3.0770 |           7.6743 |
[32m[20230113 20:11:47 @agent_ppo2.py:186][0m |          -0.0133 |           3.0096 |           7.6673 |
[32m[20230113 20:11:47 @agent_ppo2.py:186][0m |          -0.0154 |           2.9197 |           7.6697 |
[32m[20230113 20:11:47 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:11:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.42
[32m[20230113 20:11:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.06
[32m[20230113 20:11:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.34
[32m[20230113 20:11:47 @agent_ppo2.py:144][0m Total time:      27.24 min
[32m[20230113 20:11:47 @agent_ppo2.py:146][0m 2502656 total steps have happened
[32m[20230113 20:11:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1222 --------------------------#
[32m[20230113 20:11:47 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:48 @agent_ppo2.py:186][0m |          -0.0012 |           6.4020 |           7.7302 |
[32m[20230113 20:11:48 @agent_ppo2.py:186][0m |          -0.0055 |           4.6761 |           7.7096 |
[32m[20230113 20:11:48 @agent_ppo2.py:186][0m |          -0.0075 |           4.1757 |           7.7072 |
[32m[20230113 20:11:48 @agent_ppo2.py:186][0m |          -0.0087 |           3.8747 |           7.7045 |
[32m[20230113 20:11:48 @agent_ppo2.py:186][0m |          -0.0105 |           3.6595 |           7.7029 |
[32m[20230113 20:11:48 @agent_ppo2.py:186][0m |          -0.0115 |           3.5106 |           7.7008 |
[32m[20230113 20:11:48 @agent_ppo2.py:186][0m |          -0.0119 |           3.3530 |           7.6879 |
[32m[20230113 20:11:48 @agent_ppo2.py:186][0m |          -0.0135 |           3.2650 |           7.6922 |
[32m[20230113 20:11:48 @agent_ppo2.py:186][0m |          -0.0138 |           3.1775 |           7.6888 |
[32m[20230113 20:11:48 @agent_ppo2.py:186][0m |          -0.0146 |           3.0723 |           7.6929 |
[32m[20230113 20:11:48 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.89
[32m[20230113 20:11:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.25
[32m[20230113 20:11:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.54
[32m[20230113 20:11:48 @agent_ppo2.py:144][0m Total time:      27.26 min
[32m[20230113 20:11:48 @agent_ppo2.py:146][0m 2504704 total steps have happened
[32m[20230113 20:11:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1223 --------------------------#
[32m[20230113 20:11:49 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:49 @agent_ppo2.py:186][0m |           0.0013 |           7.0584 |           7.6628 |
[32m[20230113 20:11:49 @agent_ppo2.py:186][0m |          -0.0026 |           5.7088 |           7.6587 |
[32m[20230113 20:11:49 @agent_ppo2.py:186][0m |          -0.0066 |           4.9396 |           7.6590 |
[32m[20230113 20:11:49 @agent_ppo2.py:186][0m |          -0.0083 |           4.4817 |           7.6537 |
[32m[20230113 20:11:49 @agent_ppo2.py:186][0m |          -0.0094 |           4.1860 |           7.6592 |
[32m[20230113 20:11:49 @agent_ppo2.py:186][0m |          -0.0099 |           4.0099 |           7.6562 |
[32m[20230113 20:11:49 @agent_ppo2.py:186][0m |          -0.0106 |           3.7394 |           7.6527 |
[32m[20230113 20:11:49 @agent_ppo2.py:186][0m |          -0.0102 |           3.5360 |           7.6481 |
[32m[20230113 20:11:49 @agent_ppo2.py:186][0m |          -0.0124 |           3.4241 |           7.6514 |
[32m[20230113 20:11:49 @agent_ppo2.py:186][0m |          -0.0122 |           3.2610 |           7.6527 |
[32m[20230113 20:11:49 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.85
[32m[20230113 20:11:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.27
[32m[20230113 20:11:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.04
[32m[20230113 20:11:50 @agent_ppo2.py:144][0m Total time:      27.28 min
[32m[20230113 20:11:50 @agent_ppo2.py:146][0m 2506752 total steps have happened
[32m[20230113 20:11:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1224 --------------------------#
[32m[20230113 20:11:50 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:11:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:50 @agent_ppo2.py:186][0m |          -0.0028 |           6.1699 |           7.7319 |
[32m[20230113 20:11:50 @agent_ppo2.py:186][0m |          -0.0087 |           4.8094 |           7.7102 |
[32m[20230113 20:11:50 @agent_ppo2.py:186][0m |          -0.0136 |           4.3122 |           7.7094 |
[32m[20230113 20:11:50 @agent_ppo2.py:186][0m |          -0.0133 |           4.0744 |           7.7062 |
[32m[20230113 20:11:50 @agent_ppo2.py:186][0m |          -0.0139 |           3.8043 |           7.7003 |
[32m[20230113 20:11:50 @agent_ppo2.py:186][0m |          -0.0137 |           3.6311 |           7.6982 |
[32m[20230113 20:11:50 @agent_ppo2.py:186][0m |          -0.0143 |           3.5966 |           7.6941 |
[32m[20230113 20:11:50 @agent_ppo2.py:186][0m |          -0.0154 |           3.4331 |           7.6971 |
[32m[20230113 20:11:50 @agent_ppo2.py:186][0m |          -0.0140 |           3.3530 |           7.6944 |
[32m[20230113 20:11:50 @agent_ppo2.py:186][0m |          -0.0157 |           3.2375 |           7.6912 |
[32m[20230113 20:11:50 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.99
[32m[20230113 20:11:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.21
[32m[20230113 20:11:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.87
[32m[20230113 20:11:51 @agent_ppo2.py:144][0m Total time:      27.30 min
[32m[20230113 20:11:51 @agent_ppo2.py:146][0m 2508800 total steps have happened
[32m[20230113 20:11:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1225 --------------------------#
[32m[20230113 20:11:51 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:51 @agent_ppo2.py:186][0m |           0.0022 |           6.7041 |           7.5002 |
[32m[20230113 20:11:51 @agent_ppo2.py:186][0m |          -0.0068 |           5.0646 |           7.4883 |
[32m[20230113 20:11:51 @agent_ppo2.py:186][0m |           0.0117 |           4.7745 |           7.4879 |
[32m[20230113 20:11:51 @agent_ppo2.py:186][0m |          -0.0043 |           4.3468 |           7.4780 |
[32m[20230113 20:11:52 @agent_ppo2.py:186][0m |          -0.0117 |           4.1400 |           7.4754 |
[32m[20230113 20:11:52 @agent_ppo2.py:186][0m |          -0.0140 |           3.9567 |           7.4810 |
[32m[20230113 20:11:52 @agent_ppo2.py:186][0m |          -0.0154 |           3.8305 |           7.4762 |
[32m[20230113 20:11:52 @agent_ppo2.py:186][0m |          -0.0129 |           3.7271 |           7.4776 |
[32m[20230113 20:11:52 @agent_ppo2.py:186][0m |          -0.0093 |           3.6618 |           7.4748 |
[32m[20230113 20:11:52 @agent_ppo2.py:186][0m |          -0.0216 |           3.5064 |           7.4718 |
[32m[20230113 20:11:52 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:11:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.12
[32m[20230113 20:11:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.62
[32m[20230113 20:11:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 106.91
[32m[20230113 20:11:52 @agent_ppo2.py:144][0m Total time:      27.32 min
[32m[20230113 20:11:52 @agent_ppo2.py:146][0m 2510848 total steps have happened
[32m[20230113 20:11:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1226 --------------------------#
[32m[20230113 20:11:53 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:11:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:53 @agent_ppo2.py:186][0m |          -0.0017 |           6.9045 |           7.6552 |
[32m[20230113 20:11:53 @agent_ppo2.py:186][0m |          -0.0101 |           5.4449 |           7.6504 |
[32m[20230113 20:11:53 @agent_ppo2.py:186][0m |          -0.0068 |           4.8827 |           7.6476 |
[32m[20230113 20:11:53 @agent_ppo2.py:186][0m |          -0.0107 |           4.5000 |           7.6528 |
[32m[20230113 20:11:53 @agent_ppo2.py:186][0m |          -0.0128 |           4.3128 |           7.6499 |
[32m[20230113 20:11:53 @agent_ppo2.py:186][0m |          -0.0176 |           4.1142 |           7.6498 |
[32m[20230113 20:11:53 @agent_ppo2.py:186][0m |          -0.0112 |           3.9294 |           7.6508 |
[32m[20230113 20:11:53 @agent_ppo2.py:186][0m |          -0.0188 |           3.8318 |           7.6543 |
[32m[20230113 20:11:53 @agent_ppo2.py:186][0m |          -0.0143 |           3.7346 |           7.6542 |
[32m[20230113 20:11:53 @agent_ppo2.py:186][0m |          -0.0134 |           3.6635 |           7.6454 |
[32m[20230113 20:11:53 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.03
[32m[20230113 20:11:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.26
[32m[20230113 20:11:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.54
[32m[20230113 20:11:53 @agent_ppo2.py:144][0m Total time:      27.35 min
[32m[20230113 20:11:53 @agent_ppo2.py:146][0m 2512896 total steps have happened
[32m[20230113 20:11:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1227 --------------------------#
[32m[20230113 20:11:54 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:54 @agent_ppo2.py:186][0m |           0.0007 |           6.0120 |           7.8228 |
[32m[20230113 20:11:54 @agent_ppo2.py:186][0m |          -0.0051 |           4.9945 |           7.8107 |
[32m[20230113 20:11:54 @agent_ppo2.py:186][0m |          -0.0085 |           4.4952 |           7.8080 |
[32m[20230113 20:11:54 @agent_ppo2.py:186][0m |          -0.0104 |           4.2067 |           7.8122 |
[32m[20230113 20:11:54 @agent_ppo2.py:186][0m |          -0.0105 |           3.9478 |           7.8093 |
[32m[20230113 20:11:54 @agent_ppo2.py:186][0m |          -0.0120 |           3.7828 |           7.8070 |
[32m[20230113 20:11:54 @agent_ppo2.py:186][0m |          -0.0128 |           3.6352 |           7.8042 |
[32m[20230113 20:11:54 @agent_ppo2.py:186][0m |          -0.0129 |           3.5065 |           7.8046 |
[32m[20230113 20:11:54 @agent_ppo2.py:186][0m |          -0.0127 |           3.4780 |           7.8057 |
[32m[20230113 20:11:54 @agent_ppo2.py:186][0m |          -0.0135 |           3.3398 |           7.8084 |
[32m[20230113 20:11:54 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:11:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.76
[32m[20230113 20:11:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.96
[32m[20230113 20:11:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.54
[32m[20230113 20:11:55 @agent_ppo2.py:144][0m Total time:      27.37 min
[32m[20230113 20:11:55 @agent_ppo2.py:146][0m 2514944 total steps have happened
[32m[20230113 20:11:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1228 --------------------------#
[32m[20230113 20:11:55 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:11:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:55 @agent_ppo2.py:186][0m |           0.0025 |           5.0878 |           7.7371 |
[32m[20230113 20:11:55 @agent_ppo2.py:186][0m |          -0.0041 |           3.7190 |           7.7382 |
[32m[20230113 20:11:55 @agent_ppo2.py:186][0m |          -0.0073 |           3.3384 |           7.7242 |
[32m[20230113 20:11:55 @agent_ppo2.py:186][0m |          -0.0095 |           3.1893 |           7.7228 |
[32m[20230113 20:11:55 @agent_ppo2.py:186][0m |          -0.0113 |           3.0404 |           7.7305 |
[32m[20230113 20:11:55 @agent_ppo2.py:186][0m |          -0.0127 |           2.9356 |           7.7249 |
[32m[20230113 20:11:55 @agent_ppo2.py:186][0m |          -0.0148 |           2.8806 |           7.7254 |
[32m[20230113 20:11:55 @agent_ppo2.py:186][0m |          -0.0145 |           2.8129 |           7.7240 |
[32m[20230113 20:11:56 @agent_ppo2.py:186][0m |          -0.0157 |           2.7560 |           7.7269 |
[32m[20230113 20:11:56 @agent_ppo2.py:186][0m |          -0.0160 |           2.7041 |           7.7252 |
[32m[20230113 20:11:56 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:11:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.42
[32m[20230113 20:11:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.68
[32m[20230113 20:11:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.42
[32m[20230113 20:11:56 @agent_ppo2.py:144][0m Total time:      27.39 min
[32m[20230113 20:11:56 @agent_ppo2.py:146][0m 2516992 total steps have happened
[32m[20230113 20:11:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1229 --------------------------#
[32m[20230113 20:11:56 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:11:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:57 @agent_ppo2.py:186][0m |          -0.0010 |           5.9013 |           7.6823 |
[32m[20230113 20:11:57 @agent_ppo2.py:186][0m |          -0.0060 |           4.3376 |           7.6829 |
[32m[20230113 20:11:57 @agent_ppo2.py:186][0m |          -0.0136 |           3.9412 |           7.6802 |
[32m[20230113 20:11:57 @agent_ppo2.py:186][0m |          -0.0077 |           3.7313 |           7.6748 |
[32m[20230113 20:11:57 @agent_ppo2.py:186][0m |          -0.0094 |           3.5088 |           7.6755 |
[32m[20230113 20:11:57 @agent_ppo2.py:186][0m |          -0.0109 |           3.3835 |           7.6697 |
[32m[20230113 20:11:57 @agent_ppo2.py:186][0m |          -0.0107 |           3.2640 |           7.6662 |
[32m[20230113 20:11:57 @agent_ppo2.py:186][0m |          -0.0125 |           3.1350 |           7.6645 |
[32m[20230113 20:11:57 @agent_ppo2.py:186][0m |          -0.0093 |           3.0504 |           7.6666 |
[32m[20230113 20:11:57 @agent_ppo2.py:186][0m |          -0.0149 |           2.9533 |           7.6562 |
[32m[20230113 20:11:57 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:11:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.85
[32m[20230113 20:11:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.33
[32m[20230113 20:11:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.68
[32m[20230113 20:11:57 @agent_ppo2.py:144][0m Total time:      27.41 min
[32m[20230113 20:11:57 @agent_ppo2.py:146][0m 2519040 total steps have happened
[32m[20230113 20:11:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1230 --------------------------#
[32m[20230113 20:11:58 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:11:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:58 @agent_ppo2.py:186][0m |          -0.0008 |           5.2925 |           7.7444 |
[32m[20230113 20:11:58 @agent_ppo2.py:186][0m |          -0.0047 |           4.3348 |           7.7395 |
[32m[20230113 20:11:58 @agent_ppo2.py:186][0m |          -0.0076 |           3.9540 |           7.7380 |
[32m[20230113 20:11:58 @agent_ppo2.py:186][0m |          -0.0084 |           3.6834 |           7.7346 |
[32m[20230113 20:11:58 @agent_ppo2.py:186][0m |          -0.0096 |           3.5450 |           7.7388 |
[32m[20230113 20:11:58 @agent_ppo2.py:186][0m |          -0.0110 |           3.4264 |           7.7328 |
[32m[20230113 20:11:58 @agent_ppo2.py:186][0m |          -0.0113 |           3.3419 |           7.7343 |
[32m[20230113 20:11:58 @agent_ppo2.py:186][0m |          -0.0119 |           3.2498 |           7.7367 |
[32m[20230113 20:11:58 @agent_ppo2.py:186][0m |          -0.0116 |           3.1736 |           7.7359 |
[32m[20230113 20:11:58 @agent_ppo2.py:186][0m |          -0.0122 |           3.0996 |           7.7260 |
[32m[20230113 20:11:58 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:11:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.03
[32m[20230113 20:11:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.96
[32m[20230113 20:11:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 231.63
[32m[20230113 20:11:59 @agent_ppo2.py:144][0m Total time:      27.43 min
[32m[20230113 20:11:59 @agent_ppo2.py:146][0m 2521088 total steps have happened
[32m[20230113 20:11:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1231 --------------------------#
[32m[20230113 20:11:59 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:11:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:11:59 @agent_ppo2.py:186][0m |           0.0016 |           5.6421 |           7.5510 |
[32m[20230113 20:11:59 @agent_ppo2.py:186][0m |          -0.0077 |           4.4190 |           7.5379 |
[32m[20230113 20:11:59 @agent_ppo2.py:186][0m |          -0.0090 |           3.9452 |           7.5339 |
[32m[20230113 20:11:59 @agent_ppo2.py:186][0m |          -0.0053 |           3.5722 |           7.5337 |
[32m[20230113 20:11:59 @agent_ppo2.py:186][0m |          -0.0093 |           3.3771 |           7.5345 |
[32m[20230113 20:11:59 @agent_ppo2.py:186][0m |          -0.0089 |           3.2328 |           7.5347 |
[32m[20230113 20:11:59 @agent_ppo2.py:186][0m |          -0.0116 |           3.0991 |           7.5308 |
[32m[20230113 20:11:59 @agent_ppo2.py:186][0m |          -0.0118 |           2.9760 |           7.5317 |
[32m[20230113 20:11:59 @agent_ppo2.py:186][0m |          -0.0137 |           2.8670 |           7.5275 |
[32m[20230113 20:12:00 @agent_ppo2.py:186][0m |          -0.0085 |           2.8196 |           7.5301 |
[32m[20230113 20:12:00 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:12:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.25
[32m[20230113 20:12:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.22
[32m[20230113 20:12:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 164.34
[32m[20230113 20:12:00 @agent_ppo2.py:144][0m Total time:      27.46 min
[32m[20230113 20:12:00 @agent_ppo2.py:146][0m 2523136 total steps have happened
[32m[20230113 20:12:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1232 --------------------------#
[32m[20230113 20:12:00 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:00 @agent_ppo2.py:186][0m |          -0.0034 |           6.0706 |           7.9308 |
[32m[20230113 20:12:00 @agent_ppo2.py:186][0m |          -0.0071 |           4.7674 |           7.9205 |
[32m[20230113 20:12:01 @agent_ppo2.py:186][0m |          -0.0071 |           4.3357 |           7.9257 |
[32m[20230113 20:12:01 @agent_ppo2.py:186][0m |          -0.0140 |           3.9780 |           7.9231 |
[32m[20230113 20:12:01 @agent_ppo2.py:186][0m |          -0.0121 |           3.7954 |           7.9159 |
[32m[20230113 20:12:01 @agent_ppo2.py:186][0m |          -0.0125 |           3.6882 |           7.9151 |
[32m[20230113 20:12:01 @agent_ppo2.py:186][0m |          -0.0138 |           3.5593 |           7.9105 |
[32m[20230113 20:12:01 @agent_ppo2.py:186][0m |          -0.0146 |           3.4766 |           7.9101 |
[32m[20230113 20:12:01 @agent_ppo2.py:186][0m |          -0.0174 |           3.3716 |           7.9037 |
[32m[20230113 20:12:01 @agent_ppo2.py:186][0m |          -0.0178 |           3.3254 |           7.9035 |
[32m[20230113 20:12:01 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:12:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.35
[32m[20230113 20:12:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.65
[32m[20230113 20:12:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.42
[32m[20230113 20:12:01 @agent_ppo2.py:144][0m Total time:      27.48 min
[32m[20230113 20:12:01 @agent_ppo2.py:146][0m 2525184 total steps have happened
[32m[20230113 20:12:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1233 --------------------------#
[32m[20230113 20:12:02 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:02 @agent_ppo2.py:186][0m |           0.0053 |           6.2578 |           7.5358 |
[32m[20230113 20:12:02 @agent_ppo2.py:186][0m |          -0.0079 |           4.8999 |           7.5210 |
[32m[20230113 20:12:02 @agent_ppo2.py:186][0m |          -0.0048 |           4.1924 |           7.5112 |
[32m[20230113 20:12:02 @agent_ppo2.py:186][0m |          -0.0092 |           3.7860 |           7.5015 |
[32m[20230113 20:12:02 @agent_ppo2.py:186][0m |          -0.0336 |           3.7835 |           7.5114 |
[32m[20230113 20:12:02 @agent_ppo2.py:186][0m |          -0.0033 |           3.4232 |           7.4481 |
[32m[20230113 20:12:02 @agent_ppo2.py:186][0m |          -0.0200 |           3.0932 |           7.5069 |
[32m[20230113 20:12:02 @agent_ppo2.py:186][0m |           0.0240 |           3.1427 |           7.5018 |
[32m[20230113 20:12:02 @agent_ppo2.py:186][0m |           0.0051 |           2.9281 |           7.5016 |
[32m[20230113 20:12:02 @agent_ppo2.py:186][0m |          -0.0030 |           2.7394 |           7.5123 |
[32m[20230113 20:12:02 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:12:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.70
[32m[20230113 20:12:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.71
[32m[20230113 20:12:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.70
[32m[20230113 20:12:03 @agent_ppo2.py:144][0m Total time:      27.50 min
[32m[20230113 20:12:03 @agent_ppo2.py:146][0m 2527232 total steps have happened
[32m[20230113 20:12:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1234 --------------------------#
[32m[20230113 20:12:03 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:12:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:03 @agent_ppo2.py:186][0m |          -0.0031 |           7.2949 |           7.5840 |
[32m[20230113 20:12:03 @agent_ppo2.py:186][0m |           0.0013 |           5.5841 |           7.5776 |
[32m[20230113 20:12:03 @agent_ppo2.py:186][0m |          -0.0087 |           4.7359 |           7.5708 |
[32m[20230113 20:12:03 @agent_ppo2.py:186][0m |          -0.0083 |           4.3390 |           7.5671 |
[32m[20230113 20:12:03 @agent_ppo2.py:186][0m |          -0.0093 |           3.9985 |           7.5670 |
[32m[20230113 20:12:03 @agent_ppo2.py:186][0m |          -0.0100 |           3.6927 |           7.5590 |
[32m[20230113 20:12:03 @agent_ppo2.py:186][0m |          -0.0103 |           3.4917 |           7.5606 |
[32m[20230113 20:12:03 @agent_ppo2.py:186][0m |          -0.0118 |           3.3739 |           7.5502 |
[32m[20230113 20:12:03 @agent_ppo2.py:186][0m |          -0.0118 |           3.2365 |           7.5538 |
[32m[20230113 20:12:03 @agent_ppo2.py:186][0m |          -0.0120 |           3.0764 |           7.5482 |
[32m[20230113 20:12:03 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:12:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.57
[32m[20230113 20:12:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.75
[32m[20230113 20:12:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.18
[32m[20230113 20:12:04 @agent_ppo2.py:144][0m Total time:      27.52 min
[32m[20230113 20:12:04 @agent_ppo2.py:146][0m 2529280 total steps have happened
[32m[20230113 20:12:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1235 --------------------------#
[32m[20230113 20:12:04 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:12:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:04 @agent_ppo2.py:186][0m |          -0.0043 |           5.6234 |           7.6220 |
[32m[20230113 20:12:04 @agent_ppo2.py:186][0m |          -0.0021 |           4.0256 |           7.6159 |
[32m[20230113 20:12:04 @agent_ppo2.py:186][0m |          -0.0135 |           3.5363 |           7.6097 |
[32m[20230113 20:12:04 @agent_ppo2.py:186][0m |          -0.0063 |           3.2379 |           7.6089 |
[32m[20230113 20:12:05 @agent_ppo2.py:186][0m |          -0.0012 |           3.0818 |           7.6103 |
[32m[20230113 20:12:05 @agent_ppo2.py:186][0m |          -0.0278 |           2.9520 |           7.6029 |
[32m[20230113 20:12:05 @agent_ppo2.py:186][0m |           0.0071 |           2.8829 |           7.6020 |
[32m[20230113 20:12:05 @agent_ppo2.py:186][0m |          -0.0036 |           2.7826 |           7.6041 |
[32m[20230113 20:12:05 @agent_ppo2.py:186][0m |          -0.0010 |           2.6890 |           7.6067 |
[32m[20230113 20:12:05 @agent_ppo2.py:186][0m |           0.0087 |           2.8491 |           7.6119 |
[32m[20230113 20:12:05 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.45
[32m[20230113 20:12:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.15
[32m[20230113 20:12:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.66
[32m[20230113 20:12:05 @agent_ppo2.py:144][0m Total time:      27.54 min
[32m[20230113 20:12:05 @agent_ppo2.py:146][0m 2531328 total steps have happened
[32m[20230113 20:12:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1236 --------------------------#
[32m[20230113 20:12:06 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:06 @agent_ppo2.py:186][0m |           0.0003 |           5.6213 |           7.8739 |
[32m[20230113 20:12:06 @agent_ppo2.py:186][0m |          -0.0035 |           4.3040 |           7.8708 |
[32m[20230113 20:12:06 @agent_ppo2.py:186][0m |          -0.0089 |           3.9469 |           7.8717 |
[32m[20230113 20:12:06 @agent_ppo2.py:186][0m |          -0.0094 |           3.7488 |           7.8650 |
[32m[20230113 20:12:06 @agent_ppo2.py:186][0m |          -0.0110 |           3.6261 |           7.8653 |
[32m[20230113 20:12:06 @agent_ppo2.py:186][0m |          -0.0099 |           3.5074 |           7.8642 |
[32m[20230113 20:12:06 @agent_ppo2.py:186][0m |          -0.0120 |           3.3728 |           7.8616 |
[32m[20230113 20:12:06 @agent_ppo2.py:186][0m |          -0.0121 |           3.3503 |           7.8622 |
[32m[20230113 20:12:06 @agent_ppo2.py:186][0m |          -0.0122 |           3.2360 |           7.8632 |
[32m[20230113 20:12:06 @agent_ppo2.py:186][0m |          -0.0148 |           3.2202 |           7.8553 |
[32m[20230113 20:12:06 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:12:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.98
[32m[20230113 20:12:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.46
[32m[20230113 20:12:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.37
[32m[20230113 20:12:06 @agent_ppo2.py:144][0m Total time:      27.56 min
[32m[20230113 20:12:06 @agent_ppo2.py:146][0m 2533376 total steps have happened
[32m[20230113 20:12:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1237 --------------------------#
[32m[20230113 20:12:07 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:12:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:07 @agent_ppo2.py:186][0m |          -0.0008 |           6.3963 |           7.6711 |
[32m[20230113 20:12:07 @agent_ppo2.py:186][0m |          -0.0066 |           4.0861 |           7.6557 |
[32m[20230113 20:12:07 @agent_ppo2.py:186][0m |          -0.0095 |           3.5061 |           7.6518 |
[32m[20230113 20:12:07 @agent_ppo2.py:186][0m |          -0.0120 |           3.1430 |           7.6461 |
[32m[20230113 20:12:07 @agent_ppo2.py:186][0m |          -0.0127 |           2.9169 |           7.6341 |
[32m[20230113 20:12:07 @agent_ppo2.py:186][0m |          -0.0141 |           2.7870 |           7.6469 |
[32m[20230113 20:12:07 @agent_ppo2.py:186][0m |          -0.0138 |           2.6587 |           7.6349 |
[32m[20230113 20:12:07 @agent_ppo2.py:186][0m |          -0.0162 |           2.5684 |           7.6364 |
[32m[20230113 20:12:07 @agent_ppo2.py:186][0m |          -0.0161 |           2.4832 |           7.6269 |
[32m[20230113 20:12:07 @agent_ppo2.py:186][0m |          -0.0173 |           2.4333 |           7.6377 |
[32m[20230113 20:12:07 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:12:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.66
[32m[20230113 20:12:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.29
[32m[20230113 20:12:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.00
[32m[20230113 20:12:08 @agent_ppo2.py:144][0m Total time:      27.59 min
[32m[20230113 20:12:08 @agent_ppo2.py:146][0m 2535424 total steps have happened
[32m[20230113 20:12:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1238 --------------------------#
[32m[20230113 20:12:08 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:08 @agent_ppo2.py:186][0m |          -0.0016 |           6.1396 |           7.6550 |
[32m[20230113 20:12:08 @agent_ppo2.py:186][0m |          -0.0037 |           4.7969 |           7.6563 |
[32m[20230113 20:12:08 @agent_ppo2.py:186][0m |          -0.0051 |           4.2273 |           7.6407 |
[32m[20230113 20:12:08 @agent_ppo2.py:186][0m |          -0.0042 |           3.9130 |           7.6383 |
[32m[20230113 20:12:08 @agent_ppo2.py:186][0m |          -0.0088 |           3.7226 |           7.6415 |
[32m[20230113 20:12:08 @agent_ppo2.py:186][0m |          -0.0081 |           3.5599 |           7.6411 |
[32m[20230113 20:12:08 @agent_ppo2.py:186][0m |          -0.0103 |           3.4291 |           7.6433 |
[32m[20230113 20:12:09 @agent_ppo2.py:186][0m |          -0.0096 |           3.3179 |           7.6508 |
[32m[20230113 20:12:09 @agent_ppo2.py:186][0m |          -0.0059 |           3.2208 |           7.6439 |
[32m[20230113 20:12:09 @agent_ppo2.py:186][0m |          -0.0083 |           3.1226 |           7.6475 |
[32m[20230113 20:12:09 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.18
[32m[20230113 20:12:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.70
[32m[20230113 20:12:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.97
[32m[20230113 20:12:09 @agent_ppo2.py:144][0m Total time:      27.61 min
[32m[20230113 20:12:09 @agent_ppo2.py:146][0m 2537472 total steps have happened
[32m[20230113 20:12:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1239 --------------------------#
[32m[20230113 20:12:09 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 20:12:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:09 @agent_ppo2.py:186][0m |           0.0016 |          15.5332 |           7.5976 |
[32m[20230113 20:12:10 @agent_ppo2.py:186][0m |          -0.0121 |           6.9949 |           7.5918 |
[32m[20230113 20:12:10 @agent_ppo2.py:186][0m |          -0.0080 |           5.4672 |           7.5858 |
[32m[20230113 20:12:10 @agent_ppo2.py:186][0m |          -0.0161 |           4.8490 |           7.5887 |
[32m[20230113 20:12:10 @agent_ppo2.py:186][0m |          -0.0136 |           4.4187 |           7.5837 |
[32m[20230113 20:12:10 @agent_ppo2.py:186][0m |          -0.0204 |           4.0756 |           7.5817 |
[32m[20230113 20:12:10 @agent_ppo2.py:186][0m |          -0.0179 |           3.8725 |           7.5826 |
[32m[20230113 20:12:10 @agent_ppo2.py:186][0m |          -0.0183 |           3.7031 |           7.5854 |
[32m[20230113 20:12:10 @agent_ppo2.py:186][0m |          -0.0224 |           3.5074 |           7.5806 |
[32m[20230113 20:12:10 @agent_ppo2.py:186][0m |          -0.0224 |           3.3680 |           7.5796 |
[32m[20230113 20:12:10 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 20:12:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 151.03
[32m[20230113 20:12:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.84
[32m[20230113 20:12:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 146.21
[32m[20230113 20:12:10 @agent_ppo2.py:144][0m Total time:      27.63 min
[32m[20230113 20:12:10 @agent_ppo2.py:146][0m 2539520 total steps have happened
[32m[20230113 20:12:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1240 --------------------------#
[32m[20230113 20:12:11 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:12:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:11 @agent_ppo2.py:186][0m |           0.0020 |           5.8206 |           7.6569 |
[32m[20230113 20:12:11 @agent_ppo2.py:186][0m |          -0.0059 |           4.1175 |           7.6630 |
[32m[20230113 20:12:11 @agent_ppo2.py:186][0m |          -0.0056 |           3.6625 |           7.6631 |
[32m[20230113 20:12:11 @agent_ppo2.py:186][0m |          -0.0096 |           3.3063 |           7.6502 |
[32m[20230113 20:12:11 @agent_ppo2.py:186][0m |          -0.0110 |           3.1267 |           7.6550 |
[32m[20230113 20:12:11 @agent_ppo2.py:186][0m |          -0.0117 |           3.0038 |           7.6603 |
[32m[20230113 20:12:11 @agent_ppo2.py:186][0m |          -0.0104 |           2.8756 |           7.6544 |
[32m[20230113 20:12:11 @agent_ppo2.py:186][0m |          -0.0117 |           2.7795 |           7.6554 |
[32m[20230113 20:12:11 @agent_ppo2.py:186][0m |          -0.0136 |           2.7076 |           7.6587 |
[32m[20230113 20:12:11 @agent_ppo2.py:186][0m |          -0.0117 |           2.6363 |           7.6600 |
[32m[20230113 20:12:11 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:12:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.82
[32m[20230113 20:12:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.84
[32m[20230113 20:12:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.06
[32m[20230113 20:12:12 @agent_ppo2.py:144][0m Total time:      27.65 min
[32m[20230113 20:12:12 @agent_ppo2.py:146][0m 2541568 total steps have happened
[32m[20230113 20:12:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1241 --------------------------#
[32m[20230113 20:12:12 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:12:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:12 @agent_ppo2.py:186][0m |          -0.0007 |           6.4934 |           7.8058 |
[32m[20230113 20:12:12 @agent_ppo2.py:186][0m |          -0.0036 |           5.4966 |           7.7963 |
[32m[20230113 20:12:12 @agent_ppo2.py:186][0m |          -0.0048 |           5.0595 |           7.8022 |
[32m[20230113 20:12:12 @agent_ppo2.py:186][0m |          -0.0110 |           4.7301 |           7.7997 |
[32m[20230113 20:12:12 @agent_ppo2.py:186][0m |          -0.0094 |           4.4571 |           7.7988 |
[32m[20230113 20:12:12 @agent_ppo2.py:186][0m |          -0.0136 |           4.2580 |           7.8001 |
[32m[20230113 20:12:12 @agent_ppo2.py:186][0m |          -0.0103 |           4.1302 |           7.7996 |
[32m[20230113 20:12:12 @agent_ppo2.py:186][0m |          -0.0141 |           3.9408 |           7.8000 |
[32m[20230113 20:12:12 @agent_ppo2.py:186][0m |          -0.0081 |           3.9281 |           7.7996 |
[32m[20230113 20:12:12 @agent_ppo2.py:186][0m |          -0.0162 |           3.7685 |           7.7985 |
[32m[20230113 20:12:12 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.95
[32m[20230113 20:12:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.55
[32m[20230113 20:12:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 118.28
[32m[20230113 20:12:13 @agent_ppo2.py:144][0m Total time:      27.67 min
[32m[20230113 20:12:13 @agent_ppo2.py:146][0m 2543616 total steps have happened
[32m[20230113 20:12:13 @agent_ppo2.py:122][0m #------------------------ Iteration 1242 --------------------------#
[32m[20230113 20:12:13 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:12:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:13 @agent_ppo2.py:186][0m |           0.0022 |           6.2936 |           7.7181 |
[32m[20230113 20:12:13 @agent_ppo2.py:186][0m |          -0.0023 |           5.0437 |           7.7081 |
[32m[20230113 20:12:13 @agent_ppo2.py:186][0m |          -0.0080 |           4.6016 |           7.7044 |
[32m[20230113 20:12:13 @agent_ppo2.py:186][0m |          -0.0033 |           4.2635 |           7.7018 |
[32m[20230113 20:12:13 @agent_ppo2.py:186][0m |          -0.0078 |           4.0672 |           7.6948 |
[32m[20230113 20:12:13 @agent_ppo2.py:186][0m |          -0.0103 |           3.9244 |           7.6970 |
[32m[20230113 20:12:14 @agent_ppo2.py:186][0m |          -0.0089 |           3.7536 |           7.6998 |
[32m[20230113 20:12:14 @agent_ppo2.py:186][0m |          -0.0037 |           3.6704 |           7.6930 |
[32m[20230113 20:12:14 @agent_ppo2.py:186][0m |          -0.0095 |           3.6079 |           7.6933 |
[32m[20230113 20:12:14 @agent_ppo2.py:186][0m |          -0.0066 |           3.5188 |           7.6932 |
[32m[20230113 20:12:14 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.89
[32m[20230113 20:12:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.51
[32m[20230113 20:12:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.55
[32m[20230113 20:12:14 @agent_ppo2.py:144][0m Total time:      27.69 min
[32m[20230113 20:12:14 @agent_ppo2.py:146][0m 2545664 total steps have happened
[32m[20230113 20:12:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1243 --------------------------#
[32m[20230113 20:12:15 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:15 @agent_ppo2.py:186][0m |          -0.0000 |           5.3538 |           7.8044 |
[32m[20230113 20:12:15 @agent_ppo2.py:186][0m |          -0.0057 |           4.2993 |           7.7985 |
[32m[20230113 20:12:15 @agent_ppo2.py:186][0m |          -0.0071 |           4.0278 |           7.7969 |
[32m[20230113 20:12:15 @agent_ppo2.py:186][0m |          -0.0078 |           3.7891 |           7.7946 |
[32m[20230113 20:12:15 @agent_ppo2.py:186][0m |          -0.0088 |           3.7113 |           7.7964 |
[32m[20230113 20:12:15 @agent_ppo2.py:186][0m |          -0.0100 |           3.5929 |           7.7963 |
[32m[20230113 20:12:15 @agent_ppo2.py:186][0m |          -0.0103 |           3.5060 |           7.7954 |
[32m[20230113 20:12:15 @agent_ppo2.py:186][0m |          -0.0121 |           3.3677 |           7.7932 |
[32m[20230113 20:12:15 @agent_ppo2.py:186][0m |          -0.0138 |           3.3527 |           7.7927 |
[32m[20230113 20:12:15 @agent_ppo2.py:186][0m |          -0.0145 |           3.2951 |           7.7847 |
[32m[20230113 20:12:15 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.01
[32m[20230113 20:12:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.16
[32m[20230113 20:12:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.46
[32m[20230113 20:12:15 @agent_ppo2.py:144][0m Total time:      27.71 min
[32m[20230113 20:12:15 @agent_ppo2.py:146][0m 2547712 total steps have happened
[32m[20230113 20:12:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1244 --------------------------#
[32m[20230113 20:12:16 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:16 @agent_ppo2.py:186][0m |           0.0009 |           5.0790 |           7.8163 |
[32m[20230113 20:12:16 @agent_ppo2.py:186][0m |          -0.0043 |           3.8793 |           7.8183 |
[32m[20230113 20:12:16 @agent_ppo2.py:186][0m |          -0.0055 |           3.5152 |           7.8133 |
[32m[20230113 20:12:16 @agent_ppo2.py:186][0m |          -0.0081 |           3.2705 |           7.8187 |
[32m[20230113 20:12:16 @agent_ppo2.py:186][0m |          -0.0087 |           3.1307 |           7.8179 |
[32m[20230113 20:12:16 @agent_ppo2.py:186][0m |          -0.0095 |           2.9886 |           7.8178 |
[32m[20230113 20:12:16 @agent_ppo2.py:186][0m |          -0.0106 |           2.8565 |           7.8177 |
[32m[20230113 20:12:16 @agent_ppo2.py:186][0m |          -0.0122 |           2.7585 |           7.8165 |
[32m[20230113 20:12:16 @agent_ppo2.py:186][0m |          -0.0120 |           2.7031 |           7.8206 |
[32m[20230113 20:12:16 @agent_ppo2.py:186][0m |          -0.0118 |           2.6323 |           7.8233 |
[32m[20230113 20:12:16 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.27
[32m[20230113 20:12:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.57
[32m[20230113 20:12:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.56
[32m[20230113 20:12:17 @agent_ppo2.py:144][0m Total time:      27.73 min
[32m[20230113 20:12:17 @agent_ppo2.py:146][0m 2549760 total steps have happened
[32m[20230113 20:12:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1245 --------------------------#
[32m[20230113 20:12:17 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:17 @agent_ppo2.py:186][0m |           0.0099 |           7.2133 |           7.7811 |
[32m[20230113 20:12:17 @agent_ppo2.py:186][0m |          -0.0081 |           5.9535 |           7.7537 |
[32m[20230113 20:12:17 @agent_ppo2.py:186][0m |          -0.0081 |           5.4583 |           7.7591 |
[32m[20230113 20:12:17 @agent_ppo2.py:186][0m |          -0.0041 |           5.1771 |           7.7565 |
[32m[20230113 20:12:17 @agent_ppo2.py:186][0m |          -0.0099 |           4.9077 |           7.7636 |
[32m[20230113 20:12:17 @agent_ppo2.py:186][0m |          -0.0067 |           4.6985 |           7.7563 |
[32m[20230113 20:12:17 @agent_ppo2.py:186][0m |          -0.0150 |           4.6840 |           7.7578 |
[32m[20230113 20:12:17 @agent_ppo2.py:186][0m |          -0.0158 |           4.6357 |           7.7655 |
[32m[20230113 20:12:18 @agent_ppo2.py:186][0m |          -0.0083 |           4.3530 |           7.7525 |
[32m[20230113 20:12:18 @agent_ppo2.py:186][0m |          -0.0140 |           4.2184 |           7.7596 |
[32m[20230113 20:12:18 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:12:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.73
[32m[20230113 20:12:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.87
[32m[20230113 20:12:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.48
[32m[20230113 20:12:18 @agent_ppo2.py:144][0m Total time:      27.76 min
[32m[20230113 20:12:18 @agent_ppo2.py:146][0m 2551808 total steps have happened
[32m[20230113 20:12:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1246 --------------------------#
[32m[20230113 20:12:18 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:18 @agent_ppo2.py:186][0m |          -0.0070 |           6.4805 |           7.6782 |
[32m[20230113 20:12:19 @agent_ppo2.py:186][0m |          -0.0064 |           4.9587 |           7.6659 |
[32m[20230113 20:12:19 @agent_ppo2.py:186][0m |           0.0010 |           4.7496 |           7.6648 |
[32m[20230113 20:12:19 @agent_ppo2.py:186][0m |          -0.0053 |           4.1511 |           7.6552 |
[32m[20230113 20:12:19 @agent_ppo2.py:186][0m |          -0.0068 |           4.1067 |           7.6633 |
[32m[20230113 20:12:19 @agent_ppo2.py:186][0m |          -0.0105 |           3.8182 |           7.6645 |
[32m[20230113 20:12:19 @agent_ppo2.py:186][0m |          -0.0104 |           3.6585 |           7.6630 |
[32m[20230113 20:12:19 @agent_ppo2.py:186][0m |          -0.0133 |           3.6079 |           7.6643 |
[32m[20230113 20:12:19 @agent_ppo2.py:186][0m |          -0.0068 |           3.5664 |           7.6636 |
[32m[20230113 20:12:19 @agent_ppo2.py:186][0m |          -0.0160 |           3.4078 |           7.6585 |
[32m[20230113 20:12:19 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:12:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.25
[32m[20230113 20:12:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.63
[32m[20230113 20:12:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.29
[32m[20230113 20:12:19 @agent_ppo2.py:144][0m Total time:      27.78 min
[32m[20230113 20:12:19 @agent_ppo2.py:146][0m 2553856 total steps have happened
[32m[20230113 20:12:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1247 --------------------------#
[32m[20230113 20:12:20 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 20:12:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:20 @agent_ppo2.py:186][0m |           0.0015 |          25.2477 |           7.8279 |
[32m[20230113 20:12:20 @agent_ppo2.py:186][0m |          -0.0065 |           6.8396 |           7.8256 |
[32m[20230113 20:12:20 @agent_ppo2.py:186][0m |          -0.0081 |           4.8292 |           7.8308 |
[32m[20230113 20:12:20 @agent_ppo2.py:186][0m |          -0.0083 |           5.1489 |           7.8290 |
[32m[20230113 20:12:20 @agent_ppo2.py:186][0m |          -0.0152 |           4.0753 |           7.8320 |
[32m[20230113 20:12:20 @agent_ppo2.py:186][0m |          -0.0178 |           3.8506 |           7.8291 |
[32m[20230113 20:12:20 @agent_ppo2.py:186][0m |          -0.0147 |           3.7270 |           7.8279 |
[32m[20230113 20:12:20 @agent_ppo2.py:186][0m |          -0.0128 |           3.5393 |           7.8256 |
[32m[20230113 20:12:20 @agent_ppo2.py:186][0m |          -0.0170 |           3.4361 |           7.8281 |
[32m[20230113 20:12:20 @agent_ppo2.py:186][0m |          -0.0110 |           3.4262 |           7.8199 |
[32m[20230113 20:12:20 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:12:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 98.45
[32m[20230113 20:12:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.43
[32m[20230113 20:12:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.94
[32m[20230113 20:12:20 @agent_ppo2.py:144][0m Total time:      27.80 min
[32m[20230113 20:12:20 @agent_ppo2.py:146][0m 2555904 total steps have happened
[32m[20230113 20:12:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1248 --------------------------#
[32m[20230113 20:12:21 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:12:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:21 @agent_ppo2.py:186][0m |           0.0020 |           6.6319 |           7.7671 |
[32m[20230113 20:12:21 @agent_ppo2.py:186][0m |          -0.0056 |           5.1847 |           7.7561 |
[32m[20230113 20:12:21 @agent_ppo2.py:186][0m |          -0.0071 |           4.7779 |           7.7514 |
[32m[20230113 20:12:21 @agent_ppo2.py:186][0m |          -0.0114 |           4.4347 |           7.7536 |
[32m[20230113 20:12:21 @agent_ppo2.py:186][0m |          -0.0132 |           4.2160 |           7.7490 |
[32m[20230113 20:12:21 @agent_ppo2.py:186][0m |          -0.0121 |           4.1063 |           7.7502 |
[32m[20230113 20:12:21 @agent_ppo2.py:186][0m |          -0.0129 |           3.9351 |           7.7482 |
[32m[20230113 20:12:21 @agent_ppo2.py:186][0m |          -0.0136 |           3.8028 |           7.7528 |
[32m[20230113 20:12:21 @agent_ppo2.py:186][0m |          -0.0150 |           3.7152 |           7.7479 |
[32m[20230113 20:12:21 @agent_ppo2.py:186][0m |          -0.0168 |           3.6016 |           7.7519 |
[32m[20230113 20:12:21 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:12:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.90
[32m[20230113 20:12:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.11
[32m[20230113 20:12:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.71
[32m[20230113 20:12:22 @agent_ppo2.py:144][0m Total time:      27.82 min
[32m[20230113 20:12:22 @agent_ppo2.py:146][0m 2557952 total steps have happened
[32m[20230113 20:12:22 @agent_ppo2.py:122][0m #------------------------ Iteration 1249 --------------------------#
[32m[20230113 20:12:22 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:12:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:22 @agent_ppo2.py:186][0m |           0.0025 |           5.1738 |           7.9126 |
[32m[20230113 20:12:22 @agent_ppo2.py:186][0m |           0.0015 |           3.6709 |           7.9022 |
[32m[20230113 20:12:22 @agent_ppo2.py:186][0m |          -0.0037 |           3.3027 |           7.8913 |
[32m[20230113 20:12:22 @agent_ppo2.py:186][0m |          -0.0064 |           3.1396 |           7.8979 |
[32m[20230113 20:12:22 @agent_ppo2.py:186][0m |          -0.0067 |           2.9614 |           7.8961 |
[32m[20230113 20:12:22 @agent_ppo2.py:186][0m |          -0.0102 |           2.8629 |           7.8994 |
[32m[20230113 20:12:22 @agent_ppo2.py:186][0m |          -0.0077 |           2.7659 |           7.8926 |
[32m[20230113 20:12:23 @agent_ppo2.py:186][0m |          -0.0116 |           2.7098 |           7.8976 |
[32m[20230113 20:12:23 @agent_ppo2.py:186][0m |          -0.0111 |           2.6323 |           7.8974 |
[32m[20230113 20:12:23 @agent_ppo2.py:186][0m |          -0.0106 |           2.5346 |           7.8961 |
[32m[20230113 20:12:23 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:12:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 218.79
[32m[20230113 20:12:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.89
[32m[20230113 20:12:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 75.45
[32m[20230113 20:12:23 @agent_ppo2.py:144][0m Total time:      27.84 min
[32m[20230113 20:12:23 @agent_ppo2.py:146][0m 2560000 total steps have happened
[32m[20230113 20:12:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1250 --------------------------#
[32m[20230113 20:12:23 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:23 @agent_ppo2.py:186][0m |           0.0002 |           5.3792 |           8.0620 |
[32m[20230113 20:12:24 @agent_ppo2.py:186][0m |          -0.0050 |           4.2069 |           8.0498 |
[32m[20230113 20:12:24 @agent_ppo2.py:186][0m |          -0.0078 |           3.8516 |           8.0420 |
[32m[20230113 20:12:24 @agent_ppo2.py:186][0m |          -0.0102 |           3.6504 |           8.0389 |
[32m[20230113 20:12:24 @agent_ppo2.py:186][0m |          -0.0099 |           3.4880 |           8.0364 |
[32m[20230113 20:12:24 @agent_ppo2.py:186][0m |          -0.0113 |           3.2965 |           8.0348 |
[32m[20230113 20:12:24 @agent_ppo2.py:186][0m |          -0.0124 |           3.2178 |           8.0325 |
[32m[20230113 20:12:24 @agent_ppo2.py:186][0m |          -0.0135 |           3.1157 |           8.0283 |
[32m[20230113 20:12:24 @agent_ppo2.py:186][0m |          -0.0134 |           3.0508 |           8.0209 |
[32m[20230113 20:12:24 @agent_ppo2.py:186][0m |          -0.0146 |           3.0295 |           8.0304 |
[32m[20230113 20:12:24 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.13
[32m[20230113 20:12:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.89
[32m[20230113 20:12:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 72.73
[32m[20230113 20:12:24 @agent_ppo2.py:144][0m Total time:      27.86 min
[32m[20230113 20:12:24 @agent_ppo2.py:146][0m 2562048 total steps have happened
[32m[20230113 20:12:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1251 --------------------------#
[32m[20230113 20:12:25 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:12:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:25 @agent_ppo2.py:186][0m |          -0.0004 |           5.4211 |           7.8817 |
[32m[20230113 20:12:25 @agent_ppo2.py:186][0m |          -0.0099 |           4.4294 |           7.8703 |
[32m[20230113 20:12:25 @agent_ppo2.py:186][0m |          -0.0129 |           4.0164 |           7.8674 |
[32m[20230113 20:12:25 @agent_ppo2.py:186][0m |          -0.0149 |           3.7933 |           7.8632 |
[32m[20230113 20:12:25 @agent_ppo2.py:186][0m |          -0.0148 |           3.6365 |           7.8616 |
[32m[20230113 20:12:25 @agent_ppo2.py:186][0m |          -0.0149 |           3.5066 |           7.8569 |
[32m[20230113 20:12:25 @agent_ppo2.py:186][0m |          -0.0161 |           3.3933 |           7.8645 |
[32m[20230113 20:12:25 @agent_ppo2.py:186][0m |          -0.0168 |           3.2855 |           7.8584 |
[32m[20230113 20:12:25 @agent_ppo2.py:186][0m |          -0.0186 |           3.2494 |           7.8567 |
[32m[20230113 20:12:25 @agent_ppo2.py:186][0m |          -0.0181 |           3.1940 |           7.8662 |
[32m[20230113 20:12:25 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:12:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.75
[32m[20230113 20:12:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.17
[32m[20230113 20:12:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.73
[32m[20230113 20:12:26 @agent_ppo2.py:144][0m Total time:      27.88 min
[32m[20230113 20:12:26 @agent_ppo2.py:146][0m 2564096 total steps have happened
[32m[20230113 20:12:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1252 --------------------------#
[32m[20230113 20:12:26 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:26 @agent_ppo2.py:186][0m |           0.0003 |           5.1839 |           7.9087 |
[32m[20230113 20:12:26 @agent_ppo2.py:186][0m |          -0.0063 |           4.3146 |           7.8948 |
[32m[20230113 20:12:26 @agent_ppo2.py:186][0m |          -0.0085 |           4.0654 |           7.8946 |
[32m[20230113 20:12:26 @agent_ppo2.py:186][0m |          -0.0107 |           3.8256 |           7.8899 |
[32m[20230113 20:12:26 @agent_ppo2.py:186][0m |          -0.0120 |           3.7357 |           7.8928 |
[32m[20230113 20:12:26 @agent_ppo2.py:186][0m |          -0.0128 |           3.6268 |           7.8880 |
[32m[20230113 20:12:26 @agent_ppo2.py:186][0m |          -0.0123 |           3.4744 |           7.8846 |
[32m[20230113 20:12:26 @agent_ppo2.py:186][0m |          -0.0145 |           3.4025 |           7.8805 |
[32m[20230113 20:12:26 @agent_ppo2.py:186][0m |          -0.0149 |           3.3282 |           7.8820 |
[32m[20230113 20:12:26 @agent_ppo2.py:186][0m |          -0.0148 |           3.2588 |           7.8767 |
[32m[20230113 20:12:26 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.90
[32m[20230113 20:12:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.34
[32m[20230113 20:12:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 145.02
[32m[20230113 20:12:27 @agent_ppo2.py:144][0m Total time:      27.91 min
[32m[20230113 20:12:27 @agent_ppo2.py:146][0m 2566144 total steps have happened
[32m[20230113 20:12:27 @agent_ppo2.py:122][0m #------------------------ Iteration 1253 --------------------------#
[32m[20230113 20:12:27 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:12:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:28 @agent_ppo2.py:186][0m |          -0.0004 |           6.0152 |           7.9020 |
[32m[20230113 20:12:28 @agent_ppo2.py:186][0m |          -0.0049 |           4.7362 |           7.8952 |
[32m[20230113 20:12:28 @agent_ppo2.py:186][0m |          -0.0073 |           4.3027 |           7.8894 |
[32m[20230113 20:12:28 @agent_ppo2.py:186][0m |          -0.0092 |           4.0375 |           7.8938 |
[32m[20230113 20:12:28 @agent_ppo2.py:186][0m |          -0.0098 |           3.7806 |           7.8949 |
[32m[20230113 20:12:28 @agent_ppo2.py:186][0m |          -0.0111 |           3.5838 |           7.8937 |
[32m[20230113 20:12:28 @agent_ppo2.py:186][0m |          -0.0129 |           3.4094 |           7.8959 |
[32m[20230113 20:12:28 @agent_ppo2.py:186][0m |          -0.0132 |           3.2695 |           7.8980 |
[32m[20230113 20:12:28 @agent_ppo2.py:186][0m |          -0.0135 |           3.1052 |           7.8996 |
[32m[20230113 20:12:28 @agent_ppo2.py:186][0m |          -0.0148 |           2.9913 |           7.8971 |
[32m[20230113 20:12:28 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:12:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.88
[32m[20230113 20:12:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.36
[32m[20230113 20:12:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.56
[32m[20230113 20:12:28 @agent_ppo2.py:144][0m Total time:      27.93 min
[32m[20230113 20:12:28 @agent_ppo2.py:146][0m 2568192 total steps have happened
[32m[20230113 20:12:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1254 --------------------------#
[32m[20230113 20:12:29 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:12:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:29 @agent_ppo2.py:186][0m |           0.0008 |          13.6690 |           7.9762 |
[32m[20230113 20:12:29 @agent_ppo2.py:186][0m |          -0.0081 |           6.4298 |           7.9708 |
[32m[20230113 20:12:29 @agent_ppo2.py:186][0m |          -0.0107 |           5.1376 |           7.9615 |
[32m[20230113 20:12:29 @agent_ppo2.py:186][0m |          -0.0133 |           4.4834 |           7.9564 |
[32m[20230113 20:12:29 @agent_ppo2.py:186][0m |          -0.0145 |           4.1196 |           7.9511 |
[32m[20230113 20:12:29 @agent_ppo2.py:186][0m |          -0.0146 |           3.8128 |           7.9547 |
[32m[20230113 20:12:29 @agent_ppo2.py:186][0m |          -0.0152 |           3.7535 |           7.9432 |
[32m[20230113 20:12:29 @agent_ppo2.py:186][0m |          -0.0177 |           3.4637 |           7.9496 |
[32m[20230113 20:12:29 @agent_ppo2.py:186][0m |          -0.0175 |           3.3200 |           7.9456 |
[32m[20230113 20:12:29 @agent_ppo2.py:186][0m |          -0.0179 |           3.1806 |           7.9480 |
[32m[20230113 20:12:29 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 20:12:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 135.73
[32m[20230113 20:12:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.35
[32m[20230113 20:12:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.93
[32m[20230113 20:12:29 @agent_ppo2.py:144][0m Total time:      27.95 min
[32m[20230113 20:12:29 @agent_ppo2.py:146][0m 2570240 total steps have happened
[32m[20230113 20:12:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1255 --------------------------#
[32m[20230113 20:12:30 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:12:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:30 @agent_ppo2.py:186][0m |           0.0019 |           5.1348 |           7.8855 |
[32m[20230113 20:12:30 @agent_ppo2.py:186][0m |          -0.0060 |           4.1343 |           7.8749 |
[32m[20230113 20:12:30 @agent_ppo2.py:186][0m |          -0.0105 |           3.7102 |           7.8680 |
[32m[20230113 20:12:30 @agent_ppo2.py:186][0m |          -0.0133 |           3.4728 |           7.8546 |
[32m[20230113 20:12:30 @agent_ppo2.py:186][0m |          -0.0078 |           3.3042 |           7.8583 |
[32m[20230113 20:12:30 @agent_ppo2.py:186][0m |          -0.0096 |           3.2005 |           7.8481 |
[32m[20230113 20:12:30 @agent_ppo2.py:186][0m |          -0.0085 |           3.0804 |           7.8533 |
[32m[20230113 20:12:30 @agent_ppo2.py:186][0m |          -0.0140 |           2.9875 |           7.8594 |
[32m[20230113 20:12:30 @agent_ppo2.py:186][0m |          -0.0206 |           2.9121 |           7.8484 |
[32m[20230113 20:12:30 @agent_ppo2.py:186][0m |          -0.0133 |           2.8516 |           7.8467 |
[32m[20230113 20:12:30 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:12:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.25
[32m[20230113 20:12:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.85
[32m[20230113 20:12:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.79
[32m[20230113 20:12:31 @agent_ppo2.py:144][0m Total time:      27.97 min
[32m[20230113 20:12:31 @agent_ppo2.py:146][0m 2572288 total steps have happened
[32m[20230113 20:12:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1256 --------------------------#
[32m[20230113 20:12:31 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:12:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:31 @agent_ppo2.py:186][0m |           0.0032 |           9.0222 |           7.9148 |
[32m[20230113 20:12:31 @agent_ppo2.py:186][0m |          -0.0060 |           6.7616 |           7.9127 |
[32m[20230113 20:12:31 @agent_ppo2.py:186][0m |          -0.0073 |           6.0451 |           7.9091 |
[32m[20230113 20:12:31 @agent_ppo2.py:186][0m |          -0.0077 |           5.6043 |           7.9126 |
[32m[20230113 20:12:32 @agent_ppo2.py:186][0m |          -0.0122 |           5.2360 |           7.9150 |
[32m[20230113 20:12:32 @agent_ppo2.py:186][0m |          -0.0123 |           4.9777 |           7.9031 |
[32m[20230113 20:12:32 @agent_ppo2.py:186][0m |          -0.0120 |           4.7595 |           7.9127 |
[32m[20230113 20:12:32 @agent_ppo2.py:186][0m |          -0.0157 |           4.5659 |           7.9125 |
[32m[20230113 20:12:32 @agent_ppo2.py:186][0m |          -0.0139 |           4.4407 |           7.9132 |
[32m[20230113 20:12:32 @agent_ppo2.py:186][0m |          -0.0152 |           4.2946 |           7.9190 |
[32m[20230113 20:12:32 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:12:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 222.15
[32m[20230113 20:12:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.27
[32m[20230113 20:12:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 134.78
[32m[20230113 20:12:32 @agent_ppo2.py:144][0m Total time:      27.99 min
[32m[20230113 20:12:32 @agent_ppo2.py:146][0m 2574336 total steps have happened
[32m[20230113 20:12:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1257 --------------------------#
[32m[20230113 20:12:33 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:12:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:33 @agent_ppo2.py:186][0m |           0.0011 |           7.5140 |           7.9537 |
[32m[20230113 20:12:33 @agent_ppo2.py:186][0m |          -0.0024 |           5.2636 |           7.9386 |
[32m[20230113 20:12:33 @agent_ppo2.py:186][0m |          -0.0030 |           4.5875 |           7.9362 |
[32m[20230113 20:12:33 @agent_ppo2.py:186][0m |          -0.0053 |           4.1903 |           7.9283 |
[32m[20230113 20:12:33 @agent_ppo2.py:186][0m |          -0.0132 |           3.9468 |           7.9334 |
[32m[20230113 20:12:33 @agent_ppo2.py:186][0m |          -0.0142 |           3.7273 |           7.9240 |
[32m[20230113 20:12:33 @agent_ppo2.py:186][0m |          -0.0218 |           3.6500 |           7.9279 |
[32m[20230113 20:12:33 @agent_ppo2.py:186][0m |          -0.0273 |           3.4946 |           7.9252 |
[32m[20230113 20:12:33 @agent_ppo2.py:186][0m |          -0.0126 |           3.4108 |           7.9223 |
[32m[20230113 20:12:33 @agent_ppo2.py:186][0m |          -0.0308 |           3.2763 |           7.9268 |
[32m[20230113 20:12:33 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:12:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.58
[32m[20230113 20:12:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.88
[32m[20230113 20:12:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.93
[32m[20230113 20:12:33 @agent_ppo2.py:144][0m Total time:      28.02 min
[32m[20230113 20:12:33 @agent_ppo2.py:146][0m 2576384 total steps have happened
[32m[20230113 20:12:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1258 --------------------------#
[32m[20230113 20:12:34 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:12:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:34 @agent_ppo2.py:186][0m |          -0.0008 |           5.0043 |           8.0130 |
[32m[20230113 20:12:34 @agent_ppo2.py:186][0m |          -0.0061 |           3.9444 |           8.0069 |
[32m[20230113 20:12:34 @agent_ppo2.py:186][0m |          -0.0047 |           3.6111 |           7.9999 |
[32m[20230113 20:12:34 @agent_ppo2.py:186][0m |          -0.0108 |           3.3358 |           7.9922 |
[32m[20230113 20:12:34 @agent_ppo2.py:186][0m |          -0.0117 |           3.1470 |           7.9887 |
[32m[20230113 20:12:34 @agent_ppo2.py:186][0m |          -0.0107 |           3.0422 |           7.9910 |
[32m[20230113 20:12:34 @agent_ppo2.py:186][0m |          -0.0134 |           2.9246 |           7.9922 |
[32m[20230113 20:12:34 @agent_ppo2.py:186][0m |          -0.0096 |           2.8868 |           7.9902 |
[32m[20230113 20:12:34 @agent_ppo2.py:186][0m |          -0.0137 |           2.7315 |           7.9861 |
[32m[20230113 20:12:34 @agent_ppo2.py:186][0m |          -0.0131 |           2.6867 |           7.9853 |
[32m[20230113 20:12:34 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:12:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.47
[32m[20230113 20:12:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.58
[32m[20230113 20:12:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.03
[32m[20230113 20:12:35 @agent_ppo2.py:144][0m Total time:      28.04 min
[32m[20230113 20:12:35 @agent_ppo2.py:146][0m 2578432 total steps have happened
[32m[20230113 20:12:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1259 --------------------------#
[32m[20230113 20:12:35 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:12:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:35 @agent_ppo2.py:186][0m |          -0.0007 |           5.5193 |           7.9515 |
[32m[20230113 20:12:35 @agent_ppo2.py:186][0m |          -0.0068 |           4.6159 |           7.9463 |
[32m[20230113 20:12:35 @agent_ppo2.py:186][0m |          -0.0089 |           4.2485 |           7.9329 |
[32m[20230113 20:12:35 @agent_ppo2.py:186][0m |          -0.0011 |           4.0690 |           7.9323 |
[32m[20230113 20:12:36 @agent_ppo2.py:186][0m |          -0.0064 |           3.8448 |           7.9430 |
[32m[20230113 20:12:36 @agent_ppo2.py:186][0m |          -0.0147 |           3.7629 |           7.9316 |
[32m[20230113 20:12:36 @agent_ppo2.py:186][0m |          -0.0149 |           3.6605 |           7.9370 |
[32m[20230113 20:12:36 @agent_ppo2.py:186][0m |          -0.0081 |           3.5727 |           7.9336 |
[32m[20230113 20:12:36 @agent_ppo2.py:186][0m |          -0.0124 |           3.4310 |           7.9313 |
[32m[20230113 20:12:36 @agent_ppo2.py:186][0m |          -0.0120 |           3.3414 |           7.9315 |
[32m[20230113 20:12:36 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:12:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.61
[32m[20230113 20:12:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.25
[32m[20230113 20:12:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.34
[32m[20230113 20:12:36 @agent_ppo2.py:144][0m Total time:      28.06 min
[32m[20230113 20:12:36 @agent_ppo2.py:146][0m 2580480 total steps have happened
[32m[20230113 20:12:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1260 --------------------------#
[32m[20230113 20:12:37 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:37 @agent_ppo2.py:186][0m |           0.0000 |           5.4111 |           7.9779 |
[32m[20230113 20:12:37 @agent_ppo2.py:186][0m |          -0.0072 |           4.2282 |           7.9880 |
[32m[20230113 20:12:37 @agent_ppo2.py:186][0m |          -0.0092 |           3.8677 |           7.9785 |
[32m[20230113 20:12:37 @agent_ppo2.py:186][0m |          -0.0111 |           3.5727 |           7.9784 |
[32m[20230113 20:12:37 @agent_ppo2.py:186][0m |          -0.0117 |           3.4053 |           7.9792 |
[32m[20230113 20:12:37 @agent_ppo2.py:186][0m |          -0.0125 |           3.2644 |           7.9739 |
[32m[20230113 20:12:37 @agent_ppo2.py:186][0m |          -0.0132 |           3.1602 |           7.9740 |
[32m[20230113 20:12:37 @agent_ppo2.py:186][0m |          -0.0123 |           3.0420 |           7.9744 |
[32m[20230113 20:12:37 @agent_ppo2.py:186][0m |          -0.0141 |           2.9647 |           7.9720 |
[32m[20230113 20:12:37 @agent_ppo2.py:186][0m |          -0.0131 |           2.8724 |           7.9781 |
[32m[20230113 20:12:37 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.96
[32m[20230113 20:12:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.28
[32m[20230113 20:12:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.61
[32m[20230113 20:12:37 @agent_ppo2.py:144][0m Total time:      28.08 min
[32m[20230113 20:12:37 @agent_ppo2.py:146][0m 2582528 total steps have happened
[32m[20230113 20:12:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1261 --------------------------#
[32m[20230113 20:12:38 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:12:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:38 @agent_ppo2.py:186][0m |          -0.0022 |           5.5390 |           8.1815 |
[32m[20230113 20:12:38 @agent_ppo2.py:186][0m |          -0.0056 |           4.6079 |           8.1750 |
[32m[20230113 20:12:38 @agent_ppo2.py:186][0m |          -0.0080 |           4.2705 |           8.1700 |
[32m[20230113 20:12:38 @agent_ppo2.py:186][0m |          -0.0090 |           4.0244 |           8.1706 |
[32m[20230113 20:12:38 @agent_ppo2.py:186][0m |          -0.0100 |           3.8916 |           8.1653 |
[32m[20230113 20:12:38 @agent_ppo2.py:186][0m |          -0.0114 |           3.7569 |           8.1672 |
[32m[20230113 20:12:38 @agent_ppo2.py:186][0m |          -0.0128 |           3.6694 |           8.1607 |
[32m[20230113 20:12:38 @agent_ppo2.py:186][0m |          -0.0130 |           3.5890 |           8.1630 |
[32m[20230113 20:12:38 @agent_ppo2.py:186][0m |          -0.0133 |           3.5144 |           8.1584 |
[32m[20230113 20:12:38 @agent_ppo2.py:186][0m |          -0.0138 |           3.4477 |           8.1586 |
[32m[20230113 20:12:38 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:12:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.60
[32m[20230113 20:12:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.61
[32m[20230113 20:12:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.94
[32m[20230113 20:12:39 @agent_ppo2.py:144][0m Total time:      28.10 min
[32m[20230113 20:12:39 @agent_ppo2.py:146][0m 2584576 total steps have happened
[32m[20230113 20:12:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1262 --------------------------#
[32m[20230113 20:12:39 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:12:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:39 @agent_ppo2.py:186][0m |           0.0034 |           5.6225 |           8.0243 |
[32m[20230113 20:12:39 @agent_ppo2.py:186][0m |           0.0040 |           4.7865 |           8.0197 |
[32m[20230113 20:12:39 @agent_ppo2.py:186][0m |           0.0010 |           4.5046 |           8.0142 |
[32m[20230113 20:12:39 @agent_ppo2.py:186][0m |          -0.0013 |           4.1641 |           8.0163 |
[32m[20230113 20:12:39 @agent_ppo2.py:186][0m |          -0.0125 |           4.0251 |           8.0277 |
[32m[20230113 20:12:39 @agent_ppo2.py:186][0m |          -0.0047 |           3.9439 |           8.0213 |
[32m[20230113 20:12:40 @agent_ppo2.py:186][0m |          -0.0191 |           3.8187 |           8.0204 |
[32m[20230113 20:12:40 @agent_ppo2.py:186][0m |          -0.0230 |           3.6975 |           8.0191 |
[32m[20230113 20:12:40 @agent_ppo2.py:186][0m |          -0.0074 |           3.6409 |           8.0194 |
[32m[20230113 20:12:40 @agent_ppo2.py:186][0m |          -0.0119 |           3.5364 |           8.0175 |
[32m[20230113 20:12:40 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:12:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.59
[32m[20230113 20:12:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.95
[32m[20230113 20:12:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 229.54
[32m[20230113 20:12:40 @agent_ppo2.py:144][0m Total time:      28.13 min
[32m[20230113 20:12:40 @agent_ppo2.py:146][0m 2586624 total steps have happened
[32m[20230113 20:12:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1263 --------------------------#
[32m[20230113 20:12:41 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:12:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:41 @agent_ppo2.py:186][0m |           0.0008 |           5.1510 |           8.1957 |
[32m[20230113 20:12:41 @agent_ppo2.py:186][0m |          -0.0049 |           3.8544 |           8.1875 |
[32m[20230113 20:12:41 @agent_ppo2.py:186][0m |          -0.0067 |           3.4579 |           8.1917 |
[32m[20230113 20:12:41 @agent_ppo2.py:186][0m |          -0.0088 |           3.2318 |           8.1787 |
[32m[20230113 20:12:41 @agent_ppo2.py:186][0m |          -0.0097 |           3.0251 |           8.1705 |
[32m[20230113 20:12:41 @agent_ppo2.py:186][0m |          -0.0101 |           2.9272 |           8.1712 |
[32m[20230113 20:12:41 @agent_ppo2.py:186][0m |          -0.0113 |           2.7830 |           8.1675 |
[32m[20230113 20:12:41 @agent_ppo2.py:186][0m |          -0.0121 |           2.6805 |           8.1706 |
[32m[20230113 20:12:41 @agent_ppo2.py:186][0m |          -0.0116 |           2.6031 |           8.1655 |
[32m[20230113 20:12:41 @agent_ppo2.py:186][0m |          -0.0123 |           2.5527 |           8.1624 |
[32m[20230113 20:12:41 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:12:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.63
[32m[20230113 20:12:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.96
[32m[20230113 20:12:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.73
[32m[20230113 20:12:41 @agent_ppo2.py:144][0m Total time:      28.15 min
[32m[20230113 20:12:41 @agent_ppo2.py:146][0m 2588672 total steps have happened
[32m[20230113 20:12:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1264 --------------------------#
[32m[20230113 20:12:42 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:42 @agent_ppo2.py:186][0m |          -0.0049 |           5.9615 |           8.0216 |
[32m[20230113 20:12:42 @agent_ppo2.py:186][0m |          -0.0063 |           4.6564 |           8.0109 |
[32m[20230113 20:12:42 @agent_ppo2.py:186][0m |          -0.0107 |           4.2390 |           8.0013 |
[32m[20230113 20:12:42 @agent_ppo2.py:186][0m |          -0.0145 |           3.9743 |           8.0056 |
[32m[20230113 20:12:42 @agent_ppo2.py:186][0m |          -0.0162 |           3.7846 |           7.9976 |
[32m[20230113 20:12:42 @agent_ppo2.py:186][0m |          -0.0145 |           3.6135 |           7.9958 |
[32m[20230113 20:12:42 @agent_ppo2.py:186][0m |          -0.0140 |           3.5020 |           7.9937 |
[32m[20230113 20:12:42 @agent_ppo2.py:186][0m |          -0.0174 |           3.3889 |           7.9946 |
[32m[20230113 20:12:42 @agent_ppo2.py:186][0m |          -0.0101 |           3.3292 |           7.9908 |
[32m[20230113 20:12:42 @agent_ppo2.py:186][0m |          -0.0157 |           3.2133 |           7.9906 |
[32m[20230113 20:12:42 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:12:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.84
[32m[20230113 20:12:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.32
[32m[20230113 20:12:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.97
[32m[20230113 20:12:43 @agent_ppo2.py:144][0m Total time:      28.17 min
[32m[20230113 20:12:43 @agent_ppo2.py:146][0m 2590720 total steps have happened
[32m[20230113 20:12:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1265 --------------------------#
[32m[20230113 20:12:43 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:12:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:43 @agent_ppo2.py:186][0m |          -0.0004 |           6.5041 |           7.9069 |
[32m[20230113 20:12:43 @agent_ppo2.py:186][0m |          -0.0052 |           4.8628 |           7.9041 |
[32m[20230113 20:12:43 @agent_ppo2.py:186][0m |          -0.0072 |           4.3438 |           7.8908 |
[32m[20230113 20:12:43 @agent_ppo2.py:186][0m |          -0.0085 |           3.9755 |           7.8946 |
[32m[20230113 20:12:43 @agent_ppo2.py:186][0m |          -0.0120 |           3.7127 |           7.8829 |
[32m[20230113 20:12:43 @agent_ppo2.py:186][0m |          -0.0097 |           3.5587 |           7.8882 |
[32m[20230113 20:12:43 @agent_ppo2.py:186][0m |          -0.0111 |           3.3866 |           7.8813 |
[32m[20230113 20:12:44 @agent_ppo2.py:186][0m |          -0.0111 |           3.2576 |           7.8838 |
[32m[20230113 20:12:44 @agent_ppo2.py:186][0m |          -0.0130 |           3.1925 |           7.8868 |
[32m[20230113 20:12:44 @agent_ppo2.py:186][0m |          -0.0111 |           3.1041 |           7.8843 |
[32m[20230113 20:12:44 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.39
[32m[20230113 20:12:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.18
[32m[20230113 20:12:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.95
[32m[20230113 20:12:44 @agent_ppo2.py:144][0m Total time:      28.19 min
[32m[20230113 20:12:44 @agent_ppo2.py:146][0m 2592768 total steps have happened
[32m[20230113 20:12:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1266 --------------------------#
[32m[20230113 20:12:44 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:12:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:45 @agent_ppo2.py:186][0m |           0.0030 |           6.3293 |           8.0589 |
[32m[20230113 20:12:45 @agent_ppo2.py:186][0m |          -0.0048 |           4.8871 |           8.0404 |
[32m[20230113 20:12:45 @agent_ppo2.py:186][0m |          -0.0142 |           4.4856 |           8.0387 |
[32m[20230113 20:12:45 @agent_ppo2.py:186][0m |          -0.0111 |           4.0911 |           8.0266 |
[32m[20230113 20:12:45 @agent_ppo2.py:186][0m |          -0.0156 |           3.9330 |           8.0378 |
[32m[20230113 20:12:45 @agent_ppo2.py:186][0m |          -0.0141 |           3.7806 |           8.0321 |
[32m[20230113 20:12:45 @agent_ppo2.py:186][0m |          -0.0152 |           3.5747 |           8.0278 |
[32m[20230113 20:12:45 @agent_ppo2.py:186][0m |          -0.0141 |           3.4614 |           8.0253 |
[32m[20230113 20:12:45 @agent_ppo2.py:186][0m |          -0.0111 |           3.3629 |           8.0198 |
[32m[20230113 20:12:45 @agent_ppo2.py:186][0m |          -0.0137 |           3.2576 |           8.0202 |
[32m[20230113 20:12:45 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:12:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.13
[32m[20230113 20:12:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.13
[32m[20230113 20:12:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.11
[32m[20230113 20:12:45 @agent_ppo2.py:144][0m Total time:      28.21 min
[32m[20230113 20:12:45 @agent_ppo2.py:146][0m 2594816 total steps have happened
[32m[20230113 20:12:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1267 --------------------------#
[32m[20230113 20:12:46 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 20:12:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:46 @agent_ppo2.py:186][0m |           0.0009 |          16.2321 |           7.9309 |
[32m[20230113 20:12:46 @agent_ppo2.py:186][0m |          -0.0036 |           9.1801 |           7.9131 |
[32m[20230113 20:12:46 @agent_ppo2.py:186][0m |          -0.0054 |           6.8271 |           7.9185 |
[32m[20230113 20:12:46 @agent_ppo2.py:186][0m |          -0.0061 |           5.8519 |           7.9156 |
[32m[20230113 20:12:46 @agent_ppo2.py:186][0m |          -0.0070 |           5.8776 |           7.9193 |
[32m[20230113 20:12:46 @agent_ppo2.py:186][0m |          -0.0090 |           5.1514 |           7.9113 |
[32m[20230113 20:12:46 @agent_ppo2.py:186][0m |          -0.0100 |           4.8573 |           7.9074 |
[32m[20230113 20:12:46 @agent_ppo2.py:186][0m |          -0.0102 |           4.6616 |           7.9055 |
[32m[20230113 20:12:46 @agent_ppo2.py:186][0m |          -0.0116 |           4.3645 |           7.9085 |
[32m[20230113 20:12:46 @agent_ppo2.py:186][0m |          -0.0129 |           4.2866 |           7.9061 |
[32m[20230113 20:12:46 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 20:12:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 130.32
[32m[20230113 20:12:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 222.81
[32m[20230113 20:12:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.43
[32m[20230113 20:12:47 @agent_ppo2.py:144][0m Total time:      28.23 min
[32m[20230113 20:12:47 @agent_ppo2.py:146][0m 2596864 total steps have happened
[32m[20230113 20:12:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1268 --------------------------#
[32m[20230113 20:12:47 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:12:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:47 @agent_ppo2.py:186][0m |           0.0029 |           6.0533 |           7.7803 |
[32m[20230113 20:12:47 @agent_ppo2.py:186][0m |          -0.0084 |           4.3274 |           7.7903 |
[32m[20230113 20:12:47 @agent_ppo2.py:186][0m |          -0.0066 |           3.9659 |           7.7823 |
[32m[20230113 20:12:47 @agent_ppo2.py:186][0m |          -0.0115 |           3.7490 |           7.7807 |
[32m[20230113 20:12:47 @agent_ppo2.py:186][0m |          -0.0089 |           3.5818 |           7.7761 |
[32m[20230113 20:12:47 @agent_ppo2.py:186][0m |          -0.0112 |           3.4246 |           7.7766 |
[32m[20230113 20:12:47 @agent_ppo2.py:186][0m |          -0.0113 |           3.3039 |           7.7788 |
[32m[20230113 20:12:47 @agent_ppo2.py:186][0m |          -0.0111 |           3.3104 |           7.7801 |
[32m[20230113 20:12:47 @agent_ppo2.py:186][0m |          -0.0233 |           3.1474 |           7.7770 |
[32m[20230113 20:12:47 @agent_ppo2.py:186][0m |          -0.0127 |           3.1026 |           7.7725 |
[32m[20230113 20:12:47 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:12:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.48
[32m[20230113 20:12:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.40
[32m[20230113 20:12:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.31
[32m[20230113 20:12:48 @agent_ppo2.py:144][0m Total time:      28.26 min
[32m[20230113 20:12:48 @agent_ppo2.py:146][0m 2598912 total steps have happened
[32m[20230113 20:12:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1269 --------------------------#
[32m[20230113 20:12:48 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:12:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:48 @agent_ppo2.py:186][0m |           0.0020 |           6.6036 |           7.9389 |
[32m[20230113 20:12:49 @agent_ppo2.py:186][0m |           0.0054 |           5.0876 |           7.9389 |
[32m[20230113 20:12:49 @agent_ppo2.py:186][0m |          -0.0078 |           4.4433 |           7.9201 |
[32m[20230113 20:12:49 @agent_ppo2.py:186][0m |          -0.0111 |           4.1725 |           7.9231 |
[32m[20230113 20:12:49 @agent_ppo2.py:186][0m |          -0.0108 |           3.9748 |           7.9284 |
[32m[20230113 20:12:49 @agent_ppo2.py:186][0m |          -0.0112 |           3.8649 |           7.9240 |
[32m[20230113 20:12:49 @agent_ppo2.py:186][0m |          -0.0166 |           3.7138 |           7.9226 |
[32m[20230113 20:12:49 @agent_ppo2.py:186][0m |          -0.0043 |           3.6595 |           7.9168 |
[32m[20230113 20:12:49 @agent_ppo2.py:186][0m |          -0.0066 |           3.5430 |           7.9176 |
[32m[20230113 20:12:49 @agent_ppo2.py:186][0m |          -0.0162 |           3.4300 |           7.9101 |
[32m[20230113 20:12:49 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:12:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.70
[32m[20230113 20:12:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.12
[32m[20230113 20:12:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.88
[32m[20230113 20:12:49 @agent_ppo2.py:144][0m Total time:      28.28 min
[32m[20230113 20:12:49 @agent_ppo2.py:146][0m 2600960 total steps have happened
[32m[20230113 20:12:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1270 --------------------------#
[32m[20230113 20:12:50 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:12:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:50 @agent_ppo2.py:186][0m |          -0.0017 |          19.9578 |           8.1743 |
[32m[20230113 20:12:50 @agent_ppo2.py:186][0m |          -0.0055 |           8.4320 |           8.1662 |
[32m[20230113 20:12:50 @agent_ppo2.py:186][0m |          -0.0059 |           5.9715 |           8.1677 |
[32m[20230113 20:12:50 @agent_ppo2.py:186][0m |          -0.0074 |           4.9495 |           8.1609 |
[32m[20230113 20:12:50 @agent_ppo2.py:186][0m |          -0.0079 |           4.3821 |           8.1660 |
[32m[20230113 20:12:50 @agent_ppo2.py:186][0m |          -0.0085 |           3.9054 |           8.1590 |
[32m[20230113 20:12:50 @agent_ppo2.py:186][0m |          -0.0095 |           3.6394 |           8.1667 |
[32m[20230113 20:12:50 @agent_ppo2.py:186][0m |          -0.0094 |           3.4119 |           8.1639 |
[32m[20230113 20:12:50 @agent_ppo2.py:186][0m |          -0.0109 |           3.2380 |           8.1614 |
[32m[20230113 20:12:50 @agent_ppo2.py:186][0m |          -0.0113 |           3.0557 |           8.1667 |
[32m[20230113 20:12:50 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:12:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 163.99
[32m[20230113 20:12:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.82
[32m[20230113 20:12:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.92
[32m[20230113 20:12:50 @agent_ppo2.py:144][0m Total time:      28.30 min
[32m[20230113 20:12:50 @agent_ppo2.py:146][0m 2603008 total steps have happened
[32m[20230113 20:12:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1271 --------------------------#
[32m[20230113 20:12:51 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:12:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:51 @agent_ppo2.py:186][0m |           0.0010 |           7.8500 |           8.1070 |
[32m[20230113 20:12:51 @agent_ppo2.py:186][0m |          -0.0055 |           5.3092 |           8.0943 |
[32m[20230113 20:12:51 @agent_ppo2.py:186][0m |          -0.0085 |           4.5975 |           8.0997 |
[32m[20230113 20:12:51 @agent_ppo2.py:186][0m |          -0.0096 |           4.2306 |           8.1072 |
[32m[20230113 20:12:51 @agent_ppo2.py:186][0m |          -0.0118 |           4.0021 |           8.0969 |
[32m[20230113 20:12:51 @agent_ppo2.py:186][0m |          -0.0125 |           3.8219 |           8.1019 |
[32m[20230113 20:12:51 @agent_ppo2.py:186][0m |          -0.0131 |           3.7296 |           8.0951 |
[32m[20230113 20:12:51 @agent_ppo2.py:186][0m |          -0.0142 |           3.5749 |           8.1039 |
[32m[20230113 20:12:51 @agent_ppo2.py:186][0m |          -0.0147 |           3.4779 |           8.1100 |
[32m[20230113 20:12:51 @agent_ppo2.py:186][0m |          -0.0157 |           3.4002 |           8.1069 |
[32m[20230113 20:12:51 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:12:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.76
[32m[20230113 20:12:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.03
[32m[20230113 20:12:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.95
[32m[20230113 20:12:52 @agent_ppo2.py:144][0m Total time:      28.32 min
[32m[20230113 20:12:52 @agent_ppo2.py:146][0m 2605056 total steps have happened
[32m[20230113 20:12:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1272 --------------------------#
[32m[20230113 20:12:52 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:12:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:52 @agent_ppo2.py:186][0m |          -0.0013 |           5.6756 |           7.7884 |
[32m[20230113 20:12:52 @agent_ppo2.py:186][0m |          -0.0076 |           4.4891 |           7.7633 |
[32m[20230113 20:12:52 @agent_ppo2.py:186][0m |          -0.0071 |           4.0011 |           7.7539 |
[32m[20230113 20:12:52 @agent_ppo2.py:186][0m |          -0.0093 |           3.6785 |           7.7444 |
[32m[20230113 20:12:53 @agent_ppo2.py:186][0m |          -0.0135 |           3.4643 |           7.7427 |
[32m[20230113 20:12:53 @agent_ppo2.py:186][0m |          -0.0120 |           3.3046 |           7.7413 |
[32m[20230113 20:12:53 @agent_ppo2.py:186][0m |          -0.0061 |           3.2257 |           7.7447 |
[32m[20230113 20:12:53 @agent_ppo2.py:186][0m |          -0.0146 |           3.0975 |           7.7380 |
[32m[20230113 20:12:53 @agent_ppo2.py:186][0m |          -0.0187 |           2.9869 |           7.7364 |
[32m[20230113 20:12:53 @agent_ppo2.py:186][0m |          -0.0170 |           2.9269 |           7.7317 |
[32m[20230113 20:12:53 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.11
[32m[20230113 20:12:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.93
[32m[20230113 20:12:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.13
[32m[20230113 20:12:53 @agent_ppo2.py:144][0m Total time:      28.34 min
[32m[20230113 20:12:53 @agent_ppo2.py:146][0m 2607104 total steps have happened
[32m[20230113 20:12:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1273 --------------------------#
[32m[20230113 20:12:54 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:12:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:54 @agent_ppo2.py:186][0m |           0.0006 |           5.9834 |           8.0648 |
[32m[20230113 20:12:54 @agent_ppo2.py:186][0m |          -0.0047 |           5.0038 |           8.0519 |
[32m[20230113 20:12:54 @agent_ppo2.py:186][0m |          -0.0057 |           4.5022 |           8.0453 |
[32m[20230113 20:12:54 @agent_ppo2.py:186][0m |          -0.0079 |           4.2863 |           8.0415 |
[32m[20230113 20:12:54 @agent_ppo2.py:186][0m |          -0.0094 |           4.1202 |           8.0520 |
[32m[20230113 20:12:54 @agent_ppo2.py:186][0m |          -0.0102 |           4.0189 |           8.0478 |
[32m[20230113 20:12:54 @agent_ppo2.py:186][0m |          -0.0112 |           3.9280 |           8.0489 |
[32m[20230113 20:12:54 @agent_ppo2.py:186][0m |          -0.0118 |           3.8253 |           8.0482 |
[32m[20230113 20:12:54 @agent_ppo2.py:186][0m |          -0.0122 |           3.7507 |           8.0462 |
[32m[20230113 20:12:54 @agent_ppo2.py:186][0m |          -0.0119 |           3.6507 |           8.0458 |
[32m[20230113 20:12:54 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.87
[32m[20230113 20:12:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.94
[32m[20230113 20:12:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.41
[32m[20230113 20:12:54 @agent_ppo2.py:144][0m Total time:      28.36 min
[32m[20230113 20:12:54 @agent_ppo2.py:146][0m 2609152 total steps have happened
[32m[20230113 20:12:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1274 --------------------------#
[32m[20230113 20:12:55 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:12:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:55 @agent_ppo2.py:186][0m |          -0.0001 |          13.9692 |           7.8224 |
[32m[20230113 20:12:55 @agent_ppo2.py:186][0m |          -0.0036 |           5.6521 |           7.8098 |
[32m[20230113 20:12:55 @agent_ppo2.py:186][0m |          -0.0078 |           4.8705 |           7.8007 |
[32m[20230113 20:12:55 @agent_ppo2.py:186][0m |          -0.0099 |           4.4865 |           7.8012 |
[32m[20230113 20:12:55 @agent_ppo2.py:186][0m |          -0.0046 |           4.2817 |           7.8033 |
[32m[20230113 20:12:55 @agent_ppo2.py:186][0m |          -0.0111 |           4.1864 |           7.7972 |
[32m[20230113 20:12:55 @agent_ppo2.py:186][0m |          -0.0149 |           3.8607 |           7.7911 |
[32m[20230113 20:12:55 @agent_ppo2.py:186][0m |          -0.0151 |           3.7328 |           7.7943 |
[32m[20230113 20:12:55 @agent_ppo2.py:186][0m |          -0.0080 |           3.6007 |           7.7913 |
[32m[20230113 20:12:55 @agent_ppo2.py:186][0m |          -0.0194 |           3.5387 |           7.7936 |
[32m[20230113 20:12:55 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 20:12:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 121.99
[32m[20230113 20:12:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.39
[32m[20230113 20:12:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.36
[32m[20230113 20:12:56 @agent_ppo2.py:144][0m Total time:      28.38 min
[32m[20230113 20:12:56 @agent_ppo2.py:146][0m 2611200 total steps have happened
[32m[20230113 20:12:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1275 --------------------------#
[32m[20230113 20:12:56 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:12:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:56 @agent_ppo2.py:186][0m |          -0.0013 |           5.5709 |           7.8132 |
[32m[20230113 20:12:56 @agent_ppo2.py:186][0m |          -0.0017 |           4.0935 |           7.8154 |
[32m[20230113 20:12:56 @agent_ppo2.py:186][0m |          -0.0001 |           3.7690 |           7.8172 |
[32m[20230113 20:12:56 @agent_ppo2.py:186][0m |          -0.0032 |           3.5793 |           7.8219 |
[32m[20230113 20:12:56 @agent_ppo2.py:186][0m |          -0.0044 |           3.4464 |           7.8204 |
[32m[20230113 20:12:56 @agent_ppo2.py:186][0m |          -0.0056 |           3.2635 |           7.8309 |
[32m[20230113 20:12:56 @agent_ppo2.py:186][0m |          -0.0098 |           3.1762 |           7.8299 |
[32m[20230113 20:12:56 @agent_ppo2.py:186][0m |          -0.0045 |           3.1791 |           7.8358 |
[32m[20230113 20:12:56 @agent_ppo2.py:186][0m |          -0.0070 |           3.0208 |           7.8391 |
[32m[20230113 20:12:56 @agent_ppo2.py:186][0m |          -0.0142 |           2.9732 |           7.8388 |
[32m[20230113 20:12:56 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:12:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.33
[32m[20230113 20:12:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.54
[32m[20230113 20:12:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.29
[32m[20230113 20:12:57 @agent_ppo2.py:144][0m Total time:      28.41 min
[32m[20230113 20:12:57 @agent_ppo2.py:146][0m 2613248 total steps have happened
[32m[20230113 20:12:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1276 --------------------------#
[32m[20230113 20:12:57 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:12:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:57 @agent_ppo2.py:186][0m |           0.0018 |          19.8408 |           7.8460 |
[32m[20230113 20:12:57 @agent_ppo2.py:186][0m |          -0.0089 |           8.6317 |           7.8432 |
[32m[20230113 20:12:57 @agent_ppo2.py:186][0m |          -0.0116 |           6.1361 |           7.8353 |
[32m[20230113 20:12:58 @agent_ppo2.py:186][0m |          -0.0135 |           5.0356 |           7.8319 |
[32m[20230113 20:12:58 @agent_ppo2.py:186][0m |          -0.0156 |           4.3019 |           7.8275 |
[32m[20230113 20:12:58 @agent_ppo2.py:186][0m |          -0.0144 |           3.8552 |           7.8235 |
[32m[20230113 20:12:58 @agent_ppo2.py:186][0m |          -0.0140 |           3.5663 |           7.8151 |
[32m[20230113 20:12:58 @agent_ppo2.py:186][0m |          -0.0106 |           3.4266 |           7.8142 |
[32m[20230113 20:12:58 @agent_ppo2.py:186][0m |          -0.0157 |           3.1675 |           7.8142 |
[32m[20230113 20:12:58 @agent_ppo2.py:186][0m |          -0.0130 |           3.1789 |           7.8045 |
[32m[20230113 20:12:58 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:12:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 150.49
[32m[20230113 20:12:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.46
[32m[20230113 20:12:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.18
[32m[20230113 20:12:58 @agent_ppo2.py:144][0m Total time:      28.43 min
[32m[20230113 20:12:58 @agent_ppo2.py:146][0m 2615296 total steps have happened
[32m[20230113 20:12:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1277 --------------------------#
[32m[20230113 20:12:59 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:12:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:12:59 @agent_ppo2.py:186][0m |           0.0031 |           6.6846 |           7.9234 |
[32m[20230113 20:12:59 @agent_ppo2.py:186][0m |          -0.0023 |           4.5718 |           7.8964 |
[32m[20230113 20:12:59 @agent_ppo2.py:186][0m |          -0.0038 |           4.1208 |           7.8978 |
[32m[20230113 20:12:59 @agent_ppo2.py:186][0m |          -0.0136 |           3.7189 |           7.8999 |
[32m[20230113 20:12:59 @agent_ppo2.py:186][0m |          -0.0077 |           3.4359 |           7.8963 |
[32m[20230113 20:12:59 @agent_ppo2.py:186][0m |          -0.0083 |           3.3028 |           7.8987 |
[32m[20230113 20:12:59 @agent_ppo2.py:186][0m |          -0.0141 |           3.1455 |           7.8983 |
[32m[20230113 20:12:59 @agent_ppo2.py:186][0m |          -0.0119 |           3.0370 |           7.8970 |
[32m[20230113 20:12:59 @agent_ppo2.py:186][0m |          -0.0113 |           2.9643 |           7.9009 |
[32m[20230113 20:12:59 @agent_ppo2.py:186][0m |          -0.0134 |           2.8608 |           7.8978 |
[32m[20230113 20:12:59 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:12:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.80
[32m[20230113 20:12:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.53
[32m[20230113 20:12:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.08
[32m[20230113 20:12:59 @agent_ppo2.py:144][0m Total time:      28.45 min
[32m[20230113 20:12:59 @agent_ppo2.py:146][0m 2617344 total steps have happened
[32m[20230113 20:12:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1278 --------------------------#
[32m[20230113 20:13:00 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:13:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:00 @agent_ppo2.py:186][0m |          -0.0050 |          11.4996 |           7.9754 |
[32m[20230113 20:13:00 @agent_ppo2.py:186][0m |          -0.0086 |           5.9016 |           7.9511 |
[32m[20230113 20:13:00 @agent_ppo2.py:186][0m |          -0.0106 |           4.5406 |           7.9541 |
[32m[20230113 20:13:00 @agent_ppo2.py:186][0m |          -0.0131 |           3.9561 |           7.9503 |
[32m[20230113 20:13:00 @agent_ppo2.py:186][0m |          -0.0069 |           3.4990 |           7.9460 |
[32m[20230113 20:13:00 @agent_ppo2.py:186][0m |          -0.0132 |           3.3389 |           7.9516 |
[32m[20230113 20:13:00 @agent_ppo2.py:186][0m |          -0.0130 |           3.1230 |           7.9453 |
[32m[20230113 20:13:00 @agent_ppo2.py:186][0m |          -0.0154 |           2.9879 |           7.9306 |
[32m[20230113 20:13:00 @agent_ppo2.py:186][0m |          -0.0142 |           2.8936 |           7.9309 |
[32m[20230113 20:13:00 @agent_ppo2.py:186][0m |          -0.0198 |           2.7863 |           7.9246 |
[32m[20230113 20:13:00 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 20:13:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 147.93
[32m[20230113 20:13:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.79
[32m[20230113 20:13:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.57
[32m[20230113 20:13:01 @agent_ppo2.py:144][0m Total time:      28.47 min
[32m[20230113 20:13:01 @agent_ppo2.py:146][0m 2619392 total steps have happened
[32m[20230113 20:13:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1279 --------------------------#
[32m[20230113 20:13:01 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 20:13:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:01 @agent_ppo2.py:186][0m |          -0.0007 |           9.5026 |           7.7300 |
[32m[20230113 20:13:01 @agent_ppo2.py:186][0m |          -0.0057 |           6.1324 |           7.7170 |
[32m[20230113 20:13:01 @agent_ppo2.py:186][0m |          -0.0134 |           5.0102 |           7.7030 |
[32m[20230113 20:13:01 @agent_ppo2.py:186][0m |          -0.0090 |           4.5369 |           7.6950 |
[32m[20230113 20:13:01 @agent_ppo2.py:186][0m |          -0.0143 |           4.0899 |           7.7019 |
[32m[20230113 20:13:01 @agent_ppo2.py:186][0m |          -0.0133 |           3.9182 |           7.6885 |
[32m[20230113 20:13:01 @agent_ppo2.py:186][0m |          -0.0173 |           3.7586 |           7.6793 |
[32m[20230113 20:13:01 @agent_ppo2.py:186][0m |          -0.0152 |           3.5470 |           7.6856 |
[32m[20230113 20:13:01 @agent_ppo2.py:186][0m |          -0.0176 |           3.3661 |           7.6821 |
[32m[20230113 20:13:01 @agent_ppo2.py:186][0m |          -0.0164 |           3.2596 |           7.6787 |
[32m[20230113 20:13:01 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:13:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 167.67
[32m[20230113 20:13:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.81
[32m[20230113 20:13:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.58
[32m[20230113 20:13:02 @agent_ppo2.py:144][0m Total time:      28.49 min
[32m[20230113 20:13:02 @agent_ppo2.py:146][0m 2621440 total steps have happened
[32m[20230113 20:13:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1280 --------------------------#
[32m[20230113 20:13:02 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:13:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:02 @agent_ppo2.py:186][0m |           0.0117 |           8.6490 |           7.8502 |
[32m[20230113 20:13:02 @agent_ppo2.py:186][0m |          -0.0126 |           6.3692 |           7.8440 |
[32m[20230113 20:13:02 @agent_ppo2.py:186][0m |           0.0035 |           5.8583 |           7.8360 |
[32m[20230113 20:13:03 @agent_ppo2.py:186][0m |          -0.0076 |           5.2064 |           7.8375 |
[32m[20230113 20:13:03 @agent_ppo2.py:186][0m |           0.0002 |           4.9439 |           7.8306 |
[32m[20230113 20:13:03 @agent_ppo2.py:186][0m |          -0.0148 |           4.5898 |           7.8269 |
[32m[20230113 20:13:03 @agent_ppo2.py:186][0m |          -0.0172 |           4.4262 |           7.8391 |
[32m[20230113 20:13:03 @agent_ppo2.py:186][0m |          -0.0110 |           4.2115 |           7.8358 |
[32m[20230113 20:13:03 @agent_ppo2.py:186][0m |          -0.0122 |           4.0886 |           7.8395 |
[32m[20230113 20:13:03 @agent_ppo2.py:186][0m |          -0.0166 |           3.9858 |           7.8444 |
[32m[20230113 20:13:03 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:13:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.42
[32m[20230113 20:13:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.93
[32m[20230113 20:13:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.88
[32m[20230113 20:13:03 @agent_ppo2.py:144][0m Total time:      28.51 min
[32m[20230113 20:13:03 @agent_ppo2.py:146][0m 2623488 total steps have happened
[32m[20230113 20:13:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1281 --------------------------#
[32m[20230113 20:13:04 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:13:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:04 @agent_ppo2.py:186][0m |           0.0024 |           6.4641 |           7.9780 |
[32m[20230113 20:13:04 @agent_ppo2.py:186][0m |          -0.0046 |           5.0804 |           7.9785 |
[32m[20230113 20:13:04 @agent_ppo2.py:186][0m |          -0.0078 |           4.5820 |           7.9696 |
[32m[20230113 20:13:04 @agent_ppo2.py:186][0m |          -0.0088 |           4.3293 |           7.9648 |
[32m[20230113 20:13:04 @agent_ppo2.py:186][0m |          -0.0096 |           4.0657 |           7.9622 |
[32m[20230113 20:13:04 @agent_ppo2.py:186][0m |          -0.0106 |           3.9252 |           7.9610 |
[32m[20230113 20:13:04 @agent_ppo2.py:186][0m |          -0.0128 |           3.8023 |           7.9569 |
[32m[20230113 20:13:04 @agent_ppo2.py:186][0m |          -0.0121 |           3.6246 |           7.9577 |
[32m[20230113 20:13:04 @agent_ppo2.py:186][0m |          -0.0130 |           3.5767 |           7.9549 |
[32m[20230113 20:13:04 @agent_ppo2.py:186][0m |          -0.0132 |           3.4332 |           7.9514 |
[32m[20230113 20:13:04 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.58
[32m[20230113 20:13:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.66
[32m[20230113 20:13:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.05
[32m[20230113 20:13:04 @agent_ppo2.py:144][0m Total time:      28.53 min
[32m[20230113 20:13:04 @agent_ppo2.py:146][0m 2625536 total steps have happened
[32m[20230113 20:13:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1282 --------------------------#
[32m[20230113 20:13:05 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:13:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:05 @agent_ppo2.py:186][0m |          -0.0000 |           6.6939 |           7.9258 |
[32m[20230113 20:13:05 @agent_ppo2.py:186][0m |          -0.0071 |           4.6175 |           7.9014 |
[32m[20230113 20:13:05 @agent_ppo2.py:186][0m |          -0.0106 |           4.0456 |           7.9073 |
[32m[20230113 20:13:05 @agent_ppo2.py:186][0m |          -0.0129 |           3.7491 |           7.8916 |
[32m[20230113 20:13:05 @agent_ppo2.py:186][0m |          -0.0127 |           3.5285 |           7.8964 |
[32m[20230113 20:13:05 @agent_ppo2.py:186][0m |          -0.0134 |           3.3676 |           7.8876 |
[32m[20230113 20:13:05 @agent_ppo2.py:186][0m |          -0.0151 |           3.2292 |           7.8961 |
[32m[20230113 20:13:05 @agent_ppo2.py:186][0m |          -0.0161 |           3.1249 |           7.8952 |
[32m[20230113 20:13:05 @agent_ppo2.py:186][0m |          -0.0170 |           3.0526 |           7.8899 |
[32m[20230113 20:13:05 @agent_ppo2.py:186][0m |          -0.0176 |           2.9761 |           7.8917 |
[32m[20230113 20:13:05 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:13:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.78
[32m[20230113 20:13:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.05
[32m[20230113 20:13:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.22
[32m[20230113 20:13:06 @agent_ppo2.py:144][0m Total time:      28.55 min
[32m[20230113 20:13:06 @agent_ppo2.py:146][0m 2627584 total steps have happened
[32m[20230113 20:13:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1283 --------------------------#
[32m[20230113 20:13:06 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:06 @agent_ppo2.py:186][0m |           0.0050 |           5.7313 |           7.8875 |
[32m[20230113 20:13:06 @agent_ppo2.py:186][0m |          -0.0070 |           4.5244 |           7.8866 |
[32m[20230113 20:13:06 @agent_ppo2.py:186][0m |          -0.0029 |           4.0301 |           7.8813 |
[32m[20230113 20:13:06 @agent_ppo2.py:186][0m |          -0.0024 |           3.7996 |           7.8793 |
[32m[20230113 20:13:06 @agent_ppo2.py:186][0m |          -0.0094 |           3.5226 |           7.8780 |
[32m[20230113 20:13:07 @agent_ppo2.py:186][0m |          -0.0096 |           3.3608 |           7.8735 |
[32m[20230113 20:13:07 @agent_ppo2.py:186][0m |          -0.0129 |           3.2792 |           7.8784 |
[32m[20230113 20:13:07 @agent_ppo2.py:186][0m |          -0.0101 |           3.1394 |           7.8824 |
[32m[20230113 20:13:07 @agent_ppo2.py:186][0m |          -0.0150 |           3.0657 |           7.8760 |
[32m[20230113 20:13:07 @agent_ppo2.py:186][0m |          -0.0184 |           2.9781 |           7.8812 |
[32m[20230113 20:13:07 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.83
[32m[20230113 20:13:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.91
[32m[20230113 20:13:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.19
[32m[20230113 20:13:07 @agent_ppo2.py:144][0m Total time:      28.58 min
[32m[20230113 20:13:07 @agent_ppo2.py:146][0m 2629632 total steps have happened
[32m[20230113 20:13:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1284 --------------------------#
[32m[20230113 20:13:08 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:13:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:08 @agent_ppo2.py:186][0m |           0.0001 |           5.7511 |           7.9140 |
[32m[20230113 20:13:08 @agent_ppo2.py:186][0m |          -0.0053 |           4.3920 |           7.9091 |
[32m[20230113 20:13:08 @agent_ppo2.py:186][0m |          -0.0120 |           3.8330 |           7.9065 |
[32m[20230113 20:13:08 @agent_ppo2.py:186][0m |          -0.0125 |           3.4602 |           7.9165 |
[32m[20230113 20:13:08 @agent_ppo2.py:186][0m |          -0.0108 |           3.2079 |           7.9127 |
[32m[20230113 20:13:08 @agent_ppo2.py:186][0m |          -0.0129 |           3.0591 |           7.9224 |
[32m[20230113 20:13:08 @agent_ppo2.py:186][0m |          -0.0163 |           2.9552 |           7.9221 |
[32m[20230113 20:13:08 @agent_ppo2.py:186][0m |          -0.0142 |           2.8446 |           7.9121 |
[32m[20230113 20:13:08 @agent_ppo2.py:186][0m |          -0.0172 |           2.7920 |           7.9147 |
[32m[20230113 20:13:08 @agent_ppo2.py:186][0m |          -0.0114 |           2.7299 |           7.9186 |
[32m[20230113 20:13:08 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:13:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.52
[32m[20230113 20:13:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.18
[32m[20230113 20:13:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.25
[32m[20230113 20:13:08 @agent_ppo2.py:144][0m Total time:      28.60 min
[32m[20230113 20:13:08 @agent_ppo2.py:146][0m 2631680 total steps have happened
[32m[20230113 20:13:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1285 --------------------------#
[32m[20230113 20:13:09 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:09 @agent_ppo2.py:186][0m |           0.0004 |           5.6593 |           8.0836 |
[32m[20230113 20:13:09 @agent_ppo2.py:186][0m |          -0.0050 |           4.2839 |           8.0641 |
[32m[20230113 20:13:09 @agent_ppo2.py:186][0m |          -0.0077 |           3.8483 |           8.0653 |
[32m[20230113 20:13:09 @agent_ppo2.py:186][0m |          -0.0077 |           3.5684 |           8.0686 |
[32m[20230113 20:13:09 @agent_ppo2.py:186][0m |          -0.0102 |           3.3333 |           8.0570 |
[32m[20230113 20:13:09 @agent_ppo2.py:186][0m |          -0.0113 |           3.1952 |           8.0574 |
[32m[20230113 20:13:09 @agent_ppo2.py:186][0m |          -0.0119 |           3.0702 |           8.0564 |
[32m[20230113 20:13:09 @agent_ppo2.py:186][0m |          -0.0124 |           2.9435 |           8.0594 |
[32m[20230113 20:13:09 @agent_ppo2.py:186][0m |          -0.0126 |           2.8645 |           8.0569 |
[32m[20230113 20:13:09 @agent_ppo2.py:186][0m |          -0.0138 |           2.7404 |           8.0603 |
[32m[20230113 20:13:09 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:13:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.47
[32m[20230113 20:13:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.97
[32m[20230113 20:13:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.24
[32m[20230113 20:13:10 @agent_ppo2.py:144][0m Total time:      28.62 min
[32m[20230113 20:13:10 @agent_ppo2.py:146][0m 2633728 total steps have happened
[32m[20230113 20:13:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1286 --------------------------#
[32m[20230113 20:13:10 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:10 @agent_ppo2.py:186][0m |          -0.0417 |           6.1006 |           7.9296 |
[32m[20230113 20:13:10 @agent_ppo2.py:186][0m |           0.0374 |           5.3617 |           7.9251 |
[32m[20230113 20:13:10 @agent_ppo2.py:186][0m |          -0.0064 |           4.6795 |           7.9255 |
[32m[20230113 20:13:10 @agent_ppo2.py:186][0m |           0.0039 |           4.2383 |           7.9261 |
[32m[20230113 20:13:10 @agent_ppo2.py:186][0m |          -0.0728 |           4.0043 |           7.9236 |
[32m[20230113 20:13:10 @agent_ppo2.py:186][0m |          -0.0134 |           3.8825 |           7.8957 |
[32m[20230113 20:13:10 @agent_ppo2.py:186][0m |          -0.0082 |           3.7874 |           7.9131 |
[32m[20230113 20:13:11 @agent_ppo2.py:186][0m |          -0.0147 |           3.6539 |           7.9112 |
[32m[20230113 20:13:11 @agent_ppo2.py:186][0m |          -0.0040 |           3.6474 |           7.9222 |
[32m[20230113 20:13:11 @agent_ppo2.py:186][0m |          -0.0222 |           3.5500 |           7.9164 |
[32m[20230113 20:13:11 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.52
[32m[20230113 20:13:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.30
[32m[20230113 20:13:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.17
[32m[20230113 20:13:11 @agent_ppo2.py:144][0m Total time:      28.64 min
[32m[20230113 20:13:11 @agent_ppo2.py:146][0m 2635776 total steps have happened
[32m[20230113 20:13:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1287 --------------------------#
[32m[20230113 20:13:11 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:12 @agent_ppo2.py:186][0m |          -0.0009 |           5.4163 |           8.0755 |
[32m[20230113 20:13:12 @agent_ppo2.py:186][0m |          -0.0042 |           4.4529 |           8.0711 |
[32m[20230113 20:13:12 @agent_ppo2.py:186][0m |          -0.0056 |           4.1293 |           8.0741 |
[32m[20230113 20:13:12 @agent_ppo2.py:186][0m |          -0.0096 |           3.9115 |           8.0626 |
[32m[20230113 20:13:12 @agent_ppo2.py:186][0m |          -0.0137 |           3.7997 |           8.0623 |
[32m[20230113 20:13:12 @agent_ppo2.py:186][0m |          -0.0150 |           3.6373 |           8.0618 |
[32m[20230113 20:13:12 @agent_ppo2.py:186][0m |          -0.0133 |           3.5608 |           8.0568 |
[32m[20230113 20:13:12 @agent_ppo2.py:186][0m |          -0.0136 |           3.4720 |           8.0559 |
[32m[20230113 20:13:12 @agent_ppo2.py:186][0m |          -0.0184 |           3.3518 |           8.0566 |
[32m[20230113 20:13:12 @agent_ppo2.py:186][0m |          -0.0149 |           3.2952 |           8.0556 |
[32m[20230113 20:13:12 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.11
[32m[20230113 20:13:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.19
[32m[20230113 20:13:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.15
[32m[20230113 20:13:12 @agent_ppo2.py:144][0m Total time:      28.66 min
[32m[20230113 20:13:12 @agent_ppo2.py:146][0m 2637824 total steps have happened
[32m[20230113 20:13:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1288 --------------------------#
[32m[20230113 20:13:13 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:13 @agent_ppo2.py:186][0m |          -0.0009 |           5.2697 |           8.0493 |
[32m[20230113 20:13:13 @agent_ppo2.py:186][0m |          -0.0058 |           4.4015 |           8.0547 |
[32m[20230113 20:13:13 @agent_ppo2.py:186][0m |          -0.0086 |           4.0545 |           8.0522 |
[32m[20230113 20:13:13 @agent_ppo2.py:186][0m |          -0.0090 |           3.9416 |           8.0485 |
[32m[20230113 20:13:13 @agent_ppo2.py:186][0m |          -0.0106 |           3.7650 |           8.0528 |
[32m[20230113 20:13:13 @agent_ppo2.py:186][0m |          -0.0119 |           3.6180 |           8.0485 |
[32m[20230113 20:13:13 @agent_ppo2.py:186][0m |          -0.0120 |           3.5739 |           8.0439 |
[32m[20230113 20:13:13 @agent_ppo2.py:186][0m |          -0.0128 |           3.4583 |           8.0437 |
[32m[20230113 20:13:13 @agent_ppo2.py:186][0m |          -0.0133 |           3.4046 |           8.0476 |
[32m[20230113 20:13:13 @agent_ppo2.py:186][0m |          -0.0129 |           3.3095 |           8.0450 |
[32m[20230113 20:13:13 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:13:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.66
[32m[20230113 20:13:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.40
[32m[20230113 20:13:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.70
[32m[20230113 20:13:14 @agent_ppo2.py:144][0m Total time:      28.68 min
[32m[20230113 20:13:14 @agent_ppo2.py:146][0m 2639872 total steps have happened
[32m[20230113 20:13:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1289 --------------------------#
[32m[20230113 20:13:14 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:13:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:14 @agent_ppo2.py:186][0m |           0.0024 |           6.0118 |           8.0244 |
[32m[20230113 20:13:14 @agent_ppo2.py:186][0m |          -0.0083 |           5.0585 |           8.0274 |
[32m[20230113 20:13:14 @agent_ppo2.py:186][0m |          -0.0096 |           4.6021 |           8.0256 |
[32m[20230113 20:13:14 @agent_ppo2.py:186][0m |          -0.0093 |           4.4074 |           8.0146 |
[32m[20230113 20:13:14 @agent_ppo2.py:186][0m |          -0.0122 |           4.1521 |           8.0095 |
[32m[20230113 20:13:14 @agent_ppo2.py:186][0m |          -0.0141 |           3.9790 |           8.0019 |
[32m[20230113 20:13:14 @agent_ppo2.py:186][0m |          -0.0146 |           3.8149 |           8.0031 |
[32m[20230113 20:13:14 @agent_ppo2.py:186][0m |          -0.0143 |           3.7301 |           8.0025 |
[32m[20230113 20:13:14 @agent_ppo2.py:186][0m |          -0.0160 |           3.6538 |           8.0055 |
[32m[20230113 20:13:15 @agent_ppo2.py:186][0m |          -0.0189 |           3.4867 |           8.0055 |
[32m[20230113 20:13:15 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:13:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.62
[32m[20230113 20:13:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.05
[32m[20230113 20:13:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.84
[32m[20230113 20:13:15 @agent_ppo2.py:144][0m Total time:      28.71 min
[32m[20230113 20:13:15 @agent_ppo2.py:146][0m 2641920 total steps have happened
[32m[20230113 20:13:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1290 --------------------------#
[32m[20230113 20:13:15 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:13:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:15 @agent_ppo2.py:186][0m |           0.0003 |           7.2458 |           8.1514 |
[32m[20230113 20:13:16 @agent_ppo2.py:186][0m |          -0.0041 |           5.7172 |           8.1190 |
[32m[20230113 20:13:16 @agent_ppo2.py:186][0m |          -0.0078 |           4.9799 |           8.1241 |
[32m[20230113 20:13:16 @agent_ppo2.py:186][0m |          -0.0060 |           4.7377 |           8.1239 |
[32m[20230113 20:13:16 @agent_ppo2.py:186][0m |          -0.0115 |           4.2847 |           8.1290 |
[32m[20230113 20:13:16 @agent_ppo2.py:186][0m |          -0.0041 |           4.2103 |           8.1242 |
[32m[20230113 20:13:16 @agent_ppo2.py:186][0m |          -0.0144 |           3.9525 |           8.1262 |
[32m[20230113 20:13:16 @agent_ppo2.py:186][0m |          -0.0126 |           3.7888 |           8.1300 |
[32m[20230113 20:13:16 @agent_ppo2.py:186][0m |          -0.0124 |           3.6768 |           8.1303 |
[32m[20230113 20:13:16 @agent_ppo2.py:186][0m |          -0.0114 |           3.6535 |           8.1302 |
[32m[20230113 20:13:16 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:13:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.94
[32m[20230113 20:13:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.05
[32m[20230113 20:13:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.73
[32m[20230113 20:13:16 @agent_ppo2.py:144][0m Total time:      28.73 min
[32m[20230113 20:13:16 @agent_ppo2.py:146][0m 2643968 total steps have happened
[32m[20230113 20:13:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1291 --------------------------#
[32m[20230113 20:13:17 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:17 @agent_ppo2.py:186][0m |          -0.0066 |           4.8223 |           8.0529 |
[32m[20230113 20:13:17 @agent_ppo2.py:186][0m |           0.0005 |           3.9047 |           8.0453 |
[32m[20230113 20:13:17 @agent_ppo2.py:186][0m |          -0.0045 |           3.5007 |           8.0430 |
[32m[20230113 20:13:17 @agent_ppo2.py:186][0m |          -0.0038 |           3.2423 |           8.0482 |
[32m[20230113 20:13:17 @agent_ppo2.py:186][0m |          -0.0084 |           3.1059 |           8.0447 |
[32m[20230113 20:13:17 @agent_ppo2.py:186][0m |          -0.0021 |           2.9767 |           8.0495 |
[32m[20230113 20:13:17 @agent_ppo2.py:186][0m |          -0.0051 |           2.8742 |           8.0455 |
[32m[20230113 20:13:17 @agent_ppo2.py:186][0m |          -0.0147 |           2.7933 |           8.0390 |
[32m[20230113 20:13:17 @agent_ppo2.py:186][0m |          -0.0079 |           2.6753 |           8.0434 |
[32m[20230113 20:13:17 @agent_ppo2.py:186][0m |          -0.0146 |           2.6731 |           8.0476 |
[32m[20230113 20:13:17 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.40
[32m[20230113 20:13:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.42
[32m[20230113 20:13:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.76
[32m[20230113 20:13:18 @agent_ppo2.py:144][0m Total time:      28.75 min
[32m[20230113 20:13:18 @agent_ppo2.py:146][0m 2646016 total steps have happened
[32m[20230113 20:13:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1292 --------------------------#
[32m[20230113 20:13:18 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 20:13:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:18 @agent_ppo2.py:186][0m |           0.0044 |           4.9372 |           8.2915 |
[32m[20230113 20:13:18 @agent_ppo2.py:186][0m |          -0.0009 |           3.9748 |           8.2857 |
[32m[20230113 20:13:18 @agent_ppo2.py:186][0m |          -0.0025 |           3.6684 |           8.2802 |
[32m[20230113 20:13:18 @agent_ppo2.py:186][0m |          -0.0071 |           3.4014 |           8.2791 |
[32m[20230113 20:13:18 @agent_ppo2.py:186][0m |          -0.0068 |           3.2419 |           8.2750 |
[32m[20230113 20:13:18 @agent_ppo2.py:186][0m |          -0.0118 |           3.0668 |           8.2737 |
[32m[20230113 20:13:18 @agent_ppo2.py:186][0m |          -0.0111 |           2.9581 |           8.2749 |
[32m[20230113 20:13:18 @agent_ppo2.py:186][0m |          -0.0085 |           2.8643 |           8.2704 |
[32m[20230113 20:13:18 @agent_ppo2.py:186][0m |          -0.0133 |           2.8145 |           8.2669 |
[32m[20230113 20:13:19 @agent_ppo2.py:186][0m |          -0.0115 |           2.7255 |           8.2742 |
[32m[20230113 20:13:19 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:13:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.00
[32m[20230113 20:13:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.35
[32m[20230113 20:13:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.91
[32m[20230113 20:13:19 @agent_ppo2.py:144][0m Total time:      28.77 min
[32m[20230113 20:13:19 @agent_ppo2.py:146][0m 2648064 total steps have happened
[32m[20230113 20:13:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1293 --------------------------#
[32m[20230113 20:13:19 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:13:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:19 @agent_ppo2.py:186][0m |           0.0000 |           5.8834 |           8.0693 |
[32m[20230113 20:13:20 @agent_ppo2.py:186][0m |          -0.0053 |           4.8956 |           8.0658 |
[32m[20230113 20:13:20 @agent_ppo2.py:186][0m |          -0.0045 |           4.4666 |           8.0574 |
[32m[20230113 20:13:20 @agent_ppo2.py:186][0m |          -0.0078 |           4.2294 |           8.0542 |
[32m[20230113 20:13:20 @agent_ppo2.py:186][0m |          -0.0091 |           4.0126 |           8.0525 |
[32m[20230113 20:13:20 @agent_ppo2.py:186][0m |          -0.0098 |           3.8764 |           8.0505 |
[32m[20230113 20:13:20 @agent_ppo2.py:186][0m |          -0.0099 |           3.8375 |           8.0566 |
[32m[20230113 20:13:20 @agent_ppo2.py:186][0m |          -0.0124 |           3.6422 |           8.0422 |
[32m[20230113 20:13:20 @agent_ppo2.py:186][0m |          -0.0127 |           3.5240 |           8.0517 |
[32m[20230113 20:13:20 @agent_ppo2.py:186][0m |          -0.0137 |           3.4484 |           8.0453 |
[32m[20230113 20:13:20 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.98
[32m[20230113 20:13:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.26
[32m[20230113 20:13:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.83
[32m[20230113 20:13:20 @agent_ppo2.py:144][0m Total time:      28.79 min
[32m[20230113 20:13:20 @agent_ppo2.py:146][0m 2650112 total steps have happened
[32m[20230113 20:13:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1294 --------------------------#
[32m[20230113 20:13:21 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:13:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:21 @agent_ppo2.py:186][0m |           0.0017 |          16.9404 |           8.0912 |
[32m[20230113 20:13:21 @agent_ppo2.py:186][0m |          -0.0034 |           8.8157 |           8.0787 |
[32m[20230113 20:13:21 @agent_ppo2.py:186][0m |          -0.0067 |           6.7559 |           8.0751 |
[32m[20230113 20:13:21 @agent_ppo2.py:186][0m |          -0.0078 |           5.6860 |           8.0709 |
[32m[20230113 20:13:21 @agent_ppo2.py:186][0m |          -0.0040 |           5.0125 |           8.0696 |
[32m[20230113 20:13:21 @agent_ppo2.py:186][0m |          -0.0114 |           4.5978 |           8.0702 |
[32m[20230113 20:13:21 @agent_ppo2.py:186][0m |          -0.0123 |           4.3142 |           8.0657 |
[32m[20230113 20:13:21 @agent_ppo2.py:186][0m |          -0.0107 |           4.1639 |           8.0615 |
[32m[20230113 20:13:21 @agent_ppo2.py:186][0m |          -0.0113 |           3.9214 |           8.0601 |
[32m[20230113 20:13:21 @agent_ppo2.py:186][0m |          -0.0146 |           3.7755 |           8.0586 |
[32m[20230113 20:13:21 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 20:13:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 125.27
[32m[20230113 20:13:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.01
[32m[20230113 20:13:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.72
[32m[20230113 20:13:22 @agent_ppo2.py:144][0m Total time:      28.82 min
[32m[20230113 20:13:22 @agent_ppo2.py:146][0m 2652160 total steps have happened
[32m[20230113 20:13:22 @agent_ppo2.py:122][0m #------------------------ Iteration 1295 --------------------------#
[32m[20230113 20:13:22 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:22 @agent_ppo2.py:186][0m |          -0.0010 |           6.8701 |           7.9665 |
[32m[20230113 20:13:22 @agent_ppo2.py:186][0m |          -0.0075 |           4.8302 |           7.9480 |
[32m[20230113 20:13:22 @agent_ppo2.py:186][0m |          -0.0131 |           4.3646 |           7.9613 |
[32m[20230113 20:13:22 @agent_ppo2.py:186][0m |          -0.0142 |           4.0690 |           7.9472 |
[32m[20230113 20:13:22 @agent_ppo2.py:186][0m |          -0.0126 |           3.8821 |           7.9504 |
[32m[20230113 20:13:22 @agent_ppo2.py:186][0m |          -0.0160 |           3.7297 |           7.9610 |
[32m[20230113 20:13:22 @agent_ppo2.py:186][0m |          -0.0166 |           3.5837 |           7.9577 |
[32m[20230113 20:13:22 @agent_ppo2.py:186][0m |          -0.0221 |           3.4655 |           7.9616 |
[32m[20230113 20:13:23 @agent_ppo2.py:186][0m |          -0.0159 |           3.3911 |           7.9619 |
[32m[20230113 20:13:23 @agent_ppo2.py:186][0m |          -0.0200 |           3.2865 |           7.9653 |
[32m[20230113 20:13:23 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.94
[32m[20230113 20:13:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.06
[32m[20230113 20:13:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.57
[32m[20230113 20:13:23 @agent_ppo2.py:144][0m Total time:      28.84 min
[32m[20230113 20:13:23 @agent_ppo2.py:146][0m 2654208 total steps have happened
[32m[20230113 20:13:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1296 --------------------------#
[32m[20230113 20:13:23 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:13:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:24 @agent_ppo2.py:186][0m |           0.0008 |           5.5419 |           8.1657 |
[32m[20230113 20:13:24 @agent_ppo2.py:186][0m |          -0.0046 |           4.4038 |           8.1406 |
[32m[20230113 20:13:24 @agent_ppo2.py:186][0m |          -0.0079 |           3.8763 |           8.1439 |
[32m[20230113 20:13:24 @agent_ppo2.py:186][0m |          -0.0059 |           3.6096 |           8.1425 |
[32m[20230113 20:13:24 @agent_ppo2.py:186][0m |          -0.0104 |           3.4306 |           8.1442 |
[32m[20230113 20:13:24 @agent_ppo2.py:186][0m |          -0.0108 |           3.3243 |           8.1382 |
[32m[20230113 20:13:24 @agent_ppo2.py:186][0m |          -0.0146 |           3.1533 |           8.1421 |
[32m[20230113 20:13:24 @agent_ppo2.py:186][0m |          -0.0140 |           3.1023 |           8.1413 |
[32m[20230113 20:13:24 @agent_ppo2.py:186][0m |          -0.0148 |           3.0242 |           8.1403 |
[32m[20230113 20:13:24 @agent_ppo2.py:186][0m |          -0.0158 |           2.9703 |           8.1371 |
[32m[20230113 20:13:24 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:13:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.17
[32m[20230113 20:13:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.54
[32m[20230113 20:13:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 129.84
[32m[20230113 20:13:24 @agent_ppo2.py:144][0m Total time:      28.86 min
[32m[20230113 20:13:24 @agent_ppo2.py:146][0m 2656256 total steps have happened
[32m[20230113 20:13:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1297 --------------------------#
[32m[20230113 20:13:25 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:13:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:25 @agent_ppo2.py:186][0m |          -0.0043 |           5.7624 |           8.1689 |
[32m[20230113 20:13:25 @agent_ppo2.py:186][0m |          -0.0072 |           4.0889 |           8.1763 |
[32m[20230113 20:13:25 @agent_ppo2.py:186][0m |          -0.0023 |           3.7384 |           8.1678 |
[32m[20230113 20:13:25 @agent_ppo2.py:186][0m |          -0.0092 |           3.3595 |           8.1661 |
[32m[20230113 20:13:25 @agent_ppo2.py:186][0m |          -0.0121 |           3.1556 |           8.1557 |
[32m[20230113 20:13:25 @agent_ppo2.py:186][0m |          -0.0112 |           3.0382 |           8.1532 |
[32m[20230113 20:13:25 @agent_ppo2.py:186][0m |          -0.0143 |           2.9194 |           8.1557 |
[32m[20230113 20:13:25 @agent_ppo2.py:186][0m |          -0.0072 |           2.9724 |           8.1597 |
[32m[20230113 20:13:25 @agent_ppo2.py:186][0m |          -0.0110 |           2.7656 |           8.1472 |
[32m[20230113 20:13:25 @agent_ppo2.py:186][0m |          -0.0154 |           2.6870 |           8.1527 |
[32m[20230113 20:13:25 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:13:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.21
[32m[20230113 20:13:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.26
[32m[20230113 20:13:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.74
[32m[20230113 20:13:26 @agent_ppo2.py:144][0m Total time:      28.88 min
[32m[20230113 20:13:26 @agent_ppo2.py:146][0m 2658304 total steps have happened
[32m[20230113 20:13:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1298 --------------------------#
[32m[20230113 20:13:26 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:13:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:26 @agent_ppo2.py:186][0m |           0.0008 |           5.1741 |           8.3648 |
[32m[20230113 20:13:26 @agent_ppo2.py:186][0m |          -0.0054 |           4.2076 |           8.3577 |
[32m[20230113 20:13:26 @agent_ppo2.py:186][0m |          -0.0079 |           3.8521 |           8.3565 |
[32m[20230113 20:13:26 @agent_ppo2.py:186][0m |          -0.0088 |           3.5865 |           8.3631 |
[32m[20230113 20:13:26 @agent_ppo2.py:186][0m |          -0.0099 |           3.3715 |           8.3513 |
[32m[20230113 20:13:26 @agent_ppo2.py:186][0m |          -0.0111 |           3.2286 |           8.3585 |
[32m[20230113 20:13:26 @agent_ppo2.py:186][0m |          -0.0130 |           3.1260 |           8.3367 |
[32m[20230113 20:13:26 @agent_ppo2.py:186][0m |          -0.0134 |           2.9876 |           8.3544 |
[32m[20230113 20:13:26 @agent_ppo2.py:186][0m |          -0.0148 |           2.9161 |           8.3416 |
[32m[20230113 20:13:26 @agent_ppo2.py:186][0m |          -0.0158 |           2.8432 |           8.3454 |
[32m[20230113 20:13:26 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:13:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.26
[32m[20230113 20:13:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.36
[32m[20230113 20:13:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.58
[32m[20230113 20:13:27 @agent_ppo2.py:144][0m Total time:      28.90 min
[32m[20230113 20:13:27 @agent_ppo2.py:146][0m 2660352 total steps have happened
[32m[20230113 20:13:27 @agent_ppo2.py:122][0m #------------------------ Iteration 1299 --------------------------#
[32m[20230113 20:13:27 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:27 @agent_ppo2.py:186][0m |           0.0009 |           5.6937 |           8.0915 |
[32m[20230113 20:13:27 @agent_ppo2.py:186][0m |          -0.0015 |           4.5337 |           8.0715 |
[32m[20230113 20:13:27 @agent_ppo2.py:186][0m |          -0.0112 |           4.1999 |           8.0620 |
[32m[20230113 20:13:28 @agent_ppo2.py:186][0m |          -0.0050 |           3.9379 |           8.0606 |
[32m[20230113 20:13:28 @agent_ppo2.py:186][0m |          -0.0097 |           3.6871 |           8.0675 |
[32m[20230113 20:13:28 @agent_ppo2.py:186][0m |          -0.0126 |           3.5391 |           8.0616 |
[32m[20230113 20:13:28 @agent_ppo2.py:186][0m |          -0.0154 |           3.4374 |           8.0680 |
[32m[20230113 20:13:28 @agent_ppo2.py:186][0m |          -0.0154 |           3.3956 |           8.0582 |
[32m[20230113 20:13:28 @agent_ppo2.py:186][0m |          -0.0110 |           3.2673 |           8.0624 |
[32m[20230113 20:13:28 @agent_ppo2.py:186][0m |          -0.0154 |           3.1954 |           8.0622 |
[32m[20230113 20:13:28 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.24
[32m[20230113 20:13:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.59
[32m[20230113 20:13:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.80
[32m[20230113 20:13:28 @agent_ppo2.py:144][0m Total time:      28.93 min
[32m[20230113 20:13:28 @agent_ppo2.py:146][0m 2662400 total steps have happened
[32m[20230113 20:13:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1300 --------------------------#
[32m[20230113 20:13:29 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:13:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:29 @agent_ppo2.py:186][0m |          -0.0003 |           5.5415 |           8.1892 |
[32m[20230113 20:13:29 @agent_ppo2.py:186][0m |          -0.0073 |           4.5068 |           8.1606 |
[32m[20230113 20:13:29 @agent_ppo2.py:186][0m |          -0.0101 |           4.1554 |           8.1596 |
[32m[20230113 20:13:29 @agent_ppo2.py:186][0m |          -0.0101 |           4.0210 |           8.1596 |
[32m[20230113 20:13:29 @agent_ppo2.py:186][0m |          -0.0112 |           3.8383 |           8.1632 |
[32m[20230113 20:13:29 @agent_ppo2.py:186][0m |          -0.0135 |           3.5729 |           8.1562 |
[32m[20230113 20:13:29 @agent_ppo2.py:186][0m |          -0.0141 |           3.4562 |           8.1565 |
[32m[20230113 20:13:29 @agent_ppo2.py:186][0m |          -0.0143 |           3.3417 |           8.1570 |
[32m[20230113 20:13:29 @agent_ppo2.py:186][0m |          -0.0134 |           3.3135 |           8.1538 |
[32m[20230113 20:13:29 @agent_ppo2.py:186][0m |          -0.0169 |           3.2227 |           8.1514 |
[32m[20230113 20:13:29 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:13:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.53
[32m[20230113 20:13:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.58
[32m[20230113 20:13:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 171.03
[32m[20230113 20:13:29 @agent_ppo2.py:144][0m Total time:      28.95 min
[32m[20230113 20:13:29 @agent_ppo2.py:146][0m 2664448 total steps have happened
[32m[20230113 20:13:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1301 --------------------------#
[32m[20230113 20:13:30 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:30 @agent_ppo2.py:186][0m |          -0.0003 |           5.8730 |           8.1750 |
[32m[20230113 20:13:30 @agent_ppo2.py:186][0m |          -0.0067 |           4.7479 |           8.1650 |
[32m[20230113 20:13:30 @agent_ppo2.py:186][0m |          -0.0097 |           4.3775 |           8.1688 |
[32m[20230113 20:13:30 @agent_ppo2.py:186][0m |          -0.0121 |           4.0674 |           8.1707 |
[32m[20230113 20:13:30 @agent_ppo2.py:186][0m |          -0.0094 |           3.9146 |           8.1600 |
[32m[20230113 20:13:30 @agent_ppo2.py:186][0m |          -0.0112 |           3.7610 |           8.1624 |
[32m[20230113 20:13:30 @agent_ppo2.py:186][0m |          -0.0139 |           3.6629 |           8.1619 |
[32m[20230113 20:13:30 @agent_ppo2.py:186][0m |          -0.0140 |           3.5401 |           8.1586 |
[32m[20230113 20:13:30 @agent_ppo2.py:186][0m |          -0.0131 |           3.4117 |           8.1566 |
[32m[20230113 20:13:30 @agent_ppo2.py:186][0m |          -0.0151 |           3.3624 |           8.1499 |
[32m[20230113 20:13:30 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.80
[32m[20230113 20:13:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.80
[32m[20230113 20:13:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.46
[32m[20230113 20:13:31 @agent_ppo2.py:144][0m Total time:      28.97 min
[32m[20230113 20:13:31 @agent_ppo2.py:146][0m 2666496 total steps have happened
[32m[20230113 20:13:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1302 --------------------------#
[32m[20230113 20:13:31 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:13:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:31 @agent_ppo2.py:186][0m |          -0.0026 |           4.8631 |           8.1908 |
[32m[20230113 20:13:31 @agent_ppo2.py:186][0m |          -0.0070 |           3.8638 |           8.1755 |
[32m[20230113 20:13:31 @agent_ppo2.py:186][0m |          -0.0117 |           3.4383 |           8.1664 |
[32m[20230113 20:13:31 @agent_ppo2.py:186][0m |          -0.0085 |           3.2232 |           8.1719 |
[32m[20230113 20:13:31 @agent_ppo2.py:186][0m |          -0.0141 |           3.0772 |           8.1748 |
[32m[20230113 20:13:32 @agent_ppo2.py:186][0m |          -0.0082 |           2.9419 |           8.1752 |
[32m[20230113 20:13:32 @agent_ppo2.py:186][0m |          -0.0142 |           2.8053 |           8.1703 |
[32m[20230113 20:13:32 @agent_ppo2.py:186][0m |          -0.0154 |           2.7045 |           8.1763 |
[32m[20230113 20:13:32 @agent_ppo2.py:186][0m |          -0.0203 |           2.6474 |           8.1754 |
[32m[20230113 20:13:32 @agent_ppo2.py:186][0m |          -0.0174 |           2.5791 |           8.1765 |
[32m[20230113 20:13:32 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:13:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.24
[32m[20230113 20:13:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.82
[32m[20230113 20:13:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.29
[32m[20230113 20:13:32 @agent_ppo2.py:144][0m Total time:      28.99 min
[32m[20230113 20:13:32 @agent_ppo2.py:146][0m 2668544 total steps have happened
[32m[20230113 20:13:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1303 --------------------------#
[32m[20230113 20:13:33 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:13:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:33 @agent_ppo2.py:186][0m |          -0.0042 |           6.1974 |           8.1638 |
[32m[20230113 20:13:33 @agent_ppo2.py:186][0m |          -0.0040 |           4.4489 |           8.1572 |
[32m[20230113 20:13:33 @agent_ppo2.py:186][0m |          -0.0024 |           3.9621 |           8.1481 |
[32m[20230113 20:13:33 @agent_ppo2.py:186][0m |          -0.0101 |           3.6873 |           8.1491 |
[32m[20230113 20:13:33 @agent_ppo2.py:186][0m |          -0.0108 |           3.4815 |           8.1412 |
[32m[20230113 20:13:33 @agent_ppo2.py:186][0m |          -0.0075 |           3.3132 |           8.1425 |
[32m[20230113 20:13:33 @agent_ppo2.py:186][0m |          -0.0169 |           3.2395 |           8.1417 |
[32m[20230113 20:13:33 @agent_ppo2.py:186][0m |          -0.0162 |           3.1360 |           8.1355 |
[32m[20230113 20:13:33 @agent_ppo2.py:186][0m |          -0.0128 |           3.0471 |           8.1376 |
[32m[20230113 20:13:33 @agent_ppo2.py:186][0m |          -0.0138 |           2.9713 |           8.1427 |
[32m[20230113 20:13:33 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:13:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.94
[32m[20230113 20:13:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.26
[32m[20230113 20:13:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.82
[32m[20230113 20:13:33 @agent_ppo2.py:144][0m Total time:      29.01 min
[32m[20230113 20:13:33 @agent_ppo2.py:146][0m 2670592 total steps have happened
[32m[20230113 20:13:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1304 --------------------------#
[32m[20230113 20:13:34 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:13:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:34 @agent_ppo2.py:186][0m |           0.0081 |           9.2912 |           8.0055 |
[32m[20230113 20:13:34 @agent_ppo2.py:186][0m |          -0.0001 |           4.9549 |           7.9978 |
[32m[20230113 20:13:34 @agent_ppo2.py:186][0m |          -0.0140 |           4.1303 |           7.9929 |
[32m[20230113 20:13:34 @agent_ppo2.py:186][0m |          -0.0112 |           3.5180 |           7.9881 |
[32m[20230113 20:13:34 @agent_ppo2.py:186][0m |          -0.0013 |           3.2784 |           7.9933 |
[32m[20230113 20:13:34 @agent_ppo2.py:186][0m |          -0.0117 |           2.9075 |           7.9785 |
[32m[20230113 20:13:34 @agent_ppo2.py:186][0m |          -0.0174 |           2.7037 |           7.9756 |
[32m[20230113 20:13:34 @agent_ppo2.py:186][0m |          -0.0129 |           2.6301 |           7.9691 |
[32m[20230113 20:13:34 @agent_ppo2.py:186][0m |          -0.0206 |           2.5790 |           7.9728 |
[32m[20230113 20:13:34 @agent_ppo2.py:186][0m |          -0.0178 |           2.4421 |           7.9705 |
[32m[20230113 20:13:34 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:13:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 165.60
[32m[20230113 20:13:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.81
[32m[20230113 20:13:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.11
[32m[20230113 20:13:35 @agent_ppo2.py:144][0m Total time:      29.04 min
[32m[20230113 20:13:35 @agent_ppo2.py:146][0m 2672640 total steps have happened
[32m[20230113 20:13:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1305 --------------------------#
[32m[20230113 20:13:35 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:35 @agent_ppo2.py:186][0m |          -0.0009 |           5.9657 |           8.2012 |
[32m[20230113 20:13:35 @agent_ppo2.py:186][0m |          -0.0046 |           4.6013 |           8.1911 |
[32m[20230113 20:13:35 @agent_ppo2.py:186][0m |           0.0066 |           4.1434 |           8.1987 |
[32m[20230113 20:13:35 @agent_ppo2.py:186][0m |          -0.0052 |           3.8868 |           8.1952 |
[32m[20230113 20:13:35 @agent_ppo2.py:186][0m |          -0.0010 |           3.7059 |           8.1878 |
[32m[20230113 20:13:35 @agent_ppo2.py:186][0m |          -0.0015 |           3.5461 |           8.1919 |
[32m[20230113 20:13:35 @agent_ppo2.py:186][0m |          -0.0052 |           3.4177 |           8.1950 |
[32m[20230113 20:13:36 @agent_ppo2.py:186][0m |          -0.0115 |           3.3549 |           8.1901 |
[32m[20230113 20:13:36 @agent_ppo2.py:186][0m |          -0.0202 |           3.3043 |           8.1959 |
[32m[20230113 20:13:36 @agent_ppo2.py:186][0m |          -0.0038 |           3.2433 |           8.1807 |
[32m[20230113 20:13:36 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.94
[32m[20230113 20:13:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.41
[32m[20230113 20:13:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.27
[32m[20230113 20:13:36 @agent_ppo2.py:144][0m Total time:      29.06 min
[32m[20230113 20:13:36 @agent_ppo2.py:146][0m 2674688 total steps have happened
[32m[20230113 20:13:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1306 --------------------------#
[32m[20230113 20:13:36 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 20:13:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:36 @agent_ppo2.py:186][0m |          -0.0142 |          10.6338 |           8.2778 |
[32m[20230113 20:13:36 @agent_ppo2.py:186][0m |          -0.0233 |           4.4521 |           8.2675 |
[32m[20230113 20:13:36 @agent_ppo2.py:186][0m |          -0.0150 |           3.7113 |           8.2608 |
[32m[20230113 20:13:36 @agent_ppo2.py:186][0m |          -0.0061 |           3.3119 |           8.2694 |
[32m[20230113 20:13:37 @agent_ppo2.py:186][0m |           0.0845 |           3.6111 |           8.2705 |
[32m[20230113 20:13:37 @agent_ppo2.py:186][0m |          -0.0084 |           4.7166 |           8.2640 |
[32m[20230113 20:13:37 @agent_ppo2.py:186][0m |          -0.0190 |           3.1382 |           8.2679 |
[32m[20230113 20:13:37 @agent_ppo2.py:186][0m |          -0.0197 |           2.7625 |           8.2628 |
[32m[20230113 20:13:37 @agent_ppo2.py:186][0m |          -0.0116 |           2.6035 |           8.2667 |
[32m[20230113 20:13:37 @agent_ppo2.py:186][0m |          -0.0214 |           2.5177 |           8.2712 |
[32m[20230113 20:13:37 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 20:13:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 124.96
[32m[20230113 20:13:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.78
[32m[20230113 20:13:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.72
[32m[20230113 20:13:37 @agent_ppo2.py:144][0m Total time:      29.08 min
[32m[20230113 20:13:37 @agent_ppo2.py:146][0m 2676736 total steps have happened
[32m[20230113 20:13:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1307 --------------------------#
[32m[20230113 20:13:38 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:38 @agent_ppo2.py:186][0m |          -0.0038 |           4.9021 |           8.2130 |
[32m[20230113 20:13:38 @agent_ppo2.py:186][0m |           0.0028 |           3.9830 |           8.1935 |
[32m[20230113 20:13:38 @agent_ppo2.py:186][0m |          -0.0150 |           3.6302 |           8.1857 |
[32m[20230113 20:13:38 @agent_ppo2.py:186][0m |          -0.0104 |           3.4040 |           8.1936 |
[32m[20230113 20:13:38 @agent_ppo2.py:186][0m |          -0.0265 |           3.3746 |           8.1976 |
[32m[20230113 20:13:38 @agent_ppo2.py:186][0m |          -0.0111 |           3.1795 |           8.1835 |
[32m[20230113 20:13:38 @agent_ppo2.py:186][0m |          -0.0037 |           3.0286 |           8.1876 |
[32m[20230113 20:13:38 @agent_ppo2.py:186][0m |          -0.0056 |           2.9090 |           8.1734 |
[32m[20230113 20:13:38 @agent_ppo2.py:186][0m |          -0.0086 |           2.8930 |           8.1783 |
[32m[20230113 20:13:38 @agent_ppo2.py:186][0m |          -0.0205 |           2.7846 |           8.1802 |
[32m[20230113 20:13:38 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.82
[32m[20230113 20:13:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.49
[32m[20230113 20:13:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.15
[32m[20230113 20:13:38 @agent_ppo2.py:144][0m Total time:      29.10 min
[32m[20230113 20:13:38 @agent_ppo2.py:146][0m 2678784 total steps have happened
[32m[20230113 20:13:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1308 --------------------------#
[32m[20230113 20:13:39 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:13:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:39 @agent_ppo2.py:186][0m |           0.0019 |           5.9163 |           8.3160 |
[32m[20230113 20:13:39 @agent_ppo2.py:186][0m |          -0.0035 |           4.7840 |           8.3073 |
[32m[20230113 20:13:39 @agent_ppo2.py:186][0m |          -0.0079 |           4.1462 |           8.2958 |
[32m[20230113 20:13:39 @agent_ppo2.py:186][0m |          -0.0098 |           3.8211 |           8.2924 |
[32m[20230113 20:13:39 @agent_ppo2.py:186][0m |          -0.0108 |           3.6058 |           8.2825 |
[32m[20230113 20:13:39 @agent_ppo2.py:186][0m |          -0.0116 |           3.3913 |           8.2847 |
[32m[20230113 20:13:39 @agent_ppo2.py:186][0m |          -0.0123 |           3.2663 |           8.2760 |
[32m[20230113 20:13:39 @agent_ppo2.py:186][0m |          -0.0141 |           3.1690 |           8.2730 |
[32m[20230113 20:13:39 @agent_ppo2.py:186][0m |          -0.0140 |           3.0903 |           8.2788 |
[32m[20230113 20:13:39 @agent_ppo2.py:186][0m |          -0.0154 |           2.9808 |           8.2746 |
[32m[20230113 20:13:39 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:13:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.10
[32m[20230113 20:13:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.39
[32m[20230113 20:13:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.08
[32m[20230113 20:13:40 @agent_ppo2.py:144][0m Total time:      29.12 min
[32m[20230113 20:13:40 @agent_ppo2.py:146][0m 2680832 total steps have happened
[32m[20230113 20:13:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1309 --------------------------#
[32m[20230113 20:13:40 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:13:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:40 @agent_ppo2.py:186][0m |           0.0028 |           6.1786 |           8.3391 |
[32m[20230113 20:13:40 @agent_ppo2.py:186][0m |           0.0033 |           4.6402 |           8.3310 |
[32m[20230113 20:13:40 @agent_ppo2.py:186][0m |           0.0018 |           4.1867 |           8.3287 |
[32m[20230113 20:13:40 @agent_ppo2.py:186][0m |          -0.0151 |           3.8090 |           8.3267 |
[32m[20230113 20:13:40 @agent_ppo2.py:186][0m |          -0.0056 |           3.6219 |           8.3235 |
[32m[20230113 20:13:40 @agent_ppo2.py:186][0m |          -0.0324 |           3.4436 |           8.3166 |
[32m[20230113 20:13:40 @agent_ppo2.py:186][0m |          -0.0123 |           3.3356 |           8.3159 |
[32m[20230113 20:13:40 @agent_ppo2.py:186][0m |          -0.0176 |           3.2410 |           8.3221 |
[32m[20230113 20:13:41 @agent_ppo2.py:186][0m |          -0.0143 |           3.0925 |           8.3335 |
[32m[20230113 20:13:41 @agent_ppo2.py:186][0m |          -0.0113 |           3.0315 |           8.3306 |
[32m[20230113 20:13:41 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:13:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.93
[32m[20230113 20:13:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.50
[32m[20230113 20:13:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.61
[32m[20230113 20:13:41 @agent_ppo2.py:144][0m Total time:      29.14 min
[32m[20230113 20:13:41 @agent_ppo2.py:146][0m 2682880 total steps have happened
[32m[20230113 20:13:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1310 --------------------------#
[32m[20230113 20:13:41 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:42 @agent_ppo2.py:186][0m |           0.0043 |           5.0282 |           8.2378 |
[32m[20230113 20:13:42 @agent_ppo2.py:186][0m |          -0.0013 |           4.0326 |           8.2334 |
[32m[20230113 20:13:42 @agent_ppo2.py:186][0m |          -0.0090 |           3.6830 |           8.2357 |
[32m[20230113 20:13:42 @agent_ppo2.py:186][0m |          -0.0093 |           3.4665 |           8.2372 |
[32m[20230113 20:13:42 @agent_ppo2.py:186][0m |          -0.0113 |           3.3118 |           8.2315 |
[32m[20230113 20:13:42 @agent_ppo2.py:186][0m |          -0.0106 |           3.2092 |           8.2307 |
[32m[20230113 20:13:42 @agent_ppo2.py:186][0m |          -0.0132 |           3.1017 |           8.2291 |
[32m[20230113 20:13:42 @agent_ppo2.py:186][0m |          -0.0102 |           3.0297 |           8.2246 |
[32m[20230113 20:13:42 @agent_ppo2.py:186][0m |          -0.0125 |           2.9534 |           8.2232 |
[32m[20230113 20:13:42 @agent_ppo2.py:186][0m |          -0.0189 |           2.8940 |           8.2212 |
[32m[20230113 20:13:42 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.54
[32m[20230113 20:13:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.39
[32m[20230113 20:13:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.25
[32m[20230113 20:13:42 @agent_ppo2.py:144][0m Total time:      29.16 min
[32m[20230113 20:13:42 @agent_ppo2.py:146][0m 2684928 total steps have happened
[32m[20230113 20:13:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1311 --------------------------#
[32m[20230113 20:13:43 @agent_ppo2.py:128][0m Sampling time: 0.35 s by 1 slaves
[32m[20230113 20:13:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:43 @agent_ppo2.py:186][0m |           0.0021 |          17.0533 |           8.5919 |
[32m[20230113 20:13:43 @agent_ppo2.py:186][0m |          -0.0051 |           7.0413 |           8.5854 |
[32m[20230113 20:13:43 @agent_ppo2.py:186][0m |          -0.0063 |           5.7474 |           8.5674 |
[32m[20230113 20:13:43 @agent_ppo2.py:186][0m |          -0.0094 |           5.3388 |           8.5933 |
[32m[20230113 20:13:43 @agent_ppo2.py:186][0m |          -0.0111 |           4.8666 |           8.5809 |
[32m[20230113 20:13:43 @agent_ppo2.py:186][0m |          -0.0089 |           4.6008 |           8.5757 |
[32m[20230113 20:13:43 @agent_ppo2.py:186][0m |          -0.0113 |           4.3228 |           8.5737 |
[32m[20230113 20:13:43 @agent_ppo2.py:186][0m |          -0.0118 |           4.2376 |           8.5849 |
[32m[20230113 20:13:43 @agent_ppo2.py:186][0m |          -0.0142 |           3.8707 |           8.5769 |
[32m[20230113 20:13:43 @agent_ppo2.py:186][0m |          -0.0146 |           3.8113 |           8.5832 |
[32m[20230113 20:13:43 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 20:13:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 137.20
[32m[20230113 20:13:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.06
[32m[20230113 20:13:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.70
[32m[20230113 20:13:43 @agent_ppo2.py:144][0m Total time:      29.18 min
[32m[20230113 20:13:43 @agent_ppo2.py:146][0m 2686976 total steps have happened
[32m[20230113 20:13:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1312 --------------------------#
[32m[20230113 20:13:44 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:13:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:44 @agent_ppo2.py:186][0m |          -0.0021 |           5.1859 |           8.4016 |
[32m[20230113 20:13:44 @agent_ppo2.py:186][0m |          -0.0046 |           4.2686 |           8.3916 |
[32m[20230113 20:13:44 @agent_ppo2.py:186][0m |          -0.0028 |           3.9142 |           8.3924 |
[32m[20230113 20:13:44 @agent_ppo2.py:186][0m |          -0.0103 |           3.6570 |           8.3805 |
[32m[20230113 20:13:44 @agent_ppo2.py:186][0m |          -0.0090 |           3.5059 |           8.3906 |
[32m[20230113 20:13:44 @agent_ppo2.py:186][0m |          -0.0139 |           3.3685 |           8.3861 |
[32m[20230113 20:13:44 @agent_ppo2.py:186][0m |          -0.0165 |           3.2986 |           8.3942 |
[32m[20230113 20:13:44 @agent_ppo2.py:186][0m |          -0.0157 |           3.1912 |           8.3901 |
[32m[20230113 20:13:44 @agent_ppo2.py:186][0m |          -0.0149 |           3.0958 |           8.3811 |
[32m[20230113 20:13:44 @agent_ppo2.py:186][0m |          -0.0146 |           3.0567 |           8.3901 |
[32m[20230113 20:13:44 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:13:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.47
[32m[20230113 20:13:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.78
[32m[20230113 20:13:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.16
[32m[20230113 20:13:45 @agent_ppo2.py:144][0m Total time:      29.20 min
[32m[20230113 20:13:45 @agent_ppo2.py:146][0m 2689024 total steps have happened
[32m[20230113 20:13:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1313 --------------------------#
[32m[20230113 20:13:45 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:45 @agent_ppo2.py:186][0m |          -0.0006 |           6.4682 |           8.4508 |
[32m[20230113 20:13:45 @agent_ppo2.py:186][0m |          -0.0056 |           4.9792 |           8.4434 |
[32m[20230113 20:13:45 @agent_ppo2.py:186][0m |          -0.0076 |           4.4654 |           8.4433 |
[32m[20230113 20:13:45 @agent_ppo2.py:186][0m |          -0.0101 |           4.1103 |           8.4375 |
[32m[20230113 20:13:45 @agent_ppo2.py:186][0m |          -0.0111 |           3.8577 |           8.4384 |
[32m[20230113 20:13:45 @agent_ppo2.py:186][0m |          -0.0109 |           3.7022 |           8.4396 |
[32m[20230113 20:13:45 @agent_ppo2.py:186][0m |          -0.0112 |           3.5455 |           8.4344 |
[32m[20230113 20:13:45 @agent_ppo2.py:186][0m |          -0.0144 |           3.4532 |           8.4291 |
[32m[20230113 20:13:46 @agent_ppo2.py:186][0m |          -0.0140 |           3.3387 |           8.4239 |
[32m[20230113 20:13:46 @agent_ppo2.py:186][0m |          -0.0155 |           3.2565 |           8.4281 |
[32m[20230113 20:13:46 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:13:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.81
[32m[20230113 20:13:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.53
[32m[20230113 20:13:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.40
[32m[20230113 20:13:46 @agent_ppo2.py:144][0m Total time:      29.22 min
[32m[20230113 20:13:46 @agent_ppo2.py:146][0m 2691072 total steps have happened
[32m[20230113 20:13:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1314 --------------------------#
[32m[20230113 20:13:46 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:13:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:47 @agent_ppo2.py:186][0m |           0.0004 |           6.4361 |           8.5233 |
[32m[20230113 20:13:47 @agent_ppo2.py:186][0m |          -0.0039 |           4.8847 |           8.5203 |
[32m[20230113 20:13:47 @agent_ppo2.py:186][0m |          -0.0080 |           4.4037 |           8.5155 |
[32m[20230113 20:13:47 @agent_ppo2.py:186][0m |          -0.0102 |           4.0458 |           8.5091 |
[32m[20230113 20:13:47 @agent_ppo2.py:186][0m |          -0.0093 |           3.8083 |           8.5114 |
[32m[20230113 20:13:47 @agent_ppo2.py:186][0m |          -0.0129 |           3.6028 |           8.5106 |
[32m[20230113 20:13:47 @agent_ppo2.py:186][0m |          -0.0131 |           3.4689 |           8.5199 |
[32m[20230113 20:13:47 @agent_ppo2.py:186][0m |          -0.0138 |           3.3276 |           8.5141 |
[32m[20230113 20:13:47 @agent_ppo2.py:186][0m |          -0.0148 |           3.2387 |           8.5192 |
[32m[20230113 20:13:47 @agent_ppo2.py:186][0m |          -0.0159 |           3.1400 |           8.5195 |
[32m[20230113 20:13:47 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:13:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.78
[32m[20230113 20:13:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.72
[32m[20230113 20:13:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.86
[32m[20230113 20:13:47 @agent_ppo2.py:144][0m Total time:      29.25 min
[32m[20230113 20:13:47 @agent_ppo2.py:146][0m 2693120 total steps have happened
[32m[20230113 20:13:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1315 --------------------------#
[32m[20230113 20:13:48 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:13:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:48 @agent_ppo2.py:186][0m |          -0.0002 |           6.1952 |           8.4401 |
[32m[20230113 20:13:48 @agent_ppo2.py:186][0m |          -0.0028 |           4.6060 |           8.4460 |
[32m[20230113 20:13:48 @agent_ppo2.py:186][0m |           0.0007 |           4.3809 |           8.4432 |
[32m[20230113 20:13:48 @agent_ppo2.py:186][0m |          -0.0071 |           3.7175 |           8.4383 |
[32m[20230113 20:13:48 @agent_ppo2.py:186][0m |          -0.0091 |           3.4994 |           8.4448 |
[32m[20230113 20:13:48 @agent_ppo2.py:186][0m |          -0.0080 |           3.3250 |           8.4381 |
[32m[20230113 20:13:48 @agent_ppo2.py:186][0m |          -0.0091 |           3.2135 |           8.4423 |
[32m[20230113 20:13:48 @agent_ppo2.py:186][0m |          -0.0093 |           3.0936 |           8.4467 |
[32m[20230113 20:13:48 @agent_ppo2.py:186][0m |          -0.0107 |           2.9925 |           8.4471 |
[32m[20230113 20:13:48 @agent_ppo2.py:186][0m |           0.0010 |           3.1936 |           8.4515 |
[32m[20230113 20:13:48 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:13:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.74
[32m[20230113 20:13:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.96
[32m[20230113 20:13:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.04
[32m[20230113 20:13:49 @agent_ppo2.py:144][0m Total time:      29.27 min
[32m[20230113 20:13:49 @agent_ppo2.py:146][0m 2695168 total steps have happened
[32m[20230113 20:13:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1316 --------------------------#
[32m[20230113 20:13:49 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:49 @agent_ppo2.py:186][0m |          -0.0025 |           4.6457 |           8.3900 |
[32m[20230113 20:13:49 @agent_ppo2.py:186][0m |          -0.0059 |           3.8197 |           8.3805 |
[32m[20230113 20:13:49 @agent_ppo2.py:186][0m |          -0.0105 |           3.5345 |           8.3728 |
[32m[20230113 20:13:49 @agent_ppo2.py:186][0m |          -0.0093 |           3.3045 |           8.3756 |
[32m[20230113 20:13:49 @agent_ppo2.py:186][0m |          -0.0121 |           3.2025 |           8.3707 |
[32m[20230113 20:13:49 @agent_ppo2.py:186][0m |          -0.0116 |           3.0307 |           8.3744 |
[32m[20230113 20:13:49 @agent_ppo2.py:186][0m |          -0.0110 |           2.9184 |           8.3667 |
[32m[20230113 20:13:49 @agent_ppo2.py:186][0m |          -0.0141 |           2.8604 |           8.3591 |
[32m[20230113 20:13:49 @agent_ppo2.py:186][0m |          -0.0131 |           2.7785 |           8.3625 |
[32m[20230113 20:13:50 @agent_ppo2.py:186][0m |          -0.0153 |           2.7315 |           8.3645 |
[32m[20230113 20:13:50 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.71
[32m[20230113 20:13:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.34
[32m[20230113 20:13:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.96
[32m[20230113 20:13:50 @agent_ppo2.py:144][0m Total time:      29.29 min
[32m[20230113 20:13:50 @agent_ppo2.py:146][0m 2697216 total steps have happened
[32m[20230113 20:13:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1317 --------------------------#
[32m[20230113 20:13:50 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 20:13:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:50 @agent_ppo2.py:186][0m |           0.0013 |           5.7308 |           8.5106 |
[32m[20230113 20:13:51 @agent_ppo2.py:186][0m |          -0.0048 |           4.6948 |           8.4938 |
[32m[20230113 20:13:51 @agent_ppo2.py:186][0m |          -0.0065 |           4.0456 |           8.4992 |
[32m[20230113 20:13:51 @agent_ppo2.py:186][0m |          -0.0104 |           3.7953 |           8.5006 |
[32m[20230113 20:13:51 @agent_ppo2.py:186][0m |          -0.0100 |           3.5345 |           8.4971 |
[32m[20230113 20:13:51 @agent_ppo2.py:186][0m |          -0.0092 |           3.3878 |           8.4957 |
[32m[20230113 20:13:51 @agent_ppo2.py:186][0m |          -0.0120 |           3.2438 |           8.4956 |
[32m[20230113 20:13:51 @agent_ppo2.py:186][0m |          -0.0128 |           3.1070 |           8.4949 |
[32m[20230113 20:13:51 @agent_ppo2.py:186][0m |          -0.0122 |           3.0043 |           8.5014 |
[32m[20230113 20:13:51 @agent_ppo2.py:186][0m |          -0.0135 |           2.9511 |           8.4981 |
[32m[20230113 20:13:51 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.09
[32m[20230113 20:13:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.77
[32m[20230113 20:13:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 233.77
[32m[20230113 20:13:51 @agent_ppo2.py:144][0m Total time:      29.31 min
[32m[20230113 20:13:51 @agent_ppo2.py:146][0m 2699264 total steps have happened
[32m[20230113 20:13:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1318 --------------------------#
[32m[20230113 20:13:52 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:13:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:52 @agent_ppo2.py:186][0m |           0.0021 |           5.5757 |           8.4882 |
[32m[20230113 20:13:52 @agent_ppo2.py:186][0m |          -0.0077 |           4.5217 |           8.4682 |
[32m[20230113 20:13:52 @agent_ppo2.py:186][0m |          -0.0065 |           4.1036 |           8.4702 |
[32m[20230113 20:13:52 @agent_ppo2.py:186][0m |          -0.0117 |           3.8236 |           8.4680 |
[32m[20230113 20:13:52 @agent_ppo2.py:186][0m |          -0.0009 |           3.8010 |           8.4682 |
[32m[20230113 20:13:52 @agent_ppo2.py:186][0m |          -0.0120 |           3.6054 |           8.4682 |
[32m[20230113 20:13:52 @agent_ppo2.py:186][0m |          -0.0125 |           3.5360 |           8.4693 |
[32m[20230113 20:13:52 @agent_ppo2.py:186][0m |          -0.0167 |           3.3322 |           8.4790 |
[32m[20230113 20:13:52 @agent_ppo2.py:186][0m |          -0.0124 |           3.2417 |           8.4723 |
[32m[20230113 20:13:52 @agent_ppo2.py:186][0m |          -0.0128 |           3.1480 |           8.4692 |
[32m[20230113 20:13:52 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:13:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.24
[32m[20230113 20:13:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.64
[32m[20230113 20:13:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.46
[32m[20230113 20:13:53 @agent_ppo2.py:144][0m Total time:      29.33 min
[32m[20230113 20:13:53 @agent_ppo2.py:146][0m 2701312 total steps have happened
[32m[20230113 20:13:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1319 --------------------------#
[32m[20230113 20:13:53 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:13:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:53 @agent_ppo2.py:186][0m |          -0.0112 |           5.8446 |           8.5663 |
[32m[20230113 20:13:53 @agent_ppo2.py:186][0m |           0.0034 |           4.5015 |           8.5522 |
[32m[20230113 20:13:53 @agent_ppo2.py:186][0m |          -0.0152 |           3.7584 |           8.5552 |
[32m[20230113 20:13:53 @agent_ppo2.py:186][0m |           0.0145 |           3.6457 |           8.5407 |
[32m[20230113 20:13:53 @agent_ppo2.py:186][0m |          -0.0097 |           3.1979 |           8.5472 |
[32m[20230113 20:13:53 @agent_ppo2.py:186][0m |          -0.0116 |           2.9821 |           8.5463 |
[32m[20230113 20:13:53 @agent_ppo2.py:186][0m |          -0.0067 |           2.9620 |           8.5426 |
[32m[20230113 20:13:53 @agent_ppo2.py:186][0m |           0.0230 |           2.7617 |           8.5434 |
[32m[20230113 20:13:53 @agent_ppo2.py:186][0m |          -0.0029 |           2.6502 |           8.5468 |
[32m[20230113 20:13:53 @agent_ppo2.py:186][0m |          -0.0177 |           2.6216 |           8.5423 |
[32m[20230113 20:13:53 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:13:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.19
[32m[20230113 20:13:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.44
[32m[20230113 20:13:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.44
[32m[20230113 20:13:54 @agent_ppo2.py:144][0m Total time:      29.36 min
[32m[20230113 20:13:54 @agent_ppo2.py:146][0m 2703360 total steps have happened
[32m[20230113 20:13:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1320 --------------------------#
[32m[20230113 20:13:54 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:54 @agent_ppo2.py:186][0m |          -0.0000 |           6.9473 |           8.6326 |
[32m[20230113 20:13:54 @agent_ppo2.py:186][0m |          -0.0043 |           5.2983 |           8.6312 |
[32m[20230113 20:13:54 @agent_ppo2.py:186][0m |          -0.0079 |           4.6747 |           8.6318 |
[32m[20230113 20:13:55 @agent_ppo2.py:186][0m |          -0.0092 |           4.1920 |           8.6217 |
[32m[20230113 20:13:55 @agent_ppo2.py:186][0m |          -0.0097 |           3.8615 |           8.6202 |
[32m[20230113 20:13:55 @agent_ppo2.py:186][0m |          -0.0110 |           3.6622 |           8.6173 |
[32m[20230113 20:13:55 @agent_ppo2.py:186][0m |          -0.0120 |           3.5195 |           8.6204 |
[32m[20230113 20:13:55 @agent_ppo2.py:186][0m |          -0.0129 |           3.3648 |           8.6155 |
[32m[20230113 20:13:55 @agent_ppo2.py:186][0m |          -0.0142 |           3.2614 |           8.6129 |
[32m[20230113 20:13:55 @agent_ppo2.py:186][0m |          -0.0145 |           3.1846 |           8.6174 |
[32m[20230113 20:13:55 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:13:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.23
[32m[20230113 20:13:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.25
[32m[20230113 20:13:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.45
[32m[20230113 20:13:55 @agent_ppo2.py:144][0m Total time:      29.38 min
[32m[20230113 20:13:55 @agent_ppo2.py:146][0m 2705408 total steps have happened
[32m[20230113 20:13:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1321 --------------------------#
[32m[20230113 20:13:56 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:13:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:56 @agent_ppo2.py:186][0m |           0.0001 |           6.0828 |           8.4053 |
[32m[20230113 20:13:56 @agent_ppo2.py:186][0m |          -0.0064 |           4.6518 |           8.3995 |
[32m[20230113 20:13:56 @agent_ppo2.py:186][0m |          -0.0096 |           4.1866 |           8.3879 |
[32m[20230113 20:13:56 @agent_ppo2.py:186][0m |          -0.0123 |           3.8306 |           8.3853 |
[32m[20230113 20:13:56 @agent_ppo2.py:186][0m |          -0.0098 |           3.6565 |           8.3759 |
[32m[20230113 20:13:56 @agent_ppo2.py:186][0m |          -0.0112 |           3.5156 |           8.3665 |
[32m[20230113 20:13:56 @agent_ppo2.py:186][0m |          -0.0153 |           3.3683 |           8.3643 |
[32m[20230113 20:13:56 @agent_ppo2.py:186][0m |          -0.0131 |           3.2439 |           8.3616 |
[32m[20230113 20:13:56 @agent_ppo2.py:186][0m |          -0.0163 |           3.1654 |           8.3561 |
[32m[20230113 20:13:56 @agent_ppo2.py:186][0m |          -0.0164 |           3.0828 |           8.3556 |
[32m[20230113 20:13:56 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:13:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 224.28
[32m[20230113 20:13:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.52
[32m[20230113 20:13:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.28
[32m[20230113 20:13:57 @agent_ppo2.py:144][0m Total time:      29.40 min
[32m[20230113 20:13:57 @agent_ppo2.py:146][0m 2707456 total steps have happened
[32m[20230113 20:13:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1322 --------------------------#
[32m[20230113 20:13:57 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:13:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:57 @agent_ppo2.py:186][0m |           0.0008 |           5.1542 |           8.5523 |
[32m[20230113 20:13:57 @agent_ppo2.py:186][0m |          -0.0070 |           4.1332 |           8.5486 |
[32m[20230113 20:13:57 @agent_ppo2.py:186][0m |          -0.0076 |           3.6710 |           8.5463 |
[32m[20230113 20:13:57 @agent_ppo2.py:186][0m |          -0.0091 |           3.4711 |           8.5379 |
[32m[20230113 20:13:57 @agent_ppo2.py:186][0m |          -0.0106 |           3.3290 |           8.5382 |
[32m[20230113 20:13:57 @agent_ppo2.py:186][0m |          -0.0116 |           3.2195 |           8.5315 |
[32m[20230113 20:13:57 @agent_ppo2.py:186][0m |          -0.0127 |           3.0963 |           8.5297 |
[32m[20230113 20:13:57 @agent_ppo2.py:186][0m |          -0.0138 |           3.0195 |           8.5290 |
[32m[20230113 20:13:57 @agent_ppo2.py:186][0m |          -0.0136 |           3.0301 |           8.5293 |
[32m[20230113 20:13:57 @agent_ppo2.py:186][0m |          -0.0146 |           2.8952 |           8.5186 |
[32m[20230113 20:13:57 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:13:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 227.16
[32m[20230113 20:13:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.01
[32m[20230113 20:13:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.48
[32m[20230113 20:13:58 @agent_ppo2.py:144][0m Total time:      29.42 min
[32m[20230113 20:13:58 @agent_ppo2.py:146][0m 2709504 total steps have happened
[32m[20230113 20:13:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1323 --------------------------#
[32m[20230113 20:13:58 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:13:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:13:58 @agent_ppo2.py:186][0m |          -0.0010 |           4.9447 |           8.5847 |
[32m[20230113 20:13:58 @agent_ppo2.py:186][0m |          -0.0083 |           3.8035 |           8.5749 |
[32m[20230113 20:13:58 @agent_ppo2.py:186][0m |          -0.0094 |           3.4368 |           8.5673 |
[32m[20230113 20:13:58 @agent_ppo2.py:186][0m |          -0.0096 |           3.2050 |           8.5760 |
[32m[20230113 20:13:59 @agent_ppo2.py:186][0m |          -0.0109 |           3.0516 |           8.5742 |
[32m[20230113 20:13:59 @agent_ppo2.py:186][0m |          -0.0123 |           2.9535 |           8.5735 |
[32m[20230113 20:13:59 @agent_ppo2.py:186][0m |          -0.0164 |           2.8516 |           8.5754 |
[32m[20230113 20:13:59 @agent_ppo2.py:186][0m |          -0.0152 |           2.7591 |           8.5740 |
[32m[20230113 20:13:59 @agent_ppo2.py:186][0m |          -0.0183 |           2.6930 |           8.5738 |
[32m[20230113 20:13:59 @agent_ppo2.py:186][0m |          -0.0184 |           2.6144 |           8.5805 |
[32m[20230113 20:13:59 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:13:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.89
[32m[20230113 20:13:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.69
[32m[20230113 20:13:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.79
[32m[20230113 20:13:59 @agent_ppo2.py:144][0m Total time:      29.44 min
[32m[20230113 20:13:59 @agent_ppo2.py:146][0m 2711552 total steps have happened
[32m[20230113 20:13:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1324 --------------------------#
[32m[20230113 20:14:00 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:14:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:00 @agent_ppo2.py:186][0m |           0.0004 |           6.3921 |           8.3377 |
[32m[20230113 20:14:00 @agent_ppo2.py:186][0m |          -0.0085 |           4.9549 |           8.3270 |
[32m[20230113 20:14:00 @agent_ppo2.py:186][0m |          -0.0091 |           4.3880 |           8.3276 |
[32m[20230113 20:14:00 @agent_ppo2.py:186][0m |          -0.0083 |           4.1326 |           8.3143 |
[32m[20230113 20:14:00 @agent_ppo2.py:186][0m |          -0.0040 |           3.8457 |           8.3240 |
[32m[20230113 20:14:00 @agent_ppo2.py:186][0m |          -0.0141 |           3.6711 |           8.3178 |
[32m[20230113 20:14:00 @agent_ppo2.py:186][0m |          -0.0157 |           3.5291 |           8.3091 |
[32m[20230113 20:14:00 @agent_ppo2.py:186][0m |          -0.0145 |           3.4483 |           8.3155 |
[32m[20230113 20:14:00 @agent_ppo2.py:186][0m |          -0.0166 |           3.3472 |           8.3004 |
[32m[20230113 20:14:00 @agent_ppo2.py:186][0m |          -0.0158 |           3.2455 |           8.3174 |
[32m[20230113 20:14:00 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.69
[32m[20230113 20:14:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.67
[32m[20230113 20:14:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 152.37
[32m[20230113 20:14:00 @agent_ppo2.py:144][0m Total time:      29.47 min
[32m[20230113 20:14:00 @agent_ppo2.py:146][0m 2713600 total steps have happened
[32m[20230113 20:14:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1325 --------------------------#
[32m[20230113 20:14:01 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:14:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:01 @agent_ppo2.py:186][0m |           0.0014 |           4.7866 |           8.6796 |
[32m[20230113 20:14:01 @agent_ppo2.py:186][0m |          -0.0043 |           3.9533 |           8.6645 |
[32m[20230113 20:14:01 @agent_ppo2.py:186][0m |          -0.0063 |           3.5486 |           8.6711 |
[32m[20230113 20:14:01 @agent_ppo2.py:186][0m |          -0.0078 |           3.3661 |           8.6635 |
[32m[20230113 20:14:01 @agent_ppo2.py:186][0m |          -0.0094 |           3.1844 |           8.6653 |
[32m[20230113 20:14:01 @agent_ppo2.py:186][0m |          -0.0091 |           3.0667 |           8.6697 |
[32m[20230113 20:14:01 @agent_ppo2.py:186][0m |          -0.0103 |           2.9933 |           8.6628 |
[32m[20230113 20:14:01 @agent_ppo2.py:186][0m |          -0.0098 |           2.8750 |           8.6640 |
[32m[20230113 20:14:01 @agent_ppo2.py:186][0m |          -0.0111 |           2.8393 |           8.6642 |
[32m[20230113 20:14:01 @agent_ppo2.py:186][0m |          -0.0128 |           2.8213 |           8.6626 |
[32m[20230113 20:14:01 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.25
[32m[20230113 20:14:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.36
[32m[20230113 20:14:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.49
[32m[20230113 20:14:02 @agent_ppo2.py:144][0m Total time:      29.49 min
[32m[20230113 20:14:02 @agent_ppo2.py:146][0m 2715648 total steps have happened
[32m[20230113 20:14:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1326 --------------------------#
[32m[20230113 20:14:02 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:02 @agent_ppo2.py:186][0m |           0.0026 |           5.7527 |           8.5615 |
[32m[20230113 20:14:02 @agent_ppo2.py:186][0m |           0.0035 |           4.1717 |           8.5576 |
[32m[20230113 20:14:02 @agent_ppo2.py:186][0m |          -0.0023 |           3.5355 |           8.5502 |
[32m[20230113 20:14:02 @agent_ppo2.py:186][0m |          -0.0071 |           3.2161 |           8.5481 |
[32m[20230113 20:14:03 @agent_ppo2.py:186][0m |          -0.0076 |           3.0229 |           8.5410 |
[32m[20230113 20:14:03 @agent_ppo2.py:186][0m |          -0.0100 |           2.8232 |           8.5493 |
[32m[20230113 20:14:03 @agent_ppo2.py:186][0m |          -0.0140 |           2.6364 |           8.5482 |
[32m[20230113 20:14:03 @agent_ppo2.py:186][0m |          -0.0138 |           2.5487 |           8.5480 |
[32m[20230113 20:14:03 @agent_ppo2.py:186][0m |          -0.0106 |           2.4571 |           8.5536 |
[32m[20230113 20:14:03 @agent_ppo2.py:186][0m |          -0.0134 |           2.3583 |           8.5586 |
[32m[20230113 20:14:03 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:14:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.97
[32m[20230113 20:14:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.82
[32m[20230113 20:14:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.08
[32m[20230113 20:14:03 @agent_ppo2.py:144][0m Total time:      29.51 min
[32m[20230113 20:14:03 @agent_ppo2.py:146][0m 2717696 total steps have happened
[32m[20230113 20:14:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1327 --------------------------#
[32m[20230113 20:14:04 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:04 @agent_ppo2.py:186][0m |           0.0024 |          12.0702 |           8.6353 |
[32m[20230113 20:14:04 @agent_ppo2.py:186][0m |          -0.0117 |           6.5187 |           8.6152 |
[32m[20230113 20:14:04 @agent_ppo2.py:186][0m |          -0.0110 |           5.6737 |           8.6106 |
[32m[20230113 20:14:04 @agent_ppo2.py:186][0m |          -0.0104 |           4.8229 |           8.5973 |
[32m[20230113 20:14:04 @agent_ppo2.py:186][0m |          -0.0134 |           4.4239 |           8.6021 |
[32m[20230113 20:14:04 @agent_ppo2.py:186][0m |          -0.0181 |           4.1309 |           8.6073 |
[32m[20230113 20:14:04 @agent_ppo2.py:186][0m |          -0.0150 |           3.9773 |           8.6028 |
[32m[20230113 20:14:04 @agent_ppo2.py:186][0m |          -0.0173 |           3.8258 |           8.5985 |
[32m[20230113 20:14:04 @agent_ppo2.py:186][0m |          -0.0090 |           4.1043 |           8.5975 |
[32m[20230113 20:14:04 @agent_ppo2.py:186][0m |          -0.0172 |           3.6642 |           8.5930 |
[32m[20230113 20:14:04 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:14:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 115.37
[32m[20230113 20:14:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.52
[32m[20230113 20:14:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.24
[32m[20230113 20:14:04 @agent_ppo2.py:144][0m Total time:      29.53 min
[32m[20230113 20:14:04 @agent_ppo2.py:146][0m 2719744 total steps have happened
[32m[20230113 20:14:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1328 --------------------------#
[32m[20230113 20:14:05 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:14:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:05 @agent_ppo2.py:186][0m |           0.0093 |           5.4263 |           8.4463 |
[32m[20230113 20:14:05 @agent_ppo2.py:186][0m |          -0.0079 |           4.6440 |           8.4430 |
[32m[20230113 20:14:05 @agent_ppo2.py:186][0m |          -0.0117 |           4.1905 |           8.4396 |
[32m[20230113 20:14:05 @agent_ppo2.py:186][0m |          -0.0050 |           4.0631 |           8.4472 |
[32m[20230113 20:14:05 @agent_ppo2.py:186][0m |          -0.0084 |           3.9049 |           8.4406 |
[32m[20230113 20:14:05 @agent_ppo2.py:186][0m |          -0.0127 |           3.8309 |           8.4465 |
[32m[20230113 20:14:05 @agent_ppo2.py:186][0m |          -0.0097 |           3.7737 |           8.4423 |
[32m[20230113 20:14:05 @agent_ppo2.py:186][0m |          -0.0155 |           3.5704 |           8.4494 |
[32m[20230113 20:14:05 @agent_ppo2.py:186][0m |          -0.0164 |           3.5339 |           8.4488 |
[32m[20230113 20:14:05 @agent_ppo2.py:186][0m |          -0.0189 |           3.4872 |           8.4449 |
[32m[20230113 20:14:05 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.04
[32m[20230113 20:14:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.99
[32m[20230113 20:14:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.42
[32m[20230113 20:14:06 @agent_ppo2.py:144][0m Total time:      29.55 min
[32m[20230113 20:14:06 @agent_ppo2.py:146][0m 2721792 total steps have happened
[32m[20230113 20:14:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1329 --------------------------#
[32m[20230113 20:14:06 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:06 @agent_ppo2.py:186][0m |          -0.0009 |           6.1184 |           8.7590 |
[32m[20230113 20:14:06 @agent_ppo2.py:186][0m |          -0.0057 |           4.8251 |           8.7120 |
[32m[20230113 20:14:06 @agent_ppo2.py:186][0m |          -0.0063 |           4.2670 |           8.7179 |
[32m[20230113 20:14:06 @agent_ppo2.py:186][0m |          -0.0094 |           4.0594 |           8.7128 |
[32m[20230113 20:14:06 @agent_ppo2.py:186][0m |          -0.0130 |           3.7295 |           8.6950 |
[32m[20230113 20:14:06 @agent_ppo2.py:186][0m |          -0.0137 |           3.6544 |           8.7088 |
[32m[20230113 20:14:06 @agent_ppo2.py:186][0m |          -0.0119 |           3.5060 |           8.6935 |
[32m[20230113 20:14:07 @agent_ppo2.py:186][0m |          -0.0164 |           3.3683 |           8.7006 |
[32m[20230113 20:14:07 @agent_ppo2.py:186][0m |          -0.0150 |           3.2615 |           8.7059 |
[32m[20230113 20:14:07 @agent_ppo2.py:186][0m |          -0.0135 |           3.2210 |           8.6980 |
[32m[20230113 20:14:07 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.37
[32m[20230113 20:14:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.39
[32m[20230113 20:14:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.17
[32m[20230113 20:14:07 @agent_ppo2.py:144][0m Total time:      29.57 min
[32m[20230113 20:14:07 @agent_ppo2.py:146][0m 2723840 total steps have happened
[32m[20230113 20:14:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1330 --------------------------#
[32m[20230113 20:14:07 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:08 @agent_ppo2.py:186][0m |           0.0001 |           5.4569 |           8.5838 |
[32m[20230113 20:14:08 @agent_ppo2.py:186][0m |          -0.0065 |           3.8726 |           8.5768 |
[32m[20230113 20:14:08 @agent_ppo2.py:186][0m |          -0.0074 |           3.4054 |           8.5729 |
[32m[20230113 20:14:08 @agent_ppo2.py:186][0m |          -0.0110 |           3.1606 |           8.5682 |
[32m[20230113 20:14:08 @agent_ppo2.py:186][0m |          -0.0122 |           2.9721 |           8.5650 |
[32m[20230113 20:14:08 @agent_ppo2.py:186][0m |          -0.0124 |           2.8289 |           8.5689 |
[32m[20230113 20:14:08 @agent_ppo2.py:186][0m |          -0.0133 |           2.7155 |           8.5718 |
[32m[20230113 20:14:08 @agent_ppo2.py:186][0m |          -0.0157 |           2.6160 |           8.5679 |
[32m[20230113 20:14:08 @agent_ppo2.py:186][0m |          -0.0157 |           2.5219 |           8.5671 |
[32m[20230113 20:14:08 @agent_ppo2.py:186][0m |          -0.0156 |           2.4429 |           8.5762 |
[32m[20230113 20:14:08 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.67
[32m[20230113 20:14:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.93
[32m[20230113 20:14:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.55
[32m[20230113 20:14:08 @agent_ppo2.py:144][0m Total time:      29.60 min
[32m[20230113 20:14:08 @agent_ppo2.py:146][0m 2725888 total steps have happened
[32m[20230113 20:14:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1331 --------------------------#
[32m[20230113 20:14:09 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:09 @agent_ppo2.py:186][0m |          -0.0004 |           5.9366 |           8.5132 |
[32m[20230113 20:14:09 @agent_ppo2.py:186][0m |          -0.0062 |           4.8077 |           8.4914 |
[32m[20230113 20:14:09 @agent_ppo2.py:186][0m |          -0.0105 |           4.2230 |           8.4798 |
[32m[20230113 20:14:09 @agent_ppo2.py:186][0m |          -0.0111 |           3.8425 |           8.4689 |
[32m[20230113 20:14:09 @agent_ppo2.py:186][0m |          -0.0132 |           3.6854 |           8.4682 |
[32m[20230113 20:14:09 @agent_ppo2.py:186][0m |          -0.0138 |           3.5119 |           8.4734 |
[32m[20230113 20:14:09 @agent_ppo2.py:186][0m |          -0.0139 |           3.3685 |           8.4633 |
[32m[20230113 20:14:09 @agent_ppo2.py:186][0m |          -0.0150 |           3.2925 |           8.4649 |
[32m[20230113 20:14:09 @agent_ppo2.py:186][0m |          -0.0150 |           3.1983 |           8.4711 |
[32m[20230113 20:14:09 @agent_ppo2.py:186][0m |          -0.0164 |           3.1380 |           8.4781 |
[32m[20230113 20:14:09 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.42
[32m[20230113 20:14:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.13
[32m[20230113 20:14:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.76
[32m[20230113 20:14:10 @agent_ppo2.py:144][0m Total time:      29.62 min
[32m[20230113 20:14:10 @agent_ppo2.py:146][0m 2727936 total steps have happened
[32m[20230113 20:14:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1332 --------------------------#
[32m[20230113 20:14:10 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:14:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:10 @agent_ppo2.py:186][0m |          -0.0017 |          15.9450 |           8.6754 |
[32m[20230113 20:14:10 @agent_ppo2.py:186][0m |          -0.0053 |           5.0909 |           8.6709 |
[32m[20230113 20:14:10 @agent_ppo2.py:186][0m |          -0.0068 |           4.0960 |           8.6693 |
[32m[20230113 20:14:10 @agent_ppo2.py:186][0m |          -0.0109 |           3.6520 |           8.6694 |
[32m[20230113 20:14:10 @agent_ppo2.py:186][0m |          -0.0122 |           3.3368 |           8.6716 |
[32m[20230113 20:14:10 @agent_ppo2.py:186][0m |          -0.0147 |           3.0938 |           8.6694 |
[32m[20230113 20:14:10 @agent_ppo2.py:186][0m |          -0.0156 |           2.9612 |           8.6682 |
[32m[20230113 20:14:10 @agent_ppo2.py:186][0m |          -0.0118 |           2.7890 |           8.6648 |
[32m[20230113 20:14:10 @agent_ppo2.py:186][0m |          -0.0129 |           2.7394 |           8.6647 |
[32m[20230113 20:14:10 @agent_ppo2.py:186][0m |          -0.0164 |           2.5944 |           8.6642 |
[32m[20230113 20:14:10 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:14:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 169.03
[32m[20230113 20:14:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.56
[32m[20230113 20:14:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.12
[32m[20230113 20:14:11 @agent_ppo2.py:144][0m Total time:      29.64 min
[32m[20230113 20:14:11 @agent_ppo2.py:146][0m 2729984 total steps have happened
[32m[20230113 20:14:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1333 --------------------------#
[32m[20230113 20:14:11 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 20:14:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:11 @agent_ppo2.py:186][0m |          -0.0030 |          15.7230 |           8.5129 |
[32m[20230113 20:14:11 @agent_ppo2.py:186][0m |          -0.0091 |          11.0584 |           8.5014 |
[32m[20230113 20:14:11 @agent_ppo2.py:186][0m |          -0.0091 |           9.2444 |           8.4928 |
[32m[20230113 20:14:11 @agent_ppo2.py:186][0m |          -0.0153 |           7.9869 |           8.4994 |
[32m[20230113 20:14:11 @agent_ppo2.py:186][0m |          -0.0128 |           7.3785 |           8.5001 |
[32m[20230113 20:14:11 @agent_ppo2.py:186][0m |          -0.0170 |           6.8388 |           8.4929 |
[32m[20230113 20:14:11 @agent_ppo2.py:186][0m |          -0.0116 |           6.4865 |           8.5004 |
[32m[20230113 20:14:11 @agent_ppo2.py:186][0m |          -0.0225 |           6.0410 |           8.4910 |
[32m[20230113 20:14:11 @agent_ppo2.py:186][0m |          -0.0211 |           5.6034 |           8.4985 |
[32m[20230113 20:14:12 @agent_ppo2.py:186][0m |          -0.0081 |           5.6934 |           8.4998 |
[32m[20230113 20:14:12 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:14:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 121.58
[32m[20230113 20:14:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.99
[32m[20230113 20:14:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.58
[32m[20230113 20:14:12 @agent_ppo2.py:144][0m Total time:      29.66 min
[32m[20230113 20:14:12 @agent_ppo2.py:146][0m 2732032 total steps have happened
[32m[20230113 20:14:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1334 --------------------------#
[32m[20230113 20:14:12 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:12 @agent_ppo2.py:186][0m |          -0.0007 |          12.3567 |           8.6685 |
[32m[20230113 20:14:13 @agent_ppo2.py:186][0m |          -0.0051 |           8.1168 |           8.6652 |
[32m[20230113 20:14:13 @agent_ppo2.py:186][0m |          -0.0093 |           7.1233 |           8.6659 |
[32m[20230113 20:14:13 @agent_ppo2.py:186][0m |          -0.0112 |           6.4830 |           8.6699 |
[32m[20230113 20:14:13 @agent_ppo2.py:186][0m |          -0.0140 |           6.0465 |           8.6625 |
[32m[20230113 20:14:13 @agent_ppo2.py:186][0m |          -0.0147 |           5.6850 |           8.6699 |
[32m[20230113 20:14:13 @agent_ppo2.py:186][0m |          -0.0156 |           5.3898 |           8.6636 |
[32m[20230113 20:14:13 @agent_ppo2.py:186][0m |          -0.0173 |           5.1070 |           8.6641 |
[32m[20230113 20:14:13 @agent_ppo2.py:186][0m |          -0.0185 |           4.9311 |           8.6632 |
[32m[20230113 20:14:13 @agent_ppo2.py:186][0m |          -0.0185 |           4.6885 |           8.6708 |
[32m[20230113 20:14:13 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.32
[32m[20230113 20:14:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.43
[32m[20230113 20:14:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.78
[32m[20230113 20:14:13 @agent_ppo2.py:144][0m Total time:      29.68 min
[32m[20230113 20:14:13 @agent_ppo2.py:146][0m 2734080 total steps have happened
[32m[20230113 20:14:13 @agent_ppo2.py:122][0m #------------------------ Iteration 1335 --------------------------#
[32m[20230113 20:14:14 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:14 @agent_ppo2.py:186][0m |           0.0008 |           7.4846 |           8.7485 |
[32m[20230113 20:14:14 @agent_ppo2.py:186][0m |          -0.0060 |           5.5071 |           8.7472 |
[32m[20230113 20:14:14 @agent_ppo2.py:186][0m |          -0.0098 |           4.9353 |           8.7413 |
[32m[20230113 20:14:14 @agent_ppo2.py:186][0m |          -0.0127 |           4.5810 |           8.7368 |
[32m[20230113 20:14:14 @agent_ppo2.py:186][0m |          -0.0113 |           4.3132 |           8.7336 |
[32m[20230113 20:14:14 @agent_ppo2.py:186][0m |          -0.0139 |           4.1251 |           8.7284 |
[32m[20230113 20:14:14 @agent_ppo2.py:186][0m |          -0.0101 |           3.9528 |           8.7328 |
[32m[20230113 20:14:14 @agent_ppo2.py:186][0m |          -0.0096 |           3.8413 |           8.7265 |
[32m[20230113 20:14:14 @agent_ppo2.py:186][0m |          -0.0136 |           3.7206 |           8.7293 |
[32m[20230113 20:14:14 @agent_ppo2.py:186][0m |          -0.0124 |           3.6301 |           8.7259 |
[32m[20230113 20:14:14 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:14:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.56
[32m[20230113 20:14:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.17
[32m[20230113 20:14:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.85
[32m[20230113 20:14:15 @agent_ppo2.py:144][0m Total time:      29.70 min
[32m[20230113 20:14:15 @agent_ppo2.py:146][0m 2736128 total steps have happened
[32m[20230113 20:14:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1336 --------------------------#
[32m[20230113 20:14:15 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:15 @agent_ppo2.py:186][0m |          -0.0005 |           6.1735 |           8.7174 |
[32m[20230113 20:14:15 @agent_ppo2.py:186][0m |          -0.0092 |           5.2391 |           8.7158 |
[32m[20230113 20:14:15 @agent_ppo2.py:186][0m |          -0.0114 |           4.8011 |           8.7083 |
[32m[20230113 20:14:15 @agent_ppo2.py:186][0m |          -0.0067 |           4.6283 |           8.7070 |
[32m[20230113 20:14:15 @agent_ppo2.py:186][0m |          -0.0066 |           4.4459 |           8.7080 |
[32m[20230113 20:14:15 @agent_ppo2.py:186][0m |          -0.0160 |           4.2300 |           8.6987 |
[32m[20230113 20:14:15 @agent_ppo2.py:186][0m |          -0.0158 |           4.0767 |           8.7053 |
[32m[20230113 20:14:15 @agent_ppo2.py:186][0m |          -0.0141 |           3.9828 |           8.7025 |
[32m[20230113 20:14:15 @agent_ppo2.py:186][0m |          -0.0068 |           3.9276 |           8.6984 |
[32m[20230113 20:14:15 @agent_ppo2.py:186][0m |          -0.0127 |           3.7638 |           8.7022 |
[32m[20230113 20:14:15 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.53
[32m[20230113 20:14:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.83
[32m[20230113 20:14:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.14
[32m[20230113 20:14:16 @agent_ppo2.py:144][0m Total time:      29.72 min
[32m[20230113 20:14:16 @agent_ppo2.py:146][0m 2738176 total steps have happened
[32m[20230113 20:14:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1337 --------------------------#
[32m[20230113 20:14:16 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:16 @agent_ppo2.py:186][0m |           0.0026 |           6.1942 |           8.5669 |
[32m[20230113 20:14:16 @agent_ppo2.py:186][0m |          -0.0075 |           4.6120 |           8.5512 |
[32m[20230113 20:14:16 @agent_ppo2.py:186][0m |          -0.0071 |           4.0261 |           8.5398 |
[32m[20230113 20:14:16 @agent_ppo2.py:186][0m |          -0.0098 |           3.7116 |           8.5327 |
[32m[20230113 20:14:17 @agent_ppo2.py:186][0m |          -0.0121 |           3.5679 |           8.5262 |
[32m[20230113 20:14:17 @agent_ppo2.py:186][0m |          -0.0099 |           3.3408 |           8.5307 |
[32m[20230113 20:14:17 @agent_ppo2.py:186][0m |          -0.0077 |           3.3243 |           8.5268 |
[32m[20230113 20:14:17 @agent_ppo2.py:186][0m |          -0.0124 |           3.1236 |           8.5302 |
[32m[20230113 20:14:17 @agent_ppo2.py:186][0m |          -0.0146 |           3.0477 |           8.5309 |
[32m[20230113 20:14:17 @agent_ppo2.py:186][0m |          -0.0153 |           2.9863 |           8.5251 |
[32m[20230113 20:14:17 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.14
[32m[20230113 20:14:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.01
[32m[20230113 20:14:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.47
[32m[20230113 20:14:17 @agent_ppo2.py:144][0m Total time:      29.74 min
[32m[20230113 20:14:17 @agent_ppo2.py:146][0m 2740224 total steps have happened
[32m[20230113 20:14:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1338 --------------------------#
[32m[20230113 20:14:18 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:14:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:18 @agent_ppo2.py:186][0m |          -0.0005 |          12.4966 |           8.7046 |
[32m[20230113 20:14:18 @agent_ppo2.py:186][0m |          -0.0073 |           6.7020 |           8.7134 |
[32m[20230113 20:14:18 @agent_ppo2.py:186][0m |          -0.0093 |           5.6051 |           8.7105 |
[32m[20230113 20:14:18 @agent_ppo2.py:186][0m |          -0.0097 |           5.2020 |           8.7074 |
[32m[20230113 20:14:18 @agent_ppo2.py:186][0m |          -0.0109 |           4.8055 |           8.7141 |
[32m[20230113 20:14:18 @agent_ppo2.py:186][0m |          -0.0124 |           4.4640 |           8.7126 |
[32m[20230113 20:14:18 @agent_ppo2.py:186][0m |          -0.0125 |           4.3434 |           8.7102 |
[32m[20230113 20:14:18 @agent_ppo2.py:186][0m |          -0.0125 |           4.2207 |           8.7177 |
[32m[20230113 20:14:18 @agent_ppo2.py:186][0m |          -0.0138 |           4.0852 |           8.7108 |
[32m[20230113 20:14:18 @agent_ppo2.py:186][0m |          -0.0139 |           3.9682 |           8.7077 |
[32m[20230113 20:14:18 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:14:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 130.11
[32m[20230113 20:14:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.54
[32m[20230113 20:14:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 153.73
[32m[20230113 20:14:19 @agent_ppo2.py:144][0m Total time:      29.77 min
[32m[20230113 20:14:19 @agent_ppo2.py:146][0m 2742272 total steps have happened
[32m[20230113 20:14:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1339 --------------------------#
[32m[20230113 20:14:19 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:14:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:19 @agent_ppo2.py:186][0m |          -0.0011 |           5.8764 |           8.6916 |
[32m[20230113 20:14:19 @agent_ppo2.py:186][0m |          -0.0074 |           4.2255 |           8.6692 |
[32m[20230113 20:14:19 @agent_ppo2.py:186][0m |          -0.0116 |           3.6549 |           8.6749 |
[32m[20230113 20:14:19 @agent_ppo2.py:186][0m |          -0.0106 |           3.2824 |           8.6682 |
[32m[20230113 20:14:19 @agent_ppo2.py:186][0m |          -0.0125 |           3.0316 |           8.6660 |
[32m[20230113 20:14:19 @agent_ppo2.py:186][0m |          -0.0134 |           2.8677 |           8.6700 |
[32m[20230113 20:14:19 @agent_ppo2.py:186][0m |          -0.0152 |           2.7387 |           8.6720 |
[32m[20230113 20:14:19 @agent_ppo2.py:186][0m |          -0.0137 |           2.6553 |           8.6679 |
[32m[20230113 20:14:19 @agent_ppo2.py:186][0m |          -0.0150 |           2.5775 |           8.6721 |
[32m[20230113 20:14:20 @agent_ppo2.py:186][0m |          -0.0167 |           2.5328 |           8.6670 |
[32m[20230113 20:14:20 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.66
[32m[20230113 20:14:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.79
[32m[20230113 20:14:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.65
[32m[20230113 20:14:20 @agent_ppo2.py:144][0m Total time:      29.79 min
[32m[20230113 20:14:20 @agent_ppo2.py:146][0m 2744320 total steps have happened
[32m[20230113 20:14:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1340 --------------------------#
[32m[20230113 20:14:20 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:20 @agent_ppo2.py:186][0m |           0.0004 |           5.4418 |           8.8258 |
[32m[20230113 20:14:20 @agent_ppo2.py:186][0m |          -0.0078 |           4.4123 |           8.8249 |
[32m[20230113 20:14:21 @agent_ppo2.py:186][0m |          -0.0107 |           3.9878 |           8.8232 |
[32m[20230113 20:14:21 @agent_ppo2.py:186][0m |          -0.0121 |           3.7511 |           8.8255 |
[32m[20230113 20:14:21 @agent_ppo2.py:186][0m |          -0.0133 |           3.5919 |           8.8284 |
[32m[20230113 20:14:21 @agent_ppo2.py:186][0m |          -0.0143 |           3.4498 |           8.8197 |
[32m[20230113 20:14:21 @agent_ppo2.py:186][0m |          -0.0153 |           3.3432 |           8.8260 |
[32m[20230113 20:14:21 @agent_ppo2.py:186][0m |          -0.0159 |           3.2574 |           8.8212 |
[32m[20230113 20:14:21 @agent_ppo2.py:186][0m |          -0.0164 |           3.1751 |           8.8242 |
[32m[20230113 20:14:21 @agent_ppo2.py:186][0m |          -0.0166 |           3.0893 |           8.8264 |
[32m[20230113 20:14:21 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.03
[32m[20230113 20:14:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.07
[32m[20230113 20:14:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.65
[32m[20230113 20:14:21 @agent_ppo2.py:144][0m Total time:      29.81 min
[32m[20230113 20:14:21 @agent_ppo2.py:146][0m 2746368 total steps have happened
[32m[20230113 20:14:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1341 --------------------------#
[32m[20230113 20:14:22 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:22 @agent_ppo2.py:186][0m |           0.0024 |           5.9350 |           8.6559 |
[32m[20230113 20:14:22 @agent_ppo2.py:186][0m |          -0.0023 |           4.8188 |           8.6419 |
[32m[20230113 20:14:22 @agent_ppo2.py:186][0m |          -0.0098 |           4.3432 |           8.6351 |
[32m[20230113 20:14:22 @agent_ppo2.py:186][0m |          -0.0132 |           4.1058 |           8.6465 |
[32m[20230113 20:14:22 @agent_ppo2.py:186][0m |          -0.0006 |           3.9868 |           8.6303 |
[32m[20230113 20:14:22 @agent_ppo2.py:186][0m |          -0.0118 |           3.7706 |           8.6338 |
[32m[20230113 20:14:22 @agent_ppo2.py:186][0m |          -0.0149 |           3.6153 |           8.6250 |
[32m[20230113 20:14:22 @agent_ppo2.py:186][0m |          -0.0094 |           3.5014 |           8.6258 |
[32m[20230113 20:14:22 @agent_ppo2.py:186][0m |          -0.0083 |           3.4019 |           8.6202 |
[32m[20230113 20:14:22 @agent_ppo2.py:186][0m |          -0.0166 |           3.3275 |           8.6171 |
[32m[20230113 20:14:22 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.26
[32m[20230113 20:14:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.41
[32m[20230113 20:14:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.66
[32m[20230113 20:14:23 @agent_ppo2.py:144][0m Total time:      29.83 min
[32m[20230113 20:14:23 @agent_ppo2.py:146][0m 2748416 total steps have happened
[32m[20230113 20:14:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1342 --------------------------#
[32m[20230113 20:14:23 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:23 @agent_ppo2.py:186][0m |           0.0011 |          19.3415 |           8.6252 |
[32m[20230113 20:14:23 @agent_ppo2.py:186][0m |          -0.0066 |          10.8252 |           8.6235 |
[32m[20230113 20:14:23 @agent_ppo2.py:186][0m |          -0.0074 |           8.1829 |           8.6245 |
[32m[20230113 20:14:23 @agent_ppo2.py:186][0m |          -0.0107 |           6.8332 |           8.6265 |
[32m[20230113 20:14:23 @agent_ppo2.py:186][0m |          -0.0117 |           6.2206 |           8.6224 |
[32m[20230113 20:14:23 @agent_ppo2.py:186][0m |          -0.0104 |           5.4674 |           8.6286 |
[32m[20230113 20:14:23 @agent_ppo2.py:186][0m |          -0.0109 |           5.4320 |           8.6254 |
[32m[20230113 20:14:23 @agent_ppo2.py:186][0m |          -0.0159 |           4.8640 |           8.6209 |
[32m[20230113 20:14:23 @agent_ppo2.py:186][0m |          -0.0147 |           4.6807 |           8.6252 |
[32m[20230113 20:14:23 @agent_ppo2.py:186][0m |          -0.0151 |           4.4796 |           8.6231 |
[32m[20230113 20:14:23 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 73.33
[32m[20230113 20:14:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.22
[32m[20230113 20:14:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.81
[32m[20230113 20:14:24 @agent_ppo2.py:144][0m Total time:      29.85 min
[32m[20230113 20:14:24 @agent_ppo2.py:146][0m 2750464 total steps have happened
[32m[20230113 20:14:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1343 --------------------------#
[32m[20230113 20:14:24 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:14:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:24 @agent_ppo2.py:186][0m |          -0.0018 |           6.2962 |           8.8726 |
[32m[20230113 20:14:24 @agent_ppo2.py:186][0m |          -0.0039 |           4.5084 |           8.8697 |
[32m[20230113 20:14:24 @agent_ppo2.py:186][0m |          -0.0041 |           3.9620 |           8.8637 |
[32m[20230113 20:14:24 @agent_ppo2.py:186][0m |          -0.0123 |           3.6267 |           8.8604 |
[32m[20230113 20:14:24 @agent_ppo2.py:186][0m |          -0.0170 |           3.4231 |           8.8592 |
[32m[20230113 20:14:25 @agent_ppo2.py:186][0m |          -0.0074 |           3.2216 |           8.8610 |
[32m[20230113 20:14:25 @agent_ppo2.py:186][0m |          -0.0097 |           3.0668 |           8.8560 |
[32m[20230113 20:14:25 @agent_ppo2.py:186][0m |          -0.0101 |           3.0110 |           8.8532 |
[32m[20230113 20:14:25 @agent_ppo2.py:186][0m |          -0.0122 |           2.8510 |           8.8530 |
[32m[20230113 20:14:25 @agent_ppo2.py:186][0m |          -0.0140 |           2.7608 |           8.8573 |
[32m[20230113 20:14:25 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.47
[32m[20230113 20:14:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.62
[32m[20230113 20:14:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.01
[32m[20230113 20:14:25 @agent_ppo2.py:144][0m Total time:      29.88 min
[32m[20230113 20:14:25 @agent_ppo2.py:146][0m 2752512 total steps have happened
[32m[20230113 20:14:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1344 --------------------------#
[32m[20230113 20:14:26 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:26 @agent_ppo2.py:186][0m |          -0.0004 |           5.5852 |           8.6227 |
[32m[20230113 20:14:26 @agent_ppo2.py:186][0m |          -0.0066 |           4.6934 |           8.6155 |
[32m[20230113 20:14:26 @agent_ppo2.py:186][0m |          -0.0089 |           4.2624 |           8.6118 |
[32m[20230113 20:14:26 @agent_ppo2.py:186][0m |          -0.0100 |           4.0266 |           8.6108 |
[32m[20230113 20:14:26 @agent_ppo2.py:186][0m |          -0.0103 |           3.8490 |           8.6096 |
[32m[20230113 20:14:26 @agent_ppo2.py:186][0m |          -0.0116 |           3.6934 |           8.6098 |
[32m[20230113 20:14:26 @agent_ppo2.py:186][0m |          -0.0125 |           3.5677 |           8.6153 |
[32m[20230113 20:14:26 @agent_ppo2.py:186][0m |          -0.0120 |           3.5091 |           8.6051 |
[32m[20230113 20:14:26 @agent_ppo2.py:186][0m |          -0.0139 |           3.4075 |           8.6089 |
[32m[20230113 20:14:26 @agent_ppo2.py:186][0m |          -0.0134 |           3.3572 |           8.6120 |
[32m[20230113 20:14:26 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.02
[32m[20230113 20:14:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.12
[32m[20230113 20:14:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.17
[32m[20230113 20:14:26 @agent_ppo2.py:144][0m Total time:      29.90 min
[32m[20230113 20:14:26 @agent_ppo2.py:146][0m 2754560 total steps have happened
[32m[20230113 20:14:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1345 --------------------------#
[32m[20230113 20:14:27 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:27 @agent_ppo2.py:186][0m |           0.0020 |           5.4738 |           8.7653 |
[32m[20230113 20:14:27 @agent_ppo2.py:186][0m |          -0.0045 |           4.4822 |           8.7615 |
[32m[20230113 20:14:27 @agent_ppo2.py:186][0m |          -0.0081 |           4.0278 |           8.7550 |
[32m[20230113 20:14:27 @agent_ppo2.py:186][0m |          -0.0108 |           3.7966 |           8.7527 |
[32m[20230113 20:14:27 @agent_ppo2.py:186][0m |          -0.0137 |           3.6100 |           8.7428 |
[32m[20230113 20:14:27 @agent_ppo2.py:186][0m |          -0.0138 |           3.4697 |           8.7425 |
[32m[20230113 20:14:27 @agent_ppo2.py:186][0m |          -0.0146 |           3.3732 |           8.7381 |
[32m[20230113 20:14:27 @agent_ppo2.py:186][0m |          -0.0157 |           3.2654 |           8.7406 |
[32m[20230113 20:14:27 @agent_ppo2.py:186][0m |          -0.0160 |           3.1612 |           8.7318 |
[32m[20230113 20:14:27 @agent_ppo2.py:186][0m |          -0.0152 |           3.0995 |           8.7263 |
[32m[20230113 20:14:27 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.39
[32m[20230113 20:14:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.58
[32m[20230113 20:14:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.52
[32m[20230113 20:14:28 @agent_ppo2.py:144][0m Total time:      29.92 min
[32m[20230113 20:14:28 @agent_ppo2.py:146][0m 2756608 total steps have happened
[32m[20230113 20:14:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1346 --------------------------#
[32m[20230113 20:14:28 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:28 @agent_ppo2.py:186][0m |          -0.0025 |           5.7463 |           8.8606 |
[32m[20230113 20:14:28 @agent_ppo2.py:186][0m |          -0.0090 |           4.3715 |           8.8669 |
[32m[20230113 20:14:28 @agent_ppo2.py:186][0m |          -0.0104 |           3.9585 |           8.8521 |
[32m[20230113 20:14:28 @agent_ppo2.py:186][0m |          -0.0092 |           3.7267 |           8.8509 |
[32m[20230113 20:14:28 @agent_ppo2.py:186][0m |          -0.0108 |           3.5230 |           8.8553 |
[32m[20230113 20:14:28 @agent_ppo2.py:186][0m |          -0.0134 |           3.3590 |           8.8459 |
[32m[20230113 20:14:28 @agent_ppo2.py:186][0m |          -0.0128 |           3.2464 |           8.8482 |
[32m[20230113 20:14:29 @agent_ppo2.py:186][0m |          -0.0166 |           3.1545 |           8.8444 |
[32m[20230113 20:14:29 @agent_ppo2.py:186][0m |          -0.0139 |           3.0680 |           8.8460 |
[32m[20230113 20:14:29 @agent_ppo2.py:186][0m |          -0.0150 |           2.9696 |           8.8511 |
[32m[20230113 20:14:29 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:14:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.83
[32m[20230113 20:14:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.86
[32m[20230113 20:14:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.36
[32m[20230113 20:14:29 @agent_ppo2.py:144][0m Total time:      29.94 min
[32m[20230113 20:14:29 @agent_ppo2.py:146][0m 2758656 total steps have happened
[32m[20230113 20:14:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1347 --------------------------#
[32m[20230113 20:14:29 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 20:14:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:29 @agent_ppo2.py:186][0m |          -0.0042 |          12.8359 |           8.7300 |
[32m[20230113 20:14:30 @agent_ppo2.py:186][0m |          -0.0109 |           5.9043 |           8.7215 |
[32m[20230113 20:14:30 @agent_ppo2.py:186][0m |          -0.0109 |           4.9304 |           8.7337 |
[32m[20230113 20:14:30 @agent_ppo2.py:186][0m |          -0.0097 |           4.3672 |           8.7318 |
[32m[20230113 20:14:30 @agent_ppo2.py:186][0m |          -0.0158 |           4.0341 |           8.7332 |
[32m[20230113 20:14:30 @agent_ppo2.py:186][0m |          -0.0026 |           3.7838 |           8.7259 |
[32m[20230113 20:14:30 @agent_ppo2.py:186][0m |           0.0254 |           3.4831 |           8.7336 |
[32m[20230113 20:14:30 @agent_ppo2.py:186][0m |           0.0207 |           3.4941 |           8.7266 |
[32m[20230113 20:14:30 @agent_ppo2.py:186][0m |          -0.0194 |           3.1890 |           8.7394 |
[32m[20230113 20:14:30 @agent_ppo2.py:186][0m |          -0.0217 |           3.1199 |           8.7166 |
[32m[20230113 20:14:30 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:14:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 166.84
[32m[20230113 20:14:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.53
[32m[20230113 20:14:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 158.03
[32m[20230113 20:14:30 @agent_ppo2.py:144][0m Total time:      29.96 min
[32m[20230113 20:14:30 @agent_ppo2.py:146][0m 2760704 total steps have happened
[32m[20230113 20:14:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1348 --------------------------#
[32m[20230113 20:14:31 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 20:14:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:31 @agent_ppo2.py:186][0m |           0.0053 |          24.8036 |           8.8737 |
[32m[20230113 20:14:31 @agent_ppo2.py:186][0m |           0.0019 |          11.5499 |           8.8765 |
[32m[20230113 20:14:31 @agent_ppo2.py:186][0m |          -0.0097 |           8.5210 |           8.8652 |
[32m[20230113 20:14:31 @agent_ppo2.py:186][0m |          -0.0134 |           7.1503 |           8.8610 |
[32m[20230113 20:14:31 @agent_ppo2.py:186][0m |           0.0062 |           6.4416 |           8.8600 |
[32m[20230113 20:14:31 @agent_ppo2.py:186][0m |          -0.0105 |           5.8269 |           8.8464 |
[32m[20230113 20:14:31 @agent_ppo2.py:186][0m |          -0.0140 |           5.5041 |           8.8498 |
[32m[20230113 20:14:31 @agent_ppo2.py:186][0m |          -0.0128 |           5.1776 |           8.8485 |
[32m[20230113 20:14:31 @agent_ppo2.py:186][0m |          -0.0168 |           4.9404 |           8.8484 |
[32m[20230113 20:14:31 @agent_ppo2.py:186][0m |          -0.0146 |           4.7446 |           8.8445 |
[32m[20230113 20:14:31 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 20:14:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 139.98
[32m[20230113 20:14:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.55
[32m[20230113 20:14:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.14
[32m[20230113 20:14:32 @agent_ppo2.py:144][0m Total time:      29.99 min
[32m[20230113 20:14:32 @agent_ppo2.py:146][0m 2762752 total steps have happened
[32m[20230113 20:14:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1349 --------------------------#
[32m[20230113 20:14:32 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:32 @agent_ppo2.py:186][0m |          -0.0001 |           8.0521 |           8.7819 |
[32m[20230113 20:14:32 @agent_ppo2.py:186][0m |          -0.0050 |           6.2422 |           8.7671 |
[32m[20230113 20:14:32 @agent_ppo2.py:186][0m |          -0.0073 |           5.2911 |           8.7770 |
[32m[20230113 20:14:32 @agent_ppo2.py:186][0m |          -0.0089 |           4.7984 |           8.7682 |
[32m[20230113 20:14:32 @agent_ppo2.py:186][0m |          -0.0122 |           4.4883 |           8.7634 |
[32m[20230113 20:14:32 @agent_ppo2.py:186][0m |          -0.0128 |           4.2324 |           8.7574 |
[32m[20230113 20:14:33 @agent_ppo2.py:186][0m |          -0.0136 |           4.0300 |           8.7505 |
[32m[20230113 20:14:33 @agent_ppo2.py:186][0m |          -0.0151 |           3.8777 |           8.7593 |
[32m[20230113 20:14:33 @agent_ppo2.py:186][0m |          -0.0153 |           3.7503 |           8.7517 |
[32m[20230113 20:14:33 @agent_ppo2.py:186][0m |          -0.0165 |           3.6130 |           8.7474 |
[32m[20230113 20:14:33 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.20
[32m[20230113 20:14:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.67
[32m[20230113 20:14:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.97
[32m[20230113 20:14:33 @agent_ppo2.py:144][0m Total time:      30.01 min
[32m[20230113 20:14:33 @agent_ppo2.py:146][0m 2764800 total steps have happened
[32m[20230113 20:14:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1350 --------------------------#
[32m[20230113 20:14:34 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:34 @agent_ppo2.py:186][0m |           0.0004 |          10.7009 |           8.8876 |
[32m[20230113 20:14:34 @agent_ppo2.py:186][0m |          -0.0055 |           7.8801 |           8.8857 |
[32m[20230113 20:14:34 @agent_ppo2.py:186][0m |          -0.0094 |           7.0002 |           8.8825 |
[32m[20230113 20:14:34 @agent_ppo2.py:186][0m |          -0.0104 |           6.4303 |           8.8870 |
[32m[20230113 20:14:34 @agent_ppo2.py:186][0m |          -0.0140 |           5.9790 |           8.8844 |
[32m[20230113 20:14:34 @agent_ppo2.py:186][0m |          -0.0138 |           5.6850 |           8.8838 |
[32m[20230113 20:14:34 @agent_ppo2.py:186][0m |          -0.0142 |           5.4452 |           8.8832 |
[32m[20230113 20:14:34 @agent_ppo2.py:186][0m |          -0.0160 |           5.2397 |           8.8842 |
[32m[20230113 20:14:34 @agent_ppo2.py:186][0m |          -0.0166 |           5.0326 |           8.8876 |
[32m[20230113 20:14:34 @agent_ppo2.py:186][0m |          -0.0175 |           4.8710 |           8.8910 |
[32m[20230113 20:14:34 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.24
[32m[20230113 20:14:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.33
[32m[20230113 20:14:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.39
[32m[20230113 20:14:34 @agent_ppo2.py:144][0m Total time:      30.03 min
[32m[20230113 20:14:34 @agent_ppo2.py:146][0m 2766848 total steps have happened
[32m[20230113 20:14:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1351 --------------------------#
[32m[20230113 20:14:35 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:35 @agent_ppo2.py:186][0m |           0.0011 |           8.0553 |           8.7955 |
[32m[20230113 20:14:35 @agent_ppo2.py:186][0m |          -0.0017 |           5.9226 |           8.7807 |
[32m[20230113 20:14:35 @agent_ppo2.py:186][0m |          -0.0075 |           5.0828 |           8.7807 |
[32m[20230113 20:14:35 @agent_ppo2.py:186][0m |          -0.0072 |           4.6727 |           8.7877 |
[32m[20230113 20:14:35 @agent_ppo2.py:186][0m |          -0.0108 |           4.4508 |           8.7826 |
[32m[20230113 20:14:35 @agent_ppo2.py:186][0m |          -0.0060 |           4.2269 |           8.7807 |
[32m[20230113 20:14:35 @agent_ppo2.py:186][0m |          -0.0081 |           4.1653 |           8.7796 |
[32m[20230113 20:14:35 @agent_ppo2.py:186][0m |          -0.0148 |           3.9563 |           8.7802 |
[32m[20230113 20:14:35 @agent_ppo2.py:186][0m |          -0.0146 |           3.7778 |           8.7775 |
[32m[20230113 20:14:35 @agent_ppo2.py:186][0m |          -0.0105 |           3.7721 |           8.7864 |
[32m[20230113 20:14:35 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.08
[32m[20230113 20:14:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.70
[32m[20230113 20:14:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 173.46
[32m[20230113 20:14:36 @agent_ppo2.py:144][0m Total time:      30.05 min
[32m[20230113 20:14:36 @agent_ppo2.py:146][0m 2768896 total steps have happened
[32m[20230113 20:14:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1352 --------------------------#
[32m[20230113 20:14:36 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:36 @agent_ppo2.py:186][0m |          -0.0311 |          13.0173 |           8.7785 |
[32m[20230113 20:14:36 @agent_ppo2.py:186][0m |          -0.0073 |           7.5722 |           8.7558 |
[32m[20230113 20:14:36 @agent_ppo2.py:186][0m |          -0.0228 |           6.5837 |           8.7670 |
[32m[20230113 20:14:36 @agent_ppo2.py:186][0m |          -0.0044 |           6.4367 |           8.7484 |
[32m[20230113 20:14:36 @agent_ppo2.py:186][0m |          -0.0195 |           5.4986 |           8.7536 |
[32m[20230113 20:14:36 @agent_ppo2.py:186][0m |          -0.0132 |           5.1139 |           8.7533 |
[32m[20230113 20:14:36 @agent_ppo2.py:186][0m |          -0.0271 |           4.9521 |           8.7564 |
[32m[20230113 20:14:36 @agent_ppo2.py:186][0m |          -0.0147 |           4.6676 |           8.7627 |
[32m[20230113 20:14:37 @agent_ppo2.py:186][0m |          -0.0215 |           4.5723 |           8.7583 |
[32m[20230113 20:14:37 @agent_ppo2.py:186][0m |          -0.0174 |           4.4603 |           8.7553 |
[32m[20230113 20:14:37 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:14:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 117.13
[32m[20230113 20:14:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.86
[32m[20230113 20:14:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.99
[32m[20230113 20:14:37 @agent_ppo2.py:144][0m Total time:      30.07 min
[32m[20230113 20:14:37 @agent_ppo2.py:146][0m 2770944 total steps have happened
[32m[20230113 20:14:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1353 --------------------------#
[32m[20230113 20:14:37 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:38 @agent_ppo2.py:186][0m |          -0.0000 |           6.2476 |           9.0215 |
[32m[20230113 20:14:38 @agent_ppo2.py:186][0m |          -0.0045 |           4.5499 |           9.0347 |
[32m[20230113 20:14:38 @agent_ppo2.py:186][0m |          -0.0114 |           4.1236 |           9.0317 |
[32m[20230113 20:14:38 @agent_ppo2.py:186][0m |          -0.0121 |           3.8148 |           9.0303 |
[32m[20230113 20:14:38 @agent_ppo2.py:186][0m |          -0.0136 |           3.6429 |           9.0241 |
[32m[20230113 20:14:38 @agent_ppo2.py:186][0m |          -0.0155 |           3.5234 |           9.0317 |
[32m[20230113 20:14:38 @agent_ppo2.py:186][0m |          -0.0145 |           3.3735 |           9.0225 |
[32m[20230113 20:14:38 @agent_ppo2.py:186][0m |          -0.0192 |           3.2654 |           9.0315 |
[32m[20230113 20:14:38 @agent_ppo2.py:186][0m |          -0.0166 |           3.1842 |           9.0323 |
[32m[20230113 20:14:38 @agent_ppo2.py:186][0m |          -0.0191 |           3.1210 |           9.0324 |
[32m[20230113 20:14:38 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.73
[32m[20230113 20:14:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.37
[32m[20230113 20:14:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.16
[32m[20230113 20:14:38 @agent_ppo2.py:144][0m Total time:      30.10 min
[32m[20230113 20:14:38 @agent_ppo2.py:146][0m 2772992 total steps have happened
[32m[20230113 20:14:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1354 --------------------------#
[32m[20230113 20:14:39 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:39 @agent_ppo2.py:186][0m |           0.0022 |           5.9063 |           8.8972 |
[32m[20230113 20:14:39 @agent_ppo2.py:186][0m |          -0.0066 |           4.6653 |           8.8922 |
[32m[20230113 20:14:39 @agent_ppo2.py:186][0m |          -0.0099 |           4.2275 |           8.8827 |
[32m[20230113 20:14:39 @agent_ppo2.py:186][0m |          -0.0119 |           3.9753 |           8.8833 |
[32m[20230113 20:14:39 @agent_ppo2.py:186][0m |          -0.0126 |           3.7891 |           8.8855 |
[32m[20230113 20:14:39 @agent_ppo2.py:186][0m |          -0.0136 |           3.6717 |           8.8857 |
[32m[20230113 20:14:39 @agent_ppo2.py:186][0m |          -0.0139 |           3.5594 |           8.8922 |
[32m[20230113 20:14:39 @agent_ppo2.py:186][0m |          -0.0151 |           3.4459 |           8.8879 |
[32m[20230113 20:14:39 @agent_ppo2.py:186][0m |          -0.0158 |           3.3584 |           8.8962 |
[32m[20230113 20:14:39 @agent_ppo2.py:186][0m |          -0.0156 |           3.2942 |           8.8883 |
[32m[20230113 20:14:39 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.83
[32m[20230113 20:14:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.46
[32m[20230113 20:14:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.74
[32m[20230113 20:14:40 @agent_ppo2.py:144][0m Total time:      30.12 min
[32m[20230113 20:14:40 @agent_ppo2.py:146][0m 2775040 total steps have happened
[32m[20230113 20:14:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1355 --------------------------#
[32m[20230113 20:14:40 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:14:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:40 @agent_ppo2.py:186][0m |           0.0028 |           6.0923 |           9.0127 |
[32m[20230113 20:14:40 @agent_ppo2.py:186][0m |          -0.0054 |           4.8694 |           8.9942 |
[32m[20230113 20:14:40 @agent_ppo2.py:186][0m |          -0.0056 |           4.4553 |           9.0030 |
[32m[20230113 20:14:40 @agent_ppo2.py:186][0m |          -0.0098 |           4.1797 |           9.0024 |
[32m[20230113 20:14:40 @agent_ppo2.py:186][0m |          -0.0117 |           3.8960 |           8.9887 |
[32m[20230113 20:14:40 @agent_ppo2.py:186][0m |          -0.0090 |           3.7444 |           8.9883 |
[32m[20230113 20:14:40 @agent_ppo2.py:186][0m |          -0.0126 |           3.6308 |           8.9852 |
[32m[20230113 20:14:40 @agent_ppo2.py:186][0m |          -0.0136 |           3.5507 |           8.9902 |
[32m[20230113 20:14:40 @agent_ppo2.py:186][0m |          -0.0142 |           3.4221 |           8.9843 |
[32m[20230113 20:14:40 @agent_ppo2.py:186][0m |          -0.0160 |           3.3125 |           8.9778 |
[32m[20230113 20:14:40 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:14:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.66
[32m[20230113 20:14:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.77
[32m[20230113 20:14:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.52
[32m[20230113 20:14:41 @agent_ppo2.py:144][0m Total time:      30.14 min
[32m[20230113 20:14:41 @agent_ppo2.py:146][0m 2777088 total steps have happened
[32m[20230113 20:14:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1356 --------------------------#
[32m[20230113 20:14:41 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:14:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:41 @agent_ppo2.py:186][0m |           0.0004 |           6.0594 |           8.9006 |
[32m[20230113 20:14:41 @agent_ppo2.py:186][0m |          -0.0069 |           5.1817 |           8.8789 |
[32m[20230113 20:14:41 @agent_ppo2.py:186][0m |          -0.0057 |           4.7332 |           8.8734 |
[32m[20230113 20:14:41 @agent_ppo2.py:186][0m |          -0.0062 |           4.5317 |           8.8697 |
[32m[20230113 20:14:41 @agent_ppo2.py:186][0m |          -0.0091 |           4.2711 |           8.8832 |
[32m[20230113 20:14:42 @agent_ppo2.py:186][0m |          -0.0106 |           4.1324 |           8.8823 |
[32m[20230113 20:14:42 @agent_ppo2.py:186][0m |          -0.0147 |           4.0188 |           8.8643 |
[32m[20230113 20:14:42 @agent_ppo2.py:186][0m |          -0.0127 |           3.9264 |           8.8690 |
[32m[20230113 20:14:42 @agent_ppo2.py:186][0m |          -0.0118 |           3.8167 |           8.8724 |
[32m[20230113 20:14:42 @agent_ppo2.py:186][0m |          -0.0163 |           3.7303 |           8.8657 |
[32m[20230113 20:14:42 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.80
[32m[20230113 20:14:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.07
[32m[20230113 20:14:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 147.74
[32m[20230113 20:14:42 @agent_ppo2.py:144][0m Total time:      30.16 min
[32m[20230113 20:14:42 @agent_ppo2.py:146][0m 2779136 total steps have happened
[32m[20230113 20:14:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1357 --------------------------#
[32m[20230113 20:14:43 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:14:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:43 @agent_ppo2.py:186][0m |          -0.0015 |           5.9907 |           9.0471 |
[32m[20230113 20:14:43 @agent_ppo2.py:186][0m |          -0.0037 |           4.6540 |           9.0342 |
[32m[20230113 20:14:43 @agent_ppo2.py:186][0m |          -0.0123 |           4.0972 |           9.0297 |
[32m[20230113 20:14:43 @agent_ppo2.py:186][0m |          -0.0014 |           3.9595 |           9.0333 |
[32m[20230113 20:14:43 @agent_ppo2.py:186][0m |          -0.0089 |           3.7682 |           9.0289 |
[32m[20230113 20:14:43 @agent_ppo2.py:186][0m |          -0.0163 |           3.4773 |           9.0301 |
[32m[20230113 20:14:43 @agent_ppo2.py:186][0m |          -0.0145 |           3.3264 |           9.0276 |
[32m[20230113 20:14:43 @agent_ppo2.py:186][0m |          -0.0134 |           3.2820 |           9.0377 |
[32m[20230113 20:14:43 @agent_ppo2.py:186][0m |          -0.0184 |           3.1839 |           9.0415 |
[32m[20230113 20:14:43 @agent_ppo2.py:186][0m |          -0.0152 |           3.1082 |           9.0364 |
[32m[20230113 20:14:43 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:14:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.07
[32m[20230113 20:14:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.79
[32m[20230113 20:14:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.52
[32m[20230113 20:14:43 @agent_ppo2.py:144][0m Total time:      30.18 min
[32m[20230113 20:14:43 @agent_ppo2.py:146][0m 2781184 total steps have happened
[32m[20230113 20:14:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1358 --------------------------#
[32m[20230113 20:14:44 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:44 @agent_ppo2.py:186][0m |          -0.0027 |           4.5518 |           8.9510 |
[32m[20230113 20:14:44 @agent_ppo2.py:186][0m |          -0.0062 |           3.6696 |           8.9313 |
[32m[20230113 20:14:44 @agent_ppo2.py:186][0m |          -0.0099 |           3.2930 |           8.9336 |
[32m[20230113 20:14:44 @agent_ppo2.py:186][0m |          -0.0092 |           3.1513 |           8.9207 |
[32m[20230113 20:14:44 @agent_ppo2.py:186][0m |          -0.0146 |           2.9857 |           8.9279 |
[32m[20230113 20:14:44 @agent_ppo2.py:186][0m |          -0.0124 |           2.8938 |           8.9319 |
[32m[20230113 20:14:44 @agent_ppo2.py:186][0m |          -0.0150 |           2.7982 |           8.9235 |
[32m[20230113 20:14:44 @agent_ppo2.py:186][0m |          -0.0145 |           2.7555 |           8.9175 |
[32m[20230113 20:14:44 @agent_ppo2.py:186][0m |          -0.0164 |           2.7227 |           8.9172 |
[32m[20230113 20:14:44 @agent_ppo2.py:186][0m |          -0.0175 |           2.6388 |           8.9153 |
[32m[20230113 20:14:44 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.66
[32m[20230113 20:14:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.47
[32m[20230113 20:14:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.15
[32m[20230113 20:14:45 @agent_ppo2.py:144][0m Total time:      30.20 min
[32m[20230113 20:14:45 @agent_ppo2.py:146][0m 2783232 total steps have happened
[32m[20230113 20:14:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1359 --------------------------#
[32m[20230113 20:14:45 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:14:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:45 @agent_ppo2.py:186][0m |           0.0029 |           6.0024 |           9.1666 |
[32m[20230113 20:14:45 @agent_ppo2.py:186][0m |          -0.0055 |           4.8949 |           9.1620 |
[32m[20230113 20:14:45 @agent_ppo2.py:186][0m |          -0.0098 |           4.3393 |           9.1695 |
[32m[20230113 20:14:45 @agent_ppo2.py:186][0m |          -0.0075 |           4.1258 |           9.1679 |
[32m[20230113 20:14:45 @agent_ppo2.py:186][0m |          -0.0105 |           3.9034 |           9.1541 |
[32m[20230113 20:14:45 @agent_ppo2.py:186][0m |          -0.0102 |           3.6982 |           9.1612 |
[32m[20230113 20:14:45 @agent_ppo2.py:186][0m |          -0.0116 |           3.5785 |           9.1542 |
[32m[20230113 20:14:45 @agent_ppo2.py:186][0m |          -0.0123 |           3.4280 |           9.1555 |
[32m[20230113 20:14:46 @agent_ppo2.py:186][0m |          -0.0101 |           3.3833 |           9.1537 |
[32m[20230113 20:14:46 @agent_ppo2.py:186][0m |          -0.0203 |           3.2814 |           9.1508 |
[32m[20230113 20:14:46 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.84
[32m[20230113 20:14:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.05
[32m[20230113 20:14:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.07
[32m[20230113 20:14:46 @agent_ppo2.py:144][0m Total time:      30.22 min
[32m[20230113 20:14:46 @agent_ppo2.py:146][0m 2785280 total steps have happened
[32m[20230113 20:14:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1360 --------------------------#
[32m[20230113 20:14:46 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:47 @agent_ppo2.py:186][0m |           0.0010 |           6.0921 |           9.0313 |
[32m[20230113 20:14:47 @agent_ppo2.py:186][0m |          -0.0033 |           5.3739 |           9.0116 |
[32m[20230113 20:14:47 @agent_ppo2.py:186][0m |          -0.0057 |           4.8578 |           9.0103 |
[32m[20230113 20:14:47 @agent_ppo2.py:186][0m |          -0.0090 |           4.6944 |           9.0097 |
[32m[20230113 20:14:47 @agent_ppo2.py:186][0m |          -0.0095 |           4.5814 |           9.0053 |
[32m[20230113 20:14:47 @agent_ppo2.py:186][0m |          -0.0118 |           4.3656 |           9.0017 |
[32m[20230113 20:14:47 @agent_ppo2.py:186][0m |          -0.0121 |           4.2023 |           9.0047 |
[32m[20230113 20:14:47 @agent_ppo2.py:186][0m |          -0.0133 |           4.1140 |           9.0000 |
[32m[20230113 20:14:47 @agent_ppo2.py:186][0m |          -0.0141 |           4.0423 |           9.0021 |
[32m[20230113 20:14:47 @agent_ppo2.py:186][0m |          -0.0137 |           3.9144 |           8.9964 |
[32m[20230113 20:14:47 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.13
[32m[20230113 20:14:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.53
[32m[20230113 20:14:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.03
[32m[20230113 20:14:47 @agent_ppo2.py:144][0m Total time:      30.24 min
[32m[20230113 20:14:47 @agent_ppo2.py:146][0m 2787328 total steps have happened
[32m[20230113 20:14:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1361 --------------------------#
[32m[20230113 20:14:48 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:48 @agent_ppo2.py:186][0m |          -0.0003 |           5.5546 |           8.9712 |
[32m[20230113 20:14:48 @agent_ppo2.py:186][0m |          -0.0031 |           4.1924 |           8.9680 |
[32m[20230113 20:14:48 @agent_ppo2.py:186][0m |          -0.0089 |           3.6658 |           8.9531 |
[32m[20230113 20:14:48 @agent_ppo2.py:186][0m |          -0.0084 |           3.3857 |           8.9543 |
[32m[20230113 20:14:48 @agent_ppo2.py:186][0m |          -0.0102 |           3.2205 |           8.9617 |
[32m[20230113 20:14:48 @agent_ppo2.py:186][0m |          -0.0108 |           3.1018 |           8.9519 |
[32m[20230113 20:14:48 @agent_ppo2.py:186][0m |          -0.0130 |           2.9789 |           8.9560 |
[32m[20230113 20:14:48 @agent_ppo2.py:186][0m |          -0.0153 |           2.8722 |           8.9518 |
[32m[20230113 20:14:48 @agent_ppo2.py:186][0m |          -0.0164 |           2.7852 |           8.9545 |
[32m[20230113 20:14:48 @agent_ppo2.py:186][0m |          -0.0144 |           2.7052 |           8.9521 |
[32m[20230113 20:14:48 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:14:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.78
[32m[20230113 20:14:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.30
[32m[20230113 20:14:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.39
[32m[20230113 20:14:49 @agent_ppo2.py:144][0m Total time:      30.27 min
[32m[20230113 20:14:49 @agent_ppo2.py:146][0m 2789376 total steps have happened
[32m[20230113 20:14:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1362 --------------------------#
[32m[20230113 20:14:49 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:49 @agent_ppo2.py:186][0m |          -0.0020 |           5.4531 |           8.9386 |
[32m[20230113 20:14:49 @agent_ppo2.py:186][0m |          -0.0060 |           4.5735 |           8.9293 |
[32m[20230113 20:14:49 @agent_ppo2.py:186][0m |          -0.0107 |           4.1799 |           8.9293 |
[32m[20230113 20:14:49 @agent_ppo2.py:186][0m |          -0.0057 |           4.0515 |           8.9222 |
[32m[20230113 20:14:49 @agent_ppo2.py:186][0m |          -0.0099 |           3.7447 |           8.9223 |
[32m[20230113 20:14:49 @agent_ppo2.py:186][0m |          -0.0106 |           3.5583 |           8.9217 |
[32m[20230113 20:14:49 @agent_ppo2.py:186][0m |          -0.0131 |           3.4026 |           8.9185 |
[32m[20230113 20:14:49 @agent_ppo2.py:186][0m |          -0.0138 |           3.3415 |           8.9207 |
[32m[20230113 20:14:49 @agent_ppo2.py:186][0m |          -0.0169 |           3.2086 |           8.9155 |
[32m[20230113 20:14:49 @agent_ppo2.py:186][0m |          -0.0169 |           3.1660 |           8.9154 |
[32m[20230113 20:14:49 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.11
[32m[20230113 20:14:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.46
[32m[20230113 20:14:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.12
[32m[20230113 20:14:50 @agent_ppo2.py:144][0m Total time:      30.29 min
[32m[20230113 20:14:50 @agent_ppo2.py:146][0m 2791424 total steps have happened
[32m[20230113 20:14:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1363 --------------------------#
[32m[20230113 20:14:50 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:14:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:50 @agent_ppo2.py:186][0m |          -0.0098 |           5.8206 |           8.9249 |
[32m[20230113 20:14:50 @agent_ppo2.py:186][0m |          -0.0145 |           4.1498 |           8.9117 |
[32m[20230113 20:14:50 @agent_ppo2.py:186][0m |          -0.0165 |           3.7267 |           8.8984 |
[32m[20230113 20:14:51 @agent_ppo2.py:186][0m |          -0.0078 |           3.5245 |           8.9083 |
[32m[20230113 20:14:51 @agent_ppo2.py:186][0m |          -0.0139 |           3.4047 |           8.8806 |
[32m[20230113 20:14:51 @agent_ppo2.py:186][0m |          -0.0052 |           3.1681 |           8.9017 |
[32m[20230113 20:14:51 @agent_ppo2.py:186][0m |          -0.0054 |           3.0973 |           8.8936 |
[32m[20230113 20:14:51 @agent_ppo2.py:186][0m |          -0.0455 |           3.0535 |           8.8947 |
[32m[20230113 20:14:51 @agent_ppo2.py:186][0m |          -0.0201 |           2.9464 |           8.8888 |
[32m[20230113 20:14:51 @agent_ppo2.py:186][0m |          -0.0268 |           2.9156 |           8.8969 |
[32m[20230113 20:14:51 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.84
[32m[20230113 20:14:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.05
[32m[20230113 20:14:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.24
[32m[20230113 20:14:51 @agent_ppo2.py:144][0m Total time:      30.31 min
[32m[20230113 20:14:51 @agent_ppo2.py:146][0m 2793472 total steps have happened
[32m[20230113 20:14:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1364 --------------------------#
[32m[20230113 20:14:52 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:52 @agent_ppo2.py:186][0m |           0.0023 |           6.4078 |           9.2572 |
[32m[20230113 20:14:52 @agent_ppo2.py:186][0m |          -0.0018 |           4.6733 |           9.2546 |
[32m[20230113 20:14:52 @agent_ppo2.py:186][0m |          -0.0041 |           4.1006 |           9.2568 |
[32m[20230113 20:14:52 @agent_ppo2.py:186][0m |          -0.0068 |           3.8070 |           9.2526 |
[32m[20230113 20:14:52 @agent_ppo2.py:186][0m |          -0.0069 |           3.5926 |           9.2554 |
[32m[20230113 20:14:52 @agent_ppo2.py:186][0m |          -0.0076 |           3.3905 |           9.2573 |
[32m[20230113 20:14:52 @agent_ppo2.py:186][0m |          -0.0095 |           3.2555 |           9.2571 |
[32m[20230113 20:14:52 @agent_ppo2.py:186][0m |          -0.0098 |           3.1471 |           9.2575 |
[32m[20230113 20:14:52 @agent_ppo2.py:186][0m |          -0.0109 |           3.0481 |           9.2599 |
[32m[20230113 20:14:52 @agent_ppo2.py:186][0m |          -0.0099 |           2.9596 |           9.2519 |
[32m[20230113 20:14:52 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.04
[32m[20230113 20:14:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.15
[32m[20230113 20:14:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 143.50
[32m[20230113 20:14:52 @agent_ppo2.py:144][0m Total time:      30.33 min
[32m[20230113 20:14:52 @agent_ppo2.py:146][0m 2795520 total steps have happened
[32m[20230113 20:14:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1365 --------------------------#
[32m[20230113 20:14:53 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:14:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:53 @agent_ppo2.py:186][0m |           0.0015 |           6.2374 |           9.2430 |
[32m[20230113 20:14:53 @agent_ppo2.py:186][0m |          -0.0042 |           4.9340 |           9.2403 |
[32m[20230113 20:14:53 @agent_ppo2.py:186][0m |          -0.0075 |           4.4533 |           9.2413 |
[32m[20230113 20:14:53 @agent_ppo2.py:186][0m |          -0.0098 |           4.2210 |           9.2409 |
[32m[20230113 20:14:53 @agent_ppo2.py:186][0m |          -0.0112 |           4.0319 |           9.2409 |
[32m[20230113 20:14:53 @agent_ppo2.py:186][0m |          -0.0121 |           3.8635 |           9.2405 |
[32m[20230113 20:14:53 @agent_ppo2.py:186][0m |          -0.0137 |           3.7567 |           9.2357 |
[32m[20230113 20:14:53 @agent_ppo2.py:186][0m |          -0.0137 |           3.6319 |           9.2400 |
[32m[20230113 20:14:53 @agent_ppo2.py:186][0m |          -0.0153 |           3.5364 |           9.2411 |
[32m[20230113 20:14:53 @agent_ppo2.py:186][0m |          -0.0164 |           3.4323 |           9.2408 |
[32m[20230113 20:14:53 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.25
[32m[20230113 20:14:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.45
[32m[20230113 20:14:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.73
[32m[20230113 20:14:54 @agent_ppo2.py:144][0m Total time:      30.35 min
[32m[20230113 20:14:54 @agent_ppo2.py:146][0m 2797568 total steps have happened
[32m[20230113 20:14:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1366 --------------------------#
[32m[20230113 20:14:54 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:14:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:54 @agent_ppo2.py:186][0m |           0.0044 |           6.0787 |           9.2501 |
[32m[20230113 20:14:54 @agent_ppo2.py:186][0m |          -0.0050 |           4.7906 |           9.2324 |
[32m[20230113 20:14:54 @agent_ppo2.py:186][0m |          -0.0065 |           4.2960 |           9.2362 |
[32m[20230113 20:14:54 @agent_ppo2.py:186][0m |          -0.0096 |           4.0162 |           9.2387 |
[32m[20230113 20:14:54 @agent_ppo2.py:186][0m |          -0.0125 |           3.8249 |           9.2379 |
[32m[20230113 20:14:55 @agent_ppo2.py:186][0m |          -0.0136 |           3.6841 |           9.2351 |
[32m[20230113 20:14:55 @agent_ppo2.py:186][0m |          -0.0058 |           3.5959 |           9.2363 |
[32m[20230113 20:14:55 @agent_ppo2.py:186][0m |          -0.0224 |           3.4569 |           9.2251 |
[32m[20230113 20:14:55 @agent_ppo2.py:186][0m |          -0.0210 |           3.5129 |           9.2271 |
[32m[20230113 20:14:55 @agent_ppo2.py:186][0m |          -0.0179 |           3.3996 |           9.2349 |
[32m[20230113 20:14:55 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.98
[32m[20230113 20:14:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.83
[32m[20230113 20:14:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.99
[32m[20230113 20:14:55 @agent_ppo2.py:144][0m Total time:      30.38 min
[32m[20230113 20:14:55 @agent_ppo2.py:146][0m 2799616 total steps have happened
[32m[20230113 20:14:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1367 --------------------------#
[32m[20230113 20:14:56 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:14:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:56 @agent_ppo2.py:186][0m |           0.0019 |           5.9036 |           9.0164 |
[32m[20230113 20:14:56 @agent_ppo2.py:186][0m |           0.0099 |           4.8581 |           9.0082 |
[32m[20230113 20:14:56 @agent_ppo2.py:186][0m |          -0.0053 |           4.3652 |           9.0072 |
[32m[20230113 20:14:56 @agent_ppo2.py:186][0m |          -0.0113 |           4.0247 |           9.0061 |
[32m[20230113 20:14:56 @agent_ppo2.py:186][0m |          -0.0133 |           3.7943 |           9.0016 |
[32m[20230113 20:14:56 @agent_ppo2.py:186][0m |          -0.0131 |           3.7420 |           9.0047 |
[32m[20230113 20:14:56 @agent_ppo2.py:186][0m |           0.0011 |           3.6139 |           8.9941 |
[32m[20230113 20:14:56 @agent_ppo2.py:186][0m |          -0.0238 |           3.4668 |           8.9970 |
[32m[20230113 20:14:56 @agent_ppo2.py:186][0m |          -0.0168 |           3.3006 |           8.9997 |
[32m[20230113 20:14:56 @agent_ppo2.py:186][0m |          -0.0188 |           3.2307 |           8.9896 |
[32m[20230113 20:14:56 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:14:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.72
[32m[20230113 20:14:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.54
[32m[20230113 20:14:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.53
[32m[20230113 20:14:56 @agent_ppo2.py:144][0m Total time:      30.40 min
[32m[20230113 20:14:56 @agent_ppo2.py:146][0m 2801664 total steps have happened
[32m[20230113 20:14:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1368 --------------------------#
[32m[20230113 20:14:57 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:14:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:57 @agent_ppo2.py:186][0m |           0.0048 |           6.4690 |           9.2809 |
[32m[20230113 20:14:57 @agent_ppo2.py:186][0m |          -0.0033 |           4.6898 |           9.2715 |
[32m[20230113 20:14:57 @agent_ppo2.py:186][0m |          -0.0085 |           4.1119 |           9.2837 |
[32m[20230113 20:14:57 @agent_ppo2.py:186][0m |          -0.0084 |           3.8576 |           9.2666 |
[32m[20230113 20:14:57 @agent_ppo2.py:186][0m |          -0.0075 |           3.7565 |           9.2624 |
[32m[20230113 20:14:57 @agent_ppo2.py:186][0m |          -0.0106 |           3.4989 |           9.2665 |
[32m[20230113 20:14:57 @agent_ppo2.py:186][0m |          -0.0103 |           3.3352 |           9.2574 |
[32m[20230113 20:14:57 @agent_ppo2.py:186][0m |          -0.0142 |           3.2510 |           9.2598 |
[32m[20230113 20:14:57 @agent_ppo2.py:186][0m |          -0.0135 |           3.1701 |           9.2554 |
[32m[20230113 20:14:57 @agent_ppo2.py:186][0m |          -0.0109 |           3.1124 |           9.2586 |
[32m[20230113 20:14:57 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.12
[32m[20230113 20:14:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.80
[32m[20230113 20:14:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.08
[32m[20230113 20:14:58 @agent_ppo2.py:144][0m Total time:      30.42 min
[32m[20230113 20:14:58 @agent_ppo2.py:146][0m 2803712 total steps have happened
[32m[20230113 20:14:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1369 --------------------------#
[32m[20230113 20:14:58 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:14:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:58 @agent_ppo2.py:186][0m |          -0.0010 |           5.6760 |           9.0504 |
[32m[20230113 20:14:58 @agent_ppo2.py:186][0m |          -0.0071 |           4.2205 |           9.0520 |
[32m[20230113 20:14:58 @agent_ppo2.py:186][0m |          -0.0046 |           3.8858 |           9.0255 |
[32m[20230113 20:14:58 @agent_ppo2.py:186][0m |          -0.0050 |           3.5987 |           9.0460 |
[32m[20230113 20:14:58 @agent_ppo2.py:186][0m |          -0.0078 |           3.3714 |           9.0532 |
[32m[20230113 20:14:58 @agent_ppo2.py:186][0m |          -0.0067 |           3.2540 |           9.0537 |
[32m[20230113 20:14:58 @agent_ppo2.py:186][0m |          -0.0091 |           3.2097 |           9.0516 |
[32m[20230113 20:14:58 @agent_ppo2.py:186][0m |          -0.0062 |           3.1464 |           9.0564 |
[32m[20230113 20:14:59 @agent_ppo2.py:186][0m |          -0.0119 |           2.9887 |           9.0511 |
[32m[20230113 20:14:59 @agent_ppo2.py:186][0m |          -0.0027 |           3.0471 |           9.0526 |
[32m[20230113 20:14:59 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:14:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.32
[32m[20230113 20:14:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.48
[32m[20230113 20:14:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.86
[32m[20230113 20:14:59 @agent_ppo2.py:144][0m Total time:      30.44 min
[32m[20230113 20:14:59 @agent_ppo2.py:146][0m 2805760 total steps have happened
[32m[20230113 20:14:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1370 --------------------------#
[32m[20230113 20:14:59 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:14:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:14:59 @agent_ppo2.py:186][0m |          -0.0016 |           5.5106 |           9.2040 |
[32m[20230113 20:15:00 @agent_ppo2.py:186][0m |          -0.0081 |           4.4695 |           9.1886 |
[32m[20230113 20:15:00 @agent_ppo2.py:186][0m |          -0.0107 |           4.0741 |           9.1890 |
[32m[20230113 20:15:00 @agent_ppo2.py:186][0m |          -0.0091 |           3.8505 |           9.1855 |
[32m[20230113 20:15:00 @agent_ppo2.py:186][0m |          -0.0084 |           3.6881 |           9.1850 |
[32m[20230113 20:15:00 @agent_ppo2.py:186][0m |          -0.0099 |           3.6088 |           9.1841 |
[32m[20230113 20:15:00 @agent_ppo2.py:186][0m |          -0.0126 |           3.4652 |           9.1796 |
[32m[20230113 20:15:00 @agent_ppo2.py:186][0m |          -0.0118 |           3.3252 |           9.1879 |
[32m[20230113 20:15:00 @agent_ppo2.py:186][0m |          -0.0185 |           3.2804 |           9.1821 |
[32m[20230113 20:15:00 @agent_ppo2.py:186][0m |          -0.0146 |           3.2332 |           9.1810 |
[32m[20230113 20:15:00 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.66
[32m[20230113 20:15:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.65
[32m[20230113 20:15:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.22
[32m[20230113 20:15:00 @agent_ppo2.py:144][0m Total time:      30.46 min
[32m[20230113 20:15:00 @agent_ppo2.py:146][0m 2807808 total steps have happened
[32m[20230113 20:15:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1371 --------------------------#
[32m[20230113 20:15:01 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:15:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:01 @agent_ppo2.py:186][0m |           0.0050 |           6.7429 |           9.2828 |
[32m[20230113 20:15:01 @agent_ppo2.py:186][0m |          -0.0054 |           5.0216 |           9.2816 |
[32m[20230113 20:15:01 @agent_ppo2.py:186][0m |          -0.0029 |           4.6424 |           9.2647 |
[32m[20230113 20:15:01 @agent_ppo2.py:186][0m |          -0.0054 |           4.2154 |           9.2784 |
[32m[20230113 20:15:01 @agent_ppo2.py:186][0m |          -0.0085 |           3.9808 |           9.2775 |
[32m[20230113 20:15:01 @agent_ppo2.py:186][0m |          -0.0087 |           3.9006 |           9.2752 |
[32m[20230113 20:15:01 @agent_ppo2.py:186][0m |          -0.0119 |           3.7030 |           9.2829 |
[32m[20230113 20:15:01 @agent_ppo2.py:186][0m |          -0.0114 |           3.5743 |           9.2828 |
[32m[20230113 20:15:01 @agent_ppo2.py:186][0m |          -0.0130 |           3.4937 |           9.2869 |
[32m[20230113 20:15:01 @agent_ppo2.py:186][0m |          -0.0070 |           3.3707 |           9.2785 |
[32m[20230113 20:15:01 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:15:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.43
[32m[20230113 20:15:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.43
[32m[20230113 20:15:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.71
[32m[20230113 20:15:02 @agent_ppo2.py:144][0m Total time:      30.48 min
[32m[20230113 20:15:02 @agent_ppo2.py:146][0m 2809856 total steps have happened
[32m[20230113 20:15:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1372 --------------------------#
[32m[20230113 20:15:02 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:15:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:02 @agent_ppo2.py:186][0m |           0.0014 |           6.3457 |           9.2124 |
[32m[20230113 20:15:02 @agent_ppo2.py:186][0m |          -0.0048 |           4.7886 |           9.1913 |
[32m[20230113 20:15:02 @agent_ppo2.py:186][0m |          -0.0077 |           4.2267 |           9.1914 |
[32m[20230113 20:15:02 @agent_ppo2.py:186][0m |          -0.0070 |           3.8752 |           9.1905 |
[32m[20230113 20:15:02 @agent_ppo2.py:186][0m |          -0.0087 |           3.6484 |           9.1846 |
[32m[20230113 20:15:02 @agent_ppo2.py:186][0m |          -0.0096 |           3.4534 |           9.1834 |
[32m[20230113 20:15:02 @agent_ppo2.py:186][0m |          -0.0095 |           3.3196 |           9.1840 |
[32m[20230113 20:15:02 @agent_ppo2.py:186][0m |          -0.0118 |           3.1919 |           9.1918 |
[32m[20230113 20:15:02 @agent_ppo2.py:186][0m |          -0.0114 |           3.1291 |           9.1878 |
[32m[20230113 20:15:02 @agent_ppo2.py:186][0m |          -0.0122 |           3.0352 |           9.1714 |
[32m[20230113 20:15:02 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.13
[32m[20230113 20:15:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.62
[32m[20230113 20:15:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.88
[32m[20230113 20:15:03 @agent_ppo2.py:144][0m Total time:      30.50 min
[32m[20230113 20:15:03 @agent_ppo2.py:146][0m 2811904 total steps have happened
[32m[20230113 20:15:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1373 --------------------------#
[32m[20230113 20:15:03 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:15:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:03 @agent_ppo2.py:186][0m |          -0.0004 |           5.5898 |           9.2959 |
[32m[20230113 20:15:03 @agent_ppo2.py:186][0m |          -0.0065 |           4.2086 |           9.2914 |
[32m[20230113 20:15:03 @agent_ppo2.py:186][0m |          -0.0104 |           3.7489 |           9.2833 |
[32m[20230113 20:15:03 @agent_ppo2.py:186][0m |          -0.0126 |           3.4254 |           9.2856 |
[32m[20230113 20:15:04 @agent_ppo2.py:186][0m |          -0.0141 |           3.2184 |           9.2792 |
[32m[20230113 20:15:04 @agent_ppo2.py:186][0m |          -0.0146 |           3.0994 |           9.2755 |
[32m[20230113 20:15:04 @agent_ppo2.py:186][0m |          -0.0158 |           2.9959 |           9.2759 |
[32m[20230113 20:15:04 @agent_ppo2.py:186][0m |          -0.0162 |           2.8616 |           9.2782 |
[32m[20230113 20:15:04 @agent_ppo2.py:186][0m |          -0.0168 |           2.7896 |           9.2787 |
[32m[20230113 20:15:04 @agent_ppo2.py:186][0m |          -0.0169 |           2.7062 |           9.2764 |
[32m[20230113 20:15:04 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:15:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.31
[32m[20230113 20:15:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.37
[32m[20230113 20:15:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.69
[32m[20230113 20:15:04 @agent_ppo2.py:144][0m Total time:      30.53 min
[32m[20230113 20:15:04 @agent_ppo2.py:146][0m 2813952 total steps have happened
[32m[20230113 20:15:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1374 --------------------------#
[32m[20230113 20:15:05 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:05 @agent_ppo2.py:186][0m |           0.0086 |           5.7417 |           9.3355 |
[32m[20230113 20:15:05 @agent_ppo2.py:186][0m |          -0.0037 |           4.7447 |           9.3263 |
[32m[20230113 20:15:05 @agent_ppo2.py:186][0m |          -0.0063 |           4.2973 |           9.3244 |
[32m[20230113 20:15:05 @agent_ppo2.py:186][0m |          -0.0109 |           4.0607 |           9.3254 |
[32m[20230113 20:15:05 @agent_ppo2.py:186][0m |          -0.0135 |           3.9170 |           9.3278 |
[32m[20230113 20:15:05 @agent_ppo2.py:186][0m |          -0.0102 |           3.7957 |           9.3251 |
[32m[20230113 20:15:05 @agent_ppo2.py:186][0m |          -0.0168 |           3.6823 |           9.3330 |
[32m[20230113 20:15:05 @agent_ppo2.py:186][0m |          -0.0157 |           3.6128 |           9.3291 |
[32m[20230113 20:15:05 @agent_ppo2.py:186][0m |          -0.0120 |           3.5720 |           9.3288 |
[32m[20230113 20:15:05 @agent_ppo2.py:186][0m |          -0.0168 |           3.4864 |           9.3338 |
[32m[20230113 20:15:05 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.41
[32m[20230113 20:15:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.02
[32m[20230113 20:15:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.45
[32m[20230113 20:15:05 @agent_ppo2.py:144][0m Total time:      30.55 min
[32m[20230113 20:15:05 @agent_ppo2.py:146][0m 2816000 total steps have happened
[32m[20230113 20:15:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1375 --------------------------#
[32m[20230113 20:15:06 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:15:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:06 @agent_ppo2.py:186][0m |           0.0019 |           5.9241 |           9.2294 |
[32m[20230113 20:15:06 @agent_ppo2.py:186][0m |          -0.0199 |           5.0128 |           9.2192 |
[32m[20230113 20:15:06 @agent_ppo2.py:186][0m |          -0.0130 |           4.5789 |           9.2211 |
[32m[20230113 20:15:06 @agent_ppo2.py:186][0m |           0.0113 |           4.3286 |           9.2149 |
[32m[20230113 20:15:06 @agent_ppo2.py:186][0m |          -0.0225 |           4.2176 |           9.2185 |
[32m[20230113 20:15:06 @agent_ppo2.py:186][0m |          -0.0200 |           3.9293 |           9.2188 |
[32m[20230113 20:15:06 @agent_ppo2.py:186][0m |          -0.0037 |           3.7747 |           9.2171 |
[32m[20230113 20:15:06 @agent_ppo2.py:186][0m |          -0.0006 |           3.7774 |           9.2177 |
[32m[20230113 20:15:06 @agent_ppo2.py:186][0m |          -0.0064 |           3.7212 |           9.2136 |
[32m[20230113 20:15:06 @agent_ppo2.py:186][0m |          -0.0226 |           3.6742 |           9.2218 |
[32m[20230113 20:15:06 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:15:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.86
[32m[20230113 20:15:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.45
[32m[20230113 20:15:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.32
[32m[20230113 20:15:07 @agent_ppo2.py:144][0m Total time:      30.57 min
[32m[20230113 20:15:07 @agent_ppo2.py:146][0m 2818048 total steps have happened
[32m[20230113 20:15:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1376 --------------------------#
[32m[20230113 20:15:07 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:07 @agent_ppo2.py:186][0m |           0.0036 |           6.7353 |           9.0359 |
[32m[20230113 20:15:07 @agent_ppo2.py:186][0m |          -0.0069 |           5.2695 |           9.0449 |
[32m[20230113 20:15:07 @agent_ppo2.py:186][0m |           0.0013 |           4.9131 |           9.0375 |
[32m[20230113 20:15:07 @agent_ppo2.py:186][0m |          -0.0085 |           4.4433 |           9.0349 |
[32m[20230113 20:15:07 @agent_ppo2.py:186][0m |          -0.0033 |           4.1102 |           9.0305 |
[32m[20230113 20:15:07 @agent_ppo2.py:186][0m |          -0.0038 |           4.0822 |           9.0268 |
[32m[20230113 20:15:07 @agent_ppo2.py:186][0m |          -0.0068 |           3.8020 |           9.0288 |
[32m[20230113 20:15:08 @agent_ppo2.py:186][0m |          -0.0135 |           3.6710 |           9.0309 |
[32m[20230113 20:15:08 @agent_ppo2.py:186][0m |          -0.0114 |           3.5805 |           9.0382 |
[32m[20230113 20:15:08 @agent_ppo2.py:186][0m |          -0.0087 |           3.5703 |           9.0397 |
[32m[20230113 20:15:08 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.89
[32m[20230113 20:15:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.89
[32m[20230113 20:15:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.97
[32m[20230113 20:15:08 @agent_ppo2.py:144][0m Total time:      30.59 min
[32m[20230113 20:15:08 @agent_ppo2.py:146][0m 2820096 total steps have happened
[32m[20230113 20:15:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1377 --------------------------#
[32m[20230113 20:15:08 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:09 @agent_ppo2.py:186][0m |           0.0073 |           6.8199 |           9.1718 |
[32m[20230113 20:15:09 @agent_ppo2.py:186][0m |          -0.0023 |           5.3027 |           9.1633 |
[32m[20230113 20:15:09 @agent_ppo2.py:186][0m |          -0.0122 |           4.6205 |           9.1597 |
[32m[20230113 20:15:09 @agent_ppo2.py:186][0m |          -0.0208 |           4.2641 |           9.1549 |
[32m[20230113 20:15:09 @agent_ppo2.py:186][0m |          -0.0021 |           3.9762 |           9.1544 |
[32m[20230113 20:15:09 @agent_ppo2.py:186][0m |           0.0012 |           3.7856 |           9.1288 |
[32m[20230113 20:15:09 @agent_ppo2.py:186][0m |          -0.0068 |           3.6647 |           9.1489 |
[32m[20230113 20:15:09 @agent_ppo2.py:186][0m |          -0.0015 |           3.5262 |           9.1506 |
[32m[20230113 20:15:09 @agent_ppo2.py:186][0m |          -0.0176 |           3.3820 |           9.1419 |
[32m[20230113 20:15:09 @agent_ppo2.py:186][0m |           0.0159 |           3.3647 |           9.1443 |
[32m[20230113 20:15:09 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.91
[32m[20230113 20:15:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.49
[32m[20230113 20:15:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.88
[32m[20230113 20:15:09 @agent_ppo2.py:144][0m Total time:      30.61 min
[32m[20230113 20:15:09 @agent_ppo2.py:146][0m 2822144 total steps have happened
[32m[20230113 20:15:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1378 --------------------------#
[32m[20230113 20:15:10 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:10 @agent_ppo2.py:186][0m |           0.0005 |           7.0608 |           9.3613 |
[32m[20230113 20:15:10 @agent_ppo2.py:186][0m |          -0.0009 |           5.0256 |           9.3606 |
[32m[20230113 20:15:10 @agent_ppo2.py:186][0m |          -0.0047 |           4.4067 |           9.3479 |
[32m[20230113 20:15:10 @agent_ppo2.py:186][0m |          -0.0073 |           4.1455 |           9.3480 |
[32m[20230113 20:15:10 @agent_ppo2.py:186][0m |          -0.0058 |           3.9047 |           9.3483 |
[32m[20230113 20:15:10 @agent_ppo2.py:186][0m |          -0.0078 |           3.6703 |           9.3412 |
[32m[20230113 20:15:10 @agent_ppo2.py:186][0m |          -0.0091 |           3.5093 |           9.3375 |
[32m[20230113 20:15:10 @agent_ppo2.py:186][0m |          -0.0097 |           3.4417 |           9.3455 |
[32m[20230113 20:15:10 @agent_ppo2.py:186][0m |          -0.0089 |           3.2716 |           9.3434 |
[32m[20230113 20:15:10 @agent_ppo2.py:186][0m |          -0.0083 |           3.2238 |           9.3362 |
[32m[20230113 20:15:10 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:15:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.61
[32m[20230113 20:15:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.44
[32m[20230113 20:15:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.89
[32m[20230113 20:15:11 @agent_ppo2.py:144][0m Total time:      30.63 min
[32m[20230113 20:15:11 @agent_ppo2.py:146][0m 2824192 total steps have happened
[32m[20230113 20:15:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1379 --------------------------#
[32m[20230113 20:15:11 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:11 @agent_ppo2.py:186][0m |          -0.0021 |           6.7580 |           9.2426 |
[32m[20230113 20:15:11 @agent_ppo2.py:186][0m |          -0.0037 |           5.3618 |           9.2308 |
[32m[20230113 20:15:11 @agent_ppo2.py:186][0m |          -0.0099 |           4.8448 |           9.2210 |
[32m[20230113 20:15:11 @agent_ppo2.py:186][0m |          -0.0131 |           4.5035 |           9.2127 |
[32m[20230113 20:15:11 @agent_ppo2.py:186][0m |          -0.0123 |           4.2494 |           9.2095 |
[32m[20230113 20:15:11 @agent_ppo2.py:186][0m |          -0.0107 |           4.0704 |           9.2090 |
[32m[20230113 20:15:11 @agent_ppo2.py:186][0m |          -0.0143 |           3.9060 |           9.2155 |
[32m[20230113 20:15:11 @agent_ppo2.py:186][0m |          -0.0140 |           3.7412 |           9.2145 |
[32m[20230113 20:15:11 @agent_ppo2.py:186][0m |          -0.0134 |           3.6757 |           9.2016 |
[32m[20230113 20:15:11 @agent_ppo2.py:186][0m |          -0.0166 |           3.5399 |           9.1980 |
[32m[20230113 20:15:11 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.82
[32m[20230113 20:15:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.46
[32m[20230113 20:15:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.83
[32m[20230113 20:15:12 @agent_ppo2.py:144][0m Total time:      30.65 min
[32m[20230113 20:15:12 @agent_ppo2.py:146][0m 2826240 total steps have happened
[32m[20230113 20:15:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1380 --------------------------#
[32m[20230113 20:15:12 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:12 @agent_ppo2.py:186][0m |           0.0019 |           6.4181 |           9.4338 |
[32m[20230113 20:15:12 @agent_ppo2.py:186][0m |          -0.0020 |           4.8714 |           9.4145 |
[32m[20230113 20:15:12 @agent_ppo2.py:186][0m |          -0.0021 |           4.4294 |           9.4277 |
[32m[20230113 20:15:12 @agent_ppo2.py:186][0m |          -0.0063 |           4.0880 |           9.4190 |
[32m[20230113 20:15:12 @agent_ppo2.py:186][0m |          -0.0111 |           3.8763 |           9.4021 |
[32m[20230113 20:15:13 @agent_ppo2.py:186][0m |          -0.0089 |           3.7618 |           9.4042 |
[32m[20230113 20:15:13 @agent_ppo2.py:186][0m |          -0.0087 |           3.6214 |           9.4080 |
[32m[20230113 20:15:13 @agent_ppo2.py:186][0m |          -0.0102 |           3.5404 |           9.4044 |
[32m[20230113 20:15:13 @agent_ppo2.py:186][0m |          -0.0125 |           3.4699 |           9.4027 |
[32m[20230113 20:15:13 @agent_ppo2.py:186][0m |          -0.0117 |           3.3699 |           9.4094 |
[32m[20230113 20:15:13 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.25
[32m[20230113 20:15:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.70
[32m[20230113 20:15:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 157.62
[32m[20230113 20:15:13 @agent_ppo2.py:144][0m Total time:      30.68 min
[32m[20230113 20:15:13 @agent_ppo2.py:146][0m 2828288 total steps have happened
[32m[20230113 20:15:13 @agent_ppo2.py:122][0m #------------------------ Iteration 1381 --------------------------#
[32m[20230113 20:15:14 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:14 @agent_ppo2.py:186][0m |           0.0011 |           7.0894 |           9.1987 |
[32m[20230113 20:15:14 @agent_ppo2.py:186][0m |          -0.0056 |           5.1496 |           9.1818 |
[32m[20230113 20:15:14 @agent_ppo2.py:186][0m |          -0.0070 |           4.5870 |           9.1954 |
[32m[20230113 20:15:14 @agent_ppo2.py:186][0m |          -0.0099 |           4.2078 |           9.1806 |
[32m[20230113 20:15:14 @agent_ppo2.py:186][0m |          -0.0105 |           3.9520 |           9.1784 |
[32m[20230113 20:15:14 @agent_ppo2.py:186][0m |          -0.0127 |           3.8389 |           9.1865 |
[32m[20230113 20:15:14 @agent_ppo2.py:186][0m |          -0.0140 |           3.6836 |           9.1850 |
[32m[20230113 20:15:14 @agent_ppo2.py:186][0m |          -0.0123 |           3.7227 |           9.1892 |
[32m[20230113 20:15:14 @agent_ppo2.py:186][0m |          -0.0130 |           3.4153 |           9.1791 |
[32m[20230113 20:15:14 @agent_ppo2.py:186][0m |          -0.0147 |           3.4286 |           9.1770 |
[32m[20230113 20:15:14 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:15:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.14
[32m[20230113 20:15:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.47
[32m[20230113 20:15:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 154.61
[32m[20230113 20:15:15 @agent_ppo2.py:144][0m Total time:      30.70 min
[32m[20230113 20:15:15 @agent_ppo2.py:146][0m 2830336 total steps have happened
[32m[20230113 20:15:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1382 --------------------------#
[32m[20230113 20:15:15 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:15:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:15 @agent_ppo2.py:186][0m |           0.0018 |           5.4361 |           9.4511 |
[32m[20230113 20:15:15 @agent_ppo2.py:186][0m |          -0.0029 |           4.0755 |           9.4470 |
[32m[20230113 20:15:15 @agent_ppo2.py:186][0m |          -0.0043 |           3.5857 |           9.4383 |
[32m[20230113 20:15:15 @agent_ppo2.py:186][0m |          -0.0066 |           3.3659 |           9.4402 |
[32m[20230113 20:15:15 @agent_ppo2.py:186][0m |          -0.0070 |           3.1938 |           9.4230 |
[32m[20230113 20:15:15 @agent_ppo2.py:186][0m |          -0.0082 |           3.0493 |           9.4349 |
[32m[20230113 20:15:15 @agent_ppo2.py:186][0m |          -0.0084 |           2.9597 |           9.4272 |
[32m[20230113 20:15:15 @agent_ppo2.py:186][0m |          -0.0099 |           2.8624 |           9.4240 |
[32m[20230113 20:15:15 @agent_ppo2.py:186][0m |          -0.0097 |           2.8012 |           9.4254 |
[32m[20230113 20:15:15 @agent_ppo2.py:186][0m |          -0.0109 |           2.7272 |           9.4299 |
[32m[20230113 20:15:15 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.93
[32m[20230113 20:15:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.80
[32m[20230113 20:15:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.52
[32m[20230113 20:15:16 @agent_ppo2.py:144][0m Total time:      30.72 min
[32m[20230113 20:15:16 @agent_ppo2.py:146][0m 2832384 total steps have happened
[32m[20230113 20:15:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1383 --------------------------#
[32m[20230113 20:15:16 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:15:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:16 @agent_ppo2.py:186][0m |           0.0040 |           5.5024 |           9.4009 |
[32m[20230113 20:15:16 @agent_ppo2.py:186][0m |          -0.0108 |           3.8248 |           9.3973 |
[32m[20230113 20:15:16 @agent_ppo2.py:186][0m |          -0.0096 |           3.4234 |           9.3964 |
[32m[20230113 20:15:16 @agent_ppo2.py:186][0m |          -0.0101 |           3.2003 |           9.3999 |
[32m[20230113 20:15:16 @agent_ppo2.py:186][0m |          -0.0151 |           3.0328 |           9.3919 |
[32m[20230113 20:15:16 @agent_ppo2.py:186][0m |          -0.0127 |           3.0502 |           9.3903 |
[32m[20230113 20:15:17 @agent_ppo2.py:186][0m |          -0.0145 |           2.8745 |           9.3792 |
[32m[20230113 20:15:17 @agent_ppo2.py:186][0m |          -0.0138 |           2.7739 |           9.3962 |
[32m[20230113 20:15:17 @agent_ppo2.py:186][0m |          -0.0163 |           2.7029 |           9.3888 |
[32m[20230113 20:15:17 @agent_ppo2.py:186][0m |          -0.0173 |           2.6426 |           9.3925 |
[32m[20230113 20:15:17 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:15:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.32
[32m[20230113 20:15:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.48
[32m[20230113 20:15:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.24
[32m[20230113 20:15:17 @agent_ppo2.py:144][0m Total time:      30.74 min
[32m[20230113 20:15:17 @agent_ppo2.py:146][0m 2834432 total steps have happened
[32m[20230113 20:15:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1384 --------------------------#
[32m[20230113 20:15:17 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:18 @agent_ppo2.py:186][0m |           0.0020 |           6.8491 |           9.2929 |
[32m[20230113 20:15:18 @agent_ppo2.py:186][0m |          -0.0041 |           5.6677 |           9.2848 |
[32m[20230113 20:15:18 @agent_ppo2.py:186][0m |          -0.0052 |           5.0156 |           9.2755 |
[32m[20230113 20:15:18 @agent_ppo2.py:186][0m |          -0.0074 |           4.7752 |           9.2763 |
[32m[20230113 20:15:18 @agent_ppo2.py:186][0m |          -0.0123 |           4.4013 |           9.2728 |
[32m[20230113 20:15:18 @agent_ppo2.py:186][0m |          -0.0141 |           4.0704 |           9.2744 |
[32m[20230113 20:15:18 @agent_ppo2.py:186][0m |          -0.0137 |           3.8732 |           9.2778 |
[32m[20230113 20:15:18 @agent_ppo2.py:186][0m |          -0.0149 |           3.7072 |           9.2821 |
[32m[20230113 20:15:18 @agent_ppo2.py:186][0m |          -0.0161 |           3.5815 |           9.2826 |
[32m[20230113 20:15:18 @agent_ppo2.py:186][0m |          -0.0162 |           3.4910 |           9.2794 |
[32m[20230113 20:15:18 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:15:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.38
[32m[20230113 20:15:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.89
[32m[20230113 20:15:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 176.93
[32m[20230113 20:15:18 @agent_ppo2.py:144][0m Total time:      30.76 min
[32m[20230113 20:15:18 @agent_ppo2.py:146][0m 2836480 total steps have happened
[32m[20230113 20:15:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1385 --------------------------#
[32m[20230113 20:15:19 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:19 @agent_ppo2.py:186][0m |          -0.0024 |           5.7919 |           9.3710 |
[32m[20230113 20:15:19 @agent_ppo2.py:186][0m |          -0.0040 |           3.8862 |           9.3573 |
[32m[20230113 20:15:19 @agent_ppo2.py:186][0m |          -0.0151 |           3.5064 |           9.3509 |
[32m[20230113 20:15:19 @agent_ppo2.py:186][0m |          -0.0104 |           3.2078 |           9.3444 |
[32m[20230113 20:15:19 @agent_ppo2.py:186][0m |          -0.0082 |           3.0261 |           9.3335 |
[32m[20230113 20:15:19 @agent_ppo2.py:186][0m |          -0.0207 |           2.9056 |           9.3362 |
[32m[20230113 20:15:19 @agent_ppo2.py:186][0m |          -0.0151 |           2.7754 |           9.3334 |
[32m[20230113 20:15:19 @agent_ppo2.py:186][0m |          -0.0103 |           2.7294 |           9.3332 |
[32m[20230113 20:15:19 @agent_ppo2.py:186][0m |          -0.0201 |           2.6671 |           9.3247 |
[32m[20230113 20:15:19 @agent_ppo2.py:186][0m |          -0.0089 |           2.5858 |           9.3346 |
[32m[20230113 20:15:19 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.04
[32m[20230113 20:15:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.79
[32m[20230113 20:15:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.92
[32m[20230113 20:15:20 @agent_ppo2.py:144][0m Total time:      30.78 min
[32m[20230113 20:15:20 @agent_ppo2.py:146][0m 2838528 total steps have happened
[32m[20230113 20:15:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1386 --------------------------#
[32m[20230113 20:15:20 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:15:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:20 @agent_ppo2.py:186][0m |           0.0011 |           6.8677 |           8.9450 |
[32m[20230113 20:15:20 @agent_ppo2.py:186][0m |           0.0057 |           5.2190 |           8.9559 |
[32m[20230113 20:15:20 @agent_ppo2.py:186][0m |          -0.0061 |           4.7411 |           8.9538 |
[32m[20230113 20:15:20 @agent_ppo2.py:186][0m |          -0.0092 |           4.3821 |           8.9470 |
[32m[20230113 20:15:20 @agent_ppo2.py:186][0m |           0.0020 |           4.2813 |           8.9501 |
[32m[20230113 20:15:20 @agent_ppo2.py:186][0m |          -0.0076 |           4.0350 |           8.9425 |
[32m[20230113 20:15:20 @agent_ppo2.py:186][0m |          -0.0072 |           3.8900 |           8.9366 |
[32m[20230113 20:15:20 @agent_ppo2.py:186][0m |          -0.0094 |           3.8015 |           8.9321 |
[32m[20230113 20:15:20 @agent_ppo2.py:186][0m |          -0.0148 |           3.6321 |           8.9344 |
[32m[20230113 20:15:20 @agent_ppo2.py:186][0m |          -0.0274 |           3.8039 |           8.9410 |
[32m[20230113 20:15:20 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:15:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.64
[32m[20230113 20:15:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.92
[32m[20230113 20:15:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.48
[32m[20230113 20:15:21 @agent_ppo2.py:144][0m Total time:      30.81 min
[32m[20230113 20:15:21 @agent_ppo2.py:146][0m 2840576 total steps have happened
[32m[20230113 20:15:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1387 --------------------------#
[32m[20230113 20:15:21 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:21 @agent_ppo2.py:186][0m |          -0.0001 |           8.5304 |           9.1992 |
[32m[20230113 20:15:21 @agent_ppo2.py:186][0m |          -0.0032 |           5.8891 |           9.2076 |
[32m[20230113 20:15:21 @agent_ppo2.py:186][0m |          -0.0064 |           5.1866 |           9.1991 |
[32m[20230113 20:15:22 @agent_ppo2.py:186][0m |          -0.0079 |           4.8336 |           9.1887 |
[32m[20230113 20:15:22 @agent_ppo2.py:186][0m |          -0.0089 |           4.5112 |           9.2004 |
[32m[20230113 20:15:22 @agent_ppo2.py:186][0m |          -0.0117 |           4.2818 |           9.2070 |
[32m[20230113 20:15:22 @agent_ppo2.py:186][0m |          -0.0113 |           4.0605 |           9.1956 |
[32m[20230113 20:15:22 @agent_ppo2.py:186][0m |          -0.0127 |           3.9155 |           9.2062 |
[32m[20230113 20:15:22 @agent_ppo2.py:186][0m |          -0.0135 |           3.7845 |           9.2004 |
[32m[20230113 20:15:22 @agent_ppo2.py:186][0m |          -0.0135 |           3.7027 |           9.2039 |
[32m[20230113 20:15:22 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.43
[32m[20230113 20:15:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.43
[32m[20230113 20:15:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.42
[32m[20230113 20:15:22 @agent_ppo2.py:144][0m Total time:      30.83 min
[32m[20230113 20:15:22 @agent_ppo2.py:146][0m 2842624 total steps have happened
[32m[20230113 20:15:22 @agent_ppo2.py:122][0m #------------------------ Iteration 1388 --------------------------#
[32m[20230113 20:15:23 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:23 @agent_ppo2.py:186][0m |           0.0011 |           7.5101 |           9.3316 |
[32m[20230113 20:15:23 @agent_ppo2.py:186][0m |          -0.0009 |           5.2535 |           9.3227 |
[32m[20230113 20:15:23 @agent_ppo2.py:186][0m |          -0.0120 |           4.5428 |           9.3211 |
[32m[20230113 20:15:23 @agent_ppo2.py:186][0m |          -0.0069 |           4.2561 |           9.3175 |
[32m[20230113 20:15:23 @agent_ppo2.py:186][0m |          -0.0069 |           3.9553 |           9.3182 |
[32m[20230113 20:15:23 @agent_ppo2.py:186][0m |          -0.0073 |           3.7594 |           9.3121 |
[32m[20230113 20:15:23 @agent_ppo2.py:186][0m |          -0.0047 |           3.6381 |           9.3094 |
[32m[20230113 20:15:23 @agent_ppo2.py:186][0m |          -0.0030 |           3.5132 |           9.3130 |
[32m[20230113 20:15:23 @agent_ppo2.py:186][0m |          -0.0092 |           3.3630 |           9.3139 |
[32m[20230113 20:15:23 @agent_ppo2.py:186][0m |          -0.0151 |           3.2418 |           9.3091 |
[32m[20230113 20:15:23 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:15:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.22
[32m[20230113 20:15:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.06
[32m[20230113 20:15:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.64
[32m[20230113 20:15:23 @agent_ppo2.py:144][0m Total time:      30.85 min
[32m[20230113 20:15:23 @agent_ppo2.py:146][0m 2844672 total steps have happened
[32m[20230113 20:15:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1389 --------------------------#
[32m[20230113 20:15:24 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:24 @agent_ppo2.py:186][0m |           0.0014 |           7.5440 |           9.5470 |
[32m[20230113 20:15:24 @agent_ppo2.py:186][0m |          -0.0038 |           5.0457 |           9.5339 |
[32m[20230113 20:15:24 @agent_ppo2.py:186][0m |          -0.0049 |           4.4096 |           9.5251 |
[32m[20230113 20:15:24 @agent_ppo2.py:186][0m |          -0.0127 |           4.0219 |           9.5309 |
[32m[20230113 20:15:24 @agent_ppo2.py:186][0m |          -0.0088 |           3.7990 |           9.5196 |
[32m[20230113 20:15:24 @agent_ppo2.py:186][0m |          -0.0079 |           3.6491 |           9.5186 |
[32m[20230113 20:15:24 @agent_ppo2.py:186][0m |          -0.0125 |           3.5039 |           9.5216 |
[32m[20230113 20:15:24 @agent_ppo2.py:186][0m |          -0.0088 |           3.3918 |           9.5184 |
[32m[20230113 20:15:24 @agent_ppo2.py:186][0m |          -0.0175 |           3.2529 |           9.5182 |
[32m[20230113 20:15:24 @agent_ppo2.py:186][0m |          -0.0097 |           3.2003 |           9.5131 |
[32m[20230113 20:15:24 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.39
[32m[20230113 20:15:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.44
[32m[20230113 20:15:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.84
[32m[20230113 20:15:25 @agent_ppo2.py:144][0m Total time:      30.87 min
[32m[20230113 20:15:25 @agent_ppo2.py:146][0m 2846720 total steps have happened
[32m[20230113 20:15:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1390 --------------------------#
[32m[20230113 20:15:25 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:15:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:25 @agent_ppo2.py:186][0m |          -0.0026 |           7.2590 |           9.4664 |
[32m[20230113 20:15:25 @agent_ppo2.py:186][0m |          -0.0051 |           5.4257 |           9.4575 |
[32m[20230113 20:15:25 @agent_ppo2.py:186][0m |          -0.0095 |           4.7795 |           9.4494 |
[32m[20230113 20:15:25 @agent_ppo2.py:186][0m |          -0.0100 |           4.3947 |           9.4489 |
[32m[20230113 20:15:25 @agent_ppo2.py:186][0m |          -0.0107 |           4.1119 |           9.4476 |
[32m[20230113 20:15:25 @agent_ppo2.py:186][0m |          -0.0132 |           3.8517 |           9.4354 |
[32m[20230113 20:15:25 @agent_ppo2.py:186][0m |          -0.0142 |           3.6719 |           9.4383 |
[32m[20230113 20:15:25 @agent_ppo2.py:186][0m |          -0.0143 |           3.5400 |           9.4301 |
[32m[20230113 20:15:26 @agent_ppo2.py:186][0m |          -0.0144 |           3.4207 |           9.4290 |
[32m[20230113 20:15:26 @agent_ppo2.py:186][0m |          -0.0173 |           3.2909 |           9.4246 |
[32m[20230113 20:15:26 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:15:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.30
[32m[20230113 20:15:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.02
[32m[20230113 20:15:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.57
[32m[20230113 20:15:26 @agent_ppo2.py:144][0m Total time:      30.89 min
[32m[20230113 20:15:26 @agent_ppo2.py:146][0m 2848768 total steps have happened
[32m[20230113 20:15:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1391 --------------------------#
[32m[20230113 20:15:26 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:15:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:26 @agent_ppo2.py:186][0m |           0.0014 |           7.0132 |           9.4900 |
[32m[20230113 20:15:27 @agent_ppo2.py:186][0m |          -0.0057 |           5.2731 |           9.4746 |
[32m[20230113 20:15:27 @agent_ppo2.py:186][0m |          -0.0074 |           4.6486 |           9.4626 |
[32m[20230113 20:15:27 @agent_ppo2.py:186][0m |          -0.0093 |           4.3139 |           9.4547 |
[32m[20230113 20:15:27 @agent_ppo2.py:186][0m |          -0.0101 |           4.0422 |           9.4506 |
[32m[20230113 20:15:27 @agent_ppo2.py:186][0m |          -0.0106 |           3.8865 |           9.4428 |
[32m[20230113 20:15:27 @agent_ppo2.py:186][0m |          -0.0130 |           3.6970 |           9.4438 |
[32m[20230113 20:15:27 @agent_ppo2.py:186][0m |          -0.0127 |           3.5651 |           9.4401 |
[32m[20230113 20:15:27 @agent_ppo2.py:186][0m |          -0.0129 |           3.4560 |           9.4371 |
[32m[20230113 20:15:27 @agent_ppo2.py:186][0m |          -0.0143 |           3.3851 |           9.4428 |
[32m[20230113 20:15:27 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.07
[32m[20230113 20:15:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.42
[32m[20230113 20:15:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.36
[32m[20230113 20:15:27 @agent_ppo2.py:144][0m Total time:      30.91 min
[32m[20230113 20:15:27 @agent_ppo2.py:146][0m 2850816 total steps have happened
[32m[20230113 20:15:27 @agent_ppo2.py:122][0m #------------------------ Iteration 1392 --------------------------#
[32m[20230113 20:15:28 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:15:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:28 @agent_ppo2.py:186][0m |           0.0020 |           6.5039 |           9.4535 |
[32m[20230113 20:15:28 @agent_ppo2.py:186][0m |          -0.0035 |           5.2843 |           9.4575 |
[32m[20230113 20:15:28 @agent_ppo2.py:186][0m |          -0.0130 |           4.8625 |           9.4541 |
[32m[20230113 20:15:28 @agent_ppo2.py:186][0m |          -0.0079 |           4.5534 |           9.4534 |
[32m[20230113 20:15:28 @agent_ppo2.py:186][0m |          -0.0088 |           4.3083 |           9.4508 |
[32m[20230113 20:15:28 @agent_ppo2.py:186][0m |          -0.0105 |           4.0805 |           9.4535 |
[32m[20230113 20:15:28 @agent_ppo2.py:186][0m |          -0.0098 |           3.9932 |           9.4495 |
[32m[20230113 20:15:28 @agent_ppo2.py:186][0m |          -0.0188 |           3.9006 |           9.4436 |
[32m[20230113 20:15:28 @agent_ppo2.py:186][0m |          -0.0152 |           3.7247 |           9.4409 |
[32m[20230113 20:15:28 @agent_ppo2.py:186][0m |          -0.0099 |           3.6810 |           9.4443 |
[32m[20230113 20:15:28 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:15:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.08
[32m[20230113 20:15:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.25
[32m[20230113 20:15:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.11
[32m[20230113 20:15:29 @agent_ppo2.py:144][0m Total time:      30.93 min
[32m[20230113 20:15:29 @agent_ppo2.py:146][0m 2852864 total steps have happened
[32m[20230113 20:15:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1393 --------------------------#
[32m[20230113 20:15:29 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:29 @agent_ppo2.py:186][0m |          -0.0019 |           6.1432 |           9.2258 |
[32m[20230113 20:15:29 @agent_ppo2.py:186][0m |          -0.0044 |           4.1887 |           9.2098 |
[32m[20230113 20:15:29 @agent_ppo2.py:186][0m |          -0.0108 |           3.6365 |           9.2107 |
[32m[20230113 20:15:29 @agent_ppo2.py:186][0m |          -0.0069 |           3.2587 |           9.2193 |
[32m[20230113 20:15:29 @agent_ppo2.py:186][0m |          -0.0126 |           3.0057 |           9.2112 |
[32m[20230113 20:15:29 @agent_ppo2.py:186][0m |          -0.0168 |           2.8230 |           9.2176 |
[32m[20230113 20:15:29 @agent_ppo2.py:186][0m |          -0.0179 |           2.7330 |           9.2051 |
[32m[20230113 20:15:29 @agent_ppo2.py:186][0m |          -0.0139 |           2.5915 |           9.2118 |
[32m[20230113 20:15:29 @agent_ppo2.py:186][0m |          -0.0131 |           2.5304 |           9.2088 |
[32m[20230113 20:15:29 @agent_ppo2.py:186][0m |          -0.0202 |           2.4415 |           9.2119 |
[32m[20230113 20:15:29 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.04
[32m[20230113 20:15:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.13
[32m[20230113 20:15:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.55
[32m[20230113 20:15:30 @agent_ppo2.py:144][0m Total time:      30.95 min
[32m[20230113 20:15:30 @agent_ppo2.py:146][0m 2854912 total steps have happened
[32m[20230113 20:15:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1394 --------------------------#
[32m[20230113 20:15:30 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:15:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:30 @agent_ppo2.py:186][0m |          -0.0019 |           5.7542 |           9.4209 |
[32m[20230113 20:15:30 @agent_ppo2.py:186][0m |          -0.0080 |           4.2907 |           9.3866 |
[32m[20230113 20:15:30 @agent_ppo2.py:186][0m |          -0.0131 |           3.6745 |           9.3913 |
[32m[20230113 20:15:30 @agent_ppo2.py:186][0m |          -0.0111 |           3.4101 |           9.3942 |
[32m[20230113 20:15:31 @agent_ppo2.py:186][0m |          -0.0180 |           3.2345 |           9.3903 |
[32m[20230113 20:15:31 @agent_ppo2.py:186][0m |          -0.0150 |           3.0341 |           9.3807 |
[32m[20230113 20:15:31 @agent_ppo2.py:186][0m |          -0.0151 |           2.9584 |           9.3891 |
[32m[20230113 20:15:31 @agent_ppo2.py:186][0m |          -0.0189 |           2.8570 |           9.3812 |
[32m[20230113 20:15:31 @agent_ppo2.py:186][0m |          -0.0218 |           2.7629 |           9.3877 |
[32m[20230113 20:15:31 @agent_ppo2.py:186][0m |          -0.0194 |           2.7383 |           9.3914 |
[32m[20230113 20:15:31 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:15:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.19
[32m[20230113 20:15:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.52
[32m[20230113 20:15:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.79
[32m[20230113 20:15:31 @agent_ppo2.py:144][0m Total time:      30.98 min
[32m[20230113 20:15:31 @agent_ppo2.py:146][0m 2856960 total steps have happened
[32m[20230113 20:15:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1395 --------------------------#
[32m[20230113 20:15:32 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:32 @agent_ppo2.py:186][0m |           0.0025 |           6.2034 |           9.5661 |
[32m[20230113 20:15:32 @agent_ppo2.py:186][0m |          -0.0091 |           4.9253 |           9.5466 |
[32m[20230113 20:15:32 @agent_ppo2.py:186][0m |          -0.0087 |           4.3130 |           9.5414 |
[32m[20230113 20:15:32 @agent_ppo2.py:186][0m |          -0.0074 |           4.0141 |           9.5383 |
[32m[20230113 20:15:32 @agent_ppo2.py:186][0m |          -0.0106 |           3.7554 |           9.5455 |
[32m[20230113 20:15:32 @agent_ppo2.py:186][0m |          -0.0115 |           3.5756 |           9.5292 |
[32m[20230113 20:15:32 @agent_ppo2.py:186][0m |          -0.0108 |           3.4730 |           9.5360 |
[32m[20230113 20:15:32 @agent_ppo2.py:186][0m |          -0.0157 |           3.2953 |           9.5321 |
[32m[20230113 20:15:32 @agent_ppo2.py:186][0m |          -0.0164 |           3.2119 |           9.5316 |
[32m[20230113 20:15:32 @agent_ppo2.py:186][0m |          -0.0170 |           3.0830 |           9.5311 |
[32m[20230113 20:15:32 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.87
[32m[20230113 20:15:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.94
[32m[20230113 20:15:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.81
[32m[20230113 20:15:32 @agent_ppo2.py:144][0m Total time:      31.00 min
[32m[20230113 20:15:32 @agent_ppo2.py:146][0m 2859008 total steps have happened
[32m[20230113 20:15:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1396 --------------------------#
[32m[20230113 20:15:33 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:15:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:33 @agent_ppo2.py:186][0m |           0.0002 |           6.1625 |           9.5165 |
[32m[20230113 20:15:33 @agent_ppo2.py:186][0m |          -0.0038 |           4.7619 |           9.5159 |
[32m[20230113 20:15:33 @agent_ppo2.py:186][0m |          -0.0084 |           4.1129 |           9.4987 |
[32m[20230113 20:15:33 @agent_ppo2.py:186][0m |          -0.0102 |           3.8557 |           9.5048 |
[32m[20230113 20:15:33 @agent_ppo2.py:186][0m |          -0.0117 |           3.6154 |           9.5059 |
[32m[20230113 20:15:33 @agent_ppo2.py:186][0m |          -0.0125 |           3.4265 |           9.5078 |
[32m[20230113 20:15:33 @agent_ppo2.py:186][0m |          -0.0136 |           3.2697 |           9.5009 |
[32m[20230113 20:15:33 @agent_ppo2.py:186][0m |          -0.0139 |           3.1364 |           9.4985 |
[32m[20230113 20:15:33 @agent_ppo2.py:186][0m |          -0.0152 |           3.0473 |           9.4974 |
[32m[20230113 20:15:33 @agent_ppo2.py:186][0m |          -0.0154 |           2.9595 |           9.5029 |
[32m[20230113 20:15:33 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:15:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.99
[32m[20230113 20:15:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.06
[32m[20230113 20:15:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 146.38
[32m[20230113 20:15:34 @agent_ppo2.py:144][0m Total time:      31.02 min
[32m[20230113 20:15:34 @agent_ppo2.py:146][0m 2861056 total steps have happened
[32m[20230113 20:15:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1397 --------------------------#
[32m[20230113 20:15:34 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:34 @agent_ppo2.py:186][0m |           0.0011 |           6.8630 |           9.2874 |
[32m[20230113 20:15:34 @agent_ppo2.py:186][0m |          -0.0087 |           4.8574 |           9.2745 |
[32m[20230113 20:15:34 @agent_ppo2.py:186][0m |          -0.0161 |           4.1212 |           9.2679 |
[32m[20230113 20:15:34 @agent_ppo2.py:186][0m |          -0.0143 |           3.6492 |           9.2649 |
[32m[20230113 20:15:34 @agent_ppo2.py:186][0m |          -0.0168 |           3.4262 |           9.2531 |
[32m[20230113 20:15:34 @agent_ppo2.py:186][0m |          -0.0158 |           3.1940 |           9.2650 |
[32m[20230113 20:15:35 @agent_ppo2.py:186][0m |          -0.0116 |           3.0691 |           9.2563 |
[32m[20230113 20:15:35 @agent_ppo2.py:186][0m |          -0.0191 |           2.9051 |           9.2541 |
[32m[20230113 20:15:35 @agent_ppo2.py:186][0m |          -0.0122 |           2.8006 |           9.2501 |
[32m[20230113 20:15:35 @agent_ppo2.py:186][0m |          -0.0134 |           2.7216 |           9.2558 |
[32m[20230113 20:15:35 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.61
[32m[20230113 20:15:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.28
[32m[20230113 20:15:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.28
[32m[20230113 20:15:35 @agent_ppo2.py:144][0m Total time:      31.04 min
[32m[20230113 20:15:35 @agent_ppo2.py:146][0m 2863104 total steps have happened
[32m[20230113 20:15:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1398 --------------------------#
[32m[20230113 20:15:35 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:36 @agent_ppo2.py:186][0m |          -0.0002 |           6.9055 |           9.3534 |
[32m[20230113 20:15:36 @agent_ppo2.py:186][0m |          -0.0075 |           5.0493 |           9.3315 |
[32m[20230113 20:15:36 @agent_ppo2.py:186][0m |          -0.0106 |           4.4295 |           9.3296 |
[32m[20230113 20:15:36 @agent_ppo2.py:186][0m |          -0.0132 |           4.1602 |           9.3277 |
[32m[20230113 20:15:36 @agent_ppo2.py:186][0m |          -0.0145 |           3.9232 |           9.3242 |
[32m[20230113 20:15:36 @agent_ppo2.py:186][0m |          -0.0135 |           3.8097 |           9.3255 |
[32m[20230113 20:15:36 @agent_ppo2.py:186][0m |          -0.0159 |           3.6804 |           9.3236 |
[32m[20230113 20:15:36 @agent_ppo2.py:186][0m |          -0.0153 |           3.5173 |           9.3259 |
[32m[20230113 20:15:36 @agent_ppo2.py:186][0m |          -0.0160 |           3.4502 |           9.3271 |
[32m[20230113 20:15:36 @agent_ppo2.py:186][0m |          -0.0175 |           3.3612 |           9.3254 |
[32m[20230113 20:15:36 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:15:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.89
[32m[20230113 20:15:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.98
[32m[20230113 20:15:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.92
[32m[20230113 20:15:36 @agent_ppo2.py:144][0m Total time:      31.06 min
[32m[20230113 20:15:36 @agent_ppo2.py:146][0m 2865152 total steps have happened
[32m[20230113 20:15:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1399 --------------------------#
[32m[20230113 20:15:37 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:37 @agent_ppo2.py:186][0m |          -0.0059 |           6.8659 |           9.4415 |
[32m[20230113 20:15:37 @agent_ppo2.py:186][0m |          -0.0090 |           5.4771 |           9.4117 |
[32m[20230113 20:15:37 @agent_ppo2.py:186][0m |          -0.0075 |           4.9127 |           9.4172 |
[32m[20230113 20:15:37 @agent_ppo2.py:186][0m |          -0.0112 |           4.6807 |           9.4152 |
[32m[20230113 20:15:37 @agent_ppo2.py:186][0m |          -0.0110 |           4.3391 |           9.4053 |
[32m[20230113 20:15:37 @agent_ppo2.py:186][0m |          -0.0152 |           4.0218 |           9.4112 |
[32m[20230113 20:15:37 @agent_ppo2.py:186][0m |          -0.0157 |           3.8356 |           9.4086 |
[32m[20230113 20:15:37 @agent_ppo2.py:186][0m |          -0.0122 |           3.7348 |           9.4035 |
[32m[20230113 20:15:37 @agent_ppo2.py:186][0m |          -0.0141 |           3.5314 |           9.4015 |
[32m[20230113 20:15:37 @agent_ppo2.py:186][0m |          -0.0150 |           3.4119 |           9.4008 |
[32m[20230113 20:15:37 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.97
[32m[20230113 20:15:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.92
[32m[20230113 20:15:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 31.34
[32m[20230113 20:15:37 @agent_ppo2.py:144][0m Total time:      31.08 min
[32m[20230113 20:15:37 @agent_ppo2.py:146][0m 2867200 total steps have happened
[32m[20230113 20:15:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1400 --------------------------#
[32m[20230113 20:15:38 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:15:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:38 @agent_ppo2.py:186][0m |           0.0000 |           6.4012 |           9.5067 |
[32m[20230113 20:15:38 @agent_ppo2.py:186][0m |          -0.0032 |           4.7181 |           9.4950 |
[32m[20230113 20:15:38 @agent_ppo2.py:186][0m |          -0.0037 |           4.2003 |           9.5004 |
[32m[20230113 20:15:38 @agent_ppo2.py:186][0m |          -0.0072 |           3.9777 |           9.5020 |
[32m[20230113 20:15:38 @agent_ppo2.py:186][0m |          -0.0082 |           3.8084 |           9.4977 |
[32m[20230113 20:15:38 @agent_ppo2.py:186][0m |          -0.0097 |           3.6326 |           9.5011 |
[32m[20230113 20:15:38 @agent_ppo2.py:186][0m |          -0.0074 |           3.5492 |           9.5021 |
[32m[20230113 20:15:38 @agent_ppo2.py:186][0m |          -0.0079 |           3.4376 |           9.4999 |
[32m[20230113 20:15:38 @agent_ppo2.py:186][0m |          -0.0108 |           3.3285 |           9.4987 |
[32m[20230113 20:15:38 @agent_ppo2.py:186][0m |          -0.0114 |           3.2753 |           9.5066 |
[32m[20230113 20:15:38 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:15:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.06
[32m[20230113 20:15:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.09
[32m[20230113 20:15:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 149.79
[32m[20230113 20:15:39 @agent_ppo2.py:144][0m Total time:      31.10 min
[32m[20230113 20:15:39 @agent_ppo2.py:146][0m 2869248 total steps have happened
[32m[20230113 20:15:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1401 --------------------------#
[32m[20230113 20:15:39 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:15:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:39 @agent_ppo2.py:186][0m |          -0.0009 |           6.6867 |           9.4759 |
[32m[20230113 20:15:39 @agent_ppo2.py:186][0m |          -0.0073 |           4.6191 |           9.4636 |
[32m[20230113 20:15:39 @agent_ppo2.py:186][0m |          -0.0094 |           4.1217 |           9.4477 |
[32m[20230113 20:15:39 @agent_ppo2.py:186][0m |          -0.0127 |           3.8480 |           9.4409 |
[32m[20230113 20:15:39 @agent_ppo2.py:186][0m |          -0.0154 |           3.6736 |           9.4320 |
[32m[20230113 20:15:40 @agent_ppo2.py:186][0m |          -0.0085 |           3.6693 |           9.4251 |
[32m[20230113 20:15:40 @agent_ppo2.py:186][0m |          -0.0191 |           3.4032 |           9.4193 |
[32m[20230113 20:15:40 @agent_ppo2.py:186][0m |          -0.0164 |           3.2929 |           9.4198 |
[32m[20230113 20:15:40 @agent_ppo2.py:186][0m |          -0.0172 |           3.2400 |           9.4165 |
[32m[20230113 20:15:40 @agent_ppo2.py:186][0m |          -0.0182 |           3.1582 |           9.4091 |
[32m[20230113 20:15:40 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:15:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.15
[32m[20230113 20:15:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.28
[32m[20230113 20:15:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.81
[32m[20230113 20:15:40 @agent_ppo2.py:144][0m Total time:      31.13 min
[32m[20230113 20:15:40 @agent_ppo2.py:146][0m 2871296 total steps have happened
[32m[20230113 20:15:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1402 --------------------------#
[32m[20230113 20:15:41 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:15:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:41 @agent_ppo2.py:186][0m |          -0.0014 |           6.4637 |           9.3357 |
[32m[20230113 20:15:41 @agent_ppo2.py:186][0m |          -0.0046 |           4.6504 |           9.3330 |
[32m[20230113 20:15:41 @agent_ppo2.py:186][0m |          -0.0078 |           4.1015 |           9.3300 |
[32m[20230113 20:15:41 @agent_ppo2.py:186][0m |          -0.0077 |           3.8620 |           9.3252 |
[32m[20230113 20:15:41 @agent_ppo2.py:186][0m |          -0.0097 |           3.6621 |           9.3290 |
[32m[20230113 20:15:41 @agent_ppo2.py:186][0m |          -0.0139 |           3.4852 |           9.3301 |
[32m[20230113 20:15:41 @agent_ppo2.py:186][0m |          -0.0112 |           3.2984 |           9.3257 |
[32m[20230113 20:15:41 @agent_ppo2.py:186][0m |          -0.0154 |           3.2232 |           9.3149 |
[32m[20230113 20:15:41 @agent_ppo2.py:186][0m |          -0.0130 |           3.1388 |           9.3272 |
[32m[20230113 20:15:41 @agent_ppo2.py:186][0m |          -0.0119 |           3.0568 |           9.3237 |
[32m[20230113 20:15:41 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:15:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.08
[32m[20230113 20:15:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.52
[32m[20230113 20:15:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.94
[32m[20230113 20:15:41 @agent_ppo2.py:144][0m Total time:      31.15 min
[32m[20230113 20:15:41 @agent_ppo2.py:146][0m 2873344 total steps have happened
[32m[20230113 20:15:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1403 --------------------------#
[32m[20230113 20:15:42 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:15:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:42 @agent_ppo2.py:186][0m |           0.0008 |           5.6467 |           9.4252 |
[32m[20230113 20:15:42 @agent_ppo2.py:186][0m |          -0.0062 |           4.6541 |           9.4267 |
[32m[20230113 20:15:42 @agent_ppo2.py:186][0m |          -0.0072 |           4.2396 |           9.4177 |
[32m[20230113 20:15:42 @agent_ppo2.py:186][0m |          -0.0095 |           3.9558 |           9.4283 |
[32m[20230113 20:15:42 @agent_ppo2.py:186][0m |          -0.0112 |           3.7862 |           9.4182 |
[32m[20230113 20:15:42 @agent_ppo2.py:186][0m |          -0.0114 |           3.5719 |           9.4269 |
[32m[20230113 20:15:42 @agent_ppo2.py:186][0m |          -0.0119 |           3.4759 |           9.4222 |
[32m[20230113 20:15:42 @agent_ppo2.py:186][0m |          -0.0129 |           3.3546 |           9.4285 |
[32m[20230113 20:15:42 @agent_ppo2.py:186][0m |          -0.0137 |           3.2200 |           9.4228 |
[32m[20230113 20:15:42 @agent_ppo2.py:186][0m |          -0.0145 |           3.0973 |           9.4272 |
[32m[20230113 20:15:42 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.65
[32m[20230113 20:15:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.30
[32m[20230113 20:15:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 174.47
[32m[20230113 20:15:43 @agent_ppo2.py:144][0m Total time:      31.17 min
[32m[20230113 20:15:43 @agent_ppo2.py:146][0m 2875392 total steps have happened
[32m[20230113 20:15:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1404 --------------------------#
[32m[20230113 20:15:43 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:43 @agent_ppo2.py:186][0m |          -0.0024 |           6.4047 |           9.1033 |
[32m[20230113 20:15:43 @agent_ppo2.py:186][0m |          -0.0054 |           4.9821 |           9.0934 |
[32m[20230113 20:15:43 @agent_ppo2.py:186][0m |          -0.0083 |           4.4846 |           9.0910 |
[32m[20230113 20:15:43 @agent_ppo2.py:186][0m |          -0.0125 |           4.2939 |           9.0920 |
[32m[20230113 20:15:43 @agent_ppo2.py:186][0m |          -0.0125 |           4.0853 |           9.0767 |
[32m[20230113 20:15:43 @agent_ppo2.py:186][0m |          -0.0121 |           3.9635 |           9.0883 |
[32m[20230113 20:15:43 @agent_ppo2.py:186][0m |          -0.0130 |           3.8303 |           9.0815 |
[32m[20230113 20:15:43 @agent_ppo2.py:186][0m |          -0.0147 |           3.6564 |           9.0890 |
[32m[20230113 20:15:43 @agent_ppo2.py:186][0m |          -0.0153 |           3.6234 |           9.0813 |
[32m[20230113 20:15:43 @agent_ppo2.py:186][0m |          -0.0145 |           3.5014 |           9.0781 |
[32m[20230113 20:15:43 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.46
[32m[20230113 20:15:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.24
[32m[20230113 20:15:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.40
[32m[20230113 20:15:44 @agent_ppo2.py:144][0m Total time:      31.19 min
[32m[20230113 20:15:44 @agent_ppo2.py:146][0m 2877440 total steps have happened
[32m[20230113 20:15:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1405 --------------------------#
[32m[20230113 20:15:44 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:15:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:44 @agent_ppo2.py:186][0m |           0.0044 |          15.8257 |           9.1886 |
[32m[20230113 20:15:44 @agent_ppo2.py:186][0m |          -0.0088 |           7.2075 |           9.1696 |
[32m[20230113 20:15:45 @agent_ppo2.py:186][0m |          -0.0127 |           6.0594 |           9.1618 |
[32m[20230113 20:15:45 @agent_ppo2.py:186][0m |           0.0144 |           5.3950 |           9.1477 |
[32m[20230113 20:15:45 @agent_ppo2.py:186][0m |          -0.0081 |           5.0471 |           9.1277 |
[32m[20230113 20:15:45 @agent_ppo2.py:186][0m |          -0.0121 |           4.5913 |           9.1449 |
[32m[20230113 20:15:45 @agent_ppo2.py:186][0m |          -0.0165 |           4.3520 |           9.1479 |
[32m[20230113 20:15:45 @agent_ppo2.py:186][0m |          -0.0089 |           4.3146 |           9.1410 |
[32m[20230113 20:15:45 @agent_ppo2.py:186][0m |          -0.0004 |           4.0155 |           9.1404 |
[32m[20230113 20:15:45 @agent_ppo2.py:186][0m |          -0.0178 |           3.8956 |           9.1405 |
[32m[20230113 20:15:45 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:15:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 133.20
[32m[20230113 20:15:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.53
[32m[20230113 20:15:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 92.15
[32m[20230113 20:15:45 @agent_ppo2.py:144][0m Total time:      31.21 min
[32m[20230113 20:15:45 @agent_ppo2.py:146][0m 2879488 total steps have happened
[32m[20230113 20:15:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1406 --------------------------#
[32m[20230113 20:15:46 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 20:15:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:46 @agent_ppo2.py:186][0m |           0.0054 |          23.9934 |           9.2476 |
[32m[20230113 20:15:46 @agent_ppo2.py:186][0m |          -0.0066 |          12.1760 |           9.2219 |
[32m[20230113 20:15:46 @agent_ppo2.py:186][0m |          -0.0113 |           9.0482 |           9.2132 |
[32m[20230113 20:15:46 @agent_ppo2.py:186][0m |          -0.0129 |           7.0512 |           9.2115 |
[32m[20230113 20:15:46 @agent_ppo2.py:186][0m |          -0.0019 |           6.0308 |           9.2118 |
[32m[20230113 20:15:46 @agent_ppo2.py:186][0m |          -0.0167 |           5.1950 |           9.2033 |
[32m[20230113 20:15:46 @agent_ppo2.py:186][0m |          -0.0173 |           4.6888 |           9.1925 |
[32m[20230113 20:15:46 @agent_ppo2.py:186][0m |          -0.0192 |           4.3637 |           9.1946 |
[32m[20230113 20:15:46 @agent_ppo2.py:186][0m |          -0.0138 |           4.0807 |           9.1858 |
[32m[20230113 20:15:46 @agent_ppo2.py:186][0m |          -0.0155 |           3.9332 |           9.1949 |
[32m[20230113 20:15:46 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 20:15:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 132.75
[32m[20230113 20:15:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.17
[32m[20230113 20:15:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.01
[32m[20230113 20:15:46 @agent_ppo2.py:144][0m Total time:      31.23 min
[32m[20230113 20:15:46 @agent_ppo2.py:146][0m 2881536 total steps have happened
[32m[20230113 20:15:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1407 --------------------------#
[32m[20230113 20:15:47 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:47 @agent_ppo2.py:186][0m |          -0.0002 |           8.1546 |           9.3889 |
[32m[20230113 20:15:47 @agent_ppo2.py:186][0m |          -0.0063 |           5.8471 |           9.3891 |
[32m[20230113 20:15:47 @agent_ppo2.py:186][0m |          -0.0090 |           5.1310 |           9.3801 |
[32m[20230113 20:15:47 @agent_ppo2.py:186][0m |          -0.0112 |           4.6762 |           9.3781 |
[32m[20230113 20:15:47 @agent_ppo2.py:186][0m |          -0.0114 |           4.3876 |           9.3744 |
[32m[20230113 20:15:47 @agent_ppo2.py:186][0m |          -0.0123 |           4.1646 |           9.3633 |
[32m[20230113 20:15:47 @agent_ppo2.py:186][0m |          -0.0140 |           4.0563 |           9.3672 |
[32m[20230113 20:15:47 @agent_ppo2.py:186][0m |          -0.0143 |           3.8787 |           9.3686 |
[32m[20230113 20:15:47 @agent_ppo2.py:186][0m |          -0.0145 |           3.7124 |           9.3638 |
[32m[20230113 20:15:47 @agent_ppo2.py:186][0m |          -0.0150 |           3.6546 |           9.3617 |
[32m[20230113 20:15:47 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.74
[32m[20230113 20:15:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.48
[32m[20230113 20:15:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.64
[32m[20230113 20:15:48 @agent_ppo2.py:144][0m Total time:      31.25 min
[32m[20230113 20:15:48 @agent_ppo2.py:146][0m 2883584 total steps have happened
[32m[20230113 20:15:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1408 --------------------------#
[32m[20230113 20:15:48 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:48 @agent_ppo2.py:186][0m |          -0.0009 |           5.5447 |           9.4532 |
[32m[20230113 20:15:48 @agent_ppo2.py:186][0m |          -0.0042 |           4.6721 |           9.4523 |
[32m[20230113 20:15:48 @agent_ppo2.py:186][0m |          -0.0083 |           4.2479 |           9.4454 |
[32m[20230113 20:15:48 @agent_ppo2.py:186][0m |          -0.0102 |           3.9000 |           9.4495 |
[32m[20230113 20:15:48 @agent_ppo2.py:186][0m |          -0.0114 |           3.6794 |           9.4504 |
[32m[20230113 20:15:48 @agent_ppo2.py:186][0m |          -0.0131 |           3.4932 |           9.4510 |
[32m[20230113 20:15:48 @agent_ppo2.py:186][0m |          -0.0128 |           3.3322 |           9.4494 |
[32m[20230113 20:15:48 @agent_ppo2.py:186][0m |          -0.0134 |           3.2204 |           9.4429 |
[32m[20230113 20:15:48 @agent_ppo2.py:186][0m |          -0.0143 |           3.1026 |           9.4465 |
[32m[20230113 20:15:48 @agent_ppo2.py:186][0m |          -0.0142 |           3.0178 |           9.4509 |
[32m[20230113 20:15:48 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.55
[32m[20230113 20:15:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.58
[32m[20230113 20:15:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 124.02
[32m[20230113 20:15:49 @agent_ppo2.py:144][0m Total time:      31.27 min
[32m[20230113 20:15:49 @agent_ppo2.py:146][0m 2885632 total steps have happened
[32m[20230113 20:15:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1409 --------------------------#
[32m[20230113 20:15:49 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:15:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:49 @agent_ppo2.py:186][0m |          -0.0023 |           5.2858 |           9.1640 |
[32m[20230113 20:15:49 @agent_ppo2.py:186][0m |          -0.0060 |           4.0134 |           9.1442 |
[32m[20230113 20:15:49 @agent_ppo2.py:186][0m |          -0.0073 |           3.5392 |           9.1524 |
[32m[20230113 20:15:49 @agent_ppo2.py:186][0m |          -0.0102 |           3.2905 |           9.1413 |
[32m[20230113 20:15:50 @agent_ppo2.py:186][0m |          -0.0066 |           3.1813 |           9.1429 |
[32m[20230113 20:15:50 @agent_ppo2.py:186][0m |          -0.0105 |           3.0226 |           9.1359 |
[32m[20230113 20:15:50 @agent_ppo2.py:186][0m |          -0.0085 |           2.8802 |           9.1469 |
[32m[20230113 20:15:50 @agent_ppo2.py:186][0m |          -0.0197 |           2.7949 |           9.1389 |
[32m[20230113 20:15:50 @agent_ppo2.py:186][0m |          -0.0160 |           2.7390 |           9.1450 |
[32m[20230113 20:15:50 @agent_ppo2.py:186][0m |          -0.0157 |           2.6592 |           9.1377 |
[32m[20230113 20:15:50 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.48
[32m[20230113 20:15:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.93
[32m[20230113 20:15:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.83
[32m[20230113 20:15:50 @agent_ppo2.py:144][0m Total time:      31.29 min
[32m[20230113 20:15:50 @agent_ppo2.py:146][0m 2887680 total steps have happened
[32m[20230113 20:15:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1410 --------------------------#
[32m[20230113 20:15:51 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:51 @agent_ppo2.py:186][0m |          -0.0057 |          11.8890 |           9.2103 |
[32m[20230113 20:15:51 @agent_ppo2.py:186][0m |          -0.0104 |           7.3943 |           9.1720 |
[32m[20230113 20:15:51 @agent_ppo2.py:186][0m |          -0.0127 |           6.5430 |           9.1852 |
[32m[20230113 20:15:51 @agent_ppo2.py:186][0m |          -0.0177 |           5.9945 |           9.1867 |
[32m[20230113 20:15:51 @agent_ppo2.py:186][0m |          -0.0134 |           5.6166 |           9.1718 |
[32m[20230113 20:15:51 @agent_ppo2.py:186][0m |          -0.0121 |           5.4648 |           9.1768 |
[32m[20230113 20:15:51 @agent_ppo2.py:186][0m |          -0.0194 |           5.2344 |           9.1796 |
[32m[20230113 20:15:51 @agent_ppo2.py:186][0m |          -0.0142 |           5.1021 |           9.1816 |
[32m[20230113 20:15:51 @agent_ppo2.py:186][0m |          -0.0155 |           4.9476 |           9.1867 |
[32m[20230113 20:15:51 @agent_ppo2.py:186][0m |          -0.0211 |           4.7140 |           9.1804 |
[32m[20230113 20:15:51 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 178.65
[32m[20230113 20:15:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.33
[32m[20230113 20:15:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.10
[32m[20230113 20:15:51 @agent_ppo2.py:144][0m Total time:      31.31 min
[32m[20230113 20:15:51 @agent_ppo2.py:146][0m 2889728 total steps have happened
[32m[20230113 20:15:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1411 --------------------------#
[32m[20230113 20:15:52 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:52 @agent_ppo2.py:186][0m |           0.0002 |           6.0764 |           9.3251 |
[32m[20230113 20:15:52 @agent_ppo2.py:186][0m |          -0.0076 |           4.6797 |           9.2949 |
[32m[20230113 20:15:52 @agent_ppo2.py:186][0m |          -0.0096 |           4.2118 |           9.2941 |
[32m[20230113 20:15:52 @agent_ppo2.py:186][0m |          -0.0106 |           3.8989 |           9.2854 |
[32m[20230113 20:15:52 @agent_ppo2.py:186][0m |          -0.0109 |           3.7095 |           9.2913 |
[32m[20230113 20:15:52 @agent_ppo2.py:186][0m |          -0.0131 |           3.5600 |           9.2820 |
[32m[20230113 20:15:52 @agent_ppo2.py:186][0m |          -0.0134 |           3.4304 |           9.2787 |
[32m[20230113 20:15:52 @agent_ppo2.py:186][0m |          -0.0136 |           3.2984 |           9.2732 |
[32m[20230113 20:15:52 @agent_ppo2.py:186][0m |          -0.0149 |           3.2197 |           9.2786 |
[32m[20230113 20:15:52 @agent_ppo2.py:186][0m |          -0.0156 |           3.1417 |           9.2711 |
[32m[20230113 20:15:52 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:15:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.30
[32m[20230113 20:15:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.06
[32m[20230113 20:15:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: -1.34
[32m[20230113 20:15:53 @agent_ppo2.py:144][0m Total time:      31.33 min
[32m[20230113 20:15:53 @agent_ppo2.py:146][0m 2891776 total steps have happened
[32m[20230113 20:15:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1412 --------------------------#
[32m[20230113 20:15:53 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:15:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:53 @agent_ppo2.py:186][0m |          -0.0002 |          12.8533 |           9.1350 |
[32m[20230113 20:15:53 @agent_ppo2.py:186][0m |          -0.0081 |           8.8689 |           9.1269 |
[32m[20230113 20:15:53 @agent_ppo2.py:186][0m |          -0.0141 |           7.7533 |           9.1250 |
[32m[20230113 20:15:53 @agent_ppo2.py:186][0m |          -0.0138 |           7.1056 |           9.1272 |
[32m[20230113 20:15:53 @agent_ppo2.py:186][0m |          -0.0075 |           6.7083 |           9.1237 |
[32m[20230113 20:15:53 @agent_ppo2.py:186][0m |          -0.0028 |           6.3959 |           9.1200 |
[32m[20230113 20:15:53 @agent_ppo2.py:186][0m |          -0.0086 |           6.3865 |           9.1246 |
[32m[20230113 20:15:53 @agent_ppo2.py:186][0m |          -0.0100 |           5.7581 |           9.1170 |
[32m[20230113 20:15:53 @agent_ppo2.py:186][0m |          -0.0190 |           5.5837 |           9.1087 |
[32m[20230113 20:15:53 @agent_ppo2.py:186][0m |          -0.0188 |           5.4530 |           9.1109 |
[32m[20230113 20:15:53 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:15:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 128.08
[32m[20230113 20:15:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.26
[32m[20230113 20:15:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.06
[32m[20230113 20:15:54 @agent_ppo2.py:144][0m Total time:      31.35 min
[32m[20230113 20:15:54 @agent_ppo2.py:146][0m 2893824 total steps have happened
[32m[20230113 20:15:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1413 --------------------------#
[32m[20230113 20:15:54 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:15:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:54 @agent_ppo2.py:186][0m |          -0.0020 |           6.7737 |           9.4391 |
[32m[20230113 20:15:54 @agent_ppo2.py:186][0m |          -0.0072 |           4.6472 |           9.4262 |
[32m[20230113 20:15:54 @agent_ppo2.py:186][0m |          -0.0091 |           4.0350 |           9.4209 |
[32m[20230113 20:15:54 @agent_ppo2.py:186][0m |          -0.0087 |           3.7042 |           9.4087 |
[32m[20230113 20:15:54 @agent_ppo2.py:186][0m |          -0.0120 |           3.4486 |           9.4009 |
[32m[20230113 20:15:55 @agent_ppo2.py:186][0m |          -0.0117 |           3.2431 |           9.3957 |
[32m[20230113 20:15:55 @agent_ppo2.py:186][0m |          -0.0134 |           3.1189 |           9.4025 |
[32m[20230113 20:15:55 @agent_ppo2.py:186][0m |          -0.0146 |           3.0198 |           9.3956 |
[32m[20230113 20:15:55 @agent_ppo2.py:186][0m |          -0.0125 |           2.9502 |           9.3860 |
[32m[20230113 20:15:55 @agent_ppo2.py:186][0m |          -0.0161 |           2.8731 |           9.3996 |
[32m[20230113 20:15:55 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:15:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.54
[32m[20230113 20:15:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.59
[32m[20230113 20:15:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.81
[32m[20230113 20:15:55 @agent_ppo2.py:144][0m Total time:      31.38 min
[32m[20230113 20:15:55 @agent_ppo2.py:146][0m 2895872 total steps have happened
[32m[20230113 20:15:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1414 --------------------------#
[32m[20230113 20:15:55 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:15:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:56 @agent_ppo2.py:186][0m |          -0.0007 |           5.7526 |           9.0692 |
[32m[20230113 20:15:56 @agent_ppo2.py:186][0m |          -0.0069 |           4.5063 |           9.0526 |
[32m[20230113 20:15:56 @agent_ppo2.py:186][0m |          -0.0070 |           4.0746 |           9.0598 |
[32m[20230113 20:15:56 @agent_ppo2.py:186][0m |          -0.0141 |           3.7583 |           9.0470 |
[32m[20230113 20:15:56 @agent_ppo2.py:186][0m |          -0.0143 |           3.5954 |           9.0375 |
[32m[20230113 20:15:56 @agent_ppo2.py:186][0m |          -0.0138 |           3.4266 |           9.0413 |
[32m[20230113 20:15:56 @agent_ppo2.py:186][0m |          -0.0124 |           3.3575 |           9.0382 |
[32m[20230113 20:15:56 @agent_ppo2.py:186][0m |          -0.0146 |           3.2251 |           9.0392 |
[32m[20230113 20:15:56 @agent_ppo2.py:186][0m |          -0.0149 |           3.2192 |           9.0454 |
[32m[20230113 20:15:56 @agent_ppo2.py:186][0m |          -0.0153 |           3.1230 |           9.0405 |
[32m[20230113 20:15:56 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:15:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.80
[32m[20230113 20:15:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.30
[32m[20230113 20:15:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.33
[32m[20230113 20:15:56 @agent_ppo2.py:144][0m Total time:      31.40 min
[32m[20230113 20:15:56 @agent_ppo2.py:146][0m 2897920 total steps have happened
[32m[20230113 20:15:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1415 --------------------------#
[32m[20230113 20:15:57 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:15:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:57 @agent_ppo2.py:186][0m |           0.0007 |           6.4145 |           9.5272 |
[32m[20230113 20:15:57 @agent_ppo2.py:186][0m |          -0.0078 |           5.0138 |           9.5267 |
[32m[20230113 20:15:57 @agent_ppo2.py:186][0m |          -0.0102 |           4.5760 |           9.5140 |
[32m[20230113 20:15:57 @agent_ppo2.py:186][0m |          -0.0123 |           4.2826 |           9.5129 |
[32m[20230113 20:15:57 @agent_ppo2.py:186][0m |          -0.0131 |           4.0256 |           9.5046 |
[32m[20230113 20:15:57 @agent_ppo2.py:186][0m |          -0.0150 |           3.8672 |           9.5024 |
[32m[20230113 20:15:57 @agent_ppo2.py:186][0m |          -0.0151 |           3.7490 |           9.4982 |
[32m[20230113 20:15:57 @agent_ppo2.py:186][0m |          -0.0145 |           3.6487 |           9.4904 |
[32m[20230113 20:15:57 @agent_ppo2.py:186][0m |          -0.0155 |           3.5486 |           9.4913 |
[32m[20230113 20:15:57 @agent_ppo2.py:186][0m |          -0.0154 |           3.5271 |           9.4808 |
[32m[20230113 20:15:57 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:15:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.30
[32m[20230113 20:15:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.15
[32m[20230113 20:15:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.59
[32m[20230113 20:15:58 @agent_ppo2.py:144][0m Total time:      31.42 min
[32m[20230113 20:15:58 @agent_ppo2.py:146][0m 2899968 total steps have happened
[32m[20230113 20:15:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1416 --------------------------#
[32m[20230113 20:15:58 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:15:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:58 @agent_ppo2.py:186][0m |           0.0088 |           6.6864 |           9.1183 |
[32m[20230113 20:15:58 @agent_ppo2.py:186][0m |          -0.0079 |           4.1624 |           9.0985 |
[32m[20230113 20:15:58 @agent_ppo2.py:186][0m |           0.0004 |           3.5364 |           9.1028 |
[32m[20230113 20:15:58 @agent_ppo2.py:186][0m |          -0.0091 |           3.2531 |           9.0987 |
[32m[20230113 20:15:58 @agent_ppo2.py:186][0m |          -0.0091 |           3.0508 |           9.0913 |
[32m[20230113 20:15:58 @agent_ppo2.py:186][0m |          -0.0125 |           2.9341 |           9.0829 |
[32m[20230113 20:15:58 @agent_ppo2.py:186][0m |          -0.0156 |           2.7833 |           9.0781 |
[32m[20230113 20:15:58 @agent_ppo2.py:186][0m |          -0.0168 |           2.6926 |           9.0721 |
[32m[20230113 20:15:58 @agent_ppo2.py:186][0m |          -0.0153 |           2.6107 |           9.0707 |
[32m[20230113 20:15:58 @agent_ppo2.py:186][0m |          -0.0159 |           2.5532 |           9.0725 |
[32m[20230113 20:15:58 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:15:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.69
[32m[20230113 20:15:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.17
[32m[20230113 20:15:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.84
[32m[20230113 20:15:59 @agent_ppo2.py:144][0m Total time:      31.44 min
[32m[20230113 20:15:59 @agent_ppo2.py:146][0m 2902016 total steps have happened
[32m[20230113 20:15:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1417 --------------------------#
[32m[20230113 20:15:59 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:15:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:15:59 @agent_ppo2.py:186][0m |          -0.0060 |           6.1175 |           9.0252 |
[32m[20230113 20:15:59 @agent_ppo2.py:186][0m |          -0.0072 |           4.5786 |           9.0130 |
[32m[20230113 20:15:59 @agent_ppo2.py:186][0m |          -0.0065 |           4.0681 |           9.0071 |
[32m[20230113 20:15:59 @agent_ppo2.py:186][0m |          -0.0108 |           3.7618 |           9.0138 |
[32m[20230113 20:15:59 @agent_ppo2.py:186][0m |          -0.0181 |           3.5347 |           9.0123 |
[32m[20230113 20:16:00 @agent_ppo2.py:186][0m |          -0.0102 |           3.4054 |           9.0078 |
[32m[20230113 20:16:00 @agent_ppo2.py:186][0m |          -0.0196 |           3.2776 |           9.0033 |
[32m[20230113 20:16:00 @agent_ppo2.py:186][0m |          -0.0197 |           3.2371 |           9.0061 |
[32m[20230113 20:16:00 @agent_ppo2.py:186][0m |          -0.0156 |           3.0693 |           9.0125 |
[32m[20230113 20:16:00 @agent_ppo2.py:186][0m |          -0.0192 |           2.9786 |           9.0106 |
[32m[20230113 20:16:00 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.01
[32m[20230113 20:16:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.81
[32m[20230113 20:16:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.12
[32m[20230113 20:16:00 @agent_ppo2.py:144][0m Total time:      31.46 min
[32m[20230113 20:16:00 @agent_ppo2.py:146][0m 2904064 total steps have happened
[32m[20230113 20:16:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1418 --------------------------#
[32m[20230113 20:16:01 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:01 @agent_ppo2.py:186][0m |          -0.0024 |           5.7458 |           9.1057 |
[32m[20230113 20:16:01 @agent_ppo2.py:186][0m |          -0.0067 |           4.4356 |           9.0905 |
[32m[20230113 20:16:01 @agent_ppo2.py:186][0m |          -0.0105 |           3.9198 |           9.0919 |
[32m[20230113 20:16:01 @agent_ppo2.py:186][0m |          -0.0108 |           3.7014 |           9.0860 |
[32m[20230113 20:16:01 @agent_ppo2.py:186][0m |          -0.0148 |           3.4526 |           9.0936 |
[32m[20230113 20:16:01 @agent_ppo2.py:186][0m |          -0.0130 |           3.2914 |           9.0881 |
[32m[20230113 20:16:01 @agent_ppo2.py:186][0m |          -0.0134 |           3.2000 |           9.0859 |
[32m[20230113 20:16:01 @agent_ppo2.py:186][0m |          -0.0160 |           3.1218 |           9.0900 |
[32m[20230113 20:16:01 @agent_ppo2.py:186][0m |          -0.0153 |           3.0290 |           9.0842 |
[32m[20230113 20:16:01 @agent_ppo2.py:186][0m |          -0.0170 |           2.9909 |           9.0874 |
[32m[20230113 20:16:01 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.03
[32m[20230113 20:16:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.64
[32m[20230113 20:16:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.91
[32m[20230113 20:16:01 @agent_ppo2.py:144][0m Total time:      31.48 min
[32m[20230113 20:16:01 @agent_ppo2.py:146][0m 2906112 total steps have happened
[32m[20230113 20:16:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1419 --------------------------#
[32m[20230113 20:16:02 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:16:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:02 @agent_ppo2.py:186][0m |           0.0006 |           6.5098 |           9.3068 |
[32m[20230113 20:16:02 @agent_ppo2.py:186][0m |          -0.0041 |           5.2057 |           9.3014 |
[32m[20230113 20:16:02 @agent_ppo2.py:186][0m |          -0.0071 |           4.5966 |           9.2965 |
[32m[20230113 20:16:02 @agent_ppo2.py:186][0m |          -0.0092 |           4.4145 |           9.2888 |
[32m[20230113 20:16:02 @agent_ppo2.py:186][0m |          -0.0119 |           4.0925 |           9.2902 |
[32m[20230113 20:16:02 @agent_ppo2.py:186][0m |          -0.0145 |           3.9351 |           9.2859 |
[32m[20230113 20:16:02 @agent_ppo2.py:186][0m |          -0.0129 |           3.7564 |           9.2817 |
[32m[20230113 20:16:02 @agent_ppo2.py:186][0m |          -0.0138 |           3.6646 |           9.2779 |
[32m[20230113 20:16:02 @agent_ppo2.py:186][0m |          -0.0144 |           3.5313 |           9.2796 |
[32m[20230113 20:16:02 @agent_ppo2.py:186][0m |          -0.0146 |           3.4337 |           9.2791 |
[32m[20230113 20:16:02 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:16:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.65
[32m[20230113 20:16:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.37
[32m[20230113 20:16:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.62
[32m[20230113 20:16:03 @agent_ppo2.py:144][0m Total time:      31.50 min
[32m[20230113 20:16:03 @agent_ppo2.py:146][0m 2908160 total steps have happened
[32m[20230113 20:16:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1420 --------------------------#
[32m[20230113 20:16:03 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:03 @agent_ppo2.py:186][0m |          -0.0009 |           6.6534 |           9.3723 |
[32m[20230113 20:16:03 @agent_ppo2.py:186][0m |          -0.0087 |           4.8363 |           9.3539 |
[32m[20230113 20:16:03 @agent_ppo2.py:186][0m |          -0.0109 |           4.2555 |           9.3450 |
[32m[20230113 20:16:03 @agent_ppo2.py:186][0m |          -0.0136 |           3.8427 |           9.3442 |
[32m[20230113 20:16:03 @agent_ppo2.py:186][0m |          -0.0153 |           3.6087 |           9.3378 |
[32m[20230113 20:16:03 @agent_ppo2.py:186][0m |          -0.0149 |           3.3652 |           9.3332 |
[32m[20230113 20:16:03 @agent_ppo2.py:186][0m |          -0.0167 |           3.1763 |           9.3312 |
[32m[20230113 20:16:03 @agent_ppo2.py:186][0m |          -0.0165 |           3.0412 |           9.3263 |
[32m[20230113 20:16:03 @agent_ppo2.py:186][0m |          -0.0174 |           2.9861 |           9.3194 |
[32m[20230113 20:16:04 @agent_ppo2.py:186][0m |          -0.0182 |           2.7954 |           9.3240 |
[32m[20230113 20:16:04 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.37
[32m[20230113 20:16:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.13
[32m[20230113 20:16:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.42
[32m[20230113 20:16:04 @agent_ppo2.py:144][0m Total time:      31.52 min
[32m[20230113 20:16:04 @agent_ppo2.py:146][0m 2910208 total steps have happened
[32m[20230113 20:16:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1421 --------------------------#
[32m[20230113 20:16:04 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:04 @agent_ppo2.py:186][0m |           0.0051 |           5.7847 |           9.0002 |
[32m[20230113 20:16:04 @agent_ppo2.py:186][0m |          -0.0046 |           4.1331 |           8.9870 |
[32m[20230113 20:16:05 @agent_ppo2.py:186][0m |          -0.0090 |           3.6791 |           8.9879 |
[32m[20230113 20:16:05 @agent_ppo2.py:186][0m |          -0.0080 |           3.3492 |           8.9860 |
[32m[20230113 20:16:05 @agent_ppo2.py:186][0m |          -0.0129 |           3.1095 |           8.9765 |
[32m[20230113 20:16:05 @agent_ppo2.py:186][0m |          -0.0061 |           3.0441 |           8.9623 |
[32m[20230113 20:16:05 @agent_ppo2.py:186][0m |          -0.0113 |           2.8699 |           8.9778 |
[32m[20230113 20:16:05 @agent_ppo2.py:186][0m |          -0.0181 |           2.7491 |           8.9719 |
[32m[20230113 20:16:05 @agent_ppo2.py:186][0m |          -0.0200 |           2.6626 |           8.9751 |
[32m[20230113 20:16:05 @agent_ppo2.py:186][0m |          -0.0161 |           2.5935 |           8.9700 |
[32m[20230113 20:16:05 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.36
[32m[20230113 20:16:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.78
[32m[20230113 20:16:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.14
[32m[20230113 20:16:05 @agent_ppo2.py:144][0m Total time:      31.54 min
[32m[20230113 20:16:05 @agent_ppo2.py:146][0m 2912256 total steps have happened
[32m[20230113 20:16:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1422 --------------------------#
[32m[20230113 20:16:06 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:06 @agent_ppo2.py:186][0m |           0.0007 |           5.5962 |           9.1596 |
[32m[20230113 20:16:06 @agent_ppo2.py:186][0m |          -0.0039 |           4.4723 |           9.1478 |
[32m[20230113 20:16:06 @agent_ppo2.py:186][0m |          -0.0091 |           4.0812 |           9.1405 |
[32m[20230113 20:16:06 @agent_ppo2.py:186][0m |          -0.0110 |           3.6933 |           9.1300 |
[32m[20230113 20:16:06 @agent_ppo2.py:186][0m |          -0.0106 |           3.5686 |           9.1199 |
[32m[20230113 20:16:06 @agent_ppo2.py:186][0m |          -0.0137 |           3.3648 |           9.1177 |
[32m[20230113 20:16:06 @agent_ppo2.py:186][0m |          -0.0129 |           3.2684 |           9.1217 |
[32m[20230113 20:16:06 @agent_ppo2.py:186][0m |          -0.0135 |           3.1837 |           9.1210 |
[32m[20230113 20:16:06 @agent_ppo2.py:186][0m |          -0.0137 |           3.0969 |           9.1204 |
[32m[20230113 20:16:06 @agent_ppo2.py:186][0m |          -0.0137 |           3.0465 |           9.1196 |
[32m[20230113 20:16:06 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:16:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.69
[32m[20230113 20:16:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.23
[32m[20230113 20:16:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 118.45
[32m[20230113 20:16:06 @agent_ppo2.py:144][0m Total time:      31.56 min
[32m[20230113 20:16:06 @agent_ppo2.py:146][0m 2914304 total steps have happened
[32m[20230113 20:16:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1423 --------------------------#
[32m[20230113 20:16:07 @agent_ppo2.py:128][0m Sampling time: 0.31 s by 1 slaves
[32m[20230113 20:16:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:07 @agent_ppo2.py:186][0m |          -0.0061 |          13.9774 |           9.1344 |
[32m[20230113 20:16:07 @agent_ppo2.py:186][0m |          -0.0007 |           6.8613 |           9.1226 |
[32m[20230113 20:16:07 @agent_ppo2.py:186][0m |          -0.0183 |           5.7241 |           9.1277 |
[32m[20230113 20:16:07 @agent_ppo2.py:186][0m |          -0.0125 |           5.0657 |           9.1223 |
[32m[20230113 20:16:07 @agent_ppo2.py:186][0m |          -0.0149 |           4.5332 |           9.1229 |
[32m[20230113 20:16:07 @agent_ppo2.py:186][0m |          -0.0208 |           4.2897 |           9.1279 |
[32m[20230113 20:16:07 @agent_ppo2.py:186][0m |          -0.0182 |           3.9906 |           9.1319 |
[32m[20230113 20:16:07 @agent_ppo2.py:186][0m |          -0.0171 |           3.8729 |           9.1275 |
[32m[20230113 20:16:07 @agent_ppo2.py:186][0m |          -0.0244 |           3.5887 |           9.1368 |
[32m[20230113 20:16:07 @agent_ppo2.py:186][0m |          -0.0139 |           3.5042 |           9.1365 |
[32m[20230113 20:16:07 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 20:16:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 103.18
[32m[20230113 20:16:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.20
[32m[20230113 20:16:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.41
[32m[20230113 20:16:07 @agent_ppo2.py:144][0m Total time:      31.58 min
[32m[20230113 20:16:07 @agent_ppo2.py:146][0m 2916352 total steps have happened
[32m[20230113 20:16:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1424 --------------------------#
[32m[20230113 20:16:08 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:16:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:08 @agent_ppo2.py:186][0m |          -0.0017 |           5.6086 |           9.1954 |
[32m[20230113 20:16:08 @agent_ppo2.py:186][0m |          -0.0063 |           4.7624 |           9.1768 |
[32m[20230113 20:16:08 @agent_ppo2.py:186][0m |          -0.0099 |           4.2812 |           9.1900 |
[32m[20230113 20:16:08 @agent_ppo2.py:186][0m |          -0.0098 |           4.0916 |           9.1879 |
[32m[20230113 20:16:08 @agent_ppo2.py:186][0m |          -0.0139 |           3.9290 |           9.1805 |
[32m[20230113 20:16:08 @agent_ppo2.py:186][0m |          -0.0149 |           3.7869 |           9.1788 |
[32m[20230113 20:16:08 @agent_ppo2.py:186][0m |          -0.0161 |           3.7268 |           9.1795 |
[32m[20230113 20:16:08 @agent_ppo2.py:186][0m |          -0.0161 |           3.6662 |           9.1754 |
[32m[20230113 20:16:08 @agent_ppo2.py:186][0m |          -0.0172 |           3.5501 |           9.1791 |
[32m[20230113 20:16:08 @agent_ppo2.py:186][0m |          -0.0189 |           3.4956 |           9.1720 |
[32m[20230113 20:16:08 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:16:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.68
[32m[20230113 20:16:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.54
[32m[20230113 20:16:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 142.14
[32m[20230113 20:16:09 @agent_ppo2.py:144][0m Total time:      31.60 min
[32m[20230113 20:16:09 @agent_ppo2.py:146][0m 2918400 total steps have happened
[32m[20230113 20:16:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1425 --------------------------#
[32m[20230113 20:16:09 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:09 @agent_ppo2.py:186][0m |           0.0014 |           5.9528 |           9.4360 |
[32m[20230113 20:16:09 @agent_ppo2.py:186][0m |          -0.0047 |           4.8839 |           9.4334 |
[32m[20230113 20:16:09 @agent_ppo2.py:186][0m |          -0.0062 |           4.4893 |           9.4243 |
[32m[20230113 20:16:09 @agent_ppo2.py:186][0m |          -0.0080 |           4.2161 |           9.4215 |
[32m[20230113 20:16:09 @agent_ppo2.py:186][0m |          -0.0108 |           4.0846 |           9.4248 |
[32m[20230113 20:16:09 @agent_ppo2.py:186][0m |          -0.0094 |           3.9334 |           9.4187 |
[32m[20230113 20:16:09 @agent_ppo2.py:186][0m |          -0.0112 |           3.8378 |           9.4177 |
[32m[20230113 20:16:09 @agent_ppo2.py:186][0m |          -0.0116 |           3.6998 |           9.4203 |
[32m[20230113 20:16:10 @agent_ppo2.py:186][0m |          -0.0131 |           3.6698 |           9.4137 |
[32m[20230113 20:16:10 @agent_ppo2.py:186][0m |          -0.0132 |           3.5858 |           9.4223 |
[32m[20230113 20:16:10 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.36
[32m[20230113 20:16:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.76
[32m[20230113 20:16:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.79
[32m[20230113 20:16:10 @agent_ppo2.py:144][0m Total time:      31.62 min
[32m[20230113 20:16:10 @agent_ppo2.py:146][0m 2920448 total steps have happened
[32m[20230113 20:16:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1426 --------------------------#
[32m[20230113 20:16:10 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:10 @agent_ppo2.py:186][0m |           0.0018 |           7.4554 |           9.0733 |
[32m[20230113 20:16:11 @agent_ppo2.py:186][0m |          -0.0049 |           5.5864 |           9.0722 |
[32m[20230113 20:16:11 @agent_ppo2.py:186][0m |          -0.0078 |           5.1095 |           9.0555 |
[32m[20230113 20:16:11 @agent_ppo2.py:186][0m |          -0.0092 |           4.8198 |           9.0512 |
[32m[20230113 20:16:11 @agent_ppo2.py:186][0m |          -0.0097 |           4.6124 |           9.0373 |
[32m[20230113 20:16:11 @agent_ppo2.py:186][0m |          -0.0106 |           4.4594 |           9.0409 |
[32m[20230113 20:16:11 @agent_ppo2.py:186][0m |          -0.0116 |           4.2577 |           9.0350 |
[32m[20230113 20:16:11 @agent_ppo2.py:186][0m |          -0.0112 |           4.1871 |           9.0356 |
[32m[20230113 20:16:11 @agent_ppo2.py:186][0m |          -0.0111 |           4.1836 |           9.0308 |
[32m[20230113 20:16:11 @agent_ppo2.py:186][0m |          -0.0142 |           3.9645 |           9.0353 |
[32m[20230113 20:16:11 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.24
[32m[20230113 20:16:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.10
[32m[20230113 20:16:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.45
[32m[20230113 20:16:11 @agent_ppo2.py:144][0m Total time:      31.64 min
[32m[20230113 20:16:11 @agent_ppo2.py:146][0m 2922496 total steps have happened
[32m[20230113 20:16:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1427 --------------------------#
[32m[20230113 20:16:12 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:16:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:12 @agent_ppo2.py:186][0m |           0.0019 |           6.4790 |           9.1186 |
[32m[20230113 20:16:12 @agent_ppo2.py:186][0m |          -0.0035 |           5.2770 |           9.1112 |
[32m[20230113 20:16:12 @agent_ppo2.py:186][0m |          -0.0049 |           4.9743 |           9.1022 |
[32m[20230113 20:16:12 @agent_ppo2.py:186][0m |          -0.0079 |           4.6993 |           9.1016 |
[32m[20230113 20:16:12 @agent_ppo2.py:186][0m |          -0.0104 |           4.5085 |           9.1030 |
[32m[20230113 20:16:12 @agent_ppo2.py:186][0m |          -0.0106 |           4.3557 |           9.0967 |
[32m[20230113 20:16:12 @agent_ppo2.py:186][0m |          -0.0116 |           4.2803 |           9.0968 |
[32m[20230113 20:16:12 @agent_ppo2.py:186][0m |          -0.0123 |           4.1547 |           9.0998 |
[32m[20230113 20:16:12 @agent_ppo2.py:186][0m |          -0.0121 |           4.0700 |           9.0907 |
[32m[20230113 20:16:12 @agent_ppo2.py:186][0m |          -0.0119 |           4.0375 |           9.0992 |
[32m[20230113 20:16:12 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.01
[32m[20230113 20:16:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.52
[32m[20230113 20:16:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 136.34
[32m[20230113 20:16:12 @agent_ppo2.py:144][0m Total time:      31.66 min
[32m[20230113 20:16:12 @agent_ppo2.py:146][0m 2924544 total steps have happened
[32m[20230113 20:16:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1428 --------------------------#
[32m[20230113 20:16:13 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:13 @agent_ppo2.py:186][0m |          -0.0013 |           7.3049 |           9.3204 |
[32m[20230113 20:16:13 @agent_ppo2.py:186][0m |          -0.0059 |           5.1009 |           9.3215 |
[32m[20230113 20:16:13 @agent_ppo2.py:186][0m |          -0.0124 |           4.5070 |           9.3092 |
[32m[20230113 20:16:13 @agent_ppo2.py:186][0m |          -0.0136 |           4.1529 |           9.2989 |
[32m[20230113 20:16:13 @agent_ppo2.py:186][0m |          -0.0147 |           3.8695 |           9.2976 |
[32m[20230113 20:16:13 @agent_ppo2.py:186][0m |          -0.0164 |           3.7299 |           9.2952 |
[32m[20230113 20:16:13 @agent_ppo2.py:186][0m |          -0.0152 |           3.5271 |           9.2895 |
[32m[20230113 20:16:13 @agent_ppo2.py:186][0m |          -0.0137 |           3.4554 |           9.2873 |
[32m[20230113 20:16:13 @agent_ppo2.py:186][0m |          -0.0190 |           3.2898 |           9.2890 |
[32m[20230113 20:16:13 @agent_ppo2.py:186][0m |          -0.0174 |           3.2196 |           9.2886 |
[32m[20230113 20:16:13 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:16:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.44
[32m[20230113 20:16:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.28
[32m[20230113 20:16:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 130.63
[32m[20230113 20:16:14 @agent_ppo2.py:144][0m Total time:      31.68 min
[32m[20230113 20:16:14 @agent_ppo2.py:146][0m 2926592 total steps have happened
[32m[20230113 20:16:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1429 --------------------------#
[32m[20230113 20:16:14 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:14 @agent_ppo2.py:186][0m |          -0.0007 |           6.2155 |           9.3845 |
[32m[20230113 20:16:14 @agent_ppo2.py:186][0m |          -0.0049 |           4.5958 |           9.3713 |
[32m[20230113 20:16:14 @agent_ppo2.py:186][0m |          -0.0089 |           3.9583 |           9.3859 |
[32m[20230113 20:16:14 @agent_ppo2.py:186][0m |          -0.0102 |           3.5445 |           9.3782 |
[32m[20230113 20:16:14 @agent_ppo2.py:186][0m |          -0.0110 |           3.3034 |           9.3844 |
[32m[20230113 20:16:14 @agent_ppo2.py:186][0m |          -0.0137 |           3.1147 |           9.3818 |
[32m[20230113 20:16:14 @agent_ppo2.py:186][0m |          -0.0120 |           2.9397 |           9.3735 |
[32m[20230113 20:16:14 @agent_ppo2.py:186][0m |          -0.0149 |           2.8260 |           9.3825 |
[32m[20230113 20:16:14 @agent_ppo2.py:186][0m |          -0.0162 |           2.7464 |           9.3819 |
[32m[20230113 20:16:14 @agent_ppo2.py:186][0m |          -0.0159 |           2.6415 |           9.3864 |
[32m[20230113 20:16:14 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.42
[32m[20230113 20:16:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.72
[32m[20230113 20:16:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 143.30
[32m[20230113 20:16:15 @agent_ppo2.py:144][0m Total time:      31.70 min
[32m[20230113 20:16:15 @agent_ppo2.py:146][0m 2928640 total steps have happened
[32m[20230113 20:16:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1430 --------------------------#
[32m[20230113 20:16:15 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:16:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:15 @agent_ppo2.py:186][0m |          -0.0024 |          22.5212 |           9.1638 |
[32m[20230113 20:16:15 @agent_ppo2.py:186][0m |          -0.0060 |          10.8861 |           9.1541 |
[32m[20230113 20:16:15 @agent_ppo2.py:186][0m |          -0.0121 |           8.6001 |           9.1496 |
[32m[20230113 20:16:15 @agent_ppo2.py:186][0m |          -0.0131 |           7.9099 |           9.1395 |
[32m[20230113 20:16:15 @agent_ppo2.py:186][0m |          -0.0131 |           7.0565 |           9.1319 |
[32m[20230113 20:16:16 @agent_ppo2.py:186][0m |          -0.0157 |           6.5156 |           9.1350 |
[32m[20230113 20:16:16 @agent_ppo2.py:186][0m |          -0.0152 |           5.9732 |           9.1316 |
[32m[20230113 20:16:16 @agent_ppo2.py:186][0m |          -0.0074 |           5.6950 |           9.1347 |
[32m[20230113 20:16:16 @agent_ppo2.py:186][0m |          -0.0127 |           5.3340 |           9.1336 |
[32m[20230113 20:16:16 @agent_ppo2.py:186][0m |          -0.0166 |           5.0364 |           9.1397 |
[32m[20230113 20:16:16 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:16:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 173.98
[32m[20230113 20:16:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.66
[32m[20230113 20:16:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.86
[32m[20230113 20:16:16 @agent_ppo2.py:144][0m Total time:      31.73 min
[32m[20230113 20:16:16 @agent_ppo2.py:146][0m 2930688 total steps have happened
[32m[20230113 20:16:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1431 --------------------------#
[32m[20230113 20:16:17 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:17 @agent_ppo2.py:186][0m |           0.0018 |           6.7316 |           9.3456 |
[32m[20230113 20:16:17 @agent_ppo2.py:186][0m |          -0.0105 |           5.1665 |           9.3527 |
[32m[20230113 20:16:17 @agent_ppo2.py:186][0m |          -0.0072 |           4.5102 |           9.3509 |
[32m[20230113 20:16:17 @agent_ppo2.py:186][0m |          -0.0093 |           4.1209 |           9.3486 |
[32m[20230113 20:16:17 @agent_ppo2.py:186][0m |          -0.0089 |           3.8721 |           9.3529 |
[32m[20230113 20:16:17 @agent_ppo2.py:186][0m |          -0.0132 |           3.7075 |           9.3507 |
[32m[20230113 20:16:17 @agent_ppo2.py:186][0m |          -0.0132 |           3.6027 |           9.3530 |
[32m[20230113 20:16:17 @agent_ppo2.py:186][0m |          -0.0130 |           3.4540 |           9.3455 |
[32m[20230113 20:16:17 @agent_ppo2.py:186][0m |          -0.0155 |           3.3522 |           9.3432 |
[32m[20230113 20:16:17 @agent_ppo2.py:186][0m |          -0.0146 |           3.2618 |           9.3500 |
[32m[20230113 20:16:17 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.05
[32m[20230113 20:16:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.66
[32m[20230113 20:16:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.21
[32m[20230113 20:16:17 @agent_ppo2.py:144][0m Total time:      31.75 min
[32m[20230113 20:16:17 @agent_ppo2.py:146][0m 2932736 total steps have happened
[32m[20230113 20:16:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1432 --------------------------#
[32m[20230113 20:16:18 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:16:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:18 @agent_ppo2.py:186][0m |          -0.0016 |           7.7996 |           9.3861 |
[32m[20230113 20:16:18 @agent_ppo2.py:186][0m |          -0.0106 |           5.9214 |           9.3681 |
[32m[20230113 20:16:18 @agent_ppo2.py:186][0m |          -0.0096 |           5.3345 |           9.3547 |
[32m[20230113 20:16:18 @agent_ppo2.py:186][0m |          -0.0096 |           5.0128 |           9.3453 |
[32m[20230113 20:16:18 @agent_ppo2.py:186][0m |          -0.0160 |           4.7233 |           9.3470 |
[32m[20230113 20:16:18 @agent_ppo2.py:186][0m |          -0.0118 |           4.5521 |           9.3451 |
[32m[20230113 20:16:18 @agent_ppo2.py:186][0m |          -0.0204 |           4.3943 |           9.3364 |
[32m[20230113 20:16:18 @agent_ppo2.py:186][0m |          -0.0163 |           4.2222 |           9.3443 |
[32m[20230113 20:16:18 @agent_ppo2.py:186][0m |          -0.0170 |           4.1080 |           9.3447 |
[32m[20230113 20:16:18 @agent_ppo2.py:186][0m |          -0.0158 |           3.9544 |           9.3440 |
[32m[20230113 20:16:18 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.39
[32m[20230113 20:16:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.49
[32m[20230113 20:16:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.57
[32m[20230113 20:16:19 @agent_ppo2.py:144][0m Total time:      31.77 min
[32m[20230113 20:16:19 @agent_ppo2.py:146][0m 2934784 total steps have happened
[32m[20230113 20:16:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1433 --------------------------#
[32m[20230113 20:16:19 @agent_ppo2.py:128][0m Sampling time: 0.31 s by 1 slaves
[32m[20230113 20:16:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:19 @agent_ppo2.py:186][0m |          -0.0036 |          28.5472 |           9.2443 |
[32m[20230113 20:16:19 @agent_ppo2.py:186][0m |          -0.0104 |          15.4848 |           9.2362 |
[32m[20230113 20:16:19 @agent_ppo2.py:186][0m |          -0.0124 |          10.3738 |           9.2374 |
[32m[20230113 20:16:19 @agent_ppo2.py:186][0m |          -0.0117 |           8.6451 |           9.2336 |
[32m[20230113 20:16:19 @agent_ppo2.py:186][0m |          -0.0146 |           7.3854 |           9.2370 |
[32m[20230113 20:16:19 @agent_ppo2.py:186][0m |          -0.0170 |           6.7140 |           9.2356 |
[32m[20230113 20:16:19 @agent_ppo2.py:186][0m |          -0.0167 |           6.2261 |           9.2307 |
[32m[20230113 20:16:19 @agent_ppo2.py:186][0m |          -0.0191 |           5.6956 |           9.2337 |
[32m[20230113 20:16:19 @agent_ppo2.py:186][0m |          -0.0191 |           5.3792 |           9.2297 |
[32m[20230113 20:16:19 @agent_ppo2.py:186][0m |          -0.0204 |           5.1244 |           9.2309 |
[32m[20230113 20:16:19 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 20:16:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 114.65
[32m[20230113 20:16:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.87
[32m[20230113 20:16:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.87
[32m[20230113 20:16:20 @agent_ppo2.py:144][0m Total time:      31.78 min
[32m[20230113 20:16:20 @agent_ppo2.py:146][0m 2936832 total steps have happened
[32m[20230113 20:16:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1434 --------------------------#
[32m[20230113 20:16:20 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 20:16:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:20 @agent_ppo2.py:186][0m |          -0.0039 |          18.4847 |           9.2169 |
[32m[20230113 20:16:20 @agent_ppo2.py:186][0m |          -0.0078 |          11.9493 |           9.1998 |
[32m[20230113 20:16:20 @agent_ppo2.py:186][0m |          -0.0064 |          10.1129 |           9.1994 |
[32m[20230113 20:16:20 @agent_ppo2.py:186][0m |          -0.0089 |           8.6416 |           9.2037 |
[32m[20230113 20:16:20 @agent_ppo2.py:186][0m |          -0.0151 |           7.9924 |           9.2024 |
[32m[20230113 20:16:20 @agent_ppo2.py:186][0m |          -0.0177 |           7.4500 |           9.2037 |
[32m[20230113 20:16:20 @agent_ppo2.py:186][0m |          -0.0155 |           6.8950 |           9.2020 |
[32m[20230113 20:16:21 @agent_ppo2.py:186][0m |          -0.0152 |           6.7435 |           9.2040 |
[32m[20230113 20:16:21 @agent_ppo2.py:186][0m |          -0.0203 |           6.1155 |           9.2017 |
[32m[20230113 20:16:21 @agent_ppo2.py:186][0m |          -0.0192 |           5.8534 |           9.1981 |
[32m[20230113 20:16:21 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 20:16:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 139.14
[32m[20230113 20:16:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.79
[32m[20230113 20:16:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.82
[32m[20230113 20:16:21 @agent_ppo2.py:144][0m Total time:      31.81 min
[32m[20230113 20:16:21 @agent_ppo2.py:146][0m 2938880 total steps have happened
[32m[20230113 20:16:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1435 --------------------------#
[32m[20230113 20:16:21 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:16:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:22 @agent_ppo2.py:186][0m |           0.0005 |           7.3583 |           9.2362 |
[32m[20230113 20:16:22 @agent_ppo2.py:186][0m |          -0.0061 |           5.5714 |           9.2234 |
[32m[20230113 20:16:22 @agent_ppo2.py:186][0m |          -0.0077 |           4.9642 |           9.2218 |
[32m[20230113 20:16:22 @agent_ppo2.py:186][0m |          -0.0097 |           4.6124 |           9.2054 |
[32m[20230113 20:16:22 @agent_ppo2.py:186][0m |          -0.0112 |           4.3629 |           9.2176 |
[32m[20230113 20:16:22 @agent_ppo2.py:186][0m |          -0.0135 |           4.1049 |           9.2055 |
[32m[20230113 20:16:22 @agent_ppo2.py:186][0m |          -0.0136 |           3.9666 |           9.2051 |
[32m[20230113 20:16:22 @agent_ppo2.py:186][0m |          -0.0139 |           3.8113 |           9.2026 |
[32m[20230113 20:16:22 @agent_ppo2.py:186][0m |          -0.0150 |           3.6828 |           9.1951 |
[32m[20230113 20:16:22 @agent_ppo2.py:186][0m |          -0.0157 |           3.5718 |           9.2032 |
[32m[20230113 20:16:22 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.09
[32m[20230113 20:16:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.94
[32m[20230113 20:16:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 140.45
[32m[20230113 20:16:22 @agent_ppo2.py:144][0m Total time:      31.83 min
[32m[20230113 20:16:22 @agent_ppo2.py:146][0m 2940928 total steps have happened
[32m[20230113 20:16:22 @agent_ppo2.py:122][0m #------------------------ Iteration 1436 --------------------------#
[32m[20230113 20:16:23 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:23 @agent_ppo2.py:186][0m |          -0.0011 |           6.5347 |           9.2110 |
[32m[20230113 20:16:23 @agent_ppo2.py:186][0m |          -0.0097 |           4.8353 |           9.2160 |
[32m[20230113 20:16:23 @agent_ppo2.py:186][0m |          -0.0102 |           4.3501 |           9.2111 |
[32m[20230113 20:16:23 @agent_ppo2.py:186][0m |          -0.0173 |           4.0812 |           9.2117 |
[32m[20230113 20:16:23 @agent_ppo2.py:186][0m |          -0.0160 |           3.8346 |           9.2061 |
[32m[20230113 20:16:23 @agent_ppo2.py:186][0m |          -0.0154 |           3.6941 |           9.1980 |
[32m[20230113 20:16:23 @agent_ppo2.py:186][0m |          -0.0168 |           3.5389 |           9.1974 |
[32m[20230113 20:16:23 @agent_ppo2.py:186][0m |          -0.0112 |           3.4608 |           9.1950 |
[32m[20230113 20:16:23 @agent_ppo2.py:186][0m |          -0.0133 |           3.3590 |           9.1940 |
[32m[20230113 20:16:23 @agent_ppo2.py:186][0m |          -0.0185 |           3.2687 |           9.1904 |
[32m[20230113 20:16:23 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.77
[32m[20230113 20:16:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.06
[32m[20230113 20:16:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.25
[32m[20230113 20:16:24 @agent_ppo2.py:144][0m Total time:      31.85 min
[32m[20230113 20:16:24 @agent_ppo2.py:146][0m 2942976 total steps have happened
[32m[20230113 20:16:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1437 --------------------------#
[32m[20230113 20:16:24 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:24 @agent_ppo2.py:186][0m |           0.0021 |           7.3325 |           9.1011 |
[32m[20230113 20:16:24 @agent_ppo2.py:186][0m |          -0.0035 |           6.0967 |           9.0905 |
[32m[20230113 20:16:24 @agent_ppo2.py:186][0m |          -0.0052 |           5.5543 |           9.0847 |
[32m[20230113 20:16:24 @agent_ppo2.py:186][0m |          -0.0052 |           5.3763 |           9.0849 |
[32m[20230113 20:16:24 @agent_ppo2.py:186][0m |          -0.0067 |           4.9801 |           9.0896 |
[32m[20230113 20:16:24 @agent_ppo2.py:186][0m |          -0.0108 |           4.8360 |           9.0959 |
[32m[20230113 20:16:24 @agent_ppo2.py:186][0m |          -0.0130 |           4.6660 |           9.0938 |
[32m[20230113 20:16:24 @agent_ppo2.py:186][0m |          -0.0118 |           4.5200 |           9.0996 |
[32m[20230113 20:16:24 @agent_ppo2.py:186][0m |          -0.0105 |           4.4091 |           9.1086 |
[32m[20230113 20:16:24 @agent_ppo2.py:186][0m |          -0.0112 |           4.2857 |           9.1054 |
[32m[20230113 20:16:24 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.79
[32m[20230113 20:16:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.23
[32m[20230113 20:16:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.31
[32m[20230113 20:16:25 @agent_ppo2.py:144][0m Total time:      31.87 min
[32m[20230113 20:16:25 @agent_ppo2.py:146][0m 2945024 total steps have happened
[32m[20230113 20:16:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1438 --------------------------#
[32m[20230113 20:16:25 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:16:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:25 @agent_ppo2.py:186][0m |           0.0002 |           6.5420 |           9.3606 |
[32m[20230113 20:16:25 @agent_ppo2.py:186][0m |          -0.0059 |           5.0311 |           9.3546 |
[32m[20230113 20:16:25 @agent_ppo2.py:186][0m |          -0.0110 |           4.3105 |           9.3440 |
[32m[20230113 20:16:25 @agent_ppo2.py:186][0m |          -0.0123 |           3.9279 |           9.3354 |
[32m[20230113 20:16:25 @agent_ppo2.py:186][0m |          -0.0126 |           3.6670 |           9.3307 |
[32m[20230113 20:16:25 @agent_ppo2.py:186][0m |          -0.0145 |           3.4689 |           9.3389 |
[32m[20230113 20:16:26 @agent_ppo2.py:186][0m |          -0.0159 |           3.2922 |           9.3463 |
[32m[20230113 20:16:26 @agent_ppo2.py:186][0m |          -0.0168 |           3.1802 |           9.3442 |
[32m[20230113 20:16:26 @agent_ppo2.py:186][0m |          -0.0147 |           3.0816 |           9.3431 |
[32m[20230113 20:16:26 @agent_ppo2.py:186][0m |          -0.0175 |           2.9807 |           9.3396 |
[32m[20230113 20:16:26 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.30
[32m[20230113 20:16:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.09
[32m[20230113 20:16:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.76
[32m[20230113 20:16:26 @agent_ppo2.py:144][0m Total time:      31.89 min
[32m[20230113 20:16:26 @agent_ppo2.py:146][0m 2947072 total steps have happened
[32m[20230113 20:16:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1439 --------------------------#
[32m[20230113 20:16:26 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:27 @agent_ppo2.py:186][0m |           0.0059 |           6.8177 |           9.2310 |
[32m[20230113 20:16:27 @agent_ppo2.py:186][0m |          -0.0065 |           5.0950 |           9.2222 |
[32m[20230113 20:16:27 @agent_ppo2.py:186][0m |          -0.0083 |           4.3471 |           9.2216 |
[32m[20230113 20:16:27 @agent_ppo2.py:186][0m |          -0.0050 |           4.0401 |           9.2163 |
[32m[20230113 20:16:27 @agent_ppo2.py:186][0m |          -0.0121 |           3.6918 |           9.2127 |
[32m[20230113 20:16:27 @agent_ppo2.py:186][0m |          -0.0084 |           3.4794 |           9.2102 |
[32m[20230113 20:16:27 @agent_ppo2.py:186][0m |          -0.0140 |           3.3389 |           9.2092 |
[32m[20230113 20:16:27 @agent_ppo2.py:186][0m |          -0.0090 |           3.2246 |           9.2130 |
[32m[20230113 20:16:27 @agent_ppo2.py:186][0m |          -0.0131 |           3.1037 |           9.2102 |
[32m[20230113 20:16:27 @agent_ppo2.py:186][0m |          -0.0151 |           3.0100 |           9.2064 |
[32m[20230113 20:16:27 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.99
[32m[20230113 20:16:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.38
[32m[20230113 20:16:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.71
[32m[20230113 20:16:27 @agent_ppo2.py:144][0m Total time:      31.91 min
[32m[20230113 20:16:27 @agent_ppo2.py:146][0m 2949120 total steps have happened
[32m[20230113 20:16:27 @agent_ppo2.py:122][0m #------------------------ Iteration 1440 --------------------------#
[32m[20230113 20:16:28 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:16:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:28 @agent_ppo2.py:186][0m |          -0.0013 |           6.9804 |           9.5345 |
[32m[20230113 20:16:28 @agent_ppo2.py:186][0m |          -0.0076 |           4.7121 |           9.5464 |
[32m[20230113 20:16:28 @agent_ppo2.py:186][0m |          -0.0104 |           4.1634 |           9.5271 |
[32m[20230113 20:16:28 @agent_ppo2.py:186][0m |          -0.0088 |           3.7964 |           9.5242 |
[32m[20230113 20:16:28 @agent_ppo2.py:186][0m |          -0.0137 |           3.5900 |           9.5195 |
[32m[20230113 20:16:28 @agent_ppo2.py:186][0m |          -0.0119 |           3.4041 |           9.5142 |
[32m[20230113 20:16:28 @agent_ppo2.py:186][0m |          -0.0132 |           3.2336 |           9.5188 |
[32m[20230113 20:16:28 @agent_ppo2.py:186][0m |          -0.0108 |           3.1152 |           9.5153 |
[32m[20230113 20:16:28 @agent_ppo2.py:186][0m |          -0.0161 |           3.0019 |           9.5095 |
[32m[20230113 20:16:28 @agent_ppo2.py:186][0m |          -0.0143 |           2.9242 |           9.5105 |
[32m[20230113 20:16:28 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:16:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.30
[32m[20230113 20:16:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.25
[32m[20230113 20:16:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.67
[32m[20230113 20:16:29 @agent_ppo2.py:144][0m Total time:      31.93 min
[32m[20230113 20:16:29 @agent_ppo2.py:146][0m 2951168 total steps have happened
[32m[20230113 20:16:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1441 --------------------------#
[32m[20230113 20:16:29 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:29 @agent_ppo2.py:186][0m |          -0.0206 |           6.3563 |           9.3108 |
[32m[20230113 20:16:29 @agent_ppo2.py:186][0m |           0.0030 |           4.7072 |           9.2945 |
[32m[20230113 20:16:29 @agent_ppo2.py:186][0m |          -0.0183 |           4.1120 |           9.2763 |
[32m[20230113 20:16:29 @agent_ppo2.py:186][0m |          -0.0259 |           4.3149 |           9.2757 |
[32m[20230113 20:16:29 @agent_ppo2.py:186][0m |           0.0148 |           3.9616 |           9.2441 |
[32m[20230113 20:16:29 @agent_ppo2.py:186][0m |          -0.0069 |           3.5446 |           9.2761 |
[32m[20230113 20:16:29 @agent_ppo2.py:186][0m |          -0.0108 |           3.3687 |           9.2785 |
[32m[20230113 20:16:29 @agent_ppo2.py:186][0m |          -0.0217 |           3.2378 |           9.2677 |
[32m[20230113 20:16:29 @agent_ppo2.py:186][0m |          -0.0015 |           3.1774 |           9.2604 |
[32m[20230113 20:16:29 @agent_ppo2.py:186][0m |          -0.0256 |           3.2395 |           9.2536 |
[32m[20230113 20:16:29 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.83
[32m[20230113 20:16:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.02
[32m[20230113 20:16:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 154.73
[32m[20230113 20:16:30 @agent_ppo2.py:144][0m Total time:      31.95 min
[32m[20230113 20:16:30 @agent_ppo2.py:146][0m 2953216 total steps have happened
[32m[20230113 20:16:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1442 --------------------------#
[32m[20230113 20:16:30 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:30 @agent_ppo2.py:186][0m |           0.0153 |           6.6156 |           9.3002 |
[32m[20230113 20:16:30 @agent_ppo2.py:186][0m |          -0.0070 |           5.0461 |           9.2821 |
[32m[20230113 20:16:30 @agent_ppo2.py:186][0m |          -0.0139 |           4.5583 |           9.2718 |
[32m[20230113 20:16:30 @agent_ppo2.py:186][0m |          -0.0140 |           4.3029 |           9.2735 |
[32m[20230113 20:16:30 @agent_ppo2.py:186][0m |           0.0034 |           4.2389 |           9.2709 |
[32m[20230113 20:16:30 @agent_ppo2.py:186][0m |          -0.0254 |           3.9191 |           9.2720 |
[32m[20230113 20:16:31 @agent_ppo2.py:186][0m |          -0.0031 |           3.8462 |           9.2626 |
[32m[20230113 20:16:31 @agent_ppo2.py:186][0m |          -0.0188 |           3.7399 |           9.2665 |
[32m[20230113 20:16:31 @agent_ppo2.py:186][0m |          -0.0172 |           3.5873 |           9.2663 |
[32m[20230113 20:16:31 @agent_ppo2.py:186][0m |          -0.0145 |           3.5146 |           9.2650 |
[32m[20230113 20:16:31 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.84
[32m[20230113 20:16:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.35
[32m[20230113 20:16:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 232.24
[32m[20230113 20:16:31 @agent_ppo2.py:144][0m Total time:      31.97 min
[32m[20230113 20:16:31 @agent_ppo2.py:146][0m 2955264 total steps have happened
[32m[20230113 20:16:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1443 --------------------------#
[32m[20230113 20:16:31 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:16:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:31 @agent_ppo2.py:186][0m |           0.0009 |          47.4437 |           9.3973 |
[32m[20230113 20:16:31 @agent_ppo2.py:186][0m |          -0.0007 |          20.4474 |           9.3989 |
[32m[20230113 20:16:31 @agent_ppo2.py:186][0m |          -0.0058 |          15.2696 |           9.3928 |
[32m[20230113 20:16:32 @agent_ppo2.py:186][0m |          -0.0082 |          13.3469 |           9.4063 |
[32m[20230113 20:16:32 @agent_ppo2.py:186][0m |          -0.0088 |          11.7070 |           9.4043 |
[32m[20230113 20:16:32 @agent_ppo2.py:186][0m |          -0.0099 |          10.7683 |           9.3998 |
[32m[20230113 20:16:32 @agent_ppo2.py:186][0m |          -0.0091 |           9.9583 |           9.3970 |
[32m[20230113 20:16:32 @agent_ppo2.py:186][0m |          -0.0122 |           9.2229 |           9.3940 |
[32m[20230113 20:16:32 @agent_ppo2.py:186][0m |          -0.0129 |           8.8497 |           9.3974 |
[32m[20230113 20:16:32 @agent_ppo2.py:186][0m |          -0.0109 |           8.3980 |           9.4046 |
[32m[20230113 20:16:32 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:16:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: -1.33
[32m[20230113 20:16:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 85.15
[32m[20230113 20:16:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 63.27
[32m[20230113 20:16:32 @agent_ppo2.py:144][0m Total time:      31.99 min
[32m[20230113 20:16:32 @agent_ppo2.py:146][0m 2957312 total steps have happened
[32m[20230113 20:16:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1444 --------------------------#
[32m[20230113 20:16:32 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:16:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:33 @agent_ppo2.py:186][0m |           0.0007 |           9.5514 |           9.1836 |
[32m[20230113 20:16:33 @agent_ppo2.py:186][0m |          -0.0054 |           6.8815 |           9.1734 |
[32m[20230113 20:16:33 @agent_ppo2.py:186][0m |          -0.0084 |           5.9258 |           9.1707 |
[32m[20230113 20:16:33 @agent_ppo2.py:186][0m |          -0.0111 |           5.3886 |           9.1756 |
[32m[20230113 20:16:33 @agent_ppo2.py:186][0m |          -0.0109 |           5.0331 |           9.1711 |
[32m[20230113 20:16:33 @agent_ppo2.py:186][0m |          -0.0118 |           4.7166 |           9.1762 |
[32m[20230113 20:16:33 @agent_ppo2.py:186][0m |          -0.0134 |           4.5281 |           9.1661 |
[32m[20230113 20:16:33 @agent_ppo2.py:186][0m |          -0.0131 |           4.3415 |           9.1765 |
[32m[20230113 20:16:33 @agent_ppo2.py:186][0m |          -0.0135 |           4.2120 |           9.1737 |
[32m[20230113 20:16:33 @agent_ppo2.py:186][0m |          -0.0145 |           4.0890 |           9.1755 |
[32m[20230113 20:16:33 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.69
[32m[20230113 20:16:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.32
[32m[20230113 20:16:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.50
[32m[20230113 20:16:33 @agent_ppo2.py:144][0m Total time:      32.01 min
[32m[20230113 20:16:33 @agent_ppo2.py:146][0m 2959360 total steps have happened
[32m[20230113 20:16:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1445 --------------------------#
[32m[20230113 20:16:34 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:16:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:34 @agent_ppo2.py:186][0m |          -0.0010 |           8.2648 |           9.1341 |
[32m[20230113 20:16:34 @agent_ppo2.py:186][0m |          -0.0029 |           6.0472 |           9.1339 |
[32m[20230113 20:16:34 @agent_ppo2.py:186][0m |          -0.0115 |           5.3246 |           9.1258 |
[32m[20230113 20:16:34 @agent_ppo2.py:186][0m |          -0.0139 |           4.7806 |           9.1238 |
[32m[20230113 20:16:34 @agent_ppo2.py:186][0m |          -0.0130 |           4.4177 |           9.1172 |
[32m[20230113 20:16:34 @agent_ppo2.py:186][0m |          -0.0218 |           4.2220 |           9.1131 |
[32m[20230113 20:16:34 @agent_ppo2.py:186][0m |          -0.0113 |           3.9996 |           9.1166 |
[32m[20230113 20:16:34 @agent_ppo2.py:186][0m |          -0.0121 |           3.8176 |           9.1213 |
[32m[20230113 20:16:34 @agent_ppo2.py:186][0m |          -0.0135 |           3.7045 |           9.1133 |
[32m[20230113 20:16:34 @agent_ppo2.py:186][0m |          -0.0251 |           3.6236 |           9.1150 |
[32m[20230113 20:16:34 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.03
[32m[20230113 20:16:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.96
[32m[20230113 20:16:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 102.37
[32m[20230113 20:16:35 @agent_ppo2.py:144][0m Total time:      32.03 min
[32m[20230113 20:16:35 @agent_ppo2.py:146][0m 2961408 total steps have happened
[32m[20230113 20:16:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1446 --------------------------#
[32m[20230113 20:16:35 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:35 @agent_ppo2.py:186][0m |           0.0011 |           7.7950 |           9.3150 |
[32m[20230113 20:16:35 @agent_ppo2.py:186][0m |          -0.0029 |           5.9838 |           9.3107 |
[32m[20230113 20:16:35 @agent_ppo2.py:186][0m |          -0.0070 |           5.5109 |           9.3065 |
[32m[20230113 20:16:35 @agent_ppo2.py:186][0m |          -0.0088 |           5.0679 |           9.3100 |
[32m[20230113 20:16:35 @agent_ppo2.py:186][0m |          -0.0114 |           4.7943 |           9.3108 |
[32m[20230113 20:16:35 @agent_ppo2.py:186][0m |          -0.0127 |           4.5733 |           9.3053 |
[32m[20230113 20:16:35 @agent_ppo2.py:186][0m |          -0.0130 |           4.4057 |           9.3057 |
[32m[20230113 20:16:35 @agent_ppo2.py:186][0m |          -0.0151 |           4.2582 |           9.3035 |
[32m[20230113 20:16:35 @agent_ppo2.py:186][0m |          -0.0148 |           4.0969 |           9.3037 |
[32m[20230113 20:16:35 @agent_ppo2.py:186][0m |          -0.0163 |           4.0115 |           9.3001 |
[32m[20230113 20:16:35 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.32
[32m[20230113 20:16:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.91
[32m[20230113 20:16:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.52
[32m[20230113 20:16:36 @agent_ppo2.py:144][0m Total time:      32.06 min
[32m[20230113 20:16:36 @agent_ppo2.py:146][0m 2963456 total steps have happened
[32m[20230113 20:16:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1447 --------------------------#
[32m[20230113 20:16:36 @agent_ppo2.py:128][0m Sampling time: 0.31 s by 1 slaves
[32m[20230113 20:16:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:36 @agent_ppo2.py:186][0m |          -0.0026 |          19.8466 |           9.3980 |
[32m[20230113 20:16:36 @agent_ppo2.py:186][0m |          -0.0098 |           9.7458 |           9.3856 |
[32m[20230113 20:16:36 @agent_ppo2.py:186][0m |          -0.0130 |           7.6167 |           9.3776 |
[32m[20230113 20:16:36 @agent_ppo2.py:186][0m |          -0.0193 |           6.6376 |           9.3689 |
[32m[20230113 20:16:36 @agent_ppo2.py:186][0m |          -0.0138 |           6.0186 |           9.3755 |
[32m[20230113 20:16:36 @agent_ppo2.py:186][0m |          -0.0195 |           5.6307 |           9.3697 |
[32m[20230113 20:16:36 @agent_ppo2.py:186][0m |          -0.0132 |           5.3332 |           9.3633 |
[32m[20230113 20:16:36 @agent_ppo2.py:186][0m |          -0.0254 |           5.0078 |           9.3622 |
[32m[20230113 20:16:36 @agent_ppo2.py:186][0m |          -0.0116 |           4.8097 |           9.3635 |
[32m[20230113 20:16:37 @agent_ppo2.py:186][0m |          -0.0240 |           4.6948 |           9.3640 |
[32m[20230113 20:16:37 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 20:16:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 102.39
[32m[20230113 20:16:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.39
[32m[20230113 20:16:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.09
[32m[20230113 20:16:37 @agent_ppo2.py:144][0m Total time:      32.07 min
[32m[20230113 20:16:37 @agent_ppo2.py:146][0m 2965504 total steps have happened
[32m[20230113 20:16:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1448 --------------------------#
[32m[20230113 20:16:37 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:16:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:37 @agent_ppo2.py:186][0m |          -0.0068 |           8.1516 |           9.2515 |
[32m[20230113 20:16:37 @agent_ppo2.py:186][0m |          -0.0002 |           6.6499 |           9.2209 |
[32m[20230113 20:16:37 @agent_ppo2.py:186][0m |          -0.0147 |           5.5202 |           9.2069 |
[32m[20230113 20:16:38 @agent_ppo2.py:186][0m |          -0.0138 |           5.1214 |           9.2079 |
[32m[20230113 20:16:38 @agent_ppo2.py:186][0m |          -0.0186 |           4.8027 |           9.1902 |
[32m[20230113 20:16:38 @agent_ppo2.py:186][0m |          -0.0095 |           4.6136 |           9.1933 |
[32m[20230113 20:16:38 @agent_ppo2.py:186][0m |          -0.0221 |           4.3529 |           9.1936 |
[32m[20230113 20:16:38 @agent_ppo2.py:186][0m |          -0.0141 |           4.1650 |           9.1857 |
[32m[20230113 20:16:38 @agent_ppo2.py:186][0m |          -0.0156 |           4.1231 |           9.1852 |
[32m[20230113 20:16:38 @agent_ppo2.py:186][0m |          -0.0171 |           3.8875 |           9.1825 |
[32m[20230113 20:16:38 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:16:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.19
[32m[20230113 20:16:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.12
[32m[20230113 20:16:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.30
[32m[20230113 20:16:38 @agent_ppo2.py:144][0m Total time:      32.09 min
[32m[20230113 20:16:38 @agent_ppo2.py:146][0m 2967552 total steps have happened
[32m[20230113 20:16:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1449 --------------------------#
[32m[20230113 20:16:39 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 20:16:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:39 @agent_ppo2.py:186][0m |          -0.0029 |          23.5334 |           9.3302 |
[32m[20230113 20:16:39 @agent_ppo2.py:186][0m |          -0.0090 |          11.5183 |           9.3350 |
[32m[20230113 20:16:39 @agent_ppo2.py:186][0m |          -0.0122 |           8.8036 |           9.3279 |
[32m[20230113 20:16:39 @agent_ppo2.py:186][0m |          -0.0150 |           7.0710 |           9.3242 |
[32m[20230113 20:16:39 @agent_ppo2.py:186][0m |          -0.0172 |           6.2048 |           9.3180 |
[32m[20230113 20:16:39 @agent_ppo2.py:186][0m |          -0.0154 |           5.8329 |           9.3127 |
[32m[20230113 20:16:39 @agent_ppo2.py:186][0m |          -0.0186 |           5.4513 |           9.3178 |
[32m[20230113 20:16:39 @agent_ppo2.py:186][0m |          -0.0209 |           5.0905 |           9.3138 |
[32m[20230113 20:16:39 @agent_ppo2.py:186][0m |          -0.0144 |           4.9206 |           9.3108 |
[32m[20230113 20:16:39 @agent_ppo2.py:186][0m |          -0.0219 |           4.7155 |           9.3090 |
[32m[20230113 20:16:39 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 20:16:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 152.39
[32m[20230113 20:16:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.74
[32m[20230113 20:16:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.56
[32m[20230113 20:16:39 @agent_ppo2.py:144][0m Total time:      32.11 min
[32m[20230113 20:16:39 @agent_ppo2.py:146][0m 2969600 total steps have happened
[32m[20230113 20:16:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1450 --------------------------#
[32m[20230113 20:16:40 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:40 @agent_ppo2.py:186][0m |          -0.0003 |           7.3124 |           9.5050 |
[32m[20230113 20:16:40 @agent_ppo2.py:186][0m |          -0.0069 |           5.4624 |           9.4809 |
[32m[20230113 20:16:40 @agent_ppo2.py:186][0m |          -0.0089 |           4.8902 |           9.4734 |
[32m[20230113 20:16:40 @agent_ppo2.py:186][0m |          -0.0116 |           4.6596 |           9.4608 |
[32m[20230113 20:16:40 @agent_ppo2.py:186][0m |          -0.0125 |           4.4406 |           9.4677 |
[32m[20230113 20:16:40 @agent_ppo2.py:186][0m |          -0.0141 |           4.2817 |           9.4563 |
[32m[20230113 20:16:40 @agent_ppo2.py:186][0m |          -0.0139 |           4.1766 |           9.4647 |
[32m[20230113 20:16:40 @agent_ppo2.py:186][0m |          -0.0146 |           4.0912 |           9.4662 |
[32m[20230113 20:16:40 @agent_ppo2.py:186][0m |          -0.0149 |           4.0180 |           9.4670 |
[32m[20230113 20:16:40 @agent_ppo2.py:186][0m |          -0.0154 |           3.9263 |           9.4674 |
[32m[20230113 20:16:40 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.56
[32m[20230113 20:16:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.15
[32m[20230113 20:16:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.46
[32m[20230113 20:16:41 @agent_ppo2.py:144][0m Total time:      32.13 min
[32m[20230113 20:16:41 @agent_ppo2.py:146][0m 2971648 total steps have happened
[32m[20230113 20:16:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1451 --------------------------#
[32m[20230113 20:16:41 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:41 @agent_ppo2.py:186][0m |          -0.0028 |          14.2516 |           9.3908 |
[32m[20230113 20:16:41 @agent_ppo2.py:186][0m |          -0.0058 |           9.5260 |           9.3651 |
[32m[20230113 20:16:41 @agent_ppo2.py:186][0m |          -0.0064 |           7.9610 |           9.3735 |
[32m[20230113 20:16:41 @agent_ppo2.py:186][0m |          -0.0086 |           6.9812 |           9.3799 |
[32m[20230113 20:16:41 @agent_ppo2.py:186][0m |          -0.0110 |           6.3957 |           9.3802 |
[32m[20230113 20:16:41 @agent_ppo2.py:186][0m |          -0.0122 |           6.0390 |           9.3767 |
[32m[20230113 20:16:41 @agent_ppo2.py:186][0m |          -0.0122 |           5.4822 |           9.3738 |
[32m[20230113 20:16:41 @agent_ppo2.py:186][0m |          -0.0149 |           5.2455 |           9.3827 |
[32m[20230113 20:16:41 @agent_ppo2.py:186][0m |          -0.0147 |           5.0127 |           9.3745 |
[32m[20230113 20:16:41 @agent_ppo2.py:186][0m |          -0.0161 |           4.7829 |           9.3796 |
[32m[20230113 20:16:41 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.86
[32m[20230113 20:16:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.79
[32m[20230113 20:16:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.60
[32m[20230113 20:16:42 @agent_ppo2.py:144][0m Total time:      32.15 min
[32m[20230113 20:16:42 @agent_ppo2.py:146][0m 2973696 total steps have happened
[32m[20230113 20:16:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1452 --------------------------#
[32m[20230113 20:16:42 @agent_ppo2.py:128][0m Sampling time: 0.35 s by 1 slaves
[32m[20230113 20:16:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:42 @agent_ppo2.py:186][0m |          -0.0021 |          28.3004 |           9.6514 |
[32m[20230113 20:16:42 @agent_ppo2.py:186][0m |          -0.0109 |          15.9330 |           9.6182 |
[32m[20230113 20:16:42 @agent_ppo2.py:186][0m |          -0.0145 |          11.9518 |           9.6217 |
[32m[20230113 20:16:42 @agent_ppo2.py:186][0m |          -0.0159 |           9.6174 |           9.6135 |
[32m[20230113 20:16:42 @agent_ppo2.py:186][0m |          -0.0167 |           8.0458 |           9.6134 |
[32m[20230113 20:16:42 @agent_ppo2.py:186][0m |          -0.0170 |           7.2482 |           9.6015 |
[32m[20230113 20:16:42 @agent_ppo2.py:186][0m |          -0.0185 |           6.4210 |           9.5997 |
[32m[20230113 20:16:42 @agent_ppo2.py:186][0m |          -0.0203 |           5.8944 |           9.6029 |
[32m[20230113 20:16:42 @agent_ppo2.py:186][0m |          -0.0208 |           5.5869 |           9.5943 |
[32m[20230113 20:16:43 @agent_ppo2.py:186][0m |          -0.0209 |           5.1626 |           9.5880 |
[32m[20230113 20:16:43 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 20:16:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 139.85
[32m[20230113 20:16:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.17
[32m[20230113 20:16:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.47
[32m[20230113 20:16:43 @agent_ppo2.py:144][0m Total time:      32.17 min
[32m[20230113 20:16:43 @agent_ppo2.py:146][0m 2975744 total steps have happened
[32m[20230113 20:16:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1453 --------------------------#
[32m[20230113 20:16:43 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 20:16:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:43 @agent_ppo2.py:186][0m |           0.0026 |          24.6905 |           9.2978 |
[32m[20230113 20:16:43 @agent_ppo2.py:186][0m |           0.0298 |          15.3908 |           9.2955 |
[32m[20230113 20:16:43 @agent_ppo2.py:186][0m |           0.0067 |          12.2433 |           9.2777 |
[32m[20230113 20:16:43 @agent_ppo2.py:186][0m |          -0.0075 |           9.2125 |           9.2875 |
[32m[20230113 20:16:44 @agent_ppo2.py:186][0m |          -0.0070 |           8.1185 |           9.2827 |
[32m[20230113 20:16:44 @agent_ppo2.py:186][0m |           0.0280 |           8.8164 |           9.2862 |
[32m[20230113 20:16:44 @agent_ppo2.py:186][0m |          -0.0121 |           6.9516 |           9.2775 |
[32m[20230113 20:16:44 @agent_ppo2.py:186][0m |          -0.0269 |           6.3600 |           9.2762 |
[32m[20230113 20:16:44 @agent_ppo2.py:186][0m |          -0.0109 |           6.0533 |           9.2779 |
[32m[20230113 20:16:44 @agent_ppo2.py:186][0m |          -0.0096 |           5.5370 |           9.2823 |
[32m[20230113 20:16:44 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 20:16:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 163.32
[32m[20230113 20:16:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.40
[32m[20230113 20:16:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.71
[32m[20230113 20:16:44 @agent_ppo2.py:144][0m Total time:      32.19 min
[32m[20230113 20:16:44 @agent_ppo2.py:146][0m 2977792 total steps have happened
[32m[20230113 20:16:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1454 --------------------------#
[32m[20230113 20:16:45 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:16:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:45 @agent_ppo2.py:186][0m |           0.0065 |           7.1727 |           9.3628 |
[32m[20230113 20:16:45 @agent_ppo2.py:186][0m |          -0.0119 |           5.7786 |           9.3592 |
[32m[20230113 20:16:45 @agent_ppo2.py:186][0m |           0.0087 |           5.1844 |           9.3561 |
[32m[20230113 20:16:45 @agent_ppo2.py:186][0m |          -0.0152 |           4.7199 |           9.3545 |
[32m[20230113 20:16:45 @agent_ppo2.py:186][0m |           0.0066 |           4.5215 |           9.3557 |
[32m[20230113 20:16:45 @agent_ppo2.py:186][0m |          -0.0227 |           4.2124 |           9.3544 |
[32m[20230113 20:16:45 @agent_ppo2.py:186][0m |          -0.0020 |           3.9745 |           9.3553 |
[32m[20230113 20:16:45 @agent_ppo2.py:186][0m |          -0.0209 |           3.9106 |           9.3644 |
[32m[20230113 20:16:45 @agent_ppo2.py:186][0m |          -0.0222 |           3.6791 |           9.3638 |
[32m[20230113 20:16:45 @agent_ppo2.py:186][0m |          -0.0254 |           3.5411 |           9.3648 |
[32m[20230113 20:16:45 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:16:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.80
[32m[20230113 20:16:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.31
[32m[20230113 20:16:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.98
[32m[20230113 20:16:45 @agent_ppo2.py:144][0m Total time:      32.21 min
[32m[20230113 20:16:45 @agent_ppo2.py:146][0m 2979840 total steps have happened
[32m[20230113 20:16:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1455 --------------------------#
[32m[20230113 20:16:46 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 20:16:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:46 @agent_ppo2.py:186][0m |          -0.0006 |          15.6341 |           9.3252 |
[32m[20230113 20:16:46 @agent_ppo2.py:186][0m |          -0.0050 |           9.7276 |           9.3182 |
[32m[20230113 20:16:46 @agent_ppo2.py:186][0m |          -0.0016 |           8.2630 |           9.3224 |
[32m[20230113 20:16:46 @agent_ppo2.py:186][0m |          -0.0063 |           7.2826 |           9.3107 |
[32m[20230113 20:16:46 @agent_ppo2.py:186][0m |          -0.0114 |           6.6968 |           9.3145 |
[32m[20230113 20:16:46 @agent_ppo2.py:186][0m |          -0.0094 |           6.2599 |           9.3159 |
[32m[20230113 20:16:46 @agent_ppo2.py:186][0m |          -0.0110 |           5.9467 |           9.2988 |
[32m[20230113 20:16:46 @agent_ppo2.py:186][0m |          -0.0077 |           5.7095 |           9.3148 |
[32m[20230113 20:16:46 @agent_ppo2.py:186][0m |          -0.0110 |           5.4395 |           9.3143 |
[32m[20230113 20:16:46 @agent_ppo2.py:186][0m |          -0.0125 |           5.2316 |           9.3078 |
[32m[20230113 20:16:46 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 20:16:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 132.90
[32m[20230113 20:16:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.51
[32m[20230113 20:16:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.13
[32m[20230113 20:16:47 @agent_ppo2.py:144][0m Total time:      32.24 min
[32m[20230113 20:16:47 @agent_ppo2.py:146][0m 2981888 total steps have happened
[32m[20230113 20:16:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1456 --------------------------#
[32m[20230113 20:16:47 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:16:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:47 @agent_ppo2.py:186][0m |          -0.0074 |           5.9626 |           9.6932 |
[32m[20230113 20:16:47 @agent_ppo2.py:186][0m |          -0.0033 |           4.9372 |           9.6668 |
[32m[20230113 20:16:47 @agent_ppo2.py:186][0m |          -0.0107 |           4.2451 |           9.6594 |
[32m[20230113 20:16:47 @agent_ppo2.py:186][0m |          -0.0140 |           3.8762 |           9.6610 |
[32m[20230113 20:16:47 @agent_ppo2.py:186][0m |          -0.0115 |           3.6496 |           9.6458 |
[32m[20230113 20:16:47 @agent_ppo2.py:186][0m |          -0.0155 |           3.4566 |           9.6551 |
[32m[20230113 20:16:48 @agent_ppo2.py:186][0m |          -0.0152 |           3.3233 |           9.6573 |
[32m[20230113 20:16:48 @agent_ppo2.py:186][0m |          -0.0150 |           3.2475 |           9.6498 |
[32m[20230113 20:16:48 @agent_ppo2.py:186][0m |          -0.0168 |           3.1133 |           9.6546 |
[32m[20230113 20:16:48 @agent_ppo2.py:186][0m |          -0.0164 |           3.0615 |           9.6501 |
[32m[20230113 20:16:48 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.62
[32m[20230113 20:16:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.68
[32m[20230113 20:16:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 127.28
[32m[20230113 20:16:48 @agent_ppo2.py:144][0m Total time:      32.26 min
[32m[20230113 20:16:48 @agent_ppo2.py:146][0m 2983936 total steps have happened
[32m[20230113 20:16:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1457 --------------------------#
[32m[20230113 20:16:48 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:16:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:49 @agent_ppo2.py:186][0m |          -0.0087 |           8.5513 |           9.3816 |
[32m[20230113 20:16:49 @agent_ppo2.py:186][0m |          -0.0115 |           6.5400 |           9.3770 |
[32m[20230113 20:16:49 @agent_ppo2.py:186][0m |          -0.0080 |           5.8804 |           9.3676 |
[32m[20230113 20:16:49 @agent_ppo2.py:186][0m |          -0.0125 |           5.3732 |           9.3747 |
[32m[20230113 20:16:49 @agent_ppo2.py:186][0m |           0.0003 |           5.2137 |           9.3649 |
[32m[20230113 20:16:49 @agent_ppo2.py:186][0m |          -0.0103 |           4.9306 |           9.3681 |
[32m[20230113 20:16:49 @agent_ppo2.py:186][0m |          -0.0139 |           4.7581 |           9.3682 |
[32m[20230113 20:16:49 @agent_ppo2.py:186][0m |          -0.0130 |           4.5663 |           9.3638 |
[32m[20230113 20:16:49 @agent_ppo2.py:186][0m |          -0.0140 |           4.4071 |           9.3619 |
[32m[20230113 20:16:49 @agent_ppo2.py:186][0m |          -0.0057 |           4.6805 |           9.3566 |
[32m[20230113 20:16:49 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.77
[32m[20230113 20:16:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.15
[32m[20230113 20:16:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.22
[32m[20230113 20:16:49 @agent_ppo2.py:144][0m Total time:      32.28 min
[32m[20230113 20:16:49 @agent_ppo2.py:146][0m 2985984 total steps have happened
[32m[20230113 20:16:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1458 --------------------------#
[32m[20230113 20:16:50 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:50 @agent_ppo2.py:186][0m |           0.0017 |           6.0644 |           9.3684 |
[32m[20230113 20:16:50 @agent_ppo2.py:186][0m |          -0.0183 |           4.4917 |           9.3634 |
[32m[20230113 20:16:50 @agent_ppo2.py:186][0m |          -0.0122 |           3.9591 |           9.3701 |
[32m[20230113 20:16:50 @agent_ppo2.py:186][0m |          -0.0153 |           3.5933 |           9.3657 |
[32m[20230113 20:16:50 @agent_ppo2.py:186][0m |          -0.0305 |           3.5474 |           9.3659 |
[32m[20230113 20:16:50 @agent_ppo2.py:186][0m |          -0.0155 |           3.2609 |           9.3593 |
[32m[20230113 20:16:50 @agent_ppo2.py:186][0m |          -0.0097 |           3.0936 |           9.3655 |
[32m[20230113 20:16:50 @agent_ppo2.py:186][0m |          -0.0160 |           2.9545 |           9.3635 |
[32m[20230113 20:16:50 @agent_ppo2.py:186][0m |          -0.0129 |           2.8729 |           9.3548 |
[32m[20230113 20:16:50 @agent_ppo2.py:186][0m |          -0.0140 |           2.7896 |           9.3578 |
[32m[20230113 20:16:50 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.13
[32m[20230113 20:16:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.10
[32m[20230113 20:16:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.41
[32m[20230113 20:16:51 @agent_ppo2.py:144][0m Total time:      32.30 min
[32m[20230113 20:16:51 @agent_ppo2.py:146][0m 2988032 total steps have happened
[32m[20230113 20:16:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1459 --------------------------#
[32m[20230113 20:16:51 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:16:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:51 @agent_ppo2.py:186][0m |           0.0001 |           7.1728 |           9.2782 |
[32m[20230113 20:16:51 @agent_ppo2.py:186][0m |          -0.0025 |           5.3810 |           9.2624 |
[32m[20230113 20:16:51 @agent_ppo2.py:186][0m |          -0.0107 |           4.6863 |           9.2737 |
[32m[20230113 20:16:51 @agent_ppo2.py:186][0m |          -0.0141 |           4.2121 |           9.2611 |
[32m[20230113 20:16:51 @agent_ppo2.py:186][0m |          -0.0152 |           3.9123 |           9.2597 |
[32m[20230113 20:16:51 @agent_ppo2.py:186][0m |          -0.0188 |           3.6563 |           9.2585 |
[32m[20230113 20:16:51 @agent_ppo2.py:186][0m |          -0.0164 |           3.4099 |           9.2624 |
[32m[20230113 20:16:51 @agent_ppo2.py:186][0m |          -0.0083 |           3.2938 |           9.2565 |
[32m[20230113 20:16:51 @agent_ppo2.py:186][0m |          -0.0230 |           3.1784 |           9.2555 |
[32m[20230113 20:16:51 @agent_ppo2.py:186][0m |          -0.0226 |           3.0678 |           9.2665 |
[32m[20230113 20:16:51 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:16:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.50
[32m[20230113 20:16:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.88
[32m[20230113 20:16:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.06
[32m[20230113 20:16:52 @agent_ppo2.py:144][0m Total time:      32.32 min
[32m[20230113 20:16:52 @agent_ppo2.py:146][0m 2990080 total steps have happened
[32m[20230113 20:16:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1460 --------------------------#
[32m[20230113 20:16:52 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:52 @agent_ppo2.py:186][0m |          -0.0021 |           6.6023 |           9.4795 |
[32m[20230113 20:16:52 @agent_ppo2.py:186][0m |          -0.0059 |           5.0623 |           9.4773 |
[32m[20230113 20:16:52 @agent_ppo2.py:186][0m |          -0.0083 |           4.4676 |           9.4775 |
[32m[20230113 20:16:52 @agent_ppo2.py:186][0m |          -0.0100 |           4.1745 |           9.4792 |
[32m[20230113 20:16:52 @agent_ppo2.py:186][0m |          -0.0111 |           3.9162 |           9.4705 |
[32m[20230113 20:16:53 @agent_ppo2.py:186][0m |          -0.0129 |           3.7313 |           9.4781 |
[32m[20230113 20:16:53 @agent_ppo2.py:186][0m |          -0.0141 |           3.6172 |           9.4793 |
[32m[20230113 20:16:53 @agent_ppo2.py:186][0m |          -0.0140 |           3.4995 |           9.4752 |
[32m[20230113 20:16:53 @agent_ppo2.py:186][0m |          -0.0154 |           3.4319 |           9.4767 |
[32m[20230113 20:16:53 @agent_ppo2.py:186][0m |          -0.0159 |           3.3289 |           9.4764 |
[32m[20230113 20:16:53 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.14
[32m[20230113 20:16:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.43
[32m[20230113 20:16:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.67
[32m[20230113 20:16:53 @agent_ppo2.py:144][0m Total time:      32.34 min
[32m[20230113 20:16:53 @agent_ppo2.py:146][0m 2992128 total steps have happened
[32m[20230113 20:16:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1461 --------------------------#
[32m[20230113 20:16:53 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:16:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:54 @agent_ppo2.py:186][0m |          -0.0022 |          35.6381 |           9.4997 |
[32m[20230113 20:16:54 @agent_ppo2.py:186][0m |          -0.0033 |          18.0030 |           9.4981 |
[32m[20230113 20:16:54 @agent_ppo2.py:186][0m |          -0.0077 |          14.3024 |           9.4952 |
[32m[20230113 20:16:54 @agent_ppo2.py:186][0m |          -0.0077 |          12.3484 |           9.4871 |
[32m[20230113 20:16:54 @agent_ppo2.py:186][0m |          -0.0133 |          10.7820 |           9.4846 |
[32m[20230113 20:16:54 @agent_ppo2.py:186][0m |          -0.0120 |           9.7156 |           9.4798 |
[32m[20230113 20:16:54 @agent_ppo2.py:186][0m |          -0.0169 |           9.0055 |           9.4729 |
[32m[20230113 20:16:54 @agent_ppo2.py:186][0m |          -0.0154 |           8.1593 |           9.4783 |
[32m[20230113 20:16:54 @agent_ppo2.py:186][0m |          -0.0178 |           7.5368 |           9.4798 |
[32m[20230113 20:16:54 @agent_ppo2.py:186][0m |          -0.0184 |           7.0020 |           9.4786 |
[32m[20230113 20:16:54 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 20:16:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 49.03
[32m[20230113 20:16:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.03
[32m[20230113 20:16:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.15
[32m[20230113 20:16:54 @agent_ppo2.py:144][0m Total time:      32.36 min
[32m[20230113 20:16:54 @agent_ppo2.py:146][0m 2994176 total steps have happened
[32m[20230113 20:16:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1462 --------------------------#
[32m[20230113 20:16:55 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:16:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:55 @agent_ppo2.py:186][0m |          -0.0044 |          12.4195 |           9.3964 |
[32m[20230113 20:16:55 @agent_ppo2.py:186][0m |          -0.0041 |           8.9655 |           9.3714 |
[32m[20230113 20:16:55 @agent_ppo2.py:186][0m |          -0.0098 |           7.8492 |           9.3830 |
[32m[20230113 20:16:55 @agent_ppo2.py:186][0m |          -0.0059 |           7.1067 |           9.3821 |
[32m[20230113 20:16:55 @agent_ppo2.py:186][0m |          -0.0111 |           6.5458 |           9.3697 |
[32m[20230113 20:16:55 @agent_ppo2.py:186][0m |          -0.0103 |           6.1690 |           9.3729 |
[32m[20230113 20:16:55 @agent_ppo2.py:186][0m |          -0.0089 |           5.9700 |           9.3772 |
[32m[20230113 20:16:55 @agent_ppo2.py:186][0m |          -0.0141 |           5.6403 |           9.3677 |
[32m[20230113 20:16:55 @agent_ppo2.py:186][0m |          -0.0149 |           5.4813 |           9.3676 |
[32m[20230113 20:16:55 @agent_ppo2.py:186][0m |          -0.0202 |           5.2834 |           9.3630 |
[32m[20230113 20:16:55 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:16:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.96
[32m[20230113 20:16:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.17
[32m[20230113 20:16:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.15
[32m[20230113 20:16:56 @agent_ppo2.py:144][0m Total time:      32.38 min
[32m[20230113 20:16:56 @agent_ppo2.py:146][0m 2996224 total steps have happened
[32m[20230113 20:16:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1463 --------------------------#
[32m[20230113 20:16:56 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 20:16:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:56 @agent_ppo2.py:186][0m |           0.0018 |          16.0567 |           9.5325 |
[32m[20230113 20:16:56 @agent_ppo2.py:186][0m |          -0.0022 |           8.6433 |           9.5317 |
[32m[20230113 20:16:56 @agent_ppo2.py:186][0m |          -0.0084 |           7.2951 |           9.5279 |
[32m[20230113 20:16:56 @agent_ppo2.py:186][0m |          -0.0088 |           6.7409 |           9.5237 |
[32m[20230113 20:16:56 @agent_ppo2.py:186][0m |          -0.0110 |           6.1228 |           9.5225 |
[32m[20230113 20:16:56 @agent_ppo2.py:186][0m |          -0.0116 |           5.8367 |           9.5177 |
[32m[20230113 20:16:56 @agent_ppo2.py:186][0m |          -0.0116 |           5.5333 |           9.5157 |
[32m[20230113 20:16:56 @agent_ppo2.py:186][0m |          -0.0128 |           5.3357 |           9.5120 |
[32m[20230113 20:16:56 @agent_ppo2.py:186][0m |          -0.0150 |           5.0373 |           9.5142 |
[32m[20230113 20:16:57 @agent_ppo2.py:186][0m |          -0.0153 |           4.8132 |           9.4989 |
[32m[20230113 20:16:57 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 20:16:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 131.89
[32m[20230113 20:16:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.88
[32m[20230113 20:16:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.25
[32m[20230113 20:16:57 @agent_ppo2.py:144][0m Total time:      32.41 min
[32m[20230113 20:16:57 @agent_ppo2.py:146][0m 2998272 total steps have happened
[32m[20230113 20:16:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1464 --------------------------#
[32m[20230113 20:16:57 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:16:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:57 @agent_ppo2.py:186][0m |           0.0053 |          11.0841 |           9.4605 |
[32m[20230113 20:16:57 @agent_ppo2.py:186][0m |          -0.0061 |           7.3785 |           9.4698 |
[32m[20230113 20:16:58 @agent_ppo2.py:186][0m |          -0.0081 |           6.4435 |           9.4641 |
[32m[20230113 20:16:58 @agent_ppo2.py:186][0m |          -0.0121 |           5.9679 |           9.4607 |
[32m[20230113 20:16:58 @agent_ppo2.py:186][0m |          -0.0097 |           5.5587 |           9.4668 |
[32m[20230113 20:16:58 @agent_ppo2.py:186][0m |          -0.0111 |           5.4256 |           9.4766 |
[32m[20230113 20:16:58 @agent_ppo2.py:186][0m |          -0.0174 |           5.1195 |           9.4656 |
[32m[20230113 20:16:58 @agent_ppo2.py:186][0m |          -0.0161 |           4.9426 |           9.4709 |
[32m[20230113 20:16:58 @agent_ppo2.py:186][0m |          -0.0161 |           4.7911 |           9.4728 |
[32m[20230113 20:16:58 @agent_ppo2.py:186][0m |          -0.0167 |           4.7564 |           9.4748 |
[32m[20230113 20:16:58 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:16:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.93
[32m[20230113 20:16:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.48
[32m[20230113 20:16:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 174.47
[32m[20230113 20:16:58 @agent_ppo2.py:144][0m Total time:      32.43 min
[32m[20230113 20:16:58 @agent_ppo2.py:146][0m 3000320 total steps have happened
[32m[20230113 20:16:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1465 --------------------------#
[32m[20230113 20:16:59 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:16:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:16:59 @agent_ppo2.py:186][0m |           0.0052 |           6.7499 |           9.5835 |
[32m[20230113 20:16:59 @agent_ppo2.py:186][0m |          -0.0038 |           5.5340 |           9.5731 |
[32m[20230113 20:16:59 @agent_ppo2.py:186][0m |          -0.0118 |           4.8754 |           9.5684 |
[32m[20230113 20:16:59 @agent_ppo2.py:186][0m |          -0.0085 |           4.5894 |           9.5606 |
[32m[20230113 20:16:59 @agent_ppo2.py:186][0m |          -0.0032 |           4.3369 |           9.5664 |
[32m[20230113 20:16:59 @agent_ppo2.py:186][0m |          -0.0177 |           4.1555 |           9.5665 |
[32m[20230113 20:16:59 @agent_ppo2.py:186][0m |          -0.0122 |           4.0783 |           9.5636 |
[32m[20230113 20:16:59 @agent_ppo2.py:186][0m |          -0.0204 |           3.8966 |           9.5661 |
[32m[20230113 20:16:59 @agent_ppo2.py:186][0m |          -0.0201 |           3.7810 |           9.5679 |
[32m[20230113 20:16:59 @agent_ppo2.py:186][0m |          -0.0160 |           3.7315 |           9.5655 |
[32m[20230113 20:16:59 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:16:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.90
[32m[20230113 20:16:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.58
[32m[20230113 20:16:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 180.31
[32m[20230113 20:16:59 @agent_ppo2.py:144][0m Total time:      32.45 min
[32m[20230113 20:16:59 @agent_ppo2.py:146][0m 3002368 total steps have happened
[32m[20230113 20:16:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1466 --------------------------#
[32m[20230113 20:17:00 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:17:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:00 @agent_ppo2.py:186][0m |           0.0001 |           6.7953 |           9.6903 |
[32m[20230113 20:17:00 @agent_ppo2.py:186][0m |          -0.0085 |           5.0836 |           9.6740 |
[32m[20230113 20:17:00 @agent_ppo2.py:186][0m |          -0.0101 |           4.4389 |           9.6731 |
[32m[20230113 20:17:00 @agent_ppo2.py:186][0m |          -0.0121 |           4.1229 |           9.6792 |
[32m[20230113 20:17:00 @agent_ppo2.py:186][0m |          -0.0118 |           3.8887 |           9.6747 |
[32m[20230113 20:17:00 @agent_ppo2.py:186][0m |          -0.0135 |           3.6975 |           9.6699 |
[32m[20230113 20:17:00 @agent_ppo2.py:186][0m |          -0.0148 |           3.5833 |           9.6682 |
[32m[20230113 20:17:00 @agent_ppo2.py:186][0m |          -0.0154 |           3.5318 |           9.6682 |
[32m[20230113 20:17:00 @agent_ppo2.py:186][0m |          -0.0165 |           3.3676 |           9.6723 |
[32m[20230113 20:17:00 @agent_ppo2.py:186][0m |          -0.0169 |           3.3059 |           9.6705 |
[32m[20230113 20:17:00 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:17:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.99
[32m[20230113 20:17:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.34
[32m[20230113 20:17:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 93.08
[32m[20230113 20:17:01 @agent_ppo2.py:144][0m Total time:      32.47 min
[32m[20230113 20:17:01 @agent_ppo2.py:146][0m 3004416 total steps have happened
[32m[20230113 20:17:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1467 --------------------------#
[32m[20230113 20:17:01 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:17:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:01 @agent_ppo2.py:186][0m |          -0.0021 |           6.9698 |           9.6506 |
[32m[20230113 20:17:01 @agent_ppo2.py:186][0m |          -0.0117 |           5.0246 |           9.6248 |
[32m[20230113 20:17:01 @agent_ppo2.py:186][0m |          -0.0114 |           4.5160 |           9.6174 |
[32m[20230113 20:17:01 @agent_ppo2.py:186][0m |          -0.0098 |           4.2629 |           9.6168 |
[32m[20230113 20:17:01 @agent_ppo2.py:186][0m |          -0.0138 |           3.9925 |           9.6163 |
[32m[20230113 20:17:01 @agent_ppo2.py:186][0m |          -0.0147 |           3.8224 |           9.6194 |
[32m[20230113 20:17:01 @agent_ppo2.py:186][0m |          -0.0117 |           3.7347 |           9.6173 |
[32m[20230113 20:17:01 @agent_ppo2.py:186][0m |          -0.0139 |           3.6092 |           9.6217 |
[32m[20230113 20:17:02 @agent_ppo2.py:186][0m |          -0.0162 |           3.5288 |           9.6191 |
[32m[20230113 20:17:02 @agent_ppo2.py:186][0m |          -0.0135 |           3.4085 |           9.6186 |
[32m[20230113 20:17:02 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:17:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.22
[32m[20230113 20:17:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.15
[32m[20230113 20:17:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 157.34
[32m[20230113 20:17:02 @agent_ppo2.py:144][0m Total time:      32.49 min
[32m[20230113 20:17:02 @agent_ppo2.py:146][0m 3006464 total steps have happened
[32m[20230113 20:17:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1468 --------------------------#
[32m[20230113 20:17:02 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:17:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:03 @agent_ppo2.py:186][0m |          -0.0043 |          15.6225 |           9.6683 |
[32m[20230113 20:17:03 @agent_ppo2.py:186][0m |          -0.0164 |           6.0152 |           9.6459 |
[32m[20230113 20:17:03 @agent_ppo2.py:186][0m |          -0.0139 |           4.9691 |           9.6446 |
[32m[20230113 20:17:03 @agent_ppo2.py:186][0m |          -0.0161 |           4.3549 |           9.6508 |
[32m[20230113 20:17:03 @agent_ppo2.py:186][0m |          -0.0159 |           3.9474 |           9.6506 |
[32m[20230113 20:17:03 @agent_ppo2.py:186][0m |          -0.0097 |           3.9217 |           9.6458 |
[32m[20230113 20:17:03 @agent_ppo2.py:186][0m |          -0.0041 |           4.5263 |           9.6454 |
[32m[20230113 20:17:03 @agent_ppo2.py:186][0m |          -0.0105 |           3.6881 |           9.6460 |
[32m[20230113 20:17:03 @agent_ppo2.py:186][0m |          -0.0228 |           3.2787 |           9.6500 |
[32m[20230113 20:17:03 @agent_ppo2.py:186][0m |          -0.0206 |           3.1733 |           9.6471 |
[32m[20230113 20:17:03 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:17:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 169.48
[32m[20230113 20:17:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.60
[32m[20230113 20:17:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.52
[32m[20230113 20:17:03 @agent_ppo2.py:144][0m Total time:      32.51 min
[32m[20230113 20:17:03 @agent_ppo2.py:146][0m 3008512 total steps have happened
[32m[20230113 20:17:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1469 --------------------------#
[32m[20230113 20:17:04 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:17:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:04 @agent_ppo2.py:186][0m |           0.0029 |           6.9699 |           9.6121 |
[32m[20230113 20:17:04 @agent_ppo2.py:186][0m |          -0.0054 |           5.5240 |           9.6103 |
[32m[20230113 20:17:04 @agent_ppo2.py:186][0m |          -0.0043 |           4.8640 |           9.5979 |
[32m[20230113 20:17:04 @agent_ppo2.py:186][0m |          -0.0071 |           4.5359 |           9.6078 |
[32m[20230113 20:17:04 @agent_ppo2.py:186][0m |          -0.0030 |           4.3541 |           9.6035 |
[32m[20230113 20:17:04 @agent_ppo2.py:186][0m |          -0.0172 |           4.1026 |           9.6112 |
[32m[20230113 20:17:04 @agent_ppo2.py:186][0m |          -0.0114 |           3.8753 |           9.6039 |
[32m[20230113 20:17:04 @agent_ppo2.py:186][0m |          -0.0141 |           3.7130 |           9.6010 |
[32m[20230113 20:17:04 @agent_ppo2.py:186][0m |          -0.0140 |           3.6167 |           9.6022 |
[32m[20230113 20:17:04 @agent_ppo2.py:186][0m |          -0.0125 |           3.5504 |           9.6057 |
[32m[20230113 20:17:04 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.96
[32m[20230113 20:17:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.26
[32m[20230113 20:17:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.76
[32m[20230113 20:17:05 @agent_ppo2.py:144][0m Total time:      32.53 min
[32m[20230113 20:17:05 @agent_ppo2.py:146][0m 3010560 total steps have happened
[32m[20230113 20:17:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1470 --------------------------#
[32m[20230113 20:17:05 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:17:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:05 @agent_ppo2.py:186][0m |           0.0016 |           7.1987 |           9.7355 |
[32m[20230113 20:17:05 @agent_ppo2.py:186][0m |          -0.0039 |           5.2570 |           9.7290 |
[32m[20230113 20:17:05 @agent_ppo2.py:186][0m |          -0.0087 |           4.6905 |           9.7134 |
[32m[20230113 20:17:05 @agent_ppo2.py:186][0m |          -0.0113 |           4.3128 |           9.7245 |
[32m[20230113 20:17:05 @agent_ppo2.py:186][0m |          -0.0126 |           4.0743 |           9.7257 |
[32m[20230113 20:17:05 @agent_ppo2.py:186][0m |          -0.0122 |           3.8940 |           9.7226 |
[32m[20230113 20:17:05 @agent_ppo2.py:186][0m |          -0.0127 |           3.7576 |           9.7277 |
[32m[20230113 20:17:05 @agent_ppo2.py:186][0m |          -0.0153 |           3.6124 |           9.7271 |
[32m[20230113 20:17:05 @agent_ppo2.py:186][0m |          -0.0151 |           3.5140 |           9.7237 |
[32m[20230113 20:17:05 @agent_ppo2.py:186][0m |          -0.0173 |           3.4130 |           9.7209 |
[32m[20230113 20:17:05 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.36
[32m[20230113 20:17:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.76
[32m[20230113 20:17:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.81
[32m[20230113 20:17:06 @agent_ppo2.py:144][0m Total time:      32.55 min
[32m[20230113 20:17:06 @agent_ppo2.py:146][0m 3012608 total steps have happened
[32m[20230113 20:17:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1471 --------------------------#
[32m[20230113 20:17:06 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:06 @agent_ppo2.py:186][0m |           0.0046 |           5.3233 |           9.6979 |
[32m[20230113 20:17:06 @agent_ppo2.py:186][0m |          -0.0060 |           3.9456 |           9.6706 |
[32m[20230113 20:17:06 @agent_ppo2.py:186][0m |          -0.0042 |           3.5127 |           9.6808 |
[32m[20230113 20:17:06 @agent_ppo2.py:186][0m |          -0.0091 |           3.2343 |           9.6739 |
[32m[20230113 20:17:06 @agent_ppo2.py:186][0m |          -0.0151 |           3.0077 |           9.6704 |
[32m[20230113 20:17:07 @agent_ppo2.py:186][0m |          -0.0152 |           2.8827 |           9.6786 |
[32m[20230113 20:17:07 @agent_ppo2.py:186][0m |          -0.0092 |           2.8473 |           9.6706 |
[32m[20230113 20:17:07 @agent_ppo2.py:186][0m |          -0.0156 |           2.6782 |           9.6755 |
[32m[20230113 20:17:07 @agent_ppo2.py:186][0m |          -0.0165 |           2.5993 |           9.6751 |
[32m[20230113 20:17:07 @agent_ppo2.py:186][0m |          -0.0148 |           2.5475 |           9.6675 |
[32m[20230113 20:17:07 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.47
[32m[20230113 20:17:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.19
[32m[20230113 20:17:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.29
[32m[20230113 20:17:07 @agent_ppo2.py:144][0m Total time:      32.58 min
[32m[20230113 20:17:07 @agent_ppo2.py:146][0m 3014656 total steps have happened
[32m[20230113 20:17:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1472 --------------------------#
[32m[20230113 20:17:07 @agent_ppo2.py:128][0m Sampling time: 0.31 s by 1 slaves
[32m[20230113 20:17:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:07 @agent_ppo2.py:186][0m |          -0.0143 |          20.0512 |           9.2341 |
[32m[20230113 20:17:07 @agent_ppo2.py:186][0m |          -0.0079 |           7.7909 |           9.2412 |
[32m[20230113 20:17:08 @agent_ppo2.py:186][0m |          -0.0078 |           6.3021 |           9.2623 |
[32m[20230113 20:17:08 @agent_ppo2.py:186][0m |          -0.0065 |           5.2396 |           9.2491 |
[32m[20230113 20:17:08 @agent_ppo2.py:186][0m |           0.0088 |           4.9229 |           9.2612 |
[32m[20230113 20:17:08 @agent_ppo2.py:186][0m |          -0.0187 |           4.5260 |           9.2495 |
[32m[20230113 20:17:08 @agent_ppo2.py:186][0m |          -0.0221 |           4.2410 |           9.2562 |
[32m[20230113 20:17:08 @agent_ppo2.py:186][0m |           0.0046 |           4.2082 |           9.2630 |
[32m[20230113 20:17:08 @agent_ppo2.py:186][0m |          -0.0276 |           3.7267 |           9.2610 |
[32m[20230113 20:17:08 @agent_ppo2.py:186][0m |          -0.0219 |           3.4491 |           9.2641 |
[32m[20230113 20:17:08 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 20:17:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 103.28
[32m[20230113 20:17:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.09
[32m[20230113 20:17:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.05
[32m[20230113 20:17:08 @agent_ppo2.py:144][0m Total time:      32.59 min
[32m[20230113 20:17:08 @agent_ppo2.py:146][0m 3016704 total steps have happened
[32m[20230113 20:17:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1473 --------------------------#
[32m[20230113 20:17:09 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:17:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:09 @agent_ppo2.py:186][0m |          -0.0001 |          10.5625 |           9.4675 |
[32m[20230113 20:17:09 @agent_ppo2.py:186][0m |          -0.0076 |           7.5250 |           9.4435 |
[32m[20230113 20:17:09 @agent_ppo2.py:186][0m |          -0.0077 |           6.4491 |           9.4448 |
[32m[20230113 20:17:09 @agent_ppo2.py:186][0m |          -0.0082 |           5.8539 |           9.4456 |
[32m[20230113 20:17:09 @agent_ppo2.py:186][0m |          -0.0119 |           5.4937 |           9.4418 |
[32m[20230113 20:17:09 @agent_ppo2.py:186][0m |          -0.0140 |           5.0945 |           9.4364 |
[32m[20230113 20:17:09 @agent_ppo2.py:186][0m |          -0.0149 |           4.8446 |           9.4304 |
[32m[20230113 20:17:09 @agent_ppo2.py:186][0m |          -0.0121 |           4.6887 |           9.4284 |
[32m[20230113 20:17:09 @agent_ppo2.py:186][0m |          -0.0155 |           4.4796 |           9.4148 |
[32m[20230113 20:17:09 @agent_ppo2.py:186][0m |          -0.0157 |           4.3897 |           9.4222 |
[32m[20230113 20:17:09 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:17:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 223.00
[32m[20230113 20:17:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.14
[32m[20230113 20:17:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.23
[32m[20230113 20:17:09 @agent_ppo2.py:144][0m Total time:      32.62 min
[32m[20230113 20:17:09 @agent_ppo2.py:146][0m 3018752 total steps have happened
[32m[20230113 20:17:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1474 --------------------------#
[32m[20230113 20:17:10 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:17:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:10 @agent_ppo2.py:186][0m |           0.0001 |           7.0026 |           9.7661 |
[32m[20230113 20:17:10 @agent_ppo2.py:186][0m |          -0.0076 |           5.1262 |           9.7565 |
[32m[20230113 20:17:10 @agent_ppo2.py:186][0m |          -0.0102 |           4.6353 |           9.7571 |
[32m[20230113 20:17:10 @agent_ppo2.py:186][0m |          -0.0111 |           4.2320 |           9.7523 |
[32m[20230113 20:17:10 @agent_ppo2.py:186][0m |          -0.0162 |           4.0082 |           9.7527 |
[32m[20230113 20:17:10 @agent_ppo2.py:186][0m |          -0.0169 |           3.8345 |           9.7471 |
[32m[20230113 20:17:10 @agent_ppo2.py:186][0m |          -0.0165 |           3.6731 |           9.7489 |
[32m[20230113 20:17:10 @agent_ppo2.py:186][0m |          -0.0168 |           3.5948 |           9.7530 |
[32m[20230113 20:17:10 @agent_ppo2.py:186][0m |          -0.0176 |           3.4450 |           9.7516 |
[32m[20230113 20:17:10 @agent_ppo2.py:186][0m |          -0.0191 |           3.3780 |           9.7526 |
[32m[20230113 20:17:10 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:17:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.07
[32m[20230113 20:17:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.53
[32m[20230113 20:17:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.98
[32m[20230113 20:17:11 @agent_ppo2.py:144][0m Total time:      32.64 min
[32m[20230113 20:17:11 @agent_ppo2.py:146][0m 3020800 total steps have happened
[32m[20230113 20:17:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1475 --------------------------#
[32m[20230113 20:17:11 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:17:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:11 @agent_ppo2.py:186][0m |           0.0023 |           6.6082 |           9.5838 |
[32m[20230113 20:17:11 @agent_ppo2.py:186][0m |          -0.0068 |           5.1556 |           9.5706 |
[32m[20230113 20:17:11 @agent_ppo2.py:186][0m |          -0.0103 |           4.6858 |           9.5681 |
[32m[20230113 20:17:11 @agent_ppo2.py:186][0m |           0.0024 |           4.2808 |           9.5785 |
[32m[20230113 20:17:11 @agent_ppo2.py:186][0m |          -0.0140 |           4.0012 |           9.5623 |
[32m[20230113 20:17:11 @agent_ppo2.py:186][0m |          -0.0097 |           3.8116 |           9.5679 |
[32m[20230113 20:17:12 @agent_ppo2.py:186][0m |          -0.0043 |           3.5959 |           9.5592 |
[32m[20230113 20:17:12 @agent_ppo2.py:186][0m |          -0.0013 |           3.4759 |           9.5570 |
[32m[20230113 20:17:12 @agent_ppo2.py:186][0m |          -0.0106 |           3.3620 |           9.5552 |
[32m[20230113 20:17:12 @agent_ppo2.py:186][0m |          -0.0129 |           3.3127 |           9.5578 |
[32m[20230113 20:17:12 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:17:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.18
[32m[20230113 20:17:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.06
[32m[20230113 20:17:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.50
[32m[20230113 20:17:12 @agent_ppo2.py:144][0m Total time:      32.66 min
[32m[20230113 20:17:12 @agent_ppo2.py:146][0m 3022848 total steps have happened
[32m[20230113 20:17:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1476 --------------------------#
[32m[20230113 20:17:12 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:13 @agent_ppo2.py:186][0m |           0.0009 |           6.6139 |           9.6611 |
[32m[20230113 20:17:13 @agent_ppo2.py:186][0m |          -0.0059 |           4.6897 |           9.6516 |
[32m[20230113 20:17:13 @agent_ppo2.py:186][0m |          -0.0076 |           4.2481 |           9.6433 |
[32m[20230113 20:17:13 @agent_ppo2.py:186][0m |          -0.0094 |           3.8494 |           9.6444 |
[32m[20230113 20:17:13 @agent_ppo2.py:186][0m |          -0.0105 |           3.6204 |           9.6325 |
[32m[20230113 20:17:13 @agent_ppo2.py:186][0m |          -0.0110 |           3.4444 |           9.6374 |
[32m[20230113 20:17:13 @agent_ppo2.py:186][0m |          -0.0129 |           3.3148 |           9.6357 |
[32m[20230113 20:17:13 @agent_ppo2.py:186][0m |          -0.0136 |           3.2273 |           9.6383 |
[32m[20230113 20:17:13 @agent_ppo2.py:186][0m |          -0.0141 |           3.1294 |           9.6375 |
[32m[20230113 20:17:13 @agent_ppo2.py:186][0m |          -0.0156 |           3.1099 |           9.6280 |
[32m[20230113 20:17:13 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:17:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.14
[32m[20230113 20:17:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.29
[32m[20230113 20:17:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.83
[32m[20230113 20:17:13 @agent_ppo2.py:144][0m Total time:      32.68 min
[32m[20230113 20:17:13 @agent_ppo2.py:146][0m 3024896 total steps have happened
[32m[20230113 20:17:13 @agent_ppo2.py:122][0m #------------------------ Iteration 1477 --------------------------#
[32m[20230113 20:17:14 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:14 @agent_ppo2.py:186][0m |          -0.0005 |           5.0854 |           9.5794 |
[32m[20230113 20:17:14 @agent_ppo2.py:186][0m |          -0.0059 |           3.8895 |           9.5621 |
[32m[20230113 20:17:14 @agent_ppo2.py:186][0m |          -0.0078 |           3.5193 |           9.5635 |
[32m[20230113 20:17:14 @agent_ppo2.py:186][0m |          -0.0090 |           3.3151 |           9.5536 |
[32m[20230113 20:17:14 @agent_ppo2.py:186][0m |          -0.0094 |           3.1470 |           9.5415 |
[32m[20230113 20:17:14 @agent_ppo2.py:186][0m |          -0.0131 |           3.0395 |           9.5504 |
[32m[20230113 20:17:14 @agent_ppo2.py:186][0m |          -0.0147 |           2.9120 |           9.5481 |
[32m[20230113 20:17:14 @agent_ppo2.py:186][0m |          -0.0159 |           2.8398 |           9.5382 |
[32m[20230113 20:17:14 @agent_ppo2.py:186][0m |          -0.0165 |           2.7388 |           9.5510 |
[32m[20230113 20:17:14 @agent_ppo2.py:186][0m |          -0.0140 |           2.6987 |           9.5386 |
[32m[20230113 20:17:14 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.48
[32m[20230113 20:17:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.42
[32m[20230113 20:17:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.00
[32m[20230113 20:17:15 @agent_ppo2.py:144][0m Total time:      32.70 min
[32m[20230113 20:17:15 @agent_ppo2.py:146][0m 3026944 total steps have happened
[32m[20230113 20:17:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1478 --------------------------#
[32m[20230113 20:17:15 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:15 @agent_ppo2.py:186][0m |           0.0011 |           5.5951 |           9.6439 |
[32m[20230113 20:17:15 @agent_ppo2.py:186][0m |          -0.0113 |           4.0554 |           9.6144 |
[32m[20230113 20:17:15 @agent_ppo2.py:186][0m |          -0.0086 |           3.5950 |           9.6182 |
[32m[20230113 20:17:15 @agent_ppo2.py:186][0m |          -0.0083 |           3.3571 |           9.6121 |
[32m[20230113 20:17:15 @agent_ppo2.py:186][0m |          -0.0097 |           3.1374 |           9.6195 |
[32m[20230113 20:17:15 @agent_ppo2.py:186][0m |          -0.0143 |           3.0116 |           9.6161 |
[32m[20230113 20:17:15 @agent_ppo2.py:186][0m |          -0.0075 |           2.9368 |           9.6152 |
[32m[20230113 20:17:15 @agent_ppo2.py:186][0m |          -0.0164 |           2.8430 |           9.6263 |
[32m[20230113 20:17:15 @agent_ppo2.py:186][0m |          -0.0134 |           2.8141 |           9.6224 |
[32m[20230113 20:17:15 @agent_ppo2.py:186][0m |          -0.0134 |           2.7003 |           9.6208 |
[32m[20230113 20:17:15 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.86
[32m[20230113 20:17:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.36
[32m[20230113 20:17:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.61
[32m[20230113 20:17:16 @agent_ppo2.py:144][0m Total time:      32.72 min
[32m[20230113 20:17:16 @agent_ppo2.py:146][0m 3028992 total steps have happened
[32m[20230113 20:17:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1479 --------------------------#
[32m[20230113 20:17:16 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:17:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:16 @agent_ppo2.py:186][0m |           0.0027 |           6.3679 |           9.6685 |
[32m[20230113 20:17:16 @agent_ppo2.py:186][0m |          -0.0031 |           4.9878 |           9.6629 |
[32m[20230113 20:17:16 @agent_ppo2.py:186][0m |          -0.0060 |           4.5564 |           9.6604 |
[32m[20230113 20:17:17 @agent_ppo2.py:186][0m |          -0.0078 |           4.2294 |           9.6597 |
[32m[20230113 20:17:17 @agent_ppo2.py:186][0m |          -0.0085 |           4.0022 |           9.6543 |
[32m[20230113 20:17:17 @agent_ppo2.py:186][0m |          -0.0077 |           3.8562 |           9.6471 |
[32m[20230113 20:17:17 @agent_ppo2.py:186][0m |          -0.0100 |           3.7029 |           9.6560 |
[32m[20230113 20:17:17 @agent_ppo2.py:186][0m |          -0.0106 |           3.6314 |           9.6561 |
[32m[20230113 20:17:17 @agent_ppo2.py:186][0m |          -0.0109 |           3.5569 |           9.6568 |
[32m[20230113 20:17:17 @agent_ppo2.py:186][0m |          -0.0113 |           3.4409 |           9.6533 |
[32m[20230113 20:17:17 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.95
[32m[20230113 20:17:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.45
[32m[20230113 20:17:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.40
[32m[20230113 20:17:17 @agent_ppo2.py:144][0m Total time:      32.74 min
[32m[20230113 20:17:17 @agent_ppo2.py:146][0m 3031040 total steps have happened
[32m[20230113 20:17:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1480 --------------------------#
[32m[20230113 20:17:18 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:18 @agent_ppo2.py:186][0m |           0.0020 |           5.3342 |           9.6130 |
[32m[20230113 20:17:18 @agent_ppo2.py:186][0m |          -0.0010 |           4.3456 |           9.6113 |
[32m[20230113 20:17:18 @agent_ppo2.py:186][0m |          -0.0062 |           3.9826 |           9.5994 |
[32m[20230113 20:17:18 @agent_ppo2.py:186][0m |          -0.0074 |           3.7205 |           9.5892 |
[32m[20230113 20:17:18 @agent_ppo2.py:186][0m |          -0.0131 |           3.5510 |           9.6046 |
[32m[20230113 20:17:18 @agent_ppo2.py:186][0m |          -0.0113 |           3.3899 |           9.6070 |
[32m[20230113 20:17:18 @agent_ppo2.py:186][0m |          -0.0080 |           3.3648 |           9.6038 |
[32m[20230113 20:17:18 @agent_ppo2.py:186][0m |          -0.0116 |           3.1561 |           9.5954 |
[32m[20230113 20:17:18 @agent_ppo2.py:186][0m |          -0.0123 |           3.1217 |           9.6052 |
[32m[20230113 20:17:18 @agent_ppo2.py:186][0m |          -0.0130 |           3.0226 |           9.5975 |
[32m[20230113 20:17:18 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.39
[32m[20230113 20:17:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.55
[32m[20230113 20:17:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.93
[32m[20230113 20:17:18 @agent_ppo2.py:144][0m Total time:      32.76 min
[32m[20230113 20:17:18 @agent_ppo2.py:146][0m 3033088 total steps have happened
[32m[20230113 20:17:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1481 --------------------------#
[32m[20230113 20:17:19 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:19 @agent_ppo2.py:186][0m |           0.0036 |           6.2109 |           9.5893 |
[32m[20230113 20:17:19 @agent_ppo2.py:186][0m |          -0.0085 |           5.0157 |           9.5885 |
[32m[20230113 20:17:19 @agent_ppo2.py:186][0m |          -0.0047 |           4.4746 |           9.5768 |
[32m[20230113 20:17:19 @agent_ppo2.py:186][0m |          -0.0057 |           4.1748 |           9.5655 |
[32m[20230113 20:17:19 @agent_ppo2.py:186][0m |           0.0076 |           4.9244 |           9.5700 |
[32m[20230113 20:17:19 @agent_ppo2.py:186][0m |          -0.0110 |           4.0580 |           9.5594 |
[32m[20230113 20:17:19 @agent_ppo2.py:186][0m |          -0.0113 |           3.7361 |           9.5604 |
[32m[20230113 20:17:19 @agent_ppo2.py:186][0m |          -0.0138 |           3.5584 |           9.5581 |
[32m[20230113 20:17:19 @agent_ppo2.py:186][0m |          -0.0324 |           3.5489 |           9.5582 |
[32m[20230113 20:17:19 @agent_ppo2.py:186][0m |          -0.0054 |           3.4381 |           9.5541 |
[32m[20230113 20:17:19 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.29
[32m[20230113 20:17:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.77
[32m[20230113 20:17:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.98
[32m[20230113 20:17:20 @agent_ppo2.py:144][0m Total time:      32.79 min
[32m[20230113 20:17:20 @agent_ppo2.py:146][0m 3035136 total steps have happened
[32m[20230113 20:17:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1482 --------------------------#
[32m[20230113 20:17:20 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:17:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:20 @agent_ppo2.py:186][0m |           0.0048 |           4.9891 |           9.7486 |
[32m[20230113 20:17:20 @agent_ppo2.py:186][0m |          -0.0029 |           3.9969 |           9.7288 |
[32m[20230113 20:17:20 @agent_ppo2.py:186][0m |          -0.0064 |           3.5993 |           9.7228 |
[32m[20230113 20:17:20 @agent_ppo2.py:186][0m |          -0.0049 |           3.4863 |           9.7242 |
[32m[20230113 20:17:20 @agent_ppo2.py:186][0m |          -0.0089 |           3.2153 |           9.7238 |
[32m[20230113 20:17:20 @agent_ppo2.py:186][0m |          -0.0099 |           3.0649 |           9.7254 |
[32m[20230113 20:17:20 @agent_ppo2.py:186][0m |          -0.0113 |           2.9671 |           9.7269 |
[32m[20230113 20:17:20 @agent_ppo2.py:186][0m |          -0.0099 |           2.8508 |           9.7167 |
[32m[20230113 20:17:21 @agent_ppo2.py:186][0m |          -0.0115 |           2.7999 |           9.7300 |
[32m[20230113 20:17:21 @agent_ppo2.py:186][0m |          -0.0104 |           2.7112 |           9.7233 |
[32m[20230113 20:17:21 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:17:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.81
[32m[20230113 20:17:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.92
[32m[20230113 20:17:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.45
[32m[20230113 20:17:21 @agent_ppo2.py:144][0m Total time:      32.81 min
[32m[20230113 20:17:21 @agent_ppo2.py:146][0m 3037184 total steps have happened
[32m[20230113 20:17:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1483 --------------------------#
[32m[20230113 20:17:21 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:21 @agent_ppo2.py:186][0m |           0.0014 |           5.3528 |           9.7154 |
[32m[20230113 20:17:22 @agent_ppo2.py:186][0m |          -0.0027 |           4.3900 |           9.7131 |
[32m[20230113 20:17:22 @agent_ppo2.py:186][0m |          -0.0061 |           4.0117 |           9.7080 |
[32m[20230113 20:17:22 @agent_ppo2.py:186][0m |          -0.0070 |           3.8174 |           9.6995 |
[32m[20230113 20:17:22 @agent_ppo2.py:186][0m |          -0.0100 |           3.5660 |           9.7020 |
[32m[20230113 20:17:22 @agent_ppo2.py:186][0m |          -0.0099 |           3.4896 |           9.6915 |
[32m[20230113 20:17:22 @agent_ppo2.py:186][0m |          -0.0087 |           3.3360 |           9.6847 |
[32m[20230113 20:17:22 @agent_ppo2.py:186][0m |          -0.0115 |           3.2492 |           9.6950 |
[32m[20230113 20:17:22 @agent_ppo2.py:186][0m |          -0.0150 |           3.1735 |           9.6803 |
[32m[20230113 20:17:22 @agent_ppo2.py:186][0m |          -0.0151 |           3.0822 |           9.6869 |
[32m[20230113 20:17:22 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.27
[32m[20230113 20:17:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.81
[32m[20230113 20:17:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.72
[32m[20230113 20:17:22 @agent_ppo2.py:144][0m Total time:      32.83 min
[32m[20230113 20:17:22 @agent_ppo2.py:146][0m 3039232 total steps have happened
[32m[20230113 20:17:22 @agent_ppo2.py:122][0m #------------------------ Iteration 1484 --------------------------#
[32m[20230113 20:17:23 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:23 @agent_ppo2.py:186][0m |           0.0022 |           4.8690 |           9.7307 |
[32m[20230113 20:17:23 @agent_ppo2.py:186][0m |          -0.0014 |           4.1489 |           9.7134 |
[32m[20230113 20:17:23 @agent_ppo2.py:186][0m |          -0.0046 |           3.8909 |           9.7132 |
[32m[20230113 20:17:23 @agent_ppo2.py:186][0m |          -0.0083 |           3.6206 |           9.7055 |
[32m[20230113 20:17:23 @agent_ppo2.py:186][0m |          -0.0076 |           3.4773 |           9.7062 |
[32m[20230113 20:17:23 @agent_ppo2.py:186][0m |          -0.0090 |           3.3675 |           9.7059 |
[32m[20230113 20:17:23 @agent_ppo2.py:186][0m |          -0.0108 |           3.2795 |           9.7049 |
[32m[20230113 20:17:23 @agent_ppo2.py:186][0m |          -0.0106 |           3.1486 |           9.7038 |
[32m[20230113 20:17:23 @agent_ppo2.py:186][0m |          -0.0111 |           3.0970 |           9.7013 |
[32m[20230113 20:17:23 @agent_ppo2.py:186][0m |          -0.0136 |           3.0034 |           9.6984 |
[32m[20230113 20:17:23 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:17:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.21
[32m[20230113 20:17:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.75
[32m[20230113 20:17:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.08
[32m[20230113 20:17:23 @agent_ppo2.py:144][0m Total time:      32.85 min
[32m[20230113 20:17:23 @agent_ppo2.py:146][0m 3041280 total steps have happened
[32m[20230113 20:17:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1485 --------------------------#
[32m[20230113 20:17:24 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:17:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:24 @agent_ppo2.py:186][0m |          -0.0006 |           5.7956 |           9.7269 |
[32m[20230113 20:17:24 @agent_ppo2.py:186][0m |          -0.0046 |           4.4850 |           9.7288 |
[32m[20230113 20:17:24 @agent_ppo2.py:186][0m |          -0.0065 |           4.0108 |           9.7339 |
[32m[20230113 20:17:24 @agent_ppo2.py:186][0m |          -0.0095 |           3.7429 |           9.7302 |
[32m[20230113 20:17:24 @agent_ppo2.py:186][0m |          -0.0068 |           3.5313 |           9.7357 |
[32m[20230113 20:17:24 @agent_ppo2.py:186][0m |          -0.0086 |           3.4460 |           9.7405 |
[32m[20230113 20:17:24 @agent_ppo2.py:186][0m |          -0.0136 |           3.2507 |           9.7327 |
[32m[20230113 20:17:24 @agent_ppo2.py:186][0m |          -0.0127 |           3.1887 |           9.7339 |
[32m[20230113 20:17:24 @agent_ppo2.py:186][0m |          -0.0143 |           3.0976 |           9.7404 |
[32m[20230113 20:17:24 @agent_ppo2.py:186][0m |          -0.0155 |           3.0567 |           9.7351 |
[32m[20230113 20:17:24 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:17:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.95
[32m[20230113 20:17:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.25
[32m[20230113 20:17:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.50
[32m[20230113 20:17:25 @agent_ppo2.py:144][0m Total time:      32.87 min
[32m[20230113 20:17:25 @agent_ppo2.py:146][0m 3043328 total steps have happened
[32m[20230113 20:17:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1486 --------------------------#
[32m[20230113 20:17:25 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:25 @agent_ppo2.py:186][0m |           0.0006 |           6.0879 |           9.9274 |
[32m[20230113 20:17:25 @agent_ppo2.py:186][0m |          -0.0061 |           5.0158 |           9.9110 |
[32m[20230113 20:17:25 @agent_ppo2.py:186][0m |          -0.0068 |           4.4667 |           9.9239 |
[32m[20230113 20:17:25 @agent_ppo2.py:186][0m |          -0.0076 |           4.1866 |           9.9237 |
[32m[20230113 20:17:25 @agent_ppo2.py:186][0m |          -0.0108 |           3.9612 |           9.9079 |
[32m[20230113 20:17:25 @agent_ppo2.py:186][0m |          -0.0122 |           3.8252 |           9.9255 |
[32m[20230113 20:17:25 @agent_ppo2.py:186][0m |          -0.0116 |           3.6728 |           9.9140 |
[32m[20230113 20:17:26 @agent_ppo2.py:186][0m |          -0.0132 |           3.5662 |           9.9243 |
[32m[20230113 20:17:26 @agent_ppo2.py:186][0m |          -0.0139 |           3.4905 |           9.9267 |
[32m[20230113 20:17:26 @agent_ppo2.py:186][0m |          -0.0148 |           3.4204 |           9.9253 |
[32m[20230113 20:17:26 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:17:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.32
[32m[20230113 20:17:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.35
[32m[20230113 20:17:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.52
[32m[20230113 20:17:26 @agent_ppo2.py:144][0m Total time:      32.89 min
[32m[20230113 20:17:26 @agent_ppo2.py:146][0m 3045376 total steps have happened
[32m[20230113 20:17:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1487 --------------------------#
[32m[20230113 20:17:26 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:17:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:27 @agent_ppo2.py:186][0m |           0.0013 |           6.0940 |           9.9237 |
[32m[20230113 20:17:27 @agent_ppo2.py:186][0m |          -0.0054 |           5.0570 |           9.9252 |
[32m[20230113 20:17:27 @agent_ppo2.py:186][0m |          -0.0067 |           4.5519 |           9.9117 |
[32m[20230113 20:17:27 @agent_ppo2.py:186][0m |          -0.0109 |           4.1731 |           9.9008 |
[32m[20230113 20:17:27 @agent_ppo2.py:186][0m |          -0.0113 |           3.9703 |           9.8972 |
[32m[20230113 20:17:27 @agent_ppo2.py:186][0m |          -0.0120 |           3.7790 |           9.9017 |
[32m[20230113 20:17:27 @agent_ppo2.py:186][0m |          -0.0127 |           3.6499 |           9.9082 |
[32m[20230113 20:17:27 @agent_ppo2.py:186][0m |          -0.0143 |           3.5183 |           9.9050 |
[32m[20230113 20:17:27 @agent_ppo2.py:186][0m |          -0.0145 |           3.4054 |           9.9059 |
[32m[20230113 20:17:27 @agent_ppo2.py:186][0m |          -0.0151 |           3.3279 |           9.9019 |
[32m[20230113 20:17:27 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.86
[32m[20230113 20:17:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.99
[32m[20230113 20:17:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.03
[32m[20230113 20:17:27 @agent_ppo2.py:144][0m Total time:      32.91 min
[32m[20230113 20:17:27 @agent_ppo2.py:146][0m 3047424 total steps have happened
[32m[20230113 20:17:27 @agent_ppo2.py:122][0m #------------------------ Iteration 1488 --------------------------#
[32m[20230113 20:17:28 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:17:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:28 @agent_ppo2.py:186][0m |           0.0017 |          11.8106 |          10.0565 |
[32m[20230113 20:17:28 @agent_ppo2.py:186][0m |          -0.0053 |           6.8619 |          10.0387 |
[32m[20230113 20:17:28 @agent_ppo2.py:186][0m |          -0.0063 |           5.5450 |          10.0392 |
[32m[20230113 20:17:28 @agent_ppo2.py:186][0m |          -0.0086 |           5.0123 |          10.0344 |
[32m[20230113 20:17:28 @agent_ppo2.py:186][0m |          -0.0098 |           4.6560 |          10.0181 |
[32m[20230113 20:17:28 @agent_ppo2.py:186][0m |          -0.0104 |           4.4127 |          10.0211 |
[32m[20230113 20:17:28 @agent_ppo2.py:186][0m |          -0.0130 |           4.2374 |          10.0236 |
[32m[20230113 20:17:28 @agent_ppo2.py:186][0m |          -0.0142 |           4.0664 |          10.0176 |
[32m[20230113 20:17:28 @agent_ppo2.py:186][0m |          -0.0148 |           3.9087 |          10.0181 |
[32m[20230113 20:17:28 @agent_ppo2.py:186][0m |          -0.0160 |           3.7720 |          10.0185 |
[32m[20230113 20:17:28 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 20:17:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 141.68
[32m[20230113 20:17:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.50
[32m[20230113 20:17:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.24
[32m[20230113 20:17:29 @agent_ppo2.py:144][0m Total time:      32.94 min
[32m[20230113 20:17:29 @agent_ppo2.py:146][0m 3049472 total steps have happened
[32m[20230113 20:17:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1489 --------------------------#
[32m[20230113 20:17:29 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:17:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:29 @agent_ppo2.py:186][0m |           0.0014 |           6.6322 |           9.7039 |
[32m[20230113 20:17:29 @agent_ppo2.py:186][0m |          -0.0060 |           5.0968 |           9.6894 |
[32m[20230113 20:17:29 @agent_ppo2.py:186][0m |          -0.0096 |           4.5245 |           9.6853 |
[32m[20230113 20:17:29 @agent_ppo2.py:186][0m |          -0.0111 |           4.3036 |           9.6864 |
[32m[20230113 20:17:29 @agent_ppo2.py:186][0m |          -0.0117 |           3.9925 |           9.6855 |
[32m[20230113 20:17:29 @agent_ppo2.py:186][0m |          -0.0121 |           3.9099 |           9.6852 |
[32m[20230113 20:17:29 @agent_ppo2.py:186][0m |          -0.0020 |           3.8112 |           9.6810 |
[32m[20230113 20:17:29 @agent_ppo2.py:186][0m |          -0.0140 |           3.6488 |           9.6687 |
[32m[20230113 20:17:29 @agent_ppo2.py:186][0m |          -0.0137 |           3.6073 |           9.6753 |
[32m[20230113 20:17:30 @agent_ppo2.py:186][0m |          -0.0092 |           3.5487 |           9.6740 |
[32m[20230113 20:17:30 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:17:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.82
[32m[20230113 20:17:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.18
[32m[20230113 20:17:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.54
[32m[20230113 20:17:30 @agent_ppo2.py:144][0m Total time:      32.96 min
[32m[20230113 20:17:30 @agent_ppo2.py:146][0m 3051520 total steps have happened
[32m[20230113 20:17:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1490 --------------------------#
[32m[20230113 20:17:30 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:17:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:30 @agent_ppo2.py:186][0m |          -0.0001 |          13.9121 |           9.9462 |
[32m[20230113 20:17:31 @agent_ppo2.py:186][0m |          -0.0040 |           7.5905 |           9.9538 |
[32m[20230113 20:17:31 @agent_ppo2.py:186][0m |          -0.0096 |           6.2174 |           9.9527 |
[32m[20230113 20:17:31 @agent_ppo2.py:186][0m |          -0.0082 |           5.6415 |           9.9634 |
[32m[20230113 20:17:31 @agent_ppo2.py:186][0m |          -0.0099 |           5.1158 |           9.9579 |
[32m[20230113 20:17:31 @agent_ppo2.py:186][0m |          -0.0126 |           4.7423 |           9.9625 |
[32m[20230113 20:17:31 @agent_ppo2.py:186][0m |          -0.0122 |           4.4416 |           9.9619 |
[32m[20230113 20:17:31 @agent_ppo2.py:186][0m |          -0.0116 |           4.2104 |           9.9629 |
[32m[20230113 20:17:31 @agent_ppo2.py:186][0m |          -0.0155 |           3.9798 |           9.9659 |
[32m[20230113 20:17:31 @agent_ppo2.py:186][0m |          -0.0119 |           3.8408 |           9.9640 |
[32m[20230113 20:17:31 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:17:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 131.74
[32m[20230113 20:17:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.94
[32m[20230113 20:17:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.85
[32m[20230113 20:17:31 @agent_ppo2.py:144][0m Total time:      32.98 min
[32m[20230113 20:17:31 @agent_ppo2.py:146][0m 3053568 total steps have happened
[32m[20230113 20:17:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1491 --------------------------#
[32m[20230113 20:17:32 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:17:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:32 @agent_ppo2.py:186][0m |          -0.0006 |           6.3591 |           9.9457 |
[32m[20230113 20:17:32 @agent_ppo2.py:186][0m |          -0.0071 |           4.6407 |           9.9359 |
[32m[20230113 20:17:32 @agent_ppo2.py:186][0m |          -0.0103 |           4.2082 |           9.9324 |
[32m[20230113 20:17:32 @agent_ppo2.py:186][0m |          -0.0096 |           3.9563 |           9.9281 |
[32m[20230113 20:17:32 @agent_ppo2.py:186][0m |          -0.0143 |           3.7583 |           9.9309 |
[32m[20230113 20:17:32 @agent_ppo2.py:186][0m |          -0.0132 |           3.6101 |           9.9340 |
[32m[20230113 20:17:32 @agent_ppo2.py:186][0m |          -0.0150 |           3.5137 |           9.9305 |
[32m[20230113 20:17:32 @agent_ppo2.py:186][0m |          -0.0155 |           3.3840 |           9.9369 |
[32m[20230113 20:17:32 @agent_ppo2.py:186][0m |          -0.0159 |           3.3075 |           9.9276 |
[32m[20230113 20:17:32 @agent_ppo2.py:186][0m |          -0.0163 |           3.2458 |           9.9325 |
[32m[20230113 20:17:32 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:17:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.30
[32m[20230113 20:17:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.34
[32m[20230113 20:17:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.46
[32m[20230113 20:17:33 @agent_ppo2.py:144][0m Total time:      33.00 min
[32m[20230113 20:17:33 @agent_ppo2.py:146][0m 3055616 total steps have happened
[32m[20230113 20:17:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1492 --------------------------#
[32m[20230113 20:17:33 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:33 @agent_ppo2.py:186][0m |           0.0018 |           5.3482 |           9.9261 |
[32m[20230113 20:17:33 @agent_ppo2.py:186][0m |          -0.0079 |           4.2935 |           9.9222 |
[32m[20230113 20:17:33 @agent_ppo2.py:186][0m |          -0.0092 |           3.9351 |           9.9237 |
[32m[20230113 20:17:33 @agent_ppo2.py:186][0m |          -0.0120 |           3.7342 |           9.9141 |
[32m[20230113 20:17:33 @agent_ppo2.py:186][0m |          -0.0095 |           3.5695 |           9.9136 |
[32m[20230113 20:17:33 @agent_ppo2.py:186][0m |          -0.0112 |           3.4522 |           9.9174 |
[32m[20230113 20:17:33 @agent_ppo2.py:186][0m |          -0.0132 |           3.4287 |           9.9115 |
[32m[20230113 20:17:33 @agent_ppo2.py:186][0m |          -0.0146 |           3.2925 |           9.9114 |
[32m[20230113 20:17:33 @agent_ppo2.py:186][0m |          -0.0160 |           3.2410 |           9.9115 |
[32m[20230113 20:17:33 @agent_ppo2.py:186][0m |          -0.0139 |           3.1756 |           9.9111 |
[32m[20230113 20:17:33 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.71
[32m[20230113 20:17:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.77
[32m[20230113 20:17:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.74
[32m[20230113 20:17:34 @agent_ppo2.py:144][0m Total time:      33.02 min
[32m[20230113 20:17:34 @agent_ppo2.py:146][0m 3057664 total steps have happened
[32m[20230113 20:17:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1493 --------------------------#
[32m[20230113 20:17:34 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 20:17:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:34 @agent_ppo2.py:186][0m |           0.0012 |          15.2834 |          10.1407 |
[32m[20230113 20:17:34 @agent_ppo2.py:186][0m |          -0.0056 |           6.7055 |          10.1370 |
[32m[20230113 20:17:34 @agent_ppo2.py:186][0m |          -0.0109 |           5.3231 |          10.1305 |
[32m[20230113 20:17:34 @agent_ppo2.py:186][0m |          -0.0118 |           4.7986 |          10.1142 |
[32m[20230113 20:17:34 @agent_ppo2.py:186][0m |          -0.0127 |           4.4101 |          10.1243 |
[32m[20230113 20:17:34 @agent_ppo2.py:186][0m |          -0.0147 |           4.0957 |          10.1260 |
[32m[20230113 20:17:34 @agent_ppo2.py:186][0m |          -0.0149 |           3.8754 |          10.1191 |
[32m[20230113 20:17:34 @agent_ppo2.py:186][0m |          -0.0173 |           3.6686 |          10.1173 |
[32m[20230113 20:17:34 @agent_ppo2.py:186][0m |          -0.0174 |           3.5019 |          10.1179 |
[32m[20230113 20:17:35 @agent_ppo2.py:186][0m |          -0.0169 |           3.3829 |          10.1156 |
[32m[20230113 20:17:35 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 20:17:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 125.41
[32m[20230113 20:17:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 225.90
[32m[20230113 20:17:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.39
[32m[20230113 20:17:35 @agent_ppo2.py:144][0m Total time:      33.04 min
[32m[20230113 20:17:35 @agent_ppo2.py:146][0m 3059712 total steps have happened
[32m[20230113 20:17:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1494 --------------------------#
[32m[20230113 20:17:35 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 20:17:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:35 @agent_ppo2.py:186][0m |           0.0046 |          34.1937 |           9.8755 |
[32m[20230113 20:17:35 @agent_ppo2.py:186][0m |          -0.0103 |          19.9835 |           9.8789 |
[32m[20230113 20:17:35 @agent_ppo2.py:186][0m |          -0.0071 |          14.4317 |           9.8757 |
[32m[20230113 20:17:35 @agent_ppo2.py:186][0m |          -0.0183 |          11.3963 |           9.8662 |
[32m[20230113 20:17:35 @agent_ppo2.py:186][0m |          -0.0054 |           9.0577 |           9.8562 |
[32m[20230113 20:17:35 @agent_ppo2.py:186][0m |          -0.0162 |           7.6344 |           9.8663 |
[32m[20230113 20:17:35 @agent_ppo2.py:186][0m |          -0.0170 |           6.9708 |           9.8617 |
[32m[20230113 20:17:36 @agent_ppo2.py:186][0m |          -0.0162 |           6.4434 |           9.8643 |
[32m[20230113 20:17:36 @agent_ppo2.py:186][0m |          -0.0190 |           5.9579 |           9.8612 |
[32m[20230113 20:17:36 @agent_ppo2.py:186][0m |          -0.0193 |           5.6403 |           9.8613 |
[32m[20230113 20:17:36 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:17:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 116.15
[32m[20230113 20:17:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.47
[32m[20230113 20:17:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.94
[32m[20230113 20:17:36 @agent_ppo2.py:144][0m Total time:      33.06 min
[32m[20230113 20:17:36 @agent_ppo2.py:146][0m 3061760 total steps have happened
[32m[20230113 20:17:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1495 --------------------------#
[32m[20230113 20:17:36 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:36 @agent_ppo2.py:186][0m |           0.0199 |          18.5907 |           9.9481 |
[32m[20230113 20:17:37 @agent_ppo2.py:186][0m |          -0.0018 |          10.3632 |           9.9494 |
[32m[20230113 20:17:37 @agent_ppo2.py:186][0m |          -0.0061 |           8.4785 |           9.9335 |
[32m[20230113 20:17:37 @agent_ppo2.py:186][0m |          -0.0086 |           7.4392 |           9.9271 |
[32m[20230113 20:17:37 @agent_ppo2.py:186][0m |          -0.0083 |           6.7840 |           9.9273 |
[32m[20230113 20:17:37 @agent_ppo2.py:186][0m |          -0.0031 |           6.6289 |           9.9171 |
[32m[20230113 20:17:37 @agent_ppo2.py:186][0m |          -0.0072 |           6.1511 |           9.9193 |
[32m[20230113 20:17:37 @agent_ppo2.py:186][0m |          -0.0184 |           5.6306 |           9.9273 |
[32m[20230113 20:17:37 @agent_ppo2.py:186][0m |          -0.0352 |           5.4111 |           9.9281 |
[32m[20230113 20:17:37 @agent_ppo2.py:186][0m |          -0.0127 |           5.1407 |           9.9274 |
[32m[20230113 20:17:37 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:17:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.14
[32m[20230113 20:17:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.35
[32m[20230113 20:17:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 149.58
[32m[20230113 20:17:37 @agent_ppo2.py:144][0m Total time:      33.08 min
[32m[20230113 20:17:37 @agent_ppo2.py:146][0m 3063808 total steps have happened
[32m[20230113 20:17:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1496 --------------------------#
[32m[20230113 20:17:38 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:17:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:38 @agent_ppo2.py:186][0m |           0.0015 |          18.5077 |          10.0196 |
[32m[20230113 20:17:38 @agent_ppo2.py:186][0m |          -0.0081 |          11.6620 |          10.0180 |
[32m[20230113 20:17:38 @agent_ppo2.py:186][0m |          -0.0079 |           9.9511 |          10.0257 |
[32m[20230113 20:17:38 @agent_ppo2.py:186][0m |          -0.0142 |           8.9681 |          10.0123 |
[32m[20230113 20:17:38 @agent_ppo2.py:186][0m |          -0.0101 |           8.8064 |          10.0041 |
[32m[20230113 20:17:38 @agent_ppo2.py:186][0m |          -0.0160 |           7.7262 |          10.0050 |
[32m[20230113 20:17:38 @agent_ppo2.py:186][0m |          -0.0169 |           7.3217 |           9.9983 |
[32m[20230113 20:17:38 @agent_ppo2.py:186][0m |          -0.0130 |           7.0863 |           9.9992 |
[32m[20230113 20:17:38 @agent_ppo2.py:186][0m |          -0.0189 |           6.7359 |           9.9892 |
[32m[20230113 20:17:38 @agent_ppo2.py:186][0m |          -0.0179 |           6.5294 |           9.9899 |
[32m[20230113 20:17:38 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:17:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.26
[32m[20230113 20:17:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.01
[32m[20230113 20:17:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 178.61
[32m[20230113 20:17:39 @agent_ppo2.py:144][0m Total time:      33.10 min
[32m[20230113 20:17:39 @agent_ppo2.py:146][0m 3065856 total steps have happened
[32m[20230113 20:17:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1497 --------------------------#
[32m[20230113 20:17:39 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:39 @agent_ppo2.py:186][0m |           0.0005 |           7.8405 |           9.8719 |
[32m[20230113 20:17:39 @agent_ppo2.py:186][0m |          -0.0121 |           6.2196 |           9.8520 |
[32m[20230113 20:17:39 @agent_ppo2.py:186][0m |          -0.0001 |           5.5876 |           9.8550 |
[32m[20230113 20:17:39 @agent_ppo2.py:186][0m |          -0.0098 |           5.0914 |           9.8448 |
[32m[20230113 20:17:39 @agent_ppo2.py:186][0m |          -0.0093 |           4.7579 |           9.8521 |
[32m[20230113 20:17:39 @agent_ppo2.py:186][0m |          -0.0070 |           4.5417 |           9.8456 |
[32m[20230113 20:17:39 @agent_ppo2.py:186][0m |          -0.0115 |           4.3451 |           9.8407 |
[32m[20230113 20:17:39 @agent_ppo2.py:186][0m |          -0.0145 |           4.2146 |           9.8453 |
[32m[20230113 20:17:39 @agent_ppo2.py:186][0m |          -0.0142 |           4.0398 |           9.8434 |
[32m[20230113 20:17:39 @agent_ppo2.py:186][0m |          -0.0235 |           3.9446 |           9.8394 |
[32m[20230113 20:17:39 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.75
[32m[20230113 20:17:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.10
[32m[20230113 20:17:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.11
[32m[20230113 20:17:40 @agent_ppo2.py:144][0m Total time:      33.12 min
[32m[20230113 20:17:40 @agent_ppo2.py:146][0m 3067904 total steps have happened
[32m[20230113 20:17:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1498 --------------------------#
[32m[20230113 20:17:40 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 20:17:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:40 @agent_ppo2.py:186][0m |           0.0063 |          39.0958 |           9.7113 |
[32m[20230113 20:17:40 @agent_ppo2.py:186][0m |          -0.0035 |          20.4620 |           9.7104 |
[32m[20230113 20:17:40 @agent_ppo2.py:186][0m |           0.0215 |          13.5180 |           9.7031 |
[32m[20230113 20:17:40 @agent_ppo2.py:186][0m |          -0.0128 |           9.2244 |           9.6905 |
[32m[20230113 20:17:40 @agent_ppo2.py:186][0m |          -0.0140 |           6.2394 |           9.6907 |
[32m[20230113 20:17:40 @agent_ppo2.py:186][0m |          -0.0153 |           5.1234 |           9.6984 |
[32m[20230113 20:17:41 @agent_ppo2.py:186][0m |          -0.0178 |           4.6563 |           9.6960 |
[32m[20230113 20:17:41 @agent_ppo2.py:186][0m |          -0.0239 |           4.3307 |           9.6970 |
[32m[20230113 20:17:41 @agent_ppo2.py:186][0m |          -0.0168 |           4.0486 |           9.6921 |
[32m[20230113 20:17:41 @agent_ppo2.py:186][0m |          -0.0088 |           4.0604 |           9.6927 |
[32m[20230113 20:17:41 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:17:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 147.24
[32m[20230113 20:17:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.93
[32m[20230113 20:17:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.36
[32m[20230113 20:17:41 @agent_ppo2.py:144][0m Total time:      33.14 min
[32m[20230113 20:17:41 @agent_ppo2.py:146][0m 3069952 total steps have happened
[32m[20230113 20:17:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1499 --------------------------#
[32m[20230113 20:17:41 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:17:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:42 @agent_ppo2.py:186][0m |           0.0017 |           8.0388 |          10.0236 |
[32m[20230113 20:17:42 @agent_ppo2.py:186][0m |          -0.0080 |           6.1311 |          10.0118 |
[32m[20230113 20:17:42 @agent_ppo2.py:186][0m |          -0.0086 |           5.4221 |          10.0060 |
[32m[20230113 20:17:42 @agent_ppo2.py:186][0m |          -0.0094 |           4.8853 |           9.9956 |
[32m[20230113 20:17:42 @agent_ppo2.py:186][0m |          -0.0124 |           4.5520 |           9.9891 |
[32m[20230113 20:17:42 @agent_ppo2.py:186][0m |          -0.0138 |           4.3296 |           9.9894 |
[32m[20230113 20:17:42 @agent_ppo2.py:186][0m |          -0.0130 |           4.1521 |           9.9952 |
[32m[20230113 20:17:42 @agent_ppo2.py:186][0m |          -0.0115 |           3.9678 |           9.9927 |
[32m[20230113 20:17:42 @agent_ppo2.py:186][0m |          -0.0164 |           3.8739 |           9.9927 |
[32m[20230113 20:17:42 @agent_ppo2.py:186][0m |          -0.0143 |           3.7513 |           9.9935 |
[32m[20230113 20:17:42 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:17:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.47
[32m[20230113 20:17:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.35
[32m[20230113 20:17:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.08
[32m[20230113 20:17:42 @agent_ppo2.py:104][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 258.49
[32m[20230113 20:17:42 @agent_ppo2.py:144][0m Total time:      33.16 min
[32m[20230113 20:17:42 @agent_ppo2.py:146][0m 3072000 total steps have happened
[32m[20230113 20:17:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1500 --------------------------#
[32m[20230113 20:17:43 @agent_ppo2.py:128][0m Sampling time: 0.52 s by 1 slaves
[32m[20230113 20:17:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:43 @agent_ppo2.py:186][0m |           0.0001 |          14.1198 |          10.0870 |
[32m[20230113 20:17:43 @agent_ppo2.py:186][0m |          -0.0048 |           8.4383 |          10.0767 |
[32m[20230113 20:17:43 @agent_ppo2.py:186][0m |          -0.0085 |           6.7505 |          10.0604 |
[32m[20230113 20:17:43 @agent_ppo2.py:186][0m |          -0.0120 |           5.9171 |          10.0577 |
[32m[20230113 20:17:43 @agent_ppo2.py:186][0m |          -0.0128 |           5.4097 |          10.0525 |
[32m[20230113 20:17:43 @agent_ppo2.py:186][0m |          -0.0114 |           5.0530 |          10.0515 |
[32m[20230113 20:17:43 @agent_ppo2.py:186][0m |          -0.0139 |           4.7783 |          10.0400 |
[32m[20230113 20:17:43 @agent_ppo2.py:186][0m |          -0.0126 |           4.5322 |          10.0444 |
[32m[20230113 20:17:43 @agent_ppo2.py:186][0m |          -0.0106 |           4.3613 |          10.0358 |
[32m[20230113 20:17:43 @agent_ppo2.py:186][0m |          -0.0177 |           4.1962 |          10.0350 |
[32m[20230113 20:17:43 @agent_ppo2.py:131][0m Policy update time: 0.53 s
[32m[20230113 20:17:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 141.56
[32m[20230113 20:17:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.36
[32m[20230113 20:17:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 146.95
[32m[20230113 20:17:44 @agent_ppo2.py:144][0m Total time:      33.19 min
[32m[20230113 20:17:44 @agent_ppo2.py:146][0m 3074048 total steps have happened
[32m[20230113 20:17:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1501 --------------------------#
[32m[20230113 20:17:44 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 20:17:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:44 @agent_ppo2.py:186][0m |           0.0012 |          16.2115 |           9.6684 |
[32m[20230113 20:17:44 @agent_ppo2.py:186][0m |          -0.0012 |           6.8627 |           9.6596 |
[32m[20230113 20:17:44 @agent_ppo2.py:186][0m |          -0.0093 |           5.4566 |           9.6613 |
[32m[20230113 20:17:44 @agent_ppo2.py:186][0m |          -0.0109 |           4.8965 |           9.6546 |
[32m[20230113 20:17:44 @agent_ppo2.py:186][0m |          -0.0104 |           4.5113 |           9.6606 |
[32m[20230113 20:17:44 @agent_ppo2.py:186][0m |          -0.0115 |           4.2701 |           9.6577 |
[32m[20230113 20:17:44 @agent_ppo2.py:186][0m |          -0.0053 |           4.0955 |           9.6564 |
[32m[20230113 20:17:44 @agent_ppo2.py:186][0m |          -0.0047 |           3.9439 |           9.6543 |
[32m[20230113 20:17:45 @agent_ppo2.py:186][0m |          -0.0153 |           3.7385 |           9.6459 |
[32m[20230113 20:17:45 @agent_ppo2.py:186][0m |          -0.0085 |           3.6293 |           9.6488 |
[32m[20230113 20:17:45 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 20:17:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 133.25
[32m[20230113 20:17:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.03
[32m[20230113 20:17:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.62
[32m[20230113 20:17:45 @agent_ppo2.py:144][0m Total time:      33.21 min
[32m[20230113 20:17:45 @agent_ppo2.py:146][0m 3076096 total steps have happened
[32m[20230113 20:17:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1502 --------------------------#
[32m[20230113 20:17:45 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:17:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:45 @agent_ppo2.py:186][0m |           0.0016 |           7.4478 |           9.8614 |
[32m[20230113 20:17:46 @agent_ppo2.py:186][0m |          -0.0026 |           5.4147 |           9.8561 |
[32m[20230113 20:17:46 @agent_ppo2.py:186][0m |          -0.0084 |           4.8042 |           9.8529 |
[32m[20230113 20:17:46 @agent_ppo2.py:186][0m |          -0.0069 |           4.3562 |           9.8498 |
[32m[20230113 20:17:46 @agent_ppo2.py:186][0m |          -0.0084 |           4.0956 |           9.8403 |
[32m[20230113 20:17:46 @agent_ppo2.py:186][0m |          -0.0089 |           3.9098 |           9.8489 |
[32m[20230113 20:17:46 @agent_ppo2.py:186][0m |          -0.0109 |           3.6834 |           9.8444 |
[32m[20230113 20:17:46 @agent_ppo2.py:186][0m |          -0.0120 |           3.5685 |           9.8516 |
[32m[20230113 20:17:46 @agent_ppo2.py:186][0m |          -0.0068 |           3.5541 |           9.8523 |
[32m[20230113 20:17:46 @agent_ppo2.py:186][0m |          -0.0123 |           3.3922 |           9.8500 |
[32m[20230113 20:17:46 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:17:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.15
[32m[20230113 20:17:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.76
[32m[20230113 20:17:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.71
[32m[20230113 20:17:46 @agent_ppo2.py:144][0m Total time:      33.23 min
[32m[20230113 20:17:46 @agent_ppo2.py:146][0m 3078144 total steps have happened
[32m[20230113 20:17:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1503 --------------------------#
[32m[20230113 20:17:47 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:17:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:47 @agent_ppo2.py:186][0m |           0.0015 |           7.9429 |          10.0300 |
[32m[20230113 20:17:47 @agent_ppo2.py:186][0m |          -0.0046 |           6.3950 |          10.0273 |
[32m[20230113 20:17:47 @agent_ppo2.py:186][0m |          -0.0066 |           5.7343 |          10.0293 |
[32m[20230113 20:17:47 @agent_ppo2.py:186][0m |          -0.0101 |           5.3308 |          10.0216 |
[32m[20230113 20:17:47 @agent_ppo2.py:186][0m |          -0.0097 |           5.0560 |          10.0121 |
[32m[20230113 20:17:47 @agent_ppo2.py:186][0m |          -0.0123 |           4.8685 |          10.0160 |
[32m[20230113 20:17:47 @agent_ppo2.py:186][0m |          -0.0117 |           4.7379 |          10.0025 |
[32m[20230113 20:17:47 @agent_ppo2.py:186][0m |          -0.0114 |           4.5242 |          10.0107 |
[32m[20230113 20:17:47 @agent_ppo2.py:186][0m |          -0.0128 |           4.4372 |          10.0116 |
[32m[20230113 20:17:47 @agent_ppo2.py:186][0m |          -0.0149 |           4.2820 |          10.0028 |
[32m[20230113 20:17:47 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:17:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.75
[32m[20230113 20:17:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.92
[32m[20230113 20:17:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.82
[32m[20230113 20:17:48 @agent_ppo2.py:144][0m Total time:      33.25 min
[32m[20230113 20:17:48 @agent_ppo2.py:146][0m 3080192 total steps have happened
[32m[20230113 20:17:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1504 --------------------------#
[32m[20230113 20:17:48 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:17:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:48 @agent_ppo2.py:186][0m |          -0.0010 |           6.7263 |           9.8465 |
[32m[20230113 20:17:48 @agent_ppo2.py:186][0m |          -0.0071 |           5.7351 |           9.8420 |
[32m[20230113 20:17:48 @agent_ppo2.py:186][0m |          -0.0080 |           5.2919 |           9.8356 |
[32m[20230113 20:17:48 @agent_ppo2.py:186][0m |          -0.0102 |           4.9848 |           9.8409 |
[32m[20230113 20:17:48 @agent_ppo2.py:186][0m |          -0.0121 |           4.8131 |           9.8401 |
[32m[20230113 20:17:48 @agent_ppo2.py:186][0m |          -0.0110 |           4.6129 |           9.8290 |
[32m[20230113 20:17:48 @agent_ppo2.py:186][0m |          -0.0128 |           4.4303 |           9.8419 |
[32m[20230113 20:17:48 @agent_ppo2.py:186][0m |          -0.0148 |           4.3172 |           9.8257 |
[32m[20230113 20:17:48 @agent_ppo2.py:186][0m |          -0.0135 |           4.1876 |           9.8411 |
[32m[20230113 20:17:48 @agent_ppo2.py:186][0m |          -0.0149 |           4.1061 |           9.8404 |
[32m[20230113 20:17:48 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:17:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.20
[32m[20230113 20:17:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.52
[32m[20230113 20:17:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 230.74
[32m[20230113 20:17:49 @agent_ppo2.py:144][0m Total time:      33.27 min
[32m[20230113 20:17:49 @agent_ppo2.py:146][0m 3082240 total steps have happened
[32m[20230113 20:17:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1505 --------------------------#
[32m[20230113 20:17:49 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 20:17:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:49 @agent_ppo2.py:186][0m |          -0.0006 |          16.2394 |           9.7305 |
[32m[20230113 20:17:49 @agent_ppo2.py:186][0m |          -0.0025 |           9.5205 |           9.7263 |
[32m[20230113 20:17:49 @agent_ppo2.py:186][0m |          -0.0073 |           7.3341 |           9.7138 |
[32m[20230113 20:17:49 @agent_ppo2.py:186][0m |          -0.0107 |           6.1036 |           9.7086 |
[32m[20230113 20:17:49 @agent_ppo2.py:186][0m |          -0.0105 |           5.4275 |           9.7016 |
[32m[20230113 20:17:49 @agent_ppo2.py:186][0m |          -0.0071 |           4.8620 |           9.6973 |
[32m[20230113 20:17:49 @agent_ppo2.py:186][0m |          -0.0098 |           4.5478 |           9.6948 |
[32m[20230113 20:17:49 @agent_ppo2.py:186][0m |          -0.0118 |           4.2524 |           9.6925 |
[32m[20230113 20:17:49 @agent_ppo2.py:186][0m |          -0.0174 |           4.0053 |           9.6873 |
[32m[20230113 20:17:50 @agent_ppo2.py:186][0m |          -0.0167 |           3.7587 |           9.6821 |
[32m[20230113 20:17:50 @agent_ppo2.py:131][0m Policy update time: 0.36 s
[32m[20230113 20:17:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 127.33
[32m[20230113 20:17:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 226.60
[32m[20230113 20:17:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.75
[32m[20230113 20:17:50 @agent_ppo2.py:144][0m Total time:      33.29 min
[32m[20230113 20:17:50 @agent_ppo2.py:146][0m 3084288 total steps have happened
[32m[20230113 20:17:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1506 --------------------------#
[32m[20230113 20:17:50 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:17:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:50 @agent_ppo2.py:186][0m |          -0.0021 |          20.8959 |           9.8536 |
[32m[20230113 20:17:50 @agent_ppo2.py:186][0m |          -0.0109 |          10.9668 |           9.8460 |
[32m[20230113 20:17:50 @agent_ppo2.py:186][0m |          -0.0134 |           8.1026 |           9.8477 |
[32m[20230113 20:17:50 @agent_ppo2.py:186][0m |          -0.0141 |           6.7571 |           9.8478 |
[32m[20230113 20:17:50 @agent_ppo2.py:186][0m |          -0.0158 |           6.0418 |           9.8451 |
[32m[20230113 20:17:50 @agent_ppo2.py:186][0m |          -0.0180 |           5.4039 |           9.8500 |
[32m[20230113 20:17:50 @agent_ppo2.py:186][0m |          -0.0188 |           4.9958 |           9.8505 |
[32m[20230113 20:17:50 @agent_ppo2.py:186][0m |          -0.0201 |           4.5962 |           9.8481 |
[32m[20230113 20:17:51 @agent_ppo2.py:186][0m |          -0.0171 |           4.4214 |           9.8437 |
[32m[20230113 20:17:51 @agent_ppo2.py:186][0m |          -0.0198 |           4.1484 |           9.8490 |
[32m[20230113 20:17:51 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 20:17:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 108.74
[32m[20230113 20:17:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.23
[32m[20230113 20:17:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.83
[32m[20230113 20:17:51 @agent_ppo2.py:144][0m Total time:      33.31 min
[32m[20230113 20:17:51 @agent_ppo2.py:146][0m 3086336 total steps have happened
[32m[20230113 20:17:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1507 --------------------------#
[32m[20230113 20:17:51 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:51 @agent_ppo2.py:186][0m |           0.0043 |          12.4927 |           9.7635 |
[32m[20230113 20:17:52 @agent_ppo2.py:186][0m |           0.0011 |           8.3258 |           9.7694 |
[32m[20230113 20:17:52 @agent_ppo2.py:186][0m |           0.0079 |           7.2746 |           9.7502 |
[32m[20230113 20:17:52 @agent_ppo2.py:186][0m |          -0.0085 |           6.7503 |           9.7692 |
[32m[20230113 20:17:52 @agent_ppo2.py:186][0m |          -0.0269 |           6.2510 |           9.7492 |
[32m[20230113 20:17:52 @agent_ppo2.py:186][0m |          -0.0031 |           6.1517 |           9.7419 |
[32m[20230113 20:17:52 @agent_ppo2.py:186][0m |          -0.0241 |           5.7263 |           9.7507 |
[32m[20230113 20:17:52 @agent_ppo2.py:186][0m |          -0.0067 |           5.4866 |           9.7503 |
[32m[20230113 20:17:52 @agent_ppo2.py:186][0m |          -0.0089 |           5.3853 |           9.7414 |
[32m[20230113 20:17:52 @agent_ppo2.py:186][0m |          -0.0148 |           5.1319 |           9.7441 |
[32m[20230113 20:17:52 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.25
[32m[20230113 20:17:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.92
[32m[20230113 20:17:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.10
[32m[20230113 20:17:52 @agent_ppo2.py:144][0m Total time:      33.33 min
[32m[20230113 20:17:52 @agent_ppo2.py:146][0m 3088384 total steps have happened
[32m[20230113 20:17:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1508 --------------------------#
[32m[20230113 20:17:53 @agent_ppo2.py:128][0m Sampling time: 0.35 s by 1 slaves
[32m[20230113 20:17:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:53 @agent_ppo2.py:186][0m |           0.0025 |          19.8226 |           9.9649 |
[32m[20230113 20:17:53 @agent_ppo2.py:186][0m |          -0.0041 |          10.9582 |           9.9649 |
[32m[20230113 20:17:53 @agent_ppo2.py:186][0m |          -0.0090 |           8.4361 |           9.9489 |
[32m[20230113 20:17:53 @agent_ppo2.py:186][0m |          -0.0113 |           7.4199 |           9.9546 |
[32m[20230113 20:17:53 @agent_ppo2.py:186][0m |          -0.0135 |           6.6788 |           9.9497 |
[32m[20230113 20:17:53 @agent_ppo2.py:186][0m |          -0.0150 |           6.2537 |           9.9530 |
[32m[20230113 20:17:53 @agent_ppo2.py:186][0m |          -0.0168 |           5.8232 |           9.9480 |
[32m[20230113 20:17:53 @agent_ppo2.py:186][0m |          -0.0169 |           5.5135 |           9.9450 |
[32m[20230113 20:17:53 @agent_ppo2.py:186][0m |          -0.0186 |           5.3866 |           9.9483 |
[32m[20230113 20:17:53 @agent_ppo2.py:186][0m |          -0.0191 |           5.1504 |           9.9441 |
[32m[20230113 20:17:53 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:17:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 114.86
[32m[20230113 20:17:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 223.29
[32m[20230113 20:17:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.69
[32m[20230113 20:17:53 @agent_ppo2.py:144][0m Total time:      33.35 min
[32m[20230113 20:17:53 @agent_ppo2.py:146][0m 3090432 total steps have happened
[32m[20230113 20:17:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1509 --------------------------#
[32m[20230113 20:17:54 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:17:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:54 @agent_ppo2.py:186][0m |           0.0182 |           9.1215 |           9.6963 |
[32m[20230113 20:17:54 @agent_ppo2.py:186][0m |          -0.0037 |           5.9843 |           9.6766 |
[32m[20230113 20:17:54 @agent_ppo2.py:186][0m |          -0.0219 |           5.2945 |           9.6838 |
[32m[20230113 20:17:54 @agent_ppo2.py:186][0m |          -0.0228 |           4.8407 |           9.6653 |
[32m[20230113 20:17:54 @agent_ppo2.py:186][0m |          -0.0061 |           4.4669 |           9.6839 |
[32m[20230113 20:17:54 @agent_ppo2.py:186][0m |          -0.0158 |           4.2734 |           9.6790 |
[32m[20230113 20:17:54 @agent_ppo2.py:186][0m |          -0.0112 |           4.1340 |           9.6818 |
[32m[20230113 20:17:54 @agent_ppo2.py:186][0m |          -0.0174 |           4.0275 |           9.6738 |
[32m[20230113 20:17:54 @agent_ppo2.py:186][0m |          -0.0177 |           3.9376 |           9.6764 |
[32m[20230113 20:17:54 @agent_ppo2.py:186][0m |          -0.0140 |           3.9577 |           9.6721 |
[32m[20230113 20:17:54 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:17:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.15
[32m[20230113 20:17:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.29
[32m[20230113 20:17:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.13
[32m[20230113 20:17:55 @agent_ppo2.py:144][0m Total time:      33.37 min
[32m[20230113 20:17:55 @agent_ppo2.py:146][0m 3092480 total steps have happened
[32m[20230113 20:17:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1510 --------------------------#
[32m[20230113 20:17:55 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:17:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:55 @agent_ppo2.py:186][0m |           0.0042 |          11.1615 |           9.9634 |
[32m[20230113 20:17:55 @agent_ppo2.py:186][0m |          -0.0052 |           8.5340 |           9.9528 |
[32m[20230113 20:17:55 @agent_ppo2.py:186][0m |          -0.0118 |           7.4551 |           9.9487 |
[32m[20230113 20:17:55 @agent_ppo2.py:186][0m |          -0.0112 |           6.8688 |           9.9456 |
[32m[20230113 20:17:55 @agent_ppo2.py:186][0m |          -0.0128 |           6.4046 |           9.9403 |
[32m[20230113 20:17:55 @agent_ppo2.py:186][0m |          -0.0145 |           5.9706 |           9.9378 |
[32m[20230113 20:17:55 @agent_ppo2.py:186][0m |          -0.0201 |           5.6745 |           9.9337 |
[32m[20230113 20:17:55 @agent_ppo2.py:186][0m |          -0.0175 |           5.4016 |           9.9338 |
[32m[20230113 20:17:55 @agent_ppo2.py:186][0m |          -0.0162 |           5.1650 |           9.9389 |
[32m[20230113 20:17:56 @agent_ppo2.py:186][0m |          -0.0159 |           4.9769 |           9.9395 |
[32m[20230113 20:17:56 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:17:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.31
[32m[20230113 20:17:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.70
[32m[20230113 20:17:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 236.07
[32m[20230113 20:17:56 @agent_ppo2.py:144][0m Total time:      33.39 min
[32m[20230113 20:17:56 @agent_ppo2.py:146][0m 3094528 total steps have happened
[32m[20230113 20:17:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1511 --------------------------#
[32m[20230113 20:17:56 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:17:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:56 @agent_ppo2.py:186][0m |           0.0017 |           8.1592 |           9.7884 |
[32m[20230113 20:17:57 @agent_ppo2.py:186][0m |          -0.0055 |           6.0244 |           9.7767 |
[32m[20230113 20:17:57 @agent_ppo2.py:186][0m |          -0.0058 |           5.2643 |           9.7614 |
[32m[20230113 20:17:57 @agent_ppo2.py:186][0m |          -0.0139 |           4.8771 |           9.7674 |
[32m[20230113 20:17:57 @agent_ppo2.py:186][0m |          -0.0114 |           4.5446 |           9.7625 |
[32m[20230113 20:17:57 @agent_ppo2.py:186][0m |          -0.0180 |           4.3308 |           9.7746 |
[32m[20230113 20:17:57 @agent_ppo2.py:186][0m |          -0.0168 |           4.2447 |           9.7641 |
[32m[20230113 20:17:57 @agent_ppo2.py:186][0m |          -0.0221 |           4.0978 |           9.7607 |
[32m[20230113 20:17:57 @agent_ppo2.py:186][0m |          -0.0073 |           4.0529 |           9.7652 |
[32m[20230113 20:17:57 @agent_ppo2.py:186][0m |          -0.0080 |           3.8511 |           9.7555 |
[32m[20230113 20:17:57 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:17:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.47
[32m[20230113 20:17:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.81
[32m[20230113 20:17:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.75
[32m[20230113 20:17:57 @agent_ppo2.py:144][0m Total time:      33.41 min
[32m[20230113 20:17:57 @agent_ppo2.py:146][0m 3096576 total steps have happened
[32m[20230113 20:17:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1512 --------------------------#
[32m[20230113 20:17:58 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:17:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:58 @agent_ppo2.py:186][0m |           0.0003 |           7.3154 |          10.1311 |
[32m[20230113 20:17:58 @agent_ppo2.py:186][0m |          -0.0045 |           5.8634 |          10.1076 |
[32m[20230113 20:17:58 @agent_ppo2.py:186][0m |          -0.0090 |           5.2984 |          10.1206 |
[32m[20230113 20:17:58 @agent_ppo2.py:186][0m |          -0.0073 |           5.0241 |          10.1329 |
[32m[20230113 20:17:58 @agent_ppo2.py:186][0m |          -0.0083 |           4.6924 |          10.1207 |
[32m[20230113 20:17:58 @agent_ppo2.py:186][0m |          -0.0104 |           4.5010 |          10.1260 |
[32m[20230113 20:17:58 @agent_ppo2.py:186][0m |          -0.0103 |           4.3053 |          10.1306 |
[32m[20230113 20:17:58 @agent_ppo2.py:186][0m |          -0.0130 |           4.1338 |          10.1183 |
[32m[20230113 20:17:58 @agent_ppo2.py:186][0m |          -0.0133 |           4.0359 |          10.1218 |
[32m[20230113 20:17:58 @agent_ppo2.py:186][0m |          -0.0155 |           3.9132 |          10.1225 |
[32m[20230113 20:17:58 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:17:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.93
[32m[20230113 20:17:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.80
[32m[20230113 20:17:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.54
[32m[20230113 20:17:58 @agent_ppo2.py:144][0m Total time:      33.43 min
[32m[20230113 20:17:58 @agent_ppo2.py:146][0m 3098624 total steps have happened
[32m[20230113 20:17:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1513 --------------------------#
[32m[20230113 20:17:59 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 20:17:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:17:59 @agent_ppo2.py:186][0m |           0.0021 |          12.2449 |           9.9280 |
[32m[20230113 20:17:59 @agent_ppo2.py:186][0m |          -0.0064 |           7.8512 |           9.9135 |
[32m[20230113 20:17:59 @agent_ppo2.py:186][0m |          -0.0105 |           6.5091 |           9.9074 |
[32m[20230113 20:17:59 @agent_ppo2.py:186][0m |          -0.0137 |           5.8732 |           9.9041 |
[32m[20230113 20:17:59 @agent_ppo2.py:186][0m |          -0.0094 |           5.4616 |           9.9031 |
[32m[20230113 20:17:59 @agent_ppo2.py:186][0m |          -0.0120 |           5.0471 |           9.8917 |
[32m[20230113 20:17:59 @agent_ppo2.py:186][0m |          -0.0170 |           4.8131 |           9.8979 |
[32m[20230113 20:17:59 @agent_ppo2.py:186][0m |          -0.0149 |           4.6787 |           9.8925 |
[32m[20230113 20:17:59 @agent_ppo2.py:186][0m |          -0.0179 |           4.3971 |           9.8979 |
[32m[20230113 20:17:59 @agent_ppo2.py:186][0m |          -0.0165 |           4.2569 |           9.8919 |
[32m[20230113 20:17:59 @agent_ppo2.py:131][0m Policy update time: 0.50 s
[32m[20230113 20:18:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 133.56
[32m[20230113 20:18:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.99
[32m[20230113 20:18:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.37
[32m[20230113 20:18:00 @agent_ppo2.py:144][0m Total time:      33.46 min
[32m[20230113 20:18:00 @agent_ppo2.py:146][0m 3100672 total steps have happened
[32m[20230113 20:18:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1514 --------------------------#
[32m[20230113 20:18:00 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:18:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:00 @agent_ppo2.py:186][0m |          -0.0010 |           6.6042 |           9.8862 |
[32m[20230113 20:18:00 @agent_ppo2.py:186][0m |          -0.0055 |           5.2891 |           9.8673 |
[32m[20230113 20:18:00 @agent_ppo2.py:186][0m |          -0.0068 |           4.8309 |           9.8670 |
[32m[20230113 20:18:01 @agent_ppo2.py:186][0m |          -0.0114 |           4.5568 |           9.8669 |
[32m[20230113 20:18:01 @agent_ppo2.py:186][0m |          -0.0103 |           4.3284 |           9.8671 |
[32m[20230113 20:18:01 @agent_ppo2.py:186][0m |          -0.0118 |           4.1756 |           9.8590 |
[32m[20230113 20:18:01 @agent_ppo2.py:186][0m |          -0.0147 |           4.0663 |           9.8696 |
[32m[20230113 20:18:01 @agent_ppo2.py:186][0m |          -0.0117 |           3.9540 |           9.8723 |
[32m[20230113 20:18:01 @agent_ppo2.py:186][0m |          -0.0145 |           3.8174 |           9.8623 |
[32m[20230113 20:18:01 @agent_ppo2.py:186][0m |          -0.0139 |           3.7850 |           9.8679 |
[32m[20230113 20:18:01 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:18:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.09
[32m[20230113 20:18:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.36
[32m[20230113 20:18:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.69
[32m[20230113 20:18:01 @agent_ppo2.py:144][0m Total time:      33.48 min
[32m[20230113 20:18:01 @agent_ppo2.py:146][0m 3102720 total steps have happened
[32m[20230113 20:18:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1515 --------------------------#
[32m[20230113 20:18:02 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:02 @agent_ppo2.py:186][0m |           0.0005 |           5.2785 |          10.0163 |
[32m[20230113 20:18:02 @agent_ppo2.py:186][0m |          -0.0041 |           3.9665 |          10.0118 |
[32m[20230113 20:18:02 @agent_ppo2.py:186][0m |          -0.0079 |           3.5845 |          10.0049 |
[32m[20230113 20:18:02 @agent_ppo2.py:186][0m |          -0.0101 |           3.4031 |          10.0055 |
[32m[20230113 20:18:02 @agent_ppo2.py:186][0m |          -0.0117 |           3.3149 |           9.9942 |
[32m[20230113 20:18:02 @agent_ppo2.py:186][0m |          -0.0118 |           3.1411 |          10.0024 |
[32m[20230113 20:18:02 @agent_ppo2.py:186][0m |          -0.0138 |           3.0707 |           9.9908 |
[32m[20230113 20:18:02 @agent_ppo2.py:186][0m |          -0.0136 |           3.0037 |          10.0064 |
[32m[20230113 20:18:02 @agent_ppo2.py:186][0m |          -0.0155 |           2.9399 |           9.9945 |
[32m[20230113 20:18:02 @agent_ppo2.py:186][0m |          -0.0175 |           2.8750 |           9.9966 |
[32m[20230113 20:18:02 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:18:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.22
[32m[20230113 20:18:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.22
[32m[20230113 20:18:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.96
[32m[20230113 20:18:02 @agent_ppo2.py:144][0m Total time:      33.50 min
[32m[20230113 20:18:02 @agent_ppo2.py:146][0m 3104768 total steps have happened
[32m[20230113 20:18:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1516 --------------------------#
[32m[20230113 20:18:03 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:18:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:03 @agent_ppo2.py:186][0m |          -0.0016 |           5.5817 |          10.2070 |
[32m[20230113 20:18:03 @agent_ppo2.py:186][0m |          -0.0067 |           4.1413 |          10.2027 |
[32m[20230113 20:18:03 @agent_ppo2.py:186][0m |          -0.0100 |           3.7694 |          10.2073 |
[32m[20230113 20:18:03 @agent_ppo2.py:186][0m |          -0.0120 |           3.5411 |          10.1933 |
[32m[20230113 20:18:03 @agent_ppo2.py:186][0m |          -0.0116 |           3.3838 |          10.1937 |
[32m[20230113 20:18:03 @agent_ppo2.py:186][0m |          -0.0123 |           3.2714 |          10.1952 |
[32m[20230113 20:18:03 @agent_ppo2.py:186][0m |          -0.0143 |           3.1570 |          10.1983 |
[32m[20230113 20:18:03 @agent_ppo2.py:186][0m |          -0.0169 |           3.1073 |          10.1995 |
[32m[20230113 20:18:03 @agent_ppo2.py:186][0m |          -0.0139 |           3.0163 |          10.1958 |
[32m[20230113 20:18:03 @agent_ppo2.py:186][0m |          -0.0171 |           2.9727 |          10.1938 |
[32m[20230113 20:18:03 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:18:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.11
[32m[20230113 20:18:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.07
[32m[20230113 20:18:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.78
[32m[20230113 20:18:04 @agent_ppo2.py:144][0m Total time:      33.52 min
[32m[20230113 20:18:04 @agent_ppo2.py:146][0m 3106816 total steps have happened
[32m[20230113 20:18:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1517 --------------------------#
[32m[20230113 20:18:04 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:18:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:04 @agent_ppo2.py:186][0m |          -0.0002 |           6.2622 |          10.2439 |
[32m[20230113 20:18:04 @agent_ppo2.py:186][0m |          -0.0044 |           4.1462 |          10.2442 |
[32m[20230113 20:18:04 @agent_ppo2.py:186][0m |          -0.0074 |           3.6721 |          10.2483 |
[32m[20230113 20:18:04 @agent_ppo2.py:186][0m |          -0.0116 |           3.4255 |          10.2479 |
[32m[20230113 20:18:04 @agent_ppo2.py:186][0m |          -0.0102 |           3.1995 |          10.2519 |
[32m[20230113 20:18:04 @agent_ppo2.py:186][0m |          -0.0119 |           3.0727 |          10.2521 |
[32m[20230113 20:18:04 @agent_ppo2.py:186][0m |          -0.0120 |           2.9613 |          10.2491 |
[32m[20230113 20:18:04 @agent_ppo2.py:186][0m |          -0.0140 |           2.8660 |          10.2443 |
[32m[20230113 20:18:05 @agent_ppo2.py:186][0m |          -0.0130 |           2.8035 |          10.2616 |
[32m[20230113 20:18:05 @agent_ppo2.py:186][0m |          -0.0145 |           2.7305 |          10.2606 |
[32m[20230113 20:18:05 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.20
[32m[20230113 20:18:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.06
[32m[20230113 20:18:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.45
[32m[20230113 20:18:05 @agent_ppo2.py:144][0m Total time:      33.54 min
[32m[20230113 20:18:05 @agent_ppo2.py:146][0m 3108864 total steps have happened
[32m[20230113 20:18:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1518 --------------------------#
[32m[20230113 20:18:05 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:05 @agent_ppo2.py:186][0m |           0.0046 |           5.6701 |           9.9008 |
[32m[20230113 20:18:06 @agent_ppo2.py:186][0m |          -0.0183 |           4.1241 |           9.8785 |
[32m[20230113 20:18:06 @agent_ppo2.py:186][0m |          -0.0150 |           3.5534 |           9.8734 |
[32m[20230113 20:18:06 @agent_ppo2.py:186][0m |          -0.0016 |           3.4880 |           9.8736 |
[32m[20230113 20:18:06 @agent_ppo2.py:186][0m |           0.0284 |           3.3378 |           9.8708 |
[32m[20230113 20:18:06 @agent_ppo2.py:186][0m |          -0.0120 |           3.0887 |           9.8482 |
[32m[20230113 20:18:06 @agent_ppo2.py:186][0m |          -0.0304 |           2.8415 |           9.8661 |
[32m[20230113 20:18:06 @agent_ppo2.py:186][0m |          -0.0377 |           2.7306 |           9.8639 |
[32m[20230113 20:18:06 @agent_ppo2.py:186][0m |          -0.0027 |           2.6379 |           9.8392 |
[32m[20230113 20:18:06 @agent_ppo2.py:186][0m |          -0.0142 |           2.5685 |           9.8476 |
[32m[20230113 20:18:06 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.70
[32m[20230113 20:18:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.68
[32m[20230113 20:18:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.41
[32m[20230113 20:18:06 @agent_ppo2.py:144][0m Total time:      33.56 min
[32m[20230113 20:18:06 @agent_ppo2.py:146][0m 3110912 total steps have happened
[32m[20230113 20:18:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1519 --------------------------#
[32m[20230113 20:18:07 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:18:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:07 @agent_ppo2.py:186][0m |           0.0039 |           8.1356 |          10.0008 |
[32m[20230113 20:18:07 @agent_ppo2.py:186][0m |          -0.0094 |           5.4162 |           9.9872 |
[32m[20230113 20:18:07 @agent_ppo2.py:186][0m |          -0.0031 |           4.8951 |           9.9813 |
[32m[20230113 20:18:07 @agent_ppo2.py:186][0m |          -0.0139 |           4.1978 |           9.9743 |
[32m[20230113 20:18:07 @agent_ppo2.py:186][0m |          -0.0060 |           3.9143 |           9.9653 |
[32m[20230113 20:18:07 @agent_ppo2.py:186][0m |          -0.0190 |           3.7021 |           9.9565 |
[32m[20230113 20:18:07 @agent_ppo2.py:186][0m |          -0.0157 |           3.4475 |           9.9574 |
[32m[20230113 20:18:07 @agent_ppo2.py:186][0m |          -0.0098 |           3.4146 |           9.9547 |
[32m[20230113 20:18:07 @agent_ppo2.py:186][0m |          -0.0174 |           3.2123 |           9.9593 |
[32m[20230113 20:18:07 @agent_ppo2.py:186][0m |          -0.0197 |           3.0896 |           9.9408 |
[32m[20230113 20:18:07 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.28
[32m[20230113 20:18:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.16
[32m[20230113 20:18:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.26
[32m[20230113 20:18:08 @agent_ppo2.py:144][0m Total time:      33.58 min
[32m[20230113 20:18:08 @agent_ppo2.py:146][0m 3112960 total steps have happened
[32m[20230113 20:18:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1520 --------------------------#
[32m[20230113 20:18:08 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:18:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:08 @agent_ppo2.py:186][0m |          -0.0011 |          17.9788 |           9.8449 |
[32m[20230113 20:18:08 @agent_ppo2.py:186][0m |          -0.0094 |           9.8540 |           9.8227 |
[32m[20230113 20:18:08 @agent_ppo2.py:186][0m |          -0.0094 |           7.4907 |           9.8118 |
[32m[20230113 20:18:08 @agent_ppo2.py:186][0m |          -0.0139 |           5.8839 |           9.8104 |
[32m[20230113 20:18:08 @agent_ppo2.py:186][0m |          -0.0148 |           5.2062 |           9.8057 |
[32m[20230113 20:18:08 @agent_ppo2.py:186][0m |          -0.0193 |           4.6861 |           9.8064 |
[32m[20230113 20:18:08 @agent_ppo2.py:186][0m |          -0.0209 |           4.2684 |           9.8024 |
[32m[20230113 20:18:08 @agent_ppo2.py:186][0m |          -0.0224 |           3.9435 |           9.7992 |
[32m[20230113 20:18:08 @agent_ppo2.py:186][0m |          -0.0246 |           3.6916 |           9.7958 |
[32m[20230113 20:18:08 @agent_ppo2.py:186][0m |          -0.0244 |           3.4398 |           9.7998 |
[32m[20230113 20:18:08 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:18:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 112.01
[32m[20230113 20:18:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.64
[32m[20230113 20:18:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.86
[32m[20230113 20:18:09 @agent_ppo2.py:144][0m Total time:      33.60 min
[32m[20230113 20:18:09 @agent_ppo2.py:146][0m 3115008 total steps have happened
[32m[20230113 20:18:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1521 --------------------------#
[32m[20230113 20:18:09 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:18:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:09 @agent_ppo2.py:186][0m |          -0.0025 |           6.8430 |          10.2017 |
[32m[20230113 20:18:09 @agent_ppo2.py:186][0m |          -0.0075 |           5.6668 |          10.1975 |
[32m[20230113 20:18:09 @agent_ppo2.py:186][0m |          -0.0108 |           4.9390 |          10.1908 |
[32m[20230113 20:18:09 @agent_ppo2.py:186][0m |          -0.0144 |           4.5957 |          10.1910 |
[32m[20230113 20:18:09 @agent_ppo2.py:186][0m |          -0.0122 |           4.3135 |          10.1812 |
[32m[20230113 20:18:09 @agent_ppo2.py:186][0m |          -0.0085 |           4.0886 |          10.1762 |
[32m[20230113 20:18:09 @agent_ppo2.py:186][0m |          -0.0110 |           3.9170 |          10.1844 |
[32m[20230113 20:18:09 @agent_ppo2.py:186][0m |          -0.0128 |           3.7824 |          10.1771 |
[32m[20230113 20:18:09 @agent_ppo2.py:186][0m |          -0.0268 |           3.6681 |          10.1771 |
[32m[20230113 20:18:09 @agent_ppo2.py:186][0m |          -0.0173 |           3.5573 |          10.1671 |
[32m[20230113 20:18:09 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:18:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.99
[32m[20230113 20:18:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.35
[32m[20230113 20:18:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.55
[32m[20230113 20:18:10 @agent_ppo2.py:144][0m Total time:      33.62 min
[32m[20230113 20:18:10 @agent_ppo2.py:146][0m 3117056 total steps have happened
[32m[20230113 20:18:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1522 --------------------------#
[32m[20230113 20:18:10 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:10 @agent_ppo2.py:186][0m |           0.0007 |           6.4046 |          10.0775 |
[32m[20230113 20:18:10 @agent_ppo2.py:186][0m |          -0.0067 |           4.3222 |          10.0578 |
[32m[20230113 20:18:10 @agent_ppo2.py:186][0m |          -0.0094 |           3.7466 |          10.0541 |
[32m[20230113 20:18:11 @agent_ppo2.py:186][0m |          -0.0092 |           3.4383 |          10.0630 |
[32m[20230113 20:18:11 @agent_ppo2.py:186][0m |          -0.0107 |           3.2120 |          10.0542 |
[32m[20230113 20:18:11 @agent_ppo2.py:186][0m |          -0.0116 |           3.0457 |          10.0631 |
[32m[20230113 20:18:11 @agent_ppo2.py:186][0m |          -0.0131 |           2.9402 |          10.0552 |
[32m[20230113 20:18:11 @agent_ppo2.py:186][0m |          -0.0134 |           2.8291 |          10.0576 |
[32m[20230113 20:18:11 @agent_ppo2.py:186][0m |          -0.0146 |           2.7661 |          10.0622 |
[32m[20230113 20:18:11 @agent_ppo2.py:186][0m |          -0.0147 |           2.7074 |          10.0579 |
[32m[20230113 20:18:11 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:18:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.37
[32m[20230113 20:18:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.61
[32m[20230113 20:18:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.58
[32m[20230113 20:18:11 @agent_ppo2.py:144][0m Total time:      33.64 min
[32m[20230113 20:18:11 @agent_ppo2.py:146][0m 3119104 total steps have happened
[32m[20230113 20:18:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1523 --------------------------#
[32m[20230113 20:18:12 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:12 @agent_ppo2.py:186][0m |           0.0031 |           6.7210 |           9.9639 |
[32m[20230113 20:18:12 @agent_ppo2.py:186][0m |          -0.0020 |           4.9821 |           9.9658 |
[32m[20230113 20:18:12 @agent_ppo2.py:186][0m |          -0.0037 |           4.1636 |           9.9495 |
[32m[20230113 20:18:12 @agent_ppo2.py:186][0m |          -0.0054 |           3.6987 |           9.9511 |
[32m[20230113 20:18:12 @agent_ppo2.py:186][0m |          -0.0072 |           3.4677 |           9.9383 |
[32m[20230113 20:18:12 @agent_ppo2.py:186][0m |          -0.0074 |           3.2940 |           9.9522 |
[32m[20230113 20:18:12 @agent_ppo2.py:186][0m |          -0.0095 |           3.1362 |           9.9527 |
[32m[20230113 20:18:12 @agent_ppo2.py:186][0m |          -0.0106 |           3.0345 |           9.9541 |
[32m[20230113 20:18:12 @agent_ppo2.py:186][0m |          -0.0121 |           2.9316 |           9.9481 |
[32m[20230113 20:18:12 @agent_ppo2.py:186][0m |          -0.0109 |           2.8573 |           9.9515 |
[32m[20230113 20:18:12 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:18:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.82
[32m[20230113 20:18:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.14
[32m[20230113 20:18:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.63
[32m[20230113 20:18:12 @agent_ppo2.py:144][0m Total time:      33.66 min
[32m[20230113 20:18:12 @agent_ppo2.py:146][0m 3121152 total steps have happened
[32m[20230113 20:18:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1524 --------------------------#
[32m[20230113 20:18:13 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:18:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:13 @agent_ppo2.py:186][0m |           0.0016 |           5.4857 |          10.0994 |
[32m[20230113 20:18:13 @agent_ppo2.py:186][0m |          -0.0071 |           4.3674 |          10.0610 |
[32m[20230113 20:18:13 @agent_ppo2.py:186][0m |          -0.0053 |           4.1503 |          10.0616 |
[32m[20230113 20:18:13 @agent_ppo2.py:186][0m |          -0.0071 |           3.7738 |          10.0668 |
[32m[20230113 20:18:13 @agent_ppo2.py:186][0m |          -0.0099 |           3.5970 |          10.0623 |
[32m[20230113 20:18:13 @agent_ppo2.py:186][0m |          -0.0120 |           3.4375 |          10.0666 |
[32m[20230113 20:18:13 @agent_ppo2.py:186][0m |          -0.0127 |           3.3447 |          10.0604 |
[32m[20230113 20:18:13 @agent_ppo2.py:186][0m |          -0.0123 |           3.3010 |          10.0596 |
[32m[20230113 20:18:13 @agent_ppo2.py:186][0m |          -0.0132 |           3.1929 |          10.0552 |
[32m[20230113 20:18:13 @agent_ppo2.py:186][0m |          -0.0128 |           3.1441 |          10.0496 |
[32m[20230113 20:18:13 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.62
[32m[20230113 20:18:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.21
[32m[20230113 20:18:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.79
[32m[20230113 20:18:14 @agent_ppo2.py:144][0m Total time:      33.69 min
[32m[20230113 20:18:14 @agent_ppo2.py:146][0m 3123200 total steps have happened
[32m[20230113 20:18:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1525 --------------------------#
[32m[20230113 20:18:14 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:14 @agent_ppo2.py:186][0m |           0.0097 |           5.3885 |           9.9322 |
[32m[20230113 20:18:14 @agent_ppo2.py:186][0m |          -0.0025 |           4.4721 |           9.8899 |
[32m[20230113 20:18:14 @agent_ppo2.py:186][0m |          -0.0093 |           4.0014 |           9.8898 |
[32m[20230113 20:18:14 @agent_ppo2.py:186][0m |          -0.0090 |           3.7338 |           9.8971 |
[32m[20230113 20:18:14 @agent_ppo2.py:186][0m |          -0.0088 |           3.5240 |           9.8941 |
[32m[20230113 20:18:14 @agent_ppo2.py:186][0m |          -0.0079 |           3.4167 |           9.8850 |
[32m[20230113 20:18:14 @agent_ppo2.py:186][0m |          -0.0208 |           3.3223 |           9.8862 |
[32m[20230113 20:18:15 @agent_ppo2.py:186][0m |          -0.0153 |           3.1430 |           9.8796 |
[32m[20230113 20:18:15 @agent_ppo2.py:186][0m |          -0.0154 |           3.0700 |           9.8851 |
[32m[20230113 20:18:15 @agent_ppo2.py:186][0m |          -0.0189 |           2.9977 |           9.8902 |
[32m[20230113 20:18:15 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:18:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.82
[32m[20230113 20:18:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.72
[32m[20230113 20:18:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.78
[32m[20230113 20:18:15 @agent_ppo2.py:144][0m Total time:      33.71 min
[32m[20230113 20:18:15 @agent_ppo2.py:146][0m 3125248 total steps have happened
[32m[20230113 20:18:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1526 --------------------------#
[32m[20230113 20:18:15 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:16 @agent_ppo2.py:186][0m |           0.0024 |           5.2462 |           9.9621 |
[32m[20230113 20:18:16 @agent_ppo2.py:186][0m |          -0.0023 |           4.0832 |           9.9588 |
[32m[20230113 20:18:16 @agent_ppo2.py:186][0m |          -0.0076 |           3.6152 |           9.9554 |
[32m[20230113 20:18:16 @agent_ppo2.py:186][0m |          -0.0105 |           3.3501 |           9.9615 |
[32m[20230113 20:18:16 @agent_ppo2.py:186][0m |          -0.0131 |           3.1800 |           9.9523 |
[32m[20230113 20:18:16 @agent_ppo2.py:186][0m |          -0.0119 |           3.0451 |           9.9580 |
[32m[20230113 20:18:16 @agent_ppo2.py:186][0m |          -0.0102 |           2.9293 |           9.9543 |
[32m[20230113 20:18:16 @agent_ppo2.py:186][0m |          -0.0095 |           2.8595 |           9.9576 |
[32m[20230113 20:18:16 @agent_ppo2.py:186][0m |          -0.0177 |           2.7644 |           9.9474 |
[32m[20230113 20:18:16 @agent_ppo2.py:186][0m |          -0.0151 |           2.7132 |           9.9495 |
[32m[20230113 20:18:16 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.05
[32m[20230113 20:18:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.88
[32m[20230113 20:18:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.98
[32m[20230113 20:18:16 @agent_ppo2.py:144][0m Total time:      33.73 min
[32m[20230113 20:18:16 @agent_ppo2.py:146][0m 3127296 total steps have happened
[32m[20230113 20:18:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1527 --------------------------#
[32m[20230113 20:18:17 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:18:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:17 @agent_ppo2.py:186][0m |           0.0026 |           5.8471 |          10.1867 |
[32m[20230113 20:18:17 @agent_ppo2.py:186][0m |          -0.0044 |           4.8078 |          10.1851 |
[32m[20230113 20:18:17 @agent_ppo2.py:186][0m |          -0.0082 |           4.3882 |          10.1727 |
[32m[20230113 20:18:17 @agent_ppo2.py:186][0m |          -0.0080 |           4.1399 |          10.1684 |
[32m[20230113 20:18:17 @agent_ppo2.py:186][0m |          -0.0091 |           3.9462 |          10.1800 |
[32m[20230113 20:18:17 @agent_ppo2.py:186][0m |          -0.0101 |           3.7842 |          10.1693 |
[32m[20230113 20:18:17 @agent_ppo2.py:186][0m |          -0.0076 |           3.6716 |          10.1628 |
[32m[20230113 20:18:17 @agent_ppo2.py:186][0m |          -0.0107 |           3.6356 |          10.1676 |
[32m[20230113 20:18:17 @agent_ppo2.py:186][0m |          -0.0113 |           3.5083 |          10.1696 |
[32m[20230113 20:18:17 @agent_ppo2.py:186][0m |          -0.0122 |           3.3991 |          10.1671 |
[32m[20230113 20:18:17 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:18:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.54
[32m[20230113 20:18:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.10
[32m[20230113 20:18:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.11
[32m[20230113 20:18:18 @agent_ppo2.py:144][0m Total time:      33.75 min
[32m[20230113 20:18:18 @agent_ppo2.py:146][0m 3129344 total steps have happened
[32m[20230113 20:18:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1528 --------------------------#
[32m[20230113 20:18:18 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:18:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:18 @agent_ppo2.py:186][0m |           0.0074 |           6.8311 |          10.2525 |
[32m[20230113 20:18:18 @agent_ppo2.py:186][0m |          -0.0098 |           4.9614 |          10.2367 |
[32m[20230113 20:18:18 @agent_ppo2.py:186][0m |          -0.0054 |           4.2504 |          10.2256 |
[32m[20230113 20:18:18 @agent_ppo2.py:186][0m |          -0.0209 |           3.7748 |          10.2295 |
[32m[20230113 20:18:18 @agent_ppo2.py:186][0m |          -0.0066 |           3.5522 |          10.2339 |
[32m[20230113 20:18:18 @agent_ppo2.py:186][0m |          -0.0091 |           3.3071 |          10.2216 |
[32m[20230113 20:18:18 @agent_ppo2.py:186][0m |          -0.0098 |           3.1134 |          10.2218 |
[32m[20230113 20:18:18 @agent_ppo2.py:186][0m |          -0.0188 |           2.9842 |          10.2292 |
[32m[20230113 20:18:18 @agent_ppo2.py:186][0m |          -0.0182 |           2.8872 |          10.2129 |
[32m[20230113 20:18:18 @agent_ppo2.py:186][0m |          -0.0079 |           2.7780 |          10.2190 |
[32m[20230113 20:18:18 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.53
[32m[20230113 20:18:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.22
[32m[20230113 20:18:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.63
[32m[20230113 20:18:19 @agent_ppo2.py:144][0m Total time:      33.77 min
[32m[20230113 20:18:19 @agent_ppo2.py:146][0m 3131392 total steps have happened
[32m[20230113 20:18:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1529 --------------------------#
[32m[20230113 20:18:19 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:18:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:19 @agent_ppo2.py:186][0m |           0.0020 |           5.1519 |          10.0491 |
[32m[20230113 20:18:19 @agent_ppo2.py:186][0m |          -0.0031 |           3.6999 |          10.0421 |
[32m[20230113 20:18:20 @agent_ppo2.py:186][0m |          -0.0055 |           3.3338 |          10.0320 |
[32m[20230113 20:18:20 @agent_ppo2.py:186][0m |          -0.0067 |           3.1345 |          10.0276 |
[32m[20230113 20:18:20 @agent_ppo2.py:186][0m |          -0.0090 |           2.9633 |          10.0363 |
[32m[20230113 20:18:20 @agent_ppo2.py:186][0m |          -0.0100 |           2.8617 |          10.0262 |
[32m[20230113 20:18:20 @agent_ppo2.py:186][0m |          -0.0098 |           2.7664 |          10.0208 |
[32m[20230113 20:18:20 @agent_ppo2.py:186][0m |          -0.0137 |           2.6650 |          10.0199 |
[32m[20230113 20:18:20 @agent_ppo2.py:186][0m |          -0.0123 |           2.6143 |          10.0203 |
[32m[20230113 20:18:20 @agent_ppo2.py:186][0m |          -0.0137 |           2.5449 |          10.0224 |
[32m[20230113 20:18:20 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.16
[32m[20230113 20:18:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.51
[32m[20230113 20:18:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.83
[32m[20230113 20:18:20 @agent_ppo2.py:144][0m Total time:      33.79 min
[32m[20230113 20:18:20 @agent_ppo2.py:146][0m 3133440 total steps have happened
[32m[20230113 20:18:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1530 --------------------------#
[32m[20230113 20:18:21 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:21 @agent_ppo2.py:186][0m |           0.0041 |           5.4056 |          10.1801 |
[32m[20230113 20:18:21 @agent_ppo2.py:186][0m |          -0.0066 |           3.9541 |          10.1594 |
[32m[20230113 20:18:21 @agent_ppo2.py:186][0m |          -0.0088 |           3.4405 |          10.1605 |
[32m[20230113 20:18:21 @agent_ppo2.py:186][0m |          -0.0097 |           3.1707 |          10.1548 |
[32m[20230113 20:18:21 @agent_ppo2.py:186][0m |          -0.0067 |           3.0001 |          10.1584 |
[32m[20230113 20:18:21 @agent_ppo2.py:186][0m |          -0.0109 |           2.8683 |          10.1572 |
[32m[20230113 20:18:21 @agent_ppo2.py:186][0m |          -0.0100 |           2.7722 |          10.1586 |
[32m[20230113 20:18:21 @agent_ppo2.py:186][0m |          -0.0178 |           2.7241 |          10.1513 |
[32m[20230113 20:18:21 @agent_ppo2.py:186][0m |          -0.0135 |           2.6347 |          10.1534 |
[32m[20230113 20:18:21 @agent_ppo2.py:186][0m |          -0.0143 |           2.5715 |          10.1471 |
[32m[20230113 20:18:21 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:18:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.80
[32m[20230113 20:18:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.78
[32m[20230113 20:18:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.95
[32m[20230113 20:18:21 @agent_ppo2.py:144][0m Total time:      33.82 min
[32m[20230113 20:18:21 @agent_ppo2.py:146][0m 3135488 total steps have happened
[32m[20230113 20:18:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1531 --------------------------#
[32m[20230113 20:18:22 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:22 @agent_ppo2.py:186][0m |          -0.0001 |           5.9916 |          10.1389 |
[32m[20230113 20:18:22 @agent_ppo2.py:186][0m |          -0.0062 |           4.0871 |          10.1312 |
[32m[20230113 20:18:22 @agent_ppo2.py:186][0m |          -0.0071 |           3.4805 |          10.1186 |
[32m[20230113 20:18:22 @agent_ppo2.py:186][0m |          -0.0082 |           3.2720 |          10.1120 |
[32m[20230113 20:18:22 @agent_ppo2.py:186][0m |          -0.0139 |           3.0961 |          10.1062 |
[32m[20230113 20:18:22 @agent_ppo2.py:186][0m |          -0.0124 |           2.9873 |          10.1064 |
[32m[20230113 20:18:22 @agent_ppo2.py:186][0m |          -0.0140 |           2.8393 |          10.1089 |
[32m[20230113 20:18:22 @agent_ppo2.py:186][0m |          -0.0120 |           2.7625 |          10.1122 |
[32m[20230113 20:18:22 @agent_ppo2.py:186][0m |          -0.0141 |           2.7141 |          10.1170 |
[32m[20230113 20:18:22 @agent_ppo2.py:186][0m |          -0.0137 |           2.6197 |          10.1135 |
[32m[20230113 20:18:22 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:18:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.63
[32m[20230113 20:18:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.29
[32m[20230113 20:18:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.33
[32m[20230113 20:18:23 @agent_ppo2.py:144][0m Total time:      33.84 min
[32m[20230113 20:18:23 @agent_ppo2.py:146][0m 3137536 total steps have happened
[32m[20230113 20:18:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1532 --------------------------#
[32m[20230113 20:18:23 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:23 @agent_ppo2.py:186][0m |           0.0005 |           5.6226 |           9.9969 |
[32m[20230113 20:18:23 @agent_ppo2.py:186][0m |          -0.0066 |           4.3251 |           9.9770 |
[32m[20230113 20:18:23 @agent_ppo2.py:186][0m |          -0.0088 |           3.8838 |           9.9782 |
[32m[20230113 20:18:23 @agent_ppo2.py:186][0m |          -0.0112 |           3.6285 |           9.9769 |
[32m[20230113 20:18:23 @agent_ppo2.py:186][0m |          -0.0112 |           3.4604 |           9.9678 |
[32m[20230113 20:18:23 @agent_ppo2.py:186][0m |          -0.0119 |           3.3457 |           9.9686 |
[32m[20230113 20:18:23 @agent_ppo2.py:186][0m |          -0.0138 |           3.2170 |           9.9603 |
[32m[20230113 20:18:24 @agent_ppo2.py:186][0m |          -0.0138 |           3.1077 |           9.9583 |
[32m[20230113 20:18:24 @agent_ppo2.py:186][0m |          -0.0169 |           3.0452 |           9.9658 |
[32m[20230113 20:18:24 @agent_ppo2.py:186][0m |          -0.0141 |           2.9374 |           9.9615 |
[32m[20230113 20:18:24 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:18:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.53
[32m[20230113 20:18:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.06
[32m[20230113 20:18:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.11
[32m[20230113 20:18:24 @agent_ppo2.py:144][0m Total time:      33.86 min
[32m[20230113 20:18:24 @agent_ppo2.py:146][0m 3139584 total steps have happened
[32m[20230113 20:18:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1533 --------------------------#
[32m[20230113 20:18:24 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:18:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:25 @agent_ppo2.py:186][0m |           0.0002 |           6.2014 |          10.1323 |
[32m[20230113 20:18:25 @agent_ppo2.py:186][0m |          -0.0072 |           4.8335 |          10.1167 |
[32m[20230113 20:18:25 @agent_ppo2.py:186][0m |          -0.0097 |           4.2452 |          10.1158 |
[32m[20230113 20:18:25 @agent_ppo2.py:186][0m |          -0.0112 |           3.8382 |          10.1038 |
[32m[20230113 20:18:25 @agent_ppo2.py:186][0m |          -0.0125 |           3.5747 |          10.1074 |
[32m[20230113 20:18:25 @agent_ppo2.py:186][0m |          -0.0111 |           3.3715 |          10.1065 |
[32m[20230113 20:18:25 @agent_ppo2.py:186][0m |          -0.0135 |           3.2587 |          10.1050 |
[32m[20230113 20:18:25 @agent_ppo2.py:186][0m |          -0.0148 |           3.1193 |          10.1040 |
[32m[20230113 20:18:25 @agent_ppo2.py:186][0m |          -0.0160 |           3.0059 |          10.1084 |
[32m[20230113 20:18:25 @agent_ppo2.py:186][0m |          -0.0173 |           2.9301 |          10.0985 |
[32m[20230113 20:18:25 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.34
[32m[20230113 20:18:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.22
[32m[20230113 20:18:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.20
[32m[20230113 20:18:25 @agent_ppo2.py:144][0m Total time:      33.88 min
[32m[20230113 20:18:25 @agent_ppo2.py:146][0m 3141632 total steps have happened
[32m[20230113 20:18:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1534 --------------------------#
[32m[20230113 20:18:26 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:18:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:26 @agent_ppo2.py:186][0m |          -0.0022 |           5.7335 |          10.2521 |
[32m[20230113 20:18:26 @agent_ppo2.py:186][0m |          -0.0079 |           4.7147 |          10.2330 |
[32m[20230113 20:18:26 @agent_ppo2.py:186][0m |          -0.0130 |           4.2143 |          10.2269 |
[32m[20230113 20:18:26 @agent_ppo2.py:186][0m |          -0.0078 |           3.9080 |          10.2288 |
[32m[20230113 20:18:26 @agent_ppo2.py:186][0m |          -0.0081 |           3.7460 |          10.2170 |
[32m[20230113 20:18:26 @agent_ppo2.py:186][0m |          -0.0103 |           3.4670 |          10.2137 |
[32m[20230113 20:18:26 @agent_ppo2.py:186][0m |          -0.0133 |           3.4187 |          10.2081 |
[32m[20230113 20:18:26 @agent_ppo2.py:186][0m |          -0.0156 |           3.3036 |          10.2055 |
[32m[20230113 20:18:26 @agent_ppo2.py:186][0m |          -0.0179 |           3.1374 |          10.2091 |
[32m[20230113 20:18:26 @agent_ppo2.py:186][0m |          -0.0167 |           3.1001 |          10.2033 |
[32m[20230113 20:18:26 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.56
[32m[20230113 20:18:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.79
[32m[20230113 20:18:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.70
[32m[20230113 20:18:27 @agent_ppo2.py:144][0m Total time:      33.90 min
[32m[20230113 20:18:27 @agent_ppo2.py:146][0m 3143680 total steps have happened
[32m[20230113 20:18:27 @agent_ppo2.py:122][0m #------------------------ Iteration 1535 --------------------------#
[32m[20230113 20:18:27 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 20:18:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:27 @agent_ppo2.py:186][0m |           0.0003 |          11.3167 |           9.9795 |
[32m[20230113 20:18:27 @agent_ppo2.py:186][0m |          -0.0162 |           4.8489 |           9.9609 |
[32m[20230113 20:18:27 @agent_ppo2.py:186][0m |          -0.0127 |           4.1597 |           9.9428 |
[32m[20230113 20:18:27 @agent_ppo2.py:186][0m |          -0.0154 |           3.7282 |           9.9432 |
[32m[20230113 20:18:27 @agent_ppo2.py:186][0m |           0.0256 |           4.0273 |           9.9490 |
[32m[20230113 20:18:27 @agent_ppo2.py:186][0m |          -0.0046 |           7.1914 |           9.8948 |
[32m[20230113 20:18:27 @agent_ppo2.py:186][0m |          -0.0126 |           3.3758 |           9.9259 |
[32m[20230113 20:18:27 @agent_ppo2.py:186][0m |          -0.0251 |           3.0361 |           9.9237 |
[32m[20230113 20:18:27 @agent_ppo2.py:186][0m |          -0.0287 |           3.0004 |           9.9338 |
[32m[20230113 20:18:27 @agent_ppo2.py:186][0m |          -0.0179 |           2.6528 |           9.9254 |
[32m[20230113 20:18:27 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:18:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 123.68
[32m[20230113 20:18:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.41
[32m[20230113 20:18:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.58
[32m[20230113 20:18:28 @agent_ppo2.py:144][0m Total time:      33.92 min
[32m[20230113 20:18:28 @agent_ppo2.py:146][0m 3145728 total steps have happened
[32m[20230113 20:18:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1536 --------------------------#
[32m[20230113 20:18:28 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:18:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:28 @agent_ppo2.py:186][0m |           0.0012 |          15.6134 |          10.1153 |
[32m[20230113 20:18:28 @agent_ppo2.py:186][0m |          -0.0068 |           7.9393 |          10.1024 |
[32m[20230113 20:18:28 @agent_ppo2.py:186][0m |          -0.0093 |           6.5917 |          10.1074 |
[32m[20230113 20:18:29 @agent_ppo2.py:186][0m |          -0.0118 |           5.8345 |          10.1004 |
[32m[20230113 20:18:29 @agent_ppo2.py:186][0m |          -0.0136 |           5.3453 |          10.0995 |
[32m[20230113 20:18:29 @agent_ppo2.py:186][0m |          -0.0145 |           4.9827 |          10.1046 |
[32m[20230113 20:18:29 @agent_ppo2.py:186][0m |          -0.0151 |           4.7187 |          10.1018 |
[32m[20230113 20:18:29 @agent_ppo2.py:186][0m |          -0.0170 |           4.4401 |          10.1085 |
[32m[20230113 20:18:29 @agent_ppo2.py:186][0m |          -0.0172 |           4.2517 |          10.0938 |
[32m[20230113 20:18:29 @agent_ppo2.py:186][0m |          -0.0173 |           4.1295 |          10.1016 |
[32m[20230113 20:18:29 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 20:18:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 142.97
[32m[20230113 20:18:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.90
[32m[20230113 20:18:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.49
[32m[20230113 20:18:29 @agent_ppo2.py:144][0m Total time:      33.94 min
[32m[20230113 20:18:29 @agent_ppo2.py:146][0m 3147776 total steps have happened
[32m[20230113 20:18:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1537 --------------------------#
[32m[20230113 20:18:30 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:18:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:30 @agent_ppo2.py:186][0m |          -0.0015 |           8.6422 |          10.2810 |
[32m[20230113 20:18:30 @agent_ppo2.py:186][0m |          -0.0105 |           7.0254 |          10.2697 |
[32m[20230113 20:18:30 @agent_ppo2.py:186][0m |          -0.0109 |           6.4410 |          10.2628 |
[32m[20230113 20:18:30 @agent_ppo2.py:186][0m |          -0.0124 |           6.0573 |          10.2629 |
[32m[20230113 20:18:30 @agent_ppo2.py:186][0m |          -0.0204 |           5.8174 |          10.2644 |
[32m[20230113 20:18:30 @agent_ppo2.py:186][0m |          -0.0128 |           5.5275 |          10.2618 |
[32m[20230113 20:18:30 @agent_ppo2.py:186][0m |          -0.0140 |           5.3520 |          10.2687 |
[32m[20230113 20:18:30 @agent_ppo2.py:186][0m |          -0.0198 |           5.1042 |          10.2603 |
[32m[20230113 20:18:30 @agent_ppo2.py:186][0m |          -0.0133 |           4.9111 |          10.2646 |
[32m[20230113 20:18:30 @agent_ppo2.py:186][0m |          -0.0174 |           4.7279 |          10.2707 |
[32m[20230113 20:18:30 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:18:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.40
[32m[20230113 20:18:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.77
[32m[20230113 20:18:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.09
[32m[20230113 20:18:30 @agent_ppo2.py:144][0m Total time:      33.96 min
[32m[20230113 20:18:30 @agent_ppo2.py:146][0m 3149824 total steps have happened
[32m[20230113 20:18:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1538 --------------------------#
[32m[20230113 20:18:31 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:18:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:31 @agent_ppo2.py:186][0m |          -0.0090 |           7.8448 |          10.0740 |
[32m[20230113 20:18:31 @agent_ppo2.py:186][0m |          -0.0026 |           5.6351 |          10.0631 |
[32m[20230113 20:18:31 @agent_ppo2.py:186][0m |          -0.0125 |           4.9187 |          10.0789 |
[32m[20230113 20:18:31 @agent_ppo2.py:186][0m |          -0.0146 |           4.3495 |          10.0753 |
[32m[20230113 20:18:31 @agent_ppo2.py:186][0m |          -0.0143 |           4.2055 |          10.0626 |
[32m[20230113 20:18:31 @agent_ppo2.py:186][0m |          -0.0158 |           3.8345 |          10.0703 |
[32m[20230113 20:18:31 @agent_ppo2.py:186][0m |          -0.0149 |           3.6576 |          10.0701 |
[32m[20230113 20:18:31 @agent_ppo2.py:186][0m |          -0.0144 |           3.5272 |          10.0558 |
[32m[20230113 20:18:31 @agent_ppo2.py:186][0m |           0.0019 |           3.5505 |          10.0605 |
[32m[20230113 20:18:31 @agent_ppo2.py:186][0m |          -0.0156 |           3.3436 |          10.0627 |
[32m[20230113 20:18:31 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:18:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.79
[32m[20230113 20:18:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.94
[32m[20230113 20:18:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.17
[32m[20230113 20:18:32 @agent_ppo2.py:144][0m Total time:      33.99 min
[32m[20230113 20:18:32 @agent_ppo2.py:146][0m 3151872 total steps have happened
[32m[20230113 20:18:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1539 --------------------------#
[32m[20230113 20:18:32 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:18:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:32 @agent_ppo2.py:186][0m |           0.0045 |           6.6615 |          10.1736 |
[32m[20230113 20:18:32 @agent_ppo2.py:186][0m |          -0.0094 |           5.0357 |          10.1530 |
[32m[20230113 20:18:32 @agent_ppo2.py:186][0m |          -0.0118 |           4.4911 |          10.1499 |
[32m[20230113 20:18:32 @agent_ppo2.py:186][0m |          -0.0136 |           4.1381 |          10.1425 |
[32m[20230113 20:18:32 @agent_ppo2.py:186][0m |          -0.0171 |           3.8692 |          10.1373 |
[32m[20230113 20:18:32 @agent_ppo2.py:186][0m |          -0.0158 |           3.7235 |          10.1386 |
[32m[20230113 20:18:32 @agent_ppo2.py:186][0m |          -0.0152 |           3.5601 |          10.1364 |
[32m[20230113 20:18:33 @agent_ppo2.py:186][0m |          -0.0139 |           3.4215 |          10.1306 |
[32m[20230113 20:18:33 @agent_ppo2.py:186][0m |          -0.0201 |           3.3016 |          10.1202 |
[32m[20230113 20:18:33 @agent_ppo2.py:186][0m |          -0.0236 |           3.2203 |          10.1252 |
[32m[20230113 20:18:33 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:18:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.60
[32m[20230113 20:18:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.58
[32m[20230113 20:18:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.80
[32m[20230113 20:18:33 @agent_ppo2.py:144][0m Total time:      34.01 min
[32m[20230113 20:18:33 @agent_ppo2.py:146][0m 3153920 total steps have happened
[32m[20230113 20:18:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1540 --------------------------#
[32m[20230113 20:18:33 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:18:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:34 @agent_ppo2.py:186][0m |          -0.0005 |           6.6362 |          10.4995 |
[32m[20230113 20:18:34 @agent_ppo2.py:186][0m |          -0.0061 |           4.8176 |          10.4778 |
[32m[20230113 20:18:34 @agent_ppo2.py:186][0m |          -0.0079 |           4.2686 |          10.4795 |
[32m[20230113 20:18:34 @agent_ppo2.py:186][0m |          -0.0103 |           3.9389 |          10.4815 |
[32m[20230113 20:18:34 @agent_ppo2.py:186][0m |          -0.0113 |           3.8051 |          10.4698 |
[32m[20230113 20:18:34 @agent_ppo2.py:186][0m |          -0.0122 |           3.6476 |          10.4585 |
[32m[20230113 20:18:34 @agent_ppo2.py:186][0m |          -0.0143 |           3.5169 |          10.4688 |
[32m[20230113 20:18:34 @agent_ppo2.py:186][0m |          -0.0144 |           3.4435 |          10.4652 |
[32m[20230113 20:18:34 @agent_ppo2.py:186][0m |          -0.0152 |           3.3564 |          10.4575 |
[32m[20230113 20:18:34 @agent_ppo2.py:186][0m |          -0.0156 |           3.2924 |          10.4515 |
[32m[20230113 20:18:34 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:18:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.56
[32m[20230113 20:18:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.00
[32m[20230113 20:18:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.15
[32m[20230113 20:18:34 @agent_ppo2.py:144][0m Total time:      34.03 min
[32m[20230113 20:18:34 @agent_ppo2.py:146][0m 3155968 total steps have happened
[32m[20230113 20:18:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1541 --------------------------#
[32m[20230113 20:18:35 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:18:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:35 @agent_ppo2.py:186][0m |          -0.0148 |           7.3523 |           9.9837 |
[32m[20230113 20:18:35 @agent_ppo2.py:186][0m |          -0.0127 |           5.5345 |           9.9696 |
[32m[20230113 20:18:35 @agent_ppo2.py:186][0m |          -0.0095 |           4.6899 |           9.9593 |
[32m[20230113 20:18:35 @agent_ppo2.py:186][0m |          -0.0068 |           4.3107 |           9.9671 |
[32m[20230113 20:18:35 @agent_ppo2.py:186][0m |          -0.0112 |           4.0893 |           9.9554 |
[32m[20230113 20:18:35 @agent_ppo2.py:186][0m |          -0.0116 |           3.8387 |           9.9676 |
[32m[20230113 20:18:35 @agent_ppo2.py:186][0m |          -0.0127 |           3.6797 |           9.9655 |
[32m[20230113 20:18:35 @agent_ppo2.py:186][0m |          -0.0071 |           3.5845 |           9.9688 |
[32m[20230113 20:18:35 @agent_ppo2.py:186][0m |          -0.0073 |           3.4951 |           9.9632 |
[32m[20230113 20:18:35 @agent_ppo2.py:186][0m |          -0.0151 |           3.3620 |           9.9582 |
[32m[20230113 20:18:35 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:18:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.20
[32m[20230113 20:18:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.44
[32m[20230113 20:18:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.38
[32m[20230113 20:18:36 @agent_ppo2.py:144][0m Total time:      34.05 min
[32m[20230113 20:18:36 @agent_ppo2.py:146][0m 3158016 total steps have happened
[32m[20230113 20:18:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1542 --------------------------#
[32m[20230113 20:18:36 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:18:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:36 @agent_ppo2.py:186][0m |          -0.0012 |           6.6651 |          10.2152 |
[32m[20230113 20:18:36 @agent_ppo2.py:186][0m |          -0.0068 |           5.0420 |          10.1980 |
[32m[20230113 20:18:36 @agent_ppo2.py:186][0m |          -0.0093 |           4.5686 |          10.2001 |
[32m[20230113 20:18:36 @agent_ppo2.py:186][0m |          -0.0119 |           4.3223 |          10.1934 |
[32m[20230113 20:18:36 @agent_ppo2.py:186][0m |          -0.0126 |           4.1199 |          10.1820 |
[32m[20230113 20:18:36 @agent_ppo2.py:186][0m |          -0.0139 |           3.9519 |          10.2009 |
[32m[20230113 20:18:36 @agent_ppo2.py:186][0m |          -0.0137 |           3.8723 |          10.1871 |
[32m[20230113 20:18:36 @agent_ppo2.py:186][0m |          -0.0168 |           3.7397 |          10.1885 |
[32m[20230113 20:18:36 @agent_ppo2.py:186][0m |          -0.0177 |           3.6255 |          10.1910 |
[32m[20230113 20:18:36 @agent_ppo2.py:186][0m |          -0.0174 |           3.5249 |          10.1760 |
[32m[20230113 20:18:36 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:18:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.78
[32m[20230113 20:18:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.62
[32m[20230113 20:18:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.71
[32m[20230113 20:18:37 @agent_ppo2.py:144][0m Total time:      34.07 min
[32m[20230113 20:18:37 @agent_ppo2.py:146][0m 3160064 total steps have happened
[32m[20230113 20:18:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1543 --------------------------#
[32m[20230113 20:18:37 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:18:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:37 @agent_ppo2.py:186][0m |           0.0016 |          26.2923 |          10.3094 |
[32m[20230113 20:18:37 @agent_ppo2.py:186][0m |          -0.0070 |          13.0466 |          10.3095 |
[32m[20230113 20:18:37 @agent_ppo2.py:186][0m |          -0.0105 |           9.2951 |          10.3076 |
[32m[20230113 20:18:37 @agent_ppo2.py:186][0m |          -0.0123 |           7.4684 |          10.3007 |
[32m[20230113 20:18:37 @agent_ppo2.py:186][0m |          -0.0148 |           6.6471 |          10.2957 |
[32m[20230113 20:18:37 @agent_ppo2.py:186][0m |          -0.0157 |           6.1295 |          10.2918 |
[32m[20230113 20:18:37 @agent_ppo2.py:186][0m |          -0.0172 |           5.6723 |          10.2859 |
[32m[20230113 20:18:37 @agent_ppo2.py:186][0m |          -0.0175 |           5.3611 |          10.2860 |
[32m[20230113 20:18:37 @agent_ppo2.py:186][0m |          -0.0170 |           5.1216 |          10.2879 |
[32m[20230113 20:18:37 @agent_ppo2.py:186][0m |          -0.0190 |           4.8962 |          10.2840 |
[32m[20230113 20:18:37 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:18:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 117.65
[32m[20230113 20:18:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.42
[32m[20230113 20:18:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.39
[32m[20230113 20:18:38 @agent_ppo2.py:144][0m Total time:      34.09 min
[32m[20230113 20:18:38 @agent_ppo2.py:146][0m 3162112 total steps have happened
[32m[20230113 20:18:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1544 --------------------------#
[32m[20230113 20:18:38 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:18:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:38 @agent_ppo2.py:186][0m |           0.0043 |           7.1938 |          10.1060 |
[32m[20230113 20:18:38 @agent_ppo2.py:186][0m |          -0.0062 |           5.3038 |          10.0787 |
[32m[20230113 20:18:38 @agent_ppo2.py:186][0m |          -0.0098 |           4.5472 |          10.0808 |
[32m[20230113 20:18:39 @agent_ppo2.py:186][0m |          -0.0141 |           4.1319 |          10.0866 |
[32m[20230113 20:18:39 @agent_ppo2.py:186][0m |          -0.0098 |           3.8608 |          10.0719 |
[32m[20230113 20:18:39 @agent_ppo2.py:186][0m |          -0.0109 |           3.6621 |          10.0961 |
[32m[20230113 20:18:39 @agent_ppo2.py:186][0m |          -0.0192 |           3.5145 |          10.0841 |
[32m[20230113 20:18:39 @agent_ppo2.py:186][0m |          -0.0163 |           3.3948 |          10.0880 |
[32m[20230113 20:18:39 @agent_ppo2.py:186][0m |          -0.0177 |           3.2517 |          10.0782 |
[32m[20230113 20:18:39 @agent_ppo2.py:186][0m |          -0.0180 |           3.1500 |          10.0903 |
[32m[20230113 20:18:39 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:18:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.89
[32m[20230113 20:18:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.16
[32m[20230113 20:18:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.19
[32m[20230113 20:18:39 @agent_ppo2.py:144][0m Total time:      34.11 min
[32m[20230113 20:18:39 @agent_ppo2.py:146][0m 3164160 total steps have happened
[32m[20230113 20:18:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1545 --------------------------#
[32m[20230113 20:18:40 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 20:18:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:40 @agent_ppo2.py:186][0m |          -0.0072 |          29.2680 |          10.3816 |
[32m[20230113 20:18:40 @agent_ppo2.py:186][0m |          -0.0074 |          15.5305 |          10.3660 |
[32m[20230113 20:18:40 @agent_ppo2.py:186][0m |          -0.0006 |          13.7244 |          10.3551 |
[32m[20230113 20:18:40 @agent_ppo2.py:186][0m |          -0.0040 |          10.9082 |          10.3401 |
[32m[20230113 20:18:40 @agent_ppo2.py:186][0m |          -0.0179 |           9.1115 |          10.3426 |
[32m[20230113 20:18:40 @agent_ppo2.py:186][0m |          -0.0125 |           8.0261 |          10.3348 |
[32m[20230113 20:18:40 @agent_ppo2.py:186][0m |          -0.0233 |           7.3113 |          10.3300 |
[32m[20230113 20:18:40 @agent_ppo2.py:186][0m |          -0.0237 |           7.1897 |          10.3270 |
[32m[20230113 20:18:40 @agent_ppo2.py:186][0m |          -0.0147 |           6.5338 |          10.3338 |
[32m[20230113 20:18:40 @agent_ppo2.py:186][0m |          -0.0204 |           6.2726 |          10.3215 |
[32m[20230113 20:18:40 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 20:18:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 135.37
[32m[20230113 20:18:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.60
[32m[20230113 20:18:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.46
[32m[20230113 20:18:40 @agent_ppo2.py:144][0m Total time:      34.13 min
[32m[20230113 20:18:40 @agent_ppo2.py:146][0m 3166208 total steps have happened
[32m[20230113 20:18:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1546 --------------------------#
[32m[20230113 20:18:41 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:18:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:41 @agent_ppo2.py:186][0m |          -0.0006 |           9.0292 |          10.1462 |
[32m[20230113 20:18:41 @agent_ppo2.py:186][0m |          -0.0080 |           5.9613 |          10.1477 |
[32m[20230113 20:18:41 @agent_ppo2.py:186][0m |          -0.0071 |           5.1665 |          10.1303 |
[32m[20230113 20:18:41 @agent_ppo2.py:186][0m |          -0.0066 |           4.5513 |          10.1361 |
[32m[20230113 20:18:41 @agent_ppo2.py:186][0m |          -0.0096 |           4.2016 |          10.1372 |
[32m[20230113 20:18:41 @agent_ppo2.py:186][0m |          -0.0123 |           3.9769 |          10.1433 |
[32m[20230113 20:18:41 @agent_ppo2.py:186][0m |          -0.0141 |           3.8224 |          10.1377 |
[32m[20230113 20:18:41 @agent_ppo2.py:186][0m |          -0.0158 |           3.5971 |          10.1443 |
[32m[20230113 20:18:41 @agent_ppo2.py:186][0m |          -0.0156 |           3.4949 |          10.1305 |
[32m[20230113 20:18:41 @agent_ppo2.py:186][0m |          -0.0152 |           3.3901 |          10.1317 |
[32m[20230113 20:18:41 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.58
[32m[20230113 20:18:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 229.19
[32m[20230113 20:18:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.13
[32m[20230113 20:18:42 @agent_ppo2.py:144][0m Total time:      34.15 min
[32m[20230113 20:18:42 @agent_ppo2.py:146][0m 3168256 total steps have happened
[32m[20230113 20:18:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1547 --------------------------#
[32m[20230113 20:18:42 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:18:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:42 @agent_ppo2.py:186][0m |           0.0007 |           6.8886 |          10.3299 |
[32m[20230113 20:18:42 @agent_ppo2.py:186][0m |          -0.0042 |           4.9865 |          10.3016 |
[32m[20230113 20:18:42 @agent_ppo2.py:186][0m |          -0.0075 |           4.4381 |          10.3140 |
[32m[20230113 20:18:42 @agent_ppo2.py:186][0m |          -0.0085 |           4.1419 |          10.3226 |
[32m[20230113 20:18:42 @agent_ppo2.py:186][0m |          -0.0097 |           3.9518 |          10.3173 |
[32m[20230113 20:18:42 @agent_ppo2.py:186][0m |          -0.0123 |           3.7464 |          10.3325 |
[32m[20230113 20:18:42 @agent_ppo2.py:186][0m |          -0.0125 |           3.5959 |          10.3285 |
[32m[20230113 20:18:42 @agent_ppo2.py:186][0m |          -0.0144 |           3.4915 |          10.3271 |
[32m[20230113 20:18:42 @agent_ppo2.py:186][0m |          -0.0157 |           3.3653 |          10.3286 |
[32m[20230113 20:18:42 @agent_ppo2.py:186][0m |          -0.0161 |           3.2826 |          10.3248 |
[32m[20230113 20:18:42 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:18:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.38
[32m[20230113 20:18:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.54
[32m[20230113 20:18:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.74
[32m[20230113 20:18:43 @agent_ppo2.py:144][0m Total time:      34.17 min
[32m[20230113 20:18:43 @agent_ppo2.py:146][0m 3170304 total steps have happened
[32m[20230113 20:18:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1548 --------------------------#
[32m[20230113 20:18:43 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:43 @agent_ppo2.py:186][0m |           0.0044 |           7.0196 |          10.3154 |
[32m[20230113 20:18:43 @agent_ppo2.py:186][0m |          -0.0076 |           4.9277 |          10.2879 |
[32m[20230113 20:18:43 @agent_ppo2.py:186][0m |          -0.0085 |           4.3249 |          10.3029 |
[32m[20230113 20:18:43 @agent_ppo2.py:186][0m |          -0.0030 |           4.0365 |          10.2789 |
[32m[20230113 20:18:44 @agent_ppo2.py:186][0m |          -0.0110 |           3.7600 |          10.2909 |
[32m[20230113 20:18:44 @agent_ppo2.py:186][0m |          -0.0162 |           3.5926 |          10.2849 |
[32m[20230113 20:18:44 @agent_ppo2.py:186][0m |          -0.0124 |           3.4714 |          10.2835 |
[32m[20230113 20:18:44 @agent_ppo2.py:186][0m |          -0.0146 |           3.3760 |          10.2849 |
[32m[20230113 20:18:44 @agent_ppo2.py:186][0m |          -0.0155 |           3.3281 |          10.2928 |
[32m[20230113 20:18:44 @agent_ppo2.py:186][0m |          -0.0150 |           3.2092 |          10.2860 |
[32m[20230113 20:18:44 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:18:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.31
[32m[20230113 20:18:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.65
[32m[20230113 20:18:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 184.27
[32m[20230113 20:18:44 @agent_ppo2.py:144][0m Total time:      34.19 min
[32m[20230113 20:18:44 @agent_ppo2.py:146][0m 3172352 total steps have happened
[32m[20230113 20:18:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1549 --------------------------#
[32m[20230113 20:18:45 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:45 @agent_ppo2.py:186][0m |          -0.0006 |           5.9515 |          10.4288 |
[32m[20230113 20:18:45 @agent_ppo2.py:186][0m |          -0.0076 |           4.6246 |          10.4159 |
[32m[20230113 20:18:45 @agent_ppo2.py:186][0m |          -0.0100 |           4.0759 |          10.4161 |
[32m[20230113 20:18:45 @agent_ppo2.py:186][0m |          -0.0110 |           3.7413 |          10.4230 |
[32m[20230113 20:18:45 @agent_ppo2.py:186][0m |          -0.0118 |           3.5203 |          10.4218 |
[32m[20230113 20:18:45 @agent_ppo2.py:186][0m |          -0.0137 |           3.4038 |          10.4153 |
[32m[20230113 20:18:45 @agent_ppo2.py:186][0m |          -0.0139 |           3.2652 |          10.4175 |
[32m[20230113 20:18:45 @agent_ppo2.py:186][0m |          -0.0148 |           3.1626 |          10.4166 |
[32m[20230113 20:18:45 @agent_ppo2.py:186][0m |          -0.0141 |           3.0845 |          10.4205 |
[32m[20230113 20:18:45 @agent_ppo2.py:186][0m |          -0.0167 |           3.0342 |          10.4186 |
[32m[20230113 20:18:45 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:18:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.23
[32m[20230113 20:18:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.72
[32m[20230113 20:18:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.34
[32m[20230113 20:18:45 @agent_ppo2.py:144][0m Total time:      34.21 min
[32m[20230113 20:18:45 @agent_ppo2.py:146][0m 3174400 total steps have happened
[32m[20230113 20:18:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1550 --------------------------#
[32m[20230113 20:18:46 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:46 @agent_ppo2.py:186][0m |           0.0053 |           6.3207 |          10.3577 |
[32m[20230113 20:18:46 @agent_ppo2.py:186][0m |          -0.0028 |           5.6179 |          10.3350 |
[32m[20230113 20:18:46 @agent_ppo2.py:186][0m |          -0.0058 |           5.1244 |          10.3440 |
[32m[20230113 20:18:46 @agent_ppo2.py:186][0m |          -0.0093 |           4.8506 |          10.3317 |
[32m[20230113 20:18:46 @agent_ppo2.py:186][0m |          -0.0089 |           4.5937 |          10.3227 |
[32m[20230113 20:18:46 @agent_ppo2.py:186][0m |          -0.0101 |           4.5446 |          10.3297 |
[32m[20230113 20:18:46 @agent_ppo2.py:186][0m |          -0.0104 |           4.4868 |          10.3245 |
[32m[20230113 20:18:46 @agent_ppo2.py:186][0m |          -0.0091 |           4.2213 |          10.3245 |
[32m[20230113 20:18:46 @agent_ppo2.py:186][0m |          -0.0130 |           4.1184 |          10.3326 |
[32m[20230113 20:18:46 @agent_ppo2.py:186][0m |          -0.0125 |           4.0333 |          10.3256 |
[32m[20230113 20:18:46 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.61
[32m[20230113 20:18:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.50
[32m[20230113 20:18:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 167.90
[32m[20230113 20:18:47 @agent_ppo2.py:144][0m Total time:      34.24 min
[32m[20230113 20:18:47 @agent_ppo2.py:146][0m 3176448 total steps have happened
[32m[20230113 20:18:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1551 --------------------------#
[32m[20230113 20:18:47 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:18:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:47 @agent_ppo2.py:186][0m |           0.0042 |           6.3247 |           9.9542 |
[32m[20230113 20:18:47 @agent_ppo2.py:186][0m |          -0.0018 |           5.0496 |           9.9475 |
[32m[20230113 20:18:47 @agent_ppo2.py:186][0m |          -0.0185 |           4.5849 |           9.9506 |
[32m[20230113 20:18:47 @agent_ppo2.py:186][0m |          -0.0300 |           4.2751 |           9.9505 |
[32m[20230113 20:18:47 @agent_ppo2.py:186][0m |           0.0019 |           4.0876 |           9.9269 |
[32m[20230113 20:18:47 @agent_ppo2.py:186][0m |          -0.0214 |           3.9958 |           9.9435 |
[32m[20230113 20:18:47 @agent_ppo2.py:186][0m |          -0.0069 |           3.8501 |           9.9550 |
[32m[20230113 20:18:47 @agent_ppo2.py:186][0m |          -0.0067 |           3.7314 |           9.9556 |
[32m[20230113 20:18:48 @agent_ppo2.py:186][0m |          -0.0361 |           3.6301 |           9.9517 |
[32m[20230113 20:18:48 @agent_ppo2.py:186][0m |          -0.0109 |           3.5504 |           9.9577 |
[32m[20230113 20:18:48 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:18:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.84
[32m[20230113 20:18:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.30
[32m[20230113 20:18:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.00
[32m[20230113 20:18:48 @agent_ppo2.py:144][0m Total time:      34.26 min
[32m[20230113 20:18:48 @agent_ppo2.py:146][0m 3178496 total steps have happened
[32m[20230113 20:18:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1552 --------------------------#
[32m[20230113 20:18:48 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:18:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:49 @agent_ppo2.py:186][0m |           0.0026 |           5.8244 |          10.4139 |
[32m[20230113 20:18:49 @agent_ppo2.py:186][0m |          -0.0046 |           4.7334 |          10.3806 |
[32m[20230113 20:18:49 @agent_ppo2.py:186][0m |          -0.0077 |           4.3011 |          10.3816 |
[32m[20230113 20:18:49 @agent_ppo2.py:186][0m |          -0.0071 |           4.0297 |          10.3883 |
[32m[20230113 20:18:49 @agent_ppo2.py:186][0m |          -0.0107 |           3.8606 |          10.3602 |
[32m[20230113 20:18:49 @agent_ppo2.py:186][0m |          -0.0104 |           3.6689 |          10.3609 |
[32m[20230113 20:18:49 @agent_ppo2.py:186][0m |          -0.0094 |           3.5510 |          10.3492 |
[32m[20230113 20:18:49 @agent_ppo2.py:186][0m |          -0.0109 |           3.4566 |          10.3457 |
[32m[20230113 20:18:49 @agent_ppo2.py:186][0m |          -0.0143 |           3.3555 |          10.3411 |
[32m[20230113 20:18:49 @agent_ppo2.py:186][0m |          -0.0110 |           3.2791 |          10.3334 |
[32m[20230113 20:18:49 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.45
[32m[20230113 20:18:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.87
[32m[20230113 20:18:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.48
[32m[20230113 20:18:49 @agent_ppo2.py:144][0m Total time:      34.28 min
[32m[20230113 20:18:49 @agent_ppo2.py:146][0m 3180544 total steps have happened
[32m[20230113 20:18:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1553 --------------------------#
[32m[20230113 20:18:50 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:18:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:50 @agent_ppo2.py:186][0m |           0.0011 |           5.0743 |          10.5063 |
[32m[20230113 20:18:50 @agent_ppo2.py:186][0m |          -0.0054 |           4.0779 |          10.4729 |
[32m[20230113 20:18:50 @agent_ppo2.py:186][0m |          -0.0127 |           3.6037 |          10.4726 |
[32m[20230113 20:18:50 @agent_ppo2.py:186][0m |          -0.0124 |           3.3876 |          10.4700 |
[32m[20230113 20:18:50 @agent_ppo2.py:186][0m |          -0.0141 |           3.2028 |          10.4673 |
[32m[20230113 20:18:50 @agent_ppo2.py:186][0m |          -0.0142 |           3.0884 |          10.4703 |
[32m[20230113 20:18:50 @agent_ppo2.py:186][0m |          -0.0161 |           3.0119 |          10.4684 |
[32m[20230113 20:18:50 @agent_ppo2.py:186][0m |          -0.0158 |           2.9544 |          10.4633 |
[32m[20230113 20:18:50 @agent_ppo2.py:186][0m |          -0.0175 |           2.8542 |          10.4585 |
[32m[20230113 20:18:50 @agent_ppo2.py:186][0m |          -0.0161 |           2.7942 |          10.4542 |
[32m[20230113 20:18:50 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.69
[32m[20230113 20:18:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.80
[32m[20230113 20:18:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.47
[32m[20230113 20:18:51 @agent_ppo2.py:144][0m Total time:      34.30 min
[32m[20230113 20:18:51 @agent_ppo2.py:146][0m 3182592 total steps have happened
[32m[20230113 20:18:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1554 --------------------------#
[32m[20230113 20:18:51 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 20:18:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:51 @agent_ppo2.py:186][0m |           0.0032 |          11.3192 |          10.3695 |
[32m[20230113 20:18:51 @agent_ppo2.py:186][0m |          -0.0038 |           6.6418 |          10.3707 |
[32m[20230113 20:18:51 @agent_ppo2.py:186][0m |          -0.0069 |           5.8461 |          10.3746 |
[32m[20230113 20:18:51 @agent_ppo2.py:186][0m |          -0.0069 |           5.2871 |          10.3767 |
[32m[20230113 20:18:51 @agent_ppo2.py:186][0m |          -0.0096 |           5.0856 |          10.3758 |
[32m[20230113 20:18:51 @agent_ppo2.py:186][0m |          -0.0108 |           4.7674 |          10.3794 |
[32m[20230113 20:18:51 @agent_ppo2.py:186][0m |          -0.0126 |           4.6183 |          10.3764 |
[32m[20230113 20:18:52 @agent_ppo2.py:186][0m |          -0.0116 |           4.4956 |          10.3691 |
[32m[20230113 20:18:52 @agent_ppo2.py:186][0m |          -0.0128 |           4.4129 |          10.3737 |
[32m[20230113 20:18:52 @agent_ppo2.py:186][0m |          -0.0156 |           4.2220 |          10.3700 |
[32m[20230113 20:18:52 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:18:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 146.09
[32m[20230113 20:18:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.31
[32m[20230113 20:18:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.52
[32m[20230113 20:18:52 @agent_ppo2.py:144][0m Total time:      34.32 min
[32m[20230113 20:18:52 @agent_ppo2.py:146][0m 3184640 total steps have happened
[32m[20230113 20:18:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1555 --------------------------#
[32m[20230113 20:18:52 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:18:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:53 @agent_ppo2.py:186][0m |          -0.0017 |           6.7385 |          10.2553 |
[32m[20230113 20:18:53 @agent_ppo2.py:186][0m |          -0.0064 |           4.6850 |          10.2642 |
[32m[20230113 20:18:53 @agent_ppo2.py:186][0m |          -0.0086 |           4.0565 |          10.2490 |
[32m[20230113 20:18:53 @agent_ppo2.py:186][0m |          -0.0095 |           3.7124 |          10.2531 |
[32m[20230113 20:18:53 @agent_ppo2.py:186][0m |          -0.0115 |           3.5047 |          10.2416 |
[32m[20230113 20:18:53 @agent_ppo2.py:186][0m |          -0.0128 |           3.2948 |          10.2453 |
[32m[20230113 20:18:53 @agent_ppo2.py:186][0m |          -0.0129 |           3.1779 |          10.2511 |
[32m[20230113 20:18:53 @agent_ppo2.py:186][0m |          -0.0145 |           3.0858 |          10.2468 |
[32m[20230113 20:18:53 @agent_ppo2.py:186][0m |          -0.0152 |           3.0132 |          10.2534 |
[32m[20230113 20:18:53 @agent_ppo2.py:186][0m |          -0.0147 |           2.9644 |          10.2406 |
[32m[20230113 20:18:53 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:18:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.10
[32m[20230113 20:18:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.66
[32m[20230113 20:18:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.24
[32m[20230113 20:18:53 @agent_ppo2.py:144][0m Total time:      34.35 min
[32m[20230113 20:18:53 @agent_ppo2.py:146][0m 3186688 total steps have happened
[32m[20230113 20:18:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1556 --------------------------#
[32m[20230113 20:18:54 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:18:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:54 @agent_ppo2.py:186][0m |           0.0022 |           6.8074 |          10.2170 |
[32m[20230113 20:18:54 @agent_ppo2.py:186][0m |          -0.0020 |           5.7817 |          10.1932 |
[32m[20230113 20:18:54 @agent_ppo2.py:186][0m |          -0.0080 |           5.1661 |          10.1838 |
[32m[20230113 20:18:54 @agent_ppo2.py:186][0m |          -0.0103 |           4.8196 |          10.1785 |
[32m[20230113 20:18:54 @agent_ppo2.py:186][0m |          -0.0105 |           4.5641 |          10.1830 |
[32m[20230113 20:18:54 @agent_ppo2.py:186][0m |          -0.0122 |           4.4839 |          10.1989 |
[32m[20230113 20:18:54 @agent_ppo2.py:186][0m |          -0.0129 |           4.2400 |          10.1878 |
[32m[20230113 20:18:54 @agent_ppo2.py:186][0m |          -0.0121 |           4.1564 |          10.1855 |
[32m[20230113 20:18:54 @agent_ppo2.py:186][0m |          -0.0165 |           4.0257 |          10.1859 |
[32m[20230113 20:18:54 @agent_ppo2.py:186][0m |          -0.0159 |           3.9760 |          10.1895 |
[32m[20230113 20:18:54 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.09
[32m[20230113 20:18:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.44
[32m[20230113 20:18:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.77
[32m[20230113 20:18:55 @agent_ppo2.py:144][0m Total time:      34.37 min
[32m[20230113 20:18:55 @agent_ppo2.py:146][0m 3188736 total steps have happened
[32m[20230113 20:18:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1557 --------------------------#
[32m[20230113 20:18:55 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:18:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:55 @agent_ppo2.py:186][0m |           0.0001 |          16.3639 |          10.3153 |
[32m[20230113 20:18:55 @agent_ppo2.py:186][0m |          -0.0042 |          11.0200 |          10.3063 |
[32m[20230113 20:18:55 @agent_ppo2.py:186][0m |          -0.0073 |           9.2897 |          10.2965 |
[32m[20230113 20:18:55 @agent_ppo2.py:186][0m |          -0.0092 |           8.4764 |          10.2884 |
[32m[20230113 20:18:55 @agent_ppo2.py:186][0m |          -0.0100 |           7.8202 |          10.2909 |
[32m[20230113 20:18:55 @agent_ppo2.py:186][0m |          -0.0133 |           7.8704 |          10.2885 |
[32m[20230113 20:18:55 @agent_ppo2.py:186][0m |          -0.0128 |           7.1048 |          10.2696 |
[32m[20230113 20:18:55 @agent_ppo2.py:186][0m |          -0.0143 |           6.8690 |          10.2736 |
[32m[20230113 20:18:55 @agent_ppo2.py:186][0m |          -0.0125 |           6.7145 |          10.2707 |
[32m[20230113 20:18:56 @agent_ppo2.py:186][0m |          -0.0179 |           6.5652 |          10.2658 |
[32m[20230113 20:18:56 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 118.11
[32m[20230113 20:18:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.57
[32m[20230113 20:18:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.25
[32m[20230113 20:18:56 @agent_ppo2.py:144][0m Total time:      34.39 min
[32m[20230113 20:18:56 @agent_ppo2.py:146][0m 3190784 total steps have happened
[32m[20230113 20:18:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1558 --------------------------#
[32m[20230113 20:18:56 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:18:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:56 @agent_ppo2.py:186][0m |          -0.0029 |           6.0995 |          10.2417 |
[32m[20230113 20:18:56 @agent_ppo2.py:186][0m |          -0.0102 |           4.5255 |          10.2374 |
[32m[20230113 20:18:57 @agent_ppo2.py:186][0m |          -0.0125 |           4.0704 |          10.2186 |
[32m[20230113 20:18:57 @agent_ppo2.py:186][0m |          -0.0124 |           3.7618 |          10.2262 |
[32m[20230113 20:18:57 @agent_ppo2.py:186][0m |          -0.0167 |           3.5851 |          10.2249 |
[32m[20230113 20:18:57 @agent_ppo2.py:186][0m |          -0.0180 |           3.4419 |          10.2163 |
[32m[20230113 20:18:57 @agent_ppo2.py:186][0m |          -0.0185 |           3.3182 |          10.2176 |
[32m[20230113 20:18:57 @agent_ppo2.py:186][0m |          -0.0191 |           3.2524 |          10.2110 |
[32m[20230113 20:18:57 @agent_ppo2.py:186][0m |          -0.0163 |           3.1824 |          10.2083 |
[32m[20230113 20:18:57 @agent_ppo2.py:186][0m |          -0.0196 |           3.0993 |          10.2122 |
[32m[20230113 20:18:57 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:18:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.66
[32m[20230113 20:18:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.03
[32m[20230113 20:18:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 125.11
[32m[20230113 20:18:57 @agent_ppo2.py:144][0m Total time:      34.41 min
[32m[20230113 20:18:57 @agent_ppo2.py:146][0m 3192832 total steps have happened
[32m[20230113 20:18:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1559 --------------------------#
[32m[20230113 20:18:58 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 20:18:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:58 @agent_ppo2.py:186][0m |           0.0015 |          12.8669 |          10.4816 |
[32m[20230113 20:18:58 @agent_ppo2.py:186][0m |          -0.0026 |           6.8669 |          10.5036 |
[32m[20230113 20:18:58 @agent_ppo2.py:186][0m |          -0.0058 |           5.7930 |          10.4926 |
[32m[20230113 20:18:58 @agent_ppo2.py:186][0m |          -0.0080 |           5.2540 |          10.4923 |
[32m[20230113 20:18:58 @agent_ppo2.py:186][0m |          -0.0091 |           4.8832 |          10.4906 |
[32m[20230113 20:18:58 @agent_ppo2.py:186][0m |          -0.0103 |           4.6134 |          10.5002 |
[32m[20230113 20:18:58 @agent_ppo2.py:186][0m |          -0.0101 |           4.3503 |          10.4931 |
[32m[20230113 20:18:58 @agent_ppo2.py:186][0m |          -0.0127 |           4.1876 |          10.4886 |
[32m[20230113 20:18:58 @agent_ppo2.py:186][0m |          -0.0131 |           4.0365 |          10.4934 |
[32m[20230113 20:18:58 @agent_ppo2.py:186][0m |          -0.0142 |           3.9203 |          10.4824 |
[32m[20230113 20:18:58 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:18:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 136.48
[32m[20230113 20:18:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.51
[32m[20230113 20:18:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.12
[32m[20230113 20:18:59 @agent_ppo2.py:144][0m Total time:      34.43 min
[32m[20230113 20:18:59 @agent_ppo2.py:146][0m 3194880 total steps have happened
[32m[20230113 20:18:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1560 --------------------------#
[32m[20230113 20:18:59 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:18:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:18:59 @agent_ppo2.py:186][0m |          -0.0053 |           7.2956 |          10.1057 |
[32m[20230113 20:18:59 @agent_ppo2.py:186][0m |          -0.0019 |           5.4305 |          10.1044 |
[32m[20230113 20:18:59 @agent_ppo2.py:186][0m |          -0.0020 |           4.8575 |          10.1012 |
[32m[20230113 20:18:59 @agent_ppo2.py:186][0m |          -0.0077 |           4.4810 |          10.0911 |
[32m[20230113 20:18:59 @agent_ppo2.py:186][0m |          -0.0187 |           4.2554 |          10.0889 |
[32m[20230113 20:18:59 @agent_ppo2.py:186][0m |          -0.0057 |           4.1329 |          10.0816 |
[32m[20230113 20:18:59 @agent_ppo2.py:186][0m |          -0.0016 |           4.3454 |          10.0825 |
[32m[20230113 20:18:59 @agent_ppo2.py:186][0m |          -0.0500 |           4.0653 |          10.0884 |
[32m[20230113 20:18:59 @agent_ppo2.py:186][0m |          -0.0100 |           3.7810 |          10.0658 |
[32m[20230113 20:18:59 @agent_ppo2.py:186][0m |          -0.0057 |           3.7280 |          10.0539 |
[32m[20230113 20:18:59 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:19:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.36
[32m[20230113 20:19:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.64
[32m[20230113 20:19:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.46
[32m[20230113 20:19:00 @agent_ppo2.py:144][0m Total time:      34.45 min
[32m[20230113 20:19:00 @agent_ppo2.py:146][0m 3196928 total steps have happened
[32m[20230113 20:19:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1561 --------------------------#
[32m[20230113 20:19:00 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:19:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:00 @agent_ppo2.py:186][0m |          -0.0013 |           6.8858 |          10.3386 |
[32m[20230113 20:19:00 @agent_ppo2.py:186][0m |          -0.0054 |           4.9959 |          10.3170 |
[32m[20230113 20:19:00 @agent_ppo2.py:186][0m |          -0.0107 |           4.3635 |          10.3194 |
[32m[20230113 20:19:00 @agent_ppo2.py:186][0m |          -0.0094 |           4.0651 |          10.3250 |
[32m[20230113 20:19:01 @agent_ppo2.py:186][0m |          -0.0095 |           3.8530 |          10.3154 |
[32m[20230113 20:19:01 @agent_ppo2.py:186][0m |          -0.0105 |           3.6945 |          10.3093 |
[32m[20230113 20:19:01 @agent_ppo2.py:186][0m |          -0.0131 |           3.5569 |          10.3162 |
[32m[20230113 20:19:01 @agent_ppo2.py:186][0m |          -0.0134 |           3.4447 |          10.3144 |
[32m[20230113 20:19:01 @agent_ppo2.py:186][0m |          -0.0157 |           3.3562 |          10.3131 |
[32m[20230113 20:19:01 @agent_ppo2.py:186][0m |          -0.0138 |           3.2802 |          10.3200 |
[32m[20230113 20:19:01 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:19:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.92
[32m[20230113 20:19:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.70
[32m[20230113 20:19:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.25
[32m[20230113 20:19:01 @agent_ppo2.py:144][0m Total time:      34.48 min
[32m[20230113 20:19:01 @agent_ppo2.py:146][0m 3198976 total steps have happened
[32m[20230113 20:19:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1562 --------------------------#
[32m[20230113 20:19:02 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:19:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:02 @agent_ppo2.py:186][0m |          -0.0024 |          12.1185 |          10.0449 |
[32m[20230113 20:19:02 @agent_ppo2.py:186][0m |          -0.0026 |           7.5966 |          10.0306 |
[32m[20230113 20:19:02 @agent_ppo2.py:186][0m |          -0.0070 |           6.1276 |          10.0276 |
[32m[20230113 20:19:02 @agent_ppo2.py:186][0m |          -0.0100 |           5.3677 |          10.0220 |
[32m[20230113 20:19:02 @agent_ppo2.py:186][0m |          -0.0151 |           5.0260 |          10.0270 |
[32m[20230113 20:19:02 @agent_ppo2.py:186][0m |          -0.0056 |           4.6241 |          10.0153 |
[32m[20230113 20:19:02 @agent_ppo2.py:186][0m |          -0.0080 |           4.3710 |          10.0167 |
[32m[20230113 20:19:02 @agent_ppo2.py:186][0m |          -0.0148 |           4.1781 |          10.0169 |
[32m[20230113 20:19:02 @agent_ppo2.py:186][0m |          -0.0106 |           3.9953 |          10.0210 |
[32m[20230113 20:19:02 @agent_ppo2.py:186][0m |          -0.0109 |           3.8757 |          10.0210 |
[32m[20230113 20:19:02 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:19:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.96
[32m[20230113 20:19:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.71
[32m[20230113 20:19:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.35
[32m[20230113 20:19:02 @agent_ppo2.py:144][0m Total time:      34.50 min
[32m[20230113 20:19:02 @agent_ppo2.py:146][0m 3201024 total steps have happened
[32m[20230113 20:19:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1563 --------------------------#
[32m[20230113 20:19:03 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:19:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:03 @agent_ppo2.py:186][0m |           0.0026 |           7.9145 |          10.3216 |
[32m[20230113 20:19:03 @agent_ppo2.py:186][0m |          -0.0092 |           4.9532 |          10.3139 |
[32m[20230113 20:19:03 @agent_ppo2.py:186][0m |          -0.0096 |           4.4472 |          10.3082 |
[32m[20230113 20:19:03 @agent_ppo2.py:186][0m |          -0.0117 |           4.1903 |          10.3060 |
[32m[20230113 20:19:03 @agent_ppo2.py:186][0m |          -0.0122 |           3.9907 |          10.3094 |
[32m[20230113 20:19:03 @agent_ppo2.py:186][0m |          -0.0157 |           3.8377 |          10.3106 |
[32m[20230113 20:19:03 @agent_ppo2.py:186][0m |          -0.0087 |           3.7333 |          10.3005 |
[32m[20230113 20:19:03 @agent_ppo2.py:186][0m |          -0.0221 |           3.6715 |          10.3035 |
[32m[20230113 20:19:03 @agent_ppo2.py:186][0m |          -0.0137 |           3.5997 |          10.3041 |
[32m[20230113 20:19:03 @agent_ppo2.py:186][0m |          -0.0169 |           3.4586 |          10.2959 |
[32m[20230113 20:19:03 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:19:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.75
[32m[20230113 20:19:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.66
[32m[20230113 20:19:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 152.79
[32m[20230113 20:19:04 @agent_ppo2.py:144][0m Total time:      34.52 min
[32m[20230113 20:19:04 @agent_ppo2.py:146][0m 3203072 total steps have happened
[32m[20230113 20:19:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1564 --------------------------#
[32m[20230113 20:19:04 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:19:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:04 @agent_ppo2.py:186][0m |          -0.0000 |           6.2798 |          10.3938 |
[32m[20230113 20:19:04 @agent_ppo2.py:186][0m |          -0.0065 |           5.2684 |          10.3833 |
[32m[20230113 20:19:04 @agent_ppo2.py:186][0m |          -0.0086 |           4.9296 |          10.3974 |
[32m[20230113 20:19:04 @agent_ppo2.py:186][0m |          -0.0091 |           4.6397 |          10.3858 |
[32m[20230113 20:19:04 @agent_ppo2.py:186][0m |          -0.0122 |           4.5023 |          10.3777 |
[32m[20230113 20:19:05 @agent_ppo2.py:186][0m |          -0.0124 |           4.3173 |          10.3874 |
[32m[20230113 20:19:05 @agent_ppo2.py:186][0m |          -0.0140 |           4.2028 |          10.3877 |
[32m[20230113 20:19:05 @agent_ppo2.py:186][0m |          -0.0136 |           4.1087 |          10.3775 |
[32m[20230113 20:19:05 @agent_ppo2.py:186][0m |          -0.0152 |           4.0046 |          10.3897 |
[32m[20230113 20:19:05 @agent_ppo2.py:186][0m |          -0.0161 |           3.9856 |          10.3790 |
[32m[20230113 20:19:05 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.69
[32m[20230113 20:19:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.65
[32m[20230113 20:19:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.87
[32m[20230113 20:19:05 @agent_ppo2.py:144][0m Total time:      34.54 min
[32m[20230113 20:19:05 @agent_ppo2.py:146][0m 3205120 total steps have happened
[32m[20230113 20:19:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1565 --------------------------#
[32m[20230113 20:19:06 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:19:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:06 @agent_ppo2.py:186][0m |          -0.0024 |           7.4842 |          10.5936 |
[32m[20230113 20:19:06 @agent_ppo2.py:186][0m |          -0.0097 |           5.0561 |          10.5803 |
[32m[20230113 20:19:06 @agent_ppo2.py:186][0m |          -0.0129 |           4.5103 |          10.5608 |
[32m[20230113 20:19:06 @agent_ppo2.py:186][0m |          -0.0145 |           4.1904 |          10.5567 |
[32m[20230113 20:19:06 @agent_ppo2.py:186][0m |          -0.0156 |           3.9859 |          10.5599 |
[32m[20230113 20:19:06 @agent_ppo2.py:186][0m |          -0.0177 |           3.8145 |          10.5582 |
[32m[20230113 20:19:06 @agent_ppo2.py:186][0m |          -0.0174 |           3.6588 |          10.5502 |
[32m[20230113 20:19:06 @agent_ppo2.py:186][0m |          -0.0185 |           3.5520 |          10.5592 |
[32m[20230113 20:19:06 @agent_ppo2.py:186][0m |          -0.0202 |           3.4392 |          10.5468 |
[32m[20230113 20:19:06 @agent_ppo2.py:186][0m |          -0.0197 |           3.3612 |          10.5533 |
[32m[20230113 20:19:06 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.11
[32m[20230113 20:19:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.18
[32m[20230113 20:19:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.85
[32m[20230113 20:19:06 @agent_ppo2.py:144][0m Total time:      34.56 min
[32m[20230113 20:19:06 @agent_ppo2.py:146][0m 3207168 total steps have happened
[32m[20230113 20:19:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1566 --------------------------#
[32m[20230113 20:19:07 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:07 @agent_ppo2.py:186][0m |          -0.0000 |           6.2907 |          10.3172 |
[32m[20230113 20:19:07 @agent_ppo2.py:186][0m |          -0.0044 |           4.8119 |          10.3099 |
[32m[20230113 20:19:07 @agent_ppo2.py:186][0m |          -0.0092 |           4.3624 |          10.3026 |
[32m[20230113 20:19:07 @agent_ppo2.py:186][0m |          -0.0097 |           4.0944 |          10.3042 |
[32m[20230113 20:19:07 @agent_ppo2.py:186][0m |          -0.0083 |           3.8647 |          10.2975 |
[32m[20230113 20:19:07 @agent_ppo2.py:186][0m |          -0.0115 |           3.6646 |          10.2995 |
[32m[20230113 20:19:07 @agent_ppo2.py:186][0m |          -0.0127 |           3.5654 |          10.2932 |
[32m[20230113 20:19:07 @agent_ppo2.py:186][0m |          -0.0138 |           3.4063 |          10.2819 |
[32m[20230113 20:19:07 @agent_ppo2.py:186][0m |          -0.0087 |           3.4242 |          10.2997 |
[32m[20230113 20:19:07 @agent_ppo2.py:186][0m |          -0.0153 |           3.2570 |          10.2952 |
[32m[20230113 20:19:07 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.30
[32m[20230113 20:19:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.36
[32m[20230113 20:19:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.60
[32m[20230113 20:19:08 @agent_ppo2.py:144][0m Total time:      34.58 min
[32m[20230113 20:19:08 @agent_ppo2.py:146][0m 3209216 total steps have happened
[32m[20230113 20:19:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1567 --------------------------#
[32m[20230113 20:19:08 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:19:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:08 @agent_ppo2.py:186][0m |          -0.0041 |           7.5770 |          10.6799 |
[32m[20230113 20:19:08 @agent_ppo2.py:186][0m |           0.0051 |           5.7672 |          10.6675 |
[32m[20230113 20:19:08 @agent_ppo2.py:186][0m |          -0.0046 |           5.1822 |          10.6529 |
[32m[20230113 20:19:08 @agent_ppo2.py:186][0m |          -0.0098 |           4.7483 |          10.6459 |
[32m[20230113 20:19:08 @agent_ppo2.py:186][0m |          -0.0155 |           4.5689 |          10.6496 |
[32m[20230113 20:19:08 @agent_ppo2.py:186][0m |          -0.0140 |           4.3725 |          10.6478 |
[32m[20230113 20:19:08 @agent_ppo2.py:186][0m |          -0.0455 |           4.2075 |          10.6369 |
[32m[20230113 20:19:08 @agent_ppo2.py:186][0m |          -0.0001 |           4.0180 |          10.6347 |
[32m[20230113 20:19:08 @agent_ppo2.py:186][0m |          -0.0118 |           3.9290 |          10.6529 |
[32m[20230113 20:19:08 @agent_ppo2.py:186][0m |          -0.0243 |           3.8211 |          10.6510 |
[32m[20230113 20:19:08 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:19:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.88
[32m[20230113 20:19:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.55
[32m[20230113 20:19:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 131.51
[32m[20230113 20:19:09 @agent_ppo2.py:144][0m Total time:      34.60 min
[32m[20230113 20:19:09 @agent_ppo2.py:146][0m 3211264 total steps have happened
[32m[20230113 20:19:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1568 --------------------------#
[32m[20230113 20:19:09 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:19:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:09 @agent_ppo2.py:186][0m |           0.0013 |           6.0742 |          10.2933 |
[32m[20230113 20:19:09 @agent_ppo2.py:186][0m |          -0.0034 |           5.1778 |          10.3022 |
[32m[20230113 20:19:09 @agent_ppo2.py:186][0m |          -0.0083 |           4.9010 |          10.3163 |
[32m[20230113 20:19:09 @agent_ppo2.py:186][0m |          -0.0068 |           4.6694 |          10.3105 |
[32m[20230113 20:19:09 @agent_ppo2.py:186][0m |          -0.0092 |           4.4917 |          10.3177 |
[32m[20230113 20:19:09 @agent_ppo2.py:186][0m |          -0.0132 |           4.3670 |          10.3219 |
[32m[20230113 20:19:10 @agent_ppo2.py:186][0m |          -0.0115 |           4.2945 |          10.3230 |
[32m[20230113 20:19:10 @agent_ppo2.py:186][0m |          -0.0133 |           4.1731 |          10.3199 |
[32m[20230113 20:19:10 @agent_ppo2.py:186][0m |          -0.0169 |           4.0892 |          10.3213 |
[32m[20230113 20:19:10 @agent_ppo2.py:186][0m |          -0.0147 |           4.0371 |          10.3294 |
[32m[20230113 20:19:10 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:19:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.44
[32m[20230113 20:19:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.80
[32m[20230113 20:19:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.55
[32m[20230113 20:19:10 @agent_ppo2.py:144][0m Total time:      34.63 min
[32m[20230113 20:19:10 @agent_ppo2.py:146][0m 3213312 total steps have happened
[32m[20230113 20:19:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1569 --------------------------#
[32m[20230113 20:19:10 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:11 @agent_ppo2.py:186][0m |          -0.0008 |           6.4554 |          10.6982 |
[32m[20230113 20:19:11 @agent_ppo2.py:186][0m |          -0.0086 |           5.2351 |          10.6820 |
[32m[20230113 20:19:11 @agent_ppo2.py:186][0m |          -0.0108 |           4.7492 |          10.6847 |
[32m[20230113 20:19:11 @agent_ppo2.py:186][0m |          -0.0131 |           4.4403 |          10.6733 |
[32m[20230113 20:19:11 @agent_ppo2.py:186][0m |          -0.0136 |           4.2190 |          10.6920 |
[32m[20230113 20:19:11 @agent_ppo2.py:186][0m |          -0.0154 |           4.0203 |          10.6832 |
[32m[20230113 20:19:11 @agent_ppo2.py:186][0m |          -0.0110 |           3.9340 |          10.6789 |
[32m[20230113 20:19:11 @agent_ppo2.py:186][0m |          -0.0156 |           3.7424 |          10.6840 |
[32m[20230113 20:19:11 @agent_ppo2.py:186][0m |          -0.0195 |           3.6661 |          10.6835 |
[32m[20230113 20:19:11 @agent_ppo2.py:186][0m |          -0.0173 |           3.5278 |          10.6880 |
[32m[20230113 20:19:11 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.12
[32m[20230113 20:19:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.38
[32m[20230113 20:19:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.34
[32m[20230113 20:19:11 @agent_ppo2.py:144][0m Total time:      34.65 min
[32m[20230113 20:19:11 @agent_ppo2.py:146][0m 3215360 total steps have happened
[32m[20230113 20:19:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1570 --------------------------#
[32m[20230113 20:19:12 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:12 @agent_ppo2.py:186][0m |           0.0023 |           7.3163 |          10.7034 |
[32m[20230113 20:19:12 @agent_ppo2.py:186][0m |          -0.0067 |           4.9570 |          10.6936 |
[32m[20230113 20:19:12 @agent_ppo2.py:186][0m |          -0.0094 |           4.3000 |          10.6823 |
[32m[20230113 20:19:12 @agent_ppo2.py:186][0m |          -0.0105 |           3.9327 |          10.6873 |
[32m[20230113 20:19:12 @agent_ppo2.py:186][0m |          -0.0120 |           3.7017 |          10.6871 |
[32m[20230113 20:19:12 @agent_ppo2.py:186][0m |          -0.0129 |           3.5239 |          10.6820 |
[32m[20230113 20:19:12 @agent_ppo2.py:186][0m |          -0.0127 |           3.3876 |          10.6736 |
[32m[20230113 20:19:12 @agent_ppo2.py:186][0m |          -0.0139 |           3.2318 |          10.6785 |
[32m[20230113 20:19:12 @agent_ppo2.py:186][0m |          -0.0166 |           3.1747 |          10.6808 |
[32m[20230113 20:19:12 @agent_ppo2.py:186][0m |          -0.0182 |           3.1108 |          10.6730 |
[32m[20230113 20:19:12 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:19:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.45
[32m[20230113 20:19:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.66
[32m[20230113 20:19:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.35
[32m[20230113 20:19:13 @agent_ppo2.py:144][0m Total time:      34.67 min
[32m[20230113 20:19:13 @agent_ppo2.py:146][0m 3217408 total steps have happened
[32m[20230113 20:19:13 @agent_ppo2.py:122][0m #------------------------ Iteration 1571 --------------------------#
[32m[20230113 20:19:13 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:19:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:13 @agent_ppo2.py:186][0m |           0.0020 |          16.3173 |          10.5207 |
[32m[20230113 20:19:13 @agent_ppo2.py:186][0m |          -0.0049 |           6.2643 |          10.5003 |
[32m[20230113 20:19:13 @agent_ppo2.py:186][0m |          -0.0090 |           5.1329 |          10.4936 |
[32m[20230113 20:19:13 @agent_ppo2.py:186][0m |          -0.0116 |           4.5651 |          10.4902 |
[32m[20230113 20:19:13 @agent_ppo2.py:186][0m |          -0.0127 |           4.1533 |          10.4801 |
[32m[20230113 20:19:13 @agent_ppo2.py:186][0m |          -0.0132 |           3.8932 |          10.4809 |
[32m[20230113 20:19:13 @agent_ppo2.py:186][0m |          -0.0128 |           3.6976 |          10.4870 |
[32m[20230113 20:19:13 @agent_ppo2.py:186][0m |          -0.0155 |           3.5372 |          10.4857 |
[32m[20230113 20:19:13 @agent_ppo2.py:186][0m |          -0.0167 |           3.3812 |          10.4766 |
[32m[20230113 20:19:13 @agent_ppo2.py:186][0m |          -0.0168 |           3.2984 |          10.4769 |
[32m[20230113 20:19:13 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:19:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 111.76
[32m[20230113 20:19:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.36
[32m[20230113 20:19:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.45
[32m[20230113 20:19:14 @agent_ppo2.py:144][0m Total time:      34.69 min
[32m[20230113 20:19:14 @agent_ppo2.py:146][0m 3219456 total steps have happened
[32m[20230113 20:19:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1572 --------------------------#
[32m[20230113 20:19:14 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:19:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:14 @agent_ppo2.py:186][0m |           0.0009 |           7.8045 |          10.7491 |
[32m[20230113 20:19:14 @agent_ppo2.py:186][0m |          -0.0024 |           5.8902 |          10.7421 |
[32m[20230113 20:19:14 @agent_ppo2.py:186][0m |          -0.0061 |           5.3545 |          10.7433 |
[32m[20230113 20:19:14 @agent_ppo2.py:186][0m |          -0.0064 |           4.8482 |          10.7429 |
[32m[20230113 20:19:14 @agent_ppo2.py:186][0m |          -0.0089 |           4.5924 |          10.7323 |
[32m[20230113 20:19:14 @agent_ppo2.py:186][0m |          -0.0089 |           4.4075 |          10.7319 |
[32m[20230113 20:19:14 @agent_ppo2.py:186][0m |          -0.0096 |           4.2424 |          10.7419 |
[32m[20230113 20:19:15 @agent_ppo2.py:186][0m |          -0.0103 |           4.1562 |          10.7301 |
[32m[20230113 20:19:15 @agent_ppo2.py:186][0m |          -0.0110 |           3.9612 |          10.7267 |
[32m[20230113 20:19:15 @agent_ppo2.py:186][0m |          -0.0135 |           3.8832 |          10.7400 |
[32m[20230113 20:19:15 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.17
[32m[20230113 20:19:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.06
[32m[20230113 20:19:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.19
[32m[20230113 20:19:15 @agent_ppo2.py:144][0m Total time:      34.71 min
[32m[20230113 20:19:15 @agent_ppo2.py:146][0m 3221504 total steps have happened
[32m[20230113 20:19:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1573 --------------------------#
[32m[20230113 20:19:15 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:16 @agent_ppo2.py:186][0m |           0.0011 |           6.6101 |          10.5704 |
[32m[20230113 20:19:16 @agent_ppo2.py:186][0m |          -0.0076 |           4.7895 |          10.5564 |
[32m[20230113 20:19:16 @agent_ppo2.py:186][0m |          -0.0120 |           4.2983 |          10.5461 |
[32m[20230113 20:19:16 @agent_ppo2.py:186][0m |          -0.0112 |           4.0362 |          10.5411 |
[32m[20230113 20:19:16 @agent_ppo2.py:186][0m |          -0.0117 |           3.8605 |          10.5510 |
[32m[20230113 20:19:16 @agent_ppo2.py:186][0m |          -0.0136 |           3.7400 |          10.5438 |
[32m[20230113 20:19:16 @agent_ppo2.py:186][0m |          -0.0138 |           3.6678 |          10.5467 |
[32m[20230113 20:19:16 @agent_ppo2.py:186][0m |          -0.0149 |           3.4990 |          10.5438 |
[32m[20230113 20:19:16 @agent_ppo2.py:186][0m |          -0.0172 |           3.4204 |          10.5481 |
[32m[20230113 20:19:16 @agent_ppo2.py:186][0m |          -0.0160 |           3.3618 |          10.5386 |
[32m[20230113 20:19:16 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.03
[32m[20230113 20:19:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.51
[32m[20230113 20:19:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.20
[32m[20230113 20:19:16 @agent_ppo2.py:144][0m Total time:      34.73 min
[32m[20230113 20:19:16 @agent_ppo2.py:146][0m 3223552 total steps have happened
[32m[20230113 20:19:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1574 --------------------------#
[32m[20230113 20:19:17 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:17 @agent_ppo2.py:186][0m |           0.0005 |           6.5288 |          10.6802 |
[32m[20230113 20:19:17 @agent_ppo2.py:186][0m |          -0.0069 |           4.7452 |          10.6628 |
[32m[20230113 20:19:17 @agent_ppo2.py:186][0m |          -0.0101 |           4.3005 |          10.6648 |
[32m[20230113 20:19:17 @agent_ppo2.py:186][0m |          -0.0098 |           4.0704 |          10.6522 |
[32m[20230113 20:19:17 @agent_ppo2.py:186][0m |          -0.0164 |           3.8632 |          10.6554 |
[32m[20230113 20:19:17 @agent_ppo2.py:186][0m |          -0.0116 |           3.7252 |          10.6570 |
[32m[20230113 20:19:17 @agent_ppo2.py:186][0m |          -0.0135 |           3.6157 |          10.6494 |
[32m[20230113 20:19:17 @agent_ppo2.py:186][0m |          -0.0091 |           3.5641 |          10.6452 |
[32m[20230113 20:19:17 @agent_ppo2.py:186][0m |          -0.0128 |           3.4535 |          10.6538 |
[32m[20230113 20:19:17 @agent_ppo2.py:186][0m |          -0.0179 |           3.3465 |          10.6504 |
[32m[20230113 20:19:17 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.63
[32m[20230113 20:19:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.35
[32m[20230113 20:19:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.22
[32m[20230113 20:19:18 @agent_ppo2.py:144][0m Total time:      34.75 min
[32m[20230113 20:19:18 @agent_ppo2.py:146][0m 3225600 total steps have happened
[32m[20230113 20:19:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1575 --------------------------#
[32m[20230113 20:19:18 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:19:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:18 @agent_ppo2.py:186][0m |           0.0016 |          11.7397 |          10.6026 |
[32m[20230113 20:19:18 @agent_ppo2.py:186][0m |          -0.0073 |           6.6323 |          10.5905 |
[32m[20230113 20:19:18 @agent_ppo2.py:186][0m |          -0.0098 |           5.3393 |          10.5863 |
[32m[20230113 20:19:18 @agent_ppo2.py:186][0m |          -0.0144 |           4.5819 |          10.5913 |
[32m[20230113 20:19:18 @agent_ppo2.py:186][0m |          -0.0159 |           4.1374 |          10.5785 |
[32m[20230113 20:19:18 @agent_ppo2.py:186][0m |          -0.0121 |           3.8788 |          10.5672 |
[32m[20230113 20:19:18 @agent_ppo2.py:186][0m |          -0.0188 |           3.6514 |          10.5742 |
[32m[20230113 20:19:18 @agent_ppo2.py:186][0m |          -0.0190 |           3.4487 |          10.5814 |
[32m[20230113 20:19:18 @agent_ppo2.py:186][0m |          -0.0197 |           3.3073 |          10.5817 |
[32m[20230113 20:19:18 @agent_ppo2.py:186][0m |          -0.0193 |           3.1746 |          10.5683 |
[32m[20230113 20:19:18 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:19:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 163.03
[32m[20230113 20:19:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.85
[32m[20230113 20:19:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.04
[32m[20230113 20:19:19 @agent_ppo2.py:144][0m Total time:      34.77 min
[32m[20230113 20:19:19 @agent_ppo2.py:146][0m 3227648 total steps have happened
[32m[20230113 20:19:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1576 --------------------------#
[32m[20230113 20:19:19 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:19 @agent_ppo2.py:186][0m |           0.0055 |           6.9727 |          10.4316 |
[32m[20230113 20:19:19 @agent_ppo2.py:186][0m |          -0.0059 |           5.2317 |          10.4068 |
[32m[20230113 20:19:19 @agent_ppo2.py:186][0m |          -0.0115 |           4.6351 |          10.4377 |
[32m[20230113 20:19:19 @agent_ppo2.py:186][0m |          -0.0187 |           4.4815 |          10.4387 |
[32m[20230113 20:19:19 @agent_ppo2.py:186][0m |          -0.0041 |           4.0997 |          10.4341 |
[32m[20230113 20:19:19 @agent_ppo2.py:186][0m |          -0.0159 |           3.8505 |          10.4318 |
[32m[20230113 20:19:20 @agent_ppo2.py:186][0m |          -0.0102 |           3.7407 |          10.4293 |
[32m[20230113 20:19:20 @agent_ppo2.py:186][0m |          -0.0150 |           3.6075 |          10.4427 |
[32m[20230113 20:19:20 @agent_ppo2.py:186][0m |          -0.0200 |           3.5074 |          10.4427 |
[32m[20230113 20:19:20 @agent_ppo2.py:186][0m |          -0.0051 |           3.4230 |          10.4267 |
[32m[20230113 20:19:20 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.52
[32m[20230113 20:19:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.90
[32m[20230113 20:19:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.74
[32m[20230113 20:19:20 @agent_ppo2.py:144][0m Total time:      34.79 min
[32m[20230113 20:19:20 @agent_ppo2.py:146][0m 3229696 total steps have happened
[32m[20230113 20:19:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1577 --------------------------#
[32m[20230113 20:19:20 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:21 @agent_ppo2.py:186][0m |           0.0008 |           6.7854 |          10.6449 |
[32m[20230113 20:19:21 @agent_ppo2.py:186][0m |           0.0054 |           5.3207 |          10.6357 |
[32m[20230113 20:19:21 @agent_ppo2.py:186][0m |          -0.0075 |           4.6881 |          10.6220 |
[32m[20230113 20:19:21 @agent_ppo2.py:186][0m |          -0.0078 |           4.2841 |          10.6344 |
[32m[20230113 20:19:21 @agent_ppo2.py:186][0m |          -0.0086 |           4.0073 |          10.6221 |
[32m[20230113 20:19:21 @agent_ppo2.py:186][0m |          -0.0100 |           3.8722 |          10.6287 |
[32m[20230113 20:19:21 @agent_ppo2.py:186][0m |          -0.0093 |           3.7626 |          10.6178 |
[32m[20230113 20:19:21 @agent_ppo2.py:186][0m |          -0.0026 |           3.6694 |          10.6142 |
[32m[20230113 20:19:21 @agent_ppo2.py:186][0m |          -0.0149 |           3.5822 |          10.6172 |
[32m[20230113 20:19:21 @agent_ppo2.py:186][0m |          -0.0107 |           3.4911 |          10.6345 |
[32m[20230113 20:19:21 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.69
[32m[20230113 20:19:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.59
[32m[20230113 20:19:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.05
[32m[20230113 20:19:21 @agent_ppo2.py:144][0m Total time:      34.81 min
[32m[20230113 20:19:21 @agent_ppo2.py:146][0m 3231744 total steps have happened
[32m[20230113 20:19:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1578 --------------------------#
[32m[20230113 20:19:22 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:22 @agent_ppo2.py:186][0m |           0.0012 |           6.6357 |          10.6953 |
[32m[20230113 20:19:22 @agent_ppo2.py:186][0m |          -0.0056 |           4.1169 |          10.6922 |
[32m[20230113 20:19:22 @agent_ppo2.py:186][0m |          -0.0079 |           3.4527 |          10.6826 |
[32m[20230113 20:19:22 @agent_ppo2.py:186][0m |          -0.0125 |           3.1886 |          10.6768 |
[32m[20230113 20:19:22 @agent_ppo2.py:186][0m |          -0.0133 |           2.8958 |          10.6719 |
[32m[20230113 20:19:22 @agent_ppo2.py:186][0m |          -0.0146 |           2.7303 |          10.6700 |
[32m[20230113 20:19:22 @agent_ppo2.py:186][0m |          -0.0155 |           2.6642 |          10.6663 |
[32m[20230113 20:19:22 @agent_ppo2.py:186][0m |          -0.0156 |           2.5383 |          10.6552 |
[32m[20230113 20:19:22 @agent_ppo2.py:186][0m |          -0.0180 |           2.4481 |          10.6494 |
[32m[20230113 20:19:22 @agent_ppo2.py:186][0m |          -0.0173 |           2.4061 |          10.6598 |
[32m[20230113 20:19:22 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.43
[32m[20230113 20:19:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.31
[32m[20230113 20:19:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 133.63
[32m[20230113 20:19:23 @agent_ppo2.py:144][0m Total time:      34.83 min
[32m[20230113 20:19:23 @agent_ppo2.py:146][0m 3233792 total steps have happened
[32m[20230113 20:19:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1579 --------------------------#
[32m[20230113 20:19:23 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:19:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:23 @agent_ppo2.py:186][0m |          -0.0037 |           8.1523 |          10.9422 |
[32m[20230113 20:19:23 @agent_ppo2.py:186][0m |          -0.0073 |           5.9350 |          10.9009 |
[32m[20230113 20:19:23 @agent_ppo2.py:186][0m |          -0.0084 |           4.9175 |          10.8917 |
[32m[20230113 20:19:23 @agent_ppo2.py:186][0m |          -0.0086 |           4.4537 |          10.9058 |
[32m[20230113 20:19:23 @agent_ppo2.py:186][0m |          -0.0084 |           4.1709 |          10.8892 |
[32m[20230113 20:19:23 @agent_ppo2.py:186][0m |          -0.0089 |           3.9036 |          10.9043 |
[32m[20230113 20:19:23 @agent_ppo2.py:186][0m |          -0.0104 |           3.7325 |          10.8840 |
[32m[20230113 20:19:23 @agent_ppo2.py:186][0m |          -0.0111 |           3.6029 |          10.9026 |
[32m[20230113 20:19:23 @agent_ppo2.py:186][0m |          -0.0133 |           3.4897 |          10.8892 |
[32m[20230113 20:19:23 @agent_ppo2.py:186][0m |          -0.0155 |           3.4061 |          10.8988 |
[32m[20230113 20:19:23 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:19:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.64
[32m[20230113 20:19:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.07
[32m[20230113 20:19:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.69
[32m[20230113 20:19:24 @agent_ppo2.py:144][0m Total time:      34.85 min
[32m[20230113 20:19:24 @agent_ppo2.py:146][0m 3235840 total steps have happened
[32m[20230113 20:19:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1580 --------------------------#
[32m[20230113 20:19:24 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:19:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:24 @agent_ppo2.py:186][0m |          -0.0019 |           6.2264 |          10.8482 |
[32m[20230113 20:19:24 @agent_ppo2.py:186][0m |          -0.0031 |           4.4227 |          10.8331 |
[32m[20230113 20:19:24 @agent_ppo2.py:186][0m |          -0.0070 |           3.8112 |          10.8242 |
[32m[20230113 20:19:24 @agent_ppo2.py:186][0m |          -0.0111 |           3.4607 |          10.8284 |
[32m[20230113 20:19:24 @agent_ppo2.py:186][0m |          -0.0124 |           3.2898 |          10.8308 |
[32m[20230113 20:19:25 @agent_ppo2.py:186][0m |          -0.0034 |           3.3838 |          10.8273 |
[32m[20230113 20:19:25 @agent_ppo2.py:186][0m |          -0.0166 |           3.0990 |          10.8292 |
[32m[20230113 20:19:25 @agent_ppo2.py:186][0m |          -0.0060 |           2.9410 |          10.8225 |
[32m[20230113 20:19:25 @agent_ppo2.py:186][0m |          -0.0161 |           2.8890 |          10.8144 |
[32m[20230113 20:19:25 @agent_ppo2.py:186][0m |          -0.0139 |           2.8253 |          10.8225 |
[32m[20230113 20:19:25 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:19:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.72
[32m[20230113 20:19:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.67
[32m[20230113 20:19:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 128.41
[32m[20230113 20:19:25 @agent_ppo2.py:144][0m Total time:      34.87 min
[32m[20230113 20:19:25 @agent_ppo2.py:146][0m 3237888 total steps have happened
[32m[20230113 20:19:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1581 --------------------------#
[32m[20230113 20:19:25 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:26 @agent_ppo2.py:186][0m |           0.0034 |           6.7682 |          11.0978 |
[32m[20230113 20:19:26 @agent_ppo2.py:186][0m |          -0.0034 |           5.0968 |          11.0770 |
[32m[20230113 20:19:26 @agent_ppo2.py:186][0m |          -0.0035 |           4.4278 |          11.0646 |
[32m[20230113 20:19:26 @agent_ppo2.py:186][0m |          -0.0068 |           4.0272 |          11.0721 |
[32m[20230113 20:19:26 @agent_ppo2.py:186][0m |          -0.0096 |           3.8092 |          11.0639 |
[32m[20230113 20:19:26 @agent_ppo2.py:186][0m |          -0.0096 |           3.6002 |          11.0700 |
[32m[20230113 20:19:26 @agent_ppo2.py:186][0m |          -0.0097 |           3.4169 |          11.0517 |
[32m[20230113 20:19:26 @agent_ppo2.py:186][0m |          -0.0124 |           3.2903 |          11.0694 |
[32m[20230113 20:19:26 @agent_ppo2.py:186][0m |          -0.0123 |           3.2095 |          11.0617 |
[32m[20230113 20:19:26 @agent_ppo2.py:186][0m |          -0.0117 |           3.1035 |          11.0685 |
[32m[20230113 20:19:26 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.62
[32m[20230113 20:19:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.78
[32m[20230113 20:19:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.29
[32m[20230113 20:19:26 @agent_ppo2.py:144][0m Total time:      34.90 min
[32m[20230113 20:19:26 @agent_ppo2.py:146][0m 3239936 total steps have happened
[32m[20230113 20:19:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1582 --------------------------#
[32m[20230113 20:19:27 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:27 @agent_ppo2.py:186][0m |          -0.0005 |           5.1211 |          11.0423 |
[32m[20230113 20:19:27 @agent_ppo2.py:186][0m |          -0.0075 |           3.7332 |          11.0288 |
[32m[20230113 20:19:27 @agent_ppo2.py:186][0m |          -0.0074 |           3.3324 |          11.0370 |
[32m[20230113 20:19:27 @agent_ppo2.py:186][0m |          -0.0113 |           3.1047 |          11.0189 |
[32m[20230113 20:19:27 @agent_ppo2.py:186][0m |          -0.0114 |           2.9411 |          11.0315 |
[32m[20230113 20:19:27 @agent_ppo2.py:186][0m |          -0.0112 |           2.8334 |          11.0296 |
[32m[20230113 20:19:27 @agent_ppo2.py:186][0m |          -0.0127 |           2.7445 |          11.0126 |
[32m[20230113 20:19:27 @agent_ppo2.py:186][0m |          -0.0151 |           2.6856 |          11.0307 |
[32m[20230113 20:19:27 @agent_ppo2.py:186][0m |          -0.0148 |           2.6216 |          11.0316 |
[32m[20230113 20:19:27 @agent_ppo2.py:186][0m |          -0.0155 |           2.5866 |          11.0342 |
[32m[20230113 20:19:27 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:19:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.70
[32m[20230113 20:19:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.67
[32m[20230113 20:19:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.86
[32m[20230113 20:19:28 @agent_ppo2.py:144][0m Total time:      34.92 min
[32m[20230113 20:19:28 @agent_ppo2.py:146][0m 3241984 total steps have happened
[32m[20230113 20:19:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1583 --------------------------#
[32m[20230113 20:19:28 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:19:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:28 @agent_ppo2.py:186][0m |          -0.0002 |           6.1568 |          11.1355 |
[32m[20230113 20:19:28 @agent_ppo2.py:186][0m |          -0.0042 |           4.0378 |          11.1130 |
[32m[20230113 20:19:28 @agent_ppo2.py:186][0m |          -0.0056 |           3.5180 |          11.1130 |
[32m[20230113 20:19:28 @agent_ppo2.py:186][0m |          -0.0082 |           3.2911 |          11.1074 |
[32m[20230113 20:19:28 @agent_ppo2.py:186][0m |          -0.0099 |           3.0915 |          11.1037 |
[32m[20230113 20:19:28 @agent_ppo2.py:186][0m |          -0.0108 |           2.9499 |          11.1028 |
[32m[20230113 20:19:28 @agent_ppo2.py:186][0m |          -0.0103 |           2.8615 |          11.1027 |
[32m[20230113 20:19:28 @agent_ppo2.py:186][0m |          -0.0116 |           2.7868 |          11.1023 |
[32m[20230113 20:19:28 @agent_ppo2.py:186][0m |          -0.0126 |           2.7482 |          11.0935 |
[32m[20230113 20:19:28 @agent_ppo2.py:186][0m |          -0.0122 |           2.6400 |          11.0833 |
[32m[20230113 20:19:28 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:19:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.82
[32m[20230113 20:19:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.20
[32m[20230113 20:19:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.24
[32m[20230113 20:19:29 @agent_ppo2.py:144][0m Total time:      34.94 min
[32m[20230113 20:19:29 @agent_ppo2.py:146][0m 3244032 total steps have happened
[32m[20230113 20:19:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1584 --------------------------#
[32m[20230113 20:19:29 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:19:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:29 @agent_ppo2.py:186][0m |           0.0037 |           6.8585 |          11.0561 |
[32m[20230113 20:19:29 @agent_ppo2.py:186][0m |          -0.0058 |           5.4067 |          11.0307 |
[32m[20230113 20:19:29 @agent_ppo2.py:186][0m |           0.0001 |           5.0223 |          11.0411 |
[32m[20230113 20:19:29 @agent_ppo2.py:186][0m |          -0.0157 |           4.8188 |          11.0347 |
[32m[20230113 20:19:29 @agent_ppo2.py:186][0m |          -0.0120 |           4.3265 |          11.0331 |
[32m[20230113 20:19:29 @agent_ppo2.py:186][0m |          -0.0089 |           4.2624 |          11.0449 |
[32m[20230113 20:19:30 @agent_ppo2.py:186][0m |          -0.0118 |           4.1479 |          11.0531 |
[32m[20230113 20:19:30 @agent_ppo2.py:186][0m |          -0.0016 |           3.9754 |          11.0465 |
[32m[20230113 20:19:30 @agent_ppo2.py:186][0m |          -0.0135 |           3.8157 |          11.0436 |
[32m[20230113 20:19:30 @agent_ppo2.py:186][0m |          -0.0149 |           3.6793 |          11.0408 |
[32m[20230113 20:19:30 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:19:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.08
[32m[20230113 20:19:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.50
[32m[20230113 20:19:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.81
[32m[20230113 20:19:30 @agent_ppo2.py:144][0m Total time:      34.96 min
[32m[20230113 20:19:30 @agent_ppo2.py:146][0m 3246080 total steps have happened
[32m[20230113 20:19:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1585 --------------------------#
[32m[20230113 20:19:30 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:31 @agent_ppo2.py:186][0m |           0.0003 |           5.8356 |          11.0875 |
[32m[20230113 20:19:31 @agent_ppo2.py:186][0m |          -0.0069 |           4.8085 |          11.0776 |
[32m[20230113 20:19:31 @agent_ppo2.py:186][0m |          -0.0106 |           4.3670 |          11.0627 |
[32m[20230113 20:19:31 @agent_ppo2.py:186][0m |          -0.0121 |           4.1278 |          11.0691 |
[32m[20230113 20:19:31 @agent_ppo2.py:186][0m |          -0.0133 |           3.9705 |          11.0730 |
[32m[20230113 20:19:31 @agent_ppo2.py:186][0m |          -0.0139 |           3.8132 |          11.0658 |
[32m[20230113 20:19:31 @agent_ppo2.py:186][0m |          -0.0136 |           3.6646 |          11.0700 |
[32m[20230113 20:19:31 @agent_ppo2.py:186][0m |          -0.0150 |           3.5651 |          11.0747 |
[32m[20230113 20:19:31 @agent_ppo2.py:186][0m |          -0.0169 |           3.4218 |          11.0764 |
[32m[20230113 20:19:31 @agent_ppo2.py:186][0m |          -0.0185 |           3.3201 |          11.0698 |
[32m[20230113 20:19:31 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.11
[32m[20230113 20:19:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.89
[32m[20230113 20:19:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.45
[32m[20230113 20:19:31 @agent_ppo2.py:144][0m Total time:      34.98 min
[32m[20230113 20:19:31 @agent_ppo2.py:146][0m 3248128 total steps have happened
[32m[20230113 20:19:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1586 --------------------------#
[32m[20230113 20:19:32 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:32 @agent_ppo2.py:186][0m |           0.0005 |           7.1083 |          11.0283 |
[32m[20230113 20:19:32 @agent_ppo2.py:186][0m |           0.0020 |           5.1538 |          11.0155 |
[32m[20230113 20:19:32 @agent_ppo2.py:186][0m |          -0.0080 |           4.5307 |          10.9956 |
[32m[20230113 20:19:32 @agent_ppo2.py:186][0m |          -0.0015 |           4.1416 |          11.0001 |
[32m[20230113 20:19:32 @agent_ppo2.py:186][0m |          -0.0087 |           3.9055 |          10.9942 |
[32m[20230113 20:19:32 @agent_ppo2.py:186][0m |           0.0021 |           3.7893 |          10.9901 |
[32m[20230113 20:19:32 @agent_ppo2.py:186][0m |          -0.0116 |           3.5762 |          10.9894 |
[32m[20230113 20:19:32 @agent_ppo2.py:186][0m |          -0.0120 |           3.4906 |          10.9942 |
[32m[20230113 20:19:32 @agent_ppo2.py:186][0m |          -0.0225 |           3.3516 |          10.9885 |
[32m[20230113 20:19:32 @agent_ppo2.py:186][0m |          -0.0142 |           3.3376 |          10.9888 |
[32m[20230113 20:19:32 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.13
[32m[20230113 20:19:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.34
[32m[20230113 20:19:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.13
[32m[20230113 20:19:33 @agent_ppo2.py:144][0m Total time:      35.00 min
[32m[20230113 20:19:33 @agent_ppo2.py:146][0m 3250176 total steps have happened
[32m[20230113 20:19:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1587 --------------------------#
[32m[20230113 20:19:33 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:19:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:33 @agent_ppo2.py:186][0m |          -0.0014 |           6.3409 |          10.8864 |
[32m[20230113 20:19:33 @agent_ppo2.py:186][0m |          -0.0067 |           4.8290 |          10.8702 |
[32m[20230113 20:19:33 @agent_ppo2.py:186][0m |          -0.0070 |           4.2880 |          10.8671 |
[32m[20230113 20:19:33 @agent_ppo2.py:186][0m |          -0.0110 |           3.8648 |          10.8539 |
[32m[20230113 20:19:33 @agent_ppo2.py:186][0m |          -0.0164 |           3.6529 |          10.8599 |
[32m[20230113 20:19:33 @agent_ppo2.py:186][0m |          -0.0127 |           3.5828 |          10.8636 |
[32m[20230113 20:19:33 @agent_ppo2.py:186][0m |          -0.0126 |           3.4857 |          10.8582 |
[32m[20230113 20:19:33 @agent_ppo2.py:186][0m |          -0.0149 |           3.3707 |          10.8613 |
[32m[20230113 20:19:33 @agent_ppo2.py:186][0m |          -0.0132 |           3.2757 |          10.8634 |
[32m[20230113 20:19:34 @agent_ppo2.py:186][0m |          -0.0160 |           3.2137 |          10.8622 |
[32m[20230113 20:19:34 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:19:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.75
[32m[20230113 20:19:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.99
[32m[20230113 20:19:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.69
[32m[20230113 20:19:34 @agent_ppo2.py:144][0m Total time:      35.02 min
[32m[20230113 20:19:34 @agent_ppo2.py:146][0m 3252224 total steps have happened
[32m[20230113 20:19:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1588 --------------------------#
[32m[20230113 20:19:34 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:19:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:34 @agent_ppo2.py:186][0m |           0.0021 |           6.7637 |          10.9248 |
[32m[20230113 20:19:34 @agent_ppo2.py:186][0m |          -0.0064 |           4.7539 |          10.9333 |
[32m[20230113 20:19:34 @agent_ppo2.py:186][0m |          -0.0089 |           4.0671 |          10.9369 |
[32m[20230113 20:19:35 @agent_ppo2.py:186][0m |          -0.0110 |           3.6732 |          10.9177 |
[32m[20230113 20:19:35 @agent_ppo2.py:186][0m |          -0.0101 |           3.4049 |          10.9282 |
[32m[20230113 20:19:35 @agent_ppo2.py:186][0m |          -0.0128 |           3.2073 |          10.9280 |
[32m[20230113 20:19:35 @agent_ppo2.py:186][0m |          -0.0144 |           3.0752 |          10.9330 |
[32m[20230113 20:19:35 @agent_ppo2.py:186][0m |          -0.0153 |           2.9520 |          10.9206 |
[32m[20230113 20:19:35 @agent_ppo2.py:186][0m |          -0.0171 |           2.8444 |          10.9259 |
[32m[20230113 20:19:35 @agent_ppo2.py:186][0m |          -0.0154 |           2.7417 |          10.9362 |
[32m[20230113 20:19:35 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:19:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.73
[32m[20230113 20:19:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.20
[32m[20230113 20:19:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 121.97
[32m[20230113 20:19:35 @agent_ppo2.py:144][0m Total time:      35.04 min
[32m[20230113 20:19:35 @agent_ppo2.py:146][0m 3254272 total steps have happened
[32m[20230113 20:19:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1589 --------------------------#
[32m[20230113 20:19:36 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:19:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:36 @agent_ppo2.py:186][0m |           0.0017 |           6.7436 |          10.9850 |
[32m[20230113 20:19:36 @agent_ppo2.py:186][0m |          -0.0070 |           5.3650 |          10.9614 |
[32m[20230113 20:19:36 @agent_ppo2.py:186][0m |          -0.0077 |           4.7824 |          10.9668 |
[32m[20230113 20:19:36 @agent_ppo2.py:186][0m |          -0.0100 |           4.3865 |          10.9487 |
[32m[20230113 20:19:36 @agent_ppo2.py:186][0m |          -0.0118 |           4.1205 |          10.9396 |
[32m[20230113 20:19:36 @agent_ppo2.py:186][0m |          -0.0153 |           3.8771 |          10.9478 |
[32m[20230113 20:19:36 @agent_ppo2.py:186][0m |          -0.0133 |           3.7589 |          10.9437 |
[32m[20230113 20:19:36 @agent_ppo2.py:186][0m |          -0.0179 |           3.6385 |          10.9361 |
[32m[20230113 20:19:36 @agent_ppo2.py:186][0m |          -0.0169 |           3.4902 |          10.9278 |
[32m[20230113 20:19:36 @agent_ppo2.py:186][0m |          -0.0184 |           3.3807 |          10.9304 |
[32m[20230113 20:19:36 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:19:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.39
[32m[20230113 20:19:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.25
[32m[20230113 20:19:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.85
[32m[20230113 20:19:36 @agent_ppo2.py:144][0m Total time:      35.06 min
[32m[20230113 20:19:36 @agent_ppo2.py:146][0m 3256320 total steps have happened
[32m[20230113 20:19:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1590 --------------------------#
[32m[20230113 20:19:37 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:19:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:37 @agent_ppo2.py:186][0m |          -0.0022 |           7.8804 |          10.8762 |
[32m[20230113 20:19:37 @agent_ppo2.py:186][0m |          -0.0088 |           5.6179 |          10.8655 |
[32m[20230113 20:19:37 @agent_ppo2.py:186][0m |          -0.0170 |           5.0494 |          10.8700 |
[32m[20230113 20:19:37 @agent_ppo2.py:186][0m |          -0.0047 |           4.5355 |          10.8636 |
[32m[20230113 20:19:37 @agent_ppo2.py:186][0m |          -0.0119 |           4.1966 |          10.8559 |
[32m[20230113 20:19:37 @agent_ppo2.py:186][0m |          -0.0098 |           4.0495 |          10.8658 |
[32m[20230113 20:19:37 @agent_ppo2.py:186][0m |          -0.0145 |           3.8112 |          10.8646 |
[32m[20230113 20:19:37 @agent_ppo2.py:186][0m |          -0.0046 |           3.9806 |          10.8607 |
[32m[20230113 20:19:37 @agent_ppo2.py:186][0m |          -0.0120 |           3.6312 |          10.8555 |
[32m[20230113 20:19:37 @agent_ppo2.py:186][0m |          -0.0224 |           3.4450 |          10.8492 |
[32m[20230113 20:19:37 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:19:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.43
[32m[20230113 20:19:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.22
[32m[20230113 20:19:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.31
[32m[20230113 20:19:38 @agent_ppo2.py:144][0m Total time:      35.08 min
[32m[20230113 20:19:38 @agent_ppo2.py:146][0m 3258368 total steps have happened
[32m[20230113 20:19:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1591 --------------------------#
[32m[20230113 20:19:38 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 20:19:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:38 @agent_ppo2.py:186][0m |          -0.0029 |          21.8310 |          11.1809 |
[32m[20230113 20:19:38 @agent_ppo2.py:186][0m |          -0.0078 |           6.3791 |          11.1794 |
[32m[20230113 20:19:38 @agent_ppo2.py:186][0m |          -0.0090 |           5.0376 |          11.1770 |
[32m[20230113 20:19:38 @agent_ppo2.py:186][0m |          -0.0092 |           4.3658 |          11.1772 |
[32m[20230113 20:19:38 @agent_ppo2.py:186][0m |          -0.0129 |           4.0078 |          11.1747 |
[32m[20230113 20:19:38 @agent_ppo2.py:186][0m |          -0.0112 |           3.7029 |          11.1751 |
[32m[20230113 20:19:38 @agent_ppo2.py:186][0m |          -0.0138 |           3.5215 |          11.1776 |
[32m[20230113 20:19:38 @agent_ppo2.py:186][0m |          -0.0141 |           3.3154 |          11.1686 |
[32m[20230113 20:19:38 @agent_ppo2.py:186][0m |          -0.0152 |           3.1878 |          11.1745 |
[32m[20230113 20:19:38 @agent_ppo2.py:186][0m |          -0.0187 |           3.0650 |          11.1718 |
[32m[20230113 20:19:38 @agent_ppo2.py:131][0m Policy update time: 0.34 s
[32m[20230113 20:19:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 40.93
[32m[20230113 20:19:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.01
[32m[20230113 20:19:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.43
[32m[20230113 20:19:39 @agent_ppo2.py:144][0m Total time:      35.10 min
[32m[20230113 20:19:39 @agent_ppo2.py:146][0m 3260416 total steps have happened
[32m[20230113 20:19:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1592 --------------------------#
[32m[20230113 20:19:39 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:39 @agent_ppo2.py:186][0m |           0.0007 |           8.8592 |          11.0058 |
[32m[20230113 20:19:39 @agent_ppo2.py:186][0m |          -0.0029 |           6.5094 |          10.9834 |
[32m[20230113 20:19:39 @agent_ppo2.py:186][0m |          -0.0036 |           5.7187 |          10.9797 |
[32m[20230113 20:19:39 @agent_ppo2.py:186][0m |          -0.0102 |           5.2710 |          10.9801 |
[32m[20230113 20:19:39 @agent_ppo2.py:186][0m |          -0.0146 |           5.0045 |          10.9872 |
[32m[20230113 20:19:39 @agent_ppo2.py:186][0m |          -0.0086 |           4.6985 |          10.9734 |
[32m[20230113 20:19:39 @agent_ppo2.py:186][0m |          -0.0167 |           4.4051 |          10.9798 |
[32m[20230113 20:19:40 @agent_ppo2.py:186][0m |          -0.0199 |           4.2464 |          10.9781 |
[32m[20230113 20:19:40 @agent_ppo2.py:186][0m |          -0.0170 |           4.0703 |          10.9753 |
[32m[20230113 20:19:40 @agent_ppo2.py:186][0m |          -0.0147 |           3.9346 |          10.9608 |
[32m[20230113 20:19:40 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.76
[32m[20230113 20:19:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.85
[32m[20230113 20:19:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.91
[32m[20230113 20:19:40 @agent_ppo2.py:144][0m Total time:      35.12 min
[32m[20230113 20:19:40 @agent_ppo2.py:146][0m 3262464 total steps have happened
[32m[20230113 20:19:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1593 --------------------------#
[32m[20230113 20:19:40 @agent_ppo2.py:128][0m Sampling time: 0.39 s by 1 slaves
[32m[20230113 20:19:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:40 @agent_ppo2.py:186][0m |           0.0005 |          26.5703 |          11.1774 |
[32m[20230113 20:19:40 @agent_ppo2.py:186][0m |          -0.0077 |          13.0324 |          11.1455 |
[32m[20230113 20:19:41 @agent_ppo2.py:186][0m |          -0.0090 |           8.8368 |          11.1404 |
[32m[20230113 20:19:41 @agent_ppo2.py:186][0m |          -0.0116 |           6.8979 |          11.1364 |
[32m[20230113 20:19:41 @agent_ppo2.py:186][0m |          -0.0123 |           5.8465 |          11.1426 |
[32m[20230113 20:19:41 @agent_ppo2.py:186][0m |          -0.0126 |           5.1933 |          11.1335 |
[32m[20230113 20:19:41 @agent_ppo2.py:186][0m |          -0.0143 |           4.7976 |          11.1344 |
[32m[20230113 20:19:41 @agent_ppo2.py:186][0m |          -0.0135 |           4.4342 |          11.1379 |
[32m[20230113 20:19:41 @agent_ppo2.py:186][0m |          -0.0150 |           4.2057 |          11.1373 |
[32m[20230113 20:19:41 @agent_ppo2.py:186][0m |          -0.0136 |           3.9992 |          11.1408 |
[32m[20230113 20:19:41 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 20:19:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 161.72
[32m[20230113 20:19:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.30
[32m[20230113 20:19:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.11
[32m[20230113 20:19:41 @agent_ppo2.py:144][0m Total time:      35.14 min
[32m[20230113 20:19:41 @agent_ppo2.py:146][0m 3264512 total steps have happened
[32m[20230113 20:19:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1594 --------------------------#
[32m[20230113 20:19:42 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:42 @agent_ppo2.py:186][0m |           0.0023 |           9.0773 |          10.8059 |
[32m[20230113 20:19:42 @agent_ppo2.py:186][0m |          -0.0082 |           6.2070 |          10.7995 |
[32m[20230113 20:19:42 @agent_ppo2.py:186][0m |          -0.0089 |           5.3856 |          10.7970 |
[32m[20230113 20:19:42 @agent_ppo2.py:186][0m |          -0.0118 |           4.8066 |          10.7985 |
[32m[20230113 20:19:42 @agent_ppo2.py:186][0m |          -0.0126 |           4.5239 |          10.7907 |
[32m[20230113 20:19:42 @agent_ppo2.py:186][0m |          -0.0081 |           4.2224 |          10.7894 |
[32m[20230113 20:19:42 @agent_ppo2.py:186][0m |          -0.0135 |           3.9888 |          10.7977 |
[32m[20230113 20:19:42 @agent_ppo2.py:186][0m |          -0.0134 |           3.8366 |          10.7884 |
[32m[20230113 20:19:42 @agent_ppo2.py:186][0m |          -0.0134 |           3.6784 |          10.7896 |
[32m[20230113 20:19:42 @agent_ppo2.py:186][0m |          -0.0116 |           3.6657 |          10.7847 |
[32m[20230113 20:19:42 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.74
[32m[20230113 20:19:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.88
[32m[20230113 20:19:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 141.51
[32m[20230113 20:19:42 @agent_ppo2.py:144][0m Total time:      35.16 min
[32m[20230113 20:19:42 @agent_ppo2.py:146][0m 3266560 total steps have happened
[32m[20230113 20:19:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1595 --------------------------#
[32m[20230113 20:19:43 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:19:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:43 @agent_ppo2.py:186][0m |          -0.0010 |           6.9082 |          11.2213 |
[32m[20230113 20:19:43 @agent_ppo2.py:186][0m |          -0.0073 |           5.3095 |          11.1984 |
[32m[20230113 20:19:43 @agent_ppo2.py:186][0m |          -0.0091 |           4.7361 |          11.2011 |
[32m[20230113 20:19:43 @agent_ppo2.py:186][0m |          -0.0112 |           4.3513 |          11.2125 |
[32m[20230113 20:19:43 @agent_ppo2.py:186][0m |          -0.0137 |           4.1006 |          11.2065 |
[32m[20230113 20:19:43 @agent_ppo2.py:186][0m |          -0.0141 |           3.8715 |          11.1900 |
[32m[20230113 20:19:43 @agent_ppo2.py:186][0m |          -0.0158 |           3.7199 |          11.1963 |
[32m[20230113 20:19:43 @agent_ppo2.py:186][0m |          -0.0156 |           3.6041 |          11.1954 |
[32m[20230113 20:19:43 @agent_ppo2.py:186][0m |          -0.0178 |           3.4636 |          11.1898 |
[32m[20230113 20:19:43 @agent_ppo2.py:186][0m |          -0.0186 |           3.3800 |          11.1941 |
[32m[20230113 20:19:43 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:19:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.85
[32m[20230113 20:19:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.56
[32m[20230113 20:19:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 133.43
[32m[20230113 20:19:44 @agent_ppo2.py:144][0m Total time:      35.18 min
[32m[20230113 20:19:44 @agent_ppo2.py:146][0m 3268608 total steps have happened
[32m[20230113 20:19:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1596 --------------------------#
[32m[20230113 20:19:44 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:44 @agent_ppo2.py:186][0m |           0.0222 |          13.0138 |          10.9676 |
[32m[20230113 20:19:44 @agent_ppo2.py:186][0m |          -0.0124 |           6.5384 |          10.9511 |
[32m[20230113 20:19:44 @agent_ppo2.py:186][0m |           0.0083 |           5.3684 |          10.9389 |
[32m[20230113 20:19:44 @agent_ppo2.py:186][0m |           0.0013 |           4.8093 |          10.9449 |
[32m[20230113 20:19:44 @agent_ppo2.py:186][0m |          -0.0208 |           4.4812 |          10.9306 |
[32m[20230113 20:19:44 @agent_ppo2.py:186][0m |          -0.0207 |           4.1146 |          10.9367 |
[32m[20230113 20:19:44 @agent_ppo2.py:186][0m |          -0.0240 |           3.8625 |          10.9291 |
[32m[20230113 20:19:44 @agent_ppo2.py:186][0m |          -0.0224 |           3.6979 |          10.9350 |
[32m[20230113 20:19:44 @agent_ppo2.py:186][0m |          -0.0228 |           3.5976 |          10.9390 |
[32m[20230113 20:19:44 @agent_ppo2.py:186][0m |          -0.0160 |           3.4688 |          10.9357 |
[32m[20230113 20:19:44 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:19:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 163.45
[32m[20230113 20:19:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.25
[32m[20230113 20:19:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.23
[32m[20230113 20:19:45 @agent_ppo2.py:144][0m Total time:      35.20 min
[32m[20230113 20:19:45 @agent_ppo2.py:146][0m 3270656 total steps have happened
[32m[20230113 20:19:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1597 --------------------------#
[32m[20230113 20:19:45 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:45 @agent_ppo2.py:186][0m |          -0.0026 |           6.6867 |          10.9429 |
[32m[20230113 20:19:45 @agent_ppo2.py:186][0m |          -0.0083 |           4.9652 |          10.9164 |
[32m[20230113 20:19:45 @agent_ppo2.py:186][0m |          -0.0097 |           4.3774 |          10.9344 |
[32m[20230113 20:19:45 @agent_ppo2.py:186][0m |          -0.0118 |           3.9997 |          10.9205 |
[32m[20230113 20:19:45 @agent_ppo2.py:186][0m |          -0.0130 |           3.8406 |          10.9220 |
[32m[20230113 20:19:46 @agent_ppo2.py:186][0m |          -0.0135 |           3.6118 |          10.9263 |
[32m[20230113 20:19:46 @agent_ppo2.py:186][0m |          -0.0149 |           3.5119 |          10.9285 |
[32m[20230113 20:19:46 @agent_ppo2.py:186][0m |          -0.0171 |           3.3664 |          10.9305 |
[32m[20230113 20:19:46 @agent_ppo2.py:186][0m |          -0.0178 |           3.2708 |          10.9190 |
[32m[20230113 20:19:46 @agent_ppo2.py:186][0m |          -0.0154 |           3.1745 |          10.9279 |
[32m[20230113 20:19:46 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.20
[32m[20230113 20:19:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.41
[32m[20230113 20:19:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.13
[32m[20230113 20:19:46 @agent_ppo2.py:144][0m Total time:      35.22 min
[32m[20230113 20:19:46 @agent_ppo2.py:146][0m 3272704 total steps have happened
[32m[20230113 20:19:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1598 --------------------------#
[32m[20230113 20:19:46 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:19:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:47 @agent_ppo2.py:186][0m |          -0.0015 |           6.8011 |          11.2371 |
[32m[20230113 20:19:47 @agent_ppo2.py:186][0m |          -0.0086 |           5.4965 |          11.2269 |
[32m[20230113 20:19:47 @agent_ppo2.py:186][0m |          -0.0076 |           5.0252 |          11.2295 |
[32m[20230113 20:19:47 @agent_ppo2.py:186][0m |          -0.0102 |           4.6799 |          11.2140 |
[32m[20230113 20:19:47 @agent_ppo2.py:186][0m |          -0.0104 |           4.4231 |          11.2148 |
[32m[20230113 20:19:47 @agent_ppo2.py:186][0m |          -0.0110 |           4.2232 |          11.2126 |
[32m[20230113 20:19:47 @agent_ppo2.py:186][0m |          -0.0091 |           4.1034 |          11.2095 |
[32m[20230113 20:19:47 @agent_ppo2.py:186][0m |          -0.0097 |           3.9441 |          11.2088 |
[32m[20230113 20:19:47 @agent_ppo2.py:186][0m |          -0.0161 |           3.7903 |          11.2095 |
[32m[20230113 20:19:47 @agent_ppo2.py:186][0m |          -0.0165 |           3.7021 |          11.2061 |
[32m[20230113 20:19:47 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:19:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.95
[32m[20230113 20:19:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.92
[32m[20230113 20:19:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.80
[32m[20230113 20:19:47 @agent_ppo2.py:144][0m Total time:      35.25 min
[32m[20230113 20:19:47 @agent_ppo2.py:146][0m 3274752 total steps have happened
[32m[20230113 20:19:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1599 --------------------------#
[32m[20230113 20:19:48 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:19:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:48 @agent_ppo2.py:186][0m |          -0.0024 |           5.7348 |          11.1348 |
[32m[20230113 20:19:48 @agent_ppo2.py:186][0m |          -0.0037 |           4.3917 |          11.1190 |
[32m[20230113 20:19:48 @agent_ppo2.py:186][0m |          -0.0106 |           3.8706 |          11.1091 |
[32m[20230113 20:19:48 @agent_ppo2.py:186][0m |          -0.0110 |           3.6738 |          11.1059 |
[32m[20230113 20:19:48 @agent_ppo2.py:186][0m |          -0.0138 |           3.4641 |          11.1098 |
[32m[20230113 20:19:48 @agent_ppo2.py:186][0m |          -0.0128 |           3.3323 |          11.1019 |
[32m[20230113 20:19:48 @agent_ppo2.py:186][0m |          -0.0140 |           3.2114 |          11.0973 |
[32m[20230113 20:19:48 @agent_ppo2.py:186][0m |          -0.0133 |           3.1509 |          11.1043 |
[32m[20230113 20:19:48 @agent_ppo2.py:186][0m |          -0.0179 |           3.0770 |          11.1084 |
[32m[20230113 20:19:48 @agent_ppo2.py:186][0m |          -0.0153 |           2.9755 |          11.0982 |
[32m[20230113 20:19:48 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:19:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.00
[32m[20230113 20:19:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.30
[32m[20230113 20:19:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.80
[32m[20230113 20:19:49 @agent_ppo2.py:144][0m Total time:      35.27 min
[32m[20230113 20:19:49 @agent_ppo2.py:146][0m 3276800 total steps have happened
[32m[20230113 20:19:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1600 --------------------------#
[32m[20230113 20:19:49 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:19:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:49 @agent_ppo2.py:186][0m |           0.0021 |           6.6137 |          10.9645 |
[32m[20230113 20:19:49 @agent_ppo2.py:186][0m |          -0.0029 |           4.9880 |          10.9510 |
[32m[20230113 20:19:49 @agent_ppo2.py:186][0m |          -0.0078 |           4.4802 |          10.9434 |
[32m[20230113 20:19:49 @agent_ppo2.py:186][0m |          -0.0093 |           4.1493 |          10.9409 |
[32m[20230113 20:19:49 @agent_ppo2.py:186][0m |          -0.0111 |           3.9221 |          10.9439 |
[32m[20230113 20:19:49 @agent_ppo2.py:186][0m |          -0.0109 |           3.7804 |          10.9397 |
[32m[20230113 20:19:49 @agent_ppo2.py:186][0m |          -0.0115 |           3.6226 |          10.9400 |
[32m[20230113 20:19:49 @agent_ppo2.py:186][0m |          -0.0134 |           3.5351 |          10.9332 |
[32m[20230113 20:19:49 @agent_ppo2.py:186][0m |          -0.0143 |           3.4263 |          10.9398 |
[32m[20230113 20:19:50 @agent_ppo2.py:186][0m |          -0.0138 |           3.3310 |          10.9323 |
[32m[20230113 20:19:50 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.88
[32m[20230113 20:19:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.77
[32m[20230113 20:19:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.26
[32m[20230113 20:19:50 @agent_ppo2.py:144][0m Total time:      35.29 min
[32m[20230113 20:19:50 @agent_ppo2.py:146][0m 3278848 total steps have happened
[32m[20230113 20:19:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1601 --------------------------#
[32m[20230113 20:19:50 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:19:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:50 @agent_ppo2.py:186][0m |           0.0020 |           6.4194 |          11.0317 |
[32m[20230113 20:19:50 @agent_ppo2.py:186][0m |          -0.0045 |           4.6890 |          11.0221 |
[32m[20230113 20:19:51 @agent_ppo2.py:186][0m |          -0.0087 |           4.2544 |          11.0108 |
[32m[20230113 20:19:51 @agent_ppo2.py:186][0m |          -0.0074 |           3.9838 |          11.0302 |
[32m[20230113 20:19:51 @agent_ppo2.py:186][0m |          -0.0109 |           3.8374 |          11.0184 |
[32m[20230113 20:19:51 @agent_ppo2.py:186][0m |          -0.0122 |           3.6612 |          11.0210 |
[32m[20230113 20:19:51 @agent_ppo2.py:186][0m |          -0.0136 |           3.5362 |          11.0166 |
[32m[20230113 20:19:51 @agent_ppo2.py:186][0m |          -0.0141 |           3.4614 |          11.0161 |
[32m[20230113 20:19:51 @agent_ppo2.py:186][0m |          -0.0148 |           3.3641 |          11.0121 |
[32m[20230113 20:19:51 @agent_ppo2.py:186][0m |          -0.0170 |           3.2547 |          11.0129 |
[32m[20230113 20:19:51 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:19:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.47
[32m[20230113 20:19:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.43
[32m[20230113 20:19:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.92
[32m[20230113 20:19:51 @agent_ppo2.py:144][0m Total time:      35.31 min
[32m[20230113 20:19:51 @agent_ppo2.py:146][0m 3280896 total steps have happened
[32m[20230113 20:19:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1602 --------------------------#
[32m[20230113 20:19:52 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:19:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:52 @agent_ppo2.py:186][0m |           0.0038 |          10.2410 |          11.0249 |
[32m[20230113 20:19:52 @agent_ppo2.py:186][0m |          -0.0036 |           6.8414 |          11.0010 |
[32m[20230113 20:19:52 @agent_ppo2.py:186][0m |          -0.0084 |           6.0149 |          11.0004 |
[32m[20230113 20:19:52 @agent_ppo2.py:186][0m |          -0.0077 |           5.4710 |          10.9900 |
[32m[20230113 20:19:52 @agent_ppo2.py:186][0m |          -0.0084 |           5.0827 |          10.9808 |
[32m[20230113 20:19:52 @agent_ppo2.py:186][0m |          -0.0079 |           4.8712 |          10.9878 |
[32m[20230113 20:19:52 @agent_ppo2.py:186][0m |          -0.0090 |           4.6506 |          10.9905 |
[32m[20230113 20:19:52 @agent_ppo2.py:186][0m |          -0.0149 |           4.4760 |          10.9966 |
[32m[20230113 20:19:52 @agent_ppo2.py:186][0m |          -0.0163 |           4.3128 |          11.0001 |
[32m[20230113 20:19:52 @agent_ppo2.py:186][0m |          -0.0166 |           4.2563 |          10.9953 |
[32m[20230113 20:19:52 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 20:19:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 142.92
[32m[20230113 20:19:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.80
[32m[20230113 20:19:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.91
[32m[20230113 20:19:53 @agent_ppo2.py:144][0m Total time:      35.33 min
[32m[20230113 20:19:53 @agent_ppo2.py:146][0m 3282944 total steps have happened
[32m[20230113 20:19:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1603 --------------------------#
[32m[20230113 20:19:53 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:53 @agent_ppo2.py:186][0m |           0.0006 |           7.8592 |          11.2811 |
[32m[20230113 20:19:53 @agent_ppo2.py:186][0m |          -0.0051 |           5.7818 |          11.2562 |
[32m[20230113 20:19:53 @agent_ppo2.py:186][0m |          -0.0080 |           4.8599 |          11.2584 |
[32m[20230113 20:19:53 @agent_ppo2.py:186][0m |          -0.0104 |           4.3104 |          11.2616 |
[32m[20230113 20:19:53 @agent_ppo2.py:186][0m |          -0.0108 |           3.9867 |          11.2586 |
[32m[20230113 20:19:53 @agent_ppo2.py:186][0m |          -0.0122 |           3.7620 |          11.2515 |
[32m[20230113 20:19:53 @agent_ppo2.py:186][0m |          -0.0128 |           3.5346 |          11.2611 |
[32m[20230113 20:19:53 @agent_ppo2.py:186][0m |          -0.0142 |           3.4233 |          11.2538 |
[32m[20230113 20:19:53 @agent_ppo2.py:186][0m |          -0.0142 |           3.2689 |          11.2547 |
[32m[20230113 20:19:53 @agent_ppo2.py:186][0m |          -0.0153 |           3.1632 |          11.2448 |
[32m[20230113 20:19:53 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:19:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.49
[32m[20230113 20:19:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.72
[32m[20230113 20:19:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.29
[32m[20230113 20:19:54 @agent_ppo2.py:144][0m Total time:      35.36 min
[32m[20230113 20:19:54 @agent_ppo2.py:146][0m 3284992 total steps have happened
[32m[20230113 20:19:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1604 --------------------------#
[32m[20230113 20:19:54 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 20:19:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:54 @agent_ppo2.py:186][0m |          -0.0035 |           5.6868 |          11.2261 |
[32m[20230113 20:19:54 @agent_ppo2.py:186][0m |          -0.0062 |           4.3689 |          11.2019 |
[32m[20230113 20:19:54 @agent_ppo2.py:186][0m |          -0.0149 |           4.0157 |          11.1909 |
[32m[20230113 20:19:54 @agent_ppo2.py:186][0m |          -0.0136 |           3.7447 |          11.1768 |
[32m[20230113 20:19:55 @agent_ppo2.py:186][0m |          -0.0017 |           3.5703 |          11.1883 |
[32m[20230113 20:19:55 @agent_ppo2.py:186][0m |          -0.0162 |           3.3464 |          11.1764 |
[32m[20230113 20:19:55 @agent_ppo2.py:186][0m |          -0.0110 |           3.2457 |          11.1837 |
[32m[20230113 20:19:55 @agent_ppo2.py:186][0m |          -0.0159 |           3.0863 |          11.1901 |
[32m[20230113 20:19:55 @agent_ppo2.py:186][0m |          -0.0081 |           3.0320 |          11.1869 |
[32m[20230113 20:19:55 @agent_ppo2.py:186][0m |          -0.0191 |           2.9384 |          11.1859 |
[32m[20230113 20:19:55 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:19:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 244.35
[32m[20230113 20:19:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 246.18
[32m[20230113 20:19:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.80
[32m[20230113 20:19:55 @agent_ppo2.py:144][0m Total time:      35.38 min
[32m[20230113 20:19:55 @agent_ppo2.py:146][0m 3287040 total steps have happened
[32m[20230113 20:19:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1605 --------------------------#
[32m[20230113 20:19:56 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:19:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:56 @agent_ppo2.py:186][0m |          -0.0011 |          13.7017 |          11.2516 |
[32m[20230113 20:19:56 @agent_ppo2.py:186][0m |          -0.0076 |           7.5211 |          11.2358 |
[32m[20230113 20:19:56 @agent_ppo2.py:186][0m |          -0.0093 |           6.3349 |          11.2177 |
[32m[20230113 20:19:56 @agent_ppo2.py:186][0m |          -0.0129 |           5.6082 |          11.2317 |
[32m[20230113 20:19:56 @agent_ppo2.py:186][0m |          -0.0140 |           5.1780 |          11.2293 |
[32m[20230113 20:19:56 @agent_ppo2.py:186][0m |          -0.0150 |           4.9012 |          11.2211 |
[32m[20230113 20:19:56 @agent_ppo2.py:186][0m |          -0.0170 |           4.7081 |          11.2244 |
[32m[20230113 20:19:56 @agent_ppo2.py:186][0m |          -0.0163 |           4.4228 |          11.2207 |
[32m[20230113 20:19:56 @agent_ppo2.py:186][0m |          -0.0174 |           4.3829 |          11.2227 |
[32m[20230113 20:19:56 @agent_ppo2.py:186][0m |          -0.0166 |           4.2466 |          11.2202 |
[32m[20230113 20:19:56 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:19:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 154.55
[32m[20230113 20:19:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.21
[32m[20230113 20:19:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.54
[32m[20230113 20:19:57 @agent_ppo2.py:144][0m Total time:      35.40 min
[32m[20230113 20:19:57 @agent_ppo2.py:146][0m 3289088 total steps have happened
[32m[20230113 20:19:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1606 --------------------------#
[32m[20230113 20:19:57 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:19:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:57 @agent_ppo2.py:186][0m |           0.0036 |          10.8782 |          11.0830 |
[32m[20230113 20:19:57 @agent_ppo2.py:186][0m |          -0.0040 |           7.5758 |          11.0735 |
[32m[20230113 20:19:57 @agent_ppo2.py:186][0m |          -0.0197 |           6.2993 |          11.0743 |
[32m[20230113 20:19:57 @agent_ppo2.py:186][0m |          -0.0179 |           5.7849 |          11.0716 |
[32m[20230113 20:19:57 @agent_ppo2.py:186][0m |          -0.0194 |           5.4426 |          11.0699 |
[32m[20230113 20:19:57 @agent_ppo2.py:186][0m |          -0.0106 |           5.0517 |          11.0768 |
[32m[20230113 20:19:57 @agent_ppo2.py:186][0m |          -0.0170 |           4.7982 |          11.0695 |
[32m[20230113 20:19:57 @agent_ppo2.py:186][0m |          -0.0128 |           4.7095 |          11.0730 |
[32m[20230113 20:19:57 @agent_ppo2.py:186][0m |          -0.0212 |           4.4279 |          11.0712 |
[32m[20230113 20:19:57 @agent_ppo2.py:186][0m |          -0.0156 |           4.3162 |          11.0829 |
[32m[20230113 20:19:57 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:19:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.52
[32m[20230113 20:19:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.79
[32m[20230113 20:19:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 147.31
[32m[20230113 20:19:58 @agent_ppo2.py:144][0m Total time:      35.42 min
[32m[20230113 20:19:58 @agent_ppo2.py:146][0m 3291136 total steps have happened
[32m[20230113 20:19:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1607 --------------------------#
[32m[20230113 20:19:58 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:19:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:58 @agent_ppo2.py:186][0m |           0.0031 |           7.8561 |          11.1662 |
[32m[20230113 20:19:58 @agent_ppo2.py:186][0m |          -0.0080 |           5.3352 |          11.1490 |
[32m[20230113 20:19:58 @agent_ppo2.py:186][0m |          -0.0090 |           4.6234 |          11.1583 |
[32m[20230113 20:19:58 @agent_ppo2.py:186][0m |          -0.0117 |           4.2486 |          11.1460 |
[32m[20230113 20:19:58 @agent_ppo2.py:186][0m |          -0.0121 |           3.9837 |          11.1585 |
[32m[20230113 20:19:58 @agent_ppo2.py:186][0m |          -0.0120 |           3.8001 |          11.1469 |
[32m[20230113 20:19:58 @agent_ppo2.py:186][0m |          -0.0159 |           3.6200 |          11.1439 |
[32m[20230113 20:19:58 @agent_ppo2.py:186][0m |          -0.0209 |           3.4387 |          11.1447 |
[32m[20230113 20:19:59 @agent_ppo2.py:186][0m |          -0.0238 |           3.3113 |          11.1502 |
[32m[20230113 20:19:59 @agent_ppo2.py:186][0m |          -0.0227 |           3.2024 |          11.1478 |
[32m[20230113 20:19:59 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:19:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.90
[32m[20230113 20:19:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.70
[32m[20230113 20:19:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.82
[32m[20230113 20:19:59 @agent_ppo2.py:144][0m Total time:      35.44 min
[32m[20230113 20:19:59 @agent_ppo2.py:146][0m 3293184 total steps have happened
[32m[20230113 20:19:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1608 --------------------------#
[32m[20230113 20:19:59 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:19:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:19:59 @agent_ppo2.py:186][0m |           0.0005 |           7.1660 |          11.3193 |
[32m[20230113 20:20:00 @agent_ppo2.py:186][0m |          -0.0087 |           5.9909 |          11.3075 |
[32m[20230113 20:20:00 @agent_ppo2.py:186][0m |          -0.0094 |           5.5838 |          11.3080 |
[32m[20230113 20:20:00 @agent_ppo2.py:186][0m |          -0.0140 |           5.3157 |          11.3041 |
[32m[20230113 20:20:00 @agent_ppo2.py:186][0m |          -0.0156 |           4.9758 |          11.2991 |
[32m[20230113 20:20:00 @agent_ppo2.py:186][0m |          -0.0143 |           4.8079 |          11.2941 |
[32m[20230113 20:20:00 @agent_ppo2.py:186][0m |          -0.0165 |           4.6276 |          11.2955 |
[32m[20230113 20:20:00 @agent_ppo2.py:186][0m |          -0.0174 |           4.5629 |          11.3055 |
[32m[20230113 20:20:00 @agent_ppo2.py:186][0m |          -0.0185 |           4.4526 |          11.2991 |
[32m[20230113 20:20:00 @agent_ppo2.py:186][0m |          -0.0170 |           4.3111 |          11.2988 |
[32m[20230113 20:20:00 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:20:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.14
[32m[20230113 20:20:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.50
[32m[20230113 20:20:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.57
[32m[20230113 20:20:00 @agent_ppo2.py:144][0m Total time:      35.46 min
[32m[20230113 20:20:00 @agent_ppo2.py:146][0m 3295232 total steps have happened
[32m[20230113 20:20:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1609 --------------------------#
[32m[20230113 20:20:01 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:20:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:01 @agent_ppo2.py:186][0m |           0.0004 |          22.5859 |          11.6335 |
[32m[20230113 20:20:01 @agent_ppo2.py:186][0m |          -0.0107 |           7.6169 |          11.6190 |
[32m[20230113 20:20:01 @agent_ppo2.py:186][0m |          -0.0113 |           5.7977 |          11.6247 |
[32m[20230113 20:20:01 @agent_ppo2.py:186][0m |          -0.0097 |           5.0619 |          11.6093 |
[32m[20230113 20:20:01 @agent_ppo2.py:186][0m |          -0.0121 |           4.5765 |          11.6179 |
[32m[20230113 20:20:01 @agent_ppo2.py:186][0m |          -0.0125 |           4.2931 |          11.6243 |
[32m[20230113 20:20:01 @agent_ppo2.py:186][0m |          -0.0125 |           3.9668 |          11.6123 |
[32m[20230113 20:20:01 @agent_ppo2.py:186][0m |          -0.0139 |           3.7777 |          11.6048 |
[32m[20230113 20:20:01 @agent_ppo2.py:186][0m |          -0.0164 |           3.5903 |          11.6113 |
[32m[20230113 20:20:01 @agent_ppo2.py:186][0m |          -0.0145 |           3.4515 |          11.6110 |
[32m[20230113 20:20:01 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:20:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.82
[32m[20230113 20:20:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.23
[32m[20230113 20:20:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.12
[32m[20230113 20:20:01 @agent_ppo2.py:144][0m Total time:      35.48 min
[32m[20230113 20:20:01 @agent_ppo2.py:146][0m 3297280 total steps have happened
[32m[20230113 20:20:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1610 --------------------------#
[32m[20230113 20:20:02 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:02 @agent_ppo2.py:186][0m |           0.0006 |           8.0466 |          11.3237 |
[32m[20230113 20:20:02 @agent_ppo2.py:186][0m |          -0.0089 |           5.6208 |          11.3075 |
[32m[20230113 20:20:02 @agent_ppo2.py:186][0m |          -0.0134 |           4.8248 |          11.2891 |
[32m[20230113 20:20:02 @agent_ppo2.py:186][0m |          -0.0147 |           4.3153 |          11.2913 |
[32m[20230113 20:20:02 @agent_ppo2.py:186][0m |          -0.0178 |           4.0614 |          11.2833 |
[32m[20230113 20:20:02 @agent_ppo2.py:186][0m |          -0.0169 |           3.8302 |          11.2911 |
[32m[20230113 20:20:02 @agent_ppo2.py:186][0m |          -0.0190 |           3.6564 |          11.2833 |
[32m[20230113 20:20:02 @agent_ppo2.py:186][0m |          -0.0191 |           3.5121 |          11.2771 |
[32m[20230113 20:20:02 @agent_ppo2.py:186][0m |          -0.0198 |           3.4073 |          11.2844 |
[32m[20230113 20:20:02 @agent_ppo2.py:186][0m |          -0.0207 |           3.2909 |          11.2797 |
[32m[20230113 20:20:02 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:20:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.54
[32m[20230113 20:20:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.20
[32m[20230113 20:20:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 155.93
[32m[20230113 20:20:03 @agent_ppo2.py:144][0m Total time:      35.50 min
[32m[20230113 20:20:03 @agent_ppo2.py:146][0m 3299328 total steps have happened
[32m[20230113 20:20:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1611 --------------------------#
[32m[20230113 20:20:03 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:20:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:03 @agent_ppo2.py:186][0m |           0.0003 |           7.3229 |          11.4527 |
[32m[20230113 20:20:03 @agent_ppo2.py:186][0m |          -0.0070 |           5.3682 |          11.4370 |
[32m[20230113 20:20:03 @agent_ppo2.py:186][0m |          -0.0067 |           4.7632 |          11.4228 |
[32m[20230113 20:20:03 @agent_ppo2.py:186][0m |          -0.0107 |           4.4323 |          11.4262 |
[32m[20230113 20:20:03 @agent_ppo2.py:186][0m |          -0.0106 |           4.1680 |          11.4114 |
[32m[20230113 20:20:04 @agent_ppo2.py:186][0m |          -0.0123 |           4.0307 |          11.3942 |
[32m[20230113 20:20:04 @agent_ppo2.py:186][0m |          -0.0116 |           3.8440 |          11.3952 |
[32m[20230113 20:20:04 @agent_ppo2.py:186][0m |          -0.0160 |           3.7379 |          11.3836 |
[32m[20230113 20:20:04 @agent_ppo2.py:186][0m |          -0.0149 |           3.6675 |          11.3926 |
[32m[20230113 20:20:04 @agent_ppo2.py:186][0m |          -0.0144 |           3.5655 |          11.3836 |
[32m[20230113 20:20:04 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.87
[32m[20230113 20:20:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.51
[32m[20230113 20:20:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.26
[32m[20230113 20:20:04 @agent_ppo2.py:144][0m Total time:      35.53 min
[32m[20230113 20:20:04 @agent_ppo2.py:146][0m 3301376 total steps have happened
[32m[20230113 20:20:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1612 --------------------------#
[32m[20230113 20:20:05 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:05 @agent_ppo2.py:186][0m |           0.0059 |           6.6494 |          11.1016 |
[32m[20230113 20:20:05 @agent_ppo2.py:186][0m |          -0.0054 |           5.2463 |          11.0943 |
[32m[20230113 20:20:05 @agent_ppo2.py:186][0m |           0.0012 |           4.8595 |          11.1103 |
[32m[20230113 20:20:05 @agent_ppo2.py:186][0m |          -0.0167 |           4.5482 |          11.0980 |
[32m[20230113 20:20:05 @agent_ppo2.py:186][0m |          -0.0097 |           4.3930 |          11.0970 |
[32m[20230113 20:20:05 @agent_ppo2.py:186][0m |          -0.0153 |           4.2430 |          11.0920 |
[32m[20230113 20:20:05 @agent_ppo2.py:186][0m |           0.0120 |           4.7572 |          11.0974 |
[32m[20230113 20:20:05 @agent_ppo2.py:186][0m |          -0.0054 |           4.0822 |          11.0806 |
[32m[20230113 20:20:05 @agent_ppo2.py:186][0m |          -0.0145 |           3.9125 |          11.0898 |
[32m[20230113 20:20:05 @agent_ppo2.py:186][0m |          -0.0017 |           4.8314 |          11.0780 |
[32m[20230113 20:20:05 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:20:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.03
[32m[20230113 20:20:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.80
[32m[20230113 20:20:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 20.39
[32m[20230113 20:20:05 @agent_ppo2.py:144][0m Total time:      35.55 min
[32m[20230113 20:20:05 @agent_ppo2.py:146][0m 3303424 total steps have happened
[32m[20230113 20:20:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1613 --------------------------#
[32m[20230113 20:20:06 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:20:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:06 @agent_ppo2.py:186][0m |          -0.0064 |           5.5840 |          11.2682 |
[32m[20230113 20:20:06 @agent_ppo2.py:186][0m |          -0.0040 |           4.7156 |          11.2761 |
[32m[20230113 20:20:06 @agent_ppo2.py:186][0m |          -0.0067 |           4.3408 |          11.2559 |
[32m[20230113 20:20:06 @agent_ppo2.py:186][0m |          -0.0107 |           4.0836 |          11.2592 |
[32m[20230113 20:20:06 @agent_ppo2.py:186][0m |          -0.0109 |           3.9613 |          11.2681 |
[32m[20230113 20:20:06 @agent_ppo2.py:186][0m |          -0.0123 |           3.8375 |          11.2545 |
[32m[20230113 20:20:06 @agent_ppo2.py:186][0m |          -0.0115 |           3.6545 |          11.2606 |
[32m[20230113 20:20:06 @agent_ppo2.py:186][0m |          -0.0137 |           3.5184 |          11.2608 |
[32m[20230113 20:20:06 @agent_ppo2.py:186][0m |          -0.0134 |           3.4022 |          11.2599 |
[32m[20230113 20:20:06 @agent_ppo2.py:186][0m |          -0.0161 |           3.3325 |          11.2676 |
[32m[20230113 20:20:06 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.78
[32m[20230113 20:20:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.79
[32m[20230113 20:20:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.67
[32m[20230113 20:20:07 @agent_ppo2.py:144][0m Total time:      35.57 min
[32m[20230113 20:20:07 @agent_ppo2.py:146][0m 3305472 total steps have happened
[32m[20230113 20:20:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1614 --------------------------#
[32m[20230113 20:20:07 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:20:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:07 @agent_ppo2.py:186][0m |           0.0002 |           5.0215 |          11.2645 |
[32m[20230113 20:20:07 @agent_ppo2.py:186][0m |          -0.0061 |           3.7544 |          11.2456 |
[32m[20230113 20:20:07 @agent_ppo2.py:186][0m |          -0.0084 |           3.4327 |          11.2386 |
[32m[20230113 20:20:07 @agent_ppo2.py:186][0m |          -0.0112 |           3.2439 |          11.2437 |
[32m[20230113 20:20:07 @agent_ppo2.py:186][0m |          -0.0108 |           3.1090 |          11.2376 |
[32m[20230113 20:20:07 @agent_ppo2.py:186][0m |          -0.0121 |           3.0074 |          11.2373 |
[32m[20230113 20:20:07 @agent_ppo2.py:186][0m |          -0.0118 |           2.9354 |          11.2391 |
[32m[20230113 20:20:07 @agent_ppo2.py:186][0m |          -0.0143 |           2.8400 |          11.2352 |
[32m[20230113 20:20:07 @agent_ppo2.py:186][0m |          -0.0138 |           2.7994 |          11.2375 |
[32m[20230113 20:20:07 @agent_ppo2.py:186][0m |          -0.0133 |           2.7412 |          11.2304 |
[32m[20230113 20:20:07 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.78
[32m[20230113 20:20:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.53
[32m[20230113 20:20:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 152.01
[32m[20230113 20:20:08 @agent_ppo2.py:144][0m Total time:      35.59 min
[32m[20230113 20:20:08 @agent_ppo2.py:146][0m 3307520 total steps have happened
[32m[20230113 20:20:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1615 --------------------------#
[32m[20230113 20:20:08 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:20:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:08 @agent_ppo2.py:186][0m |           0.0000 |          13.8993 |          11.3306 |
[32m[20230113 20:20:08 @agent_ppo2.py:186][0m |          -0.0065 |           7.3715 |          11.3048 |
[32m[20230113 20:20:08 @agent_ppo2.py:186][0m |          -0.0095 |           6.1435 |          11.3117 |
[32m[20230113 20:20:09 @agent_ppo2.py:186][0m |          -0.0119 |           5.5581 |          11.3103 |
[32m[20230113 20:20:09 @agent_ppo2.py:186][0m |          -0.0136 |           5.2208 |          11.3118 |
[32m[20230113 20:20:09 @agent_ppo2.py:186][0m |          -0.0153 |           4.9958 |          11.3026 |
[32m[20230113 20:20:09 @agent_ppo2.py:186][0m |          -0.0166 |           4.7802 |          11.3019 |
[32m[20230113 20:20:09 @agent_ppo2.py:186][0m |          -0.0174 |           4.6685 |          11.3036 |
[32m[20230113 20:20:09 @agent_ppo2.py:186][0m |          -0.0181 |           4.5180 |          11.3036 |
[32m[20230113 20:20:09 @agent_ppo2.py:186][0m |          -0.0179 |           4.4073 |          11.3025 |
[32m[20230113 20:20:09 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:20:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 136.82
[32m[20230113 20:20:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.41
[32m[20230113 20:20:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 235.89
[32m[20230113 20:20:09 @agent_ppo2.py:144][0m Total time:      35.61 min
[32m[20230113 20:20:09 @agent_ppo2.py:146][0m 3309568 total steps have happened
[32m[20230113 20:20:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1616 --------------------------#
[32m[20230113 20:20:10 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:20:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:10 @agent_ppo2.py:186][0m |           0.0028 |           7.1321 |          11.3035 |
[32m[20230113 20:20:10 @agent_ppo2.py:186][0m |          -0.0044 |           5.4939 |          11.2861 |
[32m[20230113 20:20:10 @agent_ppo2.py:186][0m |          -0.0109 |           4.8845 |          11.2772 |
[32m[20230113 20:20:10 @agent_ppo2.py:186][0m |          -0.0100 |           4.4378 |          11.2747 |
[32m[20230113 20:20:10 @agent_ppo2.py:186][0m |          -0.0126 |           4.1695 |          11.2669 |
[32m[20230113 20:20:10 @agent_ppo2.py:186][0m |          -0.0144 |           3.9702 |          11.2714 |
[32m[20230113 20:20:10 @agent_ppo2.py:186][0m |          -0.0155 |           3.8145 |          11.2689 |
[32m[20230113 20:20:10 @agent_ppo2.py:186][0m |          -0.0157 |           3.6593 |          11.2655 |
[32m[20230113 20:20:10 @agent_ppo2.py:186][0m |          -0.0170 |           3.5057 |          11.2564 |
[32m[20230113 20:20:10 @agent_ppo2.py:186][0m |          -0.0176 |           3.4893 |          11.2660 |
[32m[20230113 20:20:10 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:20:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.35
[32m[20230113 20:20:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.11
[32m[20230113 20:20:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.67
[32m[20230113 20:20:10 @agent_ppo2.py:144][0m Total time:      35.63 min
[32m[20230113 20:20:10 @agent_ppo2.py:146][0m 3311616 total steps have happened
[32m[20230113 20:20:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1617 --------------------------#
[32m[20230113 20:20:11 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:20:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:11 @agent_ppo2.py:186][0m |           0.0163 |          17.8133 |          11.3222 |
[32m[20230113 20:20:11 @agent_ppo2.py:186][0m |          -0.0001 |           9.9936 |          11.3057 |
[32m[20230113 20:20:11 @agent_ppo2.py:186][0m |          -0.0132 |           7.1744 |          11.3047 |
[32m[20230113 20:20:11 @agent_ppo2.py:186][0m |          -0.0144 |           6.3493 |          11.3041 |
[32m[20230113 20:20:11 @agent_ppo2.py:186][0m |          -0.0157 |           5.7428 |          11.2953 |
[32m[20230113 20:20:11 @agent_ppo2.py:186][0m |          -0.0127 |           5.2958 |          11.2913 |
[32m[20230113 20:20:11 @agent_ppo2.py:186][0m |          -0.0128 |           4.9642 |          11.2971 |
[32m[20230113 20:20:11 @agent_ppo2.py:186][0m |          -0.0173 |           4.7590 |          11.2919 |
[32m[20230113 20:20:11 @agent_ppo2.py:186][0m |          -0.0208 |           4.5125 |          11.2884 |
[32m[20230113 20:20:11 @agent_ppo2.py:186][0m |          -0.0150 |           4.3771 |          11.2902 |
[32m[20230113 20:20:11 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:20:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 120.98
[32m[20230113 20:20:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.40
[32m[20230113 20:20:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.56
[32m[20230113 20:20:12 @agent_ppo2.py:144][0m Total time:      35.65 min
[32m[20230113 20:20:12 @agent_ppo2.py:146][0m 3313664 total steps have happened
[32m[20230113 20:20:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1618 --------------------------#
[32m[20230113 20:20:12 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:20:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:12 @agent_ppo2.py:186][0m |           0.0003 |           7.1101 |          11.3839 |
[32m[20230113 20:20:12 @agent_ppo2.py:186][0m |          -0.0069 |           5.7068 |          11.3827 |
[32m[20230113 20:20:12 @agent_ppo2.py:186][0m |          -0.0113 |           5.1254 |          11.3668 |
[32m[20230113 20:20:12 @agent_ppo2.py:186][0m |          -0.0092 |           4.6557 |          11.3718 |
[32m[20230113 20:20:12 @agent_ppo2.py:186][0m |          -0.0140 |           4.3942 |          11.3662 |
[32m[20230113 20:20:12 @agent_ppo2.py:186][0m |          -0.0106 |           4.1833 |          11.3579 |
[32m[20230113 20:20:13 @agent_ppo2.py:186][0m |          -0.0161 |           4.0240 |          11.3682 |
[32m[20230113 20:20:13 @agent_ppo2.py:186][0m |          -0.0169 |           3.8496 |          11.3576 |
[32m[20230113 20:20:13 @agent_ppo2.py:186][0m |          -0.0142 |           3.7241 |          11.3630 |
[32m[20230113 20:20:13 @agent_ppo2.py:186][0m |          -0.0167 |           3.6109 |          11.3570 |
[32m[20230113 20:20:13 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.92
[32m[20230113 20:20:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.10
[32m[20230113 20:20:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 123.38
[32m[20230113 20:20:13 @agent_ppo2.py:144][0m Total time:      35.67 min
[32m[20230113 20:20:13 @agent_ppo2.py:146][0m 3315712 total steps have happened
[32m[20230113 20:20:13 @agent_ppo2.py:122][0m #------------------------ Iteration 1619 --------------------------#
[32m[20230113 20:20:13 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:13 @agent_ppo2.py:186][0m |           0.0052 |          19.8857 |          11.2084 |
[32m[20230113 20:20:13 @agent_ppo2.py:186][0m |          -0.0085 |          11.2594 |          11.1950 |
[32m[20230113 20:20:14 @agent_ppo2.py:186][0m |           0.0069 |           9.3943 |          11.2034 |
[32m[20230113 20:20:14 @agent_ppo2.py:186][0m |          -0.0168 |           8.0632 |          11.1989 |
[32m[20230113 20:20:14 @agent_ppo2.py:186][0m |          -0.0155 |           7.1655 |          11.1875 |
[32m[20230113 20:20:14 @agent_ppo2.py:186][0m |          -0.0104 |           6.7618 |          11.1924 |
[32m[20230113 20:20:14 @agent_ppo2.py:186][0m |          -0.0080 |           6.2912 |          11.1886 |
[32m[20230113 20:20:14 @agent_ppo2.py:186][0m |          -0.0083 |           6.0055 |          11.1994 |
[32m[20230113 20:20:14 @agent_ppo2.py:186][0m |          -0.0161 |           5.6353 |          11.1821 |
[32m[20230113 20:20:14 @agent_ppo2.py:186][0m |          -0.0250 |           5.5199 |          11.1867 |
[32m[20230113 20:20:14 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:20:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 177.85
[32m[20230113 20:20:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.92
[32m[20230113 20:20:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.39
[32m[20230113 20:20:14 @agent_ppo2.py:144][0m Total time:      35.69 min
[32m[20230113 20:20:14 @agent_ppo2.py:146][0m 3317760 total steps have happened
[32m[20230113 20:20:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1620 --------------------------#
[32m[20230113 20:20:15 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:15 @agent_ppo2.py:186][0m |           0.0021 |           7.7969 |          11.1637 |
[32m[20230113 20:20:15 @agent_ppo2.py:186][0m |          -0.0034 |           5.6985 |          11.1534 |
[32m[20230113 20:20:15 @agent_ppo2.py:186][0m |          -0.0079 |           5.0878 |          11.1411 |
[32m[20230113 20:20:15 @agent_ppo2.py:186][0m |          -0.0094 |           4.8005 |          11.1513 |
[32m[20230113 20:20:15 @agent_ppo2.py:186][0m |          -0.0114 |           4.4282 |          11.1519 |
[32m[20230113 20:20:15 @agent_ppo2.py:186][0m |          -0.0106 |           4.2119 |          11.1488 |
[32m[20230113 20:20:15 @agent_ppo2.py:186][0m |          -0.0116 |           4.2053 |          11.1460 |
[32m[20230113 20:20:15 @agent_ppo2.py:186][0m |          -0.0159 |           3.9842 |          11.1564 |
[32m[20230113 20:20:15 @agent_ppo2.py:186][0m |          -0.0106 |           3.9568 |          11.1414 |
[32m[20230113 20:20:15 @agent_ppo2.py:186][0m |          -0.0121 |           3.7533 |          11.1448 |
[32m[20230113 20:20:15 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:20:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.56
[32m[20230113 20:20:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.86
[32m[20230113 20:20:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 177.11
[32m[20230113 20:20:15 @agent_ppo2.py:144][0m Total time:      35.72 min
[32m[20230113 20:20:15 @agent_ppo2.py:146][0m 3319808 total steps have happened
[32m[20230113 20:20:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1621 --------------------------#
[32m[20230113 20:20:16 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:20:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:16 @agent_ppo2.py:186][0m |          -0.0041 |           6.6137 |          11.1987 |
[32m[20230113 20:20:16 @agent_ppo2.py:186][0m |          -0.0039 |           4.9440 |          11.1749 |
[32m[20230113 20:20:16 @agent_ppo2.py:186][0m |           0.0055 |           4.3903 |          11.1636 |
[32m[20230113 20:20:16 @agent_ppo2.py:186][0m |          -0.0079 |           3.9745 |          11.1428 |
[32m[20230113 20:20:16 @agent_ppo2.py:186][0m |          -0.0218 |           3.8157 |          11.1695 |
[32m[20230113 20:20:16 @agent_ppo2.py:186][0m |          -0.0071 |           3.5216 |          11.1540 |
[32m[20230113 20:20:16 @agent_ppo2.py:186][0m |          -0.0099 |           3.3857 |          11.1495 |
[32m[20230113 20:20:16 @agent_ppo2.py:186][0m |          -0.0189 |           3.2848 |          11.1501 |
[32m[20230113 20:20:16 @agent_ppo2.py:186][0m |          -0.0077 |           3.1842 |          11.1455 |
[32m[20230113 20:20:16 @agent_ppo2.py:186][0m |          -0.0227 |           3.0566 |          11.1446 |
[32m[20230113 20:20:16 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.65
[32m[20230113 20:20:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.98
[32m[20230113 20:20:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 88.90
[32m[20230113 20:20:17 @agent_ppo2.py:144][0m Total time:      35.74 min
[32m[20230113 20:20:17 @agent_ppo2.py:146][0m 3321856 total steps have happened
[32m[20230113 20:20:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1622 --------------------------#
[32m[20230113 20:20:17 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:17 @agent_ppo2.py:186][0m |          -0.0083 |           6.6320 |          11.2447 |
[32m[20230113 20:20:17 @agent_ppo2.py:186][0m |          -0.0164 |           4.6902 |          11.2400 |
[32m[20230113 20:20:17 @agent_ppo2.py:186][0m |          -0.0063 |           3.9932 |          11.2381 |
[32m[20230113 20:20:17 @agent_ppo2.py:186][0m |          -0.0103 |           3.6251 |          11.2453 |
[32m[20230113 20:20:17 @agent_ppo2.py:186][0m |          -0.0137 |           3.4087 |          11.2369 |
[32m[20230113 20:20:17 @agent_ppo2.py:186][0m |          -0.0218 |           3.1935 |          11.2375 |
[32m[20230113 20:20:18 @agent_ppo2.py:186][0m |          -0.0136 |           3.0814 |          11.2374 |
[32m[20230113 20:20:18 @agent_ppo2.py:186][0m |           0.0022 |           3.2013 |          11.2388 |
[32m[20230113 20:20:18 @agent_ppo2.py:186][0m |          -0.0144 |           2.9572 |          11.2368 |
[32m[20230113 20:20:18 @agent_ppo2.py:186][0m |          -0.0147 |           2.7641 |          11.2302 |
[32m[20230113 20:20:18 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:20:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.30
[32m[20230113 20:20:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.35
[32m[20230113 20:20:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.18
[32m[20230113 20:20:18 @agent_ppo2.py:144][0m Total time:      35.76 min
[32m[20230113 20:20:18 @agent_ppo2.py:146][0m 3323904 total steps have happened
[32m[20230113 20:20:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1623 --------------------------#
[32m[20230113 20:20:18 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:20:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:19 @agent_ppo2.py:186][0m |           0.0001 |           7.4867 |          11.2672 |
[32m[20230113 20:20:19 @agent_ppo2.py:186][0m |          -0.0053 |           6.0114 |          11.2570 |
[32m[20230113 20:20:19 @agent_ppo2.py:186][0m |          -0.0087 |           5.4862 |          11.2621 |
[32m[20230113 20:20:19 @agent_ppo2.py:186][0m |          -0.0101 |           5.1086 |          11.2480 |
[32m[20230113 20:20:19 @agent_ppo2.py:186][0m |          -0.0103 |           4.8257 |          11.2586 |
[32m[20230113 20:20:19 @agent_ppo2.py:186][0m |          -0.0135 |           4.5374 |          11.2496 |
[32m[20230113 20:20:19 @agent_ppo2.py:186][0m |          -0.0131 |           4.2543 |          11.2461 |
[32m[20230113 20:20:19 @agent_ppo2.py:186][0m |          -0.0143 |           4.0914 |          11.2451 |
[32m[20230113 20:20:19 @agent_ppo2.py:186][0m |          -0.0141 |           3.9585 |          11.2438 |
[32m[20230113 20:20:19 @agent_ppo2.py:186][0m |          -0.0165 |           3.7889 |          11.2403 |
[32m[20230113 20:20:19 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.93
[32m[20230113 20:20:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.15
[32m[20230113 20:20:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 150.08
[32m[20230113 20:20:19 @agent_ppo2.py:144][0m Total time:      35.78 min
[32m[20230113 20:20:19 @agent_ppo2.py:146][0m 3325952 total steps have happened
[32m[20230113 20:20:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1624 --------------------------#
[32m[20230113 20:20:20 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:20:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:20 @agent_ppo2.py:186][0m |           0.0019 |           6.4532 |          11.3530 |
[32m[20230113 20:20:20 @agent_ppo2.py:186][0m |          -0.0058 |           4.8053 |          11.3456 |
[32m[20230113 20:20:20 @agent_ppo2.py:186][0m |          -0.0095 |           4.2386 |          11.3406 |
[32m[20230113 20:20:20 @agent_ppo2.py:186][0m |          -0.0104 |           3.9461 |          11.3372 |
[32m[20230113 20:20:20 @agent_ppo2.py:186][0m |          -0.0132 |           3.7161 |          11.3275 |
[32m[20230113 20:20:20 @agent_ppo2.py:186][0m |          -0.0131 |           3.5521 |          11.3309 |
[32m[20230113 20:20:20 @agent_ppo2.py:186][0m |          -0.0143 |           3.4641 |          11.3257 |
[32m[20230113 20:20:20 @agent_ppo2.py:186][0m |          -0.0158 |           3.3125 |          11.3300 |
[32m[20230113 20:20:20 @agent_ppo2.py:186][0m |          -0.0159 |           3.2597 |          11.3302 |
[32m[20230113 20:20:20 @agent_ppo2.py:186][0m |          -0.0167 |           3.1689 |          11.3268 |
[32m[20230113 20:20:20 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.30
[32m[20230113 20:20:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.16
[32m[20230113 20:20:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 114.60
[32m[20230113 20:20:20 @agent_ppo2.py:144][0m Total time:      35.80 min
[32m[20230113 20:20:20 @agent_ppo2.py:146][0m 3328000 total steps have happened
[32m[20230113 20:20:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1625 --------------------------#
[32m[20230113 20:20:21 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:21 @agent_ppo2.py:186][0m |           0.0015 |           7.0139 |          11.2693 |
[32m[20230113 20:20:21 @agent_ppo2.py:186][0m |          -0.0041 |           5.7453 |          11.2694 |
[32m[20230113 20:20:21 @agent_ppo2.py:186][0m |          -0.0064 |           4.9542 |          11.2541 |
[32m[20230113 20:20:21 @agent_ppo2.py:186][0m |          -0.0080 |           4.5352 |          11.2546 |
[32m[20230113 20:20:21 @agent_ppo2.py:186][0m |          -0.0094 |           4.2164 |          11.2489 |
[32m[20230113 20:20:21 @agent_ppo2.py:186][0m |          -0.0149 |           3.9902 |          11.2462 |
[32m[20230113 20:20:21 @agent_ppo2.py:186][0m |          -0.0121 |           3.7833 |          11.2402 |
[32m[20230113 20:20:21 @agent_ppo2.py:186][0m |          -0.0121 |           3.6582 |          11.2510 |
[32m[20230113 20:20:21 @agent_ppo2.py:186][0m |          -0.0167 |           3.5199 |          11.2489 |
[32m[20230113 20:20:21 @agent_ppo2.py:186][0m |          -0.0164 |           3.3483 |          11.2381 |
[32m[20230113 20:20:21 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:20:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.28
[32m[20230113 20:20:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.10
[32m[20230113 20:20:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.92
[32m[20230113 20:20:22 @agent_ppo2.py:144][0m Total time:      35.82 min
[32m[20230113 20:20:22 @agent_ppo2.py:146][0m 3330048 total steps have happened
[32m[20230113 20:20:22 @agent_ppo2.py:122][0m #------------------------ Iteration 1626 --------------------------#
[32m[20230113 20:20:22 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:22 @agent_ppo2.py:186][0m |          -0.0010 |           6.5926 |          11.2467 |
[32m[20230113 20:20:22 @agent_ppo2.py:186][0m |          -0.0031 |           5.1965 |          11.2235 |
[32m[20230113 20:20:22 @agent_ppo2.py:186][0m |          -0.0066 |           4.7207 |          11.2216 |
[32m[20230113 20:20:22 @agent_ppo2.py:186][0m |          -0.0091 |           4.4407 |          11.2113 |
[32m[20230113 20:20:22 @agent_ppo2.py:186][0m |          -0.0116 |           4.2661 |          11.2024 |
[32m[20230113 20:20:22 @agent_ppo2.py:186][0m |          -0.0133 |           4.0628 |          11.2094 |
[32m[20230113 20:20:23 @agent_ppo2.py:186][0m |          -0.0137 |           3.9088 |          11.1991 |
[32m[20230113 20:20:23 @agent_ppo2.py:186][0m |          -0.0130 |           3.7456 |          11.1926 |
[32m[20230113 20:20:23 @agent_ppo2.py:186][0m |          -0.0165 |           3.6728 |          11.1967 |
[32m[20230113 20:20:23 @agent_ppo2.py:186][0m |          -0.0160 |           3.5637 |          11.1962 |
[32m[20230113 20:20:23 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:20:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.93
[32m[20230113 20:20:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.76
[32m[20230113 20:20:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.61
[32m[20230113 20:20:23 @agent_ppo2.py:144][0m Total time:      35.84 min
[32m[20230113 20:20:23 @agent_ppo2.py:146][0m 3332096 total steps have happened
[32m[20230113 20:20:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1627 --------------------------#
[32m[20230113 20:20:23 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:24 @agent_ppo2.py:186][0m |          -0.0008 |           6.0157 |          11.1198 |
[32m[20230113 20:20:24 @agent_ppo2.py:186][0m |          -0.0043 |           5.1298 |          11.1164 |
[32m[20230113 20:20:24 @agent_ppo2.py:186][0m |          -0.0055 |           4.6867 |          11.1184 |
[32m[20230113 20:20:24 @agent_ppo2.py:186][0m |          -0.0123 |           4.4764 |          11.0990 |
[32m[20230113 20:20:24 @agent_ppo2.py:186][0m |          -0.0071 |           4.3211 |          11.1114 |
[32m[20230113 20:20:24 @agent_ppo2.py:186][0m |          -0.0134 |           4.1584 |          11.1099 |
[32m[20230113 20:20:24 @agent_ppo2.py:186][0m |          -0.0110 |           4.0175 |          11.1036 |
[32m[20230113 20:20:24 @agent_ppo2.py:186][0m |          -0.0185 |           4.0202 |          11.1145 |
[32m[20230113 20:20:24 @agent_ppo2.py:186][0m |          -0.0198 |           3.8327 |          11.1032 |
[32m[20230113 20:20:24 @agent_ppo2.py:186][0m |          -0.0151 |           3.7305 |          11.1055 |
[32m[20230113 20:20:24 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:20:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.02
[32m[20230113 20:20:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.53
[32m[20230113 20:20:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.07
[32m[20230113 20:20:24 @agent_ppo2.py:144][0m Total time:      35.86 min
[32m[20230113 20:20:24 @agent_ppo2.py:146][0m 3334144 total steps have happened
[32m[20230113 20:20:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1628 --------------------------#
[32m[20230113 20:20:25 @agent_ppo2.py:128][0m Sampling time: 0.31 s by 1 slaves
[32m[20230113 20:20:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:25 @agent_ppo2.py:186][0m |           0.0006 |          18.1903 |          11.5709 |
[32m[20230113 20:20:25 @agent_ppo2.py:186][0m |          -0.0102 |          10.1705 |          11.5450 |
[32m[20230113 20:20:25 @agent_ppo2.py:186][0m |          -0.0128 |           7.2844 |          11.5423 |
[32m[20230113 20:20:25 @agent_ppo2.py:186][0m |          -0.0162 |           6.2647 |          11.5447 |
[32m[20230113 20:20:25 @agent_ppo2.py:186][0m |          -0.0169 |           5.6560 |          11.5415 |
[32m[20230113 20:20:25 @agent_ppo2.py:186][0m |          -0.0178 |           5.2528 |          11.5420 |
[32m[20230113 20:20:25 @agent_ppo2.py:186][0m |          -0.0195 |           4.8623 |          11.5423 |
[32m[20230113 20:20:25 @agent_ppo2.py:186][0m |          -0.0205 |           4.6092 |          11.5378 |
[32m[20230113 20:20:25 @agent_ppo2.py:186][0m |          -0.0206 |           4.3843 |          11.5438 |
[32m[20230113 20:20:25 @agent_ppo2.py:186][0m |          -0.0230 |           4.2305 |          11.5467 |
[32m[20230113 20:20:25 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 20:20:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 125.84
[32m[20230113 20:20:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.52
[32m[20230113 20:20:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 138.34
[32m[20230113 20:20:25 @agent_ppo2.py:144][0m Total time:      35.88 min
[32m[20230113 20:20:25 @agent_ppo2.py:146][0m 3336192 total steps have happened
[32m[20230113 20:20:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1629 --------------------------#
[32m[20230113 20:20:26 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 20:20:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:26 @agent_ppo2.py:186][0m |          -0.0011 |          16.1478 |          11.3898 |
[32m[20230113 20:20:26 @agent_ppo2.py:186][0m |          -0.0045 |           7.2589 |          11.3688 |
[32m[20230113 20:20:26 @agent_ppo2.py:186][0m |          -0.0062 |           6.1017 |          11.3666 |
[32m[20230113 20:20:26 @agent_ppo2.py:186][0m |          -0.0055 |           5.1889 |          11.3462 |
[32m[20230113 20:20:26 @agent_ppo2.py:186][0m |          -0.0073 |           4.7634 |          11.3358 |
[32m[20230113 20:20:26 @agent_ppo2.py:186][0m |          -0.0064 |           4.5084 |          11.3716 |
[32m[20230113 20:20:26 @agent_ppo2.py:186][0m |          -0.0137 |           4.2272 |          11.3582 |
[32m[20230113 20:20:26 @agent_ppo2.py:186][0m |          -0.0097 |           4.0013 |          11.3541 |
[32m[20230113 20:20:26 @agent_ppo2.py:186][0m |          -0.0141 |           3.8174 |          11.3496 |
[32m[20230113 20:20:26 @agent_ppo2.py:186][0m |          -0.0148 |           3.7217 |          11.3594 |
[32m[20230113 20:20:26 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 20:20:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 137.18
[32m[20230113 20:20:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.64
[32m[20230113 20:20:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 151.86
[32m[20230113 20:20:26 @agent_ppo2.py:144][0m Total time:      35.90 min
[32m[20230113 20:20:26 @agent_ppo2.py:146][0m 3338240 total steps have happened
[32m[20230113 20:20:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1630 --------------------------#
[32m[20230113 20:20:27 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 20:20:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:27 @agent_ppo2.py:186][0m |          -0.0010 |          23.7664 |          11.3699 |
[32m[20230113 20:20:27 @agent_ppo2.py:186][0m |          -0.0086 |          13.4007 |          11.3608 |
[32m[20230113 20:20:27 @agent_ppo2.py:186][0m |          -0.0129 |          11.0916 |          11.3496 |
[32m[20230113 20:20:27 @agent_ppo2.py:186][0m |          -0.0179 |           9.7071 |          11.3466 |
[32m[20230113 20:20:27 @agent_ppo2.py:186][0m |          -0.0148 |           8.7987 |          11.3400 |
[32m[20230113 20:20:27 @agent_ppo2.py:186][0m |          -0.0238 |           8.3386 |          11.3356 |
[32m[20230113 20:20:27 @agent_ppo2.py:186][0m |          -0.0175 |           7.8271 |          11.3455 |
[32m[20230113 20:20:27 @agent_ppo2.py:186][0m |          -0.0214 |           7.3096 |          11.3379 |
[32m[20230113 20:20:27 @agent_ppo2.py:186][0m |          -0.0214 |           6.9123 |          11.3348 |
[32m[20230113 20:20:27 @agent_ppo2.py:186][0m |          -0.0249 |           6.8083 |          11.3489 |
[32m[20230113 20:20:27 @agent_ppo2.py:131][0m Policy update time: 0.37 s
[32m[20230113 20:20:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 139.17
[32m[20230113 20:20:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.29
[32m[20230113 20:20:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 161.93
[32m[20230113 20:20:28 @agent_ppo2.py:144][0m Total time:      35.92 min
[32m[20230113 20:20:28 @agent_ppo2.py:146][0m 3340288 total steps have happened
[32m[20230113 20:20:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1631 --------------------------#
[32m[20230113 20:20:28 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:20:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:28 @agent_ppo2.py:186][0m |           0.0012 |           7.5900 |          11.3742 |
[32m[20230113 20:20:28 @agent_ppo2.py:186][0m |          -0.0062 |           5.1766 |          11.3847 |
[32m[20230113 20:20:28 @agent_ppo2.py:186][0m |          -0.0092 |           4.5190 |          11.3791 |
[32m[20230113 20:20:28 @agent_ppo2.py:186][0m |          -0.0108 |           4.2197 |          11.3699 |
[32m[20230113 20:20:28 @agent_ppo2.py:186][0m |          -0.0121 |           3.9970 |          11.3604 |
[32m[20230113 20:20:28 @agent_ppo2.py:186][0m |          -0.0125 |           3.8263 |          11.3653 |
[32m[20230113 20:20:28 @agent_ppo2.py:186][0m |          -0.0145 |           3.6968 |          11.3702 |
[32m[20230113 20:20:28 @agent_ppo2.py:186][0m |          -0.0143 |           3.5739 |          11.3656 |
[32m[20230113 20:20:28 @agent_ppo2.py:186][0m |          -0.0152 |           3.4682 |          11.3554 |
[32m[20230113 20:20:28 @agent_ppo2.py:186][0m |          -0.0166 |           3.3935 |          11.3574 |
[32m[20230113 20:20:28 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:20:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.66
[32m[20230113 20:20:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.77
[32m[20230113 20:20:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.90
[32m[20230113 20:20:29 @agent_ppo2.py:144][0m Total time:      35.94 min
[32m[20230113 20:20:29 @agent_ppo2.py:146][0m 3342336 total steps have happened
[32m[20230113 20:20:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1632 --------------------------#
[32m[20230113 20:20:29 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:20:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:29 @agent_ppo2.py:186][0m |           0.0018 |           6.8925 |          11.2264 |
[32m[20230113 20:20:29 @agent_ppo2.py:186][0m |          -0.0146 |           5.7841 |          11.2166 |
[32m[20230113 20:20:29 @agent_ppo2.py:186][0m |          -0.0178 |           5.2760 |          11.2115 |
[32m[20230113 20:20:29 @agent_ppo2.py:186][0m |          -0.0205 |           5.0117 |          11.2195 |
[32m[20230113 20:20:29 @agent_ppo2.py:186][0m |           0.0003 |           4.9935 |          11.2058 |
[32m[20230113 20:20:29 @agent_ppo2.py:186][0m |          -0.0151 |           4.6405 |          11.1949 |
[32m[20230113 20:20:30 @agent_ppo2.py:186][0m |          -0.0128 |           4.4673 |          11.2067 |
[32m[20230113 20:20:30 @agent_ppo2.py:186][0m |          -0.0201 |           4.3302 |          11.2074 |
[32m[20230113 20:20:30 @agent_ppo2.py:186][0m |          -0.0283 |           4.3410 |          11.2093 |
[32m[20230113 20:20:30 @agent_ppo2.py:186][0m |          -0.0201 |           4.1643 |          11.2093 |
[32m[20230113 20:20:30 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.97
[32m[20230113 20:20:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.12
[32m[20230113 20:20:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 144.39
[32m[20230113 20:20:30 @agent_ppo2.py:144][0m Total time:      35.96 min
[32m[20230113 20:20:30 @agent_ppo2.py:146][0m 3344384 total steps have happened
[32m[20230113 20:20:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1633 --------------------------#
[32m[20230113 20:20:30 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:20:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:30 @agent_ppo2.py:186][0m |           0.0003 |           7.4887 |          11.4320 |
[32m[20230113 20:20:30 @agent_ppo2.py:186][0m |          -0.0052 |           5.8570 |          11.4261 |
[32m[20230113 20:20:31 @agent_ppo2.py:186][0m |          -0.0098 |           5.1771 |          11.4241 |
[32m[20230113 20:20:31 @agent_ppo2.py:186][0m |          -0.0112 |           4.7939 |          11.4180 |
[32m[20230113 20:20:31 @agent_ppo2.py:186][0m |          -0.0118 |           4.4911 |          11.4111 |
[32m[20230113 20:20:31 @agent_ppo2.py:186][0m |          -0.0129 |           4.2527 |          11.4209 |
[32m[20230113 20:20:31 @agent_ppo2.py:186][0m |          -0.0158 |           4.0312 |          11.4207 |
[32m[20230113 20:20:31 @agent_ppo2.py:186][0m |          -0.0158 |           3.8629 |          11.4126 |
[32m[20230113 20:20:31 @agent_ppo2.py:186][0m |          -0.0149 |           3.7470 |          11.4202 |
[32m[20230113 20:20:31 @agent_ppo2.py:186][0m |          -0.0160 |           3.5950 |          11.4180 |
[32m[20230113 20:20:31 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:20:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.39
[32m[20230113 20:20:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.50
[32m[20230113 20:20:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.71
[32m[20230113 20:20:31 @agent_ppo2.py:144][0m Total time:      35.98 min
[32m[20230113 20:20:31 @agent_ppo2.py:146][0m 3346432 total steps have happened
[32m[20230113 20:20:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1634 --------------------------#
[32m[20230113 20:20:32 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:32 @agent_ppo2.py:186][0m |          -0.0010 |           7.1225 |          11.4044 |
[32m[20230113 20:20:32 @agent_ppo2.py:186][0m |          -0.0005 |           5.8661 |          11.3771 |
[32m[20230113 20:20:32 @agent_ppo2.py:186][0m |          -0.0018 |           5.1683 |          11.3944 |
[32m[20230113 20:20:32 @agent_ppo2.py:186][0m |          -0.0108 |           4.6525 |          11.3726 |
[32m[20230113 20:20:32 @agent_ppo2.py:186][0m |          -0.0110 |           4.3674 |          11.3858 |
[32m[20230113 20:20:32 @agent_ppo2.py:186][0m |          -0.0100 |           4.1583 |          11.3852 |
[32m[20230113 20:20:32 @agent_ppo2.py:186][0m |          -0.0028 |           4.1379 |          11.3803 |
[32m[20230113 20:20:32 @agent_ppo2.py:186][0m |          -0.0133 |           3.8223 |          11.3702 |
[32m[20230113 20:20:32 @agent_ppo2.py:186][0m |          -0.0125 |           3.7255 |          11.3785 |
[32m[20230113 20:20:32 @agent_ppo2.py:186][0m |          -0.0155 |           3.6076 |          11.3789 |
[32m[20230113 20:20:32 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:20:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.33
[32m[20230113 20:20:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.67
[32m[20230113 20:20:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.85
[32m[20230113 20:20:32 @agent_ppo2.py:144][0m Total time:      36.00 min
[32m[20230113 20:20:32 @agent_ppo2.py:146][0m 3348480 total steps have happened
[32m[20230113 20:20:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1635 --------------------------#
[32m[20230113 20:20:33 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:33 @agent_ppo2.py:186][0m |           0.0191 |           7.9832 |          11.1336 |
[32m[20230113 20:20:33 @agent_ppo2.py:186][0m |          -0.0138 |           6.1714 |          11.0932 |
[32m[20230113 20:20:33 @agent_ppo2.py:186][0m |          -0.0089 |           5.2121 |          11.1103 |
[32m[20230113 20:20:33 @agent_ppo2.py:186][0m |          -0.0120 |           4.8685 |          11.1016 |
[32m[20230113 20:20:33 @agent_ppo2.py:186][0m |          -0.0181 |           4.5915 |          11.1015 |
[32m[20230113 20:20:33 @agent_ppo2.py:186][0m |          -0.0076 |           4.5244 |          11.1084 |
[32m[20230113 20:20:33 @agent_ppo2.py:186][0m |          -0.0203 |           4.3267 |          11.1093 |
[32m[20230113 20:20:33 @agent_ppo2.py:186][0m |          -0.0292 |           4.3172 |          11.1059 |
[32m[20230113 20:20:33 @agent_ppo2.py:186][0m |          -0.0321 |           4.1715 |          11.0955 |
[32m[20230113 20:20:33 @agent_ppo2.py:186][0m |          -0.0136 |           3.9011 |          11.1128 |
[32m[20230113 20:20:33 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:20:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.43
[32m[20230113 20:20:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.56
[32m[20230113 20:20:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.89
[32m[20230113 20:20:34 @agent_ppo2.py:144][0m Total time:      36.02 min
[32m[20230113 20:20:34 @agent_ppo2.py:146][0m 3350528 total steps have happened
[32m[20230113 20:20:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1636 --------------------------#
[32m[20230113 20:20:34 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:20:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:34 @agent_ppo2.py:186][0m |           0.0020 |           7.1819 |          11.2898 |
[32m[20230113 20:20:34 @agent_ppo2.py:186][0m |          -0.0103 |           5.4847 |          11.2685 |
[32m[20230113 20:20:34 @agent_ppo2.py:186][0m |          -0.0093 |           5.0732 |          11.2874 |
[32m[20230113 20:20:34 @agent_ppo2.py:186][0m |          -0.0093 |           4.6365 |          11.2993 |
[32m[20230113 20:20:34 @agent_ppo2.py:186][0m |          -0.0097 |           4.3009 |          11.2763 |
[32m[20230113 20:20:34 @agent_ppo2.py:186][0m |          -0.0136 |           4.1475 |          11.2775 |
[32m[20230113 20:20:34 @agent_ppo2.py:186][0m |          -0.0105 |           4.0393 |          11.2810 |
[32m[20230113 20:20:34 @agent_ppo2.py:186][0m |          -0.0078 |           4.1098 |          11.2719 |
[32m[20230113 20:20:35 @agent_ppo2.py:186][0m |          -0.0130 |           3.7572 |          11.2770 |
[32m[20230113 20:20:35 @agent_ppo2.py:186][0m |          -0.0176 |           3.6266 |          11.2815 |
[32m[20230113 20:20:35 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:20:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.59
[32m[20230113 20:20:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.48
[32m[20230113 20:20:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 157.02
[32m[20230113 20:20:35 @agent_ppo2.py:144][0m Total time:      36.04 min
[32m[20230113 20:20:35 @agent_ppo2.py:146][0m 3352576 total steps have happened
[32m[20230113 20:20:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1637 --------------------------#
[32m[20230113 20:20:35 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:20:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:36 @agent_ppo2.py:186][0m |           0.0012 |           6.7081 |          11.3924 |
[32m[20230113 20:20:36 @agent_ppo2.py:186][0m |          -0.0033 |           5.0205 |          11.3671 |
[32m[20230113 20:20:36 @agent_ppo2.py:186][0m |          -0.0071 |           4.5057 |          11.3535 |
[32m[20230113 20:20:36 @agent_ppo2.py:186][0m |          -0.0056 |           4.1965 |          11.3475 |
[32m[20230113 20:20:36 @agent_ppo2.py:186][0m |          -0.0078 |           4.0316 |          11.3470 |
[32m[20230113 20:20:36 @agent_ppo2.py:186][0m |          -0.0100 |           3.8422 |          11.3474 |
[32m[20230113 20:20:36 @agent_ppo2.py:186][0m |          -0.0100 |           3.7051 |          11.3489 |
[32m[20230113 20:20:36 @agent_ppo2.py:186][0m |          -0.0110 |           3.5833 |          11.3415 |
[32m[20230113 20:20:36 @agent_ppo2.py:186][0m |          -0.0115 |           3.5852 |          11.3386 |
[32m[20230113 20:20:36 @agent_ppo2.py:186][0m |          -0.0110 |           3.4353 |          11.3491 |
[32m[20230113 20:20:36 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:20:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.56
[32m[20230113 20:20:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.27
[32m[20230113 20:20:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.34
[32m[20230113 20:20:36 @agent_ppo2.py:144][0m Total time:      36.06 min
[32m[20230113 20:20:36 @agent_ppo2.py:146][0m 3354624 total steps have happened
[32m[20230113 20:20:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1638 --------------------------#
[32m[20230113 20:20:37 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:20:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:37 @agent_ppo2.py:186][0m |          -0.0303 |          15.1790 |          11.1591 |
[32m[20230113 20:20:37 @agent_ppo2.py:186][0m |          -0.0020 |           7.1403 |          11.1413 |
[32m[20230113 20:20:37 @agent_ppo2.py:186][0m |          -0.0200 |           5.7207 |          11.1430 |
[32m[20230113 20:20:37 @agent_ppo2.py:186][0m |          -0.0285 |           4.7858 |          11.1300 |
[32m[20230113 20:20:37 @agent_ppo2.py:186][0m |          -0.0211 |           4.5087 |          11.1337 |
[32m[20230113 20:20:37 @agent_ppo2.py:186][0m |          -0.0149 |           4.4209 |          11.1326 |
[32m[20230113 20:20:37 @agent_ppo2.py:186][0m |          -0.0116 |           4.2140 |          11.1348 |
[32m[20230113 20:20:37 @agent_ppo2.py:186][0m |          -0.0228 |           4.2370 |          11.1406 |
[32m[20230113 20:20:37 @agent_ppo2.py:186][0m |          -0.0345 |           4.0264 |          11.1293 |
[32m[20230113 20:20:37 @agent_ppo2.py:186][0m |          -0.0233 |           3.9690 |          11.1363 |
[32m[20230113 20:20:37 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:20:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 128.90
[32m[20230113 20:20:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.25
[32m[20230113 20:20:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.74
[32m[20230113 20:20:37 @agent_ppo2.py:144][0m Total time:      36.08 min
[32m[20230113 20:20:37 @agent_ppo2.py:146][0m 3356672 total steps have happened
[32m[20230113 20:20:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1639 --------------------------#
[32m[20230113 20:20:38 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:38 @agent_ppo2.py:186][0m |           0.0004 |           8.9515 |          11.4151 |
[32m[20230113 20:20:38 @agent_ppo2.py:186][0m |          -0.0066 |           6.0532 |          11.4011 |
[32m[20230113 20:20:38 @agent_ppo2.py:186][0m |          -0.0099 |           5.1889 |          11.3980 |
[32m[20230113 20:20:38 @agent_ppo2.py:186][0m |          -0.0104 |           4.7490 |          11.3950 |
[32m[20230113 20:20:38 @agent_ppo2.py:186][0m |          -0.0136 |           4.4307 |          11.3951 |
[32m[20230113 20:20:38 @agent_ppo2.py:186][0m |          -0.0145 |           4.1745 |          11.3896 |
[32m[20230113 20:20:38 @agent_ppo2.py:186][0m |          -0.0176 |           4.0017 |          11.3852 |
[32m[20230113 20:20:38 @agent_ppo2.py:186][0m |          -0.0158 |           3.8565 |          11.3948 |
[32m[20230113 20:20:38 @agent_ppo2.py:186][0m |          -0.0176 |           3.7294 |          11.3863 |
[32m[20230113 20:20:38 @agent_ppo2.py:186][0m |          -0.0164 |           3.5917 |          11.3758 |
[32m[20230113 20:20:38 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.18
[32m[20230113 20:20:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.73
[32m[20230113 20:20:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.63
[32m[20230113 20:20:39 @agent_ppo2.py:144][0m Total time:      36.10 min
[32m[20230113 20:20:39 @agent_ppo2.py:146][0m 3358720 total steps have happened
[32m[20230113 20:20:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1640 --------------------------#
[32m[20230113 20:20:39 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:20:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:39 @agent_ppo2.py:186][0m |           0.0037 |           5.3930 |          11.4225 |
[32m[20230113 20:20:39 @agent_ppo2.py:186][0m |          -0.0054 |           4.4314 |          11.4025 |
[32m[20230113 20:20:39 @agent_ppo2.py:186][0m |          -0.0087 |           4.1198 |          11.3944 |
[32m[20230113 20:20:39 @agent_ppo2.py:186][0m |          -0.0115 |           3.9492 |          11.3948 |
[32m[20230113 20:20:39 @agent_ppo2.py:186][0m |          -0.0144 |           3.8006 |          11.3980 |
[32m[20230113 20:20:39 @agent_ppo2.py:186][0m |          -0.0149 |           3.6435 |          11.3957 |
[32m[20230113 20:20:39 @agent_ppo2.py:186][0m |          -0.0154 |           3.5626 |          11.4001 |
[32m[20230113 20:20:39 @agent_ppo2.py:186][0m |          -0.0139 |           3.5029 |          11.3996 |
[32m[20230113 20:20:39 @agent_ppo2.py:186][0m |          -0.0154 |           3.4221 |          11.3879 |
[32m[20230113 20:20:39 @agent_ppo2.py:186][0m |          -0.0213 |           3.3315 |          11.3992 |
[32m[20230113 20:20:39 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:20:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.62
[32m[20230113 20:20:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.27
[32m[20230113 20:20:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.13
[32m[20230113 20:20:40 @agent_ppo2.py:144][0m Total time:      36.12 min
[32m[20230113 20:20:40 @agent_ppo2.py:146][0m 3360768 total steps have happened
[32m[20230113 20:20:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1641 --------------------------#
[32m[20230113 20:20:40 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:40 @agent_ppo2.py:186][0m |          -0.0014 |           5.9928 |          11.7025 |
[32m[20230113 20:20:40 @agent_ppo2.py:186][0m |          -0.0078 |           5.1971 |          11.6981 |
[32m[20230113 20:20:40 @agent_ppo2.py:186][0m |          -0.0091 |           4.8299 |          11.6983 |
[32m[20230113 20:20:41 @agent_ppo2.py:186][0m |          -0.0110 |           4.6262 |          11.7122 |
[32m[20230113 20:20:41 @agent_ppo2.py:186][0m |          -0.0127 |           4.4353 |          11.6874 |
[32m[20230113 20:20:41 @agent_ppo2.py:186][0m |          -0.0120 |           4.3225 |          11.7045 |
[32m[20230113 20:20:41 @agent_ppo2.py:186][0m |          -0.0133 |           4.1705 |          11.6913 |
[32m[20230113 20:20:41 @agent_ppo2.py:186][0m |          -0.0148 |           4.0544 |          11.7085 |
[32m[20230113 20:20:41 @agent_ppo2.py:186][0m |          -0.0159 |           3.9115 |          11.7087 |
[32m[20230113 20:20:41 @agent_ppo2.py:186][0m |          -0.0152 |           3.9001 |          11.7034 |
[32m[20230113 20:20:41 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.26
[32m[20230113 20:20:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.18
[32m[20230113 20:20:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.18
[32m[20230113 20:20:41 @agent_ppo2.py:144][0m Total time:      36.14 min
[32m[20230113 20:20:41 @agent_ppo2.py:146][0m 3362816 total steps have happened
[32m[20230113 20:20:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1642 --------------------------#
[32m[20230113 20:20:42 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:42 @agent_ppo2.py:186][0m |          -0.0009 |           7.9344 |          11.8336 |
[32m[20230113 20:20:42 @agent_ppo2.py:186][0m |          -0.0040 |           6.0849 |          11.8213 |
[32m[20230113 20:20:42 @agent_ppo2.py:186][0m |          -0.0080 |           5.4989 |          11.7950 |
[32m[20230113 20:20:42 @agent_ppo2.py:186][0m |          -0.0075 |           5.2077 |          11.8093 |
[32m[20230113 20:20:42 @agent_ppo2.py:186][0m |          -0.0078 |           4.9595 |          11.7967 |
[32m[20230113 20:20:42 @agent_ppo2.py:186][0m |          -0.0109 |           4.7572 |          11.7974 |
[32m[20230113 20:20:42 @agent_ppo2.py:186][0m |          -0.0100 |           4.6247 |          11.7922 |
[32m[20230113 20:20:42 @agent_ppo2.py:186][0m |          -0.0114 |           4.4331 |          11.7988 |
[32m[20230113 20:20:42 @agent_ppo2.py:186][0m |          -0.0148 |           4.3426 |          11.7982 |
[32m[20230113 20:20:42 @agent_ppo2.py:186][0m |          -0.0133 |           4.2515 |          11.7899 |
[32m[20230113 20:20:42 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:20:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.16
[32m[20230113 20:20:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.32
[32m[20230113 20:20:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 132.92
[32m[20230113 20:20:42 @agent_ppo2.py:144][0m Total time:      36.16 min
[32m[20230113 20:20:42 @agent_ppo2.py:146][0m 3364864 total steps have happened
[32m[20230113 20:20:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1643 --------------------------#
[32m[20230113 20:20:43 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:20:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:43 @agent_ppo2.py:186][0m |           0.0018 |          14.7769 |          11.7762 |
[32m[20230113 20:20:43 @agent_ppo2.py:186][0m |          -0.0042 |           6.6032 |          11.7708 |
[32m[20230113 20:20:43 @agent_ppo2.py:186][0m |          -0.0080 |           5.6002 |          11.7623 |
[32m[20230113 20:20:43 @agent_ppo2.py:186][0m |          -0.0072 |           5.0907 |          11.7571 |
[32m[20230113 20:20:43 @agent_ppo2.py:186][0m |          -0.0119 |           4.7229 |          11.7602 |
[32m[20230113 20:20:43 @agent_ppo2.py:186][0m |          -0.0127 |           4.4944 |          11.7526 |
[32m[20230113 20:20:43 @agent_ppo2.py:186][0m |          -0.0131 |           4.2294 |          11.7596 |
[32m[20230113 20:20:43 @agent_ppo2.py:186][0m |          -0.0158 |           4.0443 |          11.7394 |
[32m[20230113 20:20:43 @agent_ppo2.py:186][0m |          -0.0156 |           3.8885 |          11.7470 |
[32m[20230113 20:20:43 @agent_ppo2.py:186][0m |          -0.0175 |           3.7767 |          11.7451 |
[32m[20230113 20:20:43 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:20:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 126.99
[32m[20230113 20:20:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.83
[32m[20230113 20:20:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.04
[32m[20230113 20:20:43 @agent_ppo2.py:144][0m Total time:      36.18 min
[32m[20230113 20:20:43 @agent_ppo2.py:146][0m 3366912 total steps have happened
[32m[20230113 20:20:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1644 --------------------------#
[32m[20230113 20:20:44 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:20:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:44 @agent_ppo2.py:186][0m |           0.0014 |           6.9229 |          11.5457 |
[32m[20230113 20:20:44 @agent_ppo2.py:186][0m |          -0.0021 |           5.4384 |          11.5188 |
[32m[20230113 20:20:44 @agent_ppo2.py:186][0m |          -0.0079 |           4.9269 |          11.5300 |
[32m[20230113 20:20:44 @agent_ppo2.py:186][0m |          -0.0124 |           4.6683 |          11.5267 |
[32m[20230113 20:20:44 @agent_ppo2.py:186][0m |          -0.0064 |           4.4887 |          11.5053 |
[32m[20230113 20:20:44 @agent_ppo2.py:186][0m |          -0.0115 |           4.1673 |          11.5185 |
[32m[20230113 20:20:44 @agent_ppo2.py:186][0m |          -0.0142 |           4.0268 |          11.5279 |
[32m[20230113 20:20:44 @agent_ppo2.py:186][0m |          -0.0213 |           3.8779 |          11.5087 |
[32m[20230113 20:20:44 @agent_ppo2.py:186][0m |          -0.0198 |           3.7953 |          11.5200 |
[32m[20230113 20:20:44 @agent_ppo2.py:186][0m |          -0.0219 |           3.6773 |          11.5198 |
[32m[20230113 20:20:44 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:20:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.38
[32m[20230113 20:20:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 245.28
[32m[20230113 20:20:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.21
[32m[20230113 20:20:45 @agent_ppo2.py:144][0m Total time:      36.20 min
[32m[20230113 20:20:45 @agent_ppo2.py:146][0m 3368960 total steps have happened
[32m[20230113 20:20:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1645 --------------------------#
[32m[20230113 20:20:45 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:20:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:45 @agent_ppo2.py:186][0m |           0.0005 |           6.8286 |          11.4657 |
[32m[20230113 20:20:45 @agent_ppo2.py:186][0m |          -0.0150 |           5.1719 |          11.4631 |
[32m[20230113 20:20:45 @agent_ppo2.py:186][0m |          -0.0073 |           4.3246 |          11.4697 |
[32m[20230113 20:20:45 @agent_ppo2.py:186][0m |          -0.0074 |           3.8668 |          11.4664 |
[32m[20230113 20:20:45 @agent_ppo2.py:186][0m |          -0.0278 |           3.6014 |          11.4766 |
[32m[20230113 20:20:45 @agent_ppo2.py:186][0m |          -0.0094 |           3.3838 |          11.4764 |
[32m[20230113 20:20:45 @agent_ppo2.py:186][0m |          -0.0177 |           3.2895 |          11.4710 |
[32m[20230113 20:20:45 @agent_ppo2.py:186][0m |          -0.0129 |           3.1089 |          11.4743 |
[32m[20230113 20:20:45 @agent_ppo2.py:186][0m |          -0.0230 |           2.9822 |          11.4707 |
[32m[20230113 20:20:45 @agent_ppo2.py:186][0m |          -0.0261 |           2.9016 |          11.4752 |
[32m[20230113 20:20:45 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.32
[32m[20230113 20:20:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.06
[32m[20230113 20:20:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.69
[32m[20230113 20:20:46 @agent_ppo2.py:144][0m Total time:      36.22 min
[32m[20230113 20:20:46 @agent_ppo2.py:146][0m 3371008 total steps have happened
[32m[20230113 20:20:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1646 --------------------------#
[32m[20230113 20:20:46 @agent_ppo2.py:128][0m Sampling time: 0.33 s by 1 slaves
[32m[20230113 20:20:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:46 @agent_ppo2.py:186][0m |          -0.0022 |          22.0249 |          11.5759 |
[32m[20230113 20:20:46 @agent_ppo2.py:186][0m |          -0.0057 |           8.1337 |          11.5777 |
[32m[20230113 20:20:46 @agent_ppo2.py:186][0m |          -0.0127 |           6.1809 |          11.5741 |
[32m[20230113 20:20:46 @agent_ppo2.py:186][0m |          -0.0055 |           5.5954 |          11.5644 |
[32m[20230113 20:20:46 @agent_ppo2.py:186][0m |          -0.0072 |           5.0955 |          11.5795 |
[32m[20230113 20:20:46 @agent_ppo2.py:186][0m |          -0.0118 |           4.7897 |          11.5688 |
[32m[20230113 20:20:46 @agent_ppo2.py:186][0m |          -0.0161 |           4.5331 |          11.5686 |
[32m[20230113 20:20:46 @agent_ppo2.py:186][0m |          -0.0170 |           4.4150 |          11.5636 |
[32m[20230113 20:20:46 @agent_ppo2.py:186][0m |          -0.0176 |           4.1900 |          11.5660 |
[32m[20230113 20:20:47 @agent_ppo2.py:186][0m |          -0.0170 |           4.0406 |          11.5638 |
[32m[20230113 20:20:47 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:20:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 125.61
[32m[20230113 20:20:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.63
[32m[20230113 20:20:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.51
[32m[20230113 20:20:47 @agent_ppo2.py:144][0m Total time:      36.24 min
[32m[20230113 20:20:47 @agent_ppo2.py:146][0m 3373056 total steps have happened
[32m[20230113 20:20:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1647 --------------------------#
[32m[20230113 20:20:47 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:20:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:47 @agent_ppo2.py:186][0m |          -0.0035 |           9.8650 |          11.7664 |
[32m[20230113 20:20:48 @agent_ppo2.py:186][0m |          -0.0039 |           7.5559 |          11.7575 |
[32m[20230113 20:20:48 @agent_ppo2.py:186][0m |          -0.0046 |           6.4053 |          11.7581 |
[32m[20230113 20:20:48 @agent_ppo2.py:186][0m |          -0.0120 |           5.8261 |          11.7613 |
[32m[20230113 20:20:48 @agent_ppo2.py:186][0m |          -0.0086 |           5.4604 |          11.7451 |
[32m[20230113 20:20:48 @agent_ppo2.py:186][0m |          -0.0141 |           5.1982 |          11.7497 |
[32m[20230113 20:20:48 @agent_ppo2.py:186][0m |          -0.0134 |           4.9422 |          11.7576 |
[32m[20230113 20:20:48 @agent_ppo2.py:186][0m |          -0.0220 |           4.6910 |          11.7525 |
[32m[20230113 20:20:48 @agent_ppo2.py:186][0m |          -0.0126 |           4.5281 |          11.7420 |
[32m[20230113 20:20:48 @agent_ppo2.py:186][0m |          -0.0141 |           4.5343 |          11.7576 |
[32m[20230113 20:20:48 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:20:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.83
[32m[20230113 20:20:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.87
[32m[20230113 20:20:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.10
[32m[20230113 20:20:48 @agent_ppo2.py:144][0m Total time:      36.26 min
[32m[20230113 20:20:48 @agent_ppo2.py:146][0m 3375104 total steps have happened
[32m[20230113 20:20:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1648 --------------------------#
[32m[20230113 20:20:49 @agent_ppo2.py:128][0m Sampling time: 0.49 s by 1 slaves
[32m[20230113 20:20:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:49 @agent_ppo2.py:186][0m |          -0.0006 |          16.7771 |          11.8186 |
[32m[20230113 20:20:49 @agent_ppo2.py:186][0m |           0.0292 |          11.5840 |          11.8129 |
[32m[20230113 20:20:49 @agent_ppo2.py:186][0m |          -0.0005 |          10.4672 |          11.7808 |
[32m[20230113 20:20:49 @agent_ppo2.py:186][0m |          -0.0006 |           7.6326 |          11.7823 |
[32m[20230113 20:20:49 @agent_ppo2.py:186][0m |           0.0027 |           7.2914 |          11.7766 |
[32m[20230113 20:20:49 @agent_ppo2.py:186][0m |           0.0003 |           7.2601 |          11.7886 |
[32m[20230113 20:20:49 @agent_ppo2.py:186][0m |          -0.0284 |           6.3057 |          11.7761 |
[32m[20230113 20:20:49 @agent_ppo2.py:186][0m |           0.0117 |           5.9813 |          11.7869 |
[32m[20230113 20:20:49 @agent_ppo2.py:186][0m |          -0.0144 |           5.6739 |          11.7648 |
[32m[20230113 20:20:49 @agent_ppo2.py:186][0m |          -0.0222 |           5.4674 |          11.7753 |
[32m[20230113 20:20:49 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:20:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 150.62
[32m[20230113 20:20:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.86
[32m[20230113 20:20:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 111.64
[32m[20230113 20:20:50 @agent_ppo2.py:144][0m Total time:      36.28 min
[32m[20230113 20:20:50 @agent_ppo2.py:146][0m 3377152 total steps have happened
[32m[20230113 20:20:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1649 --------------------------#
[32m[20230113 20:20:50 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:20:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:50 @agent_ppo2.py:186][0m |           0.0026 |           8.6788 |          12.0277 |
[32m[20230113 20:20:50 @agent_ppo2.py:186][0m |          -0.0053 |           6.2338 |          12.0149 |
[32m[20230113 20:20:50 @agent_ppo2.py:186][0m |          -0.0075 |           5.5863 |          11.9917 |
[32m[20230113 20:20:50 @agent_ppo2.py:186][0m |          -0.0117 |           5.0783 |          12.0070 |
[32m[20230113 20:20:50 @agent_ppo2.py:186][0m |          -0.0126 |           4.8168 |          11.9952 |
[32m[20230113 20:20:50 @agent_ppo2.py:186][0m |          -0.0129 |           4.5772 |          11.9929 |
[32m[20230113 20:20:50 @agent_ppo2.py:186][0m |          -0.0143 |           4.3923 |          11.9922 |
[32m[20230113 20:20:50 @agent_ppo2.py:186][0m |          -0.0145 |           4.2673 |          11.9978 |
[32m[20230113 20:20:50 @agent_ppo2.py:186][0m |          -0.0157 |           4.1374 |          11.9976 |
[32m[20230113 20:20:50 @agent_ppo2.py:186][0m |          -0.0162 |           3.9904 |          11.9926 |
[32m[20230113 20:20:50 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:20:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.08
[32m[20230113 20:20:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.54
[32m[20230113 20:20:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.59
[32m[20230113 20:20:51 @agent_ppo2.py:144][0m Total time:      36.30 min
[32m[20230113 20:20:51 @agent_ppo2.py:146][0m 3379200 total steps have happened
[32m[20230113 20:20:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1650 --------------------------#
[32m[20230113 20:20:51 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:20:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:51 @agent_ppo2.py:186][0m |           0.0004 |           6.0655 |          11.8574 |
[32m[20230113 20:20:51 @agent_ppo2.py:186][0m |          -0.0080 |           4.6390 |          11.8494 |
[32m[20230113 20:20:51 @agent_ppo2.py:186][0m |          -0.0114 |           4.1669 |          11.8393 |
[32m[20230113 20:20:51 @agent_ppo2.py:186][0m |          -0.0129 |           3.8662 |          11.8405 |
[32m[20230113 20:20:51 @agent_ppo2.py:186][0m |          -0.0140 |           3.6717 |          11.8404 |
[32m[20230113 20:20:51 @agent_ppo2.py:186][0m |          -0.0151 |           3.5433 |          11.8290 |
[32m[20230113 20:20:51 @agent_ppo2.py:186][0m |          -0.0161 |           3.4404 |          11.8291 |
[32m[20230113 20:20:52 @agent_ppo2.py:186][0m |          -0.0157 |           3.3278 |          11.8288 |
[32m[20230113 20:20:52 @agent_ppo2.py:186][0m |          -0.0170 |           3.2749 |          11.8274 |
[32m[20230113 20:20:52 @agent_ppo2.py:186][0m |          -0.0178 |           3.1896 |          11.8301 |
[32m[20230113 20:20:52 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.56
[32m[20230113 20:20:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.86
[32m[20230113 20:20:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.87
[32m[20230113 20:20:52 @agent_ppo2.py:144][0m Total time:      36.32 min
[32m[20230113 20:20:52 @agent_ppo2.py:146][0m 3381248 total steps have happened
[32m[20230113 20:20:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1651 --------------------------#
[32m[20230113 20:20:52 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:20:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:52 @agent_ppo2.py:186][0m |           0.0131 |          14.2014 |          11.5400 |
[32m[20230113 20:20:52 @agent_ppo2.py:186][0m |          -0.0183 |           7.6116 |          11.5110 |
[32m[20230113 20:20:52 @agent_ppo2.py:186][0m |          -0.0232 |           6.2348 |          11.4857 |
[32m[20230113 20:20:52 @agent_ppo2.py:186][0m |          -0.0129 |           5.9338 |          11.5016 |
[32m[20230113 20:20:53 @agent_ppo2.py:186][0m |          -0.0212 |           5.3376 |          11.4860 |
[32m[20230113 20:20:53 @agent_ppo2.py:186][0m |          -0.0266 |           4.9514 |          11.4908 |
[32m[20230113 20:20:53 @agent_ppo2.py:186][0m |          -0.0227 |           4.7078 |          11.4898 |
[32m[20230113 20:20:53 @agent_ppo2.py:186][0m |           0.0034 |           4.4601 |          11.4988 |
[32m[20230113 20:20:53 @agent_ppo2.py:186][0m |          -0.0204 |           4.3944 |          11.4935 |
[32m[20230113 20:20:53 @agent_ppo2.py:186][0m |          -0.0082 |           5.1615 |          11.4861 |
[32m[20230113 20:20:53 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:20:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 128.40
[32m[20230113 20:20:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.47
[32m[20230113 20:20:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 176.60
[32m[20230113 20:20:53 @agent_ppo2.py:144][0m Total time:      36.34 min
[32m[20230113 20:20:53 @agent_ppo2.py:146][0m 3383296 total steps have happened
[32m[20230113 20:20:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1652 --------------------------#
[32m[20230113 20:20:53 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:20:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:54 @agent_ppo2.py:186][0m |           0.0022 |          16.6573 |          11.9722 |
[32m[20230113 20:20:54 @agent_ppo2.py:186][0m |          -0.0042 |          10.2763 |          11.9755 |
[32m[20230113 20:20:54 @agent_ppo2.py:186][0m |          -0.0091 |           8.2149 |          11.9627 |
[32m[20230113 20:20:54 @agent_ppo2.py:186][0m |          -0.0092 |           7.1493 |          11.9579 |
[32m[20230113 20:20:54 @agent_ppo2.py:186][0m |          -0.0211 |           6.3725 |          11.9354 |
[32m[20230113 20:20:54 @agent_ppo2.py:186][0m |          -0.0129 |           5.8018 |          11.9529 |
[32m[20230113 20:20:54 @agent_ppo2.py:186][0m |          -0.0161 |           5.3543 |          11.9529 |
[32m[20230113 20:20:54 @agent_ppo2.py:186][0m |          -0.0166 |           5.0130 |          11.9465 |
[32m[20230113 20:20:54 @agent_ppo2.py:186][0m |          -0.0150 |           4.7194 |          11.9316 |
[32m[20230113 20:20:54 @agent_ppo2.py:186][0m |          -0.0170 |           4.4765 |          11.9414 |
[32m[20230113 20:20:54 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:20:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.29
[32m[20230113 20:20:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.18
[32m[20230113 20:20:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.83
[32m[20230113 20:20:54 @agent_ppo2.py:144][0m Total time:      36.36 min
[32m[20230113 20:20:54 @agent_ppo2.py:146][0m 3385344 total steps have happened
[32m[20230113 20:20:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1653 --------------------------#
[32m[20230113 20:20:55 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:20:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:55 @agent_ppo2.py:186][0m |          -0.0031 |           8.9762 |          11.6561 |
[32m[20230113 20:20:55 @agent_ppo2.py:186][0m |          -0.0095 |           7.1555 |          11.6385 |
[32m[20230113 20:20:55 @agent_ppo2.py:186][0m |          -0.0041 |           6.6382 |          11.6355 |
[32m[20230113 20:20:55 @agent_ppo2.py:186][0m |          -0.0021 |           6.2311 |          11.6338 |
[32m[20230113 20:20:55 @agent_ppo2.py:186][0m |          -0.0170 |           5.8742 |          11.6280 |
[32m[20230113 20:20:55 @agent_ppo2.py:186][0m |          -0.0169 |           5.6301 |          11.6274 |
[32m[20230113 20:20:55 @agent_ppo2.py:186][0m |          -0.0139 |           5.4509 |          11.6274 |
[32m[20230113 20:20:55 @agent_ppo2.py:186][0m |          -0.0224 |           5.2056 |          11.6314 |
[32m[20230113 20:20:55 @agent_ppo2.py:186][0m |          -0.0247 |           5.1040 |          11.6163 |
[32m[20230113 20:20:55 @agent_ppo2.py:186][0m |          -0.0139 |           4.9409 |          11.6189 |
[32m[20230113 20:20:55 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.84
[32m[20230113 20:20:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.55
[32m[20230113 20:20:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.36
[32m[20230113 20:20:56 @agent_ppo2.py:144][0m Total time:      36.38 min
[32m[20230113 20:20:56 @agent_ppo2.py:146][0m 3387392 total steps have happened
[32m[20230113 20:20:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1654 --------------------------#
[32m[20230113 20:20:56 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:20:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:56 @agent_ppo2.py:186][0m |          -0.0008 |           8.0355 |          11.8409 |
[32m[20230113 20:20:56 @agent_ppo2.py:186][0m |          -0.0069 |           6.1222 |          11.8177 |
[32m[20230113 20:20:56 @agent_ppo2.py:186][0m |          -0.0081 |           5.2205 |          11.8270 |
[32m[20230113 20:20:56 @agent_ppo2.py:186][0m |          -0.0173 |           4.8179 |          11.8199 |
[32m[20230113 20:20:56 @agent_ppo2.py:186][0m |          -0.0157 |           4.5038 |          11.8141 |
[32m[20230113 20:20:56 @agent_ppo2.py:186][0m |          -0.0098 |           4.7096 |          11.8151 |
[32m[20230113 20:20:56 @agent_ppo2.py:186][0m |          -0.0008 |           4.2570 |          11.7979 |
[32m[20230113 20:20:56 @agent_ppo2.py:186][0m |          -0.0167 |           3.9617 |          11.8059 |
[32m[20230113 20:20:56 @agent_ppo2.py:186][0m |          -0.0147 |           3.8254 |          11.8067 |
[32m[20230113 20:20:56 @agent_ppo2.py:186][0m |          -0.0177 |           3.7111 |          11.8019 |
[32m[20230113 20:20:56 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.38
[32m[20230113 20:20:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.05
[32m[20230113 20:20:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.69
[32m[20230113 20:20:57 @agent_ppo2.py:144][0m Total time:      36.40 min
[32m[20230113 20:20:57 @agent_ppo2.py:146][0m 3389440 total steps have happened
[32m[20230113 20:20:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1655 --------------------------#
[32m[20230113 20:20:57 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:20:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:57 @agent_ppo2.py:186][0m |          -0.0002 |           6.2331 |          11.6631 |
[32m[20230113 20:20:57 @agent_ppo2.py:186][0m |          -0.0045 |           5.0549 |          11.6739 |
[32m[20230113 20:20:57 @agent_ppo2.py:186][0m |          -0.0078 |           4.4685 |          11.6588 |
[32m[20230113 20:20:57 @agent_ppo2.py:186][0m |          -0.0109 |           4.1315 |          11.6506 |
[32m[20230113 20:20:57 @agent_ppo2.py:186][0m |          -0.0118 |           3.9074 |          11.6657 |
[32m[20230113 20:20:57 @agent_ppo2.py:186][0m |          -0.0135 |           3.7560 |          11.6532 |
[32m[20230113 20:20:58 @agent_ppo2.py:186][0m |          -0.0147 |           3.5847 |          11.6512 |
[32m[20230113 20:20:58 @agent_ppo2.py:186][0m |          -0.0139 |           3.4791 |          11.6546 |
[32m[20230113 20:20:58 @agent_ppo2.py:186][0m |          -0.0157 |           3.3678 |          11.6617 |
[32m[20230113 20:20:58 @agent_ppo2.py:186][0m |          -0.0174 |           3.2887 |          11.6605 |
[32m[20230113 20:20:58 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:20:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.39
[32m[20230113 20:20:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.03
[32m[20230113 20:20:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.92
[32m[20230113 20:20:58 @agent_ppo2.py:144][0m Total time:      36.42 min
[32m[20230113 20:20:58 @agent_ppo2.py:146][0m 3391488 total steps have happened
[32m[20230113 20:20:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1656 --------------------------#
[32m[20230113 20:20:58 @agent_ppo2.py:128][0m Sampling time: 0.37 s by 1 slaves
[32m[20230113 20:20:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:20:59 @agent_ppo2.py:186][0m |           0.0007 |          11.8870 |          11.8147 |
[32m[20230113 20:20:59 @agent_ppo2.py:186][0m |          -0.0032 |           6.2776 |          11.8150 |
[32m[20230113 20:20:59 @agent_ppo2.py:186][0m |          -0.0057 |           5.2078 |          11.8143 |
[32m[20230113 20:20:59 @agent_ppo2.py:186][0m |          -0.0061 |           4.5388 |          11.8132 |
[32m[20230113 20:20:59 @agent_ppo2.py:186][0m |          -0.0074 |           4.2450 |          11.8106 |
[32m[20230113 20:20:59 @agent_ppo2.py:186][0m |          -0.0095 |           3.9889 |          11.8153 |
[32m[20230113 20:20:59 @agent_ppo2.py:186][0m |          -0.0127 |           3.8747 |          11.8086 |
[32m[20230113 20:20:59 @agent_ppo2.py:186][0m |          -0.0133 |           3.7858 |          11.8096 |
[32m[20230113 20:20:59 @agent_ppo2.py:186][0m |          -0.0161 |           3.6155 |          11.8119 |
[32m[20230113 20:20:59 @agent_ppo2.py:186][0m |          -0.0150 |           3.5347 |          11.8042 |
[32m[20230113 20:20:59 @agent_ppo2.py:131][0m Policy update time: 0.38 s
[32m[20230113 20:20:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 146.49
[32m[20230113 20:20:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.70
[32m[20230113 20:20:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.01
[32m[20230113 20:20:59 @agent_ppo2.py:144][0m Total time:      36.44 min
[32m[20230113 20:20:59 @agent_ppo2.py:146][0m 3393536 total steps have happened
[32m[20230113 20:20:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1657 --------------------------#
[32m[20230113 20:21:00 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:21:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:00 @agent_ppo2.py:186][0m |          -0.0039 |          24.0128 |          11.7629 |
[32m[20230113 20:21:00 @agent_ppo2.py:186][0m |          -0.0099 |          16.0698 |          11.7361 |
[32m[20230113 20:21:00 @agent_ppo2.py:186][0m |          -0.0147 |          12.7089 |          11.7378 |
[32m[20230113 20:21:00 @agent_ppo2.py:186][0m |          -0.0164 |          11.2460 |          11.7393 |
[32m[20230113 20:21:00 @agent_ppo2.py:186][0m |          -0.0151 |          10.2560 |          11.7385 |
[32m[20230113 20:21:00 @agent_ppo2.py:186][0m |          -0.0179 |           9.4398 |          11.7312 |
[32m[20230113 20:21:00 @agent_ppo2.py:186][0m |          -0.0182 |           8.8497 |          11.7239 |
[32m[20230113 20:21:00 @agent_ppo2.py:186][0m |          -0.0209 |           8.3496 |          11.7235 |
[32m[20230113 20:21:00 @agent_ppo2.py:186][0m |          -0.0185 |           8.0215 |          11.7193 |
[32m[20230113 20:21:00 @agent_ppo2.py:186][0m |          -0.0198 |           7.5846 |          11.7233 |
[32m[20230113 20:21:00 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:21:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 126.98
[32m[20230113 20:21:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.50
[32m[20230113 20:21:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.21
[32m[20230113 20:21:01 @agent_ppo2.py:144][0m Total time:      36.47 min
[32m[20230113 20:21:01 @agent_ppo2.py:146][0m 3395584 total steps have happened
[32m[20230113 20:21:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1658 --------------------------#
[32m[20230113 20:21:01 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:01 @agent_ppo2.py:186][0m |          -0.0028 |           8.0340 |          11.6911 |
[32m[20230113 20:21:01 @agent_ppo2.py:186][0m |           0.0065 |           6.7934 |          11.6870 |
[32m[20230113 20:21:01 @agent_ppo2.py:186][0m |          -0.0017 |           6.3714 |          11.6755 |
[32m[20230113 20:21:01 @agent_ppo2.py:186][0m |          -0.0229 |           5.8939 |          11.6754 |
[32m[20230113 20:21:01 @agent_ppo2.py:186][0m |          -0.0079 |           5.6850 |          11.6746 |
[32m[20230113 20:21:01 @agent_ppo2.py:186][0m |          -0.0050 |           5.4325 |          11.6804 |
[32m[20230113 20:21:01 @agent_ppo2.py:186][0m |          -0.0223 |           5.3624 |          11.6825 |
[32m[20230113 20:21:01 @agent_ppo2.py:186][0m |          -0.0209 |           5.1214 |          11.6673 |
[32m[20230113 20:21:01 @agent_ppo2.py:186][0m |          -0.0199 |           4.9891 |          11.6629 |
[32m[20230113 20:21:01 @agent_ppo2.py:186][0m |          -0.0198 |           4.8752 |          11.6730 |
[32m[20230113 20:21:01 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:21:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.90
[32m[20230113 20:21:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.70
[32m[20230113 20:21:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.76
[32m[20230113 20:21:02 @agent_ppo2.py:144][0m Total time:      36.49 min
[32m[20230113 20:21:02 @agent_ppo2.py:146][0m 3397632 total steps have happened
[32m[20230113 20:21:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1659 --------------------------#
[32m[20230113 20:21:02 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:02 @agent_ppo2.py:186][0m |           0.0028 |           7.2544 |          11.8779 |
[32m[20230113 20:21:02 @agent_ppo2.py:186][0m |          -0.0054 |           5.4229 |          11.8576 |
[32m[20230113 20:21:02 @agent_ppo2.py:186][0m |          -0.0077 |           4.7386 |          11.8646 |
[32m[20230113 20:21:03 @agent_ppo2.py:186][0m |          -0.0087 |           4.3607 |          11.8537 |
[32m[20230113 20:21:03 @agent_ppo2.py:186][0m |          -0.0116 |           4.0995 |          11.8600 |
[32m[20230113 20:21:03 @agent_ppo2.py:186][0m |          -0.0121 |           3.9224 |          11.8588 |
[32m[20230113 20:21:03 @agent_ppo2.py:186][0m |          -0.0131 |           3.8119 |          11.8562 |
[32m[20230113 20:21:03 @agent_ppo2.py:186][0m |          -0.0139 |           3.6455 |          11.8522 |
[32m[20230113 20:21:03 @agent_ppo2.py:186][0m |          -0.0154 |           3.5563 |          11.8581 |
[32m[20230113 20:21:03 @agent_ppo2.py:186][0m |          -0.0155 |           3.4854 |          11.8620 |
[32m[20230113 20:21:03 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:21:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.81
[32m[20230113 20:21:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.29
[32m[20230113 20:21:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 188.07
[32m[20230113 20:21:03 @agent_ppo2.py:144][0m Total time:      36.51 min
[32m[20230113 20:21:03 @agent_ppo2.py:146][0m 3399680 total steps have happened
[32m[20230113 20:21:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1660 --------------------------#
[32m[20230113 20:21:04 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:04 @agent_ppo2.py:186][0m |          -0.0052 |           8.0511 |          11.6484 |
[32m[20230113 20:21:04 @agent_ppo2.py:186][0m |          -0.0093 |           6.2117 |          11.6159 |
[32m[20230113 20:21:04 @agent_ppo2.py:186][0m |          -0.0130 |           5.5187 |          11.6039 |
[32m[20230113 20:21:04 @agent_ppo2.py:186][0m |          -0.0135 |           5.0886 |          11.6134 |
[32m[20230113 20:21:04 @agent_ppo2.py:186][0m |          -0.0134 |           4.7225 |          11.6046 |
[32m[20230113 20:21:04 @agent_ppo2.py:186][0m |          -0.0154 |           4.5087 |          11.6099 |
[32m[20230113 20:21:04 @agent_ppo2.py:186][0m |          -0.0123 |           4.3398 |          11.6134 |
[32m[20230113 20:21:04 @agent_ppo2.py:186][0m |          -0.0172 |           4.1506 |          11.5969 |
[32m[20230113 20:21:04 @agent_ppo2.py:186][0m |          -0.0193 |           4.0068 |          11.5973 |
[32m[20230113 20:21:04 @agent_ppo2.py:186][0m |          -0.0157 |           3.8759 |          11.6014 |
[32m[20230113 20:21:04 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:21:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.99
[32m[20230113 20:21:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.31
[32m[20230113 20:21:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.84
[32m[20230113 20:21:04 @agent_ppo2.py:144][0m Total time:      36.53 min
[32m[20230113 20:21:04 @agent_ppo2.py:146][0m 3401728 total steps have happened
[32m[20230113 20:21:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1661 --------------------------#
[32m[20230113 20:21:05 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:05 @agent_ppo2.py:186][0m |          -0.0010 |           7.9531 |          12.1000 |
[32m[20230113 20:21:05 @agent_ppo2.py:186][0m |          -0.0081 |           6.6112 |          12.0808 |
[32m[20230113 20:21:05 @agent_ppo2.py:186][0m |          -0.0097 |           5.8985 |          12.0706 |
[32m[20230113 20:21:05 @agent_ppo2.py:186][0m |          -0.0115 |           5.4138 |          12.0664 |
[32m[20230113 20:21:05 @agent_ppo2.py:186][0m |          -0.0136 |           5.0601 |          12.0652 |
[32m[20230113 20:21:05 @agent_ppo2.py:186][0m |          -0.0137 |           4.8073 |          12.0756 |
[32m[20230113 20:21:05 @agent_ppo2.py:186][0m |          -0.0143 |           4.6170 |          12.0730 |
[32m[20230113 20:21:05 @agent_ppo2.py:186][0m |          -0.0158 |           4.4591 |          12.0627 |
[32m[20230113 20:21:05 @agent_ppo2.py:186][0m |          -0.0161 |           4.3040 |          12.0616 |
[32m[20230113 20:21:05 @agent_ppo2.py:186][0m |          -0.0169 |           4.1602 |          12.0595 |
[32m[20230113 20:21:05 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:21:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.87
[32m[20230113 20:21:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.64
[32m[20230113 20:21:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.72
[32m[20230113 20:21:06 @agent_ppo2.py:144][0m Total time:      36.55 min
[32m[20230113 20:21:06 @agent_ppo2.py:146][0m 3403776 total steps have happened
[32m[20230113 20:21:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1662 --------------------------#
[32m[20230113 20:21:06 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:21:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:06 @agent_ppo2.py:186][0m |           0.0052 |          14.3840 |          12.1767 |
[32m[20230113 20:21:06 @agent_ppo2.py:186][0m |          -0.0080 |           7.0609 |          12.1590 |
[32m[20230113 20:21:06 @agent_ppo2.py:186][0m |          -0.0118 |           5.7081 |          12.1553 |
[32m[20230113 20:21:06 @agent_ppo2.py:186][0m |          -0.0132 |           5.2494 |          12.1510 |
[32m[20230113 20:21:06 @agent_ppo2.py:186][0m |          -0.0133 |           4.9553 |          12.1500 |
[32m[20230113 20:21:06 @agent_ppo2.py:186][0m |          -0.0155 |           4.7460 |          12.1466 |
[32m[20230113 20:21:07 @agent_ppo2.py:186][0m |          -0.0170 |           4.5418 |          12.1405 |
[32m[20230113 20:21:07 @agent_ppo2.py:186][0m |          -0.0180 |           4.6073 |          12.1480 |
[32m[20230113 20:21:07 @agent_ppo2.py:186][0m |          -0.0150 |           4.3109 |          12.1421 |
[32m[20230113 20:21:07 @agent_ppo2.py:186][0m |          -0.0174 |           4.1407 |          12.1449 |
[32m[20230113 20:21:07 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:21:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 143.22
[32m[20230113 20:21:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.82
[32m[20230113 20:21:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.33
[32m[20230113 20:21:07 @agent_ppo2.py:144][0m Total time:      36.57 min
[32m[20230113 20:21:07 @agent_ppo2.py:146][0m 3405824 total steps have happened
[32m[20230113 20:21:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1663 --------------------------#
[32m[20230113 20:21:07 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:08 @agent_ppo2.py:186][0m |          -0.0002 |           6.9268 |          12.0670 |
[32m[20230113 20:21:08 @agent_ppo2.py:186][0m |          -0.0070 |           5.4099 |          12.0570 |
[32m[20230113 20:21:08 @agent_ppo2.py:186][0m |          -0.0110 |           4.7981 |          12.0505 |
[32m[20230113 20:21:08 @agent_ppo2.py:186][0m |          -0.0131 |           4.5581 |          12.0536 |
[32m[20230113 20:21:08 @agent_ppo2.py:186][0m |          -0.0151 |           4.3472 |          12.0420 |
[32m[20230113 20:21:08 @agent_ppo2.py:186][0m |          -0.0136 |           4.2422 |          12.0499 |
[32m[20230113 20:21:08 @agent_ppo2.py:186][0m |          -0.0163 |           4.0888 |          12.0478 |
[32m[20230113 20:21:08 @agent_ppo2.py:186][0m |          -0.0179 |           3.9514 |          12.0457 |
[32m[20230113 20:21:08 @agent_ppo2.py:186][0m |          -0.0184 |           3.9008 |          12.0403 |
[32m[20230113 20:21:08 @agent_ppo2.py:186][0m |          -0.0180 |           3.7743 |          12.0450 |
[32m[20230113 20:21:08 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.53
[32m[20230113 20:21:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.63
[32m[20230113 20:21:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.39
[32m[20230113 20:21:08 @agent_ppo2.py:144][0m Total time:      36.60 min
[32m[20230113 20:21:08 @agent_ppo2.py:146][0m 3407872 total steps have happened
[32m[20230113 20:21:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1664 --------------------------#
[32m[20230113 20:21:09 @agent_ppo2.py:128][0m Sampling time: 0.51 s by 1 slaves
[32m[20230113 20:21:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:09 @agent_ppo2.py:186][0m |          -0.0041 |           9.6145 |          11.8126 |
[32m[20230113 20:21:09 @agent_ppo2.py:186][0m |          -0.0080 |           6.8588 |          11.8016 |
[32m[20230113 20:21:09 @agent_ppo2.py:186][0m |          -0.0121 |           6.1281 |          11.7878 |
[32m[20230113 20:21:09 @agent_ppo2.py:186][0m |          -0.0093 |           5.7403 |          11.7885 |
[32m[20230113 20:21:09 @agent_ppo2.py:186][0m |          -0.0282 |           5.5824 |          11.7732 |
[32m[20230113 20:21:09 @agent_ppo2.py:186][0m |          -0.0151 |           5.2986 |          11.7619 |
[32m[20230113 20:21:09 @agent_ppo2.py:186][0m |          -0.0122 |           5.0196 |          11.7746 |
[32m[20230113 20:21:09 @agent_ppo2.py:186][0m |          -0.0208 |           4.8556 |          11.7773 |
[32m[20230113 20:21:09 @agent_ppo2.py:186][0m |          -0.0163 |           4.7096 |          11.7770 |
[32m[20230113 20:21:09 @agent_ppo2.py:186][0m |          -0.0207 |           4.5858 |          11.7733 |
[32m[20230113 20:21:09 @agent_ppo2.py:131][0m Policy update time: 0.52 s
[32m[20230113 20:21:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 143.43
[32m[20230113 20:21:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.95
[32m[20230113 20:21:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.66
[32m[20230113 20:21:10 @agent_ppo2.py:144][0m Total time:      36.62 min
[32m[20230113 20:21:10 @agent_ppo2.py:146][0m 3409920 total steps have happened
[32m[20230113 20:21:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1665 --------------------------#
[32m[20230113 20:21:10 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:21:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:10 @agent_ppo2.py:186][0m |           0.0032 |           5.8753 |          12.0078 |
[32m[20230113 20:21:10 @agent_ppo2.py:186][0m |          -0.0044 |           4.7864 |          12.0125 |
[32m[20230113 20:21:10 @agent_ppo2.py:186][0m |          -0.0066 |           4.3672 |          12.0021 |
[32m[20230113 20:21:10 @agent_ppo2.py:186][0m |          -0.0061 |           4.1086 |          11.9998 |
[32m[20230113 20:21:10 @agent_ppo2.py:186][0m |          -0.0100 |           3.9343 |          12.0025 |
[32m[20230113 20:21:11 @agent_ppo2.py:186][0m |          -0.0104 |           3.7920 |          11.9981 |
[32m[20230113 20:21:11 @agent_ppo2.py:186][0m |          -0.0126 |           3.6996 |          11.9917 |
[32m[20230113 20:21:11 @agent_ppo2.py:186][0m |          -0.0109 |           3.5619 |          11.9938 |
[32m[20230113 20:21:11 @agent_ppo2.py:186][0m |          -0.0154 |           3.4862 |          11.9962 |
[32m[20230113 20:21:11 @agent_ppo2.py:186][0m |          -0.0142 |           3.4030 |          11.9970 |
[32m[20230113 20:21:11 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:21:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.80
[32m[20230113 20:21:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.69
[32m[20230113 20:21:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.16
[32m[20230113 20:21:11 @agent_ppo2.py:144][0m Total time:      36.64 min
[32m[20230113 20:21:11 @agent_ppo2.py:146][0m 3411968 total steps have happened
[32m[20230113 20:21:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1666 --------------------------#
[32m[20230113 20:21:11 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:21:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:12 @agent_ppo2.py:186][0m |           0.0006 |           7.1314 |          12.0788 |
[32m[20230113 20:21:12 @agent_ppo2.py:186][0m |          -0.0053 |           5.3379 |          12.0583 |
[32m[20230113 20:21:12 @agent_ppo2.py:186][0m |          -0.0087 |           4.6414 |          12.0558 |
[32m[20230113 20:21:12 @agent_ppo2.py:186][0m |          -0.0103 |           4.2964 |          12.0407 |
[32m[20230113 20:21:12 @agent_ppo2.py:186][0m |          -0.0091 |           4.0225 |          12.0384 |
[32m[20230113 20:21:12 @agent_ppo2.py:186][0m |          -0.0116 |           3.8059 |          12.0442 |
[32m[20230113 20:21:12 @agent_ppo2.py:186][0m |          -0.0132 |           3.6652 |          12.0431 |
[32m[20230113 20:21:12 @agent_ppo2.py:186][0m |          -0.0137 |           3.5040 |          12.0305 |
[32m[20230113 20:21:12 @agent_ppo2.py:186][0m |          -0.0141 |           3.3946 |          12.0319 |
[32m[20230113 20:21:12 @agent_ppo2.py:186][0m |          -0.0147 |           3.2702 |          12.0199 |
[32m[20230113 20:21:12 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:21:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.94
[32m[20230113 20:21:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.95
[32m[20230113 20:21:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.29
[32m[20230113 20:21:12 @agent_ppo2.py:144][0m Total time:      36.66 min
[32m[20230113 20:21:12 @agent_ppo2.py:146][0m 3414016 total steps have happened
[32m[20230113 20:21:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1667 --------------------------#
[32m[20230113 20:21:13 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:13 @agent_ppo2.py:186][0m |          -0.0012 |           7.1018 |          11.7682 |
[32m[20230113 20:21:13 @agent_ppo2.py:186][0m |          -0.0033 |           5.9220 |          11.7373 |
[32m[20230113 20:21:13 @agent_ppo2.py:186][0m |          -0.0082 |           5.2826 |          11.7678 |
[32m[20230113 20:21:13 @agent_ppo2.py:186][0m |          -0.0165 |           4.9015 |          11.7459 |
[32m[20230113 20:21:13 @agent_ppo2.py:186][0m |          -0.0136 |           4.7169 |          11.7471 |
[32m[20230113 20:21:13 @agent_ppo2.py:186][0m |          -0.0121 |           4.5230 |          11.7347 |
[32m[20230113 20:21:13 @agent_ppo2.py:186][0m |          -0.0128 |           4.3817 |          11.7341 |
[32m[20230113 20:21:13 @agent_ppo2.py:186][0m |          -0.0128 |           4.2709 |          11.7412 |
[32m[20230113 20:21:13 @agent_ppo2.py:186][0m |          -0.0066 |           4.5816 |          11.7393 |
[32m[20230113 20:21:13 @agent_ppo2.py:186][0m |          -0.0118 |           4.1129 |          11.7478 |
[32m[20230113 20:21:13 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.11
[32m[20230113 20:21:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.99
[32m[20230113 20:21:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.13
[32m[20230113 20:21:14 @agent_ppo2.py:144][0m Total time:      36.68 min
[32m[20230113 20:21:14 @agent_ppo2.py:146][0m 3416064 total steps have happened
[32m[20230113 20:21:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1668 --------------------------#
[32m[20230113 20:21:14 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:14 @agent_ppo2.py:186][0m |           0.0004 |           9.3772 |          11.9123 |
[32m[20230113 20:21:14 @agent_ppo2.py:186][0m |          -0.0069 |           4.6262 |          11.8975 |
[32m[20230113 20:21:14 @agent_ppo2.py:186][0m |          -0.0082 |           3.6968 |          11.8856 |
[32m[20230113 20:21:14 @agent_ppo2.py:186][0m |          -0.0094 |           3.3517 |          11.8825 |
[32m[20230113 20:21:14 @agent_ppo2.py:186][0m |          -0.0077 |           3.1364 |          11.8764 |
[32m[20230113 20:21:14 @agent_ppo2.py:186][0m |          -0.0217 |           2.9990 |          11.8652 |
[32m[20230113 20:21:14 @agent_ppo2.py:186][0m |          -0.0054 |           2.8746 |          11.8693 |
[32m[20230113 20:21:14 @agent_ppo2.py:186][0m |          -0.0137 |           2.7591 |          11.8757 |
[32m[20230113 20:21:14 @agent_ppo2.py:186][0m |          -0.0058 |           2.7143 |          11.8719 |
[32m[20230113 20:21:14 @agent_ppo2.py:186][0m |          -0.0179 |           2.6210 |          11.8811 |
[32m[20230113 20:21:14 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:21:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.21
[32m[20230113 20:21:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.72
[32m[20230113 20:21:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.82
[32m[20230113 20:21:15 @agent_ppo2.py:144][0m Total time:      36.70 min
[32m[20230113 20:21:15 @agent_ppo2.py:146][0m 3418112 total steps have happened
[32m[20230113 20:21:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1669 --------------------------#
[32m[20230113 20:21:15 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:15 @agent_ppo2.py:186][0m |          -0.0004 |           7.0261 |          11.7741 |
[32m[20230113 20:21:15 @agent_ppo2.py:186][0m |          -0.0047 |           5.4279 |          11.7692 |
[32m[20230113 20:21:15 @agent_ppo2.py:186][0m |          -0.0128 |           4.8804 |          11.7772 |
[32m[20230113 20:21:15 @agent_ppo2.py:186][0m |          -0.0088 |           4.6225 |          11.7678 |
[32m[20230113 20:21:16 @agent_ppo2.py:186][0m |          -0.0156 |           4.3491 |          11.7718 |
[32m[20230113 20:21:16 @agent_ppo2.py:186][0m |          -0.0136 |           4.1914 |          11.7760 |
[32m[20230113 20:21:16 @agent_ppo2.py:186][0m |          -0.0150 |           4.0946 |          11.7786 |
[32m[20230113 20:21:16 @agent_ppo2.py:186][0m |          -0.0159 |           3.9866 |          11.7755 |
[32m[20230113 20:21:16 @agent_ppo2.py:186][0m |          -0.0172 |           3.9778 |          11.7693 |
[32m[20230113 20:21:16 @agent_ppo2.py:186][0m |          -0.0209 |           3.8184 |          11.7823 |
[32m[20230113 20:21:16 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:21:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.92
[32m[20230113 20:21:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.39
[32m[20230113 20:21:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.63
[32m[20230113 20:21:16 @agent_ppo2.py:144][0m Total time:      36.73 min
[32m[20230113 20:21:16 @agent_ppo2.py:146][0m 3420160 total steps have happened
[32m[20230113 20:21:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1670 --------------------------#
[32m[20230113 20:21:17 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:17 @agent_ppo2.py:186][0m |          -0.0101 |           6.1970 |          11.9816 |
[32m[20230113 20:21:17 @agent_ppo2.py:186][0m |          -0.0076 |           4.8668 |          11.9922 |
[32m[20230113 20:21:17 @agent_ppo2.py:186][0m |          -0.0102 |           4.4375 |          11.9941 |
[32m[20230113 20:21:17 @agent_ppo2.py:186][0m |          -0.0099 |           4.1773 |          11.9906 |
[32m[20230113 20:21:17 @agent_ppo2.py:186][0m |          -0.0149 |           3.9256 |          12.0044 |
[32m[20230113 20:21:17 @agent_ppo2.py:186][0m |          -0.0156 |           3.7577 |          12.0021 |
[32m[20230113 20:21:17 @agent_ppo2.py:186][0m |          -0.0130 |           3.6247 |          12.0027 |
[32m[20230113 20:21:17 @agent_ppo2.py:186][0m |          -0.0171 |           3.5269 |          11.9981 |
[32m[20230113 20:21:17 @agent_ppo2.py:186][0m |          -0.0126 |           3.4285 |          11.9854 |
[32m[20230113 20:21:17 @agent_ppo2.py:186][0m |          -0.0191 |           3.3830 |          11.9911 |
[32m[20230113 20:21:17 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.27
[32m[20230113 20:21:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.83
[32m[20230113 20:21:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.85
[32m[20230113 20:21:17 @agent_ppo2.py:144][0m Total time:      36.75 min
[32m[20230113 20:21:17 @agent_ppo2.py:146][0m 3422208 total steps have happened
[32m[20230113 20:21:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1671 --------------------------#
[32m[20230113 20:21:18 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:18 @agent_ppo2.py:186][0m |           0.0015 |           6.8698 |          12.0511 |
[32m[20230113 20:21:18 @agent_ppo2.py:186][0m |          -0.0040 |           5.1399 |          12.0376 |
[32m[20230113 20:21:18 @agent_ppo2.py:186][0m |          -0.0067 |           4.7018 |          12.0294 |
[32m[20230113 20:21:18 @agent_ppo2.py:186][0m |          -0.0065 |           4.4070 |          12.0397 |
[32m[20230113 20:21:18 @agent_ppo2.py:186][0m |          -0.0078 |           4.2117 |          12.0438 |
[32m[20230113 20:21:18 @agent_ppo2.py:186][0m |          -0.0117 |           4.0277 |          12.0431 |
[32m[20230113 20:21:18 @agent_ppo2.py:186][0m |          -0.0094 |           3.8921 |          12.0485 |
[32m[20230113 20:21:18 @agent_ppo2.py:186][0m |          -0.0108 |           3.8082 |          12.0404 |
[32m[20230113 20:21:18 @agent_ppo2.py:186][0m |          -0.0122 |           3.7357 |          12.0481 |
[32m[20230113 20:21:18 @agent_ppo2.py:186][0m |          -0.0134 |           3.6199 |          12.0543 |
[32m[20230113 20:21:18 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:21:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.34
[32m[20230113 20:21:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.28
[32m[20230113 20:21:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.43
[32m[20230113 20:21:19 @agent_ppo2.py:144][0m Total time:      36.77 min
[32m[20230113 20:21:19 @agent_ppo2.py:146][0m 3424256 total steps have happened
[32m[20230113 20:21:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1672 --------------------------#
[32m[20230113 20:21:19 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 20:21:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:19 @agent_ppo2.py:186][0m |          -0.0014 |          18.2307 |          12.3145 |
[32m[20230113 20:21:19 @agent_ppo2.py:186][0m |          -0.0103 |           6.7670 |          12.2953 |
[32m[20230113 20:21:19 @agent_ppo2.py:186][0m |          -0.0137 |           5.3069 |          12.2808 |
[32m[20230113 20:21:19 @agent_ppo2.py:186][0m |          -0.0143 |           4.5074 |          12.2889 |
[32m[20230113 20:21:19 @agent_ppo2.py:186][0m |          -0.0158 |           4.0038 |          12.2695 |
[32m[20230113 20:21:19 @agent_ppo2.py:186][0m |          -0.0165 |           3.7061 |          12.2787 |
[32m[20230113 20:21:19 @agent_ppo2.py:186][0m |          -0.0184 |           3.4965 |          12.2745 |
[32m[20230113 20:21:19 @agent_ppo2.py:186][0m |          -0.0190 |           3.2806 |          12.2692 |
[32m[20230113 20:21:19 @agent_ppo2.py:186][0m |          -0.0186 |           3.1416 |          12.2700 |
[32m[20230113 20:21:19 @agent_ppo2.py:186][0m |          -0.0190 |           2.9961 |          12.2673 |
[32m[20230113 20:21:19 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:21:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 137.37
[32m[20230113 20:21:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.88
[32m[20230113 20:21:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.96
[32m[20230113 20:21:20 @agent_ppo2.py:144][0m Total time:      36.79 min
[32m[20230113 20:21:20 @agent_ppo2.py:146][0m 3426304 total steps have happened
[32m[20230113 20:21:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1673 --------------------------#
[32m[20230113 20:21:20 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:20 @agent_ppo2.py:186][0m |          -0.0014 |           7.1066 |          11.9608 |
[32m[20230113 20:21:20 @agent_ppo2.py:186][0m |          -0.0079 |           5.7883 |          11.9439 |
[32m[20230113 20:21:20 @agent_ppo2.py:186][0m |          -0.0114 |           5.2907 |          11.9438 |
[32m[20230113 20:21:20 @agent_ppo2.py:186][0m |          -0.0109 |           4.9898 |          11.9325 |
[32m[20230113 20:21:20 @agent_ppo2.py:186][0m |          -0.0142 |           4.8193 |          11.9463 |
[32m[20230113 20:21:20 @agent_ppo2.py:186][0m |          -0.0156 |           4.5701 |          11.9371 |
[32m[20230113 20:21:20 @agent_ppo2.py:186][0m |          -0.0148 |           4.4442 |          11.9384 |
[32m[20230113 20:21:20 @agent_ppo2.py:186][0m |          -0.0147 |           4.3710 |          11.9547 |
[32m[20230113 20:21:21 @agent_ppo2.py:186][0m |          -0.0165 |           4.1901 |          11.9445 |
[32m[20230113 20:21:21 @agent_ppo2.py:186][0m |          -0.0166 |           4.1371 |          11.9564 |
[32m[20230113 20:21:21 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.84
[32m[20230113 20:21:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.51
[32m[20230113 20:21:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 150.46
[32m[20230113 20:21:21 @agent_ppo2.py:144][0m Total time:      36.81 min
[32m[20230113 20:21:21 @agent_ppo2.py:146][0m 3428352 total steps have happened
[32m[20230113 20:21:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1674 --------------------------#
[32m[20230113 20:21:21 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:21:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:22 @agent_ppo2.py:186][0m |           0.0000 |           6.4033 |          12.2122 |
[32m[20230113 20:21:22 @agent_ppo2.py:186][0m |          -0.0072 |           4.8265 |          12.2103 |
[32m[20230113 20:21:22 @agent_ppo2.py:186][0m |          -0.0124 |           4.1751 |          12.2044 |
[32m[20230113 20:21:22 @agent_ppo2.py:186][0m |          -0.0110 |           3.8559 |          12.2018 |
[32m[20230113 20:21:22 @agent_ppo2.py:186][0m |          -0.0127 |           3.5616 |          12.2022 |
[32m[20230113 20:21:22 @agent_ppo2.py:186][0m |          -0.0134 |           3.4022 |          12.2111 |
[32m[20230113 20:21:22 @agent_ppo2.py:186][0m |          -0.0147 |           3.2601 |          12.2037 |
[32m[20230113 20:21:22 @agent_ppo2.py:186][0m |          -0.0172 |           3.1172 |          12.2075 |
[32m[20230113 20:21:22 @agent_ppo2.py:186][0m |          -0.0167 |           3.0065 |          12.2062 |
[32m[20230113 20:21:22 @agent_ppo2.py:186][0m |          -0.0182 |           2.9298 |          12.2098 |
[32m[20230113 20:21:22 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.58
[32m[20230113 20:21:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.81
[32m[20230113 20:21:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.76
[32m[20230113 20:21:22 @agent_ppo2.py:144][0m Total time:      36.83 min
[32m[20230113 20:21:22 @agent_ppo2.py:146][0m 3430400 total steps have happened
[32m[20230113 20:21:22 @agent_ppo2.py:122][0m #------------------------ Iteration 1675 --------------------------#
[32m[20230113 20:21:23 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:23 @agent_ppo2.py:186][0m |           0.0126 |           6.8019 |          12.0854 |
[32m[20230113 20:21:23 @agent_ppo2.py:186][0m |          -0.0253 |           6.0788 |          12.0715 |
[32m[20230113 20:21:23 @agent_ppo2.py:186][0m |          -0.0126 |           5.1327 |          12.0361 |
[32m[20230113 20:21:23 @agent_ppo2.py:186][0m |          -0.0244 |           4.7707 |          12.0475 |
[32m[20230113 20:21:23 @agent_ppo2.py:186][0m |           0.0073 |           4.8638 |          12.0519 |
[32m[20230113 20:21:23 @agent_ppo2.py:186][0m |          -0.0132 |           4.6597 |          12.0381 |
[32m[20230113 20:21:23 @agent_ppo2.py:186][0m |          -0.0124 |           4.1825 |          12.0411 |
[32m[20230113 20:21:23 @agent_ppo2.py:186][0m |          -0.0226 |           4.0971 |          12.0517 |
[32m[20230113 20:21:23 @agent_ppo2.py:186][0m |          -0.0253 |           4.0952 |          12.0574 |
[32m[20230113 20:21:23 @agent_ppo2.py:186][0m |          -0.0037 |           3.9537 |          12.0425 |
[32m[20230113 20:21:23 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:21:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.35
[32m[20230113 20:21:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.88
[32m[20230113 20:21:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.70
[32m[20230113 20:21:24 @agent_ppo2.py:144][0m Total time:      36.85 min
[32m[20230113 20:21:24 @agent_ppo2.py:146][0m 3432448 total steps have happened
[32m[20230113 20:21:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1676 --------------------------#
[32m[20230113 20:21:24 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:24 @agent_ppo2.py:186][0m |           0.0002 |           6.7375 |          12.2808 |
[32m[20230113 20:21:24 @agent_ppo2.py:186][0m |          -0.0068 |           5.3536 |          12.2732 |
[32m[20230113 20:21:24 @agent_ppo2.py:186][0m |          -0.0091 |           4.7942 |          12.2591 |
[32m[20230113 20:21:24 @agent_ppo2.py:186][0m |          -0.0084 |           4.6012 |          12.2581 |
[32m[20230113 20:21:24 @agent_ppo2.py:186][0m |          -0.0121 |           4.2307 |          12.2523 |
[32m[20230113 20:21:24 @agent_ppo2.py:186][0m |          -0.0141 |           4.0681 |          12.2490 |
[32m[20230113 20:21:24 @agent_ppo2.py:186][0m |          -0.0149 |           3.8932 |          12.2489 |
[32m[20230113 20:21:24 @agent_ppo2.py:186][0m |          -0.0164 |           3.7440 |          12.2426 |
[32m[20230113 20:21:24 @agent_ppo2.py:186][0m |          -0.0179 |           3.6375 |          12.2417 |
[32m[20230113 20:21:24 @agent_ppo2.py:186][0m |          -0.0176 |           3.5417 |          12.2448 |
[32m[20230113 20:21:24 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.71
[32m[20230113 20:21:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.75
[32m[20230113 20:21:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.03
[32m[20230113 20:21:25 @agent_ppo2.py:144][0m Total time:      36.87 min
[32m[20230113 20:21:25 @agent_ppo2.py:146][0m 3434496 total steps have happened
[32m[20230113 20:21:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1677 --------------------------#
[32m[20230113 20:21:25 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:25 @agent_ppo2.py:186][0m |           0.0115 |           6.4271 |          12.1507 |
[32m[20230113 20:21:25 @agent_ppo2.py:186][0m |           0.0015 |           4.8922 |          12.1541 |
[32m[20230113 20:21:25 @agent_ppo2.py:186][0m |          -0.0138 |           4.4097 |          12.1628 |
[32m[20230113 20:21:25 @agent_ppo2.py:186][0m |          -0.0003 |           4.1828 |          12.1524 |
[32m[20230113 20:21:26 @agent_ppo2.py:186][0m |          -0.0231 |           3.9563 |          12.1556 |
[32m[20230113 20:21:26 @agent_ppo2.py:186][0m |          -0.0135 |           3.6800 |          12.1530 |
[32m[20230113 20:21:26 @agent_ppo2.py:186][0m |          -0.0184 |           3.6033 |          12.1423 |
[32m[20230113 20:21:26 @agent_ppo2.py:186][0m |          -0.0026 |           3.4252 |          12.1426 |
[32m[20230113 20:21:26 @agent_ppo2.py:186][0m |          -0.0224 |           3.3481 |          12.1416 |
[32m[20230113 20:21:26 @agent_ppo2.py:186][0m |          -0.0238 |           3.3728 |          12.1429 |
[32m[20230113 20:21:26 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:21:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.61
[32m[20230113 20:21:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.61
[32m[20230113 20:21:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.86
[32m[20230113 20:21:26 @agent_ppo2.py:144][0m Total time:      36.89 min
[32m[20230113 20:21:26 @agent_ppo2.py:146][0m 3436544 total steps have happened
[32m[20230113 20:21:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1678 --------------------------#
[32m[20230113 20:21:27 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:27 @agent_ppo2.py:186][0m |           0.0016 |           7.4081 |          12.2844 |
[32m[20230113 20:21:27 @agent_ppo2.py:186][0m |          -0.0093 |           5.4967 |          12.2790 |
[32m[20230113 20:21:27 @agent_ppo2.py:186][0m |          -0.0082 |           5.0070 |          12.2595 |
[32m[20230113 20:21:27 @agent_ppo2.py:186][0m |          -0.0101 |           4.6981 |          12.2675 |
[32m[20230113 20:21:27 @agent_ppo2.py:186][0m |          -0.0146 |           4.4638 |          12.2753 |
[32m[20230113 20:21:27 @agent_ppo2.py:186][0m |          -0.0150 |           4.3115 |          12.2631 |
[32m[20230113 20:21:27 @agent_ppo2.py:186][0m |          -0.0141 |           4.1700 |          12.2667 |
[32m[20230113 20:21:27 @agent_ppo2.py:186][0m |          -0.0169 |           4.0239 |          12.2619 |
[32m[20230113 20:21:27 @agent_ppo2.py:186][0m |          -0.0174 |           3.9559 |          12.2682 |
[32m[20230113 20:21:27 @agent_ppo2.py:186][0m |          -0.0148 |           3.9166 |          12.2621 |
[32m[20230113 20:21:27 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:21:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.28
[32m[20230113 20:21:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.43
[32m[20230113 20:21:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.38
[32m[20230113 20:21:27 @agent_ppo2.py:144][0m Total time:      36.91 min
[32m[20230113 20:21:27 @agent_ppo2.py:146][0m 3438592 total steps have happened
[32m[20230113 20:21:27 @agent_ppo2.py:122][0m #------------------------ Iteration 1679 --------------------------#
[32m[20230113 20:21:28 @agent_ppo2.py:128][0m Sampling time: 0.38 s by 1 slaves
[32m[20230113 20:21:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:28 @agent_ppo2.py:186][0m |           0.0026 |          14.8052 |          12.1188 |
[32m[20230113 20:21:28 @agent_ppo2.py:186][0m |          -0.0049 |           7.6933 |          12.1017 |
[32m[20230113 20:21:28 @agent_ppo2.py:186][0m |          -0.0076 |           6.2150 |          12.0890 |
[32m[20230113 20:21:28 @agent_ppo2.py:186][0m |          -0.0110 |           5.3278 |          12.0940 |
[32m[20230113 20:21:28 @agent_ppo2.py:186][0m |          -0.0133 |           4.7677 |          12.0756 |
[32m[20230113 20:21:28 @agent_ppo2.py:186][0m |          -0.0132 |           4.3744 |          12.0860 |
[32m[20230113 20:21:28 @agent_ppo2.py:186][0m |          -0.0149 |           4.0829 |          12.0789 |
[32m[20230113 20:21:28 @agent_ppo2.py:186][0m |          -0.0161 |           3.8203 |          12.0729 |
[32m[20230113 20:21:28 @agent_ppo2.py:186][0m |          -0.0162 |           3.6266 |          12.0743 |
[32m[20230113 20:21:28 @agent_ppo2.py:186][0m |          -0.0170 |           3.4446 |          12.0695 |
[32m[20230113 20:21:28 @agent_ppo2.py:131][0m Policy update time: 0.39 s
[32m[20230113 20:21:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 168.32
[32m[20230113 20:21:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.03
[32m[20230113 20:21:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.07
[32m[20230113 20:21:29 @agent_ppo2.py:144][0m Total time:      36.93 min
[32m[20230113 20:21:29 @agent_ppo2.py:146][0m 3440640 total steps have happened
[32m[20230113 20:21:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1680 --------------------------#
[32m[20230113 20:21:29 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:29 @agent_ppo2.py:186][0m |           0.0006 |           8.1080 |          12.2689 |
[32m[20230113 20:21:29 @agent_ppo2.py:186][0m |          -0.0060 |           5.6438 |          12.2628 |
[32m[20230113 20:21:29 @agent_ppo2.py:186][0m |          -0.0072 |           5.0142 |          12.2570 |
[32m[20230113 20:21:29 @agent_ppo2.py:186][0m |          -0.0104 |           4.6573 |          12.2600 |
[32m[20230113 20:21:29 @agent_ppo2.py:186][0m |          -0.0112 |           4.3739 |          12.2529 |
[32m[20230113 20:21:29 @agent_ppo2.py:186][0m |          -0.0114 |           4.2487 |          12.2574 |
[32m[20230113 20:21:29 @agent_ppo2.py:186][0m |          -0.0115 |           4.0852 |          12.2520 |
[32m[20230113 20:21:29 @agent_ppo2.py:186][0m |          -0.0126 |           3.9478 |          12.2459 |
[32m[20230113 20:21:29 @agent_ppo2.py:186][0m |          -0.0142 |           3.8490 |          12.2563 |
[32m[20230113 20:21:29 @agent_ppo2.py:186][0m |          -0.0152 |           3.7604 |          12.2430 |
[32m[20230113 20:21:29 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.57
[32m[20230113 20:21:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.64
[32m[20230113 20:21:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.24
[32m[20230113 20:21:30 @agent_ppo2.py:144][0m Total time:      36.95 min
[32m[20230113 20:21:30 @agent_ppo2.py:146][0m 3442688 total steps have happened
[32m[20230113 20:21:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1681 --------------------------#
[32m[20230113 20:21:30 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:30 @agent_ppo2.py:186][0m |          -0.0001 |           6.2624 |          12.1862 |
[32m[20230113 20:21:30 @agent_ppo2.py:186][0m |          -0.0089 |           5.1214 |          12.1494 |
[32m[20230113 20:21:30 @agent_ppo2.py:186][0m |          -0.0131 |           4.6966 |          12.1403 |
[32m[20230113 20:21:30 @agent_ppo2.py:186][0m |          -0.0129 |           4.4572 |          12.1403 |
[32m[20230113 20:21:30 @agent_ppo2.py:186][0m |          -0.0147 |           4.2489 |          12.1353 |
[32m[20230113 20:21:30 @agent_ppo2.py:186][0m |          -0.0171 |           4.0797 |          12.1321 |
[32m[20230113 20:21:31 @agent_ppo2.py:186][0m |          -0.0180 |           3.9917 |          12.1276 |
[32m[20230113 20:21:31 @agent_ppo2.py:186][0m |          -0.0192 |           3.8862 |          12.1261 |
[32m[20230113 20:21:31 @agent_ppo2.py:186][0m |          -0.0201 |           3.8058 |          12.1313 |
[32m[20230113 20:21:31 @agent_ppo2.py:186][0m |          -0.0197 |           3.6681 |          12.1288 |
[32m[20230113 20:21:31 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.29
[32m[20230113 20:21:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.56
[32m[20230113 20:21:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.08
[32m[20230113 20:21:31 @agent_ppo2.py:144][0m Total time:      36.97 min
[32m[20230113 20:21:31 @agent_ppo2.py:146][0m 3444736 total steps have happened
[32m[20230113 20:21:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1682 --------------------------#
[32m[20230113 20:21:31 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:32 @agent_ppo2.py:186][0m |          -0.0014 |           7.1736 |          11.9501 |
[32m[20230113 20:21:32 @agent_ppo2.py:186][0m |          -0.0070 |           4.7402 |          11.9362 |
[32m[20230113 20:21:32 @agent_ppo2.py:186][0m |          -0.0099 |           3.9981 |          11.9152 |
[32m[20230113 20:21:32 @agent_ppo2.py:186][0m |          -0.0110 |           3.6441 |          11.9314 |
[32m[20230113 20:21:32 @agent_ppo2.py:186][0m |          -0.0146 |           3.4388 |          11.9278 |
[32m[20230113 20:21:32 @agent_ppo2.py:186][0m |          -0.0148 |           3.2609 |          11.9140 |
[32m[20230113 20:21:32 @agent_ppo2.py:186][0m |          -0.0148 |           3.1222 |          11.9214 |
[32m[20230113 20:21:32 @agent_ppo2.py:186][0m |          -0.0163 |           3.0069 |          11.9178 |
[32m[20230113 20:21:32 @agent_ppo2.py:186][0m |          -0.0173 |           2.9024 |          11.9297 |
[32m[20230113 20:21:32 @agent_ppo2.py:186][0m |          -0.0176 |           2.8370 |          11.9168 |
[32m[20230113 20:21:32 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:21:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.78
[32m[20230113 20:21:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.17
[32m[20230113 20:21:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.69
[32m[20230113 20:21:32 @agent_ppo2.py:144][0m Total time:      37.00 min
[32m[20230113 20:21:32 @agent_ppo2.py:146][0m 3446784 total steps have happened
[32m[20230113 20:21:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1683 --------------------------#
[32m[20230113 20:21:33 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:21:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:33 @agent_ppo2.py:186][0m |           0.0019 |           6.9365 |          12.0443 |
[32m[20230113 20:21:33 @agent_ppo2.py:186][0m |          -0.0035 |           5.5045 |          12.0193 |
[32m[20230113 20:21:33 @agent_ppo2.py:186][0m |          -0.0063 |           4.8616 |          12.0101 |
[32m[20230113 20:21:33 @agent_ppo2.py:186][0m |          -0.0088 |           4.5404 |          12.0082 |
[32m[20230113 20:21:33 @agent_ppo2.py:186][0m |          -0.0061 |           4.3295 |          12.0033 |
[32m[20230113 20:21:33 @agent_ppo2.py:186][0m |          -0.0069 |           4.1522 |          11.9991 |
[32m[20230113 20:21:33 @agent_ppo2.py:186][0m |          -0.0056 |           4.0579 |          11.9962 |
[32m[20230113 20:21:33 @agent_ppo2.py:186][0m |          -0.0116 |           3.8724 |          11.9981 |
[32m[20230113 20:21:33 @agent_ppo2.py:186][0m |          -0.0141 |           3.7776 |          11.9900 |
[32m[20230113 20:21:33 @agent_ppo2.py:186][0m |          -0.0120 |           3.6978 |          11.9961 |
[32m[20230113 20:21:33 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:21:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.21
[32m[20230113 20:21:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.43
[32m[20230113 20:21:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.01
[32m[20230113 20:21:34 @agent_ppo2.py:144][0m Total time:      37.02 min
[32m[20230113 20:21:34 @agent_ppo2.py:146][0m 3448832 total steps have happened
[32m[20230113 20:21:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1684 --------------------------#
[32m[20230113 20:21:34 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:21:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:34 @agent_ppo2.py:186][0m |           0.0121 |          12.9927 |          11.8814 |
[32m[20230113 20:21:34 @agent_ppo2.py:186][0m |          -0.0069 |           9.1699 |          11.8695 |
[32m[20230113 20:21:34 @agent_ppo2.py:186][0m |          -0.0143 |           7.1276 |          11.8487 |
[32m[20230113 20:21:34 @agent_ppo2.py:186][0m |          -0.0098 |           6.3611 |          11.8615 |
[32m[20230113 20:21:34 @agent_ppo2.py:186][0m |          -0.0167 |           5.8853 |          11.8487 |
[32m[20230113 20:21:34 @agent_ppo2.py:186][0m |          -0.0201 |           5.6625 |          11.8542 |
[32m[20230113 20:21:34 @agent_ppo2.py:186][0m |          -0.0080 |           5.4398 |          11.8456 |
[32m[20230113 20:21:34 @agent_ppo2.py:186][0m |          -0.0176 |           5.0736 |          11.8480 |
[32m[20230113 20:21:34 @agent_ppo2.py:186][0m |          -0.0294 |           4.8976 |          11.8534 |
[32m[20230113 20:21:34 @agent_ppo2.py:186][0m |          -0.0105 |           4.7189 |          11.8547 |
[32m[20230113 20:21:34 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:21:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.42
[32m[20230113 20:21:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.07
[32m[20230113 20:21:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.19
[32m[20230113 20:21:35 @agent_ppo2.py:144][0m Total time:      37.04 min
[32m[20230113 20:21:35 @agent_ppo2.py:146][0m 3450880 total steps have happened
[32m[20230113 20:21:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1685 --------------------------#
[32m[20230113 20:21:35 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:35 @agent_ppo2.py:186][0m |          -0.0000 |           6.2177 |          11.9537 |
[32m[20230113 20:21:35 @agent_ppo2.py:186][0m |          -0.0075 |           4.9886 |          11.9491 |
[32m[20230113 20:21:35 @agent_ppo2.py:186][0m |          -0.0114 |           4.4107 |          11.9378 |
[32m[20230113 20:21:35 @agent_ppo2.py:186][0m |          -0.0122 |           4.0988 |          11.9297 |
[32m[20230113 20:21:35 @agent_ppo2.py:186][0m |          -0.0126 |           3.9030 |          11.9301 |
[32m[20230113 20:21:36 @agent_ppo2.py:186][0m |          -0.0168 |           3.7718 |          11.9218 |
[32m[20230113 20:21:36 @agent_ppo2.py:186][0m |          -0.0165 |           3.6564 |          11.9166 |
[32m[20230113 20:21:36 @agent_ppo2.py:186][0m |          -0.0170 |           3.5534 |          11.9125 |
[32m[20230113 20:21:36 @agent_ppo2.py:186][0m |          -0.0151 |           3.4904 |          11.9122 |
[32m[20230113 20:21:36 @agent_ppo2.py:186][0m |          -0.0176 |           3.4096 |          11.9078 |
[32m[20230113 20:21:36 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.06
[32m[20230113 20:21:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.25
[32m[20230113 20:21:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.39
[32m[20230113 20:21:36 @agent_ppo2.py:144][0m Total time:      37.06 min
[32m[20230113 20:21:36 @agent_ppo2.py:146][0m 3452928 total steps have happened
[32m[20230113 20:21:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1686 --------------------------#
[32m[20230113 20:21:37 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:21:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:37 @agent_ppo2.py:186][0m |           0.0015 |           6.6125 |          12.0336 |
[32m[20230113 20:21:37 @agent_ppo2.py:186][0m |          -0.0045 |           5.3129 |          12.0280 |
[32m[20230113 20:21:37 @agent_ppo2.py:186][0m |          -0.0079 |           4.9326 |          12.0313 |
[32m[20230113 20:21:37 @agent_ppo2.py:186][0m |          -0.0115 |           4.6192 |          12.0220 |
[32m[20230113 20:21:37 @agent_ppo2.py:186][0m |          -0.0104 |           4.5037 |          12.0226 |
[32m[20230113 20:21:37 @agent_ppo2.py:186][0m |          -0.0090 |           4.3184 |          12.0238 |
[32m[20230113 20:21:37 @agent_ppo2.py:186][0m |          -0.0144 |           4.1406 |          12.0236 |
[32m[20230113 20:21:37 @agent_ppo2.py:186][0m |          -0.0129 |           4.0505 |          12.0204 |
[32m[20230113 20:21:37 @agent_ppo2.py:186][0m |          -0.0148 |           3.9417 |          12.0154 |
[32m[20230113 20:21:37 @agent_ppo2.py:186][0m |          -0.0100 |           3.8805 |          12.0117 |
[32m[20230113 20:21:37 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:21:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.55
[32m[20230113 20:21:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.35
[32m[20230113 20:21:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.20
[32m[20230113 20:21:37 @agent_ppo2.py:144][0m Total time:      37.08 min
[32m[20230113 20:21:37 @agent_ppo2.py:146][0m 3454976 total steps have happened
[32m[20230113 20:21:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1687 --------------------------#
[32m[20230113 20:21:38 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:38 @agent_ppo2.py:186][0m |           0.0055 |           6.1994 |          12.3215 |
[32m[20230113 20:21:38 @agent_ppo2.py:186][0m |          -0.0064 |           5.0619 |          12.2952 |
[32m[20230113 20:21:38 @agent_ppo2.py:186][0m |          -0.0062 |           4.6260 |          12.2919 |
[32m[20230113 20:21:38 @agent_ppo2.py:186][0m |          -0.0076 |           4.3774 |          12.2983 |
[32m[20230113 20:21:38 @agent_ppo2.py:186][0m |          -0.0166 |           4.2154 |          12.2898 |
[32m[20230113 20:21:38 @agent_ppo2.py:186][0m |          -0.0112 |           4.0658 |          12.2896 |
[32m[20230113 20:21:38 @agent_ppo2.py:186][0m |          -0.0114 |           3.9098 |          12.2936 |
[32m[20230113 20:21:38 @agent_ppo2.py:186][0m |          -0.0162 |           3.7921 |          12.2818 |
[32m[20230113 20:21:38 @agent_ppo2.py:186][0m |          -0.0157 |           3.6953 |          12.2906 |
[32m[20230113 20:21:38 @agent_ppo2.py:186][0m |          -0.0172 |           3.6622 |          12.2867 |
[32m[20230113 20:21:38 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:21:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.84
[32m[20230113 20:21:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.59
[32m[20230113 20:21:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.29
[32m[20230113 20:21:39 @agent_ppo2.py:144][0m Total time:      37.10 min
[32m[20230113 20:21:39 @agent_ppo2.py:146][0m 3457024 total steps have happened
[32m[20230113 20:21:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1688 --------------------------#
[32m[20230113 20:21:39 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:39 @agent_ppo2.py:186][0m |           0.0014 |           7.0365 |          11.8483 |
[32m[20230113 20:21:39 @agent_ppo2.py:186][0m |          -0.0071 |           5.3520 |          11.8319 |
[32m[20230113 20:21:39 @agent_ppo2.py:186][0m |          -0.0039 |           4.8370 |          11.8225 |
[32m[20230113 20:21:39 @agent_ppo2.py:186][0m |          -0.0108 |           4.4407 |          11.8163 |
[32m[20230113 20:21:39 @agent_ppo2.py:186][0m |          -0.0154 |           4.2368 |          11.8162 |
[32m[20230113 20:21:39 @agent_ppo2.py:186][0m |          -0.0115 |           4.0512 |          11.8070 |
[32m[20230113 20:21:39 @agent_ppo2.py:186][0m |          -0.0126 |           3.9049 |          11.8073 |
[32m[20230113 20:21:39 @agent_ppo2.py:186][0m |          -0.0119 |           3.8073 |          11.7938 |
[32m[20230113 20:21:39 @agent_ppo2.py:186][0m |          -0.0176 |           3.6909 |          11.8092 |
[32m[20230113 20:21:40 @agent_ppo2.py:186][0m |          -0.0148 |           3.6335 |          11.7998 |
[32m[20230113 20:21:40 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.08
[32m[20230113 20:21:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.79
[32m[20230113 20:21:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.57
[32m[20230113 20:21:40 @agent_ppo2.py:144][0m Total time:      37.12 min
[32m[20230113 20:21:40 @agent_ppo2.py:146][0m 3459072 total steps have happened
[32m[20230113 20:21:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1689 --------------------------#
[32m[20230113 20:21:40 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:40 @agent_ppo2.py:186][0m |           0.0023 |           7.5276 |          11.7097 |
[32m[20230113 20:21:40 @agent_ppo2.py:186][0m |          -0.0017 |           5.5327 |          11.7077 |
[32m[20230113 20:21:41 @agent_ppo2.py:186][0m |          -0.0065 |           4.6678 |          11.7253 |
[32m[20230113 20:21:41 @agent_ppo2.py:186][0m |          -0.0075 |           4.2092 |          11.7084 |
[32m[20230113 20:21:41 @agent_ppo2.py:186][0m |          -0.0095 |           3.9782 |          11.7063 |
[32m[20230113 20:21:41 @agent_ppo2.py:186][0m |          -0.0138 |           3.7846 |          11.7224 |
[32m[20230113 20:21:41 @agent_ppo2.py:186][0m |          -0.0136 |           3.6728 |          11.7122 |
[32m[20230113 20:21:41 @agent_ppo2.py:186][0m |          -0.0112 |           3.5363 |          11.7219 |
[32m[20230113 20:21:41 @agent_ppo2.py:186][0m |          -0.0143 |           3.4409 |          11.7184 |
[32m[20230113 20:21:41 @agent_ppo2.py:186][0m |          -0.0148 |           3.3615 |          11.7169 |
[32m[20230113 20:21:41 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.62
[32m[20230113 20:21:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.21
[32m[20230113 20:21:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.76
[32m[20230113 20:21:41 @agent_ppo2.py:144][0m Total time:      37.14 min
[32m[20230113 20:21:41 @agent_ppo2.py:146][0m 3461120 total steps have happened
[32m[20230113 20:21:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1690 --------------------------#
[32m[20230113 20:21:42 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:42 @agent_ppo2.py:186][0m |          -0.0033 |           6.6357 |          11.9448 |
[32m[20230113 20:21:42 @agent_ppo2.py:186][0m |          -0.0065 |           4.4428 |          11.9321 |
[32m[20230113 20:21:42 @agent_ppo2.py:186][0m |          -0.0058 |           3.8994 |          11.9502 |
[32m[20230113 20:21:42 @agent_ppo2.py:186][0m |          -0.0064 |           3.6491 |          11.9378 |
[32m[20230113 20:21:42 @agent_ppo2.py:186][0m |          -0.0121 |           3.4556 |          11.9402 |
[32m[20230113 20:21:42 @agent_ppo2.py:186][0m |          -0.0172 |           3.2744 |          11.9481 |
[32m[20230113 20:21:42 @agent_ppo2.py:186][0m |          -0.0158 |           3.1291 |          11.9385 |
[32m[20230113 20:21:42 @agent_ppo2.py:186][0m |          -0.0165 |           3.0658 |          11.9429 |
[32m[20230113 20:21:42 @agent_ppo2.py:186][0m |          -0.0161 |           2.9725 |          11.9447 |
[32m[20230113 20:21:42 @agent_ppo2.py:186][0m |          -0.0172 |           2.8769 |          11.9481 |
[32m[20230113 20:21:42 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.48
[32m[20230113 20:21:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.65
[32m[20230113 20:21:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.38
[32m[20230113 20:21:42 @agent_ppo2.py:144][0m Total time:      37.16 min
[32m[20230113 20:21:42 @agent_ppo2.py:146][0m 3463168 total steps have happened
[32m[20230113 20:21:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1691 --------------------------#
[32m[20230113 20:21:43 @agent_ppo2.py:128][0m Sampling time: 0.36 s by 1 slaves
[32m[20230113 20:21:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:43 @agent_ppo2.py:186][0m |          -0.0020 |          12.3414 |          12.0080 |
[32m[20230113 20:21:43 @agent_ppo2.py:186][0m |          -0.0100 |           6.3044 |          12.0040 |
[32m[20230113 20:21:43 @agent_ppo2.py:186][0m |          -0.0097 |           5.0675 |          11.9943 |
[32m[20230113 20:21:43 @agent_ppo2.py:186][0m |          -0.0159 |           4.5914 |          11.9829 |
[32m[20230113 20:21:43 @agent_ppo2.py:186][0m |          -0.0164 |           4.2252 |          11.9685 |
[32m[20230113 20:21:43 @agent_ppo2.py:186][0m |          -0.0153 |           3.9902 |          11.9722 |
[32m[20230113 20:21:43 @agent_ppo2.py:186][0m |          -0.0189 |           3.8708 |          11.9763 |
[32m[20230113 20:21:43 @agent_ppo2.py:186][0m |          -0.0222 |           3.6356 |          11.9793 |
[32m[20230113 20:21:43 @agent_ppo2.py:186][0m |          -0.0177 |           3.5772 |          11.9756 |
[32m[20230113 20:21:43 @agent_ppo2.py:186][0m |          -0.0216 |           3.3864 |          11.9808 |
[32m[20230113 20:21:43 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:21:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 140.11
[32m[20230113 20:21:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.46
[32m[20230113 20:21:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.58
[32m[20230113 20:21:44 @agent_ppo2.py:144][0m Total time:      37.18 min
[32m[20230113 20:21:44 @agent_ppo2.py:146][0m 3465216 total steps have happened
[32m[20230113 20:21:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1692 --------------------------#
[32m[20230113 20:21:44 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:21:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:44 @agent_ppo2.py:186][0m |           0.0045 |           6.5573 |          12.0592 |
[32m[20230113 20:21:44 @agent_ppo2.py:186][0m |          -0.0050 |           5.1592 |          12.0655 |
[32m[20230113 20:21:44 @agent_ppo2.py:186][0m |          -0.0116 |           4.7497 |          12.0586 |
[32m[20230113 20:21:44 @agent_ppo2.py:186][0m |          -0.0116 |           4.4569 |          12.0607 |
[32m[20230113 20:21:44 @agent_ppo2.py:186][0m |          -0.0129 |           4.1752 |          12.0653 |
[32m[20230113 20:21:44 @agent_ppo2.py:186][0m |          -0.0118 |           4.0935 |          12.0546 |
[32m[20230113 20:21:44 @agent_ppo2.py:186][0m |          -0.0145 |           3.8969 |          12.0410 |
[32m[20230113 20:21:44 @agent_ppo2.py:186][0m |          -0.0154 |           3.8519 |          12.0669 |
[32m[20230113 20:21:44 @agent_ppo2.py:186][0m |          -0.0150 |           3.7113 |          12.0555 |
[32m[20230113 20:21:44 @agent_ppo2.py:186][0m |          -0.0164 |           3.5795 |          12.0607 |
[32m[20230113 20:21:44 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:21:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.16
[32m[20230113 20:21:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.43
[32m[20230113 20:21:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 160.66
[32m[20230113 20:21:45 @agent_ppo2.py:144][0m Total time:      37.20 min
[32m[20230113 20:21:45 @agent_ppo2.py:146][0m 3467264 total steps have happened
[32m[20230113 20:21:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1693 --------------------------#
[32m[20230113 20:21:45 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:45 @agent_ppo2.py:186][0m |           0.0010 |           5.5936 |          11.9028 |
[32m[20230113 20:21:45 @agent_ppo2.py:186][0m |          -0.0064 |           4.5270 |          11.8857 |
[32m[20230113 20:21:45 @agent_ppo2.py:186][0m |          -0.0100 |           4.2186 |          11.8769 |
[32m[20230113 20:21:45 @agent_ppo2.py:186][0m |          -0.0113 |           3.9592 |          11.8733 |
[32m[20230113 20:21:45 @agent_ppo2.py:186][0m |          -0.0129 |           3.8091 |          11.8655 |
[32m[20230113 20:21:45 @agent_ppo2.py:186][0m |          -0.0140 |           3.6522 |          11.8764 |
[32m[20230113 20:21:46 @agent_ppo2.py:186][0m |          -0.0142 |           3.5845 |          11.8566 |
[32m[20230113 20:21:46 @agent_ppo2.py:186][0m |          -0.0151 |           3.4964 |          11.8743 |
[32m[20230113 20:21:46 @agent_ppo2.py:186][0m |          -0.0172 |           3.4107 |          11.8677 |
[32m[20230113 20:21:46 @agent_ppo2.py:186][0m |          -0.0185 |           3.3389 |          11.8719 |
[32m[20230113 20:21:46 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.33
[32m[20230113 20:21:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.61
[32m[20230113 20:21:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.23
[32m[20230113 20:21:46 @agent_ppo2.py:144][0m Total time:      37.22 min
[32m[20230113 20:21:46 @agent_ppo2.py:146][0m 3469312 total steps have happened
[32m[20230113 20:21:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1694 --------------------------#
[32m[20230113 20:21:46 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:47 @agent_ppo2.py:186][0m |           0.0008 |          12.9587 |          12.2228 |
[32m[20230113 20:21:47 @agent_ppo2.py:186][0m |          -0.0073 |           7.8596 |          12.2211 |
[32m[20230113 20:21:47 @agent_ppo2.py:186][0m |          -0.0104 |           7.0752 |          12.2121 |
[32m[20230113 20:21:47 @agent_ppo2.py:186][0m |          -0.0121 |           6.2552 |          12.2103 |
[32m[20230113 20:21:47 @agent_ppo2.py:186][0m |          -0.0158 |           5.9983 |          12.2116 |
[32m[20230113 20:21:47 @agent_ppo2.py:186][0m |          -0.0159 |           5.6465 |          12.2131 |
[32m[20230113 20:21:47 @agent_ppo2.py:186][0m |          -0.0164 |           5.3012 |          12.2034 |
[32m[20230113 20:21:47 @agent_ppo2.py:186][0m |          -0.0171 |           5.1268 |          12.2112 |
[32m[20230113 20:21:47 @agent_ppo2.py:186][0m |          -0.0179 |           4.9987 |          12.2128 |
[32m[20230113 20:21:47 @agent_ppo2.py:186][0m |          -0.0178 |           4.8986 |          12.2103 |
[32m[20230113 20:21:47 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:21:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 118.12
[32m[20230113 20:21:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.51
[32m[20230113 20:21:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.09
[32m[20230113 20:21:47 @agent_ppo2.py:144][0m Total time:      37.25 min
[32m[20230113 20:21:47 @agent_ppo2.py:146][0m 3471360 total steps have happened
[32m[20230113 20:21:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1695 --------------------------#
[32m[20230113 20:21:48 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:48 @agent_ppo2.py:186][0m |          -0.0024 |           6.9406 |          12.6188 |
[32m[20230113 20:21:48 @agent_ppo2.py:186][0m |          -0.0087 |           5.2512 |          12.5986 |
[32m[20230113 20:21:48 @agent_ppo2.py:186][0m |          -0.0119 |           4.9383 |          12.5884 |
[32m[20230113 20:21:48 @agent_ppo2.py:186][0m |          -0.0151 |           4.5164 |          12.5944 |
[32m[20230113 20:21:48 @agent_ppo2.py:186][0m |          -0.0161 |           4.3476 |          12.6004 |
[32m[20230113 20:21:48 @agent_ppo2.py:186][0m |          -0.0163 |           4.1737 |          12.5954 |
[32m[20230113 20:21:48 @agent_ppo2.py:186][0m |          -0.0172 |           4.0624 |          12.5896 |
[32m[20230113 20:21:48 @agent_ppo2.py:186][0m |          -0.0182 |           3.9775 |          12.6001 |
[32m[20230113 20:21:48 @agent_ppo2.py:186][0m |          -0.0184 |           3.9136 |          12.5833 |
[32m[20230113 20:21:48 @agent_ppo2.py:186][0m |          -0.0200 |           3.7799 |          12.5958 |
[32m[20230113 20:21:48 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.28
[32m[20230113 20:21:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.89
[32m[20230113 20:21:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.02
[32m[20230113 20:21:49 @agent_ppo2.py:144][0m Total time:      37.27 min
[32m[20230113 20:21:49 @agent_ppo2.py:146][0m 3473408 total steps have happened
[32m[20230113 20:21:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1696 --------------------------#
[32m[20230113 20:21:49 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:49 @agent_ppo2.py:186][0m |           0.0004 |           8.4453 |          11.9107 |
[32m[20230113 20:21:49 @agent_ppo2.py:186][0m |           0.0006 |           6.3671 |          11.8947 |
[32m[20230113 20:21:49 @agent_ppo2.py:186][0m |          -0.0016 |           5.5836 |          11.8941 |
[32m[20230113 20:21:49 @agent_ppo2.py:186][0m |           0.0038 |           5.2887 |          11.8832 |
[32m[20230113 20:21:49 @agent_ppo2.py:186][0m |          -0.0127 |           4.8083 |          11.8862 |
[32m[20230113 20:21:49 @agent_ppo2.py:186][0m |          -0.0054 |           4.5445 |          11.8757 |
[32m[20230113 20:21:49 @agent_ppo2.py:186][0m |          -0.0150 |           4.5417 |          11.8815 |
[32m[20230113 20:21:49 @agent_ppo2.py:186][0m |          -0.0111 |           4.2563 |          11.8685 |
[32m[20230113 20:21:49 @agent_ppo2.py:186][0m |          -0.0151 |           4.1729 |          11.8735 |
[32m[20230113 20:21:49 @agent_ppo2.py:186][0m |          -0.0125 |           3.9571 |          11.8733 |
[32m[20230113 20:21:49 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.67
[32m[20230113 20:21:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.26
[32m[20230113 20:21:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.99
[32m[20230113 20:21:50 @agent_ppo2.py:144][0m Total time:      37.29 min
[32m[20230113 20:21:50 @agent_ppo2.py:146][0m 3475456 total steps have happened
[32m[20230113 20:21:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1697 --------------------------#
[32m[20230113 20:21:50 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:21:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:50 @agent_ppo2.py:186][0m |           0.0056 |           6.6766 |          12.2727 |
[32m[20230113 20:21:50 @agent_ppo2.py:186][0m |          -0.0028 |           5.4578 |          12.2433 |
[32m[20230113 20:21:50 @agent_ppo2.py:186][0m |          -0.0083 |           4.5889 |          12.2484 |
[32m[20230113 20:21:50 @agent_ppo2.py:186][0m |          -0.0071 |           4.1634 |          12.2626 |
[32m[20230113 20:21:51 @agent_ppo2.py:186][0m |          -0.0116 |           3.8418 |          12.2565 |
[32m[20230113 20:21:51 @agent_ppo2.py:186][0m |          -0.0135 |           3.6698 |          12.2526 |
[32m[20230113 20:21:51 @agent_ppo2.py:186][0m |          -0.0137 |           3.4660 |          12.2568 |
[32m[20230113 20:21:51 @agent_ppo2.py:186][0m |          -0.0131 |           3.3654 |          12.2552 |
[32m[20230113 20:21:51 @agent_ppo2.py:186][0m |          -0.0139 |           3.2639 |          12.2537 |
[32m[20230113 20:21:51 @agent_ppo2.py:186][0m |          -0.0161 |           3.1939 |          12.2568 |
[32m[20230113 20:21:51 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:21:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.54
[32m[20230113 20:21:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.13
[32m[20230113 20:21:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.82
[32m[20230113 20:21:51 @agent_ppo2.py:144][0m Total time:      37.31 min
[32m[20230113 20:21:51 @agent_ppo2.py:146][0m 3477504 total steps have happened
[32m[20230113 20:21:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1698 --------------------------#
[32m[20230113 20:21:52 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:52 @agent_ppo2.py:186][0m |          -0.0017 |           6.3438 |          12.2356 |
[32m[20230113 20:21:52 @agent_ppo2.py:186][0m |          -0.0026 |           5.0673 |          12.2165 |
[32m[20230113 20:21:52 @agent_ppo2.py:186][0m |          -0.0084 |           4.4821 |          12.2061 |
[32m[20230113 20:21:52 @agent_ppo2.py:186][0m |          -0.0112 |           4.1249 |          12.2093 |
[32m[20230113 20:21:52 @agent_ppo2.py:186][0m |          -0.0110 |           3.9469 |          12.2144 |
[32m[20230113 20:21:52 @agent_ppo2.py:186][0m |          -0.0117 |           3.8316 |          12.2067 |
[32m[20230113 20:21:52 @agent_ppo2.py:186][0m |          -0.0165 |           3.6491 |          12.2027 |
[32m[20230113 20:21:52 @agent_ppo2.py:186][0m |          -0.0167 |           3.5647 |          12.2044 |
[32m[20230113 20:21:52 @agent_ppo2.py:186][0m |          -0.0170 |           3.4615 |          12.1996 |
[32m[20230113 20:21:52 @agent_ppo2.py:186][0m |          -0.0152 |           3.3349 |          12.1908 |
[32m[20230113 20:21:52 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.50
[32m[20230113 20:21:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.13
[32m[20230113 20:21:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.94
[32m[20230113 20:21:52 @agent_ppo2.py:144][0m Total time:      37.33 min
[32m[20230113 20:21:52 @agent_ppo2.py:146][0m 3479552 total steps have happened
[32m[20230113 20:21:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1699 --------------------------#
[32m[20230113 20:21:53 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:21:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:53 @agent_ppo2.py:186][0m |           0.0002 |           7.0947 |          12.2927 |
[32m[20230113 20:21:53 @agent_ppo2.py:186][0m |          -0.0135 |           5.1853 |          12.2877 |
[32m[20230113 20:21:53 @agent_ppo2.py:186][0m |          -0.0043 |           4.5517 |          12.2864 |
[32m[20230113 20:21:53 @agent_ppo2.py:186][0m |          -0.0111 |           4.1218 |          12.2841 |
[32m[20230113 20:21:53 @agent_ppo2.py:186][0m |          -0.0096 |           3.7937 |          12.2856 |
[32m[20230113 20:21:53 @agent_ppo2.py:186][0m |          -0.0096 |           3.5592 |          12.2796 |
[32m[20230113 20:21:53 @agent_ppo2.py:186][0m |          -0.0128 |           3.4162 |          12.2862 |
[32m[20230113 20:21:53 @agent_ppo2.py:186][0m |          -0.0165 |           3.2577 |          12.2793 |
[32m[20230113 20:21:53 @agent_ppo2.py:186][0m |          -0.0151 |           3.1532 |          12.2853 |
[32m[20230113 20:21:53 @agent_ppo2.py:186][0m |          -0.0166 |           3.0312 |          12.2718 |
[32m[20230113 20:21:53 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.38
[32m[20230113 20:21:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.13
[32m[20230113 20:21:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.72
[32m[20230113 20:21:54 @agent_ppo2.py:144][0m Total time:      37.35 min
[32m[20230113 20:21:54 @agent_ppo2.py:146][0m 3481600 total steps have happened
[32m[20230113 20:21:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1700 --------------------------#
[32m[20230113 20:21:54 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:21:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:54 @agent_ppo2.py:186][0m |           0.0024 |           6.4824 |          12.4702 |
[32m[20230113 20:21:54 @agent_ppo2.py:186][0m |          -0.0036 |           4.5552 |          12.4542 |
[32m[20230113 20:21:54 @agent_ppo2.py:186][0m |          -0.0069 |           4.0980 |          12.4374 |
[32m[20230113 20:21:54 @agent_ppo2.py:186][0m |          -0.0095 |           3.7078 |          12.4379 |
[32m[20230113 20:21:54 @agent_ppo2.py:186][0m |          -0.0101 |           3.4925 |          12.4403 |
[32m[20230113 20:21:54 @agent_ppo2.py:186][0m |          -0.0105 |           3.3287 |          12.4356 |
[32m[20230113 20:21:54 @agent_ppo2.py:186][0m |          -0.0112 |           3.2255 |          12.4322 |
[32m[20230113 20:21:54 @agent_ppo2.py:186][0m |          -0.0134 |           3.1267 |          12.4323 |
[32m[20230113 20:21:54 @agent_ppo2.py:186][0m |          -0.0122 |           3.0695 |          12.4264 |
[32m[20230113 20:21:54 @agent_ppo2.py:186][0m |          -0.0136 |           2.9909 |          12.4345 |
[32m[20230113 20:21:54 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:21:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.39
[32m[20230113 20:21:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.35
[32m[20230113 20:21:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.98
[32m[20230113 20:21:55 @agent_ppo2.py:144][0m Total time:      37.37 min
[32m[20230113 20:21:55 @agent_ppo2.py:146][0m 3483648 total steps have happened
[32m[20230113 20:21:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1701 --------------------------#
[32m[20230113 20:21:55 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:55 @agent_ppo2.py:186][0m |          -0.0024 |           6.6394 |          12.0330 |
[32m[20230113 20:21:55 @agent_ppo2.py:186][0m |          -0.0137 |           5.1941 |          12.0299 |
[32m[20230113 20:21:55 @agent_ppo2.py:186][0m |          -0.0189 |           4.8675 |          12.0101 |
[32m[20230113 20:21:55 @agent_ppo2.py:186][0m |          -0.0118 |           4.4124 |          12.0172 |
[32m[20230113 20:21:56 @agent_ppo2.py:186][0m |          -0.0155 |           4.2175 |          12.0169 |
[32m[20230113 20:21:56 @agent_ppo2.py:186][0m |          -0.0116 |           3.9474 |          12.0060 |
[32m[20230113 20:21:56 @agent_ppo2.py:186][0m |          -0.0131 |           3.8126 |          12.0084 |
[32m[20230113 20:21:56 @agent_ppo2.py:186][0m |          -0.0046 |           3.7018 |          11.9985 |
[32m[20230113 20:21:56 @agent_ppo2.py:186][0m |          -0.0185 |           3.5926 |          12.0037 |
[32m[20230113 20:21:56 @agent_ppo2.py:186][0m |          -0.0116 |           3.4943 |          11.9951 |
[32m[20230113 20:21:56 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.42
[32m[20230113 20:21:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.44
[32m[20230113 20:21:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.86
[32m[20230113 20:21:56 @agent_ppo2.py:144][0m Total time:      37.39 min
[32m[20230113 20:21:56 @agent_ppo2.py:146][0m 3485696 total steps have happened
[32m[20230113 20:21:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1702 --------------------------#
[32m[20230113 20:21:57 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:57 @agent_ppo2.py:186][0m |          -0.0058 |           5.8684 |          12.4425 |
[32m[20230113 20:21:57 @agent_ppo2.py:186][0m |          -0.0119 |           4.4803 |          12.4084 |
[32m[20230113 20:21:57 @agent_ppo2.py:186][0m |          -0.0147 |           3.9272 |          12.4101 |
[32m[20230113 20:21:57 @agent_ppo2.py:186][0m |          -0.0146 |           3.5891 |          12.3924 |
[32m[20230113 20:21:57 @agent_ppo2.py:186][0m |          -0.0177 |           3.3667 |          12.4052 |
[32m[20230113 20:21:57 @agent_ppo2.py:186][0m |          -0.0181 |           3.2519 |          12.4050 |
[32m[20230113 20:21:57 @agent_ppo2.py:186][0m |          -0.0210 |           3.0917 |          12.3895 |
[32m[20230113 20:21:57 @agent_ppo2.py:186][0m |          -0.0287 |           3.0537 |          12.3999 |
[32m[20230113 20:21:57 @agent_ppo2.py:186][0m |          -0.0153 |           2.9285 |          12.4032 |
[32m[20230113 20:21:57 @agent_ppo2.py:186][0m |          -0.0119 |           2.8781 |          12.3977 |
[32m[20230113 20:21:57 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:21:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.81
[32m[20230113 20:21:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.28
[32m[20230113 20:21:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.17
[32m[20230113 20:21:57 @agent_ppo2.py:144][0m Total time:      37.41 min
[32m[20230113 20:21:57 @agent_ppo2.py:146][0m 3487744 total steps have happened
[32m[20230113 20:21:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1703 --------------------------#
[32m[20230113 20:21:58 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:58 @agent_ppo2.py:186][0m |          -0.0000 |           6.9012 |          12.1699 |
[32m[20230113 20:21:58 @agent_ppo2.py:186][0m |          -0.0105 |           5.3136 |          12.1769 |
[32m[20230113 20:21:58 @agent_ppo2.py:186][0m |          -0.0108 |           4.6914 |          12.1801 |
[32m[20230113 20:21:58 @agent_ppo2.py:186][0m |          -0.0071 |           4.5247 |          12.1762 |
[32m[20230113 20:21:58 @agent_ppo2.py:186][0m |          -0.0172 |           4.1443 |          12.1675 |
[32m[20230113 20:21:58 @agent_ppo2.py:186][0m |          -0.0157 |           3.8739 |          12.1655 |
[32m[20230113 20:21:58 @agent_ppo2.py:186][0m |          -0.0149 |           3.7577 |          12.1695 |
[32m[20230113 20:21:58 @agent_ppo2.py:186][0m |          -0.0174 |           3.5548 |          12.1698 |
[32m[20230113 20:21:58 @agent_ppo2.py:186][0m |          -0.0200 |           3.4323 |          12.1707 |
[32m[20230113 20:21:58 @agent_ppo2.py:186][0m |          -0.0187 |           3.3524 |          12.1602 |
[32m[20230113 20:21:58 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:21:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.05
[32m[20230113 20:21:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.22
[32m[20230113 20:21:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.29
[32m[20230113 20:21:59 @agent_ppo2.py:144][0m Total time:      37.43 min
[32m[20230113 20:21:59 @agent_ppo2.py:146][0m 3489792 total steps have happened
[32m[20230113 20:21:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1704 --------------------------#
[32m[20230113 20:21:59 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:21:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:21:59 @agent_ppo2.py:186][0m |           0.0021 |           6.6266 |          12.4850 |
[32m[20230113 20:21:59 @agent_ppo2.py:186][0m |          -0.0049 |           4.7800 |          12.4700 |
[32m[20230113 20:21:59 @agent_ppo2.py:186][0m |          -0.0052 |           4.1576 |          12.4718 |
[32m[20230113 20:21:59 @agent_ppo2.py:186][0m |          -0.0096 |           3.8632 |          12.4600 |
[32m[20230113 20:21:59 @agent_ppo2.py:186][0m |          -0.0102 |           3.5686 |          12.4559 |
[32m[20230113 20:21:59 @agent_ppo2.py:186][0m |          -0.0132 |           3.3349 |          12.4632 |
[32m[20230113 20:21:59 @agent_ppo2.py:186][0m |          -0.0114 |           3.2061 |          12.4655 |
[32m[20230113 20:21:59 @agent_ppo2.py:186][0m |          -0.0148 |           3.0856 |          12.4579 |
[32m[20230113 20:21:59 @agent_ppo2.py:186][0m |          -0.0167 |           2.9764 |          12.4599 |
[32m[20230113 20:21:59 @agent_ppo2.py:186][0m |          -0.0142 |           2.9016 |          12.4600 |
[32m[20230113 20:21:59 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.78
[32m[20230113 20:22:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.11
[32m[20230113 20:22:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 234.87
[32m[20230113 20:22:00 @agent_ppo2.py:144][0m Total time:      37.46 min
[32m[20230113 20:22:00 @agent_ppo2.py:146][0m 3491840 total steps have happened
[32m[20230113 20:22:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1705 --------------------------#
[32m[20230113 20:22:00 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:00 @agent_ppo2.py:186][0m |          -0.0013 |           5.5887 |          12.4838 |
[32m[20230113 20:22:00 @agent_ppo2.py:186][0m |          -0.0097 |           4.2302 |          12.4711 |
[32m[20230113 20:22:00 @agent_ppo2.py:186][0m |          -0.0111 |           3.7267 |          12.4662 |
[32m[20230113 20:22:01 @agent_ppo2.py:186][0m |          -0.0105 |           3.3457 |          12.4558 |
[32m[20230113 20:22:01 @agent_ppo2.py:186][0m |          -0.0106 |           3.1401 |          12.4735 |
[32m[20230113 20:22:01 @agent_ppo2.py:186][0m |          -0.0146 |           2.9616 |          12.4664 |
[32m[20230113 20:22:01 @agent_ppo2.py:186][0m |          -0.0117 |           2.8527 |          12.4531 |
[32m[20230113 20:22:01 @agent_ppo2.py:186][0m |          -0.0200 |           2.7631 |          12.4491 |
[32m[20230113 20:22:01 @agent_ppo2.py:186][0m |          -0.0192 |           2.6771 |          12.4551 |
[32m[20230113 20:22:01 @agent_ppo2.py:186][0m |          -0.0221 |           2.6076 |          12.4546 |
[32m[20230113 20:22:01 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.66
[32m[20230113 20:22:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.09
[32m[20230113 20:22:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.69
[32m[20230113 20:22:01 @agent_ppo2.py:144][0m Total time:      37.48 min
[32m[20230113 20:22:01 @agent_ppo2.py:146][0m 3493888 total steps have happened
[32m[20230113 20:22:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1706 --------------------------#
[32m[20230113 20:22:02 @agent_ppo2.py:128][0m Sampling time: 0.48 s by 1 slaves
[32m[20230113 20:22:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:02 @agent_ppo2.py:186][0m |           0.0014 |          14.2402 |          12.4015 |
[32m[20230113 20:22:02 @agent_ppo2.py:186][0m |           0.0024 |           7.8223 |          12.4381 |
[32m[20230113 20:22:02 @agent_ppo2.py:186][0m |          -0.0065 |           6.5328 |          12.4189 |
[32m[20230113 20:22:02 @agent_ppo2.py:186][0m |          -0.0093 |           6.1980 |          12.4173 |
[32m[20230113 20:22:02 @agent_ppo2.py:186][0m |          -0.0106 |           5.4839 |          12.4120 |
[32m[20230113 20:22:02 @agent_ppo2.py:186][0m |          -0.0137 |           5.2849 |          12.4108 |
[32m[20230113 20:22:02 @agent_ppo2.py:186][0m |          -0.0110 |           4.8895 |          12.4092 |
[32m[20230113 20:22:02 @agent_ppo2.py:186][0m |          -0.0141 |           4.7162 |          12.4127 |
[32m[20230113 20:22:02 @agent_ppo2.py:186][0m |          -0.0149 |           4.5236 |          12.4156 |
[32m[20230113 20:22:02 @agent_ppo2.py:186][0m |          -0.0191 |           4.3408 |          12.4097 |
[32m[20230113 20:22:02 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 20:22:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 133.28
[32m[20230113 20:22:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.34
[32m[20230113 20:22:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.61
[32m[20230113 20:22:03 @agent_ppo2.py:144][0m Total time:      37.50 min
[32m[20230113 20:22:03 @agent_ppo2.py:146][0m 3495936 total steps have happened
[32m[20230113 20:22:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1707 --------------------------#
[32m[20230113 20:22:03 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:22:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:03 @agent_ppo2.py:186][0m |           0.0016 |           9.5208 |          12.5913 |
[32m[20230113 20:22:03 @agent_ppo2.py:186][0m |          -0.0076 |           6.8868 |          12.5778 |
[32m[20230113 20:22:03 @agent_ppo2.py:186][0m |          -0.0108 |           6.1890 |          12.5704 |
[32m[20230113 20:22:03 @agent_ppo2.py:186][0m |          -0.0117 |           5.6920 |          12.5713 |
[32m[20230113 20:22:03 @agent_ppo2.py:186][0m |          -0.0163 |           5.3544 |          12.5804 |
[32m[20230113 20:22:03 @agent_ppo2.py:186][0m |          -0.0137 |           5.1194 |          12.5657 |
[32m[20230113 20:22:03 @agent_ppo2.py:186][0m |          -0.0168 |           4.9224 |          12.5781 |
[32m[20230113 20:22:03 @agent_ppo2.py:186][0m |          -0.0165 |           4.7593 |          12.5758 |
[32m[20230113 20:22:03 @agent_ppo2.py:186][0m |          -0.0164 |           4.6995 |          12.5754 |
[32m[20230113 20:22:03 @agent_ppo2.py:186][0m |          -0.0181 |           4.5389 |          12.5698 |
[32m[20230113 20:22:03 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:22:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.83
[32m[20230113 20:22:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.86
[32m[20230113 20:22:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.62
[32m[20230113 20:22:04 @agent_ppo2.py:144][0m Total time:      37.52 min
[32m[20230113 20:22:04 @agent_ppo2.py:146][0m 3497984 total steps have happened
[32m[20230113 20:22:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1708 --------------------------#
[32m[20230113 20:22:04 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:04 @agent_ppo2.py:186][0m |           0.0064 |           8.0107 |          12.4419 |
[32m[20230113 20:22:04 @agent_ppo2.py:186][0m |          -0.0076 |           6.1654 |          12.4387 |
[32m[20230113 20:22:04 @agent_ppo2.py:186][0m |          -0.0092 |           5.5399 |          12.4334 |
[32m[20230113 20:22:04 @agent_ppo2.py:186][0m |          -0.0108 |           5.2060 |          12.4337 |
[32m[20230113 20:22:04 @agent_ppo2.py:186][0m |          -0.0113 |           4.8699 |          12.4299 |
[32m[20230113 20:22:04 @agent_ppo2.py:186][0m |          -0.0117 |           4.6606 |          12.4207 |
[32m[20230113 20:22:04 @agent_ppo2.py:186][0m |          -0.0131 |           4.5142 |          12.4303 |
[32m[20230113 20:22:05 @agent_ppo2.py:186][0m |          -0.0202 |           4.3314 |          12.4232 |
[32m[20230113 20:22:05 @agent_ppo2.py:186][0m |          -0.0166 |           4.1651 |          12.4303 |
[32m[20230113 20:22:05 @agent_ppo2.py:186][0m |          -0.0190 |           4.1133 |          12.4151 |
[32m[20230113 20:22:05 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:22:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.61
[32m[20230113 20:22:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.43
[32m[20230113 20:22:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.06
[32m[20230113 20:22:05 @agent_ppo2.py:144][0m Total time:      37.54 min
[32m[20230113 20:22:05 @agent_ppo2.py:146][0m 3500032 total steps have happened
[32m[20230113 20:22:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1709 --------------------------#
[32m[20230113 20:22:05 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:22:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:05 @agent_ppo2.py:186][0m |          -0.0007 |           6.4273 |          12.3116 |
[32m[20230113 20:22:06 @agent_ppo2.py:186][0m |          -0.0071 |           5.1753 |          12.3207 |
[32m[20230113 20:22:06 @agent_ppo2.py:186][0m |          -0.0089 |           4.7620 |          12.3188 |
[32m[20230113 20:22:06 @agent_ppo2.py:186][0m |          -0.0112 |           4.5381 |          12.3186 |
[32m[20230113 20:22:06 @agent_ppo2.py:186][0m |          -0.0132 |           4.3197 |          12.3186 |
[32m[20230113 20:22:06 @agent_ppo2.py:186][0m |          -0.0130 |           4.1914 |          12.3136 |
[32m[20230113 20:22:06 @agent_ppo2.py:186][0m |          -0.0150 |           4.0371 |          12.3101 |
[32m[20230113 20:22:06 @agent_ppo2.py:186][0m |          -0.0161 |           3.9534 |          12.3137 |
[32m[20230113 20:22:06 @agent_ppo2.py:186][0m |          -0.0156 |           3.8601 |          12.3061 |
[32m[20230113 20:22:06 @agent_ppo2.py:186][0m |          -0.0174 |           3.8286 |          12.3156 |
[32m[20230113 20:22:06 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:22:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.20
[32m[20230113 20:22:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.44
[32m[20230113 20:22:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.71
[32m[20230113 20:22:06 @agent_ppo2.py:144][0m Total time:      37.56 min
[32m[20230113 20:22:06 @agent_ppo2.py:146][0m 3502080 total steps have happened
[32m[20230113 20:22:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1710 --------------------------#
[32m[20230113 20:22:07 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:07 @agent_ppo2.py:186][0m |           0.0018 |           7.4257 |          12.2008 |
[32m[20230113 20:22:07 @agent_ppo2.py:186][0m |          -0.0051 |           6.0718 |          12.2019 |
[32m[20230113 20:22:07 @agent_ppo2.py:186][0m |          -0.0103 |           5.4705 |          12.2053 |
[32m[20230113 20:22:07 @agent_ppo2.py:186][0m |          -0.0087 |           5.1902 |          12.2064 |
[32m[20230113 20:22:07 @agent_ppo2.py:186][0m |          -0.0093 |           4.9414 |          12.1993 |
[32m[20230113 20:22:07 @agent_ppo2.py:186][0m |          -0.0116 |           4.7504 |          12.2083 |
[32m[20230113 20:22:07 @agent_ppo2.py:186][0m |          -0.0102 |           4.5908 |          12.2068 |
[32m[20230113 20:22:07 @agent_ppo2.py:186][0m |          -0.0159 |           4.4251 |          12.2096 |
[32m[20230113 20:22:07 @agent_ppo2.py:186][0m |          -0.0130 |           4.4443 |          12.2064 |
[32m[20230113 20:22:07 @agent_ppo2.py:186][0m |          -0.0167 |           4.2309 |          12.1957 |
[32m[20230113 20:22:07 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.15
[32m[20230113 20:22:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.12
[32m[20230113 20:22:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.14
[32m[20230113 20:22:08 @agent_ppo2.py:144][0m Total time:      37.58 min
[32m[20230113 20:22:08 @agent_ppo2.py:146][0m 3504128 total steps have happened
[32m[20230113 20:22:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1711 --------------------------#
[32m[20230113 20:22:08 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:22:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:08 @agent_ppo2.py:186][0m |           0.0022 |           5.7186 |          12.3375 |
[32m[20230113 20:22:08 @agent_ppo2.py:186][0m |          -0.0082 |           4.5304 |          12.3565 |
[32m[20230113 20:22:08 @agent_ppo2.py:186][0m |          -0.0112 |           4.2199 |          12.3551 |
[32m[20230113 20:22:08 @agent_ppo2.py:186][0m |          -0.0099 |           3.9771 |          12.3400 |
[32m[20230113 20:22:08 @agent_ppo2.py:186][0m |          -0.0150 |           3.7754 |          12.3370 |
[32m[20230113 20:22:08 @agent_ppo2.py:186][0m |          -0.0131 |           3.6895 |          12.3437 |
[32m[20230113 20:22:08 @agent_ppo2.py:186][0m |          -0.0141 |           3.5453 |          12.3405 |
[32m[20230113 20:22:08 @agent_ppo2.py:186][0m |          -0.0157 |           3.4802 |          12.3452 |
[32m[20230113 20:22:08 @agent_ppo2.py:186][0m |          -0.0146 |           3.4190 |          12.3434 |
[32m[20230113 20:22:08 @agent_ppo2.py:186][0m |          -0.0137 |           3.3270 |          12.3436 |
[32m[20230113 20:22:08 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:22:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.21
[32m[20230113 20:22:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.37
[32m[20230113 20:22:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.69
[32m[20230113 20:22:09 @agent_ppo2.py:144][0m Total time:      37.60 min
[32m[20230113 20:22:09 @agent_ppo2.py:146][0m 3506176 total steps have happened
[32m[20230113 20:22:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1712 --------------------------#
[32m[20230113 20:22:09 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:22:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:09 @agent_ppo2.py:186][0m |           0.0009 |           6.9514 |          12.3745 |
[32m[20230113 20:22:09 @agent_ppo2.py:186][0m |          -0.0046 |           5.5524 |          12.3807 |
[32m[20230113 20:22:09 @agent_ppo2.py:186][0m |          -0.0070 |           4.8881 |          12.3860 |
[32m[20230113 20:22:09 @agent_ppo2.py:186][0m |          -0.0093 |           4.5267 |          12.3718 |
[32m[20230113 20:22:09 @agent_ppo2.py:186][0m |          -0.0111 |           4.2820 |          12.3885 |
[32m[20230113 20:22:10 @agent_ppo2.py:186][0m |          -0.0121 |           4.1230 |          12.3804 |
[32m[20230113 20:22:10 @agent_ppo2.py:186][0m |          -0.0132 |           3.9763 |          12.3753 |
[32m[20230113 20:22:10 @agent_ppo2.py:186][0m |          -0.0136 |           3.8208 |          12.3811 |
[32m[20230113 20:22:10 @agent_ppo2.py:186][0m |          -0.0143 |           3.7002 |          12.3857 |
[32m[20230113 20:22:10 @agent_ppo2.py:186][0m |          -0.0145 |           3.6223 |          12.3917 |
[32m[20230113 20:22:10 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:22:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.46
[32m[20230113 20:22:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.36
[32m[20230113 20:22:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 237.28
[32m[20230113 20:22:10 @agent_ppo2.py:144][0m Total time:      37.63 min
[32m[20230113 20:22:10 @agent_ppo2.py:146][0m 3508224 total steps have happened
[32m[20230113 20:22:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1713 --------------------------#
[32m[20230113 20:22:11 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:22:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:11 @agent_ppo2.py:186][0m |          -0.0108 |           6.7775 |          12.2189 |
[32m[20230113 20:22:11 @agent_ppo2.py:186][0m |          -0.0016 |           4.9368 |          12.1939 |
[32m[20230113 20:22:11 @agent_ppo2.py:186][0m |          -0.0076 |           4.4105 |          12.1653 |
[32m[20230113 20:22:11 @agent_ppo2.py:186][0m |          -0.0385 |           4.1278 |          12.1673 |
[32m[20230113 20:22:11 @agent_ppo2.py:186][0m |          -0.0216 |           3.9211 |          12.1672 |
[32m[20230113 20:22:11 @agent_ppo2.py:186][0m |          -0.0069 |           3.7558 |          12.1653 |
[32m[20230113 20:22:11 @agent_ppo2.py:186][0m |          -0.0151 |           3.6857 |          12.1693 |
[32m[20230113 20:22:11 @agent_ppo2.py:186][0m |           0.0043 |           3.5571 |          12.1702 |
[32m[20230113 20:22:11 @agent_ppo2.py:186][0m |          -0.0138 |           3.4357 |          12.1628 |
[32m[20230113 20:22:11 @agent_ppo2.py:186][0m |          -0.0172 |           3.3243 |          12.1670 |
[32m[20230113 20:22:11 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:22:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.09
[32m[20230113 20:22:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.22
[32m[20230113 20:22:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.75
[32m[20230113 20:22:11 @agent_ppo2.py:144][0m Total time:      37.65 min
[32m[20230113 20:22:11 @agent_ppo2.py:146][0m 3510272 total steps have happened
[32m[20230113 20:22:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1714 --------------------------#
[32m[20230113 20:22:12 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:22:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:12 @agent_ppo2.py:186][0m |          -0.0001 |           7.4387 |          12.5552 |
[32m[20230113 20:22:12 @agent_ppo2.py:186][0m |          -0.0056 |           5.7125 |          12.5423 |
[32m[20230113 20:22:12 @agent_ppo2.py:186][0m |          -0.0060 |           5.0666 |          12.5401 |
[32m[20230113 20:22:12 @agent_ppo2.py:186][0m |          -0.0091 |           4.6215 |          12.5322 |
[32m[20230113 20:22:12 @agent_ppo2.py:186][0m |          -0.0075 |           4.3639 |          12.5285 |
[32m[20230113 20:22:12 @agent_ppo2.py:186][0m |          -0.0143 |           4.1338 |          12.5428 |
[32m[20230113 20:22:12 @agent_ppo2.py:186][0m |          -0.0081 |           3.9786 |          12.5349 |
[32m[20230113 20:22:12 @agent_ppo2.py:186][0m |          -0.0142 |           3.8217 |          12.5353 |
[32m[20230113 20:22:12 @agent_ppo2.py:186][0m |          -0.0143 |           3.7038 |          12.5292 |
[32m[20230113 20:22:12 @agent_ppo2.py:186][0m |          -0.0107 |           3.6032 |          12.5340 |
[32m[20230113 20:22:12 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:22:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.83
[32m[20230113 20:22:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.39
[32m[20230113 20:22:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.00
[32m[20230113 20:22:13 @agent_ppo2.py:144][0m Total time:      37.67 min
[32m[20230113 20:22:13 @agent_ppo2.py:146][0m 3512320 total steps have happened
[32m[20230113 20:22:13 @agent_ppo2.py:122][0m #------------------------ Iteration 1715 --------------------------#
[32m[20230113 20:22:13 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:13 @agent_ppo2.py:186][0m |           0.0009 |           7.3491 |          12.6346 |
[32m[20230113 20:22:13 @agent_ppo2.py:186][0m |          -0.0068 |           5.0141 |          12.6102 |
[32m[20230113 20:22:13 @agent_ppo2.py:186][0m |          -0.0063 |           4.3732 |          12.6109 |
[32m[20230113 20:22:13 @agent_ppo2.py:186][0m |          -0.0104 |           3.9697 |          12.6052 |
[32m[20230113 20:22:13 @agent_ppo2.py:186][0m |          -0.0154 |           3.7625 |          12.6053 |
[32m[20230113 20:22:13 @agent_ppo2.py:186][0m |          -0.0131 |           3.5970 |          12.6002 |
[32m[20230113 20:22:13 @agent_ppo2.py:186][0m |          -0.0132 |           3.4473 |          12.5931 |
[32m[20230113 20:22:13 @agent_ppo2.py:186][0m |          -0.0143 |           3.3588 |          12.6020 |
[32m[20230113 20:22:13 @agent_ppo2.py:186][0m |          -0.0166 |           3.2274 |          12.5931 |
[32m[20230113 20:22:14 @agent_ppo2.py:186][0m |          -0.0160 |           3.1424 |          12.5884 |
[32m[20230113 20:22:14 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.66
[32m[20230113 20:22:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.75
[32m[20230113 20:22:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.79
[32m[20230113 20:22:14 @agent_ppo2.py:144][0m Total time:      37.69 min
[32m[20230113 20:22:14 @agent_ppo2.py:146][0m 3514368 total steps have happened
[32m[20230113 20:22:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1716 --------------------------#
[32m[20230113 20:22:14 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:14 @agent_ppo2.py:186][0m |           0.0008 |           5.8529 |          12.4264 |
[32m[20230113 20:22:14 @agent_ppo2.py:186][0m |          -0.0074 |           4.3802 |          12.4174 |
[32m[20230113 20:22:14 @agent_ppo2.py:186][0m |          -0.0057 |           3.8843 |          12.4180 |
[32m[20230113 20:22:15 @agent_ppo2.py:186][0m |          -0.0102 |           3.6206 |          12.4080 |
[32m[20230113 20:22:15 @agent_ppo2.py:186][0m |          -0.0118 |           3.4011 |          12.4102 |
[32m[20230113 20:22:15 @agent_ppo2.py:186][0m |          -0.0140 |           3.3065 |          12.4007 |
[32m[20230113 20:22:15 @agent_ppo2.py:186][0m |          -0.0159 |           3.2081 |          12.4089 |
[32m[20230113 20:22:15 @agent_ppo2.py:186][0m |          -0.0161 |           3.0720 |          12.4034 |
[32m[20230113 20:22:15 @agent_ppo2.py:186][0m |          -0.0199 |           3.0077 |          12.4010 |
[32m[20230113 20:22:15 @agent_ppo2.py:186][0m |          -0.0127 |           2.9836 |          12.3909 |
[32m[20230113 20:22:15 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.87
[32m[20230113 20:22:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.26
[32m[20230113 20:22:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 142.25
[32m[20230113 20:22:15 @agent_ppo2.py:144][0m Total time:      37.71 min
[32m[20230113 20:22:15 @agent_ppo2.py:146][0m 3516416 total steps have happened
[32m[20230113 20:22:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1717 --------------------------#
[32m[20230113 20:22:16 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:22:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:16 @agent_ppo2.py:186][0m |          -0.0039 |           6.8220 |          12.4336 |
[32m[20230113 20:22:16 @agent_ppo2.py:186][0m |          -0.0104 |           5.0529 |          12.4078 |
[32m[20230113 20:22:16 @agent_ppo2.py:186][0m |          -0.0154 |           4.4860 |          12.4006 |
[32m[20230113 20:22:16 @agent_ppo2.py:186][0m |          -0.0114 |           4.1268 |          12.3986 |
[32m[20230113 20:22:16 @agent_ppo2.py:186][0m |          -0.0127 |           3.8769 |          12.3998 |
[32m[20230113 20:22:16 @agent_ppo2.py:186][0m |          -0.0177 |           3.7029 |          12.4039 |
[32m[20230113 20:22:16 @agent_ppo2.py:186][0m |          -0.0167 |           3.5737 |          12.4098 |
[32m[20230113 20:22:16 @agent_ppo2.py:186][0m |          -0.0105 |           3.4940 |          12.4001 |
[32m[20230113 20:22:16 @agent_ppo2.py:186][0m |          -0.0198 |           3.3949 |          12.4008 |
[32m[20230113 20:22:16 @agent_ppo2.py:186][0m |          -0.0156 |           3.3386 |          12.4099 |
[32m[20230113 20:22:16 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:22:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.15
[32m[20230113 20:22:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.12
[32m[20230113 20:22:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.10
[32m[20230113 20:22:16 @agent_ppo2.py:144][0m Total time:      37.73 min
[32m[20230113 20:22:16 @agent_ppo2.py:146][0m 3518464 total steps have happened
[32m[20230113 20:22:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1718 --------------------------#
[32m[20230113 20:22:17 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:22:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:17 @agent_ppo2.py:186][0m |           0.0057 |           8.3565 |          12.6387 |
[32m[20230113 20:22:17 @agent_ppo2.py:186][0m |          -0.0057 |           6.1467 |          12.6263 |
[32m[20230113 20:22:17 @agent_ppo2.py:186][0m |          -0.0097 |           5.3673 |          12.6087 |
[32m[20230113 20:22:17 @agent_ppo2.py:186][0m |          -0.0106 |           4.9203 |          12.6096 |
[32m[20230113 20:22:17 @agent_ppo2.py:186][0m |          -0.0130 |           4.5443 |          12.6113 |
[32m[20230113 20:22:17 @agent_ppo2.py:186][0m |          -0.0155 |           4.3382 |          12.6040 |
[32m[20230113 20:22:17 @agent_ppo2.py:186][0m |          -0.0144 |           4.1298 |          12.5961 |
[32m[20230113 20:22:17 @agent_ppo2.py:186][0m |          -0.0136 |           3.9451 |          12.5983 |
[32m[20230113 20:22:17 @agent_ppo2.py:186][0m |          -0.0174 |           3.7950 |          12.6065 |
[32m[20230113 20:22:17 @agent_ppo2.py:186][0m |          -0.0172 |           3.6866 |          12.6005 |
[32m[20230113 20:22:17 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:22:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.11
[32m[20230113 20:22:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.34
[32m[20230113 20:22:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.46
[32m[20230113 20:22:18 @agent_ppo2.py:144][0m Total time:      37.75 min
[32m[20230113 20:22:18 @agent_ppo2.py:146][0m 3520512 total steps have happened
[32m[20230113 20:22:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1719 --------------------------#
[32m[20230113 20:22:18 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:18 @agent_ppo2.py:186][0m |           0.0050 |           7.9511 |          12.4121 |
[32m[20230113 20:22:18 @agent_ppo2.py:186][0m |           0.0190 |           5.9124 |          12.3915 |
[32m[20230113 20:22:18 @agent_ppo2.py:186][0m |          -0.0099 |           5.0383 |          12.3903 |
[32m[20230113 20:22:18 @agent_ppo2.py:186][0m |          -0.0183 |           4.5665 |          12.4062 |
[32m[20230113 20:22:18 @agent_ppo2.py:186][0m |          -0.0031 |           4.3891 |          12.3817 |
[32m[20230113 20:22:18 @agent_ppo2.py:186][0m |          -0.0112 |           4.1126 |          12.3836 |
[32m[20230113 20:22:18 @agent_ppo2.py:186][0m |          -0.0068 |           3.9751 |          12.3940 |
[32m[20230113 20:22:18 @agent_ppo2.py:186][0m |           0.0010 |           3.8529 |          12.3828 |
[32m[20230113 20:22:18 @agent_ppo2.py:186][0m |          -0.0084 |           3.7712 |          12.3878 |
[32m[20230113 20:22:19 @agent_ppo2.py:186][0m |          -0.0110 |           3.6968 |          12.3775 |
[32m[20230113 20:22:19 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.20
[32m[20230113 20:22:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.60
[32m[20230113 20:22:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.05
[32m[20230113 20:22:19 @agent_ppo2.py:144][0m Total time:      37.77 min
[32m[20230113 20:22:19 @agent_ppo2.py:146][0m 3522560 total steps have happened
[32m[20230113 20:22:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1720 --------------------------#
[32m[20230113 20:22:19 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:19 @agent_ppo2.py:186][0m |          -0.0060 |           6.6990 |          12.6774 |
[32m[20230113 20:22:19 @agent_ppo2.py:186][0m |          -0.0046 |           5.1912 |          12.6693 |
[32m[20230113 20:22:20 @agent_ppo2.py:186][0m |          -0.0073 |           4.6714 |          12.6747 |
[32m[20230113 20:22:20 @agent_ppo2.py:186][0m |          -0.0101 |           4.4537 |          12.6599 |
[32m[20230113 20:22:20 @agent_ppo2.py:186][0m |          -0.0068 |           4.4167 |          12.6621 |
[32m[20230113 20:22:20 @agent_ppo2.py:186][0m |          -0.0174 |           4.1575 |          12.6587 |
[32m[20230113 20:22:20 @agent_ppo2.py:186][0m |          -0.0140 |           4.0888 |          12.6555 |
[32m[20230113 20:22:20 @agent_ppo2.py:186][0m |          -0.0134 |           4.0299 |          12.6620 |
[32m[20230113 20:22:20 @agent_ppo2.py:186][0m |          -0.0187 |           3.9899 |          12.6677 |
[32m[20230113 20:22:20 @agent_ppo2.py:186][0m |          -0.0188 |           3.9071 |          12.6605 |
[32m[20230113 20:22:20 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:22:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.95
[32m[20230113 20:22:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.12
[32m[20230113 20:22:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.47
[32m[20230113 20:22:20 @agent_ppo2.py:144][0m Total time:      37.79 min
[32m[20230113 20:22:20 @agent_ppo2.py:146][0m 3524608 total steps have happened
[32m[20230113 20:22:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1721 --------------------------#
[32m[20230113 20:22:21 @agent_ppo2.py:128][0m Sampling time: 0.35 s by 1 slaves
[32m[20230113 20:22:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:21 @agent_ppo2.py:186][0m |          -0.0025 |          12.4996 |          12.4449 |
[32m[20230113 20:22:21 @agent_ppo2.py:186][0m |          -0.0121 |           6.9875 |          12.4358 |
[32m[20230113 20:22:21 @agent_ppo2.py:186][0m |          -0.0095 |           6.2594 |          12.4406 |
[32m[20230113 20:22:21 @agent_ppo2.py:186][0m |          -0.0126 |           5.3146 |          12.4324 |
[32m[20230113 20:22:21 @agent_ppo2.py:186][0m |          -0.0110 |           5.1275 |          12.4241 |
[32m[20230113 20:22:21 @agent_ppo2.py:186][0m |          -0.0197 |           4.6455 |          12.4252 |
[32m[20230113 20:22:21 @agent_ppo2.py:186][0m |          -0.0190 |           4.3001 |          12.4270 |
[32m[20230113 20:22:21 @agent_ppo2.py:186][0m |          -0.0179 |           4.2453 |          12.4332 |
[32m[20230113 20:22:21 @agent_ppo2.py:186][0m |          -0.0212 |           3.9649 |          12.4308 |
[32m[20230113 20:22:21 @agent_ppo2.py:186][0m |          -0.0218 |           3.7620 |          12.4180 |
[32m[20230113 20:22:21 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:22:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 134.29
[32m[20230113 20:22:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.67
[32m[20230113 20:22:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.24
[32m[20230113 20:22:21 @agent_ppo2.py:144][0m Total time:      37.81 min
[32m[20230113 20:22:21 @agent_ppo2.py:146][0m 3526656 total steps have happened
[32m[20230113 20:22:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1722 --------------------------#
[32m[20230113 20:22:22 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:22 @agent_ppo2.py:186][0m |          -0.0084 |           6.6594 |          12.6679 |
[32m[20230113 20:22:22 @agent_ppo2.py:186][0m |          -0.0100 |           5.1470 |          12.6389 |
[32m[20230113 20:22:22 @agent_ppo2.py:186][0m |           0.0588 |           5.2339 |          12.6519 |
[32m[20230113 20:22:22 @agent_ppo2.py:186][0m |          -0.0046 |           4.3984 |          12.6392 |
[32m[20230113 20:22:22 @agent_ppo2.py:186][0m |          -0.0270 |           4.0672 |          12.6261 |
[32m[20230113 20:22:22 @agent_ppo2.py:186][0m |           0.0090 |           3.9523 |          12.6247 |
[32m[20230113 20:22:22 @agent_ppo2.py:186][0m |          -0.0105 |           3.6642 |          12.6347 |
[32m[20230113 20:22:22 @agent_ppo2.py:186][0m |          -0.0173 |           3.5035 |          12.6065 |
[32m[20230113 20:22:22 @agent_ppo2.py:186][0m |           0.1198 |           3.6713 |          12.6334 |
[32m[20230113 20:22:22 @agent_ppo2.py:186][0m |          -0.0217 |           4.0108 |          12.6319 |
[32m[20230113 20:22:22 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.21
[32m[20230113 20:22:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.41
[32m[20230113 20:22:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.65
[32m[20230113 20:22:23 @agent_ppo2.py:144][0m Total time:      37.83 min
[32m[20230113 20:22:23 @agent_ppo2.py:146][0m 3528704 total steps have happened
[32m[20230113 20:22:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1723 --------------------------#
[32m[20230113 20:22:23 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:22:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:23 @agent_ppo2.py:186][0m |          -0.0125 |           7.4019 |          12.5756 |
[32m[20230113 20:22:23 @agent_ppo2.py:186][0m |          -0.0065 |           4.9758 |          12.5553 |
[32m[20230113 20:22:23 @agent_ppo2.py:186][0m |          -0.0070 |           4.4862 |          12.5396 |
[32m[20230113 20:22:23 @agent_ppo2.py:186][0m |          -0.0144 |           3.9845 |          12.5407 |
[32m[20230113 20:22:23 @agent_ppo2.py:186][0m |          -0.0171 |           3.7637 |          12.5250 |
[32m[20230113 20:22:23 @agent_ppo2.py:186][0m |          -0.0169 |           3.5677 |          12.5283 |
[32m[20230113 20:22:23 @agent_ppo2.py:186][0m |          -0.0149 |           3.4501 |          12.5219 |
[32m[20230113 20:22:23 @agent_ppo2.py:186][0m |          -0.0157 |           3.3597 |          12.5208 |
[32m[20230113 20:22:23 @agent_ppo2.py:186][0m |          -0.0146 |           3.2332 |          12.5233 |
[32m[20230113 20:22:23 @agent_ppo2.py:186][0m |          -0.0222 |           3.1433 |          12.5187 |
[32m[20230113 20:22:23 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:22:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.40
[32m[20230113 20:22:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.29
[32m[20230113 20:22:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.09
[32m[20230113 20:22:24 @agent_ppo2.py:144][0m Total time:      37.85 min
[32m[20230113 20:22:24 @agent_ppo2.py:146][0m 3530752 total steps have happened
[32m[20230113 20:22:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1724 --------------------------#
[32m[20230113 20:22:24 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:22:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:24 @agent_ppo2.py:186][0m |           0.0020 |           5.9755 |          12.6282 |
[32m[20230113 20:22:24 @agent_ppo2.py:186][0m |          -0.0047 |           4.5910 |          12.6051 |
[32m[20230113 20:22:24 @agent_ppo2.py:186][0m |          -0.0080 |           4.3750 |          12.5998 |
[32m[20230113 20:22:24 @agent_ppo2.py:186][0m |          -0.0105 |           4.1281 |          12.5990 |
[32m[20230113 20:22:24 @agent_ppo2.py:186][0m |          -0.0118 |           3.9174 |          12.5958 |
[32m[20230113 20:22:25 @agent_ppo2.py:186][0m |          -0.0121 |           3.8440 |          12.5939 |
[32m[20230113 20:22:25 @agent_ppo2.py:186][0m |          -0.0078 |           3.7602 |          12.5958 |
[32m[20230113 20:22:25 @agent_ppo2.py:186][0m |          -0.0112 |           3.6037 |          12.5999 |
[32m[20230113 20:22:25 @agent_ppo2.py:186][0m |          -0.0143 |           3.5484 |          12.5975 |
[32m[20230113 20:22:25 @agent_ppo2.py:186][0m |          -0.0168 |           3.4623 |          12.5939 |
[32m[20230113 20:22:25 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.77
[32m[20230113 20:22:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.27
[32m[20230113 20:22:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.71
[32m[20230113 20:22:25 @agent_ppo2.py:144][0m Total time:      37.88 min
[32m[20230113 20:22:25 @agent_ppo2.py:146][0m 3532800 total steps have happened
[32m[20230113 20:22:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1725 --------------------------#
[32m[20230113 20:22:26 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:26 @agent_ppo2.py:186][0m |           0.0009 |           6.4925 |          12.7102 |
[32m[20230113 20:22:26 @agent_ppo2.py:186][0m |          -0.0068 |           5.2040 |          12.7012 |
[32m[20230113 20:22:26 @agent_ppo2.py:186][0m |          -0.0087 |           4.6779 |          12.6946 |
[32m[20230113 20:22:26 @agent_ppo2.py:186][0m |          -0.0102 |           4.3698 |          12.6829 |
[32m[20230113 20:22:26 @agent_ppo2.py:186][0m |          -0.0098 |           4.0794 |          12.6916 |
[32m[20230113 20:22:26 @agent_ppo2.py:186][0m |          -0.0138 |           3.8989 |          12.6745 |
[32m[20230113 20:22:26 @agent_ppo2.py:186][0m |          -0.0148 |           3.7707 |          12.6755 |
[32m[20230113 20:22:26 @agent_ppo2.py:186][0m |          -0.0155 |           3.6549 |          12.6698 |
[32m[20230113 20:22:26 @agent_ppo2.py:186][0m |          -0.0158 |           3.5347 |          12.6616 |
[32m[20230113 20:22:26 @agent_ppo2.py:186][0m |          -0.0134 |           3.5121 |          12.6615 |
[32m[20230113 20:22:26 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.95
[32m[20230113 20:22:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.94
[32m[20230113 20:22:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.37
[32m[20230113 20:22:26 @agent_ppo2.py:144][0m Total time:      37.90 min
[32m[20230113 20:22:26 @agent_ppo2.py:146][0m 3534848 total steps have happened
[32m[20230113 20:22:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1726 --------------------------#
[32m[20230113 20:22:27 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:22:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:27 @agent_ppo2.py:186][0m |          -0.0033 |           6.6053 |          12.6036 |
[32m[20230113 20:22:27 @agent_ppo2.py:186][0m |          -0.0082 |           5.1268 |          12.6003 |
[32m[20230113 20:22:27 @agent_ppo2.py:186][0m |          -0.0083 |           4.6927 |          12.5916 |
[32m[20230113 20:22:27 @agent_ppo2.py:186][0m |          -0.0048 |           4.3909 |          12.5764 |
[32m[20230113 20:22:27 @agent_ppo2.py:186][0m |          -0.0095 |           4.1440 |          12.5704 |
[32m[20230113 20:22:27 @agent_ppo2.py:186][0m |          -0.0037 |           4.0372 |          12.5815 |
[32m[20230113 20:22:27 @agent_ppo2.py:186][0m |          -0.0210 |           3.8059 |          12.5739 |
[32m[20230113 20:22:27 @agent_ppo2.py:186][0m |          -0.0200 |           3.6576 |          12.5722 |
[32m[20230113 20:22:27 @agent_ppo2.py:186][0m |          -0.0111 |           3.5898 |          12.5676 |
[32m[20230113 20:22:27 @agent_ppo2.py:186][0m |          -0.0166 |           3.4673 |          12.5728 |
[32m[20230113 20:22:27 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:22:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 225.28
[32m[20230113 20:22:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 227.67
[32m[20230113 20:22:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.68
[32m[20230113 20:22:28 @agent_ppo2.py:144][0m Total time:      37.92 min
[32m[20230113 20:22:28 @agent_ppo2.py:146][0m 3536896 total steps have happened
[32m[20230113 20:22:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1727 --------------------------#
[32m[20230113 20:22:28 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:28 @agent_ppo2.py:186][0m |           0.0002 |           6.3573 |          12.5358 |
[32m[20230113 20:22:28 @agent_ppo2.py:186][0m |          -0.0031 |           4.7297 |          12.5140 |
[32m[20230113 20:22:28 @agent_ppo2.py:186][0m |          -0.0043 |           4.1002 |          12.5049 |
[32m[20230113 20:22:28 @agent_ppo2.py:186][0m |          -0.0074 |           3.7689 |          12.5076 |
[32m[20230113 20:22:28 @agent_ppo2.py:186][0m |          -0.0068 |           3.5342 |          12.5040 |
[32m[20230113 20:22:28 @agent_ppo2.py:186][0m |          -0.0106 |           3.3762 |          12.5127 |
[32m[20230113 20:22:28 @agent_ppo2.py:186][0m |          -0.0119 |           3.1790 |          12.5000 |
[32m[20230113 20:22:28 @agent_ppo2.py:186][0m |          -0.0116 |           3.0527 |          12.5033 |
[32m[20230113 20:22:28 @agent_ppo2.py:186][0m |          -0.0114 |           2.9870 |          12.5034 |
[32m[20230113 20:22:28 @agent_ppo2.py:186][0m |          -0.0132 |           2.8898 |          12.5084 |
[32m[20230113 20:22:28 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:22:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.19
[32m[20230113 20:22:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.31
[32m[20230113 20:22:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 142.30
[32m[20230113 20:22:29 @agent_ppo2.py:144][0m Total time:      37.94 min
[32m[20230113 20:22:29 @agent_ppo2.py:146][0m 3538944 total steps have happened
[32m[20230113 20:22:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1728 --------------------------#
[32m[20230113 20:22:29 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:22:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:29 @agent_ppo2.py:186][0m |           0.0086 |           5.3055 |          12.7067 |
[32m[20230113 20:22:29 @agent_ppo2.py:186][0m |          -0.0066 |           4.3358 |          12.6799 |
[32m[20230113 20:22:29 @agent_ppo2.py:186][0m |           0.0029 |           3.9340 |          12.6607 |
[32m[20230113 20:22:29 @agent_ppo2.py:186][0m |          -0.0108 |           3.6711 |          12.6720 |
[32m[20230113 20:22:29 @agent_ppo2.py:186][0m |          -0.0130 |           3.5094 |          12.6604 |
[32m[20230113 20:22:30 @agent_ppo2.py:186][0m |          -0.0172 |           3.3479 |          12.6534 |
[32m[20230113 20:22:30 @agent_ppo2.py:186][0m |          -0.0139 |           3.2773 |          12.6564 |
[32m[20230113 20:22:30 @agent_ppo2.py:186][0m |          -0.0139 |           3.1732 |          12.6386 |
[32m[20230113 20:22:30 @agent_ppo2.py:186][0m |          -0.0182 |           3.1142 |          12.6511 |
[32m[20230113 20:22:30 @agent_ppo2.py:186][0m |          -0.0196 |           3.0119 |          12.6297 |
[32m[20230113 20:22:30 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:22:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.29
[32m[20230113 20:22:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.05
[32m[20230113 20:22:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.73
[32m[20230113 20:22:30 @agent_ppo2.py:144][0m Total time:      37.96 min
[32m[20230113 20:22:30 @agent_ppo2.py:146][0m 3540992 total steps have happened
[32m[20230113 20:22:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1729 --------------------------#
[32m[20230113 20:22:30 @agent_ppo2.py:128][0m Sampling time: 0.34 s by 1 slaves
[32m[20230113 20:22:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:30 @agent_ppo2.py:186][0m |           0.0066 |          19.2347 |          13.1209 |
[32m[20230113 20:22:31 @agent_ppo2.py:186][0m |          -0.0033 |           7.1915 |          13.1169 |
[32m[20230113 20:22:31 @agent_ppo2.py:186][0m |          -0.0076 |           5.4592 |          13.1059 |
[32m[20230113 20:22:31 @agent_ppo2.py:186][0m |          -0.0123 |           4.7195 |          13.1102 |
[32m[20230113 20:22:31 @agent_ppo2.py:186][0m |          -0.0106 |           4.2278 |          13.1011 |
[32m[20230113 20:22:31 @agent_ppo2.py:186][0m |          -0.0143 |           3.8789 |          13.1005 |
[32m[20230113 20:22:31 @agent_ppo2.py:186][0m |          -0.0151 |           3.6477 |          13.0994 |
[32m[20230113 20:22:31 @agent_ppo2.py:186][0m |          -0.0171 |           3.4417 |          13.0987 |
[32m[20230113 20:22:31 @agent_ppo2.py:186][0m |          -0.0163 |           3.3283 |          13.1021 |
[32m[20230113 20:22:31 @agent_ppo2.py:186][0m |          -0.0192 |           3.1965 |          13.1012 |
[32m[20230113 20:22:31 @agent_ppo2.py:131][0m Policy update time: 0.35 s
[32m[20230113 20:22:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 120.15
[32m[20230113 20:22:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.75
[32m[20230113 20:22:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.16
[32m[20230113 20:22:31 @agent_ppo2.py:144][0m Total time:      37.98 min
[32m[20230113 20:22:31 @agent_ppo2.py:146][0m 3543040 total steps have happened
[32m[20230113 20:22:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1730 --------------------------#
[32m[20230113 20:22:32 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:22:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:32 @agent_ppo2.py:186][0m |          -0.0004 |           8.0185 |          12.5440 |
[32m[20230113 20:22:32 @agent_ppo2.py:186][0m |          -0.0052 |           5.4840 |          12.5491 |
[32m[20230113 20:22:32 @agent_ppo2.py:186][0m |          -0.0095 |           4.7322 |          12.5507 |
[32m[20230113 20:22:32 @agent_ppo2.py:186][0m |          -0.0038 |           4.4422 |          12.5629 |
[32m[20230113 20:22:32 @agent_ppo2.py:186][0m |          -0.0071 |           4.1991 |          12.5402 |
[32m[20230113 20:22:32 @agent_ppo2.py:186][0m |          -0.0148 |           3.9504 |          12.5468 |
[32m[20230113 20:22:32 @agent_ppo2.py:186][0m |          -0.0146 |           3.7929 |          12.5386 |
[32m[20230113 20:22:32 @agent_ppo2.py:186][0m |          -0.0172 |           3.6775 |          12.5488 |
[32m[20230113 20:22:32 @agent_ppo2.py:186][0m |          -0.0142 |           3.5601 |          12.5389 |
[32m[20230113 20:22:32 @agent_ppo2.py:186][0m |          -0.0159 |           3.4940 |          12.5440 |
[32m[20230113 20:22:32 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:22:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 228.09
[32m[20230113 20:22:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 228.98
[32m[20230113 20:22:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.06
[32m[20230113 20:22:33 @agent_ppo2.py:144][0m Total time:      38.00 min
[32m[20230113 20:22:33 @agent_ppo2.py:146][0m 3545088 total steps have happened
[32m[20230113 20:22:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1731 --------------------------#
[32m[20230113 20:22:33 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:22:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:33 @agent_ppo2.py:186][0m |          -0.0026 |           7.5147 |          12.5465 |
[32m[20230113 20:22:33 @agent_ppo2.py:186][0m |          -0.0095 |           5.7104 |          12.5236 |
[32m[20230113 20:22:33 @agent_ppo2.py:186][0m |          -0.0088 |           5.1628 |          12.5260 |
[32m[20230113 20:22:33 @agent_ppo2.py:186][0m |          -0.0034 |           4.8246 |          12.5191 |
[32m[20230113 20:22:33 @agent_ppo2.py:186][0m |          -0.0142 |           4.5401 |          12.5198 |
[32m[20230113 20:22:33 @agent_ppo2.py:186][0m |          -0.0151 |           4.2361 |          12.5190 |
[32m[20230113 20:22:33 @agent_ppo2.py:186][0m |          -0.0129 |           4.0257 |          12.5145 |
[32m[20230113 20:22:33 @agent_ppo2.py:186][0m |          -0.0154 |           3.9506 |          12.5159 |
[32m[20230113 20:22:33 @agent_ppo2.py:186][0m |          -0.0127 |           3.8134 |          12.5190 |
[32m[20230113 20:22:33 @agent_ppo2.py:186][0m |          -0.0184 |           3.7244 |          12.5187 |
[32m[20230113 20:22:33 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:22:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.04
[32m[20230113 20:22:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.87
[32m[20230113 20:22:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.92
[32m[20230113 20:22:34 @agent_ppo2.py:144][0m Total time:      38.02 min
[32m[20230113 20:22:34 @agent_ppo2.py:146][0m 3547136 total steps have happened
[32m[20230113 20:22:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1732 --------------------------#
[32m[20230113 20:22:34 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:34 @agent_ppo2.py:186][0m |           0.0004 |           7.4922 |          12.3775 |
[32m[20230113 20:22:34 @agent_ppo2.py:186][0m |          -0.0060 |           6.0113 |          12.3614 |
[32m[20230113 20:22:34 @agent_ppo2.py:186][0m |          -0.0065 |           5.3639 |          12.3558 |
[32m[20230113 20:22:34 @agent_ppo2.py:186][0m |          -0.0069 |           4.8961 |          12.3656 |
[32m[20230113 20:22:34 @agent_ppo2.py:186][0m |          -0.0031 |           4.6570 |          12.3530 |
[32m[20230113 20:22:35 @agent_ppo2.py:186][0m |          -0.0150 |           4.3942 |          12.3534 |
[32m[20230113 20:22:35 @agent_ppo2.py:186][0m |          -0.0123 |           4.1917 |          12.3622 |
[32m[20230113 20:22:35 @agent_ppo2.py:186][0m |          -0.0153 |           4.1033 |          12.3522 |
[32m[20230113 20:22:35 @agent_ppo2.py:186][0m |          -0.0166 |           3.9698 |          12.3501 |
[32m[20230113 20:22:35 @agent_ppo2.py:186][0m |          -0.0137 |           3.8846 |          12.3526 |
[32m[20230113 20:22:35 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.84
[32m[20230113 20:22:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.48
[32m[20230113 20:22:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.38
[32m[20230113 20:22:35 @agent_ppo2.py:144][0m Total time:      38.04 min
[32m[20230113 20:22:35 @agent_ppo2.py:146][0m 3549184 total steps have happened
[32m[20230113 20:22:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1733 --------------------------#
[32m[20230113 20:22:36 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:22:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:36 @agent_ppo2.py:186][0m |          -0.0040 |           5.4709 |          12.5705 |
[32m[20230113 20:22:36 @agent_ppo2.py:186][0m |          -0.0001 |           4.1871 |          12.5538 |
[32m[20230113 20:22:36 @agent_ppo2.py:186][0m |          -0.0201 |           3.7674 |          12.5618 |
[32m[20230113 20:22:36 @agent_ppo2.py:186][0m |          -0.0172 |           3.4590 |          12.5367 |
[32m[20230113 20:22:36 @agent_ppo2.py:186][0m |          -0.0313 |           3.3147 |          12.5639 |
[32m[20230113 20:22:36 @agent_ppo2.py:186][0m |          -0.0199 |           3.2140 |          12.5445 |
[32m[20230113 20:22:36 @agent_ppo2.py:186][0m |          -0.0084 |           3.0788 |          12.5622 |
[32m[20230113 20:22:36 @agent_ppo2.py:186][0m |          -0.0062 |           3.0050 |          12.5624 |
[32m[20230113 20:22:36 @agent_ppo2.py:186][0m |          -0.0013 |           3.2615 |          12.5603 |
[32m[20230113 20:22:36 @agent_ppo2.py:186][0m |          -0.0085 |           2.9915 |          12.5517 |
[32m[20230113 20:22:36 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:22:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.03
[32m[20230113 20:22:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.49
[32m[20230113 20:22:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.39
[32m[20230113 20:22:36 @agent_ppo2.py:144][0m Total time:      38.06 min
[32m[20230113 20:22:36 @agent_ppo2.py:146][0m 3551232 total steps have happened
[32m[20230113 20:22:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1734 --------------------------#
[32m[20230113 20:22:37 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:37 @agent_ppo2.py:186][0m |           0.0011 |           6.2035 |          12.8343 |
[32m[20230113 20:22:37 @agent_ppo2.py:186][0m |          -0.0023 |           4.9447 |          12.7861 |
[32m[20230113 20:22:37 @agent_ppo2.py:186][0m |          -0.0189 |           4.4284 |          12.8102 |
[32m[20230113 20:22:37 @agent_ppo2.py:186][0m |           0.0110 |           4.2129 |          12.8008 |
[32m[20230113 20:22:37 @agent_ppo2.py:186][0m |          -0.0156 |           3.9279 |          12.8166 |
[32m[20230113 20:22:37 @agent_ppo2.py:186][0m |           0.0078 |           3.8487 |          12.8012 |
[32m[20230113 20:22:37 @agent_ppo2.py:186][0m |          -0.0052 |           3.7941 |          12.7544 |
[32m[20230113 20:22:37 @agent_ppo2.py:186][0m |          -0.0098 |           3.5191 |          12.7913 |
[32m[20230113 20:22:37 @agent_ppo2.py:186][0m |          -0.0155 |           3.4222 |          12.8061 |
[32m[20230113 20:22:37 @agent_ppo2.py:186][0m |          -0.0158 |           3.3746 |          12.8037 |
[32m[20230113 20:22:37 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.56
[32m[20230113 20:22:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.38
[32m[20230113 20:22:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.20
[32m[20230113 20:22:38 @agent_ppo2.py:144][0m Total time:      38.08 min
[32m[20230113 20:22:38 @agent_ppo2.py:146][0m 3553280 total steps have happened
[32m[20230113 20:22:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1735 --------------------------#
[32m[20230113 20:22:38 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:22:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:38 @agent_ppo2.py:186][0m |           0.0010 |           6.4125 |          13.1265 |
[32m[20230113 20:22:38 @agent_ppo2.py:186][0m |          -0.0079 |           4.5290 |          13.1209 |
[32m[20230113 20:22:38 @agent_ppo2.py:186][0m |          -0.0100 |           4.2028 |          13.1166 |
[32m[20230113 20:22:38 @agent_ppo2.py:186][0m |          -0.0108 |           3.9520 |          13.1126 |
[32m[20230113 20:22:38 @agent_ppo2.py:186][0m |          -0.0127 |           3.7066 |          13.1016 |
[32m[20230113 20:22:38 @agent_ppo2.py:186][0m |          -0.0141 |           3.6313 |          13.1111 |
[32m[20230113 20:22:38 @agent_ppo2.py:186][0m |          -0.0136 |           3.4899 |          13.0930 |
[32m[20230113 20:22:38 @agent_ppo2.py:186][0m |          -0.0151 |           3.3816 |          13.0912 |
[32m[20230113 20:22:38 @agent_ppo2.py:186][0m |          -0.0159 |           3.2706 |          13.0977 |
[32m[20230113 20:22:38 @agent_ppo2.py:186][0m |          -0.0171 |           3.1893 |          13.1024 |
[32m[20230113 20:22:38 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.83
[32m[20230113 20:22:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.00
[32m[20230113 20:22:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.90
[32m[20230113 20:22:39 @agent_ppo2.py:144][0m Total time:      38.11 min
[32m[20230113 20:22:39 @agent_ppo2.py:146][0m 3555328 total steps have happened
[32m[20230113 20:22:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1736 --------------------------#
[32m[20230113 20:22:39 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:22:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:39 @agent_ppo2.py:186][0m |          -0.0023 |           6.3036 |          13.0584 |
[32m[20230113 20:22:39 @agent_ppo2.py:186][0m |          -0.0060 |           4.8212 |          13.0626 |
[32m[20230113 20:22:40 @agent_ppo2.py:186][0m |          -0.0124 |           4.2360 |          13.0592 |
[32m[20230113 20:22:40 @agent_ppo2.py:186][0m |          -0.0109 |           3.9802 |          13.0482 |
[32m[20230113 20:22:40 @agent_ppo2.py:186][0m |          -0.0130 |           3.7510 |          13.0502 |
[32m[20230113 20:22:40 @agent_ppo2.py:186][0m |          -0.0119 |           3.6180 |          13.0535 |
[32m[20230113 20:22:40 @agent_ppo2.py:186][0m |          -0.0135 |           3.5123 |          13.0498 |
[32m[20230113 20:22:40 @agent_ppo2.py:186][0m |          -0.0144 |           3.4267 |          13.0554 |
[32m[20230113 20:22:40 @agent_ppo2.py:186][0m |          -0.0160 |           3.3468 |          13.0429 |
[32m[20230113 20:22:40 @agent_ppo2.py:186][0m |          -0.0164 |           3.2704 |          13.0487 |
[32m[20230113 20:22:40 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:22:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.24
[32m[20230113 20:22:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.28
[32m[20230113 20:22:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.83
[32m[20230113 20:22:40 @agent_ppo2.py:144][0m Total time:      38.13 min
[32m[20230113 20:22:40 @agent_ppo2.py:146][0m 3557376 total steps have happened
[32m[20230113 20:22:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1737 --------------------------#
[32m[20230113 20:22:41 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:22:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:41 @agent_ppo2.py:186][0m |          -0.0010 |           6.8538 |          12.9538 |
[32m[20230113 20:22:41 @agent_ppo2.py:186][0m |          -0.0049 |           4.2124 |          12.9339 |
[32m[20230113 20:22:41 @agent_ppo2.py:186][0m |          -0.0084 |           3.7155 |          12.9447 |
[32m[20230113 20:22:41 @agent_ppo2.py:186][0m |          -0.0094 |           3.4598 |          12.9311 |
[32m[20230113 20:22:41 @agent_ppo2.py:186][0m |          -0.0108 |           3.2896 |          12.9295 |
[32m[20230113 20:22:41 @agent_ppo2.py:186][0m |          -0.0117 |           3.1995 |          12.9271 |
[32m[20230113 20:22:41 @agent_ppo2.py:186][0m |          -0.0121 |           3.0764 |          12.9264 |
[32m[20230113 20:22:41 @agent_ppo2.py:186][0m |          -0.0131 |           2.9915 |          12.9221 |
[32m[20230113 20:22:41 @agent_ppo2.py:186][0m |          -0.0131 |           2.9181 |          12.9172 |
[32m[20230113 20:22:41 @agent_ppo2.py:186][0m |          -0.0142 |           2.8654 |          12.9198 |
[32m[20230113 20:22:41 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.92
[32m[20230113 20:22:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.10
[32m[20230113 20:22:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.04
[32m[20230113 20:22:41 @agent_ppo2.py:144][0m Total time:      38.15 min
[32m[20230113 20:22:41 @agent_ppo2.py:146][0m 3559424 total steps have happened
[32m[20230113 20:22:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1738 --------------------------#
[32m[20230113 20:22:42 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:22:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:42 @agent_ppo2.py:186][0m |           0.0044 |           5.4347 |          12.9782 |
[32m[20230113 20:22:42 @agent_ppo2.py:186][0m |          -0.0034 |           4.5235 |          12.9630 |
[32m[20230113 20:22:42 @agent_ppo2.py:186][0m |          -0.0076 |           4.1408 |          12.9397 |
[32m[20230113 20:22:42 @agent_ppo2.py:186][0m |          -0.0049 |           3.9215 |          12.9449 |
[32m[20230113 20:22:42 @agent_ppo2.py:186][0m |          -0.0090 |           3.8217 |          12.9410 |
[32m[20230113 20:22:42 @agent_ppo2.py:186][0m |          -0.0126 |           3.5959 |          12.9428 |
[32m[20230113 20:22:42 @agent_ppo2.py:186][0m |          -0.0116 |           3.4905 |          12.9355 |
[32m[20230113 20:22:42 @agent_ppo2.py:186][0m |          -0.0102 |           3.3929 |          12.9391 |
[32m[20230113 20:22:42 @agent_ppo2.py:186][0m |          -0.0100 |           3.2866 |          12.9515 |
[32m[20230113 20:22:42 @agent_ppo2.py:186][0m |          -0.0148 |           3.2243 |          12.9337 |
[32m[20230113 20:22:42 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:22:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.68
[32m[20230113 20:22:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.20
[32m[20230113 20:22:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.63
[32m[20230113 20:22:43 @agent_ppo2.py:144][0m Total time:      38.17 min
[32m[20230113 20:22:43 @agent_ppo2.py:146][0m 3561472 total steps have happened
[32m[20230113 20:22:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1739 --------------------------#
[32m[20230113 20:22:43 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:22:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:43 @agent_ppo2.py:186][0m |          -0.0050 |           5.8302 |          12.8227 |
[32m[20230113 20:22:43 @agent_ppo2.py:186][0m |          -0.0137 |           4.6553 |          12.8172 |
[32m[20230113 20:22:43 @agent_ppo2.py:186][0m |          -0.0127 |           4.3395 |          12.7960 |
[32m[20230113 20:22:43 @agent_ppo2.py:186][0m |          -0.0147 |           4.2399 |          12.7904 |
[32m[20230113 20:22:43 @agent_ppo2.py:186][0m |          -0.0033 |           4.0592 |          12.7879 |
[32m[20230113 20:22:43 @agent_ppo2.py:186][0m |          -0.0089 |           3.8440 |          12.7843 |
[32m[20230113 20:22:44 @agent_ppo2.py:186][0m |          -0.0068 |           3.6770 |          12.7905 |
[32m[20230113 20:22:44 @agent_ppo2.py:186][0m |          -0.0081 |           3.5930 |          12.7769 |
[32m[20230113 20:22:44 @agent_ppo2.py:186][0m |          -0.0084 |           3.5107 |          12.7899 |
[32m[20230113 20:22:44 @agent_ppo2.py:186][0m |          -0.0196 |           3.4941 |          12.7853 |
[32m[20230113 20:22:44 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:22:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.00
[32m[20230113 20:22:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.10
[32m[20230113 20:22:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.79
[32m[20230113 20:22:44 @agent_ppo2.py:144][0m Total time:      38.19 min
[32m[20230113 20:22:44 @agent_ppo2.py:146][0m 3563520 total steps have happened
[32m[20230113 20:22:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1740 --------------------------#
[32m[20230113 20:22:44 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:22:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:45 @agent_ppo2.py:186][0m |           0.0012 |           5.6369 |          13.0435 |
[32m[20230113 20:22:45 @agent_ppo2.py:186][0m |          -0.0035 |           4.4960 |          13.0374 |
[32m[20230113 20:22:45 @agent_ppo2.py:186][0m |          -0.0074 |           4.0974 |          13.0209 |
[32m[20230113 20:22:45 @agent_ppo2.py:186][0m |          -0.0101 |           3.7444 |          13.0158 |
[32m[20230113 20:22:45 @agent_ppo2.py:186][0m |          -0.0111 |           3.6016 |          13.0093 |
[32m[20230113 20:22:45 @agent_ppo2.py:186][0m |          -0.0126 |           3.4506 |          13.0083 |
[32m[20230113 20:22:45 @agent_ppo2.py:186][0m |          -0.0136 |           3.4014 |          13.0086 |
[32m[20230113 20:22:45 @agent_ppo2.py:186][0m |          -0.0141 |           3.2424 |          13.0060 |
[32m[20230113 20:22:45 @agent_ppo2.py:186][0m |          -0.0159 |           3.1537 |          12.9979 |
[32m[20230113 20:22:45 @agent_ppo2.py:186][0m |          -0.0161 |           3.1286 |          13.0059 |
[32m[20230113 20:22:45 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:22:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.15
[32m[20230113 20:22:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.86
[32m[20230113 20:22:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 150.54
[32m[20230113 20:22:45 @agent_ppo2.py:144][0m Total time:      38.21 min
[32m[20230113 20:22:45 @agent_ppo2.py:146][0m 3565568 total steps have happened
[32m[20230113 20:22:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1741 --------------------------#
[32m[20230113 20:22:46 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:22:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:46 @agent_ppo2.py:186][0m |           0.0013 |           6.2211 |          13.0123 |
[32m[20230113 20:22:46 @agent_ppo2.py:186][0m |          -0.0035 |           4.6519 |          12.9984 |
[32m[20230113 20:22:46 @agent_ppo2.py:186][0m |          -0.0071 |           4.2517 |          12.9870 |
[32m[20230113 20:22:46 @agent_ppo2.py:186][0m |          -0.0090 |           4.0560 |          12.9789 |
[32m[20230113 20:22:46 @agent_ppo2.py:186][0m |          -0.0113 |           3.8610 |          12.9764 |
[32m[20230113 20:22:46 @agent_ppo2.py:186][0m |          -0.0122 |           3.7291 |          12.9675 |
[32m[20230113 20:22:46 @agent_ppo2.py:186][0m |          -0.0134 |           3.6100 |          12.9686 |
[32m[20230113 20:22:46 @agent_ppo2.py:186][0m |          -0.0149 |           3.5468 |          12.9610 |
[32m[20230113 20:22:46 @agent_ppo2.py:186][0m |          -0.0141 |           3.5029 |          12.9506 |
[32m[20230113 20:22:46 @agent_ppo2.py:186][0m |          -0.0144 |           3.4209 |          12.9543 |
[32m[20230113 20:22:46 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:22:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.20
[32m[20230113 20:22:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.20
[32m[20230113 20:22:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.59
[32m[20230113 20:22:47 @agent_ppo2.py:144][0m Total time:      38.24 min
[32m[20230113 20:22:47 @agent_ppo2.py:146][0m 3567616 total steps have happened
[32m[20230113 20:22:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1742 --------------------------#
[32m[20230113 20:22:47 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:22:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:47 @agent_ppo2.py:186][0m |          -0.0012 |           6.2357 |          12.9432 |
[32m[20230113 20:22:47 @agent_ppo2.py:186][0m |          -0.0046 |           4.8713 |          12.9374 |
[32m[20230113 20:22:47 @agent_ppo2.py:186][0m |          -0.0075 |           4.3249 |          12.9375 |
[32m[20230113 20:22:47 @agent_ppo2.py:186][0m |          -0.0052 |           4.1240 |          12.9366 |
[32m[20230113 20:22:47 @agent_ppo2.py:186][0m |          -0.0150 |           3.8830 |          12.9138 |
[32m[20230113 20:22:47 @agent_ppo2.py:186][0m |          -0.0140 |           3.6584 |          12.9257 |
[32m[20230113 20:22:48 @agent_ppo2.py:186][0m |          -0.0131 |           3.5256 |          12.9252 |
[32m[20230113 20:22:48 @agent_ppo2.py:186][0m |          -0.0141 |           3.4458 |          12.9160 |
[32m[20230113 20:22:48 @agent_ppo2.py:186][0m |          -0.0145 |           3.3412 |          12.9119 |
[32m[20230113 20:22:48 @agent_ppo2.py:186][0m |          -0.0159 |           3.2501 |          12.9220 |
[32m[20230113 20:22:48 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:22:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.75
[32m[20230113 20:22:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.42
[32m[20230113 20:22:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.52
[32m[20230113 20:22:48 @agent_ppo2.py:144][0m Total time:      38.26 min
[32m[20230113 20:22:48 @agent_ppo2.py:146][0m 3569664 total steps have happened
[32m[20230113 20:22:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1743 --------------------------#
[32m[20230113 20:22:48 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:49 @agent_ppo2.py:186][0m |           0.0016 |           7.4260 |          13.1421 |
[32m[20230113 20:22:49 @agent_ppo2.py:186][0m |          -0.0025 |           5.6418 |          13.1476 |
[32m[20230113 20:22:49 @agent_ppo2.py:186][0m |          -0.0070 |           5.0444 |          13.1341 |
[32m[20230113 20:22:49 @agent_ppo2.py:186][0m |          -0.0069 |           4.7350 |          13.1401 |
[32m[20230113 20:22:49 @agent_ppo2.py:186][0m |          -0.0095 |           4.5396 |          13.1410 |
[32m[20230113 20:22:49 @agent_ppo2.py:186][0m |          -0.0103 |           4.3648 |          13.1379 |
[32m[20230113 20:22:49 @agent_ppo2.py:186][0m |          -0.0108 |           4.2300 |          13.1439 |
[32m[20230113 20:22:49 @agent_ppo2.py:186][0m |          -0.0122 |           4.1054 |          13.1470 |
[32m[20230113 20:22:49 @agent_ppo2.py:186][0m |          -0.0136 |           4.0958 |          13.1447 |
[32m[20230113 20:22:49 @agent_ppo2.py:186][0m |          -0.0137 |           3.9951 |          13.1416 |
[32m[20230113 20:22:49 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:22:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.40
[32m[20230113 20:22:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.29
[32m[20230113 20:22:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 238.24
[32m[20230113 20:22:49 @agent_ppo2.py:144][0m Total time:      38.28 min
[32m[20230113 20:22:49 @agent_ppo2.py:146][0m 3571712 total steps have happened
[32m[20230113 20:22:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1744 --------------------------#
[32m[20230113 20:22:50 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:22:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:50 @agent_ppo2.py:186][0m |          -0.0003 |           5.9651 |          13.3489 |
[32m[20230113 20:22:50 @agent_ppo2.py:186][0m |          -0.0049 |           4.8994 |          13.3349 |
[32m[20230113 20:22:50 @agent_ppo2.py:186][0m |          -0.0041 |           4.4136 |          13.3033 |
[32m[20230113 20:22:50 @agent_ppo2.py:186][0m |          -0.0064 |           4.1454 |          13.3259 |
[32m[20230113 20:22:50 @agent_ppo2.py:186][0m |          -0.0094 |           4.0455 |          13.3083 |
[32m[20230113 20:22:50 @agent_ppo2.py:186][0m |          -0.0084 |           3.8689 |          13.3142 |
[32m[20230113 20:22:50 @agent_ppo2.py:186][0m |          -0.0092 |           3.7076 |          13.3083 |
[32m[20230113 20:22:50 @agent_ppo2.py:186][0m |          -0.0101 |           3.6243 |          13.3104 |
[32m[20230113 20:22:50 @agent_ppo2.py:186][0m |          -0.0121 |           3.6057 |          13.3131 |
[32m[20230113 20:22:50 @agent_ppo2.py:186][0m |          -0.0118 |           3.5404 |          13.3061 |
[32m[20230113 20:22:50 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:22:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.23
[32m[20230113 20:22:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.47
[32m[20230113 20:22:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.50
[32m[20230113 20:22:51 @agent_ppo2.py:144][0m Total time:      38.30 min
[32m[20230113 20:22:51 @agent_ppo2.py:146][0m 3573760 total steps have happened
[32m[20230113 20:22:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1745 --------------------------#
[32m[20230113 20:22:51 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:22:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:51 @agent_ppo2.py:186][0m |          -0.0033 |           7.0362 |          13.0594 |
[32m[20230113 20:22:51 @agent_ppo2.py:186][0m |          -0.0102 |           5.0725 |          13.0492 |
[32m[20230113 20:22:51 @agent_ppo2.py:186][0m |          -0.0122 |           4.3752 |          13.0273 |
[32m[20230113 20:22:51 @agent_ppo2.py:186][0m |          -0.0133 |           4.0792 |          13.0256 |
[32m[20230113 20:22:51 @agent_ppo2.py:186][0m |          -0.0147 |           3.8311 |          13.0202 |
[32m[20230113 20:22:51 @agent_ppo2.py:186][0m |          -0.0156 |           3.6506 |          13.0149 |
[32m[20230113 20:22:51 @agent_ppo2.py:186][0m |          -0.0162 |           3.5296 |          13.0135 |
[32m[20230113 20:22:51 @agent_ppo2.py:186][0m |          -0.0163 |           3.4238 |          13.0206 |
[32m[20230113 20:22:51 @agent_ppo2.py:186][0m |          -0.0176 |           3.3325 |          13.0072 |
[32m[20230113 20:22:51 @agent_ppo2.py:186][0m |          -0.0170 |           3.2383 |          13.0099 |
[32m[20230113 20:22:51 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.82
[32m[20230113 20:22:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.61
[32m[20230113 20:22:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.09
[32m[20230113 20:22:52 @agent_ppo2.py:144][0m Total time:      38.32 min
[32m[20230113 20:22:52 @agent_ppo2.py:146][0m 3575808 total steps have happened
[32m[20230113 20:22:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1746 --------------------------#
[32m[20230113 20:22:52 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:22:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:52 @agent_ppo2.py:186][0m |           0.0026 |           7.2304 |          12.9708 |
[32m[20230113 20:22:52 @agent_ppo2.py:186][0m |          -0.0012 |           5.3264 |          12.9568 |
[32m[20230113 20:22:52 @agent_ppo2.py:186][0m |          -0.0043 |           4.7032 |          12.9614 |
[32m[20230113 20:22:52 @agent_ppo2.py:186][0m |          -0.0069 |           4.1639 |          12.9585 |
[32m[20230113 20:22:52 @agent_ppo2.py:186][0m |          -0.0079 |           3.9517 |          12.9538 |
[32m[20230113 20:22:53 @agent_ppo2.py:186][0m |          -0.0099 |           3.7717 |          12.9563 |
[32m[20230113 20:22:53 @agent_ppo2.py:186][0m |          -0.0081 |           3.6003 |          12.9635 |
[32m[20230113 20:22:53 @agent_ppo2.py:186][0m |          -0.0100 |           3.5019 |          12.9535 |
[32m[20230113 20:22:53 @agent_ppo2.py:186][0m |          -0.0111 |           3.3458 |          12.9473 |
[32m[20230113 20:22:53 @agent_ppo2.py:186][0m |          -0.0128 |           3.3212 |          12.9431 |
[32m[20230113 20:22:53 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.62
[32m[20230113 20:22:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.13
[32m[20230113 20:22:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.31
[32m[20230113 20:22:53 @agent_ppo2.py:144][0m Total time:      38.34 min
[32m[20230113 20:22:53 @agent_ppo2.py:146][0m 3577856 total steps have happened
[32m[20230113 20:22:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1747 --------------------------#
[32m[20230113 20:22:54 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:22:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:54 @agent_ppo2.py:186][0m |          -0.0017 |           5.5033 |          12.8349 |
[32m[20230113 20:22:54 @agent_ppo2.py:186][0m |          -0.0118 |           4.1163 |          12.8310 |
[32m[20230113 20:22:54 @agent_ppo2.py:186][0m |          -0.0047 |           3.7177 |          12.7945 |
[32m[20230113 20:22:54 @agent_ppo2.py:186][0m |          -0.0140 |           3.4433 |          12.8066 |
[32m[20230113 20:22:54 @agent_ppo2.py:186][0m |          -0.0076 |           3.2801 |          12.8111 |
[32m[20230113 20:22:54 @agent_ppo2.py:186][0m |          -0.0129 |           3.1363 |          12.7973 |
[32m[20230113 20:22:54 @agent_ppo2.py:186][0m |          -0.0147 |           3.0141 |          12.7962 |
[32m[20230113 20:22:54 @agent_ppo2.py:186][0m |          -0.0154 |           2.9470 |          12.8085 |
[32m[20230113 20:22:54 @agent_ppo2.py:186][0m |          -0.0162 |           2.8697 |          12.7915 |
[32m[20230113 20:22:54 @agent_ppo2.py:186][0m |          -0.0144 |           2.7975 |          12.8055 |
[32m[20230113 20:22:54 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:22:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.19
[32m[20230113 20:22:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.02
[32m[20230113 20:22:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.83
[32m[20230113 20:22:54 @agent_ppo2.py:144][0m Total time:      38.36 min
[32m[20230113 20:22:54 @agent_ppo2.py:146][0m 3579904 total steps have happened
[32m[20230113 20:22:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1748 --------------------------#
[32m[20230113 20:22:55 @agent_ppo2.py:128][0m Sampling time: 0.32 s by 1 slaves
[32m[20230113 20:22:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:55 @agent_ppo2.py:186][0m |           0.0029 |          13.3647 |          13.0884 |
[32m[20230113 20:22:55 @agent_ppo2.py:186][0m |          -0.0030 |           5.0645 |          13.0797 |
[32m[20230113 20:22:55 @agent_ppo2.py:186][0m |          -0.0130 |           4.0989 |          13.0682 |
[32m[20230113 20:22:55 @agent_ppo2.py:186][0m |          -0.0111 |           3.7949 |          13.0504 |
[32m[20230113 20:22:55 @agent_ppo2.py:186][0m |          -0.0134 |           3.6251 |          13.0424 |
[32m[20230113 20:22:55 @agent_ppo2.py:186][0m |          -0.0154 |           3.4659 |          13.0481 |
[32m[20230113 20:22:55 @agent_ppo2.py:186][0m |          -0.0138 |           3.4068 |          13.0443 |
[32m[20230113 20:22:55 @agent_ppo2.py:186][0m |          -0.0175 |           3.3171 |          13.0413 |
[32m[20230113 20:22:55 @agent_ppo2.py:186][0m |          -0.0197 |           3.1985 |          13.0335 |
[32m[20230113 20:22:55 @agent_ppo2.py:186][0m |          -0.0192 |           3.1770 |          13.0264 |
[32m[20230113 20:22:55 @agent_ppo2.py:131][0m Policy update time: 0.33 s
[32m[20230113 20:22:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 127.76
[32m[20230113 20:22:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.90
[32m[20230113 20:22:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.90
[32m[20230113 20:22:55 @agent_ppo2.py:144][0m Total time:      38.38 min
[32m[20230113 20:22:55 @agent_ppo2.py:146][0m 3581952 total steps have happened
[32m[20230113 20:22:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1749 --------------------------#
[32m[20230113 20:22:56 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:22:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:56 @agent_ppo2.py:186][0m |           0.0023 |           6.4655 |          13.0370 |
[32m[20230113 20:22:56 @agent_ppo2.py:186][0m |          -0.0041 |           5.0781 |          13.0423 |
[32m[20230113 20:22:56 @agent_ppo2.py:186][0m |          -0.0078 |           4.5094 |          13.0239 |
[32m[20230113 20:22:56 @agent_ppo2.py:186][0m |          -0.0097 |           4.2165 |          13.0153 |
[32m[20230113 20:22:56 @agent_ppo2.py:186][0m |          -0.0105 |           3.9188 |          13.0048 |
[32m[20230113 20:22:56 @agent_ppo2.py:186][0m |          -0.0127 |           3.7942 |          13.0095 |
[32m[20230113 20:22:56 @agent_ppo2.py:186][0m |          -0.0133 |           3.6296 |          13.0128 |
[32m[20230113 20:22:56 @agent_ppo2.py:186][0m |          -0.0141 |           3.4881 |          13.0044 |
[32m[20230113 20:22:56 @agent_ppo2.py:186][0m |          -0.0120 |           3.3836 |          12.9965 |
[32m[20230113 20:22:56 @agent_ppo2.py:186][0m |          -0.0153 |           3.3097 |          12.9942 |
[32m[20230113 20:22:56 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.21
[32m[20230113 20:22:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.25
[32m[20230113 20:22:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.94
[32m[20230113 20:22:57 @agent_ppo2.py:144][0m Total time:      38.40 min
[32m[20230113 20:22:57 @agent_ppo2.py:146][0m 3584000 total steps have happened
[32m[20230113 20:22:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1750 --------------------------#
[32m[20230113 20:22:57 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:22:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:57 @agent_ppo2.py:186][0m |           0.0037 |           5.8771 |          12.9685 |
[32m[20230113 20:22:57 @agent_ppo2.py:186][0m |          -0.0048 |           4.4936 |          12.9627 |
[32m[20230113 20:22:57 @agent_ppo2.py:186][0m |          -0.0072 |           3.8880 |          12.9603 |
[32m[20230113 20:22:57 @agent_ppo2.py:186][0m |          -0.0090 |           3.5524 |          12.9415 |
[32m[20230113 20:22:57 @agent_ppo2.py:186][0m |          -0.0107 |           3.3491 |          12.9459 |
[32m[20230113 20:22:57 @agent_ppo2.py:186][0m |          -0.0108 |           3.2072 |          12.9352 |
[32m[20230113 20:22:57 @agent_ppo2.py:186][0m |          -0.0132 |           3.0760 |          12.9363 |
[32m[20230113 20:22:57 @agent_ppo2.py:186][0m |          -0.0120 |           2.9642 |          12.9322 |
[32m[20230113 20:22:58 @agent_ppo2.py:186][0m |          -0.0133 |           2.9108 |          12.9251 |
[32m[20230113 20:22:58 @agent_ppo2.py:186][0m |          -0.0140 |           2.8225 |          12.9374 |
[32m[20230113 20:22:58 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:22:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.58
[32m[20230113 20:22:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.42
[32m[20230113 20:22:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.41
[32m[20230113 20:22:58 @agent_ppo2.py:144][0m Total time:      38.42 min
[32m[20230113 20:22:58 @agent_ppo2.py:146][0m 3586048 total steps have happened
[32m[20230113 20:22:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1751 --------------------------#
[32m[20230113 20:22:58 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:22:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:22:58 @agent_ppo2.py:186][0m |           0.0028 |           6.0817 |          13.0807 |
[32m[20230113 20:22:59 @agent_ppo2.py:186][0m |          -0.0068 |           4.6185 |          13.0792 |
[32m[20230113 20:22:59 @agent_ppo2.py:186][0m |          -0.0096 |           4.0765 |          13.0714 |
[32m[20230113 20:22:59 @agent_ppo2.py:186][0m |          -0.0107 |           3.7773 |          13.0607 |
[32m[20230113 20:22:59 @agent_ppo2.py:186][0m |          -0.0125 |           3.5636 |          13.0738 |
[32m[20230113 20:22:59 @agent_ppo2.py:186][0m |          -0.0104 |           3.4238 |          13.0535 |
[32m[20230113 20:22:59 @agent_ppo2.py:186][0m |          -0.0128 |           3.2724 |          13.0662 |
[32m[20230113 20:22:59 @agent_ppo2.py:186][0m |          -0.0126 |           3.1891 |          13.0701 |
[32m[20230113 20:22:59 @agent_ppo2.py:186][0m |          -0.0155 |           3.1424 |          13.0684 |
[32m[20230113 20:22:59 @agent_ppo2.py:186][0m |          -0.0165 |           3.0566 |          13.0577 |
[32m[20230113 20:22:59 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:22:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.21
[32m[20230113 20:22:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.36
[32m[20230113 20:22:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.74
[32m[20230113 20:22:59 @agent_ppo2.py:144][0m Total time:      38.44 min
[32m[20230113 20:22:59 @agent_ppo2.py:146][0m 3588096 total steps have happened
[32m[20230113 20:22:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1752 --------------------------#
[32m[20230113 20:23:00 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:23:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:00 @agent_ppo2.py:186][0m |           0.0043 |           6.4877 |          12.9608 |
[32m[20230113 20:23:00 @agent_ppo2.py:186][0m |          -0.0033 |           4.3673 |          12.9315 |
[32m[20230113 20:23:00 @agent_ppo2.py:186][0m |          -0.0069 |           4.0131 |          12.9198 |
[32m[20230113 20:23:00 @agent_ppo2.py:186][0m |          -0.0092 |           3.7186 |          12.9304 |
[32m[20230113 20:23:00 @agent_ppo2.py:186][0m |          -0.0112 |           3.5404 |          12.9281 |
[32m[20230113 20:23:00 @agent_ppo2.py:186][0m |          -0.0080 |           3.4105 |          12.9220 |
[32m[20230113 20:23:00 @agent_ppo2.py:186][0m |          -0.0088 |           3.3502 |          12.9207 |
[32m[20230113 20:23:00 @agent_ppo2.py:186][0m |          -0.0118 |           3.2936 |          12.9288 |
[32m[20230113 20:23:00 @agent_ppo2.py:186][0m |          -0.0141 |           3.1667 |          12.9403 |
[32m[20230113 20:23:00 @agent_ppo2.py:186][0m |          -0.0163 |           3.0850 |          12.9226 |
[32m[20230113 20:23:00 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.53
[32m[20230113 20:23:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.47
[32m[20230113 20:23:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.82
[32m[20230113 20:23:00 @agent_ppo2.py:144][0m Total time:      38.47 min
[32m[20230113 20:23:00 @agent_ppo2.py:146][0m 3590144 total steps have happened
[32m[20230113 20:23:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1753 --------------------------#
[32m[20230113 20:23:01 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:01 @agent_ppo2.py:186][0m |           0.0023 |           5.9803 |          12.9395 |
[32m[20230113 20:23:01 @agent_ppo2.py:186][0m |          -0.0069 |           4.3158 |          12.9111 |
[32m[20230113 20:23:01 @agent_ppo2.py:186][0m |          -0.0060 |           3.8774 |          12.9205 |
[32m[20230113 20:23:01 @agent_ppo2.py:186][0m |          -0.0108 |           3.6098 |          12.9015 |
[32m[20230113 20:23:01 @agent_ppo2.py:186][0m |          -0.0141 |           3.4501 |          12.9126 |
[32m[20230113 20:23:01 @agent_ppo2.py:186][0m |          -0.0150 |           3.3416 |          12.9198 |
[32m[20230113 20:23:01 @agent_ppo2.py:186][0m |          -0.0085 |           3.2264 |          12.9114 |
[32m[20230113 20:23:01 @agent_ppo2.py:186][0m |          -0.0163 |           3.1549 |          12.9042 |
[32m[20230113 20:23:01 @agent_ppo2.py:186][0m |          -0.0126 |           3.0281 |          12.9262 |
[32m[20230113 20:23:01 @agent_ppo2.py:186][0m |          -0.0135 |           2.9493 |          12.9085 |
[32m[20230113 20:23:01 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.22
[32m[20230113 20:23:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.47
[32m[20230113 20:23:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.75
[32m[20230113 20:23:02 @agent_ppo2.py:144][0m Total time:      38.49 min
[32m[20230113 20:23:02 @agent_ppo2.py:146][0m 3592192 total steps have happened
[32m[20230113 20:23:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1754 --------------------------#
[32m[20230113 20:23:02 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:02 @agent_ppo2.py:186][0m |           0.0017 |           6.0222 |          13.2958 |
[32m[20230113 20:23:02 @agent_ppo2.py:186][0m |          -0.0046 |           4.6910 |          13.3009 |
[32m[20230113 20:23:02 @agent_ppo2.py:186][0m |          -0.0055 |           4.2226 |          13.2866 |
[32m[20230113 20:23:02 @agent_ppo2.py:186][0m |          -0.0084 |           3.9852 |          13.2955 |
[32m[20230113 20:23:02 @agent_ppo2.py:186][0m |          -0.0078 |           3.7806 |          13.2897 |
[32m[20230113 20:23:02 @agent_ppo2.py:186][0m |          -0.0103 |           3.6452 |          13.2941 |
[32m[20230113 20:23:03 @agent_ppo2.py:186][0m |          -0.0096 |           3.5426 |          13.2951 |
[32m[20230113 20:23:03 @agent_ppo2.py:186][0m |          -0.0108 |           3.4068 |          13.2903 |
[32m[20230113 20:23:03 @agent_ppo2.py:186][0m |          -0.0140 |           3.3751 |          13.2971 |
[32m[20230113 20:23:03 @agent_ppo2.py:186][0m |          -0.0123 |           3.2688 |          13.2877 |
[32m[20230113 20:23:03 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.24
[32m[20230113 20:23:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.31
[32m[20230113 20:23:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.65
[32m[20230113 20:23:03 @agent_ppo2.py:144][0m Total time:      38.51 min
[32m[20230113 20:23:03 @agent_ppo2.py:146][0m 3594240 total steps have happened
[32m[20230113 20:23:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1755 --------------------------#
[32m[20230113 20:23:03 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:04 @agent_ppo2.py:186][0m |           0.0012 |           6.6540 |          13.5490 |
[32m[20230113 20:23:04 @agent_ppo2.py:186][0m |          -0.0039 |           5.1237 |          13.5540 |
[32m[20230113 20:23:04 @agent_ppo2.py:186][0m |          -0.0068 |           4.6716 |          13.5548 |
[32m[20230113 20:23:04 @agent_ppo2.py:186][0m |          -0.0068 |           4.3221 |          13.5382 |
[32m[20230113 20:23:04 @agent_ppo2.py:186][0m |          -0.0112 |           4.0793 |          13.5359 |
[32m[20230113 20:23:04 @agent_ppo2.py:186][0m |          -0.0093 |           4.0116 |          13.5387 |
[32m[20230113 20:23:04 @agent_ppo2.py:186][0m |          -0.0134 |           3.8281 |          13.5323 |
[32m[20230113 20:23:04 @agent_ppo2.py:186][0m |          -0.0145 |           3.6945 |          13.5279 |
[32m[20230113 20:23:04 @agent_ppo2.py:186][0m |          -0.0101 |           3.6318 |          13.5321 |
[32m[20230113 20:23:04 @agent_ppo2.py:186][0m |          -0.0136 |           3.5273 |          13.5348 |
[32m[20230113 20:23:04 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.68
[32m[20230113 20:23:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.87
[32m[20230113 20:23:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.87
[32m[20230113 20:23:04 @agent_ppo2.py:144][0m Total time:      38.53 min
[32m[20230113 20:23:04 @agent_ppo2.py:146][0m 3596288 total steps have happened
[32m[20230113 20:23:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1756 --------------------------#
[32m[20230113 20:23:05 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:05 @agent_ppo2.py:186][0m |          -0.0024 |           7.2219 |          13.2979 |
[32m[20230113 20:23:05 @agent_ppo2.py:186][0m |          -0.0056 |           5.4161 |          13.2622 |
[32m[20230113 20:23:05 @agent_ppo2.py:186][0m |          -0.0097 |           4.6936 |          13.2599 |
[32m[20230113 20:23:05 @agent_ppo2.py:186][0m |          -0.0121 |           4.4106 |          13.2586 |
[32m[20230113 20:23:05 @agent_ppo2.py:186][0m |          -0.0158 |           4.1330 |          13.2560 |
[32m[20230113 20:23:05 @agent_ppo2.py:186][0m |          -0.0164 |           4.0674 |          13.2532 |
[32m[20230113 20:23:05 @agent_ppo2.py:186][0m |          -0.0093 |           3.8886 |          13.2360 |
[32m[20230113 20:23:05 @agent_ppo2.py:186][0m |          -0.0188 |           3.6920 |          13.2574 |
[32m[20230113 20:23:05 @agent_ppo2.py:186][0m |          -0.0153 |           3.6416 |          13.2418 |
[32m[20230113 20:23:05 @agent_ppo2.py:186][0m |          -0.0167 |           3.5284 |          13.2529 |
[32m[20230113 20:23:05 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.07
[32m[20230113 20:23:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.79
[32m[20230113 20:23:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.00
[32m[20230113 20:23:06 @agent_ppo2.py:144][0m Total time:      38.55 min
[32m[20230113 20:23:06 @agent_ppo2.py:146][0m 3598336 total steps have happened
[32m[20230113 20:23:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1757 --------------------------#
[32m[20230113 20:23:06 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:06 @agent_ppo2.py:186][0m |           0.0009 |           6.5571 |          13.1556 |
[32m[20230113 20:23:06 @agent_ppo2.py:186][0m |          -0.0038 |           4.5595 |          13.1324 |
[32m[20230113 20:23:06 @agent_ppo2.py:186][0m |          -0.0063 |           3.9982 |          13.1415 |
[32m[20230113 20:23:06 @agent_ppo2.py:186][0m |          -0.0092 |           3.8019 |          13.1344 |
[32m[20230113 20:23:06 @agent_ppo2.py:186][0m |          -0.0092 |           3.5280 |          13.1461 |
[32m[20230113 20:23:06 @agent_ppo2.py:186][0m |          -0.0128 |           3.3911 |          13.1480 |
[32m[20230113 20:23:06 @agent_ppo2.py:186][0m |          -0.0123 |           3.2672 |          13.1519 |
[32m[20230113 20:23:06 @agent_ppo2.py:186][0m |          -0.0139 |           3.1720 |          13.1423 |
[32m[20230113 20:23:06 @agent_ppo2.py:186][0m |          -0.0159 |           3.0759 |          13.1402 |
[32m[20230113 20:23:06 @agent_ppo2.py:186][0m |          -0.0146 |           3.0151 |          13.1432 |
[32m[20230113 20:23:06 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.74
[32m[20230113 20:23:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.04
[32m[20230113 20:23:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 141.27
[32m[20230113 20:23:07 @agent_ppo2.py:144][0m Total time:      38.57 min
[32m[20230113 20:23:07 @agent_ppo2.py:146][0m 3600384 total steps have happened
[32m[20230113 20:23:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1758 --------------------------#
[32m[20230113 20:23:07 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:23:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:07 @agent_ppo2.py:186][0m |           0.0060 |           6.3290 |          13.2530 |
[32m[20230113 20:23:07 @agent_ppo2.py:186][0m |          -0.0061 |           4.8749 |          13.2192 |
[32m[20230113 20:23:07 @agent_ppo2.py:186][0m |          -0.0063 |           4.4104 |          13.2068 |
[32m[20230113 20:23:07 @agent_ppo2.py:186][0m |          -0.0076 |           4.0332 |          13.2158 |
[32m[20230113 20:23:07 @agent_ppo2.py:186][0m |          -0.0069 |           3.8799 |          13.2093 |
[32m[20230113 20:23:07 @agent_ppo2.py:186][0m |          -0.0086 |           3.7095 |          13.2005 |
[32m[20230113 20:23:08 @agent_ppo2.py:186][0m |          -0.0100 |           3.6318 |          13.2064 |
[32m[20230113 20:23:08 @agent_ppo2.py:186][0m |          -0.0121 |           3.6064 |          13.2070 |
[32m[20230113 20:23:08 @agent_ppo2.py:186][0m |          -0.0092 |           3.4742 |          13.1982 |
[32m[20230113 20:23:08 @agent_ppo2.py:186][0m |          -0.0136 |           3.3937 |          13.1942 |
[32m[20230113 20:23:08 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:23:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 229.96
[32m[20230113 20:23:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.70
[32m[20230113 20:23:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.66
[32m[20230113 20:23:08 @agent_ppo2.py:144][0m Total time:      38.59 min
[32m[20230113 20:23:08 @agent_ppo2.py:146][0m 3602432 total steps have happened
[32m[20230113 20:23:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1759 --------------------------#
[32m[20230113 20:23:08 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:09 @agent_ppo2.py:186][0m |           0.0006 |           5.9621 |          13.5346 |
[32m[20230113 20:23:09 @agent_ppo2.py:186][0m |          -0.0045 |           4.5821 |          13.5242 |
[32m[20230113 20:23:09 @agent_ppo2.py:186][0m |          -0.0081 |           4.1929 |          13.5010 |
[32m[20230113 20:23:09 @agent_ppo2.py:186][0m |          -0.0110 |           3.9393 |          13.5008 |
[32m[20230113 20:23:09 @agent_ppo2.py:186][0m |          -0.0125 |           3.7617 |          13.4940 |
[32m[20230113 20:23:09 @agent_ppo2.py:186][0m |          -0.0143 |           3.6635 |          13.4909 |
[32m[20230113 20:23:09 @agent_ppo2.py:186][0m |          -0.0152 |           3.5463 |          13.4890 |
[32m[20230113 20:23:09 @agent_ppo2.py:186][0m |          -0.0161 |           3.4396 |          13.4838 |
[32m[20230113 20:23:09 @agent_ppo2.py:186][0m |          -0.0170 |           3.3465 |          13.4902 |
[32m[20230113 20:23:09 @agent_ppo2.py:186][0m |          -0.0180 |           3.3079 |          13.4830 |
[32m[20230113 20:23:09 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.77
[32m[20230113 20:23:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.78
[32m[20230113 20:23:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.76
[32m[20230113 20:23:09 @agent_ppo2.py:144][0m Total time:      38.61 min
[32m[20230113 20:23:09 @agent_ppo2.py:146][0m 3604480 total steps have happened
[32m[20230113 20:23:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1760 --------------------------#
[32m[20230113 20:23:10 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:10 @agent_ppo2.py:186][0m |           0.0014 |           6.1022 |          13.4763 |
[32m[20230113 20:23:10 @agent_ppo2.py:186][0m |          -0.0046 |           4.5758 |          13.4408 |
[32m[20230113 20:23:10 @agent_ppo2.py:186][0m |          -0.0054 |           4.2151 |          13.4693 |
[32m[20230113 20:23:10 @agent_ppo2.py:186][0m |          -0.0102 |           3.9688 |          13.4575 |
[32m[20230113 20:23:10 @agent_ppo2.py:186][0m |          -0.0110 |           3.7859 |          13.4459 |
[32m[20230113 20:23:10 @agent_ppo2.py:186][0m |          -0.0108 |           3.6767 |          13.4443 |
[32m[20230113 20:23:10 @agent_ppo2.py:186][0m |          -0.0138 |           3.4888 |          13.4486 |
[32m[20230113 20:23:10 @agent_ppo2.py:186][0m |          -0.0137 |           3.4103 |          13.4489 |
[32m[20230113 20:23:10 @agent_ppo2.py:186][0m |          -0.0156 |           3.3405 |          13.4471 |
[32m[20230113 20:23:10 @agent_ppo2.py:186][0m |          -0.0148 |           3.2895 |          13.4405 |
[32m[20230113 20:23:10 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.69
[32m[20230113 20:23:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.29
[32m[20230113 20:23:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.55
[32m[20230113 20:23:11 @agent_ppo2.py:144][0m Total time:      38.63 min
[32m[20230113 20:23:11 @agent_ppo2.py:146][0m 3606528 total steps have happened
[32m[20230113 20:23:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1761 --------------------------#
[32m[20230113 20:23:11 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:11 @agent_ppo2.py:186][0m |          -0.0022 |           6.3526 |          13.4018 |
[32m[20230113 20:23:11 @agent_ppo2.py:186][0m |          -0.0083 |           5.0640 |          13.3996 |
[32m[20230113 20:23:11 @agent_ppo2.py:186][0m |          -0.0119 |           4.5926 |          13.3896 |
[32m[20230113 20:23:11 @agent_ppo2.py:186][0m |          -0.0184 |           4.3292 |          13.3901 |
[32m[20230113 20:23:11 @agent_ppo2.py:186][0m |          -0.0149 |           4.1463 |          13.3994 |
[32m[20230113 20:23:11 @agent_ppo2.py:186][0m |          -0.0122 |           4.0036 |          13.3813 |
[32m[20230113 20:23:11 @agent_ppo2.py:186][0m |          -0.0167 |           3.9495 |          13.3923 |
[32m[20230113 20:23:11 @agent_ppo2.py:186][0m |          -0.0161 |           3.8265 |          13.3866 |
[32m[20230113 20:23:11 @agent_ppo2.py:186][0m |          -0.0173 |           3.7870 |          13.3876 |
[32m[20230113 20:23:11 @agent_ppo2.py:186][0m |          -0.0181 |           3.6991 |          13.3847 |
[32m[20230113 20:23:11 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.79
[32m[20230113 20:23:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.36
[32m[20230113 20:23:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.07
[32m[20230113 20:23:12 @agent_ppo2.py:144][0m Total time:      38.65 min
[32m[20230113 20:23:12 @agent_ppo2.py:146][0m 3608576 total steps have happened
[32m[20230113 20:23:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1762 --------------------------#
[32m[20230113 20:23:12 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:12 @agent_ppo2.py:186][0m |           0.0004 |           5.3974 |          13.6584 |
[32m[20230113 20:23:12 @agent_ppo2.py:186][0m |          -0.0058 |           4.5005 |          13.6591 |
[32m[20230113 20:23:12 @agent_ppo2.py:186][0m |          -0.0105 |           4.1732 |          13.6503 |
[32m[20230113 20:23:12 @agent_ppo2.py:186][0m |          -0.0106 |           4.0199 |          13.6452 |
[32m[20230113 20:23:13 @agent_ppo2.py:186][0m |          -0.0116 |           3.8715 |          13.6326 |
[32m[20230113 20:23:13 @agent_ppo2.py:186][0m |          -0.0124 |           3.7938 |          13.6413 |
[32m[20230113 20:23:13 @agent_ppo2.py:186][0m |          -0.0142 |           3.6966 |          13.6435 |
[32m[20230113 20:23:13 @agent_ppo2.py:186][0m |          -0.0121 |           3.6075 |          13.6314 |
[32m[20230113 20:23:13 @agent_ppo2.py:186][0m |          -0.0139 |           3.5573 |          13.6350 |
[32m[20230113 20:23:13 @agent_ppo2.py:186][0m |          -0.0155 |           3.5045 |          13.6386 |
[32m[20230113 20:23:13 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.22
[32m[20230113 20:23:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.55
[32m[20230113 20:23:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.05
[32m[20230113 20:23:13 @agent_ppo2.py:144][0m Total time:      38.68 min
[32m[20230113 20:23:13 @agent_ppo2.py:146][0m 3610624 total steps have happened
[32m[20230113 20:23:13 @agent_ppo2.py:122][0m #------------------------ Iteration 1763 --------------------------#
[32m[20230113 20:23:14 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:23:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:14 @agent_ppo2.py:186][0m |           0.0040 |           5.1967 |          13.6947 |
[32m[20230113 20:23:14 @agent_ppo2.py:186][0m |          -0.0049 |           4.3605 |          13.6731 |
[32m[20230113 20:23:14 @agent_ppo2.py:186][0m |          -0.0068 |           4.1104 |          13.6618 |
[32m[20230113 20:23:14 @agent_ppo2.py:186][0m |          -0.0076 |           3.9476 |          13.6509 |
[32m[20230113 20:23:14 @agent_ppo2.py:186][0m |          -0.0087 |           3.7536 |          13.6468 |
[32m[20230113 20:23:14 @agent_ppo2.py:186][0m |          -0.0105 |           3.6536 |          13.6456 |
[32m[20230113 20:23:14 @agent_ppo2.py:186][0m |          -0.0119 |           3.5758 |          13.6460 |
[32m[20230113 20:23:14 @agent_ppo2.py:186][0m |          -0.0116 |           3.5450 |          13.6414 |
[32m[20230113 20:23:14 @agent_ppo2.py:186][0m |          -0.0126 |           3.4441 |          13.6486 |
[32m[20230113 20:23:14 @agent_ppo2.py:186][0m |          -0.0147 |           3.3250 |          13.6393 |
[32m[20230113 20:23:14 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:23:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.05
[32m[20230113 20:23:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.12
[32m[20230113 20:23:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.61
[32m[20230113 20:23:14 @agent_ppo2.py:144][0m Total time:      38.70 min
[32m[20230113 20:23:14 @agent_ppo2.py:146][0m 3612672 total steps have happened
[32m[20230113 20:23:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1764 --------------------------#
[32m[20230113 20:23:15 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:15 @agent_ppo2.py:186][0m |          -0.0019 |           5.9656 |          13.5662 |
[32m[20230113 20:23:15 @agent_ppo2.py:186][0m |          -0.0055 |           3.9461 |          13.5303 |
[32m[20230113 20:23:15 @agent_ppo2.py:186][0m |          -0.0009 |           3.4867 |          13.5558 |
[32m[20230113 20:23:15 @agent_ppo2.py:186][0m |          -0.0104 |           3.2034 |          13.5336 |
[32m[20230113 20:23:15 @agent_ppo2.py:186][0m |          -0.0241 |           3.0742 |          13.5437 |
[32m[20230113 20:23:15 @agent_ppo2.py:186][0m |          -0.0089 |           2.9219 |          13.5359 |
[32m[20230113 20:23:15 @agent_ppo2.py:186][0m |          -0.0200 |           2.8662 |          13.5306 |
[32m[20230113 20:23:15 @agent_ppo2.py:186][0m |          -0.0109 |           2.7442 |          13.5239 |
[32m[20230113 20:23:15 @agent_ppo2.py:186][0m |          -0.0140 |           2.6745 |          13.5298 |
[32m[20230113 20:23:15 @agent_ppo2.py:186][0m |          -0.0119 |           2.5861 |          13.5253 |
[32m[20230113 20:23:15 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.52
[32m[20230113 20:23:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.97
[32m[20230113 20:23:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.01
[32m[20230113 20:23:16 @agent_ppo2.py:144][0m Total time:      38.72 min
[32m[20230113 20:23:16 @agent_ppo2.py:146][0m 3614720 total steps have happened
[32m[20230113 20:23:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1765 --------------------------#
[32m[20230113 20:23:16 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:16 @agent_ppo2.py:186][0m |          -0.0022 |           5.4801 |          13.4282 |
[32m[20230113 20:23:16 @agent_ppo2.py:186][0m |          -0.0060 |           4.3292 |          13.4049 |
[32m[20230113 20:23:16 @agent_ppo2.py:186][0m |          -0.0106 |           3.9021 |          13.4147 |
[32m[20230113 20:23:16 @agent_ppo2.py:186][0m |          -0.0105 |           3.6693 |          13.4062 |
[32m[20230113 20:23:16 @agent_ppo2.py:186][0m |          -0.0109 |           3.5815 |          13.4064 |
[32m[20230113 20:23:16 @agent_ppo2.py:186][0m |          -0.0134 |           3.4499 |          13.4065 |
[32m[20230113 20:23:16 @agent_ppo2.py:186][0m |          -0.0122 |           3.3662 |          13.4117 |
[32m[20230113 20:23:16 @agent_ppo2.py:186][0m |          -0.0125 |           3.2995 |          13.3940 |
[32m[20230113 20:23:16 @agent_ppo2.py:186][0m |          -0.0112 |           3.2236 |          13.3971 |
[32m[20230113 20:23:17 @agent_ppo2.py:186][0m |          -0.0136 |           3.1763 |          13.4086 |
[32m[20230113 20:23:17 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.15
[32m[20230113 20:23:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.00
[32m[20230113 20:23:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.94
[32m[20230113 20:23:17 @agent_ppo2.py:144][0m Total time:      38.74 min
[32m[20230113 20:23:17 @agent_ppo2.py:146][0m 3616768 total steps have happened
[32m[20230113 20:23:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1766 --------------------------#
[32m[20230113 20:23:17 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:17 @agent_ppo2.py:186][0m |          -0.0010 |           5.5714 |          13.2908 |
[32m[20230113 20:23:17 @agent_ppo2.py:186][0m |          -0.0032 |           4.3607 |          13.2683 |
[32m[20230113 20:23:18 @agent_ppo2.py:186][0m |          -0.0067 |           3.9535 |          13.2629 |
[32m[20230113 20:23:18 @agent_ppo2.py:186][0m |          -0.0116 |           3.7339 |          13.2676 |
[32m[20230113 20:23:18 @agent_ppo2.py:186][0m |           0.0047 |           3.5230 |          13.2391 |
[32m[20230113 20:23:18 @agent_ppo2.py:186][0m |          -0.0238 |           3.3792 |          13.2585 |
[32m[20230113 20:23:18 @agent_ppo2.py:186][0m |          -0.0192 |           3.3107 |          13.2713 |
[32m[20230113 20:23:18 @agent_ppo2.py:186][0m |          -0.0214 |           3.1805 |          13.2573 |
[32m[20230113 20:23:18 @agent_ppo2.py:186][0m |          -0.0105 |           3.1214 |          13.2658 |
[32m[20230113 20:23:18 @agent_ppo2.py:186][0m |          -0.0129 |           3.0222 |          13.2629 |
[32m[20230113 20:23:18 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.51
[32m[20230113 20:23:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.83
[32m[20230113 20:23:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.19
[32m[20230113 20:23:18 @agent_ppo2.py:144][0m Total time:      38.76 min
[32m[20230113 20:23:18 @agent_ppo2.py:146][0m 3618816 total steps have happened
[32m[20230113 20:23:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1767 --------------------------#
[32m[20230113 20:23:19 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:19 @agent_ppo2.py:186][0m |          -0.0005 |           5.4069 |          13.6582 |
[32m[20230113 20:23:19 @agent_ppo2.py:186][0m |          -0.0063 |           3.8913 |          13.6493 |
[32m[20230113 20:23:19 @agent_ppo2.py:186][0m |          -0.0078 |           3.5420 |          13.6610 |
[32m[20230113 20:23:19 @agent_ppo2.py:186][0m |          -0.0087 |           3.3216 |          13.6587 |
[32m[20230113 20:23:19 @agent_ppo2.py:186][0m |          -0.0108 |           3.2133 |          13.6596 |
[32m[20230113 20:23:19 @agent_ppo2.py:186][0m |          -0.0121 |           3.1113 |          13.6565 |
[32m[20230113 20:23:19 @agent_ppo2.py:186][0m |          -0.0128 |           3.0182 |          13.6574 |
[32m[20230113 20:23:19 @agent_ppo2.py:186][0m |          -0.0143 |           2.9457 |          13.6623 |
[32m[20230113 20:23:19 @agent_ppo2.py:186][0m |          -0.0151 |           2.8663 |          13.6600 |
[32m[20230113 20:23:19 @agent_ppo2.py:186][0m |          -0.0165 |           2.8497 |          13.6492 |
[32m[20230113 20:23:19 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.34
[32m[20230113 20:23:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.59
[32m[20230113 20:23:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.01
[32m[20230113 20:23:20 @agent_ppo2.py:144][0m Total time:      38.78 min
[32m[20230113 20:23:20 @agent_ppo2.py:146][0m 3620864 total steps have happened
[32m[20230113 20:23:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1768 --------------------------#
[32m[20230113 20:23:20 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:20 @agent_ppo2.py:186][0m |           0.0008 |           6.3594 |          13.7564 |
[32m[20230113 20:23:20 @agent_ppo2.py:186][0m |          -0.0061 |           4.9921 |          13.7432 |
[32m[20230113 20:23:20 @agent_ppo2.py:186][0m |          -0.0086 |           4.4851 |          13.7312 |
[32m[20230113 20:23:20 @agent_ppo2.py:186][0m |          -0.0104 |           4.2270 |          13.7163 |
[32m[20230113 20:23:20 @agent_ppo2.py:186][0m |          -0.0123 |           3.9902 |          13.7158 |
[32m[20230113 20:23:20 @agent_ppo2.py:186][0m |          -0.0123 |           3.8490 |          13.7131 |
[32m[20230113 20:23:20 @agent_ppo2.py:186][0m |          -0.0137 |           3.7374 |          13.7204 |
[32m[20230113 20:23:20 @agent_ppo2.py:186][0m |          -0.0158 |           3.6196 |          13.7084 |
[32m[20230113 20:23:20 @agent_ppo2.py:186][0m |          -0.0167 |           3.5560 |          13.7183 |
[32m[20230113 20:23:20 @agent_ppo2.py:186][0m |          -0.0167 |           3.4707 |          13.7122 |
[32m[20230113 20:23:20 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:23:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.30
[32m[20230113 20:23:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.37
[32m[20230113 20:23:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.84
[32m[20230113 20:23:21 @agent_ppo2.py:144][0m Total time:      38.80 min
[32m[20230113 20:23:21 @agent_ppo2.py:146][0m 3622912 total steps have happened
[32m[20230113 20:23:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1769 --------------------------#
[32m[20230113 20:23:21 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:21 @agent_ppo2.py:186][0m |          -0.0006 |           5.6248 |          13.5437 |
[32m[20230113 20:23:21 @agent_ppo2.py:186][0m |          -0.0041 |           4.3722 |          13.5419 |
[32m[20230113 20:23:21 @agent_ppo2.py:186][0m |          -0.0099 |           4.0159 |          13.5252 |
[32m[20230113 20:23:21 @agent_ppo2.py:186][0m |          -0.0115 |           3.7299 |          13.5189 |
[32m[20230113 20:23:21 @agent_ppo2.py:186][0m |          -0.0115 |           3.6810 |          13.5041 |
[32m[20230113 20:23:21 @agent_ppo2.py:186][0m |          -0.0170 |           3.5222 |          13.5254 |
[32m[20230113 20:23:22 @agent_ppo2.py:186][0m |          -0.0149 |           3.5004 |          13.4957 |
[32m[20230113 20:23:22 @agent_ppo2.py:186][0m |          -0.0156 |           3.4122 |          13.4969 |
[32m[20230113 20:23:22 @agent_ppo2.py:186][0m |          -0.0137 |           3.2749 |          13.4941 |
[32m[20230113 20:23:22 @agent_ppo2.py:186][0m |          -0.0114 |           3.2167 |          13.4871 |
[32m[20230113 20:23:22 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.83
[32m[20230113 20:23:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.98
[32m[20230113 20:23:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.98
[32m[20230113 20:23:22 @agent_ppo2.py:144][0m Total time:      38.82 min
[32m[20230113 20:23:22 @agent_ppo2.py:146][0m 3624960 total steps have happened
[32m[20230113 20:23:22 @agent_ppo2.py:122][0m #------------------------ Iteration 1770 --------------------------#
[32m[20230113 20:23:22 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:23 @agent_ppo2.py:186][0m |           0.0017 |           6.3706 |          13.7067 |
[32m[20230113 20:23:23 @agent_ppo2.py:186][0m |          -0.0083 |           4.3653 |          13.6963 |
[32m[20230113 20:23:23 @agent_ppo2.py:186][0m |          -0.0046 |           3.8189 |          13.6993 |
[32m[20230113 20:23:23 @agent_ppo2.py:186][0m |          -0.0141 |           3.5395 |          13.6968 |
[32m[20230113 20:23:23 @agent_ppo2.py:186][0m |          -0.0113 |           3.3868 |          13.6906 |
[32m[20230113 20:23:23 @agent_ppo2.py:186][0m |          -0.0091 |           3.2922 |          13.6803 |
[32m[20230113 20:23:23 @agent_ppo2.py:186][0m |          -0.0132 |           3.1549 |          13.6708 |
[32m[20230113 20:23:23 @agent_ppo2.py:186][0m |          -0.0147 |           3.0976 |          13.6793 |
[32m[20230113 20:23:23 @agent_ppo2.py:186][0m |          -0.0175 |           3.0001 |          13.6710 |
[32m[20230113 20:23:23 @agent_ppo2.py:186][0m |          -0.0127 |           2.9378 |          13.6743 |
[32m[20230113 20:23:23 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.41
[32m[20230113 20:23:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.48
[32m[20230113 20:23:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.20
[32m[20230113 20:23:23 @agent_ppo2.py:144][0m Total time:      38.85 min
[32m[20230113 20:23:23 @agent_ppo2.py:146][0m 3627008 total steps have happened
[32m[20230113 20:23:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1771 --------------------------#
[32m[20230113 20:23:24 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:24 @agent_ppo2.py:186][0m |          -0.0012 |           5.9428 |          13.5619 |
[32m[20230113 20:23:24 @agent_ppo2.py:186][0m |          -0.0050 |           4.7452 |          13.5384 |
[32m[20230113 20:23:24 @agent_ppo2.py:186][0m |          -0.0095 |           4.1897 |          13.5263 |
[32m[20230113 20:23:24 @agent_ppo2.py:186][0m |          -0.0010 |           3.8980 |          13.5429 |
[32m[20230113 20:23:24 @agent_ppo2.py:186][0m |          -0.0131 |           3.7087 |          13.5347 |
[32m[20230113 20:23:24 @agent_ppo2.py:186][0m |          -0.0149 |           3.5343 |          13.5348 |
[32m[20230113 20:23:24 @agent_ppo2.py:186][0m |          -0.0151 |           3.3996 |          13.5250 |
[32m[20230113 20:23:24 @agent_ppo2.py:186][0m |          -0.0184 |           3.3061 |          13.5194 |
[32m[20230113 20:23:24 @agent_ppo2.py:186][0m |          -0.0144 |           3.2192 |          13.5200 |
[32m[20230113 20:23:24 @agent_ppo2.py:186][0m |          -0.0122 |           3.1597 |          13.5189 |
[32m[20230113 20:23:24 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.90
[32m[20230113 20:23:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.22
[32m[20230113 20:23:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.55
[32m[20230113 20:23:25 @agent_ppo2.py:144][0m Total time:      38.87 min
[32m[20230113 20:23:25 @agent_ppo2.py:146][0m 3629056 total steps have happened
[32m[20230113 20:23:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1772 --------------------------#
[32m[20230113 20:23:25 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:25 @agent_ppo2.py:186][0m |           0.0021 |           5.6955 |          13.6982 |
[32m[20230113 20:23:25 @agent_ppo2.py:186][0m |          -0.0053 |           4.1638 |          13.6766 |
[32m[20230113 20:23:25 @agent_ppo2.py:186][0m |          -0.0078 |           3.6744 |          13.6692 |
[32m[20230113 20:23:25 @agent_ppo2.py:186][0m |          -0.0087 |           3.3776 |          13.6690 |
[32m[20230113 20:23:25 @agent_ppo2.py:186][0m |          -0.0114 |           3.1566 |          13.6673 |
[32m[20230113 20:23:25 @agent_ppo2.py:186][0m |          -0.0113 |           3.0137 |          13.6727 |
[32m[20230113 20:23:25 @agent_ppo2.py:186][0m |          -0.0118 |           2.9152 |          13.6694 |
[32m[20230113 20:23:25 @agent_ppo2.py:186][0m |          -0.0147 |           2.8436 |          13.6669 |
[32m[20230113 20:23:25 @agent_ppo2.py:186][0m |          -0.0146 |           2.7121 |          13.6717 |
[32m[20230113 20:23:25 @agent_ppo2.py:186][0m |          -0.0166 |           2.6347 |          13.6592 |
[32m[20230113 20:23:25 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:23:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.58
[32m[20230113 20:23:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.50
[32m[20230113 20:23:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.60
[32m[20230113 20:23:26 @agent_ppo2.py:144][0m Total time:      38.89 min
[32m[20230113 20:23:26 @agent_ppo2.py:146][0m 3631104 total steps have happened
[32m[20230113 20:23:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1773 --------------------------#
[32m[20230113 20:23:26 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:26 @agent_ppo2.py:186][0m |           0.0016 |           6.4066 |          13.6671 |
[32m[20230113 20:23:26 @agent_ppo2.py:186][0m |          -0.0056 |           4.7080 |          13.6679 |
[32m[20230113 20:23:26 @agent_ppo2.py:186][0m |          -0.0070 |           3.9968 |          13.6634 |
[32m[20230113 20:23:26 @agent_ppo2.py:186][0m |          -0.0095 |           3.6746 |          13.6566 |
[32m[20230113 20:23:26 @agent_ppo2.py:186][0m |          -0.0122 |           3.4767 |          13.6501 |
[32m[20230113 20:23:26 @agent_ppo2.py:186][0m |          -0.0129 |           3.3145 |          13.6597 |
[32m[20230113 20:23:27 @agent_ppo2.py:186][0m |          -0.0140 |           3.1306 |          13.6583 |
[32m[20230113 20:23:27 @agent_ppo2.py:186][0m |          -0.0146 |           3.0136 |          13.6582 |
[32m[20230113 20:23:27 @agent_ppo2.py:186][0m |          -0.0164 |           2.9412 |          13.6596 |
[32m[20230113 20:23:27 @agent_ppo2.py:186][0m |          -0.0167 |           2.8641 |          13.6444 |
[32m[20230113 20:23:27 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.49
[32m[20230113 20:23:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.03
[32m[20230113 20:23:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.15
[32m[20230113 20:23:27 @agent_ppo2.py:144][0m Total time:      38.91 min
[32m[20230113 20:23:27 @agent_ppo2.py:146][0m 3633152 total steps have happened
[32m[20230113 20:23:27 @agent_ppo2.py:122][0m #------------------------ Iteration 1774 --------------------------#
[32m[20230113 20:23:27 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:28 @agent_ppo2.py:186][0m |           0.0015 |           6.3950 |          13.8310 |
[32m[20230113 20:23:28 @agent_ppo2.py:186][0m |          -0.0050 |           5.1096 |          13.8256 |
[32m[20230113 20:23:28 @agent_ppo2.py:186][0m |          -0.0073 |           4.6697 |          13.8207 |
[32m[20230113 20:23:28 @agent_ppo2.py:186][0m |          -0.0102 |           4.3356 |          13.8046 |
[32m[20230113 20:23:28 @agent_ppo2.py:186][0m |          -0.0067 |           4.1694 |          13.8075 |
[32m[20230113 20:23:28 @agent_ppo2.py:186][0m |          -0.0101 |           3.9486 |          13.8131 |
[32m[20230113 20:23:28 @agent_ppo2.py:186][0m |          -0.0116 |           3.8404 |          13.8021 |
[32m[20230113 20:23:28 @agent_ppo2.py:186][0m |          -0.0124 |           3.7205 |          13.7962 |
[32m[20230113 20:23:28 @agent_ppo2.py:186][0m |          -0.0148 |           3.6216 |          13.7894 |
[32m[20230113 20:23:28 @agent_ppo2.py:186][0m |          -0.0141 |           3.5108 |          13.7774 |
[32m[20230113 20:23:28 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:23:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.68
[32m[20230113 20:23:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.88
[32m[20230113 20:23:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.82
[32m[20230113 20:23:28 @agent_ppo2.py:144][0m Total time:      38.93 min
[32m[20230113 20:23:28 @agent_ppo2.py:146][0m 3635200 total steps have happened
[32m[20230113 20:23:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1775 --------------------------#
[32m[20230113 20:23:29 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:29 @agent_ppo2.py:186][0m |           0.0014 |           7.3792 |          13.7517 |
[32m[20230113 20:23:29 @agent_ppo2.py:186][0m |          -0.0062 |           4.8069 |          13.7398 |
[32m[20230113 20:23:29 @agent_ppo2.py:186][0m |          -0.0081 |           4.0675 |          13.7377 |
[32m[20230113 20:23:29 @agent_ppo2.py:186][0m |          -0.0114 |           3.6941 |          13.7359 |
[32m[20230113 20:23:29 @agent_ppo2.py:186][0m |          -0.0121 |           3.5285 |          13.7231 |
[32m[20230113 20:23:29 @agent_ppo2.py:186][0m |          -0.0124 |           3.2931 |          13.7250 |
[32m[20230113 20:23:29 @agent_ppo2.py:186][0m |          -0.0148 |           3.1648 |          13.7262 |
[32m[20230113 20:23:29 @agent_ppo2.py:186][0m |          -0.0153 |           3.0698 |          13.7270 |
[32m[20230113 20:23:29 @agent_ppo2.py:186][0m |          -0.0166 |           2.9583 |          13.7271 |
[32m[20230113 20:23:29 @agent_ppo2.py:186][0m |          -0.0169 |           2.8816 |          13.7223 |
[32m[20230113 20:23:29 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.24
[32m[20230113 20:23:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.22
[32m[20230113 20:23:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.90
[32m[20230113 20:23:30 @agent_ppo2.py:144][0m Total time:      38.95 min
[32m[20230113 20:23:30 @agent_ppo2.py:146][0m 3637248 total steps have happened
[32m[20230113 20:23:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1776 --------------------------#
[32m[20230113 20:23:30 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:30 @agent_ppo2.py:186][0m |           0.0011 |           6.3018 |          13.6079 |
[32m[20230113 20:23:30 @agent_ppo2.py:186][0m |          -0.0056 |           4.9694 |          13.6038 |
[32m[20230113 20:23:30 @agent_ppo2.py:186][0m |          -0.0090 |           4.5506 |          13.5986 |
[32m[20230113 20:23:30 @agent_ppo2.py:186][0m |          -0.0103 |           4.3288 |          13.5993 |
[32m[20230113 20:23:30 @agent_ppo2.py:186][0m |          -0.0123 |           4.0746 |          13.6033 |
[32m[20230113 20:23:30 @agent_ppo2.py:186][0m |          -0.0113 |           4.0005 |          13.5927 |
[32m[20230113 20:23:30 @agent_ppo2.py:186][0m |          -0.0164 |           3.8227 |          13.5924 |
[32m[20230113 20:23:30 @agent_ppo2.py:186][0m |          -0.0145 |           3.7222 |          13.5878 |
[32m[20230113 20:23:30 @agent_ppo2.py:186][0m |          -0.0173 |           3.6263 |          13.5893 |
[32m[20230113 20:23:30 @agent_ppo2.py:186][0m |          -0.0184 |           3.5503 |          13.5930 |
[32m[20230113 20:23:30 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:23:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.25
[32m[20230113 20:23:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.38
[32m[20230113 20:23:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.26
[32m[20230113 20:23:31 @agent_ppo2.py:144][0m Total time:      38.97 min
[32m[20230113 20:23:31 @agent_ppo2.py:146][0m 3639296 total steps have happened
[32m[20230113 20:23:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1777 --------------------------#
[32m[20230113 20:23:31 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:23:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:31 @agent_ppo2.py:186][0m |           0.0016 |           6.5168 |          13.6827 |
[32m[20230113 20:23:31 @agent_ppo2.py:186][0m |          -0.0055 |           5.4745 |          13.6603 |
[32m[20230113 20:23:31 @agent_ppo2.py:186][0m |          -0.0094 |           4.9097 |          13.6580 |
[32m[20230113 20:23:31 @agent_ppo2.py:186][0m |          -0.0120 |           4.6023 |          13.6457 |
[32m[20230113 20:23:31 @agent_ppo2.py:186][0m |          -0.0139 |           4.4127 |          13.6499 |
[32m[20230113 20:23:32 @agent_ppo2.py:186][0m |          -0.0141 |           4.1948 |          13.6429 |
[32m[20230113 20:23:32 @agent_ppo2.py:186][0m |          -0.0156 |           3.9779 |          13.6478 |
[32m[20230113 20:23:32 @agent_ppo2.py:186][0m |          -0.0171 |           3.9323 |          13.6583 |
[32m[20230113 20:23:32 @agent_ppo2.py:186][0m |          -0.0194 |           3.7823 |          13.6479 |
[32m[20230113 20:23:32 @agent_ppo2.py:186][0m |          -0.0176 |           3.6850 |          13.6520 |
[32m[20230113 20:23:32 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:23:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.86
[32m[20230113 20:23:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.62
[32m[20230113 20:23:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.06
[32m[20230113 20:23:32 @agent_ppo2.py:144][0m Total time:      38.99 min
[32m[20230113 20:23:32 @agent_ppo2.py:146][0m 3641344 total steps have happened
[32m[20230113 20:23:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1778 --------------------------#
[32m[20230113 20:23:33 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:33 @agent_ppo2.py:186][0m |           0.0012 |           6.5466 |          13.7678 |
[32m[20230113 20:23:33 @agent_ppo2.py:186][0m |          -0.0073 |           5.4949 |          13.7348 |
[32m[20230113 20:23:33 @agent_ppo2.py:186][0m |          -0.0058 |           4.7821 |          13.7377 |
[32m[20230113 20:23:33 @agent_ppo2.py:186][0m |          -0.0099 |           4.2303 |          13.7230 |
[32m[20230113 20:23:33 @agent_ppo2.py:186][0m |          -0.0123 |           3.9559 |          13.7286 |
[32m[20230113 20:23:33 @agent_ppo2.py:186][0m |          -0.0138 |           3.7178 |          13.7357 |
[32m[20230113 20:23:33 @agent_ppo2.py:186][0m |          -0.0118 |           3.5914 |          13.7414 |
[32m[20230113 20:23:33 @agent_ppo2.py:186][0m |          -0.0102 |           3.4536 |          13.7323 |
[32m[20230113 20:23:33 @agent_ppo2.py:186][0m |          -0.0107 |           3.3432 |          13.7470 |
[32m[20230113 20:23:33 @agent_ppo2.py:186][0m |          -0.0160 |           3.2025 |          13.7401 |
[32m[20230113 20:23:33 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.77
[32m[20230113 20:23:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.20
[32m[20230113 20:23:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.58
[32m[20230113 20:23:33 @agent_ppo2.py:144][0m Total time:      39.01 min
[32m[20230113 20:23:33 @agent_ppo2.py:146][0m 3643392 total steps have happened
[32m[20230113 20:23:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1779 --------------------------#
[32m[20230113 20:23:34 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:34 @agent_ppo2.py:186][0m |           0.0016 |           6.3757 |          14.0146 |
[32m[20230113 20:23:34 @agent_ppo2.py:186][0m |          -0.0032 |           4.7875 |          14.0057 |
[32m[20230113 20:23:34 @agent_ppo2.py:186][0m |          -0.0073 |           4.2848 |          13.9855 |
[32m[20230113 20:23:34 @agent_ppo2.py:186][0m |          -0.0102 |           4.0288 |          13.9873 |
[32m[20230113 20:23:34 @agent_ppo2.py:186][0m |          -0.0112 |           3.8081 |          13.9678 |
[32m[20230113 20:23:34 @agent_ppo2.py:186][0m |          -0.0131 |           3.7138 |          13.9789 |
[32m[20230113 20:23:34 @agent_ppo2.py:186][0m |          -0.0129 |           3.4966 |          13.9716 |
[32m[20230113 20:23:34 @agent_ppo2.py:186][0m |          -0.0141 |           3.4364 |          13.9850 |
[32m[20230113 20:23:34 @agent_ppo2.py:186][0m |          -0.0150 |           3.3063 |          13.9829 |
[32m[20230113 20:23:34 @agent_ppo2.py:186][0m |          -0.0155 |           3.2362 |          13.9857 |
[32m[20230113 20:23:34 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.95
[32m[20230113 20:23:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.09
[32m[20230113 20:23:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.90
[32m[20230113 20:23:35 @agent_ppo2.py:144][0m Total time:      39.03 min
[32m[20230113 20:23:35 @agent_ppo2.py:146][0m 3645440 total steps have happened
[32m[20230113 20:23:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1780 --------------------------#
[32m[20230113 20:23:35 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:35 @agent_ppo2.py:186][0m |           0.0035 |           6.6178 |          13.6389 |
[32m[20230113 20:23:35 @agent_ppo2.py:186][0m |          -0.0047 |           5.4319 |          13.6446 |
[32m[20230113 20:23:35 @agent_ppo2.py:186][0m |          -0.0087 |           5.0012 |          13.6256 |
[32m[20230113 20:23:35 @agent_ppo2.py:186][0m |          -0.0093 |           4.6452 |          13.6122 |
[32m[20230113 20:23:35 @agent_ppo2.py:186][0m |          -0.0111 |           4.4417 |          13.6278 |
[32m[20230113 20:23:35 @agent_ppo2.py:186][0m |          -0.0122 |           4.2898 |          13.6162 |
[32m[20230113 20:23:35 @agent_ppo2.py:186][0m |          -0.0125 |           4.1432 |          13.6134 |
[32m[20230113 20:23:35 @agent_ppo2.py:186][0m |          -0.0128 |           3.9789 |          13.6111 |
[32m[20230113 20:23:35 @agent_ppo2.py:186][0m |          -0.0130 |           3.8982 |          13.6094 |
[32m[20230113 20:23:35 @agent_ppo2.py:186][0m |          -0.0132 |           3.8684 |          13.6030 |
[32m[20230113 20:23:35 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.14
[32m[20230113 20:23:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.28
[32m[20230113 20:23:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 169.21
[32m[20230113 20:23:36 @agent_ppo2.py:144][0m Total time:      39.06 min
[32m[20230113 20:23:36 @agent_ppo2.py:146][0m 3647488 total steps have happened
[32m[20230113 20:23:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1781 --------------------------#
[32m[20230113 20:23:36 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:36 @agent_ppo2.py:186][0m |           0.0005 |           6.6369 |          13.6778 |
[32m[20230113 20:23:36 @agent_ppo2.py:186][0m |          -0.0086 |           4.9968 |          13.6499 |
[32m[20230113 20:23:36 @agent_ppo2.py:186][0m |          -0.0154 |           4.2838 |          13.6377 |
[32m[20230113 20:23:36 @agent_ppo2.py:186][0m |          -0.0128 |           3.9772 |          13.6289 |
[32m[20230113 20:23:37 @agent_ppo2.py:186][0m |          -0.0081 |           3.8258 |          13.6574 |
[32m[20230113 20:23:37 @agent_ppo2.py:186][0m |          -0.0111 |           3.7135 |          13.6296 |
[32m[20230113 20:23:37 @agent_ppo2.py:186][0m |          -0.0054 |           3.4865 |          13.6337 |
[32m[20230113 20:23:37 @agent_ppo2.py:186][0m |          -0.0130 |           3.4017 |          13.6140 |
[32m[20230113 20:23:37 @agent_ppo2.py:186][0m |           0.0092 |           3.4924 |          13.6448 |
[32m[20230113 20:23:37 @agent_ppo2.py:186][0m |          -0.0151 |           3.2704 |          13.6336 |
[32m[20230113 20:23:37 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.77
[32m[20230113 20:23:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.05
[32m[20230113 20:23:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.45
[32m[20230113 20:23:37 @agent_ppo2.py:144][0m Total time:      39.08 min
[32m[20230113 20:23:37 @agent_ppo2.py:146][0m 3649536 total steps have happened
[32m[20230113 20:23:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1782 --------------------------#
[32m[20230113 20:23:38 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:23:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:38 @agent_ppo2.py:186][0m |          -0.0140 |           6.4679 |          13.5543 |
[32m[20230113 20:23:38 @agent_ppo2.py:186][0m |          -0.0098 |           4.9488 |          13.5326 |
[32m[20230113 20:23:38 @agent_ppo2.py:186][0m |          -0.0092 |           4.4874 |          13.5269 |
[32m[20230113 20:23:38 @agent_ppo2.py:186][0m |          -0.0147 |           4.2635 |          13.5108 |
[32m[20230113 20:23:38 @agent_ppo2.py:186][0m |           0.0078 |           4.1740 |          13.5166 |
[32m[20230113 20:23:38 @agent_ppo2.py:186][0m |           0.0022 |           4.0274 |          13.5007 |
[32m[20230113 20:23:38 @agent_ppo2.py:186][0m |          -0.0112 |           3.6988 |          13.5201 |
[32m[20230113 20:23:38 @agent_ppo2.py:186][0m |          -0.0298 |           3.6673 |          13.5059 |
[32m[20230113 20:23:38 @agent_ppo2.py:186][0m |          -0.0176 |           3.4886 |          13.5126 |
[32m[20230113 20:23:38 @agent_ppo2.py:186][0m |          -0.0191 |           3.4311 |          13.5062 |
[32m[20230113 20:23:38 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:23:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.85
[32m[20230113 20:23:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.15
[32m[20230113 20:23:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.21
[32m[20230113 20:23:38 @agent_ppo2.py:144][0m Total time:      39.10 min
[32m[20230113 20:23:38 @agent_ppo2.py:146][0m 3651584 total steps have happened
[32m[20230113 20:23:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1783 --------------------------#
[32m[20230113 20:23:39 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:39 @agent_ppo2.py:186][0m |          -0.0039 |           6.9278 |          13.5466 |
[32m[20230113 20:23:39 @agent_ppo2.py:186][0m |          -0.0087 |           5.1859 |          13.5307 |
[32m[20230113 20:23:39 @agent_ppo2.py:186][0m |          -0.0115 |           4.6012 |          13.5322 |
[32m[20230113 20:23:39 @agent_ppo2.py:186][0m |          -0.0128 |           4.2411 |          13.5342 |
[32m[20230113 20:23:39 @agent_ppo2.py:186][0m |          -0.0135 |           4.0190 |          13.5298 |
[32m[20230113 20:23:39 @agent_ppo2.py:186][0m |          -0.0150 |           3.7825 |          13.5321 |
[32m[20230113 20:23:39 @agent_ppo2.py:186][0m |          -0.0180 |           3.6581 |          13.5306 |
[32m[20230113 20:23:39 @agent_ppo2.py:186][0m |          -0.0155 |           3.5434 |          13.5279 |
[32m[20230113 20:23:39 @agent_ppo2.py:186][0m |          -0.0189 |           3.3933 |          13.5343 |
[32m[20230113 20:23:39 @agent_ppo2.py:186][0m |          -0.0157 |           3.3012 |          13.5323 |
[32m[20230113 20:23:39 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.59
[32m[20230113 20:23:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.23
[32m[20230113 20:23:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.63
[32m[20230113 20:23:40 @agent_ppo2.py:144][0m Total time:      39.12 min
[32m[20230113 20:23:40 @agent_ppo2.py:146][0m 3653632 total steps have happened
[32m[20230113 20:23:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1784 --------------------------#
[32m[20230113 20:23:40 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:40 @agent_ppo2.py:186][0m |          -0.0004 |           5.9451 |          13.8573 |
[32m[20230113 20:23:40 @agent_ppo2.py:186][0m |          -0.0046 |           4.5173 |          13.8333 |
[32m[20230113 20:23:40 @agent_ppo2.py:186][0m |          -0.0074 |           3.8686 |          13.8192 |
[32m[20230113 20:23:40 @agent_ppo2.py:186][0m |          -0.0088 |           3.5377 |          13.8242 |
[32m[20230113 20:23:40 @agent_ppo2.py:186][0m |          -0.0109 |           3.3493 |          13.8210 |
[32m[20230113 20:23:40 @agent_ppo2.py:186][0m |          -0.0120 |           3.2027 |          13.8084 |
[32m[20230113 20:23:40 @agent_ppo2.py:186][0m |          -0.0124 |           3.0649 |          13.8151 |
[32m[20230113 20:23:40 @agent_ppo2.py:186][0m |          -0.0129 |           2.9754 |          13.8135 |
[32m[20230113 20:23:40 @agent_ppo2.py:186][0m |          -0.0131 |           2.8993 |          13.8151 |
[32m[20230113 20:23:40 @agent_ppo2.py:186][0m |          -0.0143 |           2.8535 |          13.8113 |
[32m[20230113 20:23:40 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.83
[32m[20230113 20:23:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.74
[32m[20230113 20:23:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.65
[32m[20230113 20:23:41 @agent_ppo2.py:144][0m Total time:      39.14 min
[32m[20230113 20:23:41 @agent_ppo2.py:146][0m 3655680 total steps have happened
[32m[20230113 20:23:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1785 --------------------------#
[32m[20230113 20:23:41 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:41 @agent_ppo2.py:186][0m |           0.0022 |           6.4877 |          13.6481 |
[32m[20230113 20:23:41 @agent_ppo2.py:186][0m |          -0.0053 |           5.1665 |          13.6237 |
[32m[20230113 20:23:41 @agent_ppo2.py:186][0m |          -0.0065 |           4.5818 |          13.6253 |
[32m[20230113 20:23:42 @agent_ppo2.py:186][0m |          -0.0095 |           4.2836 |          13.6270 |
[32m[20230113 20:23:42 @agent_ppo2.py:186][0m |          -0.0087 |           4.0578 |          13.6151 |
[32m[20230113 20:23:42 @agent_ppo2.py:186][0m |          -0.0105 |           3.9059 |          13.6246 |
[32m[20230113 20:23:42 @agent_ppo2.py:186][0m |          -0.0075 |           3.8939 |          13.6214 |
[32m[20230113 20:23:42 @agent_ppo2.py:186][0m |          -0.0111 |           3.7013 |          13.6191 |
[32m[20230113 20:23:42 @agent_ppo2.py:186][0m |          -0.0145 |           3.6092 |          13.6223 |
[32m[20230113 20:23:42 @agent_ppo2.py:186][0m |          -0.0095 |           3.5667 |          13.6149 |
[32m[20230113 20:23:42 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.87
[32m[20230113 20:23:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.41
[32m[20230113 20:23:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.88
[32m[20230113 20:23:42 @agent_ppo2.py:144][0m Total time:      39.16 min
[32m[20230113 20:23:42 @agent_ppo2.py:146][0m 3657728 total steps have happened
[32m[20230113 20:23:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1786 --------------------------#
[32m[20230113 20:23:43 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:23:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:43 @agent_ppo2.py:186][0m |           0.0034 |           6.1249 |          13.4380 |
[32m[20230113 20:23:43 @agent_ppo2.py:186][0m |          -0.0058 |           5.1223 |          13.4286 |
[32m[20230113 20:23:43 @agent_ppo2.py:186][0m |          -0.0077 |           4.7603 |          13.4174 |
[32m[20230113 20:23:43 @agent_ppo2.py:186][0m |          -0.0078 |           4.4647 |          13.3847 |
[32m[20230113 20:23:43 @agent_ppo2.py:186][0m |          -0.0146 |           4.2487 |          13.3991 |
[32m[20230113 20:23:43 @agent_ppo2.py:186][0m |          -0.0081 |           4.2070 |          13.3963 |
[32m[20230113 20:23:43 @agent_ppo2.py:186][0m |          -0.0147 |           4.0213 |          13.4041 |
[32m[20230113 20:23:43 @agent_ppo2.py:186][0m |          -0.0095 |           4.0087 |          13.4032 |
[32m[20230113 20:23:43 @agent_ppo2.py:186][0m |          -0.0167 |           3.8952 |          13.3948 |
[32m[20230113 20:23:43 @agent_ppo2.py:186][0m |          -0.0173 |           3.8616 |          13.4031 |
[32m[20230113 20:23:43 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.80
[32m[20230113 20:23:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.67
[32m[20230113 20:23:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.61
[32m[20230113 20:23:43 @agent_ppo2.py:144][0m Total time:      39.18 min
[32m[20230113 20:23:43 @agent_ppo2.py:146][0m 3659776 total steps have happened
[32m[20230113 20:23:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1787 --------------------------#
[32m[20230113 20:23:44 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:23:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:44 @agent_ppo2.py:186][0m |          -0.0026 |           5.4721 |          13.6672 |
[32m[20230113 20:23:44 @agent_ppo2.py:186][0m |           0.0161 |           5.0723 |          13.6520 |
[32m[20230113 20:23:44 @agent_ppo2.py:186][0m |           0.0059 |           4.4665 |          13.6227 |
[32m[20230113 20:23:44 @agent_ppo2.py:186][0m |          -0.0024 |           4.1075 |          13.6327 |
[32m[20230113 20:23:44 @agent_ppo2.py:186][0m |          -0.0221 |           3.9337 |          13.6450 |
[32m[20230113 20:23:44 @agent_ppo2.py:186][0m |           0.0033 |           3.8948 |          13.6407 |
[32m[20230113 20:23:44 @agent_ppo2.py:186][0m |          -0.0115 |           3.7145 |          13.6290 |
[32m[20230113 20:23:44 @agent_ppo2.py:186][0m |          -0.0208 |           3.6307 |          13.6313 |
[32m[20230113 20:23:44 @agent_ppo2.py:186][0m |          -0.0247 |           3.5473 |          13.6317 |
[32m[20230113 20:23:44 @agent_ppo2.py:186][0m |          -0.0240 |           3.4863 |          13.6252 |
[32m[20230113 20:23:44 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:23:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.32
[32m[20230113 20:23:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.29
[32m[20230113 20:23:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.09
[32m[20230113 20:23:45 @agent_ppo2.py:144][0m Total time:      39.20 min
[32m[20230113 20:23:45 @agent_ppo2.py:146][0m 3661824 total steps have happened
[32m[20230113 20:23:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1788 --------------------------#
[32m[20230113 20:23:45 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:45 @agent_ppo2.py:186][0m |           0.0013 |           7.1928 |          13.6259 |
[32m[20230113 20:23:45 @agent_ppo2.py:186][0m |          -0.0070 |           5.0458 |          13.6033 |
[32m[20230113 20:23:45 @agent_ppo2.py:186][0m |          -0.0103 |           4.4285 |          13.6002 |
[32m[20230113 20:23:45 @agent_ppo2.py:186][0m |          -0.0124 |           4.1615 |          13.5997 |
[32m[20230113 20:23:45 @agent_ppo2.py:186][0m |          -0.0139 |           3.9614 |          13.5876 |
[32m[20230113 20:23:45 @agent_ppo2.py:186][0m |          -0.0155 |           3.7413 |          13.5939 |
[32m[20230113 20:23:45 @agent_ppo2.py:186][0m |          -0.0170 |           3.5764 |          13.5788 |
[32m[20230113 20:23:45 @agent_ppo2.py:186][0m |          -0.0156 |           3.5024 |          13.5707 |
[32m[20230113 20:23:46 @agent_ppo2.py:186][0m |          -0.0181 |           3.3538 |          13.5811 |
[32m[20230113 20:23:46 @agent_ppo2.py:186][0m |          -0.0182 |           3.3155 |          13.5814 |
[32m[20230113 20:23:46 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:23:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.10
[32m[20230113 20:23:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.26
[32m[20230113 20:23:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.03
[32m[20230113 20:23:46 @agent_ppo2.py:144][0m Total time:      39.22 min
[32m[20230113 20:23:46 @agent_ppo2.py:146][0m 3663872 total steps have happened
[32m[20230113 20:23:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1789 --------------------------#
[32m[20230113 20:23:46 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:46 @agent_ppo2.py:186][0m |          -0.0020 |           5.7146 |          13.7950 |
[32m[20230113 20:23:47 @agent_ppo2.py:186][0m |          -0.0065 |           4.4221 |          13.7543 |
[32m[20230113 20:23:47 @agent_ppo2.py:186][0m |          -0.0101 |           4.0453 |          13.7543 |
[32m[20230113 20:23:47 @agent_ppo2.py:186][0m |          -0.0114 |           3.8345 |          13.7284 |
[32m[20230113 20:23:47 @agent_ppo2.py:186][0m |          -0.0135 |           3.6548 |          13.7553 |
[32m[20230113 20:23:47 @agent_ppo2.py:186][0m |          -0.0138 |           3.5466 |          13.7537 |
[32m[20230113 20:23:47 @agent_ppo2.py:186][0m |          -0.0161 |           3.4487 |          13.7215 |
[32m[20230113 20:23:47 @agent_ppo2.py:186][0m |          -0.0146 |           3.3393 |          13.7478 |
[32m[20230113 20:23:47 @agent_ppo2.py:186][0m |          -0.0133 |           3.2787 |          13.7491 |
[32m[20230113 20:23:47 @agent_ppo2.py:186][0m |          -0.0167 |           3.1882 |          13.7476 |
[32m[20230113 20:23:47 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.30
[32m[20230113 20:23:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.72
[32m[20230113 20:23:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.09
[32m[20230113 20:23:47 @agent_ppo2.py:144][0m Total time:      39.24 min
[32m[20230113 20:23:47 @agent_ppo2.py:146][0m 3665920 total steps have happened
[32m[20230113 20:23:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1790 --------------------------#
[32m[20230113 20:23:48 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:23:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:48 @agent_ppo2.py:186][0m |          -0.0203 |           7.2892 |          13.7412 |
[32m[20230113 20:23:48 @agent_ppo2.py:186][0m |          -0.0123 |           5.0131 |          13.7115 |
[32m[20230113 20:23:48 @agent_ppo2.py:186][0m |          -0.0229 |           4.5720 |          13.6979 |
[32m[20230113 20:23:48 @agent_ppo2.py:186][0m |          -0.0107 |           4.2379 |          13.6826 |
[32m[20230113 20:23:48 @agent_ppo2.py:186][0m |          -0.0126 |           4.0007 |          13.6930 |
[32m[20230113 20:23:48 @agent_ppo2.py:186][0m |          -0.0114 |           3.7126 |          13.6930 |
[32m[20230113 20:23:48 @agent_ppo2.py:186][0m |          -0.0407 |           3.6025 |          13.6882 |
[32m[20230113 20:23:48 @agent_ppo2.py:186][0m |          -0.0013 |           3.4844 |          13.6739 |
[32m[20230113 20:23:48 @agent_ppo2.py:186][0m |          -0.0106 |           3.3715 |          13.6810 |
[32m[20230113 20:23:48 @agent_ppo2.py:186][0m |          -0.0249 |           3.2759 |          13.6885 |
[32m[20230113 20:23:48 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:23:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.78
[32m[20230113 20:23:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.46
[32m[20230113 20:23:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.96
[32m[20230113 20:23:48 @agent_ppo2.py:144][0m Total time:      39.26 min
[32m[20230113 20:23:48 @agent_ppo2.py:146][0m 3667968 total steps have happened
[32m[20230113 20:23:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1791 --------------------------#
[32m[20230113 20:23:49 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:49 @agent_ppo2.py:186][0m |           0.0011 |           6.4672 |          13.6566 |
[32m[20230113 20:23:49 @agent_ppo2.py:186][0m |          -0.0051 |           5.3933 |          13.6405 |
[32m[20230113 20:23:49 @agent_ppo2.py:186][0m |          -0.0063 |           4.8630 |          13.6378 |
[32m[20230113 20:23:49 @agent_ppo2.py:186][0m |          -0.0077 |           4.6531 |          13.6410 |
[32m[20230113 20:23:49 @agent_ppo2.py:186][0m |          -0.0091 |           4.4948 |          13.6352 |
[32m[20230113 20:23:49 @agent_ppo2.py:186][0m |          -0.0102 |           4.3341 |          13.6395 |
[32m[20230113 20:23:49 @agent_ppo2.py:186][0m |          -0.0110 |           4.2553 |          13.6257 |
[32m[20230113 20:23:49 @agent_ppo2.py:186][0m |          -0.0134 |           4.0874 |          13.6248 |
[32m[20230113 20:23:49 @agent_ppo2.py:186][0m |          -0.0139 |           4.0404 |          13.6228 |
[32m[20230113 20:23:49 @agent_ppo2.py:186][0m |          -0.0144 |           3.9475 |          13.6302 |
[32m[20230113 20:23:49 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.88
[32m[20230113 20:23:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 245.15
[32m[20230113 20:23:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.63
[32m[20230113 20:23:50 @agent_ppo2.py:144][0m Total time:      39.29 min
[32m[20230113 20:23:50 @agent_ppo2.py:146][0m 3670016 total steps have happened
[32m[20230113 20:23:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1792 --------------------------#
[32m[20230113 20:23:50 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:50 @agent_ppo2.py:186][0m |          -0.0021 |           6.5308 |          14.1371 |
[32m[20230113 20:23:50 @agent_ppo2.py:186][0m |          -0.0083 |           4.9263 |          14.0999 |
[32m[20230113 20:23:50 @agent_ppo2.py:186][0m |          -0.0097 |           4.4065 |          14.0847 |
[32m[20230113 20:23:50 @agent_ppo2.py:186][0m |          -0.0122 |           4.1097 |          14.0887 |
[32m[20230113 20:23:50 @agent_ppo2.py:186][0m |          -0.0138 |           3.9167 |          14.0923 |
[32m[20230113 20:23:50 @agent_ppo2.py:186][0m |          -0.0145 |           3.8033 |          14.0816 |
[32m[20230113 20:23:50 @agent_ppo2.py:186][0m |          -0.0159 |           3.6715 |          14.0862 |
[32m[20230113 20:23:50 @agent_ppo2.py:186][0m |          -0.0170 |           3.5507 |          14.0830 |
[32m[20230113 20:23:50 @agent_ppo2.py:186][0m |          -0.0175 |           3.4613 |          14.0849 |
[32m[20230113 20:23:51 @agent_ppo2.py:186][0m |          -0.0183 |           3.3695 |          14.0835 |
[32m[20230113 20:23:51 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:23:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.76
[32m[20230113 20:23:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.33
[32m[20230113 20:23:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.11
[32m[20230113 20:23:51 @agent_ppo2.py:144][0m Total time:      39.31 min
[32m[20230113 20:23:51 @agent_ppo2.py:146][0m 3672064 total steps have happened
[32m[20230113 20:23:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1793 --------------------------#
[32m[20230113 20:23:51 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:51 @agent_ppo2.py:186][0m |          -0.0009 |           6.2422 |          13.6152 |
[32m[20230113 20:23:51 @agent_ppo2.py:186][0m |          -0.0069 |           4.6757 |          13.6066 |
[32m[20230113 20:23:52 @agent_ppo2.py:186][0m |          -0.0092 |           4.0601 |          13.5871 |
[32m[20230113 20:23:52 @agent_ppo2.py:186][0m |          -0.0085 |           3.7710 |          13.5895 |
[32m[20230113 20:23:52 @agent_ppo2.py:186][0m |          -0.0092 |           3.5937 |          13.6026 |
[32m[20230113 20:23:52 @agent_ppo2.py:186][0m |          -0.0161 |           3.4415 |          13.5967 |
[32m[20230113 20:23:52 @agent_ppo2.py:186][0m |          -0.0145 |           3.3027 |          13.5897 |
[32m[20230113 20:23:52 @agent_ppo2.py:186][0m |          -0.0157 |           3.2324 |          13.5863 |
[32m[20230113 20:23:52 @agent_ppo2.py:186][0m |          -0.0152 |           3.0911 |          13.5847 |
[32m[20230113 20:23:52 @agent_ppo2.py:186][0m |          -0.0171 |           3.0223 |          13.5762 |
[32m[20230113 20:23:52 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.83
[32m[20230113 20:23:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.73
[32m[20230113 20:23:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.27
[32m[20230113 20:23:52 @agent_ppo2.py:144][0m Total time:      39.33 min
[32m[20230113 20:23:52 @agent_ppo2.py:146][0m 3674112 total steps have happened
[32m[20230113 20:23:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1794 --------------------------#
[32m[20230113 20:23:53 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:53 @agent_ppo2.py:186][0m |          -0.0032 |           6.2803 |          13.7312 |
[32m[20230113 20:23:53 @agent_ppo2.py:186][0m |          -0.0070 |           4.3735 |          13.7014 |
[32m[20230113 20:23:53 @agent_ppo2.py:186][0m |          -0.0125 |           3.7848 |          13.6828 |
[32m[20230113 20:23:53 @agent_ppo2.py:186][0m |          -0.0128 |           3.4687 |          13.6825 |
[32m[20230113 20:23:53 @agent_ppo2.py:186][0m |          -0.0158 |           3.2921 |          13.6666 |
[32m[20230113 20:23:53 @agent_ppo2.py:186][0m |          -0.0124 |           3.1454 |          13.6939 |
[32m[20230113 20:23:53 @agent_ppo2.py:186][0m |          -0.0160 |           3.0096 |          13.6648 |
[32m[20230113 20:23:53 @agent_ppo2.py:186][0m |          -0.0137 |           2.9110 |          13.6640 |
[32m[20230113 20:23:53 @agent_ppo2.py:186][0m |          -0.0187 |           2.8271 |          13.6641 |
[32m[20230113 20:23:53 @agent_ppo2.py:186][0m |          -0.0196 |           2.7256 |          13.6656 |
[32m[20230113 20:23:53 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.96
[32m[20230113 20:23:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.23
[32m[20230113 20:23:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.86
[32m[20230113 20:23:53 @agent_ppo2.py:144][0m Total time:      39.35 min
[32m[20230113 20:23:53 @agent_ppo2.py:146][0m 3676160 total steps have happened
[32m[20230113 20:23:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1795 --------------------------#
[32m[20230113 20:23:54 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:54 @agent_ppo2.py:186][0m |          -0.0010 |           5.7265 |          14.0995 |
[32m[20230113 20:23:54 @agent_ppo2.py:186][0m |          -0.0061 |           4.2770 |          14.0774 |
[32m[20230113 20:23:54 @agent_ppo2.py:186][0m |          -0.0081 |           3.7680 |          14.0752 |
[32m[20230113 20:23:54 @agent_ppo2.py:186][0m |          -0.0106 |           3.5124 |          14.0759 |
[32m[20230113 20:23:54 @agent_ppo2.py:186][0m |          -0.0105 |           3.3482 |          14.0811 |
[32m[20230113 20:23:54 @agent_ppo2.py:186][0m |          -0.0119 |           3.2109 |          14.0670 |
[32m[20230113 20:23:54 @agent_ppo2.py:186][0m |          -0.0112 |           3.0938 |          14.0703 |
[32m[20230113 20:23:54 @agent_ppo2.py:186][0m |          -0.0131 |           3.0055 |          14.0716 |
[32m[20230113 20:23:54 @agent_ppo2.py:186][0m |          -0.0143 |           2.9636 |          14.0646 |
[32m[20230113 20:23:54 @agent_ppo2.py:186][0m |          -0.0150 |           2.9142 |          14.0672 |
[32m[20230113 20:23:54 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 230.07
[32m[20230113 20:23:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 230.17
[32m[20230113 20:23:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.56
[32m[20230113 20:23:55 @agent_ppo2.py:144][0m Total time:      39.37 min
[32m[20230113 20:23:55 @agent_ppo2.py:146][0m 3678208 total steps have happened
[32m[20230113 20:23:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1796 --------------------------#
[32m[20230113 20:23:55 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:55 @agent_ppo2.py:186][0m |          -0.0015 |           6.5090 |          13.4284 |
[32m[20230113 20:23:55 @agent_ppo2.py:186][0m |          -0.0065 |           4.5381 |          13.4220 |
[32m[20230113 20:23:55 @agent_ppo2.py:186][0m |          -0.0130 |           3.9059 |          13.4216 |
[32m[20230113 20:23:55 @agent_ppo2.py:186][0m |           0.0118 |           4.0892 |          13.4175 |
[32m[20230113 20:23:55 @agent_ppo2.py:186][0m |          -0.0103 |           3.4584 |          13.3964 |
[32m[20230113 20:23:55 @agent_ppo2.py:186][0m |          -0.0256 |           3.1497 |          13.4198 |
[32m[20230113 20:23:56 @agent_ppo2.py:186][0m |          -0.0133 |           2.9808 |          13.4154 |
[32m[20230113 20:23:56 @agent_ppo2.py:186][0m |          -0.0202 |           2.9919 |          13.4215 |
[32m[20230113 20:23:56 @agent_ppo2.py:186][0m |           0.0110 |           2.8673 |          13.4143 |
[32m[20230113 20:23:56 @agent_ppo2.py:186][0m |          -0.0101 |           2.7803 |          13.3944 |
[32m[20230113 20:23:56 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.00
[32m[20230113 20:23:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.86
[32m[20230113 20:23:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.88
[32m[20230113 20:23:56 @agent_ppo2.py:144][0m Total time:      39.39 min
[32m[20230113 20:23:56 @agent_ppo2.py:146][0m 3680256 total steps have happened
[32m[20230113 20:23:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1797 --------------------------#
[32m[20230113 20:23:56 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:23:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:57 @agent_ppo2.py:186][0m |          -0.0018 |           6.4128 |          13.9373 |
[32m[20230113 20:23:57 @agent_ppo2.py:186][0m |          -0.0038 |           5.0825 |          13.9476 |
[32m[20230113 20:23:57 @agent_ppo2.py:186][0m |          -0.0055 |           4.3857 |          13.9252 |
[32m[20230113 20:23:57 @agent_ppo2.py:186][0m |          -0.0101 |           4.0456 |          13.9316 |
[32m[20230113 20:23:57 @agent_ppo2.py:186][0m |          -0.0100 |           3.7749 |          13.9178 |
[32m[20230113 20:23:57 @agent_ppo2.py:186][0m |          -0.0130 |           3.6471 |          13.9258 |
[32m[20230113 20:23:57 @agent_ppo2.py:186][0m |          -0.0145 |           3.4964 |          13.9246 |
[32m[20230113 20:23:57 @agent_ppo2.py:186][0m |          -0.0159 |           3.4361 |          13.9271 |
[32m[20230113 20:23:57 @agent_ppo2.py:186][0m |          -0.0154 |           3.2944 |          13.9318 |
[32m[20230113 20:23:57 @agent_ppo2.py:186][0m |          -0.0157 |           3.2079 |          13.9251 |
[32m[20230113 20:23:57 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:23:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.62
[32m[20230113 20:23:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.66
[32m[20230113 20:23:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.85
[32m[20230113 20:23:57 @agent_ppo2.py:144][0m Total time:      39.41 min
[32m[20230113 20:23:57 @agent_ppo2.py:146][0m 3682304 total steps have happened
[32m[20230113 20:23:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1798 --------------------------#
[32m[20230113 20:23:58 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:23:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:58 @agent_ppo2.py:186][0m |           0.0006 |           6.2279 |          13.8544 |
[32m[20230113 20:23:58 @agent_ppo2.py:186][0m |          -0.0044 |           5.0450 |          13.8231 |
[32m[20230113 20:23:58 @agent_ppo2.py:186][0m |          -0.0061 |           4.5679 |          13.8337 |
[32m[20230113 20:23:58 @agent_ppo2.py:186][0m |          -0.0094 |           4.2792 |          13.8177 |
[32m[20230113 20:23:58 @agent_ppo2.py:186][0m |          -0.0101 |           4.1011 |          13.8269 |
[32m[20230113 20:23:58 @agent_ppo2.py:186][0m |          -0.0143 |           3.9145 |          13.8154 |
[32m[20230113 20:23:58 @agent_ppo2.py:186][0m |          -0.0124 |           3.7809 |          13.8183 |
[32m[20230113 20:23:58 @agent_ppo2.py:186][0m |          -0.0153 |           3.7185 |          13.8203 |
[32m[20230113 20:23:58 @agent_ppo2.py:186][0m |          -0.0162 |           3.5855 |          13.8250 |
[32m[20230113 20:23:58 @agent_ppo2.py:186][0m |          -0.0157 |           3.5295 |          13.8079 |
[32m[20230113 20:23:58 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:23:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.88
[32m[20230113 20:23:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.85
[32m[20230113 20:23:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.85
[32m[20230113 20:23:59 @agent_ppo2.py:144][0m Total time:      39.43 min
[32m[20230113 20:23:59 @agent_ppo2.py:146][0m 3684352 total steps have happened
[32m[20230113 20:23:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1799 --------------------------#
[32m[20230113 20:23:59 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:23:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:23:59 @agent_ppo2.py:186][0m |          -0.0009 |           6.4966 |          13.6093 |
[32m[20230113 20:23:59 @agent_ppo2.py:186][0m |          -0.0114 |           5.3206 |          13.5786 |
[32m[20230113 20:23:59 @agent_ppo2.py:186][0m |          -0.0181 |           4.7855 |          13.5540 |
[32m[20230113 20:23:59 @agent_ppo2.py:186][0m |          -0.0144 |           4.4631 |          13.5599 |
[32m[20230113 20:23:59 @agent_ppo2.py:186][0m |          -0.0304 |           4.3135 |          13.5610 |
[32m[20230113 20:23:59 @agent_ppo2.py:186][0m |          -0.0174 |           4.0659 |          13.5400 |
[32m[20230113 20:23:59 @agent_ppo2.py:186][0m |          -0.0308 |           3.9549 |          13.5677 |
[32m[20230113 20:23:59 @agent_ppo2.py:186][0m |          -0.0224 |           3.9811 |          13.5499 |
[32m[20230113 20:23:59 @agent_ppo2.py:186][0m |          -0.0136 |           3.8000 |          13.5663 |
[32m[20230113 20:23:59 @agent_ppo2.py:186][0m |          -0.0065 |           3.8225 |          13.5673 |
[32m[20230113 20:23:59 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.54
[32m[20230113 20:24:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.10
[32m[20230113 20:24:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.41
[32m[20230113 20:24:00 @agent_ppo2.py:144][0m Total time:      39.45 min
[32m[20230113 20:24:00 @agent_ppo2.py:146][0m 3686400 total steps have happened
[32m[20230113 20:24:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1800 --------------------------#
[32m[20230113 20:24:00 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:00 @agent_ppo2.py:186][0m |          -0.0014 |           6.9653 |          14.0437 |
[32m[20230113 20:24:00 @agent_ppo2.py:186][0m |          -0.0081 |           5.3694 |          14.0303 |
[32m[20230113 20:24:00 @agent_ppo2.py:186][0m |          -0.0108 |           4.8973 |          14.0278 |
[32m[20230113 20:24:00 @agent_ppo2.py:186][0m |          -0.0125 |           4.6357 |          14.0249 |
[32m[20230113 20:24:00 @agent_ppo2.py:186][0m |          -0.0145 |           4.4509 |          14.0236 |
[32m[20230113 20:24:00 @agent_ppo2.py:186][0m |          -0.0158 |           4.2466 |          14.0214 |
[32m[20230113 20:24:01 @agent_ppo2.py:186][0m |          -0.0169 |           4.0975 |          14.0150 |
[32m[20230113 20:24:01 @agent_ppo2.py:186][0m |          -0.0173 |           3.9930 |          14.0184 |
[32m[20230113 20:24:01 @agent_ppo2.py:186][0m |          -0.0177 |           3.9143 |          14.0109 |
[32m[20230113 20:24:01 @agent_ppo2.py:186][0m |          -0.0192 |           3.8234 |          14.0168 |
[32m[20230113 20:24:01 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.79
[32m[20230113 20:24:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.06
[32m[20230113 20:24:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.77
[32m[20230113 20:24:01 @agent_ppo2.py:144][0m Total time:      39.47 min
[32m[20230113 20:24:01 @agent_ppo2.py:146][0m 3688448 total steps have happened
[32m[20230113 20:24:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1801 --------------------------#
[32m[20230113 20:24:01 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:02 @agent_ppo2.py:186][0m |           0.0015 |           5.7229 |          13.8252 |
[32m[20230113 20:24:02 @agent_ppo2.py:186][0m |          -0.0066 |           4.2364 |          13.8300 |
[32m[20230113 20:24:02 @agent_ppo2.py:186][0m |          -0.0079 |           3.7371 |          13.8245 |
[32m[20230113 20:24:02 @agent_ppo2.py:186][0m |          -0.0068 |           3.5240 |          13.8190 |
[32m[20230113 20:24:02 @agent_ppo2.py:186][0m |          -0.0105 |           3.3290 |          13.8122 |
[32m[20230113 20:24:02 @agent_ppo2.py:186][0m |          -0.0114 |           3.2176 |          13.8189 |
[32m[20230113 20:24:02 @agent_ppo2.py:186][0m |          -0.0141 |           3.1172 |          13.8097 |
[32m[20230113 20:24:02 @agent_ppo2.py:186][0m |          -0.0114 |           3.0348 |          13.8147 |
[32m[20230113 20:24:02 @agent_ppo2.py:186][0m |          -0.0144 |           2.9531 |          13.8035 |
[32m[20230113 20:24:02 @agent_ppo2.py:186][0m |          -0.0150 |           2.9050 |          13.8029 |
[32m[20230113 20:24:02 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.09
[32m[20230113 20:24:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.68
[32m[20230113 20:24:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.56
[32m[20230113 20:24:02 @agent_ppo2.py:144][0m Total time:      39.50 min
[32m[20230113 20:24:02 @agent_ppo2.py:146][0m 3690496 total steps have happened
[32m[20230113 20:24:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1802 --------------------------#
[32m[20230113 20:24:03 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:03 @agent_ppo2.py:186][0m |           0.0024 |           6.3845 |          13.7235 |
[32m[20230113 20:24:03 @agent_ppo2.py:186][0m |          -0.0031 |           5.0080 |          13.7289 |
[32m[20230113 20:24:03 @agent_ppo2.py:186][0m |          -0.0044 |           4.5075 |          13.7112 |
[32m[20230113 20:24:03 @agent_ppo2.py:186][0m |          -0.0062 |           4.2042 |          13.7030 |
[32m[20230113 20:24:03 @agent_ppo2.py:186][0m |          -0.0087 |           3.9273 |          13.7127 |
[32m[20230113 20:24:03 @agent_ppo2.py:186][0m |          -0.0093 |           3.7320 |          13.7070 |
[32m[20230113 20:24:03 @agent_ppo2.py:186][0m |          -0.0104 |           3.6054 |          13.7057 |
[32m[20230113 20:24:03 @agent_ppo2.py:186][0m |          -0.0111 |           3.4865 |          13.6909 |
[32m[20230113 20:24:03 @agent_ppo2.py:186][0m |          -0.0121 |           3.3552 |          13.7061 |
[32m[20230113 20:24:03 @agent_ppo2.py:186][0m |          -0.0131 |           3.2787 |          13.7123 |
[32m[20230113 20:24:03 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.35
[32m[20230113 20:24:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.45
[32m[20230113 20:24:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.42
[32m[20230113 20:24:04 @agent_ppo2.py:144][0m Total time:      39.52 min
[32m[20230113 20:24:04 @agent_ppo2.py:146][0m 3692544 total steps have happened
[32m[20230113 20:24:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1803 --------------------------#
[32m[20230113 20:24:04 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:04 @agent_ppo2.py:186][0m |          -0.0008 |           6.4098 |          13.8762 |
[32m[20230113 20:24:04 @agent_ppo2.py:186][0m |          -0.0057 |           5.1474 |          13.8674 |
[32m[20230113 20:24:04 @agent_ppo2.py:186][0m |          -0.0084 |           4.5691 |          13.8831 |
[32m[20230113 20:24:04 @agent_ppo2.py:186][0m |          -0.0119 |           4.2130 |          13.8810 |
[32m[20230113 20:24:04 @agent_ppo2.py:186][0m |          -0.0128 |           3.9456 |          13.8710 |
[32m[20230113 20:24:04 @agent_ppo2.py:186][0m |          -0.0160 |           3.8031 |          13.8704 |
[32m[20230113 20:24:04 @agent_ppo2.py:186][0m |          -0.0157 |           3.6460 |          13.8745 |
[32m[20230113 20:24:04 @agent_ppo2.py:186][0m |          -0.0165 |           3.5702 |          13.8698 |
[32m[20230113 20:24:04 @agent_ppo2.py:186][0m |          -0.0183 |           3.4076 |          13.8763 |
[32m[20230113 20:24:04 @agent_ppo2.py:186][0m |          -0.0189 |           3.2816 |          13.8724 |
[32m[20230113 20:24:04 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.47
[32m[20230113 20:24:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.63
[32m[20230113 20:24:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.91
[32m[20230113 20:24:05 @agent_ppo2.py:144][0m Total time:      39.54 min
[32m[20230113 20:24:05 @agent_ppo2.py:146][0m 3694592 total steps have happened
[32m[20230113 20:24:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1804 --------------------------#
[32m[20230113 20:24:05 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:24:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:05 @agent_ppo2.py:186][0m |           0.0025 |           6.3433 |          13.6055 |
[32m[20230113 20:24:05 @agent_ppo2.py:186][0m |          -0.0062 |           4.7631 |          13.5840 |
[32m[20230113 20:24:05 @agent_ppo2.py:186][0m |          -0.0080 |           4.2528 |          13.5704 |
[32m[20230113 20:24:05 @agent_ppo2.py:186][0m |          -0.0085 |           3.9815 |          13.5785 |
[32m[20230113 20:24:06 @agent_ppo2.py:186][0m |          -0.0120 |           3.7120 |          13.5549 |
[32m[20230113 20:24:06 @agent_ppo2.py:186][0m |          -0.0111 |           3.6365 |          13.5684 |
[32m[20230113 20:24:06 @agent_ppo2.py:186][0m |          -0.0111 |           3.5019 |          13.5633 |
[32m[20230113 20:24:06 @agent_ppo2.py:186][0m |          -0.0122 |           3.4304 |          13.5547 |
[32m[20230113 20:24:06 @agent_ppo2.py:186][0m |          -0.0144 |           3.2774 |          13.5533 |
[32m[20230113 20:24:06 @agent_ppo2.py:186][0m |          -0.0145 |           3.2090 |          13.5567 |
[32m[20230113 20:24:06 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.39
[32m[20230113 20:24:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.87
[32m[20230113 20:24:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 125.23
[32m[20230113 20:24:06 @agent_ppo2.py:144][0m Total time:      39.56 min
[32m[20230113 20:24:06 @agent_ppo2.py:146][0m 3696640 total steps have happened
[32m[20230113 20:24:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1805 --------------------------#
[32m[20230113 20:24:06 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:24:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:07 @agent_ppo2.py:186][0m |          -0.0024 |           6.3790 |          13.5719 |
[32m[20230113 20:24:07 @agent_ppo2.py:186][0m |          -0.0051 |           4.5336 |          13.5595 |
[32m[20230113 20:24:07 @agent_ppo2.py:186][0m |          -0.0109 |           3.9407 |          13.5448 |
[32m[20230113 20:24:07 @agent_ppo2.py:186][0m |          -0.0141 |           3.6310 |          13.5423 |
[32m[20230113 20:24:07 @agent_ppo2.py:186][0m |          -0.0193 |           3.4295 |          13.5277 |
[32m[20230113 20:24:07 @agent_ppo2.py:186][0m |          -0.0147 |           3.3005 |          13.5440 |
[32m[20230113 20:24:07 @agent_ppo2.py:186][0m |          -0.0163 |           3.1879 |          13.5339 |
[32m[20230113 20:24:07 @agent_ppo2.py:186][0m |          -0.0167 |           3.0213 |          13.5185 |
[32m[20230113 20:24:07 @agent_ppo2.py:186][0m |          -0.0197 |           2.9724 |          13.5210 |
[32m[20230113 20:24:07 @agent_ppo2.py:186][0m |          -0.0173 |           2.8907 |          13.5344 |
[32m[20230113 20:24:07 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.50
[32m[20230113 20:24:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.69
[32m[20230113 20:24:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.49
[32m[20230113 20:24:07 @agent_ppo2.py:144][0m Total time:      39.58 min
[32m[20230113 20:24:07 @agent_ppo2.py:146][0m 3698688 total steps have happened
[32m[20230113 20:24:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1806 --------------------------#
[32m[20230113 20:24:08 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:24:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:08 @agent_ppo2.py:186][0m |          -0.0021 |           5.2086 |          13.7180 |
[32m[20230113 20:24:08 @agent_ppo2.py:186][0m |          -0.0081 |           3.9323 |          13.7092 |
[32m[20230113 20:24:08 @agent_ppo2.py:186][0m |          -0.0104 |           3.6397 |          13.7017 |
[32m[20230113 20:24:08 @agent_ppo2.py:186][0m |          -0.0121 |           3.5199 |          13.7025 |
[32m[20230113 20:24:08 @agent_ppo2.py:186][0m |          -0.0129 |           3.3224 |          13.6975 |
[32m[20230113 20:24:08 @agent_ppo2.py:186][0m |          -0.0140 |           3.2130 |          13.6960 |
[32m[20230113 20:24:08 @agent_ppo2.py:186][0m |          -0.0163 |           3.1209 |          13.7041 |
[32m[20230113 20:24:08 @agent_ppo2.py:186][0m |          -0.0154 |           3.0555 |          13.6976 |
[32m[20230113 20:24:08 @agent_ppo2.py:186][0m |          -0.0167 |           3.0154 |          13.6973 |
[32m[20230113 20:24:08 @agent_ppo2.py:186][0m |          -0.0169 |           2.9411 |          13.7070 |
[32m[20230113 20:24:08 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.60
[32m[20230113 20:24:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.61
[32m[20230113 20:24:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.70
[32m[20230113 20:24:09 @agent_ppo2.py:144][0m Total time:      39.60 min
[32m[20230113 20:24:09 @agent_ppo2.py:146][0m 3700736 total steps have happened
[32m[20230113 20:24:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1807 --------------------------#
[32m[20230113 20:24:09 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:24:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:09 @agent_ppo2.py:186][0m |          -0.0002 |           6.4667 |          13.7722 |
[32m[20230113 20:24:09 @agent_ppo2.py:186][0m |          -0.0060 |           4.7418 |          13.7355 |
[32m[20230113 20:24:09 @agent_ppo2.py:186][0m |          -0.0097 |           4.1310 |          13.7411 |
[32m[20230113 20:24:09 @agent_ppo2.py:186][0m |          -0.0126 |           3.7647 |          13.7262 |
[32m[20230113 20:24:09 @agent_ppo2.py:186][0m |          -0.0136 |           3.5339 |          13.7277 |
[32m[20230113 20:24:09 @agent_ppo2.py:186][0m |          -0.0148 |           3.3959 |          13.7249 |
[32m[20230113 20:24:09 @agent_ppo2.py:186][0m |          -0.0172 |           3.2531 |          13.7288 |
[32m[20230113 20:24:09 @agent_ppo2.py:186][0m |          -0.0170 |           3.1966 |          13.7135 |
[32m[20230113 20:24:09 @agent_ppo2.py:186][0m |          -0.0169 |           3.0862 |          13.7121 |
[32m[20230113 20:24:09 @agent_ppo2.py:186][0m |          -0.0187 |           3.0528 |          13.7181 |
[32m[20230113 20:24:09 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:24:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.26
[32m[20230113 20:24:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.03
[32m[20230113 20:24:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.04
[32m[20230113 20:24:10 @agent_ppo2.py:144][0m Total time:      39.62 min
[32m[20230113 20:24:10 @agent_ppo2.py:146][0m 3702784 total steps have happened
[32m[20230113 20:24:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1808 --------------------------#
[32m[20230113 20:24:10 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:10 @agent_ppo2.py:186][0m |           0.0029 |           7.0797 |          13.7767 |
[32m[20230113 20:24:10 @agent_ppo2.py:186][0m |          -0.0067 |           4.5347 |          13.7826 |
[32m[20230113 20:24:10 @agent_ppo2.py:186][0m |          -0.0061 |           3.8118 |          13.7861 |
[32m[20230113 20:24:10 @agent_ppo2.py:186][0m |          -0.0124 |           3.5020 |          13.7816 |
[32m[20230113 20:24:10 @agent_ppo2.py:186][0m |          -0.0133 |           3.2853 |          13.7779 |
[32m[20230113 20:24:10 @agent_ppo2.py:186][0m |          -0.0125 |           3.1593 |          13.7660 |
[32m[20230113 20:24:11 @agent_ppo2.py:186][0m |          -0.0147 |           3.0460 |          13.7777 |
[32m[20230113 20:24:11 @agent_ppo2.py:186][0m |          -0.0154 |           2.9705 |          13.7739 |
[32m[20230113 20:24:11 @agent_ppo2.py:186][0m |          -0.0158 |           2.8956 |          13.7758 |
[32m[20230113 20:24:11 @agent_ppo2.py:186][0m |          -0.0175 |           2.8300 |          13.7641 |
[32m[20230113 20:24:11 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.18
[32m[20230113 20:24:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.25
[32m[20230113 20:24:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.82
[32m[20230113 20:24:11 @agent_ppo2.py:144][0m Total time:      39.64 min
[32m[20230113 20:24:11 @agent_ppo2.py:146][0m 3704832 total steps have happened
[32m[20230113 20:24:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1809 --------------------------#
[32m[20230113 20:24:11 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:12 @agent_ppo2.py:186][0m |          -0.0005 |           6.5439 |          13.6358 |
[32m[20230113 20:24:12 @agent_ppo2.py:186][0m |          -0.0106 |           5.0942 |          13.6206 |
[32m[20230113 20:24:12 @agent_ppo2.py:186][0m |          -0.0070 |           4.4171 |          13.6182 |
[32m[20230113 20:24:12 @agent_ppo2.py:186][0m |          -0.0105 |           4.4123 |          13.6089 |
[32m[20230113 20:24:12 @agent_ppo2.py:186][0m |          -0.0085 |           3.8336 |          13.6093 |
[32m[20230113 20:24:12 @agent_ppo2.py:186][0m |          -0.0094 |           3.7288 |          13.6154 |
[32m[20230113 20:24:12 @agent_ppo2.py:186][0m |          -0.0141 |           3.6797 |          13.6112 |
[32m[20230113 20:24:12 @agent_ppo2.py:186][0m |          -0.0146 |           3.4730 |          13.6171 |
[32m[20230113 20:24:12 @agent_ppo2.py:186][0m |          -0.0187 |           3.3775 |          13.6184 |
[32m[20230113 20:24:12 @agent_ppo2.py:186][0m |          -0.0195 |           3.2739 |          13.6178 |
[32m[20230113 20:24:12 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.12
[32m[20230113 20:24:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.25
[32m[20230113 20:24:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.25
[32m[20230113 20:24:12 @agent_ppo2.py:144][0m Total time:      39.66 min
[32m[20230113 20:24:12 @agent_ppo2.py:146][0m 3706880 total steps have happened
[32m[20230113 20:24:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1810 --------------------------#
[32m[20230113 20:24:13 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:24:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:13 @agent_ppo2.py:186][0m |           0.0015 |          14.2521 |          13.8423 |
[32m[20230113 20:24:13 @agent_ppo2.py:186][0m |          -0.0076 |           5.3751 |          13.8222 |
[32m[20230113 20:24:13 @agent_ppo2.py:186][0m |          -0.0117 |           4.4023 |          13.8228 |
[32m[20230113 20:24:13 @agent_ppo2.py:186][0m |          -0.0141 |           3.8963 |          13.8252 |
[32m[20230113 20:24:13 @agent_ppo2.py:186][0m |          -0.0154 |           3.6586 |          13.8330 |
[32m[20230113 20:24:13 @agent_ppo2.py:186][0m |          -0.0169 |           3.4550 |          13.8371 |
[32m[20230113 20:24:13 @agent_ppo2.py:186][0m |          -0.0158 |           3.3089 |          13.8252 |
[32m[20230113 20:24:13 @agent_ppo2.py:186][0m |          -0.0173 |           3.1781 |          13.8308 |
[32m[20230113 20:24:13 @agent_ppo2.py:186][0m |          -0.0176 |           3.1077 |          13.8265 |
[32m[20230113 20:24:13 @agent_ppo2.py:186][0m |          -0.0191 |           3.0414 |          13.8399 |
[32m[20230113 20:24:13 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:24:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 170.38
[32m[20230113 20:24:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.04
[32m[20230113 20:24:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.87
[32m[20230113 20:24:14 @agent_ppo2.py:144][0m Total time:      39.68 min
[32m[20230113 20:24:14 @agent_ppo2.py:146][0m 3708928 total steps have happened
[32m[20230113 20:24:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1811 --------------------------#
[32m[20230113 20:24:14 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:24:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:14 @agent_ppo2.py:186][0m |           0.0022 |           6.6197 |          13.6603 |
[32m[20230113 20:24:14 @agent_ppo2.py:186][0m |          -0.0071 |           5.3732 |          13.6372 |
[32m[20230113 20:24:14 @agent_ppo2.py:186][0m |          -0.0102 |           4.7832 |          13.6296 |
[32m[20230113 20:24:14 @agent_ppo2.py:186][0m |          -0.0135 |           4.3580 |          13.6298 |
[32m[20230113 20:24:14 @agent_ppo2.py:186][0m |          -0.0140 |           4.1079 |          13.6223 |
[32m[20230113 20:24:14 @agent_ppo2.py:186][0m |          -0.0117 |           3.8956 |          13.6235 |
[32m[20230113 20:24:14 @agent_ppo2.py:186][0m |          -0.0181 |           3.7335 |          13.6248 |
[32m[20230113 20:24:14 @agent_ppo2.py:186][0m |          -0.0184 |           3.5839 |          13.6213 |
[32m[20230113 20:24:14 @agent_ppo2.py:186][0m |          -0.0159 |           3.4848 |          13.6150 |
[32m[20230113 20:24:14 @agent_ppo2.py:186][0m |          -0.0184 |           3.4590 |          13.6137 |
[32m[20230113 20:24:14 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:24:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.23
[32m[20230113 20:24:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.15
[32m[20230113 20:24:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 151.07
[32m[20230113 20:24:15 @agent_ppo2.py:144][0m Total time:      39.71 min
[32m[20230113 20:24:15 @agent_ppo2.py:146][0m 3710976 total steps have happened
[32m[20230113 20:24:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1812 --------------------------#
[32m[20230113 20:24:15 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:15 @agent_ppo2.py:186][0m |           0.0006 |           6.3454 |          13.8630 |
[32m[20230113 20:24:15 @agent_ppo2.py:186][0m |          -0.0093 |           4.8409 |          13.8566 |
[32m[20230113 20:24:16 @agent_ppo2.py:186][0m |          -0.0102 |           4.3262 |          13.8552 |
[32m[20230113 20:24:16 @agent_ppo2.py:186][0m |          -0.0099 |           4.0763 |          13.8478 |
[32m[20230113 20:24:16 @agent_ppo2.py:186][0m |          -0.0128 |           3.9275 |          13.8437 |
[32m[20230113 20:24:16 @agent_ppo2.py:186][0m |          -0.0141 |           3.7108 |          13.8278 |
[32m[20230113 20:24:16 @agent_ppo2.py:186][0m |          -0.0126 |           3.5725 |          13.8406 |
[32m[20230113 20:24:16 @agent_ppo2.py:186][0m |          -0.0136 |           3.4666 |          13.8289 |
[32m[20230113 20:24:16 @agent_ppo2.py:186][0m |          -0.0126 |           3.4386 |          13.8239 |
[32m[20230113 20:24:16 @agent_ppo2.py:186][0m |          -0.0145 |           3.3065 |          13.8237 |
[32m[20230113 20:24:16 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:24:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.13
[32m[20230113 20:24:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.42
[32m[20230113 20:24:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.66
[32m[20230113 20:24:16 @agent_ppo2.py:144][0m Total time:      39.73 min
[32m[20230113 20:24:16 @agent_ppo2.py:146][0m 3713024 total steps have happened
[32m[20230113 20:24:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1813 --------------------------#
[32m[20230113 20:24:17 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:24:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:17 @agent_ppo2.py:186][0m |           0.0023 |           5.8372 |          13.5434 |
[32m[20230113 20:24:17 @agent_ppo2.py:186][0m |          -0.0048 |           4.0972 |          13.5272 |
[32m[20230113 20:24:17 @agent_ppo2.py:186][0m |          -0.0065 |           3.6498 |          13.5105 |
[32m[20230113 20:24:17 @agent_ppo2.py:186][0m |          -0.0122 |           3.4099 |          13.5242 |
[32m[20230113 20:24:17 @agent_ppo2.py:186][0m |          -0.0120 |           3.2450 |          13.5327 |
[32m[20230113 20:24:17 @agent_ppo2.py:186][0m |          -0.0143 |           3.0605 |          13.5266 |
[32m[20230113 20:24:17 @agent_ppo2.py:186][0m |          -0.0162 |           2.9863 |          13.5253 |
[32m[20230113 20:24:17 @agent_ppo2.py:186][0m |          -0.0172 |           2.9015 |          13.5220 |
[32m[20230113 20:24:17 @agent_ppo2.py:186][0m |          -0.0173 |           2.8454 |          13.5279 |
[32m[20230113 20:24:17 @agent_ppo2.py:186][0m |          -0.0189 |           2.7704 |          13.5241 |
[32m[20230113 20:24:17 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.81
[32m[20230113 20:24:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.00
[32m[20230113 20:24:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.62
[32m[20230113 20:24:17 @agent_ppo2.py:144][0m Total time:      39.75 min
[32m[20230113 20:24:17 @agent_ppo2.py:146][0m 3715072 total steps have happened
[32m[20230113 20:24:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1814 --------------------------#
[32m[20230113 20:24:18 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:24:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:18 @agent_ppo2.py:186][0m |          -0.0019 |           5.5081 |          14.1314 |
[32m[20230113 20:24:18 @agent_ppo2.py:186][0m |          -0.0085 |           4.1892 |          14.1248 |
[32m[20230113 20:24:18 @agent_ppo2.py:186][0m |          -0.0106 |           3.7944 |          14.1282 |
[32m[20230113 20:24:18 @agent_ppo2.py:186][0m |          -0.0125 |           3.5610 |          14.1077 |
[32m[20230113 20:24:18 @agent_ppo2.py:186][0m |          -0.0130 |           3.4077 |          14.1271 |
[32m[20230113 20:24:18 @agent_ppo2.py:186][0m |          -0.0136 |           3.3138 |          14.1321 |
[32m[20230113 20:24:18 @agent_ppo2.py:186][0m |          -0.0138 |           3.2049 |          14.1125 |
[32m[20230113 20:24:18 @agent_ppo2.py:186][0m |          -0.0143 |           3.1283 |          14.1164 |
[32m[20230113 20:24:18 @agent_ppo2.py:186][0m |          -0.0157 |           3.0670 |          14.1256 |
[32m[20230113 20:24:18 @agent_ppo2.py:186][0m |          -0.0186 |           3.0027 |          14.1151 |
[32m[20230113 20:24:18 @agent_ppo2.py:131][0m Policy update time: 0.54 s
[32m[20230113 20:24:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.61
[32m[20230113 20:24:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.48
[32m[20230113 20:24:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.88
[32m[20230113 20:24:19 @agent_ppo2.py:144][0m Total time:      39.77 min
[32m[20230113 20:24:19 @agent_ppo2.py:146][0m 3717120 total steps have happened
[32m[20230113 20:24:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1815 --------------------------#
[32m[20230113 20:24:19 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:19 @agent_ppo2.py:186][0m |           0.0016 |           5.5073 |          13.6149 |
[32m[20230113 20:24:19 @agent_ppo2.py:186][0m |          -0.0147 |           4.5072 |          13.5906 |
[32m[20230113 20:24:19 @agent_ppo2.py:186][0m |          -0.0050 |           4.3494 |          13.5901 |
[32m[20230113 20:24:19 @agent_ppo2.py:186][0m |          -0.0100 |           3.8941 |          13.5739 |
[32m[20230113 20:24:20 @agent_ppo2.py:186][0m |          -0.0105 |           3.8357 |          13.5735 |
[32m[20230113 20:24:20 @agent_ppo2.py:186][0m |          -0.0004 |           3.9950 |          13.5704 |
[32m[20230113 20:24:20 @agent_ppo2.py:186][0m |          -0.0156 |           3.6640 |          13.5634 |
[32m[20230113 20:24:20 @agent_ppo2.py:186][0m |          -0.0159 |           3.5273 |          13.5621 |
[32m[20230113 20:24:20 @agent_ppo2.py:186][0m |          -0.0123 |           3.3594 |          13.5762 |
[32m[20230113 20:24:20 @agent_ppo2.py:186][0m |          -0.0233 |           3.3280 |          13.5665 |
[32m[20230113 20:24:20 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:24:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.00
[32m[20230113 20:24:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.57
[32m[20230113 20:24:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.27
[32m[20230113 20:24:20 @agent_ppo2.py:144][0m Total time:      39.79 min
[32m[20230113 20:24:20 @agent_ppo2.py:146][0m 3719168 total steps have happened
[32m[20230113 20:24:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1816 --------------------------#
[32m[20230113 20:24:21 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:21 @agent_ppo2.py:186][0m |           0.0021 |           6.1042 |          13.9267 |
[32m[20230113 20:24:21 @agent_ppo2.py:186][0m |          -0.0072 |           4.5982 |          13.9163 |
[32m[20230113 20:24:21 @agent_ppo2.py:186][0m |          -0.0054 |           4.1088 |          13.9101 |
[32m[20230113 20:24:21 @agent_ppo2.py:186][0m |          -0.0051 |           3.8546 |          13.9074 |
[32m[20230113 20:24:21 @agent_ppo2.py:186][0m |          -0.0091 |           3.6755 |          13.9108 |
[32m[20230113 20:24:21 @agent_ppo2.py:186][0m |          -0.0125 |           3.5544 |          13.8935 |
[32m[20230113 20:24:21 @agent_ppo2.py:186][0m |          -0.0117 |           3.4303 |          13.8940 |
[32m[20230113 20:24:21 @agent_ppo2.py:186][0m |          -0.0106 |           3.3289 |          13.8981 |
[32m[20230113 20:24:21 @agent_ppo2.py:186][0m |          -0.0136 |           3.2143 |          13.8984 |
[32m[20230113 20:24:21 @agent_ppo2.py:186][0m |          -0.0132 |           3.1747 |          13.8858 |
[32m[20230113 20:24:21 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.56
[32m[20230113 20:24:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.84
[32m[20230113 20:24:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.68
[32m[20230113 20:24:21 @agent_ppo2.py:144][0m Total time:      39.81 min
[32m[20230113 20:24:21 @agent_ppo2.py:146][0m 3721216 total steps have happened
[32m[20230113 20:24:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1817 --------------------------#
[32m[20230113 20:24:22 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:22 @agent_ppo2.py:186][0m |           0.0003 |           6.1575 |          13.6618 |
[32m[20230113 20:24:22 @agent_ppo2.py:186][0m |          -0.0055 |           4.7911 |          13.6346 |
[32m[20230113 20:24:22 @agent_ppo2.py:186][0m |          -0.0131 |           4.3906 |          13.6458 |
[32m[20230113 20:24:22 @agent_ppo2.py:186][0m |          -0.0091 |           4.2025 |          13.6429 |
[32m[20230113 20:24:22 @agent_ppo2.py:186][0m |          -0.0123 |           3.9420 |          13.6407 |
[32m[20230113 20:24:22 @agent_ppo2.py:186][0m |          -0.0149 |           3.7498 |          13.6336 |
[32m[20230113 20:24:22 @agent_ppo2.py:186][0m |          -0.0082 |           3.6813 |          13.6345 |
[32m[20230113 20:24:22 @agent_ppo2.py:186][0m |          -0.0122 |           3.5918 |          13.6404 |
[32m[20230113 20:24:22 @agent_ppo2.py:186][0m |          -0.0146 |           3.4396 |          13.6338 |
[32m[20230113 20:24:22 @agent_ppo2.py:186][0m |          -0.0160 |           3.3727 |          13.6348 |
[32m[20230113 20:24:22 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.80
[32m[20230113 20:24:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.05
[32m[20230113 20:24:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.85
[32m[20230113 20:24:23 @agent_ppo2.py:144][0m Total time:      39.84 min
[32m[20230113 20:24:23 @agent_ppo2.py:146][0m 3723264 total steps have happened
[32m[20230113 20:24:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1818 --------------------------#
[32m[20230113 20:24:23 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:24:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:23 @agent_ppo2.py:186][0m |          -0.0024 |           6.4826 |          13.6512 |
[32m[20230113 20:24:23 @agent_ppo2.py:186][0m |          -0.0078 |           4.6574 |          13.6317 |
[32m[20230113 20:24:23 @agent_ppo2.py:186][0m |          -0.0083 |           4.0899 |          13.6410 |
[32m[20230113 20:24:23 @agent_ppo2.py:186][0m |          -0.0097 |           3.8293 |          13.6245 |
[32m[20230113 20:24:23 @agent_ppo2.py:186][0m |          -0.0141 |           3.5543 |          13.6325 |
[32m[20230113 20:24:23 @agent_ppo2.py:186][0m |          -0.0083 |           3.4907 |          13.6172 |
[32m[20230113 20:24:23 @agent_ppo2.py:186][0m |          -0.0162 |           3.3140 |          13.6193 |
[32m[20230113 20:24:24 @agent_ppo2.py:186][0m |          -0.0176 |           3.2017 |          13.6136 |
[32m[20230113 20:24:24 @agent_ppo2.py:186][0m |          -0.0154 |           3.1082 |          13.6133 |
[32m[20230113 20:24:24 @agent_ppo2.py:186][0m |          -0.0171 |           3.0528 |          13.6169 |
[32m[20230113 20:24:24 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:24:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 231.18
[32m[20230113 20:24:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.78
[32m[20230113 20:24:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.32
[32m[20230113 20:24:24 @agent_ppo2.py:144][0m Total time:      39.86 min
[32m[20230113 20:24:24 @agent_ppo2.py:146][0m 3725312 total steps have happened
[32m[20230113 20:24:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1819 --------------------------#
[32m[20230113 20:24:25 @agent_ppo2.py:128][0m Sampling time: 0.54 s by 1 slaves
[32m[20230113 20:24:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:25 @agent_ppo2.py:186][0m |           0.0003 |          11.4596 |          13.7712 |
[32m[20230113 20:24:25 @agent_ppo2.py:186][0m |          -0.0024 |           5.5407 |          13.7426 |
[32m[20230113 20:24:25 @agent_ppo2.py:186][0m |          -0.0099 |           4.6114 |          13.7491 |
[32m[20230113 20:24:25 @agent_ppo2.py:186][0m |          -0.0121 |           4.1528 |          13.7475 |
[32m[20230113 20:24:25 @agent_ppo2.py:186][0m |          -0.0147 |           3.8912 |          13.7519 |
[32m[20230113 20:24:25 @agent_ppo2.py:186][0m |          -0.0148 |           3.6752 |          13.7352 |
[32m[20230113 20:24:25 @agent_ppo2.py:186][0m |          -0.0157 |           3.5262 |          13.7422 |
[32m[20230113 20:24:25 @agent_ppo2.py:186][0m |          -0.0185 |           3.3617 |          13.7324 |
[32m[20230113 20:24:25 @agent_ppo2.py:186][0m |          -0.0183 |           3.2812 |          13.7406 |
[32m[20230113 20:24:25 @agent_ppo2.py:186][0m |          -0.0195 |           3.2107 |          13.7402 |
[32m[20230113 20:24:25 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:24:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 134.75
[32m[20230113 20:24:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.95
[32m[20230113 20:24:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 78.59
[32m[20230113 20:24:25 @agent_ppo2.py:144][0m Total time:      39.88 min
[32m[20230113 20:24:25 @agent_ppo2.py:146][0m 3727360 total steps have happened
[32m[20230113 20:24:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1820 --------------------------#
[32m[20230113 20:24:26 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:26 @agent_ppo2.py:186][0m |          -0.0006 |           6.2120 |          13.7090 |
[32m[20230113 20:24:26 @agent_ppo2.py:186][0m |          -0.0079 |           4.3144 |          13.7078 |
[32m[20230113 20:24:26 @agent_ppo2.py:186][0m |          -0.0129 |           3.7400 |          13.7007 |
[32m[20230113 20:24:26 @agent_ppo2.py:186][0m |          -0.0155 |           3.4338 |          13.6838 |
[32m[20230113 20:24:26 @agent_ppo2.py:186][0m |          -0.0143 |           3.2254 |          13.7035 |
[32m[20230113 20:24:26 @agent_ppo2.py:186][0m |          -0.0161 |           3.0530 |          13.6969 |
[32m[20230113 20:24:26 @agent_ppo2.py:186][0m |          -0.0169 |           2.9372 |          13.6891 |
[32m[20230113 20:24:26 @agent_ppo2.py:186][0m |          -0.0191 |           2.8416 |          13.6962 |
[32m[20230113 20:24:26 @agent_ppo2.py:186][0m |          -0.0187 |           2.7622 |          13.6935 |
[32m[20230113 20:24:26 @agent_ppo2.py:186][0m |          -0.0191 |           2.6920 |          13.6830 |
[32m[20230113 20:24:26 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.83
[32m[20230113 20:24:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.96
[32m[20230113 20:24:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.10
[32m[20230113 20:24:27 @agent_ppo2.py:144][0m Total time:      39.90 min
[32m[20230113 20:24:27 @agent_ppo2.py:146][0m 3729408 total steps have happened
[32m[20230113 20:24:27 @agent_ppo2.py:122][0m #------------------------ Iteration 1821 --------------------------#
[32m[20230113 20:24:27 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:27 @agent_ppo2.py:186][0m |           0.0021 |           5.7767 |          13.8920 |
[32m[20230113 20:24:27 @agent_ppo2.py:186][0m |          -0.0032 |           4.5073 |          13.8746 |
[32m[20230113 20:24:27 @agent_ppo2.py:186][0m |          -0.0047 |           3.9017 |          13.8720 |
[32m[20230113 20:24:27 @agent_ppo2.py:186][0m |           0.0116 |           4.0370 |          13.8562 |
[32m[20230113 20:24:27 @agent_ppo2.py:186][0m |          -0.0163 |           3.6618 |          13.8660 |
[32m[20230113 20:24:27 @agent_ppo2.py:186][0m |          -0.0182 |           3.3582 |          13.8531 |
[32m[20230113 20:24:27 @agent_ppo2.py:186][0m |          -0.0153 |           3.2340 |          13.8623 |
[32m[20230113 20:24:28 @agent_ppo2.py:186][0m |          -0.0161 |           3.1571 |          13.8475 |
[32m[20230113 20:24:28 @agent_ppo2.py:186][0m |           0.0040 |           3.1247 |          13.8495 |
[32m[20230113 20:24:28 @agent_ppo2.py:186][0m |          -0.0063 |           3.0337 |          13.8217 |
[32m[20230113 20:24:28 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.21
[32m[20230113 20:24:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.20
[32m[20230113 20:24:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.87
[32m[20230113 20:24:28 @agent_ppo2.py:144][0m Total time:      39.92 min
[32m[20230113 20:24:28 @agent_ppo2.py:146][0m 3731456 total steps have happened
[32m[20230113 20:24:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1822 --------------------------#
[32m[20230113 20:24:28 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:24:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:29 @agent_ppo2.py:186][0m |          -0.0029 |           7.4469 |          13.9530 |
[32m[20230113 20:24:29 @agent_ppo2.py:186][0m |          -0.0059 |           5.2465 |          13.9400 |
[32m[20230113 20:24:29 @agent_ppo2.py:186][0m |          -0.0077 |           4.6507 |          13.9342 |
[32m[20230113 20:24:29 @agent_ppo2.py:186][0m |          -0.0075 |           4.2229 |          13.9239 |
[32m[20230113 20:24:29 @agent_ppo2.py:186][0m |          -0.0084 |           3.9190 |          13.9393 |
[32m[20230113 20:24:29 @agent_ppo2.py:186][0m |          -0.0142 |           3.7061 |          13.9294 |
[32m[20230113 20:24:29 @agent_ppo2.py:186][0m |          -0.0152 |           3.5280 |          13.9190 |
[32m[20230113 20:24:29 @agent_ppo2.py:186][0m |          -0.0163 |           3.3817 |          13.9211 |
[32m[20230113 20:24:29 @agent_ppo2.py:186][0m |          -0.0134 |           3.2802 |          13.9306 |
[32m[20230113 20:24:29 @agent_ppo2.py:186][0m |          -0.0142 |           3.1880 |          13.9261 |
[32m[20230113 20:24:29 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.13
[32m[20230113 20:24:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.19
[32m[20230113 20:24:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.69
[32m[20230113 20:24:29 @agent_ppo2.py:144][0m Total time:      39.94 min
[32m[20230113 20:24:29 @agent_ppo2.py:146][0m 3733504 total steps have happened
[32m[20230113 20:24:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1823 --------------------------#
[32m[20230113 20:24:30 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:24:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:30 @agent_ppo2.py:186][0m |           0.0019 |           5.7685 |          13.7142 |
[32m[20230113 20:24:30 @agent_ppo2.py:186][0m |          -0.0043 |           4.4246 |          13.7061 |
[32m[20230113 20:24:30 @agent_ppo2.py:186][0m |          -0.0078 |           3.9585 |          13.6914 |
[32m[20230113 20:24:30 @agent_ppo2.py:186][0m |          -0.0091 |           3.6886 |          13.6803 |
[32m[20230113 20:24:30 @agent_ppo2.py:186][0m |          -0.0149 |           3.4880 |          13.6896 |
[32m[20230113 20:24:30 @agent_ppo2.py:186][0m |          -0.0124 |           3.3341 |          13.6951 |
[32m[20230113 20:24:30 @agent_ppo2.py:186][0m |          -0.0147 |           3.1847 |          13.6857 |
[32m[20230113 20:24:30 @agent_ppo2.py:186][0m |          -0.0154 |           3.0671 |          13.6947 |
[32m[20230113 20:24:30 @agent_ppo2.py:186][0m |          -0.0160 |           2.9947 |          13.6757 |
[32m[20230113 20:24:30 @agent_ppo2.py:186][0m |          -0.0149 |           2.9079 |          13.6834 |
[32m[20230113 20:24:30 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.30
[32m[20230113 20:24:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.50
[32m[20230113 20:24:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.88
[32m[20230113 20:24:31 @agent_ppo2.py:144][0m Total time:      39.97 min
[32m[20230113 20:24:31 @agent_ppo2.py:146][0m 3735552 total steps have happened
[32m[20230113 20:24:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1824 --------------------------#
[32m[20230113 20:24:31 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:24:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:31 @agent_ppo2.py:186][0m |           0.0082 |           7.0783 |          13.9866 |
[32m[20230113 20:24:31 @agent_ppo2.py:186][0m |          -0.0011 |           5.3195 |          13.9843 |
[32m[20230113 20:24:31 @agent_ppo2.py:186][0m |          -0.0001 |           4.5810 |          13.9779 |
[32m[20230113 20:24:31 @agent_ppo2.py:186][0m |          -0.0022 |           4.1788 |          13.9908 |
[32m[20230113 20:24:31 @agent_ppo2.py:186][0m |           0.0030 |           4.0786 |          13.9883 |
[32m[20230113 20:24:31 @agent_ppo2.py:186][0m |          -0.0186 |           3.6980 |          13.9623 |
[32m[20230113 20:24:31 @agent_ppo2.py:186][0m |          -0.0144 |           3.3943 |          13.9688 |
[32m[20230113 20:24:31 @agent_ppo2.py:186][0m |          -0.0169 |           3.3270 |          13.9830 |
[32m[20230113 20:24:31 @agent_ppo2.py:186][0m |           0.0077 |           3.1485 |          13.9711 |
[32m[20230113 20:24:31 @agent_ppo2.py:186][0m |          -0.0162 |           3.0766 |          13.9614 |
[32m[20230113 20:24:31 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.96
[32m[20230113 20:24:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.42
[32m[20230113 20:24:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.58
[32m[20230113 20:24:32 @agent_ppo2.py:144][0m Total time:      39.99 min
[32m[20230113 20:24:32 @agent_ppo2.py:146][0m 3737600 total steps have happened
[32m[20230113 20:24:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1825 --------------------------#
[32m[20230113 20:24:32 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:32 @agent_ppo2.py:186][0m |          -0.0008 |           5.9564 |          14.1221 |
[32m[20230113 20:24:32 @agent_ppo2.py:186][0m |          -0.0056 |           4.7762 |          14.0979 |
[32m[20230113 20:24:32 @agent_ppo2.py:186][0m |          -0.0099 |           4.2139 |          14.0864 |
[32m[20230113 20:24:32 @agent_ppo2.py:186][0m |          -0.0112 |           3.9228 |          14.0767 |
[32m[20230113 20:24:32 @agent_ppo2.py:186][0m |          -0.0120 |           3.7006 |          14.0683 |
[32m[20230113 20:24:32 @agent_ppo2.py:186][0m |          -0.0116 |           3.5730 |          14.0650 |
[32m[20230113 20:24:33 @agent_ppo2.py:186][0m |          -0.0130 |           3.4227 |          14.0629 |
[32m[20230113 20:24:33 @agent_ppo2.py:186][0m |          -0.0144 |           3.2920 |          14.0675 |
[32m[20230113 20:24:33 @agent_ppo2.py:186][0m |          -0.0148 |           3.2033 |          14.0682 |
[32m[20230113 20:24:33 @agent_ppo2.py:186][0m |          -0.0150 |           3.1354 |          14.0630 |
[32m[20230113 20:24:33 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.51
[32m[20230113 20:24:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.10
[32m[20230113 20:24:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.63
[32m[20230113 20:24:33 @agent_ppo2.py:144][0m Total time:      40.01 min
[32m[20230113 20:24:33 @agent_ppo2.py:146][0m 3739648 total steps have happened
[32m[20230113 20:24:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1826 --------------------------#
[32m[20230113 20:24:33 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:34 @agent_ppo2.py:186][0m |          -0.0040 |           6.4964 |          13.6766 |
[32m[20230113 20:24:34 @agent_ppo2.py:186][0m |          -0.0075 |           5.1574 |          13.6611 |
[32m[20230113 20:24:34 @agent_ppo2.py:186][0m |          -0.0078 |           4.7377 |          13.6614 |
[32m[20230113 20:24:34 @agent_ppo2.py:186][0m |          -0.0077 |           4.3337 |          13.6595 |
[32m[20230113 20:24:34 @agent_ppo2.py:186][0m |          -0.0081 |           4.1094 |          13.6481 |
[32m[20230113 20:24:34 @agent_ppo2.py:186][0m |          -0.0095 |           3.9506 |          13.6558 |
[32m[20230113 20:24:34 @agent_ppo2.py:186][0m |          -0.0101 |           3.7397 |          13.6564 |
[32m[20230113 20:24:34 @agent_ppo2.py:186][0m |          -0.0088 |           3.6673 |          13.6484 |
[32m[20230113 20:24:34 @agent_ppo2.py:186][0m |          -0.0116 |           3.5121 |          13.6469 |
[32m[20230113 20:24:34 @agent_ppo2.py:186][0m |          -0.0134 |           3.3943 |          13.6575 |
[32m[20230113 20:24:34 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.65
[32m[20230113 20:24:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.43
[32m[20230113 20:24:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.73
[32m[20230113 20:24:34 @agent_ppo2.py:144][0m Total time:      40.03 min
[32m[20230113 20:24:34 @agent_ppo2.py:146][0m 3741696 total steps have happened
[32m[20230113 20:24:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1827 --------------------------#
[32m[20230113 20:24:35 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:24:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:35 @agent_ppo2.py:186][0m |          -0.0080 |           7.0595 |          14.1318 |
[32m[20230113 20:24:35 @agent_ppo2.py:186][0m |          -0.0065 |           5.4552 |          14.1226 |
[32m[20230113 20:24:35 @agent_ppo2.py:186][0m |          -0.0069 |           4.9211 |          14.1196 |
[32m[20230113 20:24:35 @agent_ppo2.py:186][0m |          -0.0111 |           4.6741 |          14.1119 |
[32m[20230113 20:24:35 @agent_ppo2.py:186][0m |          -0.0136 |           4.2450 |          14.1062 |
[32m[20230113 20:24:35 @agent_ppo2.py:186][0m |          -0.0149 |           3.9883 |          14.1159 |
[32m[20230113 20:24:35 @agent_ppo2.py:186][0m |          -0.0097 |           3.8504 |          14.1131 |
[32m[20230113 20:24:35 @agent_ppo2.py:186][0m |          -0.0144 |           3.6897 |          14.1182 |
[32m[20230113 20:24:35 @agent_ppo2.py:186][0m |          -0.0139 |           3.5838 |          14.1201 |
[32m[20230113 20:24:35 @agent_ppo2.py:186][0m |          -0.0123 |           3.5133 |          14.1214 |
[32m[20230113 20:24:35 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:24:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.74
[32m[20230113 20:24:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.86
[32m[20230113 20:24:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.99
[32m[20230113 20:24:36 @agent_ppo2.py:144][0m Total time:      40.05 min
[32m[20230113 20:24:36 @agent_ppo2.py:146][0m 3743744 total steps have happened
[32m[20230113 20:24:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1828 --------------------------#
[32m[20230113 20:24:36 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:24:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:36 @agent_ppo2.py:186][0m |          -0.0010 |           7.1110 |          13.8736 |
[32m[20230113 20:24:36 @agent_ppo2.py:186][0m |          -0.0048 |           4.2619 |          13.8642 |
[32m[20230113 20:24:36 @agent_ppo2.py:186][0m |          -0.0082 |           3.6248 |          13.8588 |
[32m[20230113 20:24:36 @agent_ppo2.py:186][0m |          -0.0065 |           3.3293 |          13.8509 |
[32m[20230113 20:24:36 @agent_ppo2.py:186][0m |          -0.0103 |           3.1142 |          13.8578 |
[32m[20230113 20:24:36 @agent_ppo2.py:186][0m |          -0.0118 |           3.0016 |          13.8496 |
[32m[20230113 20:24:36 @agent_ppo2.py:186][0m |          -0.0132 |           2.9520 |          13.8518 |
[32m[20230113 20:24:36 @agent_ppo2.py:186][0m |          -0.0133 |           2.8420 |          13.8439 |
[32m[20230113 20:24:36 @agent_ppo2.py:186][0m |          -0.0147 |           2.7702 |          13.8377 |
[32m[20230113 20:24:36 @agent_ppo2.py:186][0m |          -0.0179 |           2.7382 |          13.8390 |
[32m[20230113 20:24:36 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:24:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.64
[32m[20230113 20:24:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.91
[32m[20230113 20:24:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.06
[32m[20230113 20:24:37 @agent_ppo2.py:144][0m Total time:      40.07 min
[32m[20230113 20:24:37 @agent_ppo2.py:146][0m 3745792 total steps have happened
[32m[20230113 20:24:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1829 --------------------------#
[32m[20230113 20:24:37 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:37 @agent_ppo2.py:186][0m |          -0.0025 |           6.1171 |          13.8330 |
[32m[20230113 20:24:37 @agent_ppo2.py:186][0m |          -0.0114 |           4.6496 |          13.8123 |
[32m[20230113 20:24:37 @agent_ppo2.py:186][0m |          -0.0111 |           4.2268 |          13.8094 |
[32m[20230113 20:24:37 @agent_ppo2.py:186][0m |          -0.0135 |           3.8875 |          13.8062 |
[32m[20230113 20:24:37 @agent_ppo2.py:186][0m |          -0.0162 |           3.6857 |          13.8084 |
[32m[20230113 20:24:37 @agent_ppo2.py:186][0m |          -0.0144 |           3.5701 |          13.8096 |
[32m[20230113 20:24:37 @agent_ppo2.py:186][0m |          -0.0183 |           3.4453 |          13.8067 |
[32m[20230113 20:24:38 @agent_ppo2.py:186][0m |          -0.0162 |           3.3358 |          13.8089 |
[32m[20230113 20:24:38 @agent_ppo2.py:186][0m |          -0.0184 |           3.2352 |          13.8080 |
[32m[20230113 20:24:38 @agent_ppo2.py:186][0m |          -0.0192 |           3.1609 |          13.8079 |
[32m[20230113 20:24:38 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.40
[32m[20230113 20:24:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.92
[32m[20230113 20:24:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 239.18
[32m[20230113 20:24:38 @agent_ppo2.py:144][0m Total time:      40.09 min
[32m[20230113 20:24:38 @agent_ppo2.py:146][0m 3747840 total steps have happened
[32m[20230113 20:24:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1830 --------------------------#
[32m[20230113 20:24:38 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:24:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:39 @agent_ppo2.py:186][0m |          -0.0003 |           6.0684 |          14.0363 |
[32m[20230113 20:24:39 @agent_ppo2.py:186][0m |          -0.0071 |           3.8441 |          14.0354 |
[32m[20230113 20:24:39 @agent_ppo2.py:186][0m |          -0.0077 |           3.3980 |          14.0198 |
[32m[20230113 20:24:39 @agent_ppo2.py:186][0m |          -0.0110 |           3.1677 |          14.0195 |
[32m[20230113 20:24:39 @agent_ppo2.py:186][0m |          -0.0120 |           2.9966 |          14.0127 |
[32m[20230113 20:24:39 @agent_ppo2.py:186][0m |          -0.0111 |           2.8686 |          14.0092 |
[32m[20230113 20:24:39 @agent_ppo2.py:186][0m |          -0.0125 |           2.7901 |          14.0048 |
[32m[20230113 20:24:39 @agent_ppo2.py:186][0m |          -0.0132 |           2.7026 |          14.0008 |
[32m[20230113 20:24:39 @agent_ppo2.py:186][0m |          -0.0154 |           2.6443 |          14.0108 |
[32m[20230113 20:24:39 @agent_ppo2.py:186][0m |          -0.0151 |           2.5992 |          14.0044 |
[32m[20230113 20:24:39 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.03
[32m[20230113 20:24:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.08
[32m[20230113 20:24:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.62
[32m[20230113 20:24:39 @agent_ppo2.py:144][0m Total time:      40.11 min
[32m[20230113 20:24:39 @agent_ppo2.py:146][0m 3749888 total steps have happened
[32m[20230113 20:24:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1831 --------------------------#
[32m[20230113 20:24:40 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:24:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:40 @agent_ppo2.py:186][0m |           0.0035 |           6.9366 |          14.1490 |
[32m[20230113 20:24:40 @agent_ppo2.py:186][0m |          -0.0083 |           4.5973 |          14.1558 |
[32m[20230113 20:24:40 @agent_ppo2.py:186][0m |          -0.0104 |           3.9519 |          14.1446 |
[32m[20230113 20:24:40 @agent_ppo2.py:186][0m |          -0.0087 |           3.5240 |          14.1408 |
[32m[20230113 20:24:40 @agent_ppo2.py:186][0m |          -0.0115 |           3.3571 |          14.1448 |
[32m[20230113 20:24:40 @agent_ppo2.py:186][0m |          -0.0126 |           3.1226 |          14.1424 |
[32m[20230113 20:24:40 @agent_ppo2.py:186][0m |          -0.0117 |           3.0214 |          14.1365 |
[32m[20230113 20:24:40 @agent_ppo2.py:186][0m |          -0.0142 |           2.9072 |          14.1424 |
[32m[20230113 20:24:40 @agent_ppo2.py:186][0m |          -0.0147 |           2.8185 |          14.1355 |
[32m[20230113 20:24:40 @agent_ppo2.py:186][0m |          -0.0170 |           2.7191 |          14.1301 |
[32m[20230113 20:24:40 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:24:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.75
[32m[20230113 20:24:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.20
[32m[20230113 20:24:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.15
[32m[20230113 20:24:41 @agent_ppo2.py:144][0m Total time:      40.13 min
[32m[20230113 20:24:41 @agent_ppo2.py:146][0m 3751936 total steps have happened
[32m[20230113 20:24:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1832 --------------------------#
[32m[20230113 20:24:41 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:24:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:41 @agent_ppo2.py:186][0m |           0.0004 |           5.5180 |          14.0302 |
[32m[20230113 20:24:41 @agent_ppo2.py:186][0m |          -0.0054 |           4.1924 |          14.0019 |
[32m[20230113 20:24:41 @agent_ppo2.py:186][0m |          -0.0088 |           3.8153 |          14.0023 |
[32m[20230113 20:24:41 @agent_ppo2.py:186][0m |          -0.0106 |           3.5347 |          14.0096 |
[32m[20230113 20:24:41 @agent_ppo2.py:186][0m |          -0.0109 |           3.3657 |          13.9865 |
[32m[20230113 20:24:41 @agent_ppo2.py:186][0m |          -0.0128 |           3.2379 |          13.9974 |
[32m[20230113 20:24:41 @agent_ppo2.py:186][0m |          -0.0136 |           3.1417 |          14.0042 |
[32m[20230113 20:24:41 @agent_ppo2.py:186][0m |          -0.0154 |           3.0369 |          13.9948 |
[32m[20230113 20:24:41 @agent_ppo2.py:186][0m |          -0.0170 |           2.9595 |          13.9927 |
[32m[20230113 20:24:41 @agent_ppo2.py:186][0m |          -0.0162 |           2.8555 |          14.0010 |
[32m[20230113 20:24:41 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.07
[32m[20230113 20:24:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.44
[32m[20230113 20:24:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 132.49
[32m[20230113 20:24:42 @agent_ppo2.py:144][0m Total time:      40.15 min
[32m[20230113 20:24:42 @agent_ppo2.py:146][0m 3753984 total steps have happened
[32m[20230113 20:24:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1833 --------------------------#
[32m[20230113 20:24:42 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:42 @agent_ppo2.py:186][0m |           0.0006 |           6.9090 |          14.1151 |
[32m[20230113 20:24:42 @agent_ppo2.py:186][0m |          -0.0065 |           5.3381 |          14.1069 |
[32m[20230113 20:24:42 @agent_ppo2.py:186][0m |          -0.0101 |           4.5770 |          14.0922 |
[32m[20230113 20:24:42 @agent_ppo2.py:186][0m |          -0.0113 |           4.1379 |          14.0721 |
[32m[20230113 20:24:42 @agent_ppo2.py:186][0m |          -0.0125 |           3.8001 |          14.0613 |
[32m[20230113 20:24:42 @agent_ppo2.py:186][0m |          -0.0162 |           3.5722 |          14.0817 |
[32m[20230113 20:24:43 @agent_ppo2.py:186][0m |          -0.0163 |           3.4297 |          14.0668 |
[32m[20230113 20:24:43 @agent_ppo2.py:186][0m |          -0.0187 |           3.2792 |          14.0686 |
[32m[20230113 20:24:43 @agent_ppo2.py:186][0m |          -0.0187 |           3.1671 |          14.0636 |
[32m[20230113 20:24:43 @agent_ppo2.py:186][0m |          -0.0201 |           3.0565 |          14.0537 |
[32m[20230113 20:24:43 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.29
[32m[20230113 20:24:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.65
[32m[20230113 20:24:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.49
[32m[20230113 20:24:43 @agent_ppo2.py:144][0m Total time:      40.17 min
[32m[20230113 20:24:43 @agent_ppo2.py:146][0m 3756032 total steps have happened
[32m[20230113 20:24:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1834 --------------------------#
[32m[20230113 20:24:43 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:44 @agent_ppo2.py:186][0m |           0.0001 |           5.9657 |          14.0684 |
[32m[20230113 20:24:44 @agent_ppo2.py:186][0m |          -0.0031 |           4.4993 |          14.0682 |
[32m[20230113 20:24:44 @agent_ppo2.py:186][0m |          -0.0053 |           3.9419 |          14.0482 |
[32m[20230113 20:24:44 @agent_ppo2.py:186][0m |          -0.0071 |           3.6584 |          14.0467 |
[32m[20230113 20:24:44 @agent_ppo2.py:186][0m |          -0.0088 |           3.4044 |          14.0465 |
[32m[20230113 20:24:44 @agent_ppo2.py:186][0m |          -0.0110 |           3.2524 |          14.0361 |
[32m[20230113 20:24:44 @agent_ppo2.py:186][0m |          -0.0126 |           3.1205 |          14.0356 |
[32m[20230113 20:24:44 @agent_ppo2.py:186][0m |          -0.0123 |           3.0020 |          14.0378 |
[32m[20230113 20:24:44 @agent_ppo2.py:186][0m |          -0.0129 |           2.9002 |          14.0228 |
[32m[20230113 20:24:44 @agent_ppo2.py:186][0m |          -0.0140 |           2.8429 |          14.0182 |
[32m[20230113 20:24:44 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.81
[32m[20230113 20:24:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.88
[32m[20230113 20:24:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.98
[32m[20230113 20:24:44 @agent_ppo2.py:144][0m Total time:      40.20 min
[32m[20230113 20:24:44 @agent_ppo2.py:146][0m 3758080 total steps have happened
[32m[20230113 20:24:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1835 --------------------------#
[32m[20230113 20:24:45 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:24:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:45 @agent_ppo2.py:186][0m |           0.0008 |           6.8297 |          13.9796 |
[32m[20230113 20:24:45 @agent_ppo2.py:186][0m |          -0.0052 |           4.9173 |          13.9989 |
[32m[20230113 20:24:45 @agent_ppo2.py:186][0m |          -0.0083 |           4.3331 |          13.9998 |
[32m[20230113 20:24:45 @agent_ppo2.py:186][0m |          -0.0098 |           3.9526 |          13.9966 |
[32m[20230113 20:24:45 @agent_ppo2.py:186][0m |          -0.0103 |           3.8128 |          13.9880 |
[32m[20230113 20:24:45 @agent_ppo2.py:186][0m |          -0.0112 |           3.5798 |          13.9927 |
[32m[20230113 20:24:45 @agent_ppo2.py:186][0m |          -0.0133 |           3.4230 |          13.9910 |
[32m[20230113 20:24:45 @agent_ppo2.py:186][0m |          -0.0144 |           3.3613 |          13.9978 |
[32m[20230113 20:24:45 @agent_ppo2.py:186][0m |          -0.0133 |           3.2165 |          13.9904 |
[32m[20230113 20:24:45 @agent_ppo2.py:186][0m |          -0.0157 |           3.1464 |          13.9965 |
[32m[20230113 20:24:45 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.25
[32m[20230113 20:24:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.51
[32m[20230113 20:24:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.57
[32m[20230113 20:24:46 @agent_ppo2.py:144][0m Total time:      40.22 min
[32m[20230113 20:24:46 @agent_ppo2.py:146][0m 3760128 total steps have happened
[32m[20230113 20:24:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1836 --------------------------#
[32m[20230113 20:24:46 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 20:24:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:46 @agent_ppo2.py:186][0m |          -0.0008 |          17.3526 |          14.0607 |
[32m[20230113 20:24:46 @agent_ppo2.py:186][0m |          -0.0030 |           8.1642 |          14.0537 |
[32m[20230113 20:24:46 @agent_ppo2.py:186][0m |          -0.0046 |           6.1460 |          14.0408 |
[32m[20230113 20:24:46 @agent_ppo2.py:186][0m |          -0.0110 |           5.4913 |          14.0418 |
[32m[20230113 20:24:46 @agent_ppo2.py:186][0m |          -0.0062 |           5.5149 |          14.0432 |
[32m[20230113 20:24:46 @agent_ppo2.py:186][0m |          -0.0154 |           4.8019 |          14.0447 |
[32m[20230113 20:24:46 @agent_ppo2.py:186][0m |          -0.0140 |           4.4975 |          14.0406 |
[32m[20230113 20:24:46 @agent_ppo2.py:186][0m |          -0.0137 |           4.2802 |          14.0379 |
[32m[20230113 20:24:47 @agent_ppo2.py:186][0m |          -0.0131 |           4.1184 |          14.0435 |
[32m[20230113 20:24:47 @agent_ppo2.py:186][0m |          -0.0172 |           3.9788 |          14.0399 |
[32m[20230113 20:24:47 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:24:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 143.18
[32m[20230113 20:24:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.36
[32m[20230113 20:24:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.46
[32m[20230113 20:24:47 @agent_ppo2.py:144][0m Total time:      40.24 min
[32m[20230113 20:24:47 @agent_ppo2.py:146][0m 3762176 total steps have happened
[32m[20230113 20:24:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1837 --------------------------#
[32m[20230113 20:24:47 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:24:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:47 @agent_ppo2.py:186][0m |           0.0016 |           6.2039 |          14.0220 |
[32m[20230113 20:24:47 @agent_ppo2.py:186][0m |          -0.0056 |           4.8834 |          14.0176 |
[32m[20230113 20:24:48 @agent_ppo2.py:186][0m |          -0.0079 |           4.4364 |          14.0036 |
[32m[20230113 20:24:48 @agent_ppo2.py:186][0m |          -0.0101 |           4.2216 |          14.0091 |
[32m[20230113 20:24:48 @agent_ppo2.py:186][0m |          -0.0111 |           4.0131 |          14.0003 |
[32m[20230113 20:24:48 @agent_ppo2.py:186][0m |          -0.0126 |           3.8867 |          14.0186 |
[32m[20230113 20:24:48 @agent_ppo2.py:186][0m |          -0.0136 |           3.6901 |          14.0138 |
[32m[20230113 20:24:48 @agent_ppo2.py:186][0m |          -0.0156 |           3.6297 |          14.0041 |
[32m[20230113 20:24:48 @agent_ppo2.py:186][0m |          -0.0145 |           3.5087 |          14.0042 |
[32m[20230113 20:24:48 @agent_ppo2.py:186][0m |          -0.0162 |           3.4230 |          14.0059 |
[32m[20230113 20:24:48 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:24:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.92
[32m[20230113 20:24:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.88
[32m[20230113 20:24:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.91
[32m[20230113 20:24:48 @agent_ppo2.py:144][0m Total time:      40.26 min
[32m[20230113 20:24:48 @agent_ppo2.py:146][0m 3764224 total steps have happened
[32m[20230113 20:24:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1838 --------------------------#
[32m[20230113 20:24:49 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:24:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:49 @agent_ppo2.py:186][0m |          -0.0003 |           5.7683 |          13.8012 |
[32m[20230113 20:24:49 @agent_ppo2.py:186][0m |          -0.0067 |           4.8980 |          13.8038 |
[32m[20230113 20:24:49 @agent_ppo2.py:186][0m |          -0.0080 |           4.4904 |          13.8080 |
[32m[20230113 20:24:49 @agent_ppo2.py:186][0m |          -0.0099 |           4.2543 |          13.8110 |
[32m[20230113 20:24:49 @agent_ppo2.py:186][0m |          -0.0107 |           3.9961 |          13.8186 |
[32m[20230113 20:24:49 @agent_ppo2.py:186][0m |          -0.0139 |           3.9066 |          13.8044 |
[32m[20230113 20:24:49 @agent_ppo2.py:186][0m |          -0.0156 |           3.7204 |          13.8104 |
[32m[20230113 20:24:49 @agent_ppo2.py:186][0m |          -0.0141 |           3.6454 |          13.8104 |
[32m[20230113 20:24:49 @agent_ppo2.py:186][0m |          -0.0178 |           3.5602 |          13.8127 |
[32m[20230113 20:24:49 @agent_ppo2.py:186][0m |          -0.0190 |           3.4677 |          13.7977 |
[32m[20230113 20:24:49 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.97
[32m[20230113 20:24:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.04
[32m[20230113 20:24:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 241.00
[32m[20230113 20:24:49 @agent_ppo2.py:144][0m Total time:      40.28 min
[32m[20230113 20:24:49 @agent_ppo2.py:146][0m 3766272 total steps have happened
[32m[20230113 20:24:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1839 --------------------------#
[32m[20230113 20:24:50 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:24:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:50 @agent_ppo2.py:186][0m |          -0.0006 |           6.4985 |          14.0539 |
[32m[20230113 20:24:50 @agent_ppo2.py:186][0m |          -0.0064 |           4.8655 |          14.0369 |
[32m[20230113 20:24:50 @agent_ppo2.py:186][0m |          -0.0106 |           4.2812 |          14.0058 |
[32m[20230113 20:24:50 @agent_ppo2.py:186][0m |          -0.0078 |           3.9593 |          14.0186 |
[32m[20230113 20:24:50 @agent_ppo2.py:186][0m |          -0.0143 |           3.7927 |          14.0093 |
[32m[20230113 20:24:50 @agent_ppo2.py:186][0m |          -0.0153 |           3.6464 |          14.0103 |
[32m[20230113 20:24:50 @agent_ppo2.py:186][0m |          -0.0198 |           3.4387 |          14.0101 |
[32m[20230113 20:24:50 @agent_ppo2.py:186][0m |          -0.0171 |           3.3958 |          14.0173 |
[32m[20230113 20:24:50 @agent_ppo2.py:186][0m |          -0.0184 |           3.2778 |          14.0149 |
[32m[20230113 20:24:50 @agent_ppo2.py:186][0m |          -0.0134 |           3.2719 |          14.0093 |
[32m[20230113 20:24:50 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.34
[32m[20230113 20:24:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.97
[32m[20230113 20:24:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.07
[32m[20230113 20:24:51 @agent_ppo2.py:144][0m Total time:      40.30 min
[32m[20230113 20:24:51 @agent_ppo2.py:146][0m 3768320 total steps have happened
[32m[20230113 20:24:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1840 --------------------------#
[32m[20230113 20:24:51 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:24:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:51 @agent_ppo2.py:186][0m |           0.0001 |           5.8372 |          13.9949 |
[32m[20230113 20:24:51 @agent_ppo2.py:186][0m |          -0.0064 |           4.7176 |          13.9870 |
[32m[20230113 20:24:51 @agent_ppo2.py:186][0m |          -0.0102 |           4.2860 |          13.9931 |
[32m[20230113 20:24:51 @agent_ppo2.py:186][0m |          -0.0115 |           3.9331 |          13.9786 |
[32m[20230113 20:24:51 @agent_ppo2.py:186][0m |          -0.0115 |           3.8632 |          13.9822 |
[32m[20230113 20:24:51 @agent_ppo2.py:186][0m |          -0.0121 |           3.6463 |          13.9799 |
[32m[20230113 20:24:52 @agent_ppo2.py:186][0m |          -0.0149 |           3.5023 |          13.9907 |
[32m[20230113 20:24:52 @agent_ppo2.py:186][0m |          -0.0162 |           3.3964 |          13.9821 |
[32m[20230113 20:24:52 @agent_ppo2.py:186][0m |          -0.0170 |           3.2514 |          13.9744 |
[32m[20230113 20:24:52 @agent_ppo2.py:186][0m |          -0.0175 |           3.2069 |          13.9657 |
[32m[20230113 20:24:52 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:24:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.40
[32m[20230113 20:24:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.36
[32m[20230113 20:24:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.03
[32m[20230113 20:24:52 @agent_ppo2.py:144][0m Total time:      40.32 min
[32m[20230113 20:24:52 @agent_ppo2.py:146][0m 3770368 total steps have happened
[32m[20230113 20:24:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1841 --------------------------#
[32m[20230113 20:24:52 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:24:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:53 @agent_ppo2.py:186][0m |           0.0013 |           6.3560 |          14.3794 |
[32m[20230113 20:24:53 @agent_ppo2.py:186][0m |          -0.0037 |           5.2381 |          14.3620 |
[32m[20230113 20:24:53 @agent_ppo2.py:186][0m |          -0.0086 |           4.7861 |          14.3514 |
[32m[20230113 20:24:53 @agent_ppo2.py:186][0m |          -0.0099 |           4.4635 |          14.3527 |
[32m[20230113 20:24:53 @agent_ppo2.py:186][0m |          -0.0103 |           4.3142 |          14.3425 |
[32m[20230113 20:24:53 @agent_ppo2.py:186][0m |          -0.0125 |           4.0757 |          14.3468 |
[32m[20230113 20:24:53 @agent_ppo2.py:186][0m |          -0.0139 |           3.9615 |          14.3393 |
[32m[20230113 20:24:53 @agent_ppo2.py:186][0m |          -0.0151 |           3.8448 |          14.3429 |
[32m[20230113 20:24:53 @agent_ppo2.py:186][0m |          -0.0146 |           3.7098 |          14.3346 |
[32m[20230113 20:24:53 @agent_ppo2.py:186][0m |          -0.0167 |           3.6105 |          14.3393 |
[32m[20230113 20:24:53 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:24:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.21
[32m[20230113 20:24:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.99
[32m[20230113 20:24:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.64
[32m[20230113 20:24:53 @agent_ppo2.py:144][0m Total time:      40.34 min
[32m[20230113 20:24:53 @agent_ppo2.py:146][0m 3772416 total steps have happened
[32m[20230113 20:24:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1842 --------------------------#
[32m[20230113 20:24:54 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:24:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:54 @agent_ppo2.py:186][0m |           0.0557 |           7.7557 |          13.8145 |
[32m[20230113 20:24:54 @agent_ppo2.py:186][0m |           0.0022 |           4.9693 |          13.7577 |
[32m[20230113 20:24:54 @agent_ppo2.py:186][0m |          -0.0189 |           4.0778 |          13.7643 |
[32m[20230113 20:24:54 @agent_ppo2.py:186][0m |           0.0400 |           3.8872 |          13.7762 |
[32m[20230113 20:24:54 @agent_ppo2.py:186][0m |          -0.0049 |           3.5030 |          13.7575 |
[32m[20230113 20:24:54 @agent_ppo2.py:186][0m |          -0.0206 |           3.3789 |          13.7707 |
[32m[20230113 20:24:54 @agent_ppo2.py:186][0m |          -0.0070 |           3.2725 |          13.7593 |
[32m[20230113 20:24:54 @agent_ppo2.py:186][0m |          -0.0084 |           3.2033 |          13.7592 |
[32m[20230113 20:24:54 @agent_ppo2.py:186][0m |          -0.0255 |           3.1165 |          13.7525 |
[32m[20230113 20:24:54 @agent_ppo2.py:186][0m |           0.0040 |           3.1097 |          13.7501 |
[32m[20230113 20:24:54 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:24:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.48
[32m[20230113 20:24:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.17
[32m[20230113 20:24:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.56
[32m[20230113 20:24:55 @agent_ppo2.py:144][0m Total time:      40.37 min
[32m[20230113 20:24:55 @agent_ppo2.py:146][0m 3774464 total steps have happened
[32m[20230113 20:24:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1843 --------------------------#
[32m[20230113 20:24:55 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:24:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:55 @agent_ppo2.py:186][0m |           0.0029 |           6.7313 |          14.2957 |
[32m[20230113 20:24:55 @agent_ppo2.py:186][0m |          -0.0018 |           5.2459 |          14.2848 |
[32m[20230113 20:24:55 @agent_ppo2.py:186][0m |          -0.0052 |           4.6930 |          14.2920 |
[32m[20230113 20:24:55 @agent_ppo2.py:186][0m |          -0.0031 |           4.3704 |          14.2724 |
[32m[20230113 20:24:55 @agent_ppo2.py:186][0m |          -0.0069 |           4.0777 |          14.2758 |
[32m[20230113 20:24:55 @agent_ppo2.py:186][0m |          -0.0076 |           3.8702 |          14.2817 |
[32m[20230113 20:24:55 @agent_ppo2.py:186][0m |          -0.0089 |           3.7214 |          14.2756 |
[32m[20230113 20:24:55 @agent_ppo2.py:186][0m |          -0.0120 |           3.6172 |          14.2753 |
[32m[20230113 20:24:55 @agent_ppo2.py:186][0m |          -0.0098 |           3.4517 |          14.2734 |
[32m[20230113 20:24:55 @agent_ppo2.py:186][0m |          -0.0132 |           3.3448 |          14.2696 |
[32m[20230113 20:24:55 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:24:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.04
[32m[20230113 20:24:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.69
[32m[20230113 20:24:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 240.89
[32m[20230113 20:24:56 @agent_ppo2.py:144][0m Total time:      40.39 min
[32m[20230113 20:24:56 @agent_ppo2.py:146][0m 3776512 total steps have happened
[32m[20230113 20:24:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1844 --------------------------#
[32m[20230113 20:24:56 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:24:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:56 @agent_ppo2.py:186][0m |          -0.0005 |           6.1401 |          14.2944 |
[32m[20230113 20:24:56 @agent_ppo2.py:186][0m |          -0.0076 |           4.5299 |          14.2864 |
[32m[20230113 20:24:56 @agent_ppo2.py:186][0m |          -0.0110 |           4.0019 |          14.2954 |
[32m[20230113 20:24:56 @agent_ppo2.py:186][0m |          -0.0130 |           3.6493 |          14.2887 |
[32m[20230113 20:24:57 @agent_ppo2.py:186][0m |          -0.0155 |           3.4702 |          14.2876 |
[32m[20230113 20:24:57 @agent_ppo2.py:186][0m |          -0.0161 |           3.2465 |          14.2808 |
[32m[20230113 20:24:57 @agent_ppo2.py:186][0m |          -0.0174 |           3.1084 |          14.2768 |
[32m[20230113 20:24:57 @agent_ppo2.py:186][0m |          -0.0188 |           2.9958 |          14.2761 |
[32m[20230113 20:24:57 @agent_ppo2.py:186][0m |          -0.0187 |           2.8617 |          14.2570 |
[32m[20230113 20:24:57 @agent_ppo2.py:186][0m |          -0.0192 |           2.7957 |          14.2737 |
[32m[20230113 20:24:57 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:24:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.99
[32m[20230113 20:24:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.85
[32m[20230113 20:24:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.90
[32m[20230113 20:24:57 @agent_ppo2.py:144][0m Total time:      40.41 min
[32m[20230113 20:24:57 @agent_ppo2.py:146][0m 3778560 total steps have happened
[32m[20230113 20:24:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1845 --------------------------#
[32m[20230113 20:24:58 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:24:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:58 @agent_ppo2.py:186][0m |           0.0010 |           8.1000 |          13.7399 |
[32m[20230113 20:24:58 @agent_ppo2.py:186][0m |          -0.0103 |           5.4699 |          13.7362 |
[32m[20230113 20:24:58 @agent_ppo2.py:186][0m |          -0.0050 |           4.6500 |          13.7229 |
[32m[20230113 20:24:58 @agent_ppo2.py:186][0m |          -0.0153 |           4.2220 |          13.7167 |
[32m[20230113 20:24:58 @agent_ppo2.py:186][0m |          -0.0121 |           4.0742 |          13.7158 |
[32m[20230113 20:24:58 @agent_ppo2.py:186][0m |          -0.0160 |           4.3985 |          13.7094 |
[32m[20230113 20:24:58 @agent_ppo2.py:186][0m |          -0.0230 |           3.6038 |          13.7119 |
[32m[20230113 20:24:58 @agent_ppo2.py:186][0m |          -0.0285 |           3.4156 |          13.6904 |
[32m[20230113 20:24:58 @agent_ppo2.py:186][0m |          -0.0118 |           3.2707 |          13.6966 |
[32m[20230113 20:24:58 @agent_ppo2.py:186][0m |          -0.0169 |           3.1700 |          13.7077 |
[32m[20230113 20:24:58 @agent_ppo2.py:131][0m Policy update time: 0.47 s
[32m[20230113 20:24:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.29
[32m[20230113 20:24:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 231.40
[32m[20230113 20:24:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.26
[32m[20230113 20:24:58 @agent_ppo2.py:144][0m Total time:      40.43 min
[32m[20230113 20:24:58 @agent_ppo2.py:146][0m 3780608 total steps have happened
[32m[20230113 20:24:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1846 --------------------------#
[32m[20230113 20:24:59 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:24:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:24:59 @agent_ppo2.py:186][0m |          -0.0031 |           7.5615 |          13.8771 |
[32m[20230113 20:24:59 @agent_ppo2.py:186][0m |          -0.0045 |           5.6218 |          13.8670 |
[32m[20230113 20:24:59 @agent_ppo2.py:186][0m |          -0.0026 |           5.0589 |          13.8601 |
[32m[20230113 20:24:59 @agent_ppo2.py:186][0m |          -0.0165 |           4.5567 |          13.8389 |
[32m[20230113 20:24:59 @agent_ppo2.py:186][0m |          -0.0054 |           4.2984 |          13.8397 |
[32m[20230113 20:24:59 @agent_ppo2.py:186][0m |          -0.0294 |           4.1488 |          13.8309 |
[32m[20230113 20:24:59 @agent_ppo2.py:186][0m |          -0.0123 |           3.9079 |          13.8108 |
[32m[20230113 20:24:59 @agent_ppo2.py:186][0m |          -0.0096 |           3.7391 |          13.8224 |
[32m[20230113 20:24:59 @agent_ppo2.py:186][0m |          -0.0189 |           3.6616 |          13.8191 |
[32m[20230113 20:24:59 @agent_ppo2.py:186][0m |          -0.0141 |           3.5413 |          13.8260 |
[32m[20230113 20:24:59 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.82
[32m[20230113 20:25:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.03
[32m[20230113 20:25:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.28
[32m[20230113 20:25:00 @agent_ppo2.py:144][0m Total time:      40.45 min
[32m[20230113 20:25:00 @agent_ppo2.py:146][0m 3782656 total steps have happened
[32m[20230113 20:25:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1847 --------------------------#
[32m[20230113 20:25:00 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:00 @agent_ppo2.py:186][0m |          -0.0030 |           5.8424 |          14.1621 |
[32m[20230113 20:25:00 @agent_ppo2.py:186][0m |          -0.0049 |           4.5229 |          14.1459 |
[32m[20230113 20:25:00 @agent_ppo2.py:186][0m |          -0.0114 |           3.9918 |          14.1338 |
[32m[20230113 20:25:00 @agent_ppo2.py:186][0m |          -0.0119 |           3.7439 |          14.1299 |
[32m[20230113 20:25:00 @agent_ppo2.py:186][0m |          -0.0096 |           3.5528 |          14.1330 |
[32m[20230113 20:25:00 @agent_ppo2.py:186][0m |          -0.0151 |           3.3648 |          14.1228 |
[32m[20230113 20:25:00 @agent_ppo2.py:186][0m |          -0.0108 |           3.3876 |          14.1230 |
[32m[20230113 20:25:01 @agent_ppo2.py:186][0m |          -0.0148 |           3.1663 |          14.1152 |
[32m[20230113 20:25:01 @agent_ppo2.py:186][0m |          -0.0184 |           3.0369 |          14.1122 |
[32m[20230113 20:25:01 @agent_ppo2.py:186][0m |          -0.0183 |           2.9659 |          14.1230 |
[32m[20230113 20:25:01 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.54
[32m[20230113 20:25:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.33
[32m[20230113 20:25:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.01
[32m[20230113 20:25:01 @agent_ppo2.py:144][0m Total time:      40.47 min
[32m[20230113 20:25:01 @agent_ppo2.py:146][0m 3784704 total steps have happened
[32m[20230113 20:25:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1848 --------------------------#
[32m[20230113 20:25:01 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:02 @agent_ppo2.py:186][0m |          -0.0053 |           5.5292 |          14.6766 |
[32m[20230113 20:25:02 @agent_ppo2.py:186][0m |          -0.0086 |           4.3545 |          14.6639 |
[32m[20230113 20:25:02 @agent_ppo2.py:186][0m |          -0.0107 |           3.8630 |          14.6826 |
[32m[20230113 20:25:02 @agent_ppo2.py:186][0m |          -0.0080 |           3.7225 |          14.6724 |
[32m[20230113 20:25:02 @agent_ppo2.py:186][0m |          -0.0128 |           3.4735 |          14.6615 |
[32m[20230113 20:25:02 @agent_ppo2.py:186][0m |          -0.0111 |           3.3730 |          14.6881 |
[32m[20230113 20:25:02 @agent_ppo2.py:186][0m |          -0.0183 |           3.1780 |          14.6661 |
[32m[20230113 20:25:02 @agent_ppo2.py:186][0m |          -0.0094 |           3.1170 |          14.6693 |
[32m[20230113 20:25:02 @agent_ppo2.py:186][0m |          -0.0168 |           2.9933 |          14.6578 |
[32m[20230113 20:25:02 @agent_ppo2.py:186][0m |          -0.0200 |           2.9293 |          14.6697 |
[32m[20230113 20:25:02 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.81
[32m[20230113 20:25:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.88
[32m[20230113 20:25:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.83
[32m[20230113 20:25:02 @agent_ppo2.py:144][0m Total time:      40.50 min
[32m[20230113 20:25:02 @agent_ppo2.py:146][0m 3786752 total steps have happened
[32m[20230113 20:25:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1849 --------------------------#
[32m[20230113 20:25:03 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:03 @agent_ppo2.py:186][0m |          -0.0052 |           5.3026 |          14.1992 |
[32m[20230113 20:25:03 @agent_ppo2.py:186][0m |           0.0003 |           4.2930 |          14.1964 |
[32m[20230113 20:25:03 @agent_ppo2.py:186][0m |          -0.0128 |           3.8704 |          14.1981 |
[32m[20230113 20:25:03 @agent_ppo2.py:186][0m |          -0.0124 |           3.5976 |          14.1844 |
[32m[20230113 20:25:03 @agent_ppo2.py:186][0m |          -0.0092 |           3.4272 |          14.1999 |
[32m[20230113 20:25:03 @agent_ppo2.py:186][0m |          -0.0088 |           3.3662 |          14.2028 |
[32m[20230113 20:25:03 @agent_ppo2.py:186][0m |          -0.0175 |           3.2087 |          14.1958 |
[32m[20230113 20:25:03 @agent_ppo2.py:186][0m |          -0.0205 |           3.1386 |          14.1826 |
[32m[20230113 20:25:03 @agent_ppo2.py:186][0m |          -0.0226 |           3.0574 |          14.2014 |
[32m[20230113 20:25:03 @agent_ppo2.py:186][0m |          -0.0156 |           2.9782 |          14.1978 |
[32m[20230113 20:25:03 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.19
[32m[20230113 20:25:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.77
[32m[20230113 20:25:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.09
[32m[20230113 20:25:04 @agent_ppo2.py:144][0m Total time:      40.52 min
[32m[20230113 20:25:04 @agent_ppo2.py:146][0m 3788800 total steps have happened
[32m[20230113 20:25:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1850 --------------------------#
[32m[20230113 20:25:04 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:04 @agent_ppo2.py:186][0m |           0.0014 |           5.0296 |          14.4464 |
[32m[20230113 20:25:04 @agent_ppo2.py:186][0m |          -0.0064 |           3.6330 |          14.4006 |
[32m[20230113 20:25:04 @agent_ppo2.py:186][0m |          -0.0103 |           3.2675 |          14.4132 |
[32m[20230113 20:25:04 @agent_ppo2.py:186][0m |          -0.0122 |           3.0937 |          14.4094 |
[32m[20230113 20:25:04 @agent_ppo2.py:186][0m |          -0.0154 |           2.9574 |          14.4053 |
[32m[20230113 20:25:04 @agent_ppo2.py:186][0m |          -0.0130 |           2.8375 |          14.4068 |
[32m[20230113 20:25:04 @agent_ppo2.py:186][0m |          -0.0166 |           2.7730 |          14.4088 |
[32m[20230113 20:25:04 @agent_ppo2.py:186][0m |          -0.0164 |           2.6788 |          14.3973 |
[32m[20230113 20:25:04 @agent_ppo2.py:186][0m |          -0.0174 |           2.6228 |          14.4031 |
[32m[20230113 20:25:04 @agent_ppo2.py:186][0m |          -0.0192 |           2.5811 |          14.3911 |
[32m[20230113 20:25:04 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.86
[32m[20230113 20:25:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.05
[32m[20230113 20:25:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.23
[32m[20230113 20:25:05 @agent_ppo2.py:144][0m Total time:      40.54 min
[32m[20230113 20:25:05 @agent_ppo2.py:146][0m 3790848 total steps have happened
[32m[20230113 20:25:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1851 --------------------------#
[32m[20230113 20:25:05 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:05 @agent_ppo2.py:186][0m |           0.0001 |           6.3162 |          14.4967 |
[32m[20230113 20:25:05 @agent_ppo2.py:186][0m |          -0.0084 |           4.3453 |          14.4655 |
[32m[20230113 20:25:05 @agent_ppo2.py:186][0m |          -0.0118 |           3.6186 |          14.4581 |
[32m[20230113 20:25:05 @agent_ppo2.py:186][0m |          -0.0135 |           3.2851 |          14.4697 |
[32m[20230113 20:25:05 @agent_ppo2.py:186][0m |          -0.0131 |           3.0750 |          14.4616 |
[32m[20230113 20:25:06 @agent_ppo2.py:186][0m |          -0.0190 |           2.9196 |          14.4622 |
[32m[20230113 20:25:06 @agent_ppo2.py:186][0m |          -0.0175 |           2.8323 |          14.4711 |
[32m[20230113 20:25:06 @agent_ppo2.py:186][0m |          -0.0176 |           2.7318 |          14.4758 |
[32m[20230113 20:25:06 @agent_ppo2.py:186][0m |          -0.0176 |           2.6445 |          14.4638 |
[32m[20230113 20:25:06 @agent_ppo2.py:186][0m |          -0.0165 |           2.6120 |          14.4625 |
[32m[20230113 20:25:06 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:25:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.68
[32m[20230113 20:25:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.95
[32m[20230113 20:25:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 141.84
[32m[20230113 20:25:06 @agent_ppo2.py:144][0m Total time:      40.56 min
[32m[20230113 20:25:06 @agent_ppo2.py:146][0m 3792896 total steps have happened
[32m[20230113 20:25:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1852 --------------------------#
[32m[20230113 20:25:06 @agent_ppo2.py:128][0m Sampling time: 0.31 s by 1 slaves
[32m[20230113 20:25:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:07 @agent_ppo2.py:186][0m |           0.0062 |          31.6557 |          14.6236 |
[32m[20230113 20:25:07 @agent_ppo2.py:186][0m |          -0.0071 |           6.0695 |          14.6275 |
[32m[20230113 20:25:07 @agent_ppo2.py:186][0m |          -0.0087 |           4.4559 |          14.6256 |
[32m[20230113 20:25:07 @agent_ppo2.py:186][0m |          -0.0078 |           4.0177 |          14.6141 |
[32m[20230113 20:25:07 @agent_ppo2.py:186][0m |          -0.0142 |           3.6544 |          14.6170 |
[32m[20230113 20:25:07 @agent_ppo2.py:186][0m |          -0.0145 |           3.4401 |          14.6164 |
[32m[20230113 20:25:07 @agent_ppo2.py:186][0m |          -0.0136 |           3.2841 |          14.6130 |
[32m[20230113 20:25:07 @agent_ppo2.py:186][0m |          -0.0168 |           3.1267 |          14.6177 |
[32m[20230113 20:25:07 @agent_ppo2.py:186][0m |          -0.0147 |           3.0326 |          14.6118 |
[32m[20230113 20:25:07 @agent_ppo2.py:186][0m |          -0.0174 |           2.9037 |          14.6138 |
[32m[20230113 20:25:07 @agent_ppo2.py:131][0m Policy update time: 0.32 s
[32m[20230113 20:25:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 103.54
[32m[20230113 20:25:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.03
[32m[20230113 20:25:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.58
[32m[20230113 20:25:07 @agent_ppo2.py:144][0m Total time:      40.58 min
[32m[20230113 20:25:07 @agent_ppo2.py:146][0m 3794944 total steps have happened
[32m[20230113 20:25:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1853 --------------------------#
[32m[20230113 20:25:08 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:08 @agent_ppo2.py:186][0m |           0.0042 |           6.2274 |          14.2959 |
[32m[20230113 20:25:08 @agent_ppo2.py:186][0m |           0.0000 |           5.0201 |          14.2815 |
[32m[20230113 20:25:08 @agent_ppo2.py:186][0m |          -0.0010 |           4.5472 |          14.2510 |
[32m[20230113 20:25:08 @agent_ppo2.py:186][0m |          -0.0250 |           4.2927 |          14.2649 |
[32m[20230113 20:25:08 @agent_ppo2.py:186][0m |          -0.0097 |           4.0313 |          14.2565 |
[32m[20230113 20:25:08 @agent_ppo2.py:186][0m |          -0.0142 |           3.8526 |          14.2538 |
[32m[20230113 20:25:08 @agent_ppo2.py:186][0m |          -0.0108 |           3.7814 |          14.2480 |
[32m[20230113 20:25:08 @agent_ppo2.py:186][0m |          -0.0115 |           3.6619 |          14.2334 |
[32m[20230113 20:25:08 @agent_ppo2.py:186][0m |          -0.0161 |           3.5562 |          14.2379 |
[32m[20230113 20:25:08 @agent_ppo2.py:186][0m |          -0.0163 |           3.4647 |          14.2530 |
[32m[20230113 20:25:08 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.80
[32m[20230113 20:25:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 234.50
[32m[20230113 20:25:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.32
[32m[20230113 20:25:08 @agent_ppo2.py:144][0m Total time:      40.60 min
[32m[20230113 20:25:08 @agent_ppo2.py:146][0m 3796992 total steps have happened
[32m[20230113 20:25:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1854 --------------------------#
[32m[20230113 20:25:09 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:09 @agent_ppo2.py:186][0m |           0.0009 |           6.6963 |          14.4420 |
[32m[20230113 20:25:09 @agent_ppo2.py:186][0m |          -0.0047 |           5.0522 |          14.4552 |
[32m[20230113 20:25:09 @agent_ppo2.py:186][0m |          -0.0083 |           4.4049 |          14.4400 |
[32m[20230113 20:25:09 @agent_ppo2.py:186][0m |          -0.0082 |           4.0983 |          14.4389 |
[32m[20230113 20:25:09 @agent_ppo2.py:186][0m |          -0.0075 |           3.8655 |          14.4409 |
[32m[20230113 20:25:09 @agent_ppo2.py:186][0m |          -0.0106 |           3.6746 |          14.4461 |
[32m[20230113 20:25:09 @agent_ppo2.py:186][0m |          -0.0068 |           3.5896 |          14.4432 |
[32m[20230113 20:25:09 @agent_ppo2.py:186][0m |          -0.0183 |           3.4230 |          14.4478 |
[32m[20230113 20:25:09 @agent_ppo2.py:186][0m |          -0.0139 |           3.3611 |          14.4405 |
[32m[20230113 20:25:09 @agent_ppo2.py:186][0m |          -0.0168 |           3.2863 |          14.4367 |
[32m[20230113 20:25:09 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.42
[32m[20230113 20:25:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.60
[32m[20230113 20:25:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.13
[32m[20230113 20:25:10 @agent_ppo2.py:144][0m Total time:      40.62 min
[32m[20230113 20:25:10 @agent_ppo2.py:146][0m 3799040 total steps have happened
[32m[20230113 20:25:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1855 --------------------------#
[32m[20230113 20:25:10 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:25:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:10 @agent_ppo2.py:186][0m |           0.0026 |           6.1951 |          14.6418 |
[32m[20230113 20:25:10 @agent_ppo2.py:186][0m |          -0.0069 |           4.9255 |          14.6379 |
[32m[20230113 20:25:10 @agent_ppo2.py:186][0m |          -0.0087 |           4.4271 |          14.6365 |
[32m[20230113 20:25:10 @agent_ppo2.py:186][0m |          -0.0102 |           4.1564 |          14.6287 |
[32m[20230113 20:25:10 @agent_ppo2.py:186][0m |          -0.0107 |           3.8945 |          14.6160 |
[32m[20230113 20:25:10 @agent_ppo2.py:186][0m |          -0.0110 |           3.7115 |          14.6198 |
[32m[20230113 20:25:10 @agent_ppo2.py:186][0m |          -0.0131 |           3.5656 |          14.6107 |
[32m[20230113 20:25:11 @agent_ppo2.py:186][0m |          -0.0141 |           3.4604 |          14.6246 |
[32m[20230113 20:25:11 @agent_ppo2.py:186][0m |          -0.0158 |           3.3457 |          14.6199 |
[32m[20230113 20:25:11 @agent_ppo2.py:186][0m |          -0.0179 |           3.2666 |          14.6111 |
[32m[20230113 20:25:11 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.13
[32m[20230113 20:25:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.27
[32m[20230113 20:25:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.49
[32m[20230113 20:25:11 @agent_ppo2.py:144][0m Total time:      40.64 min
[32m[20230113 20:25:11 @agent_ppo2.py:146][0m 3801088 total steps have happened
[32m[20230113 20:25:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1856 --------------------------#
[32m[20230113 20:25:11 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:12 @agent_ppo2.py:186][0m |           0.0037 |           6.8025 |          14.4613 |
[32m[20230113 20:25:12 @agent_ppo2.py:186][0m |          -0.0041 |           5.0211 |          14.4341 |
[32m[20230113 20:25:12 @agent_ppo2.py:186][0m |          -0.0125 |           4.4329 |          14.4211 |
[32m[20230113 20:25:12 @agent_ppo2.py:186][0m |          -0.0156 |           4.2632 |          14.4290 |
[32m[20230113 20:25:12 @agent_ppo2.py:186][0m |          -0.0153 |           3.9041 |          14.4195 |
[32m[20230113 20:25:12 @agent_ppo2.py:186][0m |          -0.0075 |           3.7285 |          14.4226 |
[32m[20230113 20:25:12 @agent_ppo2.py:186][0m |          -0.0054 |           3.6326 |          14.4041 |
[32m[20230113 20:25:12 @agent_ppo2.py:186][0m |          -0.0153 |           3.5149 |          14.4021 |
[32m[20230113 20:25:12 @agent_ppo2.py:186][0m |          -0.0187 |           3.3269 |          14.4228 |
[32m[20230113 20:25:12 @agent_ppo2.py:186][0m |          -0.0190 |           3.2877 |          14.4069 |
[32m[20230113 20:25:12 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.05
[32m[20230113 20:25:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.44
[32m[20230113 20:25:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 153.53
[32m[20230113 20:25:12 @agent_ppo2.py:144][0m Total time:      40.66 min
[32m[20230113 20:25:12 @agent_ppo2.py:146][0m 3803136 total steps have happened
[32m[20230113 20:25:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1857 --------------------------#
[32m[20230113 20:25:13 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:25:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:13 @agent_ppo2.py:186][0m |           0.0010 |           6.9999 |          14.3775 |
[32m[20230113 20:25:13 @agent_ppo2.py:186][0m |           0.0004 |           5.0192 |          14.3571 |
[32m[20230113 20:25:13 @agent_ppo2.py:186][0m |          -0.0053 |           4.5446 |          14.3586 |
[32m[20230113 20:25:13 @agent_ppo2.py:186][0m |           0.0020 |           4.2530 |          14.3433 |
[32m[20230113 20:25:13 @agent_ppo2.py:186][0m |          -0.0133 |           4.0479 |          14.3550 |
[32m[20230113 20:25:13 @agent_ppo2.py:186][0m |          -0.0047 |           3.9878 |          14.3494 |
[32m[20230113 20:25:13 @agent_ppo2.py:186][0m |          -0.0071 |           3.8024 |          14.3388 |
[32m[20230113 20:25:13 @agent_ppo2.py:186][0m |          -0.0114 |           3.6623 |          14.3517 |
[32m[20230113 20:25:13 @agent_ppo2.py:186][0m |           0.0138 |           4.2810 |          14.3549 |
[32m[20230113 20:25:13 @agent_ppo2.py:186][0m |          -0.0151 |           3.7325 |          14.3546 |
[32m[20230113 20:25:13 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:25:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 226.98
[32m[20230113 20:25:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 232.68
[32m[20230113 20:25:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 242.68
[32m[20230113 20:25:14 @agent_ppo2.py:144][0m Total time:      40.68 min
[32m[20230113 20:25:14 @agent_ppo2.py:146][0m 3805184 total steps have happened
[32m[20230113 20:25:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1858 --------------------------#
[32m[20230113 20:25:14 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 20:25:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:14 @agent_ppo2.py:186][0m |           0.0010 |          14.0139 |          14.6197 |
[32m[20230113 20:25:14 @agent_ppo2.py:186][0m |           0.0000 |           5.2895 |          14.6129 |
[32m[20230113 20:25:14 @agent_ppo2.py:186][0m |          -0.0060 |           4.4536 |          14.5881 |
[32m[20230113 20:25:14 @agent_ppo2.py:186][0m |          -0.0110 |           4.0379 |          14.6100 |
[32m[20230113 20:25:14 @agent_ppo2.py:186][0m |          -0.0140 |           3.8181 |          14.5866 |
[32m[20230113 20:25:14 @agent_ppo2.py:186][0m |          -0.0110 |           3.5746 |          14.5997 |
[32m[20230113 20:25:14 @agent_ppo2.py:186][0m |          -0.0173 |           3.3822 |          14.5981 |
[32m[20230113 20:25:14 @agent_ppo2.py:186][0m |          -0.0139 |           3.2137 |          14.5884 |
[32m[20230113 20:25:14 @agent_ppo2.py:186][0m |          -0.0169 |           3.1372 |          14.6010 |
[32m[20230113 20:25:14 @agent_ppo2.py:186][0m |          -0.0179 |           3.0302 |          14.5947 |
[32m[20230113 20:25:14 @agent_ppo2.py:131][0m Policy update time: 0.40 s
[32m[20230113 20:25:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 172.82
[32m[20230113 20:25:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.62
[32m[20230113 20:25:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.93
[32m[20230113 20:25:15 @agent_ppo2.py:144][0m Total time:      40.70 min
[32m[20230113 20:25:15 @agent_ppo2.py:146][0m 3807232 total steps have happened
[32m[20230113 20:25:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1859 --------------------------#
[32m[20230113 20:25:15 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:15 @agent_ppo2.py:186][0m |           0.0026 |           6.3180 |          14.6306 |
[32m[20230113 20:25:15 @agent_ppo2.py:186][0m |          -0.0026 |           4.7611 |          14.6302 |
[32m[20230113 20:25:15 @agent_ppo2.py:186][0m |          -0.0059 |           4.3280 |          14.6202 |
[32m[20230113 20:25:15 @agent_ppo2.py:186][0m |          -0.0083 |           4.1688 |          14.6254 |
[32m[20230113 20:25:15 @agent_ppo2.py:186][0m |          -0.0084 |           3.9792 |          14.6283 |
[32m[20230113 20:25:15 @agent_ppo2.py:186][0m |          -0.0107 |           3.7598 |          14.6157 |
[32m[20230113 20:25:15 @agent_ppo2.py:186][0m |          -0.0103 |           3.6580 |          14.6117 |
[32m[20230113 20:25:16 @agent_ppo2.py:186][0m |          -0.0106 |           3.5433 |          14.6220 |
[32m[20230113 20:25:16 @agent_ppo2.py:186][0m |          -0.0114 |           3.4442 |          14.5973 |
[32m[20230113 20:25:16 @agent_ppo2.py:186][0m |          -0.0132 |           3.3731 |          14.6161 |
[32m[20230113 20:25:16 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.24
[32m[20230113 20:25:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.54
[32m[20230113 20:25:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.32
[32m[20230113 20:25:16 @agent_ppo2.py:144][0m Total time:      40.72 min
[32m[20230113 20:25:16 @agent_ppo2.py:146][0m 3809280 total steps have happened
[32m[20230113 20:25:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1860 --------------------------#
[32m[20230113 20:25:16 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:17 @agent_ppo2.py:186][0m |           0.0026 |           4.8314 |          14.8018 |
[32m[20230113 20:25:17 @agent_ppo2.py:186][0m |          -0.0022 |           3.9039 |          14.7810 |
[32m[20230113 20:25:17 @agent_ppo2.py:186][0m |          -0.0056 |           3.6393 |          14.7835 |
[32m[20230113 20:25:17 @agent_ppo2.py:186][0m |          -0.0074 |           3.4497 |          14.7679 |
[32m[20230113 20:25:17 @agent_ppo2.py:186][0m |          -0.0098 |           3.2562 |          14.7651 |
[32m[20230113 20:25:17 @agent_ppo2.py:186][0m |          -0.0097 |           3.1530 |          14.7678 |
[32m[20230113 20:25:17 @agent_ppo2.py:186][0m |          -0.0110 |           3.0735 |          14.7654 |
[32m[20230113 20:25:17 @agent_ppo2.py:186][0m |          -0.0118 |           3.0260 |          14.7621 |
[32m[20230113 20:25:17 @agent_ppo2.py:186][0m |          -0.0123 |           2.9592 |          14.7498 |
[32m[20230113 20:25:17 @agent_ppo2.py:186][0m |          -0.0131 |           2.8868 |          14.7560 |
[32m[20230113 20:25:17 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:25:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.62
[32m[20230113 20:25:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.45
[32m[20230113 20:25:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.39
[32m[20230113 20:25:17 @agent_ppo2.py:144][0m Total time:      40.74 min
[32m[20230113 20:25:17 @agent_ppo2.py:146][0m 3811328 total steps have happened
[32m[20230113 20:25:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1861 --------------------------#
[32m[20230113 20:25:18 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:18 @agent_ppo2.py:186][0m |           0.0044 |           5.6897 |          14.3062 |
[32m[20230113 20:25:18 @agent_ppo2.py:186][0m |           0.0015 |           4.9950 |          14.2991 |
[32m[20230113 20:25:18 @agent_ppo2.py:186][0m |          -0.0004 |           4.3966 |          14.2925 |
[32m[20230113 20:25:18 @agent_ppo2.py:186][0m |          -0.0122 |           4.1373 |          14.2864 |
[32m[20230113 20:25:18 @agent_ppo2.py:186][0m |          -0.0050 |           3.9388 |          14.2844 |
[32m[20230113 20:25:18 @agent_ppo2.py:186][0m |          -0.0131 |           3.8072 |          14.2626 |
[32m[20230113 20:25:18 @agent_ppo2.py:186][0m |          -0.0092 |           3.6821 |          14.2512 |
[32m[20230113 20:25:18 @agent_ppo2.py:186][0m |          -0.0060 |           3.6934 |          14.2689 |
[32m[20230113 20:25:18 @agent_ppo2.py:186][0m |          -0.0209 |           3.5237 |          14.2549 |
[32m[20230113 20:25:18 @agent_ppo2.py:186][0m |          -0.0243 |           3.4862 |          14.2574 |
[32m[20230113 20:25:18 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.84
[32m[20230113 20:25:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.41
[32m[20230113 20:25:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.98
[32m[20230113 20:25:19 @agent_ppo2.py:144][0m Total time:      40.77 min
[32m[20230113 20:25:19 @agent_ppo2.py:146][0m 3813376 total steps have happened
[32m[20230113 20:25:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1862 --------------------------#
[32m[20230113 20:25:19 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:25:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:19 @agent_ppo2.py:186][0m |          -0.0022 |           6.9531 |          14.4819 |
[32m[20230113 20:25:19 @agent_ppo2.py:186][0m |          -0.0066 |           5.0054 |          14.4742 |
[32m[20230113 20:25:19 @agent_ppo2.py:186][0m |          -0.0098 |           4.4323 |          14.4754 |
[32m[20230113 20:25:19 @agent_ppo2.py:186][0m |          -0.0099 |           4.0751 |          14.4760 |
[32m[20230113 20:25:19 @agent_ppo2.py:186][0m |          -0.0130 |           3.8681 |          14.4820 |
[32m[20230113 20:25:19 @agent_ppo2.py:186][0m |          -0.0177 |           3.7303 |          14.4664 |
[32m[20230113 20:25:19 @agent_ppo2.py:186][0m |          -0.0134 |           3.6160 |          14.4709 |
[32m[20230113 20:25:19 @agent_ppo2.py:186][0m |          -0.0122 |           3.5905 |          14.4643 |
[32m[20230113 20:25:19 @agent_ppo2.py:186][0m |          -0.0099 |           3.4521 |          14.4648 |
[32m[20230113 20:25:19 @agent_ppo2.py:186][0m |          -0.0141 |           3.3318 |          14.4676 |
[32m[20230113 20:25:19 @agent_ppo2.py:131][0m Policy update time: 0.46 s
[32m[20230113 20:25:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 232.18
[32m[20230113 20:25:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.74
[32m[20230113 20:25:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.68
[32m[20230113 20:25:20 @agent_ppo2.py:144][0m Total time:      40.79 min
[32m[20230113 20:25:20 @agent_ppo2.py:146][0m 3815424 total steps have happened
[32m[20230113 20:25:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1863 --------------------------#
[32m[20230113 20:25:20 @agent_ppo2.py:128][0m Sampling time: 0.50 s by 1 slaves
[32m[20230113 20:25:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:20 @agent_ppo2.py:186][0m |           0.0022 |          12.3503 |          14.7392 |
[32m[20230113 20:25:20 @agent_ppo2.py:186][0m |           0.0005 |           5.9381 |          14.7095 |
[32m[20230113 20:25:21 @agent_ppo2.py:186][0m |          -0.0100 |           5.0975 |          14.7149 |
[32m[20230113 20:25:21 @agent_ppo2.py:186][0m |          -0.0113 |           4.6680 |          14.7085 |
[32m[20230113 20:25:21 @agent_ppo2.py:186][0m |          -0.0133 |           4.4030 |          14.6972 |
[32m[20230113 20:25:21 @agent_ppo2.py:186][0m |          -0.0140 |           4.1940 |          14.7054 |
[32m[20230113 20:25:21 @agent_ppo2.py:186][0m |          -0.0142 |           3.9462 |          14.7043 |
[32m[20230113 20:25:21 @agent_ppo2.py:186][0m |          -0.0136 |           3.9141 |          14.6929 |
[32m[20230113 20:25:21 @agent_ppo2.py:186][0m |          -0.0149 |           3.7538 |          14.7050 |
[32m[20230113 20:25:21 @agent_ppo2.py:186][0m |          -0.0179 |           3.6404 |          14.6964 |
[32m[20230113 20:25:21 @agent_ppo2.py:131][0m Policy update time: 0.51 s
[32m[20230113 20:25:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 147.81
[32m[20230113 20:25:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.20
[32m[20230113 20:25:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.91
[32m[20230113 20:25:21 @agent_ppo2.py:144][0m Total time:      40.81 min
[32m[20230113 20:25:21 @agent_ppo2.py:146][0m 3817472 total steps have happened
[32m[20230113 20:25:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1864 --------------------------#
[32m[20230113 20:25:22 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:22 @agent_ppo2.py:186][0m |          -0.0030 |           6.0475 |          14.3621 |
[32m[20230113 20:25:22 @agent_ppo2.py:186][0m |          -0.0083 |           5.0133 |          14.3456 |
[32m[20230113 20:25:22 @agent_ppo2.py:186][0m |          -0.0095 |           4.4716 |          14.3256 |
[32m[20230113 20:25:22 @agent_ppo2.py:186][0m |          -0.0075 |           4.2552 |          14.3339 |
[32m[20230113 20:25:22 @agent_ppo2.py:186][0m |          -0.0161 |           4.0421 |          14.3380 |
[32m[20230113 20:25:22 @agent_ppo2.py:186][0m |          -0.0043 |           3.9682 |          14.3338 |
[32m[20230113 20:25:22 @agent_ppo2.py:186][0m |          -0.0071 |           3.7648 |          14.3422 |
[32m[20230113 20:25:22 @agent_ppo2.py:186][0m |          -0.0170 |           3.6875 |          14.3181 |
[32m[20230113 20:25:22 @agent_ppo2.py:186][0m |          -0.0067 |           3.7196 |          14.3456 |
[32m[20230113 20:25:22 @agent_ppo2.py:186][0m |          -0.0170 |           3.5117 |          14.3204 |
[32m[20230113 20:25:22 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.40
[32m[20230113 20:25:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.22
[32m[20230113 20:25:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.72
[32m[20230113 20:25:23 @agent_ppo2.py:144][0m Total time:      40.83 min
[32m[20230113 20:25:23 @agent_ppo2.py:146][0m 3819520 total steps have happened
[32m[20230113 20:25:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1865 --------------------------#
[32m[20230113 20:25:23 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:23 @agent_ppo2.py:186][0m |           0.0049 |           6.6670 |          14.3381 |
[32m[20230113 20:25:23 @agent_ppo2.py:186][0m |           0.0001 |           5.0422 |          14.3274 |
[32m[20230113 20:25:23 @agent_ppo2.py:186][0m |          -0.0162 |           4.6520 |          14.3089 |
[32m[20230113 20:25:23 @agent_ppo2.py:186][0m |          -0.0249 |           4.3844 |          14.2966 |
[32m[20230113 20:25:23 @agent_ppo2.py:186][0m |          -0.0139 |           4.0636 |          14.3018 |
[32m[20230113 20:25:23 @agent_ppo2.py:186][0m |          -0.0183 |           3.8858 |          14.2860 |
[32m[20230113 20:25:23 @agent_ppo2.py:186][0m |          -0.0147 |           3.7645 |          14.2970 |
[32m[20230113 20:25:23 @agent_ppo2.py:186][0m |          -0.0066 |           3.7940 |          14.2732 |
[32m[20230113 20:25:23 @agent_ppo2.py:186][0m |          -0.0218 |           3.5840 |          14.2878 |
[32m[20230113 20:25:23 @agent_ppo2.py:186][0m |           0.0022 |           3.5344 |          14.2765 |
[32m[20230113 20:25:23 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.09
[32m[20230113 20:25:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.53
[32m[20230113 20:25:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.60
[32m[20230113 20:25:24 @agent_ppo2.py:144][0m Total time:      40.85 min
[32m[20230113 20:25:24 @agent_ppo2.py:146][0m 3821568 total steps have happened
[32m[20230113 20:25:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1866 --------------------------#
[32m[20230113 20:25:24 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:24 @agent_ppo2.py:186][0m |           0.0034 |           6.4701 |          14.5055 |
[32m[20230113 20:25:24 @agent_ppo2.py:186][0m |          -0.0029 |           5.2033 |          14.4953 |
[32m[20230113 20:25:24 @agent_ppo2.py:186][0m |          -0.0078 |           4.6089 |          14.4826 |
[32m[20230113 20:25:24 @agent_ppo2.py:186][0m |          -0.0100 |           4.3075 |          14.4843 |
[32m[20230113 20:25:24 @agent_ppo2.py:186][0m |          -0.0121 |           4.0379 |          14.4797 |
[32m[20230113 20:25:24 @agent_ppo2.py:186][0m |          -0.0127 |           3.8554 |          14.4835 |
[32m[20230113 20:25:25 @agent_ppo2.py:186][0m |          -0.0147 |           3.7277 |          14.4802 |
[32m[20230113 20:25:25 @agent_ppo2.py:186][0m |          -0.0155 |           3.5881 |          14.4591 |
[32m[20230113 20:25:25 @agent_ppo2.py:186][0m |          -0.0168 |           3.4854 |          14.4911 |
[32m[20230113 20:25:25 @agent_ppo2.py:186][0m |          -0.0177 |           3.3795 |          14.4847 |
[32m[20230113 20:25:25 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.86
[32m[20230113 20:25:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.19
[32m[20230113 20:25:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.20
[32m[20230113 20:25:25 @agent_ppo2.py:144][0m Total time:      40.87 min
[32m[20230113 20:25:25 @agent_ppo2.py:146][0m 3823616 total steps have happened
[32m[20230113 20:25:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1867 --------------------------#
[32m[20230113 20:25:25 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:25:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:26 @agent_ppo2.py:186][0m |          -0.0021 |           7.0224 |          14.3302 |
[32m[20230113 20:25:26 @agent_ppo2.py:186][0m |          -0.0131 |           5.2467 |          14.3004 |
[32m[20230113 20:25:26 @agent_ppo2.py:186][0m |          -0.0081 |           5.0819 |          14.2683 |
[32m[20230113 20:25:26 @agent_ppo2.py:186][0m |          -0.0224 |           4.6642 |          14.2806 |
[32m[20230113 20:25:26 @agent_ppo2.py:186][0m |          -0.0114 |           4.3194 |          14.2739 |
[32m[20230113 20:25:26 @agent_ppo2.py:186][0m |          -0.0179 |           4.0675 |          14.2759 |
[32m[20230113 20:25:26 @agent_ppo2.py:186][0m |          -0.0161 |           3.9930 |          14.2829 |
[32m[20230113 20:25:26 @agent_ppo2.py:186][0m |          -0.0111 |           3.9921 |          14.2503 |
[32m[20230113 20:25:26 @agent_ppo2.py:186][0m |          -0.0188 |           3.8578 |          14.2769 |
[32m[20230113 20:25:26 @agent_ppo2.py:186][0m |          -0.0194 |           3.7691 |          14.2566 |
[32m[20230113 20:25:26 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:25:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.98
[32m[20230113 20:25:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.21
[32m[20230113 20:25:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.37
[32m[20230113 20:25:26 @agent_ppo2.py:144][0m Total time:      40.90 min
[32m[20230113 20:25:26 @agent_ppo2.py:146][0m 3825664 total steps have happened
[32m[20230113 20:25:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1868 --------------------------#
[32m[20230113 20:25:27 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:25:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:27 @agent_ppo2.py:186][0m |           0.0025 |           5.9099 |          14.5268 |
[32m[20230113 20:25:27 @agent_ppo2.py:186][0m |          -0.0070 |           4.7291 |          14.5279 |
[32m[20230113 20:25:27 @agent_ppo2.py:186][0m |          -0.0075 |           4.2637 |          14.5069 |
[32m[20230113 20:25:27 @agent_ppo2.py:186][0m |          -0.0095 |           3.9917 |          14.5079 |
[32m[20230113 20:25:27 @agent_ppo2.py:186][0m |          -0.0090 |           3.8172 |          14.5059 |
[32m[20230113 20:25:27 @agent_ppo2.py:186][0m |          -0.0125 |           3.6380 |          14.4869 |
[32m[20230113 20:25:27 @agent_ppo2.py:186][0m |          -0.0148 |           3.5219 |          14.4913 |
[32m[20230113 20:25:27 @agent_ppo2.py:186][0m |          -0.0144 |           3.4173 |          14.4846 |
[32m[20230113 20:25:27 @agent_ppo2.py:186][0m |          -0.0155 |           3.3064 |          14.4865 |
[32m[20230113 20:25:27 @agent_ppo2.py:186][0m |          -0.0161 |           3.2623 |          14.4685 |
[32m[20230113 20:25:27 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:25:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.29
[32m[20230113 20:25:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.81
[32m[20230113 20:25:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.83
[32m[20230113 20:25:28 @agent_ppo2.py:144][0m Total time:      40.92 min
[32m[20230113 20:25:28 @agent_ppo2.py:146][0m 3827712 total steps have happened
[32m[20230113 20:25:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1869 --------------------------#
[32m[20230113 20:25:28 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:28 @agent_ppo2.py:186][0m |           0.0030 |           6.5456 |          14.5140 |
[32m[20230113 20:25:28 @agent_ppo2.py:186][0m |          -0.0050 |           4.6371 |          14.5163 |
[32m[20230113 20:25:28 @agent_ppo2.py:186][0m |          -0.0117 |           3.8261 |          14.5062 |
[32m[20230113 20:25:28 @agent_ppo2.py:186][0m |          -0.0052 |           3.4677 |          14.4992 |
[32m[20230113 20:25:28 @agent_ppo2.py:186][0m |          -0.0162 |           3.3249 |          14.4886 |
[32m[20230113 20:25:28 @agent_ppo2.py:186][0m |          -0.0024 |           3.2205 |          14.4861 |
[32m[20230113 20:25:28 @agent_ppo2.py:186][0m |          -0.0149 |           3.0407 |          14.4648 |
[32m[20230113 20:25:28 @agent_ppo2.py:186][0m |          -0.0132 |           2.9153 |          14.4693 |
[32m[20230113 20:25:28 @agent_ppo2.py:186][0m |          -0.0194 |           2.8700 |          14.4684 |
[32m[20230113 20:25:28 @agent_ppo2.py:186][0m |          -0.0197 |           2.8125 |          14.4655 |
[32m[20230113 20:25:28 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.59
[32m[20230113 20:25:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.37
[32m[20230113 20:25:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.71
[32m[20230113 20:25:29 @agent_ppo2.py:144][0m Total time:      40.94 min
[32m[20230113 20:25:29 @agent_ppo2.py:146][0m 3829760 total steps have happened
[32m[20230113 20:25:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1870 --------------------------#
[32m[20230113 20:25:29 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:25:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:29 @agent_ppo2.py:186][0m |          -0.0017 |           5.9751 |          14.4502 |
[32m[20230113 20:25:29 @agent_ppo2.py:186][0m |          -0.0069 |           4.6474 |          14.4161 |
[32m[20230113 20:25:29 @agent_ppo2.py:186][0m |          -0.0081 |           4.1691 |          14.4157 |
[32m[20230113 20:25:29 @agent_ppo2.py:186][0m |          -0.0124 |           4.0325 |          14.4069 |
[32m[20230113 20:25:29 @agent_ppo2.py:186][0m |          -0.0168 |           3.6799 |          14.4160 |
[32m[20230113 20:25:30 @agent_ppo2.py:186][0m |          -0.0146 |           3.5318 |          14.4147 |
[32m[20230113 20:25:30 @agent_ppo2.py:186][0m |          -0.0127 |           3.3724 |          14.4172 |
[32m[20230113 20:25:30 @agent_ppo2.py:186][0m |          -0.0134 |           3.2877 |          14.4007 |
[32m[20230113 20:25:30 @agent_ppo2.py:186][0m |          -0.0153 |           3.1972 |          14.4028 |
[32m[20230113 20:25:30 @agent_ppo2.py:186][0m |          -0.0144 |           3.0990 |          14.4094 |
[32m[20230113 20:25:30 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:25:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.19
[32m[20230113 20:25:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 233.93
[32m[20230113 20:25:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.56
[32m[20230113 20:25:30 @agent_ppo2.py:144][0m Total time:      40.96 min
[32m[20230113 20:25:30 @agent_ppo2.py:146][0m 3831808 total steps have happened
[32m[20230113 20:25:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1871 --------------------------#
[32m[20230113 20:25:31 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:31 @agent_ppo2.py:186][0m |          -0.0008 |           5.7065 |          14.3958 |
[32m[20230113 20:25:31 @agent_ppo2.py:186][0m |          -0.0103 |           4.5173 |          14.3713 |
[32m[20230113 20:25:31 @agent_ppo2.py:186][0m |          -0.0174 |           4.0429 |          14.3630 |
[32m[20230113 20:25:31 @agent_ppo2.py:186][0m |          -0.0121 |           3.7336 |          14.3635 |
[32m[20230113 20:25:31 @agent_ppo2.py:186][0m |          -0.0133 |           3.5180 |          14.3592 |
[32m[20230113 20:25:31 @agent_ppo2.py:186][0m |          -0.0150 |           3.3501 |          14.3618 |
[32m[20230113 20:25:31 @agent_ppo2.py:186][0m |          -0.0120 |           3.2340 |          14.3611 |
[32m[20230113 20:25:31 @agent_ppo2.py:186][0m |          -0.0220 |           3.1151 |          14.3545 |
[32m[20230113 20:25:31 @agent_ppo2.py:186][0m |          -0.0189 |           3.0215 |          14.3454 |
[32m[20230113 20:25:31 @agent_ppo2.py:186][0m |          -0.0183 |           2.9780 |          14.3539 |
[32m[20230113 20:25:31 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.33
[32m[20230113 20:25:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.43
[32m[20230113 20:25:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.05
[32m[20230113 20:25:31 @agent_ppo2.py:144][0m Total time:      40.98 min
[32m[20230113 20:25:31 @agent_ppo2.py:146][0m 3833856 total steps have happened
[32m[20230113 20:25:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1872 --------------------------#
[32m[20230113 20:25:32 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:25:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:32 @agent_ppo2.py:186][0m |           0.0003 |          12.2594 |          14.7615 |
[32m[20230113 20:25:32 @agent_ppo2.py:186][0m |          -0.0058 |           5.3936 |          14.7446 |
[32m[20230113 20:25:32 @agent_ppo2.py:186][0m |          -0.0055 |           4.6471 |          14.7407 |
[32m[20230113 20:25:32 @agent_ppo2.py:186][0m |          -0.0069 |           4.3035 |          14.7526 |
[32m[20230113 20:25:32 @agent_ppo2.py:186][0m |          -0.0051 |           3.9865 |          14.7324 |
[32m[20230113 20:25:32 @agent_ppo2.py:186][0m |          -0.0076 |           3.8395 |          14.7406 |
[32m[20230113 20:25:32 @agent_ppo2.py:186][0m |          -0.0118 |           3.6614 |          14.7461 |
[32m[20230113 20:25:32 @agent_ppo2.py:186][0m |          -0.0144 |           3.5412 |          14.7406 |
[32m[20230113 20:25:32 @agent_ppo2.py:186][0m |          -0.0091 |           3.4648 |          14.7336 |
[32m[20230113 20:25:32 @agent_ppo2.py:186][0m |          -0.0145 |           3.3702 |          14.7323 |
[32m[20230113 20:25:32 @agent_ppo2.py:131][0m Policy update time: 0.48 s
[32m[20230113 20:25:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 146.94
[32m[20230113 20:25:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.22
[32m[20230113 20:25:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.11
[32m[20230113 20:25:33 @agent_ppo2.py:144][0m Total time:      41.00 min
[32m[20230113 20:25:33 @agent_ppo2.py:146][0m 3835904 total steps have happened
[32m[20230113 20:25:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1873 --------------------------#
[32m[20230113 20:25:33 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:33 @agent_ppo2.py:186][0m |          -0.0018 |           7.0709 |          14.6600 |
[32m[20230113 20:25:33 @agent_ppo2.py:186][0m |          -0.0123 |           5.3195 |          14.6204 |
[32m[20230113 20:25:33 @agent_ppo2.py:186][0m |          -0.0114 |           4.7166 |          14.6466 |
[32m[20230113 20:25:33 @agent_ppo2.py:186][0m |          -0.0138 |           4.3768 |          14.6219 |
[32m[20230113 20:25:33 @agent_ppo2.py:186][0m |          -0.0171 |           4.2376 |          14.6357 |
[32m[20230113 20:25:33 @agent_ppo2.py:186][0m |          -0.0164 |           4.0074 |          14.6226 |
[32m[20230113 20:25:33 @agent_ppo2.py:186][0m |          -0.0166 |           3.8835 |          14.6222 |
[32m[20230113 20:25:34 @agent_ppo2.py:186][0m |          -0.0202 |           3.7855 |          14.6027 |
[32m[20230113 20:25:34 @agent_ppo2.py:186][0m |          -0.0215 |           3.6821 |          14.6232 |
[32m[20230113 20:25:34 @agent_ppo2.py:186][0m |          -0.0238 |           3.6180 |          14.6203 |
[32m[20230113 20:25:34 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.63
[32m[20230113 20:25:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.69
[32m[20230113 20:25:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.45
[32m[20230113 20:25:34 @agent_ppo2.py:144][0m Total time:      41.02 min
[32m[20230113 20:25:34 @agent_ppo2.py:146][0m 3837952 total steps have happened
[32m[20230113 20:25:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1874 --------------------------#
[32m[20230113 20:25:34 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:34 @agent_ppo2.py:186][0m |          -0.0006 |           7.1019 |          15.0134 |
[32m[20230113 20:25:35 @agent_ppo2.py:186][0m |          -0.0067 |           5.4410 |          14.9790 |
[32m[20230113 20:25:35 @agent_ppo2.py:186][0m |          -0.0103 |           4.6721 |          14.9679 |
[32m[20230113 20:25:35 @agent_ppo2.py:186][0m |          -0.0107 |           4.2869 |          14.9605 |
[32m[20230113 20:25:35 @agent_ppo2.py:186][0m |          -0.0140 |           4.0290 |          14.9849 |
[32m[20230113 20:25:35 @agent_ppo2.py:186][0m |          -0.0145 |           3.8306 |          14.9532 |
[32m[20230113 20:25:35 @agent_ppo2.py:186][0m |          -0.0140 |           3.6634 |          14.9649 |
[32m[20230113 20:25:35 @agent_ppo2.py:186][0m |          -0.0146 |           3.5636 |          14.9599 |
[32m[20230113 20:25:35 @agent_ppo2.py:186][0m |          -0.0165 |           3.4567 |          14.9617 |
[32m[20230113 20:25:35 @agent_ppo2.py:186][0m |          -0.0161 |           3.3963 |          14.9692 |
[32m[20230113 20:25:35 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.78
[32m[20230113 20:25:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.90
[32m[20230113 20:25:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 243.72
[32m[20230113 20:25:35 @agent_ppo2.py:144][0m Total time:      41.04 min
[32m[20230113 20:25:35 @agent_ppo2.py:146][0m 3840000 total steps have happened
[32m[20230113 20:25:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1875 --------------------------#
[32m[20230113 20:25:36 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:36 @agent_ppo2.py:186][0m |          -0.0008 |           6.2815 |          14.6269 |
[32m[20230113 20:25:36 @agent_ppo2.py:186][0m |          -0.0030 |           4.8903 |          14.5891 |
[32m[20230113 20:25:36 @agent_ppo2.py:186][0m |          -0.0090 |           4.4340 |          14.5934 |
[32m[20230113 20:25:36 @agent_ppo2.py:186][0m |          -0.0087 |           4.0786 |          14.5935 |
[32m[20230113 20:25:36 @agent_ppo2.py:186][0m |          -0.0080 |           3.8947 |          14.5926 |
[32m[20230113 20:25:36 @agent_ppo2.py:186][0m |          -0.0119 |           3.6758 |          14.5882 |
[32m[20230113 20:25:36 @agent_ppo2.py:186][0m |          -0.0129 |           3.5517 |          14.6001 |
[32m[20230113 20:25:36 @agent_ppo2.py:186][0m |          -0.0137 |           3.4833 |          14.5948 |
[32m[20230113 20:25:36 @agent_ppo2.py:186][0m |          -0.0127 |           3.3498 |          14.5892 |
[32m[20230113 20:25:36 @agent_ppo2.py:186][0m |          -0.0152 |           3.2794 |          14.5902 |
[32m[20230113 20:25:36 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.73
[32m[20230113 20:25:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.63
[32m[20230113 20:25:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.79
[32m[20230113 20:25:37 @agent_ppo2.py:144][0m Total time:      41.07 min
[32m[20230113 20:25:37 @agent_ppo2.py:146][0m 3842048 total steps have happened
[32m[20230113 20:25:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1876 --------------------------#
[32m[20230113 20:25:37 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:37 @agent_ppo2.py:186][0m |           0.0021 |           5.3981 |          14.6686 |
[32m[20230113 20:25:37 @agent_ppo2.py:186][0m |          -0.0048 |           3.5242 |          14.6497 |
[32m[20230113 20:25:37 @agent_ppo2.py:186][0m |          -0.0055 |           3.1873 |          14.6438 |
[32m[20230113 20:25:37 @agent_ppo2.py:186][0m |          -0.0088 |           2.9662 |          14.6565 |
[32m[20230113 20:25:37 @agent_ppo2.py:186][0m |          -0.0106 |           2.8302 |          14.6570 |
[32m[20230113 20:25:37 @agent_ppo2.py:186][0m |          -0.0118 |           2.7211 |          14.6527 |
[32m[20230113 20:25:37 @agent_ppo2.py:186][0m |          -0.0135 |           2.6255 |          14.6513 |
[32m[20230113 20:25:37 @agent_ppo2.py:186][0m |          -0.0132 |           2.5605 |          14.6283 |
[32m[20230113 20:25:37 @agent_ppo2.py:186][0m |          -0.0140 |           2.4964 |          14.6592 |
[32m[20230113 20:25:37 @agent_ppo2.py:186][0m |          -0.0158 |           2.4370 |          14.6461 |
[32m[20230113 20:25:37 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.60
[32m[20230113 20:25:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.36
[32m[20230113 20:25:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.81
[32m[20230113 20:25:38 @agent_ppo2.py:144][0m Total time:      41.09 min
[32m[20230113 20:25:38 @agent_ppo2.py:146][0m 3844096 total steps have happened
[32m[20230113 20:25:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1877 --------------------------#
[32m[20230113 20:25:38 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:38 @agent_ppo2.py:186][0m |          -0.0000 |           6.6753 |          14.5548 |
[32m[20230113 20:25:38 @agent_ppo2.py:186][0m |          -0.0143 |           5.0093 |          14.5455 |
[32m[20230113 20:25:38 @agent_ppo2.py:186][0m |          -0.0176 |           4.4248 |          14.5369 |
[32m[20230113 20:25:38 @agent_ppo2.py:186][0m |          -0.0029 |           4.0221 |          14.5448 |
[32m[20230113 20:25:38 @agent_ppo2.py:186][0m |          -0.0014 |           3.6430 |          14.5436 |
[32m[20230113 20:25:38 @agent_ppo2.py:186][0m |          -0.0170 |           3.5208 |          14.5288 |
[32m[20230113 20:25:39 @agent_ppo2.py:186][0m |          -0.0303 |           3.4073 |          14.5214 |
[32m[20230113 20:25:39 @agent_ppo2.py:186][0m |          -0.0141 |           3.2215 |          14.5037 |
[32m[20230113 20:25:39 @agent_ppo2.py:186][0m |           0.0193 |           3.0865 |          14.5438 |
[32m[20230113 20:25:39 @agent_ppo2.py:186][0m |          -0.0145 |           3.0117 |          14.4836 |
[32m[20230113 20:25:39 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.08
[32m[20230113 20:25:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.19
[32m[20230113 20:25:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.80
[32m[20230113 20:25:39 @agent_ppo2.py:144][0m Total time:      41.11 min
[32m[20230113 20:25:39 @agent_ppo2.py:146][0m 3846144 total steps have happened
[32m[20230113 20:25:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1878 --------------------------#
[32m[20230113 20:25:39 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:25:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:40 @agent_ppo2.py:186][0m |          -0.0065 |           5.3054 |          14.5482 |
[32m[20230113 20:25:40 @agent_ppo2.py:186][0m |          -0.0079 |           4.3633 |          14.5308 |
[32m[20230113 20:25:40 @agent_ppo2.py:186][0m |          -0.0101 |           3.9963 |          14.5046 |
[32m[20230113 20:25:40 @agent_ppo2.py:186][0m |          -0.0131 |           3.8085 |          14.5259 |
[32m[20230113 20:25:40 @agent_ppo2.py:186][0m |          -0.0142 |           3.5857 |          14.5173 |
[32m[20230113 20:25:40 @agent_ppo2.py:186][0m |          -0.0163 |           3.4309 |          14.5000 |
[32m[20230113 20:25:40 @agent_ppo2.py:186][0m |          -0.0152 |           3.3881 |          14.5030 |
[32m[20230113 20:25:40 @agent_ppo2.py:186][0m |          -0.0242 |           3.2852 |          14.5139 |
[32m[20230113 20:25:40 @agent_ppo2.py:186][0m |          -0.0195 |           3.2056 |          14.5027 |
[32m[20230113 20:25:40 @agent_ppo2.py:186][0m |          -0.0115 |           3.0627 |          14.4944 |
[32m[20230113 20:25:40 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:25:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.84
[32m[20230113 20:25:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.58
[32m[20230113 20:25:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.94
[32m[20230113 20:25:40 @agent_ppo2.py:144][0m Total time:      41.13 min
[32m[20230113 20:25:40 @agent_ppo2.py:146][0m 3848192 total steps have happened
[32m[20230113 20:25:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1879 --------------------------#
[32m[20230113 20:25:41 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:41 @agent_ppo2.py:186][0m |          -0.0014 |           5.9532 |          14.3979 |
[32m[20230113 20:25:41 @agent_ppo2.py:186][0m |          -0.0059 |           5.0057 |          14.3817 |
[32m[20230113 20:25:41 @agent_ppo2.py:186][0m |          -0.0015 |           4.4588 |          14.3746 |
[32m[20230113 20:25:41 @agent_ppo2.py:186][0m |          -0.0089 |           4.3762 |          14.3655 |
[32m[20230113 20:25:41 @agent_ppo2.py:186][0m |          -0.0081 |           4.2157 |          14.3730 |
[32m[20230113 20:25:41 @agent_ppo2.py:186][0m |          -0.0057 |           4.0000 |          14.3748 |
[32m[20230113 20:25:41 @agent_ppo2.py:186][0m |          -0.0090 |           3.9227 |          14.3735 |
[32m[20230113 20:25:41 @agent_ppo2.py:186][0m |          -0.0057 |           3.8012 |          14.3615 |
[32m[20230113 20:25:41 @agent_ppo2.py:186][0m |          -0.0163 |           3.7322 |          14.3715 |
[32m[20230113 20:25:41 @agent_ppo2.py:186][0m |          -0.0144 |           3.5968 |          14.3690 |
[32m[20230113 20:25:41 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.52
[32m[20230113 20:25:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.25
[32m[20230113 20:25:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.19
[32m[20230113 20:25:42 @agent_ppo2.py:144][0m Total time:      41.15 min
[32m[20230113 20:25:42 @agent_ppo2.py:146][0m 3850240 total steps have happened
[32m[20230113 20:25:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1880 --------------------------#
[32m[20230113 20:25:42 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:42 @agent_ppo2.py:186][0m |           0.0080 |           6.4279 |          14.7262 |
[32m[20230113 20:25:42 @agent_ppo2.py:186][0m |          -0.0068 |           4.7436 |          14.7070 |
[32m[20230113 20:25:42 @agent_ppo2.py:186][0m |          -0.0060 |           4.4315 |          14.6972 |
[32m[20230113 20:25:42 @agent_ppo2.py:186][0m |          -0.0130 |           4.2704 |          14.6938 |
[32m[20230113 20:25:42 @agent_ppo2.py:186][0m |          -0.0072 |           4.0802 |          14.6897 |
[32m[20230113 20:25:42 @agent_ppo2.py:186][0m |          -0.0127 |           3.9203 |          14.6911 |
[32m[20230113 20:25:42 @agent_ppo2.py:186][0m |          -0.0071 |           3.8978 |          14.6881 |
[32m[20230113 20:25:42 @agent_ppo2.py:186][0m |          -0.0127 |           3.7447 |          14.6932 |
[32m[20230113 20:25:42 @agent_ppo2.py:186][0m |          -0.0154 |           3.6891 |          14.6999 |
[32m[20230113 20:25:42 @agent_ppo2.py:186][0m |          -0.0137 |           3.6367 |          14.6838 |
[32m[20230113 20:25:42 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.29
[32m[20230113 20:25:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.82
[32m[20230113 20:25:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.96
[32m[20230113 20:25:43 @agent_ppo2.py:144][0m Total time:      41.17 min
[32m[20230113 20:25:43 @agent_ppo2.py:146][0m 3852288 total steps have happened
[32m[20230113 20:25:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1881 --------------------------#
[32m[20230113 20:25:43 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:43 @agent_ppo2.py:186][0m |          -0.0010 |           5.6755 |          14.5864 |
[32m[20230113 20:25:43 @agent_ppo2.py:186][0m |          -0.0039 |           4.5097 |          14.5759 |
[32m[20230113 20:25:43 @agent_ppo2.py:186][0m |          -0.0087 |           4.0695 |          14.5821 |
[32m[20230113 20:25:43 @agent_ppo2.py:186][0m |          -0.0117 |           3.8016 |          14.5706 |
[32m[20230113 20:25:43 @agent_ppo2.py:186][0m |          -0.0134 |           3.6185 |          14.5706 |
[32m[20230113 20:25:43 @agent_ppo2.py:186][0m |          -0.0145 |           3.4661 |          14.5741 |
[32m[20230113 20:25:44 @agent_ppo2.py:186][0m |          -0.0158 |           3.3676 |          14.5590 |
[32m[20230113 20:25:44 @agent_ppo2.py:186][0m |          -0.0154 |           3.3027 |          14.5714 |
[32m[20230113 20:25:44 @agent_ppo2.py:186][0m |          -0.0179 |           3.2199 |          14.5588 |
[32m[20230113 20:25:44 @agent_ppo2.py:186][0m |          -0.0173 |           3.1392 |          14.5606 |
[32m[20230113 20:25:44 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.13
[32m[20230113 20:25:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.51
[32m[20230113 20:25:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.49
[32m[20230113 20:25:44 @agent_ppo2.py:144][0m Total time:      41.19 min
[32m[20230113 20:25:44 @agent_ppo2.py:146][0m 3854336 total steps have happened
[32m[20230113 20:25:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1882 --------------------------#
[32m[20230113 20:25:45 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:45 @agent_ppo2.py:186][0m |           0.0053 |           5.6919 |          14.6245 |
[32m[20230113 20:25:45 @agent_ppo2.py:186][0m |          -0.0071 |           4.0876 |          14.5894 |
[32m[20230113 20:25:45 @agent_ppo2.py:186][0m |          -0.0011 |           3.6818 |          14.5803 |
[32m[20230113 20:25:45 @agent_ppo2.py:186][0m |          -0.0119 |           3.4571 |          14.5875 |
[32m[20230113 20:25:45 @agent_ppo2.py:186][0m |          -0.0110 |           3.2453 |          14.5823 |
[32m[20230113 20:25:45 @agent_ppo2.py:186][0m |          -0.0193 |           3.0995 |          14.5861 |
[32m[20230113 20:25:45 @agent_ppo2.py:186][0m |          -0.0098 |           3.0492 |          14.5801 |
[32m[20230113 20:25:45 @agent_ppo2.py:186][0m |          -0.0139 |           2.9177 |          14.5775 |
[32m[20230113 20:25:45 @agent_ppo2.py:186][0m |          -0.0151 |           2.8784 |          14.5698 |
[32m[20230113 20:25:45 @agent_ppo2.py:186][0m |          -0.0093 |           2.7983 |          14.5767 |
[32m[20230113 20:25:45 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.14
[32m[20230113 20:25:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.24
[32m[20230113 20:25:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.17
[32m[20230113 20:25:45 @agent_ppo2.py:144][0m Total time:      41.21 min
[32m[20230113 20:25:45 @agent_ppo2.py:146][0m 3856384 total steps have happened
[32m[20230113 20:25:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1883 --------------------------#
[32m[20230113 20:25:46 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:46 @agent_ppo2.py:186][0m |           0.0021 |           5.4859 |          14.6931 |
[32m[20230113 20:25:46 @agent_ppo2.py:186][0m |          -0.0056 |           4.2885 |          14.6972 |
[32m[20230113 20:25:46 @agent_ppo2.py:186][0m |          -0.0035 |           3.9988 |          14.6867 |
[32m[20230113 20:25:46 @agent_ppo2.py:186][0m |          -0.0163 |           3.5859 |          14.6818 |
[32m[20230113 20:25:46 @agent_ppo2.py:186][0m |          -0.0130 |           3.3612 |          14.6676 |
[32m[20230113 20:25:46 @agent_ppo2.py:186][0m |          -0.0128 |           3.1765 |          14.6678 |
[32m[20230113 20:25:46 @agent_ppo2.py:186][0m |          -0.0137 |           3.0560 |          14.6653 |
[32m[20230113 20:25:46 @agent_ppo2.py:186][0m |          -0.0161 |           2.9671 |          14.6601 |
[32m[20230113 20:25:46 @agent_ppo2.py:186][0m |          -0.0251 |           2.8976 |          14.6618 |
[32m[20230113 20:25:46 @agent_ppo2.py:186][0m |          -0.0189 |           2.8336 |          14.6653 |
[32m[20230113 20:25:46 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.27
[32m[20230113 20:25:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.96
[32m[20230113 20:25:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.19
[32m[20230113 20:25:47 @agent_ppo2.py:144][0m Total time:      41.23 min
[32m[20230113 20:25:47 @agent_ppo2.py:146][0m 3858432 total steps have happened
[32m[20230113 20:25:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1884 --------------------------#
[32m[20230113 20:25:47 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:47 @agent_ppo2.py:186][0m |           0.0064 |           6.8326 |          14.7165 |
[32m[20230113 20:25:47 @agent_ppo2.py:186][0m |          -0.0030 |           5.1942 |          14.6935 |
[32m[20230113 20:25:47 @agent_ppo2.py:186][0m |          -0.0088 |           4.5677 |          14.6908 |
[32m[20230113 20:25:47 @agent_ppo2.py:186][0m |          -0.0102 |           4.1950 |          14.6738 |
[32m[20230113 20:25:47 @agent_ppo2.py:186][0m |          -0.0139 |           3.9473 |          14.6764 |
[32m[20230113 20:25:47 @agent_ppo2.py:186][0m |          -0.0070 |           3.7976 |          14.6682 |
[32m[20230113 20:25:47 @agent_ppo2.py:186][0m |          -0.0142 |           3.6703 |          14.6820 |
[32m[20230113 20:25:47 @agent_ppo2.py:186][0m |          -0.0160 |           3.5657 |          14.6599 |
[32m[20230113 20:25:47 @agent_ppo2.py:186][0m |          -0.0168 |           3.4961 |          14.6487 |
[32m[20230113 20:25:47 @agent_ppo2.py:186][0m |          -0.0232 |           3.4221 |          14.6654 |
[32m[20230113 20:25:47 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.82
[32m[20230113 20:25:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.83
[32m[20230113 20:25:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.27
[32m[20230113 20:25:48 @agent_ppo2.py:144][0m Total time:      41.25 min
[32m[20230113 20:25:48 @agent_ppo2.py:146][0m 3860480 total steps have happened
[32m[20230113 20:25:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1885 --------------------------#
[32m[20230113 20:25:48 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:25:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:48 @agent_ppo2.py:186][0m |          -0.0074 |           7.2381 |          14.5351 |
[32m[20230113 20:25:48 @agent_ppo2.py:186][0m |          -0.0027 |           5.3057 |          14.5133 |
[32m[20230113 20:25:48 @agent_ppo2.py:186][0m |          -0.0201 |           4.4949 |          14.5019 |
[32m[20230113 20:25:48 @agent_ppo2.py:186][0m |          -0.0360 |           4.2962 |          14.4879 |
[32m[20230113 20:25:48 @agent_ppo2.py:186][0m |          -0.0011 |           3.9997 |          14.4943 |
[32m[20230113 20:25:49 @agent_ppo2.py:186][0m |          -0.0142 |           3.6701 |          14.4927 |
[32m[20230113 20:25:49 @agent_ppo2.py:186][0m |          -0.0073 |           3.5157 |          14.4962 |
[32m[20230113 20:25:49 @agent_ppo2.py:186][0m |          -0.0063 |           3.3335 |          14.4920 |
[32m[20230113 20:25:49 @agent_ppo2.py:186][0m |          -0.0218 |           3.2594 |          14.4814 |
[32m[20230113 20:25:49 @agent_ppo2.py:186][0m |           0.0005 |           3.6551 |          14.4726 |
[32m[20230113 20:25:49 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:25:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 243.83
[32m[20230113 20:25:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.98
[32m[20230113 20:25:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 36.45
[32m[20230113 20:25:49 @agent_ppo2.py:144][0m Total time:      41.27 min
[32m[20230113 20:25:49 @agent_ppo2.py:146][0m 3862528 total steps have happened
[32m[20230113 20:25:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1886 --------------------------#
[32m[20230113 20:25:49 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:49 @agent_ppo2.py:186][0m |           0.0025 |           5.9448 |          14.7503 |
[32m[20230113 20:25:50 @agent_ppo2.py:186][0m |           0.0035 |           4.9125 |          14.7453 |
[32m[20230113 20:25:50 @agent_ppo2.py:186][0m |          -0.0063 |           4.4326 |          14.7341 |
[32m[20230113 20:25:50 @agent_ppo2.py:186][0m |          -0.0102 |           4.1895 |          14.7529 |
[32m[20230113 20:25:50 @agent_ppo2.py:186][0m |          -0.0091 |           3.9833 |          14.7438 |
[32m[20230113 20:25:50 @agent_ppo2.py:186][0m |          -0.0122 |           3.8262 |          14.7363 |
[32m[20230113 20:25:50 @agent_ppo2.py:186][0m |          -0.0129 |           3.7768 |          14.7467 |
[32m[20230113 20:25:50 @agent_ppo2.py:186][0m |          -0.0150 |           3.6205 |          14.7408 |
[32m[20230113 20:25:50 @agent_ppo2.py:186][0m |          -0.0145 |           3.5513 |          14.7388 |
[32m[20230113 20:25:50 @agent_ppo2.py:186][0m |          -0.0207 |           3.4999 |          14.7346 |
[32m[20230113 20:25:50 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.08
[32m[20230113 20:25:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.23
[32m[20230113 20:25:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.12
[32m[20230113 20:25:50 @agent_ppo2.py:144][0m Total time:      41.29 min
[32m[20230113 20:25:50 @agent_ppo2.py:146][0m 3864576 total steps have happened
[32m[20230113 20:25:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1887 --------------------------#
[32m[20230113 20:25:51 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:51 @agent_ppo2.py:186][0m |           0.0009 |           5.9209 |          15.0387 |
[32m[20230113 20:25:51 @agent_ppo2.py:186][0m |          -0.0057 |           5.0480 |          15.0237 |
[32m[20230113 20:25:51 @agent_ppo2.py:186][0m |          -0.0081 |           4.6846 |          15.0120 |
[32m[20230113 20:25:51 @agent_ppo2.py:186][0m |          -0.0108 |           4.3768 |          15.0191 |
[32m[20230113 20:25:51 @agent_ppo2.py:186][0m |          -0.0110 |           4.1759 |          15.0012 |
[32m[20230113 20:25:51 @agent_ppo2.py:186][0m |          -0.0131 |           4.0767 |          15.0016 |
[32m[20230113 20:25:51 @agent_ppo2.py:186][0m |          -0.0137 |           3.9287 |          14.9998 |
[32m[20230113 20:25:51 @agent_ppo2.py:186][0m |          -0.0156 |           3.8451 |          15.0054 |
[32m[20230113 20:25:51 @agent_ppo2.py:186][0m |          -0.0165 |           3.8313 |          14.9959 |
[32m[20230113 20:25:51 @agent_ppo2.py:186][0m |          -0.0165 |           3.6823 |          14.9931 |
[32m[20230113 20:25:51 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.99
[32m[20230113 20:25:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.95
[32m[20230113 20:25:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.78
[32m[20230113 20:25:52 @agent_ppo2.py:144][0m Total time:      41.32 min
[32m[20230113 20:25:52 @agent_ppo2.py:146][0m 3866624 total steps have happened
[32m[20230113 20:25:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1888 --------------------------#
[32m[20230113 20:25:52 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:52 @agent_ppo2.py:186][0m |           0.0012 |           6.3031 |          14.5848 |
[32m[20230113 20:25:52 @agent_ppo2.py:186][0m |          -0.0053 |           4.5889 |          14.5833 |
[32m[20230113 20:25:52 @agent_ppo2.py:186][0m |          -0.0061 |           4.1643 |          14.5693 |
[32m[20230113 20:25:52 @agent_ppo2.py:186][0m |          -0.0057 |           3.8488 |          14.5652 |
[32m[20230113 20:25:52 @agent_ppo2.py:186][0m |          -0.0086 |           3.5812 |          14.5538 |
[32m[20230113 20:25:52 @agent_ppo2.py:186][0m |          -0.0118 |           3.4547 |          14.5640 |
[32m[20230113 20:25:52 @agent_ppo2.py:186][0m |          -0.0132 |           3.3026 |          14.5630 |
[32m[20230113 20:25:52 @agent_ppo2.py:186][0m |          -0.0149 |           3.2082 |          14.5686 |
[32m[20230113 20:25:52 @agent_ppo2.py:186][0m |          -0.0126 |           3.1529 |          14.5728 |
[32m[20230113 20:25:52 @agent_ppo2.py:186][0m |          -0.0147 |           3.0525 |          14.5546 |
[32m[20230113 20:25:52 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.06
[32m[20230113 20:25:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 246.52
[32m[20230113 20:25:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.99
[32m[20230113 20:25:53 @agent_ppo2.py:144][0m Total time:      41.34 min
[32m[20230113 20:25:53 @agent_ppo2.py:146][0m 3868672 total steps have happened
[32m[20230113 20:25:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1889 --------------------------#
[32m[20230113 20:25:53 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:53 @agent_ppo2.py:186][0m |          -0.0036 |           6.0800 |          14.5658 |
[32m[20230113 20:25:53 @agent_ppo2.py:186][0m |          -0.0067 |           4.3393 |          14.5437 |
[32m[20230113 20:25:53 @agent_ppo2.py:186][0m |          -0.0065 |           3.8917 |          14.5253 |
[32m[20230113 20:25:53 @agent_ppo2.py:186][0m |          -0.0120 |           3.6622 |          14.5270 |
[32m[20230113 20:25:53 @agent_ppo2.py:186][0m |          -0.0197 |           3.5466 |          14.5205 |
[32m[20230113 20:25:53 @agent_ppo2.py:186][0m |          -0.0122 |           3.4125 |          14.5240 |
[32m[20230113 20:25:54 @agent_ppo2.py:186][0m |          -0.0206 |           3.3340 |          14.5302 |
[32m[20230113 20:25:54 @agent_ppo2.py:186][0m |          -0.0092 |           3.2255 |          14.5203 |
[32m[20230113 20:25:54 @agent_ppo2.py:186][0m |          -0.0217 |           3.1242 |          14.5259 |
[32m[20230113 20:25:54 @agent_ppo2.py:186][0m |          -0.0079 |           3.0494 |          14.5146 |
[32m[20230113 20:25:54 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.22
[32m[20230113 20:25:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.13
[32m[20230113 20:25:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.50
[32m[20230113 20:25:54 @agent_ppo2.py:144][0m Total time:      41.36 min
[32m[20230113 20:25:54 @agent_ppo2.py:146][0m 3870720 total steps have happened
[32m[20230113 20:25:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1890 --------------------------#
[32m[20230113 20:25:54 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:55 @agent_ppo2.py:186][0m |           0.0039 |           5.6132 |          14.8400 |
[32m[20230113 20:25:55 @agent_ppo2.py:186][0m |          -0.0035 |           4.2602 |          14.8250 |
[32m[20230113 20:25:55 @agent_ppo2.py:186][0m |          -0.0026 |           3.6505 |          14.8124 |
[32m[20230113 20:25:55 @agent_ppo2.py:186][0m |          -0.0117 |           3.3867 |          14.8152 |
[32m[20230113 20:25:55 @agent_ppo2.py:186][0m |          -0.0084 |           3.3604 |          14.8095 |
[32m[20230113 20:25:55 @agent_ppo2.py:186][0m |          -0.0100 |           3.1379 |          14.7961 |
[32m[20230113 20:25:55 @agent_ppo2.py:186][0m |          -0.0145 |           3.0430 |          14.7933 |
[32m[20230113 20:25:55 @agent_ppo2.py:186][0m |          -0.0135 |           2.9750 |          14.7975 |
[32m[20230113 20:25:55 @agent_ppo2.py:186][0m |          -0.0131 |           2.8647 |          14.8010 |
[32m[20230113 20:25:55 @agent_ppo2.py:186][0m |          -0.0145 |           2.8215 |          14.7905 |
[32m[20230113 20:25:55 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.29
[32m[20230113 20:25:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.12
[32m[20230113 20:25:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.99
[32m[20230113 20:25:55 @agent_ppo2.py:144][0m Total time:      41.38 min
[32m[20230113 20:25:55 @agent_ppo2.py:146][0m 3872768 total steps have happened
[32m[20230113 20:25:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1891 --------------------------#
[32m[20230113 20:25:56 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:56 @agent_ppo2.py:186][0m |           0.0005 |           5.7045 |          14.7502 |
[32m[20230113 20:25:56 @agent_ppo2.py:186][0m |          -0.0004 |           3.9559 |          14.7116 |
[32m[20230113 20:25:56 @agent_ppo2.py:186][0m |          -0.0023 |           3.5852 |          14.7154 |
[32m[20230113 20:25:56 @agent_ppo2.py:186][0m |          -0.0358 |           3.4284 |          14.7165 |
[32m[20230113 20:25:56 @agent_ppo2.py:186][0m |          -0.0046 |           3.3316 |          14.7080 |
[32m[20230113 20:25:56 @agent_ppo2.py:186][0m |          -0.0089 |           3.0474 |          14.7070 |
[32m[20230113 20:25:56 @agent_ppo2.py:186][0m |          -0.0106 |           2.9477 |          14.7136 |
[32m[20230113 20:25:56 @agent_ppo2.py:186][0m |          -0.0162 |           2.8768 |          14.7095 |
[32m[20230113 20:25:56 @agent_ppo2.py:186][0m |          -0.0029 |           2.7993 |          14.7129 |
[32m[20230113 20:25:56 @agent_ppo2.py:186][0m |          -0.0174 |           2.7400 |          14.7171 |
[32m[20230113 20:25:56 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.65
[32m[20230113 20:25:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.93
[32m[20230113 20:25:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.34
[32m[20230113 20:25:57 @agent_ppo2.py:144][0m Total time:      41.40 min
[32m[20230113 20:25:57 @agent_ppo2.py:146][0m 3874816 total steps have happened
[32m[20230113 20:25:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1892 --------------------------#
[32m[20230113 20:25:57 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:25:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:57 @agent_ppo2.py:186][0m |           0.0010 |           5.6731 |          14.7980 |
[32m[20230113 20:25:57 @agent_ppo2.py:186][0m |          -0.0087 |           3.8385 |          14.7780 |
[32m[20230113 20:25:57 @agent_ppo2.py:186][0m |          -0.0110 |           3.4024 |          14.7658 |
[32m[20230113 20:25:57 @agent_ppo2.py:186][0m |          -0.0131 |           3.1776 |          14.7490 |
[32m[20230113 20:25:57 @agent_ppo2.py:186][0m |          -0.0132 |           3.0840 |          14.7204 |
[32m[20230113 20:25:57 @agent_ppo2.py:186][0m |          -0.0150 |           2.9578 |          14.7502 |
[32m[20230113 20:25:57 @agent_ppo2.py:186][0m |          -0.0159 |           2.8805 |          14.7299 |
[32m[20230113 20:25:57 @agent_ppo2.py:186][0m |          -0.0168 |           2.7977 |          14.7470 |
[32m[20230113 20:25:57 @agent_ppo2.py:186][0m |          -0.0181 |           2.7197 |          14.7173 |
[32m[20230113 20:25:57 @agent_ppo2.py:186][0m |          -0.0178 |           2.6012 |          14.7327 |
[32m[20230113 20:25:57 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:25:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.67
[32m[20230113 20:25:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.01
[32m[20230113 20:25:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.30
[32m[20230113 20:25:58 @agent_ppo2.py:144][0m Total time:      41.42 min
[32m[20230113 20:25:58 @agent_ppo2.py:146][0m 3876864 total steps have happened
[32m[20230113 20:25:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1893 --------------------------#
[32m[20230113 20:25:58 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:25:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:25:58 @agent_ppo2.py:186][0m |           0.0065 |           5.0799 |          14.6427 |
[32m[20230113 20:25:58 @agent_ppo2.py:186][0m |          -0.0023 |           3.8659 |          14.6165 |
[32m[20230113 20:25:58 @agent_ppo2.py:186][0m |          -0.0076 |           3.3694 |          14.6288 |
[32m[20230113 20:25:58 @agent_ppo2.py:186][0m |          -0.0119 |           3.1371 |          14.6173 |
[32m[20230113 20:25:59 @agent_ppo2.py:186][0m |          -0.0167 |           3.0479 |          14.6114 |
[32m[20230113 20:25:59 @agent_ppo2.py:186][0m |          -0.0126 |           2.8975 |          14.6153 |
[32m[20230113 20:25:59 @agent_ppo2.py:186][0m |          -0.0161 |           2.8235 |          14.6103 |
[32m[20230113 20:25:59 @agent_ppo2.py:186][0m |          -0.0152 |           2.7375 |          14.6104 |
[32m[20230113 20:25:59 @agent_ppo2.py:186][0m |          -0.0209 |           2.6677 |          14.6081 |
[32m[20230113 20:25:59 @agent_ppo2.py:186][0m |          -0.0208 |           2.6370 |          14.6195 |
[32m[20230113 20:25:59 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:25:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.11
[32m[20230113 20:25:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.46
[32m[20230113 20:25:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.57
[32m[20230113 20:25:59 @agent_ppo2.py:144][0m Total time:      41.44 min
[32m[20230113 20:25:59 @agent_ppo2.py:146][0m 3878912 total steps have happened
[32m[20230113 20:25:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1894 --------------------------#
[32m[20230113 20:26:00 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:00 @agent_ppo2.py:186][0m |          -0.0010 |           6.1564 |          14.7928 |
[32m[20230113 20:26:00 @agent_ppo2.py:186][0m |          -0.0035 |           4.5388 |          14.7654 |
[32m[20230113 20:26:00 @agent_ppo2.py:186][0m |          -0.0102 |           3.9690 |          14.7658 |
[32m[20230113 20:26:00 @agent_ppo2.py:186][0m |          -0.0136 |           3.8451 |          14.7599 |
[32m[20230113 20:26:00 @agent_ppo2.py:186][0m |          -0.0090 |           3.6552 |          14.7633 |
[32m[20230113 20:26:00 @agent_ppo2.py:186][0m |          -0.0133 |           3.4808 |          14.7633 |
[32m[20230113 20:26:00 @agent_ppo2.py:186][0m |          -0.0105 |           3.3834 |          14.7458 |
[32m[20230113 20:26:00 @agent_ppo2.py:186][0m |          -0.0136 |           3.3765 |          14.7506 |
[32m[20230113 20:26:00 @agent_ppo2.py:186][0m |          -0.0202 |           3.1511 |          14.7670 |
[32m[20230113 20:26:00 @agent_ppo2.py:186][0m |          -0.0179 |           3.1559 |          14.7509 |
[32m[20230113 20:26:00 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.88
[32m[20230113 20:26:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.98
[32m[20230113 20:26:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.47
[32m[20230113 20:26:00 @agent_ppo2.py:144][0m Total time:      41.46 min
[32m[20230113 20:26:00 @agent_ppo2.py:146][0m 3880960 total steps have happened
[32m[20230113 20:26:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1895 --------------------------#
[32m[20230113 20:26:01 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:26:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:01 @agent_ppo2.py:186][0m |           0.0010 |           5.3952 |          14.8028 |
[32m[20230113 20:26:01 @agent_ppo2.py:186][0m |          -0.0046 |           4.1540 |          14.7972 |
[32m[20230113 20:26:01 @agent_ppo2.py:186][0m |          -0.0075 |           3.7901 |          14.7954 |
[32m[20230113 20:26:01 @agent_ppo2.py:186][0m |          -0.0124 |           3.5896 |          14.7903 |
[32m[20230113 20:26:01 @agent_ppo2.py:186][0m |          -0.0115 |           3.4018 |          14.7806 |
[32m[20230113 20:26:01 @agent_ppo2.py:186][0m |          -0.0124 |           3.2469 |          14.7834 |
[32m[20230113 20:26:01 @agent_ppo2.py:186][0m |          -0.0178 |           3.1904 |          14.7842 |
[32m[20230113 20:26:01 @agent_ppo2.py:186][0m |          -0.0132 |           3.0844 |          14.7756 |
[32m[20230113 20:26:01 @agent_ppo2.py:186][0m |          -0.0170 |           3.0068 |          14.7717 |
[32m[20230113 20:26:01 @agent_ppo2.py:186][0m |          -0.0155 |           2.9368 |          14.7742 |
[32m[20230113 20:26:01 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:26:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 243.30
[32m[20230113 20:26:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.66
[32m[20230113 20:26:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 253.42
[32m[20230113 20:26:02 @agent_ppo2.py:144][0m Total time:      41.48 min
[32m[20230113 20:26:02 @agent_ppo2.py:146][0m 3883008 total steps have happened
[32m[20230113 20:26:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1896 --------------------------#
[32m[20230113 20:26:02 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:02 @agent_ppo2.py:186][0m |           0.0029 |           5.9190 |          14.8682 |
[32m[20230113 20:26:02 @agent_ppo2.py:186][0m |          -0.0048 |           4.6221 |          14.8932 |
[32m[20230113 20:26:02 @agent_ppo2.py:186][0m |          -0.0079 |           4.0406 |          14.8807 |
[32m[20230113 20:26:02 @agent_ppo2.py:186][0m |          -0.0096 |           3.7071 |          14.8761 |
[32m[20230113 20:26:02 @agent_ppo2.py:186][0m |          -0.0113 |           3.5091 |          14.8686 |
[32m[20230113 20:26:02 @agent_ppo2.py:186][0m |          -0.0138 |           3.3815 |          14.8729 |
[32m[20230113 20:26:02 @agent_ppo2.py:186][0m |          -0.0125 |           3.2213 |          14.8800 |
[32m[20230113 20:26:02 @agent_ppo2.py:186][0m |          -0.0150 |           3.1082 |          14.8848 |
[32m[20230113 20:26:02 @agent_ppo2.py:186][0m |          -0.0156 |           3.0223 |          14.8728 |
[32m[20230113 20:26:02 @agent_ppo2.py:186][0m |          -0.0169 |           2.9458 |          14.8763 |
[32m[20230113 20:26:02 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:26:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.53
[32m[20230113 20:26:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.99
[32m[20230113 20:26:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.08
[32m[20230113 20:26:03 @agent_ppo2.py:144][0m Total time:      41.50 min
[32m[20230113 20:26:03 @agent_ppo2.py:146][0m 3885056 total steps have happened
[32m[20230113 20:26:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1897 --------------------------#
[32m[20230113 20:26:03 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:26:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:03 @agent_ppo2.py:186][0m |          -0.0002 |           7.3583 |          14.7429 |
[32m[20230113 20:26:03 @agent_ppo2.py:186][0m |          -0.0053 |           5.8118 |          14.7222 |
[32m[20230113 20:26:03 @agent_ppo2.py:186][0m |          -0.0101 |           5.2309 |          14.7145 |
[32m[20230113 20:26:03 @agent_ppo2.py:186][0m |          -0.0080 |           4.8599 |          14.7160 |
[32m[20230113 20:26:03 @agent_ppo2.py:186][0m |          -0.0135 |           4.6416 |          14.7154 |
[32m[20230113 20:26:04 @agent_ppo2.py:186][0m |          -0.0157 |           4.3826 |          14.7163 |
[32m[20230113 20:26:04 @agent_ppo2.py:186][0m |          -0.0143 |           4.2683 |          14.7182 |
[32m[20230113 20:26:04 @agent_ppo2.py:186][0m |          -0.0173 |           4.1091 |          14.7031 |
[32m[20230113 20:26:04 @agent_ppo2.py:186][0m |          -0.0162 |           4.0001 |          14.7113 |
[32m[20230113 20:26:04 @agent_ppo2.py:186][0m |          -0.0170 |           3.8988 |          14.7032 |
[32m[20230113 20:26:04 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:26:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.49
[32m[20230113 20:26:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.00
[32m[20230113 20:26:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.40
[32m[20230113 20:26:04 @agent_ppo2.py:144][0m Total time:      41.53 min
[32m[20230113 20:26:04 @agent_ppo2.py:146][0m 3887104 total steps have happened
[32m[20230113 20:26:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1898 --------------------------#
[32m[20230113 20:26:05 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:26:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:05 @agent_ppo2.py:186][0m |          -0.0008 |           7.3266 |          14.6384 |
[32m[20230113 20:26:05 @agent_ppo2.py:186][0m |          -0.0101 |           5.3321 |          14.6363 |
[32m[20230113 20:26:05 @agent_ppo2.py:186][0m |          -0.0120 |           4.7058 |          14.6276 |
[32m[20230113 20:26:05 @agent_ppo2.py:186][0m |          -0.0152 |           4.2902 |          14.6329 |
[32m[20230113 20:26:05 @agent_ppo2.py:186][0m |          -0.0121 |           3.9691 |          14.6211 |
[32m[20230113 20:26:05 @agent_ppo2.py:186][0m |          -0.0166 |           3.7577 |          14.6247 |
[32m[20230113 20:26:05 @agent_ppo2.py:186][0m |          -0.0181 |           3.6431 |          14.6258 |
[32m[20230113 20:26:05 @agent_ppo2.py:186][0m |          -0.0174 |           3.4750 |          14.6217 |
[32m[20230113 20:26:05 @agent_ppo2.py:186][0m |          -0.0184 |           3.4035 |          14.6212 |
[32m[20230113 20:26:05 @agent_ppo2.py:186][0m |          -0.0189 |           3.2976 |          14.6238 |
[32m[20230113 20:26:05 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.12
[32m[20230113 20:26:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 245.08
[32m[20230113 20:26:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.10
[32m[20230113 20:26:05 @agent_ppo2.py:144][0m Total time:      41.55 min
[32m[20230113 20:26:05 @agent_ppo2.py:146][0m 3889152 total steps have happened
[32m[20230113 20:26:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1899 --------------------------#
[32m[20230113 20:26:06 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:26:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:06 @agent_ppo2.py:186][0m |           0.0013 |           6.4018 |          15.0960 |
[32m[20230113 20:26:06 @agent_ppo2.py:186][0m |          -0.0055 |           4.9048 |          15.0538 |
[32m[20230113 20:26:06 @agent_ppo2.py:186][0m |          -0.0078 |           4.3466 |          15.0601 |
[32m[20230113 20:26:06 @agent_ppo2.py:186][0m |          -0.0102 |           4.0556 |          15.0641 |
[32m[20230113 20:26:06 @agent_ppo2.py:186][0m |          -0.0105 |           3.8467 |          15.0529 |
[32m[20230113 20:26:06 @agent_ppo2.py:186][0m |          -0.0144 |           3.7293 |          15.0396 |
[32m[20230113 20:26:06 @agent_ppo2.py:186][0m |          -0.0131 |           3.5869 |          15.0493 |
[32m[20230113 20:26:06 @agent_ppo2.py:186][0m |          -0.0144 |           3.4539 |          15.0434 |
[32m[20230113 20:26:06 @agent_ppo2.py:186][0m |          -0.0157 |           3.3812 |          15.0232 |
[32m[20230113 20:26:06 @agent_ppo2.py:186][0m |          -0.0170 |           3.3097 |          15.0337 |
[32m[20230113 20:26:06 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.91
[32m[20230113 20:26:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.38
[32m[20230113 20:26:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.36
[32m[20230113 20:26:07 @agent_ppo2.py:144][0m Total time:      41.57 min
[32m[20230113 20:26:07 @agent_ppo2.py:146][0m 3891200 total steps have happened
[32m[20230113 20:26:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1900 --------------------------#
[32m[20230113 20:26:07 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:07 @agent_ppo2.py:186][0m |           0.0007 |           5.5083 |          14.7465 |
[32m[20230113 20:26:07 @agent_ppo2.py:186][0m |          -0.0050 |           4.0570 |          14.7250 |
[32m[20230113 20:26:07 @agent_ppo2.py:186][0m |          -0.0071 |           3.6014 |          14.7182 |
[32m[20230113 20:26:07 @agent_ppo2.py:186][0m |          -0.0100 |           3.3041 |          14.7207 |
[32m[20230113 20:26:07 @agent_ppo2.py:186][0m |          -0.0107 |           3.1265 |          14.7140 |
[32m[20230113 20:26:07 @agent_ppo2.py:186][0m |          -0.0108 |           2.9873 |          14.7052 |
[32m[20230113 20:26:07 @agent_ppo2.py:186][0m |          -0.0153 |           2.8823 |          14.7021 |
[32m[20230113 20:26:07 @agent_ppo2.py:186][0m |          -0.0152 |           2.8112 |          14.7039 |
[32m[20230113 20:26:07 @agent_ppo2.py:186][0m |          -0.0134 |           2.7167 |          14.7076 |
[32m[20230113 20:26:07 @agent_ppo2.py:186][0m |          -0.0151 |           2.6705 |          14.6957 |
[32m[20230113 20:26:07 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.62
[32m[20230113 20:26:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.92
[32m[20230113 20:26:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.92
[32m[20230113 20:26:08 @agent_ppo2.py:144][0m Total time:      41.59 min
[32m[20230113 20:26:08 @agent_ppo2.py:146][0m 3893248 total steps have happened
[32m[20230113 20:26:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1901 --------------------------#
[32m[20230113 20:26:08 @agent_ppo2.py:128][0m Sampling time: 0.40 s by 1 slaves
[32m[20230113 20:26:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:08 @agent_ppo2.py:186][0m |           0.0018 |           5.5169 |          15.0560 |
[32m[20230113 20:26:08 @agent_ppo2.py:186][0m |          -0.0074 |           4.4578 |          15.0638 |
[32m[20230113 20:26:08 @agent_ppo2.py:186][0m |          -0.0103 |           4.0028 |          15.0421 |
[32m[20230113 20:26:08 @agent_ppo2.py:186][0m |          -0.0106 |           3.7769 |          15.0555 |
[32m[20230113 20:26:08 @agent_ppo2.py:186][0m |          -0.0119 |           3.5846 |          15.0591 |
[32m[20230113 20:26:09 @agent_ppo2.py:186][0m |          -0.0156 |           3.4469 |          15.0519 |
[32m[20230113 20:26:09 @agent_ppo2.py:186][0m |          -0.0155 |           3.3264 |          15.0526 |
[32m[20230113 20:26:09 @agent_ppo2.py:186][0m |          -0.0160 |           3.2374 |          15.0458 |
[32m[20230113 20:26:09 @agent_ppo2.py:186][0m |          -0.0174 |           3.1767 |          15.0418 |
[32m[20230113 20:26:09 @agent_ppo2.py:186][0m |          -0.0173 |           3.0913 |          15.0393 |
[32m[20230113 20:26:09 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:26:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 245.78
[32m[20230113 20:26:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 246.92
[32m[20230113 20:26:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 150.04
[32m[20230113 20:26:09 @agent_ppo2.py:144][0m Total time:      41.61 min
[32m[20230113 20:26:09 @agent_ppo2.py:146][0m 3895296 total steps have happened
[32m[20230113 20:26:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1902 --------------------------#
[32m[20230113 20:26:09 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:09 @agent_ppo2.py:186][0m |           0.0036 |           5.7656 |          14.9513 |
[32m[20230113 20:26:10 @agent_ppo2.py:186][0m |          -0.0069 |           4.1172 |          14.9506 |
[32m[20230113 20:26:10 @agent_ppo2.py:186][0m |          -0.0032 |           3.7331 |          14.9359 |
[32m[20230113 20:26:10 @agent_ppo2.py:186][0m |          -0.0102 |           3.4500 |          14.9275 |
[32m[20230113 20:26:10 @agent_ppo2.py:186][0m |          -0.0131 |           3.2934 |          14.9289 |
[32m[20230113 20:26:10 @agent_ppo2.py:186][0m |          -0.0138 |           3.1558 |          14.9217 |
[32m[20230113 20:26:10 @agent_ppo2.py:186][0m |          -0.0149 |           3.0673 |          14.9286 |
[32m[20230113 20:26:10 @agent_ppo2.py:186][0m |          -0.0150 |           2.9680 |          14.9196 |
[32m[20230113 20:26:10 @agent_ppo2.py:186][0m |          -0.0140 |           2.9077 |          14.9110 |
[32m[20230113 20:26:10 @agent_ppo2.py:186][0m |          -0.0131 |           2.8398 |          14.9093 |
[32m[20230113 20:26:10 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.12
[32m[20230113 20:26:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.76
[32m[20230113 20:26:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.51
[32m[20230113 20:26:10 @agent_ppo2.py:144][0m Total time:      41.63 min
[32m[20230113 20:26:10 @agent_ppo2.py:146][0m 3897344 total steps have happened
[32m[20230113 20:26:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1903 --------------------------#
[32m[20230113 20:26:11 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:11 @agent_ppo2.py:186][0m |          -0.0024 |           6.1759 |          15.1416 |
[32m[20230113 20:26:11 @agent_ppo2.py:186][0m |          -0.0018 |           4.7457 |          15.1294 |
[32m[20230113 20:26:11 @agent_ppo2.py:186][0m |          -0.0094 |           4.1086 |          15.1204 |
[32m[20230113 20:26:11 @agent_ppo2.py:186][0m |          -0.0091 |           3.7758 |          15.1205 |
[32m[20230113 20:26:11 @agent_ppo2.py:186][0m |          -0.0094 |           3.6959 |          15.1121 |
[32m[20230113 20:26:11 @agent_ppo2.py:186][0m |          -0.0134 |           3.4779 |          15.1205 |
[32m[20230113 20:26:11 @agent_ppo2.py:186][0m |          -0.0083 |           3.4513 |          15.1055 |
[32m[20230113 20:26:11 @agent_ppo2.py:186][0m |          -0.0109 |           3.2975 |          15.1076 |
[32m[20230113 20:26:11 @agent_ppo2.py:186][0m |          -0.0125 |           3.1575 |          15.1083 |
[32m[20230113 20:26:11 @agent_ppo2.py:186][0m |          -0.0146 |           3.0811 |          15.1153 |
[32m[20230113 20:26:11 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.43
[32m[20230113 20:26:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.58
[32m[20230113 20:26:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.13
[32m[20230113 20:26:11 @agent_ppo2.py:144][0m Total time:      41.65 min
[32m[20230113 20:26:11 @agent_ppo2.py:146][0m 3899392 total steps have happened
[32m[20230113 20:26:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1904 --------------------------#
[32m[20230113 20:26:12 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:12 @agent_ppo2.py:186][0m |           0.0001 |           6.3447 |          14.8432 |
[32m[20230113 20:26:12 @agent_ppo2.py:186][0m |          -0.0026 |           4.5490 |          14.7838 |
[32m[20230113 20:26:12 @agent_ppo2.py:186][0m |          -0.0114 |           4.0315 |          14.7856 |
[32m[20230113 20:26:12 @agent_ppo2.py:186][0m |          -0.0127 |           3.6815 |          14.7819 |
[32m[20230113 20:26:12 @agent_ppo2.py:186][0m |          -0.0139 |           3.4402 |          14.7686 |
[32m[20230113 20:26:12 @agent_ppo2.py:186][0m |          -0.0125 |           3.3103 |          14.7845 |
[32m[20230113 20:26:12 @agent_ppo2.py:186][0m |          -0.0116 |           3.2680 |          14.7834 |
[32m[20230113 20:26:12 @agent_ppo2.py:186][0m |          -0.0144 |           3.1204 |          14.7633 |
[32m[20230113 20:26:12 @agent_ppo2.py:186][0m |          -0.0175 |           3.0355 |          14.7662 |
[32m[20230113 20:26:12 @agent_ppo2.py:186][0m |          -0.0147 |           2.9775 |          14.7715 |
[32m[20230113 20:26:12 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.94
[32m[20230113 20:26:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 246.61
[32m[20230113 20:26:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.64
[32m[20230113 20:26:13 @agent_ppo2.py:144][0m Total time:      41.67 min
[32m[20230113 20:26:13 @agent_ppo2.py:146][0m 3901440 total steps have happened
[32m[20230113 20:26:13 @agent_ppo2.py:122][0m #------------------------ Iteration 1905 --------------------------#
[32m[20230113 20:26:13 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:13 @agent_ppo2.py:186][0m |           0.0001 |           7.5639 |          15.0595 |
[32m[20230113 20:26:13 @agent_ppo2.py:186][0m |          -0.0079 |           5.7846 |          15.0483 |
[32m[20230113 20:26:13 @agent_ppo2.py:186][0m |          -0.0106 |           5.1888 |          15.0315 |
[32m[20230113 20:26:13 @agent_ppo2.py:186][0m |          -0.0118 |           4.9312 |          15.0402 |
[32m[20230113 20:26:13 @agent_ppo2.py:186][0m |          -0.0147 |           4.5722 |          15.0302 |
[32m[20230113 20:26:13 @agent_ppo2.py:186][0m |          -0.0153 |           4.3362 |          15.0295 |
[32m[20230113 20:26:13 @agent_ppo2.py:186][0m |          -0.0158 |           4.1963 |          15.0143 |
[32m[20230113 20:26:13 @agent_ppo2.py:186][0m |          -0.0176 |           4.0609 |          15.0165 |
[32m[20230113 20:26:14 @agent_ppo2.py:186][0m |          -0.0180 |           3.9647 |          15.0265 |
[32m[20230113 20:26:14 @agent_ppo2.py:186][0m |          -0.0181 |           3.8238 |          15.0261 |
[32m[20230113 20:26:14 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:26:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.24
[32m[20230113 20:26:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.82
[32m[20230113 20:26:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.89
[32m[20230113 20:26:14 @agent_ppo2.py:144][0m Total time:      41.69 min
[32m[20230113 20:26:14 @agent_ppo2.py:146][0m 3903488 total steps have happened
[32m[20230113 20:26:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1906 --------------------------#
[32m[20230113 20:26:14 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:14 @agent_ppo2.py:186][0m |          -0.0015 |           6.8365 |          14.7638 |
[32m[20230113 20:26:15 @agent_ppo2.py:186][0m |          -0.0051 |           5.0969 |          14.7670 |
[32m[20230113 20:26:15 @agent_ppo2.py:186][0m |          -0.0081 |           4.4813 |          14.7507 |
[32m[20230113 20:26:15 @agent_ppo2.py:186][0m |          -0.0091 |           4.2196 |          14.7487 |
[32m[20230113 20:26:15 @agent_ppo2.py:186][0m |          -0.0163 |           4.0155 |          14.7501 |
[32m[20230113 20:26:15 @agent_ppo2.py:186][0m |          -0.0133 |           3.9120 |          14.7435 |
[32m[20230113 20:26:15 @agent_ppo2.py:186][0m |          -0.0144 |           3.6306 |          14.7461 |
[32m[20230113 20:26:15 @agent_ppo2.py:186][0m |          -0.0151 |           3.5510 |          14.7405 |
[32m[20230113 20:26:15 @agent_ppo2.py:186][0m |          -0.0166 |           3.4682 |          14.7407 |
[32m[20230113 20:26:15 @agent_ppo2.py:186][0m |          -0.0158 |           3.3513 |          14.7332 |
[32m[20230113 20:26:15 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.84
[32m[20230113 20:26:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.00
[32m[20230113 20:26:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.89
[32m[20230113 20:26:15 @agent_ppo2.py:144][0m Total time:      41.71 min
[32m[20230113 20:26:15 @agent_ppo2.py:146][0m 3905536 total steps have happened
[32m[20230113 20:26:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1907 --------------------------#
[32m[20230113 20:26:16 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:16 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:16 @agent_ppo2.py:186][0m |           0.0002 |           5.5307 |          15.2559 |
[32m[20230113 20:26:16 @agent_ppo2.py:186][0m |          -0.0080 |           4.3770 |          15.2197 |
[32m[20230113 20:26:16 @agent_ppo2.py:186][0m |          -0.0092 |           4.0821 |          15.2024 |
[32m[20230113 20:26:16 @agent_ppo2.py:186][0m |          -0.0126 |           3.8173 |          15.2112 |
[32m[20230113 20:26:16 @agent_ppo2.py:186][0m |          -0.0160 |           3.6619 |          15.2056 |
[32m[20230113 20:26:16 @agent_ppo2.py:186][0m |          -0.0144 |           3.5062 |          15.2127 |
[32m[20230113 20:26:16 @agent_ppo2.py:186][0m |          -0.0148 |           3.4401 |          15.1984 |
[32m[20230113 20:26:16 @agent_ppo2.py:186][0m |          -0.0178 |           3.3456 |          15.1942 |
[32m[20230113 20:26:16 @agent_ppo2.py:186][0m |          -0.0193 |           3.2839 |          15.2033 |
[32m[20230113 20:26:16 @agent_ppo2.py:186][0m |          -0.0194 |           3.2061 |          15.2010 |
[32m[20230113 20:26:16 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:26:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.08
[32m[20230113 20:26:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.29
[32m[20230113 20:26:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.74
[32m[20230113 20:26:17 @agent_ppo2.py:144][0m Total time:      41.73 min
[32m[20230113 20:26:17 @agent_ppo2.py:146][0m 3907584 total steps have happened
[32m[20230113 20:26:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1908 --------------------------#
[32m[20230113 20:26:17 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:17 @agent_ppo2.py:186][0m |          -0.0001 |           6.3752 |          15.3551 |
[32m[20230113 20:26:17 @agent_ppo2.py:186][0m |          -0.0058 |           4.9670 |          15.3670 |
[32m[20230113 20:26:17 @agent_ppo2.py:186][0m |          -0.0091 |           4.3378 |          15.3790 |
[32m[20230113 20:26:17 @agent_ppo2.py:186][0m |          -0.0112 |           3.9207 |          15.3587 |
[32m[20230113 20:26:17 @agent_ppo2.py:186][0m |          -0.0118 |           3.6228 |          15.3536 |
[32m[20230113 20:26:17 @agent_ppo2.py:186][0m |          -0.0136 |           3.4189 |          15.3586 |
[32m[20230113 20:26:17 @agent_ppo2.py:186][0m |          -0.0139 |           3.2711 |          15.3614 |
[32m[20230113 20:26:17 @agent_ppo2.py:186][0m |          -0.0166 |           3.1602 |          15.3621 |
[32m[20230113 20:26:17 @agent_ppo2.py:186][0m |          -0.0160 |           3.0439 |          15.3519 |
[32m[20230113 20:26:17 @agent_ppo2.py:186][0m |          -0.0173 |           2.9650 |          15.3532 |
[32m[20230113 20:26:17 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:18 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.79
[32m[20230113 20:26:18 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 245.85
[32m[20230113 20:26:18 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.51
[32m[20230113 20:26:18 @agent_ppo2.py:144][0m Total time:      41.75 min
[32m[20230113 20:26:18 @agent_ppo2.py:146][0m 3909632 total steps have happened
[32m[20230113 20:26:18 @agent_ppo2.py:122][0m #------------------------ Iteration 1909 --------------------------#
[32m[20230113 20:26:18 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:18 @agent_ppo2.py:186][0m |           0.0048 |           6.9454 |          15.2810 |
[32m[20230113 20:26:18 @agent_ppo2.py:186][0m |          -0.0108 |           5.5798 |          15.2755 |
[32m[20230113 20:26:18 @agent_ppo2.py:186][0m |          -0.0124 |           5.0353 |          15.2545 |
[32m[20230113 20:26:18 @agent_ppo2.py:186][0m |          -0.0049 |           4.7466 |          15.2702 |
[32m[20230113 20:26:18 @agent_ppo2.py:186][0m |          -0.0109 |           4.5260 |          15.2602 |
[32m[20230113 20:26:18 @agent_ppo2.py:186][0m |          -0.0137 |           4.3555 |          15.2471 |
[32m[20230113 20:26:19 @agent_ppo2.py:186][0m |          -0.0164 |           4.2571 |          15.2506 |
[32m[20230113 20:26:19 @agent_ppo2.py:186][0m |          -0.0070 |           4.1542 |          15.2472 |
[32m[20230113 20:26:19 @agent_ppo2.py:186][0m |          -0.0143 |           3.9449 |          15.2417 |
[32m[20230113 20:26:19 @agent_ppo2.py:186][0m |          -0.0141 |           3.8727 |          15.2366 |
[32m[20230113 20:26:19 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.92
[32m[20230113 20:26:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 246.54
[32m[20230113 20:26:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.65
[32m[20230113 20:26:19 @agent_ppo2.py:144][0m Total time:      41.77 min
[32m[20230113 20:26:19 @agent_ppo2.py:146][0m 3911680 total steps have happened
[32m[20230113 20:26:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1910 --------------------------#
[32m[20230113 20:26:19 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:20 @agent_ppo2.py:186][0m |           0.0024 |           6.9257 |          15.1637 |
[32m[20230113 20:26:20 @agent_ppo2.py:186][0m |          -0.0037 |           5.4137 |          15.1561 |
[32m[20230113 20:26:20 @agent_ppo2.py:186][0m |          -0.0048 |           4.8394 |          15.1393 |
[32m[20230113 20:26:20 @agent_ppo2.py:186][0m |          -0.0064 |           4.5406 |          15.1385 |
[32m[20230113 20:26:20 @agent_ppo2.py:186][0m |          -0.0101 |           4.4106 |          15.1290 |
[32m[20230113 20:26:20 @agent_ppo2.py:186][0m |          -0.0107 |           4.1251 |          15.1355 |
[32m[20230113 20:26:20 @agent_ppo2.py:186][0m |          -0.0105 |           4.0197 |          15.1301 |
[32m[20230113 20:26:20 @agent_ppo2.py:186][0m |          -0.0118 |           3.8479 |          15.1359 |
[32m[20230113 20:26:20 @agent_ppo2.py:186][0m |          -0.0124 |           3.8516 |          15.1302 |
[32m[20230113 20:26:20 @agent_ppo2.py:186][0m |          -0.0128 |           3.7322 |          15.1355 |
[32m[20230113 20:26:20 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.77
[32m[20230113 20:26:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.17
[32m[20230113 20:26:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.42
[32m[20230113 20:26:20 @agent_ppo2.py:144][0m Total time:      41.80 min
[32m[20230113 20:26:20 @agent_ppo2.py:146][0m 3913728 total steps have happened
[32m[20230113 20:26:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1911 --------------------------#
[32m[20230113 20:26:21 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:26:21 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:21 @agent_ppo2.py:186][0m |           0.0090 |           6.2391 |          14.8321 |
[32m[20230113 20:26:21 @agent_ppo2.py:186][0m |          -0.0003 |           4.7497 |          14.8219 |
[32m[20230113 20:26:21 @agent_ppo2.py:186][0m |          -0.0090 |           4.2732 |          14.8112 |
[32m[20230113 20:26:21 @agent_ppo2.py:186][0m |          -0.0103 |           4.0189 |          14.8113 |
[32m[20230113 20:26:21 @agent_ppo2.py:186][0m |          -0.0128 |           3.8073 |          14.8208 |
[32m[20230113 20:26:21 @agent_ppo2.py:186][0m |          -0.0096 |           3.6405 |          14.8245 |
[32m[20230113 20:26:21 @agent_ppo2.py:186][0m |          -0.0167 |           3.5536 |          14.8232 |
[32m[20230113 20:26:21 @agent_ppo2.py:186][0m |          -0.0178 |           3.4696 |          14.8201 |
[32m[20230113 20:26:21 @agent_ppo2.py:186][0m |          -0.0178 |           3.3222 |          14.8237 |
[32m[20230113 20:26:21 @agent_ppo2.py:186][0m |          -0.0101 |           3.2618 |          14.8195 |
[32m[20230113 20:26:21 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:26:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.85
[32m[20230113 20:26:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.55
[32m[20230113 20:26:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.67
[32m[20230113 20:26:22 @agent_ppo2.py:144][0m Total time:      41.82 min
[32m[20230113 20:26:22 @agent_ppo2.py:146][0m 3915776 total steps have happened
[32m[20230113 20:26:22 @agent_ppo2.py:122][0m #------------------------ Iteration 1912 --------------------------#
[32m[20230113 20:26:22 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:26:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:22 @agent_ppo2.py:186][0m |           0.0002 |           6.2338 |          14.9661 |
[32m[20230113 20:26:22 @agent_ppo2.py:186][0m |          -0.0106 |           4.4975 |          14.9445 |
[32m[20230113 20:26:22 @agent_ppo2.py:186][0m |          -0.0131 |           4.0417 |          14.9387 |
[32m[20230113 20:26:22 @agent_ppo2.py:186][0m |          -0.0133 |           3.8451 |          14.9293 |
[32m[20230113 20:26:22 @agent_ppo2.py:186][0m |          -0.0137 |           3.6175 |          14.9034 |
[32m[20230113 20:26:22 @agent_ppo2.py:186][0m |          -0.0145 |           3.4648 |          14.9086 |
[32m[20230113 20:26:22 @agent_ppo2.py:186][0m |          -0.0166 |           3.3451 |          14.9080 |
[32m[20230113 20:26:22 @agent_ppo2.py:186][0m |          -0.0190 |           3.2313 |          14.9062 |
[32m[20230113 20:26:22 @agent_ppo2.py:186][0m |          -0.0180 |           3.1557 |          14.9043 |
[32m[20230113 20:26:22 @agent_ppo2.py:186][0m |          -0.0204 |           3.1108 |          14.8916 |
[32m[20230113 20:26:22 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:26:23 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.94
[32m[20230113 20:26:23 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.99
[32m[20230113 20:26:23 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.99
[32m[20230113 20:26:23 @agent_ppo2.py:144][0m Total time:      41.84 min
[32m[20230113 20:26:23 @agent_ppo2.py:146][0m 3917824 total steps have happened
[32m[20230113 20:26:23 @agent_ppo2.py:122][0m #------------------------ Iteration 1913 --------------------------#
[32m[20230113 20:26:23 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:26:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:23 @agent_ppo2.py:186][0m |          -0.0023 |           6.3582 |          15.0248 |
[32m[20230113 20:26:23 @agent_ppo2.py:186][0m |          -0.0059 |           5.3560 |          15.0056 |
[32m[20230113 20:26:23 @agent_ppo2.py:186][0m |          -0.0093 |           4.8290 |          15.0173 |
[32m[20230113 20:26:23 @agent_ppo2.py:186][0m |          -0.0063 |           4.6782 |          15.0076 |
[32m[20230113 20:26:23 @agent_ppo2.py:186][0m |          -0.0107 |           4.3519 |          15.0136 |
[32m[20230113 20:26:23 @agent_ppo2.py:186][0m |          -0.0138 |           4.1156 |          14.9844 |
[32m[20230113 20:26:24 @agent_ppo2.py:186][0m |          -0.0149 |           4.0263 |          14.9958 |
[32m[20230113 20:26:24 @agent_ppo2.py:186][0m |          -0.0107 |           3.9808 |          14.9970 |
[32m[20230113 20:26:24 @agent_ppo2.py:186][0m |          -0.0136 |           3.7780 |          15.0070 |
[32m[20230113 20:26:24 @agent_ppo2.py:186][0m |          -0.0167 |           3.6912 |          14.9968 |
[32m[20230113 20:26:24 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:26:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.03
[32m[20230113 20:26:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.56
[32m[20230113 20:26:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.45
[32m[20230113 20:26:24 @agent_ppo2.py:144][0m Total time:      41.86 min
[32m[20230113 20:26:24 @agent_ppo2.py:146][0m 3919872 total steps have happened
[32m[20230113 20:26:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1914 --------------------------#
[32m[20230113 20:26:24 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:25 @agent_ppo2.py:186][0m |           0.0009 |           6.4858 |          15.3923 |
[32m[20230113 20:26:25 @agent_ppo2.py:186][0m |          -0.0031 |           5.4694 |          15.3834 |
[32m[20230113 20:26:25 @agent_ppo2.py:186][0m |          -0.0051 |           4.8999 |          15.3949 |
[32m[20230113 20:26:25 @agent_ppo2.py:186][0m |          -0.0061 |           4.5606 |          15.3689 |
[32m[20230113 20:26:25 @agent_ppo2.py:186][0m |          -0.0070 |           4.4308 |          15.3850 |
[32m[20230113 20:26:25 @agent_ppo2.py:186][0m |          -0.0087 |           4.1476 |          15.3636 |
[32m[20230113 20:26:25 @agent_ppo2.py:186][0m |          -0.0095 |           4.0060 |          15.3804 |
[32m[20230113 20:26:25 @agent_ppo2.py:186][0m |          -0.0114 |           3.8743 |          15.3700 |
[32m[20230113 20:26:25 @agent_ppo2.py:186][0m |          -0.0114 |           3.8161 |          15.3748 |
[32m[20230113 20:26:25 @agent_ppo2.py:186][0m |          -0.0129 |           3.6916 |          15.3647 |
[32m[20230113 20:26:25 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.67
[32m[20230113 20:26:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.22
[32m[20230113 20:26:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.96
[32m[20230113 20:26:25 @agent_ppo2.py:144][0m Total time:      41.88 min
[32m[20230113 20:26:25 @agent_ppo2.py:146][0m 3921920 total steps have happened
[32m[20230113 20:26:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1915 --------------------------#
[32m[20230113 20:26:26 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:26 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:26 @agent_ppo2.py:186][0m |          -0.0030 |           5.9828 |          15.0380 |
[32m[20230113 20:26:26 @agent_ppo2.py:186][0m |          -0.0128 |           4.7206 |          15.0389 |
[32m[20230113 20:26:26 @agent_ppo2.py:186][0m |           0.0079 |           4.7334 |          15.0275 |
[32m[20230113 20:26:26 @agent_ppo2.py:186][0m |           0.0116 |           4.2655 |          15.0043 |
[32m[20230113 20:26:26 @agent_ppo2.py:186][0m |          -0.0142 |           3.8202 |          14.9978 |
[32m[20230113 20:26:26 @agent_ppo2.py:186][0m |          -0.0174 |           3.6114 |          15.0103 |
[32m[20230113 20:26:26 @agent_ppo2.py:186][0m |          -0.0201 |           3.4687 |          15.0100 |
[32m[20230113 20:26:26 @agent_ppo2.py:186][0m |          -0.0223 |           3.3546 |          15.0146 |
[32m[20230113 20:26:26 @agent_ppo2.py:186][0m |          -0.0027 |           3.2839 |          15.0092 |
[32m[20230113 20:26:26 @agent_ppo2.py:186][0m |          -0.0156 |           3.2106 |          15.0156 |
[32m[20230113 20:26:26 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:27 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.43
[32m[20230113 20:26:27 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.51
[32m[20230113 20:26:27 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.96
[32m[20230113 20:26:27 @agent_ppo2.py:144][0m Total time:      41.90 min
[32m[20230113 20:26:27 @agent_ppo2.py:146][0m 3923968 total steps have happened
[32m[20230113 20:26:27 @agent_ppo2.py:122][0m #------------------------ Iteration 1916 --------------------------#
[32m[20230113 20:26:27 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:26:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:27 @agent_ppo2.py:186][0m |           0.0001 |           7.2338 |          15.4781 |
[32m[20230113 20:26:27 @agent_ppo2.py:186][0m |          -0.0077 |           5.6018 |          15.4590 |
[32m[20230113 20:26:27 @agent_ppo2.py:186][0m |          -0.0110 |           5.0211 |          15.4589 |
[32m[20230113 20:26:27 @agent_ppo2.py:186][0m |          -0.0120 |           4.7456 |          15.4541 |
[32m[20230113 20:26:27 @agent_ppo2.py:186][0m |          -0.0136 |           4.5415 |          15.4498 |
[32m[20230113 20:26:27 @agent_ppo2.py:186][0m |          -0.0145 |           4.3475 |          15.4495 |
[32m[20230113 20:26:27 @agent_ppo2.py:186][0m |          -0.0165 |           4.2172 |          15.4491 |
[32m[20230113 20:26:27 @agent_ppo2.py:186][0m |          -0.0149 |           4.1129 |          15.4453 |
[32m[20230113 20:26:27 @agent_ppo2.py:186][0m |          -0.0174 |           4.0192 |          15.4471 |
[32m[20230113 20:26:27 @agent_ppo2.py:186][0m |          -0.0181 |           3.9101 |          15.4511 |
[32m[20230113 20:26:27 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.78
[32m[20230113 20:26:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.16
[32m[20230113 20:26:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.19
[32m[20230113 20:26:28 @agent_ppo2.py:144][0m Total time:      41.92 min
[32m[20230113 20:26:28 @agent_ppo2.py:146][0m 3926016 total steps have happened
[32m[20230113 20:26:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1917 --------------------------#
[32m[20230113 20:26:28 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:28 @agent_ppo2.py:186][0m |          -0.0018 |           6.4056 |          15.2130 |
[32m[20230113 20:26:28 @agent_ppo2.py:186][0m |          -0.0089 |           4.7010 |          15.2008 |
[32m[20230113 20:26:28 @agent_ppo2.py:186][0m |          -0.0135 |           4.2343 |          15.1981 |
[32m[20230113 20:26:28 @agent_ppo2.py:186][0m |          -0.0049 |           3.9406 |          15.1562 |
[32m[20230113 20:26:29 @agent_ppo2.py:186][0m |          -0.0125 |           3.7005 |          15.2114 |
[32m[20230113 20:26:29 @agent_ppo2.py:186][0m |          -0.0128 |           3.5999 |          15.2030 |
[32m[20230113 20:26:29 @agent_ppo2.py:186][0m |          -0.0130 |           3.3920 |          15.1943 |
[32m[20230113 20:26:29 @agent_ppo2.py:186][0m |          -0.0074 |           3.2822 |          15.1938 |
[32m[20230113 20:26:29 @agent_ppo2.py:186][0m |          -0.0105 |           3.2156 |          15.1888 |
[32m[20230113 20:26:29 @agent_ppo2.py:186][0m |          -0.0146 |           3.0999 |          15.2052 |
[32m[20230113 20:26:29 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.44
[32m[20230113 20:26:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.36
[32m[20230113 20:26:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.57
[32m[20230113 20:26:29 @agent_ppo2.py:144][0m Total time:      41.94 min
[32m[20230113 20:26:29 @agent_ppo2.py:146][0m 3928064 total steps have happened
[32m[20230113 20:26:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1918 --------------------------#
[32m[20230113 20:26:30 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:30 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:30 @agent_ppo2.py:186][0m |          -0.0016 |           6.2635 |          15.0213 |
[32m[20230113 20:26:30 @agent_ppo2.py:186][0m |          -0.0189 |           4.9873 |          15.0245 |
[32m[20230113 20:26:30 @agent_ppo2.py:186][0m |          -0.0121 |           4.4315 |          15.0190 |
[32m[20230113 20:26:30 @agent_ppo2.py:186][0m |          -0.0097 |           4.1452 |          15.0179 |
[32m[20230113 20:26:30 @agent_ppo2.py:186][0m |          -0.0202 |           3.8661 |          15.0176 |
[32m[20230113 20:26:30 @agent_ppo2.py:186][0m |          -0.0204 |           3.5694 |          15.0051 |
[32m[20230113 20:26:30 @agent_ppo2.py:186][0m |          -0.0111 |           3.4152 |          15.0148 |
[32m[20230113 20:26:30 @agent_ppo2.py:186][0m |          -0.0203 |           3.3211 |          15.0120 |
[32m[20230113 20:26:30 @agent_ppo2.py:186][0m |          -0.0211 |           3.2365 |          15.0001 |
[32m[20230113 20:26:30 @agent_ppo2.py:186][0m |          -0.0174 |           3.0911 |          15.0098 |
[32m[20230113 20:26:30 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.59
[32m[20230113 20:26:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.94
[32m[20230113 20:26:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.71
[32m[20230113 20:26:30 @agent_ppo2.py:144][0m Total time:      41.96 min
[32m[20230113 20:26:30 @agent_ppo2.py:146][0m 3930112 total steps have happened
[32m[20230113 20:26:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1919 --------------------------#
[32m[20230113 20:26:31 @agent_ppo2.py:128][0m Sampling time: 0.47 s by 1 slaves
[32m[20230113 20:26:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:31 @agent_ppo2.py:186][0m |          -0.0013 |          11.3029 |          15.2916 |
[32m[20230113 20:26:31 @agent_ppo2.py:186][0m |          -0.0069 |           6.0961 |          15.2771 |
[32m[20230113 20:26:31 @agent_ppo2.py:186][0m |          -0.0134 |           5.2036 |          15.2748 |
[32m[20230113 20:26:31 @agent_ppo2.py:186][0m |          -0.0178 |           4.5385 |          15.2622 |
[32m[20230113 20:26:31 @agent_ppo2.py:186][0m |          -0.0203 |           4.1725 |          15.2578 |
[32m[20230113 20:26:31 @agent_ppo2.py:186][0m |          -0.0199 |           3.9388 |          15.2609 |
[32m[20230113 20:26:31 @agent_ppo2.py:186][0m |          -0.0229 |           3.7807 |          15.2575 |
[32m[20230113 20:26:31 @agent_ppo2.py:186][0m |          -0.0239 |           3.6226 |          15.2603 |
[32m[20230113 20:26:31 @agent_ppo2.py:186][0m |          -0.0251 |           3.5349 |          15.2546 |
[32m[20230113 20:26:31 @agent_ppo2.py:186][0m |          -0.0242 |           3.4191 |          15.2474 |
[32m[20230113 20:26:31 @agent_ppo2.py:131][0m Policy update time: 0.49 s
[32m[20230113 20:26:32 @agent_ppo2.py:139][0m Average TRAINING episode reward: 128.75
[32m[20230113 20:26:32 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.07
[32m[20230113 20:26:32 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.30
[32m[20230113 20:26:32 @agent_ppo2.py:144][0m Total time:      41.99 min
[32m[20230113 20:26:32 @agent_ppo2.py:146][0m 3932160 total steps have happened
[32m[20230113 20:26:32 @agent_ppo2.py:122][0m #------------------------ Iteration 1920 --------------------------#
[32m[20230113 20:26:32 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:32 @agent_ppo2.py:186][0m |           0.0032 |           5.7940 |          15.5082 |
[32m[20230113 20:26:32 @agent_ppo2.py:186][0m |          -0.0091 |           4.1208 |          15.4774 |
[32m[20230113 20:26:32 @agent_ppo2.py:186][0m |          -0.0064 |           3.7603 |          15.4723 |
[32m[20230113 20:26:32 @agent_ppo2.py:186][0m |          -0.0114 |           3.4493 |          15.4668 |
[32m[20230113 20:26:32 @agent_ppo2.py:186][0m |          -0.0180 |           3.2918 |          15.4533 |
[32m[20230113 20:26:32 @agent_ppo2.py:186][0m |          -0.0161 |           3.1434 |          15.4492 |
[32m[20230113 20:26:33 @agent_ppo2.py:186][0m |          -0.0140 |           3.0965 |          15.4445 |
[32m[20230113 20:26:33 @agent_ppo2.py:186][0m |          -0.0205 |           2.9836 |          15.4283 |
[32m[20230113 20:26:33 @agent_ppo2.py:186][0m |          -0.0161 |           2.9189 |          15.4424 |
[32m[20230113 20:26:33 @agent_ppo2.py:186][0m |          -0.0134 |           2.8507 |          15.4162 |
[32m[20230113 20:26:33 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:26:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.60
[32m[20230113 20:26:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.11
[32m[20230113 20:26:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.78
[32m[20230113 20:26:33 @agent_ppo2.py:144][0m Total time:      42.01 min
[32m[20230113 20:26:33 @agent_ppo2.py:146][0m 3934208 total steps have happened
[32m[20230113 20:26:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1921 --------------------------#
[32m[20230113 20:26:33 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:34 @agent_ppo2.py:186][0m |           0.0091 |           6.6024 |          15.3743 |
[32m[20230113 20:26:34 @agent_ppo2.py:186][0m |          -0.0056 |           4.7887 |          15.3584 |
[32m[20230113 20:26:34 @agent_ppo2.py:186][0m |          -0.0066 |           4.3851 |          15.3501 |
[32m[20230113 20:26:34 @agent_ppo2.py:186][0m |          -0.0131 |           4.0416 |          15.3479 |
[32m[20230113 20:26:34 @agent_ppo2.py:186][0m |          -0.0056 |           3.8621 |          15.3289 |
[32m[20230113 20:26:34 @agent_ppo2.py:186][0m |          -0.0121 |           3.6096 |          15.3207 |
[32m[20230113 20:26:34 @agent_ppo2.py:186][0m |          -0.0139 |           3.4692 |          15.3364 |
[32m[20230113 20:26:34 @agent_ppo2.py:186][0m |          -0.0135 |           3.3700 |          15.3251 |
[32m[20230113 20:26:34 @agent_ppo2.py:186][0m |          -0.0140 |           3.2756 |          15.3281 |
[32m[20230113 20:26:34 @agent_ppo2.py:186][0m |          -0.0197 |           3.1873 |          15.3273 |
[32m[20230113 20:26:34 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.02
[32m[20230113 20:26:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.25
[32m[20230113 20:26:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.73
[32m[20230113 20:26:34 @agent_ppo2.py:144][0m Total time:      42.03 min
[32m[20230113 20:26:34 @agent_ppo2.py:146][0m 3936256 total steps have happened
[32m[20230113 20:26:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1922 --------------------------#
[32m[20230113 20:26:35 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:35 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:35 @agent_ppo2.py:186][0m |          -0.0002 |           6.6113 |          15.1963 |
[32m[20230113 20:26:35 @agent_ppo2.py:186][0m |          -0.0067 |           5.1916 |          15.1683 |
[32m[20230113 20:26:35 @agent_ppo2.py:186][0m |          -0.0097 |           4.6733 |          15.1591 |
[32m[20230113 20:26:35 @agent_ppo2.py:186][0m |          -0.0107 |           4.3647 |          15.1641 |
[32m[20230113 20:26:35 @agent_ppo2.py:186][0m |          -0.0133 |           4.2600 |          15.1417 |
[32m[20230113 20:26:35 @agent_ppo2.py:186][0m |          -0.0117 |           4.0546 |          15.1380 |
[32m[20230113 20:26:35 @agent_ppo2.py:186][0m |          -0.0140 |           3.9172 |          15.1392 |
[32m[20230113 20:26:35 @agent_ppo2.py:186][0m |          -0.0154 |           3.7921 |          15.1223 |
[32m[20230113 20:26:35 @agent_ppo2.py:186][0m |          -0.0164 |           3.6819 |          15.1261 |
[32m[20230113 20:26:35 @agent_ppo2.py:186][0m |          -0.0154 |           3.6475 |          15.1362 |
[32m[20230113 20:26:35 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:26:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.00
[32m[20230113 20:26:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.96
[32m[20230113 20:26:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.32
[32m[20230113 20:26:36 @agent_ppo2.py:144][0m Total time:      42.05 min
[32m[20230113 20:26:36 @agent_ppo2.py:146][0m 3938304 total steps have happened
[32m[20230113 20:26:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1923 --------------------------#
[32m[20230113 20:26:36 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:26:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:36 @agent_ppo2.py:186][0m |           0.0022 |           6.2647 |          15.1315 |
[32m[20230113 20:26:36 @agent_ppo2.py:186][0m |          -0.0099 |           4.7318 |          15.0845 |
[32m[20230113 20:26:36 @agent_ppo2.py:186][0m |          -0.0064 |           4.2109 |          15.0906 |
[32m[20230113 20:26:36 @agent_ppo2.py:186][0m |          -0.0124 |           3.6904 |          15.0787 |
[32m[20230113 20:26:36 @agent_ppo2.py:186][0m |          -0.0148 |           3.5233 |          15.0805 |
[32m[20230113 20:26:36 @agent_ppo2.py:186][0m |          -0.0139 |           3.3731 |          15.0731 |
[32m[20230113 20:26:36 @agent_ppo2.py:186][0m |          -0.0187 |           3.2468 |          15.0642 |
[32m[20230113 20:26:36 @agent_ppo2.py:186][0m |          -0.0193 |           3.1554 |          15.0828 |
[32m[20230113 20:26:36 @agent_ppo2.py:186][0m |          -0.0168 |           3.0821 |          15.0746 |
[32m[20230113 20:26:36 @agent_ppo2.py:186][0m |          -0.0215 |           3.0114 |          15.0599 |
[32m[20230113 20:26:36 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:37 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.24
[32m[20230113 20:26:37 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.10
[32m[20230113 20:26:37 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 253.89
[32m[20230113 20:26:37 @agent_ppo2.py:144][0m Total time:      42.07 min
[32m[20230113 20:26:37 @agent_ppo2.py:146][0m 3940352 total steps have happened
[32m[20230113 20:26:37 @agent_ppo2.py:122][0m #------------------------ Iteration 1924 --------------------------#
[32m[20230113 20:26:37 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:37 @agent_ppo2.py:186][0m |           0.0026 |           6.5590 |          15.4974 |
[32m[20230113 20:26:37 @agent_ppo2.py:186][0m |          -0.0082 |           4.6070 |          15.5100 |
[32m[20230113 20:26:37 @agent_ppo2.py:186][0m |          -0.0111 |           3.9428 |          15.4952 |
[32m[20230113 20:26:37 @agent_ppo2.py:186][0m |          -0.0126 |           3.5876 |          15.4943 |
[32m[20230113 20:26:37 @agent_ppo2.py:186][0m |          -0.0155 |           3.3663 |          15.4797 |
[32m[20230113 20:26:38 @agent_ppo2.py:186][0m |          -0.0147 |           3.1618 |          15.4911 |
[32m[20230113 20:26:38 @agent_ppo2.py:186][0m |          -0.0156 |           3.0575 |          15.4861 |
[32m[20230113 20:26:38 @agent_ppo2.py:186][0m |          -0.0168 |           2.9296 |          15.4738 |
[32m[20230113 20:26:38 @agent_ppo2.py:186][0m |          -0.0181 |           2.8825 |          15.4803 |
[32m[20230113 20:26:38 @agent_ppo2.py:186][0m |          -0.0196 |           2.7960 |          15.4728 |
[32m[20230113 20:26:38 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.12
[32m[20230113 20:26:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.09
[32m[20230113 20:26:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.13
[32m[20230113 20:26:38 @agent_ppo2.py:144][0m Total time:      42.09 min
[32m[20230113 20:26:38 @agent_ppo2.py:146][0m 3942400 total steps have happened
[32m[20230113 20:26:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1925 --------------------------#
[32m[20230113 20:26:39 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:39 @agent_ppo2.py:186][0m |           0.0037 |           5.6749 |          15.2858 |
[32m[20230113 20:26:39 @agent_ppo2.py:186][0m |          -0.0169 |           4.6124 |          15.2456 |
[32m[20230113 20:26:39 @agent_ppo2.py:186][0m |          -0.0089 |           4.2281 |          15.2429 |
[32m[20230113 20:26:39 @agent_ppo2.py:186][0m |          -0.0140 |           3.8282 |          15.2465 |
[32m[20230113 20:26:39 @agent_ppo2.py:186][0m |          -0.0098 |           3.6727 |          15.2457 |
[32m[20230113 20:26:39 @agent_ppo2.py:186][0m |          -0.0130 |           3.5413 |          15.2383 |
[32m[20230113 20:26:39 @agent_ppo2.py:186][0m |          -0.0086 |           3.3859 |          15.2427 |
[32m[20230113 20:26:39 @agent_ppo2.py:186][0m |          -0.0187 |           3.3010 |          15.2478 |
[32m[20230113 20:26:39 @agent_ppo2.py:186][0m |          -0.0238 |           3.2133 |          15.2423 |
[32m[20230113 20:26:39 @agent_ppo2.py:186][0m |          -0.0196 |           3.0764 |          15.2437 |
[32m[20230113 20:26:39 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.91
[32m[20230113 20:26:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.04
[32m[20230113 20:26:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.41
[32m[20230113 20:26:39 @agent_ppo2.py:144][0m Total time:      42.11 min
[32m[20230113 20:26:39 @agent_ppo2.py:146][0m 3944448 total steps have happened
[32m[20230113 20:26:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1926 --------------------------#
[32m[20230113 20:26:40 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:40 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:40 @agent_ppo2.py:186][0m |           0.0043 |           6.3011 |          15.0326 |
[32m[20230113 20:26:40 @agent_ppo2.py:186][0m |          -0.0014 |           4.7014 |          14.9981 |
[32m[20230113 20:26:40 @agent_ppo2.py:186][0m |          -0.0093 |           4.1598 |          15.0111 |
[32m[20230113 20:26:40 @agent_ppo2.py:186][0m |          -0.0124 |           3.7985 |          15.0102 |
[32m[20230113 20:26:40 @agent_ppo2.py:186][0m |          -0.0104 |           3.5352 |          14.9903 |
[32m[20230113 20:26:40 @agent_ppo2.py:186][0m |          -0.0125 |           3.4100 |          14.9773 |
[32m[20230113 20:26:40 @agent_ppo2.py:186][0m |          -0.0154 |           3.2483 |          14.9930 |
[32m[20230113 20:26:40 @agent_ppo2.py:186][0m |          -0.0237 |           3.1389 |          14.9877 |
[32m[20230113 20:26:40 @agent_ppo2.py:186][0m |          -0.0117 |           3.0965 |          14.9792 |
[32m[20230113 20:26:40 @agent_ppo2.py:186][0m |          -0.0162 |           2.9995 |          14.9784 |
[32m[20230113 20:26:40 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.88
[32m[20230113 20:26:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.26
[32m[20230113 20:26:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.42
[32m[20230113 20:26:41 @agent_ppo2.py:144][0m Total time:      42.13 min
[32m[20230113 20:26:41 @agent_ppo2.py:146][0m 3946496 total steps have happened
[32m[20230113 20:26:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1927 --------------------------#
[32m[20230113 20:26:41 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:41 @agent_ppo2.py:186][0m |           0.0023 |           5.1785 |          15.1217 |
[32m[20230113 20:26:41 @agent_ppo2.py:186][0m |          -0.0045 |           3.9591 |          15.0891 |
[32m[20230113 20:26:41 @agent_ppo2.py:186][0m |          -0.0128 |           3.5800 |          15.1105 |
[32m[20230113 20:26:41 @agent_ppo2.py:186][0m |          -0.0150 |           3.3770 |          15.0979 |
[32m[20230113 20:26:41 @agent_ppo2.py:186][0m |          -0.0219 |           3.3203 |          15.0908 |
[32m[20230113 20:26:41 @agent_ppo2.py:186][0m |          -0.0203 |           3.3345 |          15.0989 |
[32m[20230113 20:26:41 @agent_ppo2.py:186][0m |          -0.0069 |           3.0759 |          15.0695 |
[32m[20230113 20:26:41 @agent_ppo2.py:186][0m |          -0.0186 |           2.9512 |          15.0897 |
[32m[20230113 20:26:41 @agent_ppo2.py:186][0m |          -0.0263 |           2.8812 |          15.0858 |
[32m[20230113 20:26:41 @agent_ppo2.py:186][0m |          -0.0091 |           2.8483 |          15.0856 |
[32m[20230113 20:26:41 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:42 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.68
[32m[20230113 20:26:42 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.08
[32m[20230113 20:26:42 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.85
[32m[20230113 20:26:42 @agent_ppo2.py:144][0m Total time:      42.16 min
[32m[20230113 20:26:42 @agent_ppo2.py:146][0m 3948544 total steps have happened
[32m[20230113 20:26:42 @agent_ppo2.py:122][0m #------------------------ Iteration 1928 --------------------------#
[32m[20230113 20:26:42 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:26:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:42 @agent_ppo2.py:186][0m |          -0.0016 |           5.6579 |          15.4614 |
[32m[20230113 20:26:42 @agent_ppo2.py:186][0m |          -0.0076 |           4.3361 |          15.4529 |
[32m[20230113 20:26:42 @agent_ppo2.py:186][0m |          -0.0106 |           3.8940 |          15.4424 |
[32m[20230113 20:26:43 @agent_ppo2.py:186][0m |          -0.0120 |           3.6514 |          15.4438 |
[32m[20230113 20:26:43 @agent_ppo2.py:186][0m |          -0.0127 |           3.4487 |          15.4297 |
[32m[20230113 20:26:43 @agent_ppo2.py:186][0m |          -0.0132 |           3.3354 |          15.4443 |
[32m[20230113 20:26:43 @agent_ppo2.py:186][0m |          -0.0156 |           3.2681 |          15.4386 |
[32m[20230113 20:26:43 @agent_ppo2.py:186][0m |          -0.0170 |           3.1675 |          15.4407 |
[32m[20230113 20:26:43 @agent_ppo2.py:186][0m |          -0.0172 |           3.1515 |          15.4244 |
[32m[20230113 20:26:43 @agent_ppo2.py:186][0m |          -0.0172 |           3.0563 |          15.4229 |
[32m[20230113 20:26:43 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.42
[32m[20230113 20:26:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 235.98
[32m[20230113 20:26:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.57
[32m[20230113 20:26:43 @agent_ppo2.py:144][0m Total time:      42.18 min
[32m[20230113 20:26:43 @agent_ppo2.py:146][0m 3950592 total steps have happened
[32m[20230113 20:26:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1929 --------------------------#
[32m[20230113 20:26:44 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:44 @agent_ppo2.py:186][0m |           0.0025 |           6.5297 |          15.2750 |
[32m[20230113 20:26:44 @agent_ppo2.py:186][0m |          -0.0065 |           5.0111 |          15.2328 |
[32m[20230113 20:26:44 @agent_ppo2.py:186][0m |          -0.0095 |           4.5042 |          15.2206 |
[32m[20230113 20:26:44 @agent_ppo2.py:186][0m |          -0.0107 |           4.1688 |          15.2103 |
[32m[20230113 20:26:44 @agent_ppo2.py:186][0m |          -0.0167 |           3.8991 |          15.2097 |
[32m[20230113 20:26:44 @agent_ppo2.py:186][0m |          -0.0110 |           3.7762 |          15.2129 |
[32m[20230113 20:26:44 @agent_ppo2.py:186][0m |          -0.0137 |           3.5889 |          15.2074 |
[32m[20230113 20:26:44 @agent_ppo2.py:186][0m |          -0.0114 |           3.4773 |          15.2135 |
[32m[20230113 20:26:44 @agent_ppo2.py:186][0m |          -0.0160 |           3.3801 |          15.1924 |
[32m[20230113 20:26:44 @agent_ppo2.py:186][0m |          -0.0142 |           3.2443 |          15.2088 |
[32m[20230113 20:26:44 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.39
[32m[20230113 20:26:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.73
[32m[20230113 20:26:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.86
[32m[20230113 20:26:44 @agent_ppo2.py:144][0m Total time:      42.20 min
[32m[20230113 20:26:44 @agent_ppo2.py:146][0m 3952640 total steps have happened
[32m[20230113 20:26:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1930 --------------------------#
[32m[20230113 20:26:45 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:45 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:45 @agent_ppo2.py:186][0m |           0.0007 |           4.6993 |          15.1528 |
[32m[20230113 20:26:45 @agent_ppo2.py:186][0m |          -0.0076 |           3.4315 |          15.1422 |
[32m[20230113 20:26:45 @agent_ppo2.py:186][0m |          -0.0089 |           3.1707 |          15.1381 |
[32m[20230113 20:26:45 @agent_ppo2.py:186][0m |          -0.0090 |           2.9650 |          15.1263 |
[32m[20230113 20:26:45 @agent_ppo2.py:186][0m |          -0.0094 |           2.8109 |          15.1227 |
[32m[20230113 20:26:45 @agent_ppo2.py:186][0m |          -0.0135 |           2.7218 |          15.1073 |
[32m[20230113 20:26:45 @agent_ppo2.py:186][0m |          -0.0136 |           2.6298 |          15.1071 |
[32m[20230113 20:26:45 @agent_ppo2.py:186][0m |          -0.0141 |           2.5531 |          15.1092 |
[32m[20230113 20:26:45 @agent_ppo2.py:186][0m |          -0.0176 |           2.5100 |          15.1049 |
[32m[20230113 20:26:45 @agent_ppo2.py:186][0m |          -0.0167 |           2.4618 |          15.0962 |
[32m[20230113 20:26:45 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:46 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.01
[32m[20230113 20:26:46 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 247.04
[32m[20230113 20:26:46 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.86
[32m[20230113 20:26:46 @agent_ppo2.py:144][0m Total time:      42.22 min
[32m[20230113 20:26:46 @agent_ppo2.py:146][0m 3954688 total steps have happened
[32m[20230113 20:26:46 @agent_ppo2.py:122][0m #------------------------ Iteration 1931 --------------------------#
[32m[20230113 20:26:46 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:26:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:46 @agent_ppo2.py:186][0m |           0.0006 |           4.6785 |          15.3252 |
[32m[20230113 20:26:46 @agent_ppo2.py:186][0m |          -0.0077 |           3.7323 |          15.3091 |
[32m[20230113 20:26:46 @agent_ppo2.py:186][0m |          -0.0101 |           3.3972 |          15.3050 |
[32m[20230113 20:26:46 @agent_ppo2.py:186][0m |          -0.0106 |           3.2558 |          15.3066 |
[32m[20230113 20:26:46 @agent_ppo2.py:186][0m |          -0.0127 |           3.0809 |          15.2734 |
[32m[20230113 20:26:46 @agent_ppo2.py:186][0m |          -0.0140 |           2.9884 |          15.2874 |
[32m[20230113 20:26:46 @agent_ppo2.py:186][0m |          -0.0149 |           2.9064 |          15.2664 |
[32m[20230113 20:26:46 @agent_ppo2.py:186][0m |          -0.0158 |           2.8173 |          15.2740 |
[32m[20230113 20:26:47 @agent_ppo2.py:186][0m |          -0.0161 |           2.7506 |          15.2663 |
[32m[20230113 20:26:47 @agent_ppo2.py:186][0m |          -0.0179 |           2.7050 |          15.2682 |
[32m[20230113 20:26:47 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.89
[32m[20230113 20:26:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.16
[32m[20230113 20:26:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.42
[32m[20230113 20:26:47 @agent_ppo2.py:144][0m Total time:      42.24 min
[32m[20230113 20:26:47 @agent_ppo2.py:146][0m 3956736 total steps have happened
[32m[20230113 20:26:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1932 --------------------------#
[32m[20230113 20:26:47 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:47 @agent_ppo2.py:186][0m |          -0.0012 |           6.1268 |          15.4523 |
[32m[20230113 20:26:48 @agent_ppo2.py:186][0m |          -0.0131 |           4.8075 |          15.4332 |
[32m[20230113 20:26:48 @agent_ppo2.py:186][0m |          -0.0120 |           4.3083 |          15.4207 |
[32m[20230113 20:26:48 @agent_ppo2.py:186][0m |          -0.0132 |           4.0718 |          15.4114 |
[32m[20230113 20:26:48 @agent_ppo2.py:186][0m |          -0.0103 |           3.7816 |          15.4274 |
[32m[20230113 20:26:48 @agent_ppo2.py:186][0m |          -0.0130 |           3.6573 |          15.4092 |
[32m[20230113 20:26:48 @agent_ppo2.py:186][0m |          -0.0077 |           3.6131 |          15.4053 |
[32m[20230113 20:26:48 @agent_ppo2.py:186][0m |          -0.0205 |           3.4388 |          15.4069 |
[32m[20230113 20:26:48 @agent_ppo2.py:186][0m |          -0.0182 |           3.3963 |          15.4051 |
[32m[20230113 20:26:48 @agent_ppo2.py:186][0m |          -0.0196 |           3.3315 |          15.4060 |
[32m[20230113 20:26:48 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.68
[32m[20230113 20:26:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.29
[32m[20230113 20:26:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.90
[32m[20230113 20:26:48 @agent_ppo2.py:144][0m Total time:      42.26 min
[32m[20230113 20:26:48 @agent_ppo2.py:146][0m 3958784 total steps have happened
[32m[20230113 20:26:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1933 --------------------------#
[32m[20230113 20:26:49 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:49 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:49 @agent_ppo2.py:186][0m |           0.0024 |           6.4561 |          15.3953 |
[32m[20230113 20:26:49 @agent_ppo2.py:186][0m |          -0.0053 |           5.1300 |          15.3821 |
[32m[20230113 20:26:49 @agent_ppo2.py:186][0m |          -0.0067 |           4.6065 |          15.3757 |
[32m[20230113 20:26:49 @agent_ppo2.py:186][0m |          -0.0100 |           4.3215 |          15.3569 |
[32m[20230113 20:26:49 @agent_ppo2.py:186][0m |          -0.0085 |           4.0683 |          15.3771 |
[32m[20230113 20:26:49 @agent_ppo2.py:186][0m |          -0.0161 |           3.8943 |          15.3590 |
[32m[20230113 20:26:49 @agent_ppo2.py:186][0m |          -0.0135 |           3.7876 |          15.3673 |
[32m[20230113 20:26:49 @agent_ppo2.py:186][0m |          -0.0150 |           3.7205 |          15.3522 |
[32m[20230113 20:26:49 @agent_ppo2.py:186][0m |          -0.0154 |           3.6263 |          15.3518 |
[32m[20230113 20:26:49 @agent_ppo2.py:186][0m |          -0.0165 |           3.5001 |          15.3490 |
[32m[20230113 20:26:49 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.87
[32m[20230113 20:26:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.48
[32m[20230113 20:26:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.70
[32m[20230113 20:26:49 @agent_ppo2.py:144][0m Total time:      42.28 min
[32m[20230113 20:26:49 @agent_ppo2.py:146][0m 3960832 total steps have happened
[32m[20230113 20:26:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1934 --------------------------#
[32m[20230113 20:26:50 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:26:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:50 @agent_ppo2.py:186][0m |           0.0008 |           6.0003 |          15.6490 |
[32m[20230113 20:26:50 @agent_ppo2.py:186][0m |          -0.0058 |           4.2237 |          15.6399 |
[32m[20230113 20:26:50 @agent_ppo2.py:186][0m |          -0.0089 |           3.8050 |          15.6376 |
[32m[20230113 20:26:50 @agent_ppo2.py:186][0m |          -0.0099 |           3.5856 |          15.6243 |
[32m[20230113 20:26:50 @agent_ppo2.py:186][0m |          -0.0111 |           3.4012 |          15.6041 |
[32m[20230113 20:26:50 @agent_ppo2.py:186][0m |          -0.0132 |           3.2866 |          15.6155 |
[32m[20230113 20:26:50 @agent_ppo2.py:186][0m |          -0.0140 |           3.1668 |          15.6078 |
[32m[20230113 20:26:50 @agent_ppo2.py:186][0m |          -0.0144 |           3.0831 |          15.5983 |
[32m[20230113 20:26:50 @agent_ppo2.py:186][0m |          -0.0152 |           2.9904 |          15.6000 |
[32m[20230113 20:26:50 @agent_ppo2.py:186][0m |          -0.0169 |           2.9385 |          15.5996 |
[32m[20230113 20:26:50 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:51 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.96
[32m[20230113 20:26:51 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.80
[32m[20230113 20:26:51 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.34
[32m[20230113 20:26:51 @agent_ppo2.py:144][0m Total time:      42.30 min
[32m[20230113 20:26:51 @agent_ppo2.py:146][0m 3962880 total steps have happened
[32m[20230113 20:26:51 @agent_ppo2.py:122][0m #------------------------ Iteration 1935 --------------------------#
[32m[20230113 20:26:51 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:26:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:51 @agent_ppo2.py:186][0m |           0.0015 |           5.8777 |          15.4378 |
[32m[20230113 20:26:51 @agent_ppo2.py:186][0m |          -0.0037 |           4.5098 |          15.4229 |
[32m[20230113 20:26:51 @agent_ppo2.py:186][0m |          -0.0092 |           3.9442 |          15.4289 |
[32m[20230113 20:26:51 @agent_ppo2.py:186][0m |          -0.0088 |           3.6004 |          15.4213 |
[32m[20230113 20:26:51 @agent_ppo2.py:186][0m |          -0.0129 |           3.4467 |          15.4247 |
[32m[20230113 20:26:52 @agent_ppo2.py:186][0m |          -0.0132 |           3.2018 |          15.4157 |
[32m[20230113 20:26:52 @agent_ppo2.py:186][0m |          -0.0150 |           3.1613 |          15.4175 |
[32m[20230113 20:26:52 @agent_ppo2.py:186][0m |          -0.0154 |           3.0182 |          15.4236 |
[32m[20230113 20:26:52 @agent_ppo2.py:186][0m |          -0.0157 |           2.9437 |          15.4162 |
[32m[20230113 20:26:52 @agent_ppo2.py:186][0m |          -0.0159 |           2.8801 |          15.4153 |
[32m[20230113 20:26:52 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.68
[32m[20230113 20:26:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.06
[32m[20230113 20:26:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.58
[32m[20230113 20:26:52 @agent_ppo2.py:144][0m Total time:      42.32 min
[32m[20230113 20:26:52 @agent_ppo2.py:146][0m 3964928 total steps have happened
[32m[20230113 20:26:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1936 --------------------------#
[32m[20230113 20:26:52 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:53 @agent_ppo2.py:186][0m |           0.0022 |           6.0761 |          15.6099 |
[32m[20230113 20:26:53 @agent_ppo2.py:186][0m |          -0.0051 |           4.5864 |          15.6031 |
[32m[20230113 20:26:53 @agent_ppo2.py:186][0m |          -0.0068 |           4.1117 |          15.5994 |
[32m[20230113 20:26:53 @agent_ppo2.py:186][0m |          -0.0065 |           3.7743 |          15.6038 |
[32m[20230113 20:26:53 @agent_ppo2.py:186][0m |          -0.0090 |           3.5851 |          15.5845 |
[32m[20230113 20:26:53 @agent_ppo2.py:186][0m |          -0.0093 |           3.4121 |          15.5856 |
[32m[20230113 20:26:53 @agent_ppo2.py:186][0m |          -0.0123 |           3.3213 |          15.5792 |
[32m[20230113 20:26:53 @agent_ppo2.py:186][0m |          -0.0125 |           3.2215 |          15.5861 |
[32m[20230113 20:26:53 @agent_ppo2.py:186][0m |          -0.0157 |           3.2079 |          15.5841 |
[32m[20230113 20:26:53 @agent_ppo2.py:186][0m |          -0.0145 |           3.1313 |          15.5617 |
[32m[20230113 20:26:53 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.16
[32m[20230113 20:26:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.88
[32m[20230113 20:26:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.43
[32m[20230113 20:26:53 @agent_ppo2.py:144][0m Total time:      42.35 min
[32m[20230113 20:26:53 @agent_ppo2.py:146][0m 3966976 total steps have happened
[32m[20230113 20:26:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1937 --------------------------#
[32m[20230113 20:26:54 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:54 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:54 @agent_ppo2.py:186][0m |           0.0026 |           5.9302 |          15.3079 |
[32m[20230113 20:26:54 @agent_ppo2.py:186][0m |          -0.0058 |           4.8184 |          15.2628 |
[32m[20230113 20:26:54 @agent_ppo2.py:186][0m |          -0.0098 |           4.3441 |          15.2702 |
[32m[20230113 20:26:54 @agent_ppo2.py:186][0m |          -0.0120 |           4.0436 |          15.2712 |
[32m[20230113 20:26:54 @agent_ppo2.py:186][0m |          -0.0110 |           3.8501 |          15.2523 |
[32m[20230113 20:26:54 @agent_ppo2.py:186][0m |          -0.0161 |           3.7128 |          15.2536 |
[32m[20230113 20:26:54 @agent_ppo2.py:186][0m |          -0.0141 |           3.6052 |          15.2598 |
[32m[20230113 20:26:54 @agent_ppo2.py:186][0m |          -0.0145 |           3.4874 |          15.2481 |
[32m[20230113 20:26:54 @agent_ppo2.py:186][0m |          -0.0136 |           3.4168 |          15.2307 |
[32m[20230113 20:26:54 @agent_ppo2.py:186][0m |          -0.0158 |           3.3422 |          15.2340 |
[32m[20230113 20:26:54 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.19
[32m[20230113 20:26:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.87
[32m[20230113 20:26:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.88
[32m[20230113 20:26:55 @agent_ppo2.py:144][0m Total time:      42.37 min
[32m[20230113 20:26:55 @agent_ppo2.py:146][0m 3969024 total steps have happened
[32m[20230113 20:26:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1938 --------------------------#
[32m[20230113 20:26:55 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:55 @agent_ppo2.py:186][0m |           0.0203 |           5.4408 |          14.9381 |
[32m[20230113 20:26:55 @agent_ppo2.py:186][0m |           0.0217 |           4.6081 |          14.9146 |
[32m[20230113 20:26:55 @agent_ppo2.py:186][0m |           0.0017 |           3.9625 |          14.9193 |
[32m[20230113 20:26:55 @agent_ppo2.py:186][0m |          -0.0066 |           3.7367 |          14.9168 |
[32m[20230113 20:26:55 @agent_ppo2.py:186][0m |          -0.0178 |           3.5628 |          14.9167 |
[32m[20230113 20:26:55 @agent_ppo2.py:186][0m |          -0.0174 |           3.4628 |          14.9118 |
[32m[20230113 20:26:55 @agent_ppo2.py:186][0m |          -0.0107 |           3.3249 |          14.9102 |
[32m[20230113 20:26:55 @agent_ppo2.py:186][0m |          -0.0220 |           3.3076 |          14.9090 |
[32m[20230113 20:26:55 @agent_ppo2.py:186][0m |          -0.0232 |           3.2023 |          14.8971 |
[32m[20230113 20:26:55 @agent_ppo2.py:186][0m |          -0.0162 |           3.1020 |          14.9045 |
[32m[20230113 20:26:55 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:56 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.68
[32m[20230113 20:26:56 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.10
[32m[20230113 20:26:56 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.70
[32m[20230113 20:26:56 @agent_ppo2.py:144][0m Total time:      42.39 min
[32m[20230113 20:26:56 @agent_ppo2.py:146][0m 3971072 total steps have happened
[32m[20230113 20:26:56 @agent_ppo2.py:122][0m #------------------------ Iteration 1939 --------------------------#
[32m[20230113 20:26:56 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:56 @agent_ppo2.py:186][0m |          -0.0008 |           5.2144 |          15.6162 |
[32m[20230113 20:26:56 @agent_ppo2.py:186][0m |          -0.0066 |           4.5107 |          15.6004 |
[32m[20230113 20:26:56 @agent_ppo2.py:186][0m |          -0.0108 |           4.1067 |          15.6078 |
[32m[20230113 20:26:56 @agent_ppo2.py:186][0m |          -0.0110 |           3.9157 |          15.5929 |
[32m[20230113 20:26:57 @agent_ppo2.py:186][0m |          -0.0141 |           3.7479 |          15.6080 |
[32m[20230113 20:26:57 @agent_ppo2.py:186][0m |          -0.0152 |           3.6434 |          15.6109 |
[32m[20230113 20:26:57 @agent_ppo2.py:186][0m |          -0.0173 |           3.5509 |          15.6002 |
[32m[20230113 20:26:57 @agent_ppo2.py:186][0m |          -0.0173 |           3.4729 |          15.6028 |
[32m[20230113 20:26:57 @agent_ppo2.py:186][0m |          -0.0172 |           3.4191 |          15.6032 |
[32m[20230113 20:26:57 @agent_ppo2.py:186][0m |          -0.0202 |           3.3350 |          15.6057 |
[32m[20230113 20:26:57 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:26:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.96
[32m[20230113 20:26:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.02
[32m[20230113 20:26:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.64
[32m[20230113 20:26:57 @agent_ppo2.py:144][0m Total time:      42.41 min
[32m[20230113 20:26:57 @agent_ppo2.py:146][0m 3973120 total steps have happened
[32m[20230113 20:26:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1940 --------------------------#
[32m[20230113 20:26:58 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:58 @agent_ppo2.py:186][0m |           0.0029 |           6.7068 |          15.3084 |
[32m[20230113 20:26:58 @agent_ppo2.py:186][0m |          -0.0176 |           4.6415 |          15.3006 |
[32m[20230113 20:26:58 @agent_ppo2.py:186][0m |          -0.0101 |           4.2602 |          15.2799 |
[32m[20230113 20:26:58 @agent_ppo2.py:186][0m |          -0.0113 |           3.9863 |          15.2860 |
[32m[20230113 20:26:58 @agent_ppo2.py:186][0m |          -0.0089 |           3.7487 |          15.2921 |
[32m[20230113 20:26:58 @agent_ppo2.py:186][0m |          -0.0133 |           3.6430 |          15.2793 |
[32m[20230113 20:26:58 @agent_ppo2.py:186][0m |          -0.0113 |           3.5282 |          15.2837 |
[32m[20230113 20:26:58 @agent_ppo2.py:186][0m |          -0.0090 |           3.4042 |          15.2866 |
[32m[20230113 20:26:58 @agent_ppo2.py:186][0m |          -0.0415 |           3.3998 |          15.2561 |
[32m[20230113 20:26:58 @agent_ppo2.py:186][0m |          -0.0187 |           3.4561 |          15.2802 |
[32m[20230113 20:26:58 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:26:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.00
[32m[20230113 20:26:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.93
[32m[20230113 20:26:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.48
[32m[20230113 20:26:58 @agent_ppo2.py:144][0m Total time:      42.43 min
[32m[20230113 20:26:58 @agent_ppo2.py:146][0m 3975168 total steps have happened
[32m[20230113 20:26:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1941 --------------------------#
[32m[20230113 20:26:59 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:26:59 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:26:59 @agent_ppo2.py:186][0m |          -0.0030 |           6.0651 |          15.2812 |
[32m[20230113 20:26:59 @agent_ppo2.py:186][0m |          -0.0046 |           4.6483 |          15.2463 |
[32m[20230113 20:26:59 @agent_ppo2.py:186][0m |          -0.0085 |           4.1970 |          15.2488 |
[32m[20230113 20:26:59 @agent_ppo2.py:186][0m |          -0.0042 |           3.8938 |          15.2516 |
[32m[20230113 20:26:59 @agent_ppo2.py:186][0m |          -0.0127 |           3.7142 |          15.2377 |
[32m[20230113 20:26:59 @agent_ppo2.py:186][0m |           0.0013 |           3.6187 |          15.2346 |
[32m[20230113 20:26:59 @agent_ppo2.py:186][0m |          -0.0175 |           3.4193 |          15.2442 |
[32m[20230113 20:26:59 @agent_ppo2.py:186][0m |          -0.0147 |           3.3308 |          15.2411 |
[32m[20230113 20:26:59 @agent_ppo2.py:186][0m |          -0.0143 |           3.3127 |          15.2385 |
[32m[20230113 20:26:59 @agent_ppo2.py:186][0m |          -0.0193 |           3.2043 |          15.2377 |
[32m[20230113 20:26:59 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.39
[32m[20230113 20:27:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.99
[32m[20230113 20:27:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.79
[32m[20230113 20:27:00 @agent_ppo2.py:144][0m Total time:      42.45 min
[32m[20230113 20:27:00 @agent_ppo2.py:146][0m 3977216 total steps have happened
[32m[20230113 20:27:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1942 --------------------------#
[32m[20230113 20:27:00 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:00 @agent_ppo2.py:186][0m |           0.0005 |           7.3140 |          15.4017 |
[32m[20230113 20:27:00 @agent_ppo2.py:186][0m |          -0.0039 |           5.3762 |          15.4011 |
[32m[20230113 20:27:00 @agent_ppo2.py:186][0m |          -0.0073 |           4.7394 |          15.3918 |
[32m[20230113 20:27:00 @agent_ppo2.py:186][0m |          -0.0131 |           4.2222 |          15.3906 |
[32m[20230113 20:27:00 @agent_ppo2.py:186][0m |          -0.0067 |           3.9818 |          15.3784 |
[32m[20230113 20:27:00 @agent_ppo2.py:186][0m |          -0.0143 |           3.8039 |          15.3835 |
[32m[20230113 20:27:00 @agent_ppo2.py:186][0m |          -0.0163 |           3.6383 |          15.3819 |
[32m[20230113 20:27:00 @agent_ppo2.py:186][0m |          -0.0155 |           3.5195 |          15.3791 |
[32m[20230113 20:27:00 @agent_ppo2.py:186][0m |          -0.0159 |           3.4252 |          15.3859 |
[32m[20230113 20:27:01 @agent_ppo2.py:186][0m |          -0.0158 |           3.3714 |          15.3792 |
[32m[20230113 20:27:01 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:01 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.81
[32m[20230113 20:27:01 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.43
[32m[20230113 20:27:01 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.85
[32m[20230113 20:27:01 @agent_ppo2.py:144][0m Total time:      42.47 min
[32m[20230113 20:27:01 @agent_ppo2.py:146][0m 3979264 total steps have happened
[32m[20230113 20:27:01 @agent_ppo2.py:122][0m #------------------------ Iteration 1943 --------------------------#
[32m[20230113 20:27:01 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:27:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:01 @agent_ppo2.py:186][0m |          -0.0023 |           6.0416 |          15.2530 |
[32m[20230113 20:27:02 @agent_ppo2.py:186][0m |          -0.0086 |           4.1239 |          15.2423 |
[32m[20230113 20:27:02 @agent_ppo2.py:186][0m |          -0.0216 |           3.7244 |          15.2323 |
[32m[20230113 20:27:02 @agent_ppo2.py:186][0m |          -0.0127 |           3.4813 |          15.2325 |
[32m[20230113 20:27:02 @agent_ppo2.py:186][0m |          -0.0184 |           3.2615 |          15.2324 |
[32m[20230113 20:27:02 @agent_ppo2.py:186][0m |          -0.0044 |           3.1383 |          15.2266 |
[32m[20230113 20:27:02 @agent_ppo2.py:186][0m |          -0.0139 |           3.0073 |          15.2296 |
[32m[20230113 20:27:02 @agent_ppo2.py:186][0m |          -0.0124 |           2.9490 |          15.2012 |
[32m[20230113 20:27:02 @agent_ppo2.py:186][0m |          -0.0146 |           2.8750 |          15.2147 |
[32m[20230113 20:27:02 @agent_ppo2.py:186][0m |          -0.0223 |           2.7680 |          15.2002 |
[32m[20230113 20:27:02 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.75
[32m[20230113 20:27:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.89
[32m[20230113 20:27:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.93
[32m[20230113 20:27:02 @agent_ppo2.py:144][0m Total time:      42.49 min
[32m[20230113 20:27:02 @agent_ppo2.py:146][0m 3981312 total steps have happened
[32m[20230113 20:27:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1944 --------------------------#
[32m[20230113 20:27:03 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:27:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:03 @agent_ppo2.py:186][0m |           0.0027 |           5.7970 |          15.7482 |
[32m[20230113 20:27:03 @agent_ppo2.py:186][0m |          -0.0023 |           4.5505 |          15.7498 |
[32m[20230113 20:27:03 @agent_ppo2.py:186][0m |          -0.0040 |           4.1620 |          15.7430 |
[32m[20230113 20:27:03 @agent_ppo2.py:186][0m |          -0.0063 |           3.9586 |          15.7336 |
[32m[20230113 20:27:03 @agent_ppo2.py:186][0m |          -0.0062 |           3.7392 |          15.7280 |
[32m[20230113 20:27:03 @agent_ppo2.py:186][0m |          -0.0100 |           3.5930 |          15.7450 |
[32m[20230113 20:27:03 @agent_ppo2.py:186][0m |          -0.0082 |           3.4581 |          15.7209 |
[32m[20230113 20:27:03 @agent_ppo2.py:186][0m |          -0.0110 |           3.3336 |          15.7330 |
[32m[20230113 20:27:03 @agent_ppo2.py:186][0m |          -0.0131 |           3.2577 |          15.7240 |
[32m[20230113 20:27:03 @agent_ppo2.py:186][0m |          -0.0126 |           3.1806 |          15.7161 |
[32m[20230113 20:27:03 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.36
[32m[20230113 20:27:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.06
[32m[20230113 20:27:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.31
[32m[20230113 20:27:04 @agent_ppo2.py:144][0m Total time:      42.52 min
[32m[20230113 20:27:04 @agent_ppo2.py:146][0m 3983360 total steps have happened
[32m[20230113 20:27:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1945 --------------------------#
[32m[20230113 20:27:04 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:04 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:04 @agent_ppo2.py:186][0m |           0.0019 |           7.4303 |          15.4942 |
[32m[20230113 20:27:04 @agent_ppo2.py:186][0m |          -0.0055 |           5.5818 |          15.4845 |
[32m[20230113 20:27:04 @agent_ppo2.py:186][0m |          -0.0090 |           4.8615 |          15.4807 |
[32m[20230113 20:27:04 @agent_ppo2.py:186][0m |          -0.0095 |           4.4518 |          15.4595 |
[32m[20230113 20:27:04 @agent_ppo2.py:186][0m |          -0.0089 |           4.1689 |          15.4510 |
[32m[20230113 20:27:04 @agent_ppo2.py:186][0m |          -0.0138 |           3.9345 |          15.4774 |
[32m[20230113 20:27:04 @agent_ppo2.py:186][0m |          -0.0146 |           3.7778 |          15.4556 |
[32m[20230113 20:27:04 @agent_ppo2.py:186][0m |          -0.0123 |           3.6803 |          15.4626 |
[32m[20230113 20:27:04 @agent_ppo2.py:186][0m |          -0.0156 |           3.5650 |          15.4479 |
[32m[20230113 20:27:04 @agent_ppo2.py:186][0m |          -0.0173 |           3.4620 |          15.4500 |
[32m[20230113 20:27:04 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.89
[32m[20230113 20:27:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.03
[32m[20230113 20:27:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.98
[32m[20230113 20:27:05 @agent_ppo2.py:144][0m Total time:      42.54 min
[32m[20230113 20:27:05 @agent_ppo2.py:146][0m 3985408 total steps have happened
[32m[20230113 20:27:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1946 --------------------------#
[32m[20230113 20:27:05 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:05 @agent_ppo2.py:186][0m |          -0.0002 |           6.8392 |          15.5273 |
[32m[20230113 20:27:05 @agent_ppo2.py:186][0m |          -0.0045 |           5.0998 |          15.5134 |
[32m[20230113 20:27:05 @agent_ppo2.py:186][0m |          -0.0029 |           4.4090 |          15.5065 |
[32m[20230113 20:27:05 @agent_ppo2.py:186][0m |          -0.0076 |           4.0687 |          15.5015 |
[32m[20230113 20:27:05 @agent_ppo2.py:186][0m |          -0.0103 |           3.8327 |          15.4911 |
[32m[20230113 20:27:05 @agent_ppo2.py:186][0m |          -0.0186 |           3.6372 |          15.4984 |
[32m[20230113 20:27:06 @agent_ppo2.py:186][0m |          -0.0096 |           3.4825 |          15.5000 |
[32m[20230113 20:27:06 @agent_ppo2.py:186][0m |          -0.0070 |           3.4465 |          15.4922 |
[32m[20230113 20:27:06 @agent_ppo2.py:186][0m |          -0.0167 |           3.2567 |          15.4816 |
[32m[20230113 20:27:06 @agent_ppo2.py:186][0m |          -0.0174 |           3.1806 |          15.4861 |
[32m[20230113 20:27:06 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:06 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.88
[32m[20230113 20:27:06 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.68
[32m[20230113 20:27:06 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 253.33
[32m[20230113 20:27:06 @agent_ppo2.py:144][0m Total time:      42.56 min
[32m[20230113 20:27:06 @agent_ppo2.py:146][0m 3987456 total steps have happened
[32m[20230113 20:27:06 @agent_ppo2.py:122][0m #------------------------ Iteration 1947 --------------------------#
[32m[20230113 20:27:06 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:27:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:07 @agent_ppo2.py:186][0m |           0.0023 |           6.2749 |          15.6953 |
[32m[20230113 20:27:07 @agent_ppo2.py:186][0m |          -0.0044 |           4.7817 |          15.6913 |
[32m[20230113 20:27:07 @agent_ppo2.py:186][0m |          -0.0076 |           4.2707 |          15.6748 |
[32m[20230113 20:27:07 @agent_ppo2.py:186][0m |          -0.0112 |           3.9499 |          15.6678 |
[32m[20230113 20:27:07 @agent_ppo2.py:186][0m |          -0.0122 |           3.7140 |          15.6563 |
[32m[20230113 20:27:07 @agent_ppo2.py:186][0m |          -0.0129 |           3.5588 |          15.6584 |
[32m[20230113 20:27:07 @agent_ppo2.py:186][0m |          -0.0147 |           3.4355 |          15.6578 |
[32m[20230113 20:27:07 @agent_ppo2.py:186][0m |          -0.0156 |           3.2921 |          15.6554 |
[32m[20230113 20:27:07 @agent_ppo2.py:186][0m |          -0.0163 |           3.1969 |          15.6492 |
[32m[20230113 20:27:07 @agent_ppo2.py:186][0m |          -0.0161 |           3.1039 |          15.6567 |
[32m[20230113 20:27:07 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:27:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.34
[32m[20230113 20:27:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.11
[32m[20230113 20:27:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.26
[32m[20230113 20:27:07 @agent_ppo2.py:144][0m Total time:      42.58 min
[32m[20230113 20:27:07 @agent_ppo2.py:146][0m 3989504 total steps have happened
[32m[20230113 20:27:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1948 --------------------------#
[32m[20230113 20:27:08 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:08 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:08 @agent_ppo2.py:186][0m |           0.0089 |           6.9693 |          15.1801 |
[32m[20230113 20:27:08 @agent_ppo2.py:186][0m |          -0.0043 |           5.0849 |          15.1188 |
[32m[20230113 20:27:08 @agent_ppo2.py:186][0m |          -0.0119 |           4.4434 |          15.1428 |
[32m[20230113 20:27:08 @agent_ppo2.py:186][0m |          -0.0108 |           4.0362 |          15.1063 |
[32m[20230113 20:27:08 @agent_ppo2.py:186][0m |          -0.0134 |           3.8038 |          15.1343 |
[32m[20230113 20:27:08 @agent_ppo2.py:186][0m |          -0.0031 |           3.7121 |          15.1289 |
[32m[20230113 20:27:08 @agent_ppo2.py:186][0m |          -0.0049 |           3.5694 |          15.1422 |
[32m[20230113 20:27:08 @agent_ppo2.py:186][0m |          -0.0187 |           3.4144 |          15.1419 |
[32m[20230113 20:27:08 @agent_ppo2.py:186][0m |          -0.0133 |           3.3310 |          15.1273 |
[32m[20230113 20:27:08 @agent_ppo2.py:186][0m |          -0.0122 |           3.2133 |          15.1236 |
[32m[20230113 20:27:08 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.52
[32m[20230113 20:27:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 246.80
[32m[20230113 20:27:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.04
[32m[20230113 20:27:09 @agent_ppo2.py:144][0m Total time:      42.60 min
[32m[20230113 20:27:09 @agent_ppo2.py:146][0m 3991552 total steps have happened
[32m[20230113 20:27:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1949 --------------------------#
[32m[20230113 20:27:09 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:09 @agent_ppo2.py:186][0m |          -0.0057 |           5.8193 |          15.5838 |
[32m[20230113 20:27:09 @agent_ppo2.py:186][0m |          -0.0062 |           4.2426 |          15.5513 |
[32m[20230113 20:27:09 @agent_ppo2.py:186][0m |          -0.0073 |           3.7267 |          15.5772 |
[32m[20230113 20:27:09 @agent_ppo2.py:186][0m |          -0.0129 |           3.4658 |          15.5411 |
[32m[20230113 20:27:09 @agent_ppo2.py:186][0m |          -0.0105 |           3.3121 |          15.5561 |
[32m[20230113 20:27:09 @agent_ppo2.py:186][0m |          -0.0179 |           3.1777 |          15.5538 |
[32m[20230113 20:27:09 @agent_ppo2.py:186][0m |          -0.0155 |           3.0594 |          15.5532 |
[32m[20230113 20:27:09 @agent_ppo2.py:186][0m |          -0.0125 |           2.9793 |          15.5550 |
[32m[20230113 20:27:09 @agent_ppo2.py:186][0m |          -0.0149 |           2.9015 |          15.5430 |
[32m[20230113 20:27:09 @agent_ppo2.py:186][0m |          -0.0155 |           2.8541 |          15.5374 |
[32m[20230113 20:27:09 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:10 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.06
[32m[20230113 20:27:10 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.24
[32m[20230113 20:27:10 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.95
[32m[20230113 20:27:10 @agent_ppo2.py:144][0m Total time:      42.62 min
[32m[20230113 20:27:10 @agent_ppo2.py:146][0m 3993600 total steps have happened
[32m[20230113 20:27:10 @agent_ppo2.py:122][0m #------------------------ Iteration 1950 --------------------------#
[32m[20230113 20:27:10 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:10 @agent_ppo2.py:186][0m |           0.0023 |           6.3995 |          15.6797 |
[32m[20230113 20:27:10 @agent_ppo2.py:186][0m |          -0.0048 |           4.4016 |          15.6971 |
[32m[20230113 20:27:10 @agent_ppo2.py:186][0m |          -0.0096 |           3.9280 |          15.6865 |
[32m[20230113 20:27:10 @agent_ppo2.py:186][0m |          -0.0119 |           3.6546 |          15.6906 |
[32m[20230113 20:27:10 @agent_ppo2.py:186][0m |          -0.0133 |           3.4586 |          15.6843 |
[32m[20230113 20:27:10 @agent_ppo2.py:186][0m |          -0.0135 |           3.3148 |          15.6885 |
[32m[20230113 20:27:11 @agent_ppo2.py:186][0m |          -0.0154 |           3.1902 |          15.6918 |
[32m[20230113 20:27:11 @agent_ppo2.py:186][0m |          -0.0167 |           3.1187 |          15.6897 |
[32m[20230113 20:27:11 @agent_ppo2.py:186][0m |          -0.0167 |           3.0438 |          15.6935 |
[32m[20230113 20:27:11 @agent_ppo2.py:186][0m |          -0.0173 |           2.9944 |          15.6911 |
[32m[20230113 20:27:11 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.59
[32m[20230113 20:27:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 245.53
[32m[20230113 20:27:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.94
[32m[20230113 20:27:11 @agent_ppo2.py:144][0m Total time:      42.64 min
[32m[20230113 20:27:11 @agent_ppo2.py:146][0m 3995648 total steps have happened
[32m[20230113 20:27:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1951 --------------------------#
[32m[20230113 20:27:11 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:12 @agent_ppo2.py:186][0m |          -0.0012 |           5.9626 |          15.6079 |
[32m[20230113 20:27:12 @agent_ppo2.py:186][0m |          -0.0070 |           4.7863 |          15.6014 |
[32m[20230113 20:27:12 @agent_ppo2.py:186][0m |          -0.0119 |           4.3815 |          15.6062 |
[32m[20230113 20:27:12 @agent_ppo2.py:186][0m |          -0.0089 |           3.9807 |          15.6092 |
[32m[20230113 20:27:12 @agent_ppo2.py:186][0m |          -0.0112 |           3.8297 |          15.6076 |
[32m[20230113 20:27:12 @agent_ppo2.py:186][0m |          -0.0113 |           3.6854 |          15.5959 |
[32m[20230113 20:27:12 @agent_ppo2.py:186][0m |          -0.0129 |           3.5542 |          15.5993 |
[32m[20230113 20:27:12 @agent_ppo2.py:186][0m |          -0.0190 |           3.4570 |          15.5950 |
[32m[20230113 20:27:12 @agent_ppo2.py:186][0m |          -0.0124 |           3.3997 |          15.5883 |
[32m[20230113 20:27:12 @agent_ppo2.py:186][0m |          -0.0123 |           3.2912 |          15.5822 |
[32m[20230113 20:27:12 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:27:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 243.04
[32m[20230113 20:27:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.49
[32m[20230113 20:27:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.23
[32m[20230113 20:27:12 @agent_ppo2.py:144][0m Total time:      42.66 min
[32m[20230113 20:27:12 @agent_ppo2.py:146][0m 3997696 total steps have happened
[32m[20230113 20:27:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1952 --------------------------#
[32m[20230113 20:27:13 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:27:13 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:13 @agent_ppo2.py:186][0m |           0.0014 |           5.8369 |          15.4737 |
[32m[20230113 20:27:13 @agent_ppo2.py:186][0m |          -0.0051 |           4.2864 |          15.4596 |
[32m[20230113 20:27:13 @agent_ppo2.py:186][0m |          -0.0094 |           3.8548 |          15.4422 |
[32m[20230113 20:27:13 @agent_ppo2.py:186][0m |          -0.0114 |           3.5563 |          15.4459 |
[32m[20230113 20:27:13 @agent_ppo2.py:186][0m |          -0.0121 |           3.3863 |          15.4446 |
[32m[20230113 20:27:13 @agent_ppo2.py:186][0m |          -0.0128 |           3.2701 |          15.4350 |
[32m[20230113 20:27:13 @agent_ppo2.py:186][0m |          -0.0148 |           3.1586 |          15.4371 |
[32m[20230113 20:27:13 @agent_ppo2.py:186][0m |          -0.0154 |           3.0990 |          15.4400 |
[32m[20230113 20:27:13 @agent_ppo2.py:186][0m |          -0.0155 |           3.0089 |          15.4080 |
[32m[20230113 20:27:13 @agent_ppo2.py:186][0m |          -0.0182 |           2.9561 |          15.4248 |
[32m[20230113 20:27:13 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:14 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.05
[32m[20230113 20:27:14 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.45
[32m[20230113 20:27:14 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.60
[32m[20230113 20:27:14 @agent_ppo2.py:144][0m Total time:      42.68 min
[32m[20230113 20:27:14 @agent_ppo2.py:146][0m 3999744 total steps have happened
[32m[20230113 20:27:14 @agent_ppo2.py:122][0m #------------------------ Iteration 1953 --------------------------#
[32m[20230113 20:27:14 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:14 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:14 @agent_ppo2.py:186][0m |           0.0001 |           6.1417 |          15.4888 |
[32m[20230113 20:27:14 @agent_ppo2.py:186][0m |          -0.0036 |           4.7325 |          15.4593 |
[32m[20230113 20:27:14 @agent_ppo2.py:186][0m |          -0.0092 |           4.4313 |          15.4523 |
[32m[20230113 20:27:14 @agent_ppo2.py:186][0m |          -0.0076 |           4.1108 |          15.4415 |
[32m[20230113 20:27:14 @agent_ppo2.py:186][0m |          -0.0084 |           3.9649 |          15.4508 |
[32m[20230113 20:27:14 @agent_ppo2.py:186][0m |          -0.0133 |           3.8220 |          15.4337 |
[32m[20230113 20:27:14 @agent_ppo2.py:186][0m |          -0.0157 |           3.6826 |          15.4417 |
[32m[20230113 20:27:14 @agent_ppo2.py:186][0m |          -0.0128 |           3.6334 |          15.4347 |
[32m[20230113 20:27:14 @agent_ppo2.py:186][0m |          -0.0146 |           3.5666 |          15.4157 |
[32m[20230113 20:27:14 @agent_ppo2.py:186][0m |          -0.0177 |           3.4797 |          15.4317 |
[32m[20230113 20:27:14 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:15 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.54
[32m[20230113 20:27:15 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.47
[32m[20230113 20:27:15 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.29
[32m[20230113 20:27:15 @agent_ppo2.py:144][0m Total time:      42.70 min
[32m[20230113 20:27:15 @agent_ppo2.py:146][0m 4001792 total steps have happened
[32m[20230113 20:27:15 @agent_ppo2.py:122][0m #------------------------ Iteration 1954 --------------------------#
[32m[20230113 20:27:15 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:15 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:15 @agent_ppo2.py:186][0m |          -0.0007 |           6.3229 |          15.6305 |
[32m[20230113 20:27:15 @agent_ppo2.py:186][0m |          -0.0066 |           5.2261 |          15.5910 |
[32m[20230113 20:27:15 @agent_ppo2.py:186][0m |          -0.0069 |           4.8074 |          15.5795 |
[32m[20230113 20:27:15 @agent_ppo2.py:186][0m |          -0.0100 |           4.5601 |          15.5727 |
[32m[20230113 20:27:16 @agent_ppo2.py:186][0m |          -0.0119 |           4.4444 |          15.5898 |
[32m[20230113 20:27:16 @agent_ppo2.py:186][0m |          -0.0120 |           4.3265 |          15.5906 |
[32m[20230113 20:27:16 @agent_ppo2.py:186][0m |          -0.0135 |           4.1278 |          15.5846 |
[32m[20230113 20:27:16 @agent_ppo2.py:186][0m |          -0.0144 |           4.0776 |          15.5738 |
[32m[20230113 20:27:16 @agent_ppo2.py:186][0m |          -0.0151 |           3.9735 |          15.5795 |
[32m[20230113 20:27:16 @agent_ppo2.py:186][0m |          -0.0160 |           3.9139 |          15.5690 |
[32m[20230113 20:27:16 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:16 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.26
[32m[20230113 20:27:16 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.35
[32m[20230113 20:27:16 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.29
[32m[20230113 20:27:16 @agent_ppo2.py:144][0m Total time:      42.73 min
[32m[20230113 20:27:16 @agent_ppo2.py:146][0m 4003840 total steps have happened
[32m[20230113 20:27:16 @agent_ppo2.py:122][0m #------------------------ Iteration 1955 --------------------------#
[32m[20230113 20:27:17 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:27:17 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:17 @agent_ppo2.py:186][0m |          -0.0000 |           5.3976 |          15.6614 |
[32m[20230113 20:27:17 @agent_ppo2.py:186][0m |          -0.0058 |           4.4910 |          15.6469 |
[32m[20230113 20:27:17 @agent_ppo2.py:186][0m |          -0.0068 |           4.1926 |          15.6471 |
[32m[20230113 20:27:17 @agent_ppo2.py:186][0m |          -0.0083 |           3.9651 |          15.6314 |
[32m[20230113 20:27:17 @agent_ppo2.py:186][0m |          -0.0113 |           3.8063 |          15.6443 |
[32m[20230113 20:27:17 @agent_ppo2.py:186][0m |          -0.0104 |           3.6756 |          15.6204 |
[32m[20230113 20:27:17 @agent_ppo2.py:186][0m |          -0.0130 |           3.5844 |          15.6231 |
[32m[20230113 20:27:17 @agent_ppo2.py:186][0m |          -0.0126 |           3.4926 |          15.6178 |
[32m[20230113 20:27:17 @agent_ppo2.py:186][0m |          -0.0144 |           3.3949 |          15.6249 |
[32m[20230113 20:27:17 @agent_ppo2.py:186][0m |          -0.0161 |           3.3552 |          15.6235 |
[32m[20230113 20:27:17 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:27:17 @agent_ppo2.py:139][0m Average TRAINING episode reward: 233.34
[32m[20230113 20:27:17 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.08
[32m[20230113 20:27:17 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.56
[32m[20230113 20:27:17 @agent_ppo2.py:144][0m Total time:      42.75 min
[32m[20230113 20:27:17 @agent_ppo2.py:146][0m 4005888 total steps have happened
[32m[20230113 20:27:17 @agent_ppo2.py:122][0m #------------------------ Iteration 1956 --------------------------#
[32m[20230113 20:27:18 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:18 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:18 @agent_ppo2.py:186][0m |           0.0006 |           5.3102 |          15.7183 |
[32m[20230113 20:27:18 @agent_ppo2.py:186][0m |          -0.0044 |           4.4916 |          15.6799 |
[32m[20230113 20:27:18 @agent_ppo2.py:186][0m |          -0.0083 |           4.1540 |          15.6732 |
[32m[20230113 20:27:18 @agent_ppo2.py:186][0m |          -0.0096 |           4.0217 |          15.6683 |
[32m[20230113 20:27:18 @agent_ppo2.py:186][0m |          -0.0109 |           3.8593 |          15.6481 |
[32m[20230113 20:27:18 @agent_ppo2.py:186][0m |          -0.0126 |           3.7248 |          15.6694 |
[32m[20230113 20:27:18 @agent_ppo2.py:186][0m |          -0.0114 |           3.6460 |          15.6501 |
[32m[20230113 20:27:18 @agent_ppo2.py:186][0m |          -0.0135 |           3.6126 |          15.6559 |
[32m[20230113 20:27:18 @agent_ppo2.py:186][0m |          -0.0146 |           3.4824 |          15.6663 |
[32m[20230113 20:27:18 @agent_ppo2.py:186][0m |          -0.0148 |           3.4328 |          15.6561 |
[32m[20230113 20:27:18 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:27:19 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.87
[32m[20230113 20:27:19 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.61
[32m[20230113 20:27:19 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.19
[32m[20230113 20:27:19 @agent_ppo2.py:144][0m Total time:      42.77 min
[32m[20230113 20:27:19 @agent_ppo2.py:146][0m 4007936 total steps have happened
[32m[20230113 20:27:19 @agent_ppo2.py:122][0m #------------------------ Iteration 1957 --------------------------#
[32m[20230113 20:27:19 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:19 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:19 @agent_ppo2.py:186][0m |           0.0003 |           5.5849 |          15.2607 |
[32m[20230113 20:27:19 @agent_ppo2.py:186][0m |          -0.0076 |           4.2925 |          15.2410 |
[32m[20230113 20:27:19 @agent_ppo2.py:186][0m |          -0.0067 |           3.9215 |          15.2428 |
[32m[20230113 20:27:19 @agent_ppo2.py:186][0m |          -0.0112 |           3.6780 |          15.2304 |
[32m[20230113 20:27:19 @agent_ppo2.py:186][0m |          -0.0172 |           3.4786 |          15.2277 |
[32m[20230113 20:27:19 @agent_ppo2.py:186][0m |          -0.0357 |           3.3469 |          15.2290 |
[32m[20230113 20:27:19 @agent_ppo2.py:186][0m |          -0.0140 |           3.2576 |          15.2030 |
[32m[20230113 20:27:19 @agent_ppo2.py:186][0m |          -0.0167 |           3.1469 |          15.2067 |
[32m[20230113 20:27:19 @agent_ppo2.py:186][0m |          -0.0033 |           3.1546 |          15.2178 |
[32m[20230113 20:27:20 @agent_ppo2.py:186][0m |          -0.0183 |           2.9951 |          15.2032 |
[32m[20230113 20:27:20 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:20 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.30
[32m[20230113 20:27:20 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.03
[32m[20230113 20:27:20 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.82
[32m[20230113 20:27:20 @agent_ppo2.py:144][0m Total time:      42.79 min
[32m[20230113 20:27:20 @agent_ppo2.py:146][0m 4009984 total steps have happened
[32m[20230113 20:27:20 @agent_ppo2.py:122][0m #------------------------ Iteration 1958 --------------------------#
[32m[20230113 20:27:20 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:20 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:20 @agent_ppo2.py:186][0m |           0.0065 |           7.4696 |          15.3948 |
[32m[20230113 20:27:20 @agent_ppo2.py:186][0m |          -0.0050 |           5.4686 |          15.3814 |
[32m[20230113 20:27:21 @agent_ppo2.py:186][0m |          -0.0086 |           4.8587 |          15.3724 |
[32m[20230113 20:27:21 @agent_ppo2.py:186][0m |          -0.0065 |           4.6648 |          15.3554 |
[32m[20230113 20:27:21 @agent_ppo2.py:186][0m |          -0.0085 |           4.3054 |          15.3730 |
[32m[20230113 20:27:21 @agent_ppo2.py:186][0m |          -0.0143 |           4.1288 |          15.3722 |
[32m[20230113 20:27:21 @agent_ppo2.py:186][0m |          -0.0134 |           4.0660 |          15.3697 |
[32m[20230113 20:27:21 @agent_ppo2.py:186][0m |          -0.0081 |           3.9557 |          15.3739 |
[32m[20230113 20:27:21 @agent_ppo2.py:186][0m |          -0.0121 |           3.8064 |          15.3779 |
[32m[20230113 20:27:21 @agent_ppo2.py:186][0m |          -0.0160 |           3.7527 |          15.3733 |
[32m[20230113 20:27:21 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:21 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.55
[32m[20230113 20:27:21 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.19
[32m[20230113 20:27:21 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.32
[32m[20230113 20:27:21 @agent_ppo2.py:144][0m Total time:      42.81 min
[32m[20230113 20:27:21 @agent_ppo2.py:146][0m 4012032 total steps have happened
[32m[20230113 20:27:21 @agent_ppo2.py:122][0m #------------------------ Iteration 1959 --------------------------#
[32m[20230113 20:27:22 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:22 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:22 @agent_ppo2.py:186][0m |           0.0017 |           7.0788 |          15.7539 |
[32m[20230113 20:27:22 @agent_ppo2.py:186][0m |          -0.0056 |           5.4212 |          15.7278 |
[32m[20230113 20:27:22 @agent_ppo2.py:186][0m |          -0.0078 |           4.9854 |          15.7117 |
[32m[20230113 20:27:22 @agent_ppo2.py:186][0m |          -0.0126 |           4.5348 |          15.7059 |
[32m[20230113 20:27:22 @agent_ppo2.py:186][0m |          -0.0118 |           4.3044 |          15.6864 |
[32m[20230113 20:27:22 @agent_ppo2.py:186][0m |          -0.0159 |           4.1759 |          15.7089 |
[32m[20230113 20:27:22 @agent_ppo2.py:186][0m |          -0.0173 |           4.0089 |          15.6948 |
[32m[20230113 20:27:22 @agent_ppo2.py:186][0m |          -0.0150 |           3.8690 |          15.6981 |
[32m[20230113 20:27:22 @agent_ppo2.py:186][0m |          -0.0192 |           3.7741 |          15.6835 |
[32m[20230113 20:27:22 @agent_ppo2.py:186][0m |          -0.0183 |           3.6750 |          15.6709 |
[32m[20230113 20:27:22 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:27:22 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.18
[32m[20230113 20:27:22 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.45
[32m[20230113 20:27:22 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.36
[32m[20230113 20:27:22 @agent_ppo2.py:144][0m Total time:      42.83 min
[32m[20230113 20:27:22 @agent_ppo2.py:146][0m 4014080 total steps have happened
[32m[20230113 20:27:22 @agent_ppo2.py:122][0m #------------------------ Iteration 1960 --------------------------#
[32m[20230113 20:27:23 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:23 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:23 @agent_ppo2.py:186][0m |           0.0002 |           5.6919 |          15.7353 |
[32m[20230113 20:27:23 @agent_ppo2.py:186][0m |          -0.0032 |           4.7101 |          15.7197 |
[32m[20230113 20:27:23 @agent_ppo2.py:186][0m |          -0.0071 |           4.2977 |          15.7085 |
[32m[20230113 20:27:23 @agent_ppo2.py:186][0m |          -0.0065 |           4.0564 |          15.7309 |
[32m[20230113 20:27:23 @agent_ppo2.py:186][0m |          -0.0090 |           3.8665 |          15.6949 |
[32m[20230113 20:27:23 @agent_ppo2.py:186][0m |          -0.0089 |           3.7246 |          15.7073 |
[32m[20230113 20:27:23 @agent_ppo2.py:186][0m |          -0.0106 |           3.6268 |          15.6933 |
[32m[20230113 20:27:23 @agent_ppo2.py:186][0m |          -0.0129 |           3.5164 |          15.6866 |
[32m[20230113 20:27:23 @agent_ppo2.py:186][0m |          -0.0139 |           3.4323 |          15.6899 |
[32m[20230113 20:27:23 @agent_ppo2.py:186][0m |          -0.0131 |           3.3764 |          15.6766 |
[32m[20230113 20:27:23 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:24 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.11
[32m[20230113 20:27:24 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.33
[32m[20230113 20:27:24 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.42
[32m[20230113 20:27:24 @agent_ppo2.py:144][0m Total time:      42.85 min
[32m[20230113 20:27:24 @agent_ppo2.py:146][0m 4016128 total steps have happened
[32m[20230113 20:27:24 @agent_ppo2.py:122][0m #------------------------ Iteration 1961 --------------------------#
[32m[20230113 20:27:24 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:24 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:24 @agent_ppo2.py:186][0m |           0.0067 |           5.3900 |          15.8217 |
[32m[20230113 20:27:24 @agent_ppo2.py:186][0m |          -0.0072 |           4.2465 |          15.7709 |
[32m[20230113 20:27:24 @agent_ppo2.py:186][0m |          -0.0096 |           3.7776 |          15.7873 |
[32m[20230113 20:27:24 @agent_ppo2.py:186][0m |          -0.0131 |           3.5859 |          15.7685 |
[32m[20230113 20:27:24 @agent_ppo2.py:186][0m |          -0.0135 |           3.4539 |          15.7693 |
[32m[20230113 20:27:24 @agent_ppo2.py:186][0m |          -0.0170 |           3.3775 |          15.7584 |
[32m[20230113 20:27:24 @agent_ppo2.py:186][0m |          -0.0145 |           3.2373 |          15.7673 |
[32m[20230113 20:27:24 @agent_ppo2.py:186][0m |          -0.0145 |           3.1949 |          15.7699 |
[32m[20230113 20:27:25 @agent_ppo2.py:186][0m |          -0.0162 |           3.1244 |          15.7636 |
[32m[20230113 20:27:25 @agent_ppo2.py:186][0m |          -0.0195 |           3.1160 |          15.7598 |
[32m[20230113 20:27:25 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:25 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.33
[32m[20230113 20:27:25 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.94
[32m[20230113 20:27:25 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.10
[32m[20230113 20:27:25 @agent_ppo2.py:144][0m Total time:      42.87 min
[32m[20230113 20:27:25 @agent_ppo2.py:146][0m 4018176 total steps have happened
[32m[20230113 20:27:25 @agent_ppo2.py:122][0m #------------------------ Iteration 1962 --------------------------#
[32m[20230113 20:27:25 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:25 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:25 @agent_ppo2.py:186][0m |          -0.0016 |           6.6116 |          15.4652 |
[32m[20230113 20:27:26 @agent_ppo2.py:186][0m |          -0.0065 |           5.0385 |          15.4383 |
[32m[20230113 20:27:26 @agent_ppo2.py:186][0m |          -0.0057 |           4.4403 |          15.4384 |
[32m[20230113 20:27:26 @agent_ppo2.py:186][0m |          -0.0070 |           4.1955 |          15.4108 |
[32m[20230113 20:27:26 @agent_ppo2.py:186][0m |          -0.0105 |           3.9445 |          15.4115 |
[32m[20230113 20:27:26 @agent_ppo2.py:186][0m |          -0.0141 |           3.8415 |          15.3865 |
[32m[20230113 20:27:26 @agent_ppo2.py:186][0m |          -0.0149 |           3.6583 |          15.4016 |
[32m[20230113 20:27:26 @agent_ppo2.py:186][0m |          -0.0148 |           3.5908 |          15.3777 |
[32m[20230113 20:27:26 @agent_ppo2.py:186][0m |          -0.0149 |           3.5152 |          15.3888 |
[32m[20230113 20:27:26 @agent_ppo2.py:186][0m |          -0.0190 |           3.3750 |          15.3744 |
[32m[20230113 20:27:26 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:26 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.16
[32m[20230113 20:27:26 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.92
[32m[20230113 20:27:26 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.03
[32m[20230113 20:27:26 @agent_ppo2.py:144][0m Total time:      42.89 min
[32m[20230113 20:27:26 @agent_ppo2.py:146][0m 4020224 total steps have happened
[32m[20230113 20:27:26 @agent_ppo2.py:122][0m #------------------------ Iteration 1963 --------------------------#
[32m[20230113 20:27:27 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:27 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:27 @agent_ppo2.py:186][0m |          -0.0019 |           6.0362 |          15.5657 |
[32m[20230113 20:27:27 @agent_ppo2.py:186][0m |          -0.0071 |           4.6199 |          15.5553 |
[32m[20230113 20:27:27 @agent_ppo2.py:186][0m |          -0.0127 |           4.1064 |          15.5496 |
[32m[20230113 20:27:27 @agent_ppo2.py:186][0m |          -0.0132 |           3.8962 |          15.5494 |
[32m[20230113 20:27:27 @agent_ppo2.py:186][0m |          -0.0143 |           3.6746 |          15.5487 |
[32m[20230113 20:27:27 @agent_ppo2.py:186][0m |          -0.0109 |           3.5329 |          15.5388 |
[32m[20230113 20:27:27 @agent_ppo2.py:186][0m |          -0.0130 |           3.4364 |          15.5453 |
[32m[20230113 20:27:27 @agent_ppo2.py:186][0m |          -0.0167 |           3.3517 |          15.5545 |
[32m[20230113 20:27:27 @agent_ppo2.py:186][0m |          -0.0103 |           3.3351 |          15.5417 |
[32m[20230113 20:27:27 @agent_ppo2.py:186][0m |          -0.0136 |           3.1550 |          15.5536 |
[32m[20230113 20:27:27 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:28 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.31
[32m[20230113 20:27:28 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 246.44
[32m[20230113 20:27:28 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.24
[32m[20230113 20:27:28 @agent_ppo2.py:144][0m Total time:      42.92 min
[32m[20230113 20:27:28 @agent_ppo2.py:146][0m 4022272 total steps have happened
[32m[20230113 20:27:28 @agent_ppo2.py:122][0m #------------------------ Iteration 1964 --------------------------#
[32m[20230113 20:27:28 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:28 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:28 @agent_ppo2.py:186][0m |           0.0025 |           5.1607 |          15.7035 |
[32m[20230113 20:27:28 @agent_ppo2.py:186][0m |          -0.0090 |           4.2431 |          15.6856 |
[32m[20230113 20:27:28 @agent_ppo2.py:186][0m |          -0.0121 |           3.9115 |          15.6735 |
[32m[20230113 20:27:28 @agent_ppo2.py:186][0m |          -0.0138 |           3.6781 |          15.6783 |
[32m[20230113 20:27:28 @agent_ppo2.py:186][0m |          -0.0164 |           3.5332 |          15.6560 |
[32m[20230113 20:27:28 @agent_ppo2.py:186][0m |          -0.0163 |           3.4538 |          15.6620 |
[32m[20230113 20:27:28 @agent_ppo2.py:186][0m |          -0.0199 |           3.3336 |          15.6639 |
[32m[20230113 20:27:28 @agent_ppo2.py:186][0m |          -0.0189 |           3.2363 |          15.6603 |
[32m[20230113 20:27:28 @agent_ppo2.py:186][0m |          -0.0202 |           3.1736 |          15.6626 |
[32m[20230113 20:27:28 @agent_ppo2.py:186][0m |          -0.0199 |           3.1437 |          15.6379 |
[32m[20230113 20:27:28 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:29 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.17
[32m[20230113 20:27:29 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.20
[32m[20230113 20:27:29 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.89
[32m[20230113 20:27:29 @agent_ppo2.py:144][0m Total time:      42.94 min
[32m[20230113 20:27:29 @agent_ppo2.py:146][0m 4024320 total steps have happened
[32m[20230113 20:27:29 @agent_ppo2.py:122][0m #------------------------ Iteration 1965 --------------------------#
[32m[20230113 20:27:29 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:29 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:29 @agent_ppo2.py:186][0m |          -0.0003 |           5.1877 |          15.7476 |
[32m[20230113 20:27:29 @agent_ppo2.py:186][0m |           0.0061 |           3.9033 |          15.7395 |
[32m[20230113 20:27:29 @agent_ppo2.py:186][0m |          -0.0078 |           3.4554 |          15.7158 |
[32m[20230113 20:27:29 @agent_ppo2.py:186][0m |          -0.0117 |           3.1122 |          15.7185 |
[32m[20230113 20:27:29 @agent_ppo2.py:186][0m |          -0.0152 |           2.9523 |          15.7207 |
[32m[20230113 20:27:29 @agent_ppo2.py:186][0m |          -0.0129 |           2.8493 |          15.7094 |
[32m[20230113 20:27:30 @agent_ppo2.py:186][0m |          -0.0186 |           2.7639 |          15.6955 |
[32m[20230113 20:27:30 @agent_ppo2.py:186][0m |          -0.0169 |           2.6652 |          15.7013 |
[32m[20230113 20:27:30 @agent_ppo2.py:186][0m |          -0.0236 |           2.5939 |          15.6942 |
[32m[20230113 20:27:30 @agent_ppo2.py:186][0m |          -0.0226 |           2.5616 |          15.6886 |
[32m[20230113 20:27:30 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:30 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.18
[32m[20230113 20:27:30 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.20
[32m[20230113 20:27:30 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.63
[32m[20230113 20:27:30 @agent_ppo2.py:144][0m Total time:      42.96 min
[32m[20230113 20:27:30 @agent_ppo2.py:146][0m 4026368 total steps have happened
[32m[20230113 20:27:30 @agent_ppo2.py:122][0m #------------------------ Iteration 1966 --------------------------#
[32m[20230113 20:27:30 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:27:31 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:31 @agent_ppo2.py:186][0m |          -0.0023 |           5.2930 |          15.8736 |
[32m[20230113 20:27:31 @agent_ppo2.py:186][0m |          -0.0070 |           3.6910 |          15.8834 |
[32m[20230113 20:27:31 @agent_ppo2.py:186][0m |          -0.0088 |           3.3233 |          15.8606 |
[32m[20230113 20:27:31 @agent_ppo2.py:186][0m |          -0.0119 |           3.1319 |          15.8649 |
[32m[20230113 20:27:31 @agent_ppo2.py:186][0m |          -0.0118 |           2.9799 |          15.8679 |
[32m[20230113 20:27:31 @agent_ppo2.py:186][0m |          -0.0140 |           2.8676 |          15.8682 |
[32m[20230113 20:27:31 @agent_ppo2.py:186][0m |          -0.0143 |           2.7947 |          15.8535 |
[32m[20230113 20:27:31 @agent_ppo2.py:186][0m |          -0.0156 |           2.7130 |          15.8713 |
[32m[20230113 20:27:31 @agent_ppo2.py:186][0m |          -0.0152 |           2.6558 |          15.8667 |
[32m[20230113 20:27:31 @agent_ppo2.py:186][0m |          -0.0154 |           2.6151 |          15.8687 |
[32m[20230113 20:27:31 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:31 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.82
[32m[20230113 20:27:31 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.03
[32m[20230113 20:27:31 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.11
[32m[20230113 20:27:31 @agent_ppo2.py:144][0m Total time:      42.98 min
[32m[20230113 20:27:31 @agent_ppo2.py:146][0m 4028416 total steps have happened
[32m[20230113 20:27:31 @agent_ppo2.py:122][0m #------------------------ Iteration 1967 --------------------------#
[32m[20230113 20:27:32 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:27:32 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:32 @agent_ppo2.py:186][0m |          -0.0001 |           5.6817 |          15.5634 |
[32m[20230113 20:27:32 @agent_ppo2.py:186][0m |          -0.0059 |           4.3436 |          15.5562 |
[32m[20230113 20:27:32 @agent_ppo2.py:186][0m |          -0.0102 |           3.8928 |          15.5241 |
[32m[20230113 20:27:32 @agent_ppo2.py:186][0m |          -0.0126 |           3.6883 |          15.5231 |
[32m[20230113 20:27:32 @agent_ppo2.py:186][0m |          -0.0121 |           3.5545 |          15.5255 |
[32m[20230113 20:27:32 @agent_ppo2.py:186][0m |          -0.0146 |           3.4242 |          15.5209 |
[32m[20230113 20:27:32 @agent_ppo2.py:186][0m |          -0.0123 |           3.3810 |          15.5016 |
[32m[20230113 20:27:32 @agent_ppo2.py:186][0m |          -0.0148 |           3.2821 |          15.5141 |
[32m[20230113 20:27:32 @agent_ppo2.py:186][0m |          -0.0152 |           3.2061 |          15.4973 |
[32m[20230113 20:27:32 @agent_ppo2.py:186][0m |          -0.0152 |           3.1686 |          15.5049 |
[32m[20230113 20:27:32 @agent_ppo2.py:131][0m Policy update time: 0.45 s
[32m[20230113 20:27:33 @agent_ppo2.py:139][0m Average TRAINING episode reward: 234.70
[32m[20230113 20:27:33 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 236.55
[32m[20230113 20:27:33 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.40
[32m[20230113 20:27:33 @agent_ppo2.py:144][0m Total time:      43.00 min
[32m[20230113 20:27:33 @agent_ppo2.py:146][0m 4030464 total steps have happened
[32m[20230113 20:27:33 @agent_ppo2.py:122][0m #------------------------ Iteration 1968 --------------------------#
[32m[20230113 20:27:33 @agent_ppo2.py:128][0m Sampling time: 0.45 s by 1 slaves
[32m[20230113 20:27:33 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:33 @agent_ppo2.py:186][0m |          -0.0002 |           5.6722 |          15.4887 |
[32m[20230113 20:27:33 @agent_ppo2.py:186][0m |          -0.0003 |           4.4396 |          15.5016 |
[32m[20230113 20:27:33 @agent_ppo2.py:186][0m |           0.0042 |           4.0845 |          15.4911 |
[32m[20230113 20:27:33 @agent_ppo2.py:186][0m |          -0.0015 |           3.8666 |          15.4797 |
[32m[20230113 20:27:33 @agent_ppo2.py:186][0m |          -0.0093 |           3.6481 |          15.4687 |
[32m[20230113 20:27:33 @agent_ppo2.py:186][0m |          -0.0049 |           3.6057 |          15.4703 |
[32m[20230113 20:27:33 @agent_ppo2.py:186][0m |          -0.0172 |           3.4323 |          15.4432 |
[32m[20230113 20:27:33 @agent_ppo2.py:186][0m |          -0.0161 |           3.3612 |          15.4586 |
[32m[20230113 20:27:33 @agent_ppo2.py:186][0m |          -0.0167 |           3.3407 |          15.4673 |
[32m[20230113 20:27:34 @agent_ppo2.py:186][0m |          -0.0135 |           3.2364 |          15.4635 |
[32m[20230113 20:27:34 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:34 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.61
[32m[20230113 20:27:34 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.53
[32m[20230113 20:27:34 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.47
[32m[20230113 20:27:34 @agent_ppo2.py:144][0m Total time:      43.02 min
[32m[20230113 20:27:34 @agent_ppo2.py:146][0m 4032512 total steps have happened
[32m[20230113 20:27:34 @agent_ppo2.py:122][0m #------------------------ Iteration 1969 --------------------------#
[32m[20230113 20:27:34 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:34 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:34 @agent_ppo2.py:186][0m |           0.0015 |           5.3950 |          15.6150 |
[32m[20230113 20:27:34 @agent_ppo2.py:186][0m |          -0.0038 |           4.2071 |          15.5639 |
[32m[20230113 20:27:35 @agent_ppo2.py:186][0m |          -0.0082 |           3.8472 |          15.5778 |
[32m[20230113 20:27:35 @agent_ppo2.py:186][0m |          -0.0116 |           3.6684 |          15.5640 |
[32m[20230113 20:27:35 @agent_ppo2.py:186][0m |          -0.0123 |           3.5353 |          15.5630 |
[32m[20230113 20:27:35 @agent_ppo2.py:186][0m |          -0.0131 |           3.3547 |          15.5614 |
[32m[20230113 20:27:35 @agent_ppo2.py:186][0m |          -0.0146 |           3.2682 |          15.5521 |
[32m[20230113 20:27:35 @agent_ppo2.py:186][0m |          -0.0165 |           3.1737 |          15.5578 |
[32m[20230113 20:27:35 @agent_ppo2.py:186][0m |          -0.0140 |           3.1060 |          15.5576 |
[32m[20230113 20:27:35 @agent_ppo2.py:186][0m |          -0.0154 |           3.0296 |          15.5581 |
[32m[20230113 20:27:35 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:35 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.60
[32m[20230113 20:27:35 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.68
[32m[20230113 20:27:35 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.95
[32m[20230113 20:27:35 @agent_ppo2.py:144][0m Total time:      43.04 min
[32m[20230113 20:27:35 @agent_ppo2.py:146][0m 4034560 total steps have happened
[32m[20230113 20:27:35 @agent_ppo2.py:122][0m #------------------------ Iteration 1970 --------------------------#
[32m[20230113 20:27:36 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:36 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:36 @agent_ppo2.py:186][0m |           0.0001 |           5.0476 |          15.7993 |
[32m[20230113 20:27:36 @agent_ppo2.py:186][0m |          -0.0068 |           3.9004 |          15.7771 |
[32m[20230113 20:27:36 @agent_ppo2.py:186][0m |          -0.0115 |           3.5398 |          15.7618 |
[32m[20230113 20:27:36 @agent_ppo2.py:186][0m |          -0.0127 |           3.4094 |          15.7805 |
[32m[20230113 20:27:36 @agent_ppo2.py:186][0m |          -0.0123 |           3.2516 |          15.7754 |
[32m[20230113 20:27:36 @agent_ppo2.py:186][0m |          -0.0143 |           3.1295 |          15.7743 |
[32m[20230113 20:27:36 @agent_ppo2.py:186][0m |          -0.0159 |           3.0298 |          15.7769 |
[32m[20230113 20:27:36 @agent_ppo2.py:186][0m |          -0.0169 |           2.9549 |          15.7730 |
[32m[20230113 20:27:36 @agent_ppo2.py:186][0m |          -0.0175 |           2.8921 |          15.7669 |
[32m[20230113 20:27:36 @agent_ppo2.py:186][0m |          -0.0180 |           2.8047 |          15.7675 |
[32m[20230113 20:27:36 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:36 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.90
[32m[20230113 20:27:36 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.18
[32m[20230113 20:27:36 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 251.08
[32m[20230113 20:27:36 @agent_ppo2.py:144][0m Total time:      43.06 min
[32m[20230113 20:27:36 @agent_ppo2.py:146][0m 4036608 total steps have happened
[32m[20230113 20:27:36 @agent_ppo2.py:122][0m #------------------------ Iteration 1971 --------------------------#
[32m[20230113 20:27:37 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:37 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:37 @agent_ppo2.py:186][0m |           0.0017 |           6.1200 |          15.7977 |
[32m[20230113 20:27:37 @agent_ppo2.py:186][0m |          -0.0028 |           4.6405 |          15.7863 |
[32m[20230113 20:27:37 @agent_ppo2.py:186][0m |          -0.0055 |           4.2776 |          15.7799 |
[32m[20230113 20:27:37 @agent_ppo2.py:186][0m |          -0.0073 |           4.0556 |          15.7708 |
[32m[20230113 20:27:37 @agent_ppo2.py:186][0m |          -0.0081 |           3.7507 |          15.7769 |
[32m[20230113 20:27:37 @agent_ppo2.py:186][0m |          -0.0103 |           3.5694 |          15.7755 |
[32m[20230113 20:27:37 @agent_ppo2.py:186][0m |          -0.0121 |           3.4849 |          15.7549 |
[32m[20230113 20:27:37 @agent_ppo2.py:186][0m |          -0.0117 |           3.3727 |          15.7688 |
[32m[20230113 20:27:37 @agent_ppo2.py:186][0m |          -0.0140 |           3.2289 |          15.7739 |
[32m[20230113 20:27:37 @agent_ppo2.py:186][0m |          -0.0094 |           3.2348 |          15.7624 |
[32m[20230113 20:27:37 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:38 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.69
[32m[20230113 20:27:38 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 246.50
[32m[20230113 20:27:38 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.44
[32m[20230113 20:27:38 @agent_ppo2.py:144][0m Total time:      43.09 min
[32m[20230113 20:27:38 @agent_ppo2.py:146][0m 4038656 total steps have happened
[32m[20230113 20:27:38 @agent_ppo2.py:122][0m #------------------------ Iteration 1972 --------------------------#
[32m[20230113 20:27:38 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:38 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:38 @agent_ppo2.py:186][0m |           0.0011 |           4.8959 |          15.7840 |
[32m[20230113 20:27:38 @agent_ppo2.py:186][0m |          -0.0046 |           3.9696 |          15.7262 |
[32m[20230113 20:27:38 @agent_ppo2.py:186][0m |          -0.0077 |           3.6169 |          15.7223 |
[32m[20230113 20:27:38 @agent_ppo2.py:186][0m |          -0.0105 |           3.4399 |          15.7260 |
[32m[20230113 20:27:38 @agent_ppo2.py:186][0m |          -0.0103 |           3.2912 |          15.7187 |
[32m[20230113 20:27:38 @agent_ppo2.py:186][0m |          -0.0113 |           3.1743 |          15.7116 |
[32m[20230113 20:27:38 @agent_ppo2.py:186][0m |          -0.0135 |           3.0989 |          15.6978 |
[32m[20230113 20:27:38 @agent_ppo2.py:186][0m |          -0.0149 |           2.9764 |          15.7112 |
[32m[20230113 20:27:39 @agent_ppo2.py:186][0m |          -0.0160 |           2.9240 |          15.7168 |
[32m[20230113 20:27:39 @agent_ppo2.py:186][0m |          -0.0147 |           2.8705 |          15.7150 |
[32m[20230113 20:27:39 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:39 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.06
[32m[20230113 20:27:39 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.03
[32m[20230113 20:27:39 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.62
[32m[20230113 20:27:39 @agent_ppo2.py:144][0m Total time:      43.11 min
[32m[20230113 20:27:39 @agent_ppo2.py:146][0m 4040704 total steps have happened
[32m[20230113 20:27:39 @agent_ppo2.py:122][0m #------------------------ Iteration 1973 --------------------------#
[32m[20230113 20:27:39 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:39 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:39 @agent_ppo2.py:186][0m |           0.0027 |           5.4687 |          15.2850 |
[32m[20230113 20:27:39 @agent_ppo2.py:186][0m |          -0.0017 |           4.2089 |          15.2800 |
[32m[20230113 20:27:40 @agent_ppo2.py:186][0m |          -0.0129 |           3.8664 |          15.2655 |
[32m[20230113 20:27:40 @agent_ppo2.py:186][0m |          -0.0014 |           3.6581 |          15.2549 |
[32m[20230113 20:27:40 @agent_ppo2.py:186][0m |          -0.0082 |           3.5061 |          15.2765 |
[32m[20230113 20:27:40 @agent_ppo2.py:186][0m |          -0.0090 |           3.3943 |          15.2601 |
[32m[20230113 20:27:40 @agent_ppo2.py:186][0m |          -0.0136 |           3.2163 |          15.2483 |
[32m[20230113 20:27:40 @agent_ppo2.py:186][0m |           0.0023 |           3.1720 |          15.2512 |
[32m[20230113 20:27:40 @agent_ppo2.py:186][0m |          -0.0105 |           3.1054 |          15.2501 |
[32m[20230113 20:27:40 @agent_ppo2.py:186][0m |          -0.0117 |           3.0582 |          15.2538 |
[32m[20230113 20:27:40 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:40 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.56
[32m[20230113 20:27:40 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 245.42
[32m[20230113 20:27:40 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.66
[32m[20230113 20:27:40 @agent_ppo2.py:144][0m Total time:      43.13 min
[32m[20230113 20:27:40 @agent_ppo2.py:146][0m 4042752 total steps have happened
[32m[20230113 20:27:40 @agent_ppo2.py:122][0m #------------------------ Iteration 1974 --------------------------#
[32m[20230113 20:27:41 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:41 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:41 @agent_ppo2.py:186][0m |           0.0019 |           5.6813 |          15.6162 |
[32m[20230113 20:27:41 @agent_ppo2.py:186][0m |          -0.0049 |           4.3959 |          15.5930 |
[32m[20230113 20:27:41 @agent_ppo2.py:186][0m |          -0.0089 |           3.9700 |          15.5843 |
[32m[20230113 20:27:41 @agent_ppo2.py:186][0m |          -0.0092 |           3.7209 |          15.5796 |
[32m[20230113 20:27:41 @agent_ppo2.py:186][0m |          -0.0112 |           3.5733 |          15.5825 |
[32m[20230113 20:27:41 @agent_ppo2.py:186][0m |          -0.0105 |           3.4536 |          15.5753 |
[32m[20230113 20:27:41 @agent_ppo2.py:186][0m |          -0.0112 |           3.3386 |          15.5865 |
[32m[20230113 20:27:41 @agent_ppo2.py:186][0m |          -0.0138 |           3.2573 |          15.5825 |
[32m[20230113 20:27:41 @agent_ppo2.py:186][0m |          -0.0144 |           3.1560 |          15.5854 |
[32m[20230113 20:27:41 @agent_ppo2.py:186][0m |          -0.0150 |           3.1395 |          15.5753 |
[32m[20230113 20:27:41 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:41 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.94
[32m[20230113 20:27:41 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.02
[32m[20230113 20:27:41 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.43
[32m[20230113 20:27:41 @agent_ppo2.py:144][0m Total time:      43.15 min
[32m[20230113 20:27:41 @agent_ppo2.py:146][0m 4044800 total steps have happened
[32m[20230113 20:27:41 @agent_ppo2.py:122][0m #------------------------ Iteration 1975 --------------------------#
[32m[20230113 20:27:42 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:42 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:42 @agent_ppo2.py:186][0m |           0.0009 |           5.8656 |          15.8101 |
[32m[20230113 20:27:42 @agent_ppo2.py:186][0m |          -0.0059 |           4.3470 |          15.7928 |
[32m[20230113 20:27:42 @agent_ppo2.py:186][0m |          -0.0073 |           3.8797 |          15.7746 |
[32m[20230113 20:27:42 @agent_ppo2.py:186][0m |          -0.0103 |           3.6806 |          15.7737 |
[32m[20230113 20:27:42 @agent_ppo2.py:186][0m |          -0.0115 |           3.4830 |          15.7661 |
[32m[20230113 20:27:42 @agent_ppo2.py:186][0m |          -0.0127 |           3.3340 |          15.7724 |
[32m[20230113 20:27:42 @agent_ppo2.py:186][0m |          -0.0125 |           3.2284 |          15.7697 |
[32m[20230113 20:27:42 @agent_ppo2.py:186][0m |          -0.0123 |           3.1589 |          15.7706 |
[32m[20230113 20:27:42 @agent_ppo2.py:186][0m |          -0.0163 |           3.0473 |          15.7646 |
[32m[20230113 20:27:42 @agent_ppo2.py:186][0m |          -0.0154 |           2.9783 |          15.7659 |
[32m[20230113 20:27:42 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:43 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.48
[32m[20230113 20:27:43 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.37
[32m[20230113 20:27:43 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 253.59
[32m[20230113 20:27:43 @agent_ppo2.py:144][0m Total time:      43.17 min
[32m[20230113 20:27:43 @agent_ppo2.py:146][0m 4046848 total steps have happened
[32m[20230113 20:27:43 @agent_ppo2.py:122][0m #------------------------ Iteration 1976 --------------------------#
[32m[20230113 20:27:43 @agent_ppo2.py:128][0m Sampling time: 0.46 s by 1 slaves
[32m[20230113 20:27:43 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:43 @agent_ppo2.py:186][0m |          -0.0027 |           6.6362 |          15.7885 |
[32m[20230113 20:27:43 @agent_ppo2.py:186][0m |          -0.0062 |           4.5428 |          15.7634 |
[32m[20230113 20:27:43 @agent_ppo2.py:186][0m |          -0.0116 |           3.9341 |          15.7558 |
[32m[20230113 20:27:43 @agent_ppo2.py:186][0m |          -0.0157 |           3.6354 |          15.7391 |
[32m[20230113 20:27:43 @agent_ppo2.py:186][0m |          -0.0153 |           3.4222 |          15.7529 |
[32m[20230113 20:27:43 @agent_ppo2.py:186][0m |          -0.0168 |           3.2554 |          15.7452 |
[32m[20230113 20:27:44 @agent_ppo2.py:186][0m |          -0.0178 |           3.1103 |          15.7447 |
[32m[20230113 20:27:44 @agent_ppo2.py:186][0m |          -0.0181 |           3.0068 |          15.7378 |
[32m[20230113 20:27:44 @agent_ppo2.py:186][0m |          -0.0196 |           2.9032 |          15.7465 |
[32m[20230113 20:27:44 @agent_ppo2.py:186][0m |          -0.0195 |           2.8343 |          15.7528 |
[32m[20230113 20:27:44 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:44 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.28
[32m[20230113 20:27:44 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.53
[32m[20230113 20:27:44 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.91
[32m[20230113 20:27:44 @agent_ppo2.py:144][0m Total time:      43.19 min
[32m[20230113 20:27:44 @agent_ppo2.py:146][0m 4048896 total steps have happened
[32m[20230113 20:27:44 @agent_ppo2.py:122][0m #------------------------ Iteration 1977 --------------------------#
[32m[20230113 20:27:44 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:44 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:45 @agent_ppo2.py:186][0m |           0.0087 |           5.9501 |          15.7132 |
[32m[20230113 20:27:45 @agent_ppo2.py:186][0m |          -0.0013 |           4.2985 |          15.6774 |
[32m[20230113 20:27:45 @agent_ppo2.py:186][0m |          -0.0112 |           3.8691 |          15.6743 |
[32m[20230113 20:27:45 @agent_ppo2.py:186][0m |          -0.0055 |           3.5295 |          15.6833 |
[32m[20230113 20:27:45 @agent_ppo2.py:186][0m |          -0.0064 |           3.3443 |          15.6830 |
[32m[20230113 20:27:45 @agent_ppo2.py:186][0m |          -0.0096 |           3.2028 |          15.6696 |
[32m[20230113 20:27:45 @agent_ppo2.py:186][0m |          -0.0136 |           3.1671 |          15.6721 |
[32m[20230113 20:27:45 @agent_ppo2.py:186][0m |          -0.0228 |           3.0292 |          15.6771 |
[32m[20230113 20:27:45 @agent_ppo2.py:186][0m |          -0.0087 |           2.9116 |          15.6843 |
[32m[20230113 20:27:45 @agent_ppo2.py:186][0m |          -0.0160 |           2.8223 |          15.6667 |
[32m[20230113 20:27:45 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:45 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.42
[32m[20230113 20:27:45 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.19
[32m[20230113 20:27:45 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 255.92
[32m[20230113 20:27:45 @agent_ppo2.py:144][0m Total time:      43.21 min
[32m[20230113 20:27:45 @agent_ppo2.py:146][0m 4050944 total steps have happened
[32m[20230113 20:27:45 @agent_ppo2.py:122][0m #------------------------ Iteration 1978 --------------------------#
[32m[20230113 20:27:46 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:46 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:46 @agent_ppo2.py:186][0m |           0.0008 |           6.7057 |          15.6871 |
[32m[20230113 20:27:46 @agent_ppo2.py:186][0m |          -0.0043 |           5.1599 |          15.6764 |
[32m[20230113 20:27:46 @agent_ppo2.py:186][0m |          -0.0059 |           4.4693 |          15.6794 |
[32m[20230113 20:27:46 @agent_ppo2.py:186][0m |          -0.0077 |           4.1835 |          15.6689 |
[32m[20230113 20:27:46 @agent_ppo2.py:186][0m |          -0.0098 |           3.9316 |          15.6641 |
[32m[20230113 20:27:46 @agent_ppo2.py:186][0m |          -0.0114 |           3.7619 |          15.6780 |
[32m[20230113 20:27:46 @agent_ppo2.py:186][0m |          -0.0132 |           3.5952 |          15.6838 |
[32m[20230113 20:27:46 @agent_ppo2.py:186][0m |          -0.0127 |           3.4555 |          15.6864 |
[32m[20230113 20:27:46 @agent_ppo2.py:186][0m |          -0.0135 |           3.3986 |          15.6726 |
[32m[20230113 20:27:46 @agent_ppo2.py:186][0m |          -0.0146 |           3.2891 |          15.6712 |
[32m[20230113 20:27:46 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:47 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.96
[32m[20230113 20:27:47 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.50
[32m[20230113 20:27:47 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.99
[32m[20230113 20:27:47 @agent_ppo2.py:144][0m Total time:      43.23 min
[32m[20230113 20:27:47 @agent_ppo2.py:146][0m 4052992 total steps have happened
[32m[20230113 20:27:47 @agent_ppo2.py:122][0m #------------------------ Iteration 1979 --------------------------#
[32m[20230113 20:27:47 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:47 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:47 @agent_ppo2.py:186][0m |          -0.0006 |           6.1661 |          15.2762 |
[32m[20230113 20:27:47 @agent_ppo2.py:186][0m |          -0.0047 |           4.6854 |          15.2714 |
[32m[20230113 20:27:47 @agent_ppo2.py:186][0m |          -0.0047 |           4.1562 |          15.2493 |
[32m[20230113 20:27:47 @agent_ppo2.py:186][0m |          -0.0097 |           3.9073 |          15.2458 |
[32m[20230113 20:27:47 @agent_ppo2.py:186][0m |          -0.0087 |           3.6939 |          15.2509 |
[32m[20230113 20:27:47 @agent_ppo2.py:186][0m |          -0.0083 |           3.5491 |          15.2504 |
[32m[20230113 20:27:47 @agent_ppo2.py:186][0m |          -0.0125 |           3.4418 |          15.2360 |
[32m[20230113 20:27:47 @agent_ppo2.py:186][0m |          -0.0150 |           3.3116 |          15.2498 |
[32m[20230113 20:27:47 @agent_ppo2.py:186][0m |          -0.0130 |           3.2322 |          15.2480 |
[32m[20230113 20:27:47 @agent_ppo2.py:186][0m |          -0.0159 |           3.1616 |          15.2318 |
[32m[20230113 20:27:47 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:48 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.83
[32m[20230113 20:27:48 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.36
[32m[20230113 20:27:48 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.59
[32m[20230113 20:27:48 @agent_ppo2.py:144][0m Total time:      43.25 min
[32m[20230113 20:27:48 @agent_ppo2.py:146][0m 4055040 total steps have happened
[32m[20230113 20:27:48 @agent_ppo2.py:122][0m #------------------------ Iteration 1980 --------------------------#
[32m[20230113 20:27:48 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:48 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:48 @agent_ppo2.py:186][0m |           0.0003 |           5.2180 |          15.6000 |
[32m[20230113 20:27:48 @agent_ppo2.py:186][0m |          -0.0109 |           4.0213 |          15.5841 |
[32m[20230113 20:27:48 @agent_ppo2.py:186][0m |          -0.0200 |           3.7227 |          15.5575 |
[32m[20230113 20:27:48 @agent_ppo2.py:186][0m |          -0.0089 |           3.4599 |          15.5735 |
[32m[20230113 20:27:48 @agent_ppo2.py:186][0m |          -0.0156 |           3.2938 |          15.5844 |
[32m[20230113 20:27:49 @agent_ppo2.py:186][0m |          -0.0183 |           3.1899 |          15.5608 |
[32m[20230113 20:27:49 @agent_ppo2.py:186][0m |          -0.0075 |           3.0347 |          15.5607 |
[32m[20230113 20:27:49 @agent_ppo2.py:186][0m |           0.0083 |           3.0157 |          15.5520 |
[32m[20230113 20:27:49 @agent_ppo2.py:186][0m |          -0.0153 |           2.9073 |          15.5460 |
[32m[20230113 20:27:49 @agent_ppo2.py:186][0m |          -0.0030 |           2.9000 |          15.5351 |
[32m[20230113 20:27:49 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:49 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.66
[32m[20230113 20:27:49 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.44
[32m[20230113 20:27:49 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.36
[32m[20230113 20:27:49 @agent_ppo2.py:144][0m Total time:      43.28 min
[32m[20230113 20:27:49 @agent_ppo2.py:146][0m 4057088 total steps have happened
[32m[20230113 20:27:49 @agent_ppo2.py:122][0m #------------------------ Iteration 1981 --------------------------#
[32m[20230113 20:27:50 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:50 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:50 @agent_ppo2.py:186][0m |          -0.0007 |           6.1161 |          16.0057 |
[32m[20230113 20:27:50 @agent_ppo2.py:186][0m |          -0.0067 |           4.8355 |          15.9990 |
[32m[20230113 20:27:50 @agent_ppo2.py:186][0m |          -0.0100 |           4.5086 |          15.9989 |
[32m[20230113 20:27:50 @agent_ppo2.py:186][0m |          -0.0101 |           4.2207 |          16.0030 |
[32m[20230113 20:27:50 @agent_ppo2.py:186][0m |          -0.0130 |           3.9852 |          15.9973 |
[32m[20230113 20:27:50 @agent_ppo2.py:186][0m |          -0.0152 |           3.8128 |          15.9909 |
[32m[20230113 20:27:50 @agent_ppo2.py:186][0m |          -0.0159 |           3.5816 |          16.0046 |
[32m[20230113 20:27:50 @agent_ppo2.py:186][0m |          -0.0167 |           3.4888 |          16.0016 |
[32m[20230113 20:27:50 @agent_ppo2.py:186][0m |          -0.0192 |           3.4294 |          15.9823 |
[32m[20230113 20:27:50 @agent_ppo2.py:186][0m |          -0.0171 |           3.3243 |          15.9875 |
[32m[20230113 20:27:50 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:50 @agent_ppo2.py:139][0m Average TRAINING episode reward: 235.91
[32m[20230113 20:27:50 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 237.56
[32m[20230113 20:27:50 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.06
[32m[20230113 20:27:50 @agent_ppo2.py:144][0m Total time:      43.30 min
[32m[20230113 20:27:50 @agent_ppo2.py:146][0m 4059136 total steps have happened
[32m[20230113 20:27:50 @agent_ppo2.py:122][0m #------------------------ Iteration 1982 --------------------------#
[32m[20230113 20:27:51 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:51 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:51 @agent_ppo2.py:186][0m |          -0.0005 |           6.5136 |          15.4131 |
[32m[20230113 20:27:51 @agent_ppo2.py:186][0m |          -0.0051 |           5.3763 |          15.3949 |
[32m[20230113 20:27:51 @agent_ppo2.py:186][0m |          -0.0093 |           4.9386 |          15.4021 |
[32m[20230113 20:27:51 @agent_ppo2.py:186][0m |          -0.0104 |           4.6386 |          15.3800 |
[32m[20230113 20:27:51 @agent_ppo2.py:186][0m |          -0.0115 |           4.4932 |          15.3995 |
[32m[20230113 20:27:51 @agent_ppo2.py:186][0m |          -0.0154 |           4.2295 |          15.3934 |
[32m[20230113 20:27:51 @agent_ppo2.py:186][0m |          -0.0136 |           4.1267 |          15.3924 |
[32m[20230113 20:27:51 @agent_ppo2.py:186][0m |          -0.0164 |           4.0515 |          15.3815 |
[32m[20230113 20:27:51 @agent_ppo2.py:186][0m |          -0.0178 |           3.8735 |          15.3766 |
[32m[20230113 20:27:51 @agent_ppo2.py:186][0m |          -0.0183 |           3.7574 |          15.3794 |
[32m[20230113 20:27:51 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:52 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.94
[32m[20230113 20:27:52 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.10
[32m[20230113 20:27:52 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 246.53
[32m[20230113 20:27:52 @agent_ppo2.py:144][0m Total time:      43.32 min
[32m[20230113 20:27:52 @agent_ppo2.py:146][0m 4061184 total steps have happened
[32m[20230113 20:27:52 @agent_ppo2.py:122][0m #------------------------ Iteration 1983 --------------------------#
[32m[20230113 20:27:52 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:52 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:52 @agent_ppo2.py:186][0m |           0.0005 |           5.6962 |          15.8234 |
[32m[20230113 20:27:52 @agent_ppo2.py:186][0m |          -0.0025 |           4.4348 |          15.8077 |
[32m[20230113 20:27:52 @agent_ppo2.py:186][0m |          -0.0083 |           3.9367 |          15.8193 |
[32m[20230113 20:27:52 @agent_ppo2.py:186][0m |          -0.0104 |           3.6829 |          15.8054 |
[32m[20230113 20:27:52 @agent_ppo2.py:186][0m |          -0.0108 |           3.4456 |          15.8137 |
[32m[20230113 20:27:52 @agent_ppo2.py:186][0m |          -0.0073 |           3.3103 |          15.8139 |
[32m[20230113 20:27:52 @agent_ppo2.py:186][0m |          -0.0141 |           3.1755 |          15.8176 |
[32m[20230113 20:27:52 @agent_ppo2.py:186][0m |          -0.0137 |           3.1262 |          15.8155 |
[32m[20230113 20:27:52 @agent_ppo2.py:186][0m |          -0.0172 |           3.0268 |          15.8132 |
[32m[20230113 20:27:52 @agent_ppo2.py:186][0m |          -0.0138 |           2.9336 |          15.8097 |
[32m[20230113 20:27:52 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:53 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.72
[32m[20230113 20:27:53 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.71
[32m[20230113 20:27:53 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 252.11
[32m[20230113 20:27:53 @agent_ppo2.py:144][0m Total time:      43.34 min
[32m[20230113 20:27:53 @agent_ppo2.py:146][0m 4063232 total steps have happened
[32m[20230113 20:27:53 @agent_ppo2.py:122][0m #------------------------ Iteration 1984 --------------------------#
[32m[20230113 20:27:53 @agent_ppo2.py:128][0m Sampling time: 0.41 s by 1 slaves
[32m[20230113 20:27:53 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:53 @agent_ppo2.py:186][0m |           0.0017 |          12.2575 |          15.8939 |
[32m[20230113 20:27:53 @agent_ppo2.py:186][0m |          -0.0032 |           4.8406 |          15.8875 |
[32m[20230113 20:27:53 @agent_ppo2.py:186][0m |          -0.0082 |           4.1166 |          15.8762 |
[32m[20230113 20:27:53 @agent_ppo2.py:186][0m |          -0.0097 |           3.7186 |          15.8899 |
[32m[20230113 20:27:53 @agent_ppo2.py:186][0m |          -0.0107 |           3.4878 |          15.8940 |
[32m[20230113 20:27:54 @agent_ppo2.py:186][0m |          -0.0128 |           3.3077 |          15.8961 |
[32m[20230113 20:27:54 @agent_ppo2.py:186][0m |          -0.0149 |           3.1722 |          15.8945 |
[32m[20230113 20:27:54 @agent_ppo2.py:186][0m |          -0.0137 |           3.0579 |          15.8921 |
[32m[20230113 20:27:54 @agent_ppo2.py:186][0m |          -0.0178 |           2.9744 |          15.8966 |
[32m[20230113 20:27:54 @agent_ppo2.py:186][0m |          -0.0184 |           2.8878 |          15.8946 |
[32m[20230113 20:27:54 @agent_ppo2.py:131][0m Policy update time: 0.41 s
[32m[20230113 20:27:54 @agent_ppo2.py:139][0m Average TRAINING episode reward: 175.29
[32m[20230113 20:27:54 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.05
[32m[20230113 20:27:54 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.94
[32m[20230113 20:27:54 @agent_ppo2.py:144][0m Total time:      43.36 min
[32m[20230113 20:27:54 @agent_ppo2.py:146][0m 4065280 total steps have happened
[32m[20230113 20:27:54 @agent_ppo2.py:122][0m #------------------------ Iteration 1985 --------------------------#
[32m[20230113 20:27:55 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:55 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:55 @agent_ppo2.py:186][0m |          -0.0015 |           6.9421 |          15.6640 |
[32m[20230113 20:27:55 @agent_ppo2.py:186][0m |          -0.0043 |           5.2522 |          15.6544 |
[32m[20230113 20:27:55 @agent_ppo2.py:186][0m |          -0.0065 |           4.7145 |          15.6418 |
[32m[20230113 20:27:55 @agent_ppo2.py:186][0m |          -0.0122 |           4.3656 |          15.6437 |
[32m[20230113 20:27:55 @agent_ppo2.py:186][0m |          -0.0133 |           4.1062 |          15.6382 |
[32m[20230113 20:27:55 @agent_ppo2.py:186][0m |          -0.0157 |           3.9490 |          15.6507 |
[32m[20230113 20:27:55 @agent_ppo2.py:186][0m |          -0.0141 |           3.7357 |          15.6406 |
[32m[20230113 20:27:55 @agent_ppo2.py:186][0m |          -0.0173 |           3.6371 |          15.6333 |
[32m[20230113 20:27:55 @agent_ppo2.py:186][0m |          -0.0184 |           3.5498 |          15.6446 |
[32m[20230113 20:27:55 @agent_ppo2.py:186][0m |          -0.0173 |           3.4860 |          15.6396 |
[32m[20230113 20:27:55 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:55 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.15
[32m[20230113 20:27:55 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.27
[32m[20230113 20:27:55 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.82
[32m[20230113 20:27:55 @agent_ppo2.py:144][0m Total time:      43.38 min
[32m[20230113 20:27:55 @agent_ppo2.py:146][0m 4067328 total steps have happened
[32m[20230113 20:27:55 @agent_ppo2.py:122][0m #------------------------ Iteration 1986 --------------------------#
[32m[20230113 20:27:56 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:27:56 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:56 @agent_ppo2.py:186][0m |           0.0010 |           4.9852 |          15.9260 |
[32m[20230113 20:27:56 @agent_ppo2.py:186][0m |          -0.0077 |           3.8660 |          15.8951 |
[32m[20230113 20:27:56 @agent_ppo2.py:186][0m |          -0.0111 |           3.5516 |          15.8890 |
[32m[20230113 20:27:56 @agent_ppo2.py:186][0m |          -0.0123 |           3.3415 |          15.8882 |
[32m[20230113 20:27:56 @agent_ppo2.py:186][0m |          -0.0143 |           3.2125 |          15.8862 |
[32m[20230113 20:27:56 @agent_ppo2.py:186][0m |          -0.0155 |           3.1126 |          15.8886 |
[32m[20230113 20:27:56 @agent_ppo2.py:186][0m |          -0.0159 |           3.0270 |          15.8848 |
[32m[20230113 20:27:56 @agent_ppo2.py:186][0m |          -0.0172 |           2.9567 |          15.8841 |
[32m[20230113 20:27:56 @agent_ppo2.py:186][0m |          -0.0172 |           2.9172 |          15.8834 |
[32m[20230113 20:27:56 @agent_ppo2.py:186][0m |          -0.0170 |           2.8485 |          15.8736 |
[32m[20230113 20:27:56 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:57 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.61
[32m[20230113 20:27:57 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.16
[32m[20230113 20:27:57 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.20
[32m[20230113 20:27:57 @agent_ppo2.py:144][0m Total time:      43.40 min
[32m[20230113 20:27:57 @agent_ppo2.py:146][0m 4069376 total steps have happened
[32m[20230113 20:27:57 @agent_ppo2.py:122][0m #------------------------ Iteration 1987 --------------------------#
[32m[20230113 20:27:57 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:27:57 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:57 @agent_ppo2.py:186][0m |           0.0013 |           5.5243 |          15.9501 |
[32m[20230113 20:27:57 @agent_ppo2.py:186][0m |          -0.0040 |           3.5278 |          15.9607 |
[32m[20230113 20:27:57 @agent_ppo2.py:186][0m |          -0.0082 |           3.1245 |          15.9442 |
[32m[20230113 20:27:57 @agent_ppo2.py:186][0m |          -0.0082 |           2.9410 |          15.9364 |
[32m[20230113 20:27:57 @agent_ppo2.py:186][0m |          -0.0112 |           2.8252 |          15.9430 |
[32m[20230113 20:27:57 @agent_ppo2.py:186][0m |          -0.0116 |           2.6991 |          15.9352 |
[32m[20230113 20:27:57 @agent_ppo2.py:186][0m |          -0.0133 |           2.6114 |          15.9495 |
[32m[20230113 20:27:57 @agent_ppo2.py:186][0m |          -0.0148 |           2.5367 |          15.9373 |
[32m[20230113 20:27:57 @agent_ppo2.py:186][0m |          -0.0129 |           2.4874 |          15.9311 |
[32m[20230113 20:27:57 @agent_ppo2.py:186][0m |          -0.0167 |           2.4299 |          15.9250 |
[32m[20230113 20:27:57 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:27:58 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.41
[32m[20230113 20:27:58 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.20
[32m[20230113 20:27:58 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.81
[32m[20230113 20:27:58 @agent_ppo2.py:144][0m Total time:      43.42 min
[32m[20230113 20:27:58 @agent_ppo2.py:146][0m 4071424 total steps have happened
[32m[20230113 20:27:58 @agent_ppo2.py:122][0m #------------------------ Iteration 1988 --------------------------#
[32m[20230113 20:27:58 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:27:58 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:27:58 @agent_ppo2.py:186][0m |           0.0009 |           5.8830 |          15.9703 |
[32m[20230113 20:27:58 @agent_ppo2.py:186][0m |          -0.0046 |           4.1736 |          15.9660 |
[32m[20230113 20:27:58 @agent_ppo2.py:186][0m |          -0.0056 |           3.7445 |          15.9519 |
[32m[20230113 20:27:59 @agent_ppo2.py:186][0m |          -0.0072 |           3.4784 |          15.9500 |
[32m[20230113 20:27:59 @agent_ppo2.py:186][0m |          -0.0087 |           3.3324 |          15.9649 |
[32m[20230113 20:27:59 @agent_ppo2.py:186][0m |          -0.0103 |           3.2100 |          15.9679 |
[32m[20230113 20:27:59 @agent_ppo2.py:186][0m |          -0.0101 |           3.1413 |          15.9589 |
[32m[20230113 20:27:59 @agent_ppo2.py:186][0m |          -0.0123 |           3.0279 |          15.9536 |
[32m[20230113 20:27:59 @agent_ppo2.py:186][0m |          -0.0122 |           2.9649 |          15.9582 |
[32m[20230113 20:27:59 @agent_ppo2.py:186][0m |          -0.0118 |           2.8855 |          15.9749 |
[32m[20230113 20:27:59 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:27:59 @agent_ppo2.py:139][0m Average TRAINING episode reward: 238.88
[32m[20230113 20:27:59 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.62
[32m[20230113 20:27:59 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.01
[32m[20230113 20:27:59 @agent_ppo2.py:144][0m Total time:      43.44 min
[32m[20230113 20:27:59 @agent_ppo2.py:146][0m 4073472 total steps have happened
[32m[20230113 20:27:59 @agent_ppo2.py:122][0m #------------------------ Iteration 1989 --------------------------#
[32m[20230113 20:28:00 @agent_ppo2.py:128][0m Sampling time: 0.44 s by 1 slaves
[32m[20230113 20:28:00 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:28:00 @agent_ppo2.py:186][0m |          -0.0028 |           5.7895 |          15.5676 |
[32m[20230113 20:28:00 @agent_ppo2.py:186][0m |          -0.0063 |           4.2127 |          15.5383 |
[32m[20230113 20:28:00 @agent_ppo2.py:186][0m |          -0.0100 |           3.7266 |          15.5222 |
[32m[20230113 20:28:00 @agent_ppo2.py:186][0m |          -0.0119 |           3.4972 |          15.5281 |
[32m[20230113 20:28:00 @agent_ppo2.py:186][0m |          -0.0115 |           3.3167 |          15.5303 |
[32m[20230113 20:28:00 @agent_ppo2.py:186][0m |          -0.0120 |           3.1774 |          15.5229 |
[32m[20230113 20:28:00 @agent_ppo2.py:186][0m |          -0.0147 |           3.1347 |          15.5100 |
[32m[20230113 20:28:00 @agent_ppo2.py:186][0m |          -0.0174 |           3.0512 |          15.5138 |
[32m[20230113 20:28:00 @agent_ppo2.py:186][0m |          -0.0163 |           2.9793 |          15.5111 |
[32m[20230113 20:28:00 @agent_ppo2.py:186][0m |          -0.0165 |           2.9052 |          15.5003 |
[32m[20230113 20:28:00 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:28:00 @agent_ppo2.py:139][0m Average TRAINING episode reward: 236.63
[32m[20230113 20:28:00 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 238.96
[32m[20230113 20:28:00 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 254.12
[32m[20230113 20:28:00 @agent_ppo2.py:144][0m Total time:      43.46 min
[32m[20230113 20:28:00 @agent_ppo2.py:146][0m 4075520 total steps have happened
[32m[20230113 20:28:00 @agent_ppo2.py:122][0m #------------------------ Iteration 1990 --------------------------#
[32m[20230113 20:28:01 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:28:01 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:28:01 @agent_ppo2.py:186][0m |           0.0027 |           5.4660 |          15.8471 |
[32m[20230113 20:28:01 @agent_ppo2.py:186][0m |          -0.0050 |           4.2636 |          15.7978 |
[32m[20230113 20:28:01 @agent_ppo2.py:186][0m |          -0.0064 |           3.8547 |          15.8014 |
[32m[20230113 20:28:01 @agent_ppo2.py:186][0m |          -0.0103 |           3.6146 |          15.7907 |
[32m[20230113 20:28:01 @agent_ppo2.py:186][0m |          -0.0091 |           3.4387 |          15.7897 |
[32m[20230113 20:28:01 @agent_ppo2.py:186][0m |          -0.0112 |           3.3423 |          15.7982 |
[32m[20230113 20:28:01 @agent_ppo2.py:186][0m |          -0.0118 |           3.2160 |          15.7794 |
[32m[20230113 20:28:01 @agent_ppo2.py:186][0m |          -0.0120 |           3.1570 |          15.7749 |
[32m[20230113 20:28:01 @agent_ppo2.py:186][0m |          -0.0135 |           3.0468 |          15.7921 |
[32m[20230113 20:28:01 @agent_ppo2.py:186][0m |          -0.0151 |           2.9880 |          15.7775 |
[32m[20230113 20:28:01 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:28:02 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.07
[32m[20230113 20:28:02 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.26
[32m[20230113 20:28:02 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.59
[32m[20230113 20:28:02 @agent_ppo2.py:144][0m Total time:      43.49 min
[32m[20230113 20:28:02 @agent_ppo2.py:146][0m 4077568 total steps have happened
[32m[20230113 20:28:02 @agent_ppo2.py:122][0m #------------------------ Iteration 1991 --------------------------#
[32m[20230113 20:28:02 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:28:02 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:28:02 @agent_ppo2.py:186][0m |          -0.0017 |           5.6296 |          15.8087 |
[32m[20230113 20:28:02 @agent_ppo2.py:186][0m |          -0.0087 |           4.2656 |          15.7851 |
[32m[20230113 20:28:02 @agent_ppo2.py:186][0m |          -0.0108 |           3.7720 |          15.7747 |
[32m[20230113 20:28:02 @agent_ppo2.py:186][0m |          -0.0119 |           3.4708 |          15.7757 |
[32m[20230113 20:28:02 @agent_ppo2.py:186][0m |          -0.0133 |           3.3808 |          15.7629 |
[32m[20230113 20:28:02 @agent_ppo2.py:186][0m |          -0.0137 |           3.1657 |          15.7729 |
[32m[20230113 20:28:02 @agent_ppo2.py:186][0m |          -0.0163 |           3.0547 |          15.7529 |
[32m[20230113 20:28:02 @agent_ppo2.py:186][0m |          -0.0151 |           2.9403 |          15.7463 |
[32m[20230113 20:28:02 @agent_ppo2.py:186][0m |          -0.0171 |           2.8601 |          15.7608 |
[32m[20230113 20:28:03 @agent_ppo2.py:186][0m |          -0.0181 |           2.7845 |          15.7450 |
[32m[20230113 20:28:03 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:28:03 @agent_ppo2.py:139][0m Average TRAINING episode reward: 242.76
[32m[20230113 20:28:03 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.01
[32m[20230113 20:28:03 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.73
[32m[20230113 20:28:03 @agent_ppo2.py:144][0m Total time:      43.51 min
[32m[20230113 20:28:03 @agent_ppo2.py:146][0m 4079616 total steps have happened
[32m[20230113 20:28:03 @agent_ppo2.py:122][0m #------------------------ Iteration 1992 --------------------------#
[32m[20230113 20:28:03 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:28:03 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:28:03 @agent_ppo2.py:186][0m |          -0.0000 |           5.0925 |          15.7144 |
[32m[20230113 20:28:03 @agent_ppo2.py:186][0m |          -0.0072 |           3.9945 |          15.6840 |
[32m[20230113 20:28:04 @agent_ppo2.py:186][0m |          -0.0088 |           3.6222 |          15.6756 |
[32m[20230113 20:28:04 @agent_ppo2.py:186][0m |          -0.0068 |           3.4619 |          15.6925 |
[32m[20230113 20:28:04 @agent_ppo2.py:186][0m |          -0.0127 |           3.2925 |          15.6824 |
[32m[20230113 20:28:04 @agent_ppo2.py:186][0m |          -0.0117 |           3.1733 |          15.6686 |
[32m[20230113 20:28:04 @agent_ppo2.py:186][0m |          -0.0113 |           3.1495 |          15.6801 |
[32m[20230113 20:28:04 @agent_ppo2.py:186][0m |          -0.0142 |           2.9759 |          15.6838 |
[32m[20230113 20:28:04 @agent_ppo2.py:186][0m |          -0.0120 |           2.9341 |          15.6906 |
[32m[20230113 20:28:04 @agent_ppo2.py:186][0m |          -0.0155 |           2.8579 |          15.6785 |
[32m[20230113 20:28:04 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:28:04 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.59
[32m[20230113 20:28:04 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.16
[32m[20230113 20:28:04 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 247.19
[32m[20230113 20:28:04 @agent_ppo2.py:144][0m Total time:      43.53 min
[32m[20230113 20:28:04 @agent_ppo2.py:146][0m 4081664 total steps have happened
[32m[20230113 20:28:04 @agent_ppo2.py:122][0m #------------------------ Iteration 1993 --------------------------#
[32m[20230113 20:28:05 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:28:05 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:28:05 @agent_ppo2.py:186][0m |           0.0045 |           5.5295 |          15.7114 |
[32m[20230113 20:28:05 @agent_ppo2.py:186][0m |          -0.0057 |           4.4028 |          15.6831 |
[32m[20230113 20:28:05 @agent_ppo2.py:186][0m |           0.0000 |           4.0803 |          15.6742 |
[32m[20230113 20:28:05 @agent_ppo2.py:186][0m |          -0.0134 |           3.7505 |          15.6769 |
[32m[20230113 20:28:05 @agent_ppo2.py:186][0m |          -0.0107 |           3.5921 |          15.6689 |
[32m[20230113 20:28:05 @agent_ppo2.py:186][0m |          -0.0217 |           3.4412 |          15.6772 |
[32m[20230113 20:28:05 @agent_ppo2.py:186][0m |          -0.0157 |           3.3507 |          15.6732 |
[32m[20230113 20:28:05 @agent_ppo2.py:186][0m |          -0.0100 |           3.1996 |          15.6730 |
[32m[20230113 20:28:05 @agent_ppo2.py:186][0m |          -0.0092 |           3.1594 |          15.6764 |
[32m[20230113 20:28:05 @agent_ppo2.py:186][0m |          -0.0167 |           3.0992 |          15.6650 |
[32m[20230113 20:28:05 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:28:05 @agent_ppo2.py:139][0m Average TRAINING episode reward: 241.20
[32m[20230113 20:28:05 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 242.35
[32m[20230113 20:28:05 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 248.48
[32m[20230113 20:28:05 @agent_ppo2.py:144][0m Total time:      43.55 min
[32m[20230113 20:28:05 @agent_ppo2.py:146][0m 4083712 total steps have happened
[32m[20230113 20:28:05 @agent_ppo2.py:122][0m #------------------------ Iteration 1994 --------------------------#
[32m[20230113 20:28:06 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:28:06 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:28:06 @agent_ppo2.py:186][0m |           0.0004 |           5.7932 |          15.9393 |
[32m[20230113 20:28:06 @agent_ppo2.py:186][0m |          -0.0074 |           4.1013 |          15.9302 |
[32m[20230113 20:28:06 @agent_ppo2.py:186][0m |          -0.0104 |           3.5888 |          15.9245 |
[32m[20230113 20:28:06 @agent_ppo2.py:186][0m |          -0.0122 |           3.3281 |          15.9319 |
[32m[20230113 20:28:06 @agent_ppo2.py:186][0m |          -0.0138 |           3.1769 |          15.9333 |
[32m[20230113 20:28:06 @agent_ppo2.py:186][0m |          -0.0162 |           2.9602 |          15.9215 |
[32m[20230113 20:28:06 @agent_ppo2.py:186][0m |          -0.0167 |           2.8921 |          15.9365 |
[32m[20230113 20:28:06 @agent_ppo2.py:186][0m |          -0.0170 |           2.8235 |          15.9067 |
[32m[20230113 20:28:06 @agent_ppo2.py:186][0m |          -0.0189 |           2.7527 |          15.9282 |
[32m[20230113 20:28:06 @agent_ppo2.py:186][0m |          -0.0187 |           2.6582 |          15.9252 |
[32m[20230113 20:28:06 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:28:07 @agent_ppo2.py:139][0m Average TRAINING episode reward: 239.67
[32m[20230113 20:28:07 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 241.29
[32m[20230113 20:28:07 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 244.89
[32m[20230113 20:28:07 @agent_ppo2.py:144][0m Total time:      43.57 min
[32m[20230113 20:28:07 @agent_ppo2.py:146][0m 4085760 total steps have happened
[32m[20230113 20:28:07 @agent_ppo2.py:122][0m #------------------------ Iteration 1995 --------------------------#
[32m[20230113 20:28:07 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:28:07 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:28:07 @agent_ppo2.py:186][0m |          -0.0028 |           6.1610 |          15.7008 |
[32m[20230113 20:28:07 @agent_ppo2.py:186][0m |          -0.0001 |           4.1423 |          15.6641 |
[32m[20230113 20:28:07 @agent_ppo2.py:186][0m |          -0.0115 |           3.5574 |          15.6610 |
[32m[20230113 20:28:07 @agent_ppo2.py:186][0m |          -0.0101 |           3.3055 |          15.6512 |
[32m[20230113 20:28:07 @agent_ppo2.py:186][0m |          -0.0044 |           3.1527 |          15.6368 |
[32m[20230113 20:28:07 @agent_ppo2.py:186][0m |          -0.0137 |           3.0136 |          15.6510 |
[32m[20230113 20:28:08 @agent_ppo2.py:186][0m |          -0.0143 |           2.8980 |          15.6464 |
[32m[20230113 20:28:08 @agent_ppo2.py:186][0m |          -0.0139 |           2.8237 |          15.6391 |
[32m[20230113 20:28:08 @agent_ppo2.py:186][0m |          -0.0016 |           2.7527 |          15.6341 |
[32m[20230113 20:28:08 @agent_ppo2.py:186][0m |          -0.0101 |           2.6403 |          15.6369 |
[32m[20230113 20:28:08 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:28:08 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.45
[32m[20230113 20:28:08 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 239.87
[32m[20230113 20:28:08 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.10
[32m[20230113 20:28:08 @agent_ppo2.py:144][0m Total time:      43.59 min
[32m[20230113 20:28:08 @agent_ppo2.py:146][0m 4087808 total steps have happened
[32m[20230113 20:28:08 @agent_ppo2.py:122][0m #------------------------ Iteration 1996 --------------------------#
[32m[20230113 20:28:08 @agent_ppo2.py:128][0m Sampling time: 0.43 s by 1 slaves
[32m[20230113 20:28:09 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:28:09 @agent_ppo2.py:186][0m |           0.0020 |           5.8366 |          15.4367 |
[32m[20230113 20:28:09 @agent_ppo2.py:186][0m |          -0.0041 |           4.6741 |          15.4300 |
[32m[20230113 20:28:09 @agent_ppo2.py:186][0m |          -0.0073 |           4.3102 |          15.3941 |
[32m[20230113 20:28:09 @agent_ppo2.py:186][0m |          -0.0102 |           4.0499 |          15.4010 |
[32m[20230113 20:28:09 @agent_ppo2.py:186][0m |          -0.0134 |           3.8309 |          15.3964 |
[32m[20230113 20:28:09 @agent_ppo2.py:186][0m |          -0.0133 |           3.7554 |          15.3910 |
[32m[20230113 20:28:09 @agent_ppo2.py:186][0m |          -0.0130 |           3.6294 |          15.3940 |
[32m[20230113 20:28:09 @agent_ppo2.py:186][0m |          -0.0149 |           3.5720 |          15.3877 |
[32m[20230113 20:28:09 @agent_ppo2.py:186][0m |          -0.0154 |           3.4837 |          15.3999 |
[32m[20230113 20:28:09 @agent_ppo2.py:186][0m |          -0.0172 |           3.4204 |          15.3977 |
[32m[20230113 20:28:09 @agent_ppo2.py:131][0m Policy update time: 0.44 s
[32m[20230113 20:28:09 @agent_ppo2.py:139][0m Average TRAINING episode reward: 237.83
[32m[20230113 20:28:09 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 240.52
[32m[20230113 20:28:09 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 249.90
[32m[20230113 20:28:09 @agent_ppo2.py:144][0m Total time:      43.61 min
[32m[20230113 20:28:09 @agent_ppo2.py:146][0m 4089856 total steps have happened
[32m[20230113 20:28:09 @agent_ppo2.py:122][0m #------------------------ Iteration 1997 --------------------------#
[32m[20230113 20:28:10 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:28:10 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:28:10 @agent_ppo2.py:186][0m |           0.0013 |           5.8803 |          15.7560 |
[32m[20230113 20:28:10 @agent_ppo2.py:186][0m |          -0.0054 |           4.4553 |          15.7310 |
[32m[20230113 20:28:10 @agent_ppo2.py:186][0m |          -0.0032 |           3.9733 |          15.7226 |
[32m[20230113 20:28:10 @agent_ppo2.py:186][0m |          -0.0106 |           3.7481 |          15.7110 |
[32m[20230113 20:28:10 @agent_ppo2.py:186][0m |          -0.0161 |           3.5504 |          15.7302 |
[32m[20230113 20:28:10 @agent_ppo2.py:186][0m |          -0.0147 |           3.3834 |          15.7208 |
[32m[20230113 20:28:10 @agent_ppo2.py:186][0m |          -0.0141 |           3.2834 |          15.7138 |
[32m[20230113 20:28:10 @agent_ppo2.py:186][0m |          -0.0082 |           3.3104 |          15.7127 |
[32m[20230113 20:28:10 @agent_ppo2.py:186][0m |          -0.0153 |           3.0918 |          15.7034 |
[32m[20230113 20:28:10 @agent_ppo2.py:186][0m |          -0.0178 |           3.0105 |          15.7075 |
[32m[20230113 20:28:10 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:28:11 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.69
[32m[20230113 20:28:11 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 243.97
[32m[20230113 20:28:11 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 156.62
[32m[20230113 20:28:11 @agent_ppo2.py:144][0m Total time:      43.63 min
[32m[20230113 20:28:11 @agent_ppo2.py:146][0m 4091904 total steps have happened
[32m[20230113 20:28:11 @agent_ppo2.py:122][0m #------------------------ Iteration 1998 --------------------------#
[32m[20230113 20:28:11 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:28:11 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:28:11 @agent_ppo2.py:186][0m |           0.0008 |           5.9847 |          15.9987 |
[32m[20230113 20:28:11 @agent_ppo2.py:186][0m |          -0.0056 |           4.1905 |          15.9941 |
[32m[20230113 20:28:11 @agent_ppo2.py:186][0m |          -0.0080 |           3.7049 |          15.9915 |
[32m[20230113 20:28:11 @agent_ppo2.py:186][0m |          -0.0097 |           3.4768 |          15.9769 |
[32m[20230113 20:28:11 @agent_ppo2.py:186][0m |          -0.0112 |           3.2671 |          15.9730 |
[32m[20230113 20:28:11 @agent_ppo2.py:186][0m |          -0.0118 |           3.1269 |          15.9672 |
[32m[20230113 20:28:11 @agent_ppo2.py:186][0m |          -0.0132 |           3.0036 |          15.9734 |
[32m[20230113 20:28:11 @agent_ppo2.py:186][0m |          -0.0142 |           2.9114 |          15.9663 |
[32m[20230113 20:28:11 @agent_ppo2.py:186][0m |          -0.0154 |           2.8366 |          15.9660 |
[32m[20230113 20:28:11 @agent_ppo2.py:186][0m |          -0.0166 |           2.7719 |          15.9686 |
[32m[20230113 20:28:11 @agent_ppo2.py:131][0m Policy update time: 0.42 s
[32m[20230113 20:28:12 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.57
[32m[20230113 20:28:12 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 247.92
[32m[20230113 20:28:12 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 250.62
[32m[20230113 20:28:12 @agent_ppo2.py:144][0m Total time:      43.66 min
[32m[20230113 20:28:12 @agent_ppo2.py:146][0m 4093952 total steps have happened
[32m[20230113 20:28:12 @agent_ppo2.py:122][0m #------------------------ Iteration 1999 --------------------------#
[32m[20230113 20:28:12 @agent_ppo2.py:128][0m Sampling time: 0.42 s by 1 slaves
[32m[20230113 20:28:12 @agent_ppo2.py:162][0m |      policy_loss |       value_loss |          entropy |
[32m[20230113 20:28:12 @agent_ppo2.py:186][0m |           0.0020 |           6.1395 |          15.7899 |
[32m[20230113 20:28:12 @agent_ppo2.py:186][0m |          -0.0043 |           4.4102 |          15.7617 |
[32m[20230113 20:28:12 @agent_ppo2.py:186][0m |          -0.0112 |           3.7526 |          15.7754 |
[32m[20230113 20:28:13 @agent_ppo2.py:186][0m |          -0.0129 |           3.3825 |          15.7746 |
[32m[20230113 20:28:13 @agent_ppo2.py:186][0m |          -0.0143 |           3.1881 |          15.7734 |
[32m[20230113 20:28:13 @agent_ppo2.py:186][0m |          -0.0156 |           3.0303 |          15.7702 |
[32m[20230113 20:28:13 @agent_ppo2.py:186][0m |          -0.0145 |           2.8546 |          15.7636 |
[32m[20230113 20:28:13 @agent_ppo2.py:186][0m |          -0.0177 |           2.7804 |          15.7605 |
[32m[20230113 20:28:13 @agent_ppo2.py:186][0m |          -0.0184 |           2.7063 |          15.7649 |
[32m[20230113 20:28:13 @agent_ppo2.py:186][0m |          -0.0198 |           2.6010 |          15.7482 |
[32m[20230113 20:28:13 @agent_ppo2.py:131][0m Policy update time: 0.43 s
[32m[20230113 20:28:13 @agent_ppo2.py:139][0m Average TRAINING episode reward: 240.43
[32m[20230113 20:28:13 @agent_ppo2.py:140][0m Maximum TRAINING episode reward: 244.32
[32m[20230113 20:28:13 @agent_ppo2.py:141][0m Average EVALUATION episode reward: 245.67
[32m[20230113 20:28:13 @agent_ppo2.py:104][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 258.49
[32m[20230113 20:28:13 @agent_ppo2.py:144][0m Total time:      43.68 min
[32m[20230113 20:28:13 @agent_ppo2.py:146][0m 4096000 total steps have happened
[32m[20230113 20:28:13 @train.py:54][0m [4m[34mCRITICAL[0m Training completed!
