[32m[20230205 18:13:27 @logger.py:106][0m Log file set to ./tmp/bipedalwalker/easy/20230205_181327/log/bipedalwalker_easy-20230205_181327.log
[32m[20230205 18:13:27 @agent_ppo2.py:127][0m #------------------------ Iteration 0 --------------------------#
[32m[20230205 18:13:28 @agent_ppo2.py:133][0m Sampling time: 0.85 s by 1 slaves
[32m[20230205 18:13:28 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:28 @agent_ppo2.py:191][0m |           0.0043 |          85.5758 |           1.8335 |
[32m[20230205 18:13:28 @agent_ppo2.py:191][0m |          -0.0038 |          82.4880 |           1.8324 |
[32m[20230205 18:13:29 @agent_ppo2.py:191][0m |          -0.0051 |          79.6342 |           1.8314 |
[32m[20230205 18:13:29 @agent_ppo2.py:191][0m |          -0.0035 |          76.2157 |           1.8306 |
[32m[20230205 18:13:29 @agent_ppo2.py:191][0m |          -0.0065 |          73.8982 |           1.8294 |
[32m[20230205 18:13:29 @agent_ppo2.py:191][0m |           0.0060 |          73.9979 |           1.8282 |
[32m[20230205 18:13:29 @agent_ppo2.py:191][0m |          -0.0052 |          71.5788 |           1.8275 |
[32m[20230205 18:13:29 @agent_ppo2.py:191][0m |          -0.0069 |          70.6496 |           1.8265 |
[32m[20230205 18:13:29 @agent_ppo2.py:191][0m |          -0.0078 |          69.7005 |           1.8253 |
[32m[20230205 18:13:29 @agent_ppo2.py:191][0m |          -0.0083 |          68.6806 |           1.8243 |
[32m[20230205 18:13:29 @agent_ppo2.py:136][0m Policy update time: 0.83 s
[32m[20230205 18:13:30 @agent_ppo2.py:144][0m Average TRAINING episode reward: -108.09
[32m[20230205 18:13:30 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -104.27
[32m[20230205 18:13:30 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -101.24
[32m[20230205 18:13:30 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -101.24
[32m[20230205 18:13:30 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -101.24
[32m[20230205 18:13:30 @agent_ppo2.py:149][0m Total time:       0.04 min
[32m[20230205 18:13:30 @agent_ppo2.py:151][0m 2048 total steps have happened
[32m[20230205 18:13:30 @agent_ppo2.py:127][0m #------------------------ Iteration 1 --------------------------#
[32m[20230205 18:13:30 @agent_ppo2.py:133][0m Sampling time: 0.85 s by 1 slaves
[32m[20230205 18:13:30 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:31 @agent_ppo2.py:191][0m |           0.0000 |          93.1649 |           1.8885 |
[32m[20230205 18:13:31 @agent_ppo2.py:191][0m |          -0.0004 |          88.4609 |           1.8881 |
[32m[20230205 18:13:31 @agent_ppo2.py:191][0m |          -0.0006 |          85.2014 |           1.8880 |
[32m[20230205 18:13:31 @agent_ppo2.py:191][0m |          -0.0010 |          81.7235 |           1.8873 |
[32m[20230205 18:13:31 @agent_ppo2.py:191][0m |          -0.0016 |          78.3530 |           1.8871 |
[32m[20230205 18:13:31 @agent_ppo2.py:191][0m |          -0.0022 |          75.0066 |           1.8868 |
[32m[20230205 18:13:31 @agent_ppo2.py:191][0m |          -0.0027 |          71.8656 |           1.8862 |
[32m[20230205 18:13:31 @agent_ppo2.py:191][0m |          -0.0029 |          69.0273 |           1.8857 |
[32m[20230205 18:13:31 @agent_ppo2.py:191][0m |          -0.0033 |          66.3232 |           1.8856 |
[32m[20230205 18:13:31 @agent_ppo2.py:191][0m |          -0.0036 |          63.7252 |           1.8848 |
[32m[20230205 18:13:31 @agent_ppo2.py:136][0m Policy update time: 0.91 s
[32m[20230205 18:13:32 @agent_ppo2.py:144][0m Average TRAINING episode reward: -109.62
[32m[20230205 18:13:32 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -99.97
[32m[20230205 18:13:32 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -93.48
[32m[20230205 18:13:32 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -93.48
[32m[20230205 18:13:32 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -93.48
[32m[20230205 18:13:32 @agent_ppo2.py:149][0m Total time:       0.07 min
[32m[20230205 18:13:32 @agent_ppo2.py:151][0m 4096 total steps have happened
[32m[20230205 18:13:32 @agent_ppo2.py:127][0m #------------------------ Iteration 2 --------------------------#
[32m[20230205 18:13:32 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:13:32 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:32 @agent_ppo2.py:191][0m |          -0.0006 |         287.9088 |           1.8598 |
[32m[20230205 18:13:32 @agent_ppo2.py:191][0m |          -0.0010 |         227.4035 |           1.8595 |
[32m[20230205 18:13:33 @agent_ppo2.py:191][0m |          -0.0018 |         205.0118 |           1.8594 |
[32m[20230205 18:13:33 @agent_ppo2.py:191][0m |          -0.0029 |         193.9007 |           1.8592 |
[32m[20230205 18:13:33 @agent_ppo2.py:191][0m |          -0.0047 |         185.7912 |           1.8592 |
[32m[20230205 18:13:33 @agent_ppo2.py:191][0m |          -0.0056 |         178.7553 |           1.8588 |
[32m[20230205 18:13:33 @agent_ppo2.py:191][0m |          -0.0070 |         172.3141 |           1.8586 |
[32m[20230205 18:13:33 @agent_ppo2.py:191][0m |          -0.0080 |         166.7432 |           1.8582 |
[32m[20230205 18:13:33 @agent_ppo2.py:191][0m |          -0.0098 |         160.9500 |           1.8577 |
[32m[20230205 18:13:33 @agent_ppo2.py:191][0m |          -0.0105 |         156.3617 |           1.8572 |
[32m[20230205 18:13:33 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:13:33 @agent_ppo2.py:144][0m Average TRAINING episode reward: -109.61
[32m[20230205 18:13:33 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -99.94
[32m[20230205 18:13:33 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -92.28
[32m[20230205 18:13:33 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -92.28
[32m[20230205 18:13:33 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -92.28
[32m[20230205 18:13:33 @agent_ppo2.py:149][0m Total time:       0.10 min
[32m[20230205 18:13:33 @agent_ppo2.py:151][0m 6144 total steps have happened
[32m[20230205 18:13:33 @agent_ppo2.py:127][0m #------------------------ Iteration 3 --------------------------#
[32m[20230205 18:13:34 @agent_ppo2.py:133][0m Sampling time: 0.78 s by 1 slaves
[32m[20230205 18:13:34 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:34 @agent_ppo2.py:191][0m |          -0.0002 |          22.2792 |           1.8416 |
[32m[20230205 18:13:34 @agent_ppo2.py:191][0m |          -0.0025 |          19.5883 |           1.8410 |
[32m[20230205 18:13:34 @agent_ppo2.py:191][0m |          -0.0039 |          18.6113 |           1.8401 |
[32m[20230205 18:13:35 @agent_ppo2.py:191][0m |          -0.0060 |          18.1140 |           1.8392 |
[32m[20230205 18:13:35 @agent_ppo2.py:191][0m |          -0.0081 |          17.7359 |           1.8384 |
[32m[20230205 18:13:35 @agent_ppo2.py:191][0m |          -0.0088 |          17.4914 |           1.8371 |
[32m[20230205 18:13:35 @agent_ppo2.py:191][0m |          -0.0093 |          17.3010 |           1.8366 |
[32m[20230205 18:13:35 @agent_ppo2.py:191][0m |          -0.0089 |          17.2106 |           1.8354 |
[32m[20230205 18:13:35 @agent_ppo2.py:191][0m |          -0.0098 |          17.0932 |           1.8349 |
[32m[20230205 18:13:35 @agent_ppo2.py:191][0m |          -0.0104 |          16.7993 |           1.8340 |
[32m[20230205 18:13:35 @agent_ppo2.py:136][0m Policy update time: 0.80 s
[32m[20230205 18:13:36 @agent_ppo2.py:144][0m Average TRAINING episode reward: -104.77
[32m[20230205 18:13:36 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -95.34
[32m[20230205 18:13:36 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -92.34
[32m[20230205 18:13:36 @agent_ppo2.py:149][0m Total time:       0.14 min
[32m[20230205 18:13:36 @agent_ppo2.py:151][0m 8192 total steps have happened
[32m[20230205 18:13:36 @agent_ppo2.py:127][0m #------------------------ Iteration 4 --------------------------#
[32m[20230205 18:13:36 @agent_ppo2.py:133][0m Sampling time: 0.86 s by 1 slaves
[32m[20230205 18:13:37 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:37 @agent_ppo2.py:191][0m |          -0.0186 |          61.1554 |           1.7990 |
[32m[20230205 18:13:37 @agent_ppo2.py:191][0m |          -0.0008 |          56.9641 |           1.7989 |
[32m[20230205 18:13:37 @agent_ppo2.py:191][0m |          -0.0075 |          54.5383 |           1.7985 |
[32m[20230205 18:13:37 @agent_ppo2.py:191][0m |          -0.0095 |          52.5968 |           1.7975 |
[32m[20230205 18:13:37 @agent_ppo2.py:191][0m |          -0.0042 |          50.7913 |           1.7968 |
[32m[20230205 18:13:37 @agent_ppo2.py:191][0m |          -0.0092 |          49.1611 |           1.7961 |
[32m[20230205 18:13:37 @agent_ppo2.py:191][0m |          -0.0073 |          47.5815 |           1.7951 |
[32m[20230205 18:13:37 @agent_ppo2.py:191][0m |          -0.0105 |          46.1501 |           1.7938 |
[32m[20230205 18:13:37 @agent_ppo2.py:191][0m |          -0.0196 |          44.7977 |           1.7929 |
[32m[20230205 18:13:37 @agent_ppo2.py:191][0m |          -0.0093 |          43.3947 |           1.7920 |
[32m[20230205 18:13:37 @agent_ppo2.py:136][0m Policy update time: 1.02 s
[32m[20230205 18:13:38 @agent_ppo2.py:144][0m Average TRAINING episode reward: -106.87
[32m[20230205 18:13:38 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -98.35
[32m[20230205 18:13:38 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -111.08
[32m[20230205 18:13:38 @agent_ppo2.py:149][0m Total time:       0.18 min
[32m[20230205 18:13:38 @agent_ppo2.py:151][0m 10240 total steps have happened
[32m[20230205 18:13:38 @agent_ppo2.py:127][0m #------------------------ Iteration 5 --------------------------#
[32m[20230205 18:13:38 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:13:38 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:39 @agent_ppo2.py:191][0m |          -0.0004 |         123.2341 |           1.8554 |
[32m[20230205 18:13:39 @agent_ppo2.py:191][0m |          -0.0002 |         116.8429 |           1.8560 |
[32m[20230205 18:13:39 @agent_ppo2.py:191][0m |          -0.0012 |         112.4348 |           1.8566 |
[32m[20230205 18:13:39 @agent_ppo2.py:191][0m |          -0.0028 |         107.4619 |           1.8570 |
[32m[20230205 18:13:39 @agent_ppo2.py:191][0m |          -0.0042 |         103.4876 |           1.8570 |
[32m[20230205 18:13:39 @agent_ppo2.py:191][0m |          -0.0037 |         100.5796 |           1.8574 |
[32m[20230205 18:13:39 @agent_ppo2.py:191][0m |          -0.0065 |          96.3610 |           1.8573 |
[32m[20230205 18:13:39 @agent_ppo2.py:191][0m |          -0.0079 |          93.0483 |           1.8573 |
[32m[20230205 18:13:39 @agent_ppo2.py:191][0m |          -0.0084 |          90.0855 |           1.8575 |
[32m[20230205 18:13:39 @agent_ppo2.py:191][0m |          -0.0085 |          87.2378 |           1.8578 |
[32m[20230205 18:13:39 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:13:40 @agent_ppo2.py:144][0m Average TRAINING episode reward: -107.64
[32m[20230205 18:13:40 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -100.13
[32m[20230205 18:13:40 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -77.75
[32m[20230205 18:13:40 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -77.75
[32m[20230205 18:13:40 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -77.75
[32m[20230205 18:13:40 @agent_ppo2.py:149][0m Total time:       0.20 min
[32m[20230205 18:13:40 @agent_ppo2.py:151][0m 12288 total steps have happened
[32m[20230205 18:13:40 @agent_ppo2.py:127][0m #------------------------ Iteration 6 --------------------------#
[32m[20230205 18:13:40 @agent_ppo2.py:133][0m Sampling time: 0.81 s by 1 slaves
[32m[20230205 18:13:40 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:40 @agent_ppo2.py:191][0m |          -0.0009 |          12.4805 |           1.8173 |
[32m[20230205 18:13:41 @agent_ppo2.py:191][0m |          -0.0019 |          11.7980 |           1.8171 |
[32m[20230205 18:13:41 @agent_ppo2.py:191][0m |          -0.0042 |          11.6092 |           1.8171 |
[32m[20230205 18:13:41 @agent_ppo2.py:191][0m |          -0.0056 |          11.4638 |           1.8166 |
[32m[20230205 18:13:41 @agent_ppo2.py:191][0m |          -0.0065 |          11.3658 |           1.8167 |
[32m[20230205 18:13:41 @agent_ppo2.py:191][0m |          -0.0065 |          11.4264 |           1.8162 |
[32m[20230205 18:13:41 @agent_ppo2.py:191][0m |          -0.0087 |          11.1042 |           1.8160 |
[32m[20230205 18:13:41 @agent_ppo2.py:191][0m |          -0.0079 |          11.1521 |           1.8157 |
[32m[20230205 18:13:41 @agent_ppo2.py:191][0m |          -0.0092 |          10.9526 |           1.8153 |
[32m[20230205 18:13:41 @agent_ppo2.py:191][0m |          -0.0095 |          10.9349 |           1.8151 |
[32m[20230205 18:13:41 @agent_ppo2.py:136][0m Policy update time: 0.78 s
[32m[20230205 18:13:42 @agent_ppo2.py:144][0m Average TRAINING episode reward: -104.64
[32m[20230205 18:13:42 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -102.65
[32m[20230205 18:13:42 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -93.57
[32m[20230205 18:13:42 @agent_ppo2.py:149][0m Total time:       0.24 min
[32m[20230205 18:13:42 @agent_ppo2.py:151][0m 14336 total steps have happened
[32m[20230205 18:13:42 @agent_ppo2.py:127][0m #------------------------ Iteration 7 --------------------------#
[32m[20230205 18:13:42 @agent_ppo2.py:133][0m Sampling time: 0.78 s by 1 slaves
[32m[20230205 18:13:42 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:43 @agent_ppo2.py:191][0m |           0.0011 |          13.3919 |           1.8798 |
[32m[20230205 18:13:43 @agent_ppo2.py:191][0m |          -0.0001 |          13.1307 |           1.8787 |
[32m[20230205 18:13:43 @agent_ppo2.py:191][0m |           0.0006 |          12.7412 |           1.8785 |
[32m[20230205 18:13:43 @agent_ppo2.py:191][0m |          -0.0028 |          12.3518 |           1.8778 |
[32m[20230205 18:13:43 @agent_ppo2.py:191][0m |          -0.0046 |          12.1789 |           1.8764 |
[32m[20230205 18:13:43 @agent_ppo2.py:191][0m |          -0.0052 |          12.0671 |           1.8754 |
[32m[20230205 18:13:43 @agent_ppo2.py:191][0m |          -0.0024 |          12.3431 |           1.8742 |
[32m[20230205 18:13:43 @agent_ppo2.py:191][0m |          -0.0042 |          11.8789 |           1.8738 |
[32m[20230205 18:13:43 @agent_ppo2.py:191][0m |          -0.0069 |          11.6346 |           1.8726 |
[32m[20230205 18:13:43 @agent_ppo2.py:191][0m |          -0.0050 |          11.7214 |           1.8720 |
[32m[20230205 18:13:43 @agent_ppo2.py:136][0m Policy update time: 0.75 s
[32m[20230205 18:13:44 @agent_ppo2.py:144][0m Average TRAINING episode reward: -98.49
[32m[20230205 18:13:44 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -95.71
[32m[20230205 18:13:44 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -88.69
[32m[20230205 18:13:44 @agent_ppo2.py:149][0m Total time:       0.27 min
[32m[20230205 18:13:44 @agent_ppo2.py:151][0m 16384 total steps have happened
[32m[20230205 18:13:44 @agent_ppo2.py:127][0m #------------------------ Iteration 8 --------------------------#
[32m[20230205 18:13:45 @agent_ppo2.py:133][0m Sampling time: 0.81 s by 1 slaves
[32m[20230205 18:13:45 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:45 @agent_ppo2.py:191][0m |           0.0015 |          20.0354 |           1.8055 |
[32m[20230205 18:13:45 @agent_ppo2.py:191][0m |          -0.0030 |          18.1459 |           1.8055 |
[32m[20230205 18:13:45 @agent_ppo2.py:191][0m |          -0.0060 |          17.2349 |           1.8056 |
[32m[20230205 18:13:45 @agent_ppo2.py:191][0m |          -0.0048 |          16.5611 |           1.8058 |
[32m[20230205 18:13:45 @agent_ppo2.py:191][0m |          -0.0025 |          16.0555 |           1.8058 |
[32m[20230205 18:13:45 @agent_ppo2.py:191][0m |          -0.0051 |          15.7084 |           1.8057 |
[32m[20230205 18:13:45 @agent_ppo2.py:191][0m |          -0.0041 |          15.2105 |           1.8052 |
[32m[20230205 18:13:45 @agent_ppo2.py:191][0m |          -0.0121 |          14.8114 |           1.8054 |
[32m[20230205 18:13:45 @agent_ppo2.py:191][0m |          -0.0024 |          14.5503 |           1.8055 |
[32m[20230205 18:13:45 @agent_ppo2.py:191][0m |          -0.0002 |          14.8516 |           1.8057 |
[32m[20230205 18:13:45 @agent_ppo2.py:136][0m Policy update time: 0.78 s
[32m[20230205 18:13:46 @agent_ppo2.py:144][0m Average TRAINING episode reward: -103.67
[32m[20230205 18:13:46 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -102.43
[32m[20230205 18:13:46 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -12.73
[32m[20230205 18:13:46 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -12.73
[32m[20230205 18:13:46 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -12.73
[32m[20230205 18:13:46 @agent_ppo2.py:149][0m Total time:       0.31 min
[32m[20230205 18:13:46 @agent_ppo2.py:151][0m 18432 total steps have happened
[32m[20230205 18:13:46 @agent_ppo2.py:127][0m #------------------------ Iteration 9 --------------------------#
[32m[20230205 18:13:47 @agent_ppo2.py:133][0m Sampling time: 0.78 s by 1 slaves
[32m[20230205 18:13:47 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:47 @agent_ppo2.py:191][0m |           0.0001 |           2.8922 |           1.8307 |
[32m[20230205 18:13:47 @agent_ppo2.py:191][0m |          -0.0009 |           2.1428 |           1.8308 |
[32m[20230205 18:13:47 @agent_ppo2.py:191][0m |          -0.0020 |           2.0037 |           1.8308 |
[32m[20230205 18:13:47 @agent_ppo2.py:191][0m |          -0.0028 |           1.8562 |           1.8308 |
[32m[20230205 18:13:47 @agent_ppo2.py:191][0m |          -0.0040 |           1.7879 |           1.8310 |
[32m[20230205 18:13:47 @agent_ppo2.py:191][0m |          -0.0047 |           1.7414 |           1.8312 |
[32m[20230205 18:13:48 @agent_ppo2.py:191][0m |          -0.0053 |           1.7134 |           1.8310 |
[32m[20230205 18:13:48 @agent_ppo2.py:191][0m |          -0.0058 |           1.6855 |           1.8315 |
[32m[20230205 18:13:48 @agent_ppo2.py:191][0m |          -0.0063 |           1.6595 |           1.8316 |
[32m[20230205 18:13:48 @agent_ppo2.py:191][0m |          -0.0067 |           1.6435 |           1.8320 |
[32m[20230205 18:13:48 @agent_ppo2.py:136][0m Policy update time: 0.74 s
[32m[20230205 18:13:49 @agent_ppo2.py:144][0m Average TRAINING episode reward: -106.10
[32m[20230205 18:13:49 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -105.72
[32m[20230205 18:13:49 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -9.57
[32m[20230205 18:13:49 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: -9.57
[32m[20230205 18:13:49 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -9.57
[32m[20230205 18:13:49 @agent_ppo2.py:149][0m Total time:       0.35 min
[32m[20230205 18:13:49 @agent_ppo2.py:151][0m 20480 total steps have happened
[32m[20230205 18:13:49 @agent_ppo2.py:127][0m #------------------------ Iteration 10 --------------------------#
[32m[20230205 18:13:49 @agent_ppo2.py:133][0m Sampling time: 0.80 s by 1 slaves
[32m[20230205 18:13:49 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:49 @agent_ppo2.py:191][0m |          -0.0005 |          13.6837 |           1.8332 |
[32m[20230205 18:13:50 @agent_ppo2.py:191][0m |          -0.0018 |           6.9750 |           1.8324 |
[32m[20230205 18:13:50 @agent_ppo2.py:191][0m |          -0.0033 |           6.3274 |           1.8316 |
[32m[20230205 18:13:50 @agent_ppo2.py:191][0m |          -0.0030 |           6.1233 |           1.8306 |
[32m[20230205 18:13:50 @agent_ppo2.py:191][0m |          -0.0049 |           5.7984 |           1.8294 |
[32m[20230205 18:13:50 @agent_ppo2.py:191][0m |          -0.0057 |           5.6539 |           1.8284 |
[32m[20230205 18:13:50 @agent_ppo2.py:191][0m |          -0.0064 |           5.5458 |           1.8274 |
[32m[20230205 18:13:50 @agent_ppo2.py:191][0m |          -0.0064 |           5.4276 |           1.8265 |
[32m[20230205 18:13:50 @agent_ppo2.py:191][0m |          -0.0064 |           5.3362 |           1.8253 |
[32m[20230205 18:13:50 @agent_ppo2.py:191][0m |          -0.0073 |           5.2725 |           1.8245 |
[32m[20230205 18:13:50 @agent_ppo2.py:136][0m Policy update time: 0.77 s
[32m[20230205 18:13:51 @agent_ppo2.py:144][0m Average TRAINING episode reward: -107.60
[32m[20230205 18:13:51 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -103.67
[32m[20230205 18:13:51 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -18.89
[32m[20230205 18:13:51 @agent_ppo2.py:149][0m Total time:       0.39 min
[32m[20230205 18:13:51 @agent_ppo2.py:151][0m 22528 total steps have happened
[32m[20230205 18:13:51 @agent_ppo2.py:127][0m #------------------------ Iteration 11 --------------------------#
[32m[20230205 18:13:52 @agent_ppo2.py:133][0m Sampling time: 0.80 s by 1 slaves
[32m[20230205 18:13:52 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:52 @agent_ppo2.py:191][0m |           0.0001 |           1.5486 |           1.8343 |
[32m[20230205 18:13:52 @agent_ppo2.py:191][0m |          -0.0010 |           1.2246 |           1.8342 |
[32m[20230205 18:13:52 @agent_ppo2.py:191][0m |          -0.0023 |           1.1459 |           1.8341 |
[32m[20230205 18:13:52 @agent_ppo2.py:191][0m |          -0.0034 |           1.1084 |           1.8337 |
[32m[20230205 18:13:52 @agent_ppo2.py:191][0m |          -0.0039 |           1.0826 |           1.8332 |
[32m[20230205 18:13:52 @agent_ppo2.py:191][0m |          -0.0045 |           1.0673 |           1.8332 |
[32m[20230205 18:13:52 @agent_ppo2.py:191][0m |          -0.0050 |           1.0512 |           1.8334 |
[32m[20230205 18:13:52 @agent_ppo2.py:191][0m |          -0.0053 |           1.0458 |           1.8335 |
[32m[20230205 18:13:52 @agent_ppo2.py:191][0m |          -0.0057 |           1.0337 |           1.8334 |
[32m[20230205 18:13:53 @agent_ppo2.py:191][0m |          -0.0059 |           1.0234 |           1.8341 |
[32m[20230205 18:13:53 @agent_ppo2.py:136][0m Policy update time: 0.73 s
[32m[20230205 18:13:53 @agent_ppo2.py:144][0m Average TRAINING episode reward: -102.98
[32m[20230205 18:13:53 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -97.72
[32m[20230205 18:13:53 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -13.90
[32m[20230205 18:13:53 @agent_ppo2.py:149][0m Total time:       0.43 min
[32m[20230205 18:13:53 @agent_ppo2.py:151][0m 24576 total steps have happened
[32m[20230205 18:13:53 @agent_ppo2.py:127][0m #------------------------ Iteration 12 --------------------------#
[32m[20230205 18:13:54 @agent_ppo2.py:133][0m Sampling time: 0.77 s by 1 slaves
[32m[20230205 18:13:54 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:54 @agent_ppo2.py:191][0m |           0.0000 |           1.2806 |           1.8689 |
[32m[20230205 18:13:54 @agent_ppo2.py:191][0m |          -0.0007 |           1.1817 |           1.8697 |
[32m[20230205 18:13:54 @agent_ppo2.py:191][0m |          -0.0015 |           1.1490 |           1.8703 |
[32m[20230205 18:13:54 @agent_ppo2.py:191][0m |          -0.0024 |           1.1312 |           1.8708 |
[32m[20230205 18:13:55 @agent_ppo2.py:191][0m |          -0.0032 |           1.1188 |           1.8715 |
[32m[20230205 18:13:55 @agent_ppo2.py:191][0m |          -0.0042 |           1.1102 |           1.8717 |
[32m[20230205 18:13:55 @agent_ppo2.py:191][0m |          -0.0048 |           1.1035 |           1.8719 |
[32m[20230205 18:13:55 @agent_ppo2.py:191][0m |          -0.0053 |           1.0964 |           1.8725 |
[32m[20230205 18:13:55 @agent_ppo2.py:191][0m |          -0.0058 |           1.0909 |           1.8733 |
[32m[20230205 18:13:55 @agent_ppo2.py:191][0m |          -0.0064 |           1.0872 |           1.8739 |
[32m[20230205 18:13:55 @agent_ppo2.py:136][0m Policy update time: 0.94 s
[32m[20230205 18:13:56 @agent_ppo2.py:144][0m Average TRAINING episode reward: -103.63
[32m[20230205 18:13:56 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -100.14
[32m[20230205 18:13:56 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -21.49
[32m[20230205 18:13:56 @agent_ppo2.py:149][0m Total time:       0.48 min
[32m[20230205 18:13:56 @agent_ppo2.py:151][0m 26624 total steps have happened
[32m[20230205 18:13:56 @agent_ppo2.py:127][0m #------------------------ Iteration 13 --------------------------#
[32m[20230205 18:13:57 @agent_ppo2.py:133][0m Sampling time: 0.78 s by 1 slaves
[32m[20230205 18:13:57 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:57 @agent_ppo2.py:191][0m |          -0.0006 |           1.1512 |           1.8500 |
[32m[20230205 18:13:57 @agent_ppo2.py:191][0m |          -0.0034 |           1.0616 |           1.8509 |
[32m[20230205 18:13:57 @agent_ppo2.py:191][0m |          -0.0054 |           1.0427 |           1.8517 |
[32m[20230205 18:13:57 @agent_ppo2.py:191][0m |          -0.0068 |           1.0300 |           1.8525 |
[32m[20230205 18:13:57 @agent_ppo2.py:191][0m |          -0.0078 |           1.0179 |           1.8530 |
[32m[20230205 18:13:57 @agent_ppo2.py:191][0m |          -0.0084 |           1.0121 |           1.8537 |
[32m[20230205 18:13:57 @agent_ppo2.py:191][0m |          -0.0090 |           1.0045 |           1.8545 |
[32m[20230205 18:13:57 @agent_ppo2.py:191][0m |          -0.0095 |           0.9936 |           1.8551 |
[32m[20230205 18:13:57 @agent_ppo2.py:191][0m |          -0.0099 |           0.9867 |           1.8558 |
[32m[20230205 18:13:57 @agent_ppo2.py:191][0m |          -0.0103 |           0.9798 |           1.8565 |
[32m[20230205 18:13:57 @agent_ppo2.py:136][0m Policy update time: 0.73 s
[32m[20230205 18:13:58 @agent_ppo2.py:144][0m Average TRAINING episode reward: -101.13
[32m[20230205 18:13:58 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -96.53
[32m[20230205 18:13:58 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -22.46
[32m[20230205 18:13:58 @agent_ppo2.py:149][0m Total time:       0.51 min
[32m[20230205 18:13:58 @agent_ppo2.py:151][0m 28672 total steps have happened
[32m[20230205 18:13:58 @agent_ppo2.py:127][0m #------------------------ Iteration 14 --------------------------#
[32m[20230205 18:13:59 @agent_ppo2.py:133][0m Sampling time: 0.79 s by 1 slaves
[32m[20230205 18:13:59 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:13:59 @agent_ppo2.py:191][0m |           0.0001 |           1.0950 |           1.8991 |
[32m[20230205 18:13:59 @agent_ppo2.py:191][0m |          -0.0011 |           0.9681 |           1.8989 |
[32m[20230205 18:13:59 @agent_ppo2.py:191][0m |          -0.0024 |           0.9322 |           1.8990 |
[32m[20230205 18:13:59 @agent_ppo2.py:191][0m |          -0.0036 |           0.9125 |           1.8985 |
[32m[20230205 18:13:59 @agent_ppo2.py:191][0m |          -0.0046 |           0.8979 |           1.8986 |
[32m[20230205 18:13:59 @agent_ppo2.py:191][0m |          -0.0052 |           0.8899 |           1.8987 |
[32m[20230205 18:14:00 @agent_ppo2.py:191][0m |          -0.0056 |           0.8792 |           1.8993 |
[32m[20230205 18:14:00 @agent_ppo2.py:191][0m |          -0.0063 |           0.8710 |           1.8995 |
[32m[20230205 18:14:00 @agent_ppo2.py:191][0m |          -0.0067 |           0.8641 |           1.8999 |
[32m[20230205 18:14:00 @agent_ppo2.py:191][0m |          -0.0072 |           0.8574 |           1.9002 |
[32m[20230205 18:14:00 @agent_ppo2.py:136][0m Policy update time: 0.76 s
[32m[20230205 18:14:01 @agent_ppo2.py:144][0m Average TRAINING episode reward: -93.04
[32m[20230205 18:14:01 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -91.41
[32m[20230205 18:14:01 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -23.90
[32m[20230205 18:14:01 @agent_ppo2.py:149][0m Total time:       0.55 min
[32m[20230205 18:14:01 @agent_ppo2.py:151][0m 30720 total steps have happened
[32m[20230205 18:14:01 @agent_ppo2.py:127][0m #------------------------ Iteration 15 --------------------------#
[32m[20230205 18:14:01 @agent_ppo2.py:133][0m Sampling time: 0.77 s by 1 slaves
[32m[20230205 18:14:01 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:02 @agent_ppo2.py:191][0m |          -0.0009 |           1.0429 |           1.9207 |
[32m[20230205 18:14:02 @agent_ppo2.py:191][0m |          -0.0048 |           0.9805 |           1.9208 |
[32m[20230205 18:14:02 @agent_ppo2.py:191][0m |          -0.0081 |           0.9628 |           1.9202 |
[32m[20230205 18:14:02 @agent_ppo2.py:191][0m |          -0.0100 |           0.9516 |           1.9202 |
[32m[20230205 18:14:02 @agent_ppo2.py:191][0m |          -0.0110 |           0.9439 |           1.9207 |
[32m[20230205 18:14:02 @agent_ppo2.py:191][0m |          -0.0117 |           0.9365 |           1.9219 |
[32m[20230205 18:14:02 @agent_ppo2.py:191][0m |          -0.0123 |           0.9320 |           1.9223 |
[32m[20230205 18:14:02 @agent_ppo2.py:191][0m |          -0.0127 |           0.9237 |           1.9237 |
[32m[20230205 18:14:02 @agent_ppo2.py:191][0m |          -0.0132 |           0.9199 |           1.9245 |
[32m[20230205 18:14:02 @agent_ppo2.py:191][0m |          -0.0135 |           0.9145 |           1.9253 |
[32m[20230205 18:14:02 @agent_ppo2.py:136][0m Policy update time: 0.76 s
[32m[20230205 18:14:03 @agent_ppo2.py:144][0m Average TRAINING episode reward: -96.07
[32m[20230205 18:14:03 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -87.44
[32m[20230205 18:14:03 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -37.53
[32m[20230205 18:14:03 @agent_ppo2.py:149][0m Total time:       0.59 min
[32m[20230205 18:14:03 @agent_ppo2.py:151][0m 32768 total steps have happened
[32m[20230205 18:14:03 @agent_ppo2.py:127][0m #------------------------ Iteration 16 --------------------------#
[32m[20230205 18:14:04 @agent_ppo2.py:133][0m Sampling time: 0.83 s by 1 slaves
[32m[20230205 18:14:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:04 @agent_ppo2.py:191][0m |          -0.0000 |           1.0110 |           1.9789 |
[32m[20230205 18:14:04 @agent_ppo2.py:191][0m |          -0.0016 |           0.9278 |           1.9787 |
[32m[20230205 18:14:04 @agent_ppo2.py:191][0m |          -0.0031 |           0.9018 |           1.9784 |
[32m[20230205 18:14:04 @agent_ppo2.py:191][0m |          -0.0047 |           0.8836 |           1.9784 |
[32m[20230205 18:14:04 @agent_ppo2.py:191][0m |          -0.0057 |           0.8734 |           1.9784 |
[32m[20230205 18:14:04 @agent_ppo2.py:191][0m |          -0.0065 |           0.8643 |           1.9777 |
[32m[20230205 18:14:04 @agent_ppo2.py:191][0m |          -0.0072 |           0.8559 |           1.9780 |
[32m[20230205 18:14:04 @agent_ppo2.py:191][0m |          -0.0077 |           0.8482 |           1.9783 |
[32m[20230205 18:14:04 @agent_ppo2.py:191][0m |          -0.0083 |           0.8411 |           1.9785 |
[32m[20230205 18:14:05 @agent_ppo2.py:191][0m |          -0.0087 |           0.8373 |           1.9789 |
[32m[20230205 18:14:05 @agent_ppo2.py:136][0m Policy update time: 0.72 s
[32m[20230205 18:14:05 @agent_ppo2.py:144][0m Average TRAINING episode reward: -92.31
[32m[20230205 18:14:05 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -90.73
[32m[20230205 18:14:05 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -48.21
[32m[20230205 18:14:05 @agent_ppo2.py:149][0m Total time:       0.63 min
[32m[20230205 18:14:05 @agent_ppo2.py:151][0m 34816 total steps have happened
[32m[20230205 18:14:05 @agent_ppo2.py:127][0m #------------------------ Iteration 17 --------------------------#
[32m[20230205 18:14:06 @agent_ppo2.py:133][0m Sampling time: 0.77 s by 1 slaves
[32m[20230205 18:14:06 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:06 @agent_ppo2.py:191][0m |          -0.0001 |           0.8325 |           1.9702 |
[32m[20230205 18:14:06 @agent_ppo2.py:191][0m |          -0.0035 |           0.7596 |           1.9707 |
[32m[20230205 18:14:06 @agent_ppo2.py:191][0m |          -0.0053 |           0.7302 |           1.9700 |
[32m[20230205 18:14:06 @agent_ppo2.py:191][0m |          -0.0060 |           0.7113 |           1.9708 |
[32m[20230205 18:14:06 @agent_ppo2.py:191][0m |          -0.0067 |           0.6935 |           1.9703 |
[32m[20230205 18:14:06 @agent_ppo2.py:191][0m |          -0.0074 |           0.6774 |           1.9709 |
[32m[20230205 18:14:07 @agent_ppo2.py:191][0m |          -0.0080 |           0.6629 |           1.9716 |
[32m[20230205 18:14:07 @agent_ppo2.py:191][0m |          -0.0084 |           0.6544 |           1.9718 |
[32m[20230205 18:14:07 @agent_ppo2.py:191][0m |          -0.0089 |           0.6444 |           1.9716 |
[32m[20230205 18:14:07 @agent_ppo2.py:191][0m |          -0.0094 |           0.6354 |           1.9724 |
[32m[20230205 18:14:07 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:14:07 @agent_ppo2.py:144][0m Average TRAINING episode reward: -82.16
[32m[20230205 18:14:07 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -77.57
[32m[20230205 18:14:07 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -45.47
[32m[20230205 18:14:07 @agent_ppo2.py:149][0m Total time:       0.67 min
[32m[20230205 18:14:07 @agent_ppo2.py:151][0m 36864 total steps have happened
[32m[20230205 18:14:07 @agent_ppo2.py:127][0m #------------------------ Iteration 18 --------------------------#
[32m[20230205 18:14:08 @agent_ppo2.py:133][0m Sampling time: 0.77 s by 1 slaves
[32m[20230205 18:14:08 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:08 @agent_ppo2.py:191][0m |          -0.0003 |           0.9165 |           1.9604 |
[32m[20230205 18:14:08 @agent_ppo2.py:191][0m |          -0.0048 |           0.8469 |           1.9594 |
[32m[20230205 18:14:08 @agent_ppo2.py:191][0m |          -0.0073 |           0.8326 |           1.9572 |
[32m[20230205 18:14:09 @agent_ppo2.py:191][0m |          -0.0094 |           0.8227 |           1.9559 |
[32m[20230205 18:14:09 @agent_ppo2.py:191][0m |          -0.0106 |           0.8170 |           1.9549 |
[32m[20230205 18:14:09 @agent_ppo2.py:191][0m |          -0.0115 |           0.8075 |           1.9545 |
[32m[20230205 18:14:09 @agent_ppo2.py:191][0m |          -0.0123 |           0.8023 |           1.9545 |
[32m[20230205 18:14:09 @agent_ppo2.py:191][0m |          -0.0127 |           0.7973 |           1.9546 |
[32m[20230205 18:14:09 @agent_ppo2.py:191][0m |          -0.0133 |           0.7924 |           1.9545 |
[32m[20230205 18:14:09 @agent_ppo2.py:191][0m |          -0.0135 |           0.7870 |           1.9546 |
[32m[20230205 18:14:09 @agent_ppo2.py:136][0m Policy update time: 0.72 s
[32m[20230205 18:14:10 @agent_ppo2.py:144][0m Average TRAINING episode reward: -75.08
[32m[20230205 18:14:10 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -70.64
[32m[20230205 18:14:10 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -55.23
[32m[20230205 18:14:10 @agent_ppo2.py:149][0m Total time:       0.70 min
[32m[20230205 18:14:10 @agent_ppo2.py:151][0m 38912 total steps have happened
[32m[20230205 18:14:10 @agent_ppo2.py:127][0m #------------------------ Iteration 19 --------------------------#
[32m[20230205 18:14:10 @agent_ppo2.py:133][0m Sampling time: 0.79 s by 1 slaves
[32m[20230205 18:14:11 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:11 @agent_ppo2.py:191][0m |          -0.0025 |           7.7951 |           1.9052 |
[32m[20230205 18:14:11 @agent_ppo2.py:191][0m |          -0.0063 |           6.5410 |           1.9047 |
[32m[20230205 18:14:11 @agent_ppo2.py:191][0m |          -0.0067 |           6.0493 |           1.9034 |
[32m[20230205 18:14:11 @agent_ppo2.py:191][0m |          -0.0077 |           5.7107 |           1.9025 |
[32m[20230205 18:14:11 @agent_ppo2.py:191][0m |          -0.0096 |           5.3954 |           1.9010 |
[32m[20230205 18:14:11 @agent_ppo2.py:191][0m |          -0.0100 |           5.1449 |           1.9015 |
[32m[20230205 18:14:11 @agent_ppo2.py:191][0m |          -0.0107 |           4.9237 |           1.8998 |
[32m[20230205 18:14:11 @agent_ppo2.py:191][0m |          -0.0111 |           4.7285 |           1.9000 |
[32m[20230205 18:14:11 @agent_ppo2.py:191][0m |          -0.0121 |           4.5680 |           1.8991 |
[32m[20230205 18:14:11 @agent_ppo2.py:191][0m |          -0.0105 |           4.4125 |           1.8987 |
[32m[20230205 18:14:11 @agent_ppo2.py:136][0m Policy update time: 0.75 s
[32m[20230205 18:14:12 @agent_ppo2.py:144][0m Average TRAINING episode reward: -80.24
[32m[20230205 18:14:12 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -66.71
[32m[20230205 18:14:12 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -63.31
[32m[20230205 18:14:12 @agent_ppo2.py:149][0m Total time:       0.74 min
[32m[20230205 18:14:12 @agent_ppo2.py:151][0m 40960 total steps have happened
[32m[20230205 18:14:12 @agent_ppo2.py:127][0m #------------------------ Iteration 20 --------------------------#
[32m[20230205 18:14:13 @agent_ppo2.py:133][0m Sampling time: 0.79 s by 1 slaves
[32m[20230205 18:14:13 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:13 @agent_ppo2.py:191][0m |           0.0014 |          14.8414 |           1.9193 |
[32m[20230205 18:14:13 @agent_ppo2.py:191][0m |          -0.0031 |          10.7175 |           1.9185 |
[32m[20230205 18:14:13 @agent_ppo2.py:191][0m |          -0.0046 |           9.8216 |           1.9182 |
[32m[20230205 18:14:13 @agent_ppo2.py:191][0m |          -0.0053 |           9.0673 |           1.9193 |
[32m[20230205 18:14:13 @agent_ppo2.py:191][0m |          -0.0057 |           8.4930 |           1.9190 |
[32m[20230205 18:14:13 @agent_ppo2.py:191][0m |          -0.0059 |           7.9536 |           1.9203 |
[32m[20230205 18:14:13 @agent_ppo2.py:191][0m |          -0.0060 |           7.6017 |           1.9215 |
[32m[20230205 18:14:13 @agent_ppo2.py:191][0m |          -0.0076 |           7.2075 |           1.9200 |
[32m[20230205 18:14:13 @agent_ppo2.py:191][0m |          -0.0085 |           6.8583 |           1.9210 |
[32m[20230205 18:14:13 @agent_ppo2.py:191][0m |          -0.0090 |           6.5667 |           1.9219 |
[32m[20230205 18:14:13 @agent_ppo2.py:136][0m Policy update time: 0.74 s
[32m[20230205 18:14:14 @agent_ppo2.py:144][0m Average TRAINING episode reward: -87.27
[32m[20230205 18:14:14 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -65.25
[32m[20230205 18:14:14 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -100.40
[32m[20230205 18:14:14 @agent_ppo2.py:149][0m Total time:       0.78 min
[32m[20230205 18:14:14 @agent_ppo2.py:151][0m 43008 total steps have happened
[32m[20230205 18:14:14 @agent_ppo2.py:127][0m #------------------------ Iteration 21 --------------------------#
[32m[20230205 18:14:15 @agent_ppo2.py:133][0m Sampling time: 0.76 s by 1 slaves
[32m[20230205 18:14:15 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:15 @agent_ppo2.py:191][0m |          -0.0004 |           1.0802 |           1.9739 |
[32m[20230205 18:14:15 @agent_ppo2.py:191][0m |          -0.0030 |           0.8949 |           1.9733 |
[32m[20230205 18:14:15 @agent_ppo2.py:191][0m |          -0.0042 |           0.8498 |           1.9729 |
[32m[20230205 18:14:15 @agent_ppo2.py:191][0m |          -0.0050 |           0.8309 |           1.9734 |
[32m[20230205 18:14:15 @agent_ppo2.py:191][0m |          -0.0056 |           0.8166 |           1.9744 |
[32m[20230205 18:14:15 @agent_ppo2.py:191][0m |          -0.0060 |           0.8055 |           1.9737 |
[32m[20230205 18:14:15 @agent_ppo2.py:191][0m |          -0.0067 |           0.8010 |           1.9750 |
[32m[20230205 18:14:15 @agent_ppo2.py:191][0m |          -0.0071 |           0.8019 |           1.9750 |
[32m[20230205 18:14:15 @agent_ppo2.py:191][0m |          -0.0074 |           0.7910 |           1.9754 |
[32m[20230205 18:14:15 @agent_ppo2.py:191][0m |          -0.0075 |           0.7873 |           1.9767 |
[32m[20230205 18:14:15 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:14:16 @agent_ppo2.py:144][0m Average TRAINING episode reward: -72.92
[32m[20230205 18:14:16 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -72.11
[32m[20230205 18:14:16 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -68.99
[32m[20230205 18:14:16 @agent_ppo2.py:149][0m Total time:       0.81 min
[32m[20230205 18:14:16 @agent_ppo2.py:151][0m 45056 total steps have happened
[32m[20230205 18:14:16 @agent_ppo2.py:127][0m #------------------------ Iteration 22 --------------------------#
[32m[20230205 18:14:17 @agent_ppo2.py:133][0m Sampling time: 0.75 s by 1 slaves
[32m[20230205 18:14:17 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:17 @agent_ppo2.py:191][0m |          -0.0001 |           0.9818 |           2.0353 |
[32m[20230205 18:14:17 @agent_ppo2.py:191][0m |          -0.0025 |           0.8533 |           2.0344 |
[32m[20230205 18:14:17 @agent_ppo2.py:191][0m |          -0.0049 |           0.8249 |           2.0325 |
[32m[20230205 18:14:17 @agent_ppo2.py:191][0m |          -0.0063 |           0.7894 |           2.0314 |
[32m[20230205 18:14:17 @agent_ppo2.py:191][0m |          -0.0074 |           0.7618 |           2.0298 |
[32m[20230205 18:14:17 @agent_ppo2.py:191][0m |          -0.0076 |           0.7470 |           2.0295 |
[32m[20230205 18:14:17 @agent_ppo2.py:191][0m |          -0.0085 |           0.7330 |           2.0294 |
[32m[20230205 18:14:17 @agent_ppo2.py:191][0m |          -0.0091 |           0.7228 |           2.0291 |
[32m[20230205 18:14:17 @agent_ppo2.py:191][0m |          -0.0095 |           0.7141 |           2.0288 |
[32m[20230205 18:14:18 @agent_ppo2.py:191][0m |          -0.0099 |           0.7027 |           2.0285 |
[32m[20230205 18:14:18 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:14:18 @agent_ppo2.py:144][0m Average TRAINING episode reward: -68.47
[32m[20230205 18:14:18 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -65.20
[32m[20230205 18:14:18 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -63.09
[32m[20230205 18:14:18 @agent_ppo2.py:149][0m Total time:       0.85 min
[32m[20230205 18:14:18 @agent_ppo2.py:151][0m 47104 total steps have happened
[32m[20230205 18:14:18 @agent_ppo2.py:127][0m #------------------------ Iteration 23 --------------------------#
[32m[20230205 18:14:19 @agent_ppo2.py:133][0m Sampling time: 0.77 s by 1 slaves
[32m[20230205 18:14:19 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:19 @agent_ppo2.py:191][0m |          -0.0003 |           6.2494 |           1.9878 |
[32m[20230205 18:14:19 @agent_ppo2.py:191][0m |          -0.0028 |           4.0107 |           1.9847 |
[32m[20230205 18:14:19 @agent_ppo2.py:191][0m |          -0.0054 |           3.5131 |           1.9829 |
[32m[20230205 18:14:19 @agent_ppo2.py:191][0m |          -0.0053 |           3.3163 |           1.9805 |
[32m[20230205 18:14:19 @agent_ppo2.py:191][0m |          -0.0070 |           3.2005 |           1.9798 |
[32m[20230205 18:14:20 @agent_ppo2.py:191][0m |          -0.0070 |           3.0664 |           1.9778 |
[32m[20230205 18:14:20 @agent_ppo2.py:191][0m |          -0.0079 |           2.9392 |           1.9787 |
[32m[20230205 18:14:20 @agent_ppo2.py:191][0m |          -0.0092 |           2.8574 |           1.9766 |
[32m[20230205 18:14:20 @agent_ppo2.py:191][0m |          -0.0089 |           2.7419 |           1.9774 |
[32m[20230205 18:14:20 @agent_ppo2.py:191][0m |          -0.0089 |           2.6788 |           1.9770 |
[32m[20230205 18:14:20 @agent_ppo2.py:136][0m Policy update time: 0.72 s
[32m[20230205 18:14:21 @agent_ppo2.py:144][0m Average TRAINING episode reward: -80.97
[32m[20230205 18:14:21 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -67.54
[32m[20230205 18:14:21 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -70.13
[32m[20230205 18:14:21 @agent_ppo2.py:149][0m Total time:       0.89 min
[32m[20230205 18:14:21 @agent_ppo2.py:151][0m 49152 total steps have happened
[32m[20230205 18:14:21 @agent_ppo2.py:127][0m #------------------------ Iteration 24 --------------------------#
[32m[20230205 18:14:21 @agent_ppo2.py:133][0m Sampling time: 0.81 s by 1 slaves
[32m[20230205 18:14:21 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:21 @agent_ppo2.py:191][0m |          -0.0001 |           1.0528 |           2.0054 |
[32m[20230205 18:14:22 @agent_ppo2.py:191][0m |          -0.0032 |           0.9459 |           2.0064 |
[32m[20230205 18:14:22 @agent_ppo2.py:191][0m |          -0.0048 |           0.8682 |           2.0048 |
[32m[20230205 18:14:22 @agent_ppo2.py:191][0m |          -0.0063 |           0.8343 |           2.0050 |
[32m[20230205 18:14:22 @agent_ppo2.py:191][0m |          -0.0070 |           0.8167 |           2.0062 |
[32m[20230205 18:14:22 @agent_ppo2.py:191][0m |          -0.0079 |           0.8078 |           2.0064 |
[32m[20230205 18:14:22 @agent_ppo2.py:191][0m |          -0.0086 |           0.7922 |           2.0066 |
[32m[20230205 18:14:22 @agent_ppo2.py:191][0m |          -0.0085 |           0.7910 |           2.0071 |
[32m[20230205 18:14:22 @agent_ppo2.py:191][0m |          -0.0089 |           0.7779 |           2.0074 |
[32m[20230205 18:14:22 @agent_ppo2.py:191][0m |          -0.0095 |           0.7771 |           2.0081 |
[32m[20230205 18:14:22 @agent_ppo2.py:136][0m Policy update time: 0.74 s
[32m[20230205 18:14:23 @agent_ppo2.py:144][0m Average TRAINING episode reward: -61.30
[32m[20230205 18:14:23 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -55.51
[32m[20230205 18:14:23 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -75.01
[32m[20230205 18:14:23 @agent_ppo2.py:149][0m Total time:       0.92 min
[32m[20230205 18:14:23 @agent_ppo2.py:151][0m 51200 total steps have happened
[32m[20230205 18:14:23 @agent_ppo2.py:127][0m #------------------------ Iteration 25 --------------------------#
[32m[20230205 18:14:24 @agent_ppo2.py:133][0m Sampling time: 0.76 s by 1 slaves
[32m[20230205 18:14:24 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:24 @agent_ppo2.py:191][0m |          -0.0036 |           6.1603 |           1.9937 |
[32m[20230205 18:14:24 @agent_ppo2.py:191][0m |          -0.0071 |           3.6528 |           1.9909 |
[32m[20230205 18:14:24 @agent_ppo2.py:191][0m |          -0.0066 |           3.2837 |           1.9896 |
[32m[20230205 18:14:24 @agent_ppo2.py:191][0m |          -0.0085 |           3.0660 |           1.9887 |
[32m[20230205 18:14:24 @agent_ppo2.py:191][0m |           0.0117 |           3.2856 |           1.9875 |
[32m[20230205 18:14:24 @agent_ppo2.py:191][0m |          -0.0106 |           2.9900 |           1.9880 |
[32m[20230205 18:14:24 @agent_ppo2.py:191][0m |          -0.0096 |           2.6721 |           1.9874 |
[32m[20230205 18:14:24 @agent_ppo2.py:191][0m |          -0.0011 |           2.5707 |           1.9865 |
[32m[20230205 18:14:24 @agent_ppo2.py:191][0m |          -0.0105 |           2.4945 |           1.9861 |
[32m[20230205 18:14:24 @agent_ppo2.py:191][0m |           0.0031 |           2.4159 |           1.9857 |
[32m[20230205 18:14:24 @agent_ppo2.py:136][0m Policy update time: 0.63 s
[32m[20230205 18:14:25 @agent_ppo2.py:144][0m Average TRAINING episode reward: -98.65
[32m[20230205 18:14:25 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -58.83
[32m[20230205 18:14:25 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -60.56
[32m[20230205 18:14:25 @agent_ppo2.py:149][0m Total time:       0.96 min
[32m[20230205 18:14:25 @agent_ppo2.py:151][0m 53248 total steps have happened
[32m[20230205 18:14:25 @agent_ppo2.py:127][0m #------------------------ Iteration 26 --------------------------#
[32m[20230205 18:14:26 @agent_ppo2.py:133][0m Sampling time: 0.83 s by 1 slaves
[32m[20230205 18:14:26 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:26 @agent_ppo2.py:191][0m |           0.0003 |           1.6717 |           2.0069 |
[32m[20230205 18:14:26 @agent_ppo2.py:191][0m |          -0.0021 |           1.3686 |           2.0076 |
[32m[20230205 18:14:26 @agent_ppo2.py:191][0m |          -0.0039 |           1.3264 |           2.0054 |
[32m[20230205 18:14:26 @agent_ppo2.py:191][0m |          -0.0051 |           1.2773 |           2.0054 |
[32m[20230205 18:14:26 @agent_ppo2.py:191][0m |          -0.0063 |           1.2426 |           2.0045 |
[32m[20230205 18:14:26 @agent_ppo2.py:191][0m |          -0.0071 |           1.2175 |           2.0044 |
[32m[20230205 18:14:26 @agent_ppo2.py:191][0m |          -0.0077 |           1.1971 |           2.0041 |
[32m[20230205 18:14:26 @agent_ppo2.py:191][0m |          -0.0083 |           1.1808 |           2.0046 |
[32m[20230205 18:14:26 @agent_ppo2.py:191][0m |          -0.0086 |           1.1544 |           2.0043 |
[32m[20230205 18:14:27 @agent_ppo2.py:191][0m |          -0.0092 |           1.1400 |           2.0046 |
[32m[20230205 18:14:27 @agent_ppo2.py:136][0m Policy update time: 0.80 s
[32m[20230205 18:14:27 @agent_ppo2.py:144][0m Average TRAINING episode reward: -60.14
[32m[20230205 18:14:27 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -56.72
[32m[20230205 18:14:27 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -60.74
[32m[20230205 18:14:27 @agent_ppo2.py:149][0m Total time:       1.00 min
[32m[20230205 18:14:27 @agent_ppo2.py:151][0m 55296 total steps have happened
[32m[20230205 18:14:27 @agent_ppo2.py:127][0m #------------------------ Iteration 27 --------------------------#
[32m[20230205 18:14:28 @agent_ppo2.py:133][0m Sampling time: 0.78 s by 1 slaves
[32m[20230205 18:14:28 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:28 @agent_ppo2.py:191][0m |          -0.0014 |           0.8929 |           1.9621 |
[32m[20230205 18:14:28 @agent_ppo2.py:191][0m |          -0.0078 |           0.8187 |           1.9611 |
[32m[20230205 18:14:28 @agent_ppo2.py:191][0m |          -0.0098 |           0.8020 |           1.9600 |
[32m[20230205 18:14:28 @agent_ppo2.py:191][0m |          -0.0108 |           0.7858 |           1.9600 |
[32m[20230205 18:14:29 @agent_ppo2.py:191][0m |          -0.0119 |           0.7764 |           1.9605 |
[32m[20230205 18:14:29 @agent_ppo2.py:191][0m |          -0.0124 |           0.7634 |           1.9616 |
[32m[20230205 18:14:29 @agent_ppo2.py:191][0m |          -0.0132 |           0.7569 |           1.9616 |
[32m[20230205 18:14:29 @agent_ppo2.py:191][0m |          -0.0137 |           0.7490 |           1.9621 |
[32m[20230205 18:14:29 @agent_ppo2.py:191][0m |          -0.0141 |           0.7443 |           1.9633 |
[32m[20230205 18:14:29 @agent_ppo2.py:191][0m |          -0.0145 |           0.7406 |           1.9634 |
[32m[20230205 18:14:29 @agent_ppo2.py:136][0m Policy update time: 0.91 s
[32m[20230205 18:14:30 @agent_ppo2.py:144][0m Average TRAINING episode reward: -50.43
[32m[20230205 18:14:30 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -50.40
[32m[20230205 18:14:30 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -32.59
[32m[20230205 18:14:30 @agent_ppo2.py:149][0m Total time:       1.04 min
[32m[20230205 18:14:30 @agent_ppo2.py:151][0m 57344 total steps have happened
[32m[20230205 18:14:30 @agent_ppo2.py:127][0m #------------------------ Iteration 28 --------------------------#
[32m[20230205 18:14:30 @agent_ppo2.py:133][0m Sampling time: 0.77 s by 1 slaves
[32m[20230205 18:14:31 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:31 @agent_ppo2.py:191][0m |          -0.0013 |           1.0504 |           2.0730 |
[32m[20230205 18:14:31 @agent_ppo2.py:191][0m |          -0.0049 |           0.9071 |           2.0679 |
[32m[20230205 18:14:31 @agent_ppo2.py:191][0m |          -0.0062 |           0.8685 |           2.0677 |
[32m[20230205 18:14:31 @agent_ppo2.py:191][0m |          -0.0075 |           0.8379 |           2.0655 |
[32m[20230205 18:14:31 @agent_ppo2.py:191][0m |          -0.0082 |           0.8254 |           2.0653 |
[32m[20230205 18:14:31 @agent_ppo2.py:191][0m |          -0.0089 |           0.8048 |           2.0664 |
[32m[20230205 18:14:31 @agent_ppo2.py:191][0m |          -0.0096 |           0.7883 |           2.0663 |
[32m[20230205 18:14:31 @agent_ppo2.py:191][0m |          -0.0100 |           0.7858 |           2.0670 |
[32m[20230205 18:14:31 @agent_ppo2.py:191][0m |          -0.0102 |           0.7706 |           2.0666 |
[32m[20230205 18:14:31 @agent_ppo2.py:191][0m |          -0.0107 |           0.7655 |           2.0676 |
[32m[20230205 18:14:31 @agent_ppo2.py:136][0m Policy update time: 0.75 s
[32m[20230205 18:14:32 @agent_ppo2.py:144][0m Average TRAINING episode reward: -47.22
[32m[20230205 18:14:32 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -30.47
[32m[20230205 18:14:32 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -16.61
[32m[20230205 18:14:32 @agent_ppo2.py:149][0m Total time:       1.07 min
[32m[20230205 18:14:32 @agent_ppo2.py:151][0m 59392 total steps have happened
[32m[20230205 18:14:32 @agent_ppo2.py:127][0m #------------------------ Iteration 29 --------------------------#
[32m[20230205 18:14:32 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:14:32 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:32 @agent_ppo2.py:191][0m |          -0.0005 |           5.6345 |           2.0159 |
[32m[20230205 18:14:32 @agent_ppo2.py:191][0m |           0.0131 |           4.3567 |           2.0154 |
[32m[20230205 18:14:33 @agent_ppo2.py:191][0m |          -0.0067 |           3.2294 |           2.0154 |
[32m[20230205 18:14:33 @agent_ppo2.py:191][0m |          -0.0086 |           2.8460 |           2.0145 |
[32m[20230205 18:14:33 @agent_ppo2.py:191][0m |          -0.0058 |           2.6327 |           2.0133 |
[32m[20230205 18:14:33 @agent_ppo2.py:191][0m |          -0.0116 |           2.4949 |           2.0134 |
[32m[20230205 18:14:33 @agent_ppo2.py:191][0m |          -0.0085 |           2.3541 |           2.0129 |
[32m[20230205 18:14:33 @agent_ppo2.py:191][0m |          -0.0077 |           2.2877 |           2.0130 |
[32m[20230205 18:14:33 @agent_ppo2.py:191][0m |          -0.0129 |           2.1510 |           2.0121 |
[32m[20230205 18:14:33 @agent_ppo2.py:191][0m |          -0.0122 |           2.0803 |           2.0114 |
[32m[20230205 18:14:33 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:14:34 @agent_ppo2.py:144][0m Average TRAINING episode reward: -79.20
[32m[20230205 18:14:34 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -33.39
[32m[20230205 18:14:34 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 79.91
[32m[20230205 18:14:34 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 79.91
[32m[20230205 18:14:34 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 79.91
[32m[20230205 18:14:34 @agent_ppo2.py:149][0m Total time:       1.10 min
[32m[20230205 18:14:34 @agent_ppo2.py:151][0m 61440 total steps have happened
[32m[20230205 18:14:34 @agent_ppo2.py:127][0m #------------------------ Iteration 30 --------------------------#
[32m[20230205 18:14:34 @agent_ppo2.py:133][0m Sampling time: 0.74 s by 1 slaves
[32m[20230205 18:14:34 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:35 @agent_ppo2.py:191][0m |          -0.0007 |           1.0364 |           2.0428 |
[32m[20230205 18:14:35 @agent_ppo2.py:191][0m |          -0.0051 |           0.8866 |           2.0428 |
[32m[20230205 18:14:35 @agent_ppo2.py:191][0m |          -0.0082 |           0.8511 |           2.0413 |
[32m[20230205 18:14:35 @agent_ppo2.py:191][0m |          -0.0102 |           0.8347 |           2.0417 |
[32m[20230205 18:14:35 @agent_ppo2.py:191][0m |          -0.0119 |           0.8198 |           2.0415 |
[32m[20230205 18:14:35 @agent_ppo2.py:191][0m |          -0.0130 |           0.8091 |           2.0420 |
[32m[20230205 18:14:35 @agent_ppo2.py:191][0m |          -0.0139 |           0.8027 |           2.0421 |
[32m[20230205 18:14:35 @agent_ppo2.py:191][0m |          -0.0146 |           0.7976 |           2.0423 |
[32m[20230205 18:14:35 @agent_ppo2.py:191][0m |          -0.0153 |           0.7906 |           2.0422 |
[32m[20230205 18:14:35 @agent_ppo2.py:191][0m |          -0.0156 |           0.7844 |           2.0422 |
[32m[20230205 18:14:35 @agent_ppo2.py:136][0m Policy update time: 0.78 s
[32m[20230205 18:14:36 @agent_ppo2.py:144][0m Average TRAINING episode reward: -35.57
[32m[20230205 18:14:36 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -35.18
[32m[20230205 18:14:36 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 90.79
[32m[20230205 18:14:36 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 90.79
[32m[20230205 18:14:36 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 90.79
[32m[20230205 18:14:36 @agent_ppo2.py:149][0m Total time:       1.14 min
[32m[20230205 18:14:36 @agent_ppo2.py:151][0m 63488 total steps have happened
[32m[20230205 18:14:36 @agent_ppo2.py:127][0m #------------------------ Iteration 31 --------------------------#
[32m[20230205 18:14:37 @agent_ppo2.py:133][0m Sampling time: 0.76 s by 1 slaves
[32m[20230205 18:14:37 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:37 @agent_ppo2.py:191][0m |          -0.0000 |           6.2641 |           2.0271 |
[32m[20230205 18:14:37 @agent_ppo2.py:191][0m |          -0.0053 |           4.3545 |           2.0252 |
[32m[20230205 18:14:37 @agent_ppo2.py:191][0m |          -0.0067 |           4.0782 |           2.0234 |
[32m[20230205 18:14:37 @agent_ppo2.py:191][0m |          -0.0079 |           3.7906 |           2.0220 |
[32m[20230205 18:14:37 @agent_ppo2.py:191][0m |          -0.0082 |           3.5826 |           2.0211 |
[32m[20230205 18:14:37 @agent_ppo2.py:191][0m |          -0.0093 |           3.5029 |           2.0203 |
[32m[20230205 18:14:37 @agent_ppo2.py:191][0m |          -0.0079 |           3.3256 |           2.0205 |
[32m[20230205 18:14:37 @agent_ppo2.py:191][0m |          -0.0098 |           3.2253 |           2.0198 |
[32m[20230205 18:14:37 @agent_ppo2.py:191][0m |          -0.0096 |           3.1757 |           2.0206 |
[32m[20230205 18:14:37 @agent_ppo2.py:191][0m |          -0.0108 |           2.9569 |           2.0211 |
[32m[20230205 18:14:37 @agent_ppo2.py:136][0m Policy update time: 0.73 s
[32m[20230205 18:14:38 @agent_ppo2.py:144][0m Average TRAINING episode reward: -53.68
[32m[20230205 18:14:38 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -16.29
[32m[20230205 18:14:38 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 89.61
[32m[20230205 18:14:38 @agent_ppo2.py:149][0m Total time:       1.18 min
[32m[20230205 18:14:38 @agent_ppo2.py:151][0m 65536 total steps have happened
[32m[20230205 18:14:38 @agent_ppo2.py:127][0m #------------------------ Iteration 32 --------------------------#
[32m[20230205 18:14:39 @agent_ppo2.py:133][0m Sampling time: 0.84 s by 1 slaves
[32m[20230205 18:14:39 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:39 @agent_ppo2.py:191][0m |          -0.0010 |           0.9768 |           1.9923 |
[32m[20230205 18:14:39 @agent_ppo2.py:191][0m |          -0.0059 |           0.7745 |           1.9922 |
[32m[20230205 18:14:39 @agent_ppo2.py:191][0m |          -0.0086 |           0.7208 |           1.9924 |
[32m[20230205 18:14:39 @agent_ppo2.py:191][0m |          -0.0101 |           0.7222 |           1.9924 |
[32m[20230205 18:14:40 @agent_ppo2.py:191][0m |          -0.0115 |           0.6777 |           1.9928 |
[32m[20230205 18:14:40 @agent_ppo2.py:191][0m |          -0.0119 |           0.6561 |           1.9944 |
[32m[20230205 18:14:40 @agent_ppo2.py:191][0m |          -0.0124 |           0.6355 |           1.9947 |
[32m[20230205 18:14:40 @agent_ppo2.py:191][0m |          -0.0132 |           0.6224 |           1.9963 |
[32m[20230205 18:14:40 @agent_ppo2.py:191][0m |          -0.0136 |           0.6149 |           1.9977 |
[32m[20230205 18:14:40 @agent_ppo2.py:191][0m |          -0.0140 |           0.6060 |           1.9984 |
[32m[20230205 18:14:40 @agent_ppo2.py:136][0m Policy update time: 0.82 s
[32m[20230205 18:14:41 @agent_ppo2.py:144][0m Average TRAINING episode reward: -27.94
[32m[20230205 18:14:41 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -21.68
[32m[20230205 18:14:41 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 89.99
[32m[20230205 18:14:41 @agent_ppo2.py:149][0m Total time:       1.22 min
[32m[20230205 18:14:41 @agent_ppo2.py:151][0m 67584 total steps have happened
[32m[20230205 18:14:41 @agent_ppo2.py:127][0m #------------------------ Iteration 33 --------------------------#
[32m[20230205 18:14:41 @agent_ppo2.py:133][0m Sampling time: 0.79 s by 1 slaves
[32m[20230205 18:14:41 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:42 @agent_ppo2.py:191][0m |          -0.0005 |           0.8255 |           2.1109 |
[32m[20230205 18:14:42 @agent_ppo2.py:191][0m |          -0.0046 |           0.7304 |           2.1104 |
[32m[20230205 18:14:42 @agent_ppo2.py:191][0m |          -0.0070 |           0.7102 |           2.1093 |
[32m[20230205 18:14:42 @agent_ppo2.py:191][0m |          -0.0081 |           0.6965 |           2.1092 |
[32m[20230205 18:14:42 @agent_ppo2.py:191][0m |          -0.0088 |           0.6858 |           2.1097 |
[32m[20230205 18:14:42 @agent_ppo2.py:191][0m |          -0.0097 |           0.6796 |           2.1094 |
[32m[20230205 18:14:42 @agent_ppo2.py:191][0m |          -0.0099 |           0.6707 |           2.1095 |
[32m[20230205 18:14:42 @agent_ppo2.py:191][0m |          -0.0107 |           0.6624 |           2.1098 |
[32m[20230205 18:14:42 @agent_ppo2.py:191][0m |          -0.0109 |           0.6607 |           2.1105 |
[32m[20230205 18:14:42 @agent_ppo2.py:191][0m |          -0.0113 |           0.6534 |           2.1108 |
[32m[20230205 18:14:42 @agent_ppo2.py:136][0m Policy update time: 0.84 s
[32m[20230205 18:14:43 @agent_ppo2.py:144][0m Average TRAINING episode reward: -22.98
[32m[20230205 18:14:43 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -20.94
[32m[20230205 18:14:43 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 103.70
[32m[20230205 18:14:43 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 103.70
[32m[20230205 18:14:43 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 103.70
[32m[20230205 18:14:43 @agent_ppo2.py:149][0m Total time:       1.26 min
[32m[20230205 18:14:43 @agent_ppo2.py:151][0m 69632 total steps have happened
[32m[20230205 18:14:43 @agent_ppo2.py:127][0m #------------------------ Iteration 34 --------------------------#
[32m[20230205 18:14:44 @agent_ppo2.py:133][0m Sampling time: 0.81 s by 1 slaves
[32m[20230205 18:14:44 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:44 @agent_ppo2.py:191][0m |          -0.0007 |           0.6739 |           2.0739 |
[32m[20230205 18:14:44 @agent_ppo2.py:191][0m |          -0.0057 |           0.5915 |           2.0737 |
[32m[20230205 18:14:44 @agent_ppo2.py:191][0m |          -0.0085 |           0.5766 |           2.0730 |
[32m[20230205 18:14:44 @agent_ppo2.py:191][0m |          -0.0101 |           0.5641 |           2.0733 |
[32m[20230205 18:14:44 @agent_ppo2.py:191][0m |          -0.0111 |           0.5568 |           2.0735 |
[32m[20230205 18:14:44 @agent_ppo2.py:191][0m |          -0.0122 |           0.5494 |           2.0742 |
[32m[20230205 18:14:44 @agent_ppo2.py:191][0m |          -0.0130 |           0.5458 |           2.0745 |
[32m[20230205 18:14:44 @agent_ppo2.py:191][0m |          -0.0135 |           0.5389 |           2.0754 |
[32m[20230205 18:14:45 @agent_ppo2.py:191][0m |          -0.0139 |           0.5351 |           2.0758 |
[32m[20230205 18:14:45 @agent_ppo2.py:191][0m |          -0.0145 |           0.5304 |           2.0768 |
[32m[20230205 18:14:45 @agent_ppo2.py:136][0m Policy update time: 0.81 s
[32m[20230205 18:14:45 @agent_ppo2.py:144][0m Average TRAINING episode reward: -15.55
[32m[20230205 18:14:45 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -12.62
[32m[20230205 18:14:45 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 118.43
[32m[20230205 18:14:45 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 118.43
[32m[20230205 18:14:45 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 118.43
[32m[20230205 18:14:45 @agent_ppo2.py:149][0m Total time:       1.30 min
[32m[20230205 18:14:45 @agent_ppo2.py:151][0m 71680 total steps have happened
[32m[20230205 18:14:45 @agent_ppo2.py:127][0m #------------------------ Iteration 35 --------------------------#
[32m[20230205 18:14:46 @agent_ppo2.py:133][0m Sampling time: 0.75 s by 1 slaves
[32m[20230205 18:14:46 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:46 @agent_ppo2.py:191][0m |          -0.0004 |           0.6822 |           2.1180 |
[32m[20230205 18:14:46 @agent_ppo2.py:191][0m |          -0.0039 |           0.6362 |           2.1173 |
[32m[20230205 18:14:46 @agent_ppo2.py:191][0m |          -0.0068 |           0.6190 |           2.1167 |
[32m[20230205 18:14:46 @agent_ppo2.py:191][0m |          -0.0082 |           0.6079 |           2.1151 |
[32m[20230205 18:14:47 @agent_ppo2.py:191][0m |          -0.0094 |           0.5978 |           2.1140 |
[32m[20230205 18:14:47 @agent_ppo2.py:191][0m |          -0.0105 |           0.5924 |           2.1134 |
[32m[20230205 18:14:47 @agent_ppo2.py:191][0m |          -0.0110 |           0.5844 |           2.1127 |
[32m[20230205 18:14:47 @agent_ppo2.py:191][0m |          -0.0115 |           0.5780 |           2.1133 |
[32m[20230205 18:14:47 @agent_ppo2.py:191][0m |          -0.0122 |           0.5731 |           2.1139 |
[32m[20230205 18:14:47 @agent_ppo2.py:191][0m |          -0.0124 |           0.5707 |           2.1149 |
[32m[20230205 18:14:47 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:14:48 @agent_ppo2.py:144][0m Average TRAINING episode reward: -17.36
[32m[20230205 18:14:48 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -13.68
[32m[20230205 18:14:48 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 108.99
[32m[20230205 18:14:48 @agent_ppo2.py:149][0m Total time:       1.34 min
[32m[20230205 18:14:48 @agent_ppo2.py:151][0m 73728 total steps have happened
[32m[20230205 18:14:48 @agent_ppo2.py:127][0m #------------------------ Iteration 36 --------------------------#
[32m[20230205 18:14:48 @agent_ppo2.py:133][0m Sampling time: 0.78 s by 1 slaves
[32m[20230205 18:14:48 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:48 @agent_ppo2.py:191][0m |           0.0040 |           4.4021 |           2.0696 |
[32m[20230205 18:14:49 @agent_ppo2.py:191][0m |          -0.0050 |           2.6170 |           2.0707 |
[32m[20230205 18:14:49 @agent_ppo2.py:191][0m |          -0.0067 |           2.2956 |           2.0705 |
[32m[20230205 18:14:49 @agent_ppo2.py:191][0m |           0.0004 |           2.0646 |           2.0696 |
[32m[20230205 18:14:49 @agent_ppo2.py:191][0m |          -0.0076 |           1.9480 |           2.0692 |
[32m[20230205 18:14:49 @agent_ppo2.py:191][0m |          -0.0061 |           1.8717 |           2.0684 |
[32m[20230205 18:14:49 @agent_ppo2.py:191][0m |          -0.0089 |           1.7871 |           2.0686 |
[32m[20230205 18:14:49 @agent_ppo2.py:191][0m |          -0.0104 |           1.7312 |           2.0677 |
[32m[20230205 18:14:49 @agent_ppo2.py:191][0m |          -0.0118 |           1.6819 |           2.0679 |
[32m[20230205 18:14:49 @agent_ppo2.py:191][0m |          -0.0113 |           1.6653 |           2.0673 |
[32m[20230205 18:14:49 @agent_ppo2.py:136][0m Policy update time: 0.80 s
[32m[20230205 18:14:50 @agent_ppo2.py:144][0m Average TRAINING episode reward: -39.77
[32m[20230205 18:14:50 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -3.35
[32m[20230205 18:14:50 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 113.32
[32m[20230205 18:14:50 @agent_ppo2.py:149][0m Total time:       1.38 min
[32m[20230205 18:14:50 @agent_ppo2.py:151][0m 75776 total steps have happened
[32m[20230205 18:14:50 @agent_ppo2.py:127][0m #------------------------ Iteration 37 --------------------------#
[32m[20230205 18:14:51 @agent_ppo2.py:133][0m Sampling time: 0.80 s by 1 slaves
[32m[20230205 18:14:51 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:51 @agent_ppo2.py:191][0m |          -0.0020 |           3.6391 |           2.0585 |
[32m[20230205 18:14:51 @agent_ppo2.py:191][0m |          -0.0032 |           1.9168 |           2.0573 |
[32m[20230205 18:14:51 @agent_ppo2.py:191][0m |           0.0220 |           1.7905 |           2.0586 |
[32m[20230205 18:14:51 @agent_ppo2.py:191][0m |          -0.0059 |           1.6302 |           2.0580 |
[32m[20230205 18:14:51 @agent_ppo2.py:191][0m |          -0.0077 |           1.5132 |           2.0585 |
[32m[20230205 18:14:51 @agent_ppo2.py:191][0m |          -0.0054 |           1.4333 |           2.0589 |
[32m[20230205 18:14:51 @agent_ppo2.py:191][0m |          -0.0097 |           1.3822 |           2.0583 |
[32m[20230205 18:14:52 @agent_ppo2.py:191][0m |           0.0054 |           1.3638 |           2.0584 |
[32m[20230205 18:14:52 @agent_ppo2.py:191][0m |          -0.0043 |           1.3794 |           2.0576 |
[32m[20230205 18:14:52 @agent_ppo2.py:191][0m |          -0.0085 |           1.3056 |           2.0581 |
[32m[20230205 18:14:52 @agent_ppo2.py:136][0m Policy update time: 0.93 s
[32m[20230205 18:14:52 @agent_ppo2.py:144][0m Average TRAINING episode reward: -36.93
[32m[20230205 18:14:52 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 11.28
[32m[20230205 18:14:52 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 122.08
[32m[20230205 18:14:52 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 122.08
[32m[20230205 18:14:52 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 122.08
[32m[20230205 18:14:52 @agent_ppo2.py:149][0m Total time:       1.42 min
[32m[20230205 18:14:52 @agent_ppo2.py:151][0m 77824 total steps have happened
[32m[20230205 18:14:52 @agent_ppo2.py:127][0m #------------------------ Iteration 38 --------------------------#
[32m[20230205 18:14:53 @agent_ppo2.py:133][0m Sampling time: 0.74 s by 1 slaves
[32m[20230205 18:14:53 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:53 @agent_ppo2.py:191][0m |          -0.0008 |           0.8904 |           2.1356 |
[32m[20230205 18:14:53 @agent_ppo2.py:191][0m |          -0.0052 |           0.7193 |           2.1334 |
[32m[20230205 18:14:54 @agent_ppo2.py:191][0m |          -0.0072 |           0.6870 |           2.1327 |
[32m[20230205 18:14:54 @agent_ppo2.py:191][0m |          -0.0084 |           0.6694 |           2.1320 |
[32m[20230205 18:14:54 @agent_ppo2.py:191][0m |          -0.0091 |           0.6575 |           2.1318 |
[32m[20230205 18:14:54 @agent_ppo2.py:191][0m |          -0.0096 |           0.6509 |           2.1324 |
[32m[20230205 18:14:54 @agent_ppo2.py:191][0m |          -0.0097 |           0.6437 |           2.1316 |
[32m[20230205 18:14:54 @agent_ppo2.py:191][0m |          -0.0103 |           0.6379 |           2.1336 |
[32m[20230205 18:14:54 @agent_ppo2.py:191][0m |          -0.0108 |           0.6342 |           2.1333 |
[32m[20230205 18:14:54 @agent_ppo2.py:191][0m |          -0.0111 |           0.6305 |           2.1339 |
[32m[20230205 18:14:54 @agent_ppo2.py:136][0m Policy update time: 0.85 s
[32m[20230205 18:14:55 @agent_ppo2.py:144][0m Average TRAINING episode reward: 14.60
[32m[20230205 18:14:55 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 15.36
[32m[20230205 18:14:55 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 115.72
[32m[20230205 18:14:55 @agent_ppo2.py:149][0m Total time:       1.46 min
[32m[20230205 18:14:55 @agent_ppo2.py:151][0m 79872 total steps have happened
[32m[20230205 18:14:55 @agent_ppo2.py:127][0m #------------------------ Iteration 39 --------------------------#
[32m[20230205 18:14:56 @agent_ppo2.py:133][0m Sampling time: 0.74 s by 1 slaves
[32m[20230205 18:14:56 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:56 @agent_ppo2.py:191][0m |          -0.0011 |           1.0917 |           2.1727 |
[32m[20230205 18:14:56 @agent_ppo2.py:191][0m |          -0.0073 |           0.9185 |           2.1687 |
[32m[20230205 18:14:56 @agent_ppo2.py:191][0m |          -0.0107 |           0.8964 |           2.1695 |
[32m[20230205 18:14:56 @agent_ppo2.py:191][0m |          -0.0119 |           0.8828 |           2.1701 |
[32m[20230205 18:14:56 @agent_ppo2.py:191][0m |          -0.0133 |           0.8627 |           2.1708 |
[32m[20230205 18:14:56 @agent_ppo2.py:191][0m |          -0.0139 |           0.8453 |           2.1716 |
[32m[20230205 18:14:56 @agent_ppo2.py:191][0m |          -0.0147 |           0.8386 |           2.1725 |
[32m[20230205 18:14:56 @agent_ppo2.py:191][0m |          -0.0154 |           0.8277 |           2.1748 |
[32m[20230205 18:14:56 @agent_ppo2.py:191][0m |          -0.0158 |           0.8207 |           2.1744 |
[32m[20230205 18:14:56 @agent_ppo2.py:191][0m |          -0.0162 |           0.8138 |           2.1766 |
[32m[20230205 18:14:56 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:14:57 @agent_ppo2.py:144][0m Average TRAINING episode reward: 39.51
[32m[20230205 18:14:57 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 40.13
[32m[20230205 18:14:57 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 121.39
[32m[20230205 18:14:57 @agent_ppo2.py:149][0m Total time:       1.49 min
[32m[20230205 18:14:57 @agent_ppo2.py:151][0m 81920 total steps have happened
[32m[20230205 18:14:57 @agent_ppo2.py:127][0m #------------------------ Iteration 40 --------------------------#
[32m[20230205 18:14:58 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:14:58 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:14:58 @agent_ppo2.py:191][0m |           0.0011 |          10.6675 |           2.1369 |
[32m[20230205 18:14:58 @agent_ppo2.py:191][0m |          -0.0027 |           7.3639 |           2.1370 |
[32m[20230205 18:14:58 @agent_ppo2.py:191][0m |          -0.0035 |           5.9792 |           2.1382 |
[32m[20230205 18:14:58 @agent_ppo2.py:191][0m |          -0.0025 |           5.5836 |           2.1377 |
[32m[20230205 18:14:58 @agent_ppo2.py:191][0m |          -0.0038 |           5.1384 |           2.1382 |
[32m[20230205 18:14:58 @agent_ppo2.py:191][0m |          -0.0003 |           4.8052 |           2.1373 |
[32m[20230205 18:14:58 @agent_ppo2.py:191][0m |          -0.0085 |           4.4952 |           2.1373 |
[32m[20230205 18:14:58 @agent_ppo2.py:191][0m |          -0.0050 |           4.3797 |           2.1374 |
[32m[20230205 18:14:58 @agent_ppo2.py:191][0m |          -0.0078 |           4.1577 |           2.1373 |
[32m[20230205 18:14:58 @agent_ppo2.py:191][0m |          -0.0081 |           4.1690 |           2.1369 |
[32m[20230205 18:14:58 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:14:59 @agent_ppo2.py:144][0m Average TRAINING episode reward: -23.45
[32m[20230205 18:14:59 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 64.23
[32m[20230205 18:14:59 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 121.69
[32m[20230205 18:14:59 @agent_ppo2.py:149][0m Total time:       1.52 min
[32m[20230205 18:14:59 @agent_ppo2.py:151][0m 83968 total steps have happened
[32m[20230205 18:14:59 @agent_ppo2.py:127][0m #------------------------ Iteration 41 --------------------------#
[32m[20230205 18:15:00 @agent_ppo2.py:133][0m Sampling time: 0.77 s by 1 slaves
[32m[20230205 18:15:00 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:00 @agent_ppo2.py:191][0m |          -0.0000 |           1.7494 |           2.1605 |
[32m[20230205 18:15:00 @agent_ppo2.py:191][0m |          -0.0023 |           1.3532 |           2.1597 |
[32m[20230205 18:15:00 @agent_ppo2.py:191][0m |          -0.0043 |           1.2967 |           2.1598 |
[32m[20230205 18:15:00 @agent_ppo2.py:191][0m |          -0.0058 |           1.2659 |           2.1597 |
[32m[20230205 18:15:00 @agent_ppo2.py:191][0m |          -0.0068 |           1.2455 |           2.1608 |
[32m[20230205 18:15:00 @agent_ppo2.py:191][0m |          -0.0078 |           1.2302 |           2.1612 |
[32m[20230205 18:15:00 @agent_ppo2.py:191][0m |          -0.0087 |           1.2123 |           2.1618 |
[32m[20230205 18:15:00 @agent_ppo2.py:191][0m |          -0.0096 |           1.1993 |           2.1628 |
[32m[20230205 18:15:00 @agent_ppo2.py:191][0m |          -0.0101 |           1.1868 |           2.1631 |
[32m[20230205 18:15:00 @agent_ppo2.py:191][0m |          -0.0106 |           1.1757 |           2.1632 |
[32m[20230205 18:15:00 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:15:01 @agent_ppo2.py:144][0m Average TRAINING episode reward: 36.45
[32m[20230205 18:15:01 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 38.70
[32m[20230205 18:15:01 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 114.75
[32m[20230205 18:15:01 @agent_ppo2.py:149][0m Total time:       1.56 min
[32m[20230205 18:15:01 @agent_ppo2.py:151][0m 86016 total steps have happened
[32m[20230205 18:15:01 @agent_ppo2.py:127][0m #------------------------ Iteration 42 --------------------------#
[32m[20230205 18:15:02 @agent_ppo2.py:133][0m Sampling time: 0.74 s by 1 slaves
[32m[20230205 18:15:02 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:02 @agent_ppo2.py:191][0m |          -0.0020 |           1.1657 |           2.2143 |
[32m[20230205 18:15:02 @agent_ppo2.py:191][0m |          -0.0095 |           1.0286 |           2.2125 |
[32m[20230205 18:15:02 @agent_ppo2.py:191][0m |          -0.0126 |           0.9910 |           2.2125 |
[32m[20230205 18:15:02 @agent_ppo2.py:191][0m |          -0.0137 |           0.9722 |           2.2125 |
[32m[20230205 18:15:02 @agent_ppo2.py:191][0m |          -0.0145 |           0.9537 |           2.2134 |
[32m[20230205 18:15:02 @agent_ppo2.py:191][0m |          -0.0154 |           0.9437 |           2.2145 |
[32m[20230205 18:15:02 @agent_ppo2.py:191][0m |          -0.0159 |           0.9304 |           2.2159 |
[32m[20230205 18:15:02 @agent_ppo2.py:191][0m |          -0.0163 |           0.9240 |           2.2156 |
[32m[20230205 18:15:02 @agent_ppo2.py:191][0m |          -0.0168 |           0.9167 |           2.2172 |
[32m[20230205 18:15:02 @agent_ppo2.py:191][0m |          -0.0173 |           0.9118 |           2.2177 |
[32m[20230205 18:15:02 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:15:03 @agent_ppo2.py:144][0m Average TRAINING episode reward: 48.51
[32m[20230205 18:15:03 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 59.31
[32m[20230205 18:15:03 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 129.43
[32m[20230205 18:15:03 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 129.43
[32m[20230205 18:15:03 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 129.43
[32m[20230205 18:15:03 @agent_ppo2.py:149][0m Total time:       1.60 min
[32m[20230205 18:15:03 @agent_ppo2.py:151][0m 88064 total steps have happened
[32m[20230205 18:15:03 @agent_ppo2.py:127][0m #------------------------ Iteration 43 --------------------------#
[32m[20230205 18:15:04 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:15:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:04 @agent_ppo2.py:191][0m |          -0.0023 |           1.3282 |           2.2277 |
[32m[20230205 18:15:04 @agent_ppo2.py:191][0m |          -0.0085 |           1.2021 |           2.2234 |
[32m[20230205 18:15:04 @agent_ppo2.py:191][0m |          -0.0100 |           1.1748 |           2.2212 |
[32m[20230205 18:15:04 @agent_ppo2.py:191][0m |          -0.0107 |           1.1571 |           2.2224 |
[32m[20230205 18:15:04 @agent_ppo2.py:191][0m |          -0.0115 |           1.1393 |           2.2216 |
[32m[20230205 18:15:04 @agent_ppo2.py:191][0m |          -0.0116 |           1.1295 |           2.2228 |
[32m[20230205 18:15:04 @agent_ppo2.py:191][0m |          -0.0128 |           1.1187 |           2.2235 |
[32m[20230205 18:15:05 @agent_ppo2.py:191][0m |          -0.0132 |           1.1067 |           2.2236 |
[32m[20230205 18:15:05 @agent_ppo2.py:191][0m |          -0.0139 |           1.0971 |           2.2238 |
[32m[20230205 18:15:05 @agent_ppo2.py:191][0m |          -0.0140 |           1.0959 |           2.2251 |
[32m[20230205 18:15:05 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:15:05 @agent_ppo2.py:144][0m Average TRAINING episode reward: 56.39
[32m[20230205 18:15:05 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 63.33
[32m[20230205 18:15:05 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 63.84
[32m[20230205 18:15:05 @agent_ppo2.py:149][0m Total time:       1.63 min
[32m[20230205 18:15:05 @agent_ppo2.py:151][0m 90112 total steps have happened
[32m[20230205 18:15:05 @agent_ppo2.py:127][0m #------------------------ Iteration 44 --------------------------#
[32m[20230205 18:15:06 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:15:06 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:06 @agent_ppo2.py:191][0m |          -0.0004 |           6.1963 |           2.1888 |
[32m[20230205 18:15:06 @agent_ppo2.py:191][0m |          -0.0043 |           3.7683 |           2.1873 |
[32m[20230205 18:15:06 @agent_ppo2.py:191][0m |          -0.0056 |           3.1552 |           2.1881 |
[32m[20230205 18:15:07 @agent_ppo2.py:191][0m |          -0.0062 |           2.8371 |           2.1874 |
[32m[20230205 18:15:07 @agent_ppo2.py:191][0m |          -0.0069 |           2.6591 |           2.1886 |
[32m[20230205 18:15:07 @agent_ppo2.py:191][0m |          -0.0080 |           2.5692 |           2.1890 |
[32m[20230205 18:15:07 @agent_ppo2.py:191][0m |          -0.0083 |           2.4996 |           2.1899 |
[32m[20230205 18:15:07 @agent_ppo2.py:191][0m |          -0.0091 |           2.5163 |           2.1891 |
[32m[20230205 18:15:07 @agent_ppo2.py:191][0m |          -0.0095 |           2.3960 |           2.1896 |
[32m[20230205 18:15:07 @agent_ppo2.py:191][0m |          -0.0098 |           2.2916 |           2.1902 |
[32m[20230205 18:15:07 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:15:08 @agent_ppo2.py:144][0m Average TRAINING episode reward: 12.26
[32m[20230205 18:15:08 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 68.58
[32m[20230205 18:15:08 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 131.62
[32m[20230205 18:15:08 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 131.62
[32m[20230205 18:15:08 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 131.62
[32m[20230205 18:15:08 @agent_ppo2.py:149][0m Total time:       1.67 min
[32m[20230205 18:15:08 @agent_ppo2.py:151][0m 92160 total steps have happened
[32m[20230205 18:15:08 @agent_ppo2.py:127][0m #------------------------ Iteration 45 --------------------------#
[32m[20230205 18:15:08 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:15:08 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:08 @agent_ppo2.py:191][0m |           0.0022 |          10.0308 |           2.2448 |
[32m[20230205 18:15:08 @agent_ppo2.py:191][0m |           0.0023 |           8.1002 |           2.2428 |
[32m[20230205 18:15:08 @agent_ppo2.py:191][0m |          -0.0026 |           7.2852 |           2.2423 |
[32m[20230205 18:15:08 @agent_ppo2.py:191][0m |          -0.0052 |           6.8470 |           2.2434 |
[32m[20230205 18:15:08 @agent_ppo2.py:191][0m |          -0.0095 |           6.5982 |           2.2432 |
[32m[20230205 18:15:09 @agent_ppo2.py:191][0m |          -0.0039 |           6.3454 |           2.2446 |
[32m[20230205 18:15:09 @agent_ppo2.py:191][0m |          -0.0107 |           6.1837 |           2.2415 |
[32m[20230205 18:15:09 @agent_ppo2.py:191][0m |          -0.0052 |           6.4179 |           2.2427 |
[32m[20230205 18:15:09 @agent_ppo2.py:191][0m |          -0.0102 |           5.9692 |           2.2441 |
[32m[20230205 18:15:09 @agent_ppo2.py:191][0m |          -0.0033 |           5.8521 |           2.2424 |
[32m[20230205 18:15:09 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:15:09 @agent_ppo2.py:144][0m Average TRAINING episode reward: 6.80
[32m[20230205 18:15:09 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 88.08
[32m[20230205 18:15:09 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 54.89
[32m[20230205 18:15:09 @agent_ppo2.py:149][0m Total time:       1.70 min
[32m[20230205 18:15:09 @agent_ppo2.py:151][0m 94208 total steps have happened
[32m[20230205 18:15:09 @agent_ppo2.py:127][0m #------------------------ Iteration 46 --------------------------#
[32m[20230205 18:15:10 @agent_ppo2.py:133][0m Sampling time: 0.79 s by 1 slaves
[32m[20230205 18:15:10 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:10 @agent_ppo2.py:191][0m |           0.0005 |           2.1308 |           2.2721 |
[32m[20230205 18:15:10 @agent_ppo2.py:191][0m |          -0.0041 |           1.6802 |           2.2712 |
[32m[20230205 18:15:10 @agent_ppo2.py:191][0m |          -0.0063 |           1.6049 |           2.2736 |
[32m[20230205 18:15:11 @agent_ppo2.py:191][0m |          -0.0075 |           1.5560 |           2.2723 |
[32m[20230205 18:15:11 @agent_ppo2.py:191][0m |          -0.0083 |           1.5170 |           2.2736 |
[32m[20230205 18:15:11 @agent_ppo2.py:191][0m |          -0.0091 |           1.4889 |           2.2725 |
[32m[20230205 18:15:11 @agent_ppo2.py:191][0m |          -0.0092 |           1.4601 |           2.2721 |
[32m[20230205 18:15:11 @agent_ppo2.py:191][0m |          -0.0100 |           1.4401 |           2.2725 |
[32m[20230205 18:15:11 @agent_ppo2.py:191][0m |          -0.0105 |           1.4239 |           2.2725 |
[32m[20230205 18:15:11 @agent_ppo2.py:191][0m |          -0.0108 |           1.4038 |           2.2721 |
[32m[20230205 18:15:11 @agent_ppo2.py:136][0m Policy update time: 0.88 s
[32m[20230205 18:15:12 @agent_ppo2.py:144][0m Average TRAINING episode reward: 65.09
[32m[20230205 18:15:12 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 70.11
[32m[20230205 18:15:12 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 115.44
[32m[20230205 18:15:12 @agent_ppo2.py:149][0m Total time:       1.74 min
[32m[20230205 18:15:12 @agent_ppo2.py:151][0m 96256 total steps have happened
[32m[20230205 18:15:12 @agent_ppo2.py:127][0m #------------------------ Iteration 47 --------------------------#
[32m[20230205 18:15:12 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:15:12 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:12 @agent_ppo2.py:191][0m |           0.0009 |          10.3048 |           2.2775 |
[32m[20230205 18:15:12 @agent_ppo2.py:191][0m |          -0.0059 |           6.9195 |           2.2754 |
[32m[20230205 18:15:12 @agent_ppo2.py:191][0m |          -0.0071 |           6.3441 |           2.2734 |
[32m[20230205 18:15:12 @agent_ppo2.py:191][0m |          -0.0104 |           6.3225 |           2.2722 |
[32m[20230205 18:15:12 @agent_ppo2.py:191][0m |          -0.0062 |           6.0629 |           2.2727 |
[32m[20230205 18:15:13 @agent_ppo2.py:191][0m |          -0.0105 |           5.4175 |           2.2724 |
[32m[20230205 18:15:13 @agent_ppo2.py:191][0m |          -0.0116 |           5.2741 |           2.2720 |
[32m[20230205 18:15:13 @agent_ppo2.py:191][0m |          -0.0122 |           5.2211 |           2.2729 |
[32m[20230205 18:15:13 @agent_ppo2.py:191][0m |          -0.0126 |           4.9069 |           2.2725 |
[32m[20230205 18:15:13 @agent_ppo2.py:191][0m |          -0.0137 |           4.6592 |           2.2731 |
[32m[20230205 18:15:13 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:15:13 @agent_ppo2.py:144][0m Average TRAINING episode reward: -1.60
[32m[20230205 18:15:13 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 65.70
[32m[20230205 18:15:13 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 118.22
[32m[20230205 18:15:13 @agent_ppo2.py:149][0m Total time:       1.77 min
[32m[20230205 18:15:13 @agent_ppo2.py:151][0m 98304 total steps have happened
[32m[20230205 18:15:13 @agent_ppo2.py:127][0m #------------------------ Iteration 48 --------------------------#
[32m[20230205 18:15:14 @agent_ppo2.py:133][0m Sampling time: 0.47 s by 1 slaves
[32m[20230205 18:15:14 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:14 @agent_ppo2.py:191][0m |           0.0141 |          19.7365 |           2.2456 |
[32m[20230205 18:15:14 @agent_ppo2.py:191][0m |          -0.0081 |          14.3951 |           2.2453 |
[32m[20230205 18:15:14 @agent_ppo2.py:191][0m |          -0.0137 |          12.2020 |           2.2441 |
[32m[20230205 18:15:14 @agent_ppo2.py:191][0m |          -0.0007 |          11.2167 |           2.2435 |
[32m[20230205 18:15:14 @agent_ppo2.py:191][0m |          -0.0126 |          10.4884 |           2.2427 |
[32m[20230205 18:15:14 @agent_ppo2.py:191][0m |          -0.0106 |           9.9480 |           2.2423 |
[32m[20230205 18:15:14 @agent_ppo2.py:191][0m |           0.0109 |          13.0891 |           2.2431 |
[32m[20230205 18:15:14 @agent_ppo2.py:191][0m |          -0.0019 |          11.2240 |           2.2424 |
[32m[20230205 18:15:14 @agent_ppo2.py:191][0m |          -0.0128 |           9.0088 |           2.2420 |
[32m[20230205 18:15:14 @agent_ppo2.py:191][0m |          -0.0178 |           8.9404 |           2.2416 |
[32m[20230205 18:15:14 @agent_ppo2.py:136][0m Policy update time: 0.47 s
[32m[20230205 18:15:15 @agent_ppo2.py:144][0m Average TRAINING episode reward: 3.64
[32m[20230205 18:15:15 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 93.32
[32m[20230205 18:15:15 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 121.90
[32m[20230205 18:15:15 @agent_ppo2.py:149][0m Total time:       1.80 min
[32m[20230205 18:15:15 @agent_ppo2.py:151][0m 100352 total steps have happened
[32m[20230205 18:15:15 @agent_ppo2.py:127][0m #------------------------ Iteration 49 --------------------------#
[32m[20230205 18:15:16 @agent_ppo2.py:133][0m Sampling time: 0.75 s by 1 slaves
[32m[20230205 18:15:16 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:16 @agent_ppo2.py:191][0m |          -0.0005 |           2.1124 |           2.3476 |
[32m[20230205 18:15:16 @agent_ppo2.py:191][0m |          -0.0052 |           1.6616 |           2.3415 |
[32m[20230205 18:15:16 @agent_ppo2.py:191][0m |          -0.0076 |           1.5635 |           2.3419 |
[32m[20230205 18:15:16 @agent_ppo2.py:191][0m |          -0.0089 |           1.5054 |           2.3409 |
[32m[20230205 18:15:16 @agent_ppo2.py:191][0m |          -0.0099 |           1.4655 |           2.3419 |
[32m[20230205 18:15:16 @agent_ppo2.py:191][0m |          -0.0105 |           1.4352 |           2.3415 |
[32m[20230205 18:15:16 @agent_ppo2.py:191][0m |          -0.0114 |           1.4121 |           2.3414 |
[32m[20230205 18:15:16 @agent_ppo2.py:191][0m |          -0.0117 |           1.3895 |           2.3417 |
[32m[20230205 18:15:17 @agent_ppo2.py:191][0m |          -0.0125 |           1.3712 |           2.3423 |
[32m[20230205 18:15:17 @agent_ppo2.py:191][0m |          -0.0130 |           1.3561 |           2.3421 |
[32m[20230205 18:15:17 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:15:17 @agent_ppo2.py:144][0m Average TRAINING episode reward: 78.47
[32m[20230205 18:15:17 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 81.70
[32m[20230205 18:15:17 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 121.39
[32m[20230205 18:15:17 @agent_ppo2.py:149][0m Total time:       1.83 min
[32m[20230205 18:15:17 @agent_ppo2.py:151][0m 102400 total steps have happened
[32m[20230205 18:15:17 @agent_ppo2.py:127][0m #------------------------ Iteration 50 --------------------------#
[32m[20230205 18:15:18 @agent_ppo2.py:133][0m Sampling time: 0.74 s by 1 slaves
[32m[20230205 18:15:18 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:18 @agent_ppo2.py:191][0m |          -0.0008 |           1.5697 |           2.2940 |
[32m[20230205 18:15:18 @agent_ppo2.py:191][0m |          -0.0059 |           1.4057 |           2.2927 |
[32m[20230205 18:15:18 @agent_ppo2.py:191][0m |          -0.0072 |           1.3587 |           2.2894 |
[32m[20230205 18:15:18 @agent_ppo2.py:191][0m |          -0.0087 |           1.3339 |           2.2900 |
[32m[20230205 18:15:18 @agent_ppo2.py:191][0m |          -0.0094 |           1.3087 |           2.2918 |
[32m[20230205 18:15:19 @agent_ppo2.py:191][0m |          -0.0098 |           1.2953 |           2.2901 |
[32m[20230205 18:15:19 @agent_ppo2.py:191][0m |          -0.0107 |           1.2789 |           2.2915 |
[32m[20230205 18:15:19 @agent_ppo2.py:191][0m |          -0.0108 |           1.2663 |           2.2916 |
[32m[20230205 18:15:19 @agent_ppo2.py:191][0m |          -0.0116 |           1.2534 |           2.2924 |
[32m[20230205 18:15:19 @agent_ppo2.py:191][0m |          -0.0116 |           1.2507 |           2.2922 |
[32m[20230205 18:15:19 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:15:19 @agent_ppo2.py:144][0m Average TRAINING episode reward: 94.10
[32m[20230205 18:15:19 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 109.76
[32m[20230205 18:15:19 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 136.49
[32m[20230205 18:15:19 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 136.49
[32m[20230205 18:15:19 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 136.49
[32m[20230205 18:15:19 @agent_ppo2.py:149][0m Total time:       1.87 min
[32m[20230205 18:15:19 @agent_ppo2.py:151][0m 104448 total steps have happened
[32m[20230205 18:15:19 @agent_ppo2.py:127][0m #------------------------ Iteration 51 --------------------------#
[32m[20230205 18:15:20 @agent_ppo2.py:133][0m Sampling time: 0.75 s by 1 slaves
[32m[20230205 18:15:20 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:20 @agent_ppo2.py:191][0m |           0.0006 |          34.1879 |           2.2956 |
[32m[20230205 18:15:20 @agent_ppo2.py:191][0m |          -0.0025 |          30.4641 |           2.2950 |
[32m[20230205 18:15:20 @agent_ppo2.py:191][0m |          -0.0038 |          28.0332 |           2.2932 |
[32m[20230205 18:15:21 @agent_ppo2.py:191][0m |          -0.0049 |          26.9356 |           2.2929 |
[32m[20230205 18:15:21 @agent_ppo2.py:191][0m |          -0.0068 |          24.8579 |           2.2942 |
[32m[20230205 18:15:21 @agent_ppo2.py:191][0m |          -0.0062 |          23.9399 |           2.2947 |
[32m[20230205 18:15:21 @agent_ppo2.py:191][0m |          -0.0082 |          18.3875 |           2.2968 |
[32m[20230205 18:15:21 @agent_ppo2.py:191][0m |          -0.0078 |           9.6405 |           2.2958 |
[32m[20230205 18:15:21 @agent_ppo2.py:191][0m |          -0.0094 |           7.5826 |           2.2973 |
[32m[20230205 18:15:21 @agent_ppo2.py:191][0m |          -0.0098 |           6.7278 |           2.2981 |
[32m[20230205 18:15:21 @agent_ppo2.py:136][0m Policy update time: 0.72 s
[32m[20230205 18:15:22 @agent_ppo2.py:144][0m Average TRAINING episode reward: 21.69
[32m[20230205 18:15:22 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 91.69
[32m[20230205 18:15:22 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -24.48
[32m[20230205 18:15:22 @agent_ppo2.py:149][0m Total time:       1.91 min
[32m[20230205 18:15:22 @agent_ppo2.py:151][0m 106496 total steps have happened
[32m[20230205 18:15:22 @agent_ppo2.py:127][0m #------------------------ Iteration 52 --------------------------#
[32m[20230205 18:15:22 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:15:22 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:22 @agent_ppo2.py:191][0m |          -0.0016 |           6.3942 |           2.3346 |
[32m[20230205 18:15:22 @agent_ppo2.py:191][0m |           0.0015 |           4.0875 |           2.3358 |
[32m[20230205 18:15:22 @agent_ppo2.py:191][0m |          -0.0015 |           3.1319 |           2.3376 |
[32m[20230205 18:15:23 @agent_ppo2.py:191][0m |          -0.0046 |           2.8158 |           2.3371 |
[32m[20230205 18:15:23 @agent_ppo2.py:191][0m |          -0.0069 |           2.5909 |           2.3376 |
[32m[20230205 18:15:23 @agent_ppo2.py:191][0m |          -0.0064 |           2.4824 |           2.3381 |
[32m[20230205 18:15:23 @agent_ppo2.py:191][0m |          -0.0084 |           2.3431 |           2.3369 |
[32m[20230205 18:15:23 @agent_ppo2.py:191][0m |          -0.0052 |           2.2672 |           2.3379 |
[32m[20230205 18:15:23 @agent_ppo2.py:191][0m |          -0.0079 |           2.2785 |           2.3379 |
[32m[20230205 18:15:23 @agent_ppo2.py:191][0m |          -0.0111 |           2.1298 |           2.3374 |
[32m[20230205 18:15:23 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:15:23 @agent_ppo2.py:144][0m Average TRAINING episode reward: 17.58
[32m[20230205 18:15:23 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 88.96
[32m[20230205 18:15:23 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 39.62
[32m[20230205 18:15:23 @agent_ppo2.py:149][0m Total time:       1.93 min
[32m[20230205 18:15:23 @agent_ppo2.py:151][0m 108544 total steps have happened
[32m[20230205 18:15:23 @agent_ppo2.py:127][0m #------------------------ Iteration 53 --------------------------#
[32m[20230205 18:15:24 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:15:24 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:24 @agent_ppo2.py:191][0m |          -0.0003 |           2.5940 |           2.3408 |
[32m[20230205 18:15:24 @agent_ppo2.py:191][0m |          -0.0039 |           2.0928 |           2.3398 |
[32m[20230205 18:15:24 @agent_ppo2.py:191][0m |          -0.0056 |           1.9882 |           2.3411 |
[32m[20230205 18:15:24 @agent_ppo2.py:191][0m |          -0.0066 |           1.9302 |           2.3399 |
[32m[20230205 18:15:24 @agent_ppo2.py:191][0m |          -0.0074 |           1.8818 |           2.3402 |
[32m[20230205 18:15:25 @agent_ppo2.py:191][0m |          -0.0085 |           1.8421 |           2.3391 |
[32m[20230205 18:15:25 @agent_ppo2.py:191][0m |          -0.0093 |           1.8007 |           2.3392 |
[32m[20230205 18:15:25 @agent_ppo2.py:191][0m |          -0.0101 |           1.7696 |           2.3396 |
[32m[20230205 18:15:25 @agent_ppo2.py:191][0m |          -0.0103 |           1.7586 |           2.3397 |
[32m[20230205 18:15:25 @agent_ppo2.py:191][0m |          -0.0110 |           1.7259 |           2.3399 |
[32m[20230205 18:15:25 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:15:25 @agent_ppo2.py:144][0m Average TRAINING episode reward: 101.24
[32m[20230205 18:15:25 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 103.80
[32m[20230205 18:15:25 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 132.82
[32m[20230205 18:15:25 @agent_ppo2.py:149][0m Total time:       1.97 min
[32m[20230205 18:15:25 @agent_ppo2.py:151][0m 110592 total steps have happened
[32m[20230205 18:15:25 @agent_ppo2.py:127][0m #------------------------ Iteration 54 --------------------------#
[32m[20230205 18:15:26 @agent_ppo2.py:133][0m Sampling time: 0.74 s by 1 slaves
[32m[20230205 18:15:26 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:26 @agent_ppo2.py:191][0m |          -0.0005 |           1.7920 |           2.2909 |
[32m[20230205 18:15:26 @agent_ppo2.py:191][0m |          -0.0035 |           1.4448 |           2.2899 |
[32m[20230205 18:15:27 @agent_ppo2.py:191][0m |          -0.0055 |           1.4010 |           2.2889 |
[32m[20230205 18:15:27 @agent_ppo2.py:191][0m |          -0.0065 |           1.3665 |           2.2906 |
[32m[20230205 18:15:27 @agent_ppo2.py:191][0m |          -0.0080 |           1.3467 |           2.2905 |
[32m[20230205 18:15:27 @agent_ppo2.py:191][0m |          -0.0086 |           1.3196 |           2.2913 |
[32m[20230205 18:15:27 @agent_ppo2.py:191][0m |          -0.0088 |           1.3001 |           2.2924 |
[32m[20230205 18:15:27 @agent_ppo2.py:191][0m |          -0.0094 |           1.2810 |           2.2939 |
[32m[20230205 18:15:27 @agent_ppo2.py:191][0m |          -0.0099 |           1.2675 |           2.2938 |
[32m[20230205 18:15:27 @agent_ppo2.py:191][0m |          -0.0105 |           1.2463 |           2.2943 |
[32m[20230205 18:15:27 @agent_ppo2.py:136][0m Policy update time: 1.16 s
[32m[20230205 18:15:28 @agent_ppo2.py:144][0m Average TRAINING episode reward: 102.17
[32m[20230205 18:15:28 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 105.94
[32m[20230205 18:15:28 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 29.22
[32m[20230205 18:15:28 @agent_ppo2.py:149][0m Total time:       2.01 min
[32m[20230205 18:15:28 @agent_ppo2.py:151][0m 112640 total steps have happened
[32m[20230205 18:15:28 @agent_ppo2.py:127][0m #------------------------ Iteration 55 --------------------------#
[32m[20230205 18:15:29 @agent_ppo2.py:133][0m Sampling time: 0.79 s by 1 slaves
[32m[20230205 18:15:29 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:29 @agent_ppo2.py:191][0m |           0.0017 |           6.8781 |           2.3768 |
[32m[20230205 18:15:29 @agent_ppo2.py:191][0m |          -0.0029 |           3.8244 |           2.3738 |
[32m[20230205 18:15:29 @agent_ppo2.py:191][0m |          -0.0036 |           3.6133 |           2.3712 |
[32m[20230205 18:15:29 @agent_ppo2.py:191][0m |          -0.0044 |           2.5592 |           2.3718 |
[32m[20230205 18:15:29 @agent_ppo2.py:191][0m |          -0.0057 |           2.5655 |           2.3718 |
[32m[20230205 18:15:29 @agent_ppo2.py:191][0m |          -0.0046 |           2.4409 |           2.3715 |
[32m[20230205 18:15:30 @agent_ppo2.py:191][0m |          -0.0068 |           2.1058 |           2.3707 |
[32m[20230205 18:15:30 @agent_ppo2.py:191][0m |          -0.0065 |           1.9583 |           2.3707 |
[32m[20230205 18:15:30 @agent_ppo2.py:191][0m |          -0.0079 |           1.9399 |           2.3700 |
[32m[20230205 18:15:30 @agent_ppo2.py:191][0m |          -0.0079 |           1.7865 |           2.3693 |
[32m[20230205 18:15:30 @agent_ppo2.py:136][0m Policy update time: 0.79 s
[32m[20230205 18:15:30 @agent_ppo2.py:144][0m Average TRAINING episode reward: 26.10
[32m[20230205 18:15:30 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 93.94
[32m[20230205 18:15:30 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -6.12
[32m[20230205 18:15:30 @agent_ppo2.py:149][0m Total time:       2.05 min
[32m[20230205 18:15:30 @agent_ppo2.py:151][0m 114688 total steps have happened
[32m[20230205 18:15:30 @agent_ppo2.py:127][0m #------------------------ Iteration 56 --------------------------#
[32m[20230205 18:15:31 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:15:31 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:31 @agent_ppo2.py:191][0m |          -0.0001 |           1.8036 |           2.3900 |
[32m[20230205 18:15:31 @agent_ppo2.py:191][0m |          -0.0060 |           1.6075 |           2.3874 |
[32m[20230205 18:15:31 @agent_ppo2.py:191][0m |          -0.0080 |           1.5732 |           2.3849 |
[32m[20230205 18:15:32 @agent_ppo2.py:191][0m |          -0.0100 |           1.5489 |           2.3846 |
[32m[20230205 18:15:32 @agent_ppo2.py:191][0m |          -0.0112 |           1.5300 |           2.3856 |
[32m[20230205 18:15:32 @agent_ppo2.py:191][0m |          -0.0122 |           1.5151 |           2.3855 |
[32m[20230205 18:15:32 @agent_ppo2.py:191][0m |          -0.0126 |           1.5040 |           2.3856 |
[32m[20230205 18:15:32 @agent_ppo2.py:191][0m |          -0.0131 |           1.4905 |           2.3853 |
[32m[20230205 18:15:32 @agent_ppo2.py:191][0m |          -0.0139 |           1.4827 |           2.3866 |
[32m[20230205 18:15:32 @agent_ppo2.py:191][0m |          -0.0147 |           1.4735 |           2.3871 |
[32m[20230205 18:15:32 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:15:33 @agent_ppo2.py:144][0m Average TRAINING episode reward: 98.59
[32m[20230205 18:15:33 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 101.17
[32m[20230205 18:15:33 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 35.89
[32m[20230205 18:15:33 @agent_ppo2.py:149][0m Total time:       2.09 min
[32m[20230205 18:15:33 @agent_ppo2.py:151][0m 116736 total steps have happened
[32m[20230205 18:15:33 @agent_ppo2.py:127][0m #------------------------ Iteration 57 --------------------------#
[32m[20230205 18:15:33 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:15:33 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:33 @agent_ppo2.py:191][0m |           0.0011 |           6.7728 |           2.3769 |
[32m[20230205 18:15:33 @agent_ppo2.py:191][0m |          -0.0021 |           4.4692 |           2.3733 |
[32m[20230205 18:15:33 @agent_ppo2.py:191][0m |          -0.0040 |           3.5254 |           2.3726 |
[32m[20230205 18:15:33 @agent_ppo2.py:191][0m |          -0.0051 |           3.3411 |           2.3719 |
[32m[20230205 18:15:33 @agent_ppo2.py:191][0m |          -0.0059 |           3.1441 |           2.3706 |
[32m[20230205 18:15:34 @agent_ppo2.py:191][0m |          -0.0066 |           3.0617 |           2.3704 |
[32m[20230205 18:15:34 @agent_ppo2.py:191][0m |          -0.0076 |           3.0536 |           2.3690 |
[32m[20230205 18:15:34 @agent_ppo2.py:191][0m |          -0.0070 |           3.1176 |           2.3704 |
[32m[20230205 18:15:34 @agent_ppo2.py:191][0m |          -0.0089 |           2.9339 |           2.3686 |
[32m[20230205 18:15:34 @agent_ppo2.py:191][0m |          -0.0082 |           2.9394 |           2.3685 |
[32m[20230205 18:15:34 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:15:34 @agent_ppo2.py:144][0m Average TRAINING episode reward: 20.54
[32m[20230205 18:15:34 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 101.86
[32m[20230205 18:15:34 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -45.72
[32m[20230205 18:15:34 @agent_ppo2.py:149][0m Total time:       2.11 min
[32m[20230205 18:15:34 @agent_ppo2.py:151][0m 118784 total steps have happened
[32m[20230205 18:15:34 @agent_ppo2.py:127][0m #------------------------ Iteration 58 --------------------------#
[32m[20230205 18:15:35 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:15:35 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:35 @agent_ppo2.py:191][0m |          -0.0019 |          11.5641 |           2.3296 |
[32m[20230205 18:15:35 @agent_ppo2.py:191][0m |          -0.0088 |           4.6868 |           2.3293 |
[32m[20230205 18:15:35 @agent_ppo2.py:191][0m |          -0.0076 |           3.7084 |           2.3287 |
[32m[20230205 18:15:35 @agent_ppo2.py:191][0m |          -0.0090 |           3.3358 |           2.3259 |
[32m[20230205 18:15:35 @agent_ppo2.py:191][0m |          -0.0041 |           2.9798 |           2.3255 |
[32m[20230205 18:15:35 @agent_ppo2.py:191][0m |          -0.0106 |           2.8069 |           2.3248 |
[32m[20230205 18:15:35 @agent_ppo2.py:191][0m |          -0.0113 |           2.6132 |           2.3234 |
[32m[20230205 18:15:35 @agent_ppo2.py:191][0m |          -0.0031 |           2.6012 |           2.3229 |
[32m[20230205 18:15:35 @agent_ppo2.py:191][0m |          -0.0062 |           2.3620 |           2.3230 |
[32m[20230205 18:15:35 @agent_ppo2.py:191][0m |          -0.0032 |           2.4105 |           2.3229 |
[32m[20230205 18:15:35 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:15:36 @agent_ppo2.py:144][0m Average TRAINING episode reward: 29.93
[32m[20230205 18:15:36 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 108.72
[32m[20230205 18:15:36 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -49.46
[32m[20230205 18:15:36 @agent_ppo2.py:149][0m Total time:       2.14 min
[32m[20230205 18:15:36 @agent_ppo2.py:151][0m 120832 total steps have happened
[32m[20230205 18:15:36 @agent_ppo2.py:127][0m #------------------------ Iteration 59 --------------------------#
[32m[20230205 18:15:37 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:15:37 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:37 @agent_ppo2.py:191][0m |          -0.0009 |          13.8315 |           2.4051 |
[32m[20230205 18:15:37 @agent_ppo2.py:191][0m |          -0.0043 |          10.1482 |           2.4028 |
[32m[20230205 18:15:37 @agent_ppo2.py:191][0m |          -0.0056 |           9.1533 |           2.4007 |
[32m[20230205 18:15:37 @agent_ppo2.py:191][0m |          -0.0067 |           8.3924 |           2.3996 |
[32m[20230205 18:15:37 @agent_ppo2.py:191][0m |          -0.0072 |           8.1708 |           2.3985 |
[32m[20230205 18:15:37 @agent_ppo2.py:191][0m |          -0.0082 |           7.9481 |           2.3973 |
[32m[20230205 18:15:37 @agent_ppo2.py:191][0m |          -0.0073 |           7.6532 |           2.3965 |
[32m[20230205 18:15:37 @agent_ppo2.py:191][0m |          -0.0096 |           7.4255 |           2.3956 |
[32m[20230205 18:15:37 @agent_ppo2.py:191][0m |          -0.0095 |           7.4205 |           2.3957 |
[32m[20230205 18:15:37 @agent_ppo2.py:191][0m |          -0.0106 |           7.3504 |           2.3952 |
[32m[20230205 18:15:37 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:15:38 @agent_ppo2.py:144][0m Average TRAINING episode reward: 32.71
[32m[20230205 18:15:38 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 98.48
[32m[20230205 18:15:38 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -62.07
[32m[20230205 18:15:38 @agent_ppo2.py:149][0m Total time:       2.17 min
[32m[20230205 18:15:38 @agent_ppo2.py:151][0m 122880 total steps have happened
[32m[20230205 18:15:38 @agent_ppo2.py:127][0m #------------------------ Iteration 60 --------------------------#
[32m[20230205 18:15:38 @agent_ppo2.py:133][0m Sampling time: 0.74 s by 1 slaves
[32m[20230205 18:15:38 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:38 @agent_ppo2.py:191][0m |          -0.0000 |           1.9423 |           2.3654 |
[32m[20230205 18:15:39 @agent_ppo2.py:191][0m |          -0.0030 |           1.5394 |           2.3641 |
[32m[20230205 18:15:39 @agent_ppo2.py:191][0m |          -0.0047 |           1.4176 |           2.3626 |
[32m[20230205 18:15:39 @agent_ppo2.py:191][0m |          -0.0059 |           1.3564 |           2.3614 |
[32m[20230205 18:15:39 @agent_ppo2.py:191][0m |          -0.0066 |           1.3143 |           2.3624 |
[32m[20230205 18:15:39 @agent_ppo2.py:191][0m |          -0.0079 |           1.2787 |           2.3620 |
[32m[20230205 18:15:39 @agent_ppo2.py:191][0m |          -0.0081 |           1.2530 |           2.3631 |
[32m[20230205 18:15:39 @agent_ppo2.py:191][0m |          -0.0090 |           1.2356 |           2.3616 |
[32m[20230205 18:15:39 @agent_ppo2.py:191][0m |          -0.0096 |           1.2190 |           2.3627 |
[32m[20230205 18:15:39 @agent_ppo2.py:191][0m |          -0.0099 |           1.2038 |           2.3635 |
[32m[20230205 18:15:39 @agent_ppo2.py:136][0m Policy update time: 0.83 s
[32m[20230205 18:15:40 @agent_ppo2.py:144][0m Average TRAINING episode reward: 113.24
[32m[20230205 18:15:40 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 124.18
[32m[20230205 18:15:40 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -24.80
[32m[20230205 18:15:40 @agent_ppo2.py:149][0m Total time:       2.21 min
[32m[20230205 18:15:40 @agent_ppo2.py:151][0m 124928 total steps have happened
[32m[20230205 18:15:40 @agent_ppo2.py:127][0m #------------------------ Iteration 61 --------------------------#
[32m[20230205 18:15:40 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:15:41 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:41 @agent_ppo2.py:191][0m |          -0.0022 |          20.1348 |           2.3642 |
[32m[20230205 18:15:41 @agent_ppo2.py:191][0m |          -0.0080 |          14.0185 |           2.3607 |
[32m[20230205 18:15:41 @agent_ppo2.py:191][0m |          -0.0108 |          11.2143 |           2.3562 |
[32m[20230205 18:15:41 @agent_ppo2.py:191][0m |          -0.0118 |           9.3498 |           2.3547 |
[32m[20230205 18:15:41 @agent_ppo2.py:191][0m |          -0.0127 |           8.4415 |           2.3536 |
[32m[20230205 18:15:41 @agent_ppo2.py:191][0m |          -0.0132 |           7.6573 |           2.3547 |
[32m[20230205 18:15:41 @agent_ppo2.py:191][0m |          -0.0138 |           7.3117 |           2.3542 |
[32m[20230205 18:15:41 @agent_ppo2.py:191][0m |          -0.0140 |           7.0429 |           2.3550 |
[32m[20230205 18:15:41 @agent_ppo2.py:191][0m |          -0.0142 |           6.8056 |           2.3549 |
[32m[20230205 18:15:41 @agent_ppo2.py:191][0m |          -0.0145 |           6.5025 |           2.3551 |
[32m[20230205 18:15:41 @agent_ppo2.py:136][0m Policy update time: 0.62 s
[32m[20230205 18:15:42 @agent_ppo2.py:144][0m Average TRAINING episode reward: 51.62
[32m[20230205 18:15:42 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 123.46
[32m[20230205 18:15:42 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -71.71
[32m[20230205 18:15:42 @agent_ppo2.py:149][0m Total time:       2.24 min
[32m[20230205 18:15:42 @agent_ppo2.py:151][0m 126976 total steps have happened
[32m[20230205 18:15:42 @agent_ppo2.py:127][0m #------------------------ Iteration 62 --------------------------#
[32m[20230205 18:15:42 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:15:42 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:42 @agent_ppo2.py:191][0m |          -0.0017 |           2.3531 |           2.3870 |
[32m[20230205 18:15:43 @agent_ppo2.py:191][0m |          -0.0063 |           1.7715 |           2.3859 |
[32m[20230205 18:15:43 @agent_ppo2.py:191][0m |          -0.0085 |           1.6356 |           2.3866 |
[32m[20230205 18:15:43 @agent_ppo2.py:191][0m |          -0.0096 |           1.5622 |           2.3895 |
[32m[20230205 18:15:43 @agent_ppo2.py:191][0m |          -0.0105 |           1.5076 |           2.3907 |
[32m[20230205 18:15:43 @agent_ppo2.py:191][0m |          -0.0111 |           1.4676 |           2.3927 |
[32m[20230205 18:15:43 @agent_ppo2.py:191][0m |          -0.0115 |           1.4410 |           2.3959 |
[32m[20230205 18:15:43 @agent_ppo2.py:191][0m |          -0.0119 |           1.4104 |           2.3972 |
[32m[20230205 18:15:43 @agent_ppo2.py:191][0m |          -0.0126 |           1.3906 |           2.3982 |
[32m[20230205 18:15:43 @agent_ppo2.py:191][0m |          -0.0129 |           1.3735 |           2.4010 |
[32m[20230205 18:15:43 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:15:44 @agent_ppo2.py:144][0m Average TRAINING episode reward: 134.17
[32m[20230205 18:15:44 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 149.02
[32m[20230205 18:15:44 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 5.74
[32m[20230205 18:15:44 @agent_ppo2.py:149][0m Total time:       2.27 min
[32m[20230205 18:15:44 @agent_ppo2.py:151][0m 129024 total steps have happened
[32m[20230205 18:15:44 @agent_ppo2.py:127][0m #------------------------ Iteration 63 --------------------------#
[32m[20230205 18:15:44 @agent_ppo2.py:133][0m Sampling time: 0.81 s by 1 slaves
[32m[20230205 18:15:44 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:44 @agent_ppo2.py:191][0m |           0.0004 |          10.3456 |           2.4211 |
[32m[20230205 18:15:45 @agent_ppo2.py:191][0m |          -0.0021 |           8.1751 |           2.4207 |
[32m[20230205 18:15:45 @agent_ppo2.py:191][0m |          -0.0005 |           7.4792 |           2.4181 |
[32m[20230205 18:15:45 @agent_ppo2.py:191][0m |          -0.0057 |           6.4018 |           2.4167 |
[32m[20230205 18:15:45 @agent_ppo2.py:191][0m |          -0.0055 |           5.9268 |           2.4165 |
[32m[20230205 18:15:45 @agent_ppo2.py:191][0m |          -0.0078 |           5.4778 |           2.4158 |
[32m[20230205 18:15:45 @agent_ppo2.py:191][0m |          -0.0087 |           5.0922 |           2.4147 |
[32m[20230205 18:15:45 @agent_ppo2.py:191][0m |          -0.0072 |           4.8482 |           2.4134 |
[32m[20230205 18:15:45 @agent_ppo2.py:191][0m |          -0.0076 |           4.3956 |           2.4124 |
[32m[20230205 18:15:45 @agent_ppo2.py:191][0m |          -0.0093 |           4.2233 |           2.4119 |
[32m[20230205 18:15:45 @agent_ppo2.py:136][0m Policy update time: 0.79 s
[32m[20230205 18:15:46 @agent_ppo2.py:144][0m Average TRAINING episode reward: 54.17
[32m[20230205 18:15:46 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 127.68
[32m[20230205 18:15:46 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -41.80
[32m[20230205 18:15:46 @agent_ppo2.py:149][0m Total time:       2.30 min
[32m[20230205 18:15:46 @agent_ppo2.py:151][0m 131072 total steps have happened
[32m[20230205 18:15:46 @agent_ppo2.py:127][0m #------------------------ Iteration 64 --------------------------#
[32m[20230205 18:15:46 @agent_ppo2.py:133][0m Sampling time: 0.47 s by 1 slaves
[32m[20230205 18:15:46 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:46 @agent_ppo2.py:191][0m |          -0.0046 |          34.5775 |           2.4203 |
[32m[20230205 18:15:46 @agent_ppo2.py:191][0m |          -0.0107 |          23.5657 |           2.4169 |
[32m[20230205 18:15:46 @agent_ppo2.py:191][0m |          -0.0112 |          21.2637 |           2.4139 |
[32m[20230205 18:15:46 @agent_ppo2.py:191][0m |          -0.0043 |          19.5391 |           2.4105 |
[32m[20230205 18:15:46 @agent_ppo2.py:191][0m |          -0.0044 |          18.0152 |           2.4081 |
[32m[20230205 18:15:46 @agent_ppo2.py:191][0m |          -0.0053 |          17.0757 |           2.4077 |
[32m[20230205 18:15:46 @agent_ppo2.py:191][0m |          -0.0079 |          16.0513 |           2.4081 |
[32m[20230205 18:15:47 @agent_ppo2.py:191][0m |          -0.0028 |          14.8520 |           2.4068 |
[32m[20230205 18:15:47 @agent_ppo2.py:191][0m |          -0.0120 |          13.8094 |           2.4078 |
[32m[20230205 18:15:47 @agent_ppo2.py:191][0m |          -0.0121 |          13.0739 |           2.4076 |
[32m[20230205 18:15:47 @agent_ppo2.py:136][0m Policy update time: 0.48 s
[32m[20230205 18:15:47 @agent_ppo2.py:144][0m Average TRAINING episode reward: -11.65
[32m[20230205 18:15:47 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 144.93
[32m[20230205 18:15:47 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 4.50
[32m[20230205 18:15:47 @agent_ppo2.py:149][0m Total time:       2.33 min
[32m[20230205 18:15:47 @agent_ppo2.py:151][0m 133120 total steps have happened
[32m[20230205 18:15:47 @agent_ppo2.py:127][0m #------------------------ Iteration 65 --------------------------#
[32m[20230205 18:15:48 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:15:48 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:48 @agent_ppo2.py:191][0m |          -0.0048 |          28.5004 |           2.3979 |
[32m[20230205 18:15:48 @agent_ppo2.py:191][0m |          -0.0081 |          17.6476 |           2.3941 |
[32m[20230205 18:15:48 @agent_ppo2.py:191][0m |          -0.0057 |          15.1055 |           2.3917 |
[32m[20230205 18:15:48 @agent_ppo2.py:191][0m |          -0.0090 |          13.8510 |           2.3909 |
[32m[20230205 18:15:48 @agent_ppo2.py:191][0m |          -0.0104 |          13.1541 |           2.3875 |
[32m[20230205 18:15:48 @agent_ppo2.py:191][0m |          -0.0066 |          12.2622 |           2.3891 |
[32m[20230205 18:15:48 @agent_ppo2.py:191][0m |          -0.0094 |          11.8663 |           2.3860 |
[32m[20230205 18:15:48 @agent_ppo2.py:191][0m |          -0.0115 |          11.7854 |           2.3874 |
[32m[20230205 18:15:48 @agent_ppo2.py:191][0m |          -0.0093 |          11.2915 |           2.3876 |
[32m[20230205 18:15:48 @agent_ppo2.py:191][0m |          -0.0144 |          10.7623 |           2.3858 |
[32m[20230205 18:15:48 @agent_ppo2.py:136][0m Policy update time: 0.46 s
[32m[20230205 18:15:49 @agent_ppo2.py:144][0m Average TRAINING episode reward: 15.14
[32m[20230205 18:15:49 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 108.54
[32m[20230205 18:15:49 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -9.92
[32m[20230205 18:15:49 @agent_ppo2.py:149][0m Total time:       2.36 min
[32m[20230205 18:15:49 @agent_ppo2.py:151][0m 135168 total steps have happened
[32m[20230205 18:15:49 @agent_ppo2.py:127][0m #------------------------ Iteration 66 --------------------------#
[32m[20230205 18:15:50 @agent_ppo2.py:133][0m Sampling time: 0.74 s by 1 slaves
[32m[20230205 18:15:50 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:50 @agent_ppo2.py:191][0m |          -0.0027 |          39.0043 |           2.3888 |
[32m[20230205 18:15:50 @agent_ppo2.py:191][0m |          -0.0065 |          33.9571 |           2.3861 |
[32m[20230205 18:15:50 @agent_ppo2.py:191][0m |          -0.0082 |          28.7588 |           2.3857 |
[32m[20230205 18:15:50 @agent_ppo2.py:191][0m |          -0.0103 |          26.0657 |           2.3866 |
[32m[20230205 18:15:50 @agent_ppo2.py:191][0m |          -0.0105 |          22.5509 |           2.3865 |
[32m[20230205 18:15:50 @agent_ppo2.py:191][0m |          -0.0112 |          17.9169 |           2.3868 |
[32m[20230205 18:15:50 @agent_ppo2.py:191][0m |          -0.0105 |          13.3133 |           2.3884 |
[32m[20230205 18:15:50 @agent_ppo2.py:191][0m |          -0.0103 |          10.5613 |           2.3889 |
[32m[20230205 18:15:50 @agent_ppo2.py:191][0m |          -0.0120 |           8.8396 |           2.3903 |
[32m[20230205 18:15:50 @agent_ppo2.py:191][0m |          -0.0120 |           7.2202 |           2.3893 |
[32m[20230205 18:15:50 @agent_ppo2.py:136][0m Policy update time: 0.72 s
[32m[20230205 18:15:51 @agent_ppo2.py:144][0m Average TRAINING episode reward: 59.31
[32m[20230205 18:15:51 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 147.03
[32m[20230205 18:15:51 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 16.91
[32m[20230205 18:15:51 @agent_ppo2.py:149][0m Total time:       2.39 min
[32m[20230205 18:15:51 @agent_ppo2.py:151][0m 137216 total steps have happened
[32m[20230205 18:15:51 @agent_ppo2.py:127][0m #------------------------ Iteration 67 --------------------------#
[32m[20230205 18:15:52 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:15:52 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:52 @agent_ppo2.py:191][0m |           0.0002 |           3.7810 |           2.4065 |
[32m[20230205 18:15:52 @agent_ppo2.py:191][0m |          -0.0042 |           2.9230 |           2.4062 |
[32m[20230205 18:15:52 @agent_ppo2.py:191][0m |          -0.0073 |           2.7088 |           2.4044 |
[32m[20230205 18:15:52 @agent_ppo2.py:191][0m |          -0.0091 |           2.5870 |           2.4049 |
[32m[20230205 18:15:52 @agent_ppo2.py:191][0m |          -0.0109 |           2.4831 |           2.4053 |
[32m[20230205 18:15:52 @agent_ppo2.py:191][0m |          -0.0116 |           2.4220 |           2.4058 |
[32m[20230205 18:15:52 @agent_ppo2.py:191][0m |          -0.0125 |           2.3622 |           2.4053 |
[32m[20230205 18:15:52 @agent_ppo2.py:191][0m |          -0.0133 |           2.3169 |           2.4065 |
[32m[20230205 18:15:52 @agent_ppo2.py:191][0m |          -0.0138 |           2.2758 |           2.4065 |
[32m[20230205 18:15:52 @agent_ppo2.py:191][0m |          -0.0142 |           2.2461 |           2.4065 |
[32m[20230205 18:15:52 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:15:53 @agent_ppo2.py:144][0m Average TRAINING episode reward: 126.02
[32m[20230205 18:15:53 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 143.28
[32m[20230205 18:15:53 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -110.98
[32m[20230205 18:15:53 @agent_ppo2.py:149][0m Total time:       2.43 min
[32m[20230205 18:15:53 @agent_ppo2.py:151][0m 139264 total steps have happened
[32m[20230205 18:15:53 @agent_ppo2.py:127][0m #------------------------ Iteration 68 --------------------------#
[32m[20230205 18:15:54 @agent_ppo2.py:133][0m Sampling time: 0.67 s by 1 slaves
[32m[20230205 18:15:54 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:54 @agent_ppo2.py:191][0m |          -0.0043 |          14.7448 |           2.3987 |
[32m[20230205 18:15:54 @agent_ppo2.py:191][0m |          -0.0071 |          10.3563 |           2.3936 |
[32m[20230205 18:15:54 @agent_ppo2.py:191][0m |          -0.0100 |           9.4691 |           2.3940 |
[32m[20230205 18:15:54 @agent_ppo2.py:191][0m |          -0.0112 |           8.6204 |           2.3923 |
[32m[20230205 18:15:54 @agent_ppo2.py:191][0m |          -0.0108 |           7.8276 |           2.3901 |
[32m[20230205 18:15:54 @agent_ppo2.py:191][0m |          -0.0115 |           7.2348 |           2.3902 |
[32m[20230205 18:15:54 @agent_ppo2.py:191][0m |          -0.0123 |           7.1067 |           2.3897 |
[32m[20230205 18:15:54 @agent_ppo2.py:191][0m |          -0.0110 |           6.6693 |           2.3871 |
[32m[20230205 18:15:54 @agent_ppo2.py:191][0m |          -0.0127 |           6.2047 |           2.3880 |
[32m[20230205 18:15:54 @agent_ppo2.py:191][0m |          -0.0124 |           6.1074 |           2.3870 |
[32m[20230205 18:15:54 @agent_ppo2.py:136][0m Policy update time: 0.65 s
[32m[20230205 18:15:55 @agent_ppo2.py:144][0m Average TRAINING episode reward: 98.09
[32m[20230205 18:15:55 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 164.68
[32m[20230205 18:15:55 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -2.81
[32m[20230205 18:15:55 @agent_ppo2.py:149][0m Total time:       2.46 min
[32m[20230205 18:15:55 @agent_ppo2.py:151][0m 141312 total steps have happened
[32m[20230205 18:15:55 @agent_ppo2.py:127][0m #------------------------ Iteration 69 --------------------------#
[32m[20230205 18:15:56 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:15:56 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:56 @agent_ppo2.py:191][0m |          -0.0015 |           4.6244 |           2.4480 |
[32m[20230205 18:15:56 @agent_ppo2.py:191][0m |          -0.0055 |           3.5596 |           2.4455 |
[32m[20230205 18:15:56 @agent_ppo2.py:191][0m |          -0.0069 |           3.3235 |           2.4450 |
[32m[20230205 18:15:56 @agent_ppo2.py:191][0m |          -0.0078 |           3.1823 |           2.4436 |
[32m[20230205 18:15:56 @agent_ppo2.py:191][0m |          -0.0087 |           3.0950 |           2.4441 |
[32m[20230205 18:15:56 @agent_ppo2.py:191][0m |          -0.0092 |           3.0213 |           2.4426 |
[32m[20230205 18:15:56 @agent_ppo2.py:191][0m |          -0.0104 |           2.9499 |           2.4446 |
[32m[20230205 18:15:56 @agent_ppo2.py:191][0m |          -0.0110 |           2.9105 |           2.4442 |
[32m[20230205 18:15:57 @agent_ppo2.py:191][0m |          -0.0109 |           2.8561 |           2.4446 |
[32m[20230205 18:15:57 @agent_ppo2.py:191][0m |          -0.0113 |           2.8276 |           2.4414 |
[32m[20230205 18:15:57 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:15:57 @agent_ppo2.py:144][0m Average TRAINING episode reward: 126.27
[32m[20230205 18:15:57 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 139.80
[32m[20230205 18:15:57 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 162.29
[32m[20230205 18:15:57 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 162.29
[32m[20230205 18:15:57 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 162.29
[32m[20230205 18:15:57 @agent_ppo2.py:149][0m Total time:       2.50 min
[32m[20230205 18:15:57 @agent_ppo2.py:151][0m 143360 total steps have happened
[32m[20230205 18:15:57 @agent_ppo2.py:127][0m #------------------------ Iteration 70 --------------------------#
[32m[20230205 18:15:58 @agent_ppo2.py:133][0m Sampling time: 0.68 s by 1 slaves
[32m[20230205 18:15:58 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:15:58 @agent_ppo2.py:191][0m |           0.0004 |          15.9915 |           2.4835 |
[32m[20230205 18:15:58 @agent_ppo2.py:191][0m |          -0.0026 |          11.7469 |           2.4815 |
[32m[20230205 18:15:58 @agent_ppo2.py:191][0m |          -0.0045 |          10.2993 |           2.4787 |
[32m[20230205 18:15:58 @agent_ppo2.py:191][0m |          -0.0053 |           9.3994 |           2.4763 |
[32m[20230205 18:15:58 @agent_ppo2.py:191][0m |          -0.0062 |           8.7954 |           2.4761 |
[32m[20230205 18:15:58 @agent_ppo2.py:191][0m |          -0.0071 |           8.0074 |           2.4738 |
[32m[20230205 18:15:58 @agent_ppo2.py:191][0m |          -0.0074 |           7.6129 |           2.4735 |
[32m[20230205 18:15:58 @agent_ppo2.py:191][0m |          -0.0079 |           7.5935 |           2.4729 |
[32m[20230205 18:15:58 @agent_ppo2.py:191][0m |          -0.0084 |           7.4065 |           2.4707 |
[32m[20230205 18:15:59 @agent_ppo2.py:191][0m |          -0.0084 |           7.0074 |           2.4705 |
[32m[20230205 18:15:59 @agent_ppo2.py:136][0m Policy update time: 0.66 s
[32m[20230205 18:15:59 @agent_ppo2.py:144][0m Average TRAINING episode reward: 72.81
[32m[20230205 18:15:59 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 120.73
[32m[20230205 18:15:59 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -8.30
[32m[20230205 18:15:59 @agent_ppo2.py:149][0m Total time:       2.53 min
[32m[20230205 18:15:59 @agent_ppo2.py:151][0m 145408 total steps have happened
[32m[20230205 18:15:59 @agent_ppo2.py:127][0m #------------------------ Iteration 71 --------------------------#
[32m[20230205 18:16:00 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:16:00 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:00 @agent_ppo2.py:191][0m |          -0.0006 |           4.5031 |           2.4766 |
[32m[20230205 18:16:00 @agent_ppo2.py:191][0m |          -0.0042 |           3.6000 |           2.4742 |
[32m[20230205 18:16:00 @agent_ppo2.py:191][0m |          -0.0068 |           3.3929 |           2.4768 |
[32m[20230205 18:16:00 @agent_ppo2.py:191][0m |          -0.0076 |           3.2411 |           2.4768 |
[32m[20230205 18:16:00 @agent_ppo2.py:191][0m |          -0.0088 |           3.1540 |           2.4783 |
[32m[20230205 18:16:00 @agent_ppo2.py:191][0m |          -0.0086 |           3.0875 |           2.4795 |
[32m[20230205 18:16:00 @agent_ppo2.py:191][0m |          -0.0100 |           2.9925 |           2.4797 |
[32m[20230205 18:16:00 @agent_ppo2.py:191][0m |          -0.0107 |           2.9190 |           2.4815 |
[32m[20230205 18:16:00 @agent_ppo2.py:191][0m |          -0.0113 |           2.8692 |           2.4830 |
[32m[20230205 18:16:00 @agent_ppo2.py:191][0m |          -0.0113 |           2.8070 |           2.4843 |
[32m[20230205 18:16:00 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:16:01 @agent_ppo2.py:144][0m Average TRAINING episode reward: 151.67
[32m[20230205 18:16:01 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 171.48
[32m[20230205 18:16:01 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 60.33
[32m[20230205 18:16:01 @agent_ppo2.py:149][0m Total time:       2.56 min
[32m[20230205 18:16:01 @agent_ppo2.py:151][0m 147456 total steps have happened
[32m[20230205 18:16:01 @agent_ppo2.py:127][0m #------------------------ Iteration 72 --------------------------#
[32m[20230205 18:16:02 @agent_ppo2.py:133][0m Sampling time: 0.83 s by 1 slaves
[32m[20230205 18:16:02 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:02 @agent_ppo2.py:191][0m |          -0.0000 |           7.6620 |           2.4973 |
[32m[20230205 18:16:02 @agent_ppo2.py:191][0m |          -0.0069 |           4.5852 |           2.4957 |
[32m[20230205 18:16:02 @agent_ppo2.py:191][0m |          -0.0076 |           4.3467 |           2.4945 |
[32m[20230205 18:16:02 @agent_ppo2.py:191][0m |          -0.0063 |           4.1505 |           2.4931 |
[32m[20230205 18:16:02 @agent_ppo2.py:191][0m |          -0.0101 |           4.0492 |           2.4926 |
[32m[20230205 18:16:02 @agent_ppo2.py:191][0m |          -0.0112 |           3.8561 |           2.4936 |
[32m[20230205 18:16:02 @agent_ppo2.py:191][0m |          -0.0101 |           3.8518 |           2.4917 |
[32m[20230205 18:16:02 @agent_ppo2.py:191][0m |          -0.0106 |           3.8071 |           2.4915 |
[32m[20230205 18:16:02 @agent_ppo2.py:191][0m |          -0.0129 |           3.6295 |           2.4908 |
[32m[20230205 18:16:03 @agent_ppo2.py:191][0m |          -0.0143 |           3.5729 |           2.4909 |
[32m[20230205 18:16:03 @agent_ppo2.py:136][0m Policy update time: 0.79 s
[32m[20230205 18:16:03 @agent_ppo2.py:144][0m Average TRAINING episode reward: 73.19
[32m[20230205 18:16:03 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 150.87
[32m[20230205 18:16:03 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -31.56
[32m[20230205 18:16:03 @agent_ppo2.py:149][0m Total time:       2.59 min
[32m[20230205 18:16:03 @agent_ppo2.py:151][0m 149504 total steps have happened
[32m[20230205 18:16:03 @agent_ppo2.py:127][0m #------------------------ Iteration 73 --------------------------#
[32m[20230205 18:16:04 @agent_ppo2.py:133][0m Sampling time: 0.82 s by 1 slaves
[32m[20230205 18:16:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:04 @agent_ppo2.py:191][0m |           0.0008 |          16.8129 |           2.4633 |
[32m[20230205 18:16:04 @agent_ppo2.py:191][0m |          -0.0032 |          12.0390 |           2.4584 |
[32m[20230205 18:16:04 @agent_ppo2.py:191][0m |          -0.0037 |          10.1592 |           2.4583 |
[32m[20230205 18:16:04 @agent_ppo2.py:191][0m |          -0.0049 |           9.5078 |           2.4582 |
[32m[20230205 18:16:04 @agent_ppo2.py:191][0m |          -0.0069 |           8.8857 |           2.4574 |
[32m[20230205 18:16:04 @agent_ppo2.py:191][0m |          -0.0063 |           8.4808 |           2.4556 |
[32m[20230205 18:16:04 @agent_ppo2.py:191][0m |          -0.0064 |           8.1937 |           2.4559 |
[32m[20230205 18:16:05 @agent_ppo2.py:191][0m |          -0.0049 |           7.8519 |           2.4558 |
[32m[20230205 18:16:05 @agent_ppo2.py:191][0m |          -0.0080 |           7.5303 |           2.4549 |
[32m[20230205 18:16:05 @agent_ppo2.py:191][0m |          -0.0084 |           7.3442 |           2.4549 |
[32m[20230205 18:16:05 @agent_ppo2.py:136][0m Policy update time: 0.76 s
[32m[20230205 18:16:05 @agent_ppo2.py:144][0m Average TRAINING episode reward: 36.13
[32m[20230205 18:16:05 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 146.02
[32m[20230205 18:16:05 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 60.23
[32m[20230205 18:16:05 @agent_ppo2.py:149][0m Total time:       2.63 min
[32m[20230205 18:16:05 @agent_ppo2.py:151][0m 151552 total steps have happened
[32m[20230205 18:16:05 @agent_ppo2.py:127][0m #------------------------ Iteration 74 --------------------------#
[32m[20230205 18:16:06 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:16:06 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:06 @agent_ppo2.py:191][0m |           0.0002 |           3.3960 |           2.4818 |
[32m[20230205 18:16:06 @agent_ppo2.py:191][0m |          -0.0038 |           2.4869 |           2.4770 |
[32m[20230205 18:16:06 @agent_ppo2.py:191][0m |          -0.0061 |           2.3022 |           2.4770 |
[32m[20230205 18:16:06 @agent_ppo2.py:191][0m |          -0.0078 |           2.2044 |           2.4775 |
[32m[20230205 18:16:06 @agent_ppo2.py:191][0m |          -0.0088 |           2.1257 |           2.4776 |
[32m[20230205 18:16:06 @agent_ppo2.py:191][0m |          -0.0099 |           2.0793 |           2.4807 |
[32m[20230205 18:16:06 @agent_ppo2.py:191][0m |          -0.0106 |           2.0399 |           2.4807 |
[32m[20230205 18:16:06 @agent_ppo2.py:191][0m |          -0.0106 |           2.0041 |           2.4853 |
[32m[20230205 18:16:06 @agent_ppo2.py:191][0m |          -0.0118 |           1.9708 |           2.4831 |
[32m[20230205 18:16:07 @agent_ppo2.py:191][0m |          -0.0124 |           1.9427 |           2.4845 |
[32m[20230205 18:16:07 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:16:07 @agent_ppo2.py:144][0m Average TRAINING episode reward: 158.68
[32m[20230205 18:16:07 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 159.63
[32m[20230205 18:16:07 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 194.97
[32m[20230205 18:16:07 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 194.97
[32m[20230205 18:16:07 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 194.97
[32m[20230205 18:16:07 @agent_ppo2.py:149][0m Total time:       2.66 min
[32m[20230205 18:16:07 @agent_ppo2.py:151][0m 153600 total steps have happened
[32m[20230205 18:16:07 @agent_ppo2.py:127][0m #------------------------ Iteration 75 --------------------------#
[32m[20230205 18:16:08 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:16:08 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:08 @agent_ppo2.py:191][0m |          -0.0019 |          14.2105 |           2.5034 |
[32m[20230205 18:16:08 @agent_ppo2.py:191][0m |          -0.0037 |           8.9212 |           2.5022 |
[32m[20230205 18:16:08 @agent_ppo2.py:191][0m |          -0.0045 |           7.8269 |           2.5004 |
[32m[20230205 18:16:08 @agent_ppo2.py:191][0m |          -0.0088 |           7.2946 |           2.4998 |
[32m[20230205 18:16:08 @agent_ppo2.py:191][0m |          -0.0099 |           7.0396 |           2.5003 |
[32m[20230205 18:16:08 @agent_ppo2.py:191][0m |          -0.0090 |           6.8586 |           2.5011 |
[32m[20230205 18:16:08 @agent_ppo2.py:191][0m |          -0.0082 |           6.5431 |           2.4993 |
[32m[20230205 18:16:08 @agent_ppo2.py:191][0m |          -0.0115 |           6.5124 |           2.4999 |
[32m[20230205 18:16:08 @agent_ppo2.py:191][0m |          -0.0110 |           6.3304 |           2.4995 |
[32m[20230205 18:16:08 @agent_ppo2.py:191][0m |          -0.0123 |           6.2151 |           2.4987 |
[32m[20230205 18:16:08 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:16:09 @agent_ppo2.py:144][0m Average TRAINING episode reward: 59.43
[32m[20230205 18:16:09 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 159.83
[32m[20230205 18:16:09 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 129.99
[32m[20230205 18:16:09 @agent_ppo2.py:149][0m Total time:       2.69 min
[32m[20230205 18:16:09 @agent_ppo2.py:151][0m 155648 total steps have happened
[32m[20230205 18:16:09 @agent_ppo2.py:127][0m #------------------------ Iteration 76 --------------------------#
[32m[20230205 18:16:10 @agent_ppo2.py:133][0m Sampling time: 0.66 s by 1 slaves
[32m[20230205 18:16:10 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:10 @agent_ppo2.py:191][0m |          -0.0034 |          18.0065 |           2.3904 |
[32m[20230205 18:16:10 @agent_ppo2.py:191][0m |          -0.0055 |          11.5140 |           2.3874 |
[32m[20230205 18:16:10 @agent_ppo2.py:191][0m |          -0.0095 |           9.3910 |           2.3843 |
[32m[20230205 18:16:10 @agent_ppo2.py:191][0m |          -0.0145 |           8.5381 |           2.3860 |
[32m[20230205 18:16:10 @agent_ppo2.py:191][0m |          -0.0125 |           8.1630 |           2.3844 |
[32m[20230205 18:16:10 @agent_ppo2.py:191][0m |          -0.0162 |           7.8042 |           2.3844 |
[32m[20230205 18:16:10 @agent_ppo2.py:191][0m |          -0.0164 |           7.5001 |           2.3827 |
[32m[20230205 18:16:10 @agent_ppo2.py:191][0m |          -0.0182 |           7.3184 |           2.3821 |
[32m[20230205 18:16:10 @agent_ppo2.py:191][0m |          -0.0100 |           7.0962 |           2.3824 |
[32m[20230205 18:16:10 @agent_ppo2.py:191][0m |          -0.0174 |           6.8325 |           2.3818 |
[32m[20230205 18:16:10 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:16:11 @agent_ppo2.py:144][0m Average TRAINING episode reward: 57.99
[32m[20230205 18:16:11 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 114.82
[32m[20230205 18:16:11 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 129.30
[32m[20230205 18:16:11 @agent_ppo2.py:149][0m Total time:       2.72 min
[32m[20230205 18:16:11 @agent_ppo2.py:151][0m 157696 total steps have happened
[32m[20230205 18:16:11 @agent_ppo2.py:127][0m #------------------------ Iteration 77 --------------------------#
[32m[20230205 18:16:11 @agent_ppo2.py:133][0m Sampling time: 0.80 s by 1 slaves
[32m[20230205 18:16:12 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:12 @agent_ppo2.py:191][0m |           0.0011 |          21.4672 |           2.4441 |
[32m[20230205 18:16:12 @agent_ppo2.py:191][0m |           0.0024 |          18.7460 |           2.4423 |
[32m[20230205 18:16:12 @agent_ppo2.py:191][0m |           0.0148 |          19.8076 |           2.4415 |
[32m[20230205 18:16:12 @agent_ppo2.py:191][0m |          -0.0139 |          17.1090 |           2.4432 |
[32m[20230205 18:16:12 @agent_ppo2.py:191][0m |          -0.0105 |          15.5398 |           2.4427 |
[32m[20230205 18:16:12 @agent_ppo2.py:191][0m |          -0.0137 |          15.0905 |           2.4427 |
[32m[20230205 18:16:12 @agent_ppo2.py:191][0m |           0.0011 |          17.2371 |           2.4425 |
[32m[20230205 18:16:12 @agent_ppo2.py:191][0m |          -0.0119 |          14.3012 |           2.4422 |
[32m[20230205 18:16:12 @agent_ppo2.py:191][0m |          -0.0099 |          13.8932 |           2.4421 |
[32m[20230205 18:16:12 @agent_ppo2.py:191][0m |          -0.0011 |          16.8043 |           2.4425 |
[32m[20230205 18:16:12 @agent_ppo2.py:136][0m Policy update time: 0.78 s
[32m[20230205 18:16:13 @agent_ppo2.py:144][0m Average TRAINING episode reward: 79.90
[32m[20230205 18:16:13 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 171.30
[32m[20230205 18:16:13 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 145.76
[32m[20230205 18:16:13 @agent_ppo2.py:149][0m Total time:       2.76 min
[32m[20230205 18:16:13 @agent_ppo2.py:151][0m 159744 total steps have happened
[32m[20230205 18:16:13 @agent_ppo2.py:127][0m #------------------------ Iteration 78 --------------------------#
[32m[20230205 18:16:13 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:16:13 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:14 @agent_ppo2.py:191][0m |          -0.0039 |          21.0748 |           2.4442 |
[32m[20230205 18:16:14 @agent_ppo2.py:191][0m |           0.0081 |           8.9520 |           2.4418 |
[32m[20230205 18:16:14 @agent_ppo2.py:191][0m |          -0.0028 |           7.9350 |           2.4414 |
[32m[20230205 18:16:14 @agent_ppo2.py:191][0m |          -0.0075 |           6.7507 |           2.4386 |
[32m[20230205 18:16:14 @agent_ppo2.py:191][0m |          -0.0076 |           6.4328 |           2.4384 |
[32m[20230205 18:16:14 @agent_ppo2.py:191][0m |          -0.0109 |           6.1354 |           2.4377 |
[32m[20230205 18:16:14 @agent_ppo2.py:191][0m |          -0.0096 |           5.9499 |           2.4383 |
[32m[20230205 18:16:14 @agent_ppo2.py:191][0m |          -0.0114 |           5.9468 |           2.4381 |
[32m[20230205 18:16:14 @agent_ppo2.py:191][0m |          -0.0132 |           5.7077 |           2.4364 |
[32m[20230205 18:16:14 @agent_ppo2.py:191][0m |          -0.0124 |           5.6632 |           2.4356 |
[32m[20230205 18:16:14 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:16:15 @agent_ppo2.py:144][0m Average TRAINING episode reward: 70.67
[32m[20230205 18:16:15 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 156.01
[32m[20230205 18:16:15 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 247.69
[32m[20230205 18:16:15 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 247.69
[32m[20230205 18:16:15 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 247.69
[32m[20230205 18:16:15 @agent_ppo2.py:149][0m Total time:       2.79 min
[32m[20230205 18:16:15 @agent_ppo2.py:151][0m 161792 total steps have happened
[32m[20230205 18:16:15 @agent_ppo2.py:127][0m #------------------------ Iteration 79 --------------------------#
[32m[20230205 18:16:15 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:16:15 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:16 @agent_ppo2.py:191][0m |           0.0005 |           4.8859 |           2.5634 |
[32m[20230205 18:16:16 @agent_ppo2.py:191][0m |          -0.0048 |           4.0401 |           2.5589 |
[32m[20230205 18:16:16 @agent_ppo2.py:191][0m |          -0.0084 |           3.7430 |           2.5565 |
[32m[20230205 18:16:16 @agent_ppo2.py:191][0m |          -0.0104 |           3.5696 |           2.5566 |
[32m[20230205 18:16:16 @agent_ppo2.py:191][0m |          -0.0118 |           3.4451 |           2.5548 |
[32m[20230205 18:16:16 @agent_ppo2.py:191][0m |          -0.0131 |           3.3161 |           2.5540 |
[32m[20230205 18:16:16 @agent_ppo2.py:191][0m |          -0.0126 |           3.2163 |           2.5513 |
[32m[20230205 18:16:16 @agent_ppo2.py:191][0m |          -0.0137 |           3.1493 |           2.5519 |
[32m[20230205 18:16:16 @agent_ppo2.py:191][0m |          -0.0141 |           3.0916 |           2.5515 |
[32m[20230205 18:16:16 @agent_ppo2.py:191][0m |          -0.0143 |           3.0431 |           2.5499 |
[32m[20230205 18:16:16 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:16:17 @agent_ppo2.py:144][0m Average TRAINING episode reward: 136.70
[32m[20230205 18:16:17 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 148.66
[32m[20230205 18:16:17 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 132.60
[32m[20230205 18:16:17 @agent_ppo2.py:149][0m Total time:       2.83 min
[32m[20230205 18:16:17 @agent_ppo2.py:151][0m 163840 total steps have happened
[32m[20230205 18:16:17 @agent_ppo2.py:127][0m #------------------------ Iteration 80 --------------------------#
[32m[20230205 18:16:18 @agent_ppo2.py:133][0m Sampling time: 0.75 s by 1 slaves
[32m[20230205 18:16:18 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:18 @agent_ppo2.py:191][0m |          -0.0008 |          62.0989 |           2.5217 |
[32m[20230205 18:16:18 @agent_ppo2.py:191][0m |          -0.0065 |          32.3113 |           2.5196 |
[32m[20230205 18:16:18 @agent_ppo2.py:191][0m |          -0.0102 |          22.1385 |           2.5157 |
[32m[20230205 18:16:18 @agent_ppo2.py:191][0m |          -0.0112 |          17.9517 |           2.5147 |
[32m[20230205 18:16:18 @agent_ppo2.py:191][0m |          -0.0119 |          15.3727 |           2.5163 |
[32m[20230205 18:16:18 @agent_ppo2.py:191][0m |          -0.0124 |          13.3679 |           2.5139 |
[32m[20230205 18:16:18 @agent_ppo2.py:191][0m |          -0.0124 |          11.9315 |           2.5131 |
[32m[20230205 18:16:18 @agent_ppo2.py:191][0m |          -0.0128 |          10.6453 |           2.5127 |
[32m[20230205 18:16:18 @agent_ppo2.py:191][0m |          -0.0126 |           9.4920 |           2.5122 |
[32m[20230205 18:16:18 @agent_ppo2.py:191][0m |          -0.0134 |           8.9137 |           2.5123 |
[32m[20230205 18:16:18 @agent_ppo2.py:136][0m Policy update time: 0.72 s
[32m[20230205 18:16:19 @agent_ppo2.py:144][0m Average TRAINING episode reward: 21.95
[32m[20230205 18:16:19 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 158.95
[32m[20230205 18:16:19 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -9.51
[32m[20230205 18:16:19 @agent_ppo2.py:149][0m Total time:       2.86 min
[32m[20230205 18:16:19 @agent_ppo2.py:151][0m 165888 total steps have happened
[32m[20230205 18:16:19 @agent_ppo2.py:127][0m #------------------------ Iteration 81 --------------------------#
[32m[20230205 18:16:20 @agent_ppo2.py:133][0m Sampling time: 0.75 s by 1 slaves
[32m[20230205 18:16:20 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:20 @agent_ppo2.py:191][0m |           0.0012 |           3.8334 |           2.4888 |
[32m[20230205 18:16:20 @agent_ppo2.py:191][0m |          -0.0041 |           3.2615 |           2.4869 |
[32m[20230205 18:16:20 @agent_ppo2.py:191][0m |          -0.0083 |           3.0724 |           2.4804 |
[32m[20230205 18:16:20 @agent_ppo2.py:191][0m |          -0.0100 |           2.9371 |           2.4787 |
[32m[20230205 18:16:20 @agent_ppo2.py:191][0m |          -0.0110 |           2.8442 |           2.4760 |
[32m[20230205 18:16:20 @agent_ppo2.py:191][0m |          -0.0115 |           2.7693 |           2.4753 |
[32m[20230205 18:16:20 @agent_ppo2.py:191][0m |          -0.0121 |           2.7046 |           2.4742 |
[32m[20230205 18:16:20 @agent_ppo2.py:191][0m |          -0.0125 |           2.6544 |           2.4758 |
[32m[20230205 18:16:20 @agent_ppo2.py:191][0m |          -0.0129 |           2.6080 |           2.4747 |
[32m[20230205 18:16:20 @agent_ppo2.py:191][0m |          -0.0132 |           2.5646 |           2.4735 |
[32m[20230205 18:16:20 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:16:21 @agent_ppo2.py:144][0m Average TRAINING episode reward: 166.22
[32m[20230205 18:16:21 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 179.24
[32m[20230205 18:16:21 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 237.91
[32m[20230205 18:16:21 @agent_ppo2.py:149][0m Total time:       2.89 min
[32m[20230205 18:16:21 @agent_ppo2.py:151][0m 167936 total steps have happened
[32m[20230205 18:16:21 @agent_ppo2.py:127][0m #------------------------ Iteration 82 --------------------------#
[32m[20230205 18:16:22 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:16:22 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:22 @agent_ppo2.py:191][0m |          -0.0002 |          48.8203 |           2.4612 |
[32m[20230205 18:16:22 @agent_ppo2.py:191][0m |          -0.0065 |          24.2564 |           2.4564 |
[32m[20230205 18:16:22 @agent_ppo2.py:191][0m |          -0.0093 |          20.0137 |           2.4557 |
[32m[20230205 18:16:22 @agent_ppo2.py:191][0m |          -0.0102 |          18.5994 |           2.4521 |
[32m[20230205 18:16:22 @agent_ppo2.py:191][0m |          -0.0075 |          16.4587 |           2.4505 |
[32m[20230205 18:16:22 @agent_ppo2.py:191][0m |          -0.0148 |          16.0178 |           2.4500 |
[32m[20230205 18:16:22 @agent_ppo2.py:191][0m |          -0.0114 |          15.1892 |           2.4490 |
[32m[20230205 18:16:22 @agent_ppo2.py:191][0m |          -0.0164 |          14.2321 |           2.4499 |
[32m[20230205 18:16:22 @agent_ppo2.py:191][0m |          -0.0159 |          13.6792 |           2.4489 |
[32m[20230205 18:16:22 @agent_ppo2.py:191][0m |          -0.0182 |          13.1326 |           2.4483 |
[32m[20230205 18:16:22 @agent_ppo2.py:136][0m Policy update time: 0.47 s
[32m[20230205 18:16:23 @agent_ppo2.py:144][0m Average TRAINING episode reward: -27.58
[32m[20230205 18:16:23 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 33.80
[32m[20230205 18:16:23 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -79.88
[32m[20230205 18:16:23 @agent_ppo2.py:149][0m Total time:       2.92 min
[32m[20230205 18:16:23 @agent_ppo2.py:151][0m 169984 total steps have happened
[32m[20230205 18:16:23 @agent_ppo2.py:127][0m #------------------------ Iteration 83 --------------------------#
[32m[20230205 18:16:23 @agent_ppo2.py:133][0m Sampling time: 0.70 s by 1 slaves
[32m[20230205 18:16:23 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:24 @agent_ppo2.py:191][0m |          -0.0046 |          27.1155 |           2.4983 |
[32m[20230205 18:16:24 @agent_ppo2.py:191][0m |          -0.0029 |          12.8412 |           2.4954 |
[32m[20230205 18:16:24 @agent_ppo2.py:191][0m |          -0.0056 |           9.9297 |           2.4934 |
[32m[20230205 18:16:24 @agent_ppo2.py:191][0m |          -0.0097 |           8.5094 |           2.4936 |
[32m[20230205 18:16:24 @agent_ppo2.py:191][0m |          -0.0055 |           7.6708 |           2.4937 |
[32m[20230205 18:16:24 @agent_ppo2.py:191][0m |          -0.0031 |           7.1998 |           2.4942 |
[32m[20230205 18:16:24 @agent_ppo2.py:191][0m |          -0.0058 |           6.9558 |           2.4928 |
[32m[20230205 18:16:24 @agent_ppo2.py:191][0m |          -0.0134 |           6.3575 |           2.4925 |
[32m[20230205 18:16:24 @agent_ppo2.py:191][0m |          -0.0165 |           6.1569 |           2.4903 |
[32m[20230205 18:16:24 @agent_ppo2.py:191][0m |          -0.0060 |           5.9840 |           2.4902 |
[32m[20230205 18:16:24 @agent_ppo2.py:136][0m Policy update time: 0.68 s
[32m[20230205 18:16:25 @agent_ppo2.py:144][0m Average TRAINING episode reward: 102.70
[32m[20230205 18:16:25 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 158.56
[32m[20230205 18:16:25 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 87.47
[32m[20230205 18:16:25 @agent_ppo2.py:149][0m Total time:       2.96 min
[32m[20230205 18:16:25 @agent_ppo2.py:151][0m 172032 total steps have happened
[32m[20230205 18:16:25 @agent_ppo2.py:127][0m #------------------------ Iteration 84 --------------------------#
[32m[20230205 18:16:25 @agent_ppo2.py:133][0m Sampling time: 0.66 s by 1 slaves
[32m[20230205 18:16:25 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:25 @agent_ppo2.py:191][0m |          -0.0024 |          14.1191 |           2.5115 |
[32m[20230205 18:16:26 @agent_ppo2.py:191][0m |          -0.0022 |           8.3953 |           2.5100 |
[32m[20230205 18:16:26 @agent_ppo2.py:191][0m |          -0.0071 |           6.7689 |           2.5081 |
[32m[20230205 18:16:26 @agent_ppo2.py:191][0m |          -0.0072 |           6.1691 |           2.5069 |
[32m[20230205 18:16:26 @agent_ppo2.py:191][0m |          -0.0063 |           5.5366 |           2.5076 |
[32m[20230205 18:16:26 @agent_ppo2.py:191][0m |          -0.0088 |           5.2868 |           2.5059 |
[32m[20230205 18:16:26 @agent_ppo2.py:191][0m |          -0.0059 |           5.0489 |           2.5067 |
[32m[20230205 18:16:26 @agent_ppo2.py:191][0m |          -0.0104 |           4.8449 |           2.5074 |
[32m[20230205 18:16:26 @agent_ppo2.py:191][0m |          -0.0130 |           4.6964 |           2.5087 |
[32m[20230205 18:16:26 @agent_ppo2.py:191][0m |          -0.0086 |           4.7277 |           2.5083 |
[32m[20230205 18:16:26 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:16:27 @agent_ppo2.py:144][0m Average TRAINING episode reward: 97.24
[32m[20230205 18:16:27 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 158.14
[32m[20230205 18:16:27 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 102.35
[32m[20230205 18:16:27 @agent_ppo2.py:149][0m Total time:       2.99 min
[32m[20230205 18:16:27 @agent_ppo2.py:151][0m 174080 total steps have happened
[32m[20230205 18:16:27 @agent_ppo2.py:127][0m #------------------------ Iteration 85 --------------------------#
[32m[20230205 18:16:27 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:16:27 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:27 @agent_ppo2.py:191][0m |           0.0004 |           2.9454 |           2.4744 |
[32m[20230205 18:16:27 @agent_ppo2.py:191][0m |          -0.0025 |           2.5984 |           2.4728 |
[32m[20230205 18:16:28 @agent_ppo2.py:191][0m |          -0.0047 |           2.4630 |           2.4723 |
[32m[20230205 18:16:28 @agent_ppo2.py:191][0m |          -0.0060 |           2.3763 |           2.4739 |
[32m[20230205 18:16:28 @agent_ppo2.py:191][0m |          -0.0071 |           2.3117 |           2.4730 |
[32m[20230205 18:16:28 @agent_ppo2.py:191][0m |          -0.0082 |           2.2642 |           2.4738 |
[32m[20230205 18:16:28 @agent_ppo2.py:191][0m |          -0.0089 |           2.2261 |           2.4747 |
[32m[20230205 18:16:28 @agent_ppo2.py:191][0m |          -0.0099 |           2.1897 |           2.4741 |
[32m[20230205 18:16:28 @agent_ppo2.py:191][0m |          -0.0106 |           2.1579 |           2.4755 |
[32m[20230205 18:16:28 @agent_ppo2.py:191][0m |          -0.0110 |           2.1337 |           2.4748 |
[32m[20230205 18:16:28 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:16:29 @agent_ppo2.py:144][0m Average TRAINING episode reward: 109.85
[32m[20230205 18:16:29 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 118.32
[32m[20230205 18:16:29 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 159.85
[32m[20230205 18:16:29 @agent_ppo2.py:149][0m Total time:       3.02 min
[32m[20230205 18:16:29 @agent_ppo2.py:151][0m 176128 total steps have happened
[32m[20230205 18:16:29 @agent_ppo2.py:127][0m #------------------------ Iteration 86 --------------------------#
[32m[20230205 18:16:29 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:16:29 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:29 @agent_ppo2.py:191][0m |          -0.0004 |           2.9404 |           2.5231 |
[32m[20230205 18:16:30 @agent_ppo2.py:191][0m |          -0.0058 |           2.7046 |           2.5214 |
[32m[20230205 18:16:30 @agent_ppo2.py:191][0m |          -0.0076 |           2.6137 |           2.5199 |
[32m[20230205 18:16:30 @agent_ppo2.py:191][0m |          -0.0087 |           2.5549 |           2.5219 |
[32m[20230205 18:16:30 @agent_ppo2.py:191][0m |          -0.0096 |           2.5078 |           2.5219 |
[32m[20230205 18:16:30 @agent_ppo2.py:191][0m |          -0.0105 |           2.4705 |           2.5239 |
[32m[20230205 18:16:30 @agent_ppo2.py:191][0m |          -0.0114 |           2.4326 |           2.5232 |
[32m[20230205 18:16:30 @agent_ppo2.py:191][0m |          -0.0118 |           2.4038 |           2.5239 |
[32m[20230205 18:16:30 @agent_ppo2.py:191][0m |          -0.0124 |           2.3737 |           2.5252 |
[32m[20230205 18:16:30 @agent_ppo2.py:191][0m |          -0.0123 |           2.3533 |           2.5251 |
[32m[20230205 18:16:30 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:16:31 @agent_ppo2.py:144][0m Average TRAINING episode reward: 129.94
[32m[20230205 18:16:31 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 148.64
[32m[20230205 18:16:31 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 59.88
[32m[20230205 18:16:31 @agent_ppo2.py:149][0m Total time:       3.05 min
[32m[20230205 18:16:31 @agent_ppo2.py:151][0m 178176 total steps have happened
[32m[20230205 18:16:31 @agent_ppo2.py:127][0m #------------------------ Iteration 87 --------------------------#
[32m[20230205 18:16:31 @agent_ppo2.py:133][0m Sampling time: 0.76 s by 1 slaves
[32m[20230205 18:16:31 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:32 @agent_ppo2.py:191][0m |           0.0006 |          10.3724 |           2.4918 |
[32m[20230205 18:16:32 @agent_ppo2.py:191][0m |          -0.0042 |           5.7549 |           2.4920 |
[32m[20230205 18:16:32 @agent_ppo2.py:191][0m |          -0.0063 |           5.2765 |           2.4911 |
[32m[20230205 18:16:32 @agent_ppo2.py:191][0m |          -0.0088 |           5.1723 |           2.4894 |
[32m[20230205 18:16:32 @agent_ppo2.py:191][0m |          -0.0017 |           4.7619 |           2.4876 |
[32m[20230205 18:16:32 @agent_ppo2.py:191][0m |          -0.0066 |           4.4585 |           2.4867 |
[32m[20230205 18:16:32 @agent_ppo2.py:191][0m |          -0.0082 |           4.4157 |           2.4893 |
[32m[20230205 18:16:32 @agent_ppo2.py:191][0m |          -0.0056 |           4.3181 |           2.4890 |
[32m[20230205 18:16:32 @agent_ppo2.py:191][0m |          -0.0110 |           4.1577 |           2.4865 |
[32m[20230205 18:16:32 @agent_ppo2.py:191][0m |          -0.0095 |           4.1271 |           2.4884 |
[32m[20230205 18:16:32 @agent_ppo2.py:136][0m Policy update time: 0.74 s
[32m[20230205 18:16:33 @agent_ppo2.py:144][0m Average TRAINING episode reward: 67.09
[32m[20230205 18:16:33 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 151.15
[32m[20230205 18:16:33 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 230.21
[32m[20230205 18:16:33 @agent_ppo2.py:149][0m Total time:       3.09 min
[32m[20230205 18:16:33 @agent_ppo2.py:151][0m 180224 total steps have happened
[32m[20230205 18:16:33 @agent_ppo2.py:127][0m #------------------------ Iteration 88 --------------------------#
[32m[20230205 18:16:34 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:16:34 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:34 @agent_ppo2.py:191][0m |          -0.0005 |           2.1746 |           2.5495 |
[32m[20230205 18:16:34 @agent_ppo2.py:191][0m |          -0.0056 |           2.0406 |           2.5471 |
[32m[20230205 18:16:34 @agent_ppo2.py:191][0m |          -0.0082 |           1.9751 |           2.5471 |
[32m[20230205 18:16:34 @agent_ppo2.py:191][0m |          -0.0096 |           1.9346 |           2.5459 |
[32m[20230205 18:16:34 @agent_ppo2.py:191][0m |          -0.0105 |           1.9010 |           2.5462 |
[32m[20230205 18:16:34 @agent_ppo2.py:191][0m |          -0.0113 |           1.8782 |           2.5477 |
[32m[20230205 18:16:34 @agent_ppo2.py:191][0m |          -0.0122 |           1.8573 |           2.5492 |
[32m[20230205 18:16:34 @agent_ppo2.py:191][0m |          -0.0126 |           1.8368 |           2.5498 |
[32m[20230205 18:16:34 @agent_ppo2.py:191][0m |          -0.0133 |           1.8262 |           2.5506 |
[32m[20230205 18:16:34 @agent_ppo2.py:191][0m |          -0.0135 |           1.8055 |           2.5505 |
[32m[20230205 18:16:34 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:16:35 @agent_ppo2.py:144][0m Average TRAINING episode reward: 150.98
[32m[20230205 18:16:35 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 163.71
[32m[20230205 18:16:35 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 141.29
[32m[20230205 18:16:35 @agent_ppo2.py:149][0m Total time:       3.13 min
[32m[20230205 18:16:35 @agent_ppo2.py:151][0m 182272 total steps have happened
[32m[20230205 18:16:35 @agent_ppo2.py:127][0m #------------------------ Iteration 89 --------------------------#
[32m[20230205 18:16:36 @agent_ppo2.py:133][0m Sampling time: 0.70 s by 1 slaves
[32m[20230205 18:16:36 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:36 @agent_ppo2.py:191][0m |          -0.0004 |           9.3417 |           2.6220 |
[32m[20230205 18:16:36 @agent_ppo2.py:191][0m |          -0.0024 |           6.2909 |           2.6200 |
[32m[20230205 18:16:36 @agent_ppo2.py:191][0m |          -0.0055 |           5.8649 |           2.6203 |
[32m[20230205 18:16:36 @agent_ppo2.py:191][0m |          -0.0066 |           5.3686 |           2.6205 |
[32m[20230205 18:16:36 @agent_ppo2.py:191][0m |          -0.0083 |           5.1778 |           2.6212 |
[32m[20230205 18:16:36 @agent_ppo2.py:191][0m |          -0.0089 |           4.9959 |           2.6192 |
[32m[20230205 18:16:36 @agent_ppo2.py:191][0m |          -0.0094 |           4.8875 |           2.6188 |
[32m[20230205 18:16:36 @agent_ppo2.py:191][0m |          -0.0110 |           4.7721 |           2.6194 |
[32m[20230205 18:16:36 @agent_ppo2.py:191][0m |          -0.0120 |           4.7502 |           2.6187 |
[32m[20230205 18:16:36 @agent_ppo2.py:191][0m |          -0.0127 |           4.6804 |           2.6192 |
[32m[20230205 18:16:36 @agent_ppo2.py:136][0m Policy update time: 0.66 s
[32m[20230205 18:16:37 @agent_ppo2.py:144][0m Average TRAINING episode reward: 115.42
[32m[20230205 18:16:37 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 176.45
[32m[20230205 18:16:37 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 126.96
[32m[20230205 18:16:37 @agent_ppo2.py:149][0m Total time:       3.16 min
[32m[20230205 18:16:37 @agent_ppo2.py:151][0m 184320 total steps have happened
[32m[20230205 18:16:37 @agent_ppo2.py:127][0m #------------------------ Iteration 90 --------------------------#
[32m[20230205 18:16:38 @agent_ppo2.py:133][0m Sampling time: 0.69 s by 1 slaves
[32m[20230205 18:16:38 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:38 @agent_ppo2.py:191][0m |          -0.0000 |          12.2491 |           2.6494 |
[32m[20230205 18:16:38 @agent_ppo2.py:191][0m |          -0.0036 |           8.0157 |           2.6456 |
[32m[20230205 18:16:38 @agent_ppo2.py:191][0m |          -0.0062 |           7.0533 |           2.6435 |
[32m[20230205 18:16:38 @agent_ppo2.py:191][0m |          -0.0061 |           6.6258 |           2.6426 |
[32m[20230205 18:16:38 @agent_ppo2.py:191][0m |          -0.0051 |           6.6901 |           2.6405 |
[32m[20230205 18:16:38 @agent_ppo2.py:191][0m |          -0.0091 |           6.1849 |           2.6408 |
[32m[20230205 18:16:38 @agent_ppo2.py:191][0m |          -0.0064 |           5.9108 |           2.6398 |
[32m[20230205 18:16:38 @agent_ppo2.py:191][0m |          -0.0106 |           5.7460 |           2.6394 |
[32m[20230205 18:16:38 @agent_ppo2.py:191][0m |          -0.0093 |           5.5324 |           2.6404 |
[32m[20230205 18:16:38 @agent_ppo2.py:191][0m |          -0.0111 |           5.3567 |           2.6400 |
[32m[20230205 18:16:38 @agent_ppo2.py:136][0m Policy update time: 0.67 s
[32m[20230205 18:16:39 @agent_ppo2.py:144][0m Average TRAINING episode reward: 107.64
[32m[20230205 18:16:39 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 159.55
[32m[20230205 18:16:39 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 255.50
[32m[20230205 18:16:39 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 255.50
[32m[20230205 18:16:39 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 255.50
[32m[20230205 18:16:39 @agent_ppo2.py:149][0m Total time:       3.19 min
[32m[20230205 18:16:39 @agent_ppo2.py:151][0m 186368 total steps have happened
[32m[20230205 18:16:39 @agent_ppo2.py:127][0m #------------------------ Iteration 91 --------------------------#
[32m[20230205 18:16:40 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:16:40 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:40 @agent_ppo2.py:191][0m |           0.0008 |           3.3956 |           2.6039 |
[32m[20230205 18:16:40 @agent_ppo2.py:191][0m |          -0.0020 |           3.0067 |           2.6039 |
[32m[20230205 18:16:40 @agent_ppo2.py:191][0m |          -0.0041 |           2.9177 |           2.6051 |
[32m[20230205 18:16:40 @agent_ppo2.py:191][0m |          -0.0061 |           2.8601 |           2.6015 |
[32m[20230205 18:16:40 @agent_ppo2.py:191][0m |          -0.0072 |           2.8270 |           2.6032 |
[32m[20230205 18:16:40 @agent_ppo2.py:191][0m |          -0.0076 |           2.7857 |           2.6045 |
[32m[20230205 18:16:40 @agent_ppo2.py:191][0m |          -0.0085 |           2.7644 |           2.6054 |
[32m[20230205 18:16:40 @agent_ppo2.py:191][0m |          -0.0094 |           2.7342 |           2.6087 |
[32m[20230205 18:16:40 @agent_ppo2.py:191][0m |          -0.0102 |           2.7146 |           2.6094 |
[32m[20230205 18:16:40 @agent_ppo2.py:191][0m |          -0.0106 |           2.6954 |           2.6102 |
[32m[20230205 18:16:40 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:16:41 @agent_ppo2.py:144][0m Average TRAINING episode reward: 143.84
[32m[20230205 18:16:41 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 153.00
[32m[20230205 18:16:41 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.50
[32m[20230205 18:16:41 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 267.50
[32m[20230205 18:16:41 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 267.50
[32m[20230205 18:16:41 @agent_ppo2.py:149][0m Total time:       3.23 min
[32m[20230205 18:16:41 @agent_ppo2.py:151][0m 188416 total steps have happened
[32m[20230205 18:16:41 @agent_ppo2.py:127][0m #------------------------ Iteration 92 --------------------------#
[32m[20230205 18:16:42 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:16:42 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:42 @agent_ppo2.py:191][0m |           0.0004 |           5.0311 |           2.5931 |
[32m[20230205 18:16:42 @agent_ppo2.py:191][0m |          -0.0038 |           3.2045 |           2.5909 |
[32m[20230205 18:16:42 @agent_ppo2.py:191][0m |          -0.0071 |           2.9542 |           2.5904 |
[32m[20230205 18:16:42 @agent_ppo2.py:191][0m |          -0.0079 |           2.8169 |           2.5895 |
[32m[20230205 18:16:42 @agent_ppo2.py:191][0m |          -0.0091 |           2.7191 |           2.5901 |
[32m[20230205 18:16:42 @agent_ppo2.py:191][0m |          -0.0097 |           2.6319 |           2.5917 |
[32m[20230205 18:16:42 @agent_ppo2.py:191][0m |          -0.0103 |           2.5522 |           2.5914 |
[32m[20230205 18:16:42 @agent_ppo2.py:191][0m |          -0.0108 |           2.4980 |           2.5907 |
[32m[20230205 18:16:42 @agent_ppo2.py:191][0m |          -0.0109 |           2.4297 |           2.5917 |
[32m[20230205 18:16:42 @agent_ppo2.py:191][0m |          -0.0117 |           2.3770 |           2.5917 |
[32m[20230205 18:16:42 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:16:43 @agent_ppo2.py:144][0m Average TRAINING episode reward: 165.71
[32m[20230205 18:16:43 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 167.11
[32m[20230205 18:16:43 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 254.15
[32m[20230205 18:16:43 @agent_ppo2.py:149][0m Total time:       3.26 min
[32m[20230205 18:16:43 @agent_ppo2.py:151][0m 190464 total steps have happened
[32m[20230205 18:16:43 @agent_ppo2.py:127][0m #------------------------ Iteration 93 --------------------------#
[32m[20230205 18:16:44 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:16:44 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:44 @agent_ppo2.py:191][0m |           0.0006 |           2.8321 |           2.6760 |
[32m[20230205 18:16:44 @agent_ppo2.py:191][0m |          -0.0030 |           2.4771 |           2.6767 |
[32m[20230205 18:16:44 @agent_ppo2.py:191][0m |          -0.0054 |           2.3680 |           2.6748 |
[32m[20230205 18:16:44 @agent_ppo2.py:191][0m |          -0.0066 |           2.3185 |           2.6727 |
[32m[20230205 18:16:44 @agent_ppo2.py:191][0m |          -0.0076 |           2.2517 |           2.6740 |
[32m[20230205 18:16:44 @agent_ppo2.py:191][0m |          -0.0080 |           2.2066 |           2.6734 |
[32m[20230205 18:16:44 @agent_ppo2.py:191][0m |          -0.0086 |           2.1763 |           2.6753 |
[32m[20230205 18:16:44 @agent_ppo2.py:191][0m |          -0.0094 |           2.1415 |           2.6745 |
[32m[20230205 18:16:44 @agent_ppo2.py:191][0m |          -0.0096 |           2.1114 |           2.6725 |
[32m[20230205 18:16:45 @agent_ppo2.py:191][0m |          -0.0100 |           2.0847 |           2.6743 |
[32m[20230205 18:16:45 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:16:45 @agent_ppo2.py:144][0m Average TRAINING episode reward: 167.32
[32m[20230205 18:16:45 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 170.68
[32m[20230205 18:16:45 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 258.09
[32m[20230205 18:16:45 @agent_ppo2.py:149][0m Total time:       3.30 min
[32m[20230205 18:16:45 @agent_ppo2.py:151][0m 192512 total steps have happened
[32m[20230205 18:16:45 @agent_ppo2.py:127][0m #------------------------ Iteration 94 --------------------------#
[32m[20230205 18:16:46 @agent_ppo2.py:133][0m Sampling time: 0.68 s by 1 slaves
[32m[20230205 18:16:46 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:46 @agent_ppo2.py:191][0m |          -0.0054 |          12.2805 |           2.6260 |
[32m[20230205 18:16:46 @agent_ppo2.py:191][0m |          -0.0090 |           5.7143 |           2.6235 |
[32m[20230205 18:16:46 @agent_ppo2.py:191][0m |          -0.0096 |           4.9639 |           2.6210 |
[32m[20230205 18:16:46 @agent_ppo2.py:191][0m |          -0.0129 |           4.4845 |           2.6215 |
[32m[20230205 18:16:46 @agent_ppo2.py:191][0m |          -0.0134 |           4.1744 |           2.6209 |
[32m[20230205 18:16:46 @agent_ppo2.py:191][0m |          -0.0112 |           3.8532 |           2.6209 |
[32m[20230205 18:16:46 @agent_ppo2.py:191][0m |          -0.0075 |           3.5932 |           2.6187 |
[32m[20230205 18:16:46 @agent_ppo2.py:191][0m |          -0.0074 |           3.4124 |           2.6190 |
[32m[20230205 18:16:47 @agent_ppo2.py:191][0m |          -0.0055 |           3.3027 |           2.6179 |
[32m[20230205 18:16:47 @agent_ppo2.py:191][0m |          -0.0138 |           3.1884 |           2.6189 |
[32m[20230205 18:16:47 @agent_ppo2.py:136][0m Policy update time: 0.67 s
[32m[20230205 18:16:47 @agent_ppo2.py:144][0m Average TRAINING episode reward: 103.83
[32m[20230205 18:16:47 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 180.35
[32m[20230205 18:16:47 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 255.94
[32m[20230205 18:16:47 @agent_ppo2.py:149][0m Total time:       3.33 min
[32m[20230205 18:16:47 @agent_ppo2.py:151][0m 194560 total steps have happened
[32m[20230205 18:16:47 @agent_ppo2.py:127][0m #------------------------ Iteration 95 --------------------------#
[32m[20230205 18:16:48 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:16:48 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:48 @agent_ppo2.py:191][0m |          -0.0005 |           3.1061 |           2.6003 |
[32m[20230205 18:16:48 @agent_ppo2.py:191][0m |          -0.0054 |           2.7326 |           2.5975 |
[32m[20230205 18:16:48 @agent_ppo2.py:191][0m |          -0.0074 |           2.6142 |           2.5984 |
[32m[20230205 18:16:48 @agent_ppo2.py:191][0m |          -0.0085 |           2.5299 |           2.6004 |
[32m[20230205 18:16:48 @agent_ppo2.py:191][0m |          -0.0093 |           2.4718 |           2.5997 |
[32m[20230205 18:16:48 @agent_ppo2.py:191][0m |          -0.0100 |           2.4270 |           2.6002 |
[32m[20230205 18:16:48 @agent_ppo2.py:191][0m |          -0.0110 |           2.3761 |           2.6003 |
[32m[20230205 18:16:49 @agent_ppo2.py:191][0m |          -0.0116 |           2.3397 |           2.6005 |
[32m[20230205 18:16:49 @agent_ppo2.py:191][0m |          -0.0119 |           2.3077 |           2.6014 |
[32m[20230205 18:16:49 @agent_ppo2.py:191][0m |          -0.0118 |           2.2709 |           2.5995 |
[32m[20230205 18:16:49 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:16:49 @agent_ppo2.py:144][0m Average TRAINING episode reward: 170.08
[32m[20230205 18:16:49 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 181.68
[32m[20230205 18:16:49 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -94.55
[32m[20230205 18:16:49 @agent_ppo2.py:149][0m Total time:       3.36 min
[32m[20230205 18:16:49 @agent_ppo2.py:151][0m 196608 total steps have happened
[32m[20230205 18:16:49 @agent_ppo2.py:127][0m #------------------------ Iteration 96 --------------------------#
[32m[20230205 18:16:50 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:16:50 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:50 @agent_ppo2.py:191][0m |          -0.0009 |           2.8275 |           2.6594 |
[32m[20230205 18:16:50 @agent_ppo2.py:191][0m |          -0.0050 |           2.5973 |           2.6582 |
[32m[20230205 18:16:50 @agent_ppo2.py:191][0m |          -0.0068 |           2.5021 |           2.6591 |
[32m[20230205 18:16:50 @agent_ppo2.py:191][0m |          -0.0077 |           2.4464 |           2.6589 |
[32m[20230205 18:16:50 @agent_ppo2.py:191][0m |          -0.0083 |           2.3843 |           2.6577 |
[32m[20230205 18:16:50 @agent_ppo2.py:191][0m |          -0.0090 |           2.3496 |           2.6581 |
[32m[20230205 18:16:50 @agent_ppo2.py:191][0m |          -0.0092 |           2.3215 |           2.6575 |
[32m[20230205 18:16:51 @agent_ppo2.py:191][0m |          -0.0101 |           2.2878 |           2.6584 |
[32m[20230205 18:16:51 @agent_ppo2.py:191][0m |          -0.0099 |           2.2644 |           2.6568 |
[32m[20230205 18:16:51 @agent_ppo2.py:191][0m |          -0.0107 |           2.2422 |           2.6571 |
[32m[20230205 18:16:51 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:16:51 @agent_ppo2.py:144][0m Average TRAINING episode reward: 201.89
[32m[20230205 18:16:51 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 217.18
[32m[20230205 18:16:51 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 159.00
[32m[20230205 18:16:51 @agent_ppo2.py:149][0m Total time:       3.40 min
[32m[20230205 18:16:51 @agent_ppo2.py:151][0m 198656 total steps have happened
[32m[20230205 18:16:51 @agent_ppo2.py:127][0m #------------------------ Iteration 97 --------------------------#
[32m[20230205 18:16:52 @agent_ppo2.py:133][0m Sampling time: 0.58 s by 1 slaves
[32m[20230205 18:16:52 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:52 @agent_ppo2.py:191][0m |          -0.0029 |          16.0741 |           2.6354 |
[32m[20230205 18:16:52 @agent_ppo2.py:191][0m |          -0.0084 |           6.8690 |           2.6292 |
[32m[20230205 18:16:52 @agent_ppo2.py:191][0m |          -0.0140 |           6.2791 |           2.6297 |
[32m[20230205 18:16:52 @agent_ppo2.py:191][0m |          -0.0041 |           6.0915 |           2.6271 |
[32m[20230205 18:16:52 @agent_ppo2.py:191][0m |          -0.0103 |           5.5204 |           2.6241 |
[32m[20230205 18:16:52 @agent_ppo2.py:191][0m |          -0.0121 |           5.2683 |           2.6259 |
[32m[20230205 18:16:52 @agent_ppo2.py:191][0m |          -0.0160 |           5.2740 |           2.6242 |
[32m[20230205 18:16:52 @agent_ppo2.py:191][0m |          -0.0070 |           5.2607 |           2.6230 |
[32m[20230205 18:16:52 @agent_ppo2.py:191][0m |          -0.0149 |           4.9088 |           2.6237 |
[32m[20230205 18:16:52 @agent_ppo2.py:191][0m |          -0.0160 |           4.8667 |           2.6225 |
[32m[20230205 18:16:52 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:16:53 @agent_ppo2.py:144][0m Average TRAINING episode reward: 96.56
[32m[20230205 18:16:53 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 175.24
[32m[20230205 18:16:53 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 177.55
[32m[20230205 18:16:53 @agent_ppo2.py:149][0m Total time:       3.43 min
[32m[20230205 18:16:53 @agent_ppo2.py:151][0m 200704 total steps have happened
[32m[20230205 18:16:53 @agent_ppo2.py:127][0m #------------------------ Iteration 98 --------------------------#
[32m[20230205 18:16:54 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:16:54 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:54 @agent_ppo2.py:191][0m |           0.0005 |           4.4519 |           2.6793 |
[32m[20230205 18:16:54 @agent_ppo2.py:191][0m |          -0.0026 |           3.6511 |           2.6786 |
[32m[20230205 18:16:54 @agent_ppo2.py:191][0m |          -0.0046 |           3.4874 |           2.6785 |
[32m[20230205 18:16:54 @agent_ppo2.py:191][0m |          -0.0054 |           3.3630 |           2.6783 |
[32m[20230205 18:16:54 @agent_ppo2.py:191][0m |          -0.0068 |           3.3031 |           2.6762 |
[32m[20230205 18:16:54 @agent_ppo2.py:191][0m |          -0.0080 |           3.2514 |           2.6779 |
[32m[20230205 18:16:54 @agent_ppo2.py:191][0m |          -0.0084 |           3.1953 |           2.6781 |
[32m[20230205 18:16:54 @agent_ppo2.py:191][0m |          -0.0090 |           3.1675 |           2.6795 |
[32m[20230205 18:16:54 @agent_ppo2.py:191][0m |          -0.0099 |           3.1214 |           2.6790 |
[32m[20230205 18:16:54 @agent_ppo2.py:191][0m |          -0.0097 |           3.0712 |           2.6798 |
[32m[20230205 18:16:54 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:16:55 @agent_ppo2.py:144][0m Average TRAINING episode reward: 191.62
[32m[20230205 18:16:55 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 192.52
[32m[20230205 18:16:55 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 266.08
[32m[20230205 18:16:55 @agent_ppo2.py:149][0m Total time:       3.46 min
[32m[20230205 18:16:55 @agent_ppo2.py:151][0m 202752 total steps have happened
[32m[20230205 18:16:55 @agent_ppo2.py:127][0m #------------------------ Iteration 99 --------------------------#
[32m[20230205 18:16:55 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:16:56 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:56 @agent_ppo2.py:191][0m |          -0.0023 |          43.8661 |           2.6053 |
[32m[20230205 18:16:56 @agent_ppo2.py:191][0m |          -0.0029 |          23.5845 |           2.5983 |
[32m[20230205 18:16:56 @agent_ppo2.py:191][0m |          -0.0074 |          19.4336 |           2.5945 |
[32m[20230205 18:16:56 @agent_ppo2.py:191][0m |          -0.0077 |          17.6012 |           2.5952 |
[32m[20230205 18:16:56 @agent_ppo2.py:191][0m |          -0.0132 |          16.5158 |           2.5947 |
[32m[20230205 18:16:56 @agent_ppo2.py:191][0m |          -0.0144 |          15.7749 |           2.5938 |
[32m[20230205 18:16:56 @agent_ppo2.py:191][0m |          -0.0055 |          15.1725 |           2.5916 |
[32m[20230205 18:16:56 @agent_ppo2.py:191][0m |          -0.0086 |          14.6916 |           2.5933 |
[32m[20230205 18:16:56 @agent_ppo2.py:191][0m |          -0.0122 |          13.8316 |           2.5908 |
[32m[20230205 18:16:56 @agent_ppo2.py:191][0m |          -0.0128 |          13.1914 |           2.5939 |
[32m[20230205 18:16:56 @agent_ppo2.py:136][0m Policy update time: 0.47 s
[32m[20230205 18:16:57 @agent_ppo2.py:144][0m Average TRAINING episode reward: 26.15
[32m[20230205 18:16:57 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 98.86
[32m[20230205 18:16:57 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.11
[32m[20230205 18:16:57 @agent_ppo2.py:149][0m Total time:       3.49 min
[32m[20230205 18:16:57 @agent_ppo2.py:151][0m 204800 total steps have happened
[32m[20230205 18:16:57 @agent_ppo2.py:127][0m #------------------------ Iteration 100 --------------------------#
[32m[20230205 18:16:57 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:16:57 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:16:57 @agent_ppo2.py:191][0m |           0.0002 |          10.4556 |           2.7069 |
[32m[20230205 18:16:57 @agent_ppo2.py:191][0m |          -0.0037 |           7.4795 |           2.7041 |
[32m[20230205 18:16:58 @agent_ppo2.py:191][0m |          -0.0060 |           6.8779 |           2.7047 |
[32m[20230205 18:16:58 @agent_ppo2.py:191][0m |          -0.0077 |           6.5147 |           2.7073 |
[32m[20230205 18:16:58 @agent_ppo2.py:191][0m |          -0.0096 |           6.2424 |           2.7060 |
[32m[20230205 18:16:58 @agent_ppo2.py:191][0m |          -0.0097 |           6.0778 |           2.7055 |
[32m[20230205 18:16:58 @agent_ppo2.py:191][0m |          -0.0111 |           5.8977 |           2.7080 |
[32m[20230205 18:16:58 @agent_ppo2.py:191][0m |          -0.0111 |           5.8010 |           2.7083 |
[32m[20230205 18:16:58 @agent_ppo2.py:191][0m |          -0.0120 |           5.6764 |           2.7093 |
[32m[20230205 18:16:58 @agent_ppo2.py:191][0m |          -0.0121 |           5.6224 |           2.7096 |
[32m[20230205 18:16:58 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:16:59 @agent_ppo2.py:144][0m Average TRAINING episode reward: 181.81
[32m[20230205 18:16:59 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 184.68
[32m[20230205 18:16:59 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 109.20
[32m[20230205 18:16:59 @agent_ppo2.py:149][0m Total time:       3.52 min
[32m[20230205 18:16:59 @agent_ppo2.py:151][0m 206848 total steps have happened
[32m[20230205 18:16:59 @agent_ppo2.py:127][0m #------------------------ Iteration 101 --------------------------#
[32m[20230205 18:16:59 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:17:00 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:00 @agent_ppo2.py:191][0m |          -0.0006 |           3.2647 |           2.6716 |
[32m[20230205 18:17:00 @agent_ppo2.py:191][0m |          -0.0059 |           2.8019 |           2.6666 |
[32m[20230205 18:17:00 @agent_ppo2.py:191][0m |          -0.0067 |           2.6702 |           2.6646 |
[32m[20230205 18:17:00 @agent_ppo2.py:191][0m |          -0.0078 |           2.5746 |           2.6636 |
[32m[20230205 18:17:00 @agent_ppo2.py:191][0m |          -0.0088 |           2.5034 |           2.6621 |
[32m[20230205 18:17:00 @agent_ppo2.py:191][0m |          -0.0093 |           2.4444 |           2.6623 |
[32m[20230205 18:17:00 @agent_ppo2.py:191][0m |          -0.0102 |           2.3973 |           2.6620 |
[32m[20230205 18:17:00 @agent_ppo2.py:191][0m |          -0.0110 |           2.3650 |           2.6612 |
[32m[20230205 18:17:00 @agent_ppo2.py:191][0m |          -0.0111 |           2.3269 |           2.6600 |
[32m[20230205 18:17:00 @agent_ppo2.py:191][0m |          -0.0117 |           2.2895 |           2.6601 |
[32m[20230205 18:17:00 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:17:01 @agent_ppo2.py:144][0m Average TRAINING episode reward: 189.72
[32m[20230205 18:17:01 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 203.24
[32m[20230205 18:17:01 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 157.88
[32m[20230205 18:17:01 @agent_ppo2.py:149][0m Total time:       3.56 min
[32m[20230205 18:17:01 @agent_ppo2.py:151][0m 208896 total steps have happened
[32m[20230205 18:17:01 @agent_ppo2.py:127][0m #------------------------ Iteration 102 --------------------------#
[32m[20230205 18:17:02 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:17:02 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:02 @agent_ppo2.py:191][0m |          -0.0006 |           3.6802 |           2.7005 |
[32m[20230205 18:17:02 @agent_ppo2.py:191][0m |          -0.0049 |           3.1186 |           2.7003 |
[32m[20230205 18:17:02 @agent_ppo2.py:191][0m |          -0.0068 |           2.9700 |           2.7004 |
[32m[20230205 18:17:02 @agent_ppo2.py:191][0m |          -0.0081 |           2.8788 |           2.7002 |
[32m[20230205 18:17:02 @agent_ppo2.py:191][0m |          -0.0090 |           2.8033 |           2.7018 |
[32m[20230205 18:17:02 @agent_ppo2.py:191][0m |          -0.0094 |           2.7517 |           2.6997 |
[32m[20230205 18:17:02 @agent_ppo2.py:191][0m |          -0.0106 |           2.6974 |           2.7033 |
[32m[20230205 18:17:02 @agent_ppo2.py:191][0m |          -0.0108 |           2.6556 |           2.7020 |
[32m[20230205 18:17:02 @agent_ppo2.py:191][0m |          -0.0112 |           2.6212 |           2.7026 |
[32m[20230205 18:17:02 @agent_ppo2.py:191][0m |          -0.0116 |           2.5802 |           2.7042 |
[32m[20230205 18:17:02 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:17:03 @agent_ppo2.py:144][0m Average TRAINING episode reward: 186.10
[32m[20230205 18:17:03 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 193.64
[32m[20230205 18:17:03 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 77.21
[32m[20230205 18:17:03 @agent_ppo2.py:149][0m Total time:       3.59 min
[32m[20230205 18:17:03 @agent_ppo2.py:151][0m 210944 total steps have happened
[32m[20230205 18:17:03 @agent_ppo2.py:127][0m #------------------------ Iteration 103 --------------------------#
[32m[20230205 18:17:04 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:17:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:04 @agent_ppo2.py:191][0m |          -0.0007 |           2.6874 |           2.6702 |
[32m[20230205 18:17:04 @agent_ppo2.py:191][0m |          -0.0060 |           2.4286 |           2.6659 |
[32m[20230205 18:17:04 @agent_ppo2.py:191][0m |          -0.0081 |           2.3255 |           2.6640 |
[32m[20230205 18:17:04 @agent_ppo2.py:191][0m |          -0.0097 |           2.2624 |           2.6633 |
[32m[20230205 18:17:04 @agent_ppo2.py:191][0m |          -0.0105 |           2.2082 |           2.6625 |
[32m[20230205 18:17:04 @agent_ppo2.py:191][0m |          -0.0110 |           2.1739 |           2.6619 |
[32m[20230205 18:17:04 @agent_ppo2.py:191][0m |          -0.0118 |           2.1391 |           2.6626 |
[32m[20230205 18:17:04 @agent_ppo2.py:191][0m |          -0.0118 |           2.1096 |           2.6627 |
[32m[20230205 18:17:04 @agent_ppo2.py:191][0m |          -0.0123 |           2.0941 |           2.6615 |
[32m[20230205 18:17:04 @agent_ppo2.py:191][0m |          -0.0128 |           2.0651 |           2.6614 |
[32m[20230205 18:17:04 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:17:05 @agent_ppo2.py:144][0m Average TRAINING episode reward: 186.93
[32m[20230205 18:17:05 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 192.77
[32m[20230205 18:17:05 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 94.73
[32m[20230205 18:17:05 @agent_ppo2.py:149][0m Total time:       3.63 min
[32m[20230205 18:17:05 @agent_ppo2.py:151][0m 212992 total steps have happened
[32m[20230205 18:17:05 @agent_ppo2.py:127][0m #------------------------ Iteration 104 --------------------------#
[32m[20230205 18:17:06 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:17:06 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:06 @agent_ppo2.py:191][0m |          -0.0015 |          66.7107 |           2.6595 |
[32m[20230205 18:17:06 @agent_ppo2.py:191][0m |          -0.0079 |          32.1742 |           2.6556 |
[32m[20230205 18:17:06 @agent_ppo2.py:191][0m |          -0.0155 |          27.7204 |           2.6535 |
[32m[20230205 18:17:06 @agent_ppo2.py:191][0m |          -0.0150 |          25.0464 |           2.6531 |
[32m[20230205 18:17:06 @agent_ppo2.py:191][0m |          -0.0158 |          22.8892 |           2.6530 |
[32m[20230205 18:17:06 @agent_ppo2.py:191][0m |          -0.0167 |          20.7034 |           2.6522 |
[32m[20230205 18:17:06 @agent_ppo2.py:191][0m |          -0.0184 |          20.7891 |           2.6524 |
[32m[20230205 18:17:06 @agent_ppo2.py:191][0m |          -0.0194 |          19.4048 |           2.6517 |
[32m[20230205 18:17:06 @agent_ppo2.py:191][0m |          -0.0190 |          18.8569 |           2.6517 |
[32m[20230205 18:17:06 @agent_ppo2.py:191][0m |          -0.0200 |          17.0948 |           2.6525 |
[32m[20230205 18:17:06 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:17:07 @agent_ppo2.py:144][0m Average TRAINING episode reward: -20.27
[32m[20230205 18:17:07 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: -10.69
[32m[20230205 18:17:07 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 259.80
[32m[20230205 18:17:07 @agent_ppo2.py:149][0m Total time:       3.66 min
[32m[20230205 18:17:07 @agent_ppo2.py:151][0m 215040 total steps have happened
[32m[20230205 18:17:07 @agent_ppo2.py:127][0m #------------------------ Iteration 105 --------------------------#
[32m[20230205 18:17:07 @agent_ppo2.py:133][0m Sampling time: 0.62 s by 1 slaves
[32m[20230205 18:17:07 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:07 @agent_ppo2.py:191][0m |          -0.0025 |          17.7961 |           2.6843 |
[32m[20230205 18:17:08 @agent_ppo2.py:191][0m |          -0.0074 |          10.4001 |           2.6800 |
[32m[20230205 18:17:08 @agent_ppo2.py:191][0m |          -0.0088 |           9.8280 |           2.6801 |
[32m[20230205 18:17:08 @agent_ppo2.py:191][0m |          -0.0098 |           9.0147 |           2.6773 |
[32m[20230205 18:17:08 @agent_ppo2.py:191][0m |          -0.0106 |           8.4514 |           2.6774 |
[32m[20230205 18:17:08 @agent_ppo2.py:191][0m |          -0.0121 |           8.2162 |           2.6774 |
[32m[20230205 18:17:08 @agent_ppo2.py:191][0m |          -0.0129 |           8.1054 |           2.6746 |
[32m[20230205 18:17:08 @agent_ppo2.py:191][0m |          -0.0130 |           7.5185 |           2.6737 |
[32m[20230205 18:17:08 @agent_ppo2.py:191][0m |          -0.0132 |           7.2323 |           2.6709 |
[32m[20230205 18:17:08 @agent_ppo2.py:191][0m |          -0.0138 |           7.2548 |           2.6726 |
[32m[20230205 18:17:08 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:17:09 @agent_ppo2.py:144][0m Average TRAINING episode reward: 103.34
[32m[20230205 18:17:09 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 204.16
[32m[20230205 18:17:09 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 261.29
[32m[20230205 18:17:09 @agent_ppo2.py:149][0m Total time:       3.69 min
[32m[20230205 18:17:09 @agent_ppo2.py:151][0m 217088 total steps have happened
[32m[20230205 18:17:09 @agent_ppo2.py:127][0m #------------------------ Iteration 106 --------------------------#
[32m[20230205 18:17:09 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:17:09 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:09 @agent_ppo2.py:191][0m |          -0.0125 |          42.9617 |           2.6805 |
[32m[20230205 18:17:09 @agent_ppo2.py:191][0m |          -0.0068 |          22.0281 |           2.6795 |
[32m[20230205 18:17:09 @agent_ppo2.py:191][0m |          -0.0060 |          17.5094 |           2.6832 |
[32m[20230205 18:17:09 @agent_ppo2.py:191][0m |          -0.0078 |          15.6652 |           2.6850 |
[32m[20230205 18:17:09 @agent_ppo2.py:191][0m |          -0.0033 |          14.1065 |           2.6846 |
[32m[20230205 18:17:09 @agent_ppo2.py:191][0m |          -0.0079 |          12.9718 |           2.6829 |
[32m[20230205 18:17:09 @agent_ppo2.py:191][0m |          -0.0163 |          12.0090 |           2.6828 |
[32m[20230205 18:17:10 @agent_ppo2.py:191][0m |          -0.0096 |          11.5201 |           2.6824 |
[32m[20230205 18:17:10 @agent_ppo2.py:191][0m |          -0.0146 |          10.6058 |           2.6832 |
[32m[20230205 18:17:10 @agent_ppo2.py:191][0m |          -0.0182 |          10.2074 |           2.6833 |
[32m[20230205 18:17:10 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:17:10 @agent_ppo2.py:144][0m Average TRAINING episode reward: 25.98
[32m[20230205 18:17:10 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 212.39
[32m[20230205 18:17:10 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 259.16
[32m[20230205 18:17:10 @agent_ppo2.py:149][0m Total time:       3.71 min
[32m[20230205 18:17:10 @agent_ppo2.py:151][0m 219136 total steps have happened
[32m[20230205 18:17:10 @agent_ppo2.py:127][0m #------------------------ Iteration 107 --------------------------#
[32m[20230205 18:17:11 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:17:11 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:11 @agent_ppo2.py:191][0m |           0.0008 |           9.0751 |           2.7111 |
[32m[20230205 18:17:11 @agent_ppo2.py:191][0m |          -0.0019 |           6.6234 |           2.7083 |
[32m[20230205 18:17:11 @agent_ppo2.py:191][0m |          -0.0048 |           6.2131 |           2.7092 |
[32m[20230205 18:17:11 @agent_ppo2.py:191][0m |          -0.0069 |           5.9641 |           2.7080 |
[32m[20230205 18:17:11 @agent_ppo2.py:191][0m |          -0.0086 |           5.7712 |           2.7052 |
[32m[20230205 18:17:11 @agent_ppo2.py:191][0m |          -0.0096 |           5.6192 |           2.7067 |
[32m[20230205 18:17:11 @agent_ppo2.py:191][0m |          -0.0106 |           5.4822 |           2.7058 |
[32m[20230205 18:17:12 @agent_ppo2.py:191][0m |          -0.0113 |           5.3660 |           2.7040 |
[32m[20230205 18:17:12 @agent_ppo2.py:191][0m |          -0.0122 |           5.2533 |           2.7059 |
[32m[20230205 18:17:12 @agent_ppo2.py:191][0m |          -0.0127 |           5.1634 |           2.7059 |
[32m[20230205 18:17:12 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:17:12 @agent_ppo2.py:144][0m Average TRAINING episode reward: 196.00
[32m[20230205 18:17:12 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 223.24
[32m[20230205 18:17:12 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 252.63
[32m[20230205 18:17:12 @agent_ppo2.py:149][0m Total time:       3.75 min
[32m[20230205 18:17:12 @agent_ppo2.py:151][0m 221184 total steps have happened
[32m[20230205 18:17:12 @agent_ppo2.py:127][0m #------------------------ Iteration 108 --------------------------#
[32m[20230205 18:17:13 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:17:13 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:13 @agent_ppo2.py:191][0m |           0.0003 |           4.4778 |           2.7026 |
[32m[20230205 18:17:13 @agent_ppo2.py:191][0m |          -0.0041 |           3.4141 |           2.7007 |
[32m[20230205 18:17:13 @agent_ppo2.py:191][0m |          -0.0065 |           3.2276 |           2.6975 |
[32m[20230205 18:17:13 @agent_ppo2.py:191][0m |          -0.0074 |           3.1367 |           2.6967 |
[32m[20230205 18:17:13 @agent_ppo2.py:191][0m |          -0.0084 |           3.0661 |           2.6972 |
[32m[20230205 18:17:13 @agent_ppo2.py:191][0m |          -0.0090 |           3.0098 |           2.6987 |
[32m[20230205 18:17:14 @agent_ppo2.py:191][0m |          -0.0092 |           2.9641 |           2.6973 |
[32m[20230205 18:17:14 @agent_ppo2.py:191][0m |          -0.0101 |           2.9261 |           2.6989 |
[32m[20230205 18:17:14 @agent_ppo2.py:191][0m |          -0.0106 |           2.8923 |           2.6990 |
[32m[20230205 18:17:14 @agent_ppo2.py:191][0m |          -0.0110 |           2.8662 |           2.6986 |
[32m[20230205 18:17:14 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:17:14 @agent_ppo2.py:144][0m Average TRAINING episode reward: 188.16
[32m[20230205 18:17:14 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 190.44
[32m[20230205 18:17:14 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 101.25
[32m[20230205 18:17:14 @agent_ppo2.py:149][0m Total time:       3.78 min
[32m[20230205 18:17:14 @agent_ppo2.py:151][0m 223232 total steps have happened
[32m[20230205 18:17:14 @agent_ppo2.py:127][0m #------------------------ Iteration 109 --------------------------#
[32m[20230205 18:17:15 @agent_ppo2.py:133][0m Sampling time: 0.48 s by 1 slaves
[32m[20230205 18:17:15 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:15 @agent_ppo2.py:191][0m |           0.0023 |          15.8554 |           2.6243 |
[32m[20230205 18:17:15 @agent_ppo2.py:191][0m |          -0.0036 |           9.3199 |           2.6233 |
[32m[20230205 18:17:15 @agent_ppo2.py:191][0m |          -0.0115 |           7.7374 |           2.6232 |
[32m[20230205 18:17:15 @agent_ppo2.py:191][0m |          -0.0122 |           7.1697 |           2.6228 |
[32m[20230205 18:17:15 @agent_ppo2.py:191][0m |          -0.0048 |           7.8388 |           2.6217 |
[32m[20230205 18:17:15 @agent_ppo2.py:191][0m |          -0.0100 |           6.6458 |           2.6230 |
[32m[20230205 18:17:15 @agent_ppo2.py:191][0m |          -0.0137 |           6.5129 |           2.6229 |
[32m[20230205 18:17:15 @agent_ppo2.py:191][0m |          -0.0157 |           6.3603 |           2.6230 |
[32m[20230205 18:17:15 @agent_ppo2.py:191][0m |          -0.0055 |           6.2701 |           2.6231 |
[32m[20230205 18:17:15 @agent_ppo2.py:191][0m |          -0.0124 |           6.4565 |           2.6234 |
[32m[20230205 18:17:15 @agent_ppo2.py:136][0m Policy update time: 0.47 s
[32m[20230205 18:17:16 @agent_ppo2.py:144][0m Average TRAINING episode reward: 72.75
[32m[20230205 18:17:16 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 176.31
[32m[20230205 18:17:16 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 263.37
[32m[20230205 18:17:16 @agent_ppo2.py:149][0m Total time:       3.81 min
[32m[20230205 18:17:16 @agent_ppo2.py:151][0m 225280 total steps have happened
[32m[20230205 18:17:16 @agent_ppo2.py:127][0m #------------------------ Iteration 110 --------------------------#
[32m[20230205 18:17:17 @agent_ppo2.py:133][0m Sampling time: 0.71 s by 1 slaves
[32m[20230205 18:17:17 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:17 @agent_ppo2.py:191][0m |           0.0006 |           2.9117 |           2.7146 |
[32m[20230205 18:17:17 @agent_ppo2.py:191][0m |          -0.0030 |           2.6744 |           2.7123 |
[32m[20230205 18:17:17 @agent_ppo2.py:191][0m |          -0.0051 |           2.5728 |           2.7098 |
[32m[20230205 18:17:17 @agent_ppo2.py:191][0m |          -0.0065 |           2.5078 |           2.7118 |
[32m[20230205 18:17:17 @agent_ppo2.py:191][0m |          -0.0078 |           2.4610 |           2.7098 |
[32m[20230205 18:17:17 @agent_ppo2.py:191][0m |          -0.0080 |           2.4247 |           2.7107 |
[32m[20230205 18:17:17 @agent_ppo2.py:191][0m |          -0.0093 |           2.3967 |           2.7104 |
[32m[20230205 18:17:17 @agent_ppo2.py:191][0m |          -0.0100 |           2.3706 |           2.7109 |
[32m[20230205 18:17:17 @agent_ppo2.py:191][0m |          -0.0099 |           2.3504 |           2.7112 |
[32m[20230205 18:17:17 @agent_ppo2.py:191][0m |          -0.0107 |           2.3340 |           2.7133 |
[32m[20230205 18:17:17 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:17:18 @agent_ppo2.py:144][0m Average TRAINING episode reward: 213.23
[32m[20230205 18:17:18 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 224.17
[32m[20230205 18:17:18 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 166.55
[32m[20230205 18:17:18 @agent_ppo2.py:149][0m Total time:       3.84 min
[32m[20230205 18:17:18 @agent_ppo2.py:151][0m 227328 total steps have happened
[32m[20230205 18:17:18 @agent_ppo2.py:127][0m #------------------------ Iteration 111 --------------------------#
[32m[20230205 18:17:19 @agent_ppo2.py:133][0m Sampling time: 0.81 s by 1 slaves
[32m[20230205 18:17:19 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:19 @agent_ppo2.py:191][0m |           0.0024 |          27.4241 |           2.7360 |
[32m[20230205 18:17:19 @agent_ppo2.py:191][0m |          -0.0046 |          19.5536 |           2.7282 |
[32m[20230205 18:17:19 @agent_ppo2.py:191][0m |          -0.0033 |          17.8464 |           2.7327 |
[32m[20230205 18:17:19 @agent_ppo2.py:191][0m |          -0.0089 |          15.8244 |           2.7294 |
[32m[20230205 18:17:19 @agent_ppo2.py:191][0m |          -0.0061 |          14.5444 |           2.7302 |
[32m[20230205 18:17:19 @agent_ppo2.py:191][0m |          -0.0068 |          13.7278 |           2.7280 |
[32m[20230205 18:17:19 @agent_ppo2.py:191][0m |          -0.0107 |          12.6444 |           2.7286 |
[32m[20230205 18:17:19 @agent_ppo2.py:191][0m |          -0.0089 |          12.0954 |           2.7297 |
[32m[20230205 18:17:20 @agent_ppo2.py:191][0m |          -0.0056 |          11.2189 |           2.7290 |
[32m[20230205 18:17:20 @agent_ppo2.py:191][0m |          -0.0118 |          10.9529 |           2.7305 |
[32m[20230205 18:17:20 @agent_ppo2.py:136][0m Policy update time: 0.79 s
[32m[20230205 18:17:20 @agent_ppo2.py:144][0m Average TRAINING episode reward: 77.41
[32m[20230205 18:17:20 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 210.47
[32m[20230205 18:17:20 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 30.42
[32m[20230205 18:17:20 @agent_ppo2.py:149][0m Total time:       3.88 min
[32m[20230205 18:17:20 @agent_ppo2.py:151][0m 229376 total steps have happened
[32m[20230205 18:17:20 @agent_ppo2.py:127][0m #------------------------ Iteration 112 --------------------------#
[32m[20230205 18:17:21 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:17:21 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:21 @agent_ppo2.py:191][0m |          -0.0009 |           5.5744 |           2.7935 |
[32m[20230205 18:17:21 @agent_ppo2.py:191][0m |          -0.0067 |           4.2825 |           2.7951 |
[32m[20230205 18:17:21 @agent_ppo2.py:191][0m |          -0.0081 |           4.1264 |           2.7933 |
[32m[20230205 18:17:21 @agent_ppo2.py:191][0m |          -0.0090 |           4.0099 |           2.7943 |
[32m[20230205 18:17:21 @agent_ppo2.py:191][0m |          -0.0098 |           3.9412 |           2.7917 |
[32m[20230205 18:17:22 @agent_ppo2.py:191][0m |          -0.0104 |           3.8834 |           2.7937 |
[32m[20230205 18:17:22 @agent_ppo2.py:191][0m |          -0.0103 |           3.8333 |           2.7941 |
[32m[20230205 18:17:22 @agent_ppo2.py:191][0m |          -0.0114 |           3.7995 |           2.7924 |
[32m[20230205 18:17:22 @agent_ppo2.py:191][0m |          -0.0119 |           3.7634 |           2.7950 |
[32m[20230205 18:17:22 @agent_ppo2.py:191][0m |          -0.0124 |           3.7291 |           2.7943 |
[32m[20230205 18:17:22 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:17:22 @agent_ppo2.py:144][0m Average TRAINING episode reward: 213.48
[32m[20230205 18:17:22 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 228.99
[32m[20230205 18:17:22 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.06
[32m[20230205 18:17:22 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 273.06
[32m[20230205 18:17:22 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 273.06
[32m[20230205 18:17:22 @agent_ppo2.py:149][0m Total time:       3.92 min
[32m[20230205 18:17:22 @agent_ppo2.py:151][0m 231424 total steps have happened
[32m[20230205 18:17:22 @agent_ppo2.py:127][0m #------------------------ Iteration 113 --------------------------#
[32m[20230205 18:17:23 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:17:23 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:23 @agent_ppo2.py:191][0m |           0.0014 |          27.8158 |           2.7227 |
[32m[20230205 18:17:23 @agent_ppo2.py:191][0m |          -0.0061 |          10.7199 |           2.7188 |
[32m[20230205 18:17:23 @agent_ppo2.py:191][0m |          -0.0065 |           8.7025 |           2.7136 |
[32m[20230205 18:17:23 @agent_ppo2.py:191][0m |          -0.0073 |           7.5750 |           2.7122 |
[32m[20230205 18:17:23 @agent_ppo2.py:191][0m |          -0.0112 |           6.9366 |           2.7111 |
[32m[20230205 18:17:23 @agent_ppo2.py:191][0m |          -0.0112 |           6.3024 |           2.7108 |
[32m[20230205 18:17:23 @agent_ppo2.py:191][0m |          -0.0107 |           6.0548 |           2.7073 |
[32m[20230205 18:17:23 @agent_ppo2.py:191][0m |          -0.0127 |           5.6979 |           2.7079 |
[32m[20230205 18:17:23 @agent_ppo2.py:191][0m |          -0.0112 |           5.4726 |           2.7091 |
[32m[20230205 18:17:23 @agent_ppo2.py:191][0m |          -0.0125 |           5.2036 |           2.7059 |
[32m[20230205 18:17:23 @agent_ppo2.py:136][0m Policy update time: 0.49 s
[32m[20230205 18:17:24 @agent_ppo2.py:144][0m Average TRAINING episode reward: 20.82
[32m[20230205 18:17:24 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 197.35
[32m[20230205 18:17:24 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 262.94
[32m[20230205 18:17:24 @agent_ppo2.py:149][0m Total time:       3.94 min
[32m[20230205 18:17:24 @agent_ppo2.py:151][0m 233472 total steps have happened
[32m[20230205 18:17:24 @agent_ppo2.py:127][0m #------------------------ Iteration 114 --------------------------#
[32m[20230205 18:17:25 @agent_ppo2.py:133][0m Sampling time: 0.65 s by 1 slaves
[32m[20230205 18:17:25 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:25 @agent_ppo2.py:191][0m |          -0.0013 |          10.6780 |           2.7741 |
[32m[20230205 18:17:25 @agent_ppo2.py:191][0m |          -0.0093 |           6.2902 |           2.7659 |
[32m[20230205 18:17:25 @agent_ppo2.py:191][0m |          -0.0089 |           5.7972 |           2.7631 |
[32m[20230205 18:17:25 @agent_ppo2.py:191][0m |          -0.0114 |           5.4477 |           2.7621 |
[32m[20230205 18:17:25 @agent_ppo2.py:191][0m |          -0.0125 |           5.2103 |           2.7604 |
[32m[20230205 18:17:25 @agent_ppo2.py:191][0m |          -0.0130 |           5.0889 |           2.7577 |
[32m[20230205 18:17:25 @agent_ppo2.py:191][0m |          -0.0129 |           4.9916 |           2.7569 |
[32m[20230205 18:17:25 @agent_ppo2.py:191][0m |          -0.0133 |           4.8105 |           2.7568 |
[32m[20230205 18:17:25 @agent_ppo2.py:191][0m |          -0.0119 |           4.6277 |           2.7548 |
[32m[20230205 18:17:25 @agent_ppo2.py:191][0m |          -0.0125 |           4.5904 |           2.7541 |
[32m[20230205 18:17:25 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:17:26 @agent_ppo2.py:144][0m Average TRAINING episode reward: 132.50
[32m[20230205 18:17:26 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 202.06
[32m[20230205 18:17:26 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 204.60
[32m[20230205 18:17:26 @agent_ppo2.py:149][0m Total time:       3.97 min
[32m[20230205 18:17:26 @agent_ppo2.py:151][0m 235520 total steps have happened
[32m[20230205 18:17:26 @agent_ppo2.py:127][0m #------------------------ Iteration 115 --------------------------#
[32m[20230205 18:17:27 @agent_ppo2.py:133][0m Sampling time: 0.71 s by 1 slaves
[32m[20230205 18:17:27 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:27 @agent_ppo2.py:191][0m |           0.0006 |           3.8325 |           2.7397 |
[32m[20230205 18:17:27 @agent_ppo2.py:191][0m |          -0.0040 |           3.3771 |           2.7414 |
[32m[20230205 18:17:27 @agent_ppo2.py:191][0m |          -0.0060 |           3.2332 |           2.7384 |
[32m[20230205 18:17:27 @agent_ppo2.py:191][0m |          -0.0077 |           3.1596 |           2.7385 |
[32m[20230205 18:17:27 @agent_ppo2.py:191][0m |          -0.0092 |           3.0996 |           2.7389 |
[32m[20230205 18:17:27 @agent_ppo2.py:191][0m |          -0.0102 |           3.0630 |           2.7390 |
[32m[20230205 18:17:27 @agent_ppo2.py:191][0m |          -0.0104 |           3.0205 |           2.7393 |
[32m[20230205 18:17:27 @agent_ppo2.py:191][0m |          -0.0111 |           2.9882 |           2.7388 |
[32m[20230205 18:17:27 @agent_ppo2.py:191][0m |          -0.0118 |           2.9561 |           2.7386 |
[32m[20230205 18:17:27 @agent_ppo2.py:191][0m |          -0.0119 |           2.9318 |           2.7375 |
[32m[20230205 18:17:27 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:17:28 @agent_ppo2.py:144][0m Average TRAINING episode reward: 223.97
[32m[20230205 18:17:28 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 230.01
[32m[20230205 18:17:28 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 133.08
[32m[20230205 18:17:28 @agent_ppo2.py:149][0m Total time:       4.01 min
[32m[20230205 18:17:28 @agent_ppo2.py:151][0m 237568 total steps have happened
[32m[20230205 18:17:28 @agent_ppo2.py:127][0m #------------------------ Iteration 116 --------------------------#
[32m[20230205 18:17:28 @agent_ppo2.py:133][0m Sampling time: 0.67 s by 1 slaves
[32m[20230205 18:17:28 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:29 @agent_ppo2.py:191][0m |          -0.0032 |          12.1859 |           2.6854 |
[32m[20230205 18:17:29 @agent_ppo2.py:191][0m |          -0.0067 |           9.4152 |           2.6805 |
[32m[20230205 18:17:29 @agent_ppo2.py:191][0m |          -0.0109 |           9.0447 |           2.6761 |
[32m[20230205 18:17:29 @agent_ppo2.py:191][0m |          -0.0112 |           8.1618 |           2.6753 |
[32m[20230205 18:17:29 @agent_ppo2.py:191][0m |          -0.0117 |           8.4006 |           2.6716 |
[32m[20230205 18:17:29 @agent_ppo2.py:191][0m |          -0.0125 |           7.6320 |           2.6756 |
[32m[20230205 18:17:29 @agent_ppo2.py:191][0m |          -0.0093 |           7.3101 |           2.6726 |
[32m[20230205 18:17:29 @agent_ppo2.py:191][0m |          -0.0141 |           7.0486 |           2.6737 |
[32m[20230205 18:17:29 @agent_ppo2.py:191][0m |          -0.0166 |           6.6935 |           2.6728 |
[32m[20230205 18:17:29 @agent_ppo2.py:191][0m |          -0.0151 |           6.3955 |           2.6733 |
[32m[20230205 18:17:29 @agent_ppo2.py:136][0m Policy update time: 0.67 s
[32m[20230205 18:17:30 @agent_ppo2.py:144][0m Average TRAINING episode reward: 146.10
[32m[20230205 18:17:30 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 218.74
[32m[20230205 18:17:30 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 108.63
[32m[20230205 18:17:30 @agent_ppo2.py:149][0m Total time:       4.04 min
[32m[20230205 18:17:30 @agent_ppo2.py:151][0m 239616 total steps have happened
[32m[20230205 18:17:30 @agent_ppo2.py:127][0m #------------------------ Iteration 117 --------------------------#
[32m[20230205 18:17:30 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:17:30 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:30 @agent_ppo2.py:191][0m |          -0.0026 |          23.3931 |           2.6214 |
[32m[20230205 18:17:30 @agent_ppo2.py:191][0m |          -0.0079 |          15.7363 |           2.6178 |
[32m[20230205 18:17:30 @agent_ppo2.py:191][0m |          -0.0110 |          11.9209 |           2.6137 |
[32m[20230205 18:17:30 @agent_ppo2.py:191][0m |          -0.0137 |           9.3343 |           2.6155 |
[32m[20230205 18:17:31 @agent_ppo2.py:191][0m |          -0.0136 |           8.1594 |           2.6150 |
[32m[20230205 18:17:31 @agent_ppo2.py:191][0m |          -0.0145 |           7.4832 |           2.6145 |
[32m[20230205 18:17:31 @agent_ppo2.py:191][0m |          -0.0153 |           6.8236 |           2.6129 |
[32m[20230205 18:17:31 @agent_ppo2.py:191][0m |          -0.0203 |           6.4487 |           2.6129 |
[32m[20230205 18:17:31 @agent_ppo2.py:191][0m |          -0.0072 |           6.5036 |           2.6114 |
[32m[20230205 18:17:31 @agent_ppo2.py:191][0m |          -0.0181 |           6.6839 |           2.6099 |
[32m[20230205 18:17:31 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:17:31 @agent_ppo2.py:144][0m Average TRAINING episode reward: 91.89
[32m[20230205 18:17:31 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 214.82
[32m[20230205 18:17:31 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 71.44
[32m[20230205 18:17:31 @agent_ppo2.py:149][0m Total time:       4.07 min
[32m[20230205 18:17:31 @agent_ppo2.py:151][0m 241664 total steps have happened
[32m[20230205 18:17:31 @agent_ppo2.py:127][0m #------------------------ Iteration 118 --------------------------#
[32m[20230205 18:17:32 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:17:32 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:32 @agent_ppo2.py:191][0m |          -0.0018 |          32.1933 |           2.6363 |
[32m[20230205 18:17:32 @agent_ppo2.py:191][0m |           0.0054 |          15.7186 |           2.6351 |
[32m[20230205 18:17:32 @agent_ppo2.py:191][0m |           0.0012 |          11.8814 |           2.6317 |
[32m[20230205 18:17:32 @agent_ppo2.py:191][0m |          -0.0044 |          10.6361 |           2.6304 |
[32m[20230205 18:17:32 @agent_ppo2.py:191][0m |          -0.0113 |           8.8969 |           2.6269 |
[32m[20230205 18:17:32 @agent_ppo2.py:191][0m |          -0.0048 |           8.5598 |           2.6296 |
[32m[20230205 18:17:32 @agent_ppo2.py:191][0m |          -0.0105 |           7.8172 |           2.6286 |
[32m[20230205 18:17:32 @agent_ppo2.py:191][0m |          -0.0107 |           7.5320 |           2.6261 |
[32m[20230205 18:17:32 @agent_ppo2.py:191][0m |          -0.0134 |           7.2721 |           2.6282 |
[32m[20230205 18:17:33 @agent_ppo2.py:191][0m |          -0.0106 |           7.2008 |           2.6293 |
[32m[20230205 18:17:33 @agent_ppo2.py:136][0m Policy update time: 0.60 s
[32m[20230205 18:17:33 @agent_ppo2.py:144][0m Average TRAINING episode reward: 53.98
[32m[20230205 18:17:33 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 204.81
[32m[20230205 18:17:33 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -19.73
[32m[20230205 18:17:33 @agent_ppo2.py:149][0m Total time:       4.09 min
[32m[20230205 18:17:33 @agent_ppo2.py:151][0m 243712 total steps have happened
[32m[20230205 18:17:33 @agent_ppo2.py:127][0m #------------------------ Iteration 119 --------------------------#
[32m[20230205 18:17:34 @agent_ppo2.py:133][0m Sampling time: 0.58 s by 1 slaves
[32m[20230205 18:17:34 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:34 @agent_ppo2.py:191][0m |           0.0021 |          25.7004 |           2.6927 |
[32m[20230205 18:17:34 @agent_ppo2.py:191][0m |          -0.0090 |          13.0236 |           2.6924 |
[32m[20230205 18:17:34 @agent_ppo2.py:191][0m |           0.0051 |          10.6925 |           2.6880 |
[32m[20230205 18:17:34 @agent_ppo2.py:191][0m |           0.0051 |           9.5979 |           2.6844 |
[32m[20230205 18:17:34 @agent_ppo2.py:191][0m |          -0.0122 |           8.5379 |           2.6832 |
[32m[20230205 18:17:34 @agent_ppo2.py:191][0m |          -0.0090 |           8.0476 |           2.6832 |
[32m[20230205 18:17:34 @agent_ppo2.py:191][0m |          -0.0077 |           7.8939 |           2.6839 |
[32m[20230205 18:17:34 @agent_ppo2.py:191][0m |          -0.0137 |           7.3506 |           2.6814 |
[32m[20230205 18:17:34 @agent_ppo2.py:191][0m |          -0.0192 |           6.9299 |           2.6799 |
[32m[20230205 18:17:34 @agent_ppo2.py:191][0m |          -0.0087 |           6.7164 |           2.6790 |
[32m[20230205 18:17:34 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:17:35 @agent_ppo2.py:144][0m Average TRAINING episode reward: 44.44
[32m[20230205 18:17:35 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 81.67
[32m[20230205 18:17:35 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 131.57
[32m[20230205 18:17:35 @agent_ppo2.py:149][0m Total time:       4.12 min
[32m[20230205 18:17:35 @agent_ppo2.py:151][0m 245760 total steps have happened
[32m[20230205 18:17:35 @agent_ppo2.py:127][0m #------------------------ Iteration 120 --------------------------#
[32m[20230205 18:17:35 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:17:35 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:35 @agent_ppo2.py:191][0m |           0.0054 |          30.2161 |           2.6331 |
[32m[20230205 18:17:35 @agent_ppo2.py:191][0m |           0.0002 |          26.2998 |           2.6306 |
[32m[20230205 18:17:35 @agent_ppo2.py:191][0m |          -0.0109 |          22.6831 |           2.6285 |
[32m[20230205 18:17:35 @agent_ppo2.py:191][0m |          -0.0102 |          22.0820 |           2.6279 |
[32m[20230205 18:17:35 @agent_ppo2.py:191][0m |          -0.0118 |          20.8153 |           2.6285 |
[32m[20230205 18:17:35 @agent_ppo2.py:191][0m |          -0.0170 |          19.9888 |           2.6282 |
[32m[20230205 18:17:35 @agent_ppo2.py:191][0m |          -0.0191 |          19.8694 |           2.6298 |
[32m[20230205 18:17:36 @agent_ppo2.py:191][0m |          -0.0213 |          19.2321 |           2.6286 |
[32m[20230205 18:17:36 @agent_ppo2.py:191][0m |          -0.0214 |          18.4772 |           2.6312 |
[32m[20230205 18:17:36 @agent_ppo2.py:191][0m |          -0.0216 |          18.1793 |           2.6336 |
[32m[20230205 18:17:36 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:17:36 @agent_ppo2.py:144][0m Average TRAINING episode reward: 54.37
[32m[20230205 18:17:36 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 90.96
[32m[20230205 18:17:36 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 182.00
[32m[20230205 18:17:36 @agent_ppo2.py:149][0m Total time:       4.15 min
[32m[20230205 18:17:36 @agent_ppo2.py:151][0m 247808 total steps have happened
[32m[20230205 18:17:36 @agent_ppo2.py:127][0m #------------------------ Iteration 121 --------------------------#
[32m[20230205 18:17:37 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:17:37 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:37 @agent_ppo2.py:191][0m |           0.0005 |          25.3538 |           2.7679 |
[32m[20230205 18:17:37 @agent_ppo2.py:191][0m |          -0.0098 |          16.8848 |           2.7665 |
[32m[20230205 18:17:37 @agent_ppo2.py:191][0m |          -0.0109 |          14.9058 |           2.7651 |
[32m[20230205 18:17:37 @agent_ppo2.py:191][0m |          -0.0124 |          12.7881 |           2.7646 |
[32m[20230205 18:17:37 @agent_ppo2.py:191][0m |          -0.0094 |          12.1044 |           2.7658 |
[32m[20230205 18:17:37 @agent_ppo2.py:191][0m |          -0.0142 |          11.2702 |           2.7648 |
[32m[20230205 18:17:37 @agent_ppo2.py:191][0m |          -0.0157 |          10.8806 |           2.7649 |
[32m[20230205 18:17:37 @agent_ppo2.py:191][0m |          -0.0123 |          10.5723 |           2.7641 |
[32m[20230205 18:17:37 @agent_ppo2.py:191][0m |          -0.0154 |          10.1780 |           2.7648 |
[32m[20230205 18:17:37 @agent_ppo2.py:191][0m |          -0.0192 |           9.8694 |           2.7644 |
[32m[20230205 18:17:37 @agent_ppo2.py:136][0m Policy update time: 0.49 s
[32m[20230205 18:17:38 @agent_ppo2.py:144][0m Average TRAINING episode reward: 112.89
[32m[20230205 18:17:38 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 234.23
[32m[20230205 18:17:38 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 270.15
[32m[20230205 18:17:38 @agent_ppo2.py:149][0m Total time:       4.17 min
[32m[20230205 18:17:38 @agent_ppo2.py:151][0m 249856 total steps have happened
[32m[20230205 18:17:38 @agent_ppo2.py:127][0m #------------------------ Iteration 122 --------------------------#
[32m[20230205 18:17:38 @agent_ppo2.py:133][0m Sampling time: 0.74 s by 1 slaves
[32m[20230205 18:17:39 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:39 @agent_ppo2.py:191][0m |          -0.0009 |           5.9209 |           2.7862 |
[32m[20230205 18:17:39 @agent_ppo2.py:191][0m |          -0.0056 |           5.1934 |           2.7769 |
[32m[20230205 18:17:39 @agent_ppo2.py:191][0m |          -0.0075 |           4.9367 |           2.7773 |
[32m[20230205 18:17:39 @agent_ppo2.py:191][0m |          -0.0087 |           4.7540 |           2.7756 |
[32m[20230205 18:17:39 @agent_ppo2.py:191][0m |          -0.0093 |           4.6112 |           2.7764 |
[32m[20230205 18:17:39 @agent_ppo2.py:191][0m |          -0.0103 |           4.5255 |           2.7778 |
[32m[20230205 18:17:39 @agent_ppo2.py:191][0m |          -0.0112 |           4.4141 |           2.7767 |
[32m[20230205 18:17:39 @agent_ppo2.py:191][0m |          -0.0110 |           4.3163 |           2.7769 |
[32m[20230205 18:17:39 @agent_ppo2.py:191][0m |          -0.0120 |           4.2264 |           2.7756 |
[32m[20230205 18:17:39 @agent_ppo2.py:191][0m |          -0.0119 |           4.1504 |           2.7785 |
[32m[20230205 18:17:39 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:17:40 @agent_ppo2.py:144][0m Average TRAINING episode reward: 171.83
[32m[20230205 18:17:40 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 186.68
[32m[20230205 18:17:40 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 270.72
[32m[20230205 18:17:40 @agent_ppo2.py:149][0m Total time:       4.21 min
[32m[20230205 18:17:40 @agent_ppo2.py:151][0m 251904 total steps have happened
[32m[20230205 18:17:40 @agent_ppo2.py:127][0m #------------------------ Iteration 123 --------------------------#
[32m[20230205 18:17:41 @agent_ppo2.py:133][0m Sampling time: 0.71 s by 1 slaves
[32m[20230205 18:17:41 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:41 @agent_ppo2.py:191][0m |          -0.0010 |           3.5291 |           2.7337 |
[32m[20230205 18:17:41 @agent_ppo2.py:191][0m |          -0.0055 |           3.1056 |           2.7326 |
[32m[20230205 18:17:41 @agent_ppo2.py:191][0m |          -0.0087 |           2.9728 |           2.7333 |
[32m[20230205 18:17:41 @agent_ppo2.py:191][0m |          -0.0100 |           2.8851 |           2.7325 |
[32m[20230205 18:17:41 @agent_ppo2.py:191][0m |          -0.0110 |           2.8234 |           2.7316 |
[32m[20230205 18:17:41 @agent_ppo2.py:191][0m |          -0.0119 |           2.7781 |           2.7319 |
[32m[20230205 18:17:41 @agent_ppo2.py:191][0m |          -0.0123 |           2.7375 |           2.7323 |
[32m[20230205 18:17:41 @agent_ppo2.py:191][0m |          -0.0132 |           2.7049 |           2.7314 |
[32m[20230205 18:17:41 @agent_ppo2.py:191][0m |          -0.0137 |           2.6744 |           2.7332 |
[32m[20230205 18:17:41 @agent_ppo2.py:191][0m |          -0.0141 |           2.6472 |           2.7335 |
[32m[20230205 18:17:41 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:17:42 @agent_ppo2.py:144][0m Average TRAINING episode reward: 209.17
[32m[20230205 18:17:42 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 214.34
[32m[20230205 18:17:42 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 204.14
[32m[20230205 18:17:42 @agent_ppo2.py:149][0m Total time:       4.24 min
[32m[20230205 18:17:42 @agent_ppo2.py:151][0m 253952 total steps have happened
[32m[20230205 18:17:42 @agent_ppo2.py:127][0m #------------------------ Iteration 124 --------------------------#
[32m[20230205 18:17:42 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:17:43 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:43 @agent_ppo2.py:191][0m |          -0.0026 |           4.5250 |           2.7461 |
[32m[20230205 18:17:43 @agent_ppo2.py:191][0m |          -0.0094 |           3.9434 |           2.7438 |
[32m[20230205 18:17:43 @agent_ppo2.py:191][0m |          -0.0120 |           3.7631 |           2.7403 |
[32m[20230205 18:17:43 @agent_ppo2.py:191][0m |          -0.0135 |           3.6476 |           2.7396 |
[32m[20230205 18:17:43 @agent_ppo2.py:191][0m |          -0.0145 |           3.5622 |           2.7354 |
[32m[20230205 18:17:43 @agent_ppo2.py:191][0m |          -0.0151 |           3.4962 |           2.7359 |
[32m[20230205 18:17:43 @agent_ppo2.py:191][0m |          -0.0158 |           3.4259 |           2.7326 |
[32m[20230205 18:17:43 @agent_ppo2.py:191][0m |          -0.0162 |           3.3759 |           2.7322 |
[32m[20230205 18:17:43 @agent_ppo2.py:191][0m |          -0.0165 |           3.3280 |           2.7304 |
[32m[20230205 18:17:43 @agent_ppo2.py:191][0m |          -0.0170 |           3.2854 |           2.7288 |
[32m[20230205 18:17:43 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:17:44 @agent_ppo2.py:144][0m Average TRAINING episode reward: 194.81
[32m[20230205 18:17:44 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 201.92
[32m[20230205 18:17:44 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 219.64
[32m[20230205 18:17:44 @agent_ppo2.py:149][0m Total time:       4.27 min
[32m[20230205 18:17:44 @agent_ppo2.py:151][0m 256000 total steps have happened
[32m[20230205 18:17:44 @agent_ppo2.py:127][0m #------------------------ Iteration 125 --------------------------#
[32m[20230205 18:17:44 @agent_ppo2.py:133][0m Sampling time: 0.71 s by 1 slaves
[32m[20230205 18:17:45 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:45 @agent_ppo2.py:191][0m |           0.0006 |           3.6634 |           2.7339 |
[32m[20230205 18:17:45 @agent_ppo2.py:191][0m |          -0.0059 |           3.3722 |           2.7343 |
[32m[20230205 18:17:45 @agent_ppo2.py:191][0m |          -0.0067 |           3.2997 |           2.7342 |
[32m[20230205 18:17:45 @agent_ppo2.py:191][0m |          -0.0093 |           3.2396 |           2.7343 |
[32m[20230205 18:17:45 @agent_ppo2.py:191][0m |          -0.0095 |           3.1969 |           2.7356 |
[32m[20230205 18:17:45 @agent_ppo2.py:191][0m |          -0.0110 |           3.1607 |           2.7353 |
[32m[20230205 18:17:45 @agent_ppo2.py:191][0m |          -0.0106 |           3.1332 |           2.7359 |
[32m[20230205 18:17:45 @agent_ppo2.py:191][0m |          -0.0117 |           3.0978 |           2.7359 |
[32m[20230205 18:17:45 @agent_ppo2.py:191][0m |          -0.0120 |           3.0733 |           2.7373 |
[32m[20230205 18:17:45 @agent_ppo2.py:191][0m |          -0.0121 |           3.0454 |           2.7384 |
[32m[20230205 18:17:45 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:17:46 @agent_ppo2.py:144][0m Average TRAINING episode reward: 212.73
[32m[20230205 18:17:46 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 233.24
[32m[20230205 18:17:46 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.01
[32m[20230205 18:17:46 @agent_ppo2.py:149][0m Total time:       4.31 min
[32m[20230205 18:17:46 @agent_ppo2.py:151][0m 258048 total steps have happened
[32m[20230205 18:17:46 @agent_ppo2.py:127][0m #------------------------ Iteration 126 --------------------------#
[32m[20230205 18:17:46 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:17:46 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:46 @agent_ppo2.py:191][0m |           0.0041 |          14.2511 |           2.7724 |
[32m[20230205 18:17:46 @agent_ppo2.py:191][0m |          -0.0033 |           7.8416 |           2.7654 |
[32m[20230205 18:17:46 @agent_ppo2.py:191][0m |          -0.0073 |           7.4471 |           2.7621 |
[32m[20230205 18:17:46 @agent_ppo2.py:191][0m |          -0.0076 |           6.4590 |           2.7628 |
[32m[20230205 18:17:47 @agent_ppo2.py:191][0m |          -0.0099 |           6.1609 |           2.7595 |
[32m[20230205 18:17:47 @agent_ppo2.py:191][0m |          -0.0104 |           5.9646 |           2.7585 |
[32m[20230205 18:17:47 @agent_ppo2.py:191][0m |          -0.0119 |           5.7648 |           2.7629 |
[32m[20230205 18:17:47 @agent_ppo2.py:191][0m |          -0.0123 |           5.6620 |           2.7649 |
[32m[20230205 18:17:47 @agent_ppo2.py:191][0m |          -0.0128 |           5.4833 |           2.7652 |
[32m[20230205 18:17:47 @agent_ppo2.py:191][0m |          -0.0133 |           5.4202 |           2.7649 |
[32m[20230205 18:17:47 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:17:47 @agent_ppo2.py:144][0m Average TRAINING episode reward: 125.88
[32m[20230205 18:17:47 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 234.91
[32m[20230205 18:17:47 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 271.65
[32m[20230205 18:17:47 @agent_ppo2.py:149][0m Total time:       4.33 min
[32m[20230205 18:17:47 @agent_ppo2.py:151][0m 260096 total steps have happened
[32m[20230205 18:17:47 @agent_ppo2.py:127][0m #------------------------ Iteration 127 --------------------------#
[32m[20230205 18:17:48 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:17:48 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:48 @agent_ppo2.py:191][0m |           0.0006 |           5.4758 |           2.8008 |
[32m[20230205 18:17:48 @agent_ppo2.py:191][0m |          -0.0024 |           4.4264 |           2.7975 |
[32m[20230205 18:17:48 @agent_ppo2.py:191][0m |          -0.0045 |           4.1512 |           2.7968 |
[32m[20230205 18:17:48 @agent_ppo2.py:191][0m |          -0.0063 |           3.9894 |           2.7989 |
[32m[20230205 18:17:48 @agent_ppo2.py:191][0m |          -0.0075 |           3.8835 |           2.7963 |
[32m[20230205 18:17:49 @agent_ppo2.py:191][0m |          -0.0086 |           3.8019 |           2.7988 |
[32m[20230205 18:17:49 @agent_ppo2.py:191][0m |          -0.0093 |           3.7340 |           2.7989 |
[32m[20230205 18:17:49 @agent_ppo2.py:191][0m |          -0.0102 |           3.6562 |           2.8003 |
[32m[20230205 18:17:49 @agent_ppo2.py:191][0m |          -0.0108 |           3.6027 |           2.8001 |
[32m[20230205 18:17:49 @agent_ppo2.py:191][0m |          -0.0114 |           3.5561 |           2.8010 |
[32m[20230205 18:17:49 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:17:49 @agent_ppo2.py:144][0m Average TRAINING episode reward: 208.85
[32m[20230205 18:17:49 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 222.07
[32m[20230205 18:17:49 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 197.35
[32m[20230205 18:17:49 @agent_ppo2.py:149][0m Total time:       4.37 min
[32m[20230205 18:17:49 @agent_ppo2.py:151][0m 262144 total steps have happened
[32m[20230205 18:17:49 @agent_ppo2.py:127][0m #------------------------ Iteration 128 --------------------------#
[32m[20230205 18:17:50 @agent_ppo2.py:133][0m Sampling time: 0.78 s by 1 slaves
[32m[20230205 18:17:50 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:50 @agent_ppo2.py:191][0m |           0.0280 |          17.3852 |           2.8079 |
[32m[20230205 18:17:50 @agent_ppo2.py:191][0m |          -0.0005 |          12.0998 |           2.8064 |
[32m[20230205 18:17:50 @agent_ppo2.py:191][0m |          -0.0058 |          10.8979 |           2.8066 |
[32m[20230205 18:17:50 @agent_ppo2.py:191][0m |           0.0034 |          10.9029 |           2.8069 |
[32m[20230205 18:17:51 @agent_ppo2.py:191][0m |          -0.0101 |           9.5228 |           2.8068 |
[32m[20230205 18:17:51 @agent_ppo2.py:191][0m |          -0.0128 |           9.1240 |           2.8047 |
[32m[20230205 18:17:51 @agent_ppo2.py:191][0m |          -0.0084 |           8.8932 |           2.8046 |
[32m[20230205 18:17:51 @agent_ppo2.py:191][0m |          -0.0171 |           8.7979 |           2.8053 |
[32m[20230205 18:17:51 @agent_ppo2.py:191][0m |           0.0333 |           8.9765 |           2.8052 |
[32m[20230205 18:17:51 @agent_ppo2.py:191][0m |          -0.0095 |           8.7611 |           2.8001 |
[32m[20230205 18:17:51 @agent_ppo2.py:136][0m Policy update time: 0.76 s
[32m[20230205 18:17:51 @agent_ppo2.py:144][0m Average TRAINING episode reward: 128.99
[32m[20230205 18:17:51 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 231.79
[32m[20230205 18:17:51 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 265.80
[32m[20230205 18:17:51 @agent_ppo2.py:149][0m Total time:       4.40 min
[32m[20230205 18:17:51 @agent_ppo2.py:151][0m 264192 total steps have happened
[32m[20230205 18:17:51 @agent_ppo2.py:127][0m #------------------------ Iteration 129 --------------------------#
[32m[20230205 18:17:52 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:17:52 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:52 @agent_ppo2.py:191][0m |          -0.0020 |          24.9203 |           2.8099 |
[32m[20230205 18:17:52 @agent_ppo2.py:191][0m |          -0.0085 |          17.1001 |           2.8079 |
[32m[20230205 18:17:52 @agent_ppo2.py:191][0m |          -0.0096 |          15.0595 |           2.8064 |
[32m[20230205 18:17:53 @agent_ppo2.py:191][0m |          -0.0100 |          14.2479 |           2.8057 |
[32m[20230205 18:17:53 @agent_ppo2.py:191][0m |          -0.0028 |          13.3717 |           2.8063 |
[32m[20230205 18:17:53 @agent_ppo2.py:191][0m |          -0.0146 |          12.5977 |           2.8033 |
[32m[20230205 18:17:53 @agent_ppo2.py:191][0m |          -0.0036 |          13.9900 |           2.8044 |
[32m[20230205 18:17:53 @agent_ppo2.py:191][0m |          -0.0136 |          11.3415 |           2.8049 |
[32m[20230205 18:17:53 @agent_ppo2.py:191][0m |          -0.0266 |          11.3575 |           2.8042 |
[32m[20230205 18:17:53 @agent_ppo2.py:191][0m |          -0.0156 |          10.6910 |           2.8043 |
[32m[20230205 18:17:53 @agent_ppo2.py:136][0m Policy update time: 0.72 s
[32m[20230205 18:17:53 @agent_ppo2.py:144][0m Average TRAINING episode reward: 128.16
[32m[20230205 18:17:53 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 240.95
[32m[20230205 18:17:53 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 184.39
[32m[20230205 18:17:53 @agent_ppo2.py:149][0m Total time:       4.43 min
[32m[20230205 18:17:53 @agent_ppo2.py:151][0m 266240 total steps have happened
[32m[20230205 18:17:53 @agent_ppo2.py:127][0m #------------------------ Iteration 130 --------------------------#
[32m[20230205 18:17:54 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:17:54 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:54 @agent_ppo2.py:191][0m |           0.0012 |          45.8880 |           2.8070 |
[32m[20230205 18:17:54 @agent_ppo2.py:191][0m |          -0.0005 |          22.9310 |           2.8039 |
[32m[20230205 18:17:54 @agent_ppo2.py:191][0m |          -0.0006 |          18.6103 |           2.8015 |
[32m[20230205 18:17:54 @agent_ppo2.py:191][0m |          -0.0110 |          16.8460 |           2.7996 |
[32m[20230205 18:17:54 @agent_ppo2.py:191][0m |          -0.0021 |          15.5949 |           2.7969 |
[32m[20230205 18:17:54 @agent_ppo2.py:191][0m |          -0.0110 |          14.6027 |           2.7973 |
[32m[20230205 18:17:54 @agent_ppo2.py:191][0m |          -0.0166 |          13.8712 |           2.7944 |
[32m[20230205 18:17:54 @agent_ppo2.py:191][0m |          -0.0148 |          13.1963 |           2.7920 |
[32m[20230205 18:17:54 @agent_ppo2.py:191][0m |          -0.0088 |          12.7225 |           2.7946 |
[32m[20230205 18:17:54 @agent_ppo2.py:191][0m |          -0.0141 |          12.8662 |           2.7951 |
[32m[20230205 18:17:54 @agent_ppo2.py:136][0m Policy update time: 0.47 s
[32m[20230205 18:17:55 @agent_ppo2.py:144][0m Average TRAINING episode reward: 103.11
[32m[20230205 18:17:55 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 235.10
[32m[20230205 18:17:55 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 58.09
[32m[20230205 18:17:55 @agent_ppo2.py:149][0m Total time:       4.46 min
[32m[20230205 18:17:55 @agent_ppo2.py:151][0m 268288 total steps have happened
[32m[20230205 18:17:55 @agent_ppo2.py:127][0m #------------------------ Iteration 131 --------------------------#
[32m[20230205 18:17:56 @agent_ppo2.py:133][0m Sampling time: 0.71 s by 1 slaves
[32m[20230205 18:17:56 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:56 @agent_ppo2.py:191][0m |          -0.0022 |           5.2639 |           2.8597 |
[32m[20230205 18:17:56 @agent_ppo2.py:191][0m |          -0.0070 |           4.4785 |           2.8546 |
[32m[20230205 18:17:56 @agent_ppo2.py:191][0m |          -0.0101 |           4.1650 |           2.8544 |
[32m[20230205 18:17:56 @agent_ppo2.py:191][0m |          -0.0109 |           3.9710 |           2.8522 |
[32m[20230205 18:17:56 @agent_ppo2.py:191][0m |          -0.0114 |           3.8593 |           2.8532 |
[32m[20230205 18:17:56 @agent_ppo2.py:191][0m |          -0.0133 |           3.7624 |           2.8538 |
[32m[20230205 18:17:56 @agent_ppo2.py:191][0m |          -0.0129 |           3.6971 |           2.8546 |
[32m[20230205 18:17:56 @agent_ppo2.py:191][0m |          -0.0138 |           3.6339 |           2.8546 |
[32m[20230205 18:17:56 @agent_ppo2.py:191][0m |          -0.0149 |           3.5920 |           2.8585 |
[32m[20230205 18:17:56 @agent_ppo2.py:191][0m |          -0.0150 |           3.5488 |           2.8575 |
[32m[20230205 18:17:56 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:17:57 @agent_ppo2.py:144][0m Average TRAINING episode reward: 225.67
[32m[20230205 18:17:57 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 230.30
[32m[20230205 18:17:57 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 269.61
[32m[20230205 18:17:57 @agent_ppo2.py:149][0m Total time:       4.50 min
[32m[20230205 18:17:57 @agent_ppo2.py:151][0m 270336 total steps have happened
[32m[20230205 18:17:57 @agent_ppo2.py:127][0m #------------------------ Iteration 132 --------------------------#
[32m[20230205 18:17:58 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:17:58 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:17:58 @agent_ppo2.py:191][0m |           0.0033 |          27.9261 |           2.9255 |
[32m[20230205 18:17:58 @agent_ppo2.py:191][0m |           0.0002 |          11.3550 |           2.9278 |
[32m[20230205 18:17:58 @agent_ppo2.py:191][0m |          -0.0032 |           8.7047 |           2.9257 |
[32m[20230205 18:17:58 @agent_ppo2.py:191][0m |          -0.0020 |           7.6105 |           2.9248 |
[32m[20230205 18:17:58 @agent_ppo2.py:191][0m |          -0.0064 |           7.2253 |           2.9263 |
[32m[20230205 18:17:58 @agent_ppo2.py:191][0m |          -0.0069 |           6.3735 |           2.9260 |
[32m[20230205 18:17:58 @agent_ppo2.py:191][0m |          -0.0082 |           6.0266 |           2.9268 |
[32m[20230205 18:17:58 @agent_ppo2.py:191][0m |          -0.0103 |           5.7873 |           2.9252 |
[32m[20230205 18:17:58 @agent_ppo2.py:191][0m |          -0.0097 |           5.2676 |           2.9256 |
[32m[20230205 18:17:58 @agent_ppo2.py:191][0m |          -0.0101 |           4.9785 |           2.9254 |
[32m[20230205 18:17:58 @agent_ppo2.py:136][0m Policy update time: 0.62 s
[32m[20230205 18:17:59 @agent_ppo2.py:144][0m Average TRAINING episode reward: 153.71
[32m[20230205 18:17:59 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 211.13
[32m[20230205 18:17:59 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 274.88
[32m[20230205 18:17:59 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 274.88
[32m[20230205 18:17:59 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 274.88
[32m[20230205 18:17:59 @agent_ppo2.py:149][0m Total time:       4.53 min
[32m[20230205 18:17:59 @agent_ppo2.py:151][0m 272384 total steps have happened
[32m[20230205 18:17:59 @agent_ppo2.py:127][0m #------------------------ Iteration 133 --------------------------#
[32m[20230205 18:18:00 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:18:00 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:00 @agent_ppo2.py:191][0m |           0.0001 |          43.3398 |           2.9249 |
[32m[20230205 18:18:00 @agent_ppo2.py:191][0m |          -0.0059 |          30.9176 |           2.9255 |
[32m[20230205 18:18:00 @agent_ppo2.py:191][0m |          -0.0093 |          27.3677 |           2.9245 |
[32m[20230205 18:18:00 @agent_ppo2.py:191][0m |          -0.0098 |          24.2367 |           2.9246 |
[32m[20230205 18:18:00 @agent_ppo2.py:191][0m |          -0.0092 |          22.6486 |           2.9243 |
[32m[20230205 18:18:00 @agent_ppo2.py:191][0m |          -0.0118 |          21.6792 |           2.9220 |
[32m[20230205 18:18:00 @agent_ppo2.py:191][0m |          -0.0123 |          20.5430 |           2.9218 |
[32m[20230205 18:18:00 @agent_ppo2.py:191][0m |          -0.0128 |          19.8629 |           2.9213 |
[32m[20230205 18:18:00 @agent_ppo2.py:191][0m |          -0.0124 |          19.3218 |           2.9199 |
[32m[20230205 18:18:00 @agent_ppo2.py:191][0m |          -0.0122 |          18.8192 |           2.9197 |
[32m[20230205 18:18:00 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:18:01 @agent_ppo2.py:144][0m Average TRAINING episode reward: 88.72
[32m[20230205 18:18:01 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 226.75
[32m[20230205 18:18:01 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 179.11
[32m[20230205 18:18:01 @agent_ppo2.py:149][0m Total time:       4.56 min
[32m[20230205 18:18:01 @agent_ppo2.py:151][0m 274432 total steps have happened
[32m[20230205 18:18:01 @agent_ppo2.py:127][0m #------------------------ Iteration 134 --------------------------#
[32m[20230205 18:18:01 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:18:01 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:02 @agent_ppo2.py:191][0m |          -0.0020 |          30.9394 |           2.8471 |
[32m[20230205 18:18:02 @agent_ppo2.py:191][0m |          -0.0101 |          20.0562 |           2.8456 |
[32m[20230205 18:18:02 @agent_ppo2.py:191][0m |          -0.0060 |          16.0535 |           2.8413 |
[32m[20230205 18:18:02 @agent_ppo2.py:191][0m |          -0.0076 |          13.4821 |           2.8422 |
[32m[20230205 18:18:02 @agent_ppo2.py:191][0m |           0.0038 |          11.9119 |           2.8399 |
[32m[20230205 18:18:02 @agent_ppo2.py:191][0m |          -0.0107 |          10.4805 |           2.8363 |
[32m[20230205 18:18:02 @agent_ppo2.py:191][0m |          -0.0089 |           9.4314 |           2.8370 |
[32m[20230205 18:18:02 @agent_ppo2.py:191][0m |          -0.0066 |           8.6367 |           2.8340 |
[32m[20230205 18:18:02 @agent_ppo2.py:191][0m |          -0.0149 |           7.8881 |           2.8326 |
[32m[20230205 18:18:02 @agent_ppo2.py:191][0m |          -0.0174 |           7.3968 |           2.8318 |
[32m[20230205 18:18:02 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:18:02 @agent_ppo2.py:144][0m Average TRAINING episode reward: 125.52
[32m[20230205 18:18:02 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 226.80
[32m[20230205 18:18:02 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 135.81
[32m[20230205 18:18:02 @agent_ppo2.py:149][0m Total time:       4.59 min
[32m[20230205 18:18:02 @agent_ppo2.py:151][0m 276480 total steps have happened
[32m[20230205 18:18:02 @agent_ppo2.py:127][0m #------------------------ Iteration 135 --------------------------#
[32m[20230205 18:18:03 @agent_ppo2.py:133][0m Sampling time: 0.70 s by 1 slaves
[32m[20230205 18:18:03 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:03 @agent_ppo2.py:191][0m |           0.0013 |          12.6828 |           2.9255 |
[32m[20230205 18:18:03 @agent_ppo2.py:191][0m |          -0.0059 |           9.3171 |           2.9170 |
[32m[20230205 18:18:03 @agent_ppo2.py:191][0m |          -0.0072 |           8.9282 |           2.9144 |
[32m[20230205 18:18:03 @agent_ppo2.py:191][0m |          -0.0093 |           8.6622 |           2.9127 |
[32m[20230205 18:18:04 @agent_ppo2.py:191][0m |          -0.0098 |           8.4535 |           2.9097 |
[32m[20230205 18:18:04 @agent_ppo2.py:191][0m |          -0.0127 |           8.2504 |           2.9069 |
[32m[20230205 18:18:04 @agent_ppo2.py:191][0m |          -0.0121 |           8.1862 |           2.9061 |
[32m[20230205 18:18:04 @agent_ppo2.py:191][0m |          -0.0137 |           8.0137 |           2.9047 |
[32m[20230205 18:18:04 @agent_ppo2.py:191][0m |          -0.0134 |           7.9111 |           2.9054 |
[32m[20230205 18:18:04 @agent_ppo2.py:191][0m |          -0.0129 |           7.8945 |           2.9053 |
[32m[20230205 18:18:04 @agent_ppo2.py:136][0m Policy update time: 0.68 s
[32m[20230205 18:18:04 @agent_ppo2.py:144][0m Average TRAINING episode reward: 233.05
[32m[20230205 18:18:04 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 240.17
[32m[20230205 18:18:04 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.09
[32m[20230205 18:18:04 @agent_ppo2.py:149][0m Total time:       4.62 min
[32m[20230205 18:18:04 @agent_ppo2.py:151][0m 278528 total steps have happened
[32m[20230205 18:18:04 @agent_ppo2.py:127][0m #------------------------ Iteration 136 --------------------------#
[32m[20230205 18:18:05 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:18:05 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:05 @agent_ppo2.py:191][0m |          -0.0015 |           4.3462 |           2.8447 |
[32m[20230205 18:18:05 @agent_ppo2.py:191][0m |          -0.0077 |           3.9622 |           2.8382 |
[32m[20230205 18:18:05 @agent_ppo2.py:191][0m |          -0.0091 |           3.7359 |           2.8325 |
[32m[20230205 18:18:06 @agent_ppo2.py:191][0m |          -0.0107 |           3.5731 |           2.8312 |
[32m[20230205 18:18:06 @agent_ppo2.py:191][0m |          -0.0119 |           3.4596 |           2.8324 |
[32m[20230205 18:18:06 @agent_ppo2.py:191][0m |          -0.0122 |           3.3726 |           2.8337 |
[32m[20230205 18:18:06 @agent_ppo2.py:191][0m |          -0.0128 |           3.2986 |           2.8308 |
[32m[20230205 18:18:06 @agent_ppo2.py:191][0m |          -0.0131 |           3.2375 |           2.8332 |
[32m[20230205 18:18:06 @agent_ppo2.py:191][0m |          -0.0140 |           3.1866 |           2.8318 |
[32m[20230205 18:18:06 @agent_ppo2.py:191][0m |          -0.0143 |           3.1511 |           2.8322 |
[32m[20230205 18:18:06 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:18:07 @agent_ppo2.py:144][0m Average TRAINING episode reward: 201.67
[32m[20230205 18:18:07 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 206.13
[32m[20230205 18:18:07 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 261.83
[32m[20230205 18:18:07 @agent_ppo2.py:149][0m Total time:       4.65 min
[32m[20230205 18:18:07 @agent_ppo2.py:151][0m 280576 total steps have happened
[32m[20230205 18:18:07 @agent_ppo2.py:127][0m #------------------------ Iteration 137 --------------------------#
[32m[20230205 18:18:07 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:18:07 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:07 @agent_ppo2.py:191][0m |          -0.0008 |           3.0115 |           2.8148 |
[32m[20230205 18:18:07 @agent_ppo2.py:191][0m |          -0.0047 |           2.8705 |           2.8093 |
[32m[20230205 18:18:08 @agent_ppo2.py:191][0m |          -0.0075 |           2.8061 |           2.8092 |
[32m[20230205 18:18:08 @agent_ppo2.py:191][0m |          -0.0087 |           2.7555 |           2.8082 |
[32m[20230205 18:18:08 @agent_ppo2.py:191][0m |          -0.0101 |           2.7145 |           2.8053 |
[32m[20230205 18:18:08 @agent_ppo2.py:191][0m |          -0.0113 |           2.6831 |           2.8070 |
[32m[20230205 18:18:08 @agent_ppo2.py:191][0m |          -0.0114 |           2.6560 |           2.8061 |
[32m[20230205 18:18:08 @agent_ppo2.py:191][0m |          -0.0121 |           2.6347 |           2.8027 |
[32m[20230205 18:18:08 @agent_ppo2.py:191][0m |          -0.0130 |           2.6078 |           2.8032 |
[32m[20230205 18:18:08 @agent_ppo2.py:191][0m |          -0.0131 |           2.5924 |           2.8036 |
[32m[20230205 18:18:08 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:18:09 @agent_ppo2.py:144][0m Average TRAINING episode reward: 220.50
[32m[20230205 18:18:09 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 225.14
[32m[20230205 18:18:09 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 143.93
[32m[20230205 18:18:09 @agent_ppo2.py:149][0m Total time:       4.69 min
[32m[20230205 18:18:09 @agent_ppo2.py:151][0m 282624 total steps have happened
[32m[20230205 18:18:09 @agent_ppo2.py:127][0m #------------------------ Iteration 138 --------------------------#
[32m[20230205 18:18:09 @agent_ppo2.py:133][0m Sampling time: 0.64 s by 1 slaves
[32m[20230205 18:18:09 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:09 @agent_ppo2.py:191][0m |           0.0003 |          30.1431 |           2.7858 |
[32m[20230205 18:18:09 @agent_ppo2.py:191][0m |          -0.0079 |          15.6045 |           2.7836 |
[32m[20230205 18:18:09 @agent_ppo2.py:191][0m |           0.0015 |          13.6536 |           2.7841 |
[32m[20230205 18:18:09 @agent_ppo2.py:191][0m |           0.0073 |          12.8095 |           2.7794 |
[32m[20230205 18:18:10 @agent_ppo2.py:191][0m |          -0.0132 |           9.8407 |           2.7753 |
[32m[20230205 18:18:10 @agent_ppo2.py:191][0m |          -0.0122 |           9.4863 |           2.7785 |
[32m[20230205 18:18:10 @agent_ppo2.py:191][0m |          -0.0117 |           9.0380 |           2.7776 |
[32m[20230205 18:18:10 @agent_ppo2.py:191][0m |          -0.0126 |           8.7088 |           2.7771 |
[32m[20230205 18:18:10 @agent_ppo2.py:191][0m |          -0.0122 |           8.5711 |           2.7756 |
[32m[20230205 18:18:10 @agent_ppo2.py:191][0m |          -0.0138 |           8.4070 |           2.7762 |
[32m[20230205 18:18:10 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:18:10 @agent_ppo2.py:144][0m Average TRAINING episode reward: 147.25
[32m[20230205 18:18:10 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 238.85
[32m[20230205 18:18:10 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.25
[32m[20230205 18:18:10 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 275.25
[32m[20230205 18:18:10 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 275.25
[32m[20230205 18:18:10 @agent_ppo2.py:149][0m Total time:       4.72 min
[32m[20230205 18:18:10 @agent_ppo2.py:151][0m 284672 total steps have happened
[32m[20230205 18:18:10 @agent_ppo2.py:127][0m #------------------------ Iteration 139 --------------------------#
[32m[20230205 18:18:11 @agent_ppo2.py:133][0m Sampling time: 0.47 s by 1 slaves
[32m[20230205 18:18:11 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:11 @agent_ppo2.py:191][0m |           0.0038 |          46.2174 |           2.8448 |
[32m[20230205 18:18:11 @agent_ppo2.py:191][0m |          -0.0070 |          30.8267 |           2.8400 |
[32m[20230205 18:18:11 @agent_ppo2.py:191][0m |          -0.0090 |          28.7664 |           2.8420 |
[32m[20230205 18:18:11 @agent_ppo2.py:191][0m |          -0.0113 |          25.4832 |           2.8427 |
[32m[20230205 18:18:11 @agent_ppo2.py:191][0m |          -0.0095 |          23.5497 |           2.8399 |
[32m[20230205 18:18:11 @agent_ppo2.py:191][0m |          -0.0123 |          21.3675 |           2.8422 |
[32m[20230205 18:18:11 @agent_ppo2.py:191][0m |          -0.0120 |          20.2698 |           2.8436 |
[32m[20230205 18:18:11 @agent_ppo2.py:191][0m |          -0.0125 |          18.6613 |           2.8421 |
[32m[20230205 18:18:11 @agent_ppo2.py:191][0m |          -0.0142 |          16.9845 |           2.8423 |
[32m[20230205 18:18:11 @agent_ppo2.py:191][0m |          -0.0172 |          16.1500 |           2.8446 |
[32m[20230205 18:18:11 @agent_ppo2.py:136][0m Policy update time: 0.46 s
[32m[20230205 18:18:12 @agent_ppo2.py:144][0m Average TRAINING episode reward: 94.23
[32m[20230205 18:18:12 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 224.70
[32m[20230205 18:18:12 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.16
[32m[20230205 18:18:12 @agent_ppo2.py:149][0m Total time:       4.74 min
[32m[20230205 18:18:12 @agent_ppo2.py:151][0m 286720 total steps have happened
[32m[20230205 18:18:12 @agent_ppo2.py:127][0m #------------------------ Iteration 140 --------------------------#
[32m[20230205 18:18:13 @agent_ppo2.py:133][0m Sampling time: 0.71 s by 1 slaves
[32m[20230205 18:18:13 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:13 @agent_ppo2.py:191][0m |          -0.0033 |           8.5478 |           2.8165 |
[32m[20230205 18:18:13 @agent_ppo2.py:191][0m |          -0.0075 |           6.4369 |           2.8118 |
[32m[20230205 18:18:13 @agent_ppo2.py:191][0m |          -0.0080 |           6.1548 |           2.8110 |
[32m[20230205 18:18:13 @agent_ppo2.py:191][0m |          -0.0065 |           5.9816 |           2.8110 |
[32m[20230205 18:18:13 @agent_ppo2.py:191][0m |          -0.0084 |           5.9255 |           2.8128 |
[32m[20230205 18:18:13 @agent_ppo2.py:191][0m |          -0.0099 |           5.8183 |           2.8121 |
[32m[20230205 18:18:13 @agent_ppo2.py:191][0m |          -0.0118 |           5.7850 |           2.8132 |
[32m[20230205 18:18:13 @agent_ppo2.py:191][0m |          -0.0110 |           5.7506 |           2.8137 |
[32m[20230205 18:18:13 @agent_ppo2.py:191][0m |          -0.0128 |           5.6938 |           2.8126 |
[32m[20230205 18:18:13 @agent_ppo2.py:191][0m |          -0.0122 |           5.7020 |           2.8139 |
[32m[20230205 18:18:13 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:18:14 @agent_ppo2.py:144][0m Average TRAINING episode reward: 219.21
[32m[20230205 18:18:14 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 229.72
[32m[20230205 18:18:14 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 169.04
[32m[20230205 18:18:14 @agent_ppo2.py:149][0m Total time:       4.78 min
[32m[20230205 18:18:14 @agent_ppo2.py:151][0m 288768 total steps have happened
[32m[20230205 18:18:14 @agent_ppo2.py:127][0m #------------------------ Iteration 141 --------------------------#
[32m[20230205 18:18:15 @agent_ppo2.py:133][0m Sampling time: 0.70 s by 1 slaves
[32m[20230205 18:18:15 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:15 @agent_ppo2.py:191][0m |           0.0040 |           4.0770 |           2.8899 |
[32m[20230205 18:18:15 @agent_ppo2.py:191][0m |          -0.0055 |           3.4904 |           2.8865 |
[32m[20230205 18:18:15 @agent_ppo2.py:191][0m |          -0.0036 |           3.3299 |           2.8854 |
[32m[20230205 18:18:15 @agent_ppo2.py:191][0m |          -0.0070 |           3.2611 |           2.8853 |
[32m[20230205 18:18:15 @agent_ppo2.py:191][0m |          -0.0113 |           3.1797 |           2.8853 |
[32m[20230205 18:18:15 @agent_ppo2.py:191][0m |          -0.0140 |           3.1439 |           2.8838 |
[32m[20230205 18:18:15 @agent_ppo2.py:191][0m |          -0.0117 |           3.1008 |           2.8841 |
[32m[20230205 18:18:15 @agent_ppo2.py:191][0m |          -0.0098 |           3.0605 |           2.8829 |
[32m[20230205 18:18:15 @agent_ppo2.py:191][0m |          -0.0151 |           3.0347 |           2.8824 |
[32m[20230205 18:18:15 @agent_ppo2.py:191][0m |          -0.0115 |           3.0003 |           2.8821 |
[32m[20230205 18:18:15 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:18:16 @agent_ppo2.py:144][0m Average TRAINING episode reward: 230.17
[32m[20230205 18:18:16 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 233.71
[32m[20230205 18:18:16 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 153.43
[32m[20230205 18:18:16 @agent_ppo2.py:149][0m Total time:       4.81 min
[32m[20230205 18:18:16 @agent_ppo2.py:151][0m 290816 total steps have happened
[32m[20230205 18:18:16 @agent_ppo2.py:127][0m #------------------------ Iteration 142 --------------------------#
[32m[20230205 18:18:17 @agent_ppo2.py:133][0m Sampling time: 0.70 s by 1 slaves
[32m[20230205 18:18:17 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:17 @agent_ppo2.py:191][0m |          -0.0016 |           3.8584 |           2.7952 |
[32m[20230205 18:18:17 @agent_ppo2.py:191][0m |          -0.0092 |           3.5967 |           2.7899 |
[32m[20230205 18:18:17 @agent_ppo2.py:191][0m |          -0.0078 |           3.4784 |           2.7894 |
[32m[20230205 18:18:17 @agent_ppo2.py:191][0m |          -0.0109 |           3.3902 |           2.7878 |
[32m[20230205 18:18:17 @agent_ppo2.py:191][0m |          -0.0140 |           3.3333 |           2.7889 |
[32m[20230205 18:18:17 @agent_ppo2.py:191][0m |          -0.0112 |           3.2857 |           2.7882 |
[32m[20230205 18:18:17 @agent_ppo2.py:191][0m |          -0.0120 |           3.2521 |           2.7882 |
[32m[20230205 18:18:17 @agent_ppo2.py:191][0m |          -0.0125 |           3.2268 |           2.7883 |
[32m[20230205 18:18:17 @agent_ppo2.py:191][0m |          -0.0145 |           3.2040 |           2.7872 |
[32m[20230205 18:18:17 @agent_ppo2.py:191][0m |          -0.0142 |           3.2652 |           2.7883 |
[32m[20230205 18:18:17 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:18:18 @agent_ppo2.py:144][0m Average TRAINING episode reward: 227.74
[32m[20230205 18:18:18 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 232.63
[32m[20230205 18:18:18 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 281.28
[32m[20230205 18:18:18 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 281.28
[32m[20230205 18:18:18 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 281.28
[32m[20230205 18:18:18 @agent_ppo2.py:149][0m Total time:       4.84 min
[32m[20230205 18:18:18 @agent_ppo2.py:151][0m 292864 total steps have happened
[32m[20230205 18:18:18 @agent_ppo2.py:127][0m #------------------------ Iteration 143 --------------------------#
[32m[20230205 18:18:18 @agent_ppo2.py:133][0m Sampling time: 0.69 s by 1 slaves
[32m[20230205 18:18:19 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:19 @agent_ppo2.py:191][0m |           0.0026 |           5.4026 |           2.8456 |
[32m[20230205 18:18:19 @agent_ppo2.py:191][0m |          -0.0019 |           5.2204 |           2.8453 |
[32m[20230205 18:18:19 @agent_ppo2.py:191][0m |          -0.0057 |           4.9124 |           2.8475 |
[32m[20230205 18:18:19 @agent_ppo2.py:191][0m |          -0.0097 |           4.7966 |           2.8489 |
[32m[20230205 18:18:19 @agent_ppo2.py:191][0m |          -0.0121 |           4.7516 |           2.8502 |
[32m[20230205 18:18:19 @agent_ppo2.py:191][0m |          -0.0125 |           4.6974 |           2.8496 |
[32m[20230205 18:18:19 @agent_ppo2.py:191][0m |          -0.0127 |           4.6555 |           2.8484 |
[32m[20230205 18:18:19 @agent_ppo2.py:191][0m |          -0.0135 |           4.6079 |           2.8509 |
[32m[20230205 18:18:19 @agent_ppo2.py:191][0m |          -0.0160 |           4.5871 |           2.8510 |
[32m[20230205 18:18:19 @agent_ppo2.py:191][0m |          -0.0100 |           4.5875 |           2.8514 |
[32m[20230205 18:18:19 @agent_ppo2.py:136][0m Policy update time: 0.68 s
[32m[20230205 18:18:20 @agent_ppo2.py:144][0m Average TRAINING episode reward: 235.15
[32m[20230205 18:18:20 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 237.68
[32m[20230205 18:18:20 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 160.83
[32m[20230205 18:18:20 @agent_ppo2.py:149][0m Total time:       4.87 min
[32m[20230205 18:18:20 @agent_ppo2.py:151][0m 294912 total steps have happened
[32m[20230205 18:18:20 @agent_ppo2.py:127][0m #------------------------ Iteration 144 --------------------------#
[32m[20230205 18:18:20 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:18:20 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:20 @agent_ppo2.py:191][0m |           0.0010 |          25.3825 |           2.8794 |
[32m[20230205 18:18:21 @agent_ppo2.py:191][0m |          -0.0012 |          13.9344 |           2.8780 |
[32m[20230205 18:18:21 @agent_ppo2.py:191][0m |          -0.0033 |          11.1242 |           2.8741 |
[32m[20230205 18:18:21 @agent_ppo2.py:191][0m |          -0.0037 |          10.0257 |           2.8709 |
[32m[20230205 18:18:21 @agent_ppo2.py:191][0m |          -0.0045 |           9.4690 |           2.8671 |
[32m[20230205 18:18:21 @agent_ppo2.py:191][0m |          -0.0063 |           8.8683 |           2.8670 |
[32m[20230205 18:18:21 @agent_ppo2.py:191][0m |          -0.0061 |           8.7924 |           2.8656 |
[32m[20230205 18:18:21 @agent_ppo2.py:191][0m |          -0.0068 |           8.3933 |           2.8642 |
[32m[20230205 18:18:21 @agent_ppo2.py:191][0m |          -0.0076 |           8.3833 |           2.8624 |
[32m[20230205 18:18:21 @agent_ppo2.py:191][0m |          -0.0073 |           7.9568 |           2.8588 |
[32m[20230205 18:18:21 @agent_ppo2.py:136][0m Policy update time: 0.62 s
[32m[20230205 18:18:22 @agent_ppo2.py:144][0m Average TRAINING episode reward: 168.98
[32m[20230205 18:18:22 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 231.66
[32m[20230205 18:18:22 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 109.96
[32m[20230205 18:18:22 @agent_ppo2.py:149][0m Total time:       4.90 min
[32m[20230205 18:18:22 @agent_ppo2.py:151][0m 296960 total steps have happened
[32m[20230205 18:18:22 @agent_ppo2.py:127][0m #------------------------ Iteration 145 --------------------------#
[32m[20230205 18:18:22 @agent_ppo2.py:133][0m Sampling time: 0.76 s by 1 slaves
[32m[20230205 18:18:22 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:22 @agent_ppo2.py:191][0m |           0.0040 |          22.6394 |           2.8778 |
[32m[20230205 18:18:23 @agent_ppo2.py:191][0m |          -0.0025 |          13.3542 |           2.8787 |
[32m[20230205 18:18:23 @agent_ppo2.py:191][0m |          -0.0045 |          11.6828 |           2.8809 |
[32m[20230205 18:18:23 @agent_ppo2.py:191][0m |          -0.0043 |          10.9360 |           2.8834 |
[32m[20230205 18:18:23 @agent_ppo2.py:191][0m |          -0.0090 |          10.1381 |           2.8818 |
[32m[20230205 18:18:23 @agent_ppo2.py:191][0m |          -0.0098 |           9.6301 |           2.8831 |
[32m[20230205 18:18:23 @agent_ppo2.py:191][0m |          -0.0103 |           9.2174 |           2.8843 |
[32m[20230205 18:18:23 @agent_ppo2.py:191][0m |          -0.0125 |           8.9041 |           2.8841 |
[32m[20230205 18:18:23 @agent_ppo2.py:191][0m |          -0.0110 |           8.6735 |           2.8838 |
[32m[20230205 18:18:23 @agent_ppo2.py:191][0m |          -0.0125 |           8.2459 |           2.8859 |
[32m[20230205 18:18:23 @agent_ppo2.py:136][0m Policy update time: 0.74 s
[32m[20230205 18:18:24 @agent_ppo2.py:144][0m Average TRAINING episode reward: 141.00
[32m[20230205 18:18:24 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 244.23
[32m[20230205 18:18:24 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 89.69
[32m[20230205 18:18:24 @agent_ppo2.py:149][0m Total time:       4.94 min
[32m[20230205 18:18:24 @agent_ppo2.py:151][0m 299008 total steps have happened
[32m[20230205 18:18:24 @agent_ppo2.py:127][0m #------------------------ Iteration 146 --------------------------#
[32m[20230205 18:18:24 @agent_ppo2.py:133][0m Sampling time: 0.69 s by 1 slaves
[32m[20230205 18:18:24 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:24 @agent_ppo2.py:191][0m |          -0.0013 |           6.1497 |           2.8932 |
[32m[20230205 18:18:24 @agent_ppo2.py:191][0m |          -0.0050 |           5.8487 |           2.8892 |
[32m[20230205 18:18:25 @agent_ppo2.py:191][0m |          -0.0071 |           5.6323 |           2.8854 |
[32m[20230205 18:18:25 @agent_ppo2.py:191][0m |          -0.0089 |           5.5385 |           2.8866 |
[32m[20230205 18:18:25 @agent_ppo2.py:191][0m |          -0.0100 |           5.4694 |           2.8851 |
[32m[20230205 18:18:25 @agent_ppo2.py:191][0m |          -0.0107 |           5.4095 |           2.8841 |
[32m[20230205 18:18:25 @agent_ppo2.py:191][0m |          -0.0100 |           5.4768 |           2.8808 |
[32m[20230205 18:18:25 @agent_ppo2.py:191][0m |          -0.0104 |           5.3749 |           2.8816 |
[32m[20230205 18:18:25 @agent_ppo2.py:191][0m |          -0.0125 |           5.3155 |           2.8791 |
[32m[20230205 18:18:25 @agent_ppo2.py:191][0m |          -0.0131 |           5.2535 |           2.8802 |
[32m[20230205 18:18:25 @agent_ppo2.py:136][0m Policy update time: 0.68 s
[32m[20230205 18:18:26 @agent_ppo2.py:144][0m Average TRAINING episode reward: 232.37
[32m[20230205 18:18:26 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 234.19
[32m[20230205 18:18:26 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 278.29
[32m[20230205 18:18:26 @agent_ppo2.py:149][0m Total time:       4.97 min
[32m[20230205 18:18:26 @agent_ppo2.py:151][0m 301056 total steps have happened
[32m[20230205 18:18:26 @agent_ppo2.py:127][0m #------------------------ Iteration 147 --------------------------#
[32m[20230205 18:18:26 @agent_ppo2.py:133][0m Sampling time: 0.70 s by 1 slaves
[32m[20230205 18:18:26 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:26 @agent_ppo2.py:191][0m |           0.0021 |          28.6161 |           2.9364 |
[32m[20230205 18:18:26 @agent_ppo2.py:191][0m |          -0.0066 |          14.3200 |           2.9314 |
[32m[20230205 18:18:26 @agent_ppo2.py:191][0m |          -0.0092 |          11.8688 |           2.9284 |
[32m[20230205 18:18:27 @agent_ppo2.py:191][0m |          -0.0086 |          10.5633 |           2.9282 |
[32m[20230205 18:18:27 @agent_ppo2.py:191][0m |          -0.0052 |           9.8012 |           2.9272 |
[32m[20230205 18:18:27 @agent_ppo2.py:191][0m |          -0.0109 |           9.1760 |           2.9252 |
[32m[20230205 18:18:27 @agent_ppo2.py:191][0m |          -0.0081 |           8.8490 |           2.9238 |
[32m[20230205 18:18:27 @agent_ppo2.py:191][0m |          -0.0126 |           8.2817 |           2.9218 |
[32m[20230205 18:18:27 @agent_ppo2.py:191][0m |          -0.0111 |           8.1820 |           2.9236 |
[32m[20230205 18:18:27 @agent_ppo2.py:191][0m |          -0.0126 |           7.8312 |           2.9220 |
[32m[20230205 18:18:27 @agent_ppo2.py:136][0m Policy update time: 0.68 s
[32m[20230205 18:18:27 @agent_ppo2.py:144][0m Average TRAINING episode reward: 84.64
[32m[20230205 18:18:27 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 230.51
[32m[20230205 18:18:27 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 93.33
[32m[20230205 18:18:27 @agent_ppo2.py:149][0m Total time:       5.00 min
[32m[20230205 18:18:27 @agent_ppo2.py:151][0m 303104 total steps have happened
[32m[20230205 18:18:27 @agent_ppo2.py:127][0m #------------------------ Iteration 148 --------------------------#
[32m[20230205 18:18:28 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:18:28 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:28 @agent_ppo2.py:191][0m |          -0.0006 |          39.7301 |           3.0417 |
[32m[20230205 18:18:28 @agent_ppo2.py:191][0m |          -0.0048 |          30.3663 |           3.0395 |
[32m[20230205 18:18:28 @agent_ppo2.py:191][0m |          -0.0067 |          25.5127 |           3.0398 |
[32m[20230205 18:18:28 @agent_ppo2.py:191][0m |          -0.0080 |          22.2548 |           3.0372 |
[32m[20230205 18:18:28 @agent_ppo2.py:191][0m |          -0.0086 |          20.4723 |           3.0355 |
[32m[20230205 18:18:28 @agent_ppo2.py:191][0m |          -0.0097 |          18.8230 |           3.0350 |
[32m[20230205 18:18:28 @agent_ppo2.py:191][0m |          -0.0103 |          18.0241 |           3.0355 |
[32m[20230205 18:18:29 @agent_ppo2.py:191][0m |          -0.0107 |          17.4227 |           3.0347 |
[32m[20230205 18:18:29 @agent_ppo2.py:191][0m |          -0.0112 |          16.8213 |           3.0336 |
[32m[20230205 18:18:29 @agent_ppo2.py:191][0m |          -0.0118 |          16.2436 |           3.0340 |
[32m[20230205 18:18:29 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:18:29 @agent_ppo2.py:144][0m Average TRAINING episode reward: 165.99
[32m[20230205 18:18:29 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 243.71
[32m[20230205 18:18:29 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 277.32
[32m[20230205 18:18:29 @agent_ppo2.py:149][0m Total time:       5.03 min
[32m[20230205 18:18:29 @agent_ppo2.py:151][0m 305152 total steps have happened
[32m[20230205 18:18:29 @agent_ppo2.py:127][0m #------------------------ Iteration 149 --------------------------#
[32m[20230205 18:18:30 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:18:30 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:30 @agent_ppo2.py:191][0m |          -0.0025 |          26.4378 |           2.9595 |
[32m[20230205 18:18:30 @agent_ppo2.py:191][0m |          -0.0066 |          20.0557 |           2.9511 |
[32m[20230205 18:18:30 @agent_ppo2.py:191][0m |          -0.0091 |          18.2697 |           2.9480 |
[32m[20230205 18:18:30 @agent_ppo2.py:191][0m |          -0.0112 |          17.0421 |           2.9473 |
[32m[20230205 18:18:30 @agent_ppo2.py:191][0m |          -0.0123 |          16.0182 |           2.9468 |
[32m[20230205 18:18:30 @agent_ppo2.py:191][0m |          -0.0133 |          15.3658 |           2.9477 |
[32m[20230205 18:18:30 @agent_ppo2.py:191][0m |          -0.0149 |          14.9182 |           2.9485 |
[32m[20230205 18:18:30 @agent_ppo2.py:191][0m |          -0.0154 |          13.9298 |           2.9460 |
[32m[20230205 18:18:30 @agent_ppo2.py:191][0m |          -0.0155 |          13.6280 |           2.9450 |
[32m[20230205 18:18:30 @agent_ppo2.py:191][0m |          -0.0163 |          13.0658 |           2.9449 |
[32m[20230205 18:18:30 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:18:31 @agent_ppo2.py:144][0m Average TRAINING episode reward: 159.60
[32m[20230205 18:18:31 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 248.11
[32m[20230205 18:18:31 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.30
[32m[20230205 18:18:31 @agent_ppo2.py:149][0m Total time:       5.06 min
[32m[20230205 18:18:31 @agent_ppo2.py:151][0m 307200 total steps have happened
[32m[20230205 18:18:31 @agent_ppo2.py:127][0m #------------------------ Iteration 150 --------------------------#
[32m[20230205 18:18:32 @agent_ppo2.py:133][0m Sampling time: 0.69 s by 1 slaves
[32m[20230205 18:18:32 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:32 @agent_ppo2.py:191][0m |           0.0005 |           8.4132 |           2.8562 |
[32m[20230205 18:18:32 @agent_ppo2.py:191][0m |          -0.0117 |           6.6663 |           2.8503 |
[32m[20230205 18:18:32 @agent_ppo2.py:191][0m |          -0.0100 |           6.4179 |           2.8503 |
[32m[20230205 18:18:32 @agent_ppo2.py:191][0m |          -0.0096 |           6.3670 |           2.8507 |
[32m[20230205 18:18:32 @agent_ppo2.py:191][0m |          -0.0168 |           6.2273 |           2.8483 |
[32m[20230205 18:18:32 @agent_ppo2.py:191][0m |          -0.0114 |           6.1728 |           2.8490 |
[32m[20230205 18:18:32 @agent_ppo2.py:191][0m |          -0.0135 |           6.1151 |           2.8479 |
[32m[20230205 18:18:32 @agent_ppo2.py:191][0m |          -0.0129 |           6.0391 |           2.8472 |
[32m[20230205 18:18:32 @agent_ppo2.py:191][0m |          -0.0167 |           5.9919 |           2.8477 |
[32m[20230205 18:18:32 @agent_ppo2.py:191][0m |          -0.0139 |           5.9934 |           2.8467 |
[32m[20230205 18:18:32 @agent_ppo2.py:136][0m Policy update time: 0.68 s
[32m[20230205 18:18:33 @agent_ppo2.py:144][0m Average TRAINING episode reward: 233.05
[32m[20230205 18:18:33 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 237.25
[32m[20230205 18:18:33 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 215.15
[32m[20230205 18:18:33 @agent_ppo2.py:149][0m Total time:       5.09 min
[32m[20230205 18:18:33 @agent_ppo2.py:151][0m 309248 total steps have happened
[32m[20230205 18:18:33 @agent_ppo2.py:127][0m #------------------------ Iteration 151 --------------------------#
[32m[20230205 18:18:33 @agent_ppo2.py:133][0m Sampling time: 0.64 s by 1 slaves
[32m[20230205 18:18:33 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:33 @agent_ppo2.py:191][0m |          -0.0014 |           7.1983 |           2.8794 |
[32m[20230205 18:18:34 @agent_ppo2.py:191][0m |          -0.0011 |           7.0007 |           2.8742 |
[32m[20230205 18:18:34 @agent_ppo2.py:191][0m |          -0.0056 |           6.3640 |           2.8740 |
[32m[20230205 18:18:34 @agent_ppo2.py:191][0m |          -0.0111 |           6.2299 |           2.8750 |
[32m[20230205 18:18:34 @agent_ppo2.py:191][0m |          -0.0090 |           6.1424 |           2.8723 |
[32m[20230205 18:18:34 @agent_ppo2.py:191][0m |          -0.0090 |           6.1378 |           2.8735 |
[32m[20230205 18:18:34 @agent_ppo2.py:191][0m |          -0.0120 |           6.0235 |           2.8743 |
[32m[20230205 18:18:34 @agent_ppo2.py:191][0m |          -0.0094 |           5.9724 |           2.8698 |
[32m[20230205 18:18:34 @agent_ppo2.py:191][0m |          -0.0106 |           5.9375 |           2.8716 |
[32m[20230205 18:18:34 @agent_ppo2.py:191][0m |          -0.0124 |           5.8552 |           2.8714 |
[32m[20230205 18:18:34 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:18:35 @agent_ppo2.py:144][0m Average TRAINING episode reward: 242.71
[32m[20230205 18:18:35 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 248.16
[32m[20230205 18:18:35 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 170.58
[32m[20230205 18:18:35 @agent_ppo2.py:149][0m Total time:       5.12 min
[32m[20230205 18:18:35 @agent_ppo2.py:151][0m 311296 total steps have happened
[32m[20230205 18:18:35 @agent_ppo2.py:127][0m #------------------------ Iteration 152 --------------------------#
[32m[20230205 18:18:35 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:18:35 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:35 @agent_ppo2.py:191][0m |          -0.0004 |          52.0914 |           2.8696 |
[32m[20230205 18:18:35 @agent_ppo2.py:191][0m |          -0.0092 |          20.6551 |           2.8628 |
[32m[20230205 18:18:35 @agent_ppo2.py:191][0m |          -0.0081 |          14.4560 |           2.8621 |
[32m[20230205 18:18:35 @agent_ppo2.py:191][0m |          -0.0100 |          12.5516 |           2.8613 |
[32m[20230205 18:18:35 @agent_ppo2.py:191][0m |          -0.0110 |          11.1075 |           2.8577 |
[32m[20230205 18:18:35 @agent_ppo2.py:191][0m |          -0.0047 |          10.2560 |           2.8542 |
[32m[20230205 18:18:35 @agent_ppo2.py:191][0m |          -0.0169 |           9.1049 |           2.8518 |
[32m[20230205 18:18:36 @agent_ppo2.py:191][0m |          -0.0063 |           8.6883 |           2.8548 |
[32m[20230205 18:18:36 @agent_ppo2.py:191][0m |          -0.0207 |           8.0607 |           2.8535 |
[32m[20230205 18:18:36 @agent_ppo2.py:191][0m |          -0.0202 |           7.7300 |           2.8556 |
[32m[20230205 18:18:36 @agent_ppo2.py:136][0m Policy update time: 0.49 s
[32m[20230205 18:18:36 @agent_ppo2.py:144][0m Average TRAINING episode reward: -6.35
[32m[20230205 18:18:36 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 41.39
[32m[20230205 18:18:36 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 280.07
[32m[20230205 18:18:36 @agent_ppo2.py:149][0m Total time:       5.15 min
[32m[20230205 18:18:36 @agent_ppo2.py:151][0m 313344 total steps have happened
[32m[20230205 18:18:36 @agent_ppo2.py:127][0m #------------------------ Iteration 153 --------------------------#
[32m[20230205 18:18:37 @agent_ppo2.py:133][0m Sampling time: 0.70 s by 1 slaves
[32m[20230205 18:18:37 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:37 @agent_ppo2.py:191][0m |           0.0020 |          16.3344 |           2.8926 |
[32m[20230205 18:18:37 @agent_ppo2.py:191][0m |          -0.0099 |          11.8513 |           2.8907 |
[32m[20230205 18:18:37 @agent_ppo2.py:191][0m |          -0.0117 |          10.8327 |           2.8887 |
[32m[20230205 18:18:37 @agent_ppo2.py:191][0m |          -0.0160 |          10.2949 |           2.8887 |
[32m[20230205 18:18:37 @agent_ppo2.py:191][0m |          -0.0193 |           9.8156 |           2.8875 |
[32m[20230205 18:18:37 @agent_ppo2.py:191][0m |          -0.0136 |           9.6324 |           2.8887 |
[32m[20230205 18:18:37 @agent_ppo2.py:191][0m |          -0.0193 |           9.2743 |           2.8887 |
[32m[20230205 18:18:37 @agent_ppo2.py:191][0m |          -0.0176 |           9.0678 |           2.8879 |
[32m[20230205 18:18:38 @agent_ppo2.py:191][0m |          -0.0240 |           9.0221 |           2.8884 |
[32m[20230205 18:18:38 @agent_ppo2.py:191][0m |          -0.0108 |           9.0101 |           2.8886 |
[32m[20230205 18:18:38 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:18:38 @agent_ppo2.py:144][0m Average TRAINING episode reward: 138.62
[32m[20230205 18:18:38 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 245.17
[32m[20230205 18:18:38 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 221.77
[32m[20230205 18:18:38 @agent_ppo2.py:149][0m Total time:       5.18 min
[32m[20230205 18:18:38 @agent_ppo2.py:151][0m 315392 total steps have happened
[32m[20230205 18:18:38 @agent_ppo2.py:127][0m #------------------------ Iteration 154 --------------------------#
[32m[20230205 18:18:39 @agent_ppo2.py:133][0m Sampling time: 0.67 s by 1 slaves
[32m[20230205 18:18:39 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:39 @agent_ppo2.py:191][0m |          -0.0002 |           8.6728 |           3.0018 |
[32m[20230205 18:18:39 @agent_ppo2.py:191][0m |          -0.0043 |           6.5999 |           2.9943 |
[32m[20230205 18:18:39 @agent_ppo2.py:191][0m |          -0.0059 |           6.2456 |           2.9953 |
[32m[20230205 18:18:39 @agent_ppo2.py:191][0m |          -0.0074 |           6.0587 |           2.9924 |
[32m[20230205 18:18:39 @agent_ppo2.py:191][0m |          -0.0084 |           5.9436 |           2.9917 |
[32m[20230205 18:18:39 @agent_ppo2.py:191][0m |          -0.0084 |           5.8437 |           2.9921 |
[32m[20230205 18:18:39 @agent_ppo2.py:191][0m |          -0.0096 |           5.7811 |           2.9931 |
[32m[20230205 18:18:39 @agent_ppo2.py:191][0m |          -0.0097 |           5.7266 |           2.9933 |
[32m[20230205 18:18:39 @agent_ppo2.py:191][0m |          -0.0107 |           5.6692 |           2.9920 |
[32m[20230205 18:18:39 @agent_ppo2.py:191][0m |          -0.0097 |           5.6156 |           2.9929 |
[32m[20230205 18:18:39 @agent_ppo2.py:136][0m Policy update time: 0.66 s
[32m[20230205 18:18:40 @agent_ppo2.py:144][0m Average TRAINING episode reward: 238.88
[32m[20230205 18:18:40 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 239.07
[32m[20230205 18:18:40 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 269.25
[32m[20230205 18:18:40 @agent_ppo2.py:149][0m Total time:       5.21 min
[32m[20230205 18:18:40 @agent_ppo2.py:151][0m 317440 total steps have happened
[32m[20230205 18:18:40 @agent_ppo2.py:127][0m #------------------------ Iteration 155 --------------------------#
[32m[20230205 18:18:41 @agent_ppo2.py:133][0m Sampling time: 0.67 s by 1 slaves
[32m[20230205 18:18:41 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:41 @agent_ppo2.py:191][0m |          -0.0025 |           5.6801 |           2.8452 |
[32m[20230205 18:18:41 @agent_ppo2.py:191][0m |          -0.0020 |           5.2656 |           2.8472 |
[32m[20230205 18:18:41 @agent_ppo2.py:191][0m |          -0.0035 |           5.0801 |           2.8467 |
[32m[20230205 18:18:41 @agent_ppo2.py:191][0m |          -0.0186 |           4.9424 |           2.8482 |
[32m[20230205 18:18:41 @agent_ppo2.py:191][0m |          -0.0026 |           4.7766 |           2.8531 |
[32m[20230205 18:18:41 @agent_ppo2.py:191][0m |          -0.0091 |           4.6652 |           2.8506 |
[32m[20230205 18:18:41 @agent_ppo2.py:191][0m |          -0.0130 |           4.5576 |           2.8524 |
[32m[20230205 18:18:41 @agent_ppo2.py:191][0m |          -0.0100 |           4.4704 |           2.8525 |
[32m[20230205 18:18:41 @agent_ppo2.py:191][0m |          -0.0136 |           4.3706 |           2.8545 |
[32m[20230205 18:18:41 @agent_ppo2.py:191][0m |          -0.0231 |           4.2938 |           2.8544 |
[32m[20230205 18:18:41 @agent_ppo2.py:136][0m Policy update time: 0.66 s
[32m[20230205 18:18:42 @agent_ppo2.py:144][0m Average TRAINING episode reward: 236.59
[32m[20230205 18:18:42 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 242.74
[32m[20230205 18:18:42 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 278.89
[32m[20230205 18:18:42 @agent_ppo2.py:149][0m Total time:       5.24 min
[32m[20230205 18:18:42 @agent_ppo2.py:151][0m 319488 total steps have happened
[32m[20230205 18:18:42 @agent_ppo2.py:127][0m #------------------------ Iteration 156 --------------------------#
[32m[20230205 18:18:43 @agent_ppo2.py:133][0m Sampling time: 0.69 s by 1 slaves
[32m[20230205 18:18:43 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:43 @agent_ppo2.py:191][0m |           0.0006 |           5.9707 |           2.9309 |
[32m[20230205 18:18:43 @agent_ppo2.py:191][0m |           0.0107 |           6.1486 |           2.9285 |
[32m[20230205 18:18:43 @agent_ppo2.py:191][0m |          -0.0106 |           5.6003 |           2.9257 |
[32m[20230205 18:18:43 @agent_ppo2.py:191][0m |          -0.0046 |           5.5070 |           2.9252 |
[32m[20230205 18:18:43 @agent_ppo2.py:191][0m |          -0.0057 |           5.4492 |           2.9265 |
[32m[20230205 18:18:43 @agent_ppo2.py:191][0m |          -0.0106 |           5.4148 |           2.9235 |
[32m[20230205 18:18:43 @agent_ppo2.py:191][0m |          -0.0020 |           5.5156 |           2.9229 |
[32m[20230205 18:18:43 @agent_ppo2.py:191][0m |          -0.0043 |           5.4401 |           2.9226 |
[32m[20230205 18:18:43 @agent_ppo2.py:191][0m |          -0.0141 |           5.3085 |           2.9218 |
[32m[20230205 18:18:43 @agent_ppo2.py:191][0m |          -0.0149 |           5.2512 |           2.9215 |
[32m[20230205 18:18:43 @agent_ppo2.py:136][0m Policy update time: 0.66 s
[32m[20230205 18:18:44 @agent_ppo2.py:144][0m Average TRAINING episode reward: 236.58
[32m[20230205 18:18:44 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 237.23
[32m[20230205 18:18:44 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.12
[32m[20230205 18:18:44 @agent_ppo2.py:149][0m Total time:       5.27 min
[32m[20230205 18:18:44 @agent_ppo2.py:151][0m 321536 total steps have happened
[32m[20230205 18:18:44 @agent_ppo2.py:127][0m #------------------------ Iteration 157 --------------------------#
[32m[20230205 18:18:44 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:18:44 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:45 @agent_ppo2.py:191][0m |          -0.0031 |          28.3744 |           2.9792 |
[32m[20230205 18:18:45 @agent_ppo2.py:191][0m |          -0.0065 |          19.2749 |           2.9751 |
[32m[20230205 18:18:45 @agent_ppo2.py:191][0m |          -0.0059 |          16.8809 |           2.9730 |
[32m[20230205 18:18:45 @agent_ppo2.py:191][0m |          -0.0087 |          14.9790 |           2.9729 |
[32m[20230205 18:18:45 @agent_ppo2.py:191][0m |          -0.0110 |          13.7651 |           2.9690 |
[32m[20230205 18:18:45 @agent_ppo2.py:191][0m |          -0.0017 |          13.0848 |           2.9685 |
[32m[20230205 18:18:45 @agent_ppo2.py:191][0m |          -0.0098 |          12.1778 |           2.9658 |
[32m[20230205 18:18:45 @agent_ppo2.py:191][0m |          -0.0146 |          11.4544 |           2.9664 |
[32m[20230205 18:18:45 @agent_ppo2.py:191][0m |          -0.0117 |          11.2053 |           2.9671 |
[32m[20230205 18:18:45 @agent_ppo2.py:191][0m |          -0.0100 |          11.1396 |           2.9669 |
[32m[20230205 18:18:45 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:18:46 @agent_ppo2.py:144][0m Average TRAINING episode reward: 170.37
[32m[20230205 18:18:46 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 245.06
[32m[20230205 18:18:46 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 274.83
[32m[20230205 18:18:46 @agent_ppo2.py:149][0m Total time:       5.30 min
[32m[20230205 18:18:46 @agent_ppo2.py:151][0m 323584 total steps have happened
[32m[20230205 18:18:46 @agent_ppo2.py:127][0m #------------------------ Iteration 158 --------------------------#
[32m[20230205 18:18:46 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:18:46 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:46 @agent_ppo2.py:191][0m |           0.0003 |           7.1220 |           3.0313 |
[32m[20230205 18:18:46 @agent_ppo2.py:191][0m |          -0.0046 |           6.2240 |           3.0302 |
[32m[20230205 18:18:46 @agent_ppo2.py:191][0m |          -0.0057 |           5.9038 |           3.0254 |
[32m[20230205 18:18:46 @agent_ppo2.py:191][0m |          -0.0082 |           5.6570 |           3.0243 |
[32m[20230205 18:18:47 @agent_ppo2.py:191][0m |          -0.0094 |           5.4565 |           3.0243 |
[32m[20230205 18:18:47 @agent_ppo2.py:191][0m |          -0.0103 |           5.3127 |           3.0218 |
[32m[20230205 18:18:47 @agent_ppo2.py:191][0m |          -0.0099 |           5.2401 |           3.0208 |
[32m[20230205 18:18:47 @agent_ppo2.py:191][0m |          -0.0123 |           5.0799 |           3.0204 |
[32m[20230205 18:18:47 @agent_ppo2.py:191][0m |          -0.0128 |           4.9939 |           3.0192 |
[32m[20230205 18:18:47 @agent_ppo2.py:191][0m |          -0.0131 |           4.9261 |           3.0185 |
[32m[20230205 18:18:47 @agent_ppo2.py:136][0m Policy update time: 0.63 s
[32m[20230205 18:18:47 @agent_ppo2.py:144][0m Average TRAINING episode reward: 243.69
[32m[20230205 18:18:47 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 245.11
[32m[20230205 18:18:47 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 182.42
[32m[20230205 18:18:47 @agent_ppo2.py:149][0m Total time:       5.34 min
[32m[20230205 18:18:47 @agent_ppo2.py:151][0m 325632 total steps have happened
[32m[20230205 18:18:47 @agent_ppo2.py:127][0m #------------------------ Iteration 159 --------------------------#
[32m[20230205 18:18:48 @agent_ppo2.py:133][0m Sampling time: 0.68 s by 1 slaves
[32m[20230205 18:18:48 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:48 @agent_ppo2.py:191][0m |           0.0032 |           6.7359 |           2.9819 |
[32m[20230205 18:18:48 @agent_ppo2.py:191][0m |          -0.0023 |           6.1785 |           2.9766 |
[32m[20230205 18:18:48 @agent_ppo2.py:191][0m |          -0.0040 |           6.2505 |           2.9733 |
[32m[20230205 18:18:48 @agent_ppo2.py:191][0m |          -0.0071 |           5.9197 |           2.9713 |
[32m[20230205 18:18:49 @agent_ppo2.py:191][0m |          -0.0075 |           5.8397 |           2.9689 |
[32m[20230205 18:18:49 @agent_ppo2.py:191][0m |          -0.0077 |           5.7871 |           2.9676 |
[32m[20230205 18:18:49 @agent_ppo2.py:191][0m |          -0.0078 |           5.7168 |           2.9674 |
[32m[20230205 18:18:49 @agent_ppo2.py:191][0m |          -0.0045 |           5.8594 |           2.9656 |
[32m[20230205 18:18:49 @agent_ppo2.py:191][0m |          -0.0073 |           5.6122 |           2.9641 |
[32m[20230205 18:18:49 @agent_ppo2.py:191][0m |          -0.0058 |           5.6004 |           2.9644 |
[32m[20230205 18:18:49 @agent_ppo2.py:136][0m Policy update time: 0.66 s
[32m[20230205 18:18:49 @agent_ppo2.py:144][0m Average TRAINING episode reward: 235.20
[32m[20230205 18:18:49 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 238.32
[32m[20230205 18:18:49 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 278.02
[32m[20230205 18:18:49 @agent_ppo2.py:149][0m Total time:       5.37 min
[32m[20230205 18:18:49 @agent_ppo2.py:151][0m 327680 total steps have happened
[32m[20230205 18:18:49 @agent_ppo2.py:127][0m #------------------------ Iteration 160 --------------------------#
[32m[20230205 18:18:50 @agent_ppo2.py:133][0m Sampling time: 0.65 s by 1 slaves
[32m[20230205 18:18:50 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:50 @agent_ppo2.py:191][0m |          -0.0010 |           6.4555 |           2.9875 |
[32m[20230205 18:18:50 @agent_ppo2.py:191][0m |          -0.0047 |           6.4400 |           2.9812 |
[32m[20230205 18:18:50 @agent_ppo2.py:191][0m |          -0.0078 |           6.1872 |           2.9797 |
[32m[20230205 18:18:50 @agent_ppo2.py:191][0m |          -0.0100 |           6.1154 |           2.9794 |
[32m[20230205 18:18:50 @agent_ppo2.py:191][0m |          -0.0091 |           6.1072 |           2.9783 |
[32m[20230205 18:18:50 @agent_ppo2.py:191][0m |          -0.0122 |           5.9728 |           2.9775 |
[32m[20230205 18:18:50 @agent_ppo2.py:191][0m |          -0.0140 |           5.9358 |           2.9761 |
[32m[20230205 18:18:51 @agent_ppo2.py:191][0m |          -0.0138 |           5.8943 |           2.9765 |
[32m[20230205 18:18:51 @agent_ppo2.py:191][0m |          -0.0128 |           5.9646 |           2.9777 |
[32m[20230205 18:18:51 @agent_ppo2.py:191][0m |          -0.0137 |           5.8033 |           2.9753 |
[32m[20230205 18:18:51 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:18:51 @agent_ppo2.py:144][0m Average TRAINING episode reward: 240.50
[32m[20230205 18:18:51 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 240.81
[32m[20230205 18:18:51 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 214.58
[32m[20230205 18:18:51 @agent_ppo2.py:149][0m Total time:       5.40 min
[32m[20230205 18:18:51 @agent_ppo2.py:151][0m 329728 total steps have happened
[32m[20230205 18:18:51 @agent_ppo2.py:127][0m #------------------------ Iteration 161 --------------------------#
[32m[20230205 18:18:52 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:18:52 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:52 @agent_ppo2.py:191][0m |           0.0058 |          29.5454 |           2.8920 |
[32m[20230205 18:18:52 @agent_ppo2.py:191][0m |          -0.0070 |          16.6518 |           2.8938 |
[32m[20230205 18:18:52 @agent_ppo2.py:191][0m |          -0.0085 |          14.6619 |           2.8935 |
[32m[20230205 18:18:52 @agent_ppo2.py:191][0m |          -0.0076 |          14.1291 |           2.8921 |
[32m[20230205 18:18:52 @agent_ppo2.py:191][0m |          -0.0098 |          12.9697 |           2.8929 |
[32m[20230205 18:18:52 @agent_ppo2.py:191][0m |          -0.0137 |          12.0581 |           2.8932 |
[32m[20230205 18:18:52 @agent_ppo2.py:191][0m |          -0.0140 |          11.7365 |           2.8917 |
[32m[20230205 18:18:52 @agent_ppo2.py:191][0m |          -0.0148 |          11.1467 |           2.8918 |
[32m[20230205 18:18:52 @agent_ppo2.py:191][0m |          -0.0027 |          10.8127 |           2.8905 |
[32m[20230205 18:18:52 @agent_ppo2.py:191][0m |          -0.0157 |          10.3803 |           2.8885 |
[32m[20230205 18:18:52 @agent_ppo2.py:136][0m Policy update time: 0.62 s
[32m[20230205 18:18:53 @agent_ppo2.py:144][0m Average TRAINING episode reward: 94.58
[32m[20230205 18:18:53 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 248.19
[32m[20230205 18:18:53 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.03
[32m[20230205 18:18:53 @agent_ppo2.py:149][0m Total time:       5.43 min
[32m[20230205 18:18:53 @agent_ppo2.py:151][0m 331776 total steps have happened
[32m[20230205 18:18:53 @agent_ppo2.py:127][0m #------------------------ Iteration 162 --------------------------#
[32m[20230205 18:18:54 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:18:54 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:54 @agent_ppo2.py:191][0m |          -0.0017 |          10.2614 |           2.9437 |
[32m[20230205 18:18:54 @agent_ppo2.py:191][0m |          -0.0084 |           8.0497 |           2.9408 |
[32m[20230205 18:18:54 @agent_ppo2.py:191][0m |          -0.0090 |           7.6211 |           2.9401 |
[32m[20230205 18:18:54 @agent_ppo2.py:191][0m |           0.0004 |           8.6289 |           2.9420 |
[32m[20230205 18:18:54 @agent_ppo2.py:191][0m |          -0.0093 |           7.3106 |           2.9371 |
[32m[20230205 18:18:54 @agent_ppo2.py:191][0m |          -0.0194 |           7.2584 |           2.9406 |
[32m[20230205 18:18:54 @agent_ppo2.py:191][0m |          -0.0124 |           7.1690 |           2.9458 |
[32m[20230205 18:18:54 @agent_ppo2.py:191][0m |          -0.0075 |           7.1340 |           2.9443 |
[32m[20230205 18:18:54 @agent_ppo2.py:191][0m |          -0.0272 |           7.0181 |           2.9438 |
[32m[20230205 18:18:54 @agent_ppo2.py:191][0m |          -0.0114 |           6.9813 |           2.9500 |
[32m[20230205 18:18:54 @agent_ppo2.py:136][0m Policy update time: 0.62 s
[32m[20230205 18:18:55 @agent_ppo2.py:144][0m Average TRAINING episode reward: 245.13
[32m[20230205 18:18:55 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 246.27
[32m[20230205 18:18:55 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.72
[32m[20230205 18:18:55 @agent_ppo2.py:149][0m Total time:       5.46 min
[32m[20230205 18:18:55 @agent_ppo2.py:151][0m 333824 total steps have happened
[32m[20230205 18:18:55 @agent_ppo2.py:127][0m #------------------------ Iteration 163 --------------------------#
[32m[20230205 18:18:55 @agent_ppo2.py:133][0m Sampling time: 0.66 s by 1 slaves
[32m[20230205 18:18:56 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:56 @agent_ppo2.py:191][0m |           0.0027 |           7.6443 |           2.9916 |
[32m[20230205 18:18:56 @agent_ppo2.py:191][0m |          -0.0024 |           7.0812 |           2.9925 |
[32m[20230205 18:18:56 @agent_ppo2.py:191][0m |          -0.0055 |           6.7429 |           2.9930 |
[32m[20230205 18:18:56 @agent_ppo2.py:191][0m |          -0.0084 |           6.6271 |           2.9943 |
[32m[20230205 18:18:56 @agent_ppo2.py:191][0m |          -0.0046 |           6.8995 |           2.9960 |
[32m[20230205 18:18:56 @agent_ppo2.py:191][0m |          -0.0005 |           7.0115 |           2.9919 |
[32m[20230205 18:18:56 @agent_ppo2.py:191][0m |          -0.0070 |           6.7777 |           2.9935 |
[32m[20230205 18:18:56 @agent_ppo2.py:191][0m |          -0.0083 |           6.6883 |           2.9937 |
[32m[20230205 18:18:56 @agent_ppo2.py:191][0m |          -0.0153 |           6.2985 |           2.9921 |
[32m[20230205 18:18:56 @agent_ppo2.py:191][0m |          -0.0077 |           6.6298 |           2.9938 |
[32m[20230205 18:18:56 @agent_ppo2.py:136][0m Policy update time: 0.65 s
[32m[20230205 18:18:57 @agent_ppo2.py:144][0m Average TRAINING episode reward: 238.45
[32m[20230205 18:18:57 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 239.01
[32m[20230205 18:18:57 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 281.60
[32m[20230205 18:18:57 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 281.60
[32m[20230205 18:18:57 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 281.60
[32m[20230205 18:18:57 @agent_ppo2.py:149][0m Total time:       5.49 min
[32m[20230205 18:18:57 @agent_ppo2.py:151][0m 335872 total steps have happened
[32m[20230205 18:18:57 @agent_ppo2.py:127][0m #------------------------ Iteration 164 --------------------------#
[32m[20230205 18:18:57 @agent_ppo2.py:133][0m Sampling time: 0.64 s by 1 slaves
[32m[20230205 18:18:57 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:57 @agent_ppo2.py:191][0m |          -0.0035 |           7.2653 |           2.9816 |
[32m[20230205 18:18:57 @agent_ppo2.py:191][0m |           0.0031 |           7.0807 |           2.9786 |
[32m[20230205 18:18:58 @agent_ppo2.py:191][0m |          -0.0038 |           6.9048 |           2.9772 |
[32m[20230205 18:18:58 @agent_ppo2.py:191][0m |          -0.0029 |           6.7925 |           2.9756 |
[32m[20230205 18:18:58 @agent_ppo2.py:191][0m |          -0.0283 |           6.8034 |           2.9744 |
[32m[20230205 18:18:58 @agent_ppo2.py:191][0m |          -0.0174 |           6.6717 |           2.9736 |
[32m[20230205 18:18:58 @agent_ppo2.py:191][0m |          -0.0136 |           6.6777 |           2.9718 |
[32m[20230205 18:18:58 @agent_ppo2.py:191][0m |          -0.0134 |           6.6024 |           2.9691 |
[32m[20230205 18:18:58 @agent_ppo2.py:191][0m |          -0.0143 |           6.5523 |           2.9696 |
[32m[20230205 18:18:58 @agent_ppo2.py:191][0m |           0.0097 |           7.8294 |           2.9647 |
[32m[20230205 18:18:58 @agent_ppo2.py:136][0m Policy update time: 0.63 s
[32m[20230205 18:18:58 @agent_ppo2.py:144][0m Average TRAINING episode reward: 241.51
[32m[20230205 18:18:58 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 242.17
[32m[20230205 18:18:58 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 194.15
[32m[20230205 18:18:58 @agent_ppo2.py:149][0m Total time:       5.52 min
[32m[20230205 18:18:58 @agent_ppo2.py:151][0m 337920 total steps have happened
[32m[20230205 18:18:58 @agent_ppo2.py:127][0m #------------------------ Iteration 165 --------------------------#
[32m[20230205 18:18:59 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:18:59 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:18:59 @agent_ppo2.py:191][0m |          -0.0022 |          38.1070 |           3.0288 |
[32m[20230205 18:18:59 @agent_ppo2.py:191][0m |          -0.0058 |          23.4688 |           3.0236 |
[32m[20230205 18:18:59 @agent_ppo2.py:191][0m |          -0.0093 |          19.5922 |           3.0224 |
[32m[20230205 18:18:59 @agent_ppo2.py:191][0m |          -0.0102 |          17.7168 |           3.0259 |
[32m[20230205 18:18:59 @agent_ppo2.py:191][0m |          -0.0134 |          15.3554 |           3.0232 |
[32m[20230205 18:18:59 @agent_ppo2.py:191][0m |          -0.0144 |          15.3973 |           3.0224 |
[32m[20230205 18:18:59 @agent_ppo2.py:191][0m |          -0.0144 |          13.3666 |           3.0223 |
[32m[20230205 18:18:59 @agent_ppo2.py:191][0m |          -0.0166 |          12.3602 |           3.0226 |
[32m[20230205 18:19:00 @agent_ppo2.py:191][0m |          -0.0173 |          11.6127 |           3.0231 |
[32m[20230205 18:19:00 @agent_ppo2.py:191][0m |          -0.0165 |          11.1788 |           3.0244 |
[32m[20230205 18:19:00 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:19:00 @agent_ppo2.py:144][0m Average TRAINING episode reward: 60.98
[32m[20230205 18:19:00 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 245.06
[32m[20230205 18:19:00 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 199.29
[32m[20230205 18:19:00 @agent_ppo2.py:149][0m Total time:       5.54 min
[32m[20230205 18:19:00 @agent_ppo2.py:151][0m 339968 total steps have happened
[32m[20230205 18:19:00 @agent_ppo2.py:127][0m #------------------------ Iteration 166 --------------------------#
[32m[20230205 18:19:01 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:19:01 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:01 @agent_ppo2.py:191][0m |           0.0037 |          38.1410 |           2.9894 |
[32m[20230205 18:19:01 @agent_ppo2.py:191][0m |          -0.0028 |          23.4155 |           2.9832 |
[32m[20230205 18:19:01 @agent_ppo2.py:191][0m |          -0.0064 |          16.8114 |           2.9842 |
[32m[20230205 18:19:01 @agent_ppo2.py:191][0m |          -0.0065 |          13.1523 |           2.9843 |
[32m[20230205 18:19:01 @agent_ppo2.py:191][0m |          -0.0082 |          11.6582 |           2.9780 |
[32m[20230205 18:19:01 @agent_ppo2.py:191][0m |          -0.0117 |          10.2887 |           2.9822 |
[32m[20230205 18:19:01 @agent_ppo2.py:191][0m |          -0.0101 |           9.7032 |           2.9780 |
[32m[20230205 18:19:01 @agent_ppo2.py:191][0m |          -0.0111 |           8.9658 |           2.9760 |
[32m[20230205 18:19:01 @agent_ppo2.py:191][0m |          -0.0120 |           8.4211 |           2.9733 |
[32m[20230205 18:19:01 @agent_ppo2.py:191][0m |          -0.0137 |           7.8623 |           2.9730 |
[32m[20230205 18:19:01 @agent_ppo2.py:136][0m Policy update time: 0.60 s
[32m[20230205 18:19:02 @agent_ppo2.py:144][0m Average TRAINING episode reward: 188.09
[32m[20230205 18:19:02 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 247.81
[32m[20230205 18:19:02 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 217.01
[32m[20230205 18:19:02 @agent_ppo2.py:149][0m Total time:       5.57 min
[32m[20230205 18:19:02 @agent_ppo2.py:151][0m 342016 total steps have happened
[32m[20230205 18:19:02 @agent_ppo2.py:127][0m #------------------------ Iteration 167 --------------------------#
[32m[20230205 18:19:02 @agent_ppo2.py:133][0m Sampling time: 0.65 s by 1 slaves
[32m[20230205 18:19:02 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:02 @agent_ppo2.py:191][0m |           0.0007 |           7.3182 |           2.9835 |
[32m[20230205 18:19:03 @agent_ppo2.py:191][0m |          -0.0042 |           6.9450 |           2.9748 |
[32m[20230205 18:19:03 @agent_ppo2.py:191][0m |          -0.0050 |           6.8058 |           2.9708 |
[32m[20230205 18:19:03 @agent_ppo2.py:191][0m |          -0.0066 |           6.6963 |           2.9706 |
[32m[20230205 18:19:03 @agent_ppo2.py:191][0m |          -0.0073 |           6.6738 |           2.9703 |
[32m[20230205 18:19:03 @agent_ppo2.py:191][0m |          -0.0087 |           6.5398 |           2.9698 |
[32m[20230205 18:19:03 @agent_ppo2.py:191][0m |          -0.0086 |           6.5451 |           2.9717 |
[32m[20230205 18:19:03 @agent_ppo2.py:191][0m |          -0.0109 |           6.4565 |           2.9696 |
[32m[20230205 18:19:03 @agent_ppo2.py:191][0m |          -0.0110 |           6.4043 |           2.9692 |
[32m[20230205 18:19:03 @agent_ppo2.py:191][0m |          -0.0102 |           6.4433 |           2.9704 |
[32m[20230205 18:19:03 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:19:04 @agent_ppo2.py:144][0m Average TRAINING episode reward: 240.15
[32m[20230205 18:19:04 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 242.26
[32m[20230205 18:19:04 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 183.23
[32m[20230205 18:19:04 @agent_ppo2.py:149][0m Total time:       5.60 min
[32m[20230205 18:19:04 @agent_ppo2.py:151][0m 344064 total steps have happened
[32m[20230205 18:19:04 @agent_ppo2.py:127][0m #------------------------ Iteration 168 --------------------------#
[32m[20230205 18:19:04 @agent_ppo2.py:133][0m Sampling time: 0.71 s by 1 slaves
[32m[20230205 18:19:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:04 @agent_ppo2.py:191][0m |          -0.0033 |          36.0802 |           2.9565 |
[32m[20230205 18:19:05 @agent_ppo2.py:191][0m |          -0.0064 |          28.4780 |           2.9519 |
[32m[20230205 18:19:05 @agent_ppo2.py:191][0m |           0.0210 |          28.7460 |           2.9509 |
[32m[20230205 18:19:05 @agent_ppo2.py:191][0m |          -0.0113 |          26.3820 |           2.9477 |
[32m[20230205 18:19:05 @agent_ppo2.py:191][0m |          -0.0122 |          22.3982 |           2.9499 |
[32m[20230205 18:19:05 @agent_ppo2.py:191][0m |          -0.0137 |          21.5892 |           2.9499 |
[32m[20230205 18:19:05 @agent_ppo2.py:191][0m |          -0.0139 |          19.8670 |           2.9471 |
[32m[20230205 18:19:05 @agent_ppo2.py:191][0m |          -0.0158 |          19.1309 |           2.9504 |
[32m[20230205 18:19:05 @agent_ppo2.py:191][0m |          -0.0138 |          18.1433 |           2.9496 |
[32m[20230205 18:19:05 @agent_ppo2.py:191][0m |          -0.0157 |          17.5705 |           2.9485 |
[32m[20230205 18:19:05 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:19:06 @agent_ppo2.py:144][0m Average TRAINING episode reward: 115.22
[32m[20230205 18:19:06 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 250.12
[32m[20230205 18:19:06 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 187.58
[32m[20230205 18:19:06 @agent_ppo2.py:149][0m Total time:       5.64 min
[32m[20230205 18:19:06 @agent_ppo2.py:151][0m 346112 total steps have happened
[32m[20230205 18:19:06 @agent_ppo2.py:127][0m #------------------------ Iteration 169 --------------------------#
[32m[20230205 18:19:06 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:19:06 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:06 @agent_ppo2.py:191][0m |          -0.0010 |          10.3024 |           3.0070 |
[32m[20230205 18:19:06 @agent_ppo2.py:191][0m |          -0.0045 |           8.1300 |           3.0058 |
[32m[20230205 18:19:07 @agent_ppo2.py:191][0m |          -0.0065 |           7.8713 |           3.0019 |
[32m[20230205 18:19:07 @agent_ppo2.py:191][0m |          -0.0051 |           7.8655 |           2.9996 |
[32m[20230205 18:19:07 @agent_ppo2.py:191][0m |          -0.0106 |           7.6501 |           3.0016 |
[32m[20230205 18:19:07 @agent_ppo2.py:191][0m |          -0.0112 |           7.5800 |           3.0015 |
[32m[20230205 18:19:07 @agent_ppo2.py:191][0m |          -0.0097 |           7.6249 |           3.0003 |
[32m[20230205 18:19:07 @agent_ppo2.py:191][0m |          -0.0127 |           7.4924 |           3.0026 |
[32m[20230205 18:19:07 @agent_ppo2.py:191][0m |          -0.0128 |           7.4809 |           3.0024 |
[32m[20230205 18:19:07 @agent_ppo2.py:191][0m |          -0.0136 |           7.4018 |           3.0019 |
[32m[20230205 18:19:07 @agent_ppo2.py:136][0m Policy update time: 0.62 s
[32m[20230205 18:19:08 @agent_ppo2.py:144][0m Average TRAINING episode reward: 246.72
[32m[20230205 18:19:08 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 248.91
[32m[20230205 18:19:08 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 196.30
[32m[20230205 18:19:08 @agent_ppo2.py:149][0m Total time:       5.67 min
[32m[20230205 18:19:08 @agent_ppo2.py:151][0m 348160 total steps have happened
[32m[20230205 18:19:08 @agent_ppo2.py:127][0m #------------------------ Iteration 170 --------------------------#
[32m[20230205 18:19:08 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:19:08 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:08 @agent_ppo2.py:191][0m |          -0.0001 |          11.5842 |           3.0375 |
[32m[20230205 18:19:08 @agent_ppo2.py:191][0m |          -0.0062 |           7.9101 |           3.0316 |
[32m[20230205 18:19:08 @agent_ppo2.py:191][0m |          -0.0072 |           7.4722 |           3.0257 |
[32m[20230205 18:19:08 @agent_ppo2.py:191][0m |          -0.0077 |           7.4318 |           3.0240 |
[32m[20230205 18:19:09 @agent_ppo2.py:191][0m |          -0.0099 |           7.2380 |           3.0248 |
[32m[20230205 18:19:09 @agent_ppo2.py:191][0m |          -0.0111 |           7.1174 |           3.0257 |
[32m[20230205 18:19:09 @agent_ppo2.py:191][0m |          -0.0126 |           7.0830 |           3.0269 |
[32m[20230205 18:19:09 @agent_ppo2.py:191][0m |          -0.0113 |           6.9809 |           3.0262 |
[32m[20230205 18:19:09 @agent_ppo2.py:191][0m |          -0.0135 |           6.9601 |           3.0271 |
[32m[20230205 18:19:09 @agent_ppo2.py:191][0m |          -0.0129 |           6.8347 |           3.0273 |
[32m[20230205 18:19:09 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:19:09 @agent_ppo2.py:144][0m Average TRAINING episode reward: 247.34
[32m[20230205 18:19:09 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 248.63
[32m[20230205 18:19:09 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 213.88
[32m[20230205 18:19:09 @agent_ppo2.py:149][0m Total time:       5.70 min
[32m[20230205 18:19:09 @agent_ppo2.py:151][0m 350208 total steps have happened
[32m[20230205 18:19:09 @agent_ppo2.py:127][0m #------------------------ Iteration 171 --------------------------#
[32m[20230205 18:19:10 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:19:10 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:10 @agent_ppo2.py:191][0m |          -0.0008 |          27.6593 |           3.0553 |
[32m[20230205 18:19:10 @agent_ppo2.py:191][0m |          -0.0054 |          14.6167 |           3.0551 |
[32m[20230205 18:19:10 @agent_ppo2.py:191][0m |          -0.0079 |          12.0315 |           3.0537 |
[32m[20230205 18:19:10 @agent_ppo2.py:191][0m |          -0.0099 |          10.7282 |           3.0557 |
[32m[20230205 18:19:10 @agent_ppo2.py:191][0m |          -0.0103 |          10.0734 |           3.0551 |
[32m[20230205 18:19:10 @agent_ppo2.py:191][0m |          -0.0108 |           9.5092 |           3.0537 |
[32m[20230205 18:19:11 @agent_ppo2.py:191][0m |          -0.0093 |           9.0846 |           3.0573 |
[32m[20230205 18:19:11 @agent_ppo2.py:191][0m |          -0.0128 |           8.6728 |           3.0540 |
[32m[20230205 18:19:11 @agent_ppo2.py:191][0m |          -0.0119 |           8.4515 |           3.0564 |
[32m[20230205 18:19:11 @agent_ppo2.py:191][0m |          -0.0114 |           8.1836 |           3.0549 |
[32m[20230205 18:19:11 @agent_ppo2.py:136][0m Policy update time: 0.72 s
[32m[20230205 18:19:11 @agent_ppo2.py:144][0m Average TRAINING episode reward: 155.84
[32m[20230205 18:19:11 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 248.85
[32m[20230205 18:19:11 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 285.50
[32m[20230205 18:19:11 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 285.50
[32m[20230205 18:19:11 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 285.50
[32m[20230205 18:19:11 @agent_ppo2.py:149][0m Total time:       5.73 min
[32m[20230205 18:19:11 @agent_ppo2.py:151][0m 352256 total steps have happened
[32m[20230205 18:19:11 @agent_ppo2.py:127][0m #------------------------ Iteration 172 --------------------------#
[32m[20230205 18:19:12 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:19:12 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:12 @agent_ppo2.py:191][0m |          -0.0031 |          28.3992 |           2.9917 |
[32m[20230205 18:19:12 @agent_ppo2.py:191][0m |          -0.0056 |          21.9433 |           2.9820 |
[32m[20230205 18:19:12 @agent_ppo2.py:191][0m |          -0.0106 |          19.1582 |           2.9818 |
[32m[20230205 18:19:12 @agent_ppo2.py:191][0m |          -0.0116 |          16.5448 |           2.9792 |
[32m[20230205 18:19:12 @agent_ppo2.py:191][0m |          -0.0132 |          15.0775 |           2.9807 |
[32m[20230205 18:19:12 @agent_ppo2.py:191][0m |          -0.0141 |          13.5841 |           2.9817 |
[32m[20230205 18:19:12 @agent_ppo2.py:191][0m |          -0.0129 |          12.6221 |           2.9806 |
[32m[20230205 18:19:12 @agent_ppo2.py:191][0m |          -0.0136 |          11.8812 |           2.9805 |
[32m[20230205 18:19:12 @agent_ppo2.py:191][0m |          -0.0164 |          10.9655 |           2.9816 |
[32m[20230205 18:19:12 @agent_ppo2.py:191][0m |          -0.0136 |          10.7607 |           2.9814 |
[32m[20230205 18:19:12 @agent_ppo2.py:136][0m Policy update time: 0.49 s
[32m[20230205 18:19:13 @agent_ppo2.py:144][0m Average TRAINING episode reward: 141.93
[32m[20230205 18:19:13 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 245.76
[32m[20230205 18:19:13 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 106.77
[32m[20230205 18:19:13 @agent_ppo2.py:149][0m Total time:       5.76 min
[32m[20230205 18:19:13 @agent_ppo2.py:151][0m 354304 total steps have happened
[32m[20230205 18:19:13 @agent_ppo2.py:127][0m #------------------------ Iteration 173 --------------------------#
[32m[20230205 18:19:13 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:19:13 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:13 @agent_ppo2.py:191][0m |           0.0004 |          28.7265 |           3.1467 |
[32m[20230205 18:19:13 @agent_ppo2.py:191][0m |          -0.0041 |          23.1088 |           3.1370 |
[32m[20230205 18:19:13 @agent_ppo2.py:191][0m |          -0.0071 |          20.0688 |           3.1301 |
[32m[20230205 18:19:13 @agent_ppo2.py:191][0m |          -0.0081 |          18.9847 |           3.1273 |
[32m[20230205 18:19:13 @agent_ppo2.py:191][0m |          -0.0094 |          17.6586 |           3.1282 |
[32m[20230205 18:19:14 @agent_ppo2.py:191][0m |          -0.0093 |          16.9210 |           3.1285 |
[32m[20230205 18:19:14 @agent_ppo2.py:191][0m |          -0.0081 |          16.1618 |           3.1294 |
[32m[20230205 18:19:14 @agent_ppo2.py:191][0m |          -0.0097 |          15.5896 |           3.1268 |
[32m[20230205 18:19:14 @agent_ppo2.py:191][0m |          -0.0101 |          15.0788 |           3.1259 |
[32m[20230205 18:19:14 @agent_ppo2.py:191][0m |          -0.0111 |          14.6549 |           3.1280 |
[32m[20230205 18:19:14 @agent_ppo2.py:136][0m Policy update time: 0.49 s
[32m[20230205 18:19:14 @agent_ppo2.py:144][0m Average TRAINING episode reward: 161.55
[32m[20230205 18:19:14 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.24
[32m[20230205 18:19:14 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 223.78
[32m[20230205 18:19:14 @agent_ppo2.py:149][0m Total time:       5.78 min
[32m[20230205 18:19:14 @agent_ppo2.py:151][0m 356352 total steps have happened
[32m[20230205 18:19:14 @agent_ppo2.py:127][0m #------------------------ Iteration 174 --------------------------#
[32m[20230205 18:19:15 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:19:15 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:15 @agent_ppo2.py:191][0m |          -0.0023 |          16.2270 |           3.0664 |
[32m[20230205 18:19:15 @agent_ppo2.py:191][0m |          -0.0022 |          11.0857 |           3.0625 |
[32m[20230205 18:19:15 @agent_ppo2.py:191][0m |          -0.0117 |          11.0792 |           3.0605 |
[32m[20230205 18:19:15 @agent_ppo2.py:191][0m |          -0.0053 |          10.2173 |           3.0619 |
[32m[20230205 18:19:15 @agent_ppo2.py:191][0m |          -0.0081 |           9.9074 |           3.0614 |
[32m[20230205 18:19:15 @agent_ppo2.py:191][0m |          -0.0096 |           9.7421 |           3.0583 |
[32m[20230205 18:19:15 @agent_ppo2.py:191][0m |          -0.0064 |           9.7261 |           3.0614 |
[32m[20230205 18:19:15 @agent_ppo2.py:191][0m |          -0.0042 |          10.3549 |           3.0609 |
[32m[20230205 18:19:15 @agent_ppo2.py:191][0m |          -0.0108 |           9.3212 |           3.0565 |
[32m[20230205 18:19:15 @agent_ppo2.py:191][0m |          -0.0082 |           9.3242 |           3.0579 |
[32m[20230205 18:19:15 @agent_ppo2.py:136][0m Policy update time: 0.62 s
[32m[20230205 18:19:16 @agent_ppo2.py:144][0m Average TRAINING episode reward: 248.42
[32m[20230205 18:19:16 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 251.99
[32m[20230205 18:19:16 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 280.73
[32m[20230205 18:19:16 @agent_ppo2.py:149][0m Total time:       5.81 min
[32m[20230205 18:19:16 @agent_ppo2.py:151][0m 358400 total steps have happened
[32m[20230205 18:19:16 @agent_ppo2.py:127][0m #------------------------ Iteration 175 --------------------------#
[32m[20230205 18:19:17 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:19:17 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:17 @agent_ppo2.py:191][0m |           0.0011 |           7.7940 |           3.1297 |
[32m[20230205 18:19:17 @agent_ppo2.py:191][0m |          -0.0030 |           7.2306 |           3.1253 |
[32m[20230205 18:19:17 @agent_ppo2.py:191][0m |          -0.0023 |           6.9494 |           3.1178 |
[32m[20230205 18:19:17 @agent_ppo2.py:191][0m |          -0.0055 |           6.7281 |           3.1173 |
[32m[20230205 18:19:17 @agent_ppo2.py:191][0m |          -0.0090 |           6.5758 |           3.1158 |
[32m[20230205 18:19:17 @agent_ppo2.py:191][0m |          -0.0081 |           6.4677 |           3.1175 |
[32m[20230205 18:19:17 @agent_ppo2.py:191][0m |          -0.0080 |           6.3576 |           3.1146 |
[32m[20230205 18:19:17 @agent_ppo2.py:191][0m |          -0.0112 |           6.2234 |           3.1127 |
[32m[20230205 18:19:17 @agent_ppo2.py:191][0m |          -0.0107 |           6.1275 |           3.1134 |
[32m[20230205 18:19:17 @agent_ppo2.py:191][0m |          -0.0104 |           6.0212 |           3.1127 |
[32m[20230205 18:19:17 @agent_ppo2.py:136][0m Policy update time: 0.60 s
[32m[20230205 18:19:18 @agent_ppo2.py:144][0m Average TRAINING episode reward: 249.35
[32m[20230205 18:19:18 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.93
[32m[20230205 18:19:18 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.83
[32m[20230205 18:19:18 @agent_ppo2.py:149][0m Total time:       5.84 min
[32m[20230205 18:19:18 @agent_ppo2.py:151][0m 360448 total steps have happened
[32m[20230205 18:19:18 @agent_ppo2.py:127][0m #------------------------ Iteration 176 --------------------------#
[32m[20230205 18:19:18 @agent_ppo2.py:133][0m Sampling time: 0.65 s by 1 slaves
[32m[20230205 18:19:18 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:18 @agent_ppo2.py:191][0m |           0.0018 |           7.2174 |           3.0571 |
[32m[20230205 18:19:19 @agent_ppo2.py:191][0m |          -0.0055 |           6.4569 |           3.0555 |
[32m[20230205 18:19:19 @agent_ppo2.py:191][0m |          -0.0027 |           6.3173 |           3.0513 |
[32m[20230205 18:19:19 @agent_ppo2.py:191][0m |          -0.0114 |           6.1630 |           3.0470 |
[32m[20230205 18:19:19 @agent_ppo2.py:191][0m |          -0.0139 |           6.0612 |           3.0448 |
[32m[20230205 18:19:19 @agent_ppo2.py:191][0m |          -0.0044 |           6.1439 |           3.0447 |
[32m[20230205 18:19:19 @agent_ppo2.py:191][0m |          -0.0091 |           5.9556 |           3.0445 |
[32m[20230205 18:19:19 @agent_ppo2.py:191][0m |          -0.0140 |           5.8212 |           3.0410 |
[32m[20230205 18:19:19 @agent_ppo2.py:191][0m |          -0.0130 |           5.7984 |           3.0425 |
[32m[20230205 18:19:19 @agent_ppo2.py:191][0m |          -0.0114 |           5.6866 |           3.0406 |
[32m[20230205 18:19:19 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:19:20 @agent_ppo2.py:144][0m Average TRAINING episode reward: 242.51
[32m[20230205 18:19:20 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 242.96
[32m[20230205 18:19:20 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 280.23
[32m[20230205 18:19:20 @agent_ppo2.py:149][0m Total time:       5.87 min
[32m[20230205 18:19:20 @agent_ppo2.py:151][0m 362496 total steps have happened
[32m[20230205 18:19:20 @agent_ppo2.py:127][0m #------------------------ Iteration 177 --------------------------#
[32m[20230205 18:19:20 @agent_ppo2.py:133][0m Sampling time: 0.65 s by 1 slaves
[32m[20230205 18:19:20 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:20 @agent_ppo2.py:191][0m |          -0.0040 |           8.6656 |           3.0009 |
[32m[20230205 18:19:20 @agent_ppo2.py:191][0m |          -0.0048 |           7.7593 |           2.9996 |
[32m[20230205 18:19:20 @agent_ppo2.py:191][0m |          -0.0098 |           7.5093 |           3.0005 |
[32m[20230205 18:19:20 @agent_ppo2.py:191][0m |          -0.0101 |           7.3742 |           2.9988 |
[32m[20230205 18:19:21 @agent_ppo2.py:191][0m |          -0.0106 |           7.2634 |           3.0002 |
[32m[20230205 18:19:21 @agent_ppo2.py:191][0m |          -0.0088 |           7.5017 |           2.9963 |
[32m[20230205 18:19:21 @agent_ppo2.py:191][0m |          -0.0117 |           7.1786 |           2.9978 |
[32m[20230205 18:19:21 @agent_ppo2.py:191][0m |          -0.0120 |           7.0874 |           2.9973 |
[32m[20230205 18:19:21 @agent_ppo2.py:191][0m |          -0.0139 |           7.0291 |           2.9970 |
[32m[20230205 18:19:21 @agent_ppo2.py:191][0m |          -0.0151 |           7.0044 |           2.9952 |
[32m[20230205 18:19:21 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:19:21 @agent_ppo2.py:144][0m Average TRAINING episode reward: 239.95
[32m[20230205 18:19:21 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 240.99
[32m[20230205 18:19:21 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 202.61
[32m[20230205 18:19:21 @agent_ppo2.py:149][0m Total time:       5.90 min
[32m[20230205 18:19:21 @agent_ppo2.py:151][0m 364544 total steps have happened
[32m[20230205 18:19:21 @agent_ppo2.py:127][0m #------------------------ Iteration 178 --------------------------#
[32m[20230205 18:19:22 @agent_ppo2.py:133][0m Sampling time: 0.65 s by 1 slaves
[32m[20230205 18:19:22 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:22 @agent_ppo2.py:191][0m |           0.0011 |           7.1034 |           3.1016 |
[32m[20230205 18:19:22 @agent_ppo2.py:191][0m |          -0.0039 |           7.0922 |           3.0953 |
[32m[20230205 18:19:22 @agent_ppo2.py:191][0m |          -0.0108 |           6.8685 |           3.0920 |
[32m[20230205 18:19:22 @agent_ppo2.py:191][0m |          -0.0084 |           6.9127 |           3.0911 |
[32m[20230205 18:19:22 @agent_ppo2.py:191][0m |          -0.0119 |           6.8245 |           3.0902 |
[32m[20230205 18:19:22 @agent_ppo2.py:191][0m |          -0.0146 |           6.7246 |           3.0897 |
[32m[20230205 18:19:22 @agent_ppo2.py:191][0m |          -0.0122 |           6.7136 |           3.0901 |
[32m[20230205 18:19:22 @agent_ppo2.py:191][0m |          -0.0129 |           6.6903 |           3.0897 |
[32m[20230205 18:19:23 @agent_ppo2.py:191][0m |          -0.0151 |           6.6175 |           3.0906 |
[32m[20230205 18:19:23 @agent_ppo2.py:191][0m |          -0.0148 |           6.6452 |           3.0898 |
[32m[20230205 18:19:23 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:19:23 @agent_ppo2.py:144][0m Average TRAINING episode reward: 242.50
[32m[20230205 18:19:23 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 246.73
[32m[20230205 18:19:23 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.56
[32m[20230205 18:19:23 @agent_ppo2.py:149][0m Total time:       5.93 min
[32m[20230205 18:19:23 @agent_ppo2.py:151][0m 366592 total steps have happened
[32m[20230205 18:19:23 @agent_ppo2.py:127][0m #------------------------ Iteration 179 --------------------------#
[32m[20230205 18:19:24 @agent_ppo2.py:133][0m Sampling time: 0.67 s by 1 slaves
[32m[20230205 18:19:24 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:24 @agent_ppo2.py:191][0m |          -0.0045 |           7.1358 |           3.0202 |
[32m[20230205 18:19:24 @agent_ppo2.py:191][0m |          -0.0064 |           5.1281 |           3.0216 |
[32m[20230205 18:19:24 @agent_ppo2.py:191][0m |          -0.0023 |           5.3523 |           3.0198 |
[32m[20230205 18:19:24 @agent_ppo2.py:191][0m |          -0.0080 |           4.8280 |           3.0195 |
[32m[20230205 18:19:24 @agent_ppo2.py:191][0m |          -0.0132 |           4.6591 |           3.0198 |
[32m[20230205 18:19:24 @agent_ppo2.py:191][0m |          -0.0109 |           4.6115 |           3.0155 |
[32m[20230205 18:19:24 @agent_ppo2.py:191][0m |          -0.0160 |           4.5430 |           3.0210 |
[32m[20230205 18:19:24 @agent_ppo2.py:191][0m |          -0.0052 |           4.8442 |           3.0178 |
[32m[20230205 18:19:24 @agent_ppo2.py:191][0m |          -0.0197 |           4.4362 |           3.0203 |
[32m[20230205 18:19:24 @agent_ppo2.py:191][0m |          -0.0121 |           4.4918 |           3.0215 |
[32m[20230205 18:19:24 @agent_ppo2.py:136][0m Policy update time: 0.65 s
[32m[20230205 18:19:25 @agent_ppo2.py:144][0m Average TRAINING episode reward: 196.07
[32m[20230205 18:19:25 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 248.75
[32m[20230205 18:19:25 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 281.33
[32m[20230205 18:19:25 @agent_ppo2.py:149][0m Total time:       5.96 min
[32m[20230205 18:19:25 @agent_ppo2.py:151][0m 368640 total steps have happened
[32m[20230205 18:19:25 @agent_ppo2.py:127][0m #------------------------ Iteration 180 --------------------------#
[32m[20230205 18:19:26 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:19:26 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:26 @agent_ppo2.py:191][0m |          -0.0111 |          18.3719 |           3.0608 |
[32m[20230205 18:19:26 @agent_ppo2.py:191][0m |          -0.0086 |           9.8080 |           3.0526 |
[32m[20230205 18:19:26 @agent_ppo2.py:191][0m |          -0.0085 |           8.7823 |           3.0549 |
[32m[20230205 18:19:26 @agent_ppo2.py:191][0m |          -0.0102 |           6.9278 |           3.0550 |
[32m[20230205 18:19:26 @agent_ppo2.py:191][0m |           0.0069 |           7.3609 |           3.0561 |
[32m[20230205 18:19:26 @agent_ppo2.py:191][0m |          -0.0164 |           6.1524 |           3.0586 |
[32m[20230205 18:19:26 @agent_ppo2.py:191][0m |          -0.0213 |           5.9036 |           3.0597 |
[32m[20230205 18:19:26 @agent_ppo2.py:191][0m |          -0.0098 |           5.3944 |           3.0572 |
[32m[20230205 18:19:26 @agent_ppo2.py:191][0m |          -0.0182 |           5.1829 |           3.0566 |
[32m[20230205 18:19:26 @agent_ppo2.py:191][0m |           0.0262 |           5.3645 |           3.0570 |
[32m[20230205 18:19:26 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:19:27 @agent_ppo2.py:144][0m Average TRAINING episode reward: 174.09
[32m[20230205 18:19:27 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 246.04
[32m[20230205 18:19:27 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 186.34
[32m[20230205 18:19:27 @agent_ppo2.py:149][0m Total time:       5.99 min
[32m[20230205 18:19:27 @agent_ppo2.py:151][0m 370688 total steps have happened
[32m[20230205 18:19:27 @agent_ppo2.py:127][0m #------------------------ Iteration 181 --------------------------#
[32m[20230205 18:19:27 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:19:27 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:27 @agent_ppo2.py:191][0m |          -0.0052 |          46.4697 |           3.1925 |
[32m[20230205 18:19:27 @agent_ppo2.py:191][0m |           0.0080 |          35.8110 |           3.1882 |
[32m[20230205 18:19:27 @agent_ppo2.py:191][0m |          -0.0127 |          25.6020 |           3.1874 |
[32m[20230205 18:19:27 @agent_ppo2.py:191][0m |          -0.0090 |          22.6604 |           3.1892 |
[32m[20230205 18:19:28 @agent_ppo2.py:191][0m |          -0.0165 |          21.2807 |           3.1871 |
[32m[20230205 18:19:28 @agent_ppo2.py:191][0m |          -0.0133 |          19.3337 |           3.1883 |
[32m[20230205 18:19:28 @agent_ppo2.py:191][0m |          -0.0196 |          18.1360 |           3.1867 |
[32m[20230205 18:19:28 @agent_ppo2.py:191][0m |          -0.0165 |          17.3833 |           3.1878 |
[32m[20230205 18:19:28 @agent_ppo2.py:191][0m |          -0.0181 |          15.9898 |           3.1892 |
[32m[20230205 18:19:28 @agent_ppo2.py:191][0m |          -0.0183 |          15.3410 |           3.1901 |
[32m[20230205 18:19:28 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:19:28 @agent_ppo2.py:144][0m Average TRAINING episode reward: 159.93
[32m[20230205 18:19:28 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.27
[32m[20230205 18:19:28 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 286.57
[32m[20230205 18:19:28 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 286.57
[32m[20230205 18:19:28 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 286.57
[32m[20230205 18:19:28 @agent_ppo2.py:149][0m Total time:       6.01 min
[32m[20230205 18:19:28 @agent_ppo2.py:151][0m 372736 total steps have happened
[32m[20230205 18:19:28 @agent_ppo2.py:127][0m #------------------------ Iteration 182 --------------------------#
[32m[20230205 18:19:29 @agent_ppo2.py:133][0m Sampling time: 0.65 s by 1 slaves
[32m[20230205 18:19:29 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:29 @agent_ppo2.py:191][0m |           0.0022 |           9.6225 |           3.1202 |
[32m[20230205 18:19:29 @agent_ppo2.py:191][0m |          -0.0040 |           8.7937 |           3.1152 |
[32m[20230205 18:19:29 @agent_ppo2.py:191][0m |          -0.0094 |           8.6298 |           3.1131 |
[32m[20230205 18:19:29 @agent_ppo2.py:191][0m |          -0.0079 |           8.4517 |           3.1108 |
[32m[20230205 18:19:29 @agent_ppo2.py:191][0m |          -0.0098 |           8.3364 |           3.1107 |
[32m[20230205 18:19:29 @agent_ppo2.py:191][0m |          -0.0093 |           8.3201 |           3.1141 |
[32m[20230205 18:19:29 @agent_ppo2.py:191][0m |          -0.0092 |           8.5174 |           3.1156 |
[32m[20230205 18:19:29 @agent_ppo2.py:191][0m |          -0.0100 |           8.1967 |           3.1157 |
[32m[20230205 18:19:29 @agent_ppo2.py:191][0m |          -0.0112 |           8.0836 |           3.1128 |
[32m[20230205 18:19:30 @agent_ppo2.py:191][0m |          -0.0125 |           8.0024 |           3.1174 |
[32m[20230205 18:19:30 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:19:30 @agent_ppo2.py:144][0m Average TRAINING episode reward: 241.35
[32m[20230205 18:19:30 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 241.75
[32m[20230205 18:19:30 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 199.13
[32m[20230205 18:19:30 @agent_ppo2.py:149][0m Total time:       6.04 min
[32m[20230205 18:19:30 @agent_ppo2.py:151][0m 374784 total steps have happened
[32m[20230205 18:19:30 @agent_ppo2.py:127][0m #------------------------ Iteration 183 --------------------------#
[32m[20230205 18:19:31 @agent_ppo2.py:133][0m Sampling time: 0.62 s by 1 slaves
[32m[20230205 18:19:31 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:31 @agent_ppo2.py:191][0m |          -0.0019 |           8.5669 |           3.1550 |
[32m[20230205 18:19:31 @agent_ppo2.py:191][0m |          -0.0076 |           8.3478 |           3.1468 |
[32m[20230205 18:19:31 @agent_ppo2.py:191][0m |          -0.0086 |           8.2275 |           3.1429 |
[32m[20230205 18:19:31 @agent_ppo2.py:191][0m |          -0.0088 |           8.4828 |           3.1407 |
[32m[20230205 18:19:31 @agent_ppo2.py:191][0m |          -0.0146 |           8.1620 |           3.1411 |
[32m[20230205 18:19:31 @agent_ppo2.py:191][0m |          -0.0179 |           7.9306 |           3.1399 |
[32m[20230205 18:19:31 @agent_ppo2.py:191][0m |          -0.0128 |           8.1159 |           3.1384 |
[32m[20230205 18:19:31 @agent_ppo2.py:191][0m |          -0.0164 |           7.7803 |           3.1365 |
[32m[20230205 18:19:31 @agent_ppo2.py:191][0m |          -0.0145 |           7.9513 |           3.1334 |
[32m[20230205 18:19:31 @agent_ppo2.py:191][0m |          -0.0179 |           7.6573 |           3.1334 |
[32m[20230205 18:19:31 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:19:32 @agent_ppo2.py:144][0m Average TRAINING episode reward: 246.51
[32m[20230205 18:19:32 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 247.39
[32m[20230205 18:19:32 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 193.98
[32m[20230205 18:19:32 @agent_ppo2.py:149][0m Total time:       6.07 min
[32m[20230205 18:19:32 @agent_ppo2.py:151][0m 376832 total steps have happened
[32m[20230205 18:19:32 @agent_ppo2.py:127][0m #------------------------ Iteration 184 --------------------------#
[32m[20230205 18:19:32 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:19:32 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:32 @agent_ppo2.py:191][0m |           0.0007 |           7.9965 |           3.1685 |
[32m[20230205 18:19:32 @agent_ppo2.py:191][0m |          -0.0051 |           7.7544 |           3.1608 |
[32m[20230205 18:19:33 @agent_ppo2.py:191][0m |          -0.0080 |           7.5689 |           3.1578 |
[32m[20230205 18:19:33 @agent_ppo2.py:191][0m |          -0.0096 |           7.4489 |           3.1573 |
[32m[20230205 18:19:33 @agent_ppo2.py:191][0m |          -0.0108 |           7.3758 |           3.1551 |
[32m[20230205 18:19:33 @agent_ppo2.py:191][0m |          -0.0116 |           7.2644 |           3.1512 |
[32m[20230205 18:19:33 @agent_ppo2.py:191][0m |          -0.0123 |           7.1663 |           3.1511 |
[32m[20230205 18:19:33 @agent_ppo2.py:191][0m |          -0.0121 |           7.1232 |           3.1518 |
[32m[20230205 18:19:33 @agent_ppo2.py:191][0m |          -0.0135 |           7.0255 |           3.1518 |
[32m[20230205 18:19:33 @agent_ppo2.py:191][0m |          -0.0135 |           6.9595 |           3.1520 |
[32m[20230205 18:19:33 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:19:33 @agent_ppo2.py:144][0m Average TRAINING episode reward: 250.39
[32m[20230205 18:19:33 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.99
[32m[20230205 18:19:33 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 280.96
[32m[20230205 18:19:33 @agent_ppo2.py:149][0m Total time:       6.10 min
[32m[20230205 18:19:33 @agent_ppo2.py:151][0m 378880 total steps have happened
[32m[20230205 18:19:33 @agent_ppo2.py:127][0m #------------------------ Iteration 185 --------------------------#
[32m[20230205 18:19:34 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:19:34 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:34 @agent_ppo2.py:191][0m |           0.0003 |           9.8185 |           3.2347 |
[32m[20230205 18:19:34 @agent_ppo2.py:191][0m |          -0.0050 |           8.1911 |           3.2308 |
[32m[20230205 18:19:34 @agent_ppo2.py:191][0m |          -0.0063 |           7.7607 |           3.2248 |
[32m[20230205 18:19:34 @agent_ppo2.py:191][0m |          -0.0081 |           7.5087 |           3.2257 |
[32m[20230205 18:19:34 @agent_ppo2.py:191][0m |          -0.0095 |           7.3279 |           3.2226 |
[32m[20230205 18:19:34 @agent_ppo2.py:191][0m |          -0.0091 |           7.1843 |           3.2210 |
[32m[20230205 18:19:34 @agent_ppo2.py:191][0m |          -0.0116 |           6.9930 |           3.2225 |
[32m[20230205 18:19:34 @agent_ppo2.py:191][0m |          -0.0111 |           6.8582 |           3.2210 |
[32m[20230205 18:19:35 @agent_ppo2.py:191][0m |          -0.0121 |           6.7430 |           3.2210 |
[32m[20230205 18:19:35 @agent_ppo2.py:191][0m |          -0.0123 |           6.6181 |           3.2195 |
[32m[20230205 18:19:35 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:19:35 @agent_ppo2.py:144][0m Average TRAINING episode reward: 252.91
[32m[20230205 18:19:35 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.90
[32m[20230205 18:19:35 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 178.13
[32m[20230205 18:19:35 @agent_ppo2.py:149][0m Total time:       6.13 min
[32m[20230205 18:19:35 @agent_ppo2.py:151][0m 380928 total steps have happened
[32m[20230205 18:19:35 @agent_ppo2.py:127][0m #------------------------ Iteration 186 --------------------------#
[32m[20230205 18:19:36 @agent_ppo2.py:133][0m Sampling time: 0.62 s by 1 slaves
[32m[20230205 18:19:36 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:36 @agent_ppo2.py:191][0m |           0.0016 |          10.5906 |           3.0823 |
[32m[20230205 18:19:36 @agent_ppo2.py:191][0m |          -0.0053 |           7.9236 |           3.0856 |
[32m[20230205 18:19:36 @agent_ppo2.py:191][0m |           0.0008 |           7.8731 |           3.0854 |
[32m[20230205 18:19:36 @agent_ppo2.py:191][0m |          -0.0101 |           7.1580 |           3.0864 |
[32m[20230205 18:19:36 @agent_ppo2.py:191][0m |          -0.0104 |           7.0042 |           3.0873 |
[32m[20230205 18:19:36 @agent_ppo2.py:191][0m |          -0.0125 |           6.8194 |           3.0872 |
[32m[20230205 18:19:36 @agent_ppo2.py:191][0m |          -0.0102 |           6.6611 |           3.0860 |
[32m[20230205 18:19:36 @agent_ppo2.py:191][0m |          -0.0101 |           6.5369 |           3.0868 |
[32m[20230205 18:19:36 @agent_ppo2.py:191][0m |          -0.0130 |           6.5568 |           3.0883 |
[32m[20230205 18:19:36 @agent_ppo2.py:191][0m |          -0.0148 |           6.4197 |           3.0896 |
[32m[20230205 18:19:36 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:19:37 @agent_ppo2.py:144][0m Average TRAINING episode reward: 248.81
[32m[20230205 18:19:37 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.91
[32m[20230205 18:19:37 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 185.41
[32m[20230205 18:19:37 @agent_ppo2.py:149][0m Total time:       6.16 min
[32m[20230205 18:19:37 @agent_ppo2.py:151][0m 382976 total steps have happened
[32m[20230205 18:19:37 @agent_ppo2.py:127][0m #------------------------ Iteration 187 --------------------------#
[32m[20230205 18:19:38 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:19:38 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:38 @agent_ppo2.py:191][0m |          -0.0104 |           9.9014 |           3.1544 |
[32m[20230205 18:19:38 @agent_ppo2.py:191][0m |          -0.0137 |           9.1925 |           3.1502 |
[32m[20230205 18:19:38 @agent_ppo2.py:191][0m |          -0.0067 |           8.9610 |           3.1488 |
[32m[20230205 18:19:38 @agent_ppo2.py:191][0m |          -0.0147 |           8.7681 |           3.1499 |
[32m[20230205 18:19:38 @agent_ppo2.py:191][0m |          -0.0037 |           8.7520 |           3.1492 |
[32m[20230205 18:19:38 @agent_ppo2.py:191][0m |          -0.0094 |           8.6181 |           3.1488 |
[32m[20230205 18:19:38 @agent_ppo2.py:191][0m |          -0.0125 |           8.5402 |           3.1514 |
[32m[20230205 18:19:38 @agent_ppo2.py:191][0m |          -0.0541 |           8.4341 |           3.1516 |
[32m[20230205 18:19:38 @agent_ppo2.py:191][0m |          -0.0074 |           8.4538 |           3.1515 |
[32m[20230205 18:19:38 @agent_ppo2.py:191][0m |          -0.0184 |           8.2848 |           3.1500 |
[32m[20230205 18:19:38 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:19:39 @agent_ppo2.py:144][0m Average TRAINING episode reward: 252.04
[32m[20230205 18:19:39 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.94
[32m[20230205 18:19:39 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 279.81
[32m[20230205 18:19:39 @agent_ppo2.py:149][0m Total time:       6.19 min
[32m[20230205 18:19:39 @agent_ppo2.py:151][0m 385024 total steps have happened
[32m[20230205 18:19:39 @agent_ppo2.py:127][0m #------------------------ Iteration 188 --------------------------#
[32m[20230205 18:19:39 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:19:39 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:40 @agent_ppo2.py:191][0m |           0.0001 |          44.8826 |           3.0518 |
[32m[20230205 18:19:40 @agent_ppo2.py:191][0m |          -0.0029 |          26.8032 |           3.0460 |
[32m[20230205 18:19:40 @agent_ppo2.py:191][0m |          -0.0031 |          23.3089 |           3.0479 |
[32m[20230205 18:19:40 @agent_ppo2.py:191][0m |          -0.0063 |          20.7122 |           3.0513 |
[32m[20230205 18:19:40 @agent_ppo2.py:191][0m |          -0.0086 |          18.6912 |           3.0501 |
[32m[20230205 18:19:40 @agent_ppo2.py:191][0m |          -0.0084 |          18.7666 |           3.0482 |
[32m[20230205 18:19:40 @agent_ppo2.py:191][0m |          -0.0086 |          17.6071 |           3.0503 |
[32m[20230205 18:19:40 @agent_ppo2.py:191][0m |          -0.0060 |          17.9291 |           3.0493 |
[32m[20230205 18:19:40 @agent_ppo2.py:191][0m |          -0.0091 |          16.7870 |           3.0491 |
[32m[20230205 18:19:40 @agent_ppo2.py:191][0m |          -0.0067 |          16.4047 |           3.0461 |
[32m[20230205 18:19:40 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:19:41 @agent_ppo2.py:144][0m Average TRAINING episode reward: 95.50
[32m[20230205 18:19:41 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.11
[32m[20230205 18:19:41 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 163.81
[32m[20230205 18:19:41 @agent_ppo2.py:149][0m Total time:       6.22 min
[32m[20230205 18:19:41 @agent_ppo2.py:151][0m 387072 total steps have happened
[32m[20230205 18:19:41 @agent_ppo2.py:127][0m #------------------------ Iteration 189 --------------------------#
[32m[20230205 18:19:41 @agent_ppo2.py:133][0m Sampling time: 0.74 s by 1 slaves
[32m[20230205 18:19:42 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:42 @agent_ppo2.py:191][0m |           0.0007 |          27.9507 |           3.1956 |
[32m[20230205 18:19:42 @agent_ppo2.py:191][0m |          -0.0046 |          19.5115 |           3.1982 |
[32m[20230205 18:19:42 @agent_ppo2.py:191][0m |          -0.0052 |          16.9770 |           3.1953 |
[32m[20230205 18:19:42 @agent_ppo2.py:191][0m |          -0.0065 |          15.3301 |           3.1955 |
[32m[20230205 18:19:42 @agent_ppo2.py:191][0m |          -0.0084 |          14.2608 |           3.1982 |
[32m[20230205 18:19:42 @agent_ppo2.py:191][0m |          -0.0086 |          13.8532 |           3.1920 |
[32m[20230205 18:19:42 @agent_ppo2.py:191][0m |          -0.0091 |          13.3601 |           3.1950 |
[32m[20230205 18:19:42 @agent_ppo2.py:191][0m |          -0.0095 |          13.1010 |           3.1948 |
[32m[20230205 18:19:42 @agent_ppo2.py:191][0m |          -0.0117 |          12.7937 |           3.1923 |
[32m[20230205 18:19:42 @agent_ppo2.py:191][0m |          -0.0119 |          12.5882 |           3.1913 |
[32m[20230205 18:19:42 @agent_ppo2.py:136][0m Policy update time: 0.73 s
[32m[20230205 18:19:43 @agent_ppo2.py:144][0m Average TRAINING episode reward: 177.60
[32m[20230205 18:19:43 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.21
[32m[20230205 18:19:43 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.51
[32m[20230205 18:19:43 @agent_ppo2.py:149][0m Total time:       6.26 min
[32m[20230205 18:19:43 @agent_ppo2.py:151][0m 389120 total steps have happened
[32m[20230205 18:19:43 @agent_ppo2.py:127][0m #------------------------ Iteration 190 --------------------------#
[32m[20230205 18:19:43 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:19:43 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:43 @agent_ppo2.py:191][0m |          -0.0018 |          50.7682 |           3.1883 |
[32m[20230205 18:19:43 @agent_ppo2.py:191][0m |           0.0126 |          37.5754 |           3.1809 |
[32m[20230205 18:19:43 @agent_ppo2.py:191][0m |          -0.0263 |          22.9572 |           3.1725 |
[32m[20230205 18:19:44 @agent_ppo2.py:191][0m |           0.0247 |          20.0104 |           3.1785 |
[32m[20230205 18:19:44 @agent_ppo2.py:191][0m |          -0.0128 |          13.6340 |           3.1760 |
[32m[20230205 18:19:44 @agent_ppo2.py:191][0m |          -0.0134 |          12.3331 |           3.1725 |
[32m[20230205 18:19:44 @agent_ppo2.py:191][0m |          -0.0089 |          11.3805 |           3.1751 |
[32m[20230205 18:19:44 @agent_ppo2.py:191][0m |          -0.0164 |          10.7247 |           3.1718 |
[32m[20230205 18:19:44 @agent_ppo2.py:191][0m |          -0.0167 |          10.2440 |           3.1752 |
[32m[20230205 18:19:44 @agent_ppo2.py:191][0m |          -0.0168 |           9.9253 |           3.1734 |
[32m[20230205 18:19:44 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:19:44 @agent_ppo2.py:144][0m Average TRAINING episode reward: 162.66
[32m[20230205 18:19:44 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.49
[32m[20230205 18:19:44 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 124.94
[32m[20230205 18:19:44 @agent_ppo2.py:149][0m Total time:       6.28 min
[32m[20230205 18:19:44 @agent_ppo2.py:151][0m 391168 total steps have happened
[32m[20230205 18:19:44 @agent_ppo2.py:127][0m #------------------------ Iteration 191 --------------------------#
[32m[20230205 18:19:45 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:19:45 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:45 @agent_ppo2.py:191][0m |           0.0008 |          89.8837 |           3.1125 |
[32m[20230205 18:19:45 @agent_ppo2.py:191][0m |          -0.0031 |          68.4951 |           3.1067 |
[32m[20230205 18:19:45 @agent_ppo2.py:191][0m |          -0.0065 |          58.8627 |           3.1035 |
[32m[20230205 18:19:45 @agent_ppo2.py:191][0m |          -0.0053 |          53.6897 |           3.1050 |
[32m[20230205 18:19:45 @agent_ppo2.py:191][0m |          -0.0070 |          48.9880 |           3.1017 |
[32m[20230205 18:19:45 @agent_ppo2.py:191][0m |          -0.0091 |          46.4126 |           3.1008 |
[32m[20230205 18:19:45 @agent_ppo2.py:191][0m |          -0.0105 |          43.4909 |           3.1015 |
[32m[20230205 18:19:45 @agent_ppo2.py:191][0m |          -0.0124 |          41.7873 |           3.1000 |
[32m[20230205 18:19:45 @agent_ppo2.py:191][0m |          -0.0125 |          40.0918 |           3.0987 |
[32m[20230205 18:19:45 @agent_ppo2.py:191][0m |          -0.0149 |          38.8502 |           3.1003 |
[32m[20230205 18:19:45 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:19:46 @agent_ppo2.py:144][0m Average TRAINING episode reward: 36.55
[32m[20230205 18:19:46 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 43.91
[32m[20230205 18:19:46 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 192.34
[32m[20230205 18:19:46 @agent_ppo2.py:149][0m Total time:       6.31 min
[32m[20230205 18:19:46 @agent_ppo2.py:151][0m 393216 total steps have happened
[32m[20230205 18:19:46 @agent_ppo2.py:127][0m #------------------------ Iteration 192 --------------------------#
[32m[20230205 18:19:47 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:19:47 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:47 @agent_ppo2.py:191][0m |          -0.0013 |          52.4296 |           3.2294 |
[32m[20230205 18:19:47 @agent_ppo2.py:191][0m |          -0.0106 |          44.3797 |           3.2171 |
[32m[20230205 18:19:47 @agent_ppo2.py:191][0m |          -0.0030 |          42.8521 |           3.2155 |
[32m[20230205 18:19:47 @agent_ppo2.py:191][0m |          -0.0123 |          39.9643 |           3.2130 |
[32m[20230205 18:19:47 @agent_ppo2.py:191][0m |          -0.0096 |          38.0683 |           3.2105 |
[32m[20230205 18:19:47 @agent_ppo2.py:191][0m |          -0.0108 |          36.4752 |           3.2051 |
[32m[20230205 18:19:47 @agent_ppo2.py:191][0m |          -0.0150 |          35.2911 |           3.2079 |
[32m[20230205 18:19:47 @agent_ppo2.py:191][0m |          -0.0122 |          34.2020 |           3.2069 |
[32m[20230205 18:19:47 @agent_ppo2.py:191][0m |          -0.0137 |          33.3736 |           3.2083 |
[32m[20230205 18:19:47 @agent_ppo2.py:191][0m |          -0.0099 |          32.8791 |           3.2020 |
[32m[20230205 18:19:47 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:19:48 @agent_ppo2.py:144][0m Average TRAINING episode reward: 146.94
[32m[20230205 18:19:48 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 244.66
[32m[20230205 18:19:48 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.59
[32m[20230205 18:19:48 @agent_ppo2.py:149][0m Total time:       6.34 min
[32m[20230205 18:19:48 @agent_ppo2.py:151][0m 395264 total steps have happened
[32m[20230205 18:19:48 @agent_ppo2.py:127][0m #------------------------ Iteration 193 --------------------------#
[32m[20230205 18:19:48 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:19:48 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:48 @agent_ppo2.py:191][0m |          -0.0019 |          58.0683 |           3.0871 |
[32m[20230205 18:19:48 @agent_ppo2.py:191][0m |          -0.0102 |          37.9131 |           3.0841 |
[32m[20230205 18:19:48 @agent_ppo2.py:191][0m |          -0.0142 |          31.4242 |           3.0791 |
[32m[20230205 18:19:49 @agent_ppo2.py:191][0m |          -0.0108 |          27.7937 |           3.0801 |
[32m[20230205 18:19:49 @agent_ppo2.py:191][0m |          -0.0121 |          25.5041 |           3.0779 |
[32m[20230205 18:19:49 @agent_ppo2.py:191][0m |          -0.0195 |          23.1219 |           3.0782 |
[32m[20230205 18:19:49 @agent_ppo2.py:191][0m |          -0.0208 |          22.3386 |           3.0767 |
[32m[20230205 18:19:49 @agent_ppo2.py:191][0m |          -0.0195 |          21.4863 |           3.0772 |
[32m[20230205 18:19:49 @agent_ppo2.py:191][0m |          -0.0190 |          21.0596 |           3.0767 |
[32m[20230205 18:19:49 @agent_ppo2.py:191][0m |          -0.0309 |          20.4376 |           3.0770 |
[32m[20230205 18:19:49 @agent_ppo2.py:136][0m Policy update time: 0.49 s
[32m[20230205 18:19:49 @agent_ppo2.py:144][0m Average TRAINING episode reward: 143.56
[32m[20230205 18:19:49 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 248.90
[32m[20230205 18:19:49 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.75
[32m[20230205 18:19:49 @agent_ppo2.py:149][0m Total time:       6.37 min
[32m[20230205 18:19:49 @agent_ppo2.py:151][0m 397312 total steps have happened
[32m[20230205 18:19:49 @agent_ppo2.py:127][0m #------------------------ Iteration 194 --------------------------#
[32m[20230205 18:19:50 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:19:50 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:50 @agent_ppo2.py:191][0m |          -0.0015 |          11.5766 |           3.2210 |
[32m[20230205 18:19:50 @agent_ppo2.py:191][0m |          -0.0085 |          10.0587 |           3.2134 |
[32m[20230205 18:19:50 @agent_ppo2.py:191][0m |          -0.0106 |           9.5160 |           3.2152 |
[32m[20230205 18:19:50 @agent_ppo2.py:191][0m |          -0.0115 |           9.2622 |           3.2099 |
[32m[20230205 18:19:50 @agent_ppo2.py:191][0m |          -0.0129 |           8.9970 |           3.2099 |
[32m[20230205 18:19:50 @agent_ppo2.py:191][0m |          -0.0141 |           8.8094 |           3.2087 |
[32m[20230205 18:19:50 @agent_ppo2.py:191][0m |          -0.0148 |           8.6588 |           3.2096 |
[32m[20230205 18:19:50 @agent_ppo2.py:191][0m |          -0.0135 |           8.6332 |           3.2101 |
[32m[20230205 18:19:50 @agent_ppo2.py:191][0m |          -0.0162 |           8.4722 |           3.2071 |
[32m[20230205 18:19:51 @agent_ppo2.py:191][0m |          -0.0155 |           8.4015 |           3.2106 |
[32m[20230205 18:19:51 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:19:51 @agent_ppo2.py:144][0m Average TRAINING episode reward: 252.74
[32m[20230205 18:19:51 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.25
[32m[20230205 18:19:51 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 127.80
[32m[20230205 18:19:51 @agent_ppo2.py:149][0m Total time:       6.39 min
[32m[20230205 18:19:51 @agent_ppo2.py:151][0m 399360 total steps have happened
[32m[20230205 18:19:51 @agent_ppo2.py:127][0m #------------------------ Iteration 195 --------------------------#
[32m[20230205 18:19:52 @agent_ppo2.py:133][0m Sampling time: 0.62 s by 1 slaves
[32m[20230205 18:19:52 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:52 @agent_ppo2.py:191][0m |           0.0031 |           9.6629 |           3.1313 |
[32m[20230205 18:19:52 @agent_ppo2.py:191][0m |          -0.0094 |           9.2266 |           3.1290 |
[32m[20230205 18:19:52 @agent_ppo2.py:191][0m |          -0.0050 |           9.0232 |           3.1275 |
[32m[20230205 18:19:52 @agent_ppo2.py:191][0m |           0.0137 |          10.5309 |           3.1281 |
[32m[20230205 18:19:52 @agent_ppo2.py:191][0m |          -0.0016 |           8.8268 |           3.1224 |
[32m[20230205 18:19:52 @agent_ppo2.py:191][0m |          -0.0098 |           8.6073 |           3.1243 |
[32m[20230205 18:19:52 @agent_ppo2.py:191][0m |          -0.0093 |           8.4966 |           3.1263 |
[32m[20230205 18:19:52 @agent_ppo2.py:191][0m |          -0.0158 |           8.4264 |           3.1272 |
[32m[20230205 18:19:52 @agent_ppo2.py:191][0m |          -0.0074 |           8.3313 |           3.1276 |
[32m[20230205 18:19:52 @agent_ppo2.py:191][0m |          -0.0162 |           8.3853 |           3.1279 |
[32m[20230205 18:19:52 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:19:53 @agent_ppo2.py:144][0m Average TRAINING episode reward: 247.60
[32m[20230205 18:19:53 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.06
[32m[20230205 18:19:53 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 190.68
[32m[20230205 18:19:53 @agent_ppo2.py:149][0m Total time:       6.43 min
[32m[20230205 18:19:53 @agent_ppo2.py:151][0m 401408 total steps have happened
[32m[20230205 18:19:53 @agent_ppo2.py:127][0m #------------------------ Iteration 196 --------------------------#
[32m[20230205 18:19:54 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:19:54 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:54 @agent_ppo2.py:191][0m |          -0.0008 |          19.4993 |           3.1927 |
[32m[20230205 18:19:54 @agent_ppo2.py:191][0m |          -0.0034 |          14.8287 |           3.1891 |
[32m[20230205 18:19:54 @agent_ppo2.py:191][0m |          -0.0052 |          13.3040 |           3.1871 |
[32m[20230205 18:19:54 @agent_ppo2.py:191][0m |          -0.0101 |          12.3365 |           3.1858 |
[32m[20230205 18:19:54 @agent_ppo2.py:191][0m |          -0.0098 |          11.6191 |           3.1855 |
[32m[20230205 18:19:54 @agent_ppo2.py:191][0m |          -0.0095 |          11.2753 |           3.1864 |
[32m[20230205 18:19:54 @agent_ppo2.py:191][0m |          -0.0107 |          10.8655 |           3.1852 |
[32m[20230205 18:19:54 @agent_ppo2.py:191][0m |          -0.0123 |          10.8030 |           3.1846 |
[32m[20230205 18:19:54 @agent_ppo2.py:191][0m |          -0.0122 |          10.0573 |           3.1819 |
[32m[20230205 18:19:54 @agent_ppo2.py:191][0m |          -0.0143 |           9.6895 |           3.1822 |
[32m[20230205 18:19:54 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:19:55 @agent_ppo2.py:144][0m Average TRAINING episode reward: 164.43
[32m[20230205 18:19:55 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.27
[32m[20230205 18:19:55 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 89.32
[32m[20230205 18:19:55 @agent_ppo2.py:149][0m Total time:       6.46 min
[32m[20230205 18:19:55 @agent_ppo2.py:151][0m 403456 total steps have happened
[32m[20230205 18:19:55 @agent_ppo2.py:127][0m #------------------------ Iteration 197 --------------------------#
[32m[20230205 18:19:56 @agent_ppo2.py:133][0m Sampling time: 0.73 s by 1 slaves
[32m[20230205 18:19:56 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:56 @agent_ppo2.py:191][0m |           0.0010 |          20.0801 |           3.2022 |
[32m[20230205 18:19:56 @agent_ppo2.py:191][0m |          -0.0032 |          15.1033 |           3.1976 |
[32m[20230205 18:19:56 @agent_ppo2.py:191][0m |          -0.0090 |          14.2149 |           3.1943 |
[32m[20230205 18:19:56 @agent_ppo2.py:191][0m |          -0.0082 |          13.2208 |           3.1919 |
[32m[20230205 18:19:56 @agent_ppo2.py:191][0m |          -0.0074 |          12.8061 |           3.1923 |
[32m[20230205 18:19:56 @agent_ppo2.py:191][0m |          -0.0090 |          12.1766 |           3.1943 |
[32m[20230205 18:19:56 @agent_ppo2.py:191][0m |          -0.0071 |          11.8814 |           3.1917 |
[32m[20230205 18:19:56 @agent_ppo2.py:191][0m |          -0.0002 |          11.2976 |           3.1906 |
[32m[20230205 18:19:56 @agent_ppo2.py:191][0m |          -0.0092 |          10.9971 |           3.1852 |
[32m[20230205 18:19:56 @agent_ppo2.py:191][0m |          -0.0062 |          11.0583 |           3.1929 |
[32m[20230205 18:19:56 @agent_ppo2.py:136][0m Policy update time: 0.72 s
[32m[20230205 18:19:57 @agent_ppo2.py:144][0m Average TRAINING episode reward: 179.50
[32m[20230205 18:19:57 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.48
[32m[20230205 18:19:57 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 174.06
[32m[20230205 18:19:57 @agent_ppo2.py:149][0m Total time:       6.49 min
[32m[20230205 18:19:57 @agent_ppo2.py:151][0m 405504 total steps have happened
[32m[20230205 18:19:57 @agent_ppo2.py:127][0m #------------------------ Iteration 198 --------------------------#
[32m[20230205 18:19:57 @agent_ppo2.py:133][0m Sampling time: 0.47 s by 1 slaves
[32m[20230205 18:19:57 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:57 @agent_ppo2.py:191][0m |          -0.0003 |          76.9198 |           3.2486 |
[32m[20230205 18:19:58 @agent_ppo2.py:191][0m |          -0.0058 |          60.0955 |           3.2371 |
[32m[20230205 18:19:58 @agent_ppo2.py:191][0m |          -0.0081 |          54.5506 |           3.2345 |
[32m[20230205 18:19:58 @agent_ppo2.py:191][0m |          -0.0126 |          50.6675 |           3.2314 |
[32m[20230205 18:19:58 @agent_ppo2.py:191][0m |          -0.0076 |          51.3101 |           3.2299 |
[32m[20230205 18:19:58 @agent_ppo2.py:191][0m |          -0.0087 |          47.1655 |           3.2279 |
[32m[20230205 18:19:58 @agent_ppo2.py:191][0m |          -0.0123 |          44.6331 |           3.2232 |
[32m[20230205 18:19:58 @agent_ppo2.py:191][0m |          -0.0152 |          42.5278 |           3.2203 |
[32m[20230205 18:19:58 @agent_ppo2.py:191][0m |          -0.0102 |          44.1107 |           3.2203 |
[32m[20230205 18:19:58 @agent_ppo2.py:191][0m |          -0.0171 |          40.3163 |           3.2158 |
[32m[20230205 18:19:58 @agent_ppo2.py:136][0m Policy update time: 0.46 s
[32m[20230205 18:19:59 @agent_ppo2.py:144][0m Average TRAINING episode reward: 20.67
[32m[20230205 18:19:59 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 208.97
[32m[20230205 18:19:59 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 186.26
[32m[20230205 18:19:59 @agent_ppo2.py:149][0m Total time:       6.52 min
[32m[20230205 18:19:59 @agent_ppo2.py:151][0m 407552 total steps have happened
[32m[20230205 18:19:59 @agent_ppo2.py:127][0m #------------------------ Iteration 199 --------------------------#
[32m[20230205 18:19:59 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:19:59 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:19:59 @agent_ppo2.py:191][0m |           0.0179 |          12.3123 |           3.1736 |
[32m[20230205 18:19:59 @agent_ppo2.py:191][0m |          -0.0045 |           9.4979 |           3.1685 |
[32m[20230205 18:19:59 @agent_ppo2.py:191][0m |          -0.0231 |           9.3828 |           3.1682 |
[32m[20230205 18:19:59 @agent_ppo2.py:191][0m |          -0.0183 |           8.3238 |           3.1643 |
[32m[20230205 18:20:00 @agent_ppo2.py:191][0m |          -0.0239 |           7.9447 |           3.1645 |
[32m[20230205 18:20:00 @agent_ppo2.py:191][0m |          -0.0194 |           7.7200 |           3.1638 |
[32m[20230205 18:20:00 @agent_ppo2.py:191][0m |          -0.0123 |           7.4822 |           3.1645 |
[32m[20230205 18:20:00 @agent_ppo2.py:191][0m |          -0.0122 |           7.3052 |           3.1639 |
[32m[20230205 18:20:00 @agent_ppo2.py:191][0m |          -0.0221 |           7.1750 |           3.1633 |
[32m[20230205 18:20:00 @agent_ppo2.py:191][0m |          -0.0130 |           7.0852 |           3.1621 |
[32m[20230205 18:20:00 @agent_ppo2.py:136][0m Policy update time: 0.63 s
[32m[20230205 18:20:00 @agent_ppo2.py:144][0m Average TRAINING episode reward: 244.57
[32m[20230205 18:20:00 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 247.64
[32m[20230205 18:20:00 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 270.62
[32m[20230205 18:20:00 @agent_ppo2.py:149][0m Total time:       6.55 min
[32m[20230205 18:20:00 @agent_ppo2.py:151][0m 409600 total steps have happened
[32m[20230205 18:20:00 @agent_ppo2.py:127][0m #------------------------ Iteration 200 --------------------------#
[32m[20230205 18:20:01 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:20:01 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:01 @agent_ppo2.py:191][0m |          -0.0004 |          11.4487 |           3.1343 |
[32m[20230205 18:20:01 @agent_ppo2.py:191][0m |          -0.0046 |           9.1846 |           3.1306 |
[32m[20230205 18:20:01 @agent_ppo2.py:191][0m |          -0.0090 |           8.7354 |           3.1261 |
[32m[20230205 18:20:01 @agent_ppo2.py:191][0m |          -0.0076 |           8.7052 |           3.1263 |
[32m[20230205 18:20:01 @agent_ppo2.py:191][0m |          -0.0110 |           8.3870 |           3.1259 |
[32m[20230205 18:20:01 @agent_ppo2.py:191][0m |          -0.0135 |           8.2103 |           3.1250 |
[32m[20230205 18:20:01 @agent_ppo2.py:191][0m |          -0.0128 |           8.1333 |           3.1232 |
[32m[20230205 18:20:01 @agent_ppo2.py:191][0m |          -0.0136 |           8.0012 |           3.1225 |
[32m[20230205 18:20:01 @agent_ppo2.py:191][0m |          -0.0132 |           7.9523 |           3.1238 |
[32m[20230205 18:20:02 @agent_ppo2.py:191][0m |          -0.0147 |           7.8384 |           3.1234 |
[32m[20230205 18:20:02 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:20:02 @agent_ppo2.py:144][0m Average TRAINING episode reward: 249.16
[32m[20230205 18:20:02 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.47
[32m[20230205 18:20:02 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 270.71
[32m[20230205 18:20:02 @agent_ppo2.py:149][0m Total time:       6.58 min
[32m[20230205 18:20:02 @agent_ppo2.py:151][0m 411648 total steps have happened
[32m[20230205 18:20:02 @agent_ppo2.py:127][0m #------------------------ Iteration 201 --------------------------#
[32m[20230205 18:20:03 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:20:03 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:03 @agent_ppo2.py:191][0m |          -0.0001 |           9.6552 |           3.1239 |
[32m[20230205 18:20:03 @agent_ppo2.py:191][0m |          -0.0020 |           9.0076 |           3.1173 |
[32m[20230205 18:20:03 @agent_ppo2.py:191][0m |          -0.0080 |           8.7059 |           3.1139 |
[32m[20230205 18:20:03 @agent_ppo2.py:191][0m |          -0.0047 |           8.5577 |           3.1123 |
[32m[20230205 18:20:03 @agent_ppo2.py:191][0m |          -0.0149 |           8.4823 |           3.1119 |
[32m[20230205 18:20:03 @agent_ppo2.py:191][0m |          -0.0130 |           8.3863 |           3.1120 |
[32m[20230205 18:20:03 @agent_ppo2.py:191][0m |          -0.0138 |           8.2850 |           3.1097 |
[32m[20230205 18:20:03 @agent_ppo2.py:191][0m |          -0.0088 |           8.2325 |           3.1096 |
[32m[20230205 18:20:03 @agent_ppo2.py:191][0m |          -0.0113 |           8.1828 |           3.1108 |
[32m[20230205 18:20:03 @agent_ppo2.py:191][0m |          -0.0133 |           8.1708 |           3.1123 |
[32m[20230205 18:20:03 @agent_ppo2.py:136][0m Policy update time: 0.60 s
[32m[20230205 18:20:04 @agent_ppo2.py:144][0m Average TRAINING episode reward: 248.64
[32m[20230205 18:20:04 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 250.30
[32m[20230205 18:20:04 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 271.64
[32m[20230205 18:20:04 @agent_ppo2.py:149][0m Total time:       6.61 min
[32m[20230205 18:20:04 @agent_ppo2.py:151][0m 413696 total steps have happened
[32m[20230205 18:20:04 @agent_ppo2.py:127][0m #------------------------ Iteration 202 --------------------------#
[32m[20230205 18:20:04 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:20:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:05 @agent_ppo2.py:191][0m |           0.0013 |           8.7550 |           3.1028 |
[32m[20230205 18:20:05 @agent_ppo2.py:191][0m |          -0.0043 |           8.4916 |           3.1042 |
[32m[20230205 18:20:05 @agent_ppo2.py:191][0m |          -0.0080 |           8.2918 |           3.1036 |
[32m[20230205 18:20:05 @agent_ppo2.py:191][0m |          -0.0051 |           8.4250 |           3.1047 |
[32m[20230205 18:20:05 @agent_ppo2.py:191][0m |          -0.0098 |           8.1664 |           3.1040 |
[32m[20230205 18:20:05 @agent_ppo2.py:191][0m |          -0.0115 |           8.0056 |           3.1038 |
[32m[20230205 18:20:05 @agent_ppo2.py:191][0m |          -0.0104 |           7.8941 |           3.1013 |
[32m[20230205 18:20:05 @agent_ppo2.py:191][0m |          -0.0132 |           7.8385 |           3.1034 |
[32m[20230205 18:20:05 @agent_ppo2.py:191][0m |          -0.0120 |           7.7791 |           3.1043 |
[32m[20230205 18:20:05 @agent_ppo2.py:191][0m |          -0.0126 |           7.7181 |           3.1045 |
[32m[20230205 18:20:05 @agent_ppo2.py:136][0m Policy update time: 0.63 s
[32m[20230205 18:20:06 @agent_ppo2.py:144][0m Average TRAINING episode reward: 243.01
[32m[20230205 18:20:06 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 247.55
[32m[20230205 18:20:06 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 264.06
[32m[20230205 18:20:06 @agent_ppo2.py:149][0m Total time:       6.64 min
[32m[20230205 18:20:06 @agent_ppo2.py:151][0m 415744 total steps have happened
[32m[20230205 18:20:06 @agent_ppo2.py:127][0m #------------------------ Iteration 203 --------------------------#
[32m[20230205 18:20:06 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:20:06 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:06 @agent_ppo2.py:191][0m |           0.0005 |           8.3270 |           3.1842 |
[32m[20230205 18:20:06 @agent_ppo2.py:191][0m |          -0.0059 |           7.9478 |           3.1775 |
[32m[20230205 18:20:06 @agent_ppo2.py:191][0m |          -0.0117 |           7.6604 |           3.1754 |
[32m[20230205 18:20:07 @agent_ppo2.py:191][0m |          -0.0100 |           7.5954 |           3.1752 |
[32m[20230205 18:20:07 @agent_ppo2.py:191][0m |          -0.0129 |           7.4315 |           3.1725 |
[32m[20230205 18:20:07 @agent_ppo2.py:191][0m |          -0.0141 |           7.3029 |           3.1737 |
[32m[20230205 18:20:07 @agent_ppo2.py:191][0m |          -0.0089 |           7.5905 |           3.1707 |
[32m[20230205 18:20:07 @agent_ppo2.py:191][0m |          -0.0078 |           7.3448 |           3.1686 |
[32m[20230205 18:20:07 @agent_ppo2.py:191][0m |          -0.0113 |           7.0806 |           3.1687 |
[32m[20230205 18:20:07 @agent_ppo2.py:191][0m |          -0.0129 |           7.2992 |           3.1700 |
[32m[20230205 18:20:07 @agent_ppo2.py:136][0m Policy update time: 0.60 s
[32m[20230205 18:20:07 @agent_ppo2.py:144][0m Average TRAINING episode reward: 249.86
[32m[20230205 18:20:07 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 251.54
[32m[20230205 18:20:07 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 148.79
[32m[20230205 18:20:07 @agent_ppo2.py:149][0m Total time:       6.67 min
[32m[20230205 18:20:07 @agent_ppo2.py:151][0m 417792 total steps have happened
[32m[20230205 18:20:07 @agent_ppo2.py:127][0m #------------------------ Iteration 204 --------------------------#
[32m[20230205 18:20:08 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:20:08 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:08 @agent_ppo2.py:191][0m |          -0.0002 |           9.1410 |           3.0972 |
[32m[20230205 18:20:08 @agent_ppo2.py:191][0m |           0.0009 |           9.1182 |           3.0947 |
[32m[20230205 18:20:08 @agent_ppo2.py:191][0m |          -0.0073 |           8.6955 |           3.0961 |
[32m[20230205 18:20:08 @agent_ppo2.py:191][0m |          -0.0062 |           8.7011 |           3.0944 |
[32m[20230205 18:20:08 @agent_ppo2.py:191][0m |          -0.0090 |           8.5259 |           3.0932 |
[32m[20230205 18:20:08 @agent_ppo2.py:191][0m |          -0.0108 |           8.3905 |           3.0948 |
[32m[20230205 18:20:09 @agent_ppo2.py:191][0m |          -0.0098 |           8.3483 |           3.0922 |
[32m[20230205 18:20:09 @agent_ppo2.py:191][0m |          -0.0121 |           8.2406 |           3.0901 |
[32m[20230205 18:20:09 @agent_ppo2.py:191][0m |          -0.0111 |           8.1904 |           3.0918 |
[32m[20230205 18:20:09 @agent_ppo2.py:191][0m |          -0.0137 |           8.1354 |           3.0906 |
[32m[20230205 18:20:09 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:20:09 @agent_ppo2.py:144][0m Average TRAINING episode reward: 249.58
[32m[20230205 18:20:09 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.25
[32m[20230205 18:20:09 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 256.82
[32m[20230205 18:20:09 @agent_ppo2.py:149][0m Total time:       6.70 min
[32m[20230205 18:20:09 @agent_ppo2.py:151][0m 419840 total steps have happened
[32m[20230205 18:20:09 @agent_ppo2.py:127][0m #------------------------ Iteration 205 --------------------------#
[32m[20230205 18:20:10 @agent_ppo2.py:133][0m Sampling time: 0.66 s by 1 slaves
[32m[20230205 18:20:10 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:10 @agent_ppo2.py:191][0m |          -0.0004 |           8.5587 |           3.1578 |
[32m[20230205 18:20:10 @agent_ppo2.py:191][0m |          -0.0058 |           8.2089 |           3.1546 |
[32m[20230205 18:20:10 @agent_ppo2.py:191][0m |          -0.0065 |           8.0542 |           3.1507 |
[32m[20230205 18:20:10 @agent_ppo2.py:191][0m |          -0.0063 |           7.9345 |           3.1524 |
[32m[20230205 18:20:10 @agent_ppo2.py:191][0m |          -0.0072 |           7.8845 |           3.1522 |
[32m[20230205 18:20:10 @agent_ppo2.py:191][0m |          -0.0123 |           7.7873 |           3.1524 |
[32m[20230205 18:20:10 @agent_ppo2.py:191][0m |          -0.0116 |           7.7271 |           3.1518 |
[32m[20230205 18:20:10 @agent_ppo2.py:191][0m |          -0.0095 |           7.7498 |           3.1516 |
[32m[20230205 18:20:11 @agent_ppo2.py:191][0m |          -0.0071 |           8.1049 |           3.1552 |
[32m[20230205 18:20:11 @agent_ppo2.py:191][0m |          -0.0138 |           7.5981 |           3.1547 |
[32m[20230205 18:20:11 @agent_ppo2.py:136][0m Policy update time: 0.65 s
[32m[20230205 18:20:11 @agent_ppo2.py:144][0m Average TRAINING episode reward: 237.26
[32m[20230205 18:20:11 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 237.78
[32m[20230205 18:20:11 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 138.45
[32m[20230205 18:20:11 @agent_ppo2.py:149][0m Total time:       6.73 min
[32m[20230205 18:20:11 @agent_ppo2.py:151][0m 421888 total steps have happened
[32m[20230205 18:20:11 @agent_ppo2.py:127][0m #------------------------ Iteration 206 --------------------------#
[32m[20230205 18:20:12 @agent_ppo2.py:133][0m Sampling time: 0.58 s by 1 slaves
[32m[20230205 18:20:12 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:12 @agent_ppo2.py:191][0m |           0.0008 |          10.0471 |           3.3188 |
[32m[20230205 18:20:12 @agent_ppo2.py:191][0m |          -0.0066 |           9.6484 |           3.3135 |
[32m[20230205 18:20:12 @agent_ppo2.py:191][0m |          -0.0096 |           9.3905 |           3.3091 |
[32m[20230205 18:20:12 @agent_ppo2.py:191][0m |          -0.0112 |           9.1643 |           3.3081 |
[32m[20230205 18:20:12 @agent_ppo2.py:191][0m |          -0.0122 |           8.9469 |           3.3080 |
[32m[20230205 18:20:12 @agent_ppo2.py:191][0m |          -0.0121 |           8.7149 |           3.3098 |
[32m[20230205 18:20:12 @agent_ppo2.py:191][0m |          -0.0131 |           8.5019 |           3.3080 |
[32m[20230205 18:20:12 @agent_ppo2.py:191][0m |          -0.0133 |           8.3221 |           3.3088 |
[32m[20230205 18:20:12 @agent_ppo2.py:191][0m |          -0.0139 |           8.0842 |           3.3081 |
[32m[20230205 18:20:12 @agent_ppo2.py:191][0m |          -0.0145 |           7.8769 |           3.3094 |
[32m[20230205 18:20:12 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:20:13 @agent_ppo2.py:144][0m Average TRAINING episode reward: 250.65
[32m[20230205 18:20:13 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 251.67
[32m[20230205 18:20:13 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 264.76
[32m[20230205 18:20:13 @agent_ppo2.py:149][0m Total time:       6.76 min
[32m[20230205 18:20:13 @agent_ppo2.py:151][0m 423936 total steps have happened
[32m[20230205 18:20:13 @agent_ppo2.py:127][0m #------------------------ Iteration 207 --------------------------#
[32m[20230205 18:20:14 @agent_ppo2.py:133][0m Sampling time: 0.74 s by 1 slaves
[32m[20230205 18:20:14 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:14 @agent_ppo2.py:191][0m |           0.0068 |          19.7509 |           3.1668 |
[32m[20230205 18:20:14 @agent_ppo2.py:191][0m |          -0.0053 |          13.3622 |           3.1605 |
[32m[20230205 18:20:14 @agent_ppo2.py:191][0m |          -0.0060 |          12.5916 |           3.1553 |
[32m[20230205 18:20:14 @agent_ppo2.py:191][0m |          -0.0091 |          11.8710 |           3.1548 |
[32m[20230205 18:20:14 @agent_ppo2.py:191][0m |          -0.0122 |          11.7147 |           3.1510 |
[32m[20230205 18:20:14 @agent_ppo2.py:191][0m |          -0.0099 |          11.4081 |           3.1490 |
[32m[20230205 18:20:14 @agent_ppo2.py:191][0m |          -0.0109 |          11.3569 |           3.1484 |
[32m[20230205 18:20:14 @agent_ppo2.py:191][0m |          -0.0092 |          11.2870 |           3.1457 |
[32m[20230205 18:20:14 @agent_ppo2.py:191][0m |          -0.0124 |          10.8118 |           3.1454 |
[32m[20230205 18:20:14 @agent_ppo2.py:191][0m |          -0.0130 |          10.6475 |           3.1426 |
[32m[20230205 18:20:14 @agent_ppo2.py:136][0m Policy update time: 0.73 s
[32m[20230205 18:20:15 @agent_ppo2.py:144][0m Average TRAINING episode reward: 168.35
[32m[20230205 18:20:15 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.18
[32m[20230205 18:20:15 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 279.15
[32m[20230205 18:20:15 @agent_ppo2.py:149][0m Total time:       6.79 min
[32m[20230205 18:20:15 @agent_ppo2.py:151][0m 425984 total steps have happened
[32m[20230205 18:20:15 @agent_ppo2.py:127][0m #------------------------ Iteration 208 --------------------------#
[32m[20230205 18:20:15 @agent_ppo2.py:133][0m Sampling time: 0.75 s by 1 slaves
[32m[20230205 18:20:16 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:16 @agent_ppo2.py:191][0m |          -0.0010 |          30.5458 |           3.1861 |
[32m[20230205 18:20:16 @agent_ppo2.py:191][0m |          -0.0014 |          22.0634 |           3.1838 |
[32m[20230205 18:20:16 @agent_ppo2.py:191][0m |          -0.0099 |          19.8775 |           3.1782 |
[32m[20230205 18:20:16 @agent_ppo2.py:191][0m |          -0.0047 |          19.2023 |           3.1762 |
[32m[20230205 18:20:16 @agent_ppo2.py:191][0m |          -0.0102 |          18.1500 |           3.1730 |
[32m[20230205 18:20:16 @agent_ppo2.py:191][0m |          -0.0089 |          17.5417 |           3.1714 |
[32m[20230205 18:20:16 @agent_ppo2.py:191][0m |          -0.0123 |          16.5709 |           3.1707 |
[32m[20230205 18:20:16 @agent_ppo2.py:191][0m |          -0.0133 |          15.9191 |           3.1687 |
[32m[20230205 18:20:16 @agent_ppo2.py:191][0m |          -0.0119 |          15.8821 |           3.1660 |
[32m[20230205 18:20:16 @agent_ppo2.py:191][0m |          -0.0116 |          15.3722 |           3.1633 |
[32m[20230205 18:20:16 @agent_ppo2.py:136][0m Policy update time: 0.74 s
[32m[20230205 18:20:17 @agent_ppo2.py:144][0m Average TRAINING episode reward: 108.00
[32m[20230205 18:20:17 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.95
[32m[20230205 18:20:17 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 269.66
[32m[20230205 18:20:17 @agent_ppo2.py:149][0m Total time:       6.82 min
[32m[20230205 18:20:17 @agent_ppo2.py:151][0m 428032 total steps have happened
[32m[20230205 18:20:17 @agent_ppo2.py:127][0m #------------------------ Iteration 209 --------------------------#
[32m[20230205 18:20:18 @agent_ppo2.py:133][0m Sampling time: 0.79 s by 1 slaves
[32m[20230205 18:20:18 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:18 @agent_ppo2.py:191][0m |           0.0025 |          58.0568 |           3.1583 |
[32m[20230205 18:20:18 @agent_ppo2.py:191][0m |          -0.0066 |          41.5821 |           3.1533 |
[32m[20230205 18:20:18 @agent_ppo2.py:191][0m |           0.0164 |          41.8791 |           3.1558 |
[32m[20230205 18:20:18 @agent_ppo2.py:191][0m |           0.0112 |          40.0450 |           3.1486 |
[32m[20230205 18:20:18 @agent_ppo2.py:191][0m |          -0.0059 |          31.2770 |           3.1560 |
[32m[20230205 18:20:18 @agent_ppo2.py:191][0m |          -0.0110 |          29.6325 |           3.1581 |
[32m[20230205 18:20:18 @agent_ppo2.py:191][0m |          -0.0016 |          32.3580 |           3.1588 |
[32m[20230205 18:20:18 @agent_ppo2.py:191][0m |          -0.0099 |          27.4345 |           3.1609 |
[32m[20230205 18:20:18 @agent_ppo2.py:191][0m |          -0.0148 |          26.4619 |           3.1628 |
[32m[20230205 18:20:18 @agent_ppo2.py:191][0m |          -0.0121 |          26.0415 |           3.1653 |
[32m[20230205 18:20:18 @agent_ppo2.py:136][0m Policy update time: 0.77 s
[32m[20230205 18:20:19 @agent_ppo2.py:144][0m Average TRAINING episode reward: 104.94
[32m[20230205 18:20:19 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.22
[32m[20230205 18:20:19 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.19
[32m[20230205 18:20:19 @agent_ppo2.py:149][0m Total time:       6.86 min
[32m[20230205 18:20:19 @agent_ppo2.py:151][0m 430080 total steps have happened
[32m[20230205 18:20:19 @agent_ppo2.py:127][0m #------------------------ Iteration 210 --------------------------#
[32m[20230205 18:20:20 @agent_ppo2.py:133][0m Sampling time: 0.75 s by 1 slaves
[32m[20230205 18:20:20 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:20 @agent_ppo2.py:191][0m |          -0.0021 |          39.7622 |           3.1774 |
[32m[20230205 18:20:20 @agent_ppo2.py:191][0m |          -0.0014 |          37.7404 |           3.1762 |
[32m[20230205 18:20:20 @agent_ppo2.py:191][0m |          -0.0092 |          34.1137 |           3.1733 |
[32m[20230205 18:20:20 @agent_ppo2.py:191][0m |          -0.0082 |          32.1112 |           3.1720 |
[32m[20230205 18:20:20 @agent_ppo2.py:191][0m |          -0.0062 |          31.9565 |           3.1721 |
[32m[20230205 18:20:20 @agent_ppo2.py:191][0m |          -0.0126 |          29.4857 |           3.1683 |
[32m[20230205 18:20:20 @agent_ppo2.py:191][0m |          -0.0121 |          28.2099 |           3.1667 |
[32m[20230205 18:20:20 @agent_ppo2.py:191][0m |          -0.0127 |          27.1259 |           3.1698 |
[32m[20230205 18:20:20 @agent_ppo2.py:191][0m |          -0.0157 |          26.4832 |           3.1700 |
[32m[20230205 18:20:20 @agent_ppo2.py:191][0m |          -0.0124 |          25.5909 |           3.1670 |
[32m[20230205 18:20:20 @agent_ppo2.py:136][0m Policy update time: 0.75 s
[32m[20230205 18:20:21 @agent_ppo2.py:144][0m Average TRAINING episode reward: 162.60
[32m[20230205 18:20:21 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 249.96
[32m[20230205 18:20:21 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.01
[32m[20230205 18:20:21 @agent_ppo2.py:149][0m Total time:       6.89 min
[32m[20230205 18:20:21 @agent_ppo2.py:151][0m 432128 total steps have happened
[32m[20230205 18:20:21 @agent_ppo2.py:127][0m #------------------------ Iteration 211 --------------------------#
[32m[20230205 18:20:22 @agent_ppo2.py:133][0m Sampling time: 0.70 s by 1 slaves
[32m[20230205 18:20:22 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:22 @agent_ppo2.py:191][0m |           0.0015 |          32.6372 |           3.2125 |
[32m[20230205 18:20:22 @agent_ppo2.py:191][0m |          -0.0044 |          29.0987 |           3.2026 |
[32m[20230205 18:20:22 @agent_ppo2.py:191][0m |          -0.0056 |          27.8429 |           3.2001 |
[32m[20230205 18:20:22 @agent_ppo2.py:191][0m |          -0.0069 |          26.3189 |           3.2002 |
[32m[20230205 18:20:22 @agent_ppo2.py:191][0m |          -0.0100 |          24.6938 |           3.1992 |
[32m[20230205 18:20:22 @agent_ppo2.py:191][0m |          -0.0097 |          23.9805 |           3.1990 |
[32m[20230205 18:20:22 @agent_ppo2.py:191][0m |          -0.0106 |          23.4903 |           3.1959 |
[32m[20230205 18:20:22 @agent_ppo2.py:191][0m |          -0.0132 |          22.8903 |           3.1938 |
[32m[20230205 18:20:22 @agent_ppo2.py:191][0m |          -0.0101 |          23.1463 |           3.1934 |
[32m[20230205 18:20:22 @agent_ppo2.py:191][0m |          -0.0123 |          21.9970 |           3.1958 |
[32m[20230205 18:20:22 @agent_ppo2.py:136][0m Policy update time: 0.68 s
[32m[20230205 18:20:23 @agent_ppo2.py:144][0m Average TRAINING episode reward: 153.91
[32m[20230205 18:20:23 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.10
[32m[20230205 18:20:23 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 271.99
[32m[20230205 18:20:23 @agent_ppo2.py:149][0m Total time:       6.92 min
[32m[20230205 18:20:23 @agent_ppo2.py:151][0m 434176 total steps have happened
[32m[20230205 18:20:23 @agent_ppo2.py:127][0m #------------------------ Iteration 212 --------------------------#
[32m[20230205 18:20:23 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:20:23 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:23 @agent_ppo2.py:191][0m |          -0.0012 |          23.5892 |           3.1183 |
[32m[20230205 18:20:24 @agent_ppo2.py:191][0m |          -0.0476 |          12.2074 |           3.1142 |
[32m[20230205 18:20:24 @agent_ppo2.py:191][0m |          -0.0212 |          10.8138 |           3.1147 |
[32m[20230205 18:20:24 @agent_ppo2.py:191][0m |          -0.0007 |          10.1102 |           3.1075 |
[32m[20230205 18:20:24 @agent_ppo2.py:191][0m |          -0.0164 |           9.5315 |           3.1088 |
[32m[20230205 18:20:24 @agent_ppo2.py:191][0m |          -0.0107 |           9.1868 |           3.1095 |
[32m[20230205 18:20:24 @agent_ppo2.py:191][0m |          -0.0154 |           8.8155 |           3.1076 |
[32m[20230205 18:20:24 @agent_ppo2.py:191][0m |          -0.0255 |           8.7730 |           3.1080 |
[32m[20230205 18:20:24 @agent_ppo2.py:191][0m |          -0.0176 |           8.4277 |           3.1085 |
[32m[20230205 18:20:24 @agent_ppo2.py:191][0m |          -0.0313 |           8.3394 |           3.1088 |
[32m[20230205 18:20:24 @agent_ppo2.py:136][0m Policy update time: 0.63 s
[32m[20230205 18:20:25 @agent_ppo2.py:144][0m Average TRAINING episode reward: 183.79
[32m[20230205 18:20:25 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 239.22
[32m[20230205 18:20:25 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 193.58
[32m[20230205 18:20:25 @agent_ppo2.py:149][0m Total time:       6.96 min
[32m[20230205 18:20:25 @agent_ppo2.py:151][0m 436224 total steps have happened
[32m[20230205 18:20:25 @agent_ppo2.py:127][0m #------------------------ Iteration 213 --------------------------#
[32m[20230205 18:20:25 @agent_ppo2.py:133][0m Sampling time: 0.64 s by 1 slaves
[32m[20230205 18:20:25 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:25 @agent_ppo2.py:191][0m |          -0.0015 |           8.2470 |           3.0999 |
[32m[20230205 18:20:25 @agent_ppo2.py:191][0m |          -0.0000 |           7.9114 |           3.1004 |
[32m[20230205 18:20:26 @agent_ppo2.py:191][0m |          -0.0053 |           7.7143 |           3.0988 |
[32m[20230205 18:20:26 @agent_ppo2.py:191][0m |          -0.0083 |           7.5538 |           3.0996 |
[32m[20230205 18:20:26 @agent_ppo2.py:191][0m |           0.0013 |           8.0125 |           3.0970 |
[32m[20230205 18:20:26 @agent_ppo2.py:191][0m |          -0.0101 |           7.3504 |           3.0969 |
[32m[20230205 18:20:26 @agent_ppo2.py:191][0m |          -0.0119 |           7.2857 |           3.0960 |
[32m[20230205 18:20:26 @agent_ppo2.py:191][0m |          -0.0132 |           7.2403 |           3.0969 |
[32m[20230205 18:20:26 @agent_ppo2.py:191][0m |          -0.0138 |           7.1809 |           3.0971 |
[32m[20230205 18:20:26 @agent_ppo2.py:191][0m |          -0.0110 |           7.1211 |           3.0971 |
[32m[20230205 18:20:26 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:20:26 @agent_ppo2.py:144][0m Average TRAINING episode reward: 240.56
[32m[20230205 18:20:26 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 241.00
[32m[20230205 18:20:26 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 274.52
[32m[20230205 18:20:26 @agent_ppo2.py:149][0m Total time:       6.98 min
[32m[20230205 18:20:26 @agent_ppo2.py:151][0m 438272 total steps have happened
[32m[20230205 18:20:26 @agent_ppo2.py:127][0m #------------------------ Iteration 214 --------------------------#
[32m[20230205 18:20:27 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:20:27 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:27 @agent_ppo2.py:191][0m |           0.0017 |           8.4455 |           3.1518 |
[32m[20230205 18:20:27 @agent_ppo2.py:191][0m |          -0.0075 |           8.1280 |           3.1462 |
[32m[20230205 18:20:27 @agent_ppo2.py:191][0m |          -0.0072 |           7.9856 |           3.1441 |
[32m[20230205 18:20:27 @agent_ppo2.py:191][0m |          -0.0063 |           7.8692 |           3.1408 |
[32m[20230205 18:20:27 @agent_ppo2.py:191][0m |          -0.0079 |           7.7969 |           3.1396 |
[32m[20230205 18:20:27 @agent_ppo2.py:191][0m |          -0.0113 |           7.6796 |           3.1399 |
[32m[20230205 18:20:27 @agent_ppo2.py:191][0m |          -0.0110 |           7.6013 |           3.1381 |
[32m[20230205 18:20:28 @agent_ppo2.py:191][0m |          -0.0116 |           7.5454 |           3.1355 |
[32m[20230205 18:20:28 @agent_ppo2.py:191][0m |          -0.0125 |           7.4853 |           3.1352 |
[32m[20230205 18:20:28 @agent_ppo2.py:191][0m |          -0.0135 |           7.4369 |           3.1346 |
[32m[20230205 18:20:28 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:20:28 @agent_ppo2.py:144][0m Average TRAINING episode reward: 251.14
[32m[20230205 18:20:28 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 251.27
[32m[20230205 18:20:28 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.65
[32m[20230205 18:20:28 @agent_ppo2.py:149][0m Total time:       7.01 min
[32m[20230205 18:20:28 @agent_ppo2.py:151][0m 440320 total steps have happened
[32m[20230205 18:20:28 @agent_ppo2.py:127][0m #------------------------ Iteration 215 --------------------------#
[32m[20230205 18:20:29 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:20:29 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:29 @agent_ppo2.py:191][0m |          -0.0029 |          26.9763 |           3.1643 |
[32m[20230205 18:20:29 @agent_ppo2.py:191][0m |          -0.0077 |          13.3878 |           3.1578 |
[32m[20230205 18:20:29 @agent_ppo2.py:191][0m |           0.0047 |           9.6780 |           3.1536 |
[32m[20230205 18:20:29 @agent_ppo2.py:191][0m |          -0.0117 |           8.6257 |           3.1533 |
[32m[20230205 18:20:29 @agent_ppo2.py:191][0m |          -0.0134 |           7.9602 |           3.1544 |
[32m[20230205 18:20:29 @agent_ppo2.py:191][0m |          -0.0128 |           7.6333 |           3.1547 |
[32m[20230205 18:20:29 @agent_ppo2.py:191][0m |          -0.0137 |           7.5577 |           3.1530 |
[32m[20230205 18:20:29 @agent_ppo2.py:191][0m |          -0.0104 |           7.3289 |           3.1511 |
[32m[20230205 18:20:29 @agent_ppo2.py:191][0m |          -0.0189 |           7.0514 |           3.1494 |
[32m[20230205 18:20:29 @agent_ppo2.py:191][0m |          -0.0137 |           6.9172 |           3.1492 |
[32m[20230205 18:20:29 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:20:30 @agent_ppo2.py:144][0m Average TRAINING episode reward: 167.62
[32m[20230205 18:20:30 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.41
[32m[20230205 18:20:30 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 258.08
[32m[20230205 18:20:30 @agent_ppo2.py:149][0m Total time:       7.04 min
[32m[20230205 18:20:30 @agent_ppo2.py:151][0m 442368 total steps have happened
[32m[20230205 18:20:30 @agent_ppo2.py:127][0m #------------------------ Iteration 216 --------------------------#
[32m[20230205 18:20:30 @agent_ppo2.py:133][0m Sampling time: 0.62 s by 1 slaves
[32m[20230205 18:20:30 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:30 @agent_ppo2.py:191][0m |           0.0021 |          15.4898 |           3.1195 |
[32m[20230205 18:20:31 @agent_ppo2.py:191][0m |           0.0013 |          12.2357 |           3.1161 |
[32m[20230205 18:20:31 @agent_ppo2.py:191][0m |          -0.0020 |          11.3153 |           3.1143 |
[32m[20230205 18:20:31 @agent_ppo2.py:191][0m |          -0.0114 |          10.8232 |           3.1117 |
[32m[20230205 18:20:31 @agent_ppo2.py:191][0m |          -0.0140 |          10.5410 |           3.1124 |
[32m[20230205 18:20:31 @agent_ppo2.py:191][0m |          -0.0066 |          10.3676 |           3.1133 |
[32m[20230205 18:20:31 @agent_ppo2.py:191][0m |          -0.0098 |          10.1758 |           3.1117 |
[32m[20230205 18:20:31 @agent_ppo2.py:191][0m |          -0.0129 |           9.9675 |           3.1127 |
[32m[20230205 18:20:31 @agent_ppo2.py:191][0m |          -0.0097 |          10.0515 |           3.1127 |
[32m[20230205 18:20:31 @agent_ppo2.py:191][0m |          -0.0079 |           9.8452 |           3.1114 |
[32m[20230205 18:20:31 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:20:32 @agent_ppo2.py:144][0m Average TRAINING episode reward: 244.07
[32m[20230205 18:20:32 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 244.93
[32m[20230205 18:20:32 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 262.43
[32m[20230205 18:20:32 @agent_ppo2.py:149][0m Total time:       7.07 min
[32m[20230205 18:20:32 @agent_ppo2.py:151][0m 444416 total steps have happened
[32m[20230205 18:20:32 @agent_ppo2.py:127][0m #------------------------ Iteration 217 --------------------------#
[32m[20230205 18:20:32 @agent_ppo2.py:133][0m Sampling time: 0.66 s by 1 slaves
[32m[20230205 18:20:32 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:32 @agent_ppo2.py:191][0m |          -0.0023 |          12.8101 |           3.1962 |
[32m[20230205 18:20:32 @agent_ppo2.py:191][0m |          -0.0077 |           9.4328 |           3.1905 |
[32m[20230205 18:20:32 @agent_ppo2.py:191][0m |          -0.0102 |           8.8234 |           3.1906 |
[32m[20230205 18:20:32 @agent_ppo2.py:191][0m |          -0.0111 |           8.4455 |           3.1890 |
[32m[20230205 18:20:33 @agent_ppo2.py:191][0m |          -0.0118 |           8.2634 |           3.1889 |
[32m[20230205 18:20:33 @agent_ppo2.py:191][0m |          -0.0131 |           8.1040 |           3.1887 |
[32m[20230205 18:20:33 @agent_ppo2.py:191][0m |          -0.0121 |           7.9449 |           3.1883 |
[32m[20230205 18:20:33 @agent_ppo2.py:191][0m |          -0.0137 |           7.8588 |           3.1891 |
[32m[20230205 18:20:33 @agent_ppo2.py:191][0m |          -0.0133 |           7.8787 |           3.1901 |
[32m[20230205 18:20:33 @agent_ppo2.py:191][0m |          -0.0139 |           7.8059 |           3.1909 |
[32m[20230205 18:20:33 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:20:33 @agent_ppo2.py:144][0m Average TRAINING episode reward: 238.40
[32m[20230205 18:20:33 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 240.12
[32m[20230205 18:20:33 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 173.29
[32m[20230205 18:20:33 @agent_ppo2.py:149][0m Total time:       7.10 min
[32m[20230205 18:20:33 @agent_ppo2.py:151][0m 446464 total steps have happened
[32m[20230205 18:20:33 @agent_ppo2.py:127][0m #------------------------ Iteration 218 --------------------------#
[32m[20230205 18:20:34 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:20:34 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:34 @agent_ppo2.py:191][0m |           0.0011 |           8.3204 |           3.2098 |
[32m[20230205 18:20:34 @agent_ppo2.py:191][0m |          -0.0043 |           7.9447 |           3.2114 |
[32m[20230205 18:20:34 @agent_ppo2.py:191][0m |          -0.0039 |           7.7231 |           3.2132 |
[32m[20230205 18:20:34 @agent_ppo2.py:191][0m |          -0.0045 |           7.5749 |           3.2151 |
[32m[20230205 18:20:34 @agent_ppo2.py:191][0m |          -0.0061 |           7.5562 |           3.2156 |
[32m[20230205 18:20:34 @agent_ppo2.py:191][0m |          -0.0077 |           7.3803 |           3.2176 |
[32m[20230205 18:20:34 @agent_ppo2.py:191][0m |          -0.0082 |           7.3129 |           3.2174 |
[32m[20230205 18:20:35 @agent_ppo2.py:191][0m |          -0.0089 |           7.2419 |           3.2199 |
[32m[20230205 18:20:35 @agent_ppo2.py:191][0m |          -0.0097 |           7.1875 |           3.2206 |
[32m[20230205 18:20:35 @agent_ppo2.py:191][0m |          -0.0104 |           7.1018 |           3.2207 |
[32m[20230205 18:20:35 @agent_ppo2.py:136][0m Policy update time: 0.60 s
[32m[20230205 18:20:35 @agent_ppo2.py:144][0m Average TRAINING episode reward: 246.64
[32m[20230205 18:20:35 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.47
[32m[20230205 18:20:35 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 256.97
[32m[20230205 18:20:35 @agent_ppo2.py:149][0m Total time:       7.13 min
[32m[20230205 18:20:35 @agent_ppo2.py:151][0m 448512 total steps have happened
[32m[20230205 18:20:35 @agent_ppo2.py:127][0m #------------------------ Iteration 219 --------------------------#
[32m[20230205 18:20:36 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:20:36 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:36 @agent_ppo2.py:191][0m |          -0.0016 |           8.1864 |           3.1761 |
[32m[20230205 18:20:36 @agent_ppo2.py:191][0m |          -0.0116 |           7.7092 |           3.1737 |
[32m[20230205 18:20:36 @agent_ppo2.py:191][0m |          -0.0106 |           7.4637 |           3.1712 |
[32m[20230205 18:20:36 @agent_ppo2.py:191][0m |          -0.0112 |           7.4246 |           3.1735 |
[32m[20230205 18:20:36 @agent_ppo2.py:191][0m |          -0.0106 |           7.1950 |           3.1712 |
[32m[20230205 18:20:36 @agent_ppo2.py:191][0m |          -0.0080 |           7.2243 |           3.1703 |
[32m[20230205 18:20:36 @agent_ppo2.py:191][0m |          -0.0123 |           6.9661 |           3.1688 |
[32m[20230205 18:20:36 @agent_ppo2.py:191][0m |          -0.0202 |           6.8839 |           3.1676 |
[32m[20230205 18:20:36 @agent_ppo2.py:191][0m |          -0.0092 |           6.8550 |           3.1672 |
[32m[20230205 18:20:36 @agent_ppo2.py:191][0m |          -0.0159 |           6.7234 |           3.1672 |
[32m[20230205 18:20:36 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:20:37 @agent_ppo2.py:144][0m Average TRAINING episode reward: 247.18
[32m[20230205 18:20:37 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 247.48
[32m[20230205 18:20:37 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 178.24
[32m[20230205 18:20:37 @agent_ppo2.py:149][0m Total time:       7.16 min
[32m[20230205 18:20:37 @agent_ppo2.py:151][0m 450560 total steps have happened
[32m[20230205 18:20:37 @agent_ppo2.py:127][0m #------------------------ Iteration 220 --------------------------#
[32m[20230205 18:20:37 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:20:38 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:38 @agent_ppo2.py:191][0m |          -0.0024 |          25.5991 |           3.2605 |
[32m[20230205 18:20:38 @agent_ppo2.py:191][0m |          -0.0100 |          18.6750 |           3.2548 |
[32m[20230205 18:20:38 @agent_ppo2.py:191][0m |          -0.0064 |          17.4174 |           3.2511 |
[32m[20230205 18:20:38 @agent_ppo2.py:191][0m |          -0.0127 |          15.1784 |           3.2454 |
[32m[20230205 18:20:38 @agent_ppo2.py:191][0m |          -0.0148 |          14.1039 |           3.2444 |
[32m[20230205 18:20:38 @agent_ppo2.py:191][0m |          -0.0096 |          13.4238 |           3.2436 |
[32m[20230205 18:20:38 @agent_ppo2.py:191][0m |          -0.0161 |          12.9176 |           3.2424 |
[32m[20230205 18:20:38 @agent_ppo2.py:191][0m |          -0.0163 |          12.7351 |           3.2438 |
[32m[20230205 18:20:38 @agent_ppo2.py:191][0m |          -0.0194 |          12.3030 |           3.2391 |
[32m[20230205 18:20:38 @agent_ppo2.py:191][0m |          -0.0029 |          13.3314 |           3.2383 |
[32m[20230205 18:20:38 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:20:39 @agent_ppo2.py:144][0m Average TRAINING episode reward: 169.90
[32m[20230205 18:20:39 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 244.78
[32m[20230205 18:20:39 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 265.89
[32m[20230205 18:20:39 @agent_ppo2.py:149][0m Total time:       7.19 min
[32m[20230205 18:20:39 @agent_ppo2.py:151][0m 452608 total steps have happened
[32m[20230205 18:20:39 @agent_ppo2.py:127][0m #------------------------ Iteration 221 --------------------------#
[32m[20230205 18:20:39 @agent_ppo2.py:133][0m Sampling time: 0.64 s by 1 slaves
[32m[20230205 18:20:39 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:39 @agent_ppo2.py:191][0m |           0.0001 |           9.2172 |           3.2443 |
[32m[20230205 18:20:39 @agent_ppo2.py:191][0m |          -0.0041 |           8.7106 |           3.2400 |
[32m[20230205 18:20:39 @agent_ppo2.py:191][0m |          -0.0055 |           8.5679 |           3.2405 |
[32m[20230205 18:20:39 @agent_ppo2.py:191][0m |          -0.0091 |           8.3571 |           3.2384 |
[32m[20230205 18:20:40 @agent_ppo2.py:191][0m |          -0.0078 |           8.3453 |           3.2392 |
[32m[20230205 18:20:40 @agent_ppo2.py:191][0m |          -0.0102 |           8.1602 |           3.2381 |
[32m[20230205 18:20:40 @agent_ppo2.py:191][0m |          -0.0112 |           8.0941 |           3.2393 |
[32m[20230205 18:20:40 @agent_ppo2.py:191][0m |          -0.0108 |           8.1346 |           3.2366 |
[32m[20230205 18:20:40 @agent_ppo2.py:191][0m |          -0.0101 |           8.1192 |           3.2344 |
[32m[20230205 18:20:40 @agent_ppo2.py:191][0m |          -0.0113 |           7.9449 |           3.2376 |
[32m[20230205 18:20:40 @agent_ppo2.py:136][0m Policy update time: 0.63 s
[32m[20230205 18:20:40 @agent_ppo2.py:144][0m Average TRAINING episode reward: 243.78
[32m[20230205 18:20:40 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 245.82
[32m[20230205 18:20:40 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 266.18
[32m[20230205 18:20:40 @agent_ppo2.py:149][0m Total time:       7.22 min
[32m[20230205 18:20:40 @agent_ppo2.py:151][0m 454656 total steps have happened
[32m[20230205 18:20:40 @agent_ppo2.py:127][0m #------------------------ Iteration 222 --------------------------#
[32m[20230205 18:20:41 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:20:41 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:41 @agent_ppo2.py:191][0m |          -0.0025 |           7.8524 |           3.2722 |
[32m[20230205 18:20:41 @agent_ppo2.py:191][0m |          -0.0061 |           7.6174 |           3.2588 |
[32m[20230205 18:20:41 @agent_ppo2.py:191][0m |          -0.0084 |           7.3127 |           3.2551 |
[32m[20230205 18:20:41 @agent_ppo2.py:191][0m |          -0.0086 |           7.2909 |           3.2499 |
[32m[20230205 18:20:41 @agent_ppo2.py:191][0m |          -0.0109 |           7.2433 |           3.2528 |
[32m[20230205 18:20:41 @agent_ppo2.py:191][0m |          -0.0145 |           6.8902 |           3.2477 |
[32m[20230205 18:20:41 @agent_ppo2.py:191][0m |          -0.0126 |           6.8420 |           3.2484 |
[32m[20230205 18:20:41 @agent_ppo2.py:191][0m |          -0.0145 |           6.7324 |           3.2459 |
[32m[20230205 18:20:41 @agent_ppo2.py:191][0m |          -0.0126 |           6.7455 |           3.2471 |
[32m[20230205 18:20:42 @agent_ppo2.py:191][0m |          -0.0141 |           6.7023 |           3.2472 |
[32m[20230205 18:20:42 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:20:42 @agent_ppo2.py:144][0m Average TRAINING episode reward: 249.43
[32m[20230205 18:20:42 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 250.81
[32m[20230205 18:20:42 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 269.44
[32m[20230205 18:20:42 @agent_ppo2.py:149][0m Total time:       7.25 min
[32m[20230205 18:20:42 @agent_ppo2.py:151][0m 456704 total steps have happened
[32m[20230205 18:20:42 @agent_ppo2.py:127][0m #------------------------ Iteration 223 --------------------------#
[32m[20230205 18:20:43 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:20:43 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:43 @agent_ppo2.py:191][0m |           0.0011 |           7.7762 |           3.2883 |
[32m[20230205 18:20:43 @agent_ppo2.py:191][0m |          -0.0047 |           7.3192 |           3.2790 |
[32m[20230205 18:20:43 @agent_ppo2.py:191][0m |          -0.0036 |           7.1903 |           3.2717 |
[32m[20230205 18:20:43 @agent_ppo2.py:191][0m |          -0.0110 |           6.8813 |           3.2680 |
[32m[20230205 18:20:43 @agent_ppo2.py:191][0m |          -0.0126 |           6.7465 |           3.2662 |
[32m[20230205 18:20:43 @agent_ppo2.py:191][0m |          -0.0118 |           6.6420 |           3.2651 |
[32m[20230205 18:20:43 @agent_ppo2.py:191][0m |          -0.0060 |           6.8644 |           3.2619 |
[32m[20230205 18:20:43 @agent_ppo2.py:191][0m |          -0.0139 |           6.4951 |           3.2606 |
[32m[20230205 18:20:43 @agent_ppo2.py:191][0m |          -0.0144 |           6.4438 |           3.2589 |
[32m[20230205 18:20:43 @agent_ppo2.py:191][0m |          -0.0141 |           6.3797 |           3.2568 |
[32m[20230205 18:20:43 @agent_ppo2.py:136][0m Policy update time: 0.60 s
[32m[20230205 18:20:44 @agent_ppo2.py:144][0m Average TRAINING episode reward: 250.50
[32m[20230205 18:20:44 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.79
[32m[20230205 18:20:44 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.50
[32m[20230205 18:20:44 @agent_ppo2.py:149][0m Total time:       7.27 min
[32m[20230205 18:20:44 @agent_ppo2.py:151][0m 458752 total steps have happened
[32m[20230205 18:20:44 @agent_ppo2.py:127][0m #------------------------ Iteration 224 --------------------------#
[32m[20230205 18:20:44 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:20:44 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:45 @agent_ppo2.py:191][0m |           0.0014 |           8.2861 |           3.2368 |
[32m[20230205 18:20:45 @agent_ppo2.py:191][0m |          -0.0020 |           7.6711 |           3.2363 |
[32m[20230205 18:20:45 @agent_ppo2.py:191][0m |          -0.0046 |           7.4978 |           3.2333 |
[32m[20230205 18:20:45 @agent_ppo2.py:191][0m |          -0.0049 |           7.2585 |           3.2334 |
[32m[20230205 18:20:45 @agent_ppo2.py:191][0m |          -0.0078 |           7.1436 |           3.2332 |
[32m[20230205 18:20:45 @agent_ppo2.py:191][0m |          -0.0053 |           7.2081 |           3.2299 |
[32m[20230205 18:20:45 @agent_ppo2.py:191][0m |          -0.0100 |           6.9572 |           3.2279 |
[32m[20230205 18:20:45 @agent_ppo2.py:191][0m |          -0.0085 |           6.9080 |           3.2286 |
[32m[20230205 18:20:45 @agent_ppo2.py:191][0m |          -0.0085 |           6.8506 |           3.2277 |
[32m[20230205 18:20:45 @agent_ppo2.py:191][0m |          -0.0109 |           6.7591 |           3.2265 |
[32m[20230205 18:20:45 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:20:46 @agent_ppo2.py:144][0m Average TRAINING episode reward: 246.53
[32m[20230205 18:20:46 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 248.84
[32m[20230205 18:20:46 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.97
[32m[20230205 18:20:46 @agent_ppo2.py:149][0m Total time:       7.30 min
[32m[20230205 18:20:46 @agent_ppo2.py:151][0m 460800 total steps have happened
[32m[20230205 18:20:46 @agent_ppo2.py:127][0m #------------------------ Iteration 225 --------------------------#
[32m[20230205 18:20:46 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:20:46 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:46 @agent_ppo2.py:191][0m |           0.0011 |           8.7706 |           3.2233 |
[32m[20230205 18:20:46 @agent_ppo2.py:191][0m |          -0.0059 |           8.1366 |           3.2161 |
[32m[20230205 18:20:46 @agent_ppo2.py:191][0m |          -0.0081 |           7.8431 |           3.2099 |
[32m[20230205 18:20:46 @agent_ppo2.py:191][0m |          -0.0088 |           7.6146 |           3.2062 |
[32m[20230205 18:20:46 @agent_ppo2.py:191][0m |          -0.0105 |           7.4141 |           3.2053 |
[32m[20230205 18:20:47 @agent_ppo2.py:191][0m |          -0.0109 |           7.2441 |           3.2042 |
[32m[20230205 18:20:47 @agent_ppo2.py:191][0m |          -0.0126 |           7.0721 |           3.2021 |
[32m[20230205 18:20:47 @agent_ppo2.py:191][0m |          -0.0122 |           6.9558 |           3.1998 |
[32m[20230205 18:20:47 @agent_ppo2.py:191][0m |          -0.0125 |           6.8403 |           3.2003 |
[32m[20230205 18:20:47 @agent_ppo2.py:191][0m |          -0.0127 |           6.7581 |           3.1960 |
[32m[20230205 18:20:47 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:20:47 @agent_ppo2.py:144][0m Average TRAINING episode reward: 251.91
[32m[20230205 18:20:47 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.27
[32m[20230205 18:20:47 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.66
[32m[20230205 18:20:47 @agent_ppo2.py:149][0m Total time:       7.33 min
[32m[20230205 18:20:47 @agent_ppo2.py:151][0m 462848 total steps have happened
[32m[20230205 18:20:47 @agent_ppo2.py:127][0m #------------------------ Iteration 226 --------------------------#
[32m[20230205 18:20:48 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:20:48 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:48 @agent_ppo2.py:191][0m |           0.0017 |          24.3998 |           3.2720 |
[32m[20230205 18:20:48 @agent_ppo2.py:191][0m |          -0.0027 |          15.0232 |           3.2712 |
[32m[20230205 18:20:48 @agent_ppo2.py:191][0m |          -0.0060 |          11.0407 |           3.2704 |
[32m[20230205 18:20:48 @agent_ppo2.py:191][0m |          -0.0085 |           9.8867 |           3.2702 |
[32m[20230205 18:20:48 @agent_ppo2.py:191][0m |          -0.0089 |           9.0579 |           3.2723 |
[32m[20230205 18:20:48 @agent_ppo2.py:191][0m |          -0.0108 |           8.4546 |           3.2668 |
[32m[20230205 18:20:48 @agent_ppo2.py:191][0m |          -0.0113 |           8.0120 |           3.2684 |
[32m[20230205 18:20:49 @agent_ppo2.py:191][0m |          -0.0104 |           7.6436 |           3.2654 |
[32m[20230205 18:20:49 @agent_ppo2.py:191][0m |          -0.0114 |           7.3985 |           3.2665 |
[32m[20230205 18:20:49 @agent_ppo2.py:191][0m |          -0.0120 |           7.0235 |           3.2667 |
[32m[20230205 18:20:49 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:20:49 @agent_ppo2.py:144][0m Average TRAINING episode reward: 120.77
[32m[20230205 18:20:49 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 243.73
[32m[20230205 18:20:49 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 190.62
[32m[20230205 18:20:49 @agent_ppo2.py:149][0m Total time:       7.37 min
[32m[20230205 18:20:49 @agent_ppo2.py:151][0m 464896 total steps have happened
[32m[20230205 18:20:49 @agent_ppo2.py:127][0m #------------------------ Iteration 227 --------------------------#
[32m[20230205 18:20:50 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:20:50 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:50 @agent_ppo2.py:191][0m |          -0.0022 |           8.3454 |           3.1981 |
[32m[20230205 18:20:50 @agent_ppo2.py:191][0m |          -0.0171 |           8.0862 |           3.1926 |
[32m[20230205 18:20:50 @agent_ppo2.py:191][0m |           0.0151 |           9.6336 |           3.1891 |
[32m[20230205 18:20:50 @agent_ppo2.py:191][0m |           0.0071 |           8.6584 |           3.1828 |
[32m[20230205 18:20:50 @agent_ppo2.py:191][0m |          -0.0200 |           7.5974 |           3.1832 |
[32m[20230205 18:20:50 @agent_ppo2.py:191][0m |          -0.0110 |           7.4337 |           3.1827 |
[32m[20230205 18:20:50 @agent_ppo2.py:191][0m |          -0.0203 |           7.2116 |           3.1805 |
[32m[20230205 18:20:50 @agent_ppo2.py:191][0m |          -0.0181 |           7.0783 |           3.1784 |
[32m[20230205 18:20:50 @agent_ppo2.py:191][0m |          -0.0209 |           6.9247 |           3.1763 |
[32m[20230205 18:20:51 @agent_ppo2.py:191][0m |          -0.0235 |           6.7861 |           3.1764 |
[32m[20230205 18:20:51 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:20:51 @agent_ppo2.py:144][0m Average TRAINING episode reward: 249.30
[32m[20230205 18:20:51 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 249.90
[32m[20230205 18:20:51 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 270.11
[32m[20230205 18:20:51 @agent_ppo2.py:149][0m Total time:       7.39 min
[32m[20230205 18:20:51 @agent_ppo2.py:151][0m 466944 total steps have happened
[32m[20230205 18:20:51 @agent_ppo2.py:127][0m #------------------------ Iteration 228 --------------------------#
[32m[20230205 18:20:52 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:20:52 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:52 @agent_ppo2.py:191][0m |          -0.0007 |           9.9848 |           3.2205 |
[32m[20230205 18:20:52 @agent_ppo2.py:191][0m |          -0.0084 |           8.7051 |           3.2189 |
[32m[20230205 18:20:52 @agent_ppo2.py:191][0m |          -0.0108 |           8.2308 |           3.2212 |
[32m[20230205 18:20:52 @agent_ppo2.py:191][0m |          -0.0126 |           7.9442 |           3.2192 |
[32m[20230205 18:20:52 @agent_ppo2.py:191][0m |          -0.0132 |           7.7032 |           3.2207 |
[32m[20230205 18:20:52 @agent_ppo2.py:191][0m |          -0.0136 |           7.5167 |           3.2185 |
[32m[20230205 18:20:52 @agent_ppo2.py:191][0m |          -0.0142 |           7.3468 |           3.2196 |
[32m[20230205 18:20:52 @agent_ppo2.py:191][0m |          -0.0152 |           7.2090 |           3.2179 |
[32m[20230205 18:20:52 @agent_ppo2.py:191][0m |          -0.0144 |           7.0671 |           3.2186 |
[32m[20230205 18:20:52 @agent_ppo2.py:191][0m |          -0.0161 |           6.9374 |           3.2199 |
[32m[20230205 18:20:52 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:20:53 @agent_ppo2.py:144][0m Average TRAINING episode reward: 250.49
[32m[20230205 18:20:53 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.75
[32m[20230205 18:20:53 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 277.44
[32m[20230205 18:20:53 @agent_ppo2.py:149][0m Total time:       7.42 min
[32m[20230205 18:20:53 @agent_ppo2.py:151][0m 468992 total steps have happened
[32m[20230205 18:20:53 @agent_ppo2.py:127][0m #------------------------ Iteration 229 --------------------------#
[32m[20230205 18:20:53 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:20:53 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:53 @agent_ppo2.py:191][0m |          -0.0027 |          24.5212 |           3.1968 |
[32m[20230205 18:20:53 @agent_ppo2.py:191][0m |          -0.0099 |           8.2802 |           3.1920 |
[32m[20230205 18:20:54 @agent_ppo2.py:191][0m |          -0.0123 |           7.0829 |           3.1894 |
[32m[20230205 18:20:54 @agent_ppo2.py:191][0m |          -0.0110 |           6.5749 |           3.1879 |
[32m[20230205 18:20:54 @agent_ppo2.py:191][0m |          -0.0132 |           6.2850 |           3.1877 |
[32m[20230205 18:20:54 @agent_ppo2.py:191][0m |          -0.0134 |           6.0448 |           3.1872 |
[32m[20230205 18:20:54 @agent_ppo2.py:191][0m |          -0.0144 |           5.8645 |           3.1880 |
[32m[20230205 18:20:54 @agent_ppo2.py:191][0m |          -0.0143 |           5.7702 |           3.1869 |
[32m[20230205 18:20:54 @agent_ppo2.py:191][0m |          -0.0148 |           5.6227 |           3.1858 |
[32m[20230205 18:20:54 @agent_ppo2.py:191][0m |          -0.0155 |           5.4941 |           3.1868 |
[32m[20230205 18:20:54 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:20:54 @agent_ppo2.py:144][0m Average TRAINING episode reward: 174.37
[32m[20230205 18:20:54 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 249.66
[32m[20230205 18:20:54 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 278.47
[32m[20230205 18:20:54 @agent_ppo2.py:149][0m Total time:       7.45 min
[32m[20230205 18:20:54 @agent_ppo2.py:151][0m 471040 total steps have happened
[32m[20230205 18:20:54 @agent_ppo2.py:127][0m #------------------------ Iteration 230 --------------------------#
[32m[20230205 18:20:55 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:20:55 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:55 @agent_ppo2.py:191][0m |          -0.0006 |          10.0158 |           3.2810 |
[32m[20230205 18:20:55 @agent_ppo2.py:191][0m |          -0.0020 |           9.5889 |           3.2722 |
[32m[20230205 18:20:55 @agent_ppo2.py:191][0m |          -0.0077 |           9.2290 |           3.2709 |
[32m[20230205 18:20:55 @agent_ppo2.py:191][0m |          -0.0078 |           9.1028 |           3.2729 |
[32m[20230205 18:20:55 @agent_ppo2.py:191][0m |          -0.0098 |           8.9358 |           3.2726 |
[32m[20230205 18:20:55 @agent_ppo2.py:191][0m |          -0.0114 |           8.8118 |           3.2708 |
[32m[20230205 18:20:55 @agent_ppo2.py:191][0m |          -0.0107 |           8.8248 |           3.2724 |
[32m[20230205 18:20:55 @agent_ppo2.py:191][0m |          -0.0107 |           8.6662 |           3.2693 |
[32m[20230205 18:20:55 @agent_ppo2.py:191][0m |          -0.0138 |           8.5478 |           3.2713 |
[32m[20230205 18:20:56 @agent_ppo2.py:191][0m |          -0.0135 |           8.4698 |           3.2698 |
[32m[20230205 18:20:56 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:20:56 @agent_ppo2.py:144][0m Average TRAINING episode reward: 253.06
[32m[20230205 18:20:56 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.14
[32m[20230205 18:20:56 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.03
[32m[20230205 18:20:56 @agent_ppo2.py:149][0m Total time:       7.48 min
[32m[20230205 18:20:56 @agent_ppo2.py:151][0m 473088 total steps have happened
[32m[20230205 18:20:56 @agent_ppo2.py:127][0m #------------------------ Iteration 231 --------------------------#
[32m[20230205 18:20:57 @agent_ppo2.py:133][0m Sampling time: 0.65 s by 1 slaves
[32m[20230205 18:20:57 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:57 @agent_ppo2.py:191][0m |          -0.0025 |           6.6489 |           3.1892 |
[32m[20230205 18:20:57 @agent_ppo2.py:191][0m |          -0.0088 |           5.9814 |           3.1827 |
[32m[20230205 18:20:57 @agent_ppo2.py:191][0m |          -0.0116 |           5.7812 |           3.1804 |
[32m[20230205 18:20:57 @agent_ppo2.py:191][0m |          -0.0125 |           5.6552 |           3.1852 |
[32m[20230205 18:20:57 @agent_ppo2.py:191][0m |          -0.0147 |           5.5918 |           3.1852 |
[32m[20230205 18:20:57 @agent_ppo2.py:191][0m |          -0.0154 |           5.4908 |           3.1846 |
[32m[20230205 18:20:57 @agent_ppo2.py:191][0m |          -0.0172 |           5.4263 |           3.1856 |
[32m[20230205 18:20:57 @agent_ppo2.py:191][0m |          -0.0168 |           5.3838 |           3.1898 |
[32m[20230205 18:20:57 @agent_ppo2.py:191][0m |          -0.0169 |           5.3355 |           3.1848 |
[32m[20230205 18:20:57 @agent_ppo2.py:191][0m |          -0.0161 |           5.2741 |           3.1905 |
[32m[20230205 18:20:57 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:20:58 @agent_ppo2.py:144][0m Average TRAINING episode reward: 220.08
[32m[20230205 18:20:58 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 250.57
[32m[20230205 18:20:58 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 274.09
[32m[20230205 18:20:58 @agent_ppo2.py:149][0m Total time:       7.51 min
[32m[20230205 18:20:58 @agent_ppo2.py:151][0m 475136 total steps have happened
[32m[20230205 18:20:58 @agent_ppo2.py:127][0m #------------------------ Iteration 232 --------------------------#
[32m[20230205 18:20:58 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:20:58 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:20:59 @agent_ppo2.py:191][0m |          -0.0007 |           9.6858 |           3.2970 |
[32m[20230205 18:20:59 @agent_ppo2.py:191][0m |          -0.0068 |           9.4374 |           3.2902 |
[32m[20230205 18:20:59 @agent_ppo2.py:191][0m |          -0.0087 |           9.1612 |           3.2817 |
[32m[20230205 18:20:59 @agent_ppo2.py:191][0m |          -0.0097 |           9.1473 |           3.2862 |
[32m[20230205 18:20:59 @agent_ppo2.py:191][0m |          -0.0116 |           8.9726 |           3.2864 |
[32m[20230205 18:20:59 @agent_ppo2.py:191][0m |          -0.0121 |           8.8847 |           3.2869 |
[32m[20230205 18:20:59 @agent_ppo2.py:191][0m |          -0.0123 |           8.8490 |           3.2878 |
[32m[20230205 18:20:59 @agent_ppo2.py:191][0m |          -0.0125 |           8.8044 |           3.2860 |
[32m[20230205 18:20:59 @agent_ppo2.py:191][0m |          -0.0137 |           8.6799 |           3.2883 |
[32m[20230205 18:20:59 @agent_ppo2.py:191][0m |          -0.0135 |           8.6378 |           3.2873 |
[32m[20230205 18:20:59 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:20:59 @agent_ppo2.py:144][0m Average TRAINING episode reward: 255.37
[32m[20230205 18:20:59 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.22
[32m[20230205 18:20:59 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 278.45
[32m[20230205 18:20:59 @agent_ppo2.py:149][0m Total time:       7.53 min
[32m[20230205 18:20:59 @agent_ppo2.py:151][0m 477184 total steps have happened
[32m[20230205 18:20:59 @agent_ppo2.py:127][0m #------------------------ Iteration 233 --------------------------#
[32m[20230205 18:21:00 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:21:00 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:00 @agent_ppo2.py:191][0m |           0.0014 |           8.9376 |           3.2098 |
[32m[20230205 18:21:00 @agent_ppo2.py:191][0m |          -0.0035 |           8.1301 |           3.2066 |
[32m[20230205 18:21:00 @agent_ppo2.py:191][0m |          -0.0066 |           7.7758 |           3.2030 |
[32m[20230205 18:21:00 @agent_ppo2.py:191][0m |          -0.0066 |           7.5565 |           3.2038 |
[32m[20230205 18:21:00 @agent_ppo2.py:191][0m |          -0.0073 |           7.4605 |           3.2011 |
[32m[20230205 18:21:00 @agent_ppo2.py:191][0m |          -0.0096 |           7.2462 |           3.2014 |
[32m[20230205 18:21:00 @agent_ppo2.py:191][0m |          -0.0078 |           7.1435 |           3.2010 |
[32m[20230205 18:21:00 @agent_ppo2.py:191][0m |          -0.0123 |           6.8142 |           3.2030 |
[32m[20230205 18:21:01 @agent_ppo2.py:191][0m |          -0.0125 |           6.7145 |           3.2029 |
[32m[20230205 18:21:01 @agent_ppo2.py:191][0m |          -0.0092 |           6.7738 |           3.2029 |
[32m[20230205 18:21:01 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:21:01 @agent_ppo2.py:144][0m Average TRAINING episode reward: 254.48
[32m[20230205 18:21:01 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.58
[32m[20230205 18:21:01 @agent_ppo2.py:146][0m Average EVALUATION episode reward: -25.90
[32m[20230205 18:21:01 @agent_ppo2.py:149][0m Total time:       7.56 min
[32m[20230205 18:21:01 @agent_ppo2.py:151][0m 479232 total steps have happened
[32m[20230205 18:21:01 @agent_ppo2.py:127][0m #------------------------ Iteration 234 --------------------------#
[32m[20230205 18:21:02 @agent_ppo2.py:133][0m Sampling time: 0.67 s by 1 slaves
[32m[20230205 18:21:02 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:02 @agent_ppo2.py:191][0m |           0.0006 |          31.2679 |           3.2680 |
[32m[20230205 18:21:02 @agent_ppo2.py:191][0m |          -0.0047 |          22.1738 |           3.2582 |
[32m[20230205 18:21:02 @agent_ppo2.py:191][0m |          -0.0084 |          18.9571 |           3.2523 |
[32m[20230205 18:21:02 @agent_ppo2.py:191][0m |          -0.0107 |          17.9198 |           3.2511 |
[32m[20230205 18:21:02 @agent_ppo2.py:191][0m |          -0.0098 |          17.5539 |           3.2483 |
[32m[20230205 18:21:02 @agent_ppo2.py:191][0m |          -0.0110 |          16.8603 |           3.2490 |
[32m[20230205 18:21:02 @agent_ppo2.py:191][0m |          -0.0126 |          16.4988 |           3.2437 |
[32m[20230205 18:21:02 @agent_ppo2.py:191][0m |          -0.0114 |          16.5704 |           3.2393 |
[32m[20230205 18:21:02 @agent_ppo2.py:191][0m |          -0.0124 |          16.2450 |           3.2407 |
[32m[20230205 18:21:02 @agent_ppo2.py:191][0m |          -0.0129 |          15.6935 |           3.2364 |
[32m[20230205 18:21:02 @agent_ppo2.py:136][0m Policy update time: 0.66 s
[32m[20230205 18:21:03 @agent_ppo2.py:144][0m Average TRAINING episode reward: 164.80
[32m[20230205 18:21:03 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.05
[32m[20230205 18:21:03 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 94.43
[32m[20230205 18:21:03 @agent_ppo2.py:149][0m Total time:       7.59 min
[32m[20230205 18:21:03 @agent_ppo2.py:151][0m 481280 total steps have happened
[32m[20230205 18:21:03 @agent_ppo2.py:127][0m #------------------------ Iteration 235 --------------------------#
[32m[20230205 18:21:04 @agent_ppo2.py:133][0m Sampling time: 0.62 s by 1 slaves
[32m[20230205 18:21:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:04 @agent_ppo2.py:191][0m |           0.0010 |          39.7234 |           3.3443 |
[32m[20230205 18:21:04 @agent_ppo2.py:191][0m |          -0.0047 |          28.4354 |           3.3451 |
[32m[20230205 18:21:04 @agent_ppo2.py:191][0m |          -0.0063 |          24.9521 |           3.3435 |
[32m[20230205 18:21:04 @agent_ppo2.py:191][0m |          -0.0078 |          23.3253 |           3.3422 |
[32m[20230205 18:21:04 @agent_ppo2.py:191][0m |          -0.0080 |          21.8288 |           3.3398 |
[32m[20230205 18:21:04 @agent_ppo2.py:191][0m |          -0.0100 |          20.6857 |           3.3413 |
[32m[20230205 18:21:04 @agent_ppo2.py:191][0m |          -0.0111 |          20.0457 |           3.3404 |
[32m[20230205 18:21:04 @agent_ppo2.py:191][0m |          -0.0112 |          19.4075 |           3.3407 |
[32m[20230205 18:21:04 @agent_ppo2.py:191][0m |          -0.0118 |          18.8854 |           3.3417 |
[32m[20230205 18:21:04 @agent_ppo2.py:191][0m |          -0.0118 |          18.2726 |           3.3398 |
[32m[20230205 18:21:04 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:21:05 @agent_ppo2.py:144][0m Average TRAINING episode reward: 108.81
[32m[20230205 18:21:05 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 249.08
[32m[20230205 18:21:05 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.31
[32m[20230205 18:21:05 @agent_ppo2.py:149][0m Total time:       7.62 min
[32m[20230205 18:21:05 @agent_ppo2.py:151][0m 483328 total steps have happened
[32m[20230205 18:21:05 @agent_ppo2.py:127][0m #------------------------ Iteration 236 --------------------------#
[32m[20230205 18:21:05 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:21:05 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:05 @agent_ppo2.py:191][0m |           0.0060 |          55.4316 |           3.2270 |
[32m[20230205 18:21:05 @agent_ppo2.py:191][0m |           0.0013 |          24.3199 |           3.2251 |
[32m[20230205 18:21:05 @agent_ppo2.py:191][0m |          -0.0085 |          15.1560 |           3.2229 |
[32m[20230205 18:21:05 @agent_ppo2.py:191][0m |          -0.0112 |          12.4897 |           3.2248 |
[32m[20230205 18:21:06 @agent_ppo2.py:191][0m |          -0.0108 |          11.5418 |           3.2234 |
[32m[20230205 18:21:06 @agent_ppo2.py:191][0m |          -0.0155 |          10.9683 |           3.2216 |
[32m[20230205 18:21:06 @agent_ppo2.py:191][0m |          -0.0141 |          10.7842 |           3.2229 |
[32m[20230205 18:21:06 @agent_ppo2.py:191][0m |          -0.0143 |          10.1891 |           3.2240 |
[32m[20230205 18:21:06 @agent_ppo2.py:191][0m |          -0.0161 |           9.8453 |           3.2247 |
[32m[20230205 18:21:06 @agent_ppo2.py:191][0m |          -0.0120 |           9.5489 |           3.2237 |
[32m[20230205 18:21:06 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:21:06 @agent_ppo2.py:144][0m Average TRAINING episode reward: 131.78
[32m[20230205 18:21:06 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.26
[32m[20230205 18:21:06 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 96.49
[32m[20230205 18:21:06 @agent_ppo2.py:149][0m Total time:       7.65 min
[32m[20230205 18:21:06 @agent_ppo2.py:151][0m 485376 total steps have happened
[32m[20230205 18:21:06 @agent_ppo2.py:127][0m #------------------------ Iteration 237 --------------------------#
[32m[20230205 18:21:07 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:21:07 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:07 @agent_ppo2.py:191][0m |          -0.0026 |          11.5217 |           3.2266 |
[32m[20230205 18:21:07 @agent_ppo2.py:191][0m |           0.0022 |          10.3401 |           3.2241 |
[32m[20230205 18:21:07 @agent_ppo2.py:191][0m |          -0.0059 |          10.0660 |           3.2187 |
[32m[20230205 18:21:07 @agent_ppo2.py:191][0m |          -0.0103 |           9.9093 |           3.2167 |
[32m[20230205 18:21:07 @agent_ppo2.py:191][0m |          -0.0131 |           9.7934 |           3.2171 |
[32m[20230205 18:21:07 @agent_ppo2.py:191][0m |          -0.0150 |           9.6469 |           3.2187 |
[32m[20230205 18:21:07 @agent_ppo2.py:191][0m |          -0.0154 |           9.5611 |           3.2193 |
[32m[20230205 18:21:07 @agent_ppo2.py:191][0m |          -0.0117 |           9.4653 |           3.2142 |
[32m[20230205 18:21:07 @agent_ppo2.py:191][0m |          -0.0154 |           9.4990 |           3.2166 |
[32m[20230205 18:21:08 @agent_ppo2.py:191][0m |          -0.0031 |           9.3674 |           3.2155 |
[32m[20230205 18:21:08 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:21:08 @agent_ppo2.py:144][0m Average TRAINING episode reward: 255.27
[32m[20230205 18:21:08 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.36
[32m[20230205 18:21:08 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.71
[32m[20230205 18:21:08 @agent_ppo2.py:149][0m Total time:       7.68 min
[32m[20230205 18:21:08 @agent_ppo2.py:151][0m 487424 total steps have happened
[32m[20230205 18:21:08 @agent_ppo2.py:127][0m #------------------------ Iteration 238 --------------------------#
[32m[20230205 18:21:09 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:21:09 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:09 @agent_ppo2.py:191][0m |           0.0020 |          24.0876 |           3.2136 |
[32m[20230205 18:21:09 @agent_ppo2.py:191][0m |          -0.0046 |          11.3712 |           3.2113 |
[32m[20230205 18:21:09 @agent_ppo2.py:191][0m |          -0.0073 |           9.8186 |           3.2111 |
[32m[20230205 18:21:09 @agent_ppo2.py:191][0m |          -0.0078 |           9.3664 |           3.2092 |
[32m[20230205 18:21:09 @agent_ppo2.py:191][0m |          -0.0100 |           8.8008 |           3.2133 |
[32m[20230205 18:21:09 @agent_ppo2.py:191][0m |          -0.0114 |           8.5299 |           3.2148 |
[32m[20230205 18:21:09 @agent_ppo2.py:191][0m |          -0.0107 |           8.3341 |           3.2146 |
[32m[20230205 18:21:09 @agent_ppo2.py:191][0m |          -0.0138 |           8.1869 |           3.2120 |
[32m[20230205 18:21:09 @agent_ppo2.py:191][0m |          -0.0137 |           8.0229 |           3.2126 |
[32m[20230205 18:21:09 @agent_ppo2.py:191][0m |          -0.0145 |           7.9107 |           3.2128 |
[32m[20230205 18:21:09 @agent_ppo2.py:136][0m Policy update time: 0.49 s
[32m[20230205 18:21:10 @agent_ppo2.py:144][0m Average TRAINING episode reward: 157.27
[32m[20230205 18:21:10 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 246.34
[32m[20230205 18:21:10 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 150.50
[32m[20230205 18:21:10 @agent_ppo2.py:149][0m Total time:       7.70 min
[32m[20230205 18:21:10 @agent_ppo2.py:151][0m 489472 total steps have happened
[32m[20230205 18:21:10 @agent_ppo2.py:127][0m #------------------------ Iteration 239 --------------------------#
[32m[20230205 18:21:10 @agent_ppo2.py:133][0m Sampling time: 0.65 s by 1 slaves
[32m[20230205 18:21:10 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:10 @agent_ppo2.py:191][0m |          -0.0019 |          33.1225 |           3.2602 |
[32m[20230205 18:21:10 @agent_ppo2.py:191][0m |          -0.0094 |          25.3482 |           3.2571 |
[32m[20230205 18:21:10 @agent_ppo2.py:191][0m |          -0.0108 |          20.8593 |           3.2585 |
[32m[20230205 18:21:11 @agent_ppo2.py:191][0m |          -0.0104 |          17.8227 |           3.2544 |
[32m[20230205 18:21:11 @agent_ppo2.py:191][0m |          -0.0099 |          16.6926 |           3.2586 |
[32m[20230205 18:21:11 @agent_ppo2.py:191][0m |          -0.0108 |          15.1368 |           3.2586 |
[32m[20230205 18:21:11 @agent_ppo2.py:191][0m |          -0.0119 |          14.2641 |           3.2565 |
[32m[20230205 18:21:11 @agent_ppo2.py:191][0m |          -0.0142 |          13.8519 |           3.2545 |
[32m[20230205 18:21:11 @agent_ppo2.py:191][0m |          -0.0136 |          13.2097 |           3.2559 |
[32m[20230205 18:21:11 @agent_ppo2.py:191][0m |          -0.0152 |          12.9605 |           3.2559 |
[32m[20230205 18:21:11 @agent_ppo2.py:136][0m Policy update time: 0.66 s
[32m[20230205 18:21:11 @agent_ppo2.py:144][0m Average TRAINING episode reward: 163.03
[32m[20230205 18:21:11 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.17
[32m[20230205 18:21:11 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 163.04
[32m[20230205 18:21:11 @agent_ppo2.py:149][0m Total time:       7.73 min
[32m[20230205 18:21:11 @agent_ppo2.py:151][0m 491520 total steps have happened
[32m[20230205 18:21:11 @agent_ppo2.py:127][0m #------------------------ Iteration 240 --------------------------#
[32m[20230205 18:21:12 @agent_ppo2.py:133][0m Sampling time: 0.68 s by 1 slaves
[32m[20230205 18:21:12 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:12 @agent_ppo2.py:191][0m |          -0.0017 |          28.6236 |           3.2296 |
[32m[20230205 18:21:12 @agent_ppo2.py:191][0m |          -0.0030 |          22.5093 |           3.2259 |
[32m[20230205 18:21:12 @agent_ppo2.py:191][0m |          -0.0120 |          20.6683 |           3.2221 |
[32m[20230205 18:21:12 @agent_ppo2.py:191][0m |          -0.0089 |          19.7316 |           3.2223 |
[32m[20230205 18:21:13 @agent_ppo2.py:191][0m |           0.0024 |          19.9729 |           3.2205 |
[32m[20230205 18:21:13 @agent_ppo2.py:191][0m |          -0.0104 |          18.3732 |           3.2163 |
[32m[20230205 18:21:13 @agent_ppo2.py:191][0m |          -0.0098 |          17.8023 |           3.2163 |
[32m[20230205 18:21:13 @agent_ppo2.py:191][0m |          -0.0081 |          17.4079 |           3.2189 |
[32m[20230205 18:21:13 @agent_ppo2.py:191][0m |          -0.0049 |          17.8461 |           3.2172 |
[32m[20230205 18:21:13 @agent_ppo2.py:191][0m |          -0.0121 |          16.4546 |           3.2184 |
[32m[20230205 18:21:13 @agent_ppo2.py:136][0m Policy update time: 0.68 s
[32m[20230205 18:21:13 @agent_ppo2.py:144][0m Average TRAINING episode reward: 162.66
[32m[20230205 18:21:13 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.93
[32m[20230205 18:21:13 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 274.34
[32m[20230205 18:21:13 @agent_ppo2.py:149][0m Total time:       7.77 min
[32m[20230205 18:21:13 @agent_ppo2.py:151][0m 493568 total steps have happened
[32m[20230205 18:21:13 @agent_ppo2.py:127][0m #------------------------ Iteration 241 --------------------------#
[32m[20230205 18:21:14 @agent_ppo2.py:133][0m Sampling time: 0.64 s by 1 slaves
[32m[20230205 18:21:14 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:14 @agent_ppo2.py:191][0m |          -0.0018 |          37.9087 |           3.1589 |
[32m[20230205 18:21:14 @agent_ppo2.py:191][0m |          -0.0080 |          33.9001 |           3.1582 |
[32m[20230205 18:21:14 @agent_ppo2.py:191][0m |          -0.0093 |          31.3931 |           3.1548 |
[32m[20230205 18:21:14 @agent_ppo2.py:191][0m |          -0.0063 |          30.8733 |           3.1520 |
[32m[20230205 18:21:14 @agent_ppo2.py:191][0m |          -0.0121 |          29.1779 |           3.1486 |
[32m[20230205 18:21:14 @agent_ppo2.py:191][0m |          -0.0127 |          28.2527 |           3.1509 |
[32m[20230205 18:21:14 @agent_ppo2.py:191][0m |          -0.0119 |          27.5657 |           3.1478 |
[32m[20230205 18:21:14 @agent_ppo2.py:191][0m |           0.0011 |          31.2369 |           3.1443 |
[32m[20230205 18:21:15 @agent_ppo2.py:191][0m |          -0.0099 |          27.7223 |           3.1448 |
[32m[20230205 18:21:15 @agent_ppo2.py:191][0m |          -0.0145 |          25.6917 |           3.1435 |
[32m[20230205 18:21:15 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:21:15 @agent_ppo2.py:144][0m Average TRAINING episode reward: 140.24
[32m[20230205 18:21:15 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 250.93
[32m[20230205 18:21:15 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 268.44
[32m[20230205 18:21:15 @agent_ppo2.py:149][0m Total time:       7.80 min
[32m[20230205 18:21:15 @agent_ppo2.py:151][0m 495616 total steps have happened
[32m[20230205 18:21:15 @agent_ppo2.py:127][0m #------------------------ Iteration 242 --------------------------#
[32m[20230205 18:21:16 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:21:16 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:16 @agent_ppo2.py:191][0m |          -0.0017 |          13.7786 |           3.2319 |
[32m[20230205 18:21:16 @agent_ppo2.py:191][0m |          -0.0073 |          11.6906 |           3.2271 |
[32m[20230205 18:21:16 @agent_ppo2.py:191][0m |          -0.0085 |          11.2803 |           3.2201 |
[32m[20230205 18:21:16 @agent_ppo2.py:191][0m |          -0.0119 |          10.8235 |           3.2207 |
[32m[20230205 18:21:16 @agent_ppo2.py:191][0m |          -0.0113 |          10.5804 |           3.2207 |
[32m[20230205 18:21:16 @agent_ppo2.py:191][0m |          -0.0123 |          10.4275 |           3.2165 |
[32m[20230205 18:21:16 @agent_ppo2.py:191][0m |          -0.0138 |          10.0997 |           3.2161 |
[32m[20230205 18:21:16 @agent_ppo2.py:191][0m |          -0.0138 |           9.9477 |           3.2158 |
[32m[20230205 18:21:16 @agent_ppo2.py:191][0m |          -0.0120 |           9.9278 |           3.2153 |
[32m[20230205 18:21:16 @agent_ppo2.py:191][0m |          -0.0125 |           9.7077 |           3.2158 |
[32m[20230205 18:21:16 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:21:17 @agent_ppo2.py:144][0m Average TRAINING episode reward: 254.67
[32m[20230205 18:21:17 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.83
[32m[20230205 18:21:17 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 169.58
[32m[20230205 18:21:17 @agent_ppo2.py:149][0m Total time:       7.82 min
[32m[20230205 18:21:17 @agent_ppo2.py:151][0m 497664 total steps have happened
[32m[20230205 18:21:17 @agent_ppo2.py:127][0m #------------------------ Iteration 243 --------------------------#
[32m[20230205 18:21:17 @agent_ppo2.py:133][0m Sampling time: 0.58 s by 1 slaves
[32m[20230205 18:21:17 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:17 @agent_ppo2.py:191][0m |           0.0121 |          13.6825 |           3.1441 |
[32m[20230205 18:21:18 @agent_ppo2.py:191][0m |          -0.0075 |          10.2235 |           3.1316 |
[32m[20230205 18:21:18 @agent_ppo2.py:191][0m |          -0.0090 |           9.7733 |           3.1308 |
[32m[20230205 18:21:18 @agent_ppo2.py:191][0m |          -0.0116 |           9.5418 |           3.1312 |
[32m[20230205 18:21:18 @agent_ppo2.py:191][0m |          -0.0047 |           9.4938 |           3.1296 |
[32m[20230205 18:21:18 @agent_ppo2.py:191][0m |          -0.0162 |           9.2767 |           3.1263 |
[32m[20230205 18:21:18 @agent_ppo2.py:191][0m |          -0.0137 |           9.1271 |           3.1260 |
[32m[20230205 18:21:18 @agent_ppo2.py:191][0m |          -0.0144 |           9.0403 |           3.1238 |
[32m[20230205 18:21:18 @agent_ppo2.py:191][0m |          -0.0107 |           8.9434 |           3.1227 |
[32m[20230205 18:21:18 @agent_ppo2.py:191][0m |          -0.0030 |           9.8523 |           3.1223 |
[32m[20230205 18:21:18 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:21:18 @agent_ppo2.py:144][0m Average TRAINING episode reward: 250.08
[32m[20230205 18:21:18 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.49
[32m[20230205 18:21:18 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 271.46
[32m[20230205 18:21:18 @agent_ppo2.py:149][0m Total time:       7.85 min
[32m[20230205 18:21:18 @agent_ppo2.py:151][0m 499712 total steps have happened
[32m[20230205 18:21:18 @agent_ppo2.py:127][0m #------------------------ Iteration 244 --------------------------#
[32m[20230205 18:21:19 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:21:19 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:19 @agent_ppo2.py:191][0m |          -0.0009 |           8.8791 |           3.2497 |
[32m[20230205 18:21:19 @agent_ppo2.py:191][0m |          -0.0072 |           8.6412 |           3.2529 |
[32m[20230205 18:21:19 @agent_ppo2.py:191][0m |          -0.0084 |           8.4547 |           3.2530 |
[32m[20230205 18:21:19 @agent_ppo2.py:191][0m |          -0.0039 |           8.7689 |           3.2511 |
[32m[20230205 18:21:19 @agent_ppo2.py:191][0m |          -0.0009 |           8.7849 |           3.2539 |
[32m[20230205 18:21:19 @agent_ppo2.py:191][0m |          -0.0085 |           8.4504 |           3.2557 |
[32m[20230205 18:21:19 @agent_ppo2.py:191][0m |          -0.0134 |           8.0801 |           3.2562 |
[32m[20230205 18:21:20 @agent_ppo2.py:191][0m |          -0.0145 |           8.0180 |           3.2577 |
[32m[20230205 18:21:20 @agent_ppo2.py:191][0m |          -0.0144 |           7.9635 |           3.2549 |
[32m[20230205 18:21:20 @agent_ppo2.py:191][0m |          -0.0131 |           7.9361 |           3.2573 |
[32m[20230205 18:21:20 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:21:20 @agent_ppo2.py:144][0m Average TRAINING episode reward: 247.66
[32m[20230205 18:21:20 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 250.96
[32m[20230205 18:21:20 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.28
[32m[20230205 18:21:20 @agent_ppo2.py:149][0m Total time:       7.88 min
[32m[20230205 18:21:20 @agent_ppo2.py:151][0m 501760 total steps have happened
[32m[20230205 18:21:20 @agent_ppo2.py:127][0m #------------------------ Iteration 245 --------------------------#
[32m[20230205 18:21:21 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:21:21 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:21 @agent_ppo2.py:191][0m |           0.0010 |          47.1397 |           3.3202 |
[32m[20230205 18:21:21 @agent_ppo2.py:191][0m |          -0.0031 |          30.4877 |           3.3172 |
[32m[20230205 18:21:21 @agent_ppo2.py:191][0m |          -0.0061 |          23.1350 |           3.3164 |
[32m[20230205 18:21:21 @agent_ppo2.py:191][0m |          -0.0086 |          20.2384 |           3.3143 |
[32m[20230205 18:21:21 @agent_ppo2.py:191][0m |          -0.0090 |          18.6947 |           3.3126 |
[32m[20230205 18:21:21 @agent_ppo2.py:191][0m |          -0.0088 |          17.5693 |           3.3102 |
[32m[20230205 18:21:21 @agent_ppo2.py:191][0m |          -0.0110 |          16.5745 |           3.3120 |
[32m[20230205 18:21:21 @agent_ppo2.py:191][0m |          -0.0107 |          15.9380 |           3.3111 |
[32m[20230205 18:21:21 @agent_ppo2.py:191][0m |          -0.0125 |          15.4629 |           3.3096 |
[32m[20230205 18:21:21 @agent_ppo2.py:191][0m |          -0.0131 |          15.0584 |           3.3079 |
[32m[20230205 18:21:21 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:21:22 @agent_ppo2.py:144][0m Average TRAINING episode reward: 159.31
[32m[20230205 18:21:22 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 245.61
[32m[20230205 18:21:22 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 117.57
[32m[20230205 18:21:22 @agent_ppo2.py:149][0m Total time:       7.91 min
[32m[20230205 18:21:22 @agent_ppo2.py:151][0m 503808 total steps have happened
[32m[20230205 18:21:22 @agent_ppo2.py:127][0m #------------------------ Iteration 246 --------------------------#
[32m[20230205 18:21:22 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:21:22 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:22 @agent_ppo2.py:191][0m |          -0.0024 |          13.2432 |           3.2359 |
[32m[20230205 18:21:23 @agent_ppo2.py:191][0m |          -0.0041 |          11.0910 |           3.2305 |
[32m[20230205 18:21:23 @agent_ppo2.py:191][0m |          -0.0088 |          10.4845 |           3.2320 |
[32m[20230205 18:21:23 @agent_ppo2.py:191][0m |          -0.0098 |          10.3328 |           3.2276 |
[32m[20230205 18:21:23 @agent_ppo2.py:191][0m |          -0.0129 |          10.0311 |           3.2300 |
[32m[20230205 18:21:23 @agent_ppo2.py:191][0m |          -0.0123 |           9.8722 |           3.2283 |
[32m[20230205 18:21:23 @agent_ppo2.py:191][0m |          -0.0135 |           9.7298 |           3.2278 |
[32m[20230205 18:21:23 @agent_ppo2.py:191][0m |          -0.0135 |           9.6533 |           3.2278 |
[32m[20230205 18:21:23 @agent_ppo2.py:191][0m |          -0.0132 |           9.5356 |           3.2279 |
[32m[20230205 18:21:23 @agent_ppo2.py:191][0m |          -0.0147 |           9.4134 |           3.2278 |
[32m[20230205 18:21:23 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:21:23 @agent_ppo2.py:144][0m Average TRAINING episode reward: 251.07
[32m[20230205 18:21:23 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.02
[32m[20230205 18:21:23 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 52.93
[32m[20230205 18:21:23 @agent_ppo2.py:149][0m Total time:       7.93 min
[32m[20230205 18:21:23 @agent_ppo2.py:151][0m 505856 total steps have happened
[32m[20230205 18:21:23 @agent_ppo2.py:127][0m #------------------------ Iteration 247 --------------------------#
[32m[20230205 18:21:24 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:21:24 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:24 @agent_ppo2.py:191][0m |           0.0023 |          10.6131 |           3.2773 |
[32m[20230205 18:21:24 @agent_ppo2.py:191][0m |          -0.0072 |           9.2792 |           3.2794 |
[32m[20230205 18:21:24 @agent_ppo2.py:191][0m |          -0.0089 |           8.8983 |           3.2848 |
[32m[20230205 18:21:24 @agent_ppo2.py:191][0m |          -0.0086 |           8.8353 |           3.2844 |
[32m[20230205 18:21:24 @agent_ppo2.py:191][0m |          -0.0104 |           8.4994 |           3.2887 |
[32m[20230205 18:21:24 @agent_ppo2.py:191][0m |          -0.0110 |           8.3623 |           3.2883 |
[32m[20230205 18:21:24 @agent_ppo2.py:191][0m |          -0.0124 |           8.2037 |           3.2884 |
[32m[20230205 18:21:24 @agent_ppo2.py:191][0m |          -0.0114 |           8.1147 |           3.2906 |
[32m[20230205 18:21:25 @agent_ppo2.py:191][0m |          -0.0149 |           7.9502 |           3.2927 |
[32m[20230205 18:21:25 @agent_ppo2.py:191][0m |          -0.0140 |           7.8171 |           3.2906 |
[32m[20230205 18:21:25 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:21:25 @agent_ppo2.py:144][0m Average TRAINING episode reward: 248.64
[32m[20230205 18:21:25 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 249.98
[32m[20230205 18:21:25 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 259.32
[32m[20230205 18:21:25 @agent_ppo2.py:149][0m Total time:       7.96 min
[32m[20230205 18:21:25 @agent_ppo2.py:151][0m 507904 total steps have happened
[32m[20230205 18:21:25 @agent_ppo2.py:127][0m #------------------------ Iteration 248 --------------------------#
[32m[20230205 18:21:26 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:21:26 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:26 @agent_ppo2.py:191][0m |          -0.0001 |          11.1655 |           3.2988 |
[32m[20230205 18:21:26 @agent_ppo2.py:191][0m |          -0.0054 |           9.6928 |           3.2958 |
[32m[20230205 18:21:26 @agent_ppo2.py:191][0m |          -0.0082 |           9.1635 |           3.2960 |
[32m[20230205 18:21:26 @agent_ppo2.py:191][0m |          -0.0100 |           8.7183 |           3.2939 |
[32m[20230205 18:21:26 @agent_ppo2.py:191][0m |          -0.0114 |           8.4212 |           3.2927 |
[32m[20230205 18:21:26 @agent_ppo2.py:191][0m |          -0.0122 |           8.1736 |           3.2933 |
[32m[20230205 18:21:26 @agent_ppo2.py:191][0m |          -0.0129 |           7.9405 |           3.2917 |
[32m[20230205 18:21:26 @agent_ppo2.py:191][0m |          -0.0137 |           7.7723 |           3.2918 |
[32m[20230205 18:21:26 @agent_ppo2.py:191][0m |          -0.0136 |           7.6018 |           3.2917 |
[32m[20230205 18:21:26 @agent_ppo2.py:191][0m |          -0.0144 |           7.4654 |           3.2919 |
[32m[20230205 18:21:26 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:21:27 @agent_ppo2.py:144][0m Average TRAINING episode reward: 254.89
[32m[20230205 18:21:27 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.49
[32m[20230205 18:21:27 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 83.19
[32m[20230205 18:21:27 @agent_ppo2.py:149][0m Total time:       7.99 min
[32m[20230205 18:21:27 @agent_ppo2.py:151][0m 509952 total steps have happened
[32m[20230205 18:21:27 @agent_ppo2.py:127][0m #------------------------ Iteration 249 --------------------------#
[32m[20230205 18:21:27 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:21:27 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:27 @agent_ppo2.py:191][0m |          -0.0001 |           9.4452 |           3.3657 |
[32m[20230205 18:21:27 @agent_ppo2.py:191][0m |          -0.0070 |           8.5916 |           3.3593 |
[32m[20230205 18:21:28 @agent_ppo2.py:191][0m |          -0.0091 |           8.2057 |           3.3552 |
[32m[20230205 18:21:28 @agent_ppo2.py:191][0m |          -0.0110 |           7.9194 |           3.3525 |
[32m[20230205 18:21:28 @agent_ppo2.py:191][0m |          -0.0113 |           7.7232 |           3.3556 |
[32m[20230205 18:21:28 @agent_ppo2.py:191][0m |          -0.0119 |           7.6349 |           3.3564 |
[32m[20230205 18:21:28 @agent_ppo2.py:191][0m |          -0.0145 |           7.4261 |           3.3534 |
[32m[20230205 18:21:28 @agent_ppo2.py:191][0m |          -0.0151 |           7.2767 |           3.3523 |
[32m[20230205 18:21:28 @agent_ppo2.py:191][0m |          -0.0148 |           7.1668 |           3.3555 |
[32m[20230205 18:21:28 @agent_ppo2.py:191][0m |          -0.0162 |           7.0894 |           3.3534 |
[32m[20230205 18:21:28 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:21:28 @agent_ppo2.py:144][0m Average TRAINING episode reward: 249.61
[32m[20230205 18:21:28 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.40
[32m[20230205 18:21:28 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 150.06
[32m[20230205 18:21:28 @agent_ppo2.py:149][0m Total time:       8.02 min
[32m[20230205 18:21:28 @agent_ppo2.py:151][0m 512000 total steps have happened
[32m[20230205 18:21:28 @agent_ppo2.py:127][0m #------------------------ Iteration 250 --------------------------#
[32m[20230205 18:21:29 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:21:29 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:29 @agent_ppo2.py:191][0m |           0.0032 |          36.4380 |           3.3027 |
[32m[20230205 18:21:29 @agent_ppo2.py:191][0m |          -0.0056 |          19.7193 |           3.3011 |
[32m[20230205 18:21:29 @agent_ppo2.py:191][0m |          -0.0086 |          14.1612 |           3.2957 |
[32m[20230205 18:21:29 @agent_ppo2.py:191][0m |          -0.0107 |          11.4997 |           3.2968 |
[32m[20230205 18:21:29 @agent_ppo2.py:191][0m |          -0.0114 |          10.1104 |           3.2938 |
[32m[20230205 18:21:29 @agent_ppo2.py:191][0m |          -0.0120 |           9.1456 |           3.2912 |
[32m[20230205 18:21:30 @agent_ppo2.py:191][0m |          -0.0136 |           8.5364 |           3.2920 |
[32m[20230205 18:21:30 @agent_ppo2.py:191][0m |          -0.0151 |           8.1058 |           3.2881 |
[32m[20230205 18:21:30 @agent_ppo2.py:191][0m |          -0.0147 |           7.8542 |           3.2858 |
[32m[20230205 18:21:30 @agent_ppo2.py:191][0m |          -0.0166 |           7.5310 |           3.2883 |
[32m[20230205 18:21:30 @agent_ppo2.py:136][0m Policy update time: 0.60 s
[32m[20230205 18:21:30 @agent_ppo2.py:144][0m Average TRAINING episode reward: 103.94
[32m[20230205 18:21:30 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.97
[32m[20230205 18:21:30 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 268.59
[32m[20230205 18:21:30 @agent_ppo2.py:149][0m Total time:       8.05 min
[32m[20230205 18:21:30 @agent_ppo2.py:151][0m 514048 total steps have happened
[32m[20230205 18:21:30 @agent_ppo2.py:127][0m #------------------------ Iteration 251 --------------------------#
[32m[20230205 18:21:31 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:21:31 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:31 @agent_ppo2.py:191][0m |          -0.0023 |          10.9177 |           3.1898 |
[32m[20230205 18:21:31 @agent_ppo2.py:191][0m |          -0.0061 |           9.8766 |           3.1916 |
[32m[20230205 18:21:31 @agent_ppo2.py:191][0m |          -0.0040 |           9.5778 |           3.1933 |
[32m[20230205 18:21:31 @agent_ppo2.py:191][0m |          -0.0086 |           9.2511 |           3.1942 |
[32m[20230205 18:21:31 @agent_ppo2.py:191][0m |          -0.0103 |           9.1311 |           3.1942 |
[32m[20230205 18:21:31 @agent_ppo2.py:191][0m |          -0.0095 |           9.0122 |           3.1944 |
[32m[20230205 18:21:31 @agent_ppo2.py:191][0m |          -0.0097 |           8.9892 |           3.1942 |
[32m[20230205 18:21:31 @agent_ppo2.py:191][0m |          -0.0090 |           8.8388 |           3.1954 |
[32m[20230205 18:21:31 @agent_ppo2.py:191][0m |          -0.0114 |           8.7166 |           3.1952 |
[32m[20230205 18:21:31 @agent_ppo2.py:191][0m |          -0.0088 |           8.8856 |           3.1958 |
[32m[20230205 18:21:31 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:21:32 @agent_ppo2.py:144][0m Average TRAINING episode reward: 255.16
[32m[20230205 18:21:32 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.96
[32m[20230205 18:21:32 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 153.70
[32m[20230205 18:21:32 @agent_ppo2.py:149][0m Total time:       8.08 min
[32m[20230205 18:21:32 @agent_ppo2.py:151][0m 516096 total steps have happened
[32m[20230205 18:21:32 @agent_ppo2.py:127][0m #------------------------ Iteration 252 --------------------------#
[32m[20230205 18:21:32 @agent_ppo2.py:133][0m Sampling time: 0.58 s by 1 slaves
[32m[20230205 18:21:33 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:33 @agent_ppo2.py:191][0m |          -0.0006 |          24.3802 |           3.2540 |
[32m[20230205 18:21:33 @agent_ppo2.py:191][0m |          -0.0068 |          14.4184 |           3.2510 |
[32m[20230205 18:21:33 @agent_ppo2.py:191][0m |          -0.0089 |          11.3248 |           3.2482 |
[32m[20230205 18:21:33 @agent_ppo2.py:191][0m |          -0.0104 |          10.4463 |           3.2478 |
[32m[20230205 18:21:33 @agent_ppo2.py:191][0m |          -0.0117 |           9.7434 |           3.2428 |
[32m[20230205 18:21:33 @agent_ppo2.py:191][0m |          -0.0127 |           9.4354 |           3.2435 |
[32m[20230205 18:21:33 @agent_ppo2.py:191][0m |          -0.0136 |           8.9590 |           3.2411 |
[32m[20230205 18:21:33 @agent_ppo2.py:191][0m |          -0.0139 |           8.7433 |           3.2403 |
[32m[20230205 18:21:33 @agent_ppo2.py:191][0m |          -0.0141 |           8.4863 |           3.2385 |
[32m[20230205 18:21:33 @agent_ppo2.py:191][0m |          -0.0152 |           8.2530 |           3.2373 |
[32m[20230205 18:21:33 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:21:34 @agent_ppo2.py:144][0m Average TRAINING episode reward: 192.60
[32m[20230205 18:21:34 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 245.32
[32m[20230205 18:21:34 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 204.19
[32m[20230205 18:21:34 @agent_ppo2.py:149][0m Total time:       8.10 min
[32m[20230205 18:21:34 @agent_ppo2.py:151][0m 518144 total steps have happened
[32m[20230205 18:21:34 @agent_ppo2.py:127][0m #------------------------ Iteration 253 --------------------------#
[32m[20230205 18:21:34 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:21:34 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:34 @agent_ppo2.py:191][0m |           0.0050 |          10.9093 |           3.3406 |
[32m[20230205 18:21:34 @agent_ppo2.py:191][0m |          -0.0033 |          10.1652 |           3.3456 |
[32m[20230205 18:21:34 @agent_ppo2.py:191][0m |          -0.0055 |           9.7079 |           3.3416 |
[32m[20230205 18:21:34 @agent_ppo2.py:191][0m |          -0.0072 |           9.5905 |           3.3364 |
[32m[20230205 18:21:34 @agent_ppo2.py:191][0m |          -0.0096 |           9.5532 |           3.3420 |
[32m[20230205 18:21:34 @agent_ppo2.py:191][0m |          -0.0088 |           9.5031 |           3.3370 |
[32m[20230205 18:21:35 @agent_ppo2.py:191][0m |          -0.0151 |           9.3352 |           3.3396 |
[32m[20230205 18:21:35 @agent_ppo2.py:191][0m |          -0.0148 |           9.2506 |           3.3372 |
[32m[20230205 18:21:35 @agent_ppo2.py:191][0m |          -0.0143 |           9.2006 |           3.3366 |
[32m[20230205 18:21:35 @agent_ppo2.py:191][0m |          -0.0118 |           9.2666 |           3.3361 |
[32m[20230205 18:21:35 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:21:35 @agent_ppo2.py:144][0m Average TRAINING episode reward: 252.01
[32m[20230205 18:21:35 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.63
[32m[20230205 18:21:35 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 264.14
[32m[20230205 18:21:35 @agent_ppo2.py:149][0m Total time:       8.13 min
[32m[20230205 18:21:35 @agent_ppo2.py:151][0m 520192 total steps have happened
[32m[20230205 18:21:35 @agent_ppo2.py:127][0m #------------------------ Iteration 254 --------------------------#
[32m[20230205 18:21:36 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:21:36 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:36 @agent_ppo2.py:191][0m |           0.0019 |          30.8308 |           3.3171 |
[32m[20230205 18:21:36 @agent_ppo2.py:191][0m |          -0.0032 |          24.8936 |           3.3140 |
[32m[20230205 18:21:36 @agent_ppo2.py:191][0m |          -0.0081 |          23.0988 |           3.3164 |
[32m[20230205 18:21:36 @agent_ppo2.py:191][0m |          -0.0092 |          22.2338 |           3.3158 |
[32m[20230205 18:21:36 @agent_ppo2.py:191][0m |          -0.0103 |          21.5179 |           3.3182 |
[32m[20230205 18:21:36 @agent_ppo2.py:191][0m |          -0.0110 |          20.9592 |           3.3175 |
[32m[20230205 18:21:36 @agent_ppo2.py:191][0m |          -0.0117 |          20.1203 |           3.3215 |
[32m[20230205 18:21:36 @agent_ppo2.py:191][0m |          -0.0116 |          19.3709 |           3.3203 |
[32m[20230205 18:21:36 @agent_ppo2.py:191][0m |          -0.0139 |          18.8652 |           3.3195 |
[32m[20230205 18:21:36 @agent_ppo2.py:191][0m |          -0.0122 |          18.1119 |           3.3225 |
[32m[20230205 18:21:36 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:21:37 @agent_ppo2.py:144][0m Average TRAINING episode reward: 183.10
[32m[20230205 18:21:37 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 243.01
[32m[20230205 18:21:37 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 264.33
[32m[20230205 18:21:37 @agent_ppo2.py:149][0m Total time:       8.16 min
[32m[20230205 18:21:37 @agent_ppo2.py:151][0m 522240 total steps have happened
[32m[20230205 18:21:37 @agent_ppo2.py:127][0m #------------------------ Iteration 255 --------------------------#
[32m[20230205 18:21:38 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:21:38 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:38 @agent_ppo2.py:191][0m |           0.0004 |          44.8109 |           3.3812 |
[32m[20230205 18:21:38 @agent_ppo2.py:191][0m |          -0.0053 |          33.2076 |           3.3843 |
[32m[20230205 18:21:38 @agent_ppo2.py:191][0m |          -0.0066 |          30.5109 |           3.3822 |
[32m[20230205 18:21:38 @agent_ppo2.py:191][0m |          -0.0087 |          28.6185 |           3.3770 |
[32m[20230205 18:21:38 @agent_ppo2.py:191][0m |          -0.0108 |          28.2105 |           3.3758 |
[32m[20230205 18:21:38 @agent_ppo2.py:191][0m |          -0.0120 |          27.2577 |           3.3752 |
[32m[20230205 18:21:38 @agent_ppo2.py:191][0m |          -0.0143 |          27.0915 |           3.3726 |
[32m[20230205 18:21:38 @agent_ppo2.py:191][0m |          -0.0154 |          26.5328 |           3.3717 |
[32m[20230205 18:21:38 @agent_ppo2.py:191][0m |          -0.0133 |          25.2565 |           3.3704 |
[32m[20230205 18:21:38 @agent_ppo2.py:191][0m |          -0.0156 |          24.9121 |           3.3704 |
[32m[20230205 18:21:38 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:21:39 @agent_ppo2.py:144][0m Average TRAINING episode reward: 247.78
[32m[20230205 18:21:39 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.21
[32m[20230205 18:21:39 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 167.00
[32m[20230205 18:21:39 @agent_ppo2.py:149][0m Total time:       8.19 min
[32m[20230205 18:21:39 @agent_ppo2.py:151][0m 524288 total steps have happened
[32m[20230205 18:21:39 @agent_ppo2.py:127][0m #------------------------ Iteration 256 --------------------------#
[32m[20230205 18:21:39 @agent_ppo2.py:133][0m Sampling time: 0.58 s by 1 slaves
[32m[20230205 18:21:39 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:39 @agent_ppo2.py:191][0m |           0.0009 |           9.0315 |           3.3756 |
[32m[20230205 18:21:40 @agent_ppo2.py:191][0m |          -0.0078 |           8.4578 |           3.3643 |
[32m[20230205 18:21:40 @agent_ppo2.py:191][0m |          -0.0085 |           8.1689 |           3.3616 |
[32m[20230205 18:21:40 @agent_ppo2.py:191][0m |          -0.0112 |           7.9469 |           3.3573 |
[32m[20230205 18:21:40 @agent_ppo2.py:191][0m |          -0.0112 |           7.8865 |           3.3582 |
[32m[20230205 18:21:40 @agent_ppo2.py:191][0m |          -0.0113 |           7.6362 |           3.3541 |
[32m[20230205 18:21:40 @agent_ppo2.py:191][0m |          -0.0113 |           7.5371 |           3.3554 |
[32m[20230205 18:21:40 @agent_ppo2.py:191][0m |          -0.0120 |           7.4816 |           3.3556 |
[32m[20230205 18:21:40 @agent_ppo2.py:191][0m |          -0.0139 |           7.2140 |           3.3547 |
[32m[20230205 18:21:40 @agent_ppo2.py:191][0m |          -0.0130 |           7.1446 |           3.3518 |
[32m[20230205 18:21:40 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:21:41 @agent_ppo2.py:144][0m Average TRAINING episode reward: 252.37
[32m[20230205 18:21:41 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.66
[32m[20230205 18:21:41 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 107.68
[32m[20230205 18:21:41 @agent_ppo2.py:149][0m Total time:       8.22 min
[32m[20230205 18:21:41 @agent_ppo2.py:151][0m 526336 total steps have happened
[32m[20230205 18:21:41 @agent_ppo2.py:127][0m #------------------------ Iteration 257 --------------------------#
[32m[20230205 18:21:41 @agent_ppo2.py:133][0m Sampling time: 0.64 s by 1 slaves
[32m[20230205 18:21:41 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:41 @agent_ppo2.py:191][0m |          -0.0013 |          37.1518 |           3.3246 |
[32m[20230205 18:21:41 @agent_ppo2.py:191][0m |          -0.0084 |          15.3258 |           3.3233 |
[32m[20230205 18:21:41 @agent_ppo2.py:191][0m |          -0.0073 |          10.8169 |           3.3171 |
[32m[20230205 18:21:41 @agent_ppo2.py:191][0m |          -0.0099 |           9.1367 |           3.3165 |
[32m[20230205 18:21:41 @agent_ppo2.py:191][0m |          -0.0126 |           8.6592 |           3.3113 |
[32m[20230205 18:21:42 @agent_ppo2.py:191][0m |          -0.0125 |           8.1676 |           3.3089 |
[32m[20230205 18:21:42 @agent_ppo2.py:191][0m |          -0.0141 |           8.0852 |           3.3090 |
[32m[20230205 18:21:42 @agent_ppo2.py:191][0m |          -0.0122 |           7.6824 |           3.3062 |
[32m[20230205 18:21:42 @agent_ppo2.py:191][0m |          -0.0154 |           7.4111 |           3.3060 |
[32m[20230205 18:21:42 @agent_ppo2.py:191][0m |          -0.0147 |           7.2436 |           3.3018 |
[32m[20230205 18:21:42 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:21:42 @agent_ppo2.py:144][0m Average TRAINING episode reward: 142.13
[32m[20230205 18:21:42 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.11
[32m[20230205 18:21:42 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 258.24
[32m[20230205 18:21:42 @agent_ppo2.py:149][0m Total time:       8.25 min
[32m[20230205 18:21:42 @agent_ppo2.py:151][0m 528384 total steps have happened
[32m[20230205 18:21:42 @agent_ppo2.py:127][0m #------------------------ Iteration 258 --------------------------#
[32m[20230205 18:21:43 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:21:43 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:43 @agent_ppo2.py:191][0m |          -0.0010 |          23.1160 |           3.3324 |
[32m[20230205 18:21:43 @agent_ppo2.py:191][0m |          -0.0069 |          17.9146 |           3.3255 |
[32m[20230205 18:21:43 @agent_ppo2.py:191][0m |          -0.0096 |          16.6443 |           3.3225 |
[32m[20230205 18:21:43 @agent_ppo2.py:191][0m |          -0.0132 |          15.7162 |           3.3205 |
[32m[20230205 18:21:43 @agent_ppo2.py:191][0m |          -0.0140 |          15.0329 |           3.3174 |
[32m[20230205 18:21:43 @agent_ppo2.py:191][0m |          -0.0134 |          14.5124 |           3.3154 |
[32m[20230205 18:21:43 @agent_ppo2.py:191][0m |          -0.0156 |          14.0626 |           3.3175 |
[32m[20230205 18:21:43 @agent_ppo2.py:191][0m |          -0.0169 |          13.6384 |           3.3138 |
[32m[20230205 18:21:43 @agent_ppo2.py:191][0m |          -0.0187 |          13.3700 |           3.3164 |
[32m[20230205 18:21:43 @agent_ppo2.py:191][0m |          -0.0190 |          13.0636 |           3.3179 |
[32m[20230205 18:21:43 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:21:44 @agent_ppo2.py:144][0m Average TRAINING episode reward: 251.49
[32m[20230205 18:21:44 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.03
[32m[20230205 18:21:44 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 264.89
[32m[20230205 18:21:44 @agent_ppo2.py:149][0m Total time:       8.28 min
[32m[20230205 18:21:44 @agent_ppo2.py:151][0m 530432 total steps have happened
[32m[20230205 18:21:44 @agent_ppo2.py:127][0m #------------------------ Iteration 259 --------------------------#
[32m[20230205 18:21:45 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:21:45 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:45 @agent_ppo2.py:191][0m |          -0.0011 |          12.0711 |           3.2556 |
[32m[20230205 18:21:45 @agent_ppo2.py:191][0m |           0.0003 |          12.2411 |           3.2541 |
[32m[20230205 18:21:45 @agent_ppo2.py:191][0m |           0.0020 |          12.2064 |           3.2530 |
[32m[20230205 18:21:45 @agent_ppo2.py:191][0m |          -0.0041 |          11.7503 |           3.2543 |
[32m[20230205 18:21:45 @agent_ppo2.py:191][0m |          -0.0093 |          10.8877 |           3.2554 |
[32m[20230205 18:21:45 @agent_ppo2.py:191][0m |          -0.0137 |          10.7324 |           3.2512 |
[32m[20230205 18:21:45 @agent_ppo2.py:191][0m |          -0.0167 |          10.6508 |           3.2533 |
[32m[20230205 18:21:45 @agent_ppo2.py:191][0m |          -0.0135 |          10.5969 |           3.2539 |
[32m[20230205 18:21:45 @agent_ppo2.py:191][0m |          -0.0157 |          10.5296 |           3.2512 |
[32m[20230205 18:21:45 @agent_ppo2.py:191][0m |          -0.0186 |          10.4746 |           3.2512 |
[32m[20230205 18:21:45 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:21:46 @agent_ppo2.py:144][0m Average TRAINING episode reward: 253.94
[32m[20230205 18:21:46 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.38
[32m[20230205 18:21:46 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 258.84
[32m[20230205 18:21:46 @agent_ppo2.py:149][0m Total time:       8.30 min
[32m[20230205 18:21:46 @agent_ppo2.py:151][0m 532480 total steps have happened
[32m[20230205 18:21:46 @agent_ppo2.py:127][0m #------------------------ Iteration 260 --------------------------#
[32m[20230205 18:21:46 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:21:46 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:46 @agent_ppo2.py:191][0m |          -0.0007 |          10.9761 |           3.3686 |
[32m[20230205 18:21:46 @agent_ppo2.py:191][0m |          -0.0045 |          10.6037 |           3.3629 |
[32m[20230205 18:21:46 @agent_ppo2.py:191][0m |          -0.0065 |          10.3869 |           3.3580 |
[32m[20230205 18:21:46 @agent_ppo2.py:191][0m |          -0.0081 |          10.2408 |           3.3572 |
[32m[20230205 18:21:47 @agent_ppo2.py:191][0m |          -0.0089 |          10.1098 |           3.3510 |
[32m[20230205 18:21:47 @agent_ppo2.py:191][0m |          -0.0095 |           9.9849 |           3.3511 |
[32m[20230205 18:21:47 @agent_ppo2.py:191][0m |          -0.0105 |           9.8892 |           3.3468 |
[32m[20230205 18:21:47 @agent_ppo2.py:191][0m |          -0.0114 |           9.7927 |           3.3462 |
[32m[20230205 18:21:47 @agent_ppo2.py:191][0m |          -0.0113 |           9.7241 |           3.3435 |
[32m[20230205 18:21:47 @agent_ppo2.py:191][0m |          -0.0120 |           9.6430 |           3.3408 |
[32m[20230205 18:21:47 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:21:47 @agent_ppo2.py:144][0m Average TRAINING episode reward: 252.92
[32m[20230205 18:21:47 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.42
[32m[20230205 18:21:47 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 259.12
[32m[20230205 18:21:47 @agent_ppo2.py:149][0m Total time:       8.33 min
[32m[20230205 18:21:47 @agent_ppo2.py:151][0m 534528 total steps have happened
[32m[20230205 18:21:47 @agent_ppo2.py:127][0m #------------------------ Iteration 261 --------------------------#
[32m[20230205 18:21:48 @agent_ppo2.py:133][0m Sampling time: 0.58 s by 1 slaves
[32m[20230205 18:21:48 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:48 @agent_ppo2.py:191][0m |           0.0007 |          10.9422 |           3.2807 |
[32m[20230205 18:21:48 @agent_ppo2.py:191][0m |          -0.0049 |          10.2815 |           3.2732 |
[32m[20230205 18:21:48 @agent_ppo2.py:191][0m |          -0.0076 |           9.9192 |           3.2702 |
[32m[20230205 18:21:48 @agent_ppo2.py:191][0m |          -0.0092 |           9.6254 |           3.2677 |
[32m[20230205 18:21:48 @agent_ppo2.py:191][0m |          -0.0106 |           9.4101 |           3.2665 |
[32m[20230205 18:21:48 @agent_ppo2.py:191][0m |          -0.0114 |           9.2228 |           3.2632 |
[32m[20230205 18:21:48 @agent_ppo2.py:191][0m |          -0.0120 |           9.0635 |           3.2618 |
[32m[20230205 18:21:48 @agent_ppo2.py:191][0m |          -0.0128 |           8.9854 |           3.2595 |
[32m[20230205 18:21:48 @agent_ppo2.py:191][0m |          -0.0129 |           8.8239 |           3.2629 |
[32m[20230205 18:21:48 @agent_ppo2.py:191][0m |          -0.0134 |           8.6979 |           3.2609 |
[32m[20230205 18:21:48 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:21:49 @agent_ppo2.py:144][0m Average TRAINING episode reward: 249.29
[32m[20230205 18:21:49 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.62
[32m[20230205 18:21:49 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 114.63
[32m[20230205 18:21:49 @agent_ppo2.py:149][0m Total time:       8.36 min
[32m[20230205 18:21:49 @agent_ppo2.py:151][0m 536576 total steps have happened
[32m[20230205 18:21:49 @agent_ppo2.py:127][0m #------------------------ Iteration 262 --------------------------#
[32m[20230205 18:21:50 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:21:50 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:50 @agent_ppo2.py:191][0m |           0.0012 |          12.0370 |           3.3084 |
[32m[20230205 18:21:50 @agent_ppo2.py:191][0m |          -0.0064 |          11.5152 |           3.3165 |
[32m[20230205 18:21:50 @agent_ppo2.py:191][0m |          -0.0091 |          11.3287 |           3.3121 |
[32m[20230205 18:21:50 @agent_ppo2.py:191][0m |          -0.0109 |          11.2188 |           3.3119 |
[32m[20230205 18:21:50 @agent_ppo2.py:191][0m |          -0.0136 |          11.0022 |           3.3126 |
[32m[20230205 18:21:50 @agent_ppo2.py:191][0m |          -0.0145 |          10.9040 |           3.3105 |
[32m[20230205 18:21:50 @agent_ppo2.py:191][0m |          -0.0145 |          10.8711 |           3.3094 |
[32m[20230205 18:21:50 @agent_ppo2.py:191][0m |          -0.0126 |          10.9715 |           3.3070 |
[32m[20230205 18:21:50 @agent_ppo2.py:191][0m |          -0.0152 |          10.7254 |           3.3086 |
[32m[20230205 18:21:50 @agent_ppo2.py:191][0m |          -0.0166 |          10.5672 |           3.3063 |
[32m[20230205 18:21:50 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:21:51 @agent_ppo2.py:144][0m Average TRAINING episode reward: 251.40
[32m[20230205 18:21:51 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.64
[32m[20230205 18:21:51 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 139.94
[32m[20230205 18:21:51 @agent_ppo2.py:149][0m Total time:       8.39 min
[32m[20230205 18:21:51 @agent_ppo2.py:151][0m 538624 total steps have happened
[32m[20230205 18:21:51 @agent_ppo2.py:127][0m #------------------------ Iteration 263 --------------------------#
[32m[20230205 18:21:51 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:21:51 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:51 @agent_ppo2.py:191][0m |           0.0020 |          38.0664 |           3.3363 |
[32m[20230205 18:21:51 @agent_ppo2.py:191][0m |          -0.0042 |          22.0918 |           3.3334 |
[32m[20230205 18:21:51 @agent_ppo2.py:191][0m |          -0.0070 |          17.2199 |           3.3315 |
[32m[20230205 18:21:51 @agent_ppo2.py:191][0m |          -0.0088 |          15.3189 |           3.3267 |
[32m[20230205 18:21:51 @agent_ppo2.py:191][0m |          -0.0107 |          14.1263 |           3.3279 |
[32m[20230205 18:21:52 @agent_ppo2.py:191][0m |          -0.0110 |          12.8363 |           3.3308 |
[32m[20230205 18:21:52 @agent_ppo2.py:191][0m |          -0.0125 |          12.5889 |           3.3309 |
[32m[20230205 18:21:52 @agent_ppo2.py:191][0m |          -0.0132 |          11.5851 |           3.3309 |
[32m[20230205 18:21:52 @agent_ppo2.py:191][0m |          -0.0128 |          10.8811 |           3.3312 |
[32m[20230205 18:21:52 @agent_ppo2.py:191][0m |          -0.0130 |          10.5924 |           3.3311 |
[32m[20230205 18:21:52 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:21:52 @agent_ppo2.py:144][0m Average TRAINING episode reward: 189.94
[32m[20230205 18:21:52 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 247.15
[32m[20230205 18:21:52 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 252.48
[32m[20230205 18:21:52 @agent_ppo2.py:149][0m Total time:       8.42 min
[32m[20230205 18:21:52 @agent_ppo2.py:151][0m 540672 total steps have happened
[32m[20230205 18:21:52 @agent_ppo2.py:127][0m #------------------------ Iteration 264 --------------------------#
[32m[20230205 18:21:53 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:21:53 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:53 @agent_ppo2.py:191][0m |           0.0022 |          11.4878 |           3.3310 |
[32m[20230205 18:21:53 @agent_ppo2.py:191][0m |          -0.0067 |           9.5781 |           3.3264 |
[32m[20230205 18:21:53 @agent_ppo2.py:191][0m |          -0.0041 |           9.5008 |           3.3239 |
[32m[20230205 18:21:53 @agent_ppo2.py:191][0m |          -0.0102 |           8.8878 |           3.3243 |
[32m[20230205 18:21:53 @agent_ppo2.py:191][0m |          -0.0106 |           8.5324 |           3.3239 |
[32m[20230205 18:21:53 @agent_ppo2.py:191][0m |          -0.0104 |           8.1936 |           3.3229 |
[32m[20230205 18:21:53 @agent_ppo2.py:191][0m |          -0.0144 |           7.7326 |           3.3230 |
[32m[20230205 18:21:53 @agent_ppo2.py:191][0m |          -0.0129 |           7.3322 |           3.3239 |
[32m[20230205 18:21:53 @agent_ppo2.py:191][0m |          -0.0142 |           6.9385 |           3.3255 |
[32m[20230205 18:21:53 @agent_ppo2.py:191][0m |          -0.0154 |           6.6691 |           3.3258 |
[32m[20230205 18:21:53 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:21:54 @agent_ppo2.py:144][0m Average TRAINING episode reward: 254.89
[32m[20230205 18:21:54 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.00
[32m[20230205 18:21:54 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 260.40
[32m[20230205 18:21:54 @agent_ppo2.py:149][0m Total time:       8.44 min
[32m[20230205 18:21:54 @agent_ppo2.py:151][0m 542720 total steps have happened
[32m[20230205 18:21:54 @agent_ppo2.py:127][0m #------------------------ Iteration 265 --------------------------#
[32m[20230205 18:21:55 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:21:55 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:55 @agent_ppo2.py:191][0m |           0.0025 |          14.0822 |           3.3609 |
[32m[20230205 18:21:55 @agent_ppo2.py:191][0m |          -0.0040 |          11.8356 |           3.3600 |
[32m[20230205 18:21:55 @agent_ppo2.py:191][0m |          -0.0085 |          11.2982 |           3.3555 |
[32m[20230205 18:21:55 @agent_ppo2.py:191][0m |          -0.0126 |          10.9499 |           3.3548 |
[32m[20230205 18:21:55 @agent_ppo2.py:191][0m |          -0.0092 |          10.7300 |           3.3535 |
[32m[20230205 18:21:55 @agent_ppo2.py:191][0m |          -0.0106 |          10.6436 |           3.3516 |
[32m[20230205 18:21:55 @agent_ppo2.py:191][0m |          -0.0131 |          10.2948 |           3.3510 |
[32m[20230205 18:21:55 @agent_ppo2.py:191][0m |          -0.0140 |          10.1551 |           3.3514 |
[32m[20230205 18:21:55 @agent_ppo2.py:191][0m |          -0.0130 |          10.0450 |           3.3463 |
[32m[20230205 18:21:55 @agent_ppo2.py:191][0m |          -0.0151 |           9.9081 |           3.3473 |
[32m[20230205 18:21:55 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:21:56 @agent_ppo2.py:144][0m Average TRAINING episode reward: 251.43
[32m[20230205 18:21:56 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.59
[32m[20230205 18:21:56 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 171.41
[32m[20230205 18:21:56 @agent_ppo2.py:149][0m Total time:       8.47 min
[32m[20230205 18:21:56 @agent_ppo2.py:151][0m 544768 total steps have happened
[32m[20230205 18:21:56 @agent_ppo2.py:127][0m #------------------------ Iteration 266 --------------------------#
[32m[20230205 18:21:56 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:21:56 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:56 @agent_ppo2.py:191][0m |          -0.0084 |          39.0045 |           3.3014 |
[32m[20230205 18:21:56 @agent_ppo2.py:191][0m |          -0.0056 |          28.8094 |           3.2962 |
[32m[20230205 18:21:57 @agent_ppo2.py:191][0m |          -0.0167 |          25.2165 |           3.2942 |
[32m[20230205 18:21:57 @agent_ppo2.py:191][0m |          -0.0266 |          23.0853 |           3.2938 |
[32m[20230205 18:21:57 @agent_ppo2.py:191][0m |          -0.0175 |          21.9142 |           3.2896 |
[32m[20230205 18:21:57 @agent_ppo2.py:191][0m |          -0.0209 |          20.7907 |           3.2923 |
[32m[20230205 18:21:57 @agent_ppo2.py:191][0m |          -0.0231 |          20.2044 |           3.2914 |
[32m[20230205 18:21:57 @agent_ppo2.py:191][0m |          -0.0179 |          19.6695 |           3.2908 |
[32m[20230205 18:21:57 @agent_ppo2.py:191][0m |          -0.0147 |          19.1529 |           3.2900 |
[32m[20230205 18:21:57 @agent_ppo2.py:191][0m |          -0.0138 |          18.7893 |           3.2902 |
[32m[20230205 18:21:57 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:21:57 @agent_ppo2.py:144][0m Average TRAINING episode reward: 194.35
[32m[20230205 18:21:57 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.42
[32m[20230205 18:21:57 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 157.70
[32m[20230205 18:21:57 @agent_ppo2.py:149][0m Total time:       8.50 min
[32m[20230205 18:21:57 @agent_ppo2.py:151][0m 546816 total steps have happened
[32m[20230205 18:21:57 @agent_ppo2.py:127][0m #------------------------ Iteration 267 --------------------------#
[32m[20230205 18:21:58 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:21:58 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:21:58 @agent_ppo2.py:191][0m |           0.0006 |          13.2318 |           3.3836 |
[32m[20230205 18:21:58 @agent_ppo2.py:191][0m |          -0.0052 |          12.1445 |           3.3731 |
[32m[20230205 18:21:58 @agent_ppo2.py:191][0m |          -0.0092 |          11.7266 |           3.3666 |
[32m[20230205 18:21:58 @agent_ppo2.py:191][0m |          -0.0071 |          11.7791 |           3.3671 |
[32m[20230205 18:21:58 @agent_ppo2.py:191][0m |          -0.0099 |          11.4234 |           3.3671 |
[32m[20230205 18:21:58 @agent_ppo2.py:191][0m |          -0.0116 |          11.3151 |           3.3678 |
[32m[20230205 18:21:58 @agent_ppo2.py:191][0m |          -0.0135 |          11.1631 |           3.3671 |
[32m[20230205 18:21:58 @agent_ppo2.py:191][0m |          -0.0128 |          11.0983 |           3.3673 |
[32m[20230205 18:21:58 @agent_ppo2.py:191][0m |          -0.0163 |          11.0341 |           3.3647 |
[32m[20230205 18:21:58 @agent_ppo2.py:191][0m |          -0.0131 |          11.3147 |           3.3624 |
[32m[20230205 18:21:58 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:21:59 @agent_ppo2.py:144][0m Average TRAINING episode reward: 252.70
[32m[20230205 18:21:59 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.71
[32m[20230205 18:21:59 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 174.95
[32m[20230205 18:21:59 @agent_ppo2.py:149][0m Total time:       8.53 min
[32m[20230205 18:21:59 @agent_ppo2.py:151][0m 548864 total steps have happened
[32m[20230205 18:21:59 @agent_ppo2.py:127][0m #------------------------ Iteration 268 --------------------------#
[32m[20230205 18:22:00 @agent_ppo2.py:133][0m Sampling time: 0.72 s by 1 slaves
[32m[20230205 18:22:00 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:00 @agent_ppo2.py:191][0m |          -0.0018 |          31.0543 |           3.2929 |
[32m[20230205 18:22:00 @agent_ppo2.py:191][0m |          -0.0056 |          19.9755 |           3.2919 |
[32m[20230205 18:22:00 @agent_ppo2.py:191][0m |          -0.0060 |          18.1056 |           3.2912 |
[32m[20230205 18:22:00 @agent_ppo2.py:191][0m |          -0.0067 |          17.5324 |           3.2908 |
[32m[20230205 18:22:00 @agent_ppo2.py:191][0m |          -0.0080 |          16.3573 |           3.2849 |
[32m[20230205 18:22:00 @agent_ppo2.py:191][0m |          -0.0077 |          15.9965 |           3.2894 |
[32m[20230205 18:22:00 @agent_ppo2.py:191][0m |          -0.0096 |          15.5122 |           3.2840 |
[32m[20230205 18:22:00 @agent_ppo2.py:191][0m |          -0.0100 |          15.3495 |           3.2882 |
[32m[20230205 18:22:00 @agent_ppo2.py:191][0m |          -0.0077 |          14.7537 |           3.2912 |
[32m[20230205 18:22:00 @agent_ppo2.py:191][0m |          -0.0115 |          14.5055 |           3.2870 |
[32m[20230205 18:22:00 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:22:01 @agent_ppo2.py:144][0m Average TRAINING episode reward: 179.67
[32m[20230205 18:22:01 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.33
[32m[20230205 18:22:01 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 260.93
[32m[20230205 18:22:01 @agent_ppo2.py:149][0m Total time:       8.56 min
[32m[20230205 18:22:01 @agent_ppo2.py:151][0m 550912 total steps have happened
[32m[20230205 18:22:01 @agent_ppo2.py:127][0m #------------------------ Iteration 269 --------------------------#
[32m[20230205 18:22:01 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:22:02 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:02 @agent_ppo2.py:191][0m |          -0.0010 |          15.9515 |           3.2790 |
[32m[20230205 18:22:02 @agent_ppo2.py:191][0m |          -0.0058 |          13.8608 |           3.2749 |
[32m[20230205 18:22:02 @agent_ppo2.py:191][0m |          -0.0095 |          12.9287 |           3.2702 |
[32m[20230205 18:22:02 @agent_ppo2.py:191][0m |          -0.0083 |          12.5848 |           3.2689 |
[32m[20230205 18:22:02 @agent_ppo2.py:191][0m |          -0.0119 |          12.2052 |           3.2692 |
[32m[20230205 18:22:02 @agent_ppo2.py:191][0m |          -0.0131 |          12.0605 |           3.2668 |
[32m[20230205 18:22:02 @agent_ppo2.py:191][0m |          -0.0138 |          11.8660 |           3.2705 |
[32m[20230205 18:22:02 @agent_ppo2.py:191][0m |          -0.0149 |          11.7114 |           3.2716 |
[32m[20230205 18:22:02 @agent_ppo2.py:191][0m |          -0.0155 |          11.5789 |           3.2726 |
[32m[20230205 18:22:02 @agent_ppo2.py:191][0m |          -0.0158 |          11.5726 |           3.2729 |
[32m[20230205 18:22:02 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:22:03 @agent_ppo2.py:144][0m Average TRAINING episode reward: 242.89
[32m[20230205 18:22:03 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 250.41
[32m[20230205 18:22:03 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 260.97
[32m[20230205 18:22:03 @agent_ppo2.py:149][0m Total time:       8.59 min
[32m[20230205 18:22:03 @agent_ppo2.py:151][0m 552960 total steps have happened
[32m[20230205 18:22:03 @agent_ppo2.py:127][0m #------------------------ Iteration 270 --------------------------#
[32m[20230205 18:22:03 @agent_ppo2.py:133][0m Sampling time: 0.48 s by 1 slaves
[32m[20230205 18:22:03 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:03 @agent_ppo2.py:191][0m |          -0.0032 |          29.8154 |           3.2436 |
[32m[20230205 18:22:03 @agent_ppo2.py:191][0m |           0.0190 |          17.9080 |           3.2388 |
[32m[20230205 18:22:03 @agent_ppo2.py:191][0m |          -0.0107 |          13.9056 |           3.2281 |
[32m[20230205 18:22:03 @agent_ppo2.py:191][0m |          -0.0134 |          11.7599 |           3.2323 |
[32m[20230205 18:22:03 @agent_ppo2.py:191][0m |          -0.0174 |          11.0867 |           3.2344 |
[32m[20230205 18:22:03 @agent_ppo2.py:191][0m |          -0.0013 |          11.3712 |           3.2401 |
[32m[20230205 18:22:03 @agent_ppo2.py:191][0m |          -0.0126 |          10.1634 |           3.2389 |
[32m[20230205 18:22:04 @agent_ppo2.py:191][0m |          -0.0182 |           9.7245 |           3.2387 |
[32m[20230205 18:22:04 @agent_ppo2.py:191][0m |          -0.0173 |           9.4543 |           3.2404 |
[32m[20230205 18:22:04 @agent_ppo2.py:191][0m |          -0.0183 |           9.1827 |           3.2376 |
[32m[20230205 18:22:04 @agent_ppo2.py:136][0m Policy update time: 0.49 s
[32m[20230205 18:22:04 @agent_ppo2.py:144][0m Average TRAINING episode reward: 153.04
[32m[20230205 18:22:04 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.26
[32m[20230205 18:22:04 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 265.98
[32m[20230205 18:22:04 @agent_ppo2.py:149][0m Total time:       8.61 min
[32m[20230205 18:22:04 @agent_ppo2.py:151][0m 555008 total steps have happened
[32m[20230205 18:22:04 @agent_ppo2.py:127][0m #------------------------ Iteration 271 --------------------------#
[32m[20230205 18:22:05 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:22:05 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:05 @agent_ppo2.py:191][0m |           0.0011 |          13.1302 |           3.3078 |
[32m[20230205 18:22:05 @agent_ppo2.py:191][0m |          -0.0034 |          12.1594 |           3.2999 |
[32m[20230205 18:22:05 @agent_ppo2.py:191][0m |          -0.0063 |          11.7442 |           3.3000 |
[32m[20230205 18:22:05 @agent_ppo2.py:191][0m |          -0.0075 |          11.6056 |           3.2966 |
[32m[20230205 18:22:05 @agent_ppo2.py:191][0m |          -0.0085 |          11.5112 |           3.2982 |
[32m[20230205 18:22:05 @agent_ppo2.py:191][0m |          -0.0097 |          11.3697 |           3.2981 |
[32m[20230205 18:22:05 @agent_ppo2.py:191][0m |          -0.0108 |          11.3290 |           3.2954 |
[32m[20230205 18:22:05 @agent_ppo2.py:191][0m |          -0.0116 |          11.2139 |           3.2992 |
[32m[20230205 18:22:05 @agent_ppo2.py:191][0m |          -0.0107 |          11.2388 |           3.2962 |
[32m[20230205 18:22:05 @agent_ppo2.py:191][0m |          -0.0133 |          11.0872 |           3.2974 |
[32m[20230205 18:22:05 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:22:06 @agent_ppo2.py:144][0m Average TRAINING episode reward: 255.66
[32m[20230205 18:22:06 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.65
[32m[20230205 18:22:06 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 261.91
[32m[20230205 18:22:06 @agent_ppo2.py:149][0m Total time:       8.64 min
[32m[20230205 18:22:06 @agent_ppo2.py:151][0m 557056 total steps have happened
[32m[20230205 18:22:06 @agent_ppo2.py:127][0m #------------------------ Iteration 272 --------------------------#
[32m[20230205 18:22:06 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:22:06 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:06 @agent_ppo2.py:191][0m |           0.0018 |           9.7648 |           3.3264 |
[32m[20230205 18:22:07 @agent_ppo2.py:191][0m |          -0.0018 |           9.3680 |           3.3196 |
[32m[20230205 18:22:07 @agent_ppo2.py:191][0m |          -0.0063 |           8.6739 |           3.3155 |
[32m[20230205 18:22:07 @agent_ppo2.py:191][0m |          -0.0065 |           8.4664 |           3.3165 |
[32m[20230205 18:22:07 @agent_ppo2.py:191][0m |          -0.0096 |           8.3127 |           3.3135 |
[32m[20230205 18:22:07 @agent_ppo2.py:191][0m |          -0.0090 |           8.2279 |           3.3127 |
[32m[20230205 18:22:07 @agent_ppo2.py:191][0m |          -0.0112 |           8.1243 |           3.3142 |
[32m[20230205 18:22:07 @agent_ppo2.py:191][0m |          -0.0056 |           8.1620 |           3.3131 |
[32m[20230205 18:22:07 @agent_ppo2.py:191][0m |          -0.0084 |           8.1033 |           3.3138 |
[32m[20230205 18:22:07 @agent_ppo2.py:191][0m |          -0.0133 |           7.8242 |           3.3130 |
[32m[20230205 18:22:07 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:22:07 @agent_ppo2.py:144][0m Average TRAINING episode reward: 246.47
[32m[20230205 18:22:07 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 251.44
[32m[20230205 18:22:07 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 268.46
[32m[20230205 18:22:07 @agent_ppo2.py:149][0m Total time:       8.67 min
[32m[20230205 18:22:07 @agent_ppo2.py:151][0m 559104 total steps have happened
[32m[20230205 18:22:07 @agent_ppo2.py:127][0m #------------------------ Iteration 273 --------------------------#
[32m[20230205 18:22:08 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:22:08 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:08 @agent_ppo2.py:191][0m |          -0.0018 |          10.7489 |           3.2110 |
[32m[20230205 18:22:08 @agent_ppo2.py:191][0m |          -0.0057 |           9.9905 |           3.2035 |
[32m[20230205 18:22:08 @agent_ppo2.py:191][0m |          -0.0086 |           9.5733 |           3.2008 |
[32m[20230205 18:22:08 @agent_ppo2.py:191][0m |          -0.0109 |           9.4114 |           3.1998 |
[32m[20230205 18:22:08 @agent_ppo2.py:191][0m |          -0.0119 |           9.2729 |           3.2004 |
[32m[20230205 18:22:08 @agent_ppo2.py:191][0m |          -0.0152 |           9.1319 |           3.1989 |
[32m[20230205 18:22:09 @agent_ppo2.py:191][0m |          -0.0161 |           9.0684 |           3.1984 |
[32m[20230205 18:22:09 @agent_ppo2.py:191][0m |          -0.0114 |           9.2528 |           3.1990 |
[32m[20230205 18:22:09 @agent_ppo2.py:191][0m |          -0.0094 |           9.2588 |           3.1963 |
[32m[20230205 18:22:09 @agent_ppo2.py:191][0m |          -0.0163 |           8.7675 |           3.1962 |
[32m[20230205 18:22:09 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:22:09 @agent_ppo2.py:144][0m Average TRAINING episode reward: 248.87
[32m[20230205 18:22:09 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.92
[32m[20230205 18:22:09 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 263.88
[32m[20230205 18:22:09 @agent_ppo2.py:149][0m Total time:       8.70 min
[32m[20230205 18:22:09 @agent_ppo2.py:151][0m 561152 total steps have happened
[32m[20230205 18:22:09 @agent_ppo2.py:127][0m #------------------------ Iteration 274 --------------------------#
[32m[20230205 18:22:10 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:22:10 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:10 @agent_ppo2.py:191][0m |           0.0016 |          11.0731 |           3.2690 |
[32m[20230205 18:22:10 @agent_ppo2.py:191][0m |          -0.0108 |          10.3205 |           3.2667 |
[32m[20230205 18:22:10 @agent_ppo2.py:191][0m |          -0.0108 |           9.9602 |           3.2684 |
[32m[20230205 18:22:10 @agent_ppo2.py:191][0m |          -0.0096 |           9.8812 |           3.2648 |
[32m[20230205 18:22:10 @agent_ppo2.py:191][0m |          -0.0075 |          10.0578 |           3.2659 |
[32m[20230205 18:22:10 @agent_ppo2.py:191][0m |          -0.0137 |           9.1041 |           3.2647 |
[32m[20230205 18:22:10 @agent_ppo2.py:191][0m |          -0.0161 |           8.9301 |           3.2638 |
[32m[20230205 18:22:10 @agent_ppo2.py:191][0m |          -0.0098 |           9.1484 |           3.2641 |
[32m[20230205 18:22:10 @agent_ppo2.py:191][0m |          -0.0164 |           8.4421 |           3.2620 |
[32m[20230205 18:22:10 @agent_ppo2.py:191][0m |          -0.0070 |           9.0813 |           3.2630 |
[32m[20230205 18:22:10 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:22:11 @agent_ppo2.py:144][0m Average TRAINING episode reward: 256.22
[32m[20230205 18:22:11 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.10
[32m[20230205 18:22:11 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 262.48
[32m[20230205 18:22:11 @agent_ppo2.py:149][0m Total time:       8.72 min
[32m[20230205 18:22:11 @agent_ppo2.py:151][0m 563200 total steps have happened
[32m[20230205 18:22:11 @agent_ppo2.py:127][0m #------------------------ Iteration 275 --------------------------#
[32m[20230205 18:22:11 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:22:11 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:11 @agent_ppo2.py:191][0m |          -0.0030 |          11.9745 |           3.1928 |
[32m[20230205 18:22:12 @agent_ppo2.py:191][0m |          -0.0106 |          11.0032 |           3.1818 |
[32m[20230205 18:22:12 @agent_ppo2.py:191][0m |          -0.0178 |          10.6776 |           3.1792 |
[32m[20230205 18:22:12 @agent_ppo2.py:191][0m |          -0.0096 |          10.6607 |           3.1787 |
[32m[20230205 18:22:12 @agent_ppo2.py:191][0m |          -0.0164 |          10.3238 |           3.1764 |
[32m[20230205 18:22:12 @agent_ppo2.py:191][0m |          -0.0227 |          10.1811 |           3.1748 |
[32m[20230205 18:22:12 @agent_ppo2.py:191][0m |          -0.0155 |          10.0719 |           3.1738 |
[32m[20230205 18:22:12 @agent_ppo2.py:191][0m |          -0.0070 |          10.8971 |           3.1688 |
[32m[20230205 18:22:12 @agent_ppo2.py:191][0m |          -0.0165 |          10.0105 |           3.1695 |
[32m[20230205 18:22:12 @agent_ppo2.py:191][0m |          -0.0185 |           9.7483 |           3.1684 |
[32m[20230205 18:22:12 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:22:13 @agent_ppo2.py:144][0m Average TRAINING episode reward: 253.21
[32m[20230205 18:22:13 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.42
[32m[20230205 18:22:13 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 175.82
[32m[20230205 18:22:13 @agent_ppo2.py:149][0m Total time:       8.75 min
[32m[20230205 18:22:13 @agent_ppo2.py:151][0m 565248 total steps have happened
[32m[20230205 18:22:13 @agent_ppo2.py:127][0m #------------------------ Iteration 276 --------------------------#
[32m[20230205 18:22:13 @agent_ppo2.py:133][0m Sampling time: 0.58 s by 1 slaves
[32m[20230205 18:22:13 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:13 @agent_ppo2.py:191][0m |           0.0001 |          12.5191 |           3.2685 |
[32m[20230205 18:22:13 @agent_ppo2.py:191][0m |          -0.0037 |          11.3583 |           3.2667 |
[32m[20230205 18:22:13 @agent_ppo2.py:191][0m |          -0.0036 |          10.6907 |           3.2626 |
[32m[20230205 18:22:13 @agent_ppo2.py:191][0m |          -0.0027 |          10.4468 |           3.2589 |
[32m[20230205 18:22:14 @agent_ppo2.py:191][0m |          -0.0093 |          10.2627 |           3.2592 |
[32m[20230205 18:22:14 @agent_ppo2.py:191][0m |          -0.0106 |          10.0496 |           3.2569 |
[32m[20230205 18:22:14 @agent_ppo2.py:191][0m |          -0.0063 |          10.0839 |           3.2539 |
[32m[20230205 18:22:14 @agent_ppo2.py:191][0m |          -0.0099 |           9.8661 |           3.2536 |
[32m[20230205 18:22:14 @agent_ppo2.py:191][0m |          -0.0085 |           9.7341 |           3.2546 |
[32m[20230205 18:22:14 @agent_ppo2.py:191][0m |          -0.0131 |           9.5607 |           3.2491 |
[32m[20230205 18:22:14 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:22:14 @agent_ppo2.py:144][0m Average TRAINING episode reward: 249.21
[32m[20230205 18:22:14 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 249.98
[32m[20230205 18:22:14 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.31
[32m[20230205 18:22:14 @agent_ppo2.py:149][0m Total time:       8.78 min
[32m[20230205 18:22:14 @agent_ppo2.py:151][0m 567296 total steps have happened
[32m[20230205 18:22:14 @agent_ppo2.py:127][0m #------------------------ Iteration 277 --------------------------#
[32m[20230205 18:22:15 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:22:15 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:15 @agent_ppo2.py:191][0m |           0.0009 |          26.9056 |           3.2729 |
[32m[20230205 18:22:15 @agent_ppo2.py:191][0m |          -0.0138 |          15.7535 |           3.2602 |
[32m[20230205 18:22:15 @agent_ppo2.py:191][0m |          -0.0106 |          14.7600 |           3.2570 |
[32m[20230205 18:22:15 @agent_ppo2.py:191][0m |          -0.0123 |          13.9599 |           3.2544 |
[32m[20230205 18:22:15 @agent_ppo2.py:191][0m |          -0.0184 |          11.8204 |           3.2516 |
[32m[20230205 18:22:15 @agent_ppo2.py:191][0m |          -0.0222 |          11.2837 |           3.2510 |
[32m[20230205 18:22:15 @agent_ppo2.py:191][0m |          -0.0223 |          10.9268 |           3.2503 |
[32m[20230205 18:22:15 @agent_ppo2.py:191][0m |           0.0061 |          10.7179 |           3.2489 |
[32m[20230205 18:22:15 @agent_ppo2.py:191][0m |          -0.0241 |          10.4902 |           3.2491 |
[32m[20230205 18:22:15 @agent_ppo2.py:191][0m |          -0.0231 |          10.1962 |           3.2486 |
[32m[20230205 18:22:15 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:22:16 @agent_ppo2.py:144][0m Average TRAINING episode reward: 131.88
[32m[20230205 18:22:16 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.86
[32m[20230205 18:22:16 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.85
[32m[20230205 18:22:16 @agent_ppo2.py:149][0m Total time:       8.81 min
[32m[20230205 18:22:16 @agent_ppo2.py:151][0m 569344 total steps have happened
[32m[20230205 18:22:16 @agent_ppo2.py:127][0m #------------------------ Iteration 278 --------------------------#
[32m[20230205 18:22:16 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:22:17 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:17 @agent_ppo2.py:191][0m |           0.0030 |          12.9021 |           3.2963 |
[32m[20230205 18:22:17 @agent_ppo2.py:191][0m |          -0.0045 |          12.1073 |           3.2869 |
[32m[20230205 18:22:17 @agent_ppo2.py:191][0m |          -0.0028 |          11.9933 |           3.2912 |
[32m[20230205 18:22:17 @agent_ppo2.py:191][0m |          -0.0083 |          11.6200 |           3.2856 |
[32m[20230205 18:22:17 @agent_ppo2.py:191][0m |          -0.0064 |          11.5824 |           3.2909 |
[32m[20230205 18:22:17 @agent_ppo2.py:191][0m |          -0.0075 |          11.5352 |           3.2892 |
[32m[20230205 18:22:17 @agent_ppo2.py:191][0m |          -0.0086 |          11.2685 |           3.2868 |
[32m[20230205 18:22:17 @agent_ppo2.py:191][0m |          -0.0085 |          11.3226 |           3.2899 |
[32m[20230205 18:22:17 @agent_ppo2.py:191][0m |          -0.0105 |          11.0845 |           3.2907 |
[32m[20230205 18:22:17 @agent_ppo2.py:191][0m |          -0.0101 |          11.2687 |           3.2908 |
[32m[20230205 18:22:17 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:22:18 @agent_ppo2.py:144][0m Average TRAINING episode reward: 254.96
[32m[20230205 18:22:18 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.49
[32m[20230205 18:22:18 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 268.68
[32m[20230205 18:22:18 @agent_ppo2.py:149][0m Total time:       8.84 min
[32m[20230205 18:22:18 @agent_ppo2.py:151][0m 571392 total steps have happened
[32m[20230205 18:22:18 @agent_ppo2.py:127][0m #------------------------ Iteration 279 --------------------------#
[32m[20230205 18:22:18 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:22:18 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:18 @agent_ppo2.py:191][0m |          -0.0006 |          10.2899 |           3.2242 |
[32m[20230205 18:22:18 @agent_ppo2.py:191][0m |          -0.0033 |           9.8507 |           3.2129 |
[32m[20230205 18:22:18 @agent_ppo2.py:191][0m |          -0.0066 |           9.5644 |           3.2119 |
[32m[20230205 18:22:18 @agent_ppo2.py:191][0m |          -0.0099 |           9.3121 |           3.2107 |
[32m[20230205 18:22:18 @agent_ppo2.py:191][0m |          -0.0095 |           9.1416 |           3.2141 |
[32m[20230205 18:22:18 @agent_ppo2.py:191][0m |          -0.0111 |           9.0211 |           3.2108 |
[32m[20230205 18:22:19 @agent_ppo2.py:191][0m |          -0.0144 |           8.8891 |           3.2113 |
[32m[20230205 18:22:19 @agent_ppo2.py:191][0m |          -0.0119 |           8.7312 |           3.2112 |
[32m[20230205 18:22:19 @agent_ppo2.py:191][0m |          -0.0093 |           8.7357 |           3.2132 |
[32m[20230205 18:22:19 @agent_ppo2.py:191][0m |          -0.0114 |           8.5720 |           3.2094 |
[32m[20230205 18:22:19 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:22:19 @agent_ppo2.py:144][0m Average TRAINING episode reward: 253.16
[32m[20230205 18:22:19 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.68
[32m[20230205 18:22:19 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 199.61
[32m[20230205 18:22:19 @agent_ppo2.py:149][0m Total time:       8.87 min
[32m[20230205 18:22:19 @agent_ppo2.py:151][0m 573440 total steps have happened
[32m[20230205 18:22:19 @agent_ppo2.py:127][0m #------------------------ Iteration 280 --------------------------#
[32m[20230205 18:22:20 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:22:20 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:20 @agent_ppo2.py:191][0m |           0.0012 |          11.8513 |           3.3392 |
[32m[20230205 18:22:20 @agent_ppo2.py:191][0m |          -0.0050 |          11.4291 |           3.3291 |
[32m[20230205 18:22:20 @agent_ppo2.py:191][0m |          -0.0082 |          11.2672 |           3.3248 |
[32m[20230205 18:22:20 @agent_ppo2.py:191][0m |          -0.0088 |          11.2309 |           3.3185 |
[32m[20230205 18:22:20 @agent_ppo2.py:191][0m |          -0.0120 |          11.0144 |           3.3168 |
[32m[20230205 18:22:20 @agent_ppo2.py:191][0m |          -0.0122 |          10.9143 |           3.3179 |
[32m[20230205 18:22:20 @agent_ppo2.py:191][0m |          -0.0134 |          10.8062 |           3.3177 |
[32m[20230205 18:22:20 @agent_ppo2.py:191][0m |          -0.0140 |          10.7147 |           3.3162 |
[32m[20230205 18:22:20 @agent_ppo2.py:191][0m |          -0.0134 |          10.7320 |           3.3177 |
[32m[20230205 18:22:20 @agent_ppo2.py:191][0m |          -0.0150 |          10.5482 |           3.3175 |
[32m[20230205 18:22:20 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:22:21 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.69
[32m[20230205 18:22:21 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.93
[32m[20230205 18:22:21 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 193.62
[32m[20230205 18:22:21 @agent_ppo2.py:149][0m Total time:       8.89 min
[32m[20230205 18:22:21 @agent_ppo2.py:151][0m 575488 total steps have happened
[32m[20230205 18:22:21 @agent_ppo2.py:127][0m #------------------------ Iteration 281 --------------------------#
[32m[20230205 18:22:21 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:22:21 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:21 @agent_ppo2.py:191][0m |           0.0041 |          11.1948 |           3.2293 |
[32m[20230205 18:22:22 @agent_ppo2.py:191][0m |          -0.0049 |          10.4105 |           3.2328 |
[32m[20230205 18:22:22 @agent_ppo2.py:191][0m |          -0.0091 |           9.9301 |           3.2300 |
[32m[20230205 18:22:22 @agent_ppo2.py:191][0m |          -0.0120 |           9.7441 |           3.2312 |
[32m[20230205 18:22:22 @agent_ppo2.py:191][0m |          -0.0117 |           9.5644 |           3.2308 |
[32m[20230205 18:22:22 @agent_ppo2.py:191][0m |          -0.0115 |           9.4574 |           3.2292 |
[32m[20230205 18:22:22 @agent_ppo2.py:191][0m |          -0.0124 |           9.4601 |           3.2302 |
[32m[20230205 18:22:22 @agent_ppo2.py:191][0m |          -0.0141 |           9.2045 |           3.2283 |
[32m[20230205 18:22:22 @agent_ppo2.py:191][0m |          -0.0146 |           9.0793 |           3.2268 |
[32m[20230205 18:22:22 @agent_ppo2.py:191][0m |          -0.0162 |           8.9702 |           3.2284 |
[32m[20230205 18:22:22 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:22:22 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.48
[32m[20230205 18:22:22 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.47
[32m[20230205 18:22:22 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 105.51
[32m[20230205 18:22:22 @agent_ppo2.py:149][0m Total time:       8.92 min
[32m[20230205 18:22:22 @agent_ppo2.py:151][0m 577536 total steps have happened
[32m[20230205 18:22:22 @agent_ppo2.py:127][0m #------------------------ Iteration 282 --------------------------#
[32m[20230205 18:22:23 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:22:23 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:23 @agent_ppo2.py:191][0m |           0.0025 |          37.0263 |           3.3255 |
[32m[20230205 18:22:23 @agent_ppo2.py:191][0m |          -0.0037 |          16.6563 |           3.3260 |
[32m[20230205 18:22:23 @agent_ppo2.py:191][0m |          -0.0059 |          15.1232 |           3.3196 |
[32m[20230205 18:22:23 @agent_ppo2.py:191][0m |          -0.0075 |          14.0658 |           3.3204 |
[32m[20230205 18:22:23 @agent_ppo2.py:191][0m |          -0.0090 |          13.2933 |           3.3192 |
[32m[20230205 18:22:23 @agent_ppo2.py:191][0m |          -0.0110 |          12.6076 |           3.3164 |
[32m[20230205 18:22:23 @agent_ppo2.py:191][0m |          -0.0102 |          12.1884 |           3.3131 |
[32m[20230205 18:22:23 @agent_ppo2.py:191][0m |          -0.0115 |          11.7783 |           3.3152 |
[32m[20230205 18:22:24 @agent_ppo2.py:191][0m |          -0.0104 |          11.2432 |           3.3121 |
[32m[20230205 18:22:24 @agent_ppo2.py:191][0m |          -0.0108 |          10.9775 |           3.3116 |
[32m[20230205 18:22:24 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:22:24 @agent_ppo2.py:144][0m Average TRAINING episode reward: 147.91
[32m[20230205 18:22:24 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.79
[32m[20230205 18:22:24 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 88.69
[32m[20230205 18:22:24 @agent_ppo2.py:149][0m Total time:       8.94 min
[32m[20230205 18:22:24 @agent_ppo2.py:151][0m 579584 total steps have happened
[32m[20230205 18:22:24 @agent_ppo2.py:127][0m #------------------------ Iteration 283 --------------------------#
[32m[20230205 18:22:25 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:22:25 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:25 @agent_ppo2.py:191][0m |           0.0053 |          15.3352 |           3.3243 |
[32m[20230205 18:22:25 @agent_ppo2.py:191][0m |          -0.0003 |          14.2035 |           3.3179 |
[32m[20230205 18:22:25 @agent_ppo2.py:191][0m |          -0.0071 |          13.5283 |           3.3167 |
[32m[20230205 18:22:25 @agent_ppo2.py:191][0m |          -0.0103 |          13.1957 |           3.3113 |
[32m[20230205 18:22:25 @agent_ppo2.py:191][0m |          -0.0107 |          12.9280 |           3.3130 |
[32m[20230205 18:22:25 @agent_ppo2.py:191][0m |          -0.0101 |          12.7999 |           3.3058 |
[32m[20230205 18:22:25 @agent_ppo2.py:191][0m |          -0.0120 |          12.5654 |           3.3070 |
[32m[20230205 18:22:25 @agent_ppo2.py:191][0m |          -0.0132 |          12.4282 |           3.3047 |
[32m[20230205 18:22:25 @agent_ppo2.py:191][0m |          -0.0137 |          12.3058 |           3.3017 |
[32m[20230205 18:22:25 @agent_ppo2.py:191][0m |          -0.0137 |          12.3399 |           3.3005 |
[32m[20230205 18:22:25 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:22:26 @agent_ppo2.py:144][0m Average TRAINING episode reward: 256.06
[32m[20230205 18:22:26 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.47
[32m[20230205 18:22:26 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 218.25
[32m[20230205 18:22:26 @agent_ppo2.py:149][0m Total time:       8.97 min
[32m[20230205 18:22:26 @agent_ppo2.py:151][0m 581632 total steps have happened
[32m[20230205 18:22:26 @agent_ppo2.py:127][0m #------------------------ Iteration 284 --------------------------#
[32m[20230205 18:22:26 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:22:26 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:26 @agent_ppo2.py:191][0m |          -0.0086 |           8.1615 |           3.2740 |
[32m[20230205 18:22:26 @agent_ppo2.py:191][0m |          -0.0058 |           6.7619 |           3.2659 |
[32m[20230205 18:22:26 @agent_ppo2.py:191][0m |          -0.0205 |           6.3148 |           3.2658 |
[32m[20230205 18:22:26 @agent_ppo2.py:191][0m |           0.0056 |           6.1085 |           3.2632 |
[32m[20230205 18:22:27 @agent_ppo2.py:191][0m |           0.0225 |           5.8054 |           3.2614 |
[32m[20230205 18:22:27 @agent_ppo2.py:191][0m |          -0.0220 |           5.6764 |           3.2532 |
[32m[20230205 18:22:27 @agent_ppo2.py:191][0m |          -0.0127 |           5.5542 |           3.2612 |
[32m[20230205 18:22:27 @agent_ppo2.py:191][0m |          -0.0228 |           5.3737 |           3.2615 |
[32m[20230205 18:22:27 @agent_ppo2.py:191][0m |          -0.0056 |           5.2370 |           3.2616 |
[32m[20230205 18:22:27 @agent_ppo2.py:191][0m |           0.0259 |           5.9492 |           3.2592 |
[32m[20230205 18:22:27 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:22:27 @agent_ppo2.py:144][0m Average TRAINING episode reward: 227.75
[32m[20230205 18:22:27 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.65
[32m[20230205 18:22:27 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 112.29
[32m[20230205 18:22:27 @agent_ppo2.py:149][0m Total time:       9.00 min
[32m[20230205 18:22:27 @agent_ppo2.py:151][0m 583680 total steps have happened
[32m[20230205 18:22:27 @agent_ppo2.py:127][0m #------------------------ Iteration 285 --------------------------#
[32m[20230205 18:22:28 @agent_ppo2.py:133][0m Sampling time: 0.70 s by 1 slaves
[32m[20230205 18:22:28 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:28 @agent_ppo2.py:191][0m |           0.0037 |          24.5962 |           3.2940 |
[32m[20230205 18:22:28 @agent_ppo2.py:191][0m |          -0.0012 |          15.5302 |           3.2949 |
[32m[20230205 18:22:28 @agent_ppo2.py:191][0m |          -0.0098 |          14.8554 |           3.2924 |
[32m[20230205 18:22:28 @agent_ppo2.py:191][0m |           0.0066 |          13.8349 |           3.2877 |
[32m[20230205 18:22:28 @agent_ppo2.py:191][0m |          -0.0046 |          13.2577 |           3.2825 |
[32m[20230205 18:22:28 @agent_ppo2.py:191][0m |          -0.0105 |          12.9912 |           3.2817 |
[32m[20230205 18:22:29 @agent_ppo2.py:191][0m |          -0.0139 |          12.3211 |           3.2829 |
[32m[20230205 18:22:29 @agent_ppo2.py:191][0m |          -0.0113 |          11.9079 |           3.2800 |
[32m[20230205 18:22:29 @agent_ppo2.py:191][0m |          -0.0146 |          11.6891 |           3.2807 |
[32m[20230205 18:22:29 @agent_ppo2.py:191][0m |          -0.0151 |          11.3900 |           3.2764 |
[32m[20230205 18:22:29 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:22:29 @agent_ppo2.py:144][0m Average TRAINING episode reward: 195.50
[32m[20230205 18:22:29 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.41
[32m[20230205 18:22:29 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 200.67
[32m[20230205 18:22:29 @agent_ppo2.py:149][0m Total time:       9.03 min
[32m[20230205 18:22:29 @agent_ppo2.py:151][0m 585728 total steps have happened
[32m[20230205 18:22:29 @agent_ppo2.py:127][0m #------------------------ Iteration 286 --------------------------#
[32m[20230205 18:22:30 @agent_ppo2.py:133][0m Sampling time: 0.46 s by 1 slaves
[32m[20230205 18:22:30 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:30 @agent_ppo2.py:191][0m |          -0.0010 |          18.5732 |           3.2383 |
[32m[20230205 18:22:30 @agent_ppo2.py:191][0m |          -0.0032 |          12.5315 |           3.2323 |
[32m[20230205 18:22:30 @agent_ppo2.py:191][0m |          -0.0083 |          11.4251 |           3.2241 |
[32m[20230205 18:22:30 @agent_ppo2.py:191][0m |          -0.0111 |          11.0591 |           3.2171 |
[32m[20230205 18:22:30 @agent_ppo2.py:191][0m |          -0.0116 |          10.5577 |           3.2174 |
[32m[20230205 18:22:30 @agent_ppo2.py:191][0m |          -0.0145 |          10.1238 |           3.2163 |
[32m[20230205 18:22:30 @agent_ppo2.py:191][0m |          -0.0122 |           9.8011 |           3.2135 |
[32m[20230205 18:22:30 @agent_ppo2.py:191][0m |          -0.0137 |           9.9474 |           3.2115 |
[32m[20230205 18:22:30 @agent_ppo2.py:191][0m |          -0.0164 |           9.3806 |           3.2134 |
[32m[20230205 18:22:30 @agent_ppo2.py:191][0m |          -0.0162 |           9.1929 |           3.2122 |
[32m[20230205 18:22:30 @agent_ppo2.py:136][0m Policy update time: 0.46 s
[32m[20230205 18:22:31 @agent_ppo2.py:144][0m Average TRAINING episode reward: 174.41
[32m[20230205 18:22:31 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.03
[32m[20230205 18:22:31 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 124.16
[32m[20230205 18:22:31 @agent_ppo2.py:149][0m Total time:       9.06 min
[32m[20230205 18:22:31 @agent_ppo2.py:151][0m 587776 total steps have happened
[32m[20230205 18:22:31 @agent_ppo2.py:127][0m #------------------------ Iteration 287 --------------------------#
[32m[20230205 18:22:31 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:22:31 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:31 @agent_ppo2.py:191][0m |           0.0088 |          49.6739 |           3.2064 |
[32m[20230205 18:22:31 @agent_ppo2.py:191][0m |          -0.0072 |          35.2281 |           3.2011 |
[32m[20230205 18:22:31 @agent_ppo2.py:191][0m |          -0.0148 |          31.0107 |           3.2036 |
[32m[20230205 18:22:31 @agent_ppo2.py:191][0m |          -0.0090 |          29.2846 |           3.1995 |
[32m[20230205 18:22:31 @agent_ppo2.py:191][0m |          -0.0185 |          27.4013 |           3.2028 |
[32m[20230205 18:22:32 @agent_ppo2.py:191][0m |          -0.0210 |          26.3494 |           3.2001 |
[32m[20230205 18:22:32 @agent_ppo2.py:191][0m |          -0.0180 |          25.3883 |           3.2020 |
[32m[20230205 18:22:32 @agent_ppo2.py:191][0m |          -0.0148 |          25.0116 |           3.1962 |
[32m[20230205 18:22:32 @agent_ppo2.py:191][0m |          -0.0206 |          23.7242 |           3.1990 |
[32m[20230205 18:22:32 @agent_ppo2.py:191][0m |          -0.0201 |          23.0954 |           3.2000 |
[32m[20230205 18:22:32 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:22:32 @agent_ppo2.py:144][0m Average TRAINING episode reward: 195.30
[32m[20230205 18:22:32 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.10
[32m[20230205 18:22:32 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 271.34
[32m[20230205 18:22:32 @agent_ppo2.py:149][0m Total time:       9.08 min
[32m[20230205 18:22:32 @agent_ppo2.py:151][0m 589824 total steps have happened
[32m[20230205 18:22:32 @agent_ppo2.py:127][0m #------------------------ Iteration 288 --------------------------#
[32m[20230205 18:22:33 @agent_ppo2.py:133][0m Sampling time: 0.62 s by 1 slaves
[32m[20230205 18:22:33 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:33 @agent_ppo2.py:191][0m |           0.0008 |          11.6271 |           3.2906 |
[32m[20230205 18:22:33 @agent_ppo2.py:191][0m |          -0.0075 |           9.5943 |           3.2845 |
[32m[20230205 18:22:33 @agent_ppo2.py:191][0m |          -0.0120 |           8.9889 |           3.2800 |
[32m[20230205 18:22:33 @agent_ppo2.py:191][0m |          -0.0142 |           8.6561 |           3.2804 |
[32m[20230205 18:22:33 @agent_ppo2.py:191][0m |          -0.0131 |           8.4213 |           3.2781 |
[32m[20230205 18:22:33 @agent_ppo2.py:191][0m |          -0.0156 |           8.2367 |           3.2761 |
[32m[20230205 18:22:33 @agent_ppo2.py:191][0m |          -0.0166 |           8.1080 |           3.2755 |
[32m[20230205 18:22:33 @agent_ppo2.py:191][0m |          -0.0164 |           8.0158 |           3.2733 |
[32m[20230205 18:22:33 @agent_ppo2.py:191][0m |          -0.0182 |           7.8820 |           3.2732 |
[32m[20230205 18:22:33 @agent_ppo2.py:191][0m |          -0.0175 |           7.7905 |           3.2717 |
[32m[20230205 18:22:33 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:22:34 @agent_ppo2.py:144][0m Average TRAINING episode reward: 211.16
[32m[20230205 18:22:34 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.98
[32m[20230205 18:22:34 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 83.41
[32m[20230205 18:22:34 @agent_ppo2.py:149][0m Total time:       9.11 min
[32m[20230205 18:22:34 @agent_ppo2.py:151][0m 591872 total steps have happened
[32m[20230205 18:22:34 @agent_ppo2.py:127][0m #------------------------ Iteration 289 --------------------------#
[32m[20230205 18:22:34 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:22:34 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:35 @agent_ppo2.py:191][0m |           0.0010 |          42.7873 |           3.2685 |
[32m[20230205 18:22:35 @agent_ppo2.py:191][0m |           0.0044 |          34.9340 |           3.2673 |
[32m[20230205 18:22:35 @agent_ppo2.py:191][0m |          -0.0082 |          29.8441 |           3.2607 |
[32m[20230205 18:22:35 @agent_ppo2.py:191][0m |          -0.0116 |          26.9642 |           3.2593 |
[32m[20230205 18:22:35 @agent_ppo2.py:191][0m |          -0.0130 |          26.0223 |           3.2577 |
[32m[20230205 18:22:35 @agent_ppo2.py:191][0m |          -0.0134 |          25.3251 |           3.2583 |
[32m[20230205 18:22:35 @agent_ppo2.py:191][0m |          -0.0154 |          24.8633 |           3.2599 |
[32m[20230205 18:22:35 @agent_ppo2.py:191][0m |          -0.0155 |          24.0689 |           3.2593 |
[32m[20230205 18:22:35 @agent_ppo2.py:191][0m |          -0.0195 |          23.5408 |           3.2585 |
[32m[20230205 18:22:35 @agent_ppo2.py:191][0m |          -0.0192 |          22.9622 |           3.2593 |
[32m[20230205 18:22:35 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:22:35 @agent_ppo2.py:144][0m Average TRAINING episode reward: 198.58
[32m[20230205 18:22:35 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.68
[32m[20230205 18:22:35 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 174.59
[32m[20230205 18:22:35 @agent_ppo2.py:149][0m Total time:       9.13 min
[32m[20230205 18:22:35 @agent_ppo2.py:151][0m 593920 total steps have happened
[32m[20230205 18:22:35 @agent_ppo2.py:127][0m #------------------------ Iteration 290 --------------------------#
[32m[20230205 18:22:36 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:22:36 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:36 @agent_ppo2.py:191][0m |          -0.0009 |          12.9843 |           3.2110 |
[32m[20230205 18:22:36 @agent_ppo2.py:191][0m |           0.0065 |          12.9384 |           3.2104 |
[32m[20230205 18:22:36 @agent_ppo2.py:191][0m |          -0.0095 |          10.7762 |           3.2082 |
[32m[20230205 18:22:36 @agent_ppo2.py:191][0m |          -0.0093 |          10.3863 |           3.2073 |
[32m[20230205 18:22:36 @agent_ppo2.py:191][0m |          -0.0116 |          10.1601 |           3.2053 |
[32m[20230205 18:22:36 @agent_ppo2.py:191][0m |          -0.0134 |           9.9516 |           3.2045 |
[32m[20230205 18:22:36 @agent_ppo2.py:191][0m |          -0.0103 |           9.8521 |           3.2036 |
[32m[20230205 18:22:36 @agent_ppo2.py:191][0m |          -0.0099 |          10.3569 |           3.2006 |
[32m[20230205 18:22:36 @agent_ppo2.py:191][0m |          -0.0121 |           9.6317 |           3.2023 |
[32m[20230205 18:22:36 @agent_ppo2.py:191][0m |          -0.0126 |           9.5057 |           3.2019 |
[32m[20230205 18:22:36 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:22:37 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.85
[32m[20230205 18:22:37 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.60
[32m[20230205 18:22:37 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.34
[32m[20230205 18:22:37 @agent_ppo2.py:149][0m Total time:       9.16 min
[32m[20230205 18:22:37 @agent_ppo2.py:151][0m 595968 total steps have happened
[32m[20230205 18:22:37 @agent_ppo2.py:127][0m #------------------------ Iteration 291 --------------------------#
[32m[20230205 18:22:38 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:22:38 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:38 @agent_ppo2.py:191][0m |          -0.0003 |          13.4314 |           3.2327 |
[32m[20230205 18:22:38 @agent_ppo2.py:191][0m |          -0.0071 |          12.6011 |           3.2235 |
[32m[20230205 18:22:38 @agent_ppo2.py:191][0m |          -0.0104 |          12.2141 |           3.2202 |
[32m[20230205 18:22:38 @agent_ppo2.py:191][0m |          -0.0121 |          11.9889 |           3.2186 |
[32m[20230205 18:22:38 @agent_ppo2.py:191][0m |          -0.0124 |          11.7520 |           3.2165 |
[32m[20230205 18:22:38 @agent_ppo2.py:191][0m |          -0.0147 |          11.5611 |           3.2143 |
[32m[20230205 18:22:38 @agent_ppo2.py:191][0m |          -0.0150 |          11.3690 |           3.2151 |
[32m[20230205 18:22:38 @agent_ppo2.py:191][0m |          -0.0152 |          11.1952 |           3.2145 |
[32m[20230205 18:22:38 @agent_ppo2.py:191][0m |          -0.0162 |          11.0304 |           3.2130 |
[32m[20230205 18:22:38 @agent_ppo2.py:191][0m |          -0.0164 |          10.9144 |           3.2115 |
[32m[20230205 18:22:38 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:22:39 @agent_ppo2.py:144][0m Average TRAINING episode reward: 264.12
[32m[20230205 18:22:39 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 267.62
[32m[20230205 18:22:39 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 169.78
[32m[20230205 18:22:39 @agent_ppo2.py:149][0m Total time:       9.19 min
[32m[20230205 18:22:39 @agent_ppo2.py:151][0m 598016 total steps have happened
[32m[20230205 18:22:39 @agent_ppo2.py:127][0m #------------------------ Iteration 292 --------------------------#
[32m[20230205 18:22:39 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:22:39 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:39 @agent_ppo2.py:191][0m |          -0.0030 |          13.2506 |           3.2642 |
[32m[20230205 18:22:39 @agent_ppo2.py:191][0m |          -0.0117 |          12.6361 |           3.2538 |
[32m[20230205 18:22:39 @agent_ppo2.py:191][0m |          -0.0300 |          12.4919 |           3.2515 |
[32m[20230205 18:22:39 @agent_ppo2.py:191][0m |          -0.0099 |          12.0408 |           3.2555 |
[32m[20230205 18:22:39 @agent_ppo2.py:191][0m |          -0.0191 |          11.7667 |           3.2546 |
[32m[20230205 18:22:39 @agent_ppo2.py:191][0m |           0.0049 |          11.9344 |           3.2571 |
[32m[20230205 18:22:40 @agent_ppo2.py:191][0m |          -0.0157 |          11.4400 |           3.2545 |
[32m[20230205 18:22:40 @agent_ppo2.py:191][0m |           0.0017 |          11.1364 |           3.2499 |
[32m[20230205 18:22:40 @agent_ppo2.py:191][0m |          -0.0002 |          11.1861 |           3.2517 |
[32m[20230205 18:22:40 @agent_ppo2.py:191][0m |          -0.0210 |          10.5928 |           3.2552 |
[32m[20230205 18:22:40 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:22:40 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.04
[32m[20230205 18:22:40 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.47
[32m[20230205 18:22:40 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 206.84
[32m[20230205 18:22:40 @agent_ppo2.py:149][0m Total time:       9.21 min
[32m[20230205 18:22:40 @agent_ppo2.py:151][0m 600064 total steps have happened
[32m[20230205 18:22:40 @agent_ppo2.py:127][0m #------------------------ Iteration 293 --------------------------#
[32m[20230205 18:22:41 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:22:41 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:41 @agent_ppo2.py:191][0m |           0.0016 |          28.1913 |           3.3487 |
[32m[20230205 18:22:41 @agent_ppo2.py:191][0m |          -0.0066 |          19.0150 |           3.3406 |
[32m[20230205 18:22:41 @agent_ppo2.py:191][0m |          -0.0095 |          17.3585 |           3.3352 |
[32m[20230205 18:22:41 @agent_ppo2.py:191][0m |          -0.0094 |          16.5507 |           3.3304 |
[32m[20230205 18:22:41 @agent_ppo2.py:191][0m |          -0.0091 |          16.2810 |           3.3321 |
[32m[20230205 18:22:41 @agent_ppo2.py:191][0m |          -0.0121 |          15.2911 |           3.3279 |
[32m[20230205 18:22:41 @agent_ppo2.py:191][0m |          -0.0123 |          14.7439 |           3.3285 |
[32m[20230205 18:22:41 @agent_ppo2.py:191][0m |          -0.0143 |          14.2532 |           3.3268 |
[32m[20230205 18:22:41 @agent_ppo2.py:191][0m |          -0.0160 |          13.9225 |           3.3250 |
[32m[20230205 18:22:41 @agent_ppo2.py:191][0m |          -0.0135 |          13.6833 |           3.3260 |
[32m[20230205 18:22:41 @agent_ppo2.py:136][0m Policy update time: 0.49 s
[32m[20230205 18:22:42 @agent_ppo2.py:144][0m Average TRAINING episode reward: 172.07
[32m[20230205 18:22:42 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.12
[32m[20230205 18:22:42 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 104.15
[32m[20230205 18:22:42 @agent_ppo2.py:149][0m Total time:       9.24 min
[32m[20230205 18:22:42 @agent_ppo2.py:151][0m 602112 total steps have happened
[32m[20230205 18:22:42 @agent_ppo2.py:127][0m #------------------------ Iteration 294 --------------------------#
[32m[20230205 18:22:42 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:22:42 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:42 @agent_ppo2.py:191][0m |          -0.0015 |          14.6512 |           3.2281 |
[32m[20230205 18:22:43 @agent_ppo2.py:191][0m |          -0.0033 |          13.0678 |           3.2205 |
[32m[20230205 18:22:43 @agent_ppo2.py:191][0m |          -0.0081 |          12.5859 |           3.2198 |
[32m[20230205 18:22:43 @agent_ppo2.py:191][0m |          -0.0070 |          12.3335 |           3.2245 |
[32m[20230205 18:22:43 @agent_ppo2.py:191][0m |          -0.0053 |          12.4175 |           3.2204 |
[32m[20230205 18:22:43 @agent_ppo2.py:191][0m |          -0.0062 |          12.2497 |           3.2137 |
[32m[20230205 18:22:43 @agent_ppo2.py:191][0m |          -0.0090 |          11.7754 |           3.2173 |
[32m[20230205 18:22:43 @agent_ppo2.py:191][0m |          -0.0108 |          11.5199 |           3.2190 |
[32m[20230205 18:22:43 @agent_ppo2.py:191][0m |          -0.0122 |          11.2218 |           3.2138 |
[32m[20230205 18:22:43 @agent_ppo2.py:191][0m |          -0.0127 |          10.9807 |           3.2136 |
[32m[20230205 18:22:43 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:22:43 @agent_ppo2.py:144][0m Average TRAINING episode reward: 252.63
[32m[20230205 18:22:43 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.09
[32m[20230205 18:22:43 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 97.41
[32m[20230205 18:22:43 @agent_ppo2.py:149][0m Total time:       9.27 min
[32m[20230205 18:22:43 @agent_ppo2.py:151][0m 604160 total steps have happened
[32m[20230205 18:22:43 @agent_ppo2.py:127][0m #------------------------ Iteration 295 --------------------------#
[32m[20230205 18:22:44 @agent_ppo2.py:133][0m Sampling time: 0.46 s by 1 slaves
[32m[20230205 18:22:44 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:44 @agent_ppo2.py:191][0m |           0.0007 |          48.4047 |           3.2134 |
[32m[20230205 18:22:44 @agent_ppo2.py:191][0m |          -0.0054 |          31.8256 |           3.2132 |
[32m[20230205 18:22:44 @agent_ppo2.py:191][0m |          -0.0111 |          28.2161 |           3.2117 |
[32m[20230205 18:22:44 @agent_ppo2.py:191][0m |          -0.0047 |          28.5414 |           3.2092 |
[32m[20230205 18:22:44 @agent_ppo2.py:191][0m |          -0.0072 |          25.6503 |           3.2083 |
[32m[20230205 18:22:44 @agent_ppo2.py:191][0m |          -0.0123 |          25.6004 |           3.2066 |
[32m[20230205 18:22:44 @agent_ppo2.py:191][0m |          -0.0131 |          24.5564 |           3.2073 |
[32m[20230205 18:22:44 @agent_ppo2.py:191][0m |          -0.0163 |          23.8425 |           3.2072 |
[32m[20230205 18:22:44 @agent_ppo2.py:191][0m |          -0.0183 |          23.0215 |           3.2076 |
[32m[20230205 18:22:44 @agent_ppo2.py:191][0m |          -0.0162 |          23.2771 |           3.2071 |
[32m[20230205 18:22:44 @agent_ppo2.py:136][0m Policy update time: 0.46 s
[32m[20230205 18:22:45 @agent_ppo2.py:144][0m Average TRAINING episode reward: 172.40
[32m[20230205 18:22:45 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.41
[32m[20230205 18:22:45 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 268.77
[32m[20230205 18:22:45 @agent_ppo2.py:149][0m Total time:       9.29 min
[32m[20230205 18:22:45 @agent_ppo2.py:151][0m 606208 total steps have happened
[32m[20230205 18:22:45 @agent_ppo2.py:127][0m #------------------------ Iteration 296 --------------------------#
[32m[20230205 18:22:45 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:22:45 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:45 @agent_ppo2.py:191][0m |           0.0040 |          16.3977 |           3.2275 |
[32m[20230205 18:22:46 @agent_ppo2.py:191][0m |          -0.0069 |          13.5906 |           3.2241 |
[32m[20230205 18:22:46 @agent_ppo2.py:191][0m |          -0.0092 |          13.2689 |           3.2245 |
[32m[20230205 18:22:46 @agent_ppo2.py:191][0m |           0.0016 |          14.1234 |           3.2227 |
[32m[20230205 18:22:46 @agent_ppo2.py:191][0m |          -0.0141 |          12.8374 |           3.2214 |
[32m[20230205 18:22:46 @agent_ppo2.py:191][0m |          -0.0076 |          13.2501 |           3.2223 |
[32m[20230205 18:22:46 @agent_ppo2.py:191][0m |          -0.0109 |          13.0354 |           3.2210 |
[32m[20230205 18:22:46 @agent_ppo2.py:191][0m |          -0.0137 |          12.4084 |           3.2229 |
[32m[20230205 18:22:46 @agent_ppo2.py:191][0m |          -0.0089 |          12.5986 |           3.2255 |
[32m[20230205 18:22:46 @agent_ppo2.py:191][0m |          -0.0133 |          12.2512 |           3.2217 |
[32m[20230205 18:22:46 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:22:46 @agent_ppo2.py:144][0m Average TRAINING episode reward: 255.57
[32m[20230205 18:22:46 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.16
[32m[20230205 18:22:46 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 187.24
[32m[20230205 18:22:46 @agent_ppo2.py:149][0m Total time:       9.32 min
[32m[20230205 18:22:46 @agent_ppo2.py:151][0m 608256 total steps have happened
[32m[20230205 18:22:46 @agent_ppo2.py:127][0m #------------------------ Iteration 297 --------------------------#
[32m[20230205 18:22:47 @agent_ppo2.py:133][0m Sampling time: 0.58 s by 1 slaves
[32m[20230205 18:22:47 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:47 @agent_ppo2.py:191][0m |           0.0033 |          50.9303 |           3.2114 |
[32m[20230205 18:22:47 @agent_ppo2.py:191][0m |          -0.0035 |          20.2763 |           3.2046 |
[32m[20230205 18:22:47 @agent_ppo2.py:191][0m |          -0.0082 |          14.9110 |           3.1974 |
[32m[20230205 18:22:47 @agent_ppo2.py:191][0m |          -0.0073 |          13.9990 |           3.1933 |
[32m[20230205 18:22:47 @agent_ppo2.py:191][0m |          -0.0095 |          13.2247 |           3.1943 |
[32m[20230205 18:22:47 @agent_ppo2.py:191][0m |          -0.0111 |          12.8179 |           3.1933 |
[32m[20230205 18:22:47 @agent_ppo2.py:191][0m |          -0.0110 |          12.6222 |           3.1918 |
[32m[20230205 18:22:47 @agent_ppo2.py:191][0m |          -0.0129 |          12.3017 |           3.1914 |
[32m[20230205 18:22:47 @agent_ppo2.py:191][0m |          -0.0109 |          12.0932 |           3.1918 |
[32m[20230205 18:22:48 @agent_ppo2.py:191][0m |          -0.0115 |          11.8825 |           3.1919 |
[32m[20230205 18:22:48 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:22:48 @agent_ppo2.py:144][0m Average TRAINING episode reward: 131.57
[32m[20230205 18:22:48 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.22
[32m[20230205 18:22:48 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 268.07
[32m[20230205 18:22:48 @agent_ppo2.py:149][0m Total time:       9.34 min
[32m[20230205 18:22:48 @agent_ppo2.py:151][0m 610304 total steps have happened
[32m[20230205 18:22:48 @agent_ppo2.py:127][0m #------------------------ Iteration 298 --------------------------#
[32m[20230205 18:22:49 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:22:49 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:49 @agent_ppo2.py:191][0m |          -0.0007 |          29.5912 |           3.3941 |
[32m[20230205 18:22:49 @agent_ppo2.py:191][0m |          -0.0076 |          15.4138 |           3.3855 |
[32m[20230205 18:22:49 @agent_ppo2.py:191][0m |          -0.0109 |          13.8423 |           3.3843 |
[32m[20230205 18:22:49 @agent_ppo2.py:191][0m |          -0.0120 |          13.3797 |           3.3839 |
[32m[20230205 18:22:49 @agent_ppo2.py:191][0m |          -0.0135 |          12.8530 |           3.3814 |
[32m[20230205 18:22:49 @agent_ppo2.py:191][0m |          -0.0148 |          12.6138 |           3.3816 |
[32m[20230205 18:22:49 @agent_ppo2.py:191][0m |          -0.0148 |          12.5247 |           3.3810 |
[32m[20230205 18:22:49 @agent_ppo2.py:191][0m |          -0.0168 |          12.1559 |           3.3793 |
[32m[20230205 18:22:49 @agent_ppo2.py:191][0m |          -0.0165 |          11.9918 |           3.3784 |
[32m[20230205 18:22:49 @agent_ppo2.py:191][0m |          -0.0174 |          11.8535 |           3.3800 |
[32m[20230205 18:22:49 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:22:50 @agent_ppo2.py:144][0m Average TRAINING episode reward: 133.48
[32m[20230205 18:22:50 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.25
[32m[20230205 18:22:50 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 268.30
[32m[20230205 18:22:50 @agent_ppo2.py:149][0m Total time:       9.37 min
[32m[20230205 18:22:50 @agent_ppo2.py:151][0m 612352 total steps have happened
[32m[20230205 18:22:50 @agent_ppo2.py:127][0m #------------------------ Iteration 299 --------------------------#
[32m[20230205 18:22:50 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:22:50 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:50 @agent_ppo2.py:191][0m |           0.0015 |          13.8104 |           3.3061 |
[32m[20230205 18:22:50 @agent_ppo2.py:191][0m |          -0.0038 |          12.8856 |           3.3018 |
[32m[20230205 18:22:50 @agent_ppo2.py:191][0m |          -0.0079 |          12.4461 |           3.2967 |
[32m[20230205 18:22:50 @agent_ppo2.py:191][0m |          -0.0087 |          12.1613 |           3.2957 |
[32m[20230205 18:22:50 @agent_ppo2.py:191][0m |          -0.0093 |          11.9491 |           3.2968 |
[32m[20230205 18:22:50 @agent_ppo2.py:191][0m |          -0.0102 |          11.8317 |           3.2963 |
[32m[20230205 18:22:51 @agent_ppo2.py:191][0m |          -0.0116 |          11.6330 |           3.2983 |
[32m[20230205 18:22:51 @agent_ppo2.py:191][0m |          -0.0107 |          11.6414 |           3.2947 |
[32m[20230205 18:22:51 @agent_ppo2.py:191][0m |          -0.0109 |          11.5911 |           3.2969 |
[32m[20230205 18:22:51 @agent_ppo2.py:191][0m |          -0.0128 |          11.3120 |           3.2940 |
[32m[20230205 18:22:51 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:22:51 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.99
[32m[20230205 18:22:51 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.02
[32m[20230205 18:22:51 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 195.57
[32m[20230205 18:22:51 @agent_ppo2.py:149][0m Total time:       9.40 min
[32m[20230205 18:22:51 @agent_ppo2.py:151][0m 614400 total steps have happened
[32m[20230205 18:22:51 @agent_ppo2.py:127][0m #------------------------ Iteration 300 --------------------------#
[32m[20230205 18:22:52 @agent_ppo2.py:133][0m Sampling time: 0.64 s by 1 slaves
[32m[20230205 18:22:52 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:52 @agent_ppo2.py:191][0m |          -0.0026 |          60.4516 |           3.2953 |
[32m[20230205 18:22:52 @agent_ppo2.py:191][0m |          -0.0114 |          36.5395 |           3.2849 |
[32m[20230205 18:22:52 @agent_ppo2.py:191][0m |          -0.0133 |          30.2443 |           3.2788 |
[32m[20230205 18:22:52 @agent_ppo2.py:191][0m |          -0.0143 |          26.6536 |           3.2814 |
[32m[20230205 18:22:52 @agent_ppo2.py:191][0m |          -0.0154 |          24.9888 |           3.2732 |
[32m[20230205 18:22:52 @agent_ppo2.py:191][0m |          -0.0166 |          23.1792 |           3.2733 |
[32m[20230205 18:22:52 @agent_ppo2.py:191][0m |          -0.0169 |          21.3955 |           3.2713 |
[32m[20230205 18:22:52 @agent_ppo2.py:191][0m |          -0.0147 |          19.6862 |           3.2661 |
[32m[20230205 18:22:52 @agent_ppo2.py:191][0m |          -0.0143 |          18.4314 |           3.2648 |
[32m[20230205 18:22:52 @agent_ppo2.py:191][0m |          -0.0193 |          17.5508 |           3.2624 |
[32m[20230205 18:22:52 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:22:53 @agent_ppo2.py:144][0m Average TRAINING episode reward: 130.77
[32m[20230205 18:22:53 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.87
[32m[20230205 18:22:53 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 127.98
[32m[20230205 18:22:53 @agent_ppo2.py:149][0m Total time:       9.43 min
[32m[20230205 18:22:53 @agent_ppo2.py:151][0m 616448 total steps have happened
[32m[20230205 18:22:53 @agent_ppo2.py:127][0m #------------------------ Iteration 301 --------------------------#
[32m[20230205 18:22:54 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:22:54 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:54 @agent_ppo2.py:191][0m |           0.0006 |          27.6659 |           3.3301 |
[32m[20230205 18:22:54 @agent_ppo2.py:191][0m |          -0.0029 |          19.0614 |           3.3232 |
[32m[20230205 18:22:54 @agent_ppo2.py:191][0m |          -0.0097 |          17.3454 |           3.3180 |
[32m[20230205 18:22:54 @agent_ppo2.py:191][0m |          -0.0108 |          16.5440 |           3.3182 |
[32m[20230205 18:22:54 @agent_ppo2.py:191][0m |          -0.0110 |          16.0895 |           3.3190 |
[32m[20230205 18:22:54 @agent_ppo2.py:191][0m |          -0.0140 |          15.6188 |           3.3196 |
[32m[20230205 18:22:54 @agent_ppo2.py:191][0m |          -0.0139 |          15.3107 |           3.3198 |
[32m[20230205 18:22:54 @agent_ppo2.py:191][0m |          -0.0146 |          15.0155 |           3.3172 |
[32m[20230205 18:22:54 @agent_ppo2.py:191][0m |          -0.0158 |          14.7564 |           3.3182 |
[32m[20230205 18:22:54 @agent_ppo2.py:191][0m |          -0.0170 |          14.4753 |           3.3190 |
[32m[20230205 18:22:54 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:22:55 @agent_ppo2.py:144][0m Average TRAINING episode reward: 254.93
[32m[20230205 18:22:55 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.18
[32m[20230205 18:22:55 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 271.71
[32m[20230205 18:22:55 @agent_ppo2.py:149][0m Total time:       9.45 min
[32m[20230205 18:22:55 @agent_ppo2.py:151][0m 618496 total steps have happened
[32m[20230205 18:22:55 @agent_ppo2.py:127][0m #------------------------ Iteration 302 --------------------------#
[32m[20230205 18:22:55 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:22:55 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:55 @agent_ppo2.py:191][0m |           0.0016 |          15.0876 |           3.2309 |
[32m[20230205 18:22:55 @agent_ppo2.py:191][0m |          -0.0134 |          12.6348 |           3.2269 |
[32m[20230205 18:22:55 @agent_ppo2.py:191][0m |          -0.0127 |          12.0408 |           3.2265 |
[32m[20230205 18:22:55 @agent_ppo2.py:191][0m |          -0.0138 |          11.6035 |           3.2243 |
[32m[20230205 18:22:56 @agent_ppo2.py:191][0m |          -0.0171 |          11.1843 |           3.2278 |
[32m[20230205 18:22:56 @agent_ppo2.py:191][0m |          -0.0180 |          10.8629 |           3.2245 |
[32m[20230205 18:22:56 @agent_ppo2.py:191][0m |          -0.0188 |          10.7065 |           3.2256 |
[32m[20230205 18:22:56 @agent_ppo2.py:191][0m |          -0.0215 |          10.3582 |           3.2237 |
[32m[20230205 18:22:56 @agent_ppo2.py:191][0m |          -0.0192 |          10.3268 |           3.2241 |
[32m[20230205 18:22:56 @agent_ppo2.py:191][0m |          -0.0210 |          10.1135 |           3.2247 |
[32m[20230205 18:22:56 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:22:56 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.01
[32m[20230205 18:22:56 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.83
[32m[20230205 18:22:56 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 174.46
[32m[20230205 18:22:56 @agent_ppo2.py:149][0m Total time:       9.48 min
[32m[20230205 18:22:56 @agent_ppo2.py:151][0m 620544 total steps have happened
[32m[20230205 18:22:56 @agent_ppo2.py:127][0m #------------------------ Iteration 303 --------------------------#
[32m[20230205 18:22:57 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:22:57 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:57 @agent_ppo2.py:191][0m |           0.0021 |          15.6326 |           3.3761 |
[32m[20230205 18:22:57 @agent_ppo2.py:191][0m |          -0.0016 |          13.8123 |           3.3784 |
[32m[20230205 18:22:57 @agent_ppo2.py:191][0m |          -0.0054 |          13.2428 |           3.3760 |
[32m[20230205 18:22:57 @agent_ppo2.py:191][0m |          -0.0074 |          12.9967 |           3.3750 |
[32m[20230205 18:22:57 @agent_ppo2.py:191][0m |          -0.0081 |          12.7945 |           3.3736 |
[32m[20230205 18:22:57 @agent_ppo2.py:191][0m |          -0.0109 |          12.4995 |           3.3762 |
[32m[20230205 18:22:57 @agent_ppo2.py:191][0m |          -0.0107 |          12.4118 |           3.3771 |
[32m[20230205 18:22:57 @agent_ppo2.py:191][0m |          -0.0110 |          12.3013 |           3.3761 |
[32m[20230205 18:22:57 @agent_ppo2.py:191][0m |          -0.0119 |          12.1307 |           3.3780 |
[32m[20230205 18:22:57 @agent_ppo2.py:191][0m |          -0.0125 |          12.0387 |           3.3780 |
[32m[20230205 18:22:57 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:22:58 @agent_ppo2.py:144][0m Average TRAINING episode reward: 256.13
[32m[20230205 18:22:58 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.20
[32m[20230205 18:22:58 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.63
[32m[20230205 18:22:58 @agent_ppo2.py:149][0m Total time:       9.51 min
[32m[20230205 18:22:58 @agent_ppo2.py:151][0m 622592 total steps have happened
[32m[20230205 18:22:58 @agent_ppo2.py:127][0m #------------------------ Iteration 304 --------------------------#
[32m[20230205 18:22:59 @agent_ppo2.py:133][0m Sampling time: 0.71 s by 1 slaves
[32m[20230205 18:22:59 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:22:59 @agent_ppo2.py:191][0m |           0.0026 |          24.2903 |           3.2407 |
[32m[20230205 18:22:59 @agent_ppo2.py:191][0m |          -0.0004 |          13.5927 |           3.2404 |
[32m[20230205 18:22:59 @agent_ppo2.py:191][0m |          -0.0049 |          12.0290 |           3.2350 |
[32m[20230205 18:22:59 @agent_ppo2.py:191][0m |          -0.0055 |          11.2872 |           3.2319 |
[32m[20230205 18:22:59 @agent_ppo2.py:191][0m |          -0.0068 |          10.9659 |           3.2362 |
[32m[20230205 18:22:59 @agent_ppo2.py:191][0m |          -0.0072 |          10.5815 |           3.2280 |
[32m[20230205 18:22:59 @agent_ppo2.py:191][0m |          -0.0096 |          10.3385 |           3.2300 |
[32m[20230205 18:22:59 @agent_ppo2.py:191][0m |          -0.0097 |          10.2043 |           3.2313 |
[32m[20230205 18:22:59 @agent_ppo2.py:191][0m |          -0.0108 |          10.0396 |           3.2306 |
[32m[20230205 18:22:59 @agent_ppo2.py:191][0m |          -0.0110 |           9.9667 |           3.2270 |
[32m[20230205 18:22:59 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:23:00 @agent_ppo2.py:144][0m Average TRAINING episode reward: 203.13
[32m[20230205 18:23:00 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.37
[32m[20230205 18:23:00 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 191.74
[32m[20230205 18:23:00 @agent_ppo2.py:149][0m Total time:       9.54 min
[32m[20230205 18:23:00 @agent_ppo2.py:151][0m 624640 total steps have happened
[32m[20230205 18:23:00 @agent_ppo2.py:127][0m #------------------------ Iteration 305 --------------------------#
[32m[20230205 18:23:00 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:23:01 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:01 @agent_ppo2.py:191][0m |          -0.0038 |          14.6733 |           3.1771 |
[32m[20230205 18:23:01 @agent_ppo2.py:191][0m |          -0.0057 |          12.0597 |           3.1726 |
[32m[20230205 18:23:01 @agent_ppo2.py:191][0m |          -0.0079 |          11.2868 |           3.1715 |
[32m[20230205 18:23:01 @agent_ppo2.py:191][0m |          -0.0126 |          10.8002 |           3.1711 |
[32m[20230205 18:23:01 @agent_ppo2.py:191][0m |          -0.0100 |          10.3117 |           3.1677 |
[32m[20230205 18:23:01 @agent_ppo2.py:191][0m |          -0.0188 |          10.0510 |           3.1670 |
[32m[20230205 18:23:01 @agent_ppo2.py:191][0m |          -0.0094 |           9.8436 |           3.1651 |
[32m[20230205 18:23:01 @agent_ppo2.py:191][0m |          -0.0135 |           9.5521 |           3.1635 |
[32m[20230205 18:23:01 @agent_ppo2.py:191][0m |          -0.0198 |           9.4443 |           3.1634 |
[32m[20230205 18:23:01 @agent_ppo2.py:191][0m |          -0.0142 |           9.1812 |           3.1611 |
[32m[20230205 18:23:01 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:23:01 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.47
[32m[20230205 18:23:01 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.87
[32m[20230205 18:23:01 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 278.82
[32m[20230205 18:23:01 @agent_ppo2.py:149][0m Total time:       9.57 min
[32m[20230205 18:23:01 @agent_ppo2.py:151][0m 626688 total steps have happened
[32m[20230205 18:23:01 @agent_ppo2.py:127][0m #------------------------ Iteration 306 --------------------------#
[32m[20230205 18:23:02 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:23:02 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:02 @agent_ppo2.py:191][0m |           0.0100 |          58.4136 |           3.1847 |
[32m[20230205 18:23:02 @agent_ppo2.py:191][0m |          -0.0110 |          42.2310 |           3.1827 |
[32m[20230205 18:23:02 @agent_ppo2.py:191][0m |          -0.0161 |          39.3889 |           3.1784 |
[32m[20230205 18:23:02 @agent_ppo2.py:191][0m |          -0.0091 |          38.8315 |           3.1763 |
[32m[20230205 18:23:02 @agent_ppo2.py:191][0m |          -0.0079 |          38.6098 |           3.1727 |
[32m[20230205 18:23:02 @agent_ppo2.py:191][0m |          -0.0026 |          34.6499 |           3.1739 |
[32m[20230205 18:23:02 @agent_ppo2.py:191][0m |          -0.0179 |          34.3067 |           3.1726 |
[32m[20230205 18:23:02 @agent_ppo2.py:191][0m |          -0.0200 |          33.9072 |           3.1730 |
[32m[20230205 18:23:02 @agent_ppo2.py:191][0m |          -0.0139 |          32.9896 |           3.1714 |
[32m[20230205 18:23:03 @agent_ppo2.py:191][0m |          -0.0154 |          32.2772 |           3.1695 |
[32m[20230205 18:23:03 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:23:03 @agent_ppo2.py:144][0m Average TRAINING episode reward: 95.21
[32m[20230205 18:23:03 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.39
[32m[20230205 18:23:03 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 268.52
[32m[20230205 18:23:03 @agent_ppo2.py:149][0m Total time:       9.59 min
[32m[20230205 18:23:03 @agent_ppo2.py:151][0m 628736 total steps have happened
[32m[20230205 18:23:03 @agent_ppo2.py:127][0m #------------------------ Iteration 307 --------------------------#
[32m[20230205 18:23:04 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:23:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:04 @agent_ppo2.py:191][0m |          -0.0048 |          43.9403 |           3.3142 |
[32m[20230205 18:23:04 @agent_ppo2.py:191][0m |          -0.0116 |          35.9584 |           3.3092 |
[32m[20230205 18:23:04 @agent_ppo2.py:191][0m |          -0.0137 |          30.5739 |           3.3099 |
[32m[20230205 18:23:04 @agent_ppo2.py:191][0m |          -0.0150 |          27.2857 |           3.3084 |
[32m[20230205 18:23:04 @agent_ppo2.py:191][0m |          -0.0153 |          25.0866 |           3.3041 |
[32m[20230205 18:23:04 @agent_ppo2.py:191][0m |          -0.0179 |          23.6153 |           3.3035 |
[32m[20230205 18:23:04 @agent_ppo2.py:191][0m |          -0.0184 |          22.3283 |           3.3045 |
[32m[20230205 18:23:04 @agent_ppo2.py:191][0m |          -0.0178 |          21.2728 |           3.3012 |
[32m[20230205 18:23:04 @agent_ppo2.py:191][0m |          -0.0185 |          20.3648 |           3.3024 |
[32m[20230205 18:23:04 @agent_ppo2.py:191][0m |          -0.0192 |          19.4709 |           3.3018 |
[32m[20230205 18:23:04 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:23:05 @agent_ppo2.py:144][0m Average TRAINING episode reward: 191.45
[32m[20230205 18:23:05 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.51
[32m[20230205 18:23:05 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.51
[32m[20230205 18:23:05 @agent_ppo2.py:149][0m Total time:       9.62 min
[32m[20230205 18:23:05 @agent_ppo2.py:151][0m 630784 total steps have happened
[32m[20230205 18:23:05 @agent_ppo2.py:127][0m #------------------------ Iteration 308 --------------------------#
[32m[20230205 18:23:05 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:23:05 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:05 @agent_ppo2.py:191][0m |          -0.0015 |          13.2519 |           3.2577 |
[32m[20230205 18:23:05 @agent_ppo2.py:191][0m |          -0.0050 |          11.6496 |           3.2541 |
[32m[20230205 18:23:05 @agent_ppo2.py:191][0m |          -0.0070 |          11.3929 |           3.2492 |
[32m[20230205 18:23:05 @agent_ppo2.py:191][0m |          -0.0091 |          10.8783 |           3.2502 |
[32m[20230205 18:23:05 @agent_ppo2.py:191][0m |          -0.0088 |          10.7342 |           3.2499 |
[32m[20230205 18:23:05 @agent_ppo2.py:191][0m |          -0.0081 |          10.6938 |           3.2501 |
[32m[20230205 18:23:05 @agent_ppo2.py:191][0m |          -0.0106 |          10.2883 |           3.2509 |
[32m[20230205 18:23:06 @agent_ppo2.py:191][0m |          -0.0136 |           9.9663 |           3.2527 |
[32m[20230205 18:23:06 @agent_ppo2.py:191][0m |          -0.0171 |           9.8603 |           3.2532 |
[32m[20230205 18:23:06 @agent_ppo2.py:191][0m |          -0.0147 |           9.7511 |           3.2540 |
[32m[20230205 18:23:06 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:23:06 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.66
[32m[20230205 18:23:06 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.72
[32m[20230205 18:23:06 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 212.03
[32m[20230205 18:23:06 @agent_ppo2.py:149][0m Total time:       9.65 min
[32m[20230205 18:23:06 @agent_ppo2.py:151][0m 632832 total steps have happened
[32m[20230205 18:23:06 @agent_ppo2.py:127][0m #------------------------ Iteration 309 --------------------------#
[32m[20230205 18:23:07 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:23:07 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:07 @agent_ppo2.py:191][0m |          -0.0031 |          22.8728 |           3.2808 |
[32m[20230205 18:23:07 @agent_ppo2.py:191][0m |          -0.0096 |          18.9167 |           3.2753 |
[32m[20230205 18:23:07 @agent_ppo2.py:191][0m |          -0.0116 |          18.1584 |           3.2769 |
[32m[20230205 18:23:07 @agent_ppo2.py:191][0m |          -0.0114 |          17.4393 |           3.2749 |
[32m[20230205 18:23:07 @agent_ppo2.py:191][0m |          -0.0132 |          17.1623 |           3.2781 |
[32m[20230205 18:23:07 @agent_ppo2.py:191][0m |          -0.0118 |          17.1740 |           3.2770 |
[32m[20230205 18:23:07 @agent_ppo2.py:191][0m |          -0.0135 |          16.4704 |           3.2805 |
[32m[20230205 18:23:07 @agent_ppo2.py:191][0m |          -0.0168 |          16.2687 |           3.2777 |
[32m[20230205 18:23:07 @agent_ppo2.py:191][0m |          -0.0154 |          16.1659 |           3.2804 |
[32m[20230205 18:23:07 @agent_ppo2.py:191][0m |          -0.0146 |          15.9655 |           3.2802 |
[32m[20230205 18:23:07 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:23:08 @agent_ppo2.py:144][0m Average TRAINING episode reward: 256.28
[32m[20230205 18:23:08 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.17
[32m[20230205 18:23:08 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.76
[32m[20230205 18:23:08 @agent_ppo2.py:149][0m Total time:       9.67 min
[32m[20230205 18:23:08 @agent_ppo2.py:151][0m 634880 total steps have happened
[32m[20230205 18:23:08 @agent_ppo2.py:127][0m #------------------------ Iteration 310 --------------------------#
[32m[20230205 18:23:08 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:23:08 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:08 @agent_ppo2.py:191][0m |          -0.0017 |          12.8053 |           3.1634 |
[32m[20230205 18:23:08 @agent_ppo2.py:191][0m |           0.0126 |          13.0638 |           3.1601 |
[32m[20230205 18:23:08 @agent_ppo2.py:191][0m |           0.0044 |          12.0125 |           3.1551 |
[32m[20230205 18:23:08 @agent_ppo2.py:191][0m |          -0.0138 |          11.3160 |           3.1534 |
[32m[20230205 18:23:09 @agent_ppo2.py:191][0m |          -0.0054 |          11.4941 |           3.1537 |
[32m[20230205 18:23:09 @agent_ppo2.py:191][0m |          -0.0138 |          11.1496 |           3.1479 |
[32m[20230205 18:23:09 @agent_ppo2.py:191][0m |          -0.0128 |          10.8744 |           3.1473 |
[32m[20230205 18:23:09 @agent_ppo2.py:191][0m |          -0.0187 |          10.7921 |           3.1479 |
[32m[20230205 18:23:09 @agent_ppo2.py:191][0m |          -0.0134 |          10.6945 |           3.1486 |
[32m[20230205 18:23:09 @agent_ppo2.py:191][0m |          -0.0172 |          10.5796 |           3.1484 |
[32m[20230205 18:23:09 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:23:09 @agent_ppo2.py:144][0m Average TRAINING episode reward: 264.34
[32m[20230205 18:23:09 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.58
[32m[20230205 18:23:09 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 222.66
[32m[20230205 18:23:09 @agent_ppo2.py:149][0m Total time:       9.70 min
[32m[20230205 18:23:09 @agent_ppo2.py:151][0m 636928 total steps have happened
[32m[20230205 18:23:09 @agent_ppo2.py:127][0m #------------------------ Iteration 311 --------------------------#
[32m[20230205 18:23:10 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:23:10 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:10 @agent_ppo2.py:191][0m |          -0.0005 |          13.0887 |           3.2791 |
[32m[20230205 18:23:10 @agent_ppo2.py:191][0m |          -0.0025 |          12.6783 |           3.2750 |
[32m[20230205 18:23:10 @agent_ppo2.py:191][0m |          -0.0074 |          12.0138 |           3.2730 |
[32m[20230205 18:23:10 @agent_ppo2.py:191][0m |          -0.0091 |          11.7831 |           3.2732 |
[32m[20230205 18:23:10 @agent_ppo2.py:191][0m |          -0.0107 |          11.5424 |           3.2725 |
[32m[20230205 18:23:10 @agent_ppo2.py:191][0m |          -0.0096 |          11.5032 |           3.2691 |
[32m[20230205 18:23:10 @agent_ppo2.py:191][0m |          -0.0120 |          11.2934 |           3.2696 |
[32m[20230205 18:23:10 @agent_ppo2.py:191][0m |          -0.0107 |          11.4088 |           3.2681 |
[32m[20230205 18:23:10 @agent_ppo2.py:191][0m |          -0.0138 |          11.0753 |           3.2699 |
[32m[20230205 18:23:10 @agent_ppo2.py:191][0m |          -0.0117 |          10.9548 |           3.2718 |
[32m[20230205 18:23:10 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:23:11 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.92
[32m[20230205 18:23:11 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.50
[32m[20230205 18:23:11 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 197.93
[32m[20230205 18:23:11 @agent_ppo2.py:149][0m Total time:       9.72 min
[32m[20230205 18:23:11 @agent_ppo2.py:151][0m 638976 total steps have happened
[32m[20230205 18:23:11 @agent_ppo2.py:127][0m #------------------------ Iteration 312 --------------------------#
[32m[20230205 18:23:11 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:23:11 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:11 @agent_ppo2.py:191][0m |          -0.0050 |          12.7561 |           3.3173 |
[32m[20230205 18:23:11 @agent_ppo2.py:191][0m |          -0.0092 |          12.1884 |           3.3056 |
[32m[20230205 18:23:12 @agent_ppo2.py:191][0m |          -0.0138 |          11.8948 |           3.3014 |
[32m[20230205 18:23:12 @agent_ppo2.py:191][0m |          -0.0145 |          11.6177 |           3.3000 |
[32m[20230205 18:23:12 @agent_ppo2.py:191][0m |          -0.0143 |          11.6511 |           3.2986 |
[32m[20230205 18:23:12 @agent_ppo2.py:191][0m |          -0.0177 |          11.3632 |           3.3003 |
[32m[20230205 18:23:12 @agent_ppo2.py:191][0m |          -0.0162 |          11.3392 |           3.3016 |
[32m[20230205 18:23:12 @agent_ppo2.py:191][0m |          -0.0182 |          11.1461 |           3.2972 |
[32m[20230205 18:23:12 @agent_ppo2.py:191][0m |          -0.0169 |          11.1177 |           3.3011 |
[32m[20230205 18:23:12 @agent_ppo2.py:191][0m |          -0.0195 |          11.0301 |           3.2988 |
[32m[20230205 18:23:12 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:23:12 @agent_ppo2.py:144][0m Average TRAINING episode reward: 256.91
[32m[20230205 18:23:12 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.24
[32m[20230205 18:23:12 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.54
[32m[20230205 18:23:12 @agent_ppo2.py:149][0m Total time:       9.75 min
[32m[20230205 18:23:12 @agent_ppo2.py:151][0m 641024 total steps have happened
[32m[20230205 18:23:12 @agent_ppo2.py:127][0m #------------------------ Iteration 313 --------------------------#
[32m[20230205 18:23:13 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:23:13 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:13 @agent_ppo2.py:191][0m |           0.0003 |          12.0504 |           3.3281 |
[32m[20230205 18:23:13 @agent_ppo2.py:191][0m |          -0.0036 |          11.6514 |           3.3228 |
[32m[20230205 18:23:13 @agent_ppo2.py:191][0m |          -0.0052 |          11.4137 |           3.3152 |
[32m[20230205 18:23:13 @agent_ppo2.py:191][0m |          -0.0067 |          11.2167 |           3.3168 |
[32m[20230205 18:23:13 @agent_ppo2.py:191][0m |          -0.0068 |          11.0788 |           3.3110 |
[32m[20230205 18:23:13 @agent_ppo2.py:191][0m |          -0.0090 |          10.9228 |           3.3117 |
[32m[20230205 18:23:13 @agent_ppo2.py:191][0m |          -0.0081 |          10.8455 |           3.3117 |
[32m[20230205 18:23:13 @agent_ppo2.py:191][0m |          -0.0095 |          10.6768 |           3.3110 |
[32m[20230205 18:23:13 @agent_ppo2.py:191][0m |          -0.0101 |          10.5599 |           3.3109 |
[32m[20230205 18:23:13 @agent_ppo2.py:191][0m |          -0.0098 |          10.5537 |           3.3117 |
[32m[20230205 18:23:13 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:23:14 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.41
[32m[20230205 18:23:14 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.15
[32m[20230205 18:23:14 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 187.59
[32m[20230205 18:23:14 @agent_ppo2.py:149][0m Total time:       9.78 min
[32m[20230205 18:23:14 @agent_ppo2.py:151][0m 643072 total steps have happened
[32m[20230205 18:23:14 @agent_ppo2.py:127][0m #------------------------ Iteration 314 --------------------------#
[32m[20230205 18:23:15 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:23:15 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:15 @agent_ppo2.py:191][0m |           0.0006 |          23.3658 |           3.3330 |
[32m[20230205 18:23:15 @agent_ppo2.py:191][0m |          -0.0091 |          16.0437 |           3.3250 |
[32m[20230205 18:23:15 @agent_ppo2.py:191][0m |          -0.0122 |          14.4784 |           3.3246 |
[32m[20230205 18:23:15 @agent_ppo2.py:191][0m |          -0.0127 |          13.7372 |           3.3231 |
[32m[20230205 18:23:15 @agent_ppo2.py:191][0m |          -0.0116 |          13.3181 |           3.3217 |
[32m[20230205 18:23:15 @agent_ppo2.py:191][0m |          -0.0144 |          12.5906 |           3.3185 |
[32m[20230205 18:23:15 @agent_ppo2.py:191][0m |          -0.0138 |          12.3041 |           3.3229 |
[32m[20230205 18:23:15 @agent_ppo2.py:191][0m |          -0.0130 |          12.0621 |           3.3206 |
[32m[20230205 18:23:15 @agent_ppo2.py:191][0m |          -0.0124 |          11.7991 |           3.3184 |
[32m[20230205 18:23:15 @agent_ppo2.py:191][0m |          -0.0149 |          11.5474 |           3.3201 |
[32m[20230205 18:23:15 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:23:16 @agent_ppo2.py:144][0m Average TRAINING episode reward: 134.59
[32m[20230205 18:23:16 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.76
[32m[20230205 18:23:16 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.91
[32m[20230205 18:23:16 @agent_ppo2.py:149][0m Total time:       9.80 min
[32m[20230205 18:23:16 @agent_ppo2.py:151][0m 645120 total steps have happened
[32m[20230205 18:23:16 @agent_ppo2.py:127][0m #------------------------ Iteration 315 --------------------------#
[32m[20230205 18:23:16 @agent_ppo2.py:133][0m Sampling time: 0.64 s by 1 slaves
[32m[20230205 18:23:16 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:16 @agent_ppo2.py:191][0m |           0.0013 |          34.6669 |           3.2768 |
[32m[20230205 18:23:16 @agent_ppo2.py:191][0m |          -0.0028 |          21.1413 |           3.2708 |
[32m[20230205 18:23:17 @agent_ppo2.py:191][0m |          -0.0075 |          17.5079 |           3.2687 |
[32m[20230205 18:23:17 @agent_ppo2.py:191][0m |          -0.0099 |          15.9767 |           3.2727 |
[32m[20230205 18:23:17 @agent_ppo2.py:191][0m |          -0.0104 |          14.8476 |           3.2711 |
[32m[20230205 18:23:17 @agent_ppo2.py:191][0m |          -0.0099 |          14.2602 |           3.2704 |
[32m[20230205 18:23:17 @agent_ppo2.py:191][0m |          -0.0126 |          13.9122 |           3.2721 |
[32m[20230205 18:23:17 @agent_ppo2.py:191][0m |          -0.0127 |          13.5946 |           3.2726 |
[32m[20230205 18:23:17 @agent_ppo2.py:191][0m |          -0.0131 |          13.2880 |           3.2739 |
[32m[20230205 18:23:17 @agent_ppo2.py:191][0m |          -0.0136 |          13.1317 |           3.2720 |
[32m[20230205 18:23:17 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:23:17 @agent_ppo2.py:144][0m Average TRAINING episode reward: 170.19
[32m[20230205 18:23:17 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.18
[32m[20230205 18:23:17 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 279.69
[32m[20230205 18:23:17 @agent_ppo2.py:149][0m Total time:       9.83 min
[32m[20230205 18:23:17 @agent_ppo2.py:151][0m 647168 total steps have happened
[32m[20230205 18:23:17 @agent_ppo2.py:127][0m #------------------------ Iteration 316 --------------------------#
[32m[20230205 18:23:18 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:23:18 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:18 @agent_ppo2.py:191][0m |           0.0013 |          14.0676 |           3.2366 |
[32m[20230205 18:23:18 @agent_ppo2.py:191][0m |          -0.0043 |          12.1525 |           3.2317 |
[32m[20230205 18:23:18 @agent_ppo2.py:191][0m |           0.0000 |          11.7575 |           3.2294 |
[32m[20230205 18:23:18 @agent_ppo2.py:191][0m |          -0.0055 |          11.4671 |           3.2328 |
[32m[20230205 18:23:18 @agent_ppo2.py:191][0m |          -0.0026 |          11.8993 |           3.2333 |
[32m[20230205 18:23:18 @agent_ppo2.py:191][0m |          -0.0121 |          10.7628 |           3.2327 |
[32m[20230205 18:23:18 @agent_ppo2.py:191][0m |          -0.0112 |          10.5677 |           3.2312 |
[32m[20230205 18:23:18 @agent_ppo2.py:191][0m |          -0.0173 |          10.7594 |           3.2369 |
[32m[20230205 18:23:18 @agent_ppo2.py:191][0m |          -0.0146 |          10.3056 |           3.2352 |
[32m[20230205 18:23:18 @agent_ppo2.py:191][0m |          -0.0185 |          10.1328 |           3.2388 |
[32m[20230205 18:23:18 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:23:19 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.81
[32m[20230205 18:23:19 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.37
[32m[20230205 18:23:19 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 279.74
[32m[20230205 18:23:19 @agent_ppo2.py:149][0m Total time:       9.86 min
[32m[20230205 18:23:19 @agent_ppo2.py:151][0m 649216 total steps have happened
[32m[20230205 18:23:19 @agent_ppo2.py:127][0m #------------------------ Iteration 317 --------------------------#
[32m[20230205 18:23:19 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:23:20 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:20 @agent_ppo2.py:191][0m |           0.0007 |          39.8478 |           3.3696 |
[32m[20230205 18:23:20 @agent_ppo2.py:191][0m |          -0.0030 |          20.1042 |           3.3684 |
[32m[20230205 18:23:20 @agent_ppo2.py:191][0m |          -0.0059 |          17.8526 |           3.3674 |
[32m[20230205 18:23:20 @agent_ppo2.py:191][0m |          -0.0069 |          16.5549 |           3.3666 |
[32m[20230205 18:23:20 @agent_ppo2.py:191][0m |          -0.0091 |          16.0086 |           3.3659 |
[32m[20230205 18:23:20 @agent_ppo2.py:191][0m |          -0.0106 |          15.3201 |           3.3689 |
[32m[20230205 18:23:20 @agent_ppo2.py:191][0m |          -0.0116 |          15.0036 |           3.3680 |
[32m[20230205 18:23:20 @agent_ppo2.py:191][0m |          -0.0128 |          14.7272 |           3.3649 |
[32m[20230205 18:23:20 @agent_ppo2.py:191][0m |          -0.0134 |          14.2198 |           3.3629 |
[32m[20230205 18:23:20 @agent_ppo2.py:191][0m |          -0.0142 |          13.9605 |           3.3643 |
[32m[20230205 18:23:20 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:23:21 @agent_ppo2.py:144][0m Average TRAINING episode reward: 136.12
[32m[20230205 18:23:21 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.85
[32m[20230205 18:23:21 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 195.66
[32m[20230205 18:23:21 @agent_ppo2.py:149][0m Total time:       9.89 min
[32m[20230205 18:23:21 @agent_ppo2.py:151][0m 651264 total steps have happened
[32m[20230205 18:23:21 @agent_ppo2.py:127][0m #------------------------ Iteration 318 --------------------------#
[32m[20230205 18:23:21 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:23:21 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:21 @agent_ppo2.py:191][0m |           0.0066 |          36.4310 |           3.3708 |
[32m[20230205 18:23:21 @agent_ppo2.py:191][0m |          -0.0058 |          25.5733 |           3.3710 |
[32m[20230205 18:23:21 @agent_ppo2.py:191][0m |          -0.0036 |          22.8839 |           3.3686 |
[32m[20230205 18:23:21 @agent_ppo2.py:191][0m |          -0.0019 |          21.5598 |           3.3688 |
[32m[20230205 18:23:21 @agent_ppo2.py:191][0m |          -0.0115 |          19.7806 |           3.3648 |
[32m[20230205 18:23:21 @agent_ppo2.py:191][0m |          -0.0103 |          18.9205 |           3.3664 |
[32m[20230205 18:23:22 @agent_ppo2.py:191][0m |          -0.0118 |          18.1283 |           3.3639 |
[32m[20230205 18:23:22 @agent_ppo2.py:191][0m |          -0.0057 |          17.7855 |           3.3618 |
[32m[20230205 18:23:22 @agent_ppo2.py:191][0m |          -0.0077 |          17.2870 |           3.3645 |
[32m[20230205 18:23:22 @agent_ppo2.py:191][0m |          -0.0103 |          16.5670 |           3.3613 |
[32m[20230205 18:23:22 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:23:22 @agent_ppo2.py:144][0m Average TRAINING episode reward: 134.56
[32m[20230205 18:23:22 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.22
[32m[20230205 18:23:22 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 142.78
[32m[20230205 18:23:22 @agent_ppo2.py:149][0m Total time:       9.92 min
[32m[20230205 18:23:22 @agent_ppo2.py:151][0m 653312 total steps have happened
[32m[20230205 18:23:22 @agent_ppo2.py:127][0m #------------------------ Iteration 319 --------------------------#
[32m[20230205 18:23:23 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:23:23 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:23 @agent_ppo2.py:191][0m |           0.0095 |          49.3154 |           3.2156 |
[32m[20230205 18:23:23 @agent_ppo2.py:191][0m |          -0.0076 |          22.0576 |           3.2153 |
[32m[20230205 18:23:23 @agent_ppo2.py:191][0m |          -0.0117 |          19.2753 |           3.2111 |
[32m[20230205 18:23:23 @agent_ppo2.py:191][0m |          -0.0109 |          17.2923 |           3.2132 |
[32m[20230205 18:23:23 @agent_ppo2.py:191][0m |          -0.0147 |          17.2049 |           3.2095 |
[32m[20230205 18:23:23 @agent_ppo2.py:191][0m |          -0.0098 |          17.0522 |           3.2081 |
[32m[20230205 18:23:23 @agent_ppo2.py:191][0m |          -0.0102 |          18.6504 |           3.2080 |
[32m[20230205 18:23:23 @agent_ppo2.py:191][0m |          -0.0147 |          17.2835 |           3.2090 |
[32m[20230205 18:23:23 @agent_ppo2.py:191][0m |           0.0002 |          17.3931 |           3.2083 |
[32m[20230205 18:23:23 @agent_ppo2.py:191][0m |          -0.0183 |          14.7044 |           3.2020 |
[32m[20230205 18:23:23 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:23:24 @agent_ppo2.py:144][0m Average TRAINING episode reward: 92.92
[32m[20230205 18:23:24 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 268.31
[32m[20230205 18:23:24 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 284.23
[32m[20230205 18:23:24 @agent_ppo2.py:149][0m Total time:       9.94 min
[32m[20230205 18:23:24 @agent_ppo2.py:151][0m 655360 total steps have happened
[32m[20230205 18:23:24 @agent_ppo2.py:127][0m #------------------------ Iteration 320 --------------------------#
[32m[20230205 18:23:24 @agent_ppo2.py:133][0m Sampling time: 0.64 s by 1 slaves
[32m[20230205 18:23:24 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:25 @agent_ppo2.py:191][0m |          -0.0032 |          28.4525 |           3.3354 |
[32m[20230205 18:23:25 @agent_ppo2.py:191][0m |          -0.0069 |          22.1114 |           3.3294 |
[32m[20230205 18:23:25 @agent_ppo2.py:191][0m |          -0.0103 |          20.8425 |           3.3305 |
[32m[20230205 18:23:25 @agent_ppo2.py:191][0m |          -0.0131 |          20.0939 |           3.3308 |
[32m[20230205 18:23:25 @agent_ppo2.py:191][0m |          -0.0148 |          19.6161 |           3.3309 |
[32m[20230205 18:23:25 @agent_ppo2.py:191][0m |          -0.0149 |          19.6596 |           3.3275 |
[32m[20230205 18:23:25 @agent_ppo2.py:191][0m |          -0.0184 |          18.8441 |           3.3290 |
[32m[20230205 18:23:25 @agent_ppo2.py:191][0m |          -0.0206 |          18.5674 |           3.3274 |
[32m[20230205 18:23:25 @agent_ppo2.py:191][0m |          -0.0135 |          19.3036 |           3.3290 |
[32m[20230205 18:23:25 @agent_ppo2.py:191][0m |          -0.0199 |          18.0267 |           3.3237 |
[32m[20230205 18:23:25 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:23:26 @agent_ppo2.py:144][0m Average TRAINING episode reward: 169.59
[32m[20230205 18:23:26 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.87
[32m[20230205 18:23:26 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 221.00
[32m[20230205 18:23:26 @agent_ppo2.py:149][0m Total time:       9.97 min
[32m[20230205 18:23:26 @agent_ppo2.py:151][0m 657408 total steps have happened
[32m[20230205 18:23:26 @agent_ppo2.py:127][0m #------------------------ Iteration 321 --------------------------#
[32m[20230205 18:23:26 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:23:26 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:26 @agent_ppo2.py:191][0m |           0.0003 |          36.4876 |           3.3213 |
[32m[20230205 18:23:26 @agent_ppo2.py:191][0m |          -0.0055 |          28.4380 |           3.3197 |
[32m[20230205 18:23:26 @agent_ppo2.py:191][0m |          -0.0086 |          25.0353 |           3.3204 |
[32m[20230205 18:23:26 @agent_ppo2.py:191][0m |          -0.0109 |          23.1521 |           3.3194 |
[32m[20230205 18:23:27 @agent_ppo2.py:191][0m |          -0.0120 |          21.9212 |           3.3183 |
[32m[20230205 18:23:27 @agent_ppo2.py:191][0m |          -0.0132 |          20.8704 |           3.3217 |
[32m[20230205 18:23:27 @agent_ppo2.py:191][0m |          -0.0145 |          20.1317 |           3.3208 |
[32m[20230205 18:23:27 @agent_ppo2.py:191][0m |          -0.0154 |          19.5584 |           3.3196 |
[32m[20230205 18:23:27 @agent_ppo2.py:191][0m |          -0.0162 |          18.9953 |           3.3200 |
[32m[20230205 18:23:27 @agent_ppo2.py:191][0m |          -0.0166 |          18.5621 |           3.3212 |
[32m[20230205 18:23:27 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:23:27 @agent_ppo2.py:144][0m Average TRAINING episode reward: 264.57
[32m[20230205 18:23:27 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 266.81
[32m[20230205 18:23:27 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 278.61
[32m[20230205 18:23:27 @agent_ppo2.py:149][0m Total time:      10.00 min
[32m[20230205 18:23:27 @agent_ppo2.py:151][0m 659456 total steps have happened
[32m[20230205 18:23:27 @agent_ppo2.py:127][0m #------------------------ Iteration 322 --------------------------#
[32m[20230205 18:23:28 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:23:28 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:28 @agent_ppo2.py:191][0m |           0.0072 |          14.7269 |           3.3392 |
[32m[20230205 18:23:28 @agent_ppo2.py:191][0m |          -0.0077 |          12.9064 |           3.3374 |
[32m[20230205 18:23:28 @agent_ppo2.py:191][0m |           0.0001 |          13.4586 |           3.3332 |
[32m[20230205 18:23:28 @agent_ppo2.py:191][0m |          -0.0125 |          12.0671 |           3.3359 |
[32m[20230205 18:23:28 @agent_ppo2.py:191][0m |          -0.0122 |          11.8136 |           3.3338 |
[32m[20230205 18:23:28 @agent_ppo2.py:191][0m |          -0.0178 |          11.6513 |           3.3325 |
[32m[20230205 18:23:28 @agent_ppo2.py:191][0m |          -0.0127 |          11.9690 |           3.3333 |
[32m[20230205 18:23:28 @agent_ppo2.py:191][0m |          -0.0131 |          11.3332 |           3.3335 |
[32m[20230205 18:23:28 @agent_ppo2.py:191][0m |          -0.0157 |          11.1579 |           3.3329 |
[32m[20230205 18:23:28 @agent_ppo2.py:191][0m |          -0.0138 |          11.1987 |           3.3326 |
[32m[20230205 18:23:28 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:23:29 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.79
[32m[20230205 18:23:29 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.14
[32m[20230205 18:23:29 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 277.63
[32m[20230205 18:23:29 @agent_ppo2.py:149][0m Total time:      10.02 min
[32m[20230205 18:23:29 @agent_ppo2.py:151][0m 661504 total steps have happened
[32m[20230205 18:23:29 @agent_ppo2.py:127][0m #------------------------ Iteration 323 --------------------------#
[32m[20230205 18:23:29 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:23:29 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:29 @agent_ppo2.py:191][0m |           0.0040 |          13.5962 |           3.2901 |
[32m[20230205 18:23:30 @agent_ppo2.py:191][0m |          -0.0078 |          12.6295 |           3.2895 |
[32m[20230205 18:23:30 @agent_ppo2.py:191][0m |          -0.0120 |          12.1692 |           3.2921 |
[32m[20230205 18:23:30 @agent_ppo2.py:191][0m |          -0.0093 |          12.2811 |           3.2898 |
[32m[20230205 18:23:30 @agent_ppo2.py:191][0m |          -0.0151 |          11.7773 |           3.2916 |
[32m[20230205 18:23:30 @agent_ppo2.py:191][0m |          -0.0087 |          12.0552 |           3.2918 |
[32m[20230205 18:23:30 @agent_ppo2.py:191][0m |          -0.0100 |          12.0409 |           3.2920 |
[32m[20230205 18:23:30 @agent_ppo2.py:191][0m |          -0.0158 |          11.5448 |           3.2890 |
[32m[20230205 18:23:30 @agent_ppo2.py:191][0m |          -0.0151 |          11.3302 |           3.2915 |
[32m[20230205 18:23:30 @agent_ppo2.py:191][0m |          -0.0143 |          11.4689 |           3.2914 |
[32m[20230205 18:23:30 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:23:31 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.56
[32m[20230205 18:23:31 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.85
[32m[20230205 18:23:31 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 218.93
[32m[20230205 18:23:31 @agent_ppo2.py:149][0m Total time:      10.05 min
[32m[20230205 18:23:31 @agent_ppo2.py:151][0m 663552 total steps have happened
[32m[20230205 18:23:31 @agent_ppo2.py:127][0m #------------------------ Iteration 324 --------------------------#
[32m[20230205 18:23:31 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:23:31 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:31 @agent_ppo2.py:191][0m |          -0.0006 |          13.0600 |           3.3742 |
[32m[20230205 18:23:31 @agent_ppo2.py:191][0m |           0.0053 |          13.1400 |           3.3701 |
[32m[20230205 18:23:31 @agent_ppo2.py:191][0m |          -0.0096 |          12.2917 |           3.3645 |
[32m[20230205 18:23:31 @agent_ppo2.py:191][0m |           0.0119 |          13.5533 |           3.3630 |
[32m[20230205 18:23:31 @agent_ppo2.py:191][0m |          -0.0112 |          11.8425 |           3.3662 |
[32m[20230205 18:23:31 @agent_ppo2.py:191][0m |          -0.0017 |          12.1980 |           3.3620 |
[32m[20230205 18:23:31 @agent_ppo2.py:191][0m |          -0.0138 |          11.5659 |           3.3608 |
[32m[20230205 18:23:31 @agent_ppo2.py:191][0m |          -0.0161 |          11.4198 |           3.3638 |
[32m[20230205 18:23:32 @agent_ppo2.py:191][0m |          -0.0035 |          11.5652 |           3.3615 |
[32m[20230205 18:23:32 @agent_ppo2.py:191][0m |          -0.0160 |          11.2052 |           3.3626 |
[32m[20230205 18:23:32 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:23:32 @agent_ppo2.py:144][0m Average TRAINING episode reward: 263.78
[32m[20230205 18:23:32 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 265.22
[32m[20230205 18:23:32 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 118.03
[32m[20230205 18:23:32 @agent_ppo2.py:149][0m Total time:      10.08 min
[32m[20230205 18:23:32 @agent_ppo2.py:151][0m 665600 total steps have happened
[32m[20230205 18:23:32 @agent_ppo2.py:127][0m #------------------------ Iteration 325 --------------------------#
[32m[20230205 18:23:33 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:23:33 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:33 @agent_ppo2.py:191][0m |          -0.0005 |          12.4545 |           3.3374 |
[32m[20230205 18:23:33 @agent_ppo2.py:191][0m |          -0.0005 |          12.3615 |           3.3382 |
[32m[20230205 18:23:33 @agent_ppo2.py:191][0m |          -0.0023 |          11.8083 |           3.3386 |
[32m[20230205 18:23:33 @agent_ppo2.py:191][0m |          -0.0069 |          11.8019 |           3.3351 |
[32m[20230205 18:23:33 @agent_ppo2.py:191][0m |          -0.0071 |          11.7135 |           3.3326 |
[32m[20230205 18:23:33 @agent_ppo2.py:191][0m |          -0.0053 |          11.8522 |           3.3350 |
[32m[20230205 18:23:33 @agent_ppo2.py:191][0m |          -0.0123 |          11.2395 |           3.3353 |
[32m[20230205 18:23:33 @agent_ppo2.py:191][0m |          -0.0130 |          11.1730 |           3.3349 |
[32m[20230205 18:23:33 @agent_ppo2.py:191][0m |          -0.0029 |          12.1128 |           3.3355 |
[32m[20230205 18:23:33 @agent_ppo2.py:191][0m |          -0.0116 |          11.0800 |           3.3350 |
[32m[20230205 18:23:33 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:23:34 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.49
[32m[20230205 18:23:34 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.55
[32m[20230205 18:23:34 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 143.57
[32m[20230205 18:23:34 @agent_ppo2.py:149][0m Total time:      10.11 min
[32m[20230205 18:23:34 @agent_ppo2.py:151][0m 667648 total steps have happened
[32m[20230205 18:23:34 @agent_ppo2.py:127][0m #------------------------ Iteration 326 --------------------------#
[32m[20230205 18:23:34 @agent_ppo2.py:133][0m Sampling time: 0.64 s by 1 slaves
[32m[20230205 18:23:35 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:35 @agent_ppo2.py:191][0m |           0.0007 |          33.4299 |           3.4449 |
[32m[20230205 18:23:35 @agent_ppo2.py:191][0m |          -0.0031 |          20.0208 |           3.4424 |
[32m[20230205 18:23:35 @agent_ppo2.py:191][0m |          -0.0088 |          17.7600 |           3.4425 |
[32m[20230205 18:23:35 @agent_ppo2.py:191][0m |          -0.0087 |          15.8959 |           3.4390 |
[32m[20230205 18:23:35 @agent_ppo2.py:191][0m |          -0.0101 |          14.7929 |           3.4370 |
[32m[20230205 18:23:35 @agent_ppo2.py:191][0m |          -0.0036 |          14.7508 |           3.4393 |
[32m[20230205 18:23:35 @agent_ppo2.py:191][0m |          -0.0102 |          13.5418 |           3.4368 |
[32m[20230205 18:23:35 @agent_ppo2.py:191][0m |          -0.0107 |          13.0729 |           3.4374 |
[32m[20230205 18:23:35 @agent_ppo2.py:191][0m |          -0.0101 |          12.5897 |           3.4371 |
[32m[20230205 18:23:35 @agent_ppo2.py:191][0m |          -0.0147 |          12.0984 |           3.4367 |
[32m[20230205 18:23:35 @agent_ppo2.py:136][0m Policy update time: 0.63 s
[32m[20230205 18:23:36 @agent_ppo2.py:144][0m Average TRAINING episode reward: 130.45
[32m[20230205 18:23:36 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.33
[32m[20230205 18:23:36 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 126.06
[32m[20230205 18:23:36 @agent_ppo2.py:149][0m Total time:      10.14 min
[32m[20230205 18:23:36 @agent_ppo2.py:151][0m 669696 total steps have happened
[32m[20230205 18:23:36 @agent_ppo2.py:127][0m #------------------------ Iteration 327 --------------------------#
[32m[20230205 18:23:36 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:23:36 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:37 @agent_ppo2.py:191][0m |          -0.0025 |          11.9672 |           3.3836 |
[32m[20230205 18:23:37 @agent_ppo2.py:191][0m |          -0.0077 |          11.2267 |           3.3782 |
[32m[20230205 18:23:37 @agent_ppo2.py:191][0m |          -0.0026 |          11.1216 |           3.3722 |
[32m[20230205 18:23:37 @agent_ppo2.py:191][0m |          -0.0106 |          10.4297 |           3.3687 |
[32m[20230205 18:23:37 @agent_ppo2.py:191][0m |          -0.0054 |          10.5825 |           3.3688 |
[32m[20230205 18:23:37 @agent_ppo2.py:191][0m |          -0.0118 |           9.9500 |           3.3667 |
[32m[20230205 18:23:37 @agent_ppo2.py:191][0m |          -0.0081 |          10.3727 |           3.3666 |
[32m[20230205 18:23:37 @agent_ppo2.py:191][0m |          -0.0136 |           9.5562 |           3.3639 |
[32m[20230205 18:23:37 @agent_ppo2.py:191][0m |          -0.0151 |           9.3729 |           3.3635 |
[32m[20230205 18:23:37 @agent_ppo2.py:191][0m |          -0.0126 |           9.2135 |           3.3621 |
[32m[20230205 18:23:37 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:23:37 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.69
[32m[20230205 18:23:37 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.12
[32m[20230205 18:23:37 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 286.60
[32m[20230205 18:23:37 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 286.60
[32m[20230205 18:23:37 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 286.60
[32m[20230205 18:23:37 @agent_ppo2.py:149][0m Total time:      10.17 min
[32m[20230205 18:23:37 @agent_ppo2.py:151][0m 671744 total steps have happened
[32m[20230205 18:23:37 @agent_ppo2.py:127][0m #------------------------ Iteration 328 --------------------------#
[32m[20230205 18:23:38 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:23:38 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:38 @agent_ppo2.py:191][0m |          -0.0074 |          11.9096 |           3.3465 |
[32m[20230205 18:23:38 @agent_ppo2.py:191][0m |          -0.0116 |          11.1443 |           3.3372 |
[32m[20230205 18:23:38 @agent_ppo2.py:191][0m |          -0.0084 |          10.5319 |           3.3349 |
[32m[20230205 18:23:38 @agent_ppo2.py:191][0m |          -0.0097 |          10.1283 |           3.3361 |
[32m[20230205 18:23:38 @agent_ppo2.py:191][0m |          -0.0179 |           9.9132 |           3.3351 |
[32m[20230205 18:23:38 @agent_ppo2.py:191][0m |          -0.0120 |           9.6254 |           3.3359 |
[32m[20230205 18:23:38 @agent_ppo2.py:191][0m |          -0.0180 |           9.5281 |           3.3327 |
[32m[20230205 18:23:38 @agent_ppo2.py:191][0m |          -0.0143 |           9.3363 |           3.3308 |
[32m[20230205 18:23:38 @agent_ppo2.py:191][0m |          -0.0176 |           9.2408 |           3.3318 |
[32m[20230205 18:23:39 @agent_ppo2.py:191][0m |          -0.0112 |           9.2435 |           3.3318 |
[32m[20230205 18:23:39 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:23:39 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.41
[32m[20230205 18:23:39 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.29
[32m[20230205 18:23:39 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 121.38
[32m[20230205 18:23:39 @agent_ppo2.py:149][0m Total time:      10.19 min
[32m[20230205 18:23:39 @agent_ppo2.py:151][0m 673792 total steps have happened
[32m[20230205 18:23:39 @agent_ppo2.py:127][0m #------------------------ Iteration 329 --------------------------#
[32m[20230205 18:23:40 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:23:40 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:40 @agent_ppo2.py:191][0m |          -0.0043 |          22.6801 |           3.3448 |
[32m[20230205 18:23:40 @agent_ppo2.py:191][0m |          -0.0110 |          15.4729 |           3.3370 |
[32m[20230205 18:23:40 @agent_ppo2.py:191][0m |          -0.0041 |          13.6219 |           3.3337 |
[32m[20230205 18:23:40 @agent_ppo2.py:191][0m |          -0.0073 |          12.9680 |           3.3283 |
[32m[20230205 18:23:40 @agent_ppo2.py:191][0m |           0.0214 |          13.6059 |           3.3287 |
[32m[20230205 18:23:40 @agent_ppo2.py:191][0m |          -0.0135 |          13.1430 |           3.3218 |
[32m[20230205 18:23:40 @agent_ppo2.py:191][0m |          -0.0087 |          11.4750 |           3.3229 |
[32m[20230205 18:23:40 @agent_ppo2.py:191][0m |          -0.0182 |          10.4489 |           3.3190 |
[32m[20230205 18:23:40 @agent_ppo2.py:191][0m |          -0.0053 |           9.8905 |           3.3184 |
[32m[20230205 18:23:40 @agent_ppo2.py:191][0m |          -0.0148 |           9.5433 |           3.3167 |
[32m[20230205 18:23:40 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:23:41 @agent_ppo2.py:144][0m Average TRAINING episode reward: 186.71
[32m[20230205 18:23:41 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.23
[32m[20230205 18:23:41 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 106.01
[32m[20230205 18:23:41 @agent_ppo2.py:149][0m Total time:      10.22 min
[32m[20230205 18:23:41 @agent_ppo2.py:151][0m 675840 total steps have happened
[32m[20230205 18:23:41 @agent_ppo2.py:127][0m #------------------------ Iteration 330 --------------------------#
[32m[20230205 18:23:41 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:23:41 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:41 @agent_ppo2.py:191][0m |           0.0002 |          13.2658 |           3.4279 |
[32m[20230205 18:23:41 @agent_ppo2.py:191][0m |          -0.0058 |          11.9811 |           3.4237 |
[32m[20230205 18:23:41 @agent_ppo2.py:191][0m |          -0.0088 |          11.3087 |           3.4211 |
[32m[20230205 18:23:41 @agent_ppo2.py:191][0m |          -0.0098 |          10.9580 |           3.4239 |
[32m[20230205 18:23:41 @agent_ppo2.py:191][0m |          -0.0111 |          10.6064 |           3.4240 |
[32m[20230205 18:23:41 @agent_ppo2.py:191][0m |          -0.0123 |          10.3222 |           3.4222 |
[32m[20230205 18:23:42 @agent_ppo2.py:191][0m |          -0.0124 |          10.1388 |           3.4198 |
[32m[20230205 18:23:42 @agent_ppo2.py:191][0m |          -0.0130 |           9.9106 |           3.4216 |
[32m[20230205 18:23:42 @agent_ppo2.py:191][0m |          -0.0131 |           9.7206 |           3.4194 |
[32m[20230205 18:23:42 @agent_ppo2.py:191][0m |          -0.0140 |           9.5368 |           3.4185 |
[32m[20230205 18:23:42 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:23:42 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.40
[32m[20230205 18:23:42 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.82
[32m[20230205 18:23:42 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 286.73
[32m[20230205 18:23:42 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 286.73
[32m[20230205 18:23:42 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 286.73
[32m[20230205 18:23:42 @agent_ppo2.py:149][0m Total time:      10.25 min
[32m[20230205 18:23:42 @agent_ppo2.py:151][0m 677888 total steps have happened
[32m[20230205 18:23:42 @agent_ppo2.py:127][0m #------------------------ Iteration 331 --------------------------#
[32m[20230205 18:23:43 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:23:43 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:43 @agent_ppo2.py:191][0m |          -0.0015 |          12.0527 |           3.4098 |
[32m[20230205 18:23:43 @agent_ppo2.py:191][0m |          -0.0070 |          11.0764 |           3.4015 |
[32m[20230205 18:23:43 @agent_ppo2.py:191][0m |          -0.0080 |          10.2760 |           3.4077 |
[32m[20230205 18:23:43 @agent_ppo2.py:191][0m |          -0.0090 |           9.6189 |           3.4075 |
[32m[20230205 18:23:43 @agent_ppo2.py:191][0m |          -0.0120 |           9.2148 |           3.4067 |
[32m[20230205 18:23:43 @agent_ppo2.py:191][0m |          -0.0089 |           9.1112 |           3.4087 |
[32m[20230205 18:23:43 @agent_ppo2.py:191][0m |          -0.0109 |           8.6553 |           3.4080 |
[32m[20230205 18:23:43 @agent_ppo2.py:191][0m |          -0.0118 |           8.3567 |           3.4087 |
[32m[20230205 18:23:43 @agent_ppo2.py:191][0m |          -0.0108 |           8.2160 |           3.4077 |
[32m[20230205 18:23:43 @agent_ppo2.py:191][0m |          -0.0091 |           8.2720 |           3.4101 |
[32m[20230205 18:23:43 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:23:44 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.48
[32m[20230205 18:23:44 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.95
[32m[20230205 18:23:44 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 37.26
[32m[20230205 18:23:44 @agent_ppo2.py:149][0m Total time:      10.27 min
[32m[20230205 18:23:44 @agent_ppo2.py:151][0m 679936 total steps have happened
[32m[20230205 18:23:44 @agent_ppo2.py:127][0m #------------------------ Iteration 332 --------------------------#
[32m[20230205 18:23:44 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:23:44 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:44 @agent_ppo2.py:191][0m |          -0.0035 |          15.3029 |           3.3651 |
[32m[20230205 18:23:44 @agent_ppo2.py:191][0m |          -0.0116 |          13.6980 |           3.3560 |
[32m[20230205 18:23:44 @agent_ppo2.py:191][0m |          -0.0105 |          13.2886 |           3.3544 |
[32m[20230205 18:23:44 @agent_ppo2.py:191][0m |          -0.0076 |          13.7626 |           3.3553 |
[32m[20230205 18:23:45 @agent_ppo2.py:191][0m |          -0.0142 |          12.9245 |           3.3497 |
[32m[20230205 18:23:45 @agent_ppo2.py:191][0m |          -0.0209 |          12.8370 |           3.3512 |
[32m[20230205 18:23:45 @agent_ppo2.py:191][0m |          -0.0173 |          12.7585 |           3.3515 |
[32m[20230205 18:23:45 @agent_ppo2.py:191][0m |          -0.0148 |          12.6454 |           3.3506 |
[32m[20230205 18:23:45 @agent_ppo2.py:191][0m |          -0.0195 |          12.6051 |           3.3515 |
[32m[20230205 18:23:45 @agent_ppo2.py:191][0m |          -0.0177 |          12.5082 |           3.3519 |
[32m[20230205 18:23:45 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:23:45 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.93
[32m[20230205 18:23:45 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 269.49
[32m[20230205 18:23:45 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 220.67
[32m[20230205 18:23:45 @agent_ppo2.py:149][0m Total time:      10.30 min
[32m[20230205 18:23:45 @agent_ppo2.py:151][0m 681984 total steps have happened
[32m[20230205 18:23:45 @agent_ppo2.py:127][0m #------------------------ Iteration 333 --------------------------#
[32m[20230205 18:23:46 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:23:46 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:46 @agent_ppo2.py:191][0m |           0.0002 |          13.5448 |           3.4150 |
[32m[20230205 18:23:46 @agent_ppo2.py:191][0m |          -0.0031 |          12.5022 |           3.4109 |
[32m[20230205 18:23:46 @agent_ppo2.py:191][0m |          -0.0048 |          12.0464 |           3.4097 |
[32m[20230205 18:23:46 @agent_ppo2.py:191][0m |          -0.0051 |          11.7971 |           3.4097 |
[32m[20230205 18:23:46 @agent_ppo2.py:191][0m |          -0.0066 |          11.6311 |           3.4082 |
[32m[20230205 18:23:46 @agent_ppo2.py:191][0m |          -0.0077 |          11.4629 |           3.4086 |
[32m[20230205 18:23:46 @agent_ppo2.py:191][0m |          -0.0080 |          11.3837 |           3.4086 |
[32m[20230205 18:23:46 @agent_ppo2.py:191][0m |          -0.0083 |          11.2952 |           3.4078 |
[32m[20230205 18:23:46 @agent_ppo2.py:191][0m |          -0.0096 |          11.1755 |           3.4067 |
[32m[20230205 18:23:46 @agent_ppo2.py:191][0m |          -0.0088 |          11.1377 |           3.4079 |
[32m[20230205 18:23:46 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:23:47 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.61
[32m[20230205 18:23:47 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.13
[32m[20230205 18:23:47 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 285.25
[32m[20230205 18:23:47 @agent_ppo2.py:149][0m Total time:      10.33 min
[32m[20230205 18:23:47 @agent_ppo2.py:151][0m 684032 total steps have happened
[32m[20230205 18:23:47 @agent_ppo2.py:127][0m #------------------------ Iteration 334 --------------------------#
[32m[20230205 18:23:47 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:23:47 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:48 @agent_ppo2.py:191][0m |           0.0006 |          13.4518 |           3.4494 |
[32m[20230205 18:23:48 @agent_ppo2.py:191][0m |          -0.0026 |          13.1281 |           3.4488 |
[32m[20230205 18:23:48 @agent_ppo2.py:191][0m |          -0.0053 |          12.9122 |           3.4496 |
[32m[20230205 18:23:48 @agent_ppo2.py:191][0m |          -0.0065 |          12.7183 |           3.4458 |
[32m[20230205 18:23:48 @agent_ppo2.py:191][0m |          -0.0068 |          12.6031 |           3.4495 |
[32m[20230205 18:23:48 @agent_ppo2.py:191][0m |          -0.0091 |          12.4886 |           3.4440 |
[32m[20230205 18:23:48 @agent_ppo2.py:191][0m |          -0.0091 |          12.4300 |           3.4452 |
[32m[20230205 18:23:48 @agent_ppo2.py:191][0m |          -0.0102 |          12.2998 |           3.4469 |
[32m[20230205 18:23:48 @agent_ppo2.py:191][0m |          -0.0106 |          12.2242 |           3.4459 |
[32m[20230205 18:23:48 @agent_ppo2.py:191][0m |          -0.0116 |          12.1050 |           3.4467 |
[32m[20230205 18:23:48 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:23:49 @agent_ppo2.py:144][0m Average TRAINING episode reward: 263.23
[32m[20230205 18:23:49 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 266.82
[32m[20230205 18:23:49 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 209.77
[32m[20230205 18:23:49 @agent_ppo2.py:149][0m Total time:      10.35 min
[32m[20230205 18:23:49 @agent_ppo2.py:151][0m 686080 total steps have happened
[32m[20230205 18:23:49 @agent_ppo2.py:127][0m #------------------------ Iteration 335 --------------------------#
[32m[20230205 18:23:49 @agent_ppo2.py:133][0m Sampling time: 0.69 s by 1 slaves
[32m[20230205 18:23:49 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:49 @agent_ppo2.py:191][0m |           0.0011 |          21.7746 |           3.3853 |
[32m[20230205 18:23:49 @agent_ppo2.py:191][0m |          -0.0045 |          12.1177 |           3.3850 |
[32m[20230205 18:23:49 @agent_ppo2.py:191][0m |          -0.0075 |          10.6645 |           3.3834 |
[32m[20230205 18:23:50 @agent_ppo2.py:191][0m |          -0.0099 |           9.9810 |           3.3843 |
[32m[20230205 18:23:50 @agent_ppo2.py:191][0m |          -0.0099 |           9.5698 |           3.3810 |
[32m[20230205 18:23:50 @agent_ppo2.py:191][0m |          -0.0118 |           9.2228 |           3.3794 |
[32m[20230205 18:23:50 @agent_ppo2.py:191][0m |          -0.0104 |           8.9865 |           3.3807 |
[32m[20230205 18:23:50 @agent_ppo2.py:191][0m |          -0.0126 |           8.7267 |           3.3799 |
[32m[20230205 18:23:50 @agent_ppo2.py:191][0m |          -0.0124 |           8.4736 |           3.3779 |
[32m[20230205 18:23:50 @agent_ppo2.py:191][0m |          -0.0132 |           8.3136 |           3.3779 |
[32m[20230205 18:23:50 @agent_ppo2.py:136][0m Policy update time: 0.69 s
[32m[20230205 18:23:50 @agent_ppo2.py:144][0m Average TRAINING episode reward: 181.39
[32m[20230205 18:23:50 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.02
[32m[20230205 18:23:50 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 287.08
[32m[20230205 18:23:50 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 287.08
[32m[20230205 18:23:50 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 287.08
[32m[20230205 18:23:50 @agent_ppo2.py:149][0m Total time:      10.38 min
[32m[20230205 18:23:50 @agent_ppo2.py:151][0m 688128 total steps have happened
[32m[20230205 18:23:50 @agent_ppo2.py:127][0m #------------------------ Iteration 336 --------------------------#
[32m[20230205 18:23:51 @agent_ppo2.py:133][0m Sampling time: 0.65 s by 1 slaves
[32m[20230205 18:23:51 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:51 @agent_ppo2.py:191][0m |           0.0061 |          27.4071 |           3.3917 |
[32m[20230205 18:23:51 @agent_ppo2.py:191][0m |          -0.0073 |          19.8056 |           3.3832 |
[32m[20230205 18:23:51 @agent_ppo2.py:191][0m |          -0.0086 |          16.9825 |           3.3753 |
[32m[20230205 18:23:51 @agent_ppo2.py:191][0m |          -0.0165 |          14.7484 |           3.3760 |
[32m[20230205 18:23:51 @agent_ppo2.py:191][0m |          -0.0203 |          13.8709 |           3.3710 |
[32m[20230205 18:23:51 @agent_ppo2.py:191][0m |          -0.0175 |          12.8174 |           3.3704 |
[32m[20230205 18:23:51 @agent_ppo2.py:191][0m |          -0.0178 |          12.0276 |           3.3731 |
[32m[20230205 18:23:52 @agent_ppo2.py:191][0m |          -0.0224 |          11.6541 |           3.3725 |
[32m[20230205 18:23:52 @agent_ppo2.py:191][0m |          -0.0157 |          11.3442 |           3.3695 |
[32m[20230205 18:23:52 @agent_ppo2.py:191][0m |          -0.0190 |          11.3931 |           3.3724 |
[32m[20230205 18:23:52 @agent_ppo2.py:136][0m Policy update time: 0.65 s
[32m[20230205 18:23:52 @agent_ppo2.py:144][0m Average TRAINING episode reward: 172.21
[32m[20230205 18:23:52 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.65
[32m[20230205 18:23:52 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 171.77
[32m[20230205 18:23:52 @agent_ppo2.py:149][0m Total time:      10.41 min
[32m[20230205 18:23:52 @agent_ppo2.py:151][0m 690176 total steps have happened
[32m[20230205 18:23:52 @agent_ppo2.py:127][0m #------------------------ Iteration 337 --------------------------#
[32m[20230205 18:23:53 @agent_ppo2.py:133][0m Sampling time: 0.58 s by 1 slaves
[32m[20230205 18:23:53 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:53 @agent_ppo2.py:191][0m |          -0.0020 |          13.6469 |           3.3879 |
[32m[20230205 18:23:53 @agent_ppo2.py:191][0m |          -0.0094 |          11.9679 |           3.3694 |
[32m[20230205 18:23:53 @agent_ppo2.py:191][0m |          -0.0137 |          11.4204 |           3.3701 |
[32m[20230205 18:23:53 @agent_ppo2.py:191][0m |          -0.0135 |          11.0773 |           3.3721 |
[32m[20230205 18:23:53 @agent_ppo2.py:191][0m |          -0.0140 |          10.7879 |           3.3727 |
[32m[20230205 18:23:53 @agent_ppo2.py:191][0m |          -0.0158 |          10.5514 |           3.3711 |
[32m[20230205 18:23:53 @agent_ppo2.py:191][0m |          -0.0139 |          10.5366 |           3.3694 |
[32m[20230205 18:23:53 @agent_ppo2.py:191][0m |          -0.0153 |          10.2632 |           3.3716 |
[32m[20230205 18:23:53 @agent_ppo2.py:191][0m |          -0.0167 |          10.1078 |           3.3724 |
[32m[20230205 18:23:53 @agent_ppo2.py:191][0m |          -0.0184 |           9.9514 |           3.3701 |
[32m[20230205 18:23:53 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:23:54 @agent_ppo2.py:144][0m Average TRAINING episode reward: 249.68
[32m[20230205 18:23:54 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.83
[32m[20230205 18:23:54 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 290.51
[32m[20230205 18:23:54 @agent_ppo2.py:112][0m [4m[34mCRITICAL[0m Get the best episode reward: 290.51
[32m[20230205 18:23:54 @agent_ppo2.py:116][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 290.51
[32m[20230205 18:23:54 @agent_ppo2.py:149][0m Total time:      10.44 min
[32m[20230205 18:23:54 @agent_ppo2.py:151][0m 692224 total steps have happened
[32m[20230205 18:23:54 @agent_ppo2.py:127][0m #------------------------ Iteration 338 --------------------------#
[32m[20230205 18:23:54 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:23:54 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:54 @agent_ppo2.py:191][0m |          -0.0028 |          52.7955 |           3.4123 |
[32m[20230205 18:23:54 @agent_ppo2.py:191][0m |          -0.0080 |          36.5454 |           3.4068 |
[32m[20230205 18:23:54 @agent_ppo2.py:191][0m |          -0.0097 |          28.3312 |           3.4055 |
[32m[20230205 18:23:55 @agent_ppo2.py:191][0m |          -0.0129 |          25.3457 |           3.4069 |
[32m[20230205 18:23:55 @agent_ppo2.py:191][0m |          -0.0130 |          23.9194 |           3.4096 |
[32m[20230205 18:23:55 @agent_ppo2.py:191][0m |          -0.0136 |          22.8231 |           3.4088 |
[32m[20230205 18:23:55 @agent_ppo2.py:191][0m |          -0.0147 |          21.8485 |           3.4089 |
[32m[20230205 18:23:55 @agent_ppo2.py:191][0m |          -0.0125 |          21.1785 |           3.4090 |
[32m[20230205 18:23:55 @agent_ppo2.py:191][0m |          -0.0145 |          20.5717 |           3.4099 |
[32m[20230205 18:23:55 @agent_ppo2.py:191][0m |          -0.0042 |          19.7218 |           3.4094 |
[32m[20230205 18:23:55 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:23:55 @agent_ppo2.py:144][0m Average TRAINING episode reward: 204.22
[32m[20230205 18:23:55 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.45
[32m[20230205 18:23:55 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 285.30
[32m[20230205 18:23:55 @agent_ppo2.py:149][0m Total time:      10.46 min
[32m[20230205 18:23:55 @agent_ppo2.py:151][0m 694272 total steps have happened
[32m[20230205 18:23:55 @agent_ppo2.py:127][0m #------------------------ Iteration 339 --------------------------#
[32m[20230205 18:23:56 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:23:56 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:56 @agent_ppo2.py:191][0m |           0.0013 |          21.2927 |           3.3919 |
[32m[20230205 18:23:56 @agent_ppo2.py:191][0m |          -0.0060 |          15.6233 |           3.3901 |
[32m[20230205 18:23:56 @agent_ppo2.py:191][0m |          -0.0072 |          14.7107 |           3.3815 |
[32m[20230205 18:23:56 @agent_ppo2.py:191][0m |          -0.0104 |          14.0068 |           3.3809 |
[32m[20230205 18:23:56 @agent_ppo2.py:191][0m |          -0.0118 |          13.5293 |           3.3803 |
[32m[20230205 18:23:56 @agent_ppo2.py:191][0m |          -0.0129 |          13.1318 |           3.3796 |
[32m[20230205 18:23:56 @agent_ppo2.py:191][0m |          -0.0146 |          12.7367 |           3.3762 |
[32m[20230205 18:23:56 @agent_ppo2.py:191][0m |          -0.0158 |          12.4414 |           3.3762 |
[32m[20230205 18:23:56 @agent_ppo2.py:191][0m |          -0.0141 |          12.0392 |           3.3736 |
[32m[20230205 18:23:56 @agent_ppo2.py:191][0m |          -0.0137 |          11.8997 |           3.3739 |
[32m[20230205 18:23:56 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:23:57 @agent_ppo2.py:144][0m Average TRAINING episode reward: 263.37
[32m[20230205 18:23:57 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 265.17
[32m[20230205 18:23:57 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 287.28
[32m[20230205 18:23:57 @agent_ppo2.py:149][0m Total time:      10.49 min
[32m[20230205 18:23:57 @agent_ppo2.py:151][0m 696320 total steps have happened
[32m[20230205 18:23:57 @agent_ppo2.py:127][0m #------------------------ Iteration 340 --------------------------#
[32m[20230205 18:23:57 @agent_ppo2.py:133][0m Sampling time: 0.64 s by 1 slaves
[32m[20230205 18:23:57 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:57 @agent_ppo2.py:191][0m |          -0.0016 |          25.0024 |           3.4103 |
[32m[20230205 18:23:58 @agent_ppo2.py:191][0m |          -0.0053 |          20.5200 |           3.4062 |
[32m[20230205 18:23:58 @agent_ppo2.py:191][0m |          -0.0058 |          18.7055 |           3.4085 |
[32m[20230205 18:23:58 @agent_ppo2.py:191][0m |          -0.0080 |          17.8812 |           3.4094 |
[32m[20230205 18:23:58 @agent_ppo2.py:191][0m |          -0.0096 |          17.1046 |           3.4080 |
[32m[20230205 18:23:58 @agent_ppo2.py:191][0m |          -0.0120 |          16.5392 |           3.4079 |
[32m[20230205 18:23:58 @agent_ppo2.py:191][0m |          -0.0092 |          16.3740 |           3.4103 |
[32m[20230205 18:23:58 @agent_ppo2.py:191][0m |          -0.0115 |          15.7805 |           3.4078 |
[32m[20230205 18:23:58 @agent_ppo2.py:191][0m |          -0.0130 |          15.4593 |           3.4046 |
[32m[20230205 18:23:58 @agent_ppo2.py:191][0m |          -0.0129 |          15.2368 |           3.4094 |
[32m[20230205 18:23:58 @agent_ppo2.py:136][0m Policy update time: 0.64 s
[32m[20230205 18:23:58 @agent_ppo2.py:144][0m Average TRAINING episode reward: 181.06
[32m[20230205 18:23:58 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.66
[32m[20230205 18:23:58 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 117.44
[32m[20230205 18:23:58 @agent_ppo2.py:149][0m Total time:      10.52 min
[32m[20230205 18:23:58 @agent_ppo2.py:151][0m 698368 total steps have happened
[32m[20230205 18:23:58 @agent_ppo2.py:127][0m #------------------------ Iteration 341 --------------------------#
[32m[20230205 18:23:59 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:23:59 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:23:59 @agent_ppo2.py:191][0m |          -0.0094 |          17.1084 |           3.2859 |
[32m[20230205 18:23:59 @agent_ppo2.py:191][0m |          -0.0063 |          14.2844 |           3.2742 |
[32m[20230205 18:23:59 @agent_ppo2.py:191][0m |          -0.0069 |          13.8569 |           3.2786 |
[32m[20230205 18:23:59 @agent_ppo2.py:191][0m |           0.0146 |          18.6499 |           3.2755 |
[32m[20230205 18:23:59 @agent_ppo2.py:191][0m |          -0.0035 |          13.6614 |           3.2723 |
[32m[20230205 18:23:59 @agent_ppo2.py:191][0m |          -0.0116 |          13.2115 |           3.2720 |
[32m[20230205 18:23:59 @agent_ppo2.py:191][0m |          -0.0171 |          13.0860 |           3.2706 |
[32m[20230205 18:23:59 @agent_ppo2.py:191][0m |          -0.0184 |          12.9949 |           3.2706 |
[32m[20230205 18:23:59 @agent_ppo2.py:191][0m |          -0.0019 |          13.9351 |           3.2694 |
[32m[20230205 18:24:00 @agent_ppo2.py:191][0m |          -0.0081 |          12.9102 |           3.2649 |
[32m[20230205 18:24:00 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:24:00 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.35
[32m[20230205 18:24:00 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.94
[32m[20230205 18:24:00 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 72.19
[32m[20230205 18:24:00 @agent_ppo2.py:149][0m Total time:      10.55 min
[32m[20230205 18:24:00 @agent_ppo2.py:151][0m 700416 total steps have happened
[32m[20230205 18:24:00 @agent_ppo2.py:127][0m #------------------------ Iteration 342 --------------------------#
[32m[20230205 18:24:01 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:24:01 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:01 @agent_ppo2.py:191][0m |          -0.0018 |          31.8436 |           3.3545 |
[32m[20230205 18:24:01 @agent_ppo2.py:191][0m |          -0.0062 |          26.5522 |           3.3511 |
[32m[20230205 18:24:01 @agent_ppo2.py:191][0m |          -0.0090 |          25.4055 |           3.3482 |
[32m[20230205 18:24:01 @agent_ppo2.py:191][0m |          -0.0103 |          24.4045 |           3.3437 |
[32m[20230205 18:24:01 @agent_ppo2.py:191][0m |          -0.0119 |          23.6267 |           3.3430 |
[32m[20230205 18:24:01 @agent_ppo2.py:191][0m |          -0.0114 |          22.8746 |           3.3408 |
[32m[20230205 18:24:01 @agent_ppo2.py:191][0m |          -0.0133 |          22.4420 |           3.3395 |
[32m[20230205 18:24:01 @agent_ppo2.py:191][0m |          -0.0101 |          21.7859 |           3.3381 |
[32m[20230205 18:24:01 @agent_ppo2.py:191][0m |          -0.0128 |          20.5274 |           3.3359 |
[32m[20230205 18:24:01 @agent_ppo2.py:191][0m |          -0.0118 |          19.7716 |           3.3358 |
[32m[20230205 18:24:01 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:24:02 @agent_ppo2.py:144][0m Average TRAINING episode reward: 150.47
[32m[20230205 18:24:02 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.77
[32m[20230205 18:24:02 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 284.62
[32m[20230205 18:24:02 @agent_ppo2.py:149][0m Total time:      10.57 min
[32m[20230205 18:24:02 @agent_ppo2.py:151][0m 702464 total steps have happened
[32m[20230205 18:24:02 @agent_ppo2.py:127][0m #------------------------ Iteration 343 --------------------------#
[32m[20230205 18:24:02 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:24:02 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:02 @agent_ppo2.py:191][0m |          -0.0003 |          25.7510 |           3.3461 |
[32m[20230205 18:24:02 @agent_ppo2.py:191][0m |          -0.0040 |          19.1943 |           3.3494 |
[32m[20230205 18:24:03 @agent_ppo2.py:191][0m |          -0.0073 |          16.7953 |           3.3498 |
[32m[20230205 18:24:03 @agent_ppo2.py:191][0m |           0.0014 |          17.4765 |           3.3493 |
[32m[20230205 18:24:03 @agent_ppo2.py:191][0m |          -0.0113 |          15.2234 |           3.3478 |
[32m[20230205 18:24:03 @agent_ppo2.py:191][0m |          -0.0121 |          14.5149 |           3.3502 |
[32m[20230205 18:24:03 @agent_ppo2.py:191][0m |          -0.0118 |          14.0621 |           3.3456 |
[32m[20230205 18:24:03 @agent_ppo2.py:191][0m |          -0.0113 |          13.6944 |           3.3489 |
[32m[20230205 18:24:03 @agent_ppo2.py:191][0m |          -0.0270 |          13.4200 |           3.3488 |
[32m[20230205 18:24:03 @agent_ppo2.py:191][0m |          -0.0180 |          13.3597 |           3.3481 |
[32m[20230205 18:24:03 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:24:03 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.80
[32m[20230205 18:24:03 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.14
[32m[20230205 18:24:03 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 186.57
[32m[20230205 18:24:03 @agent_ppo2.py:149][0m Total time:      10.60 min
[32m[20230205 18:24:03 @agent_ppo2.py:151][0m 704512 total steps have happened
[32m[20230205 18:24:03 @agent_ppo2.py:127][0m #------------------------ Iteration 344 --------------------------#
[32m[20230205 18:24:04 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:24:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:04 @agent_ppo2.py:191][0m |          -0.0113 |          13.9171 |           3.3545 |
[32m[20230205 18:24:04 @agent_ppo2.py:191][0m |          -0.0026 |          12.6419 |           3.3538 |
[32m[20230205 18:24:04 @agent_ppo2.py:191][0m |          -0.0058 |          12.2711 |           3.3484 |
[32m[20230205 18:24:04 @agent_ppo2.py:191][0m |           0.0429 |          18.6584 |           3.3491 |
[32m[20230205 18:24:04 @agent_ppo2.py:191][0m |          -0.0150 |          12.0319 |           3.3433 |
[32m[20230205 18:24:04 @agent_ppo2.py:191][0m |          -0.0151 |          11.8576 |           3.3455 |
[32m[20230205 18:24:04 @agent_ppo2.py:191][0m |          -0.0012 |          12.1821 |           3.3464 |
[32m[20230205 18:24:04 @agent_ppo2.py:191][0m |          -0.0094 |          11.5779 |           3.3453 |
[32m[20230205 18:24:04 @agent_ppo2.py:191][0m |          -0.0138 |          11.5278 |           3.3463 |
[32m[20230205 18:24:04 @agent_ppo2.py:191][0m |          -0.0193 |          11.4501 |           3.3477 |
[32m[20230205 18:24:04 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:24:05 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.57
[32m[20230205 18:24:05 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.71
[32m[20230205 18:24:05 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 284.89
[32m[20230205 18:24:05 @agent_ppo2.py:149][0m Total time:      10.63 min
[32m[20230205 18:24:05 @agent_ppo2.py:151][0m 706560 total steps have happened
[32m[20230205 18:24:05 @agent_ppo2.py:127][0m #------------------------ Iteration 345 --------------------------#
[32m[20230205 18:24:05 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:24:05 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:05 @agent_ppo2.py:191][0m |           0.0019 |          12.1247 |           3.4951 |
[32m[20230205 18:24:06 @agent_ppo2.py:191][0m |          -0.0036 |          11.5818 |           3.4922 |
[32m[20230205 18:24:06 @agent_ppo2.py:191][0m |          -0.0065 |          11.2121 |           3.4936 |
[32m[20230205 18:24:06 @agent_ppo2.py:191][0m |          -0.0089 |          11.0727 |           3.4932 |
[32m[20230205 18:24:06 @agent_ppo2.py:191][0m |          -0.0105 |          10.8536 |           3.4924 |
[32m[20230205 18:24:06 @agent_ppo2.py:191][0m |          -0.0113 |          10.6833 |           3.4916 |
[32m[20230205 18:24:06 @agent_ppo2.py:191][0m |          -0.0115 |          10.5813 |           3.4910 |
[32m[20230205 18:24:06 @agent_ppo2.py:191][0m |          -0.0119 |          10.3954 |           3.4899 |
[32m[20230205 18:24:06 @agent_ppo2.py:191][0m |          -0.0130 |          10.2419 |           3.4891 |
[32m[20230205 18:24:06 @agent_ppo2.py:191][0m |          -0.0137 |          10.1240 |           3.4897 |
[32m[20230205 18:24:06 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:24:07 @agent_ppo2.py:144][0m Average TRAINING episode reward: 263.67
[32m[20230205 18:24:07 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 267.35
[32m[20230205 18:24:07 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 206.20
[32m[20230205 18:24:07 @agent_ppo2.py:149][0m Total time:      10.65 min
[32m[20230205 18:24:07 @agent_ppo2.py:151][0m 708608 total steps have happened
[32m[20230205 18:24:07 @agent_ppo2.py:127][0m #------------------------ Iteration 346 --------------------------#
[32m[20230205 18:24:07 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:24:07 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:07 @agent_ppo2.py:191][0m |           0.0172 |          29.3790 |           3.4214 |
[32m[20230205 18:24:07 @agent_ppo2.py:191][0m |          -0.0004 |          25.2070 |           3.4189 |
[32m[20230205 18:24:07 @agent_ppo2.py:191][0m |          -0.0075 |          22.9670 |           3.4184 |
[32m[20230205 18:24:07 @agent_ppo2.py:191][0m |          -0.0058 |          22.2174 |           3.4167 |
[32m[20230205 18:24:07 @agent_ppo2.py:191][0m |          -0.0080 |          21.8004 |           3.4123 |
[32m[20230205 18:24:07 @agent_ppo2.py:191][0m |          -0.0148 |          21.0010 |           3.4104 |
[32m[20230205 18:24:07 @agent_ppo2.py:191][0m |          -0.0120 |          20.7376 |           3.4087 |
[32m[20230205 18:24:08 @agent_ppo2.py:191][0m |          -0.0115 |          20.5543 |           3.4085 |
[32m[20230205 18:24:08 @agent_ppo2.py:191][0m |          -0.0070 |          20.6163 |           3.4071 |
[32m[20230205 18:24:08 @agent_ppo2.py:191][0m |          -0.0151 |          19.5864 |           3.4079 |
[32m[20230205 18:24:08 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:24:08 @agent_ppo2.py:144][0m Average TRAINING episode reward: 151.87
[32m[20230205 18:24:08 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.22
[32m[20230205 18:24:08 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 225.07
[32m[20230205 18:24:08 @agent_ppo2.py:149][0m Total time:      10.68 min
[32m[20230205 18:24:08 @agent_ppo2.py:151][0m 710656 total steps have happened
[32m[20230205 18:24:08 @agent_ppo2.py:127][0m #------------------------ Iteration 347 --------------------------#
[32m[20230205 18:24:09 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:24:09 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:09 @agent_ppo2.py:191][0m |          -0.0010 |          86.0198 |           3.3952 |
[32m[20230205 18:24:09 @agent_ppo2.py:191][0m |          -0.0098 |          62.1381 |           3.3946 |
[32m[20230205 18:24:09 @agent_ppo2.py:191][0m |          -0.0063 |          52.2787 |           3.3918 |
[32m[20230205 18:24:09 @agent_ppo2.py:191][0m |          -0.0044 |          46.0620 |           3.3876 |
[32m[20230205 18:24:09 @agent_ppo2.py:191][0m |          -0.0056 |          37.6072 |           3.3895 |
[32m[20230205 18:24:09 @agent_ppo2.py:191][0m |          -0.0037 |          31.2622 |           3.3867 |
[32m[20230205 18:24:09 @agent_ppo2.py:191][0m |          -0.0171 |          28.0285 |           3.3840 |
[32m[20230205 18:24:09 @agent_ppo2.py:191][0m |          -0.0150 |          25.9113 |           3.3884 |
[32m[20230205 18:24:09 @agent_ppo2.py:191][0m |          -0.0119 |          26.4294 |           3.3850 |
[32m[20230205 18:24:09 @agent_ppo2.py:191][0m |          -0.0175 |          23.5418 |           3.3813 |
[32m[20230205 18:24:09 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:24:10 @agent_ppo2.py:144][0m Average TRAINING episode reward: 100.39
[32m[20230205 18:24:10 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 265.23
[32m[20230205 18:24:10 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 109.90
[32m[20230205 18:24:10 @agent_ppo2.py:149][0m Total time:      10.71 min
[32m[20230205 18:24:10 @agent_ppo2.py:151][0m 712704 total steps have happened
[32m[20230205 18:24:10 @agent_ppo2.py:127][0m #------------------------ Iteration 348 --------------------------#
[32m[20230205 18:24:10 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:24:11 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:11 @agent_ppo2.py:191][0m |          -0.0017 |          20.0542 |           3.5037 |
[32m[20230205 18:24:11 @agent_ppo2.py:191][0m |          -0.0067 |          15.2929 |           3.5003 |
[32m[20230205 18:24:11 @agent_ppo2.py:191][0m |          -0.0076 |          14.7906 |           3.4997 |
[32m[20230205 18:24:11 @agent_ppo2.py:191][0m |          -0.0124 |          14.2499 |           3.4971 |
[32m[20230205 18:24:11 @agent_ppo2.py:191][0m |          -0.0147 |          13.8914 |           3.4969 |
[32m[20230205 18:24:11 @agent_ppo2.py:191][0m |          -0.0126 |          13.7036 |           3.4969 |
[32m[20230205 18:24:11 @agent_ppo2.py:191][0m |          -0.0136 |          13.4592 |           3.4963 |
[32m[20230205 18:24:11 @agent_ppo2.py:191][0m |          -0.0145 |          13.3049 |           3.4977 |
[32m[20230205 18:24:11 @agent_ppo2.py:191][0m |          -0.0142 |          13.3162 |           3.4942 |
[32m[20230205 18:24:11 @agent_ppo2.py:191][0m |          -0.0153 |          13.1580 |           3.4962 |
[32m[20230205 18:24:11 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:24:12 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.73
[32m[20230205 18:24:12 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.92
[32m[20230205 18:24:12 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 227.94
[32m[20230205 18:24:12 @agent_ppo2.py:149][0m Total time:      10.74 min
[32m[20230205 18:24:12 @agent_ppo2.py:151][0m 714752 total steps have happened
[32m[20230205 18:24:12 @agent_ppo2.py:127][0m #------------------------ Iteration 349 --------------------------#
[32m[20230205 18:24:12 @agent_ppo2.py:133][0m Sampling time: 0.47 s by 1 slaves
[32m[20230205 18:24:12 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:12 @agent_ppo2.py:191][0m |          -0.0004 |          49.0746 |           3.4012 |
[32m[20230205 18:24:12 @agent_ppo2.py:191][0m |          -0.0058 |          35.1957 |           3.3955 |
[32m[20230205 18:24:12 @agent_ppo2.py:191][0m |          -0.0085 |          30.6889 |           3.3905 |
[32m[20230205 18:24:12 @agent_ppo2.py:191][0m |          -0.0102 |          28.3284 |           3.3877 |
[32m[20230205 18:24:12 @agent_ppo2.py:191][0m |          -0.0118 |          27.3537 |           3.3872 |
[32m[20230205 18:24:12 @agent_ppo2.py:191][0m |          -0.0122 |          26.2779 |           3.3841 |
[32m[20230205 18:24:12 @agent_ppo2.py:191][0m |          -0.0130 |          25.9316 |           3.3825 |
[32m[20230205 18:24:12 @agent_ppo2.py:191][0m |          -0.0142 |          25.3232 |           3.3817 |
[32m[20230205 18:24:13 @agent_ppo2.py:191][0m |          -0.0142 |          25.6351 |           3.3794 |
[32m[20230205 18:24:13 @agent_ppo2.py:191][0m |          -0.0144 |          24.1706 |           3.3790 |
[32m[20230205 18:24:13 @agent_ppo2.py:136][0m Policy update time: 0.47 s
[32m[20230205 18:24:13 @agent_ppo2.py:144][0m Average TRAINING episode reward: 189.85
[32m[20230205 18:24:13 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.43
[32m[20230205 18:24:13 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 278.50
[32m[20230205 18:24:13 @agent_ppo2.py:149][0m Total time:      10.76 min
[32m[20230205 18:24:13 @agent_ppo2.py:151][0m 716800 total steps have happened
[32m[20230205 18:24:13 @agent_ppo2.py:127][0m #------------------------ Iteration 350 --------------------------#
[32m[20230205 18:24:14 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:24:14 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:14 @agent_ppo2.py:191][0m |           0.0018 |          38.2130 |           3.3293 |
[32m[20230205 18:24:14 @agent_ppo2.py:191][0m |          -0.0067 |          25.8573 |           3.3166 |
[32m[20230205 18:24:14 @agent_ppo2.py:191][0m |          -0.0176 |          22.6586 |           3.3174 |
[32m[20230205 18:24:14 @agent_ppo2.py:191][0m |          -0.0090 |          21.1973 |           3.3151 |
[32m[20230205 18:24:14 @agent_ppo2.py:191][0m |          -0.0101 |          19.7877 |           3.3160 |
[32m[20230205 18:24:14 @agent_ppo2.py:191][0m |          -0.0175 |          19.0380 |           3.3182 |
[32m[20230205 18:24:14 @agent_ppo2.py:191][0m |          -0.0091 |          18.8777 |           3.3144 |
[32m[20230205 18:24:14 @agent_ppo2.py:191][0m |          -0.0149 |          17.5753 |           3.3146 |
[32m[20230205 18:24:14 @agent_ppo2.py:191][0m |          -0.0198 |          17.2681 |           3.3187 |
[32m[20230205 18:24:14 @agent_ppo2.py:191][0m |          -0.0138 |          16.3694 |           3.3188 |
[32m[20230205 18:24:14 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:24:15 @agent_ppo2.py:144][0m Average TRAINING episode reward: 263.28
[32m[20230205 18:24:15 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.78
[32m[20230205 18:24:15 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 278.49
[32m[20230205 18:24:15 @agent_ppo2.py:149][0m Total time:      10.79 min
[32m[20230205 18:24:15 @agent_ppo2.py:151][0m 718848 total steps have happened
[32m[20230205 18:24:15 @agent_ppo2.py:127][0m #------------------------ Iteration 351 --------------------------#
[32m[20230205 18:24:15 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:24:15 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:15 @agent_ppo2.py:191][0m |          -0.0013 |          43.3138 |           3.4985 |
[32m[20230205 18:24:15 @agent_ppo2.py:191][0m |          -0.0074 |          27.4157 |           3.4967 |
[32m[20230205 18:24:15 @agent_ppo2.py:191][0m |          -0.0094 |          22.9702 |           3.4949 |
[32m[20230205 18:24:15 @agent_ppo2.py:191][0m |          -0.0088 |          19.9631 |           3.4978 |
[32m[20230205 18:24:15 @agent_ppo2.py:191][0m |          -0.0122 |          18.0925 |           3.4967 |
[32m[20230205 18:24:15 @agent_ppo2.py:191][0m |          -0.0097 |          16.3080 |           3.4967 |
[32m[20230205 18:24:16 @agent_ppo2.py:191][0m |          -0.0122 |          15.2907 |           3.4909 |
[32m[20230205 18:24:16 @agent_ppo2.py:191][0m |          -0.0156 |          14.5349 |           3.4929 |
[32m[20230205 18:24:16 @agent_ppo2.py:191][0m |          -0.0141 |          14.2521 |           3.4910 |
[32m[20230205 18:24:16 @agent_ppo2.py:191][0m |          -0.0163 |          13.9678 |           3.4911 |
[32m[20230205 18:24:16 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:24:16 @agent_ppo2.py:144][0m Average TRAINING episode reward: 135.30
[32m[20230205 18:24:16 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.56
[32m[20230205 18:24:16 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 279.97
[32m[20230205 18:24:16 @agent_ppo2.py:149][0m Total time:      10.81 min
[32m[20230205 18:24:16 @agent_ppo2.py:151][0m 720896 total steps have happened
[32m[20230205 18:24:16 @agent_ppo2.py:127][0m #------------------------ Iteration 352 --------------------------#
[32m[20230205 18:24:17 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:24:17 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:17 @agent_ppo2.py:191][0m |           0.0005 |          13.1830 |           3.4244 |
[32m[20230205 18:24:17 @agent_ppo2.py:191][0m |           0.0011 |          12.2586 |           3.4225 |
[32m[20230205 18:24:17 @agent_ppo2.py:191][0m |          -0.0115 |          11.2130 |           3.4164 |
[32m[20230205 18:24:17 @agent_ppo2.py:191][0m |          -0.0139 |          10.8396 |           3.4134 |
[32m[20230205 18:24:17 @agent_ppo2.py:191][0m |          -0.0116 |          10.5275 |           3.4157 |
[32m[20230205 18:24:17 @agent_ppo2.py:191][0m |          -0.0091 |          10.4372 |           3.4126 |
[32m[20230205 18:24:17 @agent_ppo2.py:191][0m |          -0.0089 |          10.1923 |           3.4128 |
[32m[20230205 18:24:17 @agent_ppo2.py:191][0m |          -0.0153 |           9.9504 |           3.4110 |
[32m[20230205 18:24:17 @agent_ppo2.py:191][0m |          -0.0153 |           9.9197 |           3.4128 |
[32m[20230205 18:24:17 @agent_ppo2.py:191][0m |          -0.0135 |           9.6441 |           3.4103 |
[32m[20230205 18:24:17 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:24:18 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.94
[32m[20230205 18:24:18 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.00
[32m[20230205 18:24:18 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 274.02
[32m[20230205 18:24:18 @agent_ppo2.py:149][0m Total time:      10.84 min
[32m[20230205 18:24:18 @agent_ppo2.py:151][0m 722944 total steps have happened
[32m[20230205 18:24:18 @agent_ppo2.py:127][0m #------------------------ Iteration 353 --------------------------#
[32m[20230205 18:24:18 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:24:18 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:18 @agent_ppo2.py:191][0m |          -0.0030 |          11.9218 |           3.3917 |
[32m[20230205 18:24:18 @agent_ppo2.py:191][0m |          -0.0058 |          11.1380 |           3.3850 |
[32m[20230205 18:24:18 @agent_ppo2.py:191][0m |          -0.0127 |          10.3479 |           3.3813 |
[32m[20230205 18:24:18 @agent_ppo2.py:191][0m |          -0.0147 |           9.9352 |           3.3785 |
[32m[20230205 18:24:18 @agent_ppo2.py:191][0m |          -0.0137 |           9.7665 |           3.3778 |
[32m[20230205 18:24:19 @agent_ppo2.py:191][0m |          -0.0156 |           9.5200 |           3.3748 |
[32m[20230205 18:24:19 @agent_ppo2.py:191][0m |          -0.0048 |          10.0312 |           3.3758 |
[32m[20230205 18:24:19 @agent_ppo2.py:191][0m |          -0.0147 |           9.3214 |           3.3702 |
[32m[20230205 18:24:19 @agent_ppo2.py:191][0m |          -0.0175 |           9.0973 |           3.3677 |
[32m[20230205 18:24:19 @agent_ppo2.py:191][0m |          -0.0128 |           9.1491 |           3.3686 |
[32m[20230205 18:24:19 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:24:19 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.17
[32m[20230205 18:24:19 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.35
[32m[20230205 18:24:19 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 219.82
[32m[20230205 18:24:19 @agent_ppo2.py:149][0m Total time:      10.87 min
[32m[20230205 18:24:19 @agent_ppo2.py:151][0m 724992 total steps have happened
[32m[20230205 18:24:19 @agent_ppo2.py:127][0m #------------------------ Iteration 354 --------------------------#
[32m[20230205 18:24:20 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:24:20 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:20 @agent_ppo2.py:191][0m |           0.0137 |          43.2187 |           3.3706 |
[32m[20230205 18:24:20 @agent_ppo2.py:191][0m |          -0.0092 |          17.7774 |           3.3666 |
[32m[20230205 18:24:20 @agent_ppo2.py:191][0m |           0.0103 |          12.0116 |           3.3647 |
[32m[20230205 18:24:20 @agent_ppo2.py:191][0m |          -0.0092 |          11.2098 |           3.3663 |
[32m[20230205 18:24:20 @agent_ppo2.py:191][0m |          -0.0192 |           8.9943 |           3.3670 |
[32m[20230205 18:24:20 @agent_ppo2.py:191][0m |          -0.0128 |           8.4501 |           3.3649 |
[32m[20230205 18:24:20 @agent_ppo2.py:191][0m |          -0.0146 |           7.9345 |           3.3635 |
[32m[20230205 18:24:20 @agent_ppo2.py:191][0m |          -0.0179 |           7.3108 |           3.3652 |
[32m[20230205 18:24:20 @agent_ppo2.py:191][0m |          -0.0182 |           7.2209 |           3.3660 |
[32m[20230205 18:24:20 @agent_ppo2.py:191][0m |          -0.0095 |           6.8317 |           3.3648 |
[32m[20230205 18:24:20 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:24:21 @agent_ppo2.py:144][0m Average TRAINING episode reward: 158.60
[32m[20230205 18:24:21 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.19
[32m[20230205 18:24:21 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.77
[32m[20230205 18:24:21 @agent_ppo2.py:149][0m Total time:      10.89 min
[32m[20230205 18:24:21 @agent_ppo2.py:151][0m 727040 total steps have happened
[32m[20230205 18:24:21 @agent_ppo2.py:127][0m #------------------------ Iteration 355 --------------------------#
[32m[20230205 18:24:21 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:24:21 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:21 @agent_ppo2.py:191][0m |           0.0004 |          15.2867 |           3.5492 |
[32m[20230205 18:24:21 @agent_ppo2.py:191][0m |          -0.0042 |          13.0943 |           3.5442 |
[32m[20230205 18:24:22 @agent_ppo2.py:191][0m |          -0.0060 |          12.7657 |           3.5457 |
[32m[20230205 18:24:22 @agent_ppo2.py:191][0m |          -0.0081 |          12.5602 |           3.5412 |
[32m[20230205 18:24:22 @agent_ppo2.py:191][0m |          -0.0086 |          12.3678 |           3.5374 |
[32m[20230205 18:24:22 @agent_ppo2.py:191][0m |          -0.0118 |          12.1847 |           3.5358 |
[32m[20230205 18:24:22 @agent_ppo2.py:191][0m |          -0.0113 |          12.0763 |           3.5358 |
[32m[20230205 18:24:22 @agent_ppo2.py:191][0m |          -0.0119 |          11.9875 |           3.5305 |
[32m[20230205 18:24:22 @agent_ppo2.py:191][0m |          -0.0123 |          11.8620 |           3.5317 |
[32m[20230205 18:24:22 @agent_ppo2.py:191][0m |          -0.0125 |          11.7852 |           3.5317 |
[32m[20230205 18:24:22 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:24:22 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.68
[32m[20230205 18:24:22 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.02
[32m[20230205 18:24:22 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 104.81
[32m[20230205 18:24:22 @agent_ppo2.py:149][0m Total time:      10.92 min
[32m[20230205 18:24:22 @agent_ppo2.py:151][0m 729088 total steps have happened
[32m[20230205 18:24:22 @agent_ppo2.py:127][0m #------------------------ Iteration 356 --------------------------#
[32m[20230205 18:24:23 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:24:23 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:23 @agent_ppo2.py:191][0m |           0.0003 |          51.8830 |           3.3774 |
[32m[20230205 18:24:23 @agent_ppo2.py:191][0m |          -0.0065 |          26.4099 |           3.3653 |
[32m[20230205 18:24:23 @agent_ppo2.py:191][0m |          -0.0099 |          22.0961 |           3.3653 |
[32m[20230205 18:24:23 @agent_ppo2.py:191][0m |          -0.0124 |          20.1416 |           3.3612 |
[32m[20230205 18:24:23 @agent_ppo2.py:191][0m |          -0.0115 |          19.1558 |           3.3646 |
[32m[20230205 18:24:23 @agent_ppo2.py:191][0m |          -0.0123 |          18.5526 |           3.3601 |
[32m[20230205 18:24:23 @agent_ppo2.py:191][0m |          -0.0131 |          17.5217 |           3.3600 |
[32m[20230205 18:24:23 @agent_ppo2.py:191][0m |          -0.0137 |          16.9915 |           3.3607 |
[32m[20230205 18:24:23 @agent_ppo2.py:191][0m |          -0.0164 |          16.3717 |           3.3605 |
[32m[20230205 18:24:23 @agent_ppo2.py:191][0m |          -0.0158 |          15.8451 |           3.3619 |
[32m[20230205 18:24:23 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:24:24 @agent_ppo2.py:144][0m Average TRAINING episode reward: 119.44
[32m[20230205 18:24:24 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.66
[32m[20230205 18:24:24 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.49
[32m[20230205 18:24:24 @agent_ppo2.py:149][0m Total time:      10.94 min
[32m[20230205 18:24:24 @agent_ppo2.py:151][0m 731136 total steps have happened
[32m[20230205 18:24:24 @agent_ppo2.py:127][0m #------------------------ Iteration 357 --------------------------#
[32m[20230205 18:24:25 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:24:25 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:25 @agent_ppo2.py:191][0m |           0.0014 |          43.2507 |           3.4028 |
[32m[20230205 18:24:25 @agent_ppo2.py:191][0m |          -0.0055 |          35.2992 |           3.3981 |
[32m[20230205 18:24:25 @agent_ppo2.py:191][0m |          -0.0076 |          27.3129 |           3.3954 |
[32m[20230205 18:24:25 @agent_ppo2.py:191][0m |          -0.0074 |          24.9965 |           3.3925 |
[32m[20230205 18:24:25 @agent_ppo2.py:191][0m |          -0.0123 |          22.6928 |           3.3890 |
[32m[20230205 18:24:25 @agent_ppo2.py:191][0m |          -0.0119 |          21.6678 |           3.3887 |
[32m[20230205 18:24:25 @agent_ppo2.py:191][0m |          -0.0147 |          20.6870 |           3.3853 |
[32m[20230205 18:24:25 @agent_ppo2.py:191][0m |          -0.0166 |          19.9839 |           3.3839 |
[32m[20230205 18:24:25 @agent_ppo2.py:191][0m |          -0.0140 |          19.6263 |           3.3846 |
[32m[20230205 18:24:25 @agent_ppo2.py:191][0m |          -0.0122 |          19.4273 |           3.3824 |
[32m[20230205 18:24:25 @agent_ppo2.py:136][0m Policy update time: 0.60 s
[32m[20230205 18:24:26 @agent_ppo2.py:144][0m Average TRAINING episode reward: 147.12
[32m[20230205 18:24:26 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.30
[32m[20230205 18:24:26 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 217.29
[32m[20230205 18:24:26 @agent_ppo2.py:149][0m Total time:      10.97 min
[32m[20230205 18:24:26 @agent_ppo2.py:151][0m 733184 total steps have happened
[32m[20230205 18:24:26 @agent_ppo2.py:127][0m #------------------------ Iteration 358 --------------------------#
[32m[20230205 18:24:26 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:24:26 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:26 @agent_ppo2.py:191][0m |          -0.0029 |          62.0209 |           3.3552 |
[32m[20230205 18:24:26 @agent_ppo2.py:191][0m |          -0.0083 |          39.0941 |           3.3506 |
[32m[20230205 18:24:26 @agent_ppo2.py:191][0m |          -0.0134 |          24.6468 |           3.3503 |
[32m[20230205 18:24:26 @agent_ppo2.py:191][0m |          -0.0135 |          19.9163 |           3.3488 |
[32m[20230205 18:24:27 @agent_ppo2.py:191][0m |          -0.0148 |          17.4058 |           3.3476 |
[32m[20230205 18:24:27 @agent_ppo2.py:191][0m |          -0.0137 |          16.1450 |           3.3461 |
[32m[20230205 18:24:27 @agent_ppo2.py:191][0m |          -0.0155 |          15.0541 |           3.3483 |
[32m[20230205 18:24:27 @agent_ppo2.py:191][0m |          -0.0188 |          14.3972 |           3.3473 |
[32m[20230205 18:24:27 @agent_ppo2.py:191][0m |          -0.0168 |          13.8056 |           3.3461 |
[32m[20230205 18:24:27 @agent_ppo2.py:191][0m |          -0.0204 |          13.6421 |           3.3467 |
[32m[20230205 18:24:27 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:24:27 @agent_ppo2.py:144][0m Average TRAINING episode reward: 250.43
[32m[20230205 18:24:27 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.99
[32m[20230205 18:24:27 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.97
[32m[20230205 18:24:27 @agent_ppo2.py:149][0m Total time:      11.00 min
[32m[20230205 18:24:27 @agent_ppo2.py:151][0m 735232 total steps have happened
[32m[20230205 18:24:27 @agent_ppo2.py:127][0m #------------------------ Iteration 359 --------------------------#
[32m[20230205 18:24:28 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:24:28 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:28 @agent_ppo2.py:191][0m |          -0.0012 |          61.5706 |           3.4453 |
[32m[20230205 18:24:28 @agent_ppo2.py:191][0m |          -0.0100 |          41.9227 |           3.4433 |
[32m[20230205 18:24:28 @agent_ppo2.py:191][0m |          -0.0119 |          35.5397 |           3.4437 |
[32m[20230205 18:24:28 @agent_ppo2.py:191][0m |          -0.0135 |          32.3711 |           3.4415 |
[32m[20230205 18:24:28 @agent_ppo2.py:191][0m |          -0.0153 |          30.8658 |           3.4410 |
[32m[20230205 18:24:28 @agent_ppo2.py:191][0m |          -0.0165 |          28.9718 |           3.4408 |
[32m[20230205 18:24:28 @agent_ppo2.py:191][0m |          -0.0174 |          26.6212 |           3.4416 |
[32m[20230205 18:24:28 @agent_ppo2.py:191][0m |          -0.0182 |          25.7466 |           3.4407 |
[32m[20230205 18:24:28 @agent_ppo2.py:191][0m |          -0.0190 |          24.7788 |           3.4393 |
[32m[20230205 18:24:28 @agent_ppo2.py:191][0m |          -0.0196 |          23.6081 |           3.4398 |
[32m[20230205 18:24:28 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:24:29 @agent_ppo2.py:144][0m Average TRAINING episode reward: 137.01
[32m[20230205 18:24:29 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.76
[32m[20230205 18:24:29 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 139.19
[32m[20230205 18:24:29 @agent_ppo2.py:149][0m Total time:      11.02 min
[32m[20230205 18:24:29 @agent_ppo2.py:151][0m 737280 total steps have happened
[32m[20230205 18:24:29 @agent_ppo2.py:127][0m #------------------------ Iteration 360 --------------------------#
[32m[20230205 18:24:29 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:24:29 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:29 @agent_ppo2.py:191][0m |           0.0112 |          19.2318 |           3.3827 |
[32m[20230205 18:24:30 @agent_ppo2.py:191][0m |          -0.0089 |          14.7540 |           3.3791 |
[32m[20230205 18:24:30 @agent_ppo2.py:191][0m |          -0.0049 |          14.4003 |           3.3762 |
[32m[20230205 18:24:30 @agent_ppo2.py:191][0m |          -0.0127 |          13.6328 |           3.3759 |
[32m[20230205 18:24:30 @agent_ppo2.py:191][0m |          -0.0080 |          13.4129 |           3.3739 |
[32m[20230205 18:24:30 @agent_ppo2.py:191][0m |          -0.0221 |          13.2365 |           3.3736 |
[32m[20230205 18:24:30 @agent_ppo2.py:191][0m |          -0.0141 |          12.8255 |           3.3745 |
[32m[20230205 18:24:30 @agent_ppo2.py:191][0m |          -0.0070 |          13.3133 |           3.3747 |
[32m[20230205 18:24:30 @agent_ppo2.py:191][0m |          -0.0175 |          12.6189 |           3.3739 |
[32m[20230205 18:24:30 @agent_ppo2.py:191][0m |          -0.0139 |          12.4059 |           3.3759 |
[32m[20230205 18:24:30 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:24:30 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.93
[32m[20230205 18:24:30 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.05
[32m[20230205 18:24:30 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.39
[32m[20230205 18:24:30 @agent_ppo2.py:149][0m Total time:      11.05 min
[32m[20230205 18:24:30 @agent_ppo2.py:151][0m 739328 total steps have happened
[32m[20230205 18:24:30 @agent_ppo2.py:127][0m #------------------------ Iteration 361 --------------------------#
[32m[20230205 18:24:31 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:24:31 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:31 @agent_ppo2.py:191][0m |           0.0006 |          13.0663 |           3.3723 |
[32m[20230205 18:24:31 @agent_ppo2.py:191][0m |          -0.0072 |          12.4511 |           3.3695 |
[32m[20230205 18:24:31 @agent_ppo2.py:191][0m |          -0.0108 |          12.1043 |           3.3672 |
[32m[20230205 18:24:31 @agent_ppo2.py:191][0m |          -0.0142 |          11.8123 |           3.3647 |
[32m[20230205 18:24:31 @agent_ppo2.py:191][0m |          -0.0150 |          11.5518 |           3.3641 |
[32m[20230205 18:24:31 @agent_ppo2.py:191][0m |          -0.0173 |          11.2195 |           3.3598 |
[32m[20230205 18:24:31 @agent_ppo2.py:191][0m |          -0.0177 |          10.9710 |           3.3612 |
[32m[20230205 18:24:31 @agent_ppo2.py:191][0m |          -0.0180 |          10.7430 |           3.3616 |
[32m[20230205 18:24:31 @agent_ppo2.py:191][0m |          -0.0187 |          10.4721 |           3.3602 |
[32m[20230205 18:24:31 @agent_ppo2.py:191][0m |          -0.0191 |          10.2932 |           3.3609 |
[32m[20230205 18:24:31 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:24:32 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.14
[32m[20230205 18:24:32 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.65
[32m[20230205 18:24:32 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.65
[32m[20230205 18:24:32 @agent_ppo2.py:149][0m Total time:      11.08 min
[32m[20230205 18:24:32 @agent_ppo2.py:151][0m 741376 total steps have happened
[32m[20230205 18:24:32 @agent_ppo2.py:127][0m #------------------------ Iteration 362 --------------------------#
[32m[20230205 18:24:32 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:24:32 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:33 @agent_ppo2.py:191][0m |          -0.0005 |          14.3866 |           3.4883 |
[32m[20230205 18:24:33 @agent_ppo2.py:191][0m |          -0.0070 |          13.4791 |           3.4814 |
[32m[20230205 18:24:33 @agent_ppo2.py:191][0m |          -0.0093 |          13.1045 |           3.4714 |
[32m[20230205 18:24:33 @agent_ppo2.py:191][0m |          -0.0104 |          12.8134 |           3.4728 |
[32m[20230205 18:24:33 @agent_ppo2.py:191][0m |          -0.0121 |          12.6294 |           3.4739 |
[32m[20230205 18:24:33 @agent_ppo2.py:191][0m |          -0.0127 |          12.4812 |           3.4741 |
[32m[20230205 18:24:33 @agent_ppo2.py:191][0m |          -0.0128 |          12.3567 |           3.4732 |
[32m[20230205 18:24:33 @agent_ppo2.py:191][0m |          -0.0133 |          12.2749 |           3.4706 |
[32m[20230205 18:24:33 @agent_ppo2.py:191][0m |          -0.0141 |          12.1596 |           3.4722 |
[32m[20230205 18:24:33 @agent_ppo2.py:191][0m |          -0.0137 |          12.1065 |           3.4666 |
[32m[20230205 18:24:33 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:24:33 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.68
[32m[20230205 18:24:33 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.34
[32m[20230205 18:24:33 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 277.59
[32m[20230205 18:24:33 @agent_ppo2.py:149][0m Total time:      11.10 min
[32m[20230205 18:24:33 @agent_ppo2.py:151][0m 743424 total steps have happened
[32m[20230205 18:24:33 @agent_ppo2.py:127][0m #------------------------ Iteration 363 --------------------------#
[32m[20230205 18:24:34 @agent_ppo2.py:133][0m Sampling time: 0.66 s by 1 slaves
[32m[20230205 18:24:34 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:34 @agent_ppo2.py:191][0m |           0.0022 |          33.7658 |           3.4069 |
[32m[20230205 18:24:34 @agent_ppo2.py:191][0m |          -0.0010 |          24.0848 |           3.4051 |
[32m[20230205 18:24:34 @agent_ppo2.py:191][0m |          -0.0099 |          18.7728 |           3.4032 |
[32m[20230205 18:24:34 @agent_ppo2.py:191][0m |          -0.0068 |          16.6580 |           3.4000 |
[32m[20230205 18:24:34 @agent_ppo2.py:191][0m |          -0.0085 |          14.5751 |           3.3938 |
[32m[20230205 18:24:35 @agent_ppo2.py:191][0m |          -0.0140 |          13.8581 |           3.3933 |
[32m[20230205 18:24:35 @agent_ppo2.py:191][0m |          -0.0159 |          12.9290 |           3.3909 |
[32m[20230205 18:24:35 @agent_ppo2.py:191][0m |          -0.0143 |          12.5068 |           3.3894 |
[32m[20230205 18:24:35 @agent_ppo2.py:191][0m |          -0.0160 |          12.2937 |           3.3902 |
[32m[20230205 18:24:35 @agent_ppo2.py:191][0m |          -0.0174 |          11.9789 |           3.3877 |
[32m[20230205 18:24:35 @agent_ppo2.py:136][0m Policy update time: 0.67 s
[32m[20230205 18:24:35 @agent_ppo2.py:144][0m Average TRAINING episode reward: 171.52
[32m[20230205 18:24:35 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.16
[32m[20230205 18:24:35 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 217.73
[32m[20230205 18:24:35 @agent_ppo2.py:149][0m Total time:      11.13 min
[32m[20230205 18:24:35 @agent_ppo2.py:151][0m 745472 total steps have happened
[32m[20230205 18:24:35 @agent_ppo2.py:127][0m #------------------------ Iteration 364 --------------------------#
[32m[20230205 18:24:36 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:24:36 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:36 @agent_ppo2.py:191][0m |           0.0039 |          11.8450 |           3.3708 |
[32m[20230205 18:24:36 @agent_ppo2.py:191][0m |          -0.0096 |          10.8493 |           3.3633 |
[32m[20230205 18:24:36 @agent_ppo2.py:191][0m |          -0.0085 |          10.6319 |           3.3593 |
[32m[20230205 18:24:36 @agent_ppo2.py:191][0m |          -0.0127 |          10.1573 |           3.3541 |
[32m[20230205 18:24:36 @agent_ppo2.py:191][0m |          -0.0125 |          10.0038 |           3.3506 |
[32m[20230205 18:24:36 @agent_ppo2.py:191][0m |          -0.0075 |          10.0176 |           3.3500 |
[32m[20230205 18:24:36 @agent_ppo2.py:191][0m |          -0.0175 |           9.4100 |           3.3472 |
[32m[20230205 18:24:36 @agent_ppo2.py:191][0m |          -0.0155 |           9.2172 |           3.3423 |
[32m[20230205 18:24:36 @agent_ppo2.py:191][0m |          -0.0080 |           9.6078 |           3.3426 |
[32m[20230205 18:24:36 @agent_ppo2.py:191][0m |          -0.0134 |           8.9514 |           3.3388 |
[32m[20230205 18:24:36 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:24:37 @agent_ppo2.py:144][0m Average TRAINING episode reward: 255.60
[32m[20230205 18:24:37 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.32
[32m[20230205 18:24:37 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 279.15
[32m[20230205 18:24:37 @agent_ppo2.py:149][0m Total time:      11.16 min
[32m[20230205 18:24:37 @agent_ppo2.py:151][0m 747520 total steps have happened
[32m[20230205 18:24:37 @agent_ppo2.py:127][0m #------------------------ Iteration 365 --------------------------#
[32m[20230205 18:24:37 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:24:37 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:37 @agent_ppo2.py:191][0m |           0.0098 |          14.2304 |           3.3906 |
[32m[20230205 18:24:37 @agent_ppo2.py:191][0m |          -0.0019 |          13.1447 |           3.3839 |
[32m[20230205 18:24:38 @agent_ppo2.py:191][0m |          -0.0101 |          12.3509 |           3.3854 |
[32m[20230205 18:24:38 @agent_ppo2.py:191][0m |          -0.0058 |          12.7960 |           3.3890 |
[32m[20230205 18:24:38 @agent_ppo2.py:191][0m |          -0.0123 |          12.0420 |           3.3893 |
[32m[20230205 18:24:38 @agent_ppo2.py:191][0m |          -0.0171 |          11.9225 |           3.3865 |
[32m[20230205 18:24:38 @agent_ppo2.py:191][0m |          -0.0068 |          12.3771 |           3.3891 |
[32m[20230205 18:24:38 @agent_ppo2.py:191][0m |          -0.0152 |          11.7355 |           3.3894 |
[32m[20230205 18:24:38 @agent_ppo2.py:191][0m |          -0.0179 |          11.6339 |           3.3886 |
[32m[20230205 18:24:38 @agent_ppo2.py:191][0m |          -0.0183 |          11.5432 |           3.3853 |
[32m[20230205 18:24:38 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:24:38 @agent_ppo2.py:144][0m Average TRAINING episode reward: 256.96
[32m[20230205 18:24:38 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.18
[32m[20230205 18:24:38 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 277.12
[32m[20230205 18:24:38 @agent_ppo2.py:149][0m Total time:      11.18 min
[32m[20230205 18:24:38 @agent_ppo2.py:151][0m 749568 total steps have happened
[32m[20230205 18:24:38 @agent_ppo2.py:127][0m #------------------------ Iteration 366 --------------------------#
[32m[20230205 18:24:39 @agent_ppo2.py:133][0m Sampling time: 0.46 s by 1 slaves
[32m[20230205 18:24:39 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:39 @agent_ppo2.py:191][0m |          -0.0028 |          25.0680 |           3.3590 |
[32m[20230205 18:24:39 @agent_ppo2.py:191][0m |          -0.0048 |          14.0489 |           3.3606 |
[32m[20230205 18:24:39 @agent_ppo2.py:191][0m |          -0.0066 |          12.6692 |           3.3631 |
[32m[20230205 18:24:39 @agent_ppo2.py:191][0m |          -0.0108 |          11.9815 |           3.3627 |
[32m[20230205 18:24:39 @agent_ppo2.py:191][0m |          -0.0107 |          11.4545 |           3.3607 |
[32m[20230205 18:24:39 @agent_ppo2.py:191][0m |           0.0069 |          11.3903 |           3.3606 |
[32m[20230205 18:24:39 @agent_ppo2.py:191][0m |          -0.0114 |          10.4415 |           3.3586 |
[32m[20230205 18:24:39 @agent_ppo2.py:191][0m |          -0.0184 |          10.0438 |           3.3573 |
[32m[20230205 18:24:39 @agent_ppo2.py:191][0m |          -0.0201 |           9.7358 |           3.3597 |
[32m[20230205 18:24:39 @agent_ppo2.py:191][0m |          -0.0133 |           9.9070 |           3.3606 |
[32m[20230205 18:24:39 @agent_ppo2.py:136][0m Policy update time: 0.46 s
[32m[20230205 18:24:40 @agent_ppo2.py:144][0m Average TRAINING episode reward: 172.78
[32m[20230205 18:24:40 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.14
[32m[20230205 18:24:40 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.96
[32m[20230205 18:24:40 @agent_ppo2.py:149][0m Total time:      11.21 min
[32m[20230205 18:24:40 @agent_ppo2.py:151][0m 751616 total steps have happened
[32m[20230205 18:24:40 @agent_ppo2.py:127][0m #------------------------ Iteration 367 --------------------------#
[32m[20230205 18:24:40 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:24:40 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:40 @agent_ppo2.py:191][0m |           0.0013 |          13.7218 |           3.3666 |
[32m[20230205 18:24:40 @agent_ppo2.py:191][0m |          -0.0064 |          12.3381 |           3.3594 |
[32m[20230205 18:24:40 @agent_ppo2.py:191][0m |          -0.0090 |          11.9237 |           3.3551 |
[32m[20230205 18:24:40 @agent_ppo2.py:191][0m |          -0.0109 |          12.0861 |           3.3540 |
[32m[20230205 18:24:41 @agent_ppo2.py:191][0m |          -0.0138 |          11.5139 |           3.3523 |
[32m[20230205 18:24:41 @agent_ppo2.py:191][0m |          -0.0139 |          11.3936 |           3.3532 |
[32m[20230205 18:24:41 @agent_ppo2.py:191][0m |          -0.0151 |          11.2864 |           3.3525 |
[32m[20230205 18:24:41 @agent_ppo2.py:191][0m |          -0.0159 |          11.1716 |           3.3493 |
[32m[20230205 18:24:41 @agent_ppo2.py:191][0m |          -0.0126 |          11.1327 |           3.3499 |
[32m[20230205 18:24:41 @agent_ppo2.py:191][0m |          -0.0157 |          11.0797 |           3.3489 |
[32m[20230205 18:24:41 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:24:41 @agent_ppo2.py:144][0m Average TRAINING episode reward: 256.96
[32m[20230205 18:24:41 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.26
[32m[20230205 18:24:41 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.59
[32m[20230205 18:24:41 @agent_ppo2.py:149][0m Total time:      11.23 min
[32m[20230205 18:24:41 @agent_ppo2.py:151][0m 753664 total steps have happened
[32m[20230205 18:24:41 @agent_ppo2.py:127][0m #------------------------ Iteration 368 --------------------------#
[32m[20230205 18:24:42 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:24:42 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:42 @agent_ppo2.py:191][0m |           0.0006 |          13.2455 |           3.3151 |
[32m[20230205 18:24:42 @agent_ppo2.py:191][0m |          -0.0032 |          12.5074 |           3.3141 |
[32m[20230205 18:24:42 @agent_ppo2.py:191][0m |          -0.0072 |          12.2384 |           3.3115 |
[32m[20230205 18:24:42 @agent_ppo2.py:191][0m |          -0.0071 |          11.8513 |           3.3091 |
[32m[20230205 18:24:42 @agent_ppo2.py:191][0m |          -0.0118 |          11.6817 |           3.3092 |
[32m[20230205 18:24:42 @agent_ppo2.py:191][0m |          -0.0109 |          11.5113 |           3.3073 |
[32m[20230205 18:24:42 @agent_ppo2.py:191][0m |          -0.0115 |          11.4886 |           3.3088 |
[32m[20230205 18:24:42 @agent_ppo2.py:191][0m |          -0.0100 |          11.3362 |           3.3084 |
[32m[20230205 18:24:42 @agent_ppo2.py:191][0m |          -0.0144 |          11.1292 |           3.3077 |
[32m[20230205 18:24:42 @agent_ppo2.py:191][0m |          -0.0153 |          10.9884 |           3.3068 |
[32m[20230205 18:24:42 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:24:43 @agent_ppo2.py:144][0m Average TRAINING episode reward: 256.99
[32m[20230205 18:24:43 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.47
[32m[20230205 18:24:43 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.00
[32m[20230205 18:24:43 @agent_ppo2.py:149][0m Total time:      11.26 min
[32m[20230205 18:24:43 @agent_ppo2.py:151][0m 755712 total steps have happened
[32m[20230205 18:24:43 @agent_ppo2.py:127][0m #------------------------ Iteration 369 --------------------------#
[32m[20230205 18:24:43 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:24:43 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:43 @agent_ppo2.py:191][0m |           0.0005 |          11.6806 |           3.3438 |
[32m[20230205 18:24:43 @agent_ppo2.py:191][0m |          -0.0062 |          11.3775 |           3.3382 |
[32m[20230205 18:24:44 @agent_ppo2.py:191][0m |          -0.0079 |          11.1667 |           3.3384 |
[32m[20230205 18:24:44 @agent_ppo2.py:191][0m |          -0.0095 |          11.0256 |           3.3411 |
[32m[20230205 18:24:44 @agent_ppo2.py:191][0m |          -0.0112 |          10.8426 |           3.3391 |
[32m[20230205 18:24:44 @agent_ppo2.py:191][0m |          -0.0114 |          10.7241 |           3.3365 |
[32m[20230205 18:24:44 @agent_ppo2.py:191][0m |          -0.0123 |          10.5686 |           3.3372 |
[32m[20230205 18:24:44 @agent_ppo2.py:191][0m |          -0.0130 |          10.4533 |           3.3358 |
[32m[20230205 18:24:44 @agent_ppo2.py:191][0m |          -0.0137 |          10.3251 |           3.3369 |
[32m[20230205 18:24:44 @agent_ppo2.py:191][0m |          -0.0140 |          10.2516 |           3.3341 |
[32m[20230205 18:24:44 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:24:44 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.76
[32m[20230205 18:24:44 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.84
[32m[20230205 18:24:44 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 279.13
[32m[20230205 18:24:44 @agent_ppo2.py:149][0m Total time:      11.28 min
[32m[20230205 18:24:44 @agent_ppo2.py:151][0m 757760 total steps have happened
[32m[20230205 18:24:44 @agent_ppo2.py:127][0m #------------------------ Iteration 370 --------------------------#
[32m[20230205 18:24:45 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:24:45 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:45 @agent_ppo2.py:191][0m |          -0.0001 |          11.7155 |           3.3241 |
[32m[20230205 18:24:45 @agent_ppo2.py:191][0m |          -0.0053 |          10.5728 |           3.3184 |
[32m[20230205 18:24:45 @agent_ppo2.py:191][0m |          -0.0077 |           9.8803 |           3.3145 |
[32m[20230205 18:24:45 @agent_ppo2.py:191][0m |          -0.0118 |           9.4782 |           3.3118 |
[32m[20230205 18:24:45 @agent_ppo2.py:191][0m |          -0.0142 |           9.2872 |           3.3116 |
[32m[20230205 18:24:45 @agent_ppo2.py:191][0m |          -0.0114 |           9.0729 |           3.3102 |
[32m[20230205 18:24:45 @agent_ppo2.py:191][0m |          -0.0036 |           8.9994 |           3.3116 |
[32m[20230205 18:24:45 @agent_ppo2.py:191][0m |          -0.0170 |           8.7652 |           3.3063 |
[32m[20230205 18:24:45 @agent_ppo2.py:191][0m |          -0.0168 |           8.6084 |           3.3085 |
[32m[20230205 18:24:45 @agent_ppo2.py:191][0m |          -0.0205 |           8.3945 |           3.3068 |
[32m[20230205 18:24:45 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:24:46 @agent_ppo2.py:144][0m Average TRAINING episode reward: 252.30
[32m[20230205 18:24:46 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.26
[32m[20230205 18:24:46 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.67
[32m[20230205 18:24:46 @agent_ppo2.py:149][0m Total time:      11.31 min
[32m[20230205 18:24:46 @agent_ppo2.py:151][0m 759808 total steps have happened
[32m[20230205 18:24:46 @agent_ppo2.py:127][0m #------------------------ Iteration 371 --------------------------#
[32m[20230205 18:24:46 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:24:46 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:47 @agent_ppo2.py:191][0m |          -0.0025 |          13.7681 |           3.3595 |
[32m[20230205 18:24:47 @agent_ppo2.py:191][0m |          -0.0078 |          13.4386 |           3.3516 |
[32m[20230205 18:24:47 @agent_ppo2.py:191][0m |          -0.0100 |          13.0018 |           3.3532 |
[32m[20230205 18:24:47 @agent_ppo2.py:191][0m |          -0.0136 |          12.6453 |           3.3541 |
[32m[20230205 18:24:47 @agent_ppo2.py:191][0m |          -0.0123 |          12.6381 |           3.3527 |
[32m[20230205 18:24:47 @agent_ppo2.py:191][0m |          -0.0129 |          12.4903 |           3.3503 |
[32m[20230205 18:24:47 @agent_ppo2.py:191][0m |          -0.0136 |          12.3596 |           3.3532 |
[32m[20230205 18:24:47 @agent_ppo2.py:191][0m |          -0.0131 |          12.3418 |           3.3499 |
[32m[20230205 18:24:47 @agent_ppo2.py:191][0m |          -0.0135 |          12.0300 |           3.3523 |
[32m[20230205 18:24:47 @agent_ppo2.py:191][0m |          -0.0164 |          11.9131 |           3.3501 |
[32m[20230205 18:24:47 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:24:47 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.87
[32m[20230205 18:24:47 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.02
[32m[20230205 18:24:47 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.27
[32m[20230205 18:24:47 @agent_ppo2.py:149][0m Total time:      11.33 min
[32m[20230205 18:24:47 @agent_ppo2.py:151][0m 761856 total steps have happened
[32m[20230205 18:24:47 @agent_ppo2.py:127][0m #------------------------ Iteration 372 --------------------------#
[32m[20230205 18:24:48 @agent_ppo2.py:133][0m Sampling time: 0.67 s by 1 slaves
[32m[20230205 18:24:48 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:48 @agent_ppo2.py:191][0m |           0.0003 |          18.5956 |           3.3422 |
[32m[20230205 18:24:48 @agent_ppo2.py:191][0m |          -0.0107 |          11.0290 |           3.3379 |
[32m[20230205 18:24:48 @agent_ppo2.py:191][0m |          -0.0133 |           9.9010 |           3.3319 |
[32m[20230205 18:24:48 @agent_ppo2.py:191][0m |          -0.0074 |           9.0596 |           3.3319 |
[32m[20230205 18:24:48 @agent_ppo2.py:191][0m |          -0.0149 |           8.5081 |           3.3281 |
[32m[20230205 18:24:49 @agent_ppo2.py:191][0m |          -0.0137 |           7.9917 |           3.3289 |
[32m[20230205 18:24:49 @agent_ppo2.py:191][0m |          -0.0089 |           8.5939 |           3.3266 |
[32m[20230205 18:24:49 @agent_ppo2.py:191][0m |          -0.0122 |           7.4477 |           3.3255 |
[32m[20230205 18:24:49 @agent_ppo2.py:191][0m |          -0.0115 |           7.2692 |           3.3224 |
[32m[20230205 18:24:49 @agent_ppo2.py:191][0m |          -0.0132 |           7.2811 |           3.3232 |
[32m[20230205 18:24:49 @agent_ppo2.py:136][0m Policy update time: 0.67 s
[32m[20230205 18:24:49 @agent_ppo2.py:144][0m Average TRAINING episode reward: 181.55
[32m[20230205 18:24:49 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.70
[32m[20230205 18:24:49 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.07
[32m[20230205 18:24:49 @agent_ppo2.py:149][0m Total time:      11.36 min
[32m[20230205 18:24:49 @agent_ppo2.py:151][0m 763904 total steps have happened
[32m[20230205 18:24:49 @agent_ppo2.py:127][0m #------------------------ Iteration 373 --------------------------#
[32m[20230205 18:24:50 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:24:50 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:50 @agent_ppo2.py:191][0m |          -0.0078 |          13.7473 |           3.2179 |
[32m[20230205 18:24:50 @agent_ppo2.py:191][0m |          -0.0126 |          12.8844 |           3.2125 |
[32m[20230205 18:24:50 @agent_ppo2.py:191][0m |           0.0026 |          13.7520 |           3.2082 |
[32m[20230205 18:24:50 @agent_ppo2.py:191][0m |          -0.0135 |          12.4584 |           3.2068 |
[32m[20230205 18:24:50 @agent_ppo2.py:191][0m |          -0.0140 |          12.3635 |           3.2075 |
[32m[20230205 18:24:50 @agent_ppo2.py:191][0m |          -0.0155 |          12.2451 |           3.2084 |
[32m[20230205 18:24:50 @agent_ppo2.py:191][0m |          -0.0125 |          12.2822 |           3.2093 |
[32m[20230205 18:24:50 @agent_ppo2.py:191][0m |          -0.0157 |          12.1055 |           3.2078 |
[32m[20230205 18:24:50 @agent_ppo2.py:191][0m |          -0.0133 |          12.8886 |           3.2074 |
[32m[20230205 18:24:50 @agent_ppo2.py:191][0m |          -0.0142 |          11.9914 |           3.2083 |
[32m[20230205 18:24:50 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:24:51 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.68
[32m[20230205 18:24:51 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.08
[32m[20230205 18:24:51 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 215.57
[32m[20230205 18:24:51 @agent_ppo2.py:149][0m Total time:      11.39 min
[32m[20230205 18:24:51 @agent_ppo2.py:151][0m 765952 total steps have happened
[32m[20230205 18:24:51 @agent_ppo2.py:127][0m #------------------------ Iteration 374 --------------------------#
[32m[20230205 18:24:51 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:24:52 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:52 @agent_ppo2.py:191][0m |           0.0005 |          38.3288 |           3.4202 |
[32m[20230205 18:24:52 @agent_ppo2.py:191][0m |          -0.0034 |          21.9735 |           3.4146 |
[32m[20230205 18:24:52 @agent_ppo2.py:191][0m |          -0.0061 |          17.1980 |           3.4127 |
[32m[20230205 18:24:52 @agent_ppo2.py:191][0m |          -0.0073 |          15.8773 |           3.4135 |
[32m[20230205 18:24:52 @agent_ppo2.py:191][0m |          -0.0083 |          14.9500 |           3.4090 |
[32m[20230205 18:24:52 @agent_ppo2.py:191][0m |          -0.0114 |          14.4317 |           3.4129 |
[32m[20230205 18:24:52 @agent_ppo2.py:191][0m |          -0.0127 |          13.7460 |           3.4102 |
[32m[20230205 18:24:52 @agent_ppo2.py:191][0m |          -0.0116 |          13.3200 |           3.4094 |
[32m[20230205 18:24:52 @agent_ppo2.py:191][0m |          -0.0162 |          13.2566 |           3.4114 |
[32m[20230205 18:24:52 @agent_ppo2.py:191][0m |          -0.0143 |          12.7656 |           3.4097 |
[32m[20230205 18:24:52 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:24:53 @agent_ppo2.py:144][0m Average TRAINING episode reward: 254.34
[32m[20230205 18:24:53 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.83
[32m[20230205 18:24:53 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.00
[32m[20230205 18:24:53 @agent_ppo2.py:149][0m Total time:      11.42 min
[32m[20230205 18:24:53 @agent_ppo2.py:151][0m 768000 total steps have happened
[32m[20230205 18:24:53 @agent_ppo2.py:127][0m #------------------------ Iteration 375 --------------------------#
[32m[20230205 18:24:53 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:24:53 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:53 @agent_ppo2.py:191][0m |          -0.0007 |          12.9010 |           3.3589 |
[32m[20230205 18:24:53 @agent_ppo2.py:191][0m |          -0.0024 |          12.7798 |           3.3539 |
[32m[20230205 18:24:53 @agent_ppo2.py:191][0m |          -0.0115 |          11.9288 |           3.3422 |
[32m[20230205 18:24:53 @agent_ppo2.py:191][0m |          -0.0118 |          11.6946 |           3.3431 |
[32m[20230205 18:24:53 @agent_ppo2.py:191][0m |          -0.0118 |          11.7241 |           3.3409 |
[32m[20230205 18:24:53 @agent_ppo2.py:191][0m |          -0.0033 |          12.1421 |           3.3433 |
[32m[20230205 18:24:54 @agent_ppo2.py:191][0m |          -0.0159 |          11.2936 |           3.3409 |
[32m[20230205 18:24:54 @agent_ppo2.py:191][0m |          -0.0125 |          11.1689 |           3.3397 |
[32m[20230205 18:24:54 @agent_ppo2.py:191][0m |          -0.0138 |          11.0814 |           3.3411 |
[32m[20230205 18:24:54 @agent_ppo2.py:191][0m |          -0.0118 |          11.4623 |           3.3393 |
[32m[20230205 18:24:54 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:24:54 @agent_ppo2.py:144][0m Average TRAINING episode reward: 251.16
[32m[20230205 18:24:54 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.04
[32m[20230205 18:24:54 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 269.72
[32m[20230205 18:24:54 @agent_ppo2.py:149][0m Total time:      11.45 min
[32m[20230205 18:24:54 @agent_ppo2.py:151][0m 770048 total steps have happened
[32m[20230205 18:24:54 @agent_ppo2.py:127][0m #------------------------ Iteration 376 --------------------------#
[32m[20230205 18:24:55 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:24:55 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:55 @agent_ppo2.py:191][0m |           0.0028 |          12.5410 |           3.2719 |
[32m[20230205 18:24:55 @agent_ppo2.py:191][0m |          -0.0071 |          11.9796 |           3.2674 |
[32m[20230205 18:24:55 @agent_ppo2.py:191][0m |          -0.0137 |          11.5094 |           3.2633 |
[32m[20230205 18:24:55 @agent_ppo2.py:191][0m |          -0.0146 |          11.1724 |           3.2601 |
[32m[20230205 18:24:55 @agent_ppo2.py:191][0m |          -0.0165 |          10.9693 |           3.2583 |
[32m[20230205 18:24:55 @agent_ppo2.py:191][0m |          -0.0149 |          10.8030 |           3.2566 |
[32m[20230205 18:24:55 @agent_ppo2.py:191][0m |          -0.0136 |          10.7114 |           3.2564 |
[32m[20230205 18:24:55 @agent_ppo2.py:191][0m |          -0.0153 |          10.6053 |           3.2538 |
[32m[20230205 18:24:55 @agent_ppo2.py:191][0m |          -0.0170 |          10.3151 |           3.2533 |
[32m[20230205 18:24:55 @agent_ppo2.py:191][0m |          -0.0127 |          10.3786 |           3.2520 |
[32m[20230205 18:24:55 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:24:56 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.51
[32m[20230205 18:24:56 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.54
[32m[20230205 18:24:56 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.98
[32m[20230205 18:24:56 @agent_ppo2.py:149][0m Total time:      11.47 min
[32m[20230205 18:24:56 @agent_ppo2.py:151][0m 772096 total steps have happened
[32m[20230205 18:24:56 @agent_ppo2.py:127][0m #------------------------ Iteration 377 --------------------------#
[32m[20230205 18:24:56 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:24:56 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:56 @agent_ppo2.py:191][0m |           0.0007 |          29.8597 |           3.3844 |
[32m[20230205 18:24:56 @agent_ppo2.py:191][0m |          -0.0070 |          18.1675 |           3.3806 |
[32m[20230205 18:24:56 @agent_ppo2.py:191][0m |          -0.0087 |          15.1815 |           3.3771 |
[32m[20230205 18:24:57 @agent_ppo2.py:191][0m |          -0.0113 |          13.9168 |           3.3802 |
[32m[20230205 18:24:57 @agent_ppo2.py:191][0m |          -0.0132 |          13.1040 |           3.3787 |
[32m[20230205 18:24:57 @agent_ppo2.py:191][0m |          -0.0141 |          12.5483 |           3.3751 |
[32m[20230205 18:24:57 @agent_ppo2.py:191][0m |          -0.0148 |          12.1860 |           3.3743 |
[32m[20230205 18:24:57 @agent_ppo2.py:191][0m |          -0.0157 |          11.9572 |           3.3742 |
[32m[20230205 18:24:57 @agent_ppo2.py:191][0m |          -0.0159 |          11.8853 |           3.3733 |
[32m[20230205 18:24:57 @agent_ppo2.py:191][0m |          -0.0170 |          11.4905 |           3.3698 |
[32m[20230205 18:24:57 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:24:57 @agent_ppo2.py:144][0m Average TRAINING episode reward: 131.29
[32m[20230205 18:24:57 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.14
[32m[20230205 18:24:57 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 279.88
[32m[20230205 18:24:57 @agent_ppo2.py:149][0m Total time:      11.50 min
[32m[20230205 18:24:57 @agent_ppo2.py:151][0m 774144 total steps have happened
[32m[20230205 18:24:57 @agent_ppo2.py:127][0m #------------------------ Iteration 378 --------------------------#
[32m[20230205 18:24:58 @agent_ppo2.py:133][0m Sampling time: 0.62 s by 1 slaves
[32m[20230205 18:24:58 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:24:58 @agent_ppo2.py:191][0m |          -0.0034 |           7.5559 |           3.3292 |
[32m[20230205 18:24:58 @agent_ppo2.py:191][0m |          -0.0081 |           6.6918 |           3.3268 |
[32m[20230205 18:24:58 @agent_ppo2.py:191][0m |          -0.0295 |           6.5214 |           3.3261 |
[32m[20230205 18:24:58 @agent_ppo2.py:191][0m |          -0.0158 |           6.3135 |           3.3267 |
[32m[20230205 18:24:58 @agent_ppo2.py:191][0m |          -0.0067 |           6.0442 |           3.3357 |
[32m[20230205 18:24:58 @agent_ppo2.py:191][0m |          -0.0143 |           5.8863 |           3.3325 |
[32m[20230205 18:24:58 @agent_ppo2.py:191][0m |          -0.0216 |           5.7893 |           3.3289 |
[32m[20230205 18:24:58 @agent_ppo2.py:191][0m |          -0.0177 |           5.7170 |           3.3350 |
[32m[20230205 18:24:59 @agent_ppo2.py:191][0m |          -0.0132 |           5.6180 |           3.3355 |
[32m[20230205 18:24:59 @agent_ppo2.py:191][0m |          -0.0285 |           5.5781 |           3.3353 |
[32m[20230205 18:24:59 @agent_ppo2.py:136][0m Policy update time: 0.62 s
[32m[20230205 18:24:59 @agent_ppo2.py:144][0m Average TRAINING episode reward: 224.79
[32m[20230205 18:24:59 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.09
[32m[20230205 18:24:59 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.02
[32m[20230205 18:24:59 @agent_ppo2.py:149][0m Total time:      11.53 min
[32m[20230205 18:24:59 @agent_ppo2.py:151][0m 776192 total steps have happened
[32m[20230205 18:24:59 @agent_ppo2.py:127][0m #------------------------ Iteration 379 --------------------------#
[32m[20230205 18:25:00 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:25:00 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:00 @agent_ppo2.py:191][0m |          -0.0021 |          13.0384 |           3.2208 |
[32m[20230205 18:25:00 @agent_ppo2.py:191][0m |          -0.0114 |          11.9960 |           3.2161 |
[32m[20230205 18:25:00 @agent_ppo2.py:191][0m |          -0.0090 |          11.6359 |           3.2154 |
[32m[20230205 18:25:00 @agent_ppo2.py:191][0m |          -0.0083 |          11.5166 |           3.2108 |
[32m[20230205 18:25:00 @agent_ppo2.py:191][0m |          -0.0118 |          11.3778 |           3.2104 |
[32m[20230205 18:25:00 @agent_ppo2.py:191][0m |          -0.0130 |          11.1980 |           3.2085 |
[32m[20230205 18:25:00 @agent_ppo2.py:191][0m |          -0.0135 |          11.1225 |           3.2069 |
[32m[20230205 18:25:00 @agent_ppo2.py:191][0m |          -0.0069 |          11.3464 |           3.2088 |
[32m[20230205 18:25:00 @agent_ppo2.py:191][0m |          -0.0179 |          10.9279 |           3.2058 |
[32m[20230205 18:25:00 @agent_ppo2.py:191][0m |          -0.0159 |          10.9386 |           3.2063 |
[32m[20230205 18:25:00 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:25:01 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.48
[32m[20230205 18:25:01 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.33
[32m[20230205 18:25:01 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.88
[32m[20230205 18:25:01 @agent_ppo2.py:149][0m Total time:      11.55 min
[32m[20230205 18:25:01 @agent_ppo2.py:151][0m 778240 total steps have happened
[32m[20230205 18:25:01 @agent_ppo2.py:127][0m #------------------------ Iteration 380 --------------------------#
[32m[20230205 18:25:01 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:25:01 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:01 @agent_ppo2.py:191][0m |           0.0031 |          11.6034 |           3.4063 |
[32m[20230205 18:25:01 @agent_ppo2.py:191][0m |          -0.0054 |          10.9764 |           3.4064 |
[32m[20230205 18:25:01 @agent_ppo2.py:191][0m |          -0.0063 |          11.1348 |           3.4030 |
[32m[20230205 18:25:01 @agent_ppo2.py:191][0m |          -0.0077 |          10.6583 |           3.4004 |
[32m[20230205 18:25:01 @agent_ppo2.py:191][0m |          -0.0112 |          10.3924 |           3.3990 |
[32m[20230205 18:25:01 @agent_ppo2.py:191][0m |          -0.0104 |          10.6460 |           3.4004 |
[32m[20230205 18:25:02 @agent_ppo2.py:191][0m |          -0.0147 |          10.1864 |           3.3985 |
[32m[20230205 18:25:02 @agent_ppo2.py:191][0m |          -0.0158 |          10.0296 |           3.3981 |
[32m[20230205 18:25:02 @agent_ppo2.py:191][0m |          -0.0142 |           9.9292 |           3.3981 |
[32m[20230205 18:25:02 @agent_ppo2.py:191][0m |          -0.0130 |          10.3208 |           3.3996 |
[32m[20230205 18:25:02 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:25:02 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.19
[32m[20230205 18:25:02 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.47
[32m[20230205 18:25:02 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.84
[32m[20230205 18:25:02 @agent_ppo2.py:149][0m Total time:      11.58 min
[32m[20230205 18:25:02 @agent_ppo2.py:151][0m 780288 total steps have happened
[32m[20230205 18:25:02 @agent_ppo2.py:127][0m #------------------------ Iteration 381 --------------------------#
[32m[20230205 18:25:03 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:25:03 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:03 @agent_ppo2.py:191][0m |          -0.0016 |          12.4948 |           3.3065 |
[32m[20230205 18:25:03 @agent_ppo2.py:191][0m |           0.0103 |          13.0087 |           3.2981 |
[32m[20230205 18:25:03 @agent_ppo2.py:191][0m |          -0.0022 |          11.4849 |           3.2965 |
[32m[20230205 18:25:03 @agent_ppo2.py:191][0m |          -0.0134 |          10.9653 |           3.2958 |
[32m[20230205 18:25:03 @agent_ppo2.py:191][0m |          -0.0080 |          10.7418 |           3.2939 |
[32m[20230205 18:25:03 @agent_ppo2.py:191][0m |          -0.0140 |          10.5538 |           3.2949 |
[32m[20230205 18:25:03 @agent_ppo2.py:191][0m |          -0.0046 |          11.0013 |           3.2937 |
[32m[20230205 18:25:03 @agent_ppo2.py:191][0m |          -0.0062 |          10.7576 |           3.2879 |
[32m[20230205 18:25:03 @agent_ppo2.py:191][0m |          -0.0162 |          10.0094 |           3.2920 |
[32m[20230205 18:25:03 @agent_ppo2.py:191][0m |          -0.0076 |          10.6822 |           3.2917 |
[32m[20230205 18:25:03 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:25:04 @agent_ppo2.py:144][0m Average TRAINING episode reward: 250.87
[32m[20230205 18:25:04 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.97
[32m[20230205 18:25:04 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 271.57
[32m[20230205 18:25:04 @agent_ppo2.py:149][0m Total time:      11.61 min
[32m[20230205 18:25:04 @agent_ppo2.py:151][0m 782336 total steps have happened
[32m[20230205 18:25:04 @agent_ppo2.py:127][0m #------------------------ Iteration 382 --------------------------#
[32m[20230205 18:25:04 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:25:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:04 @agent_ppo2.py:191][0m |           0.0043 |          12.2291 |           3.3262 |
[32m[20230205 18:25:04 @agent_ppo2.py:191][0m |          -0.0029 |          11.4932 |           3.3176 |
[32m[20230205 18:25:05 @agent_ppo2.py:191][0m |          -0.0054 |          11.1090 |           3.3102 |
[32m[20230205 18:25:05 @agent_ppo2.py:191][0m |          -0.0079 |          10.7551 |           3.3134 |
[32m[20230205 18:25:05 @agent_ppo2.py:191][0m |          -0.0094 |          10.5499 |           3.3085 |
[32m[20230205 18:25:05 @agent_ppo2.py:191][0m |          -0.0090 |          10.5756 |           3.3068 |
[32m[20230205 18:25:05 @agent_ppo2.py:191][0m |          -0.0109 |          10.2786 |           3.3068 |
[32m[20230205 18:25:05 @agent_ppo2.py:191][0m |          -0.0114 |          10.1554 |           3.3018 |
[32m[20230205 18:25:05 @agent_ppo2.py:191][0m |          -0.0129 |          10.0342 |           3.3036 |
[32m[20230205 18:25:05 @agent_ppo2.py:191][0m |          -0.0136 |           9.9449 |           3.3050 |
[32m[20230205 18:25:05 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:25:05 @agent_ppo2.py:144][0m Average TRAINING episode reward: 253.36
[32m[20230205 18:25:05 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.64
[32m[20230205 18:25:05 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 271.34
[32m[20230205 18:25:05 @agent_ppo2.py:149][0m Total time:      11.63 min
[32m[20230205 18:25:05 @agent_ppo2.py:151][0m 784384 total steps have happened
[32m[20230205 18:25:05 @agent_ppo2.py:127][0m #------------------------ Iteration 383 --------------------------#
[32m[20230205 18:25:06 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:25:06 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:06 @agent_ppo2.py:191][0m |           0.0080 |          14.2452 |           3.3320 |
[32m[20230205 18:25:06 @agent_ppo2.py:191][0m |          -0.0035 |          12.5198 |           3.3278 |
[32m[20230205 18:25:06 @agent_ppo2.py:191][0m |          -0.0083 |          12.2945 |           3.3256 |
[32m[20230205 18:25:06 @agent_ppo2.py:191][0m |          -0.0029 |          12.7910 |           3.3235 |
[32m[20230205 18:25:06 @agent_ppo2.py:191][0m |          -0.0105 |          12.2104 |           3.3209 |
[32m[20230205 18:25:06 @agent_ppo2.py:191][0m |          -0.0154 |          11.8748 |           3.3182 |
[32m[20230205 18:25:06 @agent_ppo2.py:191][0m |          -0.0136 |          11.7760 |           3.3185 |
[32m[20230205 18:25:06 @agent_ppo2.py:191][0m |          -0.0150 |          11.7406 |           3.3182 |
[32m[20230205 18:25:06 @agent_ppo2.py:191][0m |          -0.0170 |          11.6254 |           3.3160 |
[32m[20230205 18:25:06 @agent_ppo2.py:191][0m |          -0.0159 |          11.6123 |           3.3165 |
[32m[20230205 18:25:06 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:25:07 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.92
[32m[20230205 18:25:07 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.24
[32m[20230205 18:25:07 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 174.74
[32m[20230205 18:25:07 @agent_ppo2.py:149][0m Total time:      11.66 min
[32m[20230205 18:25:07 @agent_ppo2.py:151][0m 786432 total steps have happened
[32m[20230205 18:25:07 @agent_ppo2.py:127][0m #------------------------ Iteration 384 --------------------------#
[32m[20230205 18:25:08 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:25:08 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:08 @agent_ppo2.py:191][0m |          -0.0008 |          25.1351 |           3.4228 |
[32m[20230205 18:25:08 @agent_ppo2.py:191][0m |          -0.0101 |           9.9491 |           3.4065 |
[32m[20230205 18:25:08 @agent_ppo2.py:191][0m |          -0.0099 |           8.7615 |           3.4080 |
[32m[20230205 18:25:08 @agent_ppo2.py:191][0m |          -0.0174 |           8.4186 |           3.4005 |
[32m[20230205 18:25:08 @agent_ppo2.py:191][0m |          -0.0181 |           8.1591 |           3.3971 |
[32m[20230205 18:25:08 @agent_ppo2.py:191][0m |          -0.0125 |           8.6702 |           3.3977 |
[32m[20230205 18:25:08 @agent_ppo2.py:191][0m |          -0.0183 |           7.8938 |           3.3959 |
[32m[20230205 18:25:08 @agent_ppo2.py:191][0m |          -0.0170 |           7.7156 |           3.3923 |
[32m[20230205 18:25:08 @agent_ppo2.py:191][0m |          -0.0193 |           7.9551 |           3.3932 |
[32m[20230205 18:25:08 @agent_ppo2.py:191][0m |          -0.0137 |           7.7591 |           3.3899 |
[32m[20230205 18:25:08 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:25:08 @agent_ppo2.py:144][0m Average TRAINING episode reward: 201.90
[32m[20230205 18:25:08 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.71
[32m[20230205 18:25:08 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 277.77
[32m[20230205 18:25:08 @agent_ppo2.py:149][0m Total time:      11.69 min
[32m[20230205 18:25:08 @agent_ppo2.py:151][0m 788480 total steps have happened
[32m[20230205 18:25:08 @agent_ppo2.py:127][0m #------------------------ Iteration 385 --------------------------#
[32m[20230205 18:25:09 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:25:09 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:09 @agent_ppo2.py:191][0m |          -0.0026 |          12.6465 |           3.3298 |
[32m[20230205 18:25:09 @agent_ppo2.py:191][0m |          -0.0045 |          11.9931 |           3.3258 |
[32m[20230205 18:25:09 @agent_ppo2.py:191][0m |          -0.0088 |          11.5183 |           3.3251 |
[32m[20230205 18:25:09 @agent_ppo2.py:191][0m |          -0.0096 |          11.5003 |           3.3204 |
[32m[20230205 18:25:09 @agent_ppo2.py:191][0m |          -0.0081 |          11.5974 |           3.3180 |
[32m[20230205 18:25:09 @agent_ppo2.py:191][0m |          -0.0140 |          11.0958 |           3.3175 |
[32m[20230205 18:25:09 @agent_ppo2.py:191][0m |          -0.0161 |          11.0048 |           3.3140 |
[32m[20230205 18:25:10 @agent_ppo2.py:191][0m |          -0.0172 |          10.9053 |           3.3133 |
[32m[20230205 18:25:10 @agent_ppo2.py:191][0m |          -0.0170 |          10.8364 |           3.3130 |
[32m[20230205 18:25:10 @agent_ppo2.py:191][0m |          -0.0185 |          10.7890 |           3.3118 |
[32m[20230205 18:25:10 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:25:10 @agent_ppo2.py:144][0m Average TRAINING episode reward: 253.26
[32m[20230205 18:25:10 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.11
[32m[20230205 18:25:10 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 274.72
[32m[20230205 18:25:10 @agent_ppo2.py:149][0m Total time:      11.71 min
[32m[20230205 18:25:10 @agent_ppo2.py:151][0m 790528 total steps have happened
[32m[20230205 18:25:10 @agent_ppo2.py:127][0m #------------------------ Iteration 386 --------------------------#
[32m[20230205 18:25:11 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:25:11 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:11 @agent_ppo2.py:191][0m |          -0.0009 |          12.5107 |           3.4000 |
[32m[20230205 18:25:11 @agent_ppo2.py:191][0m |          -0.0077 |          12.1868 |           3.3947 |
[32m[20230205 18:25:11 @agent_ppo2.py:191][0m |          -0.0095 |          11.9447 |           3.3916 |
[32m[20230205 18:25:11 @agent_ppo2.py:191][0m |          -0.0096 |          11.8527 |           3.3886 |
[32m[20230205 18:25:11 @agent_ppo2.py:191][0m |          -0.0097 |          11.7209 |           3.3888 |
[32m[20230205 18:25:11 @agent_ppo2.py:191][0m |          -0.0108 |          11.6461 |           3.3870 |
[32m[20230205 18:25:11 @agent_ppo2.py:191][0m |          -0.0131 |          11.5768 |           3.3853 |
[32m[20230205 18:25:11 @agent_ppo2.py:191][0m |          -0.0137 |          11.4804 |           3.3849 |
[32m[20230205 18:25:11 @agent_ppo2.py:191][0m |          -0.0148 |          11.4340 |           3.3854 |
[32m[20230205 18:25:11 @agent_ppo2.py:191][0m |           0.0298 |          15.8161 |           3.3853 |
[32m[20230205 18:25:11 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:25:12 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.03
[32m[20230205 18:25:12 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.84
[32m[20230205 18:25:12 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.18
[32m[20230205 18:25:12 @agent_ppo2.py:149][0m Total time:      11.74 min
[32m[20230205 18:25:12 @agent_ppo2.py:151][0m 792576 total steps have happened
[32m[20230205 18:25:12 @agent_ppo2.py:127][0m #------------------------ Iteration 387 --------------------------#
[32m[20230205 18:25:12 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:25:12 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:12 @agent_ppo2.py:191][0m |          -0.0018 |          11.8282 |           3.4029 |
[32m[20230205 18:25:12 @agent_ppo2.py:191][0m |          -0.0055 |          11.0110 |           3.3986 |
[32m[20230205 18:25:12 @agent_ppo2.py:191][0m |          -0.0082 |          10.3006 |           3.3969 |
[32m[20230205 18:25:12 @agent_ppo2.py:191][0m |          -0.0093 |           9.4882 |           3.3967 |
[32m[20230205 18:25:12 @agent_ppo2.py:191][0m |          -0.0101 |           8.9610 |           3.3946 |
[32m[20230205 18:25:13 @agent_ppo2.py:191][0m |          -0.0105 |           8.4540 |           3.3969 |
[32m[20230205 18:25:13 @agent_ppo2.py:191][0m |          -0.0123 |           8.1406 |           3.3949 |
[32m[20230205 18:25:13 @agent_ppo2.py:191][0m |          -0.0121 |           7.8566 |           3.3935 |
[32m[20230205 18:25:13 @agent_ppo2.py:191][0m |          -0.0130 |           7.6391 |           3.3922 |
[32m[20230205 18:25:13 @agent_ppo2.py:191][0m |          -0.0136 |           7.3929 |           3.3912 |
[32m[20230205 18:25:13 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:25:13 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.36
[32m[20230205 18:25:13 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.76
[32m[20230205 18:25:13 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.97
[32m[20230205 18:25:13 @agent_ppo2.py:149][0m Total time:      11.76 min
[32m[20230205 18:25:13 @agent_ppo2.py:151][0m 794624 total steps have happened
[32m[20230205 18:25:13 @agent_ppo2.py:127][0m #------------------------ Iteration 388 --------------------------#
[32m[20230205 18:25:14 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:25:14 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:14 @agent_ppo2.py:191][0m |          -0.0010 |          25.8859 |           3.3496 |
[32m[20230205 18:25:14 @agent_ppo2.py:191][0m |          -0.0031 |          15.1375 |           3.3485 |
[32m[20230205 18:25:14 @agent_ppo2.py:191][0m |          -0.0059 |          14.0036 |           3.3451 |
[32m[20230205 18:25:14 @agent_ppo2.py:191][0m |          -0.0088 |          13.2308 |           3.3453 |
[32m[20230205 18:25:14 @agent_ppo2.py:191][0m |          -0.0098 |          12.8963 |           3.3435 |
[32m[20230205 18:25:14 @agent_ppo2.py:191][0m |          -0.0102 |          12.4731 |           3.3457 |
[32m[20230205 18:25:14 @agent_ppo2.py:191][0m |          -0.0104 |          12.3153 |           3.3434 |
[32m[20230205 18:25:14 @agent_ppo2.py:191][0m |          -0.0114 |          12.3791 |           3.3441 |
[32m[20230205 18:25:14 @agent_ppo2.py:191][0m |          -0.0109 |          11.9255 |           3.3447 |
[32m[20230205 18:25:14 @agent_ppo2.py:191][0m |          -0.0124 |          11.6964 |           3.3425 |
[32m[20230205 18:25:14 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:25:15 @agent_ppo2.py:144][0m Average TRAINING episode reward: 133.44
[32m[20230205 18:25:15 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.93
[32m[20230205 18:25:15 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.94
[32m[20230205 18:25:15 @agent_ppo2.py:149][0m Total time:      11.79 min
[32m[20230205 18:25:15 @agent_ppo2.py:151][0m 796672 total steps have happened
[32m[20230205 18:25:15 @agent_ppo2.py:127][0m #------------------------ Iteration 389 --------------------------#
[32m[20230205 18:25:15 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:25:15 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:15 @agent_ppo2.py:191][0m |           0.0013 |          12.8956 |           3.3545 |
[32m[20230205 18:25:15 @agent_ppo2.py:191][0m |          -0.0073 |          12.1946 |           3.3510 |
[32m[20230205 18:25:15 @agent_ppo2.py:191][0m |          -0.0083 |          11.8202 |           3.3496 |
[32m[20230205 18:25:15 @agent_ppo2.py:191][0m |          -0.0105 |          11.5299 |           3.3497 |
[32m[20230205 18:25:16 @agent_ppo2.py:191][0m |          -0.0141 |          11.3143 |           3.3483 |
[32m[20230205 18:25:16 @agent_ppo2.py:191][0m |          -0.0164 |          11.1041 |           3.3426 |
[32m[20230205 18:25:16 @agent_ppo2.py:191][0m |          -0.0171 |          10.8721 |           3.3445 |
[32m[20230205 18:25:16 @agent_ppo2.py:191][0m |          -0.0093 |          10.7004 |           3.3409 |
[32m[20230205 18:25:16 @agent_ppo2.py:191][0m |          -0.0108 |          10.2890 |           3.3424 |
[32m[20230205 18:25:16 @agent_ppo2.py:191][0m |          -0.0271 |          10.1619 |           3.3399 |
[32m[20230205 18:25:16 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:25:16 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.41
[32m[20230205 18:25:16 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.25
[32m[20230205 18:25:16 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.72
[32m[20230205 18:25:16 @agent_ppo2.py:149][0m Total time:      11.81 min
[32m[20230205 18:25:16 @agent_ppo2.py:151][0m 798720 total steps have happened
[32m[20230205 18:25:16 @agent_ppo2.py:127][0m #------------------------ Iteration 390 --------------------------#
[32m[20230205 18:25:17 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:25:17 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:17 @agent_ppo2.py:191][0m |           0.0021 |          21.1431 |           3.3551 |
[32m[20230205 18:25:17 @agent_ppo2.py:191][0m |          -0.0071 |          15.6236 |           3.3478 |
[32m[20230205 18:25:17 @agent_ppo2.py:191][0m |          -0.0065 |          14.4571 |           3.3413 |
[32m[20230205 18:25:17 @agent_ppo2.py:191][0m |          -0.0087 |          14.2706 |           3.3392 |
[32m[20230205 18:25:17 @agent_ppo2.py:191][0m |          -0.0096 |          14.2943 |           3.3364 |
[32m[20230205 18:25:17 @agent_ppo2.py:191][0m |          -0.0124 |          13.7043 |           3.3354 |
[32m[20230205 18:25:17 @agent_ppo2.py:191][0m |          -0.0147 |          13.4823 |           3.3353 |
[32m[20230205 18:25:17 @agent_ppo2.py:191][0m |          -0.0123 |          13.8823 |           3.3337 |
[32m[20230205 18:25:17 @agent_ppo2.py:191][0m |          -0.0147 |          13.4995 |           3.3325 |
[32m[20230205 18:25:17 @agent_ppo2.py:191][0m |          -0.0124 |          13.7051 |           3.3322 |
[32m[20230205 18:25:17 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:25:18 @agent_ppo2.py:144][0m Average TRAINING episode reward: 132.93
[32m[20230205 18:25:18 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 265.21
[32m[20230205 18:25:18 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.76
[32m[20230205 18:25:18 @agent_ppo2.py:149][0m Total time:      11.84 min
[32m[20230205 18:25:18 @agent_ppo2.py:151][0m 800768 total steps have happened
[32m[20230205 18:25:18 @agent_ppo2.py:127][0m #------------------------ Iteration 391 --------------------------#
[32m[20230205 18:25:18 @agent_ppo2.py:133][0m Sampling time: 0.57 s by 1 slaves
[32m[20230205 18:25:18 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:18 @agent_ppo2.py:191][0m |          -0.0003 |          11.4566 |           3.3386 |
[32m[20230205 18:25:19 @agent_ppo2.py:191][0m |           0.0039 |          11.2496 |           3.3357 |
[32m[20230205 18:25:19 @agent_ppo2.py:191][0m |          -0.0142 |          10.5397 |           3.3355 |
[32m[20230205 18:25:19 @agent_ppo2.py:191][0m |          -0.0125 |          10.2314 |           3.3342 |
[32m[20230205 18:25:19 @agent_ppo2.py:191][0m |          -0.0142 |          10.0546 |           3.3330 |
[32m[20230205 18:25:19 @agent_ppo2.py:191][0m |          -0.0118 |           9.7195 |           3.3347 |
[32m[20230205 18:25:19 @agent_ppo2.py:191][0m |          -0.0158 |           9.3980 |           3.3329 |
[32m[20230205 18:25:19 @agent_ppo2.py:191][0m |          -0.0145 |           9.1948 |           3.3337 |
[32m[20230205 18:25:19 @agent_ppo2.py:191][0m |          -0.0186 |           8.9954 |           3.3328 |
[32m[20230205 18:25:19 @agent_ppo2.py:191][0m |          -0.0174 |           8.8495 |           3.3352 |
[32m[20230205 18:25:19 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:25:19 @agent_ppo2.py:144][0m Average TRAINING episode reward: 252.12
[32m[20230205 18:25:19 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.49
[32m[20230205 18:25:19 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.97
[32m[20230205 18:25:19 @agent_ppo2.py:149][0m Total time:      11.87 min
[32m[20230205 18:25:19 @agent_ppo2.py:151][0m 802816 total steps have happened
[32m[20230205 18:25:19 @agent_ppo2.py:127][0m #------------------------ Iteration 392 --------------------------#
[32m[20230205 18:25:20 @agent_ppo2.py:133][0m Sampling time: 0.47 s by 1 slaves
[32m[20230205 18:25:20 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:20 @agent_ppo2.py:191][0m |          -0.0027 |          43.3426 |           3.3965 |
[32m[20230205 18:25:20 @agent_ppo2.py:191][0m |          -0.0084 |          26.3745 |           3.3923 |
[32m[20230205 18:25:20 @agent_ppo2.py:191][0m |          -0.0118 |          21.8565 |           3.3901 |
[32m[20230205 18:25:20 @agent_ppo2.py:191][0m |          -0.0132 |          20.2827 |           3.3868 |
[32m[20230205 18:25:20 @agent_ppo2.py:191][0m |          -0.0129 |          19.7554 |           3.3857 |
[32m[20230205 18:25:20 @agent_ppo2.py:191][0m |          -0.0154 |          18.4795 |           3.3870 |
[32m[20230205 18:25:20 @agent_ppo2.py:191][0m |          -0.0164 |          18.1046 |           3.3852 |
[32m[20230205 18:25:20 @agent_ppo2.py:191][0m |          -0.0178 |          17.6335 |           3.3849 |
[32m[20230205 18:25:20 @agent_ppo2.py:191][0m |          -0.0183 |          17.2515 |           3.3847 |
[32m[20230205 18:25:20 @agent_ppo2.py:191][0m |          -0.0188 |          16.9860 |           3.3849 |
[32m[20230205 18:25:20 @agent_ppo2.py:136][0m Policy update time: 0.47 s
[32m[20230205 18:25:21 @agent_ppo2.py:144][0m Average TRAINING episode reward: 165.26
[32m[20230205 18:25:21 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.53
[32m[20230205 18:25:21 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.88
[32m[20230205 18:25:21 @agent_ppo2.py:149][0m Total time:      11.89 min
[32m[20230205 18:25:21 @agent_ppo2.py:151][0m 804864 total steps have happened
[32m[20230205 18:25:21 @agent_ppo2.py:127][0m #------------------------ Iteration 393 --------------------------#
[32m[20230205 18:25:21 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:25:21 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:21 @agent_ppo2.py:191][0m |          -0.0023 |          12.7379 |           3.2534 |
[32m[20230205 18:25:21 @agent_ppo2.py:191][0m |          -0.0117 |          11.7745 |           3.2510 |
[32m[20230205 18:25:22 @agent_ppo2.py:191][0m |          -0.0133 |          11.3234 |           3.2494 |
[32m[20230205 18:25:22 @agent_ppo2.py:191][0m |          -0.0093 |          10.8932 |           3.2471 |
[32m[20230205 18:25:22 @agent_ppo2.py:191][0m |          -0.0114 |          10.5860 |           3.2459 |
[32m[20230205 18:25:22 @agent_ppo2.py:191][0m |          -0.0163 |          10.3242 |           3.2472 |
[32m[20230205 18:25:22 @agent_ppo2.py:191][0m |          -0.0127 |          10.0256 |           3.2488 |
[32m[20230205 18:25:22 @agent_ppo2.py:191][0m |          -0.0186 |           9.7506 |           3.2505 |
[32m[20230205 18:25:22 @agent_ppo2.py:191][0m |          -0.0105 |           9.4944 |           3.2494 |
[32m[20230205 18:25:22 @agent_ppo2.py:191][0m |          -0.0159 |           9.1473 |           3.2516 |
[32m[20230205 18:25:22 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:25:22 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.71
[32m[20230205 18:25:22 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.60
[32m[20230205 18:25:22 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 165.04
[32m[20230205 18:25:22 @agent_ppo2.py:149][0m Total time:      11.92 min
[32m[20230205 18:25:22 @agent_ppo2.py:151][0m 806912 total steps have happened
[32m[20230205 18:25:22 @agent_ppo2.py:127][0m #------------------------ Iteration 394 --------------------------#
[32m[20230205 18:25:23 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:25:23 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:23 @agent_ppo2.py:191][0m |          -0.0002 |          13.5740 |           3.3212 |
[32m[20230205 18:25:23 @agent_ppo2.py:191][0m |          -0.0057 |          12.9940 |           3.3148 |
[32m[20230205 18:25:23 @agent_ppo2.py:191][0m |          -0.0080 |          12.6777 |           3.3143 |
[32m[20230205 18:25:23 @agent_ppo2.py:191][0m |          -0.0093 |          12.6117 |           3.3110 |
[32m[20230205 18:25:23 @agent_ppo2.py:191][0m |          -0.0114 |          12.3804 |           3.3100 |
[32m[20230205 18:25:23 @agent_ppo2.py:191][0m |          -0.0118 |          12.2732 |           3.3091 |
[32m[20230205 18:25:23 @agent_ppo2.py:191][0m |          -0.0137 |          12.1721 |           3.3053 |
[32m[20230205 18:25:23 @agent_ppo2.py:191][0m |          -0.0119 |          12.4083 |           3.3073 |
[32m[20230205 18:25:23 @agent_ppo2.py:191][0m |          -0.0137 |          12.0065 |           3.3060 |
[32m[20230205 18:25:23 @agent_ppo2.py:191][0m |          -0.0143 |          11.9460 |           3.3070 |
[32m[20230205 18:25:23 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:25:24 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.21
[32m[20230205 18:25:24 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.57
[32m[20230205 18:25:24 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 274.68
[32m[20230205 18:25:24 @agent_ppo2.py:149][0m Total time:      11.94 min
[32m[20230205 18:25:24 @agent_ppo2.py:151][0m 808960 total steps have happened
[32m[20230205 18:25:24 @agent_ppo2.py:127][0m #------------------------ Iteration 395 --------------------------#
[32m[20230205 18:25:24 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:25:24 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:24 @agent_ppo2.py:191][0m |           0.0002 |          12.8890 |           3.3757 |
[32m[20230205 18:25:25 @agent_ppo2.py:191][0m |          -0.0045 |          12.3411 |           3.3737 |
[32m[20230205 18:25:25 @agent_ppo2.py:191][0m |          -0.0073 |          12.1130 |           3.3715 |
[32m[20230205 18:25:25 @agent_ppo2.py:191][0m |          -0.0100 |          11.9054 |           3.3696 |
[32m[20230205 18:25:25 @agent_ppo2.py:191][0m |          -0.0113 |          11.7140 |           3.3661 |
[32m[20230205 18:25:25 @agent_ppo2.py:191][0m |          -0.0125 |          11.5601 |           3.3669 |
[32m[20230205 18:25:25 @agent_ppo2.py:191][0m |          -0.0125 |          11.3989 |           3.3660 |
[32m[20230205 18:25:25 @agent_ppo2.py:191][0m |          -0.0145 |          11.2500 |           3.3628 |
[32m[20230205 18:25:25 @agent_ppo2.py:191][0m |          -0.0146 |          11.0953 |           3.3619 |
[32m[20230205 18:25:25 @agent_ppo2.py:191][0m |          -0.0155 |          10.9661 |           3.3597 |
[32m[20230205 18:25:25 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:25:25 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.01
[32m[20230205 18:25:25 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.64
[32m[20230205 18:25:25 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.09
[32m[20230205 18:25:25 @agent_ppo2.py:149][0m Total time:      11.97 min
[32m[20230205 18:25:25 @agent_ppo2.py:151][0m 811008 total steps have happened
[32m[20230205 18:25:25 @agent_ppo2.py:127][0m #------------------------ Iteration 396 --------------------------#
[32m[20230205 18:25:26 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:25:26 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:26 @agent_ppo2.py:191][0m |          -0.0024 |          11.8893 |           3.1403 |
[32m[20230205 18:25:26 @agent_ppo2.py:191][0m |          -0.0123 |          11.3154 |           3.1314 |
[32m[20230205 18:25:26 @agent_ppo2.py:191][0m |          -0.0025 |          11.3739 |           3.1311 |
[32m[20230205 18:25:26 @agent_ppo2.py:191][0m |          -0.0202 |          10.6357 |           3.1278 |
[32m[20230205 18:25:26 @agent_ppo2.py:191][0m |          -0.0098 |          10.4789 |           3.1272 |
[32m[20230205 18:25:26 @agent_ppo2.py:191][0m |          -0.0161 |          10.2334 |           3.1272 |
[32m[20230205 18:25:26 @agent_ppo2.py:191][0m |          -0.0239 |          10.0592 |           3.1284 |
[32m[20230205 18:25:26 @agent_ppo2.py:191][0m |          -0.0153 |           9.9184 |           3.1254 |
[32m[20230205 18:25:26 @agent_ppo2.py:191][0m |          -0.0179 |           9.7930 |           3.1280 |
[32m[20230205 18:25:26 @agent_ppo2.py:191][0m |          -0.0232 |           9.6327 |           3.1237 |
[32m[20230205 18:25:26 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:25:27 @agent_ppo2.py:144][0m Average TRAINING episode reward: 255.68
[32m[20230205 18:25:27 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.48
[32m[20230205 18:25:27 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.11
[32m[20230205 18:25:27 @agent_ppo2.py:149][0m Total time:      11.99 min
[32m[20230205 18:25:27 @agent_ppo2.py:151][0m 813056 total steps have happened
[32m[20230205 18:25:27 @agent_ppo2.py:127][0m #------------------------ Iteration 397 --------------------------#
[32m[20230205 18:25:27 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:25:27 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:28 @agent_ppo2.py:191][0m |          -0.0003 |          12.8625 |           3.2502 |
[32m[20230205 18:25:28 @agent_ppo2.py:191][0m |          -0.0022 |          12.0846 |           3.2472 |
[32m[20230205 18:25:28 @agent_ppo2.py:191][0m |          -0.0022 |          11.7220 |           3.2465 |
[32m[20230205 18:25:28 @agent_ppo2.py:191][0m |          -0.0007 |          11.6286 |           3.2429 |
[32m[20230205 18:25:28 @agent_ppo2.py:191][0m |          -0.0049 |          11.0885 |           3.2425 |
[32m[20230205 18:25:28 @agent_ppo2.py:191][0m |          -0.0101 |          10.9183 |           3.2409 |
[32m[20230205 18:25:28 @agent_ppo2.py:191][0m |          -0.0070 |          10.7223 |           3.2367 |
[32m[20230205 18:25:28 @agent_ppo2.py:191][0m |          -0.0137 |          10.5532 |           3.2353 |
[32m[20230205 18:25:28 @agent_ppo2.py:191][0m |          -0.0052 |          10.5657 |           3.2364 |
[32m[20230205 18:25:28 @agent_ppo2.py:191][0m |          -0.0112 |          10.3744 |           3.2314 |
[32m[20230205 18:25:28 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:25:28 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.72
[32m[20230205 18:25:28 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.74
[32m[20230205 18:25:28 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.69
[32m[20230205 18:25:28 @agent_ppo2.py:149][0m Total time:      12.02 min
[32m[20230205 18:25:28 @agent_ppo2.py:151][0m 815104 total steps have happened
[32m[20230205 18:25:28 @agent_ppo2.py:127][0m #------------------------ Iteration 398 --------------------------#
[32m[20230205 18:25:29 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:25:29 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:29 @agent_ppo2.py:191][0m |          -0.0004 |          12.7338 |           3.3790 |
[32m[20230205 18:25:29 @agent_ppo2.py:191][0m |          -0.0063 |          12.2504 |           3.3769 |
[32m[20230205 18:25:29 @agent_ppo2.py:191][0m |          -0.0062 |          11.8012 |           3.3676 |
[32m[20230205 18:25:29 @agent_ppo2.py:191][0m |          -0.0042 |          11.6708 |           3.3694 |
[32m[20230205 18:25:29 @agent_ppo2.py:191][0m |          -0.0036 |          11.4029 |           3.3666 |
[32m[20230205 18:25:29 @agent_ppo2.py:191][0m |          -0.0111 |          10.8547 |           3.3629 |
[32m[20230205 18:25:29 @agent_ppo2.py:191][0m |          -0.0122 |          10.6064 |           3.3587 |
[32m[20230205 18:25:29 @agent_ppo2.py:191][0m |          -0.0115 |          10.4720 |           3.3578 |
[32m[20230205 18:25:29 @agent_ppo2.py:191][0m |          -0.0122 |          10.3170 |           3.3556 |
[32m[20230205 18:25:29 @agent_ppo2.py:191][0m |          -0.0114 |          10.1069 |           3.3540 |
[32m[20230205 18:25:29 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:25:30 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.12
[32m[20230205 18:25:30 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.57
[32m[20230205 18:25:30 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 269.53
[32m[20230205 18:25:30 @agent_ppo2.py:149][0m Total time:      12.04 min
[32m[20230205 18:25:30 @agent_ppo2.py:151][0m 817152 total steps have happened
[32m[20230205 18:25:30 @agent_ppo2.py:127][0m #------------------------ Iteration 399 --------------------------#
[32m[20230205 18:25:30 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:25:30 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:31 @agent_ppo2.py:191][0m |          -0.0070 |          13.8608 |           3.1893 |
[32m[20230205 18:25:31 @agent_ppo2.py:191][0m |          -0.0052 |          12.2927 |           3.1800 |
[32m[20230205 18:25:31 @agent_ppo2.py:191][0m |           0.0128 |          13.8534 |           3.1788 |
[32m[20230205 18:25:31 @agent_ppo2.py:191][0m |          -0.0091 |          10.3803 |           3.1666 |
[32m[20230205 18:25:31 @agent_ppo2.py:191][0m |          -0.0170 |           9.8569 |           3.1685 |
[32m[20230205 18:25:31 @agent_ppo2.py:191][0m |          -0.0139 |           9.5671 |           3.1686 |
[32m[20230205 18:25:31 @agent_ppo2.py:191][0m |          -0.0176 |           9.3813 |           3.1698 |
[32m[20230205 18:25:31 @agent_ppo2.py:191][0m |          -0.0186 |           9.1298 |           3.1666 |
[32m[20230205 18:25:31 @agent_ppo2.py:191][0m |          -0.0187 |           8.9130 |           3.1664 |
[32m[20230205 18:25:31 @agent_ppo2.py:191][0m |          -0.0133 |           8.7682 |           3.1647 |
[32m[20230205 18:25:31 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:25:31 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.06
[32m[20230205 18:25:31 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.38
[32m[20230205 18:25:31 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.95
[32m[20230205 18:25:31 @agent_ppo2.py:149][0m Total time:      12.07 min
[32m[20230205 18:25:31 @agent_ppo2.py:151][0m 819200 total steps have happened
[32m[20230205 18:25:31 @agent_ppo2.py:127][0m #------------------------ Iteration 400 --------------------------#
[32m[20230205 18:25:32 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:25:32 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:32 @agent_ppo2.py:191][0m |           0.0008 |          13.6589 |           3.3749 |
[32m[20230205 18:25:32 @agent_ppo2.py:191][0m |          -0.0059 |          12.7151 |           3.3692 |
[32m[20230205 18:25:32 @agent_ppo2.py:191][0m |          -0.0087 |          12.3161 |           3.3653 |
[32m[20230205 18:25:32 @agent_ppo2.py:191][0m |          -0.0116 |          11.9996 |           3.3642 |
[32m[20230205 18:25:32 @agent_ppo2.py:191][0m |          -0.0120 |          11.8154 |           3.3661 |
[32m[20230205 18:25:32 @agent_ppo2.py:191][0m |          -0.0130 |          11.6169 |           3.3652 |
[32m[20230205 18:25:32 @agent_ppo2.py:191][0m |          -0.0141 |          11.4335 |           3.3633 |
[32m[20230205 18:25:32 @agent_ppo2.py:191][0m |          -0.0138 |          11.3183 |           3.3613 |
[32m[20230205 18:25:32 @agent_ppo2.py:191][0m |          -0.0152 |          11.1624 |           3.3630 |
[32m[20230205 18:25:32 @agent_ppo2.py:191][0m |          -0.0154 |          11.1105 |           3.3620 |
[32m[20230205 18:25:32 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:25:33 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.03
[32m[20230205 18:25:33 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.44
[32m[20230205 18:25:33 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 271.72
[32m[20230205 18:25:33 @agent_ppo2.py:149][0m Total time:      12.09 min
[32m[20230205 18:25:33 @agent_ppo2.py:151][0m 821248 total steps have happened
[32m[20230205 18:25:33 @agent_ppo2.py:127][0m #------------------------ Iteration 401 --------------------------#
[32m[20230205 18:25:33 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:25:34 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:34 @agent_ppo2.py:191][0m |          -0.0003 |          13.7479 |           3.3351 |
[32m[20230205 18:25:34 @agent_ppo2.py:191][0m |          -0.0059 |          13.1847 |           3.3359 |
[32m[20230205 18:25:34 @agent_ppo2.py:191][0m |          -0.0096 |          12.8194 |           3.3320 |
[32m[20230205 18:25:34 @agent_ppo2.py:191][0m |          -0.0110 |          12.5777 |           3.3293 |
[32m[20230205 18:25:34 @agent_ppo2.py:191][0m |          -0.0127 |          12.3980 |           3.3273 |
[32m[20230205 18:25:34 @agent_ppo2.py:191][0m |          -0.0112 |          12.5249 |           3.3257 |
[32m[20230205 18:25:34 @agent_ppo2.py:191][0m |          -0.0127 |          12.2007 |           3.3260 |
[32m[20230205 18:25:34 @agent_ppo2.py:191][0m |          -0.0120 |          12.0438 |           3.3228 |
[32m[20230205 18:25:34 @agent_ppo2.py:191][0m |          -0.0099 |          12.2456 |           3.3253 |
[32m[20230205 18:25:34 @agent_ppo2.py:191][0m |          -0.0106 |          12.5998 |           3.3230 |
[32m[20230205 18:25:34 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:25:35 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.21
[32m[20230205 18:25:35 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.70
[32m[20230205 18:25:35 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 211.42
[32m[20230205 18:25:35 @agent_ppo2.py:149][0m Total time:      12.12 min
[32m[20230205 18:25:35 @agent_ppo2.py:151][0m 823296 total steps have happened
[32m[20230205 18:25:35 @agent_ppo2.py:127][0m #------------------------ Iteration 402 --------------------------#
[32m[20230205 18:25:35 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:25:35 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:35 @agent_ppo2.py:191][0m |          -0.0042 |          13.4859 |           3.3444 |
[32m[20230205 18:25:35 @agent_ppo2.py:191][0m |          -0.0084 |          12.9241 |           3.3382 |
[32m[20230205 18:25:35 @agent_ppo2.py:191][0m |          -0.0149 |          12.1620 |           3.3329 |
[32m[20230205 18:25:35 @agent_ppo2.py:191][0m |          -0.0145 |          11.7628 |           3.3280 |
[32m[20230205 18:25:35 @agent_ppo2.py:191][0m |          -0.0126 |          11.7296 |           3.3293 |
[32m[20230205 18:25:35 @agent_ppo2.py:191][0m |          -0.0157 |          11.1523 |           3.3278 |
[32m[20230205 18:25:35 @agent_ppo2.py:191][0m |          -0.0193 |          10.8490 |           3.3301 |
[32m[20230205 18:25:36 @agent_ppo2.py:191][0m |          -0.0178 |          10.5881 |           3.3298 |
[32m[20230205 18:25:36 @agent_ppo2.py:191][0m |          -0.0165 |          10.3201 |           3.3289 |
[32m[20230205 18:25:36 @agent_ppo2.py:191][0m |          -0.0169 |           9.9187 |           3.3317 |
[32m[20230205 18:25:36 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:25:36 @agent_ppo2.py:144][0m Average TRAINING episode reward: 264.25
[32m[20230205 18:25:36 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.79
[32m[20230205 18:25:36 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 161.86
[32m[20230205 18:25:36 @agent_ppo2.py:149][0m Total time:      12.15 min
[32m[20230205 18:25:36 @agent_ppo2.py:151][0m 825344 total steps have happened
[32m[20230205 18:25:36 @agent_ppo2.py:127][0m #------------------------ Iteration 403 --------------------------#
[32m[20230205 18:25:37 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:25:37 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:37 @agent_ppo2.py:191][0m |          -0.0007 |          14.2189 |           3.3100 |
[32m[20230205 18:25:37 @agent_ppo2.py:191][0m |          -0.0050 |          13.2389 |           3.3012 |
[32m[20230205 18:25:37 @agent_ppo2.py:191][0m |          -0.0077 |          12.8456 |           3.3018 |
[32m[20230205 18:25:37 @agent_ppo2.py:191][0m |          -0.0101 |          12.4047 |           3.3007 |
[32m[20230205 18:25:37 @agent_ppo2.py:191][0m |          -0.0108 |          12.1042 |           3.2998 |
[32m[20230205 18:25:37 @agent_ppo2.py:191][0m |          -0.0118 |          11.9881 |           3.2979 |
[32m[20230205 18:25:37 @agent_ppo2.py:191][0m |          -0.0129 |          11.6898 |           3.2979 |
[32m[20230205 18:25:37 @agent_ppo2.py:191][0m |          -0.0132 |          11.4197 |           3.2982 |
[32m[20230205 18:25:37 @agent_ppo2.py:191][0m |          -0.0142 |          11.1040 |           3.2984 |
[32m[20230205 18:25:37 @agent_ppo2.py:191][0m |          -0.0132 |          11.0375 |           3.2980 |
[32m[20230205 18:25:37 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:25:38 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.35
[32m[20230205 18:25:38 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.25
[32m[20230205 18:25:38 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 194.81
[32m[20230205 18:25:38 @agent_ppo2.py:149][0m Total time:      12.17 min
[32m[20230205 18:25:38 @agent_ppo2.py:151][0m 827392 total steps have happened
[32m[20230205 18:25:38 @agent_ppo2.py:127][0m #------------------------ Iteration 404 --------------------------#
[32m[20230205 18:25:38 @agent_ppo2.py:133][0m Sampling time: 0.46 s by 1 slaves
[32m[20230205 18:25:38 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:38 @agent_ppo2.py:191][0m |          -0.0005 |          47.5090 |           3.2952 |
[32m[20230205 18:25:38 @agent_ppo2.py:191][0m |          -0.0044 |          27.2983 |           3.2910 |
[32m[20230205 18:25:38 @agent_ppo2.py:191][0m |          -0.0105 |          20.0008 |           3.2911 |
[32m[20230205 18:25:38 @agent_ppo2.py:191][0m |          -0.0115 |          16.8929 |           3.2881 |
[32m[20230205 18:25:38 @agent_ppo2.py:191][0m |          -0.0139 |          15.5410 |           3.2876 |
[32m[20230205 18:25:38 @agent_ppo2.py:191][0m |          -0.0119 |          14.4028 |           3.2852 |
[32m[20230205 18:25:39 @agent_ppo2.py:191][0m |          -0.0135 |          13.4221 |           3.2841 |
[32m[20230205 18:25:39 @agent_ppo2.py:191][0m |          -0.0141 |          12.8766 |           3.2820 |
[32m[20230205 18:25:39 @agent_ppo2.py:191][0m |          -0.0103 |          12.7446 |           3.2810 |
[32m[20230205 18:25:39 @agent_ppo2.py:191][0m |          -0.0194 |          12.4793 |           3.2800 |
[32m[20230205 18:25:39 @agent_ppo2.py:136][0m Policy update time: 0.46 s
[32m[20230205 18:25:39 @agent_ppo2.py:144][0m Average TRAINING episode reward: 170.67
[32m[20230205 18:25:39 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.66
[32m[20230205 18:25:39 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 192.72
[32m[20230205 18:25:39 @agent_ppo2.py:149][0m Total time:      12.20 min
[32m[20230205 18:25:39 @agent_ppo2.py:151][0m 829440 total steps have happened
[32m[20230205 18:25:39 @agent_ppo2.py:127][0m #------------------------ Iteration 405 --------------------------#
[32m[20230205 18:25:40 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:25:40 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:40 @agent_ppo2.py:191][0m |           0.0451 |          21.0081 |           3.1582 |
[32m[20230205 18:25:40 @agent_ppo2.py:191][0m |          -0.0038 |          13.5133 |           3.1456 |
[32m[20230205 18:25:40 @agent_ppo2.py:191][0m |          -0.0071 |          13.0211 |           3.1464 |
[32m[20230205 18:25:40 @agent_ppo2.py:191][0m |          -0.0159 |          12.8406 |           3.1474 |
[32m[20230205 18:25:40 @agent_ppo2.py:191][0m |          -0.0065 |          12.4358 |           3.1473 |
[32m[20230205 18:25:40 @agent_ppo2.py:191][0m |          -0.0118 |          12.2656 |           3.1491 |
[32m[20230205 18:25:40 @agent_ppo2.py:191][0m |          -0.0149 |          12.0313 |           3.1489 |
[32m[20230205 18:25:40 @agent_ppo2.py:191][0m |          -0.0191 |          12.0867 |           3.1490 |
[32m[20230205 18:25:40 @agent_ppo2.py:191][0m |          -0.0052 |          11.9127 |           3.1481 |
[32m[20230205 18:25:40 @agent_ppo2.py:191][0m |          -0.0152 |          11.5730 |           3.1483 |
[32m[20230205 18:25:40 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:25:41 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.02
[32m[20230205 18:25:41 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.72
[32m[20230205 18:25:41 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 165.61
[32m[20230205 18:25:41 @agent_ppo2.py:149][0m Total time:      12.22 min
[32m[20230205 18:25:41 @agent_ppo2.py:151][0m 831488 total steps have happened
[32m[20230205 18:25:41 @agent_ppo2.py:127][0m #------------------------ Iteration 406 --------------------------#
[32m[20230205 18:25:41 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:25:41 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:41 @agent_ppo2.py:191][0m |          -0.0025 |          13.8420 |           3.2287 |
[32m[20230205 18:25:41 @agent_ppo2.py:191][0m |          -0.0091 |          13.5113 |           3.2264 |
[32m[20230205 18:25:41 @agent_ppo2.py:191][0m |          -0.0019 |          14.4748 |           3.2262 |
[32m[20230205 18:25:42 @agent_ppo2.py:191][0m |          -0.0055 |          13.2994 |           3.2254 |
[32m[20230205 18:25:42 @agent_ppo2.py:191][0m |          -0.0101 |          13.7867 |           3.2236 |
[32m[20230205 18:25:42 @agent_ppo2.py:191][0m |          -0.0143 |          12.7754 |           3.2202 |
[32m[20230205 18:25:42 @agent_ppo2.py:191][0m |          -0.0155 |          12.6285 |           3.2211 |
[32m[20230205 18:25:42 @agent_ppo2.py:191][0m |          -0.0192 |          12.5164 |           3.2213 |
[32m[20230205 18:25:42 @agent_ppo2.py:191][0m |          -0.0185 |          12.2752 |           3.2213 |
[32m[20230205 18:25:42 @agent_ppo2.py:191][0m |          -0.0172 |          12.1372 |           3.2202 |
[32m[20230205 18:25:42 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:25:42 @agent_ppo2.py:144][0m Average TRAINING episode reward: 263.72
[32m[20230205 18:25:42 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.70
[32m[20230205 18:25:42 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.72
[32m[20230205 18:25:42 @agent_ppo2.py:149][0m Total time:      12.25 min
[32m[20230205 18:25:42 @agent_ppo2.py:151][0m 833536 total steps have happened
[32m[20230205 18:25:42 @agent_ppo2.py:127][0m #------------------------ Iteration 407 --------------------------#
[32m[20230205 18:25:43 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:25:43 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:43 @agent_ppo2.py:191][0m |          -0.0055 |          13.8757 |           3.2579 |
[32m[20230205 18:25:43 @agent_ppo2.py:191][0m |          -0.0066 |          13.4894 |           3.2571 |
[32m[20230205 18:25:43 @agent_ppo2.py:191][0m |          -0.0130 |          13.0605 |           3.2575 |
[32m[20230205 18:25:43 @agent_ppo2.py:191][0m |          -0.0151 |          12.8593 |           3.2588 |
[32m[20230205 18:25:43 @agent_ppo2.py:191][0m |          -0.0148 |          12.7392 |           3.2571 |
[32m[20230205 18:25:43 @agent_ppo2.py:191][0m |          -0.0130 |          12.5920 |           3.2575 |
[32m[20230205 18:25:43 @agent_ppo2.py:191][0m |          -0.0191 |          12.5411 |           3.2564 |
[32m[20230205 18:25:43 @agent_ppo2.py:191][0m |          -0.0154 |          12.3484 |           3.2555 |
[32m[20230205 18:25:43 @agent_ppo2.py:191][0m |          -0.0085 |          12.8420 |           3.2561 |
[32m[20230205 18:25:43 @agent_ppo2.py:191][0m |          -0.0178 |          12.1703 |           3.2548 |
[32m[20230205 18:25:43 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:25:44 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.49
[32m[20230205 18:25:44 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 265.46
[32m[20230205 18:25:44 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 212.11
[32m[20230205 18:25:44 @agent_ppo2.py:149][0m Total time:      12.28 min
[32m[20230205 18:25:44 @agent_ppo2.py:151][0m 835584 total steps have happened
[32m[20230205 18:25:44 @agent_ppo2.py:127][0m #------------------------ Iteration 408 --------------------------#
[32m[20230205 18:25:44 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:25:45 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:45 @agent_ppo2.py:191][0m |          -0.0014 |          13.0991 |           3.1843 |
[32m[20230205 18:25:45 @agent_ppo2.py:191][0m |          -0.0052 |          12.8819 |           3.1823 |
[32m[20230205 18:25:45 @agent_ppo2.py:191][0m |          -0.0099 |          12.4847 |           3.1808 |
[32m[20230205 18:25:45 @agent_ppo2.py:191][0m |          -0.0084 |          12.6188 |           3.1828 |
[32m[20230205 18:25:45 @agent_ppo2.py:191][0m |          -0.0123 |          12.6011 |           3.1809 |
[32m[20230205 18:25:45 @agent_ppo2.py:191][0m |          -0.0113 |          12.3642 |           3.1795 |
[32m[20230205 18:25:45 @agent_ppo2.py:191][0m |          -0.0123 |          12.3079 |           3.1784 |
[32m[20230205 18:25:45 @agent_ppo2.py:191][0m |          -0.0109 |          12.1050 |           3.1772 |
[32m[20230205 18:25:45 @agent_ppo2.py:191][0m |          -0.0157 |          11.6218 |           3.1764 |
[32m[20230205 18:25:45 @agent_ppo2.py:191][0m |          -0.0118 |          11.8461 |           3.1775 |
[32m[20230205 18:25:45 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:25:46 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.76
[32m[20230205 18:25:46 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.84
[32m[20230205 18:25:46 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 189.93
[32m[20230205 18:25:46 @agent_ppo2.py:149][0m Total time:      12.30 min
[32m[20230205 18:25:46 @agent_ppo2.py:151][0m 837632 total steps have happened
[32m[20230205 18:25:46 @agent_ppo2.py:127][0m #------------------------ Iteration 409 --------------------------#
[32m[20230205 18:25:46 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:25:46 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:46 @agent_ppo2.py:191][0m |          -0.0010 |          13.9840 |           3.2070 |
[32m[20230205 18:25:46 @agent_ppo2.py:191][0m |          -0.0066 |          13.6769 |           3.2076 |
[32m[20230205 18:25:46 @agent_ppo2.py:191][0m |          -0.0073 |          13.7357 |           3.2086 |
[32m[20230205 18:25:46 @agent_ppo2.py:191][0m |          -0.0073 |          13.6403 |           3.2006 |
[32m[20230205 18:25:46 @agent_ppo2.py:191][0m |          -0.0137 |          13.3253 |           3.2026 |
[32m[20230205 18:25:47 @agent_ppo2.py:191][0m |          -0.0142 |          13.2738 |           3.2014 |
[32m[20230205 18:25:47 @agent_ppo2.py:191][0m |          -0.0122 |          13.4951 |           3.2008 |
[32m[20230205 18:25:47 @agent_ppo2.py:191][0m |          -0.0160 |          13.1470 |           3.2024 |
[32m[20230205 18:25:47 @agent_ppo2.py:191][0m |          -0.0165 |          13.1085 |           3.2016 |
[32m[20230205 18:25:47 @agent_ppo2.py:191][0m |          -0.0177 |          13.0365 |           3.2024 |
[32m[20230205 18:25:47 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:25:47 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.54
[32m[20230205 18:25:47 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.11
[32m[20230205 18:25:47 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 269.65
[32m[20230205 18:25:47 @agent_ppo2.py:149][0m Total time:      12.33 min
[32m[20230205 18:25:47 @agent_ppo2.py:151][0m 839680 total steps have happened
[32m[20230205 18:25:47 @agent_ppo2.py:127][0m #------------------------ Iteration 410 --------------------------#
[32m[20230205 18:25:48 @agent_ppo2.py:133][0m Sampling time: 0.58 s by 1 slaves
[32m[20230205 18:25:48 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:48 @agent_ppo2.py:191][0m |          -0.0007 |          49.0657 |           3.3014 |
[32m[20230205 18:25:48 @agent_ppo2.py:191][0m |          -0.0048 |          33.5999 |           3.2981 |
[32m[20230205 18:25:48 @agent_ppo2.py:191][0m |          -0.0077 |          28.6684 |           3.2952 |
[32m[20230205 18:25:48 @agent_ppo2.py:191][0m |          -0.0098 |          24.9278 |           3.2916 |
[32m[20230205 18:25:48 @agent_ppo2.py:191][0m |          -0.0104 |          23.6580 |           3.2897 |
[32m[20230205 18:25:48 @agent_ppo2.py:191][0m |          -0.0119 |          22.1911 |           3.2882 |
[32m[20230205 18:25:48 @agent_ppo2.py:191][0m |          -0.0129 |          20.9563 |           3.2854 |
[32m[20230205 18:25:48 @agent_ppo2.py:191][0m |          -0.0131 |          20.2404 |           3.2850 |
[32m[20230205 18:25:48 @agent_ppo2.py:191][0m |          -0.0125 |          19.9050 |           3.2832 |
[32m[20230205 18:25:48 @agent_ppo2.py:191][0m |          -0.0141 |          19.1069 |           3.2785 |
[32m[20230205 18:25:48 @agent_ppo2.py:136][0m Policy update time: 0.57 s
[32m[20230205 18:25:49 @agent_ppo2.py:144][0m Average TRAINING episode reward: 155.06
[32m[20230205 18:25:49 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.46
[32m[20230205 18:25:49 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 270.94
[32m[20230205 18:25:49 @agent_ppo2.py:149][0m Total time:      12.36 min
[32m[20230205 18:25:49 @agent_ppo2.py:151][0m 841728 total steps have happened
[32m[20230205 18:25:49 @agent_ppo2.py:127][0m #------------------------ Iteration 411 --------------------------#
[32m[20230205 18:25:49 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:25:49 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:49 @agent_ppo2.py:191][0m |           0.0030 |          14.4991 |           3.2134 |
[32m[20230205 18:25:50 @agent_ppo2.py:191][0m |          -0.0072 |          13.2935 |           3.2090 |
[32m[20230205 18:25:50 @agent_ppo2.py:191][0m |          -0.0088 |          12.9514 |           3.2073 |
[32m[20230205 18:25:50 @agent_ppo2.py:191][0m |          -0.0109 |          12.7004 |           3.2068 |
[32m[20230205 18:25:50 @agent_ppo2.py:191][0m |          -0.0115 |          12.5259 |           3.2082 |
[32m[20230205 18:25:50 @agent_ppo2.py:191][0m |          -0.0116 |          12.4015 |           3.2056 |
[32m[20230205 18:25:50 @agent_ppo2.py:191][0m |          -0.0130 |          12.2976 |           3.2087 |
[32m[20230205 18:25:50 @agent_ppo2.py:191][0m |          -0.0151 |          12.1891 |           3.2096 |
[32m[20230205 18:25:50 @agent_ppo2.py:191][0m |          -0.0144 |          12.1293 |           3.2069 |
[32m[20230205 18:25:50 @agent_ppo2.py:191][0m |          -0.0136 |          12.1454 |           3.2086 |
[32m[20230205 18:25:50 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:25:50 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.61
[32m[20230205 18:25:50 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.98
[32m[20230205 18:25:50 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 264.55
[32m[20230205 18:25:50 @agent_ppo2.py:149][0m Total time:      12.38 min
[32m[20230205 18:25:50 @agent_ppo2.py:151][0m 843776 total steps have happened
[32m[20230205 18:25:50 @agent_ppo2.py:127][0m #------------------------ Iteration 412 --------------------------#
[32m[20230205 18:25:51 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:25:51 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:51 @agent_ppo2.py:191][0m |          -0.0013 |          13.5252 |           3.2445 |
[32m[20230205 18:25:51 @agent_ppo2.py:191][0m |          -0.0104 |          12.8065 |           3.2331 |
[32m[20230205 18:25:51 @agent_ppo2.py:191][0m |          -0.0132 |          12.3737 |           3.2275 |
[32m[20230205 18:25:51 @agent_ppo2.py:191][0m |          -0.0145 |          12.1141 |           3.2258 |
[32m[20230205 18:25:51 @agent_ppo2.py:191][0m |          -0.0162 |          11.8738 |           3.2268 |
[32m[20230205 18:25:51 @agent_ppo2.py:191][0m |          -0.0164 |          11.6808 |           3.2248 |
[32m[20230205 18:25:51 @agent_ppo2.py:191][0m |          -0.0173 |          11.4802 |           3.2252 |
[32m[20230205 18:25:51 @agent_ppo2.py:191][0m |          -0.0171 |          11.3324 |           3.2218 |
[32m[20230205 18:25:51 @agent_ppo2.py:191][0m |          -0.0178 |          11.2008 |           3.2223 |
[32m[20230205 18:25:51 @agent_ppo2.py:191][0m |          -0.0184 |          11.0039 |           3.2215 |
[32m[20230205 18:25:51 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:25:52 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.29
[32m[20230205 18:25:52 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.36
[32m[20230205 18:25:52 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 199.41
[32m[20230205 18:25:52 @agent_ppo2.py:149][0m Total time:      12.41 min
[32m[20230205 18:25:52 @agent_ppo2.py:151][0m 845824 total steps have happened
[32m[20230205 18:25:52 @agent_ppo2.py:127][0m #------------------------ Iteration 413 --------------------------#
[32m[20230205 18:25:52 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:25:52 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:52 @agent_ppo2.py:191][0m |           0.0044 |          12.6032 |           3.2789 |
[32m[20230205 18:25:53 @agent_ppo2.py:191][0m |          -0.0049 |          11.9557 |           3.2758 |
[32m[20230205 18:25:53 @agent_ppo2.py:191][0m |          -0.0071 |          11.6593 |           3.2739 |
[32m[20230205 18:25:53 @agent_ppo2.py:191][0m |          -0.0033 |          11.8380 |           3.2734 |
[32m[20230205 18:25:53 @agent_ppo2.py:191][0m |          -0.0055 |          11.5329 |           3.2745 |
[32m[20230205 18:25:53 @agent_ppo2.py:191][0m |          -0.0088 |          11.2449 |           3.2731 |
[32m[20230205 18:25:53 @agent_ppo2.py:191][0m |          -0.0103 |          10.9628 |           3.2725 |
[32m[20230205 18:25:53 @agent_ppo2.py:191][0m |          -0.0119 |          10.7835 |           3.2691 |
[32m[20230205 18:25:53 @agent_ppo2.py:191][0m |          -0.0137 |          10.6814 |           3.2682 |
[32m[20230205 18:25:53 @agent_ppo2.py:191][0m |          -0.0102 |          10.7532 |           3.2691 |
[32m[20230205 18:25:53 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:25:54 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.65
[32m[20230205 18:25:54 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.61
[32m[20230205 18:25:54 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 103.41
[32m[20230205 18:25:54 @agent_ppo2.py:149][0m Total time:      12.44 min
[32m[20230205 18:25:54 @agent_ppo2.py:151][0m 847872 total steps have happened
[32m[20230205 18:25:54 @agent_ppo2.py:127][0m #------------------------ Iteration 414 --------------------------#
[32m[20230205 18:25:54 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:25:54 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:54 @agent_ppo2.py:191][0m |          -0.0069 |          12.6444 |           3.2187 |
[32m[20230205 18:25:54 @agent_ppo2.py:191][0m |           0.0185 |          13.0076 |           3.2169 |
[32m[20230205 18:25:54 @agent_ppo2.py:191][0m |          -0.0141 |          11.6744 |           3.2090 |
[32m[20230205 18:25:54 @agent_ppo2.py:191][0m |          -0.0063 |          11.3053 |           3.2106 |
[32m[20230205 18:25:54 @agent_ppo2.py:191][0m |          -0.0253 |          10.9755 |           3.2124 |
[32m[20230205 18:25:54 @agent_ppo2.py:191][0m |          -0.0179 |          10.7426 |           3.2155 |
[32m[20230205 18:25:55 @agent_ppo2.py:191][0m |          -0.0170 |          10.5658 |           3.2165 |
[32m[20230205 18:25:55 @agent_ppo2.py:191][0m |          -0.0136 |          10.4324 |           3.2174 |
[32m[20230205 18:25:55 @agent_ppo2.py:191][0m |          -0.0162 |          10.3011 |           3.2193 |
[32m[20230205 18:25:55 @agent_ppo2.py:191][0m |          -0.0188 |          10.2750 |           3.2166 |
[32m[20230205 18:25:55 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:25:55 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.08
[32m[20230205 18:25:55 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.36
[32m[20230205 18:25:55 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 172.32
[32m[20230205 18:25:55 @agent_ppo2.py:149][0m Total time:      12.46 min
[32m[20230205 18:25:55 @agent_ppo2.py:151][0m 849920 total steps have happened
[32m[20230205 18:25:55 @agent_ppo2.py:127][0m #------------------------ Iteration 415 --------------------------#
[32m[20230205 18:25:56 @agent_ppo2.py:133][0m Sampling time: 0.62 s by 1 slaves
[32m[20230205 18:25:56 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:56 @agent_ppo2.py:191][0m |          -0.0003 |          43.6032 |           3.2133 |
[32m[20230205 18:25:56 @agent_ppo2.py:191][0m |          -0.0039 |          29.4831 |           3.2134 |
[32m[20230205 18:25:56 @agent_ppo2.py:191][0m |          -0.0062 |          26.7510 |           3.2100 |
[32m[20230205 18:25:56 @agent_ppo2.py:191][0m |          -0.0086 |          25.2178 |           3.2162 |
[32m[20230205 18:25:56 @agent_ppo2.py:191][0m |          -0.0109 |          24.4088 |           3.2128 |
[32m[20230205 18:25:56 @agent_ppo2.py:191][0m |          -0.0132 |          22.8248 |           3.2130 |
[32m[20230205 18:25:56 @agent_ppo2.py:191][0m |          -0.0110 |          22.2872 |           3.2137 |
[32m[20230205 18:25:56 @agent_ppo2.py:191][0m |          -0.0078 |          21.3294 |           3.2116 |
[32m[20230205 18:25:56 @agent_ppo2.py:191][0m |          -0.0141 |          20.9263 |           3.2119 |
[32m[20230205 18:25:56 @agent_ppo2.py:191][0m |          -0.0150 |          20.4740 |           3.2108 |
[32m[20230205 18:25:56 @agent_ppo2.py:136][0m Policy update time: 0.62 s
[32m[20230205 18:25:57 @agent_ppo2.py:144][0m Average TRAINING episode reward: 124.26
[32m[20230205 18:25:57 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.55
[32m[20230205 18:25:57 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.33
[32m[20230205 18:25:57 @agent_ppo2.py:149][0m Total time:      12.49 min
[32m[20230205 18:25:57 @agent_ppo2.py:151][0m 851968 total steps have happened
[32m[20230205 18:25:57 @agent_ppo2.py:127][0m #------------------------ Iteration 416 --------------------------#
[32m[20230205 18:25:57 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:25:58 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:58 @agent_ppo2.py:191][0m |          -0.0001 |          16.1937 |           3.2760 |
[32m[20230205 18:25:58 @agent_ppo2.py:191][0m |          -0.0048 |          14.5667 |           3.2747 |
[32m[20230205 18:25:58 @agent_ppo2.py:191][0m |          -0.0102 |          13.3343 |           3.2746 |
[32m[20230205 18:25:58 @agent_ppo2.py:191][0m |          -0.0118 |          13.0652 |           3.2724 |
[32m[20230205 18:25:58 @agent_ppo2.py:191][0m |          -0.0102 |          13.2732 |           3.2755 |
[32m[20230205 18:25:58 @agent_ppo2.py:191][0m |          -0.0144 |          12.7289 |           3.2751 |
[32m[20230205 18:25:58 @agent_ppo2.py:191][0m |          -0.0160 |          12.6298 |           3.2763 |
[32m[20230205 18:25:58 @agent_ppo2.py:191][0m |          -0.0151 |          12.4520 |           3.2753 |
[32m[20230205 18:25:58 @agent_ppo2.py:191][0m |          -0.0121 |          12.8073 |           3.2748 |
[32m[20230205 18:25:58 @agent_ppo2.py:191][0m |          -0.0171 |          12.2420 |           3.2761 |
[32m[20230205 18:25:58 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:25:58 @agent_ppo2.py:144][0m Average TRAINING episode reward: 255.04
[32m[20230205 18:25:58 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.85
[32m[20230205 18:25:58 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 179.24
[32m[20230205 18:25:58 @agent_ppo2.py:149][0m Total time:      12.52 min
[32m[20230205 18:25:58 @agent_ppo2.py:151][0m 854016 total steps have happened
[32m[20230205 18:25:58 @agent_ppo2.py:127][0m #------------------------ Iteration 417 --------------------------#
[32m[20230205 18:25:59 @agent_ppo2.py:133][0m Sampling time: 0.68 s by 1 slaves
[32m[20230205 18:25:59 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:25:59 @agent_ppo2.py:191][0m |          -0.0057 |          66.5093 |           3.2615 |
[32m[20230205 18:25:59 @agent_ppo2.py:191][0m |          -0.0100 |          39.8090 |           3.2573 |
[32m[20230205 18:25:59 @agent_ppo2.py:191][0m |          -0.0085 |          34.7444 |           3.2523 |
[32m[20230205 18:25:59 @agent_ppo2.py:191][0m |          -0.0070 |          31.9669 |           3.2509 |
[32m[20230205 18:26:00 @agent_ppo2.py:191][0m |          -0.0170 |          30.1786 |           3.2517 |
[32m[20230205 18:26:00 @agent_ppo2.py:191][0m |           0.0012 |          32.7091 |           3.2493 |
[32m[20230205 18:26:00 @agent_ppo2.py:191][0m |          -0.0175 |          27.7272 |           3.2445 |
[32m[20230205 18:26:00 @agent_ppo2.py:191][0m |          -0.0170 |          26.8265 |           3.2460 |
[32m[20230205 18:26:00 @agent_ppo2.py:191][0m |          -0.0189 |          26.0903 |           3.2471 |
[32m[20230205 18:26:00 @agent_ppo2.py:191][0m |          -0.0160 |          25.8903 |           3.2462 |
[32m[20230205 18:26:00 @agent_ppo2.py:136][0m Policy update time: 0.66 s
[32m[20230205 18:26:00 @agent_ppo2.py:144][0m Average TRAINING episode reward: 105.82
[32m[20230205 18:26:00 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 265.57
[32m[20230205 18:26:00 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 263.83
[32m[20230205 18:26:00 @agent_ppo2.py:149][0m Total time:      12.55 min
[32m[20230205 18:26:00 @agent_ppo2.py:151][0m 856064 total steps have happened
[32m[20230205 18:26:00 @agent_ppo2.py:127][0m #------------------------ Iteration 418 --------------------------#
[32m[20230205 18:26:01 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:26:01 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:01 @agent_ppo2.py:191][0m |           0.0040 |         116.7395 |           3.1973 |
[32m[20230205 18:26:01 @agent_ppo2.py:191][0m |          -0.0066 |          96.3409 |           3.1884 |
[32m[20230205 18:26:01 @agent_ppo2.py:191][0m |          -0.0098 |          89.3258 |           3.1854 |
[32m[20230205 18:26:01 @agent_ppo2.py:191][0m |          -0.0175 |          82.8006 |           3.1855 |
[32m[20230205 18:26:01 @agent_ppo2.py:191][0m |          -0.0201 |          78.7101 |           3.1842 |
[32m[20230205 18:26:01 @agent_ppo2.py:191][0m |          -0.0110 |          76.3268 |           3.1844 |
[32m[20230205 18:26:01 @agent_ppo2.py:191][0m |          -0.0064 |          78.7453 |           3.1838 |
[32m[20230205 18:26:01 @agent_ppo2.py:191][0m |          -0.0129 |          71.9771 |           3.1835 |
[32m[20230205 18:26:01 @agent_ppo2.py:191][0m |          -0.0203 |          69.1885 |           3.1831 |
[32m[20230205 18:26:01 @agent_ppo2.py:191][0m |          -0.0211 |          67.5494 |           3.1834 |
[32m[20230205 18:26:01 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:02 @agent_ppo2.py:144][0m Average TRAINING episode reward: 46.80
[32m[20230205 18:26:02 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.45
[32m[20230205 18:26:02 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 177.59
[32m[20230205 18:26:02 @agent_ppo2.py:149][0m Total time:      12.58 min
[32m[20230205 18:26:02 @agent_ppo2.py:151][0m 858112 total steps have happened
[32m[20230205 18:26:02 @agent_ppo2.py:127][0m #------------------------ Iteration 419 --------------------------#
[32m[20230205 18:26:02 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:26:03 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:03 @agent_ppo2.py:191][0m |          -0.0047 |          28.3608 |           3.3419 |
[32m[20230205 18:26:03 @agent_ppo2.py:191][0m |          -0.0037 |          18.5214 |           3.3352 |
[32m[20230205 18:26:03 @agent_ppo2.py:191][0m |          -0.0108 |          17.0721 |           3.3333 |
[32m[20230205 18:26:03 @agent_ppo2.py:191][0m |          -0.0124 |          16.4034 |           3.3316 |
[32m[20230205 18:26:03 @agent_ppo2.py:191][0m |          -0.0118 |          17.1925 |           3.3292 |
[32m[20230205 18:26:03 @agent_ppo2.py:191][0m |          -0.0078 |          16.1370 |           3.3318 |
[32m[20230205 18:26:03 @agent_ppo2.py:191][0m |          -0.0122 |          15.6193 |           3.3320 |
[32m[20230205 18:26:03 @agent_ppo2.py:191][0m |          -0.0170 |          15.2012 |           3.3306 |
[32m[20230205 18:26:03 @agent_ppo2.py:191][0m |          -0.0128 |          15.3935 |           3.3327 |
[32m[20230205 18:26:03 @agent_ppo2.py:191][0m |          -0.0184 |          14.7437 |           3.3333 |
[32m[20230205 18:26:03 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:26:03 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.25
[32m[20230205 18:26:03 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.27
[32m[20230205 18:26:03 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 190.16
[32m[20230205 18:26:03 @agent_ppo2.py:149][0m Total time:      12.60 min
[32m[20230205 18:26:03 @agent_ppo2.py:151][0m 860160 total steps have happened
[32m[20230205 18:26:03 @agent_ppo2.py:127][0m #------------------------ Iteration 420 --------------------------#
[32m[20230205 18:26:04 @agent_ppo2.py:133][0m Sampling time: 0.68 s by 1 slaves
[32m[20230205 18:26:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:04 @agent_ppo2.py:191][0m |           0.0017 |          46.9650 |           3.2999 |
[32m[20230205 18:26:04 @agent_ppo2.py:191][0m |          -0.0045 |          38.1204 |           3.2973 |
[32m[20230205 18:26:04 @agent_ppo2.py:191][0m |          -0.0076 |          35.1918 |           3.2928 |
[32m[20230205 18:26:04 @agent_ppo2.py:191][0m |          -0.0087 |          32.5434 |           3.2935 |
[32m[20230205 18:26:04 @agent_ppo2.py:191][0m |          -0.0099 |          31.1024 |           3.2884 |
[32m[20230205 18:26:05 @agent_ppo2.py:191][0m |          -0.0113 |          29.7609 |           3.2905 |
[32m[20230205 18:26:05 @agent_ppo2.py:191][0m |          -0.0117 |          28.7668 |           3.2878 |
[32m[20230205 18:26:05 @agent_ppo2.py:191][0m |          -0.0125 |          27.9148 |           3.2867 |
[32m[20230205 18:26:05 @agent_ppo2.py:191][0m |          -0.0132 |          27.0270 |           3.2867 |
[32m[20230205 18:26:05 @agent_ppo2.py:191][0m |          -0.0132 |          26.4602 |           3.2850 |
[32m[20230205 18:26:05 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:26:05 @agent_ppo2.py:144][0m Average TRAINING episode reward: 152.41
[32m[20230205 18:26:05 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 252.56
[32m[20230205 18:26:05 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 207.23
[32m[20230205 18:26:05 @agent_ppo2.py:149][0m Total time:      12.63 min
[32m[20230205 18:26:05 @agent_ppo2.py:151][0m 862208 total steps have happened
[32m[20230205 18:26:05 @agent_ppo2.py:127][0m #------------------------ Iteration 421 --------------------------#
[32m[20230205 18:26:06 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:26:06 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:06 @agent_ppo2.py:191][0m |          -0.0005 |          14.3795 |           3.2645 |
[32m[20230205 18:26:06 @agent_ppo2.py:191][0m |          -0.0070 |          12.0048 |           3.2609 |
[32m[20230205 18:26:06 @agent_ppo2.py:191][0m |          -0.0085 |          11.6479 |           3.2596 |
[32m[20230205 18:26:06 @agent_ppo2.py:191][0m |          -0.0127 |          11.2839 |           3.2588 |
[32m[20230205 18:26:06 @agent_ppo2.py:191][0m |          -0.0134 |          11.0694 |           3.2577 |
[32m[20230205 18:26:06 @agent_ppo2.py:191][0m |          -0.0146 |          10.9079 |           3.2585 |
[32m[20230205 18:26:06 @agent_ppo2.py:191][0m |          -0.0147 |          10.8335 |           3.2601 |
[32m[20230205 18:26:06 @agent_ppo2.py:191][0m |          -0.0143 |          10.7299 |           3.2563 |
[32m[20230205 18:26:06 @agent_ppo2.py:191][0m |          -0.0156 |          10.6089 |           3.2567 |
[32m[20230205 18:26:06 @agent_ppo2.py:191][0m |          -0.0149 |          10.5798 |           3.2574 |
[32m[20230205 18:26:06 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:07 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.36
[32m[20230205 18:26:07 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.47
[32m[20230205 18:26:07 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 266.05
[32m[20230205 18:26:07 @agent_ppo2.py:149][0m Total time:      12.66 min
[32m[20230205 18:26:07 @agent_ppo2.py:151][0m 864256 total steps have happened
[32m[20230205 18:26:07 @agent_ppo2.py:127][0m #------------------------ Iteration 422 --------------------------#
[32m[20230205 18:26:07 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:26:07 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:07 @agent_ppo2.py:191][0m |           0.0032 |          14.9544 |           3.1978 |
[32m[20230205 18:26:07 @agent_ppo2.py:191][0m |          -0.0036 |          12.9290 |           3.1942 |
[32m[20230205 18:26:08 @agent_ppo2.py:191][0m |          -0.0096 |          12.5076 |           3.1950 |
[32m[20230205 18:26:08 @agent_ppo2.py:191][0m |          -0.0090 |          12.6368 |           3.1943 |
[32m[20230205 18:26:08 @agent_ppo2.py:191][0m |          -0.0081 |          12.6674 |           3.1940 |
[32m[20230205 18:26:08 @agent_ppo2.py:191][0m |          -0.0105 |          12.2379 |           3.1919 |
[32m[20230205 18:26:08 @agent_ppo2.py:191][0m |          -0.0132 |          12.0122 |           3.1928 |
[32m[20230205 18:26:08 @agent_ppo2.py:191][0m |          -0.0149 |          11.9050 |           3.1933 |
[32m[20230205 18:26:08 @agent_ppo2.py:191][0m |          -0.0159 |          11.8395 |           3.1928 |
[32m[20230205 18:26:08 @agent_ppo2.py:191][0m |          -0.0106 |          12.2389 |           3.1922 |
[32m[20230205 18:26:08 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:26:08 @agent_ppo2.py:144][0m Average TRAINING episode reward: 255.96
[32m[20230205 18:26:08 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 256.58
[32m[20230205 18:26:08 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 192.79
[32m[20230205 18:26:08 @agent_ppo2.py:149][0m Total time:      12.68 min
[32m[20230205 18:26:08 @agent_ppo2.py:151][0m 866304 total steps have happened
[32m[20230205 18:26:08 @agent_ppo2.py:127][0m #------------------------ Iteration 423 --------------------------#
[32m[20230205 18:26:09 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:26:09 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:09 @agent_ppo2.py:191][0m |          -0.0008 |          12.3421 |           3.2480 |
[32m[20230205 18:26:09 @agent_ppo2.py:191][0m |          -0.0009 |          12.2730 |           3.2439 |
[32m[20230205 18:26:09 @agent_ppo2.py:191][0m |          -0.0086 |          11.6257 |           3.2456 |
[32m[20230205 18:26:09 @agent_ppo2.py:191][0m |          -0.0097 |          11.5487 |           3.2440 |
[32m[20230205 18:26:09 @agent_ppo2.py:191][0m |          -0.0133 |          11.3237 |           3.2431 |
[32m[20230205 18:26:09 @agent_ppo2.py:191][0m |          -0.0129 |          11.5378 |           3.2438 |
[32m[20230205 18:26:09 @agent_ppo2.py:191][0m |          -0.0137 |          11.1378 |           3.2413 |
[32m[20230205 18:26:09 @agent_ppo2.py:191][0m |          -0.0153 |          11.0417 |           3.2430 |
[32m[20230205 18:26:09 @agent_ppo2.py:191][0m |          -0.0133 |          11.2643 |           3.2435 |
[32m[20230205 18:26:09 @agent_ppo2.py:191][0m |          -0.0182 |          10.9025 |           3.2443 |
[32m[20230205 18:26:10 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:10 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.61
[32m[20230205 18:26:10 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.56
[32m[20230205 18:26:10 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.22
[32m[20230205 18:26:10 @agent_ppo2.py:149][0m Total time:      12.71 min
[32m[20230205 18:26:10 @agent_ppo2.py:151][0m 868352 total steps have happened
[32m[20230205 18:26:10 @agent_ppo2.py:127][0m #------------------------ Iteration 424 --------------------------#
[32m[20230205 18:26:10 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:26:11 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:11 @agent_ppo2.py:191][0m |           0.0089 |          13.1305 |           3.2989 |
[32m[20230205 18:26:11 @agent_ppo2.py:191][0m |          -0.0032 |          12.5888 |           3.2921 |
[32m[20230205 18:26:11 @agent_ppo2.py:191][0m |          -0.0146 |          11.9398 |           3.2894 |
[32m[20230205 18:26:11 @agent_ppo2.py:191][0m |          -0.0093 |          12.0901 |           3.2905 |
[32m[20230205 18:26:11 @agent_ppo2.py:191][0m |          -0.0161 |          11.6713 |           3.2874 |
[32m[20230205 18:26:11 @agent_ppo2.py:191][0m |          -0.0155 |          11.5649 |           3.2876 |
[32m[20230205 18:26:11 @agent_ppo2.py:191][0m |          -0.0102 |          11.7387 |           3.2883 |
[32m[20230205 18:26:11 @agent_ppo2.py:191][0m |          -0.0207 |          11.3708 |           3.2851 |
[32m[20230205 18:26:11 @agent_ppo2.py:191][0m |          -0.0126 |          11.3214 |           3.2837 |
[32m[20230205 18:26:11 @agent_ppo2.py:191][0m |          -0.0156 |          11.1439 |           3.2844 |
[32m[20230205 18:26:11 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:11 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.86
[32m[20230205 18:26:11 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.77
[32m[20230205 18:26:11 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 266.03
[32m[20230205 18:26:11 @agent_ppo2.py:149][0m Total time:      12.73 min
[32m[20230205 18:26:11 @agent_ppo2.py:151][0m 870400 total steps have happened
[32m[20230205 18:26:11 @agent_ppo2.py:127][0m #------------------------ Iteration 425 --------------------------#
[32m[20230205 18:26:12 @agent_ppo2.py:133][0m Sampling time: 0.71 s by 1 slaves
[32m[20230205 18:26:12 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:12 @agent_ppo2.py:191][0m |           0.0009 |          24.2885 |           3.2410 |
[32m[20230205 18:26:12 @agent_ppo2.py:191][0m |          -0.0053 |          18.4034 |           3.2389 |
[32m[20230205 18:26:12 @agent_ppo2.py:191][0m |          -0.0078 |          16.2246 |           3.2364 |
[32m[20230205 18:26:13 @agent_ppo2.py:191][0m |          -0.0079 |          14.7163 |           3.2316 |
[32m[20230205 18:26:13 @agent_ppo2.py:191][0m |          -0.0029 |          14.3867 |           3.2312 |
[32m[20230205 18:26:13 @agent_ppo2.py:191][0m |          -0.0131 |          12.8566 |           3.2237 |
[32m[20230205 18:26:13 @agent_ppo2.py:191][0m |          -0.0116 |          12.4165 |           3.2235 |
[32m[20230205 18:26:13 @agent_ppo2.py:191][0m |          -0.0141 |          12.0553 |           3.2257 |
[32m[20230205 18:26:13 @agent_ppo2.py:191][0m |          -0.0110 |          11.6384 |           3.2208 |
[32m[20230205 18:26:13 @agent_ppo2.py:191][0m |          -0.0136 |          11.2856 |           3.2220 |
[32m[20230205 18:26:13 @agent_ppo2.py:136][0m Policy update time: 0.71 s
[32m[20230205 18:26:13 @agent_ppo2.py:144][0m Average TRAINING episode reward: 204.29
[32m[20230205 18:26:13 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.16
[32m[20230205 18:26:13 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 44.93
[32m[20230205 18:26:13 @agent_ppo2.py:149][0m Total time:      12.77 min
[32m[20230205 18:26:13 @agent_ppo2.py:151][0m 872448 total steps have happened
[32m[20230205 18:26:13 @agent_ppo2.py:127][0m #------------------------ Iteration 426 --------------------------#
[32m[20230205 18:26:14 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:26:14 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:14 @agent_ppo2.py:191][0m |          -0.0004 |          12.9464 |           3.1833 |
[32m[20230205 18:26:14 @agent_ppo2.py:191][0m |          -0.0043 |          11.7884 |           3.1822 |
[32m[20230205 18:26:14 @agent_ppo2.py:191][0m |          -0.0045 |          11.4782 |           3.1774 |
[32m[20230205 18:26:14 @agent_ppo2.py:191][0m |          -0.0069 |          11.2146 |           3.1755 |
[32m[20230205 18:26:14 @agent_ppo2.py:191][0m |          -0.0109 |          10.7497 |           3.1743 |
[32m[20230205 18:26:14 @agent_ppo2.py:191][0m |          -0.0136 |          10.5449 |           3.1750 |
[32m[20230205 18:26:14 @agent_ppo2.py:191][0m |          -0.0137 |          10.4367 |           3.1714 |
[32m[20230205 18:26:14 @agent_ppo2.py:191][0m |          -0.0110 |          10.5473 |           3.1686 |
[32m[20230205 18:26:14 @agent_ppo2.py:191][0m |          -0.0150 |          10.1997 |           3.1697 |
[32m[20230205 18:26:15 @agent_ppo2.py:191][0m |          -0.0159 |          10.0877 |           3.1694 |
[32m[20230205 18:26:15 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:15 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.34
[32m[20230205 18:26:15 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.02
[32m[20230205 18:26:15 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 154.73
[32m[20230205 18:26:15 @agent_ppo2.py:149][0m Total time:      12.79 min
[32m[20230205 18:26:15 @agent_ppo2.py:151][0m 874496 total steps have happened
[32m[20230205 18:26:15 @agent_ppo2.py:127][0m #------------------------ Iteration 427 --------------------------#
[32m[20230205 18:26:16 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:26:16 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:16 @agent_ppo2.py:191][0m |           0.0074 |          40.7320 |           3.1783 |
[32m[20230205 18:26:16 @agent_ppo2.py:191][0m |          -0.0082 |          29.4322 |           3.1792 |
[32m[20230205 18:26:16 @agent_ppo2.py:191][0m |          -0.0103 |          27.5491 |           3.1753 |
[32m[20230205 18:26:16 @agent_ppo2.py:191][0m |          -0.0110 |          25.2762 |           3.1672 |
[32m[20230205 18:26:16 @agent_ppo2.py:191][0m |           0.0031 |          24.2655 |           3.1712 |
[32m[20230205 18:26:16 @agent_ppo2.py:191][0m |          -0.0104 |          23.6512 |           3.1620 |
[32m[20230205 18:26:16 @agent_ppo2.py:191][0m |          -0.0125 |          23.2085 |           3.1613 |
[32m[20230205 18:26:16 @agent_ppo2.py:191][0m |          -0.0009 |          26.1582 |           3.1728 |
[32m[20230205 18:26:16 @agent_ppo2.py:191][0m |          -0.0108 |          22.1688 |           3.1694 |
[32m[20230205 18:26:16 @agent_ppo2.py:191][0m |          -0.0092 |          21.2505 |           3.1720 |
[32m[20230205 18:26:16 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:17 @agent_ppo2.py:144][0m Average TRAINING episode reward: 185.92
[32m[20230205 18:26:17 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 253.65
[32m[20230205 18:26:17 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 270.71
[32m[20230205 18:26:17 @agent_ppo2.py:149][0m Total time:      12.82 min
[32m[20230205 18:26:17 @agent_ppo2.py:151][0m 876544 total steps have happened
[32m[20230205 18:26:17 @agent_ppo2.py:127][0m #------------------------ Iteration 428 --------------------------#
[32m[20230205 18:26:17 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:26:17 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:17 @agent_ppo2.py:191][0m |          -0.0017 |          13.6498 |           3.1791 |
[32m[20230205 18:26:17 @agent_ppo2.py:191][0m |          -0.0046 |          12.4704 |           3.1775 |
[32m[20230205 18:26:17 @agent_ppo2.py:191][0m |          -0.0094 |          11.6086 |           3.1742 |
[32m[20230205 18:26:17 @agent_ppo2.py:191][0m |          -0.0045 |          11.6340 |           3.1712 |
[32m[20230205 18:26:17 @agent_ppo2.py:191][0m |          -0.0078 |          10.9298 |           3.1714 |
[32m[20230205 18:26:17 @agent_ppo2.py:191][0m |          -0.0071 |          10.4191 |           3.1646 |
[32m[20230205 18:26:17 @agent_ppo2.py:191][0m |          -0.0152 |          10.0595 |           3.1660 |
[32m[20230205 18:26:17 @agent_ppo2.py:191][0m |          -0.0131 |           9.7376 |           3.1639 |
[32m[20230205 18:26:18 @agent_ppo2.py:191][0m |          -0.0126 |           9.5038 |           3.1636 |
[32m[20230205 18:26:18 @agent_ppo2.py:191][0m |          -0.0144 |           9.2394 |           3.1627 |
[32m[20230205 18:26:18 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:26:18 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.22
[32m[20230205 18:26:18 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.82
[32m[20230205 18:26:18 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 171.34
[32m[20230205 18:26:18 @agent_ppo2.py:149][0m Total time:      12.85 min
[32m[20230205 18:26:18 @agent_ppo2.py:151][0m 878592 total steps have happened
[32m[20230205 18:26:18 @agent_ppo2.py:127][0m #------------------------ Iteration 429 --------------------------#
[32m[20230205 18:26:19 @agent_ppo2.py:133][0m Sampling time: 0.60 s by 1 slaves
[32m[20230205 18:26:19 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:19 @agent_ppo2.py:191][0m |          -0.0011 |          77.0098 |           3.2929 |
[32m[20230205 18:26:19 @agent_ppo2.py:191][0m |          -0.0054 |          53.6410 |           3.2876 |
[32m[20230205 18:26:19 @agent_ppo2.py:191][0m |          -0.0103 |          43.8737 |           3.2903 |
[32m[20230205 18:26:19 @agent_ppo2.py:191][0m |          -0.0118 |          40.1268 |           3.2889 |
[32m[20230205 18:26:19 @agent_ppo2.py:191][0m |          -0.0126 |          37.1616 |           3.2869 |
[32m[20230205 18:26:19 @agent_ppo2.py:191][0m |          -0.0140 |          35.1339 |           3.2879 |
[32m[20230205 18:26:19 @agent_ppo2.py:191][0m |          -0.0149 |          33.7384 |           3.2855 |
[32m[20230205 18:26:19 @agent_ppo2.py:191][0m |          -0.0154 |          32.1824 |           3.2840 |
[32m[20230205 18:26:19 @agent_ppo2.py:191][0m |          -0.0152 |          31.2633 |           3.2851 |
[32m[20230205 18:26:19 @agent_ppo2.py:191][0m |          -0.0181 |          30.5045 |           3.2847 |
[32m[20230205 18:26:19 @agent_ppo2.py:136][0m Policy update time: 0.59 s
[32m[20230205 18:26:20 @agent_ppo2.py:144][0m Average TRAINING episode reward: 143.11
[32m[20230205 18:26:20 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.84
[32m[20230205 18:26:20 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 271.14
[32m[20230205 18:26:20 @agent_ppo2.py:149][0m Total time:      12.87 min
[32m[20230205 18:26:20 @agent_ppo2.py:151][0m 880640 total steps have happened
[32m[20230205 18:26:20 @agent_ppo2.py:127][0m #------------------------ Iteration 430 --------------------------#
[32m[20230205 18:26:20 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:26:20 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:20 @agent_ppo2.py:191][0m |           0.0017 |          15.7991 |           3.2061 |
[32m[20230205 18:26:20 @agent_ppo2.py:191][0m |          -0.0204 |          13.3635 |           3.2068 |
[32m[20230205 18:26:20 @agent_ppo2.py:191][0m |           0.0102 |          12.2889 |           3.2070 |
[32m[20230205 18:26:21 @agent_ppo2.py:191][0m |           0.0038 |          11.3188 |           3.2056 |
[32m[20230205 18:26:21 @agent_ppo2.py:191][0m |          -0.0074 |          10.9092 |           3.2067 |
[32m[20230205 18:26:21 @agent_ppo2.py:191][0m |          -0.0013 |          10.5139 |           3.2053 |
[32m[20230205 18:26:21 @agent_ppo2.py:191][0m |          -0.0076 |          10.2233 |           3.2078 |
[32m[20230205 18:26:21 @agent_ppo2.py:191][0m |          -0.0099 |           9.9572 |           3.2058 |
[32m[20230205 18:26:21 @agent_ppo2.py:191][0m |          -0.0156 |           9.7331 |           3.2073 |
[32m[20230205 18:26:21 @agent_ppo2.py:191][0m |          -0.0134 |           9.5308 |           3.2077 |
[32m[20230205 18:26:21 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:26:21 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.29
[32m[20230205 18:26:21 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.31
[32m[20230205 18:26:21 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 151.81
[32m[20230205 18:26:21 @agent_ppo2.py:149][0m Total time:      12.90 min
[32m[20230205 18:26:21 @agent_ppo2.py:151][0m 882688 total steps have happened
[32m[20230205 18:26:21 @agent_ppo2.py:127][0m #------------------------ Iteration 431 --------------------------#
[32m[20230205 18:26:22 @agent_ppo2.py:133][0m Sampling time: 0.62 s by 1 slaves
[32m[20230205 18:26:22 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:22 @agent_ppo2.py:191][0m |           0.0000 |          28.6933 |           3.2991 |
[32m[20230205 18:26:22 @agent_ppo2.py:191][0m |          -0.0068 |          24.3490 |           3.2949 |
[32m[20230205 18:26:22 @agent_ppo2.py:191][0m |          -0.0079 |          22.9837 |           3.2908 |
[32m[20230205 18:26:22 @agent_ppo2.py:191][0m |          -0.0091 |          22.2594 |           3.2898 |
[32m[20230205 18:26:22 @agent_ppo2.py:191][0m |          -0.0109 |          21.4558 |           3.2918 |
[32m[20230205 18:26:22 @agent_ppo2.py:191][0m |          -0.0111 |          20.9535 |           3.2888 |
[32m[20230205 18:26:22 @agent_ppo2.py:191][0m |          -0.0134 |          20.3518 |           3.2915 |
[32m[20230205 18:26:23 @agent_ppo2.py:191][0m |          -0.0122 |          20.0497 |           3.2869 |
[32m[20230205 18:26:23 @agent_ppo2.py:191][0m |          -0.0135 |          19.6642 |           3.2889 |
[32m[20230205 18:26:23 @agent_ppo2.py:191][0m |          -0.0141 |          19.3277 |           3.2894 |
[32m[20230205 18:26:23 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:26:23 @agent_ppo2.py:144][0m Average TRAINING episode reward: 165.39
[32m[20230205 18:26:23 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.73
[32m[20230205 18:26:23 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 104.30
[32m[20230205 18:26:23 @agent_ppo2.py:149][0m Total time:      12.93 min
[32m[20230205 18:26:23 @agent_ppo2.py:151][0m 884736 total steps have happened
[32m[20230205 18:26:23 @agent_ppo2.py:127][0m #------------------------ Iteration 432 --------------------------#
[32m[20230205 18:26:24 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:26:24 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:24 @agent_ppo2.py:191][0m |          -0.0036 |          94.5428 |           3.3245 |
[32m[20230205 18:26:24 @agent_ppo2.py:191][0m |          -0.0064 |          74.5972 |           3.3233 |
[32m[20230205 18:26:24 @agent_ppo2.py:191][0m |          -0.0123 |          64.1769 |           3.3216 |
[32m[20230205 18:26:24 @agent_ppo2.py:191][0m |          -0.0121 |          58.7571 |           3.3231 |
[32m[20230205 18:26:24 @agent_ppo2.py:191][0m |          -0.0127 |          55.7671 |           3.3193 |
[32m[20230205 18:26:24 @agent_ppo2.py:191][0m |          -0.0185 |          50.6662 |           3.3185 |
[32m[20230205 18:26:24 @agent_ppo2.py:191][0m |          -0.0152 |          50.0995 |           3.3190 |
[32m[20230205 18:26:24 @agent_ppo2.py:191][0m |          -0.0203 |          47.0805 |           3.3170 |
[32m[20230205 18:26:24 @agent_ppo2.py:191][0m |          -0.0184 |          45.6464 |           3.3168 |
[32m[20230205 18:26:24 @agent_ppo2.py:191][0m |          -0.0203 |          44.5161 |           3.3133 |
[32m[20230205 18:26:24 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:26:25 @agent_ppo2.py:144][0m Average TRAINING episode reward: 83.95
[32m[20230205 18:26:25 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.47
[32m[20230205 18:26:25 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 147.96
[32m[20230205 18:26:25 @agent_ppo2.py:149][0m Total time:      12.95 min
[32m[20230205 18:26:25 @agent_ppo2.py:151][0m 886784 total steps have happened
[32m[20230205 18:26:25 @agent_ppo2.py:127][0m #------------------------ Iteration 433 --------------------------#
[32m[20230205 18:26:25 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:26:25 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:25 @agent_ppo2.py:191][0m |          -0.0007 |          28.4100 |           3.3318 |
[32m[20230205 18:26:25 @agent_ppo2.py:191][0m |          -0.0036 |          18.6784 |           3.3315 |
[32m[20230205 18:26:25 @agent_ppo2.py:191][0m |          -0.0054 |          17.6513 |           3.3306 |
[32m[20230205 18:26:25 @agent_ppo2.py:191][0m |          -0.0055 |          17.3094 |           3.3281 |
[32m[20230205 18:26:25 @agent_ppo2.py:191][0m |          -0.0082 |          16.9237 |           3.3270 |
[32m[20230205 18:26:25 @agent_ppo2.py:191][0m |          -0.0076 |          16.5875 |           3.3263 |
[32m[20230205 18:26:26 @agent_ppo2.py:191][0m |          -0.0103 |          16.3980 |           3.3242 |
[32m[20230205 18:26:26 @agent_ppo2.py:191][0m |          -0.0102 |          16.1662 |           3.3247 |
[32m[20230205 18:26:26 @agent_ppo2.py:191][0m |          -0.0117 |          15.9010 |           3.3245 |
[32m[20230205 18:26:26 @agent_ppo2.py:191][0m |          -0.0116 |          15.7469 |           3.3219 |
[32m[20230205 18:26:26 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:26 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.63
[32m[20230205 18:26:26 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.84
[32m[20230205 18:26:26 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 177.08
[32m[20230205 18:26:26 @agent_ppo2.py:149][0m Total time:      12.98 min
[32m[20230205 18:26:26 @agent_ppo2.py:151][0m 888832 total steps have happened
[32m[20230205 18:26:26 @agent_ppo2.py:127][0m #------------------------ Iteration 434 --------------------------#
[32m[20230205 18:26:27 @agent_ppo2.py:133][0m Sampling time: 0.68 s by 1 slaves
[32m[20230205 18:26:27 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:27 @agent_ppo2.py:191][0m |           0.0025 |          40.9040 |           3.3362 |
[32m[20230205 18:26:27 @agent_ppo2.py:191][0m |          -0.0011 |          25.8292 |           3.3363 |
[32m[20230205 18:26:27 @agent_ppo2.py:191][0m |          -0.0047 |          22.1666 |           3.3342 |
[32m[20230205 18:26:27 @agent_ppo2.py:191][0m |          -0.0069 |          20.9677 |           3.3296 |
[32m[20230205 18:26:27 @agent_ppo2.py:191][0m |          -0.0072 |          19.5915 |           3.3278 |
[32m[20230205 18:26:27 @agent_ppo2.py:191][0m |          -0.0080 |          18.7909 |           3.3272 |
[32m[20230205 18:26:27 @agent_ppo2.py:191][0m |          -0.0093 |          18.0294 |           3.3282 |
[32m[20230205 18:26:28 @agent_ppo2.py:191][0m |          -0.0110 |          17.4786 |           3.3244 |
[32m[20230205 18:26:28 @agent_ppo2.py:191][0m |          -0.0115 |          16.6084 |           3.3254 |
[32m[20230205 18:26:28 @agent_ppo2.py:191][0m |          -0.0101 |          16.2772 |           3.3257 |
[32m[20230205 18:26:28 @agent_ppo2.py:136][0m Policy update time: 0.66 s
[32m[20230205 18:26:28 @agent_ppo2.py:144][0m Average TRAINING episode reward: 110.23
[32m[20230205 18:26:28 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.47
[32m[20230205 18:26:28 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.75
[32m[20230205 18:26:28 @agent_ppo2.py:149][0m Total time:      13.01 min
[32m[20230205 18:26:28 @agent_ppo2.py:151][0m 890880 total steps have happened
[32m[20230205 18:26:28 @agent_ppo2.py:127][0m #------------------------ Iteration 435 --------------------------#
[32m[20230205 18:26:29 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:26:29 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:29 @agent_ppo2.py:191][0m |           0.0020 |          18.6684 |           3.2126 |
[32m[20230205 18:26:29 @agent_ppo2.py:191][0m |          -0.0073 |          15.1467 |           3.2091 |
[32m[20230205 18:26:29 @agent_ppo2.py:191][0m |          -0.0124 |          14.2906 |           3.2060 |
[32m[20230205 18:26:29 @agent_ppo2.py:191][0m |          -0.0077 |          13.8359 |           3.2098 |
[32m[20230205 18:26:29 @agent_ppo2.py:191][0m |          -0.0155 |          13.7269 |           3.2077 |
[32m[20230205 18:26:29 @agent_ppo2.py:191][0m |          -0.0149 |          12.9866 |           3.2071 |
[32m[20230205 18:26:29 @agent_ppo2.py:191][0m |           0.0047 |          13.8058 |           3.2083 |
[32m[20230205 18:26:29 @agent_ppo2.py:191][0m |          -0.0114 |          12.4816 |           3.2026 |
[32m[20230205 18:26:29 @agent_ppo2.py:191][0m |          -0.0197 |          12.3367 |           3.2068 |
[32m[20230205 18:26:29 @agent_ppo2.py:191][0m |          -0.0193 |          12.0629 |           3.2042 |
[32m[20230205 18:26:29 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:26:30 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.07
[32m[20230205 18:26:30 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.73
[32m[20230205 18:26:30 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 180.23
[32m[20230205 18:26:30 @agent_ppo2.py:149][0m Total time:      13.04 min
[32m[20230205 18:26:30 @agent_ppo2.py:151][0m 892928 total steps have happened
[32m[20230205 18:26:30 @agent_ppo2.py:127][0m #------------------------ Iteration 436 --------------------------#
[32m[20230205 18:26:30 @agent_ppo2.py:133][0m Sampling time: 0.56 s by 1 slaves
[32m[20230205 18:26:30 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:30 @agent_ppo2.py:191][0m |          -0.0045 |          13.3488 |           3.2556 |
[32m[20230205 18:26:30 @agent_ppo2.py:191][0m |          -0.0129 |          12.0315 |           3.2487 |
[32m[20230205 18:26:30 @agent_ppo2.py:191][0m |          -0.0132 |          11.4699 |           3.2422 |
[32m[20230205 18:26:31 @agent_ppo2.py:191][0m |          -0.0006 |          11.2748 |           3.2419 |
[32m[20230205 18:26:31 @agent_ppo2.py:191][0m |          -0.0165 |          10.6531 |           3.2392 |
[32m[20230205 18:26:31 @agent_ppo2.py:191][0m |          -0.0220 |          10.3215 |           3.2388 |
[32m[20230205 18:26:31 @agent_ppo2.py:191][0m |           0.0100 |          10.8001 |           3.2373 |
[32m[20230205 18:26:31 @agent_ppo2.py:191][0m |          -0.0173 |           9.7892 |           3.2339 |
[32m[20230205 18:26:31 @agent_ppo2.py:191][0m |          -0.0109 |           9.6202 |           3.2367 |
[32m[20230205 18:26:31 @agent_ppo2.py:191][0m |          -0.0185 |           9.3216 |           3.2349 |
[32m[20230205 18:26:31 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:26:31 @agent_ppo2.py:144][0m Average TRAINING episode reward: 251.08
[32m[20230205 18:26:31 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.23
[32m[20230205 18:26:31 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.25
[32m[20230205 18:26:31 @agent_ppo2.py:149][0m Total time:      13.07 min
[32m[20230205 18:26:31 @agent_ppo2.py:151][0m 894976 total steps have happened
[32m[20230205 18:26:31 @agent_ppo2.py:127][0m #------------------------ Iteration 437 --------------------------#
[32m[20230205 18:26:32 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:26:32 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:32 @agent_ppo2.py:191][0m |           0.0018 |          12.5385 |           3.3166 |
[32m[20230205 18:26:32 @agent_ppo2.py:191][0m |          -0.0083 |          11.8453 |           3.3157 |
[32m[20230205 18:26:32 @agent_ppo2.py:191][0m |          -0.0031 |          12.1482 |           3.3110 |
[32m[20230205 18:26:32 @agent_ppo2.py:191][0m |          -0.0108 |          11.1488 |           3.3061 |
[32m[20230205 18:26:32 @agent_ppo2.py:191][0m |          -0.0150 |          10.9193 |           3.3035 |
[32m[20230205 18:26:32 @agent_ppo2.py:191][0m |          -0.0134 |          10.5703 |           3.3048 |
[32m[20230205 18:26:32 @agent_ppo2.py:191][0m |          -0.0038 |          10.6980 |           3.3047 |
[32m[20230205 18:26:32 @agent_ppo2.py:191][0m |          -0.0141 |          10.1753 |           3.3036 |
[32m[20230205 18:26:32 @agent_ppo2.py:191][0m |          -0.0059 |          11.4524 |           3.3002 |
[32m[20230205 18:26:32 @agent_ppo2.py:191][0m |          -0.0129 |           9.6724 |           3.3037 |
[32m[20230205 18:26:32 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:33 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.25
[32m[20230205 18:26:33 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.98
[32m[20230205 18:26:33 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 206.74
[32m[20230205 18:26:33 @agent_ppo2.py:149][0m Total time:      13.09 min
[32m[20230205 18:26:33 @agent_ppo2.py:151][0m 897024 total steps have happened
[32m[20230205 18:26:33 @agent_ppo2.py:127][0m #------------------------ Iteration 438 --------------------------#
[32m[20230205 18:26:33 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:26:33 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:33 @agent_ppo2.py:191][0m |          -0.0006 |          12.5728 |           3.3758 |
[32m[20230205 18:26:33 @agent_ppo2.py:191][0m |          -0.0064 |          11.8566 |           3.3665 |
[32m[20230205 18:26:33 @agent_ppo2.py:191][0m |          -0.0026 |          11.8290 |           3.3676 |
[32m[20230205 18:26:34 @agent_ppo2.py:191][0m |          -0.0086 |          11.1313 |           3.3623 |
[32m[20230205 18:26:34 @agent_ppo2.py:191][0m |          -0.0029 |          11.3719 |           3.3625 |
[32m[20230205 18:26:34 @agent_ppo2.py:191][0m |          -0.0130 |          10.5679 |           3.3614 |
[32m[20230205 18:26:34 @agent_ppo2.py:191][0m |          -0.0119 |          10.3269 |           3.3611 |
[32m[20230205 18:26:34 @agent_ppo2.py:191][0m |          -0.0138 |          10.1034 |           3.3593 |
[32m[20230205 18:26:34 @agent_ppo2.py:191][0m |          -0.0143 |           9.8681 |           3.3559 |
[32m[20230205 18:26:34 @agent_ppo2.py:191][0m |          -0.0095 |           9.8475 |           3.3590 |
[32m[20230205 18:26:34 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:34 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.17
[32m[20230205 18:26:34 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.90
[32m[20230205 18:26:34 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.49
[32m[20230205 18:26:34 @agent_ppo2.py:149][0m Total time:      13.11 min
[32m[20230205 18:26:34 @agent_ppo2.py:151][0m 899072 total steps have happened
[32m[20230205 18:26:34 @agent_ppo2.py:127][0m #------------------------ Iteration 439 --------------------------#
[32m[20230205 18:26:35 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:26:35 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:35 @agent_ppo2.py:191][0m |           0.0005 |          13.1113 |           3.3795 |
[32m[20230205 18:26:35 @agent_ppo2.py:191][0m |          -0.0043 |          12.3886 |           3.3785 |
[32m[20230205 18:26:35 @agent_ppo2.py:191][0m |          -0.0053 |          12.0795 |           3.3760 |
[32m[20230205 18:26:35 @agent_ppo2.py:191][0m |          -0.0083 |          11.9093 |           3.3733 |
[32m[20230205 18:26:35 @agent_ppo2.py:191][0m |          -0.0080 |          11.7818 |           3.3728 |
[32m[20230205 18:26:35 @agent_ppo2.py:191][0m |          -0.0078 |          11.9678 |           3.3717 |
[32m[20230205 18:26:35 @agent_ppo2.py:191][0m |          -0.0099 |          11.5749 |           3.3717 |
[32m[20230205 18:26:35 @agent_ppo2.py:191][0m |          -0.0110 |          11.4907 |           3.3714 |
[32m[20230205 18:26:35 @agent_ppo2.py:191][0m |          -0.0112 |          11.4519 |           3.3705 |
[32m[20230205 18:26:35 @agent_ppo2.py:191][0m |          -0.0121 |          11.2400 |           3.3686 |
[32m[20230205 18:26:35 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:26:36 @agent_ppo2.py:144][0m Average TRAINING episode reward: 256.07
[32m[20230205 18:26:36 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.58
[32m[20230205 18:26:36 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 269.17
[32m[20230205 18:26:36 @agent_ppo2.py:149][0m Total time:      13.14 min
[32m[20230205 18:26:36 @agent_ppo2.py:151][0m 901120 total steps have happened
[32m[20230205 18:26:36 @agent_ppo2.py:127][0m #------------------------ Iteration 440 --------------------------#
[32m[20230205 18:26:36 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:26:36 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:36 @agent_ppo2.py:191][0m |           0.0003 |          13.3969 |           3.2263 |
[32m[20230205 18:26:36 @agent_ppo2.py:191][0m |          -0.0146 |          11.2602 |           3.2189 |
[32m[20230205 18:26:36 @agent_ppo2.py:191][0m |          -0.0230 |          10.6371 |           3.2170 |
[32m[20230205 18:26:37 @agent_ppo2.py:191][0m |           0.0001 |           9.8896 |           3.2137 |
[32m[20230205 18:26:37 @agent_ppo2.py:191][0m |          -0.0256 |           9.5098 |           3.2103 |
[32m[20230205 18:26:37 @agent_ppo2.py:191][0m |          -0.0119 |           9.0738 |           3.2108 |
[32m[20230205 18:26:37 @agent_ppo2.py:191][0m |          -0.0288 |           8.8729 |           3.2152 |
[32m[20230205 18:26:37 @agent_ppo2.py:191][0m |          -0.0166 |           8.6584 |           3.2143 |
[32m[20230205 18:26:37 @agent_ppo2.py:191][0m |          -0.0179 |           8.3133 |           3.2173 |
[32m[20230205 18:26:37 @agent_ppo2.py:191][0m |          -0.0278 |           8.1828 |           3.2157 |
[32m[20230205 18:26:37 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:26:37 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.34
[32m[20230205 18:26:37 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.18
[32m[20230205 18:26:37 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 268.47
[32m[20230205 18:26:37 @agent_ppo2.py:149][0m Total time:      13.17 min
[32m[20230205 18:26:37 @agent_ppo2.py:151][0m 903168 total steps have happened
[32m[20230205 18:26:37 @agent_ppo2.py:127][0m #------------------------ Iteration 441 --------------------------#
[32m[20230205 18:26:38 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:26:38 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:38 @agent_ppo2.py:191][0m |           0.0008 |          15.0507 |           3.3695 |
[32m[20230205 18:26:38 @agent_ppo2.py:191][0m |          -0.0084 |          14.1898 |           3.3719 |
[32m[20230205 18:26:38 @agent_ppo2.py:191][0m |          -0.0115 |          13.6809 |           3.3680 |
[32m[20230205 18:26:38 @agent_ppo2.py:191][0m |          -0.0118 |          13.3756 |           3.3637 |
[32m[20230205 18:26:38 @agent_ppo2.py:191][0m |          -0.0140 |          13.1254 |           3.3656 |
[32m[20230205 18:26:38 @agent_ppo2.py:191][0m |          -0.0141 |          12.8507 |           3.3623 |
[32m[20230205 18:26:38 @agent_ppo2.py:191][0m |          -0.0157 |          12.6213 |           3.3630 |
[32m[20230205 18:26:38 @agent_ppo2.py:191][0m |          -0.0160 |          12.2975 |           3.3639 |
[32m[20230205 18:26:38 @agent_ppo2.py:191][0m |          -0.0173 |          11.9614 |           3.3624 |
[32m[20230205 18:26:38 @agent_ppo2.py:191][0m |          -0.0176 |          11.5500 |           3.3622 |
[32m[20230205 18:26:38 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:39 @agent_ppo2.py:144][0m Average TRAINING episode reward: 256.78
[32m[20230205 18:26:39 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.73
[32m[20230205 18:26:39 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 264.81
[32m[20230205 18:26:39 @agent_ppo2.py:149][0m Total time:      13.19 min
[32m[20230205 18:26:39 @agent_ppo2.py:151][0m 905216 total steps have happened
[32m[20230205 18:26:39 @agent_ppo2.py:127][0m #------------------------ Iteration 442 --------------------------#
[32m[20230205 18:26:39 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:26:39 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:39 @agent_ppo2.py:191][0m |          -0.0008 |          14.8205 |           3.3283 |
[32m[20230205 18:26:39 @agent_ppo2.py:191][0m |          -0.0097 |          13.1822 |           3.3228 |
[32m[20230205 18:26:39 @agent_ppo2.py:191][0m |          -0.0104 |          12.6667 |           3.3132 |
[32m[20230205 18:26:40 @agent_ppo2.py:191][0m |          -0.0127 |          11.8775 |           3.3134 |
[32m[20230205 18:26:40 @agent_ppo2.py:191][0m |          -0.0154 |          11.1414 |           3.3134 |
[32m[20230205 18:26:40 @agent_ppo2.py:191][0m |          -0.0119 |          10.9590 |           3.3121 |
[32m[20230205 18:26:40 @agent_ppo2.py:191][0m |          -0.0154 |          10.2901 |           3.3123 |
[32m[20230205 18:26:40 @agent_ppo2.py:191][0m |          -0.0176 |           9.8165 |           3.3099 |
[32m[20230205 18:26:40 @agent_ppo2.py:191][0m |          -0.0162 |           9.5619 |           3.3095 |
[32m[20230205 18:26:40 @agent_ppo2.py:191][0m |          -0.0159 |           9.3620 |           3.3111 |
[32m[20230205 18:26:40 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:40 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.75
[32m[20230205 18:26:40 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.68
[32m[20230205 18:26:40 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 121.83
[32m[20230205 18:26:40 @agent_ppo2.py:149][0m Total time:      13.22 min
[32m[20230205 18:26:40 @agent_ppo2.py:151][0m 907264 total steps have happened
[32m[20230205 18:26:40 @agent_ppo2.py:127][0m #------------------------ Iteration 443 --------------------------#
[32m[20230205 18:26:41 @agent_ppo2.py:133][0m Sampling time: 0.45 s by 1 slaves
[32m[20230205 18:26:41 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:41 @agent_ppo2.py:191][0m |           0.0027 |          43.1356 |           3.3591 |
[32m[20230205 18:26:41 @agent_ppo2.py:191][0m |          -0.0069 |          21.4102 |           3.3600 |
[32m[20230205 18:26:41 @agent_ppo2.py:191][0m |          -0.0136 |          19.1804 |           3.3589 |
[32m[20230205 18:26:41 @agent_ppo2.py:191][0m |          -0.0131 |          18.2502 |           3.3624 |
[32m[20230205 18:26:41 @agent_ppo2.py:191][0m |          -0.0125 |          17.2844 |           3.3613 |
[32m[20230205 18:26:41 @agent_ppo2.py:191][0m |          -0.0219 |          17.3557 |           3.3618 |
[32m[20230205 18:26:41 @agent_ppo2.py:191][0m |           0.1646 |          16.0616 |           3.3614 |
[32m[20230205 18:26:41 @agent_ppo2.py:191][0m |          -0.0164 |          15.8888 |           3.3476 |
[32m[20230205 18:26:41 @agent_ppo2.py:191][0m |          -0.0169 |          15.0030 |           3.3606 |
[32m[20230205 18:26:41 @agent_ppo2.py:191][0m |           0.0134 |          16.6152 |           3.3651 |
[32m[20230205 18:26:41 @agent_ppo2.py:136][0m Policy update time: 0.46 s
[32m[20230205 18:26:42 @agent_ppo2.py:144][0m Average TRAINING episode reward: 77.79
[32m[20230205 18:26:42 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.62
[32m[20230205 18:26:42 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 265.32
[32m[20230205 18:26:42 @agent_ppo2.py:149][0m Total time:      13.24 min
[32m[20230205 18:26:42 @agent_ppo2.py:151][0m 909312 total steps have happened
[32m[20230205 18:26:42 @agent_ppo2.py:127][0m #------------------------ Iteration 444 --------------------------#
[32m[20230205 18:26:42 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:26:42 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:42 @agent_ppo2.py:191][0m |          -0.0051 |          14.3517 |           3.3088 |
[32m[20230205 18:26:42 @agent_ppo2.py:191][0m |          -0.0263 |          13.7162 |           3.2987 |
[32m[20230205 18:26:42 @agent_ppo2.py:191][0m |          -0.0042 |          13.2908 |           3.2963 |
[32m[20230205 18:26:42 @agent_ppo2.py:191][0m |          -0.0225 |          12.9254 |           3.2976 |
[32m[20230205 18:26:42 @agent_ppo2.py:191][0m |          -0.0173 |          12.6558 |           3.2954 |
[32m[20230205 18:26:43 @agent_ppo2.py:191][0m |          -0.0203 |          12.4953 |           3.2927 |
[32m[20230205 18:26:43 @agent_ppo2.py:191][0m |          -0.0219 |          12.3828 |           3.2952 |
[32m[20230205 18:26:43 @agent_ppo2.py:191][0m |           0.0145 |          15.3940 |           3.2906 |
[32m[20230205 18:26:43 @agent_ppo2.py:191][0m |          -0.0179 |          12.1666 |           3.2858 |
[32m[20230205 18:26:43 @agent_ppo2.py:191][0m |          -0.0146 |          11.9784 |           3.2880 |
[32m[20230205 18:26:43 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:43 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.91
[32m[20230205 18:26:43 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.06
[32m[20230205 18:26:43 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 184.25
[32m[20230205 18:26:43 @agent_ppo2.py:149][0m Total time:      13.27 min
[32m[20230205 18:26:43 @agent_ppo2.py:151][0m 911360 total steps have happened
[32m[20230205 18:26:43 @agent_ppo2.py:127][0m #------------------------ Iteration 445 --------------------------#
[32m[20230205 18:26:44 @agent_ppo2.py:133][0m Sampling time: 0.63 s by 1 slaves
[32m[20230205 18:26:44 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:44 @agent_ppo2.py:191][0m |           0.0003 |          31.9515 |           3.4053 |
[32m[20230205 18:26:44 @agent_ppo2.py:191][0m |          -0.0053 |          19.7250 |           3.3928 |
[32m[20230205 18:26:44 @agent_ppo2.py:191][0m |          -0.0082 |          16.4438 |           3.3898 |
[32m[20230205 18:26:44 @agent_ppo2.py:191][0m |          -0.0097 |          14.6351 |           3.3920 |
[32m[20230205 18:26:44 @agent_ppo2.py:191][0m |          -0.0116 |          13.7478 |           3.3885 |
[32m[20230205 18:26:44 @agent_ppo2.py:191][0m |          -0.0116 |          13.1546 |           3.3890 |
[32m[20230205 18:26:44 @agent_ppo2.py:191][0m |          -0.0131 |          12.7336 |           3.3885 |
[32m[20230205 18:26:44 @agent_ppo2.py:191][0m |          -0.0120 |          12.4417 |           3.3868 |
[32m[20230205 18:26:45 @agent_ppo2.py:191][0m |          -0.0134 |          12.0792 |           3.3872 |
[32m[20230205 18:26:45 @agent_ppo2.py:191][0m |          -0.0147 |          11.8373 |           3.3898 |
[32m[20230205 18:26:45 @agent_ppo2.py:136][0m Policy update time: 0.63 s
[32m[20230205 18:26:45 @agent_ppo2.py:144][0m Average TRAINING episode reward: 175.29
[32m[20230205 18:26:45 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.00
[32m[20230205 18:26:45 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 265.68
[32m[20230205 18:26:45 @agent_ppo2.py:149][0m Total time:      13.29 min
[32m[20230205 18:26:45 @agent_ppo2.py:151][0m 913408 total steps have happened
[32m[20230205 18:26:45 @agent_ppo2.py:127][0m #------------------------ Iteration 446 --------------------------#
[32m[20230205 18:26:46 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:26:46 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:46 @agent_ppo2.py:191][0m |          -0.0017 |          16.5557 |           3.3069 |
[32m[20230205 18:26:46 @agent_ppo2.py:191][0m |          -0.0062 |          13.9053 |           3.3056 |
[32m[20230205 18:26:46 @agent_ppo2.py:191][0m |          -0.0035 |          13.4034 |           3.3075 |
[32m[20230205 18:26:46 @agent_ppo2.py:191][0m |          -0.0169 |          13.0267 |           3.3076 |
[32m[20230205 18:26:46 @agent_ppo2.py:191][0m |          -0.0003 |          14.1931 |           3.3072 |
[32m[20230205 18:26:46 @agent_ppo2.py:191][0m |          -0.0034 |          13.3462 |           3.3071 |
[32m[20230205 18:26:46 @agent_ppo2.py:191][0m |          -0.0133 |          12.2948 |           3.3067 |
[32m[20230205 18:26:46 @agent_ppo2.py:191][0m |          -0.0117 |          12.1101 |           3.3079 |
[32m[20230205 18:26:46 @agent_ppo2.py:191][0m |          -0.0126 |          12.0180 |           3.3081 |
[32m[20230205 18:26:46 @agent_ppo2.py:191][0m |          -0.0169 |          11.8204 |           3.3094 |
[32m[20230205 18:26:46 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:26:47 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.42
[32m[20230205 18:26:47 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.55
[32m[20230205 18:26:47 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 270.39
[32m[20230205 18:26:47 @agent_ppo2.py:149][0m Total time:      13.32 min
[32m[20230205 18:26:47 @agent_ppo2.py:151][0m 915456 total steps have happened
[32m[20230205 18:26:47 @agent_ppo2.py:127][0m #------------------------ Iteration 447 --------------------------#
[32m[20230205 18:26:47 @agent_ppo2.py:133][0m Sampling time: 0.59 s by 1 slaves
[32m[20230205 18:26:47 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:47 @agent_ppo2.py:191][0m |          -0.0057 |          70.1876 |           3.3332 |
[32m[20230205 18:26:47 @agent_ppo2.py:191][0m |          -0.0135 |          44.3064 |           3.3269 |
[32m[20230205 18:26:47 @agent_ppo2.py:191][0m |          -0.0179 |          35.4789 |           3.3242 |
[32m[20230205 18:26:47 @agent_ppo2.py:191][0m |          -0.0171 |          32.5937 |           3.3209 |
[32m[20230205 18:26:47 @agent_ppo2.py:191][0m |          -0.0130 |          31.4042 |           3.3180 |
[32m[20230205 18:26:47 @agent_ppo2.py:191][0m |          -0.0214 |          28.7811 |           3.3176 |
[32m[20230205 18:26:48 @agent_ppo2.py:191][0m |          -0.0220 |          27.6706 |           3.3158 |
[32m[20230205 18:26:48 @agent_ppo2.py:191][0m |           0.0029 |          26.9870 |           3.3145 |
[32m[20230205 18:26:48 @agent_ppo2.py:191][0m |          -0.0255 |          25.8552 |           3.3140 |
[32m[20230205 18:26:48 @agent_ppo2.py:191][0m |          -0.0213 |          25.3173 |           3.3147 |
[32m[20230205 18:26:48 @agent_ppo2.py:136][0m Policy update time: 0.56 s
[32m[20230205 18:26:48 @agent_ppo2.py:144][0m Average TRAINING episode reward: 79.41
[32m[20230205 18:26:48 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.27
[32m[20230205 18:26:48 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 186.00
[32m[20230205 18:26:48 @agent_ppo2.py:149][0m Total time:      13.35 min
[32m[20230205 18:26:48 @agent_ppo2.py:151][0m 917504 total steps have happened
[32m[20230205 18:26:48 @agent_ppo2.py:127][0m #------------------------ Iteration 448 --------------------------#
[32m[20230205 18:26:49 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:26:49 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:49 @agent_ppo2.py:191][0m |          -0.0022 |          26.3697 |           3.4141 |
[32m[20230205 18:26:49 @agent_ppo2.py:191][0m |          -0.0059 |          19.7060 |           3.4118 |
[32m[20230205 18:26:49 @agent_ppo2.py:191][0m |          -0.0109 |          18.4630 |           3.4084 |
[32m[20230205 18:26:49 @agent_ppo2.py:191][0m |          -0.0120 |          18.0025 |           3.4068 |
[32m[20230205 18:26:49 @agent_ppo2.py:191][0m |          -0.0142 |          17.7019 |           3.4053 |
[32m[20230205 18:26:49 @agent_ppo2.py:191][0m |          -0.0197 |          17.4735 |           3.4066 |
[32m[20230205 18:26:49 @agent_ppo2.py:191][0m |          -0.0133 |          17.2268 |           3.4053 |
[32m[20230205 18:26:49 @agent_ppo2.py:191][0m |          -0.0136 |          17.0678 |           3.4043 |
[32m[20230205 18:26:49 @agent_ppo2.py:191][0m |          -0.0140 |          16.7155 |           3.4036 |
[32m[20230205 18:26:49 @agent_ppo2.py:191][0m |          -0.0122 |          16.6569 |           3.4042 |
[32m[20230205 18:26:49 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:50 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.82
[32m[20230205 18:26:50 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.68
[32m[20230205 18:26:50 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 269.45
[32m[20230205 18:26:50 @agent_ppo2.py:149][0m Total time:      13.37 min
[32m[20230205 18:26:50 @agent_ppo2.py:151][0m 919552 total steps have happened
[32m[20230205 18:26:50 @agent_ppo2.py:127][0m #------------------------ Iteration 449 --------------------------#
[32m[20230205 18:26:50 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:26:50 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:50 @agent_ppo2.py:191][0m |          -0.0030 |          15.5230 |           3.4000 |
[32m[20230205 18:26:50 @agent_ppo2.py:191][0m |          -0.0106 |          13.5035 |           3.3943 |
[32m[20230205 18:26:50 @agent_ppo2.py:191][0m |          -0.0127 |          12.9293 |           3.3907 |
[32m[20230205 18:26:50 @agent_ppo2.py:191][0m |          -0.0142 |          12.6571 |           3.3895 |
[32m[20230205 18:26:50 @agent_ppo2.py:191][0m |          -0.0146 |          12.4868 |           3.3886 |
[32m[20230205 18:26:50 @agent_ppo2.py:191][0m |          -0.0159 |          12.3534 |           3.3877 |
[32m[20230205 18:26:51 @agent_ppo2.py:191][0m |          -0.0160 |          12.2472 |           3.3875 |
[32m[20230205 18:26:51 @agent_ppo2.py:191][0m |          -0.0162 |          12.1489 |           3.3872 |
[32m[20230205 18:26:51 @agent_ppo2.py:191][0m |          -0.0170 |          12.0660 |           3.3853 |
[32m[20230205 18:26:51 @agent_ppo2.py:191][0m |          -0.0176 |          11.9968 |           3.3852 |
[32m[20230205 18:26:51 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:51 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.57
[32m[20230205 18:26:51 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.16
[32m[20230205 18:26:51 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 267.35
[32m[20230205 18:26:51 @agent_ppo2.py:149][0m Total time:      13.40 min
[32m[20230205 18:26:51 @agent_ppo2.py:151][0m 921600 total steps have happened
[32m[20230205 18:26:51 @agent_ppo2.py:127][0m #------------------------ Iteration 450 --------------------------#
[32m[20230205 18:26:52 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:26:52 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:52 @agent_ppo2.py:191][0m |          -0.0001 |          12.8989 |           3.3599 |
[32m[20230205 18:26:52 @agent_ppo2.py:191][0m |          -0.0085 |          12.0644 |           3.3553 |
[32m[20230205 18:26:52 @agent_ppo2.py:191][0m |          -0.0092 |          11.8259 |           3.3506 |
[32m[20230205 18:26:52 @agent_ppo2.py:191][0m |          -0.0127 |          11.4104 |           3.3507 |
[32m[20230205 18:26:52 @agent_ppo2.py:191][0m |          -0.0132 |          11.2473 |           3.3534 |
[32m[20230205 18:26:52 @agent_ppo2.py:191][0m |          -0.0137 |          11.0459 |           3.3539 |
[32m[20230205 18:26:52 @agent_ppo2.py:191][0m |          -0.0136 |          10.8648 |           3.3580 |
[32m[20230205 18:26:52 @agent_ppo2.py:191][0m |          -0.0143 |          10.7091 |           3.3545 |
[32m[20230205 18:26:52 @agent_ppo2.py:191][0m |          -0.0157 |          10.5127 |           3.3558 |
[32m[20230205 18:26:52 @agent_ppo2.py:191][0m |          -0.0152 |          10.5183 |           3.3538 |
[32m[20230205 18:26:52 @agent_ppo2.py:136][0m Policy update time: 0.53 s
[32m[20230205 18:26:53 @agent_ppo2.py:144][0m Average TRAINING episode reward: 255.80
[32m[20230205 18:26:53 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.96
[32m[20230205 18:26:53 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 268.81
[32m[20230205 18:26:53 @agent_ppo2.py:149][0m Total time:      13.42 min
[32m[20230205 18:26:53 @agent_ppo2.py:151][0m 923648 total steps have happened
[32m[20230205 18:26:53 @agent_ppo2.py:127][0m #------------------------ Iteration 451 --------------------------#
[32m[20230205 18:26:53 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:26:53 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:53 @agent_ppo2.py:191][0m |          -0.0021 |          13.3794 |           3.2771 |
[32m[20230205 18:26:53 @agent_ppo2.py:191][0m |          -0.0094 |          12.6313 |           3.2760 |
[32m[20230205 18:26:53 @agent_ppo2.py:191][0m |          -0.0051 |          12.9142 |           3.2709 |
[32m[20230205 18:26:53 @agent_ppo2.py:191][0m |          -0.0093 |          12.1676 |           3.2748 |
[32m[20230205 18:26:53 @agent_ppo2.py:191][0m |          -0.0069 |          12.2519 |           3.2663 |
[32m[20230205 18:26:54 @agent_ppo2.py:191][0m |          -0.0126 |          11.7384 |           3.2696 |
[32m[20230205 18:26:54 @agent_ppo2.py:191][0m |          -0.0142 |          11.5937 |           3.2679 |
[32m[20230205 18:26:54 @agent_ppo2.py:191][0m |          -0.0092 |          11.9181 |           3.2663 |
[32m[20230205 18:26:54 @agent_ppo2.py:191][0m |          -0.0168 |          11.3371 |           3.2664 |
[32m[20230205 18:26:54 @agent_ppo2.py:191][0m |          -0.0138 |          11.3770 |           3.2671 |
[32m[20230205 18:26:54 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:26:54 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.94
[32m[20230205 18:26:54 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.29
[32m[20230205 18:26:54 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.10
[32m[20230205 18:26:54 @agent_ppo2.py:149][0m Total time:      13.45 min
[32m[20230205 18:26:54 @agent_ppo2.py:151][0m 925696 total steps have happened
[32m[20230205 18:26:54 @agent_ppo2.py:127][0m #------------------------ Iteration 452 --------------------------#
[32m[20230205 18:26:55 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:26:55 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:55 @agent_ppo2.py:191][0m |           0.0009 |          13.7185 |           3.2795 |
[32m[20230205 18:26:55 @agent_ppo2.py:191][0m |          -0.0115 |          12.8441 |           3.2796 |
[32m[20230205 18:26:55 @agent_ppo2.py:191][0m |          -0.0145 |          12.5088 |           3.2794 |
[32m[20230205 18:26:55 @agent_ppo2.py:191][0m |          -0.0144 |          12.3671 |           3.2777 |
[32m[20230205 18:26:55 @agent_ppo2.py:191][0m |          -0.0151 |          12.1294 |           3.2781 |
[32m[20230205 18:26:55 @agent_ppo2.py:191][0m |          -0.0169 |          11.9066 |           3.2780 |
[32m[20230205 18:26:55 @agent_ppo2.py:191][0m |          -0.0177 |          11.8135 |           3.2771 |
[32m[20230205 18:26:55 @agent_ppo2.py:191][0m |          -0.0192 |          11.5829 |           3.2776 |
[32m[20230205 18:26:55 @agent_ppo2.py:191][0m |          -0.0187 |          11.5143 |           3.2793 |
[32m[20230205 18:26:55 @agent_ppo2.py:191][0m |          -0.0206 |          11.3444 |           3.2805 |
[32m[20230205 18:26:55 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:56 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.40
[32m[20230205 18:26:56 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.83
[32m[20230205 18:26:56 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.48
[32m[20230205 18:26:56 @agent_ppo2.py:149][0m Total time:      13.47 min
[32m[20230205 18:26:56 @agent_ppo2.py:151][0m 927744 total steps have happened
[32m[20230205 18:26:56 @agent_ppo2.py:127][0m #------------------------ Iteration 453 --------------------------#
[32m[20230205 18:26:56 @agent_ppo2.py:133][0m Sampling time: 0.47 s by 1 slaves
[32m[20230205 18:26:56 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:56 @agent_ppo2.py:191][0m |          -0.0015 |          27.8437 |           3.4025 |
[32m[20230205 18:26:56 @agent_ppo2.py:191][0m |          -0.0086 |          19.8467 |           3.3968 |
[32m[20230205 18:26:56 @agent_ppo2.py:191][0m |          -0.0118 |          17.6358 |           3.3963 |
[32m[20230205 18:26:56 @agent_ppo2.py:191][0m |          -0.0142 |          16.4365 |           3.3906 |
[32m[20230205 18:26:56 @agent_ppo2.py:191][0m |          -0.0157 |          15.5235 |           3.3959 |
[32m[20230205 18:26:56 @agent_ppo2.py:191][0m |          -0.0163 |          14.8655 |           3.3947 |
[32m[20230205 18:26:56 @agent_ppo2.py:191][0m |          -0.0172 |          14.5015 |           3.3953 |
[32m[20230205 18:26:56 @agent_ppo2.py:191][0m |          -0.0175 |          13.8282 |           3.3965 |
[32m[20230205 18:26:57 @agent_ppo2.py:191][0m |          -0.0183 |          13.2129 |           3.3924 |
[32m[20230205 18:26:57 @agent_ppo2.py:191][0m |          -0.0181 |          12.9553 |           3.3967 |
[32m[20230205 18:26:57 @agent_ppo2.py:136][0m Policy update time: 0.46 s
[32m[20230205 18:26:57 @agent_ppo2.py:144][0m Average TRAINING episode reward: 173.92
[32m[20230205 18:26:57 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 254.70
[32m[20230205 18:26:57 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 180.44
[32m[20230205 18:26:57 @agent_ppo2.py:149][0m Total time:      13.50 min
[32m[20230205 18:26:57 @agent_ppo2.py:151][0m 929792 total steps have happened
[32m[20230205 18:26:57 @agent_ppo2.py:127][0m #------------------------ Iteration 454 --------------------------#
[32m[20230205 18:26:58 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:26:58 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:58 @agent_ppo2.py:191][0m |          -0.0002 |          14.8445 |           3.3503 |
[32m[20230205 18:26:58 @agent_ppo2.py:191][0m |          -0.0052 |          12.7284 |           3.3483 |
[32m[20230205 18:26:58 @agent_ppo2.py:191][0m |          -0.0061 |          11.2453 |           3.3457 |
[32m[20230205 18:26:58 @agent_ppo2.py:191][0m |          -0.0093 |          10.8485 |           3.3474 |
[32m[20230205 18:26:58 @agent_ppo2.py:191][0m |          -0.0110 |          10.5148 |           3.3486 |
[32m[20230205 18:26:58 @agent_ppo2.py:191][0m |          -0.0128 |          10.4314 |           3.3506 |
[32m[20230205 18:26:58 @agent_ppo2.py:191][0m |          -0.0123 |          10.2956 |           3.3487 |
[32m[20230205 18:26:58 @agent_ppo2.py:191][0m |          -0.0119 |          10.0167 |           3.3479 |
[32m[20230205 18:26:58 @agent_ppo2.py:191][0m |          -0.0149 |           9.8695 |           3.3504 |
[32m[20230205 18:26:58 @agent_ppo2.py:191][0m |          -0.0125 |           9.7704 |           3.3498 |
[32m[20230205 18:26:58 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:26:59 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.50
[32m[20230205 18:26:59 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 265.62
[32m[20230205 18:26:59 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 145.49
[32m[20230205 18:26:59 @agent_ppo2.py:149][0m Total time:      13.52 min
[32m[20230205 18:26:59 @agent_ppo2.py:151][0m 931840 total steps have happened
[32m[20230205 18:26:59 @agent_ppo2.py:127][0m #------------------------ Iteration 455 --------------------------#
[32m[20230205 18:26:59 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:26:59 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:26:59 @agent_ppo2.py:191][0m |           0.0207 |          21.5855 |           3.3405 |
[32m[20230205 18:26:59 @agent_ppo2.py:191][0m |          -0.0130 |          15.5071 |           3.3267 |
[32m[20230205 18:26:59 @agent_ppo2.py:191][0m |          -0.0090 |          14.5487 |           3.3304 |
[32m[20230205 18:26:59 @agent_ppo2.py:191][0m |          -0.0094 |          14.1518 |           3.3334 |
[32m[20230205 18:27:00 @agent_ppo2.py:191][0m |          -0.0087 |          13.8246 |           3.3310 |
[32m[20230205 18:27:00 @agent_ppo2.py:191][0m |           0.0575 |          22.9412 |           3.3346 |
[32m[20230205 18:27:00 @agent_ppo2.py:191][0m |          -0.0047 |          13.3608 |           3.3275 |
[32m[20230205 18:27:00 @agent_ppo2.py:191][0m |          -0.0079 |          13.0285 |           3.3318 |
[32m[20230205 18:27:00 @agent_ppo2.py:191][0m |          -0.0150 |          12.9292 |           3.3335 |
[32m[20230205 18:27:00 @agent_ppo2.py:191][0m |           0.0038 |          13.0167 |           3.3287 |
[32m[20230205 18:27:00 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:27:00 @agent_ppo2.py:144][0m Average TRAINING episode reward: 255.99
[32m[20230205 18:27:00 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.46
[32m[20230205 18:27:00 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.51
[32m[20230205 18:27:00 @agent_ppo2.py:149][0m Total time:      13.55 min
[32m[20230205 18:27:00 @agent_ppo2.py:151][0m 933888 total steps have happened
[32m[20230205 18:27:00 @agent_ppo2.py:127][0m #------------------------ Iteration 456 --------------------------#
[32m[20230205 18:27:01 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:01 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:01 @agent_ppo2.py:191][0m |           0.0033 |          13.5185 |           3.4330 |
[32m[20230205 18:27:01 @agent_ppo2.py:191][0m |          -0.0060 |          12.9379 |           3.4292 |
[32m[20230205 18:27:01 @agent_ppo2.py:191][0m |          -0.0065 |          12.7665 |           3.4249 |
[32m[20230205 18:27:01 @agent_ppo2.py:191][0m |           0.0009 |          13.6624 |           3.4281 |
[32m[20230205 18:27:01 @agent_ppo2.py:191][0m |          -0.0020 |          13.2096 |           3.4299 |
[32m[20230205 18:27:01 @agent_ppo2.py:191][0m |          -0.0153 |          12.1794 |           3.4275 |
[32m[20230205 18:27:01 @agent_ppo2.py:191][0m |          -0.0127 |          12.0651 |           3.4267 |
[32m[20230205 18:27:01 @agent_ppo2.py:191][0m |          -0.0132 |          11.9363 |           3.4258 |
[32m[20230205 18:27:01 @agent_ppo2.py:191][0m |          -0.0157 |          11.8675 |           3.4268 |
[32m[20230205 18:27:01 @agent_ppo2.py:191][0m |          -0.0144 |          11.7886 |           3.4264 |
[32m[20230205 18:27:01 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:02 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.56
[32m[20230205 18:27:02 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.07
[32m[20230205 18:27:02 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 211.04
[32m[20230205 18:27:02 @agent_ppo2.py:149][0m Total time:      13.58 min
[32m[20230205 18:27:02 @agent_ppo2.py:151][0m 935936 total steps have happened
[32m[20230205 18:27:02 @agent_ppo2.py:127][0m #------------------------ Iteration 457 --------------------------#
[32m[20230205 18:27:02 @agent_ppo2.py:133][0m Sampling time: 0.46 s by 1 slaves
[32m[20230205 18:27:02 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:02 @agent_ppo2.py:191][0m |          -0.0030 |          63.5855 |           3.3354 |
[32m[20230205 18:27:02 @agent_ppo2.py:191][0m |          -0.0094 |          29.1716 |           3.3330 |
[32m[20230205 18:27:03 @agent_ppo2.py:191][0m |          -0.0143 |          21.8968 |           3.3302 |
[32m[20230205 18:27:03 @agent_ppo2.py:191][0m |          -0.0136 |          19.5260 |           3.3294 |
[32m[20230205 18:27:03 @agent_ppo2.py:191][0m |          -0.0169 |          18.1053 |           3.3257 |
[32m[20230205 18:27:03 @agent_ppo2.py:191][0m |          -0.0183 |          16.8824 |           3.3232 |
[32m[20230205 18:27:03 @agent_ppo2.py:191][0m |          -0.0206 |          15.9740 |           3.3231 |
[32m[20230205 18:27:03 @agent_ppo2.py:191][0m |          -0.0163 |          16.1641 |           3.3205 |
[32m[20230205 18:27:03 @agent_ppo2.py:191][0m |          -0.0213 |          14.7353 |           3.3199 |
[32m[20230205 18:27:03 @agent_ppo2.py:191][0m |          -0.0116 |          14.1750 |           3.3198 |
[32m[20230205 18:27:03 @agent_ppo2.py:136][0m Policy update time: 0.47 s
[32m[20230205 18:27:03 @agent_ppo2.py:144][0m Average TRAINING episode reward: 122.24
[32m[20230205 18:27:03 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 148.57
[32m[20230205 18:27:03 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 213.90
[32m[20230205 18:27:03 @agent_ppo2.py:149][0m Total time:      13.60 min
[32m[20230205 18:27:03 @agent_ppo2.py:151][0m 937984 total steps have happened
[32m[20230205 18:27:03 @agent_ppo2.py:127][0m #------------------------ Iteration 458 --------------------------#
[32m[20230205 18:27:04 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:04 @agent_ppo2.py:191][0m |          -0.0000 |          18.3169 |           3.4094 |
[32m[20230205 18:27:04 @agent_ppo2.py:191][0m |          -0.0054 |          15.1713 |           3.4054 |
[32m[20230205 18:27:04 @agent_ppo2.py:191][0m |          -0.0114 |          14.3368 |           3.4001 |
[32m[20230205 18:27:04 @agent_ppo2.py:191][0m |          -0.0113 |          14.1674 |           3.3982 |
[32m[20230205 18:27:04 @agent_ppo2.py:191][0m |          -0.0127 |          13.7421 |           3.3968 |
[32m[20230205 18:27:04 @agent_ppo2.py:191][0m |          -0.0157 |          13.4808 |           3.3944 |
[32m[20230205 18:27:04 @agent_ppo2.py:191][0m |          -0.0132 |          13.3228 |           3.3923 |
[32m[20230205 18:27:04 @agent_ppo2.py:191][0m |          -0.0141 |          13.1308 |           3.3936 |
[32m[20230205 18:27:04 @agent_ppo2.py:191][0m |          -0.0159 |          13.0074 |           3.3924 |
[32m[20230205 18:27:04 @agent_ppo2.py:191][0m |          -0.0184 |          12.8107 |           3.3937 |
[32m[20230205 18:27:04 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:05 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.89
[32m[20230205 18:27:05 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.77
[32m[20230205 18:27:05 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 181.07
[32m[20230205 18:27:05 @agent_ppo2.py:149][0m Total time:      13.62 min
[32m[20230205 18:27:05 @agent_ppo2.py:151][0m 940032 total steps have happened
[32m[20230205 18:27:05 @agent_ppo2.py:127][0m #------------------------ Iteration 459 --------------------------#
[32m[20230205 18:27:05 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:05 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:05 @agent_ppo2.py:191][0m |           0.0065 |          14.4738 |           3.3470 |
[32m[20230205 18:27:05 @agent_ppo2.py:191][0m |          -0.0066 |          12.7386 |           3.3359 |
[32m[20230205 18:27:06 @agent_ppo2.py:191][0m |          -0.0055 |          12.5824 |           3.3335 |
[32m[20230205 18:27:06 @agent_ppo2.py:191][0m |          -0.0093 |          11.8657 |           3.3293 |
[32m[20230205 18:27:06 @agent_ppo2.py:191][0m |          -0.0096 |          11.7768 |           3.3295 |
[32m[20230205 18:27:06 @agent_ppo2.py:191][0m |          -0.0109 |          11.6372 |           3.3295 |
[32m[20230205 18:27:06 @agent_ppo2.py:191][0m |          -0.0147 |          11.2103 |           3.3269 |
[32m[20230205 18:27:06 @agent_ppo2.py:191][0m |          -0.0152 |          11.0649 |           3.3271 |
[32m[20230205 18:27:06 @agent_ppo2.py:191][0m |          -0.0155 |          10.9617 |           3.3257 |
[32m[20230205 18:27:06 @agent_ppo2.py:191][0m |          -0.0180 |          10.8014 |           3.3255 |
[32m[20230205 18:27:06 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:06 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.56
[32m[20230205 18:27:06 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.30
[32m[20230205 18:27:06 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.38
[32m[20230205 18:27:06 @agent_ppo2.py:149][0m Total time:      13.65 min
[32m[20230205 18:27:06 @agent_ppo2.py:151][0m 942080 total steps have happened
[32m[20230205 18:27:06 @agent_ppo2.py:127][0m #------------------------ Iteration 460 --------------------------#
[32m[20230205 18:27:07 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:07 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:07 @agent_ppo2.py:191][0m |          -0.0070 |          13.2656 |           3.3641 |
[32m[20230205 18:27:07 @agent_ppo2.py:191][0m |          -0.0132 |          12.7774 |           3.3540 |
[32m[20230205 18:27:07 @agent_ppo2.py:191][0m |          -0.0115 |          12.3664 |           3.3554 |
[32m[20230205 18:27:07 @agent_ppo2.py:191][0m |          -0.0177 |          12.1167 |           3.3521 |
[32m[20230205 18:27:07 @agent_ppo2.py:191][0m |           0.0172 |          13.0323 |           3.3550 |
[32m[20230205 18:27:07 @agent_ppo2.py:191][0m |          -0.0115 |          11.6443 |           3.3494 |
[32m[20230205 18:27:07 @agent_ppo2.py:191][0m |          -0.0121 |          11.4372 |           3.3500 |
[32m[20230205 18:27:07 @agent_ppo2.py:191][0m |           0.0488 |          13.2937 |           3.3515 |
[32m[20230205 18:27:07 @agent_ppo2.py:191][0m |           0.0390 |          13.8761 |           3.3424 |
[32m[20230205 18:27:07 @agent_ppo2.py:191][0m |          -0.0057 |          11.0268 |           3.3269 |
[32m[20230205 18:27:07 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:08 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.34
[32m[20230205 18:27:08 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.68
[32m[20230205 18:27:08 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.29
[32m[20230205 18:27:08 @agent_ppo2.py:149][0m Total time:      13.67 min
[32m[20230205 18:27:08 @agent_ppo2.py:151][0m 944128 total steps have happened
[32m[20230205 18:27:08 @agent_ppo2.py:127][0m #------------------------ Iteration 461 --------------------------#
[32m[20230205 18:27:08 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:27:08 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:08 @agent_ppo2.py:191][0m |           0.0003 |          13.6465 |           3.3813 |
[32m[20230205 18:27:08 @agent_ppo2.py:191][0m |          -0.0071 |          13.0392 |           3.3781 |
[32m[20230205 18:27:08 @agent_ppo2.py:191][0m |          -0.0104 |          12.6792 |           3.3737 |
[32m[20230205 18:27:09 @agent_ppo2.py:191][0m |          -0.0136 |          12.2750 |           3.3725 |
[32m[20230205 18:27:09 @agent_ppo2.py:191][0m |          -0.0139 |          11.9895 |           3.3722 |
[32m[20230205 18:27:09 @agent_ppo2.py:191][0m |          -0.0152 |          11.7209 |           3.3688 |
[32m[20230205 18:27:09 @agent_ppo2.py:191][0m |          -0.0153 |          11.4794 |           3.3690 |
[32m[20230205 18:27:09 @agent_ppo2.py:191][0m |          -0.0158 |          11.3167 |           3.3683 |
[32m[20230205 18:27:09 @agent_ppo2.py:191][0m |          -0.0162 |          11.2143 |           3.3669 |
[32m[20230205 18:27:09 @agent_ppo2.py:191][0m |          -0.0157 |          11.1286 |           3.3661 |
[32m[20230205 18:27:09 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:09 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.35
[32m[20230205 18:27:09 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.28
[32m[20230205 18:27:09 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 207.83
[32m[20230205 18:27:09 @agent_ppo2.py:149][0m Total time:      13.70 min
[32m[20230205 18:27:09 @agent_ppo2.py:151][0m 946176 total steps have happened
[32m[20230205 18:27:09 @agent_ppo2.py:127][0m #------------------------ Iteration 462 --------------------------#
[32m[20230205 18:27:10 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:27:10 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:10 @agent_ppo2.py:191][0m |           0.0014 |          15.1264 |           3.4058 |
[32m[20230205 18:27:10 @agent_ppo2.py:191][0m |          -0.0018 |          14.0482 |           3.4029 |
[32m[20230205 18:27:10 @agent_ppo2.py:191][0m |          -0.0059 |          13.2296 |           3.4018 |
[32m[20230205 18:27:10 @agent_ppo2.py:191][0m |          -0.0092 |          12.9105 |           3.4022 |
[32m[20230205 18:27:10 @agent_ppo2.py:191][0m |          -0.0081 |          12.7642 |           3.4034 |
[32m[20230205 18:27:10 @agent_ppo2.py:191][0m |          -0.0121 |          12.4434 |           3.4022 |
[32m[20230205 18:27:10 @agent_ppo2.py:191][0m |          -0.0133 |          12.2399 |           3.4002 |
[32m[20230205 18:27:10 @agent_ppo2.py:191][0m |          -0.0125 |          12.1317 |           3.4019 |
[32m[20230205 18:27:10 @agent_ppo2.py:191][0m |          -0.0146 |          11.9404 |           3.4018 |
[32m[20230205 18:27:10 @agent_ppo2.py:191][0m |          -0.0127 |          11.9022 |           3.4023 |
[32m[20230205 18:27:10 @agent_ppo2.py:136][0m Policy update time: 0.49 s
[32m[20230205 18:27:11 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.13
[32m[20230205 18:27:11 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.53
[32m[20230205 18:27:11 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 86.32
[32m[20230205 18:27:11 @agent_ppo2.py:149][0m Total time:      13.73 min
[32m[20230205 18:27:11 @agent_ppo2.py:151][0m 948224 total steps have happened
[32m[20230205 18:27:11 @agent_ppo2.py:127][0m #------------------------ Iteration 463 --------------------------#
[32m[20230205 18:27:12 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:12 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:12 @agent_ppo2.py:191][0m |          -0.0009 |          12.8922 |           3.4222 |
[32m[20230205 18:27:12 @agent_ppo2.py:191][0m |          -0.0085 |          12.5118 |           3.4121 |
[32m[20230205 18:27:12 @agent_ppo2.py:191][0m |          -0.0113 |          12.2810 |           3.4119 |
[32m[20230205 18:27:12 @agent_ppo2.py:191][0m |          -0.0103 |          12.0628 |           3.4097 |
[32m[20230205 18:27:12 @agent_ppo2.py:191][0m |          -0.0138 |          11.9259 |           3.4135 |
[32m[20230205 18:27:12 @agent_ppo2.py:191][0m |          -0.0158 |          11.7636 |           3.4103 |
[32m[20230205 18:27:12 @agent_ppo2.py:191][0m |          -0.0117 |          11.6759 |           3.4090 |
[32m[20230205 18:27:12 @agent_ppo2.py:191][0m |          -0.0173 |          11.5356 |           3.4094 |
[32m[20230205 18:27:12 @agent_ppo2.py:191][0m |          -0.0113 |          11.6269 |           3.4097 |
[32m[20230205 18:27:12 @agent_ppo2.py:191][0m |          -0.0165 |          11.2887 |           3.4089 |
[32m[20230205 18:27:12 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:13 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.84
[32m[20230205 18:27:13 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.97
[32m[20230205 18:27:13 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.09
[32m[20230205 18:27:13 @agent_ppo2.py:149][0m Total time:      13.75 min
[32m[20230205 18:27:13 @agent_ppo2.py:151][0m 950272 total steps have happened
[32m[20230205 18:27:13 @agent_ppo2.py:127][0m #------------------------ Iteration 464 --------------------------#
[32m[20230205 18:27:13 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:27:13 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:13 @agent_ppo2.py:191][0m |           0.0018 |          12.3419 |           3.3342 |
[32m[20230205 18:27:13 @agent_ppo2.py:191][0m |          -0.0071 |          11.2087 |           3.3240 |
[32m[20230205 18:27:13 @agent_ppo2.py:191][0m |          -0.0119 |          10.1655 |           3.3189 |
[32m[20230205 18:27:13 @agent_ppo2.py:191][0m |          -0.0139 |           9.3580 |           3.3179 |
[32m[20230205 18:27:13 @agent_ppo2.py:191][0m |          -0.0170 |           8.6884 |           3.3161 |
[32m[20230205 18:27:13 @agent_ppo2.py:191][0m |          -0.0156 |           8.5397 |           3.3142 |
[32m[20230205 18:27:13 @agent_ppo2.py:191][0m |          -0.0179 |           8.0798 |           3.3141 |
[32m[20230205 18:27:13 @agent_ppo2.py:191][0m |          -0.0169 |           7.9208 |           3.3162 |
[32m[20230205 18:27:14 @agent_ppo2.py:191][0m |          -0.0208 |           7.4969 |           3.3141 |
[32m[20230205 18:27:14 @agent_ppo2.py:191][0m |          -0.0192 |           7.2752 |           3.3162 |
[32m[20230205 18:27:14 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:27:14 @agent_ppo2.py:144][0m Average TRAINING episode reward: 257.41
[32m[20230205 18:27:14 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.56
[32m[20230205 18:27:14 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 203.06
[32m[20230205 18:27:14 @agent_ppo2.py:149][0m Total time:      13.78 min
[32m[20230205 18:27:14 @agent_ppo2.py:151][0m 952320 total steps have happened
[32m[20230205 18:27:14 @agent_ppo2.py:127][0m #------------------------ Iteration 465 --------------------------#
[32m[20230205 18:27:15 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:15 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:15 @agent_ppo2.py:191][0m |          -0.0017 |          15.0071 |           3.4543 |
[32m[20230205 18:27:15 @agent_ppo2.py:191][0m |          -0.0079 |          14.1355 |           3.4477 |
[32m[20230205 18:27:15 @agent_ppo2.py:191][0m |          -0.0075 |          14.1573 |           3.4477 |
[32m[20230205 18:27:15 @agent_ppo2.py:191][0m |          -0.0132 |          13.4751 |           3.4463 |
[32m[20230205 18:27:15 @agent_ppo2.py:191][0m |          -0.0052 |          13.9153 |           3.4454 |
[32m[20230205 18:27:15 @agent_ppo2.py:191][0m |          -0.0086 |          13.5424 |           3.4471 |
[32m[20230205 18:27:15 @agent_ppo2.py:191][0m |          -0.0131 |          13.1267 |           3.4483 |
[32m[20230205 18:27:15 @agent_ppo2.py:191][0m |          -0.0144 |          12.7234 |           3.4465 |
[32m[20230205 18:27:15 @agent_ppo2.py:191][0m |          -0.0137 |          12.8102 |           3.4494 |
[32m[20230205 18:27:15 @agent_ppo2.py:191][0m |          -0.0183 |          12.4118 |           3.4509 |
[32m[20230205 18:27:15 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:16 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.57
[32m[20230205 18:27:16 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.94
[32m[20230205 18:27:16 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 271.59
[32m[20230205 18:27:16 @agent_ppo2.py:149][0m Total time:      13.81 min
[32m[20230205 18:27:16 @agent_ppo2.py:151][0m 954368 total steps have happened
[32m[20230205 18:27:16 @agent_ppo2.py:127][0m #------------------------ Iteration 466 --------------------------#
[32m[20230205 18:27:16 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:27:16 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:16 @agent_ppo2.py:191][0m |           0.0001 |          13.8294 |           3.3823 |
[32m[20230205 18:27:16 @agent_ppo2.py:191][0m |          -0.0052 |          12.6469 |           3.3697 |
[32m[20230205 18:27:16 @agent_ppo2.py:191][0m |          -0.0082 |          12.0549 |           3.3699 |
[32m[20230205 18:27:16 @agent_ppo2.py:191][0m |          -0.0098 |          11.6246 |           3.3689 |
[32m[20230205 18:27:17 @agent_ppo2.py:191][0m |          -0.0079 |          11.4205 |           3.3653 |
[32m[20230205 18:27:17 @agent_ppo2.py:191][0m |          -0.0107 |          11.1212 |           3.3669 |
[32m[20230205 18:27:17 @agent_ppo2.py:191][0m |          -0.0168 |          10.8175 |           3.3634 |
[32m[20230205 18:27:17 @agent_ppo2.py:191][0m |          -0.0131 |          10.6238 |           3.3618 |
[32m[20230205 18:27:17 @agent_ppo2.py:191][0m |          -0.0154 |          10.4811 |           3.3630 |
[32m[20230205 18:27:17 @agent_ppo2.py:191][0m |          -0.0144 |          10.3067 |           3.3657 |
[32m[20230205 18:27:17 @agent_ppo2.py:136][0m Policy update time: 0.55 s
[32m[20230205 18:27:17 @agent_ppo2.py:144][0m Average TRAINING episode reward: 253.34
[32m[20230205 18:27:17 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.82
[32m[20230205 18:27:17 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 179.32
[32m[20230205 18:27:17 @agent_ppo2.py:149][0m Total time:      13.83 min
[32m[20230205 18:27:17 @agent_ppo2.py:151][0m 956416 total steps have happened
[32m[20230205 18:27:17 @agent_ppo2.py:127][0m #------------------------ Iteration 467 --------------------------#
[32m[20230205 18:27:18 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:27:18 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:18 @agent_ppo2.py:191][0m |          -0.0025 |          13.8914 |           3.4750 |
[32m[20230205 18:27:18 @agent_ppo2.py:191][0m |          -0.0124 |          13.1598 |           3.4633 |
[32m[20230205 18:27:18 @agent_ppo2.py:191][0m |          -0.0153 |          12.7794 |           3.4566 |
[32m[20230205 18:27:18 @agent_ppo2.py:191][0m |          -0.0167 |          12.5391 |           3.4541 |
[32m[20230205 18:27:18 @agent_ppo2.py:191][0m |          -0.0175 |          12.3488 |           3.4519 |
[32m[20230205 18:27:18 @agent_ppo2.py:191][0m |          -0.0181 |          12.1931 |           3.4518 |
[32m[20230205 18:27:18 @agent_ppo2.py:191][0m |          -0.0190 |          12.0340 |           3.4498 |
[32m[20230205 18:27:18 @agent_ppo2.py:191][0m |          -0.0196 |          11.8996 |           3.4499 |
[32m[20230205 18:27:18 @agent_ppo2.py:191][0m |          -0.0199 |          11.7710 |           3.4475 |
[32m[20230205 18:27:18 @agent_ppo2.py:191][0m |          -0.0204 |          11.6653 |           3.4489 |
[32m[20230205 18:27:18 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:19 @agent_ppo2.py:144][0m Average TRAINING episode reward: 258.84
[32m[20230205 18:27:19 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.67
[32m[20230205 18:27:19 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 228.52
[32m[20230205 18:27:19 @agent_ppo2.py:149][0m Total time:      13.86 min
[32m[20230205 18:27:19 @agent_ppo2.py:151][0m 958464 total steps have happened
[32m[20230205 18:27:19 @agent_ppo2.py:127][0m #------------------------ Iteration 468 --------------------------#
[32m[20230205 18:27:20 @agent_ppo2.py:133][0m Sampling time: 0.58 s by 1 slaves
[32m[20230205 18:27:20 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:20 @agent_ppo2.py:191][0m |          -0.0072 |          21.6133 |           3.4073 |
[32m[20230205 18:27:20 @agent_ppo2.py:191][0m |          -0.0120 |          12.6757 |           3.4138 |
[32m[20230205 18:27:20 @agent_ppo2.py:191][0m |          -0.0046 |          12.3493 |           3.4144 |
[32m[20230205 18:27:20 @agent_ppo2.py:191][0m |          -0.0121 |          11.0977 |           3.4170 |
[32m[20230205 18:27:20 @agent_ppo2.py:191][0m |          -0.0135 |          10.6991 |           3.4136 |
[32m[20230205 18:27:20 @agent_ppo2.py:191][0m |          -0.0151 |           9.8614 |           3.4146 |
[32m[20230205 18:27:20 @agent_ppo2.py:191][0m |          -0.0189 |           9.6500 |           3.4133 |
[32m[20230205 18:27:20 @agent_ppo2.py:191][0m |          -0.0224 |           9.3399 |           3.4141 |
[32m[20230205 18:27:20 @agent_ppo2.py:191][0m |          -0.0206 |           9.1361 |           3.4134 |
[32m[20230205 18:27:20 @agent_ppo2.py:191][0m |          -0.0227 |           8.9002 |           3.4131 |
[32m[20230205 18:27:20 @agent_ppo2.py:136][0m Policy update time: 0.58 s
[32m[20230205 18:27:21 @agent_ppo2.py:144][0m Average TRAINING episode reward: 168.30
[32m[20230205 18:27:21 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 264.66
[32m[20230205 18:27:21 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 188.51
[32m[20230205 18:27:21 @agent_ppo2.py:149][0m Total time:      13.89 min
[32m[20230205 18:27:21 @agent_ppo2.py:151][0m 960512 total steps have happened
[32m[20230205 18:27:21 @agent_ppo2.py:127][0m #------------------------ Iteration 469 --------------------------#
[32m[20230205 18:27:21 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:27:21 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:21 @agent_ppo2.py:191][0m |           0.0001 |          13.6534 |           3.5162 |
[32m[20230205 18:27:21 @agent_ppo2.py:191][0m |          -0.0080 |          12.7977 |           3.5073 |
[32m[20230205 18:27:21 @agent_ppo2.py:191][0m |          -0.0098 |          12.6332 |           3.5027 |
[32m[20230205 18:27:22 @agent_ppo2.py:191][0m |          -0.0127 |          12.3544 |           3.5015 |
[32m[20230205 18:27:22 @agent_ppo2.py:191][0m |          -0.0138 |          12.1943 |           3.4966 |
[32m[20230205 18:27:22 @agent_ppo2.py:191][0m |          -0.0151 |          12.1039 |           3.4928 |
[32m[20230205 18:27:22 @agent_ppo2.py:191][0m |          -0.0154 |          11.9862 |           3.4930 |
[32m[20230205 18:27:22 @agent_ppo2.py:191][0m |          -0.0153 |          11.8548 |           3.4898 |
[32m[20230205 18:27:22 @agent_ppo2.py:191][0m |          -0.0160 |          11.7742 |           3.4920 |
[32m[20230205 18:27:22 @agent_ppo2.py:191][0m |          -0.0150 |          11.7647 |           3.4879 |
[32m[20230205 18:27:22 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:27:22 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.67
[32m[20230205 18:27:22 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.69
[32m[20230205 18:27:22 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 274.08
[32m[20230205 18:27:22 @agent_ppo2.py:149][0m Total time:      13.91 min
[32m[20230205 18:27:22 @agent_ppo2.py:151][0m 962560 total steps have happened
[32m[20230205 18:27:22 @agent_ppo2.py:127][0m #------------------------ Iteration 470 --------------------------#
[32m[20230205 18:27:23 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:23 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:23 @agent_ppo2.py:191][0m |          -0.0003 |          12.9705 |           3.3523 |
[32m[20230205 18:27:23 @agent_ppo2.py:191][0m |          -0.0063 |          11.7796 |           3.3453 |
[32m[20230205 18:27:23 @agent_ppo2.py:191][0m |          -0.0089 |          11.0577 |           3.3401 |
[32m[20230205 18:27:23 @agent_ppo2.py:191][0m |          -0.0098 |          10.5417 |           3.3398 |
[32m[20230205 18:27:23 @agent_ppo2.py:191][0m |          -0.0113 |          10.0595 |           3.3418 |
[32m[20230205 18:27:23 @agent_ppo2.py:191][0m |          -0.0122 |           9.5614 |           3.3394 |
[32m[20230205 18:27:23 @agent_ppo2.py:191][0m |          -0.0122 |           9.1255 |           3.3379 |
[32m[20230205 18:27:23 @agent_ppo2.py:191][0m |          -0.0132 |           8.8460 |           3.3395 |
[32m[20230205 18:27:23 @agent_ppo2.py:191][0m |          -0.0140 |           8.5268 |           3.3379 |
[32m[20230205 18:27:23 @agent_ppo2.py:191][0m |          -0.0135 |           8.3123 |           3.3427 |
[32m[20230205 18:27:23 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:27:24 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.85
[32m[20230205 18:27:24 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.77
[32m[20230205 18:27:24 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 277.55
[32m[20230205 18:27:24 @agent_ppo2.py:149][0m Total time:      13.94 min
[32m[20230205 18:27:24 @agent_ppo2.py:151][0m 964608 total steps have happened
[32m[20230205 18:27:24 @agent_ppo2.py:127][0m #------------------------ Iteration 471 --------------------------#
[32m[20230205 18:27:24 @agent_ppo2.py:133][0m Sampling time: 0.70 s by 1 slaves
[32m[20230205 18:27:24 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:24 @agent_ppo2.py:191][0m |          -0.0000 |          21.2338 |           3.3595 |
[32m[20230205 18:27:25 @agent_ppo2.py:191][0m |           0.0012 |          12.2186 |           3.3580 |
[32m[20230205 18:27:25 @agent_ppo2.py:191][0m |          -0.0067 |          11.0750 |           3.3556 |
[32m[20230205 18:27:25 @agent_ppo2.py:191][0m |          -0.0065 |          10.5869 |           3.3525 |
[32m[20230205 18:27:25 @agent_ppo2.py:191][0m |          -0.0066 |          10.6052 |           3.3516 |
[32m[20230205 18:27:25 @agent_ppo2.py:191][0m |          -0.0098 |           9.8663 |           3.3502 |
[32m[20230205 18:27:25 @agent_ppo2.py:191][0m |          -0.0052 |           9.5520 |           3.3471 |
[32m[20230205 18:27:25 @agent_ppo2.py:191][0m |          -0.0099 |           9.3123 |           3.3470 |
[32m[20230205 18:27:25 @agent_ppo2.py:191][0m |          -0.0085 |           9.2938 |           3.3466 |
[32m[20230205 18:27:25 @agent_ppo2.py:191][0m |          -0.0111 |           9.1520 |           3.3464 |
[32m[20230205 18:27:25 @agent_ppo2.py:136][0m Policy update time: 0.70 s
[32m[20230205 18:27:25 @agent_ppo2.py:144][0m Average TRAINING episode reward: 197.80
[32m[20230205 18:27:25 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.96
[32m[20230205 18:27:25 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 276.89
[32m[20230205 18:27:25 @agent_ppo2.py:149][0m Total time:      13.97 min
[32m[20230205 18:27:25 @agent_ppo2.py:151][0m 966656 total steps have happened
[32m[20230205 18:27:25 @agent_ppo2.py:127][0m #------------------------ Iteration 472 --------------------------#
[32m[20230205 18:27:26 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:27:26 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:26 @agent_ppo2.py:191][0m |          -0.0007 |          14.2206 |           3.4570 |
[32m[20230205 18:27:26 @agent_ppo2.py:191][0m |          -0.0025 |          13.6966 |           3.4566 |
[32m[20230205 18:27:26 @agent_ppo2.py:191][0m |          -0.0083 |          13.0700 |           3.4565 |
[32m[20230205 18:27:26 @agent_ppo2.py:191][0m |          -0.0083 |          12.8969 |           3.4557 |
[32m[20230205 18:27:26 @agent_ppo2.py:191][0m |          -0.0091 |          12.8824 |           3.4568 |
[32m[20230205 18:27:26 @agent_ppo2.py:191][0m |          -0.0116 |          12.4304 |           3.4591 |
[32m[20230205 18:27:26 @agent_ppo2.py:191][0m |          -0.0129 |          12.2287 |           3.4589 |
[32m[20230205 18:27:26 @agent_ppo2.py:191][0m |          -0.0114 |          12.3260 |           3.4601 |
[32m[20230205 18:27:26 @agent_ppo2.py:191][0m |          -0.0133 |          11.9261 |           3.4601 |
[32m[20230205 18:27:27 @agent_ppo2.py:191][0m |          -0.0149 |          11.6787 |           3.4618 |
[32m[20230205 18:27:27 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:27 @agent_ppo2.py:144][0m Average TRAINING episode reward: 259.97
[32m[20230205 18:27:27 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.98
[32m[20230205 18:27:27 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 210.37
[32m[20230205 18:27:27 @agent_ppo2.py:149][0m Total time:      13.99 min
[32m[20230205 18:27:27 @agent_ppo2.py:151][0m 968704 total steps have happened
[32m[20230205 18:27:27 @agent_ppo2.py:127][0m #------------------------ Iteration 473 --------------------------#
[32m[20230205 18:27:27 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:28 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:28 @agent_ppo2.py:191][0m |          -0.0038 |          13.5134 |           3.3862 |
[32m[20230205 18:27:28 @agent_ppo2.py:191][0m |          -0.0061 |          12.7680 |           3.3828 |
[32m[20230205 18:27:28 @agent_ppo2.py:191][0m |          -0.0103 |          12.4372 |           3.3800 |
[32m[20230205 18:27:28 @agent_ppo2.py:191][0m |          -0.0016 |          13.1637 |           3.3792 |
[32m[20230205 18:27:28 @agent_ppo2.py:191][0m |          -0.0010 |          13.6899 |           3.3763 |
[32m[20230205 18:27:28 @agent_ppo2.py:191][0m |          -0.0116 |          11.7403 |           3.3774 |
[32m[20230205 18:27:28 @agent_ppo2.py:191][0m |          -0.0161 |          11.5201 |           3.3796 |
[32m[20230205 18:27:28 @agent_ppo2.py:191][0m |          -0.0118 |          11.4549 |           3.3794 |
[32m[20230205 18:27:28 @agent_ppo2.py:191][0m |          -0.0129 |          11.2083 |           3.3787 |
[32m[20230205 18:27:28 @agent_ppo2.py:191][0m |          -0.0097 |          11.4626 |           3.3800 |
[32m[20230205 18:27:28 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:27:28 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.95
[32m[20230205 18:27:28 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.44
[32m[20230205 18:27:28 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.02
[32m[20230205 18:27:28 @agent_ppo2.py:149][0m Total time:      14.02 min
[32m[20230205 18:27:28 @agent_ppo2.py:151][0m 970752 total steps have happened
[32m[20230205 18:27:28 @agent_ppo2.py:127][0m #------------------------ Iteration 474 --------------------------#
[32m[20230205 18:27:29 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:27:29 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:29 @agent_ppo2.py:191][0m |           0.0018 |          14.8118 |           3.4994 |
[32m[20230205 18:27:29 @agent_ppo2.py:191][0m |          -0.0050 |          13.6226 |           3.4976 |
[32m[20230205 18:27:29 @agent_ppo2.py:191][0m |          -0.0097 |          13.0205 |           3.4955 |
[32m[20230205 18:27:29 @agent_ppo2.py:191][0m |          -0.0102 |          12.5978 |           3.4933 |
[32m[20230205 18:27:29 @agent_ppo2.py:191][0m |          -0.0117 |          12.2346 |           3.4914 |
[32m[20230205 18:27:29 @agent_ppo2.py:191][0m |          -0.0116 |          12.2358 |           3.4930 |
[32m[20230205 18:27:29 @agent_ppo2.py:191][0m |          -0.0146 |          11.7197 |           3.4906 |
[32m[20230205 18:27:29 @agent_ppo2.py:191][0m |          -0.0141 |          11.4695 |           3.4891 |
[32m[20230205 18:27:29 @agent_ppo2.py:191][0m |          -0.0155 |          11.3744 |           3.4917 |
[32m[20230205 18:27:29 @agent_ppo2.py:191][0m |          -0.0148 |          10.9841 |           3.4935 |
[32m[20230205 18:27:29 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:27:30 @agent_ppo2.py:144][0m Average TRAINING episode reward: 264.27
[32m[20230205 18:27:30 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 267.91
[32m[20230205 18:27:30 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.44
[32m[20230205 18:27:30 @agent_ppo2.py:149][0m Total time:      14.04 min
[32m[20230205 18:27:30 @agent_ppo2.py:151][0m 972800 total steps have happened
[32m[20230205 18:27:30 @agent_ppo2.py:127][0m #------------------------ Iteration 475 --------------------------#
[32m[20230205 18:27:30 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:30 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:31 @agent_ppo2.py:191][0m |           0.0016 |          13.7499 |           3.3594 |
[32m[20230205 18:27:31 @agent_ppo2.py:191][0m |          -0.0094 |          12.8516 |           3.3609 |
[32m[20230205 18:27:31 @agent_ppo2.py:191][0m |          -0.0071 |          12.4963 |           3.3552 |
[32m[20230205 18:27:31 @agent_ppo2.py:191][0m |          -0.0148 |          12.2572 |           3.3561 |
[32m[20230205 18:27:31 @agent_ppo2.py:191][0m |          -0.0167 |          12.0952 |           3.3561 |
[32m[20230205 18:27:31 @agent_ppo2.py:191][0m |          -0.0116 |          12.0995 |           3.3558 |
[32m[20230205 18:27:31 @agent_ppo2.py:191][0m |          -0.0182 |          11.9755 |           3.3570 |
[32m[20230205 18:27:31 @agent_ppo2.py:191][0m |          -0.0149 |          11.7019 |           3.3572 |
[32m[20230205 18:27:31 @agent_ppo2.py:191][0m |          -0.0191 |          11.6583 |           3.3573 |
[32m[20230205 18:27:31 @agent_ppo2.py:191][0m |          -0.0202 |          11.5306 |           3.3577 |
[32m[20230205 18:27:31 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:31 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.44
[32m[20230205 18:27:31 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.84
[32m[20230205 18:27:31 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 204.52
[32m[20230205 18:27:31 @agent_ppo2.py:149][0m Total time:      14.07 min
[32m[20230205 18:27:31 @agent_ppo2.py:151][0m 974848 total steps have happened
[32m[20230205 18:27:31 @agent_ppo2.py:127][0m #------------------------ Iteration 476 --------------------------#
[32m[20230205 18:27:32 @agent_ppo2.py:133][0m Sampling time: 0.54 s by 1 slaves
[32m[20230205 18:27:32 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:32 @agent_ppo2.py:191][0m |          -0.0020 |          13.9311 |           3.3612 |
[32m[20230205 18:27:32 @agent_ppo2.py:191][0m |          -0.0085 |          11.4413 |           3.3510 |
[32m[20230205 18:27:32 @agent_ppo2.py:191][0m |          -0.0079 |          10.5307 |           3.3501 |
[32m[20230205 18:27:32 @agent_ppo2.py:191][0m |          -0.0121 |           9.6424 |           3.3491 |
[32m[20230205 18:27:32 @agent_ppo2.py:191][0m |          -0.0128 |           9.4994 |           3.3449 |
[32m[20230205 18:27:32 @agent_ppo2.py:191][0m |          -0.0143 |           8.7563 |           3.3436 |
[32m[20230205 18:27:32 @agent_ppo2.py:191][0m |          -0.0092 |           8.5513 |           3.3448 |
[32m[20230205 18:27:32 @agent_ppo2.py:191][0m |          -0.0211 |           8.2591 |           3.3406 |
[32m[20230205 18:27:32 @agent_ppo2.py:191][0m |          -0.0199 |           8.0871 |           3.3408 |
[32m[20230205 18:27:32 @agent_ppo2.py:191][0m |          -0.0138 |           7.8675 |           3.3395 |
[32m[20230205 18:27:32 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:27:33 @agent_ppo2.py:144][0m Average TRAINING episode reward: 255.41
[32m[20230205 18:27:33 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.98
[32m[20230205 18:27:33 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 202.83
[32m[20230205 18:27:33 @agent_ppo2.py:149][0m Total time:      14.09 min
[32m[20230205 18:27:33 @agent_ppo2.py:151][0m 976896 total steps have happened
[32m[20230205 18:27:33 @agent_ppo2.py:127][0m #------------------------ Iteration 477 --------------------------#
[32m[20230205 18:27:34 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:27:34 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:34 @agent_ppo2.py:191][0m |          -0.0091 |          19.6941 |           3.4350 |
[32m[20230205 18:27:34 @agent_ppo2.py:191][0m |           0.0700 |          11.7507 |           3.4310 |
[32m[20230205 18:27:34 @agent_ppo2.py:191][0m |          -0.0182 |          10.8919 |           3.4260 |
[32m[20230205 18:27:34 @agent_ppo2.py:191][0m |          -0.0271 |          10.0717 |           3.4247 |
[32m[20230205 18:27:34 @agent_ppo2.py:191][0m |          -0.0158 |           9.4672 |           3.4256 |
[32m[20230205 18:27:34 @agent_ppo2.py:191][0m |           0.0461 |           9.1343 |           3.4289 |
[32m[20230205 18:27:34 @agent_ppo2.py:191][0m |          -0.0123 |           9.0360 |           3.4126 |
[32m[20230205 18:27:34 @agent_ppo2.py:191][0m |          -0.0133 |           8.6554 |           3.4216 |
[32m[20230205 18:27:34 @agent_ppo2.py:191][0m |          -0.0188 |           8.4896 |           3.4242 |
[32m[20230205 18:27:34 @agent_ppo2.py:191][0m |          -0.0201 |           8.2643 |           3.4250 |
[32m[20230205 18:27:34 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:27:35 @agent_ppo2.py:144][0m Average TRAINING episode reward: 194.13
[32m[20230205 18:27:35 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 257.89
[32m[20230205 18:27:35 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 191.34
[32m[20230205 18:27:35 @agent_ppo2.py:149][0m Total time:      14.12 min
[32m[20230205 18:27:35 @agent_ppo2.py:151][0m 978944 total steps have happened
[32m[20230205 18:27:35 @agent_ppo2.py:127][0m #------------------------ Iteration 478 --------------------------#
[32m[20230205 18:27:35 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:35 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:35 @agent_ppo2.py:191][0m |           0.0021 |          14.3867 |           3.3873 |
[32m[20230205 18:27:35 @agent_ppo2.py:191][0m |          -0.0035 |          13.5836 |           3.3799 |
[32m[20230205 18:27:35 @agent_ppo2.py:191][0m |          -0.0129 |          12.7108 |           3.3743 |
[32m[20230205 18:27:35 @agent_ppo2.py:191][0m |          -0.0139 |          12.4224 |           3.3693 |
[32m[20230205 18:27:35 @agent_ppo2.py:191][0m |          -0.0096 |          13.0778 |           3.3710 |
[32m[20230205 18:27:35 @agent_ppo2.py:191][0m |          -0.0149 |          12.1388 |           3.3689 |
[32m[20230205 18:27:36 @agent_ppo2.py:191][0m |          -0.0135 |          12.3152 |           3.3674 |
[32m[20230205 18:27:36 @agent_ppo2.py:191][0m |          -0.0162 |          11.9065 |           3.3677 |
[32m[20230205 18:27:36 @agent_ppo2.py:191][0m |          -0.0171 |          11.8029 |           3.3711 |
[32m[20230205 18:27:36 @agent_ppo2.py:191][0m |          -0.0205 |          11.7351 |           3.3697 |
[32m[20230205 18:27:36 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:36 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.33
[32m[20230205 18:27:36 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.45
[32m[20230205 18:27:36 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 277.13
[32m[20230205 18:27:36 @agent_ppo2.py:149][0m Total time:      14.15 min
[32m[20230205 18:27:36 @agent_ppo2.py:151][0m 980992 total steps have happened
[32m[20230205 18:27:36 @agent_ppo2.py:127][0m #------------------------ Iteration 479 --------------------------#
[32m[20230205 18:27:37 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:27:37 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:37 @agent_ppo2.py:191][0m |           0.0019 |          15.5000 |           3.4079 |
[32m[20230205 18:27:37 @agent_ppo2.py:191][0m |          -0.0082 |          13.6024 |           3.3998 |
[32m[20230205 18:27:37 @agent_ppo2.py:191][0m |          -0.0155 |          11.9387 |           3.4022 |
[32m[20230205 18:27:37 @agent_ppo2.py:191][0m |          -0.0103 |          10.8252 |           3.4006 |
[32m[20230205 18:27:37 @agent_ppo2.py:191][0m |          -0.0187 |          10.1689 |           3.3985 |
[32m[20230205 18:27:37 @agent_ppo2.py:191][0m |          -0.0159 |           9.6106 |           3.3987 |
[32m[20230205 18:27:37 @agent_ppo2.py:191][0m |          -0.0146 |           9.1163 |           3.3962 |
[32m[20230205 18:27:37 @agent_ppo2.py:191][0m |          -0.0158 |           8.7926 |           3.3994 |
[32m[20230205 18:27:37 @agent_ppo2.py:191][0m |          -0.0161 |           8.4323 |           3.3991 |
[32m[20230205 18:27:37 @agent_ppo2.py:191][0m |          -0.0199 |           8.1227 |           3.3958 |
[32m[20230205 18:27:37 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:27:38 @agent_ppo2.py:144][0m Average TRAINING episode reward: 264.98
[32m[20230205 18:27:38 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 265.48
[32m[20230205 18:27:38 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.29
[32m[20230205 18:27:38 @agent_ppo2.py:149][0m Total time:      14.17 min
[32m[20230205 18:27:38 @agent_ppo2.py:151][0m 983040 total steps have happened
[32m[20230205 18:27:38 @agent_ppo2.py:127][0m #------------------------ Iteration 480 --------------------------#
[32m[20230205 18:27:38 @agent_ppo2.py:133][0m Sampling time: 0.52 s by 1 slaves
[32m[20230205 18:27:38 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:38 @agent_ppo2.py:191][0m |          -0.0002 |          15.8438 |           3.4842 |
[32m[20230205 18:27:38 @agent_ppo2.py:191][0m |          -0.0073 |          14.0755 |           3.4801 |
[32m[20230205 18:27:38 @agent_ppo2.py:191][0m |          -0.0137 |          13.0297 |           3.4770 |
[32m[20230205 18:27:38 @agent_ppo2.py:191][0m |          -0.0146 |          12.4904 |           3.4816 |
[32m[20230205 18:27:38 @agent_ppo2.py:191][0m |          -0.0166 |          12.0470 |           3.4805 |
[32m[20230205 18:27:38 @agent_ppo2.py:191][0m |          -0.0158 |          11.8496 |           3.4818 |
[32m[20230205 18:27:38 @agent_ppo2.py:191][0m |          -0.0152 |          11.8963 |           3.4809 |
[32m[20230205 18:27:39 @agent_ppo2.py:191][0m |          -0.0175 |          11.5755 |           3.4815 |
[32m[20230205 18:27:39 @agent_ppo2.py:191][0m |          -0.0183 |          11.6085 |           3.4823 |
[32m[20230205 18:27:39 @agent_ppo2.py:191][0m |          -0.0196 |          11.3124 |           3.4820 |
[32m[20230205 18:27:39 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:39 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.12
[32m[20230205 18:27:39 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 261.87
[32m[20230205 18:27:39 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.99
[32m[20230205 18:27:39 @agent_ppo2.py:149][0m Total time:      14.19 min
[32m[20230205 18:27:39 @agent_ppo2.py:151][0m 985088 total steps have happened
[32m[20230205 18:27:39 @agent_ppo2.py:127][0m #------------------------ Iteration 481 --------------------------#
[32m[20230205 18:27:40 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:40 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:40 @agent_ppo2.py:191][0m |          -0.0009 |          16.8618 |           3.3791 |
[32m[20230205 18:27:40 @agent_ppo2.py:191][0m |          -0.0055 |          15.1628 |           3.3770 |
[32m[20230205 18:27:40 @agent_ppo2.py:191][0m |          -0.0089 |          14.7618 |           3.3784 |
[32m[20230205 18:27:40 @agent_ppo2.py:191][0m |          -0.0104 |          14.6077 |           3.3797 |
[32m[20230205 18:27:40 @agent_ppo2.py:191][0m |          -0.0094 |          14.7053 |           3.3811 |
[32m[20230205 18:27:40 @agent_ppo2.py:191][0m |          -0.0145 |          13.9632 |           3.3777 |
[32m[20230205 18:27:40 @agent_ppo2.py:191][0m |          -0.0151 |          13.7257 |           3.3809 |
[32m[20230205 18:27:40 @agent_ppo2.py:191][0m |          -0.0148 |          13.4431 |           3.3786 |
[32m[20230205 18:27:40 @agent_ppo2.py:191][0m |          -0.0162 |          13.3123 |           3.3812 |
[32m[20230205 18:27:40 @agent_ppo2.py:191][0m |          -0.0180 |          13.0390 |           3.3793 |
[32m[20230205 18:27:40 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:41 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.29
[32m[20230205 18:27:41 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.90
[32m[20230205 18:27:41 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 277.74
[32m[20230205 18:27:41 @agent_ppo2.py:149][0m Total time:      14.22 min
[32m[20230205 18:27:41 @agent_ppo2.py:151][0m 987136 total steps have happened
[32m[20230205 18:27:41 @agent_ppo2.py:127][0m #------------------------ Iteration 482 --------------------------#
[32m[20230205 18:27:41 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:41 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:41 @agent_ppo2.py:191][0m |          -0.0013 |          15.6384 |           3.5194 |
[32m[20230205 18:27:41 @agent_ppo2.py:191][0m |          -0.0084 |          14.8067 |           3.5081 |
[32m[20230205 18:27:41 @agent_ppo2.py:191][0m |          -0.0111 |          14.6756 |           3.5035 |
[32m[20230205 18:27:41 @agent_ppo2.py:191][0m |          -0.0129 |          14.4101 |           3.5027 |
[32m[20230205 18:27:41 @agent_ppo2.py:191][0m |          -0.0148 |          14.2111 |           3.5026 |
[32m[20230205 18:27:41 @agent_ppo2.py:191][0m |          -0.0134 |          14.2563 |           3.4972 |
[32m[20230205 18:27:41 @agent_ppo2.py:191][0m |          -0.0152 |          14.0391 |           3.4983 |
[32m[20230205 18:27:41 @agent_ppo2.py:191][0m |          -0.0168 |          13.9281 |           3.4960 |
[32m[20230205 18:27:41 @agent_ppo2.py:191][0m |          -0.0140 |          13.9785 |           3.4963 |
[32m[20230205 18:27:42 @agent_ppo2.py:191][0m |          -0.0162 |          13.7926 |           3.4967 |
[32m[20230205 18:27:42 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:27:42 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.30
[32m[20230205 18:27:42 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.12
[32m[20230205 18:27:42 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 180.41
[32m[20230205 18:27:42 @agent_ppo2.py:149][0m Total time:      14.25 min
[32m[20230205 18:27:42 @agent_ppo2.py:151][0m 989184 total steps have happened
[32m[20230205 18:27:42 @agent_ppo2.py:127][0m #------------------------ Iteration 483 --------------------------#
[32m[20230205 18:27:43 @agent_ppo2.py:133][0m Sampling time: 0.53 s by 1 slaves
[32m[20230205 18:27:43 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:43 @agent_ppo2.py:191][0m |           0.0002 |          13.9252 |           3.4449 |
[32m[20230205 18:27:43 @agent_ppo2.py:191][0m |          -0.0056 |          13.3683 |           3.4396 |
[32m[20230205 18:27:43 @agent_ppo2.py:191][0m |          -0.0071 |          13.1230 |           3.4360 |
[32m[20230205 18:27:43 @agent_ppo2.py:191][0m |          -0.0088 |          12.9687 |           3.4357 |
[32m[20230205 18:27:43 @agent_ppo2.py:191][0m |          -0.0103 |          12.7173 |           3.4363 |
[32m[20230205 18:27:43 @agent_ppo2.py:191][0m |          -0.0103 |          12.6379 |           3.4336 |
[32m[20230205 18:27:43 @agent_ppo2.py:191][0m |          -0.0123 |          12.5064 |           3.4312 |
[32m[20230205 18:27:43 @agent_ppo2.py:191][0m |          -0.0131 |          12.3563 |           3.4323 |
[32m[20230205 18:27:43 @agent_ppo2.py:191][0m |          -0.0133 |          12.2517 |           3.4307 |
[32m[20230205 18:27:43 @agent_ppo2.py:191][0m |          -0.0137 |          12.2370 |           3.4327 |
[32m[20230205 18:27:43 @agent_ppo2.py:136][0m Policy update time: 0.52 s
[32m[20230205 18:27:44 @agent_ppo2.py:144][0m Average TRAINING episode reward: 256.44
[32m[20230205 18:27:44 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 258.25
[32m[20230205 18:27:44 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 221.24
[32m[20230205 18:27:44 @agent_ppo2.py:149][0m Total time:      14.27 min
[32m[20230205 18:27:44 @agent_ppo2.py:151][0m 991232 total steps have happened
[32m[20230205 18:27:44 @agent_ppo2.py:127][0m #------------------------ Iteration 484 --------------------------#
[32m[20230205 18:27:44 @agent_ppo2.py:133][0m Sampling time: 0.55 s by 1 slaves
[32m[20230205 18:27:44 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:44 @agent_ppo2.py:191][0m |          -0.0004 |          36.1564 |           3.4542 |
[32m[20230205 18:27:44 @agent_ppo2.py:191][0m |          -0.0054 |          24.0169 |           3.4504 |
[32m[20230205 18:27:45 @agent_ppo2.py:191][0m |          -0.0093 |          21.2092 |           3.4499 |
[32m[20230205 18:27:45 @agent_ppo2.py:191][0m |          -0.0106 |          19.9905 |           3.4502 |
[32m[20230205 18:27:45 @agent_ppo2.py:191][0m |          -0.0119 |          19.3419 |           3.4508 |
[32m[20230205 18:27:45 @agent_ppo2.py:191][0m |          -0.0130 |          18.7015 |           3.4533 |
[32m[20230205 18:27:45 @agent_ppo2.py:191][0m |          -0.0137 |          18.2426 |           3.4514 |
[32m[20230205 18:27:45 @agent_ppo2.py:191][0m |          -0.0146 |          17.9386 |           3.4531 |
[32m[20230205 18:27:45 @agent_ppo2.py:191][0m |          -0.0148 |          17.7451 |           3.4526 |
[32m[20230205 18:27:45 @agent_ppo2.py:191][0m |          -0.0151 |          17.4798 |           3.4510 |
[32m[20230205 18:27:45 @agent_ppo2.py:136][0m Policy update time: 0.54 s
[32m[20230205 18:27:45 @agent_ppo2.py:144][0m Average TRAINING episode reward: 145.19
[32m[20230205 18:27:45 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 266.08
[32m[20230205 18:27:45 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 129.69
[32m[20230205 18:27:45 @agent_ppo2.py:149][0m Total time:      14.30 min
[32m[20230205 18:27:45 @agent_ppo2.py:151][0m 993280 total steps have happened
[32m[20230205 18:27:45 @agent_ppo2.py:127][0m #------------------------ Iteration 485 --------------------------#
[32m[20230205 18:27:46 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:27:46 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:46 @agent_ppo2.py:191][0m |           0.0019 |          15.4295 |           3.5420 |
[32m[20230205 18:27:46 @agent_ppo2.py:191][0m |          -0.0050 |          13.0186 |           3.5440 |
[32m[20230205 18:27:46 @agent_ppo2.py:191][0m |          -0.0091 |          12.0709 |           3.5416 |
[32m[20230205 18:27:46 @agent_ppo2.py:191][0m |          -0.0112 |          11.3372 |           3.5401 |
[32m[20230205 18:27:46 @agent_ppo2.py:191][0m |          -0.0116 |          10.7861 |           3.5381 |
[32m[20230205 18:27:46 @agent_ppo2.py:191][0m |          -0.0132 |          10.3576 |           3.5375 |
[32m[20230205 18:27:46 @agent_ppo2.py:191][0m |          -0.0140 |          10.0662 |           3.5381 |
[32m[20230205 18:27:46 @agent_ppo2.py:191][0m |          -0.0142 |           9.6997 |           3.5379 |
[32m[20230205 18:27:46 @agent_ppo2.py:191][0m |          -0.0152 |           9.4331 |           3.5373 |
[32m[20230205 18:27:46 @agent_ppo2.py:191][0m |          -0.0161 |           9.1405 |           3.5374 |
[32m[20230205 18:27:46 @agent_ppo2.py:136][0m Policy update time: 0.48 s
[32m[20230205 18:27:47 @agent_ppo2.py:144][0m Average TRAINING episode reward: 265.30
[32m[20230205 18:27:47 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 265.75
[32m[20230205 18:27:47 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 278.70
[32m[20230205 18:27:47 @agent_ppo2.py:149][0m Total time:      14.33 min
[32m[20230205 18:27:47 @agent_ppo2.py:151][0m 995328 total steps have happened
[32m[20230205 18:27:47 @agent_ppo2.py:127][0m #------------------------ Iteration 486 --------------------------#
[32m[20230205 18:27:47 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:47 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:47 @agent_ppo2.py:191][0m |          -0.0014 |          15.3078 |           3.4297 |
[32m[20230205 18:27:48 @agent_ppo2.py:191][0m |          -0.0088 |          13.6896 |           3.4149 |
[32m[20230205 18:27:48 @agent_ppo2.py:191][0m |          -0.0111 |          13.2252 |           3.4087 |
[32m[20230205 18:27:48 @agent_ppo2.py:191][0m |          -0.0134 |          12.9928 |           3.4092 |
[32m[20230205 18:27:48 @agent_ppo2.py:191][0m |          -0.0151 |          12.8191 |           3.4060 |
[32m[20230205 18:27:48 @agent_ppo2.py:191][0m |          -0.0155 |          12.6222 |           3.4052 |
[32m[20230205 18:27:48 @agent_ppo2.py:191][0m |          -0.0168 |          12.4710 |           3.4033 |
[32m[20230205 18:27:48 @agent_ppo2.py:191][0m |          -0.0173 |          12.3268 |           3.4040 |
[32m[20230205 18:27:48 @agent_ppo2.py:191][0m |          -0.0175 |          12.2795 |           3.4012 |
[32m[20230205 18:27:48 @agent_ppo2.py:191][0m |          -0.0176 |          12.1452 |           3.3979 |
[32m[20230205 18:27:48 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:27:48 @agent_ppo2.py:144][0m Average TRAINING episode reward: 264.17
[32m[20230205 18:27:48 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 266.88
[32m[20230205 18:27:48 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 278.04
[32m[20230205 18:27:48 @agent_ppo2.py:149][0m Total time:      14.35 min
[32m[20230205 18:27:48 @agent_ppo2.py:151][0m 997376 total steps have happened
[32m[20230205 18:27:48 @agent_ppo2.py:127][0m #------------------------ Iteration 487 --------------------------#
[32m[20230205 18:27:49 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:27:49 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:49 @agent_ppo2.py:191][0m |          -0.0018 |          13.9069 |           3.5233 |
[32m[20230205 18:27:49 @agent_ppo2.py:191][0m |          -0.0032 |          13.0853 |           3.5156 |
[32m[20230205 18:27:49 @agent_ppo2.py:191][0m |          -0.0128 |          12.5607 |           3.5167 |
[32m[20230205 18:27:49 @agent_ppo2.py:191][0m |          -0.0136 |          12.1535 |           3.5168 |
[32m[20230205 18:27:49 @agent_ppo2.py:191][0m |           0.0014 |          12.4744 |           3.5160 |
[32m[20230205 18:27:49 @agent_ppo2.py:191][0m |          -0.0194 |          11.5439 |           3.5095 |
[32m[20230205 18:27:49 @agent_ppo2.py:191][0m |          -0.0120 |          11.3092 |           3.5115 |
[32m[20230205 18:27:49 @agent_ppo2.py:191][0m |           0.0173 |          16.0086 |           3.5107 |
[32m[20230205 18:27:49 @agent_ppo2.py:191][0m |          -0.0216 |          11.0357 |           3.5106 |
[32m[20230205 18:27:49 @agent_ppo2.py:191][0m |          -0.0166 |          10.8544 |           3.5100 |
[32m[20230205 18:27:49 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:27:50 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.87
[32m[20230205 18:27:50 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.37
[32m[20230205 18:27:50 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 196.96
[32m[20230205 18:27:50 @agent_ppo2.py:149][0m Total time:      14.38 min
[32m[20230205 18:27:50 @agent_ppo2.py:151][0m 999424 total steps have happened
[32m[20230205 18:27:50 @agent_ppo2.py:127][0m #------------------------ Iteration 488 --------------------------#
[32m[20230205 18:27:50 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:27:50 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:51 @agent_ppo2.py:191][0m |           0.0009 |          81.6778 |           3.4685 |
[32m[20230205 18:27:51 @agent_ppo2.py:191][0m |          -0.0060 |          62.6719 |           3.4567 |
[32m[20230205 18:27:51 @agent_ppo2.py:191][0m |          -0.0091 |          57.5106 |           3.4497 |
[32m[20230205 18:27:51 @agent_ppo2.py:191][0m |          -0.0106 |          53.3092 |           3.4502 |
[32m[20230205 18:27:51 @agent_ppo2.py:191][0m |          -0.0138 |          51.7739 |           3.4465 |
[32m[20230205 18:27:51 @agent_ppo2.py:191][0m |          -0.0114 |          49.7794 |           3.4415 |
[32m[20230205 18:27:51 @agent_ppo2.py:191][0m |          -0.0163 |          47.0441 |           3.4420 |
[32m[20230205 18:27:51 @agent_ppo2.py:191][0m |          -0.0171 |          45.9856 |           3.4388 |
[32m[20230205 18:27:51 @agent_ppo2.py:191][0m |          -0.0176 |          44.4619 |           3.4426 |
[32m[20230205 18:27:51 @agent_ppo2.py:191][0m |          -0.0197 |          42.6881 |           3.4399 |
[32m[20230205 18:27:51 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:27:51 @agent_ppo2.py:144][0m Average TRAINING episode reward: 97.10
[32m[20230205 18:27:51 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 255.34
[32m[20230205 18:27:51 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 98.96
[32m[20230205 18:27:51 @agent_ppo2.py:149][0m Total time:      14.40 min
[32m[20230205 18:27:51 @agent_ppo2.py:151][0m 1001472 total steps have happened
[32m[20230205 18:27:51 @agent_ppo2.py:127][0m #------------------------ Iteration 489 --------------------------#
[32m[20230205 18:27:52 @agent_ppo2.py:133][0m Sampling time: 0.62 s by 1 slaves
[32m[20230205 18:27:52 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:52 @agent_ppo2.py:191][0m |          -0.0005 |          49.4024 |           3.4116 |
[32m[20230205 18:27:52 @agent_ppo2.py:191][0m |          -0.0072 |          33.3588 |           3.3977 |
[32m[20230205 18:27:52 @agent_ppo2.py:191][0m |          -0.0105 |          30.4664 |           3.3939 |
[32m[20230205 18:27:52 @agent_ppo2.py:191][0m |          -0.0125 |          28.2707 |           3.3887 |
[32m[20230205 18:27:52 @agent_ppo2.py:191][0m |          -0.0139 |          26.5269 |           3.3838 |
[32m[20230205 18:27:52 @agent_ppo2.py:191][0m |          -0.0144 |          25.3223 |           3.3827 |
[32m[20230205 18:27:52 @agent_ppo2.py:191][0m |          -0.0153 |          24.0337 |           3.3795 |
[32m[20230205 18:27:53 @agent_ppo2.py:191][0m |          -0.0160 |          23.0797 |           3.3765 |
[32m[20230205 18:27:53 @agent_ppo2.py:191][0m |          -0.0167 |          22.0810 |           3.3739 |
[32m[20230205 18:27:53 @agent_ppo2.py:191][0m |          -0.0175 |          21.2170 |           3.3748 |
[32m[20230205 18:27:53 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:27:53 @agent_ppo2.py:144][0m Average TRAINING episode reward: 170.25
[32m[20230205 18:27:53 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 260.95
[32m[20230205 18:27:53 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 54.42
[32m[20230205 18:27:53 @agent_ppo2.py:149][0m Total time:      14.43 min
[32m[20230205 18:27:53 @agent_ppo2.py:151][0m 1003520 total steps have happened
[32m[20230205 18:27:53 @agent_ppo2.py:127][0m #------------------------ Iteration 490 --------------------------#
[32m[20230205 18:27:54 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:27:54 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:54 @agent_ppo2.py:191][0m |           0.0034 |          15.0377 |           3.4377 |
[32m[20230205 18:27:54 @agent_ppo2.py:191][0m |          -0.0050 |          13.2995 |           3.4321 |
[32m[20230205 18:27:54 @agent_ppo2.py:191][0m |          -0.0078 |          12.6367 |           3.4282 |
[32m[20230205 18:27:54 @agent_ppo2.py:191][0m |          -0.0112 |          12.1458 |           3.4290 |
[32m[20230205 18:27:54 @agent_ppo2.py:191][0m |          -0.0109 |          11.9192 |           3.4282 |
[32m[20230205 18:27:54 @agent_ppo2.py:191][0m |          -0.0124 |          11.5860 |           3.4263 |
[32m[20230205 18:27:54 @agent_ppo2.py:191][0m |          -0.0123 |          11.3574 |           3.4249 |
[32m[20230205 18:27:54 @agent_ppo2.py:191][0m |          -0.0147 |          11.1419 |           3.4258 |
[32m[20230205 18:27:54 @agent_ppo2.py:191][0m |          -0.0158 |          10.9509 |           3.4249 |
[32m[20230205 18:27:54 @agent_ppo2.py:191][0m |          -0.0158 |          10.7915 |           3.4240 |
[32m[20230205 18:27:54 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:27:55 @agent_ppo2.py:144][0m Average TRAINING episode reward: 262.96
[32m[20230205 18:27:55 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 265.42
[32m[20230205 18:27:55 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 189.93
[32m[20230205 18:27:55 @agent_ppo2.py:149][0m Total time:      14.45 min
[32m[20230205 18:27:55 @agent_ppo2.py:151][0m 1005568 total steps have happened
[32m[20230205 18:27:55 @agent_ppo2.py:127][0m #------------------------ Iteration 491 --------------------------#
[32m[20230205 18:27:55 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:27:55 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:55 @agent_ppo2.py:191][0m |          -0.0142 |          14.0928 |           3.4989 |
[32m[20230205 18:27:55 @agent_ppo2.py:191][0m |          -0.0139 |          13.3893 |           3.4931 |
[32m[20230205 18:27:55 @agent_ppo2.py:191][0m |           0.0003 |          14.2598 |           3.4928 |
[32m[20230205 18:27:55 @agent_ppo2.py:191][0m |          -0.0170 |          12.6782 |           3.4902 |
[32m[20230205 18:27:55 @agent_ppo2.py:191][0m |          -0.0087 |          12.5644 |           3.4847 |
[32m[20230205 18:27:55 @agent_ppo2.py:191][0m |          -0.0140 |          12.3317 |           3.4893 |
[32m[20230205 18:27:55 @agent_ppo2.py:191][0m |          -0.0068 |          13.9384 |           3.4888 |
[32m[20230205 18:27:56 @agent_ppo2.py:191][0m |          -0.0169 |          11.9581 |           3.4898 |
[32m[20230205 18:27:56 @agent_ppo2.py:191][0m |          -0.0192 |          11.8395 |           3.4855 |
[32m[20230205 18:27:56 @agent_ppo2.py:191][0m |          -0.0196 |          11.7366 |           3.4882 |
[32m[20230205 18:27:56 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:27:56 @agent_ppo2.py:144][0m Average TRAINING episode reward: 265.57
[32m[20230205 18:27:56 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 269.92
[32m[20230205 18:27:56 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 274.33
[32m[20230205 18:27:56 @agent_ppo2.py:149][0m Total time:      14.48 min
[32m[20230205 18:27:56 @agent_ppo2.py:151][0m 1007616 total steps have happened
[32m[20230205 18:27:56 @agent_ppo2.py:127][0m #------------------------ Iteration 492 --------------------------#
[32m[20230205 18:27:57 @agent_ppo2.py:133][0m Sampling time: 0.49 s by 1 slaves
[32m[20230205 18:27:57 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:57 @agent_ppo2.py:191][0m |           0.0006 |          14.9504 |           3.3762 |
[32m[20230205 18:27:57 @agent_ppo2.py:191][0m |          -0.0058 |          14.3157 |           3.3724 |
[32m[20230205 18:27:57 @agent_ppo2.py:191][0m |          -0.0109 |          13.8296 |           3.3682 |
[32m[20230205 18:27:57 @agent_ppo2.py:191][0m |          -0.0114 |          13.7505 |           3.3664 |
[32m[20230205 18:27:57 @agent_ppo2.py:191][0m |          -0.0134 |          13.5619 |           3.3632 |
[32m[20230205 18:27:57 @agent_ppo2.py:191][0m |          -0.0136 |          13.5361 |           3.3591 |
[32m[20230205 18:27:57 @agent_ppo2.py:191][0m |          -0.0144 |          13.3681 |           3.3589 |
[32m[20230205 18:27:57 @agent_ppo2.py:191][0m |          -0.0150 |          13.2586 |           3.3560 |
[32m[20230205 18:27:57 @agent_ppo2.py:191][0m |          -0.0163 |          13.1892 |           3.3562 |
[32m[20230205 18:27:57 @agent_ppo2.py:191][0m |          -0.0165 |          13.2130 |           3.3565 |
[32m[20230205 18:27:57 @agent_ppo2.py:136][0m Policy update time: 0.48 s
[32m[20230205 18:27:57 @agent_ppo2.py:144][0m Average TRAINING episode reward: 264.81
[32m[20230205 18:27:57 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 266.92
[32m[20230205 18:27:57 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.82
[32m[20230205 18:27:57 @agent_ppo2.py:149][0m Total time:      14.50 min
[32m[20230205 18:27:57 @agent_ppo2.py:151][0m 1009664 total steps have happened
[32m[20230205 18:27:57 @agent_ppo2.py:127][0m #------------------------ Iteration 493 --------------------------#
[32m[20230205 18:27:58 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:27:58 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:27:58 @agent_ppo2.py:191][0m |          -0.0006 |          14.7534 |           3.4302 |
[32m[20230205 18:27:58 @agent_ppo2.py:191][0m |          -0.0059 |          13.6007 |           3.4276 |
[32m[20230205 18:27:58 @agent_ppo2.py:191][0m |          -0.0083 |          13.4194 |           3.4215 |
[32m[20230205 18:27:58 @agent_ppo2.py:191][0m |          -0.0093 |          13.2844 |           3.4259 |
[32m[20230205 18:27:58 @agent_ppo2.py:191][0m |          -0.0108 |          13.1933 |           3.4234 |
[32m[20230205 18:27:58 @agent_ppo2.py:191][0m |          -0.0116 |          13.1019 |           3.4233 |
[32m[20230205 18:27:58 @agent_ppo2.py:191][0m |          -0.0118 |          13.0280 |           3.4230 |
[32m[20230205 18:27:58 @agent_ppo2.py:191][0m |          -0.0131 |          12.9508 |           3.4178 |
[32m[20230205 18:27:58 @agent_ppo2.py:191][0m |          -0.0138 |          12.8782 |           3.4206 |
[32m[20230205 18:27:58 @agent_ppo2.py:191][0m |          -0.0145 |          12.7786 |           3.4206 |
[32m[20230205 18:27:58 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:27:59 @agent_ppo2.py:144][0m Average TRAINING episode reward: 261.57
[32m[20230205 18:27:59 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.06
[32m[20230205 18:27:59 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 277.63
[32m[20230205 18:27:59 @agent_ppo2.py:149][0m Total time:      14.53 min
[32m[20230205 18:27:59 @agent_ppo2.py:151][0m 1011712 total steps have happened
[32m[20230205 18:27:59 @agent_ppo2.py:127][0m #------------------------ Iteration 494 --------------------------#
[32m[20230205 18:28:00 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:28:00 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:28:00 @agent_ppo2.py:191][0m |           0.0021 |          13.8262 |           3.4194 |
[32m[20230205 18:28:00 @agent_ppo2.py:191][0m |          -0.0009 |          13.3225 |           3.4196 |
[32m[20230205 18:28:00 @agent_ppo2.py:191][0m |          -0.0076 |          12.5965 |           3.4160 |
[32m[20230205 18:28:00 @agent_ppo2.py:191][0m |          -0.0088 |          12.2296 |           3.4138 |
[32m[20230205 18:28:00 @agent_ppo2.py:191][0m |          -0.0063 |          12.2946 |           3.4144 |
[32m[20230205 18:28:00 @agent_ppo2.py:191][0m |          -0.0112 |          11.6791 |           3.4113 |
[32m[20230205 18:28:00 @agent_ppo2.py:191][0m |          -0.0112 |          11.4643 |           3.4093 |
[32m[20230205 18:28:00 @agent_ppo2.py:191][0m |          -0.0128 |          11.2294 |           3.4118 |
[32m[20230205 18:28:00 @agent_ppo2.py:191][0m |          -0.0115 |          11.1391 |           3.4106 |
[32m[20230205 18:28:00 @agent_ppo2.py:191][0m |          -0.0099 |          11.2357 |           3.4133 |
[32m[20230205 18:28:00 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:28:01 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.77
[32m[20230205 18:28:01 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.58
[32m[20230205 18:28:01 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 275.16
[32m[20230205 18:28:01 @agent_ppo2.py:149][0m Total time:      14.55 min
[32m[20230205 18:28:01 @agent_ppo2.py:151][0m 1013760 total steps have happened
[32m[20230205 18:28:01 @agent_ppo2.py:127][0m #------------------------ Iteration 495 --------------------------#
[32m[20230205 18:28:01 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:28:01 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:28:01 @agent_ppo2.py:191][0m |          -0.0028 |          14.0950 |           3.3616 |
[32m[20230205 18:28:01 @agent_ppo2.py:191][0m |          -0.0090 |          13.5833 |           3.3530 |
[32m[20230205 18:28:01 @agent_ppo2.py:191][0m |          -0.0095 |          13.3453 |           3.3475 |
[32m[20230205 18:28:01 @agent_ppo2.py:191][0m |          -0.0161 |          13.2106 |           3.3441 |
[32m[20230205 18:28:01 @agent_ppo2.py:191][0m |          -0.0106 |          13.1169 |           3.3429 |
[32m[20230205 18:28:01 @agent_ppo2.py:191][0m |          -0.0097 |          13.7392 |           3.3430 |
[32m[20230205 18:28:01 @agent_ppo2.py:191][0m |          -0.0161 |          12.8299 |           3.3402 |
[32m[20230205 18:28:02 @agent_ppo2.py:191][0m |          -0.0125 |          12.9519 |           3.3391 |
[32m[20230205 18:28:02 @agent_ppo2.py:191][0m |          -0.0099 |          13.4869 |           3.3381 |
[32m[20230205 18:28:02 @agent_ppo2.py:191][0m |          -0.0139 |          12.7236 |           3.3367 |
[32m[20230205 18:28:02 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:28:02 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.16
[32m[20230205 18:28:02 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.00
[32m[20230205 18:28:02 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 272.77
[32m[20230205 18:28:02 @agent_ppo2.py:149][0m Total time:      14.58 min
[32m[20230205 18:28:02 @agent_ppo2.py:151][0m 1015808 total steps have happened
[32m[20230205 18:28:02 @agent_ppo2.py:127][0m #------------------------ Iteration 496 --------------------------#
[32m[20230205 18:28:03 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:28:03 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:28:03 @agent_ppo2.py:191][0m |           0.0006 |          13.8782 |           3.3432 |
[32m[20230205 18:28:03 @agent_ppo2.py:191][0m |           0.0056 |          15.1430 |           3.3494 |
[32m[20230205 18:28:03 @agent_ppo2.py:191][0m |          -0.0114 |          13.1641 |           3.3407 |
[32m[20230205 18:28:03 @agent_ppo2.py:191][0m |          -0.0117 |          12.9301 |           3.3395 |
[32m[20230205 18:28:03 @agent_ppo2.py:191][0m |          -0.0101 |          13.0143 |           3.3477 |
[32m[20230205 18:28:03 @agent_ppo2.py:191][0m |          -0.0146 |          12.6359 |           3.3448 |
[32m[20230205 18:28:03 @agent_ppo2.py:191][0m |          -0.0133 |          12.5117 |           3.3471 |
[32m[20230205 18:28:03 @agent_ppo2.py:191][0m |          -0.0152 |          12.4169 |           3.3465 |
[32m[20230205 18:28:03 @agent_ppo2.py:191][0m |          -0.0109 |          12.7650 |           3.3498 |
[32m[20230205 18:28:03 @agent_ppo2.py:191][0m |          -0.0163 |          12.1921 |           3.3442 |
[32m[20230205 18:28:03 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:28:04 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.21
[32m[20230205 18:28:04 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 262.00
[32m[20230205 18:28:04 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 273.91
[32m[20230205 18:28:04 @agent_ppo2.py:149][0m Total time:      14.60 min
[32m[20230205 18:28:04 @agent_ppo2.py:151][0m 1017856 total steps have happened
[32m[20230205 18:28:04 @agent_ppo2.py:127][0m #------------------------ Iteration 497 --------------------------#
[32m[20230205 18:28:04 @agent_ppo2.py:133][0m Sampling time: 0.61 s by 1 slaves
[32m[20230205 18:28:04 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:28:04 @agent_ppo2.py:191][0m |           0.0004 |          32.0686 |           3.4735 |
[32m[20230205 18:28:04 @agent_ppo2.py:191][0m |           0.0014 |          18.6470 |           3.4680 |
[32m[20230205 18:28:04 @agent_ppo2.py:191][0m |          -0.0027 |          15.5306 |           3.4641 |
[32m[20230205 18:28:04 @agent_ppo2.py:191][0m |          -0.0095 |          13.6456 |           3.4631 |
[32m[20230205 18:28:04 @agent_ppo2.py:191][0m |          -0.0122 |          12.8541 |           3.4611 |
[32m[20230205 18:28:05 @agent_ppo2.py:191][0m |          -0.0107 |          12.5043 |           3.4612 |
[32m[20230205 18:28:05 @agent_ppo2.py:191][0m |          -0.0106 |          12.1173 |           3.4587 |
[32m[20230205 18:28:05 @agent_ppo2.py:191][0m |          -0.0114 |          11.7346 |           3.4605 |
[32m[20230205 18:28:05 @agent_ppo2.py:191][0m |          -0.0130 |          12.1801 |           3.4585 |
[32m[20230205 18:28:05 @agent_ppo2.py:191][0m |          -0.0156 |          11.3583 |           3.4601 |
[32m[20230205 18:28:05 @agent_ppo2.py:136][0m Policy update time: 0.61 s
[32m[20230205 18:28:05 @agent_ppo2.py:144][0m Average TRAINING episode reward: 154.14
[32m[20230205 18:28:05 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.70
[32m[20230205 18:28:05 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 212.26
[32m[20230205 18:28:05 @agent_ppo2.py:149][0m Total time:      14.63 min
[32m[20230205 18:28:05 @agent_ppo2.py:151][0m 1019904 total steps have happened
[32m[20230205 18:28:05 @agent_ppo2.py:127][0m #------------------------ Iteration 498 --------------------------#
[32m[20230205 18:28:06 @agent_ppo2.py:133][0m Sampling time: 0.51 s by 1 slaves
[32m[20230205 18:28:06 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:28:06 @agent_ppo2.py:191][0m |           0.0053 |          15.0524 |           3.3930 |
[32m[20230205 18:28:06 @agent_ppo2.py:191][0m |          -0.0069 |          13.5619 |           3.3903 |
[32m[20230205 18:28:06 @agent_ppo2.py:191][0m |          -0.0100 |          13.1452 |           3.3848 |
[32m[20230205 18:28:06 @agent_ppo2.py:191][0m |          -0.0072 |          13.0674 |           3.3817 |
[32m[20230205 18:28:06 @agent_ppo2.py:191][0m |          -0.0138 |          12.5047 |           3.3775 |
[32m[20230205 18:28:06 @agent_ppo2.py:191][0m |          -0.0153 |          12.2540 |           3.3779 |
[32m[20230205 18:28:06 @agent_ppo2.py:191][0m |          -0.0121 |          12.2444 |           3.3725 |
[32m[20230205 18:28:06 @agent_ppo2.py:191][0m |          -0.0165 |          11.8336 |           3.3720 |
[32m[20230205 18:28:06 @agent_ppo2.py:191][0m |          -0.0151 |          11.5785 |           3.3727 |
[32m[20230205 18:28:06 @agent_ppo2.py:191][0m |          -0.0177 |          11.4752 |           3.3713 |
[32m[20230205 18:28:06 @agent_ppo2.py:136][0m Policy update time: 0.51 s
[32m[20230205 18:28:07 @agent_ppo2.py:144][0m Average TRAINING episode reward: 260.74
[32m[20230205 18:28:07 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 263.88
[32m[20230205 18:28:07 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 45.99
[32m[20230205 18:28:07 @agent_ppo2.py:149][0m Total time:      14.65 min
[32m[20230205 18:28:07 @agent_ppo2.py:151][0m 1021952 total steps have happened
[32m[20230205 18:28:07 @agent_ppo2.py:127][0m #------------------------ Iteration 499 --------------------------#
[32m[20230205 18:28:07 @agent_ppo2.py:133][0m Sampling time: 0.50 s by 1 slaves
[32m[20230205 18:28:07 @agent_ppo2.py:167][0m |      policy_loss |       value_loss |          entropy |
[32m[20230205 18:28:07 @agent_ppo2.py:191][0m |          -0.0013 |          30.1126 |           3.2961 |
[32m[20230205 18:28:07 @agent_ppo2.py:191][0m |           0.0072 |          12.6860 |           3.2931 |
[32m[20230205 18:28:07 @agent_ppo2.py:191][0m |          -0.0073 |           9.8571 |           3.2802 |
[32m[20230205 18:28:07 @agent_ppo2.py:191][0m |          -0.0130 |           8.7077 |           3.2890 |
[32m[20230205 18:28:07 @agent_ppo2.py:191][0m |          -0.0146 |           8.2596 |           3.2852 |
[32m[20230205 18:28:07 @agent_ppo2.py:191][0m |          -0.0165 |           7.6973 |           3.2863 |
[32m[20230205 18:28:08 @agent_ppo2.py:191][0m |          -0.0127 |           8.0894 |           3.2893 |
[32m[20230205 18:28:08 @agent_ppo2.py:191][0m |          -0.0170 |           7.0728 |           3.2901 |
[32m[20230205 18:28:08 @agent_ppo2.py:191][0m |          -0.0186 |           6.8828 |           3.2921 |
[32m[20230205 18:28:08 @agent_ppo2.py:191][0m |          -0.0195 |           6.6446 |           3.2927 |
[32m[20230205 18:28:08 @agent_ppo2.py:136][0m Policy update time: 0.50 s
[32m[20230205 18:28:08 @agent_ppo2.py:144][0m Average TRAINING episode reward: 192.76
[32m[20230205 18:28:08 @agent_ppo2.py:145][0m Maximum TRAINING episode reward: 259.90
[32m[20230205 18:28:08 @agent_ppo2.py:146][0m Average EVALUATION episode reward: 164.90
[32m[20230205 18:28:08 @agent_ppo2.py:106][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 290.51
[32m[20230205 18:28:08 @agent_ppo2.py:149][0m Total time:      14.68 min
[32m[20230205 18:28:08 @agent_ppo2.py:151][0m 1024000 total steps have happened
[32m[20230205 18:28:08 @train.py:63][0m [4m[34mCRITICAL[0m Training completed!
