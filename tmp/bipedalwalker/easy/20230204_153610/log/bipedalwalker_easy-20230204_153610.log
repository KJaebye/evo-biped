[32m[20230204 15:36:10 @logger.py:106][0m Log file set to ./tmp/bipedalwalker/easy/20230204_153610/log/bipedalwalker_easy-20230204_153610.log
[32m[20230204 15:36:10 @agent_ppo2.py:130][0m #------------------------ Iteration 0 --------------------------#
[32m[20230204 15:36:11 @agent_ppo2.py:136][0m Sampling time: 0.47 s by 4 slaves
[32m[20230204 15:36:11 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:11 @agent_ppo2.py:194][0m |           0.0068 |         146.3866 |           1.8754 |
[32m[20230204 15:36:11 @agent_ppo2.py:194][0m |           0.0001 |         129.7991 |           1.8760 |
[32m[20230204 15:36:11 @agent_ppo2.py:194][0m |          -0.0016 |         123.8778 |           1.8768 |
[32m[20230204 15:36:12 @agent_ppo2.py:194][0m |          -0.0021 |         119.7515 |           1.8773 |
[32m[20230204 15:36:12 @agent_ppo2.py:194][0m |          -0.0013 |         117.6612 |           1.8782 |
[32m[20230204 15:36:12 @agent_ppo2.py:194][0m |          -0.0025 |         115.4173 |           1.8791 |
[32m[20230204 15:36:12 @agent_ppo2.py:194][0m |          -0.0017 |         113.6468 |           1.8797 |
[32m[20230204 15:36:12 @agent_ppo2.py:194][0m |          -0.0035 |         110.7726 |           1.8803 |
[32m[20230204 15:36:12 @agent_ppo2.py:194][0m |          -0.0025 |         108.2994 |           1.8812 |
[32m[20230204 15:36:12 @agent_ppo2.py:194][0m |          -0.0052 |         104.8115 |           1.8812 |
[32m[20230204 15:36:12 @agent_ppo2.py:139][0m Policy update time: 1.44 s
[32m[20230204 15:36:13 @agent_ppo2.py:147][0m Average TRAINING episode reward: -113.21
[32m[20230204 15:36:13 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -100.89
[32m[20230204 15:36:13 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -92.28
[32m[20230204 15:36:13 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: -92.28
[32m[20230204 15:36:13 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -92.28
[32m[20230204 15:36:13 @agent_ppo2.py:152][0m Total time:       0.03 min
[32m[20230204 15:36:13 @agent_ppo2.py:154][0m 2048 total steps have happened
[32m[20230204 15:36:13 @agent_ppo2.py:130][0m #------------------------ Iteration 1 --------------------------#
[32m[20230204 15:36:13 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:36:13 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:13 @agent_ppo2.py:194][0m |           0.0031 |         117.4169 |           1.9102 |
[32m[20230204 15:36:13 @agent_ppo2.py:194][0m |           0.0001 |         105.4071 |           1.9091 |
[32m[20230204 15:36:13 @agent_ppo2.py:194][0m |          -0.0057 |         100.1008 |           1.9085 |
[32m[20230204 15:36:13 @agent_ppo2.py:194][0m |          -0.0006 |         100.1950 |           1.9074 |
[32m[20230204 15:36:13 @agent_ppo2.py:194][0m |          -0.0067 |          92.0388 |           1.9057 |
[32m[20230204 15:36:14 @agent_ppo2.py:194][0m |          -0.0057 |          88.8346 |           1.9044 |
[32m[20230204 15:36:14 @agent_ppo2.py:194][0m |          -0.0042 |          88.9643 |           1.9035 |
[32m[20230204 15:36:14 @agent_ppo2.py:194][0m |          -0.0062 |          82.2684 |           1.9026 |
[32m[20230204 15:36:14 @agent_ppo2.py:194][0m |          -0.0095 |          78.6958 |           1.9027 |
[32m[20230204 15:36:14 @agent_ppo2.py:194][0m |          -0.0101 |          75.8064 |           1.9013 |
[32m[20230204 15:36:14 @agent_ppo2.py:139][0m Policy update time: 1.08 s
[32m[20230204 15:36:14 @agent_ppo2.py:147][0m Average TRAINING episode reward: -105.68
[32m[20230204 15:36:14 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -98.03
[32m[20230204 15:36:14 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -93.08
[32m[20230204 15:36:14 @agent_ppo2.py:152][0m Total time:       0.06 min
[32m[20230204 15:36:14 @agent_ppo2.py:154][0m 4096 total steps have happened
[32m[20230204 15:36:14 @agent_ppo2.py:130][0m #------------------------ Iteration 2 --------------------------#
[32m[20230204 15:36:14 @agent_ppo2.py:136][0m Sampling time: 0.36 s by 4 slaves
[32m[20230204 15:36:15 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:15 @agent_ppo2.py:194][0m |           0.0006 |          30.7904 |           1.9113 |
[32m[20230204 15:36:15 @agent_ppo2.py:194][0m |          -0.0019 |          27.8145 |           1.9120 |
[32m[20230204 15:36:15 @agent_ppo2.py:194][0m |          -0.0040 |          26.4831 |           1.9134 |
[32m[20230204 15:36:15 @agent_ppo2.py:194][0m |          -0.0019 |          25.6347 |           1.9141 |
[32m[20230204 15:36:15 @agent_ppo2.py:194][0m |          -0.0041 |          24.4080 |           1.9145 |
[32m[20230204 15:36:15 @agent_ppo2.py:194][0m |          -0.0052 |          23.5063 |           1.9149 |
[32m[20230204 15:36:15 @agent_ppo2.py:194][0m |          -0.0057 |          22.7419 |           1.9157 |
[32m[20230204 15:36:16 @agent_ppo2.py:194][0m |          -0.0009 |          24.1527 |           1.9163 |
[32m[20230204 15:36:16 @agent_ppo2.py:194][0m |          -0.0054 |          21.0924 |           1.9169 |
[32m[20230204 15:36:16 @agent_ppo2.py:194][0m |          -0.0099 |          20.2815 |           1.9171 |
[32m[20230204 15:36:16 @agent_ppo2.py:139][0m Policy update time: 1.32 s
[32m[20230204 15:36:16 @agent_ppo2.py:147][0m Average TRAINING episode reward: -108.66
[32m[20230204 15:36:16 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -104.62
[32m[20230204 15:36:16 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -93.07
[32m[20230204 15:36:16 @agent_ppo2.py:152][0m Total time:       0.09 min
[32m[20230204 15:36:16 @agent_ppo2.py:154][0m 6144 total steps have happened
[32m[20230204 15:36:16 @agent_ppo2.py:130][0m #------------------------ Iteration 3 --------------------------#
[32m[20230204 15:36:16 @agent_ppo2.py:136][0m Sampling time: 0.47 s by 4 slaves
[32m[20230204 15:36:16 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:17 @agent_ppo2.py:194][0m |          -0.0014 |          93.7859 |           1.9092 |
[32m[20230204 15:36:17 @agent_ppo2.py:194][0m |          -0.0032 |          80.3565 |           1.9087 |
[32m[20230204 15:36:17 @agent_ppo2.py:194][0m |          -0.0028 |          73.7342 |           1.9078 |
[32m[20230204 15:36:17 @agent_ppo2.py:194][0m |          -0.0026 |          68.4691 |           1.9070 |
[32m[20230204 15:36:17 @agent_ppo2.py:194][0m |          -0.0057 |          62.5019 |           1.9057 |
[32m[20230204 15:36:17 @agent_ppo2.py:194][0m |          -0.0077 |          57.1226 |           1.9042 |
[32m[20230204 15:36:17 @agent_ppo2.py:194][0m |          -0.0039 |          56.1627 |           1.9034 |
[32m[20230204 15:36:18 @agent_ppo2.py:194][0m |          -0.0085 |          49.5055 |           1.9017 |
[32m[20230204 15:36:18 @agent_ppo2.py:194][0m |          -0.0081 |          46.4296 |           1.9017 |
[32m[20230204 15:36:18 @agent_ppo2.py:194][0m |          -0.0077 |          43.4862 |           1.9009 |
[32m[20230204 15:36:18 @agent_ppo2.py:139][0m Policy update time: 1.50 s
[32m[20230204 15:36:18 @agent_ppo2.py:147][0m Average TRAINING episode reward: -108.66
[32m[20230204 15:36:18 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -98.95
[32m[20230204 15:36:18 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -93.83
[32m[20230204 15:36:18 @agent_ppo2.py:152][0m Total time:       0.13 min
[32m[20230204 15:36:18 @agent_ppo2.py:154][0m 8192 total steps have happened
[32m[20230204 15:36:18 @agent_ppo2.py:130][0m #------------------------ Iteration 4 --------------------------#
[32m[20230204 15:36:18 @agent_ppo2.py:136][0m Sampling time: 0.42 s by 4 slaves
[32m[20230204 15:36:19 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:19 @agent_ppo2.py:194][0m |           0.0001 |          78.6102 |           1.9249 |
[32m[20230204 15:36:19 @agent_ppo2.py:194][0m |          -0.0011 |          65.7719 |           1.9245 |
[32m[20230204 15:36:19 @agent_ppo2.py:194][0m |          -0.0016 |          59.8694 |           1.9238 |
[32m[20230204 15:36:19 @agent_ppo2.py:194][0m |          -0.0037 |          54.6026 |           1.9224 |
[32m[20230204 15:36:19 @agent_ppo2.py:194][0m |          -0.0044 |          50.2909 |           1.9220 |
[32m[20230204 15:36:19 @agent_ppo2.py:194][0m |          -0.0055 |          46.7513 |           1.9206 |
[32m[20230204 15:36:19 @agent_ppo2.py:194][0m |          -0.0065 |          43.6323 |           1.9192 |
[32m[20230204 15:36:19 @agent_ppo2.py:194][0m |          -0.0060 |          40.5746 |           1.9183 |
[32m[20230204 15:36:19 @agent_ppo2.py:194][0m |          -0.0070 |          37.8792 |           1.9173 |
[32m[20230204 15:36:20 @agent_ppo2.py:194][0m |          -0.0063 |          35.5263 |           1.9164 |
[32m[20230204 15:36:20 @agent_ppo2.py:139][0m Policy update time: 1.11 s
[32m[20230204 15:36:20 @agent_ppo2.py:147][0m Average TRAINING episode reward: -107.70
[32m[20230204 15:36:20 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -99.25
[32m[20230204 15:36:20 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -55.72
[32m[20230204 15:36:20 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: -55.72
[32m[20230204 15:36:20 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -55.72
[32m[20230204 15:36:20 @agent_ppo2.py:152][0m Total time:       0.16 min
[32m[20230204 15:36:20 @agent_ppo2.py:154][0m 10240 total steps have happened
[32m[20230204 15:36:20 @agent_ppo2.py:130][0m #------------------------ Iteration 5 --------------------------#
[32m[20230204 15:36:20 @agent_ppo2.py:136][0m Sampling time: 0.38 s by 4 slaves
[32m[20230204 15:36:20 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:21 @agent_ppo2.py:194][0m |          -0.0003 |          25.5458 |           1.8966 |
[32m[20230204 15:36:21 @agent_ppo2.py:194][0m |          -0.0020 |          21.4438 |           1.8962 |
[32m[20230204 15:36:21 @agent_ppo2.py:194][0m |          -0.0031 |          19.2970 |           1.8959 |
[32m[20230204 15:36:21 @agent_ppo2.py:194][0m |          -0.0047 |          17.8893 |           1.8954 |
[32m[20230204 15:36:21 @agent_ppo2.py:194][0m |          -0.0051 |          16.7683 |           1.8949 |
[32m[20230204 15:36:21 @agent_ppo2.py:194][0m |          -0.0057 |          16.0993 |           1.8946 |
[32m[20230204 15:36:21 @agent_ppo2.py:194][0m |          -0.0061 |          15.1200 |           1.8941 |
[32m[20230204 15:36:21 @agent_ppo2.py:194][0m |          -0.0070 |          14.2583 |           1.8935 |
[32m[20230204 15:36:21 @agent_ppo2.py:194][0m |          -0.0075 |          13.6473 |           1.8925 |
[32m[20230204 15:36:22 @agent_ppo2.py:194][0m |          -0.0077 |          12.8702 |           1.8921 |
[32m[20230204 15:36:22 @agent_ppo2.py:139][0m Policy update time: 1.30 s
[32m[20230204 15:36:22 @agent_ppo2.py:147][0m Average TRAINING episode reward: -110.50
[32m[20230204 15:36:22 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -98.15
[32m[20230204 15:36:22 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -24.39
[32m[20230204 15:36:22 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: -24.39
[32m[20230204 15:36:22 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -24.39
[32m[20230204 15:36:22 @agent_ppo2.py:152][0m Total time:       0.19 min
[32m[20230204 15:36:22 @agent_ppo2.py:154][0m 12288 total steps have happened
[32m[20230204 15:36:22 @agent_ppo2.py:130][0m #------------------------ Iteration 6 --------------------------#
[32m[20230204 15:36:22 @agent_ppo2.py:136][0m Sampling time: 0.38 s by 4 slaves
[32m[20230204 15:36:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:23 @agent_ppo2.py:194][0m |          -0.0004 |          14.2217 |           1.8875 |
[32m[20230204 15:36:23 @agent_ppo2.py:194][0m |          -0.0013 |          10.0294 |           1.8868 |
[32m[20230204 15:36:23 @agent_ppo2.py:194][0m |          -0.0053 |           8.1663 |           1.8874 |
[32m[20230204 15:36:23 @agent_ppo2.py:194][0m |          -0.0065 |           7.4904 |           1.8872 |
[32m[20230204 15:36:23 @agent_ppo2.py:194][0m |          -0.0063 |           6.9074 |           1.8867 |
[32m[20230204 15:36:23 @agent_ppo2.py:194][0m |          -0.0076 |           6.6224 |           1.8865 |
[32m[20230204 15:36:23 @agent_ppo2.py:194][0m |          -0.0071 |           6.2535 |           1.8865 |
[32m[20230204 15:36:23 @agent_ppo2.py:194][0m |          -0.0080 |           5.9912 |           1.8866 |
[32m[20230204 15:36:24 @agent_ppo2.py:194][0m |          -0.0078 |           5.8054 |           1.8868 |
[32m[20230204 15:36:24 @agent_ppo2.py:194][0m |          -0.0111 |           5.5870 |           1.8867 |
[32m[20230204 15:36:24 @agent_ppo2.py:139][0m Policy update time: 1.28 s
[32m[20230204 15:36:24 @agent_ppo2.py:147][0m Average TRAINING episode reward: -105.44
[32m[20230204 15:36:24 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -94.91
[32m[20230204 15:36:24 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -21.56
[32m[20230204 15:36:24 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: -21.56
[32m[20230204 15:36:24 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -21.56
[32m[20230204 15:36:24 @agent_ppo2.py:152][0m Total time:       0.23 min
[32m[20230204 15:36:24 @agent_ppo2.py:154][0m 14336 total steps have happened
[32m[20230204 15:36:24 @agent_ppo2.py:130][0m #------------------------ Iteration 7 --------------------------#
[32m[20230204 15:36:24 @agent_ppo2.py:136][0m Sampling time: 0.36 s by 4 slaves
[32m[20230204 15:36:25 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:25 @agent_ppo2.py:194][0m |           0.0002 |          19.1754 |           1.8890 |
[32m[20230204 15:36:25 @agent_ppo2.py:194][0m |          -0.0028 |          11.7845 |           1.8893 |
[32m[20230204 15:36:25 @agent_ppo2.py:194][0m |          -0.0048 |          10.1230 |           1.8890 |
[32m[20230204 15:36:25 @agent_ppo2.py:194][0m |          -0.0056 |           9.0893 |           1.8881 |
[32m[20230204 15:36:25 @agent_ppo2.py:194][0m |          -0.0064 |           8.2419 |           1.8865 |
[32m[20230204 15:36:25 @agent_ppo2.py:194][0m |          -0.0068 |           7.7419 |           1.8855 |
[32m[20230204 15:36:25 @agent_ppo2.py:194][0m |          -0.0070 |           7.4194 |           1.8848 |
[32m[20230204 15:36:25 @agent_ppo2.py:194][0m |          -0.0077 |           7.0858 |           1.8841 |
[32m[20230204 15:36:26 @agent_ppo2.py:194][0m |          -0.0082 |           6.8656 |           1.8830 |
[32m[20230204 15:36:26 @agent_ppo2.py:194][0m |          -0.0086 |           6.6189 |           1.8817 |
[32m[20230204 15:36:26 @agent_ppo2.py:139][0m Policy update time: 1.27 s
[32m[20230204 15:36:26 @agent_ppo2.py:147][0m Average TRAINING episode reward: -107.43
[32m[20230204 15:36:26 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -94.44
[32m[20230204 15:36:26 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -23.69
[32m[20230204 15:36:26 @agent_ppo2.py:152][0m Total time:       0.26 min
[32m[20230204 15:36:26 @agent_ppo2.py:154][0m 16384 total steps have happened
[32m[20230204 15:36:26 @agent_ppo2.py:130][0m #------------------------ Iteration 8 --------------------------#
[32m[20230204 15:36:26 @agent_ppo2.py:136][0m Sampling time: 0.37 s by 4 slaves
[32m[20230204 15:36:27 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:27 @agent_ppo2.py:194][0m |           0.0003 |          10.6334 |           1.8971 |
[32m[20230204 15:36:27 @agent_ppo2.py:194][0m |          -0.0042 |           6.5739 |           1.8960 |
[32m[20230204 15:36:27 @agent_ppo2.py:194][0m |          -0.0050 |           5.6372 |           1.8939 |
[32m[20230204 15:36:27 @agent_ppo2.py:194][0m |          -0.0055 |           5.0682 |           1.8927 |
[32m[20230204 15:36:27 @agent_ppo2.py:194][0m |          -0.0038 |           4.8566 |           1.8911 |
[32m[20230204 15:36:27 @agent_ppo2.py:194][0m |          -0.0069 |           4.3804 |           1.8900 |
[32m[20230204 15:36:27 @agent_ppo2.py:194][0m |          -0.0065 |           4.1152 |           1.8888 |
[32m[20230204 15:36:27 @agent_ppo2.py:194][0m |          -0.0077 |           3.8590 |           1.8878 |
[32m[20230204 15:36:28 @agent_ppo2.py:194][0m |          -0.0065 |           3.7530 |           1.8867 |
[32m[20230204 15:36:28 @agent_ppo2.py:194][0m |          -0.0077 |           3.5063 |           1.8854 |
[32m[20230204 15:36:28 @agent_ppo2.py:139][0m Policy update time: 1.27 s
[32m[20230204 15:36:28 @agent_ppo2.py:147][0m Average TRAINING episode reward: -103.00
[32m[20230204 15:36:28 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -94.41
[32m[20230204 15:36:28 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -38.75
[32m[20230204 15:36:28 @agent_ppo2.py:152][0m Total time:       0.29 min
[32m[20230204 15:36:28 @agent_ppo2.py:154][0m 18432 total steps have happened
[32m[20230204 15:36:28 @agent_ppo2.py:130][0m #------------------------ Iteration 9 --------------------------#
[32m[20230204 15:36:28 @agent_ppo2.py:136][0m Sampling time: 0.37 s by 4 slaves
[32m[20230204 15:36:29 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:29 @agent_ppo2.py:194][0m |          -0.0006 |           5.8898 |           1.8890 |
[32m[20230204 15:36:29 @agent_ppo2.py:194][0m |          -0.0034 |           3.9005 |           1.8891 |
[32m[20230204 15:36:29 @agent_ppo2.py:194][0m |          -0.0053 |           3.3347 |           1.8897 |
[32m[20230204 15:36:29 @agent_ppo2.py:194][0m |          -0.0057 |           3.0297 |           1.8909 |
[32m[20230204 15:36:29 @agent_ppo2.py:194][0m |          -0.0077 |           2.8024 |           1.8916 |
[32m[20230204 15:36:29 @agent_ppo2.py:194][0m |          -0.0084 |           2.5868 |           1.8924 |
[32m[20230204 15:36:29 @agent_ppo2.py:194][0m |          -0.0087 |           2.4936 |           1.8929 |
[32m[20230204 15:36:29 @agent_ppo2.py:194][0m |          -0.0097 |           2.3777 |           1.8935 |
[32m[20230204 15:36:30 @agent_ppo2.py:194][0m |          -0.0107 |           2.2890 |           1.8938 |
[32m[20230204 15:36:30 @agent_ppo2.py:194][0m |          -0.0098 |           2.1693 |           1.8945 |
[32m[20230204 15:36:30 @agent_ppo2.py:139][0m Policy update time: 1.25 s
[32m[20230204 15:36:30 @agent_ppo2.py:147][0m Average TRAINING episode reward: -98.95
[32m[20230204 15:36:30 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -92.60
[32m[20230204 15:36:30 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -109.92
[32m[20230204 15:36:30 @agent_ppo2.py:152][0m Total time:       0.32 min
[32m[20230204 15:36:30 @agent_ppo2.py:154][0m 20480 total steps have happened
[32m[20230204 15:36:30 @agent_ppo2.py:130][0m #------------------------ Iteration 10 --------------------------#
[32m[20230204 15:36:30 @agent_ppo2.py:136][0m Sampling time: 0.35 s by 4 slaves
[32m[20230204 15:36:30 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:30 @agent_ppo2.py:194][0m |           0.0011 |           6.4933 |           1.9041 |
[32m[20230204 15:36:31 @agent_ppo2.py:194][0m |           0.0007 |           5.2200 |           1.9035 |
[32m[20230204 15:36:31 @agent_ppo2.py:194][0m |          -0.0021 |           4.5973 |           1.9033 |
[32m[20230204 15:36:31 @agent_ppo2.py:194][0m |          -0.0014 |           4.3512 |           1.9026 |
[32m[20230204 15:36:31 @agent_ppo2.py:194][0m |          -0.0049 |           4.3677 |           1.9018 |
[32m[20230204 15:36:31 @agent_ppo2.py:194][0m |          -0.0048 |           4.0675 |           1.9017 |
[32m[20230204 15:36:31 @agent_ppo2.py:194][0m |          -0.0048 |           3.9940 |           1.9007 |
[32m[20230204 15:36:31 @agent_ppo2.py:194][0m |          -0.0056 |           4.0144 |           1.9007 |
[32m[20230204 15:36:31 @agent_ppo2.py:194][0m |          -0.0063 |           3.7499 |           1.9000 |
[32m[20230204 15:36:31 @agent_ppo2.py:194][0m |          -0.0052 |           3.6375 |           1.9003 |
[32m[20230204 15:36:31 @agent_ppo2.py:139][0m Policy update time: 1.25 s
[32m[20230204 15:36:32 @agent_ppo2.py:147][0m Average TRAINING episode reward: -98.37
[32m[20230204 15:36:32 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -88.61
[32m[20230204 15:36:32 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -115.22
[32m[20230204 15:36:32 @agent_ppo2.py:152][0m Total time:       0.35 min
[32m[20230204 15:36:32 @agent_ppo2.py:154][0m 22528 total steps have happened
[32m[20230204 15:36:32 @agent_ppo2.py:130][0m #------------------------ Iteration 11 --------------------------#
[32m[20230204 15:36:32 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:36:32 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:32 @agent_ppo2.py:194][0m |          -0.0002 |           4.4477 |           1.9055 |
[32m[20230204 15:36:32 @agent_ppo2.py:194][0m |          -0.0024 |           3.7788 |           1.9045 |
[32m[20230204 15:36:32 @agent_ppo2.py:194][0m |          -0.0037 |           3.5447 |           1.9031 |
[32m[20230204 15:36:32 @agent_ppo2.py:194][0m |          -0.0045 |           3.4220 |           1.9025 |
[32m[20230204 15:36:33 @agent_ppo2.py:194][0m |          -0.0052 |           3.3381 |           1.9015 |
[32m[20230204 15:36:33 @agent_ppo2.py:194][0m |          -0.0056 |           3.3008 |           1.9009 |
[32m[20230204 15:36:33 @agent_ppo2.py:194][0m |          -0.0060 |           3.2827 |           1.9010 |
[32m[20230204 15:36:33 @agent_ppo2.py:194][0m |          -0.0063 |           3.1750 |           1.9003 |
[32m[20230204 15:36:33 @agent_ppo2.py:194][0m |          -0.0066 |           3.1854 |           1.9006 |
[32m[20230204 15:36:33 @agent_ppo2.py:194][0m |          -0.0070 |           3.1364 |           1.9001 |
[32m[20230204 15:36:33 @agent_ppo2.py:139][0m Policy update time: 1.21 s
[32m[20230204 15:36:34 @agent_ppo2.py:147][0m Average TRAINING episode reward: -87.27
[32m[20230204 15:36:34 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -85.03
[32m[20230204 15:36:34 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -28.08
[32m[20230204 15:36:34 @agent_ppo2.py:152][0m Total time:       0.38 min
[32m[20230204 15:36:34 @agent_ppo2.py:154][0m 24576 total steps have happened
[32m[20230204 15:36:34 @agent_ppo2.py:130][0m #------------------------ Iteration 12 --------------------------#
[32m[20230204 15:36:34 @agent_ppo2.py:136][0m Sampling time: 0.38 s by 4 slaves
[32m[20230204 15:36:34 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:34 @agent_ppo2.py:194][0m |          -0.0015 |           3.5738 |           1.9125 |
[32m[20230204 15:36:34 @agent_ppo2.py:194][0m |          -0.0066 |           2.9410 |           1.9098 |
[32m[20230204 15:36:34 @agent_ppo2.py:194][0m |          -0.0074 |           2.6171 |           1.9077 |
[32m[20230204 15:36:34 @agent_ppo2.py:194][0m |          -0.0084 |           2.4314 |           1.9061 |
[32m[20230204 15:36:35 @agent_ppo2.py:194][0m |          -0.0086 |           2.3723 |           1.9043 |
[32m[20230204 15:36:35 @agent_ppo2.py:194][0m |          -0.0100 |           2.2569 |           1.9038 |
[32m[20230204 15:36:35 @agent_ppo2.py:194][0m |          -0.0089 |           2.1555 |           1.9037 |
[32m[20230204 15:36:35 @agent_ppo2.py:194][0m |          -0.0104 |           2.0672 |           1.9034 |
[32m[20230204 15:36:35 @agent_ppo2.py:194][0m |          -0.0035 |           2.0697 |           1.9025 |
[32m[20230204 15:36:35 @agent_ppo2.py:194][0m |          -0.0109 |           2.0110 |           1.9016 |
[32m[20230204 15:36:35 @agent_ppo2.py:139][0m Policy update time: 1.26 s
[32m[20230204 15:36:35 @agent_ppo2.py:147][0m Average TRAINING episode reward: -89.71
[32m[20230204 15:36:35 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -72.64
[32m[20230204 15:36:35 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -110.15
[32m[20230204 15:36:35 @agent_ppo2.py:152][0m Total time:       0.41 min
[32m[20230204 15:36:35 @agent_ppo2.py:154][0m 26624 total steps have happened
[32m[20230204 15:36:35 @agent_ppo2.py:130][0m #------------------------ Iteration 13 --------------------------#
[32m[20230204 15:36:36 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:36:36 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:36 @agent_ppo2.py:194][0m |          -0.0007 |           5.9436 |           1.9164 |
[32m[20230204 15:36:36 @agent_ppo2.py:194][0m |          -0.0039 |           2.4876 |           1.9157 |
[32m[20230204 15:36:36 @agent_ppo2.py:194][0m |          -0.0050 |           1.8481 |           1.9149 |
[32m[20230204 15:36:36 @agent_ppo2.py:194][0m |          -0.0059 |           1.5901 |           1.9135 |
[32m[20230204 15:36:36 @agent_ppo2.py:194][0m |          -0.0065 |           1.4934 |           1.9130 |
[32m[20230204 15:36:36 @agent_ppo2.py:194][0m |          -0.0070 |           1.3681 |           1.9122 |
[32m[20230204 15:36:37 @agent_ppo2.py:194][0m |          -0.0076 |           1.3298 |           1.9117 |
[32m[20230204 15:36:37 @agent_ppo2.py:194][0m |          -0.0079 |           1.2712 |           1.9105 |
[32m[20230204 15:36:37 @agent_ppo2.py:194][0m |          -0.0082 |           1.2241 |           1.9102 |
[32m[20230204 15:36:37 @agent_ppo2.py:194][0m |          -0.0086 |           1.1908 |           1.9099 |
[32m[20230204 15:36:37 @agent_ppo2.py:139][0m Policy update time: 1.22 s
[32m[20230204 15:36:37 @agent_ppo2.py:147][0m Average TRAINING episode reward: -90.94
[32m[20230204 15:36:37 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -83.50
[32m[20230204 15:36:37 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -38.72
[32m[20230204 15:36:37 @agent_ppo2.py:152][0m Total time:       0.45 min
[32m[20230204 15:36:37 @agent_ppo2.py:154][0m 28672 total steps have happened
[32m[20230204 15:36:37 @agent_ppo2.py:130][0m #------------------------ Iteration 14 --------------------------#
[32m[20230204 15:36:38 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:36:38 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:38 @agent_ppo2.py:194][0m |          -0.0001 |           2.0718 |           1.9070 |
[32m[20230204 15:36:38 @agent_ppo2.py:194][0m |          -0.0039 |           1.4175 |           1.9067 |
[32m[20230204 15:36:38 @agent_ppo2.py:194][0m |          -0.0055 |           1.2909 |           1.9063 |
[32m[20230204 15:36:38 @agent_ppo2.py:194][0m |          -0.0066 |           1.2333 |           1.9056 |
[32m[20230204 15:36:38 @agent_ppo2.py:194][0m |          -0.0072 |           1.2036 |           1.9059 |
[32m[20230204 15:36:38 @agent_ppo2.py:194][0m |          -0.0078 |           1.1751 |           1.9068 |
[32m[20230204 15:36:38 @agent_ppo2.py:194][0m |          -0.0080 |           1.1462 |           1.9079 |
[32m[20230204 15:36:39 @agent_ppo2.py:194][0m |          -0.0084 |           1.1237 |           1.9082 |
[32m[20230204 15:36:39 @agent_ppo2.py:194][0m |          -0.0086 |           1.1551 |           1.9105 |
[32m[20230204 15:36:39 @agent_ppo2.py:194][0m |          -0.0089 |           1.1142 |           1.9119 |
[32m[20230204 15:36:39 @agent_ppo2.py:139][0m Policy update time: 1.21 s
[32m[20230204 15:36:39 @agent_ppo2.py:147][0m Average TRAINING episode reward: -66.88
[32m[20230204 15:36:39 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -63.75
[32m[20230204 15:36:39 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -110.77
[32m[20230204 15:36:39 @agent_ppo2.py:152][0m Total time:       0.47 min
[32m[20230204 15:36:39 @agent_ppo2.py:154][0m 30720 total steps have happened
[32m[20230204 15:36:39 @agent_ppo2.py:130][0m #------------------------ Iteration 15 --------------------------#
[32m[20230204 15:36:39 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:36:39 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:39 @agent_ppo2.py:194][0m |          -0.0014 |           1.2710 |           1.9367 |
[32m[20230204 15:36:40 @agent_ppo2.py:194][0m |          -0.0060 |           1.0260 |           1.9330 |
[32m[20230204 15:36:40 @agent_ppo2.py:194][0m |          -0.0077 |           0.9395 |           1.9304 |
[32m[20230204 15:36:40 @agent_ppo2.py:194][0m |          -0.0085 |           0.9044 |           1.9294 |
[32m[20230204 15:36:40 @agent_ppo2.py:194][0m |          -0.0090 |           0.8761 |           1.9298 |
[32m[20230204 15:36:40 @agent_ppo2.py:194][0m |          -0.0094 |           0.8678 |           1.9302 |
[32m[20230204 15:36:40 @agent_ppo2.py:194][0m |          -0.0097 |           0.8497 |           1.9304 |
[32m[20230204 15:36:40 @agent_ppo2.py:194][0m |          -0.0100 |           0.8388 |           1.9314 |
[32m[20230204 15:36:40 @agent_ppo2.py:194][0m |          -0.0102 |           0.8360 |           1.9317 |
[32m[20230204 15:36:40 @agent_ppo2.py:194][0m |          -0.0106 |           0.8254 |           1.9327 |
[32m[20230204 15:36:40 @agent_ppo2.py:139][0m Policy update time: 1.21 s
[32m[20230204 15:36:41 @agent_ppo2.py:147][0m Average TRAINING episode reward: -68.60
[32m[20230204 15:36:41 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -65.68
[32m[20230204 15:36:41 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -106.69
[32m[20230204 15:36:41 @agent_ppo2.py:152][0m Total time:       0.51 min
[32m[20230204 15:36:41 @agent_ppo2.py:154][0m 32768 total steps have happened
[32m[20230204 15:36:41 @agent_ppo2.py:130][0m #------------------------ Iteration 16 --------------------------#
[32m[20230204 15:36:41 @agent_ppo2.py:136][0m Sampling time: 0.37 s by 4 slaves
[32m[20230204 15:36:41 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:41 @agent_ppo2.py:194][0m |           0.0000 |           3.6956 |           1.9485 |
[32m[20230204 15:36:41 @agent_ppo2.py:194][0m |          -0.0020 |           1.9512 |           1.9474 |
[32m[20230204 15:36:42 @agent_ppo2.py:194][0m |          -0.0031 |           1.7464 |           1.9468 |
[32m[20230204 15:36:42 @agent_ppo2.py:194][0m |          -0.0043 |           1.6205 |           1.9453 |
[32m[20230204 15:36:42 @agent_ppo2.py:194][0m |          -0.0051 |           1.5607 |           1.9439 |
[32m[20230204 15:36:42 @agent_ppo2.py:194][0m |          -0.0048 |           1.4929 |           1.9427 |
[32m[20230204 15:36:42 @agent_ppo2.py:194][0m |          -0.0057 |           1.4640 |           1.9423 |
[32m[20230204 15:36:42 @agent_ppo2.py:194][0m |          -0.0063 |           1.4207 |           1.9410 |
[32m[20230204 15:36:42 @agent_ppo2.py:194][0m |          -0.0065 |           1.4270 |           1.9407 |
[32m[20230204 15:36:42 @agent_ppo2.py:194][0m |          -0.0072 |           1.3945 |           1.9397 |
[32m[20230204 15:36:42 @agent_ppo2.py:139][0m Policy update time: 1.21 s
[32m[20230204 15:36:43 @agent_ppo2.py:147][0m Average TRAINING episode reward: -80.33
[32m[20230204 15:36:43 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -61.79
[32m[20230204 15:36:43 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -66.04
[32m[20230204 15:36:43 @agent_ppo2.py:152][0m Total time:       0.54 min
[32m[20230204 15:36:43 @agent_ppo2.py:154][0m 34816 total steps have happened
[32m[20230204 15:36:43 @agent_ppo2.py:130][0m #------------------------ Iteration 17 --------------------------#
[32m[20230204 15:36:43 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:36:43 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:43 @agent_ppo2.py:194][0m |           0.0003 |           3.8528 |           1.9330 |
[32m[20230204 15:36:43 @agent_ppo2.py:194][0m |          -0.0019 |           3.2104 |           1.9319 |
[32m[20230204 15:36:43 @agent_ppo2.py:194][0m |          -0.0034 |           3.0637 |           1.9306 |
[32m[20230204 15:36:44 @agent_ppo2.py:194][0m |          -0.0046 |           2.9691 |           1.9285 |
[32m[20230204 15:36:44 @agent_ppo2.py:194][0m |          -0.0054 |           2.8618 |           1.9284 |
[32m[20230204 15:36:44 @agent_ppo2.py:194][0m |          -0.0057 |           2.9364 |           1.9276 |
[32m[20230204 15:36:44 @agent_ppo2.py:194][0m |          -0.0063 |           2.8298 |           1.9275 |
[32m[20230204 15:36:44 @agent_ppo2.py:194][0m |          -0.0070 |           2.8173 |           1.9274 |
[32m[20230204 15:36:44 @agent_ppo2.py:194][0m |          -0.0072 |           2.7259 |           1.9270 |
[32m[20230204 15:36:44 @agent_ppo2.py:194][0m |          -0.0074 |           2.8441 |           1.9272 |
[32m[20230204 15:36:44 @agent_ppo2.py:139][0m Policy update time: 1.22 s
[32m[20230204 15:36:44 @agent_ppo2.py:147][0m Average TRAINING episode reward: -69.56
[32m[20230204 15:36:44 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -58.43
[32m[20230204 15:36:44 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -111.72
[32m[20230204 15:36:44 @agent_ppo2.py:152][0m Total time:       0.57 min
[32m[20230204 15:36:44 @agent_ppo2.py:154][0m 36864 total steps have happened
[32m[20230204 15:36:44 @agent_ppo2.py:130][0m #------------------------ Iteration 18 --------------------------#
[32m[20230204 15:36:45 @agent_ppo2.py:136][0m Sampling time: 0.36 s by 4 slaves
[32m[20230204 15:36:45 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:45 @agent_ppo2.py:194][0m |           0.0001 |           3.2252 |           1.9381 |
[32m[20230204 15:36:45 @agent_ppo2.py:194][0m |          -0.0016 |           2.5122 |           1.9371 |
[32m[20230204 15:36:45 @agent_ppo2.py:194][0m |          -0.0031 |           2.3795 |           1.9368 |
[32m[20230204 15:36:45 @agent_ppo2.py:194][0m |          -0.0040 |           2.3238 |           1.9365 |
[32m[20230204 15:36:45 @agent_ppo2.py:194][0m |          -0.0044 |           2.2693 |           1.9373 |
[32m[20230204 15:36:45 @agent_ppo2.py:194][0m |          -0.0051 |           2.2180 |           1.9367 |
[32m[20230204 15:36:46 @agent_ppo2.py:194][0m |          -0.0056 |           2.2002 |           1.9384 |
[32m[20230204 15:36:46 @agent_ppo2.py:194][0m |          -0.0058 |           2.1786 |           1.9383 |
[32m[20230204 15:36:46 @agent_ppo2.py:194][0m |          -0.0062 |           2.1522 |           1.9391 |
[32m[20230204 15:36:46 @agent_ppo2.py:194][0m |          -0.0068 |           2.1147 |           1.9401 |
[32m[20230204 15:36:46 @agent_ppo2.py:139][0m Policy update time: 1.21 s
[32m[20230204 15:36:46 @agent_ppo2.py:147][0m Average TRAINING episode reward: -48.95
[32m[20230204 15:36:46 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -42.91
[32m[20230204 15:36:46 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -113.22
[32m[20230204 15:36:46 @agent_ppo2.py:152][0m Total time:       0.59 min
[32m[20230204 15:36:46 @agent_ppo2.py:154][0m 38912 total steps have happened
[32m[20230204 15:36:46 @agent_ppo2.py:130][0m #------------------------ Iteration 19 --------------------------#
[32m[20230204 15:36:46 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:36:47 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:47 @agent_ppo2.py:194][0m |          -0.0016 |           1.1851 |           2.0019 |
[32m[20230204 15:36:47 @agent_ppo2.py:194][0m |          -0.0055 |           0.9421 |           2.0029 |
[32m[20230204 15:36:47 @agent_ppo2.py:194][0m |          -0.0073 |           0.8798 |           2.0032 |
[32m[20230204 15:36:47 @agent_ppo2.py:194][0m |          -0.0085 |           0.8471 |           2.0037 |
[32m[20230204 15:36:47 @agent_ppo2.py:194][0m |          -0.0089 |           0.8298 |           2.0043 |
[32m[20230204 15:36:47 @agent_ppo2.py:194][0m |          -0.0095 |           0.8091 |           2.0057 |
[32m[20230204 15:36:47 @agent_ppo2.py:194][0m |          -0.0101 |           0.7970 |           2.0067 |
[32m[20230204 15:36:47 @agent_ppo2.py:194][0m |          -0.0105 |           0.7841 |           2.0080 |
[32m[20230204 15:36:48 @agent_ppo2.py:194][0m |          -0.0107 |           0.7703 |           2.0086 |
[32m[20230204 15:36:48 @agent_ppo2.py:194][0m |          -0.0108 |           0.7632 |           2.0097 |
[32m[20230204 15:36:48 @agent_ppo2.py:139][0m Policy update time: 1.23 s
[32m[20230204 15:36:48 @agent_ppo2.py:147][0m Average TRAINING episode reward: -48.76
[32m[20230204 15:36:48 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -34.11
[32m[20230204 15:36:48 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -107.23
[32m[20230204 15:36:48 @agent_ppo2.py:152][0m Total time:       0.63 min
[32m[20230204 15:36:48 @agent_ppo2.py:154][0m 40960 total steps have happened
[32m[20230204 15:36:48 @agent_ppo2.py:130][0m #------------------------ Iteration 20 --------------------------#
[32m[20230204 15:36:48 @agent_ppo2.py:136][0m Sampling time: 0.35 s by 4 slaves
[32m[20230204 15:36:49 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:49 @agent_ppo2.py:194][0m |          -0.0036 |           1.2277 |           1.9711 |
[32m[20230204 15:36:49 @agent_ppo2.py:194][0m |          -0.0075 |           1.0702 |           1.9706 |
[32m[20230204 15:36:49 @agent_ppo2.py:194][0m |          -0.0090 |           1.0417 |           1.9715 |
[32m[20230204 15:36:49 @agent_ppo2.py:194][0m |          -0.0096 |           0.9849 |           1.9724 |
[32m[20230204 15:36:49 @agent_ppo2.py:194][0m |          -0.0104 |           0.9611 |           1.9717 |
[32m[20230204 15:36:49 @agent_ppo2.py:194][0m |          -0.0108 |           0.9431 |           1.9732 |
[32m[20230204 15:36:49 @agent_ppo2.py:194][0m |          -0.0112 |           0.9267 |           1.9743 |
[32m[20230204 15:36:49 @agent_ppo2.py:194][0m |          -0.0118 |           0.9166 |           1.9748 |
[32m[20230204 15:36:49 @agent_ppo2.py:194][0m |          -0.0117 |           0.9032 |           1.9760 |
[32m[20230204 15:36:50 @agent_ppo2.py:194][0m |          -0.0124 |           0.8908 |           1.9769 |
[32m[20230204 15:36:50 @agent_ppo2.py:139][0m Policy update time: 1.19 s
[32m[20230204 15:36:50 @agent_ppo2.py:147][0m Average TRAINING episode reward: -45.13
[32m[20230204 15:36:50 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -37.50
[32m[20230204 15:36:50 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 8.15
[32m[20230204 15:36:50 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 8.15
[32m[20230204 15:36:50 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 8.15
[32m[20230204 15:36:50 @agent_ppo2.py:152][0m Total time:       0.66 min
[32m[20230204 15:36:50 @agent_ppo2.py:154][0m 43008 total steps have happened
[32m[20230204 15:36:50 @agent_ppo2.py:130][0m #------------------------ Iteration 21 --------------------------#
[32m[20230204 15:36:50 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:36:50 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:50 @agent_ppo2.py:194][0m |          -0.0022 |           0.8791 |           2.0074 |
[32m[20230204 15:36:51 @agent_ppo2.py:194][0m |          -0.0067 |           0.7917 |           2.0069 |
[32m[20230204 15:36:51 @agent_ppo2.py:194][0m |          -0.0082 |           0.7685 |           2.0073 |
[32m[20230204 15:36:51 @agent_ppo2.py:194][0m |          -0.0092 |           0.7566 |           2.0081 |
[32m[20230204 15:36:51 @agent_ppo2.py:194][0m |          -0.0097 |           0.7513 |           2.0098 |
[32m[20230204 15:36:51 @agent_ppo2.py:194][0m |          -0.0106 |           0.7428 |           2.0114 |
[32m[20230204 15:36:51 @agent_ppo2.py:194][0m |          -0.0110 |           0.7357 |           2.0125 |
[32m[20230204 15:36:51 @agent_ppo2.py:194][0m |          -0.0114 |           0.7281 |           2.0152 |
[32m[20230204 15:36:51 @agent_ppo2.py:194][0m |          -0.0120 |           0.7245 |           2.0164 |
[32m[20230204 15:36:51 @agent_ppo2.py:194][0m |          -0.0122 |           0.7199 |           2.0179 |
[32m[20230204 15:36:51 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:36:52 @agent_ppo2.py:147][0m Average TRAINING episode reward: -42.80
[32m[20230204 15:36:52 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -35.10
[32m[20230204 15:36:52 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -33.67
[32m[20230204 15:36:52 @agent_ppo2.py:152][0m Total time:       0.69 min
[32m[20230204 15:36:52 @agent_ppo2.py:154][0m 45056 total steps have happened
[32m[20230204 15:36:52 @agent_ppo2.py:130][0m #------------------------ Iteration 22 --------------------------#
[32m[20230204 15:36:52 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:36:52 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:52 @agent_ppo2.py:194][0m |          -0.0020 |           0.7691 |           2.0591 |
[32m[20230204 15:36:52 @agent_ppo2.py:194][0m |          -0.0076 |           0.7130 |           2.0597 |
[32m[20230204 15:36:52 @agent_ppo2.py:194][0m |          -0.0091 |           0.6933 |           2.0603 |
[32m[20230204 15:36:53 @agent_ppo2.py:194][0m |          -0.0098 |           0.6795 |           2.0632 |
[32m[20230204 15:36:53 @agent_ppo2.py:194][0m |          -0.0103 |           0.6722 |           2.0641 |
[32m[20230204 15:36:53 @agent_ppo2.py:194][0m |          -0.0109 |           0.6640 |           2.0650 |
[32m[20230204 15:36:53 @agent_ppo2.py:194][0m |          -0.0112 |           0.6568 |           2.0661 |
[32m[20230204 15:36:53 @agent_ppo2.py:194][0m |          -0.0115 |           0.6563 |           2.0671 |
[32m[20230204 15:36:53 @agent_ppo2.py:194][0m |          -0.0118 |           0.6509 |           2.0690 |
[32m[20230204 15:36:53 @agent_ppo2.py:194][0m |          -0.0121 |           0.6435 |           2.0697 |
[32m[20230204 15:36:53 @agent_ppo2.py:139][0m Policy update time: 1.19 s
[32m[20230204 15:36:54 @agent_ppo2.py:147][0m Average TRAINING episode reward: -24.56
[32m[20230204 15:36:54 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -18.59
[32m[20230204 15:36:54 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 52.27
[32m[20230204 15:36:54 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 52.27
[32m[20230204 15:36:54 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 52.27
[32m[20230204 15:36:54 @agent_ppo2.py:152][0m Total time:       0.72 min
[32m[20230204 15:36:54 @agent_ppo2.py:154][0m 47104 total steps have happened
[32m[20230204 15:36:54 @agent_ppo2.py:130][0m #------------------------ Iteration 23 --------------------------#
[32m[20230204 15:36:54 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:36:54 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:54 @agent_ppo2.py:194][0m |          -0.0019 |           0.7621 |           2.0808 |
[32m[20230204 15:36:54 @agent_ppo2.py:194][0m |          -0.0067 |           0.7183 |           2.0797 |
[32m[20230204 15:36:54 @agent_ppo2.py:194][0m |          -0.0080 |           0.7014 |           2.0810 |
[32m[20230204 15:36:54 @agent_ppo2.py:194][0m |          -0.0088 |           0.6937 |           2.0835 |
[32m[20230204 15:36:55 @agent_ppo2.py:194][0m |          -0.0098 |           0.6900 |           2.0861 |
[32m[20230204 15:36:55 @agent_ppo2.py:194][0m |          -0.0104 |           0.6837 |           2.0882 |
[32m[20230204 15:36:55 @agent_ppo2.py:194][0m |          -0.0111 |           0.6761 |           2.0912 |
[32m[20230204 15:36:55 @agent_ppo2.py:194][0m |          -0.0116 |           0.6713 |           2.0937 |
[32m[20230204 15:36:55 @agent_ppo2.py:194][0m |          -0.0119 |           0.6672 |           2.0944 |
[32m[20230204 15:36:55 @agent_ppo2.py:194][0m |          -0.0124 |           0.6646 |           2.0964 |
[32m[20230204 15:36:55 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:36:55 @agent_ppo2.py:147][0m Average TRAINING episode reward: -15.62
[32m[20230204 15:36:55 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: -8.11
[32m[20230204 15:36:55 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -97.18
[32m[20230204 15:36:55 @agent_ppo2.py:152][0m Total time:       0.75 min
[32m[20230204 15:36:55 @agent_ppo2.py:154][0m 49152 total steps have happened
[32m[20230204 15:36:55 @agent_ppo2.py:130][0m #------------------------ Iteration 24 --------------------------#
[32m[20230204 15:36:56 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:36:56 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:56 @agent_ppo2.py:194][0m |          -0.0017 |           1.0535 |           2.1545 |
[32m[20230204 15:36:56 @agent_ppo2.py:194][0m |          -0.0051 |           0.9294 |           2.1537 |
[32m[20230204 15:36:56 @agent_ppo2.py:194][0m |          -0.0061 |           0.8905 |           2.1535 |
[32m[20230204 15:36:56 @agent_ppo2.py:194][0m |          -0.0069 |           0.8738 |           2.1539 |
[32m[20230204 15:36:56 @agent_ppo2.py:194][0m |          -0.0076 |           0.8551 |           2.1540 |
[32m[20230204 15:36:56 @agent_ppo2.py:194][0m |          -0.0081 |           0.8463 |           2.1552 |
[32m[20230204 15:36:56 @agent_ppo2.py:194][0m |          -0.0085 |           0.8323 |           2.1557 |
[32m[20230204 15:36:57 @agent_ppo2.py:194][0m |          -0.0090 |           0.8228 |           2.1557 |
[32m[20230204 15:36:57 @agent_ppo2.py:194][0m |          -0.0092 |           0.8170 |           2.1562 |
[32m[20230204 15:36:57 @agent_ppo2.py:194][0m |          -0.0100 |           0.8095 |           2.1568 |
[32m[20230204 15:36:57 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:36:57 @agent_ppo2.py:147][0m Average TRAINING episode reward: -3.54
[32m[20230204 15:36:57 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 12.40
[32m[20230204 15:36:57 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -47.74
[32m[20230204 15:36:57 @agent_ppo2.py:152][0m Total time:       0.78 min
[32m[20230204 15:36:57 @agent_ppo2.py:154][0m 51200 total steps have happened
[32m[20230204 15:36:57 @agent_ppo2.py:130][0m #------------------------ Iteration 25 --------------------------#
[32m[20230204 15:36:58 @agent_ppo2.py:136][0m Sampling time: 0.38 s by 4 slaves
[32m[20230204 15:36:58 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:58 @agent_ppo2.py:194][0m |          -0.0021 |           0.8512 |           2.1956 |
[32m[20230204 15:36:58 @agent_ppo2.py:194][0m |          -0.0062 |           0.7536 |           2.1938 |
[32m[20230204 15:36:58 @agent_ppo2.py:194][0m |          -0.0074 |           0.7377 |           2.1960 |
[32m[20230204 15:36:58 @agent_ppo2.py:194][0m |          -0.0084 |           0.7261 |           2.1968 |
[32m[20230204 15:36:58 @agent_ppo2.py:194][0m |          -0.0091 |           0.7168 |           2.1978 |
[32m[20230204 15:36:58 @agent_ppo2.py:194][0m |          -0.0097 |           0.7081 |           2.1981 |
[32m[20230204 15:36:58 @agent_ppo2.py:194][0m |          -0.0101 |           0.6988 |           2.2002 |
[32m[20230204 15:36:58 @agent_ppo2.py:194][0m |          -0.0105 |           0.6941 |           2.2000 |
[32m[20230204 15:36:59 @agent_ppo2.py:194][0m |          -0.0107 |           0.6877 |           2.2006 |
[32m[20230204 15:36:59 @agent_ppo2.py:194][0m |          -0.0110 |           0.6802 |           2.2009 |
[32m[20230204 15:36:59 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:36:59 @agent_ppo2.py:147][0m Average TRAINING episode reward: 10.09
[32m[20230204 15:36:59 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 17.33
[32m[20230204 15:36:59 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -94.17
[32m[20230204 15:36:59 @agent_ppo2.py:152][0m Total time:       0.81 min
[32m[20230204 15:36:59 @agent_ppo2.py:154][0m 53248 total steps have happened
[32m[20230204 15:36:59 @agent_ppo2.py:130][0m #------------------------ Iteration 26 --------------------------#
[32m[20230204 15:36:59 @agent_ppo2.py:136][0m Sampling time: 0.36 s by 4 slaves
[32m[20230204 15:36:59 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:36:59 @agent_ppo2.py:194][0m |          -0.0021 |          21.1688 |           2.1613 |
[32m[20230204 15:37:00 @agent_ppo2.py:194][0m |          -0.0049 |           6.0295 |           2.1602 |
[32m[20230204 15:37:00 @agent_ppo2.py:194][0m |          -0.0067 |           4.4795 |           2.1600 |
[32m[20230204 15:37:00 @agent_ppo2.py:194][0m |          -0.0069 |           3.9218 |           2.1585 |
[32m[20230204 15:37:00 @agent_ppo2.py:194][0m |          -0.0082 |           3.5155 |           2.1581 |
[32m[20230204 15:37:00 @agent_ppo2.py:194][0m |          -0.0083 |           3.2879 |           2.1570 |
[32m[20230204 15:37:00 @agent_ppo2.py:194][0m |          -0.0085 |           3.1447 |           2.1560 |
[32m[20230204 15:37:00 @agent_ppo2.py:194][0m |          -0.0014 |           2.9805 |           2.1559 |
[32m[20230204 15:37:00 @agent_ppo2.py:194][0m |          -0.0096 |           2.8449 |           2.1558 |
[32m[20230204 15:37:00 @agent_ppo2.py:194][0m |          -0.0104 |           2.6887 |           2.1555 |
[32m[20230204 15:37:00 @agent_ppo2.py:139][0m Policy update time: 1.29 s
[32m[20230204 15:37:01 @agent_ppo2.py:147][0m Average TRAINING episode reward: -55.62
[32m[20230204 15:37:01 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 34.38
[32m[20230204 15:37:01 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -92.12
[32m[20230204 15:37:01 @agent_ppo2.py:152][0m Total time:       0.84 min
[32m[20230204 15:37:01 @agent_ppo2.py:154][0m 55296 total steps have happened
[32m[20230204 15:37:01 @agent_ppo2.py:130][0m #------------------------ Iteration 27 --------------------------#
[32m[20230204 15:37:01 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:37:01 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:01 @agent_ppo2.py:194][0m |          -0.0000 |           4.3367 |           2.2409 |
[32m[20230204 15:37:01 @agent_ppo2.py:194][0m |          -0.0029 |           2.5627 |           2.2377 |
[32m[20230204 15:37:01 @agent_ppo2.py:194][0m |          -0.0042 |           2.2118 |           2.2393 |
[32m[20230204 15:37:01 @agent_ppo2.py:194][0m |          -0.0049 |           1.9849 |           2.2411 |
[32m[20230204 15:37:02 @agent_ppo2.py:194][0m |          -0.0057 |           1.8521 |           2.2416 |
[32m[20230204 15:37:02 @agent_ppo2.py:194][0m |          -0.0064 |           1.8078 |           2.2420 |
[32m[20230204 15:37:02 @agent_ppo2.py:194][0m |          -0.0070 |           1.7348 |           2.2433 |
[32m[20230204 15:37:02 @agent_ppo2.py:194][0m |          -0.0071 |           1.6942 |           2.2449 |
[32m[20230204 15:37:02 @agent_ppo2.py:194][0m |          -0.0075 |           1.6602 |           2.2468 |
[32m[20230204 15:37:02 @agent_ppo2.py:194][0m |          -0.0078 |           1.6333 |           2.2476 |
[32m[20230204 15:37:02 @agent_ppo2.py:139][0m Policy update time: 1.12 s
[32m[20230204 15:37:02 @agent_ppo2.py:147][0m Average TRAINING episode reward: 1.56
[32m[20230204 15:37:02 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 49.42
[32m[20230204 15:37:02 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -10.45
[32m[20230204 15:37:02 @agent_ppo2.py:152][0m Total time:       0.87 min
[32m[20230204 15:37:02 @agent_ppo2.py:154][0m 57344 total steps have happened
[32m[20230204 15:37:02 @agent_ppo2.py:130][0m #------------------------ Iteration 28 --------------------------#
[32m[20230204 15:37:03 @agent_ppo2.py:136][0m Sampling time: 0.37 s by 4 slaves
[32m[20230204 15:37:03 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:03 @agent_ppo2.py:194][0m |           0.0013 |           7.0400 |           2.2184 |
[32m[20230204 15:37:03 @agent_ppo2.py:194][0m |          -0.0048 |           5.5684 |           2.2177 |
[32m[20230204 15:37:03 @agent_ppo2.py:194][0m |          -0.0060 |           5.0096 |           2.2173 |
[32m[20230204 15:37:03 @agent_ppo2.py:194][0m |          -0.0069 |           4.6378 |           2.2178 |
[32m[20230204 15:37:03 @agent_ppo2.py:194][0m |          -0.0078 |           4.4492 |           2.2177 |
[32m[20230204 15:37:04 @agent_ppo2.py:194][0m |          -0.0081 |           4.3067 |           2.2172 |
[32m[20230204 15:37:04 @agent_ppo2.py:194][0m |          -0.0086 |           4.2093 |           2.2174 |
[32m[20230204 15:37:04 @agent_ppo2.py:194][0m |          -0.0091 |           4.0590 |           2.2184 |
[32m[20230204 15:37:04 @agent_ppo2.py:194][0m |          -0.0081 |           4.0100 |           2.2184 |
[32m[20230204 15:37:04 @agent_ppo2.py:194][0m |          -0.0099 |           3.9185 |           2.2191 |
[32m[20230204 15:37:04 @agent_ppo2.py:139][0m Policy update time: 1.27 s
[32m[20230204 15:37:04 @agent_ppo2.py:147][0m Average TRAINING episode reward: -10.35
[32m[20230204 15:37:04 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 45.13
[32m[20230204 15:37:04 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 72.22
[32m[20230204 15:37:04 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 72.22
[32m[20230204 15:37:04 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 72.22
[32m[20230204 15:37:04 @agent_ppo2.py:152][0m Total time:       0.90 min
[32m[20230204 15:37:04 @agent_ppo2.py:154][0m 59392 total steps have happened
[32m[20230204 15:37:04 @agent_ppo2.py:130][0m #------------------------ Iteration 29 --------------------------#
[32m[20230204 15:37:05 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:05 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:05 @agent_ppo2.py:194][0m |          -0.0006 |           1.4428 |           2.2247 |
[32m[20230204 15:37:05 @agent_ppo2.py:194][0m |          -0.0035 |           1.2356 |           2.2246 |
[32m[20230204 15:37:05 @agent_ppo2.py:194][0m |          -0.0048 |           1.2047 |           2.2254 |
[32m[20230204 15:37:05 @agent_ppo2.py:194][0m |          -0.0059 |           1.1789 |           2.2263 |
[32m[20230204 15:37:05 @agent_ppo2.py:194][0m |          -0.0065 |           1.1666 |           2.2267 |
[32m[20230204 15:37:05 @agent_ppo2.py:194][0m |          -0.0072 |           1.1518 |           2.2282 |
[32m[20230204 15:37:06 @agent_ppo2.py:194][0m |          -0.0076 |           1.1433 |           2.2286 |
[32m[20230204 15:37:06 @agent_ppo2.py:194][0m |          -0.0079 |           1.1457 |           2.2302 |
[32m[20230204 15:37:06 @agent_ppo2.py:194][0m |          -0.0083 |           1.1254 |           2.2310 |
[32m[20230204 15:37:06 @agent_ppo2.py:194][0m |          -0.0088 |           1.1132 |           2.2322 |
[32m[20230204 15:37:06 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:37:06 @agent_ppo2.py:147][0m Average TRAINING episode reward: 45.52
[32m[20230204 15:37:06 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 55.67
[32m[20230204 15:37:06 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 2.14
[32m[20230204 15:37:06 @agent_ppo2.py:152][0m Total time:       0.93 min
[32m[20230204 15:37:06 @agent_ppo2.py:154][0m 61440 total steps have happened
[32m[20230204 15:37:06 @agent_ppo2.py:130][0m #------------------------ Iteration 30 --------------------------#
[32m[20230204 15:37:07 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:07 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:07 @agent_ppo2.py:194][0m |          -0.0007 |           1.2155 |           2.2711 |
[32m[20230204 15:37:07 @agent_ppo2.py:194][0m |          -0.0051 |           1.0254 |           2.2692 |
[32m[20230204 15:37:07 @agent_ppo2.py:194][0m |          -0.0063 |           0.9881 |           2.2689 |
[32m[20230204 15:37:07 @agent_ppo2.py:194][0m |          -0.0071 |           0.9721 |           2.2694 |
[32m[20230204 15:37:07 @agent_ppo2.py:194][0m |          -0.0079 |           0.9670 |           2.2702 |
[32m[20230204 15:37:07 @agent_ppo2.py:194][0m |          -0.0083 |           0.9447 |           2.2702 |
[32m[20230204 15:37:07 @agent_ppo2.py:194][0m |          -0.0088 |           0.9347 |           2.2699 |
[32m[20230204 15:37:08 @agent_ppo2.py:194][0m |          -0.0093 |           0.9285 |           2.2704 |
[32m[20230204 15:37:08 @agent_ppo2.py:194][0m |          -0.0100 |           0.9210 |           2.2707 |
[32m[20230204 15:37:08 @agent_ppo2.py:194][0m |          -0.0103 |           0.9119 |           2.2712 |
[32m[20230204 15:37:08 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:37:08 @agent_ppo2.py:147][0m Average TRAINING episode reward: 61.19
[32m[20230204 15:37:08 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 67.54
[32m[20230204 15:37:08 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 78.91
[32m[20230204 15:37:08 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 78.91
[32m[20230204 15:37:08 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 78.91
[32m[20230204 15:37:08 @agent_ppo2.py:152][0m Total time:       0.96 min
[32m[20230204 15:37:08 @agent_ppo2.py:154][0m 63488 total steps have happened
[32m[20230204 15:37:08 @agent_ppo2.py:130][0m #------------------------ Iteration 31 --------------------------#
[32m[20230204 15:37:08 @agent_ppo2.py:136][0m Sampling time: 0.35 s by 4 slaves
[32m[20230204 15:37:09 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:09 @agent_ppo2.py:194][0m |          -0.0002 |           3.3068 |           2.2697 |
[32m[20230204 15:37:09 @agent_ppo2.py:194][0m |          -0.0029 |           2.0564 |           2.2669 |
[32m[20230204 15:37:09 @agent_ppo2.py:194][0m |          -0.0035 |           1.7102 |           2.2689 |
[32m[20230204 15:37:09 @agent_ppo2.py:194][0m |          -0.0057 |           1.4444 |           2.2674 |
[32m[20230204 15:37:09 @agent_ppo2.py:194][0m |          -0.0063 |           1.4022 |           2.2677 |
[32m[20230204 15:37:09 @agent_ppo2.py:194][0m |          -0.0070 |           1.3704 |           2.2675 |
[32m[20230204 15:37:09 @agent_ppo2.py:194][0m |          -0.0075 |           1.2562 |           2.2695 |
[32m[20230204 15:37:09 @agent_ppo2.py:194][0m |          -0.0083 |           1.2250 |           2.2695 |
[32m[20230204 15:37:10 @agent_ppo2.py:194][0m |          -0.0085 |           1.1956 |           2.2694 |
[32m[20230204 15:37:10 @agent_ppo2.py:194][0m |          -0.0092 |           1.1926 |           2.2715 |
[32m[20230204 15:37:10 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:37:10 @agent_ppo2.py:147][0m Average TRAINING episode reward: 35.25
[32m[20230204 15:37:10 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 73.87
[32m[20230204 15:37:10 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 84.89
[32m[20230204 15:37:10 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 84.89
[32m[20230204 15:37:10 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 84.89
[32m[20230204 15:37:10 @agent_ppo2.py:152][0m Total time:       0.99 min
[32m[20230204 15:37:10 @agent_ppo2.py:154][0m 65536 total steps have happened
[32m[20230204 15:37:10 @agent_ppo2.py:130][0m #------------------------ Iteration 32 --------------------------#
[32m[20230204 15:37:10 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:10 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:10 @agent_ppo2.py:194][0m |          -0.0006 |           6.5626 |           2.3157 |
[32m[20230204 15:37:11 @agent_ppo2.py:194][0m |          -0.0046 |           5.0376 |           2.3120 |
[32m[20230204 15:37:11 @agent_ppo2.py:194][0m |          -0.0057 |           4.5689 |           2.3136 |
[32m[20230204 15:37:11 @agent_ppo2.py:194][0m |          -0.0068 |           4.4041 |           2.3139 |
[32m[20230204 15:37:11 @agent_ppo2.py:194][0m |          -0.0077 |           4.2589 |           2.3142 |
[32m[20230204 15:37:11 @agent_ppo2.py:194][0m |          -0.0080 |           4.0802 |           2.3144 |
[32m[20230204 15:37:11 @agent_ppo2.py:194][0m |          -0.0083 |           4.0159 |           2.3145 |
[32m[20230204 15:37:11 @agent_ppo2.py:194][0m |          -0.0088 |           3.9281 |           2.3172 |
[32m[20230204 15:37:11 @agent_ppo2.py:194][0m |          -0.0094 |           3.8288 |           2.3161 |
[32m[20230204 15:37:11 @agent_ppo2.py:194][0m |          -0.0098 |           3.7554 |           2.3176 |
[32m[20230204 15:37:11 @agent_ppo2.py:139][0m Policy update time: 1.12 s
[32m[20230204 15:37:12 @agent_ppo2.py:147][0m Average TRAINING episode reward: 45.91
[32m[20230204 15:37:12 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 90.48
[32m[20230204 15:37:12 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -6.99
[32m[20230204 15:37:12 @agent_ppo2.py:152][0m Total time:       1.02 min
[32m[20230204 15:37:12 @agent_ppo2.py:154][0m 67584 total steps have happened
[32m[20230204 15:37:12 @agent_ppo2.py:130][0m #------------------------ Iteration 33 --------------------------#
[32m[20230204 15:37:12 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:12 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:12 @agent_ppo2.py:194][0m |           0.0004 |           8.5394 |           2.3266 |
[32m[20230204 15:37:12 @agent_ppo2.py:194][0m |          -0.0012 |           2.5912 |           2.3237 |
[32m[20230204 15:37:12 @agent_ppo2.py:194][0m |          -0.0049 |           2.0137 |           2.3231 |
[32m[20230204 15:37:13 @agent_ppo2.py:194][0m |          -0.0037 |           1.7439 |           2.3220 |
[32m[20230204 15:37:13 @agent_ppo2.py:194][0m |          -0.0057 |           1.5612 |           2.3202 |
[32m[20230204 15:37:13 @agent_ppo2.py:194][0m |          -0.0052 |           1.4160 |           2.3185 |
[32m[20230204 15:37:13 @agent_ppo2.py:194][0m |          -0.0077 |           1.3246 |           2.3196 |
[32m[20230204 15:37:13 @agent_ppo2.py:194][0m |          -0.0049 |           1.2559 |           2.3184 |
[32m[20230204 15:37:13 @agent_ppo2.py:194][0m |          -0.0073 |           1.2071 |           2.3184 |
[32m[20230204 15:37:13 @agent_ppo2.py:194][0m |          -0.0079 |           1.1502 |           2.3163 |
[32m[20230204 15:37:13 @agent_ppo2.py:139][0m Policy update time: 1.21 s
[32m[20230204 15:37:14 @agent_ppo2.py:147][0m Average TRAINING episode reward: 41.44
[32m[20230204 15:37:14 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 93.41
[32m[20230204 15:37:14 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 89.94
[32m[20230204 15:37:14 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 89.94
[32m[20230204 15:37:14 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 89.94
[32m[20230204 15:37:14 @agent_ppo2.py:152][0m Total time:       1.05 min
[32m[20230204 15:37:14 @agent_ppo2.py:154][0m 69632 total steps have happened
[32m[20230204 15:37:14 @agent_ppo2.py:130][0m #------------------------ Iteration 34 --------------------------#
[32m[20230204 15:37:14 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:14 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:14 @agent_ppo2.py:194][0m |          -0.0011 |           4.6720 |           2.2926 |
[32m[20230204 15:37:14 @agent_ppo2.py:194][0m |          -0.0051 |           3.6294 |           2.2919 |
[32m[20230204 15:37:14 @agent_ppo2.py:194][0m |          -0.0066 |           3.3709 |           2.2935 |
[32m[20230204 15:37:14 @agent_ppo2.py:194][0m |          -0.0083 |           3.2158 |           2.2932 |
[32m[20230204 15:37:15 @agent_ppo2.py:194][0m |          -0.0088 |           3.0262 |           2.2940 |
[32m[20230204 15:37:15 @agent_ppo2.py:194][0m |          -0.0096 |           2.8571 |           2.2934 |
[32m[20230204 15:37:15 @agent_ppo2.py:194][0m |          -0.0091 |           2.7814 |           2.2940 |
[32m[20230204 15:37:15 @agent_ppo2.py:194][0m |          -0.0100 |           2.6942 |           2.2940 |
[32m[20230204 15:37:15 @agent_ppo2.py:194][0m |          -0.0104 |           2.6744 |           2.2932 |
[32m[20230204 15:37:15 @agent_ppo2.py:194][0m |          -0.0105 |           2.5678 |           2.2947 |
[32m[20230204 15:37:15 @agent_ppo2.py:139][0m Policy update time: 1.16 s
[32m[20230204 15:37:15 @agent_ppo2.py:147][0m Average TRAINING episode reward: 52.83
[32m[20230204 15:37:15 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 84.56
[32m[20230204 15:37:15 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 121.16
[32m[20230204 15:37:15 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 121.16
[32m[20230204 15:37:15 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 121.16
[32m[20230204 15:37:15 @agent_ppo2.py:152][0m Total time:       1.08 min
[32m[20230204 15:37:15 @agent_ppo2.py:154][0m 71680 total steps have happened
[32m[20230204 15:37:15 @agent_ppo2.py:130][0m #------------------------ Iteration 35 --------------------------#
[32m[20230204 15:37:16 @agent_ppo2.py:136][0m Sampling time: 0.39 s by 4 slaves
[32m[20230204 15:37:16 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:16 @agent_ppo2.py:194][0m |          -0.0007 |           4.1636 |           2.3146 |
[32m[20230204 15:37:16 @agent_ppo2.py:194][0m |          -0.0037 |           2.7382 |           2.3121 |
[32m[20230204 15:37:16 @agent_ppo2.py:194][0m |          -0.0051 |           2.4760 |           2.3110 |
[32m[20230204 15:37:16 @agent_ppo2.py:194][0m |          -0.0059 |           2.2913 |           2.3103 |
[32m[20230204 15:37:16 @agent_ppo2.py:194][0m |          -0.0066 |           2.2051 |           2.3104 |
[32m[20230204 15:37:17 @agent_ppo2.py:194][0m |          -0.0070 |           2.1334 |           2.3104 |
[32m[20230204 15:37:17 @agent_ppo2.py:194][0m |          -0.0070 |           2.0772 |           2.3105 |
[32m[20230204 15:37:17 @agent_ppo2.py:194][0m |          -0.0072 |           2.0586 |           2.3098 |
[32m[20230204 15:37:17 @agent_ppo2.py:194][0m |          -0.0078 |           2.0073 |           2.3109 |
[32m[20230204 15:37:17 @agent_ppo2.py:194][0m |          -0.0081 |           1.9552 |           2.3097 |
[32m[20230204 15:37:17 @agent_ppo2.py:139][0m Policy update time: 1.28 s
[32m[20230204 15:37:17 @agent_ppo2.py:147][0m Average TRAINING episode reward: 51.97
[32m[20230204 15:37:17 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 103.27
[32m[20230204 15:37:17 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 95.11
[32m[20230204 15:37:17 @agent_ppo2.py:152][0m Total time:       1.12 min
[32m[20230204 15:37:17 @agent_ppo2.py:154][0m 73728 total steps have happened
[32m[20230204 15:37:17 @agent_ppo2.py:130][0m #------------------------ Iteration 36 --------------------------#
[32m[20230204 15:37:18 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:18 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:18 @agent_ppo2.py:194][0m |          -0.0002 |           3.6683 |           2.2732 |
[32m[20230204 15:37:18 @agent_ppo2.py:194][0m |          -0.0026 |           2.8032 |           2.2687 |
[32m[20230204 15:37:18 @agent_ppo2.py:194][0m |          -0.0030 |           2.5728 |           2.2680 |
[32m[20230204 15:37:18 @agent_ppo2.py:194][0m |          -0.0039 |           2.3468 |           2.2692 |
[32m[20230204 15:37:18 @agent_ppo2.py:194][0m |          -0.0047 |           2.2430 |           2.2663 |
[32m[20230204 15:37:18 @agent_ppo2.py:194][0m |          -0.0057 |           2.1615 |           2.2658 |
[32m[20230204 15:37:19 @agent_ppo2.py:194][0m |          -0.0064 |           2.1053 |           2.2659 |
[32m[20230204 15:37:19 @agent_ppo2.py:194][0m |          -0.0060 |           2.0557 |           2.2647 |
[32m[20230204 15:37:19 @agent_ppo2.py:194][0m |          -0.0064 |           2.0701 |           2.2652 |
[32m[20230204 15:37:19 @agent_ppo2.py:194][0m |          -0.0062 |           1.9639 |           2.2645 |
[32m[20230204 15:37:19 @agent_ppo2.py:139][0m Policy update time: 1.17 s
[32m[20230204 15:37:19 @agent_ppo2.py:147][0m Average TRAINING episode reward: 48.75
[32m[20230204 15:37:19 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 91.80
[32m[20230204 15:37:19 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 121.37
[32m[20230204 15:37:19 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 121.37
[32m[20230204 15:37:19 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 121.37
[32m[20230204 15:37:19 @agent_ppo2.py:152][0m Total time:       1.15 min
[32m[20230204 15:37:19 @agent_ppo2.py:154][0m 75776 total steps have happened
[32m[20230204 15:37:19 @agent_ppo2.py:130][0m #------------------------ Iteration 37 --------------------------#
[32m[20230204 15:37:20 @agent_ppo2.py:136][0m Sampling time: 0.38 s by 4 slaves
[32m[20230204 15:37:20 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:20 @agent_ppo2.py:194][0m |          -0.0029 |           7.6292 |           2.3044 |
[32m[20230204 15:37:20 @agent_ppo2.py:194][0m |          -0.0052 |           6.4222 |           2.3015 |
[32m[20230204 15:37:20 @agent_ppo2.py:194][0m |          -0.0075 |           6.1737 |           2.3015 |
[32m[20230204 15:37:20 @agent_ppo2.py:194][0m |          -0.0082 |           6.0297 |           2.3028 |
[32m[20230204 15:37:20 @agent_ppo2.py:194][0m |          -0.0084 |           5.8609 |           2.3025 |
[32m[20230204 15:37:20 @agent_ppo2.py:194][0m |          -0.0092 |           5.8041 |           2.3028 |
[32m[20230204 15:37:20 @agent_ppo2.py:194][0m |          -0.0095 |           5.6480 |           2.3035 |
[32m[20230204 15:37:20 @agent_ppo2.py:194][0m |          -0.0094 |           5.6677 |           2.3042 |
[32m[20230204 15:37:20 @agent_ppo2.py:194][0m |          -0.0106 |           5.5480 |           2.3031 |
[32m[20230204 15:37:21 @agent_ppo2.py:194][0m |          -0.0103 |           5.4671 |           2.3050 |
[32m[20230204 15:37:21 @agent_ppo2.py:139][0m Policy update time: 1.01 s
[32m[20230204 15:37:21 @agent_ppo2.py:147][0m Average TRAINING episode reward: 45.92
[32m[20230204 15:37:21 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 102.44
[32m[20230204 15:37:21 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 131.36
[32m[20230204 15:37:21 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 131.36
[32m[20230204 15:37:21 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 131.36
[32m[20230204 15:37:21 @agent_ppo2.py:152][0m Total time:       1.17 min
[32m[20230204 15:37:21 @agent_ppo2.py:154][0m 77824 total steps have happened
[32m[20230204 15:37:21 @agent_ppo2.py:130][0m #------------------------ Iteration 38 --------------------------#
[32m[20230204 15:37:21 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:21 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:21 @agent_ppo2.py:194][0m |          -0.0007 |           1.4188 |           2.2907 |
[32m[20230204 15:37:22 @agent_ppo2.py:194][0m |          -0.0042 |           1.2590 |           2.2883 |
[32m[20230204 15:37:22 @agent_ppo2.py:194][0m |          -0.0058 |           1.2153 |           2.2878 |
[32m[20230204 15:37:22 @agent_ppo2.py:194][0m |          -0.0068 |           1.1892 |           2.2884 |
[32m[20230204 15:37:22 @agent_ppo2.py:194][0m |          -0.0075 |           1.1709 |           2.2878 |
[32m[20230204 15:37:22 @agent_ppo2.py:194][0m |          -0.0080 |           1.1552 |           2.2899 |
[32m[20230204 15:37:22 @agent_ppo2.py:194][0m |          -0.0086 |           1.1408 |           2.2903 |
[32m[20230204 15:37:22 @agent_ppo2.py:194][0m |          -0.0090 |           1.1295 |           2.2904 |
[32m[20230204 15:37:22 @agent_ppo2.py:194][0m |          -0.0093 |           1.1215 |           2.2902 |
[32m[20230204 15:37:22 @agent_ppo2.py:194][0m |          -0.0097 |           1.1147 |           2.2922 |
[32m[20230204 15:37:22 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:37:23 @agent_ppo2.py:147][0m Average TRAINING episode reward: 103.62
[32m[20230204 15:37:23 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 110.77
[32m[20230204 15:37:23 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 53.32
[32m[20230204 15:37:23 @agent_ppo2.py:152][0m Total time:       1.21 min
[32m[20230204 15:37:23 @agent_ppo2.py:154][0m 79872 total steps have happened
[32m[20230204 15:37:23 @agent_ppo2.py:130][0m #------------------------ Iteration 39 --------------------------#
[32m[20230204 15:37:23 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:23 @agent_ppo2.py:194][0m |          -0.0006 |           2.2431 |           2.3384 |
[32m[20230204 15:37:23 @agent_ppo2.py:194][0m |          -0.0048 |           1.9838 |           2.3379 |
[32m[20230204 15:37:24 @agent_ppo2.py:194][0m |          -0.0061 |           1.8818 |           2.3363 |
[32m[20230204 15:37:24 @agent_ppo2.py:194][0m |          -0.0074 |           1.8396 |           2.3392 |
[32m[20230204 15:37:24 @agent_ppo2.py:194][0m |          -0.0079 |           1.7838 |           2.3406 |
[32m[20230204 15:37:24 @agent_ppo2.py:194][0m |          -0.0084 |           1.7456 |           2.3428 |
[32m[20230204 15:37:24 @agent_ppo2.py:194][0m |          -0.0090 |           1.7148 |           2.3441 |
[32m[20230204 15:37:24 @agent_ppo2.py:194][0m |          -0.0098 |           1.6837 |           2.3438 |
[32m[20230204 15:37:24 @agent_ppo2.py:194][0m |          -0.0098 |           1.6553 |           2.3476 |
[32m[20230204 15:37:24 @agent_ppo2.py:194][0m |          -0.0103 |           1.6367 |           2.3464 |
[32m[20230204 15:37:24 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:37:25 @agent_ppo2.py:147][0m Average TRAINING episode reward: 92.38
[32m[20230204 15:37:25 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 104.73
[32m[20230204 15:37:25 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 180.28
[32m[20230204 15:37:25 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 180.28
[32m[20230204 15:37:25 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 180.28
[32m[20230204 15:37:25 @agent_ppo2.py:152][0m Total time:       1.24 min
[32m[20230204 15:37:25 @agent_ppo2.py:154][0m 81920 total steps have happened
[32m[20230204 15:37:25 @agent_ppo2.py:130][0m #------------------------ Iteration 40 --------------------------#
[32m[20230204 15:37:25 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:25 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:25 @agent_ppo2.py:194][0m |           0.0001 |          11.7673 |           2.3379 |
[32m[20230204 15:37:25 @agent_ppo2.py:194][0m |          -0.0029 |           3.6971 |           2.3350 |
[32m[20230204 15:37:25 @agent_ppo2.py:194][0m |          -0.0042 |           3.1834 |           2.3324 |
[32m[20230204 15:37:25 @agent_ppo2.py:194][0m |          -0.0053 |           2.7631 |           2.3291 |
[32m[20230204 15:37:26 @agent_ppo2.py:194][0m |          -0.0061 |           2.5719 |           2.3292 |
[32m[20230204 15:37:26 @agent_ppo2.py:194][0m |          -0.0063 |           2.4175 |           2.3277 |
[32m[20230204 15:37:26 @agent_ppo2.py:194][0m |          -0.0069 |           2.3494 |           2.3266 |
[32m[20230204 15:37:26 @agent_ppo2.py:194][0m |          -0.0074 |           2.2321 |           2.3245 |
[32m[20230204 15:37:26 @agent_ppo2.py:194][0m |          -0.0072 |           2.1755 |           2.3229 |
[32m[20230204 15:37:26 @agent_ppo2.py:194][0m |          -0.0082 |           2.0521 |           2.3221 |
[32m[20230204 15:37:26 @agent_ppo2.py:139][0m Policy update time: 1.07 s
[32m[20230204 15:37:26 @agent_ppo2.py:147][0m Average TRAINING episode reward: 10.86
[32m[20230204 15:37:26 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 137.34
[32m[20230204 15:37:26 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 177.31
[32m[20230204 15:37:26 @agent_ppo2.py:152][0m Total time:       1.27 min
[32m[20230204 15:37:26 @agent_ppo2.py:154][0m 83968 total steps have happened
[32m[20230204 15:37:26 @agent_ppo2.py:130][0m #------------------------ Iteration 41 --------------------------#
[32m[20230204 15:37:27 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:27 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:27 @agent_ppo2.py:194][0m |           0.0021 |          13.8303 |           2.3081 |
[32m[20230204 15:37:27 @agent_ppo2.py:194][0m |          -0.0026 |           7.8362 |           2.3053 |
[32m[20230204 15:37:27 @agent_ppo2.py:194][0m |          -0.0044 |           6.3427 |           2.3012 |
[32m[20230204 15:37:27 @agent_ppo2.py:194][0m |          -0.0053 |           5.5279 |           2.3012 |
[32m[20230204 15:37:27 @agent_ppo2.py:194][0m |          -0.0054 |           5.2552 |           2.3002 |
[32m[20230204 15:37:27 @agent_ppo2.py:194][0m |          -0.0065 |           4.6796 |           2.3001 |
[32m[20230204 15:37:27 @agent_ppo2.py:194][0m |          -0.0072 |           4.5169 |           2.3021 |
[32m[20230204 15:37:28 @agent_ppo2.py:194][0m |          -0.0070 |           4.4346 |           2.3010 |
[32m[20230204 15:37:28 @agent_ppo2.py:194][0m |          -0.0083 |           4.2259 |           2.3004 |
[32m[20230204 15:37:28 @agent_ppo2.py:194][0m |          -0.0092 |           3.9415 |           2.3007 |
[32m[20230204 15:37:28 @agent_ppo2.py:139][0m Policy update time: 1.07 s
[32m[20230204 15:37:28 @agent_ppo2.py:147][0m Average TRAINING episode reward: 1.85
[32m[20230204 15:37:28 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 106.74
[32m[20230204 15:37:28 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 174.54
[32m[20230204 15:37:28 @agent_ppo2.py:152][0m Total time:       1.29 min
[32m[20230204 15:37:28 @agent_ppo2.py:154][0m 86016 total steps have happened
[32m[20230204 15:37:28 @agent_ppo2.py:130][0m #------------------------ Iteration 42 --------------------------#
[32m[20230204 15:37:28 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:29 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:29 @agent_ppo2.py:194][0m |          -0.0015 |           2.0397 |           2.3560 |
[32m[20230204 15:37:29 @agent_ppo2.py:194][0m |          -0.0052 |           1.8322 |           2.3554 |
[32m[20230204 15:37:29 @agent_ppo2.py:194][0m |          -0.0067 |           1.7759 |           2.3566 |
[32m[20230204 15:37:29 @agent_ppo2.py:194][0m |          -0.0081 |           1.7369 |           2.3556 |
[32m[20230204 15:37:29 @agent_ppo2.py:194][0m |          -0.0092 |           1.7030 |           2.3593 |
[32m[20230204 15:37:29 @agent_ppo2.py:194][0m |          -0.0096 |           1.6703 |           2.3602 |
[32m[20230204 15:37:29 @agent_ppo2.py:194][0m |          -0.0101 |           1.6531 |           2.3610 |
[32m[20230204 15:37:29 @agent_ppo2.py:194][0m |          -0.0101 |           1.6268 |           2.3638 |
[32m[20230204 15:37:29 @agent_ppo2.py:194][0m |          -0.0105 |           1.6126 |           2.3646 |
[32m[20230204 15:37:30 @agent_ppo2.py:194][0m |          -0.0110 |           1.5957 |           2.3660 |
[32m[20230204 15:37:30 @agent_ppo2.py:139][0m Policy update time: 1.19 s
[32m[20230204 15:37:30 @agent_ppo2.py:147][0m Average TRAINING episode reward: 104.59
[32m[20230204 15:37:30 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 111.29
[32m[20230204 15:37:30 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 152.53
[32m[20230204 15:37:30 @agent_ppo2.py:152][0m Total time:       1.32 min
[32m[20230204 15:37:30 @agent_ppo2.py:154][0m 88064 total steps have happened
[32m[20230204 15:37:30 @agent_ppo2.py:130][0m #------------------------ Iteration 43 --------------------------#
[32m[20230204 15:37:30 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:30 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:30 @agent_ppo2.py:194][0m |          -0.0005 |           4.9476 |           2.3463 |
[32m[20230204 15:37:31 @agent_ppo2.py:194][0m |          -0.0042 |           3.6459 |           2.3436 |
[32m[20230204 15:37:31 @agent_ppo2.py:194][0m |          -0.0058 |           3.3654 |           2.3417 |
[32m[20230204 15:37:31 @agent_ppo2.py:194][0m |          -0.0053 |           3.1860 |           2.3400 |
[32m[20230204 15:37:31 @agent_ppo2.py:194][0m |          -0.0078 |           3.0452 |           2.3386 |
[32m[20230204 15:37:31 @agent_ppo2.py:194][0m |          -0.0091 |           2.9315 |           2.3373 |
[32m[20230204 15:37:31 @agent_ppo2.py:194][0m |          -0.0084 |           2.8414 |           2.3365 |
[32m[20230204 15:37:31 @agent_ppo2.py:194][0m |          -0.0023 |           2.8409 |           2.3360 |
[32m[20230204 15:37:31 @agent_ppo2.py:194][0m |          -0.0100 |           2.8263 |           2.3330 |
[32m[20230204 15:37:31 @agent_ppo2.py:194][0m |          -0.0094 |           2.6907 |           2.3331 |
[32m[20230204 15:37:31 @agent_ppo2.py:139][0m Policy update time: 1.19 s
[32m[20230204 15:37:32 @agent_ppo2.py:147][0m Average TRAINING episode reward: 91.12
[32m[20230204 15:37:32 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 127.12
[32m[20230204 15:37:32 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 210.18
[32m[20230204 15:37:32 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 210.18
[32m[20230204 15:37:32 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 210.18
[32m[20230204 15:37:32 @agent_ppo2.py:152][0m Total time:       1.35 min
[32m[20230204 15:37:32 @agent_ppo2.py:154][0m 90112 total steps have happened
[32m[20230204 15:37:32 @agent_ppo2.py:130][0m #------------------------ Iteration 44 --------------------------#
[32m[20230204 15:37:32 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:32 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:32 @agent_ppo2.py:194][0m |          -0.0014 |          19.2283 |           2.3575 |
[32m[20230204 15:37:32 @agent_ppo2.py:194][0m |          -0.0047 |          10.5217 |           2.3561 |
[32m[20230204 15:37:32 @agent_ppo2.py:194][0m |          -0.0066 |           8.3974 |           2.3547 |
[32m[20230204 15:37:33 @agent_ppo2.py:194][0m |          -0.0072 |           7.6488 |           2.3546 |
[32m[20230204 15:37:33 @agent_ppo2.py:194][0m |          -0.0082 |           6.9112 |           2.3529 |
[32m[20230204 15:37:33 @agent_ppo2.py:194][0m |          -0.0092 |           6.4981 |           2.3520 |
[32m[20230204 15:37:33 @agent_ppo2.py:194][0m |          -0.0095 |           5.9210 |           2.3532 |
[32m[20230204 15:37:33 @agent_ppo2.py:194][0m |          -0.0103 |           5.7035 |           2.3532 |
[32m[20230204 15:37:33 @agent_ppo2.py:194][0m |          -0.0105 |           5.3476 |           2.3525 |
[32m[20230204 15:37:33 @agent_ppo2.py:194][0m |          -0.0108 |           5.1421 |           2.3532 |
[32m[20230204 15:37:33 @agent_ppo2.py:139][0m Policy update time: 1.02 s
[32m[20230204 15:37:33 @agent_ppo2.py:147][0m Average TRAINING episode reward: 51.56
[32m[20230204 15:37:33 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 141.05
[32m[20230204 15:37:33 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 38.14
[32m[20230204 15:37:33 @agent_ppo2.py:152][0m Total time:       1.38 min
[32m[20230204 15:37:33 @agent_ppo2.py:154][0m 92160 total steps have happened
[32m[20230204 15:37:33 @agent_ppo2.py:130][0m #------------------------ Iteration 45 --------------------------#
[32m[20230204 15:37:34 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:34 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:34 @agent_ppo2.py:194][0m |          -0.0013 |           4.2231 |           2.3558 |
[32m[20230204 15:37:34 @agent_ppo2.py:194][0m |          -0.0039 |           2.7562 |           2.3530 |
[32m[20230204 15:37:34 @agent_ppo2.py:194][0m |          -0.0054 |           2.5256 |           2.3528 |
[32m[20230204 15:37:34 @agent_ppo2.py:194][0m |          -0.0059 |           2.4017 |           2.3505 |
[32m[20230204 15:37:34 @agent_ppo2.py:194][0m |          -0.0072 |           2.3117 |           2.3507 |
[32m[20230204 15:37:34 @agent_ppo2.py:194][0m |          -0.0082 |           2.2207 |           2.3514 |
[32m[20230204 15:37:34 @agent_ppo2.py:194][0m |          -0.0076 |           2.1719 |           2.3495 |
[32m[20230204 15:37:35 @agent_ppo2.py:194][0m |          -0.0083 |           2.1421 |           2.3489 |
[32m[20230204 15:37:35 @agent_ppo2.py:194][0m |          -0.0074 |           2.0610 |           2.3498 |
[32m[20230204 15:37:35 @agent_ppo2.py:194][0m |          -0.0086 |           2.0426 |           2.3497 |
[32m[20230204 15:37:35 @agent_ppo2.py:139][0m Policy update time: 1.05 s
[32m[20230204 15:37:35 @agent_ppo2.py:147][0m Average TRAINING episode reward: 87.96
[32m[20230204 15:37:35 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 145.31
[32m[20230204 15:37:35 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 189.96
[32m[20230204 15:37:35 @agent_ppo2.py:152][0m Total time:       1.41 min
[32m[20230204 15:37:35 @agent_ppo2.py:154][0m 94208 total steps have happened
[32m[20230204 15:37:35 @agent_ppo2.py:130][0m #------------------------ Iteration 46 --------------------------#
[32m[20230204 15:37:35 @agent_ppo2.py:136][0m Sampling time: 0.35 s by 4 slaves
[32m[20230204 15:37:36 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:36 @agent_ppo2.py:194][0m |          -0.0041 |           2.6488 |           2.3517 |
[32m[20230204 15:37:36 @agent_ppo2.py:194][0m |          -0.0044 |           1.7810 |           2.3530 |
[32m[20230204 15:37:36 @agent_ppo2.py:194][0m |          -0.0001 |           1.6178 |           2.3543 |
[32m[20230204 15:37:36 @agent_ppo2.py:194][0m |          -0.0025 |           1.6557 |           2.3536 |
[32m[20230204 15:37:36 @agent_ppo2.py:194][0m |          -0.0113 |           1.5219 |           2.3544 |
[32m[20230204 15:37:36 @agent_ppo2.py:194][0m |          -0.0089 |           1.4814 |           2.3557 |
[32m[20230204 15:37:36 @agent_ppo2.py:194][0m |          -0.0100 |           1.4577 |           2.3575 |
[32m[20230204 15:37:36 @agent_ppo2.py:194][0m |          -0.0104 |           1.4223 |           2.3572 |
[32m[20230204 15:37:37 @agent_ppo2.py:194][0m |          -0.0109 |           1.4143 |           2.3590 |
[32m[20230204 15:37:37 @agent_ppo2.py:194][0m |          -0.0124 |           1.3985 |           2.3594 |
[32m[20230204 15:37:37 @agent_ppo2.py:139][0m Policy update time: 1.24 s
[32m[20230204 15:37:37 @agent_ppo2.py:147][0m Average TRAINING episode reward: 92.66
[32m[20230204 15:37:37 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 147.31
[32m[20230204 15:37:37 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 185.88
[32m[20230204 15:37:37 @agent_ppo2.py:152][0m Total time:       1.44 min
[32m[20230204 15:37:37 @agent_ppo2.py:154][0m 96256 total steps have happened
[32m[20230204 15:37:37 @agent_ppo2.py:130][0m #------------------------ Iteration 47 --------------------------#
[32m[20230204 15:37:37 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:37 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:38 @agent_ppo2.py:194][0m |          -0.0018 |           1.3858 |           2.3433 |
[32m[20230204 15:37:38 @agent_ppo2.py:194][0m |          -0.0053 |           1.2547 |           2.3453 |
[32m[20230204 15:37:38 @agent_ppo2.py:194][0m |          -0.0069 |           1.2347 |           2.3487 |
[32m[20230204 15:37:38 @agent_ppo2.py:194][0m |          -0.0075 |           1.2147 |           2.3500 |
[32m[20230204 15:37:38 @agent_ppo2.py:194][0m |          -0.0087 |           1.1970 |           2.3527 |
[32m[20230204 15:37:38 @agent_ppo2.py:194][0m |          -0.0093 |           1.1856 |           2.3550 |
[32m[20230204 15:37:38 @agent_ppo2.py:194][0m |          -0.0096 |           1.1726 |           2.3562 |
[32m[20230204 15:37:38 @agent_ppo2.py:194][0m |          -0.0100 |           1.1641 |           2.3588 |
[32m[20230204 15:37:38 @agent_ppo2.py:194][0m |          -0.0105 |           1.1518 |           2.3599 |
[32m[20230204 15:37:39 @agent_ppo2.py:194][0m |          -0.0109 |           1.1388 |           2.3603 |
[32m[20230204 15:37:39 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:37:39 @agent_ppo2.py:147][0m Average TRAINING episode reward: 124.94
[32m[20230204 15:37:39 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 138.04
[32m[20230204 15:37:39 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 228.18
[32m[20230204 15:37:39 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 228.18
[32m[20230204 15:37:39 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 228.18
[32m[20230204 15:37:39 @agent_ppo2.py:152][0m Total time:       1.47 min
[32m[20230204 15:37:39 @agent_ppo2.py:154][0m 98304 total steps have happened
[32m[20230204 15:37:39 @agent_ppo2.py:130][0m #------------------------ Iteration 48 --------------------------#
[32m[20230204 15:37:39 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:39 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:39 @agent_ppo2.py:194][0m |           0.0005 |          14.2594 |           2.4542 |
[32m[20230204 15:37:39 @agent_ppo2.py:194][0m |          -0.0018 |          10.7917 |           2.4535 |
[32m[20230204 15:37:40 @agent_ppo2.py:194][0m |          -0.0027 |           9.7741 |           2.4509 |
[32m[20230204 15:37:40 @agent_ppo2.py:194][0m |          -0.0034 |           9.1901 |           2.4499 |
[32m[20230204 15:37:40 @agent_ppo2.py:194][0m |          -0.0042 |           8.5206 |           2.4503 |
[32m[20230204 15:37:40 @agent_ppo2.py:194][0m |          -0.0046 |           8.0132 |           2.4487 |
[32m[20230204 15:37:40 @agent_ppo2.py:194][0m |          -0.0053 |           7.9725 |           2.4486 |
[32m[20230204 15:37:40 @agent_ppo2.py:194][0m |          -0.0060 |           7.4530 |           2.4487 |
[32m[20230204 15:37:40 @agent_ppo2.py:194][0m |          -0.0058 |           7.3763 |           2.4473 |
[32m[20230204 15:37:40 @agent_ppo2.py:194][0m |          -0.0063 |           7.1963 |           2.4478 |
[32m[20230204 15:37:40 @agent_ppo2.py:139][0m Policy update time: 1.03 s
[32m[20230204 15:37:40 @agent_ppo2.py:147][0m Average TRAINING episode reward: 77.57
[32m[20230204 15:37:40 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 154.40
[32m[20230204 15:37:40 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 216.51
[32m[20230204 15:37:40 @agent_ppo2.py:152][0m Total time:       1.50 min
[32m[20230204 15:37:40 @agent_ppo2.py:154][0m 100352 total steps have happened
[32m[20230204 15:37:41 @agent_ppo2.py:130][0m #------------------------ Iteration 49 --------------------------#
[32m[20230204 15:37:41 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:37:41 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:41 @agent_ppo2.py:194][0m |           0.0010 |           8.9692 |           2.4528 |
[32m[20230204 15:37:41 @agent_ppo2.py:194][0m |          -0.0027 |           7.1098 |           2.4494 |
[32m[20230204 15:37:41 @agent_ppo2.py:194][0m |          -0.0038 |           6.4298 |           2.4487 |
[32m[20230204 15:37:41 @agent_ppo2.py:194][0m |          -0.0049 |           6.1799 |           2.4494 |
[32m[20230204 15:37:41 @agent_ppo2.py:194][0m |          -0.0054 |           5.8493 |           2.4477 |
[32m[20230204 15:37:42 @agent_ppo2.py:194][0m |          -0.0056 |           5.7007 |           2.4468 |
[32m[20230204 15:37:42 @agent_ppo2.py:194][0m |          -0.0062 |           5.6092 |           2.4461 |
[32m[20230204 15:37:42 @agent_ppo2.py:194][0m |          -0.0063 |           5.4685 |           2.4461 |
[32m[20230204 15:37:42 @agent_ppo2.py:194][0m |          -0.0069 |           5.4422 |           2.4447 |
[32m[20230204 15:37:42 @agent_ppo2.py:194][0m |          -0.0069 |           5.3732 |           2.4455 |
[32m[20230204 15:37:42 @agent_ppo2.py:139][0m Policy update time: 1.17 s
[32m[20230204 15:37:42 @agent_ppo2.py:147][0m Average TRAINING episode reward: 94.36
[32m[20230204 15:37:42 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 139.12
[32m[20230204 15:37:42 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 198.37
[32m[20230204 15:37:42 @agent_ppo2.py:152][0m Total time:       1.53 min
[32m[20230204 15:37:42 @agent_ppo2.py:154][0m 102400 total steps have happened
[32m[20230204 15:37:42 @agent_ppo2.py:130][0m #------------------------ Iteration 50 --------------------------#
[32m[20230204 15:37:43 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:37:43 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:43 @agent_ppo2.py:194][0m |           0.0005 |           2.5503 |           2.4267 |
[32m[20230204 15:37:43 @agent_ppo2.py:194][0m |          -0.0028 |           2.2512 |           2.4227 |
[32m[20230204 15:37:43 @agent_ppo2.py:194][0m |          -0.0043 |           2.1762 |           2.4193 |
[32m[20230204 15:37:43 @agent_ppo2.py:194][0m |          -0.0054 |           2.1240 |           2.4154 |
[32m[20230204 15:37:43 @agent_ppo2.py:194][0m |          -0.0059 |           2.0964 |           2.4133 |
[32m[20230204 15:37:43 @agent_ppo2.py:194][0m |          -0.0064 |           2.0703 |           2.4124 |
[32m[20230204 15:37:44 @agent_ppo2.py:194][0m |          -0.0069 |           2.0482 |           2.4094 |
[32m[20230204 15:37:44 @agent_ppo2.py:194][0m |          -0.0076 |           2.0297 |           2.4087 |
[32m[20230204 15:37:44 @agent_ppo2.py:194][0m |          -0.0077 |           2.0119 |           2.4069 |
[32m[20230204 15:37:44 @agent_ppo2.py:194][0m |          -0.0082 |           1.9966 |           2.4056 |
[32m[20230204 15:37:44 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:37:44 @agent_ppo2.py:147][0m Average TRAINING episode reward: 122.99
[32m[20230204 15:37:44 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 133.18
[32m[20230204 15:37:44 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 225.59
[32m[20230204 15:37:44 @agent_ppo2.py:152][0m Total time:       1.56 min
[32m[20230204 15:37:44 @agent_ppo2.py:154][0m 104448 total steps have happened
[32m[20230204 15:37:44 @agent_ppo2.py:130][0m #------------------------ Iteration 51 --------------------------#
[32m[20230204 15:37:45 @agent_ppo2.py:136][0m Sampling time: 0.38 s by 4 slaves
[32m[20230204 15:37:45 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:45 @agent_ppo2.py:194][0m |          -0.0004 |           3.1428 |           2.4105 |
[32m[20230204 15:37:45 @agent_ppo2.py:194][0m |          -0.0040 |           2.5981 |           2.4064 |
[32m[20230204 15:37:45 @agent_ppo2.py:194][0m |          -0.0054 |           2.3887 |           2.4053 |
[32m[20230204 15:37:45 @agent_ppo2.py:194][0m |          -0.0062 |           2.2790 |           2.4052 |
[32m[20230204 15:37:45 @agent_ppo2.py:194][0m |          -0.0067 |           2.2573 |           2.4049 |
[32m[20230204 15:37:45 @agent_ppo2.py:194][0m |          -0.0078 |           2.1627 |           2.4039 |
[32m[20230204 15:37:45 @agent_ppo2.py:194][0m |          -0.0080 |           2.1100 |           2.4059 |
[32m[20230204 15:37:46 @agent_ppo2.py:194][0m |          -0.0082 |           2.0587 |           2.4054 |
[32m[20230204 15:37:46 @agent_ppo2.py:194][0m |          -0.0086 |           2.0426 |           2.4046 |
[32m[20230204 15:37:46 @agent_ppo2.py:194][0m |          -0.0089 |           2.0316 |           2.4046 |
[32m[20230204 15:37:46 @agent_ppo2.py:139][0m Policy update time: 1.29 s
[32m[20230204 15:37:46 @agent_ppo2.py:147][0m Average TRAINING episode reward: 112.70
[32m[20230204 15:37:46 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 170.28
[32m[20230204 15:37:46 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 231.58
[32m[20230204 15:37:46 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 231.58
[32m[20230204 15:37:46 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 231.58
[32m[20230204 15:37:46 @agent_ppo2.py:152][0m Total time:       1.59 min
[32m[20230204 15:37:46 @agent_ppo2.py:154][0m 106496 total steps have happened
[32m[20230204 15:37:46 @agent_ppo2.py:130][0m #------------------------ Iteration 52 --------------------------#
[32m[20230204 15:37:46 @agent_ppo2.py:136][0m Sampling time: 0.36 s by 4 slaves
[32m[20230204 15:37:47 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:47 @agent_ppo2.py:194][0m |           0.0002 |          12.3160 |           2.4213 |
[32m[20230204 15:37:47 @agent_ppo2.py:194][0m |          -0.0026 |           6.3703 |           2.4182 |
[32m[20230204 15:37:47 @agent_ppo2.py:194][0m |          -0.0038 |           5.7173 |           2.4170 |
[32m[20230204 15:37:47 @agent_ppo2.py:194][0m |          -0.0050 |           5.3426 |           2.4185 |
[32m[20230204 15:37:47 @agent_ppo2.py:194][0m |          -0.0062 |           4.9874 |           2.4177 |
[32m[20230204 15:37:47 @agent_ppo2.py:194][0m |          -0.0063 |           4.7162 |           2.4177 |
[32m[20230204 15:37:47 @agent_ppo2.py:194][0m |          -0.0066 |           4.5881 |           2.4177 |
[32m[20230204 15:37:47 @agent_ppo2.py:194][0m |          -0.0074 |           4.4192 |           2.4182 |
[32m[20230204 15:37:48 @agent_ppo2.py:194][0m |          -0.0075 |           4.2655 |           2.4176 |
[32m[20230204 15:37:48 @agent_ppo2.py:194][0m |          -0.0077 |           4.1797 |           2.4180 |
[32m[20230204 15:37:48 @agent_ppo2.py:139][0m Policy update time: 1.13 s
[32m[20230204 15:37:48 @agent_ppo2.py:147][0m Average TRAINING episode reward: 30.35
[32m[20230204 15:37:48 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 157.11
[32m[20230204 15:37:48 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -127.02
[32m[20230204 15:37:48 @agent_ppo2.py:152][0m Total time:       1.62 min
[32m[20230204 15:37:48 @agent_ppo2.py:154][0m 108544 total steps have happened
[32m[20230204 15:37:48 @agent_ppo2.py:130][0m #------------------------ Iteration 53 --------------------------#
[32m[20230204 15:37:48 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:37:48 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:48 @agent_ppo2.py:194][0m |          -0.0009 |          17.5953 |           2.4211 |
[32m[20230204 15:37:48 @agent_ppo2.py:194][0m |          -0.0049 |          13.2579 |           2.4212 |
[32m[20230204 15:37:48 @agent_ppo2.py:194][0m |          -0.0070 |          12.0338 |           2.4208 |
[32m[20230204 15:37:49 @agent_ppo2.py:194][0m |          -0.0081 |          11.5973 |           2.4187 |
[32m[20230204 15:37:49 @agent_ppo2.py:194][0m |          -0.0043 |          11.0307 |           2.4176 |
[32m[20230204 15:37:49 @agent_ppo2.py:194][0m |          -0.0080 |          10.6110 |           2.4161 |
[32m[20230204 15:37:49 @agent_ppo2.py:194][0m |          -0.0096 |          10.6228 |           2.4160 |
[32m[20230204 15:37:49 @agent_ppo2.py:194][0m |          -0.0063 |          10.2513 |           2.4138 |
[32m[20230204 15:37:49 @agent_ppo2.py:194][0m |          -0.0034 |          10.1857 |           2.4152 |
[32m[20230204 15:37:49 @agent_ppo2.py:194][0m |          -0.0040 |           9.9136 |           2.4122 |
[32m[20230204 15:37:49 @agent_ppo2.py:139][0m Policy update time: 1.05 s
[32m[20230204 15:37:49 @agent_ppo2.py:147][0m Average TRAINING episode reward: 78.54
[32m[20230204 15:37:49 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 152.95
[32m[20230204 15:37:49 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -125.59
[32m[20230204 15:37:49 @agent_ppo2.py:152][0m Total time:       1.65 min
[32m[20230204 15:37:49 @agent_ppo2.py:154][0m 110592 total steps have happened
[32m[20230204 15:37:49 @agent_ppo2.py:130][0m #------------------------ Iteration 54 --------------------------#
[32m[20230204 15:37:50 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:50 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:50 @agent_ppo2.py:194][0m |          -0.0015 |          28.3313 |           2.4068 |
[32m[20230204 15:37:50 @agent_ppo2.py:194][0m |          -0.0058 |          20.0151 |           2.4052 |
[32m[20230204 15:37:50 @agent_ppo2.py:194][0m |          -0.0076 |          17.5027 |           2.4043 |
[32m[20230204 15:37:50 @agent_ppo2.py:194][0m |          -0.0087 |          15.6281 |           2.4052 |
[32m[20230204 15:37:50 @agent_ppo2.py:194][0m |          -0.0102 |          14.8827 |           2.4045 |
[32m[20230204 15:37:50 @agent_ppo2.py:194][0m |          -0.0111 |          14.2952 |           2.4045 |
[32m[20230204 15:37:50 @agent_ppo2.py:194][0m |          -0.0112 |          13.2014 |           2.4046 |
[32m[20230204 15:37:51 @agent_ppo2.py:194][0m |          -0.0119 |          13.0928 |           2.4038 |
[32m[20230204 15:37:51 @agent_ppo2.py:194][0m |          -0.0119 |          12.3388 |           2.4057 |
[32m[20230204 15:37:51 @agent_ppo2.py:194][0m |          -0.0124 |          12.0904 |           2.4058 |
[32m[20230204 15:37:51 @agent_ppo2.py:139][0m Policy update time: 1.17 s
[32m[20230204 15:37:51 @agent_ppo2.py:147][0m Average TRAINING episode reward: 23.25
[32m[20230204 15:37:51 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 173.34
[32m[20230204 15:37:51 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -116.37
[32m[20230204 15:37:51 @agent_ppo2.py:152][0m Total time:       1.67 min
[32m[20230204 15:37:51 @agent_ppo2.py:154][0m 112640 total steps have happened
[32m[20230204 15:37:51 @agent_ppo2.py:130][0m #------------------------ Iteration 55 --------------------------#
[32m[20230204 15:37:51 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:37:51 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:51 @agent_ppo2.py:194][0m |          -0.0011 |          18.8497 |           2.4064 |
[32m[20230204 15:37:51 @agent_ppo2.py:194][0m |          -0.0053 |          14.5837 |           2.4034 |
[32m[20230204 15:37:52 @agent_ppo2.py:194][0m |          -0.0067 |          13.6877 |           2.4021 |
[32m[20230204 15:37:52 @agent_ppo2.py:194][0m |          -0.0087 |          13.1739 |           2.4042 |
[32m[20230204 15:37:52 @agent_ppo2.py:194][0m |          -0.0095 |          12.9635 |           2.4032 |
[32m[20230204 15:37:52 @agent_ppo2.py:194][0m |          -0.0101 |          12.1560 |           2.4039 |
[32m[20230204 15:37:52 @agent_ppo2.py:194][0m |          -0.0113 |          11.7435 |           2.4040 |
[32m[20230204 15:37:52 @agent_ppo2.py:194][0m |          -0.0111 |          11.2377 |           2.4052 |
[32m[20230204 15:37:52 @agent_ppo2.py:194][0m |          -0.0119 |          11.1263 |           2.4064 |
[32m[20230204 15:37:52 @agent_ppo2.py:194][0m |          -0.0134 |          10.6175 |           2.4085 |
[32m[20230204 15:37:52 @agent_ppo2.py:139][0m Policy update time: 1.03 s
[32m[20230204 15:37:52 @agent_ppo2.py:147][0m Average TRAINING episode reward: 100.08
[32m[20230204 15:37:52 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 157.95
[32m[20230204 15:37:52 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -106.37
[32m[20230204 15:37:52 @agent_ppo2.py:152][0m Total time:       1.70 min
[32m[20230204 15:37:52 @agent_ppo2.py:154][0m 114688 total steps have happened
[32m[20230204 15:37:52 @agent_ppo2.py:130][0m #------------------------ Iteration 56 --------------------------#
[32m[20230204 15:37:53 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:37:53 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:53 @agent_ppo2.py:194][0m |           0.0012 |           2.3826 |           2.4314 |
[32m[20230204 15:37:53 @agent_ppo2.py:194][0m |          -0.0011 |           1.7919 |           2.4303 |
[32m[20230204 15:37:53 @agent_ppo2.py:194][0m |          -0.0031 |           1.6847 |           2.4332 |
[32m[20230204 15:37:53 @agent_ppo2.py:194][0m |          -0.0044 |           1.6207 |           2.4354 |
[32m[20230204 15:37:53 @agent_ppo2.py:194][0m |          -0.0056 |           1.5752 |           2.4360 |
[32m[20230204 15:37:53 @agent_ppo2.py:194][0m |          -0.0060 |           1.5407 |           2.4371 |
[32m[20230204 15:37:54 @agent_ppo2.py:194][0m |          -0.0059 |           1.5127 |           2.4398 |
[32m[20230204 15:37:54 @agent_ppo2.py:194][0m |          -0.0070 |           1.4894 |           2.4396 |
[32m[20230204 15:37:54 @agent_ppo2.py:194][0m |          -0.0069 |           1.4675 |           2.4412 |
[32m[20230204 15:37:54 @agent_ppo2.py:194][0m |          -0.0074 |           1.4507 |           2.4423 |
[32m[20230204 15:37:54 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:37:54 @agent_ppo2.py:147][0m Average TRAINING episode reward: 145.30
[32m[20230204 15:37:54 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 160.41
[32m[20230204 15:37:54 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 72.54
[32m[20230204 15:37:54 @agent_ppo2.py:152][0m Total time:       1.73 min
[32m[20230204 15:37:54 @agent_ppo2.py:154][0m 116736 total steps have happened
[32m[20230204 15:37:54 @agent_ppo2.py:130][0m #------------------------ Iteration 57 --------------------------#
[32m[20230204 15:37:54 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:55 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:55 @agent_ppo2.py:194][0m |          -0.0011 |           8.3023 |           2.4670 |
[32m[20230204 15:37:55 @agent_ppo2.py:194][0m |          -0.0060 |           4.4766 |           2.4623 |
[32m[20230204 15:37:55 @agent_ppo2.py:194][0m |          -0.0072 |           3.7788 |           2.4606 |
[32m[20230204 15:37:55 @agent_ppo2.py:194][0m |          -0.0083 |           3.3950 |           2.4599 |
[32m[20230204 15:37:55 @agent_ppo2.py:194][0m |          -0.0092 |           3.1841 |           2.4585 |
[32m[20230204 15:37:55 @agent_ppo2.py:194][0m |          -0.0097 |           3.0736 |           2.4569 |
[32m[20230204 15:37:55 @agent_ppo2.py:194][0m |          -0.0102 |           2.9854 |           2.4577 |
[32m[20230204 15:37:55 @agent_ppo2.py:194][0m |          -0.0110 |           2.9207 |           2.4567 |
[32m[20230204 15:37:55 @agent_ppo2.py:194][0m |          -0.0107 |           2.8558 |           2.4574 |
[32m[20230204 15:37:55 @agent_ppo2.py:194][0m |          -0.0113 |           2.8041 |           2.4566 |
[32m[20230204 15:37:55 @agent_ppo2.py:139][0m Policy update time: 1.02 s
[32m[20230204 15:37:56 @agent_ppo2.py:147][0m Average TRAINING episode reward: 64.96
[32m[20230204 15:37:56 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 173.35
[32m[20230204 15:37:56 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 241.11
[32m[20230204 15:37:56 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 241.11
[32m[20230204 15:37:56 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 241.11
[32m[20230204 15:37:56 @agent_ppo2.py:152][0m Total time:       1.76 min
[32m[20230204 15:37:56 @agent_ppo2.py:154][0m 118784 total steps have happened
[32m[20230204 15:37:56 @agent_ppo2.py:130][0m #------------------------ Iteration 58 --------------------------#
[32m[20230204 15:37:56 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:37:56 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:56 @agent_ppo2.py:194][0m |          -0.0007 |          12.0410 |           2.4502 |
[32m[20230204 15:37:56 @agent_ppo2.py:194][0m |          -0.0057 |           7.7240 |           2.4457 |
[32m[20230204 15:37:57 @agent_ppo2.py:194][0m |          -0.0073 |           7.2206 |           2.4447 |
[32m[20230204 15:37:57 @agent_ppo2.py:194][0m |          -0.0076 |           6.7945 |           2.4463 |
[32m[20230204 15:37:57 @agent_ppo2.py:194][0m |          -0.0090 |           6.5043 |           2.4442 |
[32m[20230204 15:37:57 @agent_ppo2.py:194][0m |          -0.0093 |           6.1059 |           2.4448 |
[32m[20230204 15:37:57 @agent_ppo2.py:194][0m |          -0.0097 |           5.9328 |           2.4448 |
[32m[20230204 15:37:57 @agent_ppo2.py:194][0m |          -0.0095 |           5.7098 |           2.4441 |
[32m[20230204 15:37:57 @agent_ppo2.py:194][0m |          -0.0107 |           5.6407 |           2.4450 |
[32m[20230204 15:37:57 @agent_ppo2.py:194][0m |          -0.0105 |           5.4819 |           2.4443 |
[32m[20230204 15:37:57 @agent_ppo2.py:139][0m Policy update time: 1.03 s
[32m[20230204 15:37:57 @agent_ppo2.py:147][0m Average TRAINING episode reward: 68.94
[32m[20230204 15:37:57 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 163.28
[32m[20230204 15:37:57 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 258.64
[32m[20230204 15:37:57 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 258.64
[32m[20230204 15:37:57 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 258.64
[32m[20230204 15:37:57 @agent_ppo2.py:152][0m Total time:       1.78 min
[32m[20230204 15:37:57 @agent_ppo2.py:154][0m 120832 total steps have happened
[32m[20230204 15:37:57 @agent_ppo2.py:130][0m #------------------------ Iteration 59 --------------------------#
[32m[20230204 15:37:58 @agent_ppo2.py:136][0m Sampling time: 0.41 s by 4 slaves
[32m[20230204 15:37:58 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:37:58 @agent_ppo2.py:194][0m |          -0.0003 |           9.0303 |           2.4632 |
[32m[20230204 15:37:58 @agent_ppo2.py:194][0m |          -0.0058 |           5.5267 |           2.4588 |
[32m[20230204 15:37:58 @agent_ppo2.py:194][0m |          -0.0070 |           4.9020 |           2.4566 |
[32m[20230204 15:37:58 @agent_ppo2.py:194][0m |          -0.0092 |           4.4099 |           2.4567 |
[32m[20230204 15:37:59 @agent_ppo2.py:194][0m |          -0.0083 |           4.1946 |           2.4562 |
[32m[20230204 15:37:59 @agent_ppo2.py:194][0m |          -0.0089 |           3.9499 |           2.4550 |
[32m[20230204 15:37:59 @agent_ppo2.py:194][0m |          -0.0088 |           3.9342 |           2.4550 |
[32m[20230204 15:37:59 @agent_ppo2.py:194][0m |          -0.0084 |           3.6848 |           2.4527 |
[32m[20230204 15:37:59 @agent_ppo2.py:194][0m |          -0.0121 |           3.5642 |           2.4557 |
[32m[20230204 15:37:59 @agent_ppo2.py:194][0m |          -0.0093 |           3.4792 |           2.4535 |
[32m[20230204 15:37:59 @agent_ppo2.py:139][0m Policy update time: 1.35 s
[32m[20230204 15:38:00 @agent_ppo2.py:147][0m Average TRAINING episode reward: 63.75
[32m[20230204 15:38:00 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 156.86
[32m[20230204 15:38:00 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 112.56
[32m[20230204 15:38:00 @agent_ppo2.py:152][0m Total time:       1.82 min
[32m[20230204 15:38:00 @agent_ppo2.py:154][0m 122880 total steps have happened
[32m[20230204 15:38:00 @agent_ppo2.py:130][0m #------------------------ Iteration 60 --------------------------#
[32m[20230204 15:38:00 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:38:00 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:00 @agent_ppo2.py:194][0m |          -0.0005 |           2.5096 |           2.4864 |
[32m[20230204 15:38:00 @agent_ppo2.py:194][0m |          -0.0038 |           2.2579 |           2.4814 |
[32m[20230204 15:38:00 @agent_ppo2.py:194][0m |          -0.0052 |           2.1911 |           2.4838 |
[32m[20230204 15:38:00 @agent_ppo2.py:194][0m |          -0.0059 |           2.1471 |           2.4839 |
[32m[20230204 15:38:01 @agent_ppo2.py:194][0m |          -0.0063 |           2.1008 |           2.4825 |
[32m[20230204 15:38:01 @agent_ppo2.py:194][0m |          -0.0068 |           2.0624 |           2.4836 |
[32m[20230204 15:38:01 @agent_ppo2.py:194][0m |          -0.0065 |           2.0318 |           2.4848 |
[32m[20230204 15:38:01 @agent_ppo2.py:194][0m |          -0.0078 |           1.9996 |           2.4853 |
[32m[20230204 15:38:01 @agent_ppo2.py:194][0m |          -0.0081 |           1.9622 |           2.4856 |
[32m[20230204 15:38:01 @agent_ppo2.py:194][0m |          -0.0082 |           1.9337 |           2.4850 |
[32m[20230204 15:38:01 @agent_ppo2.py:139][0m Policy update time: 1.34 s
[32m[20230204 15:38:01 @agent_ppo2.py:147][0m Average TRAINING episode reward: 149.73
[32m[20230204 15:38:01 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 160.82
[32m[20230204 15:38:01 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -24.10
[32m[20230204 15:38:01 @agent_ppo2.py:152][0m Total time:       1.85 min
[32m[20230204 15:38:01 @agent_ppo2.py:154][0m 124928 total steps have happened
[32m[20230204 15:38:01 @agent_ppo2.py:130][0m #------------------------ Iteration 61 --------------------------#
[32m[20230204 15:38:02 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:38:02 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:02 @agent_ppo2.py:194][0m |          -0.0026 |          11.7337 |           2.4416 |
[32m[20230204 15:38:02 @agent_ppo2.py:194][0m |          -0.0065 |           6.9333 |           2.4384 |
[32m[20230204 15:38:02 @agent_ppo2.py:194][0m |          -0.0080 |           6.3841 |           2.4362 |
[32m[20230204 15:38:02 @agent_ppo2.py:194][0m |          -0.0071 |           6.1381 |           2.4378 |
[32m[20230204 15:38:02 @agent_ppo2.py:194][0m |          -0.0108 |           5.8297 |           2.4370 |
[32m[20230204 15:38:02 @agent_ppo2.py:194][0m |          -0.0076 |           5.7405 |           2.4365 |
[32m[20230204 15:38:02 @agent_ppo2.py:194][0m |          -0.0112 |           5.4299 |           2.4357 |
[32m[20230204 15:38:03 @agent_ppo2.py:194][0m |          -0.0120 |           5.3308 |           2.4370 |
[32m[20230204 15:38:03 @agent_ppo2.py:194][0m |          -0.0130 |           5.1905 |           2.4357 |
[32m[20230204 15:38:03 @agent_ppo2.py:194][0m |          -0.0111 |           5.0682 |           2.4362 |
[32m[20230204 15:38:03 @agent_ppo2.py:139][0m Policy update time: 1.02 s
[32m[20230204 15:38:03 @agent_ppo2.py:147][0m Average TRAINING episode reward: 47.64
[32m[20230204 15:38:03 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 148.76
[32m[20230204 15:38:03 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 239.35
[32m[20230204 15:38:03 @agent_ppo2.py:152][0m Total time:       1.88 min
[32m[20230204 15:38:03 @agent_ppo2.py:154][0m 126976 total steps have happened
[32m[20230204 15:38:03 @agent_ppo2.py:130][0m #------------------------ Iteration 62 --------------------------#
[32m[20230204 15:38:03 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:38:03 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:04 @agent_ppo2.py:194][0m |           0.0004 |          29.1688 |           2.4720 |
[32m[20230204 15:38:04 @agent_ppo2.py:194][0m |          -0.0054 |          22.9761 |           2.4689 |
[32m[20230204 15:38:04 @agent_ppo2.py:194][0m |          -0.0079 |          20.9329 |           2.4674 |
[32m[20230204 15:38:04 @agent_ppo2.py:194][0m |          -0.0056 |          19.5369 |           2.4654 |
[32m[20230204 15:38:04 @agent_ppo2.py:194][0m |          -0.0091 |          18.4116 |           2.4639 |
[32m[20230204 15:38:04 @agent_ppo2.py:194][0m |          -0.0109 |          17.3142 |           2.4607 |
[32m[20230204 15:38:04 @agent_ppo2.py:194][0m |          -0.0100 |          16.9884 |           2.4604 |
[32m[20230204 15:38:04 @agent_ppo2.py:194][0m |          -0.0084 |          16.2876 |           2.4595 |
[32m[20230204 15:38:04 @agent_ppo2.py:194][0m |          -0.0122 |          15.7148 |           2.4581 |
[32m[20230204 15:38:04 @agent_ppo2.py:194][0m |          -0.0125 |          15.2647 |           2.4573 |
[32m[20230204 15:38:04 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:38:04 @agent_ppo2.py:147][0m Average TRAINING episode reward: 13.09
[32m[20230204 15:38:04 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 137.91
[32m[20230204 15:38:04 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 48.18
[32m[20230204 15:38:04 @agent_ppo2.py:152][0m Total time:       1.90 min
[32m[20230204 15:38:04 @agent_ppo2.py:154][0m 129024 total steps have happened
[32m[20230204 15:38:04 @agent_ppo2.py:130][0m #------------------------ Iteration 63 --------------------------#
[32m[20230204 15:38:05 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:38:05 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:05 @agent_ppo2.py:194][0m |          -0.0000 |           2.1402 |           2.4650 |
[32m[20230204 15:38:05 @agent_ppo2.py:194][0m |          -0.0029 |           1.7473 |           2.4641 |
[32m[20230204 15:38:05 @agent_ppo2.py:194][0m |          -0.0039 |           1.5970 |           2.4652 |
[32m[20230204 15:38:05 @agent_ppo2.py:194][0m |          -0.0048 |           1.5061 |           2.4664 |
[32m[20230204 15:38:05 @agent_ppo2.py:194][0m |          -0.0052 |           1.4396 |           2.4685 |
[32m[20230204 15:38:06 @agent_ppo2.py:194][0m |          -0.0059 |           1.3905 |           2.4703 |
[32m[20230204 15:38:06 @agent_ppo2.py:194][0m |          -0.0065 |           1.3539 |           2.4720 |
[32m[20230204 15:38:06 @agent_ppo2.py:194][0m |          -0.0068 |           1.3235 |           2.4716 |
[32m[20230204 15:38:06 @agent_ppo2.py:194][0m |          -0.0070 |           1.2973 |           2.4730 |
[32m[20230204 15:38:06 @agent_ppo2.py:194][0m |          -0.0073 |           1.2761 |           2.4739 |
[32m[20230204 15:38:06 @agent_ppo2.py:139][0m Policy update time: 1.30 s
[32m[20230204 15:38:06 @agent_ppo2.py:147][0m Average TRAINING episode reward: 157.32
[32m[20230204 15:38:06 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 172.46
[32m[20230204 15:38:06 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 251.76
[32m[20230204 15:38:06 @agent_ppo2.py:152][0m Total time:       1.93 min
[32m[20230204 15:38:06 @agent_ppo2.py:154][0m 131072 total steps have happened
[32m[20230204 15:38:06 @agent_ppo2.py:130][0m #------------------------ Iteration 64 --------------------------#
[32m[20230204 15:38:07 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:38:07 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:07 @agent_ppo2.py:194][0m |          -0.0004 |           4.2068 |           2.5448 |
[32m[20230204 15:38:07 @agent_ppo2.py:194][0m |          -0.0033 |           3.4028 |           2.5439 |
[32m[20230204 15:38:07 @agent_ppo2.py:194][0m |          -0.0045 |           3.1781 |           2.5448 |
[32m[20230204 15:38:07 @agent_ppo2.py:194][0m |          -0.0055 |           3.0569 |           2.5447 |
[32m[20230204 15:38:07 @agent_ppo2.py:194][0m |          -0.0060 |           2.9984 |           2.5446 |
[32m[20230204 15:38:08 @agent_ppo2.py:194][0m |          -0.0060 |           2.9240 |           2.5438 |
[32m[20230204 15:38:08 @agent_ppo2.py:194][0m |          -0.0075 |           2.8484 |           2.5447 |
[32m[20230204 15:38:08 @agent_ppo2.py:194][0m |          -0.0073 |           2.8080 |           2.5449 |
[32m[20230204 15:38:08 @agent_ppo2.py:194][0m |          -0.0079 |           2.7530 |           2.5457 |
[32m[20230204 15:38:08 @agent_ppo2.py:194][0m |          -0.0078 |           2.7213 |           2.5471 |
[32m[20230204 15:38:08 @agent_ppo2.py:139][0m Policy update time: 1.27 s
[32m[20230204 15:38:08 @agent_ppo2.py:147][0m Average TRAINING episode reward: 161.84
[32m[20230204 15:38:08 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 171.23
[32m[20230204 15:38:08 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 240.41
[32m[20230204 15:38:08 @agent_ppo2.py:152][0m Total time:       1.96 min
[32m[20230204 15:38:08 @agent_ppo2.py:154][0m 133120 total steps have happened
[32m[20230204 15:38:08 @agent_ppo2.py:130][0m #------------------------ Iteration 65 --------------------------#
[32m[20230204 15:38:09 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:38:09 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:09 @agent_ppo2.py:194][0m |           0.0006 |           1.6959 |           2.5172 |
[32m[20230204 15:38:09 @agent_ppo2.py:194][0m |          -0.0026 |           1.4975 |           2.5141 |
[32m[20230204 15:38:09 @agent_ppo2.py:194][0m |          -0.0043 |           1.4454 |           2.5127 |
[32m[20230204 15:38:09 @agent_ppo2.py:194][0m |          -0.0052 |           1.4133 |           2.5130 |
[32m[20230204 15:38:09 @agent_ppo2.py:194][0m |          -0.0065 |           1.3967 |           2.5129 |
[32m[20230204 15:38:09 @agent_ppo2.py:194][0m |          -0.0067 |           1.3806 |           2.5142 |
[32m[20230204 15:38:10 @agent_ppo2.py:194][0m |          -0.0073 |           1.3666 |           2.5146 |
[32m[20230204 15:38:10 @agent_ppo2.py:194][0m |          -0.0076 |           1.3531 |           2.5141 |
[32m[20230204 15:38:10 @agent_ppo2.py:194][0m |          -0.0084 |           1.3424 |           2.5153 |
[32m[20230204 15:38:10 @agent_ppo2.py:194][0m |          -0.0083 |           1.3374 |           2.5156 |
[32m[20230204 15:38:10 @agent_ppo2.py:139][0m Policy update time: 1.29 s
[32m[20230204 15:38:10 @agent_ppo2.py:147][0m Average TRAINING episode reward: 149.14
[32m[20230204 15:38:10 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 157.97
[32m[20230204 15:38:10 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 254.24
[32m[20230204 15:38:10 @agent_ppo2.py:152][0m Total time:       2.00 min
[32m[20230204 15:38:10 @agent_ppo2.py:154][0m 135168 total steps have happened
[32m[20230204 15:38:10 @agent_ppo2.py:130][0m #------------------------ Iteration 66 --------------------------#
[32m[20230204 15:38:11 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:38:11 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:11 @agent_ppo2.py:194][0m |           0.0013 |           3.2514 |           2.5593 |
[32m[20230204 15:38:11 @agent_ppo2.py:194][0m |          -0.0044 |           1.8015 |           2.5596 |
[32m[20230204 15:38:11 @agent_ppo2.py:194][0m |          -0.0056 |           1.5881 |           2.5592 |
[32m[20230204 15:38:11 @agent_ppo2.py:194][0m |          -0.0066 |           1.5238 |           2.5594 |
[32m[20230204 15:38:11 @agent_ppo2.py:194][0m |          -0.0068 |           1.4800 |           2.5588 |
[32m[20230204 15:38:11 @agent_ppo2.py:194][0m |          -0.0072 |           1.4482 |           2.5592 |
[32m[20230204 15:38:11 @agent_ppo2.py:194][0m |          -0.0082 |           1.4242 |           2.5595 |
[32m[20230204 15:38:11 @agent_ppo2.py:194][0m |          -0.0088 |           1.4019 |           2.5598 |
[32m[20230204 15:38:12 @agent_ppo2.py:194][0m |          -0.0090 |           1.3545 |           2.5602 |
[32m[20230204 15:38:12 @agent_ppo2.py:194][0m |          -0.0093 |           1.3422 |           2.5591 |
[32m[20230204 15:38:12 @agent_ppo2.py:139][0m Policy update time: 1.12 s
[32m[20230204 15:38:12 @agent_ppo2.py:147][0m Average TRAINING episode reward: 122.52
[32m[20230204 15:38:12 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 169.79
[32m[20230204 15:38:12 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 260.71
[32m[20230204 15:38:12 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 260.71
[32m[20230204 15:38:12 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 260.71
[32m[20230204 15:38:12 @agent_ppo2.py:152][0m Total time:       2.03 min
[32m[20230204 15:38:12 @agent_ppo2.py:154][0m 137216 total steps have happened
[32m[20230204 15:38:12 @agent_ppo2.py:130][0m #------------------------ Iteration 67 --------------------------#
[32m[20230204 15:38:12 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:38:13 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:13 @agent_ppo2.py:194][0m |          -0.0015 |           1.2753 |           2.5330 |
[32m[20230204 15:38:13 @agent_ppo2.py:194][0m |          -0.0054 |           1.1998 |           2.5286 |
[32m[20230204 15:38:13 @agent_ppo2.py:194][0m |          -0.0066 |           1.1724 |           2.5297 |
[32m[20230204 15:38:13 @agent_ppo2.py:194][0m |          -0.0074 |           1.1539 |           2.5284 |
[32m[20230204 15:38:13 @agent_ppo2.py:194][0m |          -0.0078 |           1.1314 |           2.5290 |
[32m[20230204 15:38:13 @agent_ppo2.py:194][0m |          -0.0084 |           1.1202 |           2.5290 |
[32m[20230204 15:38:13 @agent_ppo2.py:194][0m |          -0.0083 |           1.1079 |           2.5284 |
[32m[20230204 15:38:13 @agent_ppo2.py:194][0m |          -0.0089 |           1.1010 |           2.5279 |
[32m[20230204 15:38:14 @agent_ppo2.py:194][0m |          -0.0088 |           1.0883 |           2.5276 |
[32m[20230204 15:38:14 @agent_ppo2.py:194][0m |          -0.0094 |           1.0769 |           2.5270 |
[32m[20230204 15:38:14 @agent_ppo2.py:139][0m Policy update time: 1.24 s
[32m[20230204 15:38:14 @agent_ppo2.py:147][0m Average TRAINING episode reward: 170.03
[32m[20230204 15:38:14 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 177.37
[32m[20230204 15:38:14 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 234.81
[32m[20230204 15:38:14 @agent_ppo2.py:152][0m Total time:       2.06 min
[32m[20230204 15:38:14 @agent_ppo2.py:154][0m 139264 total steps have happened
[32m[20230204 15:38:14 @agent_ppo2.py:130][0m #------------------------ Iteration 68 --------------------------#
[32m[20230204 15:38:14 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:38:14 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:14 @agent_ppo2.py:194][0m |          -0.0019 |           9.0932 |           2.5305 |
[32m[20230204 15:38:15 @agent_ppo2.py:194][0m |          -0.0010 |           4.0795 |           2.5284 |
[32m[20230204 15:38:15 @agent_ppo2.py:194][0m |          -0.0080 |           3.3850 |           2.5277 |
[32m[20230204 15:38:15 @agent_ppo2.py:194][0m |          -0.0074 |           3.1777 |           2.5273 |
[32m[20230204 15:38:15 @agent_ppo2.py:194][0m |          -0.0103 |           3.0178 |           2.5261 |
[32m[20230204 15:38:15 @agent_ppo2.py:194][0m |          -0.0093 |           2.8093 |           2.5245 |
[32m[20230204 15:38:15 @agent_ppo2.py:194][0m |          -0.0088 |           2.7397 |           2.5249 |
[32m[20230204 15:38:15 @agent_ppo2.py:194][0m |          -0.0113 |           2.6966 |           2.5237 |
[32m[20230204 15:38:15 @agent_ppo2.py:194][0m |          -0.0110 |           2.5252 |           2.5232 |
[32m[20230204 15:38:15 @agent_ppo2.py:194][0m |          -0.0108 |           2.5197 |           2.5219 |
[32m[20230204 15:38:15 @agent_ppo2.py:139][0m Policy update time: 1.03 s
[32m[20230204 15:38:16 @agent_ppo2.py:147][0m Average TRAINING episode reward: 30.68
[32m[20230204 15:38:16 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 179.47
[32m[20230204 15:38:16 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 243.89
[32m[20230204 15:38:16 @agent_ppo2.py:152][0m Total time:       2.09 min
[32m[20230204 15:38:16 @agent_ppo2.py:154][0m 141312 total steps have happened
[32m[20230204 15:38:16 @agent_ppo2.py:130][0m #------------------------ Iteration 69 --------------------------#
[32m[20230204 15:38:16 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:38:16 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:16 @agent_ppo2.py:194][0m |           0.0009 |           2.4406 |           2.5710 |
[32m[20230204 15:38:16 @agent_ppo2.py:194][0m |          -0.0042 |           2.1856 |           2.5708 |
[32m[20230204 15:38:16 @agent_ppo2.py:194][0m |          -0.0067 |           2.1156 |           2.5684 |
[32m[20230204 15:38:17 @agent_ppo2.py:194][0m |          -0.0081 |           2.0667 |           2.5677 |
[32m[20230204 15:38:17 @agent_ppo2.py:194][0m |          -0.0087 |           2.0420 |           2.5681 |
[32m[20230204 15:38:17 @agent_ppo2.py:194][0m |          -0.0096 |           2.0161 |           2.5698 |
[32m[20230204 15:38:17 @agent_ppo2.py:194][0m |          -0.0100 |           1.9960 |           2.5712 |
[32m[20230204 15:38:17 @agent_ppo2.py:194][0m |          -0.0107 |           1.9827 |           2.5717 |
[32m[20230204 15:38:17 @agent_ppo2.py:194][0m |          -0.0109 |           1.9745 |           2.5696 |
[32m[20230204 15:38:17 @agent_ppo2.py:194][0m |          -0.0108 |           1.9485 |           2.5707 |
[32m[20230204 15:38:17 @agent_ppo2.py:139][0m Policy update time: 1.25 s
[32m[20230204 15:38:17 @agent_ppo2.py:147][0m Average TRAINING episode reward: 174.12
[32m[20230204 15:38:17 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 181.42
[32m[20230204 15:38:17 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -127.37
[32m[20230204 15:38:17 @agent_ppo2.py:152][0m Total time:       2.11 min
[32m[20230204 15:38:17 @agent_ppo2.py:154][0m 143360 total steps have happened
[32m[20230204 15:38:17 @agent_ppo2.py:130][0m #------------------------ Iteration 70 --------------------------#
[32m[20230204 15:38:18 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:38:18 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:18 @agent_ppo2.py:194][0m |           0.0009 |           2.0683 |           2.6025 |
[32m[20230204 15:38:18 @agent_ppo2.py:194][0m |          -0.0014 |           1.8417 |           2.6013 |
[32m[20230204 15:38:18 @agent_ppo2.py:194][0m |          -0.0031 |           1.7627 |           2.6003 |
[32m[20230204 15:38:18 @agent_ppo2.py:194][0m |          -0.0044 |           1.7069 |           2.6016 |
[32m[20230204 15:38:18 @agent_ppo2.py:194][0m |          -0.0050 |           1.6756 |           2.6005 |
[32m[20230204 15:38:18 @agent_ppo2.py:194][0m |          -0.0058 |           1.6514 |           2.5998 |
[32m[20230204 15:38:19 @agent_ppo2.py:194][0m |          -0.0061 |           1.6188 |           2.6022 |
[32m[20230204 15:38:19 @agent_ppo2.py:194][0m |          -0.0068 |           1.5981 |           2.6014 |
[32m[20230204 15:38:19 @agent_ppo2.py:194][0m |          -0.0072 |           1.5865 |           2.6022 |
[32m[20230204 15:38:19 @agent_ppo2.py:194][0m |          -0.0071 |           1.5687 |           2.6024 |
[32m[20230204 15:38:19 @agent_ppo2.py:139][0m Policy update time: 1.27 s
[32m[20230204 15:38:19 @agent_ppo2.py:147][0m Average TRAINING episode reward: 178.14
[32m[20230204 15:38:19 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 190.03
[32m[20230204 15:38:19 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -45.32
[32m[20230204 15:38:19 @agent_ppo2.py:152][0m Total time:       2.15 min
[32m[20230204 15:38:19 @agent_ppo2.py:154][0m 145408 total steps have happened
[32m[20230204 15:38:19 @agent_ppo2.py:130][0m #------------------------ Iteration 71 --------------------------#
[32m[20230204 15:38:20 @agent_ppo2.py:136][0m Sampling time: 0.35 s by 4 slaves
[32m[20230204 15:38:20 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:20 @agent_ppo2.py:194][0m |           0.0001 |           5.0350 |           2.5363 |
[32m[20230204 15:38:20 @agent_ppo2.py:194][0m |          -0.0039 |           3.3900 |           2.5351 |
[32m[20230204 15:38:20 @agent_ppo2.py:194][0m |           0.0016 |           3.2138 |           2.5329 |
[32m[20230204 15:38:20 @agent_ppo2.py:194][0m |          -0.0023 |           3.0506 |           2.5333 |
[32m[20230204 15:38:20 @agent_ppo2.py:194][0m |          -0.0003 |           2.8543 |           2.5325 |
[32m[20230204 15:38:20 @agent_ppo2.py:194][0m |          -0.0060 |           2.8012 |           2.5340 |
[32m[20230204 15:38:21 @agent_ppo2.py:194][0m |          -0.0066 |           2.5959 |           2.5335 |
[32m[20230204 15:38:21 @agent_ppo2.py:194][0m |          -0.0072 |           2.5539 |           2.5346 |
[32m[20230204 15:38:21 @agent_ppo2.py:194][0m |          -0.0066 |           2.4908 |           2.5347 |
[32m[20230204 15:38:21 @agent_ppo2.py:194][0m |          -0.0069 |           2.4366 |           2.5361 |
[32m[20230204 15:38:21 @agent_ppo2.py:139][0m Policy update time: 1.26 s
[32m[20230204 15:38:21 @agent_ppo2.py:147][0m Average TRAINING episode reward: 122.06
[32m[20230204 15:38:21 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 180.01
[32m[20230204 15:38:21 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 64.96
[32m[20230204 15:38:21 @agent_ppo2.py:152][0m Total time:       2.18 min
[32m[20230204 15:38:21 @agent_ppo2.py:154][0m 147456 total steps have happened
[32m[20230204 15:38:21 @agent_ppo2.py:130][0m #------------------------ Iteration 72 --------------------------#
[32m[20230204 15:38:21 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:38:22 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:22 @agent_ppo2.py:194][0m |          -0.0007 |          12.2185 |           2.5908 |
[32m[20230204 15:38:22 @agent_ppo2.py:194][0m |          -0.0041 |           7.7230 |           2.5905 |
[32m[20230204 15:38:22 @agent_ppo2.py:194][0m |          -0.0055 |           6.8991 |           2.5884 |
[32m[20230204 15:38:22 @agent_ppo2.py:194][0m |          -0.0065 |           6.3970 |           2.5874 |
[32m[20230204 15:38:22 @agent_ppo2.py:194][0m |          -0.0074 |           5.9723 |           2.5879 |
[32m[20230204 15:38:22 @agent_ppo2.py:194][0m |          -0.0078 |           5.6212 |           2.5869 |
[32m[20230204 15:38:22 @agent_ppo2.py:194][0m |          -0.0084 |           5.2640 |           2.5862 |
[32m[20230204 15:38:22 @agent_ppo2.py:194][0m |          -0.0084 |           5.2218 |           2.5844 |
[32m[20230204 15:38:22 @agent_ppo2.py:194][0m |          -0.0084 |           4.9097 |           2.5842 |
[32m[20230204 15:38:23 @agent_ppo2.py:194][0m |          -0.0093 |           4.6816 |           2.5818 |
[32m[20230204 15:38:23 @agent_ppo2.py:139][0m Policy update time: 1.10 s
[32m[20230204 15:38:23 @agent_ppo2.py:147][0m Average TRAINING episode reward: 121.99
[32m[20230204 15:38:23 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 181.55
[32m[20230204 15:38:23 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 243.13
[32m[20230204 15:38:23 @agent_ppo2.py:152][0m Total time:       2.21 min
[32m[20230204 15:38:23 @agent_ppo2.py:154][0m 149504 total steps have happened
[32m[20230204 15:38:23 @agent_ppo2.py:130][0m #------------------------ Iteration 73 --------------------------#
[32m[20230204 15:38:23 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:38:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:23 @agent_ppo2.py:194][0m |          -0.0010 |           1.9039 |           2.5769 |
[32m[20230204 15:38:24 @agent_ppo2.py:194][0m |          -0.0058 |           1.7222 |           2.5720 |
[32m[20230204 15:38:24 @agent_ppo2.py:194][0m |          -0.0065 |           1.6748 |           2.5741 |
[32m[20230204 15:38:24 @agent_ppo2.py:194][0m |          -0.0078 |           1.6390 |           2.5745 |
[32m[20230204 15:38:24 @agent_ppo2.py:194][0m |          -0.0080 |           1.6191 |           2.5755 |
[32m[20230204 15:38:24 @agent_ppo2.py:194][0m |          -0.0086 |           1.5958 |           2.5745 |
[32m[20230204 15:38:24 @agent_ppo2.py:194][0m |          -0.0084 |           1.5793 |           2.5745 |
[32m[20230204 15:38:24 @agent_ppo2.py:194][0m |          -0.0094 |           1.5673 |           2.5750 |
[32m[20230204 15:38:24 @agent_ppo2.py:194][0m |          -0.0096 |           1.5521 |           2.5745 |
[32m[20230204 15:38:24 @agent_ppo2.py:194][0m |          -0.0092 |           1.5403 |           2.5765 |
[32m[20230204 15:38:24 @agent_ppo2.py:139][0m Policy update time: 1.24 s
[32m[20230204 15:38:25 @agent_ppo2.py:147][0m Average TRAINING episode reward: 176.33
[32m[20230204 15:38:25 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 182.14
[32m[20230204 15:38:25 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 254.94
[32m[20230204 15:38:25 @agent_ppo2.py:152][0m Total time:       2.24 min
[32m[20230204 15:38:25 @agent_ppo2.py:154][0m 151552 total steps have happened
[32m[20230204 15:38:25 @agent_ppo2.py:130][0m #------------------------ Iteration 74 --------------------------#
[32m[20230204 15:38:25 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:38:25 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:25 @agent_ppo2.py:194][0m |          -0.0002 |           2.0831 |           2.5792 |
[32m[20230204 15:38:25 @agent_ppo2.py:194][0m |          -0.0037 |           1.9009 |           2.5803 |
[32m[20230204 15:38:26 @agent_ppo2.py:194][0m |          -0.0049 |           1.8456 |           2.5802 |
[32m[20230204 15:38:26 @agent_ppo2.py:194][0m |          -0.0067 |           1.8105 |           2.5818 |
[32m[20230204 15:38:26 @agent_ppo2.py:194][0m |          -0.0073 |           1.7764 |           2.5826 |
[32m[20230204 15:38:26 @agent_ppo2.py:194][0m |          -0.0080 |           1.7498 |           2.5840 |
[32m[20230204 15:38:26 @agent_ppo2.py:194][0m |          -0.0087 |           1.7349 |           2.5851 |
[32m[20230204 15:38:26 @agent_ppo2.py:194][0m |          -0.0093 |           1.7136 |           2.5869 |
[32m[20230204 15:38:26 @agent_ppo2.py:194][0m |          -0.0092 |           1.6942 |           2.5865 |
[32m[20230204 15:38:26 @agent_ppo2.py:194][0m |          -0.0098 |           1.6785 |           2.5885 |
[32m[20230204 15:38:26 @agent_ppo2.py:139][0m Policy update time: 1.25 s
[32m[20230204 15:38:27 @agent_ppo2.py:147][0m Average TRAINING episode reward: 178.47
[32m[20230204 15:38:27 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 183.67
[32m[20230204 15:38:27 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 253.76
[32m[20230204 15:38:27 @agent_ppo2.py:152][0m Total time:       2.27 min
[32m[20230204 15:38:27 @agent_ppo2.py:154][0m 153600 total steps have happened
[32m[20230204 15:38:27 @agent_ppo2.py:130][0m #------------------------ Iteration 75 --------------------------#
[32m[20230204 15:38:27 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:38:27 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:27 @agent_ppo2.py:194][0m |           0.0067 |           6.4474 |           2.6078 |
[32m[20230204 15:38:27 @agent_ppo2.py:194][0m |          -0.0033 |           3.7823 |           2.6053 |
[32m[20230204 15:38:27 @agent_ppo2.py:194][0m |          -0.0044 |           3.4975 |           2.6041 |
[32m[20230204 15:38:28 @agent_ppo2.py:194][0m |          -0.0044 |           3.3153 |           2.6063 |
[32m[20230204 15:38:28 @agent_ppo2.py:194][0m |          -0.0059 |           3.1468 |           2.6024 |
[32m[20230204 15:38:28 @agent_ppo2.py:194][0m |          -0.0061 |           3.1014 |           2.6031 |
[32m[20230204 15:38:28 @agent_ppo2.py:194][0m |          -0.0080 |           3.0242 |           2.6011 |
[32m[20230204 15:38:28 @agent_ppo2.py:194][0m |          -0.0073 |           2.9672 |           2.6006 |
[32m[20230204 15:38:28 @agent_ppo2.py:194][0m |          -0.0096 |           2.8961 |           2.5986 |
[32m[20230204 15:38:28 @agent_ppo2.py:194][0m |          -0.0086 |           2.8613 |           2.6008 |
[32m[20230204 15:38:28 @agent_ppo2.py:139][0m Policy update time: 1.26 s
[32m[20230204 15:38:28 @agent_ppo2.py:147][0m Average TRAINING episode reward: 133.14
[32m[20230204 15:38:28 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 190.13
[32m[20230204 15:38:28 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -122.61
[32m[20230204 15:38:28 @agent_ppo2.py:152][0m Total time:       2.30 min
[32m[20230204 15:38:28 @agent_ppo2.py:154][0m 155648 total steps have happened
[32m[20230204 15:38:28 @agent_ppo2.py:130][0m #------------------------ Iteration 76 --------------------------#
[32m[20230204 15:38:29 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:38:29 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:29 @agent_ppo2.py:194][0m |           0.0006 |           6.3898 |           2.5736 |
[32m[20230204 15:38:29 @agent_ppo2.py:194][0m |          -0.0071 |           3.9766 |           2.5701 |
[32m[20230204 15:38:29 @agent_ppo2.py:194][0m |          -0.0052 |           3.4768 |           2.5678 |
[32m[20230204 15:38:29 @agent_ppo2.py:194][0m |          -0.0079 |           3.2424 |           2.5671 |
[32m[20230204 15:38:29 @agent_ppo2.py:194][0m |          -0.0085 |           3.0521 |           2.5663 |
[32m[20230204 15:38:29 @agent_ppo2.py:194][0m |          -0.0068 |           3.0054 |           2.5644 |
[32m[20230204 15:38:29 @agent_ppo2.py:194][0m |          -0.0103 |           2.9046 |           2.5645 |
[32m[20230204 15:38:30 @agent_ppo2.py:194][0m |          -0.0091 |           2.7933 |           2.5655 |
[32m[20230204 15:38:30 @agent_ppo2.py:194][0m |          -0.0102 |           2.7194 |           2.5641 |
[32m[20230204 15:38:30 @agent_ppo2.py:194][0m |          -0.0093 |           2.6794 |           2.5630 |
[32m[20230204 15:38:30 @agent_ppo2.py:139][0m Policy update time: 1.04 s
[32m[20230204 15:38:30 @agent_ppo2.py:147][0m Average TRAINING episode reward: 121.32
[32m[20230204 15:38:30 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 188.53
[32m[20230204 15:38:30 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -125.92
[32m[20230204 15:38:30 @agent_ppo2.py:152][0m Total time:       2.32 min
[32m[20230204 15:38:30 @agent_ppo2.py:154][0m 157696 total steps have happened
[32m[20230204 15:38:30 @agent_ppo2.py:130][0m #------------------------ Iteration 77 --------------------------#
[32m[20230204 15:38:30 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:38:30 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:30 @agent_ppo2.py:194][0m |          -0.0000 |           2.2095 |           2.6180 |
[32m[20230204 15:38:31 @agent_ppo2.py:194][0m |          -0.0030 |           1.9753 |           2.6182 |
[32m[20230204 15:38:31 @agent_ppo2.py:194][0m |          -0.0043 |           1.9401 |           2.6188 |
[32m[20230204 15:38:31 @agent_ppo2.py:194][0m |          -0.0053 |           1.9222 |           2.6183 |
[32m[20230204 15:38:31 @agent_ppo2.py:194][0m |          -0.0059 |           1.9016 |           2.6170 |
[32m[20230204 15:38:31 @agent_ppo2.py:194][0m |          -0.0061 |           1.8875 |           2.6183 |
[32m[20230204 15:38:31 @agent_ppo2.py:194][0m |          -0.0066 |           1.8717 |           2.6191 |
[32m[20230204 15:38:31 @agent_ppo2.py:194][0m |          -0.0075 |           1.8579 |           2.6200 |
[32m[20230204 15:38:31 @agent_ppo2.py:194][0m |          -0.0073 |           1.8515 |           2.6200 |
[32m[20230204 15:38:31 @agent_ppo2.py:194][0m |          -0.0076 |           1.8393 |           2.6205 |
[32m[20230204 15:38:31 @agent_ppo2.py:139][0m Policy update time: 1.25 s
[32m[20230204 15:38:32 @agent_ppo2.py:147][0m Average TRAINING episode reward: 184.47
[32m[20230204 15:38:32 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 196.51
[32m[20230204 15:38:32 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 29.78
[32m[20230204 15:38:32 @agent_ppo2.py:152][0m Total time:       2.35 min
[32m[20230204 15:38:32 @agent_ppo2.py:154][0m 159744 total steps have happened
[32m[20230204 15:38:32 @agent_ppo2.py:130][0m #------------------------ Iteration 78 --------------------------#
[32m[20230204 15:38:32 @agent_ppo2.py:136][0m Sampling time: 0.35 s by 4 slaves
[32m[20230204 15:38:32 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:32 @agent_ppo2.py:194][0m |           0.0071 |          13.8404 |           2.6047 |
[32m[20230204 15:38:32 @agent_ppo2.py:194][0m |          -0.0016 |           9.7009 |           2.5980 |
[32m[20230204 15:38:32 @agent_ppo2.py:194][0m |          -0.0062 |           8.7377 |           2.5995 |
[32m[20230204 15:38:33 @agent_ppo2.py:194][0m |          -0.0070 |           8.1437 |           2.5921 |
[32m[20230204 15:38:33 @agent_ppo2.py:194][0m |           0.0048 |           7.7631 |           2.5962 |
[32m[20230204 15:38:33 @agent_ppo2.py:194][0m |          -0.0075 |           7.3814 |           2.5968 |
[32m[20230204 15:38:33 @agent_ppo2.py:194][0m |          -0.0076 |           7.2542 |           2.5923 |
[32m[20230204 15:38:33 @agent_ppo2.py:194][0m |          -0.0096 |           6.7839 |           2.5953 |
[32m[20230204 15:38:33 @agent_ppo2.py:194][0m |          -0.0104 |           6.8826 |           2.5927 |
[32m[20230204 15:38:33 @agent_ppo2.py:194][0m |          -0.0096 |           6.5993 |           2.5947 |
[32m[20230204 15:38:33 @agent_ppo2.py:139][0m Policy update time: 1.09 s
[32m[20230204 15:38:33 @agent_ppo2.py:147][0m Average TRAINING episode reward: 111.50
[32m[20230204 15:38:33 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 209.66
[32m[20230204 15:38:33 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 82.85
[32m[20230204 15:38:33 @agent_ppo2.py:152][0m Total time:       2.38 min
[32m[20230204 15:38:33 @agent_ppo2.py:154][0m 161792 total steps have happened
[32m[20230204 15:38:33 @agent_ppo2.py:130][0m #------------------------ Iteration 79 --------------------------#
[32m[20230204 15:38:34 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:38:34 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:34 @agent_ppo2.py:194][0m |           0.0009 |          12.6031 |           2.5736 |
[32m[20230204 15:38:34 @agent_ppo2.py:194][0m |          -0.0024 |           8.3012 |           2.5674 |
[32m[20230204 15:38:34 @agent_ppo2.py:194][0m |          -0.0062 |           6.5137 |           2.5678 |
[32m[20230204 15:38:34 @agent_ppo2.py:194][0m |          -0.0070 |           6.0416 |           2.5665 |
[32m[20230204 15:38:34 @agent_ppo2.py:194][0m |          -0.0071 |           5.5445 |           2.5648 |
[32m[20230204 15:38:34 @agent_ppo2.py:194][0m |          -0.0082 |           5.2832 |           2.5638 |
[32m[20230204 15:38:35 @agent_ppo2.py:194][0m |          -0.0058 |           5.4842 |           2.5628 |
[32m[20230204 15:38:35 @agent_ppo2.py:194][0m |          -0.0088 |           4.9450 |           2.5624 |
[32m[20230204 15:38:35 @agent_ppo2.py:194][0m |          -0.0109 |           4.7961 |           2.5635 |
[32m[20230204 15:38:35 @agent_ppo2.py:194][0m |          -0.0099 |           4.5507 |           2.5618 |
[32m[20230204 15:38:35 @agent_ppo2.py:139][0m Policy update time: 1.13 s
[32m[20230204 15:38:35 @agent_ppo2.py:147][0m Average TRAINING episode reward: 138.57
[32m[20230204 15:38:35 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 191.56
[32m[20230204 15:38:35 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 233.72
[32m[20230204 15:38:35 @agent_ppo2.py:152][0m Total time:       2.41 min
[32m[20230204 15:38:35 @agent_ppo2.py:154][0m 163840 total steps have happened
[32m[20230204 15:38:35 @agent_ppo2.py:130][0m #------------------------ Iteration 80 --------------------------#
[32m[20230204 15:38:36 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:38:36 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:36 @agent_ppo2.py:194][0m |          -0.0023 |          15.8456 |           2.6270 |
[32m[20230204 15:38:36 @agent_ppo2.py:194][0m |          -0.0111 |          10.2493 |           2.6258 |
[32m[20230204 15:38:36 @agent_ppo2.py:194][0m |          -0.0084 |           8.3790 |           2.6213 |
[32m[20230204 15:38:36 @agent_ppo2.py:194][0m |          -0.0108 |           7.4423 |           2.6229 |
[32m[20230204 15:38:36 @agent_ppo2.py:194][0m |          -0.0090 |           6.7251 |           2.6230 |
[32m[20230204 15:38:36 @agent_ppo2.py:194][0m |          -0.0106 |           6.3372 |           2.6222 |
[32m[20230204 15:38:36 @agent_ppo2.py:194][0m |          -0.0106 |           5.9688 |           2.6208 |
[32m[20230204 15:38:36 @agent_ppo2.py:194][0m |          -0.0118 |           5.5642 |           2.6228 |
[32m[20230204 15:38:37 @agent_ppo2.py:194][0m |          -0.0101 |           5.2849 |           2.6219 |
[32m[20230204 15:38:37 @agent_ppo2.py:194][0m |          -0.0122 |           5.1325 |           2.6240 |
[32m[20230204 15:38:37 @agent_ppo2.py:139][0m Policy update time: 1.12 s
[32m[20230204 15:38:37 @agent_ppo2.py:147][0m Average TRAINING episode reward: 138.35
[32m[20230204 15:38:37 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 209.82
[32m[20230204 15:38:37 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -37.69
[32m[20230204 15:38:37 @agent_ppo2.py:152][0m Total time:       2.44 min
[32m[20230204 15:38:37 @agent_ppo2.py:154][0m 165888 total steps have happened
[32m[20230204 15:38:37 @agent_ppo2.py:130][0m #------------------------ Iteration 81 --------------------------#
[32m[20230204 15:38:37 @agent_ppo2.py:136][0m Sampling time: 0.40 s by 4 slaves
[32m[20230204 15:38:37 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:38 @agent_ppo2.py:194][0m |          -0.0001 |          24.0458 |           2.6970 |
[32m[20230204 15:38:38 @agent_ppo2.py:194][0m |          -0.0046 |          17.7903 |           2.6965 |
[32m[20230204 15:38:38 @agent_ppo2.py:194][0m |          -0.0060 |          16.3268 |           2.6980 |
[32m[20230204 15:38:38 @agent_ppo2.py:194][0m |          -0.0074 |          15.2661 |           2.6993 |
[32m[20230204 15:38:38 @agent_ppo2.py:194][0m |          -0.0087 |          14.8728 |           2.6982 |
[32m[20230204 15:38:38 @agent_ppo2.py:194][0m |          -0.0094 |          13.8674 |           2.7015 |
[32m[20230204 15:38:38 @agent_ppo2.py:194][0m |          -0.0098 |          13.8093 |           2.7011 |
[32m[20230204 15:38:38 @agent_ppo2.py:194][0m |          -0.0104 |          13.5348 |           2.7009 |
[32m[20230204 15:38:38 @agent_ppo2.py:194][0m |          -0.0105 |          12.9726 |           2.7017 |
[32m[20230204 15:38:39 @agent_ppo2.py:194][0m |          -0.0112 |          12.7387 |           2.7022 |
[32m[20230204 15:38:39 @agent_ppo2.py:139][0m Policy update time: 1.27 s
[32m[20230204 15:38:39 @agent_ppo2.py:147][0m Average TRAINING episode reward: 70.27
[32m[20230204 15:38:39 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 198.24
[32m[20230204 15:38:39 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 254.69
[32m[20230204 15:38:39 @agent_ppo2.py:152][0m Total time:       2.47 min
[32m[20230204 15:38:39 @agent_ppo2.py:154][0m 167936 total steps have happened
[32m[20230204 15:38:39 @agent_ppo2.py:130][0m #------------------------ Iteration 82 --------------------------#
[32m[20230204 15:38:39 @agent_ppo2.py:136][0m Sampling time: 0.37 s by 4 slaves
[32m[20230204 15:38:39 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:39 @agent_ppo2.py:194][0m |          -0.0032 |          17.0541 |           2.6116 |
[32m[20230204 15:38:40 @agent_ppo2.py:194][0m |          -0.0069 |          10.0890 |           2.6032 |
[32m[20230204 15:38:40 @agent_ppo2.py:194][0m |          -0.0062 |           8.6798 |           2.6001 |
[32m[20230204 15:38:40 @agent_ppo2.py:194][0m |          -0.0092 |           7.6129 |           2.6001 |
[32m[20230204 15:38:40 @agent_ppo2.py:194][0m |          -0.0074 |           6.9252 |           2.5988 |
[32m[20230204 15:38:40 @agent_ppo2.py:194][0m |          -0.0109 |           6.3614 |           2.5954 |
[32m[20230204 15:38:40 @agent_ppo2.py:194][0m |          -0.0093 |           6.0189 |           2.5952 |
[32m[20230204 15:38:40 @agent_ppo2.py:194][0m |          -0.0124 |           5.7086 |           2.5924 |
[32m[20230204 15:38:40 @agent_ppo2.py:194][0m |          -0.0121 |           5.4432 |           2.5933 |
[32m[20230204 15:38:40 @agent_ppo2.py:194][0m |          -0.0118 |           5.2678 |           2.5913 |
[32m[20230204 15:38:40 @agent_ppo2.py:139][0m Policy update time: 1.22 s
[32m[20230204 15:38:41 @agent_ppo2.py:147][0m Average TRAINING episode reward: 94.41
[32m[20230204 15:38:41 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 184.35
[32m[20230204 15:38:41 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 246.42
[32m[20230204 15:38:41 @agent_ppo2.py:152][0m Total time:       2.50 min
[32m[20230204 15:38:41 @agent_ppo2.py:154][0m 169984 total steps have happened
[32m[20230204 15:38:41 @agent_ppo2.py:130][0m #------------------------ Iteration 83 --------------------------#
[32m[20230204 15:38:41 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:38:41 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:41 @agent_ppo2.py:194][0m |           0.0010 |          11.0520 |           2.6633 |
[32m[20230204 15:38:41 @agent_ppo2.py:194][0m |          -0.0050 |           8.6493 |           2.6547 |
[32m[20230204 15:38:42 @agent_ppo2.py:194][0m |          -0.0067 |           8.1693 |           2.6506 |
[32m[20230204 15:38:42 @agent_ppo2.py:194][0m |          -0.0083 |           7.8894 |           2.6509 |
[32m[20230204 15:38:42 @agent_ppo2.py:194][0m |          -0.0111 |           7.7051 |           2.6515 |
[32m[20230204 15:38:42 @agent_ppo2.py:194][0m |          -0.0009 |           8.3230 |           2.6513 |
[32m[20230204 15:38:42 @agent_ppo2.py:194][0m |          -0.0103 |           7.2088 |           2.6504 |
[32m[20230204 15:38:42 @agent_ppo2.py:194][0m |          -0.0100 |           7.1805 |           2.6505 |
[32m[20230204 15:38:42 @agent_ppo2.py:194][0m |          -0.0110 |           6.8287 |           2.6477 |
[32m[20230204 15:38:42 @agent_ppo2.py:194][0m |          -0.0121 |           6.7237 |           2.6497 |
[32m[20230204 15:38:42 @agent_ppo2.py:139][0m Policy update time: 1.23 s
[32m[20230204 15:38:43 @agent_ppo2.py:147][0m Average TRAINING episode reward: 147.01
[32m[20230204 15:38:43 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 183.26
[32m[20230204 15:38:43 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 245.51
[32m[20230204 15:38:43 @agent_ppo2.py:152][0m Total time:       2.54 min
[32m[20230204 15:38:43 @agent_ppo2.py:154][0m 172032 total steps have happened
[32m[20230204 15:38:43 @agent_ppo2.py:130][0m #------------------------ Iteration 84 --------------------------#
[32m[20230204 15:38:43 @agent_ppo2.py:136][0m Sampling time: 0.35 s by 4 slaves
[32m[20230204 15:38:43 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:43 @agent_ppo2.py:194][0m |           0.0014 |           2.4252 |           2.6576 |
[32m[20230204 15:38:43 @agent_ppo2.py:194][0m |          -0.0021 |           2.1530 |           2.6581 |
[32m[20230204 15:38:43 @agent_ppo2.py:194][0m |          -0.0032 |           2.1050 |           2.6526 |
[32m[20230204 15:38:44 @agent_ppo2.py:194][0m |          -0.0046 |           2.0743 |           2.6534 |
[32m[20230204 15:38:44 @agent_ppo2.py:194][0m |          -0.0055 |           2.0506 |           2.6545 |
[32m[20230204 15:38:44 @agent_ppo2.py:194][0m |          -0.0061 |           2.0328 |           2.6543 |
[32m[20230204 15:38:44 @agent_ppo2.py:194][0m |          -0.0068 |           2.0174 |           2.6533 |
[32m[20230204 15:38:44 @agent_ppo2.py:194][0m |          -0.0073 |           2.0035 |           2.6514 |
[32m[20230204 15:38:44 @agent_ppo2.py:194][0m |          -0.0075 |           1.9893 |           2.6552 |
[32m[20230204 15:38:44 @agent_ppo2.py:194][0m |          -0.0075 |           1.9806 |           2.6542 |
[32m[20230204 15:38:44 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:38:44 @agent_ppo2.py:147][0m Average TRAINING episode reward: 186.62
[32m[20230204 15:38:44 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 201.68
[32m[20230204 15:38:44 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 236.80
[32m[20230204 15:38:44 @agent_ppo2.py:152][0m Total time:       2.57 min
[32m[20230204 15:38:44 @agent_ppo2.py:154][0m 174080 total steps have happened
[32m[20230204 15:38:44 @agent_ppo2.py:130][0m #------------------------ Iteration 85 --------------------------#
[32m[20230204 15:38:45 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:38:45 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:45 @agent_ppo2.py:194][0m |          -0.0001 |           2.4262 |           2.6991 |
[32m[20230204 15:38:45 @agent_ppo2.py:194][0m |          -0.0042 |           2.1538 |           2.6978 |
[32m[20230204 15:38:45 @agent_ppo2.py:194][0m |          -0.0059 |           2.0557 |           2.6986 |
[32m[20230204 15:38:45 @agent_ppo2.py:194][0m |          -0.0073 |           2.0045 |           2.7011 |
[32m[20230204 15:38:45 @agent_ppo2.py:194][0m |          -0.0075 |           1.9698 |           2.7018 |
[32m[20230204 15:38:46 @agent_ppo2.py:194][0m |          -0.0083 |           1.9495 |           2.7033 |
[32m[20230204 15:38:46 @agent_ppo2.py:194][0m |          -0.0086 |           1.9250 |           2.7047 |
[32m[20230204 15:38:46 @agent_ppo2.py:194][0m |          -0.0087 |           1.8988 |           2.7037 |
[32m[20230204 15:38:46 @agent_ppo2.py:194][0m |          -0.0093 |           1.8804 |           2.7071 |
[32m[20230204 15:38:46 @agent_ppo2.py:194][0m |          -0.0101 |           1.8654 |           2.7076 |
[32m[20230204 15:38:46 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:38:46 @agent_ppo2.py:147][0m Average TRAINING episode reward: 183.33
[32m[20230204 15:38:46 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 198.03
[32m[20230204 15:38:46 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 244.23
[32m[20230204 15:38:46 @agent_ppo2.py:152][0m Total time:       2.60 min
[32m[20230204 15:38:46 @agent_ppo2.py:154][0m 176128 total steps have happened
[32m[20230204 15:38:46 @agent_ppo2.py:130][0m #------------------------ Iteration 86 --------------------------#
[32m[20230204 15:38:47 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:38:47 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:47 @agent_ppo2.py:194][0m |          -0.0004 |           4.6985 |           2.6840 |
[32m[20230204 15:38:47 @agent_ppo2.py:194][0m |          -0.0054 |           3.1527 |           2.6810 |
[32m[20230204 15:38:47 @agent_ppo2.py:194][0m |          -0.0064 |           2.8938 |           2.6815 |
[32m[20230204 15:38:47 @agent_ppo2.py:194][0m |          -0.0074 |           2.7136 |           2.6844 |
[32m[20230204 15:38:47 @agent_ppo2.py:194][0m |          -0.0066 |           2.5943 |           2.6839 |
[32m[20230204 15:38:47 @agent_ppo2.py:194][0m |          -0.0075 |           2.5257 |           2.6851 |
[32m[20230204 15:38:47 @agent_ppo2.py:194][0m |          -0.0083 |           2.4400 |           2.6854 |
[32m[20230204 15:38:48 @agent_ppo2.py:194][0m |          -0.0078 |           2.3948 |           2.6872 |
[32m[20230204 15:38:48 @agent_ppo2.py:194][0m |          -0.0092 |           2.3484 |           2.6876 |
[32m[20230204 15:38:48 @agent_ppo2.py:194][0m |          -0.0090 |           2.3198 |           2.6874 |
[32m[20230204 15:38:48 @agent_ppo2.py:139][0m Policy update time: 1.09 s
[32m[20230204 15:38:48 @agent_ppo2.py:147][0m Average TRAINING episode reward: 126.52
[32m[20230204 15:38:48 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 198.09
[32m[20230204 15:38:48 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 248.09
[32m[20230204 15:38:48 @agent_ppo2.py:152][0m Total time:       2.63 min
[32m[20230204 15:38:48 @agent_ppo2.py:154][0m 178176 total steps have happened
[32m[20230204 15:38:48 @agent_ppo2.py:130][0m #------------------------ Iteration 87 --------------------------#
[32m[20230204 15:38:48 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:38:48 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:49 @agent_ppo2.py:194][0m |           0.0006 |           2.1352 |           2.7001 |
[32m[20230204 15:38:49 @agent_ppo2.py:194][0m |          -0.0027 |           1.9990 |           2.6985 |
[32m[20230204 15:38:49 @agent_ppo2.py:194][0m |          -0.0040 |           1.9530 |           2.6991 |
[32m[20230204 15:38:49 @agent_ppo2.py:194][0m |          -0.0049 |           1.9183 |           2.7022 |
[32m[20230204 15:38:49 @agent_ppo2.py:194][0m |          -0.0060 |           1.8925 |           2.7003 |
[32m[20230204 15:38:49 @agent_ppo2.py:194][0m |          -0.0062 |           1.8732 |           2.7033 |
[32m[20230204 15:38:49 @agent_ppo2.py:194][0m |          -0.0068 |           1.8482 |           2.7033 |
[32m[20230204 15:38:49 @agent_ppo2.py:194][0m |          -0.0072 |           1.8288 |           2.7040 |
[32m[20230204 15:38:49 @agent_ppo2.py:194][0m |          -0.0071 |           1.8121 |           2.7063 |
[32m[20230204 15:38:50 @agent_ppo2.py:194][0m |          -0.0076 |           1.7999 |           2.7071 |
[32m[20230204 15:38:50 @agent_ppo2.py:139][0m Policy update time: 1.26 s
[32m[20230204 15:38:50 @agent_ppo2.py:147][0m Average TRAINING episode reward: 178.81
[32m[20230204 15:38:50 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 182.40
[32m[20230204 15:38:50 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 227.79
[32m[20230204 15:38:50 @agent_ppo2.py:152][0m Total time:       2.66 min
[32m[20230204 15:38:50 @agent_ppo2.py:154][0m 180224 total steps have happened
[32m[20230204 15:38:50 @agent_ppo2.py:130][0m #------------------------ Iteration 88 --------------------------#
[32m[20230204 15:38:50 @agent_ppo2.py:136][0m Sampling time: 0.35 s by 4 slaves
[32m[20230204 15:38:50 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:50 @agent_ppo2.py:194][0m |           0.0011 |           5.7391 |           2.7691 |
[32m[20230204 15:38:51 @agent_ppo2.py:194][0m |          -0.0018 |           2.3570 |           2.7699 |
[32m[20230204 15:38:51 @agent_ppo2.py:194][0m |          -0.0037 |           2.0068 |           2.7686 |
[32m[20230204 15:38:51 @agent_ppo2.py:194][0m |          -0.0032 |           1.8960 |           2.7675 |
[32m[20230204 15:38:51 @agent_ppo2.py:194][0m |          -0.0053 |           1.8061 |           2.7659 |
[32m[20230204 15:38:51 @agent_ppo2.py:194][0m |          -0.0053 |           1.6843 |           2.7656 |
[32m[20230204 15:38:51 @agent_ppo2.py:194][0m |          -0.0059 |           1.6442 |           2.7643 |
[32m[20230204 15:38:51 @agent_ppo2.py:194][0m |          -0.0058 |           1.6220 |           2.7639 |
[32m[20230204 15:38:51 @agent_ppo2.py:194][0m |          -0.0064 |           1.5574 |           2.7631 |
[32m[20230204 15:38:51 @agent_ppo2.py:194][0m |          -0.0060 |           1.5783 |           2.7619 |
[32m[20230204 15:38:51 @agent_ppo2.py:139][0m Policy update time: 1.19 s
[32m[20230204 15:38:52 @agent_ppo2.py:147][0m Average TRAINING episode reward: 153.77
[32m[20230204 15:38:52 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 206.67
[32m[20230204 15:38:52 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 253.40
[32m[20230204 15:38:52 @agent_ppo2.py:152][0m Total time:       2.69 min
[32m[20230204 15:38:52 @agent_ppo2.py:154][0m 182272 total steps have happened
[32m[20230204 15:38:52 @agent_ppo2.py:130][0m #------------------------ Iteration 89 --------------------------#
[32m[20230204 15:38:52 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:38:52 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:52 @agent_ppo2.py:194][0m |           0.0012 |           2.2225 |           2.7887 |
[32m[20230204 15:38:52 @agent_ppo2.py:194][0m |          -0.0013 |           2.0906 |           2.7873 |
[32m[20230204 15:38:52 @agent_ppo2.py:194][0m |          -0.0025 |           2.0411 |           2.7849 |
[32m[20230204 15:38:53 @agent_ppo2.py:194][0m |          -0.0033 |           2.0108 |           2.7865 |
[32m[20230204 15:38:53 @agent_ppo2.py:194][0m |          -0.0037 |           1.9833 |           2.7854 |
[32m[20230204 15:38:53 @agent_ppo2.py:194][0m |          -0.0043 |           1.9692 |           2.7877 |
[32m[20230204 15:38:53 @agent_ppo2.py:194][0m |          -0.0047 |           1.9439 |           2.7881 |
[32m[20230204 15:38:53 @agent_ppo2.py:194][0m |          -0.0052 |           1.9313 |           2.7892 |
[32m[20230204 15:38:53 @agent_ppo2.py:194][0m |          -0.0057 |           1.9191 |           2.7902 |
[32m[20230204 15:38:53 @agent_ppo2.py:194][0m |          -0.0055 |           1.9079 |           2.7897 |
[32m[20230204 15:38:53 @agent_ppo2.py:139][0m Policy update time: 1.24 s
[32m[20230204 15:38:54 @agent_ppo2.py:147][0m Average TRAINING episode reward: 188.40
[32m[20230204 15:38:54 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 203.97
[32m[20230204 15:38:54 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 248.41
[32m[20230204 15:38:54 @agent_ppo2.py:152][0m Total time:       2.72 min
[32m[20230204 15:38:54 @agent_ppo2.py:154][0m 184320 total steps have happened
[32m[20230204 15:38:54 @agent_ppo2.py:130][0m #------------------------ Iteration 90 --------------------------#
[32m[20230204 15:38:54 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:38:54 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:54 @agent_ppo2.py:194][0m |           0.0002 |           2.2105 |           2.7667 |
[32m[20230204 15:38:54 @agent_ppo2.py:194][0m |          -0.0037 |           2.1154 |           2.7643 |
[32m[20230204 15:38:54 @agent_ppo2.py:194][0m |          -0.0055 |           2.0548 |           2.7651 |
[32m[20230204 15:38:54 @agent_ppo2.py:194][0m |          -0.0060 |           2.0176 |           2.7666 |
[32m[20230204 15:38:55 @agent_ppo2.py:194][0m |          -0.0063 |           1.9874 |           2.7685 |
[32m[20230204 15:38:55 @agent_ppo2.py:194][0m |          -0.0072 |           1.9598 |           2.7700 |
[32m[20230204 15:38:55 @agent_ppo2.py:194][0m |          -0.0081 |           1.9199 |           2.7738 |
[32m[20230204 15:38:55 @agent_ppo2.py:194][0m |          -0.0080 |           1.9080 |           2.7743 |
[32m[20230204 15:38:55 @agent_ppo2.py:194][0m |          -0.0088 |           1.8718 |           2.7772 |
[32m[20230204 15:38:55 @agent_ppo2.py:194][0m |          -0.0089 |           1.8513 |           2.7773 |
[32m[20230204 15:38:55 @agent_ppo2.py:139][0m Policy update time: 1.25 s
[32m[20230204 15:38:56 @agent_ppo2.py:147][0m Average TRAINING episode reward: 195.58
[32m[20230204 15:38:56 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 207.60
[32m[20230204 15:38:56 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 97.38
[32m[20230204 15:38:56 @agent_ppo2.py:152][0m Total time:       2.75 min
[32m[20230204 15:38:56 @agent_ppo2.py:154][0m 186368 total steps have happened
[32m[20230204 15:38:56 @agent_ppo2.py:130][0m #------------------------ Iteration 91 --------------------------#
[32m[20230204 15:38:56 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:38:56 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:56 @agent_ppo2.py:194][0m |           0.0005 |           2.2394 |           2.8362 |
[32m[20230204 15:38:56 @agent_ppo2.py:194][0m |          -0.0038 |           2.1345 |           2.8356 |
[32m[20230204 15:38:56 @agent_ppo2.py:194][0m |          -0.0059 |           2.0893 |           2.8322 |
[32m[20230204 15:38:56 @agent_ppo2.py:194][0m |          -0.0066 |           2.0651 |           2.8339 |
[32m[20230204 15:38:57 @agent_ppo2.py:194][0m |          -0.0076 |           2.0425 |           2.8332 |
[32m[20230204 15:38:57 @agent_ppo2.py:194][0m |          -0.0077 |           2.0203 |           2.8355 |
[32m[20230204 15:38:57 @agent_ppo2.py:194][0m |          -0.0081 |           2.0111 |           2.8340 |
[32m[20230204 15:38:57 @agent_ppo2.py:194][0m |          -0.0088 |           1.9988 |           2.8345 |
[32m[20230204 15:38:57 @agent_ppo2.py:194][0m |          -0.0089 |           1.9829 |           2.8352 |
[32m[20230204 15:38:57 @agent_ppo2.py:194][0m |          -0.0094 |           1.9706 |           2.8347 |
[32m[20230204 15:38:57 @agent_ppo2.py:139][0m Policy update time: 1.26 s
[32m[20230204 15:38:57 @agent_ppo2.py:147][0m Average TRAINING episode reward: 192.08
[32m[20230204 15:38:57 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 201.22
[32m[20230204 15:38:57 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 255.85
[32m[20230204 15:38:57 @agent_ppo2.py:152][0m Total time:       2.78 min
[32m[20230204 15:38:57 @agent_ppo2.py:154][0m 188416 total steps have happened
[32m[20230204 15:38:57 @agent_ppo2.py:130][0m #------------------------ Iteration 92 --------------------------#
[32m[20230204 15:38:58 @agent_ppo2.py:136][0m Sampling time: 0.39 s by 4 slaves
[32m[20230204 15:38:58 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:38:58 @agent_ppo2.py:194][0m |           0.0013 |           4.9908 |           2.8215 |
[32m[20230204 15:38:58 @agent_ppo2.py:194][0m |          -0.0029 |           3.9154 |           2.8224 |
[32m[20230204 15:38:58 @agent_ppo2.py:194][0m |          -0.0040 |           3.5738 |           2.8225 |
[32m[20230204 15:38:58 @agent_ppo2.py:194][0m |          -0.0052 |           3.3908 |           2.8242 |
[32m[20230204 15:38:59 @agent_ppo2.py:194][0m |          -0.0060 |           3.2886 |           2.8258 |
[32m[20230204 15:38:59 @agent_ppo2.py:194][0m |          -0.0074 |           3.1560 |           2.8284 |
[32m[20230204 15:38:59 @agent_ppo2.py:194][0m |          -0.0076 |           3.0849 |           2.8286 |
[32m[20230204 15:38:59 @agent_ppo2.py:194][0m |          -0.0070 |           3.0122 |           2.8305 |
[32m[20230204 15:38:59 @agent_ppo2.py:194][0m |          -0.0085 |           2.9318 |           2.8326 |
[32m[20230204 15:38:59 @agent_ppo2.py:194][0m |          -0.0078 |           2.8604 |           2.8307 |
[32m[20230204 15:38:59 @agent_ppo2.py:139][0m Policy update time: 1.28 s
[32m[20230204 15:38:59 @agent_ppo2.py:147][0m Average TRAINING episode reward: 132.97
[32m[20230204 15:38:59 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 192.05
[32m[20230204 15:38:59 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 229.63
[32m[20230204 15:38:59 @agent_ppo2.py:152][0m Total time:       2.82 min
[32m[20230204 15:38:59 @agent_ppo2.py:154][0m 190464 total steps have happened
[32m[20230204 15:38:59 @agent_ppo2.py:130][0m #------------------------ Iteration 93 --------------------------#
[32m[20230204 15:39:00 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:00 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:00 @agent_ppo2.py:194][0m |          -0.0003 |          10.4871 |           2.8770 |
[32m[20230204 15:39:00 @agent_ppo2.py:194][0m |          -0.0019 |           8.5404 |           2.8725 |
[32m[20230204 15:39:00 @agent_ppo2.py:194][0m |          -0.0017 |           8.0867 |           2.8711 |
[32m[20230204 15:39:00 @agent_ppo2.py:194][0m |          -0.0050 |           7.5777 |           2.8685 |
[32m[20230204 15:39:00 @agent_ppo2.py:194][0m |          -0.0019 |           7.3341 |           2.8680 |
[32m[20230204 15:39:00 @agent_ppo2.py:194][0m |          -0.0059 |           7.1596 |           2.8651 |
[32m[20230204 15:39:00 @agent_ppo2.py:194][0m |          -0.0073 |           6.9114 |           2.8665 |
[32m[20230204 15:39:01 @agent_ppo2.py:194][0m |          -0.0072 |           6.7161 |           2.8663 |
[32m[20230204 15:39:01 @agent_ppo2.py:194][0m |          -0.0077 |           6.6961 |           2.8643 |
[32m[20230204 15:39:01 @agent_ppo2.py:194][0m |          -0.0088 |           6.7926 |           2.8651 |
[32m[20230204 15:39:01 @agent_ppo2.py:139][0m Policy update time: 1.07 s
[32m[20230204 15:39:01 @agent_ppo2.py:147][0m Average TRAINING episode reward: 150.02
[32m[20230204 15:39:01 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 220.33
[32m[20230204 15:39:01 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 5.26
[32m[20230204 15:39:01 @agent_ppo2.py:152][0m Total time:       2.84 min
[32m[20230204 15:39:01 @agent_ppo2.py:154][0m 192512 total steps have happened
[32m[20230204 15:39:01 @agent_ppo2.py:130][0m #------------------------ Iteration 94 --------------------------#
[32m[20230204 15:39:01 @agent_ppo2.py:136][0m Sampling time: 0.35 s by 4 slaves
[32m[20230204 15:39:01 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:01 @agent_ppo2.py:194][0m |           0.0005 |          19.2002 |           2.9145 |
[32m[20230204 15:39:02 @agent_ppo2.py:194][0m |          -0.0045 |          17.4949 |           2.9112 |
[32m[20230204 15:39:02 @agent_ppo2.py:194][0m |          -0.0061 |          15.7116 |           2.9113 |
[32m[20230204 15:39:02 @agent_ppo2.py:194][0m |          -0.0074 |          14.7951 |           2.9103 |
[32m[20230204 15:39:02 @agent_ppo2.py:194][0m |          -0.0075 |          14.4424 |           2.9080 |
[32m[20230204 15:39:02 @agent_ppo2.py:194][0m |          -0.0092 |          13.6589 |           2.9077 |
[32m[20230204 15:39:02 @agent_ppo2.py:194][0m |          -0.0100 |          13.2363 |           2.9097 |
[32m[20230204 15:39:02 @agent_ppo2.py:194][0m |          -0.0103 |          13.1921 |           2.9092 |
[32m[20230204 15:39:02 @agent_ppo2.py:194][0m |          -0.0105 |          12.8952 |           2.9078 |
[32m[20230204 15:39:02 @agent_ppo2.py:194][0m |          -0.0108 |          12.8496 |           2.9068 |
[32m[20230204 15:39:02 @agent_ppo2.py:139][0m Policy update time: 1.17 s
[32m[20230204 15:39:03 @agent_ppo2.py:147][0m Average TRAINING episode reward: 159.14
[32m[20230204 15:39:03 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 201.31
[32m[20230204 15:39:03 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 241.52
[32m[20230204 15:39:03 @agent_ppo2.py:152][0m Total time:       2.87 min
[32m[20230204 15:39:03 @agent_ppo2.py:154][0m 194560 total steps have happened
[32m[20230204 15:39:03 @agent_ppo2.py:130][0m #------------------------ Iteration 95 --------------------------#
[32m[20230204 15:39:03 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:39:03 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:03 @agent_ppo2.py:194][0m |           0.0010 |           4.7833 |           2.9076 |
[32m[20230204 15:39:03 @agent_ppo2.py:194][0m |          -0.0033 |           3.9762 |           2.9049 |
[32m[20230204 15:39:04 @agent_ppo2.py:194][0m |          -0.0053 |           3.8450 |           2.9036 |
[32m[20230204 15:39:04 @agent_ppo2.py:194][0m |          -0.0069 |           3.7745 |           2.9050 |
[32m[20230204 15:39:04 @agent_ppo2.py:194][0m |          -0.0072 |           3.7218 |           2.9077 |
[32m[20230204 15:39:04 @agent_ppo2.py:194][0m |          -0.0074 |           3.6825 |           2.9069 |
[32m[20230204 15:39:04 @agent_ppo2.py:194][0m |          -0.0084 |           3.6463 |           2.9099 |
[32m[20230204 15:39:04 @agent_ppo2.py:194][0m |          -0.0083 |           3.6126 |           2.9126 |
[32m[20230204 15:39:04 @agent_ppo2.py:194][0m |          -0.0097 |           3.5826 |           2.9130 |
[32m[20230204 15:39:04 @agent_ppo2.py:194][0m |          -0.0094 |           3.5435 |           2.9140 |
[32m[20230204 15:39:04 @agent_ppo2.py:139][0m Policy update time: 1.26 s
[32m[20230204 15:39:05 @agent_ppo2.py:147][0m Average TRAINING episode reward: 202.85
[32m[20230204 15:39:05 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 209.05
[32m[20230204 15:39:05 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 260.55
[32m[20230204 15:39:05 @agent_ppo2.py:152][0m Total time:       2.90 min
[32m[20230204 15:39:05 @agent_ppo2.py:154][0m 196608 total steps have happened
[32m[20230204 15:39:05 @agent_ppo2.py:130][0m #------------------------ Iteration 96 --------------------------#
[32m[20230204 15:39:05 @agent_ppo2.py:136][0m Sampling time: 0.38 s by 4 slaves
[32m[20230204 15:39:05 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:05 @agent_ppo2.py:194][0m |          -0.0010 |           2.6892 |           2.9676 |
[32m[20230204 15:39:05 @agent_ppo2.py:194][0m |          -0.0059 |           2.4293 |           2.9633 |
[32m[20230204 15:39:05 @agent_ppo2.py:194][0m |          -0.0070 |           2.3581 |           2.9597 |
[32m[20230204 15:39:06 @agent_ppo2.py:194][0m |          -0.0078 |           2.3068 |           2.9590 |
[32m[20230204 15:39:06 @agent_ppo2.py:194][0m |          -0.0082 |           2.2767 |           2.9586 |
[32m[20230204 15:39:06 @agent_ppo2.py:194][0m |          -0.0087 |           2.2458 |           2.9570 |
[32m[20230204 15:39:06 @agent_ppo2.py:194][0m |          -0.0090 |           2.2205 |           2.9562 |
[32m[20230204 15:39:06 @agent_ppo2.py:194][0m |          -0.0094 |           2.2019 |           2.9583 |
[32m[20230204 15:39:06 @agent_ppo2.py:194][0m |          -0.0091 |           2.1852 |           2.9574 |
[32m[20230204 15:39:06 @agent_ppo2.py:194][0m |          -0.0096 |           2.1638 |           2.9556 |
[32m[20230204 15:39:06 @agent_ppo2.py:139][0m Policy update time: 1.25 s
[32m[20230204 15:39:06 @agent_ppo2.py:147][0m Average TRAINING episode reward: 201.69
[32m[20230204 15:39:06 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 214.14
[32m[20230204 15:39:06 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 46.74
[32m[20230204 15:39:06 @agent_ppo2.py:152][0m Total time:       2.93 min
[32m[20230204 15:39:06 @agent_ppo2.py:154][0m 198656 total steps have happened
[32m[20230204 15:39:06 @agent_ppo2.py:130][0m #------------------------ Iteration 97 --------------------------#
[32m[20230204 15:39:07 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:07 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:07 @agent_ppo2.py:194][0m |           0.0013 |          11.1164 |           2.9580 |
[32m[20230204 15:39:07 @agent_ppo2.py:194][0m |          -0.0022 |           5.0673 |           2.9564 |
[32m[20230204 15:39:07 @agent_ppo2.py:194][0m |          -0.0047 |           4.3208 |           2.9517 |
[32m[20230204 15:39:07 @agent_ppo2.py:194][0m |          -0.0038 |           4.1661 |           2.9519 |
[32m[20230204 15:39:07 @agent_ppo2.py:194][0m |          -0.0044 |           3.8151 |           2.9508 |
[32m[20230204 15:39:07 @agent_ppo2.py:194][0m |          -0.0056 |           3.6976 |           2.9503 |
[32m[20230204 15:39:08 @agent_ppo2.py:194][0m |          -0.0082 |           3.4603 |           2.9491 |
[32m[20230204 15:39:08 @agent_ppo2.py:194][0m |          -0.0099 |           3.3555 |           2.9496 |
[32m[20230204 15:39:08 @agent_ppo2.py:194][0m |          -0.0091 |           3.2003 |           2.9490 |
[32m[20230204 15:39:08 @agent_ppo2.py:194][0m |          -0.0093 |           3.0783 |           2.9492 |
[32m[20230204 15:39:08 @agent_ppo2.py:139][0m Policy update time: 1.03 s
[32m[20230204 15:39:08 @agent_ppo2.py:147][0m Average TRAINING episode reward: 124.16
[32m[20230204 15:39:08 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 219.16
[32m[20230204 15:39:08 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -1.83
[32m[20230204 15:39:08 @agent_ppo2.py:152][0m Total time:       2.96 min
[32m[20230204 15:39:08 @agent_ppo2.py:154][0m 200704 total steps have happened
[32m[20230204 15:39:08 @agent_ppo2.py:130][0m #------------------------ Iteration 98 --------------------------#
[32m[20230204 15:39:08 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:39:08 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:08 @agent_ppo2.py:194][0m |           0.0019 |          24.8162 |           2.8641 |
[32m[20230204 15:39:09 @agent_ppo2.py:194][0m |          -0.0056 |          18.4222 |           2.8636 |
[32m[20230204 15:39:09 @agent_ppo2.py:194][0m |          -0.0064 |          16.1805 |           2.8663 |
[32m[20230204 15:39:09 @agent_ppo2.py:194][0m |          -0.0059 |          15.4670 |           2.8662 |
[32m[20230204 15:39:09 @agent_ppo2.py:194][0m |          -0.0079 |          15.3893 |           2.8662 |
[32m[20230204 15:39:09 @agent_ppo2.py:194][0m |          -0.0116 |          14.0066 |           2.8652 |
[32m[20230204 15:39:09 @agent_ppo2.py:194][0m |          -0.0081 |          13.7879 |           2.8663 |
[32m[20230204 15:39:09 @agent_ppo2.py:194][0m |          -0.0138 |          13.3152 |           2.8658 |
[32m[20230204 15:39:09 @agent_ppo2.py:194][0m |          -0.0136 |          12.9726 |           2.8654 |
[32m[20230204 15:39:09 @agent_ppo2.py:194][0m |          -0.0129 |          12.6738 |           2.8673 |
[32m[20230204 15:39:09 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:39:09 @agent_ppo2.py:147][0m Average TRAINING episode reward: 62.99
[32m[20230204 15:39:09 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 205.72
[32m[20230204 15:39:09 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 136.13
[32m[20230204 15:39:09 @agent_ppo2.py:152][0m Total time:       2.98 min
[32m[20230204 15:39:09 @agent_ppo2.py:154][0m 202752 total steps have happened
[32m[20230204 15:39:09 @agent_ppo2.py:130][0m #------------------------ Iteration 99 --------------------------#
[32m[20230204 15:39:10 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:39:10 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:10 @agent_ppo2.py:194][0m |          -0.0005 |           8.9879 |           2.9561 |
[32m[20230204 15:39:10 @agent_ppo2.py:194][0m |          -0.0048 |           7.2531 |           2.9567 |
[32m[20230204 15:39:10 @agent_ppo2.py:194][0m |          -0.0069 |           6.6905 |           2.9578 |
[32m[20230204 15:39:10 @agent_ppo2.py:194][0m |          -0.0077 |           6.3553 |           2.9580 |
[32m[20230204 15:39:10 @agent_ppo2.py:194][0m |          -0.0087 |           6.0588 |           2.9578 |
[32m[20230204 15:39:11 @agent_ppo2.py:194][0m |          -0.0092 |           5.8268 |           2.9605 |
[32m[20230204 15:39:11 @agent_ppo2.py:194][0m |          -0.0096 |           5.7036 |           2.9612 |
[32m[20230204 15:39:11 @agent_ppo2.py:194][0m |          -0.0103 |           5.5349 |           2.9640 |
[32m[20230204 15:39:11 @agent_ppo2.py:194][0m |          -0.0104 |           5.4182 |           2.9660 |
[32m[20230204 15:39:11 @agent_ppo2.py:194][0m |          -0.0110 |           5.2950 |           2.9669 |
[32m[20230204 15:39:11 @agent_ppo2.py:139][0m Policy update time: 1.25 s
[32m[20230204 15:39:11 @agent_ppo2.py:147][0m Average TRAINING episode reward: 196.77
[32m[20230204 15:39:11 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 220.79
[32m[20230204 15:39:11 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 27.48
[32m[20230204 15:39:11 @agent_ppo2.py:152][0m Total time:       3.01 min
[32m[20230204 15:39:11 @agent_ppo2.py:154][0m 204800 total steps have happened
[32m[20230204 15:39:11 @agent_ppo2.py:130][0m #------------------------ Iteration 100 --------------------------#
[32m[20230204 15:39:12 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:12 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:12 @agent_ppo2.py:194][0m |          -0.0020 |          22.1968 |           3.0589 |
[32m[20230204 15:39:12 @agent_ppo2.py:194][0m |          -0.0072 |          17.0718 |           3.0513 |
[32m[20230204 15:39:12 @agent_ppo2.py:194][0m |          -0.0089 |          15.4012 |           3.0502 |
[32m[20230204 15:39:12 @agent_ppo2.py:194][0m |          -0.0099 |          14.4617 |           3.0489 |
[32m[20230204 15:39:12 @agent_ppo2.py:194][0m |          -0.0107 |          13.6606 |           3.0442 |
[32m[20230204 15:39:12 @agent_ppo2.py:194][0m |          -0.0112 |          12.8042 |           3.0464 |
[32m[20230204 15:39:12 @agent_ppo2.py:194][0m |          -0.0118 |          12.2551 |           3.0445 |
[32m[20230204 15:39:12 @agent_ppo2.py:194][0m |          -0.0118 |          11.9787 |           3.0454 |
[32m[20230204 15:39:12 @agent_ppo2.py:194][0m |          -0.0117 |          12.1894 |           3.0424 |
[32m[20230204 15:39:13 @agent_ppo2.py:194][0m |          -0.0128 |          11.1713 |           3.0426 |
[32m[20230204 15:39:13 @agent_ppo2.py:139][0m Policy update time: 0.97 s
[32m[20230204 15:39:13 @agent_ppo2.py:147][0m Average TRAINING episode reward: 72.83
[32m[20230204 15:39:13 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 192.02
[32m[20230204 15:39:13 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 230.98
[32m[20230204 15:39:13 @agent_ppo2.py:152][0m Total time:       3.04 min
[32m[20230204 15:39:13 @agent_ppo2.py:154][0m 206848 total steps have happened
[32m[20230204 15:39:13 @agent_ppo2.py:130][0m #------------------------ Iteration 101 --------------------------#
[32m[20230204 15:39:13 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:13 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:13 @agent_ppo2.py:194][0m |           0.0004 |          30.7513 |           3.0744 |
[32m[20230204 15:39:14 @agent_ppo2.py:194][0m |          -0.0060 |          26.2913 |           3.0756 |
[32m[20230204 15:39:14 @agent_ppo2.py:194][0m |          -0.0058 |          24.1305 |           3.0744 |
[32m[20230204 15:39:14 @agent_ppo2.py:194][0m |          -0.0060 |          22.1710 |           3.0740 |
[32m[20230204 15:39:14 @agent_ppo2.py:194][0m |          -0.0083 |          20.9110 |           3.0730 |
[32m[20230204 15:39:14 @agent_ppo2.py:194][0m |          -0.0088 |          20.0982 |           3.0742 |
[32m[20230204 15:39:14 @agent_ppo2.py:194][0m |          -0.0094 |          19.5780 |           3.0736 |
[32m[20230204 15:39:14 @agent_ppo2.py:194][0m |          -0.0084 |          18.8745 |           3.0751 |
[32m[20230204 15:39:14 @agent_ppo2.py:194][0m |          -0.0108 |          18.3500 |           3.0749 |
[32m[20230204 15:39:14 @agent_ppo2.py:194][0m |          -0.0113 |          17.8269 |           3.0754 |
[32m[20230204 15:39:14 @agent_ppo2.py:139][0m Policy update time: 1.09 s
[32m[20230204 15:39:15 @agent_ppo2.py:147][0m Average TRAINING episode reward: 108.81
[32m[20230204 15:39:15 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 210.49
[32m[20230204 15:39:15 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 253.90
[32m[20230204 15:39:15 @agent_ppo2.py:152][0m Total time:       3.07 min
[32m[20230204 15:39:15 @agent_ppo2.py:154][0m 208896 total steps have happened
[32m[20230204 15:39:15 @agent_ppo2.py:130][0m #------------------------ Iteration 102 --------------------------#
[32m[20230204 15:39:15 @agent_ppo2.py:136][0m Sampling time: 0.35 s by 4 slaves
[32m[20230204 15:39:15 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:15 @agent_ppo2.py:194][0m |          -0.0032 |           9.6457 |           3.0091 |
[32m[20230204 15:39:15 @agent_ppo2.py:194][0m |          -0.0028 |           7.1387 |           3.0050 |
[32m[20230204 15:39:15 @agent_ppo2.py:194][0m |          -0.0051 |           6.1824 |           3.0067 |
[32m[20230204 15:39:16 @agent_ppo2.py:194][0m |          -0.0062 |           5.7363 |           3.0031 |
[32m[20230204 15:39:16 @agent_ppo2.py:194][0m |          -0.0076 |           5.3967 |           3.0028 |
[32m[20230204 15:39:16 @agent_ppo2.py:194][0m |          -0.0053 |           5.2146 |           2.9996 |
[32m[20230204 15:39:16 @agent_ppo2.py:194][0m |          -0.0088 |           4.9606 |           3.0001 |
[32m[20230204 15:39:16 @agent_ppo2.py:194][0m |          -0.0088 |           4.9181 |           2.9985 |
[32m[20230204 15:39:16 @agent_ppo2.py:194][0m |          -0.0078 |           4.7102 |           2.9983 |
[32m[20230204 15:39:16 @agent_ppo2.py:194][0m |          -0.0066 |           4.6624 |           2.9984 |
[32m[20230204 15:39:16 @agent_ppo2.py:139][0m Policy update time: 1.38 s
[32m[20230204 15:39:17 @agent_ppo2.py:147][0m Average TRAINING episode reward: 138.23
[32m[20230204 15:39:17 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 201.47
[32m[20230204 15:39:17 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 26.75
[32m[20230204 15:39:17 @agent_ppo2.py:152][0m Total time:       3.10 min
[32m[20230204 15:39:17 @agent_ppo2.py:154][0m 210944 total steps have happened
[32m[20230204 15:39:17 @agent_ppo2.py:130][0m #------------------------ Iteration 103 --------------------------#
[32m[20230204 15:39:17 @agent_ppo2.py:136][0m Sampling time: 0.38 s by 4 slaves
[32m[20230204 15:39:17 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:17 @agent_ppo2.py:194][0m |           0.0011 |           3.5636 |           3.0343 |
[32m[20230204 15:39:17 @agent_ppo2.py:194][0m |          -0.0028 |           3.0914 |           3.0307 |
[32m[20230204 15:39:17 @agent_ppo2.py:194][0m |          -0.0046 |           2.9772 |           3.0296 |
[32m[20230204 15:39:18 @agent_ppo2.py:194][0m |          -0.0049 |           2.9049 |           3.0286 |
[32m[20230204 15:39:18 @agent_ppo2.py:194][0m |          -0.0055 |           2.8557 |           3.0327 |
[32m[20230204 15:39:18 @agent_ppo2.py:194][0m |          -0.0061 |           2.8125 |           3.0302 |
[32m[20230204 15:39:18 @agent_ppo2.py:194][0m |          -0.0062 |           2.7745 |           3.0301 |
[32m[20230204 15:39:18 @agent_ppo2.py:194][0m |          -0.0069 |           2.7453 |           3.0310 |
[32m[20230204 15:39:18 @agent_ppo2.py:194][0m |          -0.0073 |           2.7182 |           3.0297 |
[32m[20230204 15:39:18 @agent_ppo2.py:194][0m |          -0.0071 |           2.6921 |           3.0287 |
[32m[20230204 15:39:18 @agent_ppo2.py:139][0m Policy update time: 1.30 s
[32m[20230204 15:39:19 @agent_ppo2.py:147][0m Average TRAINING episode reward: 203.18
[32m[20230204 15:39:19 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 217.29
[32m[20230204 15:39:19 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 257.05
[32m[20230204 15:39:19 @agent_ppo2.py:152][0m Total time:       3.13 min
[32m[20230204 15:39:19 @agent_ppo2.py:154][0m 212992 total steps have happened
[32m[20230204 15:39:19 @agent_ppo2.py:130][0m #------------------------ Iteration 104 --------------------------#
[32m[20230204 15:39:19 @agent_ppo2.py:136][0m Sampling time: 0.37 s by 4 slaves
[32m[20230204 15:39:19 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:19 @agent_ppo2.py:194][0m |          -0.0002 |           9.6406 |           3.0690 |
[32m[20230204 15:39:19 @agent_ppo2.py:194][0m |          -0.0043 |           6.0564 |           3.0645 |
[32m[20230204 15:39:19 @agent_ppo2.py:194][0m |          -0.0061 |           5.2326 |           3.0637 |
[32m[20230204 15:39:20 @agent_ppo2.py:194][0m |          -0.0066 |           5.4503 |           3.0604 |
[32m[20230204 15:39:20 @agent_ppo2.py:194][0m |          -0.0076 |           5.1452 |           3.0592 |
[32m[20230204 15:39:20 @agent_ppo2.py:194][0m |          -0.0077 |           4.8412 |           3.0598 |
[32m[20230204 15:39:20 @agent_ppo2.py:194][0m |          -0.0087 |           4.7360 |           3.0580 |
[32m[20230204 15:39:20 @agent_ppo2.py:194][0m |          -0.0092 |           4.5536 |           3.0580 |
[32m[20230204 15:39:20 @agent_ppo2.py:194][0m |          -0.0087 |           4.4958 |           3.0575 |
[32m[20230204 15:39:20 @agent_ppo2.py:194][0m |          -0.0100 |           4.3939 |           3.0572 |
[32m[20230204 15:39:20 @agent_ppo2.py:139][0m Policy update time: 1.36 s
[32m[20230204 15:39:21 @agent_ppo2.py:147][0m Average TRAINING episode reward: 152.35
[32m[20230204 15:39:21 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 212.59
[32m[20230204 15:39:21 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 247.66
[32m[20230204 15:39:21 @agent_ppo2.py:152][0m Total time:       3.17 min
[32m[20230204 15:39:21 @agent_ppo2.py:154][0m 215040 total steps have happened
[32m[20230204 15:39:21 @agent_ppo2.py:130][0m #------------------------ Iteration 105 --------------------------#
[32m[20230204 15:39:21 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:21 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:21 @agent_ppo2.py:194][0m |           0.0007 |           2.8090 |           3.0439 |
[32m[20230204 15:39:21 @agent_ppo2.py:194][0m |          -0.0035 |           2.5414 |           3.0418 |
[32m[20230204 15:39:21 @agent_ppo2.py:194][0m |          -0.0046 |           2.4672 |           3.0396 |
[32m[20230204 15:39:21 @agent_ppo2.py:194][0m |          -0.0057 |           2.4227 |           3.0387 |
[32m[20230204 15:39:22 @agent_ppo2.py:194][0m |          -0.0065 |           2.3826 |           3.0406 |
[32m[20230204 15:39:22 @agent_ppo2.py:194][0m |          -0.0069 |           2.3494 |           3.0424 |
[32m[20230204 15:39:22 @agent_ppo2.py:194][0m |          -0.0077 |           2.3192 |           3.0429 |
[32m[20230204 15:39:22 @agent_ppo2.py:194][0m |          -0.0079 |           2.3004 |           3.0431 |
[32m[20230204 15:39:22 @agent_ppo2.py:194][0m |          -0.0081 |           2.2751 |           3.0446 |
[32m[20230204 15:39:22 @agent_ppo2.py:194][0m |          -0.0088 |           2.2534 |           3.0446 |
[32m[20230204 15:39:22 @agent_ppo2.py:139][0m Policy update time: 1.25 s
[32m[20230204 15:39:22 @agent_ppo2.py:147][0m Average TRAINING episode reward: 207.79
[32m[20230204 15:39:22 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 226.52
[32m[20230204 15:39:22 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 262.00
[32m[20230204 15:39:22 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 262.00
[32m[20230204 15:39:22 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 262.00
[32m[20230204 15:39:22 @agent_ppo2.py:152][0m Total time:       3.20 min
[32m[20230204 15:39:22 @agent_ppo2.py:154][0m 217088 total steps have happened
[32m[20230204 15:39:22 @agent_ppo2.py:130][0m #------------------------ Iteration 106 --------------------------#
[32m[20230204 15:39:23 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:23 @agent_ppo2.py:194][0m |          -0.0001 |          15.7676 |           3.0926 |
[32m[20230204 15:39:23 @agent_ppo2.py:194][0m |          -0.0042 |           9.9089 |           3.0904 |
[32m[20230204 15:39:23 @agent_ppo2.py:194][0m |          -0.0022 |           8.6980 |           3.0894 |
[32m[20230204 15:39:23 @agent_ppo2.py:194][0m |          -0.0090 |           8.0941 |           3.0886 |
[32m[20230204 15:39:23 @agent_ppo2.py:194][0m |          -0.0092 |           7.5093 |           3.0877 |
[32m[20230204 15:39:23 @agent_ppo2.py:194][0m |           0.0005 |           7.7944 |           3.0903 |
[32m[20230204 15:39:23 @agent_ppo2.py:194][0m |          -0.0092 |           6.8852 |           3.0869 |
[32m[20230204 15:39:24 @agent_ppo2.py:194][0m |          -0.0048 |           6.5042 |           3.0875 |
[32m[20230204 15:39:24 @agent_ppo2.py:194][0m |          -0.0074 |           6.4358 |           3.0923 |
[32m[20230204 15:39:24 @agent_ppo2.py:194][0m |          -0.0115 |           6.1920 |           3.0912 |
[32m[20230204 15:39:24 @agent_ppo2.py:139][0m Policy update time: 0.97 s
[32m[20230204 15:39:24 @agent_ppo2.py:147][0m Average TRAINING episode reward: 106.28
[32m[20230204 15:39:24 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 215.96
[32m[20230204 15:39:24 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 67.10
[32m[20230204 15:39:24 @agent_ppo2.py:152][0m Total time:       3.22 min
[32m[20230204 15:39:24 @agent_ppo2.py:154][0m 219136 total steps have happened
[32m[20230204 15:39:24 @agent_ppo2.py:130][0m #------------------------ Iteration 107 --------------------------#
[32m[20230204 15:39:24 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:24 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:24 @agent_ppo2.py:194][0m |           0.0008 |          20.3761 |           3.0681 |
[32m[20230204 15:39:25 @agent_ppo2.py:194][0m |          -0.0053 |          13.7970 |           3.0629 |
[32m[20230204 15:39:25 @agent_ppo2.py:194][0m |          -0.0071 |          12.1116 |           3.0616 |
[32m[20230204 15:39:25 @agent_ppo2.py:194][0m |          -0.0081 |          11.3961 |           3.0587 |
[32m[20230204 15:39:25 @agent_ppo2.py:194][0m |          -0.0095 |          10.8308 |           3.0570 |
[32m[20230204 15:39:25 @agent_ppo2.py:194][0m |          -0.0099 |          10.4543 |           3.0563 |
[32m[20230204 15:39:25 @agent_ppo2.py:194][0m |          -0.0107 |          10.1875 |           3.0564 |
[32m[20230204 15:39:25 @agent_ppo2.py:194][0m |          -0.0111 |           9.7935 |           3.0564 |
[32m[20230204 15:39:25 @agent_ppo2.py:194][0m |          -0.0116 |           9.5732 |           3.0579 |
[32m[20230204 15:39:25 @agent_ppo2.py:194][0m |          -0.0120 |           9.4350 |           3.0576 |
[32m[20230204 15:39:25 @agent_ppo2.py:139][0m Policy update time: 1.11 s
[32m[20230204 15:39:26 @agent_ppo2.py:147][0m Average TRAINING episode reward: 142.13
[32m[20230204 15:39:26 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 219.00
[32m[20230204 15:39:26 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 14.51
[32m[20230204 15:39:26 @agent_ppo2.py:152][0m Total time:       3.25 min
[32m[20230204 15:39:26 @agent_ppo2.py:154][0m 221184 total steps have happened
[32m[20230204 15:39:26 @agent_ppo2.py:130][0m #------------------------ Iteration 108 --------------------------#
[32m[20230204 15:39:26 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:39:26 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:26 @agent_ppo2.py:194][0m |          -0.0017 |          21.7481 |           3.0561 |
[32m[20230204 15:39:26 @agent_ppo2.py:194][0m |          -0.0060 |          16.7889 |           3.0545 |
[32m[20230204 15:39:26 @agent_ppo2.py:194][0m |          -0.0072 |          15.0019 |           3.0514 |
[32m[20230204 15:39:26 @agent_ppo2.py:194][0m |          -0.0092 |          13.4775 |           3.0530 |
[32m[20230204 15:39:26 @agent_ppo2.py:194][0m |          -0.0104 |          12.1137 |           3.0513 |
[32m[20230204 15:39:27 @agent_ppo2.py:194][0m |          -0.0102 |          11.6718 |           3.0507 |
[32m[20230204 15:39:27 @agent_ppo2.py:194][0m |          -0.0103 |          10.9366 |           3.0512 |
[32m[20230204 15:39:27 @agent_ppo2.py:194][0m |          -0.0112 |          10.3002 |           3.0531 |
[32m[20230204 15:39:27 @agent_ppo2.py:194][0m |          -0.0115 |          10.1325 |           3.0502 |
[32m[20230204 15:39:27 @agent_ppo2.py:194][0m |          -0.0124 |           9.5793 |           3.0508 |
[32m[20230204 15:39:27 @agent_ppo2.py:139][0m Policy update time: 1.11 s
[32m[20230204 15:39:27 @agent_ppo2.py:147][0m Average TRAINING episode reward: 134.75
[32m[20230204 15:39:27 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 209.40
[32m[20230204 15:39:27 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 251.32
[32m[20230204 15:39:27 @agent_ppo2.py:152][0m Total time:       3.28 min
[32m[20230204 15:39:27 @agent_ppo2.py:154][0m 223232 total steps have happened
[32m[20230204 15:39:27 @agent_ppo2.py:130][0m #------------------------ Iteration 109 --------------------------#
[32m[20230204 15:39:28 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:39:28 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:28 @agent_ppo2.py:194][0m |          -0.0002 |           3.5129 |           3.1094 |
[32m[20230204 15:39:28 @agent_ppo2.py:194][0m |          -0.0047 |           2.9396 |           3.1063 |
[32m[20230204 15:39:28 @agent_ppo2.py:194][0m |          -0.0056 |           2.8547 |           3.1088 |
[32m[20230204 15:39:28 @agent_ppo2.py:194][0m |          -0.0067 |           2.8101 |           3.1100 |
[32m[20230204 15:39:28 @agent_ppo2.py:194][0m |          -0.0071 |           2.7791 |           3.1114 |
[32m[20230204 15:39:28 @agent_ppo2.py:194][0m |          -0.0082 |           2.7545 |           3.1154 |
[32m[20230204 15:39:28 @agent_ppo2.py:194][0m |          -0.0084 |           2.7341 |           3.1158 |
[32m[20230204 15:39:29 @agent_ppo2.py:194][0m |          -0.0086 |           2.7186 |           3.1203 |
[32m[20230204 15:39:29 @agent_ppo2.py:194][0m |          -0.0089 |           2.7096 |           3.1226 |
[32m[20230204 15:39:29 @agent_ppo2.py:194][0m |          -0.0095 |           2.6949 |           3.1248 |
[32m[20230204 15:39:29 @agent_ppo2.py:139][0m Policy update time: 1.23 s
[32m[20230204 15:39:29 @agent_ppo2.py:147][0m Average TRAINING episode reward: 214.89
[32m[20230204 15:39:29 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 225.00
[32m[20230204 15:39:29 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 255.52
[32m[20230204 15:39:29 @agent_ppo2.py:152][0m Total time:       3.31 min
[32m[20230204 15:39:29 @agent_ppo2.py:154][0m 225280 total steps have happened
[32m[20230204 15:39:29 @agent_ppo2.py:130][0m #------------------------ Iteration 110 --------------------------#
[32m[20230204 15:39:29 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:39:30 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:30 @agent_ppo2.py:194][0m |          -0.0027 |          11.5465 |           3.1068 |
[32m[20230204 15:39:30 @agent_ppo2.py:194][0m |          -0.0066 |           9.5838 |           3.0998 |
[32m[20230204 15:39:30 @agent_ppo2.py:194][0m |          -0.0075 |           8.0453 |           3.0993 |
[32m[20230204 15:39:30 @agent_ppo2.py:194][0m |          -0.0077 |           7.1620 |           3.1006 |
[32m[20230204 15:39:30 @agent_ppo2.py:194][0m |          -0.0089 |           6.7131 |           3.0994 |
[32m[20230204 15:39:30 @agent_ppo2.py:194][0m |          -0.0099 |           6.3526 |           3.0979 |
[32m[20230204 15:39:30 @agent_ppo2.py:194][0m |          -0.0097 |           6.0034 |           3.0992 |
[32m[20230204 15:39:30 @agent_ppo2.py:194][0m |          -0.0092 |           5.7075 |           3.0975 |
[32m[20230204 15:39:31 @agent_ppo2.py:194][0m |          -0.0106 |           5.5262 |           3.0981 |
[32m[20230204 15:39:31 @agent_ppo2.py:194][0m |          -0.0104 |           5.3871 |           3.0997 |
[32m[20230204 15:39:31 @agent_ppo2.py:139][0m Policy update time: 1.19 s
[32m[20230204 15:39:31 @agent_ppo2.py:147][0m Average TRAINING episode reward: 184.99
[32m[20230204 15:39:31 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 224.62
[32m[20230204 15:39:31 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 259.16
[32m[20230204 15:39:31 @agent_ppo2.py:152][0m Total time:       3.34 min
[32m[20230204 15:39:31 @agent_ppo2.py:154][0m 227328 total steps have happened
[32m[20230204 15:39:31 @agent_ppo2.py:130][0m #------------------------ Iteration 111 --------------------------#
[32m[20230204 15:39:31 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:31 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:31 @agent_ppo2.py:194][0m |           0.0011 |           2.2895 |           3.1566 |
[32m[20230204 15:39:32 @agent_ppo2.py:194][0m |          -0.0022 |           2.0782 |           3.1516 |
[32m[20230204 15:39:32 @agent_ppo2.py:194][0m |          -0.0036 |           2.0085 |           3.1483 |
[32m[20230204 15:39:32 @agent_ppo2.py:194][0m |          -0.0043 |           1.9702 |           3.1498 |
[32m[20230204 15:39:32 @agent_ppo2.py:194][0m |          -0.0047 |           1.9384 |           3.1507 |
[32m[20230204 15:39:32 @agent_ppo2.py:194][0m |          -0.0052 |           1.9141 |           3.1501 |
[32m[20230204 15:39:32 @agent_ppo2.py:194][0m |          -0.0059 |           1.8961 |           3.1518 |
[32m[20230204 15:39:32 @agent_ppo2.py:194][0m |          -0.0064 |           1.8805 |           3.1524 |
[32m[20230204 15:39:32 @agent_ppo2.py:194][0m |          -0.0068 |           1.8673 |           3.1543 |
[32m[20230204 15:39:32 @agent_ppo2.py:194][0m |          -0.0070 |           1.8631 |           3.1540 |
[32m[20230204 15:39:32 @agent_ppo2.py:139][0m Policy update time: 1.24 s
[32m[20230204 15:39:33 @agent_ppo2.py:147][0m Average TRAINING episode reward: 211.50
[32m[20230204 15:39:33 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 215.86
[32m[20230204 15:39:33 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 252.30
[32m[20230204 15:39:33 @agent_ppo2.py:152][0m Total time:       3.37 min
[32m[20230204 15:39:33 @agent_ppo2.py:154][0m 229376 total steps have happened
[32m[20230204 15:39:33 @agent_ppo2.py:130][0m #------------------------ Iteration 112 --------------------------#
[32m[20230204 15:39:33 @agent_ppo2.py:136][0m Sampling time: 0.38 s by 4 slaves
[32m[20230204 15:39:33 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:33 @agent_ppo2.py:194][0m |           0.0021 |           4.3084 |           3.1739 |
[32m[20230204 15:39:33 @agent_ppo2.py:194][0m |           0.0006 |           2.1198 |           3.1718 |
[32m[20230204 15:39:34 @agent_ppo2.py:194][0m |          -0.0057 |           1.9253 |           3.1710 |
[32m[20230204 15:39:34 @agent_ppo2.py:194][0m |          -0.0069 |           1.8732 |           3.1706 |
[32m[20230204 15:39:34 @agent_ppo2.py:194][0m |          -0.0024 |           1.8245 |           3.1710 |
[32m[20230204 15:39:34 @agent_ppo2.py:194][0m |          -0.0084 |           1.7494 |           3.1703 |
[32m[20230204 15:39:34 @agent_ppo2.py:194][0m |          -0.0084 |           1.7223 |           3.1685 |
[32m[20230204 15:39:34 @agent_ppo2.py:194][0m |          -0.0091 |           1.6848 |           3.1686 |
[32m[20230204 15:39:34 @agent_ppo2.py:194][0m |          -0.0089 |           1.6693 |           3.1680 |
[32m[20230204 15:39:34 @agent_ppo2.py:194][0m |          -0.0100 |           1.6377 |           3.1679 |
[32m[20230204 15:39:34 @agent_ppo2.py:139][0m Policy update time: 1.14 s
[32m[20230204 15:39:35 @agent_ppo2.py:147][0m Average TRAINING episode reward: 149.56
[32m[20230204 15:39:35 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 215.84
[32m[20230204 15:39:35 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 242.67
[32m[20230204 15:39:35 @agent_ppo2.py:152][0m Total time:       3.40 min
[32m[20230204 15:39:35 @agent_ppo2.py:154][0m 231424 total steps have happened
[32m[20230204 15:39:35 @agent_ppo2.py:130][0m #------------------------ Iteration 113 --------------------------#
[32m[20230204 15:39:35 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:35 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:35 @agent_ppo2.py:194][0m |           0.0003 |           2.5592 |           3.1703 |
[32m[20230204 15:39:35 @agent_ppo2.py:194][0m |          -0.0033 |           2.4818 |           3.1703 |
[32m[20230204 15:39:35 @agent_ppo2.py:194][0m |          -0.0049 |           2.4452 |           3.1683 |
[32m[20230204 15:39:35 @agent_ppo2.py:194][0m |          -0.0059 |           2.4166 |           3.1710 |
[32m[20230204 15:39:36 @agent_ppo2.py:194][0m |          -0.0068 |           2.3972 |           3.1725 |
[32m[20230204 15:39:36 @agent_ppo2.py:194][0m |          -0.0074 |           2.3730 |           3.1710 |
[32m[20230204 15:39:36 @agent_ppo2.py:194][0m |          -0.0078 |           2.3547 |           3.1720 |
[32m[20230204 15:39:36 @agent_ppo2.py:194][0m |          -0.0076 |           2.3344 |           3.1736 |
[32m[20230204 15:39:36 @agent_ppo2.py:194][0m |          -0.0085 |           2.3208 |           3.1748 |
[32m[20230204 15:39:36 @agent_ppo2.py:194][0m |          -0.0084 |           2.3061 |           3.1752 |
[32m[20230204 15:39:36 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:39:36 @agent_ppo2.py:147][0m Average TRAINING episode reward: 203.54
[32m[20230204 15:39:36 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 214.90
[32m[20230204 15:39:36 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 1.66
[32m[20230204 15:39:36 @agent_ppo2.py:152][0m Total time:       3.43 min
[32m[20230204 15:39:36 @agent_ppo2.py:154][0m 233472 total steps have happened
[32m[20230204 15:39:36 @agent_ppo2.py:130][0m #------------------------ Iteration 114 --------------------------#
[32m[20230204 15:39:37 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:37 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:37 @agent_ppo2.py:194][0m |          -0.0001 |           2.9284 |           3.2452 |
[32m[20230204 15:39:37 @agent_ppo2.py:194][0m |          -0.0036 |           2.7937 |           3.2412 |
[32m[20230204 15:39:37 @agent_ppo2.py:194][0m |          -0.0055 |           2.7378 |           3.2382 |
[32m[20230204 15:39:37 @agent_ppo2.py:194][0m |          -0.0061 |           2.6992 |           3.2352 |
[32m[20230204 15:39:37 @agent_ppo2.py:194][0m |          -0.0069 |           2.6682 |           3.2347 |
[32m[20230204 15:39:37 @agent_ppo2.py:194][0m |          -0.0071 |           2.6381 |           3.2343 |
[32m[20230204 15:39:37 @agent_ppo2.py:194][0m |          -0.0078 |           2.6203 |           3.2339 |
[32m[20230204 15:39:38 @agent_ppo2.py:194][0m |          -0.0083 |           2.6057 |           3.2333 |
[32m[20230204 15:39:38 @agent_ppo2.py:194][0m |          -0.0087 |           2.5840 |           3.2331 |
[32m[20230204 15:39:38 @agent_ppo2.py:194][0m |          -0.0090 |           2.5765 |           3.2306 |
[32m[20230204 15:39:38 @agent_ppo2.py:139][0m Policy update time: 1.21 s
[32m[20230204 15:39:38 @agent_ppo2.py:147][0m Average TRAINING episode reward: 199.16
[32m[20230204 15:39:38 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 207.27
[32m[20230204 15:39:38 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 256.99
[32m[20230204 15:39:38 @agent_ppo2.py:152][0m Total time:       3.46 min
[32m[20230204 15:39:38 @agent_ppo2.py:154][0m 235520 total steps have happened
[32m[20230204 15:39:38 @agent_ppo2.py:130][0m #------------------------ Iteration 115 --------------------------#
[32m[20230204 15:39:38 @agent_ppo2.py:136][0m Sampling time: 0.36 s by 4 slaves
[32m[20230204 15:39:39 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:39 @agent_ppo2.py:194][0m |          -0.0010 |           9.8629 |           3.1578 |
[32m[20230204 15:39:39 @agent_ppo2.py:194][0m |          -0.0047 |           5.2969 |           3.1499 |
[32m[20230204 15:39:39 @agent_ppo2.py:194][0m |          -0.0065 |           4.7100 |           3.1515 |
[32m[20230204 15:39:39 @agent_ppo2.py:194][0m |          -0.0062 |           4.5260 |           3.1482 |
[32m[20230204 15:39:39 @agent_ppo2.py:194][0m |          -0.0077 |           4.3654 |           3.1494 |
[32m[20230204 15:39:39 @agent_ppo2.py:194][0m |          -0.0089 |           4.2543 |           3.1490 |
[32m[20230204 15:39:39 @agent_ppo2.py:194][0m |          -0.0064 |           4.2092 |           3.1484 |
[32m[20230204 15:39:40 @agent_ppo2.py:194][0m |          -0.0094 |           4.1227 |           3.1498 |
[32m[20230204 15:39:40 @agent_ppo2.py:194][0m |          -0.0070 |           4.0980 |           3.1502 |
[32m[20230204 15:39:40 @agent_ppo2.py:194][0m |          -0.0098 |           4.0259 |           3.1496 |
[32m[20230204 15:39:40 @agent_ppo2.py:139][0m Policy update time: 1.37 s
[32m[20230204 15:39:40 @agent_ppo2.py:147][0m Average TRAINING episode reward: 115.94
[32m[20230204 15:39:40 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 235.08
[32m[20230204 15:39:40 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -11.54
[32m[20230204 15:39:40 @agent_ppo2.py:152][0m Total time:       3.49 min
[32m[20230204 15:39:40 @agent_ppo2.py:154][0m 237568 total steps have happened
[32m[20230204 15:39:40 @agent_ppo2.py:130][0m #------------------------ Iteration 116 --------------------------#
[32m[20230204 15:39:40 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:40 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:40 @agent_ppo2.py:194][0m |          -0.0013 |          10.3788 |           3.2397 |
[32m[20230204 15:39:41 @agent_ppo2.py:194][0m |          -0.0055 |           7.7497 |           3.2381 |
[32m[20230204 15:39:41 @agent_ppo2.py:194][0m |          -0.0089 |           6.4945 |           3.2341 |
[32m[20230204 15:39:41 @agent_ppo2.py:194][0m |          -0.0095 |           6.0509 |           3.2327 |
[32m[20230204 15:39:41 @agent_ppo2.py:194][0m |          -0.0107 |           5.5200 |           3.2304 |
[32m[20230204 15:39:41 @agent_ppo2.py:194][0m |          -0.0115 |           5.2894 |           3.2298 |
[32m[20230204 15:39:41 @agent_ppo2.py:194][0m |          -0.0119 |           5.0024 |           3.2283 |
[32m[20230204 15:39:41 @agent_ppo2.py:194][0m |          -0.0114 |           4.8711 |           3.2275 |
[32m[20230204 15:39:41 @agent_ppo2.py:194][0m |          -0.0112 |           4.7606 |           3.2278 |
[32m[20230204 15:39:41 @agent_ppo2.py:194][0m |          -0.0127 |           4.6023 |           3.2265 |
[32m[20230204 15:39:41 @agent_ppo2.py:139][0m Policy update time: 1.22 s
[32m[20230204 15:39:42 @agent_ppo2.py:147][0m Average TRAINING episode reward: 180.69
[32m[20230204 15:39:42 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 231.87
[32m[20230204 15:39:42 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.59
[32m[20230204 15:39:42 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 264.59
[32m[20230204 15:39:42 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 264.59
[32m[20230204 15:39:42 @agent_ppo2.py:152][0m Total time:       3.52 min
[32m[20230204 15:39:42 @agent_ppo2.py:154][0m 239616 total steps have happened
[32m[20230204 15:39:42 @agent_ppo2.py:130][0m #------------------------ Iteration 117 --------------------------#
[32m[20230204 15:39:42 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:39:42 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:42 @agent_ppo2.py:194][0m |           0.0007 |           9.7728 |           3.2518 |
[32m[20230204 15:39:42 @agent_ppo2.py:194][0m |          -0.0037 |           6.2914 |           3.2488 |
[32m[20230204 15:39:43 @agent_ppo2.py:194][0m |          -0.0055 |           5.7708 |           3.2478 |
[32m[20230204 15:39:43 @agent_ppo2.py:194][0m |          -0.0069 |           5.3935 |           3.2484 |
[32m[20230204 15:39:43 @agent_ppo2.py:194][0m |          -0.0072 |           5.1062 |           3.2482 |
[32m[20230204 15:39:43 @agent_ppo2.py:194][0m |          -0.0081 |           4.9290 |           3.2479 |
[32m[20230204 15:39:43 @agent_ppo2.py:194][0m |          -0.0085 |           4.7401 |           3.2494 |
[32m[20230204 15:39:43 @agent_ppo2.py:194][0m |          -0.0087 |           4.6097 |           3.2506 |
[32m[20230204 15:39:43 @agent_ppo2.py:194][0m |          -0.0095 |           4.4918 |           3.2520 |
[32m[20230204 15:39:43 @agent_ppo2.py:194][0m |          -0.0100 |           4.3591 |           3.2524 |
[32m[20230204 15:39:43 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:39:44 @agent_ppo2.py:147][0m Average TRAINING episode reward: 215.28
[32m[20230204 15:39:44 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 220.02
[32m[20230204 15:39:44 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 261.22
[32m[20230204 15:39:44 @agent_ppo2.py:152][0m Total time:       3.55 min
[32m[20230204 15:39:44 @agent_ppo2.py:154][0m 241664 total steps have happened
[32m[20230204 15:39:44 @agent_ppo2.py:130][0m #------------------------ Iteration 118 --------------------------#
[32m[20230204 15:39:44 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:39:44 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:44 @agent_ppo2.py:194][0m |          -0.0021 |           5.4980 |           3.2202 |
[32m[20230204 15:39:44 @agent_ppo2.py:194][0m |          -0.0068 |           4.4346 |           3.2168 |
[32m[20230204 15:39:44 @agent_ppo2.py:194][0m |           0.0024 |           4.3514 |           3.2179 |
[32m[20230204 15:39:44 @agent_ppo2.py:194][0m |          -0.0073 |           4.1867 |           3.2171 |
[32m[20230204 15:39:45 @agent_ppo2.py:194][0m |          -0.0059 |           4.0920 |           3.2142 |
[32m[20230204 15:39:45 @agent_ppo2.py:194][0m |          -0.0146 |           3.9962 |           3.2151 |
[32m[20230204 15:39:45 @agent_ppo2.py:194][0m |          -0.0084 |           3.9504 |           3.2149 |
[32m[20230204 15:39:45 @agent_ppo2.py:194][0m |          -0.0103 |           3.9394 |           3.2133 |
[32m[20230204 15:39:45 @agent_ppo2.py:194][0m |          -0.0097 |           3.8604 |           3.2152 |
[32m[20230204 15:39:45 @agent_ppo2.py:194][0m |          -0.0114 |           3.8488 |           3.2135 |
[32m[20230204 15:39:45 @agent_ppo2.py:139][0m Policy update time: 1.23 s
[32m[20230204 15:39:45 @agent_ppo2.py:147][0m Average TRAINING episode reward: 150.59
[32m[20230204 15:39:45 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 231.37
[32m[20230204 15:39:45 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 12.52
[32m[20230204 15:39:45 @agent_ppo2.py:152][0m Total time:       3.58 min
[32m[20230204 15:39:45 @agent_ppo2.py:154][0m 243712 total steps have happened
[32m[20230204 15:39:45 @agent_ppo2.py:130][0m #------------------------ Iteration 119 --------------------------#
[32m[20230204 15:39:46 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:39:46 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:46 @agent_ppo2.py:194][0m |          -0.0034 |           9.9596 |           3.2643 |
[32m[20230204 15:39:46 @agent_ppo2.py:194][0m |          -0.0082 |           8.1690 |           3.2603 |
[32m[20230204 15:39:46 @agent_ppo2.py:194][0m |          -0.0088 |           7.8684 |           3.2610 |
[32m[20230204 15:39:46 @agent_ppo2.py:194][0m |          -0.0100 |           7.3536 |           3.2558 |
[32m[20230204 15:39:46 @agent_ppo2.py:194][0m |          -0.0111 |           6.9809 |           3.2578 |
[32m[20230204 15:39:46 @agent_ppo2.py:194][0m |          -0.0113 |           6.9998 |           3.2581 |
[32m[20230204 15:39:47 @agent_ppo2.py:194][0m |          -0.0102 |           6.9679 |           3.2591 |
[32m[20230204 15:39:47 @agent_ppo2.py:194][0m |          -0.0120 |           6.8031 |           3.2557 |
[32m[20230204 15:39:47 @agent_ppo2.py:194][0m |          -0.0124 |           6.6570 |           3.2598 |
[32m[20230204 15:39:47 @agent_ppo2.py:194][0m |          -0.0125 |           6.6482 |           3.2566 |
[32m[20230204 15:39:47 @agent_ppo2.py:139][0m Policy update time: 1.21 s
[32m[20230204 15:39:47 @agent_ppo2.py:147][0m Average TRAINING episode reward: 150.35
[32m[20230204 15:39:47 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 227.38
[32m[20230204 15:39:47 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 110.02
[32m[20230204 15:39:47 @agent_ppo2.py:152][0m Total time:       3.61 min
[32m[20230204 15:39:47 @agent_ppo2.py:154][0m 245760 total steps have happened
[32m[20230204 15:39:47 @agent_ppo2.py:130][0m #------------------------ Iteration 120 --------------------------#
[32m[20230204 15:39:47 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:39:48 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:48 @agent_ppo2.py:194][0m |          -0.0005 |           3.7215 |           3.2805 |
[32m[20230204 15:39:48 @agent_ppo2.py:194][0m |          -0.0038 |           3.6286 |           3.2777 |
[32m[20230204 15:39:48 @agent_ppo2.py:194][0m |          -0.0059 |           3.5836 |           3.2792 |
[32m[20230204 15:39:48 @agent_ppo2.py:194][0m |          -0.0041 |           3.5803 |           3.2800 |
[32m[20230204 15:39:48 @agent_ppo2.py:194][0m |          -0.0069 |           3.5357 |           3.2805 |
[32m[20230204 15:39:48 @agent_ppo2.py:194][0m |          -0.0051 |           3.5417 |           3.2809 |
[32m[20230204 15:39:48 @agent_ppo2.py:194][0m |          -0.0071 |           3.5069 |           3.2823 |
[32m[20230204 15:39:48 @agent_ppo2.py:194][0m |          -0.0092 |           3.4755 |           3.2857 |
[32m[20230204 15:39:49 @agent_ppo2.py:194][0m |          -0.0081 |           3.4677 |           3.2841 |
[32m[20230204 15:39:49 @agent_ppo2.py:194][0m |          -0.0105 |           3.4536 |           3.2866 |
[32m[20230204 15:39:49 @agent_ppo2.py:139][0m Policy update time: 1.18 s
[32m[20230204 15:39:49 @agent_ppo2.py:147][0m Average TRAINING episode reward: 231.46
[32m[20230204 15:39:49 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 240.83
[32m[20230204 15:39:49 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 4.32
[32m[20230204 15:39:49 @agent_ppo2.py:152][0m Total time:       3.64 min
[32m[20230204 15:39:49 @agent_ppo2.py:154][0m 247808 total steps have happened
[32m[20230204 15:39:49 @agent_ppo2.py:130][0m #------------------------ Iteration 121 --------------------------#
[32m[20230204 15:39:49 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:49 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:49 @agent_ppo2.py:194][0m |           0.0000 |           7.4770 |           3.2804 |
[32m[20230204 15:39:49 @agent_ppo2.py:194][0m |          -0.0055 |           5.3265 |           3.2770 |
[32m[20230204 15:39:50 @agent_ppo2.py:194][0m |          -0.0052 |           4.7705 |           3.2720 |
[32m[20230204 15:39:50 @agent_ppo2.py:194][0m |          -0.0071 |           4.5376 |           3.2721 |
[32m[20230204 15:39:50 @agent_ppo2.py:194][0m |          -0.0087 |           4.4062 |           3.2699 |
[32m[20230204 15:39:50 @agent_ppo2.py:194][0m |          -0.0081 |           4.3722 |           3.2679 |
[32m[20230204 15:39:50 @agent_ppo2.py:194][0m |          -0.0074 |           4.2071 |           3.2673 |
[32m[20230204 15:39:50 @agent_ppo2.py:194][0m |          -0.0105 |           4.1436 |           3.2678 |
[32m[20230204 15:39:50 @agent_ppo2.py:194][0m |          -0.0101 |           4.1244 |           3.2685 |
[32m[20230204 15:39:50 @agent_ppo2.py:194][0m |          -0.0096 |           4.0673 |           3.2684 |
[32m[20230204 15:39:50 @agent_ppo2.py:139][0m Policy update time: 1.18 s
[32m[20230204 15:39:50 @agent_ppo2.py:147][0m Average TRAINING episode reward: 198.08
[32m[20230204 15:39:50 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 236.79
[32m[20230204 15:39:50 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 44.18
[32m[20230204 15:39:50 @agent_ppo2.py:152][0m Total time:       3.67 min
[32m[20230204 15:39:50 @agent_ppo2.py:154][0m 249856 total steps have happened
[32m[20230204 15:39:50 @agent_ppo2.py:130][0m #------------------------ Iteration 122 --------------------------#
[32m[20230204 15:39:51 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:51 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:51 @agent_ppo2.py:194][0m |           0.0001 |           2.7651 |           3.2974 |
[32m[20230204 15:39:51 @agent_ppo2.py:194][0m |          -0.0042 |           2.4968 |           3.2918 |
[32m[20230204 15:39:51 @agent_ppo2.py:194][0m |          -0.0062 |           2.4022 |           3.2924 |
[32m[20230204 15:39:51 @agent_ppo2.py:194][0m |          -0.0073 |           2.3527 |           3.2932 |
[32m[20230204 15:39:51 @agent_ppo2.py:194][0m |          -0.0084 |           2.3206 |           3.2920 |
[32m[20230204 15:39:52 @agent_ppo2.py:194][0m |          -0.0095 |           2.2955 |           3.2953 |
[32m[20230204 15:39:52 @agent_ppo2.py:194][0m |          -0.0094 |           2.2734 |           3.2969 |
[32m[20230204 15:39:52 @agent_ppo2.py:194][0m |          -0.0098 |           2.2508 |           3.2969 |
[32m[20230204 15:39:52 @agent_ppo2.py:194][0m |          -0.0105 |           2.2372 |           3.2975 |
[32m[20230204 15:39:52 @agent_ppo2.py:194][0m |          -0.0111 |           2.2110 |           3.2995 |
[32m[20230204 15:39:52 @agent_ppo2.py:139][0m Policy update time: 1.22 s
[32m[20230204 15:39:52 @agent_ppo2.py:147][0m Average TRAINING episode reward: 218.43
[32m[20230204 15:39:52 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 230.03
[32m[20230204 15:39:52 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.20
[32m[20230204 15:39:52 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 266.20
[32m[20230204 15:39:52 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 266.20
[32m[20230204 15:39:52 @agent_ppo2.py:152][0m Total time:       3.70 min
[32m[20230204 15:39:52 @agent_ppo2.py:154][0m 251904 total steps have happened
[32m[20230204 15:39:52 @agent_ppo2.py:130][0m #------------------------ Iteration 123 --------------------------#
[32m[20230204 15:39:53 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:53 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:53 @agent_ppo2.py:194][0m |           0.0008 |           3.5037 |           3.3534 |
[32m[20230204 15:39:53 @agent_ppo2.py:194][0m |          -0.0022 |           3.3706 |           3.3512 |
[32m[20230204 15:39:53 @agent_ppo2.py:194][0m |          -0.0038 |           3.3030 |           3.3524 |
[32m[20230204 15:39:53 @agent_ppo2.py:194][0m |          -0.0034 |           3.2959 |           3.3500 |
[32m[20230204 15:39:53 @agent_ppo2.py:194][0m |          -0.0056 |           3.2012 |           3.3500 |
[32m[20230204 15:39:53 @agent_ppo2.py:194][0m |          -0.0055 |           3.1758 |           3.3541 |
[32m[20230204 15:39:53 @agent_ppo2.py:194][0m |          -0.0061 |           3.1346 |           3.3537 |
[32m[20230204 15:39:54 @agent_ppo2.py:194][0m |          -0.0071 |           3.1064 |           3.3546 |
[32m[20230204 15:39:54 @agent_ppo2.py:194][0m |          -0.0076 |           3.0641 |           3.3519 |
[32m[20230204 15:39:54 @agent_ppo2.py:194][0m |          -0.0080 |           3.0427 |           3.3526 |
[32m[20230204 15:39:54 @agent_ppo2.py:139][0m Policy update time: 1.20 s
[32m[20230204 15:39:54 @agent_ppo2.py:147][0m Average TRAINING episode reward: 227.00
[32m[20230204 15:39:54 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 235.46
[32m[20230204 15:39:54 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.60
[32m[20230204 15:39:54 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 267.60
[32m[20230204 15:39:54 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 267.60
[32m[20230204 15:39:54 @agent_ppo2.py:152][0m Total time:       3.73 min
[32m[20230204 15:39:54 @agent_ppo2.py:154][0m 253952 total steps have happened
[32m[20230204 15:39:54 @agent_ppo2.py:130][0m #------------------------ Iteration 124 --------------------------#
[32m[20230204 15:39:54 @agent_ppo2.py:136][0m Sampling time: 0.37 s by 4 slaves
[32m[20230204 15:39:55 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:55 @agent_ppo2.py:194][0m |           0.0002 |           7.6610 |           3.3212 |
[32m[20230204 15:39:55 @agent_ppo2.py:194][0m |          -0.0048 |           5.3049 |           3.3189 |
[32m[20230204 15:39:55 @agent_ppo2.py:194][0m |          -0.0058 |           4.6435 |           3.3162 |
[32m[20230204 15:39:55 @agent_ppo2.py:194][0m |          -0.0088 |           4.2629 |           3.3173 |
[32m[20230204 15:39:55 @agent_ppo2.py:194][0m |          -0.0087 |           3.8251 |           3.3161 |
[32m[20230204 15:39:55 @agent_ppo2.py:194][0m |          -0.0098 |           3.6925 |           3.3177 |
[32m[20230204 15:39:55 @agent_ppo2.py:194][0m |          -0.0065 |           3.6949 |           3.3146 |
[32m[20230204 15:39:55 @agent_ppo2.py:194][0m |          -0.0090 |           3.5014 |           3.3141 |
[32m[20230204 15:39:55 @agent_ppo2.py:194][0m |          -0.0044 |           3.4098 |           3.3143 |
[32m[20230204 15:39:56 @agent_ppo2.py:194][0m |          -0.0091 |           3.3422 |           3.3116 |
[32m[20230204 15:39:56 @agent_ppo2.py:139][0m Policy update time: 1.11 s
[32m[20230204 15:39:56 @agent_ppo2.py:147][0m Average TRAINING episode reward: 167.38
[32m[20230204 15:39:56 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 215.72
[32m[20230204 15:39:56 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.07
[32m[20230204 15:39:56 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 270.07
[32m[20230204 15:39:56 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 270.07
[32m[20230204 15:39:56 @agent_ppo2.py:152][0m Total time:       3.76 min
[32m[20230204 15:39:56 @agent_ppo2.py:154][0m 256000 total steps have happened
[32m[20230204 15:39:56 @agent_ppo2.py:130][0m #------------------------ Iteration 125 --------------------------#
[32m[20230204 15:39:56 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:39:56 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:56 @agent_ppo2.py:194][0m |          -0.0011 |           8.5783 |           3.3495 |
[32m[20230204 15:39:56 @agent_ppo2.py:194][0m |          -0.0063 |           5.9693 |           3.3475 |
[32m[20230204 15:39:57 @agent_ppo2.py:194][0m |          -0.0085 |           5.2997 |           3.3449 |
[32m[20230204 15:39:57 @agent_ppo2.py:194][0m |          -0.0082 |           5.0594 |           3.3423 |
[32m[20230204 15:39:57 @agent_ppo2.py:194][0m |          -0.0098 |           4.8020 |           3.3408 |
[32m[20230204 15:39:57 @agent_ppo2.py:194][0m |          -0.0094 |           4.5446 |           3.3402 |
[32m[20230204 15:39:57 @agent_ppo2.py:194][0m |          -0.0111 |           4.5903 |           3.3403 |
[32m[20230204 15:39:57 @agent_ppo2.py:194][0m |          -0.0107 |           4.2953 |           3.3397 |
[32m[20230204 15:39:57 @agent_ppo2.py:194][0m |          -0.0113 |           4.1634 |           3.3369 |
[32m[20230204 15:39:57 @agent_ppo2.py:194][0m |          -0.0110 |           4.0715 |           3.3365 |
[32m[20230204 15:39:57 @agent_ppo2.py:139][0m Policy update time: 1.15 s
[32m[20230204 15:39:58 @agent_ppo2.py:147][0m Average TRAINING episode reward: 200.69
[32m[20230204 15:39:58 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 237.41
[32m[20230204 15:39:58 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.54
[32m[20230204 15:39:58 @agent_ppo2.py:152][0m Total time:       3.78 min
[32m[20230204 15:39:58 @agent_ppo2.py:154][0m 258048 total steps have happened
[32m[20230204 15:39:58 @agent_ppo2.py:130][0m #------------------------ Iteration 126 --------------------------#
[32m[20230204 15:39:58 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:39:58 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:39:58 @agent_ppo2.py:194][0m |           0.0007 |          26.5295 |           3.3629 |
[32m[20230204 15:39:58 @agent_ppo2.py:194][0m |          -0.0042 |          19.2603 |           3.3597 |
[32m[20230204 15:39:58 @agent_ppo2.py:194][0m |          -0.0068 |          17.2995 |           3.3585 |
[32m[20230204 15:39:58 @agent_ppo2.py:194][0m |          -0.0087 |          16.2953 |           3.3599 |
[32m[20230204 15:39:58 @agent_ppo2.py:194][0m |          -0.0090 |          15.1976 |           3.3599 |
[32m[20230204 15:39:59 @agent_ppo2.py:194][0m |          -0.0095 |          14.6721 |           3.3642 |
[32m[20230204 15:39:59 @agent_ppo2.py:194][0m |          -0.0098 |          14.1895 |           3.3657 |
[32m[20230204 15:39:59 @agent_ppo2.py:194][0m |          -0.0110 |          13.6176 |           3.3649 |
[32m[20230204 15:39:59 @agent_ppo2.py:194][0m |          -0.0105 |          13.4758 |           3.3640 |
[32m[20230204 15:39:59 @agent_ppo2.py:194][0m |          -0.0117 |          13.1392 |           3.3668 |
[32m[20230204 15:39:59 @agent_ppo2.py:139][0m Policy update time: 1.10 s
[32m[20230204 15:39:59 @agent_ppo2.py:147][0m Average TRAINING episode reward: 121.46
[32m[20230204 15:39:59 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 236.51
[32m[20230204 15:39:59 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 107.20
[32m[20230204 15:39:59 @agent_ppo2.py:152][0m Total time:       3.81 min
[32m[20230204 15:39:59 @agent_ppo2.py:154][0m 260096 total steps have happened
[32m[20230204 15:39:59 @agent_ppo2.py:130][0m #------------------------ Iteration 127 --------------------------#
[32m[20230204 15:40:00 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:40:00 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:00 @agent_ppo2.py:194][0m |          -0.0008 |          20.7471 |           3.3776 |
[32m[20230204 15:40:00 @agent_ppo2.py:194][0m |          -0.0043 |          18.0792 |           3.3753 |
[32m[20230204 15:40:00 @agent_ppo2.py:194][0m |          -0.0007 |          17.3903 |           3.3750 |
[32m[20230204 15:40:00 @agent_ppo2.py:194][0m |          -0.0071 |          16.7813 |           3.3749 |
[32m[20230204 15:40:00 @agent_ppo2.py:194][0m |          -0.0052 |          16.6383 |           3.3744 |
[32m[20230204 15:40:00 @agent_ppo2.py:194][0m |          -0.0081 |          15.6125 |           3.3765 |
[32m[20230204 15:40:00 @agent_ppo2.py:194][0m |          -0.0091 |          15.2576 |           3.3771 |
[32m[20230204 15:40:00 @agent_ppo2.py:194][0m |          -0.0089 |          14.8218 |           3.3774 |
[32m[20230204 15:40:01 @agent_ppo2.py:194][0m |          -0.0115 |          14.7721 |           3.3776 |
[32m[20230204 15:40:01 @agent_ppo2.py:194][0m |          -0.0107 |          14.4372 |           3.3795 |
[32m[20230204 15:40:01 @agent_ppo2.py:139][0m Policy update time: 1.08 s
[32m[20230204 15:40:01 @agent_ppo2.py:147][0m Average TRAINING episode reward: 125.62
[32m[20230204 15:40:01 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 242.49
[32m[20230204 15:40:01 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.93
[32m[20230204 15:40:01 @agent_ppo2.py:152][0m Total time:       3.84 min
[32m[20230204 15:40:01 @agent_ppo2.py:154][0m 262144 total steps have happened
[32m[20230204 15:40:01 @agent_ppo2.py:130][0m #------------------------ Iteration 128 --------------------------#
[32m[20230204 15:40:01 @agent_ppo2.py:136][0m Sampling time: 0.35 s by 4 slaves
[32m[20230204 15:40:01 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:01 @agent_ppo2.py:194][0m |           0.0008 |          11.6388 |           3.3833 |
[32m[20230204 15:40:02 @agent_ppo2.py:194][0m |          -0.0050 |           9.2727 |           3.3813 |
[32m[20230204 15:40:02 @agent_ppo2.py:194][0m |          -0.0068 |           8.7717 |           3.3820 |
[32m[20230204 15:40:02 @agent_ppo2.py:194][0m |          -0.0078 |           8.4090 |           3.3837 |
[32m[20230204 15:40:02 @agent_ppo2.py:194][0m |          -0.0093 |           8.0864 |           3.3853 |
[32m[20230204 15:40:02 @agent_ppo2.py:194][0m |          -0.0097 |           7.8428 |           3.3870 |
[32m[20230204 15:40:02 @agent_ppo2.py:194][0m |          -0.0104 |           7.6625 |           3.3875 |
[32m[20230204 15:40:02 @agent_ppo2.py:194][0m |          -0.0104 |           7.4489 |           3.3902 |
[32m[20230204 15:40:02 @agent_ppo2.py:194][0m |          -0.0107 |           7.3447 |           3.3918 |
[32m[20230204 15:40:02 @agent_ppo2.py:194][0m |          -0.0107 |           7.1867 |           3.3939 |
[32m[20230204 15:40:02 @agent_ppo2.py:139][0m Policy update time: 0.98 s
[32m[20230204 15:40:03 @agent_ppo2.py:147][0m Average TRAINING episode reward: 157.25
[32m[20230204 15:40:03 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 237.75
[32m[20230204 15:40:03 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 256.57
[32m[20230204 15:40:03 @agent_ppo2.py:152][0m Total time:       3.87 min
[32m[20230204 15:40:03 @agent_ppo2.py:154][0m 264192 total steps have happened
[32m[20230204 15:40:03 @agent_ppo2.py:130][0m #------------------------ Iteration 129 --------------------------#
[32m[20230204 15:40:03 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:40:03 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:03 @agent_ppo2.py:194][0m |          -0.0006 |           5.2564 |           3.4435 |
[32m[20230204 15:40:03 @agent_ppo2.py:194][0m |          -0.0037 |           4.8109 |           3.4390 |
[32m[20230204 15:40:03 @agent_ppo2.py:194][0m |          -0.0065 |           4.6579 |           3.4374 |
[32m[20230204 15:40:03 @agent_ppo2.py:194][0m |          -0.0074 |           4.5502 |           3.4388 |
[32m[20230204 15:40:03 @agent_ppo2.py:194][0m |          -0.0062 |           4.5437 |           3.4386 |
[32m[20230204 15:40:04 @agent_ppo2.py:194][0m |          -0.0082 |           4.4757 |           3.4396 |
[32m[20230204 15:40:04 @agent_ppo2.py:194][0m |          -0.0077 |           4.4153 |           3.4424 |
[32m[20230204 15:40:04 @agent_ppo2.py:194][0m |          -0.0077 |           4.4140 |           3.4431 |
[32m[20230204 15:40:04 @agent_ppo2.py:194][0m |          -0.0099 |           4.3519 |           3.4457 |
[32m[20230204 15:40:04 @agent_ppo2.py:194][0m |          -0.0105 |           4.3333 |           3.4465 |
[32m[20230204 15:40:04 @agent_ppo2.py:139][0m Policy update time: 1.19 s
[32m[20230204 15:40:04 @agent_ppo2.py:147][0m Average TRAINING episode reward: 232.79
[32m[20230204 15:40:04 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 239.94
[32m[20230204 15:40:04 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 274.20
[32m[20230204 15:40:04 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 274.20
[32m[20230204 15:40:04 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 274.20
[32m[20230204 15:40:04 @agent_ppo2.py:152][0m Total time:       3.90 min
[32m[20230204 15:40:04 @agent_ppo2.py:154][0m 266240 total steps have happened
[32m[20230204 15:40:04 @agent_ppo2.py:130][0m #------------------------ Iteration 130 --------------------------#
[32m[20230204 15:40:05 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:40:05 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:05 @agent_ppo2.py:194][0m |           0.0009 |           4.8720 |           3.5534 |
[32m[20230204 15:40:05 @agent_ppo2.py:194][0m |          -0.0019 |           4.5844 |           3.5438 |
[32m[20230204 15:40:05 @agent_ppo2.py:194][0m |          -0.0016 |           4.5948 |           3.5423 |
[32m[20230204 15:40:05 @agent_ppo2.py:194][0m |          -0.0049 |           4.3999 |           3.5383 |
[32m[20230204 15:40:05 @agent_ppo2.py:194][0m |          -0.0058 |           4.3581 |           3.5383 |
[32m[20230204 15:40:05 @agent_ppo2.py:194][0m |          -0.0058 |           4.3138 |           3.5369 |
[32m[20230204 15:40:05 @agent_ppo2.py:194][0m |          -0.0066 |           4.2786 |           3.5365 |
[32m[20230204 15:40:06 @agent_ppo2.py:194][0m |          -0.0072 |           4.2341 |           3.5357 |
[32m[20230204 15:40:06 @agent_ppo2.py:194][0m |          -0.0068 |           4.2308 |           3.5370 |
[32m[20230204 15:40:06 @agent_ppo2.py:194][0m |          -0.0089 |           4.1774 |           3.5349 |
[32m[20230204 15:40:06 @agent_ppo2.py:139][0m Policy update time: 1.17 s
[32m[20230204 15:40:06 @agent_ppo2.py:147][0m Average TRAINING episode reward: 233.24
[32m[20230204 15:40:06 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 239.74
[32m[20230204 15:40:06 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 274.59
[32m[20230204 15:40:06 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 274.59
[32m[20230204 15:40:06 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 274.59
[32m[20230204 15:40:06 @agent_ppo2.py:152][0m Total time:       3.93 min
[32m[20230204 15:40:06 @agent_ppo2.py:154][0m 268288 total steps have happened
[32m[20230204 15:40:06 @agent_ppo2.py:130][0m #------------------------ Iteration 131 --------------------------#
[32m[20230204 15:40:06 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:40:06 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:07 @agent_ppo2.py:194][0m |          -0.0009 |          16.0938 |           3.5322 |
[32m[20230204 15:40:07 @agent_ppo2.py:194][0m |          -0.0044 |          11.7694 |           3.5284 |
[32m[20230204 15:40:07 @agent_ppo2.py:194][0m |          -0.0058 |           9.9023 |           3.5258 |
[32m[20230204 15:40:07 @agent_ppo2.py:194][0m |          -0.0065 |           9.0152 |           3.5230 |
[32m[20230204 15:40:07 @agent_ppo2.py:194][0m |          -0.0043 |           9.3672 |           3.5219 |
[32m[20230204 15:40:07 @agent_ppo2.py:194][0m |          -0.0077 |           8.3162 |           3.5230 |
[32m[20230204 15:40:07 @agent_ppo2.py:194][0m |          -0.0083 |           8.0354 |           3.5258 |
[32m[20230204 15:40:07 @agent_ppo2.py:194][0m |          -0.0087 |           7.7062 |           3.5245 |
[32m[20230204 15:40:07 @agent_ppo2.py:194][0m |          -0.0093 |           7.5009 |           3.5238 |
[32m[20230204 15:40:07 @agent_ppo2.py:194][0m |          -0.0092 |           7.5618 |           3.5295 |
[32m[20230204 15:40:07 @agent_ppo2.py:139][0m Policy update time: 1.04 s
[32m[20230204 15:40:08 @agent_ppo2.py:147][0m Average TRAINING episode reward: 181.67
[32m[20230204 15:40:08 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 240.85
[32m[20230204 15:40:08 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.96
[32m[20230204 15:40:08 @agent_ppo2.py:152][0m Total time:       3.95 min
[32m[20230204 15:40:08 @agent_ppo2.py:154][0m 270336 total steps have happened
[32m[20230204 15:40:08 @agent_ppo2.py:130][0m #------------------------ Iteration 132 --------------------------#
[32m[20230204 15:40:08 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:40:08 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:08 @agent_ppo2.py:194][0m |          -0.0002 |           6.1705 |           3.5036 |
[32m[20230204 15:40:08 @agent_ppo2.py:194][0m |          -0.0056 |           5.7232 |           3.5027 |
[32m[20230204 15:40:08 @agent_ppo2.py:194][0m |          -0.0070 |           5.5849 |           3.5023 |
[32m[20230204 15:40:08 @agent_ppo2.py:194][0m |          -0.0089 |           5.4688 |           3.5017 |
[32m[20230204 15:40:09 @agent_ppo2.py:194][0m |          -0.0090 |           5.4051 |           3.5011 |
[32m[20230204 15:40:09 @agent_ppo2.py:194][0m |          -0.0061 |           5.3640 |           3.5005 |
[32m[20230204 15:40:09 @agent_ppo2.py:194][0m |          -0.0092 |           5.2925 |           3.4985 |
[32m[20230204 15:40:09 @agent_ppo2.py:194][0m |          -0.0061 |           5.3051 |           3.4992 |
[32m[20230204 15:40:09 @agent_ppo2.py:194][0m |          -0.0074 |           5.2619 |           3.4985 |
[32m[20230204 15:40:09 @agent_ppo2.py:194][0m |          -0.0110 |           5.1542 |           3.4978 |
[32m[20230204 15:40:09 @agent_ppo2.py:139][0m Policy update time: 1.15 s
[32m[20230204 15:40:09 @agent_ppo2.py:147][0m Average TRAINING episode reward: 240.04
[32m[20230204 15:40:09 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 243.30
[32m[20230204 15:40:09 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.81
[32m[20230204 15:40:09 @agent_ppo2.py:152][0m Total time:       3.98 min
[32m[20230204 15:40:09 @agent_ppo2.py:154][0m 272384 total steps have happened
[32m[20230204 15:40:09 @agent_ppo2.py:130][0m #------------------------ Iteration 133 --------------------------#
[32m[20230204 15:40:10 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:40:10 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:10 @agent_ppo2.py:194][0m |          -0.0012 |           5.4956 |           3.5026 |
[32m[20230204 15:40:10 @agent_ppo2.py:194][0m |          -0.0072 |           5.2405 |           3.4964 |
[32m[20230204 15:40:10 @agent_ppo2.py:194][0m |          -0.0057 |           5.2185 |           3.4913 |
[32m[20230204 15:40:10 @agent_ppo2.py:194][0m |          -0.0054 |           5.1418 |           3.4942 |
[32m[20230204 15:40:10 @agent_ppo2.py:194][0m |          -0.0081 |           5.1233 |           3.4933 |
[32m[20230204 15:40:10 @agent_ppo2.py:194][0m |          -0.0060 |           5.1433 |           3.4928 |
[32m[20230204 15:40:11 @agent_ppo2.py:194][0m |          -0.0072 |           5.0101 |           3.4928 |
[32m[20230204 15:40:11 @agent_ppo2.py:194][0m |          -0.0087 |           4.9735 |           3.4970 |
[32m[20230204 15:40:11 @agent_ppo2.py:194][0m |          -0.0096 |           4.9538 |           3.4964 |
[32m[20230204 15:40:11 @agent_ppo2.py:194][0m |          -0.0095 |           4.9344 |           3.5001 |
[32m[20230204 15:40:11 @agent_ppo2.py:139][0m Policy update time: 1.19 s
[32m[20230204 15:40:11 @agent_ppo2.py:147][0m Average TRAINING episode reward: 232.66
[32m[20230204 15:40:11 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 243.48
[32m[20230204 15:40:11 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.31
[32m[20230204 15:40:11 @agent_ppo2.py:152][0m Total time:       4.01 min
[32m[20230204 15:40:11 @agent_ppo2.py:154][0m 274432 total steps have happened
[32m[20230204 15:40:11 @agent_ppo2.py:130][0m #------------------------ Iteration 134 --------------------------#
[32m[20230204 15:40:11 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:40:12 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:12 @agent_ppo2.py:194][0m |          -0.0032 |          16.2872 |           3.5987 |
[32m[20230204 15:40:12 @agent_ppo2.py:194][0m |          -0.0092 |          11.5159 |           3.5863 |
[32m[20230204 15:40:12 @agent_ppo2.py:194][0m |          -0.0116 |          10.7218 |           3.5868 |
[32m[20230204 15:40:12 @agent_ppo2.py:194][0m |          -0.0075 |           9.7811 |           3.5867 |
[32m[20230204 15:40:12 @agent_ppo2.py:194][0m |          -0.0119 |           9.1231 |           3.5845 |
[32m[20230204 15:40:12 @agent_ppo2.py:194][0m |          -0.0136 |           8.5642 |           3.5850 |
[32m[20230204 15:40:12 @agent_ppo2.py:194][0m |          -0.0131 |           8.3157 |           3.5853 |
[32m[20230204 15:40:12 @agent_ppo2.py:194][0m |          -0.0123 |           8.2553 |           3.5815 |
[32m[20230204 15:40:12 @agent_ppo2.py:194][0m |          -0.0122 |           7.9546 |           3.5826 |
[32m[20230204 15:40:12 @agent_ppo2.py:194][0m |          -0.0156 |           7.6029 |           3.5825 |
[32m[20230204 15:40:12 @agent_ppo2.py:139][0m Policy update time: 1.02 s
[32m[20230204 15:40:13 @agent_ppo2.py:147][0m Average TRAINING episode reward: 137.47
[32m[20230204 15:40:13 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 216.91
[32m[20230204 15:40:13 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 274.89
[32m[20230204 15:40:13 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 274.89
[32m[20230204 15:40:13 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 274.89
[32m[20230204 15:40:13 @agent_ppo2.py:152][0m Total time:       4.04 min
[32m[20230204 15:40:13 @agent_ppo2.py:154][0m 276480 total steps have happened
[32m[20230204 15:40:13 @agent_ppo2.py:130][0m #------------------------ Iteration 135 --------------------------#
[32m[20230204 15:40:13 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:40:13 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:13 @agent_ppo2.py:194][0m |           0.0006 |           4.6779 |           3.5544 |
[32m[20230204 15:40:13 @agent_ppo2.py:194][0m |          -0.0054 |           4.3158 |           3.5482 |
[32m[20230204 15:40:13 @agent_ppo2.py:194][0m |          -0.0064 |           4.1608 |           3.5467 |
[32m[20230204 15:40:14 @agent_ppo2.py:194][0m |          -0.0047 |           4.0658 |           3.5491 |
[32m[20230204 15:40:14 @agent_ppo2.py:194][0m |          -0.0064 |           3.9927 |           3.5464 |
[32m[20230204 15:40:14 @agent_ppo2.py:194][0m |          -0.0068 |           3.9529 |           3.5473 |
[32m[20230204 15:40:14 @agent_ppo2.py:194][0m |          -0.0080 |           3.8911 |           3.5508 |
[32m[20230204 15:40:14 @agent_ppo2.py:194][0m |          -0.0096 |           3.8450 |           3.5511 |
[32m[20230204 15:40:14 @agent_ppo2.py:194][0m |          -0.0079 |           3.8026 |           3.5518 |
[32m[20230204 15:40:14 @agent_ppo2.py:194][0m |          -0.0093 |           3.7816 |           3.5487 |
[32m[20230204 15:40:14 @agent_ppo2.py:139][0m Policy update time: 1.19 s
[32m[20230204 15:40:14 @agent_ppo2.py:147][0m Average TRAINING episode reward: 238.32
[32m[20230204 15:40:14 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 241.11
[32m[20230204 15:40:14 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 16.99
[32m[20230204 15:40:14 @agent_ppo2.py:152][0m Total time:       4.06 min
[32m[20230204 15:40:14 @agent_ppo2.py:154][0m 278528 total steps have happened
[32m[20230204 15:40:14 @agent_ppo2.py:130][0m #------------------------ Iteration 136 --------------------------#
[32m[20230204 15:40:15 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:40:15 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:15 @agent_ppo2.py:194][0m |           0.0022 |          10.9388 |           3.5238 |
[32m[20230204 15:40:15 @agent_ppo2.py:194][0m |          -0.0052 |           8.2873 |           3.5148 |
[32m[20230204 15:40:15 @agent_ppo2.py:194][0m |          -0.0070 |           7.8321 |           3.5096 |
[32m[20230204 15:40:15 @agent_ppo2.py:194][0m |          -0.0076 |           7.1229 |           3.5051 |
[32m[20230204 15:40:15 @agent_ppo2.py:194][0m |          -0.0083 |           6.9844 |           3.5074 |
[32m[20230204 15:40:15 @agent_ppo2.py:194][0m |          -0.0087 |           6.8302 |           3.5020 |
[32m[20230204 15:40:15 @agent_ppo2.py:194][0m |          -0.0098 |           6.4739 |           3.5031 |
[32m[20230204 15:40:16 @agent_ppo2.py:194][0m |          -0.0103 |           6.2790 |           3.5006 |
[32m[20230204 15:40:16 @agent_ppo2.py:194][0m |          -0.0110 |           6.1298 |           3.4978 |
[32m[20230204 15:40:16 @agent_ppo2.py:194][0m |          -0.0111 |           6.0261 |           3.4919 |
[32m[20230204 15:40:16 @agent_ppo2.py:139][0m Policy update time: 1.12 s
[32m[20230204 15:40:16 @agent_ppo2.py:147][0m Average TRAINING episode reward: 196.98
[32m[20230204 15:40:16 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 243.27
[32m[20230204 15:40:16 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 274.97
[32m[20230204 15:40:16 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 274.97
[32m[20230204 15:40:16 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 274.97
[32m[20230204 15:40:16 @agent_ppo2.py:152][0m Total time:       4.09 min
[32m[20230204 15:40:16 @agent_ppo2.py:154][0m 280576 total steps have happened
[32m[20230204 15:40:16 @agent_ppo2.py:130][0m #------------------------ Iteration 137 --------------------------#
[32m[20230204 15:40:16 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:40:16 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:17 @agent_ppo2.py:194][0m |          -0.0009 |           5.2617 |           3.4993 |
[32m[20230204 15:40:17 @agent_ppo2.py:194][0m |          -0.0039 |           4.8089 |           3.4968 |
[32m[20230204 15:40:17 @agent_ppo2.py:194][0m |          -0.0073 |           4.6246 |           3.4974 |
[32m[20230204 15:40:17 @agent_ppo2.py:194][0m |          -0.0081 |           4.5262 |           3.4947 |
[32m[20230204 15:40:17 @agent_ppo2.py:194][0m |          -0.0093 |           4.4276 |           3.4967 |
[32m[20230204 15:40:17 @agent_ppo2.py:194][0m |          -0.0098 |           4.3560 |           3.4946 |
[32m[20230204 15:40:17 @agent_ppo2.py:194][0m |          -0.0096 |           4.2975 |           3.4943 |
[32m[20230204 15:40:17 @agent_ppo2.py:194][0m |          -0.0100 |           4.2277 |           3.4956 |
[32m[20230204 15:40:17 @agent_ppo2.py:194][0m |          -0.0110 |           4.1845 |           3.4934 |
[32m[20230204 15:40:18 @agent_ppo2.py:194][0m |          -0.0111 |           4.1333 |           3.4945 |
[32m[20230204 15:40:18 @agent_ppo2.py:139][0m Policy update time: 1.18 s
[32m[20230204 15:40:18 @agent_ppo2.py:147][0m Average TRAINING episode reward: 234.32
[32m[20230204 15:40:18 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 245.04
[32m[20230204 15:40:18 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.31
[32m[20230204 15:40:18 @agent_ppo2.py:152][0m Total time:       4.12 min
[32m[20230204 15:40:18 @agent_ppo2.py:154][0m 282624 total steps have happened
[32m[20230204 15:40:18 @agent_ppo2.py:130][0m #------------------------ Iteration 138 --------------------------#
[32m[20230204 15:40:18 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:40:18 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:18 @agent_ppo2.py:194][0m |           0.0003 |          10.7240 |           3.5202 |
[32m[20230204 15:40:18 @agent_ppo2.py:194][0m |          -0.0033 |           7.7397 |           3.5161 |
[32m[20230204 15:40:18 @agent_ppo2.py:194][0m |          -0.0044 |           7.1992 |           3.5147 |
[32m[20230204 15:40:19 @agent_ppo2.py:194][0m |          -0.0047 |           6.9192 |           3.5106 |
[32m[20230204 15:40:19 @agent_ppo2.py:194][0m |          -0.0056 |           6.6245 |           3.5092 |
[32m[20230204 15:40:19 @agent_ppo2.py:194][0m |          -0.0053 |           6.5641 |           3.5070 |
[32m[20230204 15:40:19 @agent_ppo2.py:194][0m |          -0.0074 |           6.3133 |           3.5054 |
[32m[20230204 15:40:19 @agent_ppo2.py:194][0m |          -0.0062 |           6.1600 |           3.5050 |
[32m[20230204 15:40:19 @agent_ppo2.py:194][0m |          -0.0064 |           6.3054 |           3.5028 |
[32m[20230204 15:40:19 @agent_ppo2.py:194][0m |          -0.0062 |           5.9919 |           3.5020 |
[32m[20230204 15:40:19 @agent_ppo2.py:139][0m Policy update time: 0.99 s
[32m[20230204 15:40:19 @agent_ppo2.py:147][0m Average TRAINING episode reward: 112.67
[32m[20230204 15:40:19 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 245.61
[32m[20230204 15:40:19 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.77
[32m[20230204 15:40:19 @agent_ppo2.py:152][0m Total time:       4.15 min
[32m[20230204 15:40:19 @agent_ppo2.py:154][0m 284672 total steps have happened
[32m[20230204 15:40:19 @agent_ppo2.py:130][0m #------------------------ Iteration 139 --------------------------#
[32m[20230204 15:40:20 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:40:20 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:20 @agent_ppo2.py:194][0m |          -0.0011 |           6.0347 |           3.5370 |
[32m[20230204 15:40:20 @agent_ppo2.py:194][0m |          -0.0038 |           5.7109 |           3.5336 |
[32m[20230204 15:40:20 @agent_ppo2.py:194][0m |          -0.0054 |           5.5925 |           3.5322 |
[32m[20230204 15:40:20 @agent_ppo2.py:194][0m |          -0.0053 |           5.5285 |           3.5309 |
[32m[20230204 15:40:20 @agent_ppo2.py:194][0m |          -0.0068 |           5.4548 |           3.5279 |
[32m[20230204 15:40:20 @agent_ppo2.py:194][0m |          -0.0071 |           5.4261 |           3.5274 |
[32m[20230204 15:40:20 @agent_ppo2.py:194][0m |          -0.0047 |           5.4336 |           3.5281 |
[32m[20230204 15:40:21 @agent_ppo2.py:194][0m |          -0.0080 |           5.3357 |           3.5266 |
[32m[20230204 15:40:21 @agent_ppo2.py:194][0m |          -0.0083 |           5.2799 |           3.5237 |
[32m[20230204 15:40:21 @agent_ppo2.py:194][0m |          -0.0079 |           5.2548 |           3.5277 |
[32m[20230204 15:40:21 @agent_ppo2.py:139][0m Policy update time: 1.15 s
[32m[20230204 15:40:21 @agent_ppo2.py:147][0m Average TRAINING episode reward: 238.90
[32m[20230204 15:40:21 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 242.70
[32m[20230204 15:40:21 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 277.16
[32m[20230204 15:40:21 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 277.16
[32m[20230204 15:40:21 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 277.16
[32m[20230204 15:40:21 @agent_ppo2.py:152][0m Total time:       4.18 min
[32m[20230204 15:40:21 @agent_ppo2.py:154][0m 286720 total steps have happened
[32m[20230204 15:40:21 @agent_ppo2.py:130][0m #------------------------ Iteration 140 --------------------------#
[32m[20230204 15:40:21 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:40:21 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:22 @agent_ppo2.py:194][0m |           0.0012 |           5.8633 |           3.5055 |
[32m[20230204 15:40:22 @agent_ppo2.py:194][0m |          -0.0028 |           5.5264 |           3.5025 |
[32m[20230204 15:40:22 @agent_ppo2.py:194][0m |          -0.0056 |           5.2919 |           3.5006 |
[32m[20230204 15:40:22 @agent_ppo2.py:194][0m |          -0.0058 |           5.1602 |           3.5021 |
[32m[20230204 15:40:22 @agent_ppo2.py:194][0m |          -0.0066 |           4.9921 |           3.4999 |
[32m[20230204 15:40:22 @agent_ppo2.py:194][0m |          -0.0087 |           4.8582 |           3.5002 |
[32m[20230204 15:40:22 @agent_ppo2.py:194][0m |          -0.0088 |           4.6985 |           3.5013 |
[32m[20230204 15:40:22 @agent_ppo2.py:194][0m |          -0.0088 |           4.5565 |           3.4986 |
[32m[20230204 15:40:22 @agent_ppo2.py:194][0m |          -0.0091 |           4.3871 |           3.4992 |
[32m[20230204 15:40:22 @agent_ppo2.py:194][0m |          -0.0100 |           4.2064 |           3.4999 |
[32m[20230204 15:40:22 @agent_ppo2.py:139][0m Policy update time: 1.10 s
[32m[20230204 15:40:23 @agent_ppo2.py:147][0m Average TRAINING episode reward: 244.83
[32m[20230204 15:40:23 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 252.24
[32m[20230204 15:40:23 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 279.26
[32m[20230204 15:40:23 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 279.26
[32m[20230204 15:40:23 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 279.26
[32m[20230204 15:40:23 @agent_ppo2.py:152][0m Total time:       4.20 min
[32m[20230204 15:40:23 @agent_ppo2.py:154][0m 288768 total steps have happened
[32m[20230204 15:40:23 @agent_ppo2.py:130][0m #------------------------ Iteration 141 --------------------------#
[32m[20230204 15:40:23 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:40:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:23 @agent_ppo2.py:194][0m |          -0.0003 |          19.3230 |           3.4924 |
[32m[20230204 15:40:23 @agent_ppo2.py:194][0m |          -0.0033 |          15.3415 |           3.4874 |
[32m[20230204 15:40:23 @agent_ppo2.py:194][0m |          -0.0055 |          13.4837 |           3.4829 |
[32m[20230204 15:40:23 @agent_ppo2.py:194][0m |          -0.0071 |          12.6072 |           3.4820 |
[32m[20230204 15:40:24 @agent_ppo2.py:194][0m |          -0.0073 |          12.0085 |           3.4796 |
[32m[20230204 15:40:24 @agent_ppo2.py:194][0m |          -0.0074 |          11.5264 |           3.4804 |
[32m[20230204 15:40:24 @agent_ppo2.py:194][0m |          -0.0091 |          10.9286 |           3.4769 |
[32m[20230204 15:40:24 @agent_ppo2.py:194][0m |          -0.0088 |          10.2925 |           3.4770 |
[32m[20230204 15:40:24 @agent_ppo2.py:194][0m |          -0.0082 |           9.9422 |           3.4762 |
[32m[20230204 15:40:24 @agent_ppo2.py:194][0m |          -0.0096 |           9.3805 |           3.4757 |
[32m[20230204 15:40:24 @agent_ppo2.py:139][0m Policy update time: 0.97 s
[32m[20230204 15:40:24 @agent_ppo2.py:147][0m Average TRAINING episode reward: 183.33
[32m[20230204 15:40:24 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 245.79
[32m[20230204 15:40:24 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 274.60
[32m[20230204 15:40:24 @agent_ppo2.py:152][0m Total time:       4.23 min
[32m[20230204 15:40:24 @agent_ppo2.py:154][0m 290816 total steps have happened
[32m[20230204 15:40:24 @agent_ppo2.py:130][0m #------------------------ Iteration 142 --------------------------#
[32m[20230204 15:40:24 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:40:25 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:25 @agent_ppo2.py:194][0m |          -0.0014 |           6.9233 |           3.4655 |
[32m[20230204 15:40:25 @agent_ppo2.py:194][0m |          -0.0049 |           6.5269 |           3.4622 |
[32m[20230204 15:40:25 @agent_ppo2.py:194][0m |          -0.0083 |           6.3546 |           3.4625 |
[32m[20230204 15:40:25 @agent_ppo2.py:194][0m |          -0.0087 |           6.2568 |           3.4636 |
[32m[20230204 15:40:25 @agent_ppo2.py:194][0m |          -0.0089 |           6.2235 |           3.4620 |
[32m[20230204 15:40:25 @agent_ppo2.py:194][0m |          -0.0109 |           6.1109 |           3.4642 |
[32m[20230204 15:40:25 @agent_ppo2.py:194][0m |          -0.0103 |           6.0667 |           3.4655 |
[32m[20230204 15:40:25 @agent_ppo2.py:194][0m |          -0.0117 |           6.0219 |           3.4676 |
[32m[20230204 15:40:25 @agent_ppo2.py:194][0m |          -0.0105 |           6.0305 |           3.4679 |
[32m[20230204 15:40:26 @agent_ppo2.py:194][0m |          -0.0121 |           5.9366 |           3.4687 |
[32m[20230204 15:40:26 @agent_ppo2.py:139][0m Policy update time: 1.11 s
[32m[20230204 15:40:26 @agent_ppo2.py:147][0m Average TRAINING episode reward: 245.23
[32m[20230204 15:40:26 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 249.62
[32m[20230204 15:40:26 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 276.78
[32m[20230204 15:40:26 @agent_ppo2.py:152][0m Total time:       4.26 min
[32m[20230204 15:40:26 @agent_ppo2.py:154][0m 292864 total steps have happened
[32m[20230204 15:40:26 @agent_ppo2.py:130][0m #------------------------ Iteration 143 --------------------------#
[32m[20230204 15:40:26 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:40:26 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:26 @agent_ppo2.py:194][0m |          -0.0002 |           6.9313 |           3.5478 |
[32m[20230204 15:40:26 @agent_ppo2.py:194][0m |          -0.0031 |           6.7012 |           3.5468 |
[32m[20230204 15:40:27 @agent_ppo2.py:194][0m |          -0.0031 |           6.6188 |           3.5459 |
[32m[20230204 15:40:27 @agent_ppo2.py:194][0m |          -0.0062 |           6.4848 |           3.5428 |
[32m[20230204 15:40:27 @agent_ppo2.py:194][0m |          -0.0061 |           6.4100 |           3.5442 |
[32m[20230204 15:40:27 @agent_ppo2.py:194][0m |          -0.0052 |           6.5111 |           3.5436 |
[32m[20230204 15:40:27 @agent_ppo2.py:194][0m |          -0.0075 |           6.3306 |           3.5443 |
[32m[20230204 15:40:27 @agent_ppo2.py:194][0m |          -0.0087 |           6.2764 |           3.5432 |
[32m[20230204 15:40:27 @agent_ppo2.py:194][0m |          -0.0091 |           6.2408 |           3.5418 |
[32m[20230204 15:40:27 @agent_ppo2.py:194][0m |          -0.0090 |           6.2419 |           3.5396 |
[32m[20230204 15:40:27 @agent_ppo2.py:139][0m Policy update time: 1.08 s
[32m[20230204 15:40:27 @agent_ppo2.py:147][0m Average TRAINING episode reward: 246.83
[32m[20230204 15:40:27 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 250.52
[32m[20230204 15:40:27 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 278.47
[32m[20230204 15:40:27 @agent_ppo2.py:152][0m Total time:       4.28 min
[32m[20230204 15:40:27 @agent_ppo2.py:154][0m 294912 total steps have happened
[32m[20230204 15:40:27 @agent_ppo2.py:130][0m #------------------------ Iteration 144 --------------------------#
[32m[20230204 15:40:28 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:40:28 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:28 @agent_ppo2.py:194][0m |          -0.0014 |          35.3670 |           3.5269 |
[32m[20230204 15:40:28 @agent_ppo2.py:194][0m |          -0.0044 |          28.4529 |           3.5219 |
[32m[20230204 15:40:28 @agent_ppo2.py:194][0m |          -0.0072 |          24.9628 |           3.5237 |
[32m[20230204 15:40:28 @agent_ppo2.py:194][0m |          -0.0084 |          23.9758 |           3.5213 |
[32m[20230204 15:40:28 @agent_ppo2.py:194][0m |          -0.0084 |          22.4781 |           3.5228 |
[32m[20230204 15:40:28 @agent_ppo2.py:194][0m |          -0.0096 |          22.0905 |           3.5217 |
[32m[20230204 15:40:28 @agent_ppo2.py:194][0m |          -0.0089 |          21.2362 |           3.5239 |
[32m[20230204 15:40:28 @agent_ppo2.py:194][0m |          -0.0117 |          20.6328 |           3.5240 |
[32m[20230204 15:40:29 @agent_ppo2.py:194][0m |          -0.0127 |          20.3996 |           3.5242 |
[32m[20230204 15:40:29 @agent_ppo2.py:194][0m |          -0.0120 |          20.1188 |           3.5245 |
[32m[20230204 15:40:29 @agent_ppo2.py:139][0m Policy update time: 0.92 s
[32m[20230204 15:40:29 @agent_ppo2.py:147][0m Average TRAINING episode reward: 108.91
[32m[20230204 15:40:29 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 243.82
[32m[20230204 15:40:29 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 37.05
[32m[20230204 15:40:29 @agent_ppo2.py:152][0m Total time:       4.31 min
[32m[20230204 15:40:29 @agent_ppo2.py:154][0m 296960 total steps have happened
[32m[20230204 15:40:29 @agent_ppo2.py:130][0m #------------------------ Iteration 145 --------------------------#
[32m[20230204 15:40:29 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:40:29 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:29 @agent_ppo2.py:194][0m |          -0.0044 |          44.9681 |           3.5135 |
[32m[20230204 15:40:29 @agent_ppo2.py:194][0m |          -0.0076 |          32.4742 |           3.5032 |
[32m[20230204 15:40:29 @agent_ppo2.py:194][0m |          -0.0083 |          28.5540 |           3.5044 |
[32m[20230204 15:40:30 @agent_ppo2.py:194][0m |          -0.0127 |          25.5564 |           3.5021 |
[32m[20230204 15:40:30 @agent_ppo2.py:194][0m |          -0.0088 |          23.6756 |           3.4988 |
[32m[20230204 15:40:30 @agent_ppo2.py:194][0m |          -0.0127 |          22.2300 |           3.4976 |
[32m[20230204 15:40:30 @agent_ppo2.py:194][0m |          -0.0131 |          20.9689 |           3.4945 |
[32m[20230204 15:40:30 @agent_ppo2.py:194][0m |          -0.0140 |          20.2394 |           3.4916 |
[32m[20230204 15:40:30 @agent_ppo2.py:194][0m |          -0.0149 |          19.6247 |           3.4907 |
[32m[20230204 15:40:30 @agent_ppo2.py:194][0m |          -0.0149 |          19.0804 |           3.4891 |
[32m[20230204 15:40:30 @agent_ppo2.py:139][0m Policy update time: 1.02 s
[32m[20230204 15:40:30 @agent_ppo2.py:147][0m Average TRAINING episode reward: 143.02
[32m[20230204 15:40:30 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 250.11
[32m[20230204 15:40:30 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 277.07
[32m[20230204 15:40:30 @agent_ppo2.py:152][0m Total time:       4.33 min
[32m[20230204 15:40:30 @agent_ppo2.py:154][0m 299008 total steps have happened
[32m[20230204 15:40:30 @agent_ppo2.py:130][0m #------------------------ Iteration 146 --------------------------#
[32m[20230204 15:40:31 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:40:31 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:31 @agent_ppo2.py:194][0m |           0.0018 |          10.8946 |           3.4527 |
[32m[20230204 15:40:31 @agent_ppo2.py:194][0m |          -0.0048 |           8.8378 |           3.4512 |
[32m[20230204 15:40:31 @agent_ppo2.py:194][0m |          -0.0040 |           8.7486 |           3.4509 |
[32m[20230204 15:40:31 @agent_ppo2.py:194][0m |          -0.0065 |           8.3501 |           3.4511 |
[32m[20230204 15:40:31 @agent_ppo2.py:194][0m |          -0.0076 |           8.2865 |           3.4511 |
[32m[20230204 15:40:31 @agent_ppo2.py:194][0m |          -0.0091 |           8.1131 |           3.4507 |
[32m[20230204 15:40:31 @agent_ppo2.py:194][0m |          -0.0102 |           8.0022 |           3.4541 |
[32m[20230204 15:40:32 @agent_ppo2.py:194][0m |          -0.0100 |           7.9545 |           3.4518 |
[32m[20230204 15:40:32 @agent_ppo2.py:194][0m |          -0.0111 |           7.8809 |           3.4532 |
[32m[20230204 15:40:32 @agent_ppo2.py:194][0m |          -0.0104 |           7.8338 |           3.4514 |
[32m[20230204 15:40:32 @agent_ppo2.py:139][0m Policy update time: 1.12 s
[32m[20230204 15:40:32 @agent_ppo2.py:147][0m Average TRAINING episode reward: 241.73
[32m[20230204 15:40:32 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 244.42
[32m[20230204 15:40:32 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.51
[32m[20230204 15:40:32 @agent_ppo2.py:152][0m Total time:       4.36 min
[32m[20230204 15:40:32 @agent_ppo2.py:154][0m 301056 total steps have happened
[32m[20230204 15:40:32 @agent_ppo2.py:130][0m #------------------------ Iteration 147 --------------------------#
[32m[20230204 15:40:32 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:40:32 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:32 @agent_ppo2.py:194][0m |          -0.0008 |          17.7464 |           3.5387 |
[32m[20230204 15:40:33 @agent_ppo2.py:194][0m |          -0.0073 |          14.4724 |           3.5327 |
[32m[20230204 15:40:33 @agent_ppo2.py:194][0m |          -0.0097 |          13.3530 |           3.5289 |
[32m[20230204 15:40:33 @agent_ppo2.py:194][0m |          -0.0120 |          12.8543 |           3.5333 |
[32m[20230204 15:40:33 @agent_ppo2.py:194][0m |          -0.0109 |          12.4188 |           3.5299 |
[32m[20230204 15:40:33 @agent_ppo2.py:194][0m |          -0.0124 |          12.1100 |           3.5317 |
[32m[20230204 15:40:33 @agent_ppo2.py:194][0m |          -0.0113 |          11.8311 |           3.5307 |
[32m[20230204 15:40:33 @agent_ppo2.py:194][0m |          -0.0132 |          11.5044 |           3.5292 |
[32m[20230204 15:40:33 @agent_ppo2.py:194][0m |          -0.0087 |          11.3949 |           3.5293 |
[32m[20230204 15:40:33 @agent_ppo2.py:194][0m |          -0.0144 |          10.7737 |           3.5321 |
[32m[20230204 15:40:33 @agent_ppo2.py:139][0m Policy update time: 1.04 s
[32m[20230204 15:40:34 @agent_ppo2.py:147][0m Average TRAINING episode reward: 200.76
[32m[20230204 15:40:34 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 246.47
[32m[20230204 15:40:34 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.76
[32m[20230204 15:40:34 @agent_ppo2.py:152][0m Total time:       4.38 min
[32m[20230204 15:40:34 @agent_ppo2.py:154][0m 303104 total steps have happened
[32m[20230204 15:40:34 @agent_ppo2.py:130][0m #------------------------ Iteration 148 --------------------------#
[32m[20230204 15:40:34 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:40:34 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:34 @agent_ppo2.py:194][0m |           0.0011 |          25.2456 |           3.5408 |
[32m[20230204 15:40:34 @agent_ppo2.py:194][0m |          -0.0110 |          19.2095 |           3.5350 |
[32m[20230204 15:40:34 @agent_ppo2.py:194][0m |          -0.0060 |          14.3881 |           3.5336 |
[32m[20230204 15:40:34 @agent_ppo2.py:194][0m |          -0.0091 |          12.9813 |           3.5345 |
[32m[20230204 15:40:34 @agent_ppo2.py:194][0m |          -0.0087 |          12.3371 |           3.5335 |
[32m[20230204 15:40:34 @agent_ppo2.py:194][0m |          -0.0094 |          11.9421 |           3.5351 |
[32m[20230204 15:40:35 @agent_ppo2.py:194][0m |          -0.0095 |          11.7454 |           3.5363 |
[32m[20230204 15:40:35 @agent_ppo2.py:194][0m |          -0.0082 |          11.6527 |           3.5369 |
[32m[20230204 15:40:35 @agent_ppo2.py:194][0m |          -0.0113 |          11.2425 |           3.5389 |
[32m[20230204 15:40:35 @agent_ppo2.py:194][0m |          -0.0104 |          11.2050 |           3.5388 |
[32m[20230204 15:40:35 @agent_ppo2.py:139][0m Policy update time: 0.98 s
[32m[20230204 15:40:35 @agent_ppo2.py:147][0m Average TRAINING episode reward: 185.11
[32m[20230204 15:40:35 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 248.30
[32m[20230204 15:40:35 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 280.43
[32m[20230204 15:40:35 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 280.43
[32m[20230204 15:40:35 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 280.43
[32m[20230204 15:40:35 @agent_ppo2.py:152][0m Total time:       4.41 min
[32m[20230204 15:40:35 @agent_ppo2.py:154][0m 305152 total steps have happened
[32m[20230204 15:40:35 @agent_ppo2.py:130][0m #------------------------ Iteration 149 --------------------------#
[32m[20230204 15:40:35 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:40:35 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:36 @agent_ppo2.py:194][0m |           0.0008 |           7.8109 |           3.5511 |
[32m[20230204 15:40:36 @agent_ppo2.py:194][0m |          -0.0036 |           7.1933 |           3.5512 |
[32m[20230204 15:40:36 @agent_ppo2.py:194][0m |          -0.0053 |           6.9512 |           3.5507 |
[32m[20230204 15:40:36 @agent_ppo2.py:194][0m |          -0.0073 |           6.7357 |           3.5524 |
[32m[20230204 15:40:36 @agent_ppo2.py:194][0m |          -0.0076 |           6.6510 |           3.5523 |
[32m[20230204 15:40:36 @agent_ppo2.py:194][0m |          -0.0086 |           6.5685 |           3.5535 |
[32m[20230204 15:40:36 @agent_ppo2.py:194][0m |          -0.0091 |           6.4806 |           3.5541 |
[32m[20230204 15:40:36 @agent_ppo2.py:194][0m |          -0.0105 |           6.4335 |           3.5535 |
[32m[20230204 15:40:36 @agent_ppo2.py:194][0m |          -0.0103 |           6.3891 |           3.5554 |
[32m[20230204 15:40:36 @agent_ppo2.py:194][0m |          -0.0105 |           6.3428 |           3.5549 |
[32m[20230204 15:40:36 @agent_ppo2.py:139][0m Policy update time: 1.08 s
[32m[20230204 15:40:37 @agent_ppo2.py:147][0m Average TRAINING episode reward: 243.08
[32m[20230204 15:40:37 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 246.33
[32m[20230204 15:40:37 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 83.47
[32m[20230204 15:40:37 @agent_ppo2.py:152][0m Total time:       4.44 min
[32m[20230204 15:40:37 @agent_ppo2.py:154][0m 307200 total steps have happened
[32m[20230204 15:40:37 @agent_ppo2.py:130][0m #------------------------ Iteration 150 --------------------------#
[32m[20230204 15:40:37 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:40:37 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:37 @agent_ppo2.py:194][0m |          -0.0001 |           7.2728 |           3.5674 |
[32m[20230204 15:40:37 @agent_ppo2.py:194][0m |          -0.0018 |           6.9286 |           3.5673 |
[32m[20230204 15:40:37 @agent_ppo2.py:194][0m |          -0.0027 |           6.7942 |           3.5678 |
[32m[20230204 15:40:37 @agent_ppo2.py:194][0m |          -0.0057 |           6.7029 |           3.5697 |
[32m[20230204 15:40:38 @agent_ppo2.py:194][0m |          -0.0064 |           6.6330 |           3.5704 |
[32m[20230204 15:40:38 @agent_ppo2.py:194][0m |          -0.0086 |           6.5923 |           3.5689 |
[32m[20230204 15:40:38 @agent_ppo2.py:194][0m |          -0.0069 |           6.5589 |           3.5729 |
[32m[20230204 15:40:38 @agent_ppo2.py:194][0m |          -0.0071 |           6.5076 |           3.5732 |
[32m[20230204 15:40:38 @agent_ppo2.py:194][0m |          -0.0075 |           6.4663 |           3.5738 |
[32m[20230204 15:40:38 @agent_ppo2.py:194][0m |          -0.0109 |           6.4403 |           3.5764 |
[32m[20230204 15:40:38 @agent_ppo2.py:139][0m Policy update time: 1.14 s
[32m[20230204 15:40:38 @agent_ppo2.py:147][0m Average TRAINING episode reward: 238.11
[32m[20230204 15:40:38 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 245.03
[32m[20230204 15:40:38 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 278.90
[32m[20230204 15:40:38 @agent_ppo2.py:152][0m Total time:       4.46 min
[32m[20230204 15:40:38 @agent_ppo2.py:154][0m 309248 total steps have happened
[32m[20230204 15:40:38 @agent_ppo2.py:130][0m #------------------------ Iteration 151 --------------------------#
[32m[20230204 15:40:39 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:40:39 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:39 @agent_ppo2.py:194][0m |           0.0009 |           7.3688 |           3.6621 |
[32m[20230204 15:40:39 @agent_ppo2.py:194][0m |          -0.0020 |           7.1950 |           3.6643 |
[32m[20230204 15:40:39 @agent_ppo2.py:194][0m |          -0.0040 |           7.1001 |           3.6693 |
[32m[20230204 15:40:39 @agent_ppo2.py:194][0m |          -0.0045 |           7.0179 |           3.6690 |
[32m[20230204 15:40:39 @agent_ppo2.py:194][0m |          -0.0058 |           6.9616 |           3.6737 |
[32m[20230204 15:40:39 @agent_ppo2.py:194][0m |          -0.0062 |           6.9390 |           3.6737 |
[32m[20230204 15:40:39 @agent_ppo2.py:194][0m |          -0.0070 |           6.8739 |           3.6799 |
[32m[20230204 15:40:39 @agent_ppo2.py:194][0m |          -0.0077 |           6.8274 |           3.6833 |
[32m[20230204 15:40:40 @agent_ppo2.py:194][0m |          -0.0081 |           6.7942 |           3.6823 |
[32m[20230204 15:40:40 @agent_ppo2.py:194][0m |          -0.0089 |           6.7821 |           3.6855 |
[32m[20230204 15:40:40 @agent_ppo2.py:139][0m Policy update time: 1.08 s
[32m[20230204 15:40:40 @agent_ppo2.py:147][0m Average TRAINING episode reward: 244.80
[32m[20230204 15:40:40 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 247.72
[32m[20230204 15:40:40 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.51
[32m[20230204 15:40:40 @agent_ppo2.py:152][0m Total time:       4.49 min
[32m[20230204 15:40:40 @agent_ppo2.py:154][0m 311296 total steps have happened
[32m[20230204 15:40:40 @agent_ppo2.py:130][0m #------------------------ Iteration 152 --------------------------#
[32m[20230204 15:40:40 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:40:40 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:40 @agent_ppo2.py:194][0m |          -0.0014 |           7.5959 |           3.7154 |
[32m[20230204 15:40:41 @agent_ppo2.py:194][0m |          -0.0046 |           7.3553 |           3.7119 |
[32m[20230204 15:40:41 @agent_ppo2.py:194][0m |          -0.0055 |           7.2613 |           3.7096 |
[32m[20230204 15:40:41 @agent_ppo2.py:194][0m |          -0.0045 |           7.2138 |           3.7076 |
[32m[20230204 15:40:41 @agent_ppo2.py:194][0m |          -0.0076 |           7.1293 |           3.7042 |
[32m[20230204 15:40:41 @agent_ppo2.py:194][0m |          -0.0075 |           7.1067 |           3.7027 |
[32m[20230204 15:40:41 @agent_ppo2.py:194][0m |           0.0005 |           7.2502 |           3.7058 |
[32m[20230204 15:40:41 @agent_ppo2.py:194][0m |          -0.0080 |           7.0342 |           3.7007 |
[32m[20230204 15:40:41 @agent_ppo2.py:194][0m |          -0.0092 |           7.0055 |           3.6999 |
[32m[20230204 15:40:41 @agent_ppo2.py:194][0m |          -0.0087 |           6.9686 |           3.6988 |
[32m[20230204 15:40:41 @agent_ppo2.py:139][0m Policy update time: 1.08 s
[32m[20230204 15:40:42 @agent_ppo2.py:147][0m Average TRAINING episode reward: 244.99
[32m[20230204 15:40:42 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 247.15
[32m[20230204 15:40:42 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.67
[32m[20230204 15:40:42 @agent_ppo2.py:152][0m Total time:       4.52 min
[32m[20230204 15:40:42 @agent_ppo2.py:154][0m 313344 total steps have happened
[32m[20230204 15:40:42 @agent_ppo2.py:130][0m #------------------------ Iteration 153 --------------------------#
[32m[20230204 15:40:42 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:40:42 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:42 @agent_ppo2.py:194][0m |           0.0011 |           6.9835 |           3.7404 |
[32m[20230204 15:40:42 @agent_ppo2.py:194][0m |          -0.0025 |           6.7350 |           3.7302 |
[32m[20230204 15:40:42 @agent_ppo2.py:194][0m |          -0.0037 |           6.6305 |           3.7334 |
[32m[20230204 15:40:42 @agent_ppo2.py:194][0m |          -0.0047 |           6.5868 |           3.7330 |
[32m[20230204 15:40:42 @agent_ppo2.py:194][0m |          -0.0054 |           6.5117 |           3.7285 |
[32m[20230204 15:40:43 @agent_ppo2.py:194][0m |          -0.0064 |           6.4391 |           3.7305 |
[32m[20230204 15:40:43 @agent_ppo2.py:194][0m |          -0.0064 |           6.3958 |           3.7312 |
[32m[20230204 15:40:43 @agent_ppo2.py:194][0m |          -0.0071 |           6.3724 |           3.7327 |
[32m[20230204 15:40:43 @agent_ppo2.py:194][0m |          -0.0075 |           6.3303 |           3.7313 |
[32m[20230204 15:40:43 @agent_ppo2.py:194][0m |          -0.0076 |           6.3451 |           3.7297 |
[32m[20230204 15:40:43 @agent_ppo2.py:139][0m Policy update time: 1.05 s
[32m[20230204 15:40:43 @agent_ppo2.py:147][0m Average TRAINING episode reward: 247.02
[32m[20230204 15:40:43 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 251.53
[32m[20230204 15:40:43 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.47
[32m[20230204 15:40:43 @agent_ppo2.py:152][0m Total time:       4.54 min
[32m[20230204 15:40:43 @agent_ppo2.py:154][0m 315392 total steps have happened
[32m[20230204 15:40:43 @agent_ppo2.py:130][0m #------------------------ Iteration 154 --------------------------#
[32m[20230204 15:40:43 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:40:44 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:44 @agent_ppo2.py:194][0m |           0.0002 |          13.3635 |           3.6517 |
[32m[20230204 15:40:44 @agent_ppo2.py:194][0m |          -0.0057 |           9.3354 |           3.6409 |
[32m[20230204 15:40:44 @agent_ppo2.py:194][0m |          -0.0067 |           8.7135 |           3.6357 |
[32m[20230204 15:40:44 @agent_ppo2.py:194][0m |          -0.0057 |           8.2900 |           3.6336 |
[32m[20230204 15:40:44 @agent_ppo2.py:194][0m |          -0.0062 |           7.9933 |           3.6313 |
[32m[20230204 15:40:44 @agent_ppo2.py:194][0m |          -0.0068 |           7.9186 |           3.6307 |
[32m[20230204 15:40:44 @agent_ppo2.py:194][0m |          -0.0078 |           7.6342 |           3.6290 |
[32m[20230204 15:40:44 @agent_ppo2.py:194][0m |          -0.0092 |           7.4504 |           3.6267 |
[32m[20230204 15:40:44 @agent_ppo2.py:194][0m |          -0.0084 |           7.3841 |           3.6260 |
[32m[20230204 15:40:44 @agent_ppo2.py:194][0m |          -0.0122 |           7.2784 |           3.6229 |
[32m[20230204 15:40:44 @agent_ppo2.py:139][0m Policy update time: 1.03 s
[32m[20230204 15:40:45 @agent_ppo2.py:147][0m Average TRAINING episode reward: 212.91
[32m[20230204 15:40:45 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 249.50
[32m[20230204 15:40:45 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.53
[32m[20230204 15:40:45 @agent_ppo2.py:152][0m Total time:       4.57 min
[32m[20230204 15:40:45 @agent_ppo2.py:154][0m 317440 total steps have happened
[32m[20230204 15:40:45 @agent_ppo2.py:130][0m #------------------------ Iteration 155 --------------------------#
[32m[20230204 15:40:45 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:40:45 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:45 @agent_ppo2.py:194][0m |          -0.0015 |          12.9062 |           3.6055 |
[32m[20230204 15:40:45 @agent_ppo2.py:194][0m |          -0.0051 |           9.6376 |           3.6001 |
[32m[20230204 15:40:45 @agent_ppo2.py:194][0m |          -0.0070 |           8.8613 |           3.6056 |
[32m[20230204 15:40:45 @agent_ppo2.py:194][0m |          -0.0076 |           8.7292 |           3.6058 |
[32m[20230204 15:40:46 @agent_ppo2.py:194][0m |          -0.0062 |           8.5131 |           3.6026 |
[32m[20230204 15:40:46 @agent_ppo2.py:194][0m |          -0.0069 |           8.5104 |           3.6018 |
[32m[20230204 15:40:46 @agent_ppo2.py:194][0m |          -0.0098 |           8.2397 |           3.6041 |
[32m[20230204 15:40:46 @agent_ppo2.py:194][0m |          -0.0096 |           8.1739 |           3.6038 |
[32m[20230204 15:40:46 @agent_ppo2.py:194][0m |          -0.0105 |           8.0850 |           3.6034 |
[32m[20230204 15:40:46 @agent_ppo2.py:194][0m |          -0.0108 |           8.1070 |           3.6015 |
[32m[20230204 15:40:46 @agent_ppo2.py:139][0m Policy update time: 1.06 s
[32m[20230204 15:40:46 @agent_ppo2.py:147][0m Average TRAINING episode reward: 175.63
[32m[20230204 15:40:46 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 253.41
[32m[20230204 15:40:46 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 275.89
[32m[20230204 15:40:46 @agent_ppo2.py:152][0m Total time:       4.60 min
[32m[20230204 15:40:46 @agent_ppo2.py:154][0m 319488 total steps have happened
[32m[20230204 15:40:46 @agent_ppo2.py:130][0m #------------------------ Iteration 156 --------------------------#
[32m[20230204 15:40:47 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:40:47 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:47 @agent_ppo2.py:194][0m |           0.0012 |           8.3971 |           3.6742 |
[32m[20230204 15:40:47 @agent_ppo2.py:194][0m |          -0.0041 |           7.7641 |           3.6694 |
[32m[20230204 15:40:47 @agent_ppo2.py:194][0m |          -0.0084 |           7.3628 |           3.6634 |
[32m[20230204 15:40:47 @agent_ppo2.py:194][0m |          -0.0091 |           7.1857 |           3.6602 |
[32m[20230204 15:40:47 @agent_ppo2.py:194][0m |          -0.0094 |           7.0742 |           3.6615 |
[32m[20230204 15:40:47 @agent_ppo2.py:194][0m |          -0.0104 |           6.9796 |           3.6587 |
[32m[20230204 15:40:47 @agent_ppo2.py:194][0m |          -0.0098 |           6.9097 |           3.6580 |
[32m[20230204 15:40:47 @agent_ppo2.py:194][0m |          -0.0094 |           6.8282 |           3.6576 |
[32m[20230204 15:40:48 @agent_ppo2.py:194][0m |          -0.0107 |           6.7450 |           3.6538 |
[32m[20230204 15:40:48 @agent_ppo2.py:194][0m |          -0.0108 |           6.6831 |           3.6541 |
[32m[20230204 15:40:48 @agent_ppo2.py:139][0m Policy update time: 1.04 s
[32m[20230204 15:40:48 @agent_ppo2.py:147][0m Average TRAINING episode reward: 249.18
[32m[20230204 15:40:48 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 250.48
[32m[20230204 15:40:48 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 278.78
[32m[20230204 15:40:48 @agent_ppo2.py:152][0m Total time:       4.62 min
[32m[20230204 15:40:48 @agent_ppo2.py:154][0m 321536 total steps have happened
[32m[20230204 15:40:48 @agent_ppo2.py:130][0m #------------------------ Iteration 157 --------------------------#
[32m[20230204 15:40:48 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:40:48 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:48 @agent_ppo2.py:194][0m |           0.0005 |          35.2675 |           3.5864 |
[32m[20230204 15:40:48 @agent_ppo2.py:194][0m |          -0.0044 |          23.0148 |           3.5796 |
[32m[20230204 15:40:49 @agent_ppo2.py:194][0m |          -0.0086 |          19.9284 |           3.5797 |
[32m[20230204 15:40:49 @agent_ppo2.py:194][0m |          -0.0075 |          18.1891 |           3.5818 |
[32m[20230204 15:40:49 @agent_ppo2.py:194][0m |          -0.0099 |          16.8661 |           3.5816 |
[32m[20230204 15:40:49 @agent_ppo2.py:194][0m |          -0.0097 |          16.4148 |           3.5817 |
[32m[20230204 15:40:49 @agent_ppo2.py:194][0m |          -0.0108 |          16.1097 |           3.5840 |
[32m[20230204 15:40:49 @agent_ppo2.py:194][0m |          -0.0113 |          14.6546 |           3.5803 |
[32m[20230204 15:40:49 @agent_ppo2.py:194][0m |          -0.0116 |          15.0391 |           3.5823 |
[32m[20230204 15:40:49 @agent_ppo2.py:194][0m |          -0.0133 |          14.1873 |           3.5825 |
[32m[20230204 15:40:49 @agent_ppo2.py:139][0m Policy update time: 1.06 s
[32m[20230204 15:40:49 @agent_ppo2.py:147][0m Average TRAINING episode reward: 102.05
[32m[20230204 15:40:49 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 248.45
[32m[20230204 15:40:49 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.28
[32m[20230204 15:40:49 @agent_ppo2.py:152][0m Total time:       4.65 min
[32m[20230204 15:40:49 @agent_ppo2.py:154][0m 323584 total steps have happened
[32m[20230204 15:40:49 @agent_ppo2.py:130][0m #------------------------ Iteration 158 --------------------------#
[32m[20230204 15:40:50 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:40:50 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:50 @agent_ppo2.py:194][0m |          -0.0006 |          20.1687 |           3.6856 |
[32m[20230204 15:40:50 @agent_ppo2.py:194][0m |          -0.0037 |          15.5817 |           3.6743 |
[32m[20230204 15:40:50 @agent_ppo2.py:194][0m |          -0.0066 |          14.5414 |           3.6709 |
[32m[20230204 15:40:50 @agent_ppo2.py:194][0m |          -0.0078 |          13.3654 |           3.6662 |
[32m[20230204 15:40:50 @agent_ppo2.py:194][0m |          -0.0095 |          12.8830 |           3.6623 |
[32m[20230204 15:40:50 @agent_ppo2.py:194][0m |          -0.0094 |          12.5359 |           3.6621 |
[32m[20230204 15:40:50 @agent_ppo2.py:194][0m |          -0.0098 |          12.1035 |           3.6613 |
[32m[20230204 15:40:51 @agent_ppo2.py:194][0m |          -0.0101 |          11.8682 |           3.6601 |
[32m[20230204 15:40:51 @agent_ppo2.py:194][0m |          -0.0095 |          11.8325 |           3.6571 |
[32m[20230204 15:40:51 @agent_ppo2.py:194][0m |          -0.0108 |          11.5744 |           3.6572 |
[32m[20230204 15:40:51 @agent_ppo2.py:139][0m Policy update time: 1.01 s
[32m[20230204 15:40:51 @agent_ppo2.py:147][0m Average TRAINING episode reward: 203.64
[32m[20230204 15:40:51 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 249.62
[32m[20230204 15:40:51 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.40
[32m[20230204 15:40:51 @agent_ppo2.py:152][0m Total time:       4.68 min
[32m[20230204 15:40:51 @agent_ppo2.py:154][0m 325632 total steps have happened
[32m[20230204 15:40:51 @agent_ppo2.py:130][0m #------------------------ Iteration 159 --------------------------#
[32m[20230204 15:40:51 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:40:51 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:52 @agent_ppo2.py:194][0m |           0.0019 |           9.3173 |           3.6004 |
[32m[20230204 15:40:52 @agent_ppo2.py:194][0m |          -0.0023 |           8.6435 |           3.5972 |
[32m[20230204 15:40:52 @agent_ppo2.py:194][0m |          -0.0041 |           8.4802 |           3.5920 |
[32m[20230204 15:40:52 @agent_ppo2.py:194][0m |          -0.0053 |           8.3839 |           3.5924 |
[32m[20230204 15:40:52 @agent_ppo2.py:194][0m |          -0.0057 |           8.2858 |           3.5876 |
[32m[20230204 15:40:52 @agent_ppo2.py:194][0m |          -0.0060 |           8.2430 |           3.5893 |
[32m[20230204 15:40:52 @agent_ppo2.py:194][0m |          -0.0073 |           8.1538 |           3.5880 |
[32m[20230204 15:40:52 @agent_ppo2.py:194][0m |          -0.0072 |           8.1118 |           3.5862 |
[32m[20230204 15:40:52 @agent_ppo2.py:194][0m |          -0.0073 |           8.0911 |           3.5836 |
[32m[20230204 15:40:52 @agent_ppo2.py:194][0m |          -0.0076 |           8.0959 |           3.5833 |
[32m[20230204 15:40:52 @agent_ppo2.py:139][0m Policy update time: 1.07 s
[32m[20230204 15:40:53 @agent_ppo2.py:147][0m Average TRAINING episode reward: 245.57
[32m[20230204 15:40:53 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 256.06
[32m[20230204 15:40:53 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 274.09
[32m[20230204 15:40:53 @agent_ppo2.py:152][0m Total time:       4.70 min
[32m[20230204 15:40:53 @agent_ppo2.py:154][0m 327680 total steps have happened
[32m[20230204 15:40:53 @agent_ppo2.py:130][0m #------------------------ Iteration 160 --------------------------#
[32m[20230204 15:40:53 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:40:53 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:53 @agent_ppo2.py:194][0m |           0.0006 |           7.8107 |           3.5544 |
[32m[20230204 15:40:53 @agent_ppo2.py:194][0m |          -0.0048 |           7.3459 |           3.5528 |
[32m[20230204 15:40:53 @agent_ppo2.py:194][0m |          -0.0055 |           7.1618 |           3.5497 |
[32m[20230204 15:40:53 @agent_ppo2.py:194][0m |          -0.0051 |           7.0885 |           3.5477 |
[32m[20230204 15:40:53 @agent_ppo2.py:194][0m |          -0.0060 |           6.9499 |           3.5464 |
[32m[20230204 15:40:54 @agent_ppo2.py:194][0m |          -0.0082 |           6.8818 |           3.5468 |
[32m[20230204 15:40:54 @agent_ppo2.py:194][0m |          -0.0081 |           6.8295 |           3.5471 |
[32m[20230204 15:40:54 @agent_ppo2.py:194][0m |          -0.0088 |           6.7725 |           3.5472 |
[32m[20230204 15:40:54 @agent_ppo2.py:194][0m |          -0.0080 |           6.7230 |           3.5449 |
[32m[20230204 15:40:54 @agent_ppo2.py:194][0m |          -0.0099 |           6.6610 |           3.5425 |
[32m[20230204 15:40:54 @agent_ppo2.py:139][0m Policy update time: 1.06 s
[32m[20230204 15:40:54 @agent_ppo2.py:147][0m Average TRAINING episode reward: 245.25
[32m[20230204 15:40:54 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 248.06
[32m[20230204 15:40:54 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.46
[32m[20230204 15:40:54 @agent_ppo2.py:152][0m Total time:       4.73 min
[32m[20230204 15:40:54 @agent_ppo2.py:154][0m 329728 total steps have happened
[32m[20230204 15:40:54 @agent_ppo2.py:130][0m #------------------------ Iteration 161 --------------------------#
[32m[20230204 15:40:55 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:40:55 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:55 @agent_ppo2.py:194][0m |          -0.0013 |           8.2155 |           3.6550 |
[32m[20230204 15:40:55 @agent_ppo2.py:194][0m |          -0.0049 |           7.9533 |           3.6511 |
[32m[20230204 15:40:55 @agent_ppo2.py:194][0m |          -0.0075 |           7.8123 |           3.6475 |
[32m[20230204 15:40:55 @agent_ppo2.py:194][0m |          -0.0092 |           7.6900 |           3.6495 |
[32m[20230204 15:40:55 @agent_ppo2.py:194][0m |          -0.0055 |           7.7748 |           3.6473 |
[32m[20230204 15:40:55 @agent_ppo2.py:194][0m |          -0.0091 |           7.5276 |           3.6497 |
[32m[20230204 15:40:55 @agent_ppo2.py:194][0m |          -0.0104 |           7.4435 |           3.6468 |
[32m[20230204 15:40:55 @agent_ppo2.py:194][0m |          -0.0100 |           7.3514 |           3.6462 |
[32m[20230204 15:40:55 @agent_ppo2.py:194][0m |          -0.0097 |           7.2958 |           3.6483 |
[32m[20230204 15:40:56 @agent_ppo2.py:194][0m |          -0.0069 |           7.4302 |           3.6491 |
[32m[20230204 15:40:56 @agent_ppo2.py:139][0m Policy update time: 1.03 s
[32m[20230204 15:40:56 @agent_ppo2.py:147][0m Average TRAINING episode reward: 249.38
[32m[20230204 15:40:56 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 255.42
[32m[20230204 15:40:56 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.18
[32m[20230204 15:40:56 @agent_ppo2.py:152][0m Total time:       4.76 min
[32m[20230204 15:40:56 @agent_ppo2.py:154][0m 331776 total steps have happened
[32m[20230204 15:40:56 @agent_ppo2.py:130][0m #------------------------ Iteration 162 --------------------------#
[32m[20230204 15:40:56 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:40:56 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:56 @agent_ppo2.py:194][0m |           0.0020 |          15.4539 |           3.6586 |
[32m[20230204 15:40:56 @agent_ppo2.py:194][0m |           0.0022 |           9.5527 |           3.6530 |
[32m[20230204 15:40:56 @agent_ppo2.py:194][0m |          -0.0079 |           8.9896 |           3.6546 |
[32m[20230204 15:40:57 @agent_ppo2.py:194][0m |          -0.0106 |           8.3357 |           3.6566 |
[32m[20230204 15:40:57 @agent_ppo2.py:194][0m |          -0.0039 |           8.2149 |           3.6539 |
[32m[20230204 15:40:57 @agent_ppo2.py:194][0m |          -0.0111 |           7.8888 |           3.6588 |
[32m[20230204 15:40:57 @agent_ppo2.py:194][0m |          -0.0125 |           7.7532 |           3.6607 |
[32m[20230204 15:40:57 @agent_ppo2.py:194][0m |          -0.0086 |           7.6531 |           3.6640 |
[32m[20230204 15:40:57 @agent_ppo2.py:194][0m |          -0.0115 |           7.4533 |           3.6662 |
[32m[20230204 15:40:57 @agent_ppo2.py:194][0m |          -0.0076 |           7.3724 |           3.6645 |
[32m[20230204 15:40:57 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:40:57 @agent_ppo2.py:147][0m Average TRAINING episode reward: 192.62
[32m[20230204 15:40:57 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 254.74
[32m[20230204 15:40:57 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 278.08
[32m[20230204 15:40:57 @agent_ppo2.py:152][0m Total time:       4.78 min
[32m[20230204 15:40:57 @agent_ppo2.py:154][0m 333824 total steps have happened
[32m[20230204 15:40:57 @agent_ppo2.py:130][0m #------------------------ Iteration 163 --------------------------#
[32m[20230204 15:40:58 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:40:58 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:58 @agent_ppo2.py:194][0m |           0.0014 |           8.8887 |           3.7692 |
[32m[20230204 15:40:58 @agent_ppo2.py:194][0m |          -0.0029 |           8.5919 |           3.7612 |
[32m[20230204 15:40:58 @agent_ppo2.py:194][0m |          -0.0051 |           8.4702 |           3.7609 |
[32m[20230204 15:40:58 @agent_ppo2.py:194][0m |          -0.0066 |           8.3783 |           3.7619 |
[32m[20230204 15:40:58 @agent_ppo2.py:194][0m |          -0.0063 |           8.3136 |           3.7625 |
[32m[20230204 15:40:58 @agent_ppo2.py:194][0m |          -0.0070 |           8.2695 |           3.7623 |
[32m[20230204 15:40:58 @agent_ppo2.py:194][0m |          -0.0087 |           8.2225 |           3.7628 |
[32m[20230204 15:40:58 @agent_ppo2.py:194][0m |          -0.0087 |           8.1862 |           3.7625 |
[32m[20230204 15:40:58 @agent_ppo2.py:194][0m |          -0.0088 |           8.1376 |           3.7627 |
[32m[20230204 15:40:59 @agent_ppo2.py:194][0m |          -0.0103 |           8.1084 |           3.7644 |
[32m[20230204 15:40:59 @agent_ppo2.py:139][0m Policy update time: 1.00 s
[32m[20230204 15:40:59 @agent_ppo2.py:147][0m Average TRAINING episode reward: 252.45
[32m[20230204 15:40:59 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 254.14
[32m[20230204 15:40:59 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.48
[32m[20230204 15:40:59 @agent_ppo2.py:152][0m Total time:       4.81 min
[32m[20230204 15:40:59 @agent_ppo2.py:154][0m 335872 total steps have happened
[32m[20230204 15:40:59 @agent_ppo2.py:130][0m #------------------------ Iteration 164 --------------------------#
[32m[20230204 15:40:59 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:40:59 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:40:59 @agent_ppo2.py:194][0m |           0.0018 |           8.5428 |           3.6695 |
[32m[20230204 15:40:59 @agent_ppo2.py:194][0m |          -0.0084 |           7.9119 |           3.6638 |
[32m[20230204 15:40:59 @agent_ppo2.py:194][0m |          -0.0096 |           7.6085 |           3.6614 |
[32m[20230204 15:41:00 @agent_ppo2.py:194][0m |          -0.0059 |           7.5757 |           3.6613 |
[32m[20230204 15:41:00 @agent_ppo2.py:194][0m |          -0.0128 |           7.2844 |           3.6554 |
[32m[20230204 15:41:00 @agent_ppo2.py:194][0m |          -0.0110 |           7.1841 |           3.6560 |
[32m[20230204 15:41:00 @agent_ppo2.py:194][0m |          -0.0136 |           7.0587 |           3.6584 |
[32m[20230204 15:41:00 @agent_ppo2.py:194][0m |          -0.0160 |           6.9583 |           3.6559 |
[32m[20230204 15:41:00 @agent_ppo2.py:194][0m |          -0.0132 |           6.9302 |           3.6552 |
[32m[20230204 15:41:00 @agent_ppo2.py:194][0m |          -0.0136 |           6.7834 |           3.6563 |
[32m[20230204 15:41:00 @agent_ppo2.py:139][0m Policy update time: 1.03 s
[32m[20230204 15:41:00 @agent_ppo2.py:147][0m Average TRAINING episode reward: 248.94
[32m[20230204 15:41:00 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 251.58
[32m[20230204 15:41:00 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 4.25
[32m[20230204 15:41:00 @agent_ppo2.py:152][0m Total time:       4.83 min
[32m[20230204 15:41:00 @agent_ppo2.py:154][0m 337920 total steps have happened
[32m[20230204 15:41:00 @agent_ppo2.py:130][0m #------------------------ Iteration 165 --------------------------#
[32m[20230204 15:41:01 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:41:01 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:01 @agent_ppo2.py:194][0m |           0.0005 |           8.5487 |           3.6889 |
[32m[20230204 15:41:01 @agent_ppo2.py:194][0m |          -0.0043 |           8.2084 |           3.6827 |
[32m[20230204 15:41:01 @agent_ppo2.py:194][0m |          -0.0061 |           8.0491 |           3.6808 |
[32m[20230204 15:41:01 @agent_ppo2.py:194][0m |          -0.0055 |           7.9707 |           3.6798 |
[32m[20230204 15:41:01 @agent_ppo2.py:194][0m |          -0.0067 |           7.9217 |           3.6795 |
[32m[20230204 15:41:01 @agent_ppo2.py:194][0m |          -0.0070 |           7.8508 |           3.6794 |
[32m[20230204 15:41:01 @agent_ppo2.py:194][0m |          -0.0075 |           7.7788 |           3.6811 |
[32m[20230204 15:41:01 @agent_ppo2.py:194][0m |          -0.0077 |           7.7312 |           3.6804 |
[32m[20230204 15:41:01 @agent_ppo2.py:194][0m |          -0.0084 |           7.6640 |           3.6784 |
[32m[20230204 15:41:02 @agent_ppo2.py:194][0m |          -0.0079 |           7.6391 |           3.6792 |
[32m[20230204 15:41:02 @agent_ppo2.py:139][0m Policy update time: 0.99 s
[32m[20230204 15:41:02 @agent_ppo2.py:147][0m Average TRAINING episode reward: 253.00
[32m[20230204 15:41:02 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 255.75
[32m[20230204 15:41:02 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -8.99
[32m[20230204 15:41:02 @agent_ppo2.py:152][0m Total time:       4.86 min
[32m[20230204 15:41:02 @agent_ppo2.py:154][0m 339968 total steps have happened
[32m[20230204 15:41:02 @agent_ppo2.py:130][0m #------------------------ Iteration 166 --------------------------#
[32m[20230204 15:41:02 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:02 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:02 @agent_ppo2.py:194][0m |          -0.0001 |           8.2143 |           3.7680 |
[32m[20230204 15:41:02 @agent_ppo2.py:194][0m |          -0.0032 |           7.6020 |           3.7579 |
[32m[20230204 15:41:02 @agent_ppo2.py:194][0m |          -0.0028 |           7.3593 |           3.7572 |
[32m[20230204 15:41:02 @agent_ppo2.py:194][0m |          -0.0050 |           7.1414 |           3.7548 |
[32m[20230204 15:41:03 @agent_ppo2.py:194][0m |          -0.0051 |           6.9741 |           3.7511 |
[32m[20230204 15:41:03 @agent_ppo2.py:194][0m |          -0.0082 |           6.8054 |           3.7531 |
[32m[20230204 15:41:03 @agent_ppo2.py:194][0m |          -0.0071 |           6.6916 |           3.7510 |
[32m[20230204 15:41:03 @agent_ppo2.py:194][0m |          -0.0091 |           6.6030 |           3.7509 |
[32m[20230204 15:41:03 @agent_ppo2.py:194][0m |          -0.0080 |           6.4951 |           3.7483 |
[32m[20230204 15:41:03 @agent_ppo2.py:194][0m |          -0.0089 |           6.3585 |           3.7460 |
[32m[20230204 15:41:03 @agent_ppo2.py:139][0m Policy update time: 0.99 s
[32m[20230204 15:41:03 @agent_ppo2.py:147][0m Average TRAINING episode reward: 252.51
[32m[20230204 15:41:03 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 253.69
[32m[20230204 15:41:03 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 88.94
[32m[20230204 15:41:03 @agent_ppo2.py:152][0m Total time:       4.88 min
[32m[20230204 15:41:03 @agent_ppo2.py:154][0m 342016 total steps have happened
[32m[20230204 15:41:03 @agent_ppo2.py:130][0m #------------------------ Iteration 167 --------------------------#
[32m[20230204 15:41:04 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:04 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:04 @agent_ppo2.py:194][0m |           0.0004 |          16.2092 |           3.7005 |
[32m[20230204 15:41:04 @agent_ppo2.py:194][0m |          -0.0049 |          10.9869 |           3.6920 |
[32m[20230204 15:41:04 @agent_ppo2.py:194][0m |          -0.0070 |          10.3752 |           3.6904 |
[32m[20230204 15:41:04 @agent_ppo2.py:194][0m |          -0.0082 |           9.5833 |           3.6913 |
[32m[20230204 15:41:04 @agent_ppo2.py:194][0m |          -0.0096 |           9.5347 |           3.6846 |
[32m[20230204 15:41:04 @agent_ppo2.py:194][0m |          -0.0097 |           9.2513 |           3.6844 |
[32m[20230204 15:41:04 @agent_ppo2.py:194][0m |          -0.0101 |           9.2573 |           3.6841 |
[32m[20230204 15:41:04 @agent_ppo2.py:194][0m |          -0.0108 |           8.8822 |           3.6815 |
[32m[20230204 15:41:04 @agent_ppo2.py:194][0m |          -0.0117 |           8.6639 |           3.6809 |
[32m[20230204 15:41:04 @agent_ppo2.py:194][0m |          -0.0116 |           8.5748 |           3.6767 |
[32m[20230204 15:41:04 @agent_ppo2.py:139][0m Policy update time: 0.97 s
[32m[20230204 15:41:05 @agent_ppo2.py:147][0m Average TRAINING episode reward: 179.34
[32m[20230204 15:41:05 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.98
[32m[20230204 15:41:05 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 88.22
[32m[20230204 15:41:05 @agent_ppo2.py:152][0m Total time:       4.91 min
[32m[20230204 15:41:05 @agent_ppo2.py:154][0m 344064 total steps have happened
[32m[20230204 15:41:05 @agent_ppo2.py:130][0m #------------------------ Iteration 168 --------------------------#
[32m[20230204 15:41:05 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:41:05 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:05 @agent_ppo2.py:194][0m |          -0.0004 |          10.8875 |           3.6803 |
[32m[20230204 15:41:05 @agent_ppo2.py:194][0m |          -0.0058 |           9.2064 |           3.6793 |
[32m[20230204 15:41:05 @agent_ppo2.py:194][0m |          -0.0079 |           8.8359 |           3.6758 |
[32m[20230204 15:41:06 @agent_ppo2.py:194][0m |          -0.0097 |           8.5474 |           3.6770 |
[32m[20230204 15:41:06 @agent_ppo2.py:194][0m |          -0.0106 |           8.3888 |           3.6761 |
[32m[20230204 15:41:06 @agent_ppo2.py:194][0m |          -0.0113 |           8.2416 |           3.6732 |
[32m[20230204 15:41:06 @agent_ppo2.py:194][0m |          -0.0118 |           8.1167 |           3.6741 |
[32m[20230204 15:41:06 @agent_ppo2.py:194][0m |          -0.0121 |           7.9935 |           3.6697 |
[32m[20230204 15:41:06 @agent_ppo2.py:194][0m |          -0.0118 |           7.9151 |           3.6727 |
[32m[20230204 15:41:06 @agent_ppo2.py:194][0m |          -0.0127 |           7.8427 |           3.6716 |
[32m[20230204 15:41:06 @agent_ppo2.py:139][0m Policy update time: 1.01 s
[32m[20230204 15:41:06 @agent_ppo2.py:147][0m Average TRAINING episode reward: 249.90
[32m[20230204 15:41:06 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 253.52
[32m[20230204 15:41:06 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 276.99
[32m[20230204 15:41:06 @agent_ppo2.py:152][0m Total time:       4.93 min
[32m[20230204 15:41:06 @agent_ppo2.py:154][0m 346112 total steps have happened
[32m[20230204 15:41:06 @agent_ppo2.py:130][0m #------------------------ Iteration 169 --------------------------#
[32m[20230204 15:41:07 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:07 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:07 @agent_ppo2.py:194][0m |          -0.0013 |          21.4404 |           3.6773 |
[32m[20230204 15:41:07 @agent_ppo2.py:194][0m |          -0.0069 |          12.8622 |           3.6761 |
[32m[20230204 15:41:07 @agent_ppo2.py:194][0m |          -0.0094 |          11.6411 |           3.6753 |
[32m[20230204 15:41:07 @agent_ppo2.py:194][0m |          -0.0102 |          10.9649 |           3.6754 |
[32m[20230204 15:41:07 @agent_ppo2.py:194][0m |          -0.0102 |          10.5856 |           3.6765 |
[32m[20230204 15:41:07 @agent_ppo2.py:194][0m |          -0.0109 |          10.2636 |           3.6765 |
[32m[20230204 15:41:07 @agent_ppo2.py:194][0m |          -0.0106 |          10.0107 |           3.6771 |
[32m[20230204 15:41:07 @agent_ppo2.py:194][0m |          -0.0118 |           9.9105 |           3.6770 |
[32m[20230204 15:41:07 @agent_ppo2.py:194][0m |          -0.0120 |           9.6130 |           3.6805 |
[32m[20230204 15:41:08 @agent_ppo2.py:194][0m |          -0.0107 |           9.5397 |           3.6780 |
[32m[20230204 15:41:08 @agent_ppo2.py:139][0m Policy update time: 1.00 s
[32m[20230204 15:41:08 @agent_ppo2.py:147][0m Average TRAINING episode reward: 248.16
[32m[20230204 15:41:08 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 252.39
[32m[20230204 15:41:08 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 12.62
[32m[20230204 15:41:08 @agent_ppo2.py:152][0m Total time:       4.95 min
[32m[20230204 15:41:08 @agent_ppo2.py:154][0m 348160 total steps have happened
[32m[20230204 15:41:08 @agent_ppo2.py:130][0m #------------------------ Iteration 170 --------------------------#
[32m[20230204 15:41:08 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:41:08 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:08 @agent_ppo2.py:194][0m |           0.0015 |          10.5540 |           3.7521 |
[32m[20230204 15:41:08 @agent_ppo2.py:194][0m |          -0.0034 |           9.6637 |           3.7478 |
[32m[20230204 15:41:08 @agent_ppo2.py:194][0m |          -0.0070 |           9.3424 |           3.7439 |
[32m[20230204 15:41:08 @agent_ppo2.py:194][0m |          -0.0071 |           9.1531 |           3.7456 |
[32m[20230204 15:41:09 @agent_ppo2.py:194][0m |          -0.0089 |           9.0137 |           3.7431 |
[32m[20230204 15:41:09 @agent_ppo2.py:194][0m |          -0.0090 |           8.9221 |           3.7441 |
[32m[20230204 15:41:09 @agent_ppo2.py:194][0m |          -0.0091 |           8.8541 |           3.7408 |
[32m[20230204 15:41:09 @agent_ppo2.py:194][0m |          -0.0099 |           8.7652 |           3.7417 |
[32m[20230204 15:41:09 @agent_ppo2.py:194][0m |          -0.0107 |           8.7351 |           3.7432 |
[32m[20230204 15:41:09 @agent_ppo2.py:194][0m |          -0.0100 |           8.7195 |           3.7422 |
[32m[20230204 15:41:09 @agent_ppo2.py:139][0m Policy update time: 0.99 s
[32m[20230204 15:41:09 @agent_ppo2.py:147][0m Average TRAINING episode reward: 251.08
[32m[20230204 15:41:09 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 253.85
[32m[20230204 15:41:09 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.63
[32m[20230204 15:41:09 @agent_ppo2.py:152][0m Total time:       4.98 min
[32m[20230204 15:41:09 @agent_ppo2.py:154][0m 350208 total steps have happened
[32m[20230204 15:41:09 @agent_ppo2.py:130][0m #------------------------ Iteration 171 --------------------------#
[32m[20230204 15:41:09 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:10 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:10 @agent_ppo2.py:194][0m |           0.0013 |          19.9324 |           3.6806 |
[32m[20230204 15:41:10 @agent_ppo2.py:194][0m |          -0.0062 |          11.4807 |           3.6789 |
[32m[20230204 15:41:10 @agent_ppo2.py:194][0m |          -0.0113 |           9.8293 |           3.6732 |
[32m[20230204 15:41:10 @agent_ppo2.py:194][0m |          -0.0129 |           9.2111 |           3.6750 |
[32m[20230204 15:41:10 @agent_ppo2.py:194][0m |          -0.0108 |           8.9197 |           3.6723 |
[32m[20230204 15:41:10 @agent_ppo2.py:194][0m |          -0.0096 |           8.4449 |           3.6715 |
[32m[20230204 15:41:10 @agent_ppo2.py:194][0m |          -0.0151 |           8.1613 |           3.6716 |
[32m[20230204 15:41:10 @agent_ppo2.py:194][0m |          -0.0133 |           8.0846 |           3.6714 |
[32m[20230204 15:41:10 @agent_ppo2.py:194][0m |          -0.0158 |           7.8088 |           3.6698 |
[32m[20230204 15:41:10 @agent_ppo2.py:194][0m |          -0.0162 |           7.6050 |           3.6708 |
[32m[20230204 15:41:10 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:41:11 @agent_ppo2.py:147][0m Average TRAINING episode reward: 194.75
[32m[20230204 15:41:11 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 256.08
[32m[20230204 15:41:11 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 132.35
[32m[20230204 15:41:11 @agent_ppo2.py:152][0m Total time:       5.00 min
[32m[20230204 15:41:11 @agent_ppo2.py:154][0m 352256 total steps have happened
[32m[20230204 15:41:11 @agent_ppo2.py:130][0m #------------------------ Iteration 172 --------------------------#
[32m[20230204 15:41:11 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:41:11 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:11 @agent_ppo2.py:194][0m |          -0.0006 |          11.0035 |           3.6859 |
[32m[20230204 15:41:11 @agent_ppo2.py:194][0m |          -0.0064 |          10.2319 |           3.6772 |
[32m[20230204 15:41:11 @agent_ppo2.py:194][0m |          -0.0079 |           9.9890 |           3.6744 |
[32m[20230204 15:41:11 @agent_ppo2.py:194][0m |          -0.0092 |           9.8293 |           3.6744 |
[32m[20230204 15:41:11 @agent_ppo2.py:194][0m |          -0.0095 |           9.6998 |           3.6709 |
[32m[20230204 15:41:12 @agent_ppo2.py:194][0m |          -0.0096 |           9.5925 |           3.6710 |
[32m[20230204 15:41:12 @agent_ppo2.py:194][0m |          -0.0108 |           9.5483 |           3.6704 |
[32m[20230204 15:41:12 @agent_ppo2.py:194][0m |          -0.0113 |           9.4936 |           3.6721 |
[32m[20230204 15:41:12 @agent_ppo2.py:194][0m |          -0.0121 |           9.3910 |           3.6703 |
[32m[20230204 15:41:12 @agent_ppo2.py:194][0m |          -0.0117 |           9.3668 |           3.6709 |
[32m[20230204 15:41:12 @agent_ppo2.py:139][0m Policy update time: 1.00 s
[32m[20230204 15:41:12 @agent_ppo2.py:147][0m Average TRAINING episode reward: 251.85
[32m[20230204 15:41:12 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 255.52
[32m[20230204 15:41:12 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 274.33
[32m[20230204 15:41:12 @agent_ppo2.py:152][0m Total time:       5.03 min
[32m[20230204 15:41:12 @agent_ppo2.py:154][0m 354304 total steps have happened
[32m[20230204 15:41:12 @agent_ppo2.py:130][0m #------------------------ Iteration 173 --------------------------#
[32m[20230204 15:41:12 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:41:13 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:13 @agent_ppo2.py:194][0m |          -0.0001 |           9.7164 |           3.7417 |
[32m[20230204 15:41:13 @agent_ppo2.py:194][0m |          -0.0061 |           9.2078 |           3.7383 |
[32m[20230204 15:41:13 @agent_ppo2.py:194][0m |          -0.0068 |           9.0347 |           3.7380 |
[32m[20230204 15:41:13 @agent_ppo2.py:194][0m |          -0.0083 |           8.8730 |           3.7377 |
[32m[20230204 15:41:13 @agent_ppo2.py:194][0m |          -0.0089 |           8.7838 |           3.7379 |
[32m[20230204 15:41:13 @agent_ppo2.py:194][0m |          -0.0091 |           8.6452 |           3.7384 |
[32m[20230204 15:41:13 @agent_ppo2.py:194][0m |          -0.0096 |           8.5664 |           3.7398 |
[32m[20230204 15:41:13 @agent_ppo2.py:194][0m |          -0.0105 |           8.4730 |           3.7386 |
[32m[20230204 15:41:13 @agent_ppo2.py:194][0m |          -0.0106 |           8.4132 |           3.7385 |
[32m[20230204 15:41:13 @agent_ppo2.py:194][0m |          -0.0111 |           8.3609 |           3.7385 |
[32m[20230204 15:41:13 @agent_ppo2.py:139][0m Policy update time: 1.02 s
[32m[20230204 15:41:14 @agent_ppo2.py:147][0m Average TRAINING episode reward: 248.38
[32m[20230204 15:41:14 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 249.75
[32m[20230204 15:41:14 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.00
[32m[20230204 15:41:14 @agent_ppo2.py:152][0m Total time:       5.05 min
[32m[20230204 15:41:14 @agent_ppo2.py:154][0m 356352 total steps have happened
[32m[20230204 15:41:14 @agent_ppo2.py:130][0m #------------------------ Iteration 174 --------------------------#
[32m[20230204 15:41:14 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:41:14 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:14 @agent_ppo2.py:194][0m |           0.0015 |          15.6715 |           3.7511 |
[32m[20230204 15:41:14 @agent_ppo2.py:194][0m |          -0.0029 |          11.6245 |           3.7413 |
[32m[20230204 15:41:14 @agent_ppo2.py:194][0m |          -0.0056 |          10.6453 |           3.7388 |
[32m[20230204 15:41:14 @agent_ppo2.py:194][0m |          -0.0074 |          10.2717 |           3.7370 |
[32m[20230204 15:41:15 @agent_ppo2.py:194][0m |          -0.0080 |           9.8781 |           3.7337 |
[32m[20230204 15:41:15 @agent_ppo2.py:194][0m |          -0.0097 |           9.5337 |           3.7300 |
[32m[20230204 15:41:15 @agent_ppo2.py:194][0m |          -0.0103 |           9.4702 |           3.7275 |
[32m[20230204 15:41:15 @agent_ppo2.py:194][0m |          -0.0104 |           9.2715 |           3.7267 |
[32m[20230204 15:41:15 @agent_ppo2.py:194][0m |          -0.0116 |           9.0872 |           3.7240 |
[32m[20230204 15:41:15 @agent_ppo2.py:194][0m |          -0.0120 |           9.0513 |           3.7220 |
[32m[20230204 15:41:15 @agent_ppo2.py:139][0m Policy update time: 1.00 s
[32m[20230204 15:41:15 @agent_ppo2.py:147][0m Average TRAINING episode reward: 175.78
[32m[20230204 15:41:15 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 254.92
[32m[20230204 15:41:15 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -107.85
[32m[20230204 15:41:15 @agent_ppo2.py:152][0m Total time:       5.08 min
[32m[20230204 15:41:15 @agent_ppo2.py:154][0m 358400 total steps have happened
[32m[20230204 15:41:15 @agent_ppo2.py:130][0m #------------------------ Iteration 175 --------------------------#
[32m[20230204 15:41:15 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:41:15 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:16 @agent_ppo2.py:194][0m |           0.0008 |          33.8624 |           3.7304 |
[32m[20230204 15:41:16 @agent_ppo2.py:194][0m |          -0.0047 |          28.1275 |           3.7244 |
[32m[20230204 15:41:16 @agent_ppo2.py:194][0m |          -0.0068 |          23.5742 |           3.7246 |
[32m[20230204 15:41:16 @agent_ppo2.py:194][0m |          -0.0076 |          22.3029 |           3.7200 |
[32m[20230204 15:41:16 @agent_ppo2.py:194][0m |          -0.0083 |          21.3680 |           3.7198 |
[32m[20230204 15:41:16 @agent_ppo2.py:194][0m |          -0.0082 |          20.2101 |           3.7191 |
[32m[20230204 15:41:16 @agent_ppo2.py:194][0m |          -0.0087 |          18.8690 |           3.7196 |
[32m[20230204 15:41:16 @agent_ppo2.py:194][0m |          -0.0098 |          18.0397 |           3.7172 |
[32m[20230204 15:41:16 @agent_ppo2.py:194][0m |          -0.0109 |          17.4600 |           3.7166 |
[32m[20230204 15:41:16 @agent_ppo2.py:194][0m |          -0.0109 |          16.7717 |           3.7157 |
[32m[20230204 15:41:16 @agent_ppo2.py:139][0m Policy update time: 0.81 s
[32m[20230204 15:41:16 @agent_ppo2.py:147][0m Average TRAINING episode reward: 124.73
[32m[20230204 15:41:16 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.24
[32m[20230204 15:41:16 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 274.60
[32m[20230204 15:41:16 @agent_ppo2.py:152][0m Total time:       5.10 min
[32m[20230204 15:41:16 @agent_ppo2.py:154][0m 360448 total steps have happened
[32m[20230204 15:41:16 @agent_ppo2.py:130][0m #------------------------ Iteration 176 --------------------------#
[32m[20230204 15:41:17 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:41:17 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:17 @agent_ppo2.py:194][0m |           0.0016 |          19.6777 |           3.6652 |
[32m[20230204 15:41:17 @agent_ppo2.py:194][0m |          -0.0036 |          16.1322 |           3.6598 |
[32m[20230204 15:41:17 @agent_ppo2.py:194][0m |          -0.0044 |          14.8724 |           3.6624 |
[32m[20230204 15:41:17 @agent_ppo2.py:194][0m |          -0.0079 |          14.0837 |           3.6646 |
[32m[20230204 15:41:17 @agent_ppo2.py:194][0m |          -0.0094 |          13.4291 |           3.6688 |
[32m[20230204 15:41:17 @agent_ppo2.py:194][0m |          -0.0104 |          12.8944 |           3.6657 |
[32m[20230204 15:41:17 @agent_ppo2.py:194][0m |          -0.0098 |          12.5072 |           3.6648 |
[32m[20230204 15:41:18 @agent_ppo2.py:194][0m |          -0.0109 |          12.2060 |           3.6682 |
[32m[20230204 15:41:18 @agent_ppo2.py:194][0m |          -0.0107 |          11.8619 |           3.6690 |
[32m[20230204 15:41:18 @agent_ppo2.py:194][0m |          -0.0132 |          11.5913 |           3.6737 |
[32m[20230204 15:41:18 @agent_ppo2.py:139][0m Policy update time: 1.01 s
[32m[20230204 15:41:18 @agent_ppo2.py:147][0m Average TRAINING episode reward: 215.09
[32m[20230204 15:41:18 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 248.39
[32m[20230204 15:41:18 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.53
[32m[20230204 15:41:18 @agent_ppo2.py:152][0m Total time:       5.12 min
[32m[20230204 15:41:18 @agent_ppo2.py:154][0m 362496 total steps have happened
[32m[20230204 15:41:18 @agent_ppo2.py:130][0m #------------------------ Iteration 177 --------------------------#
[32m[20230204 15:41:18 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:18 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:18 @agent_ppo2.py:194][0m |          -0.0026 |           9.9699 |           3.7881 |
[32m[20230204 15:41:18 @agent_ppo2.py:194][0m |          -0.0082 |           9.4025 |           3.7785 |
[32m[20230204 15:41:19 @agent_ppo2.py:194][0m |          -0.0099 |           9.1222 |           3.7758 |
[32m[20230204 15:41:19 @agent_ppo2.py:194][0m |          -0.0107 |           8.9481 |           3.7737 |
[32m[20230204 15:41:19 @agent_ppo2.py:194][0m |          -0.0108 |           8.8177 |           3.7734 |
[32m[20230204 15:41:19 @agent_ppo2.py:194][0m |          -0.0113 |           8.6911 |           3.7720 |
[32m[20230204 15:41:19 @agent_ppo2.py:194][0m |          -0.0119 |           8.5858 |           3.7715 |
[32m[20230204 15:41:19 @agent_ppo2.py:194][0m |          -0.0126 |           8.4936 |           3.7696 |
[32m[20230204 15:41:19 @agent_ppo2.py:194][0m |          -0.0126 |           8.4103 |           3.7691 |
[32m[20230204 15:41:19 @agent_ppo2.py:194][0m |          -0.0128 |           8.3458 |           3.7664 |
[32m[20230204 15:41:19 @agent_ppo2.py:139][0m Policy update time: 0.96 s
[32m[20230204 15:41:19 @agent_ppo2.py:147][0m Average TRAINING episode reward: 252.91
[32m[20230204 15:41:19 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.36
[32m[20230204 15:41:19 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.96
[32m[20230204 15:41:19 @agent_ppo2.py:152][0m Total time:       5.15 min
[32m[20230204 15:41:19 @agent_ppo2.py:154][0m 364544 total steps have happened
[32m[20230204 15:41:19 @agent_ppo2.py:130][0m #------------------------ Iteration 178 --------------------------#
[32m[20230204 15:41:20 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:41:20 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:20 @agent_ppo2.py:194][0m |           0.0010 |           9.3713 |           3.8208 |
[32m[20230204 15:41:20 @agent_ppo2.py:194][0m |          -0.0046 |           8.7292 |           3.8117 |
[32m[20230204 15:41:20 @agent_ppo2.py:194][0m |          -0.0052 |           8.5475 |           3.8131 |
[32m[20230204 15:41:20 @agent_ppo2.py:194][0m |          -0.0071 |           8.4267 |           3.8132 |
[32m[20230204 15:41:20 @agent_ppo2.py:194][0m |          -0.0079 |           8.3128 |           3.8119 |
[32m[20230204 15:41:20 @agent_ppo2.py:194][0m |          -0.0091 |           8.2456 |           3.8110 |
[32m[20230204 15:41:20 @agent_ppo2.py:194][0m |          -0.0087 |           8.1923 |           3.8143 |
[32m[20230204 15:41:21 @agent_ppo2.py:194][0m |          -0.0099 |           8.1619 |           3.8144 |
[32m[20230204 15:41:21 @agent_ppo2.py:194][0m |          -0.0103 |           8.0705 |           3.8167 |
[32m[20230204 15:41:21 @agent_ppo2.py:194][0m |          -0.0100 |           8.0268 |           3.8228 |
[32m[20230204 15:41:21 @agent_ppo2.py:139][0m Policy update time: 0.99 s
[32m[20230204 15:41:21 @agent_ppo2.py:147][0m Average TRAINING episode reward: 248.93
[32m[20230204 15:41:21 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 254.28
[32m[20230204 15:41:21 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.87
[32m[20230204 15:41:21 @agent_ppo2.py:152][0m Total time:       5.17 min
[32m[20230204 15:41:21 @agent_ppo2.py:154][0m 366592 total steps have happened
[32m[20230204 15:41:21 @agent_ppo2.py:130][0m #------------------------ Iteration 179 --------------------------#
[32m[20230204 15:41:21 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:41:21 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:21 @agent_ppo2.py:194][0m |          -0.0001 |           9.3313 |           3.8007 |
[32m[20230204 15:41:21 @agent_ppo2.py:194][0m |          -0.0048 |           8.7552 |           3.7998 |
[32m[20230204 15:41:22 @agent_ppo2.py:194][0m |          -0.0061 |           8.4547 |           3.8004 |
[32m[20230204 15:41:22 @agent_ppo2.py:194][0m |          -0.0073 |           8.2688 |           3.7961 |
[32m[20230204 15:41:22 @agent_ppo2.py:194][0m |          -0.0082 |           8.1134 |           3.7984 |
[32m[20230204 15:41:22 @agent_ppo2.py:194][0m |          -0.0080 |           7.9557 |           3.7965 |
[32m[20230204 15:41:22 @agent_ppo2.py:194][0m |          -0.0093 |           7.8049 |           3.7972 |
[32m[20230204 15:41:22 @agent_ppo2.py:194][0m |          -0.0096 |           7.6353 |           3.7968 |
[32m[20230204 15:41:22 @agent_ppo2.py:194][0m |          -0.0104 |           7.5377 |           3.7948 |
[32m[20230204 15:41:22 @agent_ppo2.py:194][0m |          -0.0102 |           7.4521 |           3.7970 |
[32m[20230204 15:41:22 @agent_ppo2.py:139][0m Policy update time: 0.95 s
[32m[20230204 15:41:22 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.49
[32m[20230204 15:41:22 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.78
[32m[20230204 15:41:22 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 131.12
[32m[20230204 15:41:22 @agent_ppo2.py:152][0m Total time:       5.20 min
[32m[20230204 15:41:22 @agent_ppo2.py:154][0m 368640 total steps have happened
[32m[20230204 15:41:22 @agent_ppo2.py:130][0m #------------------------ Iteration 180 --------------------------#
[32m[20230204 15:41:23 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:41:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:23 @agent_ppo2.py:194][0m |          -0.0009 |          10.2642 |           3.7134 |
[32m[20230204 15:41:23 @agent_ppo2.py:194][0m |          -0.0044 |           9.9505 |           3.7127 |
[32m[20230204 15:41:23 @agent_ppo2.py:194][0m |          -0.0071 |           9.8069 |           3.7123 |
[32m[20230204 15:41:23 @agent_ppo2.py:194][0m |          -0.0052 |           9.7438 |           3.7140 |
[32m[20230204 15:41:23 @agent_ppo2.py:194][0m |          -0.0091 |           9.6171 |           3.7153 |
[32m[20230204 15:41:23 @agent_ppo2.py:194][0m |          -0.0093 |           9.5272 |           3.7169 |
[32m[20230204 15:41:23 @agent_ppo2.py:194][0m |          -0.0103 |           9.4244 |           3.7181 |
[32m[20230204 15:41:23 @agent_ppo2.py:194][0m |          -0.0080 |           9.4307 |           3.7197 |
[32m[20230204 15:41:23 @agent_ppo2.py:194][0m |          -0.0098 |           9.5936 |           3.7191 |
[32m[20230204 15:41:24 @agent_ppo2.py:194][0m |          -0.0138 |           9.2301 |           3.7242 |
[32m[20230204 15:41:24 @agent_ppo2.py:139][0m Policy update time: 0.97 s
[32m[20230204 15:41:24 @agent_ppo2.py:147][0m Average TRAINING episode reward: 253.98
[32m[20230204 15:41:24 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 256.54
[32m[20230204 15:41:24 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.23
[32m[20230204 15:41:24 @agent_ppo2.py:152][0m Total time:       5.22 min
[32m[20230204 15:41:24 @agent_ppo2.py:154][0m 370688 total steps have happened
[32m[20230204 15:41:24 @agent_ppo2.py:130][0m #------------------------ Iteration 181 --------------------------#
[32m[20230204 15:41:24 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:41:24 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:24 @agent_ppo2.py:194][0m |          -0.0027 |           9.7924 |           3.7278 |
[32m[20230204 15:41:24 @agent_ppo2.py:194][0m |          -0.0008 |           9.3346 |           3.7256 |
[32m[20230204 15:41:24 @agent_ppo2.py:194][0m |          -0.0083 |           9.0750 |           3.7214 |
[32m[20230204 15:41:24 @agent_ppo2.py:194][0m |           0.0009 |           8.9793 |           3.7210 |
[32m[20230204 15:41:25 @agent_ppo2.py:194][0m |          -0.0124 |           8.8368 |           3.7176 |
[32m[20230204 15:41:25 @agent_ppo2.py:194][0m |          -0.0123 |           8.6943 |           3.7183 |
[32m[20230204 15:41:25 @agent_ppo2.py:194][0m |          -0.0060 |           8.5261 |           3.7184 |
[32m[20230204 15:41:25 @agent_ppo2.py:194][0m |          -0.0177 |           8.4318 |           3.7157 |
[32m[20230204 15:41:25 @agent_ppo2.py:194][0m |          -0.0149 |           8.3625 |           3.7126 |
[32m[20230204 15:41:25 @agent_ppo2.py:194][0m |          -0.0116 |           8.2927 |           3.7148 |
[32m[20230204 15:41:25 @agent_ppo2.py:139][0m Policy update time: 0.97 s
[32m[20230204 15:41:25 @agent_ppo2.py:147][0m Average TRAINING episode reward: 254.62
[32m[20230204 15:41:25 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 257.37
[32m[20230204 15:41:25 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 149.55
[32m[20230204 15:41:25 @agent_ppo2.py:152][0m Total time:       5.25 min
[32m[20230204 15:41:25 @agent_ppo2.py:154][0m 372736 total steps have happened
[32m[20230204 15:41:25 @agent_ppo2.py:130][0m #------------------------ Iteration 182 --------------------------#
[32m[20230204 15:41:26 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:26 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:26 @agent_ppo2.py:194][0m |          -0.0013 |          14.7809 |           3.8133 |
[32m[20230204 15:41:26 @agent_ppo2.py:194][0m |          -0.0071 |          11.9332 |           3.8103 |
[32m[20230204 15:41:26 @agent_ppo2.py:194][0m |          -0.0087 |          10.9719 |           3.8083 |
[32m[20230204 15:41:26 @agent_ppo2.py:194][0m |          -0.0094 |          10.6222 |           3.8077 |
[32m[20230204 15:41:26 @agent_ppo2.py:194][0m |          -0.0111 |          10.2622 |           3.8043 |
[32m[20230204 15:41:26 @agent_ppo2.py:194][0m |          -0.0104 |          10.0496 |           3.8058 |
[32m[20230204 15:41:26 @agent_ppo2.py:194][0m |          -0.0116 |           9.9200 |           3.8033 |
[32m[20230204 15:41:26 @agent_ppo2.py:194][0m |          -0.0125 |           9.7572 |           3.8034 |
[32m[20230204 15:41:26 @agent_ppo2.py:194][0m |          -0.0130 |           9.6391 |           3.8003 |
[32m[20230204 15:41:26 @agent_ppo2.py:194][0m |          -0.0133 |           9.5176 |           3.7997 |
[32m[20230204 15:41:26 @agent_ppo2.py:139][0m Policy update time: 0.96 s
[32m[20230204 15:41:27 @agent_ppo2.py:147][0m Average TRAINING episode reward: 219.77
[32m[20230204 15:41:27 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 253.84
[32m[20230204 15:41:27 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 275.67
[32m[20230204 15:41:27 @agent_ppo2.py:152][0m Total time:       5.27 min
[32m[20230204 15:41:27 @agent_ppo2.py:154][0m 374784 total steps have happened
[32m[20230204 15:41:27 @agent_ppo2.py:130][0m #------------------------ Iteration 183 --------------------------#
[32m[20230204 15:41:27 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:41:27 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:27 @agent_ppo2.py:194][0m |           0.0012 |          12.3225 |           3.7686 |
[32m[20230204 15:41:27 @agent_ppo2.py:194][0m |          -0.0014 |           9.5504 |           3.7661 |
[32m[20230204 15:41:27 @agent_ppo2.py:194][0m |          -0.0033 |           9.2304 |           3.7659 |
[32m[20230204 15:41:27 @agent_ppo2.py:194][0m |          -0.0040 |           8.9820 |           3.7648 |
[32m[20230204 15:41:28 @agent_ppo2.py:194][0m |          -0.0050 |           8.7983 |           3.7635 |
[32m[20230204 15:41:28 @agent_ppo2.py:194][0m |          -0.0038 |           8.6489 |           3.7615 |
[32m[20230204 15:41:28 @agent_ppo2.py:194][0m |          -0.0051 |           8.5357 |           3.7632 |
[32m[20230204 15:41:28 @agent_ppo2.py:194][0m |          -0.0058 |           8.4390 |           3.7653 |
[32m[20230204 15:41:28 @agent_ppo2.py:194][0m |          -0.0059 |           8.3291 |           3.7636 |
[32m[20230204 15:41:28 @agent_ppo2.py:194][0m |          -0.0056 |           8.2427 |           3.7613 |
[32m[20230204 15:41:28 @agent_ppo2.py:139][0m Policy update time: 1.00 s
[32m[20230204 15:41:28 @agent_ppo2.py:147][0m Average TRAINING episode reward: 177.13
[32m[20230204 15:41:28 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 254.29
[32m[20230204 15:41:28 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.84
[32m[20230204 15:41:28 @agent_ppo2.py:152][0m Total time:       5.30 min
[32m[20230204 15:41:28 @agent_ppo2.py:154][0m 376832 total steps have happened
[32m[20230204 15:41:28 @agent_ppo2.py:130][0m #------------------------ Iteration 184 --------------------------#
[32m[20230204 15:41:28 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:29 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:29 @agent_ppo2.py:194][0m |          -0.0012 |          26.5505 |           3.8111 |
[32m[20230204 15:41:29 @agent_ppo2.py:194][0m |          -0.0063 |          19.7627 |           3.8035 |
[32m[20230204 15:41:29 @agent_ppo2.py:194][0m |          -0.0064 |          18.1213 |           3.8041 |
[32m[20230204 15:41:29 @agent_ppo2.py:194][0m |          -0.0072 |          16.9275 |           3.7978 |
[32m[20230204 15:41:29 @agent_ppo2.py:194][0m |          -0.0081 |          16.7151 |           3.7985 |
[32m[20230204 15:41:29 @agent_ppo2.py:194][0m |          -0.0073 |          15.6033 |           3.7950 |
[32m[20230204 15:41:29 @agent_ppo2.py:194][0m |          -0.0085 |          15.7198 |           3.7950 |
[32m[20230204 15:41:29 @agent_ppo2.py:194][0m |          -0.0114 |          14.8781 |           3.7962 |
[32m[20230204 15:41:29 @agent_ppo2.py:194][0m |          -0.0118 |          14.7215 |           3.7963 |
[32m[20230204 15:41:29 @agent_ppo2.py:194][0m |          -0.0098 |          14.6437 |           3.7960 |
[32m[20230204 15:41:29 @agent_ppo2.py:139][0m Policy update time: 0.91 s
[32m[20230204 15:41:30 @agent_ppo2.py:147][0m Average TRAINING episode reward: 143.79
[32m[20230204 15:41:30 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 256.31
[32m[20230204 15:41:30 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.31
[32m[20230204 15:41:30 @agent_ppo2.py:152][0m Total time:       5.32 min
[32m[20230204 15:41:30 @agent_ppo2.py:154][0m 378880 total steps have happened
[32m[20230204 15:41:30 @agent_ppo2.py:130][0m #------------------------ Iteration 185 --------------------------#
[32m[20230204 15:41:30 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:41:30 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:30 @agent_ppo2.py:194][0m |           0.0007 |          10.6067 |           3.8205 |
[32m[20230204 15:41:30 @agent_ppo2.py:194][0m |          -0.0051 |           9.7440 |           3.8144 |
[32m[20230204 15:41:30 @agent_ppo2.py:194][0m |          -0.0069 |           9.6215 |           3.8168 |
[32m[20230204 15:41:30 @agent_ppo2.py:194][0m |          -0.0075 |           9.3672 |           3.8165 |
[32m[20230204 15:41:31 @agent_ppo2.py:194][0m |          -0.0102 |           9.1948 |           3.8189 |
[32m[20230204 15:41:31 @agent_ppo2.py:194][0m |          -0.0086 |           9.1800 |           3.8208 |
[32m[20230204 15:41:31 @agent_ppo2.py:194][0m |          -0.0123 |           8.9767 |           3.8218 |
[32m[20230204 15:41:31 @agent_ppo2.py:194][0m |          -0.0118 |           8.9071 |           3.8230 |
[32m[20230204 15:41:31 @agent_ppo2.py:194][0m |          -0.0115 |           8.8161 |           3.8246 |
[32m[20230204 15:41:31 @agent_ppo2.py:194][0m |          -0.0098 |           8.7955 |           3.8271 |
[32m[20230204 15:41:31 @agent_ppo2.py:139][0m Policy update time: 1.03 s
[32m[20230204 15:41:31 @agent_ppo2.py:147][0m Average TRAINING episode reward: 247.00
[32m[20230204 15:41:31 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 250.12
[32m[20230204 15:41:31 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.92
[32m[20230204 15:41:31 @agent_ppo2.py:152][0m Total time:       5.35 min
[32m[20230204 15:41:31 @agent_ppo2.py:154][0m 380928 total steps have happened
[32m[20230204 15:41:31 @agent_ppo2.py:130][0m #------------------------ Iteration 186 --------------------------#
[32m[20230204 15:41:31 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:32 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:32 @agent_ppo2.py:194][0m |          -0.0009 |           9.2270 |           3.7537 |
[32m[20230204 15:41:32 @agent_ppo2.py:194][0m |          -0.0054 |           8.3756 |           3.7465 |
[32m[20230204 15:41:32 @agent_ppo2.py:194][0m |          -0.0048 |           8.1361 |           3.7493 |
[32m[20230204 15:41:32 @agent_ppo2.py:194][0m |          -0.0092 |           7.6751 |           3.7467 |
[32m[20230204 15:41:32 @agent_ppo2.py:194][0m |          -0.0100 |           7.4537 |           3.7505 |
[32m[20230204 15:41:32 @agent_ppo2.py:194][0m |          -0.0096 |           7.2201 |           3.7507 |
[32m[20230204 15:41:32 @agent_ppo2.py:194][0m |          -0.0102 |           7.1250 |           3.7467 |
[32m[20230204 15:41:32 @agent_ppo2.py:194][0m |          -0.0101 |           7.0353 |           3.7503 |
[32m[20230204 15:41:32 @agent_ppo2.py:194][0m |          -0.0089 |           6.9276 |           3.7515 |
[32m[20230204 15:41:32 @agent_ppo2.py:194][0m |          -0.0104 |           6.6886 |           3.7511 |
[32m[20230204 15:41:32 @agent_ppo2.py:139][0m Policy update time: 0.96 s
[32m[20230204 15:41:33 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.75
[32m[20230204 15:41:33 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.92
[32m[20230204 15:41:33 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 274.19
[32m[20230204 15:41:33 @agent_ppo2.py:152][0m Total time:       5.37 min
[32m[20230204 15:41:33 @agent_ppo2.py:154][0m 382976 total steps have happened
[32m[20230204 15:41:33 @agent_ppo2.py:130][0m #------------------------ Iteration 187 --------------------------#
[32m[20230204 15:41:33 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:33 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:33 @agent_ppo2.py:194][0m |           0.0019 |          14.5511 |           3.9300 |
[32m[20230204 15:41:33 @agent_ppo2.py:194][0m |          -0.0029 |          11.4911 |           3.9209 |
[32m[20230204 15:41:33 @agent_ppo2.py:194][0m |          -0.0053 |          10.1042 |           3.9156 |
[32m[20230204 15:41:33 @agent_ppo2.py:194][0m |          -0.0069 |           9.3739 |           3.9165 |
[32m[20230204 15:41:33 @agent_ppo2.py:194][0m |          -0.0068 |           8.9824 |           3.9128 |
[32m[20230204 15:41:34 @agent_ppo2.py:194][0m |          -0.0091 |           8.6001 |           3.9117 |
[32m[20230204 15:41:34 @agent_ppo2.py:194][0m |          -0.0097 |           8.3590 |           3.9096 |
[32m[20230204 15:41:34 @agent_ppo2.py:194][0m |          -0.0095 |           8.2018 |           3.9110 |
[32m[20230204 15:41:34 @agent_ppo2.py:194][0m |          -0.0099 |           7.9965 |           3.9083 |
[32m[20230204 15:41:34 @agent_ppo2.py:194][0m |          -0.0106 |           7.8537 |           3.9088 |
[32m[20230204 15:41:34 @agent_ppo2.py:139][0m Policy update time: 0.97 s
[32m[20230204 15:41:34 @agent_ppo2.py:147][0m Average TRAINING episode reward: 214.37
[32m[20230204 15:41:34 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 254.63
[32m[20230204 15:41:34 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.31
[32m[20230204 15:41:34 @agent_ppo2.py:152][0m Total time:       5.40 min
[32m[20230204 15:41:34 @agent_ppo2.py:154][0m 385024 total steps have happened
[32m[20230204 15:41:34 @agent_ppo2.py:130][0m #------------------------ Iteration 188 --------------------------#
[32m[20230204 15:41:34 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:41:35 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:35 @agent_ppo2.py:194][0m |           0.0001 |          13.1558 |           3.8389 |
[32m[20230204 15:41:35 @agent_ppo2.py:194][0m |          -0.0051 |          11.6172 |           3.8334 |
[32m[20230204 15:41:35 @agent_ppo2.py:194][0m |          -0.0074 |          11.1198 |           3.8344 |
[32m[20230204 15:41:35 @agent_ppo2.py:194][0m |          -0.0091 |          10.8251 |           3.8357 |
[32m[20230204 15:41:35 @agent_ppo2.py:194][0m |          -0.0098 |          10.6073 |           3.8325 |
[32m[20230204 15:41:35 @agent_ppo2.py:194][0m |          -0.0110 |          10.4336 |           3.8346 |
[32m[20230204 15:41:35 @agent_ppo2.py:194][0m |          -0.0114 |          10.2763 |           3.8342 |
[32m[20230204 15:41:35 @agent_ppo2.py:194][0m |          -0.0117 |          10.1370 |           3.8338 |
[32m[20230204 15:41:35 @agent_ppo2.py:194][0m |          -0.0132 |          10.0742 |           3.8369 |
[32m[20230204 15:41:36 @agent_ppo2.py:194][0m |          -0.0132 |           9.9529 |           3.8348 |
[32m[20230204 15:41:36 @agent_ppo2.py:139][0m Policy update time: 1.03 s
[32m[20230204 15:41:36 @agent_ppo2.py:147][0m Average TRAINING episode reward: 247.22
[32m[20230204 15:41:36 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 252.48
[32m[20230204 15:41:36 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.15
[32m[20230204 15:41:36 @agent_ppo2.py:152][0m Total time:       5.42 min
[32m[20230204 15:41:36 @agent_ppo2.py:154][0m 387072 total steps have happened
[32m[20230204 15:41:36 @agent_ppo2.py:130][0m #------------------------ Iteration 189 --------------------------#
[32m[20230204 15:41:36 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:41:36 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:36 @agent_ppo2.py:194][0m |           0.0018 |           8.3595 |           3.7474 |
[32m[20230204 15:41:36 @agent_ppo2.py:194][0m |          -0.0019 |           7.5259 |           3.7411 |
[32m[20230204 15:41:36 @agent_ppo2.py:194][0m |          -0.0018 |           7.2747 |           3.7383 |
[32m[20230204 15:41:36 @agent_ppo2.py:194][0m |          -0.0034 |           7.1034 |           3.7380 |
[32m[20230204 15:41:37 @agent_ppo2.py:194][0m |          -0.0052 |           6.8782 |           3.7357 |
[32m[20230204 15:41:37 @agent_ppo2.py:194][0m |          -0.0061 |           6.6932 |           3.7344 |
[32m[20230204 15:41:37 @agent_ppo2.py:194][0m |          -0.0071 |           6.5590 |           3.7343 |
[32m[20230204 15:41:37 @agent_ppo2.py:194][0m |          -0.0072 |           6.3896 |           3.7361 |
[32m[20230204 15:41:37 @agent_ppo2.py:194][0m |          -0.0071 |           6.2183 |           3.7337 |
[32m[20230204 15:41:37 @agent_ppo2.py:194][0m |          -0.0080 |           6.0884 |           3.7357 |
[32m[20230204 15:41:37 @agent_ppo2.py:139][0m Policy update time: 0.95 s
[32m[20230204 15:41:37 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.74
[32m[20230204 15:41:37 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.55
[32m[20230204 15:41:37 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.46
[32m[20230204 15:41:37 @agent_ppo2.py:152][0m Total time:       5.45 min
[32m[20230204 15:41:37 @agent_ppo2.py:154][0m 389120 total steps have happened
[32m[20230204 15:41:37 @agent_ppo2.py:130][0m #------------------------ Iteration 190 --------------------------#
[32m[20230204 15:41:37 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:41:38 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:38 @agent_ppo2.py:194][0m |           0.0008 |          10.1247 |           3.8848 |
[32m[20230204 15:41:38 @agent_ppo2.py:194][0m |          -0.0068 |           9.2450 |           3.8784 |
[32m[20230204 15:41:38 @agent_ppo2.py:194][0m |          -0.0096 |           8.9915 |           3.8723 |
[32m[20230204 15:41:38 @agent_ppo2.py:194][0m |          -0.0086 |           8.8169 |           3.8713 |
[32m[20230204 15:41:38 @agent_ppo2.py:194][0m |          -0.0105 |           8.6182 |           3.8720 |
[32m[20230204 15:41:38 @agent_ppo2.py:194][0m |          -0.0110 |           8.4869 |           3.8692 |
[32m[20230204 15:41:38 @agent_ppo2.py:194][0m |          -0.0101 |           8.4168 |           3.8718 |
[32m[20230204 15:41:38 @agent_ppo2.py:194][0m |          -0.0123 |           8.2632 |           3.8665 |
[32m[20230204 15:41:38 @agent_ppo2.py:194][0m |          -0.0111 |           8.2539 |           3.8704 |
[32m[20230204 15:41:38 @agent_ppo2.py:194][0m |          -0.0123 |           8.1157 |           3.8705 |
[32m[20230204 15:41:38 @agent_ppo2.py:139][0m Policy update time: 1.02 s
[32m[20230204 15:41:39 @agent_ppo2.py:147][0m Average TRAINING episode reward: 249.00
[32m[20230204 15:41:39 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 251.08
[32m[20230204 15:41:39 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 150.09
[32m[20230204 15:41:39 @agent_ppo2.py:152][0m Total time:       5.47 min
[32m[20230204 15:41:39 @agent_ppo2.py:154][0m 391168 total steps have happened
[32m[20230204 15:41:39 @agent_ppo2.py:130][0m #------------------------ Iteration 191 --------------------------#
[32m[20230204 15:41:39 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:39 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:39 @agent_ppo2.py:194][0m |           0.0017 |           9.7195 |           3.8648 |
[32m[20230204 15:41:39 @agent_ppo2.py:194][0m |          -0.0025 |           8.8120 |           3.8583 |
[32m[20230204 15:41:39 @agent_ppo2.py:194][0m |          -0.0056 |           8.5113 |           3.8597 |
[32m[20230204 15:41:39 @agent_ppo2.py:194][0m |          -0.0058 |           8.3256 |           3.8627 |
[32m[20230204 15:41:40 @agent_ppo2.py:194][0m |          -0.0065 |           8.1630 |           3.8603 |
[32m[20230204 15:41:40 @agent_ppo2.py:194][0m |          -0.0058 |           8.0577 |           3.8563 |
[32m[20230204 15:41:40 @agent_ppo2.py:194][0m |          -0.0064 |           8.0100 |           3.8564 |
[32m[20230204 15:41:40 @agent_ppo2.py:194][0m |          -0.0086 |           7.8746 |           3.8581 |
[32m[20230204 15:41:40 @agent_ppo2.py:194][0m |          -0.0083 |           7.7542 |           3.8599 |
[32m[20230204 15:41:40 @agent_ppo2.py:194][0m |          -0.0068 |           7.8014 |           3.8563 |
[32m[20230204 15:41:40 @agent_ppo2.py:139][0m Policy update time: 0.98 s
[32m[20230204 15:41:40 @agent_ppo2.py:147][0m Average TRAINING episode reward: 253.50
[32m[20230204 15:41:40 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.00
[32m[20230204 15:41:40 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.29
[32m[20230204 15:41:40 @agent_ppo2.py:152][0m Total time:       5.50 min
[32m[20230204 15:41:40 @agent_ppo2.py:154][0m 393216 total steps have happened
[32m[20230204 15:41:40 @agent_ppo2.py:130][0m #------------------------ Iteration 192 --------------------------#
[32m[20230204 15:41:40 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:41:41 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:41 @agent_ppo2.py:194][0m |          -0.0013 |          10.4733 |           3.8823 |
[32m[20230204 15:41:41 @agent_ppo2.py:194][0m |          -0.0020 |          10.1393 |           3.8730 |
[32m[20230204 15:41:41 @agent_ppo2.py:194][0m |          -0.0086 |           9.5458 |           3.8673 |
[32m[20230204 15:41:41 @agent_ppo2.py:194][0m |          -0.0062 |           9.5598 |           3.8648 |
[32m[20230204 15:41:41 @agent_ppo2.py:194][0m |          -0.0081 |           9.3012 |           3.8668 |
[32m[20230204 15:41:41 @agent_ppo2.py:194][0m |          -0.0054 |           9.2008 |           3.8680 |
[32m[20230204 15:41:41 @agent_ppo2.py:194][0m |          -0.0106 |           8.8749 |           3.8692 |
[32m[20230204 15:41:41 @agent_ppo2.py:194][0m |          -0.0088 |           8.7104 |           3.8724 |
[32m[20230204 15:41:41 @agent_ppo2.py:194][0m |          -0.0101 |           8.5659 |           3.8703 |
[32m[20230204 15:41:41 @agent_ppo2.py:194][0m |          -0.0115 |           8.4955 |           3.8714 |
[32m[20230204 15:41:41 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:41:42 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.23
[32m[20230204 15:41:42 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.51
[32m[20230204 15:41:42 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.82
[32m[20230204 15:41:42 @agent_ppo2.py:152][0m Total time:       5.52 min
[32m[20230204 15:41:42 @agent_ppo2.py:154][0m 395264 total steps have happened
[32m[20230204 15:41:42 @agent_ppo2.py:130][0m #------------------------ Iteration 193 --------------------------#
[32m[20230204 15:41:42 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:41:42 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:42 @agent_ppo2.py:194][0m |           0.0011 |          11.0812 |           3.8669 |
[32m[20230204 15:41:42 @agent_ppo2.py:194][0m |          -0.0051 |          10.3471 |           3.8549 |
[32m[20230204 15:41:42 @agent_ppo2.py:194][0m |          -0.0056 |          10.2885 |           3.8532 |
[32m[20230204 15:41:42 @agent_ppo2.py:194][0m |          -0.0049 |          10.2593 |           3.8502 |
[32m[20230204 15:41:42 @agent_ppo2.py:194][0m |          -0.0103 |           9.8261 |           3.8528 |
[32m[20230204 15:41:43 @agent_ppo2.py:194][0m |          -0.0088 |           9.6940 |           3.8526 |
[32m[20230204 15:41:43 @agent_ppo2.py:194][0m |          -0.0078 |           9.7963 |           3.8487 |
[32m[20230204 15:41:43 @agent_ppo2.py:194][0m |          -0.0116 |           9.5503 |           3.8530 |
[32m[20230204 15:41:43 @agent_ppo2.py:194][0m |          -0.0113 |           9.5300 |           3.8537 |
[32m[20230204 15:41:43 @agent_ppo2.py:194][0m |          -0.0111 |           9.4906 |           3.8512 |
[32m[20230204 15:41:43 @agent_ppo2.py:139][0m Policy update time: 1.00 s
[32m[20230204 15:41:43 @agent_ppo2.py:147][0m Average TRAINING episode reward: 249.67
[32m[20230204 15:41:43 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 253.01
[32m[20230204 15:41:43 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.32
[32m[20230204 15:41:43 @agent_ppo2.py:152][0m Total time:       5.54 min
[32m[20230204 15:41:43 @agent_ppo2.py:154][0m 397312 total steps have happened
[32m[20230204 15:41:43 @agent_ppo2.py:130][0m #------------------------ Iteration 194 --------------------------#
[32m[20230204 15:41:43 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:41:43 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:44 @agent_ppo2.py:194][0m |           0.0008 |          10.3408 |           4.0361 |
[32m[20230204 15:41:44 @agent_ppo2.py:194][0m |          -0.0035 |           9.7419 |           4.0352 |
[32m[20230204 15:41:44 @agent_ppo2.py:194][0m |          -0.0046 |           9.4692 |           4.0349 |
[32m[20230204 15:41:44 @agent_ppo2.py:194][0m |          -0.0054 |           9.2948 |           4.0349 |
[32m[20230204 15:41:44 @agent_ppo2.py:194][0m |          -0.0068 |           9.1392 |           4.0338 |
[32m[20230204 15:41:44 @agent_ppo2.py:194][0m |          -0.0078 |           8.9872 |           4.0385 |
[32m[20230204 15:41:44 @agent_ppo2.py:194][0m |          -0.0075 |           8.8532 |           4.0345 |
[32m[20230204 15:41:44 @agent_ppo2.py:194][0m |          -0.0087 |           8.7670 |           4.0406 |
[32m[20230204 15:41:44 @agent_ppo2.py:194][0m |          -0.0091 |           8.6824 |           4.0369 |
[32m[20230204 15:41:44 @agent_ppo2.py:194][0m |          -0.0093 |           8.6274 |           4.0416 |
[32m[20230204 15:41:44 @agent_ppo2.py:139][0m Policy update time: 0.97 s
[32m[20230204 15:41:45 @agent_ppo2.py:147][0m Average TRAINING episode reward: 252.09
[32m[20230204 15:41:45 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 254.83
[32m[20230204 15:41:45 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.22
[32m[20230204 15:41:45 @agent_ppo2.py:152][0m Total time:       5.57 min
[32m[20230204 15:41:45 @agent_ppo2.py:154][0m 399360 total steps have happened
[32m[20230204 15:41:45 @agent_ppo2.py:130][0m #------------------------ Iteration 195 --------------------------#
[32m[20230204 15:41:45 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:45 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:45 @agent_ppo2.py:194][0m |           0.0078 |          11.7780 |           3.8959 |
[32m[20230204 15:41:45 @agent_ppo2.py:194][0m |          -0.0059 |          10.1490 |           3.8917 |
[32m[20230204 15:41:45 @agent_ppo2.py:194][0m |          -0.0100 |           9.8255 |           3.8888 |
[32m[20230204 15:41:45 @agent_ppo2.py:194][0m |          -0.0046 |           9.7089 |           3.8871 |
[32m[20230204 15:41:45 @agent_ppo2.py:194][0m |          -0.0102 |           9.6309 |           3.8853 |
[32m[20230204 15:41:46 @agent_ppo2.py:194][0m |          -0.0123 |           9.5071 |           3.8863 |
[32m[20230204 15:41:46 @agent_ppo2.py:194][0m |          -0.0120 |           9.3921 |           3.8851 |
[32m[20230204 15:41:46 @agent_ppo2.py:194][0m |          -0.0128 |           9.3522 |           3.8875 |
[32m[20230204 15:41:46 @agent_ppo2.py:194][0m |          -0.0104 |           9.3134 |           3.8871 |
[32m[20230204 15:41:46 @agent_ppo2.py:194][0m |          -0.0103 |           9.2285 |           3.8847 |
[32m[20230204 15:41:46 @agent_ppo2.py:139][0m Policy update time: 0.99 s
[32m[20230204 15:41:46 @agent_ppo2.py:147][0m Average TRAINING episode reward: 250.69
[32m[20230204 15:41:46 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 253.05
[32m[20230204 15:41:46 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.94
[32m[20230204 15:41:46 @agent_ppo2.py:152][0m Total time:       5.59 min
[32m[20230204 15:41:46 @agent_ppo2.py:154][0m 401408 total steps have happened
[32m[20230204 15:41:46 @agent_ppo2.py:130][0m #------------------------ Iteration 196 --------------------------#
[32m[20230204 15:41:46 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:41:46 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:46 @agent_ppo2.py:194][0m |          -0.0022 |           9.8058 |           3.9951 |
[32m[20230204 15:41:47 @agent_ppo2.py:194][0m |          -0.0060 |           9.2332 |           3.9813 |
[32m[20230204 15:41:47 @agent_ppo2.py:194][0m |          -0.0094 |           8.9309 |           3.9764 |
[32m[20230204 15:41:47 @agent_ppo2.py:194][0m |          -0.0100 |           8.7127 |           3.9779 |
[32m[20230204 15:41:47 @agent_ppo2.py:194][0m |          -0.0094 |           8.5899 |           3.9785 |
[32m[20230204 15:41:47 @agent_ppo2.py:194][0m |          -0.0109 |           8.4945 |           3.9758 |
[32m[20230204 15:41:47 @agent_ppo2.py:194][0m |          -0.0135 |           8.2870 |           3.9740 |
[32m[20230204 15:41:47 @agent_ppo2.py:194][0m |          -0.0137 |           8.1622 |           3.9762 |
[32m[20230204 15:41:47 @agent_ppo2.py:194][0m |          -0.0133 |           7.9993 |           3.9760 |
[32m[20230204 15:41:47 @agent_ppo2.py:194][0m |          -0.0134 |           7.9123 |           3.9784 |
[32m[20230204 15:41:47 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:41:47 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.84
[32m[20230204 15:41:47 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.60
[32m[20230204 15:41:47 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.74
[32m[20230204 15:41:47 @agent_ppo2.py:152][0m Total time:       5.62 min
[32m[20230204 15:41:47 @agent_ppo2.py:154][0m 403456 total steps have happened
[32m[20230204 15:41:47 @agent_ppo2.py:130][0m #------------------------ Iteration 197 --------------------------#
[32m[20230204 15:41:48 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:41:48 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:48 @agent_ppo2.py:194][0m |           0.0001 |          10.7050 |           4.0408 |
[32m[20230204 15:41:48 @agent_ppo2.py:194][0m |          -0.0066 |           8.1143 |           4.0358 |
[32m[20230204 15:41:48 @agent_ppo2.py:194][0m |          -0.0084 |           7.5057 |           4.0288 |
[32m[20230204 15:41:48 @agent_ppo2.py:194][0m |          -0.0087 |           7.2472 |           4.0278 |
[32m[20230204 15:41:48 @agent_ppo2.py:194][0m |          -0.0092 |           7.0510 |           4.0268 |
[32m[20230204 15:41:48 @agent_ppo2.py:194][0m |          -0.0114 |           6.7229 |           4.0247 |
[32m[20230204 15:41:48 @agent_ppo2.py:194][0m |          -0.0119 |           6.5266 |           4.0256 |
[32m[20230204 15:41:49 @agent_ppo2.py:194][0m |          -0.0126 |           6.3627 |           4.0217 |
[32m[20230204 15:41:49 @agent_ppo2.py:194][0m |          -0.0128 |           6.1635 |           4.0189 |
[32m[20230204 15:41:49 @agent_ppo2.py:194][0m |          -0.0124 |           6.0036 |           4.0198 |
[32m[20230204 15:41:49 @agent_ppo2.py:139][0m Policy update time: 0.98 s
[32m[20230204 15:41:49 @agent_ppo2.py:147][0m Average TRAINING episode reward: 249.73
[32m[20230204 15:41:49 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 256.92
[32m[20230204 15:41:49 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.49
[32m[20230204 15:41:49 @agent_ppo2.py:152][0m Total time:       5.64 min
[32m[20230204 15:41:49 @agent_ppo2.py:154][0m 405504 total steps have happened
[32m[20230204 15:41:49 @agent_ppo2.py:130][0m #------------------------ Iteration 198 --------------------------#
[32m[20230204 15:41:49 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:41:49 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:49 @agent_ppo2.py:194][0m |          -0.0057 |          19.9592 |           3.9746 |
[32m[20230204 15:41:49 @agent_ppo2.py:194][0m |          -0.0002 |          15.0047 |           3.9682 |
[32m[20230204 15:41:50 @agent_ppo2.py:194][0m |          -0.0107 |          13.8953 |           3.9598 |
[32m[20230204 15:41:50 @agent_ppo2.py:194][0m |          -0.0107 |          12.6876 |           3.9573 |
[32m[20230204 15:41:50 @agent_ppo2.py:194][0m |          -0.0102 |          12.0782 |           3.9556 |
[32m[20230204 15:41:50 @agent_ppo2.py:194][0m |           0.0064 |          13.0959 |           3.9565 |
[32m[20230204 15:41:50 @agent_ppo2.py:194][0m |          -0.0092 |          11.3734 |           3.9478 |
[32m[20230204 15:41:50 @agent_ppo2.py:194][0m |          -0.0130 |          10.6250 |           3.9507 |
[32m[20230204 15:41:50 @agent_ppo2.py:194][0m |          -0.0125 |          10.7805 |           3.9496 |
[32m[20230204 15:41:50 @agent_ppo2.py:194][0m |          -0.0135 |          10.3299 |           3.9498 |
[32m[20230204 15:41:50 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:41:50 @agent_ppo2.py:147][0m Average TRAINING episode reward: 205.30
[32m[20230204 15:41:50 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 254.89
[32m[20230204 15:41:50 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.47
[32m[20230204 15:41:50 @agent_ppo2.py:152][0m Total time:       5.66 min
[32m[20230204 15:41:50 @agent_ppo2.py:154][0m 407552 total steps have happened
[32m[20230204 15:41:50 @agent_ppo2.py:130][0m #------------------------ Iteration 199 --------------------------#
[32m[20230204 15:41:51 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:41:51 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:51 @agent_ppo2.py:194][0m |           0.0012 |          25.0093 |           3.9138 |
[32m[20230204 15:41:51 @agent_ppo2.py:194][0m |           0.0148 |          24.2628 |           3.9098 |
[32m[20230204 15:41:51 @agent_ppo2.py:194][0m |           0.0192 |          22.2935 |           3.8980 |
[32m[20230204 15:41:51 @agent_ppo2.py:194][0m |          -0.0098 |          18.4864 |           3.9011 |
[32m[20230204 15:41:51 @agent_ppo2.py:194][0m |          -0.0095 |          17.6794 |           3.8983 |
[32m[20230204 15:41:51 @agent_ppo2.py:194][0m |          -0.0135 |          17.0070 |           3.8993 |
[32m[20230204 15:41:51 @agent_ppo2.py:194][0m |           0.0063 |          20.0622 |           3.8984 |
[32m[20230204 15:41:51 @agent_ppo2.py:194][0m |          -0.0122 |          16.4652 |           3.8913 |
[32m[20230204 15:41:51 @agent_ppo2.py:194][0m |          -0.0013 |          16.6484 |           3.8918 |
[32m[20230204 15:41:51 @agent_ppo2.py:194][0m |          -0.0115 |          16.0203 |           3.8957 |
[32m[20230204 15:41:51 @agent_ppo2.py:139][0m Policy update time: 0.82 s
[32m[20230204 15:41:52 @agent_ppo2.py:147][0m Average TRAINING episode reward: 193.71
[32m[20230204 15:41:52 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.75
[32m[20230204 15:41:52 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.76
[32m[20230204 15:41:52 @agent_ppo2.py:152][0m Total time:       5.69 min
[32m[20230204 15:41:52 @agent_ppo2.py:154][0m 409600 total steps have happened
[32m[20230204 15:41:52 @agent_ppo2.py:130][0m #------------------------ Iteration 200 --------------------------#
[32m[20230204 15:41:52 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:52 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:52 @agent_ppo2.py:194][0m |           0.0048 |          35.5784 |           3.8560 |
[32m[20230204 15:41:52 @agent_ppo2.py:194][0m |          -0.0049 |          26.5338 |           3.8489 |
[32m[20230204 15:41:52 @agent_ppo2.py:194][0m |          -0.0067 |          21.0538 |           3.8449 |
[32m[20230204 15:41:52 @agent_ppo2.py:194][0m |          -0.0108 |          19.0418 |           3.8424 |
[32m[20230204 15:41:52 @agent_ppo2.py:194][0m |          -0.0393 |          18.1324 |           3.8384 |
[32m[20230204 15:41:52 @agent_ppo2.py:194][0m |          -0.0087 |          17.0915 |           3.8378 |
[32m[20230204 15:41:53 @agent_ppo2.py:194][0m |          -0.0107 |          16.1744 |           3.8346 |
[32m[20230204 15:41:53 @agent_ppo2.py:194][0m |          -0.0110 |          15.7938 |           3.8359 |
[32m[20230204 15:41:53 @agent_ppo2.py:194][0m |          -0.0118 |          15.2276 |           3.8338 |
[32m[20230204 15:41:53 @agent_ppo2.py:194][0m |          -0.0010 |          17.6195 |           3.8321 |
[32m[20230204 15:41:53 @agent_ppo2.py:139][0m Policy update time: 0.96 s
[32m[20230204 15:41:53 @agent_ppo2.py:147][0m Average TRAINING episode reward: 252.26
[32m[20230204 15:41:53 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.15
[32m[20230204 15:41:53 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 15.67
[32m[20230204 15:41:53 @agent_ppo2.py:152][0m Total time:       5.71 min
[32m[20230204 15:41:53 @agent_ppo2.py:154][0m 411648 total steps have happened
[32m[20230204 15:41:53 @agent_ppo2.py:130][0m #------------------------ Iteration 201 --------------------------#
[32m[20230204 15:41:53 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:41:53 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:53 @agent_ppo2.py:194][0m |          -0.0003 |          23.2275 |           3.8562 |
[32m[20230204 15:41:54 @agent_ppo2.py:194][0m |          -0.0063 |          17.8455 |           3.8560 |
[32m[20230204 15:41:54 @agent_ppo2.py:194][0m |          -0.0073 |          16.4361 |           3.8540 |
[32m[20230204 15:41:54 @agent_ppo2.py:194][0m |          -0.0098 |          15.5685 |           3.8519 |
[32m[20230204 15:41:54 @agent_ppo2.py:194][0m |          -0.0107 |          15.2411 |           3.8504 |
[32m[20230204 15:41:54 @agent_ppo2.py:194][0m |          -0.0114 |          14.7234 |           3.8491 |
[32m[20230204 15:41:54 @agent_ppo2.py:194][0m |          -0.0121 |          14.4092 |           3.8488 |
[32m[20230204 15:41:54 @agent_ppo2.py:194][0m |          -0.0120 |          14.1549 |           3.8481 |
[32m[20230204 15:41:54 @agent_ppo2.py:194][0m |          -0.0133 |          13.8122 |           3.8470 |
[32m[20230204 15:41:54 @agent_ppo2.py:194][0m |          -0.0143 |          13.6024 |           3.8476 |
[32m[20230204 15:41:54 @agent_ppo2.py:139][0m Policy update time: 1.03 s
[32m[20230204 15:41:55 @agent_ppo2.py:147][0m Average TRAINING episode reward: 198.90
[32m[20230204 15:41:55 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.66
[32m[20230204 15:41:55 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.00
[32m[20230204 15:41:55 @agent_ppo2.py:152][0m Total time:       5.73 min
[32m[20230204 15:41:55 @agent_ppo2.py:154][0m 413696 total steps have happened
[32m[20230204 15:41:55 @agent_ppo2.py:130][0m #------------------------ Iteration 202 --------------------------#
[32m[20230204 15:41:55 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:41:55 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:55 @agent_ppo2.py:194][0m |          -0.0004 |          10.6680 |           4.0108 |
[32m[20230204 15:41:55 @agent_ppo2.py:194][0m |          -0.0036 |           9.6752 |           4.0000 |
[32m[20230204 15:41:55 @agent_ppo2.py:194][0m |          -0.0049 |           9.1683 |           3.9997 |
[32m[20230204 15:41:55 @agent_ppo2.py:194][0m |          -0.0069 |           8.7598 |           3.9928 |
[32m[20230204 15:41:55 @agent_ppo2.py:194][0m |          -0.0083 |           8.4675 |           3.9981 |
[32m[20230204 15:41:55 @agent_ppo2.py:194][0m |          -0.0077 |           8.2683 |           3.9970 |
[32m[20230204 15:41:55 @agent_ppo2.py:194][0m |          -0.0088 |           8.0526 |           3.9912 |
[32m[20230204 15:41:56 @agent_ppo2.py:194][0m |          -0.0086 |           7.9318 |           3.9952 |
[32m[20230204 15:41:56 @agent_ppo2.py:194][0m |          -0.0090 |           7.8611 |           3.9935 |
[32m[20230204 15:41:56 @agent_ppo2.py:194][0m |          -0.0098 |           7.7137 |           3.9909 |
[32m[20230204 15:41:56 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:41:56 @agent_ppo2.py:147][0m Average TRAINING episode reward: 254.64
[32m[20230204 15:41:56 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 257.42
[32m[20230204 15:41:56 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 97.78
[32m[20230204 15:41:56 @agent_ppo2.py:152][0m Total time:       5.76 min
[32m[20230204 15:41:56 @agent_ppo2.py:154][0m 415744 total steps have happened
[32m[20230204 15:41:56 @agent_ppo2.py:130][0m #------------------------ Iteration 203 --------------------------#
[32m[20230204 15:41:56 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:41:56 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:56 @agent_ppo2.py:194][0m |           0.0014 |          12.2872 |           3.9444 |
[32m[20230204 15:41:56 @agent_ppo2.py:194][0m |          -0.0033 |          11.7358 |           3.9387 |
[32m[20230204 15:41:57 @agent_ppo2.py:194][0m |          -0.0054 |          11.5277 |           3.9405 |
[32m[20230204 15:41:57 @agent_ppo2.py:194][0m |          -0.0062 |          11.3914 |           3.9372 |
[32m[20230204 15:41:57 @agent_ppo2.py:194][0m |          -0.0077 |          11.2169 |           3.9380 |
[32m[20230204 15:41:57 @agent_ppo2.py:194][0m |          -0.0074 |          11.1428 |           3.9418 |
[32m[20230204 15:41:57 @agent_ppo2.py:194][0m |          -0.0082 |          10.9898 |           3.9405 |
[32m[20230204 15:41:57 @agent_ppo2.py:194][0m |          -0.0086 |          10.9600 |           3.9438 |
[32m[20230204 15:41:57 @agent_ppo2.py:194][0m |          -0.0093 |          10.8193 |           3.9422 |
[32m[20230204 15:41:57 @agent_ppo2.py:194][0m |          -0.0100 |          10.7393 |           3.9438 |
[32m[20230204 15:41:57 @agent_ppo2.py:139][0m Policy update time: 0.95 s
[32m[20230204 15:41:57 @agent_ppo2.py:147][0m Average TRAINING episode reward: 252.60
[32m[20230204 15:41:57 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 255.55
[32m[20230204 15:41:57 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.44
[32m[20230204 15:41:57 @agent_ppo2.py:152][0m Total time:       5.78 min
[32m[20230204 15:41:57 @agent_ppo2.py:154][0m 417792 total steps have happened
[32m[20230204 15:41:57 @agent_ppo2.py:130][0m #------------------------ Iteration 204 --------------------------#
[32m[20230204 15:41:58 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:41:58 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:58 @agent_ppo2.py:194][0m |           0.0003 |          11.3299 |           3.9876 |
[32m[20230204 15:41:58 @agent_ppo2.py:194][0m |          -0.0051 |          10.6767 |           3.9831 |
[32m[20230204 15:41:58 @agent_ppo2.py:194][0m |          -0.0062 |          10.4645 |           3.9823 |
[32m[20230204 15:41:58 @agent_ppo2.py:194][0m |          -0.0080 |          10.3242 |           3.9839 |
[32m[20230204 15:41:58 @agent_ppo2.py:194][0m |          -0.0087 |          10.1810 |           3.9873 |
[32m[20230204 15:41:58 @agent_ppo2.py:194][0m |          -0.0086 |          10.0706 |           3.9862 |
[32m[20230204 15:41:58 @agent_ppo2.py:194][0m |          -0.0093 |           9.9642 |           3.9910 |
[32m[20230204 15:41:58 @agent_ppo2.py:194][0m |          -0.0104 |           9.8527 |           3.9934 |
[32m[20230204 15:41:58 @agent_ppo2.py:194][0m |          -0.0114 |           9.7615 |           3.9970 |
[32m[20230204 15:41:59 @agent_ppo2.py:194][0m |          -0.0111 |           9.6636 |           3.9984 |
[32m[20230204 15:41:59 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:41:59 @agent_ppo2.py:147][0m Average TRAINING episode reward: 253.41
[32m[20230204 15:41:59 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.38
[32m[20230204 15:41:59 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 0.68
[32m[20230204 15:41:59 @agent_ppo2.py:152][0m Total time:       5.80 min
[32m[20230204 15:41:59 @agent_ppo2.py:154][0m 419840 total steps have happened
[32m[20230204 15:41:59 @agent_ppo2.py:130][0m #------------------------ Iteration 205 --------------------------#
[32m[20230204 15:41:59 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:41:59 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:41:59 @agent_ppo2.py:194][0m |          -0.0026 |          24.9968 |           4.0264 |
[32m[20230204 15:41:59 @agent_ppo2.py:194][0m |          -0.0077 |          15.4510 |           4.0230 |
[32m[20230204 15:41:59 @agent_ppo2.py:194][0m |          -0.0097 |          13.5365 |           4.0207 |
[32m[20230204 15:42:00 @agent_ppo2.py:194][0m |          -0.0071 |          12.6498 |           4.0136 |
[32m[20230204 15:42:00 @agent_ppo2.py:194][0m |          -0.0111 |          12.2111 |           4.0153 |
[32m[20230204 15:42:00 @agent_ppo2.py:194][0m |          -0.0120 |          11.9596 |           4.0136 |
[32m[20230204 15:42:00 @agent_ppo2.py:194][0m |          -0.0127 |          11.8287 |           4.0092 |
[32m[20230204 15:42:00 @agent_ppo2.py:194][0m |          -0.0103 |          11.6621 |           4.0090 |
[32m[20230204 15:42:00 @agent_ppo2.py:194][0m |          -0.0134 |          11.6132 |           4.0092 |
[32m[20230204 15:42:00 @agent_ppo2.py:194][0m |          -0.0135 |          11.3510 |           4.0065 |
[32m[20230204 15:42:00 @agent_ppo2.py:139][0m Policy update time: 1.02 s
[32m[20230204 15:42:00 @agent_ppo2.py:147][0m Average TRAINING episode reward: 150.76
[32m[20230204 15:42:00 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.72
[32m[20230204 15:42:00 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.38
[32m[20230204 15:42:00 @agent_ppo2.py:152][0m Total time:       5.83 min
[32m[20230204 15:42:00 @agent_ppo2.py:154][0m 421888 total steps have happened
[32m[20230204 15:42:00 @agent_ppo2.py:130][0m #------------------------ Iteration 206 --------------------------#
[32m[20230204 15:42:01 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:01 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:01 @agent_ppo2.py:194][0m |          -0.0001 |          25.7932 |           3.9879 |
[32m[20230204 15:42:01 @agent_ppo2.py:194][0m |          -0.0039 |          17.8084 |           3.9819 |
[32m[20230204 15:42:01 @agent_ppo2.py:194][0m |          -0.0051 |          16.4427 |           3.9789 |
[32m[20230204 15:42:01 @agent_ppo2.py:194][0m |          -0.0083 |          15.6483 |           3.9789 |
[32m[20230204 15:42:01 @agent_ppo2.py:194][0m |          -0.0080 |          15.2094 |           3.9814 |
[32m[20230204 15:42:01 @agent_ppo2.py:194][0m |          -0.0090 |          14.9127 |           3.9788 |
[32m[20230204 15:42:01 @agent_ppo2.py:194][0m |          -0.0100 |          14.6785 |           3.9812 |
[32m[20230204 15:42:01 @agent_ppo2.py:194][0m |          -0.0110 |          14.4106 |           3.9807 |
[32m[20230204 15:42:01 @agent_ppo2.py:194][0m |          -0.0112 |          14.1789 |           3.9825 |
[32m[20230204 15:42:01 @agent_ppo2.py:194][0m |          -0.0119 |          13.9962 |           3.9823 |
[32m[20230204 15:42:01 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:42:02 @agent_ppo2.py:147][0m Average TRAINING episode reward: 207.69
[32m[20230204 15:42:02 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 253.42
[32m[20230204 15:42:02 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.60
[32m[20230204 15:42:02 @agent_ppo2.py:152][0m Total time:       5.85 min
[32m[20230204 15:42:02 @agent_ppo2.py:154][0m 423936 total steps have happened
[32m[20230204 15:42:02 @agent_ppo2.py:130][0m #------------------------ Iteration 207 --------------------------#
[32m[20230204 15:42:02 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:02 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:02 @agent_ppo2.py:194][0m |           0.0013 |          11.8809 |           3.9802 |
[32m[20230204 15:42:02 @agent_ppo2.py:194][0m |          -0.0078 |          11.3526 |           3.9747 |
[32m[20230204 15:42:02 @agent_ppo2.py:194][0m |          -0.0018 |          11.2683 |           3.9723 |
[32m[20230204 15:42:02 @agent_ppo2.py:194][0m |          -0.0118 |          10.9480 |           3.9700 |
[32m[20230204 15:42:02 @agent_ppo2.py:194][0m |          -0.0103 |          10.7694 |           3.9665 |
[32m[20230204 15:42:03 @agent_ppo2.py:194][0m |          -0.0108 |          10.6687 |           3.9672 |
[32m[20230204 15:42:03 @agent_ppo2.py:194][0m |          -0.0122 |          10.5829 |           3.9663 |
[32m[20230204 15:42:03 @agent_ppo2.py:194][0m |          -0.0111 |          10.5120 |           3.9644 |
[32m[20230204 15:42:03 @agent_ppo2.py:194][0m |          -0.0123 |          10.4447 |           3.9659 |
[32m[20230204 15:42:03 @agent_ppo2.py:194][0m |          -0.0099 |          10.3376 |           3.9613 |
[32m[20230204 15:42:03 @agent_ppo2.py:139][0m Policy update time: 0.92 s
[32m[20230204 15:42:03 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.59
[32m[20230204 15:42:03 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.91
[32m[20230204 15:42:03 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -28.67
[32m[20230204 15:42:03 @agent_ppo2.py:152][0m Total time:       5.88 min
[32m[20230204 15:42:03 @agent_ppo2.py:154][0m 425984 total steps have happened
[32m[20230204 15:42:03 @agent_ppo2.py:130][0m #------------------------ Iteration 208 --------------------------#
[32m[20230204 15:42:03 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:42:03 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:03 @agent_ppo2.py:194][0m |          -0.0045 |          19.6840 |           3.9866 |
[32m[20230204 15:42:04 @agent_ppo2.py:194][0m |          -0.0101 |          14.9655 |           3.9786 |
[32m[20230204 15:42:04 @agent_ppo2.py:194][0m |          -0.0106 |          14.4721 |           3.9762 |
[32m[20230204 15:42:04 @agent_ppo2.py:194][0m |          -0.0113 |          14.0062 |           3.9762 |
[32m[20230204 15:42:04 @agent_ppo2.py:194][0m |          -0.0121 |          13.4592 |           3.9750 |
[32m[20230204 15:42:04 @agent_ppo2.py:194][0m |          -0.0125 |          12.9690 |           3.9720 |
[32m[20230204 15:42:04 @agent_ppo2.py:194][0m |          -0.0056 |          12.6609 |           3.9734 |
[32m[20230204 15:42:04 @agent_ppo2.py:194][0m |          -0.0154 |          12.3975 |           3.9720 |
[32m[20230204 15:42:04 @agent_ppo2.py:194][0m |          -0.0142 |          12.1795 |           3.9708 |
[32m[20230204 15:42:04 @agent_ppo2.py:194][0m |          -0.0149 |          12.0525 |           3.9731 |
[32m[20230204 15:42:04 @agent_ppo2.py:139][0m Policy update time: 0.97 s
[32m[20230204 15:42:05 @agent_ppo2.py:147][0m Average TRAINING episode reward: 189.30
[32m[20230204 15:42:05 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 257.18
[32m[20230204 15:42:05 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 125.29
[32m[20230204 15:42:05 @agent_ppo2.py:152][0m Total time:       5.90 min
[32m[20230204 15:42:05 @agent_ppo2.py:154][0m 428032 total steps have happened
[32m[20230204 15:42:05 @agent_ppo2.py:130][0m #------------------------ Iteration 209 --------------------------#
[32m[20230204 15:42:05 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:05 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:05 @agent_ppo2.py:194][0m |           0.0023 |          12.1226 |           3.9558 |
[32m[20230204 15:42:05 @agent_ppo2.py:194][0m |           0.0006 |          12.0061 |           3.9486 |
[32m[20230204 15:42:05 @agent_ppo2.py:194][0m |          -0.0067 |          11.1299 |           3.9474 |
[32m[20230204 15:42:05 @agent_ppo2.py:194][0m |          -0.0071 |          10.9520 |           3.9472 |
[32m[20230204 15:42:05 @agent_ppo2.py:194][0m |          -0.0071 |          10.8059 |           3.9445 |
[32m[20230204 15:42:05 @agent_ppo2.py:194][0m |          -0.0040 |          11.3793 |           3.9470 |
[32m[20230204 15:42:06 @agent_ppo2.py:194][0m |          -0.0087 |          10.5859 |           3.9445 |
[32m[20230204 15:42:06 @agent_ppo2.py:194][0m |          -0.0080 |          10.5521 |           3.9453 |
[32m[20230204 15:42:06 @agent_ppo2.py:194][0m |          -0.0078 |          10.4025 |           3.9453 |
[32m[20230204 15:42:06 @agent_ppo2.py:194][0m |          -0.0094 |          10.3018 |           3.9445 |
[32m[20230204 15:42:06 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:42:06 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.04
[32m[20230204 15:42:06 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.83
[32m[20230204 15:42:06 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 102.35
[32m[20230204 15:42:06 @agent_ppo2.py:152][0m Total time:       5.93 min
[32m[20230204 15:42:06 @agent_ppo2.py:154][0m 430080 total steps have happened
[32m[20230204 15:42:06 @agent_ppo2.py:130][0m #------------------------ Iteration 210 --------------------------#
[32m[20230204 15:42:06 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:06 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:06 @agent_ppo2.py:194][0m |          -0.0038 |          26.8206 |           3.8555 |
[32m[20230204 15:42:07 @agent_ppo2.py:194][0m |          -0.0093 |          18.3297 |           3.8578 |
[32m[20230204 15:42:07 @agent_ppo2.py:194][0m |          -0.0113 |          15.9564 |           3.8582 |
[32m[20230204 15:42:07 @agent_ppo2.py:194][0m |          -0.0127 |          14.6365 |           3.8570 |
[32m[20230204 15:42:07 @agent_ppo2.py:194][0m |          -0.0036 |          14.0850 |           3.8583 |
[32m[20230204 15:42:07 @agent_ppo2.py:194][0m |          -0.0146 |          13.5204 |           3.8600 |
[32m[20230204 15:42:07 @agent_ppo2.py:194][0m |          -0.0123 |          12.8265 |           3.8598 |
[32m[20230204 15:42:07 @agent_ppo2.py:194][0m |          -0.0113 |          12.3155 |           3.8609 |
[32m[20230204 15:42:07 @agent_ppo2.py:194][0m |          -0.0133 |          11.8679 |           3.8611 |
[32m[20230204 15:42:07 @agent_ppo2.py:194][0m |          -0.0133 |          11.6370 |           3.8597 |
[32m[20230204 15:42:07 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:42:07 @agent_ppo2.py:147][0m Average TRAINING episode reward: 210.16
[32m[20230204 15:42:07 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.19
[32m[20230204 15:42:07 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.65
[32m[20230204 15:42:07 @agent_ppo2.py:152][0m Total time:       5.95 min
[32m[20230204 15:42:07 @agent_ppo2.py:154][0m 432128 total steps have happened
[32m[20230204 15:42:07 @agent_ppo2.py:130][0m #------------------------ Iteration 211 --------------------------#
[32m[20230204 15:42:08 @agent_ppo2.py:136][0m Sampling time: 0.35 s by 4 slaves
[32m[20230204 15:42:08 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:08 @agent_ppo2.py:194][0m |           0.0016 |          46.2746 |           4.0481 |
[32m[20230204 15:42:08 @agent_ppo2.py:194][0m |          -0.0061 |          32.5424 |           4.0397 |
[32m[20230204 15:42:08 @agent_ppo2.py:194][0m |          -0.0095 |          29.0929 |           4.0376 |
[32m[20230204 15:42:08 @agent_ppo2.py:194][0m |          -0.0102 |          27.0082 |           4.0334 |
[32m[20230204 15:42:08 @agent_ppo2.py:194][0m |          -0.0106 |          25.9193 |           4.0420 |
[32m[20230204 15:42:08 @agent_ppo2.py:194][0m |          -0.0116 |          25.1774 |           4.0351 |
[32m[20230204 15:42:08 @agent_ppo2.py:194][0m |          -0.0102 |          24.4394 |           4.0327 |
[32m[20230204 15:42:09 @agent_ppo2.py:194][0m |          -0.0137 |          23.3186 |           4.0359 |
[32m[20230204 15:42:09 @agent_ppo2.py:194][0m |          -0.0125 |          23.0412 |           4.0347 |
[32m[20230204 15:42:09 @agent_ppo2.py:194][0m |          -0.0150 |          22.3431 |           4.0312 |
[32m[20230204 15:42:09 @agent_ppo2.py:139][0m Policy update time: 1.02 s
[32m[20230204 15:42:09 @agent_ppo2.py:147][0m Average TRAINING episode reward: 117.50
[32m[20230204 15:42:09 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.22
[32m[20230204 15:42:09 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 66.07
[32m[20230204 15:42:09 @agent_ppo2.py:152][0m Total time:       5.97 min
[32m[20230204 15:42:09 @agent_ppo2.py:154][0m 434176 total steps have happened
[32m[20230204 15:42:09 @agent_ppo2.py:130][0m #------------------------ Iteration 212 --------------------------#
[32m[20230204 15:42:09 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:42:09 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:09 @agent_ppo2.py:194][0m |          -0.0021 |          44.1513 |           3.9622 |
[32m[20230204 15:42:09 @agent_ppo2.py:194][0m |          -0.0053 |          37.3969 |           3.9555 |
[32m[20230204 15:42:10 @agent_ppo2.py:194][0m |          -0.0067 |          34.1024 |           3.9527 |
[32m[20230204 15:42:10 @agent_ppo2.py:194][0m |          -0.0076 |          31.1851 |           3.9504 |
[32m[20230204 15:42:10 @agent_ppo2.py:194][0m |          -0.0092 |          29.8384 |           3.9517 |
[32m[20230204 15:42:10 @agent_ppo2.py:194][0m |          -0.0110 |          27.3348 |           3.9519 |
[32m[20230204 15:42:10 @agent_ppo2.py:194][0m |          -0.0100 |          26.7693 |           3.9481 |
[32m[20230204 15:42:10 @agent_ppo2.py:194][0m |          -0.0105 |          25.4655 |           3.9488 |
[32m[20230204 15:42:10 @agent_ppo2.py:194][0m |          -0.0105 |          24.4522 |           3.9480 |
[32m[20230204 15:42:10 @agent_ppo2.py:194][0m |          -0.0096 |          24.3774 |           3.9451 |
[32m[20230204 15:42:10 @agent_ppo2.py:139][0m Policy update time: 0.92 s
[32m[20230204 15:42:10 @agent_ppo2.py:147][0m Average TRAINING episode reward: 166.06
[32m[20230204 15:42:10 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.84
[32m[20230204 15:42:10 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 11.43
[32m[20230204 15:42:10 @agent_ppo2.py:152][0m Total time:       6.00 min
[32m[20230204 15:42:10 @agent_ppo2.py:154][0m 436224 total steps have happened
[32m[20230204 15:42:10 @agent_ppo2.py:130][0m #------------------------ Iteration 213 --------------------------#
[32m[20230204 15:42:11 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:42:11 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:11 @agent_ppo2.py:194][0m |          -0.0000 |          31.8844 |           4.0512 |
[32m[20230204 15:42:11 @agent_ppo2.py:194][0m |          -0.0049 |          26.0434 |           4.0384 |
[32m[20230204 15:42:11 @agent_ppo2.py:194][0m |          -0.0069 |          24.4057 |           4.0413 |
[32m[20230204 15:42:11 @agent_ppo2.py:194][0m |          -0.0085 |          22.9821 |           4.0387 |
[32m[20230204 15:42:11 @agent_ppo2.py:194][0m |          -0.0093 |          21.8959 |           4.0404 |
[32m[20230204 15:42:11 @agent_ppo2.py:194][0m |          -0.0119 |          21.3665 |           4.0432 |
[32m[20230204 15:42:11 @agent_ppo2.py:194][0m |          -0.0109 |          20.3210 |           4.0380 |
[32m[20230204 15:42:11 @agent_ppo2.py:194][0m |          -0.0114 |          19.8313 |           4.0437 |
[32m[20230204 15:42:11 @agent_ppo2.py:194][0m |          -0.0130 |          19.2857 |           4.0406 |
[32m[20230204 15:42:11 @agent_ppo2.py:194][0m |          -0.0137 |          18.6887 |           4.0440 |
[32m[20230204 15:42:11 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:42:12 @agent_ppo2.py:147][0m Average TRAINING episode reward: 254.10
[32m[20230204 15:42:12 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.81
[32m[20230204 15:42:12 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 115.43
[32m[20230204 15:42:12 @agent_ppo2.py:152][0m Total time:       6.02 min
[32m[20230204 15:42:12 @agent_ppo2.py:154][0m 438272 total steps have happened
[32m[20230204 15:42:12 @agent_ppo2.py:130][0m #------------------------ Iteration 214 --------------------------#
[32m[20230204 15:42:12 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:42:12 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:12 @agent_ppo2.py:194][0m |           0.0019 |          15.9527 |           4.0373 |
[32m[20230204 15:42:12 @agent_ppo2.py:194][0m |          -0.0043 |          13.0832 |           4.0347 |
[32m[20230204 15:42:12 @agent_ppo2.py:194][0m |          -0.0053 |          12.2436 |           4.0253 |
[32m[20230204 15:42:12 @agent_ppo2.py:194][0m |          -0.0071 |          11.7814 |           4.0269 |
[32m[20230204 15:42:12 @agent_ppo2.py:194][0m |          -0.0082 |          11.5067 |           4.0244 |
[32m[20230204 15:42:13 @agent_ppo2.py:194][0m |          -0.0092 |          11.2693 |           4.0227 |
[32m[20230204 15:42:13 @agent_ppo2.py:194][0m |          -0.0111 |          11.1035 |           4.0220 |
[32m[20230204 15:42:13 @agent_ppo2.py:194][0m |          -0.0099 |          10.9745 |           4.0207 |
[32m[20230204 15:42:13 @agent_ppo2.py:194][0m |          -0.0103 |          10.8404 |           4.0167 |
[32m[20230204 15:42:13 @agent_ppo2.py:194][0m |          -0.0112 |          10.7520 |           4.0154 |
[32m[20230204 15:42:13 @agent_ppo2.py:139][0m Policy update time: 0.96 s
[32m[20230204 15:42:13 @agent_ppo2.py:147][0m Average TRAINING episode reward: 254.48
[32m[20230204 15:42:13 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.49
[32m[20230204 15:42:13 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.31
[32m[20230204 15:42:13 @agent_ppo2.py:152][0m Total time:       6.04 min
[32m[20230204 15:42:13 @agent_ppo2.py:154][0m 440320 total steps have happened
[32m[20230204 15:42:13 @agent_ppo2.py:130][0m #------------------------ Iteration 215 --------------------------#
[32m[20230204 15:42:13 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:42:14 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:14 @agent_ppo2.py:194][0m |          -0.0011 |          16.1353 |           4.0356 |
[32m[20230204 15:42:14 @agent_ppo2.py:194][0m |          -0.0040 |          13.6762 |           4.0359 |
[32m[20230204 15:42:14 @agent_ppo2.py:194][0m |          -0.0060 |          13.1689 |           4.0331 |
[32m[20230204 15:42:14 @agent_ppo2.py:194][0m |          -0.0080 |          12.7515 |           4.0336 |
[32m[20230204 15:42:14 @agent_ppo2.py:194][0m |          -0.0086 |          12.3774 |           4.0326 |
[32m[20230204 15:42:14 @agent_ppo2.py:194][0m |          -0.0099 |          12.1762 |           4.0260 |
[32m[20230204 15:42:14 @agent_ppo2.py:194][0m |          -0.0112 |          12.0471 |           4.0252 |
[32m[20230204 15:42:14 @agent_ppo2.py:194][0m |          -0.0109 |          11.8112 |           4.0242 |
[32m[20230204 15:42:14 @agent_ppo2.py:194][0m |          -0.0115 |          11.5985 |           4.0252 |
[32m[20230204 15:42:14 @agent_ppo2.py:194][0m |          -0.0124 |          11.4612 |           4.0255 |
[32m[20230204 15:42:14 @agent_ppo2.py:139][0m Policy update time: 1.00 s
[32m[20230204 15:42:15 @agent_ppo2.py:147][0m Average TRAINING episode reward: 201.83
[32m[20230204 15:42:15 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.44
[32m[20230204 15:42:15 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 12.96
[32m[20230204 15:42:15 @agent_ppo2.py:152][0m Total time:       6.07 min
[32m[20230204 15:42:15 @agent_ppo2.py:154][0m 442368 total steps have happened
[32m[20230204 15:42:15 @agent_ppo2.py:130][0m #------------------------ Iteration 216 --------------------------#
[32m[20230204 15:42:15 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:15 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:15 @agent_ppo2.py:194][0m |          -0.0031 |          11.7428 |           3.9652 |
[32m[20230204 15:42:15 @agent_ppo2.py:194][0m |          -0.0050 |          10.9812 |           3.9549 |
[32m[20230204 15:42:15 @agent_ppo2.py:194][0m |          -0.0067 |          10.7639 |           3.9573 |
[32m[20230204 15:42:15 @agent_ppo2.py:194][0m |          -0.0071 |          10.5747 |           3.9581 |
[32m[20230204 15:42:15 @agent_ppo2.py:194][0m |          -0.0095 |          10.4701 |           3.9536 |
[32m[20230204 15:42:15 @agent_ppo2.py:194][0m |          -0.0081 |          10.3820 |           3.9544 |
[32m[20230204 15:42:15 @agent_ppo2.py:194][0m |          -0.0087 |          10.2655 |           3.9568 |
[32m[20230204 15:42:16 @agent_ppo2.py:194][0m |          -0.0089 |          10.1926 |           3.9569 |
[32m[20230204 15:42:16 @agent_ppo2.py:194][0m |          -0.0114 |          10.1552 |           3.9562 |
[32m[20230204 15:42:16 @agent_ppo2.py:194][0m |          -0.0097 |          10.0606 |           3.9560 |
[32m[20230204 15:42:16 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:42:16 @agent_ppo2.py:147][0m Average TRAINING episode reward: 254.55
[32m[20230204 15:42:16 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.65
[32m[20230204 15:42:16 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -7.60
[32m[20230204 15:42:16 @agent_ppo2.py:152][0m Total time:       6.09 min
[32m[20230204 15:42:16 @agent_ppo2.py:154][0m 444416 total steps have happened
[32m[20230204 15:42:16 @agent_ppo2.py:130][0m #------------------------ Iteration 217 --------------------------#
[32m[20230204 15:42:16 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:42:16 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:16 @agent_ppo2.py:194][0m |           0.0002 |          10.9398 |           4.0629 |
[32m[20230204 15:42:16 @agent_ppo2.py:194][0m |          -0.0040 |           9.5546 |           4.0579 |
[32m[20230204 15:42:16 @agent_ppo2.py:194][0m |          -0.0056 |           9.2307 |           4.0535 |
[32m[20230204 15:42:17 @agent_ppo2.py:194][0m |          -0.0079 |           8.9409 |           4.0559 |
[32m[20230204 15:42:17 @agent_ppo2.py:194][0m |          -0.0080 |           8.7545 |           4.0537 |
[32m[20230204 15:42:17 @agent_ppo2.py:194][0m |          -0.0081 |           8.6726 |           4.0570 |
[32m[20230204 15:42:17 @agent_ppo2.py:194][0m |          -0.0097 |           8.4379 |           4.0584 |
[32m[20230204 15:42:17 @agent_ppo2.py:194][0m |          -0.0094 |           8.2902 |           4.0573 |
[32m[20230204 15:42:17 @agent_ppo2.py:194][0m |          -0.0089 |           8.3567 |           4.0602 |
[32m[20230204 15:42:17 @agent_ppo2.py:194][0m |          -0.0101 |           8.0766 |           4.0548 |
[32m[20230204 15:42:17 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:42:17 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.89
[32m[20230204 15:42:17 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.95
[32m[20230204 15:42:17 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.79
[32m[20230204 15:42:17 @agent_ppo2.py:152][0m Total time:       6.11 min
[32m[20230204 15:42:17 @agent_ppo2.py:154][0m 446464 total steps have happened
[32m[20230204 15:42:17 @agent_ppo2.py:130][0m #------------------------ Iteration 218 --------------------------#
[32m[20230204 15:42:18 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:18 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:18 @agent_ppo2.py:194][0m |           0.0011 |          12.9891 |           4.0371 |
[32m[20230204 15:42:18 @agent_ppo2.py:194][0m |          -0.0039 |          12.2189 |           4.0360 |
[32m[20230204 15:42:18 @agent_ppo2.py:194][0m |          -0.0061 |          11.9113 |           4.0302 |
[32m[20230204 15:42:18 @agent_ppo2.py:194][0m |          -0.0076 |          11.6604 |           4.0319 |
[32m[20230204 15:42:18 @agent_ppo2.py:194][0m |          -0.0087 |          11.4935 |           4.0271 |
[32m[20230204 15:42:18 @agent_ppo2.py:194][0m |          -0.0093 |          11.3673 |           4.0304 |
[32m[20230204 15:42:18 @agent_ppo2.py:194][0m |          -0.0080 |          11.3763 |           4.0264 |
[32m[20230204 15:42:18 @agent_ppo2.py:194][0m |          -0.0087 |          11.2400 |           4.0262 |
[32m[20230204 15:42:18 @agent_ppo2.py:194][0m |          -0.0091 |          11.2824 |           4.0285 |
[32m[20230204 15:42:18 @agent_ppo2.py:194][0m |          -0.0108 |          11.0446 |           4.0253 |
[32m[20230204 15:42:18 @agent_ppo2.py:139][0m Policy update time: 0.91 s
[32m[20230204 15:42:19 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.60
[32m[20230204 15:42:19 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.71
[32m[20230204 15:42:19 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 8.25
[32m[20230204 15:42:19 @agent_ppo2.py:152][0m Total time:       6.14 min
[32m[20230204 15:42:19 @agent_ppo2.py:154][0m 448512 total steps have happened
[32m[20230204 15:42:19 @agent_ppo2.py:130][0m #------------------------ Iteration 219 --------------------------#
[32m[20230204 15:42:19 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:19 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:19 @agent_ppo2.py:194][0m |           0.0068 |          21.9258 |           4.0406 |
[32m[20230204 15:42:19 @agent_ppo2.py:194][0m |          -0.0060 |          15.7573 |           4.0333 |
[32m[20230204 15:42:19 @agent_ppo2.py:194][0m |          -0.0083 |          14.0169 |           4.0283 |
[32m[20230204 15:42:19 @agent_ppo2.py:194][0m |          -0.0109 |          13.1248 |           4.0238 |
[32m[20230204 15:42:19 @agent_ppo2.py:194][0m |          -0.0108 |          12.7262 |           4.0231 |
[32m[20230204 15:42:19 @agent_ppo2.py:194][0m |          -0.0126 |          12.2814 |           4.0160 |
[32m[20230204 15:42:19 @agent_ppo2.py:194][0m |          -0.0108 |          12.0350 |           4.0145 |
[32m[20230204 15:42:19 @agent_ppo2.py:194][0m |          -0.0120 |          11.6448 |           4.0156 |
[32m[20230204 15:42:20 @agent_ppo2.py:194][0m |          -0.0153 |          11.3397 |           4.0139 |
[32m[20230204 15:42:20 @agent_ppo2.py:194][0m |          -0.0119 |          11.3317 |           4.0128 |
[32m[20230204 15:42:20 @agent_ppo2.py:139][0m Policy update time: 0.80 s
[32m[20230204 15:42:20 @agent_ppo2.py:147][0m Average TRAINING episode reward: 190.73
[32m[20230204 15:42:20 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.86
[32m[20230204 15:42:20 @agent_ppo2.py:149][0m Average EVALUATION episode reward: -19.48
[32m[20230204 15:42:20 @agent_ppo2.py:152][0m Total time:       6.16 min
[32m[20230204 15:42:20 @agent_ppo2.py:154][0m 450560 total steps have happened
[32m[20230204 15:42:20 @agent_ppo2.py:130][0m #------------------------ Iteration 220 --------------------------#
[32m[20230204 15:42:20 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:42:20 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:20 @agent_ppo2.py:194][0m |           0.0008 |          13.8955 |           4.0478 |
[32m[20230204 15:42:20 @agent_ppo2.py:194][0m |          -0.0043 |          12.3378 |           4.0504 |
[32m[20230204 15:42:20 @agent_ppo2.py:194][0m |          -0.0054 |          11.9149 |           4.0504 |
[32m[20230204 15:42:20 @agent_ppo2.py:194][0m |          -0.0064 |          11.6074 |           4.0516 |
[32m[20230204 15:42:21 @agent_ppo2.py:194][0m |          -0.0075 |          11.4147 |           4.0548 |
[32m[20230204 15:42:21 @agent_ppo2.py:194][0m |          -0.0090 |          11.2224 |           4.0561 |
[32m[20230204 15:42:21 @agent_ppo2.py:194][0m |          -0.0095 |          11.0633 |           4.0592 |
[32m[20230204 15:42:21 @agent_ppo2.py:194][0m |          -0.0103 |          10.8991 |           4.0582 |
[32m[20230204 15:42:21 @agent_ppo2.py:194][0m |          -0.0106 |          10.7879 |           4.0633 |
[32m[20230204 15:42:21 @agent_ppo2.py:194][0m |          -0.0115 |          10.6305 |           4.0631 |
[32m[20230204 15:42:21 @agent_ppo2.py:139][0m Policy update time: 0.91 s
[32m[20230204 15:42:21 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.49
[32m[20230204 15:42:21 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.83
[32m[20230204 15:42:21 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 134.51
[32m[20230204 15:42:21 @agent_ppo2.py:152][0m Total time:       6.18 min
[32m[20230204 15:42:21 @agent_ppo2.py:154][0m 452608 total steps have happened
[32m[20230204 15:42:21 @agent_ppo2.py:130][0m #------------------------ Iteration 221 --------------------------#
[32m[20230204 15:42:21 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:42:22 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:22 @agent_ppo2.py:194][0m |          -0.0008 |          40.9525 |           4.0379 |
[32m[20230204 15:42:22 @agent_ppo2.py:194][0m |          -0.0056 |          30.8785 |           4.0284 |
[32m[20230204 15:42:22 @agent_ppo2.py:194][0m |          -0.0071 |          29.1808 |           4.0242 |
[32m[20230204 15:42:22 @agent_ppo2.py:194][0m |          -0.0066 |          27.9523 |           4.0260 |
[32m[20230204 15:42:22 @agent_ppo2.py:194][0m |          -0.0079 |          27.3127 |           4.0173 |
[32m[20230204 15:42:22 @agent_ppo2.py:194][0m |          -0.0090 |          26.3563 |           4.0193 |
[32m[20230204 15:42:22 @agent_ppo2.py:194][0m |          -0.0100 |          25.7348 |           4.0191 |
[32m[20230204 15:42:22 @agent_ppo2.py:194][0m |          -0.0098 |          25.3089 |           4.0223 |
[32m[20230204 15:42:22 @agent_ppo2.py:194][0m |          -0.0101 |          25.0674 |           4.0203 |
[32m[20230204 15:42:22 @agent_ppo2.py:194][0m |          -0.0107 |          24.5035 |           4.0202 |
[32m[20230204 15:42:22 @agent_ppo2.py:139][0m Policy update time: 0.78 s
[32m[20230204 15:42:23 @agent_ppo2.py:147][0m Average TRAINING episode reward: 106.48
[32m[20230204 15:42:23 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.62
[32m[20230204 15:42:23 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.86
[32m[20230204 15:42:23 @agent_ppo2.py:152][0m Total time:       6.20 min
[32m[20230204 15:42:23 @agent_ppo2.py:154][0m 454656 total steps have happened
[32m[20230204 15:42:23 @agent_ppo2.py:130][0m #------------------------ Iteration 222 --------------------------#
[32m[20230204 15:42:23 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:42:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:23 @agent_ppo2.py:194][0m |          -0.0024 |          13.8482 |           4.0045 |
[32m[20230204 15:42:23 @agent_ppo2.py:194][0m |          -0.0029 |          12.4052 |           3.9966 |
[32m[20230204 15:42:23 @agent_ppo2.py:194][0m |           0.0044 |          12.8602 |           3.9976 |
[32m[20230204 15:42:23 @agent_ppo2.py:194][0m |          -0.0029 |          11.8840 |           3.9916 |
[32m[20230204 15:42:23 @agent_ppo2.py:194][0m |          -0.0045 |          11.7928 |           3.9995 |
[32m[20230204 15:42:23 @agent_ppo2.py:194][0m |          -0.0052 |          11.6708 |           3.9987 |
[32m[20230204 15:42:23 @agent_ppo2.py:194][0m |          -0.0066 |          11.5573 |           3.9945 |
[32m[20230204 15:42:24 @agent_ppo2.py:194][0m |          -0.0096 |          11.4884 |           3.9924 |
[32m[20230204 15:42:24 @agent_ppo2.py:194][0m |          -0.0083 |          11.4269 |           3.9939 |
[32m[20230204 15:42:24 @agent_ppo2.py:194][0m |          -0.0042 |          12.9697 |           3.9914 |
[32m[20230204 15:42:24 @agent_ppo2.py:139][0m Policy update time: 0.91 s
[32m[20230204 15:42:24 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.56
[32m[20230204 15:42:24 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.72
[32m[20230204 15:42:24 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.33
[32m[20230204 15:42:24 @agent_ppo2.py:152][0m Total time:       6.22 min
[32m[20230204 15:42:24 @agent_ppo2.py:154][0m 456704 total steps have happened
[32m[20230204 15:42:24 @agent_ppo2.py:130][0m #------------------------ Iteration 223 --------------------------#
[32m[20230204 15:42:24 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:24 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:24 @agent_ppo2.py:194][0m |           0.0038 |          11.6037 |           4.1482 |
[32m[20230204 15:42:24 @agent_ppo2.py:194][0m |          -0.0028 |          10.7219 |           4.1474 |
[32m[20230204 15:42:24 @agent_ppo2.py:194][0m |          -0.0046 |          10.3802 |           4.1462 |
[32m[20230204 15:42:25 @agent_ppo2.py:194][0m |          -0.0073 |          10.1566 |           4.1482 |
[32m[20230204 15:42:25 @agent_ppo2.py:194][0m |          -0.0076 |           9.9561 |           4.1458 |
[32m[20230204 15:42:25 @agent_ppo2.py:194][0m |          -0.0088 |           9.8164 |           4.1479 |
[32m[20230204 15:42:25 @agent_ppo2.py:194][0m |          -0.0099 |           9.6699 |           4.1451 |
[32m[20230204 15:42:25 @agent_ppo2.py:194][0m |          -0.0104 |           9.5665 |           4.1484 |
[32m[20230204 15:42:25 @agent_ppo2.py:194][0m |          -0.0104 |           9.4721 |           4.1439 |
[32m[20230204 15:42:25 @agent_ppo2.py:194][0m |          -0.0113 |           9.3732 |           4.1502 |
[32m[20230204 15:42:25 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:42:25 @agent_ppo2.py:147][0m Average TRAINING episode reward: 254.95
[32m[20230204 15:42:25 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.76
[32m[20230204 15:42:25 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 64.64
[32m[20230204 15:42:25 @agent_ppo2.py:152][0m Total time:       6.25 min
[32m[20230204 15:42:25 @agent_ppo2.py:154][0m 458752 total steps have happened
[32m[20230204 15:42:25 @agent_ppo2.py:130][0m #------------------------ Iteration 224 --------------------------#
[32m[20230204 15:42:25 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:26 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:26 @agent_ppo2.py:194][0m |          -0.0031 |          12.3039 |           4.0932 |
[32m[20230204 15:42:26 @agent_ppo2.py:194][0m |          -0.0068 |          11.2906 |           4.0891 |
[32m[20230204 15:42:26 @agent_ppo2.py:194][0m |          -0.0044 |          10.8979 |           4.0850 |
[32m[20230204 15:42:26 @agent_ppo2.py:194][0m |          -0.0103 |          10.4577 |           4.0840 |
[32m[20230204 15:42:26 @agent_ppo2.py:194][0m |          -0.0097 |          10.1975 |           4.0857 |
[32m[20230204 15:42:26 @agent_ppo2.py:194][0m |          -0.0091 |          10.0454 |           4.0818 |
[32m[20230204 15:42:26 @agent_ppo2.py:194][0m |          -0.0090 |           9.8527 |           4.0839 |
[32m[20230204 15:42:26 @agent_ppo2.py:194][0m |          -0.0106 |           9.6436 |           4.0836 |
[32m[20230204 15:42:26 @agent_ppo2.py:194][0m |          -0.0040 |           9.9870 |           4.0812 |
[32m[20230204 15:42:26 @agent_ppo2.py:194][0m |          -0.0092 |           9.6270 |           4.0805 |
[32m[20230204 15:42:26 @agent_ppo2.py:139][0m Policy update time: 0.92 s
[32m[20230204 15:42:27 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.20
[32m[20230204 15:42:27 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.84
[32m[20230204 15:42:27 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.01
[32m[20230204 15:42:27 @agent_ppo2.py:152][0m Total time:       6.27 min
[32m[20230204 15:42:27 @agent_ppo2.py:154][0m 460800 total steps have happened
[32m[20230204 15:42:27 @agent_ppo2.py:130][0m #------------------------ Iteration 225 --------------------------#
[32m[20230204 15:42:27 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:42:27 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:27 @agent_ppo2.py:194][0m |          -0.0031 |          11.0178 |           4.0121 |
[32m[20230204 15:42:27 @agent_ppo2.py:194][0m |          -0.0061 |          10.5184 |           4.0028 |
[32m[20230204 15:42:27 @agent_ppo2.py:194][0m |          -0.0073 |          10.2995 |           3.9983 |
[32m[20230204 15:42:27 @agent_ppo2.py:194][0m |          -0.0099 |          10.2240 |           3.9973 |
[32m[20230204 15:42:27 @agent_ppo2.py:194][0m |          -0.0051 |          10.0250 |           3.9981 |
[32m[20230204 15:42:28 @agent_ppo2.py:194][0m |          -0.0053 |           9.9223 |           3.9939 |
[32m[20230204 15:42:28 @agent_ppo2.py:194][0m |          -0.0078 |           9.7823 |           3.9901 |
[32m[20230204 15:42:28 @agent_ppo2.py:194][0m |          -0.0103 |           9.6887 |           3.9858 |
[32m[20230204 15:42:28 @agent_ppo2.py:194][0m |          -0.0166 |           9.5554 |           3.9869 |
[32m[20230204 15:42:28 @agent_ppo2.py:194][0m |          -0.0137 |           9.4493 |           3.9885 |
[32m[20230204 15:42:28 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:42:28 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.70
[32m[20230204 15:42:28 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.30
[32m[20230204 15:42:28 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.77
[32m[20230204 15:42:28 @agent_ppo2.py:152][0m Total time:       6.29 min
[32m[20230204 15:42:28 @agent_ppo2.py:154][0m 462848 total steps have happened
[32m[20230204 15:42:28 @agent_ppo2.py:130][0m #------------------------ Iteration 226 --------------------------#
[32m[20230204 15:42:28 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:28 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:28 @agent_ppo2.py:194][0m |           0.0031 |          11.3061 |           4.0871 |
[32m[20230204 15:42:29 @agent_ppo2.py:194][0m |          -0.0043 |          10.5843 |           4.0791 |
[32m[20230204 15:42:29 @agent_ppo2.py:194][0m |          -0.0060 |          10.3866 |           4.0837 |
[32m[20230204 15:42:29 @agent_ppo2.py:194][0m |          -0.0084 |          10.1556 |           4.0769 |
[32m[20230204 15:42:29 @agent_ppo2.py:194][0m |          -0.0088 |          10.0148 |           4.0754 |
[32m[20230204 15:42:29 @agent_ppo2.py:194][0m |          -0.0090 |           9.9855 |           4.0819 |
[32m[20230204 15:42:29 @agent_ppo2.py:194][0m |          -0.0100 |           9.7965 |           4.0847 |
[32m[20230204 15:42:29 @agent_ppo2.py:194][0m |          -0.0101 |           9.7472 |           4.0852 |
[32m[20230204 15:42:29 @agent_ppo2.py:194][0m |          -0.0121 |           9.6342 |           4.0877 |
[32m[20230204 15:42:29 @agent_ppo2.py:194][0m |          -0.0108 |           9.6608 |           4.0902 |
[32m[20230204 15:42:29 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:42:29 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.86
[32m[20230204 15:42:29 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 256.77
[32m[20230204 15:42:29 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.63
[32m[20230204 15:42:29 @agent_ppo2.py:152][0m Total time:       6.32 min
[32m[20230204 15:42:29 @agent_ppo2.py:154][0m 464896 total steps have happened
[32m[20230204 15:42:29 @agent_ppo2.py:130][0m #------------------------ Iteration 227 --------------------------#
[32m[20230204 15:42:30 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:30 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:30 @agent_ppo2.py:194][0m |          -0.0001 |          11.0674 |           4.1079 |
[32m[20230204 15:42:30 @agent_ppo2.py:194][0m |          -0.0069 |           9.9176 |           4.1062 |
[32m[20230204 15:42:30 @agent_ppo2.py:194][0m |          -0.0084 |           9.5462 |           4.1049 |
[32m[20230204 15:42:30 @agent_ppo2.py:194][0m |          -0.0098 |           9.2206 |           4.1035 |
[32m[20230204 15:42:30 @agent_ppo2.py:194][0m |          -0.0113 |           8.9131 |           4.0990 |
[32m[20230204 15:42:30 @agent_ppo2.py:194][0m |          -0.0120 |           8.6558 |           4.1013 |
[32m[20230204 15:42:30 @agent_ppo2.py:194][0m |          -0.0124 |           8.3951 |           4.1014 |
[32m[20230204 15:42:31 @agent_ppo2.py:194][0m |          -0.0133 |           8.2247 |           4.0987 |
[32m[20230204 15:42:31 @agent_ppo2.py:194][0m |          -0.0140 |           7.9568 |           4.0988 |
[32m[20230204 15:42:31 @agent_ppo2.py:194][0m |          -0.0142 |           7.7823 |           4.1014 |
[32m[20230204 15:42:31 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:42:31 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.33
[32m[20230204 15:42:31 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.41
[32m[20230204 15:42:31 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 92.29
[32m[20230204 15:42:31 @agent_ppo2.py:152][0m Total time:       6.34 min
[32m[20230204 15:42:31 @agent_ppo2.py:154][0m 466944 total steps have happened
[32m[20230204 15:42:31 @agent_ppo2.py:130][0m #------------------------ Iteration 228 --------------------------#
[32m[20230204 15:42:31 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:31 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:31 @agent_ppo2.py:194][0m |          -0.0005 |          12.3324 |           4.1293 |
[32m[20230204 15:42:31 @agent_ppo2.py:194][0m |          -0.0025 |          11.6813 |           4.1270 |
[32m[20230204 15:42:32 @agent_ppo2.py:194][0m |          -0.0071 |          11.1345 |           4.1222 |
[32m[20230204 15:42:32 @agent_ppo2.py:194][0m |          -0.0081 |          10.8479 |           4.1268 |
[32m[20230204 15:42:32 @agent_ppo2.py:194][0m |          -0.0088 |          10.6943 |           4.1238 |
[32m[20230204 15:42:32 @agent_ppo2.py:194][0m |          -0.0100 |          10.5079 |           4.1276 |
[32m[20230204 15:42:32 @agent_ppo2.py:194][0m |          -0.0089 |          10.4570 |           4.1221 |
[32m[20230204 15:42:32 @agent_ppo2.py:194][0m |          -0.0100 |          10.2765 |           4.1268 |
[32m[20230204 15:42:32 @agent_ppo2.py:194][0m |          -0.0107 |          10.1461 |           4.1226 |
[32m[20230204 15:42:32 @agent_ppo2.py:194][0m |          -0.0112 |          10.0018 |           4.1228 |
[32m[20230204 15:42:32 @agent_ppo2.py:139][0m Policy update time: 0.92 s
[32m[20230204 15:42:32 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.63
[32m[20230204 15:42:32 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.69
[32m[20230204 15:42:32 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.63
[32m[20230204 15:42:32 @agent_ppo2.py:152][0m Total time:       6.36 min
[32m[20230204 15:42:32 @agent_ppo2.py:154][0m 468992 total steps have happened
[32m[20230204 15:42:32 @agent_ppo2.py:130][0m #------------------------ Iteration 229 --------------------------#
[32m[20230204 15:42:33 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:42:33 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:33 @agent_ppo2.py:194][0m |          -0.0065 |          41.6236 |           4.1302 |
[32m[20230204 15:42:33 @agent_ppo2.py:194][0m |          -0.0066 |          30.0743 |           4.1216 |
[32m[20230204 15:42:33 @agent_ppo2.py:194][0m |           0.0093 |          28.9126 |           4.1136 |
[32m[20230204 15:42:33 @agent_ppo2.py:194][0m |          -0.0125 |          23.9911 |           4.1073 |
[32m[20230204 15:42:33 @agent_ppo2.py:194][0m |          -0.0152 |          22.5483 |           4.1095 |
[32m[20230204 15:42:33 @agent_ppo2.py:194][0m |          -0.0048 |          22.7083 |           4.1083 |
[32m[20230204 15:42:33 @agent_ppo2.py:194][0m |           0.0002 |          21.1370 |           4.1054 |
[32m[20230204 15:42:33 @agent_ppo2.py:194][0m |          -0.0155 |          19.7132 |           4.1037 |
[32m[20230204 15:42:33 @agent_ppo2.py:194][0m |          -0.0149 |          19.0071 |           4.1077 |
[32m[20230204 15:42:33 @agent_ppo2.py:194][0m |          -0.0157 |          18.5187 |           4.1084 |
[32m[20230204 15:42:33 @agent_ppo2.py:139][0m Policy update time: 0.71 s
[32m[20230204 15:42:34 @agent_ppo2.py:147][0m Average TRAINING episode reward: 132.65
[32m[20230204 15:42:34 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 254.27
[32m[20230204 15:42:34 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 276.36
[32m[20230204 15:42:34 @agent_ppo2.py:152][0m Total time:       6.38 min
[32m[20230204 15:42:34 @agent_ppo2.py:154][0m 471040 total steps have happened
[32m[20230204 15:42:34 @agent_ppo2.py:130][0m #------------------------ Iteration 230 --------------------------#
[32m[20230204 15:42:34 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:42:34 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:34 @agent_ppo2.py:194][0m |           0.0014 |          29.2045 |           4.1573 |
[32m[20230204 15:42:34 @agent_ppo2.py:194][0m |           0.0052 |          26.0426 |           4.1490 |
[32m[20230204 15:42:34 @agent_ppo2.py:194][0m |          -0.0076 |          24.1734 |           4.1420 |
[32m[20230204 15:42:34 @agent_ppo2.py:194][0m |          -0.0102 |          22.7316 |           4.1441 |
[32m[20230204 15:42:34 @agent_ppo2.py:194][0m |          -0.0095 |          21.7335 |           4.1386 |
[32m[20230204 15:42:34 @agent_ppo2.py:194][0m |          -0.0084 |          21.0959 |           4.1377 |
[32m[20230204 15:42:34 @agent_ppo2.py:194][0m |          -0.0133 |          20.5234 |           4.1373 |
[32m[20230204 15:42:34 @agent_ppo2.py:194][0m |          -0.0126 |          19.9114 |           4.1338 |
[32m[20230204 15:42:35 @agent_ppo2.py:194][0m |          -0.0148 |          19.6511 |           4.1334 |
[32m[20230204 15:42:35 @agent_ppo2.py:194][0m |          -0.0129 |          19.3132 |           4.1299 |
[32m[20230204 15:42:35 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:42:35 @agent_ppo2.py:147][0m Average TRAINING episode reward: 212.78
[32m[20230204 15:42:35 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.24
[32m[20230204 15:42:35 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.24
[32m[20230204 15:42:35 @agent_ppo2.py:152][0m Total time:       6.41 min
[32m[20230204 15:42:35 @agent_ppo2.py:154][0m 473088 total steps have happened
[32m[20230204 15:42:35 @agent_ppo2.py:130][0m #------------------------ Iteration 231 --------------------------#
[32m[20230204 15:42:35 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:35 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:35 @agent_ppo2.py:194][0m |           0.0014 |          14.9182 |           4.0746 |
[32m[20230204 15:42:35 @agent_ppo2.py:194][0m |           0.0030 |          11.1125 |           4.0758 |
[32m[20230204 15:42:35 @agent_ppo2.py:194][0m |          -0.0079 |          10.1781 |           4.0749 |
[32m[20230204 15:42:35 @agent_ppo2.py:194][0m |          -0.0095 |           9.6066 |           4.0707 |
[32m[20230204 15:42:36 @agent_ppo2.py:194][0m |          -0.0071 |           9.1506 |           4.0756 |
[32m[20230204 15:42:36 @agent_ppo2.py:194][0m |          -0.0139 |           8.8357 |           4.0772 |
[32m[20230204 15:42:36 @agent_ppo2.py:194][0m |          -0.0110 |           8.5236 |           4.0763 |
[32m[20230204 15:42:36 @agent_ppo2.py:194][0m |          -0.0112 |           8.2885 |           4.0786 |
[32m[20230204 15:42:36 @agent_ppo2.py:194][0m |          -0.0099 |           8.2018 |           4.0749 |
[32m[20230204 15:42:36 @agent_ppo2.py:194][0m |          -0.0088 |           7.8245 |           4.0777 |
[32m[20230204 15:42:36 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:42:36 @agent_ppo2.py:147][0m Average TRAINING episode reward: 254.90
[32m[20230204 15:42:36 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.39
[32m[20230204 15:42:36 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.40
[32m[20230204 15:42:36 @agent_ppo2.py:152][0m Total time:       6.43 min
[32m[20230204 15:42:36 @agent_ppo2.py:154][0m 475136 total steps have happened
[32m[20230204 15:42:36 @agent_ppo2.py:130][0m #------------------------ Iteration 232 --------------------------#
[32m[20230204 15:42:37 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:42:37 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:37 @agent_ppo2.py:194][0m |           0.0031 |          28.2680 |           3.9854 |
[32m[20230204 15:42:37 @agent_ppo2.py:194][0m |          -0.0063 |          12.7578 |           3.9832 |
[32m[20230204 15:42:37 @agent_ppo2.py:194][0m |           0.0400 |          20.2843 |           3.9769 |
[32m[20230204 15:42:37 @agent_ppo2.py:194][0m |          -0.0174 |          11.9502 |           3.9793 |
[32m[20230204 15:42:37 @agent_ppo2.py:194][0m |          -0.0114 |          10.4273 |           3.9768 |
[32m[20230204 15:42:37 @agent_ppo2.py:194][0m |           0.1112 |          10.4133 |           3.9762 |
[32m[20230204 15:42:37 @agent_ppo2.py:194][0m |          -0.0153 |          12.0734 |           3.9729 |
[32m[20230204 15:42:37 @agent_ppo2.py:194][0m |          -0.0127 |           9.6523 |           3.9767 |
[32m[20230204 15:42:37 @agent_ppo2.py:194][0m |          -0.0148 |           9.4219 |           3.9767 |
[32m[20230204 15:42:37 @agent_ppo2.py:194][0m |          -0.0134 |           9.3296 |           3.9733 |
[32m[20230204 15:42:37 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:42:38 @agent_ppo2.py:147][0m Average TRAINING episode reward: 208.99
[32m[20230204 15:42:38 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 254.05
[32m[20230204 15:42:38 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.27
[32m[20230204 15:42:38 @agent_ppo2.py:152][0m Total time:       6.45 min
[32m[20230204 15:42:38 @agent_ppo2.py:154][0m 477184 total steps have happened
[32m[20230204 15:42:38 @agent_ppo2.py:130][0m #------------------------ Iteration 233 --------------------------#
[32m[20230204 15:42:38 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:38 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:38 @agent_ppo2.py:194][0m |           0.0004 |          13.3698 |           4.1760 |
[32m[20230204 15:42:38 @agent_ppo2.py:194][0m |           0.0021 |          13.0837 |           4.1756 |
[32m[20230204 15:42:38 @agent_ppo2.py:194][0m |          -0.0059 |          11.8127 |           4.1758 |
[32m[20230204 15:42:38 @agent_ppo2.py:194][0m |          -0.0054 |          11.9625 |           4.1789 |
[32m[20230204 15:42:38 @agent_ppo2.py:194][0m |          -0.0063 |          12.1319 |           4.1785 |
[32m[20230204 15:42:38 @agent_ppo2.py:194][0m |          -0.0056 |          11.9489 |           4.1815 |
[32m[20230204 15:42:39 @agent_ppo2.py:194][0m |          -0.0041 |          11.9799 |           4.1803 |
[32m[20230204 15:42:39 @agent_ppo2.py:194][0m |          -0.0137 |          11.2556 |           4.1834 |
[32m[20230204 15:42:39 @agent_ppo2.py:194][0m |          -0.0112 |          11.1718 |           4.1879 |
[32m[20230204 15:42:39 @agent_ppo2.py:194][0m |          -0.0129 |          11.1020 |           4.1881 |
[32m[20230204 15:42:39 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:42:39 @agent_ppo2.py:147][0m Average TRAINING episode reward: 254.46
[32m[20230204 15:42:39 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.61
[32m[20230204 15:42:39 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 140.36
[32m[20230204 15:42:39 @agent_ppo2.py:152][0m Total time:       6.48 min
[32m[20230204 15:42:39 @agent_ppo2.py:154][0m 479232 total steps have happened
[32m[20230204 15:42:39 @agent_ppo2.py:130][0m #------------------------ Iteration 234 --------------------------#
[32m[20230204 15:42:39 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:42:39 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:40 @agent_ppo2.py:194][0m |          -0.0001 |          13.9755 |           4.2690 |
[32m[20230204 15:42:40 @agent_ppo2.py:194][0m |          -0.0050 |          12.9617 |           4.2625 |
[32m[20230204 15:42:40 @agent_ppo2.py:194][0m |          -0.0070 |          12.5699 |           4.2610 |
[32m[20230204 15:42:40 @agent_ppo2.py:194][0m |          -0.0080 |          12.3345 |           4.2585 |
[32m[20230204 15:42:40 @agent_ppo2.py:194][0m |          -0.0082 |          12.1472 |           4.2595 |
[32m[20230204 15:42:40 @agent_ppo2.py:194][0m |          -0.0090 |          12.0067 |           4.2604 |
[32m[20230204 15:42:40 @agent_ppo2.py:194][0m |          -0.0098 |          11.8860 |           4.2582 |
[32m[20230204 15:42:40 @agent_ppo2.py:194][0m |          -0.0098 |          11.8219 |           4.2653 |
[32m[20230204 15:42:40 @agent_ppo2.py:194][0m |          -0.0100 |          11.7452 |           4.2574 |
[32m[20230204 15:42:40 @agent_ppo2.py:194][0m |          -0.0109 |          11.6562 |           4.2613 |
[32m[20230204 15:42:40 @agent_ppo2.py:139][0m Policy update time: 0.95 s
[32m[20230204 15:42:41 @agent_ppo2.py:147][0m Average TRAINING episode reward: 253.00
[32m[20230204 15:42:41 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.88
[32m[20230204 15:42:41 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.59
[32m[20230204 15:42:41 @agent_ppo2.py:152][0m Total time:       6.50 min
[32m[20230204 15:42:41 @agent_ppo2.py:154][0m 481280 total steps have happened
[32m[20230204 15:42:41 @agent_ppo2.py:130][0m #------------------------ Iteration 235 --------------------------#
[32m[20230204 15:42:41 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:42:41 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:41 @agent_ppo2.py:194][0m |           0.0000 |          12.2651 |           4.2617 |
[32m[20230204 15:42:41 @agent_ppo2.py:194][0m |          -0.0039 |          11.6736 |           4.2589 |
[32m[20230204 15:42:41 @agent_ppo2.py:194][0m |          -0.0067 |          11.3072 |           4.2558 |
[32m[20230204 15:42:41 @agent_ppo2.py:194][0m |          -0.0081 |          11.0727 |           4.2552 |
[32m[20230204 15:42:41 @agent_ppo2.py:194][0m |          -0.0080 |          10.8824 |           4.2539 |
[32m[20230204 15:42:41 @agent_ppo2.py:194][0m |          -0.0090 |          10.7346 |           4.2535 |
[32m[20230204 15:42:41 @agent_ppo2.py:194][0m |          -0.0078 |          10.6651 |           4.2545 |
[32m[20230204 15:42:42 @agent_ppo2.py:194][0m |          -0.0096 |          10.4801 |           4.2537 |
[32m[20230204 15:42:42 @agent_ppo2.py:194][0m |          -0.0110 |          10.3088 |           4.2541 |
[32m[20230204 15:42:42 @agent_ppo2.py:194][0m |          -0.0106 |          10.3346 |           4.2560 |
[32m[20230204 15:42:42 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:42:42 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.70
[32m[20230204 15:42:42 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.87
[32m[20230204 15:42:42 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.57
[32m[20230204 15:42:42 @agent_ppo2.py:152][0m Total time:       6.53 min
[32m[20230204 15:42:42 @agent_ppo2.py:154][0m 483328 total steps have happened
[32m[20230204 15:42:42 @agent_ppo2.py:130][0m #------------------------ Iteration 236 --------------------------#
[32m[20230204 15:42:42 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:42:42 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:42 @agent_ppo2.py:194][0m |           0.0014 |          12.7783 |           4.2564 |
[32m[20230204 15:42:42 @agent_ppo2.py:194][0m |          -0.0031 |          12.0270 |           4.2469 |
[32m[20230204 15:42:43 @agent_ppo2.py:194][0m |          -0.0052 |          11.7559 |           4.2449 |
[32m[20230204 15:42:43 @agent_ppo2.py:194][0m |          -0.0067 |          11.5700 |           4.2442 |
[32m[20230204 15:42:43 @agent_ppo2.py:194][0m |          -0.0071 |          11.4320 |           4.2422 |
[32m[20230204 15:42:43 @agent_ppo2.py:194][0m |          -0.0083 |          11.3003 |           4.2407 |
[32m[20230204 15:42:43 @agent_ppo2.py:194][0m |          -0.0086 |          11.2297 |           4.2403 |
[32m[20230204 15:42:43 @agent_ppo2.py:194][0m |          -0.0093 |          11.1636 |           4.2378 |
[32m[20230204 15:42:43 @agent_ppo2.py:194][0m |          -0.0092 |          11.0005 |           4.2385 |
[32m[20230204 15:42:43 @agent_ppo2.py:194][0m |          -0.0105 |          10.9030 |           4.2351 |
[32m[20230204 15:42:43 @agent_ppo2.py:139][0m Policy update time: 0.91 s
[32m[20230204 15:42:43 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.09
[32m[20230204 15:42:43 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.88
[32m[20230204 15:42:43 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.24
[32m[20230204 15:42:43 @agent_ppo2.py:152][0m Total time:       6.55 min
[32m[20230204 15:42:43 @agent_ppo2.py:154][0m 485376 total steps have happened
[32m[20230204 15:42:43 @agent_ppo2.py:130][0m #------------------------ Iteration 237 --------------------------#
[32m[20230204 15:42:44 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:42:44 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:44 @agent_ppo2.py:194][0m |          -0.0018 |          12.2960 |           4.2234 |
[32m[20230204 15:42:44 @agent_ppo2.py:194][0m |          -0.0065 |          11.6731 |           4.2143 |
[32m[20230204 15:42:44 @agent_ppo2.py:194][0m |          -0.0064 |          11.3599 |           4.2172 |
[32m[20230204 15:42:44 @agent_ppo2.py:194][0m |          -0.0078 |          11.1522 |           4.2143 |
[32m[20230204 15:42:44 @agent_ppo2.py:194][0m |          -0.0039 |          11.3442 |           4.2113 |
[32m[20230204 15:42:44 @agent_ppo2.py:194][0m |          -0.0085 |          10.8781 |           4.2139 |
[32m[20230204 15:42:44 @agent_ppo2.py:194][0m |          -0.0043 |          11.5061 |           4.2130 |
[32m[20230204 15:42:44 @agent_ppo2.py:194][0m |          -0.0110 |          10.6640 |           4.2102 |
[32m[20230204 15:42:44 @agent_ppo2.py:194][0m |          -0.0099 |          10.5941 |           4.2112 |
[32m[20230204 15:42:45 @agent_ppo2.py:194][0m |          -0.0111 |          10.4991 |           4.2081 |
[32m[20230204 15:42:45 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:42:45 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.18
[32m[20230204 15:42:45 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.85
[32m[20230204 15:42:45 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.29
[32m[20230204 15:42:45 @agent_ppo2.py:152][0m Total time:       6.57 min
[32m[20230204 15:42:45 @agent_ppo2.py:154][0m 487424 total steps have happened
[32m[20230204 15:42:45 @agent_ppo2.py:130][0m #------------------------ Iteration 238 --------------------------#
[32m[20230204 15:42:45 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:42:45 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:45 @agent_ppo2.py:194][0m |           0.0018 |          12.0692 |           4.1824 |
[32m[20230204 15:42:45 @agent_ppo2.py:194][0m |          -0.0027 |          11.3927 |           4.1814 |
[32m[20230204 15:42:45 @agent_ppo2.py:194][0m |          -0.0047 |          11.0310 |           4.1800 |
[32m[20230204 15:42:45 @agent_ppo2.py:194][0m |          -0.0057 |          10.8663 |           4.1720 |
[32m[20230204 15:42:45 @agent_ppo2.py:194][0m |          -0.0049 |          10.7128 |           4.1703 |
[32m[20230204 15:42:46 @agent_ppo2.py:194][0m |          -0.0069 |          10.6275 |           4.1707 |
[32m[20230204 15:42:46 @agent_ppo2.py:194][0m |          -0.0076 |          10.4874 |           4.1730 |
[32m[20230204 15:42:46 @agent_ppo2.py:194][0m |          -0.0086 |          10.3878 |           4.1677 |
[32m[20230204 15:42:46 @agent_ppo2.py:194][0m |          -0.0084 |          10.3181 |           4.1667 |
[32m[20230204 15:42:46 @agent_ppo2.py:194][0m |          -0.0089 |          10.2418 |           4.1635 |
[32m[20230204 15:42:46 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:42:46 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.01
[32m[20230204 15:42:46 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.06
[32m[20230204 15:42:46 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.44
[32m[20230204 15:42:46 @agent_ppo2.py:152][0m Total time:       6.59 min
[32m[20230204 15:42:46 @agent_ppo2.py:154][0m 489472 total steps have happened
[32m[20230204 15:42:46 @agent_ppo2.py:130][0m #------------------------ Iteration 239 --------------------------#
[32m[20230204 15:42:46 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:42:46 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:47 @agent_ppo2.py:194][0m |          -0.0003 |          11.9676 |           4.2104 |
[32m[20230204 15:42:47 @agent_ppo2.py:194][0m |          -0.0049 |          11.0822 |           4.2111 |
[32m[20230204 15:42:47 @agent_ppo2.py:194][0m |          -0.0062 |          10.6579 |           4.2122 |
[32m[20230204 15:42:47 @agent_ppo2.py:194][0m |          -0.0074 |          10.2882 |           4.2133 |
[32m[20230204 15:42:47 @agent_ppo2.py:194][0m |          -0.0098 |           9.8837 |           4.2124 |
[32m[20230204 15:42:47 @agent_ppo2.py:194][0m |          -0.0087 |           9.5459 |           4.2133 |
[32m[20230204 15:42:47 @agent_ppo2.py:194][0m |          -0.0104 |           9.2498 |           4.2104 |
[32m[20230204 15:42:47 @agent_ppo2.py:194][0m |          -0.0115 |           9.0260 |           4.2146 |
[32m[20230204 15:42:47 @agent_ppo2.py:194][0m |          -0.0123 |           8.8349 |           4.2149 |
[32m[20230204 15:42:47 @agent_ppo2.py:194][0m |          -0.0109 |           8.6737 |           4.2114 |
[32m[20230204 15:42:47 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:42:47 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.85
[32m[20230204 15:42:47 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.68
[32m[20230204 15:42:47 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.51
[32m[20230204 15:42:47 @agent_ppo2.py:152][0m Total time:       6.62 min
[32m[20230204 15:42:47 @agent_ppo2.py:154][0m 491520 total steps have happened
[32m[20230204 15:42:47 @agent_ppo2.py:130][0m #------------------------ Iteration 240 --------------------------#
[32m[20230204 15:42:48 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:42:48 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:48 @agent_ppo2.py:194][0m |           0.0015 |          19.7241 |           4.2709 |
[32m[20230204 15:42:48 @agent_ppo2.py:194][0m |          -0.0051 |          12.2448 |           4.2637 |
[32m[20230204 15:42:48 @agent_ppo2.py:194][0m |          -0.0075 |          10.9476 |           4.2594 |
[32m[20230204 15:42:48 @agent_ppo2.py:194][0m |          -0.0103 |          10.4187 |           4.2582 |
[32m[20230204 15:42:48 @agent_ppo2.py:194][0m |          -0.0123 |          10.1659 |           4.2570 |
[32m[20230204 15:42:48 @agent_ppo2.py:194][0m |          -0.0100 |           9.8225 |           4.2544 |
[32m[20230204 15:42:48 @agent_ppo2.py:194][0m |          -0.0110 |           9.6555 |           4.2533 |
[32m[20230204 15:42:48 @agent_ppo2.py:194][0m |          -0.0109 |           9.5644 |           4.2521 |
[32m[20230204 15:42:48 @agent_ppo2.py:194][0m |          -0.0140 |           9.3799 |           4.2500 |
[32m[20230204 15:42:49 @agent_ppo2.py:194][0m |          -0.0098 |           9.1578 |           4.2519 |
[32m[20230204 15:42:49 @agent_ppo2.py:139][0m Policy update time: 0.79 s
[32m[20230204 15:42:49 @agent_ppo2.py:147][0m Average TRAINING episode reward: 195.60
[32m[20230204 15:42:49 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.43
[32m[20230204 15:42:49 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.47
[32m[20230204 15:42:49 @agent_ppo2.py:152][0m Total time:       6.64 min
[32m[20230204 15:42:49 @agent_ppo2.py:154][0m 493568 total steps have happened
[32m[20230204 15:42:49 @agent_ppo2.py:130][0m #------------------------ Iteration 241 --------------------------#
[32m[20230204 15:42:49 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:42:49 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:49 @agent_ppo2.py:194][0m |           0.0005 |          13.4646 |           4.2433 |
[32m[20230204 15:42:49 @agent_ppo2.py:194][0m |          -0.0072 |          12.2820 |           4.2333 |
[32m[20230204 15:42:49 @agent_ppo2.py:194][0m |          -0.0087 |          11.9882 |           4.2370 |
[32m[20230204 15:42:49 @agent_ppo2.py:194][0m |          -0.0116 |          11.6617 |           4.2330 |
[32m[20230204 15:42:49 @agent_ppo2.py:194][0m |          -0.0121 |          11.5409 |           4.2339 |
[32m[20230204 15:42:50 @agent_ppo2.py:194][0m |          -0.0103 |          11.6896 |           4.2377 |
[32m[20230204 15:42:50 @agent_ppo2.py:194][0m |          -0.0144 |          11.2073 |           4.2353 |
[32m[20230204 15:42:50 @agent_ppo2.py:194][0m |          -0.0153 |          11.0397 |           4.2345 |
[32m[20230204 15:42:50 @agent_ppo2.py:194][0m |          -0.0153 |          10.9323 |           4.2365 |
[32m[20230204 15:42:50 @agent_ppo2.py:194][0m |          -0.0153 |          10.8307 |           4.2343 |
[32m[20230204 15:42:50 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:42:50 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.03
[32m[20230204 15:42:50 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.97
[32m[20230204 15:42:50 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 74.87
[32m[20230204 15:42:50 @agent_ppo2.py:152][0m Total time:       6.66 min
[32m[20230204 15:42:50 @agent_ppo2.py:154][0m 495616 total steps have happened
[32m[20230204 15:42:50 @agent_ppo2.py:130][0m #------------------------ Iteration 242 --------------------------#
[32m[20230204 15:42:50 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:42:50 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:50 @agent_ppo2.py:194][0m |          -0.0036 |          12.7264 |           4.1513 |
[32m[20230204 15:42:51 @agent_ppo2.py:194][0m |          -0.0086 |          12.0371 |           4.1513 |
[32m[20230204 15:42:51 @agent_ppo2.py:194][0m |          -0.0081 |          11.6821 |           4.1521 |
[32m[20230204 15:42:51 @agent_ppo2.py:194][0m |          -0.0115 |          11.4261 |           4.1495 |
[32m[20230204 15:42:51 @agent_ppo2.py:194][0m |          -0.0108 |          11.2588 |           4.1502 |
[32m[20230204 15:42:51 @agent_ppo2.py:194][0m |          -0.0146 |          11.0216 |           4.1531 |
[32m[20230204 15:42:51 @agent_ppo2.py:194][0m |          -0.0112 |          10.8323 |           4.1522 |
[32m[20230204 15:42:51 @agent_ppo2.py:194][0m |          -0.0141 |          10.6856 |           4.1523 |
[32m[20230204 15:42:51 @agent_ppo2.py:194][0m |          -0.0136 |          10.5955 |           4.1535 |
[32m[20230204 15:42:51 @agent_ppo2.py:194][0m |          -0.0148 |          10.4311 |           4.1551 |
[32m[20230204 15:42:51 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:42:51 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.01
[32m[20230204 15:42:51 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.99
[32m[20230204 15:42:51 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.91
[32m[20230204 15:42:51 @agent_ppo2.py:152][0m Total time:       6.68 min
[32m[20230204 15:42:51 @agent_ppo2.py:154][0m 497664 total steps have happened
[32m[20230204 15:42:51 @agent_ppo2.py:130][0m #------------------------ Iteration 243 --------------------------#
[32m[20230204 15:42:52 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:42:52 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:52 @agent_ppo2.py:194][0m |           0.0024 |          21.5698 |           4.2322 |
[32m[20230204 15:42:52 @agent_ppo2.py:194][0m |          -0.0027 |          17.4234 |           4.2239 |
[32m[20230204 15:42:52 @agent_ppo2.py:194][0m |          -0.0045 |          16.3320 |           4.2154 |
[32m[20230204 15:42:52 @agent_ppo2.py:194][0m |          -0.0058 |          15.6408 |           4.2168 |
[32m[20230204 15:42:52 @agent_ppo2.py:194][0m |          -0.0065 |          15.2958 |           4.2086 |
[32m[20230204 15:42:52 @agent_ppo2.py:194][0m |          -0.0073 |          14.8026 |           4.2103 |
[32m[20230204 15:42:52 @agent_ppo2.py:194][0m |          -0.0077 |          14.6136 |           4.2094 |
[32m[20230204 15:42:53 @agent_ppo2.py:194][0m |          -0.0080 |          14.2807 |           4.2031 |
[32m[20230204 15:42:53 @agent_ppo2.py:194][0m |          -0.0087 |          14.0987 |           4.2048 |
[32m[20230204 15:42:53 @agent_ppo2.py:194][0m |          -0.0084 |          13.9036 |           4.2031 |
[32m[20230204 15:42:53 @agent_ppo2.py:139][0m Policy update time: 0.96 s
[32m[20230204 15:42:53 @agent_ppo2.py:147][0m Average TRAINING episode reward: 188.52
[32m[20230204 15:42:53 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.22
[32m[20230204 15:42:53 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.83
[32m[20230204 15:42:53 @agent_ppo2.py:152][0m Total time:       6.71 min
[32m[20230204 15:42:53 @agent_ppo2.py:154][0m 499712 total steps have happened
[32m[20230204 15:42:53 @agent_ppo2.py:130][0m #------------------------ Iteration 244 --------------------------#
[32m[20230204 15:42:53 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:42:53 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:53 @agent_ppo2.py:194][0m |           0.0033 |          13.4439 |           4.2492 |
[32m[20230204 15:42:53 @agent_ppo2.py:194][0m |          -0.0036 |          12.2675 |           4.2465 |
[32m[20230204 15:42:54 @agent_ppo2.py:194][0m |          -0.0047 |          11.8700 |           4.2439 |
[32m[20230204 15:42:54 @agent_ppo2.py:194][0m |          -0.0048 |          11.8919 |           4.2447 |
[32m[20230204 15:42:54 @agent_ppo2.py:194][0m |          -0.0079 |          11.4750 |           4.2437 |
[32m[20230204 15:42:54 @agent_ppo2.py:194][0m |          -0.0080 |          11.3580 |           4.2425 |
[32m[20230204 15:42:54 @agent_ppo2.py:194][0m |          -0.0083 |          11.2395 |           4.2422 |
[32m[20230204 15:42:54 @agent_ppo2.py:194][0m |          -0.0095 |          11.1722 |           4.2376 |
[32m[20230204 15:42:54 @agent_ppo2.py:194][0m |          -0.0110 |          11.0990 |           4.2404 |
[32m[20230204 15:42:54 @agent_ppo2.py:194][0m |          -0.0103 |          10.9893 |           4.2379 |
[32m[20230204 15:42:54 @agent_ppo2.py:139][0m Policy update time: 0.97 s
[32m[20230204 15:42:54 @agent_ppo2.py:147][0m Average TRAINING episode reward: 252.52
[32m[20230204 15:42:54 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.28
[32m[20230204 15:42:54 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.20
[32m[20230204 15:42:54 @agent_ppo2.py:152][0m Total time:       6.73 min
[32m[20230204 15:42:54 @agent_ppo2.py:154][0m 501760 total steps have happened
[32m[20230204 15:42:54 @agent_ppo2.py:130][0m #------------------------ Iteration 245 --------------------------#
[32m[20230204 15:42:55 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:42:55 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:55 @agent_ppo2.py:194][0m |          -0.0008 |          11.8960 |           4.2616 |
[32m[20230204 15:42:55 @agent_ppo2.py:194][0m |          -0.0031 |          11.1211 |           4.2487 |
[32m[20230204 15:42:55 @agent_ppo2.py:194][0m |          -0.0047 |          10.7668 |           4.2480 |
[32m[20230204 15:42:55 @agent_ppo2.py:194][0m |          -0.0070 |          10.5513 |           4.2470 |
[32m[20230204 15:42:55 @agent_ppo2.py:194][0m |          -0.0057 |          10.4076 |           4.2466 |
[32m[20230204 15:42:55 @agent_ppo2.py:194][0m |          -0.0060 |          10.2509 |           4.2445 |
[32m[20230204 15:42:55 @agent_ppo2.py:194][0m |          -0.0073 |          10.0856 |           4.2454 |
[32m[20230204 15:42:55 @agent_ppo2.py:194][0m |          -0.0069 |           9.9515 |           4.2436 |
[32m[20230204 15:42:55 @agent_ppo2.py:194][0m |          -0.0065 |           9.8551 |           4.2421 |
[32m[20230204 15:42:56 @agent_ppo2.py:194][0m |          -0.0079 |           9.7745 |           4.2435 |
[32m[20230204 15:42:56 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:42:56 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.00
[32m[20230204 15:42:56 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.23
[32m[20230204 15:42:56 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 120.33
[32m[20230204 15:42:56 @agent_ppo2.py:152][0m Total time:       6.76 min
[32m[20230204 15:42:56 @agent_ppo2.py:154][0m 503808 total steps have happened
[32m[20230204 15:42:56 @agent_ppo2.py:130][0m #------------------------ Iteration 246 --------------------------#
[32m[20230204 15:42:56 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:42:56 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:56 @agent_ppo2.py:194][0m |           0.0022 |          11.1796 |           4.3054 |
[32m[20230204 15:42:56 @agent_ppo2.py:194][0m |          -0.0047 |          10.4941 |           4.2997 |
[32m[20230204 15:42:56 @agent_ppo2.py:194][0m |          -0.0028 |          10.4495 |           4.2947 |
[32m[20230204 15:42:57 @agent_ppo2.py:194][0m |          -0.0091 |          10.1294 |           4.2984 |
[32m[20230204 15:42:57 @agent_ppo2.py:194][0m |          -0.0102 |          10.0163 |           4.2942 |
[32m[20230204 15:42:57 @agent_ppo2.py:194][0m |          -0.0076 |          10.1002 |           4.2907 |
[32m[20230204 15:42:57 @agent_ppo2.py:194][0m |          -0.0091 |           9.8827 |           4.2933 |
[32m[20230204 15:42:57 @agent_ppo2.py:194][0m |          -0.0117 |           9.7986 |           4.2902 |
[32m[20230204 15:42:57 @agent_ppo2.py:194][0m |          -0.0094 |           9.7797 |           4.2873 |
[32m[20230204 15:42:57 @agent_ppo2.py:194][0m |          -0.0125 |           9.6817 |           4.2885 |
[32m[20230204 15:42:57 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:42:57 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.33
[32m[20230204 15:42:57 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.43
[32m[20230204 15:42:57 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.52
[32m[20230204 15:42:57 @agent_ppo2.py:152][0m Total time:       6.78 min
[32m[20230204 15:42:57 @agent_ppo2.py:154][0m 505856 total steps have happened
[32m[20230204 15:42:57 @agent_ppo2.py:130][0m #------------------------ Iteration 247 --------------------------#
[32m[20230204 15:42:58 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:42:58 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:58 @agent_ppo2.py:194][0m |          -0.0018 |          33.7334 |           4.2039 |
[32m[20230204 15:42:58 @agent_ppo2.py:194][0m |          -0.0076 |          25.9229 |           4.1904 |
[32m[20230204 15:42:58 @agent_ppo2.py:194][0m |          -0.0111 |          24.0917 |           4.1895 |
[32m[20230204 15:42:58 @agent_ppo2.py:194][0m |          -0.0124 |          22.8972 |           4.1889 |
[32m[20230204 15:42:58 @agent_ppo2.py:194][0m |          -0.0121 |          22.4950 |           4.1853 |
[32m[20230204 15:42:58 @agent_ppo2.py:194][0m |          -0.0115 |          22.3998 |           4.1834 |
[32m[20230204 15:42:58 @agent_ppo2.py:194][0m |          -0.0153 |          21.4430 |           4.1792 |
[32m[20230204 15:42:58 @agent_ppo2.py:194][0m |          -0.0158 |          20.6890 |           4.1794 |
[32m[20230204 15:42:58 @agent_ppo2.py:194][0m |          -0.0159 |          20.2797 |           4.1766 |
[32m[20230204 15:42:58 @agent_ppo2.py:194][0m |          -0.0170 |          20.0313 |           4.1754 |
[32m[20230204 15:42:58 @agent_ppo2.py:139][0m Policy update time: 0.91 s
[32m[20230204 15:42:59 @agent_ppo2.py:147][0m Average TRAINING episode reward: 140.83
[32m[20230204 15:42:59 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.36
[32m[20230204 15:42:59 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.08
[32m[20230204 15:42:59 @agent_ppo2.py:152][0m Total time:       6.80 min
[32m[20230204 15:42:59 @agent_ppo2.py:154][0m 507904 total steps have happened
[32m[20230204 15:42:59 @agent_ppo2.py:130][0m #------------------------ Iteration 248 --------------------------#
[32m[20230204 15:42:59 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:42:59 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:42:59 @agent_ppo2.py:194][0m |           0.0182 |          13.5426 |           4.1918 |
[32m[20230204 15:42:59 @agent_ppo2.py:194][0m |          -0.0077 |          11.5657 |           4.1826 |
[32m[20230204 15:42:59 @agent_ppo2.py:194][0m |           0.0154 |          13.3052 |           4.1805 |
[32m[20230204 15:42:59 @agent_ppo2.py:194][0m |          -0.0060 |          10.9956 |           4.1711 |
[32m[20230204 15:42:59 @agent_ppo2.py:194][0m |          -0.0083 |          10.7435 |           4.1744 |
[32m[20230204 15:43:00 @agent_ppo2.py:194][0m |          -0.0090 |          10.6289 |           4.1720 |
[32m[20230204 15:43:00 @agent_ppo2.py:194][0m |          -0.0111 |          10.4822 |           4.1740 |
[32m[20230204 15:43:00 @agent_ppo2.py:194][0m |          -0.0125 |          10.3807 |           4.1690 |
[32m[20230204 15:43:00 @agent_ppo2.py:194][0m |           0.0107 |          11.9906 |           4.1674 |
[32m[20230204 15:43:00 @agent_ppo2.py:194][0m |          -0.0129 |          10.2197 |           4.1628 |
[32m[20230204 15:43:00 @agent_ppo2.py:139][0m Policy update time: 0.91 s
[32m[20230204 15:43:00 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.94
[32m[20230204 15:43:00 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.52
[32m[20230204 15:43:00 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 277.57
[32m[20230204 15:43:00 @agent_ppo2.py:152][0m Total time:       6.83 min
[32m[20230204 15:43:00 @agent_ppo2.py:154][0m 509952 total steps have happened
[32m[20230204 15:43:00 @agent_ppo2.py:130][0m #------------------------ Iteration 249 --------------------------#
[32m[20230204 15:43:00 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:43:00 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:00 @agent_ppo2.py:194][0m |           0.0005 |          27.0248 |           4.1807 |
[32m[20230204 15:43:01 @agent_ppo2.py:194][0m |          -0.0062 |          20.7517 |           4.1754 |
[32m[20230204 15:43:01 @agent_ppo2.py:194][0m |          -0.0073 |          19.7218 |           4.1700 |
[32m[20230204 15:43:01 @agent_ppo2.py:194][0m |          -0.0089 |          18.3998 |           4.1686 |
[32m[20230204 15:43:01 @agent_ppo2.py:194][0m |          -0.0094 |          17.5495 |           4.1613 |
[32m[20230204 15:43:01 @agent_ppo2.py:194][0m |          -0.0101 |          17.0671 |           4.1616 |
[32m[20230204 15:43:01 @agent_ppo2.py:194][0m |          -0.0113 |          16.6231 |           4.1623 |
[32m[20230204 15:43:01 @agent_ppo2.py:194][0m |          -0.0118 |          15.7083 |           4.1608 |
[32m[20230204 15:43:01 @agent_ppo2.py:194][0m |          -0.0121 |          15.1745 |           4.1584 |
[32m[20230204 15:43:01 @agent_ppo2.py:194][0m |          -0.0131 |          14.6326 |           4.1592 |
[32m[20230204 15:43:01 @agent_ppo2.py:139][0m Policy update time: 0.82 s
[32m[20230204 15:43:01 @agent_ppo2.py:147][0m Average TRAINING episode reward: 201.12
[32m[20230204 15:43:01 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.43
[32m[20230204 15:43:01 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 9.85
[32m[20230204 15:43:01 @agent_ppo2.py:152][0m Total time:       6.85 min
[32m[20230204 15:43:01 @agent_ppo2.py:154][0m 512000 total steps have happened
[32m[20230204 15:43:01 @agent_ppo2.py:130][0m #------------------------ Iteration 250 --------------------------#
[32m[20230204 15:43:02 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:43:02 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:02 @agent_ppo2.py:194][0m |           0.0000 |          14.0850 |           4.2301 |
[32m[20230204 15:43:02 @agent_ppo2.py:194][0m |          -0.0054 |          12.7998 |           4.2231 |
[32m[20230204 15:43:02 @agent_ppo2.py:194][0m |          -0.0054 |          12.5272 |           4.2190 |
[32m[20230204 15:43:02 @agent_ppo2.py:194][0m |          -0.0066 |          12.3392 |           4.2180 |
[32m[20230204 15:43:02 @agent_ppo2.py:194][0m |          -0.0094 |          12.3493 |           4.2147 |
[32m[20230204 15:43:02 @agent_ppo2.py:194][0m |          -0.0093 |          11.9138 |           4.2206 |
[32m[20230204 15:43:02 @agent_ppo2.py:194][0m |          -0.0099 |          11.7828 |           4.2191 |
[32m[20230204 15:43:02 @agent_ppo2.py:194][0m |          -0.0113 |          11.6686 |           4.2159 |
[32m[20230204 15:43:02 @agent_ppo2.py:194][0m |          -0.0114 |          11.5856 |           4.2176 |
[32m[20230204 15:43:02 @agent_ppo2.py:194][0m |          -0.0095 |          11.7055 |           4.2141 |
[32m[20230204 15:43:02 @agent_ppo2.py:139][0m Policy update time: 0.91 s
[32m[20230204 15:43:03 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.00
[32m[20230204 15:43:03 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.42
[32m[20230204 15:43:03 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 39.77
[32m[20230204 15:43:03 @agent_ppo2.py:152][0m Total time:       6.87 min
[32m[20230204 15:43:03 @agent_ppo2.py:154][0m 514048 total steps have happened
[32m[20230204 15:43:03 @agent_ppo2.py:130][0m #------------------------ Iteration 251 --------------------------#
[32m[20230204 15:43:03 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:43:03 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:03 @agent_ppo2.py:194][0m |          -0.0018 |          49.2888 |           4.2261 |
[32m[20230204 15:43:03 @agent_ppo2.py:194][0m |          -0.0106 |          26.0816 |           4.2127 |
[32m[20230204 15:43:03 @agent_ppo2.py:194][0m |          -0.0106 |          23.9580 |           4.2104 |
[32m[20230204 15:43:03 @agent_ppo2.py:194][0m |          -0.0167 |          21.8310 |           4.2052 |
[32m[20230204 15:43:03 @agent_ppo2.py:194][0m |          -0.0178 |          20.3061 |           4.1979 |
[32m[20230204 15:43:03 @agent_ppo2.py:194][0m |          -0.0191 |          18.8094 |           4.1989 |
[32m[20230204 15:43:03 @agent_ppo2.py:194][0m |          -0.0178 |          17.7220 |           4.1976 |
[32m[20230204 15:43:03 @agent_ppo2.py:194][0m |          -0.0203 |          16.8646 |           4.1953 |
[32m[20230204 15:43:03 @agent_ppo2.py:194][0m |          -0.0197 |          16.3642 |           4.1965 |
[32m[20230204 15:43:03 @agent_ppo2.py:194][0m |          -0.0190 |          15.7319 |           4.1923 |
[32m[20230204 15:43:03 @agent_ppo2.py:139][0m Policy update time: 0.67 s
[32m[20230204 15:43:04 @agent_ppo2.py:147][0m Average TRAINING episode reward: 107.46
[32m[20230204 15:43:04 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.32
[32m[20230204 15:43:04 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.43
[32m[20230204 15:43:04 @agent_ppo2.py:152][0m Total time:       6.89 min
[32m[20230204 15:43:04 @agent_ppo2.py:154][0m 516096 total steps have happened
[32m[20230204 15:43:04 @agent_ppo2.py:130][0m #------------------------ Iteration 252 --------------------------#
[32m[20230204 15:43:04 @agent_ppo2.py:136][0m Sampling time: 0.33 s by 4 slaves
[32m[20230204 15:43:04 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:04 @agent_ppo2.py:194][0m |          -0.0014 |          59.7047 |           4.1507 |
[32m[20230204 15:43:04 @agent_ppo2.py:194][0m |          -0.0073 |          46.1914 |           4.1468 |
[32m[20230204 15:43:04 @agent_ppo2.py:194][0m |          -0.0104 |          42.8511 |           4.1509 |
[32m[20230204 15:43:04 @agent_ppo2.py:194][0m |          -0.0102 |          40.2204 |           4.1479 |
[32m[20230204 15:43:05 @agent_ppo2.py:194][0m |          -0.0121 |          37.7316 |           4.1451 |
[32m[20230204 15:43:05 @agent_ppo2.py:194][0m |          -0.0125 |          36.2553 |           4.1440 |
[32m[20230204 15:43:05 @agent_ppo2.py:194][0m |          -0.0139 |          34.6606 |           4.1437 |
[32m[20230204 15:43:05 @agent_ppo2.py:194][0m |          -0.0145 |          33.4266 |           4.1437 |
[32m[20230204 15:43:05 @agent_ppo2.py:194][0m |          -0.0152 |          32.3221 |           4.1428 |
[32m[20230204 15:43:05 @agent_ppo2.py:194][0m |          -0.0153 |          31.4514 |           4.1452 |
[32m[20230204 15:43:05 @agent_ppo2.py:139][0m Policy update time: 1.00 s
[32m[20230204 15:43:05 @agent_ppo2.py:147][0m Average TRAINING episode reward: 209.55
[32m[20230204 15:43:05 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.38
[32m[20230204 15:43:05 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 158.20
[32m[20230204 15:43:05 @agent_ppo2.py:152][0m Total time:       6.91 min
[32m[20230204 15:43:05 @agent_ppo2.py:154][0m 518144 total steps have happened
[32m[20230204 15:43:05 @agent_ppo2.py:130][0m #------------------------ Iteration 253 --------------------------#
[32m[20230204 15:43:05 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:43:06 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:06 @agent_ppo2.py:194][0m |           0.0015 |          15.5114 |           4.2510 |
[32m[20230204 15:43:06 @agent_ppo2.py:194][0m |          -0.0042 |          13.8370 |           4.2460 |
[32m[20230204 15:43:06 @agent_ppo2.py:194][0m |          -0.0065 |          13.3121 |           4.2411 |
[32m[20230204 15:43:06 @agent_ppo2.py:194][0m |          -0.0075 |          12.9506 |           4.2414 |
[32m[20230204 15:43:06 @agent_ppo2.py:194][0m |          -0.0095 |          12.6604 |           4.2424 |
[32m[20230204 15:43:06 @agent_ppo2.py:194][0m |          -0.0078 |          12.4245 |           4.2406 |
[32m[20230204 15:43:06 @agent_ppo2.py:194][0m |          -0.0061 |          12.4241 |           4.2415 |
[32m[20230204 15:43:06 @agent_ppo2.py:194][0m |          -0.0121 |          12.0444 |           4.2433 |
[32m[20230204 15:43:06 @agent_ppo2.py:194][0m |          -0.0105 |          11.9230 |           4.2444 |
[32m[20230204 15:43:06 @agent_ppo2.py:194][0m |          -0.0121 |          11.7100 |           4.2446 |
[32m[20230204 15:43:06 @agent_ppo2.py:139][0m Policy update time: 0.92 s
[32m[20230204 15:43:07 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.57
[32m[20230204 15:43:07 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.17
[32m[20230204 15:43:07 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.60
[32m[20230204 15:43:07 @agent_ppo2.py:152][0m Total time:       6.94 min
[32m[20230204 15:43:07 @agent_ppo2.py:154][0m 520192 total steps have happened
[32m[20230204 15:43:07 @agent_ppo2.py:130][0m #------------------------ Iteration 254 --------------------------#
[32m[20230204 15:43:07 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:43:07 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:07 @agent_ppo2.py:194][0m |          -0.0014 |          12.5871 |           4.2043 |
[32m[20230204 15:43:07 @agent_ppo2.py:194][0m |          -0.0067 |          11.8414 |           4.1861 |
[32m[20230204 15:43:07 @agent_ppo2.py:194][0m |          -0.0077 |          11.4854 |           4.1856 |
[32m[20230204 15:43:07 @agent_ppo2.py:194][0m |          -0.0108 |          11.1575 |           4.1830 |
[32m[20230204 15:43:07 @agent_ppo2.py:194][0m |          -0.0111 |          10.9089 |           4.1829 |
[32m[20230204 15:43:07 @agent_ppo2.py:194][0m |          -0.0180 |          10.7966 |           4.1857 |
[32m[20230204 15:43:08 @agent_ppo2.py:194][0m |          -0.0150 |          10.5604 |           4.1754 |
[32m[20230204 15:43:08 @agent_ppo2.py:194][0m |          -0.0077 |          10.5983 |           4.1815 |
[32m[20230204 15:43:08 @agent_ppo2.py:194][0m |          -0.0140 |          10.2230 |           4.1811 |
[32m[20230204 15:43:08 @agent_ppo2.py:194][0m |          -0.0070 |          11.0287 |           4.1798 |
[32m[20230204 15:43:08 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:43:08 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.15
[32m[20230204 15:43:08 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.44
[32m[20230204 15:43:08 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.85
[32m[20230204 15:43:08 @agent_ppo2.py:152][0m Total time:       6.96 min
[32m[20230204 15:43:08 @agent_ppo2.py:154][0m 522240 total steps have happened
[32m[20230204 15:43:08 @agent_ppo2.py:130][0m #------------------------ Iteration 255 --------------------------#
[32m[20230204 15:43:08 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:43:08 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:08 @agent_ppo2.py:194][0m |          -0.0005 |          13.5109 |           4.1269 |
[32m[20230204 15:43:08 @agent_ppo2.py:194][0m |          -0.0072 |          11.9847 |           4.1209 |
[32m[20230204 15:43:09 @agent_ppo2.py:194][0m |          -0.0083 |          11.5007 |           4.1169 |
[32m[20230204 15:43:09 @agent_ppo2.py:194][0m |          -0.0119 |          11.2031 |           4.1185 |
[32m[20230204 15:43:09 @agent_ppo2.py:194][0m |          -0.0115 |          11.0305 |           4.1183 |
[32m[20230204 15:43:09 @agent_ppo2.py:194][0m |          -0.0123 |          10.6618 |           4.1227 |
[32m[20230204 15:43:09 @agent_ppo2.py:194][0m |          -0.0123 |          10.5038 |           4.1192 |
[32m[20230204 15:43:09 @agent_ppo2.py:194][0m |          -0.0138 |          10.3380 |           4.1237 |
[32m[20230204 15:43:09 @agent_ppo2.py:194][0m |          -0.0128 |          10.2021 |           4.1206 |
[32m[20230204 15:43:09 @agent_ppo2.py:194][0m |          -0.0140 |          10.0768 |           4.1223 |
[32m[20230204 15:43:09 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:43:09 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.53
[32m[20230204 15:43:09 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.86
[32m[20230204 15:43:09 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.00
[32m[20230204 15:43:09 @agent_ppo2.py:152][0m Total time:       6.98 min
[32m[20230204 15:43:09 @agent_ppo2.py:154][0m 524288 total steps have happened
[32m[20230204 15:43:09 @agent_ppo2.py:130][0m #------------------------ Iteration 256 --------------------------#
[32m[20230204 15:43:10 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:43:10 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:10 @agent_ppo2.py:194][0m |           0.0022 |          11.6530 |           4.2957 |
[32m[20230204 15:43:10 @agent_ppo2.py:194][0m |          -0.0020 |          10.2997 |           4.2924 |
[32m[20230204 15:43:10 @agent_ppo2.py:194][0m |          -0.0043 |           9.6164 |           4.2906 |
[32m[20230204 15:43:10 @agent_ppo2.py:194][0m |          -0.0035 |           9.2321 |           4.2852 |
[32m[20230204 15:43:10 @agent_ppo2.py:194][0m |          -0.0044 |           8.8931 |           4.2854 |
[32m[20230204 15:43:10 @agent_ppo2.py:194][0m |          -0.0053 |           8.6549 |           4.2786 |
[32m[20230204 15:43:10 @agent_ppo2.py:194][0m |          -0.0059 |           8.4210 |           4.2823 |
[32m[20230204 15:43:10 @agent_ppo2.py:194][0m |          -0.0064 |           8.1683 |           4.2805 |
[32m[20230204 15:43:10 @agent_ppo2.py:194][0m |          -0.0064 |           7.9473 |           4.2800 |
[32m[20230204 15:43:11 @agent_ppo2.py:194][0m |          -0.0066 |           7.6583 |           4.2769 |
[32m[20230204 15:43:11 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:43:11 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.15
[32m[20230204 15:43:11 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.59
[32m[20230204 15:43:11 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.62
[32m[20230204 15:43:11 @agent_ppo2.py:152][0m Total time:       7.00 min
[32m[20230204 15:43:11 @agent_ppo2.py:154][0m 526336 total steps have happened
[32m[20230204 15:43:11 @agent_ppo2.py:130][0m #------------------------ Iteration 257 --------------------------#
[32m[20230204 15:43:11 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:11 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:11 @agent_ppo2.py:194][0m |          -0.0008 |          14.2706 |           4.2632 |
[32m[20230204 15:43:11 @agent_ppo2.py:194][0m |          -0.0057 |          12.8881 |           4.2531 |
[32m[20230204 15:43:11 @agent_ppo2.py:194][0m |          -0.0073 |          12.2225 |           4.2473 |
[32m[20230204 15:43:11 @agent_ppo2.py:194][0m |          -0.0065 |          11.8368 |           4.2452 |
[32m[20230204 15:43:11 @agent_ppo2.py:194][0m |          -0.0098 |          11.4354 |           4.2397 |
[32m[20230204 15:43:12 @agent_ppo2.py:194][0m |          -0.0066 |          11.4791 |           4.2411 |
[32m[20230204 15:43:12 @agent_ppo2.py:194][0m |          -0.0110 |          10.9404 |           4.2433 |
[32m[20230204 15:43:12 @agent_ppo2.py:194][0m |          -0.0119 |          10.7578 |           4.2424 |
[32m[20230204 15:43:12 @agent_ppo2.py:194][0m |          -0.0115 |          10.5847 |           4.2372 |
[32m[20230204 15:43:12 @agent_ppo2.py:194][0m |          -0.0123 |          10.4621 |           4.2418 |
[32m[20230204 15:43:12 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:43:12 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.58
[32m[20230204 15:43:12 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.01
[32m[20230204 15:43:12 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 276.41
[32m[20230204 15:43:12 @agent_ppo2.py:152][0m Total time:       7.03 min
[32m[20230204 15:43:12 @agent_ppo2.py:154][0m 528384 total steps have happened
[32m[20230204 15:43:12 @agent_ppo2.py:130][0m #------------------------ Iteration 258 --------------------------#
[32m[20230204 15:43:12 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:12 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:12 @agent_ppo2.py:194][0m |           0.0015 |          14.0360 |           4.2734 |
[32m[20230204 15:43:13 @agent_ppo2.py:194][0m |          -0.0029 |          13.1621 |           4.2729 |
[32m[20230204 15:43:13 @agent_ppo2.py:194][0m |          -0.0048 |          12.9798 |           4.2708 |
[32m[20230204 15:43:13 @agent_ppo2.py:194][0m |          -0.0053 |          12.8281 |           4.2736 |
[32m[20230204 15:43:13 @agent_ppo2.py:194][0m |          -0.0050 |          12.8441 |           4.2716 |
[32m[20230204 15:43:13 @agent_ppo2.py:194][0m |          -0.0066 |          12.7695 |           4.2730 |
[32m[20230204 15:43:13 @agent_ppo2.py:194][0m |          -0.0083 |          12.5253 |           4.2699 |
[32m[20230204 15:43:13 @agent_ppo2.py:194][0m |          -0.0084 |          12.4400 |           4.2683 |
[32m[20230204 15:43:13 @agent_ppo2.py:194][0m |          -0.0093 |          12.4005 |           4.2707 |
[32m[20230204 15:43:13 @agent_ppo2.py:194][0m |          -0.0097 |          12.3120 |           4.2650 |
[32m[20230204 15:43:13 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:43:13 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.00
[32m[20230204 15:43:13 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.52
[32m[20230204 15:43:13 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.42
[32m[20230204 15:43:13 @agent_ppo2.py:152][0m Total time:       7.05 min
[32m[20230204 15:43:13 @agent_ppo2.py:154][0m 530432 total steps have happened
[32m[20230204 15:43:13 @agent_ppo2.py:130][0m #------------------------ Iteration 259 --------------------------#
[32m[20230204 15:43:14 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:43:14 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:14 @agent_ppo2.py:194][0m |           0.0005 |          11.7642 |           4.2229 |
[32m[20230204 15:43:14 @agent_ppo2.py:194][0m |          -0.0052 |          10.5264 |           4.2178 |
[32m[20230204 15:43:14 @agent_ppo2.py:194][0m |          -0.0032 |           9.9808 |           4.2154 |
[32m[20230204 15:43:14 @agent_ppo2.py:194][0m |          -0.0113 |           9.6042 |           4.2093 |
[32m[20230204 15:43:14 @agent_ppo2.py:194][0m |          -0.0054 |           9.2457 |           4.2121 |
[32m[20230204 15:43:14 @agent_ppo2.py:194][0m |          -0.0020 |           9.4890 |           4.2073 |
[32m[20230204 15:43:14 @agent_ppo2.py:194][0m |          -0.0119 |           8.7863 |           4.2025 |
[32m[20230204 15:43:14 @agent_ppo2.py:194][0m |          -0.0092 |           8.5607 |           4.2095 |
[32m[20230204 15:43:14 @agent_ppo2.py:194][0m |          -0.0115 |           8.3597 |           4.2076 |
[32m[20230204 15:43:15 @agent_ppo2.py:194][0m |          -0.0097 |           8.2723 |           4.2085 |
[32m[20230204 15:43:15 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:43:15 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.58
[32m[20230204 15:43:15 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.29
[32m[20230204 15:43:15 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.15
[32m[20230204 15:43:15 @agent_ppo2.py:152][0m Total time:       7.07 min
[32m[20230204 15:43:15 @agent_ppo2.py:154][0m 532480 total steps have happened
[32m[20230204 15:43:15 @agent_ppo2.py:130][0m #------------------------ Iteration 260 --------------------------#
[32m[20230204 15:43:15 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:43:15 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:15 @agent_ppo2.py:194][0m |           0.0004 |          11.8099 |           4.2883 |
[32m[20230204 15:43:15 @agent_ppo2.py:194][0m |          -0.0052 |          10.0244 |           4.2798 |
[32m[20230204 15:43:15 @agent_ppo2.py:194][0m |          -0.0063 |           9.5061 |           4.2781 |
[32m[20230204 15:43:15 @agent_ppo2.py:194][0m |          -0.0075 |           9.1339 |           4.2704 |
[32m[20230204 15:43:15 @agent_ppo2.py:194][0m |          -0.0096 |           8.8410 |           4.2693 |
[32m[20230204 15:43:16 @agent_ppo2.py:194][0m |          -0.0105 |           8.6300 |           4.2717 |
[32m[20230204 15:43:16 @agent_ppo2.py:194][0m |          -0.0089 |           8.4725 |           4.2751 |
[32m[20230204 15:43:16 @agent_ppo2.py:194][0m |          -0.0100 |           8.2790 |           4.2796 |
[32m[20230204 15:43:16 @agent_ppo2.py:194][0m |          -0.0124 |           8.0874 |           4.2754 |
[32m[20230204 15:43:16 @agent_ppo2.py:194][0m |          -0.0124 |           7.9495 |           4.2787 |
[32m[20230204 15:43:16 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:43:16 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.30
[32m[20230204 15:43:16 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.16
[32m[20230204 15:43:16 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.70
[32m[20230204 15:43:16 @agent_ppo2.py:152][0m Total time:       7.09 min
[32m[20230204 15:43:16 @agent_ppo2.py:154][0m 534528 total steps have happened
[32m[20230204 15:43:16 @agent_ppo2.py:130][0m #------------------------ Iteration 261 --------------------------#
[32m[20230204 15:43:16 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:16 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:16 @agent_ppo2.py:194][0m |           0.0013 |          14.3943 |           4.2758 |
[32m[20230204 15:43:17 @agent_ppo2.py:194][0m |          -0.0057 |          13.4747 |           4.2758 |
[32m[20230204 15:43:17 @agent_ppo2.py:194][0m |          -0.0074 |          13.0306 |           4.2690 |
[32m[20230204 15:43:17 @agent_ppo2.py:194][0m |          -0.0094 |          12.7875 |           4.2663 |
[32m[20230204 15:43:17 @agent_ppo2.py:194][0m |          -0.0087 |          12.6533 |           4.2659 |
[32m[20230204 15:43:17 @agent_ppo2.py:194][0m |          -0.0107 |          12.3962 |           4.2616 |
[32m[20230204 15:43:17 @agent_ppo2.py:194][0m |          -0.0108 |          12.1922 |           4.2626 |
[32m[20230204 15:43:17 @agent_ppo2.py:194][0m |          -0.0124 |          12.0407 |           4.2583 |
[32m[20230204 15:43:17 @agent_ppo2.py:194][0m |          -0.0116 |          12.0023 |           4.2558 |
[32m[20230204 15:43:17 @agent_ppo2.py:194][0m |          -0.0123 |          11.8337 |           4.2589 |
[32m[20230204 15:43:17 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:43:17 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.30
[32m[20230204 15:43:17 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.08
[32m[20230204 15:43:17 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.90
[32m[20230204 15:43:17 @agent_ppo2.py:152][0m Total time:       7.12 min
[32m[20230204 15:43:17 @agent_ppo2.py:154][0m 536576 total steps have happened
[32m[20230204 15:43:17 @agent_ppo2.py:130][0m #------------------------ Iteration 262 --------------------------#
[32m[20230204 15:43:18 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:43:18 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:18 @agent_ppo2.py:194][0m |           0.0010 |           8.5753 |           4.2747 |
[32m[20230204 15:43:18 @agent_ppo2.py:194][0m |          -0.0036 |           6.1232 |           4.2717 |
[32m[20230204 15:43:18 @agent_ppo2.py:194][0m |          -0.0064 |           5.4371 |           4.2630 |
[32m[20230204 15:43:18 @agent_ppo2.py:194][0m |          -0.0045 |           5.0579 |           4.2664 |
[32m[20230204 15:43:18 @agent_ppo2.py:194][0m |          -0.0078 |           4.6770 |           4.2625 |
[32m[20230204 15:43:18 @agent_ppo2.py:194][0m |          -0.0084 |           4.3447 |           4.2681 |
[32m[20230204 15:43:18 @agent_ppo2.py:194][0m |          -0.0113 |           4.1199 |           4.2672 |
[32m[20230204 15:43:18 @agent_ppo2.py:194][0m |          -0.0117 |           3.9357 |           4.2681 |
[32m[20230204 15:43:18 @agent_ppo2.py:194][0m |          -0.0114 |           3.7400 |           4.2630 |
[32m[20230204 15:43:19 @agent_ppo2.py:194][0m |          -0.0145 |           3.6185 |           4.2650 |
[32m[20230204 15:43:19 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:43:19 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.97
[32m[20230204 15:43:19 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.13
[32m[20230204 15:43:19 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 281.11
[32m[20230204 15:43:19 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 281.11
[32m[20230204 15:43:19 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 281.11
[32m[20230204 15:43:19 @agent_ppo2.py:152][0m Total time:       7.14 min
[32m[20230204 15:43:19 @agent_ppo2.py:154][0m 538624 total steps have happened
[32m[20230204 15:43:19 @agent_ppo2.py:130][0m #------------------------ Iteration 263 --------------------------#
[32m[20230204 15:43:19 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:19 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:19 @agent_ppo2.py:194][0m |          -0.0004 |          13.1344 |           4.3502 |
[32m[20230204 15:43:19 @agent_ppo2.py:194][0m |          -0.0052 |          12.0620 |           4.3427 |
[32m[20230204 15:43:19 @agent_ppo2.py:194][0m |          -0.0065 |          11.6245 |           4.3372 |
[32m[20230204 15:43:19 @agent_ppo2.py:194][0m |          -0.0077 |          11.2913 |           4.3366 |
[32m[20230204 15:43:19 @agent_ppo2.py:194][0m |          -0.0093 |          10.9554 |           4.3349 |
[32m[20230204 15:43:19 @agent_ppo2.py:194][0m |          -0.0080 |          10.7910 |           4.3370 |
[32m[20230204 15:43:20 @agent_ppo2.py:194][0m |          -0.0107 |          10.4638 |           4.3312 |
[32m[20230204 15:43:20 @agent_ppo2.py:194][0m |          -0.0102 |          10.2448 |           4.3317 |
[32m[20230204 15:43:20 @agent_ppo2.py:194][0m |          -0.0103 |          10.0573 |           4.3367 |
[32m[20230204 15:43:20 @agent_ppo2.py:194][0m |          -0.0113 |           9.7748 |           4.3337 |
[32m[20230204 15:43:20 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:43:20 @agent_ppo2.py:147][0m Average TRAINING episode reward: 263.33
[32m[20230204 15:43:20 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 267.46
[32m[20230204 15:43:20 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.17
[32m[20230204 15:43:20 @agent_ppo2.py:152][0m Total time:       7.16 min
[32m[20230204 15:43:20 @agent_ppo2.py:154][0m 540672 total steps have happened
[32m[20230204 15:43:20 @agent_ppo2.py:130][0m #------------------------ Iteration 264 --------------------------#
[32m[20230204 15:43:20 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:43:20 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:20 @agent_ppo2.py:194][0m |          -0.0004 |          13.7848 |           4.3577 |
[32m[20230204 15:43:21 @agent_ppo2.py:194][0m |          -0.0063 |          12.6458 |           4.3436 |
[32m[20230204 15:43:21 @agent_ppo2.py:194][0m |          -0.0078 |          12.2825 |           4.3418 |
[32m[20230204 15:43:21 @agent_ppo2.py:194][0m |          -0.0085 |          11.9985 |           4.3400 |
[32m[20230204 15:43:21 @agent_ppo2.py:194][0m |          -0.0085 |          11.7389 |           4.3425 |
[32m[20230204 15:43:21 @agent_ppo2.py:194][0m |          -0.0091 |          11.5183 |           4.3381 |
[32m[20230204 15:43:21 @agent_ppo2.py:194][0m |          -0.0097 |          11.2846 |           4.3409 |
[32m[20230204 15:43:21 @agent_ppo2.py:194][0m |          -0.0099 |          11.1556 |           4.3390 |
[32m[20230204 15:43:21 @agent_ppo2.py:194][0m |          -0.0106 |          11.0377 |           4.3376 |
[32m[20230204 15:43:21 @agent_ppo2.py:194][0m |          -0.0098 |          10.9424 |           4.3393 |
[32m[20230204 15:43:21 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:43:21 @agent_ppo2.py:147][0m Average TRAINING episode reward: 253.12
[32m[20230204 15:43:21 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.22
[32m[20230204 15:43:21 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 274.57
[32m[20230204 15:43:21 @agent_ppo2.py:152][0m Total time:       7.18 min
[32m[20230204 15:43:21 @agent_ppo2.py:154][0m 542720 total steps have happened
[32m[20230204 15:43:21 @agent_ppo2.py:130][0m #------------------------ Iteration 265 --------------------------#
[32m[20230204 15:43:22 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:43:22 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:22 @agent_ppo2.py:194][0m |           0.0046 |          13.7138 |           4.3220 |
[32m[20230204 15:43:22 @agent_ppo2.py:194][0m |          -0.0049 |          12.1285 |           4.3167 |
[32m[20230204 15:43:22 @agent_ppo2.py:194][0m |          -0.0061 |          11.7621 |           4.3160 |
[32m[20230204 15:43:22 @agent_ppo2.py:194][0m |          -0.0079 |          11.4369 |           4.3097 |
[32m[20230204 15:43:22 @agent_ppo2.py:194][0m |          -0.0088 |          11.2915 |           4.3070 |
[32m[20230204 15:43:22 @agent_ppo2.py:194][0m |          -0.0096 |          10.9968 |           4.3063 |
[32m[20230204 15:43:22 @agent_ppo2.py:194][0m |          -0.0089 |          10.9765 |           4.3041 |
[32m[20230204 15:43:22 @agent_ppo2.py:194][0m |          -0.0113 |          10.7331 |           4.2998 |
[32m[20230204 15:43:22 @agent_ppo2.py:194][0m |          -0.0095 |          10.5909 |           4.3010 |
[32m[20230204 15:43:22 @agent_ppo2.py:194][0m |          -0.0107 |          10.0635 |           4.2990 |
[32m[20230204 15:43:22 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:43:23 @agent_ppo2.py:147][0m Average TRAINING episode reward: 262.22
[32m[20230204 15:43:23 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.42
[32m[20230204 15:43:23 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 274.57
[32m[20230204 15:43:23 @agent_ppo2.py:152][0m Total time:       7.20 min
[32m[20230204 15:43:23 @agent_ppo2.py:154][0m 544768 total steps have happened
[32m[20230204 15:43:23 @agent_ppo2.py:130][0m #------------------------ Iteration 266 --------------------------#
[32m[20230204 15:43:23 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:23 @agent_ppo2.py:194][0m |           0.0044 |          13.9699 |           4.2841 |
[32m[20230204 15:43:23 @agent_ppo2.py:194][0m |          -0.0048 |          12.3936 |           4.2740 |
[32m[20230204 15:43:23 @agent_ppo2.py:194][0m |          -0.0068 |          11.9575 |           4.2754 |
[32m[20230204 15:43:23 @agent_ppo2.py:194][0m |          -0.0029 |          12.1026 |           4.2770 |
[32m[20230204 15:43:23 @agent_ppo2.py:194][0m |          -0.0105 |          11.3068 |           4.2742 |
[32m[20230204 15:43:23 @agent_ppo2.py:194][0m |          -0.0120 |          10.9974 |           4.2725 |
[32m[20230204 15:43:24 @agent_ppo2.py:194][0m |          -0.0059 |          11.3958 |           4.2760 |
[32m[20230204 15:43:24 @agent_ppo2.py:194][0m |          -0.0113 |          10.5419 |           4.2718 |
[32m[20230204 15:43:24 @agent_ppo2.py:194][0m |          -0.0130 |          10.3041 |           4.2692 |
[32m[20230204 15:43:24 @agent_ppo2.py:194][0m |          -0.0099 |          10.2460 |           4.2730 |
[32m[20230204 15:43:24 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:43:24 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.78
[32m[20230204 15:43:24 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.25
[32m[20230204 15:43:24 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.68
[32m[20230204 15:43:24 @agent_ppo2.py:152][0m Total time:       7.23 min
[32m[20230204 15:43:24 @agent_ppo2.py:154][0m 546816 total steps have happened
[32m[20230204 15:43:24 @agent_ppo2.py:130][0m #------------------------ Iteration 267 --------------------------#
[32m[20230204 15:43:24 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:24 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:24 @agent_ppo2.py:194][0m |          -0.0005 |          14.3161 |           4.2669 |
[32m[20230204 15:43:24 @agent_ppo2.py:194][0m |          -0.0046 |          12.8069 |           4.2557 |
[32m[20230204 15:43:25 @agent_ppo2.py:194][0m |          -0.0075 |          12.0977 |           4.2527 |
[32m[20230204 15:43:25 @agent_ppo2.py:194][0m |          -0.0089 |          11.7707 |           4.2483 |
[32m[20230204 15:43:25 @agent_ppo2.py:194][0m |          -0.0091 |          11.4939 |           4.2463 |
[32m[20230204 15:43:25 @agent_ppo2.py:194][0m |          -0.0102 |          11.2948 |           4.2431 |
[32m[20230204 15:43:25 @agent_ppo2.py:194][0m |          -0.0105 |          11.1160 |           4.2444 |
[32m[20230204 15:43:25 @agent_ppo2.py:194][0m |          -0.0096 |          10.9873 |           4.2432 |
[32m[20230204 15:43:25 @agent_ppo2.py:194][0m |          -0.0115 |          10.8309 |           4.2412 |
[32m[20230204 15:43:25 @agent_ppo2.py:194][0m |          -0.0114 |          10.6513 |           4.2420 |
[32m[20230204 15:43:25 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:43:25 @agent_ppo2.py:147][0m Average TRAINING episode reward: 262.90
[32m[20230204 15:43:25 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.70
[32m[20230204 15:43:25 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 276.73
[32m[20230204 15:43:25 @agent_ppo2.py:152][0m Total time:       7.25 min
[32m[20230204 15:43:25 @agent_ppo2.py:154][0m 548864 total steps have happened
[32m[20230204 15:43:25 @agent_ppo2.py:130][0m #------------------------ Iteration 268 --------------------------#
[32m[20230204 15:43:26 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:26 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:26 @agent_ppo2.py:194][0m |           0.0020 |          13.3206 |           4.2689 |
[32m[20230204 15:43:26 @agent_ppo2.py:194][0m |          -0.0014 |          12.3236 |           4.2643 |
[32m[20230204 15:43:26 @agent_ppo2.py:194][0m |          -0.0036 |          11.9502 |           4.2650 |
[32m[20230204 15:43:26 @agent_ppo2.py:194][0m |          -0.0050 |          11.6629 |           4.2673 |
[32m[20230204 15:43:26 @agent_ppo2.py:194][0m |          -0.0056 |          11.4609 |           4.2638 |
[32m[20230204 15:43:26 @agent_ppo2.py:194][0m |          -0.0069 |          11.3315 |           4.2681 |
[32m[20230204 15:43:26 @agent_ppo2.py:194][0m |          -0.0068 |          11.1149 |           4.2693 |
[32m[20230204 15:43:26 @agent_ppo2.py:194][0m |          -0.0071 |          11.0277 |           4.2707 |
[32m[20230204 15:43:26 @agent_ppo2.py:194][0m |          -0.0072 |          10.8624 |           4.2724 |
[32m[20230204 15:43:26 @agent_ppo2.py:194][0m |          -0.0087 |          10.6948 |           4.2720 |
[32m[20230204 15:43:26 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:43:27 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.84
[32m[20230204 15:43:27 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.80
[32m[20230204 15:43:27 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.08
[32m[20230204 15:43:27 @agent_ppo2.py:152][0m Total time:       7.27 min
[32m[20230204 15:43:27 @agent_ppo2.py:154][0m 550912 total steps have happened
[32m[20230204 15:43:27 @agent_ppo2.py:130][0m #------------------------ Iteration 269 --------------------------#
[32m[20230204 15:43:27 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:27 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:27 @agent_ppo2.py:194][0m |          -0.0008 |          14.1668 |           4.3299 |
[32m[20230204 15:43:27 @agent_ppo2.py:194][0m |          -0.0068 |          12.7674 |           4.3253 |
[32m[20230204 15:43:27 @agent_ppo2.py:194][0m |          -0.0056 |          12.1023 |           4.3221 |
[32m[20230204 15:43:27 @agent_ppo2.py:194][0m |          -0.0076 |          11.2429 |           4.3218 |
[32m[20230204 15:43:27 @agent_ppo2.py:194][0m |          -0.0079 |          10.5771 |           4.3173 |
[32m[20230204 15:43:27 @agent_ppo2.py:194][0m |          -0.0082 |          10.0618 |           4.3211 |
[32m[20230204 15:43:28 @agent_ppo2.py:194][0m |          -0.0115 |           9.4005 |           4.3197 |
[32m[20230204 15:43:28 @agent_ppo2.py:194][0m |          -0.0102 |           8.9802 |           4.3197 |
[32m[20230204 15:43:28 @agent_ppo2.py:194][0m |          -0.0075 |           8.8674 |           4.3216 |
[32m[20230204 15:43:28 @agent_ppo2.py:194][0m |          -0.0090 |           8.5847 |           4.3194 |
[32m[20230204 15:43:28 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:43:28 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.09
[32m[20230204 15:43:28 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.55
[32m[20230204 15:43:28 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.70
[32m[20230204 15:43:28 @agent_ppo2.py:152][0m Total time:       7.29 min
[32m[20230204 15:43:28 @agent_ppo2.py:154][0m 552960 total steps have happened
[32m[20230204 15:43:28 @agent_ppo2.py:130][0m #------------------------ Iteration 270 --------------------------#
[32m[20230204 15:43:28 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:28 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:28 @agent_ppo2.py:194][0m |          -0.0014 |          13.8529 |           4.2105 |
[32m[20230204 15:43:28 @agent_ppo2.py:194][0m |          -0.0026 |          12.8997 |           4.2032 |
[32m[20230204 15:43:29 @agent_ppo2.py:194][0m |          -0.0030 |          12.6403 |           4.2033 |
[32m[20230204 15:43:29 @agent_ppo2.py:194][0m |          -0.0051 |          12.2007 |           4.1998 |
[32m[20230204 15:43:29 @agent_ppo2.py:194][0m |          -0.0051 |          12.3168 |           4.2016 |
[32m[20230204 15:43:29 @agent_ppo2.py:194][0m |          -0.0088 |          11.8760 |           4.2008 |
[32m[20230204 15:43:29 @agent_ppo2.py:194][0m |          -0.0035 |          12.4445 |           4.2034 |
[32m[20230204 15:43:29 @agent_ppo2.py:194][0m |          -0.0093 |          11.6435 |           4.1983 |
[32m[20230204 15:43:29 @agent_ppo2.py:194][0m |          -0.0098 |          11.8977 |           4.1990 |
[32m[20230204 15:43:29 @agent_ppo2.py:194][0m |          -0.0103 |          11.4209 |           4.2003 |
[32m[20230204 15:43:29 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:43:29 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.96
[32m[20230204 15:43:29 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.10
[32m[20230204 15:43:29 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.91
[32m[20230204 15:43:29 @agent_ppo2.py:152][0m Total time:       7.31 min
[32m[20230204 15:43:29 @agent_ppo2.py:154][0m 555008 total steps have happened
[32m[20230204 15:43:29 @agent_ppo2.py:130][0m #------------------------ Iteration 271 --------------------------#
[32m[20230204 15:43:30 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:43:30 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:30 @agent_ppo2.py:194][0m |          -0.0003 |          13.3549 |           4.2339 |
[32m[20230204 15:43:30 @agent_ppo2.py:194][0m |          -0.0087 |          11.2573 |           4.2272 |
[32m[20230204 15:43:30 @agent_ppo2.py:194][0m |          -0.0081 |          10.3784 |           4.2245 |
[32m[20230204 15:43:30 @agent_ppo2.py:194][0m |          -0.0095 |           9.7812 |           4.2252 |
[32m[20230204 15:43:30 @agent_ppo2.py:194][0m |          -0.0061 |           9.6222 |           4.2276 |
[32m[20230204 15:43:30 @agent_ppo2.py:194][0m |          -0.0080 |           8.7120 |           4.2236 |
[32m[20230204 15:43:30 @agent_ppo2.py:194][0m |          -0.0087 |           8.1200 |           4.2236 |
[32m[20230204 15:43:30 @agent_ppo2.py:194][0m |          -0.0124 |           7.6638 |           4.2212 |
[32m[20230204 15:43:30 @agent_ppo2.py:194][0m |          -0.0112 |           7.1923 |           4.2236 |
[32m[20230204 15:43:30 @agent_ppo2.py:194][0m |          -0.0130 |           6.8384 |           4.2271 |
[32m[20230204 15:43:30 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:43:31 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.95
[32m[20230204 15:43:31 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.54
[32m[20230204 15:43:31 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.51
[32m[20230204 15:43:31 @agent_ppo2.py:152][0m Total time:       7.34 min
[32m[20230204 15:43:31 @agent_ppo2.py:154][0m 557056 total steps have happened
[32m[20230204 15:43:31 @agent_ppo2.py:130][0m #------------------------ Iteration 272 --------------------------#
[32m[20230204 15:43:31 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:31 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:31 @agent_ppo2.py:194][0m |          -0.0008 |          16.9467 |           4.3350 |
[32m[20230204 15:43:31 @agent_ppo2.py:194][0m |          -0.0084 |          12.7617 |           4.3307 |
[32m[20230204 15:43:31 @agent_ppo2.py:194][0m |          -0.0130 |          12.0946 |           4.3307 |
[32m[20230204 15:43:31 @agent_ppo2.py:194][0m |          -0.0148 |          11.6280 |           4.3299 |
[32m[20230204 15:43:31 @agent_ppo2.py:194][0m |          -0.0154 |          11.2746 |           4.3250 |
[32m[20230204 15:43:31 @agent_ppo2.py:194][0m |          -0.0162 |          11.0124 |           4.3284 |
[32m[20230204 15:43:31 @agent_ppo2.py:194][0m |          -0.0179 |          10.7981 |           4.3277 |
[32m[20230204 15:43:32 @agent_ppo2.py:194][0m |          -0.0175 |          10.5772 |           4.3232 |
[32m[20230204 15:43:32 @agent_ppo2.py:194][0m |          -0.0187 |          10.3932 |           4.3263 |
[32m[20230204 15:43:32 @agent_ppo2.py:194][0m |          -0.0192 |          10.2187 |           4.3303 |
[32m[20230204 15:43:32 @agent_ppo2.py:139][0m Policy update time: 0.80 s
[32m[20230204 15:43:32 @agent_ppo2.py:147][0m Average TRAINING episode reward: 198.76
[32m[20230204 15:43:32 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.18
[32m[20230204 15:43:32 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.46
[32m[20230204 15:43:32 @agent_ppo2.py:152][0m Total time:       7.36 min
[32m[20230204 15:43:32 @agent_ppo2.py:154][0m 559104 total steps have happened
[32m[20230204 15:43:32 @agent_ppo2.py:130][0m #------------------------ Iteration 273 --------------------------#
[32m[20230204 15:43:32 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:43:32 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:32 @agent_ppo2.py:194][0m |           0.0032 |          32.6373 |           4.3140 |
[32m[20230204 15:43:32 @agent_ppo2.py:194][0m |          -0.0017 |          13.8654 |           4.3120 |
[32m[20230204 15:43:32 @agent_ppo2.py:194][0m |          -0.0050 |          12.4437 |           4.3116 |
[32m[20230204 15:43:33 @agent_ppo2.py:194][0m |          -0.0049 |          11.9607 |           4.3076 |
[32m[20230204 15:43:33 @agent_ppo2.py:194][0m |          -0.0075 |          11.2204 |           4.3098 |
[32m[20230204 15:43:33 @agent_ppo2.py:194][0m |          -0.0060 |          11.2313 |           4.3072 |
[32m[20230204 15:43:33 @agent_ppo2.py:194][0m |          -0.0112 |          10.5645 |           4.3061 |
[32m[20230204 15:43:33 @agent_ppo2.py:194][0m |          -0.0089 |          10.3417 |           4.3035 |
[32m[20230204 15:43:33 @agent_ppo2.py:194][0m |          -0.0093 |          10.5030 |           4.2986 |
[32m[20230204 15:43:33 @agent_ppo2.py:194][0m |          -0.0099 |          10.0484 |           4.3043 |
[32m[20230204 15:43:33 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:43:33 @agent_ppo2.py:147][0m Average TRAINING episode reward: 154.15
[32m[20230204 15:43:33 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.30
[32m[20230204 15:43:33 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.36
[32m[20230204 15:43:33 @agent_ppo2.py:152][0m Total time:       7.38 min
[32m[20230204 15:43:33 @agent_ppo2.py:154][0m 561152 total steps have happened
[32m[20230204 15:43:33 @agent_ppo2.py:130][0m #------------------------ Iteration 274 --------------------------#
[32m[20230204 15:43:34 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:34 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:34 @agent_ppo2.py:194][0m |          -0.0006 |          15.7798 |           4.3518 |
[32m[20230204 15:43:34 @agent_ppo2.py:194][0m |          -0.0057 |          14.6447 |           4.3445 |
[32m[20230204 15:43:34 @agent_ppo2.py:194][0m |          -0.0062 |          14.2060 |           4.3475 |
[32m[20230204 15:43:34 @agent_ppo2.py:194][0m |          -0.0088 |          13.7137 |           4.3512 |
[32m[20230204 15:43:34 @agent_ppo2.py:194][0m |          -0.0105 |          13.4613 |           4.3494 |
[32m[20230204 15:43:34 @agent_ppo2.py:194][0m |          -0.0112 |          13.2514 |           4.3505 |
[32m[20230204 15:43:34 @agent_ppo2.py:194][0m |          -0.0099 |          13.0562 |           4.3525 |
[32m[20230204 15:43:34 @agent_ppo2.py:194][0m |          -0.0132 |          12.8051 |           4.3532 |
[32m[20230204 15:43:34 @agent_ppo2.py:194][0m |          -0.0125 |          12.6180 |           4.3510 |
[32m[20230204 15:43:34 @agent_ppo2.py:194][0m |          -0.0117 |          12.4877 |           4.3567 |
[32m[20230204 15:43:34 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:43:35 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.81
[32m[20230204 15:43:35 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.46
[32m[20230204 15:43:35 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.41
[32m[20230204 15:43:35 @agent_ppo2.py:152][0m Total time:       7.40 min
[32m[20230204 15:43:35 @agent_ppo2.py:154][0m 563200 total steps have happened
[32m[20230204 15:43:35 @agent_ppo2.py:130][0m #------------------------ Iteration 275 --------------------------#
[32m[20230204 15:43:35 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:35 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:35 @agent_ppo2.py:194][0m |          -0.0004 |          35.8423 |           4.2985 |
[32m[20230204 15:43:35 @agent_ppo2.py:194][0m |          -0.0072 |          24.3172 |           4.2886 |
[32m[20230204 15:43:35 @agent_ppo2.py:194][0m |          -0.0111 |          20.6653 |           4.2813 |
[32m[20230204 15:43:35 @agent_ppo2.py:194][0m |          -0.0110 |          19.2531 |           4.2799 |
[32m[20230204 15:43:35 @agent_ppo2.py:194][0m |          -0.0129 |          18.1618 |           4.2779 |
[32m[20230204 15:43:35 @agent_ppo2.py:194][0m |          -0.0146 |          17.2305 |           4.2780 |
[32m[20230204 15:43:35 @agent_ppo2.py:194][0m |          -0.0150 |          16.7081 |           4.2764 |
[32m[20230204 15:43:35 @agent_ppo2.py:194][0m |          -0.0157 |          16.0732 |           4.2769 |
[32m[20230204 15:43:36 @agent_ppo2.py:194][0m |          -0.0162 |          15.5982 |           4.2798 |
[32m[20230204 15:43:36 @agent_ppo2.py:194][0m |          -0.0162 |          15.0817 |           4.2772 |
[32m[20230204 15:43:36 @agent_ppo2.py:139][0m Policy update time: 0.75 s
[32m[20230204 15:43:36 @agent_ppo2.py:147][0m Average TRAINING episode reward: 196.42
[32m[20230204 15:43:36 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.55
[32m[20230204 15:43:36 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.73
[32m[20230204 15:43:36 @agent_ppo2.py:152][0m Total time:       7.42 min
[32m[20230204 15:43:36 @agent_ppo2.py:154][0m 565248 total steps have happened
[32m[20230204 15:43:36 @agent_ppo2.py:130][0m #------------------------ Iteration 276 --------------------------#
[32m[20230204 15:43:36 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:43:36 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:36 @agent_ppo2.py:194][0m |          -0.0016 |          14.7026 |           4.3462 |
[32m[20230204 15:43:36 @agent_ppo2.py:194][0m |           0.0074 |          13.6009 |           4.3387 |
[32m[20230204 15:43:36 @agent_ppo2.py:194][0m |          -0.0025 |          12.4196 |           4.3365 |
[32m[20230204 15:43:36 @agent_ppo2.py:194][0m |          -0.0109 |          11.8105 |           4.3322 |
[32m[20230204 15:43:37 @agent_ppo2.py:194][0m |          -0.0101 |          11.4480 |           4.3249 |
[32m[20230204 15:43:37 @agent_ppo2.py:194][0m |          -0.0041 |          11.2471 |           4.3291 |
[32m[20230204 15:43:37 @agent_ppo2.py:194][0m |          -0.0101 |          10.8553 |           4.3232 |
[32m[20230204 15:43:37 @agent_ppo2.py:194][0m |          -0.0135 |          10.6103 |           4.3233 |
[32m[20230204 15:43:37 @agent_ppo2.py:194][0m |          -0.0113 |          10.4472 |           4.3208 |
[32m[20230204 15:43:37 @agent_ppo2.py:194][0m |          -0.0126 |          10.2929 |           4.3209 |
[32m[20230204 15:43:37 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:43:37 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.97
[32m[20230204 15:43:37 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.20
[32m[20230204 15:43:37 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.69
[32m[20230204 15:43:37 @agent_ppo2.py:152][0m Total time:       7.44 min
[32m[20230204 15:43:37 @agent_ppo2.py:154][0m 567296 total steps have happened
[32m[20230204 15:43:37 @agent_ppo2.py:130][0m #------------------------ Iteration 277 --------------------------#
[32m[20230204 15:43:37 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:43:37 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:38 @agent_ppo2.py:194][0m |           0.0047 |          14.6892 |           4.3169 |
[32m[20230204 15:43:38 @agent_ppo2.py:194][0m |          -0.0073 |          12.9076 |           4.3135 |
[32m[20230204 15:43:38 @agent_ppo2.py:194][0m |          -0.0098 |          12.5042 |           4.3190 |
[32m[20230204 15:43:38 @agent_ppo2.py:194][0m |          -0.0145 |          12.2316 |           4.3157 |
[32m[20230204 15:43:38 @agent_ppo2.py:194][0m |          -0.0123 |          12.0525 |           4.3201 |
[32m[20230204 15:43:38 @agent_ppo2.py:194][0m |          -0.0042 |          11.7890 |           4.3164 |
[32m[20230204 15:43:38 @agent_ppo2.py:194][0m |          -0.0261 |          11.6787 |           4.3218 |
[32m[20230204 15:43:38 @agent_ppo2.py:194][0m |          -0.0065 |          11.4858 |           4.3061 |
[32m[20230204 15:43:38 @agent_ppo2.py:194][0m |          -0.0066 |          11.3561 |           4.3205 |
[32m[20230204 15:43:38 @agent_ppo2.py:194][0m |          -0.0178 |          11.2192 |           4.3242 |
[32m[20230204 15:43:38 @agent_ppo2.py:139][0m Policy update time: 0.91 s
[32m[20230204 15:43:39 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.20
[32m[20230204 15:43:39 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.62
[32m[20230204 15:43:39 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.58
[32m[20230204 15:43:39 @agent_ppo2.py:152][0m Total time:       7.47 min
[32m[20230204 15:43:39 @agent_ppo2.py:154][0m 569344 total steps have happened
[32m[20230204 15:43:39 @agent_ppo2.py:130][0m #------------------------ Iteration 278 --------------------------#
[32m[20230204 15:43:39 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:39 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:39 @agent_ppo2.py:194][0m |           0.0006 |          15.1838 |           4.3420 |
[32m[20230204 15:43:39 @agent_ppo2.py:194][0m |          -0.0064 |          13.9540 |           4.3386 |
[32m[20230204 15:43:39 @agent_ppo2.py:194][0m |          -0.0070 |          13.4018 |           4.3345 |
[32m[20230204 15:43:39 @agent_ppo2.py:194][0m |          -0.0092 |          12.8340 |           4.3331 |
[32m[20230204 15:43:39 @agent_ppo2.py:194][0m |          -0.0101 |          12.4260 |           4.3333 |
[32m[20230204 15:43:39 @agent_ppo2.py:194][0m |          -0.0094 |          12.1353 |           4.3331 |
[32m[20230204 15:43:39 @agent_ppo2.py:194][0m |          -0.0111 |          11.6430 |           4.3306 |
[32m[20230204 15:43:39 @agent_ppo2.py:194][0m |          -0.0112 |          11.3165 |           4.3303 |
[32m[20230204 15:43:40 @agent_ppo2.py:194][0m |          -0.0113 |          11.0491 |           4.3301 |
[32m[20230204 15:43:40 @agent_ppo2.py:194][0m |          -0.0119 |          10.8500 |           4.3288 |
[32m[20230204 15:43:40 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:43:40 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.33
[32m[20230204 15:43:40 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.38
[32m[20230204 15:43:40 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.84
[32m[20230204 15:43:40 @agent_ppo2.py:152][0m Total time:       7.49 min
[32m[20230204 15:43:40 @agent_ppo2.py:154][0m 571392 total steps have happened
[32m[20230204 15:43:40 @agent_ppo2.py:130][0m #------------------------ Iteration 279 --------------------------#
[32m[20230204 15:43:40 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:40 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:40 @agent_ppo2.py:194][0m |          -0.0004 |          25.9264 |           4.3650 |
[32m[20230204 15:43:40 @agent_ppo2.py:194][0m |          -0.0060 |          16.6767 |           4.3628 |
[32m[20230204 15:43:40 @agent_ppo2.py:194][0m |          -0.0084 |          14.7139 |           4.3573 |
[32m[20230204 15:43:40 @agent_ppo2.py:194][0m |          -0.0084 |          13.6523 |           4.3570 |
[32m[20230204 15:43:40 @agent_ppo2.py:194][0m |          -0.0092 |          13.0230 |           4.3558 |
[32m[20230204 15:43:41 @agent_ppo2.py:194][0m |          -0.0102 |          12.6605 |           4.3516 |
[32m[20230204 15:43:41 @agent_ppo2.py:194][0m |          -0.0115 |          12.2591 |           4.3512 |
[32m[20230204 15:43:41 @agent_ppo2.py:194][0m |          -0.0122 |          11.9413 |           4.3515 |
[32m[20230204 15:43:41 @agent_ppo2.py:194][0m |          -0.0120 |          11.6259 |           4.3503 |
[32m[20230204 15:43:41 @agent_ppo2.py:194][0m |          -0.0121 |          11.4285 |           4.3513 |
[32m[20230204 15:43:41 @agent_ppo2.py:139][0m Policy update time: 0.79 s
[32m[20230204 15:43:41 @agent_ppo2.py:147][0m Average TRAINING episode reward: 207.31
[32m[20230204 15:43:41 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.34
[32m[20230204 15:43:41 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.94
[32m[20230204 15:43:41 @agent_ppo2.py:152][0m Total time:       7.51 min
[32m[20230204 15:43:41 @agent_ppo2.py:154][0m 573440 total steps have happened
[32m[20230204 15:43:41 @agent_ppo2.py:130][0m #------------------------ Iteration 280 --------------------------#
[32m[20230204 15:43:41 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:43:41 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:42 @agent_ppo2.py:194][0m |          -0.0008 |          21.7759 |           4.4618 |
[32m[20230204 15:43:42 @agent_ppo2.py:194][0m |          -0.0066 |          16.8871 |           4.4509 |
[32m[20230204 15:43:42 @agent_ppo2.py:194][0m |          -0.0087 |          15.6654 |           4.4587 |
[32m[20230204 15:43:42 @agent_ppo2.py:194][0m |          -0.0097 |          15.1446 |           4.4601 |
[32m[20230204 15:43:42 @agent_ppo2.py:194][0m |          -0.0124 |          14.5730 |           4.4577 |
[32m[20230204 15:43:42 @agent_ppo2.py:194][0m |          -0.0113 |          14.1941 |           4.4552 |
[32m[20230204 15:43:42 @agent_ppo2.py:194][0m |          -0.0125 |          13.8785 |           4.4551 |
[32m[20230204 15:43:42 @agent_ppo2.py:194][0m |          -0.0136 |          13.7305 |           4.4542 |
[32m[20230204 15:43:42 @agent_ppo2.py:194][0m |          -0.0139 |          13.4985 |           4.4516 |
[32m[20230204 15:43:42 @agent_ppo2.py:194][0m |          -0.0133 |          13.2758 |           4.4563 |
[32m[20230204 15:43:42 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:43:42 @agent_ppo2.py:147][0m Average TRAINING episode reward: 193.60
[32m[20230204 15:43:42 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.21
[32m[20230204 15:43:42 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.33
[32m[20230204 15:43:42 @agent_ppo2.py:152][0m Total time:       7.53 min
[32m[20230204 15:43:42 @agent_ppo2.py:154][0m 575488 total steps have happened
[32m[20230204 15:43:42 @agent_ppo2.py:130][0m #------------------------ Iteration 281 --------------------------#
[32m[20230204 15:43:43 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:43:43 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:43 @agent_ppo2.py:194][0m |           0.0030 |          40.2208 |           4.3503 |
[32m[20230204 15:43:43 @agent_ppo2.py:194][0m |          -0.0010 |          31.0385 |           4.3423 |
[32m[20230204 15:43:43 @agent_ppo2.py:194][0m |          -0.0053 |          26.1323 |           4.3400 |
[32m[20230204 15:43:43 @agent_ppo2.py:194][0m |          -0.0070 |          22.5363 |           4.3377 |
[32m[20230204 15:43:43 @agent_ppo2.py:194][0m |          -0.0070 |          20.2734 |           4.3370 |
[32m[20230204 15:43:43 @agent_ppo2.py:194][0m |          -0.0079 |          18.7228 |           4.3373 |
[32m[20230204 15:43:43 @agent_ppo2.py:194][0m |          -0.0085 |          17.3038 |           4.3361 |
[32m[20230204 15:43:43 @agent_ppo2.py:194][0m |          -0.0093 |          16.2036 |           4.3320 |
[32m[20230204 15:43:43 @agent_ppo2.py:194][0m |          -0.0118 |          15.4698 |           4.3343 |
[32m[20230204 15:43:44 @agent_ppo2.py:194][0m |          -0.0087 |          14.7795 |           4.3350 |
[32m[20230204 15:43:44 @agent_ppo2.py:139][0m Policy update time: 0.82 s
[32m[20230204 15:43:44 @agent_ppo2.py:147][0m Average TRAINING episode reward: 200.61
[32m[20230204 15:43:44 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.01
[32m[20230204 15:43:44 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.12
[32m[20230204 15:43:44 @agent_ppo2.py:152][0m Total time:       7.56 min
[32m[20230204 15:43:44 @agent_ppo2.py:154][0m 577536 total steps have happened
[32m[20230204 15:43:44 @agent_ppo2.py:130][0m #------------------------ Iteration 282 --------------------------#
[32m[20230204 15:43:44 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:43:44 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:44 @agent_ppo2.py:194][0m |          -0.0002 |          31.6210 |           4.3279 |
[32m[20230204 15:43:44 @agent_ppo2.py:194][0m |          -0.0044 |          16.4485 |           4.3248 |
[32m[20230204 15:43:44 @agent_ppo2.py:194][0m |          -0.0068 |          14.6827 |           4.3231 |
[32m[20230204 15:43:44 @agent_ppo2.py:194][0m |          -0.0093 |          13.6077 |           4.3198 |
[32m[20230204 15:43:44 @agent_ppo2.py:194][0m |          -0.0095 |          12.7473 |           4.3219 |
[32m[20230204 15:43:45 @agent_ppo2.py:194][0m |          -0.0112 |          12.0899 |           4.3219 |
[32m[20230204 15:43:45 @agent_ppo2.py:194][0m |          -0.0112 |          11.6321 |           4.3197 |
[32m[20230204 15:43:45 @agent_ppo2.py:194][0m |          -0.0113 |          11.2987 |           4.3225 |
[32m[20230204 15:43:45 @agent_ppo2.py:194][0m |          -0.0121 |          10.9754 |           4.3179 |
[32m[20230204 15:43:45 @agent_ppo2.py:194][0m |          -0.0131 |          10.6472 |           4.3191 |
[32m[20230204 15:43:45 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:43:45 @agent_ppo2.py:147][0m Average TRAINING episode reward: 154.70
[32m[20230204 15:43:45 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 257.07
[32m[20230204 15:43:45 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.97
[32m[20230204 15:43:45 @agent_ppo2.py:152][0m Total time:       7.58 min
[32m[20230204 15:43:45 @agent_ppo2.py:154][0m 579584 total steps have happened
[32m[20230204 15:43:45 @agent_ppo2.py:130][0m #------------------------ Iteration 283 --------------------------#
[32m[20230204 15:43:45 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:43:46 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:46 @agent_ppo2.py:194][0m |           0.0003 |          18.4885 |           4.2653 |
[32m[20230204 15:43:46 @agent_ppo2.py:194][0m |          -0.0020 |          14.1853 |           4.2596 |
[32m[20230204 15:43:46 @agent_ppo2.py:194][0m |          -0.0064 |          13.2377 |           4.2523 |
[32m[20230204 15:43:46 @agent_ppo2.py:194][0m |          -0.0078 |          12.8367 |           4.2501 |
[32m[20230204 15:43:46 @agent_ppo2.py:194][0m |          -0.0074 |          12.5480 |           4.2514 |
[32m[20230204 15:43:46 @agent_ppo2.py:194][0m |          -0.0075 |          12.3061 |           4.2506 |
[32m[20230204 15:43:46 @agent_ppo2.py:194][0m |          -0.0058 |          12.1340 |           4.2469 |
[32m[20230204 15:43:46 @agent_ppo2.py:194][0m |          -0.0101 |          11.9403 |           4.2517 |
[32m[20230204 15:43:46 @agent_ppo2.py:194][0m |          -0.0103 |          11.8232 |           4.2473 |
[32m[20230204 15:43:46 @agent_ppo2.py:194][0m |          -0.0106 |          11.7191 |           4.2485 |
[32m[20230204 15:43:46 @agent_ppo2.py:139][0m Policy update time: 0.91 s
[32m[20230204 15:43:47 @agent_ppo2.py:147][0m Average TRAINING episode reward: 180.99
[32m[20230204 15:43:47 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.92
[32m[20230204 15:43:47 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.27
[32m[20230204 15:43:47 @agent_ppo2.py:152][0m Total time:       7.60 min
[32m[20230204 15:43:47 @agent_ppo2.py:154][0m 581632 total steps have happened
[32m[20230204 15:43:47 @agent_ppo2.py:130][0m #------------------------ Iteration 284 --------------------------#
[32m[20230204 15:43:47 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:43:47 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:47 @agent_ppo2.py:194][0m |           0.0021 |          29.2670 |           4.3538 |
[32m[20230204 15:43:47 @agent_ppo2.py:194][0m |          -0.0050 |          24.3486 |           4.3461 |
[32m[20230204 15:43:47 @agent_ppo2.py:194][0m |          -0.0072 |          23.3071 |           4.3430 |
[32m[20230204 15:43:47 @agent_ppo2.py:194][0m |          -0.0051 |          22.7541 |           4.3369 |
[32m[20230204 15:43:47 @agent_ppo2.py:194][0m |          -0.0098 |          22.0230 |           4.3402 |
[32m[20230204 15:43:47 @agent_ppo2.py:194][0m |          -0.0089 |          21.9748 |           4.3384 |
[32m[20230204 15:43:47 @agent_ppo2.py:194][0m |          -0.0102 |          21.1810 |           4.3365 |
[32m[20230204 15:43:48 @agent_ppo2.py:194][0m |          -0.0103 |          20.8028 |           4.3392 |
[32m[20230204 15:43:48 @agent_ppo2.py:194][0m |          -0.0071 |          21.5623 |           4.3375 |
[32m[20230204 15:43:48 @agent_ppo2.py:194][0m |          -0.0116 |          20.4855 |           4.3378 |
[32m[20230204 15:43:48 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:43:48 @agent_ppo2.py:147][0m Average TRAINING episode reward: 199.33
[32m[20230204 15:43:48 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.09
[32m[20230204 15:43:48 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.16
[32m[20230204 15:43:48 @agent_ppo2.py:152][0m Total time:       7.63 min
[32m[20230204 15:43:48 @agent_ppo2.py:154][0m 583680 total steps have happened
[32m[20230204 15:43:48 @agent_ppo2.py:130][0m #------------------------ Iteration 285 --------------------------#
[32m[20230204 15:43:48 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:48 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:48 @agent_ppo2.py:194][0m |          -0.0015 |          14.7629 |           4.3510 |
[32m[20230204 15:43:48 @agent_ppo2.py:194][0m |          -0.0023 |          13.5559 |           4.3459 |
[32m[20230204 15:43:49 @agent_ppo2.py:194][0m |          -0.0071 |          13.2105 |           4.3449 |
[32m[20230204 15:43:49 @agent_ppo2.py:194][0m |          -0.0113 |          12.9937 |           4.3477 |
[32m[20230204 15:43:49 @agent_ppo2.py:194][0m |          -0.0069 |          12.8141 |           4.3420 |
[32m[20230204 15:43:49 @agent_ppo2.py:194][0m |           0.0277 |          13.8934 |           4.3487 |
[32m[20230204 15:43:49 @agent_ppo2.py:194][0m |          -0.0060 |          12.6121 |           4.3360 |
[32m[20230204 15:43:49 @agent_ppo2.py:194][0m |          -0.0156 |          12.3233 |           4.3460 |
[32m[20230204 15:43:49 @agent_ppo2.py:194][0m |          -0.0162 |          12.3352 |           4.3479 |
[32m[20230204 15:43:49 @agent_ppo2.py:194][0m |          -0.0100 |          12.1887 |           4.3533 |
[32m[20230204 15:43:49 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:43:49 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.20
[32m[20230204 15:43:49 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.85
[32m[20230204 15:43:49 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.36
[32m[20230204 15:43:49 @agent_ppo2.py:152][0m Total time:       7.65 min
[32m[20230204 15:43:49 @agent_ppo2.py:154][0m 585728 total steps have happened
[32m[20230204 15:43:49 @agent_ppo2.py:130][0m #------------------------ Iteration 286 --------------------------#
[32m[20230204 15:43:50 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:50 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:50 @agent_ppo2.py:194][0m |          -0.0012 |          12.2896 |           4.3477 |
[32m[20230204 15:43:50 @agent_ppo2.py:194][0m |          -0.0051 |          11.5677 |           4.3432 |
[32m[20230204 15:43:50 @agent_ppo2.py:194][0m |          -0.0072 |          11.1786 |           4.3418 |
[32m[20230204 15:43:50 @agent_ppo2.py:194][0m |          -0.0053 |          11.1825 |           4.3414 |
[32m[20230204 15:43:50 @agent_ppo2.py:194][0m |          -0.0136 |          10.5035 |           4.3375 |
[32m[20230204 15:43:50 @agent_ppo2.py:194][0m |          -0.0122 |          10.2632 |           4.3429 |
[32m[20230204 15:43:50 @agent_ppo2.py:194][0m |          -0.0120 |          10.1437 |           4.3415 |
[32m[20230204 15:43:50 @agent_ppo2.py:194][0m |          -0.0060 |          10.3010 |           4.3407 |
[32m[20230204 15:43:50 @agent_ppo2.py:194][0m |          -0.0128 |           9.7924 |           4.3396 |
[32m[20230204 15:43:50 @agent_ppo2.py:194][0m |          -0.0133 |           9.5597 |           4.3404 |
[32m[20230204 15:43:50 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:43:51 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.41
[32m[20230204 15:43:51 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.14
[32m[20230204 15:43:51 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.68
[32m[20230204 15:43:51 @agent_ppo2.py:152][0m Total time:       7.67 min
[32m[20230204 15:43:51 @agent_ppo2.py:154][0m 587776 total steps have happened
[32m[20230204 15:43:51 @agent_ppo2.py:130][0m #------------------------ Iteration 287 --------------------------#
[32m[20230204 15:43:51 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:43:51 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:51 @agent_ppo2.py:194][0m |          -0.0034 |          13.0632 |           4.4429 |
[32m[20230204 15:43:51 @agent_ppo2.py:194][0m |          -0.0069 |          11.7040 |           4.4341 |
[32m[20230204 15:43:51 @agent_ppo2.py:194][0m |          -0.0094 |          11.1953 |           4.4295 |
[32m[20230204 15:43:51 @agent_ppo2.py:194][0m |          -0.0097 |          10.9150 |           4.4247 |
[32m[20230204 15:43:51 @agent_ppo2.py:194][0m |          -0.0101 |          10.6918 |           4.4251 |
[32m[20230204 15:43:51 @agent_ppo2.py:194][0m |          -0.0138 |          10.4918 |           4.4217 |
[32m[20230204 15:43:51 @agent_ppo2.py:194][0m |          -0.0085 |          10.3659 |           4.4172 |
[32m[20230204 15:43:52 @agent_ppo2.py:194][0m |          -0.0059 |          10.5346 |           4.4127 |
[32m[20230204 15:43:52 @agent_ppo2.py:194][0m |          -0.0132 |          10.0536 |           4.4159 |
[32m[20230204 15:43:52 @agent_ppo2.py:194][0m |          -0.0091 |          10.0175 |           4.4120 |
[32m[20230204 15:43:52 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:43:52 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.01
[32m[20230204 15:43:52 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.15
[32m[20230204 15:43:52 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.18
[32m[20230204 15:43:52 @agent_ppo2.py:152][0m Total time:       7.69 min
[32m[20230204 15:43:52 @agent_ppo2.py:154][0m 589824 total steps have happened
[32m[20230204 15:43:52 @agent_ppo2.py:130][0m #------------------------ Iteration 288 --------------------------#
[32m[20230204 15:43:52 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:43:52 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:52 @agent_ppo2.py:194][0m |          -0.0015 |          14.7676 |           4.3781 |
[32m[20230204 15:43:52 @agent_ppo2.py:194][0m |          -0.0046 |          13.4600 |           4.3792 |
[32m[20230204 15:43:53 @agent_ppo2.py:194][0m |          -0.0055 |          12.9343 |           4.3784 |
[32m[20230204 15:43:53 @agent_ppo2.py:194][0m |          -0.0064 |          12.5562 |           4.3765 |
[32m[20230204 15:43:53 @agent_ppo2.py:194][0m |          -0.0071 |          12.2750 |           4.3760 |
[32m[20230204 15:43:53 @agent_ppo2.py:194][0m |          -0.0074 |          12.0081 |           4.3771 |
[32m[20230204 15:43:53 @agent_ppo2.py:194][0m |          -0.0086 |          11.8291 |           4.3737 |
[32m[20230204 15:43:53 @agent_ppo2.py:194][0m |          -0.0082 |          11.6812 |           4.3751 |
[32m[20230204 15:43:53 @agent_ppo2.py:194][0m |          -0.0097 |          11.4873 |           4.3708 |
[32m[20230204 15:43:53 @agent_ppo2.py:194][0m |          -0.0094 |          11.3457 |           4.3704 |
[32m[20230204 15:43:53 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:43:53 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.11
[32m[20230204 15:43:53 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.39
[32m[20230204 15:43:53 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 262.50
[32m[20230204 15:43:53 @agent_ppo2.py:152][0m Total time:       7.71 min
[32m[20230204 15:43:53 @agent_ppo2.py:154][0m 591872 total steps have happened
[32m[20230204 15:43:53 @agent_ppo2.py:130][0m #------------------------ Iteration 289 --------------------------#
[32m[20230204 15:43:54 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:43:54 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:54 @agent_ppo2.py:194][0m |          -0.0004 |          14.2970 |           4.4415 |
[32m[20230204 15:43:54 @agent_ppo2.py:194][0m |          -0.0055 |          13.1734 |           4.4283 |
[32m[20230204 15:43:54 @agent_ppo2.py:194][0m |          -0.0070 |          12.7143 |           4.4274 |
[32m[20230204 15:43:54 @agent_ppo2.py:194][0m |          -0.0076 |          12.4590 |           4.4185 |
[32m[20230204 15:43:54 @agent_ppo2.py:194][0m |          -0.0076 |          12.3368 |           4.4196 |
[32m[20230204 15:43:54 @agent_ppo2.py:194][0m |          -0.0092 |          12.0703 |           4.4187 |
[32m[20230204 15:43:54 @agent_ppo2.py:194][0m |          -0.0088 |          11.9618 |           4.4153 |
[32m[20230204 15:43:54 @agent_ppo2.py:194][0m |          -0.0097 |          11.7589 |           4.4148 |
[32m[20230204 15:43:54 @agent_ppo2.py:194][0m |          -0.0093 |          11.7471 |           4.4100 |
[32m[20230204 15:43:54 @agent_ppo2.py:194][0m |          -0.0109 |          11.4416 |           4.4108 |
[32m[20230204 15:43:54 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:43:55 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.71
[32m[20230204 15:43:55 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.67
[32m[20230204 15:43:55 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.75
[32m[20230204 15:43:55 @agent_ppo2.py:152][0m Total time:       7.73 min
[32m[20230204 15:43:55 @agent_ppo2.py:154][0m 593920 total steps have happened
[32m[20230204 15:43:55 @agent_ppo2.py:130][0m #------------------------ Iteration 290 --------------------------#
[32m[20230204 15:43:55 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:55 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:55 @agent_ppo2.py:194][0m |           0.0044 |          12.3260 |           4.3183 |
[32m[20230204 15:43:55 @agent_ppo2.py:194][0m |          -0.0065 |          10.4263 |           4.3111 |
[32m[20230204 15:43:55 @agent_ppo2.py:194][0m |          -0.0081 |           9.5292 |           4.3060 |
[32m[20230204 15:43:55 @agent_ppo2.py:194][0m |          -0.0105 |           9.1443 |           4.3069 |
[32m[20230204 15:43:55 @agent_ppo2.py:194][0m |          -0.0104 |           9.0973 |           4.3076 |
[32m[20230204 15:43:55 @agent_ppo2.py:194][0m |          -0.0101 |           8.7403 |           4.3093 |
[32m[20230204 15:43:55 @agent_ppo2.py:194][0m |          -0.0112 |           8.5311 |           4.3070 |
[32m[20230204 15:43:56 @agent_ppo2.py:194][0m |          -0.0139 |           8.4333 |           4.3037 |
[32m[20230204 15:43:56 @agent_ppo2.py:194][0m |          -0.0088 |           8.4060 |           4.3067 |
[32m[20230204 15:43:56 @agent_ppo2.py:194][0m |          -0.0113 |           8.1861 |           4.3024 |
[32m[20230204 15:43:56 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:43:56 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.07
[32m[20230204 15:43:56 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.81
[32m[20230204 15:43:56 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 262.70
[32m[20230204 15:43:56 @agent_ppo2.py:152][0m Total time:       7.76 min
[32m[20230204 15:43:56 @agent_ppo2.py:154][0m 595968 total steps have happened
[32m[20230204 15:43:56 @agent_ppo2.py:130][0m #------------------------ Iteration 291 --------------------------#
[32m[20230204 15:43:56 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:43:56 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:56 @agent_ppo2.py:194][0m |           0.0024 |          13.3790 |           4.3537 |
[32m[20230204 15:43:56 @agent_ppo2.py:194][0m |          -0.0033 |          11.8771 |           4.3470 |
[32m[20230204 15:43:56 @agent_ppo2.py:194][0m |          -0.0063 |          11.2288 |           4.3428 |
[32m[20230204 15:43:57 @agent_ppo2.py:194][0m |          -0.0071 |          10.8361 |           4.3415 |
[32m[20230204 15:43:57 @agent_ppo2.py:194][0m |          -0.0084 |          10.4614 |           4.3429 |
[32m[20230204 15:43:57 @agent_ppo2.py:194][0m |          -0.0104 |          10.0978 |           4.3433 |
[32m[20230204 15:43:57 @agent_ppo2.py:194][0m |          -0.0097 |           9.7322 |           4.3449 |
[32m[20230204 15:43:57 @agent_ppo2.py:194][0m |          -0.0105 |           9.2961 |           4.3461 |
[32m[20230204 15:43:57 @agent_ppo2.py:194][0m |          -0.0115 |           8.9673 |           4.3451 |
[32m[20230204 15:43:57 @agent_ppo2.py:194][0m |          -0.0109 |           8.8314 |           4.3482 |
[32m[20230204 15:43:57 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:43:57 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.98
[32m[20230204 15:43:57 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.76
[32m[20230204 15:43:57 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.46
[32m[20230204 15:43:57 @agent_ppo2.py:152][0m Total time:       7.78 min
[32m[20230204 15:43:57 @agent_ppo2.py:154][0m 598016 total steps have happened
[32m[20230204 15:43:57 @agent_ppo2.py:130][0m #------------------------ Iteration 292 --------------------------#
[32m[20230204 15:43:57 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:58 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:58 @agent_ppo2.py:194][0m |           0.0008 |          13.8421 |           4.3752 |
[32m[20230204 15:43:58 @agent_ppo2.py:194][0m |          -0.0027 |          12.8114 |           4.3665 |
[32m[20230204 15:43:58 @agent_ppo2.py:194][0m |          -0.0063 |          12.1080 |           4.3593 |
[32m[20230204 15:43:58 @agent_ppo2.py:194][0m |          -0.0087 |          11.5628 |           4.3584 |
[32m[20230204 15:43:58 @agent_ppo2.py:194][0m |          -0.0116 |          11.2189 |           4.3547 |
[32m[20230204 15:43:58 @agent_ppo2.py:194][0m |          -0.0093 |          10.9406 |           4.3573 |
[32m[20230204 15:43:58 @agent_ppo2.py:194][0m |          -0.0124 |          10.5447 |           4.3588 |
[32m[20230204 15:43:58 @agent_ppo2.py:194][0m |          -0.0123 |          10.3168 |           4.3556 |
[32m[20230204 15:43:58 @agent_ppo2.py:194][0m |          -0.0131 |          10.1366 |           4.3516 |
[32m[20230204 15:43:58 @agent_ppo2.py:194][0m |          -0.0120 |          10.0381 |           4.3565 |
[32m[20230204 15:43:58 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:43:59 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.21
[32m[20230204 15:43:59 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.58
[32m[20230204 15:43:59 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 263.19
[32m[20230204 15:43:59 @agent_ppo2.py:152][0m Total time:       7.80 min
[32m[20230204 15:43:59 @agent_ppo2.py:154][0m 600064 total steps have happened
[32m[20230204 15:43:59 @agent_ppo2.py:130][0m #------------------------ Iteration 293 --------------------------#
[32m[20230204 15:43:59 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:43:59 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:43:59 @agent_ppo2.py:194][0m |          -0.0037 |          14.0413 |           4.3509 |
[32m[20230204 15:43:59 @agent_ppo2.py:194][0m |          -0.0077 |          12.7332 |           4.3386 |
[32m[20230204 15:43:59 @agent_ppo2.py:194][0m |          -0.0074 |          12.4316 |           4.3385 |
[32m[20230204 15:43:59 @agent_ppo2.py:194][0m |          -0.0099 |          12.2060 |           4.3362 |
[32m[20230204 15:43:59 @agent_ppo2.py:194][0m |          -0.0152 |          11.9898 |           4.3391 |
[32m[20230204 15:43:59 @agent_ppo2.py:194][0m |          -0.0144 |          11.8181 |           4.3350 |
[32m[20230204 15:43:59 @agent_ppo2.py:194][0m |          -0.0168 |          11.6891 |           4.3377 |
[32m[20230204 15:44:00 @agent_ppo2.py:194][0m |          -0.0129 |          11.5930 |           4.3370 |
[32m[20230204 15:44:00 @agent_ppo2.py:194][0m |          -0.0129 |          11.4813 |           4.3356 |
[32m[20230204 15:44:00 @agent_ppo2.py:194][0m |          -0.0183 |          11.2987 |           4.3342 |
[32m[20230204 15:44:00 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:44:00 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.03
[32m[20230204 15:44:00 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.63
[32m[20230204 15:44:00 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.25
[32m[20230204 15:44:00 @agent_ppo2.py:152][0m Total time:       7.82 min
[32m[20230204 15:44:00 @agent_ppo2.py:154][0m 602112 total steps have happened
[32m[20230204 15:44:00 @agent_ppo2.py:130][0m #------------------------ Iteration 294 --------------------------#
[32m[20230204 15:44:00 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:00 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:00 @agent_ppo2.py:194][0m |           0.0008 |          13.6752 |           4.3774 |
[32m[20230204 15:44:00 @agent_ppo2.py:194][0m |          -0.0029 |          12.8100 |           4.3781 |
[32m[20230204 15:44:00 @agent_ppo2.py:194][0m |          -0.0069 |          12.3239 |           4.3780 |
[32m[20230204 15:44:01 @agent_ppo2.py:194][0m |          -0.0071 |          11.9357 |           4.3741 |
[32m[20230204 15:44:01 @agent_ppo2.py:194][0m |          -0.0084 |          11.5239 |           4.3757 |
[32m[20230204 15:44:01 @agent_ppo2.py:194][0m |          -0.0083 |          11.3022 |           4.3722 |
[32m[20230204 15:44:01 @agent_ppo2.py:194][0m |          -0.0077 |          11.2668 |           4.3702 |
[32m[20230204 15:44:01 @agent_ppo2.py:194][0m |          -0.0094 |          11.0444 |           4.3740 |
[32m[20230204 15:44:01 @agent_ppo2.py:194][0m |          -0.0095 |          10.9112 |           4.3705 |
[32m[20230204 15:44:01 @agent_ppo2.py:194][0m |          -0.0093 |          10.9913 |           4.3711 |
[32m[20230204 15:44:01 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:44:01 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.10
[32m[20230204 15:44:01 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.47
[32m[20230204 15:44:01 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 262.03
[32m[20230204 15:44:01 @agent_ppo2.py:152][0m Total time:       7.85 min
[32m[20230204 15:44:01 @agent_ppo2.py:154][0m 604160 total steps have happened
[32m[20230204 15:44:01 @agent_ppo2.py:130][0m #------------------------ Iteration 295 --------------------------#
[32m[20230204 15:44:01 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:44:02 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:02 @agent_ppo2.py:194][0m |           0.0011 |          13.8013 |           4.3400 |
[32m[20230204 15:44:02 @agent_ppo2.py:194][0m |          -0.0076 |          12.2135 |           4.3241 |
[32m[20230204 15:44:02 @agent_ppo2.py:194][0m |          -0.0104 |          11.7146 |           4.3243 |
[32m[20230204 15:44:02 @agent_ppo2.py:194][0m |          -0.0066 |          11.2540 |           4.3247 |
[32m[20230204 15:44:02 @agent_ppo2.py:194][0m |          -0.0060 |          11.3025 |           4.3205 |
[32m[20230204 15:44:02 @agent_ppo2.py:194][0m |          -0.0147 |          10.4907 |           4.3204 |
[32m[20230204 15:44:02 @agent_ppo2.py:194][0m |          -0.0127 |          10.1520 |           4.3184 |
[32m[20230204 15:44:02 @agent_ppo2.py:194][0m |          -0.0127 |           9.8420 |           4.3206 |
[32m[20230204 15:44:02 @agent_ppo2.py:194][0m |          -0.0118 |           9.5344 |           4.3150 |
[32m[20230204 15:44:02 @agent_ppo2.py:194][0m |          -0.0135 |           9.3017 |           4.3159 |
[32m[20230204 15:44:02 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:44:03 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.30
[32m[20230204 15:44:03 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 257.84
[32m[20230204 15:44:03 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.31
[32m[20230204 15:44:03 @agent_ppo2.py:152][0m Total time:       7.87 min
[32m[20230204 15:44:03 @agent_ppo2.py:154][0m 606208 total steps have happened
[32m[20230204 15:44:03 @agent_ppo2.py:130][0m #------------------------ Iteration 296 --------------------------#
[32m[20230204 15:44:03 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:03 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:03 @agent_ppo2.py:194][0m |          -0.0011 |          14.9774 |           4.4283 |
[32m[20230204 15:44:03 @agent_ppo2.py:194][0m |          -0.0042 |          13.4391 |           4.4179 |
[32m[20230204 15:44:03 @agent_ppo2.py:194][0m |          -0.0087 |          12.7346 |           4.4222 |
[32m[20230204 15:44:03 @agent_ppo2.py:194][0m |          -0.0090 |          12.4112 |           4.4136 |
[32m[20230204 15:44:03 @agent_ppo2.py:194][0m |          -0.0112 |          12.0756 |           4.4193 |
[32m[20230204 15:44:03 @agent_ppo2.py:194][0m |          -0.0092 |          12.0412 |           4.4150 |
[32m[20230204 15:44:03 @agent_ppo2.py:194][0m |          -0.0103 |          11.6699 |           4.4150 |
[32m[20230204 15:44:03 @agent_ppo2.py:194][0m |          -0.0116 |          11.5615 |           4.4171 |
[32m[20230204 15:44:04 @agent_ppo2.py:194][0m |          -0.0084 |          11.6253 |           4.4130 |
[32m[20230204 15:44:04 @agent_ppo2.py:194][0m |          -0.0135 |          10.9817 |           4.4097 |
[32m[20230204 15:44:04 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:44:04 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.27
[32m[20230204 15:44:04 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.74
[32m[20230204 15:44:04 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.07
[32m[20230204 15:44:04 @agent_ppo2.py:152][0m Total time:       7.89 min
[32m[20230204 15:44:04 @agent_ppo2.py:154][0m 608256 total steps have happened
[32m[20230204 15:44:04 @agent_ppo2.py:130][0m #------------------------ Iteration 297 --------------------------#
[32m[20230204 15:44:04 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:04 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:04 @agent_ppo2.py:194][0m |          -0.0011 |          13.9531 |           4.3288 |
[32m[20230204 15:44:04 @agent_ppo2.py:194][0m |          -0.0053 |          12.9636 |           4.3157 |
[32m[20230204 15:44:04 @agent_ppo2.py:194][0m |          -0.0070 |          12.4661 |           4.3072 |
[32m[20230204 15:44:05 @agent_ppo2.py:194][0m |          -0.0091 |          12.1410 |           4.3122 |
[32m[20230204 15:44:05 @agent_ppo2.py:194][0m |          -0.0104 |          11.8088 |           4.3124 |
[32m[20230204 15:44:05 @agent_ppo2.py:194][0m |          -0.0071 |          12.0351 |           4.3068 |
[32m[20230204 15:44:05 @agent_ppo2.py:194][0m |          -0.0099 |          11.5608 |           4.3099 |
[32m[20230204 15:44:05 @agent_ppo2.py:194][0m |          -0.0118 |          11.2318 |           4.3062 |
[32m[20230204 15:44:05 @agent_ppo2.py:194][0m |          -0.0097 |          11.3083 |           4.3065 |
[32m[20230204 15:44:05 @agent_ppo2.py:194][0m |          -0.0103 |          11.1208 |           4.3066 |
[32m[20230204 15:44:05 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:44:05 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.23
[32m[20230204 15:44:05 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.60
[32m[20230204 15:44:05 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.11
[32m[20230204 15:44:05 @agent_ppo2.py:152][0m Total time:       7.91 min
[32m[20230204 15:44:05 @agent_ppo2.py:154][0m 610304 total steps have happened
[32m[20230204 15:44:05 @agent_ppo2.py:130][0m #------------------------ Iteration 298 --------------------------#
[32m[20230204 15:44:05 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:05 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:06 @agent_ppo2.py:194][0m |           0.0018 |          13.0477 |           4.2883 |
[32m[20230204 15:44:06 @agent_ppo2.py:194][0m |          -0.0056 |          10.9828 |           4.2825 |
[32m[20230204 15:44:06 @agent_ppo2.py:194][0m |          -0.0070 |           9.7586 |           4.2705 |
[32m[20230204 15:44:06 @agent_ppo2.py:194][0m |          -0.0099 |           8.8898 |           4.2753 |
[32m[20230204 15:44:06 @agent_ppo2.py:194][0m |          -0.0103 |           8.3969 |           4.2709 |
[32m[20230204 15:44:06 @agent_ppo2.py:194][0m |          -0.0135 |           8.0514 |           4.2740 |
[32m[20230204 15:44:06 @agent_ppo2.py:194][0m |          -0.0095 |           7.6931 |           4.2676 |
[32m[20230204 15:44:06 @agent_ppo2.py:194][0m |          -0.0140 |           7.4505 |           4.2700 |
[32m[20230204 15:44:06 @agent_ppo2.py:194][0m |          -0.0146 |           7.2046 |           4.2639 |
[32m[20230204 15:44:06 @agent_ppo2.py:194][0m |          -0.0123 |           7.0133 |           4.2710 |
[32m[20230204 15:44:06 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:44:06 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.37
[32m[20230204 15:44:06 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.03
[32m[20230204 15:44:06 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.03
[32m[20230204 15:44:06 @agent_ppo2.py:152][0m Total time:       7.93 min
[32m[20230204 15:44:06 @agent_ppo2.py:154][0m 612352 total steps have happened
[32m[20230204 15:44:06 @agent_ppo2.py:130][0m #------------------------ Iteration 299 --------------------------#
[32m[20230204 15:44:07 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:07 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:07 @agent_ppo2.py:194][0m |           0.0010 |          14.6157 |           4.4542 |
[32m[20230204 15:44:07 @agent_ppo2.py:194][0m |          -0.0030 |          12.8099 |           4.4547 |
[32m[20230204 15:44:07 @agent_ppo2.py:194][0m |          -0.0040 |          12.2389 |           4.4523 |
[32m[20230204 15:44:07 @agent_ppo2.py:194][0m |          -0.0062 |          11.8226 |           4.4505 |
[32m[20230204 15:44:07 @agent_ppo2.py:194][0m |          -0.0060 |          11.4516 |           4.4470 |
[32m[20230204 15:44:07 @agent_ppo2.py:194][0m |          -0.0075 |          11.1771 |           4.4508 |
[32m[20230204 15:44:07 @agent_ppo2.py:194][0m |          -0.0077 |          10.9027 |           4.4472 |
[32m[20230204 15:44:07 @agent_ppo2.py:194][0m |          -0.0076 |          10.6595 |           4.4495 |
[32m[20230204 15:44:07 @agent_ppo2.py:194][0m |          -0.0086 |          10.4353 |           4.4502 |
[32m[20230204 15:44:08 @agent_ppo2.py:194][0m |          -0.0088 |          10.2413 |           4.4469 |
[32m[20230204 15:44:08 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:44:08 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.45
[32m[20230204 15:44:08 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.63
[32m[20230204 15:44:08 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.11
[32m[20230204 15:44:08 @agent_ppo2.py:152][0m Total time:       7.96 min
[32m[20230204 15:44:08 @agent_ppo2.py:154][0m 614400 total steps have happened
[32m[20230204 15:44:08 @agent_ppo2.py:130][0m #------------------------ Iteration 300 --------------------------#
[32m[20230204 15:44:08 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:08 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:08 @agent_ppo2.py:194][0m |          -0.0015 |          12.8140 |           4.4244 |
[32m[20230204 15:44:08 @agent_ppo2.py:194][0m |          -0.0066 |          10.1136 |           4.4154 |
[32m[20230204 15:44:08 @agent_ppo2.py:194][0m |          -0.0101 |           8.5289 |           4.4114 |
[32m[20230204 15:44:08 @agent_ppo2.py:194][0m |          -0.0099 |           7.7466 |           4.4117 |
[32m[20230204 15:44:08 @agent_ppo2.py:194][0m |          -0.0101 |           7.2964 |           4.4125 |
[32m[20230204 15:44:09 @agent_ppo2.py:194][0m |          -0.0116 |           6.9196 |           4.4104 |
[32m[20230204 15:44:09 @agent_ppo2.py:194][0m |          -0.0126 |           6.5215 |           4.4123 |
[32m[20230204 15:44:09 @agent_ppo2.py:194][0m |          -0.0136 |           6.2545 |           4.4117 |
[32m[20230204 15:44:09 @agent_ppo2.py:194][0m |          -0.0117 |           6.0558 |           4.4142 |
[32m[20230204 15:44:09 @agent_ppo2.py:194][0m |          -0.0111 |           5.8121 |           4.4078 |
[32m[20230204 15:44:09 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:44:09 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.95
[32m[20230204 15:44:09 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.17
[32m[20230204 15:44:09 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.61
[32m[20230204 15:44:09 @agent_ppo2.py:152][0m Total time:       7.98 min
[32m[20230204 15:44:09 @agent_ppo2.py:154][0m 616448 total steps have happened
[32m[20230204 15:44:09 @agent_ppo2.py:130][0m #------------------------ Iteration 301 --------------------------#
[32m[20230204 15:44:09 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:09 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:09 @agent_ppo2.py:194][0m |           0.0014 |          16.3963 |           4.3256 |
[32m[20230204 15:44:10 @agent_ppo2.py:194][0m |          -0.0066 |          13.2810 |           4.3142 |
[32m[20230204 15:44:10 @agent_ppo2.py:194][0m |          -0.0050 |          12.4464 |           4.3149 |
[32m[20230204 15:44:10 @agent_ppo2.py:194][0m |          -0.0069 |          11.9360 |           4.3149 |
[32m[20230204 15:44:10 @agent_ppo2.py:194][0m |          -0.0076 |          11.7035 |           4.3162 |
[32m[20230204 15:44:10 @agent_ppo2.py:194][0m |          -0.0075 |          11.2181 |           4.3182 |
[32m[20230204 15:44:10 @agent_ppo2.py:194][0m |          -0.0098 |          10.8754 |           4.3156 |
[32m[20230204 15:44:10 @agent_ppo2.py:194][0m |          -0.0092 |          10.6373 |           4.3095 |
[32m[20230204 15:44:10 @agent_ppo2.py:194][0m |          -0.0107 |          10.3261 |           4.3125 |
[32m[20230204 15:44:10 @agent_ppo2.py:194][0m |          -0.0112 |          10.2059 |           4.3108 |
[32m[20230204 15:44:10 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:44:10 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.38
[32m[20230204 15:44:10 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.41
[32m[20230204 15:44:10 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.67
[32m[20230204 15:44:10 @agent_ppo2.py:152][0m Total time:       8.00 min
[32m[20230204 15:44:10 @agent_ppo2.py:154][0m 618496 total steps have happened
[32m[20230204 15:44:10 @agent_ppo2.py:130][0m #------------------------ Iteration 302 --------------------------#
[32m[20230204 15:44:11 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:11 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:11 @agent_ppo2.py:194][0m |          -0.0001 |          14.8220 |           4.3443 |
[32m[20230204 15:44:11 @agent_ppo2.py:194][0m |          -0.0033 |          12.9131 |           4.3394 |
[32m[20230204 15:44:11 @agent_ppo2.py:194][0m |          -0.0065 |          12.1750 |           4.3394 |
[32m[20230204 15:44:11 @agent_ppo2.py:194][0m |          -0.0076 |          11.8335 |           4.3426 |
[32m[20230204 15:44:11 @agent_ppo2.py:194][0m |          -0.0087 |          11.5026 |           4.3372 |
[32m[20230204 15:44:11 @agent_ppo2.py:194][0m |          -0.0082 |          11.2971 |           4.3422 |
[32m[20230204 15:44:11 @agent_ppo2.py:194][0m |          -0.0088 |          11.0724 |           4.3379 |
[32m[20230204 15:44:11 @agent_ppo2.py:194][0m |          -0.0111 |          10.6492 |           4.3374 |
[32m[20230204 15:44:11 @agent_ppo2.py:194][0m |          -0.0110 |          10.5522 |           4.3397 |
[32m[20230204 15:44:11 @agent_ppo2.py:194][0m |          -0.0100 |          10.2933 |           4.3387 |
[32m[20230204 15:44:11 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:44:12 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.62
[32m[20230204 15:44:12 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.70
[32m[20230204 15:44:12 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.89
[32m[20230204 15:44:12 @agent_ppo2.py:152][0m Total time:       8.02 min
[32m[20230204 15:44:12 @agent_ppo2.py:154][0m 620544 total steps have happened
[32m[20230204 15:44:12 @agent_ppo2.py:130][0m #------------------------ Iteration 303 --------------------------#
[32m[20230204 15:44:12 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:44:12 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:12 @agent_ppo2.py:194][0m |           0.0029 |          25.6163 |           4.3227 |
[32m[20230204 15:44:12 @agent_ppo2.py:194][0m |          -0.0038 |          14.2794 |           4.3210 |
[32m[20230204 15:44:12 @agent_ppo2.py:194][0m |          -0.0033 |          13.0038 |           4.3163 |
[32m[20230204 15:44:12 @agent_ppo2.py:194][0m |          -0.0047 |          12.2200 |           4.3144 |
[32m[20230204 15:44:12 @agent_ppo2.py:194][0m |          -0.0070 |          11.6385 |           4.3108 |
[32m[20230204 15:44:13 @agent_ppo2.py:194][0m |          -0.0100 |          11.3222 |           4.3139 |
[32m[20230204 15:44:13 @agent_ppo2.py:194][0m |          -0.0100 |          11.2412 |           4.3079 |
[32m[20230204 15:44:13 @agent_ppo2.py:194][0m |          -0.0094 |          10.9020 |           4.3086 |
[32m[20230204 15:44:13 @agent_ppo2.py:194][0m |          -0.0101 |          10.8142 |           4.3041 |
[32m[20230204 15:44:13 @agent_ppo2.py:194][0m |          -0.0111 |          10.5269 |           4.3028 |
[32m[20230204 15:44:13 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:44:13 @agent_ppo2.py:147][0m Average TRAINING episode reward: 225.87
[32m[20230204 15:44:13 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.81
[32m[20230204 15:44:13 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.70
[32m[20230204 15:44:13 @agent_ppo2.py:152][0m Total time:       8.04 min
[32m[20230204 15:44:13 @agent_ppo2.py:154][0m 622592 total steps have happened
[32m[20230204 15:44:13 @agent_ppo2.py:130][0m #------------------------ Iteration 304 --------------------------#
[32m[20230204 15:44:13 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:13 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:13 @agent_ppo2.py:194][0m |          -0.0006 |          13.7195 |           4.3312 |
[32m[20230204 15:44:14 @agent_ppo2.py:194][0m |          -0.0049 |          11.9541 |           4.3317 |
[32m[20230204 15:44:14 @agent_ppo2.py:194][0m |          -0.0087 |          11.1926 |           4.3317 |
[32m[20230204 15:44:14 @agent_ppo2.py:194][0m |          -0.0100 |          10.5832 |           4.3301 |
[32m[20230204 15:44:14 @agent_ppo2.py:194][0m |          -0.0106 |          10.1725 |           4.3304 |
[32m[20230204 15:44:14 @agent_ppo2.py:194][0m |          -0.0121 |           9.7848 |           4.3309 |
[32m[20230204 15:44:14 @agent_ppo2.py:194][0m |          -0.0122 |           9.4275 |           4.3324 |
[32m[20230204 15:44:14 @agent_ppo2.py:194][0m |          -0.0130 |           9.1880 |           4.3295 |
[32m[20230204 15:44:14 @agent_ppo2.py:194][0m |          -0.0141 |           8.8495 |           4.3330 |
[32m[20230204 15:44:14 @agent_ppo2.py:194][0m |          -0.0138 |           8.5409 |           4.3306 |
[32m[20230204 15:44:14 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:44:14 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.11
[32m[20230204 15:44:14 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.86
[32m[20230204 15:44:14 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.72
[32m[20230204 15:44:14 @agent_ppo2.py:152][0m Total time:       8.06 min
[32m[20230204 15:44:14 @agent_ppo2.py:154][0m 624640 total steps have happened
[32m[20230204 15:44:14 @agent_ppo2.py:130][0m #------------------------ Iteration 305 --------------------------#
[32m[20230204 15:44:15 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:15 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:15 @agent_ppo2.py:194][0m |          -0.0023 |          15.4558 |           4.2914 |
[32m[20230204 15:44:15 @agent_ppo2.py:194][0m |          -0.0042 |          14.9451 |           4.2832 |
[32m[20230204 15:44:15 @agent_ppo2.py:194][0m |          -0.0061 |          14.2661 |           4.2790 |
[32m[20230204 15:44:15 @agent_ppo2.py:194][0m |          -0.0102 |          13.5775 |           4.2744 |
[32m[20230204 15:44:15 @agent_ppo2.py:194][0m |          -0.0096 |          13.3378 |           4.2739 |
[32m[20230204 15:44:15 @agent_ppo2.py:194][0m |          -0.0088 |          13.0010 |           4.2725 |
[32m[20230204 15:44:15 @agent_ppo2.py:194][0m |          -0.0141 |          12.7534 |           4.2671 |
[32m[20230204 15:44:15 @agent_ppo2.py:194][0m |          -0.0095 |          12.8702 |           4.2671 |
[32m[20230204 15:44:15 @agent_ppo2.py:194][0m |          -0.0107 |          12.3802 |           4.2640 |
[32m[20230204 15:44:15 @agent_ppo2.py:194][0m |          -0.0147 |          12.1712 |           4.2612 |
[32m[20230204 15:44:15 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:44:16 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.82
[32m[20230204 15:44:16 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.84
[32m[20230204 15:44:16 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.32
[32m[20230204 15:44:16 @agent_ppo2.py:152][0m Total time:       8.09 min
[32m[20230204 15:44:16 @agent_ppo2.py:154][0m 626688 total steps have happened
[32m[20230204 15:44:16 @agent_ppo2.py:130][0m #------------------------ Iteration 306 --------------------------#
[32m[20230204 15:44:16 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:44:16 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:16 @agent_ppo2.py:194][0m |           0.0027 |          10.6354 |           4.2912 |
[32m[20230204 15:44:16 @agent_ppo2.py:194][0m |          -0.0034 |           5.6434 |           4.2834 |
[32m[20230204 15:44:16 @agent_ppo2.py:194][0m |          -0.0031 |           4.3064 |           4.2846 |
[32m[20230204 15:44:16 @agent_ppo2.py:194][0m |          -0.0044 |           3.7802 |           4.2819 |
[32m[20230204 15:44:16 @agent_ppo2.py:194][0m |          -0.0069 |           3.4863 |           4.2799 |
[32m[20230204 15:44:17 @agent_ppo2.py:194][0m |          -0.0085 |           3.2013 |           4.2843 |
[32m[20230204 15:44:17 @agent_ppo2.py:194][0m |          -0.0069 |           3.0123 |           4.2795 |
[32m[20230204 15:44:17 @agent_ppo2.py:194][0m |          -0.0086 |           2.8827 |           4.2811 |
[32m[20230204 15:44:17 @agent_ppo2.py:194][0m |          -0.0087 |           2.7995 |           4.2840 |
[32m[20230204 15:44:17 @agent_ppo2.py:194][0m |          -0.0080 |           2.6901 |           4.2810 |
[32m[20230204 15:44:17 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:44:17 @agent_ppo2.py:147][0m Average TRAINING episode reward: 254.48
[32m[20230204 15:44:17 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 257.13
[32m[20230204 15:44:17 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.42
[32m[20230204 15:44:17 @agent_ppo2.py:152][0m Total time:       8.11 min
[32m[20230204 15:44:17 @agent_ppo2.py:154][0m 628736 total steps have happened
[32m[20230204 15:44:17 @agent_ppo2.py:130][0m #------------------------ Iteration 307 --------------------------#
[32m[20230204 15:44:17 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:17 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:17 @agent_ppo2.py:194][0m |           0.0043 |          13.8673 |           4.3301 |
[32m[20230204 15:44:18 @agent_ppo2.py:194][0m |          -0.0063 |          13.2385 |           4.3302 |
[32m[20230204 15:44:18 @agent_ppo2.py:194][0m |          -0.0095 |          12.9624 |           4.3186 |
[32m[20230204 15:44:18 @agent_ppo2.py:194][0m |          -0.0099 |          12.7816 |           4.3212 |
[32m[20230204 15:44:18 @agent_ppo2.py:194][0m |          -0.0059 |          12.6375 |           4.3270 |
[32m[20230204 15:44:18 @agent_ppo2.py:194][0m |          -0.0086 |          12.5110 |           4.3228 |
[32m[20230204 15:44:18 @agent_ppo2.py:194][0m |          -0.0125 |          12.3946 |           4.3175 |
[32m[20230204 15:44:18 @agent_ppo2.py:194][0m |          -0.0062 |          12.4457 |           4.3213 |
[32m[20230204 15:44:18 @agent_ppo2.py:194][0m |          -0.0063 |          12.4176 |           4.3186 |
[32m[20230204 15:44:18 @agent_ppo2.py:194][0m |          -0.0122 |          12.1676 |           4.3173 |
[32m[20230204 15:44:18 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:44:18 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.15
[32m[20230204 15:44:18 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.84
[32m[20230204 15:44:18 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.42
[32m[20230204 15:44:18 @agent_ppo2.py:152][0m Total time:       8.13 min
[32m[20230204 15:44:18 @agent_ppo2.py:154][0m 630784 total steps have happened
[32m[20230204 15:44:18 @agent_ppo2.py:130][0m #------------------------ Iteration 308 --------------------------#
[32m[20230204 15:44:19 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:19 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:19 @agent_ppo2.py:194][0m |           0.0020 |          12.9942 |           4.3694 |
[32m[20230204 15:44:19 @agent_ppo2.py:194][0m |          -0.0019 |          11.7999 |           4.3622 |
[32m[20230204 15:44:19 @agent_ppo2.py:194][0m |          -0.0036 |          11.1430 |           4.3580 |
[32m[20230204 15:44:19 @agent_ppo2.py:194][0m |          -0.0049 |          10.6282 |           4.3559 |
[32m[20230204 15:44:19 @agent_ppo2.py:194][0m |          -0.0053 |          10.1915 |           4.3573 |
[32m[20230204 15:44:19 @agent_ppo2.py:194][0m |          -0.0057 |           9.8469 |           4.3507 |
[32m[20230204 15:44:19 @agent_ppo2.py:194][0m |          -0.0063 |           9.5050 |           4.3543 |
[32m[20230204 15:44:19 @agent_ppo2.py:194][0m |          -0.0066 |           9.1911 |           4.3521 |
[32m[20230204 15:44:19 @agent_ppo2.py:194][0m |          -0.0072 |           9.0368 |           4.3527 |
[32m[20230204 15:44:20 @agent_ppo2.py:194][0m |          -0.0077 |           8.7973 |           4.3518 |
[32m[20230204 15:44:20 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:44:20 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.89
[32m[20230204 15:44:20 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 257.68
[32m[20230204 15:44:20 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.62
[32m[20230204 15:44:20 @agent_ppo2.py:152][0m Total time:       8.15 min
[32m[20230204 15:44:20 @agent_ppo2.py:154][0m 632832 total steps have happened
[32m[20230204 15:44:20 @agent_ppo2.py:130][0m #------------------------ Iteration 309 --------------------------#
[32m[20230204 15:44:20 @agent_ppo2.py:136][0m Sampling time: 0.29 s by 4 slaves
[32m[20230204 15:44:20 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:20 @agent_ppo2.py:194][0m |           0.0014 |          14.8270 |           4.2818 |
[32m[20230204 15:44:20 @agent_ppo2.py:194][0m |          -0.0028 |          13.8367 |           4.2806 |
[32m[20230204 15:44:20 @agent_ppo2.py:194][0m |          -0.0046 |          13.4189 |           4.2755 |
[32m[20230204 15:44:20 @agent_ppo2.py:194][0m |          -0.0031 |          13.3250 |           4.2737 |
[32m[20230204 15:44:20 @agent_ppo2.py:194][0m |          -0.0084 |          12.9246 |           4.2707 |
[32m[20230204 15:44:21 @agent_ppo2.py:194][0m |          -0.0074 |          12.6904 |           4.2716 |
[32m[20230204 15:44:21 @agent_ppo2.py:194][0m |          -0.0075 |          12.5631 |           4.2710 |
[32m[20230204 15:44:21 @agent_ppo2.py:194][0m |          -0.0065 |          12.4137 |           4.2697 |
[32m[20230204 15:44:21 @agent_ppo2.py:194][0m |          -0.0094 |          12.2684 |           4.2670 |
[32m[20230204 15:44:21 @agent_ppo2.py:194][0m |          -0.0099 |          12.2020 |           4.2682 |
[32m[20230204 15:44:21 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:44:21 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.75
[32m[20230204 15:44:21 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.86
[32m[20230204 15:44:21 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 262.87
[32m[20230204 15:44:21 @agent_ppo2.py:152][0m Total time:       8.18 min
[32m[20230204 15:44:21 @agent_ppo2.py:154][0m 634880 total steps have happened
[32m[20230204 15:44:21 @agent_ppo2.py:130][0m #------------------------ Iteration 310 --------------------------#
[32m[20230204 15:44:21 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:21 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:21 @agent_ppo2.py:194][0m |          -0.0003 |          11.8245 |           4.3266 |
[32m[20230204 15:44:22 @agent_ppo2.py:194][0m |          -0.0066 |           9.3645 |           4.3186 |
[32m[20230204 15:44:22 @agent_ppo2.py:194][0m |          -0.0094 |           8.6836 |           4.3187 |
[32m[20230204 15:44:22 @agent_ppo2.py:194][0m |          -0.0108 |           8.2448 |           4.3163 |
[32m[20230204 15:44:22 @agent_ppo2.py:194][0m |          -0.0115 |           7.9323 |           4.3185 |
[32m[20230204 15:44:22 @agent_ppo2.py:194][0m |          -0.0124 |           7.6597 |           4.3189 |
[32m[20230204 15:44:22 @agent_ppo2.py:194][0m |          -0.0129 |           7.4187 |           4.3162 |
[32m[20230204 15:44:22 @agent_ppo2.py:194][0m |          -0.0146 |           7.2030 |           4.3137 |
[32m[20230204 15:44:22 @agent_ppo2.py:194][0m |          -0.0137 |           6.7902 |           4.3119 |
[32m[20230204 15:44:22 @agent_ppo2.py:194][0m |          -0.0149 |           6.5071 |           4.3112 |
[32m[20230204 15:44:22 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:44:22 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.39
[32m[20230204 15:44:22 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.11
[32m[20230204 15:44:22 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.64
[32m[20230204 15:44:22 @agent_ppo2.py:152][0m Total time:       8.20 min
[32m[20230204 15:44:22 @agent_ppo2.py:154][0m 636928 total steps have happened
[32m[20230204 15:44:22 @agent_ppo2.py:130][0m #------------------------ Iteration 311 --------------------------#
[32m[20230204 15:44:23 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:23 @agent_ppo2.py:194][0m |           0.0012 |          15.4294 |           4.2807 |
[32m[20230204 15:44:23 @agent_ppo2.py:194][0m |          -0.0036 |          14.5073 |           4.2651 |
[32m[20230204 15:44:23 @agent_ppo2.py:194][0m |          -0.0077 |          13.8080 |           4.2597 |
[32m[20230204 15:44:23 @agent_ppo2.py:194][0m |          -0.0089 |          13.4962 |           4.2617 |
[32m[20230204 15:44:23 @agent_ppo2.py:194][0m |          -0.0093 |          13.2185 |           4.2601 |
[32m[20230204 15:44:23 @agent_ppo2.py:194][0m |          -0.0114 |          12.9952 |           4.2591 |
[32m[20230204 15:44:23 @agent_ppo2.py:194][0m |          -0.0121 |          12.8212 |           4.2531 |
[32m[20230204 15:44:23 @agent_ppo2.py:194][0m |          -0.0090 |          12.8042 |           4.2508 |
[32m[20230204 15:44:23 @agent_ppo2.py:194][0m |          -0.0107 |          12.4887 |           4.2483 |
[32m[20230204 15:44:23 @agent_ppo2.py:194][0m |          -0.0141 |          12.3530 |           4.2491 |
[32m[20230204 15:44:23 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:44:24 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.47
[32m[20230204 15:44:24 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.14
[32m[20230204 15:44:24 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.65
[32m[20230204 15:44:24 @agent_ppo2.py:152][0m Total time:       8.22 min
[32m[20230204 15:44:24 @agent_ppo2.py:154][0m 638976 total steps have happened
[32m[20230204 15:44:24 @agent_ppo2.py:130][0m #------------------------ Iteration 312 --------------------------#
[32m[20230204 15:44:24 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:24 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:24 @agent_ppo2.py:194][0m |           0.0009 |          13.8270 |           4.2622 |
[32m[20230204 15:44:24 @agent_ppo2.py:194][0m |          -0.0020 |          12.3208 |           4.2590 |
[32m[20230204 15:44:24 @agent_ppo2.py:194][0m |          -0.0082 |          11.4798 |           4.2554 |
[32m[20230204 15:44:24 @agent_ppo2.py:194][0m |          -0.0090 |          10.9849 |           4.2525 |
[32m[20230204 15:44:24 @agent_ppo2.py:194][0m |          -0.0108 |          10.4790 |           4.2576 |
[32m[20230204 15:44:25 @agent_ppo2.py:194][0m |          -0.0091 |          10.1365 |           4.2506 |
[32m[20230204 15:44:25 @agent_ppo2.py:194][0m |          -0.0098 |           9.7833 |           4.2499 |
[32m[20230204 15:44:25 @agent_ppo2.py:194][0m |          -0.0113 |           9.2807 |           4.2506 |
[32m[20230204 15:44:25 @agent_ppo2.py:194][0m |          -0.0109 |           8.7972 |           4.2481 |
[32m[20230204 15:44:25 @agent_ppo2.py:194][0m |          -0.0135 |           8.3001 |           4.2452 |
[32m[20230204 15:44:25 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:44:25 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.97
[32m[20230204 15:44:25 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.00
[32m[20230204 15:44:25 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 263.55
[32m[20230204 15:44:25 @agent_ppo2.py:152][0m Total time:       8.24 min
[32m[20230204 15:44:25 @agent_ppo2.py:154][0m 641024 total steps have happened
[32m[20230204 15:44:25 @agent_ppo2.py:130][0m #------------------------ Iteration 313 --------------------------#
[32m[20230204 15:44:25 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:25 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:25 @agent_ppo2.py:194][0m |          -0.0002 |          13.4997 |           4.3017 |
[32m[20230204 15:44:26 @agent_ppo2.py:194][0m |          -0.0051 |          11.2863 |           4.2969 |
[32m[20230204 15:44:26 @agent_ppo2.py:194][0m |          -0.0069 |          10.5342 |           4.2976 |
[32m[20230204 15:44:26 @agent_ppo2.py:194][0m |          -0.0072 |          10.1438 |           4.2920 |
[32m[20230204 15:44:26 @agent_ppo2.py:194][0m |          -0.0083 |           9.8154 |           4.2871 |
[32m[20230204 15:44:26 @agent_ppo2.py:194][0m |          -0.0092 |           9.6333 |           4.2877 |
[32m[20230204 15:44:26 @agent_ppo2.py:194][0m |          -0.0094 |           9.3159 |           4.2885 |
[32m[20230204 15:44:26 @agent_ppo2.py:194][0m |          -0.0100 |           9.1621 |           4.2882 |
[32m[20230204 15:44:26 @agent_ppo2.py:194][0m |          -0.0105 |           8.9835 |           4.2865 |
[32m[20230204 15:44:26 @agent_ppo2.py:194][0m |          -0.0109 |           8.8064 |           4.2889 |
[32m[20230204 15:44:26 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:44:26 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.34
[32m[20230204 15:44:26 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.97
[32m[20230204 15:44:26 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.85
[32m[20230204 15:44:26 @agent_ppo2.py:152][0m Total time:       8.27 min
[32m[20230204 15:44:26 @agent_ppo2.py:154][0m 643072 total steps have happened
[32m[20230204 15:44:26 @agent_ppo2.py:130][0m #------------------------ Iteration 314 --------------------------#
[32m[20230204 15:44:27 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:44:27 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:27 @agent_ppo2.py:194][0m |           0.0000 |          14.9048 |           4.2245 |
[32m[20230204 15:44:27 @agent_ppo2.py:194][0m |          -0.0037 |          13.6834 |           4.2204 |
[32m[20230204 15:44:27 @agent_ppo2.py:194][0m |          -0.0064 |          13.3627 |           4.2189 |
[32m[20230204 15:44:27 @agent_ppo2.py:194][0m |          -0.0076 |          12.9611 |           4.2121 |
[32m[20230204 15:44:27 @agent_ppo2.py:194][0m |          -0.0050 |          12.8506 |           4.2190 |
[32m[20230204 15:44:27 @agent_ppo2.py:194][0m |          -0.0087 |          12.5005 |           4.2187 |
[32m[20230204 15:44:27 @agent_ppo2.py:194][0m |          -0.0084 |          12.2262 |           4.2154 |
[32m[20230204 15:44:27 @agent_ppo2.py:194][0m |          -0.0098 |          11.8562 |           4.2155 |
[32m[20230204 15:44:27 @agent_ppo2.py:194][0m |          -0.0114 |          11.6460 |           4.2158 |
[32m[20230204 15:44:28 @agent_ppo2.py:194][0m |          -0.0099 |          11.4227 |           4.2154 |
[32m[20230204 15:44:28 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:44:28 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.59
[32m[20230204 15:44:28 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.55
[32m[20230204 15:44:28 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.73
[32m[20230204 15:44:28 @agent_ppo2.py:152][0m Total time:       8.29 min
[32m[20230204 15:44:28 @agent_ppo2.py:154][0m 645120 total steps have happened
[32m[20230204 15:44:28 @agent_ppo2.py:130][0m #------------------------ Iteration 315 --------------------------#
[32m[20230204 15:44:28 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:28 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:28 @agent_ppo2.py:194][0m |           0.0013 |          15.1908 |           4.2693 |
[32m[20230204 15:44:28 @agent_ppo2.py:194][0m |          -0.0042 |          14.0845 |           4.2631 |
[32m[20230204 15:44:28 @agent_ppo2.py:194][0m |          -0.0052 |          13.5711 |           4.2595 |
[32m[20230204 15:44:28 @agent_ppo2.py:194][0m |          -0.0070 |          13.1938 |           4.2609 |
[32m[20230204 15:44:28 @agent_ppo2.py:194][0m |          -0.0078 |          12.9088 |           4.2630 |
[32m[20230204 15:44:28 @agent_ppo2.py:194][0m |          -0.0074 |          12.7134 |           4.2587 |
[32m[20230204 15:44:29 @agent_ppo2.py:194][0m |          -0.0086 |          12.4984 |           4.2620 |
[32m[20230204 15:44:29 @agent_ppo2.py:194][0m |          -0.0087 |          12.3135 |           4.2620 |
[32m[20230204 15:44:29 @agent_ppo2.py:194][0m |          -0.0088 |          12.1167 |           4.2568 |
[32m[20230204 15:44:29 @agent_ppo2.py:194][0m |          -0.0098 |          11.9070 |           4.2628 |
[32m[20230204 15:44:29 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:44:29 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.65
[32m[20230204 15:44:29 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.28
[32m[20230204 15:44:29 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 262.77
[32m[20230204 15:44:29 @agent_ppo2.py:152][0m Total time:       8.31 min
[32m[20230204 15:44:29 @agent_ppo2.py:154][0m 647168 total steps have happened
[32m[20230204 15:44:29 @agent_ppo2.py:130][0m #------------------------ Iteration 316 --------------------------#
[32m[20230204 15:44:29 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:29 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:29 @agent_ppo2.py:194][0m |           0.0016 |          14.8105 |           4.2985 |
[32m[20230204 15:44:29 @agent_ppo2.py:194][0m |          -0.0039 |          10.9311 |           4.2918 |
[32m[20230204 15:44:30 @agent_ppo2.py:194][0m |          -0.0038 |           9.9306 |           4.2843 |
[32m[20230204 15:44:30 @agent_ppo2.py:194][0m |          -0.0079 |           9.1270 |           4.2830 |
[32m[20230204 15:44:30 @agent_ppo2.py:194][0m |          -0.0081 |           8.6527 |           4.2762 |
[32m[20230204 15:44:30 @agent_ppo2.py:194][0m |          -0.0091 |           8.2676 |           4.2764 |
[32m[20230204 15:44:30 @agent_ppo2.py:194][0m |          -0.0095 |           7.9617 |           4.2721 |
[32m[20230204 15:44:30 @agent_ppo2.py:194][0m |          -0.0085 |           7.7748 |           4.2717 |
[32m[20230204 15:44:30 @agent_ppo2.py:194][0m |          -0.0107 |           7.4393 |           4.2708 |
[32m[20230204 15:44:30 @agent_ppo2.py:194][0m |          -0.0108 |           7.1741 |           4.2697 |
[32m[20230204 15:44:30 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:44:30 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.67
[32m[20230204 15:44:30 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.64
[32m[20230204 15:44:30 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 262.82
[32m[20230204 15:44:30 @agent_ppo2.py:152][0m Total time:       8.33 min
[32m[20230204 15:44:30 @agent_ppo2.py:154][0m 649216 total steps have happened
[32m[20230204 15:44:30 @agent_ppo2.py:130][0m #------------------------ Iteration 317 --------------------------#
[32m[20230204 15:44:31 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:44:31 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:31 @agent_ppo2.py:194][0m |          -0.0031 |          15.8806 |           4.2824 |
[32m[20230204 15:44:31 @agent_ppo2.py:194][0m |          -0.0076 |          12.7860 |           4.2720 |
[32m[20230204 15:44:31 @agent_ppo2.py:194][0m |          -0.0075 |          11.8506 |           4.2732 |
[32m[20230204 15:44:31 @agent_ppo2.py:194][0m |          -0.0070 |          11.2217 |           4.2753 |
[32m[20230204 15:44:31 @agent_ppo2.py:194][0m |          -0.0019 |          11.5192 |           4.2769 |
[32m[20230204 15:44:31 @agent_ppo2.py:194][0m |          -0.0129 |          10.4186 |           4.2715 |
[32m[20230204 15:44:31 @agent_ppo2.py:194][0m |          -0.0133 |          10.0143 |           4.2735 |
[32m[20230204 15:44:31 @agent_ppo2.py:194][0m |          -0.0134 |           9.6735 |           4.2736 |
[32m[20230204 15:44:31 @agent_ppo2.py:194][0m |          -0.0112 |           9.4656 |           4.2768 |
[32m[20230204 15:44:31 @agent_ppo2.py:194][0m |          -0.0131 |           9.1102 |           4.2767 |
[32m[20230204 15:44:31 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:44:32 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.24
[32m[20230204 15:44:32 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.91
[32m[20230204 15:44:32 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 258.43
[32m[20230204 15:44:32 @agent_ppo2.py:152][0m Total time:       8.35 min
[32m[20230204 15:44:32 @agent_ppo2.py:154][0m 651264 total steps have happened
[32m[20230204 15:44:32 @agent_ppo2.py:130][0m #------------------------ Iteration 318 --------------------------#
[32m[20230204 15:44:32 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:32 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:32 @agent_ppo2.py:194][0m |           0.0014 |          16.0369 |           4.2309 |
[32m[20230204 15:44:32 @agent_ppo2.py:194][0m |           0.0005 |          14.7613 |           4.2229 |
[32m[20230204 15:44:32 @agent_ppo2.py:194][0m |          -0.0079 |          13.1505 |           4.2218 |
[32m[20230204 15:44:32 @agent_ppo2.py:194][0m |          -0.0109 |          12.3414 |           4.2176 |
[32m[20230204 15:44:32 @agent_ppo2.py:194][0m |          -0.0031 |          11.8806 |           4.2166 |
[32m[20230204 15:44:32 @agent_ppo2.py:194][0m |          -0.0085 |          11.2461 |           4.2172 |
[32m[20230204 15:44:32 @agent_ppo2.py:194][0m |          -0.0125 |          10.8691 |           4.2148 |
[32m[20230204 15:44:33 @agent_ppo2.py:194][0m |          -0.0133 |          10.5731 |           4.2155 |
[32m[20230204 15:44:33 @agent_ppo2.py:194][0m |          -0.0129 |          10.4333 |           4.2183 |
[32m[20230204 15:44:33 @agent_ppo2.py:194][0m |          -0.0088 |          10.3511 |           4.2119 |
[32m[20230204 15:44:33 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:44:33 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.74
[32m[20230204 15:44:33 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.46
[32m[20230204 15:44:33 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.68
[32m[20230204 15:44:33 @agent_ppo2.py:152][0m Total time:       8.37 min
[32m[20230204 15:44:33 @agent_ppo2.py:154][0m 653312 total steps have happened
[32m[20230204 15:44:33 @agent_ppo2.py:130][0m #------------------------ Iteration 319 --------------------------#
[32m[20230204 15:44:33 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:33 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:33 @agent_ppo2.py:194][0m |          -0.0004 |          23.7864 |           4.2492 |
[32m[20230204 15:44:33 @agent_ppo2.py:194][0m |          -0.0084 |          13.5798 |           4.2442 |
[32m[20230204 15:44:33 @agent_ppo2.py:194][0m |          -0.0091 |          12.7988 |           4.2440 |
[32m[20230204 15:44:34 @agent_ppo2.py:194][0m |          -0.0076 |          12.5486 |           4.2407 |
[32m[20230204 15:44:34 @agent_ppo2.py:194][0m |          -0.0127 |          11.8486 |           4.2427 |
[32m[20230204 15:44:34 @agent_ppo2.py:194][0m |          -0.0130 |          11.6462 |           4.2386 |
[32m[20230204 15:44:34 @agent_ppo2.py:194][0m |          -0.0152 |          11.5105 |           4.2389 |
[32m[20230204 15:44:34 @agent_ppo2.py:194][0m |          -0.0158 |          11.3827 |           4.2408 |
[32m[20230204 15:44:34 @agent_ppo2.py:194][0m |          -0.0153 |          11.3794 |           4.2394 |
[32m[20230204 15:44:34 @agent_ppo2.py:194][0m |          -0.0160 |          11.1247 |           4.2383 |
[32m[20230204 15:44:34 @agent_ppo2.py:139][0m Policy update time: 0.80 s
[32m[20230204 15:44:34 @agent_ppo2.py:147][0m Average TRAINING episode reward: 205.63
[32m[20230204 15:44:34 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.69
[32m[20230204 15:44:34 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.63
[32m[20230204 15:44:34 @agent_ppo2.py:152][0m Total time:       8.39 min
[32m[20230204 15:44:34 @agent_ppo2.py:154][0m 655360 total steps have happened
[32m[20230204 15:44:34 @agent_ppo2.py:130][0m #------------------------ Iteration 320 --------------------------#
[32m[20230204 15:44:34 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:44:34 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:35 @agent_ppo2.py:194][0m |           0.0042 |          14.4956 |           4.2311 |
[32m[20230204 15:44:35 @agent_ppo2.py:194][0m |          -0.0029 |          13.0673 |           4.2259 |
[32m[20230204 15:44:35 @agent_ppo2.py:194][0m |          -0.0078 |          12.2598 |           4.2230 |
[32m[20230204 15:44:35 @agent_ppo2.py:194][0m |          -0.0066 |          11.9154 |           4.2186 |
[32m[20230204 15:44:35 @agent_ppo2.py:194][0m |          -0.0084 |          11.7950 |           4.2207 |
[32m[20230204 15:44:35 @agent_ppo2.py:194][0m |          -0.0086 |          11.7261 |           4.2185 |
[32m[20230204 15:44:35 @agent_ppo2.py:194][0m |          -0.0111 |          11.4187 |           4.2139 |
[32m[20230204 15:44:35 @agent_ppo2.py:194][0m |          -0.0068 |          11.6616 |           4.2179 |
[32m[20230204 15:44:35 @agent_ppo2.py:194][0m |          -0.0096 |          11.2510 |           4.2134 |
[32m[20230204 15:44:35 @agent_ppo2.py:194][0m |          -0.0132 |          11.0856 |           4.2097 |
[32m[20230204 15:44:35 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:44:35 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.03
[32m[20230204 15:44:35 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.05
[32m[20230204 15:44:35 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 77.79
[32m[20230204 15:44:35 @agent_ppo2.py:152][0m Total time:       8.42 min
[32m[20230204 15:44:35 @agent_ppo2.py:154][0m 657408 total steps have happened
[32m[20230204 15:44:35 @agent_ppo2.py:130][0m #------------------------ Iteration 321 --------------------------#
[32m[20230204 15:44:36 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:36 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:36 @agent_ppo2.py:194][0m |          -0.0009 |          14.4458 |           4.2447 |
[32m[20230204 15:44:36 @agent_ppo2.py:194][0m |          -0.0061 |          13.6822 |           4.2357 |
[32m[20230204 15:44:36 @agent_ppo2.py:194][0m |          -0.0072 |          13.3410 |           4.2388 |
[32m[20230204 15:44:36 @agent_ppo2.py:194][0m |          -0.0099 |          13.1161 |           4.2335 |
[32m[20230204 15:44:36 @agent_ppo2.py:194][0m |          -0.0051 |          13.4281 |           4.2345 |
[32m[20230204 15:44:36 @agent_ppo2.py:194][0m |          -0.0089 |          12.8058 |           4.2386 |
[32m[20230204 15:44:36 @agent_ppo2.py:194][0m |          -0.0107 |          12.6913 |           4.2446 |
[32m[20230204 15:44:36 @agent_ppo2.py:194][0m |          -0.0095 |          12.6365 |           4.2433 |
[32m[20230204 15:44:36 @agent_ppo2.py:194][0m |          -0.0117 |          12.5143 |           4.2462 |
[32m[20230204 15:44:37 @agent_ppo2.py:194][0m |          -0.0115 |          12.4564 |           4.2479 |
[32m[20230204 15:44:37 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:44:37 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.29
[32m[20230204 15:44:37 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.18
[32m[20230204 15:44:37 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.93
[32m[20230204 15:44:37 @agent_ppo2.py:152][0m Total time:       8.44 min
[32m[20230204 15:44:37 @agent_ppo2.py:154][0m 659456 total steps have happened
[32m[20230204 15:44:37 @agent_ppo2.py:130][0m #------------------------ Iteration 322 --------------------------#
[32m[20230204 15:44:37 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:37 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:37 @agent_ppo2.py:194][0m |          -0.0014 |          13.8703 |           4.2111 |
[32m[20230204 15:44:37 @agent_ppo2.py:194][0m |          -0.0074 |          12.3528 |           4.2027 |
[32m[20230204 15:44:37 @agent_ppo2.py:194][0m |           0.0003 |          11.7038 |           4.1987 |
[32m[20230204 15:44:37 @agent_ppo2.py:194][0m |          -0.0093 |          10.9286 |           4.1975 |
[32m[20230204 15:44:37 @agent_ppo2.py:194][0m |          -0.0027 |          10.7010 |           4.1940 |
[32m[20230204 15:44:38 @agent_ppo2.py:194][0m |          -0.0100 |           9.8346 |           4.1935 |
[32m[20230204 15:44:38 @agent_ppo2.py:194][0m |          -0.0115 |           9.3609 |           4.1907 |
[32m[20230204 15:44:38 @agent_ppo2.py:194][0m |          -0.0113 |           9.0096 |           4.1882 |
[32m[20230204 15:44:38 @agent_ppo2.py:194][0m |          -0.0120 |           8.7186 |           4.1881 |
[32m[20230204 15:44:38 @agent_ppo2.py:194][0m |          -0.0126 |           8.4595 |           4.1888 |
[32m[20230204 15:44:38 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:44:38 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.77
[32m[20230204 15:44:38 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.76
[32m[20230204 15:44:38 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.27
[32m[20230204 15:44:38 @agent_ppo2.py:152][0m Total time:       8.46 min
[32m[20230204 15:44:38 @agent_ppo2.py:154][0m 661504 total steps have happened
[32m[20230204 15:44:38 @agent_ppo2.py:130][0m #------------------------ Iteration 323 --------------------------#
[32m[20230204 15:44:38 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:44:38 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:38 @agent_ppo2.py:194][0m |           0.0000 |          13.7337 |           4.2636 |
[32m[20230204 15:44:39 @agent_ppo2.py:194][0m |          -0.0036 |          12.9005 |           4.2626 |
[32m[20230204 15:44:39 @agent_ppo2.py:194][0m |          -0.0047 |          12.3789 |           4.2594 |
[32m[20230204 15:44:39 @agent_ppo2.py:194][0m |          -0.0067 |          11.9114 |           4.2592 |
[32m[20230204 15:44:39 @agent_ppo2.py:194][0m |          -0.0062 |          11.6927 |           4.2568 |
[32m[20230204 15:44:39 @agent_ppo2.py:194][0m |          -0.0090 |          11.3925 |           4.2601 |
[32m[20230204 15:44:39 @agent_ppo2.py:194][0m |          -0.0100 |          11.1831 |           4.2539 |
[32m[20230204 15:44:39 @agent_ppo2.py:194][0m |          -0.0095 |          11.0009 |           4.2604 |
[32m[20230204 15:44:39 @agent_ppo2.py:194][0m |          -0.0127 |          10.8190 |           4.2555 |
[32m[20230204 15:44:39 @agent_ppo2.py:194][0m |          -0.0106 |          10.7439 |           4.2587 |
[32m[20230204 15:44:39 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:44:39 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.81
[32m[20230204 15:44:39 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.60
[32m[20230204 15:44:39 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 263.22
[32m[20230204 15:44:39 @agent_ppo2.py:152][0m Total time:       8.48 min
[32m[20230204 15:44:39 @agent_ppo2.py:154][0m 663552 total steps have happened
[32m[20230204 15:44:39 @agent_ppo2.py:130][0m #------------------------ Iteration 324 --------------------------#
[32m[20230204 15:44:40 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:40 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:40 @agent_ppo2.py:194][0m |          -0.0001 |          15.5740 |           4.3006 |
[32m[20230204 15:44:40 @agent_ppo2.py:194][0m |          -0.0046 |          14.1720 |           4.2914 |
[32m[20230204 15:44:40 @agent_ppo2.py:194][0m |          -0.0066 |          13.5903 |           4.2877 |
[32m[20230204 15:44:40 @agent_ppo2.py:194][0m |          -0.0088 |          12.9844 |           4.2868 |
[32m[20230204 15:44:40 @agent_ppo2.py:194][0m |          -0.0100 |          12.6048 |           4.2882 |
[32m[20230204 15:44:40 @agent_ppo2.py:194][0m |          -0.0114 |          12.4173 |           4.2845 |
[32m[20230204 15:44:40 @agent_ppo2.py:194][0m |          -0.0131 |          12.0937 |           4.2870 |
[32m[20230204 15:44:40 @agent_ppo2.py:194][0m |          -0.0119 |          12.0474 |           4.2821 |
[32m[20230204 15:44:40 @agent_ppo2.py:194][0m |          -0.0141 |          11.6802 |           4.2848 |
[32m[20230204 15:44:40 @agent_ppo2.py:194][0m |          -0.0132 |          11.7426 |           4.2833 |
[32m[20230204 15:44:40 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:44:41 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.42
[32m[20230204 15:44:41 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.20
[32m[20230204 15:44:41 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.74
[32m[20230204 15:44:41 @agent_ppo2.py:152][0m Total time:       8.50 min
[32m[20230204 15:44:41 @agent_ppo2.py:154][0m 665600 total steps have happened
[32m[20230204 15:44:41 @agent_ppo2.py:130][0m #------------------------ Iteration 325 --------------------------#
[32m[20230204 15:44:41 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:41 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:41 @agent_ppo2.py:194][0m |           0.0021 |          14.0850 |           4.2175 |
[32m[20230204 15:44:41 @agent_ppo2.py:194][0m |          -0.0050 |          12.1709 |           4.2135 |
[32m[20230204 15:44:41 @agent_ppo2.py:194][0m |          -0.0083 |          11.3185 |           4.2135 |
[32m[20230204 15:44:41 @agent_ppo2.py:194][0m |          -0.0100 |          10.8124 |           4.2090 |
[32m[20230204 15:44:41 @agent_ppo2.py:194][0m |          -0.0078 |          10.4743 |           4.2131 |
[32m[20230204 15:44:41 @agent_ppo2.py:194][0m |          -0.0093 |          10.2278 |           4.2125 |
[32m[20230204 15:44:42 @agent_ppo2.py:194][0m |          -0.0118 |           9.8737 |           4.2156 |
[32m[20230204 15:44:42 @agent_ppo2.py:194][0m |          -0.0125 |           9.5003 |           4.2133 |
[32m[20230204 15:44:42 @agent_ppo2.py:194][0m |          -0.0114 |           9.2433 |           4.2122 |
[32m[20230204 15:44:42 @agent_ppo2.py:194][0m |          -0.0113 |           9.1245 |           4.2145 |
[32m[20230204 15:44:42 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:44:42 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.04
[32m[20230204 15:44:42 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.93
[32m[20230204 15:44:42 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 263.56
[32m[20230204 15:44:42 @agent_ppo2.py:152][0m Total time:       8.53 min
[32m[20230204 15:44:42 @agent_ppo2.py:154][0m 667648 total steps have happened
[32m[20230204 15:44:42 @agent_ppo2.py:130][0m #------------------------ Iteration 326 --------------------------#
[32m[20230204 15:44:42 @agent_ppo2.py:136][0m Sampling time: 0.32 s by 4 slaves
[32m[20230204 15:44:42 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:43 @agent_ppo2.py:194][0m |           0.0008 |          18.7562 |           4.2681 |
[32m[20230204 15:44:43 @agent_ppo2.py:194][0m |          -0.0085 |          14.3040 |           4.2551 |
[32m[20230204 15:44:43 @agent_ppo2.py:194][0m |          -0.0109 |          13.5897 |           4.2559 |
[32m[20230204 15:44:43 @agent_ppo2.py:194][0m |          -0.0109 |          13.2143 |           4.2545 |
[32m[20230204 15:44:43 @agent_ppo2.py:194][0m |          -0.0131 |          12.8398 |           4.2537 |
[32m[20230204 15:44:43 @agent_ppo2.py:194][0m |          -0.0145 |          12.5575 |           4.2493 |
[32m[20230204 15:44:43 @agent_ppo2.py:194][0m |          -0.0148 |          12.3527 |           4.2482 |
[32m[20230204 15:44:43 @agent_ppo2.py:194][0m |          -0.0188 |          12.1819 |           4.2483 |
[32m[20230204 15:44:43 @agent_ppo2.py:194][0m |          -0.0118 |          11.9948 |           4.2447 |
[32m[20230204 15:44:43 @agent_ppo2.py:194][0m |          -0.0134 |          11.7883 |           4.2422 |
[32m[20230204 15:44:43 @agent_ppo2.py:139][0m Policy update time: 0.99 s
[32m[20230204 15:44:44 @agent_ppo2.py:147][0m Average TRAINING episode reward: 191.04
[32m[20230204 15:44:44 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.67
[32m[20230204 15:44:44 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 260.72
[32m[20230204 15:44:44 @agent_ppo2.py:152][0m Total time:       8.55 min
[32m[20230204 15:44:44 @agent_ppo2.py:154][0m 669696 total steps have happened
[32m[20230204 15:44:44 @agent_ppo2.py:130][0m #------------------------ Iteration 327 --------------------------#
[32m[20230204 15:44:44 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:44 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:44 @agent_ppo2.py:194][0m |           0.0029 |          13.7287 |           4.3490 |
[32m[20230204 15:44:44 @agent_ppo2.py:194][0m |          -0.0077 |          11.9569 |           4.3422 |
[32m[20230204 15:44:44 @agent_ppo2.py:194][0m |          -0.0104 |          11.5915 |           4.3418 |
[32m[20230204 15:44:44 @agent_ppo2.py:194][0m |          -0.0110 |          11.3943 |           4.3427 |
[32m[20230204 15:44:44 @agent_ppo2.py:194][0m |          -0.0123 |          11.2585 |           4.3395 |
[32m[20230204 15:44:44 @agent_ppo2.py:194][0m |          -0.0129 |          11.1334 |           4.3424 |
[32m[20230204 15:44:44 @agent_ppo2.py:194][0m |          -0.0122 |          11.1077 |           4.3403 |
[32m[20230204 15:44:44 @agent_ppo2.py:194][0m |          -0.0136 |          10.8856 |           4.3414 |
[32m[20230204 15:44:45 @agent_ppo2.py:194][0m |          -0.0143 |          10.8318 |           4.3430 |
[32m[20230204 15:44:45 @agent_ppo2.py:194][0m |          -0.0130 |          10.8679 |           4.3455 |
[32m[20230204 15:44:45 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:44:45 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.59
[32m[20230204 15:44:45 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.02
[32m[20230204 15:44:45 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 263.77
[32m[20230204 15:44:45 @agent_ppo2.py:152][0m Total time:       8.57 min
[32m[20230204 15:44:45 @agent_ppo2.py:154][0m 671744 total steps have happened
[32m[20230204 15:44:45 @agent_ppo2.py:130][0m #------------------------ Iteration 328 --------------------------#
[32m[20230204 15:44:45 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:45 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:45 @agent_ppo2.py:194][0m |           0.0000 |          14.9732 |           4.1787 |
[32m[20230204 15:44:45 @agent_ppo2.py:194][0m |           0.0024 |          13.8813 |           4.1638 |
[32m[20230204 15:44:45 @agent_ppo2.py:194][0m |          -0.0078 |          12.1468 |           4.1648 |
[32m[20230204 15:44:45 @agent_ppo2.py:194][0m |          -0.0104 |          11.2434 |           4.1589 |
[32m[20230204 15:44:46 @agent_ppo2.py:194][0m |          -0.0109 |          10.6659 |           4.1603 |
[32m[20230204 15:44:46 @agent_ppo2.py:194][0m |          -0.0112 |          10.2728 |           4.1621 |
[32m[20230204 15:44:46 @agent_ppo2.py:194][0m |          -0.0110 |           9.9058 |           4.1607 |
[32m[20230204 15:44:46 @agent_ppo2.py:194][0m |          -0.0129 |           9.6014 |           4.1590 |
[32m[20230204 15:44:46 @agent_ppo2.py:194][0m |          -0.0149 |           9.4089 |           4.1595 |
[32m[20230204 15:44:46 @agent_ppo2.py:194][0m |          -0.0114 |           9.3510 |           4.1615 |
[32m[20230204 15:44:46 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:44:46 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.44
[32m[20230204 15:44:46 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.35
[32m[20230204 15:44:46 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.68
[32m[20230204 15:44:46 @agent_ppo2.py:152][0m Total time:       8.60 min
[32m[20230204 15:44:46 @agent_ppo2.py:154][0m 673792 total steps have happened
[32m[20230204 15:44:46 @agent_ppo2.py:130][0m #------------------------ Iteration 329 --------------------------#
[32m[20230204 15:44:46 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:46 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:47 @agent_ppo2.py:194][0m |           0.0028 |          14.9006 |           4.2920 |
[32m[20230204 15:44:47 @agent_ppo2.py:194][0m |          -0.0020 |          11.4628 |           4.2952 |
[32m[20230204 15:44:47 @agent_ppo2.py:194][0m |          -0.0025 |          10.3374 |           4.2905 |
[32m[20230204 15:44:47 @agent_ppo2.py:194][0m |          -0.0061 |           9.4770 |           4.2897 |
[32m[20230204 15:44:47 @agent_ppo2.py:194][0m |          -0.0086 |           8.9475 |           4.2896 |
[32m[20230204 15:44:47 @agent_ppo2.py:194][0m |          -0.0064 |           8.6096 |           4.2846 |
[32m[20230204 15:44:47 @agent_ppo2.py:194][0m |          -0.0068 |           8.2140 |           4.2868 |
[32m[20230204 15:44:47 @agent_ppo2.py:194][0m |          -0.0075 |           7.9007 |           4.2873 |
[32m[20230204 15:44:47 @agent_ppo2.py:194][0m |          -0.0091 |           7.5193 |           4.2832 |
[32m[20230204 15:44:47 @agent_ppo2.py:194][0m |          -0.0083 |           7.2680 |           4.2847 |
[32m[20230204 15:44:47 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:44:47 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.22
[32m[20230204 15:44:47 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.86
[32m[20230204 15:44:47 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.88
[32m[20230204 15:44:47 @agent_ppo2.py:152][0m Total time:       8.62 min
[32m[20230204 15:44:47 @agent_ppo2.py:154][0m 675840 total steps have happened
[32m[20230204 15:44:47 @agent_ppo2.py:130][0m #------------------------ Iteration 330 --------------------------#
[32m[20230204 15:44:48 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:48 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:48 @agent_ppo2.py:194][0m |           0.0009 |          15.3072 |           4.2574 |
[32m[20230204 15:44:48 @agent_ppo2.py:194][0m |          -0.0037 |          14.3103 |           4.2486 |
[32m[20230204 15:44:48 @agent_ppo2.py:194][0m |          -0.0105 |          13.2922 |           4.2452 |
[32m[20230204 15:44:48 @agent_ppo2.py:194][0m |          -0.0085 |          13.0926 |           4.2397 |
[32m[20230204 15:44:48 @agent_ppo2.py:194][0m |          -0.0112 |          12.6422 |           4.2405 |
[32m[20230204 15:44:48 @agent_ppo2.py:194][0m |          -0.0121 |          12.3542 |           4.2400 |
[32m[20230204 15:44:48 @agent_ppo2.py:194][0m |          -0.0123 |          12.0675 |           4.2374 |
[32m[20230204 15:44:48 @agent_ppo2.py:194][0m |          -0.0121 |          11.7368 |           4.2409 |
[32m[20230204 15:44:48 @agent_ppo2.py:194][0m |          -0.0143 |          11.3675 |           4.2389 |
[32m[20230204 15:44:49 @agent_ppo2.py:194][0m |          -0.0106 |          11.1841 |           4.2416 |
[32m[20230204 15:44:49 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:44:49 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.54
[32m[20230204 15:44:49 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.29
[32m[20230204 15:44:49 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.55
[32m[20230204 15:44:49 @agent_ppo2.py:152][0m Total time:       8.64 min
[32m[20230204 15:44:49 @agent_ppo2.py:154][0m 677888 total steps have happened
[32m[20230204 15:44:49 @agent_ppo2.py:130][0m #------------------------ Iteration 331 --------------------------#
[32m[20230204 15:44:49 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:44:49 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:49 @agent_ppo2.py:194][0m |           0.0003 |          15.4515 |           4.3677 |
[32m[20230204 15:44:49 @agent_ppo2.py:194][0m |          -0.0020 |          13.7879 |           4.3691 |
[32m[20230204 15:44:49 @agent_ppo2.py:194][0m |          -0.0055 |          13.1087 |           4.3641 |
[32m[20230204 15:44:49 @agent_ppo2.py:194][0m |          -0.0095 |          12.6840 |           4.3607 |
[32m[20230204 15:44:50 @agent_ppo2.py:194][0m |          -0.0091 |          12.3652 |           4.3591 |
[32m[20230204 15:44:50 @agent_ppo2.py:194][0m |          -0.0083 |          12.0956 |           4.3604 |
[32m[20230204 15:44:50 @agent_ppo2.py:194][0m |          -0.0080 |          11.8710 |           4.3536 |
[32m[20230204 15:44:50 @agent_ppo2.py:194][0m |          -0.0095 |          11.6326 |           4.3575 |
[32m[20230204 15:44:50 @agent_ppo2.py:194][0m |          -0.0077 |          11.6329 |           4.3540 |
[32m[20230204 15:44:50 @agent_ppo2.py:194][0m |          -0.0120 |          11.2003 |           4.3516 |
[32m[20230204 15:44:50 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:44:50 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.63
[32m[20230204 15:44:50 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.42
[32m[20230204 15:44:50 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.48
[32m[20230204 15:44:50 @agent_ppo2.py:152][0m Total time:       8.66 min
[32m[20230204 15:44:50 @agent_ppo2.py:154][0m 679936 total steps have happened
[32m[20230204 15:44:50 @agent_ppo2.py:130][0m #------------------------ Iteration 332 --------------------------#
[32m[20230204 15:44:50 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:50 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:50 @agent_ppo2.py:194][0m |           0.0029 |          14.3790 |           4.2493 |
[32m[20230204 15:44:51 @agent_ppo2.py:194][0m |          -0.0068 |          12.8170 |           4.2483 |
[32m[20230204 15:44:51 @agent_ppo2.py:194][0m |          -0.0088 |          11.6248 |           4.2432 |
[32m[20230204 15:44:51 @agent_ppo2.py:194][0m |          -0.0047 |          10.9338 |           4.2447 |
[32m[20230204 15:44:51 @agent_ppo2.py:194][0m |          -0.0090 |           9.8364 |           4.2433 |
[32m[20230204 15:44:51 @agent_ppo2.py:194][0m |          -0.0121 |           8.9891 |           4.2474 |
[32m[20230204 15:44:51 @agent_ppo2.py:194][0m |          -0.0058 |           8.4248 |           4.2490 |
[32m[20230204 15:44:51 @agent_ppo2.py:194][0m |          -0.0096 |           7.7105 |           4.2454 |
[32m[20230204 15:44:51 @agent_ppo2.py:194][0m |          -0.0130 |           7.2157 |           4.2483 |
[32m[20230204 15:44:51 @agent_ppo2.py:194][0m |          -0.0159 |           6.8028 |           4.2488 |
[32m[20230204 15:44:51 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:44:51 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.22
[32m[20230204 15:44:51 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.91
[32m[20230204 15:44:51 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 262.61
[32m[20230204 15:44:51 @agent_ppo2.py:152][0m Total time:       8.68 min
[32m[20230204 15:44:51 @agent_ppo2.py:154][0m 681984 total steps have happened
[32m[20230204 15:44:51 @agent_ppo2.py:130][0m #------------------------ Iteration 333 --------------------------#
[32m[20230204 15:44:52 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:52 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:52 @agent_ppo2.py:194][0m |           0.0014 |          15.0635 |           4.3087 |
[32m[20230204 15:44:52 @agent_ppo2.py:194][0m |          -0.0037 |          13.9400 |           4.3062 |
[32m[20230204 15:44:52 @agent_ppo2.py:194][0m |          -0.0040 |          13.3124 |           4.2989 |
[32m[20230204 15:44:52 @agent_ppo2.py:194][0m |          -0.0060 |          12.8354 |           4.2994 |
[32m[20230204 15:44:52 @agent_ppo2.py:194][0m |          -0.0059 |          12.3225 |           4.2972 |
[32m[20230204 15:44:52 @agent_ppo2.py:194][0m |          -0.0078 |          11.8882 |           4.2954 |
[32m[20230204 15:44:52 @agent_ppo2.py:194][0m |          -0.0087 |          11.5316 |           4.2984 |
[32m[20230204 15:44:52 @agent_ppo2.py:194][0m |          -0.0080 |          11.1980 |           4.2943 |
[32m[20230204 15:44:52 @agent_ppo2.py:194][0m |          -0.0079 |          11.1059 |           4.2975 |
[32m[20230204 15:44:53 @agent_ppo2.py:194][0m |          -0.0087 |          10.7299 |           4.2963 |
[32m[20230204 15:44:53 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:44:53 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.16
[32m[20230204 15:44:53 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.99
[32m[20230204 15:44:53 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.21
[32m[20230204 15:44:53 @agent_ppo2.py:152][0m Total time:       8.70 min
[32m[20230204 15:44:53 @agent_ppo2.py:154][0m 684032 total steps have happened
[32m[20230204 15:44:53 @agent_ppo2.py:130][0m #------------------------ Iteration 334 --------------------------#
[32m[20230204 15:44:53 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:44:53 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:53 @agent_ppo2.py:194][0m |          -0.0028 |          14.7762 |           4.3813 |
[32m[20230204 15:44:53 @agent_ppo2.py:194][0m |          -0.0087 |          13.6552 |           4.3730 |
[32m[20230204 15:44:53 @agent_ppo2.py:194][0m |          -0.0066 |          13.4279 |           4.3749 |
[32m[20230204 15:44:53 @agent_ppo2.py:194][0m |          -0.0062 |          13.1759 |           4.3692 |
[32m[20230204 15:44:53 @agent_ppo2.py:194][0m |          -0.0100 |          12.4740 |           4.3675 |
[32m[20230204 15:44:54 @agent_ppo2.py:194][0m |          -0.0053 |          12.4545 |           4.3660 |
[32m[20230204 15:44:54 @agent_ppo2.py:194][0m |          -0.0131 |          11.6768 |           4.3690 |
[32m[20230204 15:44:54 @agent_ppo2.py:194][0m |          -0.0135 |          11.2638 |           4.3657 |
[32m[20230204 15:44:54 @agent_ppo2.py:194][0m |          -0.0111 |          11.1441 |           4.3643 |
[32m[20230204 15:44:54 @agent_ppo2.py:194][0m |          -0.0162 |          10.4627 |           4.3674 |
[32m[20230204 15:44:54 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:44:54 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.13
[32m[20230204 15:44:54 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.65
[32m[20230204 15:44:54 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.44
[32m[20230204 15:44:54 @agent_ppo2.py:152][0m Total time:       8.73 min
[32m[20230204 15:44:54 @agent_ppo2.py:154][0m 686080 total steps have happened
[32m[20230204 15:44:54 @agent_ppo2.py:130][0m #------------------------ Iteration 335 --------------------------#
[32m[20230204 15:44:54 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:54 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:54 @agent_ppo2.py:194][0m |           0.0030 |          15.5073 |           4.3087 |
[32m[20230204 15:44:54 @agent_ppo2.py:194][0m |          -0.0033 |          13.6994 |           4.3059 |
[32m[20230204 15:44:55 @agent_ppo2.py:194][0m |          -0.0051 |          13.0089 |           4.3004 |
[32m[20230204 15:44:55 @agent_ppo2.py:194][0m |          -0.0071 |          12.5420 |           4.3012 |
[32m[20230204 15:44:55 @agent_ppo2.py:194][0m |          -0.0076 |          12.0904 |           4.2993 |
[32m[20230204 15:44:55 @agent_ppo2.py:194][0m |          -0.0083 |          11.7204 |           4.2974 |
[32m[20230204 15:44:55 @agent_ppo2.py:194][0m |          -0.0086 |          11.5780 |           4.2956 |
[32m[20230204 15:44:55 @agent_ppo2.py:194][0m |          -0.0106 |          11.3719 |           4.2958 |
[32m[20230204 15:44:55 @agent_ppo2.py:194][0m |          -0.0110 |          11.1895 |           4.2966 |
[32m[20230204 15:44:55 @agent_ppo2.py:194][0m |          -0.0094 |          11.0419 |           4.2934 |
[32m[20230204 15:44:55 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:44:55 @agent_ppo2.py:147][0m Average TRAINING episode reward: 262.14
[32m[20230204 15:44:55 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.11
[32m[20230204 15:44:55 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 260.63
[32m[20230204 15:44:55 @agent_ppo2.py:152][0m Total time:       8.75 min
[32m[20230204 15:44:55 @agent_ppo2.py:154][0m 688128 total steps have happened
[32m[20230204 15:44:55 @agent_ppo2.py:130][0m #------------------------ Iteration 336 --------------------------#
[32m[20230204 15:44:56 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:56 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:56 @agent_ppo2.py:194][0m |          -0.0019 |          17.5734 |           4.3170 |
[32m[20230204 15:44:56 @agent_ppo2.py:194][0m |          -0.0067 |          15.7598 |           4.3086 |
[32m[20230204 15:44:56 @agent_ppo2.py:194][0m |          -0.0053 |          15.5407 |           4.3082 |
[32m[20230204 15:44:56 @agent_ppo2.py:194][0m |          -0.0077 |          15.4922 |           4.3036 |
[32m[20230204 15:44:56 @agent_ppo2.py:194][0m |          -0.0098 |          15.2027 |           4.3034 |
[32m[20230204 15:44:56 @agent_ppo2.py:194][0m |          -0.0100 |          14.7408 |           4.3027 |
[32m[20230204 15:44:56 @agent_ppo2.py:194][0m |          -0.0132 |          14.6116 |           4.3026 |
[32m[20230204 15:44:56 @agent_ppo2.py:194][0m |          -0.0181 |          14.4778 |           4.3032 |
[32m[20230204 15:44:56 @agent_ppo2.py:194][0m |          -0.0120 |          14.6190 |           4.3005 |
[32m[20230204 15:44:56 @agent_ppo2.py:194][0m |          -0.0135 |          14.2152 |           4.3028 |
[32m[20230204 15:44:56 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:44:57 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.64
[32m[20230204 15:44:57 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.41
[32m[20230204 15:44:57 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.64
[32m[20230204 15:44:57 @agent_ppo2.py:152][0m Total time:       8.77 min
[32m[20230204 15:44:57 @agent_ppo2.py:154][0m 690176 total steps have happened
[32m[20230204 15:44:57 @agent_ppo2.py:130][0m #------------------------ Iteration 337 --------------------------#
[32m[20230204 15:44:57 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:44:57 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:57 @agent_ppo2.py:194][0m |           0.0004 |          15.1927 |           4.3890 |
[32m[20230204 15:44:57 @agent_ppo2.py:194][0m |          -0.0066 |          13.9501 |           4.3795 |
[32m[20230204 15:44:57 @agent_ppo2.py:194][0m |          -0.0081 |          13.4224 |           4.3726 |
[32m[20230204 15:44:57 @agent_ppo2.py:194][0m |          -0.0087 |          13.0140 |           4.3755 |
[32m[20230204 15:44:57 @agent_ppo2.py:194][0m |          -0.0098 |          12.8033 |           4.3721 |
[32m[20230204 15:44:57 @agent_ppo2.py:194][0m |          -0.0104 |          12.3928 |           4.3716 |
[32m[20230204 15:44:57 @agent_ppo2.py:194][0m |          -0.0117 |          12.1912 |           4.3702 |
[32m[20230204 15:44:58 @agent_ppo2.py:194][0m |          -0.0121 |          11.8114 |           4.3686 |
[32m[20230204 15:44:58 @agent_ppo2.py:194][0m |          -0.0125 |          11.5890 |           4.3711 |
[32m[20230204 15:44:58 @agent_ppo2.py:194][0m |          -0.0122 |          11.3988 |           4.3678 |
[32m[20230204 15:44:58 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:44:58 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.03
[32m[20230204 15:44:58 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.21
[32m[20230204 15:44:58 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.99
[32m[20230204 15:44:58 @agent_ppo2.py:152][0m Total time:       8.79 min
[32m[20230204 15:44:58 @agent_ppo2.py:154][0m 692224 total steps have happened
[32m[20230204 15:44:58 @agent_ppo2.py:130][0m #------------------------ Iteration 338 --------------------------#
[32m[20230204 15:44:58 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:44:58 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:44:58 @agent_ppo2.py:194][0m |          -0.0020 |          14.7541 |           4.3749 |
[32m[20230204 15:44:58 @agent_ppo2.py:194][0m |          -0.0023 |          13.9713 |           4.3711 |
[32m[20230204 15:44:58 @agent_ppo2.py:194][0m |          -0.0067 |          13.4121 |           4.3663 |
[32m[20230204 15:44:59 @agent_ppo2.py:194][0m |          -0.0062 |          13.0953 |           4.3649 |
[32m[20230204 15:44:59 @agent_ppo2.py:194][0m |          -0.0099 |          12.8427 |           4.3648 |
[32m[20230204 15:44:59 @agent_ppo2.py:194][0m |          -0.0095 |          12.6033 |           4.3638 |
[32m[20230204 15:44:59 @agent_ppo2.py:194][0m |          -0.0037 |          12.7918 |           4.3651 |
[32m[20230204 15:44:59 @agent_ppo2.py:194][0m |          -0.0098 |          12.2278 |           4.3622 |
[32m[20230204 15:44:59 @agent_ppo2.py:194][0m |          -0.0062 |          12.6481 |           4.3596 |
[32m[20230204 15:44:59 @agent_ppo2.py:194][0m |          -0.0111 |          12.0843 |           4.3574 |
[32m[20230204 15:44:59 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:44:59 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.49
[32m[20230204 15:44:59 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.68
[32m[20230204 15:44:59 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.32
[32m[20230204 15:44:59 @agent_ppo2.py:152][0m Total time:       8.81 min
[32m[20230204 15:44:59 @agent_ppo2.py:154][0m 694272 total steps have happened
[32m[20230204 15:44:59 @agent_ppo2.py:130][0m #------------------------ Iteration 339 --------------------------#
[32m[20230204 15:44:59 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:00 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:00 @agent_ppo2.py:194][0m |           0.0003 |          17.4700 |           4.3682 |
[32m[20230204 15:45:00 @agent_ppo2.py:194][0m |          -0.0064 |          12.7335 |           4.3617 |
[32m[20230204 15:45:00 @agent_ppo2.py:194][0m |          -0.0078 |          12.1385 |           4.3633 |
[32m[20230204 15:45:00 @agent_ppo2.py:194][0m |          -0.0105 |          11.6404 |           4.3616 |
[32m[20230204 15:45:00 @agent_ppo2.py:194][0m |          -0.0096 |          11.4612 |           4.3608 |
[32m[20230204 15:45:00 @agent_ppo2.py:194][0m |          -0.0125 |          11.2039 |           4.3590 |
[32m[20230204 15:45:00 @agent_ppo2.py:194][0m |          -0.0134 |          11.0314 |           4.3576 |
[32m[20230204 15:45:00 @agent_ppo2.py:194][0m |          -0.0132 |          10.8429 |           4.3588 |
[32m[20230204 15:45:00 @agent_ppo2.py:194][0m |          -0.0129 |          10.7945 |           4.3578 |
[32m[20230204 15:45:00 @agent_ppo2.py:194][0m |          -0.0154 |          10.5879 |           4.3575 |
[32m[20230204 15:45:00 @agent_ppo2.py:139][0m Policy update time: 0.81 s
[32m[20230204 15:45:00 @agent_ppo2.py:147][0m Average TRAINING episode reward: 213.04
[32m[20230204 15:45:00 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.68
[32m[20230204 15:45:00 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.90
[32m[20230204 15:45:00 @agent_ppo2.py:152][0m Total time:       8.83 min
[32m[20230204 15:45:00 @agent_ppo2.py:154][0m 696320 total steps have happened
[32m[20230204 15:45:00 @agent_ppo2.py:130][0m #------------------------ Iteration 340 --------------------------#
[32m[20230204 15:45:01 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:45:01 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:01 @agent_ppo2.py:194][0m |           0.0027 |          14.1644 |           4.3764 |
[32m[20230204 15:45:01 @agent_ppo2.py:194][0m |          -0.0038 |          13.4020 |           4.3727 |
[32m[20230204 15:45:01 @agent_ppo2.py:194][0m |          -0.0051 |          13.1789 |           4.3649 |
[32m[20230204 15:45:01 @agent_ppo2.py:194][0m |          -0.0083 |          12.5789 |           4.3627 |
[32m[20230204 15:45:01 @agent_ppo2.py:194][0m |          -0.0084 |          12.5042 |           4.3604 |
[32m[20230204 15:45:01 @agent_ppo2.py:194][0m |          -0.0072 |          12.4723 |           4.3589 |
[32m[20230204 15:45:01 @agent_ppo2.py:194][0m |          -0.0091 |          11.7899 |           4.3555 |
[32m[20230204 15:45:01 @agent_ppo2.py:194][0m |          -0.0132 |          11.3819 |           4.3525 |
[32m[20230204 15:45:01 @agent_ppo2.py:194][0m |          -0.0138 |          11.0759 |           4.3500 |
[32m[20230204 15:45:02 @agent_ppo2.py:194][0m |          -0.0131 |          10.8380 |           4.3478 |
[32m[20230204 15:45:02 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:45:02 @agent_ppo2.py:147][0m Average TRAINING episode reward: 262.40
[32m[20230204 15:45:02 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.02
[32m[20230204 15:45:02 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.55
[32m[20230204 15:45:02 @agent_ppo2.py:152][0m Total time:       8.86 min
[32m[20230204 15:45:02 @agent_ppo2.py:154][0m 698368 total steps have happened
[32m[20230204 15:45:02 @agent_ppo2.py:130][0m #------------------------ Iteration 341 --------------------------#
[32m[20230204 15:45:02 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:45:02 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:02 @agent_ppo2.py:194][0m |          -0.0000 |          24.5439 |           4.3342 |
[32m[20230204 15:45:02 @agent_ppo2.py:194][0m |          -0.0018 |          19.1904 |           4.3292 |
[32m[20230204 15:45:02 @agent_ppo2.py:194][0m |          -0.0040 |          18.1751 |           4.3261 |
[32m[20230204 15:45:03 @agent_ppo2.py:194][0m |          -0.0064 |          17.3307 |           4.3252 |
[32m[20230204 15:45:03 @agent_ppo2.py:194][0m |          -0.0073 |          16.7797 |           4.3259 |
[32m[20230204 15:45:03 @agent_ppo2.py:194][0m |          -0.0078 |          16.5373 |           4.3227 |
[32m[20230204 15:45:03 @agent_ppo2.py:194][0m |          -0.0089 |          16.0817 |           4.3235 |
[32m[20230204 15:45:03 @agent_ppo2.py:194][0m |          -0.0089 |          15.7368 |           4.3219 |
[32m[20230204 15:45:03 @agent_ppo2.py:194][0m |          -0.0108 |          15.4266 |           4.3208 |
[32m[20230204 15:45:03 @agent_ppo2.py:194][0m |          -0.0132 |          15.0891 |           4.3184 |
[32m[20230204 15:45:03 @agent_ppo2.py:139][0m Policy update time: 0.98 s
[32m[20230204 15:45:03 @agent_ppo2.py:147][0m Average TRAINING episode reward: 208.27
[32m[20230204 15:45:03 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.93
[32m[20230204 15:45:03 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.93
[32m[20230204 15:45:03 @agent_ppo2.py:152][0m Total time:       8.88 min
[32m[20230204 15:45:03 @agent_ppo2.py:154][0m 700416 total steps have happened
[32m[20230204 15:45:03 @agent_ppo2.py:130][0m #------------------------ Iteration 342 --------------------------#
[32m[20230204 15:45:03 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:04 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:04 @agent_ppo2.py:194][0m |           0.0009 |          14.4170 |           4.2981 |
[32m[20230204 15:45:04 @agent_ppo2.py:194][0m |          -0.0044 |          13.5309 |           4.2933 |
[32m[20230204 15:45:04 @agent_ppo2.py:194][0m |          -0.0065 |          13.1337 |           4.2934 |
[32m[20230204 15:45:04 @agent_ppo2.py:194][0m |          -0.0074 |          12.8868 |           4.2895 |
[32m[20230204 15:45:04 @agent_ppo2.py:194][0m |          -0.0081 |          12.7105 |           4.2904 |
[32m[20230204 15:45:04 @agent_ppo2.py:194][0m |          -0.0084 |          12.5756 |           4.2924 |
[32m[20230204 15:45:04 @agent_ppo2.py:194][0m |          -0.0095 |          12.4355 |           4.2914 |
[32m[20230204 15:45:04 @agent_ppo2.py:194][0m |          -0.0097 |          12.3082 |           4.2919 |
[32m[20230204 15:45:04 @agent_ppo2.py:194][0m |          -0.0107 |          12.2021 |           4.2893 |
[32m[20230204 15:45:04 @agent_ppo2.py:194][0m |          -0.0110 |          12.1228 |           4.2888 |
[32m[20230204 15:45:04 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:45:05 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.26
[32m[20230204 15:45:05 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.17
[32m[20230204 15:45:05 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.53
[32m[20230204 15:45:05 @agent_ppo2.py:152][0m Total time:       8.90 min
[32m[20230204 15:45:05 @agent_ppo2.py:154][0m 702464 total steps have happened
[32m[20230204 15:45:05 @agent_ppo2.py:130][0m #------------------------ Iteration 343 --------------------------#
[32m[20230204 15:45:05 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:45:05 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:05 @agent_ppo2.py:194][0m |           0.0009 |          13.7303 |           4.3832 |
[32m[20230204 15:45:05 @agent_ppo2.py:194][0m |          -0.0036 |          12.6632 |           4.3706 |
[32m[20230204 15:45:05 @agent_ppo2.py:194][0m |          -0.0058 |          12.0007 |           4.3731 |
[32m[20230204 15:45:05 @agent_ppo2.py:194][0m |          -0.0058 |          11.7111 |           4.3721 |
[32m[20230204 15:45:05 @agent_ppo2.py:194][0m |          -0.0061 |          11.4534 |           4.3638 |
[32m[20230204 15:45:05 @agent_ppo2.py:194][0m |          -0.0059 |          11.3014 |           4.3655 |
[32m[20230204 15:45:05 @agent_ppo2.py:194][0m |          -0.0089 |          10.9794 |           4.3621 |
[32m[20230204 15:45:06 @agent_ppo2.py:194][0m |          -0.0095 |          10.8009 |           4.3646 |
[32m[20230204 15:45:06 @agent_ppo2.py:194][0m |          -0.0088 |          10.9244 |           4.3629 |
[32m[20230204 15:45:06 @agent_ppo2.py:194][0m |          -0.0082 |          10.8040 |           4.3604 |
[32m[20230204 15:45:06 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:45:06 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.73
[32m[20230204 15:45:06 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.02
[32m[20230204 15:45:06 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.29
[32m[20230204 15:45:06 @agent_ppo2.py:152][0m Total time:       8.92 min
[32m[20230204 15:45:06 @agent_ppo2.py:154][0m 704512 total steps have happened
[32m[20230204 15:45:06 @agent_ppo2.py:130][0m #------------------------ Iteration 344 --------------------------#
[32m[20230204 15:45:06 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:06 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:06 @agent_ppo2.py:194][0m |           0.0013 |          14.8802 |           4.2618 |
[32m[20230204 15:45:06 @agent_ppo2.py:194][0m |          -0.0061 |          13.4032 |           4.2535 |
[32m[20230204 15:45:06 @agent_ppo2.py:194][0m |          -0.0080 |          12.8869 |           4.2552 |
[32m[20230204 15:45:07 @agent_ppo2.py:194][0m |          -0.0093 |          12.6118 |           4.2549 |
[32m[20230204 15:45:07 @agent_ppo2.py:194][0m |          -0.0099 |          12.3500 |           4.2554 |
[32m[20230204 15:45:07 @agent_ppo2.py:194][0m |          -0.0052 |          12.5699 |           4.2580 |
[32m[20230204 15:45:07 @agent_ppo2.py:194][0m |          -0.0095 |          12.1602 |           4.2552 |
[32m[20230204 15:45:07 @agent_ppo2.py:194][0m |          -0.0113 |          11.8832 |           4.2552 |
[32m[20230204 15:45:07 @agent_ppo2.py:194][0m |          -0.0002 |          12.6880 |           4.2616 |
[32m[20230204 15:45:07 @agent_ppo2.py:194][0m |          -0.0117 |          11.7059 |           4.2587 |
[32m[20230204 15:45:07 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:45:07 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.12
[32m[20230204 15:45:07 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.29
[32m[20230204 15:45:07 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 259.18
[32m[20230204 15:45:07 @agent_ppo2.py:152][0m Total time:       8.95 min
[32m[20230204 15:45:07 @agent_ppo2.py:154][0m 706560 total steps have happened
[32m[20230204 15:45:07 @agent_ppo2.py:130][0m #------------------------ Iteration 345 --------------------------#
[32m[20230204 15:45:07 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:45:08 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:08 @agent_ppo2.py:194][0m |           0.0008 |          19.5721 |           4.3849 |
[32m[20230204 15:45:08 @agent_ppo2.py:194][0m |          -0.0038 |          13.3319 |           4.3764 |
[32m[20230204 15:45:08 @agent_ppo2.py:194][0m |          -0.0088 |          12.9103 |           4.3748 |
[32m[20230204 15:45:08 @agent_ppo2.py:194][0m |          -0.0096 |          12.7105 |           4.3716 |
[32m[20230204 15:45:08 @agent_ppo2.py:194][0m |          -0.0119 |          12.3896 |           4.3685 |
[32m[20230204 15:45:08 @agent_ppo2.py:194][0m |          -0.0106 |          12.1717 |           4.3664 |
[32m[20230204 15:45:08 @agent_ppo2.py:194][0m |          -0.0123 |          12.2328 |           4.3612 |
[32m[20230204 15:45:08 @agent_ppo2.py:194][0m |          -0.0138 |          11.8845 |           4.3626 |
[32m[20230204 15:45:08 @agent_ppo2.py:194][0m |          -0.0151 |          11.7240 |           4.3609 |
[32m[20230204 15:45:08 @agent_ppo2.py:194][0m |          -0.0135 |          11.5842 |           4.3584 |
[32m[20230204 15:45:08 @agent_ppo2.py:139][0m Policy update time: 0.80 s
[32m[20230204 15:45:08 @agent_ppo2.py:147][0m Average TRAINING episode reward: 210.78
[32m[20230204 15:45:08 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.19
[32m[20230204 15:45:08 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.64
[32m[20230204 15:45:08 @agent_ppo2.py:152][0m Total time:       8.97 min
[32m[20230204 15:45:08 @agent_ppo2.py:154][0m 708608 total steps have happened
[32m[20230204 15:45:08 @agent_ppo2.py:130][0m #------------------------ Iteration 346 --------------------------#
[32m[20230204 15:45:09 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:09 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:09 @agent_ppo2.py:194][0m |          -0.0001 |          38.9391 |           4.3099 |
[32m[20230204 15:45:09 @agent_ppo2.py:194][0m |          -0.0071 |          20.6022 |           4.3083 |
[32m[20230204 15:45:09 @agent_ppo2.py:194][0m |          -0.0050 |          18.1612 |           4.3054 |
[32m[20230204 15:45:09 @agent_ppo2.py:194][0m |          -0.0107 |          17.0625 |           4.3044 |
[32m[20230204 15:45:09 @agent_ppo2.py:194][0m |          -0.0054 |          16.1365 |           4.3058 |
[32m[20230204 15:45:09 @agent_ppo2.py:194][0m |           0.0187 |          15.4066 |           4.3015 |
[32m[20230204 15:45:09 @agent_ppo2.py:194][0m |          -0.0057 |          14.9564 |           4.2993 |
[32m[20230204 15:45:09 @agent_ppo2.py:194][0m |          -0.0001 |          18.4576 |           4.2994 |
[32m[20230204 15:45:09 @agent_ppo2.py:194][0m |          -0.0119 |          14.2548 |           4.2936 |
[32m[20230204 15:45:09 @agent_ppo2.py:194][0m |          -0.0159 |          13.8452 |           4.2986 |
[32m[20230204 15:45:10 @agent_ppo2.py:139][0m Policy update time: 0.77 s
[32m[20230204 15:45:10 @agent_ppo2.py:147][0m Average TRAINING episode reward: 147.06
[32m[20230204 15:45:10 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.79
[32m[20230204 15:45:10 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.25
[32m[20230204 15:45:10 @agent_ppo2.py:152][0m Total time:       8.99 min
[32m[20230204 15:45:10 @agent_ppo2.py:154][0m 710656 total steps have happened
[32m[20230204 15:45:10 @agent_ppo2.py:130][0m #------------------------ Iteration 347 --------------------------#
[32m[20230204 15:45:10 @agent_ppo2.py:136][0m Sampling time: 0.31 s by 4 slaves
[32m[20230204 15:45:10 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:10 @agent_ppo2.py:194][0m |           0.0015 |          28.8920 |           4.4101 |
[32m[20230204 15:45:10 @agent_ppo2.py:194][0m |          -0.0062 |          22.8267 |           4.4037 |
[32m[20230204 15:45:10 @agent_ppo2.py:194][0m |          -0.0074 |          21.1736 |           4.3969 |
[32m[20230204 15:45:10 @agent_ppo2.py:194][0m |          -0.0094 |          20.1413 |           4.3951 |
[32m[20230204 15:45:11 @agent_ppo2.py:194][0m |          -0.0097 |          19.4937 |           4.3897 |
[32m[20230204 15:45:11 @agent_ppo2.py:194][0m |          -0.0115 |          18.8974 |           4.3887 |
[32m[20230204 15:45:11 @agent_ppo2.py:194][0m |          -0.0116 |          18.3964 |           4.3898 |
[32m[20230204 15:45:11 @agent_ppo2.py:194][0m |          -0.0114 |          17.9525 |           4.3823 |
[32m[20230204 15:45:11 @agent_ppo2.py:194][0m |          -0.0134 |          17.7024 |           4.3827 |
[32m[20230204 15:45:11 @agent_ppo2.py:194][0m |          -0.0112 |          17.3944 |           4.3828 |
[32m[20230204 15:45:11 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:45:11 @agent_ppo2.py:147][0m Average TRAINING episode reward: 206.26
[32m[20230204 15:45:11 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.38
[32m[20230204 15:45:11 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 262.84
[32m[20230204 15:45:11 @agent_ppo2.py:152][0m Total time:       9.01 min
[32m[20230204 15:45:11 @agent_ppo2.py:154][0m 712704 total steps have happened
[32m[20230204 15:45:11 @agent_ppo2.py:130][0m #------------------------ Iteration 348 --------------------------#
[32m[20230204 15:45:11 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:45:11 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:12 @agent_ppo2.py:194][0m |          -0.0010 |          16.3443 |           4.2802 |
[32m[20230204 15:45:12 @agent_ppo2.py:194][0m |          -0.0081 |          13.8418 |           4.2817 |
[32m[20230204 15:45:12 @agent_ppo2.py:194][0m |          -0.0074 |          13.0635 |           4.2821 |
[32m[20230204 15:45:12 @agent_ppo2.py:194][0m |          -0.0119 |          12.1432 |           4.2810 |
[32m[20230204 15:45:12 @agent_ppo2.py:194][0m |          -0.0100 |          11.6036 |           4.2806 |
[32m[20230204 15:45:12 @agent_ppo2.py:194][0m |          -0.0047 |          12.3217 |           4.2837 |
[32m[20230204 15:45:12 @agent_ppo2.py:194][0m |          -0.0060 |          11.0183 |           4.2843 |
[32m[20230204 15:45:12 @agent_ppo2.py:194][0m |          -0.0109 |          10.5509 |           4.2864 |
[32m[20230204 15:45:12 @agent_ppo2.py:194][0m |          -0.0119 |          10.2927 |           4.2850 |
[32m[20230204 15:45:12 @agent_ppo2.py:194][0m |          -0.0116 |          10.0228 |           4.2879 |
[32m[20230204 15:45:12 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:45:13 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.51
[32m[20230204 15:45:13 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.24
[32m[20230204 15:45:13 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 263.84
[32m[20230204 15:45:13 @agent_ppo2.py:152][0m Total time:       9.03 min
[32m[20230204 15:45:13 @agent_ppo2.py:154][0m 714752 total steps have happened
[32m[20230204 15:45:13 @agent_ppo2.py:130][0m #------------------------ Iteration 349 --------------------------#
[32m[20230204 15:45:13 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:45:13 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:13 @agent_ppo2.py:194][0m |           0.0027 |          15.4852 |           4.2590 |
[32m[20230204 15:45:13 @agent_ppo2.py:194][0m |          -0.0015 |          14.4902 |           4.2507 |
[32m[20230204 15:45:13 @agent_ppo2.py:194][0m |          -0.0051 |          14.2697 |           4.2457 |
[32m[20230204 15:45:13 @agent_ppo2.py:194][0m |          -0.0060 |          14.0871 |           4.2406 |
[32m[20230204 15:45:13 @agent_ppo2.py:194][0m |          -0.0078 |          13.6700 |           4.2412 |
[32m[20230204 15:45:13 @agent_ppo2.py:194][0m |          -0.0132 |          13.4519 |           4.2414 |
[32m[20230204 15:45:13 @agent_ppo2.py:194][0m |          -0.0108 |          13.2904 |           4.2385 |
[32m[20230204 15:45:13 @agent_ppo2.py:194][0m |          -0.0138 |          13.1643 |           4.2386 |
[32m[20230204 15:45:14 @agent_ppo2.py:194][0m |          -0.0091 |          13.1835 |           4.2370 |
[32m[20230204 15:45:14 @agent_ppo2.py:194][0m |          -0.0104 |          13.0155 |           4.2350 |
[32m[20230204 15:45:14 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:45:14 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.66
[32m[20230204 15:45:14 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.32
[32m[20230204 15:45:14 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 52.57
[32m[20230204 15:45:14 @agent_ppo2.py:152][0m Total time:       9.05 min
[32m[20230204 15:45:14 @agent_ppo2.py:154][0m 716800 total steps have happened
[32m[20230204 15:45:14 @agent_ppo2.py:130][0m #------------------------ Iteration 350 --------------------------#
[32m[20230204 15:45:14 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:14 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:14 @agent_ppo2.py:194][0m |           0.0066 |          13.6023 |           4.2951 |
[32m[20230204 15:45:14 @agent_ppo2.py:194][0m |          -0.0031 |          10.9614 |           4.2879 |
[32m[20230204 15:45:14 @agent_ppo2.py:194][0m |          -0.0027 |          10.0113 |           4.2895 |
[32m[20230204 15:45:14 @agent_ppo2.py:194][0m |          -0.0043 |           9.5980 |           4.2851 |
[32m[20230204 15:45:14 @agent_ppo2.py:194][0m |          -0.0069 |           9.1164 |           4.2846 |
[32m[20230204 15:45:15 @agent_ppo2.py:194][0m |          -0.0060 |           8.7978 |           4.2849 |
[32m[20230204 15:45:15 @agent_ppo2.py:194][0m |          -0.0090 |           8.5172 |           4.2803 |
[32m[20230204 15:45:15 @agent_ppo2.py:194][0m |          -0.0059 |           8.5847 |           4.2824 |
[32m[20230204 15:45:15 @agent_ppo2.py:194][0m |          -0.0093 |           8.1370 |           4.2814 |
[32m[20230204 15:45:15 @agent_ppo2.py:194][0m |          -0.0067 |           8.0170 |           4.2798 |
[32m[20230204 15:45:15 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:45:15 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.41
[32m[20230204 15:45:15 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.06
[32m[20230204 15:45:15 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.51
[32m[20230204 15:45:15 @agent_ppo2.py:152][0m Total time:       9.08 min
[32m[20230204 15:45:15 @agent_ppo2.py:154][0m 718848 total steps have happened
[32m[20230204 15:45:15 @agent_ppo2.py:130][0m #------------------------ Iteration 351 --------------------------#
[32m[20230204 15:45:15 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:45:15 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:15 @agent_ppo2.py:194][0m |          -0.0021 |          12.0338 |           4.3802 |
[32m[20230204 15:45:16 @agent_ppo2.py:194][0m |          -0.0064 |           9.6680 |           4.3726 |
[32m[20230204 15:45:16 @agent_ppo2.py:194][0m |          -0.0039 |           8.3402 |           4.3710 |
[32m[20230204 15:45:16 @agent_ppo2.py:194][0m |          -0.0094 |           7.5404 |           4.3683 |
[32m[20230204 15:45:16 @agent_ppo2.py:194][0m |          -0.0250 |           7.0670 |           4.3628 |
[32m[20230204 15:45:16 @agent_ppo2.py:194][0m |          -0.0097 |           6.7877 |           4.3665 |
[32m[20230204 15:45:16 @agent_ppo2.py:194][0m |          -0.0111 |           6.4938 |           4.3599 |
[32m[20230204 15:45:16 @agent_ppo2.py:194][0m |          -0.0107 |           6.2723 |           4.3588 |
[32m[20230204 15:45:16 @agent_ppo2.py:194][0m |          -0.0172 |           6.1504 |           4.3605 |
[32m[20230204 15:45:16 @agent_ppo2.py:194][0m |          -0.0102 |           5.9997 |           4.3583 |
[32m[20230204 15:45:16 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:45:16 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.64
[32m[20230204 15:45:16 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.72
[32m[20230204 15:45:16 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.30
[32m[20230204 15:45:16 @agent_ppo2.py:152][0m Total time:       9.10 min
[32m[20230204 15:45:16 @agent_ppo2.py:154][0m 720896 total steps have happened
[32m[20230204 15:45:16 @agent_ppo2.py:130][0m #------------------------ Iteration 352 --------------------------#
[32m[20230204 15:45:17 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:45:17 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:17 @agent_ppo2.py:194][0m |           0.0008 |          16.9467 |           4.3207 |
[32m[20230204 15:45:17 @agent_ppo2.py:194][0m |          -0.0040 |          14.9231 |           4.3096 |
[32m[20230204 15:45:17 @agent_ppo2.py:194][0m |          -0.0067 |          14.4323 |           4.3070 |
[32m[20230204 15:45:17 @agent_ppo2.py:194][0m |          -0.0068 |          14.0600 |           4.3006 |
[32m[20230204 15:45:17 @agent_ppo2.py:194][0m |          -0.0089 |          13.7746 |           4.3004 |
[32m[20230204 15:45:17 @agent_ppo2.py:194][0m |          -0.0091 |          13.5065 |           4.2944 |
[32m[20230204 15:45:17 @agent_ppo2.py:194][0m |          -0.0095 |          13.2670 |           4.2997 |
[32m[20230204 15:45:17 @agent_ppo2.py:194][0m |          -0.0098 |          13.0370 |           4.2971 |
[32m[20230204 15:45:17 @agent_ppo2.py:194][0m |          -0.0110 |          12.7800 |           4.2932 |
[32m[20230204 15:45:17 @agent_ppo2.py:194][0m |          -0.0116 |          12.6281 |           4.2961 |
[32m[20230204 15:45:17 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:45:18 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.87
[32m[20230204 15:45:18 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.73
[32m[20230204 15:45:18 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.58
[32m[20230204 15:45:18 @agent_ppo2.py:152][0m Total time:       9.12 min
[32m[20230204 15:45:18 @agent_ppo2.py:154][0m 722944 total steps have happened
[32m[20230204 15:45:18 @agent_ppo2.py:130][0m #------------------------ Iteration 353 --------------------------#
[32m[20230204 15:45:18 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:45:18 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:18 @agent_ppo2.py:194][0m |          -0.0005 |          12.0827 |           4.2020 |
[32m[20230204 15:45:18 @agent_ppo2.py:194][0m |          -0.0045 |          10.0139 |           4.1986 |
[32m[20230204 15:45:18 @agent_ppo2.py:194][0m |          -0.0077 |           9.2329 |           4.1886 |
[32m[20230204 15:45:18 @agent_ppo2.py:194][0m |          -0.0078 |           8.8410 |           4.1877 |
[32m[20230204 15:45:18 @agent_ppo2.py:194][0m |          -0.0090 |           8.5602 |           4.1868 |
[32m[20230204 15:45:18 @agent_ppo2.py:194][0m |          -0.0098 |           8.3518 |           4.1850 |
[32m[20230204 15:45:19 @agent_ppo2.py:194][0m |          -0.0106 |           8.2212 |           4.1859 |
[32m[20230204 15:45:19 @agent_ppo2.py:194][0m |          -0.0118 |           8.0251 |           4.1833 |
[32m[20230204 15:45:19 @agent_ppo2.py:194][0m |          -0.0102 |           7.9049 |           4.1851 |
[32m[20230204 15:45:19 @agent_ppo2.py:194][0m |          -0.0105 |           7.9833 |           4.1822 |
[32m[20230204 15:45:19 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:45:19 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.13
[32m[20230204 15:45:19 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.69
[32m[20230204 15:45:19 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 263.66
[32m[20230204 15:45:19 @agent_ppo2.py:152][0m Total time:       9.14 min
[32m[20230204 15:45:19 @agent_ppo2.py:154][0m 724992 total steps have happened
[32m[20230204 15:45:19 @agent_ppo2.py:130][0m #------------------------ Iteration 354 --------------------------#
[32m[20230204 15:45:19 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:45:19 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:19 @agent_ppo2.py:194][0m |           0.0014 |          32.9695 |           4.3404 |
[32m[20230204 15:45:20 @agent_ppo2.py:194][0m |          -0.0044 |          29.2489 |           4.3399 |
[32m[20230204 15:45:20 @agent_ppo2.py:194][0m |          -0.0072 |          27.0856 |           4.3356 |
[32m[20230204 15:45:20 @agent_ppo2.py:194][0m |          -0.0079 |          25.8966 |           4.3327 |
[32m[20230204 15:45:20 @agent_ppo2.py:194][0m |          -0.0094 |          25.5190 |           4.3310 |
[32m[20230204 15:45:20 @agent_ppo2.py:194][0m |          -0.0102 |          24.7653 |           4.3303 |
[32m[20230204 15:45:20 @agent_ppo2.py:194][0m |          -0.0108 |          24.0594 |           4.3280 |
[32m[20230204 15:45:20 @agent_ppo2.py:194][0m |          -0.0119 |          23.5993 |           4.3259 |
[32m[20230204 15:45:20 @agent_ppo2.py:194][0m |          -0.0113 |          23.2717 |           4.3276 |
[32m[20230204 15:45:20 @agent_ppo2.py:194][0m |          -0.0128 |          22.8346 |           4.3286 |
[32m[20230204 15:45:20 @agent_ppo2.py:139][0m Policy update time: 0.92 s
[32m[20230204 15:45:20 @agent_ppo2.py:147][0m Average TRAINING episode reward: 195.42
[32m[20230204 15:45:20 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.88
[32m[20230204 15:45:20 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.20
[32m[20230204 15:45:20 @agent_ppo2.py:152][0m Total time:       9.17 min
[32m[20230204 15:45:20 @agent_ppo2.py:154][0m 727040 total steps have happened
[32m[20230204 15:45:20 @agent_ppo2.py:130][0m #------------------------ Iteration 355 --------------------------#
[32m[20230204 15:45:21 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:45:21 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:21 @agent_ppo2.py:194][0m |           0.0017 |          15.7110 |           4.3039 |
[32m[20230204 15:45:21 @agent_ppo2.py:194][0m |          -0.0058 |          14.2655 |           4.2996 |
[32m[20230204 15:45:21 @agent_ppo2.py:194][0m |          -0.0083 |          13.8273 |           4.2964 |
[32m[20230204 15:45:21 @agent_ppo2.py:194][0m |          -0.0094 |          13.5245 |           4.2954 |
[32m[20230204 15:45:21 @agent_ppo2.py:194][0m |          -0.0107 |          13.2984 |           4.2929 |
[32m[20230204 15:45:21 @agent_ppo2.py:194][0m |          -0.0102 |          13.1284 |           4.2965 |
[32m[20230204 15:45:21 @agent_ppo2.py:194][0m |          -0.0113 |          12.9673 |           4.2951 |
[32m[20230204 15:45:21 @agent_ppo2.py:194][0m |          -0.0123 |          12.7677 |           4.2915 |
[32m[20230204 15:45:21 @agent_ppo2.py:194][0m |          -0.0119 |          12.7853 |           4.2944 |
[32m[20230204 15:45:21 @agent_ppo2.py:194][0m |          -0.0135 |          12.5026 |           4.2962 |
[32m[20230204 15:45:21 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:45:22 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.93
[32m[20230204 15:45:22 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.56
[32m[20230204 15:45:22 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 262.03
[32m[20230204 15:45:22 @agent_ppo2.py:152][0m Total time:       9.19 min
[32m[20230204 15:45:22 @agent_ppo2.py:154][0m 729088 total steps have happened
[32m[20230204 15:45:22 @agent_ppo2.py:130][0m #------------------------ Iteration 356 --------------------------#
[32m[20230204 15:45:22 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:22 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:22 @agent_ppo2.py:194][0m |           0.0139 |          14.1581 |           4.3124 |
[32m[20230204 15:45:22 @agent_ppo2.py:194][0m |           0.0040 |          13.0910 |           4.3016 |
[32m[20230204 15:45:22 @agent_ppo2.py:194][0m |          -0.0064 |          12.7207 |           4.2995 |
[32m[20230204 15:45:22 @agent_ppo2.py:194][0m |          -0.0047 |          12.3884 |           4.2988 |
[32m[20230204 15:45:22 @agent_ppo2.py:194][0m |          -0.0078 |          12.2195 |           4.2982 |
[32m[20230204 15:45:22 @agent_ppo2.py:194][0m |          -0.0066 |          12.0163 |           4.2972 |
[32m[20230204 15:45:23 @agent_ppo2.py:194][0m |          -0.0093 |          11.8390 |           4.2926 |
[32m[20230204 15:45:23 @agent_ppo2.py:194][0m |           0.0109 |          13.5044 |           4.2905 |
[32m[20230204 15:45:23 @agent_ppo2.py:194][0m |           0.0059 |          12.4612 |           4.2871 |
[32m[20230204 15:45:23 @agent_ppo2.py:194][0m |          -0.0034 |          11.8007 |           4.2844 |
[32m[20230204 15:45:23 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:45:23 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.12
[32m[20230204 15:45:23 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.35
[32m[20230204 15:45:23 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.86
[32m[20230204 15:45:23 @agent_ppo2.py:152][0m Total time:       9.21 min
[32m[20230204 15:45:23 @agent_ppo2.py:154][0m 731136 total steps have happened
[32m[20230204 15:45:23 @agent_ppo2.py:130][0m #------------------------ Iteration 357 --------------------------#
[32m[20230204 15:45:23 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:23 @agent_ppo2.py:194][0m |           0.0035 |          13.9039 |           4.2809 |
[32m[20230204 15:45:23 @agent_ppo2.py:194][0m |          -0.0011 |          13.1386 |           4.2740 |
[32m[20230204 15:45:24 @agent_ppo2.py:194][0m |          -0.0042 |          12.7840 |           4.2692 |
[32m[20230204 15:45:24 @agent_ppo2.py:194][0m |          -0.0070 |          12.5057 |           4.2723 |
[32m[20230204 15:45:24 @agent_ppo2.py:194][0m |          -0.0072 |          12.3750 |           4.2704 |
[32m[20230204 15:45:24 @agent_ppo2.py:194][0m |          -0.0029 |          12.7158 |           4.2708 |
[32m[20230204 15:45:24 @agent_ppo2.py:194][0m |          -0.0066 |          12.0614 |           4.2719 |
[32m[20230204 15:45:24 @agent_ppo2.py:194][0m |          -0.0078 |          11.9596 |           4.2735 |
[32m[20230204 15:45:24 @agent_ppo2.py:194][0m |          -0.0076 |          11.8673 |           4.2714 |
[32m[20230204 15:45:24 @agent_ppo2.py:194][0m |          -0.0069 |          11.8307 |           4.2768 |
[32m[20230204 15:45:24 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:45:24 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.84
[32m[20230204 15:45:24 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.46
[32m[20230204 15:45:24 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.14
[32m[20230204 15:45:24 @agent_ppo2.py:152][0m Total time:       9.23 min
[32m[20230204 15:45:24 @agent_ppo2.py:154][0m 733184 total steps have happened
[32m[20230204 15:45:24 @agent_ppo2.py:130][0m #------------------------ Iteration 358 --------------------------#
[32m[20230204 15:45:25 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:25 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:25 @agent_ppo2.py:194][0m |          -0.0030 |          14.4158 |           4.2773 |
[32m[20230204 15:45:25 @agent_ppo2.py:194][0m |          -0.0090 |          13.1737 |           4.2708 |
[32m[20230204 15:45:25 @agent_ppo2.py:194][0m |          -0.0104 |          12.8031 |           4.2687 |
[32m[20230204 15:45:25 @agent_ppo2.py:194][0m |          -0.0071 |          12.5818 |           4.2701 |
[32m[20230204 15:45:25 @agent_ppo2.py:194][0m |          -0.0120 |          12.4017 |           4.2694 |
[32m[20230204 15:45:25 @agent_ppo2.py:194][0m |          -0.0122 |          12.3071 |           4.2706 |
[32m[20230204 15:45:25 @agent_ppo2.py:194][0m |          -0.0116 |          12.1173 |           4.2698 |
[32m[20230204 15:45:25 @agent_ppo2.py:194][0m |          -0.0068 |          12.3491 |           4.2681 |
[32m[20230204 15:45:25 @agent_ppo2.py:194][0m |          -0.0110 |          11.4645 |           4.2657 |
[32m[20230204 15:45:25 @agent_ppo2.py:194][0m |          -0.0137 |          11.0061 |           4.2673 |
[32m[20230204 15:45:25 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:45:26 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.21
[32m[20230204 15:45:26 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.63
[32m[20230204 15:45:26 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.25
[32m[20230204 15:45:26 @agent_ppo2.py:152][0m Total time:       9.25 min
[32m[20230204 15:45:26 @agent_ppo2.py:154][0m 735232 total steps have happened
[32m[20230204 15:45:26 @agent_ppo2.py:130][0m #------------------------ Iteration 359 --------------------------#
[32m[20230204 15:45:26 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:26 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:26 @agent_ppo2.py:194][0m |          -0.0006 |          12.5495 |           4.3818 |
[32m[20230204 15:45:26 @agent_ppo2.py:194][0m |          -0.0026 |          10.8161 |           4.3794 |
[32m[20230204 15:45:26 @agent_ppo2.py:194][0m |          -0.0093 |          10.2150 |           4.3809 |
[32m[20230204 15:45:26 @agent_ppo2.py:194][0m |          -0.0097 |           9.7401 |           4.3766 |
[32m[20230204 15:45:26 @agent_ppo2.py:194][0m |           0.0778 |          16.0708 |           4.3802 |
[32m[20230204 15:45:27 @agent_ppo2.py:194][0m |          -0.0052 |           9.6379 |           4.3825 |
[32m[20230204 15:45:27 @agent_ppo2.py:194][0m |          -0.0156 |           9.0422 |           4.3778 |
[32m[20230204 15:45:27 @agent_ppo2.py:194][0m |          -0.0041 |           8.9368 |           4.3820 |
[32m[20230204 15:45:27 @agent_ppo2.py:194][0m |          -0.0150 |           8.6890 |           4.3729 |
[32m[20230204 15:45:27 @agent_ppo2.py:194][0m |          -0.0128 |           8.5402 |           4.3780 |
[32m[20230204 15:45:27 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:45:27 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.88
[32m[20230204 15:45:27 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.07
[32m[20230204 15:45:27 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 7.24
[32m[20230204 15:45:27 @agent_ppo2.py:152][0m Total time:       9.27 min
[32m[20230204 15:45:27 @agent_ppo2.py:154][0m 737280 total steps have happened
[32m[20230204 15:45:27 @agent_ppo2.py:130][0m #------------------------ Iteration 360 --------------------------#
[32m[20230204 15:45:27 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:27 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:27 @agent_ppo2.py:194][0m |          -0.0006 |          13.2431 |           4.3305 |
[32m[20230204 15:45:27 @agent_ppo2.py:194][0m |          -0.0056 |          11.6057 |           4.3298 |
[32m[20230204 15:45:27 @agent_ppo2.py:194][0m |          -0.0087 |          10.7949 |           4.3225 |
[32m[20230204 15:45:28 @agent_ppo2.py:194][0m |          -0.0104 |          10.4587 |           4.3221 |
[32m[20230204 15:45:28 @agent_ppo2.py:194][0m |          -0.0119 |          10.2494 |           4.3190 |
[32m[20230204 15:45:28 @agent_ppo2.py:194][0m |          -0.0112 |          10.2307 |           4.3199 |
[32m[20230204 15:45:28 @agent_ppo2.py:194][0m |          -0.0118 |          10.0385 |           4.3146 |
[32m[20230204 15:45:28 @agent_ppo2.py:194][0m |          -0.0124 |           9.8694 |           4.3156 |
[32m[20230204 15:45:28 @agent_ppo2.py:194][0m |          -0.0146 |           9.6563 |           4.3168 |
[32m[20230204 15:45:28 @agent_ppo2.py:194][0m |          -0.0151 |           9.5643 |           4.3125 |
[32m[20230204 15:45:28 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:45:28 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.65
[32m[20230204 15:45:28 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.96
[32m[20230204 15:45:28 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.67
[32m[20230204 15:45:28 @agent_ppo2.py:152][0m Total time:       9.30 min
[32m[20230204 15:45:28 @agent_ppo2.py:154][0m 739328 total steps have happened
[32m[20230204 15:45:28 @agent_ppo2.py:130][0m #------------------------ Iteration 361 --------------------------#
[32m[20230204 15:45:29 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:29 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:29 @agent_ppo2.py:194][0m |           0.0022 |          13.9213 |           4.3386 |
[32m[20230204 15:45:29 @agent_ppo2.py:194][0m |          -0.0026 |          12.7260 |           4.3323 |
[32m[20230204 15:45:29 @agent_ppo2.py:194][0m |          -0.0046 |          12.0343 |           4.3326 |
[32m[20230204 15:45:29 @agent_ppo2.py:194][0m |          -0.0061 |          11.5131 |           4.3328 |
[32m[20230204 15:45:29 @agent_ppo2.py:194][0m |          -0.0066 |          11.1489 |           4.3286 |
[32m[20230204 15:45:29 @agent_ppo2.py:194][0m |          -0.0072 |          10.6701 |           4.3269 |
[32m[20230204 15:45:29 @agent_ppo2.py:194][0m |          -0.0085 |          10.2546 |           4.3315 |
[32m[20230204 15:45:29 @agent_ppo2.py:194][0m |          -0.0085 |           9.7774 |           4.3261 |
[32m[20230204 15:45:29 @agent_ppo2.py:194][0m |          -0.0096 |           9.5515 |           4.3261 |
[32m[20230204 15:45:29 @agent_ppo2.py:194][0m |          -0.0096 |           9.3009 |           4.3255 |
[32m[20230204 15:45:29 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:45:30 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.62
[32m[20230204 15:45:30 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.11
[32m[20230204 15:45:30 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 85.29
[32m[20230204 15:45:30 @agent_ppo2.py:152][0m Total time:       9.32 min
[32m[20230204 15:45:30 @agent_ppo2.py:154][0m 741376 total steps have happened
[32m[20230204 15:45:30 @agent_ppo2.py:130][0m #------------------------ Iteration 362 --------------------------#
[32m[20230204 15:45:30 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:30 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:30 @agent_ppo2.py:194][0m |          -0.0004 |          27.6970 |           4.3197 |
[32m[20230204 15:45:30 @agent_ppo2.py:194][0m |          -0.0032 |          22.5451 |           4.3111 |
[32m[20230204 15:45:30 @agent_ppo2.py:194][0m |          -0.0045 |          21.0531 |           4.3119 |
[32m[20230204 15:45:30 @agent_ppo2.py:194][0m |          -0.0038 |          19.6634 |           4.3081 |
[32m[20230204 15:45:30 @agent_ppo2.py:194][0m |          -0.0050 |          19.2013 |           4.3031 |
[32m[20230204 15:45:30 @agent_ppo2.py:194][0m |          -0.0062 |          18.1505 |           4.2969 |
[32m[20230204 15:45:30 @agent_ppo2.py:194][0m |          -0.0088 |          17.7772 |           4.2992 |
[32m[20230204 15:45:31 @agent_ppo2.py:194][0m |          -0.0068 |          17.2342 |           4.2967 |
[32m[20230204 15:45:31 @agent_ppo2.py:194][0m |          -0.0075 |          16.6658 |           4.2970 |
[32m[20230204 15:45:31 @agent_ppo2.py:194][0m |          -0.0097 |          16.0035 |           4.2897 |
[32m[20230204 15:45:31 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:45:31 @agent_ppo2.py:147][0m Average TRAINING episode reward: 219.77
[32m[20230204 15:45:31 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 256.84
[32m[20230204 15:45:31 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 100.10
[32m[20230204 15:45:31 @agent_ppo2.py:152][0m Total time:       9.34 min
[32m[20230204 15:45:31 @agent_ppo2.py:154][0m 743424 total steps have happened
[32m[20230204 15:45:31 @agent_ppo2.py:130][0m #------------------------ Iteration 363 --------------------------#
[32m[20230204 15:45:31 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:45:31 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:31 @agent_ppo2.py:194][0m |           0.0024 |          17.5212 |           4.2478 |
[32m[20230204 15:45:32 @agent_ppo2.py:194][0m |          -0.0065 |          15.5751 |           4.2492 |
[32m[20230204 15:45:32 @agent_ppo2.py:194][0m |          -0.0073 |          15.0926 |           4.2538 |
[32m[20230204 15:45:32 @agent_ppo2.py:194][0m |          -0.0093 |          14.7813 |           4.2496 |
[32m[20230204 15:45:32 @agent_ppo2.py:194][0m |          -0.0087 |          14.6509 |           4.2500 |
[32m[20230204 15:45:32 @agent_ppo2.py:194][0m |          -0.0094 |          14.5457 |           4.2511 |
[32m[20230204 15:45:32 @agent_ppo2.py:194][0m |          -0.0102 |          14.1802 |           4.2517 |
[32m[20230204 15:45:32 @agent_ppo2.py:194][0m |          -0.0079 |          14.2127 |           4.2513 |
[32m[20230204 15:45:32 @agent_ppo2.py:194][0m |          -0.0111 |          13.8342 |           4.2562 |
[32m[20230204 15:45:32 @agent_ppo2.py:194][0m |          -0.0128 |          13.5702 |           4.2576 |
[32m[20230204 15:45:32 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:45:32 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.28
[32m[20230204 15:45:32 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.67
[32m[20230204 15:45:32 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.55
[32m[20230204 15:45:32 @agent_ppo2.py:152][0m Total time:       9.37 min
[32m[20230204 15:45:32 @agent_ppo2.py:154][0m 745472 total steps have happened
[32m[20230204 15:45:32 @agent_ppo2.py:130][0m #------------------------ Iteration 364 --------------------------#
[32m[20230204 15:45:33 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:45:33 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:33 @agent_ppo2.py:194][0m |           0.0018 |          14.8309 |           4.2150 |
[32m[20230204 15:45:33 @agent_ppo2.py:194][0m |          -0.0057 |          13.5587 |           4.2073 |
[32m[20230204 15:45:33 @agent_ppo2.py:194][0m |          -0.0096 |          13.0400 |           4.2076 |
[32m[20230204 15:45:33 @agent_ppo2.py:194][0m |          -0.0101 |          12.6085 |           4.1982 |
[32m[20230204 15:45:33 @agent_ppo2.py:194][0m |          -0.0103 |          12.2769 |           4.2032 |
[32m[20230204 15:45:33 @agent_ppo2.py:194][0m |          -0.0100 |          11.9848 |           4.2017 |
[32m[20230204 15:45:33 @agent_ppo2.py:194][0m |          -0.0132 |          11.7222 |           4.2018 |
[32m[20230204 15:45:33 @agent_ppo2.py:194][0m |          -0.0083 |          12.1234 |           4.1972 |
[32m[20230204 15:45:33 @agent_ppo2.py:194][0m |          -0.0113 |          11.3344 |           4.2044 |
[32m[20230204 15:45:33 @agent_ppo2.py:194][0m |          -0.0159 |          11.1753 |           4.2020 |
[32m[20230204 15:45:33 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:45:34 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.17
[32m[20230204 15:45:34 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.37
[32m[20230204 15:45:34 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.24
[32m[20230204 15:45:34 @agent_ppo2.py:152][0m Total time:       9.39 min
[32m[20230204 15:45:34 @agent_ppo2.py:154][0m 747520 total steps have happened
[32m[20230204 15:45:34 @agent_ppo2.py:130][0m #------------------------ Iteration 365 --------------------------#
[32m[20230204 15:45:34 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:34 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:34 @agent_ppo2.py:194][0m |           0.0003 |          15.0131 |           4.3533 |
[32m[20230204 15:45:34 @agent_ppo2.py:194][0m |          -0.0049 |          13.7912 |           4.3558 |
[32m[20230204 15:45:34 @agent_ppo2.py:194][0m |          -0.0075 |          13.1614 |           4.3510 |
[32m[20230204 15:45:34 @agent_ppo2.py:194][0m |          -0.0096 |          12.6638 |           4.3512 |
[32m[20230204 15:45:34 @agent_ppo2.py:194][0m |          -0.0098 |          12.2104 |           4.3490 |
[32m[20230204 15:45:34 @agent_ppo2.py:194][0m |          -0.0076 |          12.0256 |           4.3478 |
[32m[20230204 15:45:35 @agent_ppo2.py:194][0m |          -0.0107 |          11.2244 |           4.3467 |
[32m[20230204 15:45:35 @agent_ppo2.py:194][0m |          -0.0114 |          10.8222 |           4.3480 |
[32m[20230204 15:45:35 @agent_ppo2.py:194][0m |          -0.0118 |          10.4808 |           4.3456 |
[32m[20230204 15:45:35 @agent_ppo2.py:194][0m |          -0.0104 |          10.2152 |           4.3450 |
[32m[20230204 15:45:35 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:45:35 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.33
[32m[20230204 15:45:35 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.39
[32m[20230204 15:45:35 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.60
[32m[20230204 15:45:35 @agent_ppo2.py:152][0m Total time:       9.41 min
[32m[20230204 15:45:35 @agent_ppo2.py:154][0m 749568 total steps have happened
[32m[20230204 15:45:35 @agent_ppo2.py:130][0m #------------------------ Iteration 366 --------------------------#
[32m[20230204 15:45:35 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:35 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:35 @agent_ppo2.py:194][0m |           0.0007 |          14.1998 |           4.3976 |
[32m[20230204 15:45:35 @agent_ppo2.py:194][0m |          -0.0072 |          12.5844 |           4.3884 |
[32m[20230204 15:45:36 @agent_ppo2.py:194][0m |          -0.0091 |          11.9351 |           4.3845 |
[32m[20230204 15:45:36 @agent_ppo2.py:194][0m |          -0.0106 |          11.5018 |           4.3816 |
[32m[20230204 15:45:36 @agent_ppo2.py:194][0m |          -0.0102 |          11.1188 |           4.3825 |
[32m[20230204 15:45:36 @agent_ppo2.py:194][0m |          -0.0120 |          10.8573 |           4.3810 |
[32m[20230204 15:45:36 @agent_ppo2.py:194][0m |          -0.0121 |          10.5785 |           4.3817 |
[32m[20230204 15:45:36 @agent_ppo2.py:194][0m |          -0.0131 |          10.3815 |           4.3761 |
[32m[20230204 15:45:36 @agent_ppo2.py:194][0m |          -0.0133 |          10.1413 |           4.3735 |
[32m[20230204 15:45:36 @agent_ppo2.py:194][0m |          -0.0138 |           9.9157 |           4.3744 |
[32m[20230204 15:45:36 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:45:36 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.77
[32m[20230204 15:45:36 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.18
[32m[20230204 15:45:36 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 275.25
[32m[20230204 15:45:36 @agent_ppo2.py:152][0m Total time:       9.43 min
[32m[20230204 15:45:36 @agent_ppo2.py:154][0m 751616 total steps have happened
[32m[20230204 15:45:36 @agent_ppo2.py:130][0m #------------------------ Iteration 367 --------------------------#
[32m[20230204 15:45:37 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:45:37 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:37 @agent_ppo2.py:194][0m |           0.0008 |          13.5307 |           4.3339 |
[32m[20230204 15:45:37 @agent_ppo2.py:194][0m |          -0.0030 |          11.9437 |           4.3264 |
[32m[20230204 15:45:37 @agent_ppo2.py:194][0m |          -0.0047 |          11.1074 |           4.3245 |
[32m[20230204 15:45:37 @agent_ppo2.py:194][0m |          -0.0067 |          10.5929 |           4.3234 |
[32m[20230204 15:45:37 @agent_ppo2.py:194][0m |          -0.0082 |          10.0762 |           4.3220 |
[32m[20230204 15:45:37 @agent_ppo2.py:194][0m |          -0.0088 |           9.7694 |           4.3204 |
[32m[20230204 15:45:37 @agent_ppo2.py:194][0m |          -0.0074 |           9.4359 |           4.3167 |
[32m[20230204 15:45:37 @agent_ppo2.py:194][0m |          -0.0088 |           9.2150 |           4.3194 |
[32m[20230204 15:45:37 @agent_ppo2.py:194][0m |          -0.0093 |           8.9476 |           4.3167 |
[32m[20230204 15:45:37 @agent_ppo2.py:194][0m |          -0.0091 |           8.7798 |           4.3162 |
[32m[20230204 15:45:37 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:45:38 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.25
[32m[20230204 15:45:38 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.87
[32m[20230204 15:45:38 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.88
[32m[20230204 15:45:38 @agent_ppo2.py:152][0m Total time:       9.45 min
[32m[20230204 15:45:38 @agent_ppo2.py:154][0m 753664 total steps have happened
[32m[20230204 15:45:38 @agent_ppo2.py:130][0m #------------------------ Iteration 368 --------------------------#
[32m[20230204 15:45:38 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:45:38 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:38 @agent_ppo2.py:194][0m |           0.0010 |          14.2700 |           4.2957 |
[32m[20230204 15:45:38 @agent_ppo2.py:194][0m |          -0.0022 |          13.3497 |           4.2978 |
[32m[20230204 15:45:38 @agent_ppo2.py:194][0m |          -0.0042 |          12.8830 |           4.2938 |
[32m[20230204 15:45:38 @agent_ppo2.py:194][0m |          -0.0041 |          12.6649 |           4.2951 |
[32m[20230204 15:45:38 @agent_ppo2.py:194][0m |          -0.0072 |          12.3417 |           4.2937 |
[32m[20230204 15:45:38 @agent_ppo2.py:194][0m |          -0.0088 |          12.1645 |           4.2933 |
[32m[20230204 15:45:39 @agent_ppo2.py:194][0m |          -0.0089 |          12.0106 |           4.2916 |
[32m[20230204 15:45:39 @agent_ppo2.py:194][0m |          -0.0097 |          11.8239 |           4.2912 |
[32m[20230204 15:45:39 @agent_ppo2.py:194][0m |          -0.0103 |          11.7096 |           4.2908 |
[32m[20230204 15:45:39 @agent_ppo2.py:194][0m |          -0.0099 |          11.5923 |           4.2878 |
[32m[20230204 15:45:39 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:45:39 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.48
[32m[20230204 15:45:39 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.99
[32m[20230204 15:45:39 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.55
[32m[20230204 15:45:39 @agent_ppo2.py:152][0m Total time:       9.48 min
[32m[20230204 15:45:39 @agent_ppo2.py:154][0m 755712 total steps have happened
[32m[20230204 15:45:39 @agent_ppo2.py:130][0m #------------------------ Iteration 369 --------------------------#
[32m[20230204 15:45:39 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:45:39 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:39 @agent_ppo2.py:194][0m |           0.0015 |          13.7619 |           4.2418 |
[32m[20230204 15:45:39 @agent_ppo2.py:194][0m |          -0.0042 |          10.5213 |           4.2322 |
[32m[20230204 15:45:40 @agent_ppo2.py:194][0m |          -0.0033 |           9.1395 |           4.2285 |
[32m[20230204 15:45:40 @agent_ppo2.py:194][0m |          -0.0090 |           8.5576 |           4.2201 |
[32m[20230204 15:45:40 @agent_ppo2.py:194][0m |          -0.0078 |           8.1536 |           4.2208 |
[32m[20230204 15:45:40 @agent_ppo2.py:194][0m |          -0.0075 |           7.8594 |           4.2178 |
[32m[20230204 15:45:40 @agent_ppo2.py:194][0m |          -0.0068 |           7.4853 |           4.2166 |
[32m[20230204 15:45:40 @agent_ppo2.py:194][0m |          -0.0103 |           7.2914 |           4.2135 |
[32m[20230204 15:45:40 @agent_ppo2.py:194][0m |          -0.0129 |           7.0932 |           4.2139 |
[32m[20230204 15:45:40 @agent_ppo2.py:194][0m |          -0.0132 |           6.8615 |           4.2103 |
[32m[20230204 15:45:40 @agent_ppo2.py:139][0m Policy update time: 0.91 s
[32m[20230204 15:45:40 @agent_ppo2.py:147][0m Average TRAINING episode reward: 254.39
[32m[20230204 15:45:40 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 257.02
[32m[20230204 15:45:40 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.27
[32m[20230204 15:45:40 @agent_ppo2.py:152][0m Total time:       9.50 min
[32m[20230204 15:45:40 @agent_ppo2.py:154][0m 757760 total steps have happened
[32m[20230204 15:45:40 @agent_ppo2.py:130][0m #------------------------ Iteration 370 --------------------------#
[32m[20230204 15:45:41 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:41 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:41 @agent_ppo2.py:194][0m |           0.0001 |          13.6320 |           4.2818 |
[32m[20230204 15:45:41 @agent_ppo2.py:194][0m |          -0.0056 |          12.0597 |           4.2729 |
[32m[20230204 15:45:41 @agent_ppo2.py:194][0m |          -0.0073 |          11.5684 |           4.2733 |
[32m[20230204 15:45:41 @agent_ppo2.py:194][0m |          -0.0102 |          11.1618 |           4.2674 |
[32m[20230204 15:45:41 @agent_ppo2.py:194][0m |          -0.0101 |          10.8085 |           4.2638 |
[32m[20230204 15:45:41 @agent_ppo2.py:194][0m |          -0.0107 |          10.6035 |           4.2639 |
[32m[20230204 15:45:41 @agent_ppo2.py:194][0m |          -0.0122 |          10.3444 |           4.2596 |
[32m[20230204 15:45:41 @agent_ppo2.py:194][0m |          -0.0119 |          10.1789 |           4.2535 |
[32m[20230204 15:45:41 @agent_ppo2.py:194][0m |          -0.0126 |           9.9596 |           4.2607 |
[32m[20230204 15:45:41 @agent_ppo2.py:194][0m |          -0.0134 |           9.8129 |           4.2545 |
[32m[20230204 15:45:41 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:45:42 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.10
[32m[20230204 15:45:42 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.75
[32m[20230204 15:45:42 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.46
[32m[20230204 15:45:42 @agent_ppo2.py:152][0m Total time:       9.52 min
[32m[20230204 15:45:42 @agent_ppo2.py:154][0m 759808 total steps have happened
[32m[20230204 15:45:42 @agent_ppo2.py:130][0m #------------------------ Iteration 371 --------------------------#
[32m[20230204 15:45:42 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:42 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:42 @agent_ppo2.py:194][0m |           0.0000 |          16.2917 |           4.2729 |
[32m[20230204 15:45:42 @agent_ppo2.py:194][0m |          -0.0033 |          14.8106 |           4.2686 |
[32m[20230204 15:45:42 @agent_ppo2.py:194][0m |          -0.0064 |          14.2415 |           4.2661 |
[32m[20230204 15:45:42 @agent_ppo2.py:194][0m |          -0.0087 |          13.8097 |           4.2620 |
[32m[20230204 15:45:42 @agent_ppo2.py:194][0m |          -0.0085 |          13.4641 |           4.2615 |
[32m[20230204 15:45:42 @agent_ppo2.py:194][0m |          -0.0091 |          13.2129 |           4.2650 |
[32m[20230204 15:45:43 @agent_ppo2.py:194][0m |          -0.0096 |          13.0167 |           4.2638 |
[32m[20230204 15:45:43 @agent_ppo2.py:194][0m |          -0.0106 |          12.9344 |           4.2638 |
[32m[20230204 15:45:43 @agent_ppo2.py:194][0m |          -0.0117 |          12.7169 |           4.2629 |
[32m[20230204 15:45:43 @agent_ppo2.py:194][0m |          -0.0128 |          12.5688 |           4.2665 |
[32m[20230204 15:45:43 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:45:43 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.35
[32m[20230204 15:45:43 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 266.09
[32m[20230204 15:45:43 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 131.45
[32m[20230204 15:45:43 @agent_ppo2.py:152][0m Total time:       9.54 min
[32m[20230204 15:45:43 @agent_ppo2.py:154][0m 761856 total steps have happened
[32m[20230204 15:45:43 @agent_ppo2.py:130][0m #------------------------ Iteration 372 --------------------------#
[32m[20230204 15:45:43 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:43 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:43 @agent_ppo2.py:194][0m |           0.0015 |          13.7249 |           4.3252 |
[32m[20230204 15:45:43 @agent_ppo2.py:194][0m |          -0.0051 |          12.5280 |           4.3084 |
[32m[20230204 15:45:44 @agent_ppo2.py:194][0m |          -0.0058 |          12.0851 |           4.3164 |
[32m[20230204 15:45:44 @agent_ppo2.py:194][0m |          -0.0068 |          11.7075 |           4.3131 |
[32m[20230204 15:45:44 @agent_ppo2.py:194][0m |          -0.0078 |          11.3551 |           4.3140 |
[32m[20230204 15:45:44 @agent_ppo2.py:194][0m |          -0.0097 |          11.0407 |           4.3162 |
[32m[20230204 15:45:44 @agent_ppo2.py:194][0m |          -0.0091 |          10.8098 |           4.3116 |
[32m[20230204 15:45:44 @agent_ppo2.py:194][0m |          -0.0103 |          10.5899 |           4.3182 |
[32m[20230204 15:45:44 @agent_ppo2.py:194][0m |          -0.0102 |          10.3495 |           4.3180 |
[32m[20230204 15:45:44 @agent_ppo2.py:194][0m |          -0.0115 |          10.1344 |           4.3200 |
[32m[20230204 15:45:44 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:45:44 @agent_ppo2.py:147][0m Average TRAINING episode reward: 255.95
[32m[20230204 15:45:44 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.61
[32m[20230204 15:45:44 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.81
[32m[20230204 15:45:44 @agent_ppo2.py:152][0m Total time:       9.56 min
[32m[20230204 15:45:44 @agent_ppo2.py:154][0m 763904 total steps have happened
[32m[20230204 15:45:44 @agent_ppo2.py:130][0m #------------------------ Iteration 373 --------------------------#
[32m[20230204 15:45:45 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:45:45 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:45 @agent_ppo2.py:194][0m |          -0.0012 |          14.5133 |           4.3214 |
[32m[20230204 15:45:45 @agent_ppo2.py:194][0m |          -0.0050 |          12.8708 |           4.3170 |
[32m[20230204 15:45:45 @agent_ppo2.py:194][0m |          -0.0067 |          11.9387 |           4.3152 |
[32m[20230204 15:45:45 @agent_ppo2.py:194][0m |          -0.0078 |          11.3121 |           4.3168 |
[32m[20230204 15:45:45 @agent_ppo2.py:194][0m |          -0.0087 |          10.9566 |           4.3134 |
[32m[20230204 15:45:45 @agent_ppo2.py:194][0m |          -0.0099 |          10.6071 |           4.3171 |
[32m[20230204 15:45:45 @agent_ppo2.py:194][0m |          -0.0114 |          10.3932 |           4.3168 |
[32m[20230204 15:45:45 @agent_ppo2.py:194][0m |          -0.0075 |          10.2113 |           4.3165 |
[32m[20230204 15:45:45 @agent_ppo2.py:194][0m |          -0.0079 |          10.1993 |           4.3137 |
[32m[20230204 15:45:45 @agent_ppo2.py:194][0m |          -0.0122 |           9.8138 |           4.3164 |
[32m[20230204 15:45:45 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:45:46 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.38
[32m[20230204 15:45:46 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.76
[32m[20230204 15:45:46 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.48
[32m[20230204 15:45:46 @agent_ppo2.py:152][0m Total time:       9.59 min
[32m[20230204 15:45:46 @agent_ppo2.py:154][0m 765952 total steps have happened
[32m[20230204 15:45:46 @agent_ppo2.py:130][0m #------------------------ Iteration 374 --------------------------#
[32m[20230204 15:45:46 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:46 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:46 @agent_ppo2.py:194][0m |          -0.0013 |          13.1739 |           4.3589 |
[32m[20230204 15:45:46 @agent_ppo2.py:194][0m |          -0.0082 |          11.7845 |           4.3502 |
[32m[20230204 15:45:46 @agent_ppo2.py:194][0m |          -0.0070 |          11.4347 |           4.3488 |
[32m[20230204 15:45:46 @agent_ppo2.py:194][0m |          -0.0120 |          10.7544 |           4.3445 |
[32m[20230204 15:45:46 @agent_ppo2.py:194][0m |          -0.0067 |          10.5947 |           4.3473 |
[32m[20230204 15:45:46 @agent_ppo2.py:194][0m |          -0.0119 |          10.1836 |           4.3484 |
[32m[20230204 15:45:46 @agent_ppo2.py:194][0m |          -0.0140 |           9.9272 |           4.3467 |
[32m[20230204 15:45:47 @agent_ppo2.py:194][0m |          -0.0116 |           9.9147 |           4.3465 |
[32m[20230204 15:45:47 @agent_ppo2.py:194][0m |          -0.0133 |           9.5667 |           4.3481 |
[32m[20230204 15:45:47 @agent_ppo2.py:194][0m |          -0.0149 |           9.3773 |           4.3428 |
[32m[20230204 15:45:47 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:45:47 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.85
[32m[20230204 15:45:47 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.69
[32m[20230204 15:45:47 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.66
[32m[20230204 15:45:47 @agent_ppo2.py:152][0m Total time:       9.61 min
[32m[20230204 15:45:47 @agent_ppo2.py:154][0m 768000 total steps have happened
[32m[20230204 15:45:47 @agent_ppo2.py:130][0m #------------------------ Iteration 375 --------------------------#
[32m[20230204 15:45:47 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:47 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:47 @agent_ppo2.py:194][0m |           0.0031 |          13.3292 |           4.3573 |
[32m[20230204 15:45:47 @agent_ppo2.py:194][0m |          -0.0049 |          11.4302 |           4.3531 |
[32m[20230204 15:45:48 @agent_ppo2.py:194][0m |          -0.0047 |          10.6379 |           4.3499 |
[32m[20230204 15:45:48 @agent_ppo2.py:194][0m |          -0.0064 |           9.9856 |           4.3441 |
[32m[20230204 15:45:48 @agent_ppo2.py:194][0m |          -0.0069 |           9.5575 |           4.3442 |
[32m[20230204 15:45:48 @agent_ppo2.py:194][0m |          -0.0087 |           9.0431 |           4.3429 |
[32m[20230204 15:45:48 @agent_ppo2.py:194][0m |          -0.0088 |           8.7266 |           4.3438 |
[32m[20230204 15:45:48 @agent_ppo2.py:194][0m |          -0.0087 |           8.5309 |           4.3432 |
[32m[20230204 15:45:48 @agent_ppo2.py:194][0m |          -0.0083 |           8.2311 |           4.3359 |
[32m[20230204 15:45:48 @agent_ppo2.py:194][0m |          -0.0110 |           7.7575 |           4.3389 |
[32m[20230204 15:45:48 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:45:48 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.94
[32m[20230204 15:45:48 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.84
[32m[20230204 15:45:48 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.19
[32m[20230204 15:45:48 @agent_ppo2.py:152][0m Total time:       9.63 min
[32m[20230204 15:45:48 @agent_ppo2.py:154][0m 770048 total steps have happened
[32m[20230204 15:45:48 @agent_ppo2.py:130][0m #------------------------ Iteration 376 --------------------------#
[32m[20230204 15:45:49 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:49 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:49 @agent_ppo2.py:194][0m |          -0.0021 |          14.0741 |           4.3015 |
[32m[20230204 15:45:49 @agent_ppo2.py:194][0m |          -0.0049 |          13.0318 |           4.2970 |
[32m[20230204 15:45:49 @agent_ppo2.py:194][0m |          -0.0046 |          13.1438 |           4.2941 |
[32m[20230204 15:45:49 @agent_ppo2.py:194][0m |          -0.0090 |          12.4163 |           4.2938 |
[32m[20230204 15:45:49 @agent_ppo2.py:194][0m |           0.0043 |          13.3702 |           4.2973 |
[32m[20230204 15:45:49 @agent_ppo2.py:194][0m |          -0.0087 |          12.1050 |           4.2919 |
[32m[20230204 15:45:49 @agent_ppo2.py:194][0m |          -0.0131 |          11.9185 |           4.2930 |
[32m[20230204 15:45:49 @agent_ppo2.py:194][0m |          -0.0125 |          11.8073 |           4.2931 |
[32m[20230204 15:45:49 @agent_ppo2.py:194][0m |          -0.0114 |          11.6984 |           4.2947 |
[32m[20230204 15:45:49 @agent_ppo2.py:194][0m |          -0.0159 |          11.5817 |           4.2944 |
[32m[20230204 15:45:49 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:45:50 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.66
[32m[20230204 15:45:50 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.88
[32m[20230204 15:45:50 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.17
[32m[20230204 15:45:50 @agent_ppo2.py:152][0m Total time:       9.65 min
[32m[20230204 15:45:50 @agent_ppo2.py:154][0m 772096 total steps have happened
[32m[20230204 15:45:50 @agent_ppo2.py:130][0m #------------------------ Iteration 377 --------------------------#
[32m[20230204 15:45:50 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:45:50 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:50 @agent_ppo2.py:194][0m |           0.0004 |          14.6417 |           4.3620 |
[32m[20230204 15:45:50 @agent_ppo2.py:194][0m |          -0.0021 |          13.7796 |           4.3609 |
[32m[20230204 15:45:50 @agent_ppo2.py:194][0m |          -0.0036 |          13.3989 |           4.3567 |
[32m[20230204 15:45:50 @agent_ppo2.py:194][0m |          -0.0068 |          12.7102 |           4.3596 |
[32m[20230204 15:45:50 @agent_ppo2.py:194][0m |          -0.0082 |          12.3290 |           4.3568 |
[32m[20230204 15:45:50 @agent_ppo2.py:194][0m |          -0.0105 |          12.0716 |           4.3576 |
[32m[20230204 15:45:50 @agent_ppo2.py:194][0m |          -0.0107 |          11.7149 |           4.3569 |
[32m[20230204 15:45:51 @agent_ppo2.py:194][0m |          -0.0086 |          11.7004 |           4.3544 |
[32m[20230204 15:45:51 @agent_ppo2.py:194][0m |          -0.0116 |          11.2577 |           4.3565 |
[32m[20230204 15:45:51 @agent_ppo2.py:194][0m |          -0.0127 |          11.0268 |           4.3564 |
[32m[20230204 15:45:51 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:45:51 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.18
[32m[20230204 15:45:51 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.60
[32m[20230204 15:45:51 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.89
[32m[20230204 15:45:51 @agent_ppo2.py:152][0m Total time:       9.68 min
[32m[20230204 15:45:51 @agent_ppo2.py:154][0m 774144 total steps have happened
[32m[20230204 15:45:51 @agent_ppo2.py:130][0m #------------------------ Iteration 378 --------------------------#
[32m[20230204 15:45:51 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:45:51 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:51 @agent_ppo2.py:194][0m |           0.0006 |          13.2629 |           4.3164 |
[32m[20230204 15:45:51 @agent_ppo2.py:194][0m |          -0.0022 |          12.1679 |           4.3147 |
[32m[20230204 15:45:52 @agent_ppo2.py:194][0m |          -0.0024 |          11.3696 |           4.3120 |
[32m[20230204 15:45:52 @agent_ppo2.py:194][0m |          -0.0070 |          10.8177 |           4.3143 |
[32m[20230204 15:45:52 @agent_ppo2.py:194][0m |          -0.0073 |          10.4662 |           4.3095 |
[32m[20230204 15:45:52 @agent_ppo2.py:194][0m |          -0.0079 |          10.1437 |           4.3128 |
[32m[20230204 15:45:52 @agent_ppo2.py:194][0m |          -0.0092 |           9.9405 |           4.3096 |
[32m[20230204 15:45:52 @agent_ppo2.py:194][0m |          -0.0081 |           9.8433 |           4.3098 |
[32m[20230204 15:45:52 @agent_ppo2.py:194][0m |          -0.0109 |           9.6076 |           4.3105 |
[32m[20230204 15:45:52 @agent_ppo2.py:194][0m |          -0.0090 |           9.4912 |           4.3098 |
[32m[20230204 15:45:52 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:45:52 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.68
[32m[20230204 15:45:52 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.30
[32m[20230204 15:45:52 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.17
[32m[20230204 15:45:52 @agent_ppo2.py:152][0m Total time:       9.70 min
[32m[20230204 15:45:52 @agent_ppo2.py:154][0m 776192 total steps have happened
[32m[20230204 15:45:52 @agent_ppo2.py:130][0m #------------------------ Iteration 379 --------------------------#
[32m[20230204 15:45:53 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:45:53 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:53 @agent_ppo2.py:194][0m |           0.0021 |          13.8756 |           4.4055 |
[32m[20230204 15:45:53 @agent_ppo2.py:194][0m |          -0.0027 |          12.8012 |           4.4030 |
[32m[20230204 15:45:53 @agent_ppo2.py:194][0m |          -0.0045 |          12.0707 |           4.3990 |
[32m[20230204 15:45:53 @agent_ppo2.py:194][0m |          -0.0058 |          11.5782 |           4.3996 |
[32m[20230204 15:45:53 @agent_ppo2.py:194][0m |          -0.0072 |          11.2530 |           4.3953 |
[32m[20230204 15:45:53 @agent_ppo2.py:194][0m |          -0.0076 |          10.8889 |           4.3941 |
[32m[20230204 15:45:53 @agent_ppo2.py:194][0m |          -0.0082 |          10.6044 |           4.3981 |
[32m[20230204 15:45:53 @agent_ppo2.py:194][0m |          -0.0088 |          10.2880 |           4.3948 |
[32m[20230204 15:45:53 @agent_ppo2.py:194][0m |          -0.0086 |           9.9399 |           4.3948 |
[32m[20230204 15:45:53 @agent_ppo2.py:194][0m |          -0.0087 |           9.6480 |           4.3968 |
[32m[20230204 15:45:53 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:45:54 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.35
[32m[20230204 15:45:54 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.07
[32m[20230204 15:45:54 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 263.63
[32m[20230204 15:45:54 @agent_ppo2.py:152][0m Total time:       9.72 min
[32m[20230204 15:45:54 @agent_ppo2.py:154][0m 778240 total steps have happened
[32m[20230204 15:45:54 @agent_ppo2.py:130][0m #------------------------ Iteration 380 --------------------------#
[32m[20230204 15:45:54 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:45:54 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:54 @agent_ppo2.py:194][0m |          -0.0016 |          23.5087 |           4.2747 |
[32m[20230204 15:45:54 @agent_ppo2.py:194][0m |          -0.0065 |          16.0078 |           4.2690 |
[32m[20230204 15:45:54 @agent_ppo2.py:194][0m |          -0.0085 |          14.3488 |           4.2652 |
[32m[20230204 15:45:54 @agent_ppo2.py:194][0m |          -0.0108 |          13.3467 |           4.2646 |
[32m[20230204 15:45:54 @agent_ppo2.py:194][0m |          -0.0099 |          12.8722 |           4.2598 |
[32m[20230204 15:45:55 @agent_ppo2.py:194][0m |          -0.0112 |          12.1977 |           4.2558 |
[32m[20230204 15:45:55 @agent_ppo2.py:194][0m |          -0.0136 |          11.6965 |           4.2570 |
[32m[20230204 15:45:55 @agent_ppo2.py:194][0m |          -0.0126 |          11.3664 |           4.2526 |
[32m[20230204 15:45:55 @agent_ppo2.py:194][0m |          -0.0136 |          10.9875 |           4.2540 |
[32m[20230204 15:45:55 @agent_ppo2.py:194][0m |          -0.0139 |          10.7275 |           4.2511 |
[32m[20230204 15:45:55 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:45:55 @agent_ppo2.py:147][0m Average TRAINING episode reward: 202.97
[32m[20230204 15:45:55 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.26
[32m[20230204 15:45:55 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.85
[32m[20230204 15:45:55 @agent_ppo2.py:152][0m Total time:       9.74 min
[32m[20230204 15:45:55 @agent_ppo2.py:154][0m 780288 total steps have happened
[32m[20230204 15:45:55 @agent_ppo2.py:130][0m #------------------------ Iteration 381 --------------------------#
[32m[20230204 15:45:55 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:55 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:55 @agent_ppo2.py:194][0m |           0.0011 |          15.1166 |           4.3819 |
[32m[20230204 15:45:56 @agent_ppo2.py:194][0m |          -0.0052 |          13.4219 |           4.3730 |
[32m[20230204 15:45:56 @agent_ppo2.py:194][0m |          -0.0071 |          12.7495 |           4.3712 |
[32m[20230204 15:45:56 @agent_ppo2.py:194][0m |          -0.0088 |          12.2320 |           4.3683 |
[32m[20230204 15:45:56 @agent_ppo2.py:194][0m |          -0.0097 |          11.6035 |           4.3676 |
[32m[20230204 15:45:56 @agent_ppo2.py:194][0m |          -0.0097 |          10.9915 |           4.3722 |
[32m[20230204 15:45:56 @agent_ppo2.py:194][0m |          -0.0109 |          10.3025 |           4.3727 |
[32m[20230204 15:45:56 @agent_ppo2.py:194][0m |          -0.0113 |           9.8454 |           4.3708 |
[32m[20230204 15:45:56 @agent_ppo2.py:194][0m |          -0.0120 |           9.3898 |           4.3729 |
[32m[20230204 15:45:56 @agent_ppo2.py:194][0m |          -0.0120 |           9.1161 |           4.3720 |
[32m[20230204 15:45:56 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:45:56 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.57
[32m[20230204 15:45:56 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.21
[32m[20230204 15:45:56 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.17
[32m[20230204 15:45:56 @agent_ppo2.py:152][0m Total time:       9.77 min
[32m[20230204 15:45:56 @agent_ppo2.py:154][0m 782336 total steps have happened
[32m[20230204 15:45:56 @agent_ppo2.py:130][0m #------------------------ Iteration 382 --------------------------#
[32m[20230204 15:45:57 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:45:57 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:57 @agent_ppo2.py:194][0m |          -0.0003 |          15.1296 |           4.3469 |
[32m[20230204 15:45:57 @agent_ppo2.py:194][0m |          -0.0040 |          13.6139 |           4.3308 |
[32m[20230204 15:45:57 @agent_ppo2.py:194][0m |          -0.0061 |          12.7172 |           4.3300 |
[32m[20230204 15:45:57 @agent_ppo2.py:194][0m |          -0.0074 |          12.1758 |           4.3286 |
[32m[20230204 15:45:57 @agent_ppo2.py:194][0m |          -0.0076 |          11.5889 |           4.3285 |
[32m[20230204 15:45:57 @agent_ppo2.py:194][0m |          -0.0083 |          11.2658 |           4.3261 |
[32m[20230204 15:45:57 @agent_ppo2.py:194][0m |          -0.0088 |          11.0236 |           4.3275 |
[32m[20230204 15:45:57 @agent_ppo2.py:194][0m |          -0.0100 |          10.7328 |           4.3212 |
[32m[20230204 15:45:57 @agent_ppo2.py:194][0m |          -0.0111 |          10.5364 |           4.3252 |
[32m[20230204 15:45:57 @agent_ppo2.py:194][0m |          -0.0108 |          10.3230 |           4.3261 |
[32m[20230204 15:45:57 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:45:58 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.99
[32m[20230204 15:45:58 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.43
[32m[20230204 15:45:58 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.39
[32m[20230204 15:45:58 @agent_ppo2.py:152][0m Total time:       9.79 min
[32m[20230204 15:45:58 @agent_ppo2.py:154][0m 784384 total steps have happened
[32m[20230204 15:45:58 @agent_ppo2.py:130][0m #------------------------ Iteration 383 --------------------------#
[32m[20230204 15:45:58 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:45:58 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:58 @agent_ppo2.py:194][0m |          -0.0002 |          14.7910 |           4.3800 |
[32m[20230204 15:45:58 @agent_ppo2.py:194][0m |          -0.0058 |          12.9732 |           4.3716 |
[32m[20230204 15:45:58 @agent_ppo2.py:194][0m |          -0.0071 |          12.4725 |           4.3691 |
[32m[20230204 15:45:58 @agent_ppo2.py:194][0m |          -0.0061 |          12.2316 |           4.3672 |
[32m[20230204 15:45:58 @agent_ppo2.py:194][0m |          -0.0109 |          11.8941 |           4.3687 |
[32m[20230204 15:45:59 @agent_ppo2.py:194][0m |          -0.0146 |          11.7164 |           4.3672 |
[32m[20230204 15:45:59 @agent_ppo2.py:194][0m |          -0.0104 |          11.5097 |           4.3664 |
[32m[20230204 15:45:59 @agent_ppo2.py:194][0m |          -0.0118 |          11.2952 |           4.3700 |
[32m[20230204 15:45:59 @agent_ppo2.py:194][0m |          -0.0100 |          11.1792 |           4.3708 |
[32m[20230204 15:45:59 @agent_ppo2.py:194][0m |          -0.0132 |          10.9704 |           4.3720 |
[32m[20230204 15:45:59 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:45:59 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.96
[32m[20230204 15:45:59 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.20
[32m[20230204 15:45:59 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 263.22
[32m[20230204 15:45:59 @agent_ppo2.py:152][0m Total time:       9.81 min
[32m[20230204 15:45:59 @agent_ppo2.py:154][0m 786432 total steps have happened
[32m[20230204 15:45:59 @agent_ppo2.py:130][0m #------------------------ Iteration 384 --------------------------#
[32m[20230204 15:45:59 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:45:59 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:45:59 @agent_ppo2.py:194][0m |          -0.0010 |          30.2470 |           4.3730 |
[32m[20230204 15:45:59 @agent_ppo2.py:194][0m |          -0.0084 |          23.2934 |           4.3603 |
[32m[20230204 15:46:00 @agent_ppo2.py:194][0m |          -0.0096 |          21.7066 |           4.3549 |
[32m[20230204 15:46:00 @agent_ppo2.py:194][0m |          -0.0122 |          20.1675 |           4.3557 |
[32m[20230204 15:46:00 @agent_ppo2.py:194][0m |          -0.0119 |          19.1996 |           4.3519 |
[32m[20230204 15:46:00 @agent_ppo2.py:194][0m |          -0.0114 |          19.0145 |           4.3542 |
[32m[20230204 15:46:00 @agent_ppo2.py:194][0m |          -0.0137 |          17.8354 |           4.3493 |
[32m[20230204 15:46:00 @agent_ppo2.py:194][0m |          -0.0149 |          17.4138 |           4.3537 |
[32m[20230204 15:46:00 @agent_ppo2.py:194][0m |          -0.0149 |          16.8016 |           4.3543 |
[32m[20230204 15:46:00 @agent_ppo2.py:194][0m |          -0.0155 |          16.6148 |           4.3533 |
[32m[20230204 15:46:00 @agent_ppo2.py:139][0m Policy update time: 0.77 s
[32m[20230204 15:46:00 @agent_ppo2.py:147][0m Average TRAINING episode reward: 201.49
[32m[20230204 15:46:00 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.08
[32m[20230204 15:46:00 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.15
[32m[20230204 15:46:00 @agent_ppo2.py:152][0m Total time:       9.83 min
[32m[20230204 15:46:00 @agent_ppo2.py:154][0m 788480 total steps have happened
[32m[20230204 15:46:00 @agent_ppo2.py:130][0m #------------------------ Iteration 385 --------------------------#
[32m[20230204 15:46:00 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:01 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:01 @agent_ppo2.py:194][0m |           0.0002 |          16.6676 |           4.3149 |
[32m[20230204 15:46:01 @agent_ppo2.py:194][0m |          -0.0050 |          14.0121 |           4.3087 |
[32m[20230204 15:46:01 @agent_ppo2.py:194][0m |          -0.0060 |          12.7698 |           4.3037 |
[32m[20230204 15:46:01 @agent_ppo2.py:194][0m |          -0.0085 |          12.1029 |           4.3062 |
[32m[20230204 15:46:01 @agent_ppo2.py:194][0m |          -0.0096 |          11.4923 |           4.3090 |
[32m[20230204 15:46:01 @agent_ppo2.py:194][0m |          -0.0090 |          11.1850 |           4.3027 |
[32m[20230204 15:46:01 @agent_ppo2.py:194][0m |          -0.0105 |          10.7853 |           4.3043 |
[32m[20230204 15:46:01 @agent_ppo2.py:194][0m |          -0.0111 |          10.4886 |           4.3021 |
[32m[20230204 15:46:01 @agent_ppo2.py:194][0m |          -0.0118 |          10.2161 |           4.3013 |
[32m[20230204 15:46:01 @agent_ppo2.py:194][0m |          -0.0122 |           9.9622 |           4.3022 |
[32m[20230204 15:46:01 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:46:02 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.47
[32m[20230204 15:46:02 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.63
[32m[20230204 15:46:02 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.67
[32m[20230204 15:46:02 @agent_ppo2.py:152][0m Total time:       9.85 min
[32m[20230204 15:46:02 @agent_ppo2.py:154][0m 790528 total steps have happened
[32m[20230204 15:46:02 @agent_ppo2.py:130][0m #------------------------ Iteration 386 --------------------------#
[32m[20230204 15:46:02 @agent_ppo2.py:136][0m Sampling time: 0.27 s by 4 slaves
[32m[20230204 15:46:02 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:02 @agent_ppo2.py:194][0m |           0.0023 |          21.5860 |           4.3704 |
[32m[20230204 15:46:02 @agent_ppo2.py:194][0m |          -0.0035 |          14.8574 |           4.3616 |
[32m[20230204 15:46:02 @agent_ppo2.py:194][0m |          -0.0078 |          13.9025 |           4.3547 |
[32m[20230204 15:46:02 @agent_ppo2.py:194][0m |          -0.0060 |          13.4301 |           4.3535 |
[32m[20230204 15:46:02 @agent_ppo2.py:194][0m |          -0.0084 |          13.1926 |           4.3528 |
[32m[20230204 15:46:02 @agent_ppo2.py:194][0m |          -0.0098 |          12.9497 |           4.3522 |
[32m[20230204 15:46:02 @agent_ppo2.py:194][0m |          -0.0083 |          12.8571 |           4.3513 |
[32m[20230204 15:46:03 @agent_ppo2.py:194][0m |          -0.0100 |          12.6899 |           4.3526 |
[32m[20230204 15:46:03 @agent_ppo2.py:194][0m |          -0.0118 |          12.5876 |           4.3518 |
[32m[20230204 15:46:03 @agent_ppo2.py:194][0m |          -0.0119 |          12.4300 |           4.3552 |
[32m[20230204 15:46:03 @agent_ppo2.py:139][0m Policy update time: 0.82 s
[32m[20230204 15:46:03 @agent_ppo2.py:147][0m Average TRAINING episode reward: 230.82
[32m[20230204 15:46:03 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.96
[32m[20230204 15:46:03 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.25
[32m[20230204 15:46:03 @agent_ppo2.py:152][0m Total time:       9.87 min
[32m[20230204 15:46:03 @agent_ppo2.py:154][0m 792576 total steps have happened
[32m[20230204 15:46:03 @agent_ppo2.py:130][0m #------------------------ Iteration 387 --------------------------#
[32m[20230204 15:46:03 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:03 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:03 @agent_ppo2.py:194][0m |           0.0001 |          13.9131 |           4.3267 |
[32m[20230204 15:46:03 @agent_ppo2.py:194][0m |          -0.0079 |          11.5809 |           4.3188 |
[32m[20230204 15:46:03 @agent_ppo2.py:194][0m |          -0.0030 |          10.8914 |           4.3155 |
[32m[20230204 15:46:03 @agent_ppo2.py:194][0m |          -0.0045 |           9.9212 |           4.3114 |
[32m[20230204 15:46:04 @agent_ppo2.py:194][0m |          -0.0103 |           9.2526 |           4.3103 |
[32m[20230204 15:46:04 @agent_ppo2.py:194][0m |          -0.0139 |           8.8072 |           4.3079 |
[32m[20230204 15:46:04 @agent_ppo2.py:194][0m |          -0.0127 |           8.4477 |           4.3097 |
[32m[20230204 15:46:04 @agent_ppo2.py:194][0m |          -0.0092 |           8.2214 |           4.3092 |
[32m[20230204 15:46:04 @agent_ppo2.py:194][0m |          -0.0090 |           8.4669 |           4.3127 |
[32m[20230204 15:46:04 @agent_ppo2.py:194][0m |          -0.0121 |           7.6686 |           4.3021 |
[32m[20230204 15:46:04 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:46:04 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.72
[32m[20230204 15:46:04 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.82
[32m[20230204 15:46:04 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.75
[32m[20230204 15:46:04 @agent_ppo2.py:152][0m Total time:       9.90 min
[32m[20230204 15:46:04 @agent_ppo2.py:154][0m 794624 total steps have happened
[32m[20230204 15:46:04 @agent_ppo2.py:130][0m #------------------------ Iteration 388 --------------------------#
[32m[20230204 15:46:04 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:46:04 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:05 @agent_ppo2.py:194][0m |           0.0001 |          14.0647 |           4.3627 |
[32m[20230204 15:46:05 @agent_ppo2.py:194][0m |          -0.0057 |          12.1860 |           4.3585 |
[32m[20230204 15:46:05 @agent_ppo2.py:194][0m |          -0.0079 |          11.3190 |           4.3575 |
[32m[20230204 15:46:05 @agent_ppo2.py:194][0m |          -0.0079 |          10.4053 |           4.3523 |
[32m[20230204 15:46:05 @agent_ppo2.py:194][0m |          -0.0121 |           9.5608 |           4.3517 |
[32m[20230204 15:46:05 @agent_ppo2.py:194][0m |          -0.0115 |           8.7343 |           4.3513 |
[32m[20230204 15:46:05 @agent_ppo2.py:194][0m |          -0.0125 |           8.0467 |           4.3503 |
[32m[20230204 15:46:05 @agent_ppo2.py:194][0m |          -0.0087 |           7.7628 |           4.3475 |
[32m[20230204 15:46:05 @agent_ppo2.py:194][0m |          -0.0128 |           7.2079 |           4.3453 |
[32m[20230204 15:46:05 @agent_ppo2.py:194][0m |          -0.0107 |           7.1058 |           4.3465 |
[32m[20230204 15:46:05 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:46:06 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.26
[32m[20230204 15:46:06 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.59
[32m[20230204 15:46:06 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.03
[32m[20230204 15:46:06 @agent_ppo2.py:152][0m Total time:       9.92 min
[32m[20230204 15:46:06 @agent_ppo2.py:154][0m 796672 total steps have happened
[32m[20230204 15:46:06 @agent_ppo2.py:130][0m #------------------------ Iteration 389 --------------------------#
[32m[20230204 15:46:06 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:06 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:06 @agent_ppo2.py:194][0m |          -0.0021 |          15.0067 |           4.3332 |
[32m[20230204 15:46:06 @agent_ppo2.py:194][0m |          -0.0072 |          13.2991 |           4.3361 |
[32m[20230204 15:46:06 @agent_ppo2.py:194][0m |          -0.0097 |          12.4691 |           4.3334 |
[32m[20230204 15:46:06 @agent_ppo2.py:194][0m |          -0.0101 |          11.9767 |           4.3314 |
[32m[20230204 15:46:06 @agent_ppo2.py:194][0m |          -0.0124 |          11.6507 |           4.3333 |
[32m[20230204 15:46:06 @agent_ppo2.py:194][0m |          -0.0127 |          11.3202 |           4.3338 |
[32m[20230204 15:46:06 @agent_ppo2.py:194][0m |          -0.0136 |          11.1007 |           4.3347 |
[32m[20230204 15:46:06 @agent_ppo2.py:194][0m |          -0.0118 |          11.1051 |           4.3373 |
[32m[20230204 15:46:07 @agent_ppo2.py:194][0m |          -0.0137 |          10.5651 |           4.3324 |
[32m[20230204 15:46:07 @agent_ppo2.py:194][0m |          -0.0146 |          10.2682 |           4.3327 |
[32m[20230204 15:46:07 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:46:07 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.36
[32m[20230204 15:46:07 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.26
[32m[20230204 15:46:07 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 96.73
[32m[20230204 15:46:07 @agent_ppo2.py:152][0m Total time:       9.94 min
[32m[20230204 15:46:07 @agent_ppo2.py:154][0m 798720 total steps have happened
[32m[20230204 15:46:07 @agent_ppo2.py:130][0m #------------------------ Iteration 390 --------------------------#
[32m[20230204 15:46:07 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:07 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:07 @agent_ppo2.py:194][0m |           0.0034 |          28.3170 |           4.2956 |
[32m[20230204 15:46:07 @agent_ppo2.py:194][0m |          -0.0044 |          23.2774 |           4.2832 |
[32m[20230204 15:46:07 @agent_ppo2.py:194][0m |          -0.0020 |          21.9026 |           4.2788 |
[32m[20230204 15:46:07 @agent_ppo2.py:194][0m |          -0.0091 |          19.9745 |           4.2805 |
[32m[20230204 15:46:08 @agent_ppo2.py:194][0m |          -0.0122 |          18.9762 |           4.2780 |
[32m[20230204 15:46:08 @agent_ppo2.py:194][0m |          -0.0126 |          18.2916 |           4.2801 |
[32m[20230204 15:46:08 @agent_ppo2.py:194][0m |          -0.0131 |          17.6114 |           4.2776 |
[32m[20230204 15:46:08 @agent_ppo2.py:194][0m |          -0.0141 |          17.1729 |           4.2748 |
[32m[20230204 15:46:08 @agent_ppo2.py:194][0m |          -0.0151 |          16.3222 |           4.2792 |
[32m[20230204 15:46:08 @agent_ppo2.py:194][0m |          -0.0125 |          15.8794 |           4.2774 |
[32m[20230204 15:46:08 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:46:08 @agent_ppo2.py:147][0m Average TRAINING episode reward: 212.15
[32m[20230204 15:46:08 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 257.72
[32m[20230204 15:46:08 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.12
[32m[20230204 15:46:08 @agent_ppo2.py:152][0m Total time:       9.96 min
[32m[20230204 15:46:08 @agent_ppo2.py:154][0m 800768 total steps have happened
[32m[20230204 15:46:08 @agent_ppo2.py:130][0m #------------------------ Iteration 391 --------------------------#
[32m[20230204 15:46:08 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:08 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:09 @agent_ppo2.py:194][0m |          -0.0009 |          14.9315 |           4.3736 |
[32m[20230204 15:46:09 @agent_ppo2.py:194][0m |          -0.0079 |          13.3121 |           4.3646 |
[32m[20230204 15:46:09 @agent_ppo2.py:194][0m |          -0.0094 |          12.7359 |           4.3672 |
[32m[20230204 15:46:09 @agent_ppo2.py:194][0m |          -0.0111 |          12.3674 |           4.3664 |
[32m[20230204 15:46:09 @agent_ppo2.py:194][0m |          -0.0108 |          12.0386 |           4.3638 |
[32m[20230204 15:46:09 @agent_ppo2.py:194][0m |          -0.0124 |          11.8131 |           4.3641 |
[32m[20230204 15:46:09 @agent_ppo2.py:194][0m |          -0.0127 |          11.6093 |           4.3600 |
[32m[20230204 15:46:09 @agent_ppo2.py:194][0m |          -0.0133 |          11.4801 |           4.3638 |
[32m[20230204 15:46:09 @agent_ppo2.py:194][0m |          -0.0147 |          11.3169 |           4.3657 |
[32m[20230204 15:46:09 @agent_ppo2.py:194][0m |          -0.0140 |          11.3377 |           4.3655 |
[32m[20230204 15:46:09 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:46:09 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.63
[32m[20230204 15:46:09 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.27
[32m[20230204 15:46:09 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.20
[32m[20230204 15:46:09 @agent_ppo2.py:152][0m Total time:       9.98 min
[32m[20230204 15:46:09 @agent_ppo2.py:154][0m 802816 total steps have happened
[32m[20230204 15:46:09 @agent_ppo2.py:130][0m #------------------------ Iteration 392 --------------------------#
[32m[20230204 15:46:10 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:10 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:10 @agent_ppo2.py:194][0m |          -0.0024 |          15.1781 |           4.3813 |
[32m[20230204 15:46:10 @agent_ppo2.py:194][0m |          -0.0091 |          13.8624 |           4.3677 |
[32m[20230204 15:46:10 @agent_ppo2.py:194][0m |          -0.0096 |          13.3743 |           4.3668 |
[32m[20230204 15:46:10 @agent_ppo2.py:194][0m |          -0.0097 |          13.0385 |           4.3657 |
[32m[20230204 15:46:10 @agent_ppo2.py:194][0m |          -0.0106 |          12.8757 |           4.3679 |
[32m[20230204 15:46:10 @agent_ppo2.py:194][0m |          -0.0115 |          12.6537 |           4.3661 |
[32m[20230204 15:46:10 @agent_ppo2.py:194][0m |          -0.0131 |          12.4570 |           4.3686 |
[32m[20230204 15:46:10 @agent_ppo2.py:194][0m |          -0.0124 |          12.3423 |           4.3649 |
[32m[20230204 15:46:10 @agent_ppo2.py:194][0m |          -0.0129 |          12.1777 |           4.3667 |
[32m[20230204 15:46:11 @agent_ppo2.py:194][0m |          -0.0132 |          12.1326 |           4.3671 |
[32m[20230204 15:46:11 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:46:11 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.62
[32m[20230204 15:46:11 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.49
[32m[20230204 15:46:11 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.07
[32m[20230204 15:46:11 @agent_ppo2.py:152][0m Total time:      10.01 min
[32m[20230204 15:46:11 @agent_ppo2.py:154][0m 804864 total steps have happened
[32m[20230204 15:46:11 @agent_ppo2.py:130][0m #------------------------ Iteration 393 --------------------------#
[32m[20230204 15:46:11 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:11 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:11 @agent_ppo2.py:194][0m |           0.0020 |          13.2307 |           4.3703 |
[32m[20230204 15:46:11 @agent_ppo2.py:194][0m |          -0.0064 |          12.1407 |           4.3656 |
[32m[20230204 15:46:11 @agent_ppo2.py:194][0m |          -0.0082 |          11.6640 |           4.3714 |
[32m[20230204 15:46:11 @agent_ppo2.py:194][0m |          -0.0083 |          11.1032 |           4.3662 |
[32m[20230204 15:46:11 @agent_ppo2.py:194][0m |          -0.0089 |          10.5452 |           4.3671 |
[32m[20230204 15:46:12 @agent_ppo2.py:194][0m |          -0.0102 |          10.1078 |           4.3670 |
[32m[20230204 15:46:12 @agent_ppo2.py:194][0m |          -0.0101 |           9.7845 |           4.3648 |
[32m[20230204 15:46:12 @agent_ppo2.py:194][0m |          -0.0110 |           9.5631 |           4.3624 |
[32m[20230204 15:46:12 @agent_ppo2.py:194][0m |          -0.0115 |           9.3176 |           4.3647 |
[32m[20230204 15:46:12 @agent_ppo2.py:194][0m |          -0.0087 |           9.1886 |           4.3677 |
[32m[20230204 15:46:12 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:46:12 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.97
[32m[20230204 15:46:12 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.97
[32m[20230204 15:46:12 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.83
[32m[20230204 15:46:12 @agent_ppo2.py:152][0m Total time:      10.03 min
[32m[20230204 15:46:12 @agent_ppo2.py:154][0m 806912 total steps have happened
[32m[20230204 15:46:12 @agent_ppo2.py:130][0m #------------------------ Iteration 394 --------------------------#
[32m[20230204 15:46:12 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:12 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:12 @agent_ppo2.py:194][0m |           0.0021 |          15.8131 |           4.4534 |
[32m[20230204 15:46:13 @agent_ppo2.py:194][0m |          -0.0009 |          14.4466 |           4.4515 |
[32m[20230204 15:46:13 @agent_ppo2.py:194][0m |          -0.0068 |          13.3822 |           4.4542 |
[32m[20230204 15:46:13 @agent_ppo2.py:194][0m |          -0.0073 |          12.9316 |           4.4517 |
[32m[20230204 15:46:13 @agent_ppo2.py:194][0m |          -0.0085 |          12.6352 |           4.4566 |
[32m[20230204 15:46:13 @agent_ppo2.py:194][0m |          -0.0112 |          12.2497 |           4.4509 |
[32m[20230204 15:46:13 @agent_ppo2.py:194][0m |          -0.0112 |          12.0445 |           4.4557 |
[32m[20230204 15:46:13 @agent_ppo2.py:194][0m |          -0.0124 |          11.8452 |           4.4591 |
[32m[20230204 15:46:13 @agent_ppo2.py:194][0m |          -0.0123 |          11.6739 |           4.4597 |
[32m[20230204 15:46:13 @agent_ppo2.py:194][0m |          -0.0137 |          11.5697 |           4.4599 |
[32m[20230204 15:46:13 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:46:13 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.74
[32m[20230204 15:46:13 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.78
[32m[20230204 15:46:13 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.40
[32m[20230204 15:46:13 @agent_ppo2.py:152][0m Total time:      10.05 min
[32m[20230204 15:46:13 @agent_ppo2.py:154][0m 808960 total steps have happened
[32m[20230204 15:46:13 @agent_ppo2.py:130][0m #------------------------ Iteration 395 --------------------------#
[32m[20230204 15:46:14 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:14 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:14 @agent_ppo2.py:194][0m |           0.0016 |          14.8484 |           4.5608 |
[32m[20230204 15:46:14 @agent_ppo2.py:194][0m |          -0.0038 |          13.2613 |           4.5487 |
[32m[20230204 15:46:14 @agent_ppo2.py:194][0m |          -0.0045 |          12.7655 |           4.5396 |
[32m[20230204 15:46:14 @agent_ppo2.py:194][0m |          -0.0058 |          12.3099 |           4.5441 |
[32m[20230204 15:46:14 @agent_ppo2.py:194][0m |          -0.0071 |          11.7822 |           4.5419 |
[32m[20230204 15:46:14 @agent_ppo2.py:194][0m |          -0.0077 |          11.4306 |           4.5407 |
[32m[20230204 15:46:14 @agent_ppo2.py:194][0m |          -0.0087 |          11.2641 |           4.5389 |
[32m[20230204 15:46:14 @agent_ppo2.py:194][0m |          -0.0081 |          11.0856 |           4.5395 |
[32m[20230204 15:46:14 @agent_ppo2.py:194][0m |          -0.0100 |          10.9143 |           4.5386 |
[32m[20230204 15:46:15 @agent_ppo2.py:194][0m |          -0.0080 |          10.8160 |           4.5344 |
[32m[20230204 15:46:15 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:46:15 @agent_ppo2.py:147][0m Average TRAINING episode reward: 263.04
[32m[20230204 15:46:15 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 266.20
[32m[20230204 15:46:15 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.12
[32m[20230204 15:46:15 @agent_ppo2.py:152][0m Total time:      10.07 min
[32m[20230204 15:46:15 @agent_ppo2.py:154][0m 811008 total steps have happened
[32m[20230204 15:46:15 @agent_ppo2.py:130][0m #------------------------ Iteration 396 --------------------------#
[32m[20230204 15:46:15 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:15 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:15 @agent_ppo2.py:194][0m |          -0.0030 |          20.6236 |           4.5236 |
[32m[20230204 15:46:15 @agent_ppo2.py:194][0m |          -0.0107 |          14.5950 |           4.5154 |
[32m[20230204 15:46:15 @agent_ppo2.py:194][0m |          -0.0097 |          13.2803 |           4.5185 |
[32m[20230204 15:46:15 @agent_ppo2.py:194][0m |          -0.0036 |          12.9996 |           4.5164 |
[32m[20230204 15:46:15 @agent_ppo2.py:194][0m |          -0.0137 |          12.3702 |           4.5170 |
[32m[20230204 15:46:16 @agent_ppo2.py:194][0m |          -0.0095 |          12.0184 |           4.5201 |
[32m[20230204 15:46:16 @agent_ppo2.py:194][0m |          -0.0129 |          11.8020 |           4.5171 |
[32m[20230204 15:46:16 @agent_ppo2.py:194][0m |          -0.0136 |          11.5793 |           4.5190 |
[32m[20230204 15:46:16 @agent_ppo2.py:194][0m |          -0.0165 |          11.3907 |           4.5222 |
[32m[20230204 15:46:16 @agent_ppo2.py:194][0m |          -0.0154 |          11.2002 |           4.5152 |
[32m[20230204 15:46:16 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:46:16 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.55
[32m[20230204 15:46:16 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.89
[32m[20230204 15:46:16 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.78
[32m[20230204 15:46:16 @agent_ppo2.py:152][0m Total time:      10.09 min
[32m[20230204 15:46:16 @agent_ppo2.py:154][0m 813056 total steps have happened
[32m[20230204 15:46:16 @agent_ppo2.py:130][0m #------------------------ Iteration 397 --------------------------#
[32m[20230204 15:46:16 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:46:16 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:16 @agent_ppo2.py:194][0m |           0.0001 |          32.6525 |           4.4770 |
[32m[20230204 15:46:17 @agent_ppo2.py:194][0m |          -0.0041 |          23.0347 |           4.4716 |
[32m[20230204 15:46:17 @agent_ppo2.py:194][0m |          -0.0065 |          21.6963 |           4.4641 |
[32m[20230204 15:46:17 @agent_ppo2.py:194][0m |          -0.0134 |          19.2155 |           4.4618 |
[32m[20230204 15:46:17 @agent_ppo2.py:194][0m |          -0.0123 |          17.9101 |           4.4626 |
[32m[20230204 15:46:17 @agent_ppo2.py:194][0m |          -0.0138 |          16.7828 |           4.4634 |
[32m[20230204 15:46:17 @agent_ppo2.py:194][0m |          -0.0150 |          15.7785 |           4.4614 |
[32m[20230204 15:46:17 @agent_ppo2.py:194][0m |          -0.0115 |          15.1503 |           4.4605 |
[32m[20230204 15:46:17 @agent_ppo2.py:194][0m |          -0.0154 |          14.6477 |           4.4660 |
[32m[20230204 15:46:17 @agent_ppo2.py:194][0m |          -0.0190 |          14.0787 |           4.4628 |
[32m[20230204 15:46:17 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:46:17 @agent_ppo2.py:147][0m Average TRAINING episode reward: 165.86
[32m[20230204 15:46:17 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.67
[32m[20230204 15:46:17 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.76
[32m[20230204 15:46:17 @agent_ppo2.py:152][0m Total time:      10.12 min
[32m[20230204 15:46:17 @agent_ppo2.py:154][0m 815104 total steps have happened
[32m[20230204 15:46:17 @agent_ppo2.py:130][0m #------------------------ Iteration 398 --------------------------#
[32m[20230204 15:46:18 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:46:18 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:18 @agent_ppo2.py:194][0m |          -0.0003 |          41.7016 |           4.4774 |
[32m[20230204 15:46:18 @agent_ppo2.py:194][0m |          -0.0015 |          36.4002 |           4.4731 |
[32m[20230204 15:46:18 @agent_ppo2.py:194][0m |          -0.0109 |          32.0781 |           4.4620 |
[32m[20230204 15:46:18 @agent_ppo2.py:194][0m |          -0.0017 |          31.2853 |           4.4681 |
[32m[20230204 15:46:18 @agent_ppo2.py:194][0m |          -0.0138 |          29.7885 |           4.4635 |
[32m[20230204 15:46:18 @agent_ppo2.py:194][0m |          -0.0121 |          28.9154 |           4.4639 |
[32m[20230204 15:46:18 @agent_ppo2.py:194][0m |          -0.0157 |          28.7168 |           4.4648 |
[32m[20230204 15:46:18 @agent_ppo2.py:194][0m |          -0.0151 |          27.1787 |           4.4620 |
[32m[20230204 15:46:19 @agent_ppo2.py:194][0m |          -0.0163 |          26.5935 |           4.4641 |
[32m[20230204 15:46:19 @agent_ppo2.py:194][0m |          -0.0111 |          27.3682 |           4.4643 |
[32m[20230204 15:46:19 @agent_ppo2.py:139][0m Policy update time: 0.94 s
[32m[20230204 15:46:19 @agent_ppo2.py:147][0m Average TRAINING episode reward: 181.78
[32m[20230204 15:46:19 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.73
[32m[20230204 15:46:19 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 274.66
[32m[20230204 15:46:19 @agent_ppo2.py:152][0m Total time:      10.14 min
[32m[20230204 15:46:19 @agent_ppo2.py:154][0m 817152 total steps have happened
[32m[20230204 15:46:19 @agent_ppo2.py:130][0m #------------------------ Iteration 399 --------------------------#
[32m[20230204 15:46:19 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:19 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:19 @agent_ppo2.py:194][0m |          -0.0025 |          17.2232 |           4.4832 |
[32m[20230204 15:46:19 @agent_ppo2.py:194][0m |          -0.0068 |          14.1404 |           4.4805 |
[32m[20230204 15:46:19 @agent_ppo2.py:194][0m |           0.0008 |          14.4381 |           4.4814 |
[32m[20230204 15:46:19 @agent_ppo2.py:194][0m |          -0.0093 |          13.1636 |           4.4788 |
[32m[20230204 15:46:20 @agent_ppo2.py:194][0m |          -0.0125 |          12.8589 |           4.4807 |
[32m[20230204 15:46:20 @agent_ppo2.py:194][0m |          -0.0049 |          13.2838 |           4.4778 |
[32m[20230204 15:46:20 @agent_ppo2.py:194][0m |          -0.0125 |          12.4152 |           4.4764 |
[32m[20230204 15:46:20 @agent_ppo2.py:194][0m |          -0.0134 |          12.2025 |           4.4744 |
[32m[20230204 15:46:20 @agent_ppo2.py:194][0m |          -0.0162 |          12.0758 |           4.4728 |
[32m[20230204 15:46:20 @agent_ppo2.py:194][0m |          -0.0132 |          11.8700 |           4.4697 |
[32m[20230204 15:46:20 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:46:20 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.65
[32m[20230204 15:46:20 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.15
[32m[20230204 15:46:20 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.78
[32m[20230204 15:46:20 @agent_ppo2.py:152][0m Total time:      10.16 min
[32m[20230204 15:46:20 @agent_ppo2.py:154][0m 819200 total steps have happened
[32m[20230204 15:46:20 @agent_ppo2.py:130][0m #------------------------ Iteration 400 --------------------------#
[32m[20230204 15:46:20 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:20 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:21 @agent_ppo2.py:194][0m |          -0.0007 |          13.8974 |           4.4665 |
[32m[20230204 15:46:21 @agent_ppo2.py:194][0m |          -0.0054 |          13.1261 |           4.4565 |
[32m[20230204 15:46:21 @agent_ppo2.py:194][0m |          -0.0067 |          12.5159 |           4.4548 |
[32m[20230204 15:46:21 @agent_ppo2.py:194][0m |          -0.0094 |          12.2675 |           4.4493 |
[32m[20230204 15:46:21 @agent_ppo2.py:194][0m |          -0.0096 |          11.9611 |           4.4490 |
[32m[20230204 15:46:21 @agent_ppo2.py:194][0m |          -0.0117 |          11.7314 |           4.4452 |
[32m[20230204 15:46:21 @agent_ppo2.py:194][0m |          -0.0112 |          11.6268 |           4.4461 |
[32m[20230204 15:46:21 @agent_ppo2.py:194][0m |          -0.0119 |          11.4222 |           4.4433 |
[32m[20230204 15:46:21 @agent_ppo2.py:194][0m |          -0.0128 |          11.3192 |           4.4447 |
[32m[20230204 15:46:21 @agent_ppo2.py:194][0m |          -0.0133 |          11.1218 |           4.4409 |
[32m[20230204 15:46:21 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:46:21 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.10
[32m[20230204 15:46:21 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.30
[32m[20230204 15:46:21 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.68
[32m[20230204 15:46:21 @agent_ppo2.py:152][0m Total time:      10.18 min
[32m[20230204 15:46:21 @agent_ppo2.py:154][0m 821248 total steps have happened
[32m[20230204 15:46:21 @agent_ppo2.py:130][0m #------------------------ Iteration 401 --------------------------#
[32m[20230204 15:46:22 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:22 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:22 @agent_ppo2.py:194][0m |          -0.0069 |          13.9383 |           4.4473 |
[32m[20230204 15:46:22 @agent_ppo2.py:194][0m |           0.0086 |          13.3741 |           4.4407 |
[32m[20230204 15:46:22 @agent_ppo2.py:194][0m |          -0.0087 |          11.1653 |           4.4370 |
[32m[20230204 15:46:22 @agent_ppo2.py:194][0m |          -0.0104 |          10.6400 |           4.4339 |
[32m[20230204 15:46:22 @agent_ppo2.py:194][0m |          -0.0099 |          10.2732 |           4.4320 |
[32m[20230204 15:46:22 @agent_ppo2.py:194][0m |          -0.0137 |           9.9885 |           4.4290 |
[32m[20230204 15:46:22 @agent_ppo2.py:194][0m |          -0.0161 |           9.7469 |           4.4282 |
[32m[20230204 15:46:22 @agent_ppo2.py:194][0m |          -0.0196 |           9.5356 |           4.4297 |
[32m[20230204 15:46:22 @agent_ppo2.py:194][0m |          -0.0168 |           9.3076 |           4.4252 |
[32m[20230204 15:46:23 @agent_ppo2.py:194][0m |          -0.0134 |           9.0865 |           4.4265 |
[32m[20230204 15:46:23 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:46:23 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.19
[32m[20230204 15:46:23 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.16
[32m[20230204 15:46:23 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.71
[32m[20230204 15:46:23 @agent_ppo2.py:152][0m Total time:      10.21 min
[32m[20230204 15:46:23 @agent_ppo2.py:154][0m 823296 total steps have happened
[32m[20230204 15:46:23 @agent_ppo2.py:130][0m #------------------------ Iteration 402 --------------------------#
[32m[20230204 15:46:23 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:23 @agent_ppo2.py:194][0m |           0.0020 |          16.5463 |           4.5653 |
[32m[20230204 15:46:23 @agent_ppo2.py:194][0m |          -0.0009 |          15.7380 |           4.5605 |
[32m[20230204 15:46:23 @agent_ppo2.py:194][0m |          -0.0070 |          14.6115 |           4.5516 |
[32m[20230204 15:46:23 @agent_ppo2.py:194][0m |          -0.0070 |          14.2074 |           4.5518 |
[32m[20230204 15:46:23 @agent_ppo2.py:194][0m |          -0.0065 |          13.8317 |           4.5520 |
[32m[20230204 15:46:24 @agent_ppo2.py:194][0m |          -0.0074 |          13.8218 |           4.5508 |
[32m[20230204 15:46:24 @agent_ppo2.py:194][0m |          -0.0109 |          13.3436 |           4.5522 |
[32m[20230204 15:46:24 @agent_ppo2.py:194][0m |          -0.0094 |          13.2252 |           4.5520 |
[32m[20230204 15:46:24 @agent_ppo2.py:194][0m |          -0.0131 |          13.0424 |           4.5533 |
[32m[20230204 15:46:24 @agent_ppo2.py:194][0m |          -0.0110 |          12.9028 |           4.5522 |
[32m[20230204 15:46:24 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:46:24 @agent_ppo2.py:147][0m Average TRAINING episode reward: 263.45
[32m[20230204 15:46:24 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.47
[32m[20230204 15:46:24 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.60
[32m[20230204 15:46:24 @agent_ppo2.py:152][0m Total time:      10.23 min
[32m[20230204 15:46:24 @agent_ppo2.py:154][0m 825344 total steps have happened
[32m[20230204 15:46:24 @agent_ppo2.py:130][0m #------------------------ Iteration 403 --------------------------#
[32m[20230204 15:46:24 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:24 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:24 @agent_ppo2.py:194][0m |          -0.0020 |          15.6596 |           4.3687 |
[32m[20230204 15:46:25 @agent_ppo2.py:194][0m |          -0.0076 |          14.8881 |           4.3616 |
[32m[20230204 15:46:25 @agent_ppo2.py:194][0m |          -0.0098 |          14.3099 |           4.3548 |
[32m[20230204 15:46:25 @agent_ppo2.py:194][0m |          -0.0111 |          13.8161 |           4.3513 |
[32m[20230204 15:46:25 @agent_ppo2.py:194][0m |          -0.0125 |          13.4868 |           4.3540 |
[32m[20230204 15:46:25 @agent_ppo2.py:194][0m |          -0.0137 |          13.2571 |           4.3552 |
[32m[20230204 15:46:25 @agent_ppo2.py:194][0m |          -0.0146 |          13.0003 |           4.3544 |
[32m[20230204 15:46:25 @agent_ppo2.py:194][0m |          -0.0089 |          13.1005 |           4.3503 |
[32m[20230204 15:46:25 @agent_ppo2.py:194][0m |          -0.0165 |          12.6556 |           4.3502 |
[32m[20230204 15:46:25 @agent_ppo2.py:194][0m |          -0.0159 |          12.5053 |           4.3536 |
[32m[20230204 15:46:25 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:46:25 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.98
[32m[20230204 15:46:25 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.72
[32m[20230204 15:46:25 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.19
[32m[20230204 15:46:25 @agent_ppo2.py:152][0m Total time:      10.25 min
[32m[20230204 15:46:25 @agent_ppo2.py:154][0m 827392 total steps have happened
[32m[20230204 15:46:25 @agent_ppo2.py:130][0m #------------------------ Iteration 404 --------------------------#
[32m[20230204 15:46:26 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:26 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:26 @agent_ppo2.py:194][0m |          -0.0015 |          14.7112 |           4.5026 |
[32m[20230204 15:46:26 @agent_ppo2.py:194][0m |          -0.0075 |          11.1040 |           4.5011 |
[32m[20230204 15:46:26 @agent_ppo2.py:194][0m |          -0.0094 |           9.4734 |           4.4955 |
[32m[20230204 15:46:26 @agent_ppo2.py:194][0m |          -0.0105 |           8.5975 |           4.4971 |
[32m[20230204 15:46:26 @agent_ppo2.py:194][0m |          -0.0127 |           7.9153 |           4.4918 |
[32m[20230204 15:46:26 @agent_ppo2.py:194][0m |          -0.0119 |           7.4172 |           4.4966 |
[32m[20230204 15:46:26 @agent_ppo2.py:194][0m |          -0.0118 |           7.0575 |           4.4926 |
[32m[20230204 15:46:26 @agent_ppo2.py:194][0m |          -0.0128 |           6.7121 |           4.4950 |
[32m[20230204 15:46:26 @agent_ppo2.py:194][0m |          -0.0101 |           6.4810 |           4.4945 |
[32m[20230204 15:46:26 @agent_ppo2.py:194][0m |          -0.0116 |           6.4234 |           4.4921 |
[32m[20230204 15:46:26 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:46:27 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.52
[32m[20230204 15:46:27 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.96
[32m[20230204 15:46:27 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.91
[32m[20230204 15:46:27 @agent_ppo2.py:152][0m Total time:      10.27 min
[32m[20230204 15:46:27 @agent_ppo2.py:154][0m 829440 total steps have happened
[32m[20230204 15:46:27 @agent_ppo2.py:130][0m #------------------------ Iteration 405 --------------------------#
[32m[20230204 15:46:27 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:27 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:27 @agent_ppo2.py:194][0m |           0.0008 |          17.8424 |           4.4801 |
[32m[20230204 15:46:27 @agent_ppo2.py:194][0m |          -0.0057 |          16.6202 |           4.4777 |
[32m[20230204 15:46:27 @agent_ppo2.py:194][0m |          -0.0087 |          16.0276 |           4.4800 |
[32m[20230204 15:46:27 @agent_ppo2.py:194][0m |          -0.0082 |          15.6850 |           4.4763 |
[32m[20230204 15:46:27 @agent_ppo2.py:194][0m |          -0.0089 |          15.2177 |           4.4785 |
[32m[20230204 15:46:27 @agent_ppo2.py:194][0m |          -0.0096 |          14.9480 |           4.4752 |
[32m[20230204 15:46:27 @agent_ppo2.py:194][0m |          -0.0102 |          14.8515 |           4.4771 |
[32m[20230204 15:46:28 @agent_ppo2.py:194][0m |          -0.0108 |          14.6326 |           4.4754 |
[32m[20230204 15:46:28 @agent_ppo2.py:194][0m |          -0.0120 |          14.4424 |           4.4752 |
[32m[20230204 15:46:28 @agent_ppo2.py:194][0m |          -0.0124 |          14.3121 |           4.4759 |
[32m[20230204 15:46:28 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:46:28 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.18
[32m[20230204 15:46:28 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.35
[32m[20230204 15:46:28 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.78
[32m[20230204 15:46:28 @agent_ppo2.py:152][0m Total time:      10.29 min
[32m[20230204 15:46:28 @agent_ppo2.py:154][0m 831488 total steps have happened
[32m[20230204 15:46:28 @agent_ppo2.py:130][0m #------------------------ Iteration 406 --------------------------#
[32m[20230204 15:46:28 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:28 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:28 @agent_ppo2.py:194][0m |           0.0042 |          19.8448 |           4.4775 |
[32m[20230204 15:46:28 @agent_ppo2.py:194][0m |          -0.0050 |          13.3639 |           4.4681 |
[32m[20230204 15:46:28 @agent_ppo2.py:194][0m |          -0.0075 |          11.8356 |           4.4660 |
[32m[20230204 15:46:29 @agent_ppo2.py:194][0m |          -0.0066 |          11.6741 |           4.4641 |
[32m[20230204 15:46:29 @agent_ppo2.py:194][0m |          -0.0073 |          11.3873 |           4.4627 |
[32m[20230204 15:46:29 @agent_ppo2.py:194][0m |          -0.0012 |          10.7262 |           4.4386 |
[32m[20230204 15:46:29 @agent_ppo2.py:194][0m |          -0.0116 |          10.3550 |           4.4576 |
[32m[20230204 15:46:29 @agent_ppo2.py:194][0m |          -0.0105 |          10.1740 |           4.4524 |
[32m[20230204 15:46:29 @agent_ppo2.py:194][0m |          -0.0090 |          10.0885 |           4.4539 |
[32m[20230204 15:46:29 @agent_ppo2.py:194][0m |          -0.0135 |           9.7016 |           4.4500 |
[32m[20230204 15:46:29 @agent_ppo2.py:139][0m Policy update time: 0.82 s
[32m[20230204 15:46:29 @agent_ppo2.py:147][0m Average TRAINING episode reward: 218.60
[32m[20230204 15:46:29 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.64
[32m[20230204 15:46:29 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.16
[32m[20230204 15:46:29 @agent_ppo2.py:152][0m Total time:      10.31 min
[32m[20230204 15:46:29 @agent_ppo2.py:154][0m 833536 total steps have happened
[32m[20230204 15:46:29 @agent_ppo2.py:130][0m #------------------------ Iteration 407 --------------------------#
[32m[20230204 15:46:29 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:30 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:30 @agent_ppo2.py:194][0m |           0.0008 |          14.9826 |           4.5016 |
[32m[20230204 15:46:30 @agent_ppo2.py:194][0m |          -0.0062 |          13.5151 |           4.5047 |
[32m[20230204 15:46:30 @agent_ppo2.py:194][0m |          -0.0045 |          12.6659 |           4.4977 |
[32m[20230204 15:46:30 @agent_ppo2.py:194][0m |          -0.0104 |          11.9354 |           4.5010 |
[32m[20230204 15:46:30 @agent_ppo2.py:194][0m |          -0.0075 |          11.7945 |           4.4970 |
[32m[20230204 15:46:30 @agent_ppo2.py:194][0m |          -0.0106 |          11.4817 |           4.4978 |
[32m[20230204 15:46:30 @agent_ppo2.py:194][0m |          -0.0112 |          11.2564 |           4.4957 |
[32m[20230204 15:46:30 @agent_ppo2.py:194][0m |          -0.0112 |          11.1473 |           4.4982 |
[32m[20230204 15:46:30 @agent_ppo2.py:194][0m |          -0.0157 |          10.9816 |           4.4972 |
[32m[20230204 15:46:30 @agent_ppo2.py:194][0m |          -0.0149 |          10.8352 |           4.4959 |
[32m[20230204 15:46:30 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:46:31 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.25
[32m[20230204 15:46:31 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.77
[32m[20230204 15:46:31 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.72
[32m[20230204 15:46:31 @agent_ppo2.py:152][0m Total time:      10.33 min
[32m[20230204 15:46:31 @agent_ppo2.py:154][0m 835584 total steps have happened
[32m[20230204 15:46:31 @agent_ppo2.py:130][0m #------------------------ Iteration 408 --------------------------#
[32m[20230204 15:46:31 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:31 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:31 @agent_ppo2.py:194][0m |          -0.0027 |          15.8600 |           4.4030 |
[32m[20230204 15:46:31 @agent_ppo2.py:194][0m |          -0.0081 |          14.6284 |           4.3996 |
[32m[20230204 15:46:31 @agent_ppo2.py:194][0m |          -0.0103 |          13.7683 |           4.3974 |
[32m[20230204 15:46:31 @agent_ppo2.py:194][0m |          -0.0072 |          13.4911 |           4.3971 |
[32m[20230204 15:46:31 @agent_ppo2.py:194][0m |          -0.0153 |          12.7842 |           4.3934 |
[32m[20230204 15:46:31 @agent_ppo2.py:194][0m |          -0.0130 |          12.5928 |           4.3931 |
[32m[20230204 15:46:31 @agent_ppo2.py:194][0m |          -0.0140 |          12.5390 |           4.3882 |
[32m[20230204 15:46:31 @agent_ppo2.py:194][0m |          -0.0138 |          12.7101 |           4.3874 |
[32m[20230204 15:46:32 @agent_ppo2.py:194][0m |          -0.0137 |          12.0718 |           4.3930 |
[32m[20230204 15:46:32 @agent_ppo2.py:194][0m |          -0.0112 |          12.6973 |           4.3892 |
[32m[20230204 15:46:32 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:46:32 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.54
[32m[20230204 15:46:32 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.47
[32m[20230204 15:46:32 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.84
[32m[20230204 15:46:32 @agent_ppo2.py:152][0m Total time:      10.36 min
[32m[20230204 15:46:32 @agent_ppo2.py:154][0m 837632 total steps have happened
[32m[20230204 15:46:32 @agent_ppo2.py:130][0m #------------------------ Iteration 409 --------------------------#
[32m[20230204 15:46:32 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:32 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:32 @agent_ppo2.py:194][0m |          -0.0008 |          16.1835 |           4.4949 |
[32m[20230204 15:46:32 @agent_ppo2.py:194][0m |          -0.0043 |          14.3763 |           4.4905 |
[32m[20230204 15:46:32 @agent_ppo2.py:194][0m |          -0.0064 |          14.0284 |           4.4916 |
[32m[20230204 15:46:32 @agent_ppo2.py:194][0m |          -0.0077 |          13.7395 |           4.4886 |
[32m[20230204 15:46:33 @agent_ppo2.py:194][0m |          -0.0097 |          13.5676 |           4.4886 |
[32m[20230204 15:46:33 @agent_ppo2.py:194][0m |          -0.0092 |          13.4562 |           4.4909 |
[32m[20230204 15:46:33 @agent_ppo2.py:194][0m |          -0.0098 |          13.3553 |           4.4915 |
[32m[20230204 15:46:33 @agent_ppo2.py:194][0m |          -0.0104 |          13.1461 |           4.4864 |
[32m[20230204 15:46:33 @agent_ppo2.py:194][0m |          -0.0118 |          12.9702 |           4.4921 |
[32m[20230204 15:46:33 @agent_ppo2.py:194][0m |          -0.0104 |          12.9490 |           4.4889 |
[32m[20230204 15:46:33 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:46:33 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.49
[32m[20230204 15:46:33 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.26
[32m[20230204 15:46:33 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.56
[32m[20230204 15:46:33 @agent_ppo2.py:152][0m Total time:      10.38 min
[32m[20230204 15:46:33 @agent_ppo2.py:154][0m 839680 total steps have happened
[32m[20230204 15:46:33 @agent_ppo2.py:130][0m #------------------------ Iteration 410 --------------------------#
[32m[20230204 15:46:33 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:33 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:34 @agent_ppo2.py:194][0m |           0.0019 |          16.4864 |           4.4716 |
[32m[20230204 15:46:34 @agent_ppo2.py:194][0m |          -0.0051 |          14.4153 |           4.4713 |
[32m[20230204 15:46:34 @agent_ppo2.py:194][0m |          -0.0057 |          13.7716 |           4.4648 |
[32m[20230204 15:46:34 @agent_ppo2.py:194][0m |          -0.0075 |          13.3825 |           4.4692 |
[32m[20230204 15:46:34 @agent_ppo2.py:194][0m |          -0.0095 |          13.1632 |           4.4702 |
[32m[20230204 15:46:34 @agent_ppo2.py:194][0m |          -0.0098 |          12.9726 |           4.4655 |
[32m[20230204 15:46:34 @agent_ppo2.py:194][0m |          -0.0102 |          12.8476 |           4.4676 |
[32m[20230204 15:46:34 @agent_ppo2.py:194][0m |          -0.0113 |          12.6846 |           4.4675 |
[32m[20230204 15:46:34 @agent_ppo2.py:194][0m |          -0.0111 |          12.5350 |           4.4647 |
[32m[20230204 15:46:34 @agent_ppo2.py:194][0m |          -0.0120 |          12.4545 |           4.4674 |
[32m[20230204 15:46:34 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:46:34 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.95
[32m[20230204 15:46:34 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.62
[32m[20230204 15:46:34 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.24
[32m[20230204 15:46:34 @agent_ppo2.py:152][0m Total time:      10.40 min
[32m[20230204 15:46:34 @agent_ppo2.py:154][0m 841728 total steps have happened
[32m[20230204 15:46:34 @agent_ppo2.py:130][0m #------------------------ Iteration 411 --------------------------#
[32m[20230204 15:46:35 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:46:35 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:35 @agent_ppo2.py:194][0m |           0.0020 |          26.6249 |           4.5140 |
[32m[20230204 15:46:35 @agent_ppo2.py:194][0m |          -0.0078 |          16.6349 |           4.5014 |
[32m[20230204 15:46:35 @agent_ppo2.py:194][0m |          -0.0106 |          13.1591 |           4.5040 |
[32m[20230204 15:46:35 @agent_ppo2.py:194][0m |          -0.0135 |          11.8789 |           4.4992 |
[32m[20230204 15:46:35 @agent_ppo2.py:194][0m |          -0.0090 |          10.8769 |           4.4951 |
[32m[20230204 15:46:35 @agent_ppo2.py:194][0m |          -0.0136 |          10.2930 |           4.4963 |
[32m[20230204 15:46:35 @agent_ppo2.py:194][0m |          -0.0138 |           9.8360 |           4.4994 |
[32m[20230204 15:46:35 @agent_ppo2.py:194][0m |          -0.0141 |           9.5032 |           4.4961 |
[32m[20230204 15:46:35 @agent_ppo2.py:194][0m |          -0.0127 |           9.2912 |           4.4966 |
[32m[20230204 15:46:36 @agent_ppo2.py:194][0m |          -0.0129 |           9.1680 |           4.4960 |
[32m[20230204 15:46:36 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:46:36 @agent_ppo2.py:147][0m Average TRAINING episode reward: 228.84
[32m[20230204 15:46:36 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.77
[32m[20230204 15:46:36 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.48
[32m[20230204 15:46:36 @agent_ppo2.py:152][0m Total time:      10.42 min
[32m[20230204 15:46:36 @agent_ppo2.py:154][0m 843776 total steps have happened
[32m[20230204 15:46:36 @agent_ppo2.py:130][0m #------------------------ Iteration 412 --------------------------#
[32m[20230204 15:46:36 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:36 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:36 @agent_ppo2.py:194][0m |          -0.0005 |          15.7254 |           4.5897 |
[32m[20230204 15:46:36 @agent_ppo2.py:194][0m |          -0.0047 |          11.4429 |           4.5844 |
[32m[20230204 15:46:36 @agent_ppo2.py:194][0m |          -0.0055 |          10.2815 |           4.5846 |
[32m[20230204 15:46:36 @agent_ppo2.py:194][0m |          -0.0074 |           9.3159 |           4.5853 |
[32m[20230204 15:46:36 @agent_ppo2.py:194][0m |          -0.0080 |           8.7688 |           4.5861 |
[32m[20230204 15:46:37 @agent_ppo2.py:194][0m |          -0.0077 |           8.2259 |           4.5879 |
[32m[20230204 15:46:37 @agent_ppo2.py:194][0m |          -0.0092 |           7.8079 |           4.5845 |
[32m[20230204 15:46:37 @agent_ppo2.py:194][0m |          -0.0097 |           7.5904 |           4.5886 |
[32m[20230204 15:46:37 @agent_ppo2.py:194][0m |          -0.0100 |           7.1938 |           4.5864 |
[32m[20230204 15:46:37 @agent_ppo2.py:194][0m |          -0.0110 |           7.0258 |           4.5878 |
[32m[20230204 15:46:37 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:46:37 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.38
[32m[20230204 15:46:37 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.58
[32m[20230204 15:46:37 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.61
[32m[20230204 15:46:37 @agent_ppo2.py:152][0m Total time:      10.44 min
[32m[20230204 15:46:37 @agent_ppo2.py:154][0m 845824 total steps have happened
[32m[20230204 15:46:37 @agent_ppo2.py:130][0m #------------------------ Iteration 413 --------------------------#
[32m[20230204 15:46:37 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:37 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:37 @agent_ppo2.py:194][0m |           0.0011 |          15.5758 |           4.4946 |
[32m[20230204 15:46:37 @agent_ppo2.py:194][0m |          -0.0034 |          13.7764 |           4.4895 |
[32m[20230204 15:46:38 @agent_ppo2.py:194][0m |          -0.0060 |          13.2244 |           4.4848 |
[32m[20230204 15:46:38 @agent_ppo2.py:194][0m |          -0.0069 |          12.8099 |           4.4860 |
[32m[20230204 15:46:38 @agent_ppo2.py:194][0m |          -0.0080 |          12.5413 |           4.4808 |
[32m[20230204 15:46:38 @agent_ppo2.py:194][0m |          -0.0084 |          12.3248 |           4.4786 |
[32m[20230204 15:46:38 @agent_ppo2.py:194][0m |          -0.0088 |          12.1607 |           4.4787 |
[32m[20230204 15:46:38 @agent_ppo2.py:194][0m |          -0.0097 |          11.9986 |           4.4769 |
[32m[20230204 15:46:38 @agent_ppo2.py:194][0m |          -0.0099 |          11.8489 |           4.4795 |
[32m[20230204 15:46:38 @agent_ppo2.py:194][0m |          -0.0108 |          11.7559 |           4.4788 |
[32m[20230204 15:46:38 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:46:38 @agent_ppo2.py:147][0m Average TRAINING episode reward: 263.05
[32m[20230204 15:46:38 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 267.24
[32m[20230204 15:46:38 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.28
[32m[20230204 15:46:38 @agent_ppo2.py:152][0m Total time:      10.46 min
[32m[20230204 15:46:38 @agent_ppo2.py:154][0m 847872 total steps have happened
[32m[20230204 15:46:38 @agent_ppo2.py:130][0m #------------------------ Iteration 414 --------------------------#
[32m[20230204 15:46:39 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:46:39 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:39 @agent_ppo2.py:194][0m |          -0.0005 |          15.2149 |           4.5885 |
[32m[20230204 15:46:39 @agent_ppo2.py:194][0m |          -0.0072 |          13.4367 |           4.5703 |
[32m[20230204 15:46:39 @agent_ppo2.py:194][0m |          -0.0091 |          12.6142 |           4.5702 |
[32m[20230204 15:46:39 @agent_ppo2.py:194][0m |          -0.0123 |          12.0001 |           4.5678 |
[32m[20230204 15:46:39 @agent_ppo2.py:194][0m |          -0.0064 |          11.7079 |           4.5668 |
[32m[20230204 15:46:39 @agent_ppo2.py:194][0m |          -0.0131 |          11.0863 |           4.5607 |
[32m[20230204 15:46:39 @agent_ppo2.py:194][0m |          -0.0180 |          10.7198 |           4.5574 |
[32m[20230204 15:46:39 @agent_ppo2.py:194][0m |          -0.0117 |          10.4041 |           4.5605 |
[32m[20230204 15:46:39 @agent_ppo2.py:194][0m |          -0.0156 |          10.1831 |           4.5574 |
[32m[20230204 15:46:39 @agent_ppo2.py:194][0m |          -0.0147 |           9.9407 |           4.5584 |
[32m[20230204 15:46:39 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:46:40 @agent_ppo2.py:147][0m Average TRAINING episode reward: 263.20
[32m[20230204 15:46:40 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 267.42
[32m[20230204 15:46:40 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.57
[32m[20230204 15:46:40 @agent_ppo2.py:152][0m Total time:      10.49 min
[32m[20230204 15:46:40 @agent_ppo2.py:154][0m 849920 total steps have happened
[32m[20230204 15:46:40 @agent_ppo2.py:130][0m #------------------------ Iteration 415 --------------------------#
[32m[20230204 15:46:40 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:40 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:40 @agent_ppo2.py:194][0m |          -0.0019 |          29.5527 |           4.4682 |
[32m[20230204 15:46:40 @agent_ppo2.py:194][0m |          -0.0067 |          25.0078 |           4.4604 |
[32m[20230204 15:46:40 @agent_ppo2.py:194][0m |          -0.0081 |          23.3749 |           4.4509 |
[32m[20230204 15:46:40 @agent_ppo2.py:194][0m |          -0.0104 |          22.2245 |           4.4519 |
[32m[20230204 15:46:40 @agent_ppo2.py:194][0m |          -0.0107 |          21.2503 |           4.4513 |
[32m[20230204 15:46:40 @agent_ppo2.py:194][0m |          -0.0118 |          20.7019 |           4.4427 |
[32m[20230204 15:46:40 @agent_ppo2.py:194][0m |          -0.0119 |          20.0901 |           4.4423 |
[32m[20230204 15:46:41 @agent_ppo2.py:194][0m |          -0.0125 |          19.7113 |           4.4405 |
[32m[20230204 15:46:41 @agent_ppo2.py:194][0m |          -0.0131 |          19.0130 |           4.4373 |
[32m[20230204 15:46:41 @agent_ppo2.py:194][0m |          -0.0124 |          18.6390 |           4.4317 |
[32m[20230204 15:46:41 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:46:41 @agent_ppo2.py:147][0m Average TRAINING episode reward: 235.39
[32m[20230204 15:46:41 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.91
[32m[20230204 15:46:41 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.29
[32m[20230204 15:46:41 @agent_ppo2.py:152][0m Total time:      10.51 min
[32m[20230204 15:46:41 @agent_ppo2.py:154][0m 851968 total steps have happened
[32m[20230204 15:46:41 @agent_ppo2.py:130][0m #------------------------ Iteration 416 --------------------------#
[32m[20230204 15:46:41 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:41 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:41 @agent_ppo2.py:194][0m |          -0.0011 |          50.8708 |           4.4329 |
[32m[20230204 15:46:41 @agent_ppo2.py:194][0m |          -0.0089 |          34.9491 |           4.4336 |
[32m[20230204 15:46:41 @agent_ppo2.py:194][0m |          -0.0130 |          31.3590 |           4.4347 |
[32m[20230204 15:46:41 @agent_ppo2.py:194][0m |          -0.0127 |          29.8491 |           4.4355 |
[32m[20230204 15:46:42 @agent_ppo2.py:194][0m |          -0.0152 |          27.9356 |           4.4302 |
[32m[20230204 15:46:42 @agent_ppo2.py:194][0m |          -0.0147 |          26.9465 |           4.4316 |
[32m[20230204 15:46:42 @agent_ppo2.py:194][0m |          -0.0126 |          27.7600 |           4.4322 |
[32m[20230204 15:46:42 @agent_ppo2.py:194][0m |          -0.0135 |          25.5841 |           4.4300 |
[32m[20230204 15:46:42 @agent_ppo2.py:194][0m |          -0.0185 |          24.5873 |           4.4294 |
[32m[20230204 15:46:42 @agent_ppo2.py:194][0m |          -0.0194 |          24.0188 |           4.4295 |
[32m[20230204 15:46:42 @agent_ppo2.py:139][0m Policy update time: 0.78 s
[32m[20230204 15:46:42 @agent_ppo2.py:147][0m Average TRAINING episode reward: 170.97
[32m[20230204 15:46:42 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 255.60
[32m[20230204 15:46:42 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.97
[32m[20230204 15:46:42 @agent_ppo2.py:152][0m Total time:      10.53 min
[32m[20230204 15:46:42 @agent_ppo2.py:154][0m 854016 total steps have happened
[32m[20230204 15:46:42 @agent_ppo2.py:130][0m #------------------------ Iteration 417 --------------------------#
[32m[20230204 15:46:42 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:42 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:43 @agent_ppo2.py:194][0m |          -0.0007 |          15.0857 |           4.5075 |
[32m[20230204 15:46:43 @agent_ppo2.py:194][0m |          -0.0050 |          14.4762 |           4.5013 |
[32m[20230204 15:46:43 @agent_ppo2.py:194][0m |          -0.0050 |          14.2960 |           4.4950 |
[32m[20230204 15:46:43 @agent_ppo2.py:194][0m |          -0.0102 |          13.7141 |           4.4974 |
[32m[20230204 15:46:43 @agent_ppo2.py:194][0m |          -0.0096 |          13.4029 |           4.4947 |
[32m[20230204 15:46:43 @agent_ppo2.py:194][0m |          -0.0102 |          13.0850 |           4.4963 |
[32m[20230204 15:46:43 @agent_ppo2.py:194][0m |          -0.0121 |          12.8558 |           4.4947 |
[32m[20230204 15:46:43 @agent_ppo2.py:194][0m |          -0.0124 |          12.6193 |           4.4925 |
[32m[20230204 15:46:43 @agent_ppo2.py:194][0m |          -0.0121 |          12.4571 |           4.4925 |
[32m[20230204 15:46:43 @agent_ppo2.py:194][0m |          -0.0129 |          12.2839 |           4.4903 |
[32m[20230204 15:46:43 @agent_ppo2.py:139][0m Policy update time: 0.89 s
[32m[20230204 15:46:43 @agent_ppo2.py:147][0m Average TRAINING episode reward: 256.74
[32m[20230204 15:46:43 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.23
[32m[20230204 15:46:43 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.51
[32m[20230204 15:46:43 @agent_ppo2.py:152][0m Total time:      10.55 min
[32m[20230204 15:46:43 @agent_ppo2.py:154][0m 856064 total steps have happened
[32m[20230204 15:46:43 @agent_ppo2.py:130][0m #------------------------ Iteration 418 --------------------------#
[32m[20230204 15:46:44 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:44 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:44 @agent_ppo2.py:194][0m |           0.0073 |          38.3058 |           4.5620 |
[32m[20230204 15:46:44 @agent_ppo2.py:194][0m |          -0.0055 |          29.5819 |           4.5554 |
[32m[20230204 15:46:44 @agent_ppo2.py:194][0m |          -0.0096 |          26.9275 |           4.5580 |
[32m[20230204 15:46:44 @agent_ppo2.py:194][0m |          -0.0088 |          25.2559 |           4.5561 |
[32m[20230204 15:46:44 @agent_ppo2.py:194][0m |          -0.0077 |          24.2260 |           4.5523 |
[32m[20230204 15:46:44 @agent_ppo2.py:194][0m |          -0.0124 |          23.6661 |           4.5500 |
[32m[20230204 15:46:44 @agent_ppo2.py:194][0m |          -0.0103 |          22.8603 |           4.5487 |
[32m[20230204 15:46:44 @agent_ppo2.py:194][0m |          -0.0132 |          21.9179 |           4.5510 |
[32m[20230204 15:46:44 @agent_ppo2.py:194][0m |          -0.0143 |          21.4612 |           4.5466 |
[32m[20230204 15:46:45 @agent_ppo2.py:194][0m |          -0.0151 |          21.0453 |           4.5465 |
[32m[20230204 15:46:45 @agent_ppo2.py:139][0m Policy update time: 0.79 s
[32m[20230204 15:46:45 @agent_ppo2.py:147][0m Average TRAINING episode reward: 209.99
[32m[20230204 15:46:45 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.54
[32m[20230204 15:46:45 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.69
[32m[20230204 15:46:45 @agent_ppo2.py:152][0m Total time:      10.57 min
[32m[20230204 15:46:45 @agent_ppo2.py:154][0m 858112 total steps have happened
[32m[20230204 15:46:45 @agent_ppo2.py:130][0m #------------------------ Iteration 419 --------------------------#
[32m[20230204 15:46:45 @agent_ppo2.py:136][0m Sampling time: 0.34 s by 4 slaves
[32m[20230204 15:46:45 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:45 @agent_ppo2.py:194][0m |           0.0033 |          31.8871 |           4.4280 |
[32m[20230204 15:46:45 @agent_ppo2.py:194][0m |          -0.0092 |          22.5569 |           4.4166 |
[32m[20230204 15:46:45 @agent_ppo2.py:194][0m |          -0.0103 |          20.8065 |           4.4147 |
[32m[20230204 15:46:45 @agent_ppo2.py:194][0m |          -0.0087 |          19.8079 |           4.4074 |
[32m[20230204 15:46:46 @agent_ppo2.py:194][0m |          -0.0145 |          19.7833 |           4.4045 |
[32m[20230204 15:46:46 @agent_ppo2.py:194][0m |          -0.0018 |          18.5079 |           4.4041 |
[32m[20230204 15:46:46 @agent_ppo2.py:194][0m |          -0.0107 |          17.8885 |           4.3971 |
[32m[20230204 15:46:46 @agent_ppo2.py:194][0m |          -0.0198 |          17.3363 |           4.3986 |
[32m[20230204 15:46:46 @agent_ppo2.py:194][0m |          -0.0137 |          16.8656 |           4.3960 |
[32m[20230204 15:46:46 @agent_ppo2.py:194][0m |          -0.0122 |          16.5844 |           4.4014 |
[32m[20230204 15:46:46 @agent_ppo2.py:139][0m Policy update time: 0.96 s
[32m[20230204 15:46:46 @agent_ppo2.py:147][0m Average TRAINING episode reward: 199.07
[32m[20230204 15:46:46 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.98
[32m[20230204 15:46:46 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.74
[32m[20230204 15:46:46 @agent_ppo2.py:152][0m Total time:      10.60 min
[32m[20230204 15:46:46 @agent_ppo2.py:154][0m 860160 total steps have happened
[32m[20230204 15:46:46 @agent_ppo2.py:130][0m #------------------------ Iteration 420 --------------------------#
[32m[20230204 15:46:46 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:47 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:47 @agent_ppo2.py:194][0m |           0.0004 |          17.3308 |           4.4807 |
[32m[20230204 15:46:47 @agent_ppo2.py:194][0m |          -0.0053 |          15.0258 |           4.4770 |
[32m[20230204 15:46:47 @agent_ppo2.py:194][0m |          -0.0065 |          14.5953 |           4.4713 |
[32m[20230204 15:46:47 @agent_ppo2.py:194][0m |          -0.0078 |          14.1149 |           4.4722 |
[32m[20230204 15:46:47 @agent_ppo2.py:194][0m |          -0.0079 |          13.9481 |           4.4708 |
[32m[20230204 15:46:47 @agent_ppo2.py:194][0m |          -0.0093 |          13.5635 |           4.4722 |
[32m[20230204 15:46:47 @agent_ppo2.py:194][0m |          -0.0111 |          13.3587 |           4.4728 |
[32m[20230204 15:46:47 @agent_ppo2.py:194][0m |          -0.0077 |          13.2055 |           4.4749 |
[32m[20230204 15:46:47 @agent_ppo2.py:194][0m |          -0.0104 |          12.9142 |           4.4725 |
[32m[20230204 15:46:47 @agent_ppo2.py:194][0m |          -0.0114 |          12.6467 |           4.4761 |
[32m[20230204 15:46:47 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:46:47 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.60
[32m[20230204 15:46:47 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.95
[32m[20230204 15:46:47 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 99.08
[32m[20230204 15:46:47 @agent_ppo2.py:152][0m Total time:      10.62 min
[32m[20230204 15:46:47 @agent_ppo2.py:154][0m 862208 total steps have happened
[32m[20230204 15:46:47 @agent_ppo2.py:130][0m #------------------------ Iteration 421 --------------------------#
[32m[20230204 15:46:48 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:48 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:48 @agent_ppo2.py:194][0m |          -0.0013 |          16.6395 |           4.4919 |
[32m[20230204 15:46:48 @agent_ppo2.py:194][0m |          -0.0015 |          15.0915 |           4.4891 |
[32m[20230204 15:46:48 @agent_ppo2.py:194][0m |          -0.0073 |          14.2292 |           4.4844 |
[32m[20230204 15:46:48 @agent_ppo2.py:194][0m |          -0.0113 |          13.7630 |           4.4820 |
[32m[20230204 15:46:48 @agent_ppo2.py:194][0m |          -0.0095 |          13.4668 |           4.4841 |
[32m[20230204 15:46:48 @agent_ppo2.py:194][0m |          -0.0295 |          13.2738 |           4.4778 |
[32m[20230204 15:46:48 @agent_ppo2.py:194][0m |           0.0046 |          14.1227 |           4.4739 |
[32m[20230204 15:46:48 @agent_ppo2.py:194][0m |          -0.0106 |          13.0032 |           4.4710 |
[32m[20230204 15:46:48 @agent_ppo2.py:194][0m |          -0.0136 |          12.8592 |           4.4785 |
[32m[20230204 15:46:49 @agent_ppo2.py:194][0m |          -0.0117 |          12.7292 |           4.4762 |
[32m[20230204 15:46:49 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:46:49 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.81
[32m[20230204 15:46:49 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.40
[32m[20230204 15:46:49 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.23
[32m[20230204 15:46:49 @agent_ppo2.py:152][0m Total time:      10.64 min
[32m[20230204 15:46:49 @agent_ppo2.py:154][0m 864256 total steps have happened
[32m[20230204 15:46:49 @agent_ppo2.py:130][0m #------------------------ Iteration 422 --------------------------#
[32m[20230204 15:46:49 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:49 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:49 @agent_ppo2.py:194][0m |           0.0027 |          14.5423 |           4.5024 |
[32m[20230204 15:46:49 @agent_ppo2.py:194][0m |          -0.0046 |          12.9298 |           4.4933 |
[32m[20230204 15:46:49 @agent_ppo2.py:194][0m |          -0.0089 |          12.1079 |           4.4910 |
[32m[20230204 15:46:49 @agent_ppo2.py:194][0m |          -0.0089 |          11.6966 |           4.4925 |
[32m[20230204 15:46:49 @agent_ppo2.py:194][0m |          -0.0114 |          11.3487 |           4.4901 |
[32m[20230204 15:46:50 @agent_ppo2.py:194][0m |          -0.0108 |          11.0250 |           4.4883 |
[32m[20230204 15:46:50 @agent_ppo2.py:194][0m |          -0.0131 |          10.8065 |           4.4869 |
[32m[20230204 15:46:50 @agent_ppo2.py:194][0m |          -0.0127 |          10.6287 |           4.4917 |
[32m[20230204 15:46:50 @agent_ppo2.py:194][0m |          -0.0135 |          10.4338 |           4.4881 |
[32m[20230204 15:46:50 @agent_ppo2.py:194][0m |          -0.0137 |          10.2456 |           4.4899 |
[32m[20230204 15:46:50 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:46:50 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.19
[32m[20230204 15:46:50 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.29
[32m[20230204 15:46:50 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.39
[32m[20230204 15:46:50 @agent_ppo2.py:152][0m Total time:      10.66 min
[32m[20230204 15:46:50 @agent_ppo2.py:154][0m 866304 total steps have happened
[32m[20230204 15:46:50 @agent_ppo2.py:130][0m #------------------------ Iteration 423 --------------------------#
[32m[20230204 15:46:50 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:50 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:50 @agent_ppo2.py:194][0m |          -0.0004 |          15.0828 |           4.5664 |
[32m[20230204 15:46:51 @agent_ppo2.py:194][0m |          -0.0058 |          14.1404 |           4.5584 |
[32m[20230204 15:46:51 @agent_ppo2.py:194][0m |          -0.0055 |          13.9336 |           4.5589 |
[32m[20230204 15:46:51 @agent_ppo2.py:194][0m |          -0.0082 |          13.5752 |           4.5541 |
[32m[20230204 15:46:51 @agent_ppo2.py:194][0m |          -0.0128 |          12.9138 |           4.5519 |
[32m[20230204 15:46:51 @agent_ppo2.py:194][0m |          -0.0115 |          12.7227 |           4.5525 |
[32m[20230204 15:46:51 @agent_ppo2.py:194][0m |          -0.0139 |          12.2405 |           4.5525 |
[32m[20230204 15:46:51 @agent_ppo2.py:194][0m |          -0.0126 |          12.1168 |           4.5517 |
[32m[20230204 15:46:51 @agent_ppo2.py:194][0m |          -0.0127 |          11.8717 |           4.5484 |
[32m[20230204 15:46:51 @agent_ppo2.py:194][0m |          -0.0142 |          11.5810 |           4.5473 |
[32m[20230204 15:46:51 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:46:51 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.56
[32m[20230204 15:46:51 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.88
[32m[20230204 15:46:51 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.92
[32m[20230204 15:46:51 @agent_ppo2.py:152][0m Total time:      10.68 min
[32m[20230204 15:46:51 @agent_ppo2.py:154][0m 868352 total steps have happened
[32m[20230204 15:46:51 @agent_ppo2.py:130][0m #------------------------ Iteration 424 --------------------------#
[32m[20230204 15:46:52 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:52 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:52 @agent_ppo2.py:194][0m |          -0.0028 |          13.6206 |           4.4608 |
[32m[20230204 15:46:52 @agent_ppo2.py:194][0m |          -0.0074 |          12.9576 |           4.4548 |
[32m[20230204 15:46:52 @agent_ppo2.py:194][0m |          -0.0084 |          12.6263 |           4.4513 |
[32m[20230204 15:46:52 @agent_ppo2.py:194][0m |          -0.0056 |          12.4757 |           4.4537 |
[32m[20230204 15:46:52 @agent_ppo2.py:194][0m |          -0.0095 |          12.3038 |           4.4454 |
[32m[20230204 15:46:52 @agent_ppo2.py:194][0m |          -0.0078 |          12.1575 |           4.4565 |
[32m[20230204 15:46:52 @agent_ppo2.py:194][0m |          -0.0115 |          11.9735 |           4.4546 |
[32m[20230204 15:46:52 @agent_ppo2.py:194][0m |          -0.0136 |          11.8526 |           4.4487 |
[32m[20230204 15:46:52 @agent_ppo2.py:194][0m |          -0.0133 |          11.7120 |           4.4548 |
[32m[20230204 15:46:52 @agent_ppo2.py:194][0m |          -0.0109 |          11.6637 |           4.4506 |
[32m[20230204 15:46:52 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:46:53 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.90
[32m[20230204 15:46:53 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.42
[32m[20230204 15:46:53 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.55
[32m[20230204 15:46:53 @agent_ppo2.py:152][0m Total time:      10.70 min
[32m[20230204 15:46:53 @agent_ppo2.py:154][0m 870400 total steps have happened
[32m[20230204 15:46:53 @agent_ppo2.py:130][0m #------------------------ Iteration 425 --------------------------#
[32m[20230204 15:46:53 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:53 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:53 @agent_ppo2.py:194][0m |           0.0031 |          14.1208 |           4.5327 |
[32m[20230204 15:46:53 @agent_ppo2.py:194][0m |          -0.0064 |          12.3733 |           4.5311 |
[32m[20230204 15:46:53 @agent_ppo2.py:194][0m |          -0.0080 |          11.6756 |           4.5277 |
[32m[20230204 15:46:53 @agent_ppo2.py:194][0m |          -0.0097 |          11.2370 |           4.5301 |
[32m[20230204 15:46:53 @agent_ppo2.py:194][0m |          -0.0105 |          10.8783 |           4.5308 |
[32m[20230204 15:46:53 @agent_ppo2.py:194][0m |          -0.0117 |          10.6327 |           4.5334 |
[32m[20230204 15:46:54 @agent_ppo2.py:194][0m |          -0.0125 |          10.4124 |           4.5314 |
[32m[20230204 15:46:54 @agent_ppo2.py:194][0m |          -0.0126 |          10.1920 |           4.5371 |
[32m[20230204 15:46:54 @agent_ppo2.py:194][0m |          -0.0137 |          10.0078 |           4.5360 |
[32m[20230204 15:46:54 @agent_ppo2.py:194][0m |          -0.0133 |           9.8670 |           4.5365 |
[32m[20230204 15:46:54 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:46:54 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.03
[32m[20230204 15:46:54 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.75
[32m[20230204 15:46:54 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.77
[32m[20230204 15:46:54 @agent_ppo2.py:152][0m Total time:      10.73 min
[32m[20230204 15:46:54 @agent_ppo2.py:154][0m 872448 total steps have happened
[32m[20230204 15:46:54 @agent_ppo2.py:130][0m #------------------------ Iteration 426 --------------------------#
[32m[20230204 15:46:54 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:46:54 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:54 @agent_ppo2.py:194][0m |           0.0002 |          15.6347 |           4.5391 |
[32m[20230204 15:46:55 @agent_ppo2.py:194][0m |          -0.0042 |          14.3268 |           4.5318 |
[32m[20230204 15:46:55 @agent_ppo2.py:194][0m |          -0.0071 |          13.8758 |           4.5273 |
[32m[20230204 15:46:55 @agent_ppo2.py:194][0m |          -0.0078 |          13.7500 |           4.5250 |
[32m[20230204 15:46:55 @agent_ppo2.py:194][0m |          -0.0089 |          13.5776 |           4.5207 |
[32m[20230204 15:46:55 @agent_ppo2.py:194][0m |          -0.0089 |          13.4817 |           4.5188 |
[32m[20230204 15:46:55 @agent_ppo2.py:194][0m |          -0.0101 |          13.3869 |           4.5182 |
[32m[20230204 15:46:55 @agent_ppo2.py:194][0m |          -0.0103 |          13.3171 |           4.5160 |
[32m[20230204 15:46:55 @agent_ppo2.py:194][0m |          -0.0110 |          13.2174 |           4.5147 |
[32m[20230204 15:46:55 @agent_ppo2.py:194][0m |          -0.0106 |          13.1964 |           4.5146 |
[32m[20230204 15:46:55 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:46:55 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.42
[32m[20230204 15:46:55 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.26
[32m[20230204 15:46:55 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.61
[32m[20230204 15:46:55 @agent_ppo2.py:152][0m Total time:      10.75 min
[32m[20230204 15:46:55 @agent_ppo2.py:154][0m 874496 total steps have happened
[32m[20230204 15:46:55 @agent_ppo2.py:130][0m #------------------------ Iteration 427 --------------------------#
[32m[20230204 15:46:56 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:56 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:56 @agent_ppo2.py:194][0m |          -0.0014 |          14.8281 |           4.4869 |
[32m[20230204 15:46:56 @agent_ppo2.py:194][0m |          -0.0082 |          13.1972 |           4.4854 |
[32m[20230204 15:46:56 @agent_ppo2.py:194][0m |          -0.0074 |          12.5960 |           4.4812 |
[32m[20230204 15:46:56 @agent_ppo2.py:194][0m |          -0.0111 |          12.1987 |           4.4808 |
[32m[20230204 15:46:56 @agent_ppo2.py:194][0m |          -0.0082 |          11.9174 |           4.4777 |
[32m[20230204 15:46:56 @agent_ppo2.py:194][0m |          -0.0053 |          12.2771 |           4.4766 |
[32m[20230204 15:46:56 @agent_ppo2.py:194][0m |          -0.0119 |          11.5125 |           4.4730 |
[32m[20230204 15:46:56 @agent_ppo2.py:194][0m |          -0.0116 |          11.4235 |           4.4780 |
[32m[20230204 15:46:56 @agent_ppo2.py:194][0m |          -0.0129 |          11.2463 |           4.4784 |
[32m[20230204 15:46:56 @agent_ppo2.py:194][0m |          -0.0113 |          11.2626 |           4.4737 |
[32m[20230204 15:46:56 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:46:57 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.79
[32m[20230204 15:46:57 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 258.99
[32m[20230204 15:46:57 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 63.03
[32m[20230204 15:46:57 @agent_ppo2.py:152][0m Total time:      10.77 min
[32m[20230204 15:46:57 @agent_ppo2.py:154][0m 876544 total steps have happened
[32m[20230204 15:46:57 @agent_ppo2.py:130][0m #------------------------ Iteration 428 --------------------------#
[32m[20230204 15:46:57 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:46:57 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:57 @agent_ppo2.py:194][0m |           0.0008 |          14.4595 |           4.5674 |
[32m[20230204 15:46:57 @agent_ppo2.py:194][0m |          -0.0062 |          14.0055 |           4.5594 |
[32m[20230204 15:46:57 @agent_ppo2.py:194][0m |          -0.0080 |          13.7909 |           4.5567 |
[32m[20230204 15:46:57 @agent_ppo2.py:194][0m |          -0.0085 |          13.5791 |           4.5587 |
[32m[20230204 15:46:57 @agent_ppo2.py:194][0m |          -0.0080 |          13.6333 |           4.5614 |
[32m[20230204 15:46:57 @agent_ppo2.py:194][0m |          -0.0109 |          13.3262 |           4.5586 |
[32m[20230204 15:46:57 @agent_ppo2.py:194][0m |          -0.0092 |          13.3529 |           4.5600 |
[32m[20230204 15:46:57 @agent_ppo2.py:194][0m |          -0.0111 |          13.1670 |           4.5587 |
[32m[20230204 15:46:58 @agent_ppo2.py:194][0m |          -0.0114 |          13.0628 |           4.5637 |
[32m[20230204 15:46:58 @agent_ppo2.py:194][0m |          -0.0084 |          13.3570 |           4.5596 |
[32m[20230204 15:46:58 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:46:58 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.42
[32m[20230204 15:46:58 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.83
[32m[20230204 15:46:58 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.98
[32m[20230204 15:46:58 @agent_ppo2.py:152][0m Total time:      10.79 min
[32m[20230204 15:46:58 @agent_ppo2.py:154][0m 878592 total steps have happened
[32m[20230204 15:46:58 @agent_ppo2.py:130][0m #------------------------ Iteration 429 --------------------------#
[32m[20230204 15:46:58 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:46:58 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:46:58 @agent_ppo2.py:194][0m |           0.0008 |          15.1308 |           4.5418 |
[32m[20230204 15:46:58 @agent_ppo2.py:194][0m |          -0.0053 |          11.6462 |           4.5371 |
[32m[20230204 15:46:58 @agent_ppo2.py:194][0m |          -0.0064 |          10.1587 |           4.5386 |
[32m[20230204 15:46:59 @agent_ppo2.py:194][0m |          -0.0086 |           9.4830 |           4.5357 |
[32m[20230204 15:46:59 @agent_ppo2.py:194][0m |          -0.0101 |           9.1048 |           4.5347 |
[32m[20230204 15:46:59 @agent_ppo2.py:194][0m |          -0.0110 |           8.6465 |           4.5359 |
[32m[20230204 15:46:59 @agent_ppo2.py:194][0m |          -0.0113 |           8.2447 |           4.5319 |
[32m[20230204 15:46:59 @agent_ppo2.py:194][0m |          -0.0117 |           7.8905 |           4.5365 |
[32m[20230204 15:46:59 @agent_ppo2.py:194][0m |          -0.0119 |           7.5006 |           4.5381 |
[32m[20230204 15:46:59 @agent_ppo2.py:194][0m |          -0.0121 |           7.1999 |           4.5354 |
[32m[20230204 15:46:59 @agent_ppo2.py:139][0m Policy update time: 0.90 s
[32m[20230204 15:46:59 @agent_ppo2.py:147][0m Average TRAINING episode reward: 254.87
[32m[20230204 15:46:59 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.35
[32m[20230204 15:46:59 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.55
[32m[20230204 15:46:59 @agent_ppo2.py:152][0m Total time:      10.81 min
[32m[20230204 15:46:59 @agent_ppo2.py:154][0m 880640 total steps have happened
[32m[20230204 15:46:59 @agent_ppo2.py:130][0m #------------------------ Iteration 430 --------------------------#
[32m[20230204 15:46:59 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:00 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:00 @agent_ppo2.py:194][0m |           0.0053 |          11.4463 |           4.5287 |
[32m[20230204 15:47:00 @agent_ppo2.py:194][0m |          -0.0086 |           9.0823 |           4.5230 |
[32m[20230204 15:47:00 @agent_ppo2.py:194][0m |          -0.0098 |           7.7342 |           4.5217 |
[32m[20230204 15:47:00 @agent_ppo2.py:194][0m |          -0.0079 |           6.6090 |           4.5199 |
[32m[20230204 15:47:00 @agent_ppo2.py:194][0m |          -0.0124 |           5.7710 |           4.5209 |
[32m[20230204 15:47:00 @agent_ppo2.py:194][0m |          -0.0090 |           5.0987 |           4.5212 |
[32m[20230204 15:47:00 @agent_ppo2.py:194][0m |          -0.0154 |           4.6336 |           4.5150 |
[32m[20230204 15:47:00 @agent_ppo2.py:194][0m |          -0.0097 |           4.3260 |           4.5193 |
[32m[20230204 15:47:00 @agent_ppo2.py:194][0m |          -0.0111 |           3.9245 |           4.5173 |
[32m[20230204 15:47:00 @agent_ppo2.py:194][0m |          -0.0159 |           3.6656 |           4.5165 |
[32m[20230204 15:47:00 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:00 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.36
[32m[20230204 15:47:00 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.46
[32m[20230204 15:47:00 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.95
[32m[20230204 15:47:00 @agent_ppo2.py:152][0m Total time:      10.83 min
[32m[20230204 15:47:00 @agent_ppo2.py:154][0m 882688 total steps have happened
[32m[20230204 15:47:00 @agent_ppo2.py:130][0m #------------------------ Iteration 431 --------------------------#
[32m[20230204 15:47:01 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:47:01 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:01 @agent_ppo2.py:194][0m |           0.0022 |          32.1504 |           4.5883 |
[32m[20230204 15:47:01 @agent_ppo2.py:194][0m |          -0.0016 |          24.3034 |           4.5867 |
[32m[20230204 15:47:01 @agent_ppo2.py:194][0m |          -0.0061 |          22.7474 |           4.5824 |
[32m[20230204 15:47:01 @agent_ppo2.py:194][0m |          -0.0066 |          22.2099 |           4.5844 |
[32m[20230204 15:47:01 @agent_ppo2.py:194][0m |          -0.0094 |          21.2553 |           4.5835 |
[32m[20230204 15:47:01 @agent_ppo2.py:194][0m |          -0.0103 |          20.9355 |           4.5830 |
[32m[20230204 15:47:01 @agent_ppo2.py:194][0m |          -0.0111 |          20.3873 |           4.5849 |
[32m[20230204 15:47:01 @agent_ppo2.py:194][0m |          -0.0106 |          20.0061 |           4.5821 |
[32m[20230204 15:47:02 @agent_ppo2.py:194][0m |          -0.0118 |          19.7311 |           4.5833 |
[32m[20230204 15:47:02 @agent_ppo2.py:194][0m |          -0.0125 |          19.6728 |           4.5836 |
[32m[20230204 15:47:02 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:47:02 @agent_ppo2.py:147][0m Average TRAINING episode reward: 191.26
[32m[20230204 15:47:02 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.65
[32m[20230204 15:47:02 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.85
[32m[20230204 15:47:02 @agent_ppo2.py:152][0m Total time:      10.86 min
[32m[20230204 15:47:02 @agent_ppo2.py:154][0m 884736 total steps have happened
[32m[20230204 15:47:02 @agent_ppo2.py:130][0m #------------------------ Iteration 432 --------------------------#
[32m[20230204 15:47:02 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:02 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:02 @agent_ppo2.py:194][0m |           0.0008 |          18.0309 |           4.6175 |
[32m[20230204 15:47:02 @agent_ppo2.py:194][0m |          -0.0046 |          15.9787 |           4.6111 |
[32m[20230204 15:47:02 @agent_ppo2.py:194][0m |          -0.0088 |          15.2103 |           4.6090 |
[32m[20230204 15:47:02 @agent_ppo2.py:194][0m |          -0.0062 |          14.9926 |           4.6086 |
[32m[20230204 15:47:03 @agent_ppo2.py:194][0m |          -0.0099 |          14.5359 |           4.6052 |
[32m[20230204 15:47:03 @agent_ppo2.py:194][0m |          -0.0101 |          14.4081 |           4.6115 |
[32m[20230204 15:47:03 @agent_ppo2.py:194][0m |          -0.0115 |          14.0808 |           4.6073 |
[32m[20230204 15:47:03 @agent_ppo2.py:194][0m |          -0.0123 |          13.9066 |           4.6076 |
[32m[20230204 15:47:03 @agent_ppo2.py:194][0m |          -0.0120 |          13.7655 |           4.6158 |
[32m[20230204 15:47:03 @agent_ppo2.py:194][0m |          -0.0136 |          13.6457 |           4.6141 |
[32m[20230204 15:47:03 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:03 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.95
[32m[20230204 15:47:03 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.40
[32m[20230204 15:47:03 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.66
[32m[20230204 15:47:03 @agent_ppo2.py:152][0m Total time:      10.88 min
[32m[20230204 15:47:03 @agent_ppo2.py:154][0m 886784 total steps have happened
[32m[20230204 15:47:03 @agent_ppo2.py:130][0m #------------------------ Iteration 433 --------------------------#
[32m[20230204 15:47:03 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:03 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:04 @agent_ppo2.py:194][0m |           0.0040 |          17.7620 |           4.5181 |
[32m[20230204 15:47:04 @agent_ppo2.py:194][0m |          -0.0078 |          14.9573 |           4.5094 |
[32m[20230204 15:47:04 @agent_ppo2.py:194][0m |          -0.0094 |          14.3515 |           4.5024 |
[32m[20230204 15:47:04 @agent_ppo2.py:194][0m |          -0.0105 |          13.6927 |           4.4974 |
[32m[20230204 15:47:04 @agent_ppo2.py:194][0m |          -0.0079 |          13.4866 |           4.4956 |
[32m[20230204 15:47:04 @agent_ppo2.py:194][0m |          -0.0094 |          13.1421 |           4.4905 |
[32m[20230204 15:47:04 @agent_ppo2.py:194][0m |          -0.0119 |          12.4115 |           4.4966 |
[32m[20230204 15:47:04 @agent_ppo2.py:194][0m |          -0.0175 |          11.9555 |           4.4925 |
[32m[20230204 15:47:04 @agent_ppo2.py:194][0m |          -0.0140 |          11.7727 |           4.4892 |
[32m[20230204 15:47:04 @agent_ppo2.py:194][0m |          -0.0141 |          11.4894 |           4.4887 |
[32m[20230204 15:47:04 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:47:04 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.34
[32m[20230204 15:47:04 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.87
[32m[20230204 15:47:04 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.53
[32m[20230204 15:47:04 @agent_ppo2.py:152][0m Total time:      10.90 min
[32m[20230204 15:47:04 @agent_ppo2.py:154][0m 888832 total steps have happened
[32m[20230204 15:47:04 @agent_ppo2.py:130][0m #------------------------ Iteration 434 --------------------------#
[32m[20230204 15:47:05 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:05 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:05 @agent_ppo2.py:194][0m |           0.0013 |          15.0056 |           4.6037 |
[32m[20230204 15:47:05 @agent_ppo2.py:194][0m |          -0.0052 |          13.8641 |           4.5918 |
[32m[20230204 15:47:05 @agent_ppo2.py:194][0m |          -0.0071 |          13.5773 |           4.5856 |
[32m[20230204 15:47:05 @agent_ppo2.py:194][0m |          -0.0085 |          13.4012 |           4.5794 |
[32m[20230204 15:47:05 @agent_ppo2.py:194][0m |          -0.0096 |          13.2789 |           4.5753 |
[32m[20230204 15:47:05 @agent_ppo2.py:194][0m |          -0.0105 |          13.1533 |           4.5748 |
[32m[20230204 15:47:05 @agent_ppo2.py:194][0m |          -0.0115 |          13.0934 |           4.5723 |
[32m[20230204 15:47:05 @agent_ppo2.py:194][0m |          -0.0123 |          12.9914 |           4.5721 |
[32m[20230204 15:47:05 @agent_ppo2.py:194][0m |          -0.0118 |          12.9151 |           4.5721 |
[32m[20230204 15:47:05 @agent_ppo2.py:194][0m |          -0.0128 |          12.8382 |           4.5689 |
[32m[20230204 15:47:05 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:47:06 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.48
[32m[20230204 15:47:06 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.78
[32m[20230204 15:47:06 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.16
[32m[20230204 15:47:06 @agent_ppo2.py:152][0m Total time:      10.92 min
[32m[20230204 15:47:06 @agent_ppo2.py:154][0m 890880 total steps have happened
[32m[20230204 15:47:06 @agent_ppo2.py:130][0m #------------------------ Iteration 435 --------------------------#
[32m[20230204 15:47:06 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:06 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:06 @agent_ppo2.py:194][0m |           0.0003 |          13.5333 |           4.5854 |
[32m[20230204 15:47:06 @agent_ppo2.py:194][0m |          -0.0065 |          12.5845 |           4.5821 |
[32m[20230204 15:47:06 @agent_ppo2.py:194][0m |          -0.0082 |          11.9174 |           4.5758 |
[32m[20230204 15:47:06 @agent_ppo2.py:194][0m |          -0.0090 |          11.5874 |           4.5804 |
[32m[20230204 15:47:06 @agent_ppo2.py:194][0m |          -0.0096 |          11.4533 |           4.5818 |
[32m[20230204 15:47:06 @agent_ppo2.py:194][0m |          -0.0099 |          11.3324 |           4.5793 |
[32m[20230204 15:47:07 @agent_ppo2.py:194][0m |          -0.0117 |          11.2370 |           4.5802 |
[32m[20230204 15:47:07 @agent_ppo2.py:194][0m |          -0.0121 |          11.1293 |           4.5774 |
[32m[20230204 15:47:07 @agent_ppo2.py:194][0m |          -0.0115 |          11.0468 |           4.5820 |
[32m[20230204 15:47:07 @agent_ppo2.py:194][0m |          -0.0124 |          10.8722 |           4.5805 |
[32m[20230204 15:47:07 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:47:07 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.96
[32m[20230204 15:47:07 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.67
[32m[20230204 15:47:07 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 85.55
[32m[20230204 15:47:07 @agent_ppo2.py:152][0m Total time:      10.94 min
[32m[20230204 15:47:07 @agent_ppo2.py:154][0m 892928 total steps have happened
[32m[20230204 15:47:07 @agent_ppo2.py:130][0m #------------------------ Iteration 436 --------------------------#
[32m[20230204 15:47:07 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:07 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:07 @agent_ppo2.py:194][0m |          -0.0018 |          14.3583 |           4.6360 |
[32m[20230204 15:47:07 @agent_ppo2.py:194][0m |           0.0005 |          12.5052 |           4.6334 |
[32m[20230204 15:47:07 @agent_ppo2.py:194][0m |          -0.0078 |          11.2035 |           4.6325 |
[32m[20230204 15:47:08 @agent_ppo2.py:194][0m |          -0.0093 |          10.4546 |           4.6316 |
[32m[20230204 15:47:08 @agent_ppo2.py:194][0m |          -0.0100 |           9.8595 |           4.6278 |
[32m[20230204 15:47:08 @agent_ppo2.py:194][0m |          -0.0066 |           9.5704 |           4.6284 |
[32m[20230204 15:47:08 @agent_ppo2.py:194][0m |          -0.0108 |           9.2428 |           4.6291 |
[32m[20230204 15:47:08 @agent_ppo2.py:194][0m |          -0.0117 |           8.9089 |           4.6251 |
[32m[20230204 15:47:08 @agent_ppo2.py:194][0m |          -0.0119 |           8.7495 |           4.6285 |
[32m[20230204 15:47:08 @agent_ppo2.py:194][0m |          -0.0135 |           8.5074 |           4.6286 |
[32m[20230204 15:47:08 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:47:08 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.24
[32m[20230204 15:47:08 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.51
[32m[20230204 15:47:08 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 112.96
[32m[20230204 15:47:08 @agent_ppo2.py:152][0m Total time:      10.96 min
[32m[20230204 15:47:08 @agent_ppo2.py:154][0m 894976 total steps have happened
[32m[20230204 15:47:08 @agent_ppo2.py:130][0m #------------------------ Iteration 437 --------------------------#
[32m[20230204 15:47:08 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:09 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:09 @agent_ppo2.py:194][0m |           0.0012 |          13.6163 |           4.5744 |
[32m[20230204 15:47:09 @agent_ppo2.py:194][0m |          -0.0019 |          12.5463 |           4.5807 |
[32m[20230204 15:47:09 @agent_ppo2.py:194][0m |          -0.0075 |          11.8016 |           4.5748 |
[32m[20230204 15:47:09 @agent_ppo2.py:194][0m |          -0.0086 |          11.3519 |           4.5714 |
[32m[20230204 15:47:09 @agent_ppo2.py:194][0m |          -0.0082 |          10.9877 |           4.5740 |
[32m[20230204 15:47:09 @agent_ppo2.py:194][0m |          -0.0105 |          10.6585 |           4.5763 |
[32m[20230204 15:47:09 @agent_ppo2.py:194][0m |          -0.0101 |          10.3488 |           4.5770 |
[32m[20230204 15:47:09 @agent_ppo2.py:194][0m |          -0.0123 |          10.1146 |           4.5775 |
[32m[20230204 15:47:09 @agent_ppo2.py:194][0m |          -0.0111 |           9.8028 |           4.5781 |
[32m[20230204 15:47:09 @agent_ppo2.py:194][0m |          -0.0109 |           9.4097 |           4.5805 |
[32m[20230204 15:47:09 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:10 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.50
[32m[20230204 15:47:10 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.20
[32m[20230204 15:47:10 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 263.14
[32m[20230204 15:47:10 @agent_ppo2.py:152][0m Total time:      10.98 min
[32m[20230204 15:47:10 @agent_ppo2.py:154][0m 897024 total steps have happened
[32m[20230204 15:47:10 @agent_ppo2.py:130][0m #------------------------ Iteration 438 --------------------------#
[32m[20230204 15:47:10 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:10 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:10 @agent_ppo2.py:194][0m |          -0.0003 |          16.8434 |           4.6449 |
[32m[20230204 15:47:10 @agent_ppo2.py:194][0m |          -0.0058 |          14.9985 |           4.6312 |
[32m[20230204 15:47:10 @agent_ppo2.py:194][0m |          -0.0081 |          14.4431 |           4.6284 |
[32m[20230204 15:47:10 @agent_ppo2.py:194][0m |          -0.0090 |          14.0944 |           4.6268 |
[32m[20230204 15:47:10 @agent_ppo2.py:194][0m |          -0.0101 |          13.8466 |           4.6228 |
[32m[20230204 15:47:10 @agent_ppo2.py:194][0m |          -0.0107 |          13.6557 |           4.6181 |
[32m[20230204 15:47:10 @agent_ppo2.py:194][0m |          -0.0111 |          13.4909 |           4.6149 |
[32m[20230204 15:47:10 @agent_ppo2.py:194][0m |          -0.0121 |          13.3698 |           4.6137 |
[32m[20230204 15:47:11 @agent_ppo2.py:194][0m |          -0.0127 |          13.2255 |           4.6102 |
[32m[20230204 15:47:11 @agent_ppo2.py:194][0m |          -0.0133 |          13.0841 |           4.6074 |
[32m[20230204 15:47:11 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:11 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.11
[32m[20230204 15:47:11 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.55
[32m[20230204 15:47:11 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.44
[32m[20230204 15:47:11 @agent_ppo2.py:152][0m Total time:      11.01 min
[32m[20230204 15:47:11 @agent_ppo2.py:154][0m 899072 total steps have happened
[32m[20230204 15:47:11 @agent_ppo2.py:130][0m #------------------------ Iteration 439 --------------------------#
[32m[20230204 15:47:11 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:11 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:11 @agent_ppo2.py:194][0m |           0.0035 |          13.3730 |           4.5302 |
[32m[20230204 15:47:11 @agent_ppo2.py:194][0m |          -0.0005 |          12.2268 |           4.5182 |
[32m[20230204 15:47:11 @agent_ppo2.py:194][0m |          -0.0083 |          11.0756 |           4.5204 |
[32m[20230204 15:47:11 @agent_ppo2.py:194][0m |          -0.0069 |          10.7393 |           4.5187 |
[32m[20230204 15:47:12 @agent_ppo2.py:194][0m |          -0.0094 |          10.2901 |           4.5147 |
[32m[20230204 15:47:12 @agent_ppo2.py:194][0m |          -0.0098 |           9.8501 |           4.5170 |
[32m[20230204 15:47:12 @agent_ppo2.py:194][0m |          -0.0095 |           9.4956 |           4.5161 |
[32m[20230204 15:47:12 @agent_ppo2.py:194][0m |          -0.0128 |           9.1783 |           4.5140 |
[32m[20230204 15:47:12 @agent_ppo2.py:194][0m |          -0.0104 |           8.9936 |           4.5148 |
[32m[20230204 15:47:12 @agent_ppo2.py:194][0m |          -0.0107 |           8.6821 |           4.5137 |
[32m[20230204 15:47:12 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:12 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.61
[32m[20230204 15:47:12 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.42
[32m[20230204 15:47:12 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.91
[32m[20230204 15:47:12 @agent_ppo2.py:152][0m Total time:      11.03 min
[32m[20230204 15:47:12 @agent_ppo2.py:154][0m 901120 total steps have happened
[32m[20230204 15:47:12 @agent_ppo2.py:130][0m #------------------------ Iteration 440 --------------------------#
[32m[20230204 15:47:12 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:47:12 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:13 @agent_ppo2.py:194][0m |           0.0029 |          15.3232 |           4.6670 |
[32m[20230204 15:47:13 @agent_ppo2.py:194][0m |          -0.0039 |          12.6935 |           4.6686 |
[32m[20230204 15:47:13 @agent_ppo2.py:194][0m |          -0.0044 |          12.3040 |           4.6653 |
[32m[20230204 15:47:13 @agent_ppo2.py:194][0m |          -0.0065 |          12.0847 |           4.6670 |
[32m[20230204 15:47:13 @agent_ppo2.py:194][0m |          -0.0083 |          11.9374 |           4.6654 |
[32m[20230204 15:47:13 @agent_ppo2.py:194][0m |          -0.0077 |          11.7823 |           4.6670 |
[32m[20230204 15:47:13 @agent_ppo2.py:194][0m |          -0.0093 |          11.6619 |           4.6687 |
[32m[20230204 15:47:13 @agent_ppo2.py:194][0m |          -0.0100 |          11.5532 |           4.6659 |
[32m[20230204 15:47:13 @agent_ppo2.py:194][0m |          -0.0104 |          11.3941 |           4.6682 |
[32m[20230204 15:47:13 @agent_ppo2.py:194][0m |          -0.0113 |          11.2181 |           4.6677 |
[32m[20230204 15:47:13 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:47:13 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.53
[32m[20230204 15:47:13 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.50
[32m[20230204 15:47:13 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.61
[32m[20230204 15:47:13 @agent_ppo2.py:152][0m Total time:      11.05 min
[32m[20230204 15:47:13 @agent_ppo2.py:154][0m 903168 total steps have happened
[32m[20230204 15:47:13 @agent_ppo2.py:130][0m #------------------------ Iteration 441 --------------------------#
[32m[20230204 15:47:14 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:14 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:14 @agent_ppo2.py:194][0m |          -0.0074 |          15.0570 |           4.4621 |
[32m[20230204 15:47:14 @agent_ppo2.py:194][0m |          -0.0033 |          14.0221 |           4.4531 |
[32m[20230204 15:47:14 @agent_ppo2.py:194][0m |          -0.0105 |          13.6312 |           4.4503 |
[32m[20230204 15:47:14 @agent_ppo2.py:194][0m |          -0.0125 |          13.3934 |           4.4512 |
[32m[20230204 15:47:14 @agent_ppo2.py:194][0m |          -0.0091 |          13.2188 |           4.4511 |
[32m[20230204 15:47:14 @agent_ppo2.py:194][0m |          -0.0117 |          13.0646 |           4.4516 |
[32m[20230204 15:47:14 @agent_ppo2.py:194][0m |          -0.0098 |          12.9332 |           4.4506 |
[32m[20230204 15:47:14 @agent_ppo2.py:194][0m |           0.0045 |          15.9631 |           4.4506 |
[32m[20230204 15:47:14 @agent_ppo2.py:194][0m |          -0.0102 |          12.7506 |           4.4433 |
[32m[20230204 15:47:15 @agent_ppo2.py:194][0m |          -0.0100 |          12.5557 |           4.4500 |
[32m[20230204 15:47:15 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:47:15 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.77
[32m[20230204 15:47:15 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.96
[32m[20230204 15:47:15 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.91
[32m[20230204 15:47:15 @agent_ppo2.py:152][0m Total time:      11.07 min
[32m[20230204 15:47:15 @agent_ppo2.py:154][0m 905216 total steps have happened
[32m[20230204 15:47:15 @agent_ppo2.py:130][0m #------------------------ Iteration 442 --------------------------#
[32m[20230204 15:47:15 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:47:15 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:15 @agent_ppo2.py:194][0m |           0.0051 |          14.4310 |           4.5456 |
[32m[20230204 15:47:15 @agent_ppo2.py:194][0m |          -0.0048 |          13.0490 |           4.5406 |
[32m[20230204 15:47:15 @agent_ppo2.py:194][0m |          -0.0151 |          12.4471 |           4.5394 |
[32m[20230204 15:47:15 @agent_ppo2.py:194][0m |          -0.0245 |          12.0212 |           4.5350 |
[32m[20230204 15:47:15 @agent_ppo2.py:194][0m |          -0.0163 |          11.5446 |           4.5217 |
[32m[20230204 15:47:16 @agent_ppo2.py:194][0m |          -0.0047 |          11.2349 |           4.5158 |
[32m[20230204 15:47:16 @agent_ppo2.py:194][0m |          -0.0176 |          10.8429 |           4.5276 |
[32m[20230204 15:47:16 @agent_ppo2.py:194][0m |          -0.0212 |          10.4086 |           4.5125 |
[32m[20230204 15:47:16 @agent_ppo2.py:194][0m |          -0.0143 |          10.2758 |           4.5200 |
[32m[20230204 15:47:16 @agent_ppo2.py:194][0m |          -0.0097 |           9.9512 |           4.5238 |
[32m[20230204 15:47:16 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:47:16 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.73
[32m[20230204 15:47:16 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.84
[32m[20230204 15:47:16 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.34
[32m[20230204 15:47:16 @agent_ppo2.py:152][0m Total time:      11.09 min
[32m[20230204 15:47:16 @agent_ppo2.py:154][0m 907264 total steps have happened
[32m[20230204 15:47:16 @agent_ppo2.py:130][0m #------------------------ Iteration 443 --------------------------#
[32m[20230204 15:47:16 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:47:16 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:16 @agent_ppo2.py:194][0m |          -0.0017 |          15.1108 |           4.6380 |
[32m[20230204 15:47:17 @agent_ppo2.py:194][0m |          -0.0019 |          13.6180 |           4.6261 |
[32m[20230204 15:47:17 @agent_ppo2.py:194][0m |          -0.0066 |          11.8061 |           4.6232 |
[32m[20230204 15:47:17 @agent_ppo2.py:194][0m |          -0.0076 |          11.3473 |           4.6211 |
[32m[20230204 15:47:17 @agent_ppo2.py:194][0m |          -0.0104 |          11.0891 |           4.6182 |
[32m[20230204 15:47:17 @agent_ppo2.py:194][0m |          -0.0112 |          10.9496 |           4.6224 |
[32m[20230204 15:47:17 @agent_ppo2.py:194][0m |          -0.0086 |          11.0692 |           4.6201 |
[32m[20230204 15:47:17 @agent_ppo2.py:194][0m |          -0.0118 |          10.7163 |           4.6138 |
[32m[20230204 15:47:17 @agent_ppo2.py:194][0m |          -0.0089 |          10.7160 |           4.6183 |
[32m[20230204 15:47:17 @agent_ppo2.py:194][0m |          -0.0131 |          10.4711 |           4.6124 |
[32m[20230204 15:47:17 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:17 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.07
[32m[20230204 15:47:17 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.55
[32m[20230204 15:47:17 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.10
[32m[20230204 15:47:17 @agent_ppo2.py:152][0m Total time:      11.11 min
[32m[20230204 15:47:17 @agent_ppo2.py:154][0m 909312 total steps have happened
[32m[20230204 15:47:17 @agent_ppo2.py:130][0m #------------------------ Iteration 444 --------------------------#
[32m[20230204 15:47:18 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:18 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:18 @agent_ppo2.py:194][0m |          -0.0005 |          13.2106 |           4.5760 |
[32m[20230204 15:47:18 @agent_ppo2.py:194][0m |          -0.0067 |          11.0673 |           4.5647 |
[32m[20230204 15:47:18 @agent_ppo2.py:194][0m |          -0.0103 |          10.4697 |           4.5590 |
[32m[20230204 15:47:18 @agent_ppo2.py:194][0m |          -0.0085 |          10.0936 |           4.5604 |
[32m[20230204 15:47:18 @agent_ppo2.py:194][0m |          -0.0123 |           9.8391 |           4.5621 |
[32m[20230204 15:47:18 @agent_ppo2.py:194][0m |          -0.0123 |           9.5473 |           4.5611 |
[32m[20230204 15:47:18 @agent_ppo2.py:194][0m |          -0.0087 |           9.4201 |           4.5557 |
[32m[20230204 15:47:18 @agent_ppo2.py:194][0m |          -0.0113 |           9.1912 |           4.5546 |
[32m[20230204 15:47:18 @agent_ppo2.py:194][0m |          -0.0122 |           8.7411 |           4.5582 |
[32m[20230204 15:47:18 @agent_ppo2.py:194][0m |          -0.0085 |           8.6648 |           4.5562 |
[32m[20230204 15:47:18 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:47:19 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.37
[32m[20230204 15:47:19 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.79
[32m[20230204 15:47:19 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.77
[32m[20230204 15:47:19 @agent_ppo2.py:152][0m Total time:      11.14 min
[32m[20230204 15:47:19 @agent_ppo2.py:154][0m 911360 total steps have happened
[32m[20230204 15:47:19 @agent_ppo2.py:130][0m #------------------------ Iteration 445 --------------------------#
[32m[20230204 15:47:19 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:19 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:19 @agent_ppo2.py:194][0m |           0.0024 |          15.2653 |           4.5151 |
[32m[20230204 15:47:19 @agent_ppo2.py:194][0m |          -0.0036 |          13.8665 |           4.5095 |
[32m[20230204 15:47:19 @agent_ppo2.py:194][0m |          -0.0042 |          13.2506 |           4.5069 |
[32m[20230204 15:47:19 @agent_ppo2.py:194][0m |          -0.0061 |          12.8869 |           4.5102 |
[32m[20230204 15:47:19 @agent_ppo2.py:194][0m |          -0.0064 |          12.6044 |           4.5061 |
[32m[20230204 15:47:19 @agent_ppo2.py:194][0m |          -0.0082 |          12.3700 |           4.5041 |
[32m[20230204 15:47:19 @agent_ppo2.py:194][0m |          -0.0075 |          12.2182 |           4.5095 |
[32m[20230204 15:47:20 @agent_ppo2.py:194][0m |          -0.0090 |          11.9813 |           4.5098 |
[32m[20230204 15:47:20 @agent_ppo2.py:194][0m |          -0.0097 |          11.8225 |           4.5065 |
[32m[20230204 15:47:20 @agent_ppo2.py:194][0m |          -0.0087 |          11.6744 |           4.5121 |
[32m[20230204 15:47:20 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:47:20 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.70
[32m[20230204 15:47:20 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.14
[32m[20230204 15:47:20 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.95
[32m[20230204 15:47:20 @agent_ppo2.py:152][0m Total time:      11.16 min
[32m[20230204 15:47:20 @agent_ppo2.py:154][0m 913408 total steps have happened
[32m[20230204 15:47:20 @agent_ppo2.py:130][0m #------------------------ Iteration 446 --------------------------#
[32m[20230204 15:47:20 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:47:20 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:20 @agent_ppo2.py:194][0m |          -0.0005 |          14.1360 |           4.5808 |
[32m[20230204 15:47:20 @agent_ppo2.py:194][0m |          -0.0042 |          13.0402 |           4.5824 |
[32m[20230204 15:47:20 @agent_ppo2.py:194][0m |          -0.0056 |          12.4157 |           4.5756 |
[32m[20230204 15:47:21 @agent_ppo2.py:194][0m |          -0.0084 |          12.0179 |           4.5740 |
[32m[20230204 15:47:21 @agent_ppo2.py:194][0m |          -0.0092 |          11.7784 |           4.5729 |
[32m[20230204 15:47:21 @agent_ppo2.py:194][0m |          -0.0105 |          11.5653 |           4.5731 |
[32m[20230204 15:47:21 @agent_ppo2.py:194][0m |          -0.0102 |          11.3934 |           4.5703 |
[32m[20230204 15:47:21 @agent_ppo2.py:194][0m |          -0.0112 |          11.1950 |           4.5721 |
[32m[20230204 15:47:21 @agent_ppo2.py:194][0m |          -0.0105 |          11.1183 |           4.5712 |
[32m[20230204 15:47:21 @agent_ppo2.py:194][0m |          -0.0133 |          10.8714 |           4.5671 |
[32m[20230204 15:47:21 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:47:21 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.77
[32m[20230204 15:47:21 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.81
[32m[20230204 15:47:21 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.72
[32m[20230204 15:47:21 @agent_ppo2.py:152][0m Total time:      11.18 min
[32m[20230204 15:47:21 @agent_ppo2.py:154][0m 915456 total steps have happened
[32m[20230204 15:47:21 @agent_ppo2.py:130][0m #------------------------ Iteration 447 --------------------------#
[32m[20230204 15:47:21 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:47:21 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:22 @agent_ppo2.py:194][0m |           0.0010 |          13.9009 |           4.6383 |
[32m[20230204 15:47:22 @agent_ppo2.py:194][0m |          -0.0041 |          11.6644 |           4.6354 |
[32m[20230204 15:47:22 @agent_ppo2.py:194][0m |          -0.0058 |          10.9588 |           4.6324 |
[32m[20230204 15:47:22 @agent_ppo2.py:194][0m |          -0.0067 |          10.6472 |           4.6305 |
[32m[20230204 15:47:22 @agent_ppo2.py:194][0m |          -0.0071 |          10.4155 |           4.6294 |
[32m[20230204 15:47:22 @agent_ppo2.py:194][0m |          -0.0088 |          10.2238 |           4.6238 |
[32m[20230204 15:47:22 @agent_ppo2.py:194][0m |          -0.0080 |          10.1169 |           4.6256 |
[32m[20230204 15:47:22 @agent_ppo2.py:194][0m |          -0.0098 |          10.0509 |           4.6213 |
[32m[20230204 15:47:22 @agent_ppo2.py:194][0m |          -0.0100 |           9.9373 |           4.6244 |
[32m[20230204 15:47:22 @agent_ppo2.py:194][0m |          -0.0100 |           9.8426 |           4.6210 |
[32m[20230204 15:47:22 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:22 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.47
[32m[20230204 15:47:22 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.97
[32m[20230204 15:47:22 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.87
[32m[20230204 15:47:22 @agent_ppo2.py:152][0m Total time:      11.20 min
[32m[20230204 15:47:22 @agent_ppo2.py:154][0m 917504 total steps have happened
[32m[20230204 15:47:22 @agent_ppo2.py:130][0m #------------------------ Iteration 448 --------------------------#
[32m[20230204 15:47:23 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:23 @agent_ppo2.py:194][0m |           0.0011 |          13.8316 |           4.5928 |
[32m[20230204 15:47:23 @agent_ppo2.py:194][0m |          -0.0043 |          11.4975 |           4.5839 |
[32m[20230204 15:47:23 @agent_ppo2.py:194][0m |          -0.0026 |          10.6495 |           4.5856 |
[32m[20230204 15:47:23 @agent_ppo2.py:194][0m |          -0.0066 |           9.5940 |           4.5905 |
[32m[20230204 15:47:23 @agent_ppo2.py:194][0m |          -0.0076 |           8.9771 |           4.5858 |
[32m[20230204 15:47:23 @agent_ppo2.py:194][0m |          -0.0084 |           8.6750 |           4.5869 |
[32m[20230204 15:47:23 @agent_ppo2.py:194][0m |          -0.0093 |           8.3052 |           4.5866 |
[32m[20230204 15:47:23 @agent_ppo2.py:194][0m |          -0.0086 |           8.0550 |           4.5863 |
[32m[20230204 15:47:23 @agent_ppo2.py:194][0m |          -0.0105 |           7.8556 |           4.5915 |
[32m[20230204 15:47:24 @agent_ppo2.py:194][0m |          -0.0115 |           7.6292 |           4.5854 |
[32m[20230204 15:47:24 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:24 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.20
[32m[20230204 15:47:24 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.43
[32m[20230204 15:47:24 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 102.01
[32m[20230204 15:47:24 @agent_ppo2.py:152][0m Total time:      11.22 min
[32m[20230204 15:47:24 @agent_ppo2.py:154][0m 919552 total steps have happened
[32m[20230204 15:47:24 @agent_ppo2.py:130][0m #------------------------ Iteration 449 --------------------------#
[32m[20230204 15:47:24 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:24 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:24 @agent_ppo2.py:194][0m |           0.0003 |          28.7113 |           4.6088 |
[32m[20230204 15:47:24 @agent_ppo2.py:194][0m |          -0.0054 |          17.3138 |           4.5900 |
[32m[20230204 15:47:24 @agent_ppo2.py:194][0m |          -0.0080 |          15.1949 |           4.5810 |
[32m[20230204 15:47:24 @agent_ppo2.py:194][0m |          -0.0103 |          14.1822 |           4.5786 |
[32m[20230204 15:47:25 @agent_ppo2.py:194][0m |          -0.0088 |          13.6650 |           4.5794 |
[32m[20230204 15:47:25 @agent_ppo2.py:194][0m |          -0.0118 |          13.0425 |           4.5728 |
[32m[20230204 15:47:25 @agent_ppo2.py:194][0m |          -0.0121 |          12.7064 |           4.5727 |
[32m[20230204 15:47:25 @agent_ppo2.py:194][0m |          -0.0109 |          12.6546 |           4.5745 |
[32m[20230204 15:47:25 @agent_ppo2.py:194][0m |          -0.0128 |          12.0799 |           4.5678 |
[32m[20230204 15:47:25 @agent_ppo2.py:194][0m |          -0.0143 |          11.7727 |           4.5696 |
[32m[20230204 15:47:25 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:47:25 @agent_ppo2.py:147][0m Average TRAINING episode reward: 220.72
[32m[20230204 15:47:25 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.81
[32m[20230204 15:47:25 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.09
[32m[20230204 15:47:25 @agent_ppo2.py:152][0m Total time:      11.24 min
[32m[20230204 15:47:25 @agent_ppo2.py:154][0m 921600 total steps have happened
[32m[20230204 15:47:25 @agent_ppo2.py:130][0m #------------------------ Iteration 450 --------------------------#
[32m[20230204 15:47:25 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:25 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:25 @agent_ppo2.py:194][0m |          -0.0006 |          15.7703 |           4.5102 |
[32m[20230204 15:47:26 @agent_ppo2.py:194][0m |          -0.0050 |          13.8494 |           4.4958 |
[32m[20230204 15:47:26 @agent_ppo2.py:194][0m |          -0.0068 |          13.5690 |           4.4890 |
[32m[20230204 15:47:26 @agent_ppo2.py:194][0m |          -0.0079 |          13.4853 |           4.4853 |
[32m[20230204 15:47:26 @agent_ppo2.py:194][0m |          -0.0112 |          13.1416 |           4.4857 |
[32m[20230204 15:47:26 @agent_ppo2.py:194][0m |          -0.0117 |          12.9718 |           4.4781 |
[32m[20230204 15:47:26 @agent_ppo2.py:194][0m |          -0.0132 |          12.8611 |           4.4767 |
[32m[20230204 15:47:26 @agent_ppo2.py:194][0m |          -0.0127 |          12.7505 |           4.4748 |
[32m[20230204 15:47:26 @agent_ppo2.py:194][0m |          -0.0146 |          12.6014 |           4.4736 |
[32m[20230204 15:47:26 @agent_ppo2.py:194][0m |          -0.0143 |          12.6508 |           4.4709 |
[32m[20230204 15:47:26 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:47:26 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.50
[32m[20230204 15:47:26 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.51
[32m[20230204 15:47:26 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.40
[32m[20230204 15:47:26 @agent_ppo2.py:152][0m Total time:      11.27 min
[32m[20230204 15:47:26 @agent_ppo2.py:154][0m 923648 total steps have happened
[32m[20230204 15:47:26 @agent_ppo2.py:130][0m #------------------------ Iteration 451 --------------------------#
[32m[20230204 15:47:27 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:47:27 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:27 @agent_ppo2.py:194][0m |           0.0031 |          12.9902 |           4.5286 |
[32m[20230204 15:47:27 @agent_ppo2.py:194][0m |          -0.0087 |          11.2483 |           4.5209 |
[32m[20230204 15:47:27 @agent_ppo2.py:194][0m |          -0.0106 |          10.5508 |           4.5190 |
[32m[20230204 15:47:27 @agent_ppo2.py:194][0m |          -0.0099 |          10.1795 |           4.5225 |
[32m[20230204 15:47:27 @agent_ppo2.py:194][0m |          -0.0116 |           9.9814 |           4.5161 |
[32m[20230204 15:47:27 @agent_ppo2.py:194][0m |          -0.0118 |           9.8194 |           4.5165 |
[32m[20230204 15:47:27 @agent_ppo2.py:194][0m |          -0.0140 |           9.6105 |           4.5138 |
[32m[20230204 15:47:27 @agent_ppo2.py:194][0m |          -0.0119 |           9.5055 |           4.5138 |
[32m[20230204 15:47:27 @agent_ppo2.py:194][0m |          -0.0105 |           9.3528 |           4.5126 |
[32m[20230204 15:47:27 @agent_ppo2.py:194][0m |          -0.0138 |           9.2252 |           4.5087 |
[32m[20230204 15:47:27 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:47:28 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.65
[32m[20230204 15:47:28 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.04
[32m[20230204 15:47:28 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.20
[32m[20230204 15:47:28 @agent_ppo2.py:152][0m Total time:      11.29 min
[32m[20230204 15:47:28 @agent_ppo2.py:154][0m 925696 total steps have happened
[32m[20230204 15:47:28 @agent_ppo2.py:130][0m #------------------------ Iteration 452 --------------------------#
[32m[20230204 15:47:28 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:47:28 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:28 @agent_ppo2.py:194][0m |          -0.0034 |          13.0461 |           4.4787 |
[32m[20230204 15:47:28 @agent_ppo2.py:194][0m |          -0.0066 |          11.4019 |           4.4709 |
[32m[20230204 15:47:28 @agent_ppo2.py:194][0m |          -0.0115 |          10.5804 |           4.4677 |
[32m[20230204 15:47:28 @agent_ppo2.py:194][0m |          -0.0125 |          10.1522 |           4.4593 |
[32m[20230204 15:47:28 @agent_ppo2.py:194][0m |          -0.0168 |           9.9045 |           4.4636 |
[32m[20230204 15:47:29 @agent_ppo2.py:194][0m |          -0.0158 |           9.6666 |           4.4586 |
[32m[20230204 15:47:29 @agent_ppo2.py:194][0m |           0.0027 |          10.4062 |           4.4551 |
[32m[20230204 15:47:29 @agent_ppo2.py:194][0m |          -0.0074 |           9.5919 |           4.4569 |
[32m[20230204 15:47:29 @agent_ppo2.py:194][0m |          -0.0153 |           9.1141 |           4.4535 |
[32m[20230204 15:47:29 @agent_ppo2.py:194][0m |          -0.0104 |           8.9636 |           4.4509 |
[32m[20230204 15:47:29 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:47:29 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.83
[32m[20230204 15:47:29 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.40
[32m[20230204 15:47:29 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.61
[32m[20230204 15:47:29 @agent_ppo2.py:152][0m Total time:      11.31 min
[32m[20230204 15:47:29 @agent_ppo2.py:154][0m 927744 total steps have happened
[32m[20230204 15:47:29 @agent_ppo2.py:130][0m #------------------------ Iteration 453 --------------------------#
[32m[20230204 15:47:29 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:47:29 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:29 @agent_ppo2.py:194][0m |          -0.0006 |          14.7962 |           4.4905 |
[32m[20230204 15:47:30 @agent_ppo2.py:194][0m |          -0.0062 |          13.3878 |           4.4801 |
[32m[20230204 15:47:30 @agent_ppo2.py:194][0m |          -0.0078 |          12.9644 |           4.4762 |
[32m[20230204 15:47:30 @agent_ppo2.py:194][0m |          -0.0100 |          12.6965 |           4.4767 |
[32m[20230204 15:47:30 @agent_ppo2.py:194][0m |          -0.0104 |          12.5071 |           4.4775 |
[32m[20230204 15:47:30 @agent_ppo2.py:194][0m |          -0.0106 |          12.3558 |           4.4768 |
[32m[20230204 15:47:30 @agent_ppo2.py:194][0m |          -0.0116 |          12.2459 |           4.4775 |
[32m[20230204 15:47:30 @agent_ppo2.py:194][0m |          -0.0129 |          12.1019 |           4.4756 |
[32m[20230204 15:47:30 @agent_ppo2.py:194][0m |          -0.0129 |          12.0058 |           4.4757 |
[32m[20230204 15:47:30 @agent_ppo2.py:194][0m |          -0.0139 |          11.9121 |           4.4738 |
[32m[20230204 15:47:30 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:30 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.50
[32m[20230204 15:47:30 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.73
[32m[20230204 15:47:30 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.46
[32m[20230204 15:47:30 @agent_ppo2.py:152][0m Total time:      11.33 min
[32m[20230204 15:47:30 @agent_ppo2.py:154][0m 929792 total steps have happened
[32m[20230204 15:47:30 @agent_ppo2.py:130][0m #------------------------ Iteration 454 --------------------------#
[32m[20230204 15:47:31 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:31 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:31 @agent_ppo2.py:194][0m |           0.0009 |          14.7799 |           4.5577 |
[32m[20230204 15:47:31 @agent_ppo2.py:194][0m |          -0.0042 |          12.2426 |           4.5511 |
[32m[20230204 15:47:31 @agent_ppo2.py:194][0m |          -0.0065 |          11.3139 |           4.5523 |
[32m[20230204 15:47:31 @agent_ppo2.py:194][0m |          -0.0079 |          10.7472 |           4.5469 |
[32m[20230204 15:47:31 @agent_ppo2.py:194][0m |          -0.0086 |          10.2786 |           4.5516 |
[32m[20230204 15:47:31 @agent_ppo2.py:194][0m |          -0.0097 |           9.9081 |           4.5483 |
[32m[20230204 15:47:31 @agent_ppo2.py:194][0m |          -0.0106 |           9.6367 |           4.5514 |
[32m[20230204 15:47:31 @agent_ppo2.py:194][0m |          -0.0114 |           9.3717 |           4.5493 |
[32m[20230204 15:47:31 @agent_ppo2.py:194][0m |          -0.0121 |           9.0703 |           4.5528 |
[32m[20230204 15:47:31 @agent_ppo2.py:194][0m |          -0.0125 |           8.8448 |           4.5518 |
[32m[20230204 15:47:31 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:47:32 @agent_ppo2.py:147][0m Average TRAINING episode reward: 262.00
[32m[20230204 15:47:32 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.48
[32m[20230204 15:47:32 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.66
[32m[20230204 15:47:32 @agent_ppo2.py:152][0m Total time:      11.35 min
[32m[20230204 15:47:32 @agent_ppo2.py:154][0m 931840 total steps have happened
[32m[20230204 15:47:32 @agent_ppo2.py:130][0m #------------------------ Iteration 455 --------------------------#
[32m[20230204 15:47:32 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:32 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:32 @agent_ppo2.py:194][0m |           0.0016 |          13.3349 |           4.5901 |
[32m[20230204 15:47:32 @agent_ppo2.py:194][0m |          -0.0053 |          11.2092 |           4.5826 |
[32m[20230204 15:47:32 @agent_ppo2.py:194][0m |          -0.0061 |          10.2282 |           4.5776 |
[32m[20230204 15:47:32 @agent_ppo2.py:194][0m |          -0.0083 |           9.4563 |           4.5731 |
[32m[20230204 15:47:32 @agent_ppo2.py:194][0m |          -0.0091 |           8.8022 |           4.5740 |
[32m[20230204 15:47:32 @agent_ppo2.py:194][0m |          -0.0099 |           8.2936 |           4.5728 |
[32m[20230204 15:47:32 @agent_ppo2.py:194][0m |          -0.0103 |           7.8149 |           4.5730 |
[32m[20230204 15:47:33 @agent_ppo2.py:194][0m |          -0.0103 |           7.3836 |           4.5698 |
[32m[20230204 15:47:33 @agent_ppo2.py:194][0m |          -0.0116 |           6.9402 |           4.5709 |
[32m[20230204 15:47:33 @agent_ppo2.py:194][0m |          -0.0118 |           6.4280 |           4.5663 |
[32m[20230204 15:47:33 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:47:33 @agent_ppo2.py:147][0m Average TRAINING episode reward: 262.47
[32m[20230204 15:47:33 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.17
[32m[20230204 15:47:33 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.67
[32m[20230204 15:47:33 @agent_ppo2.py:152][0m Total time:      11.37 min
[32m[20230204 15:47:33 @agent_ppo2.py:154][0m 933888 total steps have happened
[32m[20230204 15:47:33 @agent_ppo2.py:130][0m #------------------------ Iteration 456 --------------------------#
[32m[20230204 15:47:33 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:33 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:33 @agent_ppo2.py:194][0m |           0.0030 |          12.9751 |           4.5329 |
[32m[20230204 15:47:33 @agent_ppo2.py:194][0m |          -0.0022 |           9.6926 |           4.5265 |
[32m[20230204 15:47:33 @agent_ppo2.py:194][0m |          -0.0077 |           7.5697 |           4.5246 |
[32m[20230204 15:47:34 @agent_ppo2.py:194][0m |          -0.0106 |           6.4233 |           4.5177 |
[32m[20230204 15:47:34 @agent_ppo2.py:194][0m |          -0.0119 |           5.6192 |           4.5194 |
[32m[20230204 15:47:34 @agent_ppo2.py:194][0m |          -0.0134 |           5.0367 |           4.5135 |
[32m[20230204 15:47:34 @agent_ppo2.py:194][0m |          -0.0105 |           4.6493 |           4.5145 |
[32m[20230204 15:47:34 @agent_ppo2.py:194][0m |          -0.0131 |           4.3171 |           4.5158 |
[32m[20230204 15:47:34 @agent_ppo2.py:194][0m |          -0.0130 |           3.9697 |           4.5136 |
[32m[20230204 15:47:34 @agent_ppo2.py:194][0m |          -0.0139 |           3.7715 |           4.5142 |
[32m[20230204 15:47:34 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:47:34 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.89
[32m[20230204 15:47:34 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.24
[32m[20230204 15:47:34 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 69.02
[32m[20230204 15:47:34 @agent_ppo2.py:152][0m Total time:      11.40 min
[32m[20230204 15:47:34 @agent_ppo2.py:154][0m 935936 total steps have happened
[32m[20230204 15:47:34 @agent_ppo2.py:130][0m #------------------------ Iteration 457 --------------------------#
[32m[20230204 15:47:34 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:47:35 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:35 @agent_ppo2.py:194][0m |           0.0007 |          16.7062 |           4.5183 |
[32m[20230204 15:47:35 @agent_ppo2.py:194][0m |          -0.0062 |          15.3096 |           4.5163 |
[32m[20230204 15:47:35 @agent_ppo2.py:194][0m |          -0.0098 |          14.6969 |           4.5172 |
[32m[20230204 15:47:35 @agent_ppo2.py:194][0m |          -0.0036 |          14.4344 |           4.5201 |
[32m[20230204 15:47:35 @agent_ppo2.py:194][0m |          -0.0108 |          13.9474 |           4.5183 |
[32m[20230204 15:47:35 @agent_ppo2.py:194][0m |          -0.0071 |          13.9689 |           4.5247 |
[32m[20230204 15:47:35 @agent_ppo2.py:194][0m |          -0.0107 |          13.5513 |           4.5205 |
[32m[20230204 15:47:35 @agent_ppo2.py:194][0m |          -0.0109 |          13.4243 |           4.5191 |
[32m[20230204 15:47:35 @agent_ppo2.py:194][0m |          -0.0116 |          13.3131 |           4.5230 |
[32m[20230204 15:47:35 @agent_ppo2.py:194][0m |          -0.0137 |          13.1121 |           4.5253 |
[32m[20230204 15:47:35 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:35 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.33
[32m[20230204 15:47:35 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.36
[32m[20230204 15:47:35 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.77
[32m[20230204 15:47:35 @agent_ppo2.py:152][0m Total time:      11.42 min
[32m[20230204 15:47:35 @agent_ppo2.py:154][0m 937984 total steps have happened
[32m[20230204 15:47:35 @agent_ppo2.py:130][0m #------------------------ Iteration 458 --------------------------#
[32m[20230204 15:47:36 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:36 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:36 @agent_ppo2.py:194][0m |          -0.0075 |          14.4233 |           4.5355 |
[32m[20230204 15:47:36 @agent_ppo2.py:194][0m |          -0.0086 |          13.3903 |           4.5293 |
[32m[20230204 15:47:36 @agent_ppo2.py:194][0m |          -0.0100 |          12.9671 |           4.5252 |
[32m[20230204 15:47:36 @agent_ppo2.py:194][0m |          -0.0089 |          12.6564 |           4.5214 |
[32m[20230204 15:47:36 @agent_ppo2.py:194][0m |          -0.0195 |          12.4455 |           4.5188 |
[32m[20230204 15:47:36 @agent_ppo2.py:194][0m |          -0.0136 |          12.2571 |           4.5162 |
[32m[20230204 15:47:36 @agent_ppo2.py:194][0m |          -0.0105 |          12.0397 |           4.5190 |
[32m[20230204 15:47:36 @agent_ppo2.py:194][0m |          -0.0052 |          11.9005 |           4.5148 |
[32m[20230204 15:47:36 @agent_ppo2.py:194][0m |          -0.0166 |          11.6943 |           4.5147 |
[32m[20230204 15:47:37 @agent_ppo2.py:194][0m |          -0.0076 |          11.5349 |           4.5120 |
[32m[20230204 15:47:37 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:47:37 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.08
[32m[20230204 15:47:37 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.71
[32m[20230204 15:47:37 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.88
[32m[20230204 15:47:37 @agent_ppo2.py:152][0m Total time:      11.44 min
[32m[20230204 15:47:37 @agent_ppo2.py:154][0m 940032 total steps have happened
[32m[20230204 15:47:37 @agent_ppo2.py:130][0m #------------------------ Iteration 459 --------------------------#
[32m[20230204 15:47:37 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:37 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:37 @agent_ppo2.py:194][0m |          -0.0000 |          14.3003 |           4.4193 |
[32m[20230204 15:47:37 @agent_ppo2.py:194][0m |          -0.0067 |          12.3350 |           4.4111 |
[32m[20230204 15:47:37 @agent_ppo2.py:194][0m |          -0.0078 |          11.6627 |           4.4120 |
[32m[20230204 15:47:37 @agent_ppo2.py:194][0m |          -0.0101 |          10.9888 |           4.4094 |
[32m[20230204 15:47:37 @agent_ppo2.py:194][0m |          -0.0124 |          10.5588 |           4.4108 |
[32m[20230204 15:47:38 @agent_ppo2.py:194][0m |          -0.0111 |          10.0562 |           4.4080 |
[32m[20230204 15:47:38 @agent_ppo2.py:194][0m |          -0.0124 |           9.7288 |           4.4050 |
[32m[20230204 15:47:38 @agent_ppo2.py:194][0m |          -0.0143 |           9.3592 |           4.4079 |
[32m[20230204 15:47:38 @agent_ppo2.py:194][0m |          -0.0121 |           9.1039 |           4.4047 |
[32m[20230204 15:47:38 @agent_ppo2.py:194][0m |          -0.0126 |           8.8358 |           4.4003 |
[32m[20230204 15:47:38 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:47:38 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.96
[32m[20230204 15:47:38 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 259.73
[32m[20230204 15:47:38 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.58
[32m[20230204 15:47:38 @agent_ppo2.py:152][0m Total time:      11.46 min
[32m[20230204 15:47:38 @agent_ppo2.py:154][0m 942080 total steps have happened
[32m[20230204 15:47:38 @agent_ppo2.py:130][0m #------------------------ Iteration 460 --------------------------#
[32m[20230204 15:47:38 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:47:38 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:38 @agent_ppo2.py:194][0m |          -0.0025 |          10.4230 |           4.5363 |
[32m[20230204 15:47:39 @agent_ppo2.py:194][0m |          -0.0048 |           7.8575 |           4.5279 |
[32m[20230204 15:47:39 @agent_ppo2.py:194][0m |          -0.0057 |           6.3381 |           4.5289 |
[32m[20230204 15:47:39 @agent_ppo2.py:194][0m |          -0.0081 |           5.4526 |           4.5341 |
[32m[20230204 15:47:39 @agent_ppo2.py:194][0m |          -0.0120 |           4.8551 |           4.5347 |
[32m[20230204 15:47:39 @agent_ppo2.py:194][0m |          -0.0108 |           4.3722 |           4.5348 |
[32m[20230204 15:47:39 @agent_ppo2.py:194][0m |          -0.0123 |           3.9297 |           4.5368 |
[32m[20230204 15:47:39 @agent_ppo2.py:194][0m |          -0.0148 |           3.6244 |           4.5356 |
[32m[20230204 15:47:39 @agent_ppo2.py:194][0m |          -0.0112 |           3.5265 |           4.5386 |
[32m[20230204 15:47:39 @agent_ppo2.py:194][0m |          -0.0162 |           3.2976 |           4.5373 |
[32m[20230204 15:47:39 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:47:39 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.14
[32m[20230204 15:47:39 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.73
[32m[20230204 15:47:39 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.54
[32m[20230204 15:47:39 @agent_ppo2.py:152][0m Total time:      11.48 min
[32m[20230204 15:47:39 @agent_ppo2.py:154][0m 944128 total steps have happened
[32m[20230204 15:47:39 @agent_ppo2.py:130][0m #------------------------ Iteration 461 --------------------------#
[32m[20230204 15:47:40 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:47:40 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:40 @agent_ppo2.py:194][0m |          -0.0008 |          15.1690 |           4.5285 |
[32m[20230204 15:47:40 @agent_ppo2.py:194][0m |          -0.0059 |          13.3587 |           4.5151 |
[32m[20230204 15:47:40 @agent_ppo2.py:194][0m |          -0.0074 |          12.0152 |           4.5079 |
[32m[20230204 15:47:40 @agent_ppo2.py:194][0m |          -0.0085 |          11.4593 |           4.5068 |
[32m[20230204 15:47:40 @agent_ppo2.py:194][0m |          -0.0081 |          11.1505 |           4.5024 |
[32m[20230204 15:47:40 @agent_ppo2.py:194][0m |          -0.0105 |          10.4945 |           4.4991 |
[32m[20230204 15:47:40 @agent_ppo2.py:194][0m |          -0.0106 |          10.0234 |           4.4920 |
[32m[20230204 15:47:40 @agent_ppo2.py:194][0m |          -0.0086 |           9.6376 |           4.4948 |
[32m[20230204 15:47:40 @agent_ppo2.py:194][0m |          -0.0121 |           9.3954 |           4.4903 |
[32m[20230204 15:47:40 @agent_ppo2.py:194][0m |          -0.0121 |           9.2926 |           4.4868 |
[32m[20230204 15:47:40 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:47:41 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.15
[32m[20230204 15:47:41 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.90
[32m[20230204 15:47:41 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.94
[32m[20230204 15:47:41 @agent_ppo2.py:152][0m Total time:      11.50 min
[32m[20230204 15:47:41 @agent_ppo2.py:154][0m 946176 total steps have happened
[32m[20230204 15:47:41 @agent_ppo2.py:130][0m #------------------------ Iteration 462 --------------------------#
[32m[20230204 15:47:41 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:41 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:41 @agent_ppo2.py:194][0m |           0.0038 |          26.0736 |           4.5352 |
[32m[20230204 15:47:41 @agent_ppo2.py:194][0m |          -0.0017 |          16.9158 |           4.5258 |
[32m[20230204 15:47:41 @agent_ppo2.py:194][0m |          -0.0050 |          14.7456 |           4.5215 |
[32m[20230204 15:47:41 @agent_ppo2.py:194][0m |          -0.0070 |          13.5588 |           4.5152 |
[32m[20230204 15:47:41 @agent_ppo2.py:194][0m |          -0.0085 |          12.9549 |           4.5152 |
[32m[20230204 15:47:41 @agent_ppo2.py:194][0m |          -0.0088 |          12.4851 |           4.5161 |
[32m[20230204 15:47:42 @agent_ppo2.py:194][0m |          -0.0103 |          12.0674 |           4.5138 |
[32m[20230204 15:47:42 @agent_ppo2.py:194][0m |          -0.0106 |          11.8182 |           4.5162 |
[32m[20230204 15:47:42 @agent_ppo2.py:194][0m |          -0.0117 |          11.5313 |           4.5117 |
[32m[20230204 15:47:42 @agent_ppo2.py:194][0m |          -0.0121 |          11.3288 |           4.5123 |
[32m[20230204 15:47:42 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:47:42 @agent_ppo2.py:147][0m Average TRAINING episode reward: 230.23
[32m[20230204 15:47:42 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.95
[32m[20230204 15:47:42 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.81
[32m[20230204 15:47:42 @agent_ppo2.py:152][0m Total time:      11.53 min
[32m[20230204 15:47:42 @agent_ppo2.py:154][0m 948224 total steps have happened
[32m[20230204 15:47:42 @agent_ppo2.py:130][0m #------------------------ Iteration 463 --------------------------#
[32m[20230204 15:47:42 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:47:42 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:42 @agent_ppo2.py:194][0m |           0.0063 |          13.7525 |           4.4347 |
[32m[20230204 15:47:42 @agent_ppo2.py:194][0m |          -0.0006 |          11.8841 |           4.4287 |
[32m[20230204 15:47:43 @agent_ppo2.py:194][0m |          -0.0133 |          10.6300 |           4.4199 |
[32m[20230204 15:47:43 @agent_ppo2.py:194][0m |           0.0040 |          10.0367 |           4.4187 |
[32m[20230204 15:47:43 @agent_ppo2.py:194][0m |          -0.0137 |           9.1095 |           4.4166 |
[32m[20230204 15:47:43 @agent_ppo2.py:194][0m |          -0.0130 |           8.3552 |           4.4169 |
[32m[20230204 15:47:43 @agent_ppo2.py:194][0m |          -0.0063 |           7.9519 |           4.4138 |
[32m[20230204 15:47:43 @agent_ppo2.py:194][0m |          -0.0077 |           7.5878 |           4.4139 |
[32m[20230204 15:47:43 @agent_ppo2.py:194][0m |          -0.0277 |           7.4635 |           4.4132 |
[32m[20230204 15:47:43 @agent_ppo2.py:194][0m |          -0.0057 |           7.0125 |           4.4047 |
[32m[20230204 15:47:43 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:47:43 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.13
[32m[20230204 15:47:43 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.71
[32m[20230204 15:47:43 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.62
[32m[20230204 15:47:43 @agent_ppo2.py:152][0m Total time:      11.55 min
[32m[20230204 15:47:43 @agent_ppo2.py:154][0m 950272 total steps have happened
[32m[20230204 15:47:43 @agent_ppo2.py:130][0m #------------------------ Iteration 464 --------------------------#
[32m[20230204 15:47:44 @agent_ppo2.py:136][0m Sampling time: 0.28 s by 4 slaves
[32m[20230204 15:47:44 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:44 @agent_ppo2.py:194][0m |          -0.0003 |          35.0098 |           4.4145 |
[32m[20230204 15:47:44 @agent_ppo2.py:194][0m |          -0.0076 |          26.2300 |           4.4033 |
[32m[20230204 15:47:44 @agent_ppo2.py:194][0m |          -0.0102 |          23.8538 |           4.4017 |
[32m[20230204 15:47:44 @agent_ppo2.py:194][0m |          -0.0112 |          22.2500 |           4.3922 |
[32m[20230204 15:47:44 @agent_ppo2.py:194][0m |          -0.0112 |          21.4448 |           4.3960 |
[32m[20230204 15:47:44 @agent_ppo2.py:194][0m |          -0.0123 |          20.7261 |           4.3895 |
[32m[20230204 15:47:44 @agent_ppo2.py:194][0m |          -0.0131 |          20.0007 |           4.3894 |
[32m[20230204 15:47:44 @agent_ppo2.py:194][0m |          -0.0134 |          19.5214 |           4.3903 |
[32m[20230204 15:47:44 @agent_ppo2.py:194][0m |          -0.0148 |          18.9726 |           4.3854 |
[32m[20230204 15:47:44 @agent_ppo2.py:194][0m |          -0.0144 |          18.5246 |           4.3910 |
[32m[20230204 15:47:44 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:45 @agent_ppo2.py:147][0m Average TRAINING episode reward: 158.35
[32m[20230204 15:47:45 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.61
[32m[20230204 15:47:45 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.36
[32m[20230204 15:47:45 @agent_ppo2.py:152][0m Total time:      11.57 min
[32m[20230204 15:47:45 @agent_ppo2.py:154][0m 952320 total steps have happened
[32m[20230204 15:47:45 @agent_ppo2.py:130][0m #------------------------ Iteration 465 --------------------------#
[32m[20230204 15:47:45 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:45 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:45 @agent_ppo2.py:194][0m |           0.0014 |          17.6773 |           4.5368 |
[32m[20230204 15:47:45 @agent_ppo2.py:194][0m |          -0.0047 |          13.9730 |           4.5330 |
[32m[20230204 15:47:45 @agent_ppo2.py:194][0m |          -0.0075 |          12.6515 |           4.5261 |
[32m[20230204 15:47:45 @agent_ppo2.py:194][0m |          -0.0071 |          11.7499 |           4.5237 |
[32m[20230204 15:47:45 @agent_ppo2.py:194][0m |          -0.0095 |          11.1035 |           4.5234 |
[32m[20230204 15:47:45 @agent_ppo2.py:194][0m |          -0.0092 |          10.5619 |           4.5261 |
[32m[20230204 15:47:46 @agent_ppo2.py:194][0m |          -0.0116 |          10.0414 |           4.5222 |
[32m[20230204 15:47:46 @agent_ppo2.py:194][0m |          -0.0118 |           9.6853 |           4.5245 |
[32m[20230204 15:47:46 @agent_ppo2.py:194][0m |          -0.0131 |           9.2795 |           4.5261 |
[32m[20230204 15:47:46 @agent_ppo2.py:194][0m |          -0.0126 |           9.0080 |           4.5212 |
[32m[20230204 15:47:46 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:47:46 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.08
[32m[20230204 15:47:46 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.61
[32m[20230204 15:47:46 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 266.96
[32m[20230204 15:47:46 @agent_ppo2.py:152][0m Total time:      11.59 min
[32m[20230204 15:47:46 @agent_ppo2.py:154][0m 954368 total steps have happened
[32m[20230204 15:47:46 @agent_ppo2.py:130][0m #------------------------ Iteration 466 --------------------------#
[32m[20230204 15:47:46 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:46 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:46 @agent_ppo2.py:194][0m |           0.0037 |          17.8862 |           4.4446 |
[32m[20230204 15:47:46 @agent_ppo2.py:194][0m |          -0.0062 |          15.7796 |           4.4263 |
[32m[20230204 15:47:47 @agent_ppo2.py:194][0m |          -0.0040 |          15.9949 |           4.4267 |
[32m[20230204 15:47:47 @agent_ppo2.py:194][0m |          -0.0048 |          15.2573 |           4.4224 |
[32m[20230204 15:47:47 @agent_ppo2.py:194][0m |          -0.0106 |          14.9013 |           4.4260 |
[32m[20230204 15:47:47 @agent_ppo2.py:194][0m |          -0.0121 |          14.7355 |           4.4218 |
[32m[20230204 15:47:47 @agent_ppo2.py:194][0m |          -0.0115 |          14.6003 |           4.4204 |
[32m[20230204 15:47:47 @agent_ppo2.py:194][0m |          -0.0139 |          14.4984 |           4.4213 |
[32m[20230204 15:47:47 @agent_ppo2.py:194][0m |          -0.0097 |          14.5359 |           4.4180 |
[32m[20230204 15:47:47 @agent_ppo2.py:194][0m |          -0.0115 |          14.5011 |           4.4150 |
[32m[20230204 15:47:47 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:47 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.39
[32m[20230204 15:47:47 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.58
[32m[20230204 15:47:47 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.78
[32m[20230204 15:47:47 @agent_ppo2.py:152][0m Total time:      11.61 min
[32m[20230204 15:47:47 @agent_ppo2.py:154][0m 956416 total steps have happened
[32m[20230204 15:47:47 @agent_ppo2.py:130][0m #------------------------ Iteration 467 --------------------------#
[32m[20230204 15:47:47 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:48 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:48 @agent_ppo2.py:194][0m |           0.0023 |          13.6419 |           4.3664 |
[32m[20230204 15:47:48 @agent_ppo2.py:194][0m |          -0.0045 |          10.2044 |           4.3551 |
[32m[20230204 15:47:48 @agent_ppo2.py:194][0m |          -0.0027 |           8.3647 |           4.3516 |
[32m[20230204 15:47:48 @agent_ppo2.py:194][0m |          -0.0088 |           6.8605 |           4.3499 |
[32m[20230204 15:47:48 @agent_ppo2.py:194][0m |          -0.0094 |           5.7245 |           4.3497 |
[32m[20230204 15:47:48 @agent_ppo2.py:194][0m |          -0.0074 |           5.2755 |           4.3442 |
[32m[20230204 15:47:48 @agent_ppo2.py:194][0m |          -0.0072 |           4.8455 |           4.3453 |
[32m[20230204 15:47:48 @agent_ppo2.py:194][0m |          -0.0081 |           4.3454 |           4.3455 |
[32m[20230204 15:47:48 @agent_ppo2.py:194][0m |          -0.0131 |           3.6227 |           4.3453 |
[32m[20230204 15:47:48 @agent_ppo2.py:194][0m |          -0.0124 |           3.3336 |           4.3420 |
[32m[20230204 15:47:48 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:47:49 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.89
[32m[20230204 15:47:49 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.15
[32m[20230204 15:47:49 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 264.80
[32m[20230204 15:47:49 @agent_ppo2.py:152][0m Total time:      11.63 min
[32m[20230204 15:47:49 @agent_ppo2.py:154][0m 958464 total steps have happened
[32m[20230204 15:47:49 @agent_ppo2.py:130][0m #------------------------ Iteration 468 --------------------------#
[32m[20230204 15:47:49 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:47:49 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:49 @agent_ppo2.py:194][0m |          -0.0025 |          16.1932 |           4.3352 |
[32m[20230204 15:47:49 @agent_ppo2.py:194][0m |          -0.0023 |          15.0913 |           4.3243 |
[32m[20230204 15:47:49 @agent_ppo2.py:194][0m |          -0.0000 |          14.4826 |           4.3156 |
[32m[20230204 15:47:49 @agent_ppo2.py:194][0m |          -0.0091 |          13.6625 |           4.3149 |
[32m[20230204 15:47:49 @agent_ppo2.py:194][0m |          -0.0108 |          13.1398 |           4.3164 |
[32m[20230204 15:47:49 @agent_ppo2.py:194][0m |          -0.0099 |          12.5501 |           4.3145 |
[32m[20230204 15:47:49 @agent_ppo2.py:194][0m |          -0.0132 |          12.0922 |           4.3138 |
[32m[20230204 15:47:49 @agent_ppo2.py:194][0m |          -0.0113 |          11.7828 |           4.3174 |
[32m[20230204 15:47:50 @agent_ppo2.py:194][0m |          -0.0127 |          11.5450 |           4.3192 |
[32m[20230204 15:47:50 @agent_ppo2.py:194][0m |          -0.0061 |          12.2343 |           4.3155 |
[32m[20230204 15:47:50 @agent_ppo2.py:139][0m Policy update time: 0.87 s
[32m[20230204 15:47:50 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.59
[32m[20230204 15:47:50 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 260.71
[32m[20230204 15:47:50 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.50
[32m[20230204 15:47:50 @agent_ppo2.py:152][0m Total time:      11.66 min
[32m[20230204 15:47:50 @agent_ppo2.py:154][0m 960512 total steps have happened
[32m[20230204 15:47:50 @agent_ppo2.py:130][0m #------------------------ Iteration 469 --------------------------#
[32m[20230204 15:47:50 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:47:50 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:50 @agent_ppo2.py:194][0m |           0.0020 |          12.3607 |           4.4119 |
[32m[20230204 15:47:50 @agent_ppo2.py:194][0m |          -0.0057 |           9.9166 |           4.4020 |
[32m[20230204 15:47:50 @agent_ppo2.py:194][0m |          -0.0077 |           8.8026 |           4.4010 |
[32m[20230204 15:47:50 @agent_ppo2.py:194][0m |          -0.0099 |           8.2385 |           4.3965 |
[32m[20230204 15:47:51 @agent_ppo2.py:194][0m |          -0.0093 |           7.6069 |           4.4014 |
[32m[20230204 15:47:51 @agent_ppo2.py:194][0m |          -0.0103 |           7.2036 |           4.3975 |
[32m[20230204 15:47:51 @agent_ppo2.py:194][0m |          -0.0114 |           6.7643 |           4.3960 |
[32m[20230204 15:47:51 @agent_ppo2.py:194][0m |          -0.0120 |           6.4409 |           4.3956 |
[32m[20230204 15:47:51 @agent_ppo2.py:194][0m |          -0.0134 |           6.1541 |           4.3967 |
[32m[20230204 15:47:51 @agent_ppo2.py:194][0m |          -0.0136 |           5.9271 |           4.3915 |
[32m[20230204 15:47:51 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:51 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.98
[32m[20230204 15:47:51 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.24
[32m[20230204 15:47:51 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.75
[32m[20230204 15:47:51 @agent_ppo2.py:152][0m Total time:      11.68 min
[32m[20230204 15:47:51 @agent_ppo2.py:154][0m 962560 total steps have happened
[32m[20230204 15:47:51 @agent_ppo2.py:130][0m #------------------------ Iteration 470 --------------------------#
[32m[20230204 15:47:51 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:51 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:52 @agent_ppo2.py:194][0m |           0.0023 |          14.9689 |           4.3864 |
[32m[20230204 15:47:52 @agent_ppo2.py:194][0m |          -0.0010 |          12.1638 |           4.3779 |
[32m[20230204 15:47:52 @agent_ppo2.py:194][0m |          -0.0071 |          11.1744 |           4.3805 |
[32m[20230204 15:47:52 @agent_ppo2.py:194][0m |          -0.0090 |          10.2971 |           4.3797 |
[32m[20230204 15:47:52 @agent_ppo2.py:194][0m |           0.0024 |           9.7292 |           4.3815 |
[32m[20230204 15:47:52 @agent_ppo2.py:194][0m |          -0.0138 |           9.3656 |           4.3791 |
[32m[20230204 15:47:52 @agent_ppo2.py:194][0m |          -0.0041 |           8.9008 |           4.3763 |
[32m[20230204 15:47:52 @agent_ppo2.py:194][0m |          -0.0147 |           8.6036 |           4.3769 |
[32m[20230204 15:47:52 @agent_ppo2.py:194][0m |          -0.0337 |           8.4685 |           4.3830 |
[32m[20230204 15:47:52 @agent_ppo2.py:194][0m |          -0.0124 |           8.0243 |           4.3813 |
[32m[20230204 15:47:52 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:47:52 @agent_ppo2.py:147][0m Average TRAINING episode reward: 260.68
[32m[20230204 15:47:52 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.39
[32m[20230204 15:47:52 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.13
[32m[20230204 15:47:52 @agent_ppo2.py:152][0m Total time:      11.70 min
[32m[20230204 15:47:52 @agent_ppo2.py:154][0m 964608 total steps have happened
[32m[20230204 15:47:52 @agent_ppo2.py:130][0m #------------------------ Iteration 471 --------------------------#
[32m[20230204 15:47:53 @agent_ppo2.py:136][0m Sampling time: 0.26 s by 4 slaves
[32m[20230204 15:47:53 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:53 @agent_ppo2.py:194][0m |           0.0013 |          15.0663 |           4.3904 |
[32m[20230204 15:47:53 @agent_ppo2.py:194][0m |          -0.0033 |          12.7759 |           4.3848 |
[32m[20230204 15:47:53 @agent_ppo2.py:194][0m |          -0.0059 |          11.9145 |           4.3872 |
[32m[20230204 15:47:53 @agent_ppo2.py:194][0m |          -0.0071 |          11.3931 |           4.3826 |
[32m[20230204 15:47:53 @agent_ppo2.py:194][0m |          -0.0085 |          10.9561 |           4.3871 |
[32m[20230204 15:47:53 @agent_ppo2.py:194][0m |          -0.0092 |          10.5845 |           4.3922 |
[32m[20230204 15:47:53 @agent_ppo2.py:194][0m |          -0.0098 |          10.1966 |           4.3886 |
[32m[20230204 15:47:53 @agent_ppo2.py:194][0m |          -0.0094 |           9.8425 |           4.3850 |
[32m[20230204 15:47:53 @agent_ppo2.py:194][0m |          -0.0110 |           9.4774 |           4.3890 |
[32m[20230204 15:47:54 @agent_ppo2.py:194][0m |          -0.0120 |           9.2122 |           4.3923 |
[32m[20230204 15:47:54 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:47:54 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.04
[32m[20230204 15:47:54 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.54
[32m[20230204 15:47:54 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.45
[32m[20230204 15:47:54 @agent_ppo2.py:152][0m Total time:      11.72 min
[32m[20230204 15:47:54 @agent_ppo2.py:154][0m 966656 total steps have happened
[32m[20230204 15:47:54 @agent_ppo2.py:130][0m #------------------------ Iteration 472 --------------------------#
[32m[20230204 15:47:54 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:54 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:54 @agent_ppo2.py:194][0m |           0.0004 |          17.2302 |           4.3470 |
[32m[20230204 15:47:54 @agent_ppo2.py:194][0m |          -0.0054 |          15.0012 |           4.3426 |
[32m[20230204 15:47:54 @agent_ppo2.py:194][0m |          -0.0069 |          14.3900 |           4.3464 |
[32m[20230204 15:47:54 @agent_ppo2.py:194][0m |          -0.0036 |          14.2096 |           4.3406 |
[32m[20230204 15:47:54 @agent_ppo2.py:194][0m |          -0.0072 |          13.5177 |           4.3464 |
[32m[20230204 15:47:54 @agent_ppo2.py:194][0m |          -0.0106 |          13.3234 |           4.3447 |
[32m[20230204 15:47:55 @agent_ppo2.py:194][0m |          -0.0098 |          13.0728 |           4.3435 |
[32m[20230204 15:47:55 @agent_ppo2.py:194][0m |          -0.0106 |          13.0752 |           4.3447 |
[32m[20230204 15:47:55 @agent_ppo2.py:194][0m |          -0.0122 |          12.7526 |           4.3438 |
[32m[20230204 15:47:55 @agent_ppo2.py:194][0m |          -0.0088 |          12.8594 |           4.3461 |
[32m[20230204 15:47:55 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:47:55 @agent_ppo2.py:147][0m Average TRAINING episode reward: 262.02
[32m[20230204 15:47:55 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.28
[32m[20230204 15:47:55 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.81
[32m[20230204 15:47:55 @agent_ppo2.py:152][0m Total time:      11.74 min
[32m[20230204 15:47:55 @agent_ppo2.py:154][0m 968704 total steps have happened
[32m[20230204 15:47:55 @agent_ppo2.py:130][0m #------------------------ Iteration 473 --------------------------#
[32m[20230204 15:47:55 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:55 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:55 @agent_ppo2.py:194][0m |           0.0005 |          20.5697 |           4.4482 |
[32m[20230204 15:47:55 @agent_ppo2.py:194][0m |          -0.0061 |          15.3406 |           4.4484 |
[32m[20230204 15:47:55 @agent_ppo2.py:194][0m |          -0.0105 |          14.4487 |           4.4460 |
[32m[20230204 15:47:56 @agent_ppo2.py:194][0m |          -0.0084 |          14.0283 |           4.4425 |
[32m[20230204 15:47:56 @agent_ppo2.py:194][0m |          -0.0128 |          13.6406 |           4.4390 |
[32m[20230204 15:47:56 @agent_ppo2.py:194][0m |          -0.0121 |          13.4331 |           4.4418 |
[32m[20230204 15:47:56 @agent_ppo2.py:194][0m |          -0.0141 |          13.0748 |           4.4404 |
[32m[20230204 15:47:56 @agent_ppo2.py:194][0m |          -0.0142 |          13.0645 |           4.4409 |
[32m[20230204 15:47:56 @agent_ppo2.py:194][0m |          -0.0155 |          12.6912 |           4.4427 |
[32m[20230204 15:47:56 @agent_ppo2.py:194][0m |          -0.0156 |          12.4665 |           4.4435 |
[32m[20230204 15:47:56 @agent_ppo2.py:139][0m Policy update time: 0.75 s
[32m[20230204 15:47:56 @agent_ppo2.py:147][0m Average TRAINING episode reward: 197.08
[32m[20230204 15:47:56 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.99
[32m[20230204 15:47:56 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.79
[32m[20230204 15:47:56 @agent_ppo2.py:152][0m Total time:      11.76 min
[32m[20230204 15:47:56 @agent_ppo2.py:154][0m 970752 total steps have happened
[32m[20230204 15:47:56 @agent_ppo2.py:130][0m #------------------------ Iteration 474 --------------------------#
[32m[20230204 15:47:56 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:47:57 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:57 @agent_ppo2.py:194][0m |          -0.0078 |          16.5751 |           4.4353 |
[32m[20230204 15:47:57 @agent_ppo2.py:194][0m |          -0.0077 |          15.2380 |           4.4358 |
[32m[20230204 15:47:57 @agent_ppo2.py:194][0m |          -0.0073 |          14.8212 |           4.4315 |
[32m[20230204 15:47:57 @agent_ppo2.py:194][0m |          -0.0065 |          14.4521 |           4.4292 |
[32m[20230204 15:47:57 @agent_ppo2.py:194][0m |          -0.0129 |          14.1128 |           4.4240 |
[32m[20230204 15:47:57 @agent_ppo2.py:194][0m |          -0.0096 |          13.8986 |           4.4279 |
[32m[20230204 15:47:57 @agent_ppo2.py:194][0m |          -0.0102 |          13.7312 |           4.4285 |
[32m[20230204 15:47:57 @agent_ppo2.py:194][0m |          -0.0140 |          13.5603 |           4.4261 |
[32m[20230204 15:47:57 @agent_ppo2.py:194][0m |          -0.0150 |          13.4514 |           4.4269 |
[32m[20230204 15:47:57 @agent_ppo2.py:194][0m |          -0.0160 |          13.2587 |           4.4226 |
[32m[20230204 15:47:57 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:47:57 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.79
[32m[20230204 15:47:57 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.51
[32m[20230204 15:47:57 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.97
[32m[20230204 15:47:57 @agent_ppo2.py:152][0m Total time:      11.78 min
[32m[20230204 15:47:57 @agent_ppo2.py:154][0m 972800 total steps have happened
[32m[20230204 15:47:57 @agent_ppo2.py:130][0m #------------------------ Iteration 475 --------------------------#
[32m[20230204 15:47:58 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:58 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:58 @agent_ppo2.py:194][0m |          -0.0012 |          23.8560 |           4.3290 |
[32m[20230204 15:47:58 @agent_ppo2.py:194][0m |          -0.0096 |          16.5520 |           4.3201 |
[32m[20230204 15:47:58 @agent_ppo2.py:194][0m |          -0.0094 |          15.4673 |           4.3160 |
[32m[20230204 15:47:58 @agent_ppo2.py:194][0m |          -0.0028 |          15.3625 |           4.3191 |
[32m[20230204 15:47:58 @agent_ppo2.py:194][0m |          -0.0078 |          14.4646 |           4.3163 |
[32m[20230204 15:47:58 @agent_ppo2.py:194][0m |          -0.0113 |          14.1555 |           4.3124 |
[32m[20230204 15:47:58 @agent_ppo2.py:194][0m |          -0.0102 |          13.9793 |           4.3133 |
[32m[20230204 15:47:58 @agent_ppo2.py:194][0m |          -0.0037 |          15.1005 |           4.3172 |
[32m[20230204 15:47:58 @agent_ppo2.py:194][0m |          -0.0153 |          13.4572 |           4.3106 |
[32m[20230204 15:47:58 @agent_ppo2.py:194][0m |          -0.0073 |          14.2202 |           4.3157 |
[32m[20230204 15:47:58 @agent_ppo2.py:139][0m Policy update time: 0.78 s
[32m[20230204 15:47:59 @agent_ppo2.py:147][0m Average TRAINING episode reward: 212.47
[32m[20230204 15:47:59 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.34
[32m[20230204 15:47:59 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.33
[32m[20230204 15:47:59 @agent_ppo2.py:152][0m Total time:      11.80 min
[32m[20230204 15:47:59 @agent_ppo2.py:154][0m 974848 total steps have happened
[32m[20230204 15:47:59 @agent_ppo2.py:130][0m #------------------------ Iteration 476 --------------------------#
[32m[20230204 15:47:59 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:47:59 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:47:59 @agent_ppo2.py:194][0m |          -0.0000 |          15.1360 |           4.4475 |
[32m[20230204 15:47:59 @agent_ppo2.py:194][0m |          -0.0079 |          13.6548 |           4.4446 |
[32m[20230204 15:47:59 @agent_ppo2.py:194][0m |          -0.0097 |          13.0988 |           4.4432 |
[32m[20230204 15:47:59 @agent_ppo2.py:194][0m |          -0.0124 |          12.6486 |           4.4393 |
[32m[20230204 15:47:59 @agent_ppo2.py:194][0m |          -0.0137 |          12.4591 |           4.4431 |
[32m[20230204 15:47:59 @agent_ppo2.py:194][0m |          -0.0115 |          12.3466 |           4.4423 |
[32m[20230204 15:48:00 @agent_ppo2.py:194][0m |          -0.0136 |          12.1374 |           4.4434 |
[32m[20230204 15:48:00 @agent_ppo2.py:194][0m |          -0.0142 |          12.0222 |           4.4428 |
[32m[20230204 15:48:00 @agent_ppo2.py:194][0m |          -0.0141 |          11.9543 |           4.4460 |
[32m[20230204 15:48:00 @agent_ppo2.py:194][0m |          -0.0153 |          11.8639 |           4.4473 |
[32m[20230204 15:48:00 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:48:00 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.38
[32m[20230204 15:48:00 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.08
[32m[20230204 15:48:00 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.74
[32m[20230204 15:48:00 @agent_ppo2.py:152][0m Total time:      11.83 min
[32m[20230204 15:48:00 @agent_ppo2.py:154][0m 976896 total steps have happened
[32m[20230204 15:48:00 @agent_ppo2.py:130][0m #------------------------ Iteration 477 --------------------------#
[32m[20230204 15:48:00 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:48:00 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:00 @agent_ppo2.py:194][0m |           0.0032 |          15.5275 |           4.4925 |
[32m[20230204 15:48:00 @agent_ppo2.py:194][0m |          -0.0022 |          14.2215 |           4.4819 |
[32m[20230204 15:48:00 @agent_ppo2.py:194][0m |          -0.0043 |          13.6348 |           4.4795 |
[32m[20230204 15:48:01 @agent_ppo2.py:194][0m |          -0.0046 |          13.3256 |           4.4825 |
[32m[20230204 15:48:01 @agent_ppo2.py:194][0m |          -0.0064 |          13.0049 |           4.4762 |
[32m[20230204 15:48:01 @agent_ppo2.py:194][0m |          -0.0063 |          12.8673 |           4.4832 |
[32m[20230204 15:48:01 @agent_ppo2.py:194][0m |          -0.0088 |          12.5454 |           4.4787 |
[32m[20230204 15:48:01 @agent_ppo2.py:194][0m |          -0.0105 |          12.3073 |           4.4786 |
[32m[20230204 15:48:01 @agent_ppo2.py:194][0m |          -0.0095 |          12.1255 |           4.4762 |
[32m[20230204 15:48:01 @agent_ppo2.py:194][0m |          -0.0116 |          11.9149 |           4.4776 |
[32m[20230204 15:48:01 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:48:01 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.00
[32m[20230204 15:48:01 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.82
[32m[20230204 15:48:01 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.93
[32m[20230204 15:48:01 @agent_ppo2.py:152][0m Total time:      11.85 min
[32m[20230204 15:48:01 @agent_ppo2.py:154][0m 978944 total steps have happened
[32m[20230204 15:48:01 @agent_ppo2.py:130][0m #------------------------ Iteration 478 --------------------------#
[32m[20230204 15:48:01 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:48:02 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:02 @agent_ppo2.py:194][0m |           0.0007 |          22.5233 |           4.4682 |
[32m[20230204 15:48:02 @agent_ppo2.py:194][0m |          -0.0080 |          15.3435 |           4.4576 |
[32m[20230204 15:48:02 @agent_ppo2.py:194][0m |          -0.0105 |          13.1506 |           4.4568 |
[32m[20230204 15:48:02 @agent_ppo2.py:194][0m |          -0.0129 |          11.8273 |           4.4529 |
[32m[20230204 15:48:02 @agent_ppo2.py:194][0m |          -0.0135 |          10.7232 |           4.4542 |
[32m[20230204 15:48:02 @agent_ppo2.py:194][0m |          -0.0149 |          10.0398 |           4.4589 |
[32m[20230204 15:48:02 @agent_ppo2.py:194][0m |          -0.0149 |           9.5042 |           4.4562 |
[32m[20230204 15:48:02 @agent_ppo2.py:194][0m |          -0.0159 |           9.1845 |           4.4556 |
[32m[20230204 15:48:02 @agent_ppo2.py:194][0m |          -0.0162 |           8.7943 |           4.4568 |
[32m[20230204 15:48:02 @agent_ppo2.py:194][0m |          -0.0159 |           8.5251 |           4.4570 |
[32m[20230204 15:48:02 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:48:03 @agent_ppo2.py:147][0m Average TRAINING episode reward: 232.61
[32m[20230204 15:48:03 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.94
[32m[20230204 15:48:03 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.08
[32m[20230204 15:48:03 @agent_ppo2.py:152][0m Total time:      11.87 min
[32m[20230204 15:48:03 @agent_ppo2.py:154][0m 980992 total steps have happened
[32m[20230204 15:48:03 @agent_ppo2.py:130][0m #------------------------ Iteration 479 --------------------------#
[32m[20230204 15:48:03 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:48:03 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:03 @agent_ppo2.py:194][0m |           0.0001 |          15.1157 |           4.5732 |
[32m[20230204 15:48:03 @agent_ppo2.py:194][0m |          -0.0045 |          13.3460 |           4.5613 |
[32m[20230204 15:48:03 @agent_ppo2.py:194][0m |          -0.0078 |          12.4108 |           4.5607 |
[32m[20230204 15:48:03 @agent_ppo2.py:194][0m |          -0.0101 |          11.8876 |           4.5508 |
[32m[20230204 15:48:03 @agent_ppo2.py:194][0m |          -0.0102 |          11.5869 |           4.5508 |
[32m[20230204 15:48:03 @agent_ppo2.py:194][0m |          -0.0116 |          11.3127 |           4.5475 |
[32m[20230204 15:48:03 @agent_ppo2.py:194][0m |          -0.0122 |          11.1496 |           4.5501 |
[32m[20230204 15:48:03 @agent_ppo2.py:194][0m |          -0.0149 |          10.9730 |           4.5460 |
[32m[20230204 15:48:04 @agent_ppo2.py:194][0m |          -0.0121 |          10.8269 |           4.5423 |
[32m[20230204 15:48:04 @agent_ppo2.py:194][0m |          -0.0142 |          10.7028 |           4.5421 |
[32m[20230204 15:48:04 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:48:04 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.27
[32m[20230204 15:48:04 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.60
[32m[20230204 15:48:04 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.21
[32m[20230204 15:48:04 @agent_ppo2.py:152][0m Total time:      11.89 min
[32m[20230204 15:48:04 @agent_ppo2.py:154][0m 983040 total steps have happened
[32m[20230204 15:48:04 @agent_ppo2.py:130][0m #------------------------ Iteration 480 --------------------------#
[32m[20230204 15:48:04 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:48:04 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:04 @agent_ppo2.py:194][0m |           0.0017 |          15.0387 |           4.4702 |
[32m[20230204 15:48:04 @agent_ppo2.py:194][0m |          -0.0034 |          14.5380 |           4.4653 |
[32m[20230204 15:48:04 @agent_ppo2.py:194][0m |          -0.0055 |          14.2834 |           4.4629 |
[32m[20230204 15:48:04 @agent_ppo2.py:194][0m |          -0.0076 |          14.1551 |           4.4609 |
[32m[20230204 15:48:04 @agent_ppo2.py:194][0m |          -0.0070 |          14.1325 |           4.4608 |
[32m[20230204 15:48:05 @agent_ppo2.py:194][0m |          -0.0086 |          13.8429 |           4.4499 |
[32m[20230204 15:48:05 @agent_ppo2.py:194][0m |          -0.0087 |          13.4558 |           4.4552 |
[32m[20230204 15:48:05 @agent_ppo2.py:194][0m |          -0.0104 |          13.0351 |           4.4529 |
[32m[20230204 15:48:05 @agent_ppo2.py:194][0m |          -0.0104 |          12.8400 |           4.4508 |
[32m[20230204 15:48:05 @agent_ppo2.py:194][0m |          -0.0102 |          12.7457 |           4.4514 |
[32m[20230204 15:48:05 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:48:05 @agent_ppo2.py:147][0m Average TRAINING episode reward: 262.99
[32m[20230204 15:48:05 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 268.32
[32m[20230204 15:48:05 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.81
[32m[20230204 15:48:05 @agent_ppo2.py:152][0m Total time:      11.91 min
[32m[20230204 15:48:05 @agent_ppo2.py:154][0m 985088 total steps have happened
[32m[20230204 15:48:05 @agent_ppo2.py:130][0m #------------------------ Iteration 481 --------------------------#
[32m[20230204 15:48:05 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:48:05 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:05 @agent_ppo2.py:194][0m |          -0.0005 |          13.2067 |           4.4473 |
[32m[20230204 15:48:06 @agent_ppo2.py:194][0m |          -0.0028 |          11.1528 |           4.4441 |
[32m[20230204 15:48:06 @agent_ppo2.py:194][0m |          -0.0103 |          10.1189 |           4.4347 |
[32m[20230204 15:48:06 @agent_ppo2.py:194][0m |          -0.0080 |          10.0313 |           4.4370 |
[32m[20230204 15:48:06 @agent_ppo2.py:194][0m |          -0.0122 |           9.2999 |           4.4338 |
[32m[20230204 15:48:06 @agent_ppo2.py:194][0m |          -0.0154 |           9.1593 |           4.4336 |
[32m[20230204 15:48:06 @agent_ppo2.py:194][0m |          -0.0150 |           8.7878 |           4.4307 |
[32m[20230204 15:48:06 @agent_ppo2.py:194][0m |          -0.0168 |           8.6946 |           4.4303 |
[32m[20230204 15:48:06 @agent_ppo2.py:194][0m |          -0.0166 |           8.5193 |           4.4313 |
[32m[20230204 15:48:06 @agent_ppo2.py:194][0m |          -0.0188 |           8.3779 |           4.4246 |
[32m[20230204 15:48:06 @agent_ppo2.py:139][0m Policy update time: 0.86 s
[32m[20230204 15:48:06 @agent_ppo2.py:147][0m Average TRAINING episode reward: 259.54
[32m[20230204 15:48:06 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.06
[32m[20230204 15:48:06 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.18
[32m[20230204 15:48:06 @agent_ppo2.py:152][0m Total time:      11.93 min
[32m[20230204 15:48:06 @agent_ppo2.py:154][0m 987136 total steps have happened
[32m[20230204 15:48:06 @agent_ppo2.py:130][0m #------------------------ Iteration 482 --------------------------#
[32m[20230204 15:48:07 @agent_ppo2.py:136][0m Sampling time: 0.23 s by 4 slaves
[32m[20230204 15:48:07 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:07 @agent_ppo2.py:194][0m |           0.0046 |          16.4646 |           4.5705 |
[32m[20230204 15:48:07 @agent_ppo2.py:194][0m |          -0.0042 |          14.8619 |           4.5597 |
[32m[20230204 15:48:07 @agent_ppo2.py:194][0m |          -0.0057 |          13.9986 |           4.5585 |
[32m[20230204 15:48:07 @agent_ppo2.py:194][0m |          -0.0068 |          13.4492 |           4.5571 |
[32m[20230204 15:48:07 @agent_ppo2.py:194][0m |          -0.0095 |          12.8933 |           4.5547 |
[32m[20230204 15:48:07 @agent_ppo2.py:194][0m |          -0.0084 |          12.7194 |           4.5567 |
[32m[20230204 15:48:07 @agent_ppo2.py:194][0m |          -0.0081 |          12.4264 |           4.5592 |
[32m[20230204 15:48:07 @agent_ppo2.py:194][0m |          -0.0107 |          12.1678 |           4.5543 |
[32m[20230204 15:48:07 @agent_ppo2.py:194][0m |          -0.0116 |          11.9241 |           4.5559 |
[32m[20230204 15:48:07 @agent_ppo2.py:194][0m |          -0.0126 |          11.7097 |           4.5538 |
[32m[20230204 15:48:07 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:48:08 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.24
[32m[20230204 15:48:08 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.31
[32m[20230204 15:48:08 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.09
[32m[20230204 15:48:08 @agent_ppo2.py:152][0m Total time:      11.95 min
[32m[20230204 15:48:08 @agent_ppo2.py:154][0m 989184 total steps have happened
[32m[20230204 15:48:08 @agent_ppo2.py:130][0m #------------------------ Iteration 483 --------------------------#
[32m[20230204 15:48:08 @agent_ppo2.py:136][0m Sampling time: 0.21 s by 4 slaves
[32m[20230204 15:48:08 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:08 @agent_ppo2.py:194][0m |          -0.0013 |          15.7706 |           4.5587 |
[32m[20230204 15:48:08 @agent_ppo2.py:194][0m |          -0.0041 |          14.2071 |           4.5498 |
[32m[20230204 15:48:08 @agent_ppo2.py:194][0m |          -0.0065 |          13.4516 |           4.5478 |
[32m[20230204 15:48:08 @agent_ppo2.py:194][0m |          -0.0111 |          12.7108 |           4.5436 |
[32m[20230204 15:48:08 @agent_ppo2.py:194][0m |          -0.0104 |          12.3085 |           4.5449 |
[32m[20230204 15:48:08 @agent_ppo2.py:194][0m |          -0.0115 |          12.0929 |           4.5402 |
[32m[20230204 15:48:09 @agent_ppo2.py:194][0m |          -0.0080 |          11.8621 |           4.5416 |
[32m[20230204 15:48:09 @agent_ppo2.py:194][0m |          -0.0136 |          11.6712 |           4.5337 |
[32m[20230204 15:48:09 @agent_ppo2.py:194][0m |          -0.0111 |          11.9972 |           4.5340 |
[32m[20230204 15:48:09 @agent_ppo2.py:194][0m |          -0.0140 |          11.4266 |           4.5291 |
[32m[20230204 15:48:09 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:48:09 @agent_ppo2.py:147][0m Average TRAINING episode reward: 264.76
[32m[20230204 15:48:09 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 266.37
[32m[20230204 15:48:09 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.00
[32m[20230204 15:48:09 @agent_ppo2.py:152][0m Total time:      11.97 min
[32m[20230204 15:48:09 @agent_ppo2.py:154][0m 991232 total steps have happened
[32m[20230204 15:48:09 @agent_ppo2.py:130][0m #------------------------ Iteration 484 --------------------------#
[32m[20230204 15:48:09 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:48:09 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:09 @agent_ppo2.py:194][0m |          -0.0005 |          16.2379 |           4.4175 |
[32m[20230204 15:48:09 @agent_ppo2.py:194][0m |          -0.0060 |          15.0344 |           4.4084 |
[32m[20230204 15:48:09 @agent_ppo2.py:194][0m |          -0.0068 |          14.7129 |           4.4039 |
[32m[20230204 15:48:10 @agent_ppo2.py:194][0m |          -0.0079 |          14.4120 |           4.4044 |
[32m[20230204 15:48:10 @agent_ppo2.py:194][0m |          -0.0087 |          14.3379 |           4.4049 |
[32m[20230204 15:48:10 @agent_ppo2.py:194][0m |          -0.0091 |          14.1651 |           4.3999 |
[32m[20230204 15:48:10 @agent_ppo2.py:194][0m |          -0.0104 |          13.9923 |           4.4001 |
[32m[20230204 15:48:10 @agent_ppo2.py:194][0m |          -0.0117 |          13.8758 |           4.4000 |
[32m[20230204 15:48:10 @agent_ppo2.py:194][0m |          -0.0093 |          13.9735 |           4.4011 |
[32m[20230204 15:48:10 @agent_ppo2.py:194][0m |          -0.0121 |          13.6460 |           4.3947 |
[32m[20230204 15:48:10 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:48:10 @agent_ppo2.py:147][0m Average TRAINING episode reward: 262.02
[32m[20230204 15:48:10 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.59
[32m[20230204 15:48:10 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.18
[32m[20230204 15:48:10 @agent_ppo2.py:152][0m Total time:      12.00 min
[32m[20230204 15:48:10 @agent_ppo2.py:154][0m 993280 total steps have happened
[32m[20230204 15:48:10 @agent_ppo2.py:130][0m #------------------------ Iteration 485 --------------------------#
[32m[20230204 15:48:10 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:48:11 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:11 @agent_ppo2.py:194][0m |           0.0010 |          14.8154 |           4.4930 |
[32m[20230204 15:48:11 @agent_ppo2.py:194][0m |          -0.0052 |          13.7369 |           4.4855 |
[32m[20230204 15:48:11 @agent_ppo2.py:194][0m |          -0.0068 |          13.0293 |           4.4845 |
[32m[20230204 15:48:11 @agent_ppo2.py:194][0m |          -0.0077 |          12.2794 |           4.4789 |
[32m[20230204 15:48:11 @agent_ppo2.py:194][0m |          -0.0094 |          11.4722 |           4.4798 |
[32m[20230204 15:48:11 @agent_ppo2.py:194][0m |          -0.0101 |          10.6008 |           4.4820 |
[32m[20230204 15:48:11 @agent_ppo2.py:194][0m |          -0.0100 |           9.9586 |           4.4759 |
[32m[20230204 15:48:11 @agent_ppo2.py:194][0m |          -0.0118 |           9.2895 |           4.4797 |
[32m[20230204 15:48:11 @agent_ppo2.py:194][0m |          -0.0123 |           8.9602 |           4.4754 |
[32m[20230204 15:48:11 @agent_ppo2.py:194][0m |          -0.0126 |           8.7988 |           4.4773 |
[32m[20230204 15:48:11 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:48:12 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.03
[32m[20230204 15:48:12 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.30
[32m[20230204 15:48:12 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 126.67
[32m[20230204 15:48:12 @agent_ppo2.py:152][0m Total time:      12.02 min
[32m[20230204 15:48:12 @agent_ppo2.py:154][0m 995328 total steps have happened
[32m[20230204 15:48:12 @agent_ppo2.py:130][0m #------------------------ Iteration 486 --------------------------#
[32m[20230204 15:48:12 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:48:12 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:12 @agent_ppo2.py:194][0m |          -0.0022 |          16.8601 |           4.4494 |
[32m[20230204 15:48:12 @agent_ppo2.py:194][0m |          -0.0067 |          15.3294 |           4.4379 |
[32m[20230204 15:48:12 @agent_ppo2.py:194][0m |          -0.0101 |          14.9749 |           4.4350 |
[32m[20230204 15:48:12 @agent_ppo2.py:194][0m |          -0.0097 |          14.7913 |           4.4322 |
[32m[20230204 15:48:12 @agent_ppo2.py:194][0m |          -0.0115 |          14.5579 |           4.4294 |
[32m[20230204 15:48:12 @agent_ppo2.py:194][0m |          -0.0109 |          14.6367 |           4.4328 |
[32m[20230204 15:48:12 @agent_ppo2.py:194][0m |          -0.0166 |          14.2654 |           4.4279 |
[32m[20230204 15:48:12 @agent_ppo2.py:194][0m |          -0.0119 |          14.6495 |           4.4271 |
[32m[20230204 15:48:12 @agent_ppo2.py:194][0m |          -0.0128 |          14.1853 |           4.4248 |
[32m[20230204 15:48:13 @agent_ppo2.py:194][0m |          -0.0128 |          14.2583 |           4.4217 |
[32m[20230204 15:48:13 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:48:13 @agent_ppo2.py:147][0m Average TRAINING episode reward: 262.78
[32m[20230204 15:48:13 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.85
[32m[20230204 15:48:13 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 1.05
[32m[20230204 15:48:13 @agent_ppo2.py:152][0m Total time:      12.04 min
[32m[20230204 15:48:13 @agent_ppo2.py:154][0m 997376 total steps have happened
[32m[20230204 15:48:13 @agent_ppo2.py:130][0m #------------------------ Iteration 487 --------------------------#
[32m[20230204 15:48:13 @agent_ppo2.py:136][0m Sampling time: 0.24 s by 4 slaves
[32m[20230204 15:48:13 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:13 @agent_ppo2.py:194][0m |           0.0024 |          15.1887 |           4.4405 |
[32m[20230204 15:48:13 @agent_ppo2.py:194][0m |          -0.0023 |          14.3354 |           4.4287 |
[32m[20230204 15:48:13 @agent_ppo2.py:194][0m |          -0.0031 |          14.2457 |           4.4280 |
[32m[20230204 15:48:13 @agent_ppo2.py:194][0m |          -0.0095 |          13.7047 |           4.4275 |
[32m[20230204 15:48:13 @agent_ppo2.py:194][0m |          -0.0083 |          13.6773 |           4.4233 |
[32m[20230204 15:48:13 @agent_ppo2.py:194][0m |          -0.0059 |          13.7690 |           4.4267 |
[32m[20230204 15:48:14 @agent_ppo2.py:194][0m |          -0.0118 |          13.2811 |           4.4255 |
[32m[20230204 15:48:14 @agent_ppo2.py:194][0m |          -0.0111 |          13.2337 |           4.4226 |
[32m[20230204 15:48:14 @agent_ppo2.py:194][0m |          -0.0132 |          13.1414 |           4.4240 |
[32m[20230204 15:48:14 @agent_ppo2.py:194][0m |          -0.0117 |          13.0532 |           4.4225 |
[32m[20230204 15:48:14 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:48:14 @agent_ppo2.py:147][0m Average TRAINING episode reward: 258.99
[32m[20230204 15:48:14 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 261.98
[32m[20230204 15:48:14 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 265.39
[32m[20230204 15:48:14 @agent_ppo2.py:152][0m Total time:      12.06 min
[32m[20230204 15:48:14 @agent_ppo2.py:154][0m 999424 total steps have happened
[32m[20230204 15:48:14 @agent_ppo2.py:130][0m #------------------------ Iteration 488 --------------------------#
[32m[20230204 15:48:14 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:48:14 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:14 @agent_ppo2.py:194][0m |           0.0013 |          11.9050 |           4.4939 |
[32m[20230204 15:48:14 @agent_ppo2.py:194][0m |          -0.0053 |           9.1924 |           4.4886 |
[32m[20230204 15:48:15 @agent_ppo2.py:194][0m |          -0.0074 |           8.3246 |           4.4827 |
[32m[20230204 15:48:15 @agent_ppo2.py:194][0m |          -0.0086 |           7.8990 |           4.4809 |
[32m[20230204 15:48:15 @agent_ppo2.py:194][0m |          -0.0094 |           7.7120 |           4.4785 |
[32m[20230204 15:48:15 @agent_ppo2.py:194][0m |          -0.0105 |           7.5264 |           4.4780 |
[32m[20230204 15:48:15 @agent_ppo2.py:194][0m |          -0.0112 |           7.3983 |           4.4760 |
[32m[20230204 15:48:15 @agent_ppo2.py:194][0m |          -0.0114 |           7.2539 |           4.4722 |
[32m[20230204 15:48:15 @agent_ppo2.py:194][0m |          -0.0111 |           7.1702 |           4.4757 |
[32m[20230204 15:48:15 @agent_ppo2.py:194][0m |          -0.0115 |           7.1226 |           4.4688 |
[32m[20230204 15:48:15 @agent_ppo2.py:139][0m Policy update time: 0.88 s
[32m[20230204 15:48:15 @agent_ppo2.py:147][0m Average TRAINING episode reward: 257.90
[32m[20230204 15:48:15 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.24
[32m[20230204 15:48:15 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.94
[32m[20230204 15:48:15 @agent_ppo2.py:152][0m Total time:      12.08 min
[32m[20230204 15:48:15 @agent_ppo2.py:154][0m 1001472 total steps have happened
[32m[20230204 15:48:15 @agent_ppo2.py:130][0m #------------------------ Iteration 489 --------------------------#
[32m[20230204 15:48:16 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:48:16 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:16 @agent_ppo2.py:194][0m |           0.0006 |          17.5618 |           4.2865 |
[32m[20230204 15:48:16 @agent_ppo2.py:194][0m |          -0.0073 |          16.3143 |           4.2799 |
[32m[20230204 15:48:16 @agent_ppo2.py:194][0m |          -0.0080 |          15.9345 |           4.2745 |
[32m[20230204 15:48:16 @agent_ppo2.py:194][0m |          -0.0105 |          15.6408 |           4.2734 |
[32m[20230204 15:48:16 @agent_ppo2.py:194][0m |          -0.0068 |          15.8019 |           4.2718 |
[32m[20230204 15:48:16 @agent_ppo2.py:194][0m |          -0.0108 |          15.3484 |           4.2649 |
[32m[20230204 15:48:16 @agent_ppo2.py:194][0m |          -0.0131 |          15.2772 |           4.2688 |
[32m[20230204 15:48:16 @agent_ppo2.py:194][0m |          -0.0082 |          16.5129 |           4.2687 |
[32m[20230204 15:48:16 @agent_ppo2.py:194][0m |          -0.0121 |          14.9803 |           4.2537 |
[32m[20230204 15:48:16 @agent_ppo2.py:194][0m |          -0.0133 |          15.0904 |           4.2614 |
[32m[20230204 15:48:16 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:48:17 @agent_ppo2.py:147][0m Average TRAINING episode reward: 264.18
[32m[20230204 15:48:17 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 266.12
[32m[20230204 15:48:17 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 276.41
[32m[20230204 15:48:17 @agent_ppo2.py:152][0m Total time:      12.10 min
[32m[20230204 15:48:17 @agent_ppo2.py:154][0m 1003520 total steps have happened
[32m[20230204 15:48:17 @agent_ppo2.py:130][0m #------------------------ Iteration 490 --------------------------#
[32m[20230204 15:48:17 @agent_ppo2.py:136][0m Sampling time: 0.21 s by 4 slaves
[32m[20230204 15:48:17 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:17 @agent_ppo2.py:194][0m |           0.0007 |          17.3360 |           4.4007 |
[32m[20230204 15:48:17 @agent_ppo2.py:194][0m |          -0.0046 |          14.6585 |           4.3987 |
[32m[20230204 15:48:17 @agent_ppo2.py:194][0m |          -0.0070 |          13.7309 |           4.4039 |
[32m[20230204 15:48:17 @agent_ppo2.py:194][0m |          -0.0088 |          13.2205 |           4.3992 |
[32m[20230204 15:48:17 @agent_ppo2.py:194][0m |          -0.0101 |          12.8368 |           4.4025 |
[32m[20230204 15:48:17 @agent_ppo2.py:194][0m |          -0.0101 |          12.6085 |           4.4001 |
[32m[20230204 15:48:17 @agent_ppo2.py:194][0m |          -0.0116 |          12.2931 |           4.4059 |
[32m[20230204 15:48:18 @agent_ppo2.py:194][0m |          -0.0113 |          12.3673 |           4.4013 |
[32m[20230204 15:48:18 @agent_ppo2.py:194][0m |          -0.0149 |          12.0281 |           4.4020 |
[32m[20230204 15:48:18 @agent_ppo2.py:194][0m |          -0.0140 |          11.8540 |           4.4052 |
[32m[20230204 15:48:18 @agent_ppo2.py:139][0m Policy update time: 0.82 s
[32m[20230204 15:48:18 @agent_ppo2.py:147][0m Average TRAINING episode reward: 265.12
[32m[20230204 15:48:18 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 267.30
[32m[20230204 15:48:18 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.65
[32m[20230204 15:48:18 @agent_ppo2.py:152][0m Total time:      12.12 min
[32m[20230204 15:48:18 @agent_ppo2.py:154][0m 1005568 total steps have happened
[32m[20230204 15:48:18 @agent_ppo2.py:130][0m #------------------------ Iteration 491 --------------------------#
[32m[20230204 15:48:18 @agent_ppo2.py:136][0m Sampling time: 0.25 s by 4 slaves
[32m[20230204 15:48:18 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:18 @agent_ppo2.py:194][0m |           0.0004 |          20.2423 |           4.3644 |
[32m[20230204 15:48:18 @agent_ppo2.py:194][0m |          -0.0037 |          15.4618 |           4.3586 |
[32m[20230204 15:48:18 @agent_ppo2.py:194][0m |          -0.0052 |          14.1399 |           4.3625 |
[32m[20230204 15:48:19 @agent_ppo2.py:194][0m |          -0.0057 |          13.1592 |           4.3580 |
[32m[20230204 15:48:19 @agent_ppo2.py:194][0m |          -0.0095 |          12.2968 |           4.3578 |
[32m[20230204 15:48:19 @agent_ppo2.py:194][0m |          -0.0073 |          11.8895 |           4.3546 |
[32m[20230204 15:48:19 @agent_ppo2.py:194][0m |          -0.0051 |          11.5997 |           4.3548 |
[32m[20230204 15:48:19 @agent_ppo2.py:194][0m |          -0.0097 |          11.1013 |           4.3570 |
[32m[20230204 15:48:19 @agent_ppo2.py:194][0m |          -0.0108 |          10.7749 |           4.3579 |
[32m[20230204 15:48:19 @agent_ppo2.py:194][0m |          -0.0119 |          10.6020 |           4.3601 |
[32m[20230204 15:48:19 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:48:19 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.03
[32m[20230204 15:48:19 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 263.60
[32m[20230204 15:48:19 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 268.32
[32m[20230204 15:48:19 @agent_ppo2.py:152][0m Total time:      12.15 min
[32m[20230204 15:48:19 @agent_ppo2.py:154][0m 1007616 total steps have happened
[32m[20230204 15:48:19 @agent_ppo2.py:130][0m #------------------------ Iteration 492 --------------------------#
[32m[20230204 15:48:19 @agent_ppo2.py:136][0m Sampling time: 0.30 s by 4 slaves
[32m[20230204 15:48:20 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:20 @agent_ppo2.py:194][0m |           0.0006 |          24.0431 |           4.4225 |
[32m[20230204 15:48:20 @agent_ppo2.py:194][0m |          -0.0027 |          15.3033 |           4.4119 |
[32m[20230204 15:48:20 @agent_ppo2.py:194][0m |          -0.0109 |          13.9974 |           4.4082 |
[32m[20230204 15:48:20 @agent_ppo2.py:194][0m |          -0.0031 |          13.2859 |           4.4100 |
[32m[20230204 15:48:20 @agent_ppo2.py:194][0m |          -0.0104 |          12.7573 |           4.4105 |
[32m[20230204 15:48:20 @agent_ppo2.py:194][0m |          -0.0096 |          12.4460 |           4.4013 |
[32m[20230204 15:48:20 @agent_ppo2.py:194][0m |          -0.0115 |          12.0868 |           4.4063 |
[32m[20230204 15:48:20 @agent_ppo2.py:194][0m |          -0.0105 |          11.8054 |           4.4042 |
[32m[20230204 15:48:20 @agent_ppo2.py:194][0m |          -0.0086 |          11.5077 |           4.4055 |
[32m[20230204 15:48:20 @agent_ppo2.py:194][0m |          -0.0200 |          11.2266 |           4.4041 |
[32m[20230204 15:48:20 @agent_ppo2.py:139][0m Policy update time: 0.93 s
[32m[20230204 15:48:21 @agent_ppo2.py:147][0m Average TRAINING episode reward: 206.90
[32m[20230204 15:48:21 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 265.21
[32m[20230204 15:48:21 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 270.63
[32m[20230204 15:48:21 @agent_ppo2.py:152][0m Total time:      12.17 min
[32m[20230204 15:48:21 @agent_ppo2.py:154][0m 1009664 total steps have happened
[32m[20230204 15:48:21 @agent_ppo2.py:130][0m #------------------------ Iteration 493 --------------------------#
[32m[20230204 15:48:21 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:48:21 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:21 @agent_ppo2.py:194][0m |           0.0039 |          17.1767 |           4.3516 |
[32m[20230204 15:48:21 @agent_ppo2.py:194][0m |          -0.0077 |          15.4794 |           4.3551 |
[32m[20230204 15:48:21 @agent_ppo2.py:194][0m |          -0.0086 |          15.0049 |           4.3519 |
[32m[20230204 15:48:21 @agent_ppo2.py:194][0m |          -0.0102 |          14.6085 |           4.3509 |
[32m[20230204 15:48:21 @agent_ppo2.py:194][0m |          -0.0116 |          14.3529 |           4.3503 |
[32m[20230204 15:48:21 @agent_ppo2.py:194][0m |          -0.0102 |          14.1442 |           4.3459 |
[32m[20230204 15:48:21 @agent_ppo2.py:194][0m |          -0.0110 |          13.9434 |           4.3450 |
[32m[20230204 15:48:22 @agent_ppo2.py:194][0m |          -0.0120 |          13.7560 |           4.3430 |
[32m[20230204 15:48:22 @agent_ppo2.py:194][0m |          -0.0138 |          13.5710 |           4.3404 |
[32m[20230204 15:48:22 @agent_ppo2.py:194][0m |          -0.0067 |          13.9331 |           4.3432 |
[32m[20230204 15:48:22 @agent_ppo2.py:139][0m Policy update time: 0.85 s
[32m[20230204 15:48:22 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.05
[32m[20230204 15:48:22 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.61
[32m[20230204 15:48:22 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 271.12
[32m[20230204 15:48:22 @agent_ppo2.py:152][0m Total time:      12.19 min
[32m[20230204 15:48:22 @agent_ppo2.py:154][0m 1011712 total steps have happened
[32m[20230204 15:48:22 @agent_ppo2.py:130][0m #------------------------ Iteration 494 --------------------------#
[32m[20230204 15:48:22 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:48:22 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:22 @agent_ppo2.py:194][0m |          -0.0009 |          14.6289 |           4.4191 |
[32m[20230204 15:48:22 @agent_ppo2.py:194][0m |          -0.0055 |          13.2111 |           4.4049 |
[32m[20230204 15:48:22 @agent_ppo2.py:194][0m |          -0.0071 |          12.6450 |           4.4063 |
[32m[20230204 15:48:23 @agent_ppo2.py:194][0m |          -0.0093 |          12.2135 |           4.4013 |
[32m[20230204 15:48:23 @agent_ppo2.py:194][0m |          -0.0065 |          12.1904 |           4.4007 |
[32m[20230204 15:48:23 @agent_ppo2.py:194][0m |          -0.0071 |          11.7562 |           4.3980 |
[32m[20230204 15:48:23 @agent_ppo2.py:194][0m |          -0.0093 |          11.3468 |           4.3966 |
[32m[20230204 15:48:23 @agent_ppo2.py:194][0m |          -0.0081 |          11.2852 |           4.3926 |
[32m[20230204 15:48:23 @agent_ppo2.py:194][0m |          -0.0086 |          10.7663 |           4.3911 |
[32m[20230204 15:48:23 @agent_ppo2.py:194][0m |          -0.0114 |          10.3051 |           4.3930 |
[32m[20230204 15:48:23 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:48:23 @agent_ppo2.py:147][0m Average TRAINING episode reward: 263.50
[32m[20230204 15:48:23 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 266.74
[32m[20230204 15:48:23 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.71
[32m[20230204 15:48:23 @agent_ppo2.py:152][0m Total time:      12.21 min
[32m[20230204 15:48:23 @agent_ppo2.py:154][0m 1013760 total steps have happened
[32m[20230204 15:48:23 @agent_ppo2.py:130][0m #------------------------ Iteration 495 --------------------------#
[32m[20230204 15:48:23 @agent_ppo2.py:136][0m Sampling time: 0.21 s by 4 slaves
[32m[20230204 15:48:23 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:24 @agent_ppo2.py:194][0m |           0.0015 |          13.6565 |           4.3526 |
[32m[20230204 15:48:24 @agent_ppo2.py:194][0m |          -0.0064 |          12.3785 |           4.3570 |
[32m[20230204 15:48:24 @agent_ppo2.py:194][0m |          -0.0076 |          11.6170 |           4.3584 |
[32m[20230204 15:48:24 @agent_ppo2.py:194][0m |          -0.0088 |          11.0339 |           4.3571 |
[32m[20230204 15:48:24 @agent_ppo2.py:194][0m |          -0.0086 |          10.6651 |           4.3593 |
[32m[20230204 15:48:24 @agent_ppo2.py:194][0m |          -0.0094 |           9.9851 |           4.3582 |
[32m[20230204 15:48:24 @agent_ppo2.py:194][0m |          -0.0085 |           9.5807 |           4.3593 |
[32m[20230204 15:48:24 @agent_ppo2.py:194][0m |          -0.0110 |           9.1973 |           4.3643 |
[32m[20230204 15:48:24 @agent_ppo2.py:194][0m |          -0.0114 |           8.9103 |           4.3640 |
[32m[20230204 15:48:24 @agent_ppo2.py:194][0m |          -0.0132 |           8.6128 |           4.3658 |
[32m[20230204 15:48:24 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:48:24 @agent_ppo2.py:147][0m Average TRAINING episode reward: 263.82
[32m[20230204 15:48:24 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 266.83
[32m[20230204 15:48:24 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 64.56
[32m[20230204 15:48:24 @agent_ppo2.py:152][0m Total time:      12.23 min
[32m[20230204 15:48:24 @agent_ppo2.py:154][0m 1015808 total steps have happened
[32m[20230204 15:48:24 @agent_ppo2.py:130][0m #------------------------ Iteration 496 --------------------------#
[32m[20230204 15:48:25 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:48:25 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:25 @agent_ppo2.py:194][0m |           0.0001 |          19.2793 |           4.4256 |
[32m[20230204 15:48:25 @agent_ppo2.py:194][0m |          -0.0073 |          17.0148 |           4.4169 |
[32m[20230204 15:48:25 @agent_ppo2.py:194][0m |          -0.0096 |          16.5067 |           4.4149 |
[32m[20230204 15:48:25 @agent_ppo2.py:194][0m |          -0.0103 |          16.2654 |           4.4154 |
[32m[20230204 15:48:25 @agent_ppo2.py:194][0m |          -0.0109 |          16.0480 |           4.4190 |
[32m[20230204 15:48:25 @agent_ppo2.py:194][0m |          -0.0120 |          15.7732 |           4.4152 |
[32m[20230204 15:48:25 @agent_ppo2.py:194][0m |          -0.0135 |          15.6430 |           4.4161 |
[32m[20230204 15:48:25 @agent_ppo2.py:194][0m |          -0.0139 |          15.5583 |           4.4207 |
[32m[20230204 15:48:25 @agent_ppo2.py:194][0m |          -0.0138 |          15.2192 |           4.4187 |
[32m[20230204 15:48:25 @agent_ppo2.py:194][0m |          -0.0156 |          15.0530 |           4.4190 |
[32m[20230204 15:48:25 @agent_ppo2.py:139][0m Policy update time: 0.83 s
[32m[20230204 15:48:26 @agent_ppo2.py:147][0m Average TRAINING episode reward: 263.58
[32m[20230204 15:48:26 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 264.99
[32m[20230204 15:48:26 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 267.34
[32m[20230204 15:48:26 @agent_ppo2.py:152][0m Total time:      12.25 min
[32m[20230204 15:48:26 @agent_ppo2.py:154][0m 1017856 total steps have happened
[32m[20230204 15:48:26 @agent_ppo2.py:130][0m #------------------------ Iteration 497 --------------------------#
[32m[20230204 15:48:26 @agent_ppo2.py:136][0m Sampling time: 0.22 s by 4 slaves
[32m[20230204 15:48:26 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:26 @agent_ppo2.py:194][0m |           0.0023 |          17.1132 |           4.4549 |
[32m[20230204 15:48:26 @agent_ppo2.py:194][0m |          -0.0041 |          15.2630 |           4.4502 |
[32m[20230204 15:48:26 @agent_ppo2.py:194][0m |          -0.0060 |          14.4436 |           4.4514 |
[32m[20230204 15:48:26 @agent_ppo2.py:194][0m |          -0.0081 |          13.9646 |           4.4481 |
[32m[20230204 15:48:26 @agent_ppo2.py:194][0m |          -0.0101 |          13.5052 |           4.4455 |
[32m[20230204 15:48:26 @agent_ppo2.py:194][0m |          -0.0103 |          13.2428 |           4.4435 |
[32m[20230204 15:48:26 @agent_ppo2.py:194][0m |          -0.0117 |          13.0274 |           4.4381 |
[32m[20230204 15:48:27 @agent_ppo2.py:194][0m |          -0.0116 |          12.9641 |           4.4386 |
[32m[20230204 15:48:27 @agent_ppo2.py:194][0m |          -0.0115 |          12.7500 |           4.4399 |
[32m[20230204 15:48:27 @agent_ppo2.py:194][0m |          -0.0112 |          12.5752 |           4.4370 |
[32m[20230204 15:48:27 @agent_ppo2.py:139][0m Policy update time: 0.84 s
[32m[20230204 15:48:27 @agent_ppo2.py:147][0m Average TRAINING episode reward: 261.07
[32m[20230204 15:48:27 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 262.94
[32m[20230204 15:48:27 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 272.23
[32m[20230204 15:48:27 @agent_ppo2.py:152][0m Total time:      12.27 min
[32m[20230204 15:48:27 @agent_ppo2.py:154][0m 1019904 total steps have happened
[32m[20230204 15:48:27 @agent_ppo2.py:130][0m #------------------------ Iteration 498 --------------------------#
[32m[20230204 15:48:27 @agent_ppo2.py:136][0m Sampling time: 0.21 s by 4 slaves
[32m[20230204 15:48:27 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:27 @agent_ppo2.py:194][0m |          -0.0025 |          15.1086 |           4.4067 |
[32m[20230204 15:48:27 @agent_ppo2.py:194][0m |          -0.0049 |          14.2645 |           4.3952 |
[32m[20230204 15:48:27 @agent_ppo2.py:194][0m |          -0.0111 |          13.4832 |           4.3995 |
[32m[20230204 15:48:27 @agent_ppo2.py:194][0m |          -0.0091 |          13.6212 |           4.3975 |
[32m[20230204 15:48:28 @agent_ppo2.py:194][0m |          -0.0132 |          12.9804 |           4.3946 |
[32m[20230204 15:48:28 @agent_ppo2.py:194][0m |          -0.0139 |          12.7985 |           4.3917 |
[32m[20230204 15:48:28 @agent_ppo2.py:194][0m |          -0.0132 |          12.6982 |           4.3949 |
[32m[20230204 15:48:28 @agent_ppo2.py:194][0m |          -0.0148 |          12.5175 |           4.3917 |
[32m[20230204 15:48:28 @agent_ppo2.py:194][0m |          -0.0140 |          12.4003 |           4.3936 |
[32m[20230204 15:48:28 @agent_ppo2.py:194][0m |          -0.0084 |          13.3299 |           4.3908 |
[32m[20230204 15:48:28 @agent_ppo2.py:139][0m Policy update time: 0.82 s
[32m[20230204 15:48:28 @agent_ppo2.py:147][0m Average TRAINING episode reward: 266.11
[32m[20230204 15:48:28 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 269.46
[32m[20230204 15:48:28 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 273.56
[32m[20230204 15:48:28 @agent_ppo2.py:152][0m Total time:      12.29 min
[32m[20230204 15:48:28 @agent_ppo2.py:154][0m 1021952 total steps have happened
[32m[20230204 15:48:28 @agent_ppo2.py:130][0m #------------------------ Iteration 499 --------------------------#
[32m[20230204 15:48:28 @agent_ppo2.py:136][0m Sampling time: 0.21 s by 4 slaves
[32m[20230204 15:48:28 @agent_ppo2.py:170][0m |      policy_loss |       value_loss |          entropy |
[32m[20230204 15:48:28 @agent_ppo2.py:194][0m |           0.0011 |          14.4740 |           4.5051 |
[32m[20230204 15:48:29 @agent_ppo2.py:194][0m |          -0.0051 |          13.4412 |           4.4964 |
[32m[20230204 15:48:29 @agent_ppo2.py:194][0m |          -0.0088 |          13.0209 |           4.4948 |
[32m[20230204 15:48:29 @agent_ppo2.py:194][0m |          -0.0097 |          12.4790 |           4.4877 |
[32m[20230204 15:48:29 @agent_ppo2.py:194][0m |          -0.0110 |          12.1446 |           4.4891 |
[32m[20230204 15:48:29 @agent_ppo2.py:194][0m |          -0.0117 |          11.7928 |           4.4872 |
[32m[20230204 15:48:29 @agent_ppo2.py:194][0m |          -0.0127 |          11.5707 |           4.4902 |
[32m[20230204 15:48:29 @agent_ppo2.py:194][0m |          -0.0130 |          11.2967 |           4.4918 |
[32m[20230204 15:48:29 @agent_ppo2.py:194][0m |          -0.0136 |          11.1272 |           4.4907 |
[32m[20230204 15:48:29 @agent_ppo2.py:194][0m |          -0.0138 |          10.9525 |           4.4902 |
[32m[20230204 15:48:29 @agent_ppo2.py:139][0m Policy update time: 0.82 s
[32m[20230204 15:48:29 @agent_ppo2.py:147][0m Average TRAINING episode reward: 264.59
[32m[20230204 15:48:29 @agent_ppo2.py:148][0m Maximum TRAINING episode reward: 268.08
[32m[20230204 15:48:29 @agent_ppo2.py:149][0m Average EVALUATION episode reward: 269.33
[32m[20230204 15:48:29 @agent_ppo2.py:108][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 281.11
[32m[20230204 15:48:29 @agent_ppo2.py:152][0m Total time:      12.32 min
[32m[20230204 15:48:29 @agent_ppo2.py:154][0m 1024000 total steps have happened
[32m[20230204 15:48:29 @train.py:63][0m [4m[34mCRITICAL[0m Training completed!
