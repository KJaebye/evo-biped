[32m[20230117 13:26:25 @logger.py:106][0m Log file set to ./tmp/bipedalwalker/easy/20230117_132625/log/bipedalwalker_easy-20230117_132625.log
[32m[20230117 13:26:25 @agent_ppo2.py:129][0m #------------------------ Iteration 0 --------------------------#
[32m[20230117 13:26:25 @agent_ppo2.py:135][0m Sampling time: 0.47 s by 4 slaves
[32m[20230117 13:26:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:26 @agent_ppo2.py:193][0m |           0.0005 |         206.6224 |           1.8743 |
[32m[20230117 13:26:26 @agent_ppo2.py:193][0m |          -0.0009 |         190.4900 |           1.8744 |
[32m[20230117 13:26:26 @agent_ppo2.py:193][0m |          -0.0020 |         179.2107 |           1.8748 |
[32m[20230117 13:26:26 @agent_ppo2.py:193][0m |          -0.0037 |         168.9613 |           1.8742 |
[32m[20230117 13:26:26 @agent_ppo2.py:193][0m |          -0.0043 |         163.1519 |           1.8746 |
[32m[20230117 13:26:26 @agent_ppo2.py:193][0m |          -0.0040 |         159.0112 |           1.8739 |
[32m[20230117 13:26:27 @agent_ppo2.py:193][0m |          -0.0030 |         155.6485 |           1.8737 |
[32m[20230117 13:26:27 @agent_ppo2.py:193][0m |          -0.0059 |         149.1662 |           1.8742 |
[32m[20230117 13:26:27 @agent_ppo2.py:193][0m |          -0.0059 |         144.5158 |           1.8742 |
[32m[20230117 13:26:27 @agent_ppo2.py:193][0m |          -0.0051 |         141.4893 |           1.8746 |
[32m[20230117 13:26:27 @agent_ppo2.py:138][0m Policy update time: 1.44 s
[32m[20230117 13:26:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: -108.30
[32m[20230117 13:26:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -100.33
[32m[20230117 13:26:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -98.78
[32m[20230117 13:26:27 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: -98.78
[32m[20230117 13:26:27 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -98.78
[32m[20230117 13:26:27 @agent_ppo2.py:151][0m Total time:       0.03 min
[32m[20230117 13:26:27 @agent_ppo2.py:153][0m 2048 total steps have happened
[32m[20230117 13:26:27 @agent_ppo2.py:129][0m #------------------------ Iteration 1 --------------------------#
[32m[20230117 13:26:27 @agent_ppo2.py:135][0m Sampling time: 0.33 s by 4 slaves
[32m[20230117 13:26:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:28 @agent_ppo2.py:193][0m |          -0.0012 |          94.5865 |           1.9028 |
[32m[20230117 13:26:28 @agent_ppo2.py:193][0m |           0.0002 |          87.4443 |           1.9036 |
[32m[20230117 13:26:28 @agent_ppo2.py:193][0m |           0.0031 |          84.8450 |           1.9036 |
[32m[20230117 13:26:28 @agent_ppo2.py:193][0m |          -0.0002 |          82.7083 |           1.9036 |
[32m[20230117 13:26:28 @agent_ppo2.py:193][0m |          -0.0014 |          83.3547 |           1.9036 |
[32m[20230117 13:26:28 @agent_ppo2.py:193][0m |          -0.0019 |          78.5481 |           1.9032 |
[32m[20230117 13:26:28 @agent_ppo2.py:193][0m |          -0.0042 |          77.7188 |           1.9031 |
[32m[20230117 13:26:28 @agent_ppo2.py:193][0m |          -0.0045 |          75.8313 |           1.9023 |
[32m[20230117 13:26:28 @agent_ppo2.py:193][0m |          -0.0026 |          74.6156 |           1.9015 |
[32m[20230117 13:26:28 @agent_ppo2.py:193][0m |          -0.0093 |          71.3336 |           1.9009 |
[32m[20230117 13:26:28 @agent_ppo2.py:138][0m Policy update time: 0.83 s
[32m[20230117 13:26:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: -121.47
[32m[20230117 13:26:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -104.56
[32m[20230117 13:26:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -103.14
[32m[20230117 13:26:28 @agent_ppo2.py:151][0m Total time:       0.06 min
[32m[20230117 13:26:28 @agent_ppo2.py:153][0m 4096 total steps have happened
[32m[20230117 13:26:28 @agent_ppo2.py:129][0m #------------------------ Iteration 2 --------------------------#
[32m[20230117 13:26:29 @agent_ppo2.py:135][0m Sampling time: 0.33 s by 4 slaves
[32m[20230117 13:26:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:29 @agent_ppo2.py:193][0m |          -0.0017 |          99.5268 |           1.9053 |
[32m[20230117 13:26:29 @agent_ppo2.py:193][0m |          -0.0002 |          92.8706 |           1.9039 |
[32m[20230117 13:26:29 @agent_ppo2.py:193][0m |          -0.0028 |          87.3249 |           1.9016 |
[32m[20230117 13:26:29 @agent_ppo2.py:193][0m |          -0.0036 |          83.7803 |           1.8998 |
[32m[20230117 13:26:29 @agent_ppo2.py:193][0m |          -0.0062 |          79.8907 |           1.8981 |
[32m[20230117 13:26:29 @agent_ppo2.py:193][0m |          -0.0042 |          77.4711 |           1.8963 |
[32m[20230117 13:26:29 @agent_ppo2.py:193][0m |          -0.0070 |          73.2954 |           1.8942 |
[32m[20230117 13:26:29 @agent_ppo2.py:193][0m |          -0.0023 |          73.1234 |           1.8932 |
[32m[20230117 13:26:30 @agent_ppo2.py:193][0m |          -0.0070 |          67.2664 |           1.8922 |
[32m[20230117 13:26:30 @agent_ppo2.py:193][0m |          -0.0068 |          65.0589 |           1.8910 |
[32m[20230117 13:26:30 @agent_ppo2.py:138][0m Policy update time: 0.96 s
[32m[20230117 13:26:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: -108.34
[32m[20230117 13:26:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -95.11
[32m[20230117 13:26:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -91.16
[32m[20230117 13:26:30 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: -91.16
[32m[20230117 13:26:30 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -91.16
[32m[20230117 13:26:30 @agent_ppo2.py:151][0m Total time:       0.08 min
[32m[20230117 13:26:30 @agent_ppo2.py:153][0m 6144 total steps have happened
[32m[20230117 13:26:30 @agent_ppo2.py:129][0m #------------------------ Iteration 3 --------------------------#
[32m[20230117 13:26:30 @agent_ppo2.py:135][0m Sampling time: 0.34 s by 4 slaves
[32m[20230117 13:26:30 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:30 @agent_ppo2.py:193][0m |          -0.0034 |          80.4620 |           1.8468 |
[32m[20230117 13:26:30 @agent_ppo2.py:193][0m |           0.0009 |          69.7432 |           1.8471 |
[32m[20230117 13:26:30 @agent_ppo2.py:193][0m |           0.0049 |          66.5803 |           1.8469 |
[32m[20230117 13:26:31 @agent_ppo2.py:193][0m |          -0.0039 |          63.9042 |           1.8467 |
[32m[20230117 13:26:31 @agent_ppo2.py:193][0m |          -0.0043 |          60.9686 |           1.8471 |
[32m[20230117 13:26:31 @agent_ppo2.py:193][0m |           0.0040 |          62.8910 |           1.8469 |
[32m[20230117 13:26:31 @agent_ppo2.py:193][0m |          -0.0050 |          56.7017 |           1.8465 |
[32m[20230117 13:26:31 @agent_ppo2.py:193][0m |          -0.0091 |          54.6315 |           1.8461 |
[32m[20230117 13:26:31 @agent_ppo2.py:193][0m |           0.0003 |          53.8685 |           1.8456 |
[32m[20230117 13:26:31 @agent_ppo2.py:193][0m |          -0.0076 |          50.9105 |           1.8451 |
[32m[20230117 13:26:31 @agent_ppo2.py:138][0m Policy update time: 0.99 s
[32m[20230117 13:26:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: -117.27
[32m[20230117 13:26:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -100.74
[32m[20230117 13:26:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -7.69
[32m[20230117 13:26:31 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: -7.69
[32m[20230117 13:26:31 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards -7.69
[32m[20230117 13:26:31 @agent_ppo2.py:151][0m Total time:       0.11 min
[32m[20230117 13:26:31 @agent_ppo2.py:153][0m 8192 total steps have happened
[32m[20230117 13:26:31 @agent_ppo2.py:129][0m #------------------------ Iteration 4 --------------------------#
[32m[20230117 13:26:32 @agent_ppo2.py:135][0m Sampling time: 0.33 s by 4 slaves
[32m[20230117 13:26:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:32 @agent_ppo2.py:193][0m |          -0.0004 |          11.8265 |           1.9078 |
[32m[20230117 13:26:32 @agent_ppo2.py:193][0m |          -0.0023 |           8.9899 |           1.9087 |
[32m[20230117 13:26:32 @agent_ppo2.py:193][0m |          -0.0041 |           8.0132 |           1.9087 |
[32m[20230117 13:26:32 @agent_ppo2.py:193][0m |          -0.0051 |           7.4035 |           1.9089 |
[32m[20230117 13:26:32 @agent_ppo2.py:193][0m |          -0.0056 |           7.0131 |           1.9097 |
[32m[20230117 13:26:32 @agent_ppo2.py:193][0m |          -0.0061 |           6.6820 |           1.9105 |
[32m[20230117 13:26:33 @agent_ppo2.py:193][0m |          -0.0063 |           6.4250 |           1.9104 |
[32m[20230117 13:26:33 @agent_ppo2.py:193][0m |          -0.0068 |           6.2364 |           1.9112 |
[32m[20230117 13:26:33 @agent_ppo2.py:193][0m |          -0.0070 |           6.0802 |           1.9115 |
[32m[20230117 13:26:33 @agent_ppo2.py:193][0m |          -0.0072 |           5.9399 |           1.9122 |
[32m[20230117 13:26:33 @agent_ppo2.py:138][0m Policy update time: 1.13 s
[32m[20230117 13:26:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: -104.64
[32m[20230117 13:26:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -97.90
[32m[20230117 13:26:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -15.26
[32m[20230117 13:26:33 @agent_ppo2.py:151][0m Total time:       0.14 min
[32m[20230117 13:26:33 @agent_ppo2.py:153][0m 10240 total steps have happened
[32m[20230117 13:26:33 @agent_ppo2.py:129][0m #------------------------ Iteration 5 --------------------------#
[32m[20230117 13:26:34 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 4 slaves
[32m[20230117 13:26:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:34 @agent_ppo2.py:193][0m |          -0.0011 |          28.5625 |           1.8971 |
[32m[20230117 13:26:34 @agent_ppo2.py:193][0m |          -0.0021 |          22.6746 |           1.8979 |
[32m[20230117 13:26:34 @agent_ppo2.py:193][0m |          -0.0032 |          20.5848 |           1.8982 |
[32m[20230117 13:26:34 @agent_ppo2.py:193][0m |          -0.0045 |          19.0589 |           1.8984 |
[32m[20230117 13:26:34 @agent_ppo2.py:193][0m |          -0.0018 |          17.8129 |           1.8984 |
[32m[20230117 13:26:34 @agent_ppo2.py:193][0m |          -0.0016 |          16.7847 |           1.8985 |
[32m[20230117 13:26:34 @agent_ppo2.py:193][0m |          -0.0037 |          15.7377 |           1.8985 |
[32m[20230117 13:26:35 @agent_ppo2.py:193][0m |          -0.0067 |          14.9417 |           1.8982 |
[32m[20230117 13:26:35 @agent_ppo2.py:193][0m |          -0.0021 |          14.0953 |           1.8980 |
[32m[20230117 13:26:35 @agent_ppo2.py:193][0m |          -0.0060 |          13.2911 |           1.8983 |
[32m[20230117 13:26:35 @agent_ppo2.py:138][0m Policy update time: 1.20 s
[32m[20230117 13:26:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: -104.38
[32m[20230117 13:26:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -99.35
[32m[20230117 13:26:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -93.92
[32m[20230117 13:26:35 @agent_ppo2.py:151][0m Total time:       0.16 min
[32m[20230117 13:26:35 @agent_ppo2.py:153][0m 12288 total steps have happened
[32m[20230117 13:26:35 @agent_ppo2.py:129][0m #------------------------ Iteration 6 --------------------------#
[32m[20230117 13:26:35 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 4 slaves
[32m[20230117 13:26:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:36 @agent_ppo2.py:193][0m |          -0.0002 |          18.2707 |           1.9147 |
[32m[20230117 13:26:36 @agent_ppo2.py:193][0m |          -0.0015 |          15.0494 |           1.9157 |
[32m[20230117 13:26:36 @agent_ppo2.py:193][0m |          -0.0023 |          13.7059 |           1.9158 |
[32m[20230117 13:26:36 @agent_ppo2.py:193][0m |          -0.0027 |          12.6384 |           1.9158 |
[32m[20230117 13:26:36 @agent_ppo2.py:193][0m |          -0.0023 |          11.7916 |           1.9166 |
[32m[20230117 13:26:36 @agent_ppo2.py:193][0m |          -0.0042 |          11.0688 |           1.9163 |
[32m[20230117 13:26:36 @agent_ppo2.py:193][0m |          -0.0039 |          10.4252 |           1.9167 |
[32m[20230117 13:26:36 @agent_ppo2.py:193][0m |          -0.0042 |           9.8273 |           1.9166 |
[32m[20230117 13:26:36 @agent_ppo2.py:193][0m |          -0.0049 |           9.3197 |           1.9169 |
[32m[20230117 13:26:36 @agent_ppo2.py:193][0m |          -0.0047 |           8.9139 |           1.9174 |
[32m[20230117 13:26:36 @agent_ppo2.py:138][0m Policy update time: 1.18 s
[32m[20230117 13:26:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: -103.24
[32m[20230117 13:26:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -98.40
[32m[20230117 13:26:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -23.47
[32m[20230117 13:26:37 @agent_ppo2.py:151][0m Total time:       0.20 min
[32m[20230117 13:26:37 @agent_ppo2.py:153][0m 14336 total steps have happened
[32m[20230117 13:26:37 @agent_ppo2.py:129][0m #------------------------ Iteration 7 --------------------------#
[32m[20230117 13:26:37 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 4 slaves
[32m[20230117 13:26:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:37 @agent_ppo2.py:193][0m |          -0.0015 |          14.4746 |           1.8928 |
[32m[20230117 13:26:37 @agent_ppo2.py:193][0m |          -0.0035 |          11.1456 |           1.8936 |
[32m[20230117 13:26:38 @agent_ppo2.py:193][0m |          -0.0050 |           9.8889 |           1.8942 |
[32m[20230117 13:26:38 @agent_ppo2.py:193][0m |          -0.0061 |           9.1925 |           1.8949 |
[32m[20230117 13:26:38 @agent_ppo2.py:193][0m |          -0.0078 |           8.6129 |           1.8950 |
[32m[20230117 13:26:38 @agent_ppo2.py:193][0m |          -0.0068 |           8.1773 |           1.8956 |
[32m[20230117 13:26:38 @agent_ppo2.py:193][0m |          -0.0083 |           7.6725 |           1.8968 |
[32m[20230117 13:26:38 @agent_ppo2.py:193][0m |          -0.0095 |           7.3155 |           1.8971 |
[32m[20230117 13:26:38 @agent_ppo2.py:193][0m |          -0.0100 |           6.9692 |           1.8980 |
[32m[20230117 13:26:38 @agent_ppo2.py:193][0m |          -0.0099 |           6.7659 |           1.8990 |
[32m[20230117 13:26:38 @agent_ppo2.py:138][0m Policy update time: 1.17 s
[32m[20230117 13:26:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: -102.45
[32m[20230117 13:26:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -98.86
[32m[20230117 13:26:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -100.93
[32m[20230117 13:26:38 @agent_ppo2.py:151][0m Total time:       0.22 min
[32m[20230117 13:26:38 @agent_ppo2.py:153][0m 16384 total steps have happened
[32m[20230117 13:26:38 @agent_ppo2.py:129][0m #------------------------ Iteration 8 --------------------------#
[32m[20230117 13:26:39 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 4 slaves
[32m[20230117 13:26:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:39 @agent_ppo2.py:193][0m |          -0.0003 |          34.5947 |           1.9338 |
[32m[20230117 13:26:39 @agent_ppo2.py:193][0m |          -0.0021 |          22.7399 |           1.9338 |
[32m[20230117 13:26:39 @agent_ppo2.py:193][0m |          -0.0036 |          19.8112 |           1.9338 |
[32m[20230117 13:26:39 @agent_ppo2.py:193][0m |          -0.0049 |          17.7281 |           1.9338 |
[32m[20230117 13:26:39 @agent_ppo2.py:193][0m |          -0.0053 |          16.1204 |           1.9337 |
[32m[20230117 13:26:40 @agent_ppo2.py:193][0m |          -0.0060 |          15.0463 |           1.9339 |
[32m[20230117 13:26:40 @agent_ppo2.py:193][0m |          -0.0065 |          14.0431 |           1.9332 |
[32m[20230117 13:26:40 @agent_ppo2.py:193][0m |          -0.0070 |          13.2667 |           1.9332 |
[32m[20230117 13:26:40 @agent_ppo2.py:193][0m |          -0.0071 |          12.4825 |           1.9326 |
[32m[20230117 13:26:40 @agent_ppo2.py:193][0m |          -0.0071 |          11.9107 |           1.9327 |
[32m[20230117 13:26:40 @agent_ppo2.py:138][0m Policy update time: 1.21 s
[32m[20230117 13:26:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: -105.91
[32m[20230117 13:26:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -91.16
[32m[20230117 13:26:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -29.94
[32m[20230117 13:26:40 @agent_ppo2.py:151][0m Total time:       0.26 min
[32m[20230117 13:26:40 @agent_ppo2.py:153][0m 18432 total steps have happened
[32m[20230117 13:26:40 @agent_ppo2.py:129][0m #------------------------ Iteration 9 --------------------------#
[32m[20230117 13:26:41 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 4 slaves
[32m[20230117 13:26:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:41 @agent_ppo2.py:193][0m |           0.0005 |          29.4467 |           1.9699 |
[32m[20230117 13:26:41 @agent_ppo2.py:193][0m |          -0.0020 |          22.5946 |           1.9699 |
[32m[20230117 13:26:41 @agent_ppo2.py:193][0m |          -0.0036 |          20.2544 |           1.9709 |
[32m[20230117 13:26:41 @agent_ppo2.py:193][0m |          -0.0051 |          18.5907 |           1.9710 |
[32m[20230117 13:26:42 @agent_ppo2.py:193][0m |          -0.0051 |          17.3855 |           1.9720 |
[32m[20230117 13:26:42 @agent_ppo2.py:193][0m |          -0.0054 |          16.7516 |           1.9724 |
[32m[20230117 13:26:42 @agent_ppo2.py:193][0m |          -0.0067 |          15.5995 |           1.9728 |
[32m[20230117 13:26:42 @agent_ppo2.py:193][0m |          -0.0075 |          15.2441 |           1.9740 |
[32m[20230117 13:26:42 @agent_ppo2.py:193][0m |          -0.0074 |          14.6900 |           1.9741 |
[32m[20230117 13:26:42 @agent_ppo2.py:193][0m |          -0.0073 |          14.3536 |           1.9744 |
[32m[20230117 13:26:42 @agent_ppo2.py:138][0m Policy update time: 1.24 s
[32m[20230117 13:26:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: -105.84
[32m[20230117 13:26:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -86.01
[32m[20230117 13:26:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -30.08
[32m[20230117 13:26:42 @agent_ppo2.py:151][0m Total time:       0.29 min
[32m[20230117 13:26:42 @agent_ppo2.py:153][0m 20480 total steps have happened
[32m[20230117 13:26:42 @agent_ppo2.py:129][0m #------------------------ Iteration 10 --------------------------#
[32m[20230117 13:26:43 @agent_ppo2.py:135][0m Sampling time: 0.38 s by 4 slaves
[32m[20230117 13:26:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:43 @agent_ppo2.py:193][0m |          -0.0011 |          17.8749 |           1.9869 |
[32m[20230117 13:26:43 @agent_ppo2.py:193][0m |          -0.0048 |          13.6341 |           1.9877 |
[32m[20230117 13:26:43 @agent_ppo2.py:193][0m |          -0.0044 |          11.7155 |           1.9874 |
[32m[20230117 13:26:43 @agent_ppo2.py:193][0m |          -0.0067 |          10.7328 |           1.9870 |
[32m[20230117 13:26:43 @agent_ppo2.py:193][0m |          -0.0039 |          10.0967 |           1.9868 |
[32m[20230117 13:26:44 @agent_ppo2.py:193][0m |          -0.0071 |           9.2953 |           1.9864 |
[32m[20230117 13:26:44 @agent_ppo2.py:193][0m |          -0.0074 |           8.9117 |           1.9875 |
[32m[20230117 13:26:44 @agent_ppo2.py:193][0m |          -0.0089 |           8.4773 |           1.9872 |
[32m[20230117 13:26:44 @agent_ppo2.py:193][0m |          -0.0084 |           8.1245 |           1.9877 |
[32m[20230117 13:26:44 @agent_ppo2.py:193][0m |          -0.0070 |           8.0162 |           1.9880 |
[32m[20230117 13:26:44 @agent_ppo2.py:138][0m Policy update time: 1.19 s
[32m[20230117 13:26:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: -101.01
[32m[20230117 13:26:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -85.26
[32m[20230117 13:26:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -28.02
[32m[20230117 13:26:44 @agent_ppo2.py:151][0m Total time:       0.32 min
[32m[20230117 13:26:44 @agent_ppo2.py:153][0m 22528 total steps have happened
[32m[20230117 13:26:44 @agent_ppo2.py:129][0m #------------------------ Iteration 11 --------------------------#
[32m[20230117 13:26:45 @agent_ppo2.py:135][0m Sampling time: 0.34 s by 4 slaves
[32m[20230117 13:26:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:45 @agent_ppo2.py:193][0m |           0.0000 |           9.9419 |           1.9634 |
[32m[20230117 13:26:45 @agent_ppo2.py:193][0m |          -0.0011 |           7.1344 |           1.9641 |
[32m[20230117 13:26:45 @agent_ppo2.py:193][0m |          -0.0011 |           6.4320 |           1.9649 |
[32m[20230117 13:26:45 @agent_ppo2.py:193][0m |          -0.0087 |           5.9603 |           1.9653 |
[32m[20230117 13:26:45 @agent_ppo2.py:193][0m |          -0.0049 |           5.7082 |           1.9660 |
[32m[20230117 13:26:45 @agent_ppo2.py:193][0m |          -0.0082 |           5.4883 |           1.9665 |
[32m[20230117 13:26:46 @agent_ppo2.py:193][0m |          -0.0050 |           5.2100 |           1.9667 |
[32m[20230117 13:26:46 @agent_ppo2.py:193][0m |          -0.0032 |           5.1705 |           1.9679 |
[32m[20230117 13:26:46 @agent_ppo2.py:193][0m |          -0.0046 |           4.9024 |           1.9689 |
[32m[20230117 13:26:46 @agent_ppo2.py:193][0m |          -0.0089 |           4.8408 |           1.9707 |
[32m[20230117 13:26:46 @agent_ppo2.py:138][0m Policy update time: 1.16 s
[32m[20230117 13:26:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: -96.87
[32m[20230117 13:26:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -88.29
[32m[20230117 13:26:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -36.89
[32m[20230117 13:26:46 @agent_ppo2.py:151][0m Total time:       0.35 min
[32m[20230117 13:26:46 @agent_ppo2.py:153][0m 24576 total steps have happened
[32m[20230117 13:26:46 @agent_ppo2.py:129][0m #------------------------ Iteration 12 --------------------------#
[32m[20230117 13:26:47 @agent_ppo2.py:135][0m Sampling time: 0.39 s by 4 slaves
[32m[20230117 13:26:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:47 @agent_ppo2.py:193][0m |          -0.0003 |           5.0593 |           1.9951 |
[32m[20230117 13:26:47 @agent_ppo2.py:193][0m |          -0.0028 |           3.9568 |           1.9933 |
[32m[20230117 13:26:47 @agent_ppo2.py:193][0m |          -0.0045 |           3.6596 |           1.9913 |
[32m[20230117 13:26:47 @agent_ppo2.py:193][0m |          -0.0059 |           3.4607 |           1.9911 |
[32m[20230117 13:26:47 @agent_ppo2.py:193][0m |          -0.0066 |           3.3377 |           1.9904 |
[32m[20230117 13:26:47 @agent_ppo2.py:193][0m |          -0.0069 |           3.2524 |           1.9908 |
[32m[20230117 13:26:47 @agent_ppo2.py:193][0m |          -0.0074 |           3.1782 |           1.9906 |
[32m[20230117 13:26:47 @agent_ppo2.py:193][0m |          -0.0074 |           3.1456 |           1.9911 |
[32m[20230117 13:26:48 @agent_ppo2.py:193][0m |          -0.0081 |           3.0827 |           1.9914 |
[32m[20230117 13:26:48 @agent_ppo2.py:193][0m |          -0.0083 |           3.0273 |           1.9916 |
[32m[20230117 13:26:48 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:26:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: -86.81
[32m[20230117 13:26:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -82.94
[32m[20230117 13:26:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -34.30
[32m[20230117 13:26:48 @agent_ppo2.py:151][0m Total time:       0.38 min
[32m[20230117 13:26:48 @agent_ppo2.py:153][0m 26624 total steps have happened
[32m[20230117 13:26:48 @agent_ppo2.py:129][0m #------------------------ Iteration 13 --------------------------#
[32m[20230117 13:26:48 @agent_ppo2.py:135][0m Sampling time: 0.34 s by 4 slaves
[32m[20230117 13:26:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:49 @agent_ppo2.py:193][0m |           0.0004 |           6.6314 |           2.0251 |
[32m[20230117 13:26:49 @agent_ppo2.py:193][0m |          -0.0027 |           4.6746 |           2.0251 |
[32m[20230117 13:26:49 @agent_ppo2.py:193][0m |          -0.0036 |           4.3473 |           2.0239 |
[32m[20230117 13:26:49 @agent_ppo2.py:193][0m |          -0.0047 |           4.2087 |           2.0241 |
[32m[20230117 13:26:49 @agent_ppo2.py:193][0m |          -0.0046 |           4.1672 |           2.0235 |
[32m[20230117 13:26:49 @agent_ppo2.py:193][0m |          -0.0054 |           4.0263 |           2.0243 |
[32m[20230117 13:26:49 @agent_ppo2.py:193][0m |          -0.0053 |           3.9084 |           2.0238 |
[32m[20230117 13:26:49 @agent_ppo2.py:193][0m |          -0.0061 |           3.8388 |           2.0237 |
[32m[20230117 13:26:49 @agent_ppo2.py:193][0m |          -0.0064 |           3.6939 |           2.0241 |
[32m[20230117 13:26:49 @agent_ppo2.py:193][0m |          -0.0068 |           3.6469 |           2.0246 |
[32m[20230117 13:26:49 @agent_ppo2.py:138][0m Policy update time: 1.13 s
[32m[20230117 13:26:50 @agent_ppo2.py:146][0m Average TRAINING episode reward: -84.08
[32m[20230117 13:26:50 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -69.97
[32m[20230117 13:26:50 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -35.51
[32m[20230117 13:26:50 @agent_ppo2.py:151][0m Total time:       0.41 min
[32m[20230117 13:26:50 @agent_ppo2.py:153][0m 28672 total steps have happened
[32m[20230117 13:26:50 @agent_ppo2.py:129][0m #------------------------ Iteration 14 --------------------------#
[32m[20230117 13:26:50 @agent_ppo2.py:135][0m Sampling time: 0.33 s by 4 slaves
[32m[20230117 13:26:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:50 @agent_ppo2.py:193][0m |          -0.0009 |           1.5646 |           2.0582 |
[32m[20230117 13:26:50 @agent_ppo2.py:193][0m |          -0.0048 |           1.1840 |           2.0592 |
[32m[20230117 13:26:50 @agent_ppo2.py:193][0m |          -0.0069 |           1.1117 |           2.0577 |
[32m[20230117 13:26:51 @agent_ppo2.py:193][0m |          -0.0080 |           1.0447 |           2.0588 |
[32m[20230117 13:26:51 @agent_ppo2.py:193][0m |          -0.0084 |           1.0186 |           2.0594 |
[32m[20230117 13:26:51 @agent_ppo2.py:193][0m |          -0.0090 |           0.9912 |           2.0603 |
[32m[20230117 13:26:51 @agent_ppo2.py:193][0m |          -0.0096 |           0.9749 |           2.0614 |
[32m[20230117 13:26:51 @agent_ppo2.py:193][0m |          -0.0099 |           0.9613 |           2.0626 |
[32m[20230117 13:26:51 @agent_ppo2.py:193][0m |          -0.0102 |           0.9505 |           2.0648 |
[32m[20230117 13:26:51 @agent_ppo2.py:193][0m |          -0.0106 |           0.9411 |           2.0658 |
[32m[20230117 13:26:51 @agent_ppo2.py:138][0m Policy update time: 1.11 s
[32m[20230117 13:26:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: -74.51
[32m[20230117 13:26:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -70.73
[32m[20230117 13:26:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -43.02
[32m[20230117 13:26:52 @agent_ppo2.py:151][0m Total time:       0.44 min
[32m[20230117 13:26:52 @agent_ppo2.py:153][0m 30720 total steps have happened
[32m[20230117 13:26:52 @agent_ppo2.py:129][0m #------------------------ Iteration 15 --------------------------#
[32m[20230117 13:26:52 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 4 slaves
[32m[20230117 13:26:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:52 @agent_ppo2.py:193][0m |          -0.0007 |           2.2804 |           2.0904 |
[32m[20230117 13:26:52 @agent_ppo2.py:193][0m |          -0.0041 |           1.6351 |           2.0902 |
[32m[20230117 13:26:52 @agent_ppo2.py:193][0m |          -0.0056 |           1.4858 |           2.0896 |
[32m[20230117 13:26:52 @agent_ppo2.py:193][0m |          -0.0066 |           1.4313 |           2.0889 |
[32m[20230117 13:26:52 @agent_ppo2.py:193][0m |          -0.0072 |           1.3727 |           2.0888 |
[32m[20230117 13:26:53 @agent_ppo2.py:193][0m |          -0.0078 |           1.3582 |           2.0886 |
[32m[20230117 13:26:53 @agent_ppo2.py:193][0m |          -0.0081 |           1.2949 |           2.0893 |
[32m[20230117 13:26:53 @agent_ppo2.py:193][0m |          -0.0085 |           1.2759 |           2.0893 |
[32m[20230117 13:26:53 @agent_ppo2.py:193][0m |          -0.0090 |           1.2460 |           2.0903 |
[32m[20230117 13:26:53 @agent_ppo2.py:193][0m |          -0.0088 |           1.2118 |           2.0913 |
[32m[20230117 13:26:53 @agent_ppo2.py:138][0m Policy update time: 1.12 s
[32m[20230117 13:26:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: -73.86
[32m[20230117 13:26:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -62.06
[32m[20230117 13:26:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -122.61
[32m[20230117 13:26:53 @agent_ppo2.py:151][0m Total time:       0.47 min
[32m[20230117 13:26:53 @agent_ppo2.py:153][0m 32768 total steps have happened
[32m[20230117 13:26:53 @agent_ppo2.py:129][0m #------------------------ Iteration 16 --------------------------#
[32m[20230117 13:26:53 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:26:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:54 @agent_ppo2.py:193][0m |          -0.0003 |           1.2448 |           2.0939 |
[32m[20230117 13:26:54 @agent_ppo2.py:193][0m |          -0.0031 |           1.0038 |           2.0944 |
[32m[20230117 13:26:54 @agent_ppo2.py:193][0m |          -0.0053 |           0.9595 |           2.0936 |
[32m[20230117 13:26:54 @agent_ppo2.py:193][0m |          -0.0063 |           0.9258 |           2.0951 |
[32m[20230117 13:26:54 @agent_ppo2.py:193][0m |          -0.0076 |           0.9127 |           2.0965 |
[32m[20230117 13:26:54 @agent_ppo2.py:193][0m |          -0.0082 |           0.8946 |           2.0975 |
[32m[20230117 13:26:54 @agent_ppo2.py:193][0m |          -0.0086 |           0.8755 |           2.0992 |
[32m[20230117 13:26:54 @agent_ppo2.py:193][0m |          -0.0090 |           0.8694 |           2.1003 |
[32m[20230117 13:26:54 @agent_ppo2.py:193][0m |          -0.0096 |           0.8548 |           2.1018 |
[32m[20230117 13:26:55 @agent_ppo2.py:193][0m |          -0.0100 |           0.8427 |           2.1029 |
[32m[20230117 13:26:55 @agent_ppo2.py:138][0m Policy update time: 1.11 s
[32m[20230117 13:26:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: -67.84
[32m[20230117 13:26:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -65.84
[32m[20230117 13:26:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -51.38
[32m[20230117 13:26:55 @agent_ppo2.py:151][0m Total time:       0.50 min
[32m[20230117 13:26:55 @agent_ppo2.py:153][0m 34816 total steps have happened
[32m[20230117 13:26:55 @agent_ppo2.py:129][0m #------------------------ Iteration 17 --------------------------#
[32m[20230117 13:26:55 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 4 slaves
[32m[20230117 13:26:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:55 @agent_ppo2.py:193][0m |          -0.0010 |           3.0509 |           2.1027 |
[32m[20230117 13:26:56 @agent_ppo2.py:193][0m |          -0.0022 |           1.9589 |           2.1010 |
[32m[20230117 13:26:56 @agent_ppo2.py:193][0m |          -0.0029 |           1.6537 |           2.0993 |
[32m[20230117 13:26:56 @agent_ppo2.py:193][0m |          -0.0042 |           1.4888 |           2.0979 |
[32m[20230117 13:26:56 @agent_ppo2.py:193][0m |          -0.0064 |           1.4750 |           2.0964 |
[32m[20230117 13:26:56 @agent_ppo2.py:193][0m |          -0.0080 |           1.3639 |           2.0945 |
[32m[20230117 13:26:56 @agent_ppo2.py:193][0m |          -0.0039 |           1.3041 |           2.0930 |
[32m[20230117 13:26:56 @agent_ppo2.py:193][0m |          -0.0064 |           1.2880 |           2.0922 |
[32m[20230117 13:26:56 @agent_ppo2.py:193][0m |          -0.0056 |           1.2497 |           2.0916 |
[32m[20230117 13:26:56 @agent_ppo2.py:193][0m |          -0.0074 |           1.2182 |           2.0908 |
[32m[20230117 13:26:56 @agent_ppo2.py:138][0m Policy update time: 1.12 s
[32m[20230117 13:26:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: -67.76
[32m[20230117 13:26:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -53.61
[32m[20230117 13:26:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -53.84
[32m[20230117 13:26:57 @agent_ppo2.py:151][0m Total time:       0.53 min
[32m[20230117 13:26:57 @agent_ppo2.py:153][0m 36864 total steps have happened
[32m[20230117 13:26:57 @agent_ppo2.py:129][0m #------------------------ Iteration 18 --------------------------#
[32m[20230117 13:26:57 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:26:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:57 @agent_ppo2.py:193][0m |          -0.0003 |           1.1047 |           2.1199 |
[32m[20230117 13:26:57 @agent_ppo2.py:193][0m |          -0.0027 |           1.0015 |           2.1182 |
[32m[20230117 13:26:57 @agent_ppo2.py:193][0m |          -0.0041 |           0.9614 |           2.1188 |
[32m[20230117 13:26:57 @agent_ppo2.py:193][0m |          -0.0049 |           0.9285 |           2.1186 |
[32m[20230117 13:26:58 @agent_ppo2.py:193][0m |          -0.0055 |           0.9194 |           2.1195 |
[32m[20230117 13:26:58 @agent_ppo2.py:193][0m |          -0.0061 |           0.9039 |           2.1204 |
[32m[20230117 13:26:58 @agent_ppo2.py:193][0m |          -0.0066 |           0.8927 |           2.1216 |
[32m[20230117 13:26:58 @agent_ppo2.py:193][0m |          -0.0070 |           0.8900 |           2.1232 |
[32m[20230117 13:26:58 @agent_ppo2.py:193][0m |          -0.0073 |           0.8802 |           2.1237 |
[32m[20230117 13:26:58 @agent_ppo2.py:193][0m |          -0.0077 |           0.8774 |           2.1246 |
[32m[20230117 13:26:58 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:26:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: -59.50
[32m[20230117 13:26:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -51.43
[32m[20230117 13:26:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -33.75
[32m[20230117 13:26:58 @agent_ppo2.py:151][0m Total time:       0.56 min
[32m[20230117 13:26:58 @agent_ppo2.py:153][0m 38912 total steps have happened
[32m[20230117 13:26:58 @agent_ppo2.py:129][0m #------------------------ Iteration 19 --------------------------#
[32m[20230117 13:26:59 @agent_ppo2.py:135][0m Sampling time: 0.34 s by 4 slaves
[32m[20230117 13:26:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:26:59 @agent_ppo2.py:193][0m |          -0.0015 |           1.6004 |           2.1516 |
[32m[20230117 13:26:59 @agent_ppo2.py:193][0m |          -0.0055 |           1.2321 |           2.1526 |
[32m[20230117 13:26:59 @agent_ppo2.py:193][0m |          -0.0073 |           1.1781 |           2.1532 |
[32m[20230117 13:26:59 @agent_ppo2.py:193][0m |          -0.0082 |           1.1547 |           2.1535 |
[32m[20230117 13:26:59 @agent_ppo2.py:193][0m |          -0.0089 |           1.1538 |           2.1545 |
[32m[20230117 13:26:59 @agent_ppo2.py:193][0m |          -0.0093 |           1.1030 |           2.1554 |
[32m[20230117 13:26:59 @agent_ppo2.py:193][0m |          -0.0099 |           1.0854 |           2.1565 |
[32m[20230117 13:27:00 @agent_ppo2.py:193][0m |          -0.0102 |           1.0755 |           2.1575 |
[32m[20230117 13:27:00 @agent_ppo2.py:193][0m |          -0.0104 |           1.0525 |           2.1587 |
[32m[20230117 13:27:00 @agent_ppo2.py:193][0m |          -0.0106 |           1.0386 |           2.1597 |
[32m[20230117 13:27:00 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:27:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: -52.62
[32m[20230117 13:27:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -40.12
[32m[20230117 13:27:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 81.04
[32m[20230117 13:27:00 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 81.04
[32m[20230117 13:27:00 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 81.04
[32m[20230117 13:27:00 @agent_ppo2.py:151][0m Total time:       0.58 min
[32m[20230117 13:27:00 @agent_ppo2.py:153][0m 40960 total steps have happened
[32m[20230117 13:27:00 @agent_ppo2.py:129][0m #------------------------ Iteration 20 --------------------------#
[32m[20230117 13:27:00 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:27:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:01 @agent_ppo2.py:193][0m |          -0.0005 |           1.1051 |           2.1677 |
[32m[20230117 13:27:01 @agent_ppo2.py:193][0m |          -0.0036 |           0.9962 |           2.1661 |
[32m[20230117 13:27:01 @agent_ppo2.py:193][0m |          -0.0052 |           0.9578 |           2.1656 |
[32m[20230117 13:27:01 @agent_ppo2.py:193][0m |          -0.0059 |           0.9420 |           2.1658 |
[32m[20230117 13:27:01 @agent_ppo2.py:193][0m |          -0.0067 |           0.9211 |           2.1662 |
[32m[20230117 13:27:01 @agent_ppo2.py:193][0m |          -0.0073 |           0.9119 |           2.1663 |
[32m[20230117 13:27:01 @agent_ppo2.py:193][0m |          -0.0076 |           0.8992 |           2.1676 |
[32m[20230117 13:27:01 @agent_ppo2.py:193][0m |          -0.0081 |           0.8881 |           2.1675 |
[32m[20230117 13:27:01 @agent_ppo2.py:193][0m |          -0.0087 |           0.8813 |           2.1671 |
[32m[20230117 13:27:02 @agent_ppo2.py:193][0m |          -0.0090 |           0.8791 |           2.1680 |
[32m[20230117 13:27:02 @agent_ppo2.py:138][0m Policy update time: 1.11 s
[32m[20230117 13:27:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: -42.58
[32m[20230117 13:27:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -30.90
[32m[20230117 13:27:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 124.16
[32m[20230117 13:27:02 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 124.16
[32m[20230117 13:27:02 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 124.16
[32m[20230117 13:27:02 @agent_ppo2.py:151][0m Total time:       0.61 min
[32m[20230117 13:27:02 @agent_ppo2.py:153][0m 43008 total steps have happened
[32m[20230117 13:27:02 @agent_ppo2.py:129][0m #------------------------ Iteration 21 --------------------------#
[32m[20230117 13:27:02 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:27:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:02 @agent_ppo2.py:193][0m |          -0.0017 |           1.1288 |           2.1710 |
[32m[20230117 13:27:02 @agent_ppo2.py:193][0m |          -0.0050 |           0.9790 |           2.1713 |
[32m[20230117 13:27:03 @agent_ppo2.py:193][0m |          -0.0065 |           0.9437 |           2.1708 |
[32m[20230117 13:27:03 @agent_ppo2.py:193][0m |          -0.0074 |           0.9231 |           2.1729 |
[32m[20230117 13:27:03 @agent_ppo2.py:193][0m |          -0.0082 |           0.9001 |           2.1734 |
[32m[20230117 13:27:03 @agent_ppo2.py:193][0m |          -0.0086 |           0.8885 |           2.1742 |
[32m[20230117 13:27:03 @agent_ppo2.py:193][0m |          -0.0090 |           0.8852 |           2.1752 |
[32m[20230117 13:27:03 @agent_ppo2.py:193][0m |          -0.0095 |           0.8743 |           2.1752 |
[32m[20230117 13:27:03 @agent_ppo2.py:193][0m |          -0.0099 |           0.8672 |           2.1760 |
[32m[20230117 13:27:03 @agent_ppo2.py:193][0m |          -0.0100 |           0.8562 |           2.1776 |
[32m[20230117 13:27:03 @agent_ppo2.py:138][0m Policy update time: 1.11 s
[32m[20230117 13:27:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: -41.57
[32m[20230117 13:27:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -34.58
[32m[20230117 13:27:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 115.02
[32m[20230117 13:27:04 @agent_ppo2.py:151][0m Total time:       0.64 min
[32m[20230117 13:27:04 @agent_ppo2.py:153][0m 45056 total steps have happened
[32m[20230117 13:27:04 @agent_ppo2.py:129][0m #------------------------ Iteration 22 --------------------------#
[32m[20230117 13:27:04 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:04 @agent_ppo2.py:193][0m |          -0.0005 |           2.0967 |           2.2022 |
[32m[20230117 13:27:04 @agent_ppo2.py:193][0m |          -0.0037 |           1.5541 |           2.2014 |
[32m[20230117 13:27:04 @agent_ppo2.py:193][0m |          -0.0051 |           1.3473 |           2.2006 |
[32m[20230117 13:27:04 @agent_ppo2.py:193][0m |          -0.0058 |           1.3086 |           2.2004 |
[32m[20230117 13:27:04 @agent_ppo2.py:193][0m |          -0.0066 |           1.1991 |           2.2005 |
[32m[20230117 13:27:05 @agent_ppo2.py:193][0m |          -0.0075 |           1.1572 |           2.1999 |
[32m[20230117 13:27:05 @agent_ppo2.py:193][0m |          -0.0077 |           1.1102 |           2.1999 |
[32m[20230117 13:27:05 @agent_ppo2.py:193][0m |          -0.0078 |           1.0859 |           2.2001 |
[32m[20230117 13:27:05 @agent_ppo2.py:193][0m |          -0.0085 |           1.0781 |           2.1994 |
[32m[20230117 13:27:05 @agent_ppo2.py:193][0m |          -0.0086 |           1.0520 |           2.2001 |
[32m[20230117 13:27:05 @agent_ppo2.py:138][0m Policy update time: 0.98 s
[32m[20230117 13:27:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: -49.34
[32m[20230117 13:27:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -16.79
[32m[20230117 13:27:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 114.34
[32m[20230117 13:27:05 @agent_ppo2.py:151][0m Total time:       0.67 min
[32m[20230117 13:27:05 @agent_ppo2.py:153][0m 47104 total steps have happened
[32m[20230117 13:27:05 @agent_ppo2.py:129][0m #------------------------ Iteration 23 --------------------------#
[32m[20230117 13:27:05 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:27:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:06 @agent_ppo2.py:193][0m |          -0.0015 |           0.9106 |           2.2251 |
[32m[20230117 13:27:06 @agent_ppo2.py:193][0m |          -0.0069 |           0.8525 |           2.2223 |
[32m[20230117 13:27:06 @agent_ppo2.py:193][0m |          -0.0087 |           0.8298 |           2.2212 |
[32m[20230117 13:27:06 @agent_ppo2.py:193][0m |          -0.0095 |           0.8178 |           2.2204 |
[32m[20230117 13:27:06 @agent_ppo2.py:193][0m |          -0.0100 |           0.8034 |           2.2207 |
[32m[20230117 13:27:06 @agent_ppo2.py:193][0m |          -0.0103 |           0.7937 |           2.2210 |
[32m[20230117 13:27:06 @agent_ppo2.py:193][0m |          -0.0107 |           0.7854 |           2.2218 |
[32m[20230117 13:27:06 @agent_ppo2.py:193][0m |          -0.0109 |           0.7769 |           2.2217 |
[32m[20230117 13:27:06 @agent_ppo2.py:193][0m |          -0.0109 |           0.7715 |           2.2227 |
[32m[20230117 13:27:07 @agent_ppo2.py:193][0m |          -0.0114 |           0.7639 |           2.2223 |
[32m[20230117 13:27:07 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:27:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: -31.85
[32m[20230117 13:27:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: -22.71
[32m[20230117 13:27:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 110.62
[32m[20230117 13:27:07 @agent_ppo2.py:151][0m Total time:       0.70 min
[32m[20230117 13:27:07 @agent_ppo2.py:153][0m 49152 total steps have happened
[32m[20230117 13:27:07 @agent_ppo2.py:129][0m #------------------------ Iteration 24 --------------------------#
[32m[20230117 13:27:07 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:27:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:07 @agent_ppo2.py:193][0m |          -0.0021 |           0.8639 |           2.2329 |
[32m[20230117 13:27:08 @agent_ppo2.py:193][0m |          -0.0064 |           0.8089 |           2.2311 |
[32m[20230117 13:27:08 @agent_ppo2.py:193][0m |          -0.0079 |           0.7871 |           2.2317 |
[32m[20230117 13:27:08 @agent_ppo2.py:193][0m |          -0.0085 |           0.7700 |           2.2333 |
[32m[20230117 13:27:08 @agent_ppo2.py:193][0m |          -0.0091 |           0.7585 |           2.2351 |
[32m[20230117 13:27:08 @agent_ppo2.py:193][0m |          -0.0096 |           0.7520 |           2.2364 |
[32m[20230117 13:27:08 @agent_ppo2.py:193][0m |          -0.0097 |           0.7483 |           2.2370 |
[32m[20230117 13:27:08 @agent_ppo2.py:193][0m |          -0.0100 |           0.7399 |           2.2383 |
[32m[20230117 13:27:08 @agent_ppo2.py:193][0m |          -0.0101 |           0.7347 |           2.2395 |
[32m[20230117 13:27:08 @agent_ppo2.py:193][0m |          -0.0105 |           0.7339 |           2.2406 |
[32m[20230117 13:27:08 @agent_ppo2.py:138][0m Policy update time: 1.12 s
[32m[20230117 13:27:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: -6.57
[32m[20230117 13:27:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 7.97
[32m[20230117 13:27:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 105.41
[32m[20230117 13:27:09 @agent_ppo2.py:151][0m Total time:       0.73 min
[32m[20230117 13:27:09 @agent_ppo2.py:153][0m 51200 total steps have happened
[32m[20230117 13:27:09 @agent_ppo2.py:129][0m #------------------------ Iteration 25 --------------------------#
[32m[20230117 13:27:09 @agent_ppo2.py:135][0m Sampling time: 0.34 s by 4 slaves
[32m[20230117 13:27:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:09 @agent_ppo2.py:193][0m |          -0.0009 |           0.8887 |           2.2802 |
[32m[20230117 13:27:09 @agent_ppo2.py:193][0m |          -0.0043 |           0.8252 |           2.2780 |
[32m[20230117 13:27:09 @agent_ppo2.py:193][0m |          -0.0056 |           0.8105 |           2.2775 |
[32m[20230117 13:27:09 @agent_ppo2.py:193][0m |          -0.0064 |           0.7948 |           2.2783 |
[32m[20230117 13:27:10 @agent_ppo2.py:193][0m |          -0.0071 |           0.7838 |           2.2790 |
[32m[20230117 13:27:10 @agent_ppo2.py:193][0m |          -0.0076 |           0.7790 |           2.2793 |
[32m[20230117 13:27:10 @agent_ppo2.py:193][0m |          -0.0080 |           0.7682 |           2.2803 |
[32m[20230117 13:27:10 @agent_ppo2.py:193][0m |          -0.0083 |           0.7625 |           2.2804 |
[32m[20230117 13:27:10 @agent_ppo2.py:193][0m |          -0.0088 |           0.7569 |           2.2825 |
[32m[20230117 13:27:10 @agent_ppo2.py:193][0m |          -0.0092 |           0.7541 |           2.2826 |
[32m[20230117 13:27:10 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:27:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: -0.01
[32m[20230117 13:27:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 14.18
[32m[20230117 13:27:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 112.66
[32m[20230117 13:27:10 @agent_ppo2.py:151][0m Total time:       0.76 min
[32m[20230117 13:27:10 @agent_ppo2.py:153][0m 53248 total steps have happened
[32m[20230117 13:27:10 @agent_ppo2.py:129][0m #------------------------ Iteration 26 --------------------------#
[32m[20230117 13:27:11 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:27:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:11 @agent_ppo2.py:193][0m |          -0.0006 |           5.6130 |           2.2486 |
[32m[20230117 13:27:11 @agent_ppo2.py:193][0m |          -0.0043 |           3.1853 |           2.2448 |
[32m[20230117 13:27:11 @agent_ppo2.py:193][0m |          -0.0063 |           2.8527 |           2.2436 |
[32m[20230117 13:27:11 @agent_ppo2.py:193][0m |          -0.0075 |           2.5054 |           2.2404 |
[32m[20230117 13:27:11 @agent_ppo2.py:193][0m |          -0.0081 |           2.3976 |           2.2397 |
[32m[20230117 13:27:11 @agent_ppo2.py:193][0m |          -0.0086 |           2.2618 |           2.2382 |
[32m[20230117 13:27:11 @agent_ppo2.py:193][0m |          -0.0090 |           2.1483 |           2.2361 |
[32m[20230117 13:27:12 @agent_ppo2.py:193][0m |          -0.0098 |           2.0110 |           2.2350 |
[32m[20230117 13:27:12 @agent_ppo2.py:193][0m |          -0.0103 |           1.9162 |           2.2337 |
[32m[20230117 13:27:12 @agent_ppo2.py:193][0m |          -0.0105 |           1.8826 |           2.2322 |
[32m[20230117 13:27:12 @agent_ppo2.py:138][0m Policy update time: 1.02 s
[32m[20230117 13:27:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: -37.35
[32m[20230117 13:27:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 15.50
[32m[20230117 13:27:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 104.50
[32m[20230117 13:27:12 @agent_ppo2.py:151][0m Total time:       0.78 min
[32m[20230117 13:27:12 @agent_ppo2.py:153][0m 55296 total steps have happened
[32m[20230117 13:27:12 @agent_ppo2.py:129][0m #------------------------ Iteration 27 --------------------------#
[32m[20230117 13:27:12 @agent_ppo2.py:135][0m Sampling time: 0.33 s by 4 slaves
[32m[20230117 13:27:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:13 @agent_ppo2.py:193][0m |          -0.0008 |           1.5293 |           2.2605 |
[32m[20230117 13:27:13 @agent_ppo2.py:193][0m |          -0.0045 |           1.3158 |           2.2601 |
[32m[20230117 13:27:13 @agent_ppo2.py:193][0m |          -0.0057 |           1.2582 |           2.2618 |
[32m[20230117 13:27:13 @agent_ppo2.py:193][0m |          -0.0067 |           1.2276 |           2.2613 |
[32m[20230117 13:27:13 @agent_ppo2.py:193][0m |          -0.0076 |           1.2037 |           2.2635 |
[32m[20230117 13:27:13 @agent_ppo2.py:193][0m |          -0.0081 |           1.1959 |           2.2659 |
[32m[20230117 13:27:13 @agent_ppo2.py:193][0m |          -0.0087 |           1.1747 |           2.2675 |
[32m[20230117 13:27:13 @agent_ppo2.py:193][0m |          -0.0090 |           1.1660 |           2.2693 |
[32m[20230117 13:27:13 @agent_ppo2.py:193][0m |          -0.0095 |           1.1563 |           2.2715 |
[32m[20230117 13:27:13 @agent_ppo2.py:193][0m |          -0.0097 |           1.1496 |           2.2735 |
[32m[20230117 13:27:13 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:27:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 12.53
[32m[20230117 13:27:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 33.72
[32m[20230117 13:27:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 101.25
[32m[20230117 13:27:14 @agent_ppo2.py:151][0m Total time:       0.81 min
[32m[20230117 13:27:14 @agent_ppo2.py:153][0m 57344 total steps have happened
[32m[20230117 13:27:14 @agent_ppo2.py:129][0m #------------------------ Iteration 28 --------------------------#
[32m[20230117 13:27:14 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:14 @agent_ppo2.py:193][0m |          -0.0007 |           1.3701 |           2.2943 |
[32m[20230117 13:27:14 @agent_ppo2.py:193][0m |          -0.0040 |           1.2017 |           2.2905 |
[32m[20230117 13:27:14 @agent_ppo2.py:193][0m |          -0.0052 |           1.1619 |           2.2915 |
[32m[20230117 13:27:15 @agent_ppo2.py:193][0m |          -0.0059 |           1.1399 |           2.2926 |
[32m[20230117 13:27:15 @agent_ppo2.py:193][0m |          -0.0064 |           1.1248 |           2.2931 |
[32m[20230117 13:27:15 @agent_ppo2.py:193][0m |          -0.0071 |           1.1039 |           2.2936 |
[32m[20230117 13:27:15 @agent_ppo2.py:193][0m |          -0.0073 |           1.0906 |           2.2947 |
[32m[20230117 13:27:15 @agent_ppo2.py:193][0m |          -0.0080 |           1.0836 |           2.2953 |
[32m[20230117 13:27:15 @agent_ppo2.py:193][0m |          -0.0082 |           1.0752 |           2.2966 |
[32m[20230117 13:27:15 @agent_ppo2.py:193][0m |          -0.0087 |           1.0649 |           2.2973 |
[32m[20230117 13:27:15 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:27:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 29.60
[32m[20230117 13:27:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 43.31
[32m[20230117 13:27:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 106.94
[32m[20230117 13:27:15 @agent_ppo2.py:151][0m Total time:       0.84 min
[32m[20230117 13:27:15 @agent_ppo2.py:153][0m 59392 total steps have happened
[32m[20230117 13:27:15 @agent_ppo2.py:129][0m #------------------------ Iteration 29 --------------------------#
[32m[20230117 13:27:16 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 4 slaves
[32m[20230117 13:27:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:16 @agent_ppo2.py:193][0m |           0.0008 |           2.2196 |           2.3413 |
[32m[20230117 13:27:16 @agent_ppo2.py:193][0m |          -0.0029 |           1.7079 |           2.3380 |
[32m[20230117 13:27:16 @agent_ppo2.py:193][0m |          -0.0048 |           1.5396 |           2.3343 |
[32m[20230117 13:27:16 @agent_ppo2.py:193][0m |          -0.0045 |           1.5021 |           2.3350 |
[32m[20230117 13:27:16 @agent_ppo2.py:193][0m |          -0.0053 |           1.4267 |           2.3331 |
[32m[20230117 13:27:17 @agent_ppo2.py:193][0m |          -0.0061 |           1.4042 |           2.3323 |
[32m[20230117 13:27:17 @agent_ppo2.py:193][0m |          -0.0068 |           1.3496 |           2.3322 |
[32m[20230117 13:27:17 @agent_ppo2.py:193][0m |          -0.0070 |           1.3213 |           2.3319 |
[32m[20230117 13:27:17 @agent_ppo2.py:193][0m |          -0.0061 |           1.3055 |           2.3328 |
[32m[20230117 13:27:17 @agent_ppo2.py:193][0m |          -0.0072 |           1.2802 |           2.3321 |
[32m[20230117 13:27:17 @agent_ppo2.py:138][0m Policy update time: 1.15 s
[32m[20230117 13:27:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 3.86
[32m[20230117 13:27:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 38.74
[32m[20230117 13:27:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 84.37
[32m[20230117 13:27:17 @agent_ppo2.py:151][0m Total time:       0.87 min
[32m[20230117 13:27:17 @agent_ppo2.py:153][0m 61440 total steps have happened
[32m[20230117 13:27:17 @agent_ppo2.py:129][0m #------------------------ Iteration 30 --------------------------#
[32m[20230117 13:27:18 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 4 slaves
[32m[20230117 13:27:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:18 @agent_ppo2.py:193][0m |          -0.0002 |           3.7266 |           2.3496 |
[32m[20230117 13:27:18 @agent_ppo2.py:193][0m |          -0.0030 |           2.1945 |           2.3503 |
[32m[20230117 13:27:18 @agent_ppo2.py:193][0m |          -0.0053 |           2.0899 |           2.3504 |
[32m[20230117 13:27:18 @agent_ppo2.py:193][0m |          -0.0057 |           2.0565 |           2.3520 |
[32m[20230117 13:27:18 @agent_ppo2.py:193][0m |          -0.0058 |           1.9071 |           2.3544 |
[32m[20230117 13:27:18 @agent_ppo2.py:193][0m |          -0.0066 |           1.8877 |           2.3574 |
[32m[20230117 13:27:18 @agent_ppo2.py:193][0m |          -0.0079 |           1.9063 |           2.3577 |
[32m[20230117 13:27:19 @agent_ppo2.py:193][0m |          -0.0081 |           1.8168 |           2.3592 |
[32m[20230117 13:27:19 @agent_ppo2.py:193][0m |          -0.0094 |           1.8198 |           2.3617 |
[32m[20230117 13:27:19 @agent_ppo2.py:193][0m |          -0.0086 |           1.7604 |           2.3635 |
[32m[20230117 13:27:19 @agent_ppo2.py:138][0m Policy update time: 1.09 s
[32m[20230117 13:27:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: -6.49
[32m[20230117 13:27:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 52.42
[32m[20230117 13:27:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 112.35
[32m[20230117 13:27:19 @agent_ppo2.py:151][0m Total time:       0.90 min
[32m[20230117 13:27:19 @agent_ppo2.py:153][0m 63488 total steps have happened
[32m[20230117 13:27:19 @agent_ppo2.py:129][0m #------------------------ Iteration 31 --------------------------#
[32m[20230117 13:27:19 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:20 @agent_ppo2.py:193][0m |          -0.0014 |           1.4530 |           2.3746 |
[32m[20230117 13:27:20 @agent_ppo2.py:193][0m |          -0.0048 |           1.3032 |           2.3738 |
[32m[20230117 13:27:20 @agent_ppo2.py:193][0m |          -0.0062 |           1.2717 |           2.3738 |
[32m[20230117 13:27:20 @agent_ppo2.py:193][0m |          -0.0068 |           1.2503 |           2.3750 |
[32m[20230117 13:27:20 @agent_ppo2.py:193][0m |          -0.0078 |           1.2327 |           2.3769 |
[32m[20230117 13:27:20 @agent_ppo2.py:193][0m |          -0.0082 |           1.2160 |           2.3774 |
[32m[20230117 13:27:20 @agent_ppo2.py:193][0m |          -0.0088 |           1.2044 |           2.3775 |
[32m[20230117 13:27:20 @agent_ppo2.py:193][0m |          -0.0095 |           1.1899 |           2.3782 |
[32m[20230117 13:27:20 @agent_ppo2.py:193][0m |          -0.0097 |           1.1828 |           2.3778 |
[32m[20230117 13:27:20 @agent_ppo2.py:193][0m |          -0.0101 |           1.1736 |           2.3786 |
[32m[20230117 13:27:21 @agent_ppo2.py:138][0m Policy update time: 1.09 s
[32m[20230117 13:27:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 54.03
[32m[20230117 13:27:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 72.53
[32m[20230117 13:27:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 110.86
[32m[20230117 13:27:21 @agent_ppo2.py:151][0m Total time:       0.93 min
[32m[20230117 13:27:21 @agent_ppo2.py:153][0m 65536 total steps have happened
[32m[20230117 13:27:21 @agent_ppo2.py:129][0m #------------------------ Iteration 32 --------------------------#
[32m[20230117 13:27:21 @agent_ppo2.py:135][0m Sampling time: 0.40 s by 4 slaves
[32m[20230117 13:27:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:21 @agent_ppo2.py:193][0m |           0.0020 |           8.6912 |           2.4063 |
[32m[20230117 13:27:21 @agent_ppo2.py:193][0m |          -0.0037 |           6.2726 |           2.4037 |
[32m[20230117 13:27:22 @agent_ppo2.py:193][0m |          -0.0035 |           5.8174 |           2.4036 |
[32m[20230117 13:27:22 @agent_ppo2.py:193][0m |          -0.0061 |           5.4042 |           2.4049 |
[32m[20230117 13:27:22 @agent_ppo2.py:193][0m |          -0.0067 |           5.1304 |           2.4035 |
[32m[20230117 13:27:22 @agent_ppo2.py:193][0m |          -0.0061 |           5.0921 |           2.4042 |
[32m[20230117 13:27:22 @agent_ppo2.py:193][0m |          -0.0076 |           4.8754 |           2.4052 |
[32m[20230117 13:27:22 @agent_ppo2.py:193][0m |          -0.0072 |           4.7518 |           2.4063 |
[32m[20230117 13:27:22 @agent_ppo2.py:193][0m |          -0.0083 |           4.5965 |           2.4062 |
[32m[20230117 13:27:22 @agent_ppo2.py:193][0m |          -0.0074 |           4.6258 |           2.4060 |
[32m[20230117 13:27:22 @agent_ppo2.py:138][0m Policy update time: 1.05 s
[32m[20230117 13:27:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: 2.99
[32m[20230117 13:27:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 69.05
[32m[20230117 13:27:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 107.65
[32m[20230117 13:27:23 @agent_ppo2.py:151][0m Total time:       0.96 min
[32m[20230117 13:27:23 @agent_ppo2.py:153][0m 67584 total steps have happened
[32m[20230117 13:27:23 @agent_ppo2.py:129][0m #------------------------ Iteration 33 --------------------------#
[32m[20230117 13:27:23 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:27:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:23 @agent_ppo2.py:193][0m |           0.0011 |           7.4956 |           2.4110 |
[32m[20230117 13:27:23 @agent_ppo2.py:193][0m |          -0.0022 |           5.5202 |           2.4099 |
[32m[20230117 13:27:23 @agent_ppo2.py:193][0m |          -0.0022 |           5.3899 |           2.4080 |
[32m[20230117 13:27:23 @agent_ppo2.py:193][0m |          -0.0056 |           4.9199 |           2.4093 |
[32m[20230117 13:27:23 @agent_ppo2.py:193][0m |          -0.0057 |           4.8583 |           2.4076 |
[32m[20230117 13:27:24 @agent_ppo2.py:193][0m |          -0.0062 |           4.6812 |           2.4089 |
[32m[20230117 13:27:24 @agent_ppo2.py:193][0m |          -0.0075 |           4.6976 |           2.4093 |
[32m[20230117 13:27:24 @agent_ppo2.py:193][0m |          -0.0076 |           4.6056 |           2.4094 |
[32m[20230117 13:27:24 @agent_ppo2.py:193][0m |          -0.0083 |           4.7530 |           2.4103 |
[32m[20230117 13:27:24 @agent_ppo2.py:193][0m |          -0.0078 |           4.5499 |           2.4102 |
[32m[20230117 13:27:24 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:27:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 26.89
[32m[20230117 13:27:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 65.32
[32m[20230117 13:27:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 113.53
[32m[20230117 13:27:24 @agent_ppo2.py:151][0m Total time:       0.99 min
[32m[20230117 13:27:24 @agent_ppo2.py:153][0m 69632 total steps have happened
[32m[20230117 13:27:24 @agent_ppo2.py:129][0m #------------------------ Iteration 34 --------------------------#
[32m[20230117 13:27:25 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 4 slaves
[32m[20230117 13:27:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:25 @agent_ppo2.py:193][0m |           0.0013 |          20.8061 |           2.4634 |
[32m[20230117 13:27:25 @agent_ppo2.py:193][0m |          -0.0012 |          15.5914 |           2.4644 |
[32m[20230117 13:27:25 @agent_ppo2.py:193][0m |          -0.0029 |          13.5713 |           2.4667 |
[32m[20230117 13:27:25 @agent_ppo2.py:193][0m |          -0.0043 |          12.5347 |           2.4691 |
[32m[20230117 13:27:25 @agent_ppo2.py:193][0m |          -0.0041 |          11.9857 |           2.4713 |
[32m[20230117 13:27:25 @agent_ppo2.py:193][0m |          -0.0053 |          11.5219 |           2.4720 |
[32m[20230117 13:27:26 @agent_ppo2.py:193][0m |          -0.0056 |          11.3240 |           2.4739 |
[32m[20230117 13:27:26 @agent_ppo2.py:193][0m |          -0.0063 |          10.9692 |           2.4749 |
[32m[20230117 13:27:26 @agent_ppo2.py:193][0m |          -0.0078 |          10.7593 |           2.4778 |
[32m[20230117 13:27:26 @agent_ppo2.py:193][0m |          -0.0067 |          10.4592 |           2.4788 |
[32m[20230117 13:27:26 @agent_ppo2.py:138][0m Policy update time: 1.17 s
[32m[20230117 13:27:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 45.13
[32m[20230117 13:27:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 84.25
[32m[20230117 13:27:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 105.07
[32m[20230117 13:27:26 @agent_ppo2.py:151][0m Total time:       1.02 min
[32m[20230117 13:27:26 @agent_ppo2.py:153][0m 71680 total steps have happened
[32m[20230117 13:27:26 @agent_ppo2.py:129][0m #------------------------ Iteration 35 --------------------------#
[32m[20230117 13:27:26 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:27 @agent_ppo2.py:193][0m |           0.0009 |           3.9884 |           2.4718 |
[32m[20230117 13:27:27 @agent_ppo2.py:193][0m |          -0.0015 |           3.1108 |           2.4683 |
[32m[20230117 13:27:27 @agent_ppo2.py:193][0m |          -0.0023 |           2.7373 |           2.4677 |
[32m[20230117 13:27:27 @agent_ppo2.py:193][0m |          -0.0036 |           2.5293 |           2.4655 |
[32m[20230117 13:27:27 @agent_ppo2.py:193][0m |          -0.0041 |           2.4309 |           2.4668 |
[32m[20230117 13:27:27 @agent_ppo2.py:193][0m |          -0.0049 |           2.3188 |           2.4678 |
[32m[20230117 13:27:27 @agent_ppo2.py:193][0m |          -0.0050 |           2.2629 |           2.4663 |
[32m[20230117 13:27:27 @agent_ppo2.py:193][0m |          -0.0059 |           2.2080 |           2.4681 |
[32m[20230117 13:27:27 @agent_ppo2.py:193][0m |          -0.0061 |           2.1730 |           2.4677 |
[32m[20230117 13:27:28 @agent_ppo2.py:193][0m |          -0.0065 |           2.1288 |           2.4678 |
[32m[20230117 13:27:28 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:27:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 71.27
[32m[20230117 13:27:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 88.56
[32m[20230117 13:27:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 122.59
[32m[20230117 13:27:28 @agent_ppo2.py:151][0m Total time:       1.05 min
[32m[20230117 13:27:28 @agent_ppo2.py:153][0m 73728 total steps have happened
[32m[20230117 13:27:28 @agent_ppo2.py:129][0m #------------------------ Iteration 36 --------------------------#
[32m[20230117 13:27:28 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 4 slaves
[32m[20230117 13:27:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:28 @agent_ppo2.py:193][0m |           0.0184 |          22.8466 |           2.4587 |
[32m[20230117 13:27:28 @agent_ppo2.py:193][0m |          -0.0038 |          10.5821 |           2.4558 |
[32m[20230117 13:27:29 @agent_ppo2.py:193][0m |          -0.0058 |           7.3287 |           2.4536 |
[32m[20230117 13:27:29 @agent_ppo2.py:193][0m |          -0.0062 |           5.6666 |           2.4523 |
[32m[20230117 13:27:29 @agent_ppo2.py:193][0m |          -0.0073 |           4.8430 |           2.4524 |
[32m[20230117 13:27:29 @agent_ppo2.py:193][0m |          -0.0052 |           4.4136 |           2.4527 |
[32m[20230117 13:27:29 @agent_ppo2.py:193][0m |          -0.0030 |           4.1581 |           2.4520 |
[32m[20230117 13:27:29 @agent_ppo2.py:193][0m |          -0.0089 |           4.0792 |           2.4490 |
[32m[20230117 13:27:29 @agent_ppo2.py:193][0m |          -0.0103 |           3.7822 |           2.4518 |
[32m[20230117 13:27:29 @agent_ppo2.py:193][0m |          -0.0171 |           3.6675 |           2.4510 |
[32m[20230117 13:27:29 @agent_ppo2.py:138][0m Policy update time: 0.93 s
[32m[20230117 13:27:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 49.06
[32m[20230117 13:27:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 105.20
[32m[20230117 13:27:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -33.88
[32m[20230117 13:27:29 @agent_ppo2.py:151][0m Total time:       1.07 min
[32m[20230117 13:27:29 @agent_ppo2.py:153][0m 75776 total steps have happened
[32m[20230117 13:27:29 @agent_ppo2.py:129][0m #------------------------ Iteration 37 --------------------------#
[32m[20230117 13:27:30 @agent_ppo2.py:135][0m Sampling time: 0.34 s by 4 slaves
[32m[20230117 13:27:30 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:30 @agent_ppo2.py:193][0m |          -0.0024 |          12.5534 |           2.5156 |
[32m[20230117 13:27:30 @agent_ppo2.py:193][0m |          -0.0056 |           9.8008 |           2.5106 |
[32m[20230117 13:27:30 @agent_ppo2.py:193][0m |          -0.0062 |           8.6721 |           2.5078 |
[32m[20230117 13:27:30 @agent_ppo2.py:193][0m |          -0.0071 |           8.3454 |           2.5069 |
[32m[20230117 13:27:30 @agent_ppo2.py:193][0m |          -0.0077 |           7.7476 |           2.5060 |
[32m[20230117 13:27:30 @agent_ppo2.py:193][0m |          -0.0085 |           7.5220 |           2.5038 |
[32m[20230117 13:27:30 @agent_ppo2.py:193][0m |          -0.0084 |           7.1192 |           2.5050 |
[32m[20230117 13:27:31 @agent_ppo2.py:193][0m |          -0.0095 |           6.9889 |           2.5037 |
[32m[20230117 13:27:31 @agent_ppo2.py:193][0m |          -0.0095 |           6.6164 |           2.5035 |
[32m[20230117 13:27:31 @agent_ppo2.py:193][0m |          -0.0100 |           6.5363 |           2.5026 |
[32m[20230117 13:27:31 @agent_ppo2.py:138][0m Policy update time: 1.12 s
[32m[20230117 13:27:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 18.48
[32m[20230117 13:27:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 99.28
[32m[20230117 13:27:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 92.30
[32m[20230117 13:27:31 @agent_ppo2.py:151][0m Total time:       1.10 min
[32m[20230117 13:27:31 @agent_ppo2.py:153][0m 77824 total steps have happened
[32m[20230117 13:27:31 @agent_ppo2.py:129][0m #------------------------ Iteration 38 --------------------------#
[32m[20230117 13:27:31 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 4 slaves
[32m[20230117 13:27:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:32 @agent_ppo2.py:193][0m |          -0.0002 |           6.3267 |           2.5040 |
[32m[20230117 13:27:32 @agent_ppo2.py:193][0m |          -0.0025 |           3.6709 |           2.5029 |
[32m[20230117 13:27:32 @agent_ppo2.py:193][0m |          -0.0036 |           3.3120 |           2.5016 |
[32m[20230117 13:27:32 @agent_ppo2.py:193][0m |          -0.0049 |           3.1626 |           2.5019 |
[32m[20230117 13:27:32 @agent_ppo2.py:193][0m |          -0.0049 |           3.0581 |           2.5021 |
[32m[20230117 13:27:32 @agent_ppo2.py:193][0m |          -0.0052 |           2.9231 |           2.5037 |
[32m[20230117 13:27:32 @agent_ppo2.py:193][0m |          -0.0060 |           2.9520 |           2.5043 |
[32m[20230117 13:27:32 @agent_ppo2.py:193][0m |          -0.0077 |           2.8322 |           2.5045 |
[32m[20230117 13:27:32 @agent_ppo2.py:193][0m |          -0.0071 |           2.7791 |           2.5052 |
[32m[20230117 13:27:33 @agent_ppo2.py:193][0m |          -0.0057 |           2.7767 |           2.5061 |
[32m[20230117 13:27:33 @agent_ppo2.py:138][0m Policy update time: 1.05 s
[32m[20230117 13:27:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 67.77
[32m[20230117 13:27:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 112.83
[32m[20230117 13:27:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 89.53
[32m[20230117 13:27:33 @agent_ppo2.py:151][0m Total time:       1.13 min
[32m[20230117 13:27:33 @agent_ppo2.py:153][0m 79872 total steps have happened
[32m[20230117 13:27:33 @agent_ppo2.py:129][0m #------------------------ Iteration 39 --------------------------#
[32m[20230117 13:27:33 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:33 @agent_ppo2.py:193][0m |           0.0001 |           1.2816 |           2.5209 |
[32m[20230117 13:27:33 @agent_ppo2.py:193][0m |          -0.0032 |           1.1545 |           2.5211 |
[32m[20230117 13:27:34 @agent_ppo2.py:193][0m |          -0.0045 |           1.1122 |           2.5249 |
[32m[20230117 13:27:34 @agent_ppo2.py:193][0m |          -0.0052 |           1.0862 |           2.5262 |
[32m[20230117 13:27:34 @agent_ppo2.py:193][0m |          -0.0064 |           1.0661 |           2.5299 |
[32m[20230117 13:27:34 @agent_ppo2.py:193][0m |          -0.0069 |           1.0509 |           2.5310 |
[32m[20230117 13:27:34 @agent_ppo2.py:193][0m |          -0.0076 |           1.0402 |           2.5335 |
[32m[20230117 13:27:34 @agent_ppo2.py:193][0m |          -0.0081 |           1.0298 |           2.5349 |
[32m[20230117 13:27:34 @agent_ppo2.py:193][0m |          -0.0085 |           1.0180 |           2.5386 |
[32m[20230117 13:27:34 @agent_ppo2.py:193][0m |          -0.0091 |           1.0103 |           2.5383 |
[32m[20230117 13:27:34 @agent_ppo2.py:138][0m Policy update time: 1.09 s
[32m[20230117 13:27:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 89.21
[32m[20230117 13:27:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 108.80
[32m[20230117 13:27:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 96.16
[32m[20230117 13:27:35 @agent_ppo2.py:151][0m Total time:       1.16 min
[32m[20230117 13:27:35 @agent_ppo2.py:153][0m 81920 total steps have happened
[32m[20230117 13:27:35 @agent_ppo2.py:129][0m #------------------------ Iteration 40 --------------------------#
[32m[20230117 13:27:35 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:35 @agent_ppo2.py:193][0m |           0.0041 |           7.8338 |           2.5975 |
[32m[20230117 13:27:35 @agent_ppo2.py:193][0m |          -0.0054 |           5.5266 |           2.5957 |
[32m[20230117 13:27:35 @agent_ppo2.py:193][0m |          -0.0067 |           5.0366 |           2.5978 |
[32m[20230117 13:27:35 @agent_ppo2.py:193][0m |          -0.0073 |           4.7626 |           2.5975 |
[32m[20230117 13:27:35 @agent_ppo2.py:193][0m |          -0.0075 |           4.5729 |           2.5983 |
[32m[20230117 13:27:35 @agent_ppo2.py:193][0m |          -0.0091 |           4.4170 |           2.5972 |
[32m[20230117 13:27:36 @agent_ppo2.py:193][0m |          -0.0080 |           4.3941 |           2.5966 |
[32m[20230117 13:27:36 @agent_ppo2.py:193][0m |          -0.0090 |           4.1856 |           2.5978 |
[32m[20230117 13:27:36 @agent_ppo2.py:193][0m |          -0.0106 |           4.1889 |           2.5988 |
[32m[20230117 13:27:36 @agent_ppo2.py:193][0m |          -0.0090 |           4.0292 |           2.5986 |
[32m[20230117 13:27:36 @agent_ppo2.py:138][0m Policy update time: 0.98 s
[32m[20230117 13:27:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 36.21
[32m[20230117 13:27:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 126.84
[32m[20230117 13:27:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 102.11
[32m[20230117 13:27:36 @agent_ppo2.py:151][0m Total time:       1.19 min
[32m[20230117 13:27:36 @agent_ppo2.py:153][0m 83968 total steps have happened
[32m[20230117 13:27:36 @agent_ppo2.py:129][0m #------------------------ Iteration 41 --------------------------#
[32m[20230117 13:27:36 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:37 @agent_ppo2.py:193][0m |           0.0002 |           1.9351 |           2.6063 |
[32m[20230117 13:27:37 @agent_ppo2.py:193][0m |          -0.0034 |           1.5874 |           2.6048 |
[32m[20230117 13:27:37 @agent_ppo2.py:193][0m |          -0.0048 |           1.5228 |           2.6044 |
[32m[20230117 13:27:37 @agent_ppo2.py:193][0m |          -0.0056 |           1.4917 |           2.6061 |
[32m[20230117 13:27:37 @agent_ppo2.py:193][0m |          -0.0065 |           1.4718 |           2.6047 |
[32m[20230117 13:27:37 @agent_ppo2.py:193][0m |          -0.0069 |           1.4574 |           2.6062 |
[32m[20230117 13:27:37 @agent_ppo2.py:193][0m |          -0.0075 |           1.4426 |           2.6071 |
[32m[20230117 13:27:37 @agent_ppo2.py:193][0m |          -0.0081 |           1.4267 |           2.6078 |
[32m[20230117 13:27:37 @agent_ppo2.py:193][0m |          -0.0085 |           1.4181 |           2.6077 |
[32m[20230117 13:27:37 @agent_ppo2.py:193][0m |          -0.0090 |           1.4076 |           2.6077 |
[32m[20230117 13:27:37 @agent_ppo2.py:138][0m Policy update time: 1.09 s
[32m[20230117 13:27:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 99.23
[32m[20230117 13:27:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 112.84
[32m[20230117 13:27:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 116.23
[32m[20230117 13:27:38 @agent_ppo2.py:151][0m Total time:       1.21 min
[32m[20230117 13:27:38 @agent_ppo2.py:153][0m 86016 total steps have happened
[32m[20230117 13:27:38 @agent_ppo2.py:129][0m #------------------------ Iteration 42 --------------------------#
[32m[20230117 13:27:38 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:38 @agent_ppo2.py:193][0m |          -0.0005 |           1.5573 |           2.6161 |
[32m[20230117 13:27:38 @agent_ppo2.py:193][0m |          -0.0039 |           1.4192 |           2.6152 |
[32m[20230117 13:27:39 @agent_ppo2.py:193][0m |          -0.0053 |           1.3837 |           2.6172 |
[32m[20230117 13:27:39 @agent_ppo2.py:193][0m |          -0.0059 |           1.3623 |           2.6179 |
[32m[20230117 13:27:39 @agent_ppo2.py:193][0m |          -0.0065 |           1.3486 |           2.6204 |
[32m[20230117 13:27:39 @agent_ppo2.py:193][0m |          -0.0075 |           1.3361 |           2.6232 |
[32m[20230117 13:27:39 @agent_ppo2.py:193][0m |          -0.0076 |           1.3241 |           2.6256 |
[32m[20230117 13:27:39 @agent_ppo2.py:193][0m |          -0.0079 |           1.3114 |           2.6281 |
[32m[20230117 13:27:39 @agent_ppo2.py:193][0m |          -0.0087 |           1.3002 |           2.6293 |
[32m[20230117 13:27:39 @agent_ppo2.py:193][0m |          -0.0085 |           1.2915 |           2.6314 |
[32m[20230117 13:27:39 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:27:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 103.05
[32m[20230117 13:27:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 125.81
[32m[20230117 13:27:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 134.26
[32m[20230117 13:27:40 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 134.26
[32m[20230117 13:27:40 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 134.26
[32m[20230117 13:27:40 @agent_ppo2.py:151][0m Total time:       1.24 min
[32m[20230117 13:27:40 @agent_ppo2.py:153][0m 88064 total steps have happened
[32m[20230117 13:27:40 @agent_ppo2.py:129][0m #------------------------ Iteration 43 --------------------------#
[32m[20230117 13:27:40 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:40 @agent_ppo2.py:193][0m |          -0.0007 |           1.8486 |           2.7009 |
[32m[20230117 13:27:40 @agent_ppo2.py:193][0m |          -0.0045 |           1.6883 |           2.6961 |
[32m[20230117 13:27:40 @agent_ppo2.py:193][0m |          -0.0060 |           1.6429 |           2.6984 |
[32m[20230117 13:27:40 @agent_ppo2.py:193][0m |          -0.0071 |           1.5905 |           2.6974 |
[32m[20230117 13:27:40 @agent_ppo2.py:193][0m |          -0.0071 |           1.5679 |           2.7021 |
[32m[20230117 13:27:41 @agent_ppo2.py:193][0m |          -0.0078 |           1.5776 |           2.7024 |
[32m[20230117 13:27:41 @agent_ppo2.py:193][0m |          -0.0081 |           1.5196 |           2.7034 |
[32m[20230117 13:27:41 @agent_ppo2.py:193][0m |          -0.0086 |           1.4977 |           2.7058 |
[32m[20230117 13:27:41 @agent_ppo2.py:193][0m |          -0.0089 |           1.4732 |           2.7072 |
[32m[20230117 13:27:41 @agent_ppo2.py:193][0m |          -0.0091 |           1.4581 |           2.7088 |
[32m[20230117 13:27:41 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:27:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 125.48
[32m[20230117 13:27:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 131.74
[32m[20230117 13:27:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 147.58
[32m[20230117 13:27:41 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 147.58
[32m[20230117 13:27:41 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 147.58
[32m[20230117 13:27:41 @agent_ppo2.py:151][0m Total time:       1.27 min
[32m[20230117 13:27:41 @agent_ppo2.py:153][0m 90112 total steps have happened
[32m[20230117 13:27:41 @agent_ppo2.py:129][0m #------------------------ Iteration 44 --------------------------#
[32m[20230117 13:27:42 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 4 slaves
[32m[20230117 13:27:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:42 @agent_ppo2.py:193][0m |           0.0004 |           1.7852 |           2.7582 |
[32m[20230117 13:27:42 @agent_ppo2.py:193][0m |          -0.0026 |           1.6475 |           2.7622 |
[32m[20230117 13:27:42 @agent_ppo2.py:193][0m |          -0.0041 |           1.5885 |           2.7637 |
[32m[20230117 13:27:42 @agent_ppo2.py:193][0m |          -0.0048 |           1.5618 |           2.7659 |
[32m[20230117 13:27:42 @agent_ppo2.py:193][0m |          -0.0054 |           1.5601 |           2.7665 |
[32m[20230117 13:27:42 @agent_ppo2.py:193][0m |          -0.0061 |           1.5382 |           2.7676 |
[32m[20230117 13:27:42 @agent_ppo2.py:193][0m |          -0.0073 |           1.5351 |           2.7707 |
[32m[20230117 13:27:42 @agent_ppo2.py:193][0m |          -0.0074 |           1.5526 |           2.7711 |
[32m[20230117 13:27:43 @agent_ppo2.py:193][0m |          -0.0081 |           1.5024 |           2.7727 |
[32m[20230117 13:27:43 @agent_ppo2.py:193][0m |          -0.0083 |           1.4986 |           2.7751 |
[32m[20230117 13:27:43 @agent_ppo2.py:138][0m Policy update time: 1.09 s
[32m[20230117 13:27:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 145.18
[32m[20230117 13:27:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 152.70
[32m[20230117 13:27:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 162.94
[32m[20230117 13:27:43 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 162.94
[32m[20230117 13:27:43 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 162.94
[32m[20230117 13:27:43 @agent_ppo2.py:151][0m Total time:       1.30 min
[32m[20230117 13:27:43 @agent_ppo2.py:153][0m 92160 total steps have happened
[32m[20230117 13:27:43 @agent_ppo2.py:129][0m #------------------------ Iteration 45 --------------------------#
[32m[20230117 13:27:43 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:43 @agent_ppo2.py:193][0m |          -0.0011 |           1.5501 |           2.7883 |
[32m[20230117 13:27:44 @agent_ppo2.py:193][0m |          -0.0051 |           1.4111 |           2.7851 |
[32m[20230117 13:27:44 @agent_ppo2.py:193][0m |          -0.0058 |           1.3836 |           2.7865 |
[32m[20230117 13:27:44 @agent_ppo2.py:193][0m |          -0.0063 |           1.3691 |           2.7862 |
[32m[20230117 13:27:44 @agent_ppo2.py:193][0m |          -0.0068 |           1.3558 |           2.7856 |
[32m[20230117 13:27:44 @agent_ppo2.py:193][0m |          -0.0073 |           1.3431 |           2.7870 |
[32m[20230117 13:27:44 @agent_ppo2.py:193][0m |          -0.0076 |           1.3273 |           2.7872 |
[32m[20230117 13:27:44 @agent_ppo2.py:193][0m |          -0.0082 |           1.3193 |           2.7862 |
[32m[20230117 13:27:44 @agent_ppo2.py:193][0m |          -0.0084 |           1.3065 |           2.7880 |
[32m[20230117 13:27:44 @agent_ppo2.py:193][0m |          -0.0086 |           1.2997 |           2.7871 |
[32m[20230117 13:27:44 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:27:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 156.72
[32m[20230117 13:27:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 173.10
[32m[20230117 13:27:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 224.61
[32m[20230117 13:27:45 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 224.61
[32m[20230117 13:27:45 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 224.61
[32m[20230117 13:27:45 @agent_ppo2.py:151][0m Total time:       1.33 min
[32m[20230117 13:27:45 @agent_ppo2.py:153][0m 94208 total steps have happened
[32m[20230117 13:27:45 @agent_ppo2.py:129][0m #------------------------ Iteration 46 --------------------------#
[32m[20230117 13:27:45 @agent_ppo2.py:135][0m Sampling time: 0.33 s by 4 slaves
[32m[20230117 13:27:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:45 @agent_ppo2.py:193][0m |           0.0004 |          20.8560 |           2.8081 |
[32m[20230117 13:27:45 @agent_ppo2.py:193][0m |          -0.0033 |          14.0452 |           2.8079 |
[32m[20230117 13:27:45 @agent_ppo2.py:193][0m |          -0.0059 |           9.8303 |           2.8045 |
[32m[20230117 13:27:45 @agent_ppo2.py:193][0m |          -0.0076 |           6.4101 |           2.8018 |
[32m[20230117 13:27:46 @agent_ppo2.py:193][0m |          -0.0084 |           4.7684 |           2.7997 |
[32m[20230117 13:27:46 @agent_ppo2.py:193][0m |          -0.0090 |           3.8690 |           2.7984 |
[32m[20230117 13:27:46 @agent_ppo2.py:193][0m |          -0.0091 |           3.3725 |           2.7971 |
[32m[20230117 13:27:46 @agent_ppo2.py:193][0m |          -0.0094 |           3.1223 |           2.7969 |
[32m[20230117 13:27:46 @agent_ppo2.py:193][0m |          -0.0096 |           2.9913 |           2.7971 |
[32m[20230117 13:27:46 @agent_ppo2.py:193][0m |          -0.0102 |           2.8212 |           2.7950 |
[32m[20230117 13:27:46 @agent_ppo2.py:138][0m Policy update time: 0.98 s
[32m[20230117 13:27:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 72.29
[32m[20230117 13:27:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 172.00
[32m[20230117 13:27:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -57.49
[32m[20230117 13:27:46 @agent_ppo2.py:151][0m Total time:       1.35 min
[32m[20230117 13:27:46 @agent_ppo2.py:153][0m 96256 total steps have happened
[32m[20230117 13:27:46 @agent_ppo2.py:129][0m #------------------------ Iteration 47 --------------------------#
[32m[20230117 13:27:46 @agent_ppo2.py:135][0m Sampling time: 0.33 s by 4 slaves
[32m[20230117 13:27:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:47 @agent_ppo2.py:193][0m |          -0.0014 |           8.1801 |           2.7413 |
[32m[20230117 13:27:47 @agent_ppo2.py:193][0m |          -0.0039 |           6.7530 |           2.7379 |
[32m[20230117 13:27:47 @agent_ppo2.py:193][0m |          -0.0041 |           6.4218 |           2.7378 |
[32m[20230117 13:27:47 @agent_ppo2.py:193][0m |          -0.0045 |           6.0732 |           2.7369 |
[32m[20230117 13:27:47 @agent_ppo2.py:193][0m |          -0.0054 |           5.8919 |           2.7372 |
[32m[20230117 13:27:47 @agent_ppo2.py:193][0m |          -0.0060 |           5.6507 |           2.7370 |
[32m[20230117 13:27:47 @agent_ppo2.py:193][0m |          -0.0071 |           5.4183 |           2.7380 |
[32m[20230117 13:27:47 @agent_ppo2.py:193][0m |          -0.0075 |           5.2322 |           2.7368 |
[32m[20230117 13:27:47 @agent_ppo2.py:193][0m |          -0.0069 |           5.2196 |           2.7376 |
[32m[20230117 13:27:48 @agent_ppo2.py:193][0m |          -0.0076 |           4.9756 |           2.7367 |
[32m[20230117 13:27:48 @agent_ppo2.py:138][0m Policy update time: 1.13 s
[32m[20230117 13:27:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 100.95
[32m[20230117 13:27:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 171.76
[32m[20230117 13:27:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -13.47
[32m[20230117 13:27:48 @agent_ppo2.py:151][0m Total time:       1.38 min
[32m[20230117 13:27:48 @agent_ppo2.py:153][0m 98304 total steps have happened
[32m[20230117 13:27:48 @agent_ppo2.py:129][0m #------------------------ Iteration 48 --------------------------#
[32m[20230117 13:27:48 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:48 @agent_ppo2.py:193][0m |           0.0001 |           7.6877 |           2.7710 |
[32m[20230117 13:27:48 @agent_ppo2.py:193][0m |          -0.0034 |           5.6438 |           2.7700 |
[32m[20230117 13:27:48 @agent_ppo2.py:193][0m |          -0.0046 |           5.0817 |           2.7687 |
[32m[20230117 13:27:48 @agent_ppo2.py:193][0m |          -0.0061 |           4.7542 |           2.7684 |
[32m[20230117 13:27:49 @agent_ppo2.py:193][0m |          -0.0062 |           4.6123 |           2.7685 |
[32m[20230117 13:27:49 @agent_ppo2.py:193][0m |          -0.0070 |           4.3779 |           2.7699 |
[32m[20230117 13:27:49 @agent_ppo2.py:193][0m |          -0.0081 |           4.3617 |           2.7704 |
[32m[20230117 13:27:49 @agent_ppo2.py:193][0m |          -0.0083 |           4.2230 |           2.7704 |
[32m[20230117 13:27:49 @agent_ppo2.py:193][0m |          -0.0089 |           4.1373 |           2.7721 |
[32m[20230117 13:27:49 @agent_ppo2.py:193][0m |          -0.0081 |           4.0349 |           2.7713 |
[32m[20230117 13:27:49 @agent_ppo2.py:138][0m Policy update time: 1.00 s
[32m[20230117 13:27:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 94.25
[32m[20230117 13:27:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 151.88
[32m[20230117 13:27:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 251.69
[32m[20230117 13:27:49 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 251.69
[32m[20230117 13:27:49 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 251.69
[32m[20230117 13:27:49 @agent_ppo2.py:151][0m Total time:       1.41 min
[32m[20230117 13:27:49 @agent_ppo2.py:153][0m 100352 total steps have happened
[32m[20230117 13:27:49 @agent_ppo2.py:129][0m #------------------------ Iteration 49 --------------------------#
[32m[20230117 13:27:50 @agent_ppo2.py:135][0m Sampling time: 0.37 s by 4 slaves
[32m[20230117 13:27:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:50 @agent_ppo2.py:193][0m |          -0.0003 |          10.8847 |           2.7768 |
[32m[20230117 13:27:50 @agent_ppo2.py:193][0m |          -0.0010 |           5.1526 |           2.7765 |
[32m[20230117 13:27:50 @agent_ppo2.py:193][0m |          -0.0049 |           4.6649 |           2.7772 |
[32m[20230117 13:27:50 @agent_ppo2.py:193][0m |          -0.0045 |           4.6477 |           2.7772 |
[32m[20230117 13:27:50 @agent_ppo2.py:193][0m |          -0.0067 |           4.2887 |           2.7788 |
[32m[20230117 13:27:50 @agent_ppo2.py:193][0m |          -0.0071 |           4.2129 |           2.7772 |
[32m[20230117 13:27:50 @agent_ppo2.py:193][0m |          -0.0072 |           4.0666 |           2.7790 |
[32m[20230117 13:27:51 @agent_ppo2.py:193][0m |          -0.0083 |           3.9561 |           2.7770 |
[32m[20230117 13:27:51 @agent_ppo2.py:193][0m |          -0.0084 |           3.9393 |           2.7785 |
[32m[20230117 13:27:51 @agent_ppo2.py:193][0m |          -0.0082 |           3.8612 |           2.7776 |
[32m[20230117 13:27:51 @agent_ppo2.py:138][0m Policy update time: 1.09 s
[32m[20230117 13:27:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 134.83
[32m[20230117 13:27:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 168.89
[32m[20230117 13:27:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 173.79
[32m[20230117 13:27:51 @agent_ppo2.py:151][0m Total time:       1.43 min
[32m[20230117 13:27:51 @agent_ppo2.py:153][0m 102400 total steps have happened
[32m[20230117 13:27:51 @agent_ppo2.py:129][0m #------------------------ Iteration 50 --------------------------#
[32m[20230117 13:27:51 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:52 @agent_ppo2.py:193][0m |           0.0010 |           7.7210 |           2.7936 |
[32m[20230117 13:27:52 @agent_ppo2.py:193][0m |          -0.0007 |           5.3310 |           2.7908 |
[32m[20230117 13:27:52 @agent_ppo2.py:193][0m |          -0.0028 |           4.8062 |           2.7884 |
[32m[20230117 13:27:52 @agent_ppo2.py:193][0m |          -0.0042 |           4.4798 |           2.7886 |
[32m[20230117 13:27:52 @agent_ppo2.py:193][0m |          -0.0044 |           4.1592 |           2.7862 |
[32m[20230117 13:27:52 @agent_ppo2.py:193][0m |          -0.0039 |           3.9973 |           2.7876 |
[32m[20230117 13:27:52 @agent_ppo2.py:193][0m |          -0.0060 |           3.8237 |           2.7881 |
[32m[20230117 13:27:52 @agent_ppo2.py:193][0m |          -0.0053 |           3.7231 |           2.7877 |
[32m[20230117 13:27:52 @agent_ppo2.py:193][0m |          -0.0066 |           3.6254 |           2.7895 |
[32m[20230117 13:27:52 @agent_ppo2.py:193][0m |          -0.0072 |           3.5313 |           2.7887 |
[32m[20230117 13:27:52 @agent_ppo2.py:138][0m Policy update time: 0.97 s
[32m[20230117 13:27:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 108.68
[32m[20230117 13:27:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 156.54
[32m[20230117 13:27:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 186.11
[32m[20230117 13:27:53 @agent_ppo2.py:151][0m Total time:       1.46 min
[32m[20230117 13:27:53 @agent_ppo2.py:153][0m 104448 total steps have happened
[32m[20230117 13:27:53 @agent_ppo2.py:129][0m #------------------------ Iteration 51 --------------------------#
[32m[20230117 13:27:53 @agent_ppo2.py:135][0m Sampling time: 0.33 s by 4 slaves
[32m[20230117 13:27:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:53 @agent_ppo2.py:193][0m |           0.0004 |          38.4346 |           2.7879 |
[32m[20230117 13:27:53 @agent_ppo2.py:193][0m |          -0.0024 |          29.2342 |           2.7863 |
[32m[20230117 13:27:53 @agent_ppo2.py:193][0m |          -0.0053 |          24.9204 |           2.7870 |
[32m[20230117 13:27:53 @agent_ppo2.py:193][0m |          -0.0076 |          20.5743 |           2.7847 |
[32m[20230117 13:27:53 @agent_ppo2.py:193][0m |          -0.0083 |          15.4987 |           2.7851 |
[32m[20230117 13:27:53 @agent_ppo2.py:193][0m |          -0.0090 |          12.6356 |           2.7858 |
[32m[20230117 13:27:54 @agent_ppo2.py:193][0m |          -0.0093 |          11.2036 |           2.7848 |
[32m[20230117 13:27:54 @agent_ppo2.py:193][0m |          -0.0105 |          10.5065 |           2.7855 |
[32m[20230117 13:27:54 @agent_ppo2.py:193][0m |          -0.0101 |          10.0484 |           2.7854 |
[32m[20230117 13:27:54 @agent_ppo2.py:193][0m |          -0.0113 |           9.5545 |           2.7844 |
[32m[20230117 13:27:54 @agent_ppo2.py:138][0m Policy update time: 0.80 s
[32m[20230117 13:27:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 48.79
[32m[20230117 13:27:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 157.94
[32m[20230117 13:27:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 184.55
[32m[20230117 13:27:54 @agent_ppo2.py:151][0m Total time:       1.48 min
[32m[20230117 13:27:54 @agent_ppo2.py:153][0m 106496 total steps have happened
[32m[20230117 13:27:54 @agent_ppo2.py:129][0m #------------------------ Iteration 52 --------------------------#
[32m[20230117 13:27:54 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:55 @agent_ppo2.py:193][0m |          -0.0002 |           6.8653 |           2.8449 |
[32m[20230117 13:27:55 @agent_ppo2.py:193][0m |          -0.0034 |           5.8477 |           2.8420 |
[32m[20230117 13:27:55 @agent_ppo2.py:193][0m |          -0.0040 |           5.5386 |           2.8414 |
[32m[20230117 13:27:55 @agent_ppo2.py:193][0m |          -0.0053 |           5.3391 |           2.8442 |
[32m[20230117 13:27:55 @agent_ppo2.py:193][0m |          -0.0061 |           5.1772 |           2.8461 |
[32m[20230117 13:27:55 @agent_ppo2.py:193][0m |          -0.0067 |           5.1076 |           2.8459 |
[32m[20230117 13:27:55 @agent_ppo2.py:193][0m |          -0.0073 |           4.9826 |           2.8473 |
[32m[20230117 13:27:55 @agent_ppo2.py:193][0m |          -0.0078 |           4.8827 |           2.8489 |
[32m[20230117 13:27:55 @agent_ppo2.py:193][0m |          -0.0086 |           4.8023 |           2.8510 |
[32m[20230117 13:27:55 @agent_ppo2.py:193][0m |          -0.0091 |           4.7269 |           2.8526 |
[32m[20230117 13:27:55 @agent_ppo2.py:138][0m Policy update time: 1.09 s
[32m[20230117 13:27:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 156.83
[32m[20230117 13:27:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 168.77
[32m[20230117 13:27:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 210.41
[32m[20230117 13:27:56 @agent_ppo2.py:151][0m Total time:       1.51 min
[32m[20230117 13:27:56 @agent_ppo2.py:153][0m 108544 total steps have happened
[32m[20230117 13:27:56 @agent_ppo2.py:129][0m #------------------------ Iteration 53 --------------------------#
[32m[20230117 13:27:56 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:27:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:56 @agent_ppo2.py:193][0m |          -0.0013 |          11.2005 |           2.8069 |
[32m[20230117 13:27:56 @agent_ppo2.py:193][0m |          -0.0060 |           5.5336 |           2.8020 |
[32m[20230117 13:27:56 @agent_ppo2.py:193][0m |          -0.0070 |           5.0212 |           2.7978 |
[32m[20230117 13:27:56 @agent_ppo2.py:193][0m |          -0.0080 |           4.6276 |           2.7989 |
[32m[20230117 13:27:57 @agent_ppo2.py:193][0m |          -0.0087 |           4.4182 |           2.7979 |
[32m[20230117 13:27:57 @agent_ppo2.py:193][0m |          -0.0100 |           4.2067 |           2.7964 |
[32m[20230117 13:27:57 @agent_ppo2.py:193][0m |          -0.0101 |           4.1255 |           2.7963 |
[32m[20230117 13:27:57 @agent_ppo2.py:193][0m |          -0.0096 |           4.0330 |           2.7961 |
[32m[20230117 13:27:57 @agent_ppo2.py:193][0m |          -0.0097 |           3.9168 |           2.7932 |
[32m[20230117 13:27:57 @agent_ppo2.py:193][0m |          -0.0122 |           3.7864 |           2.7932 |
[32m[20230117 13:27:57 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:27:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 76.05
[32m[20230117 13:27:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 176.18
[32m[20230117 13:27:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 165.56
[32m[20230117 13:27:57 @agent_ppo2.py:151][0m Total time:       1.54 min
[32m[20230117 13:27:57 @agent_ppo2.py:153][0m 110592 total steps have happened
[32m[20230117 13:27:57 @agent_ppo2.py:129][0m #------------------------ Iteration 54 --------------------------#
[32m[20230117 13:27:58 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:27:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:58 @agent_ppo2.py:193][0m |          -0.0006 |          14.5049 |           2.7988 |
[32m[20230117 13:27:58 @agent_ppo2.py:193][0m |          -0.0073 |           6.1354 |           2.7997 |
[32m[20230117 13:27:58 @agent_ppo2.py:193][0m |          -0.0067 |           4.3525 |           2.7984 |
[32m[20230117 13:27:58 @agent_ppo2.py:193][0m |          -0.0084 |           3.8675 |           2.7989 |
[32m[20230117 13:27:58 @agent_ppo2.py:193][0m |          -0.0060 |           3.4763 |           2.7963 |
[32m[20230117 13:27:58 @agent_ppo2.py:193][0m |          -0.0101 |           3.4098 |           2.7957 |
[32m[20230117 13:27:58 @agent_ppo2.py:193][0m |          -0.0089 |           3.2677 |           2.7967 |
[32m[20230117 13:27:58 @agent_ppo2.py:193][0m |          -0.0086 |           3.1720 |           2.7954 |
[32m[20230117 13:27:58 @agent_ppo2.py:193][0m |           0.0055 |           3.0924 |           2.7935 |
[32m[20230117 13:27:59 @agent_ppo2.py:193][0m |          -0.0099 |           3.0159 |           2.7932 |
[32m[20230117 13:27:59 @agent_ppo2.py:138][0m Policy update time: 0.99 s
[32m[20230117 13:27:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 84.90
[32m[20230117 13:27:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 155.35
[32m[20230117 13:27:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 233.03
[32m[20230117 13:27:59 @agent_ppo2.py:151][0m Total time:       1.56 min
[32m[20230117 13:27:59 @agent_ppo2.py:153][0m 112640 total steps have happened
[32m[20230117 13:27:59 @agent_ppo2.py:129][0m #------------------------ Iteration 55 --------------------------#
[32m[20230117 13:27:59 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:27:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:27:59 @agent_ppo2.py:193][0m |           0.0003 |           2.8657 |           2.8370 |
[32m[20230117 13:27:59 @agent_ppo2.py:193][0m |          -0.0035 |           2.5877 |           2.8358 |
[32m[20230117 13:28:00 @agent_ppo2.py:193][0m |          -0.0054 |           2.5071 |           2.8371 |
[32m[20230117 13:28:00 @agent_ppo2.py:193][0m |          -0.0062 |           2.4685 |           2.8378 |
[32m[20230117 13:28:00 @agent_ppo2.py:193][0m |          -0.0069 |           2.4346 |           2.8376 |
[32m[20230117 13:28:00 @agent_ppo2.py:193][0m |          -0.0071 |           2.4195 |           2.8387 |
[32m[20230117 13:28:00 @agent_ppo2.py:193][0m |          -0.0079 |           2.3951 |           2.8397 |
[32m[20230117 13:28:00 @agent_ppo2.py:193][0m |          -0.0080 |           2.3685 |           2.8404 |
[32m[20230117 13:28:00 @agent_ppo2.py:193][0m |          -0.0085 |           2.3464 |           2.8414 |
[32m[20230117 13:28:00 @agent_ppo2.py:193][0m |          -0.0094 |           2.3358 |           2.8442 |
[32m[20230117 13:28:00 @agent_ppo2.py:138][0m Policy update time: 1.09 s
[32m[20230117 13:28:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 168.94
[32m[20230117 13:28:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 180.82
[32m[20230117 13:28:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 201.08
[32m[20230117 13:28:01 @agent_ppo2.py:151][0m Total time:       1.59 min
[32m[20230117 13:28:01 @agent_ppo2.py:153][0m 114688 total steps have happened
[32m[20230117 13:28:01 @agent_ppo2.py:129][0m #------------------------ Iteration 56 --------------------------#
[32m[20230117 13:28:01 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:28:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:01 @agent_ppo2.py:193][0m |           0.0001 |           2.2999 |           2.9222 |
[32m[20230117 13:28:01 @agent_ppo2.py:193][0m |          -0.0042 |           2.1340 |           2.9248 |
[32m[20230117 13:28:01 @agent_ppo2.py:193][0m |          -0.0059 |           2.0786 |           2.9266 |
[32m[20230117 13:28:01 @agent_ppo2.py:193][0m |          -0.0067 |           2.0276 |           2.9299 |
[32m[20230117 13:28:01 @agent_ppo2.py:193][0m |          -0.0079 |           2.0018 |           2.9345 |
[32m[20230117 13:28:02 @agent_ppo2.py:193][0m |          -0.0085 |           1.9681 |           2.9363 |
[32m[20230117 13:28:02 @agent_ppo2.py:193][0m |          -0.0092 |           1.9473 |           2.9408 |
[32m[20230117 13:28:02 @agent_ppo2.py:193][0m |          -0.0094 |           1.9225 |           2.9423 |
[32m[20230117 13:28:02 @agent_ppo2.py:193][0m |          -0.0103 |           1.9016 |           2.9453 |
[32m[20230117 13:28:02 @agent_ppo2.py:193][0m |          -0.0103 |           1.8833 |           2.9469 |
[32m[20230117 13:28:02 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:28:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 168.65
[32m[20230117 13:28:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 196.96
[32m[20230117 13:28:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 111.47
[32m[20230117 13:28:02 @agent_ppo2.py:151][0m Total time:       1.62 min
[32m[20230117 13:28:02 @agent_ppo2.py:153][0m 116736 total steps have happened
[32m[20230117 13:28:02 @agent_ppo2.py:129][0m #------------------------ Iteration 57 --------------------------#
[32m[20230117 13:28:03 @agent_ppo2.py:135][0m Sampling time: 0.33 s by 4 slaves
[32m[20230117 13:28:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:03 @agent_ppo2.py:193][0m |          -0.0015 |           2.5569 |           2.9670 |
[32m[20230117 13:28:03 @agent_ppo2.py:193][0m |          -0.0044 |           2.3797 |           2.9639 |
[32m[20230117 13:28:03 @agent_ppo2.py:193][0m |          -0.0058 |           2.3197 |           2.9631 |
[32m[20230117 13:28:03 @agent_ppo2.py:193][0m |          -0.0068 |           2.2787 |           2.9621 |
[32m[20230117 13:28:03 @agent_ppo2.py:193][0m |          -0.0072 |           2.2358 |           2.9653 |
[32m[20230117 13:28:03 @agent_ppo2.py:193][0m |          -0.0076 |           2.2230 |           2.9654 |
[32m[20230117 13:28:03 @agent_ppo2.py:193][0m |          -0.0082 |           2.2067 |           2.9657 |
[32m[20230117 13:28:03 @agent_ppo2.py:193][0m |          -0.0084 |           2.1876 |           2.9664 |
[32m[20230117 13:28:04 @agent_ppo2.py:193][0m |          -0.0090 |           2.1729 |           2.9674 |
[32m[20230117 13:28:04 @agent_ppo2.py:193][0m |          -0.0090 |           2.1464 |           2.9684 |
[32m[20230117 13:28:04 @agent_ppo2.py:138][0m Policy update time: 1.09 s
[32m[20230117 13:28:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 167.35
[32m[20230117 13:28:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 180.89
[32m[20230117 13:28:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 243.41
[32m[20230117 13:28:04 @agent_ppo2.py:151][0m Total time:       1.65 min
[32m[20230117 13:28:04 @agent_ppo2.py:153][0m 118784 total steps have happened
[32m[20230117 13:28:04 @agent_ppo2.py:129][0m #------------------------ Iteration 58 --------------------------#
[32m[20230117 13:28:04 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:28:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:04 @agent_ppo2.py:193][0m |          -0.0000 |           2.3417 |           2.9587 |
[32m[20230117 13:28:05 @agent_ppo2.py:193][0m |          -0.0024 |           2.2658 |           2.9557 |
[32m[20230117 13:28:05 @agent_ppo2.py:193][0m |          -0.0036 |           2.2375 |           2.9545 |
[32m[20230117 13:28:05 @agent_ppo2.py:193][0m |          -0.0043 |           2.2160 |           2.9568 |
[32m[20230117 13:28:05 @agent_ppo2.py:193][0m |          -0.0050 |           2.1913 |           2.9567 |
[32m[20230117 13:28:05 @agent_ppo2.py:193][0m |          -0.0052 |           2.1766 |           2.9572 |
[32m[20230117 13:28:05 @agent_ppo2.py:193][0m |          -0.0061 |           2.1695 |           2.9566 |
[32m[20230117 13:28:05 @agent_ppo2.py:193][0m |          -0.0066 |           2.1636 |           2.9577 |
[32m[20230117 13:28:05 @agent_ppo2.py:193][0m |          -0.0070 |           2.1384 |           2.9583 |
[32m[20230117 13:28:05 @agent_ppo2.py:193][0m |          -0.0069 |           2.1338 |           2.9589 |
[32m[20230117 13:28:05 @agent_ppo2.py:138][0m Policy update time: 1.10 s
[32m[20230117 13:28:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 169.00
[32m[20230117 13:28:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 187.18
[32m[20230117 13:28:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 16.85
[32m[20230117 13:28:06 @agent_ppo2.py:151][0m Total time:       1.68 min
[32m[20230117 13:28:06 @agent_ppo2.py:153][0m 120832 total steps have happened
[32m[20230117 13:28:06 @agent_ppo2.py:129][0m #------------------------ Iteration 59 --------------------------#
[32m[20230117 13:28:06 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:28:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:06 @agent_ppo2.py:193][0m |          -0.0012 |           4.9864 |           2.9804 |
[32m[20230117 13:28:06 @agent_ppo2.py:193][0m |          -0.0040 |           3.4969 |           2.9785 |
[32m[20230117 13:28:06 @agent_ppo2.py:193][0m |          -0.0030 |           3.1872 |           2.9757 |
[32m[20230117 13:28:06 @agent_ppo2.py:193][0m |          -0.0065 |           3.0275 |           2.9777 |
[32m[20230117 13:28:06 @agent_ppo2.py:193][0m |          -0.0055 |           2.9226 |           2.9770 |
[32m[20230117 13:28:06 @agent_ppo2.py:193][0m |          -0.0079 |           2.8568 |           2.9774 |
[32m[20230117 13:28:07 @agent_ppo2.py:193][0m |          -0.0072 |           2.8047 |           2.9803 |
[32m[20230117 13:28:07 @agent_ppo2.py:193][0m |          -0.0092 |           2.7621 |           2.9788 |
[32m[20230117 13:28:07 @agent_ppo2.py:193][0m |          -0.0087 |           2.7178 |           2.9800 |
[32m[20230117 13:28:07 @agent_ppo2.py:193][0m |          -0.0094 |           2.6845 |           2.9826 |
[32m[20230117 13:28:07 @agent_ppo2.py:138][0m Policy update time: 1.01 s
[32m[20230117 13:28:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 135.27
[32m[20230117 13:28:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 182.31
[32m[20230117 13:28:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 248.28
[32m[20230117 13:28:07 @agent_ppo2.py:151][0m Total time:       1.70 min
[32m[20230117 13:28:07 @agent_ppo2.py:153][0m 122880 total steps have happened
[32m[20230117 13:28:07 @agent_ppo2.py:129][0m #------------------------ Iteration 60 --------------------------#
[32m[20230117 13:28:08 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:28:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:08 @agent_ppo2.py:193][0m |          -0.0011 |           4.9108 |           3.0055 |
[32m[20230117 13:28:08 @agent_ppo2.py:193][0m |           0.0002 |           3.6283 |           3.0068 |
[32m[20230117 13:28:08 @agent_ppo2.py:193][0m |          -0.0016 |           3.2955 |           3.0058 |
[32m[20230117 13:28:08 @agent_ppo2.py:193][0m |          -0.0075 |           3.3310 |           3.0105 |
[32m[20230117 13:28:08 @agent_ppo2.py:193][0m |          -0.0084 |           3.0099 |           3.0105 |
[32m[20230117 13:28:08 @agent_ppo2.py:193][0m |          -0.0094 |           2.9346 |           3.0115 |
[32m[20230117 13:28:08 @agent_ppo2.py:193][0m |          -0.0090 |           2.8010 |           3.0145 |
[32m[20230117 13:28:08 @agent_ppo2.py:193][0m |          -0.0048 |           2.7919 |           3.0162 |
[32m[20230117 13:28:09 @agent_ppo2.py:193][0m |          -0.0054 |           2.7642 |           3.0120 |
[32m[20230117 13:28:09 @agent_ppo2.py:193][0m |          -0.0085 |           2.6311 |           3.0159 |
[32m[20230117 13:28:09 @agent_ppo2.py:138][0m Policy update time: 1.12 s
[32m[20230117 13:28:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 131.86
[32m[20230117 13:28:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 203.43
[32m[20230117 13:28:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.44
[32m[20230117 13:28:09 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 262.44
[32m[20230117 13:28:09 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 262.44
[32m[20230117 13:28:09 @agent_ppo2.py:151][0m Total time:       1.73 min
[32m[20230117 13:28:09 @agent_ppo2.py:153][0m 124928 total steps have happened
[32m[20230117 13:28:09 @agent_ppo2.py:129][0m #------------------------ Iteration 61 --------------------------#
[32m[20230117 13:28:09 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:28:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:09 @agent_ppo2.py:193][0m |           0.0003 |           2.1174 |           3.1038 |
[32m[20230117 13:28:10 @agent_ppo2.py:193][0m |          -0.0033 |           1.9633 |           3.0983 |
[32m[20230117 13:28:10 @agent_ppo2.py:193][0m |          -0.0053 |           1.9132 |           3.0973 |
[32m[20230117 13:28:10 @agent_ppo2.py:193][0m |          -0.0064 |           1.8712 |           3.0958 |
[32m[20230117 13:28:10 @agent_ppo2.py:193][0m |          -0.0070 |           1.8414 |           3.0964 |
[32m[20230117 13:28:10 @agent_ppo2.py:193][0m |          -0.0078 |           1.8151 |           3.0966 |
[32m[20230117 13:28:10 @agent_ppo2.py:193][0m |          -0.0082 |           1.7954 |           3.0961 |
[32m[20230117 13:28:10 @agent_ppo2.py:193][0m |          -0.0088 |           1.7723 |           3.0962 |
[32m[20230117 13:28:10 @agent_ppo2.py:193][0m |          -0.0093 |           1.7510 |           3.0961 |
[32m[20230117 13:28:10 @agent_ppo2.py:193][0m |          -0.0101 |           1.7242 |           3.0955 |
[32m[20230117 13:28:10 @agent_ppo2.py:138][0m Policy update time: 1.11 s
[32m[20230117 13:28:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 184.86
[32m[20230117 13:28:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 195.54
[32m[20230117 13:28:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 200.99
[32m[20230117 13:28:11 @agent_ppo2.py:151][0m Total time:       1.76 min
[32m[20230117 13:28:11 @agent_ppo2.py:153][0m 126976 total steps have happened
[32m[20230117 13:28:11 @agent_ppo2.py:129][0m #------------------------ Iteration 62 --------------------------#
[32m[20230117 13:28:11 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:28:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:11 @agent_ppo2.py:193][0m |          -0.0007 |           6.4613 |           3.0465 |
[32m[20230117 13:28:11 @agent_ppo2.py:193][0m |          -0.0034 |           4.2369 |           3.0431 |
[32m[20230117 13:28:11 @agent_ppo2.py:193][0m |          -0.0048 |           3.7566 |           3.0430 |
[32m[20230117 13:28:11 @agent_ppo2.py:193][0m |          -0.0048 |           3.4839 |           3.0411 |
[32m[20230117 13:28:12 @agent_ppo2.py:193][0m |          -0.0069 |           3.3650 |           3.0411 |
[32m[20230117 13:28:12 @agent_ppo2.py:193][0m |          -0.0072 |           3.2034 |           3.0391 |
[32m[20230117 13:28:12 @agent_ppo2.py:193][0m |          -0.0082 |           3.1328 |           3.0379 |
[32m[20230117 13:28:12 @agent_ppo2.py:193][0m |          -0.0076 |           3.0204 |           3.0403 |
[32m[20230117 13:28:12 @agent_ppo2.py:193][0m |          -0.0088 |           2.9781 |           3.0380 |
[32m[20230117 13:28:12 @agent_ppo2.py:193][0m |          -0.0091 |           2.9228 |           3.0391 |
[32m[20230117 13:28:12 @agent_ppo2.py:138][0m Policy update time: 1.07 s
[32m[20230117 13:28:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 128.71
[32m[20230117 13:28:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 160.42
[32m[20230117 13:28:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -112.20
[32m[20230117 13:28:12 @agent_ppo2.py:151][0m Total time:       1.79 min
[32m[20230117 13:28:12 @agent_ppo2.py:153][0m 129024 total steps have happened
[32m[20230117 13:28:12 @agent_ppo2.py:129][0m #------------------------ Iteration 63 --------------------------#
[32m[20230117 13:28:12 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:28:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:13 @agent_ppo2.py:193][0m |          -0.0015 |          10.6669 |           3.0435 |
[32m[20230117 13:28:13 @agent_ppo2.py:193][0m |          -0.0058 |           8.5108 |           3.0400 |
[32m[20230117 13:28:13 @agent_ppo2.py:193][0m |          -0.0074 |           8.0514 |           3.0418 |
[32m[20230117 13:28:13 @agent_ppo2.py:193][0m |          -0.0078 |           7.8111 |           3.0424 |
[32m[20230117 13:28:13 @agent_ppo2.py:193][0m |          -0.0091 |           7.6610 |           3.0435 |
[32m[20230117 13:28:13 @agent_ppo2.py:193][0m |          -0.0088 |           7.4740 |           3.0451 |
[32m[20230117 13:28:13 @agent_ppo2.py:193][0m |          -0.0096 |           7.3920 |           3.0471 |
[32m[20230117 13:28:13 @agent_ppo2.py:193][0m |          -0.0099 |           7.2664 |           3.0455 |
[32m[20230117 13:28:13 @agent_ppo2.py:193][0m |          -0.0108 |           7.1895 |           3.0473 |
[32m[20230117 13:28:13 @agent_ppo2.py:193][0m |          -0.0116 |           7.0981 |           3.0491 |
[32m[20230117 13:28:13 @agent_ppo2.py:138][0m Policy update time: 0.99 s
[32m[20230117 13:28:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 164.87
[32m[20230117 13:28:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 240.11
[32m[20230117 13:28:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 252.05
[32m[20230117 13:28:14 @agent_ppo2.py:151][0m Total time:       1.81 min
[32m[20230117 13:28:14 @agent_ppo2.py:153][0m 131072 total steps have happened
[32m[20230117 13:28:14 @agent_ppo2.py:129][0m #------------------------ Iteration 64 --------------------------#
[32m[20230117 13:28:14 @agent_ppo2.py:135][0m Sampling time: 0.33 s by 4 slaves
[32m[20230117 13:28:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:14 @agent_ppo2.py:193][0m |           0.0002 |           2.3945 |           3.0794 |
[32m[20230117 13:28:14 @agent_ppo2.py:193][0m |          -0.0033 |           2.1240 |           3.0768 |
[32m[20230117 13:28:14 @agent_ppo2.py:193][0m |          -0.0044 |           2.0405 |           3.0780 |
[32m[20230117 13:28:15 @agent_ppo2.py:193][0m |          -0.0055 |           1.9887 |           3.0802 |
[32m[20230117 13:28:15 @agent_ppo2.py:193][0m |          -0.0060 |           1.9530 |           3.0831 |
[32m[20230117 13:28:15 @agent_ppo2.py:193][0m |          -0.0065 |           1.9159 |           3.0832 |
[32m[20230117 13:28:15 @agent_ppo2.py:193][0m |          -0.0075 |           1.8928 |           3.0843 |
[32m[20230117 13:28:15 @agent_ppo2.py:193][0m |          -0.0077 |           1.8721 |           3.0855 |
[32m[20230117 13:28:15 @agent_ppo2.py:193][0m |          -0.0081 |           1.8514 |           3.0871 |
[32m[20230117 13:28:15 @agent_ppo2.py:193][0m |          -0.0084 |           1.8392 |           3.0885 |
[32m[20230117 13:28:15 @agent_ppo2.py:138][0m Policy update time: 1.09 s
[32m[20230117 13:28:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 196.99
[32m[20230117 13:28:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 218.02
[32m[20230117 13:28:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 260.98
[32m[20230117 13:28:15 @agent_ppo2.py:151][0m Total time:       1.84 min
[32m[20230117 13:28:15 @agent_ppo2.py:153][0m 133120 total steps have happened
[32m[20230117 13:28:15 @agent_ppo2.py:129][0m #------------------------ Iteration 65 --------------------------#
[32m[20230117 13:28:16 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:28:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:16 @agent_ppo2.py:193][0m |          -0.0003 |           3.7504 |           3.1697 |
[32m[20230117 13:28:16 @agent_ppo2.py:193][0m |          -0.0013 |           3.6327 |           3.1691 |
[32m[20230117 13:28:16 @agent_ppo2.py:193][0m |          -0.0041 |           3.5903 |           3.1694 |
[32m[20230117 13:28:16 @agent_ppo2.py:193][0m |          -0.0045 |           3.5621 |           3.1703 |
[32m[20230117 13:28:16 @agent_ppo2.py:193][0m |          -0.0047 |           3.5633 |           3.1709 |
[32m[20230117 13:28:16 @agent_ppo2.py:193][0m |          -0.0060 |           3.5244 |           3.1732 |
[32m[20230117 13:28:16 @agent_ppo2.py:193][0m |          -0.0070 |           3.5021 |           3.1753 |
[32m[20230117 13:28:17 @agent_ppo2.py:193][0m |          -0.0068 |           3.4891 |           3.1748 |
[32m[20230117 13:28:17 @agent_ppo2.py:193][0m |          -0.0080 |           3.4792 |           3.1759 |
[32m[20230117 13:28:17 @agent_ppo2.py:193][0m |          -0.0063 |           3.4909 |           3.1792 |
[32m[20230117 13:28:17 @agent_ppo2.py:138][0m Policy update time: 1.08 s
[32m[20230117 13:28:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 221.39
[32m[20230117 13:28:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 241.12
[32m[20230117 13:28:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 260.36
[32m[20230117 13:28:17 @agent_ppo2.py:151][0m Total time:       1.87 min
[32m[20230117 13:28:17 @agent_ppo2.py:153][0m 135168 total steps have happened
[32m[20230117 13:28:17 @agent_ppo2.py:129][0m #------------------------ Iteration 66 --------------------------#
[32m[20230117 13:28:17 @agent_ppo2.py:135][0m Sampling time: 0.34 s by 4 slaves
[32m[20230117 13:28:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:18 @agent_ppo2.py:193][0m |           0.0001 |           2.2673 |           3.2113 |
[32m[20230117 13:28:18 @agent_ppo2.py:193][0m |          -0.0041 |           2.1139 |           3.2094 |
[32m[20230117 13:28:18 @agent_ppo2.py:193][0m |          -0.0063 |           2.0697 |           3.2133 |
[32m[20230117 13:28:18 @agent_ppo2.py:193][0m |          -0.0067 |           2.0480 |           3.2139 |
[32m[20230117 13:28:18 @agent_ppo2.py:193][0m |          -0.0079 |           2.0286 |           3.2152 |
[32m[20230117 13:28:18 @agent_ppo2.py:193][0m |          -0.0090 |           2.0057 |           3.2205 |
[32m[20230117 13:28:18 @agent_ppo2.py:193][0m |          -0.0090 |           1.9958 |           3.2206 |
[32m[20230117 13:28:18 @agent_ppo2.py:193][0m |          -0.0097 |           1.9940 |           3.2266 |
[32m[20230117 13:28:18 @agent_ppo2.py:193][0m |          -0.0100 |           1.9744 |           3.2267 |
[32m[20230117 13:28:19 @agent_ppo2.py:193][0m |          -0.0108 |           1.9532 |           3.2274 |
[32m[20230117 13:28:19 @agent_ppo2.py:138][0m Policy update time: 1.13 s
[32m[20230117 13:28:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 220.28
[32m[20230117 13:28:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 232.49
[32m[20230117 13:28:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 261.57
[32m[20230117 13:28:19 @agent_ppo2.py:151][0m Total time:       1.90 min
[32m[20230117 13:28:19 @agent_ppo2.py:153][0m 137216 total steps have happened
[32m[20230117 13:28:19 @agent_ppo2.py:129][0m #------------------------ Iteration 67 --------------------------#
[32m[20230117 13:28:19 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:28:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:19 @agent_ppo2.py:193][0m |           0.0007 |          20.6500 |           3.2549 |
[32m[20230117 13:28:19 @agent_ppo2.py:193][0m |          -0.0056 |           6.9338 |           3.2478 |
[32m[20230117 13:28:20 @agent_ppo2.py:193][0m |          -0.0056 |           6.0732 |           3.2499 |
[32m[20230117 13:28:20 @agent_ppo2.py:193][0m |          -0.0053 |           5.6884 |           3.2488 |
[32m[20230117 13:28:20 @agent_ppo2.py:193][0m |          -0.0080 |           5.2248 |           3.2467 |
[32m[20230117 13:28:20 @agent_ppo2.py:193][0m |          -0.0083 |           4.9604 |           3.2472 |
[32m[20230117 13:28:20 @agent_ppo2.py:193][0m |          -0.0090 |           4.8343 |           3.2486 |
[32m[20230117 13:28:20 @agent_ppo2.py:193][0m |          -0.0091 |           4.7308 |           3.2483 |
[32m[20230117 13:28:20 @agent_ppo2.py:193][0m |          -0.0093 |           4.6576 |           3.2508 |
[32m[20230117 13:28:20 @agent_ppo2.py:193][0m |          -0.0074 |           4.5277 |           3.2488 |
[32m[20230117 13:28:20 @agent_ppo2.py:138][0m Policy update time: 1.22 s
[32m[20230117 13:28:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 95.73
[32m[20230117 13:28:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 238.99
[32m[20230117 13:28:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -113.66
[32m[20230117 13:28:20 @agent_ppo2.py:151][0m Total time:       1.92 min
[32m[20230117 13:28:20 @agent_ppo2.py:153][0m 139264 total steps have happened
[32m[20230117 13:28:20 @agent_ppo2.py:129][0m #------------------------ Iteration 68 --------------------------#
[32m[20230117 13:28:21 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:28:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:21 @agent_ppo2.py:193][0m |           0.0005 |           3.6513 |           3.2646 |
[32m[20230117 13:28:21 @agent_ppo2.py:193][0m |          -0.0033 |           3.3127 |           3.2632 |
[32m[20230117 13:28:21 @agent_ppo2.py:193][0m |          -0.0044 |           3.2023 |           3.2617 |
[32m[20230117 13:28:21 @agent_ppo2.py:193][0m |          -0.0054 |           3.1517 |           3.2621 |
[32m[20230117 13:28:21 @agent_ppo2.py:193][0m |          -0.0064 |           3.1135 |           3.2598 |
[32m[20230117 13:28:21 @agent_ppo2.py:193][0m |          -0.0069 |           3.0769 |           3.2610 |
[32m[20230117 13:28:22 @agent_ppo2.py:193][0m |          -0.0073 |           3.0541 |           3.2611 |
[32m[20230117 13:28:22 @agent_ppo2.py:193][0m |          -0.0078 |           3.0471 |           3.2624 |
[32m[20230117 13:28:22 @agent_ppo2.py:193][0m |          -0.0081 |           3.0024 |           3.2613 |
[32m[20230117 13:28:22 @agent_ppo2.py:193][0m |          -0.0083 |           2.9922 |           3.2597 |
[32m[20230117 13:28:22 @agent_ppo2.py:138][0m Policy update time: 1.13 s
[32m[20230117 13:28:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 219.66
[32m[20230117 13:28:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 233.09
[32m[20230117 13:28:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -113.71
[32m[20230117 13:28:22 @agent_ppo2.py:151][0m Total time:       1.95 min
[32m[20230117 13:28:22 @agent_ppo2.py:153][0m 141312 total steps have happened
[32m[20230117 13:28:22 @agent_ppo2.py:129][0m #------------------------ Iteration 69 --------------------------#
[32m[20230117 13:28:22 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:28:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:23 @agent_ppo2.py:193][0m |          -0.0000 |           4.0813 |           3.2799 |
[32m[20230117 13:28:23 @agent_ppo2.py:193][0m |          -0.0021 |           3.8586 |           3.2756 |
[32m[20230117 13:28:23 @agent_ppo2.py:193][0m |          -0.0065 |           3.7726 |           3.2775 |
[32m[20230117 13:28:23 @agent_ppo2.py:193][0m |          -0.0073 |           3.7280 |           3.2787 |
[32m[20230117 13:28:23 @agent_ppo2.py:193][0m |          -0.0016 |           3.7505 |           3.2795 |
[32m[20230117 13:28:23 @agent_ppo2.py:193][0m |          -0.0083 |           3.6498 |           3.2828 |
[32m[20230117 13:28:23 @agent_ppo2.py:193][0m |          -0.0070 |           3.6351 |           3.2854 |
[32m[20230117 13:28:23 @agent_ppo2.py:193][0m |          -0.0065 |           3.6045 |           3.2836 |
[32m[20230117 13:28:23 @agent_ppo2.py:193][0m |          -0.0046 |           3.5938 |           3.2866 |
[32m[20230117 13:28:23 @agent_ppo2.py:193][0m |          -0.0021 |           3.7405 |           3.2865 |
[32m[20230117 13:28:23 @agent_ppo2.py:138][0m Policy update time: 1.06 s
[32m[20230117 13:28:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: 232.95
[32m[20230117 13:28:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.06
[32m[20230117 13:28:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -42.05
[32m[20230117 13:28:23 @agent_ppo2.py:151][0m Total time:       1.97 min
[32m[20230117 13:28:23 @agent_ppo2.py:153][0m 143360 total steps have happened
[32m[20230117 13:28:23 @agent_ppo2.py:129][0m #------------------------ Iteration 70 --------------------------#
[32m[20230117 13:28:24 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:28:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:24 @agent_ppo2.py:193][0m |           0.0019 |           8.7886 |           3.3347 |
[32m[20230117 13:28:24 @agent_ppo2.py:193][0m |           0.0000 |           6.1268 |           3.3349 |
[32m[20230117 13:28:24 @agent_ppo2.py:193][0m |          -0.0020 |           5.8865 |           3.3353 |
[32m[20230117 13:28:24 @agent_ppo2.py:193][0m |          -0.0053 |           5.7522 |           3.3357 |
[32m[20230117 13:28:24 @agent_ppo2.py:193][0m |          -0.0057 |           5.6355 |           3.3345 |
[32m[20230117 13:28:24 @agent_ppo2.py:193][0m |          -0.0062 |           5.5369 |           3.3362 |
[32m[20230117 13:28:24 @agent_ppo2.py:193][0m |          -0.0055 |           5.4438 |           3.3355 |
[32m[20230117 13:28:25 @agent_ppo2.py:193][0m |          -0.0076 |           5.3611 |           3.3365 |
[32m[20230117 13:28:25 @agent_ppo2.py:193][0m |          -0.0088 |           5.2768 |           3.3355 |
[32m[20230117 13:28:25 @agent_ppo2.py:193][0m |          -0.0079 |           5.2341 |           3.3375 |
[32m[20230117 13:28:25 @agent_ppo2.py:138][0m Policy update time: 0.89 s
[32m[20230117 13:28:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 174.04
[32m[20230117 13:28:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 238.26
[32m[20230117 13:28:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 108.88
[32m[20230117 13:28:25 @agent_ppo2.py:151][0m Total time:       2.00 min
[32m[20230117 13:28:25 @agent_ppo2.py:153][0m 145408 total steps have happened
[32m[20230117 13:28:25 @agent_ppo2.py:129][0m #------------------------ Iteration 71 --------------------------#
[32m[20230117 13:28:25 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:28:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:25 @agent_ppo2.py:193][0m |          -0.0005 |           4.5311 |           3.3094 |
[32m[20230117 13:28:25 @agent_ppo2.py:193][0m |          -0.0041 |           4.3843 |           3.3031 |
[32m[20230117 13:28:26 @agent_ppo2.py:193][0m |          -0.0060 |           4.3103 |           3.3066 |
[32m[20230117 13:28:26 @agent_ppo2.py:193][0m |          -0.0068 |           4.2665 |           3.3059 |
[32m[20230117 13:28:26 @agent_ppo2.py:193][0m |          -0.0076 |           4.2358 |           3.3095 |
[32m[20230117 13:28:26 @agent_ppo2.py:193][0m |          -0.0082 |           4.2081 |           3.3111 |
[32m[20230117 13:28:26 @agent_ppo2.py:193][0m |          -0.0090 |           4.1850 |           3.3126 |
[32m[20230117 13:28:26 @agent_ppo2.py:193][0m |          -0.0089 |           4.1829 |           3.3133 |
[32m[20230117 13:28:26 @agent_ppo2.py:193][0m |          -0.0098 |           4.1510 |           3.3165 |
[32m[20230117 13:28:26 @agent_ppo2.py:193][0m |          -0.0099 |           4.1334 |           3.3167 |
[32m[20230117 13:28:26 @agent_ppo2.py:138][0m Policy update time: 1.05 s
[32m[20230117 13:28:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 227.48
[32m[20230117 13:28:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.61
[32m[20230117 13:28:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -107.46
[32m[20230117 13:28:26 @agent_ppo2.py:151][0m Total time:       2.02 min
[32m[20230117 13:28:26 @agent_ppo2.py:153][0m 147456 total steps have happened
[32m[20230117 13:28:26 @agent_ppo2.py:129][0m #------------------------ Iteration 72 --------------------------#
[32m[20230117 13:28:27 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:28:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:27 @agent_ppo2.py:193][0m |           0.0010 |          14.0476 |           3.3074 |
[32m[20230117 13:28:27 @agent_ppo2.py:193][0m |          -0.0022 |           7.4930 |           3.3083 |
[32m[20230117 13:28:27 @agent_ppo2.py:193][0m |          -0.0036 |           6.5615 |           3.3093 |
[32m[20230117 13:28:27 @agent_ppo2.py:193][0m |          -0.0040 |           6.0782 |           3.3072 |
[32m[20230117 13:28:27 @agent_ppo2.py:193][0m |          -0.0046 |           5.8470 |           3.3050 |
[32m[20230117 13:28:27 @agent_ppo2.py:193][0m |          -0.0055 |           5.8574 |           3.3054 |
[32m[20230117 13:28:27 @agent_ppo2.py:193][0m |          -0.0061 |           5.5953 |           3.3037 |
[32m[20230117 13:28:28 @agent_ppo2.py:193][0m |          -0.0062 |           5.4949 |           3.3039 |
[32m[20230117 13:28:28 @agent_ppo2.py:193][0m |          -0.0067 |           5.4125 |           3.3036 |
[32m[20230117 13:28:28 @agent_ppo2.py:193][0m |          -0.0073 |           5.3810 |           3.3025 |
[32m[20230117 13:28:28 @agent_ppo2.py:138][0m Policy update time: 1.05 s
[32m[20230117 13:28:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 167.59
[32m[20230117 13:28:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 239.83
[32m[20230117 13:28:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -117.94
[32m[20230117 13:28:28 @agent_ppo2.py:151][0m Total time:       2.05 min
[32m[20230117 13:28:28 @agent_ppo2.py:153][0m 149504 total steps have happened
[32m[20230117 13:28:28 @agent_ppo2.py:129][0m #------------------------ Iteration 73 --------------------------#
[32m[20230117 13:28:28 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:28:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:28 @agent_ppo2.py:193][0m |          -0.0000 |          14.4666 |           3.3722 |
[32m[20230117 13:28:28 @agent_ppo2.py:193][0m |          -0.0047 |           7.0632 |           3.3690 |
[32m[20230117 13:28:28 @agent_ppo2.py:193][0m |          -0.0050 |           6.0397 |           3.3680 |
[32m[20230117 13:28:29 @agent_ppo2.py:193][0m |          -0.0080 |           5.4386 |           3.3662 |
[32m[20230117 13:28:29 @agent_ppo2.py:193][0m |          -0.0076 |           5.1918 |           3.3666 |
[32m[20230117 13:28:29 @agent_ppo2.py:193][0m |          -0.0089 |           4.9358 |           3.3664 |
[32m[20230117 13:28:29 @agent_ppo2.py:193][0m |          -0.0094 |           4.7913 |           3.3629 |
[32m[20230117 13:28:29 @agent_ppo2.py:193][0m |          -0.0099 |           4.6258 |           3.3650 |
[32m[20230117 13:28:29 @agent_ppo2.py:193][0m |          -0.0108 |           4.4564 |           3.3624 |
[32m[20230117 13:28:29 @agent_ppo2.py:193][0m |          -0.0108 |           4.4101 |           3.3617 |
[32m[20230117 13:28:29 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:28:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 112.39
[32m[20230117 13:28:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 240.34
[32m[20230117 13:28:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -118.51
[32m[20230117 13:28:29 @agent_ppo2.py:151][0m Total time:       2.07 min
[32m[20230117 13:28:29 @agent_ppo2.py:153][0m 151552 total steps have happened
[32m[20230117 13:28:29 @agent_ppo2.py:129][0m #------------------------ Iteration 74 --------------------------#
[32m[20230117 13:28:29 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:28:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:30 @agent_ppo2.py:193][0m |          -0.0003 |          12.6231 |           3.3235 |
[32m[20230117 13:28:30 @agent_ppo2.py:193][0m |          -0.0028 |           7.9049 |           3.3218 |
[32m[20230117 13:28:30 @agent_ppo2.py:193][0m |          -0.0050 |           7.1906 |           3.3203 |
[32m[20230117 13:28:30 @agent_ppo2.py:193][0m |          -0.0048 |           6.8430 |           3.3215 |
[32m[20230117 13:28:30 @agent_ppo2.py:193][0m |          -0.0055 |           6.6332 |           3.3214 |
[32m[20230117 13:28:30 @agent_ppo2.py:193][0m |          -0.0077 |           6.4761 |           3.3209 |
[32m[20230117 13:28:30 @agent_ppo2.py:193][0m |          -0.0087 |           6.3980 |           3.3211 |
[32m[20230117 13:28:30 @agent_ppo2.py:193][0m |          -0.0095 |           6.2317 |           3.3209 |
[32m[20230117 13:28:30 @agent_ppo2.py:193][0m |          -0.0087 |           6.1296 |           3.3212 |
[32m[20230117 13:28:30 @agent_ppo2.py:193][0m |          -0.0110 |           6.0833 |           3.3221 |
[32m[20230117 13:28:30 @agent_ppo2.py:138][0m Policy update time: 1.01 s
[32m[20230117 13:28:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 128.57
[32m[20230117 13:28:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 239.85
[32m[20230117 13:28:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 96.51
[32m[20230117 13:28:31 @agent_ppo2.py:151][0m Total time:       2.09 min
[32m[20230117 13:28:31 @agent_ppo2.py:153][0m 153600 total steps have happened
[32m[20230117 13:28:31 @agent_ppo2.py:129][0m #------------------------ Iteration 75 --------------------------#
[32m[20230117 13:28:31 @agent_ppo2.py:135][0m Sampling time: 0.35 s by 4 slaves
[32m[20230117 13:28:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:31 @agent_ppo2.py:193][0m |          -0.0007 |           8.8215 |           3.3530 |
[32m[20230117 13:28:31 @agent_ppo2.py:193][0m |          -0.0058 |           7.2257 |           3.3520 |
[32m[20230117 13:28:31 @agent_ppo2.py:193][0m |          -0.0073 |           6.9448 |           3.3511 |
[32m[20230117 13:28:31 @agent_ppo2.py:193][0m |          -0.0082 |           6.7466 |           3.3506 |
[32m[20230117 13:28:31 @agent_ppo2.py:193][0m |          -0.0088 |           6.5998 |           3.3526 |
[32m[20230117 13:28:32 @agent_ppo2.py:193][0m |          -0.0096 |           6.4447 |           3.3519 |
[32m[20230117 13:28:32 @agent_ppo2.py:193][0m |          -0.0102 |           6.3345 |           3.3556 |
[32m[20230117 13:28:32 @agent_ppo2.py:193][0m |          -0.0105 |           6.2799 |           3.3568 |
[32m[20230117 13:28:32 @agent_ppo2.py:193][0m |          -0.0111 |           6.1901 |           3.3572 |
[32m[20230117 13:28:32 @agent_ppo2.py:193][0m |          -0.0116 |           6.1702 |           3.3566 |
[32m[20230117 13:28:32 @agent_ppo2.py:138][0m Policy update time: 1.06 s
[32m[20230117 13:28:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 179.47
[32m[20230117 13:28:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.39
[32m[20230117 13:28:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 202.62
[32m[20230117 13:28:32 @agent_ppo2.py:151][0m Total time:       2.12 min
[32m[20230117 13:28:32 @agent_ppo2.py:153][0m 155648 total steps have happened
[32m[20230117 13:28:32 @agent_ppo2.py:129][0m #------------------------ Iteration 76 --------------------------#
[32m[20230117 13:28:33 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:28:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:33 @agent_ppo2.py:193][0m |           0.0009 |          17.4536 |           3.3856 |
[32m[20230117 13:28:33 @agent_ppo2.py:193][0m |          -0.0026 |          10.6892 |           3.3820 |
[32m[20230117 13:28:33 @agent_ppo2.py:193][0m |          -0.0059 |           7.3707 |           3.3819 |
[32m[20230117 13:28:33 @agent_ppo2.py:193][0m |          -0.0067 |           6.9319 |           3.3800 |
[32m[20230117 13:28:33 @agent_ppo2.py:193][0m |          -0.0066 |           6.7856 |           3.3833 |
[32m[20230117 13:28:33 @agent_ppo2.py:193][0m |          -0.0071 |           6.6697 |           3.3811 |
[32m[20230117 13:28:33 @agent_ppo2.py:193][0m |          -0.0073 |           6.5787 |           3.3802 |
[32m[20230117 13:28:33 @agent_ppo2.py:193][0m |          -0.0076 |           6.4805 |           3.3822 |
[32m[20230117 13:28:33 @agent_ppo2.py:193][0m |          -0.0073 |           6.4224 |           3.3824 |
[32m[20230117 13:28:34 @agent_ppo2.py:193][0m |          -0.0084 |           6.3889 |           3.3833 |
[32m[20230117 13:28:34 @agent_ppo2.py:138][0m Policy update time: 1.03 s
[32m[20230117 13:28:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 239.49
[32m[20230117 13:28:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.39
[32m[20230117 13:28:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 253.26
[32m[20230117 13:28:34 @agent_ppo2.py:151][0m Total time:       2.15 min
[32m[20230117 13:28:34 @agent_ppo2.py:153][0m 157696 total steps have happened
[32m[20230117 13:28:34 @agent_ppo2.py:129][0m #------------------------ Iteration 77 --------------------------#
[32m[20230117 13:28:34 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:28:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:34 @agent_ppo2.py:193][0m |          -0.0005 |          12.1471 |           3.4837 |
[32m[20230117 13:28:34 @agent_ppo2.py:193][0m |          -0.0019 |           9.0459 |           3.4787 |
[32m[20230117 13:28:34 @agent_ppo2.py:193][0m |          -0.0065 |           8.3423 |           3.4820 |
[32m[20230117 13:28:35 @agent_ppo2.py:193][0m |          -0.0076 |           8.0891 |           3.4819 |
[32m[20230117 13:28:35 @agent_ppo2.py:193][0m |          -0.0093 |           7.7100 |           3.4861 |
[32m[20230117 13:28:35 @agent_ppo2.py:193][0m |          -0.0100 |           7.5222 |           3.4859 |
[32m[20230117 13:28:35 @agent_ppo2.py:193][0m |          -0.0106 |           7.3874 |           3.4878 |
[32m[20230117 13:28:35 @agent_ppo2.py:193][0m |          -0.0091 |           7.2428 |           3.4858 |
[32m[20230117 13:28:35 @agent_ppo2.py:193][0m |          -0.0117 |           6.9652 |           3.4896 |
[32m[20230117 13:28:35 @agent_ppo2.py:193][0m |          -0.0123 |           6.7130 |           3.4905 |
[32m[20230117 13:28:35 @agent_ppo2.py:138][0m Policy update time: 0.93 s
[32m[20230117 13:28:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 169.00
[32m[20230117 13:28:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 232.41
[32m[20230117 13:28:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 3.41
[32m[20230117 13:28:35 @agent_ppo2.py:151][0m Total time:       2.17 min
[32m[20230117 13:28:35 @agent_ppo2.py:153][0m 159744 total steps have happened
[32m[20230117 13:28:35 @agent_ppo2.py:129][0m #------------------------ Iteration 78 --------------------------#
[32m[20230117 13:28:36 @agent_ppo2.py:135][0m Sampling time: 0.42 s by 4 slaves
[32m[20230117 13:28:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:36 @agent_ppo2.py:193][0m |           0.0016 |          17.5299 |           3.4755 |
[32m[20230117 13:28:36 @agent_ppo2.py:193][0m |          -0.0051 |          13.9553 |           3.4702 |
[32m[20230117 13:28:36 @agent_ppo2.py:193][0m |          -0.0081 |          12.4840 |           3.4650 |
[32m[20230117 13:28:36 @agent_ppo2.py:193][0m |          -0.0071 |          11.0159 |           3.4609 |
[32m[20230117 13:28:36 @agent_ppo2.py:193][0m |          -0.0133 |          10.2931 |           3.4591 |
[32m[20230117 13:28:36 @agent_ppo2.py:193][0m |          -0.0057 |           9.4670 |           3.4564 |
[32m[20230117 13:28:36 @agent_ppo2.py:193][0m |          -0.0085 |           9.0396 |           3.4563 |
[32m[20230117 13:28:36 @agent_ppo2.py:193][0m |          -0.0080 |           8.6116 |           3.4538 |
[32m[20230117 13:28:37 @agent_ppo2.py:193][0m |          -0.0127 |           8.3206 |           3.4529 |
[32m[20230117 13:28:37 @agent_ppo2.py:193][0m |          -0.0109 |           7.9982 |           3.4522 |
[32m[20230117 13:28:37 @agent_ppo2.py:138][0m Policy update time: 0.98 s
[32m[20230117 13:28:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 127.59
[32m[20230117 13:28:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 238.57
[32m[20230117 13:28:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.65
[32m[20230117 13:28:37 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 269.65
[32m[20230117 13:28:37 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 269.65
[32m[20230117 13:28:37 @agent_ppo2.py:151][0m Total time:       2.20 min
[32m[20230117 13:28:37 @agent_ppo2.py:153][0m 161792 total steps have happened
[32m[20230117 13:28:37 @agent_ppo2.py:129][0m #------------------------ Iteration 79 --------------------------#
[32m[20230117 13:28:37 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:28:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:37 @agent_ppo2.py:193][0m |           0.0012 |           6.7068 |           3.4353 |
[32m[20230117 13:28:37 @agent_ppo2.py:193][0m |          -0.0028 |           5.9611 |           3.4337 |
[32m[20230117 13:28:38 @agent_ppo2.py:193][0m |          -0.0032 |           5.7594 |           3.4319 |
[32m[20230117 13:28:38 @agent_ppo2.py:193][0m |          -0.0044 |           5.6200 |           3.4344 |
[32m[20230117 13:28:38 @agent_ppo2.py:193][0m |          -0.0056 |           5.5400 |           3.4336 |
[32m[20230117 13:28:38 @agent_ppo2.py:193][0m |          -0.0059 |           5.4813 |           3.4331 |
[32m[20230117 13:28:38 @agent_ppo2.py:193][0m |          -0.0064 |           5.4103 |           3.4345 |
[32m[20230117 13:28:38 @agent_ppo2.py:193][0m |          -0.0067 |           5.3687 |           3.4328 |
[32m[20230117 13:28:38 @agent_ppo2.py:193][0m |          -0.0077 |           5.3135 |           3.4357 |
[32m[20230117 13:28:38 @agent_ppo2.py:193][0m |          -0.0078 |           5.2744 |           3.4342 |
[32m[20230117 13:28:38 @agent_ppo2.py:138][0m Policy update time: 1.04 s
[32m[20230117 13:28:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 235.40
[32m[20230117 13:28:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.25
[32m[20230117 13:28:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 266.63
[32m[20230117 13:28:38 @agent_ppo2.py:151][0m Total time:       2.22 min
[32m[20230117 13:28:38 @agent_ppo2.py:153][0m 163840 total steps have happened
[32m[20230117 13:28:38 @agent_ppo2.py:129][0m #------------------------ Iteration 80 --------------------------#
[32m[20230117 13:28:39 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:28:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:39 @agent_ppo2.py:193][0m |          -0.0004 |          23.0725 |           3.4613 |
[32m[20230117 13:28:39 @agent_ppo2.py:193][0m |          -0.0032 |          14.3952 |           3.4560 |
[32m[20230117 13:28:39 @agent_ppo2.py:193][0m |          -0.0047 |          11.4853 |           3.4561 |
[32m[20230117 13:28:39 @agent_ppo2.py:193][0m |          -0.0057 |          10.0856 |           3.4561 |
[32m[20230117 13:28:39 @agent_ppo2.py:193][0m |          -0.0065 |           9.3379 |           3.4524 |
[32m[20230117 13:28:39 @agent_ppo2.py:193][0m |          -0.0068 |           8.7447 |           3.4533 |
[32m[20230117 13:28:39 @agent_ppo2.py:193][0m |          -0.0076 |           8.2386 |           3.4503 |
[32m[20230117 13:28:40 @agent_ppo2.py:193][0m |          -0.0075 |           7.8694 |           3.4488 |
[32m[20230117 13:28:40 @agent_ppo2.py:193][0m |          -0.0080 |           7.6189 |           3.4500 |
[32m[20230117 13:28:40 @agent_ppo2.py:193][0m |          -0.0087 |           7.4449 |           3.4481 |
[32m[20230117 13:28:40 @agent_ppo2.py:138][0m Policy update time: 0.99 s
[32m[20230117 13:28:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 134.01
[32m[20230117 13:28:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.14
[32m[20230117 13:28:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 42.75
[32m[20230117 13:28:40 @agent_ppo2.py:151][0m Total time:       2.25 min
[32m[20230117 13:28:40 @agent_ppo2.py:153][0m 165888 total steps have happened
[32m[20230117 13:28:40 @agent_ppo2.py:129][0m #------------------------ Iteration 81 --------------------------#
[32m[20230117 13:28:40 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:28:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:40 @agent_ppo2.py:193][0m |           0.0009 |           5.4352 |           3.4630 |
[32m[20230117 13:28:40 @agent_ppo2.py:193][0m |          -0.0031 |           5.2055 |           3.4626 |
[32m[20230117 13:28:41 @agent_ppo2.py:193][0m |          -0.0035 |           5.1072 |           3.4610 |
[32m[20230117 13:28:41 @agent_ppo2.py:193][0m |          -0.0045 |           5.0452 |           3.4644 |
[32m[20230117 13:28:41 @agent_ppo2.py:193][0m |          -0.0059 |           4.9844 |           3.4639 |
[32m[20230117 13:28:41 @agent_ppo2.py:193][0m |          -0.0063 |           4.9500 |           3.4658 |
[32m[20230117 13:28:41 @agent_ppo2.py:193][0m |          -0.0070 |           4.8970 |           3.4673 |
[32m[20230117 13:28:41 @agent_ppo2.py:193][0m |          -0.0071 |           4.8915 |           3.4694 |
[32m[20230117 13:28:41 @agent_ppo2.py:193][0m |          -0.0081 |           4.8358 |           3.4699 |
[32m[20230117 13:28:41 @agent_ppo2.py:193][0m |          -0.0067 |           4.8601 |           3.4693 |
[32m[20230117 13:28:41 @agent_ppo2.py:138][0m Policy update time: 1.04 s
[32m[20230117 13:28:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 235.91
[32m[20230117 13:28:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.92
[32m[20230117 13:28:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 103.33
[32m[20230117 13:28:41 @agent_ppo2.py:151][0m Total time:       2.27 min
[32m[20230117 13:28:41 @agent_ppo2.py:153][0m 167936 total steps have happened
[32m[20230117 13:28:41 @agent_ppo2.py:129][0m #------------------------ Iteration 82 --------------------------#
[32m[20230117 13:28:42 @agent_ppo2.py:135][0m Sampling time: 0.29 s by 4 slaves
[32m[20230117 13:28:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:42 @agent_ppo2.py:193][0m |          -0.0011 |           6.2308 |           3.4035 |
[32m[20230117 13:28:42 @agent_ppo2.py:193][0m |          -0.0052 |           5.9950 |           3.4009 |
[32m[20230117 13:28:42 @agent_ppo2.py:193][0m |          -0.0071 |           5.8951 |           3.4001 |
[32m[20230117 13:28:42 @agent_ppo2.py:193][0m |          -0.0077 |           5.9138 |           3.4027 |
[32m[20230117 13:28:42 @agent_ppo2.py:193][0m |          -0.0096 |           5.7821 |           3.4023 |
[32m[20230117 13:28:42 @agent_ppo2.py:193][0m |          -0.0103 |           5.7637 |           3.4032 |
[32m[20230117 13:28:42 @agent_ppo2.py:193][0m |          -0.0118 |           5.7127 |           3.4046 |
[32m[20230117 13:28:43 @agent_ppo2.py:193][0m |          -0.0099 |           5.6759 |           3.4041 |
[32m[20230117 13:28:43 @agent_ppo2.py:193][0m |          -0.0088 |           5.7846 |           3.4045 |
[32m[20230117 13:28:43 @agent_ppo2.py:193][0m |          -0.0123 |           5.6044 |           3.4065 |
[32m[20230117 13:28:43 @agent_ppo2.py:138][0m Policy update time: 1.02 s
[32m[20230117 13:28:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 240.11
[32m[20230117 13:28:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.83
[32m[20230117 13:28:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 75.77
[32m[20230117 13:28:43 @agent_ppo2.py:151][0m Total time:       2.30 min
[32m[20230117 13:28:43 @agent_ppo2.py:153][0m 169984 total steps have happened
[32m[20230117 13:28:43 @agent_ppo2.py:129][0m #------------------------ Iteration 83 --------------------------#
[32m[20230117 13:28:43 @agent_ppo2.py:135][0m Sampling time: 0.29 s by 4 slaves
[32m[20230117 13:28:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:43 @agent_ppo2.py:193][0m |           0.0007 |           6.2832 |           3.4427 |
[32m[20230117 13:28:43 @agent_ppo2.py:193][0m |          -0.0056 |           6.0394 |           3.4389 |
[32m[20230117 13:28:44 @agent_ppo2.py:193][0m |          -0.0071 |           5.9494 |           3.4364 |
[32m[20230117 13:28:44 @agent_ppo2.py:193][0m |          -0.0063 |           5.9024 |           3.4358 |
[32m[20230117 13:28:44 @agent_ppo2.py:193][0m |          -0.0069 |           5.8567 |           3.4348 |
[32m[20230117 13:28:44 @agent_ppo2.py:193][0m |          -0.0087 |           5.8078 |           3.4344 |
[32m[20230117 13:28:44 @agent_ppo2.py:193][0m |          -0.0089 |           5.8388 |           3.4331 |
[32m[20230117 13:28:44 @agent_ppo2.py:193][0m |          -0.0054 |           5.8806 |           3.4369 |
[32m[20230117 13:28:44 @agent_ppo2.py:193][0m |          -0.0095 |           5.7362 |           3.4342 |
[32m[20230117 13:28:44 @agent_ppo2.py:193][0m |          -0.0088 |           5.7138 |           3.4370 |
[32m[20230117 13:28:44 @agent_ppo2.py:138][0m Policy update time: 1.02 s
[32m[20230117 13:28:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 240.54
[32m[20230117 13:28:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 244.35
[32m[20230117 13:28:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 92.50
[32m[20230117 13:28:45 @agent_ppo2.py:151][0m Total time:       2.33 min
[32m[20230117 13:28:45 @agent_ppo2.py:153][0m 172032 total steps have happened
[32m[20230117 13:28:45 @agent_ppo2.py:129][0m #------------------------ Iteration 84 --------------------------#
[32m[20230117 13:28:45 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:28:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:45 @agent_ppo2.py:193][0m |          -0.0031 |          10.5589 |           3.5051 |
[32m[20230117 13:28:45 @agent_ppo2.py:193][0m |          -0.0062 |           4.8282 |           3.4970 |
[32m[20230117 13:28:45 @agent_ppo2.py:193][0m |          -0.0100 |           4.6048 |           3.4971 |
[32m[20230117 13:28:45 @agent_ppo2.py:193][0m |          -0.0108 |           4.2870 |           3.4957 |
[32m[20230117 13:28:45 @agent_ppo2.py:193][0m |          -0.0123 |           4.1481 |           3.4941 |
[32m[20230117 13:28:45 @agent_ppo2.py:193][0m |          -0.0142 |           4.0942 |           3.4940 |
[32m[20230117 13:28:45 @agent_ppo2.py:193][0m |          -0.0139 |           3.9518 |           3.4915 |
[32m[20230117 13:28:45 @agent_ppo2.py:193][0m |          -0.0148 |           3.8939 |           3.4907 |
[32m[20230117 13:28:45 @agent_ppo2.py:193][0m |          -0.0148 |           3.8255 |           3.4927 |
[32m[20230117 13:28:46 @agent_ppo2.py:193][0m |          -0.0161 |           3.7215 |           3.4905 |
[32m[20230117 13:28:46 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:28:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 107.80
[32m[20230117 13:28:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.50
[32m[20230117 13:28:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 132.14
[32m[20230117 13:28:46 @agent_ppo2.py:151][0m Total time:       2.35 min
[32m[20230117 13:28:46 @agent_ppo2.py:153][0m 174080 total steps have happened
[32m[20230117 13:28:46 @agent_ppo2.py:129][0m #------------------------ Iteration 85 --------------------------#
[32m[20230117 13:28:46 @agent_ppo2.py:135][0m Sampling time: 0.33 s by 4 slaves
[32m[20230117 13:28:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:46 @agent_ppo2.py:193][0m |          -0.0014 |          10.1436 |           3.4664 |
[32m[20230117 13:28:46 @agent_ppo2.py:193][0m |          -0.0069 |           8.4977 |           3.4603 |
[32m[20230117 13:28:46 @agent_ppo2.py:193][0m |          -0.0062 |           8.2483 |           3.4588 |
[32m[20230117 13:28:47 @agent_ppo2.py:193][0m |          -0.0064 |           8.2112 |           3.4593 |
[32m[20230117 13:28:47 @agent_ppo2.py:193][0m |          -0.0087 |           7.8259 |           3.4602 |
[32m[20230117 13:28:47 @agent_ppo2.py:193][0m |          -0.0087 |           7.6811 |           3.4593 |
[32m[20230117 13:28:47 @agent_ppo2.py:193][0m |          -0.0088 |           7.6812 |           3.4584 |
[32m[20230117 13:28:47 @agent_ppo2.py:193][0m |          -0.0108 |           7.4658 |           3.4575 |
[32m[20230117 13:28:47 @agent_ppo2.py:193][0m |          -0.0087 |           7.4181 |           3.4571 |
[32m[20230117 13:28:47 @agent_ppo2.py:193][0m |          -0.0095 |           7.3042 |           3.4563 |
[32m[20230117 13:28:47 @agent_ppo2.py:138][0m Policy update time: 1.05 s
[32m[20230117 13:28:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 173.97
[32m[20230117 13:28:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.86
[32m[20230117 13:28:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 266.10
[32m[20230117 13:28:47 @agent_ppo2.py:151][0m Total time:       2.37 min
[32m[20230117 13:28:47 @agent_ppo2.py:153][0m 176128 total steps have happened
[32m[20230117 13:28:47 @agent_ppo2.py:129][0m #------------------------ Iteration 86 --------------------------#
[32m[20230117 13:28:48 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:28:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:48 @agent_ppo2.py:193][0m |          -0.0011 |           7.0195 |           3.5415 |
[32m[20230117 13:28:48 @agent_ppo2.py:193][0m |           0.0023 |           6.9724 |           3.5316 |
[32m[20230117 13:28:48 @agent_ppo2.py:193][0m |          -0.0076 |           6.3758 |           3.5292 |
[32m[20230117 13:28:48 @agent_ppo2.py:193][0m |          -0.0077 |           6.1980 |           3.5293 |
[32m[20230117 13:28:48 @agent_ppo2.py:193][0m |          -0.0088 |           6.0668 |           3.5266 |
[32m[20230117 13:28:48 @agent_ppo2.py:193][0m |          -0.0094 |           5.9694 |           3.5217 |
[32m[20230117 13:28:48 @agent_ppo2.py:193][0m |          -0.0095 |           5.8556 |           3.5226 |
[32m[20230117 13:28:48 @agent_ppo2.py:193][0m |          -0.0098 |           5.7948 |           3.5204 |
[32m[20230117 13:28:49 @agent_ppo2.py:193][0m |          -0.0075 |           5.7617 |           3.5191 |
[32m[20230117 13:28:49 @agent_ppo2.py:193][0m |          -0.0088 |           5.6257 |           3.5190 |
[32m[20230117 13:28:49 @agent_ppo2.py:138][0m Policy update time: 0.96 s
[32m[20230117 13:28:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 248.86
[32m[20230117 13:28:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.44
[32m[20230117 13:28:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.01
[32m[20230117 13:28:49 @agent_ppo2.py:151][0m Total time:       2.40 min
[32m[20230117 13:28:49 @agent_ppo2.py:153][0m 178176 total steps have happened
[32m[20230117 13:28:49 @agent_ppo2.py:129][0m #------------------------ Iteration 87 --------------------------#
[32m[20230117 13:28:49 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:28:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:49 @agent_ppo2.py:193][0m |          -0.0004 |          10.3620 |           3.4725 |
[32m[20230117 13:28:49 @agent_ppo2.py:193][0m |          -0.0052 |           8.3520 |           3.4675 |
[32m[20230117 13:28:49 @agent_ppo2.py:193][0m |          -0.0075 |           8.0059 |           3.4643 |
[32m[20230117 13:28:49 @agent_ppo2.py:193][0m |          -0.0083 |           7.8241 |           3.4641 |
[32m[20230117 13:28:50 @agent_ppo2.py:193][0m |          -0.0088 |           7.6065 |           3.4642 |
[32m[20230117 13:28:50 @agent_ppo2.py:193][0m |          -0.0099 |           7.4238 |           3.4646 |
[32m[20230117 13:28:50 @agent_ppo2.py:193][0m |          -0.0100 |           7.3040 |           3.4653 |
[32m[20230117 13:28:50 @agent_ppo2.py:193][0m |          -0.0107 |           7.1904 |           3.4635 |
[32m[20230117 13:28:50 @agent_ppo2.py:193][0m |          -0.0111 |           7.0921 |           3.4651 |
[32m[20230117 13:28:50 @agent_ppo2.py:193][0m |          -0.0116 |           6.9874 |           3.4665 |
[32m[20230117 13:28:50 @agent_ppo2.py:138][0m Policy update time: 0.88 s
[32m[20230117 13:28:50 @agent_ppo2.py:146][0m Average TRAINING episode reward: 190.51
[32m[20230117 13:28:50 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.53
[32m[20230117 13:28:50 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -5.39
[32m[20230117 13:28:50 @agent_ppo2.py:151][0m Total time:       2.42 min
[32m[20230117 13:28:50 @agent_ppo2.py:153][0m 180224 total steps have happened
[32m[20230117 13:28:50 @agent_ppo2.py:129][0m #------------------------ Iteration 88 --------------------------#
[32m[20230117 13:28:50 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:28:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:51 @agent_ppo2.py:193][0m |           0.0002 |           7.4766 |           3.4721 |
[32m[20230117 13:28:51 @agent_ppo2.py:193][0m |          -0.0013 |           6.7598 |           3.4669 |
[32m[20230117 13:28:51 @agent_ppo2.py:193][0m |          -0.0115 |           6.4669 |           3.4631 |
[32m[20230117 13:28:51 @agent_ppo2.py:193][0m |          -0.0082 |           6.2512 |           3.4610 |
[32m[20230117 13:28:51 @agent_ppo2.py:193][0m |          -0.0081 |           6.1181 |           3.4629 |
[32m[20230117 13:28:51 @agent_ppo2.py:193][0m |          -0.0082 |           6.0169 |           3.4590 |
[32m[20230117 13:28:51 @agent_ppo2.py:193][0m |          -0.0133 |           5.9417 |           3.4589 |
[32m[20230117 13:28:51 @agent_ppo2.py:193][0m |          -0.0098 |           5.8319 |           3.4590 |
[32m[20230117 13:28:51 @agent_ppo2.py:193][0m |          -0.0086 |           5.7545 |           3.4563 |
[32m[20230117 13:28:51 @agent_ppo2.py:193][0m |          -0.0088 |           5.6828 |           3.4563 |
[32m[20230117 13:28:51 @agent_ppo2.py:138][0m Policy update time: 0.93 s
[32m[20230117 13:28:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.90
[32m[20230117 13:28:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.25
[32m[20230117 13:28:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 47.27
[32m[20230117 13:28:52 @agent_ppo2.py:151][0m Total time:       2.44 min
[32m[20230117 13:28:52 @agent_ppo2.py:153][0m 182272 total steps have happened
[32m[20230117 13:28:52 @agent_ppo2.py:129][0m #------------------------ Iteration 89 --------------------------#
[32m[20230117 13:28:52 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:28:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:52 @agent_ppo2.py:193][0m |          -0.0008 |          10.7185 |           3.5471 |
[32m[20230117 13:28:52 @agent_ppo2.py:193][0m |          -0.0027 |           5.9237 |           3.5434 |
[32m[20230117 13:28:52 @agent_ppo2.py:193][0m |          -0.0039 |           5.4883 |           3.5391 |
[32m[20230117 13:28:52 @agent_ppo2.py:193][0m |          -0.0046 |           5.2120 |           3.5376 |
[32m[20230117 13:28:52 @agent_ppo2.py:193][0m |          -0.0053 |           5.0310 |           3.5363 |
[32m[20230117 13:28:53 @agent_ppo2.py:193][0m |          -0.0047 |           4.9031 |           3.5338 |
[32m[20230117 13:28:53 @agent_ppo2.py:193][0m |          -0.0053 |           4.7429 |           3.5331 |
[32m[20230117 13:28:53 @agent_ppo2.py:193][0m |          -0.0062 |           4.6180 |           3.5304 |
[32m[20230117 13:28:53 @agent_ppo2.py:193][0m |          -0.0063 |           4.5512 |           3.5258 |
[32m[20230117 13:28:53 @agent_ppo2.py:193][0m |          -0.0076 |           4.3655 |           3.5258 |
[32m[20230117 13:28:53 @agent_ppo2.py:138][0m Policy update time: 0.97 s
[32m[20230117 13:28:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 214.96
[32m[20230117 13:28:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 248.22
[32m[20230117 13:28:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 7.19
[32m[20230117 13:28:53 @agent_ppo2.py:151][0m Total time:       2.47 min
[32m[20230117 13:28:53 @agent_ppo2.py:153][0m 184320 total steps have happened
[32m[20230117 13:28:53 @agent_ppo2.py:129][0m #------------------------ Iteration 90 --------------------------#
[32m[20230117 13:28:53 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:28:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:54 @agent_ppo2.py:193][0m |           0.0019 |          12.4172 |           3.4809 |
[32m[20230117 13:28:54 @agent_ppo2.py:193][0m |          -0.0019 |           8.2877 |           3.4753 |
[32m[20230117 13:28:54 @agent_ppo2.py:193][0m |          -0.0041 |           7.7709 |           3.4729 |
[32m[20230117 13:28:54 @agent_ppo2.py:193][0m |          -0.0049 |           7.5016 |           3.4717 |
[32m[20230117 13:28:54 @agent_ppo2.py:193][0m |          -0.0051 |           7.2403 |           3.4732 |
[32m[20230117 13:28:54 @agent_ppo2.py:193][0m |          -0.0051 |           7.1322 |           3.4728 |
[32m[20230117 13:28:54 @agent_ppo2.py:193][0m |          -0.0065 |           7.0075 |           3.4701 |
[32m[20230117 13:28:54 @agent_ppo2.py:193][0m |          -0.0065 |           6.8539 |           3.4730 |
[32m[20230117 13:28:54 @agent_ppo2.py:193][0m |          -0.0066 |           6.7656 |           3.4751 |
[32m[20230117 13:28:54 @agent_ppo2.py:193][0m |          -0.0075 |           6.6444 |           3.4732 |
[32m[20230117 13:28:54 @agent_ppo2.py:138][0m Policy update time: 0.89 s
[32m[20230117 13:28:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 197.54
[32m[20230117 13:28:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.57
[32m[20230117 13:28:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -8.14
[32m[20230117 13:28:55 @agent_ppo2.py:151][0m Total time:       2.49 min
[32m[20230117 13:28:55 @agent_ppo2.py:153][0m 186368 total steps have happened
[32m[20230117 13:28:55 @agent_ppo2.py:129][0m #------------------------ Iteration 91 --------------------------#
[32m[20230117 13:28:55 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:28:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:55 @agent_ppo2.py:193][0m |           0.0011 |          13.0925 |           3.3503 |
[32m[20230117 13:28:55 @agent_ppo2.py:193][0m |          -0.0041 |           9.3197 |           3.3436 |
[32m[20230117 13:28:55 @agent_ppo2.py:193][0m |          -0.0064 |           8.4435 |           3.3403 |
[32m[20230117 13:28:55 @agent_ppo2.py:193][0m |          -0.0069 |           8.1568 |           3.3417 |
[32m[20230117 13:28:55 @agent_ppo2.py:193][0m |          -0.0071 |           7.8348 |           3.3394 |
[32m[20230117 13:28:55 @agent_ppo2.py:193][0m |          -0.0115 |           7.6459 |           3.3403 |
[32m[20230117 13:28:56 @agent_ppo2.py:193][0m |          -0.0093 |           7.4264 |           3.3411 |
[32m[20230117 13:28:56 @agent_ppo2.py:193][0m |          -0.0059 |           7.4424 |           3.3391 |
[32m[20230117 13:28:56 @agent_ppo2.py:193][0m |          -0.0106 |           7.1122 |           3.3395 |
[32m[20230117 13:28:56 @agent_ppo2.py:193][0m |          -0.0060 |           7.0621 |           3.3394 |
[32m[20230117 13:28:56 @agent_ppo2.py:138][0m Policy update time: 0.98 s
[32m[20230117 13:28:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 172.23
[32m[20230117 13:28:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 246.99
[32m[20230117 13:28:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -82.24
[32m[20230117 13:28:56 @agent_ppo2.py:151][0m Total time:       2.52 min
[32m[20230117 13:28:56 @agent_ppo2.py:153][0m 188416 total steps have happened
[32m[20230117 13:28:56 @agent_ppo2.py:129][0m #------------------------ Iteration 92 --------------------------#
[32m[20230117 13:28:56 @agent_ppo2.py:135][0m Sampling time: 0.33 s by 4 slaves
[32m[20230117 13:28:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:56 @agent_ppo2.py:193][0m |          -0.0010 |          11.0108 |           3.4468 |
[32m[20230117 13:28:57 @agent_ppo2.py:193][0m |          -0.0033 |           8.9463 |           3.4461 |
[32m[20230117 13:28:57 @agent_ppo2.py:193][0m |          -0.0050 |           8.6449 |           3.4465 |
[32m[20230117 13:28:57 @agent_ppo2.py:193][0m |          -0.0069 |           8.4128 |           3.4425 |
[32m[20230117 13:28:57 @agent_ppo2.py:193][0m |          -0.0076 |           8.2464 |           3.4442 |
[32m[20230117 13:28:57 @agent_ppo2.py:193][0m |          -0.0085 |           8.1017 |           3.4421 |
[32m[20230117 13:28:57 @agent_ppo2.py:193][0m |          -0.0085 |           7.9678 |           3.4409 |
[32m[20230117 13:28:57 @agent_ppo2.py:193][0m |          -0.0096 |           7.8869 |           3.4386 |
[32m[20230117 13:28:57 @agent_ppo2.py:193][0m |          -0.0101 |           7.7951 |           3.4393 |
[32m[20230117 13:28:57 @agent_ppo2.py:193][0m |          -0.0093 |           7.6954 |           3.4425 |
[32m[20230117 13:28:57 @agent_ppo2.py:138][0m Policy update time: 1.00 s
[32m[20230117 13:28:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 191.75
[32m[20230117 13:28:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.15
[32m[20230117 13:28:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 106.48
[32m[20230117 13:28:58 @agent_ppo2.py:151][0m Total time:       2.54 min
[32m[20230117 13:28:58 @agent_ppo2.py:153][0m 190464 total steps have happened
[32m[20230117 13:28:58 @agent_ppo2.py:129][0m #------------------------ Iteration 93 --------------------------#
[32m[20230117 13:28:58 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:28:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:58 @agent_ppo2.py:193][0m |           0.0023 |          18.2745 |           3.4594 |
[32m[20230117 13:28:58 @agent_ppo2.py:193][0m |          -0.0004 |          13.9846 |           3.4544 |
[32m[20230117 13:28:58 @agent_ppo2.py:193][0m |          -0.0046 |          12.6196 |           3.4512 |
[32m[20230117 13:28:58 @agent_ppo2.py:193][0m |          -0.0089 |          12.2162 |           3.4496 |
[32m[20230117 13:28:58 @agent_ppo2.py:193][0m |          -0.0078 |          11.6035 |           3.4444 |
[32m[20230117 13:28:58 @agent_ppo2.py:193][0m |          -0.0085 |          11.4344 |           3.4462 |
[32m[20230117 13:28:59 @agent_ppo2.py:193][0m |          -0.0079 |          11.4085 |           3.4447 |
[32m[20230117 13:28:59 @agent_ppo2.py:193][0m |          -0.0105 |          10.9353 |           3.4431 |
[32m[20230117 13:28:59 @agent_ppo2.py:193][0m |          -0.0097 |          10.7945 |           3.4439 |
[32m[20230117 13:28:59 @agent_ppo2.py:193][0m |          -0.0112 |          10.6679 |           3.4404 |
[32m[20230117 13:28:59 @agent_ppo2.py:138][0m Policy update time: 0.96 s
[32m[20230117 13:28:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 128.14
[32m[20230117 13:28:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.13
[32m[20230117 13:28:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.59
[32m[20230117 13:28:59 @agent_ppo2.py:151][0m Total time:       2.57 min
[32m[20230117 13:28:59 @agent_ppo2.py:153][0m 192512 total steps have happened
[32m[20230117 13:28:59 @agent_ppo2.py:129][0m #------------------------ Iteration 94 --------------------------#
[32m[20230117 13:28:59 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:28:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:28:59 @agent_ppo2.py:193][0m |           0.0003 |           9.7975 |           3.4542 |
[32m[20230117 13:29:00 @agent_ppo2.py:193][0m |          -0.0031 |           8.8057 |           3.4466 |
[32m[20230117 13:29:00 @agent_ppo2.py:193][0m |          -0.0061 |           8.5428 |           3.4410 |
[32m[20230117 13:29:00 @agent_ppo2.py:193][0m |          -0.0066 |           8.3997 |           3.4468 |
[32m[20230117 13:29:00 @agent_ppo2.py:193][0m |          -0.0054 |           8.2531 |           3.4428 |
[32m[20230117 13:29:00 @agent_ppo2.py:193][0m |          -0.0089 |           8.1057 |           3.4427 |
[32m[20230117 13:29:00 @agent_ppo2.py:193][0m |          -0.0094 |           8.0299 |           3.4426 |
[32m[20230117 13:29:00 @agent_ppo2.py:193][0m |          -0.0086 |           7.9313 |           3.4451 |
[32m[20230117 13:29:00 @agent_ppo2.py:193][0m |          -0.0075 |           7.9373 |           3.4438 |
[32m[20230117 13:29:00 @agent_ppo2.py:193][0m |          -0.0066 |           7.8363 |           3.4417 |
[32m[20230117 13:29:00 @agent_ppo2.py:138][0m Policy update time: 0.95 s
[32m[20230117 13:29:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 245.68
[32m[20230117 13:29:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.36
[32m[20230117 13:29:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.36
[32m[20230117 13:29:00 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 272.36
[32m[20230117 13:29:00 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 272.36
[32m[20230117 13:29:00 @agent_ppo2.py:151][0m Total time:       2.59 min
[32m[20230117 13:29:00 @agent_ppo2.py:153][0m 194560 total steps have happened
[32m[20230117 13:29:00 @agent_ppo2.py:129][0m #------------------------ Iteration 95 --------------------------#
[32m[20230117 13:29:01 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:01 @agent_ppo2.py:193][0m |          -0.0004 |           9.0185 |           3.4459 |
[32m[20230117 13:29:01 @agent_ppo2.py:193][0m |          -0.0049 |           8.4406 |           3.4456 |
[32m[20230117 13:29:01 @agent_ppo2.py:193][0m |          -0.0065 |           8.0381 |           3.4445 |
[32m[20230117 13:29:01 @agent_ppo2.py:193][0m |          -0.0058 |           7.6983 |           3.4420 |
[32m[20230117 13:29:01 @agent_ppo2.py:193][0m |          -0.0079 |           7.0418 |           3.4449 |
[32m[20230117 13:29:01 @agent_ppo2.py:193][0m |          -0.0083 |           6.6850 |           3.4449 |
[32m[20230117 13:29:01 @agent_ppo2.py:193][0m |          -0.0096 |           6.4484 |           3.4446 |
[32m[20230117 13:29:01 @agent_ppo2.py:193][0m |          -0.0096 |           6.3496 |           3.4474 |
[32m[20230117 13:29:02 @agent_ppo2.py:193][0m |          -0.0096 |           6.2496 |           3.4483 |
[32m[20230117 13:29:02 @agent_ppo2.py:193][0m |          -0.0107 |           6.0897 |           3.4471 |
[32m[20230117 13:29:02 @agent_ppo2.py:138][0m Policy update time: 0.95 s
[32m[20230117 13:29:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.51
[32m[20230117 13:29:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.70
[32m[20230117 13:29:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 264.67
[32m[20230117 13:29:02 @agent_ppo2.py:151][0m Total time:       2.61 min
[32m[20230117 13:29:02 @agent_ppo2.py:153][0m 196608 total steps have happened
[32m[20230117 13:29:02 @agent_ppo2.py:129][0m #------------------------ Iteration 96 --------------------------#
[32m[20230117 13:29:02 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:29:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:02 @agent_ppo2.py:193][0m |           0.0025 |          15.7047 |           3.4394 |
[32m[20230117 13:29:02 @agent_ppo2.py:193][0m |          -0.0032 |          11.8989 |           3.4331 |
[32m[20230117 13:29:02 @agent_ppo2.py:193][0m |          -0.0063 |          11.4957 |           3.4344 |
[32m[20230117 13:29:02 @agent_ppo2.py:193][0m |          -0.0063 |          11.3331 |           3.4300 |
[32m[20230117 13:29:03 @agent_ppo2.py:193][0m |          -0.0088 |          10.9471 |           3.4335 |
[32m[20230117 13:29:03 @agent_ppo2.py:193][0m |          -0.0073 |          10.7272 |           3.4287 |
[32m[20230117 13:29:03 @agent_ppo2.py:193][0m |          -0.0076 |          10.6551 |           3.4299 |
[32m[20230117 13:29:03 @agent_ppo2.py:193][0m |          -0.0100 |          10.4728 |           3.4345 |
[32m[20230117 13:29:03 @agent_ppo2.py:193][0m |          -0.0066 |          10.3254 |           3.4321 |
[32m[20230117 13:29:03 @agent_ppo2.py:193][0m |          -0.0065 |          10.1897 |           3.4325 |
[32m[20230117 13:29:03 @agent_ppo2.py:138][0m Policy update time: 0.69 s
[32m[20230117 13:29:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 126.31
[32m[20230117 13:29:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.04
[32m[20230117 13:29:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 131.13
[32m[20230117 13:29:03 @agent_ppo2.py:151][0m Total time:       2.63 min
[32m[20230117 13:29:03 @agent_ppo2.py:153][0m 198656 total steps have happened
[32m[20230117 13:29:03 @agent_ppo2.py:129][0m #------------------------ Iteration 97 --------------------------#
[32m[20230117 13:29:03 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:03 @agent_ppo2.py:193][0m |           0.0020 |          17.7437 |           3.4038 |
[32m[20230117 13:29:04 @agent_ppo2.py:193][0m |          -0.0014 |          13.9728 |           3.4022 |
[32m[20230117 13:29:04 @agent_ppo2.py:193][0m |          -0.0088 |          13.0180 |           3.3984 |
[32m[20230117 13:29:04 @agent_ppo2.py:193][0m |          -0.0013 |          12.7625 |           3.3988 |
[32m[20230117 13:29:04 @agent_ppo2.py:193][0m |          -0.0040 |          11.7977 |           3.3982 |
[32m[20230117 13:29:04 @agent_ppo2.py:193][0m |          -0.0070 |          11.8761 |           3.3971 |
[32m[20230117 13:29:04 @agent_ppo2.py:193][0m |          -0.0047 |          11.2889 |           3.3957 |
[32m[20230117 13:29:04 @agent_ppo2.py:193][0m |          -0.0106 |          10.7511 |           3.3932 |
[32m[20230117 13:29:04 @agent_ppo2.py:193][0m |          -0.0071 |          10.4996 |           3.3934 |
[32m[20230117 13:29:04 @agent_ppo2.py:193][0m |          -0.0098 |          10.1440 |           3.3899 |
[32m[20230117 13:29:04 @agent_ppo2.py:138][0m Policy update time: 0.83 s
[32m[20230117 13:29:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 189.04
[32m[20230117 13:29:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.15
[32m[20230117 13:29:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.48
[32m[20230117 13:29:04 @agent_ppo2.py:151][0m Total time:       2.66 min
[32m[20230117 13:29:04 @agent_ppo2.py:153][0m 200704 total steps have happened
[32m[20230117 13:29:04 @agent_ppo2.py:129][0m #------------------------ Iteration 98 --------------------------#
[32m[20230117 13:29:05 @agent_ppo2.py:135][0m Sampling time: 0.34 s by 4 slaves
[32m[20230117 13:29:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:05 @agent_ppo2.py:193][0m |          -0.0007 |          24.5247 |           3.4051 |
[32m[20230117 13:29:05 @agent_ppo2.py:193][0m |          -0.0030 |          19.3844 |           3.4018 |
[32m[20230117 13:29:05 @agent_ppo2.py:193][0m |          -0.0037 |          18.2538 |           3.4060 |
[32m[20230117 13:29:05 @agent_ppo2.py:193][0m |          -0.0058 |          17.5008 |           3.4035 |
[32m[20230117 13:29:05 @agent_ppo2.py:193][0m |          -0.0069 |          16.8426 |           3.4030 |
[32m[20230117 13:29:05 @agent_ppo2.py:193][0m |          -0.0095 |          16.2718 |           3.4015 |
[32m[20230117 13:29:05 @agent_ppo2.py:193][0m |          -0.0052 |          16.4888 |           3.4023 |
[32m[20230117 13:29:06 @agent_ppo2.py:193][0m |          -0.0086 |          15.4951 |           3.4017 |
[32m[20230117 13:29:06 @agent_ppo2.py:193][0m |          -0.0098 |          15.0828 |           3.4015 |
[32m[20230117 13:29:06 @agent_ppo2.py:193][0m |          -0.0095 |          14.7814 |           3.4029 |
[32m[20230117 13:29:06 @agent_ppo2.py:138][0m Policy update time: 1.03 s
[32m[20230117 13:29:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 186.74
[32m[20230117 13:29:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.44
[32m[20230117 13:29:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 14.19
[32m[20230117 13:29:06 @agent_ppo2.py:151][0m Total time:       2.68 min
[32m[20230117 13:29:06 @agent_ppo2.py:153][0m 202752 total steps have happened
[32m[20230117 13:29:06 @agent_ppo2.py:129][0m #------------------------ Iteration 99 --------------------------#
[32m[20230117 13:29:06 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:29:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:06 @agent_ppo2.py:193][0m |           0.0011 |          21.7421 |           3.4636 |
[32m[20230117 13:29:07 @agent_ppo2.py:193][0m |          -0.0027 |          18.2798 |           3.4580 |
[32m[20230117 13:29:07 @agent_ppo2.py:193][0m |          -0.0055 |          17.0331 |           3.4552 |
[32m[20230117 13:29:07 @agent_ppo2.py:193][0m |          -0.0072 |          16.0952 |           3.4562 |
[32m[20230117 13:29:07 @agent_ppo2.py:193][0m |          -0.0083 |          15.4487 |           3.4539 |
[32m[20230117 13:29:07 @agent_ppo2.py:193][0m |          -0.0087 |          14.9855 |           3.4560 |
[32m[20230117 13:29:07 @agent_ppo2.py:193][0m |          -0.0080 |          14.5946 |           3.4545 |
[32m[20230117 13:29:07 @agent_ppo2.py:193][0m |          -0.0090 |          14.5312 |           3.4530 |
[32m[20230117 13:29:07 @agent_ppo2.py:193][0m |          -0.0099 |          13.6354 |           3.4522 |
[32m[20230117 13:29:07 @agent_ppo2.py:193][0m |          -0.0099 |          13.6974 |           3.4525 |
[32m[20230117 13:29:07 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:29:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 184.35
[32m[20230117 13:29:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 247.49
[32m[20230117 13:29:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 117.21
[32m[20230117 13:29:07 @agent_ppo2.py:151][0m Total time:       2.71 min
[32m[20230117 13:29:07 @agent_ppo2.py:153][0m 204800 total steps have happened
[32m[20230117 13:29:07 @agent_ppo2.py:129][0m #------------------------ Iteration 100 --------------------------#
[32m[20230117 13:29:08 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:08 @agent_ppo2.py:193][0m |           0.0009 |          16.8072 |           3.4570 |
[32m[20230117 13:29:08 @agent_ppo2.py:193][0m |          -0.0019 |          14.6639 |           3.4556 |
[32m[20230117 13:29:08 @agent_ppo2.py:193][0m |          -0.0037 |          13.7581 |           3.4531 |
[32m[20230117 13:29:08 @agent_ppo2.py:193][0m |          -0.0047 |          13.2161 |           3.4513 |
[32m[20230117 13:29:08 @agent_ppo2.py:193][0m |          -0.0061 |          12.7832 |           3.4499 |
[32m[20230117 13:29:08 @agent_ppo2.py:193][0m |          -0.0062 |          12.5291 |           3.4480 |
[32m[20230117 13:29:08 @agent_ppo2.py:193][0m |          -0.0074 |          12.2544 |           3.4467 |
[32m[20230117 13:29:08 @agent_ppo2.py:193][0m |          -0.0082 |          11.9723 |           3.4489 |
[32m[20230117 13:29:08 @agent_ppo2.py:193][0m |          -0.0086 |          11.7697 |           3.4493 |
[32m[20230117 13:29:09 @agent_ppo2.py:193][0m |          -0.0078 |          11.5368 |           3.4480 |
[32m[20230117 13:29:09 @agent_ppo2.py:138][0m Policy update time: 0.92 s
[32m[20230117 13:29:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 213.52
[32m[20230117 13:29:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 252.44
[32m[20230117 13:29:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.35
[32m[20230117 13:29:09 @agent_ppo2.py:151][0m Total time:       2.73 min
[32m[20230117 13:29:09 @agent_ppo2.py:153][0m 206848 total steps have happened
[32m[20230117 13:29:09 @agent_ppo2.py:129][0m #------------------------ Iteration 101 --------------------------#
[32m[20230117 13:29:09 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:09 @agent_ppo2.py:193][0m |           0.0003 |           9.3044 |           3.4494 |
[32m[20230117 13:29:09 @agent_ppo2.py:193][0m |          -0.0031 |           8.3630 |           3.4473 |
[32m[20230117 13:29:09 @agent_ppo2.py:193][0m |          -0.0036 |           8.1082 |           3.4461 |
[32m[20230117 13:29:09 @agent_ppo2.py:193][0m |          -0.0049 |           7.9506 |           3.4435 |
[32m[20230117 13:29:10 @agent_ppo2.py:193][0m |          -0.0080 |           7.8040 |           3.4417 |
[32m[20230117 13:29:10 @agent_ppo2.py:193][0m |          -0.0065 |           7.7382 |           3.4408 |
[32m[20230117 13:29:10 @agent_ppo2.py:193][0m |          -0.0067 |           7.6865 |           3.4413 |
[32m[20230117 13:29:10 @agent_ppo2.py:193][0m |          -0.0076 |           7.5839 |           3.4386 |
[32m[20230117 13:29:10 @agent_ppo2.py:193][0m |          -0.0069 |           7.5491 |           3.4391 |
[32m[20230117 13:29:10 @agent_ppo2.py:193][0m |          -0.0085 |           7.4534 |           3.4377 |
[32m[20230117 13:29:10 @agent_ppo2.py:138][0m Policy update time: 0.93 s
[32m[20230117 13:29:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.25
[32m[20230117 13:29:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.71
[32m[20230117 13:29:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.48
[32m[20230117 13:29:10 @agent_ppo2.py:151][0m Total time:       2.75 min
[32m[20230117 13:29:10 @agent_ppo2.py:153][0m 208896 total steps have happened
[32m[20230117 13:29:10 @agent_ppo2.py:129][0m #------------------------ Iteration 102 --------------------------#
[32m[20230117 13:29:11 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:29:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:11 @agent_ppo2.py:193][0m |           0.0003 |          11.4433 |           3.4222 |
[32m[20230117 13:29:11 @agent_ppo2.py:193][0m |          -0.0037 |          10.4321 |           3.4207 |
[32m[20230117 13:29:11 @agent_ppo2.py:193][0m |          -0.0043 |          10.1407 |           3.4193 |
[32m[20230117 13:29:11 @agent_ppo2.py:193][0m |          -0.0070 |           9.7613 |           3.4195 |
[32m[20230117 13:29:11 @agent_ppo2.py:193][0m |          -0.0056 |           9.6417 |           3.4168 |
[32m[20230117 13:29:11 @agent_ppo2.py:193][0m |          -0.0086 |           9.4425 |           3.4173 |
[32m[20230117 13:29:11 @agent_ppo2.py:193][0m |          -0.0082 |           9.3530 |           3.4180 |
[32m[20230117 13:29:11 @agent_ppo2.py:193][0m |          -0.0108 |           9.2012 |           3.4180 |
[32m[20230117 13:29:11 @agent_ppo2.py:193][0m |          -0.0102 |           9.1910 |           3.4145 |
[32m[20230117 13:29:11 @agent_ppo2.py:193][0m |          -0.0097 |           9.0496 |           3.4188 |
[32m[20230117 13:29:11 @agent_ppo2.py:138][0m Policy update time: 0.99 s
[32m[20230117 13:29:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 243.94
[32m[20230117 13:29:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.86
[32m[20230117 13:29:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.71
[32m[20230117 13:29:12 @agent_ppo2.py:151][0m Total time:       2.78 min
[32m[20230117 13:29:12 @agent_ppo2.py:153][0m 210944 total steps have happened
[32m[20230117 13:29:12 @agent_ppo2.py:129][0m #------------------------ Iteration 103 --------------------------#
[32m[20230117 13:29:12 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:29:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:12 @agent_ppo2.py:193][0m |          -0.0021 |          17.5967 |           3.4231 |
[32m[20230117 13:29:12 @agent_ppo2.py:193][0m |          -0.0088 |          13.0535 |           3.4206 |
[32m[20230117 13:29:12 @agent_ppo2.py:193][0m |          -0.0076 |          12.5925 |           3.4187 |
[32m[20230117 13:29:12 @agent_ppo2.py:193][0m |          -0.0112 |          11.8328 |           3.4160 |
[32m[20230117 13:29:12 @agent_ppo2.py:193][0m |          -0.0123 |          11.6054 |           3.4146 |
[32m[20230117 13:29:13 @agent_ppo2.py:193][0m |          -0.0005 |          11.4708 |           3.4130 |
[32m[20230117 13:29:13 @agent_ppo2.py:193][0m |          -0.0104 |          11.1518 |           3.4133 |
[32m[20230117 13:29:13 @agent_ppo2.py:193][0m |          -0.0144 |          10.9842 |           3.4120 |
[32m[20230117 13:29:13 @agent_ppo2.py:193][0m |          -0.0159 |          10.8796 |           3.4127 |
[32m[20230117 13:29:13 @agent_ppo2.py:193][0m |          -0.0139 |          10.7831 |           3.4126 |
[32m[20230117 13:29:13 @agent_ppo2.py:138][0m Policy update time: 0.91 s
[32m[20230117 13:29:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 118.85
[32m[20230117 13:29:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.20
[32m[20230117 13:29:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.79
[32m[20230117 13:29:13 @agent_ppo2.py:151][0m Total time:       2.80 min
[32m[20230117 13:29:13 @agent_ppo2.py:153][0m 212992 total steps have happened
[32m[20230117 13:29:13 @agent_ppo2.py:129][0m #------------------------ Iteration 104 --------------------------#
[32m[20230117 13:29:13 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:29:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:14 @agent_ppo2.py:193][0m |          -0.0023 |          40.6433 |           3.4503 |
[32m[20230117 13:29:14 @agent_ppo2.py:193][0m |          -0.0043 |          25.0834 |           3.4439 |
[32m[20230117 13:29:14 @agent_ppo2.py:193][0m |          -0.0064 |          20.9081 |           3.4404 |
[32m[20230117 13:29:14 @agent_ppo2.py:193][0m |          -0.0032 |          19.2628 |           3.4387 |
[32m[20230117 13:29:14 @agent_ppo2.py:193][0m |          -0.0105 |          17.9989 |           3.4363 |
[32m[20230117 13:29:14 @agent_ppo2.py:193][0m |          -0.0092 |          17.1734 |           3.4367 |
[32m[20230117 13:29:14 @agent_ppo2.py:193][0m |          -0.0113 |          16.6847 |           3.4363 |
[32m[20230117 13:29:14 @agent_ppo2.py:193][0m |          -0.0082 |          16.3132 |           3.4368 |
[32m[20230117 13:29:14 @agent_ppo2.py:193][0m |          -0.0106 |          15.7977 |           3.4343 |
[32m[20230117 13:29:14 @agent_ppo2.py:193][0m |          -0.0123 |          14.9978 |           3.4309 |
[32m[20230117 13:29:14 @agent_ppo2.py:138][0m Policy update time: 0.85 s
[32m[20230117 13:29:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 131.36
[32m[20230117 13:29:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.75
[32m[20230117 13:29:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 114.96
[32m[20230117 13:29:15 @agent_ppo2.py:151][0m Total time:       2.83 min
[32m[20230117 13:29:15 @agent_ppo2.py:153][0m 215040 total steps have happened
[32m[20230117 13:29:15 @agent_ppo2.py:129][0m #------------------------ Iteration 105 --------------------------#
[32m[20230117 13:29:15 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:29:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:15 @agent_ppo2.py:193][0m |          -0.0015 |           9.5227 |           3.4521 |
[32m[20230117 13:29:15 @agent_ppo2.py:193][0m |          -0.0061 |           8.1710 |           3.4494 |
[32m[20230117 13:29:15 @agent_ppo2.py:193][0m |          -0.0079 |           7.7473 |           3.4508 |
[32m[20230117 13:29:15 @agent_ppo2.py:193][0m |          -0.0091 |           7.4188 |           3.4491 |
[32m[20230117 13:29:15 @agent_ppo2.py:193][0m |          -0.0097 |           7.1667 |           3.4516 |
[32m[20230117 13:29:15 @agent_ppo2.py:193][0m |          -0.0111 |           6.9600 |           3.4516 |
[32m[20230117 13:29:16 @agent_ppo2.py:193][0m |          -0.0114 |           6.8167 |           3.4525 |
[32m[20230117 13:29:16 @agent_ppo2.py:193][0m |          -0.0122 |           6.6940 |           3.4498 |
[32m[20230117 13:29:16 @agent_ppo2.py:193][0m |          -0.0126 |           6.5727 |           3.4531 |
[32m[20230117 13:29:16 @agent_ppo2.py:193][0m |          -0.0126 |           6.4608 |           3.4501 |
[32m[20230117 13:29:16 @agent_ppo2.py:138][0m Policy update time: 0.95 s
[32m[20230117 13:29:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 247.72
[32m[20230117 13:29:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.04
[32m[20230117 13:29:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.81
[32m[20230117 13:29:16 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 272.81
[32m[20230117 13:29:16 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 272.81
[32m[20230117 13:29:16 @agent_ppo2.py:151][0m Total time:       2.85 min
[32m[20230117 13:29:16 @agent_ppo2.py:153][0m 217088 total steps have happened
[32m[20230117 13:29:16 @agent_ppo2.py:129][0m #------------------------ Iteration 106 --------------------------#
[32m[20230117 13:29:16 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:16 @agent_ppo2.py:193][0m |          -0.0014 |           7.3075 |           3.4264 |
[32m[20230117 13:29:17 @agent_ppo2.py:193][0m |          -0.0058 |           6.7331 |           3.4163 |
[32m[20230117 13:29:17 @agent_ppo2.py:193][0m |          -0.0071 |           6.4882 |           3.4180 |
[32m[20230117 13:29:17 @agent_ppo2.py:193][0m |          -0.0081 |           6.2815 |           3.4176 |
[32m[20230117 13:29:17 @agent_ppo2.py:193][0m |          -0.0089 |           6.1230 |           3.4170 |
[32m[20230117 13:29:17 @agent_ppo2.py:193][0m |          -0.0100 |           5.9733 |           3.4177 |
[32m[20230117 13:29:17 @agent_ppo2.py:193][0m |          -0.0095 |           5.8476 |           3.4170 |
[32m[20230117 13:29:17 @agent_ppo2.py:193][0m |          -0.0104 |           5.7185 |           3.4162 |
[32m[20230117 13:29:17 @agent_ppo2.py:193][0m |          -0.0107 |           5.6168 |           3.4172 |
[32m[20230117 13:29:17 @agent_ppo2.py:193][0m |          -0.0115 |           5.5061 |           3.4146 |
[32m[20230117 13:29:17 @agent_ppo2.py:138][0m Policy update time: 0.91 s
[32m[20230117 13:29:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.77
[32m[20230117 13:29:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.80
[32m[20230117 13:29:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 140.67
[32m[20230117 13:29:17 @agent_ppo2.py:151][0m Total time:       2.87 min
[32m[20230117 13:29:17 @agent_ppo2.py:153][0m 219136 total steps have happened
[32m[20230117 13:29:17 @agent_ppo2.py:129][0m #------------------------ Iteration 107 --------------------------#
[32m[20230117 13:29:18 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:29:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:18 @agent_ppo2.py:193][0m |           0.0016 |           7.5343 |           3.4085 |
[32m[20230117 13:29:18 @agent_ppo2.py:193][0m |          -0.0016 |           6.9095 |           3.4058 |
[32m[20230117 13:29:18 @agent_ppo2.py:193][0m |          -0.0071 |           6.5075 |           3.4084 |
[32m[20230117 13:29:18 @agent_ppo2.py:193][0m |          -0.0041 |           6.2243 |           3.4095 |
[32m[20230117 13:29:18 @agent_ppo2.py:193][0m |          -0.0064 |           5.9656 |           3.4087 |
[32m[20230117 13:29:18 @agent_ppo2.py:193][0m |          -0.0003 |           5.8740 |           3.4113 |
[32m[20230117 13:29:18 @agent_ppo2.py:193][0m |          -0.0111 |           5.4620 |           3.4094 |
[32m[20230117 13:29:18 @agent_ppo2.py:193][0m |          -0.0103 |           5.1662 |           3.4111 |
[32m[20230117 13:29:19 @agent_ppo2.py:193][0m |          -0.0091 |           4.8726 |           3.4120 |
[32m[20230117 13:29:19 @agent_ppo2.py:193][0m |          -0.0101 |           4.5456 |           3.4120 |
[32m[20230117 13:29:19 @agent_ppo2.py:138][0m Policy update time: 0.89 s
[32m[20230117 13:29:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.98
[32m[20230117 13:29:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.79
[32m[20230117 13:29:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 22.03
[32m[20230117 13:29:19 @agent_ppo2.py:151][0m Total time:       2.90 min
[32m[20230117 13:29:19 @agent_ppo2.py:153][0m 221184 total steps have happened
[32m[20230117 13:29:19 @agent_ppo2.py:129][0m #------------------------ Iteration 108 --------------------------#
[32m[20230117 13:29:19 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:19 @agent_ppo2.py:193][0m |           0.0052 |          14.7711 |           3.5555 |
[32m[20230117 13:29:19 @agent_ppo2.py:193][0m |          -0.0026 |          10.8077 |           3.5555 |
[32m[20230117 13:29:19 @agent_ppo2.py:193][0m |          -0.0024 |          10.0944 |           3.5513 |
[32m[20230117 13:29:19 @agent_ppo2.py:193][0m |          -0.0055 |           9.7310 |           3.5539 |
[32m[20230117 13:29:19 @agent_ppo2.py:193][0m |          -0.0071 |           9.5063 |           3.5534 |
[32m[20230117 13:29:20 @agent_ppo2.py:193][0m |          -0.0083 |           9.2635 |           3.5538 |
[32m[20230117 13:29:20 @agent_ppo2.py:193][0m |          -0.0091 |           9.1368 |           3.5525 |
[32m[20230117 13:29:20 @agent_ppo2.py:193][0m |          -0.0097 |           8.9663 |           3.5534 |
[32m[20230117 13:29:20 @agent_ppo2.py:193][0m |          -0.0096 |           8.8630 |           3.5519 |
[32m[20230117 13:29:20 @agent_ppo2.py:193][0m |          -0.0110 |           8.7459 |           3.5528 |
[32m[20230117 13:29:20 @agent_ppo2.py:138][0m Policy update time: 0.85 s
[32m[20230117 13:29:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 196.22
[32m[20230117 13:29:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 249.64
[32m[20230117 13:29:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.36
[32m[20230117 13:29:20 @agent_ppo2.py:151][0m Total time:       2.92 min
[32m[20230117 13:29:20 @agent_ppo2.py:153][0m 223232 total steps have happened
[32m[20230117 13:29:20 @agent_ppo2.py:129][0m #------------------------ Iteration 109 --------------------------#
[32m[20230117 13:29:20 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:20 @agent_ppo2.py:193][0m |          -0.0013 |          24.8358 |           3.5061 |
[32m[20230117 13:29:21 @agent_ppo2.py:193][0m |          -0.0078 |          17.3111 |           3.5002 |
[32m[20230117 13:29:21 @agent_ppo2.py:193][0m |          -0.0088 |          16.0450 |           3.5001 |
[32m[20230117 13:29:21 @agent_ppo2.py:193][0m |          -0.0097 |          15.2145 |           3.4983 |
[32m[20230117 13:29:21 @agent_ppo2.py:193][0m |          -0.0104 |          14.7138 |           3.4967 |
[32m[20230117 13:29:21 @agent_ppo2.py:193][0m |          -0.0109 |          14.2368 |           3.4989 |
[32m[20230117 13:29:21 @agent_ppo2.py:193][0m |          -0.0114 |          13.8354 |           3.4978 |
[32m[20230117 13:29:21 @agent_ppo2.py:193][0m |          -0.0113 |          13.4590 |           3.4964 |
[32m[20230117 13:29:21 @agent_ppo2.py:193][0m |          -0.0120 |          13.0063 |           3.4970 |
[32m[20230117 13:29:21 @agent_ppo2.py:193][0m |          -0.0128 |          12.6235 |           3.5001 |
[32m[20230117 13:29:21 @agent_ppo2.py:138][0m Policy update time: 0.88 s
[32m[20230117 13:29:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 219.53
[32m[20230117 13:29:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.90
[32m[20230117 13:29:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 85.46
[32m[20230117 13:29:21 @agent_ppo2.py:151][0m Total time:       2.94 min
[32m[20230117 13:29:21 @agent_ppo2.py:153][0m 225280 total steps have happened
[32m[20230117 13:29:21 @agent_ppo2.py:129][0m #------------------------ Iteration 110 --------------------------#
[32m[20230117 13:29:22 @agent_ppo2.py:135][0m Sampling time: 0.32 s by 4 slaves
[32m[20230117 13:29:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:22 @agent_ppo2.py:193][0m |           0.0174 |          20.2091 |           3.4864 |
[32m[20230117 13:29:22 @agent_ppo2.py:193][0m |          -0.0034 |          15.3113 |           3.4768 |
[32m[20230117 13:29:22 @agent_ppo2.py:193][0m |          -0.0055 |          13.8465 |           3.4806 |
[32m[20230117 13:29:22 @agent_ppo2.py:193][0m |          -0.0090 |          13.4373 |           3.4785 |
[32m[20230117 13:29:22 @agent_ppo2.py:193][0m |          -0.0047 |          13.2261 |           3.4755 |
[32m[20230117 13:29:22 @agent_ppo2.py:193][0m |          -0.0089 |          12.5196 |           3.4735 |
[32m[20230117 13:29:22 @agent_ppo2.py:193][0m |           0.0025 |          13.6552 |           3.4746 |
[32m[20230117 13:29:23 @agent_ppo2.py:193][0m |          -0.0016 |          13.2919 |           3.4751 |
[32m[20230117 13:29:23 @agent_ppo2.py:193][0m |          -0.0101 |          11.7834 |           3.4748 |
[32m[20230117 13:29:23 @agent_ppo2.py:193][0m |          -0.0106 |          11.6204 |           3.4726 |
[32m[20230117 13:29:23 @agent_ppo2.py:138][0m Policy update time: 1.00 s
[32m[20230117 13:29:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: 183.09
[32m[20230117 13:29:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.29
[32m[20230117 13:29:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.67
[32m[20230117 13:29:23 @agent_ppo2.py:151][0m Total time:       2.97 min
[32m[20230117 13:29:23 @agent_ppo2.py:153][0m 227328 total steps have happened
[32m[20230117 13:29:23 @agent_ppo2.py:129][0m #------------------------ Iteration 111 --------------------------#
[32m[20230117 13:29:23 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:29:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:23 @agent_ppo2.py:193][0m |           0.0022 |          16.5477 |           3.5021 |
[32m[20230117 13:29:23 @agent_ppo2.py:193][0m |          -0.0005 |          12.1789 |           3.4993 |
[32m[20230117 13:29:24 @agent_ppo2.py:193][0m |          -0.0025 |          11.2346 |           3.4956 |
[32m[20230117 13:29:24 @agent_ppo2.py:193][0m |          -0.0044 |          10.6010 |           3.4956 |
[32m[20230117 13:29:24 @agent_ppo2.py:193][0m |          -0.0057 |          10.1884 |           3.4943 |
[32m[20230117 13:29:24 @agent_ppo2.py:193][0m |          -0.0083 |           9.8351 |           3.4949 |
[32m[20230117 13:29:24 @agent_ppo2.py:193][0m |          -0.0066 |           9.5747 |           3.4961 |
[32m[20230117 13:29:24 @agent_ppo2.py:193][0m |          -0.0069 |           9.3722 |           3.4915 |
[32m[20230117 13:29:24 @agent_ppo2.py:193][0m |          -0.0087 |           9.1351 |           3.4926 |
[32m[20230117 13:29:24 @agent_ppo2.py:193][0m |          -0.0085 |           8.9634 |           3.4929 |
[32m[20230117 13:29:24 @agent_ppo2.py:138][0m Policy update time: 0.90 s
[32m[20230117 13:29:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 218.60
[32m[20230117 13:29:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.73
[32m[20230117 13:29:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 160.92
[32m[20230117 13:29:24 @agent_ppo2.py:151][0m Total time:       2.99 min
[32m[20230117 13:29:24 @agent_ppo2.py:153][0m 229376 total steps have happened
[32m[20230117 13:29:24 @agent_ppo2.py:129][0m #------------------------ Iteration 112 --------------------------#
[32m[20230117 13:29:25 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:25 @agent_ppo2.py:193][0m |           0.0022 |          18.3251 |           3.5050 |
[32m[20230117 13:29:25 @agent_ppo2.py:193][0m |          -0.0009 |          11.3878 |           3.5067 |
[32m[20230117 13:29:25 @agent_ppo2.py:193][0m |          -0.0033 |          11.1067 |           3.5072 |
[32m[20230117 13:29:25 @agent_ppo2.py:193][0m |          -0.0022 |          10.8628 |           3.5047 |
[32m[20230117 13:29:25 @agent_ppo2.py:193][0m |          -0.0048 |          10.7325 |           3.5089 |
[32m[20230117 13:29:25 @agent_ppo2.py:193][0m |          -0.0049 |          10.6188 |           3.5100 |
[32m[20230117 13:29:25 @agent_ppo2.py:193][0m |          -0.0063 |          10.5310 |           3.5102 |
[32m[20230117 13:29:25 @agent_ppo2.py:193][0m |          -0.0050 |          10.4409 |           3.5080 |
[32m[20230117 13:29:25 @agent_ppo2.py:193][0m |          -0.0063 |          10.3759 |           3.5126 |
[32m[20230117 13:29:25 @agent_ppo2.py:193][0m |          -0.0072 |          10.3146 |           3.5124 |
[32m[20230117 13:29:25 @agent_ppo2.py:138][0m Policy update time: 0.90 s
[32m[20230117 13:29:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 179.58
[32m[20230117 13:29:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.72
[32m[20230117 13:29:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -77.34
[32m[20230117 13:29:26 @agent_ppo2.py:151][0m Total time:       3.01 min
[32m[20230117 13:29:26 @agent_ppo2.py:153][0m 231424 total steps have happened
[32m[20230117 13:29:26 @agent_ppo2.py:129][0m #------------------------ Iteration 113 --------------------------#
[32m[20230117 13:29:26 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:29:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:26 @agent_ppo2.py:193][0m |           0.0025 |           9.5401 |           3.5554 |
[32m[20230117 13:29:26 @agent_ppo2.py:193][0m |          -0.0028 |           8.4283 |           3.5544 |
[32m[20230117 13:29:26 @agent_ppo2.py:193][0m |          -0.0050 |           8.0308 |           3.5536 |
[32m[20230117 13:29:26 @agent_ppo2.py:193][0m |          -0.0039 |           7.8700 |           3.5487 |
[32m[20230117 13:29:26 @agent_ppo2.py:193][0m |          -0.0073 |           7.5893 |           3.5480 |
[32m[20230117 13:29:26 @agent_ppo2.py:193][0m |          -0.0042 |           7.5059 |           3.5468 |
[32m[20230117 13:29:27 @agent_ppo2.py:193][0m |          -0.0079 |           7.2328 |           3.5453 |
[32m[20230117 13:29:27 @agent_ppo2.py:193][0m |          -0.0070 |           7.1636 |           3.5441 |
[32m[20230117 13:29:27 @agent_ppo2.py:193][0m |          -0.0079 |           6.9946 |           3.5442 |
[32m[20230117 13:29:27 @agent_ppo2.py:193][0m |          -0.0093 |           6.8973 |           3.5432 |
[32m[20230117 13:29:27 @agent_ppo2.py:138][0m Policy update time: 0.90 s
[32m[20230117 13:29:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.48
[32m[20230117 13:29:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.45
[32m[20230117 13:29:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -48.26
[32m[20230117 13:29:27 @agent_ppo2.py:151][0m Total time:       3.03 min
[32m[20230117 13:29:27 @agent_ppo2.py:153][0m 233472 total steps have happened
[32m[20230117 13:29:27 @agent_ppo2.py:129][0m #------------------------ Iteration 114 --------------------------#
[32m[20230117 13:29:27 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:29:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:27 @agent_ppo2.py:193][0m |          -0.0020 |          24.1499 |           3.5272 |
[32m[20230117 13:29:27 @agent_ppo2.py:193][0m |          -0.0056 |          15.4308 |           3.5198 |
[32m[20230117 13:29:27 @agent_ppo2.py:193][0m |          -0.0091 |          14.0272 |           3.5153 |
[32m[20230117 13:29:28 @agent_ppo2.py:193][0m |          -0.0101 |          13.6435 |           3.5101 |
[32m[20230117 13:29:28 @agent_ppo2.py:193][0m |          -0.0112 |          12.4287 |           3.5076 |
[32m[20230117 13:29:28 @agent_ppo2.py:193][0m |          -0.0118 |          11.9751 |           3.5057 |
[32m[20230117 13:29:28 @agent_ppo2.py:193][0m |          -0.0105 |          11.6233 |           3.5069 |
[32m[20230117 13:29:28 @agent_ppo2.py:193][0m |          -0.0132 |          11.1335 |           3.5048 |
[32m[20230117 13:29:28 @agent_ppo2.py:193][0m |          -0.0126 |          11.1333 |           3.5045 |
[32m[20230117 13:29:28 @agent_ppo2.py:193][0m |          -0.0142 |          10.8061 |           3.5007 |
[32m[20230117 13:29:28 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:29:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 133.69
[32m[20230117 13:29:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.62
[32m[20230117 13:29:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 87.23
[32m[20230117 13:29:28 @agent_ppo2.py:151][0m Total time:       3.05 min
[32m[20230117 13:29:28 @agent_ppo2.py:153][0m 235520 total steps have happened
[32m[20230117 13:29:28 @agent_ppo2.py:129][0m #------------------------ Iteration 115 --------------------------#
[32m[20230117 13:29:28 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:29 @agent_ppo2.py:193][0m |           0.0004 |          13.4687 |           3.5113 |
[32m[20230117 13:29:29 @agent_ppo2.py:193][0m |          -0.0045 |          10.9400 |           3.5030 |
[32m[20230117 13:29:29 @agent_ppo2.py:193][0m |          -0.0026 |          10.2815 |           3.4982 |
[32m[20230117 13:29:29 @agent_ppo2.py:193][0m |          -0.0003 |          10.3507 |           3.4980 |
[32m[20230117 13:29:29 @agent_ppo2.py:193][0m |          -0.0051 |           9.8196 |           3.4986 |
[32m[20230117 13:29:29 @agent_ppo2.py:193][0m |          -0.0063 |           9.7635 |           3.4983 |
[32m[20230117 13:29:29 @agent_ppo2.py:193][0m |          -0.0063 |           9.5598 |           3.4956 |
[32m[20230117 13:29:29 @agent_ppo2.py:193][0m |          -0.0058 |           9.5331 |           3.4976 |
[32m[20230117 13:29:29 @agent_ppo2.py:193][0m |          -0.0064 |           9.3437 |           3.4974 |
[32m[20230117 13:29:29 @agent_ppo2.py:193][0m |          -0.0093 |           9.2232 |           3.4947 |
[32m[20230117 13:29:29 @agent_ppo2.py:138][0m Policy update time: 0.91 s
[32m[20230117 13:29:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.57
[32m[20230117 13:29:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.02
[32m[20230117 13:29:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -23.91
[32m[20230117 13:29:30 @agent_ppo2.py:151][0m Total time:       3.08 min
[32m[20230117 13:29:30 @agent_ppo2.py:153][0m 237568 total steps have happened
[32m[20230117 13:29:30 @agent_ppo2.py:129][0m #------------------------ Iteration 116 --------------------------#
[32m[20230117 13:29:30 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:29:30 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:30 @agent_ppo2.py:193][0m |           0.0012 |          48.4977 |           3.5299 |
[32m[20230117 13:29:30 @agent_ppo2.py:193][0m |          -0.0026 |          23.9503 |           3.5288 |
[32m[20230117 13:29:30 @agent_ppo2.py:193][0m |          -0.0054 |          20.7221 |           3.5269 |
[32m[20230117 13:29:30 @agent_ppo2.py:193][0m |          -0.0010 |          18.2626 |           3.5280 |
[32m[20230117 13:29:30 @agent_ppo2.py:193][0m |          -0.0067 |          16.5487 |           3.5263 |
[32m[20230117 13:29:30 @agent_ppo2.py:193][0m |          -0.0094 |          15.3038 |           3.5253 |
[32m[20230117 13:29:30 @agent_ppo2.py:193][0m |          -0.0085 |          14.8895 |           3.5259 |
[32m[20230117 13:29:30 @agent_ppo2.py:193][0m |          -0.0087 |          14.0548 |           3.5262 |
[32m[20230117 13:29:30 @agent_ppo2.py:193][0m |          -0.0101 |          13.6634 |           3.5252 |
[32m[20230117 13:29:31 @agent_ppo2.py:193][0m |          -0.0095 |          12.8029 |           3.5264 |
[32m[20230117 13:29:31 @agent_ppo2.py:138][0m Policy update time: 0.69 s
[32m[20230117 13:29:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 99.37
[32m[20230117 13:29:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 245.66
[32m[20230117 13:29:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.59
[32m[20230117 13:29:31 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 273.59
[32m[20230117 13:29:31 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 273.59
[32m[20230117 13:29:31 @agent_ppo2.py:151][0m Total time:       3.10 min
[32m[20230117 13:29:31 @agent_ppo2.py:153][0m 239616 total steps have happened
[32m[20230117 13:29:31 @agent_ppo2.py:129][0m #------------------------ Iteration 117 --------------------------#
[32m[20230117 13:29:31 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:31 @agent_ppo2.py:193][0m |           0.0015 |          10.7778 |           3.5710 |
[32m[20230117 13:29:31 @agent_ppo2.py:193][0m |          -0.0046 |          10.1079 |           3.5684 |
[32m[20230117 13:29:31 @agent_ppo2.py:193][0m |          -0.0058 |           9.9230 |           3.5652 |
[32m[20230117 13:29:31 @agent_ppo2.py:193][0m |          -0.0053 |           9.9786 |           3.5630 |
[32m[20230117 13:29:32 @agent_ppo2.py:193][0m |          -0.0093 |           9.6967 |           3.5630 |
[32m[20230117 13:29:32 @agent_ppo2.py:193][0m |          -0.0059 |           9.8996 |           3.5622 |
[32m[20230117 13:29:32 @agent_ppo2.py:193][0m |          -0.0072 |           9.6791 |           3.5609 |
[32m[20230117 13:29:32 @agent_ppo2.py:193][0m |          -0.0062 |           9.5613 |           3.5617 |
[32m[20230117 13:29:32 @agent_ppo2.py:193][0m |          -0.0102 |           9.4980 |           3.5614 |
[32m[20230117 13:29:32 @agent_ppo2.py:193][0m |          -0.0103 |           9.4964 |           3.5631 |
[32m[20230117 13:29:32 @agent_ppo2.py:138][0m Policy update time: 0.94 s
[32m[20230117 13:29:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.60
[32m[20230117 13:29:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 250.88
[32m[20230117 13:29:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.50
[32m[20230117 13:29:32 @agent_ppo2.py:151][0m Total time:       3.12 min
[32m[20230117 13:29:32 @agent_ppo2.py:153][0m 241664 total steps have happened
[32m[20230117 13:29:32 @agent_ppo2.py:129][0m #------------------------ Iteration 118 --------------------------#
[32m[20230117 13:29:32 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:29:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:33 @agent_ppo2.py:193][0m |           0.0014 |           9.5548 |           3.5063 |
[32m[20230117 13:29:33 @agent_ppo2.py:193][0m |          -0.0030 |           8.9228 |           3.5049 |
[32m[20230117 13:29:33 @agent_ppo2.py:193][0m |          -0.0048 |           8.6993 |           3.5041 |
[32m[20230117 13:29:33 @agent_ppo2.py:193][0m |          -0.0066 |           8.5413 |           3.5011 |
[32m[20230117 13:29:33 @agent_ppo2.py:193][0m |          -0.0074 |           8.4587 |           3.5007 |
[32m[20230117 13:29:33 @agent_ppo2.py:193][0m |          -0.0077 |           8.3643 |           3.5012 |
[32m[20230117 13:29:33 @agent_ppo2.py:193][0m |          -0.0084 |           8.2607 |           3.4996 |
[32m[20230117 13:29:33 @agent_ppo2.py:193][0m |          -0.0093 |           8.2278 |           3.5008 |
[32m[20230117 13:29:33 @agent_ppo2.py:193][0m |          -0.0097 |           8.1423 |           3.5003 |
[32m[20230117 13:29:33 @agent_ppo2.py:193][0m |          -0.0100 |           8.1318 |           3.4993 |
[32m[20230117 13:29:33 @agent_ppo2.py:138][0m Policy update time: 0.93 s
[32m[20230117 13:29:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 249.08
[32m[20230117 13:29:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 251.21
[32m[20230117 13:29:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.22
[32m[20230117 13:29:34 @agent_ppo2.py:151][0m Total time:       3.14 min
[32m[20230117 13:29:34 @agent_ppo2.py:153][0m 243712 total steps have happened
[32m[20230117 13:29:34 @agent_ppo2.py:129][0m #------------------------ Iteration 119 --------------------------#
[32m[20230117 13:29:34 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:29:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:34 @agent_ppo2.py:193][0m |          -0.0023 |          10.8705 |           3.5239 |
[32m[20230117 13:29:34 @agent_ppo2.py:193][0m |          -0.0082 |          10.3716 |           3.5184 |
[32m[20230117 13:29:34 @agent_ppo2.py:193][0m |          -0.0139 |          10.1847 |           3.5218 |
[32m[20230117 13:29:34 @agent_ppo2.py:193][0m |          -0.0108 |          10.0397 |           3.5223 |
[32m[20230117 13:29:34 @agent_ppo2.py:193][0m |          -0.0137 |           9.9072 |           3.5214 |
[32m[20230117 13:29:34 @agent_ppo2.py:193][0m |          -0.0127 |           9.8051 |           3.5227 |
[32m[20230117 13:29:35 @agent_ppo2.py:193][0m |          -0.0118 |           9.7290 |           3.5233 |
[32m[20230117 13:29:35 @agent_ppo2.py:193][0m |          -0.0146 |           9.6245 |           3.5264 |
[32m[20230117 13:29:35 @agent_ppo2.py:193][0m |          -0.0139 |           9.5597 |           3.5264 |
[32m[20230117 13:29:35 @agent_ppo2.py:193][0m |          -0.0127 |           9.4768 |           3.5273 |
[32m[20230117 13:29:35 @agent_ppo2.py:138][0m Policy update time: 0.88 s
[32m[20230117 13:29:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 254.69
[32m[20230117 13:29:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.96
[32m[20230117 13:29:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.93
[32m[20230117 13:29:35 @agent_ppo2.py:151][0m Total time:       3.17 min
[32m[20230117 13:29:35 @agent_ppo2.py:153][0m 245760 total steps have happened
[32m[20230117 13:29:35 @agent_ppo2.py:129][0m #------------------------ Iteration 120 --------------------------#
[32m[20230117 13:29:35 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:35 @agent_ppo2.py:193][0m |           0.0012 |           9.6644 |           3.5298 |
[32m[20230117 13:29:35 @agent_ppo2.py:193][0m |          -0.0083 |           9.1776 |           3.5246 |
[32m[20230117 13:29:36 @agent_ppo2.py:193][0m |          -0.0058 |           8.9599 |           3.5254 |
[32m[20230117 13:29:36 @agent_ppo2.py:193][0m |          -0.0075 |           8.8253 |           3.5249 |
[32m[20230117 13:29:36 @agent_ppo2.py:193][0m |          -0.0102 |           8.7317 |           3.5274 |
[32m[20230117 13:29:36 @agent_ppo2.py:193][0m |          -0.0132 |           8.6635 |           3.5284 |
[32m[20230117 13:29:36 @agent_ppo2.py:193][0m |          -0.0151 |           8.5913 |           3.5310 |
[32m[20230117 13:29:36 @agent_ppo2.py:193][0m |          -0.0095 |           8.5091 |           3.5302 |
[32m[20230117 13:29:36 @agent_ppo2.py:193][0m |          -0.0092 |           8.4836 |           3.5298 |
[32m[20230117 13:29:36 @agent_ppo2.py:193][0m |          -0.0110 |           8.3769 |           3.5322 |
[32m[20230117 13:29:36 @agent_ppo2.py:138][0m Policy update time: 0.92 s
[32m[20230117 13:29:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.43
[32m[20230117 13:29:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.40
[32m[20230117 13:29:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.20
[32m[20230117 13:29:36 @agent_ppo2.py:151][0m Total time:       3.19 min
[32m[20230117 13:29:36 @agent_ppo2.py:153][0m 247808 total steps have happened
[32m[20230117 13:29:36 @agent_ppo2.py:129][0m #------------------------ Iteration 121 --------------------------#
[32m[20230117 13:29:37 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:29:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:37 @agent_ppo2.py:193][0m |          -0.0006 |          27.6918 |           3.5205 |
[32m[20230117 13:29:37 @agent_ppo2.py:193][0m |          -0.0039 |          17.6739 |           3.5200 |
[32m[20230117 13:29:37 @agent_ppo2.py:193][0m |          -0.0058 |          16.0579 |           3.5183 |
[32m[20230117 13:29:37 @agent_ppo2.py:193][0m |          -0.0054 |          14.7154 |           3.5165 |
[32m[20230117 13:29:37 @agent_ppo2.py:193][0m |          -0.0075 |          13.8104 |           3.5170 |
[32m[20230117 13:29:37 @agent_ppo2.py:193][0m |          -0.0080 |          13.2429 |           3.5142 |
[32m[20230117 13:29:37 @agent_ppo2.py:193][0m |          -0.0082 |          12.7386 |           3.5164 |
[32m[20230117 13:29:37 @agent_ppo2.py:193][0m |          -0.0082 |          12.6488 |           3.5158 |
[32m[20230117 13:29:37 @agent_ppo2.py:193][0m |          -0.0096 |          12.2440 |           3.5160 |
[32m[20230117 13:29:38 @agent_ppo2.py:193][0m |          -0.0103 |          12.0667 |           3.5138 |
[32m[20230117 13:29:38 @agent_ppo2.py:138][0m Policy update time: 0.91 s
[32m[20230117 13:29:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 189.77
[32m[20230117 13:29:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.72
[32m[20230117 13:29:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 97.61
[32m[20230117 13:29:38 @agent_ppo2.py:151][0m Total time:       3.21 min
[32m[20230117 13:29:38 @agent_ppo2.py:153][0m 249856 total steps have happened
[32m[20230117 13:29:38 @agent_ppo2.py:129][0m #------------------------ Iteration 122 --------------------------#
[32m[20230117 13:29:38 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:29:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:38 @agent_ppo2.py:193][0m |           0.0016 |          26.6231 |           3.5842 |
[32m[20230117 13:29:38 @agent_ppo2.py:193][0m |          -0.0001 |          22.3727 |           3.5808 |
[32m[20230117 13:29:38 @agent_ppo2.py:193][0m |          -0.0028 |          19.9960 |           3.5765 |
[32m[20230117 13:29:38 @agent_ppo2.py:193][0m |          -0.0048 |          19.4741 |           3.5798 |
[32m[20230117 13:29:39 @agent_ppo2.py:193][0m |          -0.0059 |          18.7487 |           3.5758 |
[32m[20230117 13:29:39 @agent_ppo2.py:193][0m |          -0.0067 |          17.8057 |           3.5765 |
[32m[20230117 13:29:39 @agent_ppo2.py:193][0m |          -0.0076 |          17.1683 |           3.5756 |
[32m[20230117 13:29:39 @agent_ppo2.py:193][0m |          -0.0074 |          16.4961 |           3.5776 |
[32m[20230117 13:29:39 @agent_ppo2.py:193][0m |          -0.0097 |          16.4295 |           3.5798 |
[32m[20230117 13:29:39 @agent_ppo2.py:193][0m |          -0.0083 |          16.5796 |           3.5811 |
[32m[20230117 13:29:39 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:29:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 210.15
[32m[20230117 13:29:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.51
[32m[20230117 13:29:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 34.16
[32m[20230117 13:29:39 @agent_ppo2.py:151][0m Total time:       3.24 min
[32m[20230117 13:29:39 @agent_ppo2.py:153][0m 251904 total steps have happened
[32m[20230117 13:29:39 @agent_ppo2.py:129][0m #------------------------ Iteration 123 --------------------------#
[32m[20230117 13:29:39 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:29:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:40 @agent_ppo2.py:193][0m |          -0.0000 |          16.9314 |           3.5787 |
[32m[20230117 13:29:40 @agent_ppo2.py:193][0m |          -0.0027 |          14.5883 |           3.5743 |
[32m[20230117 13:29:40 @agent_ppo2.py:193][0m |          -0.0063 |          14.2527 |           3.5692 |
[32m[20230117 13:29:40 @agent_ppo2.py:193][0m |          -0.0069 |          13.9560 |           3.5674 |
[32m[20230117 13:29:40 @agent_ppo2.py:193][0m |          -0.0095 |          13.3481 |           3.5652 |
[32m[20230117 13:29:40 @agent_ppo2.py:193][0m |          -0.0108 |          13.0706 |           3.5689 |
[32m[20230117 13:29:40 @agent_ppo2.py:193][0m |          -0.0105 |          12.8855 |           3.5660 |
[32m[20230117 13:29:40 @agent_ppo2.py:193][0m |          -0.0107 |          12.6537 |           3.5672 |
[32m[20230117 13:29:40 @agent_ppo2.py:193][0m |          -0.0081 |          12.4158 |           3.5632 |
[32m[20230117 13:29:40 @agent_ppo2.py:193][0m |          -0.0123 |          12.2460 |           3.5638 |
[32m[20230117 13:29:40 @agent_ppo2.py:138][0m Policy update time: 0.92 s
[32m[20230117 13:29:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 195.62
[32m[20230117 13:29:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.46
[32m[20230117 13:29:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.71
[32m[20230117 13:29:41 @agent_ppo2.py:151][0m Total time:       3.26 min
[32m[20230117 13:29:41 @agent_ppo2.py:153][0m 253952 total steps have happened
[32m[20230117 13:29:41 @agent_ppo2.py:129][0m #------------------------ Iteration 124 --------------------------#
[32m[20230117 13:29:41 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:29:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:41 @agent_ppo2.py:193][0m |          -0.0009 |          18.5622 |           3.6149 |
[32m[20230117 13:29:41 @agent_ppo2.py:193][0m |          -0.0039 |          13.2256 |           3.6091 |
[32m[20230117 13:29:41 @agent_ppo2.py:193][0m |          -0.0070 |          12.5518 |           3.6118 |
[32m[20230117 13:29:41 @agent_ppo2.py:193][0m |          -0.0078 |          11.9901 |           3.6080 |
[32m[20230117 13:29:41 @agent_ppo2.py:193][0m |          -0.0097 |          11.7843 |           3.6087 |
[32m[20230117 13:29:41 @agent_ppo2.py:193][0m |          -0.0102 |          11.5744 |           3.6105 |
[32m[20230117 13:29:41 @agent_ppo2.py:193][0m |          -0.0110 |          11.2932 |           3.6070 |
[32m[20230117 13:29:41 @agent_ppo2.py:193][0m |          -0.0115 |          11.1716 |           3.6059 |
[32m[20230117 13:29:42 @agent_ppo2.py:193][0m |          -0.0124 |          11.0262 |           3.6058 |
[32m[20230117 13:29:42 @agent_ppo2.py:193][0m |          -0.0122 |          10.9339 |           3.6053 |
[32m[20230117 13:29:42 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:29:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 185.63
[32m[20230117 13:29:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.57
[32m[20230117 13:29:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 91.51
[32m[20230117 13:29:42 @agent_ppo2.py:151][0m Total time:       3.28 min
[32m[20230117 13:29:42 @agent_ppo2.py:153][0m 256000 total steps have happened
[32m[20230117 13:29:42 @agent_ppo2.py:129][0m #------------------------ Iteration 125 --------------------------#
[32m[20230117 13:29:42 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:29:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:42 @agent_ppo2.py:193][0m |          -0.0003 |          21.8863 |           3.6410 |
[32m[20230117 13:29:42 @agent_ppo2.py:193][0m |          -0.0044 |          20.0377 |           3.6382 |
[32m[20230117 13:29:42 @agent_ppo2.py:193][0m |          -0.0066 |          19.1225 |           3.6380 |
[32m[20230117 13:29:42 @agent_ppo2.py:193][0m |          -0.0077 |          18.5839 |           3.6364 |
[32m[20230117 13:29:43 @agent_ppo2.py:193][0m |          -0.0088 |          18.0755 |           3.6375 |
[32m[20230117 13:29:43 @agent_ppo2.py:193][0m |          -0.0093 |          17.5635 |           3.6341 |
[32m[20230117 13:29:43 @agent_ppo2.py:193][0m |          -0.0093 |          17.1240 |           3.6362 |
[32m[20230117 13:29:43 @agent_ppo2.py:193][0m |          -0.0102 |          16.9036 |           3.6326 |
[32m[20230117 13:29:43 @agent_ppo2.py:193][0m |          -0.0107 |          16.5911 |           3.6347 |
[32m[20230117 13:29:43 @agent_ppo2.py:193][0m |          -0.0109 |          16.3311 |           3.6330 |
[32m[20230117 13:29:43 @agent_ppo2.py:138][0m Policy update time: 0.95 s
[32m[20230117 13:29:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 189.44
[32m[20230117 13:29:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.98
[32m[20230117 13:29:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 116.27
[32m[20230117 13:29:43 @agent_ppo2.py:151][0m Total time:       3.30 min
[32m[20230117 13:29:43 @agent_ppo2.py:153][0m 258048 total steps have happened
[32m[20230117 13:29:43 @agent_ppo2.py:129][0m #------------------------ Iteration 126 --------------------------#
[32m[20230117 13:29:44 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:44 @agent_ppo2.py:193][0m |          -0.0003 |          35.3355 |           3.6330 |
[32m[20230117 13:29:44 @agent_ppo2.py:193][0m |          -0.0066 |          23.8247 |           3.6298 |
[32m[20230117 13:29:44 @agent_ppo2.py:193][0m |          -0.0087 |          20.8800 |           3.6254 |
[32m[20230117 13:29:44 @agent_ppo2.py:193][0m |          -0.0105 |          19.3815 |           3.6239 |
[32m[20230117 13:29:44 @agent_ppo2.py:193][0m |          -0.0120 |          18.1541 |           3.6258 |
[32m[20230117 13:29:44 @agent_ppo2.py:193][0m |          -0.0124 |          17.0748 |           3.6237 |
[32m[20230117 13:29:44 @agent_ppo2.py:193][0m |          -0.0130 |          16.4620 |           3.6235 |
[32m[20230117 13:29:44 @agent_ppo2.py:193][0m |          -0.0139 |          15.9123 |           3.6221 |
[32m[20230117 13:29:44 @agent_ppo2.py:193][0m |          -0.0136 |          15.4711 |           3.6199 |
[32m[20230117 13:29:44 @agent_ppo2.py:193][0m |          -0.0144 |          15.4586 |           3.6220 |
[32m[20230117 13:29:44 @agent_ppo2.py:138][0m Policy update time: 0.90 s
[32m[20230117 13:29:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 105.54
[32m[20230117 13:29:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 258.21
[32m[20230117 13:29:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.64
[32m[20230117 13:29:45 @agent_ppo2.py:151][0m Total time:       3.33 min
[32m[20230117 13:29:45 @agent_ppo2.py:153][0m 260096 total steps have happened
[32m[20230117 13:29:45 @agent_ppo2.py:129][0m #------------------------ Iteration 127 --------------------------#
[32m[20230117 13:29:45 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:45 @agent_ppo2.py:193][0m |          -0.0006 |          10.7426 |           3.5936 |
[32m[20230117 13:29:45 @agent_ppo2.py:193][0m |          -0.0090 |           9.6502 |           3.5877 |
[32m[20230117 13:29:45 @agent_ppo2.py:193][0m |          -0.0081 |           9.4452 |           3.5888 |
[32m[20230117 13:29:45 @agent_ppo2.py:193][0m |          -0.0109 |           9.2193 |           3.5888 |
[32m[20230117 13:29:45 @agent_ppo2.py:193][0m |          -0.0096 |           9.0788 |           3.5911 |
[32m[20230117 13:29:46 @agent_ppo2.py:193][0m |          -0.0100 |           8.9601 |           3.5900 |
[32m[20230117 13:29:46 @agent_ppo2.py:193][0m |          -0.0004 |          10.0278 |           3.5885 |
[32m[20230117 13:29:46 @agent_ppo2.py:193][0m |          -0.0114 |           8.7441 |           3.5864 |
[32m[20230117 13:29:46 @agent_ppo2.py:193][0m |          -0.0069 |           9.0554 |           3.5880 |
[32m[20230117 13:29:46 @agent_ppo2.py:193][0m |          -0.0133 |           8.5442 |           3.5890 |
[32m[20230117 13:29:46 @agent_ppo2.py:138][0m Policy update time: 0.89 s
[32m[20230117 13:29:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 254.34
[32m[20230117 13:29:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.66
[32m[20230117 13:29:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.92
[32m[20230117 13:29:46 @agent_ppo2.py:151][0m Total time:       3.35 min
[32m[20230117 13:29:46 @agent_ppo2.py:153][0m 262144 total steps have happened
[32m[20230117 13:29:46 @agent_ppo2.py:129][0m #------------------------ Iteration 128 --------------------------#
[32m[20230117 13:29:46 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:46 @agent_ppo2.py:193][0m |          -0.0003 |          10.1725 |           3.5780 |
[32m[20230117 13:29:47 @agent_ppo2.py:193][0m |          -0.0039 |           9.6503 |           3.5739 |
[32m[20230117 13:29:47 @agent_ppo2.py:193][0m |          -0.0059 |           9.3901 |           3.5727 |
[32m[20230117 13:29:47 @agent_ppo2.py:193][0m |          -0.0053 |           9.2815 |           3.5751 |
[32m[20230117 13:29:47 @agent_ppo2.py:193][0m |          -0.0073 |           9.1218 |           3.5779 |
[32m[20230117 13:29:47 @agent_ppo2.py:193][0m |          -0.0073 |           9.0059 |           3.5773 |
[32m[20230117 13:29:47 @agent_ppo2.py:193][0m |          -0.0069 |           8.9788 |           3.5780 |
[32m[20230117 13:29:47 @agent_ppo2.py:193][0m |          -0.0074 |           8.8506 |           3.5792 |
[32m[20230117 13:29:47 @agent_ppo2.py:193][0m |          -0.0060 |           8.8622 |           3.5783 |
[32m[20230117 13:29:47 @agent_ppo2.py:193][0m |          -0.0094 |           8.7085 |           3.5809 |
[32m[20230117 13:29:47 @agent_ppo2.py:138][0m Policy update time: 0.89 s
[32m[20230117 13:29:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.95
[32m[20230117 13:29:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.84
[32m[20230117 13:29:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 161.42
[32m[20230117 13:29:47 @agent_ppo2.py:151][0m Total time:       3.37 min
[32m[20230117 13:29:47 @agent_ppo2.py:153][0m 264192 total steps have happened
[32m[20230117 13:29:47 @agent_ppo2.py:129][0m #------------------------ Iteration 129 --------------------------#
[32m[20230117 13:29:48 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:29:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:48 @agent_ppo2.py:193][0m |           0.0015 |          23.0790 |           3.6179 |
[32m[20230117 13:29:48 @agent_ppo2.py:193][0m |          -0.0061 |          15.5587 |           3.6181 |
[32m[20230117 13:29:48 @agent_ppo2.py:193][0m |          -0.0063 |          13.6562 |           3.6171 |
[32m[20230117 13:29:48 @agent_ppo2.py:193][0m |          -0.0067 |          12.3100 |           3.6179 |
[32m[20230117 13:29:48 @agent_ppo2.py:193][0m |          -0.0033 |          11.6077 |           3.6149 |
[32m[20230117 13:29:48 @agent_ppo2.py:193][0m |          -0.0117 |          10.6934 |           3.6144 |
[32m[20230117 13:29:48 @agent_ppo2.py:193][0m |          -0.0119 |          10.1631 |           3.6145 |
[32m[20230117 13:29:48 @agent_ppo2.py:193][0m |          -0.0139 |           9.7085 |           3.6154 |
[32m[20230117 13:29:48 @agent_ppo2.py:193][0m |          -0.0139 |           9.5142 |           3.6172 |
[32m[20230117 13:29:48 @agent_ppo2.py:193][0m |          -0.0154 |           9.1636 |           3.6169 |
[32m[20230117 13:29:48 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:29:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 120.58
[32m[20230117 13:29:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.06
[32m[20230117 13:29:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.90
[32m[20230117 13:29:49 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 275.90
[32m[20230117 13:29:49 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 275.90
[32m[20230117 13:29:49 @agent_ppo2.py:151][0m Total time:       3.39 min
[32m[20230117 13:29:49 @agent_ppo2.py:153][0m 266240 total steps have happened
[32m[20230117 13:29:49 @agent_ppo2.py:129][0m #------------------------ Iteration 130 --------------------------#
[32m[20230117 13:29:49 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:29:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:49 @agent_ppo2.py:193][0m |          -0.0011 |          19.2538 |           3.6621 |
[32m[20230117 13:29:49 @agent_ppo2.py:193][0m |          -0.0087 |          14.4964 |           3.6560 |
[32m[20230117 13:29:49 @agent_ppo2.py:193][0m |          -0.0080 |          13.5543 |           3.6523 |
[32m[20230117 13:29:49 @agent_ppo2.py:193][0m |          -0.0062 |          13.1558 |           3.6486 |
[32m[20230117 13:29:49 @agent_ppo2.py:193][0m |          -0.0081 |          12.9803 |           3.6440 |
[32m[20230117 13:29:49 @agent_ppo2.py:193][0m |          -0.0098 |          12.9276 |           3.6442 |
[32m[20230117 13:29:50 @agent_ppo2.py:193][0m |          -0.0108 |          12.6935 |           3.6425 |
[32m[20230117 13:29:50 @agent_ppo2.py:193][0m |          -0.0140 |          12.4572 |           3.6426 |
[32m[20230117 13:29:50 @agent_ppo2.py:193][0m |          -0.0120 |          12.2940 |           3.6422 |
[32m[20230117 13:29:50 @agent_ppo2.py:193][0m |          -0.0183 |          12.2761 |           3.6387 |
[32m[20230117 13:29:50 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:29:50 @agent_ppo2.py:146][0m Average TRAINING episode reward: 228.33
[32m[20230117 13:29:50 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.73
[32m[20230117 13:29:50 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 124.95
[32m[20230117 13:29:50 @agent_ppo2.py:151][0m Total time:       3.42 min
[32m[20230117 13:29:50 @agent_ppo2.py:153][0m 268288 total steps have happened
[32m[20230117 13:29:50 @agent_ppo2.py:129][0m #------------------------ Iteration 131 --------------------------#
[32m[20230117 13:29:50 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:29:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:50 @agent_ppo2.py:193][0m |          -0.0036 |          34.5795 |           3.6093 |
[32m[20230117 13:29:51 @agent_ppo2.py:193][0m |          -0.0012 |          21.9660 |           3.6015 |
[32m[20230117 13:29:51 @agent_ppo2.py:193][0m |          -0.0058 |          18.6777 |           3.6024 |
[32m[20230117 13:29:51 @agent_ppo2.py:193][0m |          -0.0072 |          17.0719 |           3.6015 |
[32m[20230117 13:29:51 @agent_ppo2.py:193][0m |          -0.0087 |          15.8992 |           3.5977 |
[32m[20230117 13:29:51 @agent_ppo2.py:193][0m |          -0.0072 |          15.4724 |           3.5990 |
[32m[20230117 13:29:51 @agent_ppo2.py:193][0m |          -0.0120 |          14.6053 |           3.5988 |
[32m[20230117 13:29:51 @agent_ppo2.py:193][0m |           0.0023 |          14.3591 |           3.5974 |
[32m[20230117 13:29:51 @agent_ppo2.py:193][0m |          -0.0121 |          14.2177 |           3.5955 |
[32m[20230117 13:29:51 @agent_ppo2.py:193][0m |          -0.0084 |          13.4699 |           3.5965 |
[32m[20230117 13:29:51 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:29:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 140.36
[32m[20230117 13:29:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 258.54
[32m[20230117 13:29:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.63
[32m[20230117 13:29:51 @agent_ppo2.py:151][0m Total time:       3.44 min
[32m[20230117 13:29:51 @agent_ppo2.py:153][0m 270336 total steps have happened
[32m[20230117 13:29:51 @agent_ppo2.py:129][0m #------------------------ Iteration 132 --------------------------#
[32m[20230117 13:29:52 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:29:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:52 @agent_ppo2.py:193][0m |          -0.0021 |          16.5481 |           3.6392 |
[32m[20230117 13:29:52 @agent_ppo2.py:193][0m |          -0.0050 |          14.6743 |           3.6355 |
[32m[20230117 13:29:52 @agent_ppo2.py:193][0m |          -0.0071 |          14.1652 |           3.6362 |
[32m[20230117 13:29:52 @agent_ppo2.py:193][0m |          -0.0070 |          13.7103 |           3.6361 |
[32m[20230117 13:29:52 @agent_ppo2.py:193][0m |          -0.0089 |          13.5582 |           3.6337 |
[32m[20230117 13:29:52 @agent_ppo2.py:193][0m |          -0.0097 |          13.3685 |           3.6401 |
[32m[20230117 13:29:52 @agent_ppo2.py:193][0m |          -0.0069 |          13.1563 |           3.6371 |
[32m[20230117 13:29:52 @agent_ppo2.py:193][0m |          -0.0229 |          12.9761 |           3.6365 |
[32m[20230117 13:29:52 @agent_ppo2.py:193][0m |          -0.0061 |          12.9963 |           3.6364 |
[32m[20230117 13:29:53 @agent_ppo2.py:193][0m |          -0.0030 |          14.2596 |           3.6388 |
[32m[20230117 13:29:53 @agent_ppo2.py:138][0m Policy update time: 0.88 s
[32m[20230117 13:29:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 252.00
[32m[20230117 13:29:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 253.76
[32m[20230117 13:29:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 92.39
[32m[20230117 13:29:53 @agent_ppo2.py:151][0m Total time:       3.46 min
[32m[20230117 13:29:53 @agent_ppo2.py:153][0m 272384 total steps have happened
[32m[20230117 13:29:53 @agent_ppo2.py:129][0m #------------------------ Iteration 133 --------------------------#
[32m[20230117 13:29:53 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:29:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:53 @agent_ppo2.py:193][0m |           0.0018 |          10.4530 |           3.6867 |
[32m[20230117 13:29:53 @agent_ppo2.py:193][0m |          -0.0035 |           8.6137 |           3.6821 |
[32m[20230117 13:29:53 @agent_ppo2.py:193][0m |          -0.0045 |           7.7342 |           3.6880 |
[32m[20230117 13:29:53 @agent_ppo2.py:193][0m |          -0.0053 |           7.1913 |           3.6868 |
[32m[20230117 13:29:53 @agent_ppo2.py:193][0m |          -0.0056 |           6.8459 |           3.6864 |
[32m[20230117 13:29:54 @agent_ppo2.py:193][0m |          -0.0072 |           6.5308 |           3.6918 |
[32m[20230117 13:29:54 @agent_ppo2.py:193][0m |          -0.0070 |           6.2718 |           3.6880 |
[32m[20230117 13:29:54 @agent_ppo2.py:193][0m |          -0.0070 |           6.0340 |           3.6894 |
[32m[20230117 13:29:54 @agent_ppo2.py:193][0m |          -0.0069 |           5.8051 |           3.6903 |
[32m[20230117 13:29:54 @agent_ppo2.py:193][0m |          -0.0054 |           5.8298 |           3.6908 |
[32m[20230117 13:29:54 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:29:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 257.83
[32m[20230117 13:29:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.81
[32m[20230117 13:29:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.26
[32m[20230117 13:29:54 @agent_ppo2.py:151][0m Total time:       3.49 min
[32m[20230117 13:29:54 @agent_ppo2.py:153][0m 274432 total steps have happened
[32m[20230117 13:29:54 @agent_ppo2.py:129][0m #------------------------ Iteration 134 --------------------------#
[32m[20230117 13:29:54 @agent_ppo2.py:135][0m Sampling time: 0.29 s by 4 slaves
[32m[20230117 13:29:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:55 @agent_ppo2.py:193][0m |          -0.0002 |          11.1109 |           3.7169 |
[32m[20230117 13:29:55 @agent_ppo2.py:193][0m |          -0.0054 |           9.2283 |           3.7155 |
[32m[20230117 13:29:55 @agent_ppo2.py:193][0m |          -0.0055 |           8.8951 |           3.7174 |
[32m[20230117 13:29:55 @agent_ppo2.py:193][0m |          -0.0062 |           8.6635 |           3.7168 |
[32m[20230117 13:29:55 @agent_ppo2.py:193][0m |          -0.0073 |           8.5094 |           3.7152 |
[32m[20230117 13:29:55 @agent_ppo2.py:193][0m |          -0.0092 |           8.3294 |           3.7163 |
[32m[20230117 13:29:55 @agent_ppo2.py:193][0m |          -0.0103 |           8.2050 |           3.7180 |
[32m[20230117 13:29:55 @agent_ppo2.py:193][0m |          -0.0101 |           8.1021 |           3.7164 |
[32m[20230117 13:29:55 @agent_ppo2.py:193][0m |          -0.0102 |           8.0211 |           3.7175 |
[32m[20230117 13:29:55 @agent_ppo2.py:193][0m |          -0.0109 |           7.8787 |           3.7173 |
[32m[20230117 13:29:55 @agent_ppo2.py:138][0m Policy update time: 0.88 s
[32m[20230117 13:29:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.03
[32m[20230117 13:29:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 256.83
[32m[20230117 13:29:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.68
[32m[20230117 13:29:55 @agent_ppo2.py:151][0m Total time:       3.51 min
[32m[20230117 13:29:55 @agent_ppo2.py:153][0m 276480 total steps have happened
[32m[20230117 13:29:55 @agent_ppo2.py:129][0m #------------------------ Iteration 135 --------------------------#
[32m[20230117 13:29:56 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:29:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:56 @agent_ppo2.py:193][0m |           0.0006 |          22.3167 |           3.6749 |
[32m[20230117 13:29:56 @agent_ppo2.py:193][0m |          -0.0041 |          15.7375 |           3.6787 |
[32m[20230117 13:29:56 @agent_ppo2.py:193][0m |          -0.0044 |          14.6639 |           3.6753 |
[32m[20230117 13:29:56 @agent_ppo2.py:193][0m |          -0.0076 |          14.2366 |           3.6737 |
[32m[20230117 13:29:56 @agent_ppo2.py:193][0m |          -0.0084 |          13.6975 |           3.6728 |
[32m[20230117 13:29:56 @agent_ppo2.py:193][0m |          -0.0101 |          13.4137 |           3.6713 |
[32m[20230117 13:29:56 @agent_ppo2.py:193][0m |          -0.0109 |          13.0948 |           3.6720 |
[32m[20230117 13:29:57 @agent_ppo2.py:193][0m |          -0.0066 |          13.1421 |           3.6689 |
[32m[20230117 13:29:57 @agent_ppo2.py:193][0m |          -0.0049 |          12.7934 |           3.6697 |
[32m[20230117 13:29:57 @agent_ppo2.py:193][0m |          -0.0071 |          13.5103 |           3.6688 |
[32m[20230117 13:29:57 @agent_ppo2.py:138][0m Policy update time: 0.93 s
[32m[20230117 13:29:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 136.37
[32m[20230117 13:29:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.85
[32m[20230117 13:29:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.60
[32m[20230117 13:29:57 @agent_ppo2.py:151][0m Total time:       3.53 min
[32m[20230117 13:29:57 @agent_ppo2.py:153][0m 278528 total steps have happened
[32m[20230117 13:29:57 @agent_ppo2.py:129][0m #------------------------ Iteration 136 --------------------------#
[32m[20230117 13:29:57 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:29:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:57 @agent_ppo2.py:193][0m |           0.0001 |          11.8263 |           3.7290 |
[32m[20230117 13:29:57 @agent_ppo2.py:193][0m |          -0.0050 |          10.9775 |           3.7262 |
[32m[20230117 13:29:57 @agent_ppo2.py:193][0m |          -0.0053 |          10.7097 |           3.7264 |
[32m[20230117 13:29:58 @agent_ppo2.py:193][0m |          -0.0073 |          10.5151 |           3.7269 |
[32m[20230117 13:29:58 @agent_ppo2.py:193][0m |          -0.0069 |          10.3546 |           3.7269 |
[32m[20230117 13:29:58 @agent_ppo2.py:193][0m |          -0.0074 |          10.2778 |           3.7260 |
[32m[20230117 13:29:58 @agent_ppo2.py:193][0m |          -0.0098 |          10.1510 |           3.7261 |
[32m[20230117 13:29:58 @agent_ppo2.py:193][0m |          -0.0100 |          10.0525 |           3.7279 |
[32m[20230117 13:29:58 @agent_ppo2.py:193][0m |          -0.0095 |           9.9834 |           3.7281 |
[32m[20230117 13:29:58 @agent_ppo2.py:193][0m |          -0.0112 |           9.8981 |           3.7292 |
[32m[20230117 13:29:58 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:29:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 257.36
[32m[20230117 13:29:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.92
[32m[20230117 13:29:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.63
[32m[20230117 13:29:58 @agent_ppo2.py:151][0m Total time:       3.55 min
[32m[20230117 13:29:58 @agent_ppo2.py:153][0m 280576 total steps have happened
[32m[20230117 13:29:58 @agent_ppo2.py:129][0m #------------------------ Iteration 137 --------------------------#
[32m[20230117 13:29:58 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:29:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:29:59 @agent_ppo2.py:193][0m |          -0.0004 |          10.2448 |           3.7987 |
[32m[20230117 13:29:59 @agent_ppo2.py:193][0m |          -0.0039 |           9.7218 |           3.7969 |
[32m[20230117 13:29:59 @agent_ppo2.py:193][0m |          -0.0054 |           9.5185 |           3.7946 |
[32m[20230117 13:29:59 @agent_ppo2.py:193][0m |          -0.0055 |           9.4320 |           3.7932 |
[32m[20230117 13:29:59 @agent_ppo2.py:193][0m |          -0.0052 |           9.3464 |           3.7914 |
[32m[20230117 13:29:59 @agent_ppo2.py:193][0m |          -0.0067 |           9.2751 |           3.7912 |
[32m[20230117 13:29:59 @agent_ppo2.py:193][0m |          -0.0070 |           9.2908 |           3.7913 |
[32m[20230117 13:29:59 @agent_ppo2.py:193][0m |          -0.0074 |           9.2074 |           3.7918 |
[32m[20230117 13:29:59 @agent_ppo2.py:193][0m |          -0.0084 |           9.1126 |           3.7907 |
[32m[20230117 13:29:59 @agent_ppo2.py:193][0m |          -0.0080 |           9.0587 |           3.7877 |
[32m[20230117 13:29:59 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:30:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 256.51
[32m[20230117 13:30:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.64
[32m[20230117 13:30:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.64
[32m[20230117 13:30:00 @agent_ppo2.py:151][0m Total time:       3.58 min
[32m[20230117 13:30:00 @agent_ppo2.py:153][0m 282624 total steps have happened
[32m[20230117 13:30:00 @agent_ppo2.py:129][0m #------------------------ Iteration 138 --------------------------#
[32m[20230117 13:30:00 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:30:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:00 @agent_ppo2.py:193][0m |          -0.0007 |          15.8335 |           3.7076 |
[32m[20230117 13:30:00 @agent_ppo2.py:193][0m |          -0.0062 |          12.4931 |           3.7049 |
[32m[20230117 13:30:00 @agent_ppo2.py:193][0m |          -0.0075 |          11.9324 |           3.7031 |
[32m[20230117 13:30:00 @agent_ppo2.py:193][0m |          -0.0068 |          11.6808 |           3.7028 |
[32m[20230117 13:30:00 @agent_ppo2.py:193][0m |          -0.0097 |          11.3151 |           3.7022 |
[32m[20230117 13:30:00 @agent_ppo2.py:193][0m |          -0.0098 |          11.2721 |           3.6999 |
[32m[20230117 13:30:00 @agent_ppo2.py:193][0m |          -0.0091 |          11.2984 |           3.6997 |
[32m[20230117 13:30:01 @agent_ppo2.py:193][0m |          -0.0100 |          10.9879 |           3.7011 |
[32m[20230117 13:30:01 @agent_ppo2.py:193][0m |          -0.0101 |          10.9770 |           3.7006 |
[32m[20230117 13:30:01 @agent_ppo2.py:193][0m |          -0.0108 |          10.9739 |           3.7012 |
[32m[20230117 13:30:01 @agent_ppo2.py:138][0m Policy update time: 0.85 s
[32m[20230117 13:30:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 184.24
[32m[20230117 13:30:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.92
[32m[20230117 13:30:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.62
[32m[20230117 13:30:01 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 276.62
[32m[20230117 13:30:01 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 276.62
[32m[20230117 13:30:01 @agent_ppo2.py:151][0m Total time:       3.60 min
[32m[20230117 13:30:01 @agent_ppo2.py:153][0m 284672 total steps have happened
[32m[20230117 13:30:01 @agent_ppo2.py:129][0m #------------------------ Iteration 139 --------------------------#
[32m[20230117 13:30:01 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:30:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:01 @agent_ppo2.py:193][0m |           0.0013 |          11.3164 |           3.7211 |
[32m[20230117 13:30:01 @agent_ppo2.py:193][0m |          -0.0043 |          10.5084 |           3.7146 |
[32m[20230117 13:30:01 @agent_ppo2.py:193][0m |          -0.0062 |          10.2919 |           3.7166 |
[32m[20230117 13:30:02 @agent_ppo2.py:193][0m |          -0.0048 |          10.2925 |           3.7152 |
[32m[20230117 13:30:02 @agent_ppo2.py:193][0m |          -0.0089 |           9.9799 |           3.7179 |
[32m[20230117 13:30:02 @agent_ppo2.py:193][0m |          -0.0088 |           9.9088 |           3.7177 |
[32m[20230117 13:30:02 @agent_ppo2.py:193][0m |          -0.0094 |           9.7752 |           3.7164 |
[32m[20230117 13:30:02 @agent_ppo2.py:193][0m |          -0.0088 |           9.7791 |           3.7166 |
[32m[20230117 13:30:02 @agent_ppo2.py:193][0m |          -0.0106 |           9.5915 |           3.7178 |
[32m[20230117 13:30:02 @agent_ppo2.py:193][0m |          -0.0115 |           9.4951 |           3.7180 |
[32m[20230117 13:30:02 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:30:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 259.32
[32m[20230117 13:30:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.41
[32m[20230117 13:30:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 280.58
[32m[20230117 13:30:02 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 280.58
[32m[20230117 13:30:02 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 280.58
[32m[20230117 13:30:02 @agent_ppo2.py:151][0m Total time:       3.62 min
[32m[20230117 13:30:02 @agent_ppo2.py:153][0m 286720 total steps have happened
[32m[20230117 13:30:02 @agent_ppo2.py:129][0m #------------------------ Iteration 140 --------------------------#
[32m[20230117 13:30:02 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:30:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:03 @agent_ppo2.py:193][0m |          -0.0005 |          20.1294 |           3.7460 |
[32m[20230117 13:30:03 @agent_ppo2.py:193][0m |          -0.0031 |          14.6444 |           3.7432 |
[32m[20230117 13:30:03 @agent_ppo2.py:193][0m |          -0.0010 |          13.2889 |           3.7425 |
[32m[20230117 13:30:03 @agent_ppo2.py:193][0m |          -0.0086 |          12.5037 |           3.7397 |
[32m[20230117 13:30:03 @agent_ppo2.py:193][0m |          -0.0093 |          12.1408 |           3.7406 |
[32m[20230117 13:30:03 @agent_ppo2.py:193][0m |          -0.0109 |          11.9260 |           3.7409 |
[32m[20230117 13:30:03 @agent_ppo2.py:193][0m |          -0.0128 |          11.6604 |           3.7403 |
[32m[20230117 13:30:03 @agent_ppo2.py:193][0m |          -0.0135 |          11.4964 |           3.7389 |
[32m[20230117 13:30:03 @agent_ppo2.py:193][0m |          -0.0113 |          11.3331 |           3.7366 |
[32m[20230117 13:30:03 @agent_ppo2.py:193][0m |          -0.0107 |          11.3342 |           3.7374 |
[32m[20230117 13:30:03 @agent_ppo2.py:138][0m Policy update time: 0.83 s
[32m[20230117 13:30:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 198.31
[32m[20230117 13:30:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.01
[32m[20230117 13:30:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.63
[32m[20230117 13:30:03 @agent_ppo2.py:151][0m Total time:       3.64 min
[32m[20230117 13:30:03 @agent_ppo2.py:153][0m 288768 total steps have happened
[32m[20230117 13:30:03 @agent_ppo2.py:129][0m #------------------------ Iteration 141 --------------------------#
[32m[20230117 13:30:04 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:30:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:04 @agent_ppo2.py:193][0m |           0.0005 |          11.7655 |           3.7767 |
[32m[20230117 13:30:04 @agent_ppo2.py:193][0m |          -0.0022 |          11.0570 |           3.7750 |
[32m[20230117 13:30:04 @agent_ppo2.py:193][0m |          -0.0034 |          10.9062 |           3.7728 |
[32m[20230117 13:30:04 @agent_ppo2.py:193][0m |          -0.0060 |          10.6446 |           3.7751 |
[32m[20230117 13:30:04 @agent_ppo2.py:193][0m |          -0.0060 |          10.5049 |           3.7693 |
[32m[20230117 13:30:04 @agent_ppo2.py:193][0m |          -0.0069 |          10.4257 |           3.7711 |
[32m[20230117 13:30:04 @agent_ppo2.py:193][0m |          -0.0088 |          10.3161 |           3.7729 |
[32m[20230117 13:30:04 @agent_ppo2.py:193][0m |          -0.0083 |          10.2857 |           3.7734 |
[32m[20230117 13:30:05 @agent_ppo2.py:193][0m |          -0.0095 |          10.1557 |           3.7770 |
[32m[20230117 13:30:05 @agent_ppo2.py:193][0m |          -0.0098 |          10.1074 |           3.7743 |
[32m[20230117 13:30:05 @agent_ppo2.py:138][0m Policy update time: 0.90 s
[32m[20230117 13:30:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.45
[32m[20230117 13:30:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 258.82
[32m[20230117 13:30:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 279.99
[32m[20230117 13:30:05 @agent_ppo2.py:151][0m Total time:       3.66 min
[32m[20230117 13:30:05 @agent_ppo2.py:153][0m 290816 total steps have happened
[32m[20230117 13:30:05 @agent_ppo2.py:129][0m #------------------------ Iteration 142 --------------------------#
[32m[20230117 13:30:05 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:30:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:05 @agent_ppo2.py:193][0m |          -0.0001 |          16.0438 |           3.7976 |
[32m[20230117 13:30:05 @agent_ppo2.py:193][0m |          -0.0048 |          11.7239 |           3.7955 |
[32m[20230117 13:30:05 @agent_ppo2.py:193][0m |          -0.0067 |          10.5663 |           3.7927 |
[32m[20230117 13:30:05 @agent_ppo2.py:193][0m |          -0.0078 |           9.9257 |           3.7899 |
[32m[20230117 13:30:06 @agent_ppo2.py:193][0m |          -0.0085 |           9.4163 |           3.7904 |
[32m[20230117 13:30:06 @agent_ppo2.py:193][0m |          -0.0094 |           9.2519 |           3.7864 |
[32m[20230117 13:30:06 @agent_ppo2.py:193][0m |          -0.0100 |           8.8587 |           3.7832 |
[32m[20230117 13:30:06 @agent_ppo2.py:193][0m |          -0.0106 |           8.6718 |           3.7822 |
[32m[20230117 13:30:06 @agent_ppo2.py:193][0m |          -0.0110 |           8.3597 |           3.7803 |
[32m[20230117 13:30:06 @agent_ppo2.py:193][0m |          -0.0117 |           8.2284 |           3.7785 |
[32m[20230117 13:30:06 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:30:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 230.13
[32m[20230117 13:30:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.60
[32m[20230117 13:30:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.62
[32m[20230117 13:30:06 @agent_ppo2.py:151][0m Total time:       3.69 min
[32m[20230117 13:30:06 @agent_ppo2.py:153][0m 292864 total steps have happened
[32m[20230117 13:30:06 @agent_ppo2.py:129][0m #------------------------ Iteration 143 --------------------------#
[32m[20230117 13:30:06 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:30:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:07 @agent_ppo2.py:193][0m |           0.0002 |          12.7225 |           3.8077 |
[32m[20230117 13:30:07 @agent_ppo2.py:193][0m |          -0.0041 |          12.0199 |           3.7981 |
[32m[20230117 13:30:07 @agent_ppo2.py:193][0m |          -0.0053 |          11.7155 |           3.7998 |
[32m[20230117 13:30:07 @agent_ppo2.py:193][0m |          -0.0056 |          11.5410 |           3.7934 |
[32m[20230117 13:30:07 @agent_ppo2.py:193][0m |          -0.0065 |          11.4783 |           3.7980 |
[32m[20230117 13:30:07 @agent_ppo2.py:193][0m |          -0.0060 |          11.4392 |           3.7986 |
[32m[20230117 13:30:07 @agent_ppo2.py:193][0m |          -0.0077 |          11.2418 |           3.7968 |
[32m[20230117 13:30:07 @agent_ppo2.py:193][0m |          -0.0063 |          11.4496 |           3.7934 |
[32m[20230117 13:30:07 @agent_ppo2.py:193][0m |          -0.0080 |          11.1078 |           3.7977 |
[32m[20230117 13:30:07 @agent_ppo2.py:193][0m |          -0.0087 |          11.0083 |           3.7952 |
[32m[20230117 13:30:07 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:30:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.56
[32m[20230117 13:30:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.36
[32m[20230117 13:30:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 284.92
[32m[20230117 13:30:08 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 284.92
[32m[20230117 13:30:08 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 284.92
[32m[20230117 13:30:08 @agent_ppo2.py:151][0m Total time:       3.71 min
[32m[20230117 13:30:08 @agent_ppo2.py:153][0m 294912 total steps have happened
[32m[20230117 13:30:08 @agent_ppo2.py:129][0m #------------------------ Iteration 144 --------------------------#
[32m[20230117 13:30:08 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:30:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:08 @agent_ppo2.py:193][0m |          -0.0023 |          11.7825 |           3.7357 |
[32m[20230117 13:30:08 @agent_ppo2.py:193][0m |          -0.0067 |          11.1904 |           3.7299 |
[32m[20230117 13:30:08 @agent_ppo2.py:193][0m |          -0.0053 |          11.2898 |           3.7238 |
[32m[20230117 13:30:08 @agent_ppo2.py:193][0m |          -0.0064 |          10.8397 |           3.7250 |
[32m[20230117 13:30:08 @agent_ppo2.py:193][0m |          -0.0080 |          10.7265 |           3.7249 |
[32m[20230117 13:30:08 @agent_ppo2.py:193][0m |          -0.0088 |          10.6241 |           3.7210 |
[32m[20230117 13:30:08 @agent_ppo2.py:193][0m |          -0.0083 |          10.5667 |           3.7202 |
[32m[20230117 13:30:09 @agent_ppo2.py:193][0m |          -0.0103 |          10.4525 |           3.7182 |
[32m[20230117 13:30:09 @agent_ppo2.py:193][0m |          -0.0103 |          10.3540 |           3.7194 |
[32m[20230117 13:30:09 @agent_ppo2.py:193][0m |          -0.0117 |          10.2849 |           3.7155 |
[32m[20230117 13:30:09 @agent_ppo2.py:138][0m Policy update time: 0.89 s
[32m[20230117 13:30:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 256.11
[32m[20230117 13:30:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.47
[32m[20230117 13:30:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.99
[32m[20230117 13:30:09 @agent_ppo2.py:151][0m Total time:       3.73 min
[32m[20230117 13:30:09 @agent_ppo2.py:153][0m 296960 total steps have happened
[32m[20230117 13:30:09 @agent_ppo2.py:129][0m #------------------------ Iteration 145 --------------------------#
[32m[20230117 13:30:09 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:30:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:09 @agent_ppo2.py:193][0m |          -0.0029 |          11.1439 |           3.6650 |
[32m[20230117 13:30:09 @agent_ppo2.py:193][0m |          -0.0044 |          10.7466 |           3.6606 |
[32m[20230117 13:30:09 @agent_ppo2.py:193][0m |          -0.0091 |          10.5394 |           3.6611 |
[32m[20230117 13:30:10 @agent_ppo2.py:193][0m |          -0.0030 |          10.3816 |           3.6600 |
[32m[20230117 13:30:10 @agent_ppo2.py:193][0m |          -0.0081 |          10.2361 |           3.6589 |
[32m[20230117 13:30:10 @agent_ppo2.py:193][0m |          -0.0090 |          10.0711 |           3.6620 |
[32m[20230117 13:30:10 @agent_ppo2.py:193][0m |          -0.0087 |           9.9525 |           3.6576 |
[32m[20230117 13:30:10 @agent_ppo2.py:193][0m |          -0.0042 |           9.8615 |           3.6614 |
[32m[20230117 13:30:10 @agent_ppo2.py:193][0m |          -0.0079 |           9.7558 |           3.6616 |
[32m[20230117 13:30:10 @agent_ppo2.py:193][0m |          -0.0109 |           9.6791 |           3.6634 |
[32m[20230117 13:30:10 @agent_ppo2.py:138][0m Policy update time: 0.89 s
[32m[20230117 13:30:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 254.30
[32m[20230117 13:30:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 258.19
[32m[20230117 13:30:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.84
[32m[20230117 13:30:10 @agent_ppo2.py:151][0m Total time:       3.75 min
[32m[20230117 13:30:10 @agent_ppo2.py:153][0m 299008 total steps have happened
[32m[20230117 13:30:10 @agent_ppo2.py:129][0m #------------------------ Iteration 146 --------------------------#
[32m[20230117 13:30:11 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:30:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:11 @agent_ppo2.py:193][0m |          -0.0016 |          11.2637 |           3.7809 |
[32m[20230117 13:30:11 @agent_ppo2.py:193][0m |          -0.0046 |          10.6282 |           3.7731 |
[32m[20230117 13:30:11 @agent_ppo2.py:193][0m |          -0.0072 |          10.3525 |           3.7716 |
[32m[20230117 13:30:11 @agent_ppo2.py:193][0m |          -0.0081 |          10.1564 |           3.7721 |
[32m[20230117 13:30:11 @agent_ppo2.py:193][0m |          -0.0083 |           9.9882 |           3.7732 |
[32m[20230117 13:30:11 @agent_ppo2.py:193][0m |          -0.0089 |           9.8572 |           3.7734 |
[32m[20230117 13:30:11 @agent_ppo2.py:193][0m |          -0.0092 |           9.7402 |           3.7743 |
[32m[20230117 13:30:11 @agent_ppo2.py:193][0m |          -0.0099 |           9.6258 |           3.7752 |
[32m[20230117 13:30:11 @agent_ppo2.py:193][0m |          -0.0106 |           9.5386 |           3.7735 |
[32m[20230117 13:30:11 @agent_ppo2.py:193][0m |          -0.0108 |           9.4612 |           3.7761 |
[32m[20230117 13:30:11 @agent_ppo2.py:138][0m Policy update time: 0.90 s
[32m[20230117 13:30:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 253.20
[32m[20230117 13:30:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 257.28
[32m[20230117 13:30:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 279.09
[32m[20230117 13:30:12 @agent_ppo2.py:151][0m Total time:       3.78 min
[32m[20230117 13:30:12 @agent_ppo2.py:153][0m 301056 total steps have happened
[32m[20230117 13:30:12 @agent_ppo2.py:129][0m #------------------------ Iteration 147 --------------------------#
[32m[20230117 13:30:12 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:30:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:12 @agent_ppo2.py:193][0m |          -0.0013 |          19.6656 |           3.8051 |
[32m[20230117 13:30:12 @agent_ppo2.py:193][0m |          -0.0058 |          13.0567 |           3.8007 |
[32m[20230117 13:30:12 @agent_ppo2.py:193][0m |          -0.0064 |          11.9722 |           3.7995 |
[32m[20230117 13:30:12 @agent_ppo2.py:193][0m |          -0.0083 |          11.3438 |           3.7989 |
[32m[20230117 13:30:12 @agent_ppo2.py:193][0m |          -0.0096 |          10.9557 |           3.7995 |
[32m[20230117 13:30:12 @agent_ppo2.py:193][0m |          -0.0112 |          10.6845 |           3.7946 |
[32m[20230117 13:30:13 @agent_ppo2.py:193][0m |          -0.0110 |          10.7699 |           3.7938 |
[32m[20230117 13:30:13 @agent_ppo2.py:193][0m |          -0.0119 |          10.3046 |           3.7918 |
[32m[20230117 13:30:13 @agent_ppo2.py:193][0m |          -0.0120 |          10.3155 |           3.7904 |
[32m[20230117 13:30:13 @agent_ppo2.py:193][0m |          -0.0123 |          10.0887 |           3.7882 |
[32m[20230117 13:30:13 @agent_ppo2.py:138][0m Policy update time: 0.82 s
[32m[20230117 13:30:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 131.20
[32m[20230117 13:30:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.57
[32m[20230117 13:30:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 280.00
[32m[20230117 13:30:13 @agent_ppo2.py:151][0m Total time:       3.80 min
[32m[20230117 13:30:13 @agent_ppo2.py:153][0m 303104 total steps have happened
[32m[20230117 13:30:13 @agent_ppo2.py:129][0m #------------------------ Iteration 148 --------------------------#
[32m[20230117 13:30:13 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:30:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:13 @agent_ppo2.py:193][0m |           0.0000 |          11.3357 |           3.7559 |
[32m[20230117 13:30:13 @agent_ppo2.py:193][0m |          -0.0034 |          10.7858 |           3.7496 |
[32m[20230117 13:30:14 @agent_ppo2.py:193][0m |          -0.0028 |          10.6163 |           3.7503 |
[32m[20230117 13:30:14 @agent_ppo2.py:193][0m |          -0.0058 |          10.4249 |           3.7495 |
[32m[20230117 13:30:14 @agent_ppo2.py:193][0m |          -0.0068 |          10.2664 |           3.7474 |
[32m[20230117 13:30:14 @agent_ppo2.py:193][0m |          -0.0056 |          10.2250 |           3.7475 |
[32m[20230117 13:30:14 @agent_ppo2.py:193][0m |          -0.0073 |          10.0311 |           3.7464 |
[32m[20230117 13:30:14 @agent_ppo2.py:193][0m |          -0.0091 |           9.9209 |           3.7448 |
[32m[20230117 13:30:14 @agent_ppo2.py:193][0m |          -0.0090 |           9.8317 |           3.7446 |
[32m[20230117 13:30:14 @agent_ppo2.py:193][0m |          -0.0038 |          10.4110 |           3.7440 |
[32m[20230117 13:30:14 @agent_ppo2.py:138][0m Policy update time: 0.88 s
[32m[20230117 13:30:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 257.64
[32m[20230117 13:30:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.79
[32m[20230117 13:30:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 281.12
[32m[20230117 13:30:14 @agent_ppo2.py:151][0m Total time:       3.82 min
[32m[20230117 13:30:14 @agent_ppo2.py:153][0m 305152 total steps have happened
[32m[20230117 13:30:14 @agent_ppo2.py:129][0m #------------------------ Iteration 149 --------------------------#
[32m[20230117 13:30:15 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:30:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:15 @agent_ppo2.py:193][0m |          -0.0008 |          11.8277 |           3.7709 |
[32m[20230117 13:30:15 @agent_ppo2.py:193][0m |          -0.0050 |          11.2073 |           3.7684 |
[32m[20230117 13:30:15 @agent_ppo2.py:193][0m |          -0.0068 |          10.9294 |           3.7690 |
[32m[20230117 13:30:15 @agent_ppo2.py:193][0m |          -0.0073 |          10.7727 |           3.7717 |
[32m[20230117 13:30:15 @agent_ppo2.py:193][0m |          -0.0083 |          10.6466 |           3.7757 |
[32m[20230117 13:30:15 @agent_ppo2.py:193][0m |          -0.0089 |          10.5218 |           3.7737 |
[32m[20230117 13:30:15 @agent_ppo2.py:193][0m |          -0.0096 |          10.4672 |           3.7748 |
[32m[20230117 13:30:15 @agent_ppo2.py:193][0m |          -0.0104 |          10.3764 |           3.7781 |
[32m[20230117 13:30:15 @agent_ppo2.py:193][0m |          -0.0108 |          10.2823 |           3.7784 |
[32m[20230117 13:30:15 @agent_ppo2.py:193][0m |          -0.0111 |          10.2527 |           3.7761 |
[32m[20230117 13:30:15 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:30:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 256.81
[32m[20230117 13:30:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.75
[32m[20230117 13:30:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.22
[32m[20230117 13:30:16 @agent_ppo2.py:151][0m Total time:       3.84 min
[32m[20230117 13:30:16 @agent_ppo2.py:153][0m 307200 total steps have happened
[32m[20230117 13:30:16 @agent_ppo2.py:129][0m #------------------------ Iteration 150 --------------------------#
[32m[20230117 13:30:16 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:30:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:16 @agent_ppo2.py:193][0m |           0.0017 |          12.1510 |           3.8030 |
[32m[20230117 13:30:16 @agent_ppo2.py:193][0m |          -0.0015 |          11.1444 |           3.7994 |
[32m[20230117 13:30:16 @agent_ppo2.py:193][0m |          -0.0063 |          10.1798 |           3.7989 |
[32m[20230117 13:30:16 @agent_ppo2.py:193][0m |          -0.0086 |           9.8976 |           3.7978 |
[32m[20230117 13:30:16 @agent_ppo2.py:193][0m |          -0.0078 |           9.6694 |           3.8012 |
[32m[20230117 13:30:16 @agent_ppo2.py:193][0m |          -0.0111 |           9.5056 |           3.8001 |
[32m[20230117 13:30:17 @agent_ppo2.py:193][0m |          -0.0109 |           9.3430 |           3.8032 |
[32m[20230117 13:30:17 @agent_ppo2.py:193][0m |          -0.0116 |           9.1778 |           3.8001 |
[32m[20230117 13:30:17 @agent_ppo2.py:193][0m |          -0.0120 |           9.0228 |           3.8020 |
[32m[20230117 13:30:17 @agent_ppo2.py:193][0m |          -0.0130 |           8.8796 |           3.8028 |
[32m[20230117 13:30:17 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:30:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.46
[32m[20230117 13:30:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.47
[32m[20230117 13:30:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 282.39
[32m[20230117 13:30:17 @agent_ppo2.py:151][0m Total time:       3.87 min
[32m[20230117 13:30:17 @agent_ppo2.py:153][0m 309248 total steps have happened
[32m[20230117 13:30:17 @agent_ppo2.py:129][0m #------------------------ Iteration 151 --------------------------#
[32m[20230117 13:30:17 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:30:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:17 @agent_ppo2.py:193][0m |           0.0121 |          16.3534 |           3.8435 |
[32m[20230117 13:30:17 @agent_ppo2.py:193][0m |          -0.0064 |          14.0558 |           3.8386 |
[32m[20230117 13:30:18 @agent_ppo2.py:193][0m |          -0.0092 |          13.4735 |           3.8402 |
[32m[20230117 13:30:18 @agent_ppo2.py:193][0m |          -0.0063 |          13.0328 |           3.8364 |
[32m[20230117 13:30:18 @agent_ppo2.py:193][0m |          -0.0089 |          12.6708 |           3.8364 |
[32m[20230117 13:30:18 @agent_ppo2.py:193][0m |          -0.0105 |          12.3780 |           3.8375 |
[32m[20230117 13:30:18 @agent_ppo2.py:193][0m |          -0.0066 |          12.5759 |           3.8362 |
[32m[20230117 13:30:18 @agent_ppo2.py:193][0m |          -0.0115 |          12.1305 |           3.8366 |
[32m[20230117 13:30:18 @agent_ppo2.py:193][0m |          -0.0123 |          11.9739 |           3.8344 |
[32m[20230117 13:30:18 @agent_ppo2.py:193][0m |          -0.0106 |          11.7387 |           3.8371 |
[32m[20230117 13:30:18 @agent_ppo2.py:138][0m Policy update time: 0.91 s
[32m[20230117 13:30:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: 190.22
[32m[20230117 13:30:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.48
[32m[20230117 13:30:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 283.30
[32m[20230117 13:30:18 @agent_ppo2.py:151][0m Total time:       3.89 min
[32m[20230117 13:30:18 @agent_ppo2.py:153][0m 311296 total steps have happened
[32m[20230117 13:30:18 @agent_ppo2.py:129][0m #------------------------ Iteration 152 --------------------------#
[32m[20230117 13:30:19 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:30:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:19 @agent_ppo2.py:193][0m |           0.0003 |          12.2134 |           3.8924 |
[32m[20230117 13:30:19 @agent_ppo2.py:193][0m |          -0.0022 |          11.6232 |           3.8957 |
[32m[20230117 13:30:19 @agent_ppo2.py:193][0m |          -0.0038 |          11.3136 |           3.8919 |
[32m[20230117 13:30:19 @agent_ppo2.py:193][0m |          -0.0046 |          11.1236 |           3.8953 |
[32m[20230117 13:30:19 @agent_ppo2.py:193][0m |          -0.0057 |          10.9940 |           3.8985 |
[32m[20230117 13:30:19 @agent_ppo2.py:193][0m |          -0.0056 |          10.9003 |           3.8976 |
[32m[20230117 13:30:19 @agent_ppo2.py:193][0m |          -0.0065 |          10.7903 |           3.8972 |
[32m[20230117 13:30:19 @agent_ppo2.py:193][0m |          -0.0071 |          10.7024 |           3.9009 |
[32m[20230117 13:30:19 @agent_ppo2.py:193][0m |          -0.0075 |          10.6369 |           3.9011 |
[32m[20230117 13:30:19 @agent_ppo2.py:193][0m |          -0.0080 |          10.5715 |           3.9055 |
[32m[20230117 13:30:19 @agent_ppo2.py:138][0m Policy update time: 0.91 s
[32m[20230117 13:30:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 257.49
[32m[20230117 13:30:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.58
[32m[20230117 13:30:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 282.46
[32m[20230117 13:30:20 @agent_ppo2.py:151][0m Total time:       3.91 min
[32m[20230117 13:30:20 @agent_ppo2.py:153][0m 313344 total steps have happened
[32m[20230117 13:30:20 @agent_ppo2.py:129][0m #------------------------ Iteration 153 --------------------------#
[32m[20230117 13:30:20 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:30:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:20 @agent_ppo2.py:193][0m |          -0.0032 |          29.9367 |           3.8751 |
[32m[20230117 13:30:20 @agent_ppo2.py:193][0m |          -0.0086 |          21.1263 |           3.8729 |
[32m[20230117 13:30:20 @agent_ppo2.py:193][0m |          -0.0101 |          19.8287 |           3.8749 |
[32m[20230117 13:30:20 @agent_ppo2.py:193][0m |          -0.0111 |          18.8783 |           3.8748 |
[32m[20230117 13:30:21 @agent_ppo2.py:193][0m |          -0.0103 |          18.8016 |           3.8746 |
[32m[20230117 13:30:21 @agent_ppo2.py:193][0m |          -0.0097 |          18.1253 |           3.8771 |
[32m[20230117 13:30:21 @agent_ppo2.py:193][0m |          -0.0113 |          17.8063 |           3.8758 |
[32m[20230117 13:30:21 @agent_ppo2.py:193][0m |          -0.0115 |          17.5770 |           3.8805 |
[32m[20230117 13:30:21 @agent_ppo2.py:193][0m |          -0.0130 |          17.2004 |           3.8779 |
[32m[20230117 13:30:21 @agent_ppo2.py:193][0m |          -0.0134 |          17.2602 |           3.8810 |
[32m[20230117 13:30:21 @agent_ppo2.py:138][0m Policy update time: 0.93 s
[32m[20230117 13:30:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 190.92
[32m[20230117 13:30:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.53
[32m[20230117 13:30:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 148.02
[32m[20230117 13:30:21 @agent_ppo2.py:151][0m Total time:       3.94 min
[32m[20230117 13:30:21 @agent_ppo2.py:153][0m 315392 total steps have happened
[32m[20230117 13:30:21 @agent_ppo2.py:129][0m #------------------------ Iteration 154 --------------------------#
[32m[20230117 13:30:21 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:30:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:22 @agent_ppo2.py:193][0m |          -0.0014 |          13.1270 |           3.8395 |
[32m[20230117 13:30:22 @agent_ppo2.py:193][0m |          -0.0038 |          11.4941 |           3.8407 |
[32m[20230117 13:30:22 @agent_ppo2.py:193][0m |          -0.0059 |          11.0274 |           3.8393 |
[32m[20230117 13:30:22 @agent_ppo2.py:193][0m |          -0.0064 |          10.8965 |           3.8400 |
[32m[20230117 13:30:22 @agent_ppo2.py:193][0m |          -0.0089 |          10.6835 |           3.8416 |
[32m[20230117 13:30:22 @agent_ppo2.py:193][0m |          -0.0078 |          10.6240 |           3.8432 |
[32m[20230117 13:30:22 @agent_ppo2.py:193][0m |          -0.0079 |          10.5056 |           3.8430 |
[32m[20230117 13:30:22 @agent_ppo2.py:193][0m |          -0.0091 |          10.4439 |           3.8461 |
[32m[20230117 13:30:22 @agent_ppo2.py:193][0m |          -0.0072 |          10.7064 |           3.8444 |
[32m[20230117 13:30:22 @agent_ppo2.py:193][0m |          -0.0095 |          10.2966 |           3.8490 |
[32m[20230117 13:30:22 @agent_ppo2.py:138][0m Policy update time: 0.98 s
[32m[20230117 13:30:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: 254.59
[32m[20230117 13:30:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.43
[32m[20230117 13:30:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 280.76
[32m[20230117 13:30:23 @agent_ppo2.py:151][0m Total time:       3.96 min
[32m[20230117 13:30:23 @agent_ppo2.py:153][0m 317440 total steps have happened
[32m[20230117 13:30:23 @agent_ppo2.py:129][0m #------------------------ Iteration 155 --------------------------#
[32m[20230117 13:30:23 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:30:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:23 @agent_ppo2.py:193][0m |          -0.0017 |          11.6730 |           3.9432 |
[32m[20230117 13:30:23 @agent_ppo2.py:193][0m |          -0.0048 |          11.1653 |           3.9374 |
[32m[20230117 13:30:23 @agent_ppo2.py:193][0m |          -0.0055 |          10.9462 |           3.9327 |
[32m[20230117 13:30:23 @agent_ppo2.py:193][0m |          -0.0082 |          10.6894 |           3.9335 |
[32m[20230117 13:30:23 @agent_ppo2.py:193][0m |          -0.0092 |          10.5312 |           3.9310 |
[32m[20230117 13:30:23 @agent_ppo2.py:193][0m |          -0.0104 |          10.4055 |           3.9288 |
[32m[20230117 13:30:24 @agent_ppo2.py:193][0m |          -0.0094 |          10.3049 |           3.9298 |
[32m[20230117 13:30:24 @agent_ppo2.py:193][0m |          -0.0082 |          10.2135 |           3.9279 |
[32m[20230117 13:30:24 @agent_ppo2.py:193][0m |          -0.0104 |          10.1285 |           3.9298 |
[32m[20230117 13:30:24 @agent_ppo2.py:193][0m |          -0.0083 |          10.0880 |           3.9262 |
[32m[20230117 13:30:24 @agent_ppo2.py:138][0m Policy update time: 0.96 s
[32m[20230117 13:30:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 251.12
[32m[20230117 13:30:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 254.17
[32m[20230117 13:30:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.53
[32m[20230117 13:30:24 @agent_ppo2.py:151][0m Total time:       3.98 min
[32m[20230117 13:30:24 @agent_ppo2.py:153][0m 319488 total steps have happened
[32m[20230117 13:30:24 @agent_ppo2.py:129][0m #------------------------ Iteration 156 --------------------------#
[32m[20230117 13:30:24 @agent_ppo2.py:135][0m Sampling time: 0.29 s by 4 slaves
[32m[20230117 13:30:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:24 @agent_ppo2.py:193][0m |          -0.0009 |          10.3829 |           3.9530 |
[32m[20230117 13:30:25 @agent_ppo2.py:193][0m |          -0.0038 |          10.0192 |           3.9561 |
[32m[20230117 13:30:25 @agent_ppo2.py:193][0m |          -0.0048 |           9.8633 |           3.9506 |
[32m[20230117 13:30:25 @agent_ppo2.py:193][0m |          -0.0089 |           9.6220 |           3.9495 |
[32m[20230117 13:30:25 @agent_ppo2.py:193][0m |          -0.0074 |           9.5070 |           3.9520 |
[32m[20230117 13:30:25 @agent_ppo2.py:193][0m |          -0.0073 |           9.4579 |           3.9528 |
[32m[20230117 13:30:25 @agent_ppo2.py:193][0m |          -0.0097 |           9.3031 |           3.9544 |
[32m[20230117 13:30:25 @agent_ppo2.py:193][0m |          -0.0091 |           9.2284 |           3.9523 |
[32m[20230117 13:30:25 @agent_ppo2.py:193][0m |          -0.0095 |           9.1803 |           3.9565 |
[32m[20230117 13:30:25 @agent_ppo2.py:193][0m |          -0.0110 |           9.1333 |           3.9571 |
[32m[20230117 13:30:25 @agent_ppo2.py:138][0m Policy update time: 0.99 s
[32m[20230117 13:30:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 255.49
[32m[20230117 13:30:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.16
[32m[20230117 13:30:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.67
[32m[20230117 13:30:26 @agent_ppo2.py:151][0m Total time:       4.01 min
[32m[20230117 13:30:26 @agent_ppo2.py:153][0m 321536 total steps have happened
[32m[20230117 13:30:26 @agent_ppo2.py:129][0m #------------------------ Iteration 157 --------------------------#
[32m[20230117 13:30:26 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:30:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:26 @agent_ppo2.py:193][0m |           0.0040 |          11.1438 |           3.9640 |
[32m[20230117 13:30:26 @agent_ppo2.py:193][0m |          -0.0074 |          10.3174 |           3.9595 |
[32m[20230117 13:30:26 @agent_ppo2.py:193][0m |          -0.0035 |          10.4489 |           3.9554 |
[32m[20230117 13:30:26 @agent_ppo2.py:193][0m |          -0.0063 |           9.9191 |           3.9528 |
[32m[20230117 13:30:26 @agent_ppo2.py:193][0m |          -0.0090 |           9.7990 |           3.9516 |
[32m[20230117 13:30:26 @agent_ppo2.py:193][0m |          -0.0041 |           9.8532 |           3.9484 |
[32m[20230117 13:30:27 @agent_ppo2.py:193][0m |          -0.0085 |           9.5397 |           3.9469 |
[32m[20230117 13:30:27 @agent_ppo2.py:193][0m |          -0.0097 |           9.4506 |           3.9451 |
[32m[20230117 13:30:27 @agent_ppo2.py:193][0m |          -0.0117 |           9.2808 |           3.9426 |
[32m[20230117 13:30:27 @agent_ppo2.py:193][0m |          -0.0117 |           9.1768 |           3.9408 |
[32m[20230117 13:30:27 @agent_ppo2.py:138][0m Policy update time: 0.96 s
[32m[20230117 13:30:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 250.67
[32m[20230117 13:30:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.84
[32m[20230117 13:30:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.26
[32m[20230117 13:30:27 @agent_ppo2.py:151][0m Total time:       4.03 min
[32m[20230117 13:30:27 @agent_ppo2.py:153][0m 323584 total steps have happened
[32m[20230117 13:30:27 @agent_ppo2.py:129][0m #------------------------ Iteration 158 --------------------------#
[32m[20230117 13:30:27 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:30:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:27 @agent_ppo2.py:193][0m |          -0.0007 |          11.8687 |           3.9401 |
[32m[20230117 13:30:27 @agent_ppo2.py:193][0m |          -0.0054 |          11.3552 |           3.9226 |
[32m[20230117 13:30:28 @agent_ppo2.py:193][0m |          -0.0067 |          11.0621 |           3.9222 |
[32m[20230117 13:30:28 @agent_ppo2.py:193][0m |          -0.0078 |          10.8917 |           3.9199 |
[32m[20230117 13:30:28 @agent_ppo2.py:193][0m |          -0.0082 |          10.7421 |           3.9122 |
[32m[20230117 13:30:28 @agent_ppo2.py:193][0m |          -0.0096 |          10.5870 |           3.9111 |
[32m[20230117 13:30:28 @agent_ppo2.py:193][0m |          -0.0098 |          10.4461 |           3.9147 |
[32m[20230117 13:30:28 @agent_ppo2.py:193][0m |          -0.0106 |          10.3395 |           3.9086 |
[32m[20230117 13:30:28 @agent_ppo2.py:193][0m |          -0.0100 |          10.2723 |           3.9106 |
[32m[20230117 13:30:28 @agent_ppo2.py:193][0m |          -0.0111 |          10.1521 |           3.9109 |
[32m[20230117 13:30:28 @agent_ppo2.py:138][0m Policy update time: 0.97 s
[32m[20230117 13:30:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 255.76
[32m[20230117 13:30:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.37
[32m[20230117 13:30:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 281.10
[32m[20230117 13:30:28 @agent_ppo2.py:151][0m Total time:       4.06 min
[32m[20230117 13:30:28 @agent_ppo2.py:153][0m 325632 total steps have happened
[32m[20230117 13:30:28 @agent_ppo2.py:129][0m #------------------------ Iteration 159 --------------------------#
[32m[20230117 13:30:29 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:30:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:29 @agent_ppo2.py:193][0m |          -0.0038 |          11.8978 |           3.9091 |
[32m[20230117 13:30:29 @agent_ppo2.py:193][0m |          -0.0061 |          11.2587 |           3.9008 |
[32m[20230117 13:30:29 @agent_ppo2.py:193][0m |          -0.0058 |          10.9913 |           3.9012 |
[32m[20230117 13:30:29 @agent_ppo2.py:193][0m |          -0.0104 |          10.7502 |           3.8993 |
[32m[20230117 13:30:29 @agent_ppo2.py:193][0m |          -0.0106 |          10.6203 |           3.9002 |
[32m[20230117 13:30:29 @agent_ppo2.py:193][0m |          -0.0063 |          10.4538 |           3.8939 |
[32m[20230117 13:30:29 @agent_ppo2.py:193][0m |          -0.0095 |          10.3073 |           3.8940 |
[32m[20230117 13:30:29 @agent_ppo2.py:193][0m |           0.0064 |          12.0835 |           3.8961 |
[32m[20230117 13:30:30 @agent_ppo2.py:193][0m |          -0.0113 |          10.1769 |           3.8884 |
[32m[20230117 13:30:30 @agent_ppo2.py:193][0m |          -0.0117 |          10.0221 |           3.8932 |
[32m[20230117 13:30:30 @agent_ppo2.py:138][0m Policy update time: 0.93 s
[32m[20230117 13:30:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 255.17
[32m[20230117 13:30:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 259.07
[32m[20230117 13:30:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 282.07
[32m[20230117 13:30:30 @agent_ppo2.py:151][0m Total time:       4.08 min
[32m[20230117 13:30:30 @agent_ppo2.py:153][0m 327680 total steps have happened
[32m[20230117 13:30:30 @agent_ppo2.py:129][0m #------------------------ Iteration 160 --------------------------#
[32m[20230117 13:30:30 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:30:30 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:30 @agent_ppo2.py:193][0m |           0.0013 |          11.1519 |           3.7887 |
[32m[20230117 13:30:30 @agent_ppo2.py:193][0m |          -0.0058 |          10.6113 |           3.7821 |
[32m[20230117 13:30:30 @agent_ppo2.py:193][0m |          -0.0063 |          10.2894 |           3.7854 |
[32m[20230117 13:30:31 @agent_ppo2.py:193][0m |          -0.0072 |          10.0815 |           3.7821 |
[32m[20230117 13:30:31 @agent_ppo2.py:193][0m |          -0.0100 |           9.9275 |           3.7860 |
[32m[20230117 13:30:31 @agent_ppo2.py:193][0m |          -0.0061 |           9.7665 |           3.7831 |
[32m[20230117 13:30:31 @agent_ppo2.py:193][0m |          -0.0087 |           9.6728 |           3.7880 |
[32m[20230117 13:30:31 @agent_ppo2.py:193][0m |          -0.0065 |           9.6092 |           3.7877 |
[32m[20230117 13:30:31 @agent_ppo2.py:193][0m |          -0.0029 |           9.6188 |           3.7860 |
[32m[20230117 13:30:31 @agent_ppo2.py:193][0m |          -0.0092 |           9.3220 |           3.7911 |
[32m[20230117 13:30:31 @agent_ppo2.py:138][0m Policy update time: 0.97 s
[32m[20230117 13:30:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 257.53
[32m[20230117 13:30:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.18
[32m[20230117 13:30:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.04
[32m[20230117 13:30:31 @agent_ppo2.py:151][0m Total time:       4.10 min
[32m[20230117 13:30:31 @agent_ppo2.py:153][0m 329728 total steps have happened
[32m[20230117 13:30:31 @agent_ppo2.py:129][0m #------------------------ Iteration 161 --------------------------#
[32m[20230117 13:30:32 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:30:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:32 @agent_ppo2.py:193][0m |           0.0022 |          22.1183 |           3.8641 |
[32m[20230117 13:30:32 @agent_ppo2.py:193][0m |           0.0001 |          15.5111 |           3.8573 |
[32m[20230117 13:30:32 @agent_ppo2.py:193][0m |          -0.0048 |          14.5994 |           3.8558 |
[32m[20230117 13:30:32 @agent_ppo2.py:193][0m |          -0.0052 |          13.7541 |           3.8521 |
[32m[20230117 13:30:32 @agent_ppo2.py:193][0m |          -0.0068 |          13.2402 |           3.8498 |
[32m[20230117 13:30:32 @agent_ppo2.py:193][0m |          -0.0079 |          12.8954 |           3.8491 |
[32m[20230117 13:30:32 @agent_ppo2.py:193][0m |          -0.0104 |          12.5870 |           3.8489 |
[32m[20230117 13:30:32 @agent_ppo2.py:193][0m |          -0.0109 |          12.3036 |           3.8484 |
[32m[20230117 13:30:32 @agent_ppo2.py:193][0m |          -0.0082 |          12.1079 |           3.8458 |
[32m[20230117 13:30:32 @agent_ppo2.py:193][0m |          -0.0078 |          11.9774 |           3.8483 |
[32m[20230117 13:30:32 @agent_ppo2.py:138][0m Policy update time: 0.89 s
[32m[20230117 13:30:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 110.20
[32m[20230117 13:30:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.48
[32m[20230117 13:30:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 283.06
[32m[20230117 13:30:33 @agent_ppo2.py:151][0m Total time:       4.13 min
[32m[20230117 13:30:33 @agent_ppo2.py:153][0m 331776 total steps have happened
[32m[20230117 13:30:33 @agent_ppo2.py:129][0m #------------------------ Iteration 162 --------------------------#
[32m[20230117 13:30:33 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:30:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:33 @agent_ppo2.py:193][0m |          -0.0018 |          14.5777 |           3.9138 |
[32m[20230117 13:30:33 @agent_ppo2.py:193][0m |          -0.0019 |          14.2440 |           3.9061 |
[32m[20230117 13:30:33 @agent_ppo2.py:193][0m |          -0.0083 |          13.3047 |           3.9043 |
[32m[20230117 13:30:33 @agent_ppo2.py:193][0m |          -0.0096 |          13.0788 |           3.9073 |
[32m[20230117 13:30:33 @agent_ppo2.py:193][0m |          -0.0102 |          12.8684 |           3.9047 |
[32m[20230117 13:30:34 @agent_ppo2.py:193][0m |          -0.0091 |          12.8454 |           3.9054 |
[32m[20230117 13:30:34 @agent_ppo2.py:193][0m |          -0.0119 |          12.4835 |           3.9035 |
[32m[20230117 13:30:34 @agent_ppo2.py:193][0m |          -0.0122 |          12.3629 |           3.9045 |
[32m[20230117 13:30:34 @agent_ppo2.py:193][0m |          -0.0144 |          12.2861 |           3.9045 |
[32m[20230117 13:30:34 @agent_ppo2.py:193][0m |          -0.0107 |          12.1819 |           3.9044 |
[32m[20230117 13:30:34 @agent_ppo2.py:138][0m Policy update time: 0.97 s
[32m[20230117 13:30:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.12
[32m[20230117 13:30:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.26
[32m[20230117 13:30:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.28
[32m[20230117 13:30:34 @agent_ppo2.py:151][0m Total time:       4.15 min
[32m[20230117 13:30:34 @agent_ppo2.py:153][0m 333824 total steps have happened
[32m[20230117 13:30:34 @agent_ppo2.py:129][0m #------------------------ Iteration 163 --------------------------#
[32m[20230117 13:30:34 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:30:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:35 @agent_ppo2.py:193][0m |           0.0005 |          14.8162 |           3.9123 |
[32m[20230117 13:30:35 @agent_ppo2.py:193][0m |          -0.0038 |          12.0851 |           3.9128 |
[32m[20230117 13:30:35 @agent_ppo2.py:193][0m |          -0.0064 |          11.3581 |           3.9107 |
[32m[20230117 13:30:35 @agent_ppo2.py:193][0m |          -0.0088 |          10.6573 |           3.9105 |
[32m[20230117 13:30:35 @agent_ppo2.py:193][0m |          -0.0102 |          10.2303 |           3.9107 |
[32m[20230117 13:30:35 @agent_ppo2.py:193][0m |          -0.0106 |          10.0095 |           3.9102 |
[32m[20230117 13:30:35 @agent_ppo2.py:193][0m |          -0.0114 |           9.7101 |           3.9128 |
[32m[20230117 13:30:35 @agent_ppo2.py:193][0m |          -0.0112 |           9.3597 |           3.9134 |
[32m[20230117 13:30:35 @agent_ppo2.py:193][0m |          -0.0125 |           9.1922 |           3.9148 |
[32m[20230117 13:30:35 @agent_ppo2.py:193][0m |          -0.0128 |           8.9407 |           3.9144 |
[32m[20230117 13:30:35 @agent_ppo2.py:138][0m Policy update time: 0.89 s
[32m[20230117 13:30:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.28
[32m[20230117 13:30:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.00
[32m[20230117 13:30:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 283.87
[32m[20230117 13:30:35 @agent_ppo2.py:151][0m Total time:       4.17 min
[32m[20230117 13:30:35 @agent_ppo2.py:153][0m 335872 total steps have happened
[32m[20230117 13:30:35 @agent_ppo2.py:129][0m #------------------------ Iteration 164 --------------------------#
[32m[20230117 13:30:36 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:30:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:36 @agent_ppo2.py:193][0m |           0.0003 |          11.3587 |           3.8869 |
[32m[20230117 13:30:36 @agent_ppo2.py:193][0m |          -0.0044 |          10.0259 |           3.8805 |
[32m[20230117 13:30:36 @agent_ppo2.py:193][0m |          -0.0059 |           9.4145 |           3.8835 |
[32m[20230117 13:30:36 @agent_ppo2.py:193][0m |          -0.0075 |           8.9327 |           3.8825 |
[32m[20230117 13:30:36 @agent_ppo2.py:193][0m |          -0.0081 |           8.4684 |           3.8851 |
[32m[20230117 13:30:36 @agent_ppo2.py:193][0m |          -0.0081 |           7.9794 |           3.8819 |
[32m[20230117 13:30:36 @agent_ppo2.py:193][0m |          -0.0086 |           7.5685 |           3.8808 |
[32m[20230117 13:30:36 @agent_ppo2.py:193][0m |          -0.0098 |           7.2203 |           3.8839 |
[32m[20230117 13:30:37 @agent_ppo2.py:193][0m |          -0.0101 |           6.9549 |           3.8831 |
[32m[20230117 13:30:37 @agent_ppo2.py:193][0m |          -0.0105 |           6.6528 |           3.8832 |
[32m[20230117 13:30:37 @agent_ppo2.py:138][0m Policy update time: 0.88 s
[32m[20230117 13:30:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.90
[32m[20230117 13:30:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.86
[32m[20230117 13:30:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 285.68
[32m[20230117 13:30:37 @agent_ppo2.py:114][0m [4m[34mCRITICAL[0m Get the best episode reward: 285.68
[32m[20230117 13:30:37 @agent_ppo2.py:118][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 285.68
[32m[20230117 13:30:37 @agent_ppo2.py:151][0m Total time:       4.20 min
[32m[20230117 13:30:37 @agent_ppo2.py:153][0m 337920 total steps have happened
[32m[20230117 13:30:37 @agent_ppo2.py:129][0m #------------------------ Iteration 165 --------------------------#
[32m[20230117 13:30:37 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:30:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:37 @agent_ppo2.py:193][0m |          -0.0024 |          12.5546 |           3.8890 |
[32m[20230117 13:30:37 @agent_ppo2.py:193][0m |          -0.0037 |          11.5080 |           3.8818 |
[32m[20230117 13:30:37 @agent_ppo2.py:193][0m |          -0.0051 |          11.1613 |           3.8823 |
[32m[20230117 13:30:37 @agent_ppo2.py:193][0m |          -0.0026 |          11.2706 |           3.8860 |
[32m[20230117 13:30:38 @agent_ppo2.py:193][0m |          -0.0090 |          10.7078 |           3.8853 |
[32m[20230117 13:30:38 @agent_ppo2.py:193][0m |          -0.0091 |          10.5794 |           3.8884 |
[32m[20230117 13:30:38 @agent_ppo2.py:193][0m |          -0.0084 |          10.4202 |           3.8877 |
[32m[20230117 13:30:38 @agent_ppo2.py:193][0m |          -0.0094 |          10.4574 |           3.8851 |
[32m[20230117 13:30:38 @agent_ppo2.py:193][0m |          -0.0085 |          10.3586 |           3.8891 |
[32m[20230117 13:30:38 @agent_ppo2.py:193][0m |          -0.0101 |          10.1289 |           3.8885 |
[32m[20230117 13:30:38 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:30:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.57
[32m[20230117 13:30:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.38
[32m[20230117 13:30:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 53.28
[32m[20230117 13:30:38 @agent_ppo2.py:151][0m Total time:       4.22 min
[32m[20230117 13:30:38 @agent_ppo2.py:153][0m 339968 total steps have happened
[32m[20230117 13:30:38 @agent_ppo2.py:129][0m #------------------------ Iteration 166 --------------------------#
[32m[20230117 13:30:38 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:30:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:39 @agent_ppo2.py:193][0m |          -0.0019 |          18.1587 |           3.9894 |
[32m[20230117 13:30:39 @agent_ppo2.py:193][0m |          -0.0059 |          12.8079 |           3.9832 |
[32m[20230117 13:30:39 @agent_ppo2.py:193][0m |          -0.0070 |          11.5538 |           3.9750 |
[32m[20230117 13:30:39 @agent_ppo2.py:193][0m |          -0.0083 |          10.9578 |           3.9737 |
[32m[20230117 13:30:39 @agent_ppo2.py:193][0m |          -0.0103 |          10.4852 |           3.9736 |
[32m[20230117 13:30:39 @agent_ppo2.py:193][0m |          -0.0110 |          10.1476 |           3.9683 |
[32m[20230117 13:30:39 @agent_ppo2.py:193][0m |          -0.0113 |           9.8503 |           3.9703 |
[32m[20230117 13:30:39 @agent_ppo2.py:193][0m |          -0.0119 |           9.6226 |           3.9659 |
[32m[20230117 13:30:39 @agent_ppo2.py:193][0m |          -0.0119 |           9.3840 |           3.9654 |
[32m[20230117 13:30:39 @agent_ppo2.py:193][0m |          -0.0129 |           9.1977 |           3.9603 |
[32m[20230117 13:30:39 @agent_ppo2.py:138][0m Policy update time: 0.95 s
[32m[20230117 13:30:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 140.99
[32m[20230117 13:30:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.55
[32m[20230117 13:30:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.85
[32m[20230117 13:30:40 @agent_ppo2.py:151][0m Total time:       4.24 min
[32m[20230117 13:30:40 @agent_ppo2.py:153][0m 342016 total steps have happened
[32m[20230117 13:30:40 @agent_ppo2.py:129][0m #------------------------ Iteration 167 --------------------------#
[32m[20230117 13:30:40 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:30:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:40 @agent_ppo2.py:193][0m |           0.0001 |          24.3466 |           3.9730 |
[32m[20230117 13:30:40 @agent_ppo2.py:193][0m |          -0.0036 |          18.2104 |           3.9728 |
[32m[20230117 13:30:40 @agent_ppo2.py:193][0m |          -0.0066 |          17.0860 |           3.9690 |
[32m[20230117 13:30:40 @agent_ppo2.py:193][0m |          -0.0090 |          16.4483 |           3.9647 |
[32m[20230117 13:30:40 @agent_ppo2.py:193][0m |          -0.0101 |          15.9189 |           3.9625 |
[32m[20230117 13:30:40 @agent_ppo2.py:193][0m |          -0.0111 |          15.5237 |           3.9645 |
[32m[20230117 13:30:40 @agent_ppo2.py:193][0m |          -0.0109 |          15.3233 |           3.9575 |
[32m[20230117 13:30:41 @agent_ppo2.py:193][0m |          -0.0126 |          14.9993 |           3.9603 |
[32m[20230117 13:30:41 @agent_ppo2.py:193][0m |          -0.0130 |          14.7224 |           3.9565 |
[32m[20230117 13:30:41 @agent_ppo2.py:193][0m |          -0.0132 |          14.6321 |           3.9568 |
[32m[20230117 13:30:41 @agent_ppo2.py:138][0m Policy update time: 0.91 s
[32m[20230117 13:30:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 183.60
[32m[20230117 13:30:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.19
[32m[20230117 13:30:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 282.20
[32m[20230117 13:30:41 @agent_ppo2.py:151][0m Total time:       4.27 min
[32m[20230117 13:30:41 @agent_ppo2.py:153][0m 344064 total steps have happened
[32m[20230117 13:30:41 @agent_ppo2.py:129][0m #------------------------ Iteration 168 --------------------------#
[32m[20230117 13:30:41 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:30:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:41 @agent_ppo2.py:193][0m |          -0.0005 |          14.3339 |           3.8678 |
[32m[20230117 13:30:41 @agent_ppo2.py:193][0m |          -0.0054 |          12.7040 |           3.8624 |
[32m[20230117 13:30:41 @agent_ppo2.py:193][0m |          -0.0059 |          12.3815 |           3.8587 |
[32m[20230117 13:30:42 @agent_ppo2.py:193][0m |          -0.0061 |          12.2825 |           3.8503 |
[32m[20230117 13:30:42 @agent_ppo2.py:193][0m |          -0.0122 |          12.0805 |           3.8466 |
[32m[20230117 13:30:42 @agent_ppo2.py:193][0m |          -0.0111 |          11.9850 |           3.8452 |
[32m[20230117 13:30:42 @agent_ppo2.py:193][0m |          -0.0098 |          11.9033 |           3.8417 |
[32m[20230117 13:30:42 @agent_ppo2.py:193][0m |          -0.0095 |          11.8168 |           3.8430 |
[32m[20230117 13:30:42 @agent_ppo2.py:193][0m |          -0.0116 |          11.7610 |           3.8415 |
[32m[20230117 13:30:42 @agent_ppo2.py:193][0m |          -0.0105 |          11.6677 |           3.8389 |
[32m[20230117 13:30:42 @agent_ppo2.py:138][0m Policy update time: 0.91 s
[32m[20230117 13:30:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.28
[32m[20230117 13:30:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.28
[32m[20230117 13:30:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 281.25
[32m[20230117 13:30:42 @agent_ppo2.py:151][0m Total time:       4.29 min
[32m[20230117 13:30:42 @agent_ppo2.py:153][0m 346112 total steps have happened
[32m[20230117 13:30:42 @agent_ppo2.py:129][0m #------------------------ Iteration 169 --------------------------#
[32m[20230117 13:30:43 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:30:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:43 @agent_ppo2.py:193][0m |           0.0014 |          21.4036 |           3.8495 |
[32m[20230117 13:30:43 @agent_ppo2.py:193][0m |          -0.0058 |          15.6935 |           3.8397 |
[32m[20230117 13:30:43 @agent_ppo2.py:193][0m |          -0.0072 |          14.8177 |           3.8344 |
[32m[20230117 13:30:43 @agent_ppo2.py:193][0m |          -0.0050 |          14.1285 |           3.8370 |
[32m[20230117 13:30:43 @agent_ppo2.py:193][0m |          -0.0092 |          13.7331 |           3.8311 |
[32m[20230117 13:30:43 @agent_ppo2.py:193][0m |          -0.0108 |          13.2138 |           3.8317 |
[32m[20230117 13:30:43 @agent_ppo2.py:193][0m |          -0.0114 |          12.6395 |           3.8278 |
[32m[20230117 13:30:43 @agent_ppo2.py:193][0m |          -0.0110 |          12.4531 |           3.8293 |
[32m[20230117 13:30:43 @agent_ppo2.py:193][0m |          -0.0124 |          12.1385 |           3.8288 |
[32m[20230117 13:30:43 @agent_ppo2.py:193][0m |          -0.0096 |          12.2630 |           3.8258 |
[32m[20230117 13:30:43 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:30:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 212.22
[32m[20230117 13:30:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.63
[32m[20230117 13:30:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 119.39
[32m[20230117 13:30:43 @agent_ppo2.py:151][0m Total time:       4.31 min
[32m[20230117 13:30:43 @agent_ppo2.py:153][0m 348160 total steps have happened
[32m[20230117 13:30:43 @agent_ppo2.py:129][0m #------------------------ Iteration 170 --------------------------#
[32m[20230117 13:30:44 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:30:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:44 @agent_ppo2.py:193][0m |          -0.0024 |          32.4152 |           3.8804 |
[32m[20230117 13:30:44 @agent_ppo2.py:193][0m |           0.0001 |          26.2287 |           3.8661 |
[32m[20230117 13:30:44 @agent_ppo2.py:193][0m |          -0.0062 |          24.2137 |           3.8686 |
[32m[20230117 13:30:44 @agent_ppo2.py:193][0m |          -0.0052 |          22.6238 |           3.8686 |
[32m[20230117 13:30:44 @agent_ppo2.py:193][0m |          -0.0098 |          21.4511 |           3.8655 |
[32m[20230117 13:30:44 @agent_ppo2.py:193][0m |          -0.0161 |          21.2419 |           3.8646 |
[32m[20230117 13:30:44 @agent_ppo2.py:193][0m |          -0.0100 |          21.0253 |           3.8642 |
[32m[20230117 13:30:44 @agent_ppo2.py:193][0m |          -0.0055 |          19.8441 |           3.8652 |
[32m[20230117 13:30:44 @agent_ppo2.py:193][0m |          -0.0124 |          20.7657 |           3.8682 |
[32m[20230117 13:30:45 @agent_ppo2.py:193][0m |          -0.0136 |          19.2910 |           3.8611 |
[32m[20230117 13:30:45 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:30:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 154.84
[32m[20230117 13:30:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.22
[32m[20230117 13:30:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 279.93
[32m[20230117 13:30:45 @agent_ppo2.py:151][0m Total time:       4.33 min
[32m[20230117 13:30:45 @agent_ppo2.py:153][0m 350208 total steps have happened
[32m[20230117 13:30:45 @agent_ppo2.py:129][0m #------------------------ Iteration 171 --------------------------#
[32m[20230117 13:30:45 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:30:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:45 @agent_ppo2.py:193][0m |           0.0033 |          13.9232 |           3.9198 |
[32m[20230117 13:30:45 @agent_ppo2.py:193][0m |          -0.0023 |          13.1073 |           3.9157 |
[32m[20230117 13:30:45 @agent_ppo2.py:193][0m |          -0.0054 |          12.6609 |           3.9105 |
[32m[20230117 13:30:45 @agent_ppo2.py:193][0m |          -0.0059 |          12.4888 |           3.9133 |
[32m[20230117 13:30:46 @agent_ppo2.py:193][0m |          -0.0044 |          12.5480 |           3.9113 |
[32m[20230117 13:30:46 @agent_ppo2.py:193][0m |          -0.0070 |          12.2174 |           3.9120 |
[32m[20230117 13:30:46 @agent_ppo2.py:193][0m |          -0.0059 |          12.1818 |           3.9085 |
[32m[20230117 13:30:46 @agent_ppo2.py:193][0m |          -0.0075 |          12.1278 |           3.9091 |
[32m[20230117 13:30:46 @agent_ppo2.py:193][0m |          -0.0104 |          11.9381 |           3.9070 |
[32m[20230117 13:30:46 @agent_ppo2.py:193][0m |          -0.0085 |          11.8952 |           3.9078 |
[32m[20230117 13:30:46 @agent_ppo2.py:138][0m Policy update time: 0.88 s
[32m[20230117 13:30:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 259.96
[32m[20230117 13:30:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.99
[32m[20230117 13:30:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.25
[32m[20230117 13:30:46 @agent_ppo2.py:151][0m Total time:       4.35 min
[32m[20230117 13:30:46 @agent_ppo2.py:153][0m 352256 total steps have happened
[32m[20230117 13:30:46 @agent_ppo2.py:129][0m #------------------------ Iteration 172 --------------------------#
[32m[20230117 13:30:46 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:30:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:47 @agent_ppo2.py:193][0m |          -0.0050 |          12.7379 |           3.8730 |
[32m[20230117 13:30:47 @agent_ppo2.py:193][0m |          -0.0036 |          11.5392 |           3.8641 |
[32m[20230117 13:30:47 @agent_ppo2.py:193][0m |           0.0316 |          14.0723 |           3.8653 |
[32m[20230117 13:30:47 @agent_ppo2.py:193][0m |          -0.0059 |          10.7047 |           3.8632 |
[32m[20230117 13:30:47 @agent_ppo2.py:193][0m |          -0.0056 |          10.3843 |           3.8634 |
[32m[20230117 13:30:47 @agent_ppo2.py:193][0m |          -0.0126 |          10.1535 |           3.8620 |
[32m[20230117 13:30:47 @agent_ppo2.py:193][0m |          -0.0148 |          10.0158 |           3.8625 |
[32m[20230117 13:30:47 @agent_ppo2.py:193][0m |          -0.0073 |           9.8144 |           3.8657 |
[32m[20230117 13:30:47 @agent_ppo2.py:193][0m |          -0.0115 |           9.7514 |           3.8614 |
[32m[20230117 13:30:47 @agent_ppo2.py:193][0m |          -0.0141 |           9.5379 |           3.8674 |
[32m[20230117 13:30:47 @agent_ppo2.py:138][0m Policy update time: 0.90 s
[32m[20230117 13:30:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.46
[32m[20230117 13:30:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.67
[32m[20230117 13:30:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 53.50
[32m[20230117 13:30:47 @agent_ppo2.py:151][0m Total time:       4.37 min
[32m[20230117 13:30:47 @agent_ppo2.py:153][0m 354304 total steps have happened
[32m[20230117 13:30:47 @agent_ppo2.py:129][0m #------------------------ Iteration 173 --------------------------#
[32m[20230117 13:30:48 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:30:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:48 @agent_ppo2.py:193][0m |           0.0002 |          12.6995 |           3.9962 |
[32m[20230117 13:30:48 @agent_ppo2.py:193][0m |          -0.0040 |          12.1679 |           3.9871 |
[32m[20230117 13:30:48 @agent_ppo2.py:193][0m |          -0.0058 |          11.9222 |           3.9859 |
[32m[20230117 13:30:48 @agent_ppo2.py:193][0m |          -0.0073 |          11.7483 |           3.9841 |
[32m[20230117 13:30:48 @agent_ppo2.py:193][0m |          -0.0080 |          11.6177 |           3.9839 |
[32m[20230117 13:30:48 @agent_ppo2.py:193][0m |          -0.0088 |          11.5042 |           3.9810 |
[32m[20230117 13:30:48 @agent_ppo2.py:193][0m |          -0.0093 |          11.4279 |           3.9798 |
[32m[20230117 13:30:48 @agent_ppo2.py:193][0m |          -0.0100 |          11.3429 |           3.9818 |
[32m[20230117 13:30:48 @agent_ppo2.py:193][0m |          -0.0106 |          11.2425 |           3.9785 |
[32m[20230117 13:30:49 @agent_ppo2.py:193][0m |          -0.0112 |          11.1660 |           3.9788 |
[32m[20230117 13:30:49 @agent_ppo2.py:138][0m Policy update time: 0.89 s
[32m[20230117 13:30:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 260.82
[32m[20230117 13:30:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.73
[32m[20230117 13:30:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 284.85
[32m[20230117 13:30:49 @agent_ppo2.py:151][0m Total time:       4.40 min
[32m[20230117 13:30:49 @agent_ppo2.py:153][0m 356352 total steps have happened
[32m[20230117 13:30:49 @agent_ppo2.py:129][0m #------------------------ Iteration 174 --------------------------#
[32m[20230117 13:30:49 @agent_ppo2.py:135][0m Sampling time: 0.29 s by 4 slaves
[32m[20230117 13:30:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:49 @agent_ppo2.py:193][0m |           0.0014 |          12.7990 |           3.8539 |
[32m[20230117 13:30:49 @agent_ppo2.py:193][0m |          -0.0030 |          11.9637 |           3.8480 |
[32m[20230117 13:30:49 @agent_ppo2.py:193][0m |          -0.0056 |          11.4684 |           3.8446 |
[32m[20230117 13:30:49 @agent_ppo2.py:193][0m |          -0.0064 |          11.2157 |           3.8445 |
[32m[20230117 13:30:50 @agent_ppo2.py:193][0m |          -0.0082 |          11.0238 |           3.8411 |
[32m[20230117 13:30:50 @agent_ppo2.py:193][0m |          -0.0093 |          10.8290 |           3.8417 |
[32m[20230117 13:30:50 @agent_ppo2.py:193][0m |          -0.0102 |          10.6968 |           3.8381 |
[32m[20230117 13:30:50 @agent_ppo2.py:193][0m |          -0.0109 |          10.5215 |           3.8350 |
[32m[20230117 13:30:50 @agent_ppo2.py:193][0m |          -0.0101 |          10.5949 |           3.8344 |
[32m[20230117 13:30:50 @agent_ppo2.py:193][0m |          -0.0117 |          10.3779 |           3.8320 |
[32m[20230117 13:30:50 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:30:50 @agent_ppo2.py:146][0m Average TRAINING episode reward: 260.21
[32m[20230117 13:30:50 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.53
[32m[20230117 13:30:50 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 281.00
[32m[20230117 13:30:50 @agent_ppo2.py:151][0m Total time:       4.42 min
[32m[20230117 13:30:50 @agent_ppo2.py:153][0m 358400 total steps have happened
[32m[20230117 13:30:50 @agent_ppo2.py:129][0m #------------------------ Iteration 175 --------------------------#
[32m[20230117 13:30:50 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:30:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:51 @agent_ppo2.py:193][0m |           0.0012 |          23.5950 |           3.9380 |
[32m[20230117 13:30:51 @agent_ppo2.py:193][0m |          -0.0042 |          19.3325 |           3.9276 |
[32m[20230117 13:30:51 @agent_ppo2.py:193][0m |          -0.0051 |          17.2839 |           3.9311 |
[32m[20230117 13:30:51 @agent_ppo2.py:193][0m |          -0.0069 |          16.0345 |           3.9308 |
[32m[20230117 13:30:51 @agent_ppo2.py:193][0m |          -0.0080 |          15.4767 |           3.9278 |
[32m[20230117 13:30:51 @agent_ppo2.py:193][0m |          -0.0064 |          14.8389 |           3.9305 |
[32m[20230117 13:30:51 @agent_ppo2.py:193][0m |          -0.0082 |          14.4696 |           3.9300 |
[32m[20230117 13:30:51 @agent_ppo2.py:193][0m |          -0.0085 |          14.3788 |           3.9295 |
[32m[20230117 13:30:51 @agent_ppo2.py:193][0m |          -0.0101 |          13.7003 |           3.9312 |
[32m[20230117 13:30:51 @agent_ppo2.py:193][0m |          -0.0094 |          13.5494 |           3.9304 |
[32m[20230117 13:30:51 @agent_ppo2.py:138][0m Policy update time: 0.95 s
[32m[20230117 13:30:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 198.69
[32m[20230117 13:30:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.57
[32m[20230117 13:30:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 281.44
[32m[20230117 13:30:52 @agent_ppo2.py:151][0m Total time:       4.44 min
[32m[20230117 13:30:52 @agent_ppo2.py:153][0m 360448 total steps have happened
[32m[20230117 13:30:52 @agent_ppo2.py:129][0m #------------------------ Iteration 176 --------------------------#
[32m[20230117 13:30:52 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:30:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:52 @agent_ppo2.py:193][0m |          -0.0002 |          31.6880 |           3.8904 |
[32m[20230117 13:30:52 @agent_ppo2.py:193][0m |          -0.0071 |          25.9346 |           3.8854 |
[32m[20230117 13:30:52 @agent_ppo2.py:193][0m |          -0.0060 |          23.6206 |           3.8874 |
[32m[20230117 13:30:52 @agent_ppo2.py:193][0m |          -0.0080 |          22.3660 |           3.8816 |
[32m[20230117 13:30:52 @agent_ppo2.py:193][0m |          -0.0079 |          21.5926 |           3.8865 |
[32m[20230117 13:30:52 @agent_ppo2.py:193][0m |          -0.0067 |          21.0248 |           3.8857 |
[32m[20230117 13:30:52 @agent_ppo2.py:193][0m |          -0.0113 |          20.4672 |           3.8852 |
[32m[20230117 13:30:52 @agent_ppo2.py:193][0m |          -0.0095 |          20.1702 |           3.8823 |
[32m[20230117 13:30:53 @agent_ppo2.py:193][0m |          -0.0121 |          19.8265 |           3.8832 |
[32m[20230117 13:30:53 @agent_ppo2.py:193][0m |          -0.0120 |          19.5716 |           3.8819 |
[32m[20230117 13:30:53 @agent_ppo2.py:138][0m Policy update time: 0.83 s
[32m[20230117 13:30:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 224.97
[32m[20230117 13:30:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.19
[32m[20230117 13:30:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 48.80
[32m[20230117 13:30:53 @agent_ppo2.py:151][0m Total time:       4.46 min
[32m[20230117 13:30:53 @agent_ppo2.py:153][0m 362496 total steps have happened
[32m[20230117 13:30:53 @agent_ppo2.py:129][0m #------------------------ Iteration 177 --------------------------#
[32m[20230117 13:30:53 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:30:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:53 @agent_ppo2.py:193][0m |          -0.0009 |          29.1694 |           3.8432 |
[32m[20230117 13:30:53 @agent_ppo2.py:193][0m |          -0.0067 |          25.6321 |           3.8446 |
[32m[20230117 13:30:53 @agent_ppo2.py:193][0m |          -0.0039 |          24.3499 |           3.8359 |
[32m[20230117 13:30:53 @agent_ppo2.py:193][0m |          -0.0087 |          22.8612 |           3.8421 |
[32m[20230117 13:30:53 @agent_ppo2.py:193][0m |          -0.0071 |          22.2241 |           3.8372 |
[32m[20230117 13:30:54 @agent_ppo2.py:193][0m |          -0.0085 |          21.2549 |           3.8374 |
[32m[20230117 13:30:54 @agent_ppo2.py:193][0m |          -0.0097 |          19.8305 |           3.8393 |
[32m[20230117 13:30:54 @agent_ppo2.py:193][0m |          -0.0127 |          19.2003 |           3.8377 |
[32m[20230117 13:30:54 @agent_ppo2.py:193][0m |          -0.0127 |          18.3961 |           3.8378 |
[32m[20230117 13:30:54 @agent_ppo2.py:193][0m |          -0.0133 |          17.8731 |           3.8346 |
[32m[20230117 13:30:54 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:30:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 207.64
[32m[20230117 13:30:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 261.34
[32m[20230117 13:30:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.74
[32m[20230117 13:30:54 @agent_ppo2.py:151][0m Total time:       4.48 min
[32m[20230117 13:30:54 @agent_ppo2.py:153][0m 364544 total steps have happened
[32m[20230117 13:30:54 @agent_ppo2.py:129][0m #------------------------ Iteration 178 --------------------------#
[32m[20230117 13:30:54 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:30:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:54 @agent_ppo2.py:193][0m |          -0.0018 |          13.2556 |           3.9281 |
[32m[20230117 13:30:55 @agent_ppo2.py:193][0m |          -0.0090 |          11.6295 |           3.9244 |
[32m[20230117 13:30:55 @agent_ppo2.py:193][0m |          -0.0089 |          10.9425 |           3.9247 |
[32m[20230117 13:30:55 @agent_ppo2.py:193][0m |          -0.0110 |          10.3626 |           3.9233 |
[32m[20230117 13:30:55 @agent_ppo2.py:193][0m |          -0.0093 |           9.8179 |           3.9234 |
[32m[20230117 13:30:55 @agent_ppo2.py:193][0m |          -0.0109 |           9.1990 |           3.9262 |
[32m[20230117 13:30:55 @agent_ppo2.py:193][0m |          -0.0100 |           8.7376 |           3.9234 |
[32m[20230117 13:30:55 @agent_ppo2.py:193][0m |          -0.0115 |           8.4782 |           3.9247 |
[32m[20230117 13:30:55 @agent_ppo2.py:193][0m |          -0.0136 |           8.0573 |           3.9221 |
[32m[20230117 13:30:55 @agent_ppo2.py:193][0m |          -0.0127 |           7.8776 |           3.9240 |
[32m[20230117 13:30:55 @agent_ppo2.py:138][0m Policy update time: 0.85 s
[32m[20230117 13:30:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 260.63
[32m[20230117 13:30:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.29
[32m[20230117 13:30:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.34
[32m[20230117 13:30:55 @agent_ppo2.py:151][0m Total time:       4.51 min
[32m[20230117 13:30:55 @agent_ppo2.py:153][0m 366592 total steps have happened
[32m[20230117 13:30:55 @agent_ppo2.py:129][0m #------------------------ Iteration 179 --------------------------#
[32m[20230117 13:30:56 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:30:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:56 @agent_ppo2.py:193][0m |          -0.0014 |          14.1714 |           3.8528 |
[32m[20230117 13:30:56 @agent_ppo2.py:193][0m |          -0.0051 |          12.9949 |           3.8520 |
[32m[20230117 13:30:56 @agent_ppo2.py:193][0m |          -0.0065 |          12.5369 |           3.8555 |
[32m[20230117 13:30:56 @agent_ppo2.py:193][0m |          -0.0089 |          12.2624 |           3.8536 |
[32m[20230117 13:30:56 @agent_ppo2.py:193][0m |          -0.0081 |          12.1021 |           3.8547 |
[32m[20230117 13:30:56 @agent_ppo2.py:193][0m |          -0.0081 |          11.9645 |           3.8562 |
[32m[20230117 13:30:56 @agent_ppo2.py:193][0m |          -0.0109 |          11.6944 |           3.8557 |
[32m[20230117 13:30:56 @agent_ppo2.py:193][0m |          -0.0110 |          11.5773 |           3.8597 |
[32m[20230117 13:30:56 @agent_ppo2.py:193][0m |          -0.0112 |          11.5052 |           3.8589 |
[32m[20230117 13:30:56 @agent_ppo2.py:193][0m |          -0.0115 |          11.3557 |           3.8606 |
[32m[20230117 13:30:56 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:30:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 260.70
[32m[20230117 13:30:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.00
[32m[20230117 13:30:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 281.21
[32m[20230117 13:30:57 @agent_ppo2.py:151][0m Total time:       4.53 min
[32m[20230117 13:30:57 @agent_ppo2.py:153][0m 368640 total steps have happened
[32m[20230117 13:30:57 @agent_ppo2.py:129][0m #------------------------ Iteration 180 --------------------------#
[32m[20230117 13:30:57 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:30:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:57 @agent_ppo2.py:193][0m |          -0.0196 |          15.5539 |           3.9215 |
[32m[20230117 13:30:57 @agent_ppo2.py:193][0m |          -0.0083 |          14.0205 |           3.9024 |
[32m[20230117 13:30:57 @agent_ppo2.py:193][0m |          -0.0070 |          13.6583 |           3.9081 |
[32m[20230117 13:30:57 @agent_ppo2.py:193][0m |          -0.0109 |          13.4545 |           3.9081 |
[32m[20230117 13:30:57 @agent_ppo2.py:193][0m |          -0.0133 |          13.3299 |           3.9067 |
[32m[20230117 13:30:57 @agent_ppo2.py:193][0m |          -0.0115 |          13.1395 |           3.9092 |
[32m[20230117 13:30:58 @agent_ppo2.py:193][0m |          -0.0115 |          12.9971 |           3.9104 |
[32m[20230117 13:30:58 @agent_ppo2.py:193][0m |          -0.0126 |          12.8568 |           3.9068 |
[32m[20230117 13:30:58 @agent_ppo2.py:193][0m |          -0.0179 |          12.8139 |           3.9076 |
[32m[20230117 13:30:58 @agent_ppo2.py:193][0m |          -0.0132 |          12.6652 |           3.9083 |
[32m[20230117 13:30:58 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:30:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.41
[32m[20230117 13:30:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.29
[32m[20230117 13:30:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 280.51
[32m[20230117 13:30:58 @agent_ppo2.py:151][0m Total time:       4.55 min
[32m[20230117 13:30:58 @agent_ppo2.py:153][0m 370688 total steps have happened
[32m[20230117 13:30:58 @agent_ppo2.py:129][0m #------------------------ Iteration 181 --------------------------#
[32m[20230117 13:30:58 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:30:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:30:58 @agent_ppo2.py:193][0m |           0.0017 |          13.4264 |           3.9504 |
[32m[20230117 13:30:58 @agent_ppo2.py:193][0m |          -0.0062 |          12.9092 |           3.9446 |
[32m[20230117 13:30:59 @agent_ppo2.py:193][0m |          -0.0018 |          12.6647 |           3.9462 |
[32m[20230117 13:30:59 @agent_ppo2.py:193][0m |          -0.0073 |          12.3947 |           3.9452 |
[32m[20230117 13:30:59 @agent_ppo2.py:193][0m |          -0.0110 |          12.2168 |           3.9479 |
[32m[20230117 13:30:59 @agent_ppo2.py:193][0m |          -0.0106 |          12.0468 |           3.9477 |
[32m[20230117 13:30:59 @agent_ppo2.py:193][0m |          -0.0109 |          11.9177 |           3.9488 |
[32m[20230117 13:30:59 @agent_ppo2.py:193][0m |          -0.0116 |          11.8171 |           3.9467 |
[32m[20230117 13:30:59 @agent_ppo2.py:193][0m |          -0.0002 |          12.4647 |           3.9486 |
[32m[20230117 13:30:59 @agent_ppo2.py:193][0m |          -0.0114 |          11.6842 |           3.9462 |
[32m[20230117 13:30:59 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:30:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 260.04
[32m[20230117 13:30:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.08
[32m[20230117 13:30:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 279.40
[32m[20230117 13:30:59 @agent_ppo2.py:151][0m Total time:       4.57 min
[32m[20230117 13:30:59 @agent_ppo2.py:153][0m 372736 total steps have happened
[32m[20230117 13:30:59 @agent_ppo2.py:129][0m #------------------------ Iteration 182 --------------------------#
[32m[20230117 13:31:00 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:31:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:00 @agent_ppo2.py:193][0m |           0.0019 |          14.1329 |           3.9616 |
[32m[20230117 13:31:00 @agent_ppo2.py:193][0m |          -0.0029 |          13.3504 |           3.9603 |
[32m[20230117 13:31:00 @agent_ppo2.py:193][0m |          -0.0056 |          13.0093 |           3.9625 |
[32m[20230117 13:31:00 @agent_ppo2.py:193][0m |          -0.0079 |          12.8381 |           3.9599 |
[32m[20230117 13:31:00 @agent_ppo2.py:193][0m |          -0.0063 |          12.7718 |           3.9617 |
[32m[20230117 13:31:00 @agent_ppo2.py:193][0m |          -0.0076 |          12.5929 |           3.9606 |
[32m[20230117 13:31:00 @agent_ppo2.py:193][0m |          -0.0082 |          12.5118 |           3.9645 |
[32m[20230117 13:31:00 @agent_ppo2.py:193][0m |          -0.0081 |          12.4507 |           3.9646 |
[32m[20230117 13:31:00 @agent_ppo2.py:193][0m |          -0.0080 |          12.3874 |           3.9636 |
[32m[20230117 13:31:00 @agent_ppo2.py:193][0m |          -0.0096 |          12.1780 |           3.9602 |
[32m[20230117 13:31:00 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:31:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.97
[32m[20230117 13:31:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.95
[32m[20230117 13:31:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 282.42
[32m[20230117 13:31:01 @agent_ppo2.py:151][0m Total time:       4.59 min
[32m[20230117 13:31:01 @agent_ppo2.py:153][0m 374784 total steps have happened
[32m[20230117 13:31:01 @agent_ppo2.py:129][0m #------------------------ Iteration 183 --------------------------#
[32m[20230117 13:31:01 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:01 @agent_ppo2.py:193][0m |           0.0026 |          13.1649 |           3.9911 |
[32m[20230117 13:31:01 @agent_ppo2.py:193][0m |          -0.0036 |          12.4956 |           3.9891 |
[32m[20230117 13:31:01 @agent_ppo2.py:193][0m |          -0.0050 |          12.1041 |           3.9863 |
[32m[20230117 13:31:01 @agent_ppo2.py:193][0m |          -0.0084 |          11.8684 |           3.9877 |
[32m[20230117 13:31:01 @agent_ppo2.py:193][0m |          -0.0053 |          12.0628 |           3.9865 |
[32m[20230117 13:31:01 @agent_ppo2.py:193][0m |          -0.0079 |          11.6785 |           3.9849 |
[32m[20230117 13:31:01 @agent_ppo2.py:193][0m |          -0.0090 |          11.4662 |           3.9842 |
[32m[20230117 13:31:02 @agent_ppo2.py:193][0m |          -0.0079 |          11.5817 |           3.9879 |
[32m[20230117 13:31:02 @agent_ppo2.py:193][0m |          -0.0100 |          11.2868 |           3.9852 |
[32m[20230117 13:31:02 @agent_ppo2.py:193][0m |          -0.0094 |          11.2191 |           3.9860 |
[32m[20230117 13:31:02 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:31:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.52
[32m[20230117 13:31:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.49
[32m[20230117 13:31:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.19
[32m[20230117 13:31:02 @agent_ppo2.py:151][0m Total time:       4.62 min
[32m[20230117 13:31:02 @agent_ppo2.py:153][0m 376832 total steps have happened
[32m[20230117 13:31:02 @agent_ppo2.py:129][0m #------------------------ Iteration 184 --------------------------#
[32m[20230117 13:31:02 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:02 @agent_ppo2.py:193][0m |          -0.0005 |          13.6282 |           4.0132 |
[32m[20230117 13:31:02 @agent_ppo2.py:193][0m |          -0.0044 |          12.8973 |           4.0024 |
[32m[20230117 13:31:02 @agent_ppo2.py:193][0m |          -0.0068 |          12.4910 |           4.0020 |
[32m[20230117 13:31:03 @agent_ppo2.py:193][0m |          -0.0078 |          12.1794 |           4.0043 |
[32m[20230117 13:31:03 @agent_ppo2.py:193][0m |          -0.0093 |          11.9476 |           3.9987 |
[32m[20230117 13:31:03 @agent_ppo2.py:193][0m |          -0.0096 |          11.7882 |           3.9970 |
[32m[20230117 13:31:03 @agent_ppo2.py:193][0m |          -0.0104 |          11.5744 |           3.9944 |
[32m[20230117 13:31:03 @agent_ppo2.py:193][0m |          -0.0102 |          11.4782 |           3.9932 |
[32m[20230117 13:31:03 @agent_ppo2.py:193][0m |          -0.0112 |          11.3122 |           3.9974 |
[32m[20230117 13:31:03 @agent_ppo2.py:193][0m |          -0.0111 |          11.1664 |           3.9939 |
[32m[20230117 13:31:03 @agent_ppo2.py:138][0m Policy update time: 0.85 s
[32m[20230117 13:31:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.48
[32m[20230117 13:31:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.49
[32m[20230117 13:31:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 281.18
[32m[20230117 13:31:03 @agent_ppo2.py:151][0m Total time:       4.64 min
[32m[20230117 13:31:03 @agent_ppo2.py:153][0m 378880 total steps have happened
[32m[20230117 13:31:03 @agent_ppo2.py:129][0m #------------------------ Iteration 185 --------------------------#
[32m[20230117 13:31:03 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:31:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:04 @agent_ppo2.py:193][0m |          -0.0003 |          13.1504 |           3.9592 |
[32m[20230117 13:31:04 @agent_ppo2.py:193][0m |          -0.0032 |          12.8491 |           3.9534 |
[32m[20230117 13:31:04 @agent_ppo2.py:193][0m |          -0.0036 |          12.8374 |           3.9438 |
[32m[20230117 13:31:04 @agent_ppo2.py:193][0m |          -0.0044 |          12.6565 |           3.9416 |
[32m[20230117 13:31:04 @agent_ppo2.py:193][0m |          -0.0067 |          12.2930 |           3.9401 |
[32m[20230117 13:31:04 @agent_ppo2.py:193][0m |          -0.0067 |          12.3828 |           3.9375 |
[32m[20230117 13:31:04 @agent_ppo2.py:193][0m |          -0.0063 |          12.4612 |           3.9338 |
[32m[20230117 13:31:04 @agent_ppo2.py:193][0m |          -0.0058 |          12.2081 |           3.9305 |
[32m[20230117 13:31:04 @agent_ppo2.py:193][0m |          -0.0096 |          11.8572 |           3.9277 |
[32m[20230117 13:31:04 @agent_ppo2.py:193][0m |          -0.0074 |          12.0069 |           3.9248 |
[32m[20230117 13:31:04 @agent_ppo2.py:138][0m Policy update time: 0.81 s
[32m[20230117 13:31:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.58
[32m[20230117 13:31:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.57
[32m[20230117 13:31:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 281.76
[32m[20230117 13:31:04 @agent_ppo2.py:151][0m Total time:       4.66 min
[32m[20230117 13:31:04 @agent_ppo2.py:153][0m 380928 total steps have happened
[32m[20230117 13:31:04 @agent_ppo2.py:129][0m #------------------------ Iteration 186 --------------------------#
[32m[20230117 13:31:05 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:05 @agent_ppo2.py:193][0m |          -0.0009 |          13.5833 |           3.9031 |
[32m[20230117 13:31:05 @agent_ppo2.py:193][0m |          -0.0066 |          12.5868 |           3.8916 |
[32m[20230117 13:31:05 @agent_ppo2.py:193][0m |          -0.0074 |          12.1310 |           3.8855 |
[32m[20230117 13:31:05 @agent_ppo2.py:193][0m |          -0.0094 |          11.8226 |           3.8884 |
[32m[20230117 13:31:05 @agent_ppo2.py:193][0m |          -0.0092 |          11.6462 |           3.8848 |
[32m[20230117 13:31:05 @agent_ppo2.py:193][0m |          -0.0097 |          11.5173 |           3.8851 |
[32m[20230117 13:31:05 @agent_ppo2.py:193][0m |          -0.0128 |          11.0629 |           3.8849 |
[32m[20230117 13:31:05 @agent_ppo2.py:193][0m |          -0.0127 |          10.8515 |           3.8836 |
[32m[20230117 13:31:05 @agent_ppo2.py:193][0m |          -0.0119 |          10.6899 |           3.8810 |
[32m[20230117 13:31:06 @agent_ppo2.py:193][0m |          -0.0124 |          10.4629 |           3.8822 |
[32m[20230117 13:31:06 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:31:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.18
[32m[20230117 13:31:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.36
[32m[20230117 13:31:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 126.60
[32m[20230117 13:31:06 @agent_ppo2.py:151][0m Total time:       4.68 min
[32m[20230117 13:31:06 @agent_ppo2.py:153][0m 382976 total steps have happened
[32m[20230117 13:31:06 @agent_ppo2.py:129][0m #------------------------ Iteration 187 --------------------------#
[32m[20230117 13:31:06 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:31:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:06 @agent_ppo2.py:193][0m |           0.0007 |          13.7611 |           3.9231 |
[32m[20230117 13:31:06 @agent_ppo2.py:193][0m |          -0.0027 |          12.9462 |           3.9215 |
[32m[20230117 13:31:06 @agent_ppo2.py:193][0m |          -0.0044 |          12.6488 |           3.9196 |
[32m[20230117 13:31:06 @agent_ppo2.py:193][0m |          -0.0048 |          12.4542 |           3.9174 |
[32m[20230117 13:31:06 @agent_ppo2.py:193][0m |          -0.0071 |          12.2582 |           3.9155 |
[32m[20230117 13:31:07 @agent_ppo2.py:193][0m |          -0.0070 |          12.1233 |           3.9187 |
[32m[20230117 13:31:07 @agent_ppo2.py:193][0m |          -0.0079 |          12.0629 |           3.9164 |
[32m[20230117 13:31:07 @agent_ppo2.py:193][0m |          -0.0080 |          11.9294 |           3.9169 |
[32m[20230117 13:31:07 @agent_ppo2.py:193][0m |          -0.0091 |          11.8403 |           3.9148 |
[32m[20230117 13:31:07 @agent_ppo2.py:193][0m |          -0.0091 |          11.8241 |           3.9126 |
[32m[20230117 13:31:07 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:31:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 260.43
[32m[20230117 13:31:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.63
[32m[20230117 13:31:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.72
[32m[20230117 13:31:07 @agent_ppo2.py:151][0m Total time:       4.70 min
[32m[20230117 13:31:07 @agent_ppo2.py:153][0m 385024 total steps have happened
[32m[20230117 13:31:07 @agent_ppo2.py:129][0m #------------------------ Iteration 188 --------------------------#
[32m[20230117 13:31:07 @agent_ppo2.py:135][0m Sampling time: 0.29 s by 4 slaves
[32m[20230117 13:31:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:08 @agent_ppo2.py:193][0m |           0.0028 |          25.6007 |           3.8652 |
[32m[20230117 13:31:08 @agent_ppo2.py:193][0m |          -0.0030 |          18.2941 |           3.8633 |
[32m[20230117 13:31:08 @agent_ppo2.py:193][0m |          -0.0057 |          14.9262 |           3.8620 |
[32m[20230117 13:31:08 @agent_ppo2.py:193][0m |          -0.0055 |          14.4448 |           3.8598 |
[32m[20230117 13:31:08 @agent_ppo2.py:193][0m |          -0.0071 |          13.3893 |           3.8592 |
[32m[20230117 13:31:08 @agent_ppo2.py:193][0m |          -0.0064 |          13.0425 |           3.8563 |
[32m[20230117 13:31:08 @agent_ppo2.py:193][0m |          -0.0050 |          12.4315 |           3.8558 |
[32m[20230117 13:31:08 @agent_ppo2.py:193][0m |          -0.0106 |          11.9983 |           3.8535 |
[32m[20230117 13:31:08 @agent_ppo2.py:193][0m |          -0.0011 |          12.2985 |           3.8489 |
[32m[20230117 13:31:08 @agent_ppo2.py:193][0m |          -0.0097 |          11.6621 |           3.8489 |
[32m[20230117 13:31:08 @agent_ppo2.py:138][0m Policy update time: 0.95 s
[32m[20230117 13:31:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 198.91
[32m[20230117 13:31:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.90
[32m[20230117 13:31:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 279.54
[32m[20230117 13:31:09 @agent_ppo2.py:151][0m Total time:       4.73 min
[32m[20230117 13:31:09 @agent_ppo2.py:153][0m 387072 total steps have happened
[32m[20230117 13:31:09 @agent_ppo2.py:129][0m #------------------------ Iteration 189 --------------------------#
[32m[20230117 13:31:09 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:31:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:09 @agent_ppo2.py:193][0m |          -0.0008 |          14.9787 |           3.9354 |
[32m[20230117 13:31:09 @agent_ppo2.py:193][0m |          -0.0036 |          14.2447 |           3.9325 |
[32m[20230117 13:31:09 @agent_ppo2.py:193][0m |          -0.0052 |          13.9355 |           3.9277 |
[32m[20230117 13:31:09 @agent_ppo2.py:193][0m |          -0.0061 |          13.7633 |           3.9279 |
[32m[20230117 13:31:09 @agent_ppo2.py:193][0m |          -0.0067 |          13.6143 |           3.9297 |
[32m[20230117 13:31:09 @agent_ppo2.py:193][0m |          -0.0077 |          13.5026 |           3.9303 |
[32m[20230117 13:31:09 @agent_ppo2.py:193][0m |          -0.0086 |          13.4132 |           3.9312 |
[32m[20230117 13:31:09 @agent_ppo2.py:193][0m |          -0.0092 |          13.2873 |           3.9294 |
[32m[20230117 13:31:10 @agent_ppo2.py:193][0m |          -0.0095 |          13.2066 |           3.9304 |
[32m[20230117 13:31:10 @agent_ppo2.py:193][0m |          -0.0095 |          13.1165 |           3.9321 |
[32m[20230117 13:31:10 @agent_ppo2.py:138][0m Policy update time: 0.85 s
[32m[20230117 13:31:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.12
[32m[20230117 13:31:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.92
[32m[20230117 13:31:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 112.92
[32m[20230117 13:31:10 @agent_ppo2.py:151][0m Total time:       4.75 min
[32m[20230117 13:31:10 @agent_ppo2.py:153][0m 389120 total steps have happened
[32m[20230117 13:31:10 @agent_ppo2.py:129][0m #------------------------ Iteration 190 --------------------------#
[32m[20230117 13:31:10 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:31:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:10 @agent_ppo2.py:193][0m |          -0.0017 |          19.2082 |           3.9422 |
[32m[20230117 13:31:10 @agent_ppo2.py:193][0m |          -0.0047 |          14.7280 |           3.9343 |
[32m[20230117 13:31:11 @agent_ppo2.py:193][0m |          -0.0075 |          13.3844 |           3.9374 |
[32m[20230117 13:31:11 @agent_ppo2.py:193][0m |          -0.0094 |          12.6749 |           3.9344 |
[32m[20230117 13:31:11 @agent_ppo2.py:193][0m |          -0.0099 |          12.1822 |           3.9335 |
[32m[20230117 13:31:11 @agent_ppo2.py:193][0m |          -0.0116 |          11.7720 |           3.9352 |
[32m[20230117 13:31:11 @agent_ppo2.py:193][0m |          -0.0097 |          11.5488 |           3.9331 |
[32m[20230117 13:31:11 @agent_ppo2.py:193][0m |          -0.0097 |          11.2729 |           3.9312 |
[32m[20230117 13:31:11 @agent_ppo2.py:193][0m |          -0.0113 |          11.0533 |           3.9310 |
[32m[20230117 13:31:11 @agent_ppo2.py:193][0m |          -0.0112 |          11.0742 |           3.9306 |
[32m[20230117 13:31:11 @agent_ppo2.py:138][0m Policy update time: 1.00 s
[32m[20230117 13:31:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 163.37
[32m[20230117 13:31:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.68
[32m[20230117 13:31:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 46.64
[32m[20230117 13:31:11 @agent_ppo2.py:151][0m Total time:       4.77 min
[32m[20230117 13:31:11 @agent_ppo2.py:153][0m 391168 total steps have happened
[32m[20230117 13:31:11 @agent_ppo2.py:129][0m #------------------------ Iteration 191 --------------------------#
[32m[20230117 13:31:12 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:12 @agent_ppo2.py:193][0m |          -0.0008 |          16.7164 |           3.8503 |
[32m[20230117 13:31:12 @agent_ppo2.py:193][0m |          -0.0073 |          14.4359 |           3.8440 |
[32m[20230117 13:31:12 @agent_ppo2.py:193][0m |          -0.0095 |          13.2685 |           3.8438 |
[32m[20230117 13:31:12 @agent_ppo2.py:193][0m |          -0.0132 |          12.3282 |           3.8416 |
[32m[20230117 13:31:12 @agent_ppo2.py:193][0m |          -0.0147 |          11.6137 |           3.8447 |
[32m[20230117 13:31:12 @agent_ppo2.py:193][0m |          -0.0163 |          11.0223 |           3.8411 |
[32m[20230117 13:31:12 @agent_ppo2.py:193][0m |          -0.0068 |          10.7598 |           3.8425 |
[32m[20230117 13:31:12 @agent_ppo2.py:193][0m |          -0.0120 |          10.1565 |           3.8440 |
[32m[20230117 13:31:12 @agent_ppo2.py:193][0m |          -0.0112 |           9.8028 |           3.8410 |
[32m[20230117 13:31:12 @agent_ppo2.py:193][0m |          -0.0177 |           9.3605 |           3.8414 |
[32m[20230117 13:31:12 @agent_ppo2.py:138][0m Policy update time: 0.85 s
[32m[20230117 13:31:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.78
[32m[20230117 13:31:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.78
[32m[20230117 13:31:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 282.37
[32m[20230117 13:31:13 @agent_ppo2.py:151][0m Total time:       4.79 min
[32m[20230117 13:31:13 @agent_ppo2.py:153][0m 393216 total steps have happened
[32m[20230117 13:31:13 @agent_ppo2.py:129][0m #------------------------ Iteration 192 --------------------------#
[32m[20230117 13:31:13 @agent_ppo2.py:135][0m Sampling time: 0.31 s by 4 slaves
[32m[20230117 13:31:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:13 @agent_ppo2.py:193][0m |          -0.0017 |          40.2007 |           3.8954 |
[32m[20230117 13:31:13 @agent_ppo2.py:193][0m |          -0.0052 |          25.4577 |           3.8920 |
[32m[20230117 13:31:13 @agent_ppo2.py:193][0m |           0.0364 |          25.0195 |           3.8926 |
[32m[20230117 13:31:13 @agent_ppo2.py:193][0m |          -0.0042 |          22.3160 |           3.8847 |
[32m[20230117 13:31:13 @agent_ppo2.py:193][0m |          -0.0085 |          21.4864 |           3.8911 |
[32m[20230117 13:31:13 @agent_ppo2.py:193][0m |          -0.0092 |          20.8532 |           3.8903 |
[32m[20230117 13:31:14 @agent_ppo2.py:193][0m |          -0.0131 |          20.2635 |           3.8895 |
[32m[20230117 13:31:14 @agent_ppo2.py:193][0m |          -0.0134 |          19.8390 |           3.8895 |
[32m[20230117 13:31:14 @agent_ppo2.py:193][0m |          -0.0128 |          19.3267 |           3.8894 |
[32m[20230117 13:31:14 @agent_ppo2.py:193][0m |          -0.0146 |          19.5203 |           3.8882 |
[32m[20230117 13:31:14 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:31:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 114.40
[32m[20230117 13:31:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.54
[32m[20230117 13:31:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 116.23
[32m[20230117 13:31:14 @agent_ppo2.py:151][0m Total time:       4.82 min
[32m[20230117 13:31:14 @agent_ppo2.py:153][0m 395264 total steps have happened
[32m[20230117 13:31:14 @agent_ppo2.py:129][0m #------------------------ Iteration 193 --------------------------#
[32m[20230117 13:31:14 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:31:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:14 @agent_ppo2.py:193][0m |          -0.0014 |          15.5127 |           3.8690 |
[32m[20230117 13:31:15 @agent_ppo2.py:193][0m |          -0.0093 |          13.6543 |           3.8656 |
[32m[20230117 13:31:15 @agent_ppo2.py:193][0m |          -0.0007 |          13.7242 |           3.8627 |
[32m[20230117 13:31:15 @agent_ppo2.py:193][0m |          -0.0101 |          12.9951 |           3.8647 |
[32m[20230117 13:31:15 @agent_ppo2.py:193][0m |          -0.0036 |          13.6036 |           3.8619 |
[32m[20230117 13:31:15 @agent_ppo2.py:193][0m |          -0.0025 |          13.2496 |           3.8619 |
[32m[20230117 13:31:15 @agent_ppo2.py:193][0m |          -0.0110 |          12.4802 |           3.8627 |
[32m[20230117 13:31:15 @agent_ppo2.py:193][0m |          -0.0121 |          12.3484 |           3.8635 |
[32m[20230117 13:31:15 @agent_ppo2.py:193][0m |          -0.0134 |          12.2126 |           3.8622 |
[32m[20230117 13:31:15 @agent_ppo2.py:193][0m |          -0.0116 |          12.1051 |           3.8634 |
[32m[20230117 13:31:15 @agent_ppo2.py:138][0m Policy update time: 0.88 s
[32m[20230117 13:31:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.50
[32m[20230117 13:31:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.89
[32m[20230117 13:31:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 144.52
[32m[20230117 13:31:16 @agent_ppo2.py:151][0m Total time:       4.84 min
[32m[20230117 13:31:16 @agent_ppo2.py:153][0m 397312 total steps have happened
[32m[20230117 13:31:16 @agent_ppo2.py:129][0m #------------------------ Iteration 194 --------------------------#
[32m[20230117 13:31:16 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:16 @agent_ppo2.py:193][0m |           0.0048 |          15.0610 |           3.8587 |
[32m[20230117 13:31:16 @agent_ppo2.py:193][0m |          -0.0012 |          13.9449 |           3.8463 |
[32m[20230117 13:31:16 @agent_ppo2.py:193][0m |          -0.0060 |          13.6071 |           3.8451 |
[32m[20230117 13:31:16 @agent_ppo2.py:193][0m |          -0.0062 |          13.4878 |           3.8415 |
[32m[20230117 13:31:16 @agent_ppo2.py:193][0m |          -0.0099 |          13.3844 |           3.8414 |
[32m[20230117 13:31:16 @agent_ppo2.py:193][0m |          -0.0099 |          13.3081 |           3.8385 |
[32m[20230117 13:31:16 @agent_ppo2.py:193][0m |          -0.0081 |          13.3342 |           3.8402 |
[32m[20230117 13:31:16 @agent_ppo2.py:193][0m |          -0.0068 |          13.2842 |           3.8350 |
[32m[20230117 13:31:17 @agent_ppo2.py:193][0m |          -0.0114 |          13.1883 |           3.8361 |
[32m[20230117 13:31:17 @agent_ppo2.py:193][0m |          -0.0116 |          13.1253 |           3.8350 |
[32m[20230117 13:31:17 @agent_ppo2.py:138][0m Policy update time: 0.86 s
[32m[20230117 13:31:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.02
[32m[20230117 13:31:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.24
[32m[20230117 13:31:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 146.50
[32m[20230117 13:31:17 @agent_ppo2.py:151][0m Total time:       4.86 min
[32m[20230117 13:31:17 @agent_ppo2.py:153][0m 399360 total steps have happened
[32m[20230117 13:31:17 @agent_ppo2.py:129][0m #------------------------ Iteration 195 --------------------------#
[32m[20230117 13:31:17 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:17 @agent_ppo2.py:193][0m |          -0.0010 |          24.7267 |           3.9392 |
[32m[20230117 13:31:17 @agent_ppo2.py:193][0m |          -0.0049 |          15.4371 |           3.9310 |
[32m[20230117 13:31:17 @agent_ppo2.py:193][0m |          -0.0069 |          13.7006 |           3.9286 |
[32m[20230117 13:31:17 @agent_ppo2.py:193][0m |          -0.0082 |          12.6610 |           3.9275 |
[32m[20230117 13:31:18 @agent_ppo2.py:193][0m |          -0.0101 |          12.0062 |           3.9279 |
[32m[20230117 13:31:18 @agent_ppo2.py:193][0m |          -0.0105 |          11.5067 |           3.9268 |
[32m[20230117 13:31:18 @agent_ppo2.py:193][0m |          -0.0114 |          11.1268 |           3.9240 |
[32m[20230117 13:31:18 @agent_ppo2.py:193][0m |          -0.0122 |          10.7834 |           3.9250 |
[32m[20230117 13:31:18 @agent_ppo2.py:193][0m |          -0.0111 |          10.4352 |           3.9248 |
[32m[20230117 13:31:18 @agent_ppo2.py:193][0m |          -0.0124 |          10.2123 |           3.9239 |
[32m[20230117 13:31:18 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:31:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: 205.91
[32m[20230117 13:31:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.74
[32m[20230117 13:31:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 279.93
[32m[20230117 13:31:18 @agent_ppo2.py:151][0m Total time:       4.88 min
[32m[20230117 13:31:18 @agent_ppo2.py:153][0m 401408 total steps have happened
[32m[20230117 13:31:18 @agent_ppo2.py:129][0m #------------------------ Iteration 196 --------------------------#
[32m[20230117 13:31:18 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:18 @agent_ppo2.py:193][0m |           0.0016 |          14.5941 |           3.8715 |
[32m[20230117 13:31:18 @agent_ppo2.py:193][0m |          -0.0034 |          13.3887 |           3.8660 |
[32m[20230117 13:31:19 @agent_ppo2.py:193][0m |          -0.0062 |          12.9325 |           3.8665 |
[32m[20230117 13:31:19 @agent_ppo2.py:193][0m |          -0.0095 |          12.5294 |           3.8625 |
[32m[20230117 13:31:19 @agent_ppo2.py:193][0m |          -0.0072 |          13.0227 |           3.8622 |
[32m[20230117 13:31:19 @agent_ppo2.py:193][0m |          -0.0114 |          12.1135 |           3.8610 |
[32m[20230117 13:31:19 @agent_ppo2.py:193][0m |          -0.0140 |          11.9147 |           3.8642 |
[32m[20230117 13:31:19 @agent_ppo2.py:193][0m |          -0.0132 |          11.6895 |           3.8622 |
[32m[20230117 13:31:19 @agent_ppo2.py:193][0m |          -0.0135 |          11.5586 |           3.8636 |
[32m[20230117 13:31:19 @agent_ppo2.py:193][0m |          -0.0126 |          11.4042 |           3.8624 |
[32m[20230117 13:31:19 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:31:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.17
[32m[20230117 13:31:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.82
[32m[20230117 13:31:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 279.19
[32m[20230117 13:31:19 @agent_ppo2.py:151][0m Total time:       4.91 min
[32m[20230117 13:31:19 @agent_ppo2.py:153][0m 403456 total steps have happened
[32m[20230117 13:31:19 @agent_ppo2.py:129][0m #------------------------ Iteration 197 --------------------------#
[32m[20230117 13:31:20 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:31:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:20 @agent_ppo2.py:193][0m |          -0.0022 |          13.2102 |           3.9019 |
[32m[20230117 13:31:20 @agent_ppo2.py:193][0m |          -0.0038 |          11.2426 |           3.8947 |
[32m[20230117 13:31:20 @agent_ppo2.py:193][0m |          -0.0075 |          10.6270 |           3.8905 |
[32m[20230117 13:31:20 @agent_ppo2.py:193][0m |          -0.0084 |          10.2535 |           3.8922 |
[32m[20230117 13:31:20 @agent_ppo2.py:193][0m |          -0.0040 |          10.4068 |           3.8863 |
[32m[20230117 13:31:20 @agent_ppo2.py:193][0m |          -0.0094 |           9.7231 |           3.8894 |
[32m[20230117 13:31:20 @agent_ppo2.py:193][0m |          -0.0036 |           9.5601 |           3.8828 |
[32m[20230117 13:31:20 @agent_ppo2.py:193][0m |          -0.0079 |           9.2148 |           3.8818 |
[32m[20230117 13:31:20 @agent_ppo2.py:193][0m |          -0.0101 |           9.0343 |           3.8831 |
[32m[20230117 13:31:20 @agent_ppo2.py:193][0m |          -0.0110 |           8.8057 |           3.8816 |
[32m[20230117 13:31:20 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:31:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.53
[32m[20230117 13:31:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.01
[32m[20230117 13:31:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.69
[32m[20230117 13:31:21 @agent_ppo2.py:151][0m Total time:       4.93 min
[32m[20230117 13:31:21 @agent_ppo2.py:153][0m 405504 total steps have happened
[32m[20230117 13:31:21 @agent_ppo2.py:129][0m #------------------------ Iteration 198 --------------------------#
[32m[20230117 13:31:21 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:31:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:21 @agent_ppo2.py:193][0m |          -0.0004 |          16.2652 |           3.8698 |
[32m[20230117 13:31:21 @agent_ppo2.py:193][0m |          -0.0058 |          14.5857 |           3.8627 |
[32m[20230117 13:31:21 @agent_ppo2.py:193][0m |          -0.0058 |          14.4251 |           3.8590 |
[32m[20230117 13:31:21 @agent_ppo2.py:193][0m |          -0.0084 |          13.8916 |           3.8608 |
[32m[20230117 13:31:21 @agent_ppo2.py:193][0m |          -0.0099 |          13.6849 |           3.8582 |
[32m[20230117 13:31:21 @agent_ppo2.py:193][0m |          -0.0103 |          13.5133 |           3.8562 |
[32m[20230117 13:31:21 @agent_ppo2.py:193][0m |          -0.0099 |          13.4720 |           3.8520 |
[32m[20230117 13:31:22 @agent_ppo2.py:193][0m |          -0.0108 |          13.3549 |           3.8545 |
[32m[20230117 13:31:22 @agent_ppo2.py:193][0m |          -0.0118 |          13.0856 |           3.8514 |
[32m[20230117 13:31:22 @agent_ppo2.py:193][0m |          -0.0111 |          12.8989 |           3.8492 |
[32m[20230117 13:31:22 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:31:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.00
[32m[20230117 13:31:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.26
[32m[20230117 13:31:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 56.41
[32m[20230117 13:31:22 @agent_ppo2.py:151][0m Total time:       4.95 min
[32m[20230117 13:31:22 @agent_ppo2.py:153][0m 407552 total steps have happened
[32m[20230117 13:31:22 @agent_ppo2.py:129][0m #------------------------ Iteration 199 --------------------------#
[32m[20230117 13:31:22 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:31:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:22 @agent_ppo2.py:193][0m |           0.0027 |          39.3745 |           3.7959 |
[32m[20230117 13:31:22 @agent_ppo2.py:193][0m |          -0.0020 |          21.7270 |           3.7891 |
[32m[20230117 13:31:22 @agent_ppo2.py:193][0m |          -0.0120 |          19.2974 |           3.7887 |
[32m[20230117 13:31:22 @agent_ppo2.py:193][0m |          -0.0050 |          17.9574 |           3.7872 |
[32m[20230117 13:31:23 @agent_ppo2.py:193][0m |          -0.0049 |          18.0151 |           3.7898 |
[32m[20230117 13:31:23 @agent_ppo2.py:193][0m |          -0.0088 |          16.8235 |           3.7820 |
[32m[20230117 13:31:23 @agent_ppo2.py:193][0m |          -0.0078 |          16.2356 |           3.7833 |
[32m[20230117 13:31:23 @agent_ppo2.py:193][0m |          -0.0058 |          15.8587 |           3.7791 |
[32m[20230117 13:31:23 @agent_ppo2.py:193][0m |          -0.0112 |          15.6222 |           3.7817 |
[32m[20230117 13:31:23 @agent_ppo2.py:193][0m |           0.0012 |          15.4685 |           3.7782 |
[32m[20230117 13:31:23 @agent_ppo2.py:138][0m Policy update time: 0.82 s
[32m[20230117 13:31:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: 168.85
[32m[20230117 13:31:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.99
[32m[20230117 13:31:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 8.47
[32m[20230117 13:31:23 @agent_ppo2.py:151][0m Total time:       4.97 min
[32m[20230117 13:31:23 @agent_ppo2.py:153][0m 409600 total steps have happened
[32m[20230117 13:31:23 @agent_ppo2.py:129][0m #------------------------ Iteration 200 --------------------------#
[32m[20230117 13:31:23 @agent_ppo2.py:135][0m Sampling time: 0.29 s by 4 slaves
[32m[20230117 13:31:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:24 @agent_ppo2.py:193][0m |          -0.0001 |          17.1813 |           3.8250 |
[32m[20230117 13:31:24 @agent_ppo2.py:193][0m |          -0.0089 |          14.7481 |           3.8258 |
[32m[20230117 13:31:24 @agent_ppo2.py:193][0m |          -0.0038 |          13.9478 |           3.8220 |
[32m[20230117 13:31:24 @agent_ppo2.py:193][0m |          -0.0030 |          13.7644 |           3.8213 |
[32m[20230117 13:31:24 @agent_ppo2.py:193][0m |          -0.0075 |          12.8938 |           3.8223 |
[32m[20230117 13:31:24 @agent_ppo2.py:193][0m |          -0.0001 |          12.9037 |           3.8235 |
[32m[20230117 13:31:24 @agent_ppo2.py:193][0m |          -0.0117 |          12.2243 |           3.8229 |
[32m[20230117 13:31:24 @agent_ppo2.py:193][0m |          -0.0090 |          11.9479 |           3.8249 |
[32m[20230117 13:31:24 @agent_ppo2.py:193][0m |           0.0037 |          12.0234 |           3.8245 |
[32m[20230117 13:31:24 @agent_ppo2.py:193][0m |          -0.0039 |          11.8990 |           3.8243 |
[32m[20230117 13:31:24 @agent_ppo2.py:138][0m Policy update time: 0.83 s
[32m[20230117 13:31:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.04
[32m[20230117 13:31:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.67
[32m[20230117 13:31:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.86
[32m[20230117 13:31:24 @agent_ppo2.py:151][0m Total time:       4.99 min
[32m[20230117 13:31:24 @agent_ppo2.py:153][0m 411648 total steps have happened
[32m[20230117 13:31:24 @agent_ppo2.py:129][0m #------------------------ Iteration 201 --------------------------#
[32m[20230117 13:31:25 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:25 @agent_ppo2.py:193][0m |           0.0014 |          31.3284 |           3.8683 |
[32m[20230117 13:31:25 @agent_ppo2.py:193][0m |          -0.0052 |          24.9068 |           3.8614 |
[32m[20230117 13:31:25 @agent_ppo2.py:193][0m |          -0.0083 |          23.3281 |           3.8561 |
[32m[20230117 13:31:25 @agent_ppo2.py:193][0m |          -0.0116 |          22.4026 |           3.8568 |
[32m[20230117 13:31:25 @agent_ppo2.py:193][0m |          -0.0108 |          21.7344 |           3.8566 |
[32m[20230117 13:31:25 @agent_ppo2.py:193][0m |          -0.0133 |          21.4828 |           3.8564 |
[32m[20230117 13:31:25 @agent_ppo2.py:193][0m |          -0.0127 |          21.0836 |           3.8555 |
[32m[20230117 13:31:25 @agent_ppo2.py:193][0m |          -0.0131 |          20.6859 |           3.8570 |
[32m[20230117 13:31:25 @agent_ppo2.py:193][0m |          -0.0143 |          20.3042 |           3.8577 |
[32m[20230117 13:31:25 @agent_ppo2.py:193][0m |          -0.0134 |          19.8869 |           3.8569 |
[32m[20230117 13:31:25 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230117 13:31:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 134.28
[32m[20230117 13:31:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 260.84
[32m[20230117 13:31:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 107.51
[32m[20230117 13:31:26 @agent_ppo2.py:151][0m Total time:       5.01 min
[32m[20230117 13:31:26 @agent_ppo2.py:153][0m 413696 total steps have happened
[32m[20230117 13:31:26 @agent_ppo2.py:129][0m #------------------------ Iteration 202 --------------------------#
[32m[20230117 13:31:26 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:31:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:26 @agent_ppo2.py:193][0m |           0.0016 |          22.1047 |           3.9132 |
[32m[20230117 13:31:26 @agent_ppo2.py:193][0m |          -0.0023 |          18.1377 |           3.9158 |
[32m[20230117 13:31:26 @agent_ppo2.py:193][0m |          -0.0056 |          17.4978 |           3.9126 |
[32m[20230117 13:31:26 @agent_ppo2.py:193][0m |          -0.0033 |          17.3345 |           3.9150 |
[32m[20230117 13:31:26 @agent_ppo2.py:193][0m |          -0.0060 |          16.8174 |           3.9104 |
[32m[20230117 13:31:26 @agent_ppo2.py:193][0m |          -0.0075 |          16.5518 |           3.9113 |
[32m[20230117 13:31:26 @agent_ppo2.py:193][0m |          -0.0069 |          16.3615 |           3.9096 |
[32m[20230117 13:31:26 @agent_ppo2.py:193][0m |          -0.0079 |          16.0744 |           3.9105 |
[32m[20230117 13:31:27 @agent_ppo2.py:193][0m |          -0.0085 |          15.8821 |           3.9128 |
[32m[20230117 13:31:27 @agent_ppo2.py:193][0m |          -0.0093 |          15.7035 |           3.9102 |
[32m[20230117 13:31:27 @agent_ppo2.py:138][0m Policy update time: 0.85 s
[32m[20230117 13:31:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 260.02
[32m[20230117 13:31:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.92
[32m[20230117 13:31:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 90.07
[32m[20230117 13:31:27 @agent_ppo2.py:151][0m Total time:       5.03 min
[32m[20230117 13:31:27 @agent_ppo2.py:153][0m 415744 total steps have happened
[32m[20230117 13:31:27 @agent_ppo2.py:129][0m #------------------------ Iteration 203 --------------------------#
[32m[20230117 13:31:27 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:31:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:27 @agent_ppo2.py:193][0m |           0.0002 |          22.1783 |           3.8427 |
[32m[20230117 13:31:27 @agent_ppo2.py:193][0m |          -0.0044 |          18.2760 |           3.8316 |
[32m[20230117 13:31:27 @agent_ppo2.py:193][0m |          -0.0070 |          17.2596 |           3.8302 |
[32m[20230117 13:31:27 @agent_ppo2.py:193][0m |          -0.0074 |          16.9027 |           3.8329 |
[32m[20230117 13:31:27 @agent_ppo2.py:193][0m |          -0.0077 |          16.4811 |           3.8279 |
[32m[20230117 13:31:27 @agent_ppo2.py:193][0m |          -0.0087 |          16.0873 |           3.8290 |
[32m[20230117 13:31:28 @agent_ppo2.py:193][0m |          -0.0099 |          15.8743 |           3.8306 |
[32m[20230117 13:31:28 @agent_ppo2.py:193][0m |          -0.0100 |          15.5811 |           3.8250 |
[32m[20230117 13:31:28 @agent_ppo2.py:193][0m |          -0.0105 |          15.4952 |           3.8255 |
[32m[20230117 13:31:28 @agent_ppo2.py:193][0m |          -0.0110 |          15.1863 |           3.8279 |
[32m[20230117 13:31:28 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:31:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 204.22
[32m[20230117 13:31:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.53
[32m[20230117 13:31:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.66
[32m[20230117 13:31:28 @agent_ppo2.py:151][0m Total time:       5.05 min
[32m[20230117 13:31:28 @agent_ppo2.py:153][0m 417792 total steps have happened
[32m[20230117 13:31:28 @agent_ppo2.py:129][0m #------------------------ Iteration 204 --------------------------#
[32m[20230117 13:31:28 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:31:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:28 @agent_ppo2.py:193][0m |           0.0010 |          15.5310 |           3.8659 |
[32m[20230117 13:31:28 @agent_ppo2.py:193][0m |          -0.0051 |          12.7070 |           3.8626 |
[32m[20230117 13:31:29 @agent_ppo2.py:193][0m |          -0.0049 |          11.9761 |           3.8579 |
[32m[20230117 13:31:29 @agent_ppo2.py:193][0m |          -0.0071 |          11.5378 |           3.8591 |
[32m[20230117 13:31:29 @agent_ppo2.py:193][0m |          -0.0078 |          11.2384 |           3.8573 |
[32m[20230117 13:31:29 @agent_ppo2.py:193][0m |          -0.0086 |          10.9384 |           3.8576 |
[32m[20230117 13:31:29 @agent_ppo2.py:193][0m |          -0.0079 |          10.6994 |           3.8617 |
[32m[20230117 13:31:29 @agent_ppo2.py:193][0m |          -0.0096 |          10.5277 |           3.8594 |
[32m[20230117 13:31:29 @agent_ppo2.py:193][0m |          -0.0099 |          10.2895 |           3.8646 |
[32m[20230117 13:31:29 @agent_ppo2.py:193][0m |          -0.0102 |          10.1330 |           3.8638 |
[32m[20230117 13:31:29 @agent_ppo2.py:138][0m Policy update time: 0.91 s
[32m[20230117 13:31:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 259.68
[32m[20230117 13:31:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.08
[32m[20230117 13:31:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.44
[32m[20230117 13:31:29 @agent_ppo2.py:151][0m Total time:       5.07 min
[32m[20230117 13:31:29 @agent_ppo2.py:153][0m 419840 total steps have happened
[32m[20230117 13:31:29 @agent_ppo2.py:129][0m #------------------------ Iteration 205 --------------------------#
[32m[20230117 13:31:30 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:31:30 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:30 @agent_ppo2.py:193][0m |          -0.0000 |          13.3878 |           3.8658 |
[32m[20230117 13:31:30 @agent_ppo2.py:193][0m |          -0.0059 |          11.6671 |           3.8557 |
[32m[20230117 13:31:30 @agent_ppo2.py:193][0m |          -0.0061 |          11.0318 |           3.8492 |
[32m[20230117 13:31:30 @agent_ppo2.py:193][0m |          -0.0070 |          10.5809 |           3.8540 |
[32m[20230117 13:31:30 @agent_ppo2.py:193][0m |          -0.0070 |          10.2217 |           3.8513 |
[32m[20230117 13:31:30 @agent_ppo2.py:193][0m |          -0.0092 |           9.7184 |           3.8520 |
[32m[20230117 13:31:30 @agent_ppo2.py:193][0m |          -0.0091 |           9.4028 |           3.8576 |
[32m[20230117 13:31:30 @agent_ppo2.py:193][0m |          -0.0096 |           9.0943 |           3.8548 |
[32m[20230117 13:31:30 @agent_ppo2.py:193][0m |          -0.0109 |           8.8697 |           3.8508 |
[32m[20230117 13:31:30 @agent_ppo2.py:193][0m |          -0.0110 |           8.6227 |           3.8557 |
[32m[20230117 13:31:30 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:31:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 258.12
[32m[20230117 13:31:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.72
[32m[20230117 13:31:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.73
[32m[20230117 13:31:31 @agent_ppo2.py:151][0m Total time:       5.09 min
[32m[20230117 13:31:31 @agent_ppo2.py:153][0m 421888 total steps have happened
[32m[20230117 13:31:31 @agent_ppo2.py:129][0m #------------------------ Iteration 206 --------------------------#
[32m[20230117 13:31:31 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:31:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:31 @agent_ppo2.py:193][0m |          -0.0007 |          21.8090 |           3.9459 |
[32m[20230117 13:31:31 @agent_ppo2.py:193][0m |          -0.0068 |          16.9004 |           3.9384 |
[32m[20230117 13:31:31 @agent_ppo2.py:193][0m |          -0.0078 |          15.4140 |           3.9385 |
[32m[20230117 13:31:31 @agent_ppo2.py:193][0m |          -0.0085 |          14.8931 |           3.9356 |
[32m[20230117 13:31:31 @agent_ppo2.py:193][0m |          -0.0102 |          14.1978 |           3.9371 |
[32m[20230117 13:31:31 @agent_ppo2.py:193][0m |          -0.0111 |          13.7829 |           3.9380 |
[32m[20230117 13:31:32 @agent_ppo2.py:193][0m |          -0.0116 |          13.5083 |           3.9359 |
[32m[20230117 13:31:32 @agent_ppo2.py:193][0m |          -0.0125 |          13.2485 |           3.9359 |
[32m[20230117 13:31:32 @agent_ppo2.py:193][0m |          -0.0118 |          13.0219 |           3.9328 |
[32m[20230117 13:31:32 @agent_ppo2.py:193][0m |          -0.0137 |          12.7526 |           3.9337 |
[32m[20230117 13:31:32 @agent_ppo2.py:138][0m Policy update time: 0.90 s
[32m[20230117 13:31:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 197.10
[32m[20230117 13:31:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.41
[32m[20230117 13:31:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 92.53
[32m[20230117 13:31:32 @agent_ppo2.py:151][0m Total time:       5.12 min
[32m[20230117 13:31:32 @agent_ppo2.py:153][0m 423936 total steps have happened
[32m[20230117 13:31:32 @agent_ppo2.py:129][0m #------------------------ Iteration 207 --------------------------#
[32m[20230117 13:31:32 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:32 @agent_ppo2.py:193][0m |          -0.0018 |          15.6616 |           3.9824 |
[32m[20230117 13:31:32 @agent_ppo2.py:193][0m |          -0.0084 |          14.3585 |           3.9778 |
[32m[20230117 13:31:33 @agent_ppo2.py:193][0m |          -0.0067 |          13.9525 |           3.9721 |
[32m[20230117 13:31:33 @agent_ppo2.py:193][0m |          -0.0071 |          13.6997 |           3.9706 |
[32m[20230117 13:31:33 @agent_ppo2.py:193][0m |          -0.0111 |          13.4604 |           3.9671 |
[32m[20230117 13:31:33 @agent_ppo2.py:193][0m |          -0.0112 |          13.1993 |           3.9653 |
[32m[20230117 13:31:33 @agent_ppo2.py:193][0m |          -0.0086 |          13.0101 |           3.9660 |
[32m[20230117 13:31:33 @agent_ppo2.py:193][0m |          -0.0128 |          12.8551 |           3.9621 |
[32m[20230117 13:31:33 @agent_ppo2.py:193][0m |          -0.0099 |          12.6928 |           3.9636 |
[32m[20230117 13:31:33 @agent_ppo2.py:193][0m |          -0.0124 |          12.5859 |           3.9625 |
[32m[20230117 13:31:33 @agent_ppo2.py:138][0m Policy update time: 0.83 s
[32m[20230117 13:31:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.38
[32m[20230117 13:31:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.04
[32m[20230117 13:31:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 135.99
[32m[20230117 13:31:33 @agent_ppo2.py:151][0m Total time:       5.14 min
[32m[20230117 13:31:33 @agent_ppo2.py:153][0m 425984 total steps have happened
[32m[20230117 13:31:33 @agent_ppo2.py:129][0m #------------------------ Iteration 208 --------------------------#
[32m[20230117 13:31:34 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:31:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:34 @agent_ppo2.py:193][0m |           0.0032 |          13.5603 |           3.8397 |
[32m[20230117 13:31:34 @agent_ppo2.py:193][0m |          -0.0086 |          12.2927 |           3.8307 |
[32m[20230117 13:31:34 @agent_ppo2.py:193][0m |          -0.0200 |          11.8538 |           3.8350 |
[32m[20230117 13:31:34 @agent_ppo2.py:193][0m |          -0.0077 |          11.4305 |           3.8265 |
[32m[20230117 13:31:34 @agent_ppo2.py:193][0m |          -0.0043 |          11.1520 |           3.8337 |
[32m[20230117 13:31:34 @agent_ppo2.py:193][0m |          -0.0160 |          10.8788 |           3.8329 |
[32m[20230117 13:31:34 @agent_ppo2.py:193][0m |          -0.0109 |          10.7079 |           3.8272 |
[32m[20230117 13:31:34 @agent_ppo2.py:193][0m |          -0.0229 |          10.7647 |           3.8281 |
[32m[20230117 13:31:34 @agent_ppo2.py:193][0m |          -0.0323 |          10.6291 |           3.8268 |
[32m[20230117 13:31:34 @agent_ppo2.py:193][0m |          -0.0104 |          10.2661 |           3.8278 |
[32m[20230117 13:31:34 @agent_ppo2.py:138][0m Policy update time: 0.82 s
[32m[20230117 13:31:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.48
[32m[20230117 13:31:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.00
[32m[20230117 13:31:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.07
[32m[20230117 13:31:35 @agent_ppo2.py:151][0m Total time:       5.16 min
[32m[20230117 13:31:35 @agent_ppo2.py:153][0m 428032 total steps have happened
[32m[20230117 13:31:35 @agent_ppo2.py:129][0m #------------------------ Iteration 209 --------------------------#
[32m[20230117 13:31:35 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:31:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:35 @agent_ppo2.py:193][0m |           0.0011 |          16.5822 |           3.9507 |
[32m[20230117 13:31:35 @agent_ppo2.py:193][0m |          -0.0004 |          15.5789 |           3.9495 |
[32m[20230117 13:31:35 @agent_ppo2.py:193][0m |          -0.0029 |          14.8963 |           3.9495 |
[32m[20230117 13:31:35 @agent_ppo2.py:193][0m |          -0.0035 |          14.5452 |           3.9450 |
[32m[20230117 13:31:35 @agent_ppo2.py:193][0m |          -0.0062 |          14.2377 |           3.9491 |
[32m[20230117 13:31:35 @agent_ppo2.py:193][0m |          -0.0013 |          14.2757 |           3.9455 |
[32m[20230117 13:31:35 @agent_ppo2.py:193][0m |          -0.0090 |          13.8342 |           3.9478 |
[32m[20230117 13:31:35 @agent_ppo2.py:193][0m |          -0.0059 |          13.8386 |           3.9485 |
[32m[20230117 13:31:36 @agent_ppo2.py:193][0m |          -0.0051 |          13.9075 |           3.9462 |
[32m[20230117 13:31:36 @agent_ppo2.py:193][0m |          -0.0084 |          13.2930 |           3.9491 |
[32m[20230117 13:31:36 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:31:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.42
[32m[20230117 13:31:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.28
[32m[20230117 13:31:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 174.35
[32m[20230117 13:31:36 @agent_ppo2.py:151][0m Total time:       5.18 min
[32m[20230117 13:31:36 @agent_ppo2.py:153][0m 430080 total steps have happened
[32m[20230117 13:31:36 @agent_ppo2.py:129][0m #------------------------ Iteration 210 --------------------------#
[32m[20230117 13:31:36 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:36 @agent_ppo2.py:193][0m |          -0.0032 |          48.4369 |           3.9351 |
[32m[20230117 13:31:36 @agent_ppo2.py:193][0m |          -0.0039 |          31.8590 |           3.9314 |
[32m[20230117 13:31:36 @agent_ppo2.py:193][0m |          -0.0106 |          29.3005 |           3.9292 |
[32m[20230117 13:31:36 @agent_ppo2.py:193][0m |          -0.0074 |          27.9551 |           3.9241 |
[32m[20230117 13:31:36 @agent_ppo2.py:193][0m |          -0.0136 |          27.6241 |           3.9250 |
[32m[20230117 13:31:36 @agent_ppo2.py:193][0m |          -0.0015 |          29.3636 |           3.9250 |
[32m[20230117 13:31:37 @agent_ppo2.py:193][0m |          -0.0074 |          26.0476 |           3.9189 |
[32m[20230117 13:31:37 @agent_ppo2.py:193][0m |          -0.0164 |          23.9508 |           3.9199 |
[32m[20230117 13:31:37 @agent_ppo2.py:193][0m |          -0.0187 |          23.5920 |           3.9166 |
[32m[20230117 13:31:37 @agent_ppo2.py:193][0m |          -0.0171 |          22.9608 |           3.9137 |
[32m[20230117 13:31:37 @agent_ppo2.py:138][0m Policy update time: 0.62 s
[32m[20230117 13:31:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 60.29
[32m[20230117 13:31:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 255.83
[32m[20230117 13:31:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -11.07
[32m[20230117 13:31:37 @agent_ppo2.py:151][0m Total time:       5.20 min
[32m[20230117 13:31:37 @agent_ppo2.py:153][0m 432128 total steps have happened
[32m[20230117 13:31:37 @agent_ppo2.py:129][0m #------------------------ Iteration 211 --------------------------#
[32m[20230117 13:31:37 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:31:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:37 @agent_ppo2.py:193][0m |          -0.0003 |          36.7479 |           3.9071 |
[32m[20230117 13:31:37 @agent_ppo2.py:193][0m |          -0.0065 |          27.3108 |           3.9066 |
[32m[20230117 13:31:37 @agent_ppo2.py:193][0m |          -0.0082 |          24.8057 |           3.9044 |
[32m[20230117 13:31:38 @agent_ppo2.py:193][0m |          -0.0089 |          23.1823 |           3.9084 |
[32m[20230117 13:31:38 @agent_ppo2.py:193][0m |          -0.0105 |          22.0386 |           3.9066 |
[32m[20230117 13:31:38 @agent_ppo2.py:193][0m |          -0.0106 |          21.1331 |           3.9047 |
[32m[20230117 13:31:38 @agent_ppo2.py:193][0m |          -0.0106 |          20.3257 |           3.9055 |
[32m[20230117 13:31:38 @agent_ppo2.py:193][0m |          -0.0118 |          19.5796 |           3.9078 |
[32m[20230117 13:31:38 @agent_ppo2.py:193][0m |          -0.0122 |          19.0120 |           3.9080 |
[32m[20230117 13:31:38 @agent_ppo2.py:193][0m |          -0.0125 |          18.5680 |           3.9074 |
[32m[20230117 13:31:38 @agent_ppo2.py:138][0m Policy update time: 0.83 s
[32m[20230117 13:31:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 160.35
[32m[20230117 13:31:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.24
[32m[20230117 13:31:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -17.93
[32m[20230117 13:31:38 @agent_ppo2.py:151][0m Total time:       5.22 min
[32m[20230117 13:31:38 @agent_ppo2.py:153][0m 434176 total steps have happened
[32m[20230117 13:31:38 @agent_ppo2.py:129][0m #------------------------ Iteration 212 --------------------------#
[32m[20230117 13:31:38 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:31:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:39 @agent_ppo2.py:193][0m |          -0.0011 |          55.2735 |           3.9731 |
[32m[20230117 13:31:39 @agent_ppo2.py:193][0m |          -0.0055 |          47.3806 |           3.9674 |
[32m[20230117 13:31:39 @agent_ppo2.py:193][0m |          -0.0076 |          45.5949 |           3.9689 |
[32m[20230117 13:31:39 @agent_ppo2.py:193][0m |          -0.0097 |          43.0197 |           3.9692 |
[32m[20230117 13:31:39 @agent_ppo2.py:193][0m |          -0.0082 |          41.1692 |           3.9684 |
[32m[20230117 13:31:39 @agent_ppo2.py:193][0m |          -0.0102 |          39.6221 |           3.9687 |
[32m[20230117 13:31:39 @agent_ppo2.py:193][0m |          -0.0095 |          39.3006 |           3.9688 |
[32m[20230117 13:31:39 @agent_ppo2.py:193][0m |          -0.0081 |          38.3755 |           3.9689 |
[32m[20230117 13:31:39 @agent_ppo2.py:193][0m |          -0.0104 |          37.6757 |           3.9671 |
[32m[20230117 13:31:39 @agent_ppo2.py:193][0m |          -0.0094 |          37.5706 |           3.9700 |
[32m[20230117 13:31:39 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:31:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 142.43
[32m[20230117 13:31:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.69
[32m[20230117 13:31:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 88.31
[32m[20230117 13:31:39 @agent_ppo2.py:151][0m Total time:       5.24 min
[32m[20230117 13:31:39 @agent_ppo2.py:153][0m 436224 total steps have happened
[32m[20230117 13:31:39 @agent_ppo2.py:129][0m #------------------------ Iteration 213 --------------------------#
[32m[20230117 13:31:40 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:31:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:40 @agent_ppo2.py:193][0m |          -0.0008 |          37.8841 |           4.0154 |
[32m[20230117 13:31:40 @agent_ppo2.py:193][0m |          -0.0062 |          29.1045 |           4.0143 |
[32m[20230117 13:31:40 @agent_ppo2.py:193][0m |          -0.0076 |          25.0712 |           4.0101 |
[32m[20230117 13:31:40 @agent_ppo2.py:193][0m |          -0.0087 |          23.2617 |           4.0076 |
[32m[20230117 13:31:40 @agent_ppo2.py:193][0m |          -0.0090 |          22.0424 |           4.0068 |
[32m[20230117 13:31:40 @agent_ppo2.py:193][0m |          -0.0103 |          21.4378 |           4.0053 |
[32m[20230117 13:31:40 @agent_ppo2.py:193][0m |          -0.0105 |          21.0511 |           4.0031 |
[32m[20230117 13:31:40 @agent_ppo2.py:193][0m |          -0.0108 |          20.6992 |           4.0057 |
[32m[20230117 13:31:40 @agent_ppo2.py:193][0m |          -0.0118 |          20.4521 |           4.0029 |
[32m[20230117 13:31:41 @agent_ppo2.py:193][0m |          -0.0121 |          20.1740 |           4.0067 |
[32m[20230117 13:31:41 @agent_ppo2.py:138][0m Policy update time: 0.88 s
[32m[20230117 13:31:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 202.92
[32m[20230117 13:31:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.22
[32m[20230117 13:31:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 47.88
[32m[20230117 13:31:41 @agent_ppo2.py:151][0m Total time:       5.26 min
[32m[20230117 13:31:41 @agent_ppo2.py:153][0m 438272 total steps have happened
[32m[20230117 13:31:41 @agent_ppo2.py:129][0m #------------------------ Iteration 214 --------------------------#
[32m[20230117 13:31:41 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:31:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:41 @agent_ppo2.py:193][0m |          -0.0011 |          52.0055 |           3.9264 |
[32m[20230117 13:31:41 @agent_ppo2.py:193][0m |          -0.0050 |          39.5854 |           3.9178 |
[32m[20230117 13:31:41 @agent_ppo2.py:193][0m |          -0.0064 |          26.9138 |           3.9202 |
[32m[20230117 13:31:41 @agent_ppo2.py:193][0m |          -0.0064 |          20.9629 |           3.9209 |
[32m[20230117 13:31:41 @agent_ppo2.py:193][0m |          -0.0085 |          19.4546 |           3.9175 |
[32m[20230117 13:31:41 @agent_ppo2.py:193][0m |          -0.0092 |          19.0416 |           3.9226 |
[32m[20230117 13:31:42 @agent_ppo2.py:193][0m |          -0.0093 |          18.3081 |           3.9213 |
[32m[20230117 13:31:42 @agent_ppo2.py:193][0m |          -0.0077 |          18.7304 |           3.9226 |
[32m[20230117 13:31:42 @agent_ppo2.py:193][0m |          -0.0139 |          17.6275 |           3.9254 |
[32m[20230117 13:31:42 @agent_ppo2.py:193][0m |          -0.0119 |          17.3580 |           3.9249 |
[32m[20230117 13:31:42 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:31:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 198.11
[32m[20230117 13:31:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.95
[32m[20230117 13:31:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.03
[32m[20230117 13:31:42 @agent_ppo2.py:151][0m Total time:       5.28 min
[32m[20230117 13:31:42 @agent_ppo2.py:153][0m 440320 total steps have happened
[32m[20230117 13:31:42 @agent_ppo2.py:129][0m #------------------------ Iteration 215 --------------------------#
[32m[20230117 13:31:42 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:31:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:42 @agent_ppo2.py:193][0m |          -0.0008 |          23.2594 |           3.9692 |
[32m[20230117 13:31:43 @agent_ppo2.py:193][0m |          -0.0027 |          15.8054 |           3.9649 |
[32m[20230117 13:31:43 @agent_ppo2.py:193][0m |          -0.0094 |          14.2358 |           3.9667 |
[32m[20230117 13:31:43 @agent_ppo2.py:193][0m |          -0.0096 |          13.4391 |           3.9681 |
[32m[20230117 13:31:43 @agent_ppo2.py:193][0m |          -0.0052 |          13.1914 |           3.9672 |
[32m[20230117 13:31:43 @agent_ppo2.py:193][0m |          -0.0086 |          12.5688 |           3.9676 |
[32m[20230117 13:31:43 @agent_ppo2.py:193][0m |          -0.0110 |          12.1590 |           3.9694 |
[32m[20230117 13:31:43 @agent_ppo2.py:193][0m |          -0.0072 |          11.8959 |           3.9710 |
[32m[20230117 13:31:43 @agent_ppo2.py:193][0m |          -0.0127 |          11.6729 |           3.9692 |
[32m[20230117 13:31:43 @agent_ppo2.py:193][0m |          -0.0133 |          11.4950 |           3.9670 |
[32m[20230117 13:31:43 @agent_ppo2.py:138][0m Policy update time: 0.87 s
[32m[20230117 13:31:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 195.66
[32m[20230117 13:31:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.96
[32m[20230117 13:31:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.49
[32m[20230117 13:31:43 @agent_ppo2.py:151][0m Total time:       5.31 min
[32m[20230117 13:31:43 @agent_ppo2.py:153][0m 442368 total steps have happened
[32m[20230117 13:31:43 @agent_ppo2.py:129][0m #------------------------ Iteration 216 --------------------------#
[32m[20230117 13:31:44 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:31:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:44 @agent_ppo2.py:193][0m |          -0.0002 |          13.9126 |           3.8827 |
[32m[20230117 13:31:44 @agent_ppo2.py:193][0m |          -0.0039 |          13.5052 |           3.8749 |
[32m[20230117 13:31:44 @agent_ppo2.py:193][0m |          -0.0051 |          13.2965 |           3.8736 |
[32m[20230117 13:31:44 @agent_ppo2.py:193][0m |          -0.0055 |          13.2151 |           3.8783 |
[32m[20230117 13:31:44 @agent_ppo2.py:193][0m |          -0.0067 |          13.1090 |           3.8757 |
[32m[20230117 13:31:44 @agent_ppo2.py:193][0m |          -0.0074 |          13.0493 |           3.8772 |
[32m[20230117 13:31:44 @agent_ppo2.py:193][0m |          -0.0080 |          12.9925 |           3.8745 |
[32m[20230117 13:31:44 @agent_ppo2.py:193][0m |          -0.0084 |          12.9383 |           3.8824 |
[32m[20230117 13:31:44 @agent_ppo2.py:193][0m |          -0.0093 |          12.8729 |           3.8802 |
[32m[20230117 13:31:44 @agent_ppo2.py:193][0m |          -0.0096 |          12.8347 |           3.8771 |
[32m[20230117 13:31:44 @agent_ppo2.py:138][0m Policy update time: 0.83 s
[32m[20230117 13:31:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.54
[32m[20230117 13:31:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.42
[32m[20230117 13:31:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -25.64
[32m[20230117 13:31:45 @agent_ppo2.py:151][0m Total time:       5.33 min
[32m[20230117 13:31:45 @agent_ppo2.py:153][0m 444416 total steps have happened
[32m[20230117 13:31:45 @agent_ppo2.py:129][0m #------------------------ Iteration 217 --------------------------#
[32m[20230117 13:31:45 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:45 @agent_ppo2.py:193][0m |          -0.0001 |          18.9091 |           3.8941 |
[32m[20230117 13:31:45 @agent_ppo2.py:193][0m |          -0.0111 |          12.0882 |           3.8896 |
[32m[20230117 13:31:45 @agent_ppo2.py:193][0m |           0.0219 |          11.6238 |           3.8844 |
[32m[20230117 13:31:45 @agent_ppo2.py:193][0m |          -0.0102 |          11.3318 |           3.8871 |
[32m[20230117 13:31:45 @agent_ppo2.py:193][0m |          -0.0120 |          11.0652 |           3.8833 |
[32m[20230117 13:31:45 @agent_ppo2.py:193][0m |          -0.0119 |          10.8898 |           3.8833 |
[32m[20230117 13:31:45 @agent_ppo2.py:193][0m |          -0.0109 |          10.6897 |           3.8871 |
[32m[20230117 13:31:45 @agent_ppo2.py:193][0m |          -0.0105 |          10.4621 |           3.8827 |
[32m[20230117 13:31:45 @agent_ppo2.py:193][0m |          -0.0128 |          10.3095 |           3.8828 |
[32m[20230117 13:31:46 @agent_ppo2.py:193][0m |          -0.0041 |          10.6729 |           3.8853 |
[32m[20230117 13:31:46 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:31:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 199.42
[32m[20230117 13:31:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.08
[32m[20230117 13:31:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -24.15
[32m[20230117 13:31:46 @agent_ppo2.py:151][0m Total time:       5.35 min
[32m[20230117 13:31:46 @agent_ppo2.py:153][0m 446464 total steps have happened
[32m[20230117 13:31:46 @agent_ppo2.py:129][0m #------------------------ Iteration 218 --------------------------#
[32m[20230117 13:31:46 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:31:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:46 @agent_ppo2.py:193][0m |           0.0012 |          33.3935 |           3.9397 |
[32m[20230117 13:31:46 @agent_ppo2.py:193][0m |          -0.0040 |          22.4564 |           3.9249 |
[32m[20230117 13:31:46 @agent_ppo2.py:193][0m |          -0.0059 |          16.9607 |           3.9161 |
[32m[20230117 13:31:46 @agent_ppo2.py:193][0m |          -0.0074 |          14.8748 |           3.9112 |
[32m[20230117 13:31:46 @agent_ppo2.py:193][0m |          -0.0088 |          14.1342 |           3.9227 |
[32m[20230117 13:31:46 @agent_ppo2.py:193][0m |          -0.0105 |          13.8474 |           3.9100 |
[32m[20230117 13:31:47 @agent_ppo2.py:193][0m |          -0.0112 |          13.5208 |           3.9109 |
[32m[20230117 13:31:47 @agent_ppo2.py:193][0m |          -0.0114 |          13.2318 |           3.9072 |
[32m[20230117 13:31:47 @agent_ppo2.py:193][0m |          -0.0122 |          13.0303 |           3.9086 |
[32m[20230117 13:31:47 @agent_ppo2.py:193][0m |          -0.0128 |          12.8859 |           3.9093 |
[32m[20230117 13:31:47 @agent_ppo2.py:138][0m Policy update time: 0.83 s
[32m[20230117 13:31:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.70
[32m[20230117 13:31:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.14
[32m[20230117 13:31:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.20
[32m[20230117 13:31:47 @agent_ppo2.py:151][0m Total time:       5.37 min
[32m[20230117 13:31:47 @agent_ppo2.py:153][0m 448512 total steps have happened
[32m[20230117 13:31:47 @agent_ppo2.py:129][0m #------------------------ Iteration 219 --------------------------#
[32m[20230117 13:31:47 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:31:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:47 @agent_ppo2.py:193][0m |          -0.0001 |          17.0762 |           4.0004 |
[32m[20230117 13:31:47 @agent_ppo2.py:193][0m |          -0.0062 |          14.8342 |           3.9940 |
[32m[20230117 13:31:47 @agent_ppo2.py:193][0m |          -0.0077 |          14.3335 |           3.9906 |
[32m[20230117 13:31:48 @agent_ppo2.py:193][0m |          -0.0093 |          13.9411 |           3.9889 |
[32m[20230117 13:31:48 @agent_ppo2.py:193][0m |          -0.0108 |          13.6602 |           3.9908 |
[32m[20230117 13:31:48 @agent_ppo2.py:193][0m |          -0.0112 |          13.4544 |           3.9878 |
[32m[20230117 13:31:48 @agent_ppo2.py:193][0m |          -0.0110 |          13.2941 |           3.9879 |
[32m[20230117 13:31:48 @agent_ppo2.py:193][0m |          -0.0124 |          13.0787 |           3.9878 |
[32m[20230117 13:31:48 @agent_ppo2.py:193][0m |          -0.0126 |          12.9256 |           3.9880 |
[32m[20230117 13:31:48 @agent_ppo2.py:193][0m |          -0.0136 |          12.7466 |           3.9874 |
[32m[20230117 13:31:48 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:31:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.39
[32m[20230117 13:31:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.59
[32m[20230117 13:31:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 30.59
[32m[20230117 13:31:48 @agent_ppo2.py:151][0m Total time:       5.39 min
[32m[20230117 13:31:48 @agent_ppo2.py:153][0m 450560 total steps have happened
[32m[20230117 13:31:48 @agent_ppo2.py:129][0m #------------------------ Iteration 220 --------------------------#
[32m[20230117 13:31:48 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:31:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:49 @agent_ppo2.py:193][0m |          -0.0002 |          14.7950 |           3.9929 |
[32m[20230117 13:31:49 @agent_ppo2.py:193][0m |          -0.0053 |          14.1471 |           3.9843 |
[32m[20230117 13:31:49 @agent_ppo2.py:193][0m |          -0.0066 |          13.8896 |           3.9829 |
[32m[20230117 13:31:49 @agent_ppo2.py:193][0m |          -0.0080 |          13.7276 |           3.9884 |
[32m[20230117 13:31:49 @agent_ppo2.py:193][0m |          -0.0089 |          13.6221 |           3.9827 |
[32m[20230117 13:31:49 @agent_ppo2.py:193][0m |          -0.0101 |          13.5012 |           3.9865 |
[32m[20230117 13:31:49 @agent_ppo2.py:193][0m |          -0.0102 |          13.3919 |           3.9817 |
[32m[20230117 13:31:49 @agent_ppo2.py:193][0m |          -0.0109 |          13.2888 |           3.9879 |
[32m[20230117 13:31:49 @agent_ppo2.py:193][0m |          -0.0112 |          13.1941 |           3.9841 |
[32m[20230117 13:31:49 @agent_ppo2.py:193][0m |          -0.0119 |          13.0947 |           3.9868 |
[32m[20230117 13:31:49 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:31:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.20
[32m[20230117 13:31:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.59
[32m[20230117 13:31:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -5.23
[32m[20230117 13:31:49 @agent_ppo2.py:151][0m Total time:       5.41 min
[32m[20230117 13:31:49 @agent_ppo2.py:153][0m 452608 total steps have happened
[32m[20230117 13:31:49 @agent_ppo2.py:129][0m #------------------------ Iteration 221 --------------------------#
[32m[20230117 13:31:50 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:50 @agent_ppo2.py:193][0m |          -0.0007 |          14.6180 |           3.9821 |
[32m[20230117 13:31:50 @agent_ppo2.py:193][0m |          -0.0043 |          14.0164 |           3.9749 |
[32m[20230117 13:31:50 @agent_ppo2.py:193][0m |          -0.0063 |          13.6870 |           3.9708 |
[32m[20230117 13:31:50 @agent_ppo2.py:193][0m |          -0.0051 |          13.9170 |           3.9736 |
[32m[20230117 13:31:50 @agent_ppo2.py:193][0m |          -0.0071 |          13.4708 |           3.9691 |
[32m[20230117 13:31:50 @agent_ppo2.py:193][0m |          -0.0095 |          13.2227 |           3.9665 |
[32m[20230117 13:31:50 @agent_ppo2.py:193][0m |          -0.0106 |          13.1217 |           3.9679 |
[32m[20230117 13:31:50 @agent_ppo2.py:193][0m |          -0.0096 |          13.0914 |           3.9672 |
[32m[20230117 13:31:50 @agent_ppo2.py:193][0m |          -0.0111 |          12.9694 |           3.9627 |
[32m[20230117 13:31:50 @agent_ppo2.py:193][0m |          -0.0116 |          12.8970 |           3.9637 |
[32m[20230117 13:31:50 @agent_ppo2.py:138][0m Policy update time: 0.82 s
[32m[20230117 13:31:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.48
[32m[20230117 13:31:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.67
[32m[20230117 13:31:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 109.79
[32m[20230117 13:31:51 @agent_ppo2.py:151][0m Total time:       5.43 min
[32m[20230117 13:31:51 @agent_ppo2.py:153][0m 454656 total steps have happened
[32m[20230117 13:31:51 @agent_ppo2.py:129][0m #------------------------ Iteration 222 --------------------------#
[32m[20230117 13:31:51 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:31:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:51 @agent_ppo2.py:193][0m |           0.0006 |          14.1900 |           4.0386 |
[32m[20230117 13:31:51 @agent_ppo2.py:193][0m |          -0.0025 |          13.7905 |           4.0356 |
[32m[20230117 13:31:51 @agent_ppo2.py:193][0m |          -0.0085 |          13.2029 |           4.0344 |
[32m[20230117 13:31:51 @agent_ppo2.py:193][0m |          -0.0101 |          12.8561 |           4.0326 |
[32m[20230117 13:31:51 @agent_ppo2.py:193][0m |          -0.0127 |          12.5995 |           4.0322 |
[32m[20230117 13:31:51 @agent_ppo2.py:193][0m |          -0.0139 |          12.3925 |           4.0309 |
[32m[20230117 13:31:51 @agent_ppo2.py:193][0m |          -0.0141 |          12.1909 |           4.0287 |
[32m[20230117 13:31:52 @agent_ppo2.py:193][0m |          -0.0136 |          12.0198 |           4.0272 |
[32m[20230117 13:31:52 @agent_ppo2.py:193][0m |          -0.0146 |          11.8914 |           4.0283 |
[32m[20230117 13:31:52 @agent_ppo2.py:193][0m |          -0.0090 |          12.3123 |           4.0247 |
[32m[20230117 13:31:52 @agent_ppo2.py:138][0m Policy update time: 0.82 s
[32m[20230117 13:31:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.67
[32m[20230117 13:31:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.32
[32m[20230117 13:31:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 86.98
[32m[20230117 13:31:52 @agent_ppo2.py:151][0m Total time:       5.45 min
[32m[20230117 13:31:52 @agent_ppo2.py:153][0m 456704 total steps have happened
[32m[20230117 13:31:52 @agent_ppo2.py:129][0m #------------------------ Iteration 223 --------------------------#
[32m[20230117 13:31:52 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:31:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:52 @agent_ppo2.py:193][0m |           0.0007 |          15.1616 |           3.9576 |
[32m[20230117 13:31:52 @agent_ppo2.py:193][0m |          -0.0047 |          14.3110 |           3.9500 |
[32m[20230117 13:31:52 @agent_ppo2.py:193][0m |          -0.0063 |          14.0370 |           3.9438 |
[32m[20230117 13:31:52 @agent_ppo2.py:193][0m |          -0.0075 |          13.8398 |           3.9442 |
[32m[20230117 13:31:53 @agent_ppo2.py:193][0m |          -0.0094 |          13.7563 |           3.9376 |
[32m[20230117 13:31:53 @agent_ppo2.py:193][0m |          -0.0086 |          13.7250 |           3.9381 |
[32m[20230117 13:31:53 @agent_ppo2.py:193][0m |          -0.0105 |          13.5466 |           3.9349 |
[32m[20230117 13:31:53 @agent_ppo2.py:193][0m |          -0.0109 |          13.5017 |           3.9329 |
[32m[20230117 13:31:53 @agent_ppo2.py:193][0m |          -0.0119 |          13.3987 |           3.9323 |
[32m[20230117 13:31:53 @agent_ppo2.py:193][0m |          -0.0119 |          13.3913 |           3.9324 |
[32m[20230117 13:31:53 @agent_ppo2.py:138][0m Policy update time: 0.79 s
[32m[20230117 13:31:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.02
[32m[20230117 13:31:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.50
[32m[20230117 13:31:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.41
[32m[20230117 13:31:53 @agent_ppo2.py:151][0m Total time:       5.47 min
[32m[20230117 13:31:53 @agent_ppo2.py:153][0m 458752 total steps have happened
[32m[20230117 13:31:53 @agent_ppo2.py:129][0m #------------------------ Iteration 224 --------------------------#
[32m[20230117 13:31:53 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:31:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:53 @agent_ppo2.py:193][0m |          -0.0016 |          14.0757 |           3.9350 |
[32m[20230117 13:31:54 @agent_ppo2.py:193][0m |          -0.0021 |          12.8553 |           3.9296 |
[32m[20230117 13:31:54 @agent_ppo2.py:193][0m |          -0.0084 |          11.9518 |           3.9281 |
[32m[20230117 13:31:54 @agent_ppo2.py:193][0m |          -0.0044 |          12.0423 |           3.9272 |
[32m[20230117 13:31:54 @agent_ppo2.py:193][0m |          -0.0078 |          11.2456 |           3.9254 |
[32m[20230117 13:31:54 @agent_ppo2.py:193][0m |          -0.0139 |          10.8654 |           3.9235 |
[32m[20230117 13:31:54 @agent_ppo2.py:193][0m |          -0.0069 |          11.1164 |           3.9226 |
[32m[20230117 13:31:54 @agent_ppo2.py:193][0m |          -0.0108 |          10.4154 |           3.9189 |
[32m[20230117 13:31:54 @agent_ppo2.py:193][0m |          -0.0094 |          10.9019 |           3.9184 |
[32m[20230117 13:31:54 @agent_ppo2.py:193][0m |          -0.0141 |           9.9597 |           3.9225 |
[32m[20230117 13:31:54 @agent_ppo2.py:138][0m Policy update time: 0.82 s
[32m[20230117 13:31:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.71
[32m[20230117 13:31:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.89
[32m[20230117 13:31:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.21
[32m[20230117 13:31:54 @agent_ppo2.py:151][0m Total time:       5.49 min
[32m[20230117 13:31:54 @agent_ppo2.py:153][0m 460800 total steps have happened
[32m[20230117 13:31:54 @agent_ppo2.py:129][0m #------------------------ Iteration 225 --------------------------#
[32m[20230117 13:31:55 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:31:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:55 @agent_ppo2.py:193][0m |           0.0008 |          28.9227 |           3.9010 |
[32m[20230117 13:31:55 @agent_ppo2.py:193][0m |           0.0634 |          21.6030 |           3.9021 |
[32m[20230117 13:31:55 @agent_ppo2.py:193][0m |          -0.0059 |          20.4637 |           3.8860 |
[32m[20230117 13:31:55 @agent_ppo2.py:193][0m |          -0.0098 |          19.2343 |           3.9000 |
[32m[20230117 13:31:55 @agent_ppo2.py:193][0m |          -0.0074 |          18.7577 |           3.9008 |
[32m[20230117 13:31:55 @agent_ppo2.py:193][0m |          -0.0109 |          18.3066 |           3.9018 |
[32m[20230117 13:31:55 @agent_ppo2.py:193][0m |          -0.0125 |          17.7687 |           3.9021 |
[32m[20230117 13:31:55 @agent_ppo2.py:193][0m |          -0.0167 |          17.4286 |           3.9041 |
[32m[20230117 13:31:55 @agent_ppo2.py:193][0m |          -0.0152 |          17.0877 |           3.9050 |
[32m[20230117 13:31:55 @agent_ppo2.py:193][0m |          -0.0156 |          16.8416 |           3.9067 |
[32m[20230117 13:31:55 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:31:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 215.04
[32m[20230117 13:31:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.77
[32m[20230117 13:31:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 28.06
[32m[20230117 13:31:56 @agent_ppo2.py:151][0m Total time:       5.51 min
[32m[20230117 13:31:56 @agent_ppo2.py:153][0m 462848 total steps have happened
[32m[20230117 13:31:56 @agent_ppo2.py:129][0m #------------------------ Iteration 226 --------------------------#
[32m[20230117 13:31:56 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:31:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:56 @agent_ppo2.py:193][0m |           0.0015 |          22.0910 |           3.9877 |
[32m[20230117 13:31:56 @agent_ppo2.py:193][0m |          -0.0043 |          18.7117 |           3.9880 |
[32m[20230117 13:31:56 @agent_ppo2.py:193][0m |          -0.0064 |          17.8364 |           3.9851 |
[32m[20230117 13:31:56 @agent_ppo2.py:193][0m |          -0.0078 |          17.1465 |           3.9846 |
[32m[20230117 13:31:56 @agent_ppo2.py:193][0m |          -0.0085 |          16.4246 |           3.9867 |
[32m[20230117 13:31:56 @agent_ppo2.py:193][0m |          -0.0079 |          15.9760 |           3.9870 |
[32m[20230117 13:31:56 @agent_ppo2.py:193][0m |          -0.0091 |          15.2792 |           3.9869 |
[32m[20230117 13:31:56 @agent_ppo2.py:193][0m |          -0.0115 |          14.8060 |           3.9878 |
[32m[20230117 13:31:57 @agent_ppo2.py:193][0m |          -0.0100 |          14.3683 |           3.9874 |
[32m[20230117 13:31:57 @agent_ppo2.py:193][0m |          -0.0159 |          13.8058 |           3.9833 |
[32m[20230117 13:31:57 @agent_ppo2.py:138][0m Policy update time: 0.85 s
[32m[20230117 13:31:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.54
[32m[20230117 13:31:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.32
[32m[20230117 13:31:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -55.92
[32m[20230117 13:31:57 @agent_ppo2.py:151][0m Total time:       5.53 min
[32m[20230117 13:31:57 @agent_ppo2.py:153][0m 464896 total steps have happened
[32m[20230117 13:31:57 @agent_ppo2.py:129][0m #------------------------ Iteration 227 --------------------------#
[32m[20230117 13:31:57 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:31:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:57 @agent_ppo2.py:193][0m |          -0.0002 |          40.2682 |           4.1418 |
[32m[20230117 13:31:57 @agent_ppo2.py:193][0m |          -0.0062 |          31.9055 |           4.1310 |
[32m[20230117 13:31:57 @agent_ppo2.py:193][0m |          -0.0117 |          29.3820 |           4.1285 |
[32m[20230117 13:31:57 @agent_ppo2.py:193][0m |          -0.0098 |          27.9181 |           4.1277 |
[32m[20230117 13:31:57 @agent_ppo2.py:193][0m |          -0.0111 |          26.1921 |           4.1239 |
[32m[20230117 13:31:57 @agent_ppo2.py:193][0m |          -0.0121 |          25.1658 |           4.1244 |
[32m[20230117 13:31:58 @agent_ppo2.py:193][0m |          -0.0120 |          24.1054 |           4.1253 |
[32m[20230117 13:31:58 @agent_ppo2.py:193][0m |          -0.0135 |          23.3622 |           4.1227 |
[32m[20230117 13:31:58 @agent_ppo2.py:193][0m |          -0.0123 |          23.0135 |           4.1227 |
[32m[20230117 13:31:58 @agent_ppo2.py:193][0m |          -0.0142 |          21.9377 |           4.1223 |
[32m[20230117 13:31:58 @agent_ppo2.py:138][0m Policy update time: 0.81 s
[32m[20230117 13:31:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 159.81
[32m[20230117 13:31:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.38
[32m[20230117 13:31:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 146.82
[32m[20230117 13:31:58 @agent_ppo2.py:151][0m Total time:       5.55 min
[32m[20230117 13:31:58 @agent_ppo2.py:153][0m 466944 total steps have happened
[32m[20230117 13:31:58 @agent_ppo2.py:129][0m #------------------------ Iteration 228 --------------------------#
[32m[20230117 13:31:58 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:31:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:31:58 @agent_ppo2.py:193][0m |          -0.0027 |          40.1574 |           4.0684 |
[32m[20230117 13:31:58 @agent_ppo2.py:193][0m |          -0.0078 |          26.2119 |           4.0619 |
[32m[20230117 13:31:59 @agent_ppo2.py:193][0m |          -0.0080 |          21.8488 |           4.0660 |
[32m[20230117 13:31:59 @agent_ppo2.py:193][0m |          -0.0091 |          20.4824 |           4.0646 |
[32m[20230117 13:31:59 @agent_ppo2.py:193][0m |          -0.0093 |          19.3652 |           4.0647 |
[32m[20230117 13:31:59 @agent_ppo2.py:193][0m |          -0.0100 |          18.7849 |           4.0673 |
[32m[20230117 13:31:59 @agent_ppo2.py:193][0m |          -0.0109 |          18.1421 |           4.0648 |
[32m[20230117 13:31:59 @agent_ppo2.py:193][0m |          -0.0109 |          17.7801 |           4.0624 |
[32m[20230117 13:31:59 @agent_ppo2.py:193][0m |          -0.0115 |          17.4043 |           4.0660 |
[32m[20230117 13:31:59 @agent_ppo2.py:193][0m |          -0.0120 |          17.3091 |           4.0665 |
[32m[20230117 13:31:59 @agent_ppo2.py:138][0m Policy update time: 0.85 s
[32m[20230117 13:31:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 203.32
[32m[20230117 13:31:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.88
[32m[20230117 13:31:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.17
[32m[20230117 13:31:59 @agent_ppo2.py:151][0m Total time:       5.57 min
[32m[20230117 13:31:59 @agent_ppo2.py:153][0m 468992 total steps have happened
[32m[20230117 13:31:59 @agent_ppo2.py:129][0m #------------------------ Iteration 229 --------------------------#
[32m[20230117 13:32:00 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:32:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:00 @agent_ppo2.py:193][0m |          -0.0007 |          22.1525 |           4.0693 |
[32m[20230117 13:32:00 @agent_ppo2.py:193][0m |          -0.0052 |          16.5058 |           4.0650 |
[32m[20230117 13:32:00 @agent_ppo2.py:193][0m |          -0.0074 |          15.9433 |           4.0646 |
[32m[20230117 13:32:00 @agent_ppo2.py:193][0m |          -0.0081 |          15.6257 |           4.0629 |
[32m[20230117 13:32:00 @agent_ppo2.py:193][0m |          -0.0093 |          15.3068 |           4.0641 |
[32m[20230117 13:32:00 @agent_ppo2.py:193][0m |          -0.0107 |          15.0561 |           4.0620 |
[32m[20230117 13:32:00 @agent_ppo2.py:193][0m |          -0.0101 |          14.8776 |           4.0658 |
[32m[20230117 13:32:00 @agent_ppo2.py:193][0m |          -0.0109 |          14.6518 |           4.0671 |
[32m[20230117 13:32:00 @agent_ppo2.py:193][0m |          -0.0108 |          14.4764 |           4.0663 |
[32m[20230117 13:32:00 @agent_ppo2.py:193][0m |          -0.0114 |          14.3194 |           4.0670 |
[32m[20230117 13:32:00 @agent_ppo2.py:138][0m Policy update time: 0.80 s
[32m[20230117 13:32:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 185.42
[32m[20230117 13:32:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.25
[32m[20230117 13:32:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 115.84
[32m[20230117 13:32:01 @agent_ppo2.py:151][0m Total time:       5.59 min
[32m[20230117 13:32:01 @agent_ppo2.py:153][0m 471040 total steps have happened
[32m[20230117 13:32:01 @agent_ppo2.py:129][0m #------------------------ Iteration 230 --------------------------#
[32m[20230117 13:32:01 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:32:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:01 @agent_ppo2.py:193][0m |           0.0003 |          14.9701 |           4.1725 |
[32m[20230117 13:32:01 @agent_ppo2.py:193][0m |          -0.0046 |          14.2410 |           4.1652 |
[32m[20230117 13:32:01 @agent_ppo2.py:193][0m |          -0.0066 |          13.7644 |           4.1583 |
[32m[20230117 13:32:01 @agent_ppo2.py:193][0m |          -0.0087 |          13.4093 |           4.1581 |
[32m[20230117 13:32:01 @agent_ppo2.py:193][0m |          -0.0093 |          13.1017 |           4.1554 |
[32m[20230117 13:32:01 @agent_ppo2.py:193][0m |          -0.0099 |          12.8717 |           4.1554 |
[32m[20230117 13:32:01 @agent_ppo2.py:193][0m |          -0.0109 |          12.7245 |           4.1527 |
[32m[20230117 13:32:01 @agent_ppo2.py:193][0m |          -0.0121 |          12.5118 |           4.1524 |
[32m[20230117 13:32:02 @agent_ppo2.py:193][0m |          -0.0123 |          12.3642 |           4.1472 |
[32m[20230117 13:32:02 @agent_ppo2.py:193][0m |          -0.0125 |          12.2112 |           4.1487 |
[32m[20230117 13:32:02 @agent_ppo2.py:138][0m Policy update time: 0.80 s
[32m[20230117 13:32:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.02
[32m[20230117 13:32:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.23
[32m[20230117 13:32:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 146.91
[32m[20230117 13:32:02 @agent_ppo2.py:151][0m Total time:       5.61 min
[32m[20230117 13:32:02 @agent_ppo2.py:153][0m 473088 total steps have happened
[32m[20230117 13:32:02 @agent_ppo2.py:129][0m #------------------------ Iteration 231 --------------------------#
[32m[20230117 13:32:02 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:02 @agent_ppo2.py:193][0m |          -0.0003 |          14.4796 |           4.0702 |
[32m[20230117 13:32:02 @agent_ppo2.py:193][0m |          -0.0064 |          14.0717 |           4.0662 |
[32m[20230117 13:32:02 @agent_ppo2.py:193][0m |          -0.0073 |          14.0023 |           4.0608 |
[32m[20230117 13:32:02 @agent_ppo2.py:193][0m |          -0.0083 |          13.7364 |           4.0623 |
[32m[20230117 13:32:02 @agent_ppo2.py:193][0m |          -0.0091 |          13.6592 |           4.0621 |
[32m[20230117 13:32:02 @agent_ppo2.py:193][0m |          -0.0109 |          13.3659 |           4.0594 |
[32m[20230117 13:32:03 @agent_ppo2.py:193][0m |          -0.0115 |          13.2508 |           4.0574 |
[32m[20230117 13:32:03 @agent_ppo2.py:193][0m |          -0.0120 |          13.1332 |           4.0576 |
[32m[20230117 13:32:03 @agent_ppo2.py:193][0m |          -0.0120 |          13.0360 |           4.0542 |
[32m[20230117 13:32:03 @agent_ppo2.py:193][0m |          -0.0116 |          12.9624 |           4.0515 |
[32m[20230117 13:32:03 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:32:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.12
[32m[20230117 13:32:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.15
[32m[20230117 13:32:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 281.80
[32m[20230117 13:32:03 @agent_ppo2.py:151][0m Total time:       5.63 min
[32m[20230117 13:32:03 @agent_ppo2.py:153][0m 475136 total steps have happened
[32m[20230117 13:32:03 @agent_ppo2.py:129][0m #------------------------ Iteration 232 --------------------------#
[32m[20230117 13:32:03 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:03 @agent_ppo2.py:193][0m |           0.0016 |          16.0444 |           4.1417 |
[32m[20230117 13:32:03 @agent_ppo2.py:193][0m |          -0.0056 |          14.3970 |           4.1340 |
[32m[20230117 13:32:03 @agent_ppo2.py:193][0m |          -0.0084 |          13.8844 |           4.1294 |
[32m[20230117 13:32:04 @agent_ppo2.py:193][0m |          -0.0080 |          13.7144 |           4.1259 |
[32m[20230117 13:32:04 @agent_ppo2.py:193][0m |          -0.0091 |          13.5988 |           4.1255 |
[32m[20230117 13:32:04 @agent_ppo2.py:193][0m |          -0.0106 |          13.3535 |           4.1236 |
[32m[20230117 13:32:04 @agent_ppo2.py:193][0m |          -0.0113 |          13.3469 |           4.1219 |
[32m[20230117 13:32:04 @agent_ppo2.py:193][0m |          -0.0115 |          13.0970 |           4.1207 |
[32m[20230117 13:32:04 @agent_ppo2.py:193][0m |          -0.0113 |          13.1472 |           4.1237 |
[32m[20230117 13:32:04 @agent_ppo2.py:193][0m |          -0.0124 |          12.8547 |           4.1206 |
[32m[20230117 13:32:04 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:32:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.14
[32m[20230117 13:32:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 271.59
[32m[20230117 13:32:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 165.65
[32m[20230117 13:32:04 @agent_ppo2.py:151][0m Total time:       5.65 min
[32m[20230117 13:32:04 @agent_ppo2.py:153][0m 477184 total steps have happened
[32m[20230117 13:32:04 @agent_ppo2.py:129][0m #------------------------ Iteration 233 --------------------------#
[32m[20230117 13:32:04 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:04 @agent_ppo2.py:193][0m |           0.0062 |          24.2448 |           3.9783 |
[32m[20230117 13:32:05 @agent_ppo2.py:193][0m |          -0.0064 |          15.9947 |           3.9711 |
[32m[20230117 13:32:05 @agent_ppo2.py:193][0m |          -0.0018 |          13.7355 |           3.9662 |
[32m[20230117 13:32:05 @agent_ppo2.py:193][0m |          -0.0049 |          12.1838 |           3.9662 |
[32m[20230117 13:32:05 @agent_ppo2.py:193][0m |          -0.0100 |          10.4223 |           3.9672 |
[32m[20230117 13:32:05 @agent_ppo2.py:193][0m |          -0.0088 |           9.7082 |           3.9652 |
[32m[20230117 13:32:05 @agent_ppo2.py:193][0m |          -0.0091 |           9.2413 |           3.9649 |
[32m[20230117 13:32:05 @agent_ppo2.py:193][0m |          -0.0151 |           8.6448 |           3.9643 |
[32m[20230117 13:32:05 @agent_ppo2.py:193][0m |          -0.0139 |           8.2564 |           3.9628 |
[32m[20230117 13:32:05 @agent_ppo2.py:193][0m |          -0.0152 |           8.1166 |           3.9626 |
[32m[20230117 13:32:05 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:32:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 137.71
[32m[20230117 13:32:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.64
[32m[20230117 13:32:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.54
[32m[20230117 13:32:05 @agent_ppo2.py:151][0m Total time:       5.67 min
[32m[20230117 13:32:05 @agent_ppo2.py:153][0m 479232 total steps have happened
[32m[20230117 13:32:05 @agent_ppo2.py:129][0m #------------------------ Iteration 234 --------------------------#
[32m[20230117 13:32:05 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:06 @agent_ppo2.py:193][0m |           0.0007 |          15.1375 |           4.0825 |
[32m[20230117 13:32:06 @agent_ppo2.py:193][0m |          -0.0048 |          13.3863 |           4.0732 |
[32m[20230117 13:32:06 @agent_ppo2.py:193][0m |          -0.0049 |          12.6028 |           4.0735 |
[32m[20230117 13:32:06 @agent_ppo2.py:193][0m |          -0.0063 |          11.7119 |           4.0734 |
[32m[20230117 13:32:06 @agent_ppo2.py:193][0m |          -0.0087 |          11.1521 |           4.0684 |
[32m[20230117 13:32:06 @agent_ppo2.py:193][0m |          -0.0098 |          10.5730 |           4.0722 |
[32m[20230117 13:32:06 @agent_ppo2.py:193][0m |          -0.0077 |          10.2322 |           4.0679 |
[32m[20230117 13:32:06 @agent_ppo2.py:193][0m |          -0.0092 |           9.8980 |           4.0676 |
[32m[20230117 13:32:06 @agent_ppo2.py:193][0m |          -0.0093 |           9.4453 |           4.0679 |
[32m[20230117 13:32:06 @agent_ppo2.py:193][0m |          -0.0116 |           9.1198 |           4.0688 |
[32m[20230117 13:32:06 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:32:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.29
[32m[20230117 13:32:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.94
[32m[20230117 13:32:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 281.88
[32m[20230117 13:32:06 @agent_ppo2.py:151][0m Total time:       5.69 min
[32m[20230117 13:32:06 @agent_ppo2.py:153][0m 481280 total steps have happened
[32m[20230117 13:32:06 @agent_ppo2.py:129][0m #------------------------ Iteration 235 --------------------------#
[32m[20230117 13:32:07 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:32:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:07 @agent_ppo2.py:193][0m |          -0.0025 |          26.2599 |           3.9929 |
[32m[20230117 13:32:07 @agent_ppo2.py:193][0m |          -0.0075 |          15.3208 |           3.9925 |
[32m[20230117 13:32:07 @agent_ppo2.py:193][0m |          -0.0069 |          14.2987 |           3.9921 |
[32m[20230117 13:32:07 @agent_ppo2.py:193][0m |          -0.0101 |          13.5670 |           3.9925 |
[32m[20230117 13:32:07 @agent_ppo2.py:193][0m |          -0.0109 |          13.0330 |           3.9910 |
[32m[20230117 13:32:07 @agent_ppo2.py:193][0m |          -0.0129 |          12.6640 |           3.9907 |
[32m[20230117 13:32:07 @agent_ppo2.py:193][0m |          -0.0124 |          12.4155 |           3.9925 |
[32m[20230117 13:32:07 @agent_ppo2.py:193][0m |          -0.0128 |          11.9611 |           3.9904 |
[32m[20230117 13:32:07 @agent_ppo2.py:193][0m |          -0.0138 |          11.6990 |           3.9896 |
[32m[20230117 13:32:07 @agent_ppo2.py:193][0m |          -0.0137 |          11.4612 |           3.9919 |
[32m[20230117 13:32:07 @agent_ppo2.py:138][0m Policy update time: 0.79 s
[32m[20230117 13:32:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 203.58
[32m[20230117 13:32:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 270.27
[32m[20230117 13:32:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.40
[32m[20230117 13:32:08 @agent_ppo2.py:151][0m Total time:       5.71 min
[32m[20230117 13:32:08 @agent_ppo2.py:153][0m 483328 total steps have happened
[32m[20230117 13:32:08 @agent_ppo2.py:129][0m #------------------------ Iteration 236 --------------------------#
[32m[20230117 13:32:08 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:08 @agent_ppo2.py:193][0m |           0.0000 |          19.0297 |           4.0721 |
[32m[20230117 13:32:08 @agent_ppo2.py:193][0m |          -0.0052 |          17.3016 |           4.0636 |
[32m[20230117 13:32:08 @agent_ppo2.py:193][0m |          -0.0075 |          16.7677 |           4.0635 |
[32m[20230117 13:32:08 @agent_ppo2.py:193][0m |          -0.0062 |          16.7612 |           4.0641 |
[32m[20230117 13:32:08 @agent_ppo2.py:193][0m |          -0.0112 |          16.1151 |           4.0680 |
[32m[20230117 13:32:08 @agent_ppo2.py:193][0m |          -0.0101 |          15.8507 |           4.0660 |
[32m[20230117 13:32:08 @agent_ppo2.py:193][0m |          -0.0089 |          15.7233 |           4.0661 |
[32m[20230117 13:32:09 @agent_ppo2.py:193][0m |          -0.0096 |          15.3857 |           4.0628 |
[32m[20230117 13:32:09 @agent_ppo2.py:193][0m |          -0.0107 |          15.2222 |           4.0680 |
[32m[20230117 13:32:09 @agent_ppo2.py:193][0m |          -0.0108 |          15.0453 |           4.0678 |
[32m[20230117 13:32:09 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:32:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.70
[32m[20230117 13:32:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.88
[32m[20230117 13:32:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 104.48
[32m[20230117 13:32:09 @agent_ppo2.py:151][0m Total time:       5.73 min
[32m[20230117 13:32:09 @agent_ppo2.py:153][0m 485376 total steps have happened
[32m[20230117 13:32:09 @agent_ppo2.py:129][0m #------------------------ Iteration 237 --------------------------#
[32m[20230117 13:32:09 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:09 @agent_ppo2.py:193][0m |          -0.0003 |          16.1898 |           4.1034 |
[32m[20230117 13:32:09 @agent_ppo2.py:193][0m |          -0.0067 |          13.8102 |           4.0950 |
[32m[20230117 13:32:09 @agent_ppo2.py:193][0m |          -0.0082 |          12.9821 |           4.0907 |
[32m[20230117 13:32:09 @agent_ppo2.py:193][0m |          -0.0094 |          12.3238 |           4.0886 |
[32m[20230117 13:32:10 @agent_ppo2.py:193][0m |          -0.0092 |          11.7422 |           4.0876 |
[32m[20230117 13:32:10 @agent_ppo2.py:193][0m |          -0.0108 |          11.3092 |           4.0914 |
[32m[20230117 13:32:10 @agent_ppo2.py:193][0m |          -0.0113 |          10.9262 |           4.0885 |
[32m[20230117 13:32:10 @agent_ppo2.py:193][0m |          -0.0120 |          10.6905 |           4.0913 |
[32m[20230117 13:32:10 @agent_ppo2.py:193][0m |          -0.0120 |          10.4224 |           4.0909 |
[32m[20230117 13:32:10 @agent_ppo2.py:193][0m |          -0.0124 |          10.2414 |           4.0907 |
[32m[20230117 13:32:10 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:32:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.66
[32m[20230117 13:32:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.73
[32m[20230117 13:32:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.09
[32m[20230117 13:32:10 @agent_ppo2.py:151][0m Total time:       5.75 min
[32m[20230117 13:32:10 @agent_ppo2.py:153][0m 487424 total steps have happened
[32m[20230117 13:32:10 @agent_ppo2.py:129][0m #------------------------ Iteration 238 --------------------------#
[32m[20230117 13:32:10 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:32:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:10 @agent_ppo2.py:193][0m |          -0.0005 |          17.5849 |           4.1241 |
[32m[20230117 13:32:11 @agent_ppo2.py:193][0m |          -0.0076 |          16.2826 |           4.1198 |
[32m[20230117 13:32:11 @agent_ppo2.py:193][0m |          -0.0079 |          15.9852 |           4.1167 |
[32m[20230117 13:32:11 @agent_ppo2.py:193][0m |          -0.0088 |          15.8094 |           4.1171 |
[32m[20230117 13:32:11 @agent_ppo2.py:193][0m |          -0.0101 |          15.6463 |           4.1199 |
[32m[20230117 13:32:11 @agent_ppo2.py:193][0m |          -0.0105 |          15.5359 |           4.1194 |
[32m[20230117 13:32:11 @agent_ppo2.py:193][0m |          -0.0110 |          15.4346 |           4.1205 |
[32m[20230117 13:32:11 @agent_ppo2.py:193][0m |          -0.0113 |          15.3904 |           4.1215 |
[32m[20230117 13:32:11 @agent_ppo2.py:193][0m |          -0.0114 |          15.2637 |           4.1236 |
[32m[20230117 13:32:11 @agent_ppo2.py:193][0m |          -0.0116 |          15.2196 |           4.1223 |
[32m[20230117 13:32:11 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:32:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.75
[32m[20230117 13:32:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.31
[32m[20230117 13:32:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.75
[32m[20230117 13:32:11 @agent_ppo2.py:151][0m Total time:       5.77 min
[32m[20230117 13:32:11 @agent_ppo2.py:153][0m 489472 total steps have happened
[32m[20230117 13:32:11 @agent_ppo2.py:129][0m #------------------------ Iteration 239 --------------------------#
[32m[20230117 13:32:12 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:12 @agent_ppo2.py:193][0m |          -0.0011 |          14.5599 |           4.1708 |
[32m[20230117 13:32:12 @agent_ppo2.py:193][0m |          -0.0073 |          11.8974 |           4.1544 |
[32m[20230117 13:32:12 @agent_ppo2.py:193][0m |          -0.0090 |          11.0350 |           4.1497 |
[32m[20230117 13:32:12 @agent_ppo2.py:193][0m |          -0.0095 |          10.4520 |           4.1486 |
[32m[20230117 13:32:12 @agent_ppo2.py:193][0m |          -0.0109 |           9.9607 |           4.1455 |
[32m[20230117 13:32:12 @agent_ppo2.py:193][0m |          -0.0115 |           9.5982 |           4.1405 |
[32m[20230117 13:32:12 @agent_ppo2.py:193][0m |          -0.0121 |           9.3377 |           4.1368 |
[32m[20230117 13:32:12 @agent_ppo2.py:193][0m |          -0.0123 |           9.1010 |           4.1375 |
[32m[20230117 13:32:12 @agent_ppo2.py:193][0m |          -0.0128 |           8.8757 |           4.1334 |
[32m[20230117 13:32:12 @agent_ppo2.py:193][0m |          -0.0138 |           8.6216 |           4.1329 |
[32m[20230117 13:32:12 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:32:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 267.75
[32m[20230117 13:32:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.70
[32m[20230117 13:32:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.24
[32m[20230117 13:32:12 @agent_ppo2.py:151][0m Total time:       5.79 min
[32m[20230117 13:32:12 @agent_ppo2.py:153][0m 491520 total steps have happened
[32m[20230117 13:32:12 @agent_ppo2.py:129][0m #------------------------ Iteration 240 --------------------------#
[32m[20230117 13:32:13 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:13 @agent_ppo2.py:193][0m |          -0.0019 |          18.3164 |           4.1082 |
[32m[20230117 13:32:13 @agent_ppo2.py:193][0m |          -0.0106 |          16.5413 |           4.1039 |
[32m[20230117 13:32:13 @agent_ppo2.py:193][0m |          -0.0107 |          15.9380 |           4.0985 |
[32m[20230117 13:32:13 @agent_ppo2.py:193][0m |          -0.0115 |          15.5975 |           4.0961 |
[32m[20230117 13:32:13 @agent_ppo2.py:193][0m |          -0.0090 |          15.2993 |           4.0930 |
[32m[20230117 13:32:13 @agent_ppo2.py:193][0m |          -0.0144 |          15.1240 |           4.0927 |
[32m[20230117 13:32:13 @agent_ppo2.py:193][0m |          -0.0103 |          15.0147 |           4.0924 |
[32m[20230117 13:32:13 @agent_ppo2.py:193][0m |          -0.0094 |          14.8652 |           4.0877 |
[32m[20230117 13:32:13 @agent_ppo2.py:193][0m |          -0.0123 |          14.7373 |           4.0911 |
[32m[20230117 13:32:13 @agent_ppo2.py:193][0m |          -0.0069 |          14.6845 |           4.0868 |
[32m[20230117 13:32:13 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:32:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.90
[32m[20230117 13:32:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.16
[32m[20230117 13:32:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.28
[32m[20230117 13:32:14 @agent_ppo2.py:151][0m Total time:       5.81 min
[32m[20230117 13:32:14 @agent_ppo2.py:153][0m 493568 total steps have happened
[32m[20230117 13:32:14 @agent_ppo2.py:129][0m #------------------------ Iteration 241 --------------------------#
[32m[20230117 13:32:14 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:14 @agent_ppo2.py:193][0m |           0.0023 |          16.9200 |           4.0090 |
[32m[20230117 13:32:14 @agent_ppo2.py:193][0m |          -0.0091 |          15.1600 |           4.0025 |
[32m[20230117 13:32:14 @agent_ppo2.py:193][0m |          -0.0085 |          14.4425 |           4.0008 |
[32m[20230117 13:32:14 @agent_ppo2.py:193][0m |          -0.0098 |          13.8015 |           4.0015 |
[32m[20230117 13:32:14 @agent_ppo2.py:193][0m |          -0.0065 |          13.3863 |           4.0045 |
[32m[20230117 13:32:14 @agent_ppo2.py:193][0m |          -0.0127 |          12.8625 |           4.0036 |
[32m[20230117 13:32:14 @agent_ppo2.py:193][0m |          -0.0113 |          12.4874 |           4.0059 |
[32m[20230117 13:32:15 @agent_ppo2.py:193][0m |          -0.0117 |          12.1484 |           4.0068 |
[32m[20230117 13:32:15 @agent_ppo2.py:193][0m |          -0.0114 |          11.9021 |           4.0040 |
[32m[20230117 13:32:15 @agent_ppo2.py:193][0m |          -0.0121 |          11.6356 |           4.0055 |
[32m[20230117 13:32:15 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:32:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.32
[32m[20230117 13:32:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.18
[32m[20230117 13:32:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.76
[32m[20230117 13:32:15 @agent_ppo2.py:151][0m Total time:       5.83 min
[32m[20230117 13:32:15 @agent_ppo2.py:153][0m 495616 total steps have happened
[32m[20230117 13:32:15 @agent_ppo2.py:129][0m #------------------------ Iteration 242 --------------------------#
[32m[20230117 13:32:15 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:15 @agent_ppo2.py:193][0m |          -0.0025 |          17.4131 |           4.0687 |
[32m[20230117 13:32:15 @agent_ppo2.py:193][0m |          -0.0062 |          15.4170 |           4.0575 |
[32m[20230117 13:32:15 @agent_ppo2.py:193][0m |          -0.0101 |          14.9930 |           4.0568 |
[32m[20230117 13:32:15 @agent_ppo2.py:193][0m |          -0.0074 |          14.8643 |           4.0568 |
[32m[20230117 13:32:15 @agent_ppo2.py:193][0m |          -0.0111 |          14.5910 |           4.0553 |
[32m[20230117 13:32:16 @agent_ppo2.py:193][0m |          -0.0113 |          14.4272 |           4.0509 |
[32m[20230117 13:32:16 @agent_ppo2.py:193][0m |          -0.0020 |          15.6488 |           4.0504 |
[32m[20230117 13:32:16 @agent_ppo2.py:193][0m |          -0.0069 |          14.8931 |           4.0485 |
[32m[20230117 13:32:16 @agent_ppo2.py:193][0m |          -0.0100 |          14.1111 |           4.0485 |
[32m[20230117 13:32:16 @agent_ppo2.py:193][0m |          -0.0150 |          13.8374 |           4.0468 |
[32m[20230117 13:32:16 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:32:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.54
[32m[20230117 13:32:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.92
[32m[20230117 13:32:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.77
[32m[20230117 13:32:16 @agent_ppo2.py:151][0m Total time:       5.85 min
[32m[20230117 13:32:16 @agent_ppo2.py:153][0m 497664 total steps have happened
[32m[20230117 13:32:16 @agent_ppo2.py:129][0m #------------------------ Iteration 243 --------------------------#
[32m[20230117 13:32:16 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:16 @agent_ppo2.py:193][0m |           0.0033 |          15.4909 |           4.0156 |
[32m[20230117 13:32:16 @agent_ppo2.py:193][0m |          -0.0029 |          12.6090 |           4.0117 |
[32m[20230117 13:32:17 @agent_ppo2.py:193][0m |          -0.0053 |          10.3494 |           4.0108 |
[32m[20230117 13:32:17 @agent_ppo2.py:193][0m |          -0.0035 |           9.4841 |           4.0063 |
[32m[20230117 13:32:17 @agent_ppo2.py:193][0m |          -0.0063 |           8.8393 |           4.0116 |
[32m[20230117 13:32:17 @agent_ppo2.py:193][0m |          -0.0073 |           8.2552 |           4.0070 |
[32m[20230117 13:32:17 @agent_ppo2.py:193][0m |          -0.0084 |           7.8293 |           4.0064 |
[32m[20230117 13:32:17 @agent_ppo2.py:193][0m |          -0.0087 |           7.5526 |           4.0071 |
[32m[20230117 13:32:17 @agent_ppo2.py:193][0m |          -0.0059 |           7.3815 |           4.0072 |
[32m[20230117 13:32:17 @agent_ppo2.py:193][0m |          -0.0078 |           7.0273 |           4.0091 |
[32m[20230117 13:32:17 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:32:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.75
[32m[20230117 13:32:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.63
[32m[20230117 13:32:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.94
[32m[20230117 13:32:17 @agent_ppo2.py:151][0m Total time:       5.87 min
[32m[20230117 13:32:17 @agent_ppo2.py:153][0m 499712 total steps have happened
[32m[20230117 13:32:17 @agent_ppo2.py:129][0m #------------------------ Iteration 244 --------------------------#
[32m[20230117 13:32:17 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:32:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:18 @agent_ppo2.py:193][0m |           0.0086 |          20.2628 |           4.0376 |
[32m[20230117 13:32:18 @agent_ppo2.py:193][0m |          -0.0010 |          16.7451 |           4.0303 |
[32m[20230117 13:32:18 @agent_ppo2.py:193][0m |          -0.0028 |          16.0326 |           4.0326 |
[32m[20230117 13:32:18 @agent_ppo2.py:193][0m |          -0.0070 |          15.5921 |           4.0312 |
[32m[20230117 13:32:18 @agent_ppo2.py:193][0m |          -0.0025 |          15.4493 |           4.0322 |
[32m[20230117 13:32:18 @agent_ppo2.py:193][0m |          -0.0018 |          15.9155 |           4.0290 |
[32m[20230117 13:32:18 @agent_ppo2.py:193][0m |          -0.0104 |          14.8336 |           4.0321 |
[32m[20230117 13:32:18 @agent_ppo2.py:193][0m |          -0.0009 |          15.5894 |           4.0277 |
[32m[20230117 13:32:18 @agent_ppo2.py:193][0m |          -0.0089 |          14.4258 |           4.0250 |
[32m[20230117 13:32:18 @agent_ppo2.py:193][0m |          -0.0056 |          14.2595 |           4.0293 |
[32m[20230117 13:32:18 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:32:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.31
[32m[20230117 13:32:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.28
[32m[20230117 13:32:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.98
[32m[20230117 13:32:18 @agent_ppo2.py:151][0m Total time:       5.89 min
[32m[20230117 13:32:18 @agent_ppo2.py:153][0m 501760 total steps have happened
[32m[20230117 13:32:18 @agent_ppo2.py:129][0m #------------------------ Iteration 245 --------------------------#
[32m[20230117 13:32:19 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:19 @agent_ppo2.py:193][0m |           0.0019 |          19.3864 |           4.0630 |
[32m[20230117 13:32:19 @agent_ppo2.py:193][0m |          -0.0044 |          13.2268 |           4.0591 |
[32m[20230117 13:32:19 @agent_ppo2.py:193][0m |           0.0031 |          12.3023 |           4.0565 |
[32m[20230117 13:32:19 @agent_ppo2.py:193][0m |          -0.0049 |          10.7199 |           4.0527 |
[32m[20230117 13:32:19 @agent_ppo2.py:193][0m |          -0.0134 |           9.7664 |           4.0592 |
[32m[20230117 13:32:19 @agent_ppo2.py:193][0m |          -0.0095 |          10.2609 |           4.0537 |
[32m[20230117 13:32:19 @agent_ppo2.py:193][0m |          -0.0121 |           8.6633 |           4.0489 |
[32m[20230117 13:32:19 @agent_ppo2.py:193][0m |          -0.0152 |           8.3074 |           4.0526 |
[32m[20230117 13:32:19 @agent_ppo2.py:193][0m |          -0.0162 |           7.9727 |           4.0533 |
[32m[20230117 13:32:19 @agent_ppo2.py:193][0m |          -0.0166 |           7.6377 |           4.0526 |
[32m[20230117 13:32:19 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:32:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 225.20
[32m[20230117 13:32:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.52
[32m[20230117 13:32:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.83
[32m[20230117 13:32:20 @agent_ppo2.py:151][0m Total time:       5.91 min
[32m[20230117 13:32:20 @agent_ppo2.py:153][0m 503808 total steps have happened
[32m[20230117 13:32:20 @agent_ppo2.py:129][0m #------------------------ Iteration 246 --------------------------#
[32m[20230117 13:32:20 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:20 @agent_ppo2.py:193][0m |          -0.0007 |          16.4301 |           4.0413 |
[32m[20230117 13:32:20 @agent_ppo2.py:193][0m |          -0.0039 |          15.4802 |           4.0370 |
[32m[20230117 13:32:20 @agent_ppo2.py:193][0m |          -0.0055 |          14.7512 |           4.0299 |
[32m[20230117 13:32:20 @agent_ppo2.py:193][0m |          -0.0051 |          14.4242 |           4.0257 |
[32m[20230117 13:32:20 @agent_ppo2.py:193][0m |          -0.0052 |          14.0726 |           4.0264 |
[32m[20230117 13:32:20 @agent_ppo2.py:193][0m |          -0.0066 |          13.7853 |           4.0245 |
[32m[20230117 13:32:20 @agent_ppo2.py:193][0m |          -0.0089 |          13.3745 |           4.0197 |
[32m[20230117 13:32:20 @agent_ppo2.py:193][0m |          -0.0080 |          13.3112 |           4.0179 |
[32m[20230117 13:32:20 @agent_ppo2.py:193][0m |          -0.0092 |          12.9959 |           4.0170 |
[32m[20230117 13:32:21 @agent_ppo2.py:193][0m |          -0.0094 |          12.9329 |           4.0111 |
[32m[20230117 13:32:21 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:32:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.98
[32m[20230117 13:32:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.62
[32m[20230117 13:32:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 279.12
[32m[20230117 13:32:21 @agent_ppo2.py:151][0m Total time:       5.93 min
[32m[20230117 13:32:21 @agent_ppo2.py:153][0m 505856 total steps have happened
[32m[20230117 13:32:21 @agent_ppo2.py:129][0m #------------------------ Iteration 247 --------------------------#
[32m[20230117 13:32:21 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:32:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:21 @agent_ppo2.py:193][0m |           0.0007 |          16.4390 |           3.9257 |
[32m[20230117 13:32:21 @agent_ppo2.py:193][0m |          -0.0029 |          15.1512 |           3.9209 |
[32m[20230117 13:32:21 @agent_ppo2.py:193][0m |          -0.0044 |          14.4111 |           3.9184 |
[32m[20230117 13:32:21 @agent_ppo2.py:193][0m |          -0.0086 |          13.9551 |           3.9157 |
[32m[20230117 13:32:21 @agent_ppo2.py:193][0m |           0.0009 |          14.6389 |           3.9162 |
[32m[20230117 13:32:21 @agent_ppo2.py:193][0m |          -0.0041 |          13.3315 |           3.9128 |
[32m[20230117 13:32:22 @agent_ppo2.py:193][0m |          -0.0091 |          13.1605 |           3.9142 |
[32m[20230117 13:32:22 @agent_ppo2.py:193][0m |          -0.0103 |          12.9863 |           3.9159 |
[32m[20230117 13:32:22 @agent_ppo2.py:193][0m |          -0.0084 |          12.8359 |           3.9137 |
[32m[20230117 13:32:22 @agent_ppo2.py:193][0m |          -0.0104 |          12.7233 |           3.9122 |
[32m[20230117 13:32:22 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:32:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.32
[32m[20230117 13:32:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.09
[32m[20230117 13:32:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.67
[32m[20230117 13:32:22 @agent_ppo2.py:151][0m Total time:       5.95 min
[32m[20230117 13:32:22 @agent_ppo2.py:153][0m 507904 total steps have happened
[32m[20230117 13:32:22 @agent_ppo2.py:129][0m #------------------------ Iteration 248 --------------------------#
[32m[20230117 13:32:22 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:32:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:22 @agent_ppo2.py:193][0m |          -0.0011 |          16.5069 |           3.9745 |
[32m[20230117 13:32:22 @agent_ppo2.py:193][0m |          -0.0042 |          13.7012 |           3.9691 |
[32m[20230117 13:32:23 @agent_ppo2.py:193][0m |          -0.0078 |          12.7904 |           3.9662 |
[32m[20230117 13:32:23 @agent_ppo2.py:193][0m |          -0.0101 |          12.1417 |           3.9676 |
[32m[20230117 13:32:23 @agent_ppo2.py:193][0m |          -0.0001 |          12.3136 |           3.9603 |
[32m[20230117 13:32:23 @agent_ppo2.py:193][0m |          -0.0083 |          11.3801 |           3.9640 |
[32m[20230117 13:32:23 @agent_ppo2.py:193][0m |           0.0066 |          11.3211 |           3.9645 |
[32m[20230117 13:32:23 @agent_ppo2.py:193][0m |          -0.0167 |          10.9905 |           3.9623 |
[32m[20230117 13:32:23 @agent_ppo2.py:193][0m |          -0.0150 |          10.7324 |           3.9621 |
[32m[20230117 13:32:23 @agent_ppo2.py:193][0m |          -0.0090 |          10.6345 |           3.9654 |
[32m[20230117 13:32:23 @agent_ppo2.py:138][0m Policy update time: 0.83 s
[32m[20230117 13:32:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: 257.05
[32m[20230117 13:32:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.98
[32m[20230117 13:32:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.74
[32m[20230117 13:32:23 @agent_ppo2.py:151][0m Total time:       5.97 min
[32m[20230117 13:32:23 @agent_ppo2.py:153][0m 509952 total steps have happened
[32m[20230117 13:32:23 @agent_ppo2.py:129][0m #------------------------ Iteration 249 --------------------------#
[32m[20230117 13:32:23 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:24 @agent_ppo2.py:193][0m |           0.0005 |          17.4185 |           4.0488 |
[32m[20230117 13:32:24 @agent_ppo2.py:193][0m |          -0.0037 |          15.7431 |           4.0472 |
[32m[20230117 13:32:24 @agent_ppo2.py:193][0m |          -0.0055 |          15.0770 |           4.0405 |
[32m[20230117 13:32:24 @agent_ppo2.py:193][0m |          -0.0079 |          14.6055 |           4.0424 |
[32m[20230117 13:32:24 @agent_ppo2.py:193][0m |          -0.0085 |          14.2565 |           4.0425 |
[32m[20230117 13:32:24 @agent_ppo2.py:193][0m |          -0.0095 |          13.8628 |           4.0420 |
[32m[20230117 13:32:24 @agent_ppo2.py:193][0m |          -0.0101 |          13.4954 |           4.0406 |
[32m[20230117 13:32:24 @agent_ppo2.py:193][0m |          -0.0098 |          13.2286 |           4.0428 |
[32m[20230117 13:32:24 @agent_ppo2.py:193][0m |          -0.0107 |          12.9936 |           4.0433 |
[32m[20230117 13:32:24 @agent_ppo2.py:193][0m |          -0.0113 |          12.7225 |           4.0431 |
[32m[20230117 13:32:24 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:32:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.70
[32m[20230117 13:32:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.31
[32m[20230117 13:32:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.58
[32m[20230117 13:32:24 @agent_ppo2.py:151][0m Total time:       5.99 min
[32m[20230117 13:32:24 @agent_ppo2.py:153][0m 512000 total steps have happened
[32m[20230117 13:32:24 @agent_ppo2.py:129][0m #------------------------ Iteration 250 --------------------------#
[32m[20230117 13:32:25 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:32:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:25 @agent_ppo2.py:193][0m |          -0.0016 |          15.1652 |           3.9807 |
[32m[20230117 13:32:25 @agent_ppo2.py:193][0m |          -0.0061 |          13.6005 |           3.9748 |
[32m[20230117 13:32:25 @agent_ppo2.py:193][0m |          -0.0063 |          12.9400 |           3.9725 |
[32m[20230117 13:32:25 @agent_ppo2.py:193][0m |          -0.0078 |          12.3682 |           3.9752 |
[32m[20230117 13:32:25 @agent_ppo2.py:193][0m |          -0.0109 |          11.9961 |           3.9753 |
[32m[20230117 13:32:25 @agent_ppo2.py:193][0m |          -0.0063 |          12.0618 |           3.9736 |
[32m[20230117 13:32:25 @agent_ppo2.py:193][0m |          -0.0144 |          11.4605 |           3.9752 |
[32m[20230117 13:32:25 @agent_ppo2.py:193][0m |          -0.0003 |          12.6027 |           3.9753 |
[32m[20230117 13:32:25 @agent_ppo2.py:193][0m |          -0.0099 |          11.0514 |           3.9782 |
[32m[20230117 13:32:25 @agent_ppo2.py:193][0m |          -0.0092 |          10.9893 |           3.9747 |
[32m[20230117 13:32:25 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:32:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.71
[32m[20230117 13:32:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.13
[32m[20230117 13:32:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.29
[32m[20230117 13:32:26 @agent_ppo2.py:151][0m Total time:       6.01 min
[32m[20230117 13:32:26 @agent_ppo2.py:153][0m 514048 total steps have happened
[32m[20230117 13:32:26 @agent_ppo2.py:129][0m #------------------------ Iteration 251 --------------------------#
[32m[20230117 13:32:26 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:26 @agent_ppo2.py:193][0m |           0.0011 |          17.0753 |           4.0550 |
[32m[20230117 13:32:26 @agent_ppo2.py:193][0m |          -0.0034 |          16.0061 |           4.0559 |
[32m[20230117 13:32:26 @agent_ppo2.py:193][0m |          -0.0049 |          15.6138 |           4.0552 |
[32m[20230117 13:32:26 @agent_ppo2.py:193][0m |          -0.0056 |          15.5267 |           4.0508 |
[32m[20230117 13:32:26 @agent_ppo2.py:193][0m |          -0.0057 |          15.3192 |           4.0527 |
[32m[20230117 13:32:26 @agent_ppo2.py:193][0m |          -0.0101 |          14.7778 |           4.0544 |
[32m[20230117 13:32:26 @agent_ppo2.py:193][0m |          -0.0106 |          14.6445 |           4.0519 |
[32m[20230117 13:32:26 @agent_ppo2.py:193][0m |          -0.0102 |          14.4302 |           4.0506 |
[32m[20230117 13:32:27 @agent_ppo2.py:193][0m |          -0.0108 |          14.2422 |           4.0511 |
[32m[20230117 13:32:27 @agent_ppo2.py:193][0m |          -0.0123 |          14.1142 |           4.0550 |
[32m[20230117 13:32:27 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:32:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.73
[32m[20230117 13:32:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.82
[32m[20230117 13:32:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 54.70
[32m[20230117 13:32:27 @agent_ppo2.py:151][0m Total time:       6.03 min
[32m[20230117 13:32:27 @agent_ppo2.py:153][0m 516096 total steps have happened
[32m[20230117 13:32:27 @agent_ppo2.py:129][0m #------------------------ Iteration 252 --------------------------#
[32m[20230117 13:32:27 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:27 @agent_ppo2.py:193][0m |          -0.0008 |          16.5379 |           4.0735 |
[32m[20230117 13:32:27 @agent_ppo2.py:193][0m |          -0.0073 |          15.5258 |           4.0575 |
[32m[20230117 13:32:27 @agent_ppo2.py:193][0m |          -0.0081 |          14.8547 |           4.0535 |
[32m[20230117 13:32:27 @agent_ppo2.py:193][0m |          -0.0093 |          14.3163 |           4.0535 |
[32m[20230117 13:32:27 @agent_ppo2.py:193][0m |          -0.0100 |          13.8083 |           4.0515 |
[32m[20230117 13:32:27 @agent_ppo2.py:193][0m |          -0.0106 |          13.3206 |           4.0498 |
[32m[20230117 13:32:28 @agent_ppo2.py:193][0m |          -0.0110 |          12.7782 |           4.0499 |
[32m[20230117 13:32:28 @agent_ppo2.py:193][0m |          -0.0118 |          12.4338 |           4.0463 |
[32m[20230117 13:32:28 @agent_ppo2.py:193][0m |          -0.0120 |          11.8862 |           4.0461 |
[32m[20230117 13:32:28 @agent_ppo2.py:193][0m |          -0.0125 |          11.4499 |           4.0456 |
[32m[20230117 13:32:28 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:32:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.38
[32m[20230117 13:32:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.70
[32m[20230117 13:32:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.74
[32m[20230117 13:32:28 @agent_ppo2.py:151][0m Total time:       6.05 min
[32m[20230117 13:32:28 @agent_ppo2.py:153][0m 518144 total steps have happened
[32m[20230117 13:32:28 @agent_ppo2.py:129][0m #------------------------ Iteration 253 --------------------------#
[32m[20230117 13:32:28 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:28 @agent_ppo2.py:193][0m |           0.0007 |          17.1866 |           4.0105 |
[32m[20230117 13:32:28 @agent_ppo2.py:193][0m |          -0.0057 |          16.0243 |           4.0049 |
[32m[20230117 13:32:28 @agent_ppo2.py:193][0m |          -0.0058 |          15.6978 |           4.0042 |
[32m[20230117 13:32:29 @agent_ppo2.py:193][0m |          -0.0117 |          15.4276 |           4.0067 |
[32m[20230117 13:32:29 @agent_ppo2.py:193][0m |          -0.0095 |          15.2407 |           4.0037 |
[32m[20230117 13:32:29 @agent_ppo2.py:193][0m |          -0.0127 |          15.0322 |           4.0047 |
[32m[20230117 13:32:29 @agent_ppo2.py:193][0m |          -0.0097 |          14.8886 |           4.0062 |
[32m[20230117 13:32:29 @agent_ppo2.py:193][0m |          -0.0137 |          14.6615 |           4.0102 |
[32m[20230117 13:32:29 @agent_ppo2.py:193][0m |          -0.0075 |          14.7655 |           4.0085 |
[32m[20230117 13:32:29 @agent_ppo2.py:193][0m |          -0.0145 |          14.3932 |           4.0127 |
[32m[20230117 13:32:29 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:32:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.23
[32m[20230117 13:32:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.51
[32m[20230117 13:32:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.57
[32m[20230117 13:32:29 @agent_ppo2.py:151][0m Total time:       6.07 min
[32m[20230117 13:32:29 @agent_ppo2.py:153][0m 520192 total steps have happened
[32m[20230117 13:32:29 @agent_ppo2.py:129][0m #------------------------ Iteration 254 --------------------------#
[32m[20230117 13:32:29 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:32:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:30 @agent_ppo2.py:193][0m |          -0.0014 |          16.4760 |           4.0351 |
[32m[20230117 13:32:30 @agent_ppo2.py:193][0m |          -0.0098 |          14.4855 |           4.0330 |
[32m[20230117 13:32:30 @agent_ppo2.py:193][0m |          -0.0072 |          13.7135 |           4.0294 |
[32m[20230117 13:32:30 @agent_ppo2.py:193][0m |          -0.0110 |          13.1045 |           4.0325 |
[32m[20230117 13:32:30 @agent_ppo2.py:193][0m |          -0.0115 |          12.6027 |           4.0286 |
[32m[20230117 13:32:30 @agent_ppo2.py:193][0m |          -0.0093 |          12.1816 |           4.0336 |
[32m[20230117 13:32:30 @agent_ppo2.py:193][0m |          -0.0153 |          11.5367 |           4.0319 |
[32m[20230117 13:32:30 @agent_ppo2.py:193][0m |          -0.0093 |          11.1290 |           4.0316 |
[32m[20230117 13:32:30 @agent_ppo2.py:193][0m |          -0.0113 |          10.6591 |           4.0337 |
[32m[20230117 13:32:30 @agent_ppo2.py:193][0m |           0.0042 |          11.1432 |           4.0341 |
[32m[20230117 13:32:30 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:32:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.54
[32m[20230117 13:32:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.23
[32m[20230117 13:32:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.24
[32m[20230117 13:32:30 @agent_ppo2.py:151][0m Total time:       6.09 min
[32m[20230117 13:32:30 @agent_ppo2.py:153][0m 522240 total steps have happened
[32m[20230117 13:32:30 @agent_ppo2.py:129][0m #------------------------ Iteration 255 --------------------------#
[32m[20230117 13:32:31 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:31 @agent_ppo2.py:193][0m |           0.0004 |          29.9618 |           4.1260 |
[32m[20230117 13:32:31 @agent_ppo2.py:193][0m |          -0.0060 |          18.8484 |           4.1302 |
[32m[20230117 13:32:31 @agent_ppo2.py:193][0m |          -0.0096 |          17.4437 |           4.1300 |
[32m[20230117 13:32:31 @agent_ppo2.py:193][0m |          -0.0112 |          16.7809 |           4.1303 |
[32m[20230117 13:32:31 @agent_ppo2.py:193][0m |          -0.0118 |          16.3975 |           4.1326 |
[32m[20230117 13:32:31 @agent_ppo2.py:193][0m |          -0.0124 |          16.3594 |           4.1344 |
[32m[20230117 13:32:31 @agent_ppo2.py:193][0m |          -0.0124 |          16.0018 |           4.1328 |
[32m[20230117 13:32:31 @agent_ppo2.py:193][0m |          -0.0159 |          15.5252 |           4.1322 |
[32m[20230117 13:32:31 @agent_ppo2.py:193][0m |          -0.0163 |          15.2876 |           4.1346 |
[32m[20230117 13:32:31 @agent_ppo2.py:193][0m |          -0.0154 |          15.1421 |           4.1311 |
[32m[20230117 13:32:31 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:32:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 189.94
[32m[20230117 13:32:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.60
[32m[20230117 13:32:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.58
[32m[20230117 13:32:32 @agent_ppo2.py:151][0m Total time:       6.11 min
[32m[20230117 13:32:32 @agent_ppo2.py:153][0m 524288 total steps have happened
[32m[20230117 13:32:32 @agent_ppo2.py:129][0m #------------------------ Iteration 256 --------------------------#
[32m[20230117 13:32:32 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:32 @agent_ppo2.py:193][0m |           0.0011 |          21.3527 |           4.1510 |
[32m[20230117 13:32:32 @agent_ppo2.py:193][0m |          -0.0059 |          17.9238 |           4.1456 |
[32m[20230117 13:32:32 @agent_ppo2.py:193][0m |          -0.0078 |          16.8519 |           4.1419 |
[32m[20230117 13:32:32 @agent_ppo2.py:193][0m |          -0.0106 |          16.0392 |           4.1429 |
[32m[20230117 13:32:32 @agent_ppo2.py:193][0m |          -0.0100 |          15.3992 |           4.1434 |
[32m[20230117 13:32:32 @agent_ppo2.py:193][0m |          -0.0115 |          14.7656 |           4.1430 |
[32m[20230117 13:32:32 @agent_ppo2.py:193][0m |          -0.0130 |          14.2394 |           4.1411 |
[32m[20230117 13:32:32 @agent_ppo2.py:193][0m |          -0.0133 |          13.8282 |           4.1432 |
[32m[20230117 13:32:32 @agent_ppo2.py:193][0m |          -0.0134 |          13.4065 |           4.1428 |
[32m[20230117 13:32:33 @agent_ppo2.py:193][0m |          -0.0128 |          13.2132 |           4.1406 |
[32m[20230117 13:32:33 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:32:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.96
[32m[20230117 13:32:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.02
[32m[20230117 13:32:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.18
[32m[20230117 13:32:33 @agent_ppo2.py:151][0m Total time:       6.13 min
[32m[20230117 13:32:33 @agent_ppo2.py:153][0m 526336 total steps have happened
[32m[20230117 13:32:33 @agent_ppo2.py:129][0m #------------------------ Iteration 257 --------------------------#
[32m[20230117 13:32:33 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:32:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:33 @agent_ppo2.py:193][0m |           0.0006 |          19.5923 |           4.0918 |
[32m[20230117 13:32:33 @agent_ppo2.py:193][0m |          -0.0049 |          17.9771 |           4.0915 |
[32m[20230117 13:32:33 @agent_ppo2.py:193][0m |          -0.0063 |          17.5378 |           4.0834 |
[32m[20230117 13:32:33 @agent_ppo2.py:193][0m |          -0.0075 |          17.2382 |           4.0864 |
[32m[20230117 13:32:33 @agent_ppo2.py:193][0m |          -0.0076 |          17.2182 |           4.0843 |
[32m[20230117 13:32:33 @agent_ppo2.py:193][0m |          -0.0094 |          16.7406 |           4.0846 |
[32m[20230117 13:32:34 @agent_ppo2.py:193][0m |          -0.0094 |          16.6522 |           4.0884 |
[32m[20230117 13:32:34 @agent_ppo2.py:193][0m |          -0.0108 |          16.2845 |           4.0859 |
[32m[20230117 13:32:34 @agent_ppo2.py:193][0m |          -0.0109 |          16.1126 |           4.0877 |
[32m[20230117 13:32:34 @agent_ppo2.py:193][0m |          -0.0108 |          15.8967 |           4.0904 |
[32m[20230117 13:32:34 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:32:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.84
[32m[20230117 13:32:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.09
[32m[20230117 13:32:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.97
[32m[20230117 13:32:34 @agent_ppo2.py:151][0m Total time:       6.15 min
[32m[20230117 13:32:34 @agent_ppo2.py:153][0m 528384 total steps have happened
[32m[20230117 13:32:34 @agent_ppo2.py:129][0m #------------------------ Iteration 258 --------------------------#
[32m[20230117 13:32:34 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:34 @agent_ppo2.py:193][0m |           0.0029 |          18.1107 |           4.1285 |
[32m[20230117 13:32:34 @agent_ppo2.py:193][0m |           0.0155 |          20.0708 |           4.1191 |
[32m[20230117 13:32:34 @agent_ppo2.py:193][0m |          -0.0047 |          16.1022 |           4.1144 |
[32m[20230117 13:32:34 @agent_ppo2.py:193][0m |          -0.0123 |          15.5418 |           4.1106 |
[32m[20230117 13:32:35 @agent_ppo2.py:193][0m |          -0.0123 |          15.0451 |           4.1079 |
[32m[20230117 13:32:35 @agent_ppo2.py:193][0m |           0.0028 |          16.0001 |           4.1123 |
[32m[20230117 13:32:35 @agent_ppo2.py:193][0m |          -0.0161 |          14.2659 |           4.1109 |
[32m[20230117 13:32:35 @agent_ppo2.py:193][0m |          -0.0182 |          13.8830 |           4.1129 |
[32m[20230117 13:32:35 @agent_ppo2.py:193][0m |          -0.0149 |          13.5209 |           4.1116 |
[32m[20230117 13:32:35 @agent_ppo2.py:193][0m |          -0.0195 |          13.2551 |           4.1156 |
[32m[20230117 13:32:35 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:32:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.53
[32m[20230117 13:32:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.38
[32m[20230117 13:32:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.80
[32m[20230117 13:32:35 @agent_ppo2.py:151][0m Total time:       6.17 min
[32m[20230117 13:32:35 @agent_ppo2.py:153][0m 530432 total steps have happened
[32m[20230117 13:32:35 @agent_ppo2.py:129][0m #------------------------ Iteration 259 --------------------------#
[32m[20230117 13:32:35 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:32:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:35 @agent_ppo2.py:193][0m |          -0.0012 |          19.9528 |           4.2060 |
[32m[20230117 13:32:36 @agent_ppo2.py:193][0m |          -0.0048 |          17.4448 |           4.1973 |
[32m[20230117 13:32:36 @agent_ppo2.py:193][0m |          -0.0070 |          16.7247 |           4.1988 |
[32m[20230117 13:32:36 @agent_ppo2.py:193][0m |          -0.0085 |          16.2633 |           4.2015 |
[32m[20230117 13:32:36 @agent_ppo2.py:193][0m |          -0.0096 |          15.9589 |           4.1983 |
[32m[20230117 13:32:36 @agent_ppo2.py:193][0m |          -0.0098 |          15.6503 |           4.2017 |
[32m[20230117 13:32:36 @agent_ppo2.py:193][0m |          -0.0107 |          15.4040 |           4.2016 |
[32m[20230117 13:32:36 @agent_ppo2.py:193][0m |          -0.0121 |          15.2553 |           4.2051 |
[32m[20230117 13:32:36 @agent_ppo2.py:193][0m |          -0.0120 |          15.0497 |           4.2052 |
[32m[20230117 13:32:36 @agent_ppo2.py:193][0m |          -0.0129 |          14.8099 |           4.2102 |
[32m[20230117 13:32:36 @agent_ppo2.py:138][0m Policy update time: 0.80 s
[32m[20230117 13:32:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 259.98
[32m[20230117 13:32:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.55
[32m[20230117 13:32:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.05
[32m[20230117 13:32:36 @agent_ppo2.py:151][0m Total time:       6.19 min
[32m[20230117 13:32:36 @agent_ppo2.py:153][0m 532480 total steps have happened
[32m[20230117 13:32:36 @agent_ppo2.py:129][0m #------------------------ Iteration 260 --------------------------#
[32m[20230117 13:32:37 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:37 @agent_ppo2.py:193][0m |          -0.0025 |          32.2778 |           4.1280 |
[32m[20230117 13:32:37 @agent_ppo2.py:193][0m |          -0.0101 |          16.8649 |           4.1131 |
[32m[20230117 13:32:37 @agent_ppo2.py:193][0m |          -0.0127 |          14.9437 |           4.1128 |
[32m[20230117 13:32:37 @agent_ppo2.py:193][0m |          -0.0140 |          14.0784 |           4.1104 |
[32m[20230117 13:32:37 @agent_ppo2.py:193][0m |          -0.0157 |          13.4623 |           4.1100 |
[32m[20230117 13:32:37 @agent_ppo2.py:193][0m |          -0.0151 |          12.9415 |           4.1088 |
[32m[20230117 13:32:37 @agent_ppo2.py:193][0m |          -0.0191 |          12.5362 |           4.1077 |
[32m[20230117 13:32:37 @agent_ppo2.py:193][0m |          -0.0191 |          12.2530 |           4.1095 |
[32m[20230117 13:32:37 @agent_ppo2.py:193][0m |          -0.0203 |          11.8164 |           4.1088 |
[32m[20230117 13:32:37 @agent_ppo2.py:193][0m |          -0.0193 |          11.4520 |           4.1052 |
[32m[20230117 13:32:37 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:32:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 150.62
[32m[20230117 13:32:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.03
[32m[20230117 13:32:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.45
[32m[20230117 13:32:37 @agent_ppo2.py:151][0m Total time:       6.21 min
[32m[20230117 13:32:37 @agent_ppo2.py:153][0m 534528 total steps have happened
[32m[20230117 13:32:37 @agent_ppo2.py:129][0m #------------------------ Iteration 261 --------------------------#
[32m[20230117 13:32:38 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:38 @agent_ppo2.py:193][0m |          -0.0003 |          21.2925 |           4.2305 |
[32m[20230117 13:32:38 @agent_ppo2.py:193][0m |          -0.0037 |          19.1247 |           4.2289 |
[32m[20230117 13:32:38 @agent_ppo2.py:193][0m |          -0.0066 |          18.5026 |           4.2257 |
[32m[20230117 13:32:38 @agent_ppo2.py:193][0m |          -0.0073 |          18.1395 |           4.2207 |
[32m[20230117 13:32:38 @agent_ppo2.py:193][0m |          -0.0083 |          17.9436 |           4.2251 |
[32m[20230117 13:32:38 @agent_ppo2.py:193][0m |          -0.0090 |          17.7073 |           4.2227 |
[32m[20230117 13:32:38 @agent_ppo2.py:193][0m |          -0.0095 |          17.6876 |           4.2224 |
[32m[20230117 13:32:38 @agent_ppo2.py:193][0m |          -0.0104 |          17.4551 |           4.2216 |
[32m[20230117 13:32:38 @agent_ppo2.py:193][0m |          -0.0110 |          17.3200 |           4.2186 |
[32m[20230117 13:32:38 @agent_ppo2.py:193][0m |          -0.0118 |          17.2196 |           4.2228 |
[32m[20230117 13:32:38 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:32:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 267.96
[32m[20230117 13:32:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.96
[32m[20230117 13:32:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 123.51
[32m[20230117 13:32:39 @agent_ppo2.py:151][0m Total time:       6.23 min
[32m[20230117 13:32:39 @agent_ppo2.py:153][0m 536576 total steps have happened
[32m[20230117 13:32:39 @agent_ppo2.py:129][0m #------------------------ Iteration 262 --------------------------#
[32m[20230117 13:32:39 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:32:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:39 @agent_ppo2.py:193][0m |           0.0003 |          19.4643 |           4.3006 |
[32m[20230117 13:32:39 @agent_ppo2.py:193][0m |          -0.0051 |          18.1626 |           4.2862 |
[32m[20230117 13:32:39 @agent_ppo2.py:193][0m |          -0.0066 |          17.4918 |           4.2867 |
[32m[20230117 13:32:39 @agent_ppo2.py:193][0m |          -0.0074 |          17.0237 |           4.2827 |
[32m[20230117 13:32:39 @agent_ppo2.py:193][0m |          -0.0089 |          16.6721 |           4.2865 |
[32m[20230117 13:32:39 @agent_ppo2.py:193][0m |          -0.0098 |          16.3584 |           4.2841 |
[32m[20230117 13:32:39 @agent_ppo2.py:193][0m |          -0.0105 |          16.2121 |           4.2857 |
[32m[20230117 13:32:39 @agent_ppo2.py:193][0m |          -0.0114 |          16.0585 |           4.2876 |
[32m[20230117 13:32:40 @agent_ppo2.py:193][0m |          -0.0124 |          15.8564 |           4.2833 |
[32m[20230117 13:32:40 @agent_ppo2.py:193][0m |          -0.0122 |          15.7856 |           4.2861 |
[32m[20230117 13:32:40 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:32:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.40
[32m[20230117 13:32:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.67
[32m[20230117 13:32:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 30.75
[32m[20230117 13:32:40 @agent_ppo2.py:151][0m Total time:       6.25 min
[32m[20230117 13:32:40 @agent_ppo2.py:153][0m 538624 total steps have happened
[32m[20230117 13:32:40 @agent_ppo2.py:129][0m #------------------------ Iteration 263 --------------------------#
[32m[20230117 13:32:40 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:40 @agent_ppo2.py:193][0m |          -0.0010 |          18.6605 |           4.1191 |
[32m[20230117 13:32:40 @agent_ppo2.py:193][0m |          -0.0088 |          17.5962 |           4.1088 |
[32m[20230117 13:32:40 @agent_ppo2.py:193][0m |          -0.0118 |          17.0859 |           4.1067 |
[32m[20230117 13:32:40 @agent_ppo2.py:193][0m |          -0.0123 |          16.7112 |           4.1079 |
[32m[20230117 13:32:40 @agent_ppo2.py:193][0m |          -0.0138 |          16.4145 |           4.1045 |
[32m[20230117 13:32:40 @agent_ppo2.py:193][0m |          -0.0132 |          16.1272 |           4.0999 |
[32m[20230117 13:32:41 @agent_ppo2.py:193][0m |          -0.0135 |          16.0022 |           4.0992 |
[32m[20230117 13:32:41 @agent_ppo2.py:193][0m |          -0.0159 |          15.6319 |           4.0971 |
[32m[20230117 13:32:41 @agent_ppo2.py:193][0m |          -0.0164 |          15.4707 |           4.0954 |
[32m[20230117 13:32:41 @agent_ppo2.py:193][0m |          -0.0147 |          15.2516 |           4.0969 |
[32m[20230117 13:32:41 @agent_ppo2.py:138][0m Policy update time: 0.81 s
[32m[20230117 13:32:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.74
[32m[20230117 13:32:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.93
[32m[20230117 13:32:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.30
[32m[20230117 13:32:41 @agent_ppo2.py:151][0m Total time:       6.27 min
[32m[20230117 13:32:41 @agent_ppo2.py:153][0m 540672 total steps have happened
[32m[20230117 13:32:41 @agent_ppo2.py:129][0m #------------------------ Iteration 264 --------------------------#
[32m[20230117 13:32:41 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:41 @agent_ppo2.py:193][0m |           0.0019 |          18.4129 |           4.2854 |
[32m[20230117 13:32:41 @agent_ppo2.py:193][0m |          -0.0055 |          17.1433 |           4.2783 |
[32m[20230117 13:32:42 @agent_ppo2.py:193][0m |          -0.0073 |          16.6523 |           4.2726 |
[32m[20230117 13:32:42 @agent_ppo2.py:193][0m |          -0.0083 |          16.2893 |           4.2736 |
[32m[20230117 13:32:42 @agent_ppo2.py:193][0m |          -0.0091 |          16.0418 |           4.2776 |
[32m[20230117 13:32:42 @agent_ppo2.py:193][0m |          -0.0081 |          15.8548 |           4.2767 |
[32m[20230117 13:32:42 @agent_ppo2.py:193][0m |          -0.0091 |          15.7219 |           4.2796 |
[32m[20230117 13:32:42 @agent_ppo2.py:193][0m |          -0.0101 |          15.5247 |           4.2862 |
[32m[20230117 13:32:42 @agent_ppo2.py:193][0m |          -0.0113 |          15.2910 |           4.2777 |
[32m[20230117 13:32:42 @agent_ppo2.py:193][0m |          -0.0118 |          15.1893 |           4.2849 |
[32m[20230117 13:32:42 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:32:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.55
[32m[20230117 13:32:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.19
[32m[20230117 13:32:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.75
[32m[20230117 13:32:42 @agent_ppo2.py:151][0m Total time:       6.29 min
[32m[20230117 13:32:42 @agent_ppo2.py:153][0m 542720 total steps have happened
[32m[20230117 13:32:42 @agent_ppo2.py:129][0m #------------------------ Iteration 265 --------------------------#
[32m[20230117 13:32:42 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:43 @agent_ppo2.py:193][0m |          -0.0011 |          23.9881 |           4.2527 |
[32m[20230117 13:32:43 @agent_ppo2.py:193][0m |          -0.0093 |          18.1299 |           4.2423 |
[32m[20230117 13:32:43 @agent_ppo2.py:193][0m |          -0.0085 |          17.2413 |           4.2368 |
[32m[20230117 13:32:43 @agent_ppo2.py:193][0m |          -0.0054 |          17.2543 |           4.2359 |
[32m[20230117 13:32:43 @agent_ppo2.py:193][0m |          -0.0131 |          16.1545 |           4.2338 |
[32m[20230117 13:32:43 @agent_ppo2.py:193][0m |          -0.0137 |          15.8807 |           4.2317 |
[32m[20230117 13:32:43 @agent_ppo2.py:193][0m |          -0.0137 |          15.5747 |           4.2280 |
[32m[20230117 13:32:43 @agent_ppo2.py:193][0m |          -0.0082 |          15.4119 |           4.2247 |
[32m[20230117 13:32:43 @agent_ppo2.py:193][0m |          -0.0121 |          15.2025 |           4.2217 |
[32m[20230117 13:32:43 @agent_ppo2.py:193][0m |          -0.0147 |          15.0548 |           4.2195 |
[32m[20230117 13:32:43 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:32:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 189.83
[32m[20230117 13:32:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.73
[32m[20230117 13:32:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.20
[32m[20230117 13:32:43 @agent_ppo2.py:151][0m Total time:       6.31 min
[32m[20230117 13:32:43 @agent_ppo2.py:153][0m 544768 total steps have happened
[32m[20230117 13:32:43 @agent_ppo2.py:129][0m #------------------------ Iteration 266 --------------------------#
[32m[20230117 13:32:44 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:44 @agent_ppo2.py:193][0m |          -0.0004 |          17.7977 |           4.1780 |
[32m[20230117 13:32:44 @agent_ppo2.py:193][0m |           0.0006 |          17.3201 |           4.1776 |
[32m[20230117 13:32:44 @agent_ppo2.py:193][0m |          -0.0021 |          16.6245 |           4.1716 |
[32m[20230117 13:32:44 @agent_ppo2.py:193][0m |          -0.0091 |          16.0665 |           4.1747 |
[32m[20230117 13:32:44 @agent_ppo2.py:193][0m |          -0.0083 |          15.8196 |           4.1720 |
[32m[20230117 13:32:44 @agent_ppo2.py:193][0m |          -0.0105 |          15.6366 |           4.1693 |
[32m[20230117 13:32:44 @agent_ppo2.py:193][0m |          -0.0115 |          15.4929 |           4.1668 |
[32m[20230117 13:32:44 @agent_ppo2.py:193][0m |          -0.0103 |          15.4082 |           4.1651 |
[32m[20230117 13:32:44 @agent_ppo2.py:193][0m |          -0.0106 |          15.2875 |           4.1634 |
[32m[20230117 13:32:44 @agent_ppo2.py:193][0m |          -0.0125 |          15.1332 |           4.1622 |
[32m[20230117 13:32:44 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:32:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 267.97
[32m[20230117 13:32:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 271.28
[32m[20230117 13:32:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.81
[32m[20230117 13:32:45 @agent_ppo2.py:151][0m Total time:       6.33 min
[32m[20230117 13:32:45 @agent_ppo2.py:153][0m 546816 total steps have happened
[32m[20230117 13:32:45 @agent_ppo2.py:129][0m #------------------------ Iteration 267 --------------------------#
[32m[20230117 13:32:45 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:32:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:45 @agent_ppo2.py:193][0m |          -0.0013 |          16.8023 |           4.2126 |
[32m[20230117 13:32:45 @agent_ppo2.py:193][0m |          -0.0059 |          15.3588 |           4.2063 |
[32m[20230117 13:32:45 @agent_ppo2.py:193][0m |          -0.0084 |          14.7039 |           4.2047 |
[32m[20230117 13:32:45 @agent_ppo2.py:193][0m |          -0.0095 |          14.2381 |           4.2022 |
[32m[20230117 13:32:45 @agent_ppo2.py:193][0m |          -0.0106 |          13.9467 |           4.2016 |
[32m[20230117 13:32:45 @agent_ppo2.py:193][0m |          -0.0118 |          13.7397 |           4.2001 |
[32m[20230117 13:32:45 @agent_ppo2.py:193][0m |          -0.0117 |          13.5676 |           4.2023 |
[32m[20230117 13:32:45 @agent_ppo2.py:193][0m |          -0.0115 |          13.4083 |           4.2010 |
[32m[20230117 13:32:45 @agent_ppo2.py:193][0m |          -0.0124 |          13.2436 |           4.2008 |
[32m[20230117 13:32:46 @agent_ppo2.py:193][0m |          -0.0122 |          13.1163 |           4.1983 |
[32m[20230117 13:32:46 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:32:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.71
[32m[20230117 13:32:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.73
[32m[20230117 13:32:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.70
[32m[20230117 13:32:46 @agent_ppo2.py:151][0m Total time:       6.35 min
[32m[20230117 13:32:46 @agent_ppo2.py:153][0m 548864 total steps have happened
[32m[20230117 13:32:46 @agent_ppo2.py:129][0m #------------------------ Iteration 268 --------------------------#
[32m[20230117 13:32:46 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:32:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:46 @agent_ppo2.py:193][0m |          -0.0032 |          34.6390 |           4.1926 |
[32m[20230117 13:32:46 @agent_ppo2.py:193][0m |          -0.0083 |          21.5425 |           4.1853 |
[32m[20230117 13:32:46 @agent_ppo2.py:193][0m |          -0.0095 |          19.5624 |           4.1827 |
[32m[20230117 13:32:46 @agent_ppo2.py:193][0m |          -0.0137 |          17.8577 |           4.1807 |
[32m[20230117 13:32:46 @agent_ppo2.py:193][0m |          -0.0167 |          17.1706 |           4.1788 |
[32m[20230117 13:32:47 @agent_ppo2.py:193][0m |          -0.0178 |          16.5398 |           4.1767 |
[32m[20230117 13:32:47 @agent_ppo2.py:193][0m |          -0.0186 |          16.2016 |           4.1767 |
[32m[20230117 13:32:47 @agent_ppo2.py:193][0m |          -0.0183 |          16.0356 |           4.1732 |
[32m[20230117 13:32:47 @agent_ppo2.py:193][0m |          -0.0200 |          15.5755 |           4.1706 |
[32m[20230117 13:32:47 @agent_ppo2.py:193][0m |          -0.0211 |          15.2347 |           4.1711 |
[32m[20230117 13:32:47 @agent_ppo2.py:138][0m Policy update time: 0.82 s
[32m[20230117 13:32:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 132.24
[32m[20230117 13:32:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.89
[32m[20230117 13:32:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.68
[32m[20230117 13:32:47 @agent_ppo2.py:151][0m Total time:       6.37 min
[32m[20230117 13:32:47 @agent_ppo2.py:153][0m 550912 total steps have happened
[32m[20230117 13:32:47 @agent_ppo2.py:129][0m #------------------------ Iteration 269 --------------------------#
[32m[20230117 13:32:47 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:32:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:47 @agent_ppo2.py:193][0m |           0.0002 |          21.3665 |           4.3086 |
[32m[20230117 13:32:47 @agent_ppo2.py:193][0m |          -0.0041 |          19.1766 |           4.3019 |
[32m[20230117 13:32:48 @agent_ppo2.py:193][0m |          -0.0063 |          18.3180 |           4.2956 |
[32m[20230117 13:32:48 @agent_ppo2.py:193][0m |          -0.0080 |          17.6494 |           4.2926 |
[32m[20230117 13:32:48 @agent_ppo2.py:193][0m |          -0.0087 |          17.2550 |           4.2917 |
[32m[20230117 13:32:48 @agent_ppo2.py:193][0m |          -0.0093 |          16.8953 |           4.2908 |
[32m[20230117 13:32:48 @agent_ppo2.py:193][0m |          -0.0101 |          16.7532 |           4.2871 |
[32m[20230117 13:32:48 @agent_ppo2.py:193][0m |          -0.0116 |          16.3544 |           4.2884 |
[32m[20230117 13:32:48 @agent_ppo2.py:193][0m |          -0.0103 |          16.1943 |           4.2829 |
[32m[20230117 13:32:48 @agent_ppo2.py:193][0m |          -0.0113 |          15.9517 |           4.2864 |
[32m[20230117 13:32:48 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:32:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 260.54
[32m[20230117 13:32:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.05
[32m[20230117 13:32:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 279.07
[32m[20230117 13:32:48 @agent_ppo2.py:151][0m Total time:       6.39 min
[32m[20230117 13:32:48 @agent_ppo2.py:153][0m 552960 total steps have happened
[32m[20230117 13:32:48 @agent_ppo2.py:129][0m #------------------------ Iteration 270 --------------------------#
[32m[20230117 13:32:48 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:49 @agent_ppo2.py:193][0m |          -0.0016 |          19.3750 |           4.1744 |
[32m[20230117 13:32:49 @agent_ppo2.py:193][0m |          -0.0067 |          18.3390 |           4.1696 |
[32m[20230117 13:32:49 @agent_ppo2.py:193][0m |          -0.0085 |          18.0599 |           4.1658 |
[32m[20230117 13:32:49 @agent_ppo2.py:193][0m |          -0.0096 |          17.8492 |           4.1674 |
[32m[20230117 13:32:49 @agent_ppo2.py:193][0m |          -0.0106 |          17.6922 |           4.1648 |
[32m[20230117 13:32:49 @agent_ppo2.py:193][0m |          -0.0115 |          17.5251 |           4.1643 |
[32m[20230117 13:32:49 @agent_ppo2.py:193][0m |          -0.0111 |          17.5830 |           4.1674 |
[32m[20230117 13:32:49 @agent_ppo2.py:193][0m |          -0.0106 |          17.3908 |           4.1632 |
[32m[20230117 13:32:49 @agent_ppo2.py:193][0m |          -0.0113 |          17.3659 |           4.1637 |
[32m[20230117 13:32:49 @agent_ppo2.py:193][0m |          -0.0125 |          17.0801 |           4.1625 |
[32m[20230117 13:32:49 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:32:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.52
[32m[20230117 13:32:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.17
[32m[20230117 13:32:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.02
[32m[20230117 13:32:49 @agent_ppo2.py:151][0m Total time:       6.41 min
[32m[20230117 13:32:49 @agent_ppo2.py:153][0m 555008 total steps have happened
[32m[20230117 13:32:49 @agent_ppo2.py:129][0m #------------------------ Iteration 271 --------------------------#
[32m[20230117 13:32:50 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:32:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:50 @agent_ppo2.py:193][0m |           0.0010 |          18.9309 |           4.1525 |
[32m[20230117 13:32:50 @agent_ppo2.py:193][0m |          -0.0042 |          15.0574 |           4.1493 |
[32m[20230117 13:32:50 @agent_ppo2.py:193][0m |          -0.0070 |          14.1754 |           4.1473 |
[32m[20230117 13:32:50 @agent_ppo2.py:193][0m |          -0.0076 |          13.5627 |           4.1459 |
[32m[20230117 13:32:50 @agent_ppo2.py:193][0m |          -0.0083 |          13.2800 |           4.1443 |
[32m[20230117 13:32:50 @agent_ppo2.py:193][0m |          -0.0083 |          13.0425 |           4.1412 |
[32m[20230117 13:32:50 @agent_ppo2.py:193][0m |          -0.0097 |          12.8436 |           4.1448 |
[32m[20230117 13:32:50 @agent_ppo2.py:193][0m |          -0.0101 |          12.7111 |           4.1431 |
[32m[20230117 13:32:50 @agent_ppo2.py:193][0m |          -0.0111 |          12.5542 |           4.1456 |
[32m[20230117 13:32:50 @agent_ppo2.py:193][0m |          -0.0113 |          12.3975 |           4.1423 |
[32m[20230117 13:32:50 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:32:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 230.04
[32m[20230117 13:32:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 270.43
[32m[20230117 13:32:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.35
[32m[20230117 13:32:51 @agent_ppo2.py:151][0m Total time:       6.43 min
[32m[20230117 13:32:51 @agent_ppo2.py:153][0m 557056 total steps have happened
[32m[20230117 13:32:51 @agent_ppo2.py:129][0m #------------------------ Iteration 272 --------------------------#
[32m[20230117 13:32:51 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:32:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:51 @agent_ppo2.py:193][0m |           0.0002 |          13.0381 |           4.1726 |
[32m[20230117 13:32:51 @agent_ppo2.py:193][0m |          -0.0047 |           8.2859 |           4.1635 |
[32m[20230117 13:32:51 @agent_ppo2.py:193][0m |          -0.0068 |           7.0905 |           4.1587 |
[32m[20230117 13:32:51 @agent_ppo2.py:193][0m |          -0.0086 |           6.3274 |           4.1597 |
[32m[20230117 13:32:51 @agent_ppo2.py:193][0m |          -0.0082 |           5.7330 |           4.1599 |
[32m[20230117 13:32:51 @agent_ppo2.py:193][0m |          -0.0106 |           5.2864 |           4.1588 |
[32m[20230117 13:32:51 @agent_ppo2.py:193][0m |          -0.0104 |           4.9351 |           4.1569 |
[32m[20230117 13:32:51 @agent_ppo2.py:193][0m |          -0.0099 |           4.6416 |           4.1513 |
[32m[20230117 13:32:52 @agent_ppo2.py:193][0m |          -0.0116 |           4.3223 |           4.1552 |
[32m[20230117 13:32:52 @agent_ppo2.py:193][0m |          -0.0124 |           4.0606 |           4.1541 |
[32m[20230117 13:32:52 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:32:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.62
[32m[20230117 13:32:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.38
[32m[20230117 13:32:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.10
[32m[20230117 13:32:52 @agent_ppo2.py:151][0m Total time:       6.45 min
[32m[20230117 13:32:52 @agent_ppo2.py:153][0m 559104 total steps have happened
[32m[20230117 13:32:52 @agent_ppo2.py:129][0m #------------------------ Iteration 273 --------------------------#
[32m[20230117 13:32:52 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:32:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:52 @agent_ppo2.py:193][0m |           0.0199 |          29.7906 |           4.0549 |
[32m[20230117 13:32:52 @agent_ppo2.py:193][0m |          -0.0054 |          20.7439 |           4.0332 |
[32m[20230117 13:32:52 @agent_ppo2.py:193][0m |          -0.0057 |          19.1235 |           4.0365 |
[32m[20230117 13:32:52 @agent_ppo2.py:193][0m |          -0.0083 |          17.6634 |           4.0347 |
[32m[20230117 13:32:52 @agent_ppo2.py:193][0m |          -0.0016 |          16.8981 |           4.0342 |
[32m[20230117 13:32:52 @agent_ppo2.py:193][0m |          -0.0091 |          16.3004 |           4.0186 |
[32m[20230117 13:32:53 @agent_ppo2.py:193][0m |          -0.0040 |          16.5524 |           4.0292 |
[32m[20230117 13:32:53 @agent_ppo2.py:193][0m |          -0.0145 |          15.5042 |           4.0284 |
[32m[20230117 13:32:53 @agent_ppo2.py:193][0m |          -0.0180 |          15.0275 |           4.0225 |
[32m[20230117 13:32:53 @agent_ppo2.py:193][0m |          -0.0194 |          15.1780 |           4.0215 |
[32m[20230117 13:32:53 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:32:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 138.51
[32m[20230117 13:32:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 271.10
[32m[20230117 13:32:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.65
[32m[20230117 13:32:53 @agent_ppo2.py:151][0m Total time:       6.47 min
[32m[20230117 13:32:53 @agent_ppo2.py:153][0m 561152 total steps have happened
[32m[20230117 13:32:53 @agent_ppo2.py:129][0m #------------------------ Iteration 274 --------------------------#
[32m[20230117 13:32:53 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:53 @agent_ppo2.py:193][0m |           0.0016 |          19.7158 |           4.0628 |
[32m[20230117 13:32:53 @agent_ppo2.py:193][0m |          -0.0040 |          18.0065 |           4.0522 |
[32m[20230117 13:32:53 @agent_ppo2.py:193][0m |          -0.0051 |          17.5920 |           4.0491 |
[32m[20230117 13:32:54 @agent_ppo2.py:193][0m |          -0.0077 |          17.3377 |           4.0441 |
[32m[20230117 13:32:54 @agent_ppo2.py:193][0m |          -0.0071 |          17.2992 |           4.0444 |
[32m[20230117 13:32:54 @agent_ppo2.py:193][0m |          -0.0082 |          17.0599 |           4.0396 |
[32m[20230117 13:32:54 @agent_ppo2.py:193][0m |          -0.0095 |          16.8898 |           4.0424 |
[32m[20230117 13:32:54 @agent_ppo2.py:193][0m |          -0.0072 |          16.9470 |           4.0379 |
[32m[20230117 13:32:54 @agent_ppo2.py:193][0m |          -0.0082 |          16.7910 |           4.0423 |
[32m[20230117 13:32:54 @agent_ppo2.py:193][0m |          -0.0099 |          16.5474 |           4.0354 |
[32m[20230117 13:32:54 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:32:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.88
[32m[20230117 13:32:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.09
[32m[20230117 13:32:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 145.99
[32m[20230117 13:32:54 @agent_ppo2.py:151][0m Total time:       6.48 min
[32m[20230117 13:32:54 @agent_ppo2.py:153][0m 563200 total steps have happened
[32m[20230117 13:32:54 @agent_ppo2.py:129][0m #------------------------ Iteration 275 --------------------------#
[32m[20230117 13:32:54 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:32:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:55 @agent_ppo2.py:193][0m |          -0.0018 |          22.5747 |           4.0732 |
[32m[20230117 13:32:55 @agent_ppo2.py:193][0m |          -0.0082 |          19.5624 |           4.0570 |
[32m[20230117 13:32:55 @agent_ppo2.py:193][0m |          -0.0093 |          18.8223 |           4.0574 |
[32m[20230117 13:32:55 @agent_ppo2.py:193][0m |          -0.0108 |          18.3620 |           4.0560 |
[32m[20230117 13:32:55 @agent_ppo2.py:193][0m |          -0.0121 |          18.0074 |           4.0566 |
[32m[20230117 13:32:55 @agent_ppo2.py:193][0m |          -0.0128 |          17.7336 |           4.0502 |
[32m[20230117 13:32:55 @agent_ppo2.py:193][0m |          -0.0133 |          17.5216 |           4.0529 |
[32m[20230117 13:32:55 @agent_ppo2.py:193][0m |          -0.0140 |          17.3402 |           4.0506 |
[32m[20230117 13:32:55 @agent_ppo2.py:193][0m |          -0.0147 |          17.1740 |           4.0528 |
[32m[20230117 13:32:55 @agent_ppo2.py:193][0m |          -0.0147 |          17.0153 |           4.0505 |
[32m[20230117 13:32:55 @agent_ppo2.py:138][0m Policy update time: 0.81 s
[32m[20230117 13:32:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 210.46
[32m[20230117 13:32:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 272.79
[32m[20230117 13:32:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 102.09
[32m[20230117 13:32:55 @agent_ppo2.py:151][0m Total time:       6.51 min
[32m[20230117 13:32:55 @agent_ppo2.py:153][0m 565248 total steps have happened
[32m[20230117 13:32:55 @agent_ppo2.py:129][0m #------------------------ Iteration 276 --------------------------#
[32m[20230117 13:32:56 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:32:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:56 @agent_ppo2.py:193][0m |          -0.0003 |          35.5005 |           4.0198 |
[32m[20230117 13:32:56 @agent_ppo2.py:193][0m |          -0.0076 |          22.0080 |           4.0112 |
[32m[20230117 13:32:56 @agent_ppo2.py:193][0m |          -0.0097 |          20.4294 |           4.0051 |
[32m[20230117 13:32:56 @agent_ppo2.py:193][0m |          -0.0105 |          19.6457 |           4.0042 |
[32m[20230117 13:32:56 @agent_ppo2.py:193][0m |          -0.0092 |          19.9132 |           4.0043 |
[32m[20230117 13:32:56 @agent_ppo2.py:193][0m |          -0.0095 |          18.7984 |           4.0014 |
[32m[20230117 13:32:56 @agent_ppo2.py:193][0m |          -0.0163 |          18.5075 |           3.9954 |
[32m[20230117 13:32:56 @agent_ppo2.py:193][0m |          -0.0135 |          18.2165 |           3.9912 |
[32m[20230117 13:32:56 @agent_ppo2.py:193][0m |          -0.0156 |          18.0266 |           3.9913 |
[32m[20230117 13:32:56 @agent_ppo2.py:193][0m |          -0.0140 |          17.8039 |           3.9916 |
[32m[20230117 13:32:56 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:32:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 200.67
[32m[20230117 13:32:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.65
[32m[20230117 13:32:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -14.35
[32m[20230117 13:32:57 @agent_ppo2.py:151][0m Total time:       6.53 min
[32m[20230117 13:32:57 @agent_ppo2.py:153][0m 567296 total steps have happened
[32m[20230117 13:32:57 @agent_ppo2.py:129][0m #------------------------ Iteration 277 --------------------------#
[32m[20230117 13:32:57 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:32:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:57 @agent_ppo2.py:193][0m |          -0.0016 |          16.8712 |           4.0124 |
[32m[20230117 13:32:57 @agent_ppo2.py:193][0m |          -0.0033 |          16.3457 |           4.0058 |
[32m[20230117 13:32:57 @agent_ppo2.py:193][0m |          -0.0063 |          15.9867 |           4.0027 |
[32m[20230117 13:32:57 @agent_ppo2.py:193][0m |          -0.0088 |          15.7705 |           3.9992 |
[32m[20230117 13:32:57 @agent_ppo2.py:193][0m |          -0.0087 |          15.5939 |           3.9999 |
[32m[20230117 13:32:57 @agent_ppo2.py:193][0m |          -0.0094 |          15.4487 |           4.0026 |
[32m[20230117 13:32:57 @agent_ppo2.py:193][0m |          -0.0097 |          15.2988 |           4.0010 |
[32m[20230117 13:32:57 @agent_ppo2.py:193][0m |          -0.0128 |          15.1678 |           3.9992 |
[32m[20230117 13:32:57 @agent_ppo2.py:193][0m |          -0.0064 |          15.6902 |           4.0015 |
[32m[20230117 13:32:58 @agent_ppo2.py:193][0m |          -0.0132 |          14.8770 |           3.9999 |
[32m[20230117 13:32:58 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:32:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.48
[32m[20230117 13:32:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.48
[32m[20230117 13:32:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.19
[32m[20230117 13:32:58 @agent_ppo2.py:151][0m Total time:       6.55 min
[32m[20230117 13:32:58 @agent_ppo2.py:153][0m 569344 total steps have happened
[32m[20230117 13:32:58 @agent_ppo2.py:129][0m #------------------------ Iteration 278 --------------------------#
[32m[20230117 13:32:58 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:32:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:58 @agent_ppo2.py:193][0m |           0.0006 |          16.6165 |           3.9490 |
[32m[20230117 13:32:58 @agent_ppo2.py:193][0m |          -0.0021 |          15.9539 |           3.9500 |
[32m[20230117 13:32:58 @agent_ppo2.py:193][0m |          -0.0058 |          15.4014 |           3.9445 |
[32m[20230117 13:32:58 @agent_ppo2.py:193][0m |          -0.0073 |          15.1571 |           3.9402 |
[32m[20230117 13:32:58 @agent_ppo2.py:193][0m |          -0.0081 |          14.8611 |           3.9412 |
[32m[20230117 13:32:58 @agent_ppo2.py:193][0m |          -0.0088 |          14.6628 |           3.9379 |
[32m[20230117 13:32:59 @agent_ppo2.py:193][0m |          -0.0086 |          14.4552 |           3.9391 |
[32m[20230117 13:32:59 @agent_ppo2.py:193][0m |          -0.0104 |          14.0740 |           3.9357 |
[32m[20230117 13:32:59 @agent_ppo2.py:193][0m |          -0.0110 |          13.8711 |           3.9376 |
[32m[20230117 13:32:59 @agent_ppo2.py:193][0m |          -0.0107 |          13.6689 |           3.9355 |
[32m[20230117 13:32:59 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:32:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.59
[32m[20230117 13:32:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.88
[32m[20230117 13:32:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 8.74
[32m[20230117 13:32:59 @agent_ppo2.py:151][0m Total time:       6.56 min
[32m[20230117 13:32:59 @agent_ppo2.py:153][0m 571392 total steps have happened
[32m[20230117 13:32:59 @agent_ppo2.py:129][0m #------------------------ Iteration 279 --------------------------#
[32m[20230117 13:32:59 @agent_ppo2.py:135][0m Sampling time: 0.29 s by 4 slaves
[32m[20230117 13:32:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:32:59 @agent_ppo2.py:193][0m |           0.0020 |          24.3815 |           4.0848 |
[32m[20230117 13:32:59 @agent_ppo2.py:193][0m |          -0.0053 |          18.7723 |           4.0850 |
[32m[20230117 13:32:59 @agent_ppo2.py:193][0m |          -0.0079 |          16.7037 |           4.0816 |
[32m[20230117 13:33:00 @agent_ppo2.py:193][0m |          -0.0096 |          15.6779 |           4.0773 |
[32m[20230117 13:33:00 @agent_ppo2.py:193][0m |          -0.0100 |          15.0449 |           4.0760 |
[32m[20230117 13:33:00 @agent_ppo2.py:193][0m |          -0.0123 |          14.3077 |           4.0750 |
[32m[20230117 13:33:00 @agent_ppo2.py:193][0m |          -0.0129 |          13.9381 |           4.0706 |
[32m[20230117 13:33:00 @agent_ppo2.py:193][0m |          -0.0130 |          13.6391 |           4.0700 |
[32m[20230117 13:33:00 @agent_ppo2.py:193][0m |          -0.0124 |          13.3972 |           4.0698 |
[32m[20230117 13:33:00 @agent_ppo2.py:193][0m |          -0.0138 |          13.0955 |           4.0684 |
[32m[20230117 13:33:00 @agent_ppo2.py:138][0m Policy update time: 0.82 s
[32m[20230117 13:33:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 215.45
[32m[20230117 13:33:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.40
[32m[20230117 13:33:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.72
[32m[20230117 13:33:00 @agent_ppo2.py:151][0m Total time:       6.59 min
[32m[20230117 13:33:00 @agent_ppo2.py:153][0m 573440 total steps have happened
[32m[20230117 13:33:00 @agent_ppo2.py:129][0m #------------------------ Iteration 280 --------------------------#
[32m[20230117 13:33:00 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:00 @agent_ppo2.py:193][0m |          -0.0008 |          19.7712 |           4.0857 |
[32m[20230117 13:33:01 @agent_ppo2.py:193][0m |          -0.0037 |          16.9208 |           4.0762 |
[32m[20230117 13:33:01 @agent_ppo2.py:193][0m |          -0.0059 |          15.8598 |           4.0808 |
[32m[20230117 13:33:01 @agent_ppo2.py:193][0m |          -0.0064 |          15.3717 |           4.0846 |
[32m[20230117 13:33:01 @agent_ppo2.py:193][0m |          -0.0076 |          14.8729 |           4.0800 |
[32m[20230117 13:33:01 @agent_ppo2.py:193][0m |          -0.0077 |          14.5888 |           4.0820 |
[32m[20230117 13:33:01 @agent_ppo2.py:193][0m |          -0.0068 |          14.3745 |           4.0808 |
[32m[20230117 13:33:01 @agent_ppo2.py:193][0m |          -0.0098 |          14.1481 |           4.0856 |
[32m[20230117 13:33:01 @agent_ppo2.py:193][0m |          -0.0109 |          13.7265 |           4.0844 |
[32m[20230117 13:33:01 @agent_ppo2.py:193][0m |          -0.0111 |          13.4971 |           4.0859 |
[32m[20230117 13:33:01 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:33:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 267.34
[32m[20230117 13:33:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.34
[32m[20230117 13:33:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 145.70
[32m[20230117 13:33:01 @agent_ppo2.py:151][0m Total time:       6.61 min
[32m[20230117 13:33:01 @agent_ppo2.py:153][0m 575488 total steps have happened
[32m[20230117 13:33:01 @agent_ppo2.py:129][0m #------------------------ Iteration 281 --------------------------#
[32m[20230117 13:33:02 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:02 @agent_ppo2.py:193][0m |           0.0011 |          19.4885 |           4.0670 |
[32m[20230117 13:33:02 @agent_ppo2.py:193][0m |          -0.0052 |          16.4374 |           4.0675 |
[32m[20230117 13:33:02 @agent_ppo2.py:193][0m |          -0.0066 |          15.0408 |           4.0719 |
[32m[20230117 13:33:02 @agent_ppo2.py:193][0m |          -0.0071 |          13.9813 |           4.0692 |
[32m[20230117 13:33:02 @agent_ppo2.py:193][0m |          -0.0087 |          13.0690 |           4.0687 |
[32m[20230117 13:33:02 @agent_ppo2.py:193][0m |          -0.0089 |          12.3648 |           4.0741 |
[32m[20230117 13:33:02 @agent_ppo2.py:193][0m |          -0.0100 |          11.8785 |           4.0737 |
[32m[20230117 13:33:02 @agent_ppo2.py:193][0m |          -0.0100 |          11.5067 |           4.0744 |
[32m[20230117 13:33:02 @agent_ppo2.py:193][0m |          -0.0113 |          10.8251 |           4.0794 |
[32m[20230117 13:33:02 @agent_ppo2.py:193][0m |          -0.0119 |          10.4159 |           4.0797 |
[32m[20230117 13:33:02 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:33:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 267.05
[32m[20230117 13:33:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.69
[32m[20230117 13:33:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 126.45
[32m[20230117 13:33:03 @agent_ppo2.py:151][0m Total time:       6.63 min
[32m[20230117 13:33:03 @agent_ppo2.py:153][0m 577536 total steps have happened
[32m[20230117 13:33:03 @agent_ppo2.py:129][0m #------------------------ Iteration 282 --------------------------#
[32m[20230117 13:33:03 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:03 @agent_ppo2.py:193][0m |          -0.0004 |          39.5327 |           4.1796 |
[32m[20230117 13:33:03 @agent_ppo2.py:193][0m |          -0.0061 |          28.4013 |           4.1639 |
[32m[20230117 13:33:03 @agent_ppo2.py:193][0m |          -0.0077 |          25.7662 |           4.1637 |
[32m[20230117 13:33:03 @agent_ppo2.py:193][0m |          -0.0103 |          24.2415 |           4.1623 |
[32m[20230117 13:33:03 @agent_ppo2.py:193][0m |          -0.0104 |          23.3594 |           4.1605 |
[32m[20230117 13:33:03 @agent_ppo2.py:193][0m |          -0.0115 |          22.5991 |           4.1607 |
[32m[20230117 13:33:03 @agent_ppo2.py:193][0m |          -0.0122 |          22.1918 |           4.1593 |
[32m[20230117 13:33:03 @agent_ppo2.py:193][0m |          -0.0132 |          21.5755 |           4.1555 |
[32m[20230117 13:33:03 @agent_ppo2.py:193][0m |          -0.0136 |          21.0867 |           4.1609 |
[32m[20230117 13:33:04 @agent_ppo2.py:193][0m |          -0.0150 |          20.8220 |           4.1596 |
[32m[20230117 13:33:04 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:33:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 161.28
[32m[20230117 13:33:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.62
[32m[20230117 13:33:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -28.39
[32m[20230117 13:33:04 @agent_ppo2.py:151][0m Total time:       6.64 min
[32m[20230117 13:33:04 @agent_ppo2.py:153][0m 579584 total steps have happened
[32m[20230117 13:33:04 @agent_ppo2.py:129][0m #------------------------ Iteration 283 --------------------------#
[32m[20230117 13:33:04 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:04 @agent_ppo2.py:193][0m |           0.0007 |          26.4013 |           4.0586 |
[32m[20230117 13:33:04 @agent_ppo2.py:193][0m |          -0.0025 |          15.2346 |           4.0595 |
[32m[20230117 13:33:04 @agent_ppo2.py:193][0m |          -0.0069 |          13.6558 |           4.0534 |
[32m[20230117 13:33:04 @agent_ppo2.py:193][0m |          -0.0086 |          12.8022 |           4.0526 |
[32m[20230117 13:33:04 @agent_ppo2.py:193][0m |          -0.0074 |          12.2775 |           4.0514 |
[32m[20230117 13:33:04 @agent_ppo2.py:193][0m |          -0.0077 |          11.8618 |           4.0523 |
[32m[20230117 13:33:04 @agent_ppo2.py:193][0m |          -0.0099 |          11.4763 |           4.0521 |
[32m[20230117 13:33:04 @agent_ppo2.py:193][0m |          -0.0092 |          11.3745 |           4.0528 |
[32m[20230117 13:33:05 @agent_ppo2.py:193][0m |          -0.0091 |          10.9185 |           4.0531 |
[32m[20230117 13:33:05 @agent_ppo2.py:193][0m |          -0.0065 |          10.6703 |           4.0504 |
[32m[20230117 13:33:05 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:33:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 215.86
[32m[20230117 13:33:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 270.13
[32m[20230117 13:33:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.75
[32m[20230117 13:33:05 @agent_ppo2.py:151][0m Total time:       6.66 min
[32m[20230117 13:33:05 @agent_ppo2.py:153][0m 581632 total steps have happened
[32m[20230117 13:33:05 @agent_ppo2.py:129][0m #------------------------ Iteration 284 --------------------------#
[32m[20230117 13:33:05 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:05 @agent_ppo2.py:193][0m |           0.0022 |          19.1534 |           4.1655 |
[32m[20230117 13:33:05 @agent_ppo2.py:193][0m |          -0.0052 |          15.5986 |           4.1627 |
[32m[20230117 13:33:05 @agent_ppo2.py:193][0m |          -0.0057 |          14.3409 |           4.1612 |
[32m[20230117 13:33:05 @agent_ppo2.py:193][0m |          -0.0086 |          13.2465 |           4.1599 |
[32m[20230117 13:33:05 @agent_ppo2.py:193][0m |          -0.0107 |          12.4862 |           4.1627 |
[32m[20230117 13:33:05 @agent_ppo2.py:193][0m |          -0.0110 |          11.6261 |           4.1645 |
[32m[20230117 13:33:06 @agent_ppo2.py:193][0m |          -0.0111 |          11.0684 |           4.1630 |
[32m[20230117 13:33:06 @agent_ppo2.py:193][0m |          -0.0134 |          10.4249 |           4.1663 |
[32m[20230117 13:33:06 @agent_ppo2.py:193][0m |          -0.0111 |          10.0440 |           4.1669 |
[32m[20230117 13:33:06 @agent_ppo2.py:193][0m |          -0.0129 |           9.7003 |           4.1675 |
[32m[20230117 13:33:06 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:33:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.57
[32m[20230117 13:33:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.71
[32m[20230117 13:33:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.19
[32m[20230117 13:33:06 @agent_ppo2.py:151][0m Total time:       6.68 min
[32m[20230117 13:33:06 @agent_ppo2.py:153][0m 583680 total steps have happened
[32m[20230117 13:33:06 @agent_ppo2.py:129][0m #------------------------ Iteration 285 --------------------------#
[32m[20230117 13:33:06 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:33:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:06 @agent_ppo2.py:193][0m |           0.0018 |          28.8968 |           4.1549 |
[32m[20230117 13:33:06 @agent_ppo2.py:193][0m |          -0.0031 |          22.5954 |           4.1531 |
[32m[20230117 13:33:06 @agent_ppo2.py:193][0m |          -0.0033 |          20.2159 |           4.1508 |
[32m[20230117 13:33:06 @agent_ppo2.py:193][0m |          -0.0066 |          18.6241 |           4.1494 |
[32m[20230117 13:33:07 @agent_ppo2.py:193][0m |          -0.0067 |          18.2117 |           4.1457 |
[32m[20230117 13:33:07 @agent_ppo2.py:193][0m |          -0.0076 |          17.4468 |           4.1464 |
[32m[20230117 13:33:07 @agent_ppo2.py:193][0m |          -0.0104 |          16.5502 |           4.1487 |
[32m[20230117 13:33:07 @agent_ppo2.py:193][0m |          -0.0105 |          15.7676 |           4.1482 |
[32m[20230117 13:33:07 @agent_ppo2.py:193][0m |          -0.0121 |          15.6107 |           4.1498 |
[32m[20230117 13:33:07 @agent_ppo2.py:193][0m |          -0.0125 |          14.8793 |           4.1497 |
[32m[20230117 13:33:07 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:33:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 192.21
[32m[20230117 13:33:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 270.90
[32m[20230117 13:33:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 123.10
[32m[20230117 13:33:07 @agent_ppo2.py:151][0m Total time:       6.70 min
[32m[20230117 13:33:07 @agent_ppo2.py:153][0m 585728 total steps have happened
[32m[20230117 13:33:07 @agent_ppo2.py:129][0m #------------------------ Iteration 286 --------------------------#
[32m[20230117 13:33:07 @agent_ppo2.py:135][0m Sampling time: 0.29 s by 4 slaves
[32m[20230117 13:33:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:07 @agent_ppo2.py:193][0m |          -0.0003 |          40.9057 |           4.0754 |
[32m[20230117 13:33:08 @agent_ppo2.py:193][0m |          -0.0015 |          30.9527 |           4.0693 |
[32m[20230117 13:33:08 @agent_ppo2.py:193][0m |          -0.0052 |          27.1871 |           4.0625 |
[32m[20230117 13:33:08 @agent_ppo2.py:193][0m |          -0.0081 |          25.5376 |           4.0596 |
[32m[20230117 13:33:08 @agent_ppo2.py:193][0m |          -0.0093 |          24.5396 |           4.0584 |
[32m[20230117 13:33:08 @agent_ppo2.py:193][0m |          -0.0111 |          23.6268 |           4.0544 |
[32m[20230117 13:33:08 @agent_ppo2.py:193][0m |          -0.0117 |          22.9445 |           4.0530 |
[32m[20230117 13:33:08 @agent_ppo2.py:193][0m |          -0.0114 |          22.1881 |           4.0541 |
[32m[20230117 13:33:08 @agent_ppo2.py:193][0m |          -0.0116 |          21.9168 |           4.0527 |
[32m[20230117 13:33:08 @agent_ppo2.py:193][0m |          -0.0136 |          20.9774 |           4.0473 |
[32m[20230117 13:33:08 @agent_ppo2.py:138][0m Policy update time: 0.81 s
[32m[20230117 13:33:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 213.42
[32m[20230117 13:33:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.48
[32m[20230117 13:33:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.39
[32m[20230117 13:33:08 @agent_ppo2.py:151][0m Total time:       6.72 min
[32m[20230117 13:33:08 @agent_ppo2.py:153][0m 587776 total steps have happened
[32m[20230117 13:33:08 @agent_ppo2.py:129][0m #------------------------ Iteration 287 --------------------------#
[32m[20230117 13:33:09 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:33:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:09 @agent_ppo2.py:193][0m |           0.0012 |          22.3499 |           4.1298 |
[32m[20230117 13:33:09 @agent_ppo2.py:193][0m |          -0.0032 |          16.8081 |           4.1268 |
[32m[20230117 13:33:09 @agent_ppo2.py:193][0m |          -0.0055 |          14.1884 |           4.1236 |
[32m[20230117 13:33:09 @agent_ppo2.py:193][0m |          -0.0074 |          12.6151 |           4.1208 |
[32m[20230117 13:33:09 @agent_ppo2.py:193][0m |          -0.0065 |          11.5672 |           4.1212 |
[32m[20230117 13:33:09 @agent_ppo2.py:193][0m |          -0.0100 |          10.5646 |           4.1198 |
[32m[20230117 13:33:09 @agent_ppo2.py:193][0m |          -0.0114 |           9.9366 |           4.1187 |
[32m[20230117 13:33:09 @agent_ppo2.py:193][0m |          -0.0120 |           9.4170 |           4.1195 |
[32m[20230117 13:33:09 @agent_ppo2.py:193][0m |          -0.0099 |           9.0124 |           4.1150 |
[32m[20230117 13:33:09 @agent_ppo2.py:193][0m |          -0.0096 |           8.5623 |           4.1155 |
[32m[20230117 13:33:09 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:33:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.68
[32m[20230117 13:33:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 270.32
[32m[20230117 13:33:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -43.42
[32m[20230117 13:33:09 @agent_ppo2.py:151][0m Total time:       6.74 min
[32m[20230117 13:33:09 @agent_ppo2.py:153][0m 589824 total steps have happened
[32m[20230117 13:33:09 @agent_ppo2.py:129][0m #------------------------ Iteration 288 --------------------------#
[32m[20230117 13:33:10 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:33:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:10 @agent_ppo2.py:193][0m |           0.0004 |          24.0520 |           4.1000 |
[32m[20230117 13:33:10 @agent_ppo2.py:193][0m |          -0.0114 |          20.0667 |           4.0962 |
[32m[20230117 13:33:10 @agent_ppo2.py:193][0m |          -0.0022 |          19.1350 |           4.0933 |
[32m[20230117 13:33:10 @agent_ppo2.py:193][0m |           0.0022 |          18.6615 |           4.0908 |
[32m[20230117 13:33:10 @agent_ppo2.py:193][0m |          -0.0073 |          17.9746 |           4.0857 |
[32m[20230117 13:33:10 @agent_ppo2.py:193][0m |          -0.0066 |          17.5033 |           4.0903 |
[32m[20230117 13:33:10 @agent_ppo2.py:193][0m |          -0.0194 |          17.2525 |           4.0855 |
[32m[20230117 13:33:10 @agent_ppo2.py:193][0m |          -0.0142 |          16.8952 |           4.0846 |
[32m[20230117 13:33:10 @agent_ppo2.py:193][0m |          -0.0138 |          16.5062 |           4.0859 |
[32m[20230117 13:33:10 @agent_ppo2.py:193][0m |          -0.0109 |          16.3518 |           4.0872 |
[32m[20230117 13:33:10 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:33:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 267.33
[32m[20230117 13:33:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.90
[32m[20230117 13:33:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 56.52
[32m[20230117 13:33:11 @agent_ppo2.py:151][0m Total time:       6.76 min
[32m[20230117 13:33:11 @agent_ppo2.py:153][0m 591872 total steps have happened
[32m[20230117 13:33:11 @agent_ppo2.py:129][0m #------------------------ Iteration 289 --------------------------#
[32m[20230117 13:33:11 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:33:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:11 @agent_ppo2.py:193][0m |           0.0050 |          36.7369 |           4.0742 |
[32m[20230117 13:33:11 @agent_ppo2.py:193][0m |          -0.0016 |          19.8594 |           4.0749 |
[32m[20230117 13:33:11 @agent_ppo2.py:193][0m |          -0.0001 |          17.9500 |           4.0688 |
[32m[20230117 13:33:11 @agent_ppo2.py:193][0m |          -0.0096 |          16.3579 |           4.0693 |
[32m[20230117 13:33:11 @agent_ppo2.py:193][0m |          -0.0061 |          15.4142 |           4.0696 |
[32m[20230117 13:33:11 @agent_ppo2.py:193][0m |          -0.0076 |          14.7130 |           4.0690 |
[32m[20230117 13:33:11 @agent_ppo2.py:193][0m |          -0.0128 |          14.1149 |           4.0675 |
[32m[20230117 13:33:11 @agent_ppo2.py:193][0m |          -0.0131 |          13.5875 |           4.0678 |
[32m[20230117 13:33:12 @agent_ppo2.py:193][0m |          -0.0132 |          13.2277 |           4.0685 |
[32m[20230117 13:33:12 @agent_ppo2.py:193][0m |          -0.0085 |          12.6772 |           4.0679 |
[32m[20230117 13:33:12 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:33:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 136.28
[32m[20230117 13:33:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.11
[32m[20230117 13:33:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: -0.37
[32m[20230117 13:33:12 @agent_ppo2.py:151][0m Total time:       6.78 min
[32m[20230117 13:33:12 @agent_ppo2.py:153][0m 593920 total steps have happened
[32m[20230117 13:33:12 @agent_ppo2.py:129][0m #------------------------ Iteration 290 --------------------------#
[32m[20230117 13:33:12 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:12 @agent_ppo2.py:193][0m |           0.0015 |          21.1967 |           4.1307 |
[32m[20230117 13:33:12 @agent_ppo2.py:193][0m |          -0.0043 |          18.7310 |           4.1236 |
[32m[20230117 13:33:12 @agent_ppo2.py:193][0m |          -0.0060 |          17.9584 |           4.1246 |
[32m[20230117 13:33:12 @agent_ppo2.py:193][0m |          -0.0068 |          17.4425 |           4.1231 |
[32m[20230117 13:33:12 @agent_ppo2.py:193][0m |          -0.0089 |          16.9130 |           4.1216 |
[32m[20230117 13:33:12 @agent_ppo2.py:193][0m |          -0.0075 |          16.6212 |           4.1210 |
[32m[20230117 13:33:13 @agent_ppo2.py:193][0m |          -0.0095 |          16.1679 |           4.1193 |
[32m[20230117 13:33:13 @agent_ppo2.py:193][0m |          -0.0078 |          16.1282 |           4.1182 |
[32m[20230117 13:33:13 @agent_ppo2.py:193][0m |          -0.0109 |          15.5531 |           4.1193 |
[32m[20230117 13:33:13 @agent_ppo2.py:193][0m |          -0.0113 |          15.3517 |           4.1168 |
[32m[20230117 13:33:13 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:33:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.14
[32m[20230117 13:33:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.90
[32m[20230117 13:33:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 136.99
[32m[20230117 13:33:13 @agent_ppo2.py:151][0m Total time:       6.80 min
[32m[20230117 13:33:13 @agent_ppo2.py:153][0m 595968 total steps have happened
[32m[20230117 13:33:13 @agent_ppo2.py:129][0m #------------------------ Iteration 291 --------------------------#
[32m[20230117 13:33:13 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:33:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:13 @agent_ppo2.py:193][0m |           0.0024 |          43.0755 |           4.0138 |
[32m[20230117 13:33:13 @agent_ppo2.py:193][0m |          -0.0035 |          30.2278 |           4.0048 |
[32m[20230117 13:33:13 @agent_ppo2.py:193][0m |          -0.0073 |          26.8124 |           4.0064 |
[32m[20230117 13:33:14 @agent_ppo2.py:193][0m |          -0.0075 |          25.3887 |           4.0031 |
[32m[20230117 13:33:14 @agent_ppo2.py:193][0m |          -0.0095 |          23.9038 |           4.0047 |
[32m[20230117 13:33:14 @agent_ppo2.py:193][0m |          -0.0105 |          23.2635 |           4.0058 |
[32m[20230117 13:33:14 @agent_ppo2.py:193][0m |          -0.0089 |          23.2427 |           4.0079 |
[32m[20230117 13:33:14 @agent_ppo2.py:193][0m |          -0.0094 |          22.3383 |           4.0059 |
[32m[20230117 13:33:14 @agent_ppo2.py:193][0m |          -0.0097 |          21.7374 |           4.0073 |
[32m[20230117 13:33:14 @agent_ppo2.py:193][0m |          -0.0092 |          21.9441 |           4.0047 |
[32m[20230117 13:33:14 @agent_ppo2.py:138][0m Policy update time: 0.82 s
[32m[20230117 13:33:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 199.69
[32m[20230117 13:33:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.83
[32m[20230117 13:33:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 60.03
[32m[20230117 13:33:14 @agent_ppo2.py:151][0m Total time:       6.82 min
[32m[20230117 13:33:14 @agent_ppo2.py:153][0m 598016 total steps have happened
[32m[20230117 13:33:14 @agent_ppo2.py:129][0m #------------------------ Iteration 292 --------------------------#
[32m[20230117 13:33:14 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:33:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:15 @agent_ppo2.py:193][0m |          -0.0004 |          17.7560 |           4.1226 |
[32m[20230117 13:33:15 @agent_ppo2.py:193][0m |          -0.0017 |          12.7381 |           4.1202 |
[32m[20230117 13:33:15 @agent_ppo2.py:193][0m |          -0.0050 |          10.3198 |           4.1181 |
[32m[20230117 13:33:15 @agent_ppo2.py:193][0m |          -0.0115 |           9.0263 |           4.1214 |
[32m[20230117 13:33:15 @agent_ppo2.py:193][0m |          -0.0082 |           8.4991 |           4.1195 |
[32m[20230117 13:33:15 @agent_ppo2.py:193][0m |          -0.0148 |           7.9905 |           4.1194 |
[32m[20230117 13:33:15 @agent_ppo2.py:193][0m |          -0.0115 |           7.5865 |           4.1179 |
[32m[20230117 13:33:15 @agent_ppo2.py:193][0m |          -0.0150 |           7.1575 |           4.1164 |
[32m[20230117 13:33:15 @agent_ppo2.py:193][0m |          -0.0137 |           6.8462 |           4.1165 |
[32m[20230117 13:33:15 @agent_ppo2.py:193][0m |          -0.0083 |           6.6128 |           4.1174 |
[32m[20230117 13:33:15 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:33:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.64
[32m[20230117 13:33:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.33
[32m[20230117 13:33:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 65.64
[32m[20230117 13:33:15 @agent_ppo2.py:151][0m Total time:       6.84 min
[32m[20230117 13:33:15 @agent_ppo2.py:153][0m 600064 total steps have happened
[32m[20230117 13:33:15 @agent_ppo2.py:129][0m #------------------------ Iteration 293 --------------------------#
[32m[20230117 13:33:16 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:33:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:16 @agent_ppo2.py:193][0m |           0.0004 |          24.9503 |           4.1870 |
[32m[20230117 13:33:16 @agent_ppo2.py:193][0m |          -0.0058 |          19.0353 |           4.1740 |
[32m[20230117 13:33:16 @agent_ppo2.py:193][0m |          -0.0072 |          17.6666 |           4.1701 |
[32m[20230117 13:33:16 @agent_ppo2.py:193][0m |          -0.0087 |          16.7444 |           4.1687 |
[32m[20230117 13:33:16 @agent_ppo2.py:193][0m |          -0.0104 |          15.9683 |           4.1681 |
[32m[20230117 13:33:16 @agent_ppo2.py:193][0m |          -0.0121 |          15.5721 |           4.1677 |
[32m[20230117 13:33:16 @agent_ppo2.py:193][0m |          -0.0123 |          15.1293 |           4.1652 |
[32m[20230117 13:33:16 @agent_ppo2.py:193][0m |          -0.0126 |          14.8053 |           4.1669 |
[32m[20230117 13:33:16 @agent_ppo2.py:193][0m |          -0.0144 |          14.5556 |           4.1652 |
[32m[20230117 13:33:16 @agent_ppo2.py:193][0m |          -0.0140 |          14.2604 |           4.1672 |
[32m[20230117 13:33:16 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:33:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 195.91
[32m[20230117 13:33:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.60
[32m[20230117 13:33:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.68
[32m[20230117 13:33:16 @agent_ppo2.py:151][0m Total time:       6.86 min
[32m[20230117 13:33:16 @agent_ppo2.py:153][0m 602112 total steps have happened
[32m[20230117 13:33:16 @agent_ppo2.py:129][0m #------------------------ Iteration 294 --------------------------#
[32m[20230117 13:33:17 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:33:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:17 @agent_ppo2.py:193][0m |          -0.0003 |          21.3829 |           4.2199 |
[32m[20230117 13:33:17 @agent_ppo2.py:193][0m |          -0.0053 |          19.1011 |           4.2078 |
[32m[20230117 13:33:17 @agent_ppo2.py:193][0m |          -0.0068 |          18.4830 |           4.2059 |
[32m[20230117 13:33:17 @agent_ppo2.py:193][0m |          -0.0070 |          18.0782 |           4.2084 |
[32m[20230117 13:33:17 @agent_ppo2.py:193][0m |          -0.0089 |          17.8268 |           4.2089 |
[32m[20230117 13:33:17 @agent_ppo2.py:193][0m |          -0.0095 |          17.6028 |           4.2062 |
[32m[20230117 13:33:17 @agent_ppo2.py:193][0m |          -0.0101 |          17.4350 |           4.2061 |
[32m[20230117 13:33:17 @agent_ppo2.py:193][0m |          -0.0104 |          17.2576 |           4.2077 |
[32m[20230117 13:33:17 @agent_ppo2.py:193][0m |          -0.0109 |          17.1003 |           4.2055 |
[32m[20230117 13:33:17 @agent_ppo2.py:193][0m |          -0.0118 |          16.9474 |           4.2083 |
[32m[20230117 13:33:17 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:33:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.50
[32m[20230117 13:33:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.07
[32m[20230117 13:33:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 16.73
[32m[20230117 13:33:18 @agent_ppo2.py:151][0m Total time:       6.88 min
[32m[20230117 13:33:18 @agent_ppo2.py:153][0m 604160 total steps have happened
[32m[20230117 13:33:18 @agent_ppo2.py:129][0m #------------------------ Iteration 295 --------------------------#
[32m[20230117 13:33:18 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:33:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:18 @agent_ppo2.py:193][0m |           0.0050 |          19.6592 |           4.1750 |
[32m[20230117 13:33:18 @agent_ppo2.py:193][0m |          -0.0074 |          16.5630 |           4.1687 |
[32m[20230117 13:33:18 @agent_ppo2.py:193][0m |          -0.0061 |          15.7910 |           4.1647 |
[32m[20230117 13:33:18 @agent_ppo2.py:193][0m |          -0.0115 |          15.0643 |           4.1680 |
[32m[20230117 13:33:18 @agent_ppo2.py:193][0m |          -0.0125 |          14.5751 |           4.1662 |
[32m[20230117 13:33:18 @agent_ppo2.py:193][0m |          -0.0029 |          15.4195 |           4.1689 |
[32m[20230117 13:33:18 @agent_ppo2.py:193][0m |          -0.0135 |          13.7984 |           4.1676 |
[32m[20230117 13:33:18 @agent_ppo2.py:193][0m |          -0.0151 |          13.4176 |           4.1677 |
[32m[20230117 13:33:19 @agent_ppo2.py:193][0m |          -0.0097 |          13.8630 |           4.1656 |
[32m[20230117 13:33:19 @agent_ppo2.py:193][0m |          -0.0124 |          13.0215 |           4.1673 |
[32m[20230117 13:33:19 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:33:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.90
[32m[20230117 13:33:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 270.51
[32m[20230117 13:33:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 137.44
[32m[20230117 13:33:19 @agent_ppo2.py:151][0m Total time:       6.90 min
[32m[20230117 13:33:19 @agent_ppo2.py:153][0m 606208 total steps have happened
[32m[20230117 13:33:19 @agent_ppo2.py:129][0m #------------------------ Iteration 296 --------------------------#
[32m[20230117 13:33:19 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:19 @agent_ppo2.py:193][0m |          -0.0011 |          18.2938 |           4.2525 |
[32m[20230117 13:33:19 @agent_ppo2.py:193][0m |          -0.0057 |          16.6676 |           4.2446 |
[32m[20230117 13:33:19 @agent_ppo2.py:193][0m |          -0.0066 |          15.8768 |           4.2410 |
[32m[20230117 13:33:19 @agent_ppo2.py:193][0m |          -0.0083 |          15.5080 |           4.2428 |
[32m[20230117 13:33:20 @agent_ppo2.py:193][0m |          -0.0090 |          15.1790 |           4.2419 |
[32m[20230117 13:33:20 @agent_ppo2.py:193][0m |          -0.0094 |          14.8609 |           4.2448 |
[32m[20230117 13:33:20 @agent_ppo2.py:193][0m |          -0.0106 |          14.6960 |           4.2444 |
[32m[20230117 13:33:20 @agent_ppo2.py:193][0m |          -0.0113 |          14.4672 |           4.2432 |
[32m[20230117 13:33:20 @agent_ppo2.py:193][0m |          -0.0117 |          14.2579 |           4.2401 |
[32m[20230117 13:33:20 @agent_ppo2.py:193][0m |          -0.0120 |          14.1016 |           4.2415 |
[32m[20230117 13:33:20 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:33:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.72
[32m[20230117 13:33:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.10
[32m[20230117 13:33:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 106.13
[32m[20230117 13:33:20 @agent_ppo2.py:151][0m Total time:       6.92 min
[32m[20230117 13:33:20 @agent_ppo2.py:153][0m 608256 total steps have happened
[32m[20230117 13:33:20 @agent_ppo2.py:129][0m #------------------------ Iteration 297 --------------------------#
[32m[20230117 13:33:20 @agent_ppo2.py:135][0m Sampling time: 0.30 s by 4 slaves
[32m[20230117 13:33:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:20 @agent_ppo2.py:193][0m |          -0.0007 |          21.9201 |           4.2264 |
[32m[20230117 13:33:21 @agent_ppo2.py:193][0m |          -0.0061 |          17.8222 |           4.2242 |
[32m[20230117 13:33:21 @agent_ppo2.py:193][0m |          -0.0074 |          16.9930 |           4.2226 |
[32m[20230117 13:33:21 @agent_ppo2.py:193][0m |          -0.0082 |          16.6658 |           4.2239 |
[32m[20230117 13:33:21 @agent_ppo2.py:193][0m |          -0.0097 |          16.1891 |           4.2212 |
[32m[20230117 13:33:21 @agent_ppo2.py:193][0m |          -0.0114 |          15.8781 |           4.2225 |
[32m[20230117 13:33:21 @agent_ppo2.py:193][0m |          -0.0122 |          15.5961 |           4.2222 |
[32m[20230117 13:33:21 @agent_ppo2.py:193][0m |          -0.0129 |          15.3024 |           4.2204 |
[32m[20230117 13:33:21 @agent_ppo2.py:193][0m |          -0.0118 |          15.0474 |           4.2209 |
[32m[20230117 13:33:21 @agent_ppo2.py:193][0m |          -0.0139 |          14.7630 |           4.2213 |
[32m[20230117 13:33:21 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:33:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 210.27
[32m[20230117 13:33:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.66
[32m[20230117 13:33:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.71
[32m[20230117 13:33:21 @agent_ppo2.py:151][0m Total time:       6.94 min
[32m[20230117 13:33:21 @agent_ppo2.py:153][0m 610304 total steps have happened
[32m[20230117 13:33:21 @agent_ppo2.py:129][0m #------------------------ Iteration 298 --------------------------#
[32m[20230117 13:33:22 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:33:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:22 @agent_ppo2.py:193][0m |          -0.0012 |          37.5350 |           4.1667 |
[32m[20230117 13:33:22 @agent_ppo2.py:193][0m |          -0.0063 |          24.5716 |           4.1574 |
[32m[20230117 13:33:22 @agent_ppo2.py:193][0m |          -0.0089 |          21.5503 |           4.1588 |
[32m[20230117 13:33:22 @agent_ppo2.py:193][0m |          -0.0108 |          19.6583 |           4.1567 |
[32m[20230117 13:33:22 @agent_ppo2.py:193][0m |          -0.0077 |          18.3723 |           4.1480 |
[32m[20230117 13:33:22 @agent_ppo2.py:193][0m |          -0.0119 |          16.8346 |           4.1519 |
[32m[20230117 13:33:22 @agent_ppo2.py:193][0m |          -0.0153 |          15.9568 |           4.1517 |
[32m[20230117 13:33:22 @agent_ppo2.py:193][0m |          -0.0143 |          15.4567 |           4.1477 |
[32m[20230117 13:33:22 @agent_ppo2.py:193][0m |          -0.0148 |          14.7732 |           4.1488 |
[32m[20230117 13:33:22 @agent_ppo2.py:193][0m |          -0.0130 |          14.1287 |           4.1429 |
[32m[20230117 13:33:22 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:33:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 153.23
[32m[20230117 13:33:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.40
[32m[20230117 13:33:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.05
[32m[20230117 13:33:22 @agent_ppo2.py:151][0m Total time:       6.96 min
[32m[20230117 13:33:22 @agent_ppo2.py:153][0m 612352 total steps have happened
[32m[20230117 13:33:22 @agent_ppo2.py:129][0m #------------------------ Iteration 299 --------------------------#
[32m[20230117 13:33:23 @agent_ppo2.py:135][0m Sampling time: 0.29 s by 4 slaves
[32m[20230117 13:33:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:23 @agent_ppo2.py:193][0m |          -0.0006 |          44.2198 |           4.1902 |
[32m[20230117 13:33:23 @agent_ppo2.py:193][0m |          -0.0054 |          29.7866 |           4.1882 |
[32m[20230117 13:33:23 @agent_ppo2.py:193][0m |          -0.0084 |          27.1735 |           4.1896 |
[32m[20230117 13:33:23 @agent_ppo2.py:193][0m |          -0.0108 |          25.2996 |           4.1892 |
[32m[20230117 13:33:23 @agent_ppo2.py:193][0m |          -0.0114 |          24.3578 |           4.1895 |
[32m[20230117 13:33:23 @agent_ppo2.py:193][0m |          -0.0134 |          23.4173 |           4.1903 |
[32m[20230117 13:33:23 @agent_ppo2.py:193][0m |          -0.0133 |          23.0197 |           4.1932 |
[32m[20230117 13:33:23 @agent_ppo2.py:193][0m |          -0.0143 |          22.7793 |           4.1944 |
[32m[20230117 13:33:24 @agent_ppo2.py:193][0m |          -0.0145 |          22.4041 |           4.1927 |
[32m[20230117 13:33:24 @agent_ppo2.py:193][0m |          -0.0156 |          21.8704 |           4.1947 |
[32m[20230117 13:33:24 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:33:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 154.98
[32m[20230117 13:33:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.45
[32m[20230117 13:33:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 127.62
[32m[20230117 13:33:24 @agent_ppo2.py:151][0m Total time:       6.98 min
[32m[20230117 13:33:24 @agent_ppo2.py:153][0m 614400 total steps have happened
[32m[20230117 13:33:24 @agent_ppo2.py:129][0m #------------------------ Iteration 300 --------------------------#
[32m[20230117 13:33:24 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:33:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:24 @agent_ppo2.py:193][0m |           0.0004 |          18.3078 |           4.2245 |
[32m[20230117 13:33:24 @agent_ppo2.py:193][0m |          -0.0042 |          16.4422 |           4.2224 |
[32m[20230117 13:33:24 @agent_ppo2.py:193][0m |          -0.0029 |          15.6928 |           4.2124 |
[32m[20230117 13:33:24 @agent_ppo2.py:193][0m |          -0.0070 |          14.5538 |           4.2129 |
[32m[20230117 13:33:25 @agent_ppo2.py:193][0m |          -0.0075 |          14.1628 |           4.2095 |
[32m[20230117 13:33:25 @agent_ppo2.py:193][0m |          -0.0110 |          13.4889 |           4.2111 |
[32m[20230117 13:33:25 @agent_ppo2.py:193][0m |          -0.0107 |          13.1198 |           4.2120 |
[32m[20230117 13:33:25 @agent_ppo2.py:193][0m |          -0.0113 |          12.7291 |           4.2065 |
[32m[20230117 13:33:25 @agent_ppo2.py:193][0m |          -0.0132 |          12.3717 |           4.2080 |
[32m[20230117 13:33:25 @agent_ppo2.py:193][0m |          -0.0122 |          12.1338 |           4.2114 |
[32m[20230117 13:33:25 @agent_ppo2.py:138][0m Policy update time: 0.80 s
[32m[20230117 13:33:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 259.99
[32m[20230117 13:33:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.06
[32m[20230117 13:33:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 129.07
[32m[20230117 13:33:25 @agent_ppo2.py:151][0m Total time:       7.00 min
[32m[20230117 13:33:25 @agent_ppo2.py:153][0m 616448 total steps have happened
[32m[20230117 13:33:25 @agent_ppo2.py:129][0m #------------------------ Iteration 301 --------------------------#
[32m[20230117 13:33:25 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:33:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:25 @agent_ppo2.py:193][0m |          -0.0010 |          19.1072 |           4.2253 |
[32m[20230117 13:33:25 @agent_ppo2.py:193][0m |          -0.0055 |          16.0664 |           4.2187 |
[32m[20230117 13:33:26 @agent_ppo2.py:193][0m |          -0.0072 |          14.7260 |           4.2154 |
[32m[20230117 13:33:26 @agent_ppo2.py:193][0m |          -0.0077 |          13.8109 |           4.2141 |
[32m[20230117 13:33:26 @agent_ppo2.py:193][0m |          -0.0096 |          13.0748 |           4.2145 |
[32m[20230117 13:33:26 @agent_ppo2.py:193][0m |          -0.0101 |          12.5131 |           4.2144 |
[32m[20230117 13:33:26 @agent_ppo2.py:193][0m |          -0.0107 |          12.0214 |           4.2163 |
[32m[20230117 13:33:26 @agent_ppo2.py:193][0m |          -0.0116 |          11.6004 |           4.2157 |
[32m[20230117 13:33:26 @agent_ppo2.py:193][0m |          -0.0126 |          11.2563 |           4.2152 |
[32m[20230117 13:33:26 @agent_ppo2.py:193][0m |          -0.0122 |          10.9768 |           4.2160 |
[32m[20230117 13:33:26 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:33:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 267.02
[32m[20230117 13:33:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.08
[32m[20230117 13:33:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.73
[32m[20230117 13:33:26 @agent_ppo2.py:151][0m Total time:       7.02 min
[32m[20230117 13:33:26 @agent_ppo2.py:153][0m 618496 total steps have happened
[32m[20230117 13:33:26 @agent_ppo2.py:129][0m #------------------------ Iteration 302 --------------------------#
[32m[20230117 13:33:26 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:27 @agent_ppo2.py:193][0m |          -0.0023 |          22.1622 |           4.2457 |
[32m[20230117 13:33:27 @agent_ppo2.py:193][0m |          -0.0095 |          19.6424 |           4.2382 |
[32m[20230117 13:33:27 @agent_ppo2.py:193][0m |          -0.0121 |          18.9878 |           4.2363 |
[32m[20230117 13:33:27 @agent_ppo2.py:193][0m |          -0.0062 |          19.1241 |           4.2324 |
[32m[20230117 13:33:27 @agent_ppo2.py:193][0m |          -0.0098 |          17.9438 |           4.2344 |
[32m[20230117 13:33:27 @agent_ppo2.py:193][0m |          -0.0086 |          17.8598 |           4.2323 |
[32m[20230117 13:33:27 @agent_ppo2.py:193][0m |          -0.0105 |          17.1084 |           4.2297 |
[32m[20230117 13:33:27 @agent_ppo2.py:193][0m |          -0.0112 |          16.4923 |           4.2290 |
[32m[20230117 13:33:27 @agent_ppo2.py:193][0m |          -0.0167 |          16.1244 |           4.2267 |
[32m[20230117 13:33:27 @agent_ppo2.py:193][0m |          -0.0136 |          15.7763 |           4.2264 |
[32m[20230117 13:33:27 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:33:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 267.89
[32m[20230117 13:33:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 270.31
[32m[20230117 13:33:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.00
[32m[20230117 13:33:27 @agent_ppo2.py:151][0m Total time:       7.04 min
[32m[20230117 13:33:27 @agent_ppo2.py:153][0m 620544 total steps have happened
[32m[20230117 13:33:27 @agent_ppo2.py:129][0m #------------------------ Iteration 303 --------------------------#
[32m[20230117 13:33:28 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:33:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:28 @agent_ppo2.py:193][0m |          -0.0005 |          29.6653 |           4.1849 |
[32m[20230117 13:33:28 @agent_ppo2.py:193][0m |          -0.0066 |          22.7222 |           4.1844 |
[32m[20230117 13:33:28 @agent_ppo2.py:193][0m |          -0.0090 |          20.6335 |           4.1837 |
[32m[20230117 13:33:28 @agent_ppo2.py:193][0m |          -0.0109 |          19.5389 |           4.1810 |
[32m[20230117 13:33:28 @agent_ppo2.py:193][0m |          -0.0121 |          18.8750 |           4.1810 |
[32m[20230117 13:33:28 @agent_ppo2.py:193][0m |          -0.0130 |          18.2585 |           4.1821 |
[32m[20230117 13:33:28 @agent_ppo2.py:193][0m |          -0.0135 |          17.8090 |           4.1815 |
[32m[20230117 13:33:28 @agent_ppo2.py:193][0m |          -0.0132 |          17.5614 |           4.1812 |
[32m[20230117 13:33:28 @agent_ppo2.py:193][0m |          -0.0136 |          17.2333 |           4.1779 |
[32m[20230117 13:33:28 @agent_ppo2.py:193][0m |          -0.0159 |          16.8707 |           4.1821 |
[32m[20230117 13:33:28 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:33:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 226.39
[32m[20230117 13:33:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.59
[32m[20230117 13:33:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 281.20
[32m[20230117 13:33:29 @agent_ppo2.py:151][0m Total time:       7.06 min
[32m[20230117 13:33:29 @agent_ppo2.py:153][0m 622592 total steps have happened
[32m[20230117 13:33:29 @agent_ppo2.py:129][0m #------------------------ Iteration 304 --------------------------#
[32m[20230117 13:33:29 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:29 @agent_ppo2.py:193][0m |           0.0020 |          19.2050 |           4.2201 |
[32m[20230117 13:33:29 @agent_ppo2.py:193][0m |          -0.0027 |          17.1713 |           4.2129 |
[32m[20230117 13:33:29 @agent_ppo2.py:193][0m |          -0.0028 |          16.5671 |           4.2071 |
[32m[20230117 13:33:29 @agent_ppo2.py:193][0m |          -0.0049 |          16.0632 |           4.2061 |
[32m[20230117 13:33:29 @agent_ppo2.py:193][0m |          -0.0058 |          15.8128 |           4.2012 |
[32m[20230117 13:33:29 @agent_ppo2.py:193][0m |          -0.0054 |          15.7008 |           4.1987 |
[32m[20230117 13:33:29 @agent_ppo2.py:193][0m |          -0.0048 |          15.4738 |           4.1991 |
[32m[20230117 13:33:29 @agent_ppo2.py:193][0m |          -0.0071 |          15.2831 |           4.1935 |
[32m[20230117 13:33:29 @agent_ppo2.py:193][0m |          -0.0085 |          14.9623 |           4.1952 |
[32m[20230117 13:33:29 @agent_ppo2.py:193][0m |          -0.0066 |          15.0392 |           4.1903 |
[32m[20230117 13:33:29 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:33:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.48
[32m[20230117 13:33:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.29
[32m[20230117 13:33:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 45.29
[32m[20230117 13:33:30 @agent_ppo2.py:151][0m Total time:       7.08 min
[32m[20230117 13:33:30 @agent_ppo2.py:153][0m 624640 total steps have happened
[32m[20230117 13:33:30 @agent_ppo2.py:129][0m #------------------------ Iteration 305 --------------------------#
[32m[20230117 13:33:30 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:33:30 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:30 @agent_ppo2.py:193][0m |           0.0016 |          22.2832 |           4.1538 |
[32m[20230117 13:33:30 @agent_ppo2.py:193][0m |          -0.0036 |          19.7791 |           4.1497 |
[32m[20230117 13:33:30 @agent_ppo2.py:193][0m |          -0.0050 |          19.0529 |           4.1509 |
[32m[20230117 13:33:30 @agent_ppo2.py:193][0m |          -0.0068 |          18.4088 |           4.1552 |
[32m[20230117 13:33:30 @agent_ppo2.py:193][0m |          -0.0070 |          18.0689 |           4.1561 |
[32m[20230117 13:33:30 @agent_ppo2.py:193][0m |          -0.0087 |          17.6986 |           4.1628 |
[32m[20230117 13:33:30 @agent_ppo2.py:193][0m |          -0.0093 |          17.5192 |           4.1645 |
[32m[20230117 13:33:30 @agent_ppo2.py:193][0m |          -0.0101 |          17.3101 |           4.1694 |
[32m[20230117 13:33:31 @agent_ppo2.py:193][0m |          -0.0106 |          17.1468 |           4.1671 |
[32m[20230117 13:33:31 @agent_ppo2.py:193][0m |          -0.0113 |          16.9966 |           4.1704 |
[32m[20230117 13:33:31 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:33:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.59
[32m[20230117 13:33:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 270.80
[32m[20230117 13:33:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.19
[32m[20230117 13:33:31 @agent_ppo2.py:151][0m Total time:       7.10 min
[32m[20230117 13:33:31 @agent_ppo2.py:153][0m 626688 total steps have happened
[32m[20230117 13:33:31 @agent_ppo2.py:129][0m #------------------------ Iteration 306 --------------------------#
[32m[20230117 13:33:31 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:33:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:31 @agent_ppo2.py:193][0m |           0.0010 |          17.2416 |           4.2892 |
[32m[20230117 13:33:31 @agent_ppo2.py:193][0m |          -0.0051 |          16.5034 |           4.2844 |
[32m[20230117 13:33:31 @agent_ppo2.py:193][0m |           0.0036 |          17.3917 |           4.2859 |
[32m[20230117 13:33:31 @agent_ppo2.py:193][0m |          -0.0088 |          15.9955 |           4.2843 |
[32m[20230117 13:33:31 @agent_ppo2.py:193][0m |          -0.0066 |          16.0924 |           4.2848 |
[32m[20230117 13:33:32 @agent_ppo2.py:193][0m |          -0.0103 |          15.5942 |           4.2854 |
[32m[20230117 13:33:32 @agent_ppo2.py:193][0m |          -0.0053 |          15.8935 |           4.2868 |
[32m[20230117 13:33:32 @agent_ppo2.py:193][0m |          -0.0114 |          15.2547 |           4.2833 |
[32m[20230117 13:33:32 @agent_ppo2.py:193][0m |          -0.0120 |          15.1335 |           4.2850 |
[32m[20230117 13:33:32 @agent_ppo2.py:193][0m |          -0.0115 |          15.0489 |           4.2870 |
[32m[20230117 13:33:32 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:33:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.17
[32m[20230117 13:33:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.11
[32m[20230117 13:33:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.05
[32m[20230117 13:33:32 @agent_ppo2.py:151][0m Total time:       7.12 min
[32m[20230117 13:33:32 @agent_ppo2.py:153][0m 628736 total steps have happened
[32m[20230117 13:33:32 @agent_ppo2.py:129][0m #------------------------ Iteration 307 --------------------------#
[32m[20230117 13:33:32 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:32 @agent_ppo2.py:193][0m |          -0.0006 |          17.1767 |           4.2799 |
[32m[20230117 13:33:32 @agent_ppo2.py:193][0m |          -0.0055 |          16.4646 |           4.2791 |
[32m[20230117 13:33:32 @agent_ppo2.py:193][0m |          -0.0082 |          16.0116 |           4.2747 |
[32m[20230117 13:33:33 @agent_ppo2.py:193][0m |          -0.0092 |          15.8316 |           4.2734 |
[32m[20230117 13:33:33 @agent_ppo2.py:193][0m |          -0.0081 |          15.5908 |           4.2733 |
[32m[20230117 13:33:33 @agent_ppo2.py:193][0m |          -0.0111 |          15.1755 |           4.2730 |
[32m[20230117 13:33:33 @agent_ppo2.py:193][0m |          -0.0110 |          15.0765 |           4.2725 |
[32m[20230117 13:33:33 @agent_ppo2.py:193][0m |          -0.0126 |          14.6422 |           4.2713 |
[32m[20230117 13:33:33 @agent_ppo2.py:193][0m |          -0.0126 |          14.3548 |           4.2732 |
[32m[20230117 13:33:33 @agent_ppo2.py:193][0m |          -0.0102 |          14.4067 |           4.2713 |
[32m[20230117 13:33:33 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:33:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.80
[32m[20230117 13:33:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.37
[32m[20230117 13:33:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.37
[32m[20230117 13:33:33 @agent_ppo2.py:151][0m Total time:       7.14 min
[32m[20230117 13:33:33 @agent_ppo2.py:153][0m 630784 total steps have happened
[32m[20230117 13:33:33 @agent_ppo2.py:129][0m #------------------------ Iteration 308 --------------------------#
[32m[20230117 13:33:33 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:33:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:34 @agent_ppo2.py:193][0m |          -0.0002 |          19.0560 |           4.3724 |
[32m[20230117 13:33:34 @agent_ppo2.py:193][0m |          -0.0060 |          16.6174 |           4.3685 |
[32m[20230117 13:33:34 @agent_ppo2.py:193][0m |          -0.0087 |          15.6238 |           4.3620 |
[32m[20230117 13:33:34 @agent_ppo2.py:193][0m |          -0.0096 |          14.8662 |           4.3623 |
[32m[20230117 13:33:34 @agent_ppo2.py:193][0m |          -0.0109 |          14.3230 |           4.3632 |
[32m[20230117 13:33:34 @agent_ppo2.py:193][0m |          -0.0116 |          13.9674 |           4.3635 |
[32m[20230117 13:33:34 @agent_ppo2.py:193][0m |          -0.0131 |          13.7116 |           4.3630 |
[32m[20230117 13:33:34 @agent_ppo2.py:193][0m |          -0.0134 |          13.4883 |           4.3599 |
[32m[20230117 13:33:34 @agent_ppo2.py:193][0m |          -0.0143 |          13.2811 |           4.3643 |
[32m[20230117 13:33:34 @agent_ppo2.py:193][0m |          -0.0144 |          13.1105 |           4.3619 |
[32m[20230117 13:33:34 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:33:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.43
[32m[20230117 13:33:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.45
[32m[20230117 13:33:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.16
[32m[20230117 13:33:34 @agent_ppo2.py:151][0m Total time:       7.16 min
[32m[20230117 13:33:34 @agent_ppo2.py:153][0m 632832 total steps have happened
[32m[20230117 13:33:34 @agent_ppo2.py:129][0m #------------------------ Iteration 309 --------------------------#
[32m[20230117 13:33:35 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:33:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:35 @agent_ppo2.py:193][0m |           0.0015 |          17.8674 |           4.3344 |
[32m[20230117 13:33:35 @agent_ppo2.py:193][0m |          -0.0030 |          14.9503 |           4.3343 |
[32m[20230117 13:33:35 @agent_ppo2.py:193][0m |          -0.0057 |          14.1761 |           4.3339 |
[32m[20230117 13:33:35 @agent_ppo2.py:193][0m |          -0.0068 |          13.6102 |           4.3293 |
[32m[20230117 13:33:35 @agent_ppo2.py:193][0m |          -0.0077 |          13.0336 |           4.3349 |
[32m[20230117 13:33:35 @agent_ppo2.py:193][0m |          -0.0091 |          12.5707 |           4.3319 |
[32m[20230117 13:33:35 @agent_ppo2.py:193][0m |          -0.0092 |          12.0277 |           4.3344 |
[32m[20230117 13:33:35 @agent_ppo2.py:193][0m |          -0.0109 |          11.6609 |           4.3333 |
[32m[20230117 13:33:35 @agent_ppo2.py:193][0m |          -0.0112 |          10.9805 |           4.3353 |
[32m[20230117 13:33:35 @agent_ppo2.py:193][0m |          -0.0125 |          10.6356 |           4.3335 |
[32m[20230117 13:33:35 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:33:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.85
[32m[20230117 13:33:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.65
[32m[20230117 13:33:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.22
[32m[20230117 13:33:36 @agent_ppo2.py:151][0m Total time:       7.18 min
[32m[20230117 13:33:36 @agent_ppo2.py:153][0m 634880 total steps have happened
[32m[20230117 13:33:36 @agent_ppo2.py:129][0m #------------------------ Iteration 310 --------------------------#
[32m[20230117 13:33:36 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:36 @agent_ppo2.py:193][0m |           0.0005 |          17.9925 |           4.3230 |
[32m[20230117 13:33:36 @agent_ppo2.py:193][0m |          -0.0044 |          14.9590 |           4.3199 |
[32m[20230117 13:33:36 @agent_ppo2.py:193][0m |          -0.0063 |          13.5463 |           4.3215 |
[32m[20230117 13:33:36 @agent_ppo2.py:193][0m |          -0.0056 |          13.0770 |           4.3215 |
[32m[20230117 13:33:36 @agent_ppo2.py:193][0m |          -0.0070 |          12.5468 |           4.3187 |
[32m[20230117 13:33:36 @agent_ppo2.py:193][0m |          -0.0081 |          12.1851 |           4.3236 |
[32m[20230117 13:33:36 @agent_ppo2.py:193][0m |          -0.0091 |          11.8753 |           4.3184 |
[32m[20230117 13:33:36 @agent_ppo2.py:193][0m |          -0.0094 |          11.5055 |           4.3204 |
[32m[20230117 13:33:36 @agent_ppo2.py:193][0m |          -0.0109 |          11.2145 |           4.3181 |
[32m[20230117 13:33:36 @agent_ppo2.py:193][0m |          -0.0117 |          10.8828 |           4.3177 |
[32m[20230117 13:33:36 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:33:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 267.17
[32m[20230117 13:33:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.28
[32m[20230117 13:33:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.70
[32m[20230117 13:33:37 @agent_ppo2.py:151][0m Total time:       7.19 min
[32m[20230117 13:33:37 @agent_ppo2.py:153][0m 636928 total steps have happened
[32m[20230117 13:33:37 @agent_ppo2.py:129][0m #------------------------ Iteration 311 --------------------------#
[32m[20230117 13:33:37 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:33:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:37 @agent_ppo2.py:193][0m |           0.0044 |          31.4578 |           4.2739 |
[32m[20230117 13:33:37 @agent_ppo2.py:193][0m |          -0.0076 |          19.9490 |           4.2710 |
[32m[20230117 13:33:37 @agent_ppo2.py:193][0m |          -0.0092 |          18.0702 |           4.2694 |
[32m[20230117 13:33:37 @agent_ppo2.py:193][0m |          -0.0105 |          17.3067 |           4.2694 |
[32m[20230117 13:33:37 @agent_ppo2.py:193][0m |          -0.0055 |          17.1661 |           4.2651 |
[32m[20230117 13:33:37 @agent_ppo2.py:193][0m |          -0.0137 |          16.3384 |           4.2677 |
[32m[20230117 13:33:37 @agent_ppo2.py:193][0m |          -0.0130 |          15.9063 |           4.2637 |
[32m[20230117 13:33:37 @agent_ppo2.py:193][0m |          -0.0112 |          15.6424 |           4.2646 |
[32m[20230117 13:33:38 @agent_ppo2.py:193][0m |          -0.0118 |          15.4231 |           4.2638 |
[32m[20230117 13:33:38 @agent_ppo2.py:193][0m |          -0.0144 |          15.3616 |           4.2617 |
[32m[20230117 13:33:38 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:33:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 220.33
[32m[20230117 13:33:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.35
[32m[20230117 13:33:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.89
[32m[20230117 13:33:38 @agent_ppo2.py:151][0m Total time:       7.21 min
[32m[20230117 13:33:38 @agent_ppo2.py:153][0m 638976 total steps have happened
[32m[20230117 13:33:38 @agent_ppo2.py:129][0m #------------------------ Iteration 312 --------------------------#
[32m[20230117 13:33:38 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:33:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:38 @agent_ppo2.py:193][0m |          -0.0001 |          16.1908 |           4.2992 |
[32m[20230117 13:33:38 @agent_ppo2.py:193][0m |           0.0336 |          15.1632 |           4.2836 |
[32m[20230117 13:33:38 @agent_ppo2.py:193][0m |          -0.0004 |          11.4120 |           4.2857 |
[32m[20230117 13:33:38 @agent_ppo2.py:193][0m |          -0.0102 |          10.1362 |           4.2879 |
[32m[20230117 13:33:38 @agent_ppo2.py:193][0m |          -0.0092 |           9.5651 |           4.2881 |
[32m[20230117 13:33:39 @agent_ppo2.py:193][0m |          -0.0114 |           9.0723 |           4.2858 |
[32m[20230117 13:33:39 @agent_ppo2.py:193][0m |          -0.0126 |           8.7346 |           4.2856 |
[32m[20230117 13:33:39 @agent_ppo2.py:193][0m |          -0.0103 |           8.4411 |           4.2870 |
[32m[20230117 13:33:39 @agent_ppo2.py:193][0m |          -0.0157 |           8.2443 |           4.2861 |
[32m[20230117 13:33:39 @agent_ppo2.py:193][0m |          -0.0100 |           8.0393 |           4.2811 |
[32m[20230117 13:33:39 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:33:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.56
[32m[20230117 13:33:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.44
[32m[20230117 13:33:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 148.71
[32m[20230117 13:33:39 @agent_ppo2.py:151][0m Total time:       7.23 min
[32m[20230117 13:33:39 @agent_ppo2.py:153][0m 641024 total steps have happened
[32m[20230117 13:33:39 @agent_ppo2.py:129][0m #------------------------ Iteration 313 --------------------------#
[32m[20230117 13:33:39 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:39 @agent_ppo2.py:193][0m |           0.0006 |          22.5819 |           4.4076 |
[32m[20230117 13:33:39 @agent_ppo2.py:193][0m |          -0.0044 |          17.5707 |           4.3936 |
[32m[20230117 13:33:40 @agent_ppo2.py:193][0m |          -0.0088 |          14.6946 |           4.3869 |
[32m[20230117 13:33:40 @agent_ppo2.py:193][0m |          -0.0082 |          12.9355 |           4.3864 |
[32m[20230117 13:33:40 @agent_ppo2.py:193][0m |          -0.0085 |          11.7091 |           4.3815 |
[32m[20230117 13:33:40 @agent_ppo2.py:193][0m |          -0.0081 |          10.9438 |           4.3766 |
[32m[20230117 13:33:40 @agent_ppo2.py:193][0m |          -0.0110 |          10.3049 |           4.3731 |
[32m[20230117 13:33:40 @agent_ppo2.py:193][0m |          -0.0123 |           9.6455 |           4.3728 |
[32m[20230117 13:33:40 @agent_ppo2.py:193][0m |          -0.0117 |           9.1938 |           4.3731 |
[32m[20230117 13:33:40 @agent_ppo2.py:193][0m |          -0.0128 |           8.8019 |           4.3686 |
[32m[20230117 13:33:40 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:33:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.98
[32m[20230117 13:33:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.27
[32m[20230117 13:33:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 279.90
[32m[20230117 13:33:40 @agent_ppo2.py:151][0m Total time:       7.25 min
[32m[20230117 13:33:40 @agent_ppo2.py:153][0m 643072 total steps have happened
[32m[20230117 13:33:40 @agent_ppo2.py:129][0m #------------------------ Iteration 314 --------------------------#
[32m[20230117 13:33:40 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:41 @agent_ppo2.py:193][0m |          -0.0014 |          20.0269 |           4.3739 |
[32m[20230117 13:33:41 @agent_ppo2.py:193][0m |          -0.0064 |          18.3420 |           4.3666 |
[32m[20230117 13:33:41 @agent_ppo2.py:193][0m |          -0.0076 |          17.7800 |           4.3629 |
[32m[20230117 13:33:41 @agent_ppo2.py:193][0m |          -0.0082 |          17.4098 |           4.3624 |
[32m[20230117 13:33:41 @agent_ppo2.py:193][0m |          -0.0086 |          17.1693 |           4.3596 |
[32m[20230117 13:33:41 @agent_ppo2.py:193][0m |          -0.0096 |          16.9693 |           4.3618 |
[32m[20230117 13:33:41 @agent_ppo2.py:193][0m |          -0.0101 |          16.7754 |           4.3570 |
[32m[20230117 13:33:41 @agent_ppo2.py:193][0m |          -0.0107 |          16.6380 |           4.3591 |
[32m[20230117 13:33:41 @agent_ppo2.py:193][0m |          -0.0112 |          16.4671 |           4.3583 |
[32m[20230117 13:33:41 @agent_ppo2.py:193][0m |          -0.0120 |          16.3763 |           4.3561 |
[32m[20230117 13:33:41 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:33:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.54
[32m[20230117 13:33:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.51
[32m[20230117 13:33:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.20
[32m[20230117 13:33:41 @agent_ppo2.py:151][0m Total time:       7.27 min
[32m[20230117 13:33:41 @agent_ppo2.py:153][0m 645120 total steps have happened
[32m[20230117 13:33:41 @agent_ppo2.py:129][0m #------------------------ Iteration 315 --------------------------#
[32m[20230117 13:33:42 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:33:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:42 @agent_ppo2.py:193][0m |          -0.0012 |          30.3674 |           4.3102 |
[32m[20230117 13:33:42 @agent_ppo2.py:193][0m |          -0.0064 |          21.2164 |           4.3043 |
[32m[20230117 13:33:42 @agent_ppo2.py:193][0m |          -0.0092 |          19.6014 |           4.3016 |
[32m[20230117 13:33:42 @agent_ppo2.py:193][0m |          -0.0123 |          18.4816 |           4.3021 |
[32m[20230117 13:33:42 @agent_ppo2.py:193][0m |          -0.0151 |          17.6653 |           4.3022 |
[32m[20230117 13:33:42 @agent_ppo2.py:193][0m |          -0.0159 |          16.8558 |           4.2994 |
[32m[20230117 13:33:42 @agent_ppo2.py:193][0m |          -0.0159 |          16.2081 |           4.3022 |
[32m[20230117 13:33:42 @agent_ppo2.py:193][0m |          -0.0173 |          15.6375 |           4.3022 |
[32m[20230117 13:33:42 @agent_ppo2.py:193][0m |          -0.0183 |          15.0264 |           4.3015 |
[32m[20230117 13:33:42 @agent_ppo2.py:193][0m |          -0.0190 |          14.4667 |           4.2984 |
[32m[20230117 13:33:42 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:33:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 164.16
[32m[20230117 13:33:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.02
[32m[20230117 13:33:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 279.43
[32m[20230117 13:33:43 @agent_ppo2.py:151][0m Total time:       7.29 min
[32m[20230117 13:33:43 @agent_ppo2.py:153][0m 647168 total steps have happened
[32m[20230117 13:33:43 @agent_ppo2.py:129][0m #------------------------ Iteration 316 --------------------------#
[32m[20230117 13:33:43 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:33:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:43 @agent_ppo2.py:193][0m |          -0.0003 |          20.7135 |           4.2351 |
[32m[20230117 13:33:43 @agent_ppo2.py:193][0m |          -0.0040 |          19.0859 |           4.2323 |
[32m[20230117 13:33:43 @agent_ppo2.py:193][0m |          -0.0041 |          18.8108 |           4.2268 |
[32m[20230117 13:33:43 @agent_ppo2.py:193][0m |          -0.0073 |          18.4458 |           4.2299 |
[32m[20230117 13:33:43 @agent_ppo2.py:193][0m |          -0.0074 |          18.0871 |           4.2323 |
[32m[20230117 13:33:43 @agent_ppo2.py:193][0m |          -0.0093 |          17.8768 |           4.2298 |
[32m[20230117 13:33:43 @agent_ppo2.py:193][0m |          -0.0106 |          17.6189 |           4.2292 |
[32m[20230117 13:33:43 @agent_ppo2.py:193][0m |          -0.0113 |          17.4403 |           4.2294 |
[32m[20230117 13:33:43 @agent_ppo2.py:193][0m |          -0.0098 |          17.3303 |           4.2330 |
[32m[20230117 13:33:44 @agent_ppo2.py:193][0m |          -0.0100 |          17.2281 |           4.2288 |
[32m[20230117 13:33:44 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:33:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.99
[32m[20230117 13:33:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.17
[32m[20230117 13:33:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 144.16
[32m[20230117 13:33:44 @agent_ppo2.py:151][0m Total time:       7.31 min
[32m[20230117 13:33:44 @agent_ppo2.py:153][0m 649216 total steps have happened
[32m[20230117 13:33:44 @agent_ppo2.py:129][0m #------------------------ Iteration 317 --------------------------#
[32m[20230117 13:33:44 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:33:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:44 @agent_ppo2.py:193][0m |           0.0003 |          31.5920 |           4.3189 |
[32m[20230117 13:33:44 @agent_ppo2.py:193][0m |          -0.0065 |          20.3879 |           4.3076 |
[32m[20230117 13:33:44 @agent_ppo2.py:193][0m |          -0.0089 |          17.5715 |           4.3054 |
[32m[20230117 13:33:44 @agent_ppo2.py:193][0m |          -0.0066 |          16.0357 |           4.3009 |
[32m[20230117 13:33:44 @agent_ppo2.py:193][0m |          -0.0129 |          14.9314 |           4.2989 |
[32m[20230117 13:33:44 @agent_ppo2.py:193][0m |          -0.0112 |          14.6442 |           4.2981 |
[32m[20230117 13:33:44 @agent_ppo2.py:193][0m |          -0.0133 |          13.7390 |           4.2983 |
[32m[20230117 13:33:44 @agent_ppo2.py:193][0m |          -0.0125 |          13.4347 |           4.3008 |
[32m[20230117 13:33:45 @agent_ppo2.py:193][0m |          -0.0134 |          13.1118 |           4.3004 |
[32m[20230117 13:33:45 @agent_ppo2.py:193][0m |          -0.0153 |          12.8135 |           4.2960 |
[32m[20230117 13:33:45 @agent_ppo2.py:138][0m Policy update time: 0.65 s
[32m[20230117 13:33:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 213.42
[32m[20230117 13:33:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.25
[32m[20230117 13:33:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.62
[32m[20230117 13:33:45 @agent_ppo2.py:151][0m Total time:       7.33 min
[32m[20230117 13:33:45 @agent_ppo2.py:153][0m 651264 total steps have happened
[32m[20230117 13:33:45 @agent_ppo2.py:129][0m #------------------------ Iteration 318 --------------------------#
[32m[20230117 13:33:45 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:33:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:45 @agent_ppo2.py:193][0m |          -0.0024 |          26.9225 |           4.3220 |
[32m[20230117 13:33:45 @agent_ppo2.py:193][0m |          -0.0035 |          23.3541 |           4.3198 |
[32m[20230117 13:33:45 @agent_ppo2.py:193][0m |          -0.0071 |          22.2628 |           4.3224 |
[32m[20230117 13:33:45 @agent_ppo2.py:193][0m |          -0.0149 |          21.5953 |           4.3205 |
[32m[20230117 13:33:45 @agent_ppo2.py:193][0m |          -0.0078 |          21.1917 |           4.3250 |
[32m[20230117 13:33:46 @agent_ppo2.py:193][0m |          -0.0139 |          20.7481 |           4.3207 |
[32m[20230117 13:33:46 @agent_ppo2.py:193][0m |          -0.0077 |          20.8971 |           4.3236 |
[32m[20230117 13:33:46 @agent_ppo2.py:193][0m |          -0.0130 |          20.1059 |           4.3214 |
[32m[20230117 13:33:46 @agent_ppo2.py:193][0m |          -0.0148 |          19.7025 |           4.3244 |
[32m[20230117 13:33:46 @agent_ppo2.py:193][0m |          -0.0148 |          19.5107 |           4.3230 |
[32m[20230117 13:33:46 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:33:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.90
[32m[20230117 13:33:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 271.51
[32m[20230117 13:33:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.08
[32m[20230117 13:33:46 @agent_ppo2.py:151][0m Total time:       7.35 min
[32m[20230117 13:33:46 @agent_ppo2.py:153][0m 653312 total steps have happened
[32m[20230117 13:33:46 @agent_ppo2.py:129][0m #------------------------ Iteration 319 --------------------------#
[32m[20230117 13:33:46 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:33:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:46 @agent_ppo2.py:193][0m |          -0.0015 |          19.1002 |           4.3473 |
[32m[20230117 13:33:46 @agent_ppo2.py:193][0m |          -0.0064 |          16.8947 |           4.3371 |
[32m[20230117 13:33:46 @agent_ppo2.py:193][0m |          -0.0076 |          16.4677 |           4.3347 |
[32m[20230117 13:33:47 @agent_ppo2.py:193][0m |          -0.0091 |          16.1386 |           4.3344 |
[32m[20230117 13:33:47 @agent_ppo2.py:193][0m |          -0.0090 |          15.9537 |           4.3342 |
[32m[20230117 13:33:47 @agent_ppo2.py:193][0m |          -0.0095 |          15.7605 |           4.3308 |
[32m[20230117 13:33:47 @agent_ppo2.py:193][0m |          -0.0103 |          15.5899 |           4.3287 |
[32m[20230117 13:33:47 @agent_ppo2.py:193][0m |          -0.0113 |          15.4166 |           4.3286 |
[32m[20230117 13:33:47 @agent_ppo2.py:193][0m |          -0.0116 |          15.2692 |           4.3272 |
[32m[20230117 13:33:47 @agent_ppo2.py:193][0m |          -0.0123 |          15.1647 |           4.3300 |
[32m[20230117 13:33:47 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:33:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.10
[32m[20230117 13:33:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.36
[32m[20230117 13:33:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.21
[32m[20230117 13:33:47 @agent_ppo2.py:151][0m Total time:       7.37 min
[32m[20230117 13:33:47 @agent_ppo2.py:153][0m 655360 total steps have happened
[32m[20230117 13:33:47 @agent_ppo2.py:129][0m #------------------------ Iteration 320 --------------------------#
[32m[20230117 13:33:47 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:47 @agent_ppo2.py:193][0m |          -0.0011 |          18.1182 |           4.2880 |
[32m[20230117 13:33:48 @agent_ppo2.py:193][0m |          -0.0079 |          16.2100 |           4.2866 |
[32m[20230117 13:33:48 @agent_ppo2.py:193][0m |          -0.0066 |          15.3904 |           4.2848 |
[32m[20230117 13:33:48 @agent_ppo2.py:193][0m |          -0.0138 |          14.9813 |           4.2861 |
[32m[20230117 13:33:48 @agent_ppo2.py:193][0m |          -0.0022 |          14.7702 |           4.2869 |
[32m[20230117 13:33:48 @agent_ppo2.py:193][0m |          -0.0095 |          14.4534 |           4.2853 |
[32m[20230117 13:33:48 @agent_ppo2.py:193][0m |          -0.0123 |          14.2296 |           4.2891 |
[32m[20230117 13:33:48 @agent_ppo2.py:193][0m |          -0.0112 |          14.0975 |           4.2901 |
[32m[20230117 13:33:48 @agent_ppo2.py:193][0m |          -0.0145 |          13.9661 |           4.2917 |
[32m[20230117 13:33:48 @agent_ppo2.py:193][0m |          -0.0139 |          13.6661 |           4.2897 |
[32m[20230117 13:33:48 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:33:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 267.16
[32m[20230117 13:33:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.36
[32m[20230117 13:33:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.56
[32m[20230117 13:33:48 @agent_ppo2.py:151][0m Total time:       7.39 min
[32m[20230117 13:33:48 @agent_ppo2.py:153][0m 657408 total steps have happened
[32m[20230117 13:33:48 @agent_ppo2.py:129][0m #------------------------ Iteration 321 --------------------------#
[32m[20230117 13:33:48 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:33:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:49 @agent_ppo2.py:193][0m |           0.0007 |          16.9571 |           4.3185 |
[32m[20230117 13:33:49 @agent_ppo2.py:193][0m |          -0.0065 |          14.3218 |           4.3083 |
[32m[20230117 13:33:49 @agent_ppo2.py:193][0m |          -0.0065 |          12.6129 |           4.3131 |
[32m[20230117 13:33:49 @agent_ppo2.py:193][0m |          -0.0065 |          11.6063 |           4.3117 |
[32m[20230117 13:33:49 @agent_ppo2.py:193][0m |          -0.0092 |          10.7374 |           4.3131 |
[32m[20230117 13:33:49 @agent_ppo2.py:193][0m |          -0.0118 |          10.3583 |           4.3131 |
[32m[20230117 13:33:49 @agent_ppo2.py:193][0m |          -0.0089 |          10.2217 |           4.3150 |
[32m[20230117 13:33:49 @agent_ppo2.py:193][0m |          -0.0122 |           9.8721 |           4.3129 |
[32m[20230117 13:33:49 @agent_ppo2.py:193][0m |          -0.0031 |          10.4415 |           4.3142 |
[32m[20230117 13:33:49 @agent_ppo2.py:193][0m |          -0.0120 |           9.5517 |           4.3161 |
[32m[20230117 13:33:49 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:33:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.76
[32m[20230117 13:33:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.78
[32m[20230117 13:33:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 280.60
[32m[20230117 13:33:49 @agent_ppo2.py:151][0m Total time:       7.41 min
[32m[20230117 13:33:49 @agent_ppo2.py:153][0m 659456 total steps have happened
[32m[20230117 13:33:49 @agent_ppo2.py:129][0m #------------------------ Iteration 322 --------------------------#
[32m[20230117 13:33:50 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:33:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:50 @agent_ppo2.py:193][0m |           0.0032 |          22.6869 |           4.3196 |
[32m[20230117 13:33:50 @agent_ppo2.py:193][0m |          -0.0070 |          13.3065 |           4.3091 |
[32m[20230117 13:33:50 @agent_ppo2.py:193][0m |          -0.0054 |          11.0072 |           4.3061 |
[32m[20230117 13:33:50 @agent_ppo2.py:193][0m |          -0.0065 |           9.7250 |           4.3058 |
[32m[20230117 13:33:50 @agent_ppo2.py:193][0m |          -0.0109 |           9.1110 |           4.3040 |
[32m[20230117 13:33:50 @agent_ppo2.py:193][0m |          -0.0110 |           8.4402 |           4.3052 |
[32m[20230117 13:33:50 @agent_ppo2.py:193][0m |          -0.0109 |           7.9781 |           4.3032 |
[32m[20230117 13:33:50 @agent_ppo2.py:193][0m |          -0.0107 |           7.5609 |           4.3034 |
[32m[20230117 13:33:50 @agent_ppo2.py:193][0m |          -0.0110 |           7.1899 |           4.3008 |
[32m[20230117 13:33:50 @agent_ppo2.py:193][0m |          -0.0134 |           6.8312 |           4.3048 |
[32m[20230117 13:33:50 @agent_ppo2.py:138][0m Policy update time: 0.69 s
[32m[20230117 13:33:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 218.80
[32m[20230117 13:33:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.56
[32m[20230117 13:33:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.63
[32m[20230117 13:33:51 @agent_ppo2.py:151][0m Total time:       7.43 min
[32m[20230117 13:33:51 @agent_ppo2.py:153][0m 661504 total steps have happened
[32m[20230117 13:33:51 @agent_ppo2.py:129][0m #------------------------ Iteration 323 --------------------------#
[32m[20230117 13:33:51 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:33:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:51 @agent_ppo2.py:193][0m |          -0.0044 |          17.6083 |           4.2763 |
[32m[20230117 13:33:51 @agent_ppo2.py:193][0m |          -0.0078 |          15.2573 |           4.2729 |
[32m[20230117 13:33:51 @agent_ppo2.py:193][0m |          -0.0107 |          13.4075 |           4.2697 |
[32m[20230117 13:33:51 @agent_ppo2.py:193][0m |          -0.0024 |          12.4279 |           4.2658 |
[32m[20230117 13:33:51 @agent_ppo2.py:193][0m |          -0.0066 |          11.5461 |           4.2658 |
[32m[20230117 13:33:51 @agent_ppo2.py:193][0m |          -0.0065 |          11.1659 |           4.2645 |
[32m[20230117 13:33:51 @agent_ppo2.py:193][0m |          -0.0097 |          10.4940 |           4.2628 |
[32m[20230117 13:33:51 @agent_ppo2.py:193][0m |          -0.0109 |          10.1088 |           4.2616 |
[32m[20230117 13:33:51 @agent_ppo2.py:193][0m |          -0.0114 |           9.7979 |           4.2606 |
[32m[20230117 13:33:51 @agent_ppo2.py:193][0m |          -0.0078 |           9.6390 |           4.2606 |
[32m[20230117 13:33:51 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:33:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.03
[32m[20230117 13:33:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.47
[32m[20230117 13:33:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.61
[32m[20230117 13:33:52 @agent_ppo2.py:151][0m Total time:       7.44 min
[32m[20230117 13:33:52 @agent_ppo2.py:153][0m 663552 total steps have happened
[32m[20230117 13:33:52 @agent_ppo2.py:129][0m #------------------------ Iteration 324 --------------------------#
[32m[20230117 13:33:52 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:33:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:52 @agent_ppo2.py:193][0m |           0.0017 |          23.1634 |           4.4259 |
[32m[20230117 13:33:52 @agent_ppo2.py:193][0m |          -0.0038 |          18.8059 |           4.4154 |
[32m[20230117 13:33:52 @agent_ppo2.py:193][0m |          -0.0055 |          17.7599 |           4.4159 |
[32m[20230117 13:33:52 @agent_ppo2.py:193][0m |          -0.0086 |          17.2726 |           4.4138 |
[32m[20230117 13:33:52 @agent_ppo2.py:193][0m |          -0.0090 |          16.7545 |           4.4152 |
[32m[20230117 13:33:52 @agent_ppo2.py:193][0m |          -0.0091 |          16.5021 |           4.4119 |
[32m[20230117 13:33:52 @agent_ppo2.py:193][0m |          -0.0096 |          16.2982 |           4.4161 |
[32m[20230117 13:33:53 @agent_ppo2.py:193][0m |          -0.0104 |          15.9663 |           4.4162 |
[32m[20230117 13:33:53 @agent_ppo2.py:193][0m |          -0.0112 |          15.8142 |           4.4116 |
[32m[20230117 13:33:53 @agent_ppo2.py:193][0m |          -0.0112 |          15.5868 |           4.4149 |
[32m[20230117 13:33:53 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:33:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.73
[32m[20230117 13:33:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.37
[32m[20230117 13:33:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 280.09
[32m[20230117 13:33:53 @agent_ppo2.py:151][0m Total time:       7.46 min
[32m[20230117 13:33:53 @agent_ppo2.py:153][0m 665600 total steps have happened
[32m[20230117 13:33:53 @agent_ppo2.py:129][0m #------------------------ Iteration 325 --------------------------#
[32m[20230117 13:33:53 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:33:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:53 @agent_ppo2.py:193][0m |          -0.0005 |          16.7041 |           4.3197 |
[32m[20230117 13:33:53 @agent_ppo2.py:193][0m |          -0.0057 |          14.5471 |           4.3135 |
[32m[20230117 13:33:53 @agent_ppo2.py:193][0m |          -0.0074 |          13.5321 |           4.3114 |
[32m[20230117 13:33:53 @agent_ppo2.py:193][0m |          -0.0085 |          13.0083 |           4.3086 |
[32m[20230117 13:33:53 @agent_ppo2.py:193][0m |          -0.0099 |          12.6847 |           4.3068 |
[32m[20230117 13:33:54 @agent_ppo2.py:193][0m |          -0.0102 |          12.4807 |           4.3068 |
[32m[20230117 13:33:54 @agent_ppo2.py:193][0m |          -0.0109 |          12.1860 |           4.3073 |
[32m[20230117 13:33:54 @agent_ppo2.py:193][0m |          -0.0112 |          12.0237 |           4.3029 |
[32m[20230117 13:33:54 @agent_ppo2.py:193][0m |          -0.0117 |          11.8279 |           4.3068 |
[32m[20230117 13:33:54 @agent_ppo2.py:193][0m |          -0.0121 |          11.7326 |           4.3091 |
[32m[20230117 13:33:54 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:33:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.93
[32m[20230117 13:33:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.85
[32m[20230117 13:33:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.89
[32m[20230117 13:33:54 @agent_ppo2.py:151][0m Total time:       7.48 min
[32m[20230117 13:33:54 @agent_ppo2.py:153][0m 667648 total steps have happened
[32m[20230117 13:33:54 @agent_ppo2.py:129][0m #------------------------ Iteration 326 --------------------------#
[32m[20230117 13:33:54 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:33:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:54 @agent_ppo2.py:193][0m |          -0.0015 |          18.5095 |           4.2617 |
[32m[20230117 13:33:54 @agent_ppo2.py:193][0m |          -0.0083 |          17.2436 |           4.2545 |
[32m[20230117 13:33:54 @agent_ppo2.py:193][0m |           0.0053 |          18.8179 |           4.2520 |
[32m[20230117 13:33:55 @agent_ppo2.py:193][0m |          -0.0080 |          15.9953 |           4.2510 |
[32m[20230117 13:33:55 @agent_ppo2.py:193][0m |          -0.0125 |          15.0542 |           4.2529 |
[32m[20230117 13:33:55 @agent_ppo2.py:193][0m |          -0.0143 |          14.6426 |           4.2540 |
[32m[20230117 13:33:55 @agent_ppo2.py:193][0m |          -0.0122 |          14.2105 |           4.2501 |
[32m[20230117 13:33:55 @agent_ppo2.py:193][0m |          -0.0147 |          13.9960 |           4.2518 |
[32m[20230117 13:33:55 @agent_ppo2.py:193][0m |          -0.0142 |          13.7630 |           4.2507 |
[32m[20230117 13:33:55 @agent_ppo2.py:193][0m |          -0.0134 |          13.4275 |           4.2513 |
[32m[20230117 13:33:55 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:33:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.64
[32m[20230117 13:33:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.48
[32m[20230117 13:33:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.62
[32m[20230117 13:33:55 @agent_ppo2.py:151][0m Total time:       7.50 min
[32m[20230117 13:33:55 @agent_ppo2.py:153][0m 669696 total steps have happened
[32m[20230117 13:33:55 @agent_ppo2.py:129][0m #------------------------ Iteration 327 --------------------------#
[32m[20230117 13:33:55 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:33:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:56 @agent_ppo2.py:193][0m |          -0.0015 |          27.4830 |           4.3921 |
[32m[20230117 13:33:56 @agent_ppo2.py:193][0m |          -0.0045 |          18.0433 |           4.3890 |
[32m[20230117 13:33:56 @agent_ppo2.py:193][0m |          -0.0083 |          15.1928 |           4.3857 |
[32m[20230117 13:33:56 @agent_ppo2.py:193][0m |          -0.0101 |          13.5031 |           4.3831 |
[32m[20230117 13:33:56 @agent_ppo2.py:193][0m |          -0.0116 |          12.4691 |           4.3816 |
[32m[20230117 13:33:56 @agent_ppo2.py:193][0m |          -0.0123 |          11.7027 |           4.3821 |
[32m[20230117 13:33:56 @agent_ppo2.py:193][0m |          -0.0131 |          11.0890 |           4.3818 |
[32m[20230117 13:33:56 @agent_ppo2.py:193][0m |          -0.0128 |          10.7345 |           4.3847 |
[32m[20230117 13:33:56 @agent_ppo2.py:193][0m |          -0.0133 |          10.3043 |           4.3838 |
[32m[20230117 13:33:56 @agent_ppo2.py:193][0m |          -0.0136 |          10.0082 |           4.3840 |
[32m[20230117 13:33:56 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:33:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 210.24
[32m[20230117 13:33:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.19
[32m[20230117 13:33:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.76
[32m[20230117 13:33:56 @agent_ppo2.py:151][0m Total time:       7.52 min
[32m[20230117 13:33:56 @agent_ppo2.py:153][0m 671744 total steps have happened
[32m[20230117 13:33:56 @agent_ppo2.py:129][0m #------------------------ Iteration 328 --------------------------#
[32m[20230117 13:33:57 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:33:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:57 @agent_ppo2.py:193][0m |          -0.0003 |          20.7643 |           4.2968 |
[32m[20230117 13:33:57 @agent_ppo2.py:193][0m |          -0.0049 |          18.5777 |           4.2896 |
[32m[20230117 13:33:57 @agent_ppo2.py:193][0m |          -0.0071 |          17.6925 |           4.2922 |
[32m[20230117 13:33:57 @agent_ppo2.py:193][0m |          -0.0088 |          17.1330 |           4.2899 |
[32m[20230117 13:33:57 @agent_ppo2.py:193][0m |          -0.0094 |          16.6754 |           4.2946 |
[32m[20230117 13:33:57 @agent_ppo2.py:193][0m |          -0.0104 |          16.3597 |           4.2893 |
[32m[20230117 13:33:57 @agent_ppo2.py:193][0m |          -0.0105 |          16.0329 |           4.2904 |
[32m[20230117 13:33:57 @agent_ppo2.py:193][0m |          -0.0111 |          15.7574 |           4.2881 |
[32m[20230117 13:33:57 @agent_ppo2.py:193][0m |          -0.0118 |          15.4321 |           4.2922 |
[32m[20230117 13:33:57 @agent_ppo2.py:193][0m |          -0.0127 |          15.1788 |           4.2916 |
[32m[20230117 13:33:57 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:33:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.85
[32m[20230117 13:33:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.77
[32m[20230117 13:33:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.33
[32m[20230117 13:33:58 @agent_ppo2.py:151][0m Total time:       7.54 min
[32m[20230117 13:33:58 @agent_ppo2.py:153][0m 673792 total steps have happened
[32m[20230117 13:33:58 @agent_ppo2.py:129][0m #------------------------ Iteration 329 --------------------------#
[32m[20230117 13:33:58 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:33:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:58 @agent_ppo2.py:193][0m |           0.0010 |          25.7792 |           4.4599 |
[32m[20230117 13:33:58 @agent_ppo2.py:193][0m |          -0.0051 |          19.0078 |           4.4544 |
[32m[20230117 13:33:58 @agent_ppo2.py:193][0m |          -0.0063 |          16.8860 |           4.4562 |
[32m[20230117 13:33:58 @agent_ppo2.py:193][0m |          -0.0096 |          15.4642 |           4.4491 |
[32m[20230117 13:33:58 @agent_ppo2.py:193][0m |          -0.0105 |          14.5880 |           4.4509 |
[32m[20230117 13:33:58 @agent_ppo2.py:193][0m |          -0.0122 |          13.9703 |           4.4516 |
[32m[20230117 13:33:58 @agent_ppo2.py:193][0m |          -0.0129 |          13.4459 |           4.4487 |
[32m[20230117 13:33:58 @agent_ppo2.py:193][0m |          -0.0131 |          12.9526 |           4.4498 |
[32m[20230117 13:33:59 @agent_ppo2.py:193][0m |          -0.0138 |          12.4901 |           4.4464 |
[32m[20230117 13:33:59 @agent_ppo2.py:193][0m |          -0.0151 |          12.0879 |           4.4490 |
[32m[20230117 13:33:59 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:33:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 212.14
[32m[20230117 13:33:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.60
[32m[20230117 13:33:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.87
[32m[20230117 13:33:59 @agent_ppo2.py:151][0m Total time:       7.56 min
[32m[20230117 13:33:59 @agent_ppo2.py:153][0m 675840 total steps have happened
[32m[20230117 13:33:59 @agent_ppo2.py:129][0m #------------------------ Iteration 330 --------------------------#
[32m[20230117 13:33:59 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:33:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:33:59 @agent_ppo2.py:193][0m |          -0.0035 |          19.4187 |           4.3338 |
[32m[20230117 13:33:59 @agent_ppo2.py:193][0m |          -0.0094 |          17.5945 |           4.3249 |
[32m[20230117 13:33:59 @agent_ppo2.py:193][0m |          -0.0143 |          16.7824 |           4.3194 |
[32m[20230117 13:33:59 @agent_ppo2.py:193][0m |          -0.0129 |          16.0671 |           4.3135 |
[32m[20230117 13:33:59 @agent_ppo2.py:193][0m |           0.0022 |          18.2328 |           4.3180 |
[32m[20230117 13:34:00 @agent_ppo2.py:193][0m |          -0.0094 |          15.2214 |           4.3045 |
[32m[20230117 13:34:00 @agent_ppo2.py:193][0m |          -0.0139 |          14.9254 |           4.3165 |
[32m[20230117 13:34:00 @agent_ppo2.py:193][0m |          -0.0168 |          14.7257 |           4.3208 |
[32m[20230117 13:34:00 @agent_ppo2.py:193][0m |          -0.0137 |          14.5464 |           4.3231 |
[32m[20230117 13:34:00 @agent_ppo2.py:193][0m |          -0.0143 |          14.2685 |           4.3214 |
[32m[20230117 13:34:00 @agent_ppo2.py:138][0m Policy update time: 0.79 s
[32m[20230117 13:34:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.24
[32m[20230117 13:34:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.94
[32m[20230117 13:34:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.09
[32m[20230117 13:34:00 @agent_ppo2.py:151][0m Total time:       7.58 min
[32m[20230117 13:34:00 @agent_ppo2.py:153][0m 677888 total steps have happened
[32m[20230117 13:34:00 @agent_ppo2.py:129][0m #------------------------ Iteration 331 --------------------------#
[32m[20230117 13:34:00 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:34:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:00 @agent_ppo2.py:193][0m |          -0.0022 |          17.0172 |           4.5407 |
[32m[20230117 13:34:00 @agent_ppo2.py:193][0m |          -0.0071 |          15.6844 |           4.5362 |
[32m[20230117 13:34:01 @agent_ppo2.py:193][0m |          -0.0069 |          15.0216 |           4.5358 |
[32m[20230117 13:34:01 @agent_ppo2.py:193][0m |          -0.0093 |          14.4005 |           4.5299 |
[32m[20230117 13:34:01 @agent_ppo2.py:193][0m |          -0.0101 |          13.9166 |           4.5340 |
[32m[20230117 13:34:01 @agent_ppo2.py:193][0m |          -0.0102 |          13.5972 |           4.5302 |
[32m[20230117 13:34:01 @agent_ppo2.py:193][0m |          -0.0112 |          13.0865 |           4.5283 |
[32m[20230117 13:34:01 @agent_ppo2.py:193][0m |          -0.0125 |          12.6173 |           4.5302 |
[32m[20230117 13:34:01 @agent_ppo2.py:193][0m |          -0.0130 |          12.2737 |           4.5260 |
[32m[20230117 13:34:01 @agent_ppo2.py:193][0m |          -0.0131 |          11.9427 |           4.5273 |
[32m[20230117 13:34:01 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:34:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.13
[32m[20230117 13:34:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.40
[32m[20230117 13:34:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.45
[32m[20230117 13:34:01 @agent_ppo2.py:151][0m Total time:       7.60 min
[32m[20230117 13:34:01 @agent_ppo2.py:153][0m 679936 total steps have happened
[32m[20230117 13:34:01 @agent_ppo2.py:129][0m #------------------------ Iteration 332 --------------------------#
[32m[20230117 13:34:01 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:34:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:02 @agent_ppo2.py:193][0m |           0.0008 |          17.4660 |           4.4539 |
[32m[20230117 13:34:02 @agent_ppo2.py:193][0m |          -0.0061 |          15.0333 |           4.4472 |
[32m[20230117 13:34:02 @agent_ppo2.py:193][0m |          -0.0074 |          14.1668 |           4.4450 |
[32m[20230117 13:34:02 @agent_ppo2.py:193][0m |          -0.0093 |          13.4231 |           4.4463 |
[32m[20230117 13:34:02 @agent_ppo2.py:193][0m |          -0.0103 |          12.6090 |           4.4464 |
[32m[20230117 13:34:02 @agent_ppo2.py:193][0m |          -0.0113 |          11.9840 |           4.4474 |
[32m[20230117 13:34:02 @agent_ppo2.py:193][0m |          -0.0118 |          11.6005 |           4.4452 |
[32m[20230117 13:34:02 @agent_ppo2.py:193][0m |          -0.0132 |          11.2117 |           4.4464 |
[32m[20230117 13:34:02 @agent_ppo2.py:193][0m |          -0.0129 |          10.9479 |           4.4432 |
[32m[20230117 13:34:02 @agent_ppo2.py:193][0m |          -0.0137 |          10.6269 |           4.4425 |
[32m[20230117 13:34:02 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:34:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.81
[32m[20230117 13:34:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.44
[32m[20230117 13:34:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 142.26
[32m[20230117 13:34:02 @agent_ppo2.py:151][0m Total time:       7.62 min
[32m[20230117 13:34:02 @agent_ppo2.py:153][0m 681984 total steps have happened
[32m[20230117 13:34:02 @agent_ppo2.py:129][0m #------------------------ Iteration 333 --------------------------#
[32m[20230117 13:34:03 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:03 @agent_ppo2.py:193][0m |          -0.0041 |          17.1219 |           4.3980 |
[32m[20230117 13:34:03 @agent_ppo2.py:193][0m |          -0.0033 |          13.8334 |           4.3931 |
[32m[20230117 13:34:03 @agent_ppo2.py:193][0m |          -0.0061 |          11.0455 |           4.3844 |
[32m[20230117 13:34:03 @agent_ppo2.py:193][0m |          -0.0089 |          10.0221 |           4.3815 |
[32m[20230117 13:34:03 @agent_ppo2.py:193][0m |          -0.0046 |           9.5449 |           4.3836 |
[32m[20230117 13:34:03 @agent_ppo2.py:193][0m |          -0.0108 |           9.0837 |           4.3810 |
[32m[20230117 13:34:03 @agent_ppo2.py:193][0m |          -0.0113 |           8.8012 |           4.3754 |
[32m[20230117 13:34:03 @agent_ppo2.py:193][0m |          -0.0127 |           8.7908 |           4.3783 |
[32m[20230117 13:34:03 @agent_ppo2.py:193][0m |          -0.0143 |           8.4478 |           4.3779 |
[32m[20230117 13:34:03 @agent_ppo2.py:193][0m |          -0.0028 |           8.1602 |           4.3772 |
[32m[20230117 13:34:03 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:34:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.15
[32m[20230117 13:34:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.47
[32m[20230117 13:34:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.01
[32m[20230117 13:34:04 @agent_ppo2.py:151][0m Total time:       7.64 min
[32m[20230117 13:34:04 @agent_ppo2.py:153][0m 684032 total steps have happened
[32m[20230117 13:34:04 @agent_ppo2.py:129][0m #------------------------ Iteration 334 --------------------------#
[32m[20230117 13:34:04 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:34:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:04 @agent_ppo2.py:193][0m |           0.0011 |          20.4774 |           4.3880 |
[32m[20230117 13:34:04 @agent_ppo2.py:193][0m |          -0.0018 |          18.0045 |           4.3864 |
[32m[20230117 13:34:04 @agent_ppo2.py:193][0m |          -0.0046 |          17.1755 |           4.3877 |
[32m[20230117 13:34:04 @agent_ppo2.py:193][0m |          -0.0062 |          16.5807 |           4.3860 |
[32m[20230117 13:34:04 @agent_ppo2.py:193][0m |          -0.0069 |          15.9341 |           4.3829 |
[32m[20230117 13:34:04 @agent_ppo2.py:193][0m |          -0.0079 |          15.6025 |           4.3870 |
[32m[20230117 13:34:04 @agent_ppo2.py:193][0m |          -0.0072 |          15.4996 |           4.3843 |
[32m[20230117 13:34:05 @agent_ppo2.py:193][0m |          -0.0109 |          14.9385 |           4.3861 |
[32m[20230117 13:34:05 @agent_ppo2.py:193][0m |          -0.0117 |          14.5394 |           4.3883 |
[32m[20230117 13:34:05 @agent_ppo2.py:193][0m |          -0.0130 |          14.3008 |           4.3892 |
[32m[20230117 13:34:05 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:34:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.14
[32m[20230117 13:34:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.62
[32m[20230117 13:34:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.67
[32m[20230117 13:34:05 @agent_ppo2.py:151][0m Total time:       7.66 min
[32m[20230117 13:34:05 @agent_ppo2.py:153][0m 686080 total steps have happened
[32m[20230117 13:34:05 @agent_ppo2.py:129][0m #------------------------ Iteration 335 --------------------------#
[32m[20230117 13:34:05 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:34:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:05 @agent_ppo2.py:193][0m |           0.0002 |          20.0072 |           4.5649 |
[32m[20230117 13:34:05 @agent_ppo2.py:193][0m |          -0.0052 |          17.6999 |           4.5618 |
[32m[20230117 13:34:05 @agent_ppo2.py:193][0m |          -0.0055 |          16.5438 |           4.5570 |
[32m[20230117 13:34:05 @agent_ppo2.py:193][0m |          -0.0091 |          15.9080 |           4.5548 |
[32m[20230117 13:34:06 @agent_ppo2.py:193][0m |          -0.0092 |          15.4256 |           4.5551 |
[32m[20230117 13:34:06 @agent_ppo2.py:193][0m |          -0.0120 |          14.9865 |           4.5535 |
[32m[20230117 13:34:06 @agent_ppo2.py:193][0m |          -0.0142 |          14.7257 |           4.5527 |
[32m[20230117 13:34:06 @agent_ppo2.py:193][0m |          -0.0134 |          14.4311 |           4.5494 |
[32m[20230117 13:34:06 @agent_ppo2.py:193][0m |          -0.0128 |          14.1824 |           4.5497 |
[32m[20230117 13:34:06 @agent_ppo2.py:193][0m |          -0.0115 |          13.9274 |           4.5478 |
[32m[20230117 13:34:06 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:34:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.62
[32m[20230117 13:34:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.53
[32m[20230117 13:34:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 91.57
[32m[20230117 13:34:06 @agent_ppo2.py:151][0m Total time:       7.68 min
[32m[20230117 13:34:06 @agent_ppo2.py:153][0m 688128 total steps have happened
[32m[20230117 13:34:06 @agent_ppo2.py:129][0m #------------------------ Iteration 336 --------------------------#
[32m[20230117 13:34:06 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:06 @agent_ppo2.py:193][0m |          -0.0008 |          19.5037 |           4.4923 |
[32m[20230117 13:34:06 @agent_ppo2.py:193][0m |          -0.0050 |          17.4126 |           4.4848 |
[32m[20230117 13:34:06 @agent_ppo2.py:193][0m |          -0.0065 |          16.6456 |           4.4852 |
[32m[20230117 13:34:07 @agent_ppo2.py:193][0m |          -0.0077 |          16.1340 |           4.4793 |
[32m[20230117 13:34:07 @agent_ppo2.py:193][0m |          -0.0080 |          15.7361 |           4.4813 |
[32m[20230117 13:34:07 @agent_ppo2.py:193][0m |          -0.0094 |          15.3553 |           4.4759 |
[32m[20230117 13:34:07 @agent_ppo2.py:193][0m |          -0.0096 |          15.1341 |           4.4741 |
[32m[20230117 13:34:07 @agent_ppo2.py:193][0m |          -0.0101 |          14.8632 |           4.4701 |
[32m[20230117 13:34:07 @agent_ppo2.py:193][0m |          -0.0107 |          14.7468 |           4.4723 |
[32m[20230117 13:34:07 @agent_ppo2.py:193][0m |          -0.0118 |          14.5776 |           4.4741 |
[32m[20230117 13:34:07 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:34:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.55
[32m[20230117 13:34:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.89
[32m[20230117 13:34:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.00
[32m[20230117 13:34:07 @agent_ppo2.py:151][0m Total time:       7.70 min
[32m[20230117 13:34:07 @agent_ppo2.py:153][0m 690176 total steps have happened
[32m[20230117 13:34:07 @agent_ppo2.py:129][0m #------------------------ Iteration 337 --------------------------#
[32m[20230117 13:34:07 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:34:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:08 @agent_ppo2.py:193][0m |           0.0001 |          18.8554 |           4.3636 |
[32m[20230117 13:34:08 @agent_ppo2.py:193][0m |          -0.0061 |          17.0641 |           4.3624 |
[32m[20230117 13:34:08 @agent_ppo2.py:193][0m |          -0.0093 |          16.2786 |           4.3556 |
[32m[20230117 13:34:08 @agent_ppo2.py:193][0m |          -0.0097 |          15.6383 |           4.3515 |
[32m[20230117 13:34:08 @agent_ppo2.py:193][0m |          -0.0084 |          15.5147 |           4.3546 |
[32m[20230117 13:34:08 @agent_ppo2.py:193][0m |          -0.0075 |          14.7965 |           4.3495 |
[32m[20230117 13:34:08 @agent_ppo2.py:193][0m |          -0.0101 |          14.1601 |           4.3463 |
[32m[20230117 13:34:08 @agent_ppo2.py:193][0m |          -0.0109 |          13.6595 |           4.3476 |
[32m[20230117 13:34:08 @agent_ppo2.py:193][0m |          -0.0112 |          13.5071 |           4.3454 |
[32m[20230117 13:34:08 @agent_ppo2.py:193][0m |          -0.0134 |          12.7662 |           4.3476 |
[32m[20230117 13:34:08 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:34:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.64
[32m[20230117 13:34:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.27
[32m[20230117 13:34:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.08
[32m[20230117 13:34:08 @agent_ppo2.py:151][0m Total time:       7.72 min
[32m[20230117 13:34:08 @agent_ppo2.py:153][0m 692224 total steps have happened
[32m[20230117 13:34:08 @agent_ppo2.py:129][0m #------------------------ Iteration 338 --------------------------#
[32m[20230117 13:34:09 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:34:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:09 @agent_ppo2.py:193][0m |          -0.0011 |          17.4755 |           4.2906 |
[32m[20230117 13:34:09 @agent_ppo2.py:193][0m |          -0.0059 |          16.8337 |           4.2837 |
[32m[20230117 13:34:09 @agent_ppo2.py:193][0m |          -0.0079 |          16.3793 |           4.2778 |
[32m[20230117 13:34:09 @agent_ppo2.py:193][0m |          -0.0080 |          16.2028 |           4.2781 |
[32m[20230117 13:34:09 @agent_ppo2.py:193][0m |          -0.0103 |          15.8293 |           4.2786 |
[32m[20230117 13:34:09 @agent_ppo2.py:193][0m |          -0.0079 |          15.6100 |           4.2758 |
[32m[20230117 13:34:09 @agent_ppo2.py:193][0m |          -0.0101 |          15.4339 |           4.2745 |
[32m[20230117 13:34:09 @agent_ppo2.py:193][0m |          -0.0120 |          15.3107 |           4.2759 |
[32m[20230117 13:34:09 @agent_ppo2.py:193][0m |          -0.0018 |          15.6907 |           4.2692 |
[32m[20230117 13:34:09 @agent_ppo2.py:193][0m |          -0.0134 |          15.0625 |           4.2759 |
[32m[20230117 13:34:09 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:34:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.27
[32m[20230117 13:34:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.71
[32m[20230117 13:34:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 118.57
[32m[20230117 13:34:09 @agent_ppo2.py:151][0m Total time:       7.74 min
[32m[20230117 13:34:09 @agent_ppo2.py:153][0m 694272 total steps have happened
[32m[20230117 13:34:09 @agent_ppo2.py:129][0m #------------------------ Iteration 339 --------------------------#
[32m[20230117 13:34:10 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:10 @agent_ppo2.py:193][0m |          -0.0002 |          17.4365 |           4.4124 |
[32m[20230117 13:34:10 @agent_ppo2.py:193][0m |          -0.0043 |          15.9133 |           4.4053 |
[32m[20230117 13:34:10 @agent_ppo2.py:193][0m |          -0.0056 |          15.3015 |           4.4062 |
[32m[20230117 13:34:10 @agent_ppo2.py:193][0m |          -0.0063 |          14.8589 |           4.4054 |
[32m[20230117 13:34:10 @agent_ppo2.py:193][0m |          -0.0073 |          14.4892 |           4.4033 |
[32m[20230117 13:34:10 @agent_ppo2.py:193][0m |          -0.0091 |          14.0689 |           4.4066 |
[32m[20230117 13:34:10 @agent_ppo2.py:193][0m |          -0.0090 |          13.9102 |           4.4016 |
[32m[20230117 13:34:10 @agent_ppo2.py:193][0m |          -0.0103 |          13.6316 |           4.4039 |
[32m[20230117 13:34:10 @agent_ppo2.py:193][0m |          -0.0107 |          13.4231 |           4.4024 |
[32m[20230117 13:34:10 @agent_ppo2.py:193][0m |          -0.0111 |          13.2630 |           4.4076 |
[32m[20230117 13:34:10 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:34:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.47
[32m[20230117 13:34:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.76
[32m[20230117 13:34:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 128.14
[32m[20230117 13:34:11 @agent_ppo2.py:151][0m Total time:       7.76 min
[32m[20230117 13:34:11 @agent_ppo2.py:153][0m 696320 total steps have happened
[32m[20230117 13:34:11 @agent_ppo2.py:129][0m #------------------------ Iteration 340 --------------------------#
[32m[20230117 13:34:11 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:11 @agent_ppo2.py:193][0m |           0.0104 |          23.9944 |           4.4328 |
[32m[20230117 13:34:11 @agent_ppo2.py:193][0m |          -0.0040 |          17.7794 |           4.4279 |
[32m[20230117 13:34:11 @agent_ppo2.py:193][0m |          -0.0099 |          16.4814 |           4.4238 |
[32m[20230117 13:34:11 @agent_ppo2.py:193][0m |          -0.0141 |          15.8013 |           4.4210 |
[32m[20230117 13:34:11 @agent_ppo2.py:193][0m |          -0.0132 |          15.2729 |           4.4173 |
[32m[20230117 13:34:11 @agent_ppo2.py:193][0m |          -0.0150 |          14.8186 |           4.4195 |
[32m[20230117 13:34:11 @agent_ppo2.py:193][0m |          -0.0151 |          14.4343 |           4.4165 |
[32m[20230117 13:34:11 @agent_ppo2.py:193][0m |          -0.0165 |          14.1393 |           4.4148 |
[32m[20230117 13:34:11 @agent_ppo2.py:193][0m |          -0.0130 |          13.8132 |           4.4130 |
[32m[20230117 13:34:11 @agent_ppo2.py:193][0m |          -0.0047 |          14.0285 |           4.4122 |
[32m[20230117 13:34:11 @agent_ppo2.py:138][0m Policy update time: 0.68 s
[32m[20230117 13:34:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 204.25
[32m[20230117 13:34:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.26
[32m[20230117 13:34:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 265.07
[32m[20230117 13:34:12 @agent_ppo2.py:151][0m Total time:       7.78 min
[32m[20230117 13:34:12 @agent_ppo2.py:153][0m 698368 total steps have happened
[32m[20230117 13:34:12 @agent_ppo2.py:129][0m #------------------------ Iteration 341 --------------------------#
[32m[20230117 13:34:12 @agent_ppo2.py:135][0m Sampling time: 0.29 s by 4 slaves
[32m[20230117 13:34:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:12 @agent_ppo2.py:193][0m |           0.0016 |          27.4140 |           4.3873 |
[32m[20230117 13:34:12 @agent_ppo2.py:193][0m |          -0.0037 |          22.6221 |           4.3786 |
[32m[20230117 13:34:12 @agent_ppo2.py:193][0m |          -0.0071 |          20.8924 |           4.3830 |
[32m[20230117 13:34:12 @agent_ppo2.py:193][0m |          -0.0071 |          19.9463 |           4.3803 |
[32m[20230117 13:34:12 @agent_ppo2.py:193][0m |          -0.0086 |          19.2821 |           4.3830 |
[32m[20230117 13:34:12 @agent_ppo2.py:193][0m |          -0.0091 |          18.9180 |           4.3790 |
[32m[20230117 13:34:13 @agent_ppo2.py:193][0m |          -0.0093 |          18.6626 |           4.3791 |
[32m[20230117 13:34:13 @agent_ppo2.py:193][0m |          -0.0092 |          18.3001 |           4.3790 |
[32m[20230117 13:34:13 @agent_ppo2.py:193][0m |          -0.0118 |          17.9074 |           4.3788 |
[32m[20230117 13:34:13 @agent_ppo2.py:193][0m |          -0.0118 |          17.6689 |           4.3768 |
[32m[20230117 13:34:13 @agent_ppo2.py:138][0m Policy update time: 0.80 s
[32m[20230117 13:34:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 203.68
[32m[20230117 13:34:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.03
[32m[20230117 13:34:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.75
[32m[20230117 13:34:13 @agent_ppo2.py:151][0m Total time:       7.80 min
[32m[20230117 13:34:13 @agent_ppo2.py:153][0m 700416 total steps have happened
[32m[20230117 13:34:13 @agent_ppo2.py:129][0m #------------------------ Iteration 342 --------------------------#
[32m[20230117 13:34:13 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:34:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:13 @agent_ppo2.py:193][0m |          -0.0002 |          33.3390 |           4.4124 |
[32m[20230117 13:34:13 @agent_ppo2.py:193][0m |          -0.0054 |          27.7091 |           4.4084 |
[32m[20230117 13:34:14 @agent_ppo2.py:193][0m |          -0.0076 |          25.9831 |           4.4049 |
[32m[20230117 13:34:14 @agent_ppo2.py:193][0m |          -0.0084 |          24.8229 |           4.4015 |
[32m[20230117 13:34:14 @agent_ppo2.py:193][0m |          -0.0093 |          23.9836 |           4.4007 |
[32m[20230117 13:34:14 @agent_ppo2.py:193][0m |          -0.0105 |          23.3434 |           4.3988 |
[32m[20230117 13:34:14 @agent_ppo2.py:193][0m |          -0.0113 |          22.8416 |           4.3927 |
[32m[20230117 13:34:14 @agent_ppo2.py:193][0m |          -0.0111 |          22.2711 |           4.3906 |
[32m[20230117 13:34:14 @agent_ppo2.py:193][0m |          -0.0124 |          21.9471 |           4.3857 |
[32m[20230117 13:34:14 @agent_ppo2.py:193][0m |          -0.0132 |          21.5314 |           4.3854 |
[32m[20230117 13:34:14 @agent_ppo2.py:138][0m Policy update time: 0.82 s
[32m[20230117 13:34:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 204.61
[32m[20230117 13:34:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.55
[32m[20230117 13:34:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.81
[32m[20230117 13:34:14 @agent_ppo2.py:151][0m Total time:       7.82 min
[32m[20230117 13:34:14 @agent_ppo2.py:153][0m 702464 total steps have happened
[32m[20230117 13:34:14 @agent_ppo2.py:129][0m #------------------------ Iteration 343 --------------------------#
[32m[20230117 13:34:14 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:15 @agent_ppo2.py:193][0m |          -0.0074 |          19.0053 |           4.3348 |
[32m[20230117 13:34:15 @agent_ppo2.py:193][0m |          -0.0019 |          17.5046 |           4.3284 |
[32m[20230117 13:34:15 @agent_ppo2.py:193][0m |          -0.0137 |          17.0691 |           4.3200 |
[32m[20230117 13:34:15 @agent_ppo2.py:193][0m |          -0.0161 |          16.9421 |           4.3243 |
[32m[20230117 13:34:15 @agent_ppo2.py:193][0m |          -0.0081 |          16.5928 |           4.3207 |
[32m[20230117 13:34:15 @agent_ppo2.py:193][0m |           0.0012 |          16.5116 |           4.3290 |
[32m[20230117 13:34:15 @agent_ppo2.py:193][0m |          -0.0162 |          16.2992 |           4.3243 |
[32m[20230117 13:34:15 @agent_ppo2.py:193][0m |          -0.0154 |          16.2290 |           4.3246 |
[32m[20230117 13:34:15 @agent_ppo2.py:193][0m |          -0.0083 |          16.0140 |           4.3282 |
[32m[20230117 13:34:15 @agent_ppo2.py:193][0m |          -0.0166 |          15.9987 |           4.3251 |
[32m[20230117 13:34:15 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:34:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.39
[32m[20230117 13:34:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.80
[32m[20230117 13:34:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.30
[32m[20230117 13:34:15 @agent_ppo2.py:151][0m Total time:       7.84 min
[32m[20230117 13:34:15 @agent_ppo2.py:153][0m 704512 total steps have happened
[32m[20230117 13:34:15 @agent_ppo2.py:129][0m #------------------------ Iteration 344 --------------------------#
[32m[20230117 13:34:16 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:34:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:16 @agent_ppo2.py:193][0m |          -0.0021 |          21.9514 |           4.2698 |
[32m[20230117 13:34:16 @agent_ppo2.py:193][0m |          -0.0069 |          17.1152 |           4.2680 |
[32m[20230117 13:34:16 @agent_ppo2.py:193][0m |          -0.0103 |          16.5832 |           4.2672 |
[32m[20230117 13:34:16 @agent_ppo2.py:193][0m |          -0.0121 |          16.2584 |           4.2650 |
[32m[20230117 13:34:16 @agent_ppo2.py:193][0m |          -0.0112 |          16.0851 |           4.2653 |
[32m[20230117 13:34:16 @agent_ppo2.py:193][0m |          -0.0134 |          15.8477 |           4.2629 |
[32m[20230117 13:34:16 @agent_ppo2.py:193][0m |          -0.0157 |          15.7063 |           4.2665 |
[32m[20230117 13:34:16 @agent_ppo2.py:193][0m |          -0.0158 |          15.5970 |           4.2648 |
[32m[20230117 13:34:16 @agent_ppo2.py:193][0m |          -0.0154 |          15.4758 |           4.2636 |
[32m[20230117 13:34:16 @agent_ppo2.py:193][0m |          -0.0156 |          15.3729 |           4.2637 |
[32m[20230117 13:34:16 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:34:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 186.10
[32m[20230117 13:34:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.89
[32m[20230117 13:34:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.76
[32m[20230117 13:34:17 @agent_ppo2.py:151][0m Total time:       7.86 min
[32m[20230117 13:34:17 @agent_ppo2.py:153][0m 706560 total steps have happened
[32m[20230117 13:34:17 @agent_ppo2.py:129][0m #------------------------ Iteration 345 --------------------------#
[32m[20230117 13:34:17 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:34:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:17 @agent_ppo2.py:193][0m |          -0.0013 |          16.6240 |           4.3588 |
[32m[20230117 13:34:17 @agent_ppo2.py:193][0m |           0.0003 |          15.4475 |           4.3435 |
[32m[20230117 13:34:17 @agent_ppo2.py:193][0m |          -0.0064 |          14.0272 |           4.3437 |
[32m[20230117 13:34:17 @agent_ppo2.py:193][0m |          -0.0090 |          13.4755 |           4.3374 |
[32m[20230117 13:34:17 @agent_ppo2.py:193][0m |          -0.0103 |          13.1240 |           4.3330 |
[32m[20230117 13:34:17 @agent_ppo2.py:193][0m |          -0.0108 |          12.8851 |           4.3325 |
[32m[20230117 13:34:17 @agent_ppo2.py:193][0m |          -0.0126 |          12.5854 |           4.3316 |
[32m[20230117 13:34:17 @agent_ppo2.py:193][0m |          -0.0111 |          12.3581 |           4.3307 |
[32m[20230117 13:34:18 @agent_ppo2.py:193][0m |          -0.0084 |          12.1167 |           4.3254 |
[32m[20230117 13:34:18 @agent_ppo2.py:193][0m |          -0.0116 |          11.7777 |           4.3265 |
[32m[20230117 13:34:18 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:34:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.53
[32m[20230117 13:34:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.51
[32m[20230117 13:34:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.36
[32m[20230117 13:34:18 @agent_ppo2.py:151][0m Total time:       7.88 min
[32m[20230117 13:34:18 @agent_ppo2.py:153][0m 708608 total steps have happened
[32m[20230117 13:34:18 @agent_ppo2.py:129][0m #------------------------ Iteration 346 --------------------------#
[32m[20230117 13:34:18 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:18 @agent_ppo2.py:193][0m |           0.0009 |          27.8071 |           4.2083 |
[32m[20230117 13:34:18 @agent_ppo2.py:193][0m |          -0.0043 |          19.1981 |           4.1986 |
[32m[20230117 13:34:18 @agent_ppo2.py:193][0m |          -0.0052 |          16.6801 |           4.1907 |
[32m[20230117 13:34:18 @agent_ppo2.py:193][0m |          -0.0085 |          15.6782 |           4.1916 |
[32m[20230117 13:34:18 @agent_ppo2.py:193][0m |          -0.0081 |          15.0546 |           4.1903 |
[32m[20230117 13:34:18 @agent_ppo2.py:193][0m |          -0.0110 |          14.4437 |           4.1851 |
[32m[20230117 13:34:19 @agent_ppo2.py:193][0m |          -0.0110 |          14.1683 |           4.1848 |
[32m[20230117 13:34:19 @agent_ppo2.py:193][0m |          -0.0130 |          13.7026 |           4.1805 |
[32m[20230117 13:34:19 @agent_ppo2.py:193][0m |          -0.0137 |          13.3005 |           4.1855 |
[32m[20230117 13:34:19 @agent_ppo2.py:193][0m |          -0.0135 |          13.1123 |           4.1831 |
[32m[20230117 13:34:19 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:34:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 217.86
[32m[20230117 13:34:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.14
[32m[20230117 13:34:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.82
[32m[20230117 13:34:19 @agent_ppo2.py:151][0m Total time:       7.90 min
[32m[20230117 13:34:19 @agent_ppo2.py:153][0m 710656 total steps have happened
[32m[20230117 13:34:19 @agent_ppo2.py:129][0m #------------------------ Iteration 347 --------------------------#
[32m[20230117 13:34:19 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:34:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:19 @agent_ppo2.py:193][0m |           0.0003 |          17.0319 |           4.3481 |
[32m[20230117 13:34:19 @agent_ppo2.py:193][0m |          -0.0046 |          14.5042 |           4.3394 |
[32m[20230117 13:34:19 @agent_ppo2.py:193][0m |          -0.0078 |          13.1962 |           4.3349 |
[32m[20230117 13:34:19 @agent_ppo2.py:193][0m |          -0.0091 |          12.2178 |           4.3328 |
[32m[20230117 13:34:20 @agent_ppo2.py:193][0m |          -0.0093 |          11.6806 |           4.3303 |
[32m[20230117 13:34:20 @agent_ppo2.py:193][0m |          -0.0103 |          11.1518 |           4.3278 |
[32m[20230117 13:34:20 @agent_ppo2.py:193][0m |          -0.0108 |          10.7353 |           4.3275 |
[32m[20230117 13:34:20 @agent_ppo2.py:193][0m |          -0.0098 |          10.4926 |           4.3264 |
[32m[20230117 13:34:20 @agent_ppo2.py:193][0m |          -0.0118 |          10.1038 |           4.3245 |
[32m[20230117 13:34:20 @agent_ppo2.py:193][0m |          -0.0119 |           9.8717 |           4.3226 |
[32m[20230117 13:34:20 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:34:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.73
[32m[20230117 13:34:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.44
[32m[20230117 13:34:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.82
[32m[20230117 13:34:20 @agent_ppo2.py:151][0m Total time:       7.92 min
[32m[20230117 13:34:20 @agent_ppo2.py:153][0m 712704 total steps have happened
[32m[20230117 13:34:20 @agent_ppo2.py:129][0m #------------------------ Iteration 348 --------------------------#
[32m[20230117 13:34:20 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:20 @agent_ppo2.py:193][0m |           0.0008 |          18.4444 |           4.3541 |
[32m[20230117 13:34:20 @agent_ppo2.py:193][0m |          -0.0037 |          15.4140 |           4.3470 |
[32m[20230117 13:34:21 @agent_ppo2.py:193][0m |          -0.0067 |          13.9951 |           4.3400 |
[32m[20230117 13:34:21 @agent_ppo2.py:193][0m |          -0.0072 |          13.1163 |           4.3393 |
[32m[20230117 13:34:21 @agent_ppo2.py:193][0m |          -0.0080 |          12.3715 |           4.3406 |
[32m[20230117 13:34:21 @agent_ppo2.py:193][0m |          -0.0085 |          11.8655 |           4.3407 |
[32m[20230117 13:34:21 @agent_ppo2.py:193][0m |          -0.0089 |          11.4167 |           4.3410 |
[32m[20230117 13:34:21 @agent_ppo2.py:193][0m |          -0.0096 |          11.0821 |           4.3400 |
[32m[20230117 13:34:21 @agent_ppo2.py:193][0m |          -0.0101 |          10.7057 |           4.3377 |
[32m[20230117 13:34:21 @agent_ppo2.py:193][0m |          -0.0104 |          10.4307 |           4.3416 |
[32m[20230117 13:34:21 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:34:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.29
[32m[20230117 13:34:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.88
[32m[20230117 13:34:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.97
[32m[20230117 13:34:21 @agent_ppo2.py:151][0m Total time:       7.94 min
[32m[20230117 13:34:21 @agent_ppo2.py:153][0m 714752 total steps have happened
[32m[20230117 13:34:21 @agent_ppo2.py:129][0m #------------------------ Iteration 349 --------------------------#
[32m[20230117 13:34:21 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:34:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:22 @agent_ppo2.py:193][0m |           0.0018 |          14.2513 |           4.2796 |
[32m[20230117 13:34:22 @agent_ppo2.py:193][0m |          -0.0086 |          12.0079 |           4.2783 |
[32m[20230117 13:34:22 @agent_ppo2.py:193][0m |          -0.0083 |          11.2466 |           4.2796 |
[32m[20230117 13:34:22 @agent_ppo2.py:193][0m |          -0.0063 |          11.0119 |           4.2785 |
[32m[20230117 13:34:22 @agent_ppo2.py:193][0m |          -0.0075 |          10.1459 |           4.2776 |
[32m[20230117 13:34:22 @agent_ppo2.py:193][0m |          -0.0113 |           9.5890 |           4.2795 |
[32m[20230117 13:34:22 @agent_ppo2.py:193][0m |          -0.0122 |           9.2417 |           4.2841 |
[32m[20230117 13:34:22 @agent_ppo2.py:193][0m |          -0.0108 |           8.9429 |           4.2804 |
[32m[20230117 13:34:22 @agent_ppo2.py:193][0m |          -0.0146 |           8.3750 |           4.2816 |
[32m[20230117 13:34:22 @agent_ppo2.py:193][0m |          -0.0160 |           8.0052 |           4.2802 |
[32m[20230117 13:34:22 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:34:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.99
[32m[20230117 13:34:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.85
[32m[20230117 13:34:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.50
[32m[20230117 13:34:22 @agent_ppo2.py:151][0m Total time:       7.96 min
[32m[20230117 13:34:22 @agent_ppo2.py:153][0m 716800 total steps have happened
[32m[20230117 13:34:22 @agent_ppo2.py:129][0m #------------------------ Iteration 350 --------------------------#
[32m[20230117 13:34:23 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:23 @agent_ppo2.py:193][0m |          -0.0005 |          19.0885 |           4.3471 |
[32m[20230117 13:34:23 @agent_ppo2.py:193][0m |          -0.0046 |          17.4585 |           4.3421 |
[32m[20230117 13:34:23 @agent_ppo2.py:193][0m |          -0.0094 |          17.0942 |           4.3418 |
[32m[20230117 13:34:23 @agent_ppo2.py:193][0m |          -0.0223 |          16.6492 |           4.3394 |
[32m[20230117 13:34:23 @agent_ppo2.py:193][0m |          -0.0083 |          16.3910 |           4.3290 |
[32m[20230117 13:34:23 @agent_ppo2.py:193][0m |          -0.0057 |          16.2967 |           4.3388 |
[32m[20230117 13:34:23 @agent_ppo2.py:193][0m |          -0.0155 |          15.9242 |           4.3349 |
[32m[20230117 13:34:23 @agent_ppo2.py:193][0m |          -0.0142 |          15.6759 |           4.3400 |
[32m[20230117 13:34:23 @agent_ppo2.py:193][0m |           0.0036 |          16.7436 |           4.3389 |
[32m[20230117 13:34:23 @agent_ppo2.py:193][0m |          -0.0075 |          15.2389 |           4.3403 |
[32m[20230117 13:34:23 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:34:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.10
[32m[20230117 13:34:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.08
[32m[20230117 13:34:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.85
[32m[20230117 13:34:24 @agent_ppo2.py:151][0m Total time:       7.98 min
[32m[20230117 13:34:24 @agent_ppo2.py:153][0m 718848 total steps have happened
[32m[20230117 13:34:24 @agent_ppo2.py:129][0m #------------------------ Iteration 351 --------------------------#
[32m[20230117 13:34:24 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:24 @agent_ppo2.py:193][0m |          -0.0002 |          16.8199 |           4.3085 |
[32m[20230117 13:34:24 @agent_ppo2.py:193][0m |          -0.0046 |          14.5938 |           4.3046 |
[32m[20230117 13:34:24 @agent_ppo2.py:193][0m |          -0.0055 |          13.7630 |           4.3048 |
[32m[20230117 13:34:24 @agent_ppo2.py:193][0m |          -0.0076 |          13.3278 |           4.3053 |
[32m[20230117 13:34:24 @agent_ppo2.py:193][0m |          -0.0083 |          12.9787 |           4.3067 |
[32m[20230117 13:34:24 @agent_ppo2.py:193][0m |          -0.0088 |          12.6433 |           4.3068 |
[32m[20230117 13:34:24 @agent_ppo2.py:193][0m |          -0.0099 |          12.4318 |           4.3076 |
[32m[20230117 13:34:24 @agent_ppo2.py:193][0m |          -0.0111 |          12.2707 |           4.3074 |
[32m[20230117 13:34:24 @agent_ppo2.py:193][0m |          -0.0106 |          12.0702 |           4.3084 |
[32m[20230117 13:34:25 @agent_ppo2.py:193][0m |          -0.0110 |          11.9028 |           4.3082 |
[32m[20230117 13:34:25 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:34:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.03
[32m[20230117 13:34:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.96
[32m[20230117 13:34:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.00
[32m[20230117 13:34:25 @agent_ppo2.py:151][0m Total time:       8.00 min
[32m[20230117 13:34:25 @agent_ppo2.py:153][0m 720896 total steps have happened
[32m[20230117 13:34:25 @agent_ppo2.py:129][0m #------------------------ Iteration 352 --------------------------#
[32m[20230117 13:34:25 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:34:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:25 @agent_ppo2.py:193][0m |          -0.0018 |          17.0781 |           4.2798 |
[32m[20230117 13:34:25 @agent_ppo2.py:193][0m |          -0.0058 |          16.2541 |           4.2750 |
[32m[20230117 13:34:25 @agent_ppo2.py:193][0m |          -0.0051 |          15.7213 |           4.2700 |
[32m[20230117 13:34:25 @agent_ppo2.py:193][0m |          -0.0076 |          15.3123 |           4.2681 |
[32m[20230117 13:34:25 @agent_ppo2.py:193][0m |          -0.0085 |          14.9835 |           4.2675 |
[32m[20230117 13:34:25 @agent_ppo2.py:193][0m |          -0.0108 |          14.6603 |           4.2697 |
[32m[20230117 13:34:25 @agent_ppo2.py:193][0m |          -0.0108 |          14.3795 |           4.2678 |
[32m[20230117 13:34:26 @agent_ppo2.py:193][0m |          -0.0095 |          14.0976 |           4.2649 |
[32m[20230117 13:34:26 @agent_ppo2.py:193][0m |          -0.0126 |          13.9285 |           4.2631 |
[32m[20230117 13:34:26 @agent_ppo2.py:193][0m |          -0.0085 |          14.2873 |           4.2614 |
[32m[20230117 13:34:26 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:34:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.77
[32m[20230117 13:34:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.13
[32m[20230117 13:34:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.91
[32m[20230117 13:34:26 @agent_ppo2.py:151][0m Total time:       8.01 min
[32m[20230117 13:34:26 @agent_ppo2.py:153][0m 722944 total steps have happened
[32m[20230117 13:34:26 @agent_ppo2.py:129][0m #------------------------ Iteration 353 --------------------------#
[32m[20230117 13:34:26 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:26 @agent_ppo2.py:193][0m |           0.0040 |          19.8764 |           4.3115 |
[32m[20230117 13:34:26 @agent_ppo2.py:193][0m |          -0.0027 |          18.2086 |           4.3059 |
[32m[20230117 13:34:26 @agent_ppo2.py:193][0m |          -0.0085 |          17.5204 |           4.3106 |
[32m[20230117 13:34:26 @agent_ppo2.py:193][0m |          -0.0092 |          17.0928 |           4.3058 |
[32m[20230117 13:34:27 @agent_ppo2.py:193][0m |          -0.0084 |          16.5741 |           4.3061 |
[32m[20230117 13:34:27 @agent_ppo2.py:193][0m |          -0.0106 |          16.2191 |           4.3013 |
[32m[20230117 13:34:27 @agent_ppo2.py:193][0m |          -0.0109 |          15.9931 |           4.3068 |
[32m[20230117 13:34:27 @agent_ppo2.py:193][0m |          -0.0104 |          15.7710 |           4.3051 |
[32m[20230117 13:34:27 @agent_ppo2.py:193][0m |          -0.0110 |          15.5798 |           4.3033 |
[32m[20230117 13:34:27 @agent_ppo2.py:193][0m |          -0.0113 |          15.4534 |           4.3023 |
[32m[20230117 13:34:27 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:34:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.61
[32m[20230117 13:34:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.46
[32m[20230117 13:34:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.04
[32m[20230117 13:34:27 @agent_ppo2.py:151][0m Total time:       8.03 min
[32m[20230117 13:34:27 @agent_ppo2.py:153][0m 724992 total steps have happened
[32m[20230117 13:34:27 @agent_ppo2.py:129][0m #------------------------ Iteration 354 --------------------------#
[32m[20230117 13:34:27 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:34:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:27 @agent_ppo2.py:193][0m |           0.0001 |          23.9968 |           4.3986 |
[32m[20230117 13:34:27 @agent_ppo2.py:193][0m |          -0.0070 |          18.1318 |           4.3975 |
[32m[20230117 13:34:28 @agent_ppo2.py:193][0m |          -0.0080 |          17.0718 |           4.3932 |
[32m[20230117 13:34:28 @agent_ppo2.py:193][0m |          -0.0089 |          16.4704 |           4.3895 |
[32m[20230117 13:34:28 @agent_ppo2.py:193][0m |          -0.0097 |          15.7101 |           4.3834 |
[32m[20230117 13:34:28 @agent_ppo2.py:193][0m |          -0.0112 |          15.2740 |           4.3820 |
[32m[20230117 13:34:28 @agent_ppo2.py:193][0m |          -0.0046 |          15.2127 |           4.3786 |
[32m[20230117 13:34:28 @agent_ppo2.py:193][0m |          -0.0154 |          14.7421 |           4.3692 |
[32m[20230117 13:34:28 @agent_ppo2.py:193][0m |          -0.0141 |          14.5229 |           4.3669 |
[32m[20230117 13:34:28 @agent_ppo2.py:193][0m |          -0.0113 |          14.2456 |           4.3657 |
[32m[20230117 13:34:28 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:34:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 188.77
[32m[20230117 13:34:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.25
[32m[20230117 13:34:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.57
[32m[20230117 13:34:28 @agent_ppo2.py:151][0m Total time:       8.05 min
[32m[20230117 13:34:28 @agent_ppo2.py:153][0m 727040 total steps have happened
[32m[20230117 13:34:28 @agent_ppo2.py:129][0m #------------------------ Iteration 355 --------------------------#
[32m[20230117 13:34:28 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:34:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:29 @agent_ppo2.py:193][0m |          -0.0005 |          22.0361 |           4.3130 |
[32m[20230117 13:34:29 @agent_ppo2.py:193][0m |          -0.0053 |          18.9963 |           4.3057 |
[32m[20230117 13:34:29 @agent_ppo2.py:193][0m |          -0.0104 |          18.0806 |           4.3065 |
[32m[20230117 13:34:29 @agent_ppo2.py:193][0m |          -0.0085 |          17.4818 |           4.3061 |
[32m[20230117 13:34:29 @agent_ppo2.py:193][0m |          -0.0139 |          17.2515 |           4.3078 |
[32m[20230117 13:34:29 @agent_ppo2.py:193][0m |          -0.0081 |          17.2559 |           4.3103 |
[32m[20230117 13:34:29 @agent_ppo2.py:193][0m |          -0.0136 |          16.4015 |           4.3083 |
[32m[20230117 13:34:29 @agent_ppo2.py:193][0m |          -0.0118 |          16.2460 |           4.3107 |
[32m[20230117 13:34:29 @agent_ppo2.py:193][0m |          -0.0169 |          15.8424 |           4.3138 |
[32m[20230117 13:34:29 @agent_ppo2.py:193][0m |          -0.0160 |          15.7747 |           4.3135 |
[32m[20230117 13:34:29 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:34:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 187.89
[32m[20230117 13:34:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.73
[32m[20230117 13:34:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.52
[32m[20230117 13:34:29 @agent_ppo2.py:151][0m Total time:       8.07 min
[32m[20230117 13:34:29 @agent_ppo2.py:153][0m 729088 total steps have happened
[32m[20230117 13:34:29 @agent_ppo2.py:129][0m #------------------------ Iteration 356 --------------------------#
[32m[20230117 13:34:30 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:30 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:30 @agent_ppo2.py:193][0m |           0.0017 |          19.9038 |           4.3854 |
[32m[20230117 13:34:30 @agent_ppo2.py:193][0m |          -0.0033 |          18.1446 |           4.3820 |
[32m[20230117 13:34:30 @agent_ppo2.py:193][0m |          -0.0070 |          17.5030 |           4.3811 |
[32m[20230117 13:34:30 @agent_ppo2.py:193][0m |          -0.0070 |          17.0919 |           4.3761 |
[32m[20230117 13:34:30 @agent_ppo2.py:193][0m |          -0.0087 |          16.7446 |           4.3733 |
[32m[20230117 13:34:30 @agent_ppo2.py:193][0m |          -0.0094 |          16.4172 |           4.3686 |
[32m[20230117 13:34:30 @agent_ppo2.py:193][0m |          -0.0100 |          16.2579 |           4.3741 |
[32m[20230117 13:34:30 @agent_ppo2.py:193][0m |          -0.0115 |          15.9939 |           4.3737 |
[32m[20230117 13:34:30 @agent_ppo2.py:193][0m |          -0.0115 |          15.8192 |           4.3703 |
[32m[20230117 13:34:30 @agent_ppo2.py:193][0m |          -0.0125 |          15.6519 |           4.3700 |
[32m[20230117 13:34:30 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:34:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.34
[32m[20230117 13:34:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.55
[32m[20230117 13:34:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 279.97
[32m[20230117 13:34:31 @agent_ppo2.py:151][0m Total time:       8.09 min
[32m[20230117 13:34:31 @agent_ppo2.py:153][0m 731136 total steps have happened
[32m[20230117 13:34:31 @agent_ppo2.py:129][0m #------------------------ Iteration 357 --------------------------#
[32m[20230117 13:34:31 @agent_ppo2.py:135][0m Sampling time: 0.29 s by 4 slaves
[32m[20230117 13:34:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:31 @agent_ppo2.py:193][0m |          -0.0011 |          27.0088 |           4.3775 |
[32m[20230117 13:34:31 @agent_ppo2.py:193][0m |          -0.0055 |          21.3776 |           4.3722 |
[32m[20230117 13:34:31 @agent_ppo2.py:193][0m |          -0.0065 |          19.6318 |           4.3655 |
[32m[20230117 13:34:31 @agent_ppo2.py:193][0m |          -0.0084 |          18.8634 |           4.3674 |
[32m[20230117 13:34:31 @agent_ppo2.py:193][0m |          -0.0091 |          18.2396 |           4.3609 |
[32m[20230117 13:34:31 @agent_ppo2.py:193][0m |          -0.0102 |          17.4697 |           4.3596 |
[32m[20230117 13:34:31 @agent_ppo2.py:193][0m |          -0.0105 |          17.0590 |           4.3583 |
[32m[20230117 13:34:32 @agent_ppo2.py:193][0m |          -0.0114 |          16.6687 |           4.3564 |
[32m[20230117 13:34:32 @agent_ppo2.py:193][0m |          -0.0120 |          16.1859 |           4.3534 |
[32m[20230117 13:34:32 @agent_ppo2.py:193][0m |          -0.0117 |          15.8986 |           4.3522 |
[32m[20230117 13:34:32 @agent_ppo2.py:138][0m Policy update time: 0.84 s
[32m[20230117 13:34:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 147.14
[32m[20230117 13:34:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.88
[32m[20230117 13:34:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.75
[32m[20230117 13:34:32 @agent_ppo2.py:151][0m Total time:       8.11 min
[32m[20230117 13:34:32 @agent_ppo2.py:153][0m 733184 total steps have happened
[32m[20230117 13:34:32 @agent_ppo2.py:129][0m #------------------------ Iteration 358 --------------------------#
[32m[20230117 13:34:32 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:32 @agent_ppo2.py:193][0m |          -0.0002 |          20.1004 |           4.3506 |
[32m[20230117 13:34:32 @agent_ppo2.py:193][0m |          -0.0024 |          18.7697 |           4.3484 |
[32m[20230117 13:34:32 @agent_ppo2.py:193][0m |          -0.0033 |          18.2404 |           4.3431 |
[32m[20230117 13:34:32 @agent_ppo2.py:193][0m |          -0.0052 |          17.8586 |           4.3409 |
[32m[20230117 13:34:32 @agent_ppo2.py:193][0m |          -0.0072 |          17.2975 |           4.3399 |
[32m[20230117 13:34:33 @agent_ppo2.py:193][0m |          -0.0096 |          16.8054 |           4.3415 |
[32m[20230117 13:34:33 @agent_ppo2.py:193][0m |          -0.0103 |          16.4383 |           4.3416 |
[32m[20230117 13:34:33 @agent_ppo2.py:193][0m |          -0.0096 |          16.3549 |           4.3409 |
[32m[20230117 13:34:33 @agent_ppo2.py:193][0m |          -0.0108 |          15.9687 |           4.3438 |
[32m[20230117 13:34:33 @agent_ppo2.py:193][0m |          -0.0116 |          15.7554 |           4.3412 |
[32m[20230117 13:34:33 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:34:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.26
[32m[20230117 13:34:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.06
[32m[20230117 13:34:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 155.91
[32m[20230117 13:34:33 @agent_ppo2.py:151][0m Total time:       8.13 min
[32m[20230117 13:34:33 @agent_ppo2.py:153][0m 735232 total steps have happened
[32m[20230117 13:34:33 @agent_ppo2.py:129][0m #------------------------ Iteration 359 --------------------------#
[32m[20230117 13:34:33 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:33 @agent_ppo2.py:193][0m |          -0.0022 |          19.3200 |           4.4079 |
[32m[20230117 13:34:33 @agent_ppo2.py:193][0m |          -0.0058 |          17.5289 |           4.4013 |
[32m[20230117 13:34:34 @agent_ppo2.py:193][0m |          -0.0061 |          17.0443 |           4.4006 |
[32m[20230117 13:34:34 @agent_ppo2.py:193][0m |          -0.0073 |          16.5246 |           4.4019 |
[32m[20230117 13:34:34 @agent_ppo2.py:193][0m |          -0.0085 |          16.2068 |           4.4020 |
[32m[20230117 13:34:34 @agent_ppo2.py:193][0m |          -0.0098 |          15.8791 |           4.4034 |
[32m[20230117 13:34:34 @agent_ppo2.py:193][0m |          -0.0099 |          15.5522 |           4.4024 |
[32m[20230117 13:34:34 @agent_ppo2.py:193][0m |          -0.0096 |          15.2989 |           4.4043 |
[32m[20230117 13:34:34 @agent_ppo2.py:193][0m |          -0.0115 |          14.8642 |           4.4006 |
[32m[20230117 13:34:34 @agent_ppo2.py:193][0m |          -0.0112 |          14.5740 |           4.4026 |
[32m[20230117 13:34:34 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:34:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.70
[32m[20230117 13:34:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.42
[32m[20230117 13:34:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 278.51
[32m[20230117 13:34:34 @agent_ppo2.py:151][0m Total time:       8.15 min
[32m[20230117 13:34:34 @agent_ppo2.py:153][0m 737280 total steps have happened
[32m[20230117 13:34:34 @agent_ppo2.py:129][0m #------------------------ Iteration 360 --------------------------#
[32m[20230117 13:34:34 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:34:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:35 @agent_ppo2.py:193][0m |           0.0028 |          18.5642 |           4.3335 |
[32m[20230117 13:34:35 @agent_ppo2.py:193][0m |          -0.0036 |          17.1367 |           4.3295 |
[32m[20230117 13:34:35 @agent_ppo2.py:193][0m |          -0.0033 |          16.7724 |           4.3280 |
[32m[20230117 13:34:35 @agent_ppo2.py:193][0m |          -0.0057 |          16.4442 |           4.3286 |
[32m[20230117 13:34:35 @agent_ppo2.py:193][0m |          -0.0067 |          16.0797 |           4.3281 |
[32m[20230117 13:34:35 @agent_ppo2.py:193][0m |          -0.0082 |          15.6890 |           4.3292 |
[32m[20230117 13:34:35 @agent_ppo2.py:193][0m |          -0.0086 |          15.5517 |           4.3291 |
[32m[20230117 13:34:35 @agent_ppo2.py:193][0m |          -0.0100 |          15.2399 |           4.3268 |
[32m[20230117 13:34:35 @agent_ppo2.py:193][0m |          -0.0096 |          15.1892 |           4.3308 |
[32m[20230117 13:34:35 @agent_ppo2.py:193][0m |          -0.0103 |          14.8792 |           4.3308 |
[32m[20230117 13:34:35 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:34:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.92
[32m[20230117 13:34:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.40
[32m[20230117 13:34:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 130.83
[32m[20230117 13:34:35 @agent_ppo2.py:151][0m Total time:       8.17 min
[32m[20230117 13:34:35 @agent_ppo2.py:153][0m 739328 total steps have happened
[32m[20230117 13:34:35 @agent_ppo2.py:129][0m #------------------------ Iteration 361 --------------------------#
[32m[20230117 13:34:36 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:36 @agent_ppo2.py:193][0m |          -0.0032 |          24.4875 |           4.4672 |
[32m[20230117 13:34:36 @agent_ppo2.py:193][0m |           0.0119 |          16.1469 |           4.4680 |
[32m[20230117 13:34:36 @agent_ppo2.py:193][0m |          -0.0035 |          15.2681 |           4.4639 |
[32m[20230117 13:34:36 @agent_ppo2.py:193][0m |          -0.0106 |          14.3671 |           4.4699 |
[32m[20230117 13:34:36 @agent_ppo2.py:193][0m |          -0.0114 |          14.0471 |           4.4741 |
[32m[20230117 13:34:36 @agent_ppo2.py:193][0m |          -0.0150 |          13.7194 |           4.4729 |
[32m[20230117 13:34:36 @agent_ppo2.py:193][0m |          -0.0133 |          13.4464 |           4.4748 |
[32m[20230117 13:34:36 @agent_ppo2.py:193][0m |          -0.0162 |          13.2425 |           4.4763 |
[32m[20230117 13:34:36 @agent_ppo2.py:193][0m |          -0.0168 |          13.0113 |           4.4769 |
[32m[20230117 13:34:36 @agent_ppo2.py:193][0m |          -0.0145 |          12.8960 |           4.4769 |
[32m[20230117 13:34:36 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:34:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 237.67
[32m[20230117 13:34:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.48
[32m[20230117 13:34:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 280.44
[32m[20230117 13:34:37 @agent_ppo2.py:151][0m Total time:       8.19 min
[32m[20230117 13:34:37 @agent_ppo2.py:153][0m 741376 total steps have happened
[32m[20230117 13:34:37 @agent_ppo2.py:129][0m #------------------------ Iteration 362 --------------------------#
[32m[20230117 13:34:37 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:34:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:37 @agent_ppo2.py:193][0m |          -0.0006 |          19.3368 |           4.4817 |
[32m[20230117 13:34:37 @agent_ppo2.py:193][0m |          -0.0069 |          18.2903 |           4.4767 |
[32m[20230117 13:34:37 @agent_ppo2.py:193][0m |          -0.0091 |          17.6453 |           4.4751 |
[32m[20230117 13:34:37 @agent_ppo2.py:193][0m |          -0.0103 |          17.3758 |           4.4772 |
[32m[20230117 13:34:37 @agent_ppo2.py:193][0m |          -0.0109 |          17.1134 |           4.4768 |
[32m[20230117 13:34:37 @agent_ppo2.py:193][0m |          -0.0122 |          16.9077 |           4.4718 |
[32m[20230117 13:34:37 @agent_ppo2.py:193][0m |          -0.0126 |          16.8281 |           4.4688 |
[32m[20230117 13:34:37 @agent_ppo2.py:193][0m |          -0.0138 |          16.5904 |           4.4722 |
[32m[20230117 13:34:37 @agent_ppo2.py:193][0m |          -0.0139 |          16.4779 |           4.4714 |
[32m[20230117 13:34:37 @agent_ppo2.py:193][0m |          -0.0147 |          16.3552 |           4.4700 |
[32m[20230117 13:34:37 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:34:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 267.55
[32m[20230117 13:34:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 270.77
[32m[20230117 13:34:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.73
[32m[20230117 13:34:38 @agent_ppo2.py:151][0m Total time:       8.21 min
[32m[20230117 13:34:38 @agent_ppo2.py:153][0m 743424 total steps have happened
[32m[20230117 13:34:38 @agent_ppo2.py:129][0m #------------------------ Iteration 363 --------------------------#
[32m[20230117 13:34:38 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:34:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:38 @agent_ppo2.py:193][0m |           0.0040 |          20.7338 |           4.3924 |
[32m[20230117 13:34:38 @agent_ppo2.py:193][0m |          -0.0042 |          15.7762 |           4.3812 |
[32m[20230117 13:34:38 @agent_ppo2.py:193][0m |          -0.0056 |          13.8227 |           4.3762 |
[32m[20230117 13:34:38 @agent_ppo2.py:193][0m |          -0.0073 |          12.6558 |           4.3705 |
[32m[20230117 13:34:38 @agent_ppo2.py:193][0m |          -0.0094 |          11.7098 |           4.3710 |
[32m[20230117 13:34:38 @agent_ppo2.py:193][0m |          -0.0116 |          10.7555 |           4.3706 |
[32m[20230117 13:34:38 @agent_ppo2.py:193][0m |          -0.0100 |           9.8410 |           4.3688 |
[32m[20230117 13:34:39 @agent_ppo2.py:193][0m |          -0.0122 |           9.2556 |           4.3668 |
[32m[20230117 13:34:39 @agent_ppo2.py:193][0m |          -0.0115 |           8.6352 |           4.3671 |
[32m[20230117 13:34:39 @agent_ppo2.py:193][0m |          -0.0127 |           8.2383 |           4.3693 |
[32m[20230117 13:34:39 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:34:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.39
[32m[20230117 13:34:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.42
[32m[20230117 13:34:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.39
[32m[20230117 13:34:39 @agent_ppo2.py:151][0m Total time:       8.23 min
[32m[20230117 13:34:39 @agent_ppo2.py:153][0m 745472 total steps have happened
[32m[20230117 13:34:39 @agent_ppo2.py:129][0m #------------------------ Iteration 364 --------------------------#
[32m[20230117 13:34:39 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:39 @agent_ppo2.py:193][0m |          -0.0037 |          43.3887 |           4.4449 |
[32m[20230117 13:34:39 @agent_ppo2.py:193][0m |          -0.0049 |          22.7886 |           4.4417 |
[32m[20230117 13:34:39 @agent_ppo2.py:193][0m |          -0.0121 |          18.4000 |           4.4448 |
[32m[20230117 13:34:39 @agent_ppo2.py:193][0m |          -0.0140 |          17.3748 |           4.4460 |
[32m[20230117 13:34:39 @agent_ppo2.py:193][0m |          -0.0129 |          16.7370 |           4.4457 |
[32m[20230117 13:34:39 @agent_ppo2.py:193][0m |          -0.0144 |          16.1302 |           4.4461 |
[32m[20230117 13:34:40 @agent_ppo2.py:193][0m |          -0.0149 |          15.6267 |           4.4469 |
[32m[20230117 13:34:40 @agent_ppo2.py:193][0m |          -0.0190 |          15.5520 |           4.4490 |
[32m[20230117 13:34:40 @agent_ppo2.py:193][0m |          -0.0169 |          15.1404 |           4.4458 |
[32m[20230117 13:34:40 @agent_ppo2.py:193][0m |          -0.0157 |          14.6074 |           4.4502 |
[32m[20230117 13:34:40 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230117 13:34:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 142.51
[32m[20230117 13:34:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.91
[32m[20230117 13:34:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 277.67
[32m[20230117 13:34:40 @agent_ppo2.py:151][0m Total time:       8.25 min
[32m[20230117 13:34:40 @agent_ppo2.py:153][0m 747520 total steps have happened
[32m[20230117 13:34:40 @agent_ppo2.py:129][0m #------------------------ Iteration 365 --------------------------#
[32m[20230117 13:34:40 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:34:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:40 @agent_ppo2.py:193][0m |          -0.0047 |          17.5591 |           4.3627 |
[32m[20230117 13:34:40 @agent_ppo2.py:193][0m |          -0.0049 |          14.6135 |           4.3581 |
[32m[20230117 13:34:40 @agent_ppo2.py:193][0m |          -0.0099 |          13.4530 |           4.3571 |
[32m[20230117 13:34:40 @agent_ppo2.py:193][0m |          -0.0129 |          12.9498 |           4.3553 |
[32m[20230117 13:34:41 @agent_ppo2.py:193][0m |          -0.0109 |          12.5906 |           4.3563 |
[32m[20230117 13:34:41 @agent_ppo2.py:193][0m |          -0.0077 |          12.1365 |           4.3489 |
[32m[20230117 13:34:41 @agent_ppo2.py:193][0m |          -0.0093 |          11.7347 |           4.3553 |
[32m[20230117 13:34:41 @agent_ppo2.py:193][0m |          -0.0080 |          11.5957 |           4.3511 |
[32m[20230117 13:34:41 @agent_ppo2.py:193][0m |          -0.0084 |          11.2198 |           4.3518 |
[32m[20230117 13:34:41 @agent_ppo2.py:193][0m |          -0.0119 |          11.0209 |           4.3535 |
[32m[20230117 13:34:41 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:34:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.19
[32m[20230117 13:34:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.76
[32m[20230117 13:34:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.66
[32m[20230117 13:34:41 @agent_ppo2.py:151][0m Total time:       8.27 min
[32m[20230117 13:34:41 @agent_ppo2.py:153][0m 749568 total steps have happened
[32m[20230117 13:34:41 @agent_ppo2.py:129][0m #------------------------ Iteration 366 --------------------------#
[32m[20230117 13:34:41 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:41 @agent_ppo2.py:193][0m |          -0.0001 |          40.7493 |           4.4789 |
[32m[20230117 13:34:41 @agent_ppo2.py:193][0m |          -0.0063 |          29.8961 |           4.4626 |
[32m[20230117 13:34:41 @agent_ppo2.py:193][0m |          -0.0081 |          26.6493 |           4.4672 |
[32m[20230117 13:34:42 @agent_ppo2.py:193][0m |          -0.0071 |          24.6767 |           4.4582 |
[32m[20230117 13:34:42 @agent_ppo2.py:193][0m |          -0.0094 |          22.5924 |           4.4634 |
[32m[20230117 13:34:42 @agent_ppo2.py:193][0m |          -0.0113 |          21.1106 |           4.4624 |
[32m[20230117 13:34:42 @agent_ppo2.py:193][0m |          -0.0122 |          20.0304 |           4.4614 |
[32m[20230117 13:34:42 @agent_ppo2.py:193][0m |          -0.0134 |          19.1902 |           4.4644 |
[32m[20230117 13:34:42 @agent_ppo2.py:193][0m |          -0.0111 |          18.5557 |           4.4624 |
[32m[20230117 13:34:42 @agent_ppo2.py:193][0m |          -0.0144 |          18.0237 |           4.4619 |
[32m[20230117 13:34:42 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230117 13:34:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 214.82
[32m[20230117 13:34:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.27
[32m[20230117 13:34:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.43
[32m[20230117 13:34:42 @agent_ppo2.py:151][0m Total time:       8.28 min
[32m[20230117 13:34:42 @agent_ppo2.py:153][0m 751616 total steps have happened
[32m[20230117 13:34:42 @agent_ppo2.py:129][0m #------------------------ Iteration 367 --------------------------#
[32m[20230117 13:34:42 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:42 @agent_ppo2.py:193][0m |           0.0044 |          21.3575 |           4.4070 |
[32m[20230117 13:34:42 @agent_ppo2.py:193][0m |          -0.0061 |          19.0015 |           4.3960 |
[32m[20230117 13:34:43 @agent_ppo2.py:193][0m |          -0.0044 |          18.8070 |           4.3934 |
[32m[20230117 13:34:43 @agent_ppo2.py:193][0m |          -0.0074 |          17.8876 |           4.3933 |
[32m[20230117 13:34:43 @agent_ppo2.py:193][0m |          -0.0099 |          17.4668 |           4.3961 |
[32m[20230117 13:34:43 @agent_ppo2.py:193][0m |          -0.0105 |          17.1222 |           4.3980 |
[32m[20230117 13:34:43 @agent_ppo2.py:193][0m |          -0.0131 |          16.8196 |           4.4005 |
[32m[20230117 13:34:43 @agent_ppo2.py:193][0m |          -0.0116 |          16.5425 |           4.3964 |
[32m[20230117 13:34:43 @agent_ppo2.py:193][0m |          -0.0119 |          16.3484 |           4.3969 |
[32m[20230117 13:34:43 @agent_ppo2.py:193][0m |          -0.0108 |          16.2766 |           4.3993 |
[32m[20230117 13:34:43 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:34:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.00
[32m[20230117 13:34:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.28
[32m[20230117 13:34:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.44
[32m[20230117 13:34:43 @agent_ppo2.py:151][0m Total time:       8.30 min
[32m[20230117 13:34:43 @agent_ppo2.py:153][0m 753664 total steps have happened
[32m[20230117 13:34:43 @agent_ppo2.py:129][0m #------------------------ Iteration 368 --------------------------#
[32m[20230117 13:34:43 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:34:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:44 @agent_ppo2.py:193][0m |           0.0004 |          25.1251 |           4.4710 |
[32m[20230117 13:34:44 @agent_ppo2.py:193][0m |          -0.0046 |          15.8961 |           4.4634 |
[32m[20230117 13:34:44 @agent_ppo2.py:193][0m |          -0.0056 |          13.9331 |           4.4635 |
[32m[20230117 13:34:44 @agent_ppo2.py:193][0m |          -0.0065 |          12.5575 |           4.4592 |
[32m[20230117 13:34:44 @agent_ppo2.py:193][0m |          -0.0094 |          11.3127 |           4.4587 |
[32m[20230117 13:34:44 @agent_ppo2.py:193][0m |          -0.0112 |          10.3313 |           4.4560 |
[32m[20230117 13:34:44 @agent_ppo2.py:193][0m |          -0.0111 |           9.5221 |           4.4539 |
[32m[20230117 13:34:44 @agent_ppo2.py:193][0m |          -0.0116 |           8.7230 |           4.4556 |
[32m[20230117 13:34:44 @agent_ppo2.py:193][0m |          -0.0123 |           8.0427 |           4.4537 |
[32m[20230117 13:34:44 @agent_ppo2.py:193][0m |          -0.0128 |           7.5917 |           4.4526 |
[32m[20230117 13:34:44 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230117 13:34:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 208.45
[32m[20230117 13:34:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.31
[32m[20230117 13:34:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 276.12
[32m[20230117 13:34:44 @agent_ppo2.py:151][0m Total time:       8.32 min
[32m[20230117 13:34:44 @agent_ppo2.py:153][0m 755712 total steps have happened
[32m[20230117 13:34:44 @agent_ppo2.py:129][0m #------------------------ Iteration 369 --------------------------#
[32m[20230117 13:34:45 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:34:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:45 @agent_ppo2.py:193][0m |          -0.0024 |          35.3726 |           4.4161 |
[32m[20230117 13:34:45 @agent_ppo2.py:193][0m |          -0.0052 |          26.3604 |           4.4102 |
[32m[20230117 13:34:45 @agent_ppo2.py:193][0m |          -0.0113 |          23.6598 |           4.4077 |
[32m[20230117 13:34:45 @agent_ppo2.py:193][0m |          -0.0127 |          22.2898 |           4.4041 |
[32m[20230117 13:34:45 @agent_ppo2.py:193][0m |          -0.0160 |          21.4873 |           4.4035 |
[32m[20230117 13:34:45 @agent_ppo2.py:193][0m |          -0.0130 |          20.8317 |           4.4041 |
[32m[20230117 13:34:45 @agent_ppo2.py:193][0m |          -0.0163 |          20.2385 |           4.3981 |
[32m[20230117 13:34:45 @agent_ppo2.py:193][0m |          -0.0132 |          19.9754 |           4.3974 |
[32m[20230117 13:34:45 @agent_ppo2.py:193][0m |          -0.0150 |          19.2880 |           4.3959 |
[32m[20230117 13:34:45 @agent_ppo2.py:193][0m |          -0.0135 |          19.1763 |           4.3957 |
[32m[20230117 13:34:45 @agent_ppo2.py:138][0m Policy update time: 0.79 s
[32m[20230117 13:34:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 205.75
[32m[20230117 13:34:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.51
[32m[20230117 13:34:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.44
[32m[20230117 13:34:46 @agent_ppo2.py:151][0m Total time:       8.34 min
[32m[20230117 13:34:46 @agent_ppo2.py:153][0m 757760 total steps have happened
[32m[20230117 13:34:46 @agent_ppo2.py:129][0m #------------------------ Iteration 370 --------------------------#
[32m[20230117 13:34:46 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:46 @agent_ppo2.py:193][0m |           0.0012 |          20.7750 |           4.5108 |
[32m[20230117 13:34:46 @agent_ppo2.py:193][0m |          -0.0022 |          19.2934 |           4.5124 |
[32m[20230117 13:34:46 @agent_ppo2.py:193][0m |          -0.0045 |          18.7332 |           4.5133 |
[32m[20230117 13:34:46 @agent_ppo2.py:193][0m |          -0.0062 |          18.3369 |           4.5130 |
[32m[20230117 13:34:46 @agent_ppo2.py:193][0m |          -0.0073 |          17.9398 |           4.5116 |
[32m[20230117 13:34:46 @agent_ppo2.py:193][0m |          -0.0084 |          17.6561 |           4.5131 |
[32m[20230117 13:34:46 @agent_ppo2.py:193][0m |          -0.0091 |          17.4128 |           4.5137 |
[32m[20230117 13:34:46 @agent_ppo2.py:193][0m |          -0.0101 |          17.1130 |           4.5116 |
[32m[20230117 13:34:46 @agent_ppo2.py:193][0m |          -0.0098 |          16.9237 |           4.5174 |
[32m[20230117 13:34:47 @agent_ppo2.py:193][0m |          -0.0115 |          16.7427 |           4.5183 |
[32m[20230117 13:34:47 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:34:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.05
[32m[20230117 13:34:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.18
[32m[20230117 13:34:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.72
[32m[20230117 13:34:47 @agent_ppo2.py:151][0m Total time:       8.36 min
[32m[20230117 13:34:47 @agent_ppo2.py:153][0m 759808 total steps have happened
[32m[20230117 13:34:47 @agent_ppo2.py:129][0m #------------------------ Iteration 371 --------------------------#
[32m[20230117 13:34:47 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:34:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:47 @agent_ppo2.py:193][0m |          -0.0054 |          19.5481 |           4.4071 |
[32m[20230117 13:34:47 @agent_ppo2.py:193][0m |          -0.0066 |          17.2481 |           4.3962 |
[32m[20230117 13:34:47 @agent_ppo2.py:193][0m |          -0.0145 |          16.3089 |           4.3893 |
[32m[20230117 13:34:47 @agent_ppo2.py:193][0m |           0.0005 |          16.0283 |           4.3866 |
[32m[20230117 13:34:47 @agent_ppo2.py:193][0m |          -0.0110 |          15.1506 |           4.3786 |
[32m[20230117 13:34:47 @agent_ppo2.py:193][0m |          -0.0152 |          14.7184 |           4.3846 |
[32m[20230117 13:34:47 @agent_ppo2.py:193][0m |          -0.0134 |          14.3936 |           4.3851 |
[32m[20230117 13:34:48 @agent_ppo2.py:193][0m |          -0.0160 |          14.1682 |           4.3850 |
[32m[20230117 13:34:48 @agent_ppo2.py:193][0m |          -0.0167 |          13.9116 |           4.3826 |
[32m[20230117 13:34:48 @agent_ppo2.py:193][0m |          -0.0112 |          13.6521 |           4.3803 |
[32m[20230117 13:34:48 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:34:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.12
[32m[20230117 13:34:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.94
[32m[20230117 13:34:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.90
[32m[20230117 13:34:48 @agent_ppo2.py:151][0m Total time:       8.38 min
[32m[20230117 13:34:48 @agent_ppo2.py:153][0m 761856 total steps have happened
[32m[20230117 13:34:48 @agent_ppo2.py:129][0m #------------------------ Iteration 372 --------------------------#
[32m[20230117 13:34:48 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:48 @agent_ppo2.py:193][0m |          -0.0029 |          18.6621 |           4.4070 |
[32m[20230117 13:34:48 @agent_ppo2.py:193][0m |          -0.0070 |          17.6246 |           4.4046 |
[32m[20230117 13:34:48 @agent_ppo2.py:193][0m |          -0.0098 |          17.2386 |           4.4016 |
[32m[20230117 13:34:48 @agent_ppo2.py:193][0m |          -0.0114 |          16.8940 |           4.4079 |
[32m[20230117 13:34:48 @agent_ppo2.py:193][0m |          -0.0055 |          16.7467 |           4.4085 |
[32m[20230117 13:34:49 @agent_ppo2.py:193][0m |          -0.0097 |          16.5417 |           4.4104 |
[32m[20230117 13:34:49 @agent_ppo2.py:193][0m |          -0.0125 |          16.2596 |           4.4071 |
[32m[20230117 13:34:49 @agent_ppo2.py:193][0m |          -0.0141 |          16.1348 |           4.4116 |
[32m[20230117 13:34:49 @agent_ppo2.py:193][0m |          -0.0115 |          15.9454 |           4.4146 |
[32m[20230117 13:34:49 @agent_ppo2.py:193][0m |          -0.0125 |          15.7589 |           4.4148 |
[32m[20230117 13:34:49 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:34:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.69
[32m[20230117 13:34:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.94
[32m[20230117 13:34:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.07
[32m[20230117 13:34:49 @agent_ppo2.py:151][0m Total time:       8.40 min
[32m[20230117 13:34:49 @agent_ppo2.py:153][0m 763904 total steps have happened
[32m[20230117 13:34:49 @agent_ppo2.py:129][0m #------------------------ Iteration 373 --------------------------#
[32m[20230117 13:34:49 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:49 @agent_ppo2.py:193][0m |           0.0094 |          13.0779 |           4.4750 |
[32m[20230117 13:34:49 @agent_ppo2.py:193][0m |          -0.0081 |           6.8993 |           4.4717 |
[32m[20230117 13:34:49 @agent_ppo2.py:193][0m |          -0.0056 |           4.7560 |           4.4682 |
[32m[20230117 13:34:50 @agent_ppo2.py:193][0m |          -0.0136 |           4.0583 |           4.4673 |
[32m[20230117 13:34:50 @agent_ppo2.py:193][0m |          -0.0138 |           3.6350 |           4.4720 |
[32m[20230117 13:34:50 @agent_ppo2.py:193][0m |          -0.0143 |           3.3751 |           4.4688 |
[32m[20230117 13:34:50 @agent_ppo2.py:193][0m |          -0.0142 |           3.0549 |           4.4706 |
[32m[20230117 13:34:50 @agent_ppo2.py:193][0m |          -0.0182 |           2.8539 |           4.4674 |
[32m[20230117 13:34:50 @agent_ppo2.py:193][0m |          -0.0059 |           2.9675 |           4.4682 |
[32m[20230117 13:34:50 @agent_ppo2.py:193][0m |          -0.0095 |           2.6138 |           4.4578 |
[32m[20230117 13:34:50 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:34:50 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.06
[32m[20230117 13:34:50 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.63
[32m[20230117 13:34:50 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.68
[32m[20230117 13:34:50 @agent_ppo2.py:151][0m Total time:       8.42 min
[32m[20230117 13:34:50 @agent_ppo2.py:153][0m 765952 total steps have happened
[32m[20230117 13:34:50 @agent_ppo2.py:129][0m #------------------------ Iteration 374 --------------------------#
[32m[20230117 13:34:50 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:50 @agent_ppo2.py:193][0m |          -0.0007 |          14.5709 |           4.5752 |
[32m[20230117 13:34:51 @agent_ppo2.py:193][0m |          -0.0072 |          11.6097 |           4.5645 |
[32m[20230117 13:34:51 @agent_ppo2.py:193][0m |          -0.0091 |           9.6701 |           4.5659 |
[32m[20230117 13:34:51 @agent_ppo2.py:193][0m |          -0.0102 |           8.0986 |           4.5637 |
[32m[20230117 13:34:51 @agent_ppo2.py:193][0m |          -0.0111 |           6.9772 |           4.5635 |
[32m[20230117 13:34:51 @agent_ppo2.py:193][0m |          -0.0121 |           6.2670 |           4.5621 |
[32m[20230117 13:34:51 @agent_ppo2.py:193][0m |          -0.0128 |           5.6776 |           4.5609 |
[32m[20230117 13:34:51 @agent_ppo2.py:193][0m |          -0.0134 |           5.2596 |           4.5606 |
[32m[20230117 13:34:51 @agent_ppo2.py:193][0m |          -0.0138 |           4.8672 |           4.5586 |
[32m[20230117 13:34:51 @agent_ppo2.py:193][0m |          -0.0133 |           4.5763 |           4.5584 |
[32m[20230117 13:34:51 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:34:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.55
[32m[20230117 13:34:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.42
[32m[20230117 13:34:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.47
[32m[20230117 13:34:51 @agent_ppo2.py:151][0m Total time:       8.44 min
[32m[20230117 13:34:51 @agent_ppo2.py:153][0m 768000 total steps have happened
[32m[20230117 13:34:51 @agent_ppo2.py:129][0m #------------------------ Iteration 375 --------------------------#
[32m[20230117 13:34:51 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:52 @agent_ppo2.py:193][0m |           0.0010 |          21.1941 |           4.5692 |
[32m[20230117 13:34:52 @agent_ppo2.py:193][0m |          -0.0044 |          19.6375 |           4.5602 |
[32m[20230117 13:34:52 @agent_ppo2.py:193][0m |          -0.0066 |          19.1281 |           4.5594 |
[32m[20230117 13:34:52 @agent_ppo2.py:193][0m |          -0.0079 |          18.7317 |           4.5585 |
[32m[20230117 13:34:52 @agent_ppo2.py:193][0m |          -0.0095 |          18.3147 |           4.5547 |
[32m[20230117 13:34:52 @agent_ppo2.py:193][0m |          -0.0099 |          17.8348 |           4.5504 |
[32m[20230117 13:34:52 @agent_ppo2.py:193][0m |          -0.0102 |          17.3329 |           4.5551 |
[32m[20230117 13:34:52 @agent_ppo2.py:193][0m |          -0.0121 |          16.8107 |           4.5447 |
[32m[20230117 13:34:52 @agent_ppo2.py:193][0m |          -0.0120 |          16.2752 |           4.5445 |
[32m[20230117 13:34:52 @agent_ppo2.py:193][0m |          -0.0125 |          15.6791 |           4.5420 |
[32m[20230117 13:34:52 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:34:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.08
[32m[20230117 13:34:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.49
[32m[20230117 13:34:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.95
[32m[20230117 13:34:52 @agent_ppo2.py:151][0m Total time:       8.46 min
[32m[20230117 13:34:52 @agent_ppo2.py:153][0m 770048 total steps have happened
[32m[20230117 13:34:52 @agent_ppo2.py:129][0m #------------------------ Iteration 376 --------------------------#
[32m[20230117 13:34:53 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:34:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:53 @agent_ppo2.py:193][0m |           0.0061 |          36.6696 |           4.4847 |
[32m[20230117 13:34:53 @agent_ppo2.py:193][0m |          -0.0010 |          25.8940 |           4.4818 |
[32m[20230117 13:34:53 @agent_ppo2.py:193][0m |          -0.0073 |          23.2565 |           4.4780 |
[32m[20230117 13:34:53 @agent_ppo2.py:193][0m |          -0.0086 |          22.4426 |           4.4756 |
[32m[20230117 13:34:53 @agent_ppo2.py:193][0m |          -0.0037 |          22.3828 |           4.4820 |
[32m[20230117 13:34:53 @agent_ppo2.py:193][0m |          -0.0106 |          20.9751 |           4.4800 |
[32m[20230117 13:34:53 @agent_ppo2.py:193][0m |          -0.0099 |          20.4430 |           4.4795 |
[32m[20230117 13:34:53 @agent_ppo2.py:193][0m |          -0.0112 |          19.9633 |           4.4831 |
[32m[20230117 13:34:53 @agent_ppo2.py:193][0m |          -0.0133 |          19.6187 |           4.4763 |
[32m[20230117 13:34:53 @agent_ppo2.py:193][0m |          -0.0122 |          19.3669 |           4.4808 |
[32m[20230117 13:34:53 @agent_ppo2.py:138][0m Policy update time: 0.79 s
[32m[20230117 13:34:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 199.70
[32m[20230117 13:34:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.37
[32m[20230117 13:34:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.73
[32m[20230117 13:34:54 @agent_ppo2.py:151][0m Total time:       8.48 min
[32m[20230117 13:34:54 @agent_ppo2.py:153][0m 772096 total steps have happened
[32m[20230117 13:34:54 @agent_ppo2.py:129][0m #------------------------ Iteration 377 --------------------------#
[32m[20230117 13:34:54 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:54 @agent_ppo2.py:193][0m |          -0.0002 |          26.8631 |           4.6394 |
[32m[20230117 13:34:54 @agent_ppo2.py:193][0m |          -0.0030 |          22.3443 |           4.6338 |
[32m[20230117 13:34:54 @agent_ppo2.py:193][0m |          -0.0053 |          21.0141 |           4.6352 |
[32m[20230117 13:34:54 @agent_ppo2.py:193][0m |          -0.0067 |          20.3821 |           4.6352 |
[32m[20230117 13:34:54 @agent_ppo2.py:193][0m |          -0.0082 |          19.7191 |           4.6323 |
[32m[20230117 13:34:54 @agent_ppo2.py:193][0m |          -0.0093 |          19.2888 |           4.6335 |
[32m[20230117 13:34:54 @agent_ppo2.py:193][0m |          -0.0104 |          18.8162 |           4.6333 |
[32m[20230117 13:34:54 @agent_ppo2.py:193][0m |          -0.0095 |          18.5041 |           4.6333 |
[32m[20230117 13:34:54 @agent_ppo2.py:193][0m |          -0.0111 |          18.2537 |           4.6352 |
[32m[20230117 13:34:54 @agent_ppo2.py:193][0m |          -0.0113 |          17.8553 |           4.6368 |
[32m[20230117 13:34:54 @agent_ppo2.py:138][0m Policy update time: 0.65 s
[32m[20230117 13:34:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 199.68
[32m[20230117 13:34:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.85
[32m[20230117 13:34:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.10
[32m[20230117 13:34:55 @agent_ppo2.py:151][0m Total time:       8.49 min
[32m[20230117 13:34:55 @agent_ppo2.py:153][0m 774144 total steps have happened
[32m[20230117 13:34:55 @agent_ppo2.py:129][0m #------------------------ Iteration 378 --------------------------#
[32m[20230117 13:34:55 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:55 @agent_ppo2.py:193][0m |          -0.0008 |          39.5767 |           4.4716 |
[32m[20230117 13:34:55 @agent_ppo2.py:193][0m |          -0.0027 |          30.2781 |           4.4748 |
[32m[20230117 13:34:55 @agent_ppo2.py:193][0m |          -0.0070 |          28.4519 |           4.4752 |
[32m[20230117 13:34:55 @agent_ppo2.py:193][0m |          -0.0085 |          26.5254 |           4.4730 |
[32m[20230117 13:34:55 @agent_ppo2.py:193][0m |          -0.0088 |          25.3737 |           4.4756 |
[32m[20230117 13:34:55 @agent_ppo2.py:193][0m |          -0.0095 |          24.6540 |           4.4682 |
[32m[20230117 13:34:55 @agent_ppo2.py:193][0m |          -0.0107 |          23.9809 |           4.4716 |
[32m[20230117 13:34:56 @agent_ppo2.py:193][0m |          -0.0099 |          23.8024 |           4.4726 |
[32m[20230117 13:34:56 @agent_ppo2.py:193][0m |          -0.0157 |          22.6557 |           4.4719 |
[32m[20230117 13:34:56 @agent_ppo2.py:193][0m |          -0.0149 |          22.3263 |           4.4716 |
[32m[20230117 13:34:56 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:34:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.12
[32m[20230117 13:34:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.17
[32m[20230117 13:34:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.89
[32m[20230117 13:34:56 @agent_ppo2.py:151][0m Total time:       8.51 min
[32m[20230117 13:34:56 @agent_ppo2.py:153][0m 776192 total steps have happened
[32m[20230117 13:34:56 @agent_ppo2.py:129][0m #------------------------ Iteration 379 --------------------------#
[32m[20230117 13:34:56 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:56 @agent_ppo2.py:193][0m |           0.0036 |          21.8534 |           4.5150 |
[32m[20230117 13:34:56 @agent_ppo2.py:193][0m |          -0.0046 |          17.9975 |           4.5071 |
[32m[20230117 13:34:56 @agent_ppo2.py:193][0m |          -0.0117 |          16.0074 |           4.5045 |
[32m[20230117 13:34:56 @agent_ppo2.py:193][0m |          -0.0113 |          13.7937 |           4.5026 |
[32m[20230117 13:34:56 @agent_ppo2.py:193][0m |          -0.0124 |          11.8440 |           4.4993 |
[32m[20230117 13:34:57 @agent_ppo2.py:193][0m |          -0.0135 |          10.1501 |           4.5001 |
[32m[20230117 13:34:57 @agent_ppo2.py:193][0m |          -0.0149 |           9.0193 |           4.4962 |
[32m[20230117 13:34:57 @agent_ppo2.py:193][0m |          -0.0139 |           8.2201 |           4.4960 |
[32m[20230117 13:34:57 @agent_ppo2.py:193][0m |          -0.0155 |           7.5559 |           4.4915 |
[32m[20230117 13:34:57 @agent_ppo2.py:193][0m |          -0.0154 |           7.0849 |           4.4923 |
[32m[20230117 13:34:57 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:34:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.87
[32m[20230117 13:34:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.21
[32m[20230117 13:34:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.67
[32m[20230117 13:34:57 @agent_ppo2.py:151][0m Total time:       8.53 min
[32m[20230117 13:34:57 @agent_ppo2.py:153][0m 778240 total steps have happened
[32m[20230117 13:34:57 @agent_ppo2.py:129][0m #------------------------ Iteration 380 --------------------------#
[32m[20230117 13:34:57 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:34:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:57 @agent_ppo2.py:193][0m |           0.0008 |          19.2028 |           4.6009 |
[32m[20230117 13:34:57 @agent_ppo2.py:193][0m |          -0.0037 |          17.9372 |           4.5951 |
[32m[20230117 13:34:57 @agent_ppo2.py:193][0m |          -0.0078 |          17.1045 |           4.5899 |
[32m[20230117 13:34:58 @agent_ppo2.py:193][0m |          -0.0045 |          16.8191 |           4.5858 |
[32m[20230117 13:34:58 @agent_ppo2.py:193][0m |          -0.0075 |          16.4988 |           4.5885 |
[32m[20230117 13:34:58 @agent_ppo2.py:193][0m |          -0.0086 |          15.8219 |           4.5836 |
[32m[20230117 13:34:58 @agent_ppo2.py:193][0m |          -0.0109 |          15.4838 |           4.5837 |
[32m[20230117 13:34:58 @agent_ppo2.py:193][0m |          -0.0117 |          15.1774 |           4.5817 |
[32m[20230117 13:34:58 @agent_ppo2.py:193][0m |          -0.0125 |          14.8684 |           4.5828 |
[32m[20230117 13:34:58 @agent_ppo2.py:193][0m |          -0.0125 |          14.5950 |           4.5802 |
[32m[20230117 13:34:58 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:34:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.29
[32m[20230117 13:34:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.59
[32m[20230117 13:34:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.21
[32m[20230117 13:34:58 @agent_ppo2.py:151][0m Total time:       8.55 min
[32m[20230117 13:34:58 @agent_ppo2.py:153][0m 780288 total steps have happened
[32m[20230117 13:34:58 @agent_ppo2.py:129][0m #------------------------ Iteration 381 --------------------------#
[32m[20230117 13:34:58 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:34:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:34:58 @agent_ppo2.py:193][0m |           0.0024 |          34.8848 |           4.5244 |
[32m[20230117 13:34:59 @agent_ppo2.py:193][0m |          -0.0044 |          19.7455 |           4.5221 |
[32m[20230117 13:34:59 @agent_ppo2.py:193][0m |          -0.0043 |          17.7837 |           4.5157 |
[32m[20230117 13:34:59 @agent_ppo2.py:193][0m |          -0.0075 |          16.6045 |           4.5142 |
[32m[20230117 13:34:59 @agent_ppo2.py:193][0m |          -0.0084 |          16.0783 |           4.5164 |
[32m[20230117 13:34:59 @agent_ppo2.py:193][0m |          -0.0106 |          15.6980 |           4.5149 |
[32m[20230117 13:34:59 @agent_ppo2.py:193][0m |          -0.0108 |          15.2865 |           4.5198 |
[32m[20230117 13:34:59 @agent_ppo2.py:193][0m |          -0.0120 |          14.9658 |           4.5190 |
[32m[20230117 13:34:59 @agent_ppo2.py:193][0m |          -0.0125 |          14.7386 |           4.5195 |
[32m[20230117 13:34:59 @agent_ppo2.py:193][0m |          -0.0120 |          14.4886 |           4.5178 |
[32m[20230117 13:34:59 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:34:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 232.13
[32m[20230117 13:34:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.06
[32m[20230117 13:34:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.63
[32m[20230117 13:34:59 @agent_ppo2.py:151][0m Total time:       8.57 min
[32m[20230117 13:34:59 @agent_ppo2.py:153][0m 782336 total steps have happened
[32m[20230117 13:34:59 @agent_ppo2.py:129][0m #------------------------ Iteration 382 --------------------------#
[32m[20230117 13:34:59 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:00 @agent_ppo2.py:193][0m |          -0.0053 |          17.6994 |           4.5059 |
[32m[20230117 13:35:00 @agent_ppo2.py:193][0m |          -0.0042 |          16.0247 |           4.4985 |
[32m[20230117 13:35:00 @agent_ppo2.py:193][0m |           0.0021 |          15.1780 |           4.4986 |
[32m[20230117 13:35:00 @agent_ppo2.py:193][0m |          -0.0106 |          14.3016 |           4.4986 |
[32m[20230117 13:35:00 @agent_ppo2.py:193][0m |          -0.0078 |          13.7001 |           4.4995 |
[32m[20230117 13:35:00 @agent_ppo2.py:193][0m |           0.0982 |          20.6139 |           4.4964 |
[32m[20230117 13:35:00 @agent_ppo2.py:193][0m |          -0.0164 |          13.3926 |           4.4895 |
[32m[20230117 13:35:00 @agent_ppo2.py:193][0m |          -0.0132 |          12.8143 |           4.4938 |
[32m[20230117 13:35:00 @agent_ppo2.py:193][0m |          -0.0170 |          12.5119 |           4.4942 |
[32m[20230117 13:35:00 @agent_ppo2.py:193][0m |          -0.0150 |          12.3295 |           4.4957 |
[32m[20230117 13:35:00 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.03
[32m[20230117 13:35:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.48
[32m[20230117 13:35:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.96
[32m[20230117 13:35:00 @agent_ppo2.py:151][0m Total time:       8.59 min
[32m[20230117 13:35:00 @agent_ppo2.py:153][0m 784384 total steps have happened
[32m[20230117 13:35:00 @agent_ppo2.py:129][0m #------------------------ Iteration 383 --------------------------#
[32m[20230117 13:35:01 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:35:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:01 @agent_ppo2.py:193][0m |          -0.0013 |          18.7093 |           4.5302 |
[32m[20230117 13:35:01 @agent_ppo2.py:193][0m |          -0.0051 |          18.0167 |           4.5185 |
[32m[20230117 13:35:01 @agent_ppo2.py:193][0m |          -0.0003 |          18.0109 |           4.5181 |
[32m[20230117 13:35:01 @agent_ppo2.py:193][0m |          -0.0107 |          17.5466 |           4.5126 |
[32m[20230117 13:35:01 @agent_ppo2.py:193][0m |          -0.0082 |          17.4215 |           4.5109 |
[32m[20230117 13:35:01 @agent_ppo2.py:193][0m |          -0.0131 |          17.3091 |           4.5108 |
[32m[20230117 13:35:01 @agent_ppo2.py:193][0m |          -0.0109 |          17.2594 |           4.5060 |
[32m[20230117 13:35:01 @agent_ppo2.py:193][0m |          -0.0129 |          17.1109 |           4.5065 |
[32m[20230117 13:35:01 @agent_ppo2.py:193][0m |          -0.0126 |          17.0311 |           4.5041 |
[32m[20230117 13:35:01 @agent_ppo2.py:193][0m |          -0.0168 |          16.9941 |           4.5026 |
[32m[20230117 13:35:01 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:35:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.50
[32m[20230117 13:35:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.06
[32m[20230117 13:35:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.83
[32m[20230117 13:35:02 @agent_ppo2.py:151][0m Total time:       8.61 min
[32m[20230117 13:35:02 @agent_ppo2.py:153][0m 786432 total steps have happened
[32m[20230117 13:35:02 @agent_ppo2.py:129][0m #------------------------ Iteration 384 --------------------------#
[32m[20230117 13:35:02 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:02 @agent_ppo2.py:193][0m |           0.0016 |          18.4774 |           4.5600 |
[32m[20230117 13:35:02 @agent_ppo2.py:193][0m |          -0.0049 |          17.2656 |           4.5535 |
[32m[20230117 13:35:02 @agent_ppo2.py:193][0m |          -0.0061 |          16.7538 |           4.5508 |
[32m[20230117 13:35:02 @agent_ppo2.py:193][0m |          -0.0092 |          16.1827 |           4.5542 |
[32m[20230117 13:35:02 @agent_ppo2.py:193][0m |          -0.0099 |          15.9278 |           4.5561 |
[32m[20230117 13:35:02 @agent_ppo2.py:193][0m |          -0.0103 |          15.5227 |           4.5539 |
[32m[20230117 13:35:02 @agent_ppo2.py:193][0m |          -0.0111 |          15.2453 |           4.5562 |
[32m[20230117 13:35:02 @agent_ppo2.py:193][0m |          -0.0129 |          14.9461 |           4.5552 |
[32m[20230117 13:35:02 @agent_ppo2.py:193][0m |          -0.0128 |          14.8032 |           4.5582 |
[32m[20230117 13:35:02 @agent_ppo2.py:193][0m |          -0.0136 |          14.6476 |           4.5604 |
[32m[20230117 13:35:02 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.07
[32m[20230117 13:35:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.19
[32m[20230117 13:35:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.02
[32m[20230117 13:35:03 @agent_ppo2.py:151][0m Total time:       8.63 min
[32m[20230117 13:35:03 @agent_ppo2.py:153][0m 788480 total steps have happened
[32m[20230117 13:35:03 @agent_ppo2.py:129][0m #------------------------ Iteration 385 --------------------------#
[32m[20230117 13:35:03 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:35:03 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:03 @agent_ppo2.py:193][0m |           0.0006 |          17.8604 |           4.5687 |
[32m[20230117 13:35:03 @agent_ppo2.py:193][0m |          -0.0041 |          16.3294 |           4.5569 |
[32m[20230117 13:35:03 @agent_ppo2.py:193][0m |          -0.0058 |          15.6685 |           4.5556 |
[32m[20230117 13:35:03 @agent_ppo2.py:193][0m |          -0.0076 |          15.2497 |           4.5528 |
[32m[20230117 13:35:03 @agent_ppo2.py:193][0m |          -0.0080 |          14.9698 |           4.5516 |
[32m[20230117 13:35:03 @agent_ppo2.py:193][0m |          -0.0090 |          14.7431 |           4.5533 |
[32m[20230117 13:35:03 @agent_ppo2.py:193][0m |          -0.0098 |          14.5363 |           4.5499 |
[32m[20230117 13:35:03 @agent_ppo2.py:193][0m |          -0.0096 |          14.3881 |           4.5527 |
[32m[20230117 13:35:04 @agent_ppo2.py:193][0m |          -0.0110 |          14.2625 |           4.5497 |
[32m[20230117 13:35:04 @agent_ppo2.py:193][0m |          -0.0109 |          14.1684 |           4.5496 |
[32m[20230117 13:35:04 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.19
[32m[20230117 13:35:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.07
[32m[20230117 13:35:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.17
[32m[20230117 13:35:04 @agent_ppo2.py:151][0m Total time:       8.65 min
[32m[20230117 13:35:04 @agent_ppo2.py:153][0m 790528 total steps have happened
[32m[20230117 13:35:04 @agent_ppo2.py:129][0m #------------------------ Iteration 386 --------------------------#
[32m[20230117 13:35:04 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:35:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:04 @agent_ppo2.py:193][0m |           0.0002 |          22.1630 |           4.6061 |
[32m[20230117 13:35:04 @agent_ppo2.py:193][0m |          -0.0042 |          19.4921 |           4.5971 |
[32m[20230117 13:35:04 @agent_ppo2.py:193][0m |          -0.0056 |          18.6468 |           4.6025 |
[32m[20230117 13:35:04 @agent_ppo2.py:193][0m |          -0.0073 |          17.6617 |           4.5967 |
[32m[20230117 13:35:04 @agent_ppo2.py:193][0m |          -0.0082 |          16.8955 |           4.6006 |
[32m[20230117 13:35:05 @agent_ppo2.py:193][0m |          -0.0100 |          16.0977 |           4.6021 |
[32m[20230117 13:35:05 @agent_ppo2.py:193][0m |          -0.0102 |          15.4385 |           4.6041 |
[32m[20230117 13:35:05 @agent_ppo2.py:193][0m |          -0.0116 |          14.6011 |           4.6040 |
[32m[20230117 13:35:05 @agent_ppo2.py:193][0m |          -0.0122 |          14.3773 |           4.6029 |
[32m[20230117 13:35:05 @agent_ppo2.py:193][0m |          -0.0126 |          13.9513 |           4.6009 |
[32m[20230117 13:35:05 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:35:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.74
[32m[20230117 13:35:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.87
[32m[20230117 13:35:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.95
[32m[20230117 13:35:05 @agent_ppo2.py:151][0m Total time:       8.67 min
[32m[20230117 13:35:05 @agent_ppo2.py:153][0m 792576 total steps have happened
[32m[20230117 13:35:05 @agent_ppo2.py:129][0m #------------------------ Iteration 387 --------------------------#
[32m[20230117 13:35:05 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:05 @agent_ppo2.py:193][0m |           0.0014 |          17.6677 |           4.6243 |
[32m[20230117 13:35:05 @agent_ppo2.py:193][0m |          -0.0053 |           9.8832 |           4.6132 |
[32m[20230117 13:35:05 @agent_ppo2.py:193][0m |          -0.0076 |           7.0645 |           4.6118 |
[32m[20230117 13:35:06 @agent_ppo2.py:193][0m |          -0.0093 |           6.1319 |           4.6065 |
[32m[20230117 13:35:06 @agent_ppo2.py:193][0m |          -0.0090 |           5.6809 |           4.6117 |
[32m[20230117 13:35:06 @agent_ppo2.py:193][0m |          -0.0100 |           5.3081 |           4.6086 |
[32m[20230117 13:35:06 @agent_ppo2.py:193][0m |          -0.0122 |           5.0611 |           4.6099 |
[32m[20230117 13:35:06 @agent_ppo2.py:193][0m |          -0.0120 |           4.8098 |           4.6068 |
[32m[20230117 13:35:06 @agent_ppo2.py:193][0m |          -0.0108 |           4.6178 |           4.6081 |
[32m[20230117 13:35:06 @agent_ppo2.py:193][0m |          -0.0148 |           4.4724 |           4.6061 |
[32m[20230117 13:35:06 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.55
[32m[20230117 13:35:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 270.64
[32m[20230117 13:35:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.91
[32m[20230117 13:35:06 @agent_ppo2.py:151][0m Total time:       8.69 min
[32m[20230117 13:35:06 @agent_ppo2.py:153][0m 794624 total steps have happened
[32m[20230117 13:35:06 @agent_ppo2.py:129][0m #------------------------ Iteration 388 --------------------------#
[32m[20230117 13:35:06 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:06 @agent_ppo2.py:193][0m |           0.0007 |          24.8990 |           4.5468 |
[32m[20230117 13:35:07 @agent_ppo2.py:193][0m |          -0.0056 |          20.6521 |           4.5437 |
[32m[20230117 13:35:07 @agent_ppo2.py:193][0m |          -0.0083 |          19.8263 |           4.5390 |
[32m[20230117 13:35:07 @agent_ppo2.py:193][0m |          -0.0095 |          19.4039 |           4.5323 |
[32m[20230117 13:35:07 @agent_ppo2.py:193][0m |          -0.0106 |          18.8852 |           4.5358 |
[32m[20230117 13:35:07 @agent_ppo2.py:193][0m |          -0.0119 |          18.5150 |           4.5331 |
[32m[20230117 13:35:07 @agent_ppo2.py:193][0m |          -0.0123 |          18.2793 |           4.5315 |
[32m[20230117 13:35:07 @agent_ppo2.py:193][0m |          -0.0133 |          17.8860 |           4.5326 |
[32m[20230117 13:35:07 @agent_ppo2.py:193][0m |          -0.0152 |          17.5242 |           4.5328 |
[32m[20230117 13:35:07 @agent_ppo2.py:193][0m |          -0.0148 |          17.1003 |           4.5340 |
[32m[20230117 13:35:07 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:35:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.17
[32m[20230117 13:35:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.79
[32m[20230117 13:35:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.21
[32m[20230117 13:35:07 @agent_ppo2.py:151][0m Total time:       8.70 min
[32m[20230117 13:35:07 @agent_ppo2.py:153][0m 796672 total steps have happened
[32m[20230117 13:35:07 @agent_ppo2.py:129][0m #------------------------ Iteration 389 --------------------------#
[32m[20230117 13:35:07 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:08 @agent_ppo2.py:193][0m |           0.0013 |          20.6254 |           4.6795 |
[32m[20230117 13:35:08 @agent_ppo2.py:193][0m |          -0.0070 |          18.3376 |           4.6686 |
[32m[20230117 13:35:08 @agent_ppo2.py:193][0m |          -0.0079 |          17.3156 |           4.6643 |
[32m[20230117 13:35:08 @agent_ppo2.py:193][0m |          -0.0083 |          16.7040 |           4.6629 |
[32m[20230117 13:35:08 @agent_ppo2.py:193][0m |          -0.0109 |          16.0893 |           4.6655 |
[32m[20230117 13:35:08 @agent_ppo2.py:193][0m |          -0.0113 |          15.6238 |           4.6668 |
[32m[20230117 13:35:08 @agent_ppo2.py:193][0m |          -0.0116 |          15.4252 |           4.6696 |
[32m[20230117 13:35:08 @agent_ppo2.py:193][0m |          -0.0119 |          15.1425 |           4.6709 |
[32m[20230117 13:35:08 @agent_ppo2.py:193][0m |          -0.0134 |          14.8739 |           4.6680 |
[32m[20230117 13:35:08 @agent_ppo2.py:193][0m |          -0.0134 |          14.8639 |           4.6671 |
[32m[20230117 13:35:08 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.26
[32m[20230117 13:35:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.12
[32m[20230117 13:35:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.55
[32m[20230117 13:35:08 @agent_ppo2.py:151][0m Total time:       8.72 min
[32m[20230117 13:35:08 @agent_ppo2.py:153][0m 798720 total steps have happened
[32m[20230117 13:35:08 @agent_ppo2.py:129][0m #------------------------ Iteration 390 --------------------------#
[32m[20230117 13:35:09 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:09 @agent_ppo2.py:193][0m |           0.0004 |          24.4958 |           4.6392 |
[32m[20230117 13:35:09 @agent_ppo2.py:193][0m |          -0.0056 |          19.6206 |           4.6281 |
[32m[20230117 13:35:09 @agent_ppo2.py:193][0m |          -0.0092 |          18.7864 |           4.6278 |
[32m[20230117 13:35:09 @agent_ppo2.py:193][0m |          -0.0102 |          18.2665 |           4.6296 |
[32m[20230117 13:35:09 @agent_ppo2.py:193][0m |          -0.0110 |          17.8634 |           4.6314 |
[32m[20230117 13:35:09 @agent_ppo2.py:193][0m |          -0.0124 |          17.3258 |           4.6307 |
[32m[20230117 13:35:09 @agent_ppo2.py:193][0m |          -0.0130 |          17.0639 |           4.6313 |
[32m[20230117 13:35:09 @agent_ppo2.py:193][0m |          -0.0136 |          16.7804 |           4.6329 |
[32m[20230117 13:35:09 @agent_ppo2.py:193][0m |          -0.0145 |          16.5817 |           4.6304 |
[32m[20230117 13:35:09 @agent_ppo2.py:193][0m |          -0.0144 |          16.4326 |           4.6325 |
[32m[20230117 13:35:09 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 190.34
[32m[20230117 13:35:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.56
[32m[20230117 13:35:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.85
[32m[20230117 13:35:10 @agent_ppo2.py:151][0m Total time:       8.74 min
[32m[20230117 13:35:10 @agent_ppo2.py:153][0m 800768 total steps have happened
[32m[20230117 13:35:10 @agent_ppo2.py:129][0m #------------------------ Iteration 391 --------------------------#
[32m[20230117 13:35:10 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:35:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:10 @agent_ppo2.py:193][0m |           0.0165 |          17.2231 |           4.5264 |
[32m[20230117 13:35:10 @agent_ppo2.py:193][0m |          -0.0027 |          15.0050 |           4.5249 |
[32m[20230117 13:35:10 @agent_ppo2.py:193][0m |          -0.0060 |          14.1449 |           4.5282 |
[32m[20230117 13:35:10 @agent_ppo2.py:193][0m |          -0.0095 |          13.5209 |           4.5254 |
[32m[20230117 13:35:10 @agent_ppo2.py:193][0m |          -0.0282 |          13.1751 |           4.5309 |
[32m[20230117 13:35:10 @agent_ppo2.py:193][0m |           0.0074 |          12.8293 |           4.5316 |
[32m[20230117 13:35:10 @agent_ppo2.py:193][0m |          -0.0165 |          12.3251 |           4.5310 |
[32m[20230117 13:35:10 @agent_ppo2.py:193][0m |          -0.0126 |          11.9229 |           4.5322 |
[32m[20230117 13:35:10 @agent_ppo2.py:193][0m |          -0.0188 |          11.6935 |           4.5307 |
[32m[20230117 13:35:11 @agent_ppo2.py:193][0m |          -0.0275 |          11.3843 |           4.5312 |
[32m[20230117 13:35:11 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:35:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.87
[32m[20230117 13:35:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.21
[32m[20230117 13:35:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.31
[32m[20230117 13:35:11 @agent_ppo2.py:151][0m Total time:       8.76 min
[32m[20230117 13:35:11 @agent_ppo2.py:153][0m 802816 total steps have happened
[32m[20230117 13:35:11 @agent_ppo2.py:129][0m #------------------------ Iteration 392 --------------------------#
[32m[20230117 13:35:11 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:11 @agent_ppo2.py:193][0m |          -0.0001 |          19.7996 |           4.5227 |
[32m[20230117 13:35:11 @agent_ppo2.py:193][0m |          -0.0075 |          18.2202 |           4.5163 |
[32m[20230117 13:35:11 @agent_ppo2.py:193][0m |          -0.0082 |          17.8040 |           4.5158 |
[32m[20230117 13:35:11 @agent_ppo2.py:193][0m |          -0.0114 |          17.4599 |           4.5196 |
[32m[20230117 13:35:11 @agent_ppo2.py:193][0m |          -0.0080 |          17.7685 |           4.5176 |
[32m[20230117 13:35:11 @agent_ppo2.py:193][0m |          -0.0083 |          17.7199 |           4.5133 |
[32m[20230117 13:35:11 @agent_ppo2.py:193][0m |          -0.0139 |          16.8701 |           4.5164 |
[32m[20230117 13:35:12 @agent_ppo2.py:193][0m |          -0.0124 |          17.1214 |           4.5194 |
[32m[20230117 13:35:12 @agent_ppo2.py:193][0m |          -0.0158 |          16.5749 |           4.5161 |
[32m[20230117 13:35:12 @agent_ppo2.py:193][0m |          -0.0155 |          16.4738 |           4.5176 |
[32m[20230117 13:35:12 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.52
[32m[20230117 13:35:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.20
[32m[20230117 13:35:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.54
[32m[20230117 13:35:12 @agent_ppo2.py:151][0m Total time:       8.78 min
[32m[20230117 13:35:12 @agent_ppo2.py:153][0m 804864 total steps have happened
[32m[20230117 13:35:12 @agent_ppo2.py:129][0m #------------------------ Iteration 393 --------------------------#
[32m[20230117 13:35:12 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:35:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:12 @agent_ppo2.py:193][0m |           0.0026 |          19.2274 |           4.7561 |
[32m[20230117 13:35:12 @agent_ppo2.py:193][0m |          -0.0020 |          17.9898 |           4.7558 |
[32m[20230117 13:35:12 @agent_ppo2.py:193][0m |          -0.0050 |          17.2948 |           4.7558 |
[32m[20230117 13:35:12 @agent_ppo2.py:193][0m |          -0.0066 |          16.7701 |           4.7587 |
[32m[20230117 13:35:13 @agent_ppo2.py:193][0m |          -0.0077 |          16.3496 |           4.7611 |
[32m[20230117 13:35:13 @agent_ppo2.py:193][0m |          -0.0083 |          16.0852 |           4.7619 |
[32m[20230117 13:35:13 @agent_ppo2.py:193][0m |          -0.0091 |          15.9133 |           4.7643 |
[32m[20230117 13:35:13 @agent_ppo2.py:193][0m |          -0.0103 |          15.6443 |           4.7658 |
[32m[20230117 13:35:13 @agent_ppo2.py:193][0m |          -0.0109 |          15.4873 |           4.7683 |
[32m[20230117 13:35:13 @agent_ppo2.py:193][0m |          -0.0114 |          15.3416 |           4.7706 |
[32m[20230117 13:35:13 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:35:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.56
[32m[20230117 13:35:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.52
[32m[20230117 13:35:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.44
[32m[20230117 13:35:13 @agent_ppo2.py:151][0m Total time:       8.80 min
[32m[20230117 13:35:13 @agent_ppo2.py:153][0m 806912 total steps have happened
[32m[20230117 13:35:13 @agent_ppo2.py:129][0m #------------------------ Iteration 394 --------------------------#
[32m[20230117 13:35:13 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:13 @agent_ppo2.py:193][0m |           0.0010 |          19.1828 |           4.7205 |
[32m[20230117 13:35:13 @agent_ppo2.py:193][0m |          -0.0024 |          17.5221 |           4.7128 |
[32m[20230117 13:35:14 @agent_ppo2.py:193][0m |          -0.0061 |          16.9028 |           4.7087 |
[32m[20230117 13:35:14 @agent_ppo2.py:193][0m |          -0.0041 |          17.5080 |           4.7111 |
[32m[20230117 13:35:14 @agent_ppo2.py:193][0m |          -0.0088 |          16.0953 |           4.7050 |
[32m[20230117 13:35:14 @agent_ppo2.py:193][0m |          -0.0094 |          15.7846 |           4.7094 |
[32m[20230117 13:35:14 @agent_ppo2.py:193][0m |          -0.0109 |          15.3023 |           4.7087 |
[32m[20230117 13:35:14 @agent_ppo2.py:193][0m |          -0.0141 |          14.9297 |           4.7079 |
[32m[20230117 13:35:14 @agent_ppo2.py:193][0m |          -0.0104 |          14.5430 |           4.7070 |
[32m[20230117 13:35:14 @agent_ppo2.py:193][0m |          -0.0073 |          14.5259 |           4.7027 |
[32m[20230117 13:35:14 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.33
[32m[20230117 13:35:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.62
[32m[20230117 13:35:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.94
[32m[20230117 13:35:14 @agent_ppo2.py:151][0m Total time:       8.82 min
[32m[20230117 13:35:14 @agent_ppo2.py:153][0m 808960 total steps have happened
[32m[20230117 13:35:14 @agent_ppo2.py:129][0m #------------------------ Iteration 395 --------------------------#
[32m[20230117 13:35:14 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:35:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:15 @agent_ppo2.py:193][0m |          -0.0002 |          18.1931 |           4.6793 |
[32m[20230117 13:35:15 @agent_ppo2.py:193][0m |          -0.0068 |          14.2470 |           4.6755 |
[32m[20230117 13:35:15 @agent_ppo2.py:193][0m |          -0.0083 |          12.9127 |           4.6701 |
[32m[20230117 13:35:15 @agent_ppo2.py:193][0m |          -0.0089 |          12.0422 |           4.6724 |
[32m[20230117 13:35:15 @agent_ppo2.py:193][0m |          -0.0112 |          11.5842 |           4.6751 |
[32m[20230117 13:35:15 @agent_ppo2.py:193][0m |          -0.0122 |          11.2138 |           4.6703 |
[32m[20230117 13:35:15 @agent_ppo2.py:193][0m |          -0.0120 |          10.9251 |           4.6725 |
[32m[20230117 13:35:15 @agent_ppo2.py:193][0m |          -0.0141 |          10.7320 |           4.6677 |
[32m[20230117 13:35:15 @agent_ppo2.py:193][0m |          -0.0125 |          10.5656 |           4.6686 |
[32m[20230117 13:35:15 @agent_ppo2.py:193][0m |          -0.0150 |          10.3501 |           4.6682 |
[32m[20230117 13:35:15 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:35:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 260.51
[32m[20230117 13:35:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.14
[32m[20230117 13:35:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.93
[32m[20230117 13:35:15 @agent_ppo2.py:151][0m Total time:       8.84 min
[32m[20230117 13:35:15 @agent_ppo2.py:153][0m 811008 total steps have happened
[32m[20230117 13:35:15 @agent_ppo2.py:129][0m #------------------------ Iteration 396 --------------------------#
[32m[20230117 13:35:16 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:16 @agent_ppo2.py:193][0m |           0.0022 |          18.3246 |           4.8130 |
[32m[20230117 13:35:16 @agent_ppo2.py:193][0m |          -0.0037 |          16.3825 |           4.7971 |
[32m[20230117 13:35:16 @agent_ppo2.py:193][0m |          -0.0061 |          15.6095 |           4.7952 |
[32m[20230117 13:35:16 @agent_ppo2.py:193][0m |          -0.0051 |          15.3763 |           4.7917 |
[32m[20230117 13:35:16 @agent_ppo2.py:193][0m |          -0.0080 |          15.1467 |           4.7853 |
[32m[20230117 13:35:16 @agent_ppo2.py:193][0m |          -0.0094 |          14.7541 |           4.7915 |
[32m[20230117 13:35:16 @agent_ppo2.py:193][0m |          -0.0101 |          14.6569 |           4.7884 |
[32m[20230117 13:35:16 @agent_ppo2.py:193][0m |          -0.0111 |          14.4513 |           4.7884 |
[32m[20230117 13:35:16 @agent_ppo2.py:193][0m |          -0.0121 |          14.3160 |           4.7885 |
[32m[20230117 13:35:16 @agent_ppo2.py:193][0m |          -0.0130 |          14.0871 |           4.7844 |
[32m[20230117 13:35:16 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:35:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.60
[32m[20230117 13:35:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.60
[32m[20230117 13:35:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.27
[32m[20230117 13:35:17 @agent_ppo2.py:151][0m Total time:       8.86 min
[32m[20230117 13:35:17 @agent_ppo2.py:153][0m 813056 total steps have happened
[32m[20230117 13:35:17 @agent_ppo2.py:129][0m #------------------------ Iteration 397 --------------------------#
[32m[20230117 13:35:17 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:17 @agent_ppo2.py:193][0m |          -0.0030 |          17.2421 |           4.7054 |
[32m[20230117 13:35:17 @agent_ppo2.py:193][0m |          -0.0061 |          15.7892 |           4.6920 |
[32m[20230117 13:35:17 @agent_ppo2.py:193][0m |          -0.0067 |          15.5763 |           4.6840 |
[32m[20230117 13:35:17 @agent_ppo2.py:193][0m |          -0.0104 |          14.7919 |           4.6944 |
[32m[20230117 13:35:17 @agent_ppo2.py:193][0m |          -0.0078 |          14.6474 |           4.6828 |
[32m[20230117 13:35:17 @agent_ppo2.py:193][0m |          -0.0131 |          14.1999 |           4.6867 |
[32m[20230117 13:35:17 @agent_ppo2.py:193][0m |          -0.0139 |          13.9936 |           4.6787 |
[32m[20230117 13:35:17 @agent_ppo2.py:193][0m |          -0.0135 |          13.8231 |           4.6835 |
[32m[20230117 13:35:17 @agent_ppo2.py:193][0m |          -0.0132 |          13.6225 |           4.6857 |
[32m[20230117 13:35:18 @agent_ppo2.py:193][0m |          -0.0154 |          13.4941 |           4.6861 |
[32m[20230117 13:35:18 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.81
[32m[20230117 13:35:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.30
[32m[20230117 13:35:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.78
[32m[20230117 13:35:18 @agent_ppo2.py:151][0m Total time:       8.88 min
[32m[20230117 13:35:18 @agent_ppo2.py:153][0m 815104 total steps have happened
[32m[20230117 13:35:18 @agent_ppo2.py:129][0m #------------------------ Iteration 398 --------------------------#
[32m[20230117 13:35:18 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:18 @agent_ppo2.py:193][0m |           0.0004 |          20.7089 |           4.7127 |
[32m[20230117 13:35:18 @agent_ppo2.py:193][0m |          -0.0035 |          19.0041 |           4.7070 |
[32m[20230117 13:35:18 @agent_ppo2.py:193][0m |          -0.0054 |          18.3175 |           4.6988 |
[32m[20230117 13:35:18 @agent_ppo2.py:193][0m |          -0.0071 |          17.7891 |           4.7012 |
[32m[20230117 13:35:18 @agent_ppo2.py:193][0m |          -0.0073 |          17.4386 |           4.7047 |
[32m[20230117 13:35:18 @agent_ppo2.py:193][0m |          -0.0084 |          17.1517 |           4.7029 |
[32m[20230117 13:35:18 @agent_ppo2.py:193][0m |          -0.0096 |          16.9363 |           4.7026 |
[32m[20230117 13:35:19 @agent_ppo2.py:193][0m |          -0.0103 |          16.7266 |           4.7011 |
[32m[20230117 13:35:19 @agent_ppo2.py:193][0m |          -0.0100 |          16.4858 |           4.6989 |
[32m[20230117 13:35:19 @agent_ppo2.py:193][0m |          -0.0112 |          16.3268 |           4.6959 |
[32m[20230117 13:35:19 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.82
[32m[20230117 13:35:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.39
[32m[20230117 13:35:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.30
[32m[20230117 13:35:19 @agent_ppo2.py:151][0m Total time:       8.90 min
[32m[20230117 13:35:19 @agent_ppo2.py:153][0m 817152 total steps have happened
[32m[20230117 13:35:19 @agent_ppo2.py:129][0m #------------------------ Iteration 399 --------------------------#
[32m[20230117 13:35:19 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:19 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:19 @agent_ppo2.py:193][0m |          -0.0011 |          18.6885 |           4.6404 |
[32m[20230117 13:35:19 @agent_ppo2.py:193][0m |          -0.0052 |          15.8425 |           4.6330 |
[32m[20230117 13:35:19 @agent_ppo2.py:193][0m |          -0.0096 |          14.4180 |           4.6244 |
[32m[20230117 13:35:19 @agent_ppo2.py:193][0m |          -0.0085 |          13.4788 |           4.6243 |
[32m[20230117 13:35:19 @agent_ppo2.py:193][0m |          -0.0114 |          12.2851 |           4.6230 |
[32m[20230117 13:35:20 @agent_ppo2.py:193][0m |          -0.0121 |          11.4468 |           4.6231 |
[32m[20230117 13:35:20 @agent_ppo2.py:193][0m |          -0.0132 |          10.6173 |           4.6197 |
[32m[20230117 13:35:20 @agent_ppo2.py:193][0m |          -0.0122 |           9.8785 |           4.6230 |
[32m[20230117 13:35:20 @agent_ppo2.py:193][0m |          -0.0147 |           9.1786 |           4.6188 |
[32m[20230117 13:35:20 @agent_ppo2.py:193][0m |          -0.0149 |           8.7287 |           4.6189 |
[32m[20230117 13:35:20 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:20 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.87
[32m[20230117 13:35:20 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.02
[32m[20230117 13:35:20 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.42
[32m[20230117 13:35:20 @agent_ppo2.py:151][0m Total time:       8.92 min
[32m[20230117 13:35:20 @agent_ppo2.py:153][0m 819200 total steps have happened
[32m[20230117 13:35:20 @agent_ppo2.py:129][0m #------------------------ Iteration 400 --------------------------#
[32m[20230117 13:35:20 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:20 @agent_ppo2.py:193][0m |           0.0010 |          14.3796 |           4.7652 |
[32m[20230117 13:35:20 @agent_ppo2.py:193][0m |          -0.0033 |           9.0451 |           4.7609 |
[32m[20230117 13:35:20 @agent_ppo2.py:193][0m |          -0.0061 |           7.0823 |           4.7633 |
[32m[20230117 13:35:21 @agent_ppo2.py:193][0m |          -0.0081 |           6.0563 |           4.7599 |
[32m[20230117 13:35:21 @agent_ppo2.py:193][0m |          -0.0096 |           5.3256 |           4.7627 |
[32m[20230117 13:35:21 @agent_ppo2.py:193][0m |          -0.0107 |           4.8606 |           4.7603 |
[32m[20230117 13:35:21 @agent_ppo2.py:193][0m |          -0.0121 |           4.5220 |           4.7626 |
[32m[20230117 13:35:21 @agent_ppo2.py:193][0m |          -0.0130 |           4.2822 |           4.7602 |
[32m[20230117 13:35:21 @agent_ppo2.py:193][0m |          -0.0141 |           3.9670 |           4.7609 |
[32m[20230117 13:35:21 @agent_ppo2.py:193][0m |          -0.0144 |           3.8326 |           4.7666 |
[32m[20230117 13:35:21 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:35:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.55
[32m[20230117 13:35:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.91
[32m[20230117 13:35:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.29
[32m[20230117 13:35:21 @agent_ppo2.py:151][0m Total time:       8.93 min
[32m[20230117 13:35:21 @agent_ppo2.py:153][0m 821248 total steps have happened
[32m[20230117 13:35:21 @agent_ppo2.py:129][0m #------------------------ Iteration 401 --------------------------#
[32m[20230117 13:35:21 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:35:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:22 @agent_ppo2.py:193][0m |          -0.0003 |          17.4537 |           4.6359 |
[32m[20230117 13:35:22 @agent_ppo2.py:193][0m |          -0.0074 |          13.1849 |           4.6247 |
[32m[20230117 13:35:22 @agent_ppo2.py:193][0m |          -0.0088 |          12.1057 |           4.6185 |
[32m[20230117 13:35:22 @agent_ppo2.py:193][0m |          -0.0100 |          11.3846 |           4.6201 |
[32m[20230117 13:35:22 @agent_ppo2.py:193][0m |          -0.0115 |          10.9216 |           4.6192 |
[32m[20230117 13:35:22 @agent_ppo2.py:193][0m |          -0.0119 |          10.4749 |           4.6183 |
[32m[20230117 13:35:22 @agent_ppo2.py:193][0m |          -0.0131 |          10.0660 |           4.6194 |
[32m[20230117 13:35:22 @agent_ppo2.py:193][0m |          -0.0129 |           9.5088 |           4.6190 |
[32m[20230117 13:35:22 @agent_ppo2.py:193][0m |          -0.0143 |           8.8159 |           4.6167 |
[32m[20230117 13:35:22 @agent_ppo2.py:193][0m |          -0.0146 |           8.3654 |           4.6173 |
[32m[20230117 13:35:22 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:35:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.74
[32m[20230117 13:35:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.43
[32m[20230117 13:35:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.16
[32m[20230117 13:35:22 @agent_ppo2.py:151][0m Total time:       8.96 min
[32m[20230117 13:35:22 @agent_ppo2.py:153][0m 823296 total steps have happened
[32m[20230117 13:35:22 @agent_ppo2.py:129][0m #------------------------ Iteration 402 --------------------------#
[32m[20230117 13:35:23 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:35:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:23 @agent_ppo2.py:193][0m |          -0.0014 |          25.7611 |           4.6249 |
[32m[20230117 13:35:23 @agent_ppo2.py:193][0m |          -0.0132 |          20.4032 |           4.6243 |
[32m[20230117 13:35:23 @agent_ppo2.py:193][0m |          -0.0010 |          18.7568 |           4.6258 |
[32m[20230117 13:35:23 @agent_ppo2.py:193][0m |          -0.0079 |          17.5485 |           4.6272 |
[32m[20230117 13:35:23 @agent_ppo2.py:193][0m |          -0.0057 |          16.9296 |           4.6260 |
[32m[20230117 13:35:23 @agent_ppo2.py:193][0m |          -0.0052 |          16.3159 |           4.6290 |
[32m[20230117 13:35:23 @agent_ppo2.py:193][0m |          -0.0127 |          16.5911 |           4.6237 |
[32m[20230117 13:35:23 @agent_ppo2.py:193][0m |          -0.0106 |          15.5284 |           4.6293 |
[32m[20230117 13:35:23 @agent_ppo2.py:193][0m |          -0.0132 |          14.7906 |           4.6312 |
[32m[20230117 13:35:23 @agent_ppo2.py:193][0m |          -0.0018 |          14.3230 |           4.6281 |
[32m[20230117 13:35:23 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:35:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.41
[32m[20230117 13:35:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.77
[32m[20230117 13:35:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.97
[32m[20230117 13:35:23 @agent_ppo2.py:151][0m Total time:       8.97 min
[32m[20230117 13:35:23 @agent_ppo2.py:153][0m 825344 total steps have happened
[32m[20230117 13:35:23 @agent_ppo2.py:129][0m #------------------------ Iteration 403 --------------------------#
[32m[20230117 13:35:24 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:24 @agent_ppo2.py:193][0m |           0.0009 |          23.9952 |           4.7680 |
[32m[20230117 13:35:24 @agent_ppo2.py:193][0m |          -0.0052 |          20.5018 |           4.7666 |
[32m[20230117 13:35:24 @agent_ppo2.py:193][0m |          -0.0073 |          19.5529 |           4.7605 |
[32m[20230117 13:35:24 @agent_ppo2.py:193][0m |          -0.0086 |          19.0352 |           4.7616 |
[32m[20230117 13:35:24 @agent_ppo2.py:193][0m |          -0.0098 |          18.7839 |           4.7657 |
[32m[20230117 13:35:24 @agent_ppo2.py:193][0m |          -0.0114 |          18.4669 |           4.7662 |
[32m[20230117 13:35:24 @agent_ppo2.py:193][0m |          -0.0120 |          18.2287 |           4.7652 |
[32m[20230117 13:35:24 @agent_ppo2.py:193][0m |          -0.0123 |          18.0459 |           4.7668 |
[32m[20230117 13:35:24 @agent_ppo2.py:193][0m |          -0.0126 |          17.9485 |           4.7658 |
[32m[20230117 13:35:24 @agent_ppo2.py:193][0m |          -0.0136 |          17.7050 |           4.7656 |
[32m[20230117 13:35:24 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:35:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.25
[32m[20230117 13:35:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.33
[32m[20230117 13:35:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.81
[32m[20230117 13:35:25 @agent_ppo2.py:151][0m Total time:       8.99 min
[32m[20230117 13:35:25 @agent_ppo2.py:153][0m 827392 total steps have happened
[32m[20230117 13:35:25 @agent_ppo2.py:129][0m #------------------------ Iteration 404 --------------------------#
[32m[20230117 13:35:25 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:25 @agent_ppo2.py:193][0m |          -0.0020 |          17.8066 |           4.7592 |
[32m[20230117 13:35:25 @agent_ppo2.py:193][0m |          -0.0064 |          15.6964 |           4.7524 |
[32m[20230117 13:35:25 @agent_ppo2.py:193][0m |          -0.0061 |          14.9518 |           4.7496 |
[32m[20230117 13:35:25 @agent_ppo2.py:193][0m |           0.0030 |          16.6075 |           4.7469 |
[32m[20230117 13:35:25 @agent_ppo2.py:193][0m |          -0.0102 |          13.9750 |           4.7472 |
[32m[20230117 13:35:25 @agent_ppo2.py:193][0m |          -0.0121 |          13.6485 |           4.7458 |
[32m[20230117 13:35:25 @agent_ppo2.py:193][0m |          -0.0109 |          13.3585 |           4.7476 |
[32m[20230117 13:35:25 @agent_ppo2.py:193][0m |          -0.0125 |          13.1237 |           4.7478 |
[32m[20230117 13:35:26 @agent_ppo2.py:193][0m |          -0.0110 |          13.0012 |           4.7440 |
[32m[20230117 13:35:26 @agent_ppo2.py:193][0m |          -0.0146 |          12.6969 |           4.7444 |
[32m[20230117 13:35:26 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.47
[32m[20230117 13:35:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.92
[32m[20230117 13:35:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.69
[32m[20230117 13:35:26 @agent_ppo2.py:151][0m Total time:       9.01 min
[32m[20230117 13:35:26 @agent_ppo2.py:153][0m 829440 total steps have happened
[32m[20230117 13:35:26 @agent_ppo2.py:129][0m #------------------------ Iteration 405 --------------------------#
[32m[20230117 13:35:26 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:35:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:26 @agent_ppo2.py:193][0m |           0.0109 |          19.4552 |           4.7562 |
[32m[20230117 13:35:26 @agent_ppo2.py:193][0m |          -0.0132 |          16.6264 |           4.7549 |
[32m[20230117 13:35:26 @agent_ppo2.py:193][0m |           0.0028 |          15.7229 |           4.7535 |
[32m[20230117 13:35:26 @agent_ppo2.py:193][0m |          -0.0126 |          15.1057 |           4.7536 |
[32m[20230117 13:35:26 @agent_ppo2.py:193][0m |          -0.0090 |          14.7529 |           4.7582 |
[32m[20230117 13:35:27 @agent_ppo2.py:193][0m |           0.0012 |          14.6318 |           4.7521 |
[32m[20230117 13:35:27 @agent_ppo2.py:193][0m |           0.0046 |          14.6392 |           4.7581 |
[32m[20230117 13:35:27 @agent_ppo2.py:193][0m |          -0.0080 |          14.0934 |           4.7571 |
[32m[20230117 13:35:27 @agent_ppo2.py:193][0m |          -0.0095 |          13.9112 |           4.7631 |
[32m[20230117 13:35:27 @agent_ppo2.py:193][0m |          -0.0124 |          13.6821 |           4.7644 |
[32m[20230117 13:35:27 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:35:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 259.01
[32m[20230117 13:35:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.61
[32m[20230117 13:35:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.21
[32m[20230117 13:35:27 @agent_ppo2.py:151][0m Total time:       9.03 min
[32m[20230117 13:35:27 @agent_ppo2.py:153][0m 831488 total steps have happened
[32m[20230117 13:35:27 @agent_ppo2.py:129][0m #------------------------ Iteration 406 --------------------------#
[32m[20230117 13:35:27 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:35:27 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:27 @agent_ppo2.py:193][0m |           0.0009 |          19.3853 |           4.8892 |
[32m[20230117 13:35:27 @agent_ppo2.py:193][0m |          -0.0056 |          17.5615 |           4.8861 |
[32m[20230117 13:35:27 @agent_ppo2.py:193][0m |          -0.0078 |          16.6849 |           4.8817 |
[32m[20230117 13:35:28 @agent_ppo2.py:193][0m |          -0.0052 |          16.2973 |           4.8783 |
[32m[20230117 13:35:28 @agent_ppo2.py:193][0m |          -0.0081 |          15.5535 |           4.8729 |
[32m[20230117 13:35:28 @agent_ppo2.py:193][0m |          -0.0095 |          14.8803 |           4.8754 |
[32m[20230117 13:35:28 @agent_ppo2.py:193][0m |          -0.0088 |          14.5717 |           4.8727 |
[32m[20230117 13:35:28 @agent_ppo2.py:193][0m |          -0.0103 |          13.9349 |           4.8703 |
[32m[20230117 13:35:28 @agent_ppo2.py:193][0m |          -0.0105 |          13.5833 |           4.8698 |
[32m[20230117 13:35:28 @agent_ppo2.py:193][0m |          -0.0089 |          13.6025 |           4.8684 |
[32m[20230117 13:35:28 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.23
[32m[20230117 13:35:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.57
[32m[20230117 13:35:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.41
[32m[20230117 13:35:28 @agent_ppo2.py:151][0m Total time:       9.05 min
[32m[20230117 13:35:28 @agent_ppo2.py:153][0m 833536 total steps have happened
[32m[20230117 13:35:28 @agent_ppo2.py:129][0m #------------------------ Iteration 407 --------------------------#
[32m[20230117 13:35:28 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:28 @agent_ppo2.py:193][0m |           0.0001 |          19.7487 |           4.8203 |
[32m[20230117 13:35:29 @agent_ppo2.py:193][0m |          -0.0054 |          18.3284 |           4.8067 |
[32m[20230117 13:35:29 @agent_ppo2.py:193][0m |          -0.0072 |          17.6592 |           4.8060 |
[32m[20230117 13:35:29 @agent_ppo2.py:193][0m |          -0.0084 |          17.2796 |           4.8110 |
[32m[20230117 13:35:29 @agent_ppo2.py:193][0m |          -0.0102 |          16.9647 |           4.8069 |
[32m[20230117 13:35:29 @agent_ppo2.py:193][0m |          -0.0107 |          16.8193 |           4.8081 |
[32m[20230117 13:35:29 @agent_ppo2.py:193][0m |          -0.0119 |          16.6201 |           4.7996 |
[32m[20230117 13:35:29 @agent_ppo2.py:193][0m |          -0.0120 |          16.5547 |           4.8034 |
[32m[20230117 13:35:29 @agent_ppo2.py:193][0m |          -0.0130 |          16.3405 |           4.8035 |
[32m[20230117 13:35:29 @agent_ppo2.py:193][0m |          -0.0135 |          16.1981 |           4.8021 |
[32m[20230117 13:35:29 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:35:29 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.54
[32m[20230117 13:35:29 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.46
[32m[20230117 13:35:29 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.03
[32m[20230117 13:35:29 @agent_ppo2.py:151][0m Total time:       9.07 min
[32m[20230117 13:35:29 @agent_ppo2.py:153][0m 835584 total steps have happened
[32m[20230117 13:35:29 @agent_ppo2.py:129][0m #------------------------ Iteration 408 --------------------------#
[32m[20230117 13:35:29 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:30 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:30 @agent_ppo2.py:193][0m |          -0.0006 |          17.9544 |           4.8913 |
[32m[20230117 13:35:30 @agent_ppo2.py:193][0m |          -0.0058 |          16.3235 |           4.8799 |
[32m[20230117 13:35:30 @agent_ppo2.py:193][0m |          -0.0080 |          15.5219 |           4.8815 |
[32m[20230117 13:35:30 @agent_ppo2.py:193][0m |          -0.0089 |          15.0834 |           4.8822 |
[32m[20230117 13:35:30 @agent_ppo2.py:193][0m |          -0.0097 |          14.6797 |           4.8793 |
[32m[20230117 13:35:30 @agent_ppo2.py:193][0m |          -0.0098 |          14.4166 |           4.8768 |
[32m[20230117 13:35:30 @agent_ppo2.py:193][0m |          -0.0111 |          14.1579 |           4.8758 |
[32m[20230117 13:35:30 @agent_ppo2.py:193][0m |          -0.0113 |          14.0228 |           4.8731 |
[32m[20230117 13:35:30 @agent_ppo2.py:193][0m |          -0.0120 |          13.8080 |           4.8742 |
[32m[20230117 13:35:30 @agent_ppo2.py:193][0m |          -0.0128 |          13.6271 |           4.8692 |
[32m[20230117 13:35:30 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.37
[32m[20230117 13:35:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.43
[32m[20230117 13:35:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.98
[32m[20230117 13:35:30 @agent_ppo2.py:151][0m Total time:       9.09 min
[32m[20230117 13:35:30 @agent_ppo2.py:153][0m 837632 total steps have happened
[32m[20230117 13:35:30 @agent_ppo2.py:129][0m #------------------------ Iteration 409 --------------------------#
[32m[20230117 13:35:31 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:31 @agent_ppo2.py:193][0m |          -0.0004 |          18.8769 |           4.8260 |
[32m[20230117 13:35:31 @agent_ppo2.py:193][0m |          -0.0037 |          16.9237 |           4.8260 |
[32m[20230117 13:35:31 @agent_ppo2.py:193][0m |          -0.0079 |          15.9292 |           4.8234 |
[32m[20230117 13:35:31 @agent_ppo2.py:193][0m |          -0.0101 |          15.3951 |           4.8238 |
[32m[20230117 13:35:31 @agent_ppo2.py:193][0m |          -0.0107 |          14.9737 |           4.8220 |
[32m[20230117 13:35:31 @agent_ppo2.py:193][0m |          -0.0100 |          14.5804 |           4.8197 |
[32m[20230117 13:35:31 @agent_ppo2.py:193][0m |          -0.0113 |          14.3800 |           4.8233 |
[32m[20230117 13:35:31 @agent_ppo2.py:193][0m |          -0.0131 |          13.9479 |           4.8216 |
[32m[20230117 13:35:31 @agent_ppo2.py:193][0m |          -0.0138 |          13.6902 |           4.8203 |
[32m[20230117 13:35:31 @agent_ppo2.py:193][0m |          -0.0149 |          13.2814 |           4.8193 |
[32m[20230117 13:35:31 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.53
[32m[20230117 13:35:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.24
[32m[20230117 13:35:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.06
[32m[20230117 13:35:32 @agent_ppo2.py:151][0m Total time:       9.11 min
[32m[20230117 13:35:32 @agent_ppo2.py:153][0m 839680 total steps have happened
[32m[20230117 13:35:32 @agent_ppo2.py:129][0m #------------------------ Iteration 410 --------------------------#
[32m[20230117 13:35:32 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:35:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:32 @agent_ppo2.py:193][0m |           0.0002 |          19.0138 |           4.8540 |
[32m[20230117 13:35:32 @agent_ppo2.py:193][0m |          -0.0073 |          17.3027 |           4.8473 |
[32m[20230117 13:35:32 @agent_ppo2.py:193][0m |          -0.0087 |          16.4604 |           4.8396 |
[32m[20230117 13:35:32 @agent_ppo2.py:193][0m |          -0.0108 |          15.8174 |           4.8424 |
[32m[20230117 13:35:32 @agent_ppo2.py:193][0m |          -0.0105 |          15.3889 |           4.8371 |
[32m[20230117 13:35:32 @agent_ppo2.py:193][0m |          -0.0116 |          15.0312 |           4.8364 |
[32m[20230117 13:35:32 @agent_ppo2.py:193][0m |          -0.0027 |          15.6159 |           4.8347 |
[32m[20230117 13:35:32 @agent_ppo2.py:193][0m |          -0.0148 |          14.2703 |           4.8360 |
[32m[20230117 13:35:32 @agent_ppo2.py:193][0m |          -0.0137 |          13.9605 |           4.8315 |
[32m[20230117 13:35:32 @agent_ppo2.py:193][0m |          -0.0125 |          13.7592 |           4.8288 |
[32m[20230117 13:35:32 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.81
[32m[20230117 13:35:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.31
[32m[20230117 13:35:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.00
[32m[20230117 13:35:33 @agent_ppo2.py:151][0m Total time:       9.13 min
[32m[20230117 13:35:33 @agent_ppo2.py:153][0m 841728 total steps have happened
[32m[20230117 13:35:33 @agent_ppo2.py:129][0m #------------------------ Iteration 411 --------------------------#
[32m[20230117 13:35:33 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:35:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:33 @agent_ppo2.py:193][0m |           0.0004 |          16.6580 |           4.7878 |
[32m[20230117 13:35:33 @agent_ppo2.py:193][0m |          -0.0028 |           9.4398 |           4.7836 |
[32m[20230117 13:35:33 @agent_ppo2.py:193][0m |          -0.0046 |           7.4662 |           4.7808 |
[32m[20230117 13:35:33 @agent_ppo2.py:193][0m |          -0.0065 |           6.3643 |           4.7800 |
[32m[20230117 13:35:33 @agent_ppo2.py:193][0m |          -0.0074 |           5.7167 |           4.7815 |
[32m[20230117 13:35:33 @agent_ppo2.py:193][0m |          -0.0066 |           5.2139 |           4.7781 |
[32m[20230117 13:35:33 @agent_ppo2.py:193][0m |          -0.0082 |           4.8340 |           4.7756 |
[32m[20230117 13:35:34 @agent_ppo2.py:193][0m |          -0.0091 |           4.5037 |           4.7794 |
[32m[20230117 13:35:34 @agent_ppo2.py:193][0m |          -0.0105 |           4.3241 |           4.7767 |
[32m[20230117 13:35:34 @agent_ppo2.py:193][0m |          -0.0089 |           4.0825 |           4.7815 |
[32m[20230117 13:35:34 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:35:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.92
[32m[20230117 13:35:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.85
[32m[20230117 13:35:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.19
[32m[20230117 13:35:34 @agent_ppo2.py:151][0m Total time:       9.15 min
[32m[20230117 13:35:34 @agent_ppo2.py:153][0m 843776 total steps have happened
[32m[20230117 13:35:34 @agent_ppo2.py:129][0m #------------------------ Iteration 412 --------------------------#
[32m[20230117 13:35:34 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:34 @agent_ppo2.py:193][0m |          -0.0006 |          20.1082 |           4.8618 |
[32m[20230117 13:35:34 @agent_ppo2.py:193][0m |          -0.0055 |          17.8586 |           4.8561 |
[32m[20230117 13:35:34 @agent_ppo2.py:193][0m |          -0.0077 |          17.0751 |           4.8483 |
[32m[20230117 13:35:34 @agent_ppo2.py:193][0m |          -0.0046 |          17.0169 |           4.8450 |
[32m[20230117 13:35:34 @agent_ppo2.py:193][0m |          -0.0095 |          16.3688 |           4.8440 |
[32m[20230117 13:35:34 @agent_ppo2.py:193][0m |          -0.0097 |          16.0476 |           4.8414 |
[32m[20230117 13:35:35 @agent_ppo2.py:193][0m |          -0.0111 |          15.8314 |           4.8430 |
[32m[20230117 13:35:35 @agent_ppo2.py:193][0m |          -0.0127 |          15.6147 |           4.8359 |
[32m[20230117 13:35:35 @agent_ppo2.py:193][0m |          -0.0127 |          15.5078 |           4.8427 |
[32m[20230117 13:35:35 @agent_ppo2.py:193][0m |          -0.0127 |          15.2326 |           4.8381 |
[32m[20230117 13:35:35 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.57
[32m[20230117 13:35:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.55
[32m[20230117 13:35:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.61
[32m[20230117 13:35:35 @agent_ppo2.py:151][0m Total time:       9.17 min
[32m[20230117 13:35:35 @agent_ppo2.py:153][0m 845824 total steps have happened
[32m[20230117 13:35:35 @agent_ppo2.py:129][0m #------------------------ Iteration 413 --------------------------#
[32m[20230117 13:35:35 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:35:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:35 @agent_ppo2.py:193][0m |           0.0008 |          17.4439 |           4.8544 |
[32m[20230117 13:35:35 @agent_ppo2.py:193][0m |          -0.0040 |          15.5823 |           4.8460 |
[32m[20230117 13:35:35 @agent_ppo2.py:193][0m |          -0.0063 |          14.6028 |           4.8443 |
[32m[20230117 13:35:36 @agent_ppo2.py:193][0m |          -0.0073 |          14.0019 |           4.8425 |
[32m[20230117 13:35:36 @agent_ppo2.py:193][0m |          -0.0085 |          13.5962 |           4.8465 |
[32m[20230117 13:35:36 @agent_ppo2.py:193][0m |          -0.0087 |          13.3225 |           4.8424 |
[32m[20230117 13:35:36 @agent_ppo2.py:193][0m |          -0.0094 |          13.1043 |           4.8453 |
[32m[20230117 13:35:36 @agent_ppo2.py:193][0m |          -0.0089 |          12.8736 |           4.8385 |
[32m[20230117 13:35:36 @agent_ppo2.py:193][0m |          -0.0109 |          12.6730 |           4.8443 |
[32m[20230117 13:35:36 @agent_ppo2.py:193][0m |          -0.0111 |          12.5527 |           4.8438 |
[32m[20230117 13:35:36 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.45
[32m[20230117 13:35:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.72
[32m[20230117 13:35:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.97
[32m[20230117 13:35:36 @agent_ppo2.py:151][0m Total time:       9.19 min
[32m[20230117 13:35:36 @agent_ppo2.py:153][0m 847872 total steps have happened
[32m[20230117 13:35:36 @agent_ppo2.py:129][0m #------------------------ Iteration 414 --------------------------#
[32m[20230117 13:35:36 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:36 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:36 @agent_ppo2.py:193][0m |          -0.0031 |          18.8549 |           4.7101 |
[32m[20230117 13:35:37 @agent_ppo2.py:193][0m |           0.0002 |          16.8471 |           4.7039 |
[32m[20230117 13:35:37 @agent_ppo2.py:193][0m |           0.0025 |          16.0983 |           4.7035 |
[32m[20230117 13:35:37 @agent_ppo2.py:193][0m |          -0.0072 |          14.1179 |           4.7047 |
[32m[20230117 13:35:37 @agent_ppo2.py:193][0m |          -0.0091 |          13.4268 |           4.7007 |
[32m[20230117 13:35:37 @agent_ppo2.py:193][0m |          -0.0091 |          13.1228 |           4.6998 |
[32m[20230117 13:35:37 @agent_ppo2.py:193][0m |          -0.0106 |          12.6836 |           4.6997 |
[32m[20230117 13:35:37 @agent_ppo2.py:193][0m |          -0.0120 |          12.3629 |           4.6980 |
[32m[20230117 13:35:37 @agent_ppo2.py:193][0m |          -0.0103 |          12.3884 |           4.6967 |
[32m[20230117 13:35:37 @agent_ppo2.py:193][0m |          -0.0127 |          11.8494 |           4.6947 |
[32m[20230117 13:35:37 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.83
[32m[20230117 13:35:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.70
[32m[20230117 13:35:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.54
[32m[20230117 13:35:37 @agent_ppo2.py:151][0m Total time:       9.20 min
[32m[20230117 13:35:37 @agent_ppo2.py:153][0m 849920 total steps have happened
[32m[20230117 13:35:37 @agent_ppo2.py:129][0m #------------------------ Iteration 415 --------------------------#
[32m[20230117 13:35:37 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:38 @agent_ppo2.py:193][0m |           0.0027 |          18.9808 |           4.7767 |
[32m[20230117 13:35:38 @agent_ppo2.py:193][0m |          -0.0022 |          16.6982 |           4.7623 |
[32m[20230117 13:35:38 @agent_ppo2.py:193][0m |          -0.0069 |          15.3151 |           4.7694 |
[32m[20230117 13:35:38 @agent_ppo2.py:193][0m |          -0.0097 |          14.5473 |           4.7677 |
[32m[20230117 13:35:38 @agent_ppo2.py:193][0m |          -0.0090 |          14.1865 |           4.7696 |
[32m[20230117 13:35:38 @agent_ppo2.py:193][0m |          -0.0115 |          13.6509 |           4.7714 |
[32m[20230117 13:35:38 @agent_ppo2.py:193][0m |          -0.0100 |          13.3230 |           4.7697 |
[32m[20230117 13:35:38 @agent_ppo2.py:193][0m |          -0.0124 |          12.8319 |           4.7689 |
[32m[20230117 13:35:38 @agent_ppo2.py:193][0m |          -0.0134 |          12.5779 |           4.7697 |
[32m[20230117 13:35:38 @agent_ppo2.py:193][0m |          -0.0145 |          12.3317 |           4.7717 |
[32m[20230117 13:35:38 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:38 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.69
[32m[20230117 13:35:38 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.06
[32m[20230117 13:35:38 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 275.46
[32m[20230117 13:35:38 @agent_ppo2.py:151][0m Total time:       9.22 min
[32m[20230117 13:35:38 @agent_ppo2.py:153][0m 851968 total steps have happened
[32m[20230117 13:35:38 @agent_ppo2.py:129][0m #------------------------ Iteration 416 --------------------------#
[32m[20230117 13:35:39 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:39 @agent_ppo2.py:193][0m |           0.0027 |          27.6177 |           4.9057 |
[32m[20230117 13:35:39 @agent_ppo2.py:193][0m |          -0.0053 |          18.0965 |           4.9001 |
[32m[20230117 13:35:39 @agent_ppo2.py:193][0m |          -0.0076 |          15.7882 |           4.8947 |
[32m[20230117 13:35:39 @agent_ppo2.py:193][0m |          -0.0082 |          14.5110 |           4.8975 |
[32m[20230117 13:35:39 @agent_ppo2.py:193][0m |          -0.0077 |          13.4846 |           4.8932 |
[32m[20230117 13:35:39 @agent_ppo2.py:193][0m |          -0.0116 |          12.7678 |           4.8898 |
[32m[20230117 13:35:39 @agent_ppo2.py:193][0m |          -0.0113 |          12.1136 |           4.8914 |
[32m[20230117 13:35:39 @agent_ppo2.py:193][0m |          -0.0127 |          11.6374 |           4.8892 |
[32m[20230117 13:35:39 @agent_ppo2.py:193][0m |          -0.0114 |          11.2387 |           4.8885 |
[32m[20230117 13:35:39 @agent_ppo2.py:193][0m |          -0.0139 |          10.7387 |           4.8847 |
[32m[20230117 13:35:39 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:35:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 189.64
[32m[20230117 13:35:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.27
[32m[20230117 13:35:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.57
[32m[20230117 13:35:40 @agent_ppo2.py:151][0m Total time:       9.24 min
[32m[20230117 13:35:40 @agent_ppo2.py:153][0m 854016 total steps have happened
[32m[20230117 13:35:40 @agent_ppo2.py:129][0m #------------------------ Iteration 417 --------------------------#
[32m[20230117 13:35:40 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:40 @agent_ppo2.py:193][0m |          -0.0002 |          22.6496 |           4.8030 |
[32m[20230117 13:35:40 @agent_ppo2.py:193][0m |          -0.0091 |          19.4064 |           4.8007 |
[32m[20230117 13:35:40 @agent_ppo2.py:193][0m |          -0.0064 |          18.8950 |           4.7989 |
[32m[20230117 13:35:40 @agent_ppo2.py:193][0m |          -0.0098 |          17.7736 |           4.7935 |
[32m[20230117 13:35:40 @agent_ppo2.py:193][0m |          -0.0103 |          17.2300 |           4.7952 |
[32m[20230117 13:35:40 @agent_ppo2.py:193][0m |          -0.0110 |          17.0631 |           4.7917 |
[32m[20230117 13:35:40 @agent_ppo2.py:193][0m |          -0.0103 |          16.7025 |           4.7879 |
[32m[20230117 13:35:40 @agent_ppo2.py:193][0m |          -0.0163 |          16.1236 |           4.7860 |
[32m[20230117 13:35:40 @agent_ppo2.py:193][0m |          -0.0108 |          16.2056 |           4.7865 |
[32m[20230117 13:35:41 @agent_ppo2.py:193][0m |          -0.0158 |          15.5921 |           4.7851 |
[32m[20230117 13:35:41 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.44
[32m[20230117 13:35:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.87
[32m[20230117 13:35:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.21
[32m[20230117 13:35:41 @agent_ppo2.py:151][0m Total time:       9.26 min
[32m[20230117 13:35:41 @agent_ppo2.py:153][0m 856064 total steps have happened
[32m[20230117 13:35:41 @agent_ppo2.py:129][0m #------------------------ Iteration 418 --------------------------#
[32m[20230117 13:35:41 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:41 @agent_ppo2.py:193][0m |           0.0001 |          19.4427 |           4.7847 |
[32m[20230117 13:35:41 @agent_ppo2.py:193][0m |          -0.0063 |          17.4035 |           4.7862 |
[32m[20230117 13:35:41 @agent_ppo2.py:193][0m |          -0.0073 |          16.3594 |           4.7811 |
[32m[20230117 13:35:41 @agent_ppo2.py:193][0m |          -0.0092 |          15.4657 |           4.7787 |
[32m[20230117 13:35:41 @agent_ppo2.py:193][0m |          -0.0104 |          14.7026 |           4.7755 |
[32m[20230117 13:35:41 @agent_ppo2.py:193][0m |          -0.0107 |          14.1794 |           4.7757 |
[32m[20230117 13:35:41 @agent_ppo2.py:193][0m |          -0.0111 |          13.7047 |           4.7741 |
[32m[20230117 13:35:42 @agent_ppo2.py:193][0m |          -0.0116 |          13.5728 |           4.7722 |
[32m[20230117 13:35:42 @agent_ppo2.py:193][0m |          -0.0129 |          13.1695 |           4.7737 |
[32m[20230117 13:35:42 @agent_ppo2.py:193][0m |          -0.0134 |          12.8710 |           4.7708 |
[32m[20230117 13:35:42 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:35:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.05
[32m[20230117 13:35:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.67
[32m[20230117 13:35:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.50
[32m[20230117 13:35:42 @agent_ppo2.py:151][0m Total time:       9.28 min
[32m[20230117 13:35:42 @agent_ppo2.py:153][0m 858112 total steps have happened
[32m[20230117 13:35:42 @agent_ppo2.py:129][0m #------------------------ Iteration 419 --------------------------#
[32m[20230117 13:35:42 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:42 @agent_ppo2.py:193][0m |          -0.0019 |          27.0436 |           4.7664 |
[32m[20230117 13:35:42 @agent_ppo2.py:193][0m |          -0.0088 |          14.6632 |           4.7597 |
[32m[20230117 13:35:42 @agent_ppo2.py:193][0m |          -0.0105 |          12.3587 |           4.7670 |
[32m[20230117 13:35:42 @agent_ppo2.py:193][0m |          -0.0132 |          11.1976 |           4.7586 |
[32m[20230117 13:35:42 @agent_ppo2.py:193][0m |          -0.0142 |          10.3856 |           4.7637 |
[32m[20230117 13:35:42 @agent_ppo2.py:193][0m |          -0.0151 |           9.7160 |           4.7628 |
[32m[20230117 13:35:43 @agent_ppo2.py:193][0m |          -0.0164 |           9.2540 |           4.7602 |
[32m[20230117 13:35:43 @agent_ppo2.py:193][0m |          -0.0174 |           8.7254 |           4.7589 |
[32m[20230117 13:35:43 @agent_ppo2.py:193][0m |          -0.0184 |           8.4155 |           4.7638 |
[32m[20230117 13:35:43 @agent_ppo2.py:193][0m |          -0.0190 |           8.0167 |           4.7632 |
[32m[20230117 13:35:43 @agent_ppo2.py:138][0m Policy update time: 0.69 s
[32m[20230117 13:35:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 209.68
[32m[20230117 13:35:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.79
[32m[20230117 13:35:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.84
[32m[20230117 13:35:43 @agent_ppo2.py:151][0m Total time:       9.30 min
[32m[20230117 13:35:43 @agent_ppo2.py:153][0m 860160 total steps have happened
[32m[20230117 13:35:43 @agent_ppo2.py:129][0m #------------------------ Iteration 420 --------------------------#
[32m[20230117 13:35:43 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:35:43 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:43 @agent_ppo2.py:193][0m |          -0.0004 |          19.7474 |           4.7963 |
[32m[20230117 13:35:43 @agent_ppo2.py:193][0m |          -0.0080 |          18.0180 |           4.7956 |
[32m[20230117 13:35:43 @agent_ppo2.py:193][0m |          -0.0082 |          17.6393 |           4.7942 |
[32m[20230117 13:35:43 @agent_ppo2.py:193][0m |          -0.0105 |          17.2007 |           4.7962 |
[32m[20230117 13:35:44 @agent_ppo2.py:193][0m |          -0.0120 |          16.8839 |           4.7902 |
[32m[20230117 13:35:44 @agent_ppo2.py:193][0m |          -0.0130 |          16.6963 |           4.7966 |
[32m[20230117 13:35:44 @agent_ppo2.py:193][0m |          -0.0148 |          16.5156 |           4.7933 |
[32m[20230117 13:35:44 @agent_ppo2.py:193][0m |          -0.0163 |          16.2894 |           4.7960 |
[32m[20230117 13:35:44 @agent_ppo2.py:193][0m |          -0.0155 |          16.0973 |           4.7960 |
[32m[20230117 13:35:44 @agent_ppo2.py:193][0m |          -0.0150 |          15.9096 |           4.7935 |
[32m[20230117 13:35:44 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.43
[32m[20230117 13:35:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.46
[32m[20230117 13:35:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 90.08
[32m[20230117 13:35:44 @agent_ppo2.py:151][0m Total time:       9.32 min
[32m[20230117 13:35:44 @agent_ppo2.py:153][0m 862208 total steps have happened
[32m[20230117 13:35:44 @agent_ppo2.py:129][0m #------------------------ Iteration 421 --------------------------#
[32m[20230117 13:35:44 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:35:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:44 @agent_ppo2.py:193][0m |          -0.0011 |          21.4313 |           4.8962 |
[32m[20230117 13:35:44 @agent_ppo2.py:193][0m |          -0.0065 |          19.5631 |           4.8933 |
[32m[20230117 13:35:45 @agent_ppo2.py:193][0m |          -0.0095 |          18.6341 |           4.8959 |
[32m[20230117 13:35:45 @agent_ppo2.py:193][0m |          -0.0100 |          17.7272 |           4.8915 |
[32m[20230117 13:35:45 @agent_ppo2.py:193][0m |          -0.0110 |          17.0417 |           4.8937 |
[32m[20230117 13:35:45 @agent_ppo2.py:193][0m |          -0.0128 |          16.3263 |           4.8959 |
[32m[20230117 13:35:45 @agent_ppo2.py:193][0m |          -0.0130 |          16.0694 |           4.8933 |
[32m[20230117 13:35:45 @agent_ppo2.py:193][0m |          -0.0143 |          15.7352 |           4.8950 |
[32m[20230117 13:35:45 @agent_ppo2.py:193][0m |          -0.0149 |          15.4822 |           4.8940 |
[32m[20230117 13:35:45 @agent_ppo2.py:193][0m |          -0.0149 |          15.2804 |           4.8904 |
[32m[20230117 13:35:45 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:45 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.44
[32m[20230117 13:35:45 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.86
[32m[20230117 13:35:45 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.21
[32m[20230117 13:35:45 @agent_ppo2.py:151][0m Total time:       9.34 min
[32m[20230117 13:35:45 @agent_ppo2.py:153][0m 864256 total steps have happened
[32m[20230117 13:35:45 @agent_ppo2.py:129][0m #------------------------ Iteration 422 --------------------------#
[32m[20230117 13:35:45 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:35:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:46 @agent_ppo2.py:193][0m |           0.0004 |          21.3872 |           4.8933 |
[32m[20230117 13:35:46 @agent_ppo2.py:193][0m |          -0.0031 |          19.2864 |           4.8868 |
[32m[20230117 13:35:46 @agent_ppo2.py:193][0m |          -0.0066 |          18.6299 |           4.8825 |
[32m[20230117 13:35:46 @agent_ppo2.py:193][0m |          -0.0091 |          17.9525 |           4.8813 |
[32m[20230117 13:35:46 @agent_ppo2.py:193][0m |          -0.0107 |          17.5792 |           4.8701 |
[32m[20230117 13:35:46 @agent_ppo2.py:193][0m |          -0.0125 |          17.2797 |           4.8732 |
[32m[20230117 13:35:46 @agent_ppo2.py:193][0m |          -0.0123 |          17.0770 |           4.8729 |
[32m[20230117 13:35:46 @agent_ppo2.py:193][0m |          -0.0145 |          16.8901 |           4.8736 |
[32m[20230117 13:35:46 @agent_ppo2.py:193][0m |          -0.0127 |          16.7103 |           4.8742 |
[32m[20230117 13:35:46 @agent_ppo2.py:193][0m |          -0.0157 |          16.5493 |           4.8700 |
[32m[20230117 13:35:46 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:35:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 259.67
[32m[20230117 13:35:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 262.83
[32m[20230117 13:35:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.69
[32m[20230117 13:35:46 @agent_ppo2.py:151][0m Total time:       9.36 min
[32m[20230117 13:35:46 @agent_ppo2.py:153][0m 866304 total steps have happened
[32m[20230117 13:35:46 @agent_ppo2.py:129][0m #------------------------ Iteration 423 --------------------------#
[32m[20230117 13:35:47 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:35:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:47 @agent_ppo2.py:193][0m |          -0.0015 |          19.1535 |           4.9087 |
[32m[20230117 13:35:47 @agent_ppo2.py:193][0m |          -0.0066 |          16.6959 |           4.8954 |
[32m[20230117 13:35:47 @agent_ppo2.py:193][0m |          -0.0086 |          15.8512 |           4.8888 |
[32m[20230117 13:35:47 @agent_ppo2.py:193][0m |          -0.0108 |          15.3439 |           4.8861 |
[32m[20230117 13:35:47 @agent_ppo2.py:193][0m |          -0.0102 |          14.9982 |           4.8797 |
[32m[20230117 13:35:47 @agent_ppo2.py:193][0m |          -0.0106 |          14.8181 |           4.8841 |
[32m[20230117 13:35:47 @agent_ppo2.py:193][0m |          -0.0128 |          14.5271 |           4.8732 |
[32m[20230117 13:35:47 @agent_ppo2.py:193][0m |          -0.0130 |          14.1444 |           4.8698 |
[32m[20230117 13:35:47 @agent_ppo2.py:193][0m |          -0.0155 |          13.9391 |           4.8690 |
[32m[20230117 13:35:47 @agent_ppo2.py:193][0m |          -0.0164 |          13.7624 |           4.8722 |
[32m[20230117 13:35:47 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:35:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 261.70
[32m[20230117 13:35:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.24
[32m[20230117 13:35:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.25
[32m[20230117 13:35:48 @agent_ppo2.py:151][0m Total time:       9.38 min
[32m[20230117 13:35:48 @agent_ppo2.py:153][0m 868352 total steps have happened
[32m[20230117 13:35:48 @agent_ppo2.py:129][0m #------------------------ Iteration 424 --------------------------#
[32m[20230117 13:35:48 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:48 @agent_ppo2.py:193][0m |           0.0018 |          18.6653 |           4.8112 |
[32m[20230117 13:35:48 @agent_ppo2.py:193][0m |          -0.0043 |          15.9072 |           4.7999 |
[32m[20230117 13:35:48 @agent_ppo2.py:193][0m |          -0.0047 |          14.7564 |           4.7979 |
[32m[20230117 13:35:48 @agent_ppo2.py:193][0m |          -0.0068 |          13.9043 |           4.8004 |
[32m[20230117 13:35:48 @agent_ppo2.py:193][0m |          -0.0079 |          13.3008 |           4.7989 |
[32m[20230117 13:35:48 @agent_ppo2.py:193][0m |          -0.0091 |          12.7420 |           4.7994 |
[32m[20230117 13:35:48 @agent_ppo2.py:193][0m |          -0.0086 |          12.1935 |           4.7993 |
[32m[20230117 13:35:48 @agent_ppo2.py:193][0m |          -0.0105 |          11.7200 |           4.7990 |
[32m[20230117 13:35:48 @agent_ppo2.py:193][0m |          -0.0117 |          11.0876 |           4.7991 |
[32m[20230117 13:35:48 @agent_ppo2.py:193][0m |          -0.0100 |          10.7114 |           4.7946 |
[32m[20230117 13:35:48 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.58
[32m[20230117 13:35:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.94
[32m[20230117 13:35:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.52
[32m[20230117 13:35:49 @agent_ppo2.py:151][0m Total time:       9.39 min
[32m[20230117 13:35:49 @agent_ppo2.py:153][0m 870400 total steps have happened
[32m[20230117 13:35:49 @agent_ppo2.py:129][0m #------------------------ Iteration 425 --------------------------#
[32m[20230117 13:35:49 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:35:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:49 @agent_ppo2.py:193][0m |           0.0003 |          19.2960 |           4.8848 |
[32m[20230117 13:35:49 @agent_ppo2.py:193][0m |          -0.0057 |          17.6478 |           4.8814 |
[32m[20230117 13:35:49 @agent_ppo2.py:193][0m |          -0.0084 |          17.0825 |           4.8740 |
[32m[20230117 13:35:49 @agent_ppo2.py:193][0m |          -0.0085 |          16.6384 |           4.8746 |
[32m[20230117 13:35:49 @agent_ppo2.py:193][0m |          -0.0111 |          16.2440 |           4.8703 |
[32m[20230117 13:35:49 @agent_ppo2.py:193][0m |          -0.0120 |          16.0358 |           4.8759 |
[32m[20230117 13:35:49 @agent_ppo2.py:193][0m |          -0.0119 |          15.8097 |           4.8685 |
[32m[20230117 13:35:49 @agent_ppo2.py:193][0m |          -0.0130 |          15.5014 |           4.8694 |
[32m[20230117 13:35:50 @agent_ppo2.py:193][0m |          -0.0136 |          15.3733 |           4.8696 |
[32m[20230117 13:35:50 @agent_ppo2.py:193][0m |          -0.0150 |          15.0539 |           4.8667 |
[32m[20230117 13:35:50 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:35:50 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.96
[32m[20230117 13:35:50 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.93
[32m[20230117 13:35:50 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.83
[32m[20230117 13:35:50 @agent_ppo2.py:151][0m Total time:       9.41 min
[32m[20230117 13:35:50 @agent_ppo2.py:153][0m 872448 total steps have happened
[32m[20230117 13:35:50 @agent_ppo2.py:129][0m #------------------------ Iteration 426 --------------------------#
[32m[20230117 13:35:50 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:50 @agent_ppo2.py:193][0m |           0.0001 |          27.5491 |           4.8273 |
[32m[20230117 13:35:50 @agent_ppo2.py:193][0m |          -0.0066 |          18.9611 |           4.8163 |
[32m[20230117 13:35:50 @agent_ppo2.py:193][0m |          -0.0107 |          17.3705 |           4.8095 |
[32m[20230117 13:35:50 @agent_ppo2.py:193][0m |          -0.0125 |          16.5791 |           4.8085 |
[32m[20230117 13:35:50 @agent_ppo2.py:193][0m |          -0.0135 |          16.0312 |           4.8115 |
[32m[20230117 13:35:50 @agent_ppo2.py:193][0m |          -0.0136 |          15.5233 |           4.8084 |
[32m[20230117 13:35:50 @agent_ppo2.py:193][0m |          -0.0151 |          15.1692 |           4.8067 |
[32m[20230117 13:35:51 @agent_ppo2.py:193][0m |          -0.0170 |          14.7232 |           4.8030 |
[32m[20230117 13:35:51 @agent_ppo2.py:193][0m |          -0.0167 |          14.3715 |           4.8020 |
[32m[20230117 13:35:51 @agent_ppo2.py:193][0m |          -0.0179 |          14.1053 |           4.8010 |
[32m[20230117 13:35:51 @agent_ppo2.py:138][0m Policy update time: 0.69 s
[32m[20230117 13:35:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 221.42
[32m[20230117 13:35:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.48
[32m[20230117 13:35:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.74
[32m[20230117 13:35:51 @agent_ppo2.py:151][0m Total time:       9.43 min
[32m[20230117 13:35:51 @agent_ppo2.py:153][0m 874496 total steps have happened
[32m[20230117 13:35:51 @agent_ppo2.py:129][0m #------------------------ Iteration 427 --------------------------#
[32m[20230117 13:35:51 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:51 @agent_ppo2.py:193][0m |           0.0001 |          25.7638 |           4.8106 |
[32m[20230117 13:35:51 @agent_ppo2.py:193][0m |          -0.0073 |          17.2990 |           4.7996 |
[32m[20230117 13:35:51 @agent_ppo2.py:193][0m |          -0.0107 |          15.9362 |           4.7967 |
[32m[20230117 13:35:51 @agent_ppo2.py:193][0m |          -0.0096 |          15.3088 |           4.7921 |
[32m[20230117 13:35:51 @agent_ppo2.py:193][0m |          -0.0081 |          14.4525 |           4.7939 |
[32m[20230117 13:35:52 @agent_ppo2.py:193][0m |          -0.0117 |          14.1474 |           4.7956 |
[32m[20230117 13:35:52 @agent_ppo2.py:193][0m |          -0.0150 |          13.6327 |           4.7981 |
[32m[20230117 13:35:52 @agent_ppo2.py:193][0m |          -0.0158 |          13.3052 |           4.7932 |
[32m[20230117 13:35:52 @agent_ppo2.py:193][0m |          -0.0133 |          13.3185 |           4.7971 |
[32m[20230117 13:35:52 @agent_ppo2.py:193][0m |          -0.0138 |          13.6499 |           4.7935 |
[32m[20230117 13:35:52 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230117 13:35:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 209.26
[32m[20230117 13:35:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.64
[32m[20230117 13:35:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 105.01
[32m[20230117 13:35:52 @agent_ppo2.py:151][0m Total time:       9.45 min
[32m[20230117 13:35:52 @agent_ppo2.py:153][0m 876544 total steps have happened
[32m[20230117 13:35:52 @agent_ppo2.py:129][0m #------------------------ Iteration 428 --------------------------#
[32m[20230117 13:35:52 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:52 @agent_ppo2.py:193][0m |           0.0003 |          20.1798 |           4.8442 |
[32m[20230117 13:35:52 @agent_ppo2.py:193][0m |          -0.0052 |          17.4483 |           4.8339 |
[32m[20230117 13:35:52 @agent_ppo2.py:193][0m |          -0.0080 |          16.1753 |           4.8347 |
[32m[20230117 13:35:52 @agent_ppo2.py:193][0m |          -0.0094 |          15.1241 |           4.8316 |
[32m[20230117 13:35:53 @agent_ppo2.py:193][0m |          -0.0119 |          14.3559 |           4.8343 |
[32m[20230117 13:35:53 @agent_ppo2.py:193][0m |          -0.0136 |          13.7124 |           4.8294 |
[32m[20230117 13:35:53 @agent_ppo2.py:193][0m |          -0.0126 |          13.5525 |           4.8311 |
[32m[20230117 13:35:53 @agent_ppo2.py:193][0m |          -0.0146 |          12.6417 |           4.8282 |
[32m[20230117 13:35:53 @agent_ppo2.py:193][0m |          -0.0164 |          12.1327 |           4.8298 |
[32m[20230117 13:35:53 @agent_ppo2.py:193][0m |          -0.0154 |          11.7474 |           4.8294 |
[32m[20230117 13:35:53 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:35:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.52
[32m[20230117 13:35:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.53
[32m[20230117 13:35:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.62
[32m[20230117 13:35:53 @agent_ppo2.py:151][0m Total time:       9.47 min
[32m[20230117 13:35:53 @agent_ppo2.py:153][0m 878592 total steps have happened
[32m[20230117 13:35:53 @agent_ppo2.py:129][0m #------------------------ Iteration 429 --------------------------#
[32m[20230117 13:35:53 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:35:53 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:53 @agent_ppo2.py:193][0m |          -0.0005 |          26.9208 |           4.8476 |
[32m[20230117 13:35:53 @agent_ppo2.py:193][0m |          -0.0092 |          20.0272 |           4.8499 |
[32m[20230117 13:35:53 @agent_ppo2.py:193][0m |          -0.0089 |          18.5174 |           4.8514 |
[32m[20230117 13:35:54 @agent_ppo2.py:193][0m |          -0.0133 |          17.4744 |           4.8556 |
[32m[20230117 13:35:54 @agent_ppo2.py:193][0m |          -0.0138 |          17.2137 |           4.8582 |
[32m[20230117 13:35:54 @agent_ppo2.py:193][0m |          -0.0146 |          16.5304 |           4.8573 |
[32m[20230117 13:35:54 @agent_ppo2.py:193][0m |          -0.0153 |          16.1593 |           4.8558 |
[32m[20230117 13:35:54 @agent_ppo2.py:193][0m |          -0.0147 |          15.9336 |           4.8534 |
[32m[20230117 13:35:54 @agent_ppo2.py:193][0m |          -0.0179 |          15.6017 |           4.8593 |
[32m[20230117 13:35:54 @agent_ppo2.py:193][0m |          -0.0167 |          15.4052 |           4.8607 |
[32m[20230117 13:35:54 @agent_ppo2.py:138][0m Policy update time: 0.65 s
[32m[20230117 13:35:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 202.66
[32m[20230117 13:35:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.90
[32m[20230117 13:35:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.82
[32m[20230117 13:35:54 @agent_ppo2.py:151][0m Total time:       9.48 min
[32m[20230117 13:35:54 @agent_ppo2.py:153][0m 880640 total steps have happened
[32m[20230117 13:35:54 @agent_ppo2.py:129][0m #------------------------ Iteration 430 --------------------------#
[32m[20230117 13:35:54 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:35:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:54 @agent_ppo2.py:193][0m |           0.0004 |          20.5004 |           4.8191 |
[32m[20230117 13:35:54 @agent_ppo2.py:193][0m |          -0.0038 |          18.8962 |           4.8045 |
[32m[20230117 13:35:55 @agent_ppo2.py:193][0m |          -0.0066 |          17.8588 |           4.8033 |
[32m[20230117 13:35:55 @agent_ppo2.py:193][0m |          -0.0075 |          16.7712 |           4.8015 |
[32m[20230117 13:35:55 @agent_ppo2.py:193][0m |          -0.0086 |          16.0397 |           4.7998 |
[32m[20230117 13:35:55 @agent_ppo2.py:193][0m |          -0.0094 |          15.5669 |           4.7966 |
[32m[20230117 13:35:55 @agent_ppo2.py:193][0m |          -0.0103 |          15.2793 |           4.7974 |
[32m[20230117 13:35:55 @agent_ppo2.py:193][0m |          -0.0105 |          14.9842 |           4.7939 |
[32m[20230117 13:35:55 @agent_ppo2.py:193][0m |          -0.0111 |          14.7377 |           4.7955 |
[32m[20230117 13:35:55 @agent_ppo2.py:193][0m |          -0.0119 |          14.4951 |           4.7979 |
[32m[20230117 13:35:55 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:35:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.01
[32m[20230117 13:35:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.44
[32m[20230117 13:35:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.54
[32m[20230117 13:35:55 @agent_ppo2.py:151][0m Total time:       9.50 min
[32m[20230117 13:35:55 @agent_ppo2.py:153][0m 882688 total steps have happened
[32m[20230117 13:35:55 @agent_ppo2.py:129][0m #------------------------ Iteration 431 --------------------------#
[32m[20230117 13:35:55 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:56 @agent_ppo2.py:193][0m |           0.0008 |          30.5734 |           4.8086 |
[32m[20230117 13:35:56 @agent_ppo2.py:193][0m |          -0.0062 |          23.8635 |           4.7949 |
[32m[20230117 13:35:56 @agent_ppo2.py:193][0m |          -0.0099 |          22.1567 |           4.7827 |
[32m[20230117 13:35:56 @agent_ppo2.py:193][0m |          -0.0109 |          21.0750 |           4.7837 |
[32m[20230117 13:35:56 @agent_ppo2.py:193][0m |          -0.0127 |          20.4210 |           4.7743 |
[32m[20230117 13:35:56 @agent_ppo2.py:193][0m |          -0.0135 |          19.8503 |           4.7780 |
[32m[20230117 13:35:56 @agent_ppo2.py:193][0m |          -0.0144 |          19.5245 |           4.7685 |
[32m[20230117 13:35:56 @agent_ppo2.py:193][0m |          -0.0156 |          19.1638 |           4.7680 |
[32m[20230117 13:35:56 @agent_ppo2.py:193][0m |          -0.0160 |          18.8549 |           4.7664 |
[32m[20230117 13:35:56 @agent_ppo2.py:193][0m |          -0.0171 |          18.6730 |           4.7645 |
[32m[20230117 13:35:56 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230117 13:35:56 @agent_ppo2.py:146][0m Average TRAINING episode reward: 211.47
[32m[20230117 13:35:56 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.53
[32m[20230117 13:35:56 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.94
[32m[20230117 13:35:56 @agent_ppo2.py:151][0m Total time:       9.52 min
[32m[20230117 13:35:56 @agent_ppo2.py:153][0m 884736 total steps have happened
[32m[20230117 13:35:56 @agent_ppo2.py:129][0m #------------------------ Iteration 432 --------------------------#
[32m[20230117 13:35:56 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:57 @agent_ppo2.py:193][0m |           0.0037 |          24.7423 |           4.8460 |
[32m[20230117 13:35:57 @agent_ppo2.py:193][0m |          -0.0055 |          21.1765 |           4.8425 |
[32m[20230117 13:35:57 @agent_ppo2.py:193][0m |          -0.0074 |          20.1367 |           4.8427 |
[32m[20230117 13:35:57 @agent_ppo2.py:193][0m |          -0.0106 |          19.4775 |           4.8369 |
[32m[20230117 13:35:57 @agent_ppo2.py:193][0m |          -0.0109 |          18.9126 |           4.8468 |
[32m[20230117 13:35:57 @agent_ppo2.py:193][0m |          -0.0126 |          18.5646 |           4.8418 |
[32m[20230117 13:35:57 @agent_ppo2.py:193][0m |          -0.0133 |          18.1189 |           4.8403 |
[32m[20230117 13:35:57 @agent_ppo2.py:193][0m |          -0.0135 |          17.7687 |           4.8414 |
[32m[20230117 13:35:57 @agent_ppo2.py:193][0m |          -0.0170 |          17.5138 |           4.8401 |
[32m[20230117 13:35:57 @agent_ppo2.py:193][0m |          -0.0132 |          17.2171 |           4.8398 |
[32m[20230117 13:35:57 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:35:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.28
[32m[20230117 13:35:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.88
[32m[20230117 13:35:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.71
[32m[20230117 13:35:57 @agent_ppo2.py:151][0m Total time:       9.54 min
[32m[20230117 13:35:57 @agent_ppo2.py:153][0m 886784 total steps have happened
[32m[20230117 13:35:57 @agent_ppo2.py:129][0m #------------------------ Iteration 433 --------------------------#
[32m[20230117 13:35:58 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:35:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:58 @agent_ppo2.py:193][0m |           0.0015 |          18.3493 |           4.8289 |
[32m[20230117 13:35:58 @agent_ppo2.py:193][0m |          -0.0027 |          17.3073 |           4.8245 |
[32m[20230117 13:35:58 @agent_ppo2.py:193][0m |          -0.0048 |          16.8720 |           4.8208 |
[32m[20230117 13:35:58 @agent_ppo2.py:193][0m |          -0.0060 |          16.5005 |           4.8248 |
[32m[20230117 13:35:58 @agent_ppo2.py:193][0m |          -0.0071 |          16.2705 |           4.8256 |
[32m[20230117 13:35:58 @agent_ppo2.py:193][0m |          -0.0078 |          16.0088 |           4.8270 |
[32m[20230117 13:35:58 @agent_ppo2.py:193][0m |          -0.0087 |          15.8287 |           4.8240 |
[32m[20230117 13:35:58 @agent_ppo2.py:193][0m |          -0.0091 |          15.6783 |           4.8314 |
[32m[20230117 13:35:58 @agent_ppo2.py:193][0m |          -0.0100 |          15.5554 |           4.8291 |
[32m[20230117 13:35:58 @agent_ppo2.py:193][0m |          -0.0099 |          15.4161 |           4.8258 |
[32m[20230117 13:35:58 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:35:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.16
[32m[20230117 13:35:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.30
[32m[20230117 13:35:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.00
[32m[20230117 13:35:59 @agent_ppo2.py:151][0m Total time:       9.56 min
[32m[20230117 13:35:59 @agent_ppo2.py:153][0m 888832 total steps have happened
[32m[20230117 13:35:59 @agent_ppo2.py:129][0m #------------------------ Iteration 434 --------------------------#
[32m[20230117 13:35:59 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:35:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:35:59 @agent_ppo2.py:193][0m |           0.0035 |          40.5075 |           4.8078 |
[32m[20230117 13:35:59 @agent_ppo2.py:193][0m |          -0.0061 |          28.0692 |           4.8054 |
[32m[20230117 13:35:59 @agent_ppo2.py:193][0m |          -0.0084 |          23.5366 |           4.7993 |
[32m[20230117 13:35:59 @agent_ppo2.py:193][0m |          -0.0110 |          21.7062 |           4.7975 |
[32m[20230117 13:35:59 @agent_ppo2.py:193][0m |          -0.0119 |          20.3742 |           4.7968 |
[32m[20230117 13:35:59 @agent_ppo2.py:193][0m |          -0.0135 |          19.2816 |           4.7937 |
[32m[20230117 13:35:59 @agent_ppo2.py:193][0m |          -0.0144 |          18.6929 |           4.7950 |
[32m[20230117 13:35:59 @agent_ppo2.py:193][0m |          -0.0152 |          17.8519 |           4.7918 |
[32m[20230117 13:35:59 @agent_ppo2.py:193][0m |          -0.0163 |          17.2402 |           4.7903 |
[32m[20230117 13:35:59 @agent_ppo2.py:193][0m |          -0.0164 |          16.7940 |           4.7859 |
[32m[20230117 13:35:59 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230117 13:36:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 156.97
[32m[20230117 13:36:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 263.25
[32m[20230117 13:36:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.54
[32m[20230117 13:36:00 @agent_ppo2.py:151][0m Total time:       9.58 min
[32m[20230117 13:36:00 @agent_ppo2.py:153][0m 890880 total steps have happened
[32m[20230117 13:36:00 @agent_ppo2.py:129][0m #------------------------ Iteration 435 --------------------------#
[32m[20230117 13:36:00 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:36:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:00 @agent_ppo2.py:193][0m |          -0.0022 |          18.9618 |           4.7835 |
[32m[20230117 13:36:00 @agent_ppo2.py:193][0m |          -0.0061 |          16.3606 |           4.7664 |
[32m[20230117 13:36:00 @agent_ppo2.py:193][0m |          -0.0137 |          15.6236 |           4.7604 |
[32m[20230117 13:36:00 @agent_ppo2.py:193][0m |          -0.0099 |          15.0516 |           4.7608 |
[32m[20230117 13:36:00 @agent_ppo2.py:193][0m |          -0.0136 |          14.5840 |           4.7633 |
[32m[20230117 13:36:00 @agent_ppo2.py:193][0m |          -0.0148 |          14.3644 |           4.7572 |
[32m[20230117 13:36:00 @agent_ppo2.py:193][0m |          -0.0100 |          14.0270 |           4.7547 |
[32m[20230117 13:36:00 @agent_ppo2.py:193][0m |          -0.0083 |          13.7816 |           4.7607 |
[32m[20230117 13:36:01 @agent_ppo2.py:193][0m |          -0.0001 |          14.6467 |           4.7545 |
[32m[20230117 13:36:01 @agent_ppo2.py:193][0m |          -0.0153 |          13.2522 |           4.7538 |
[32m[20230117 13:36:01 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:36:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.62
[32m[20230117 13:36:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.41
[32m[20230117 13:36:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.19
[32m[20230117 13:36:01 @agent_ppo2.py:151][0m Total time:       9.60 min
[32m[20230117 13:36:01 @agent_ppo2.py:153][0m 892928 total steps have happened
[32m[20230117 13:36:01 @agent_ppo2.py:129][0m #------------------------ Iteration 436 --------------------------#
[32m[20230117 13:36:01 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:36:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:01 @agent_ppo2.py:193][0m |           0.0009 |          18.9924 |           4.7741 |
[32m[20230117 13:36:01 @agent_ppo2.py:193][0m |          -0.0063 |          17.6207 |           4.7596 |
[32m[20230117 13:36:01 @agent_ppo2.py:193][0m |          -0.0079 |          16.7412 |           4.7560 |
[32m[20230117 13:36:01 @agent_ppo2.py:193][0m |          -0.0099 |          16.6207 |           4.7557 |
[32m[20230117 13:36:01 @agent_ppo2.py:193][0m |          -0.0112 |          16.0341 |           4.7627 |
[32m[20230117 13:36:02 @agent_ppo2.py:193][0m |          -0.0145 |          15.8239 |           4.7615 |
[32m[20230117 13:36:02 @agent_ppo2.py:193][0m |           0.0076 |          17.8029 |           4.7582 |
[32m[20230117 13:36:02 @agent_ppo2.py:193][0m |          -0.0115 |          15.5069 |           4.7656 |
[32m[20230117 13:36:02 @agent_ppo2.py:193][0m |          -0.0110 |          15.2653 |           4.7599 |
[32m[20230117 13:36:02 @agent_ppo2.py:193][0m |          -0.0165 |          15.1008 |           4.7592 |
[32m[20230117 13:36:02 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:36:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.83
[32m[20230117 13:36:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.12
[32m[20230117 13:36:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.22
[32m[20230117 13:36:02 @agent_ppo2.py:151][0m Total time:       9.62 min
[32m[20230117 13:36:02 @agent_ppo2.py:153][0m 894976 total steps have happened
[32m[20230117 13:36:02 @agent_ppo2.py:129][0m #------------------------ Iteration 437 --------------------------#
[32m[20230117 13:36:02 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:36:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:02 @agent_ppo2.py:193][0m |          -0.0019 |          37.3905 |           4.8574 |
[32m[20230117 13:36:02 @agent_ppo2.py:193][0m |          -0.0064 |          22.7612 |           4.8521 |
[32m[20230117 13:36:03 @agent_ppo2.py:193][0m |          -0.0072 |          21.1607 |           4.8475 |
[32m[20230117 13:36:03 @agent_ppo2.py:193][0m |          -0.0077 |          19.6531 |           4.8465 |
[32m[20230117 13:36:03 @agent_ppo2.py:193][0m |          -0.0062 |          20.0085 |           4.8399 |
[32m[20230117 13:36:03 @agent_ppo2.py:193][0m |          -0.0104 |          18.3123 |           4.8346 |
[32m[20230117 13:36:03 @agent_ppo2.py:193][0m |          -0.0122 |          17.4876 |           4.8345 |
[32m[20230117 13:36:03 @agent_ppo2.py:193][0m |          -0.0141 |          17.1952 |           4.8316 |
[32m[20230117 13:36:03 @agent_ppo2.py:193][0m |          -0.0157 |          17.2042 |           4.8311 |
[32m[20230117 13:36:03 @agent_ppo2.py:193][0m |          -0.0178 |          16.5344 |           4.8280 |
[32m[20230117 13:36:03 @agent_ppo2.py:138][0m Policy update time: 0.83 s
[32m[20230117 13:36:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 157.42
[32m[20230117 13:36:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.75
[32m[20230117 13:36:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.88
[32m[20230117 13:36:03 @agent_ppo2.py:151][0m Total time:       9.64 min
[32m[20230117 13:36:03 @agent_ppo2.py:153][0m 897024 total steps have happened
[32m[20230117 13:36:03 @agent_ppo2.py:129][0m #------------------------ Iteration 438 --------------------------#
[32m[20230117 13:36:04 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:36:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:04 @agent_ppo2.py:193][0m |           0.0013 |          45.6000 |           4.7294 |
[32m[20230117 13:36:04 @agent_ppo2.py:193][0m |          -0.0066 |          37.3564 |           4.7264 |
[32m[20230117 13:36:04 @agent_ppo2.py:193][0m |           0.0007 |          35.9009 |           4.7193 |
[32m[20230117 13:36:04 @agent_ppo2.py:193][0m |          -0.0077 |          32.6201 |           4.7114 |
[32m[20230117 13:36:04 @agent_ppo2.py:193][0m |          -0.0102 |          31.0203 |           4.7108 |
[32m[20230117 13:36:04 @agent_ppo2.py:193][0m |          -0.0103 |          29.7919 |           4.7108 |
[32m[20230117 13:36:04 @agent_ppo2.py:193][0m |          -0.0043 |          29.4724 |           4.7090 |
[32m[20230117 13:36:04 @agent_ppo2.py:193][0m |          -0.0128 |          28.0993 |           4.7089 |
[32m[20230117 13:36:04 @agent_ppo2.py:193][0m |          -0.0049 |          27.9352 |           4.7068 |
[32m[20230117 13:36:04 @agent_ppo2.py:193][0m |          -0.0151 |          26.9175 |           4.7007 |
[32m[20230117 13:36:04 @agent_ppo2.py:138][0m Policy update time: 0.83 s
[32m[20230117 13:36:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 212.08
[32m[20230117 13:36:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.24
[32m[20230117 13:36:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.54
[32m[20230117 13:36:05 @agent_ppo2.py:151][0m Total time:       9.66 min
[32m[20230117 13:36:05 @agent_ppo2.py:153][0m 899072 total steps have happened
[32m[20230117 13:36:05 @agent_ppo2.py:129][0m #------------------------ Iteration 439 --------------------------#
[32m[20230117 13:36:05 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:05 @agent_ppo2.py:193][0m |           0.0013 |          26.9762 |           4.6940 |
[32m[20230117 13:36:05 @agent_ppo2.py:193][0m |          -0.0018 |          24.1480 |           4.6876 |
[32m[20230117 13:36:05 @agent_ppo2.py:193][0m |          -0.0076 |          21.9696 |           4.6927 |
[32m[20230117 13:36:05 @agent_ppo2.py:193][0m |          -0.0059 |          21.5318 |           4.6895 |
[32m[20230117 13:36:05 @agent_ppo2.py:193][0m |          -0.0096 |          20.7242 |           4.6881 |
[32m[20230117 13:36:05 @agent_ppo2.py:193][0m |          -0.0120 |          20.7710 |           4.6916 |
[32m[20230117 13:36:05 @agent_ppo2.py:193][0m |          -0.0143 |          20.1197 |           4.6915 |
[32m[20230117 13:36:05 @agent_ppo2.py:193][0m |          -0.0128 |          19.6265 |           4.6900 |
[32m[20230117 13:36:05 @agent_ppo2.py:193][0m |          -0.0144 |          19.2753 |           4.6912 |
[32m[20230117 13:36:05 @agent_ppo2.py:193][0m |          -0.0157 |          18.9277 |           4.6915 |
[32m[20230117 13:36:05 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:36:06 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.01
[32m[20230117 13:36:06 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.38
[32m[20230117 13:36:06 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.75
[32m[20230117 13:36:06 @agent_ppo2.py:151][0m Total time:       9.68 min
[32m[20230117 13:36:06 @agent_ppo2.py:153][0m 901120 total steps have happened
[32m[20230117 13:36:06 @agent_ppo2.py:129][0m #------------------------ Iteration 440 --------------------------#
[32m[20230117 13:36:06 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:06 @agent_ppo2.py:193][0m |          -0.0002 |          65.8447 |           4.6957 |
[32m[20230117 13:36:06 @agent_ppo2.py:193][0m |          -0.0086 |          40.1135 |           4.6873 |
[32m[20230117 13:36:06 @agent_ppo2.py:193][0m |          -0.0115 |          29.9176 |           4.6840 |
[32m[20230117 13:36:06 @agent_ppo2.py:193][0m |          -0.0118 |          24.6208 |           4.6817 |
[32m[20230117 13:36:06 @agent_ppo2.py:193][0m |          -0.0135 |          21.3003 |           4.6854 |
[32m[20230117 13:36:06 @agent_ppo2.py:193][0m |          -0.0137 |          19.4723 |           4.6813 |
[32m[20230117 13:36:06 @agent_ppo2.py:193][0m |          -0.0145 |          18.5320 |           4.6761 |
[32m[20230117 13:36:06 @agent_ppo2.py:193][0m |          -0.0164 |          17.8302 |           4.6753 |
[32m[20230117 13:36:06 @agent_ppo2.py:193][0m |          -0.0164 |          17.2291 |           4.6791 |
[32m[20230117 13:36:07 @agent_ppo2.py:193][0m |          -0.0175 |          16.5098 |           4.6791 |
[32m[20230117 13:36:07 @agent_ppo2.py:138][0m Policy update time: 0.68 s
[32m[20230117 13:36:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 183.57
[32m[20230117 13:36:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.06
[32m[20230117 13:36:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.58
[32m[20230117 13:36:07 @agent_ppo2.py:151][0m Total time:       9.70 min
[32m[20230117 13:36:07 @agent_ppo2.py:153][0m 903168 total steps have happened
[32m[20230117 13:36:07 @agent_ppo2.py:129][0m #------------------------ Iteration 441 --------------------------#
[32m[20230117 13:36:07 @agent_ppo2.py:135][0m Sampling time: 0.28 s by 4 slaves
[32m[20230117 13:36:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:07 @agent_ppo2.py:193][0m |           0.0017 |          34.4721 |           4.7908 |
[32m[20230117 13:36:07 @agent_ppo2.py:193][0m |          -0.0043 |          30.5770 |           4.7824 |
[32m[20230117 13:36:07 @agent_ppo2.py:193][0m |          -0.0067 |          28.5481 |           4.7832 |
[32m[20230117 13:36:07 @agent_ppo2.py:193][0m |          -0.0071 |          27.2673 |           4.7815 |
[32m[20230117 13:36:07 @agent_ppo2.py:193][0m |          -0.0089 |          26.2295 |           4.7772 |
[32m[20230117 13:36:08 @agent_ppo2.py:193][0m |          -0.0087 |          25.4716 |           4.7781 |
[32m[20230117 13:36:08 @agent_ppo2.py:193][0m |          -0.0073 |          24.6780 |           4.7784 |
[32m[20230117 13:36:08 @agent_ppo2.py:193][0m |          -0.0119 |          23.6684 |           4.7776 |
[32m[20230117 13:36:08 @agent_ppo2.py:193][0m |          -0.0104 |          23.0892 |           4.7800 |
[32m[20230117 13:36:08 @agent_ppo2.py:193][0m |          -0.0137 |          22.4992 |           4.7768 |
[32m[20230117 13:36:08 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:36:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 204.31
[32m[20230117 13:36:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.82
[32m[20230117 13:36:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.47
[32m[20230117 13:36:08 @agent_ppo2.py:151][0m Total time:       9.72 min
[32m[20230117 13:36:08 @agent_ppo2.py:153][0m 905216 total steps have happened
[32m[20230117 13:36:08 @agent_ppo2.py:129][0m #------------------------ Iteration 442 --------------------------#
[32m[20230117 13:36:08 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:08 @agent_ppo2.py:193][0m |          -0.0012 |          18.9356 |           4.8810 |
[32m[20230117 13:36:08 @agent_ppo2.py:193][0m |           0.0239 |          19.4934 |           4.8769 |
[32m[20230117 13:36:08 @agent_ppo2.py:193][0m |          -0.0045 |          16.8616 |           4.8733 |
[32m[20230117 13:36:09 @agent_ppo2.py:193][0m |          -0.0123 |          16.4326 |           4.8683 |
[32m[20230117 13:36:09 @agent_ppo2.py:193][0m |           0.0231 |          20.7608 |           4.8706 |
[32m[20230117 13:36:09 @agent_ppo2.py:193][0m |          -0.0147 |          15.8712 |           4.8591 |
[32m[20230117 13:36:09 @agent_ppo2.py:193][0m |          -0.0126 |          15.5264 |           4.8659 |
[32m[20230117 13:36:09 @agent_ppo2.py:193][0m |          -0.0070 |          15.3124 |           4.8653 |
[32m[20230117 13:36:09 @agent_ppo2.py:193][0m |          -0.0096 |          15.1262 |           4.8613 |
[32m[20230117 13:36:09 @agent_ppo2.py:193][0m |          -0.0098 |          15.0224 |           4.8640 |
[32m[20230117 13:36:09 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:36:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.26
[32m[20230117 13:36:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.94
[32m[20230117 13:36:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.23
[32m[20230117 13:36:09 @agent_ppo2.py:151][0m Total time:       9.73 min
[32m[20230117 13:36:09 @agent_ppo2.py:153][0m 907264 total steps have happened
[32m[20230117 13:36:09 @agent_ppo2.py:129][0m #------------------------ Iteration 443 --------------------------#
[32m[20230117 13:36:09 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:09 @agent_ppo2.py:193][0m |          -0.0012 |          19.9252 |           4.7747 |
[32m[20230117 13:36:09 @agent_ppo2.py:193][0m |          -0.0079 |          18.3951 |           4.7743 |
[32m[20230117 13:36:10 @agent_ppo2.py:193][0m |          -0.0114 |          17.6071 |           4.7708 |
[32m[20230117 13:36:10 @agent_ppo2.py:193][0m |          -0.0124 |          16.9482 |           4.7764 |
[32m[20230117 13:36:10 @agent_ppo2.py:193][0m |          -0.0131 |          16.5299 |           4.7747 |
[32m[20230117 13:36:10 @agent_ppo2.py:193][0m |          -0.0145 |          16.1487 |           4.7731 |
[32m[20230117 13:36:10 @agent_ppo2.py:193][0m |          -0.0116 |          15.9174 |           4.7718 |
[32m[20230117 13:36:10 @agent_ppo2.py:193][0m |          -0.0142 |          15.6683 |           4.7758 |
[32m[20230117 13:36:10 @agent_ppo2.py:193][0m |          -0.0162 |          15.0408 |           4.7743 |
[32m[20230117 13:36:10 @agent_ppo2.py:193][0m |          -0.0155 |          14.7626 |           4.7730 |
[32m[20230117 13:36:10 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:36:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.90
[32m[20230117 13:36:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.49
[32m[20230117 13:36:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.62
[32m[20230117 13:36:10 @agent_ppo2.py:151][0m Total time:       9.75 min
[32m[20230117 13:36:10 @agent_ppo2.py:153][0m 909312 total steps have happened
[32m[20230117 13:36:10 @agent_ppo2.py:129][0m #------------------------ Iteration 444 --------------------------#
[32m[20230117 13:36:10 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:36:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:11 @agent_ppo2.py:193][0m |           0.0184 |          19.5650 |           4.7787 |
[32m[20230117 13:36:11 @agent_ppo2.py:193][0m |          -0.0028 |          17.3794 |           4.7737 |
[32m[20230117 13:36:11 @agent_ppo2.py:193][0m |          -0.0029 |          17.0478 |           4.7742 |
[32m[20230117 13:36:11 @agent_ppo2.py:193][0m |          -0.0109 |          16.8594 |           4.7776 |
[32m[20230117 13:36:11 @agent_ppo2.py:193][0m |          -0.0109 |          16.6804 |           4.7753 |
[32m[20230117 13:36:11 @agent_ppo2.py:193][0m |          -0.0116 |          16.5751 |           4.7718 |
[32m[20230117 13:36:11 @agent_ppo2.py:193][0m |          -0.0104 |          16.4228 |           4.7762 |
[32m[20230117 13:36:11 @agent_ppo2.py:193][0m |          -0.0113 |          16.3229 |           4.7787 |
[32m[20230117 13:36:11 @agent_ppo2.py:193][0m |          -0.0128 |          16.1662 |           4.7778 |
[32m[20230117 13:36:11 @agent_ppo2.py:193][0m |          -0.0147 |          16.1166 |           4.7806 |
[32m[20230117 13:36:11 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:36:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.94
[32m[20230117 13:36:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.37
[32m[20230117 13:36:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.85
[32m[20230117 13:36:11 @agent_ppo2.py:151][0m Total time:       9.77 min
[32m[20230117 13:36:11 @agent_ppo2.py:153][0m 911360 total steps have happened
[32m[20230117 13:36:11 @agent_ppo2.py:129][0m #------------------------ Iteration 445 --------------------------#
[32m[20230117 13:36:12 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:12 @agent_ppo2.py:193][0m |           0.0001 |          26.7696 |           4.8087 |
[32m[20230117 13:36:12 @agent_ppo2.py:193][0m |          -0.0053 |          21.8679 |           4.7960 |
[32m[20230117 13:36:12 @agent_ppo2.py:193][0m |          -0.0104 |          20.4762 |           4.7942 |
[32m[20230117 13:36:12 @agent_ppo2.py:193][0m |          -0.0006 |          20.6306 |           4.7908 |
[32m[20230117 13:36:12 @agent_ppo2.py:193][0m |          -0.0108 |          19.0822 |           4.7908 |
[32m[20230117 13:36:12 @agent_ppo2.py:193][0m |           0.0038 |          18.1323 |           4.7916 |
[32m[20230117 13:36:12 @agent_ppo2.py:193][0m |          -0.0129 |          17.7933 |           4.7814 |
[32m[20230117 13:36:12 @agent_ppo2.py:193][0m |          -0.0159 |          17.4866 |           4.7931 |
[32m[20230117 13:36:12 @agent_ppo2.py:193][0m |          -0.0175 |          17.1755 |           4.7938 |
[32m[20230117 13:36:12 @agent_ppo2.py:193][0m |          -0.0169 |          17.0308 |           4.7921 |
[32m[20230117 13:36:12 @agent_ppo2.py:138][0m Policy update time: 0.65 s
[32m[20230117 13:36:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 208.25
[32m[20230117 13:36:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.51
[32m[20230117 13:36:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.57
[32m[20230117 13:36:12 @agent_ppo2.py:151][0m Total time:       9.79 min
[32m[20230117 13:36:12 @agent_ppo2.py:153][0m 913408 total steps have happened
[32m[20230117 13:36:12 @agent_ppo2.py:129][0m #------------------------ Iteration 446 --------------------------#
[32m[20230117 13:36:13 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:36:13 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:13 @agent_ppo2.py:193][0m |           0.0018 |          37.0410 |           4.8002 |
[32m[20230117 13:36:13 @agent_ppo2.py:193][0m |          -0.0046 |          30.2461 |           4.7956 |
[32m[20230117 13:36:13 @agent_ppo2.py:193][0m |          -0.0081 |          27.5407 |           4.7874 |
[32m[20230117 13:36:13 @agent_ppo2.py:193][0m |          -0.0081 |          25.6723 |           4.7846 |
[32m[20230117 13:36:13 @agent_ppo2.py:193][0m |          -0.0097 |          24.8012 |           4.7879 |
[32m[20230117 13:36:13 @agent_ppo2.py:193][0m |          -0.0107 |          24.0041 |           4.7851 |
[32m[20230117 13:36:13 @agent_ppo2.py:193][0m |          -0.0106 |          23.6120 |           4.7814 |
[32m[20230117 13:36:13 @agent_ppo2.py:193][0m |          -0.0121 |          23.0471 |           4.7816 |
[32m[20230117 13:36:13 @agent_ppo2.py:193][0m |          -0.0118 |          22.8181 |           4.7799 |
[32m[20230117 13:36:13 @agent_ppo2.py:193][0m |          -0.0116 |          22.3710 |           4.7801 |
[32m[20230117 13:36:13 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:36:14 @agent_ppo2.py:146][0m Average TRAINING episode reward: 203.27
[32m[20230117 13:36:14 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.81
[32m[20230117 13:36:14 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 266.10
[32m[20230117 13:36:14 @agent_ppo2.py:151][0m Total time:       9.81 min
[32m[20230117 13:36:14 @agent_ppo2.py:153][0m 915456 total steps have happened
[32m[20230117 13:36:14 @agent_ppo2.py:129][0m #------------------------ Iteration 447 --------------------------#
[32m[20230117 13:36:14 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:14 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:14 @agent_ppo2.py:193][0m |           0.0002 |          20.4585 |           4.7775 |
[32m[20230117 13:36:14 @agent_ppo2.py:193][0m |          -0.0044 |          18.7704 |           4.7761 |
[32m[20230117 13:36:14 @agent_ppo2.py:193][0m |          -0.0067 |          18.1635 |           4.7708 |
[32m[20230117 13:36:14 @agent_ppo2.py:193][0m |          -0.0089 |          17.6839 |           4.7734 |
[32m[20230117 13:36:14 @agent_ppo2.py:193][0m |          -0.0092 |          17.3051 |           4.7681 |
[32m[20230117 13:36:14 @agent_ppo2.py:193][0m |          -0.0101 |          17.0512 |           4.7720 |
[32m[20230117 13:36:14 @agent_ppo2.py:193][0m |          -0.0110 |          16.7704 |           4.7728 |
[32m[20230117 13:36:14 @agent_ppo2.py:193][0m |          -0.0121 |          16.5919 |           4.7704 |
[32m[20230117 13:36:14 @agent_ppo2.py:193][0m |          -0.0118 |          16.5090 |           4.7752 |
[32m[20230117 13:36:14 @agent_ppo2.py:193][0m |          -0.0137 |          16.3502 |           4.7714 |
[32m[20230117 13:36:14 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:36:15 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.22
[32m[20230117 13:36:15 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.53
[32m[20230117 13:36:15 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 263.42
[32m[20230117 13:36:15 @agent_ppo2.py:151][0m Total time:       9.83 min
[32m[20230117 13:36:15 @agent_ppo2.py:153][0m 917504 total steps have happened
[32m[20230117 13:36:15 @agent_ppo2.py:129][0m #------------------------ Iteration 448 --------------------------#
[32m[20230117 13:36:15 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:15 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:15 @agent_ppo2.py:193][0m |           0.0005 |          17.7016 |           4.7020 |
[32m[20230117 13:36:15 @agent_ppo2.py:193][0m |           0.0003 |          16.6978 |           4.6933 |
[32m[20230117 13:36:15 @agent_ppo2.py:193][0m |          -0.0065 |          14.8368 |           4.6909 |
[32m[20230117 13:36:15 @agent_ppo2.py:193][0m |          -0.0013 |          15.0707 |           4.6933 |
[32m[20230117 13:36:15 @agent_ppo2.py:193][0m |          -0.0130 |          13.6721 |           4.6807 |
[32m[20230117 13:36:15 @agent_ppo2.py:193][0m |          -0.0108 |          13.2326 |           4.6897 |
[32m[20230117 13:36:15 @agent_ppo2.py:193][0m |          -0.0028 |          14.1366 |           4.6880 |
[32m[20230117 13:36:16 @agent_ppo2.py:193][0m |          -0.0083 |          12.5981 |           4.6860 |
[32m[20230117 13:36:16 @agent_ppo2.py:193][0m |          -0.0092 |          12.2501 |           4.6842 |
[32m[20230117 13:36:16 @agent_ppo2.py:193][0m |          -0.0116 |          11.9526 |           4.6850 |
[32m[20230117 13:36:16 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:36:16 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.75
[32m[20230117 13:36:16 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.21
[32m[20230117 13:36:16 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.00
[32m[20230117 13:36:16 @agent_ppo2.py:151][0m Total time:       9.85 min
[32m[20230117 13:36:16 @agent_ppo2.py:153][0m 919552 total steps have happened
[32m[20230117 13:36:16 @agent_ppo2.py:129][0m #------------------------ Iteration 449 --------------------------#
[32m[20230117 13:36:16 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:16 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:16 @agent_ppo2.py:193][0m |          -0.0025 |          30.7339 |           4.7741 |
[32m[20230117 13:36:16 @agent_ppo2.py:193][0m |          -0.0006 |          20.5493 |           4.7592 |
[32m[20230117 13:36:16 @agent_ppo2.py:193][0m |          -0.0130 |          17.5150 |           4.7570 |
[32m[20230117 13:36:16 @agent_ppo2.py:193][0m |           0.0122 |          17.1248 |           4.7538 |
[32m[20230117 13:36:16 @agent_ppo2.py:193][0m |          -0.0100 |          15.4317 |           4.7450 |
[32m[20230117 13:36:16 @agent_ppo2.py:193][0m |          -0.0122 |          14.5643 |           4.7420 |
[32m[20230117 13:36:17 @agent_ppo2.py:193][0m |          -0.0101 |          14.7317 |           4.7488 |
[32m[20230117 13:36:17 @agent_ppo2.py:193][0m |          -0.0180 |          13.3161 |           4.7430 |
[32m[20230117 13:36:17 @agent_ppo2.py:193][0m |          -0.0150 |          12.9109 |           4.7466 |
[32m[20230117 13:36:17 @agent_ppo2.py:193][0m |          -0.0186 |          12.3678 |           4.7473 |
[32m[20230117 13:36:17 @agent_ppo2.py:138][0m Policy update time: 0.67 s
[32m[20230117 13:36:17 @agent_ppo2.py:146][0m Average TRAINING episode reward: 213.50
[32m[20230117 13:36:17 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.83
[32m[20230117 13:36:17 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.26
[32m[20230117 13:36:17 @agent_ppo2.py:151][0m Total time:       9.87 min
[32m[20230117 13:36:17 @agent_ppo2.py:153][0m 921600 total steps have happened
[32m[20230117 13:36:17 @agent_ppo2.py:129][0m #------------------------ Iteration 450 --------------------------#
[32m[20230117 13:36:17 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:36:17 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:17 @agent_ppo2.py:193][0m |           0.0001 |          41.6722 |           4.6866 |
[32m[20230117 13:36:17 @agent_ppo2.py:193][0m |           0.0038 |          35.3460 |           4.6855 |
[32m[20230117 13:36:17 @agent_ppo2.py:193][0m |          -0.0090 |          29.1488 |           4.6820 |
[32m[20230117 13:36:18 @agent_ppo2.py:193][0m |          -0.0118 |          26.3227 |           4.6819 |
[32m[20230117 13:36:18 @agent_ppo2.py:193][0m |          -0.0117 |          24.3790 |           4.6817 |
[32m[20230117 13:36:18 @agent_ppo2.py:193][0m |          -0.0090 |          23.2694 |           4.6807 |
[32m[20230117 13:36:18 @agent_ppo2.py:193][0m |          -0.0127 |          21.5302 |           4.6791 |
[32m[20230117 13:36:18 @agent_ppo2.py:193][0m |          -0.0126 |          20.4111 |           4.6773 |
[32m[20230117 13:36:18 @agent_ppo2.py:193][0m |          -0.0134 |          19.5851 |           4.6765 |
[32m[20230117 13:36:18 @agent_ppo2.py:193][0m |          -0.0121 |          19.1779 |           4.6735 |
[32m[20230117 13:36:18 @agent_ppo2.py:138][0m Policy update time: 0.79 s
[32m[20230117 13:36:18 @agent_ppo2.py:146][0m Average TRAINING episode reward: 205.25
[32m[20230117 13:36:18 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.30
[32m[20230117 13:36:18 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.90
[32m[20230117 13:36:18 @agent_ppo2.py:151][0m Total time:       9.89 min
[32m[20230117 13:36:18 @agent_ppo2.py:153][0m 923648 total steps have happened
[32m[20230117 13:36:18 @agent_ppo2.py:129][0m #------------------------ Iteration 451 --------------------------#
[32m[20230117 13:36:18 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:18 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:19 @agent_ppo2.py:193][0m |          -0.0086 |          27.0247 |           4.6448 |
[32m[20230117 13:36:19 @agent_ppo2.py:193][0m |          -0.0053 |          22.8669 |           4.6363 |
[32m[20230117 13:36:19 @agent_ppo2.py:193][0m |          -0.0031 |          21.6337 |           4.6365 |
[32m[20230117 13:36:19 @agent_ppo2.py:193][0m |          -0.0075 |          20.8721 |           4.6403 |
[32m[20230117 13:36:19 @agent_ppo2.py:193][0m |          -0.0107 |          20.3648 |           4.6412 |
[32m[20230117 13:36:19 @agent_ppo2.py:193][0m |          -0.0085 |          19.9523 |           4.6374 |
[32m[20230117 13:36:19 @agent_ppo2.py:193][0m |          -0.0123 |          19.4889 |           4.6433 |
[32m[20230117 13:36:19 @agent_ppo2.py:193][0m |          -0.0021 |          19.3142 |           4.6428 |
[32m[20230117 13:36:19 @agent_ppo2.py:193][0m |          -0.0177 |          18.9751 |           4.6417 |
[32m[20230117 13:36:19 @agent_ppo2.py:193][0m |          -0.0134 |          18.5611 |           4.6499 |
[32m[20230117 13:36:19 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:36:19 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.24
[32m[20230117 13:36:19 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.75
[32m[20230117 13:36:19 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.73
[32m[20230117 13:36:19 @agent_ppo2.py:151][0m Total time:       9.90 min
[32m[20230117 13:36:19 @agent_ppo2.py:153][0m 925696 total steps have happened
[32m[20230117 13:36:19 @agent_ppo2.py:129][0m #------------------------ Iteration 452 --------------------------#
[32m[20230117 13:36:20 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:36:20 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:20 @agent_ppo2.py:193][0m |           0.0004 |          35.5006 |           4.8024 |
[32m[20230117 13:36:20 @agent_ppo2.py:193][0m |          -0.0054 |          25.4381 |           4.7896 |
[32m[20230117 13:36:20 @agent_ppo2.py:193][0m |          -0.0082 |          22.4207 |           4.7883 |
[32m[20230117 13:36:20 @agent_ppo2.py:193][0m |          -0.0100 |          20.7362 |           4.7863 |
[32m[20230117 13:36:20 @agent_ppo2.py:193][0m |          -0.0101 |          19.7570 |           4.7849 |
[32m[20230117 13:36:20 @agent_ppo2.py:193][0m |          -0.0103 |          19.1688 |           4.7889 |
[32m[20230117 13:36:20 @agent_ppo2.py:193][0m |          -0.0104 |          18.6179 |           4.7896 |
[32m[20230117 13:36:20 @agent_ppo2.py:193][0m |          -0.0124 |          18.3070 |           4.7885 |
[32m[20230117 13:36:20 @agent_ppo2.py:193][0m |          -0.0134 |          17.8884 |           4.7907 |
[32m[20230117 13:36:20 @agent_ppo2.py:193][0m |          -0.0126 |          17.6919 |           4.7845 |
[32m[20230117 13:36:20 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:36:21 @agent_ppo2.py:146][0m Average TRAINING episode reward: 206.04
[32m[20230117 13:36:21 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.91
[32m[20230117 13:36:21 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.51
[32m[20230117 13:36:21 @agent_ppo2.py:151][0m Total time:       9.93 min
[32m[20230117 13:36:21 @agent_ppo2.py:153][0m 927744 total steps have happened
[32m[20230117 13:36:21 @agent_ppo2.py:129][0m #------------------------ Iteration 453 --------------------------#
[32m[20230117 13:36:21 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:21 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:21 @agent_ppo2.py:193][0m |           0.0007 |          36.6072 |           4.8173 |
[32m[20230117 13:36:21 @agent_ppo2.py:193][0m |          -0.0052 |          27.4127 |           4.8142 |
[32m[20230117 13:36:21 @agent_ppo2.py:193][0m |          -0.0071 |          26.0824 |           4.8117 |
[32m[20230117 13:36:21 @agent_ppo2.py:193][0m |          -0.0075 |          24.9703 |           4.8180 |
[32m[20230117 13:36:21 @agent_ppo2.py:193][0m |          -0.0114 |          24.0141 |           4.8125 |
[32m[20230117 13:36:21 @agent_ppo2.py:193][0m |          -0.0108 |          23.2378 |           4.8135 |
[32m[20230117 13:36:21 @agent_ppo2.py:193][0m |          -0.0110 |          22.5409 |           4.8149 |
[32m[20230117 13:36:21 @agent_ppo2.py:193][0m |          -0.0124 |          21.9822 |           4.8128 |
[32m[20230117 13:36:21 @agent_ppo2.py:193][0m |          -0.0173 |          21.6602 |           4.8134 |
[32m[20230117 13:36:21 @agent_ppo2.py:193][0m |          -0.0140 |          21.3531 |           4.8114 |
[32m[20230117 13:36:21 @agent_ppo2.py:138][0m Policy update time: 0.74 s
[32m[20230117 13:36:22 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.07
[32m[20230117 13:36:22 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.88
[32m[20230117 13:36:22 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.76
[32m[20230117 13:36:22 @agent_ppo2.py:151][0m Total time:       9.95 min
[32m[20230117 13:36:22 @agent_ppo2.py:153][0m 929792 total steps have happened
[32m[20230117 13:36:22 @agent_ppo2.py:129][0m #------------------------ Iteration 454 --------------------------#
[32m[20230117 13:36:22 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:22 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:22 @agent_ppo2.py:193][0m |          -0.0007 |          30.0233 |           4.7715 |
[32m[20230117 13:36:22 @agent_ppo2.py:193][0m |          -0.0083 |          26.0452 |           4.7583 |
[32m[20230117 13:36:22 @agent_ppo2.py:193][0m |          -0.0112 |          24.5862 |           4.7554 |
[32m[20230117 13:36:22 @agent_ppo2.py:193][0m |          -0.0136 |          23.6502 |           4.7510 |
[32m[20230117 13:36:22 @agent_ppo2.py:193][0m |          -0.0158 |          22.8582 |           4.7503 |
[32m[20230117 13:36:22 @agent_ppo2.py:193][0m |          -0.0127 |          22.3807 |           4.7539 |
[32m[20230117 13:36:22 @agent_ppo2.py:193][0m |          -0.0145 |          22.1612 |           4.7487 |
[32m[20230117 13:36:23 @agent_ppo2.py:193][0m |          -0.0159 |          20.9399 |           4.7501 |
[32m[20230117 13:36:23 @agent_ppo2.py:193][0m |          -0.0147 |          21.0689 |           4.7480 |
[32m[20230117 13:36:23 @agent_ppo2.py:193][0m |          -0.0173 |          19.7658 |           4.7493 |
[32m[20230117 13:36:23 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:36:23 @agent_ppo2.py:146][0m Average TRAINING episode reward: 234.74
[32m[20230117 13:36:23 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.34
[32m[20230117 13:36:23 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 274.19
[32m[20230117 13:36:23 @agent_ppo2.py:151][0m Total time:       9.96 min
[32m[20230117 13:36:23 @agent_ppo2.py:153][0m 931840 total steps have happened
[32m[20230117 13:36:23 @agent_ppo2.py:129][0m #------------------------ Iteration 455 --------------------------#
[32m[20230117 13:36:23 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:23 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:23 @agent_ppo2.py:193][0m |          -0.0010 |          32.0001 |           4.8407 |
[32m[20230117 13:36:23 @agent_ppo2.py:193][0m |          -0.0100 |          28.5173 |           4.8328 |
[32m[20230117 13:36:23 @agent_ppo2.py:193][0m |          -0.0085 |          27.5473 |           4.8313 |
[32m[20230117 13:36:23 @agent_ppo2.py:193][0m |          -0.0130 |          25.3965 |           4.8266 |
[32m[20230117 13:36:23 @agent_ppo2.py:193][0m |          -0.0155 |          24.0572 |           4.8267 |
[32m[20230117 13:36:23 @agent_ppo2.py:193][0m |          -0.0154 |          23.1385 |           4.8262 |
[32m[20230117 13:36:24 @agent_ppo2.py:193][0m |          -0.0142 |          22.6213 |           4.8254 |
[32m[20230117 13:36:24 @agent_ppo2.py:193][0m |          -0.0156 |          21.6437 |           4.8245 |
[32m[20230117 13:36:24 @agent_ppo2.py:193][0m |          -0.0163 |          21.1530 |           4.8239 |
[32m[20230117 13:36:24 @agent_ppo2.py:193][0m |          -0.0180 |          20.3876 |           4.8196 |
[32m[20230117 13:36:24 @agent_ppo2.py:138][0m Policy update time: 0.68 s
[32m[20230117 13:36:24 @agent_ppo2.py:146][0m Average TRAINING episode reward: 220.43
[32m[20230117 13:36:24 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.20
[32m[20230117 13:36:24 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.70
[32m[20230117 13:36:24 @agent_ppo2.py:151][0m Total time:       9.98 min
[32m[20230117 13:36:24 @agent_ppo2.py:153][0m 933888 total steps have happened
[32m[20230117 13:36:24 @agent_ppo2.py:129][0m #------------------------ Iteration 456 --------------------------#
[32m[20230117 13:36:24 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:24 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:24 @agent_ppo2.py:193][0m |          -0.0014 |          26.1132 |           4.6687 |
[32m[20230117 13:36:24 @agent_ppo2.py:193][0m |          -0.0042 |          21.3527 |           4.6590 |
[32m[20230117 13:36:24 @agent_ppo2.py:193][0m |          -0.0144 |          20.5488 |           4.6485 |
[32m[20230117 13:36:24 @agent_ppo2.py:193][0m |          -0.0109 |          20.1551 |           4.6478 |
[32m[20230117 13:36:24 @agent_ppo2.py:193][0m |          -0.0175 |          19.8373 |           4.6451 |
[32m[20230117 13:36:25 @agent_ppo2.py:193][0m |          -0.0047 |          19.8683 |           4.6418 |
[32m[20230117 13:36:25 @agent_ppo2.py:193][0m |          -0.0197 |          19.3192 |           4.6437 |
[32m[20230117 13:36:25 @agent_ppo2.py:193][0m |          -0.0187 |          19.0340 |           4.6434 |
[32m[20230117 13:36:25 @agent_ppo2.py:193][0m |           0.0028 |          19.1942 |           4.6415 |
[32m[20230117 13:36:25 @agent_ppo2.py:193][0m |          -0.0186 |          18.5853 |           4.6427 |
[32m[20230117 13:36:25 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:36:25 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.88
[32m[20230117 13:36:25 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.73
[32m[20230117 13:36:25 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 262.95
[32m[20230117 13:36:25 @agent_ppo2.py:151][0m Total time:      10.00 min
[32m[20230117 13:36:25 @agent_ppo2.py:153][0m 935936 total steps have happened
[32m[20230117 13:36:25 @agent_ppo2.py:129][0m #------------------------ Iteration 457 --------------------------#
[32m[20230117 13:36:25 @agent_ppo2.py:135][0m Sampling time: 0.25 s by 4 slaves
[32m[20230117 13:36:25 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:25 @agent_ppo2.py:193][0m |           0.0006 |          31.8749 |           4.7329 |
[32m[20230117 13:36:25 @agent_ppo2.py:193][0m |          -0.0046 |          21.0552 |           4.7260 |
[32m[20230117 13:36:26 @agent_ppo2.py:193][0m |          -0.0082 |          19.2772 |           4.7216 |
[32m[20230117 13:36:26 @agent_ppo2.py:193][0m |          -0.0098 |          18.6748 |           4.7277 |
[32m[20230117 13:36:26 @agent_ppo2.py:193][0m |          -0.0126 |          18.1898 |           4.7199 |
[32m[20230117 13:36:26 @agent_ppo2.py:193][0m |          -0.0110 |          17.8584 |           4.7214 |
[32m[20230117 13:36:26 @agent_ppo2.py:193][0m |          -0.0133 |          17.5568 |           4.7206 |
[32m[20230117 13:36:26 @agent_ppo2.py:193][0m |          -0.0115 |          17.4902 |           4.7182 |
[32m[20230117 13:36:26 @agent_ppo2.py:193][0m |          -0.0157 |          17.1017 |           4.7199 |
[32m[20230117 13:36:26 @agent_ppo2.py:193][0m |          -0.0151 |          17.3410 |           4.7205 |
[32m[20230117 13:36:26 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:36:26 @agent_ppo2.py:146][0m Average TRAINING episode reward: 186.97
[32m[20230117 13:36:26 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.51
[32m[20230117 13:36:26 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 80.77
[32m[20230117 13:36:26 @agent_ppo2.py:151][0m Total time:      10.02 min
[32m[20230117 13:36:26 @agent_ppo2.py:153][0m 937984 total steps have happened
[32m[20230117 13:36:26 @agent_ppo2.py:129][0m #------------------------ Iteration 458 --------------------------#
[32m[20230117 13:36:26 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:26 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:26 @agent_ppo2.py:193][0m |           0.0003 |          17.0941 |           4.7480 |
[32m[20230117 13:36:27 @agent_ppo2.py:193][0m |          -0.0062 |          14.8780 |           4.7438 |
[32m[20230117 13:36:27 @agent_ppo2.py:193][0m |          -0.0083 |          13.7821 |           4.7444 |
[32m[20230117 13:36:27 @agent_ppo2.py:193][0m |          -0.0088 |          13.0849 |           4.7397 |
[32m[20230117 13:36:27 @agent_ppo2.py:193][0m |          -0.0099 |          12.5309 |           4.7399 |
[32m[20230117 13:36:27 @agent_ppo2.py:193][0m |          -0.0114 |          11.9902 |           4.7416 |
[32m[20230117 13:36:27 @agent_ppo2.py:193][0m |          -0.0122 |          11.5709 |           4.7377 |
[32m[20230117 13:36:27 @agent_ppo2.py:193][0m |          -0.0120 |          11.2104 |           4.7394 |
[32m[20230117 13:36:27 @agent_ppo2.py:193][0m |          -0.0143 |          10.6713 |           4.7360 |
[32m[20230117 13:36:27 @agent_ppo2.py:193][0m |          -0.0131 |          10.2881 |           4.7370 |
[32m[20230117 13:36:27 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:36:27 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.51
[32m[20230117 13:36:27 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.70
[32m[20230117 13:36:27 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.82
[32m[20230117 13:36:27 @agent_ppo2.py:151][0m Total time:      10.04 min
[32m[20230117 13:36:27 @agent_ppo2.py:153][0m 940032 total steps have happened
[32m[20230117 13:36:27 @agent_ppo2.py:129][0m #------------------------ Iteration 459 --------------------------#
[32m[20230117 13:36:27 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:28 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:28 @agent_ppo2.py:193][0m |           0.0009 |          20.6487 |           4.7631 |
[32m[20230117 13:36:28 @agent_ppo2.py:193][0m |          -0.0047 |          18.2258 |           4.7562 |
[32m[20230117 13:36:28 @agent_ppo2.py:193][0m |          -0.0066 |          17.3557 |           4.7523 |
[32m[20230117 13:36:28 @agent_ppo2.py:193][0m |          -0.0077 |          16.8328 |           4.7509 |
[32m[20230117 13:36:28 @agent_ppo2.py:193][0m |          -0.0085 |          16.4391 |           4.7502 |
[32m[20230117 13:36:28 @agent_ppo2.py:193][0m |          -0.0095 |          16.0906 |           4.7489 |
[32m[20230117 13:36:28 @agent_ppo2.py:193][0m |          -0.0109 |          15.7600 |           4.7455 |
[32m[20230117 13:36:28 @agent_ppo2.py:193][0m |          -0.0113 |          15.4154 |           4.7494 |
[32m[20230117 13:36:28 @agent_ppo2.py:193][0m |          -0.0125 |          15.1455 |           4.7477 |
[32m[20230117 13:36:28 @agent_ppo2.py:193][0m |          -0.0119 |          14.8496 |           4.7449 |
[32m[20230117 13:36:28 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:36:28 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.32
[32m[20230117 13:36:28 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.73
[32m[20230117 13:36:28 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.38
[32m[20230117 13:36:28 @agent_ppo2.py:151][0m Total time:      10.06 min
[32m[20230117 13:36:28 @agent_ppo2.py:153][0m 942080 total steps have happened
[32m[20230117 13:36:28 @agent_ppo2.py:129][0m #------------------------ Iteration 460 --------------------------#
[32m[20230117 13:36:29 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:29 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:29 @agent_ppo2.py:193][0m |          -0.0022 |          27.8129 |           4.7931 |
[32m[20230117 13:36:29 @agent_ppo2.py:193][0m |          -0.0075 |          21.4859 |           4.7871 |
[32m[20230117 13:36:29 @agent_ppo2.py:193][0m |          -0.0080 |          20.4174 |           4.7831 |
[32m[20230117 13:36:29 @agent_ppo2.py:193][0m |          -0.0100 |          19.8058 |           4.7765 |
[32m[20230117 13:36:29 @agent_ppo2.py:193][0m |          -0.0114 |          19.6771 |           4.7760 |
[32m[20230117 13:36:29 @agent_ppo2.py:193][0m |          -0.0122 |          19.1812 |           4.7760 |
[32m[20230117 13:36:29 @agent_ppo2.py:193][0m |          -0.0141 |          19.1237 |           4.7669 |
[32m[20230117 13:36:29 @agent_ppo2.py:193][0m |          -0.0137 |          19.0724 |           4.7728 |
[32m[20230117 13:36:29 @agent_ppo2.py:193][0m |          -0.0167 |          18.6485 |           4.7727 |
[32m[20230117 13:36:29 @agent_ppo2.py:193][0m |          -0.0159 |          18.3352 |           4.7739 |
[32m[20230117 13:36:29 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:36:30 @agent_ppo2.py:146][0m Average TRAINING episode reward: 189.91
[32m[20230117 13:36:30 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.97
[32m[20230117 13:36:30 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.31
[32m[20230117 13:36:30 @agent_ppo2.py:151][0m Total time:      10.08 min
[32m[20230117 13:36:30 @agent_ppo2.py:153][0m 944128 total steps have happened
[32m[20230117 13:36:30 @agent_ppo2.py:129][0m #------------------------ Iteration 461 --------------------------#
[32m[20230117 13:36:30 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:30 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:30 @agent_ppo2.py:193][0m |           0.0018 |          23.3338 |           4.7725 |
[32m[20230117 13:36:30 @agent_ppo2.py:193][0m |          -0.0061 |          18.4247 |           4.7643 |
[32m[20230117 13:36:30 @agent_ppo2.py:193][0m |          -0.0106 |          16.8631 |           4.7635 |
[32m[20230117 13:36:30 @agent_ppo2.py:193][0m |          -0.0128 |          15.8106 |           4.7603 |
[32m[20230117 13:36:30 @agent_ppo2.py:193][0m |          -0.0125 |          15.0368 |           4.7630 |
[32m[20230117 13:36:30 @agent_ppo2.py:193][0m |          -0.0138 |          14.4531 |           4.7622 |
[32m[20230117 13:36:30 @agent_ppo2.py:193][0m |          -0.0160 |          14.0049 |           4.7644 |
[32m[20230117 13:36:30 @agent_ppo2.py:193][0m |          -0.0167 |          13.7749 |           4.7646 |
[32m[20230117 13:36:30 @agent_ppo2.py:193][0m |          -0.0168 |          13.4886 |           4.7598 |
[32m[20230117 13:36:30 @agent_ppo2.py:193][0m |          -0.0174 |          13.2882 |           4.7665 |
[32m[20230117 13:36:30 @agent_ppo2.py:138][0m Policy update time: 0.69 s
[32m[20230117 13:36:31 @agent_ppo2.py:146][0m Average TRAINING episode reward: 225.66
[32m[20230117 13:36:31 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.11
[32m[20230117 13:36:31 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.43
[32m[20230117 13:36:31 @agent_ppo2.py:151][0m Total time:      10.09 min
[32m[20230117 13:36:31 @agent_ppo2.py:153][0m 946176 total steps have happened
[32m[20230117 13:36:31 @agent_ppo2.py:129][0m #------------------------ Iteration 462 --------------------------#
[32m[20230117 13:36:31 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:31 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:31 @agent_ppo2.py:193][0m |          -0.0011 |          39.9570 |           4.7564 |
[32m[20230117 13:36:31 @agent_ppo2.py:193][0m |          -0.0046 |          29.3738 |           4.7465 |
[32m[20230117 13:36:31 @agent_ppo2.py:193][0m |          -0.0067 |          26.0621 |           4.7447 |
[32m[20230117 13:36:31 @agent_ppo2.py:193][0m |          -0.0087 |          24.2368 |           4.7439 |
[32m[20230117 13:36:31 @agent_ppo2.py:193][0m |          -0.0091 |          22.9246 |           4.7410 |
[32m[20230117 13:36:31 @agent_ppo2.py:193][0m |          -0.0096 |          22.1367 |           4.7388 |
[32m[20230117 13:36:31 @agent_ppo2.py:193][0m |          -0.0125 |          21.5272 |           4.7386 |
[32m[20230117 13:36:31 @agent_ppo2.py:193][0m |          -0.0129 |          21.0040 |           4.7434 |
[32m[20230117 13:36:31 @agent_ppo2.py:193][0m |          -0.0127 |          20.5341 |           4.7373 |
[32m[20230117 13:36:32 @agent_ppo2.py:193][0m |          -0.0143 |          20.0702 |           4.7360 |
[32m[20230117 13:36:32 @agent_ppo2.py:138][0m Policy update time: 0.69 s
[32m[20230117 13:36:32 @agent_ppo2.py:146][0m Average TRAINING episode reward: 220.50
[32m[20230117 13:36:32 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.35
[32m[20230117 13:36:32 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.82
[32m[20230117 13:36:32 @agent_ppo2.py:151][0m Total time:      10.11 min
[32m[20230117 13:36:32 @agent_ppo2.py:153][0m 948224 total steps have happened
[32m[20230117 13:36:32 @agent_ppo2.py:129][0m #------------------------ Iteration 463 --------------------------#
[32m[20230117 13:36:32 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:36:32 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:32 @agent_ppo2.py:193][0m |          -0.0008 |          35.1185 |           4.7020 |
[32m[20230117 13:36:32 @agent_ppo2.py:193][0m |          -0.0057 |          28.5081 |           4.6946 |
[32m[20230117 13:36:32 @agent_ppo2.py:193][0m |          -0.0062 |          27.0525 |           4.6879 |
[32m[20230117 13:36:32 @agent_ppo2.py:193][0m |          -0.0096 |          25.1867 |           4.6832 |
[32m[20230117 13:36:32 @agent_ppo2.py:193][0m |          -0.0114 |          23.6778 |           4.6871 |
[32m[20230117 13:36:32 @agent_ppo2.py:193][0m |          -0.0115 |          23.1379 |           4.6749 |
[32m[20230117 13:36:32 @agent_ppo2.py:193][0m |          -0.0135 |          22.2537 |           4.6828 |
[32m[20230117 13:36:33 @agent_ppo2.py:193][0m |          -0.0135 |          21.4573 |           4.6798 |
[32m[20230117 13:36:33 @agent_ppo2.py:193][0m |          -0.0144 |          20.8540 |           4.6837 |
[32m[20230117 13:36:33 @agent_ppo2.py:193][0m |          -0.0148 |          20.4450 |           4.6836 |
[32m[20230117 13:36:33 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:36:33 @agent_ppo2.py:146][0m Average TRAINING episode reward: 201.62
[32m[20230117 13:36:33 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.17
[32m[20230117 13:36:33 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.20
[32m[20230117 13:36:33 @agent_ppo2.py:151][0m Total time:      10.13 min
[32m[20230117 13:36:33 @agent_ppo2.py:153][0m 950272 total steps have happened
[32m[20230117 13:36:33 @agent_ppo2.py:129][0m #------------------------ Iteration 464 --------------------------#
[32m[20230117 13:36:33 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:33 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:33 @agent_ppo2.py:193][0m |          -0.0011 |          20.4504 |           4.7033 |
[32m[20230117 13:36:33 @agent_ppo2.py:193][0m |          -0.0026 |          17.3923 |           4.6993 |
[32m[20230117 13:36:33 @agent_ppo2.py:193][0m |          -0.0048 |          15.8651 |           4.6940 |
[32m[20230117 13:36:33 @agent_ppo2.py:193][0m |           0.0026 |          15.1055 |           4.6987 |
[32m[20230117 13:36:33 @agent_ppo2.py:193][0m |          -0.0086 |          13.7625 |           4.6941 |
[32m[20230117 13:36:34 @agent_ppo2.py:193][0m |          -0.0087 |          12.9810 |           4.6927 |
[32m[20230117 13:36:34 @agent_ppo2.py:193][0m |          -0.0069 |          12.5026 |           4.6930 |
[32m[20230117 13:36:34 @agent_ppo2.py:193][0m |          -0.0102 |          12.1949 |           4.6920 |
[32m[20230117 13:36:34 @agent_ppo2.py:193][0m |          -0.0103 |          11.7865 |           4.6916 |
[32m[20230117 13:36:34 @agent_ppo2.py:193][0m |          -0.0120 |          11.6456 |           4.6942 |
[32m[20230117 13:36:34 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:36:34 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.62
[32m[20230117 13:36:34 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.38
[32m[20230117 13:36:34 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.74
[32m[20230117 13:36:34 @agent_ppo2.py:151][0m Total time:      10.15 min
[32m[20230117 13:36:34 @agent_ppo2.py:153][0m 952320 total steps have happened
[32m[20230117 13:36:34 @agent_ppo2.py:129][0m #------------------------ Iteration 465 --------------------------#
[32m[20230117 13:36:34 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:34 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:34 @agent_ppo2.py:193][0m |          -0.0038 |          24.9031 |           4.7251 |
[32m[20230117 13:36:34 @agent_ppo2.py:193][0m |          -0.0122 |          21.3475 |           4.7223 |
[32m[20230117 13:36:34 @agent_ppo2.py:193][0m |          -0.0061 |          20.6114 |           4.7170 |
[32m[20230117 13:36:35 @agent_ppo2.py:193][0m |          -0.0083 |          20.3973 |           4.7165 |
[32m[20230117 13:36:35 @agent_ppo2.py:193][0m |          -0.0040 |          20.2392 |           4.7157 |
[32m[20230117 13:36:35 @agent_ppo2.py:193][0m |          -0.0091 |          19.9028 |           4.7196 |
[32m[20230117 13:36:35 @agent_ppo2.py:193][0m |          -0.0144 |          19.7616 |           4.7174 |
[32m[20230117 13:36:35 @agent_ppo2.py:193][0m |          -0.0118 |          19.5936 |           4.7157 |
[32m[20230117 13:36:35 @agent_ppo2.py:193][0m |          -0.0241 |          19.4803 |           4.7206 |
[32m[20230117 13:36:35 @agent_ppo2.py:193][0m |           0.0042 |          21.1161 |           4.7143 |
[32m[20230117 13:36:35 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:36:35 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.44
[32m[20230117 13:36:35 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.27
[32m[20230117 13:36:35 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.72
[32m[20230117 13:36:35 @agent_ppo2.py:151][0m Total time:      10.17 min
[32m[20230117 13:36:35 @agent_ppo2.py:153][0m 954368 total steps have happened
[32m[20230117 13:36:35 @agent_ppo2.py:129][0m #------------------------ Iteration 466 --------------------------#
[32m[20230117 13:36:35 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:36:35 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:36 @agent_ppo2.py:193][0m |           0.0008 |          37.8099 |           4.8222 |
[32m[20230117 13:36:36 @agent_ppo2.py:193][0m |          -0.0042 |          33.1767 |           4.8142 |
[32m[20230117 13:36:36 @agent_ppo2.py:193][0m |          -0.0070 |          30.9039 |           4.8122 |
[32m[20230117 13:36:36 @agent_ppo2.py:193][0m |          -0.0073 |          29.4595 |           4.8143 |
[32m[20230117 13:36:36 @agent_ppo2.py:193][0m |          -0.0081 |          28.1932 |           4.8113 |
[32m[20230117 13:36:36 @agent_ppo2.py:193][0m |          -0.0102 |          27.4306 |           4.8093 |
[32m[20230117 13:36:36 @agent_ppo2.py:193][0m |          -0.0106 |          26.8080 |           4.8078 |
[32m[20230117 13:36:36 @agent_ppo2.py:193][0m |          -0.0105 |          26.1876 |           4.8109 |
[32m[20230117 13:36:36 @agent_ppo2.py:193][0m |          -0.0112 |          25.6190 |           4.8097 |
[32m[20230117 13:36:36 @agent_ppo2.py:193][0m |          -0.0119 |          25.2134 |           4.8104 |
[32m[20230117 13:36:36 @agent_ppo2.py:138][0m Policy update time: 0.77 s
[32m[20230117 13:36:36 @agent_ppo2.py:146][0m Average TRAINING episode reward: 198.26
[32m[20230117 13:36:36 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.70
[32m[20230117 13:36:36 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.83
[32m[20230117 13:36:36 @agent_ppo2.py:151][0m Total time:      10.19 min
[32m[20230117 13:36:36 @agent_ppo2.py:153][0m 956416 total steps have happened
[32m[20230117 13:36:36 @agent_ppo2.py:129][0m #------------------------ Iteration 467 --------------------------#
[32m[20230117 13:36:37 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:37 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:37 @agent_ppo2.py:193][0m |          -0.0014 |          20.3163 |           4.7818 |
[32m[20230117 13:36:37 @agent_ppo2.py:193][0m |          -0.0036 |          18.4076 |           4.7730 |
[32m[20230117 13:36:37 @agent_ppo2.py:193][0m |          -0.0027 |          17.8162 |           4.7731 |
[32m[20230117 13:36:37 @agent_ppo2.py:193][0m |          -0.0031 |          17.3896 |           4.7699 |
[32m[20230117 13:36:37 @agent_ppo2.py:193][0m |          -0.0093 |          16.7791 |           4.7689 |
[32m[20230117 13:36:37 @agent_ppo2.py:193][0m |          -0.0089 |          16.4894 |           4.7696 |
[32m[20230117 13:36:37 @agent_ppo2.py:193][0m |          -0.0090 |          16.1267 |           4.7647 |
[32m[20230117 13:36:37 @agent_ppo2.py:193][0m |          -0.0118 |          15.6891 |           4.7708 |
[32m[20230117 13:36:37 @agent_ppo2.py:193][0m |          -0.0103 |          15.4774 |           4.7652 |
[32m[20230117 13:36:37 @agent_ppo2.py:193][0m |          -0.0098 |          15.2487 |           4.7677 |
[32m[20230117 13:36:37 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:36:37 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.84
[32m[20230117 13:36:37 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 271.41
[32m[20230117 13:36:37 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.14
[32m[20230117 13:36:37 @agent_ppo2.py:151][0m Total time:      10.21 min
[32m[20230117 13:36:37 @agent_ppo2.py:153][0m 958464 total steps have happened
[32m[20230117 13:36:37 @agent_ppo2.py:129][0m #------------------------ Iteration 468 --------------------------#
[32m[20230117 13:36:38 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:38 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:38 @agent_ppo2.py:193][0m |          -0.0001 |          20.4704 |           4.8199 |
[32m[20230117 13:36:38 @agent_ppo2.py:193][0m |          -0.0064 |          18.6962 |           4.8072 |
[32m[20230117 13:36:38 @agent_ppo2.py:193][0m |          -0.0083 |          18.1824 |           4.8086 |
[32m[20230117 13:36:38 @agent_ppo2.py:193][0m |          -0.0095 |          17.9664 |           4.8049 |
[32m[20230117 13:36:38 @agent_ppo2.py:193][0m |          -0.0093 |          17.6857 |           4.8031 |
[32m[20230117 13:36:38 @agent_ppo2.py:193][0m |          -0.0103 |          17.3903 |           4.8049 |
[32m[20230117 13:36:38 @agent_ppo2.py:193][0m |          -0.0119 |          17.1668 |           4.8024 |
[32m[20230117 13:36:38 @agent_ppo2.py:193][0m |          -0.0119 |          17.0366 |           4.8024 |
[32m[20230117 13:36:38 @agent_ppo2.py:193][0m |          -0.0126 |          17.0599 |           4.8001 |
[32m[20230117 13:36:38 @agent_ppo2.py:193][0m |          -0.0114 |          16.9595 |           4.8042 |
[32m[20230117 13:36:38 @agent_ppo2.py:138][0m Policy update time: 0.75 s
[32m[20230117 13:36:39 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.87
[32m[20230117 13:36:39 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.93
[32m[20230117 13:36:39 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.45
[32m[20230117 13:36:39 @agent_ppo2.py:151][0m Total time:      10.23 min
[32m[20230117 13:36:39 @agent_ppo2.py:153][0m 960512 total steps have happened
[32m[20230117 13:36:39 @agent_ppo2.py:129][0m #------------------------ Iteration 469 --------------------------#
[32m[20230117 13:36:39 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:36:39 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:39 @agent_ppo2.py:193][0m |           0.0012 |          42.9980 |           4.7239 |
[32m[20230117 13:36:39 @agent_ppo2.py:193][0m |          -0.0032 |          25.2273 |           4.7174 |
[32m[20230117 13:36:39 @agent_ppo2.py:193][0m |          -0.0057 |          21.5807 |           4.7216 |
[32m[20230117 13:36:39 @agent_ppo2.py:193][0m |          -0.0082 |          20.4783 |           4.7210 |
[32m[20230117 13:36:39 @agent_ppo2.py:193][0m |          -0.0083 |          19.4723 |           4.7142 |
[32m[20230117 13:36:39 @agent_ppo2.py:193][0m |          -0.0107 |          18.2447 |           4.7209 |
[32m[20230117 13:36:39 @agent_ppo2.py:193][0m |          -0.0122 |          17.4510 |           4.7125 |
[32m[20230117 13:36:39 @agent_ppo2.py:193][0m |          -0.0123 |          17.1281 |           4.7177 |
[32m[20230117 13:36:40 @agent_ppo2.py:193][0m |          -0.0124 |          16.3477 |           4.7152 |
[32m[20230117 13:36:40 @agent_ppo2.py:193][0m |          -0.0133 |          16.0595 |           4.7183 |
[32m[20230117 13:36:40 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:36:40 @agent_ppo2.py:146][0m Average TRAINING episode reward: 147.13
[32m[20230117 13:36:40 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.20
[32m[20230117 13:36:40 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.34
[32m[20230117 13:36:40 @agent_ppo2.py:151][0m Total time:      10.25 min
[32m[20230117 13:36:40 @agent_ppo2.py:153][0m 962560 total steps have happened
[32m[20230117 13:36:40 @agent_ppo2.py:129][0m #------------------------ Iteration 470 --------------------------#
[32m[20230117 13:36:40 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:36:40 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:40 @agent_ppo2.py:193][0m |          -0.0004 |          18.0172 |           4.7590 |
[32m[20230117 13:36:40 @agent_ppo2.py:193][0m |          -0.0047 |          17.5066 |           4.7520 |
[32m[20230117 13:36:40 @agent_ppo2.py:193][0m |          -0.0051 |          17.0862 |           4.7456 |
[32m[20230117 13:36:40 @agent_ppo2.py:193][0m |          -0.0059 |          16.9838 |           4.7460 |
[32m[20230117 13:36:40 @agent_ppo2.py:193][0m |          -0.0085 |          16.4188 |           4.7409 |
[32m[20230117 13:36:41 @agent_ppo2.py:193][0m |          -0.0099 |          16.1202 |           4.7449 |
[32m[20230117 13:36:41 @agent_ppo2.py:193][0m |          -0.0092 |          16.0562 |           4.7420 |
[32m[20230117 13:36:41 @agent_ppo2.py:193][0m |          -0.0104 |          15.8542 |           4.7412 |
[32m[20230117 13:36:41 @agent_ppo2.py:193][0m |          -0.0117 |          15.7100 |           4.7374 |
[32m[20230117 13:36:41 @agent_ppo2.py:193][0m |          -0.0119 |          15.5960 |           4.7401 |
[32m[20230117 13:36:41 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:36:41 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.94
[32m[20230117 13:36:41 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.99
[32m[20230117 13:36:41 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.96
[32m[20230117 13:36:41 @agent_ppo2.py:151][0m Total time:      10.27 min
[32m[20230117 13:36:41 @agent_ppo2.py:153][0m 964608 total steps have happened
[32m[20230117 13:36:41 @agent_ppo2.py:129][0m #------------------------ Iteration 471 --------------------------#
[32m[20230117 13:36:41 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:36:41 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:41 @agent_ppo2.py:193][0m |           0.0024 |          20.2610 |           4.8611 |
[32m[20230117 13:36:41 @agent_ppo2.py:193][0m |          -0.0052 |          19.1671 |           4.8525 |
[32m[20230117 13:36:41 @agent_ppo2.py:193][0m |          -0.0078 |          18.4313 |           4.8383 |
[32m[20230117 13:36:42 @agent_ppo2.py:193][0m |          -0.0085 |          17.6693 |           4.8462 |
[32m[20230117 13:36:42 @agent_ppo2.py:193][0m |          -0.0111 |          17.0165 |           4.8377 |
[32m[20230117 13:36:42 @agent_ppo2.py:193][0m |          -0.0081 |          16.7909 |           4.8396 |
[32m[20230117 13:36:42 @agent_ppo2.py:193][0m |          -0.0111 |          16.2050 |           4.8377 |
[32m[20230117 13:36:42 @agent_ppo2.py:193][0m |          -0.0117 |          15.9365 |           4.8288 |
[32m[20230117 13:36:42 @agent_ppo2.py:193][0m |          -0.0130 |          15.6069 |           4.8370 |
[32m[20230117 13:36:42 @agent_ppo2.py:193][0m |          -0.0136 |          15.3899 |           4.8281 |
[32m[20230117 13:36:42 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:36:42 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.48
[32m[20230117 13:36:42 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.33
[32m[20230117 13:36:42 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.69
[32m[20230117 13:36:42 @agent_ppo2.py:151][0m Total time:      10.29 min
[32m[20230117 13:36:42 @agent_ppo2.py:153][0m 966656 total steps have happened
[32m[20230117 13:36:42 @agent_ppo2.py:129][0m #------------------------ Iteration 472 --------------------------#
[32m[20230117 13:36:42 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:36:42 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:43 @agent_ppo2.py:193][0m |           0.0028 |          32.0226 |           4.7306 |
[32m[20230117 13:36:43 @agent_ppo2.py:193][0m |          -0.0046 |          25.9941 |           4.7257 |
[32m[20230117 13:36:43 @agent_ppo2.py:193][0m |          -0.0073 |          23.7672 |           4.7270 |
[32m[20230117 13:36:43 @agent_ppo2.py:193][0m |          -0.0099 |          22.1904 |           4.7292 |
[32m[20230117 13:36:43 @agent_ppo2.py:193][0m |          -0.0100 |          21.1549 |           4.7288 |
[32m[20230117 13:36:43 @agent_ppo2.py:193][0m |          -0.0109 |          20.1784 |           4.7303 |
[32m[20230117 13:36:43 @agent_ppo2.py:193][0m |          -0.0120 |          19.6029 |           4.7268 |
[32m[20230117 13:36:43 @agent_ppo2.py:193][0m |          -0.0124 |          18.9935 |           4.7291 |
[32m[20230117 13:36:43 @agent_ppo2.py:193][0m |          -0.0116 |          18.6168 |           4.7278 |
[32m[20230117 13:36:43 @agent_ppo2.py:193][0m |          -0.0138 |          18.2776 |           4.7269 |
[32m[20230117 13:36:43 @agent_ppo2.py:138][0m Policy update time: 0.78 s
[32m[20230117 13:36:43 @agent_ppo2.py:146][0m Average TRAINING episode reward: 210.16
[32m[20230117 13:36:43 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.76
[32m[20230117 13:36:43 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.41
[32m[20230117 13:36:43 @agent_ppo2.py:151][0m Total time:      10.31 min
[32m[20230117 13:36:43 @agent_ppo2.py:153][0m 968704 total steps have happened
[32m[20230117 13:36:43 @agent_ppo2.py:129][0m #------------------------ Iteration 473 --------------------------#
[32m[20230117 13:36:44 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:44 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:44 @agent_ppo2.py:193][0m |          -0.0019 |          21.1323 |           4.7407 |
[32m[20230117 13:36:44 @agent_ppo2.py:193][0m |          -0.0066 |          19.2361 |           4.7347 |
[32m[20230117 13:36:44 @agent_ppo2.py:193][0m |          -0.0088 |          18.7111 |           4.7334 |
[32m[20230117 13:36:44 @agent_ppo2.py:193][0m |          -0.0081 |          18.2647 |           4.7346 |
[32m[20230117 13:36:44 @agent_ppo2.py:193][0m |          -0.0123 |          17.7240 |           4.7341 |
[32m[20230117 13:36:44 @agent_ppo2.py:193][0m |          -0.0152 |          17.4015 |           4.7286 |
[32m[20230117 13:36:44 @agent_ppo2.py:193][0m |          -0.0141 |          17.1520 |           4.7335 |
[32m[20230117 13:36:44 @agent_ppo2.py:193][0m |          -0.0149 |          16.8575 |           4.7339 |
[32m[20230117 13:36:44 @agent_ppo2.py:193][0m |          -0.0156 |          16.6764 |           4.7342 |
[32m[20230117 13:36:44 @agent_ppo2.py:193][0m |          -0.0167 |          16.4761 |           4.7311 |
[32m[20230117 13:36:44 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:36:44 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.03
[32m[20230117 13:36:44 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.97
[32m[20230117 13:36:44 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.71
[32m[20230117 13:36:44 @agent_ppo2.py:151][0m Total time:      10.32 min
[32m[20230117 13:36:44 @agent_ppo2.py:153][0m 970752 total steps have happened
[32m[20230117 13:36:44 @agent_ppo2.py:129][0m #------------------------ Iteration 474 --------------------------#
[32m[20230117 13:36:45 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:45 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:45 @agent_ppo2.py:193][0m |           0.0310 |          23.9473 |           4.7740 |
[32m[20230117 13:36:45 @agent_ppo2.py:193][0m |           0.0662 |          24.0318 |           4.7675 |
[32m[20230117 13:36:45 @agent_ppo2.py:193][0m |          -0.0042 |          15.5712 |           4.7677 |
[32m[20230117 13:36:45 @agent_ppo2.py:193][0m |          -0.0079 |          14.3250 |           4.7665 |
[32m[20230117 13:36:45 @agent_ppo2.py:193][0m |          -0.0105 |          13.7325 |           4.7639 |
[32m[20230117 13:36:45 @agent_ppo2.py:193][0m |          -0.0144 |          13.3154 |           4.7637 |
[32m[20230117 13:36:45 @agent_ppo2.py:193][0m |          -0.0188 |          12.8963 |           4.7620 |
[32m[20230117 13:36:45 @agent_ppo2.py:193][0m |          -0.0135 |          12.5530 |           4.7668 |
[32m[20230117 13:36:45 @agent_ppo2.py:193][0m |          -0.0180 |          12.2934 |           4.7615 |
[32m[20230117 13:36:45 @agent_ppo2.py:193][0m |          -0.0153 |          12.0782 |           4.7627 |
[32m[20230117 13:36:45 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:36:46 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.52
[32m[20230117 13:36:46 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.20
[32m[20230117 13:36:46 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.11
[32m[20230117 13:36:46 @agent_ppo2.py:151][0m Total time:      10.34 min
[32m[20230117 13:36:46 @agent_ppo2.py:153][0m 972800 total steps have happened
[32m[20230117 13:36:46 @agent_ppo2.py:129][0m #------------------------ Iteration 475 --------------------------#
[32m[20230117 13:36:46 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:46 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:46 @agent_ppo2.py:193][0m |           0.0007 |          20.7741 |           4.7848 |
[32m[20230117 13:36:46 @agent_ppo2.py:193][0m |          -0.0054 |          18.2723 |           4.7757 |
[32m[20230117 13:36:46 @agent_ppo2.py:193][0m |          -0.0065 |          17.9502 |           4.7762 |
[32m[20230117 13:36:46 @agent_ppo2.py:193][0m |          -0.0102 |          17.1156 |           4.7771 |
[32m[20230117 13:36:46 @agent_ppo2.py:193][0m |          -0.0120 |          16.7768 |           4.7720 |
[32m[20230117 13:36:46 @agent_ppo2.py:193][0m |          -0.0097 |          17.0515 |           4.7762 |
[32m[20230117 13:36:46 @agent_ppo2.py:193][0m |          -0.0132 |          16.1495 |           4.7761 |
[32m[20230117 13:36:46 @agent_ppo2.py:193][0m |          -0.0131 |          15.8239 |           4.7733 |
[32m[20230117 13:36:46 @agent_ppo2.py:193][0m |          -0.0117 |          15.6749 |           4.7757 |
[32m[20230117 13:36:46 @agent_ppo2.py:193][0m |          -0.0115 |          15.6662 |           4.7751 |
[32m[20230117 13:36:46 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:36:47 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.58
[32m[20230117 13:36:47 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.94
[32m[20230117 13:36:47 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.14
[32m[20230117 13:36:47 @agent_ppo2.py:151][0m Total time:      10.36 min
[32m[20230117 13:36:47 @agent_ppo2.py:153][0m 974848 total steps have happened
[32m[20230117 13:36:47 @agent_ppo2.py:129][0m #------------------------ Iteration 476 --------------------------#
[32m[20230117 13:36:47 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:36:47 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:47 @agent_ppo2.py:193][0m |          -0.0002 |          30.3903 |           4.8170 |
[32m[20230117 13:36:47 @agent_ppo2.py:193][0m |          -0.0115 |          24.2096 |           4.8118 |
[32m[20230117 13:36:47 @agent_ppo2.py:193][0m |          -0.0110 |          22.5745 |           4.8012 |
[32m[20230117 13:36:47 @agent_ppo2.py:193][0m |          -0.0162 |          21.7734 |           4.8054 |
[32m[20230117 13:36:47 @agent_ppo2.py:193][0m |          -0.0139 |          21.1037 |           4.8001 |
[32m[20230117 13:36:47 @agent_ppo2.py:193][0m |          -0.0181 |          22.0777 |           4.8012 |
[32m[20230117 13:36:47 @agent_ppo2.py:193][0m |          -0.0140 |          21.1860 |           4.7994 |
[32m[20230117 13:36:47 @agent_ppo2.py:193][0m |          -0.0161 |          19.8959 |           4.8058 |
[32m[20230117 13:36:48 @agent_ppo2.py:193][0m |           0.0007 |          23.8567 |           4.8024 |
[32m[20230117 13:36:48 @agent_ppo2.py:193][0m |          -0.0211 |          19.3880 |           4.7918 |
[32m[20230117 13:36:48 @agent_ppo2.py:138][0m Policy update time: 0.73 s
[32m[20230117 13:36:48 @agent_ppo2.py:146][0m Average TRAINING episode reward: 188.46
[32m[20230117 13:36:48 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.36
[32m[20230117 13:36:48 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.74
[32m[20230117 13:36:48 @agent_ppo2.py:151][0m Total time:      10.38 min
[32m[20230117 13:36:48 @agent_ppo2.py:153][0m 976896 total steps have happened
[32m[20230117 13:36:48 @agent_ppo2.py:129][0m #------------------------ Iteration 477 --------------------------#
[32m[20230117 13:36:48 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:36:48 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:48 @agent_ppo2.py:193][0m |           0.0016 |          22.5891 |           4.7432 |
[32m[20230117 13:36:48 @agent_ppo2.py:193][0m |          -0.0061 |          20.7778 |           4.7281 |
[32m[20230117 13:36:48 @agent_ppo2.py:193][0m |          -0.0068 |          20.4518 |           4.7260 |
[32m[20230117 13:36:48 @agent_ppo2.py:193][0m |          -0.0074 |          20.1990 |           4.7202 |
[32m[20230117 13:36:48 @agent_ppo2.py:193][0m |          -0.0070 |          20.0128 |           4.7213 |
[32m[20230117 13:36:48 @agent_ppo2.py:193][0m |          -0.0070 |          19.9184 |           4.7165 |
[32m[20230117 13:36:49 @agent_ppo2.py:193][0m |          -0.0103 |          19.3885 |           4.7191 |
[32m[20230117 13:36:49 @agent_ppo2.py:193][0m |          -0.0104 |          19.2478 |           4.7156 |
[32m[20230117 13:36:49 @agent_ppo2.py:193][0m |          -0.0124 |          18.8916 |           4.7149 |
[32m[20230117 13:36:49 @agent_ppo2.py:193][0m |          -0.0132 |          18.7199 |           4.7132 |
[32m[20230117 13:36:49 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:36:49 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.16
[32m[20230117 13:36:49 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.10
[32m[20230117 13:36:49 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.61
[32m[20230117 13:36:49 @agent_ppo2.py:151][0m Total time:      10.40 min
[32m[20230117 13:36:49 @agent_ppo2.py:153][0m 978944 total steps have happened
[32m[20230117 13:36:49 @agent_ppo2.py:129][0m #------------------------ Iteration 478 --------------------------#
[32m[20230117 13:36:49 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:49 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:49 @agent_ppo2.py:193][0m |          -0.0012 |          20.2066 |           4.8996 |
[32m[20230117 13:36:49 @agent_ppo2.py:193][0m |          -0.0046 |          18.9319 |           4.8933 |
[32m[20230117 13:36:49 @agent_ppo2.py:193][0m |          -0.0089 |          18.4185 |           4.8884 |
[32m[20230117 13:36:49 @agent_ppo2.py:193][0m |          -0.0089 |          18.1098 |           4.8844 |
[32m[20230117 13:36:50 @agent_ppo2.py:193][0m |          -0.0103 |          17.9107 |           4.8831 |
[32m[20230117 13:36:50 @agent_ppo2.py:193][0m |          -0.0115 |          17.7688 |           4.8827 |
[32m[20230117 13:36:50 @agent_ppo2.py:193][0m |          -0.0102 |          17.7706 |           4.8837 |
[32m[20230117 13:36:50 @agent_ppo2.py:193][0m |          -0.0144 |          17.4961 |           4.8862 |
[32m[20230117 13:36:50 @agent_ppo2.py:193][0m |          -0.0087 |          17.8395 |           4.8824 |
[32m[20230117 13:36:50 @agent_ppo2.py:193][0m |          -0.0128 |          17.3574 |           4.8853 |
[32m[20230117 13:36:50 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:36:50 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.29
[32m[20230117 13:36:50 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.28
[32m[20230117 13:36:50 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.70
[32m[20230117 13:36:50 @agent_ppo2.py:151][0m Total time:      10.42 min
[32m[20230117 13:36:50 @agent_ppo2.py:153][0m 980992 total steps have happened
[32m[20230117 13:36:50 @agent_ppo2.py:129][0m #------------------------ Iteration 479 --------------------------#
[32m[20230117 13:36:50 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:50 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:50 @agent_ppo2.py:193][0m |           0.0003 |          18.3392 |           4.7281 |
[32m[20230117 13:36:50 @agent_ppo2.py:193][0m |          -0.0032 |          14.7820 |           4.7224 |
[32m[20230117 13:36:50 @agent_ppo2.py:193][0m |          -0.0077 |          13.8966 |           4.7195 |
[32m[20230117 13:36:51 @agent_ppo2.py:193][0m |          -0.0090 |          13.2463 |           4.7198 |
[32m[20230117 13:36:51 @agent_ppo2.py:193][0m |          -0.0108 |          12.6184 |           4.7274 |
[32m[20230117 13:36:51 @agent_ppo2.py:193][0m |          -0.0121 |          11.8871 |           4.7323 |
[32m[20230117 13:36:51 @agent_ppo2.py:193][0m |          -0.0120 |          11.5660 |           4.7308 |
[32m[20230117 13:36:51 @agent_ppo2.py:193][0m |          -0.0133 |          11.3190 |           4.7312 |
[32m[20230117 13:36:51 @agent_ppo2.py:193][0m |          -0.0125 |          11.0151 |           4.7353 |
[32m[20230117 13:36:51 @agent_ppo2.py:193][0m |          -0.0131 |          10.7472 |           4.7353 |
[32m[20230117 13:36:51 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:36:51 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.86
[32m[20230117 13:36:51 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.84
[32m[20230117 13:36:51 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.26
[32m[20230117 13:36:51 @agent_ppo2.py:151][0m Total time:      10.43 min
[32m[20230117 13:36:51 @agent_ppo2.py:153][0m 983040 total steps have happened
[32m[20230117 13:36:51 @agent_ppo2.py:129][0m #------------------------ Iteration 480 --------------------------#
[32m[20230117 13:36:51 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:36:51 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:51 @agent_ppo2.py:193][0m |           0.0062 |          22.8786 |           4.7687 |
[32m[20230117 13:36:52 @agent_ppo2.py:193][0m |          -0.0081 |          19.8581 |           4.7651 |
[32m[20230117 13:36:52 @agent_ppo2.py:193][0m |          -0.0063 |          19.1720 |           4.7683 |
[32m[20230117 13:36:52 @agent_ppo2.py:193][0m |          -0.0087 |          18.7046 |           4.7701 |
[32m[20230117 13:36:52 @agent_ppo2.py:193][0m |          -0.0102 |          18.3799 |           4.7749 |
[32m[20230117 13:36:52 @agent_ppo2.py:193][0m |          -0.0124 |          18.1301 |           4.7724 |
[32m[20230117 13:36:52 @agent_ppo2.py:193][0m |          -0.0071 |          18.1563 |           4.7736 |
[32m[20230117 13:36:52 @agent_ppo2.py:193][0m |          -0.0081 |          18.3732 |           4.7776 |
[32m[20230117 13:36:52 @agent_ppo2.py:193][0m |          -0.0123 |          17.6272 |           4.7742 |
[32m[20230117 13:36:52 @agent_ppo2.py:193][0m |          -0.0052 |          18.4400 |           4.7803 |
[32m[20230117 13:36:52 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:36:52 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.62
[32m[20230117 13:36:52 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.23
[32m[20230117 13:36:52 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.31
[32m[20230117 13:36:52 @agent_ppo2.py:151][0m Total time:      10.45 min
[32m[20230117 13:36:52 @agent_ppo2.py:153][0m 985088 total steps have happened
[32m[20230117 13:36:52 @agent_ppo2.py:129][0m #------------------------ Iteration 481 --------------------------#
[32m[20230117 13:36:52 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:52 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:53 @agent_ppo2.py:193][0m |           0.0016 |          20.8543 |           4.9542 |
[32m[20230117 13:36:53 @agent_ppo2.py:193][0m |          -0.0038 |          19.6210 |           4.9502 |
[32m[20230117 13:36:53 @agent_ppo2.py:193][0m |          -0.0060 |          19.2065 |           4.9524 |
[32m[20230117 13:36:53 @agent_ppo2.py:193][0m |          -0.0073 |          18.7546 |           4.9465 |
[32m[20230117 13:36:53 @agent_ppo2.py:193][0m |          -0.0067 |          18.4309 |           4.9442 |
[32m[20230117 13:36:53 @agent_ppo2.py:193][0m |          -0.0085 |          18.0611 |           4.9439 |
[32m[20230117 13:36:53 @agent_ppo2.py:193][0m |          -0.0094 |          17.6088 |           4.9378 |
[32m[20230117 13:36:53 @agent_ppo2.py:193][0m |          -0.0104 |          17.0637 |           4.9441 |
[32m[20230117 13:36:53 @agent_ppo2.py:193][0m |          -0.0108 |          16.6035 |           4.9409 |
[32m[20230117 13:36:53 @agent_ppo2.py:193][0m |          -0.0108 |          16.3896 |           4.9370 |
[32m[20230117 13:36:53 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:36:53 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.41
[32m[20230117 13:36:53 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.68
[32m[20230117 13:36:53 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 269.56
[32m[20230117 13:36:53 @agent_ppo2.py:151][0m Total time:      10.47 min
[32m[20230117 13:36:53 @agent_ppo2.py:153][0m 987136 total steps have happened
[32m[20230117 13:36:53 @agent_ppo2.py:129][0m #------------------------ Iteration 482 --------------------------#
[32m[20230117 13:36:53 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:54 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:54 @agent_ppo2.py:193][0m |           0.0002 |          20.6707 |           4.9169 |
[32m[20230117 13:36:54 @agent_ppo2.py:193][0m |          -0.0048 |          18.6135 |           4.9132 |
[32m[20230117 13:36:54 @agent_ppo2.py:193][0m |          -0.0070 |          17.3468 |           4.9024 |
[32m[20230117 13:36:54 @agent_ppo2.py:193][0m |          -0.0095 |          16.4438 |           4.9048 |
[32m[20230117 13:36:54 @agent_ppo2.py:193][0m |          -0.0112 |          15.6231 |           4.9011 |
[32m[20230117 13:36:54 @agent_ppo2.py:193][0m |          -0.0102 |          15.1501 |           4.8977 |
[32m[20230117 13:36:54 @agent_ppo2.py:193][0m |          -0.0119 |          14.4099 |           4.8953 |
[32m[20230117 13:36:54 @agent_ppo2.py:193][0m |          -0.0117 |          13.9098 |           4.8946 |
[32m[20230117 13:36:54 @agent_ppo2.py:193][0m |          -0.0110 |          13.6946 |           4.8979 |
[32m[20230117 13:36:54 @agent_ppo2.py:193][0m |          -0.0136 |          13.1184 |           4.8937 |
[32m[20230117 13:36:54 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:36:54 @agent_ppo2.py:146][0m Average TRAINING episode reward: 266.25
[32m[20230117 13:36:54 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.85
[32m[20230117 13:36:54 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.64
[32m[20230117 13:36:54 @agent_ppo2.py:151][0m Total time:      10.49 min
[32m[20230117 13:36:54 @agent_ppo2.py:153][0m 989184 total steps have happened
[32m[20230117 13:36:54 @agent_ppo2.py:129][0m #------------------------ Iteration 483 --------------------------#
[32m[20230117 13:36:55 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:55 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:55 @agent_ppo2.py:193][0m |           0.0019 |          21.4376 |           4.7796 |
[32m[20230117 13:36:55 @agent_ppo2.py:193][0m |           0.0007 |          19.1777 |           4.7736 |
[32m[20230117 13:36:55 @agent_ppo2.py:193][0m |          -0.0053 |          18.3013 |           4.7713 |
[32m[20230117 13:36:55 @agent_ppo2.py:193][0m |          -0.0031 |          18.2196 |           4.7778 |
[32m[20230117 13:36:55 @agent_ppo2.py:193][0m |          -0.0060 |          17.4149 |           4.7749 |
[32m[20230117 13:36:55 @agent_ppo2.py:193][0m |          -0.0073 |          17.0527 |           4.7776 |
[32m[20230117 13:36:55 @agent_ppo2.py:193][0m |          -0.0098 |          16.6381 |           4.7759 |
[32m[20230117 13:36:55 @agent_ppo2.py:193][0m |          -0.0071 |          16.9455 |           4.7786 |
[32m[20230117 13:36:55 @agent_ppo2.py:193][0m |          -0.0103 |          16.0597 |           4.7764 |
[32m[20230117 13:36:55 @agent_ppo2.py:193][0m |          -0.0096 |          15.9982 |           4.7780 |
[32m[20230117 13:36:55 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:36:55 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.27
[32m[20230117 13:36:55 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 266.97
[32m[20230117 13:36:55 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 271.30
[32m[20230117 13:36:55 @agent_ppo2.py:151][0m Total time:      10.51 min
[32m[20230117 13:36:55 @agent_ppo2.py:153][0m 991232 total steps have happened
[32m[20230117 13:36:55 @agent_ppo2.py:129][0m #------------------------ Iteration 484 --------------------------#
[32m[20230117 13:36:56 @agent_ppo2.py:135][0m Sampling time: 0.26 s by 4 slaves
[32m[20230117 13:36:56 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:56 @agent_ppo2.py:193][0m |           0.0004 |          16.5764 |           4.9476 |
[32m[20230117 13:36:56 @agent_ppo2.py:193][0m |          -0.0051 |          12.5977 |           4.9437 |
[32m[20230117 13:36:56 @agent_ppo2.py:193][0m |          -0.0073 |          10.6792 |           4.9425 |
[32m[20230117 13:36:56 @agent_ppo2.py:193][0m |          -0.0090 |           9.6155 |           4.9405 |
[32m[20230117 13:36:56 @agent_ppo2.py:193][0m |          -0.0091 |           8.8180 |           4.9426 |
[32m[20230117 13:36:56 @agent_ppo2.py:193][0m |          -0.0109 |           8.2288 |           4.9392 |
[32m[20230117 13:36:56 @agent_ppo2.py:193][0m |          -0.0115 |           7.6483 |           4.9345 |
[32m[20230117 13:36:56 @agent_ppo2.py:193][0m |          -0.0118 |           7.1856 |           4.9369 |
[32m[20230117 13:36:56 @agent_ppo2.py:193][0m |          -0.0127 |           6.7067 |           4.9392 |
[32m[20230117 13:36:56 @agent_ppo2.py:193][0m |          -0.0123 |           6.2756 |           4.9365 |
[32m[20230117 13:36:56 @agent_ppo2.py:138][0m Policy update time: 0.69 s
[32m[20230117 13:36:57 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.59
[32m[20230117 13:36:57 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.45
[32m[20230117 13:36:57 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.79
[32m[20230117 13:36:57 @agent_ppo2.py:151][0m Total time:      10.53 min
[32m[20230117 13:36:57 @agent_ppo2.py:153][0m 993280 total steps have happened
[32m[20230117 13:36:57 @agent_ppo2.py:129][0m #------------------------ Iteration 485 --------------------------#
[32m[20230117 13:36:57 @agent_ppo2.py:135][0m Sampling time: 0.24 s by 4 slaves
[32m[20230117 13:36:57 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:57 @agent_ppo2.py:193][0m |          -0.0017 |          35.3875 |           4.9340 |
[32m[20230117 13:36:57 @agent_ppo2.py:193][0m |          -0.0060 |          26.3873 |           4.9291 |
[32m[20230117 13:36:57 @agent_ppo2.py:193][0m |          -0.0087 |          23.6308 |           4.9294 |
[32m[20230117 13:36:57 @agent_ppo2.py:193][0m |          -0.0112 |          22.3628 |           4.9282 |
[32m[20230117 13:36:57 @agent_ppo2.py:193][0m |          -0.0139 |          21.3646 |           4.9269 |
[32m[20230117 13:36:57 @agent_ppo2.py:193][0m |          -0.0126 |          20.4487 |           4.9247 |
[32m[20230117 13:36:57 @agent_ppo2.py:193][0m |          -0.0146 |          19.8796 |           4.9251 |
[32m[20230117 13:36:57 @agent_ppo2.py:193][0m |          -0.0156 |          19.2965 |           4.9244 |
[32m[20230117 13:36:58 @agent_ppo2.py:193][0m |          -0.0156 |          18.8600 |           4.9237 |
[32m[20230117 13:36:58 @agent_ppo2.py:193][0m |          -0.0162 |          18.4933 |           4.9262 |
[32m[20230117 13:36:58 @agent_ppo2.py:138][0m Policy update time: 0.76 s
[32m[20230117 13:36:58 @agent_ppo2.py:146][0m Average TRAINING episode reward: 204.57
[32m[20230117 13:36:58 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.96
[32m[20230117 13:36:58 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.71
[32m[20230117 13:36:58 @agent_ppo2.py:151][0m Total time:      10.55 min
[32m[20230117 13:36:58 @agent_ppo2.py:153][0m 995328 total steps have happened
[32m[20230117 13:36:58 @agent_ppo2.py:129][0m #------------------------ Iteration 486 --------------------------#
[32m[20230117 13:36:58 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:36:58 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:58 @agent_ppo2.py:193][0m |          -0.0034 |          31.1403 |           4.8259 |
[32m[20230117 13:36:58 @agent_ppo2.py:193][0m |          -0.0064 |          26.7850 |           4.8194 |
[32m[20230117 13:36:58 @agent_ppo2.py:193][0m |          -0.0083 |          24.6862 |           4.8213 |
[32m[20230117 13:36:58 @agent_ppo2.py:193][0m |          -0.0076 |          23.6097 |           4.8202 |
[32m[20230117 13:36:58 @agent_ppo2.py:193][0m |          -0.0141 |          22.8564 |           4.8161 |
[32m[20230117 13:36:58 @agent_ppo2.py:193][0m |          -0.0094 |          22.1964 |           4.8153 |
[32m[20230117 13:36:59 @agent_ppo2.py:193][0m |          -0.0098 |          21.7298 |           4.8143 |
[32m[20230117 13:36:59 @agent_ppo2.py:193][0m |          -0.0084 |          21.6186 |           4.8149 |
[32m[20230117 13:36:59 @agent_ppo2.py:193][0m |          -0.0142 |          20.9859 |           4.8121 |
[32m[20230117 13:36:59 @agent_ppo2.py:193][0m |          -0.0119 |          20.4013 |           4.8145 |
[32m[20230117 13:36:59 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:36:59 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.92
[32m[20230117 13:36:59 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.83
[32m[20230117 13:36:59 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.55
[32m[20230117 13:36:59 @agent_ppo2.py:151][0m Total time:      10.56 min
[32m[20230117 13:36:59 @agent_ppo2.py:153][0m 997376 total steps have happened
[32m[20230117 13:36:59 @agent_ppo2.py:129][0m #------------------------ Iteration 487 --------------------------#
[32m[20230117 13:36:59 @agent_ppo2.py:135][0m Sampling time: 0.23 s by 4 slaves
[32m[20230117 13:36:59 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:36:59 @agent_ppo2.py:193][0m |          -0.0022 |          23.0175 |           4.9135 |
[32m[20230117 13:36:59 @agent_ppo2.py:193][0m |          -0.0069 |          20.7502 |           4.9067 |
[32m[20230117 13:36:59 @agent_ppo2.py:193][0m |          -0.0092 |          19.8854 |           4.9093 |
[32m[20230117 13:36:59 @agent_ppo2.py:193][0m |          -0.0120 |          19.3648 |           4.9060 |
[32m[20230117 13:36:59 @agent_ppo2.py:193][0m |          -0.0120 |          19.0184 |           4.9022 |
[32m[20230117 13:37:00 @agent_ppo2.py:193][0m |          -0.0133 |          18.5951 |           4.9042 |
[32m[20230117 13:37:00 @agent_ppo2.py:193][0m |          -0.0137 |          18.4355 |           4.9034 |
[32m[20230117 13:37:00 @agent_ppo2.py:193][0m |          -0.0143 |          18.0614 |           4.9002 |
[32m[20230117 13:37:00 @agent_ppo2.py:193][0m |          -0.0148 |          18.0088 |           4.9014 |
[32m[20230117 13:37:00 @agent_ppo2.py:193][0m |          -0.0172 |          17.5786 |           4.9002 |
[32m[20230117 13:37:00 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:37:00 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.92
[32m[20230117 13:37:00 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.61
[32m[20230117 13:37:00 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.94
[32m[20230117 13:37:00 @agent_ppo2.py:151][0m Total time:      10.58 min
[32m[20230117 13:37:00 @agent_ppo2.py:153][0m 999424 total steps have happened
[32m[20230117 13:37:00 @agent_ppo2.py:129][0m #------------------------ Iteration 488 --------------------------#
[32m[20230117 13:37:00 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:37:00 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:37:00 @agent_ppo2.py:193][0m |          -0.0029 |          22.1072 |           4.9037 |
[32m[20230117 13:37:00 @agent_ppo2.py:193][0m |          -0.0096 |          18.6242 |           4.8976 |
[32m[20230117 13:37:00 @agent_ppo2.py:193][0m |          -0.0029 |          17.4588 |           4.9015 |
[32m[20230117 13:37:01 @agent_ppo2.py:193][0m |          -0.0100 |          14.8039 |           4.8964 |
[32m[20230117 13:37:01 @agent_ppo2.py:193][0m |          -0.0123 |          13.8234 |           4.9029 |
[32m[20230117 13:37:01 @agent_ppo2.py:193][0m |          -0.0016 |          14.9310 |           4.9077 |
[32m[20230117 13:37:01 @agent_ppo2.py:193][0m |          -0.0158 |          12.5506 |           4.9047 |
[32m[20230117 13:37:01 @agent_ppo2.py:193][0m |          -0.0084 |          11.9464 |           4.9095 |
[32m[20230117 13:37:01 @agent_ppo2.py:193][0m |          -0.0183 |           9.7760 |           4.9068 |
[32m[20230117 13:37:01 @agent_ppo2.py:193][0m |          -0.0157 |           8.6128 |           4.9109 |
[32m[20230117 13:37:01 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:37:01 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.82
[32m[20230117 13:37:01 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.99
[32m[20230117 13:37:01 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.93
[32m[20230117 13:37:01 @agent_ppo2.py:151][0m Total time:      10.60 min
[32m[20230117 13:37:01 @agent_ppo2.py:153][0m 1001472 total steps have happened
[32m[20230117 13:37:01 @agent_ppo2.py:129][0m #------------------------ Iteration 489 --------------------------#
[32m[20230117 13:37:01 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:37:01 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:37:01 @agent_ppo2.py:193][0m |           0.0017 |          21.1246 |           5.0322 |
[32m[20230117 13:37:01 @agent_ppo2.py:193][0m |          -0.0074 |          18.6215 |           5.0165 |
[32m[20230117 13:37:02 @agent_ppo2.py:193][0m |          -0.0096 |          17.5505 |           5.0146 |
[32m[20230117 13:37:02 @agent_ppo2.py:193][0m |          -0.0121 |          16.7366 |           5.0096 |
[32m[20230117 13:37:02 @agent_ppo2.py:193][0m |          -0.0134 |          16.2110 |           5.0098 |
[32m[20230117 13:37:02 @agent_ppo2.py:193][0m |          -0.0132 |          15.8167 |           5.0055 |
[32m[20230117 13:37:02 @agent_ppo2.py:193][0m |          -0.0154 |          15.4212 |           5.0026 |
[32m[20230117 13:37:02 @agent_ppo2.py:193][0m |          -0.0138 |          15.1210 |           5.0017 |
[32m[20230117 13:37:02 @agent_ppo2.py:193][0m |          -0.0138 |          14.8054 |           4.9989 |
[32m[20230117 13:37:02 @agent_ppo2.py:193][0m |          -0.0156 |          14.4568 |           4.9961 |
[32m[20230117 13:37:02 @agent_ppo2.py:138][0m Policy update time: 0.68 s
[32m[20230117 13:37:02 @agent_ppo2.py:146][0m Average TRAINING episode reward: 267.61
[32m[20230117 13:37:02 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.08
[32m[20230117 13:37:02 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.94
[32m[20230117 13:37:02 @agent_ppo2.py:151][0m Total time:      10.62 min
[32m[20230117 13:37:02 @agent_ppo2.py:153][0m 1003520 total steps have happened
[32m[20230117 13:37:02 @agent_ppo2.py:129][0m #------------------------ Iteration 490 --------------------------#
[32m[20230117 13:37:02 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:37:02 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:37:02 @agent_ppo2.py:193][0m |           0.0004 |          22.6736 |           4.9018 |
[32m[20230117 13:37:03 @agent_ppo2.py:193][0m |           0.0042 |          21.0964 |           4.8905 |
[32m[20230117 13:37:03 @agent_ppo2.py:193][0m |          -0.0097 |          19.6132 |           4.8843 |
[32m[20230117 13:37:03 @agent_ppo2.py:193][0m |          -0.0125 |          19.0694 |           4.8784 |
[32m[20230117 13:37:03 @agent_ppo2.py:193][0m |          -0.0055 |          18.6604 |           4.8837 |
[32m[20230117 13:37:03 @agent_ppo2.py:193][0m |          -0.0130 |          18.2748 |           4.8800 |
[32m[20230117 13:37:03 @agent_ppo2.py:193][0m |          -0.0171 |          17.9417 |           4.8772 |
[32m[20230117 13:37:03 @agent_ppo2.py:193][0m |          -0.0167 |          17.7907 |           4.8704 |
[32m[20230117 13:37:03 @agent_ppo2.py:193][0m |          -0.0148 |          17.5473 |           4.8745 |
[32m[20230117 13:37:03 @agent_ppo2.py:193][0m |          -0.0147 |          17.2970 |           4.8698 |
[32m[20230117 13:37:03 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:37:03 @agent_ppo2.py:146][0m Average TRAINING episode reward: 263.70
[32m[20230117 13:37:03 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.89
[32m[20230117 13:37:03 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.90
[32m[20230117 13:37:03 @agent_ppo2.py:151][0m Total time:      10.64 min
[32m[20230117 13:37:03 @agent_ppo2.py:153][0m 1005568 total steps have happened
[32m[20230117 13:37:03 @agent_ppo2.py:129][0m #------------------------ Iteration 491 --------------------------#
[32m[20230117 13:37:03 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:37:04 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:37:04 @agent_ppo2.py:193][0m |          -0.0021 |          19.9899 |           4.8277 |
[32m[20230117 13:37:04 @agent_ppo2.py:193][0m |          -0.0099 |          18.4163 |           4.8128 |
[32m[20230117 13:37:04 @agent_ppo2.py:193][0m |          -0.0090 |          17.8084 |           4.8067 |
[32m[20230117 13:37:04 @agent_ppo2.py:193][0m |          -0.0127 |          17.4183 |           4.8074 |
[32m[20230117 13:37:04 @agent_ppo2.py:193][0m |          -0.0110 |          17.0455 |           4.8038 |
[32m[20230117 13:37:04 @agent_ppo2.py:193][0m |          -0.0137 |          16.7048 |           4.8006 |
[32m[20230117 13:37:04 @agent_ppo2.py:193][0m |          -0.0136 |          16.3920 |           4.7998 |
[32m[20230117 13:37:04 @agent_ppo2.py:193][0m |          -0.0156 |          16.1260 |           4.8049 |
[32m[20230117 13:37:04 @agent_ppo2.py:193][0m |          -0.0120 |          15.8710 |           4.8043 |
[32m[20230117 13:37:04 @agent_ppo2.py:193][0m |          -0.0142 |          15.5276 |           4.8028 |
[32m[20230117 13:37:04 @agent_ppo2.py:138][0m Policy update time: 0.69 s
[32m[20230117 13:37:04 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.89
[32m[20230117 13:37:04 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.08
[32m[20230117 13:37:04 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.95
[32m[20230117 13:37:04 @agent_ppo2.py:151][0m Total time:      10.66 min
[32m[20230117 13:37:04 @agent_ppo2.py:153][0m 1007616 total steps have happened
[32m[20230117 13:37:04 @agent_ppo2.py:129][0m #------------------------ Iteration 492 --------------------------#
[32m[20230117 13:37:05 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:37:05 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:37:05 @agent_ppo2.py:193][0m |          -0.0048 |          20.2364 |           4.8627 |
[32m[20230117 13:37:05 @agent_ppo2.py:193][0m |          -0.0112 |          19.6191 |           4.8549 |
[32m[20230117 13:37:05 @agent_ppo2.py:193][0m |          -0.0079 |          19.2279 |           4.8564 |
[32m[20230117 13:37:05 @agent_ppo2.py:193][0m |          -0.0119 |          19.0271 |           4.8487 |
[32m[20230117 13:37:05 @agent_ppo2.py:193][0m |          -0.0144 |          18.7957 |           4.8479 |
[32m[20230117 13:37:05 @agent_ppo2.py:193][0m |          -0.0065 |          18.6064 |           4.8495 |
[32m[20230117 13:37:05 @agent_ppo2.py:193][0m |          -0.0129 |          18.3894 |           4.8475 |
[32m[20230117 13:37:05 @agent_ppo2.py:193][0m |          -0.0146 |          18.1870 |           4.8493 |
[32m[20230117 13:37:05 @agent_ppo2.py:193][0m |          -0.0165 |          18.0003 |           4.8513 |
[32m[20230117 13:37:05 @agent_ppo2.py:193][0m |          -0.0171 |          17.7720 |           4.8517 |
[32m[20230117 13:37:05 @agent_ppo2.py:138][0m Policy update time: 0.70 s
[32m[20230117 13:37:05 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.70
[32m[20230117 13:37:05 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.78
[32m[20230117 13:37:05 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 273.77
[32m[20230117 13:37:05 @agent_ppo2.py:151][0m Total time:      10.67 min
[32m[20230117 13:37:05 @agent_ppo2.py:153][0m 1009664 total steps have happened
[32m[20230117 13:37:05 @agent_ppo2.py:129][0m #------------------------ Iteration 493 --------------------------#
[32m[20230117 13:37:06 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:37:06 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:37:06 @agent_ppo2.py:193][0m |           0.0016 |          28.1614 |           4.9291 |
[32m[20230117 13:37:06 @agent_ppo2.py:193][0m |          -0.0058 |          21.7255 |           4.9244 |
[32m[20230117 13:37:06 @agent_ppo2.py:193][0m |          -0.0083 |          20.3335 |           4.9207 |
[32m[20230117 13:37:06 @agent_ppo2.py:193][0m |          -0.0094 |          19.5423 |           4.9174 |
[32m[20230117 13:37:06 @agent_ppo2.py:193][0m |          -0.0103 |          19.0511 |           4.9221 |
[32m[20230117 13:37:06 @agent_ppo2.py:193][0m |          -0.0124 |          18.9882 |           4.9226 |
[32m[20230117 13:37:06 @agent_ppo2.py:193][0m |          -0.0116 |          18.5821 |           4.9240 |
[32m[20230117 13:37:06 @agent_ppo2.py:193][0m |          -0.0135 |          18.1466 |           4.9198 |
[32m[20230117 13:37:06 @agent_ppo2.py:193][0m |          -0.0144 |          17.9253 |           4.9197 |
[32m[20230117 13:37:06 @agent_ppo2.py:193][0m |          -0.0147 |          17.5187 |           4.9176 |
[32m[20230117 13:37:06 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:37:07 @agent_ppo2.py:146][0m Average TRAINING episode reward: 188.92
[32m[20230117 13:37:07 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 268.91
[32m[20230117 13:37:07 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.46
[32m[20230117 13:37:07 @agent_ppo2.py:151][0m Total time:      10.69 min
[32m[20230117 13:37:07 @agent_ppo2.py:153][0m 1011712 total steps have happened
[32m[20230117 13:37:07 @agent_ppo2.py:129][0m #------------------------ Iteration 494 --------------------------#
[32m[20230117 13:37:07 @agent_ppo2.py:135][0m Sampling time: 0.27 s by 4 slaves
[32m[20230117 13:37:07 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:37:07 @agent_ppo2.py:193][0m |          -0.0013 |          23.6228 |           4.9823 |
[32m[20230117 13:37:07 @agent_ppo2.py:193][0m |          -0.0061 |          18.5595 |           4.9732 |
[32m[20230117 13:37:07 @agent_ppo2.py:193][0m |          -0.0118 |          16.9770 |           4.9710 |
[32m[20230117 13:37:07 @agent_ppo2.py:193][0m |          -0.0133 |          16.1660 |           4.9732 |
[32m[20230117 13:37:07 @agent_ppo2.py:193][0m |          -0.0143 |          15.5993 |           4.9657 |
[32m[20230117 13:37:07 @agent_ppo2.py:193][0m |          -0.0143 |          15.2338 |           4.9705 |
[32m[20230117 13:37:07 @agent_ppo2.py:193][0m |          -0.0158 |          14.9500 |           4.9693 |
[32m[20230117 13:37:07 @agent_ppo2.py:193][0m |          -0.0179 |          14.7098 |           4.9718 |
[32m[20230117 13:37:07 @agent_ppo2.py:193][0m |          -0.0185 |          14.4405 |           4.9710 |
[32m[20230117 13:37:08 @agent_ppo2.py:193][0m |          -0.0185 |          14.2525 |           4.9695 |
[32m[20230117 13:37:08 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:37:08 @agent_ppo2.py:146][0m Average TRAINING episode reward: 203.61
[32m[20230117 13:37:08 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 269.07
[32m[20230117 13:37:08 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 272.34
[32m[20230117 13:37:08 @agent_ppo2.py:151][0m Total time:      10.71 min
[32m[20230117 13:37:08 @agent_ppo2.py:153][0m 1013760 total steps have happened
[32m[20230117 13:37:08 @agent_ppo2.py:129][0m #------------------------ Iteration 495 --------------------------#
[32m[20230117 13:37:08 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:37:08 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:37:08 @agent_ppo2.py:193][0m |          -0.0025 |          17.8923 |           4.9310 |
[32m[20230117 13:37:08 @agent_ppo2.py:193][0m |          -0.0016 |          14.9484 |           4.9158 |
[32m[20230117 13:37:08 @agent_ppo2.py:193][0m |          -0.0082 |          13.5670 |           4.9168 |
[32m[20230117 13:37:08 @agent_ppo2.py:193][0m |          -0.0092 |          12.8674 |           4.9183 |
[32m[20230117 13:37:08 @agent_ppo2.py:193][0m |          -0.0113 |          12.4042 |           4.9091 |
[32m[20230117 13:37:08 @agent_ppo2.py:193][0m |          -0.0087 |          12.1493 |           4.9029 |
[32m[20230117 13:37:08 @agent_ppo2.py:193][0m |          -0.0102 |          11.8376 |           4.9082 |
[32m[20230117 13:37:08 @agent_ppo2.py:193][0m |          -0.0131 |          11.4021 |           4.9005 |
[32m[20230117 13:37:09 @agent_ppo2.py:193][0m |          -0.0135 |          11.1189 |           4.8991 |
[32m[20230117 13:37:09 @agent_ppo2.py:193][0m |          -0.0140 |          10.8580 |           4.8979 |
[32m[20230117 13:37:09 @agent_ppo2.py:138][0m Policy update time: 0.72 s
[32m[20230117 13:37:09 @agent_ppo2.py:146][0m Average TRAINING episode reward: 262.84
[32m[20230117 13:37:09 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 264.69
[32m[20230117 13:37:09 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.49
[32m[20230117 13:37:09 @agent_ppo2.py:151][0m Total time:      10.73 min
[32m[20230117 13:37:09 @agent_ppo2.py:153][0m 1015808 total steps have happened
[32m[20230117 13:37:09 @agent_ppo2.py:129][0m #------------------------ Iteration 496 --------------------------#
[32m[20230117 13:37:09 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:37:09 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:37:09 @agent_ppo2.py:193][0m |           0.0004 |          32.0027 |           4.8664 |
[32m[20230117 13:37:09 @agent_ppo2.py:193][0m |          -0.0090 |          24.3513 |           4.8523 |
[32m[20230117 13:37:09 @agent_ppo2.py:193][0m |          -0.0079 |          23.0403 |           4.8492 |
[32m[20230117 13:37:09 @agent_ppo2.py:193][0m |          -0.0136 |          22.2518 |           4.8444 |
[32m[20230117 13:37:09 @agent_ppo2.py:193][0m |          -0.0132 |          21.6832 |           4.8403 |
[32m[20230117 13:37:09 @agent_ppo2.py:193][0m |          -0.0160 |          21.0839 |           4.8412 |
[32m[20230117 13:37:09 @agent_ppo2.py:193][0m |          -0.0149 |          20.5781 |           4.8397 |
[32m[20230117 13:37:10 @agent_ppo2.py:193][0m |          -0.0217 |          20.1785 |           4.8364 |
[32m[20230117 13:37:10 @agent_ppo2.py:193][0m |          -0.0200 |          20.2142 |           4.8372 |
[32m[20230117 13:37:10 @agent_ppo2.py:193][0m |           0.0086 |          22.2657 |           4.8361 |
[32m[20230117 13:37:10 @agent_ppo2.py:138][0m Policy update time: 0.63 s
[32m[20230117 13:37:10 @agent_ppo2.py:146][0m Average TRAINING episode reward: 207.98
[32m[20230117 13:37:10 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.26
[32m[20230117 13:37:10 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 267.47
[32m[20230117 13:37:10 @agent_ppo2.py:151][0m Total time:      10.75 min
[32m[20230117 13:37:10 @agent_ppo2.py:153][0m 1017856 total steps have happened
[32m[20230117 13:37:10 @agent_ppo2.py:129][0m #------------------------ Iteration 497 --------------------------#
[32m[20230117 13:37:10 @agent_ppo2.py:135][0m Sampling time: 0.22 s by 4 slaves
[32m[20230117 13:37:10 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:37:10 @agent_ppo2.py:193][0m |           0.0018 |          20.3758 |           4.7760 |
[32m[20230117 13:37:10 @agent_ppo2.py:193][0m |          -0.0038 |          19.6705 |           4.7703 |
[32m[20230117 13:37:10 @agent_ppo2.py:193][0m |          -0.0074 |          19.1593 |           4.7699 |
[32m[20230117 13:37:10 @agent_ppo2.py:193][0m |          -0.0093 |          18.7806 |           4.7643 |
[32m[20230117 13:37:10 @agent_ppo2.py:193][0m |          -0.0084 |          18.6278 |           4.7620 |
[32m[20230117 13:37:10 @agent_ppo2.py:193][0m |          -0.0092 |          18.3747 |           4.7600 |
[32m[20230117 13:37:11 @agent_ppo2.py:193][0m |          -0.0117 |          18.0824 |           4.7591 |
[32m[20230117 13:37:11 @agent_ppo2.py:193][0m |          -0.0118 |          17.9390 |           4.7564 |
[32m[20230117 13:37:11 @agent_ppo2.py:193][0m |          -0.0107 |          17.9684 |           4.7535 |
[32m[20230117 13:37:11 @agent_ppo2.py:193][0m |          -0.0118 |          17.8216 |           4.7523 |
[32m[20230117 13:37:11 @agent_ppo2.py:138][0m Policy update time: 0.69 s
[32m[20230117 13:37:11 @agent_ppo2.py:146][0m Average TRAINING episode reward: 265.27
[32m[20230117 13:37:11 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.68
[32m[20230117 13:37:11 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.58
[32m[20230117 13:37:11 @agent_ppo2.py:151][0m Total time:      10.77 min
[32m[20230117 13:37:11 @agent_ppo2.py:153][0m 1019904 total steps have happened
[32m[20230117 13:37:11 @agent_ppo2.py:129][0m #------------------------ Iteration 498 --------------------------#
[32m[20230117 13:37:11 @agent_ppo2.py:135][0m Sampling time: 0.21 s by 4 slaves
[32m[20230117 13:37:11 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:37:11 @agent_ppo2.py:193][0m |           0.0005 |          20.7014 |           4.8455 |
[32m[20230117 13:37:11 @agent_ppo2.py:193][0m |          -0.0057 |          18.8724 |           4.8397 |
[32m[20230117 13:37:11 @agent_ppo2.py:193][0m |          -0.0083 |          18.1653 |           4.8351 |
[32m[20230117 13:37:11 @agent_ppo2.py:193][0m |          -0.0093 |          17.8604 |           4.8342 |
[32m[20230117 13:37:12 @agent_ppo2.py:193][0m |          -0.0097 |          17.5777 |           4.8348 |
[32m[20230117 13:37:12 @agent_ppo2.py:193][0m |          -0.0108 |          17.4021 |           4.8374 |
[32m[20230117 13:37:12 @agent_ppo2.py:193][0m |          -0.0112 |          17.2997 |           4.8352 |
[32m[20230117 13:37:12 @agent_ppo2.py:193][0m |          -0.0136 |          16.9793 |           4.8352 |
[32m[20230117 13:37:12 @agent_ppo2.py:193][0m |          -0.0127 |          16.8631 |           4.8315 |
[32m[20230117 13:37:12 @agent_ppo2.py:193][0m |          -0.0142 |          16.6752 |           4.8340 |
[32m[20230117 13:37:12 @agent_ppo2.py:138][0m Policy update time: 0.71 s
[32m[20230117 13:37:12 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.26
[32m[20230117 13:37:12 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 267.45
[32m[20230117 13:37:12 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 268.72
[32m[20230117 13:37:12 @agent_ppo2.py:151][0m Total time:      10.78 min
[32m[20230117 13:37:12 @agent_ppo2.py:153][0m 1021952 total steps have happened
[32m[20230117 13:37:12 @agent_ppo2.py:129][0m #------------------------ Iteration 499 --------------------------#
[32m[20230117 13:37:12 @agent_ppo2.py:135][0m Sampling time: 0.20 s by 4 slaves
[32m[20230117 13:37:12 @agent_ppo2.py:169][0m |      policy_loss |       value_loss |          entropy |
[32m[20230117 13:37:12 @agent_ppo2.py:193][0m |           0.0020 |          22.2960 |           4.7501 |
[32m[20230117 13:37:12 @agent_ppo2.py:193][0m |          -0.0030 |          20.2659 |           4.7546 |
[32m[20230117 13:37:12 @agent_ppo2.py:193][0m |          -0.0059 |          19.1174 |           4.7549 |
[32m[20230117 13:37:13 @agent_ppo2.py:193][0m |          -0.0063 |          18.6424 |           4.7556 |
[32m[20230117 13:37:13 @agent_ppo2.py:193][0m |          -0.0065 |          18.3897 |           4.7529 |
[32m[20230117 13:37:13 @agent_ppo2.py:193][0m |          -0.0084 |          18.0014 |           4.7510 |
[32m[20230117 13:37:13 @agent_ppo2.py:193][0m |          -0.0109 |          17.5910 |           4.7518 |
[32m[20230117 13:37:13 @agent_ppo2.py:193][0m |          -0.0114 |          17.4043 |           4.7574 |
[32m[20230117 13:37:13 @agent_ppo2.py:193][0m |          -0.0110 |          17.2642 |           4.7535 |
[32m[20230117 13:37:13 @agent_ppo2.py:193][0m |          -0.0121 |          17.1543 |           4.7552 |
[32m[20230117 13:37:13 @agent_ppo2.py:138][0m Policy update time: 0.69 s
[32m[20230117 13:37:13 @agent_ppo2.py:146][0m Average TRAINING episode reward: 264.99
[32m[20230117 13:37:13 @agent_ppo2.py:147][0m Maximum TRAINING episode reward: 265.46
[32m[20230117 13:37:13 @agent_ppo2.py:148][0m Average EVALUATION episode reward: 270.51
[32m[20230117 13:37:13 @agent_ppo2.py:108][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 285.68
[32m[20230117 13:37:13 @agent_ppo2.py:151][0m Total time:      10.80 min
[32m[20230117 13:37:13 @agent_ppo2.py:153][0m 1024000 total steps have happened
[32m[20230117 13:37:13 @train.py:54][0m [4m[34mCRITICAL[0m Training completed!
